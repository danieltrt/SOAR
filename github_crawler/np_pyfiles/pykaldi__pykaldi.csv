file_path,api_count,code
setup.py,1,"b'#!/usr/bin/env python\n""""""Setup configuration.""""""\nfrom __future__ import print_function\n\nimport os\nimport platform\nimport subprocess\nimport sys\n\nimport distutils.command.build\nimport setuptools.command.build_ext\nimport setuptools.command.install_lib\nimport setuptools.command.test\nimport setuptools.extension\n\nfrom distutils.file_util import copy_file\nfrom setuptools import setup, find_packages, Command\n\ndef check_output(*args, **kwargs):\n    return subprocess.check_output(*args, **kwargs).decode(""utf-8"").strip()\n\n################################################################################\n# Set minimum version requirements for external dependencies\n################################################################################\n\nKALDI_MIN_REQUIRED = \'5dc5d41bb603ba935c6244c7b32788ea90b9cee3\'\n\n################################################################################\n# Check variables / find programs\n################################################################################\n\nDEBUG = os.getenv(\'DEBUG\', \'NO\').upper() in [\'ON\', \'1\', \'YES\', \'TRUE\', \'Y\']\nPYCLIF = os.getenv(""PYCLIF"")\nCLIF_MATCHER = os.getenv(\'CLIF_MATCHER\')\nKALDI_DIR = os.getenv(\'KALDI_DIR\')\nCWD = os.path.dirname(os.path.abspath(__file__))\nBUILD_DIR = os.path.join(CWD, \'build\')\n\nif not PYCLIF:\n    PYCLIF = os.path.join(sys.prefix, \'bin/pyclif\')\nPYCLIF = os.path.abspath(PYCLIF)\n\nif not (os.path.isfile(PYCLIF) and os.access(PYCLIF, os.X_OK)):\n    try:\n        PYCLIF = check_output([\'which\', \'pyclif\'])\n    except subprocess.CalledProcessError:\n        print(""\\nCould not find pyclif.\\nPlease add pyclif binary to your PATH ""\n             ""or set PYCLIF environment variable."", file=sys.stderr)\n        sys.exit(1)\n\nif not CLIF_MATCHER:\n    CLIF_MATCHER = os.path.join(sys.prefix, \'clang/bin/clif-matcher\')\nCLIF_MATCHER = os.path.abspath(CLIF_MATCHER)\n\nif not (os.path.isfile(CLIF_MATCHER) and os.access(CLIF_MATCHER, os.X_OK)):\n    print(""\\nCould not find clif-matcher.\\nPlease make sure CLIF was installed ""\n          ""under the current python environment or set CLIF_MATCHER ""\n          ""environment variable."", file=sys.stderr)\n    sys.exit(1)\n\nCLANG = os.path.join(os.path.dirname(CLIF_MATCHER), ""clang"")\nRESOURCE_DIR = check_output(""echo \'#include <limits.h>\' | {} -xc -v - 2>&1 ""\n                            ""| tr \' \' \'\\n\' | grep -A1 resource-dir | tail -1""\n                            .format(CLANG), shell=True)\nCLIF_CXX_FLAGS=""-I{}/include"".format(RESOURCE_DIR)\n\nif not KALDI_DIR:\n    KALDI_DIR = os.path.join(CWD, ""tools/kaldi"")\nKALDI_DIR = os.path.abspath(KALDI_DIR)\n\nKALDI_MK_PATH = os.path.join(KALDI_DIR, ""src"", ""kaldi.mk"")\nif not os.path.isfile(KALDI_MK_PATH):\n  print(""\\nCould not find Kaldi.\\nPlease install Kaldi under the tools ""\n        ""directory or set KALDI_DIR environment variable."", file=sys.stderr)\n  sys.exit(1)\n\ntry:\n    KALDI_HEAD = check_output([\'git\', \'-C\', KALDI_DIR, \'rev-parse\', \'HEAD\'])\n    subprocess.check_call([\'git\', \'-C\', KALDI_DIR, \'merge-base\',\n                           \'--is-ancestor\', KALDI_MIN_REQUIRED, KALDI_HEAD])\nexcept subprocess.CalledProcessError:\n    print(""\\nKaldi installation at {} is not supported.\\nPlease update Kaldi ""\n          ""to match https://github.com/pykaldi/kaldi/tree/pykaldi.""\n          .format(KALDI_DIR), file=sys.stderr)\n    sys.exit(1)\n\nwith open(""Makefile"", ""w"") as makefile:\n    print(""include {}"".format(KALDI_MK_PATH), file=makefile)\n    print(""print-% : ; @echo $($*)"", file=makefile)\nCXX_FLAGS = check_output([\'make\', \'print-CXXFLAGS\'])\nLD_FLAGS = check_output([\'make\', \'print-LDFLAGS\'])\nLD_LIBS = check_output([\'make\', \'print-LDLIBS\'])\nCUDA = check_output([\'make\', \'print-CUDA\']).upper() == \'TRUE\'\nif CUDA:\n    CUDA_LD_FLAGS = check_output([\'make\', \'print-CUDA_LDFLAGS\'])\n    CUDA_LD_LIBS = check_output([\'make\', \'print-CUDA_LDLIBS\'])\nsubprocess.check_call([""rm"", ""Makefile""])\n\nTFRNNLM_LIB_PATH = os.path.join(KALDI_DIR, ""src"", ""lib"",\n                                ""libkaldi-tensorflow-rnnlm.so"")\nKALDI_TFRNNLM = True if os.path.exists(TFRNNLM_LIB_PATH) else False\nif KALDI_TFRNNLM:\n    with open(""Makefile"", ""w"") as makefile:\n        TF_DIR = os.path.join(KALDI_DIR, ""tools"", ""tensorflow"")\n        print(""TENSORFLOW = {}"".format(TF_DIR), file=makefile)\n        TFRNNLM_MK_PATH = os.path.join(KALDI_DIR, ""src"", ""tfrnnlm"",\n                                       ""Makefile"")\n        for line in open(TFRNNLM_MK_PATH):\n            if line.startswith(""include"") or line.startswith(""TENSORFLOW""):\n                continue\n            print(line, file=makefile, end=\'\')\n        print(""print-% : ; @echo $($*)"", file=makefile)\n    TFRNNLM_CXX_FLAGS = check_output([\'make\', \'print-EXTRA_CXXFLAGS\'])\n    TF_LIB_DIR = os.path.join(KALDI_DIR, ""tools"", ""tensorflow"",\n                              ""bazel-bin"", ""tensorflow"")\n    subprocess.check_call([""rm"", ""Makefile""])\n\nif platform.system() == ""Darwin"":\n    XCODE_TOOLCHAIN_DIR = ""/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain""\n    COMMAND_LINE_TOOLCHAIN_DIR = ""/Library/Developer/CommandLineTools""\n    if os.path.isdir(XCODE_TOOLCHAIN_DIR):\n        TOOLCHAIN_DIR = XCODE_TOOLCHAIN_DIR\n    elif os.path.isdir(COMMAND_LINE_TOOLCHAIN_DIR):\n        TOOLCHAIN_DIR = COMMAND_LINE_TOOLCHAIN_DIR\n    else:\n        print(""\\nCould not find toolchain directory!\\nInstall xcode command ""\n              ""line tools, e.g. xcode-select --install"", file=sys.stderr)\n        sys.exit(1)\n    CXX_SYSTEM_INCLUDE_DIR = os.path.join(TOOLCHAIN_DIR, ""usr/include/c++/v1"")\n    CLIF_CXX_FLAGS += "" -isystem {}"".format(CXX_SYSTEM_INCLUDE_DIR)\n    LD_FLAGS += "" -undefined dynamic_lookup""\nelif platform.system() == ""Linux"":\n    CXX_FLAGS += "" -Wno-maybe-uninitialized""\n    LD_FLAGS += "" -Wl,--as-needed""\n    if DEBUG:\n        LD_FLAGS += "" -Wl,--no-undefined""\n\nMAKE_NUM_JOBS = os.getenv(\'MAKE_NUM_JOBS\')\nif not MAKE_NUM_JOBS:\n    # This is the logic ninja uses to guess the number of parallel jobs.\n    NPROC = int(check_output([\'getconf\', \'_NPROCESSORS_ONLN\']))\n    if NPROC < 2:\n        MAKE_NUM_JOBS = \'2\'\n    elif NPROC == 2:\n        MAKE_NUM_JOBS = \'3\'\n    else:\n        MAKE_NUM_JOBS = str(NPROC + 2)\nMAKE_ARGS = [\'-j\', MAKE_NUM_JOBS]\ntry:\n    import ninja\n    CMAKE_GENERATOR = \'-GNinja\'\n    MAKE = \'ninja\'\n    if DEBUG:\n        MAKE_ARGS += [\'-v\']\nexcept ImportError:\n    CMAKE_GENERATOR = \'\'\n    MAKE = \'make\'\n    if DEBUG:\n        MAKE_ARGS += [\'-d\']\n\nif DEBUG:\n    print(""#""*50)\n    print(""CWD:"", CWD)\n    print(""PYCLIF:"", PYCLIF)\n    print(""CLIF_MATCHER:"", CLIF_MATCHER)\n    print(""KALDI_DIR:"", KALDI_DIR)\n    print(""CXX_FLAGS:"", CXX_FLAGS)\n    print(""CLIF_CXX_FLAGS:"", CLIF_CXX_FLAGS)\n    print(""LD_FLAGS:"", LD_FLAGS)\n    print(""LD_LIBS:"", LD_LIBS)\n    print(""BUILD_DIR:"", BUILD_DIR)\n    print(""CUDA:"", CUDA)\n    if CUDA:\n        print(""CUDA_LD_FLAGS:"", CUDA_LD_FLAGS)\n        print(""CUDA_LD_LIBS:"", CUDA_LD_LIBS)\n    print(""MAKE:"", MAKE, *MAKE_ARGS)\n    print(""#""*50)\n\n################################################################################\n# Use CMake to build Python extensions in parallel\n################################################################################\n\nclass Extension(setuptools.extension.Extension):\n    """"""Dummy extension class that only holds the name of the extension.""""""\n    def __init__(self, name):\n        setuptools.extension.Extension.__init__(self, name, [])\n        self._needs_stub = False\n    def __str__(self):\n        return ""Extension({})"".format(self.name)\n\n\ndef populate_extension_list():\n    extensions = []\n    lib_dir = os.path.join(BUILD_DIR, ""lib"")\n    for dirpath, _, filenames in os.walk(os.path.join(lib_dir, ""kaldi"")):\n\n        lib_path = os.path.relpath(dirpath, lib_dir)\n\n        if lib_path == ""."":\n            lib_path = ""kaldi""\n\n        for f in filenames:\n            r, e = os.path.splitext(f)\n            if e == "".so"":\n                ext_name = ""{}.{}"".format(lib_path, r)\n                extensions.append(Extension(ext_name))\n    return extensions\n\n\nclass build(distutils.command.build.build):\n    def finalize_options(self):\n        self.build_base = \'build\'\n        self.build_lib = \'build/lib\'\n        distutils.command.build.build.finalize_options(self)\n\n\nclass build_ext(setuptools.command.build_ext.build_ext):\n    def run(self):\n        old_inplace, self.inplace = self.inplace, 0\n\n        import numpy as np\n        CMAKE_ARGS = [\'-DKALDI_DIR=\' + KALDI_DIR,\n                      \'-DPYCLIF=\' + PYCLIF,\n                      \'-DCLIF_MATCHER=\' + CLIF_MATCHER,\n                      \'-DCXX_FLAGS=\' + CXX_FLAGS,\n                      \'-DCLIF_CXX_FLAGS=\' + CLIF_CXX_FLAGS,\n                      \'-DLD_FLAGS=\' + LD_FLAGS,\n                      \'-DLD_LIBS=\' + LD_LIBS,\n                      \'-DNUMPY_INC_DIR=\'+ np.get_include(),\n                      \'-DCUDA=TRUE\' if CUDA else \'-DCUDA=FALSE\',\n                      \'-DTFRNNLM=TRUE\' if KALDI_TFRNNLM else \'-DTFRNNLM=FALSE\',\n                      \'-DDEBUG=TRUE\' if DEBUG else \'-DDEBUG=FALSE\']\n\n        if CUDA:\n            CMAKE_ARGS +=[\'-DCUDA_LD_FLAGS=\' + CUDA_LD_FLAGS,\n                          \'-DCUDA_LD_LIBS=\' + CUDA_LD_LIBS]\n\n        if KALDI_TFRNNLM:\n            CMAKE_ARGS +=[\'-DTFRNNLM_CXX_FLAGS=\' + TFRNNLM_CXX_FLAGS,\n                          \'-DTF_LIB_DIR=\' + TF_LIB_DIR]\n\n        if CMAKE_GENERATOR:\n            CMAKE_ARGS += [CMAKE_GENERATOR]\n\n        if DEBUG:\n            CMAKE_ARGS += [\'-DCMAKE_VERBOSE_MAKEFILE:BOOL=ON\']\n        else:\n            CMAKE_ARGS += [\'-Wno-dev\']\n\n        if not os.path.exists(BUILD_DIR):\n            os.makedirs(BUILD_DIR)\n\n        try:\n            subprocess.check_call([\'cmake\', \'..\'] + CMAKE_ARGS, cwd = BUILD_DIR)\n            subprocess.check_call([MAKE] + MAKE_ARGS, cwd = BUILD_DIR)\n        except subprocess.CalledProcessError as err:\n            # We catch this exception to disable stack trace.\n            print(str(err), file=sys.stderr)\n            sys.exit(1)\n        print() # Add an empty line for cleaner output\n\n        self.extensions = populate_extension_list()\n\n        if DEBUG:\n            for ext in self.extensions:\n                print(ext)\n            self.verbose = True\n\n        self.inplace = old_inplace\n        if old_inplace:\n            self.copy_extensions_to_source()\n\n    def get_ext_filename(self, fullname):\n        """"""Convert the name of an extension (eg. ""foo.bar"") into the name\n        of the file from which it will be loaded (eg. ""foo/bar.so""). This\n        patch overrides platform specific extension suffix with "".so"".\n        """"""\n        ext_path = fullname.split(\'.\')\n        ext_suffix = \'.so\'\n        return os.path.join(*ext_path) + ext_suffix\n\n\nclass build_sphinx(Command):\n    user_options = []\n    description = ""Builds documentation using sphinx.""\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            import sphinx\n            subprocess.check_call([MAKE, \'docs\'], cwd = BUILD_DIR)\n        except ImportError:\n            print(""Sphinx was not found. Install it using pip install sphinx."",\n                  file=sys.stderr)\n            sys.exit(1)\n\n\nclass install_lib(setuptools.command.install_lib.install_lib):\n    def install(self):\n        self.build_dir = \'build/lib\'\n        setuptools.command.install_lib.install_lib.install(self)\n\n\nclass test_cuda(setuptools.command.test.test):\n    def run_tests(self):\n        from kaldi.cudamatrix import cuda_available\n        if cuda_available():\n            from kaldi.cudamatrix import CuDevice\n            CuDevice.instantiate().set_debug_stride_mode(True)\n            CuDevice.instantiate().select_gpu_id(""yes"")\n            super(test_cuda, self).run_tests()\n            CuDevice.instantiate().print_profile()\n        else:\n            print(""CUDA not available. Running tests on CPU."")\n            super(test_cuda, self).run_tests()\n\n\n################################################################################\n# Setup pykaldi\n################################################################################\n\n# We add a \'dummy\' extension so that setuptools runs the build_ext step.\nextensions = [Extension(""kaldi"")]\n\npackages = find_packages(exclude=[""tests.*"", ""tests""])\n\nwith open(os.path.join(\'kaldi\', \'__version__.py\')) as f:\n    exec(f.read())\n\nsetup(name = \'pykaldi\',\n      version = __version__,\n      description = \'A Python wrapper for Kaldi\',\n      author = \'Dogan Can, Victor Martinez\',\n      ext_modules=extensions,\n      cmdclass = {\n          \'build\': build,\n          \'build_ext\': build_ext,\n          \'build_sphinx\': build_sphinx,\n          \'install_lib\': install_lib,\n          \'test_cuda\': test_cuda,\n          },\n      packages = packages,\n      package_data = {},\n      install_requires = [\'enum34;python_version<""3.4""\', \'numpy\'],\n      setup_requires=[\'pytest-runner\'],\n      tests_require=[\'pytest\'],\n      zip_safe = False,\n      test_suite=\'tests\')\n'"
docs/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# PyKaldi documentation build configuration file, created by\n# sphinx-quickstart on Tue Aug 22 22:34:13 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'../kaldi\'))\n\nimport kaldi\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.githubpages\'\n    # \'sphinxcontrib.doxylink\'\n    ]\n\nnapoleon_use_ivar = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The name of the default reST role (builtin or Sphinx extension)\ndefault_role = \'py:obj\'\n\n# General information about the project.\nproject = \'PyKaldi\'\ncopyright = \'2017-2018, Do\xc4\x9fan Can, Victor Martinez\'\nauthor = \'Do\xc4\x9fan Can, Victor Martinez\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = kaldi.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = kaldi.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {\n#     \'github_user\': \'pykaldi\',\n#     \'github_repo\': \'pykaldi\',\n#     \'github_banner\': True\n# }\n\nhtml_theme_options = {\n    \'logo_only\': True\n}\n\nhtml_logo = ""_static/pykaldi-logo-light.png""\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# html_style_path = \'_static/pykaldi-theme.css\'\nhtml_context = {\n    \'css_files\': [\n        \'https://fonts.googleapis.com/css?family=Lato\',\n        \'_static/pykaldi-theme.css\'\n    ],\n}\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\n# html_sidebars = {\n#     \'**\': [\n#         \'about.html\',\n#         \'globaltoc.html\',\n#         \'relations.html\',  # needs \'show_related\': True theme option to display\n#         \'searchbox.html\'\n#     ]\n# }\n\n\nautosummary_generate = True\n\n# Order or members in documentation\n# autodoc_member_order = \'alphabetical\'\n\n# List of autodoc directive flags that should be\n# automatically applied to all autodoc directives\nautodoc_default_flags = [\'members\', \'undoc-members\', \'inherited-members\']\n\n# Append __init__ docstring to class docstring.\nautoclass_content = \'both\'\n\n# Custom handler for including/excluding members in documentation.\ndef skip_member_handler(app, what, name, obj, skip, options):\n    # Document __init__ methods.\n    # if name == ""__init__"":\n    #     return False\n\n    # Skip upcasts.\n    if name.startswith(""as_""):\n        return True\n    return None\n\n\nfrom sphinx.ext import autodoc\n\nclass ConstantDataDocumenter(autodoc.DataDocumenter):\n    objtype = ""constant""\n    directivetype = ""data""\n\n    def get_doc(self, encoding=None, ignore=1):\n        return []\n\ndef setup(app):\n    app.connect(""autodoc-skip-member"", skip_member_handler)\n    app.add_autodocumenter(ConstantDataDocumenter)\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'PyKaldiDoc\'\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n}\n\n# Doxylink configuration for linking to kaldi documentation.\n# doxylink = { \'kaldi\': (\'/home/dogan/tools/kaldi/src/kaldi.tag\',\n#                        \'http://kaldi-asr.org/doc/\') }\n\n# -- A patch that prevents Sphinx from cross-referencing ivar tags -------\n# See http://stackoverflow.com/a/41184353/3343043\n\nfrom docutils import nodes\nfrom sphinx.util.docfields import TypedField\nfrom sphinx import addnodes\n\n\ndef patched_make_field(self, types, domain, items, **kw):\n    # `kw` catches `env=None` needed for newer sphinx while maintaining\n    #  backwards compatibility when passed along further down!\n\n    # type: (List, unicode, Tuple) -> nodes.field\n    def handle_item(fieldarg, content):\n        par = nodes.paragraph()\n        par += addnodes.literal_strong(\'\', fieldarg)  # Patch: this line added\n        # par.extend(self.make_xrefs(self.rolename, domain, fieldarg,\n        #                           addnodes.literal_strong))\n        if fieldarg in types:\n            par += nodes.Text(\' (\')\n            # NOTE: using .pop() here to prevent a single type node to be\n            # inserted twice into the doctree, which leads to\n            # inconsistencies later when references are resolved\n            fieldtype = types.pop(fieldarg)\n            if len(fieldtype) == 1 and isinstance(fieldtype[0], nodes.Text):\n                typename = u\'\'.join(n.astext() for n in fieldtype)\n                typename = typename.replace(\'int\', \'python:int\')\n                typename = typename.replace(\'long\', \'python:long\')\n                typename = typename.replace(\'float\', \'python:float\')\n                typename = typename.replace(\'type\', \'python:type\')\n                par.extend(self.make_xrefs(self.typerolename, domain, typename,\n                                           addnodes.literal_emphasis, **kw))\n            else:\n                par += fieldtype\n            par += nodes.Text(\')\')\n        par += nodes.Text(\' -- \')\n        par += content\n        return par\n\n    fieldname = nodes.field_name(\'\', self.label)\n    if len(items) == 1 and self.can_collapse:\n        fieldarg, content = items[0]\n        bodynode = handle_item(fieldarg, content)\n    else:\n        bodynode = self.list_type()\n        for fieldarg, content in items:\n            bodynode += nodes.list_item(\'\', handle_item(fieldarg, content))\n    fieldbody = nodes.field_body(\'\', bodynode)\n    return nodes.field(\'\', fieldname, fieldbody)\n\nTypedField.make_field = patched_make_field\n\n\n# -- A patch that prevents Sphinx from grouping index entries -------\nimport re\nfrom sphinx.environment.adapters import indexentries\n\nclass IndexEntries(indexentries.IndexEntries):\n    def create_index(self, builder, group_entries=False,\n                     _fixre=re.compile(r\'(.*) ([(][^()]*[)])\')):\n        return super(IndexEntries, self).create_index(builder, group_entries,\n                                                      _fixre)\n\nindexentries.IndexEntries = IndexEntries\n'"
docs/generate_api_docs.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport pkgutil\nimport sys\n\nfrom subprocess import check_call\n\nimport kaldi\n\n\nparser = argparse.ArgumentParser(\n    description=""Generates autosummary documentation for pykaldi."")\n# parser.add_argument(\'--force\', \'-f\', action=\'store_true\',\n#                     help=\'Overwrite files. Default: False.\')\nparser.add_argument(\'--out_dir\', \'-o\', default=\'api\',\n                    help=\'Output directory. Default: api\' )\nparser.add_argument(\'--include_private\', action=\'store_true\',\n                    help=\'Include private modules. Default: False.\')\nargs = parser.parse_args()\n\nif os.path.exists(args.out_dir):\n    print(""Output directory: {} already exists."".format(args.out_dir),\n          file=sys.stderr)\n    sys.exit(1)\n\nos.mkdir(args.out_dir)\n\n\n##################################################\n# Generate autosummary lists and api\n##################################################\n\nwith open(""api.rst"", ""w"") as api, \\\n     open(""packages.rst"", ""w"") as packages, \\\n     open(""modules.rst"", ""w"") as modules:\n\n    print("".. toctree::\\n   :caption: API Guide\\n   :hidden:\\n"", file=api)\n    # print(""   {}/kaldi"".format(args.out_dir), file=api)\n    print("".. autosummary::\\n   :toctree: {}\\n"".format(args.out_dir),\n          file=packages)\n    print("".. autosummary::\\n   :toctree: {}\\n"".format(args.out_dir),\n          file=modules)\n\n    for _, modname, ispkg in pkgutil.walk_packages(path=kaldi.__path__,\n                                                   prefix=kaldi.__name__+\'.\',\n                                                   onerror=lambda x: None):\n        if modname.split(""."")[-1][0] == ""_"" and not args.include_private:\n            continue\n        if modname == ""kaldi.itf"":\n            continue\n        if ispkg:\n            print(""   {}/{}"".format(args.out_dir, modname), file=api)\n            print(""   {}"".format(modname), file=packages)\n        else:\n            if len(modname.split(""."")) == 2:\n                print(""   {}/{}"".format(args.out_dir, modname), file=api)\n            print(""   {}"".format(modname), file=modules)\n\n##################################################\n# Call autogen\n##################################################\n\ncheck_call([\'sphinx-autogen\', \'-i\', \'-o\', args.out_dir, \'packages.rst\'])\ncheck_call([\'sphinx-autogen\', \'-i\', \'-o\', args.out_dir, \'modules.rst\'])\ncheck_call([\'rm\' , \'-f\', \'packages.rst\', \'modules.rst\'])\n\n##################################################\n# Include submodules in package documentation\n##################################################\n\nfor importer, modname, ispkg in pkgutil.walk_packages(path=kaldi.__path__,\n                                                      prefix=kaldi.__name__+\'.\',\n                                                      onerror=lambda x: None):\n    if modname.split(""."")[-1][0] == ""_"" and not args.include_private:\n        continue\n    if modname == ""kaldi.itf"":\n        continue\n    if not ispkg and len(modname.split(""."")) > 2:\n        mod_file = ""{}.rst"".format(modname)\n        mod_path = os.path.join(args.out_dir, mod_file)\n\n        pkg_file = ""{}.rst"".format(""."".join(modname.split(""."")[:-1]))\n        pkg_path = os.path.join(args.out_dir, pkg_file)\n\n        # Edit submodule headers\n        check_call([\'sed\', \'-i\', \'s/=/-/g\', mod_path])\n\n        # Include submodule in pkg.rst\n        with open(pkg_path, ""a"") as pkg:\n            # pkg.write(""""""\\n.. include:: {}\\n\\n"""""".format(mod_file))\n            pkg.write(""\\n"")\n            pkg.write(open(mod_path).read())\n\n        # Remove mod.rst\n        check_call([\'rm\', \'-f\', mod_path])\n\n##################################################\n# Add autosummary nosignatures option\n##################################################\n\nfor importer, modname, ispkg in pkgutil.walk_packages(path=kaldi.__path__,\n                                                      prefix=kaldi.__name__+\'.\',\n                                                      onerror=lambda x: None):\n    if modname.split(""."")[-1][0] == ""_"" and not args.include_private:\n        continue\n    if modname == ""kaldi.itf"":\n        continue\n    if ispkg:\n        pkg_file = ""{}.rst"".format(modname)\n        pkg_path = os.path.join(args.out_dir, pkg_file)\n\n        check_call([\'sed\', \'-i\',\n                    \'s/autosummary::/autosummary::\\\\n      :nosignatures:/g\',\n                     pkg_path])\n'"
kaldi/__init__.py,0,"b'# This is needed so that extension libs can load each other via Python C API.\nimport os\nimport sys\nroot = os.path.dirname(__file__)\nfor entry in os.listdir(root):\n    path = os.path.join(root, entry)\n    if os.path.isdir(path):\n        sys.path.append(path)\n\n# Make version string available at package level\nfrom .__version__ import __version__\n\n# Configure Kaldi logging\nfrom . import base\n# We do not want Python interpreter to abort on failed Kaldi assertions.\nbase.set_abort_on_assert_failure(False)\n# Stack traces are useful during development. We are disabling them here to make\n# actual error messages easier to see in the interpreter. Users can still enable\n# them by calling set_print_stack_trace_on_error(True) in their own scripts.\nbase.set_print_stack_trace_on_error(False)\n\n# Set default logging handler to avoid ""No handler found"" warnings.\nimport logging\ntry:  # Python 2.7+\n    from logging import NullHandler\nexcept ImportError:\n    class NullHandler(logging.Handler):\n        def emit(self, record):\n            pass\nlogging.getLogger(__name__).addHandler(NullHandler())\n\n# from . import base\n# from . import chain\n# from . import cudamatrix\n# from . import decoder\n# from . import feat\n# from . import fstext\n# from . import gmm\n# from . import hmm\n# from . import itf\n# from . import ivector\n# from . import lat\n# from . import lm\n# from . import matrix\n# from . import nnet3\n# from . import online2\n# from . import sgmm2\n# from . import transform\n# from . import tree\n# from . import util\n\n__all__ = [__version__]\n'"
kaldi/__version__.py,0,"b""__version__ = '0.1.2'\n"""
kaldi/alignment.py,0,"b'from __future__ import division\n\nfrom .base import io as _base_io\nfrom . import decoder as _dec\nfrom . import fstext as _fst\nfrom .fstext import utils as _fst_utils\nfrom .gmm import am as _gmm_am\nfrom . import hmm as _hmm\nfrom .lat import align as _lat_align\nfrom .lat import functions as _lat_funcs\nfrom .matrix import _kaldi_matrix\nfrom . import nnet3 as _nnet3\nfrom . import tree as _tree\nfrom .util import io as _util_io\n\n\n__all__ = [\'Aligner\', \'MappedAligner\', \'GmmAligner\', \'NnetAligner\']\n\n\nclass Aligner(object):\n    """"""Speech aligner.\n\n    This can be used to align transition-id log-likelihood matrices with\n    reference texts.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        tree (ContextDependency): The phonetic decision tree.\n        lexicon (StdFst): The lexicon FST.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        disambig_symbols (List[int]): Disambiguation symbols.\n        graph_compiler_opts (TrainingGraphCompilerOptions): Configuration\n            options for graph compiler.\n        beam (float): Decoding beam used in alignment.\n        transition_scale (float): The scale on non-self-loop transition\n            probabilities.\n        self_loop_scale (float): The scale on self-loop transition\n            probabilities.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, tree, lexicon, symbols=None,\n                 disambig_symbols=None, graph_compiler_opts=None, beam=200.0,\n                 transition_scale=1.0, self_loop_scale=1.0, acoustic_scale=0.1):\n        self.transition_model = transition_model\n        self.symbols = symbols\n        if not graph_compiler_opts:\n            graph_compiler_opts = _dec.TrainingGraphCompilerOptions()\n        self.graph_compiler = _dec.TrainingGraphCompiler(\n            transition_model, tree, lexicon,\n            disambig_symbols, graph_compiler_opts)\n        self.decoder_opts = _dec.FasterDecoderOptions()\n        self.decoder_opts.beam = beam\n        self.transition_scale = transition_scale\n        self.self_loop_scale = self_loop_scale\n        self.acoustic_scale = acoustic_scale\n\n    @staticmethod\n    def read_tree(tree_rxfilename):\n        """"""Reads phonetic decision tree from an extended filename.\n\n        Returns:\n            ContextDependency: Phonetic decision tree.\n        """"""\n        tree = _tree.ContextDependency()\n        with _util_io.xopen(tree_rxfilename) as ki:\n            tree.read(ki.stream(), ki.binary)\n        return tree\n\n    @staticmethod\n    def read_lexicon(lexicon_rxfilename):\n        """"""Reads lexicon FST from an extended filename.\n\n        Returns:\n            StdFst: Lexicon FST.\n        """"""\n        return _fst.read_fst_kaldi(lexicon_rxfilename)\n\n    @staticmethod\n    def read_symbols(symbols_filename):\n        """"""Reads symbol table from file.\n\n        Returns:\n            SymbolTable: Symbol table.\n        """"""\n        if symbols_filename is None:\n            return None\n        else:\n            return _fst.SymbolTable.read_text(symbols_filename)\n\n    @staticmethod\n    def read_disambig_symbols(disambig_rxfilename):\n        """"""Reads disambiguation symbols from an extended filename.\n\n        Returns:\n            List[int]: List of disambiguation symbols.\n        """"""\n        if disambig_rxfilename is None:\n            return None\n        else:\n            with _util_io.xopen(disambig_rxfilename, ""rt"") as ki:\n                return [int(line.strip()) for line in ki]\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads transition model from an extended filename.\n\n        Returns:\n            TransitionModel: Transition model.\n        """"""\n        with _util_io.xopen(model_rxfilename) as ki:\n            return _hmm.TransitionModel().read(ki.stream(), ki.binary)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, tree_rxfilename, lexicon_rxfilename,\n                   symbols_filename=None, disambig_rxfilename=None,\n                   graph_compiler_opts=None, beam=200.0, transition_scale=1.0,\n                   self_loop_scale=1.0, acoustic_scale=0.1):\n        """"""Constructs a new GMM aligner from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the transition\n                model.\n            tree_rxfilename (str): Extended filename for reading the phonetic\n                decision tree.\n            lexicon_rxfilename (str): Extended filename for reading the lexicon\n                FST.\n            symbols_filename (str): The symbols file. If provided, ""text"" input\n                of :meth:`align` should include symbols instead of integer\n                indices.\n            disambig_rxfilename (str): Extended filename for reading the list\n                of disambiguation symbols.\n            graph_compiler_opts (TrainingGraphCompilerOptions): Configuration\n                options for graph compiler.\n            beam (float): Decoding beam used in alignment.\n            transition_scale (float): The scale on non-self-loop transition\n                probabilities.\n            self_loop_scale (float): The scale on self-loop transition\n                probabilities.\n            acoustic_scale (float): Acoustic score scale.\n\n        Returns:\n            A new aligner object.\n        """"""\n        transition_model = cls.read_model(model_rxfilename)\n        tree = cls.read_tree(tree_rxfilename)\n        lexicon = cls.read_lexicon(lexicon_rxfilename)\n        symbols = cls.read_symbols(symbols_filename)\n        disambig_symbols = cls.read_disambig_symbols(disambig_rxfilename)\n        return cls(transition_model, tree, lexicon, symbols,\n                   disambig_symbols, graph_compiler_opts, beam,\n                   transition_scale, self_loop_scale, acoustic_scale)\n\n    def _make_decodable(self, loglikes):\n        """"""Constructs a new decodable object from input log-likelihoods.\n\n        Args:\n            loglikes (object): Input log-likelihoods.\n\n        Returns:\n            DecodableMatrixScaled: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        if loglikes.num_rows == 0:\n            raise ValueError(""Empty loglikes matrix."")\n        return _dec.DecodableMatrixScaled(loglikes, self.acoustic_scale)\n\n    def align(self, input, text):\n        """"""Aligns input with text.\n\n        Output is a dictionary with the following `(key, value)` pairs:\n\n        ================ =========================== ===========================\n        key              value                       value type\n        ================ =========================== ===========================\n        ""alignment""      Frame-level alignment       `List[int]`\n        ""best_path""      Best lattice path           `CompactLattice`\n        ""likelihood""     Log-likelihood of best path `float`\n        ""weight""         Cost of best path           `LatticeWeight`\n        ================ =========================== ===========================\n\n        If :attr:`symbols` is ``None``, the ""text"" input should be a\n        string of space separated integer indices. Otherwise it should be a\n        string of space separated symbols. The ""weight"" output is a lattice\n        weight consisting of (graph-score, acoustic-score).\n\n        Args:\n            input (object): Input to align.\n            text (str): Reference text to align.\n\n        Returns:\n            A dictionary representing alignment output.\n\n        Raises:\n            RuntimeError: If alignment fails.\n        """"""\n        if self.symbols:\n            words = _fst.symbols_to_indices(self.symbols, text.split())\n        else:\n            words = text.split()\n\n        graph = self.graph_compiler.compile_graph_from_text(words)\n        _hmm.add_transition_probs(self.transition_model, [],\n                                  self.transition_scale, self.self_loop_scale,\n                                  graph)\n        decoder = _dec.FasterDecoder(graph, self.decoder_opts)\n        decoder.decode(self._make_decodable(input))\n\n        if not decoder.reached_final():\n            raise RuntimeError(""No final state was active on the last frame."")\n\n        try:\n            best_path = decoder.get_best_path()\n        except RuntimeError:\n            raise RuntimeError(""Empty alignment output."")\n\n        ali, _, weight = _fst_utils.get_linear_symbol_sequence(best_path)\n        likelihood = - (weight.value1 + weight.value2)\n\n        if self.acoustic_scale != 0.0:\n            scale = _fst_utils.acoustic_lattice_scale(1.0 / self.acoustic_scale)\n            _fst_utils.scale_lattice(scale, best_path)\n\n        best_path = _fst_utils.convert_lattice_to_compact_lattice(best_path)\n\n        return {\n            ""alignment"": ali,\n            ""best_path"": best_path,\n            ""likelihood"": likelihood,\n            ""weight"": weight\n        }\n\n    def to_phone_alignment(self, alignment, phones=None):\n        """"""Converts frame-level alignment to phone-level alignment.\n\n        Args:\n            alignment (List[int]): Frame-level alignment.\n            phones (SymbolTable): The phone symbol table. If provided, output\n                includes symbols instead of integer indices.\n\n        Returns:\n            List[Tuple[int,int,int]]: A list of triplets representing, for\n            each phone in the input, the phone index/symbol, the begin time (in\n            frames) and the duration (in frames).\n        """"""\n        success, split_ali = _hmm.split_to_phones(self.transition_model,\n                                                  alignment)\n        if not success:\n            raise RuntimeError(""Alignment phone split failed."")\n        phone_start, phone_alignment = 0, []\n        for entry in split_ali:\n            phone = self.transition_model.transition_id_to_phone(entry[0])\n            if phones:\n                phone = phones.find_symbol(phone)\n            phone_duration = len(entry)\n            phone_alignment.append((phone, phone_start, phone_duration))\n            phone_start += phone_duration\n        return phone_alignment\n\n    def to_word_alignment(self, best_path, word_boundary_info):\n        """"""Converts best alignment path to word-level alignment.\n\n        Args:\n            best_path (CompactLattice): Best alignment path.\n            word_boundary_info (WordBoundaryInfo): Word boundary information.\n\n        Returns:\n            List[Tuple[int,int,int]]: A list of triplets representing, for each\n            word in the input, the word index/symbol, the begin time (in frames)\n            and the duration (in frames). The zero/epsilon words correspond to\n            optional silences.\n        """"""\n        success, best_path = _lat_align.word_align_lattice(\n            best_path, self.transition_model, word_boundary_info, 0)\n        if not success:\n            raise RuntimeError(""Lattice word alignment failed."")\n        word_alignment = _lat_funcs.compact_lattice_to_word_alignment(best_path)\n        if self.symbols:\n            mapper = lambda x: (self.symbols.find_symbol(x[0]), x[1], x[2])\n        else:\n            mapper = lambda x: x\n        return list(map(mapper, zip(*word_alignment)))\n\n\nclass MappedAligner(Aligner):\n    """"""Mapped speech aligner.\n\n    This can be used to align phone-id log-likelihood matrices with reference\n    texts.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        tree (ContextDependency): The phonetic decision tree.\n        lexicon (StdFst): The lexicon FST.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        disambig_symbols (List[int]): Disambiguation symbols.\n        graph_compiler_opts (TrainingGraphCompilerOptions): Configuration\n            options for graph compiler.\n        beam (float): Decoding beam used in alignment.\n        transition_scale (float): The scale on non-self-loop transition\n            probabilities.\n        self_loop_scale (float): The scale on self-loop transition\n            probabilities.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n\n    def _make_decodable(self, loglikes):\n        """"""Constructs a new decodable object from input log-likelihoods.\n\n        Args:\n            loglikes (object): Input log-likelihoods.\n\n        Returns:\n            DecodableMatrixScaledMapped: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        if loglikes.num_rows == 0:\n            raise ValueError(""Empty loglikes matrix."")\n        return _dec.DecodableMatrixScaledMapped(self.transition_model, loglikes,\n                                                self.acoustic_scale)\n\n\nclass GmmAligner(Aligner):\n    """"""GMM based speech aligner.\n\n    This can be used to align feature matrices with reference texts.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmDiagGmm): The acoustic model.\n        tree (ContextDependency): The phonetic decision tree.\n        lexicon (StdFst): The lexicon FST.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" input of\n            :meth:`align` should include symbols instead of integer indices.\n        disambig_symbols (List[int]): Disambiguation symbols.\n        graph_compiler_opts (TrainingGraphCompilerOptions): Configuration\n            options for graph compiler.\n        beam (float): Decoding beam used in alignment.\n        transition_scale (float): The scale on non-self-loop transition\n            probabilities.\n        self_loop_scale (float): The scale on self-loop transition\n            probabilities.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, acoustic_model, tree, lexicon,\n                 symbols=None, disambig_symbols=None, graph_compiler_opts=None,\n                 beam=200.0, transition_scale=1.0, self_loop_scale=1.0,\n                 acoustic_scale=0.1):\n        if not isinstance(acoustic_model, _gmm_am.AmDiagGmm):\n            raise TypeError(""acoustic_model should be a AmDiagGmm object"")\n        self.acoustic_model = acoustic_model\n        super(GmmAligner, self).__init__(transition_model, tree, lexicon,\n                                         symbols, disambig_symbols,\n                                         graph_compiler_opts, beam,\n                                         transition_scale, self_loop_scale,\n                                         acoustic_scale)\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads model from an extended filename.\n\n        Returns:\n            Tuple[TransitionModel, AmDiagGmm]: A (transition model, acoustic\n            model) pair.\n        """"""\n        with _util_io.xopen(model_rxfilename) as ki:\n            transition_model = _hmm.TransitionModel().read(ki.stream(),\n                                                           ki.binary)\n            acoustic_model = _gmm_am.AmDiagGmm().read(ki.stream(), ki.binary)\n        return transition_model, acoustic_model\n\n    @classmethod\n    def from_files(cls, model_rxfilename, tree_rxfilename, lexicon_rxfilename,\n                   symbols_filename=None, disambig_rxfilename=None,\n                   graph_compiler_opts=None, beam=200.0, transition_scale=1.0,\n                   self_loop_scale=1.0, acoustic_scale=0.1):\n        """"""Constructs a new GMM aligner from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            tree_rxfilename (str): Extended filename for reading the phonetic\n                decision tree.\n            lexicon_rxfilename (str): Extended filename for reading the lexicon\n                FST.\n            symbols_filename (str): The symbols file. If provided, ""text"" input\n                of :meth:`align` should include symbols instead of integer\n                indices.\n            disambig_rxfilename (str): Extended filename for reading the list\n                of disambiguation symbols.\n            graph_compiler_opts (TrainingGraphCompilerOptions): Configuration\n                options for graph compiler.\n            beam (float): Decoding beam used in alignment.\n            transition_scale (float): The scale on non-self-loop transition\n                probabilities.\n            self_loop_scale (float): The scale on self-loop transition\n                probabilities.\n            acoustic_scale (float): Acoustic score scale.\n\n        Returns:\n            A new aligner object.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        tree = cls.read_tree(tree_rxfilename)\n        lexicon = cls.read_lexicon(lexicon_rxfilename)\n        symbols = cls.read_symbols(symbols_filename)\n        disambig_symbols = cls.read_disambig_symbols(disambig_rxfilename)\n        return cls(transition_model, acoustic_model, tree, lexicon, symbols,\n                   disambig_symbols, graph_compiler_opts, beam,\n                   transition_scale, self_loop_scale, acoustic_scale)\n\n    def _make_decodable(self, features):\n        """"""Constructs a new decodable object from input features.\n\n        Args:\n            features (object): Input features.\n\n        Returns:\n            DecodableAmDiagGmmScaled: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        if features.num_rows == 0:\n            raise ValueError(""Empty feature matrix."")\n        return _gmm_am.DecodableAmDiagGmmScaled(self.acoustic_model,\n                                                self.transition_model,\n                                                features, self.acoustic_scale)\n\n\nclass NnetAligner(Aligner):\n    """"""Neural network based speech aligner.\n\n    This can be used to align feature matrices with reference texts.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        tree (ContextDependency): The phonetic decision tree.\n        lexicon (StdFst): The lexicon FST.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" input of\n            :meth:`align` should include symbols instead of integer indices.\n        disambig_symbols (List[int]): Disambiguation symbols.\n        graph_compiler_opts (TrainingGraphCompilerOptions): Configuration\n            options for graph compiler.\n        beam (float): Decoding beam used in alignment.\n        transition_scale (float): The scale on non-self-loop transition\n            probabilities.\n        self_loop_scale (float): The scale on self-loop transition\n            probabilities.\n        decodable_opts (NnetSimpleComputationOptions): Configuration options for\n            simple nnet3 am decodable objects.\n        online_ivector_period (int): Onlne ivector period. Relevant only if\n            online ivectors are used.\n    """"""\n    def __init__(self, transition_model, acoustic_model, tree, lexicon,\n                 symbols=None, disambig_symbols=None, graph_compiler_opts=None,\n                 beam=200.0, transition_scale=1.0, self_loop_scale=1.0,\n                 decodable_opts=None, online_ivector_period=10):\n        if not isinstance(acoustic_model, _nnet3.AmNnetSimple):\n            raise TypeError(""acoustic_model should be a AmNnetSimple object"")\n        self.acoustic_model = acoustic_model\n        nnet = self.acoustic_model.get_nnet()\n        _nnet3.set_batchnorm_test_mode(True, nnet)\n        _nnet3.set_dropout_test_mode(True, nnet)\n        _nnet3.collapse_model(_nnet3.CollapseModelConfig(), nnet)\n        if decodable_opts:\n            if not isinstance(decodable_opts,\n                              _nnet3.NnetSimpleComputationOptions):\n                raise TypeError(""decodable_opts should be either None or a ""\n                                ""NnetSimpleComputationOptions object"")\n            self.decodable_opts = decodable_opts\n        else:\n            self.decodable_opts = _nnet3.NnetSimpleComputationOptions()\n        self.compiler = _nnet3.CachingOptimizingCompiler.new_with_optimize_opts(\n            nnet, self.decodable_opts.optimize_config)\n        self.online_ivector_period = online_ivector_period\n        super(NnetAligner, self).__init__(transition_model, tree, lexicon,\n                                          symbols, disambig_symbols,\n                                          graph_compiler_opts, beam,\n                                          transition_scale, self_loop_scale,\n                                          self.decodable_opts.acoustic_scale)\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads model from an extended filename.\n\n        Returns:\n            Tuple[TransitionModel, AmNnetSimple]: A (transition model, acoustic\n            model) pair.\n        """"""\n        with _util_io.xopen(model_rxfilename) as ki:\n            transition_model = _hmm.TransitionModel().read(ki.stream(),\n                                                           ki.binary)\n            acoustic_model = _nnet3.AmNnetSimple().read(ki.stream(), ki.binary)\n        return transition_model, acoustic_model\n\n    @classmethod\n    def from_files(cls, model_rxfilename, tree_rxfilename, lexicon_rxfilename,\n                   symbols_filename=None, disambig_rxfilename=None,\n                   graph_compiler_opts=None, beam=200.0, transition_scale=1.0,\n                   self_loop_scale=1.0, decodable_opts=None,\n                   online_ivector_period=10):\n        """"""Constructs a new nnet3 aligner from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            tree_rxfilename (str): Extended filename for reading the phonetic\n                decision tree.\n            lexicon_rxfilename (str): Extended filename for reading the lexicon\n                FST.\n            symbols_filename (str): The symbols file. If provided, ""text"" input\n                of :meth:`align` should include symbols instead of integer\n                indices.\n            disambig_rxfilename (str): Extended filename for reading the list\n                of disambiguation symbols.\n            graph_compiler_opts (TrainingGraphCompilerOptions): Configuration\n                options for graph compiler.\n            beam (float): Decoding beam used in alignment.\n            transition_scale (float): The scale on non-self-loop transition\n                probabilities.\n            self_loop_scale (float): The scale on self-loop transition\n                probabilities.\n            decodable_opts (NnetSimpleComputationOptions): Configuration options\n                for simple nnet3 am decodable objects.\n            online_ivector_period (int): Onlne ivector period. Relevant only if\n                online ivectors are used.\n\n        Returns:\n            A new aligner object.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        tree = cls.read_tree(tree_rxfilename)\n        lexicon = cls.read_lexicon(lexicon_rxfilename)\n        disambig_symbols = cls.read_disambig_symbols(disambig_rxfilename)\n        symbols = cls.read_symbols(symbols_filename)\n        return cls(transition_model, acoustic_model, tree, lexicon, symbols,\n                   disambig_symbols, graph_compiler_opts, beam,\n                   transition_scale, self_loop_scale, decodable_opts,\n                   online_ivector_period)\n\n    def _make_decodable(self, features):\n        """"""Constructs a new decodable object from input features.\n\n        Input can be just a feature matrix or a tuple of a feature matrix and\n        an ivector or a tuple of a feature matrix and an online ivector matrix.\n\n        Args:\n            features (Matrix or Tuple[Matrix, Vector] or Tuple[Matrix, Matrix]):\n                Input features.\n\n        Returns:\n            DecodableAmNnetSimple: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        ivector, online_ivectors = None, None\n        if isinstance(features, tuple):\n            features, ivector_features = features\n            if isinstance(ivector_features, _kaldi_matrix.MatrixBase):\n                online_ivectors = ivector_features\n            else:\n                ivector = ivector_features\n        if features.num_rows == 0:\n            raise ValueError(""Empty feature matrix."")\n        return _nnet3.DecodableAmNnetSimple(\n            self.decodable_opts, self.transition_model, self.acoustic_model,\n            features, ivector, online_ivectors, self.online_ivector_period,\n            self.compiler)\n'"
kaldi/asr.py,0,"b'""""""\nThis module provides a number of speech recognizers with an easy to use API.\n\nNote that in Kaldi, therefore in PyKaldi, there is no single ""canonical""\ndecoder, or a fixed interface that decoders must satisfy. Same is true for the\nmodels. The decoders and models provided by Kaldi/PyKaldi can be mixed and\nmatched to construct specialized speech recognizers. The speech recognizers in\nthis module cover only the most ""typical"" combinations.\n""""""\n\nfrom __future__ import division\n\nfrom . import cudamatrix as _cumatrix\nfrom . import decoder as _dec\nfrom . import fstext as _fst\nfrom .fstext import enums as _fst_enums\nfrom .fstext import _fst as _fst_fst\nfrom .fstext import properties as _fst_props\nfrom .fstext import special as _fst_spec\nfrom .fstext import utils as _fst_utils\nfrom .gmm import am as _gmm_am\nfrom . import hmm as _hmm\nfrom .lat import functions as _lat_funcs\nfrom . import lm as _lm\nfrom .matrix import _kaldi_matrix\nfrom . import rnnlm as _rnnlm\nfrom . import nnet3 as _nnet3\nfrom . import online2 as _online2\nfrom .util import io as _util_io\n\n\n__all__ = [\'Recognizer\',\n           \'FasterRecognizer\',\n           \'LatticeFasterRecognizer\',\n           \'LatticeBiglmFasterRecognizer\',\n           \'MappedRecognizer\',\n           \'MappedFasterRecognizer\',\n           \'MappedLatticeFasterRecognizer\',\n           \'MappedLatticeBiglmFasterRecognizer\',\n           \'GmmRecognizer\',\n           \'GmmFasterRecognizer\',\n           \'GmmLatticeFasterRecognizer\',\n           \'GmmLatticeBiglmFasterRecognizer\',\n           \'NnetRecognizer\',\n           \'NnetFasterRecognizer\',\n           \'NnetLatticeFasterRecognizer\',\n           \'NnetLatticeFasterBatchRecognizer\',\n           \'NnetLatticeFasterGrammarRecognizer\',\n           \'NnetLatticeBiglmFasterRecognizer\',\n           \'OnlineRecognizer\',\n           \'NnetOnlineRecognizer\',\n           \'NnetLatticeFasterOnlineRecognizer\',\n           \'NnetLatticeFasterOnlineGrammarRecognizer\',\n           \'LatticeLmRescorer\']\n\n\nclass Recognizer(object):\n    """"""Base class for speech recognizers.\n\n    Args:\n        decoder (object): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, decoder, symbols=None, allow_partial=True,\n                 acoustic_scale=0.1):\n        self.decoder = decoder\n        self.symbols = symbols\n        self.allow_partial = allow_partial\n        self.acoustic_scale = acoustic_scale\n\n    def _make_decodable(self, loglikes):\n        """"""Constructs a new decodable object from input log-likelihoods.\n\n        Args:\n            loglikes (object): Input log-likelihoods.\n\n        Returns:\n            DecodableMatrixScaled: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        if loglikes.num_rows == 0:\n            raise ValueError(""Empty loglikes matrix."")\n        return _dec.DecodableMatrixScaled(loglikes, self.acoustic_scale)\n\n    def _determinize_lattice(self, lattice):\n        """"""Determinizes raw state-level lattice.\n\n        Args:\n            lattice (Lattice): Raw state-level lattice.\n\n        Returns:\n            CompactLattice or Lattice: A deterministic compact lattice if the\n            decoder is configured to determinize lattices. Otherwise, a raw\n            state-level lattice.\n        """"""\n        opts = self.decoder.get_options()\n        if opts.determinize_lattice:\n            det_opts = _lat_funcs.DeterminizeLatticePrunedOptions()\n            det_opts.max_mem = opts.det_opts.max_mem\n            return _lat_funcs.determinize_lattice_pruned(\n                lattice, opts.lattice_beam, det_opts, True)\n        else:\n            return lattice\n\n    def decode(self, input):\n        """"""Decodes input.\n\n        Output is a dictionary with the following `(key, value)` pairs:\n\n        ============ =========================== ==============================\n        key          value                       value type\n        ============ =========================== ==============================\n        ""alignment""  Frame-level alignment       `List[int]`\n        ""best_path""  Best lattice path           `CompactLattice`\n        ""lattice""    Output lattice              `Lattice` or `CompactLattice`\n        ""likelihood"" Log-likelihood of best path `float`\n        ""text""       Output transcript           `str`\n        ""weight""     Cost of best path           `LatticeWeight`\n        ""words""      Words on best path          `List[int]`\n        ============ =========================== ==============================\n\n        The ""lattice"" output is produced only if the decoder can generate\n        lattices. It will be a deterministic compact lattice if the decoder is\n        configured to determinize lattices. Otherwise, it will be a raw\n        state-level lattice.\n\n        If :attr:`symbols` is ``None``, the ""text"" output will be a string of\n        space separated integer indices. Otherwise it will be a string of space\n        separated symbols. The ""weight"" output is a lattice weight consisting of\n        (graph-score, acoustic-score).\n\n        Args:\n            input (object): Input to decode.\n\n        Returns:\n            A dictionary representing decoding output.\n\n        Raises:\n            RuntimeError: If decoding fails.\n        """"""\n        self.decoder.decode(self._make_decodable(input))\n\n        if not (self.allow_partial or self.decoder.reached_final()):\n            raise RuntimeError(""No final state was active on the last frame."")\n\n        try:\n            best_path = self.decoder.get_best_path()\n        except RuntimeError:\n            raise RuntimeError(""Empty decoding output."")\n\n        ali, words, weight = _fst_utils.get_linear_symbol_sequence(best_path)\n\n        if self.symbols:\n            text = "" "".join(_fst.indices_to_symbols(self.symbols, words))\n        else:\n            text = "" "".join(map(str, words))\n\n        likelihood = - (weight.value1 + weight.value2)\n\n        if self.acoustic_scale != 0.0:\n            scale = _fst_utils.acoustic_lattice_scale(1.0 / self.acoustic_scale)\n            _fst_utils.scale_lattice(scale, best_path)\n        best_path = _fst_utils.convert_lattice_to_compact_lattice(best_path)\n\n        try:\n            lat = self.decoder.get_raw_lattice()\n        except AttributeError:\n            return {\n                ""alignment"": ali,\n                ""best_path"": best_path,\n                ""likelihood"": likelihood,\n                ""text"": text,\n                ""weight"": weight,\n                ""words"": words,\n            }\n        if lat.num_states() == 0:\n            raise RuntimeError(""Empty output lattice."")\n        lat.connect()\n\n        lat = self._determinize_lattice(lat)\n\n        if self.acoustic_scale != 0.0:\n            if isinstance(lat, _fst.CompactLatticeVectorFst):\n                _fst_utils.scale_compact_lattice(scale, lat)\n            else:\n                _fst_utils.scale_lattice(scale, lat)\n\n        return {\n            ""alignment"": ali,\n            ""best_path"": best_path,\n            ""lattice"": lat,\n            ""likelihood"": likelihood,\n            ""text"": text,\n            ""weight"": weight,\n            ""words"": words,\n        }\n\n\nclass FasterRecognizer(Recognizer):\n    """"""Faster speech recognizer.\n\n    This recognizer can be used to decode log-likelihood matrices. Non-zero\n    labels on the decoding graph, e.g. transition-ids, are looked up in the\n    log-likelihood matrices using 1-based indexing -- index 0 is reserved for\n    epsilon symbols in OpenFst.\n\n    Args:\n        decoder (FasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.FasterDecoder):\n            raise TypeError(""decoder should be a FasterDecoder"")\n        super(FasterRecognizer, self).__init__(decoder, symbols,\n                                               allow_partial, acoustic_scale)\n\n    @classmethod\n    def from_files(cls, graph_rxfilename, symbols_filename=None,\n                   allow_partial=True, acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (FasterDecoderOptions): Configuration options for the\n                decoder.\n\n        Returns:\n            FasterRecognizer: A new recognizer.\n        """"""\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.FasterDecoderOptions()\n        decoder = _dec.FasterDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(decoder, symbols, allow_partial, acoustic_scale)\n\n\nclass LatticeFasterRecognizer(Recognizer):\n    """"""Lattice-generating faster speech recognizer.\n\n    This recognizer can be used to decode log-likelihood matrices into lattices.\n    Non-zero labels on the decoding graph, e.g. transition-ids, are looked up in\n    the log-likelihood matrices using 1-based indexing -- index 0 is reserved\n    for epsilon symbols in OpenFst.\n\n    Args:\n        decoder (LatticeFasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.LatticeFasterDecoder):\n            raise TypeError(""decoder should be a LatticeFasterDecoder"")\n        super(LatticeFasterRecognizer, self).__init__(\n            decoder, symbols, allow_partial, acoustic_scale)\n\n    @classmethod\n    def from_files(cls, graph_rxfilename, symbols_filename=None,\n                   allow_partial=True, acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration options\n                for the decoder.\n\n        Returns:\n            LatticeFasterRecognizer: A new recognizer.\n        """"""\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeFasterDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(decoder, symbols, allow_partial, acoustic_scale)\n\n\nclass LatticeBiglmFasterRecognizer(Recognizer):\n    """"""Lattice generating big-LM faster speech recognizer.\n\n    This recognizer can be used to decode log-likelihood matrices into lattices.\n    Non-zero labels on the decoding graph, e.g. transition-ids, are looked up in\n    the log-likelihood matrices using 1-based indexing -- index 0 is reserved\n    for epsilon symbols in OpenFst.\n\n    Args:\n        decoder (LatticeBiglmFasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, decoder, symbols=None, allow_partial=True,\n                 acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.LatticeBiglmFasterDecoder):\n            raise TypeError(""decoder should be a LatticeBiglmFasterDecoder"")\n        super(LatticeBiglmFasterRecognizer, self).__init__(\n            decoder, symbols, allow_partial, acoustic_scale)\n\n    @classmethod\n    def from_files(cls, graph_rxfilename, old_lm_rxfilename, new_lm_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            graph_rxfilename (str): Extended filename for reading the graph.\n            old_lm_rxfilename (str): Extended filename for reading the old LM.\n            new_lm_rxfilename (str): Extended filename for reading the new LM.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration\n                options for the decoder.\n\n        Returns:\n            LatticeBiglmFasterRecognizer: A new recognizer.\n        """"""\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        self.old_lm = _fst.read_fst_kaldi(old_lm_rxfilename)\n        _fst_utils.apply_probability_scale(-1.0, self.old_lm)\n        self.new_lm = _fst.read_fst_kaldi(new_lm_rxfilename)\n        self._old_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(self.old_lm)\n        self._new_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(self.new_lm)\n        self._compose_lm = _fst_spec.StdComposeDeterministicOnDemandFst(\n            self._old_lm, self._new_lm)\n        self._cache_compose_lm = _fst_spec.StdCacheDeterministicOnDemandFst(\n            self._compose_lm)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeBiglmFasterDecoder(graph, decoder_opts,\n                                                 self._cache_compose_lm)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(decoder, symbols, allow_partial, acoustic_scale)\n\n\nclass MappedRecognizer(Recognizer):\n    """"""Base class for mapped speech recognizers.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        decoder (object): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        self.transition_model = transition_model\n        self.decoder = decoder\n        self.symbols = symbols\n        self.allow_partial = allow_partial\n        self.acoustic_scale = acoustic_scale\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads transition model from an extended filename.""""""\n        with _util_io.xopen(model_rxfilename) as ki:\n            return _hmm.TransitionModel().read(ki.stream(), ki.binary)\n\n    def _make_decodable(self, loglikes):\n        """"""Constructs a new decodable object from input log-likelihoods.\n\n        Args:\n            loglikes (object): Input log-likelihoods.\n\n        Returns:\n            DecodableMatrixScaledMapped: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        if loglikes.num_rows == 0:\n            raise ValueError(""Empty loglikes matrix."")\n        return _dec.DecodableMatrixScaledMapped(self.transition_model, loglikes,\n                                                self.acoustic_scale)\n\n    def _determinize_lattice(self, lattice):\n        """"""Determinizes raw state-level lattice.\n\n        Args:\n            lattice (Lattice): Raw state-level lattice.\n\n        Returns:\n            CompactLattice or Lattice: A deterministic compact lattice if the\n            decoder is configured to determinize lattices. Otherwise, a raw\n            state-level lattice.\n        """"""\n        opts = self.decoder.get_options()\n        if opts.determinize_lattice:\n            return _lat_funcs.determinize_lattice_phone_pruned(\n                lattice, self.transition_model, opts.lattice_beam,\n                opts.det_opts, True)\n        else:\n            return lattice\n\n\nclass MappedFasterRecognizer(MappedRecognizer):\n    """"""Mapped faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        decoder (FasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.FasterDecoder):\n            raise TypeError(""decoder should be a FasterDecoder"")\n        super(MappedFasterRecognizer, self).__init__(\n            transition_model, decoder, symbols, allow_partial, acoustic_scale)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the transition\n                model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (FasterDecoderOptions): Configuration options for the\n                decoder.\n\n        Returns:\n            MappedFasterRecognizer: A new recognizer object.\n        """"""\n        transition_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.FasterDecoderOptions()\n        decoder = _dec.FasterDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, decoder, symbols,\n                   allow_partial, acoustic_scale)\n\n\nclass MappedLatticeFasterRecognizer(MappedRecognizer):\n    """"""Mapped lattice generating faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        decoder (LatticeFasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.LatticeFasterDecoder):\n            raise TypeError(""decoder should be a LatticeFasterDecoder"")\n        super(MappedLatticeFasterRecognizer, self).__init__(\n            transition_model, decoder, symbols, allow_partial, acoustic_scale)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the transition\n                model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration options\n                for the decoder.\n\n        Returns:\n            MappedFasterRecognizer: A new recognizer object.\n        """"""\n        transition_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeFasterDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, decoder, symbols,\n                   allow_partial, acoustic_scale)\n\n\nclass MappedLatticeBiglmFasterRecognizer(MappedRecognizer):\n    """"""GMM based lattice generating big-LM faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        decoder (LatticeBiglmFasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.LatticeBiglmFasterDecoder):\n            raise TypeError(""decoder should be a LatticeBiglmFasterDecoder"")\n        super(MappedLatticeBiglmFasterRecognizer, self).__init__(\n            transition_model, decoder, symbols, allow_partial, acoustic_scale)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename, old_lm_rxfilename,\n                   new_lm_rxfilename, symbols_filename=None, allow_partial=True,\n                   acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the transition\n                model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            old_lm_rxfilename (str): Extended filename for reading the old LM.\n            new_lm_rxfilename (str): Extended filename for reading the new LM.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration\n                options for the decoder.\n\n        Returns:\n            MappedLatticeBiglmFasterRecognizer: A new recognizer.\n        """"""\n        transition_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        self.old_lm = _fst.read_fst_kaldi(old_lm_rxfilename)\n        _fst_utils.apply_probability_scale(-1.0, self.old_lm)\n        self.new_lm = _fst.read_fst_kaldi(new_lm_rxfilename)\n        self._old_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(self.old_lm)\n        self._new_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(self.new_lm)\n        self._compose_lm = _fst_spec.StdComposeDeterministicOnDemandFst(\n            self._old_lm, self._new_lm)\n        self._cache_compose_lm = _fst_spec.StdCacheDeterministicOnDemandFst(\n            self._compose_lm)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeBiglmFasterDecoder(graph, decoder_opts,\n                                                 self._cache_compose_lm)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, decoder, symbols,\n                   allow_partial, acoustic_scale)\n\n\nclass GmmRecognizer(Recognizer):\n    """"""Base class for GMM based speech recognizers.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmDiagGmm): The acoustic model.\n        decoder (object): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(acoustic_model, _gmm_am.AmDiagGmm):\n            raise TypeError(""acoustic_model argument should be a diagonal GMM"")\n        self.transition_model = transition_model\n        self.acoustic_model = acoustic_model\n        super(GmmRecognizer, self).__init__(decoder, symbols,\n                                            allow_partial, acoustic_scale)\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads model from an extended filename.""""""\n        with _util_io.xopen(model_rxfilename) as ki:\n            transition_model = _hmm.TransitionModel().read(ki.stream(),\n                                                           ki.binary)\n            acoustic_model = _gmm_am.AmDiagGmm().read(ki.stream(), ki.binary)\n        return transition_model, acoustic_model\n\n    def _make_decodable(self, features):\n        """"""Constructs a new decodable object from input features.\n\n        Args:\n            features (object): Input features.\n\n        Returns:\n            DecodableAmDiagGmmScaled: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        if features.num_rows == 0:\n            raise ValueError(""Empty feature matrix."")\n        return _gmm_am.DecodableAmDiagGmmScaled(self.acoustic_model,\n                                                self.transition_model,\n                                                features, self.acoustic_scale)\n\n    def _determinize_lattice(self, lattice):\n        """"""Determinizes raw state-level lattice.\n\n        Args:\n            lattice (Lattice): Raw state-level lattice.\n\n        Returns:\n            CompactLattice or Lattice: A deterministic compact lattice if the\n            decoder is configured to determinize lattices. Otherwise, a raw\n            state-level lattice.\n        """"""\n        opts = self.decoder.get_options()\n        if opts.determinize_lattice:\n            return _lat_funcs.determinize_lattice_phone_pruned(\n                lattice, self.transition_model, opts.lattice_beam,\n                opts.det_opts, True)\n        else:\n            return lattice\n\n\nclass GmmFasterRecognizer(GmmRecognizer):\n    """"""GMM based faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmDiagGmm): The acoustic model.\n        decoder (FasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.FasterDecoder):\n            raise TypeError(""decoder should be a FasterDecoder"")\n        super(GmmFasterRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols,\n            allow_partial, acoustic_scale)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new GMM recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (FasterDecoderOptions): Configuration options for the\n                decoder.\n\n        Returns:\n            A new GMM recognizer object.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.FasterDecoderOptions()\n        decoder = _dec.FasterDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, acoustic_scale)\n\n\nclass GmmLatticeFasterRecognizer(GmmRecognizer):\n    """"""GMM based lattice generating faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmDiagGmm): The acoustic model.\n        decoder (LatticeFasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.LatticeFasterDecoder):\n            raise TypeError(""decoder should be a LatticeFasterDecoder"")\n        super(GmmLatticeFasterRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols,\n            allow_partial, acoustic_scale)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new GMM recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration options\n                for the decoder.\n\n        Returns:\n            A new GMM recognizer object.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeFasterDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, acoustic_scale)\n\n\nclass GmmLatticeBiglmFasterRecognizer(GmmRecognizer):\n    """"""GMM based lattice generating big-LM faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmDiagGmm): The acoustic model.\n        decoder (LatticeBiglmFasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder, symbols=None,\n                 allow_partial=True, acoustic_scale=0.1):\n        if not isinstance(decoder, _dec.LatticeBiglmFasterDecoder):\n            raise TypeError(""decoder should be a LatticeBiglmFasterDecoder"")\n        super(GmmLatticeBiglmFasterRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols, allow_partial,\n            acoustic_scale)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename, old_lm_rxfilename,\n                   new_lm_rxfilename, symbols_filename=None, allow_partial=True,\n                   acoustic_scale=0.1, decoder_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            old_lm_rxfilename (str): Extended filename for reading the old LM.\n            new_lm_rxfilename (str): Extended filename for reading the new LM.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            acoustic_scale (float): Acoustic score scale.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration\n                options for the decoder.\n\n        Returns:\n            GmmLatticeBiglmFasterRecognizer: A new recognizer.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        self.old_lm = _fst.read_fst_kaldi(old_lm_rxfilename)\n        _fst_utils.apply_probability_scale(-1.0, self.old_lm)\n        self.new_lm = _fst.read_fst_kaldi(new_lm_rxfilename)\n        self._old_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(self.old_lm)\n        self._new_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(self.new_lm)\n        self._compose_lm = _fst_spec.StdComposeDeterministicOnDemandFst(\n            self._old_lm, self._new_lm)\n        self._cache_compose_lm = _fst_spec.StdCacheDeterministicOnDemandFst(\n            self._compose_lm)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeBiglmFasterDecoder(graph, decoder_opts,\n                                                 self._cache_compose_lm)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, acoustic_scale)\n\n\nclass NnetRecognizer(Recognizer):\n    """"""Base class for neural network based speech recognizers.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        decoder (object): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decodable_opts (NnetSimpleComputationOptions): Configuration options for\n            simple nnet3 am decodable objects.\n        online_ivector_period (int): Onlne ivector period. Relevant only if\n            online ivectors are used.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder,\n                 symbols=None, allow_partial=True, decodable_opts=None,\n                 online_ivector_period=10):\n        if not isinstance(acoustic_model, _nnet3.AmNnetSimple):\n            raise TypeError(""acoustic_model should be a AmNnetSimple object"")\n        self.transition_model = transition_model\n        self.acoustic_model = acoustic_model\n        nnet = self.acoustic_model.get_nnet()\n        _nnet3.set_batchnorm_test_mode(True, nnet)\n        _nnet3.set_dropout_test_mode(True, nnet)\n        _nnet3.collapse_model(_nnet3.CollapseModelConfig(), nnet)\n        if decodable_opts:\n            if not isinstance(decodable_opts,\n                              _nnet3.NnetSimpleComputationOptions):\n                raise TypeError(""decodable_opts should be either None or a ""\n                                ""NnetSimpleComputationOptions object"")\n            self.decodable_opts = decodable_opts\n        else:\n            self.decodable_opts = _nnet3.NnetSimpleComputationOptions()\n        self.compiler = _nnet3.CachingOptimizingCompiler.new_with_optimize_opts(\n            nnet, self.decodable_opts.optimize_config)\n        self.online_ivector_period = online_ivector_period\n        super(NnetRecognizer, self).__init__(decoder, symbols, allow_partial,\n                                             self.decodable_opts.acoustic_scale)\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads model from an extended filename.""""""\n        with _util_io.xopen(model_rxfilename) as ki:\n            transition_model = _hmm.TransitionModel().read(ki.stream(),\n                                                           ki.binary)\n            acoustic_model = _nnet3.AmNnetSimple().read(ki.stream(), ki.binary)\n        return transition_model, acoustic_model\n\n    def _make_decodable(self, features):\n        """"""Constructs a new decodable object from input features.\n\n        Input can be just a feature matrix or a tuple of a feature matrix and\n        an ivector or a tuple of a feature matrix and an online ivector matrix.\n\n        Args:\n            features (Matrix or Tuple[Matrix, Vector] or Tuple[Matrix, Matrix]):\n                Input features.\n\n        Returns:\n            DecodableAmNnetSimple: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        ivector, online_ivectors = None, None\n        if isinstance(features, tuple):\n            features, ivector_features = features\n            if isinstance(ivector_features, _kaldi_matrix.MatrixBase):\n                online_ivectors = ivector_features\n            else:\n                ivector = ivector_features\n        if features.num_rows == 0:\n            raise ValueError(""Empty feature matrix."")\n        return _nnet3.DecodableAmNnetSimple(\n            self.decodable_opts, self.transition_model, self.acoustic_model,\n            features, ivector, online_ivectors, self.online_ivector_period,\n            self.compiler)\n\n    def _determinize_lattice(self, lattice):\n        """"""Determinizes raw state-level lattice.\n\n        Args:\n            lattice (Lattice): Raw state-level lattice.\n\n        Returns:\n            CompactLattice or Lattice: A deterministic compact lattice if the\n            decoder is configured to determinize lattices. Otherwise, a raw\n            state-level lattice.\n        """"""\n        opts = self.decoder.get_options()\n        if opts.determinize_lattice:\n            return _lat_funcs.determinize_lattice_phone_pruned(\n                lattice, self.transition_model, opts.lattice_beam,\n                opts.det_opts, True)\n        else:\n            return lattice\n\n\nclass NnetFasterRecognizer(NnetRecognizer):\n    """"""Neural network based faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        decoder (FasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decodable_opts (NnetSimpleComputationOptions): Configuration options for\n            simple nnet3 am decodable objects.\n        online_ivector_period (int): Onlne ivector period. Relevant only if\n            online ivectors are used.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder,\n                 symbols=None, allow_partial=True, decodable_opts=None,\n                 online_ivector_period=10):\n        if not isinstance(decoder, _dec.FasterDecoder):\n            raise TypeError(""decoder argument should be a FasterDecoder"")\n        super(NnetFasterRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols, allow_partial,\n            decodable_opts, online_ivector_period)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   decoder_opts=None, decodable_opts=None,\n                   online_ivector_period=10):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            decoder_opts (FasterDecoderOptions): Configuration options for the\n                decoder.\n            decodable_opts (NnetSimpleComputationOptions): Configuration options\n                for simple nnet3 am decodable objects.\n            online_ivector_period (int): Onlne ivector period. Relevant only if\n                online ivectors are used.\n\n        Returns:\n            NnetFasterRecognizer: A new recognizer.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.FasterDecoderOptions()\n        decoder = _dec.FasterDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, decodable_opts, online_ivector_period)\n\n\nclass NnetLatticeFasterRecognizer(NnetRecognizer):\n    """"""Neural network based lattice generating faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        decoder (LatticeFasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decodable_opts (NnetSimpleComputationOptions): Configuration options for\n            simple nnet3 am decodable objects.\n        online_ivector_period (int): Onlne ivector period. Relevant only if\n            online ivectors are used.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder,\n                 symbols=None, allow_partial=True, decodable_opts=None,\n                 online_ivector_period=10):\n        if not isinstance(decoder, _dec.LatticeFasterDecoder):\n            raise TypeError(""decoder argument should be a LatticeFasterDecoder"")\n        super(NnetLatticeFasterRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols, allow_partial,\n            decodable_opts, online_ivector_period)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   decoder_opts=None, decodable_opts=None,\n                   online_ivector_period=10):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration options\n                for the decoder.\n            decodable_opts (NnetSimpleComputationOptions): Configuration options\n                for simple nnet3 am decodable objects.\n            online_ivector_period (int): Onlne ivector period. Relevant only if\n                online ivectors are used.\n\n        Returns:\n            NnetLatticeFasterRecognizer: A new recognizer.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeFasterDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, decodable_opts, online_ivector_period)\n\n\nclass NnetLatticeFasterBatchRecognizer(object):\n    """"""Neural network based lattice generating faster batch speech recognizer.\n\n    This uses multiple CPU threads for the graph search, plus a GPU thread for\n    the neural net inference. The interface of this object should be accessed\n    from only one thread, presumably the main thread of the program.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        graph (StdFst): The decoding graph.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decoder_opts (LatticeFasterDecoderOptions): Configuration options\n            for the decoder.\n        compute_opts (NnetBatchComputerOptions): Configuration options\n            for neural network batch computer.\n        num_threads (int): Number of processing threads.\n        online_ivector_period (int): Onlne ivector period. Relevant only if\n            online ivectors are used.\n    """"""\n    def __init__(self, transition_model, acoustic_model, graph, symbols=None,\n                 allow_partial=True, decoder_opts=None, compute_opts=None,\n                 num_threads=1, online_ivector_period=10):\n        self.transition_model = transition_model\n        self.acoustic_model = acoustic_model\n        nnet = self.acoustic_model.get_nnet()\n        _nnet3.set_batchnorm_test_mode(True, nnet)\n        _nnet3.set_dropout_test_mode(True, nnet)\n        _nnet3.collapse_model(_nnet3.CollapseModelConfig(), nnet)\n        self.graph = graph\n        self.symbols = symbols\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        if not compute_opts:\n            compute_opts = _nnet3.NnetBatchComputerOptions()\n        self.computer = _nnet3.NnetBatchComputer(compute_opts, nnet,\n                                                 self.acoustic_model.priors())\n        self.decoder = _nnet3.NnetBatchDecoder(\n            self.graph, decoder_opts, self.transition_model, self.symbols,\n            allow_partial, num_threads, self.computer)\n        if decoder_opts.determinize_lattice:\n            self._get_output = self.decoder.get_output\n        else:\n            self._get_output = self.decoder.get_raw_output\n        self.online_ivector_period = online_ivector_period\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads model from an extended filename.""""""\n        with _util_io.xopen(model_rxfilename) as ki:\n            transition_model = _hmm.TransitionModel().read(ki.stream(),\n                                                           ki.binary)\n            acoustic_model = _nnet3.AmNnetSimple().read(ki.stream(), ki.binary)\n        return transition_model, acoustic_model\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True, decoder_opts=None,\n                   compute_opts=None, num_threads=1, online_ivector_period=10):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration options\n                for the decoder.\n            compute_opts (NnetBatchComputerOptions): Configuration options\n                for neural network batch computer.\n            num_threads (int): Number of processing threads.\n            online_ivector_period (int): Onlne ivector period. Relevant only if\n                online ivectors are used.\n\n        Returns:\n            NnetLatticeFasterBatchRecognizer: A new recognizer.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, graph, symbols,\n                   allow_partial, decoder_opts, compute_opts, num_threads,\n                   online_ivector_period)\n\n    def accept_input(self, key, input):\n        """"""Accepts input for decoding.\n\n        This should be called for each utterance that is to be decoded\n        (interspersed with calls to :meth:`get_output`). This call will block\n        when no threads are ready to start processing this utterance.\n\n        Input can be just a feature matrix or a tuple of a feature matrix and\n        an ivector or a tuple of a feature matrix and an online ivector matrix.\n\n        Args:\n            key (str): Utterance ID. This ID will be used to identify the\n                utterance when returned by :meth:`get_output`.\n            input (Matrix or Tuple[Matrix, Vector] or Tuple[Matrix, Matrix]):\n                Input to decode.\n\n        Raises:\n            RuntimeError: If decoding fails.\n        """"""\n        ivector, online_ivectors = None, None\n        if isinstance(input, tuple):\n            features, ivector_features = input\n            if isinstance(ivector_features, _kaldi_matrix.MatrixBase):\n                online_ivectors = ivector_features\n            else:\n                ivector = ivector_features\n        else:\n            features = input\n        if features.num_rows == 0:\n            raise ValueError(""Empty feature matrix."")\n        self.decoder.accept_input(key, features, ivector, online_ivectors,\n                                  self.online_ivector_period)\n\n    def get_output(self):\n        """"""Returns the next available output.\n\n        This returns the output for the first utterance in the output queue.\n        The outputs returned by this method are guaranteed to be in the same\n        order the inputs were provieded, but they may be delayed and some\n        outputs might be missing, for instance because of search failures.\n\n        This call does not block.\n\n        Output is a dictionary with the following `(key, value)` pairs:\n\n        ============ =========================== ==============================\n        key          value                       value type\n        ============ =========================== ==============================\n        ""key""        Utterence ID                `str`\n        ""lattice""    Output lattice              `Lattice` or `CompactLattice`\n        ""text""       Output transcript           `str`\n        ============ =========================== ==============================\n\n        The ""lattice"" output will be a deterministic compact lattice if lattice\n        determinization is enabled. Otherwise, it will be a raw state-level\n        lattice. The acoustic scores in the output lattice will already be\n        divided by the acoustic scale used in decoding.\n\n        If the decoder was not initialized with a symbol table, the ""text""\n        output will be a string of space separated integer indices. Otherwise it\n        will be a string of space separated symbols.\n\n        Returns:\n            A dictionary representing decoding output.\n\n        Raises:\n            ValueError: If there is no output to return.\n        """"""\n        key, lat, text = self._get_output()\n        return {""key"": key, ""lattice"": lat, ""text"": text}\n\n    def get_outputs(self):\n        """"""Creates a generator for iterating over available outputs.\n\n        Each output generated will be a dictionary like the output of\n        :meth:`get_output`. The outputs are generated in the same order the\n        inputs were provided.\n\n        See Also: :meth:`get_output`\n        """"""\n        while True:\n            try:\n                yield self.get_output()\n            except ValueError:\n                return\n\n    def finished(self):\n        """"""Informs the decoder that all input has been provided.\n\n        This will block until all computation threads have terminated. After\n        that you can keep calling :meth:`get_output`, until it raises a\n        `ValueError`, to get the outputs for the remaining utterances.\n\n        Returns:\n            int: The number of utterances that have been successfully decoded.\n        """"""\n        return self.decoder.finished()\n\n    def utterance_failed(self):\n        """"""Informs the decoder that there was a problem with an utterance.\n\n        This will update the number of failed utterances stats.\n        """"""\n        self.decoder.utterance_failed()\n\n\nclass NnetLatticeFasterGrammarRecognizer(NnetRecognizer):\n    """"""Neural network based lattice generating faster grammar speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        decoder (LatticeFasterGrammarDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decodable_opts (NnetSimpleComputationOptions): Configuration options for\n            simple nnet3 am decodable objects.\n        online_ivector_period (int): Onlne ivector period. Relevant only if\n            online ivectors are used.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder,\n                 symbols=None, allow_partial=True, decodable_opts=None,\n                 online_ivector_period=10):\n        if not isinstance(decoder, _dec.LatticeFasterGrammarDecoder):\n            raise TypeError(""decoder argument should be a ""\n                            ""LatticeFasterGrammarDecoder"")\n        super(NnetLatticeFasterGrammarRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols, allow_partial,\n            decodable_opts, online_ivector_period)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   decoder_opts=None, decodable_opts=None,\n                   online_ivector_period=10):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            decoder_opts (FasterDecoderOptions): Configuration options for the\n                decoder.\n            decodable_opts (NnetSimpleComputationOptions): Configuration options\n                for simple nnet3 am decodable objects.\n            online_ivector_period (int): Onlne ivector period. Relevant only if\n                online ivectors are used.\n\n        Returns:\n            NnetLatticeFasterGrammarRecognizer: A new recognizer.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        with _util_io.xopen(graph_rxfilename) as ki:\n            graph = _dec.GrammarFst()\n            graph.read(ki.stream(), ki.binary)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeFasterGrammarDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, decodable_opts, online_ivector_period)\n\n\nclass NnetLatticeBiglmFasterRecognizer(NnetRecognizer):\n    """"""Neural network based lattice generating big-LM faster speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        decoder (LatticeBiglmFasterDecoder): The decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decodable_opts (NnetSimpleComputationOptions): Configuration options for\n            simple nnet3 am decodable objects.\n        online_ivector_period (int): Onlne ivector period. Relevant only if\n            online ivectors are used.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder,\n                 symbols=None, allow_partial=True, decodable_opts=None,\n                 online_ivector_period=10):\n        if not isinstance(decoder, _dec.LatticeBiglmFasterDecoder):\n            raise TypeError(""decoder argument should be a ""\n                            ""LatticeBiglmFasterDecoder"")\n        super(NnetLatticeBiglmFasterRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols, allow_partial,\n            decodable_opts, online_ivector_period)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename, old_lm_rxfilename,\n                   new_lm_rxfilename, symbols_filename=None, allow_partial=True,\n                   decoder_opts=None, decodable_opts=None,\n                   online_ivector_period=10):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            old_lm_rxfilename (str): Extended filename for reading the old LM.\n            new_lm_rxfilename (str): Extended filename for reading the new LM.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            decoder_opts (LatticeFasterDecoderOptions): Configuration\n                options for the decoder.\n            decodable_opts (NnetSimpleComputationOptions): Configuration options\n                for simple nnet3 am decodable objects.\n            online_ivector_period (int): Onlne ivector period. Relevant only if\n                online ivectors are used.\n\n        Returns:\n            NnetLatticeBiglmFasterRecognizer: A new recognizer.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        self.old_lm = _fst.read_fst_kaldi(old_lm_rxfilename)\n        _fst_utils.apply_probability_scale(-1.0, self.old_lm)\n        self.new_lm = _fst.read_fst_kaldi(new_lm_rxfilename)\n        self._old_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(self.old_lm)\n        self._new_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(self.new_lm)\n        self._compose_lm = _fst_spec.StdComposeDeterministicOnDemandFst(\n            self._old_lm, self._new_lm)\n        self._cache_compose_lm = _fst_spec.StdCacheDeterministicOnDemandFst(\n            self._compose_lm)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeBiglmFasterDecoder(graph, decoder_opts,\n                                                 self._cache_compose_lm)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, decodable_opts, online_ivector_period)\n\n\nclass OnlineRecognizer(object):\n    """"""Base class for online speech recognizers.\n\n    Args:\n        decoder (object): The online decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, decoder, symbols=None, allow_partial=True,\n                 acoustic_scale=0.1):\n        self.decoder = decoder\n        self.symbols = symbols\n        self.allow_partial = allow_partial\n        self.acoustic_scale = acoustic_scale\n\n    def _make_decodable(self, input_pipeline):\n        """"""Constructs a new online decodable object from input pipeline.\n\n        Args:\n            input_pipeline (object): Input pipeline.\n\n        Returns:\n            DecodableInterface: An online decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        raise NotImplementedError\n\n    def _determinize_lattice(self, lattice):\n        """"""Determinizes raw state-level lattice.\n\n        Args:\n            lattice (Lattice): Raw state-level lattice.\n\n        Returns:\n            CompactLattice or Lattice: A deterministic compact lattice if the\n            decoder is configured to determinize lattices. Otherwise, a raw\n            state-level lattice.\n        """"""\n        opts = self.decoder.get_options()\n        if opts.determinize_lattice:\n            det_opts = _lat_funcs.DeterminizeLatticePrunedOptions()\n            det_opts.max_mem = opts.det_opts.max_mem\n            return _lat_funcs.determinize_lattice_pruned(\n                lattice, opts.lattice_beam, det_opts, True)\n        else:\n            return lattice\n\n    def set_input_pipeline(self, input_pipeline):\n        """"""Sets input pipeline.\n\n        Args:\n            input_pipeline (object): Input pipeline to decode online.\n        """"""\n        self._decodable = self._make_decodable(input_pipeline)\n\n    def init_decoding(self):\n        """"""Initializes decoding.\n\n        This should only be used if you intend to call :meth:`advance_decoding`.\n        If you intend to call :meth:`decode`, you don\'t need to call this. You\n        can also call this method if you have already decoded an utterance and\n        want to start with a new utterance.\n        """"""\n        self.decoder.init_decoding()\n\n    def advance_decoding(self, max_num_frames=-1):\n        """"""Advances decoding.\n\n        This will decode until there are no more frames ready in the input\n        pipeline or `max_num_frames` are decoded. You can keep calling this as\n        more frames become available.\n\n        Args:\n            max_num_frames (int): Maximum number of frames to decode. If\n                negative, all available frames are decoded.\n        """"""\n        self.decoder.advance_decoding(self._decodable, max_num_frames)\n\n    def finalize_decoding(self):\n        """"""Finalizes decoding.\n\n        This function may be optionally called after :meth:`advance_decoding`,\n        when you do not plan to decode any further. It does an extra pruning\n        step that will help to prune the output lattices more accurately,\n        particularly toward the end of the utterance. It does this by using the\n        final-probs in pruning (if any final-state survived); it also does a\n        final pruning step that visits all states (the pruning that is done\n        during decoding may fail to prune states that are within pruning_scale =\n        0.1 outside of the beam). If you call this, you cannot call\n        :meth:`advance_decoding` again (it will fail), and you cannot call\n        get_lattice and related functions with use_final_probs = false.\n        """"""\n        self.decoder.finalize_decoding()\n\n    def decode(self):\n        """"""Decodes all frames in the input pipeline and returns the output.\n\n        Output is a dictionary with the following `(key, value)` pairs:\n\n        ============ =========================== ==============================\n        key          value                       value type\n        ============ =========================== ==============================\n        ""alignment""  Frame-level alignment       `List[int]`\n        ""best_path""  Best lattice path           `CompactLattice`\n        ""lattice""    Output lattice              `Lattice` or `CompactLattice`\n        ""likelihood"" Log-likelihood of best path `float`\n        ""text""       Output transcript           `str`\n        ""weight""     Cost of best path           `LatticeWeight`\n        ""words""      Words on best path          `List[int]`\n        ============ =========================== ==============================\n\n        The ""lattice"" output is produced only if the decoder can generate\n        lattices. It will be a deterministic compact lattice if the decoder is\n        configured to determinize lattices. Otherwise, it will be a raw\n        state-level lattice.\n\n        If :attr:`symbols` is ``None``, the ""text"" output will be a string of\n        space separated integer indices. Otherwise it will be a string of space\n        separated symbols. The ""weight"" output is a lattice weight consisting of\n        (graph-score, acoustic-score).\n\n        Args:\n            input (object): Input to decode.\n\n        Returns:\n            A dictionary representing decoding output.\n\n        Raises:\n            RuntimeError: If decoding fails.\n        """"""\n        self.decoder.decode(self._decodable)\n        return self.get_output()\n\n    def get_output(self):\n        """"""Returns decoding output.\n\n        Output is a dictionary with the following `(key, value)` pairs:\n\n        ============ =========================== ==============================\n        key          value                       value type\n        ============ =========================== ==============================\n        ""alignment""  Frame-level alignment       `List[int]`\n        ""best_path""  Best lattice path           `CompactLattice`\n        ""lattice""    Output lattice              `Lattice` or `CompactLattice`\n        ""likelihood"" Log-likelihood of best path `float`\n        ""text""       Output transcript           `str`\n        ""weight""     Cost of best path           `LatticeWeight`\n        ""words""      Words on best path          `List[int]`\n        ============ =========================== ==============================\n\n        The ""lattice"" output is produced only if the decoder can generate\n        lattices. It will be a deterministic compact lattice if the decoder is\n        configured to determinize lattices. Otherwise, it will be a raw\n        state-level lattice.\n\n        If :attr:`symbols` is ``None``, the ""text"" output will be a string of\n        space separated integer indices. Otherwise it will be a string of space\n        separated symbols. The ""weight"" output is a lattice weight consisting of\n        (graph-score, acoustic-score).\n\n        Returns:\n            A dictionary representing decoding output.\n\n        Raises:\n            RuntimeError: If decoding fails.\n        """"""\n        if not (self.allow_partial or self.decoder.reached_final()):\n            raise RuntimeError(""No final state was active on the last frame."")\n\n        try:\n            best_path = self.decoder.get_best_path()\n        except RuntimeError:\n            raise RuntimeError(""Empty decoding output."")\n\n        ali, words, weight = _fst_utils.get_linear_symbol_sequence(best_path)\n\n        if self.symbols:\n            text = "" "".join(_fst.indices_to_symbols(self.symbols, words))\n        else:\n            text = "" "".join(map(str, words))\n\n        likelihood = - (weight.value1 + weight.value2)\n\n        if self.acoustic_scale != 0.0:\n            scale = _fst_utils.acoustic_lattice_scale(1.0 / self.acoustic_scale)\n            _fst_utils.scale_lattice(scale, best_path)\n        best_path = _fst_utils.convert_lattice_to_compact_lattice(best_path)\n\n        try:\n            lat = self.decoder.get_raw_lattice()\n        except AttributeError:\n            return {\n                ""alignment"": ali,\n                ""best_path"": best_path,\n                ""likelihood"": likelihood,\n                ""text"": text,\n                ""weight"": weight,\n                ""words"": words,\n            }\n        if lat.num_states() == 0:\n            raise RuntimeError(""Empty output lattice."")\n        lat.connect()\n\n        lat = self._determinize_lattice(lat)\n\n        if self.acoustic_scale != 0.0:\n            if isinstance(lat, _fst.CompactLatticeVectorFst):\n                _fst_utils.scale_compact_lattice(scale, lat)\n            else:\n                _fst_utils.scale_lattice(scale, lat)\n\n        return {\n            ""alignment"": ali,\n            ""best_path"": best_path,\n            ""lattice"": lat,\n            ""likelihood"": likelihood,\n            ""text"": text,\n            ""weight"": weight,\n            ""words"": words,\n        }\n\n    def get_partial_output(self, use_final_probs=False):\n        """"""Returns partial decoding output.\n\n        Output is a dictionary with the following `(key, value)` pairs:\n\n        ============ =========================== ==============================\n        key          value                       value type\n        ============ =========================== ==============================\n        ""alignment""  Frame-level alignment       `List[int]`\n        ""best_path""  Best lattice path           `Lattice`\n        ""likelihood"" Log-likelihood of best path `float`\n        ""text""       Output transcript           `str`\n        ""weight""     Cost of best path           `LatticeWeight`\n        ""words""      Words on best path          `List[int]`\n        ============ =========================== ==============================\n\n        If :attr:`symbols` is ``None``, the ""text"" output will be a string of\n        space separated integer indices. Otherwise it will be a string of space\n        separated symbols. The ""weight"" output is a lattice weight consisting of\n        (graph-score, acoustic-score).\n\n        Args:\n            use_final_probs (bool): Whether to use final probabilities when\n                computing best path.\n\n        Returns:\n            A dictionary representing decoding output.\n\n        Raises:\n            RuntimeError: If decoding fails.\n        """"""\n        try:\n            best_path = self.decoder.get_best_path(use_final_probs)\n        except RuntimeError:\n            raise RuntimeError(""Empty decoding output."")\n\n        ali, words, weight = _fst_utils.get_linear_symbol_sequence(best_path)\n\n        if self.symbols:\n            text = "" "".join(_fst.indices_to_symbols(self.symbols, words))\n        else:\n            text = "" "".join(map(str, words))\n\n        likelihood = - (weight.value1 + weight.value2)\n\n        return {\n            ""alignment"": ali,\n            ""best_path"": best_path,\n            ""likelihood"": likelihood,\n            ""text"": text,\n            ""weight"": weight,\n            ""words"": words,\n        }\n\n\nclass NnetOnlineRecognizer(OnlineRecognizer):\n    """"""Base class for neural network based online speech recognizers.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        decoder (object): The online decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decodable_opts (NnetSimpleLoopedComputationOptions): Configuration\n            options for simple looped neural network computation.\n        endpoint_opts (OnlineEndpointConfig): Online endpointing configuration.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder, symbols=None,\n                 allow_partial=True, decodable_opts=None, endpoint_opts=None):\n        if not isinstance(acoustic_model, _nnet3.AmNnetSimple):\n            raise TypeError(""acoustic_model should be a AmNnetSimple object"")\n        self.transition_model = transition_model\n        self.acoustic_model = acoustic_model\n        nnet = self.acoustic_model.get_nnet()\n        _nnet3.set_batchnorm_test_mode(True, nnet)\n        _nnet3.set_dropout_test_mode(True, nnet)\n        _nnet3.collapse_model(_nnet3.CollapseModelConfig(), nnet)\n\n        if decodable_opts:\n            if not isinstance(decodable_opts,\n                              _nnet3.NnetSimpleLoopedComputationOptions):\n                raise TypeError(""decodable_opts should be either None or a ""\n                                ""NnetSimpleLoopedComputationOptions object"")\n            self.decodable_opts = decodable_opts\n        else:\n            self.decodable_opts = _nnet3.NnetSimpleLoopedComputationOptions()\n        self.decodable_info = _nnet3.DecodableNnetSimpleLoopedInfo.from_am(\n            self.decodable_opts, self.acoustic_model)\n\n        if endpoint_opts:\n            if not isinstance(endpoint_opts,\n                              _online2.OnlineEndpointConfig):\n                raise TypeError(""decodable_opts should be either None or a ""\n                                ""OnlineEndpointConfig object"")\n            self.endpoint_opts = endpoint_opts\n        else:\n            self.endpoint_opts = _online2.OnlineEndpointConfig()\n\n        super(NnetOnlineRecognizer, self).__init__(\n            decoder, symbols, allow_partial, self.decodable_opts.acoustic_scale)\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads model from an extended filename.""""""\n        with _util_io.xopen(model_rxfilename) as ki:\n            transition_model = _hmm.TransitionModel().read(ki.stream(),\n                                                           ki.binary)\n            acoustic_model = _nnet3.AmNnetSimple().read(ki.stream(), ki.binary)\n        return transition_model, acoustic_model\n\n    def _make_decodable(self, feature_pipeline):\n        """"""Constructs a new online decodable object from input feature pipeline.\n\n        This method also sets output_frame_shift which is used in endpointing.\n\n        Args:\n            feature_pipeline (OnlineNnetFeaturePipeline): Input feature\n                pipeline.\n\n        Returns:\n            DecodableAmNnetLoopedOnline: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        self.output_frame_shift = (feature_pipeline.frame_shift_in_seconds() *\n                                   self.decodable_opts.frame_subsampling_factor)\n        return _nnet3.DecodableAmNnetLoopedOnline(\n            self.transition_model, self.decodable_info,\n            feature_pipeline.input_feature(),\n            feature_pipeline.ivector_feature())\n\n    def _determinize_lattice(self, lattice):\n        """"""Determinizes raw state-level lattice.\n\n        Args:\n            lattice (Lattice): Raw state-level lattice.\n\n        Returns:\n            CompactLattice or Lattice: A deterministic compact lattice if the\n            decoder is configured to determinize lattices. Otherwise, a raw\n            state-level lattice.\n        """"""\n        opts = self.decoder.get_options()\n        if opts.determinize_lattice:\n            return _lat_funcs.determinize_lattice_phone_pruned(\n                lattice, self.transition_model, opts.lattice_beam,\n                opts.det_opts, True)\n        else:\n            return lattice\n\n\nclass NnetLatticeFasterOnlineRecognizer(NnetOnlineRecognizer):\n    """"""Neural network based lattice generating faster online speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        decoder (LatticeFasterOnlineDecoder): The online decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decodable_opts (NnetSimpleLoopedComputationOptions): Configuration\n            options for simple looped neural network computation.\n        endpoint_opts (OnlineEndpointConfig): Online endpointing configuration.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder, symbols=None,\n                 allow_partial=True, decodable_opts=None, endpoint_opts=None):\n        if not isinstance(decoder, _dec.LatticeFasterOnlineDecoder):\n            raise TypeError(""decoder argument should be a ""\n                            ""LatticeFasterOnlineDecoder"")\n        super(NnetLatticeFasterOnlineRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols, allow_partial,\n            decodable_opts, endpoint_opts)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   decoder_opts=None, decodable_opts=None, endpoint_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            decoder_opts (FasterDecoderOptions): Configuration options for the\n                decoder.\n            decodable_opts (NnetSimpleLoopedComputationOptions): Configuration\n                options for simple looped neural network computation.\n            endpoint_opts (OnlineEndpointConfig): Online endpointing\n                configuration.\n\n        Returns:\n            NnetLatticeFasterOnlineRecognizer: A new recognizer.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        graph = _fst.read_fst_kaldi(graph_rxfilename)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeFasterOnlineDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, decodable_opts, endpoint_opts)\n\n    def endpoint_detected(self):\n        """"""Determines if any of the endpointing rules are active.""""""\n        return _online2.decoding_endpoint_detected(\n            self.endpoint_opts, self.transition_model,\n            self.output_frame_shift, self.decoder)\n\n\nclass NnetLatticeFasterOnlineGrammarRecognizer(NnetOnlineRecognizer):\n    """"""Neural network based lattice generating faster online grammar speech recognizer.\n\n    Args:\n        transition_model (TransitionModel): The transition model.\n        acoustic_model (AmNnetSimple): The acoustic model.\n        decoder (LatticeFasterOnlineGrammarDecoder): The online decoder.\n        symbols (SymbolTable): The symbol table. If provided, ""text"" output of\n            :meth:`decode` includes symbols instead of integer indices.\n        allow_partial (bool): Whether to output decoding results if no\n            final state was active on the last frame.\n        decodable_opts (NnetSimpleLoopedComputationOptions): Configuration\n            options for simple looped neural network computation.\n        endpoint_opts (OnlineEndpointConfig): Online endpointing configuration.\n    """"""\n    def __init__(self, transition_model, acoustic_model, decoder, symbols=None,\n                 allow_partial=True, decodable_opts=None, endpoint_opts=None):\n        if not isinstance(decoder, _dec.LatticeFasterOnlineGrammarDecoder):\n            raise TypeError(""decoder argument should be a ""\n                            ""LatticeFasterOnlineGrammarDecoder"")\n        super(NnetLatticeFasterOnlineGrammarRecognizer, self).__init__(\n            transition_model, acoustic_model, decoder, symbols, allow_partial,\n            decodable_opts, endpoint_opts)\n\n    @classmethod\n    def from_files(cls, model_rxfilename, graph_rxfilename,\n                   symbols_filename=None, allow_partial=True,\n                   decoder_opts=None, decodable_opts=None, endpoint_opts=None):\n        """"""Constructs a new recognizer from given files.\n\n        Args:\n            model_rxfilename (str): Extended filename for reading the model.\n            graph_rxfilename (str): Extended filename for reading the graph.\n            symbols_filename (str): The symbols file. If provided, ""text"" output\n                of :meth:`decode` includes symbols instead of integer indices.\n            allow_partial (bool): Whether to output decoding results if no\n                final state was active on the last frame.\n            decoder_opts (FasterDecoderOptions): Configuration options for the\n                decoder.\n            decodable_opts (NnetSimpleLoopedComputationOptions): Configuration\n                options for simple looped neural network computation.\n            endpoint_opts (OnlineEndpointConfig): Online endpointing\n                configuration.\n\n        Returns:\n            NnetLatticeFasterOnlineGrammarRecognizer: A new recognizer.\n        """"""\n        transition_model, acoustic_model = cls.read_model(model_rxfilename)\n        with _util_io.xopen(graph_rxfilename) as ki:\n            graph = _dec.GrammarFst()\n            graph.read(ki.stream(), ki.binary)\n        if not decoder_opts:\n            decoder_opts = _dec.LatticeFasterDecoderOptions()\n        decoder = _dec.LatticeFasterOnlineGrammarDecoder(graph, decoder_opts)\n        if symbols_filename is None:\n            symbols = None\n        else:\n            symbols = _fst.SymbolTable.read_text(symbols_filename)\n        return cls(transition_model, acoustic_model, decoder, symbols,\n                   allow_partial, decodable_opts, endpoint_opts)\n\n    def endpoint_detected(self):\n        """"""Determines if any of the endpointing rules are active.""""""\n        return _online2.decoding_endpoint_detected_grammar(\n            self.endpoint_opts, self.transition_model,\n            self.output_frame_shift, self.decoder)\n\n\nclass LatticeLmRescorer(object):\n    """"""Lattice LM rescorer.\n\n    If `phi_label` is provided, rescoring will be ""exact"" in the sense that\n    back-off arcs in the new LM will only be taken if there are no other\n    matching arcs. Inexact rescoring can overestimate the new LM scores for some\n    paths in the output lattice. This happens when back-off paths have higher\n    scores than matching regular paths in the new LM.\n\n    Args:\n        old_lm (StdFst): Old language model FST.\n        new_lm (StdFst): New language model FST.\n        phi_label (int): Back-off label in the new LM.\n    """"""\n    def __init__(self, old_lm, new_lm, phi_label=None):\n        self.phi_label = phi_label\n        self.old_lm = _fst_utils.convert_std_to_lattice(old_lm).project(True)\n        if not bool(self.old_lm.properties(_fst_props.I_LABEL_SORTED, True)):\n            self.old_lm.arcsort()\n        self.new_lm = _fst_utils.convert_std_to_lattice(new_lm)\n        if not self.phi_label:\n            self.new_lm.project(True)\n        if not bool(self.new_lm.properties(_fst_props.I_LABEL_SORTED, True)):\n            self.new_lm.arcsort()\n        self.old_lm_compose_cache = _fst_spec.LatticeTableComposeCache.from_compose_opts(\n            _fst_spec.TableComposeOptions.from_matcher_opts(\n                _fst_spec.TableMatcherOptions(),\n                table_match_type=_fst_enums.MatchType.MATCH_INPUT))\n        if not self.phi_label:\n            self.new_lm_compose_cache = _fst_spec.LatticeTableComposeCache.from_compose_opts(\n                _fst_spec.TableComposeOptions.from_matcher_opts(\n                    _fst_spec.TableMatcherOptions(),\n                    table_match_type=_fst_enums.MatchType.MATCH_INPUT))\n\n    def rescore(self, lat):\n        """"""Rescores input lattice.\n\n        Args:\n            lat (CompactLatticeFst): Input lattice.\n\n        Returns:\n            CompactLatticeVectorFst: Rescored lattice.\n        """"""\n        if isinstance(lat, _fst_fst.CompactLatticeFst):\n            lat = _fst_utils.convert_compact_lattice_to_lattice(lat)\n        else:\n            raise TypeError(""Input should be a compact lattice."")\n        scale = _fst_utils.graph_lattice_scale(-1.0)\n        _fst_utils.scale_lattice(scale, lat)\n        if not bool(lat.properties(_fst_props.O_LABEL_SORTED, True)):\n            lat.arcsort(""olabel"")\n        composed_lat = _fst.LatticeVectorFst()\n        _fst_spec.table_compose_cache_lattice(lat, self.old_lm, composed_lat,\n                                              self.old_lm_compose_cache)\n        determinized_lat = _fst_spec.determinize_lattice(composed_lat.invert(),\n                                                         False).invert()\n        _fst_utils.scale_lattice(scale, determinized_lat)\n        if self.phi_label:\n            _fst_utils.phi_compose_lattice(determinized_lat, self.new_lm,\n                                           self.phi_label, composed_lat)\n        else:\n            _fst_spec.table_compose_cache_lattice(determinized_lat,\n                                                  self.new_lm, composed_lat,\n                                                  self.new_lm_compose_cache)\n        determinized_lat = _fst_spec.determinize_lattice(composed_lat.invert())\n        return determinized_lat\n\n    @classmethod\n    def from_files(cls, old_lm_rxfilename, new_lm_rxfilename, phi_label=None):\n        """"""Constructs a new lattice LM rescorer from given files.\n\n        Args:\n            old_lm_rxfilename (str): Extended filename for reading the old LM.\n            new_lm_rxfilename (str): Extended filename for reading the new LM.\n            phi_label (int): Back-off label in the new LM.\n\n        Returns:\n            LatticeRescorer: A new lattice LM rescorer.\n        """"""\n        old_lm = _fst.read_fst_kaldi(old_lm_rxfilename)\n        new_lm = _fst.read_fst_kaldi(new_lm_rxfilename)\n        return cls(old_lm, new_lm, phi_label)\n\n\nclass LatticeRnnlmPrunedRescorer(object):\n    """"""Lattice RNNLM rescorer.\n\n    Args:\n        old_lm (ConstArpaLm or StdFst): Old LM.\n        word_embedding_mat (CuMatrix): Word embeddings.\n        rnnlm (Nnet): RNNLM.\n        lm_scale (float): Scaling factor for RNNLM weights. Negated scaling\n            factor will be applied to old LM weights.\n        acoustic_scale (float): Scaling factor for acoustic weights.\n        max_ngram_order (int): RNNLM histories longer than this value will\n            be considered equivalent for rescoring purposes. This is an\n            approximation saving time and reducing output lattice size.\n        opts (RnnlmComputeStateComputationOptions): Options for RNNLM\n            state computation.\n        compose_opts (ComposeLatticePrunedOptions): Options for pruned\n            lattice composition.\n    """"""\n    def __init__(self, old_lm, word_embedding_mat, rnnlm,\n                 lm_scale=0.5, acoustic_scale=0.1, max_ngram_order=3,\n                 opts=None, compose_opts=None):\n        self.old_lm = old_lm\n        if isinstance(self.old_lm, _lm.ConstArpaLm):\n            self.det_old_lm = _lm.ConstArpaLmDeterministicFst(self.old_lm)\n        else:\n            if not bool(self.old_lm.properties(_fst_props.ACCEPTOR, True)):\n                self.old_lm.project(True)\n            if not bool(self.old_lm.properties(_fst_props.I_LABEL_SORTED, True)):\n                self.old_lm.arcsort()\n            self.det_old_lm = _fst_spec.StdBackoffDeterministicOnDemandFst(\n                self.old_lm)\n        self.scaled_old_lm = _fst_spec.ScaleDeterministicOnDemandFst(\n            -lm_scale, self.det_old_lm)\n        if not _nnet3.is_simple_nnet(rnnlm):\n            raise ValueError(""RNNLM should be a simple nnet"")\n        if not opts:\n            opts = _rnnlm.RnnlmComputeStateComputationOptions()\n        self.word_embedding_mat = word_embedding_mat\n        self.rnnlm = rnnlm\n        self.info = _rnnlm.RnnlmComputeStateInfo(opts, self.rnnlm,\n                                                 self.word_embedding_mat)\n        self.det_rnnlm = _rnnlm.KaldiRnnlmDeterministicFst(max_ngram_order,\n                                                           self.info)\n        self.lm_scale = lm_scale\n        self.acoustic_scale = acoustic_scale\n        if compose_opts:\n            self.compose_opts = compose_opts\n        else:\n            self.compose_opts = _lat_funcs.ComposeLatticePrunedOptions()\n\n    def rescore(self, lat):\n        """"""Rescores input lattice.\n\n        Args:\n            lat (CompactLatticeFst): Input lattice.\n\n        Returns:\n            CompactLatticeVectorFst: Rescored lattice.\n        """"""\n        scaled_rnnlm = _fst_spec.ScaleDeterministicOnDemandFst(\n            self.lm_scale, self.det_rnnlm)\n        if self.acoustic_scale != 1.0:\n            scale = _fst_utils.acoustic_lattice_scale(self.acoustic_scale)\n            _fst_utils.scale_compact_lattice(scale, lat)\n        _lat_funcs.top_sort_lattice_if_needed(lat)\n        combined_lms = _fst_spec.StdComposeDeterministicOnDemandFst(\n            self.scaled_old_lm, scaled_rnnlm)\n        composed_lat = _lat_funcs.compose_compact_lattice_pruned(\n            self.compose_opts, lat, combined_lms)\n        self.det_rnnlm.clear()\n        if self.acoustic_scale != 1.0:\n            scale = _fst_utils.acoustic_lattice_scale(1.0 / self.acoustic_scale)\n            _fst_utils.scale_compact_lattice(scale, composed_lat)\n        return composed_lat\n\n    @classmethod\n    def from_files(cls, old_lm_rxfilename, word_embedding_rxfilename,\n                   rnnlm_rxfilename, lm_scale=0.5, acoustic_scale=0.1,\n                   max_ngram_order=3, use_const_arpa=False, opts=None,\n                   compose_opts=None):\n        """"""Constructs a new lattice LM rescorer from given files.\n\n        Args:\n            old_lm_rxfilename (str): Extended filename for reading the old LM.\n            word_embedding_rxfilename (str): Extended filename for reading the\n                word embeddings.\n            rnnlm_rxfilename (str): Extended filename for reading the new RNNLM.\n            lm_scale (float): Scaling factor for RNNLM weights. Negated scaling\n                factor will be applied to old LM weights.\n            acoustic_scale (float): Scaling factor for acoustic weights.\n            max_ngram_order (int): RNNLM histories longer than this value will\n                be considered equivalent for rescoring purposes. This is an\n                approximation saving time and reducing output lattice size.\n            use_const_arpa (bool): If True, read the old LM as a const-arpa\n                file as opposed to an FST file. This is helpful when rescoring\n                with a large LM.\n            opts (RnnlmComputeStateComputationOptions): Options for RNNLM\n                state computation.\n            compose_opts (ComposeLatticePrunedOptions): Options for pruned\n                lattice composition.\n\n        Returns:\n            LatticeRnnlmPrunedRescorer: A new lattice RNNLM rescorer.\n        """"""\n        if use_const_arpa:\n            with _util_io.xopen(old_lm_rxfilename) as ki:\n                old_lm = _lm.ConstArpaLm()\n                old_lm.read(ki.stream(), ki.binary)\n        else:\n            old_lm = _fst.read_fst_kaldi(old_lm_rxfilename)\n        with _util_io.xopen(word_embedding_rxfilename) as ki:\n            word_embedding_mat = _cumatrix.CuMatrix()\n            word_embedding_mat.read(ki.stream(), ki.binary)\n        with _util_io.xopen(rnnlm_rxfilename) as ki:\n            rnnlm = _nnet3.Nnet()\n            rnnlm.read(ki.stream(), ki.binary)\n        return cls(old_lm, word_embedding_mat, rnnlm, lm_scale, acoustic_scale,\n                   max_ngram_order, opts, compose_opts)\n'"
kaldi/segmentation.py,0,"b'from __future__ import division, print_function\n\nimport math\n\nfrom . import decoder as _dec\nfrom . import fstext as _fst\nfrom .fstext import utils as _fst_utils\nfrom . import matrix as _mat\nfrom .matrix import common as _mat_comm\nfrom . import nnet3 as _nnet3\nfrom .util import io as _util_io\n\n\n__all__ = [\'Segmenter\', \'NnetSAD\', \'SegmentationProcessor\']\n\n\nclass Segmenter(object):\n    """"""Base class for speech segmenters.\n\n    Args:\n        graph (StdVectorFst): Segmentation graph.\n        beam (float): Logarithmic decoding beam.\n        max_active (int): Maximum number of active states in decoding.\n        acoustic_scale (float): Acoustic score scale.\n    """"""\n    def __init__(self, graph, beam=8, max_active=1000, acoustic_scale=0.1):\n        decoder_opts = _dec.FasterDecoderOptions()\n        decoder_opts.beam = beam\n        decoder_opts.max_active = max_active\n        self.decoder = _dec.FasterDecoder(graph, decoder_opts)\n        self.acoustic_scale = acoustic_scale\n\n    def _make_decodable(self, loglikes):\n        """"""Constructs a new decodable object from input log-likelihoods.\n\n        Args:\n            loglikes (object): Input log-likelihoods.\n\n        Returns:\n            DecodableMatrixScaled: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        if loglikes.num_rows == 0:\n            raise ValueError(""Empty loglikes matrix."")\n        return _dec.DecodableMatrixScaled(loglikes, self.acoustic_scale)\n\n    def segment(self, input):\n        """"""Segments input.\n\n        Output is a dictionary with the following `(key, value)` pairs:\n\n        ============== ============================== ==========================\n        key            value                          value type\n        ============== ============================== ==========================\n        ""alignment""    Frame-level segmentation       `List[int]`\n        ""best_path""    Best lattice path              `CompactLattice`\n        ""likelihood""   Log-likelihood of best path    `float`\n        ""weight""       Cost of best path              `LatticeWeight`\n        ============== ============================== ==========================\n\n        The ""weight"" output is a lattice weight consisting of (graph-score,\n        acoustic-score).\n\n        Args:\n            input (object): Input to segment.\n\n        Returns:\n            A dictionary representing segmentation output.\n\n        Raises:\n            RuntimeError: If segmentation fails.\n        """"""\n        self.decoder.decode(self._make_decodable(input))\n\n        if not self.decoder.reached_final():\n            raise RuntimeError(""No final state was active on the last frame."")\n\n        try:\n            best_path = self.decoder.get_best_path()\n        except RuntimeError:\n            raise RuntimeError(""Empty segmentation output."")\n\n        ali, _, weight = _fst_utils.get_linear_symbol_sequence(best_path)\n        likelihood = - (weight.value1 + weight.value2)\n\n        if self.acoustic_scale != 0.0:\n            scale = _fst_utils.acoustic_lattice_scale(1.0 / self.acoustic_scale)\n            _fst_utils.scale_lattice(scale, best_path)\n        best_path = _fst_utils.convert_lattice_to_compact_lattice(best_path)\n\n        return {\n            ""alignment"": ali,\n            ""best_path"": best_path,\n            ""likelihood"": likelihood,\n            ""weight"": weight,\n        }\n\n\nclass NnetSAD(Segmenter):\n    """"""Neural network based speech activity detection (SAD).\n\n    Args:\n        model (Nnet): SAD model. Model output should be log-posteriors for\n            [silence, speech, garbage] labels.\n        transform (Matrix): Transformation applied to SAD label posteriors. It\n            should be a 3x2 matrix mapping [silence, speech, garbage] posteriors\n            to [silence, speech] pseudo-likelihoods.\n        graph (StdVectorFst): SAD graph. Silence and speech arcs should be\n            labeled respectively with 1 and 2.\n        beam (float): Logarithmic decoding beam.\n        max_active (int): Maximum number of active states in decoding.\n        decodable_opts (NnetSimpleComputationOptions): Configuration options for\n            the SAD model.\n    """"""\n    def __init__(self, model, transform, graph, beam=8, max_active=1000,\n                 decodable_opts=None):\n        if not isinstance(model, _nnet3.Nnet):\n            raise TypeError(""model argument should be a Nnet object"")\n        self.model = model\n        self.priors = _mat.Vector()\n        self.transform = transform\n        _nnet3.set_batchnorm_test_mode(True, model)\n        _nnet3.set_dropout_test_mode(True, model)\n        _nnet3.collapse_model(_nnet3.CollapseModelConfig(), model)\n        if decodable_opts:\n            if not isinstance(decodable_opts,\n                              _nnet3.NnetSimpleComputationOptions):\n                raise TypeError(""decodable_opts should be either None or a ""\n                                ""NnetSimpleComputationOptions object"")\n            self.decodable_opts = decodable_opts\n        else:\n            self.decodable_opts = _nnet3.NnetSimpleComputationOptions()\n        self.compiler = _nnet3.CachingOptimizingCompiler.new_with_optimize_opts(\n            model, self.decodable_opts.optimize_config)\n        super(NnetSAD, self).__init__(graph, beam, max_active,\n                                      self.decodable_opts.acoustic_scale)\n\n    def _make_decodable(self, features):\n        """"""Constructs a new decodable object from input features.\n\n        Args:\n            features (Matrix): Input features.\n\n        Returns:\n            DecodableMatrixScaled: A decodable object for computing scaled\n            log-likelihoods.\n        """"""\n        if features.num_rows == 0:\n            raise ValueError(""Empty feature matrix."")\n\n        nnet_computer = _nnet3.DecodableNnetSimple(\n            self.decodable_opts, self.model, self.priors,\n            features, self.compiler, None, None, 0)\n\n        post = _mat.Matrix(nnet_computer.num_frames(),\n                           nnet_computer.output_dim())\n        for t in range(nnet_computer.num_frames()):\n            nnet_computer.get_output_for_frame(t, post[t])\n        post.apply_exp_()\n        # FIXME: Need to keep a reference to log_likes to keep it in scope\n        self._log_likes = _mat.Matrix(post.num_rows, self.transform.num_rows)\n        self._log_likes.add_mat_mat_(post, self.transform,\n                                     _mat_comm.MatrixTransposeType.NO_TRANS,\n                                     _mat_comm.MatrixTransposeType.TRANS,\n                                     1.0, 0.0)\n        self._log_likes.apply_log_()\n        return _dec.DecodableMatrixScaled(self._log_likes, self.acoustic_scale)\n\n    @staticmethod\n    def read_model(model_rxfilename):\n        """"""Reads SAD model from an extended filename.""""""\n        with _util_io.xopen(model_rxfilename) as ki:\n            return _nnet3.Nnet().read(ki.stream(), ki.binary)\n\n    @staticmethod\n    def read_average_posteriors(post_rxfilename):\n        """"""Reads average SAD label posteriors from an extended filename.""""""\n        with _util_io.xopen(post_rxfilename) as ki:\n            return _mat.Vector().read_(ki.stream(), ki.binary)\n\n    @staticmethod\n    def make_sad_transform(priors, sil_scale=1.0,\n                           sil_in_speech_weight=0.0,\n                           speech_in_sil_weight=0.0,\n                           garbage_in_speech_weight=0.0,\n                           garbage_in_sil_weight=0.0):\n        """"""Creates SAD posterior transformation matrix.\n\n        The 3x2 transformation matrix is used to convert length Nx3 posterior\n        probability matrices to Nx2 pseudo-likelihood matrices.\n\n        The ""priors"" vector can be a proper prior probability distribution over\n        SAD labels or simply average SAD label posteriors. This vector is\n        normalized to derive a prior probability distribution.\n\n        Args:\n            priors (Vector): SAD label priors to remove from the neural network\n                output posteriors to convert them to pseudo likelihoods.\n            sil_scale (float): Scale on the silence probability. Make this more\n                than one to encourage decoding silence.\n            sil_in_speech_weight (float): The fraction of silence probability to\n                add to speech probability.\n            speech_in_sil_weight (float): The fraction of speech probability to\n                add to silence probability.\n            garbage_in_speech_weight (float): The fraction of garbage\n                probability to add to speech probability.\n            garbage_in_sil_weight (float): The fraction of garbage probability\n                to add to silence probability.\n        """"""\n        priors_sum = priors.sum()\n        sil_prior = priors[0] / priors_sum\n        speech_prior = priors[1] / priors_sum\n        garbage_prior = priors[2] / priors_sum\n\n        return _mat.Matrix([[sil_scale / sil_prior,\n                             speech_in_sil_weight / speech_prior,\n                             garbage_in_sil_weight / garbage_prior],\n                            [sil_in_speech_weight / sil_prior,\n                             1.0 / speech_prior,\n                             garbage_in_speech_weight / garbage_prior]])\n\n    @staticmethod\n    def make_sad_graph(transition_scale=1.0, self_loop_scale=0.1,\n                       min_silence_duration=0.03, min_speech_duration=0.3,\n                       max_speech_duration=10.0, frame_shift=0.01,\n                       edge_silence_probability=0.5, transition_probability=0.1):\n        """"""Makes a decoding graph with a simple HMM topology suitable for SAD.\n\n        Output graph uses label 1 for \'silence\' and label 2 for \'speech\'.\n\n        Args:\n            transition_scale (float): Scale on transition log-probabilities\n                relative to LM weights.\n            self_loop_scale (float): Scale on self-loop log-probabilities\n                relative to LM weights.\n            min_silence_duration (float): Minimum duration for silence.\n            min_speech_duration (float): Minimum duration for speech.\n            max_speech_duration (float): Maximum duration for speech.\n            frame_shift (float): Frame shift in seconds.\n            edge_silence_probability (float): Probability of silence at the\n                edges.\n            transition_probability (float): Transition probability for silence\n                to speech or vice-versa.\n\n        Returns:\n            StdVectorFst: A simple decoding graph suitable for SAD.\n        """"""\n        min_states_silence = int(min_silence_duration / frame_shift + 0.5)\n        min_states_speech = int(min_speech_duration / frame_shift + 0.5)\n        max_states_speech = int(max_speech_duration / frame_shift + 0.5)\n\n        symbols = _fst.SymbolTable()\n        symbols.add_symbol(""<eps>"")\n        symbols.add_symbol(""silence"")\n        symbols.add_symbol(""speech"")\n        compiler = _fst.StdFstCompiler(symbols, symbols)\n\n        # Initial transition to silence\n        print(""0 1 silence silence {cost}"".format(\n                    cost=-math.log(edge_silence_probability)),\n              file=compiler)\n        silence_start_state = 1\n\n        # Silence min duration transitions\n        # 1->2, 2->3 and so on until\n        # (1 + min_states_silence - 2) -> (1 + min_states_silence - 1)  ...\n        for state in range(silence_start_state,\n                           silence_start_state + min_states_silence - 1):\n            print (""{state} {next_state} silence silence {cost}"".format(\n                        state=state, next_state=state + 1, cost=0.0),\n                   file=compiler)\n        silence_last_state = silence_start_state + min_states_silence - 1\n\n        # Silence self-loop\n        print (""{state} {state} silence silence {cost}"".format(\n                    state=silence_last_state, cost=0.0),\n               file=compiler)\n\n        speech_start_state = silence_last_state + 1\n\n        # Initial transition to speech\n        print (""0 {state} speech speech {cost}"".format(\n                    state=speech_start_state,\n                    cost=-math.log(1.0 - edge_silence_probability)),\n               file=compiler)\n\n        # Silence to speech transition\n        print (""{sil_state} {speech_state} speech speech {cost}"".format(\n                    sil_state=silence_last_state,\n                    speech_state=speech_start_state,\n                    cost=-math.log(transition_probability)),\n               file=compiler)\n\n        # Speech min duration\n        for state in range(speech_start_state,\n                           speech_start_state + min_states_speech - 1):\n            print (""{state} {next_state} speech speech {cost}"".format(\n                        state=state, next_state=state + 1, cost=0.0),\n                   file=compiler)\n\n        # Speech max duration\n        for state in range(speech_start_state + min_states_speech - 1,\n                           speech_start_state + max_states_speech - 1):\n            print (""{state} {next_state} speech speech {cost}"".format(\n                        state=state, next_state=state + 1, cost=0.0),\n                   file=compiler)\n\n            print (""{state} {sil_state} silence silence {cost}"".format(\n                        state=state, sil_state=silence_start_state,\n                        cost=-math.log(transition_probability)),\n                   file=compiler)\n        speech_last_state = speech_start_state + max_states_speech - 1\n\n        # Transition to silence after max duration of speech\n        print (""{state} {sil_state} silence silence {cost}"".format(\n                    state=speech_last_state, sil_state=silence_start_state,\n                    cost=0.0),\n               file=compiler)\n\n        for state in range(1, speech_start_state):\n            print (""{state} {cost}"".format(\n                        state=state, cost=-math.log(edge_silence_probability)),\n                   file=compiler)\n\n        for state in range(speech_start_state, speech_last_state + 1):\n            print (""{state} {cost}"".format(\n                        state=state,\n                        cost=-math.log(1.0 - edge_silence_probability)),\n                   file=compiler)\n\n        return compiler.compile()\n\n\nclass SegmentationProcessor(object):\n    """"""Segmentation post-processor.\n\n    This class is used for converting segmentation labels to a list of segments.\n    Output includes only those segments labeled with the target labels.\n\n    Post-processing operations include::\n        * filtering out short segments\n        * padding segments\n        * merging consecutive segments\n\n    Args:\n        target_labels (List[int]): Target labels. Typically the speech labels.\n        frame_shift (float): Frame shift in seconds.\n        segment_padding (float): Additional padding on target segments.\n            Padding does not go beyond the adjacent segment. This is typically\n            used for padding speech segments with silence. Must be an integral\n            multiple of frame shift.\n        min_segment_dur (float): Minimum duration (in seconds) required for a\n            segment to be included. This is before any padding. Segments shorter\n            than this duration will be removed.\n        max_merged_segment_dur (float): Merge consecutive segments as long as\n            the merged segment is no longer than this many seconds. The segments\n            are only merged if their boundaries are touching. This is after\n            padding by --segment-padding seconds. 0 means do not merge. Use\n            \'inf\' to not limit the duration.\n\n    Attributes:\n        stats (SegmentationProcessor.Stats): Global segmentation post-processing\n            stats.\n    """"""\n    def __init__(self, target_labels, frame_shift=0.01, segment_padding=0.2,\n                 min_segment_dur=0, max_merged_segment_dur=0):\n        if not float(segment_padding / frame_shift).is_integer():\n            raise ValueError(""segment_padding = {} is not an integral ""\n                             ""multiple of frame_shift = {}""\n                             .format(segment_padding,frame_shift))\n        self.target_labels = target_labels\n        self.frame_shift = frame_shift\n        self.segment_padding = int(segment_padding / frame_shift)\n        self.min_segment_dur = int(math.ceil(min_segment_dur / frame_shift))\n        self.max_merged_segment_dur = int(max_merged_segment_dur / frame_shift)\n        self.stats = self.Stats()\n\n    class Stats(object):\n        """"""Stores segmentation post-processing stats.""""""\n\n        def __init__(self):\n            self.num_segments_initial = 0\n            self.num_short_segments_filtered = 0\n            self.num_merges = 0\n            self.num_segments_final = 0\n            self.initial_duration = 0.0\n            self.padding_duration = 0.0\n            self.filter_short_duration = 0.0\n            self.final_duration = 0.0\n\n        def add(self, other):\n            """"""Adds stats from another""""""\n            self.num_segments_initial += other.num_segments_initial\n            self.num_short_segments_filtered += other.num_short_segments_filtered\n            self.num_merges += other.num_merges\n            self.num_segments_final += other.num_segments_final\n            self.initial_duration += other.initial_duration\n            self.filter_short_duration += other.filter_short_duration\n            self.padding_duration += other.padding_duration\n            self.final_duration += other.final_duration\n\n        def __str__(self):\n            return (""num-segments-initial={num_segments_initial}, ""\n                    ""num-short-segments-filtered={num_short_segments_filtered}, ""\n                    ""num-merges={num_merges}, ""\n                    ""num-segments-final={num_segments_final}, ""\n                    ""initial-duration={initial_duration}, ""\n                    ""filter-short-duration={filter_short_duration}, ""\n                    ""padding-duration={padding_duration}, ""\n                    ""final-duration={final_duration}"".format(\n                num_segments_initial=self.num_segments_initial,\n                num_short_segments_filtered=self.num_short_segments_filtered,\n                num_merges=self.num_merges,\n                num_segments_final=self.num_segments_final,\n                initial_duration=self.initial_duration,\n                filter_short_duration=self.filter_short_duration,\n                padding_duration=self.padding_duration,\n                final_duration=self.final_duration))\n\n    def process(self, alignment):\n        """"""Converts frame-level segmentation labels to a list of segments.\n\n        Args:\n            alignment (List[int]): Frame-level segmentation labels.\n\n        Returns:\n            Tuple[List[Tuple[int, int, int]], SegmentationProcessor.Stats]: List\n            of segments, where each entry is a (segment-beg, segment-end, label)\n            tuple, along with segmentation post-processing stats.\n        """"""\n        stats = self.Stats()\n        segments = self.initialize_segments(alignment, stats)\n        segments = self.filter_short_segments(segments, stats)\n        segments = self.pad_segments(segments, stats, len(alignment))\n        segments = self.merge_consecutive_segments(segments, stats)\n        self.stats.add(stats)\n        return segments, stats\n\n    def initialize_segments(self, alignment, stats):\n        """"""Initializes segments.\n\n        The alignment is frame-level segmentation labels. Output includes only\n        those segments labeled with the target labels.\n        """"""\n        segments = []\n        if not alignment:\n            return segments\n        num_target_frames, seg_begin, seg_label = 0, 0, alignment[0]\n        for i, label in enumerate(alignment[1:], 1):\n            if label != seg_label:\n                if seg_label in self.target_labels:\n                    segments.append((seg_begin, i, seg_label))\n                    num_target_frames += i - seg_begin\n                seg_begin, seg_label = i, label\n        if seg_label in self.target_labels:\n            segments.append((seg_begin, len(alignment), seg_label))\n            num_target_frames += len(alignment) - seg_begin\n        stats.num_segments_initial = len(segments)\n        stats.num_segments_final = len(segments)\n        stats.initial_duration = num_target_frames * self.frame_shift\n        stats.final_duration = stats.initial_duration\n        return segments\n\n    def filter_short_segments(self, segments, stats):\n        """"""Filters out short segments.""""""\n        if self.min_segment_dur <= 0:\n            return segments\n        filtered_segments = []\n        for segment in segments:\n            dur = segment[1] - segment[0]\n            if dur < self.min_segment_dur:\n                stats.filter_short_duration += dur * self.frame_shift\n                stats.num_short_segments_filtered += 1\n            else:\n                filtered_segments.append(segment)\n        stats.num_segments_final = len(filtered_segments)\n        stats.final_duration -= stats.filter_short_duration\n        return filtered_segments\n\n    def pad_segments(self, segments, stats, num_utt_frames=None):\n        """"""Pads segments on both sides.\n\n        Ensures that the segments do not go beyond the neighboring segments or\n        utterance boundaries.\n        """"""\n        num_padded_frames, padded_segments = 0, []\n        for i, (seg_beg, seg_end, label) in enumerate(segments):\n            seg_beg -= self.segment_padding  # try padding on the left side\n            num_padded_frames += self.segment_padding\n            if seg_beg < 0:\n                # Padded segment start is before the beginning of the utterance.\n                # Reduce padding.\n                num_padded_frames += seg_beg\n                seg_beg = 0\n            if i >= 1 and prev_seg_end > seg_beg:\n                # Padded segment start is before the end of previous segment.\n                # Reduce padding.\n                num_padded_frames -= prev_seg_end - seg_beg\n                seg_beg = prev_seg_end\n            seg_end += self.segment_padding\n            num_padded_frames += self.segment_padding\n            if num_utt_frames is not None and seg_end > num_utt_frames:\n                # Padded segment end is beyond the max duration.\n                # Reduce padding.\n                num_padded_frames -= seg_end - num_utt_frames\n                seg_end = num_utt_frames\n            if i + 1 < len(segments) and seg_end > segments[i + 1][0]:\n                # Padded segment end is beyond the start of the next segment.\n                # Reduce padding.\n                num_padded_frames -= seg_end - segments[i + 1][0]\n                seg_end = segments[i + 1][0]\n            padded_segments.append((seg_beg, seg_end, label))\n            prev_seg_end = seg_end\n        stats.padding_duration = num_padded_frames * self.frame_shift\n        stats.final_duration += stats.padding_duration\n        return padded_segments\n\n    def merge_consecutive_segments(self, segments, stats):\n        """"""Merges consecutive segments.\n\n        Done after padding. Consecutive segments that share a boundary are\n        merged if they have the same label and the merged segment is no longer\n        than \'max_merged_segment_dur\'.\n        """"""\n        if self.max_merged_segment_dur <= 0 or not segments:\n            return segments\n\n        merged_segments = [segments[0]]\n        for seg_beg, seg_end, label in segments[1:]:\n            prev_seg_beg, prev_seg_end, prev_label = merged_segments[-1]\n            if (seg_beg == prev_seg_end and label == prev_label and\n                seg_end - prev_seg_beg <= self.max_merged_segment_dur):\n                merged_segments[-1] = (prev_seg_beg, seg_end, label)\n                stats.num_merges += 1\n            else:\n                merged_segments.append((seg_beg, seg_end, label))\n\n        stats.num_segments_final = len(merged_segments)\n        return merged_segments\n\n    def write(self, key, segments, file_handle):\n        """"""Writes segments to file""""""\n        for begin, end, label in segments:\n            id = ""{key}-{label}-{begin:07d}-{end:07d}"".format(\n                key=key, label=label, begin=begin, end=end)\n            print(""{id} {key} {begin:.2f} {end:.2f}"".format(\n                      id=id, key=key, begin=begin * self.frame_shift,\n                      end=end * self.frame_shift),\n                  file=file_handle)\n'"
tests/__init__.py,0,b''
tools/find_python_library.py,0,"b'#############################################################################\n#\n# Code reproduced from scikit-build (https://github.com/scikit-build/scikit-build), \n# Here is the license\n#\n# Copyright 2014 Mike Sarahan\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software\n# and associated documentation files (the ""Software""), to deal in the Software without \n# restriction, including without limitation the rights to use, copy, modify, merge, publish, \n# distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the \n# Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or \n# substantial portions of the Software.\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, \n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# \n#\n#############################################################################\n\nimport distutils.sysconfig as du_sysconfig\nimport itertools\nimport os\nimport sys\nimport sysconfig\n\ndef get_python_version():\n    """"""Get version associated with the current python interpreter.""""""\n    python_version = sysconfig.get_config_var(\'VERSION\')\n\n    if not python_version:\n        python_version = sysconfig.get_config_var(\'py_version_short\')\n\n    if not python_version:\n        python_version = ""."".join(map(str, sys.version_info[:2]))\n\n    return python_version\n\ndef get_python_library(python_version):\n    """"""Get path to the python library associated with the current python\n    interpreter.""""""\n    # determine direct path to libpython\n    python_library = sysconfig.get_config_var(\'LIBRARY\')\n\n    # if static (or nonexistent), try to find a suitable dynamic libpython\n    if (python_library is None or\n            os.path.splitext(python_library)[1][-2:] == \'.a\'):\n\n        candidate_lib_prefixes = [\'\', \'lib\']\n\n        candidate_extensions = [\'.lib\', \'.so\', \'.a\']\n        if sysconfig.get_config_var(\'WITH_DYLD\'):\n            candidate_extensions.insert(0, \'.dylib\')\n\n        candidate_versions = [python_version]\n        if python_version:\n            candidate_versions.append(\'\')\n            candidate_versions.insert(\n                0, """".join(python_version.split(""."")[:2]))\n\n        abiflags = getattr(sys, \'abiflags\', \'\')\n        candidate_abiflags = [abiflags]\n        if abiflags:\n            candidate_abiflags.append(\'\')\n\n        # Ensure the value injected by virtualenv is\n        # returned on windows.\n        # Because calling `sysconfig.get_config_var(\'multiarchsubdir\')`\n        # returns an empty string on Linux, `du_sysconfig` is only used to\n        # get the value of `LIBDIR`.\n        libdir = du_sysconfig.get_config_var(\'LIBDIR\')\n        if sysconfig.get_config_var(\'MULTIARCH\'):\n            masd = sysconfig.get_config_var(\'multiarchsubdir\')\n            if masd:\n                if masd.startswith(os.sep):\n                    masd = masd[len(os.sep):]\n                libdir = os.path.join(libdir, masd)\n\n        if libdir is None:\n            libdir = os.path.abspath(os.path.join(\n                sysconfig.get_config_var(\'LIBDEST\'), "".."", ""libs""))\n\n        candidates = (\n            os.path.join(\n                libdir,\n                \'\'.join((pre, \'python\', ver, abi, ext))\n            )\n            for (pre, ext, ver, abi) in itertools.product(\n                candidate_lib_prefixes,\n                candidate_extensions,\n                candidate_versions,\n                candidate_abiflags\n            )\n        )\n\n        for candidate in candidates:\n            if os.path.exists(candidate):\n                # we found a (likely alternate) libpython\n                python_library = candidate\n                break\n\n    # TODO(opadron): what happens if we don\'t find a libpython?\n\n    return python_library\n\npython_version = get_python_version()\npython_library = get_python_library(python_version)\n\nprint(python_library)\n'"
kaldi/base/__init__.py,0,"b""from ._kaldi_error import *\nfrom ._timer import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/base/io.py,0,"b""from ._iostream import *\nfrom ._fstream import *\nfrom ._sstream import *\nfrom ._io_funcs import *\nfrom ._io_funcs_ext import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/base/math.py,0,"b'""""""\n.. autoconstant:: DBL_EPSILON\n.. autoconstant:: FLT_EPSILON\n.. autoconstant:: M_PI\n.. autoconstant:: M_SQRT2\n.. autoconstant:: M_2PI\n.. autoconstant:: M_SQRT1_2\n.. autoconstant:: M_LOG_2PI\n.. autoconstant:: M_LN2\n.. autoconstant:: M_LN10\n.. autoconstant:: LOG_ZERO_FLOAT\n.. autoconstant:: LOG_ZERO_DOUBLE\n.. autoconstant:: MIN_LOG_DIFF_DOUBLE\n.. autoconstant:: MIN_LOG_DIFF_FLOAT\n""""""\n\nfrom ._kaldi_math import *\nfrom ._kaldi_math_ext import *\n\n# Must be imported explicitly\nfrom ._kaldi_math import (_lcm, _factorize, _with_prob,\n                          _round_up_to_nearest_power_of_two, _rand_int)\nfrom ._kaldi_math_ext import _log_zero_float, _log_zero_double\n\nDBL_EPSILON = 2.2204460492503131e-16\n\nFLT_EPSILON = 1.19209290e-7\n\nM_PI = 3.1415926535897932384626433832795\n\nM_SQRT2 = 1.4142135623730950488016887\n\nM_2PI = 6.283185307179586476925286766559005\n\nM_SQRT1_2 = 0.7071067811865475244008443621048490\n\nM_LOG_2PI = 1.8378770664093454835606594728112\n\nM_LN2 = 0.693147180559945309417232121458\n\nM_LN10 = 2.302585092994045684017991454684\n\nLOG_ZERO_FLOAT = _log_zero_float()\nLOG_ZERO_DOUBLE = _log_zero_double()\nMIN_LOG_DIFF_DOUBLE = log(DBL_EPSILON)\nMIN_LOG_DIFF_FLOAT = log(FLT_EPSILON)\n\ndef lcm(x, y):\n    """"""Returns the least common multiple for x and y.\n\n    Args:\n        x (int): first positive integer\n        y (int): second positive integer\n\n    Raises:\n        ValueError if x <= 0 or y <= 0\n    """"""\n    if x <= 0 or y <= 0:\n        raise ValueError(""lcm parameters must be positive integers."")\n    return _lcm(x, y)\n\ndef factorize(x):\n    """"""Splits a positive integer into its prime factors.\n\n    Args:\n        x (int): positive integer\n\n    Returns:\n        List[int]: Prime factors in increasing order, with duplication.\n\n    Raises:\n        ValueError if x <= 0\n    """"""\n    if x <= 0:\n        raise ValueError(""Parameter x must be a positive integer."")\n    return _factorize(x)\n\ndef with_prob(prob):\n    """"""Returns `True` with probability `prob`.\n\n    Args:\n        prob (float): probability of True, 0 <= prob <= 1\n\n    Raises:\n        ValueError: If `prob` is negative or greater than 1.0.\n    """"""\n    if 0.0 <= prob <= 1.0:\n        return _with_prob(prob)\n\n    raise ValueError(""prob should be in the interval [0.0, 1.0]"")\n\ndef round_up_to_nearest_power_of_two(n):\n    """"""Rounds input integer up to the nearest power of 2.\n\n    Args:\n        n (int): Positive integer\n\n    Raises:\n        ValueError if n <= 0\n    """"""\n    if n <= 0:\n        raise ValueError(""n should be a positive integer"")\n    return _round_up_to_nearest_power_of_two(n)\n\ndef rand_int(first, last, state = None):\n    """"""Returns a random integer in the given interval.\n\n    Args:\n        first (int): Lower bound\n        last (int): Upper bound (inclusive)\n        state (RandomState or None): RNG state\n\n    Raises:\n        ValueError if first >= last\n    """"""\n    if first >= last:\n        raise ValueError(""first must be smaller than last."")\n    return _rand_int(first, last, state)\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/chain/__init__.py,0,"b""from ._chain_datastruct import *\nfrom ._chain_den_graph import *\nfrom ._chain_training import *\nfrom ._chain_training_ext import *\nfrom ._chain_denominator import *\nfrom ._chain_supervision import *\nfrom ._chain_numerator import *\nfrom ._chain_generic_numerator import *\nfrom ._language_model import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/cudamatrix/__init__.py,0,"b'try:\n    from ._cu_device import *\n    def cuda_available():\n        """"""Check if cuda is available.""""""\n        return True\nexcept ImportError:\n    def cuda_available():\n        """"""Check if cuda is available.""""""\n        return False\n\nfrom ._cu_matrixdim import *\nfrom ._cu_array import *\nfrom ._cu_vector import *\nfrom ._cu_matrix import *\nfrom ._cu_sparse_matrix import *\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/decoder/__init__.py,0,"b""from ._grammar_fst import *\nfrom ._decodable_matrix import *\nfrom ._decodable_mapped import *\nfrom ._decodable_sum import *\nfrom ._decoder import *\nfrom ._compiler import *\n\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/decoder/_compiler.py,0,"b'from .. import fstext as _fst\n\nfrom ._training_graph_compiler import *\nfrom ._training_graph_compiler_ext import *\n\n\nclass TrainingGraphCompiler(TrainingGraphCompiler):\n    """"""Training graph compiler.""""""\n    def __init__(self, trans_model, ctx_dep, lex_fst, disambig_syms, opts):\n        """"""\n        Args:\n            trans_model (TransitionModel): Transition model `H`.\n            ctx_dep (ContextDependency): Context dependency model `C`.\n            lex_fst (StdVectorFst): Lexicon `L`.\n            disambig_syms (List[int]): Disambiguation symbols.\n            opts (TrainingGraphCompilerOptions): Compiler options.\n        """"""\n        super(TrainingGraphCompiler, self).__init__(\n            trans_model, ctx_dep, lex_fst, disambig_syms, opts)\n        # keep references to these objects to keep them in scope\n        self._trans_model = trans_model\n        self._ctx_dep = ctx_dep\n        self._lex_fst = lex_fst\n\n    def compile_graph(self, word_fst):\n        """"""Compiles a single training graph from a weighted acceptor.\n\n        Args:\n            word_fst (StdVectorFst): Weighted acceptor `G` at the word level.\n\n        Returns:\n            StdVectorFst: The training graph `HCLG`.\n        """"""\n        ofst = super(TrainingGraphCompiler, self).compile_graph(word_fst)\n        return _fst.StdVectorFst(ofst)\n\n    def compile_graphs(self, word_fsts):\n        """"""Compiles training graphs from weighted acceptors.\n\n        Args:\n          word_fsts (List[StdVectorFst]): Weighted acceptors at the word level.\n\n        Returns:\n          List[StdVectorFst]: The training graphs.\n        """"""\n        ofsts = super(TrainingGraphCompiler, self).compile_graphs(word_fsts)\n        for i, fst in enumerate(ofsts):\n            ofsts[i] = _fst.StdVectorFst(fst)\n        return ofsts\n\n    def compile_graph_from_text(self, transcript):\n        """"""Compiles a single training graph from a transcript.\n\n        Args:\n            transcript (List[int]): The input transcript.\n\n        Returns:\n            StdVectorFst: The training graph `HCLG`.\n        """"""\n        ofst = super(TrainingGraphCompiler,\n                     self).compile_graph_from_text(transcript)\n        return _fst.StdVectorFst(ofst)\n\n    def compile_graphs_from_text(self, transcripts):\n        """"""Compiles training graphs from transcripts.\n\n        Args:\n          transcripts (List[List[int]]): The input transcripts.\n\n        Returns:\n          List[StdVectorFst]: The training graphs.\n        """"""\n        ofsts = super(TrainingGraphCompiler,\n                      self).compile_graphs_from_text(transcripts)\n        for i, fst in enumerate(ofsts):\n            ofsts[i] = _fst.StdVectorFst(fst)\n        return ofsts\n\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/decoder/_decoder.py,0,"b'from .. import fstext as _fst\nfrom .. import lat as _lat\n\nfrom ._faster_decoder import *\nfrom ._biglm_faster_decoder import *\nfrom ._lattice_faster_decoder import *\nfrom ._lattice_faster_decoder_ext import *\nfrom ._lattice_biglm_faster_decoder import *\nfrom ._lattice_faster_online_decoder import *\nfrom ._lattice_faster_online_decoder_ext import *\n\n\nclass _DecoderBase(object):\n    """"""Base class defining the Python API for decoders.""""""\n\n    def get_best_path(self, use_final_probs=True):\n        """"""Gets best path as a lattice.\n\n        Args:\n            use_final_probs (bool): If ``True`` and a final state of the graph\n                is reached, then the output will include final probabilities\n                given by the graph. Otherwise all final probabilities are\n                treated as one.\n\n        Returns:\n            LatticeVectorFst: The best path.\n\n        Raises:\n            RuntimeError: In the unusual circumstances where no tokens survive.\n        """"""\n        ofst = _fst.LatticeVectorFst()\n        success = self._get_best_path(ofst, use_final_probs)\n        if not success:\n            raise RuntimeError(""Decoding failed. No tokens survived."")\n        return ofst\n\n\nclass _LatticeDecoderBase(_DecoderBase):\n    """"""Base class defining the Python API for lattice generating decoders.""""""\n\n    def get_raw_lattice(self, use_final_probs=True):\n        """"""Gets raw state-level lattice.\n\n        The output raw lattice will be topologically sorted.\n\n        Args:\n            use_final_probs (bool): If ``True`` and a final state of the graph\n                is reached, then the output will include final probabilities\n                given by the graph. Otherwise all final probabilities are\n                treated as one.\n\n        Returns:\n            LatticeVectorFst: The state-level lattice.\n\n        Raises:\n            RuntimeError: In the unusual circumstances where no tokens survive.\n        """"""\n        ofst = _fst.LatticeVectorFst()\n        success = self._get_raw_lattice(ofst, use_final_probs)\n        if not success:\n            raise RuntimeError(""Decoding failed. No tokens survived."")\n        return ofst\n\n    def get_lattice(self, use_final_probs=True):\n        """"""Gets the lattice-determinized compact lattice.\n\n        The output is a deterministic compact lattice with a unique path for\n        each word sequence.\n\n        Args:\n            use_final_probs (bool): If ``True`` and a final state of the graph\n                is reached, then the output will include final probabilities\n                given by the graph. Otherwise all final probabilities are\n                treated as one.\n\n        Returns:\n            CompactLatticeVectorFst: The lattice-determinized compact lattice.\n\n        Raises:\n            RuntimeError: In the unusual circumstances where no tokens survive.\n        """"""\n        ofst = _fst.CompactLatticeVectorFst()\n        success = self._get_lattice(ofst, use_final_probs)\n        if not success:\n            raise RuntimeError(""Decoding failed. No tokens survived."")\n        return ofst\n\n\nclass _LatticeOnlineDecoderBase(_LatticeDecoderBase):\n    """"""Base class defining the Python API for lattice generating online decoders.""""""\n\n    def get_raw_lattice_pruned(self, beam, use_final_probs=True):\n        """"""Prunes and returns raw state-level lattice.\n\n        Behaves like :meth:`get_raw_lattice` but only processes tokens whose\n        extra-cost is smaller than the best-cost plus the specified beam. It is\n        worthwhile to call this function only if :attr:`beam` is less than the\n        lattice-beam specified in the decoder options. Otherwise, it returns\n        essentially the same thing as :meth:`get_raw_lattice`, but more slowly.\n\n        The output raw lattice will be topologically sorted.\n\n        Args:\n            beam (float): Pruning beam.\n            use_final_probs (bool): If ``True`` and a final state of the graph\n                is reached, then the output will include final probabilities\n                given by the graph. Otherwise all final probabilities are\n                treated as one.\n\n        Returns:\n            LatticeVectorFst: The state-level lattice.\n\n        Raises:\n            RuntimeError: In the unusual circumstances where no tokens survive.\n        """"""\n        ofst = _fst.LatticeVectorFst()\n        success = self._get_raw_lattice_pruned(ofst, use_final_probs, beam)\n        if not success:\n            raise RuntimeError(""Decoding failed. No tokens survived."")\n        return ofst\n\n\nclass FasterDecoder(_DecoderBase, FasterDecoder):\n    """"""Faster decoder.\n\n    Args:\n        fst (StdFst): Decoding graph `HCLG`.\n        opts (FasterDecoderOptions): Decoder options.\n    """"""\n    def __init__(self, fst, opts):\n        super(FasterDecoder, self).__init__(fst, opts)\n        self._fst = fst  # keep a reference to FST to keep it in scope\n\n\nclass BiglmFasterDecoder(_DecoderBase, BiglmFasterDecoder):\n    """"""Faster decoder for decoding with big language models.\n\n    This is as :class:`LatticeFasterDecoder`, but does online composition\n    between decoding graph :attr:`fst` and the difference language model\n    :attr:`lm_diff_fst`.\n\n    Args:\n        fst (StdFst): Decoding graph.\n        opts (BiglmFasterDecoderOptions): Decoder options.\n        lm_diff_fst (StdDeterministicOnDemandFst): The deterministic on-demand\n            FST representing the difference in scores between the LM to decode\n            with and the LM the decoding graph :attr:`fst` was compiled with.\n    """"""\n    def __init__(self, fst, opts, lm_diff_fst):\n        super(BiglmFasterDecoder, self).__init__(fst, opts, lm_diff_fst)\n        self._fst = fst                  # keep references to FSTs\n        self._lm_diff_fst = lm_diff_fst  # to keep them in scope\n\nclass LatticeFasterDecoder(_LatticeDecoderBase, LatticeFasterDecoder):\n    """"""Lattice generating faster decoder.\n\n    Args:\n        fst (StdFst): Decoding graph `HCLG`.\n        opts (LatticeFasterDecoderOptions): Decoder options.\n    """"""\n    def __init__(self, fst, opts):\n        super(LatticeFasterDecoder, self).__init__(fst, opts)\n        self._fst = fst  # keep a reference to FST to keep it in scope\n\nclass LatticeFasterGrammarDecoder(_LatticeDecoderBase,\n                                  LatticeFasterGrammarDecoder):\n    """"""Lattice generating faster grammar decoder.\n\n    Args:\n        fst (GrammarFst): Decoding graph `HCLG`.\n        opts (LatticeFasterDecoderOptions): Decoder options.\n    """"""\n    def __init__(self, fst, opts):\n        super(LatticeFasterGrammarDecoder, self).__init__(fst, opts)\n        self._fst = fst  # keep a reference to FST to keep it in scope\n\nclass LatticeBiglmFasterDecoder(_LatticeDecoderBase, LatticeBiglmFasterDecoder):\n    """"""Lattice generating faster decoder for decoding with big language models.\n\n    This is as :class:`LatticeFasterDecoder`, but does online composition\n    between decoding graph :attr:`fst` and the difference language model\n    :attr:`lm_diff_fst`.\n\n    Args:\n        fst (StdFst): Decoding graph `HCLG`.\n        opts (LatticeFasterDecoderOptions): Decoder options.\n        lm_diff_fst (StdDeterministicOnDemandFst): The deterministic on-demand\n            FST representing the difference in scores between the LM to decode\n            with and the LM the decoding graph :attr:`fst` was compiled with.\n    """"""\n    def __init__(self, fst, opts, lm_diff_fst):\n        super(LatticeBiglmFasterDecoder, self).__init__(fst, opts, lm_diff_fst)\n        self._fst = fst                  # keep references to FSTs\n        self._lm_diff_fst = lm_diff_fst  # to keep them in scope\n\n\nclass LatticeFasterOnlineDecoder(_LatticeOnlineDecoderBase,\n                                 LatticeFasterOnlineDecoder):\n    """"""Lattice generating faster online decoder.\n\n    Similar to :class:`LatticeFasterDecoder` but computes the best path\n    without generating the entire raw lattice and finding the best path\n    through it. Instead, it traces back through the lattice.\n\n    Args:\n        fst (StdFst): Decoding graph `HCLG`.\n        opts (LatticeFasterDecoderOptions): Decoder options.\n    """"""\n    def __init__(self, fst, opts):\n        super(LatticeFasterOnlineDecoder, self).__init__(fst, opts)\n        self._fst = fst  # keep a reference to FST to keep it in scope\n\n    # This method is missing from the C++ class so we implement it here.\n    def _get_lattice(self, use_final_probs=True):\n        raw_fst = self.get_raw_lattice(use_final_probs).invert().arcsort()\n        lat_opts = _lat.DeterminizeLatticePrunedOptions()\n        config = self.get_options()\n        lat_opts.max_mem = config.det_opts.max_mem\n        ofst = _fst.CompactLatticeVectorFst()\n        _lat.determinize_lattice_pruned(raw_fst, config.lattice_beam,\n                                        ofst, lat_opts)\n        ofst.connect()\n        if ofst.num_states() == 0:\n            raise RuntimeError(""Decoding failed. No tokens survived."")\n        return ofst\n\n\nclass LatticeFasterOnlineGrammarDecoder(_LatticeOnlineDecoderBase,\n                                        LatticeFasterOnlineGrammarDecoder):\n    """"""Lattice generating faster online grammar decoder.\n\n    Similar to :class:`LatticeFasterGrammarDecoder` but computes the best path\n    without generating the entire raw lattice and finding the best path\n    through it. Instead, it traces back through the lattice.\n\n    Args:\n        fst (GrammarFst): Decoding graph `HCLG`.\n        opts (LatticeFasterDecoderOptions): Decoder options.\n    """"""\n    def __init__(self, fst, opts):\n        super(LatticeFasterOnlineGrammarDecoder, self).__init__(fst, opts)\n        self._fst = fst  # keep a reference to FST to keep it in scope\n\n    # This method is missing from the C++ class so we implement it here.\n    def _get_lattice(self, use_final_probs=True):\n        raw_fst = self.get_raw_lattice(use_final_probs).invert().arcsort()\n        lat_opts = _lat.DeterminizeLatticePrunedOptions()\n        config = self.get_options()\n        lat_opts.max_mem = config.det_opts.max_mem\n        ofst = _fst.CompactLatticeVectorFst()\n        _lat.determinize_lattice_pruned(raw_fst, config.lattice_beam,\n                                        ofst, lat_opts)\n        ofst.connect()\n        if ofst.num_states() == 0:\n            raise RuntimeError(""Decoding failed. No tokens survived."")\n        return ofst\n\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/feat/__init__.py,0,"b""from . import fbank\nfrom . import functions\nfrom . import mel\nfrom . import mfcc\nfrom . import online\nfrom . import pitch\nfrom . import plp\nfrom . import signal\nfrom . import spectrogram\nfrom . import wave\nfrom . import window\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/fbank.py,0,"b""from ._feature_fbank import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/functions.py,0,"b""from ._feature_functions import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/mel.py,0,"b""from ._mel_computations import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/mfcc.py,0,"b""from ._feature_mfcc import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/online.py,0,"b""from ._online_feature import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/pitch.py,0,"b""from ._pitch_functions import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/plp.py,0,"b""from ._feature_plp import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/signal.py,0,"b""from ._resample import *\nfrom ._signal import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/spectrogram.py,0,"b""from ._feature_spectrogram import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/feat/wave.py,0,"b'""""""\n.. autoconstant:: WAVE_SAMPLE_MAX\n""""""\n\nfrom ._wave_reader import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/feat/window.py,0,"b""from ._feature_window import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/fstext/__init__.py,0,"b'""""""\nPyKaldi has built-in support for common FST types (including Kaldi lattices and\nKWS index) and operations. The API for the user facing PyKaldi FST types and\noperations is mostly defined in Python mimicking the API exposed by OpenFst\'s\nofficial Python wrapper `pywrapfst\n<http://www.openfst.org/twiki/bin/view/FST/PythonExtension>`_ to a large extent.\nThis includes integrations with Graphviz and IPython for interactive\nvisualization of FSTs.\n\nThere are two major differences between the PyKaldi FST package and pywrapfst:\n\n#. PyKaldi bindings are generated with CLIF while pywrapfst bindings are\n   generated with Cython. This allows PyKaldi FST types to work seamlessly with\n   the rest of the PyKaldi package.\n\n#. In contrast to pywrapfst, PyKaldi does not wrap OpenFst scripting API, which\n   uses virtual dispatch, function registration, and dynamic loading of shared\n   objects to provide a common interface shared by FSTs of different semirings.\n   While this change requires wrapping each semiring specialization separately\n   in PyKaldi, it gives users the ability to pass FST objects directly to the\n   myriad PyKaldi functions accepting FST arguments.\n\nOperations which construct new FSTs are implemented as traditional functions, as\nare two-argument boolean functions like `equal` and `equivalent`. Convert\noperation is not implemented as a separate function since FSTs already support\nconstruction from other FST types, e.g. vector FSTs can be constructed from\nconstant FSTs and vice versa. Destructive operations---those that mutate an FST,\nin place---are instance methods, as is `write`.\n\nThe following example, based on `Mohri et al. 2002`_, shows the construction of\nan ASR graph given a pronunciation lexicon L, grammar G, a transducer from\ncontext-dependent phones to context-independent phones C, and an HMM set H::\n\n    import kaldi.fstext as fst\n\n    L = fst.StdVectorFst.read(""L.fst"")\n    G = fst.StdVectorFst.read(""G.fst"")\n    C = fst.StdVectorFst.read(""C.fst"")\n    H = fst.StdVectorFst.read(""H.fst"")\n    LG = fst.determinize(fst.compose(L, G))\n    CLG = fst.determinize(fst.compose(C, LG))\n    HCLG = fst.determinize(fst.compose(H, CLG))\n    HCLG.minimize()                                      # NB: works in-place.\n\n.. _`Mohri et al. 2002`:\n   http://www.openfst.org/twiki/pub/FST/FstBackground/csl01.pdf\n.. autoconstant:: NO_STATE_ID\n.. autoconstant:: NO_LABEL\n.. autoconstant:: ENCODE_FLAGS\n.. autoconstant:: ENCODE_LABELS\n.. autoconstant:: ENCODE_WEIGHTS\n""""""\n\nfrom ._api import *\n\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/fstext/_api.py,0,"b'# The Python API was largely adapted from the official OpenFst Python wrapper.\n# See www.openfst.org for additional documentation.\n\nimport logging as _logging\nimport os as _os\nimport subprocess as _subprocess\nimport time as _time\n\nfrom ..base import io as _base_io\nfrom ..util import io as _util_io\n\nimport _getters                      # Relative/absolute import of _getters and\nimport _weight                       # _weight modules is buggy in Python 3.\nfrom . import _symbol_table\nfrom . import _float_weight\nfrom . import _lattice_weight\nfrom . import _lexicographic_weight\nfrom . import _arc\nfrom . import _compiler\nfrom . import _encode\nfrom . import _fst\nfrom . import _vector_fst\nfrom . import _const_fst\nfrom . import _fstext_shims\nfrom . import _drawer\nfrom . import _printer\nfrom . import _std_ops\nfrom . import _log_ops\nfrom . import _lat_ops\nfrom . import _clat_ops\nfrom . import _index_ops\nfrom . import properties as _props\n\nfrom ._symbol_table import *\nfrom ._encode import *\nfrom ._fst import FstHeader, FstReadOptions, FstWriteOptions\nfrom ._fst import NO_STATE_ID, NO_LABEL\n\n\n# Helpers\n\ndef _get_weight_or_default(weight_factory, weight=None, default_one=True):\n    """"""Converts weight to an instance of the weight type.\n\n    If weight is None, the weight is set to:\n    * semiring one when default_one is True,\n    * semiring zero when default_one is False.\n    """"""\n    if weight is None:\n        return weight_factory.one() if default_one else weight_factory.zero()\n    if isinstance(weight, weight_factory.__bases__):\n        return weight\n    return weight_factory(weight)\n\n\n# Encoder API\n\nclass _EncodeMapper(object):\n    """"""Arc encoder.""""""\n\n    def __init__(self, encode_labels=False, encode_weights=False, encode=True):\n        """"""\n        This class provides an object which can be used to encode or decode FST\n        arcs. This is most useful to convert an FST to an unweighted acceptor,\n        on which some FST operations are more efficient, and then decoding the\n        FST afterwards.\n\n        To use an instance of this class to encode or decode a mutable FST, pass\n        it as the first argument to the FST instance methods `encode` and\n        `decode`. Alternatively, an instance of this class can be used as a\n        callable to encode/decode arcs.\n\n        Args:\n            encode_labels (bool): Should labels be encoded?\n            encode_weights (bool): Should weights be encoded?\n            encode (bool): Encode or decode?\n        """"""\n        flags = _getters.GetEncodeFlags(encode_labels, encode_weights)\n        if encode:\n            encoder_type = _getters.EncodeType.ENCODE\n        else:\n            encoder_type = _getters.EncodeType.DECODE\n        super(_EncodeMapper, self).__init__(flags, encoder_type)\n\n\n# Compiler API\n\nclass _FstCompiler(object):\n    """"""Class used to compile FSTs from strings.""""""\n\n    def __init__(self, isymbols=None, osymbols=None, ssymbols=None,\n                 acceptor=False, keep_isymbols=False, keep_osymbols=False,\n                 keep_state_numbering=False, allow_negative_labels=False):\n        """"""\n        This class is used to compile FSTs specified using the AT&T FSM library\n        format described here:\n\n        http://web.eecs.umich.edu/~radev/NLP-fall2015/resources/fsm_archive/fsm.5.html\n\n        This is the same format used by the `fstcompile` executable.\n\n        FstCompiler options (symbol tables, etc.) are set at construction time::\n\n            compiler = FstCompiler(isymbols=ascii_syms, osymbols=ascii_syms)\n\n        Once constructed, FstCompiler instances behave like a file handle opened\n        for writing::\n\n            # /ba+/\n            print(""0 1 50 50"", file=compiler)\n            print(""1 2 49 49"", file=compiler)\n            print(""2 2 49 49"", file=compiler)\n            print(""2"", file=compiler)\n\n        The `compile` method returns an actual FST instance::\n\n            sheep_machine = compiler.compile()\n\n        Compilation flushes the internal buffer, so the compiler instance can be\n        reused to compile new machines with the same symbol tables, etc.\n\n        Args:\n            isymbols: An optional SymbolTable used to label input symbols.\n            osymbols: An optional SymbolTable used to label output symbols.\n            ssymbols: An optional SymbolTable used to label states.\n            acceptor: Should the FST be rendered in acceptor format if possible?\n            keep_isymbols: Should the input symbol table be stored in the FST?\n            keep_osymbols: Should the output symbol table be stored in the FST?\n            keep_state_numbering: Should the state numbering be preserved?\n            allow_negative_labels: Should negative labels be allowed? (Not\n                recommended; may cause conflicts).\n        """"""\n\n        self._strbuf = """"\n        self._isymbols = isymbols\n        self._osymbols = osymbols\n        self._ssymbols = ssymbols\n        self._acceptor = acceptor\n        self._keep_isymbols = keep_isymbols\n        self._keep_osymbols = keep_osymbols\n        self._keep_state_numbering = keep_state_numbering\n        self._allow_negative_labels = allow_negative_labels\n\n    def compile(self):\n        """"""\n        Compiles the FST in the string buffer.\n\n        This method compiles the FST and returns the resulting machine.\n\n        Returns:\n            The FST described by the string buffer.\n\n        Raises:\n            RuntimeError: Compilation failed.\n        """"""\n        sstrm = _base_io.stringstream.from_str(self._strbuf)\n        compiler = self._compiler_type(\n            sstrm, ""compile"",\n            self._isymbols, self._osymbols, self._ssymbols,\n            self._acceptor, self._keep_isymbols, self._keep_osymbols,\n            self._keep_state_numbering, self._allow_negative_labels)\n        ofst = compiler.fst()\n        self._strbuf = """"\n        if ofst is None:\n            raise RuntimeError(""Compilation failed"")\n        return ofst\n\n    def write(self, expression):\n        """"""\n        Writes a string into the compiler string buffer.\n\n        This method adds a line to the compiler string buffer. It can also be\n        invoked with a print call, like so::\n\n            compiler = FstCompiler()\n            print(""0 0 49 49"", file=compiler)\n            print(""0"", file=compiler)\n\n        Args:\n            expression: A string expression to add to compiler string buffer.\n        """"""\n        self._strbuf += expression\n\n\n# Drawer API\n\nclass _FstDrawer(object):\n    """"""Base class defining the Python API for FST drawers.""""""\n    def __init__(self, fst, isyms, osyms, ssyms, accep, title, width, height,\n                 portrait, vertical, ranksep, nodesep, fontsize, precision,\n                 float_format, show_weight_one):\n        super(_FstDrawer, self).__init__(\n            fst, isyms, osyms, ssyms, accep, title, width, height,\n            portrait, vertical, ranksep, nodesep, fontsize, precision,\n            float_format, show_weight_one)\n        # Keep references to these to keep them in scope\n        self._fst = fst\n        self._isyms = isyms\n        self._osyms = osyms\n        self._ssyms = ssyms\n\n\n# Printer API\n\nclass _FstPrinter(object):\n    """"""Base class defining the Python API for FST printers.""""""\n    def __init__(self, fst, isyms, osyms, ssyms, accep, show_weight_one,\n                 field_separator, missing_symbol=""""):\n        super(_FstPrinter, self).__init__(\n            fst, isyms, osyms, ssyms, accep, show_weight_one,\n            field_separator, missing_symbol)\n        # Keep references to these to keep them in scope\n        self._fst = fst\n        self._isyms = isyms\n        self._osyms = osyms\n        self._ssyms = ssyms\n\n\n# FST API\n\nclass _FstBase(object):\n    """"""Base class defining the Python API for FST types.""""""\n\n    def _repr_svg_(self):\n        """"""IPython notebook magic to produce an SVG of the FST using GraphViz.\n\n        This method produces an SVG of the internal graph. Users wishing to\n        create publication-quality graphs should instead use the method `draw`,\n        which exposes additional parameters.\n\n        Raises:\n          RuntimeError: Cannot locate the `dot` executable.\n          subprocess.CalledProcessError: `dot` returned non-zero exit code.\n\n        See also: `draw`, `text`.\n        """"""\n        try:\n            # Throws OSError if the dot executable is not found.\n            proc = _subprocess.Popen([""dot"", ""-Tsvg""],\n                                     stdin=_subprocess.PIPE,\n                                     stdout=_subprocess.PIPE,\n                                     stderr=_subprocess.PIPE)\n        except OSError:\n            raise RuntimeError(""Failed to execute \'dot -Tsvg\', make sure ""\n                               ""the Graphviz executable \'dot\' is on your PATH."")\n        sstrm = _base_io.ostringstream()\n        fstdrawer = self._drawer_type(\n            self, self._input_symbols(), self._output_symbols(), None,\n            self._properties(_props.ACCEPTOR, True) == _props.ACCEPTOR,\n            """", 8.5, 11, True, False, 0.4, 0.25, 14, 5, ""g"", False)\n        fstdrawer.draw(sstrm, ""_repr_svg"")\n        (sout, serr) = proc.communicate(sstrm.to_bytes())\n        if proc.returncode != 0:  # Just to be explicit.\n            raise _subprocess.CalledProcessError(proc.returncode, ""dot -Tsvg"")\n        return sout.decode(""utf8"")\n\n    def __str__(self):\n        return self.text(\n            acceptor=self._properties(_props.ACCEPTOR, True) == _props.ACCEPTOR,\n            show_weight_one=self._properties(_props.WEIGHTED, True) == _props.WEIGHTED)\n\n    def _valid_state_id(self, s):\n        if not self._properties(_props.EXPANDED, True):\n            _logging.error(""Cannot get number of states for unexpanded FST"")\n            return False\n        if s < 0 or s >= self._ops.count_states(self):\n            _logging.error(""State id {} not valid"".format(s))\n            return False\n        return True\n\n    def arcs(self, state):\n        """"""\n        Returns an iterator over arcs leaving the specified state.\n\n        Args:\n          state: The source state index.\n\n        Returns:\n          An ArcIterator.\n\n        See also: `mutable_arcs`, `states`.\n        """"""\n        return self._arc_iterator_type(self, state)\n\n    def copy(self):\n        """"""Makes a copy of the FST.\n\n        Returns:\n            A copy of the FST.\n        """"""\n        return self._copy()\n\n    def draw(self, filename, isymbols=None, osymbols=None, ssymbols=None,\n             acceptor=False, title="""", width=8.5, height=11, portrait=False,\n             vertical=False, ranksep=0.4, nodesep=0.25, fontsize=14,\n             precision=5, float_format=""g"", show_weight_one=False):\n        """"""\n        Writes out the FST in Graphviz text format.\n\n        This method writes out the FST in the dot graph description language.\n        The graph can be rendered using the `dot` binary provided by Graphviz.\n\n        Args:\n          filename (str): The string location of the output dot/Graphviz file.\n          isymbols: An optional symbol table used to label input symbols.\n          osymbols: An optional symbol table used to label output symbols.\n          ssymbols: An optional symbol table used to label states.\n          acceptor (bool): Should the figure be rendered in acceptor format if\n            possible? Defaults False.\n          title (str): An optional string indicating the figure title. Defaults\n            to empty string.\n          width (float): The figure width, in inches. Defaults 8.5\'\'.\n          height (float): The figure height, in inches. Defaults 11\'\'.\n          portrait (bool): Should the figure be rendered in portrait rather than\n            landscape? Defaults False.\n          vertical (bool): Should the figure be rendered bottom-to-top rather\n            than left-to-right?\n          ranksep (float): The minimum separation separation between ranks,\n            in inches. Defaults 0.4\'\'.\n          nodesep (float): The minimum separation between nodes, in inches.\n            Defaults 0.25\'\'.\n          fontsize (int): Font size, in points. Defaults 14pt.\n          precision (int): Numeric precision for floats, in number of chars.\n            Defaults to 5.\n          float_format (\'e\', \'f\' or \'g\'): One of: \'e\', \'f\' or \'g\'.\n            Defaults to \'g\'\n          show_weight_one (bool): Should weights equivalent to semiring One be\n              printed? Defaults False.\n\n        For more information about the rendering options, see `man dot`.\n\n        See also: `text`.\n        """"""\n        if isymbols is None:\n            isymbols = self._input_symbols()\n        if osymbols is None:\n            osymbols = self._output_symbols()\n        ostrm = _base_io.ofstream.from_file(filename)\n        fstdrawer = self._drawer_type(\n            self, isymbols, osymbols, ssymbols,\n            acceptor, title, width, height, portrait, vertical, ranksep,\n            nodesep, fontsize, precision, float_format, show_weight_one)\n        fstdrawer.draw(ostrm, filename)\n\n    def final(self, state):\n        """"""\n        Returns the final weight of a state.\n\n        Args:\n          state: The integer index of a state.\n\n        Returns:\n          The final Weight of that state.\n\n        Raises:\n          IndexError: State index out of range.\n        """"""\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        return self._final(state)\n\n    @classmethod\n    def from_bytes(cls, s):\n        """"""Returns the FST represented by the bytes object.\n\n        Args:\n            s (bytes): The bytes object representing the FST.\n\n        Returns:\n            An FST object.\n        """"""\n        return cls(cls._ops.from_bytes(s))\n\n    def input_symbols(self):\n        """"""\n        Returns the input symbol table.\n\n        Returns:\n          The input symbol table.\n\n        See Also: :meth:`output_symbols`.\n        """"""\n        return self._input_symbols()\n\n    def num_arcs(self, state=None):\n        """"""\n        Returns the number of arcs, counting them if necessary.\n\n        If state is ``None``, returns the number of arcs in the FST. Otherwise,\n        returns the number of arcs leaving that state.\n\n        Args:\n          state: The integer index of a state. Defaults to ``None``.\n\n        Returns:\n          The number of arcs leaving a state or the number of arcs in the FST.\n\n        Note:\n        This method counts the number of arcs in the FST by iterating over the\n        states and summing up the number of arcs leaving each state.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `num_states`.\n        """"""\n        if state is None:\n            return self._ops.count_arcs(self)\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        return self._num_arcs(state)\n\n    def num_input_epsilons(self, state):\n        """"""\n        Returns the number of arcs with epsilon input labels leaving a state.\n\n        Args:\n          state: The integer index of a state.\n\n        Returns:\n          The number of epsilon-input-labeled arcs leaving that state.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `num_output_epsilons`.\n        """"""\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        return self._num_input_epsilons(state)\n\n    def num_output_epsilons(self, state):\n        """"""\n        Returns the number of arcs with epsilon output labels leaving a state.\n\n        Args:\n          state: The integer index of a state.\n\n        Returns:\n          The number of epsilon-output-labeled arcs leaving that state.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `num_input_epsilons`.\n        """"""\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        return self._num_output_epsilons(state)\n\n    def num_states(self):\n        """"""\n        Returns the number of states, counting them if necessary.\n\n        Returns:\n          The number of states.\n\n        See also: `num_arcs`.\n        """"""\n        return self._ops.count_states(self)\n\n    def output_symbols(self):\n        """"""\n        Returns the output symbol table.\n\n        Returns:\n          The output symbol table.\n\n        See Also: :meth:`input_symbols`.\n        """"""\n        return self._output_symbols()\n\n    def properties(self, mask, test):\n        """"""Provides property bits.\n\n        This method provides user access to the properties attributes for the\n        FST. The resulting value is a long integer, but when it is cast to a\n        boolean, it represents whether or not the FST has the `mask` property.\n\n        Args:\n          mask: The property mask to be compared to the FST\'s properties.\n          test: Should any unknown values be computed before comparing against\n              the mask?\n\n        Returns:\n          A 64-bit bitmask representing the requested properties.\n        """"""\n        return self._properties(mask, test)\n\n    @classmethod\n    def read(cls, filename):\n        """"""Reads an FST from a file.\n\n        Args:\n            filename (str): The location of the input file.\n\n        Returns:\n            An FST object.\n\n        Raises:\n            RuntimeError: Read failed.\n        """"""\n        return cls._read(filename)\n\n    @classmethod\n    def read_from_stream(cls, strm, ropts):\n        """"""Reads an FST from an input stream.\n\n        Args:\n            strm (istream): The input stream to read from.\n            ropts (FstReadOptions): FST reading options.\n\n        Returns:\n            An FST object.\n\n        Raises:\n            RuntimeError: Read failed.\n        """"""\n        return cls._read_from_stream(strm, ropts)\n\n    def start(self):\n        """"""\n        Returns the start state.\n\n        Returns:\n          The start state if start state is set, -1 otherwise.\n        """"""\n        return self._start()\n\n    def states(self):\n        """"""\n        Returns an iterator over all states in the FST.\n\n        Returns:\n          A StateIterator object for the FST.\n\n        See also: `arcs`, `mutable_arcs`.\n        """"""\n        return self._state_iterator_type(self)\n\n    def text(self, isymbols=None, osymbols=None, ssymbols=None, acceptor=False,\n             show_weight_one=False, missing_symbol=""""):\n        """"""\n        Produces a human-readable string representation of the FST.\n\n        This method generates a human-readable string representation of the FST.\n        The caller may optionally specify SymbolTables used to label input\n        labels, output labels, or state labels, respectively.\n\n        Args:\n          isymbols: An optional symbol table used to label input symbols.\n          osymbols: An optional symbol table used to label output symbols.\n          ssymbols: An optional symbol table used to label states.\n          acceptor (bool): Should the FST be rendered in acceptor format if\n            possible? Defaults False.\n          show_weight_one (bool): Should weights equivalent to semiring One be\n            printed? Defaults False.\n          missing_symbol: The string to be printed when symbol table lookup\n            fails.\n\n        Returns:\n          A formatted string representing the FST.\n        """"""\n        if isymbols is None:\n            isymbols = self._input_symbols()\n        if osymbols is None:\n            osymbols = self._output_symbols()\n        sstrm = _base_io.ostringstream()\n        fstprinter = self._printer_type(\n            self, isymbols, osymbols, ssymbols,\n            acceptor, show_weight_one, ""\\t"", missing_symbol)\n        fstprinter.print_fst(sstrm, ""text"")\n        return sstrm.to_str()\n\n    def to_bytes(self):\n        """"""Returns a bytes object representing the FST.\n\n        Returns:\n            A bytes object.\n        """"""\n        return self._ops.to_bytes(self)\n\n    def type(self):\n        """"""\n        Returns the FST type.\n\n        Returns:\n          The FST type.\n        """"""\n        return self._type()\n\n    def verify(self):\n        """"""\n        Verifies that an FST\'s contents are sane.\n\n        Returns:\n          True if the contents are sane, False otherwise.\n        """"""\n        return self._ops.verify(self)\n\n    def write(self, filename):\n        """"""Serializes FST to a file.\n\n        This method writes the FST to a file in a binary format.\n\n        Args:\n          filename (str): The location of the output file.\n\n        Raises:\n          IOError: Write failed.\n        """"""\n        if not self._write(filename):\n            raise IOError(""Write failed: {!r}"".format(filename))\n\n    def write_to_stream(self, strm, wopts):\n        """"""Serializes FST to an output stream.\n\n        Args:\n            strm (ostream): The output stream to write to.\n            wopts (FstWriteOptions): FST writing options.\n\n        Returns:\n            True if write was successful, False otherwise.\n\n        Raises:\n            RuntimeError: Write failed.\n        """"""\n        return self._write_to_stream(strm, wopts)\n\n\nclass _MutableFstBase(_FstBase):\n    """"""Base class defining the Python API for mutable Fst types.""""""\n\n    def _check_mutating_imethod(self):\n        """"""\n        Checks whether an operation mutating the FST has produced an error.\n        """"""\n        if self._properties(_props.ERROR, True) == _props.ERROR:\n            raise RuntimeError(""Operation failed"")\n\n    def add_arc(self, state, arc):\n        """"""\n        Adds a new arc to the FST and returns self.\n\n        Args:\n          state: The integer index of the source state.\n          arc: The arc to add.\n\n        Returns:\n          self.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `add_state`.\n        """"""\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        self._add_arc(state, arc)\n        self._check_mutating_imethod()\n        return self\n\n    def add_state(self):\n        """"""\n        Adds a new state to the FST and returns the state ID.\n\n        Returns:\n          The integer index of the new state.\n\n        See also: `add_arc`, `set_start`, `set_final`.\n        """"""\n        result = self._add_state()\n        self._check_mutating_imethod()\n        return result\n\n    def arcsort(self, sort_type=""ilabel""):\n        """"""\n        Sorts arcs leaving each state of the FST.\n\n        This operation destructively sorts arcs leaving each state using either\n        input or output labels.\n\n        Args:\n          sort_type: Either ""ilabel"" (sort arcs according to input labels) or\n              ""olabel"" (sort arcs according to output labels).\n\n        Returns:\n          self.\n\n        Raises:\n          ValueError: Unknown sort type.\n\n        See also: `topsort`.\n        """"""\n        try:\n            sort_type = _getters.GetArcSortType(sort_type)\n        except ValueError:\n            raise ValueError(""Unknown sort type {!r}"".format(sort_type))\n        self._ops.arcsort(self, sort_type)\n        self._check_mutating_imethod()\n        return self\n\n    def closure(self, closure_plus=False):\n        """"""\n        Computes concatenative closure.\n\n        This operation destructively converts the FST to its concatenative\n        closure. If A transduces string x to y with weight a, then the closure\n        transduces x to y with weight a, xx to yy with weight a \\\\otimes a,\n        xxx to yyy with weight a \\\\otimes a \\\\otimes a, and so on. The empty\n        string is also transduced to itself with semiring One if `closure_plus`\n        is False.\n\n        Args:\n          closure_plus: If True, do not accept the empty string.\n\n        Returns:\n          self.\n        """"""\n        self._ops.closure(self, _getters.GetClosureType(closure_plus))\n        self._check_mutating_imethod()\n        return self\n\n    def concat(self, ifst):\n        """"""\n        Computes the concatenation (product) of two FSTs.\n\n        This operation destructively concatenates the FST with a second FST. If\n        A transduces string x to y with weight a and B transduces string w to v\n        with weight b, then their concatenation transduces string xw to yv with\n        weight a \\\\otimes b.\n\n        Args:\n          ifst: The second input FST.\n\n        Returns:\n          self.\n        """"""\n        self._ops.concat(self, ifst)\n        self._check_mutating_imethod()\n        return self\n\n    def connect(self):\n        """"""\n        Removes unsuccessful paths.\n\n        This operation destructively trims the FST, removing states and arcs\n        that are not part of any successful path.\n\n        Returns:\n          self.\n        """"""\n        self._ops.connect(self)\n        self._check_mutating_imethod()\n        return self\n\n    def decode(self, encoder):\n        """"""\n        Decodes encoded labels and/or weights.\n\n        This operation reverses the encoding performed by `encode`.\n\n        Args:\n          encoder: An EncodeMapper object used to encode the FST.\n\n        Returns:\n          self.\n\n        See also: `encode`.\n        """"""\n        self._ops.decode(self, encoder)\n        self._check_mutating_imethod()\n        return self\n\n    def delete_arcs(self, state, n=None):\n        """"""\n        Deletes arcs leaving a particular state.\n\n        Args:\n          state: The integer index of a state.\n          n: An optional argument indicating how many arcs to be deleted.\n              If this argument is None, all arcs from this state are deleted.\n\n        Returns:\n          self.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `delete_states`.\n        """"""\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        self._delete_arcs(state, n) if n else self._delete_all_arcs(state)\n        self._check_mutating_imethod()\n        return self\n\n    def delete_states(self, states=None):\n        """"""\n        Deletes states.\n\n        Args:\n          states: An optional iterable of integer indices of the states to be\n              deleted. If this argument is omitted, all states are deleted.\n\n        Returns:\n          self.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `delete_arcs`.\n        """"""\n        if states:\n            for state in states:\n                if not self._valid_state_id(state):\n                    raise IndexError(""State index out of range"")\n            self._delete_states(states)\n        else:\n            self._delete_all_states()\n        self._check_mutating_imethod()\n        return self\n\n    def encode(self, encoder):\n        """"""\n        Encodes labels and/or weights.\n\n        This operation allows for the representation of a weighted transducer as\n        a weighted acceptor, an unweighted transducer, or an unweighted acceptor\n        by considering the pair (input label, output label), the pair (input\n        label, weight), or the triple (input label, output label, weight) as a\n        single label. Applying this operation mutates the EncodeMapper argument,\n        which can then be used to decode.\n\n        Args:\n          encoder: An EncodeMapper object used to encode the FST.\n\n        Returns:\n          self.\n\n        See also: `decode`.\n        """"""\n        self._ops.encode(self, encoder)\n        self._check_mutating_imethod()\n        return self\n\n    def invert(self):\n        """"""\n        Inverts the FST\'s transduction.\n\n        This operation destructively inverts the FST\'s transduction by\n        exchanging input and output labels.\n\n        Returns:\n          self.\n        """"""\n        self._ops.invert(self)\n        self._check_mutating_imethod()\n        return self\n\n    def minimize(self, delta=_weight.DELTA, allow_nondet=False):\n        """"""\n        Minimizes the FST.\n\n        This operation destructively performs the minimization of deterministic\n        weighted automata and transducers. If the input FST A is an acceptor,\n        this operation produces the minimal acceptor B equivalent to A, i.e. the\n        acceptor with a minimal number of states that is equivalent to A. If the\n        input FST A is a transducer, this operation internally builds an\n        equivalent transducer with a minimal number of states. However, this\n        minimality is obtained by allowing transitions to have strings of\n        symbols as output labels, this is known in the literature as a real-time\n        transducer. Such transducers are not directly supported by the library.\n        This function will convert such transducers by expanding each\n        string-labeled transition into a sequence of transitions. This will\n        result in the creation of new states, hence losing the minimality\n        property.\n\n        Args:\n          delta: Comparison/quantization delta (default: 0.0009765625).\n          allow_nondet: Attempt minimization of non-deterministic FST?\n\n        Returns:\n          self.\n        """"""\n        self._ops.minimize(self, delta, allow_nondet)\n        self._check_mutating_imethod()\n        return self\n\n    def mutable_arcs(self, state):\n        """"""\n        Returns a mutable iterator over arcs leaving the specified state.\n\n        Args:\n          state: The source state index.\n\n        Returns:\n          A MutableArcIterator.\n\n        See also: `arcs`, `states`.\n        """"""\n        return self._mutable_arc_iterator_type(self, state)\n\n    def project(self, project_output=False):\n        """"""\n        Converts the FST to an acceptor using input or output labels.\n\n        This operation destructively projects an FST onto its domain or range by\n        either copying each arc\'s input label to its output label (the default)\n        or vice versa.\n\n        Args:\n          project_output: Project onto output labels?\n\n        Returns:\n          self.\n\n        See also: `decode`, `encode`, `relabel`, `relabel_tables`.\n        """"""\n        self._ops.project(self, _getters.GetProjectType(project_output))\n        self._check_mutating_imethod()\n        return self\n\n    def prune(self, weight=None, nstate=_fst.NO_STATE_ID, delta=_weight.DELTA):\n        """"""\n        Removes paths with weights below a certain threshold.\n\n        This operation deletes states and arcs in the input FST that do not\n        belong to a successful path whose weight is no more (w.r.t the natural\n        semiring order) than the threshold \\\\otimes the weight of the shortest\n        path in the input FST. Weights must be commutative and have the path\n        property.\n\n        Args:\n          weight: A Weight in the FST semiring or an object that can be\n              converted to a Weight in the FST semiring indicating the desired\n              weight threshold below which paths are pruned; if None, no paths\n              are pruned.\n          nstate: State number threshold (default: -1).\n          delta: Comparison/quantization delta (default: 0.0009765625).\n\n        Returns:\n          self.\n\n        See also: The constructive variant.\n        """"""\n        # Threshold is set to semiring Zero (no pruning) if weight is None.\n        weight = _get_weight_or_default(self._weight_factory, weight, False)\n        self._ops.prune(self, weight, nstate, delta)\n        self._check_mutating_imethod()\n        return self\n\n    def push(self, to_final=False, delta=_weight.DELTA,\n             remove_total_weight=False):\n        """"""\n        Pushes weights towards the initial or final states.\n\n        This operation destructively produces an equivalent transducer by\n        pushing the weights towards the initial state or toward the final\n        states. When pushing weights towards the initial state, the sum of the\n        weight of the outgoing transitions and final weight at any non-initial\n        state is equal to one in the resulting machine. When pushing weights\n        towards the final states, the sum of the weight of the incoming\n        transitions at any state is equal to one. Weights need to be left\n        distributive when pushing towards the initial state and right\n        distributive when pushing towards the final states.\n\n        Args:\n          to_final: Push towards final states?\n          delta: Comparison/quantization delta (default: 0.0009765625).\n          remove_total_weight: If pushing weights, should the total weight be\n              removed?\n\n        Returns:\n          self.\n\n        See also: The constructive variant, which also supports label pushing.\n        """"""\n        self._ops.push(self, _getters.GetReweightType(to_final),\n                       delta, remove_total_weight)\n        self._check_mutating_imethod()\n        return self\n\n    def relabel(self, ipairs=None, opairs=None):\n        """"""\n        Replaces input and/or output labels using pairs of labels.\n\n        This operation destructively relabels the input and/or output labels of\n        the FST using pairs of the form (old_ID, new_ID); omitted indices are\n        identity-mapped.\n\n        Args:\n          ipairs: An iterable containing (old index, new index) integer pairs.\n          opairs: An iterable containing (old index, new index) integer pairs.\n\n        Returns:\n          self.\n\n        Raises:\n          ValueError: No relabeling pairs specified.\n\n        See also: `decode`, `encode`, `project`, `relabel_tables`.\n        """"""\n        if not ipairs:\n            ipairs = []\n        if not opairs:\n            opairs = []\n        if len(ipairs) == 0 and len(opairs) == 0:\n            raise ValueError(""No relabeling pairs specified."")\n        self._ops.relabel(self, ipairs, opairs)\n        self._check_mutating_imethod()\n        return self\n\n    def relabel_tables(self, old_isymbols=None, new_isymbols=None,\n                       unknown_isymbol="""", attach_new_isymbols=True,\n                       old_osymbols=None, new_osymbols=None,\n                       unknown_osymbol="""", attach_new_osymbols=True):\n        """"""\n        Replaces input and/or output labels using SymbolTables.\n\n        This operation destructively relabels the input and/or output labels of\n        the FST using user-specified symbol tables; omitted symbols are\n        identity-mapped.\n\n        Args:\n           old_isymbols: The old SymbolTable for input labels, defaulting to the\n              FST\'s input symbol table.\n           new_isymbols: A SymbolTable used to relabel the input labels\n           unknown_isymbol: Input symbol to use to relabel OOVs (if empty,\n              OOVs raise an exception)\n           attach_new_isymbols: Should new_isymbols be made the FST\'s input\n              symbol table?\n           old_osymbols: The old SymbolTable for output labels, defaulting to\n              the FST\'s output symbol table.\n           new_osymbols: A SymbolTable used to relabel the output labels.\n           unknown_osymbol: Outnput symbol to use to relabel OOVs (if empty,\n              OOVs raise an exception)\n           attach_new_osymbols: Should new_osymbols be made the FST\'s output\n              symbol table?\n\n        Returns:\n          self.\n\n        Raises:\n          ValueError: No SymbolTable specified.\n\n        See also: `decode`, `encode`, `project`, `relabel`.\n        """"""\n        if new_isymbols is None and new_osymbols is None:\n            raise ValueError(""No new symbol tables specified"")\n        self._ops.relabel_tables(self,\n            self._input_symbols() if old_isymbols is None else old_isymbols,\n            new_isymbols, unknown_isymbol, attach_new_isymbols,\n            self._output_symbols() if old_osymbols is None else old_osymbols,\n            new_osymbols, unknown_osymbol, attach_new_osymbols)\n        self._check_mutating_imethod()\n        return self\n\n    def reserve_arcs(self, state, n):\n        """"""\n        Reserve n arcs at a particular state (best effort).\n\n        Args:\n          state: The integer index of a state.\n          n: The number of arcs to reserve.\n\n        Returns:\n          self.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `reserve_states`.\n        """"""\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        self._reserve_arcs(state, n)\n        self._check_mutating_imethod()\n        return self\n\n    def reserve_states(self, n):\n        """"""\n        Reserve n states (best effort).\n\n        Args:\n          n: The number of states to reserve.\n\n        Returns:\n          self.\n\n        See also: `reserve_arcs`.\n        """"""\n        self._reserve_states(n)\n        self._check_mutating_imethod()\n        return self\n\n    def reweight(self, potentials, to_final=False):\n        """"""\n        Reweights an FST using an iterable of potentials.\n\n        This operation destructively reweights an FST according to the\n        potentials and in the direction specified by the user. An arc of weight\n        w, with an origin state of potential p and destination state of\n        potential q, is reweighted by p^{-1} \\\\otimes (w \\\\otimes q) when\n        reweighting towards the initial state, and by (p \\\\otimes w) \\\\otimes\n        q^{-1} when reweighting towards the final states. The weights must be\n        left distributive when reweighting towards the initial state and right\n        distributive when reweighting towards the final states (e.g.,\n        TropicalWeight and LogWeight).\n\n        Args:\n          potentials: An iterable of TropicalWeights.\n          to_final: Push towards final states?\n\n        Returns:\n          self.\n        """"""\n        self._ops.reweight(self, potentials, _getters.GetReweightType(to_final))\n        self._check_mutating_imethod()\n        return self\n\n    def rmepsilon(self, connect=True, weight=None,\n                  nstate=_fst.NO_STATE_ID, delta=_weight.DELTA):\n        """"""\n        Removes epsilon transitions.\n\n        This operation destructively removes epsilon transitions, i.e., those\n        where both input and output labels are epsilon) from an FST.\n\n        Args:\n          connect: Should output be trimmed?\n          weight: A Weight in the FST semiring or an object that can be\n              converted to a Weight in the FST semiring indicating the desired\n              weight threshold below which paths are pruned; if None, no paths\n              are pruned.\n          nstate: State number threshold (default: -1).\n          delta: Comparison/quantization delta (default: 0.0009765625).\n\n        Returns:\n          self.\n\n        See also: The constructive variant, which also supports epsilon removal\n            in reverse (and which may be more efficient).\n        """"""\n        # Threshold is set to semiring Zero (no pruning) if weight is None.\n        weight = _get_weight_or_default(self._weight_factory, weight, False)\n        self._ops.rmepsilon(self, connect, weight, nstate, delta)\n        self._check_mutating_imethod()\n        return self\n\n    def set_final(self, state, weight=None):\n        """"""\n        Sets the final weight for a state.\n\n        Args:\n          state: The integer index of a state.\n          weight: A Weight in the FST semiring or an object that can be\n              converted to a Weight in the FST semiring indicating the desired\n              final weight; if omitted, it is set to semiring One.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `set_start`.\n        """"""\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        weight = _get_weight_or_default(self._weight_factory, weight, True)\n        self._set_final(state, weight)\n        self._check_mutating_imethod()\n        return self\n\n    def set_input_symbols(self, syms):\n        """"""Sets the input symbol table.\n\n        Passing ``None`` as a value will delete the input symbol table.\n\n        Args:\n          syms: A SymbolTable.\n\n        Returns:\n          self.\n\n        See also: `set_output_symbols`.\n        """"""\n        self._set_input_symbols(syms)\n        self._check_mutating_imethod()\n        return self\n\n    def set_output_symbols(self, syms):\n        """"""Sets the output symbol table.\n\n        Passing ``None`` as a value will delete the output symbol table.\n\n        Args:\n          syms: A SymbolTable.\n\n        Returns:\n          self.\n\n        See also: `set_input_symbols`.\n        """"""\n        self._set_output_symbols(syms)\n        self._check_mutating_imethod()\n        return self\n\n    def set_properties(self, props, mask):\n        """"""\n        Sets the properties bits.\n\n        Args:\n          props (int): The properties to be set.\n          mask (int): A mask to be applied to the `props` argument before\n            setting the FST\'s properties.\n\n        Returns:\n          self.\n        """"""\n        self._set_properties(props, mask)\n        self._check_mutating_imethod()\n        return self\n\n    def set_start(self, state):\n        """"""\n        Sets the initial state.\n\n        Args:\n          state: The integer index of a state.\n\n        Returns:\n          self.\n\n        Raises:\n          IndexError: State index out of range.\n\n        See also: `set_final`.\n        """"""\n        if not self._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        self._set_start(state)\n        self._check_mutating_imethod()\n        return self\n\n    def topsort(self):\n        """"""\n        Sorts transitions by state IDs.\n\n        This operation destructively topologically sorts the FST, if it is\n        acyclic; otherwise it remains unchanged. Once sorted, all transitions\n        are from lower state IDs to higher state IDs\n\n        Returns:\n           self.\n\n        See also: `arcsort`.\n        """"""\n        # _topsort returns False if the FST is cyclic.\n        if not self._ops.topsort(self):\n          raise RuntimeError(""Cannot topsort cyclic FST."")\n        self._check_mutating_imethod()\n        return self\n\n    def union(self, ifst):\n        """"""\n        Computes the union (sum) of two FSTs.\n\n        This operation computes the union (sum) of two FSTs. If A transduces\n        string x to y with weight a and B transduces string w to v with weight\n        b, then their union transduces x to y with weight a and w to v with\n        weight b.\n\n        Args:\n          ifst: The second input FST.\n\n        Returns:\n          self.\n        """"""\n        self._ops.union(self, ifst)\n        self._check_mutating_imethod()\n        return self\n\n\n# FST Iterator API\n\nclass _StateIteratorBase(object):\n    """"""Base class defining the Python API for state iterator types.""""""\n\n    def __init__(self, fst):\n        """"""Creates a new state iterator.\n\n        Args:\n            fst: The fst.\n        """"""\n        super(_StateIteratorBase, self).__init__(fst)\n\n    def __iter__(self):\n        while not self._done():\n            yield self._value()\n            self._next()\n\n    def next(self):\n        """"""Advances the iterator.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n        """"""\n        self._next()\n\n    def done(self):\n        """"""Indicates whether the iterator is exhausted or not.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          True if the iterator is exhausted, False otherwise.\n        """"""\n        return self._done()\n\n    def reset(self):\n        """"""Resets the iterator to the initial position.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n        """"""\n        self._reset()\n\n    def value(self):\n        """"""Returns the current state index.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n        """"""\n        return self._value()\n\n\nclass _ArcIteratorBase(object):\n    """"""Base class defining the Python API for arc iterator types.""""""\n\n    def __init__(self, fst, state):\n        """"""Creates a new arc iterator.\n\n        Args:\n            fst: The fst.\n            state: The state index.\n\n        Raises:\n            IndexError: State index out of range.\n        """"""\n        if not fst._valid_state_id(state):\n            raise IndexError(""State index out of range"")\n        super(_ArcIteratorBase, self).__init__(fst, state)\n\n    def __iter__(self):\n        while not self._done():\n            yield self._value()\n            self._next()\n\n    def next(self):\n        """"""Advances the iterator.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n        """"""\n        self._next()\n\n    def done(self):\n        """"""Indicates whether the iterator is exhausted or not.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          True if the iterator is exhausted, False otherwise.\n        """"""\n        return self._done()\n\n    def flags(self):\n        """"""Returns the current iterator behavioral flags.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          The current iterator behavioral flags as an integer.\n        """"""\n        return self._flags()\n\n    def position(self):\n        """"""Returns the position of the iterator.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          The iterator\'s position, expressed as an integer.\n        """"""\n        return self._position()\n\n    def reset(self):\n        """"""Resets the iterator to the initial position.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n        """"""\n        self._reset()\n\n    def seek(self, a):\n        """"""Advance the iterator to a new position.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Args:\n          a (int): The position to seek to.\n        """"""\n        self._seek(a)\n\n    def set_flags(self, flags, mask):\n        """"""Sets the current iterator behavioral flags.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Args:\n          flags (int): The properties to be set.\n          mask (int): A mask to be applied to the `flags` argument before\n            setting them.\n        """"""\n        self._set_flags(flags, mask)\n\n    def value(self):\n        """"""Returns the current arc.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n        """"""\n        return self._value()\n\n\nclass _MutableArcIteratorBase(_ArcIteratorBase):\n    """"""Base class defining the Python API for mutable arc iterator types.""""""\n\n    def __iter__(self):\n        while not self._done():\n            yield self._value(), self._set_value\n            self._next()\n\n    def set_value(self, arc):\n        """"""Replace the current arc with a new arc.\n\n        Args:\n          arc: The arc to replace the current arc with.\n        """"""\n        return self._set_value(arc)\n\n\n# Concrete Types\n\nclass SymbolTableIterator(_symbol_table.SymbolTableIterator):\n    """"""Symbol table iterator.\n\n    This class is used for iterating over the (index, symbol) pairs in a symbol\n    table. In addition to the full C++ API, it also supports the iterator\n    protocol, e.g. ::\n\n        # Returns a symbol table containing only symbols referenced by fst.\n        def prune_symbol_table(fst, syms, inp=True):\n            seen = set([0])\n            for s in fst.states():\n                for a in fst.arcs(s):\n                    seen.add(a.ilabel if inp else a.olabel)\n            pruned = SymbolTable()\n            for label, symbol in SymbolTableIterator(syms):\n                if label in seen:\n                    pruned.add_pair(symbol, label)\n            return pruned\n\n    Args:\n        table: The symbol table.\n    """"""\n    def __iter__(self):\n        while not self.done():\n            yield self.value(), self.symbol()\n            self.next()\n\n\n# Tropical semiring\n\nclass TropicalWeight(_float_weight.TropicalWeight):\n    """"""Tropical weight factory.\n\n    This class is used for creating new `~weight.TropicalWeight` instances.\n\n    TropicalWeight():\n        Creates an uninitialized `~weight.TropicalWeight` instance.\n\n    TropicalWeight(weight):\n        Creates a new `~weight.TropicalWeight` instance initalized with the\n        weight.\n\n    Args:\n        weight(float or FloatWeight): The weight value.\n    """"""\n    def __new__(cls, weight=None):\n        if weight is None:\n            return _float_weight.TropicalWeight()\n        if isinstance(weight, _float_weight.FloatWeight):\n            return _float_weight.TropicalWeight.from_float(weight.value)\n        return _float_weight.TropicalWeight.from_float(weight)\n\n\nclass StdArc(_arc.StdArc):\n    """"""FST arc with tropical weight.\n\n    StdArc():\n        Creates an uninitialized `StdArc` instance.\n\n    StdArc(ilabel, olabel, weight, nextstate):\n        Creates a new `StdArc` instance initalized with given arguments.\n\n    Args:\n        ilabel (int): The input label.\n        olabel (int): The output label.\n        weight (TropicalWeight): The arc weight.\n        nextstate (int): The destination state for the arc.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _arc.StdArc()\n        return _arc.StdArc.from_attrs(*args)\n\n\nclass StdEncodeMapper(_EncodeMapper, _encode.StdEncodeMapper):\n    """"""Arc encoder for an FST over the tropical semiring.""""""\n    pass\n\n\nclass StdFstCompiler(_FstCompiler):\n    """"""Compiler for FSTs over the tropical semiring.""""""\n    _compiler_type = _compiler.StdFstCompiler\n\n\nclass _StdFstDrawer(_FstDrawer, _drawer.StdFstDrawer):\n    """"""Drawer for FSTs over the tropical semiring.""""""\n    pass\n\n\nclass _StdFstPrinter(_FstPrinter, _printer.StdFstPrinter):\n    """"""Printer for FSTs over the tropical semiring.""""""\n    pass\n\n\nclass StdVectorFstStateIterator(_StateIteratorBase,\n                                _vector_fst.StdVectorFstStateIterator):\n    """"""State iterator for a vector FST over the tropical semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass StdVectorFstArcIterator(_ArcIteratorBase,\n                              _vector_fst.StdVectorFstArcIterator):\n    """"""Arc iterator for a vector FST over the tropical semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass StdVectorFstMutableArcIterator(\n        _MutableArcIteratorBase,\n        _vector_fst.StdVectorFstMutableArcIterator):\n    """"""Mutable arc iterator for a vector FST over the tropical semiring.\n\n    This class is used for iterating over the arcs leaving some state and\n    optionally replacing them with new ones. In addition to the full C++ API,\n    it also supports the iterator protocol. Calling the `__iter__` method of a\n    mutable arc iterator object returns an iterator over `(arc, setter)` pairs.\n    The `setter` is a bound method of the mutable arc iterator object that can\n    be used to replace the current arc with a new one. Most users should just\n    call the `mutable_arcs` method of a vector FST object instead of directly\n    constructing this iterator and take advantage of the Pythonic API, e.g. ::\n\n        for arc, setter in fst.mutable_arcs(0):\n            setter(StdArc(arc.ilabel, 0, arc.weight, arc.nextstate))\n    """"""\n    pass\n\n\nclass StdVectorFst(_MutableFstBase, _vector_fst.StdVectorFst):\n    """"""Vector FST over the tropical semiring.""""""\n\n    _ops = _std_ops\n    _drawer_type = _StdFstDrawer\n    _printer_type = _StdFstPrinter\n    _weight_factory = TropicalWeight\n    _state_iterator_type = StdVectorFstStateIterator\n    _arc_iterator_type = StdVectorFstArcIterator\n    _mutable_arc_iterator_type = StdVectorFstMutableArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (StdFst): The input FST over the tropical semiring.\n                If provided, its contents are used for initializing the new FST.\n                Defaults to ``None``.\n        """"""\n        super(StdVectorFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _vector_fst.StdVectorFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_std_vector_fst(fst, self)\n            elif isinstance(fst, _fst.StdFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_std_fst_to_vector_fst(fst, self)\n            else:\n                raise TypeError(""fst should be an FST over the tropical ""\n                                ""semiring"")\n\nStdVectorFst._mutable_fst_type = StdVectorFst\n\n\nclass StdConstFstStateIterator(_StateIteratorBase,\n                               _const_fst.StdConstFstStateIterator):\n    """"""State iterator for a constant FST over the tropical semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass StdConstFstArcIterator(_ArcIteratorBase,\n                             _const_fst.StdConstFstArcIterator):\n    """"""Arc iterator for a constant FST over the tropical semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass StdConstFst(_FstBase, _const_fst.StdConstFst):\n    """"""Constant FST over the tropical semiring.""""""\n\n    _ops = _std_ops\n    _drawer_type = _StdFstDrawer\n    _printer_type = _StdFstPrinter\n    _weight_factory = TropicalWeight\n    _state_iterator_type = StdConstFstStateIterator\n    _arc_iterator_type = StdConstFstArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (StdFst): The input FST over the tropical semiring.\n                If provided, its contents are used for initializing the new FST.\n                Defaults to ``None``.\n        """"""\n        super(StdConstFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _const_fst.StdConstFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_std_const_fst(fst, self)\n            elif isinstance(fst, _fst.StdFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_std_fst_to_const_fst(fst, self)\n            else:\n                raise TypeError(""fst should be an FST over the tropical ""\n                                ""semiring"")\n\nStdConstFst._mutable_fst_type = StdVectorFst\n\n\n# Log semiring\n\nclass LogWeight(_float_weight.LogWeight):\n    """"""Log weight factory.\n\n    This class is used for creating new `~weight.LogWeight` instances.\n\n    LogWeight():\n        Creates an uninitialized `~weight.LogWeight` instance.\n\n    LogWeight(weight):\n        Creates a new `~weight.LogWeight` instance initalized with the weight.\n\n    Args:\n        weight(float or FloatWeight): The weight value.\n    """"""\n    def __new__(cls, weight=None):\n        if weight is None:\n            return _float_weight.LogWeight()\n        if isinstance(weight, _float_weight.FloatWeight):\n            return _float_weight.LogWeight.from_float(weight.value)\n        return _float_weight.LogWeight.from_float(weight)\n\n\nclass LogArc(_arc.LogArc):\n    """"""FST arc with log weight.\n\n    LogArc():\n        Creates an uninitialized `LogArc` instance.\n\n    LogArc(ilabel, olabel, weight, nextstate):\n        Creates a new `LogArc` instance initalized with given arguments.\n\n    Args:\n        ilabel (int): The input label.\n        olabel (int): The output label.\n        weight (LogWeight): The arc weight.\n        nextstate (int): The destination state for the arc.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _arc.LogArc()\n        return _arc.LogArc.from_attrs(*args)\n\n\nclass LogEncodeMapper(_EncodeMapper, _encode.LogEncodeMapper):\n    """"""Arc encoder for an FST over the log semiring.""""""\n    pass\n\n\nclass LogFstCompiler(_FstCompiler):\n    """"""Compiler for FSTs over the log semiring.""""""\n    _compiler_type = _compiler.LogFstCompiler\n\n\nclass _LogFstDrawer(_FstDrawer, _drawer.LogFstDrawer):\n    """"""Drawer for FSTs over the log semiring.""""""\n    pass\n\n\nclass _LogFstPrinter(_FstPrinter, _printer.LogFstPrinter):\n    """"""Printer for FSTs over the log semiring.""""""\n    pass\n\n\nclass LogVectorFstStateIterator(_StateIteratorBase,\n                                _vector_fst.LogVectorFstStateIterator):\n    """"""State iterator for a vector FST over the log semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass LogVectorFstArcIterator(_ArcIteratorBase,\n                              _vector_fst.LogVectorFstArcIterator):\n    """"""Arc iterator for a vector FST over the log semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass LogVectorFstMutableArcIterator(\n        _MutableArcIteratorBase,\n        _vector_fst.LogVectorFstMutableArcIterator):\n    """"""Mutable arc iterator for a vector FST over the log semiring.\n\n    This class is used for iterating over the arcs leaving some state and\n    optionally replacing them with new ones. In addition to the full C++ API,\n    it also supports the iterator protocol. Calling the `__iter__` method of a\n    mutable arc iterator object returns an iterator over `(arc, setter)` pairs.\n    The `setter` is a bound method of the mutable arc iterator object that can\n    be used to replace the current arc with a new one. Most users should just\n    call the `mutable_arcs` method of a vector FST object instead of directly\n    constructing this iterator and take advantage of the Pythonic API, e.g. ::\n\n        for arc, setter in logfst.mutable_arcs(0):\n            setter(LogArc(arc.ilabel, 0, arc.weight, arc.nextstate))\n    """"""\n    pass\n\n\nclass LogVectorFst(_MutableFstBase, _vector_fst.LogVectorFst):\n    """"""Vector FST over the log semiring.""""""\n\n    _ops = _log_ops\n    _drawer_type = _LogFstDrawer\n    _printer_type = _LogFstPrinter\n    _weight_factory = LogWeight\n    _state_iterator_type = LogVectorFstStateIterator\n    _arc_iterator_type = LogVectorFstArcIterator\n    _mutable_arc_iterator_type = LogVectorFstMutableArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (LogFst): The input FST over the log semiring.\n                If provided, its contents are used for initializing the new FST.\n                Defaults to ``None``.\n        """"""\n        super(LogVectorFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _vector_fst.LogVectorFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_log_vector_fst(fst, self)\n            elif isinstance(fst, _fst.LogFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_log_fst_to_vector_fst(fst, self)\n            else:\n                raise TypeError(""fst should be an FST over the log semiring"")\n\nLogVectorFst._mutable_fst_type = LogVectorFst\n\n\nclass LogConstFstStateIterator(_StateIteratorBase,\n                               _const_fst.LogConstFstStateIterator):\n    """"""State iterator for a constant FST over the log semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass LogConstFstArcIterator(_ArcIteratorBase,\n                             _const_fst.LogConstFstArcIterator):\n    """"""Arc iterator for a constant FST over the log semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass LogConstFst(_FstBase, _const_fst.LogConstFst):\n    """"""Constant FST over the log semiring.""""""\n\n    _ops = _log_ops\n    _drawer_type = _LogFstDrawer\n    _printer_type = _LogFstPrinter\n    _weight_factory = LogWeight\n    _state_iterator_type = LogConstFstStateIterator\n    _arc_iterator_type = LogConstFstArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (LogFst): The input FST over the log semiring.\n                If provided, its contents are used for initializing the new FST.\n                Defaults to ``None``.\n        """"""\n        super(LogConstFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _const_fst.LogConstFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_log_const_fst(fst, self)\n            elif isinstance(fst, _fst.LogFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_log_fst_to_const_fst(fst, self)\n            else:\n                raise TypeError(""fst should be an FST over the log semiring"")\n\nLogConstFst._mutable_fst_type = LogVectorFst\n\n\n# Lattice semiring\n\nclass LatticeWeight(_lattice_weight.LatticeWeight):\n    """"""Lattice weight factory.\n\n    This class is used for creating new `~weight.LatticeWeight` instances.\n\n    LatticeWeight():\n        Creates an uninitialized `~weight.LatticeWeight` instance.\n\n    LatticeWeight(weight):\n        Creates a new `~weight.LatticeWeight` instance initalized with the\n        weight.\n\n    Args:\n        weight(Tuple[float, float] or LatticeWeight): A pair of weight values\n        or another `~weight.LatticeWeight` instance.\n\n    LatticeWeight(weight1, weight2):\n        Creates a new `~weight.LatticeWeight` instance initalized with the\n        weights.\n\n    Args:\n        weight1(float): The first weight value.\n        weight2(float): The second weight value.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _lattice_weight.LatticeWeight()\n        if len(args) == 1:\n            if isinstance(args[0], tuple) and len(args[0]) == 2:\n                args = args[0]\n            else:\n                return _lattice_weight.LatticeWeight.from_other(args[0])\n        return _lattice_weight.LatticeWeight.from_pair(*args)\n\n\nclass LatticeArc(_arc.LatticeArc):\n    """"""FST arc with lattice weight.\n\n    LatticeArc():\n        Creates an uninitialized `LatticeArc` instance.\n\n    LatticeArc(ilabel, olabel, weight, nextstate):\n        Creates a new `LatticeArc` instance initalized with given arguments.\n\n    Args:\n        ilabel (int): The input label.\n        olabel (int): The output label.\n        weight (LatticeWeight): The arc weight.\n        nextstate (int): The destination state for the arc.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _arc.LatticeArc()\n        return _arc.LatticeArc.from_attrs(*args)\n\n\nclass LatticeEncodeMapper(_EncodeMapper, _encode.LatticeEncodeMapper):\n    """"""Arc encoder for an FST over the lattice semiring.""""""\n    pass\n\n\nclass LatticeFstCompiler(_FstCompiler):\n    """"""Compiler for FSTs over the lattice semiring.""""""\n    _compiler_type = _compiler.LatticeFstCompiler\n\n\nclass _LatticeFstDrawer(_FstDrawer, _drawer.LatticeFstDrawer):\n    """"""Drawer for FSTs over the lattice semiring.""""""\n    pass\n\n\nclass _LatticeFstPrinter(_FstPrinter, _printer.LatticeFstPrinter):\n    """"""Printer for FSTs over the lattice semiring.""""""\n    pass\n\n\nclass LatticeVectorFstStateIterator(_StateIteratorBase,\n                                    _vector_fst.LatticeVectorFstStateIterator):\n    """"""State iterator for a vector FST over the lattice semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass LatticeVectorFstArcIterator(_ArcIteratorBase,\n                                  _vector_fst.LatticeVectorFstArcIterator):\n    """"""Arc iterator for a vector FST over the lattice semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass LatticeVectorFstMutableArcIterator(\n        _MutableArcIteratorBase,\n        _vector_fst.LatticeVectorFstMutableArcIterator):\n    """"""Mutable arc iterator for a vector FST over the lattice semiring.\n\n    This class is used for iterating over the arcs leaving some state and\n    optionally replacing them with new ones. In addition to the full C++ API,\n    it also supports the iterator protocol. Calling the `__iter__` method of a\n    mutable arc iterator object returns an iterator over `(arc, setter)` pairs.\n    The `setter` is a bound method of the mutable arc iterator object that can\n    be used to replace the current arc with a new one. Most users should just\n    call the `mutable_arcs` method of a vector FST object instead of directly\n    constructing this iterator and take advantage of the Pythonic API, e.g. ::\n\n        for arc, setter in lattice.mutable_arcs(0):\n            setter(LatticeArc(arc.ilabel, 0, arc.weight, arc.nextstate))\n    """"""\n    pass\n\n\nclass LatticeVectorFst(_MutableFstBase, _vector_fst.LatticeVectorFst):\n    """"""Vector FST over the lattice semiring.""""""\n\n    _ops = _lat_ops\n    _drawer_type = _LatticeFstDrawer\n    _printer_type = _LatticeFstPrinter\n    _weight_factory = LatticeWeight\n    _state_iterator_type = LatticeVectorFstStateIterator\n    _arc_iterator_type = LatticeVectorFstArcIterator\n    _mutable_arc_iterator_type = LatticeVectorFstMutableArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (LatticeFst): The input FST over the lattice semiring.\n                If provided, its contents are used for initializing the new FST.\n                Defaults to ``None``.\n        """"""\n        super(LatticeVectorFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _vector_fst.LatticeVectorFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_lattice_vector_fst(fst, self)\n            elif isinstance(fst, _fst.LatticeFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_lattice_fst_to_vector_fst(fst, self)\n            else:\n                raise TypeError(""fst should be an FST over the lattice ""\n                                ""semiring"")\n\nLatticeVectorFst._mutable_fst_type = LatticeVectorFst\n\n\nclass LatticeConstFstStateIterator(_StateIteratorBase,\n                                   _const_fst.LatticeConstFstStateIterator):\n    """"""State iterator for a constant FST over the lattice semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass LatticeConstFstArcIterator(_ArcIteratorBase,\n                                 _const_fst.LatticeConstFstArcIterator):\n    """"""Arc iterator for a constant FST over the lattice semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass LatticeConstFst(_FstBase, _const_fst.LatticeConstFst):\n    """"""Constant FST over the lattice semiring.""""""\n\n    _ops = _lat_ops\n    _drawer_type = _LatticeFstDrawer\n    _printer_type = _LatticeFstPrinter\n    _weight_factory = LatticeWeight\n    _state_iterator_type = LatticeConstFstStateIterator\n    _arc_iterator_type = LatticeConstFstArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (LatticeFst): The input FST over the lattice semiring.\n                If provided, its contents are used for initializing the new FST.\n                Defaults to ``None``.\n        """"""\n        super(LatticeConstFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _const_fst.LatticeConstFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_lattice_const_fst(fst, self)\n            elif isinstance(fst, _fst.LatticeFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_lattice_fst_to_const_fst(fst, self)\n            else:\n                raise TypeError(""fst should be an FST over the lattice ""\n                                ""semiring"")\n\nLatticeConstFst._mutable_fst_type = LatticeVectorFst\n\n\n# CompactLattice semiring\n\nclass CompactLatticeWeight(_lattice_weight.CompactLatticeWeight):\n    """"""Compact lattice weight factory.\n\n    This class is used for creating new `~weight.CompactLatticeWeight`\n    instances.\n\n    CompactLatticeWeight():\n        Creates an uninitialized `~weight.CompactLatticeWeight` instance.\n\n    CompactLatticeWeight(weight):\n        Creates a new `~weight.CompactLatticeWeight` instance initalized with\n        the weight.\n\n    Args:\n        weight(Tuple[Tuple[float, float], List[int]] or Tuple[LatticeWeight, List[int]] or CompactLatticeWeight):\n            A pair of weight values or another `~weight.CompactLatticeWeight`\n            instance.\n\n    CompactLatticeWeight(weight, string):\n        Creates a new `~weight.CompactLatticeWeight` instance initalized with\n        the (weight, string) pair.\n\n    Args:\n        weight(Tuple[float, float] or LatticeWeight): The weight value.\n        string(List[int]): The string value given as a list of integers.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _lattice_weight.CompactLatticeWeight()\n        if len(args) == 1:\n            if isinstance(args[0], tuple) and len(args[0]) == 2:\n                args = args[0]\n            else:\n                return _lattice_weight.CompactLatticeWeight.from_other(args[0])\n        if len(args) == 2:\n            w, s = args\n            if not isinstance(w, _lattice_weight.LatticeWeight):\n                w = LatticeWeight(w)\n            return _lattice_weight.CompactLatticeWeight.from_pair(w, s)\n        raise TypeError(""CompactLatticeWeight accepts 0 to 2 ""\n                        ""positional arguments; {} given"".format(len(args)))\n\n\nclass CompactLatticeArc(_arc.CompactLatticeArc):\n    """"""FST arc with compact lattice weight.\n\n    CompactLatticeArc():\n        Creates an uninitialized `CompactLatticeArc` instance.\n\n    CompactLatticeArc(ilabel, olabel, weight, nextstate):\n        Creates a new `CompactLatticeArc` instance initalized with given arguments.\n\n    Args:\n        ilabel (int): The input label.\n        olabel (int): The output label.\n        weight (CompactLatticeWeight): The arc weight.\n        nextstate (int): The destination state for the arc.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _arc.CompactLatticeArc()\n        return _arc.CompactLatticeArc.from_attrs(*args)\n\n\nclass CompactLatticeEncodeMapper(_EncodeMapper,\n                                 _encode.CompactLatticeEncodeMapper):\n    """"""Arc encoder for an FST over the compact lattice semiring.""""""\n    pass\n\n\nclass CompactLatticeFstCompiler(_FstCompiler):\n    """"""Compiler for FSTs over the compact lattice semiring.""""""\n    _compiler_type = _compiler.CompactLatticeFstCompiler\n\n\nclass _CompactLatticeFstDrawer(_FstDrawer, _drawer.CompactLatticeFstDrawer):\n    """"""Drawer for FSTs over the compact lattice semiring.""""""\n    pass\n\n\nclass _CompactLatticeFstPrinter(_FstPrinter, _printer.CompactLatticeFstPrinter):\n    """"""Printer for FSTs over the compact lattice semiring.""""""\n    pass\n\n\nclass CompactLatticeVectorFstStateIterator(\n        _StateIteratorBase,\n        _vector_fst.CompactLatticeVectorFstStateIterator):\n    """"""State iterator for a vector FST over the compact lattice semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass CompactLatticeVectorFstArcIterator(\n        _ArcIteratorBase,\n        _vector_fst.CompactLatticeVectorFstArcIterator):\n    """"""Arc iterator for a vector FST over the compact lattice semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass CompactLatticeVectorFstMutableArcIterator(\n        _MutableArcIteratorBase,\n        _vector_fst.CompactLatticeVectorFstMutableArcIterator):\n    """"""Mutable arc iterator for a vector FST over the compact lattice semiring.\n\n    This class is used for iterating over the arcs leaving some state and\n    optionally replacing them with new ones. In addition to the full C++ API,\n    it also supports the iterator protocol. Calling the `__iter__` method of a\n    mutable arc iterator object returns an iterator over `(arc, setter)` pairs.\n    The `setter` is a bound method of the mutable arc iterator object that can\n    be used to replace the current arc with a new one. Most users should just\n    call the `mutable_arcs` method of a vector FST object instead of directly\n    constructing this iterator and take advantage of the Pythonic API, e.g. ::\n\n        for arc, setter in lattice.mutable_arcs(0):\n            setter(LatticeArc(arc.ilabel, 0, arc.weight, arc.nextstate))\n    """"""\n    pass\n\n\nclass CompactLatticeVectorFst(_MutableFstBase,\n                        _vector_fst.CompactLatticeVectorFst):\n    """"""Vector FST over the compact lattice semiring.""""""\n\n    _ops = _clat_ops\n    _drawer_type = _CompactLatticeFstDrawer\n    _printer_type = _CompactLatticeFstPrinter\n    _weight_factory = CompactLatticeWeight\n    _state_iterator_type = CompactLatticeVectorFstStateIterator\n    _arc_iterator_type = CompactLatticeVectorFstArcIterator\n    _mutable_arc_iterator_type = CompactLatticeVectorFstMutableArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (CompactLatticeFst): The input FST over the compact lattice\n                semiring. If provided, its contents are used for initializing\n                the new FST. Defaults to ``None``.\n        """"""\n        super(CompactLatticeVectorFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _vector_fst.CompactLatticeVectorFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_compact_lattice_vector_fst(fst, self)\n            elif isinstance(fst, _fst.CompactLatticeFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_compact_lattice_fst_to_vector_fst(fst,\n                                                                        self)\n            else:\n                raise TypeError(""fst should be an FST over the compact lattice ""\n                                ""semiring"")\n\nCompactLatticeVectorFst._mutable_fst_type = CompactLatticeVectorFst\n\n\nclass CompactLatticeConstFstStateIterator(\n        _StateIteratorBase,\n        _const_fst.CompactLatticeConstFstStateIterator):\n    """"""State iterator for a constant FST over the compact lattice semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass CompactLatticeConstFstArcIterator(\n        _ArcIteratorBase,\n        _const_fst.CompactLatticeConstFstArcIterator):\n    """"""Arc iterator for a constant FST over the compact lattice semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass CompactLatticeConstFst(_FstBase, _const_fst.CompactLatticeConstFst):\n    """"""Constant FST over the compact lattice semiring.""""""\n\n    _ops = _clat_ops\n    _drawer_type = _CompactLatticeFstDrawer\n    _printer_type = _CompactLatticeFstPrinter\n    _weight_factory = CompactLatticeWeight\n    _state_iterator_type = CompactLatticeConstFstStateIterator\n    _arc_iterator_type = CompactLatticeConstFstArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (CompactLatticeFst): The input FST over the compact lattice\n                semiring. If provided, its contents are used for initializing\n                the new FST. Defaults to ``None``.\n        """"""\n        super(CompactLatticeConstFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _const_fst.CompactLatticeConstFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_compact_lattice_const_fst(fst, self)\n            elif isinstance(fst, _fst.CompactLatticeFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_compact_lattice_fst_to_const_fst(fst,\n                                                                       self)\n            else:\n                raise TypeError(""fst should be an FST over the compact lattice ""\n                                ""semiring"")\n\nCompactLatticeConstFst._mutable_fst_type = CompactLatticeVectorFst\n\n\n# KWS index semiring\n\nclass KwsTimeWeight(_lexicographic_weight.KwsTimeWeight):\n    """"""KWS time weight factory.\n\n    This class is used for creating new `~weight.KwsTimeWeight` instances.\n\n    KwsTimeWeight():\n        Creates an uninitialized `~weight.KwsTimeWeight` instance.\n\n    KwsTimeWeight(weight):\n        Creates a new `~weight.KwsTimeWeight` instance initalized with the\n        weight.\n\n    Args:\n        weight(Tuple[float, float] or KwsTimeWeight): A pair of weight values\n        or another `~weight.KwsTimeWeight` instance.\n\n    KwsTimeWeight(weight1, weight2):\n        Creates a new `~weight.KwsTimeWeight` instance initalized with the\n        weights.\n\n    Args:\n        weight1(float): The first weight value.\n        weight2(float): The second weight value.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _lexicographic_weight.KwsTimeWeight()\n        if len(args) == 1:\n            if isinstance(args[0], tuple) and len(args[0]) == 2:\n                args = args[0]\n            else:\n                args = (args[0].value1, args[0].value2)\n        args = (TropicalWeight(args[0]), TropicalWeight(args[1]))\n        return _lexicographic_weight.KwsTimeWeight.from_components(*args)\n\n\nclass KwsIndexWeight(_lexicographic_weight.KwsIndexWeight):\n    """"""KWS index weight factory.\n\n    This class is used for creating new `~weight.KwsIndexWeight`\n    instances.\n\n    KwsIndexWeight():\n        Creates an uninitialized `~weight.KwsIndexWeight` instance.\n\n    KwsIndexWeight(weight):\n        Creates a new `~weight.KwsIndexWeight` instance initalized with\n        the weight.\n\n    Args:\n        weight(Tuple[float, Tuple[float, float]] or Tuple[TropicalWeight, KwsTimeWeight] or KwsIndexWeight):\n            A pair of weight values or another `~weight.KwsIndexWeight`\n            instance.\n\n    KwsIndexWeight(weight1, weight2):\n        Creates a new `~weight.KwsIndexWeight` instance initalized with\n        weights.\n\n    Args:\n        weight1(float or TropicalWeight): The first weight value.\n        weight2(Tuple[float, float] or KwsTimeWeight): The second weight value.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _lexicographic_weight.KwsIndexWeight()\n        if len(args) == 1:\n            if isinstance(args[0], tuple) and len(args[0]) == 2:\n                args = (TropicalWeight(args[0][0]), KwsTimeWeight(args[0][1]))\n            else:\n                args = (args[0].value1, args[0].value2)\n            return _lexicographic_weight.KwsIndexWeight.from_components(*args)\n        if len(args) == 2:\n            args = (TropicalWeight(args[0]), KwsTimeWeight(args[1]))\n            return _lexicographic_weight.KwsIndexWeight.from_components(*args)\n        raise TypeError(""KwsIndexWeight accepts 0 to 2 ""\n                        ""positional arguments; {} given"".format(len(args)))\n\n\nclass KwsIndexArc(_arc.KwsIndexArc):\n    """"""FST arc with KWS index weight.\n\n    KwsIndexArc():\n        Creates an uninitialized `KwsIndexArc` instance.\n\n    KwsIndexArc(ilabel, olabel, weight, nextstate):\n        Creates a new `KwsIndexArc` instance initalized with given arguments.\n\n    Args:\n        ilabel (int): The input label.\n        olabel (int): The output label.\n        weight (KwsIndexWeight): The arc weight.\n        nextstate (int): The destination state for the arc.\n    """"""\n    def __new__(cls, *args):\n        if len(args) == 0:\n            return _arc.KwsIndexArc()\n        return _arc.KwsIndexArc.from_attrs(*args)\n\n\nclass KwsIndexEncodeMapper(_EncodeMapper, _encode.KwsIndexEncodeMapper):\n    """"""Arc encoder for an FST over the KWS index semiring.""""""\n    pass\n\n\nclass KwsIndexFstCompiler(_FstCompiler):\n    """"""Compiler for FSTs over the KWS index semiring.""""""\n    _compiler_type = _compiler.KwsIndexFstCompiler\n\n\nclass _KwsIndexFstDrawer(_FstDrawer, _drawer.KwsIndexFstDrawer):\n    """"""Drawer for FSTs over the KWS index semiring.""""""\n    pass\n\n\nclass _KwsIndexFstPrinter(_FstPrinter, _printer.KwsIndexFstPrinter):\n    """"""Printer for FSTs over the KWS index semiring.""""""\n    pass\n\n\nclass KwsIndexVectorFstStateIterator(\n        _StateIteratorBase,\n        _vector_fst.KwsIndexVectorFstStateIterator):\n    """"""State iterator for a vector FST over the KWS index semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass KwsIndexVectorFstArcIterator(_ArcIteratorBase,\n                                   _vector_fst.KwsIndexVectorFstArcIterator):\n    """"""Arc iterator for a vector FST over the KWS index semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass KwsIndexVectorFstMutableArcIterator(\n        _MutableArcIteratorBase,\n        _vector_fst.KwsIndexVectorFstMutableArcIterator):\n    """"""Mutable arc iterator for a vector FST over the KWS index semiring.\n\n    This class is used for iterating over the arcs leaving some state and\n    optionally replacing them with new ones. In addition to the full C++ API,\n    it also supports the iterator protocol. Calling the `__iter__` method of a\n    mutable arc iterator object returns an iterator over `(arc, setter)` pairs.\n    The `setter` is a bound method of the mutable arc iterator object that can\n    be used to replace the current arc with a new one. Most users should just\n    call the `mutable_arcs` method of a vector FST object instead of directly\n    constructing this iterator and take advantage of the Pythonic API, e.g. ::\n\n        for arc, setter in fst.mutable_arcs(0):\n            setter(KwsIndexArc(arc.ilabel, 0, arc.weight, arc.nextstate))\n    """"""\n    pass\n\n\nclass KwsIndexVectorFst(_MutableFstBase, _vector_fst.KwsIndexVectorFst):\n    """"""Vector FST over the KWS index semiring.""""""\n\n    _ops = _index_ops\n    _drawer_type = _KwsIndexFstDrawer\n    _printer_type = _KwsIndexFstPrinter\n    _weight_factory = KwsIndexWeight\n    _state_iterator_type = KwsIndexVectorFstStateIterator\n    _arc_iterator_type = KwsIndexVectorFstArcIterator\n    _mutable_arc_iterator_type = KwsIndexVectorFstMutableArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (KwsIndexFst): The input FST over the KWS index semiring.\n                If provided, its contents are used for initializing the new FST.\n                Defaults to ``None``.\n        """"""\n        super(KwsIndexVectorFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _vector_fst.KwsIndexVectorFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_kws_index_vector_fst(fst, self)\n            elif isinstance(fst, _fst.KwsIndexFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_kws_index_fst_to_vector_fst(fst, self)\n            else:\n                raise TypeError(""fst should be an FST over the KWS index ""\n                                ""semiring"")\n\nKwsIndexVectorFst._mutable_fst_type = KwsIndexVectorFst\n\n\nclass KwsIndexConstFstStateIterator(_StateIteratorBase,\n                                    _const_fst.KwsIndexConstFstStateIterator):\n    """"""State iterator for a constant FST over the KWS index semiring.\n\n    This class is used for iterating over the states. In addition to the full\n    C++ API, it also supports the iterator protocol. Most users should just call\n    the `states` method of an FST object instead of directly constructing this\n    iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass KwsIndexConstFstArcIterator(_ArcIteratorBase,\n                                  _const_fst.KwsIndexConstFstArcIterator):\n    """"""Arc iterator for a constant FST over the KWS index semiring.\n\n    This class is used for iterating over the arcs leaving some state. In\n    addition to the full C++ API, it also supports the iterator protocol.\n    Most users should just call the `arcs` method of an FST object instead of\n    directly constructing this iterator and take advantage of the Pythonic API.\n    """"""\n    pass\n\n\nclass KwsIndexConstFst(_FstBase, _const_fst.KwsIndexConstFst):\n    """"""Constant FST over the KWS index semiring.""""""\n\n    _ops = _index_ops\n    _drawer_type = _KwsIndexFstDrawer\n    _printer_type = _KwsIndexFstPrinter\n    _weight_factory = KwsIndexWeight\n    _state_iterator_type = KwsIndexConstFstStateIterator\n    _arc_iterator_type = KwsIndexConstFstArcIterator\n\n    def __init__(self, fst=None):\n        """"""\n        Args:\n            fst (KwsIndexFst): The input FST over the KWS index semiring.\n                If provided, its contents are used for initializing the new FST.\n                Defaults to ``None``.\n        """"""\n        super(KwsIndexConstFst, self).__init__()\n        if fst is not None:\n            if isinstance(fst, _const_fst.KwsIndexConstFst):\n                # This assignment shares implementation with COW semantics.\n                _fstext_shims._assign_kws_index_const_fst(fst, self)\n            elif isinstance(fst, _fst.KwsIndexFst):\n                # This assignment makes a copy.\n                _fstext_shims._assign_kws_index_fst_to_const_fst(fst, self)\n            else:\n                raise TypeError(""fst should be an FST over the KWS index ""\n                                ""semiring"")\n\nKwsIndexConstFst._mutable_fst_type = KwsIndexVectorFst\n\n\n# FST Operations\n\ndef arcmap(ifst, map_type=""identity"", delta=_weight.DELTA, weight=None):\n    """"""\n    Constructively applies a transform to all arcs and final states.\n\n    This operation transforms each arc and final state in the input FST\n    using one of the following:\n\n    * identity: maps to self.\n    * input_epsilon: replaces all input labels with epsilon.\n    * invert: reciprocates all non-Zero weights.\n    * output_epsilon: replaces all output labels with epsilon.\n    * plus: adds a constant to all weights.\n    * quantize: quantizes weights.\n    * rmweight: replaces all non-Zero weights with 1.\n    * superfinal: redirects final states to a new superfinal state.\n    * times: right-multiplies a constant to all weights.\n\n    Args:\n        ifst: The input FST.\n        map_type: A string matching a known mapping operation (see above).\n        delta: Comparison/quantization delta (ignored unless `map_type` is\n            `quantize`, default: 0.0009765625).\n        weight: A Weight in the FST semiring or an object that can be converted\n            to a Weight in the FST semiring passed to the arc-mapper; this is\n            ignored unless `map_type` is `plus` (in which case it defaults\n            to semiring Zero) or `times` (in which case it defaults to\n            semiring One).\n\n    Returns:\n        An FST with arcs and final states remapped.\n\n    Raises:\n        ValueError: Unknown map type.\n\n    See also: `statemap`.\n    """"""\n    # NB: Weight conversion mappers are not supported.\n    try:\n        map_type = _getters.GetMapType(map_type)\n    except ValueError:\n        raise ValueError(""Unknown map type: {!r}"".format(map_type))\n    weight = _get_weight_or_default(ifst._weight_factory, weight,\n                                    map_type == MapType.TIMES_MAPPER)\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.map(ifst, ofst, map_type, delta, weight)\n    return ofst\n\n\ndef compose(ifst1, ifst2, connect=True, compose_filter=""auto""):\n    """"""\n    Constructively composes two FSTs.\n\n    This operation computes the composition of two FSTs. If A transduces\n    string x to y with weight a and B transduces y to z with weight b, then\n    their composition transduces string x to z with weight a \\\\otimes b. The\n    output labels of the first transducer or the input labels of the second\n    transducer must be sorted (or otherwise support appropriate matchers).\n\n    Args:\n        ifst1: The first input FST.\n        ifst2: The second input FST.\n        connect: Should output be trimmed?\n        compose_filter: A string matching a known composition filter; one of:\n            ""alt_sequence"", ""auto"", ""match"", ""null"", ""sequence"", ""trivial"".\n\n    Returns:\n        A composed FST.\n\n    See also: `arcsort`.\n    """"""\n    try:\n        compose_filter = _getters.GetComposeFilter(compose_filter)\n    except ValueError:\n        raise ValueError(""Unknown compose filter: {!r}""\n                         .format(compose_filter))\n    ofst = ifst1._mutable_fst_type()\n    ifst1._ops.compose(ifst1, ifst2, ofst, connect, compose_filter)\n    return ofst\n\n\ndef determinize(ifst, delta=_weight.DELTA, weight=None, nstate=_fst.NO_STATE_ID,\n                subsequential_label=0, det_type=""functional"",\n                increment_subsequential_label=False):\n    """"""\n    Constructively determinizes a weighted FST.\n\n    This operations creates an equivalent FST that has the property that no\n    state has two transitions with the same input label. For this algorithm,\n    epsilon transitions are treated as regular symbols (cf. `rmepsilon`).\n\n    Args:\n        ifst: The input FST.\n        delta: Comparison/quantization delta (default: 0.0009765625).\n        weight: A Weight in the FST semiring or an object that can be converted\n            to a Weight in the FST semiring indicating the desired weight\n            threshold below which paths are pruned; if None, no paths are\n            pruned.\n        nstate: State number threshold (default: -1).\n        subsequential_label: Input label of arc corresponding to residual final\n            output when producing a subsequential transducer.\n        det_type: Type of determinization; one of: ""functional"" (input\n            transducer is functional), ""nonfunctional"" (input transducer is not\n            functional) and disambiguate"" (input transducer is not functional\n            but only keep the min of ambiguous outputs).\n        increment_subsequential_label: Increment subsequential when creating\n            several arcs for the residual final output at a given state.\n\n    Returns:\n        An equivalent deterministic FST.\n\n    Raises:\n        ValueError: Unknown determinization type.\n\n    See also: `disambiguate`, `rmepsilon`.\n    """"""\n    try:\n        det_type = _getters.GetDeterminizeType(det_type)\n    except ValueError:\n        raise ValueError(""Unknown determinization type: {!r}"".format(det_type))\n    # Threshold is set to semiring Zero (no pruning) if weight is None.\n    weight = _get_weight_or_default(ifst._weight_factory, weight, False)\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.determinize(ifst, ofst, delta, weight, nstate,\n                          subsequential_label, det_type,\n                          increment_subsequential_label)\n    return ofst\n\n\ndef difference(ifst1, ifst2, connect=True, compose_filter=""auto""):\n    """"""\n    Constructively computes the difference of two FSTs.\n\n    This operation computes the difference between two FSAs. Only strings that\n    are in the first automaton but not in second are retained in the result. The\n    first argument must be an acceptor; the second argument must be an\n    unweighted, epsilon-free, deterministic acceptor. The output labels of the\n    first transducer or the input labels of the second transducer must be sorted\n    (or otherwise support appropriate matchers).\n\n    Args:\n        ifst1: The first input FST.\n        ifst2: The second input FST.\n        connect: Should the output FST be trimmed?\n        compose_filter: A string matching a known composition filter; one of:\n            ""alt_sequence"", ""auto"", ""match"", ""null"", ""sequence"", ""trivial"".\n\n    Returns:\n        An FST representing the difference of the FSTs.\n    """"""\n    try:\n        compose_filter = _getters.GetComposeFilter(compose_filter)\n    except ValueError:\n        raise ValueError(""Unknown compose filter: {!r}""\n                         .format(compose_filter))\n    ofst = ifst1._mutable_fst_type()\n    ifst1._ops.difference(ifst1, ifst2, ofst, connect, compose_filter)\n    return ofst\n\ndef disambiguate(ifst, delta=_weight.DELTA, weight=None,\n                 nstate=_fst.NO_STATE_ID, subsequential_label=0):\n    """"""\n    Constructively disambiguates a weighted transducer.\n\n    This operation disambiguates a weighted transducer. The result will be an\n    equivalent FST that has the property that no two successful paths have the\n    same input labeling. For this algorithm, epsilon transitions are treated as\n    regular symbols (cf. `rmepsilon`).\n\n    Args:\n        ifst: The input FST.\n        delta: Comparison/quantization delta (default: 0.0009765625).\n        weight: A Weight in the FST semiring or an object that can be converted\n            to a Weight in the FST semiring indicating the desired weight\n            threshold below which paths are pruned; if None, no paths are\n            pruned.\n        nstate: State number threshold.\n        subsequential_label: Input label of arc corresponding to residual final\n            output when producing a subsequential transducer.\n\n    Returns:\n        An equivalent disambiguated FST.\n\n    See also: `determinize`, `rmepsilon`.\n    """"""\n    # Threshold is set to semiring Zero (no pruning) if weight is None.\n    weight = _get_weight_or_default(ifst._weight_factory, weight, False)\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.disambiguate(ifst, ofst, delta, weight, nstate,\n                           subsequential_label)\n    return ofst\n\n\ndef epsnormalize(ifst, eps_norm_output=False):\n    """"""\n    Constructively epsilon-normalizes an FST.\n\n    This operation creates an equivalent FST that is epsilon-normalized. An\n    acceptor is epsilon-normalized if it it is epsilon-removed (cf.\n    `rmepsilon`). A transducer is input epsilon-normalized if, in addition,\n    along any path, all arcs with epsilon input labels follow all arcs with\n    non-epsilon input labels. Output epsilon-normalized is defined similarly.\n    The input FST must be functional.\n\n    Args:\n        ifst: The input FST.\n        eps_norm_output: Should the FST be output epsilon-normalized?\n\n    Returns:\n        An equivalent epsilon-normalized FST.\n\n    See also: `rmepsilon`.\n    """"""\n    if eps_norm_output:\n        eps_norm_type = EpsNormalizeType.EPS_NORM_OUTPUT\n    else:\n        eps_norm_type = EpsNormalizeType.EPS_NORM_INPUT\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.epsnormalize(ifst, ofst, eps_norm_type)\n    return ofst\n\n\ndef equal(ifst1, ifst2, delta=_weight.DELTA):\n    """"""\n    Are two FSTs equal?\n\n    This function tests whether two FSTs have the same states with the same\n    numbering and the same transitions with the same labels and weights in the\n    same order.\n\n    Args:\n        ifst1: The first input FST.\n        ifst2: The second input FST.\n        delta: Comparison/quantization delta (0.0009765625).\n\n    Returns:\n        True if the FSTs satisfy the above condition, else False.\n\n    See also: `equivalent`, `isomorphic`, `randequivalent`.\n    """"""\n    return ifst1._ops.equal(ifst1, ifst2, delta)\n\n\ndef equivalent(ifst1, ifst2, delta=_weight.DELTA):\n    """"""\n    Are the two acceptors equivalent?\n\n    This operation tests whether two epsilon-free deterministic weighted\n    acceptors are equivalent, that is if they accept the same strings with the\n    same weights.\n\n    Args:\n        ifst1: The first input FST.\n        ifst2: The second input FST.\n        delta: Comparison/quantization delta (default: 0.0009765625).\n\n    Returns:\n        True if the FSTs satisfy the above condition, else False.\n\n    Raises:\n        RuntimeError: Equivalence test encountered error.\n\n    See also: `equal`, `isomorphic`, `randequivalent`.\n    """"""\n    result, error = ifst1._ops.equivalent(ifst1, ifst2, delta)\n    if error:\n        raise RuntimeError(""Equivalence test encountered error"")\n    return result\n\n\ndef intersect(ifst1, ifst2, connect=True, compose_filter=""auto""):\n    """"""\n    Constructively intersects two FSTs.\n\n    This operation computes the intersection (Hadamard product) of two FSTs.\n    Only strings that are in both automata are retained in the result. The two\n    arguments must be acceptors. One of the arguments must be label-sorted (or\n    otherwise support appropriate matchers).\n\n    Args:\n        ifst1: The first input FST.\n        ifst2: The second input FST.\n        connect: Should output be trimmed?\n        compose_filter: A string matching a known composition filter; one of:\n            ""alt_sequence"", ""auto"", ""match"", ""null"", ""sequence"", ""trivial"".\n\n    Returns:\n        An intersected FST.\n    """"""\n    try:\n        compose_filter = _getters.GetComposeFilter(compose_filter)\n    except ValueError:\n        raise ValueError(""Unknown compose filter: {!r}""\n                         .format(compose_filter))\n    ofst = ifst1._mutable_fst_type()\n    ifst1._ops.intersect(ifst1, ifst2, ofst, connect, compose_filter)\n    return ofst\n\n\ndef isomorphic(ifst1, ifst2, delta=_weight.DELTA):\n    """"""\n    Are the two acceptors isomorphic?\n\n    This operation determines if two transducers with a certain required\n    determinism have the same states, irrespective of numbering, and the same\n    transitions with the same labels and weights, irrespective of ordering. In\n    other words, FSTs A, B are isomorphic if and only if the states of A can be\n    renumbered and the transitions leaving each state reordered so the two are\n    equal (according to the definition given in `equal`).\n\n    Args:\n        ifst1: The first input FST.\n        ifst2: The second input FST.\n        delta: Comparison/quantization delta (default: 0.0009765625).\n\n    Returns:\n        True if the two transducers satisfy the above condition, else False.\n\n    See also: `equal`, `equivalent`, `randequivalent`.\n    """"""\n    return ifst1._ops.isomorphic(ifst1, ifst2, delta)\n\n\ndef prune(ifst, weight=None, nstate=_fst.NO_STATE_ID, delta=_weight.DELTA):\n    """"""\n    Constructively removes paths with weights below a certain threshold.\n\n    This operation deletes states and arcs in the input FST that do not belong\n    to a successful path whose weight is no more (w.r.t the natural semiring\n    order) than the threshold t \\\\otimes the weight of the shortest path in\n    the input FST. Weights must be commutative and have the path property.\n\n    Args:\n        ifst: The input FST.\n        weight: A Weight in the FST semiring or an object that can be converted\n            to a Weight in the FST semiring indicating the desired weight\n            threshold below which paths are pruned; if None, no paths are\n            pruned.\n        nstate: State number threshold (default: -1).\n        delta: Comparison/quantization delta (default: 0.0009765625).\n\n    Returns:\n        A pruned FST.\n\n    See also: The destructive variant.\n    """"""\n    # Threshold is set to semiring Zero (no pruning) if weight is None.\n    weight = _get_weight_or_default(ifst._weight_factory, weight, False)\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.prune_cons(ifst, ofst, weight, nstate, delta)\n    return ofst\n\n\ndef push(ifst, push_weights=False, push_labels=False, remove_common_affix=False,\n         remove_total_weight=False, to_final=False, delta=_weight.DELTA):\n    """"""\n    Constructively pushes weights/labels towards initial or final states.\n\n    This operation produces an equivalent transducer by pushing the weights\n    and/or the labels towards the initial state or toward the final states.\n\n    When pushing weights towards the initial state, the sum of the weight of the\n    outgoing transitions and final weight at any non-initial state is equal to 1\n    in the resulting machine. When pushing weights towards the final states, the\n    sum of the weight of the incoming transitions at any state is equal to 1.\n    Weights need to be left distributive when pushing towards the initial state\n    and right distributive when pushing towards the final states.\n\n    Pushing labels towards the initial state consists in minimizing at every\n    state the length of the longest common prefix of the output labels of the\n    outgoing paths. Pushing labels towards the final states consists in\n    minimizing at every state the length of the longest common suffix of the\n    output labels of the incoming paths.\n\n    Args:\n        ifst: The input FST.\n        push_weights: Should weights be pushed?\n        push_labels: Should labels be pushed?\n        remove_common_affix: If pushing labels, should common prefix/suffix be\n            removed?\n        remove_total_weight: If pushing weights, should total weight be removed?\n        to_final: Push towards final states?\n        delta: Comparison/quantization delta (default: 0.0009765625).\n\n    Returns:\n        An equivalent pushed FST.\n\n    See also: The destructive variant.\n    """"""\n    flags = _getters.GetPushFlags(push_weights, push_labels,\n                                  remove_common_affix, remove_total_weight)\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.push_cons(ifst, ofst, flags,\n                        _getters.GetReweightType(to_final), delta)\n    return ofst\n\n\ndef randequivalent(ifst1, ifst2, npath=1, delta=_weight.DELTA, seed=None,\n                   select=""uniform"", max_length=2147483647):\n    """"""\n    Are two acceptors stochastically equivalent?\n\n    This operation tests whether two FSTs are equivalent by randomly generating\n    paths alternatively in each of the two FSTs. For each randomly generated\n    path, the algorithm computes for each of the two FSTs the sum of the weights\n    of all the successful paths sharing the same input and output labels as the\n    randomly generated path and checks that these two values are within `delta`.\n\n    Args:\n        ifst1: The first input FST.\n        ifst2: The second input FST.\n        npath: The number of random paths to generate.\n        delta: Comparison/quantization delta.\n        seed: An optional seed value for random path generation; if None, the\n            current time and process ID is used.\n        select: A string matching a known random arc selection type; one of:\n            ""uniform"", ""log_prob"", ""fast_log_prob"".\n        max_length: The maximum length of each random path.\n\n    Returns:\n        True if the two transducers satisfy the above condition, else False.\n\n    Raises:\n        RuntimeError: Random equivalence test encountered error.\n\n    See also: `equal`, `equivalent`, `isomorphic`, `randgen`.\n    """"""\n    try:\n        select = _getters.GetRandArcSelection(select)\n    except ValueError:\n        raise ValueError(""Unknown random arc selection type: {!r}""\n                         .format(select))\n    if seed is None:\n        seed = int(_time.time()) + _os.getpid()\n    result, error = ifst1._ops.randequivalent(ifst1, ifst2, npath, delta,\n                                              seed, select, max_length)\n    if error:\n        raise RuntimeError(""Random equivalence test encountered error"")\n    return result\n\n\ndef randgen(ifst, npath=1, seed=None, select=""uniform"",\n            max_length=2147483647, weighted=False, remove_total_weight=False):\n    """"""\n    Randomly generate successful paths in an FST.\n\n    This operation randomly generates a set of successful paths in the input\n    FST. This relies on a mechanism for selecting arcs, specified using the\n    `select` argument. The default selector, ""uniform"", randomly selects a\n    transition using a uniform distribution. The ""log_prob"" selector randomly\n    selects a transition w.r.t. the weights treated as negative log\n    probabilities after normalizing for the total weight leaving the state. In\n    all cases, finality is treated as a transition to a super-final state.\n\n    Args:\n        ifst: The input FST.\n        npath: The number of random paths to generate.\n        seed: An optional seed value for random path generation; if zero, the\n            current time and process ID is used.\n        select: A string matching a known random arc selection type; one of:\n            ""uniform"", ""log_prob"", ""fast_log_prob"".\n        max_length: The maximum length of each random path.\n        weighted: Should the output be weighted by path count?\n        remove_total_weight: Should the total weight be removed (ignored when\n            `weighted` is False)?\n\n    Returns:\n        An FST containing one or more random paths.\n\n    See also: `randequivalent`.\n    """"""\n    try:\n        select = _getters.GetRandArcSelection(select)\n    except ValueError:\n        raise ValueError(""Unknown random arc selection type: {!r}""\n                         .format(select))\n    if seed is None:\n        seed = int(_time.time()) + _os.getpid()\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.randgen(ifst, ofst, seed, select, max_length,\n                      npath, weighted, remove_total_weight)\n    return ofst\n\n\ndef replace(pairs, root_label, call_arc_labeling=""input"",\n            return_arc_labeling=""neither"", epsilon_on_replace=False,\n            return_label=0):\n    """"""\n    Recursively replaces arcs in the root FST with other FST(s).\n\n    This operation performs the dynamic replacement of arcs in one FST with\n    another FST, allowing the definition of FSTs analogous to RTNs. It takes as\n    input a set of pairs formed by a non-terminal label and its corresponding\n    FST, and a label identifying the root FST in that set. The resulting FST is\n    obtained by taking the root FST and recursively replacing each arc having a\n    nonterminal as output label by its corresponding FST. More precisely, an arc\n    from state s to state d with (nonterminal) output label n in this FST is\n    replaced by redirecting this ""call"" arc to the initial state of a copy F of\n    the FST for n, and adding ""return"" arcs from each final state of F to d.\n    Optional arguments control how the call and return arcs are labeled; by\n    default, the only non-epsilon label is placed on the call arc.\n\n    Args:\n        pairs: An iterable of (nonterminal label, FST) pairs, where the former\n            is an unsigned integer and the latter is an Fst instance.\n        root_label: Label identifying the root FST.\n        call_arc_labeling: A string indicating which call arc labels should be\n            non-epsilon. One of: ""input"" (default), ""output"", ""both"", ""neither"".\n            This value is set to ""neither"" if epsilon_on_replace is True.\n        return_arc_labeling: A string indicating which return arc labels should\n            be non-epsilon. One of: ""input"", ""output"", ""both"", ""neither""\n            (default). This value is set to ""neither"" if epsilon_on_replace is\n            True.\n        epsilon_on_replace: Should call and return arcs be epsilon arcs? If\n            True, this effectively overrides call_arc_labeling and\n            return_arc_labeling, setting both to ""neither"".\n        return_label: The integer label for return arcs.\n\n    Returns:\n        An FST resulting from expanding the input RTN.\n    """"""\n    try:\n        call_arc_labeling = _getters.GetReplaceLabelType(call_arc_labeling,\n                                                         epsilon_on_replace)\n    except ValueError:\n        raise ValueError(""Unknown call arc replace label type: {!r}""\n                         .format(call_arc_labeling))\n    try:\n        return_arc_labeling = _getters.GetReplaceLabelType(return_arc_labeling,\n                                                           epsilon_on_replace)\n    except ValueError:\n        raise ValueError(""Unknown return arc replace label type: {!r}""\n                         .format(return_arc_labeling))\n    _, ifst = next(iter(pairs))\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.replace(pairs, ofst, root_label,\n                      call_arc_labeling, return_arc_labeling, return_label)\n    return ofst\n\n\ndef reverse(ifst, require_superinitial=True):\n    """"""\n    Constructively reverses an FST\'s transduction.\n\n    This operation reverses an FST. If A transduces string x to y with weight a,\n    then the reverse of A transduces the reverse of x to the reverse of y with\n    weight a.Reverse(). (Typically, a = a.Reverse() and Arc = RevArc, e.g.,\n    TropicalWeight and LogWeight.) In general, e.g., when the weights only form\n    a left or right semiring, the output arc type must match the input arc type.\n\n    Args:\n        ifst: The input FST.\n        require_superinitial: Should a superinitial state be created?\n\n    Returns:\n        A reversed FST.\n    """"""\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.reverse(ifst, ofst, require_superinitial)\n    return ofst\n\n\ndef rmepsilon(ifst, connect=True, reverse=False, queue_type=""auto"",\n              delta=_weight.DELTA, weight=None, nstate=_fst.NO_STATE_ID):\n    """"""\n    Constructively removes epsilon transitions from an FST.\n\n    This operation removes epsilon transitions (those where both input and\n    output labels are epsilon) from an FST.\n\n    Args:\n        ifst: The input FST.\n        connect: Should output be trimmed?\n        reverse: Should epsilon transitions be removed in reverse order?\n        queue_type: A string matching a known queue type; one of: ""auto"",\n            ""fifo"", ""lifo"", ""shortest"", ""state"", ""top"".\n        delta: Comparison/quantization delta (default: 0.0009765625).\n        weight: A Weight in the FST semiring or an object that can be converted\n            to a Weight in the FST semiring indicating the desired weight\n            threshold; paths with weights below this threshold will be pruned.\n        nstate: State number threshold (default: -1).\n\n    Returns:\n        An equivalent FST with no epsilon transitions.\n    """"""\n    try:\n        queue_type = _getters.GetQueueType(queue_type)\n    except ValueError:\n        raise ValueError(""Unknown queue type: {!r}"".format(queue_type))\n    # Threshold is set to semiring Zero (no pruning) if weight is None.\n    weight = _get_weight_or_default(ifst._weight_factory, weight, False)\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.rmepsilon_cons(ifst, ofst, connect, reverse,\n                             queue_type, delta, weight, nstate)\n    return ofst\n\n\ndef shortestdistance(ifst, reverse=False, source=_fst.NO_STATE_ID,\n                     queue_type=""auto"", delta=_weight.DELTA):\n    """"""\n    Compute the shortest distance from the initial or final state.\n\n    This operation computes the shortest distance from the initial state (when\n    `reverse` is False) or from every state to the final state (when `reverse`\n    is True). The shortest distance from p to q is the \\\\otimes-sum of the\n    weights of all the paths between p and q. The weights must be right (if\n    `reverse` is False) or left (if `reverse` is True) distributive, and\n    k-closed (i.e., 1 \\\\otimes x \\\\otimes x^2 \\\\otimes ... \\\\otimes x^{k + 1} = 1\n    \\\\otimes x \\\\otimes x^2 \\\\otimes ... \\\\otimes x^k; e.g., TropicalWeight).\n\n    Args:\n        ifst: The input FST.\n        reverse: Should the reverse distance (from each state to the final\n            state) be computed?\n        source: Source state (this is ignored if `reverse` is True).\n            If NO_STATE_ID (-1), use FST\'s initial state.\n        queue_type: A string matching a known queue type; one of: ""auto"",\n            ""fifo"", ""lifo"", ""shortest"", ""state"", ""top"" (this is ignored if\n            `reverse` is True).\n        delta: Comparison/quantization delta (default: 0.0009765625).\n\n    Returns:\n        A list of Weight objects representing the shortest distance for each\n        state.\n    """"""\n    try:\n        queue_type = _getters.GetQueueType(queue_type)\n    except ValueError:\n        raise ValueError(""Unknown queue type: {!r}"".format(queue_type))\n    return ifst._ops.shortestdistance(ifst, reverse, source, queue_type, delta)\n\n\ndef shortestpath(ifst, nshortest=1, unique=False, queue_type=""auto"",\n                 delta=_weight.DELTA, weight=None, nstate=_fst.NO_STATE_ID):\n    """"""\n    Construct an FST containing the shortest path(s) in the input FST.\n\n    This operation produces an FST containing the n-shortest paths in the input\n    FST. The n-shortest paths are the n-lowest weight paths w.r.t. the natural\n    semiring order. The single path that can be read from the ith of at most n\n    transitions leaving the initial state of the resulting FST is the ith\n    shortest path. The weights need to be right distributive and have the path\n    property. They also need to be left distributive as well for n-shortest with\n    n > 1 (e.g., TropicalWeight).\n\n    Args:\n        ifst: The input FST.\n        nshortest: The number of paths to return.\n        unique: Should the resulting FST only contain distinct paths? (Requires\n            the input FST to be an acceptor; epsilons are treated as if they are\n            regular symbols.)\n        queue_type: A string matching a known queue type; one of: ""auto"",\n            ""fifo"", ""lifo"", ""shortest"", ""state"", ""top"".\n        delta: Comparison/quantization delta (default: 0.0009765625).\n        weight: A Weight in the FST semiring or an object that can be converted\n            to a Weight in the FST semiring indicating the desired weight\n            threshold below which paths are pruned; if omitted, no paths are\n            pruned.\n        nstate: State number threshold (default: -1).\n\n    Returns:\n        An FST containing the n-shortest paths.\n    """"""\n    try:\n        queue_type = _getters.GetQueueType(queue_type)\n    except ValueError:\n        raise ValueError(""Unknown queue type: {!r}"".format(queue_type))\n    # Threshold is set to semiring Zero (no pruning) if weight is None.\n    weight = _get_weight_or_default(ifst._weight_factory, weight, False)\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.shortestpath(ifst, ofst, nshortest, unique,\n                           queue_type, delta, weight, nstate)\n    return ofst\n\n\ndef statemap(ifst, map_type):\n    """"""\n    Constructively applies a transform to all states.\n\n    This operation transforms each state according to the requested map type.\n    Note that currently, only one state-mapping operation is supported.\n\n    Args:\n        ifst: The input FST.\n        map_type: A string matching a known mapping operation; one of:\n            ""arc_sum"" (sum weights of identically-labeled multi-arcs),\n            ""arc_unique"" (deletes non-unique identically-labeled multi-arcs).\n\n    Returns:\n        An FST with states remapped.\n\n    Raises:\n        ValueError: Unknown map type.\n\n    See also: `arcmap`.\n    """"""\n    return arcmap(ifst, _weight.DELTA, map_type, None)\n\n\ndef synchronize(ifst):\n    """"""\n    Constructively synchronizes an FST.\n\n    This operation synchronizes a transducer. The result will be an equivalent\n    FST that has the property that during the traversal of a path, the delay is\n    either zero or strictly increasing, where the delay is the difference\n    between the number of non-epsilon output labels and input labels along the\n    path. For the algorithm to terminate, the input transducer must have bounded\n    delay, i.e., the delay of every cycle must be zero.\n\n    Args:\n        ifst: The input FST.\n\n    Returns:\n        An equivalent synchronized FST.\n    """"""\n    ofst = ifst._mutable_fst_type()\n    ifst._ops.synchronize(ifst, ofst)\n    return ofst\n\n\n# Utility functions\n\ndef indices_to_symbols(symbol_table, indices):\n    """"""Converts indices to symbols by looking them up in the symbol table.\n\n    Args:\n        symbol_table (SymbolTable): The symbol table.\n        indices (List[int]): The list of indices.\n\n    Returns:\n        List[str]: The list of symbols corresponding to the given indices.\n\n    Raises:\n        KeyError: If an index is not found in the symbol table.\n    """"""\n    symbols = []\n    for index in indices:\n        symbol = symbol_table.find_symbol(index)\n        if symbol == """":\n            raise KeyError(""Index {} is not found in the symbol table.""\n                           .format(index))\n        symbols.append(symbol)\n    return symbols\n\n\ndef symbols_to_indices(symbol_table, symbols):\n    """"""Converts symbols to indices by looking them up in the symbol table.\n\n    Args:\n        symbol_table (SymbolTable): The symbol table.\n        indices (List[str]): The list of symbols.\n\n    Returns:\n        List[int]: The list of indices corresponding to the given symbols.\n\n    Raises:\n        KeyError: If a symbol is not found in the symbol table.\n    """"""\n    indices = []\n    for symbol in symbols:\n        index = symbol_table.find_index(symbol)\n        if index == -1:\n            raise KeyError(""Symbol {} is not found in the symbol table.""\n                           .format(symbol))\n        indices.append(index)\n    return indices\n\n\n# Kaldi I/O\n\ndef read_fst_kaldi(rxfilename):\n    """"""Reads FST using Kaldi I/O mechanisms.\n\n    Does not support reading in text mode.\n\n    Args:\n        rxfilename (str): Extended filename for reading the FST.\n\n    Returns:\n        An FST object.\n\n    Raises:\n        IOError: If reading fails.\n        TypeError: If FST type or arc type is not supported.\n    """"""\n    with _util_io.xopen(rxfilename) as ki:\n        rxfilename = _util_io.printable_rxfilename(rxfilename)\n        if not ki.stream().good():\n            raise IOError(""Could not open {} for reading."".format(rxfilename))\n        hdr = _fst.FstHeader()\n        if not hdr.read(ki.stream(), rxfilename):\n            raise IOError(""Error reading FST header."")\n        fst_type = hdr.fst_type()\n        if fst_type not in [""vector"", ""const""]:\n            raise TypeError(""Unsupported FST type: {}."".format(fst_type))\n        arc_type = hdr.arc_type()\n        if arc_type == StdArc.type():\n            if fst_type == ""vector"":\n                fst_class = StdVectorFst\n            elif fst_type == ""const"":\n                fst_class = StdConstFst\n        elif arc_type == LogArc.type():\n            if fst_type == ""vector"":\n                fst_class = LogVectorFst\n            elif fst_type == ""const"":\n                fst_class = LogConstFst\n        elif arc_type == LatticeArc.type():\n            if fst_type == ""vector"":\n                fst_class = LatticeVectorFst\n            elif fst_type == ""const"":\n                fst_class = LatticeConstFst\n        elif arc_type == CompactLatticeArc.type():\n            if fst_type == ""vector"":\n                fst_class = CompactLatticeVectorFst\n            elif fst_type == ""const"":\n                fst_class = CompactLatticeConstFst\n        elif arc_type == KwsIndexArc.type():\n            if fst_type == ""vector"":\n                fst_class = KwsIndexVectorFst\n            elif fst_type == ""const"":\n                fst_class = KwsIndexConstFst\n        else:\n            raise TypeError(""Unsupported FST arc type: {}."".format(arc_type))\n        ropts = _fst.FstReadOptions(rxfilename, hdr)\n        fst = fst_class.read_from_stream(ki.stream(), ropts)\n        if not fst:\n            raise IOError(""Error reading FST (after reading header)."")\n        return fst\n\n\ndef write_fst_kaldi(fst, wxfilename):\n    """"""Writes FST using Kaldi I/O mechanisms.\n\n    FST is written in binary mode without Kaldi binary mode header.\n\n    Args:\n        fst: The FST to write.\n        wxfilename (str): Extended filename for writing the FST.\n\n    Raises:\n        IOError: If writing fails.\n    """"""\n    with _util_io.xopen(wxfilename, ""wb"", write_header=False) as ko:\n        wxfilename = _util_io.printable_wxfilename(wxfilename)\n        if not ko.stream().good():\n            raise IOError(""Could not open {} for writing."".format(wxfilename))\n        wopts = _fst.FstWriteOptions(wxfilename)\n        try:\n            if not fst.write_to_stream(ko.stream(), wopts):\n                raise IOError(""Error writing FST."")\n        except RuntimeError as err:\n            raise IOError(""{}"".format(err))\n\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/fstext/_clat_ops.py,0,b'from _clat_inplace_ops import *\nfrom _clat_construct1_ops import *\nfrom _clat_construct2_ops import *\n'
kaldi/fstext/_index_ops.py,0,b'from _index_inplace_ops import *\nfrom _index_construct1_ops import *\nfrom _index_construct2_ops import *\n'
kaldi/fstext/_lat_ops.py,0,b'from _lat_inplace_ops import *\nfrom _lat_construct1_ops import *\nfrom _lat_construct2_ops import *\n'
kaldi/fstext/_log_ops.py,0,b'from _log_inplace_ops import *\nfrom _log_construct1_ops import *\nfrom _log_construct2_ops import *\n'
kaldi/fstext/_std_ops.py,0,b'from _std_inplace_ops import *\nfrom _std_construct1_ops import *\nfrom _std_construct2_ops import *\n'
kaldi/fstext/enums.py,0,"b""from _getters import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/fstext/special.py,0,"b'from ._deterministic_fst import *\nfrom ._context_fst import *\nfrom ._grammar_context_fst import *\nfrom . import _determinize_lattice\nfrom ._push_special import *\nfrom . import _special_ops\nfrom ._special_ops import *\nfrom ._table_matcher import *\nfrom ._table_matcher_ext import *\n\nfrom .. import fstext as _fst\nimport _getters\nimport _weight\n\n\ndef compose_context(disambig_syms, N, P, ifst):\n    """"""Creates a context FST and composes it on the left with input fst.\n\n    Outputs the label information along with the composed FST. Input FST should\n    be mutable since the algorithm adds the subsequential loop to it.\n\n    Args:\n        disambig_syms (List[int]): Disambiguation symbols.\n        N (int): Size of context window.\n        P (int): Position of central phone in context window, from 0..N-1.\n        ifst (StdFst): Input FST.\n\n    Returns:\n        Tuple[StdVectorFst, List[List[int]]]: Output fst, label information tuple.\n    """"""\n    ofst = _fst.StdVectorFst()\n    ilabels_out = _special_ops._compose_context(disambig_syms, N, P, ifst, ofst)\n    return ofst, ilabels_out\n\n\ndef compose_deterministic_on_demand_fst(fst1, fst2, inverse=False):\n    """"""Composes an FST with a deterministic on demand FST.\n\n    If inverse is True, computes `ofst = Compose(Inverse(fst2), fst1)`. Note\n    that the arguments are reversed in this case.\n\n    This function does not trim its output.\n\n    Args:\n        fst1 (StdFst): The input FST.\n        fst2 (StdDeterministicOnDemandFst):\n            The input deterministic on demand FST.\n        inverse (bool): Deterministic FST on the left?\n\n    Returns:\n        A composed FST.\n    """"""\n    ofst = _fst.StdVectorFst()\n    if inverse:\n        _special_ops._compose_deterministic_on_demand_inverse(fst1, fst2, ofst)\n    else:\n        _special_ops._compose_deterministic_on_demand(fst1, fst2, ofst)\n    return ofst\n\n\ndef determinize_lattice(ifst, compact_output=True,\n                        delta=_weight.DELTA, max_mem=-1, max_loop=-1):\n    """"""Determinizes lattice.\n\n    Implements a special form of determinization with epsilon removal, optimized\n    for a phase of lattice generation.\n\n    See `kaldi/src/fstext/determinize-lattice.h`_ for details.\n\n    Args:\n        ifst (LatticeFst): Input lattice.\n        compact_output (bool): Whether the output is a compact lattice.\n        delta (float): Comparison/quantization delta.\n        max_mem (int): If positive, determinization will fail when the\n            algorithm\'s (approximate) memory consumption crosses this threshold.\n        max_loop (int): If positive, can be used to detect non-determinizable\n            input (a case that wouldn\'t be caught by max_mem).\n\n    Returns:\n        A determized lattice.\n\n    Raises:\n        RuntimeError: If determization fails.\n\n    .. _kaldi/src/fstext/determinize-lattice.h:\n       http://kaldi-asr.org/doc/determinize-lattice_8h_source.html\n    """"""\n    opts = _determinize_lattice.DeterminizeLatticeOptions()\n    opts.delta, opts.max_mem, opts.max_loop = delta, max_mem, max_loop\n    if compact_output:\n        ofst = _fst.CompactLatticeVectorFst()\n        success = _special_ops._determinize_lattice_to_compact(ifst, ofst, opts)\n    else:\n        ofst = _fst.LatticeVectorFst()\n        success = _special_ops._determinize_lattice(ifst, ofst, opts)\n    if success:\n        return ofst\n    else:\n        raise RuntimeError(""Lattice determinization failed."")\n\n\ndef determinize_star(ifst, delta=_weight.DELTA,\n                     max_states=-1, allow_partial=False):\n    """"""Implements a special determinization with epsilon removal.\n\n    See `kaldi/src/fstext/determinize-star.h`_ for details.\n\n    Args:\n        ifst (StdFst): Input fst over the tropical semiring.\n        delta (float): Comparison/quantization delta.\n        max_states (int): If positive, determinization will fail when max states\n            is reached.\n        allow_partial (bool): If True, the algorithm will output partial results\n            when the specified max states is reached (when larger than zero),\n            instead of raising an exception.\n\n    Returns:\n        A determized lattice.\n\n    Raises:\n        RuntimeError: If determization fails.\n\n    .. _kaldi/src/fstext/determinize-star.h:\n       http://kaldi-asr.org/doc/determinize-star_8h_source.html\n    """"""\n    ofst = _fst.StdVectorFst()\n    success = _special_ops._determinize_star(ifst, ofst, delta, max_states,\n                                             allow_partial)\n    if success:\n        return ofst\n    else:\n        raise RuntimeError(""Determinization failed."")\n\n\ndef push_in_log(ifst, push_weights=False, push_labels=False,\n                remove_common_affix=False, remove_total_weight=False,\n                to_final=False, delta=_weight.DELTA):\n    """"""Push weights/labels in log semiring.\n\n    Destructively pushes weights/labels towards initial or final states.\n\n    Args:\n        fst (StdVectorFst): Input fst over the tropical semiring.\n        push_weights: Should weights be pushed?\n        push_labels: Should labels be pushed?\n        remove_common_affix: If pushing labels, should common prefix/suffix be\n            removed?\n        remove_total_weight: If pushing weights, should total weight be removed?\n        to_final: Push towards final states?\n        delta: Comparison/quantization delta (default: 0.0009765625).\n    """"""\n    flags = _getters.GetPushFlags(push_weights, push_labels,\n                                  remove_common_affix, remove_total_weight)\n    _special_ops._push_in_log(ifst, flags, delta, to_final)\n\n\ndef remove_eps_local(fst, special=False):\n    """"""Removes epsilon arcs locally.\n\n    Removes some (but not necessarily all) epsilons in an FST, using an\n    algorithm that is guaranteed to never increase the number of arcs in the\n    FST (and will also never increase the number of states).\n\n    See `kaldi/src/fstext/remove-eps-local.h`_ for details.\n\n    Args:\n        fst (StdVectorFst): Input fst over the tropical semiring.\n        special (bool): Preserve stochasticity when casting to log semiring.\n\n    .. _kaldi/src/fstext/remove-eps-local.h:\n     http://kaldi-asr.org/doc/remove-eps-local_8h_source.html\n    """"""\n    if special:\n        _special_ops._remove_eps_local_special(fst)\n    else:\n        _special_ops._remove_eps_local(fst)\n\n\n################################################################################\n\n__all__ = [name for name in dir() if name[0] != \'_\']\n'"
kaldi/fstext/utils.py,0,"b'from .. import fstext as _fstext\nfrom . import _fst\nfrom . import _fstext_shims\nfrom . import _fstext_utils_inl\n\nfrom ._fstext_shims import *\nfrom ._fstext_utils import *\nfrom ._fstext_utils_inl import *\nfrom ._lattice_utils import *\n\n\ndef convert_lattice_to_compact_lattice(ifst, invert=True):\n    """"""Converts lattice to compact lattice.\n\n    Args:\n        ifst (LatticeFst): The input lattice.\n        invert (bool): Invert input and output labels.\n\n    Returns:\n        CompactLatticeVectorFst: The output compact lattice.\n    """"""\n    ofst = _fstext.CompactLatticeVectorFst()\n    _fstext_shims._convert_lattice_to_compact_lattice(ifst, ofst, invert)\n    return ofst\n\n\ndef convert_compact_lattice_to_lattice(ifst, invert=True):\n    """"""Converts compact lattice to lattice.\n\n    Args:\n        ifst (CompactLatticeFst): The input compact lattice.\n        invert (bool): Invert input and output labels.\n\n    Returns:\n        LatticeVectorFst: The output lattice.\n    """"""\n    ofst = _fstext.LatticeVectorFst()\n    _fstext_shims._convert_compact_lattice_to_lattice(ifst, ofst, invert)\n    return ofst\n\n\ndef convert_lattice_to_std(ifst):\n    """"""Converts lattice to FST over tropical semiring.\n\n    Args:\n        ifst (LatticeFst): The input lattice.\n\n    Returns:\n        StdVectorFst: The output FST.\n    """"""\n    ofst = _fstext.StdVectorFst()\n    _fstext_shims._convert_lattice_to_std(ifst, ofst)\n    return ofst\n\n\ndef convert_std_to_lattice(ifst):\n    """"""Converts FST over tropical semiring to lattice.\n\n    Args:\n        ifst (StdFst): The input FST.\n\n    Returns:\n        LatticeVectorFst: The output lattice.\n    """"""\n    ofst = _fstext.LatticeVectorFst()\n    _fstext_shims._convert_std_to_lattice(ifst, ofst)\n    return ofst\n\n\ndef get_linear_symbol_sequence(fst):\n    """"""Extracts linear symbol sequences from the input FST.\n\n    Args:\n        fst: The input FST.\n\n    Returns:\n        The tuple (isymbols, osymbols, total_weight).\n    """"""\n    if isinstance(fst, _fst.StdFst):\n        return _fstext_utils_inl._get_linear_symbol_sequence_from_std(fst)\n    elif isinstance(fst, _fst.LatticeFst):\n        return _fstext_shims._get_linear_symbol_sequence_from_lattice(fst)\n    elif isinstance(fst, _fst.CompactLatticeFst):\n        return _fstext_shims._get_linear_symbol_sequence_from_compact_lattice(fst)\n    else:\n        raise TypeError(""Input FST arc type is not supported."")\n\n\n################################################################################\n\n__all__ = [name for name in dir() if name[0] != \'_\']\n'"
kaldi/fstext/weight.py,0,"b'""""""\nPyKaldi has support for the following weight types:\n\n#. Tropical weight.\n#. Log weight.\n#. Lattice weight.\n#. Compact lattice weight.\n#. KWS time weight.\n#. KWS index weight.\n\n.. autoconstant:: DELTA\n.. autoconstant:: LEFT_SEMIRING\n.. autoconstant:: RIGHT_SEMIRING\n.. autoconstant:: SEMIRING\n.. autoconstant:: COMMUTATIVE\n.. autoconstant:: IDEMPOTENT\n.. autoconstant:: PATH\n.. autoconstant:: NUM_RANDOM_WEIGHTS\n""""""\n\nfrom _weight import *\nfrom ._float_weight import *\nfrom ._lattice_weight import *\nfrom ._lexicographic_weight import *\n\n################################################################################\n\n__all__ = [name for name in dir() if name[0] != \'_\']\n'"
kaldi/gmm/__init__.py,0,"b""from ._gmm import *\nfrom . import am\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/gmm/_gmm.py,0,"b'from ._model_common import *\nfrom ._diag_gmm import *\nfrom ._full_gmm import *\nfrom . import _full_gmm_ext\nfrom ._full_gmm_normal import *\nfrom ._mle_diag_gmm import *\nfrom ._mle_full_gmm import *\n\nfrom .. import matrix as _matrix\nfrom kaldi.matrix._matrix import _matrix_wrapper\nfrom kaldi.matrix.packed import _sp_matrix_wrapper\n\n\nclass DiagGmm(DiagGmm):\n    """"""Gaussian Mixture Model with diagonal covariances.\n\n    Args:\n        nmix (int): Number of Gaussians to mix\n        dim (int): Dimension of each component\n    """"""\n    def __init__(self, nmix = 0, dim = 0):\n        """"""Creates a new DiagGmm with specified number of gaussian mixtures\n         and dimensions.\n\n        Args:\n            nmix (int): number of gaussian to mix\n            dim (int): dimension\n        """"""\n        super(DiagGmm, self).__init__()\n        if nmix < 0 or dim < 0:\n            raise ValueError(""nmix and dimension must be a positive integer."")\n        if nmix > 0 and dim > 0:\n            self.resize(nmix, dim)\n\n    def copy(self, src):\n        """"""Copies data from src into this DiagGmm and returns this DiagGmm.\n\n        Args:\n            src (FullGmm or DiagGmm): Source Gmm to copy\n\n        Returns:\n            This DiagGmm after update.\n        """"""\n        if isinstance(src, FullGmm):\n            self.copy_from_full(src)\n        elif isinstance(src, DiagGmm):\n            self.copy_from_diag(src)\n        else:\n            raise ValueError(""src must be either FullGmm or DiagGmm"")\n        return self\n\n    def component_posteriors(self, data):\n        """"""Computes the posterior probabilities of all Gaussian components given\n         a data point.\n\n        Args:\n            data (VectorBase): Data point with the same dimension as each\n                component.\n\n        Returns:\n            2-element tuple containing\n\n            - **loglike** (:class:`float`): Log-likelihood\n            - **posteriors** (:class:`~kaldi.matrix.Vector`): Vector with the\n              posterior probabilities\n\n        Raises:\n            ValueError if data is not consistent with gmm dimension.\n        """"""\n        if data.dim != self.dim():\n            raise ValueError(""data dimension {} does not match gmm dimension {}""\n                             .format(data.dim, self.dim()))\n        posteriors = _matrix.Vector(self.num_gauss())\n        loglike = self._component_posteriors(data, posteriors)\n        return loglike, posteriors\n\n\nclass FullGmm(FullGmm):\n    """"""Python wrapper for Kaldi::FullGmm<Float>\n\n    Provides a more pythonic access to the C++ methods.\n\n    Args:\n        nmix (int): number of gaussian to mix\n        dim (int): dimension of each gaussian\n\n    Raises:\n        ValueError if nmix or dimension are not positive integers.\n    """"""\n    def __init__(self, nmix = 0, dim = 0):\n        """"""Creates a new FullGmm with specified number of gaussian mixtures and\n        dimensions.\n\n        Args:\n            nmix (int): number of gaussian to mix\n            dim (int): dimension\n        """"""\n        super(FullGmm, self).__init__()\n        if nmix < 0 or dim < 0:\n            raise ValueError(""nmix and dimension must be a positive integer."")\n        if nmix > 0 and dim > 0:\n            self.resize(nmix, dim)\n\n    def copy(self, src):\n        """"""Copies data from src into this FullGmm and returns this FullGmm.\n\n        Args:\n            src (FullGmm or DiagGmm): Source Gmm to copy\n\n        Returns:\n            This FullGmm after update.\n        """"""\n        if isinstance(src, FullGmm):\n            self.copy_from_full(src)\n        elif isinstance(src, DiagGmm):\n            _full_gmm_ext.copy_from_diag(self, src)\n        else:\n            raise ValueError(""src must be either FullGmm or DiagGmm"")\n        return self\n\n    def component_posteriors(self, data):\n        """"""Computes the posterior probabilities of all Gaussian components given\n         a data point.\n\n        Args:\n            data (VectorBase): Data point with the same dimension as each\n                component.\n\n        Returns:\n            2-element tuple containing\n\n            - **loglike** (:class:`float`): Log-likelihood\n            - **posteriors** (:class:`~kaldi.matrix.Vector`): Vector with the\n              posterior probabilities\n\n        Raises:\n            ValueError if data is not consistent with gmm dimension.\n        """"""\n        if data.dim != self.dim():\n            raise ValueError(""data dimension {} does not match gmm dimension {}""\n                             .format(data.dim, self.dim()))\n        posteriors = _matrix.Vector(self.num_gauss())\n        loglike = self._component_posteriors(data, posteriors)\n        return loglike, posteriors\n\n    def set_weights(self, weights):\n        """"""Sets gmm mixture weights.""""""\n        if not isinstance(weights, _matrix.Vector):\n            weights = _matrix.Vector(weights)\n        self._set_weights(weights)\n\n    def set_means(self, means):\n        """"""Sets gmm component means.""""""\n        if not isinstance(means, _matrix.Matrix):\n            means = _matrix.Matrix(means)\n        self._set_means(means)\n\n    def inv_covars(self):\n        """"""\n        Returns:\n            Component inverse covariances\n        """"""\n        return [_sp_matrix_wrapper(sp) for sp in self._inv_covars()]\n\n    def get_covars(self):\n        """"""\n        Returns:\n            Component Covariances\n        """"""\n        return [_sp_matrix_wrapper(sp) for sp in self._get_covars()]\n\n    def get_covars_and_means(self):\n        """"""\n        Returns:\n            Component Covariances\n        """"""\n        covars, means = self._get_covars_and_means()\n        covars = [_sp_matrix_wrapper(sp) for sp in covars]\n        means = _matrix_wrapper(means)\n        return covars, means\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/gmm/am.py,0,"b""from ._am_diag_gmm import *\nfrom ._decodable_am_diag_gmm import *\nfrom ._mle_am_diag_gmm import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/hmm/__init__.py,0,"b'""""""\n------------\nHMM Topology\n------------\n\nThe following would be the text form for the ""normal"" HMM topology. Note that\nthe first state is the start state, and the final state, which must have no\noutput transitions and must be nonemitting, has an exit probability of one (no\nother state can have nonzero exit probability; you can treat the transition\nprobability to the final state as an exit probability).\n\nNote also that it\'s valid to omit the ""<PdfClass>"" entry of the <State>, which\nwill mean we won\'t have a pdf on that state [non-emitting state].  This is\nequivalent to setting the <PdfClass> to -1.  We do this normally just for the\nfinal state.\n\nThe Topology object can have multiple <TopologyEntry> blocks. This is useful if\nthere are multiple types of topology in the system. ::\n\n  <Topology>\n  <TopologyEntry>\n  <ForPhones> 1 2 3 4 5 6 7 8 </ForPhones>\n  <State> 0 <PdfClass> 0\n  <Transition> 0 0.5\n  <Transition> 1 0.5\n  </State>\n  <State> 1 <PdfClass> 1\n  <Transition> 1 0.5\n  <Transition> 2 0.5\n  </State>\n  <State> 2 <PdfClass> 2\n  <Transition> 2 0.5\n  <Transition> 3 0.5\n  <Final> 0.5\n  </State>\n  <State> 3\n  </State>\n  </TopologyEntry>\n  </Topology>\n\n`NO_PDF` is used where pdf_class or pdf would be used, to indicate, none is\nthere.  Mainly useful in skippable models, but also used for end states.\n\nA caveat with non-emitting states is that their out-transitions are not\ntrainable, due to technical issues with the way we decided to accumulate the\nstats.  Any transitions arising from (*) HMM states with `NO_PDF` as the label\nare second-class transitions, They do not have ""transition-states"" or\n""transition-ids"" associated with them.  They are used to create the FST version\nof the HMMs, where they lead to epsilon arcs.\n\n(*) ""arising from"" is a bit of a technical term here, due to the way (if\nreorder == true), we put the transition-id associated with the outward arcs of\nthe state, on the input transition to the state.\n\n----------------\nTransition Model\n----------------\n\nThe class `TransitionModel` is a repository for the transition probabilities.\nIt also handles certain integer mappings.\n\nThe basic model is as follows.  Each phone has a HMM topology. Each HMM-state of\neach of these phones has a number of transitions (and final-probs) out of it.\nEach HMM-state defined in the `HmmTopology` class has an associated ""pdf_class"".\nThis gets replaced with an actual pdf-id via the tree.  The transition model\nassociates the transition probs with the (phone, HMM-state, pdf-id).  We\nassociate with each such triple a transition-state.  Each transition-state has a\nnumber of associated probabilities to estimate; this depends on the number of\ntransitions/final-probs in the topology for that (phone, HMM-state).  Each\nprobability has an associated transition-index. We associate with each\n(transition-state, transition-index) a unique transition-id. Each individual\nprobability estimated by the transition-model is asociated with a transition-id.\n\nList of the various types of quantity referred to here and what they mean:\n\nphone\n    a phone index (1, 2, 3 ...)\n\nHMM-state\n    a number (0, 1, 2...) that indexes TopologyEntry (see hmm-topology.h)\n\npdf-id\n    a number output by the compute method of `ContextDependency` (it indexes\n    pdf\'s, either forward or self-loop). Zero-based.\n\ntransition-state\n    the states for which we estimate transition probabilities for transitions\n    out of them.  In some topologies, will map one-to-one with pdf-ids.\n    One-based, since it appears on FSTs.\n\ntransition-index\n    identifier of a transition (or final-prob) in the HMM.  Indexes the\n    ""transitions"" vector in `HmmTopology.HmmState`.  [if it is out of range,\n    equal to length of transitions, it refers to the final-prob.] Zero-based.\n\ntransition-id\n    identifier of a unique parameter of the `TransitionModel`. Associated with a\n    (transition-state, transition-index) pair. One-based, since it appears on\n    FSTs.\n\nList of the possible mappings TransitionModel can do::\n\n  Forward mappings:\n\n  (phone, HMM-state, forward-pdf-id, self-loop-pdf-id) -> transition-state\n                  (transition-state, transition-index) -> transition-id\n\n  Reverse mappings:\n                                         transition-id -> transition-state\n                                         transition-id -> transition-index\n                                      transition-state -> phone\n                                      transition-state -> HMM-state\n                                      transition-state -> forward-pdf-id\n                                      transition-state -> self-loop-pdf-id\n\nThe main things the TransitionModel object can do are:\n\n* Get initialized (need ContextDependency and HmmTopology objects).\n\n* Read/write.\n\n* Update [given a vector of counts indexed by transition-id].\n\n* Do the various integer mappings mentioned above.\n\n* Get the probability (or log-probability) associated with a particular\n  transition-id.\n\n----------\n\n.. autoconstant:: NO_PDF\n""""""\n\nfrom ._hmm_topology import *\nfrom ._hmm_utils import *\nfrom ._hmm_utils import _get_h_transducer\nfrom ._hmm_utils_ext import *\nfrom ._posterior import *\nfrom ._posterior_ext import *\nfrom ._transition_model import *\nfrom ._tree_accu import *\nfrom kaldi.fstext import StdVectorFst\n\ndef get_h_transducer(ilabel_info, ctx_dep, trans_model, config):\n  """"""Python wrapper for _hmm_utils.get_h_transducer. Post-process output into StdVectorFst.""""""\n  h_transducer, disambig_syms_left = _get_h_transducer(ilabel_info, ctx_dep, trans_model, config)\n  return StdVectorFst(h_transducer), disambig_syms_left\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n\n'"
kaldi/itf/__init__.py,0,b''
kaldi/ivector/__init__.py,0,"b""from ._voice_activity_detection import *\nfrom ._ivector_extractor import *\nfrom ._logistic_regression import *\nfrom ._plda import *\nfrom ._agglomerative_clustering import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/kws/__init__.py,0,"b'from ._kws_functions import *\n\nfrom .. import fstext as _fst\n\n\ndef lattice_to_kws_index(clat, utterance_id, max_silence_frames=50,\n                         max_states=-1, allow_partial=True, destructive=False):\n    """"""Creates an inverted KWS index of the given lattice.\n\n    The output KWS index is over the `KwsIndexWeight` semiring (a triplet of\n    tropical weights with lexicographic ordering). Input lattice should be\n    topologically sorted. For details of the algorithm, see: Dogan Can and Murat\n    Saraclar, 2011, ""Lattice Indexing for Spoken Term Detection"".\n\n    Args:\n        clat (CompactLatticeVectorFst): The input lattice.\n        utterance_id (int): The integer id to use for the input lattice.\n        max_silence_frames (int): The duration of the longest silence (epsilon)\n            arcs allowed in the output. Longer silence arcs will be removed.\n            If < 0, all silence arcs are kept.\n        max_states (int): The maximum number of states allowed in the output.\n            If <= 0, any number of states in the output is OK.\n        allow_partial (bool):  Whether to allow partial output or skip\n            determinization if determinization fails.\n        destructive (bool): Whether to use the destructive implementation which\n            avoids a copy by modifying the input lattice.\n\n    Returns:\n        KwsIndexVectorFst: The output KWS index FST.\n    """"""\n    if destructive:\n        index = _kws_functions._lattice_to_kws_index_destructive(\n            clat, utterance_id, max_silence_frames, max_states, allow_partial)\n    else:\n        index = _kws_functions._lattice_to_kws_index(\n            clat, utterance_id, max_silence_frames, max_states, allow_partial)\n    return _fst.KwsIndexVectorFst(index)\n\n\ndef search_kws_index(index, keyword, encode_table, n_best=-1, detailed=False):\n    """"""Searches given keyword (FST) inside the KWS index (FST).\n\n    Returns the `n_best` results found. Each result is a tuple of `(utt_id,\n    time_beg, time_end, score)`.\n\n    Since keyword can be an FST, there can be multiple matching paths in the\n    keyword and the index in a given time period. If `detailed == True`, stats\n    output provides the results for all matching paths together with appropriate\n    scores while ilabels output provides the input labels on those paths.\n\n    Args:\n        index (KwsIndexVectorFst): The index FST.\n        keyword (StdVectorFst): The keyword FST.\n        encode_table (KwsIndexEncodeTable): The table to use for decoding\n            output labels into utterance ids. This table is produced by\n            :meth:`encode_kws_disambiguation_symbols`.\n        n_best (int): The number of best results to return. If <= 0, all results\n            found in the index are returned.\n        detailed (bool): Whether to return detailed results representing\n            individual index matches and the input label sequences for those\n            matches. If True, output is a tuple of (results, stats, ilabels).\n    """"""\n    if detailed:\n        results, matched_seq = _kws_functions._search_kws_index_detailed(\n            index, keyword, encode_table, n_best)\n        stats, ilabels = _kws_functions._compute_detailed_statistics(\n            matched_seq, encode_table)\n        return results, stats, ilabels\n    else:\n        return _kws_functions._search_kws_index(index, keyword,\n                                                encode_table, n_best)\n\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/lat/__init__.py,0,"b""from . import align\nfrom . import functions\nfrom . import sausages\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/lat/align.py,0,"b'from . import _phone_align_lattice as _pal\nfrom . import _word_align_lattice as _wal\nfrom . import _word_align_lattice_lexicon as _wall\n\nfrom ._phone_align_lattice import *\nfrom ._word_align_lattice import *\nfrom ._word_align_lattice_lexicon import *\n\nfrom .. import fstext as _fst\nfrom ..util import io as _io\n\n\ndef phone_align_lattice(lat, tmodel, opts):\n    """"""Aligns the phone labels and transition-ids.\n\n    Outputs a lattice in which the arcs correspond exactly to sequences of\n    phones, so the boundaries between the arcs correspond to the boundaries\n    between phones.\n\n    Args:\n        lat (CompactLatticeVectorFst): The input lattice.\n        tmodel (TransitionModel): The transition model.\n        opts (PhoneAlignLatticeOptions): The phone alignment options.\n\n    Returns:\n        A tuple representing the return value and the output lattice. The return\n        value is set to True if the operation was successful, False if some kind\n        of problem was detected, e.g. transition-id sequences in the lattice\n        were incompatible with the model.\n\n    Note:\n        If this function returns False, it doesn\'t mean the output lattice is\n        necessarily bad. It might just be that the input lattice was ""forced\n        out"" with partial words due to no final state being reached during\n        decoding, and in this case the output might still be usable.\n\n    Note:\n        If `opts.remove_epsilon == True` and `opts.replace_output_symbols ==\n        False`, an arc may have >1 phone on it, but the boundaries will still\n        correspond with the boundaries between phones.\n\n    Note:\n        If `opts.replace_output_symbols == False`, it is possible to have arcs\n        with words on them but no transition-ids at all.\n\n    See Also:\n        :meth:`kaldi.lat.functions.convert_lattice_to_phones`\n    """"""\n    success, lat_out = _pal._phone_align_lattice(lat, tmodel, opts)\n    return success, _fst.CompactLatticeVectorFst(lat_out)\n\n\ndef word_align_lattice(lat, tmodel, info, max_states):\n    """"""Aligns the word labels and transition-ids.\n\n    Aligns compact lattice so that each arc has the transition-ids on it that\n    correspond to the word that is on that arc. It is OK for the lattice to\n    have epsilon arcs for optional silences.\n\n    Args:\n        lat (CompactLatticeVectorFst): The input lattice.\n        tmodel (TransitionModel): The transition model.\n        info (WordBoundaryInfo): The word boundary information.\n        max_states (int): Maximum #states allowed in the output lattice. If\n            `max_states > 0` and the #states of the output will be greater than\n            `max_states`, this function will abort the computation, return False\n            and output an empty lattice.\n\n    Returns:\n        A tuple representing the return value and the output lattice. The\n        return value is set to True if the operation was successful, False if\n        some kind of problem was detected, e.g. transition-id sequences in the\n        lattice were incompatible with the word boundary information.\n\n    Note:\n        We don\'t expect silence inside words, or empty words (words with no\n        phones), and we expect the word to start with a wbegin_phone, to end\n        with a wend_phone, and to possibly have winternal_phones inside (or to\n        consist of just one wbegin_and_end_phone).\n\n    Note:\n        If this function returns False, it doesn\'t mean the output lattice is\n        necessarily bad. It might just be that the input lattice was ""forced\n        out"" with partial words due to no final state being reached during\n        decoding, and in this case the output might still be usable.\n    """"""\n    success, lat_out = _wal._word_align_lattice(lat, tmodel, info, max_states)\n    return success, _fst.CompactLatticeVectorFst(lat_out)\n\n\ndef word_align_lattice_lexicon(lat, tmodel, lexicon_info, opts):\n    """"""Aligns the word labels and transition-ids using a lexicon.\n\n    Aligns compact lattice so that each arc has the transition-ids on it that\n    correspond to the word that is on that arc. It is OK for the lattice to\n    have epsilon arcs for optional silences.\n\n    Args:\n        lat (CompactLatticeVectorFst): The input lattice.\n        tmodel (TransitionModel): The transition model.\n        lexicon_info (WordAlignLatticeLexiconInfo): The lexicon information.\n        opts (WordAlignLatticeLexiconOpts): The word alignment options.\n\n    Returns:\n        A tuple representing the return value and the output lattice. The\n        return value is set to True if the operation was successful, False if\n        some kind of problem was detected, e.g. transition-id sequences in the\n        lattice were incompatible with the lexicon information.\n\n    Note:\n        If this function returns False, it doesn\'t mean the output lattice is\n        necessarily bad. It might just be that the input lattice was ""forced\n        out"" with partial words due to no final state being reached during\n        decoding, and in this case the output might still be usable.\n    """"""\n    success, lat_out = _wall._word_align_lattice_lexicon(lat, tmodel,\n                                                         lexicon_info, opts)\n    return success, _fst.CompactLatticeVectorFst(lat_out)\n\n\ndef read_lexicon_for_word_align(rxfilename):\n    """"""Reads the lexicon in the special format required for word alignment.\n\n    Each line has a series of integers on it (at least two on each line),\n    representing:\n\n    <old-word-id> <new-word-id> [<phone-id-1> [<phone-id-2> ... ] ]\n\n    Here, <old-word-id> is the word-id that appears in the lattice before\n    alignment, and <new-word-id> is the word-is that should appear in the\n    lattice after alignment. This is mainly useful when the lattice may have\n    no symbol for the optional-silence arcs (so <old-word-id> would equal\n    zero), but we want it to be output with a symbol on those arcs (so\n    <new-word-id> would be nonzero). If the silence should not be added to\n    the lattice, both <old-word-id> and <new-word-id> may be zero.\n\n    Args:\n        rxfilename (str): Extended filename for reading the lexicon.\n\n    Returns\n        List[List[int]]: The lexicon in the format required for word alignment.\n\n    Raises:\n        ValueError: If reading the lexicon fails.\n    """"""\n    with _io.xopen(rxfilename) as ki:\n        if ki.binary:\n            raise IOError(""Not expecting binary file for lexicon."")\n        return _wall._read_lexicon_for_word_align(ki.stream())\n\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/lat/functions.py,0,"b'import logging\n\nfrom . import _confidence\nfrom . import _determinize_lattice_pruned as _dlp\nfrom . import _lattice_functions as _lat_fun\n\nfrom ._confidence import *\nfrom ._compose_lattice_pruned import *\nfrom ._determinize_lattice_pruned import *\nfrom ._lattice_functions import *\nfrom ._minimize_lattice import *\nfrom ._push_lattice import *\n\nfrom .. import fstext as _fst\nfrom ..fstext import _api\n\n\ndef sentence_level_confidence(lat):\n    """"""Computes sentence level confidence scores.\n\n    If input is a compact lattice, this function requires that distinct paths in\n    `lat` have distinct word sequences; this will automatically be the case if\n    `lat` was generated by a decoder, since a deterministic FST has this\n    property. If input is a state-level lattice, it is first determinized, but\n    this is done in a ""smart"" way so that only paths needed for this operation\n    are generated.\n\n    This function assumes that any acoustic scaling you want to apply,\n    has already been applied.\n\n    The output consists of the following. `confidence` is the score difference\n    between the best path and the second-best path in the lattice (a positive\n    number), or zero if lattice was equivalent to the empty FST (no successful\n    paths), or infinity if there was only one path in the lattice. `num_paths`\n    is a number in `{0, 1, 2}` saying how many n-best paths (up to two) were\n    found. If `num_paths >= 1`, `best_sentence` is the best word-sequence; if\n    `num_paths -= 2`, `second_best_sentence` is the second best word-sequence\n    (this may be useful for testing whether the two best word sequences are\n    somehow equivalent for the task at hand).\n\n    Args\n        lat (LatticeVectorFst or CompactLatticeVectorFst): The input lattice.\n\n    Returns:\n        Tuple[float, int, List[int], List[int]]: The tuple\n        `(confidence, num_paths, best_sentence, second_best_sentence)`.\n\n    Note:\n        This function is not the only way to get confidences in Kaldi. This only\n        gives you sentence-level (utterance-level) confidence. You can get\n        word-by-word confidence within a sentence, along with Minimum Bayes Risk\n        decoding. Also confidences estimated using this function are not very\n        accurate.\n    """"""\n    if isinstance(lat, _fst.CompactLatticeVectorFst):\n        return _confidence._sentence_level_confidence_from_compact_lattice(lat)\n    else:\n        return _confidence._sentence_level_confidence_from_lattice(lat)\n\n\ndef determinize_lattice_phone_pruned(ifst, trans_model, prune,\n                                     opts=None, destructive=True):\n    """"""Applies a specialized determinization operation to a lattice.\n\n    Determinizes a raw state-level lattice, keeping only the best output-symbol\n    sequence (typically transition ids) for each input-symbol sequence. This\n    version does phone insertion when doing a first pass determinization (if\n    `opts.phone_determinize == True`), it then removes the inserted phones and\n    does a second pass determinization on the word lattice (if\n    `opts.word_determinize == True`). It also does pruning as part of the\n    determinization algorithm, which is more efficient and prevents blowup.\n\n    Args:\n        ifst (LatticeFst): The input lattice.\n        trans_model (TransitionModel): The transition model.\n        prune (float): The pruning beam.\n        opts (DeterminizeLatticePhonePrunedOptions): The options for lattice\n            determinization.\n        destructive (bool): Whether to use the destructive version of the\n            algorithm which mutates input lattice.\n\n    Returns:\n        CompactLatticeVectorFst: The output lattice.\n\n    See Also:\n        :meth:`determinize_lattice_pruned`\n\n    Note:\n        The point of doing first a phone-level determinization pass and then a\n        word-level determinization pass is that it allows us to determinize\n        deeper lattices without ""failing early"" and returning a too-small\n        lattice due to the max-mem constraint. The result should be the same\n        as word-level determinization in general, but for deeper lattices it is\n        a bit faster, despite the fact that we now have two passes of\n        determinization by default.\n    """"""\n    if opts is None:\n        opts = DeterminizeLatticePhonePrunedOptions()\n    if not destructive or not isinstance(ifst, _api._MutableFstBase):\n        ifst = _fst.LatticeVectorFst(ifst)\n    ofst = _fst.CompactLatticeVectorFst()\n    success = _dlp._determinize_lattice_phone_pruned_wrapper(trans_model, ifst,\n                                                             prune, ofst, opts)\n    if not success:\n        logging.warning(\n            ""Lattice determinization is terminated early because at least one ""\n            ""of max_mem, max_loop or max_arcs thresholds was reached. If you ""\n            ""want a more detailed log message, rerun this function after ""\n            ""setting verbose level > 0 using kaldi.base.set_verbose_level."")\n    return ofst\n\n\ndef determinize_lattice_pruned(ifst, prune, opts=None, compact_out=True):\n    """"""Applies a specialized determinization operation to a lattice.\n\n    Determinizes a raw state-level lattice, keeping only the best output-symbol\n    sequence (typically transition ids) for each input-symbol sequence. This\n    version does determinization only on the word lattice. The output is\n    represented using either sequences of arcs (if `compact_out == False`),\n    where all but the first one has an epsilon on the input side, or directly as\n    strings using compact lattice weight type (if `compact_out == True`). It\n    also does pruning as part of the determinization algorithm, which is more\n    efficient and prevents blowup.\n\n    Args:\n        ifst (LatticeFst): The input lattice.\n        prune (float): The pruning beam.\n        opts (DeterminizeLatticePrunedOptions): The options for lattice\n            determinization.\n        compact_out (bool): Whether to output a compact lattice.\n\n    Returns:\n        LatticeVectorFst or CompactLatticeVectorFst: The output lattice.\n\n    See Also:\n        :meth:`determinize_lattice_phone_pruned`\n    """"""\n    if opts is None:\n        opts = DeterminizeLatticePrunedOptions()\n    ifst = _fst.LatticeVectorFst(ifst).invert().topsort().arcsort()\n    if compact_out:\n        ofst = _fst.CompactLatticeVectorFst()\n        success = _dlp._determinize_lattice_pruned_to_compact(ifst, prune, ofst,\n                                                              opts)\n    else:\n        ofst = _fst.LatticeVectorFst()\n        success = _dlp._determinize_lattice_pruned(ifst, prune, ofst, opts)\n    if not success:\n        logging.warning(\n            ""Lattice determinization is terminated early because at least one ""\n            ""of max_mem, max_loop or max_arcs thresholds was reached. If you ""\n            ""want a more detailed log message, rerun this function after ""\n            ""setting verbose level > 0 using kaldi.base.set_verbose_level."")\n    return ofst\n\n\ndef lattice_state_times(lat):\n    """"""Extracts lattice state times (in terms of frames).\n\n    Iterates over the states of a topologically sorted lattice and computes\n    the corresponding time instances.\n\n    Args:\n        lat (LatticeVectorFst or CompactLatticeVectorFst): The input lattice.\n\n    Returns:\n        Tuple[int, List[int]]: The number of frames and the state times.\n\n    Note:\n        If input is a regular lattice, the number of frames is equal to the\n        maximum state time in the lattice. If input is a compact lattice,  the\n        number of frames might not be equal to the maximum state time in the\n        lattice due to frames in final states.\n    """"""\n    if isinstance(lat, _fst.LatticeVectorFst):\n        return _lat_fun._lattice_state_times(lat)\n    else:\n        return _lat_fun._compact_lattice_state_times(lat)\n\n\ndef compute_lattice_alphas_and_betas(lat, viterbi):\n    """"""Computes forward and backward scores for lattice states.\n\n    If `viterbi == True`, computes the Viterbi scores, i.e. forward (alpha) and\n    backward (beta) scores are the scores of best paths reaching and leaving\n    each state. Otherwise, computes regular forward and backward scores. Note\n    that alphas and betas are negated costs. Requires the input lattice to be\n    topologically sorted.\n\n    Args:\n        lat (LatticeVectorFst or CompactLatticeVectorFst): The input lattice.\n        viterbi (bool): Whether to compute Viterbi scores.\n\n    Returns:\n        Tuple[float, List[float], List[float]]: The total-prob (or best-path\n        prob), the forward (alpha) scores and the backward (beta) scores.\n    """"""\n    if isinstance(lat, _fst.LatticeVectorFst):\n        return _lat_fun._compute_lattice_alphas_and_betas(lat, viterbi)\n    else:\n        return _lat_fun._compute_compact_lattice_alphas_and_betas(lat, viterbi)\n\n\ndef top_sort_lattice_if_needed(lat):\n    """"""Topologically sorts the lattice if it is not already sorted.\n\n    Args:\n        lat (LatticeVectorFst or CompactLatticeVectorFst): The input lattice.\n\n    Raises:\n        RuntimeError: If lattice cannot be topologically sorted.\n    """"""\n    if isinstance(lat, _fst.LatticeVectorFst):\n        _lat_fun._top_sort_lattice_if_needed(lat)\n    else:\n        _lat_fun._top_sort_compact_lattice_if_needed(lat)\n\n\ndef prune_lattice(beam, lat):\n    """"""Prunes a lattice.\n\n    Args:\n        beam (float): The pruning beam.\n        lat (LatticeVectorFst or CompactLatticeVectorFst): The input lattice.\n\n    Raises:\n        ValueError: If pruning fails.\n    """"""\n    if isinstance(lat, _fst.LatticeVectorFst):\n        _lat_fun._prune_lattice(beam, lat)\n    else:\n        _lat_fun._prune_compact_lattice(beam, lat)\n\n\ndef rescore_lattice(decodable, lat):\n    """"""Adjusts acoustic scores in the lattice.\n\n    This function *adds* the negated scores obtained from the decodable object,\n    to the acoustic scores on the arcs. If you want to replace them, you should\n    use :meth:`scale_compact_lattice` to first set the acoustic scores to zero.\n    The input labels (or the string component of arc weights if the input is a\n    compact lattice), are interpreted as transition-ids or whatever other index\n    the decodable object expects.\n\n    Args:\n        decodable (DecodableInterface): The decodable object.\n        lat (LatticeVectorFst or CompactLatticeVectorFst): The input lattice.\n\n    Raises:\n        ValueError: If the inputs are not compatible.\n\n    See Also:\n        :meth:`rescore_compact_lattice_speedup`\n    """"""\n    if isinstance(lat, _fst.LatticeVectorFst):\n        _lat_fun._rescore_lattice(decodable, lat)\n    else:\n        _lat_fun._rescore_compact_lattice(decodable, lat)\n\n\ndef longest_sentence_length_in_lattice(lat):\n    """"""Returns the number of words in the longest sentence in a lattice.\n\n    Args:\n        lat (LatticeVectorFst or CompactLatticeVectorFst): The input lattice.\n\n    Returns:\n        int: The length of the longest sentence in the lattice.\n\n    """"""\n    if isinstance(lat, _fst.LatticeVectorFst):\n        return _lat_fun._longest_sentence_length_in_lattice(lat)\n    else:\n        return _lat_fun._longest_sentence_length_in_compact_lattice(lat)\n\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/lat/sausages.py,0,"b""from ._sausages import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/lm/__init__.py,0,"b""from ._arpa_file_parser import ArpaParseOptions\nfrom ._arpa_lm_compiler import *\nfrom ._const_arpa_lm import *\nfrom ._kaldi_rnnlm import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/matrix/__init__.py,0,"b'""""""\nPyKaldi defines the following CPU vector/matrix types:\n\n================= ====================== ============================ =========================\nType              32-bit floating point  64-bit floating point        Other\n================= ====================== ============================ =========================\nDense Matrix      :class:`.Matrix`       :class:`.DoubleMatrix`       :class:`.CompressedMatrix`\nDense Vector      :class:`.Vector`       :class:`.DoubleVector`\nSymmetric Matrix  :class:`.SpMatrix`     :class:`.DoubleSpMatrix`\nTriangular Matrix :class:`.TpMatrix`     :class:`.DoubleTpMatrix`\nSparse Matrix     :class:`.SparseMatrix` :class:`.DoubleSparseMatrix`\nSparse Vector     :class:`.SparseVector` :class:`.DoubleSparseMatrix`\n================= ====================== ============================ =========================\n\nIn addition, there is a :class:`.GeneralMatrix` type which is a wrapper around\n:class:`.Matrix`, :class:`.SparseMatrix` and :class:`.CompressedMatrix` types.\n\nThe dense :class:`Vector`/:class:`Matrix` types come in two flavors.\n\n:class:`Vector`/:class:`Matrix` instances own the memory buffers backing them.\nInstantiating a new :class:`Vector`/:class:`Matrix` object allocates new memory\nfor storing the elements. They support destructive operations that reallocate\nmemory.\n\n:class:`SubVector`/:class:`SubMatrix` instances, on the other hand, share the\nmemory buffers owned by other objects. Instantiating a new\n:class:`SubVector`/:class:`SubMatrix` object does not allocate new memory. Since\nthey provide views into other existing objects, they do not support destructive\noperations that reallocate memory. Other than this caveat, they are equivalent\nto :class:`Vector`/:class:`Matrix` instances for all practical purposes. Almost\nany function or method accepting a :class:`Vector`/:class:`Matrix` instance can\ninstead be passed a :class:`SubVector`/:class:`SubMatrix` instance.\n\n.. note::\n    All mutating vector/matrix methods are marked with an underscore suffix.\n    These methods overwrite the contents and return the resulting object,\n    unless they have other return values, to support method chaining.\n""""""\n\nfrom ._matrix import *\nfrom ._str import set_printoptions\n\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/matrix/_matrix.py,0,"b'import sys\nimport numpy\n\nfrom . import _compressed_matrix\nfrom . import _kaldi_matrix\nfrom . import _kaldi_matrix_ext\nfrom . import _kaldi_vector\nfrom . import _kaldi_vector_ext\nfrom . import _matrix_ext\nimport _matrix_common  # FIXME: Relative/absolute import is buggy in Python 3.\nfrom . import _packed_matrix\nfrom . import _sp_matrix\nfrom . import _tp_matrix\nfrom . import _str\n\n################################################################################\n# single precision vector/matrix types\n################################################################################\n\n\nclass _VectorBase(object):\n    """"""Base class defining the additional API for single precision vectors.\n\n    No constructor.\n    """"""\n\n    def copy_(self, src):\n        """"""Copies the elements from another vector.\n\n        Args:\n            src (Vector or DoubleVector): The input vector.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != src.dim:\n            raise ValueError(""Vector of size {} cannot be copied into vector ""\n                             ""of size {}."".format(src.dim, self.dim))\n        if isinstance(src, _kaldi_vector.VectorBase):\n            return self._copy_from_vec_(src)\n        elif isinstance(src, _kaldi_vector.DoubleVectorBase):\n            _kaldi_vector_ext._copy_from_double_vec(self, src)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def clone(self):\n        """"""Clones the vector.\n\n        The clone allocates new memory for its contents and supports vector\n        operations that reallocate memory, i.e. it is not a view.\n\n        Returns:\n            Vector: A copy of the vector.\n        """"""\n        return Vector(self)\n\n    def size(self):\n        """"""Returns the size of the vector as a single element tuple.""""""\n        return (self.dim,)\n\n    @property\n    def shape(self):\n        """"""Single element tuple representing the size of the vector.""""""\n        return self.size()\n\n    def approx_equal(self, other, tol=0.01):\n        """"""Checks if vectors are approximately equal.\n\n        Args:\n            other (Vector): The vector to compare against.\n            tol (float): The tolerance for the equality check.\n                Defaults to ``0.01``.\n\n        Returns:\n            True if `self.dim == other.dim` and\n            `||self-other|| <= tol*||self||`. False otherwise.\n        """"""\n        if not isinstance(other, _kaldi_vector.VectorBase):\n            return False\n        if self.dim != other.dim:\n            return False\n        return self._approx_equal(other, tol)\n\n    def __eq__(self, other):\n        return self.approx_equal(other, 1e-16)\n\n    def numpy(self):\n        """"""Converts the vector to a 1-D NumPy array.\n\n        The NumPy array is a view into the vector, i.e. no data is copied.\n\n        Returns:\n            numpy.ndarray: A NumPy array sharing data with this vector.\n        """"""\n        return _matrix_ext.vector_to_numpy(self)\n\n    @property\n    def data(self):\n        """"""Vector data as a memoryview.""""""\n        return self.numpy().data\n\n    def range(self, start, length):\n        """"""Returns the given range of elements as a new vector view.\n\n        Args:\n            start (int): The start index.\n            length (int): The length.\n\n        Returns:\n            SubVector: A vector view representing the given range.\n        """"""\n        return SubVector(self, start, length)\n\n    def add_vec_(self, alpha, v):\n        """"""Adds another vector.\n\n        Performs the operation :math:`y = y + \\\\alpha\\\\ v`.\n\n        Args:\n            alpha (float): The scalar multiplier.\n            v (Vector or DoubleVector): The input vector.\n\n        Raises:\n          RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(v, _kaldi_vector.VectorBase):\n            return self._add_vec_(alpha, v)\n        elif isinstance(v, _kaldi_vector.DoubleVectorBase):\n            _kaldi_vector_ext._add_double_vec(self, alpha, v)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def add_vec2_(self, alpha, v):\n        """"""Adds the squares of elements from another vector.\n\n        Performs the operation :math:`y = y + \\\\alpha\\\\ v\\\\odot v`.\n\n        Args:\n            alpha (float): The scalar multiplier.\n            v (Vector or DoubleVector): The input vector.\n\n        Raises:\n          RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(v, _kaldi_vector.VectorBase):\n            return self._add_vec2_(alpha, v)\n        elif isinstance(v, _kaldi_vector.DoubleVectorBase):\n            _kaldi_vector_ext._add_double_vec2(self, alpha, v)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def add_mat_vec_(self, alpha, M, trans, v, beta, sparse=False):\n        """"""Computes a matrix-vector product.\n\n        Performs the operation :math:`y = \\\\alpha\\\\ M\\\\ v + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the matrix-vector product.\n            M (Matrix or SpMatrix or TpMatrix): The input matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n            v (Vector): The input vector.\n            beta (int): The scalar multiplier for the destination vector.\n            sparse (bool): Whether to use the algorithm that is faster when\n                **v** is sparse. Defaults to ``False``.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if v.dim != M.num_cols:\n            raise ValueError(""Matrix of size {}x{} cannot be multiplied with ""\n                             ""vector of size {}.""\n                             .format(M.num_rows, M.num_cols, v.dim))\n        if self.dim != M.num_rows:\n            raise ValueError(""Vector of size {} cannot be added to vector of ""\n                             ""size {}."".format(M.num_rows, self.dim))\n        if isinstance(M, _kaldi_matrix.MatrixBase):\n            if sparse:\n                _kaldi_vector_ext._add_mat_svec(self, alpha, M, trans, v, beta)\n            else:\n                _kaldi_vector_ext._add_mat_vec(self, alpha, M, trans, v, beta)\n        elif isinstance(M, _sp_matrix.SpMatrix):\n            _kaldi_vector_ext._add_sp_vec(self, alpha, M, v, beta)\n        elif isinstance(M, _tp_matrix.TpMatrix):\n            _kaldi_vector_ext._add_tp_vec(self, alpha, M, trans, v, beta)\n        return self\n\n    def mul_tp_(self, M, trans):\n        """"""Multiplies the vector with a lower-triangular matrix.\n\n        Performs the operation :math:`y = M\\\\ y`.\n\n        Args:\n            M (TpMatrix): The input lower-triangular matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""Matrix with size {}x{} cannot be multiplied ""\n                             ""with vector of size {}.""\n                             .format(M.num_rows, M.num_cols, self.dim))\n        _kaldi_vector_ext._mul_tp(self, M, trans)\n        return self\n\n    def solve_(self, M, trans):\n        """"""Solves a linear system.\n\n        The linear system is defined as :math:`M\\\\ x = b`, where :math:`b` and\n        :math:`x` are the initial and final values of the vector, respectively.\n\n        Warning:\n            Does not test for :math:`M` being singular or near-singular.\n\n        Args:\n            M (TpMatrix): The input lower-triangular matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""The number of rows of the input matrix ({}) ""\n                             ""should match the size of the vector ({}).""\n                             .format(M.num_rows, self.dim))\n        _kaldi_vector_ext._solve(self, M, trans)\n        return self\n\n    def copy_rows_from_mat_(self, M):\n        """"""Copies the elements from a matrix row-by-row.\n\n        Args:\n            M (Matrix or DoubleMatrix): The input matrix.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows * M.num_cols:\n            raise ValueError(""The number of elements of the input matrix ({}) ""\n                             ""should match the size of the vector ({}).""\n                             .format(M.num_rows * M.num_cols, self.dim))\n        if isinstance(M, _kaldi_matrix.MatrixBase):\n            _kaldi_vector_ext._copy_rows_from_mat(self, M)\n        if isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            _kaldi_vector_ext._copy_rows_from_double_mat(self, M)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def copy_cols_from_mat_(self, M):\n        """"""Copies the elements from a matrix column-by-columm.\n\n        Args:\n            M (Matrix): The input matrix.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows * M.num_cols:\n            raise ValueError(""The number of elements of the input matrix ({}) ""\n                             ""should match the size of the vector ({}).""\n                             .format(M.num_rows * M.num_cols, self.dim))\n        _kaldi_vector_ext._copy_cols_from_mat(self, M)\n        return self\n\n    def copy_row_from_mat_(self, M, row):\n        """"""Copies the elements from a matrix row.\n\n        Args:\n            M (Matrix or DoubleMatrix or SpMatrix or DoubleSpMatrix):\n                The input matrix.\n            row (int): The row index.\n\n        Raises:\n            ValueError: In case of size mismatch.\n            IndexError: If the row index is out-of-bounds.\n        """"""\n        if self.dim != M.num_cols:\n            raise ValueError(""The number of columns of the input matrix ({})""\n                             ""should match the size of the vector ({}).""\n                             .format(M.num_cols, self.dim))\n        if not isinstance(row, int) or not (0 <= row < M.num_rows):\n            raise IndexError()\n        if isinstance(M, _kaldi_matrix.MatrixBase):\n            _kaldi_vector_ext._copy_row_from_mat(self, M, row)\n        elif isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            _kaldi_vector_ext._copy_row_from_double_mat(self, M, row)\n        elif isinstance(M, _sp_matrix.SpMatrix):\n            _kaldi_vector_ext._copy_row_from_sp(self, M, row)\n        elif isinstance(M, _sp_matrix.DoubleSpMatrix):\n            _kaldi_vector_ext._copy_row_from_double_sp(self, M, row)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def copy_col_from_mat_(self, M, col):\n        """"""Copies the elements from a matrix column.\n\n        Args:\n            M (Matrix or DoubleMatrix): The input matrix.\n            col (int): The column index.\n\n        Raises:\n            ValueError: In case of size mismatch.\n            IndexError: If the column index is out-of-bounds.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""The number of rows of the input matrix ({})""\n                             ""should match the size of this vector ({}).""\n                             .format(M.num_rows, self.dim))\n        if not isinstance(col, int) or not (0 <= col < M.num_cols):\n            raise IndexError()\n        if isinstance(M, _kaldi_matrix.MatrixBase):\n            _kaldi_vector_ext._copy_col_from_mat(self, M, col)\n        elif isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            _kaldi_vector_ext._copy_col_from_double_mat(self, M, col)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def copy_diag_from_mat_(self, M):\n        """"""Copies the digonal elements from a matrix.\n\n        Args:\n            M (Matrix or SpMatrix or TpMatrix): The input matrix.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != min(M.num_rows, M.num_cols):\n            raise ValueError(""The size of the matrix diagonal ({}) should ""\n                             ""match the size of the vector ({}).""\n                             .format(min(M.size()), self.dim))\n        elif isinstance(M, _kaldi_matrix.MatrixBase):\n            _kaldi_vector_ext._copy_diag_from_mat(self, M)\n        elif isinstance(M, _sp_matrix.SpMatrix):\n            _kaldi_vector_ext._copy_diag_from_sp(self, M)\n        elif isinstance(M, _tp_matrix.TpMatrix):\n            _kaldi_vector_ext._copy_diag_from_tp(self, M)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def copy_from_packed_(self, M):\n        """"""Copies the elements from a packed matrix.\n\n        Args:\n            M (SpMatrix or TpMatrix or DoubleSpMatrix or DoubleTpMatrix):\n                The input packed matrix.\n\n        Raises:\n            ValueError: If `self.dim !=  M.num_rows * (M.num_rows + 1) / 2`.\n        """"""\n        numel = M.num_rows * (M.num_rows + 1) / 2\n        if self.dim != numel:\n            raise ValueError(""The number of elements of the input packed matrix""\n                             "" ({}) should match the size of the vector ({}).""\n                             .format(numel, self.dim))\n        elif isinstance(M, _packed_matrix.PackedMatrix):\n            _kaldi_vector_ext._copy_from_packed(self, M)\n        elif isinstance(M, _packed_matrix.DoublePackedMatrix):\n            _kaldi_vector_ext._copy_from_double_packed(self, M)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def add_row_sum_mat_(self, alpha, M, beta=1.0):\n        """"""Adds the sum of matrix rows.\n\n        Performs the operation :math:`y = \\\\alpha\\\\ \\\\sum_i M[i] + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the row sum.\n            M (Matrix): The input matrix.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n\n        Raises:\n            ValueError: If `self.dim != M.num_cols`.\n        """"""\n        if self.dim != M.num_cols:\n            raise ValueError(""Cannot add sum of rows with size {} to ""\n                             ""vector of size {}"".format(M.num_cols, self.dim))\n        _kaldi_vector_ext._add_row_sum_mat(self, alpha, M, beta)\n        return self\n\n    def add_col_sum_mat_(self, alpha, M, beta=1.0):\n        """"""Adds the sum of matrix columns.\n\n        Performs the operation\n        :math:`y = \\\\alpha\\\\ \\\\sum_i M[:,i] + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the column sum.\n            M (Matrix): The input matrix.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n\n        Raises:\n            ValueError: If `self.dim != M.num_rows`.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""Cannot add sum of columns with size {} to ""\n                             ""vector of size {}"".format(M.num_rows, self.dim))\n        _kaldi_vector_ext._add_col_sum_mat(self, alpha, M, beta)\n        return self\n\n    def add_diag_mat2_(self, alpha, M,\n                       trans=_matrix_common.MatrixTransposeType.NO_TRANS,\n                       beta=1.0):\n        """"""Adds the diagonal of a matrix multiplied with its transpose.\n\n        Performs the operation :math:`y = \\\\alpha\\\\ diag(M M^T) + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the diagonal.\n            M (Matrix): The input matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""Cannot add diagonal with size {} to ""\n                             ""vector of size {}"".format(M.num_rows, self.dim))\n        _kaldi_vector_ext._add_diag_mat2(self, alpha, M, trans, beta)\n        return self\n\n    def add_diag_mat_mat_(self, alpha, M, transM, N, transN, beta=1.0):\n        """"""Adds the diagonal of a matrix-matrix product.\n\n        Performs the operation :math:`y = \\\\alpha\\\\ diag(M N) + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the diagonal.\n            M (Matrix): The first input matrix.\n            transM (MatrixTransposeType): Whether to use **M** or its transpose.\n            N (Matrix): The second input matrix.\n            transN (MatrixTransposeType): Whether to use **N** or its transpose.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        m, n = M.size()\n        p, q = N.size()\n\n        if transM == _matrix_common.MatrixTransposeType.NO_TRANS:\n            if transN == _matrix_common.MatrixTransposeType.NO_TRANS:\n                if n != p:\n                    raise ValueError(""Cannot multiply M ({} by {}) with ""\n                                     ""N ({} by {})"".format(m, n, p, q))\n            else:\n                if n != q:\n                    raise ValueError(""Cannot multiply M ({} by {}) with ""\n                                     ""N^T ({} by {})"".format(m, n, q, p))\n        else:\n            if transN == _matrix_common.MatrixTransposeType.NO_TRANS:\n                if m != p:\n                    raise ValueError(""Cannot multiply M ({} by {}) with ""\n                                     ""N ({} by {})"".format(n, m, p, q))\n            else:\n                if m != q:\n                    raise ValueError(""Cannot multiply M ({} by {}) with ""\n                                     ""N ({} by {})"".format(n, m, q, p))\n        _kaldi_vector_ext._add_diag_mat_mat(self, alpha, M, transM,\n                                            N, transN, beta)\n\n    def mul_elements_(self, v):\n        """"""Multiplies the elements with the elements of another vector.\n\n        Performs the operation `y[i] *= v[i]`.\n\n        Args:\n            v (Vector or DoubleVector): The input vector.\n\n        Raises:\n            RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(v, _kaldi_vector.VectorBase):\n            return self._mul_elements_(v)\n        elif isinstance(v, _kaldi_vector.DoubleVectorBase):\n            _kaldi_vector_ext._mul_double_elements(self, v)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def div_elements_(self, v):\n        """"""Divides the elements with the elements of another vector.\n\n        Performs the operation `y[i] /= v[i]`.\n\n        Args:\n            v (Vector or DoubleVector): The input vector.\n\n        Raises:\n            RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(v, _kaldi_vector.VectorBase):\n            return self._div_elements_(v)\n        elif isinstance(v, _kaldi_vector.DoubleVectorBase):\n            _kaldi_vector_ext._div_double_elements(self, v)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        # All strings are unicode in Python 3, while we have to encode unicode\n        # strings in Python2. If we can\'t, let python decide the best\n        # characters to replace unicode characters with.\n        # Below implementation was taken from\n        # https://github.com/pytorch/pytorch/blob/master/torch/tensor.py\n        if sys.version_info > (3,):\n            return _str._vector_str(self)\n        else:\n            if hasattr(sys.stdout, \'encoding\'):\n                return _str._vector_str(self).encode(\n                    sys.stdout.encoding or \'UTF-8\', \'replace\')\n            else:\n                return _str._vector_str(self).encode(\'UTF-8\', \'replace\')\n\n    def __getitem__(self, index):\n        """"""Implements self[index].\n\n        This operation is offloaded to NumPy. Hence, it supports all NumPy array\n        indexing schemes: field access, basic slicing and advanced indexing.\n        For details see `NumPy Array Indexing`_.\n\n        Slicing shares data with the source vector when possible (see Caveats).\n\n        Returns:\n            - a float if the result of numpy indexing is a scalar\n            - a SubVector if the result of numpy indexing is 1 dimensional\n            - a SubMatrix if the result of numpy indexing is 2 dimensional\n\n        Caveats:\n            - Kaldi vector and matrix types do not support non-contiguous memory\n              layouts for the last dimension, i.e. the stride for the last\n              dimension should be the size of a float. If the result of numpy\n              slicing operation has an unsupported stride value for the last\n              dimension, the return value will not share any data with the\n              source vector, i.e. a copy will be made. Consider the following:\n                >>> v = Vector(5)\n                >>> s = v[0:4:2]     # s does not share data with v\n                >>> s[:] = v[1:4:2]  # changing s will not change v\n              Since the slicing operation requires a copy of the data to be\n              made, the source vector v will not be updated. On the other hand,\n              the following assignment operation will work as expected since\n              __setitem__ method does not create a new vector for representing\n              the left hand side:\n                >>> v[0:4:2] = v[1:4:2]\n\n        .. _NumPy Array Indexing:\n            https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n        """"""\n        ret = self.numpy().__getitem__(index)\n        if isinstance(ret, numpy.float32):\n            return float(ret)\n        elif isinstance(ret, numpy.ndarray):\n            if ret.ndim == 1:\n                return SubVector(ret)\n            elif ret.ndim == 2:\n                return SubMatrix(ret)\n            else:\n                raise ValueError(""indexing operation returned a numpy array ""\n                                 "" with {} dimensions."".format(ret.ndim))\n        raise TypeError(""indexing operation returned an invalid type {}.""\n                        .format(type(ret)))\n\n    def __setitem__(self, index, value):\n        """"""Implements self[index] = value.\n\n        This operation is offloaded to NumPy. Hence, it supports all NumPy array\n        indexing schemes: field access, basic slicing and advanced indexing.\n        For details see `NumPy Array Indexing`_.\n\n        .. _NumPy Array Indexing:\n            https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n        """"""\n        self.numpy().__setitem__(index, value)\n\n    # Numpy array interface methods were adapted from PyTorch.\n    # https://github.com/pytorch/pytorch/commit/c488a9e9bf9eddca6d55957304612b88f4638ca7\n\n    # Numpy array interface, to support `numpy.asarray(vector) -> ndarray`\n    def __array__(self, dtype=None):\n        if dtype is None:\n            return self.numpy()\n        else:\n            return self.numpy().astype(dtype, copy=False)\n\n    # Wrap Numpy array in a vector or matrix when done, to support e.g.\n    # `numpy.sin(vector) -> vector` or `numpy.greater(vector, 0) -> vector`\n    def __array_wrap__(self, array):\n        if array.ndim == 0:\n            if array.dtype.kind == \'b\':\n                return bool(array)\n            elif array.dtype.kind in (\'i\', \'u\'):\n                return int(array)\n            elif array.dtype.kind == \'f\':\n                return float(array)\n            elif array.dtype.kind == \'c\':\n                return complex(array)\n            else:\n                raise RuntimeError(\'bad scalar {!r}\'.format(array))\n        elif array.ndim == 1:\n            if array.dtype != numpy.float32:\n                # Vector stores single precision floats.\n                array = array.astype(\'float32\')\n            return SubVector(array)\n        elif array.ndim == 2:\n            if array.dtype != numpy.float32:\n                # Matrix stores single precision floats.\n                array = array.astype(\'float32\')\n            return SubMatrix(array)\n        else:\n            raise RuntimeError(\'{} dimensional array cannot be converted to a \'\n                               \'vector or matrix type\'.format(array.ndim))\n\n\nclass Vector(_VectorBase, _kaldi_vector.Vector):\n    """"""Single precision vector.""""""\n\n    def __init__(self, *args):\n        """"""\n        Vector():\n            Creates an empty vector.\n\n        Vector(size: int):\n            Creates a new vector of given size and fills it with zeros.\n\n        Args:\n            size (int): Size of the new vector.\n\n        Vector(obj: vector_like):\n            Creates a new vector with the elements in obj.\n\n        Args:\n            obj (vector_like): A vector, a 1-D numpy array, any object exposing\n                a 1-D array interface, an object with an __array__ method\n                returning a 1-D numpy array, or any sequence that can be\n                interpreted as a vector.\n        """"""\n        if len(args) > 1:\n            raise TypeError(""__init__() takes 1 to 2 positional arguments but ""\n                            ""{} were given"".format(len(args) + 1))\n        super(Vector, self).__init__()\n        if len(args) == 0:\n            return\n        if isinstance(args[0], int):\n            size = args[0]\n            if size < 0:\n                raise ValueError(""size should non-negative"")\n            self.resize_(size)\n            return\n        obj = args[0]\n        if not isinstance(obj, (_kaldi_vector.VectorBase,\n                                _kaldi_vector.DoubleVectorBase)):\n            obj = numpy.array(obj, dtype=numpy.float32, copy=False, order=\'C\')\n            if obj.ndim != 1:\n                raise TypeError(""obj should be a 1-D vector like object."")\n            obj = SubVector(obj)\n        self.resize_(obj.dim, _matrix_common.MatrixResizeType.UNDEFINED)\n        self.copy_(obj)\n\n    def __delitem__(self, index):\n        """"""Removes an element from the vector.""""""\n        if not (0 <= index < self.dim):\n            raise IndexError(""index={} should be in the range [0,{}).""\n                             .format(index, self.dim))\n        self._remove_element_(index)\n\n\nclass SubVector(_VectorBase, _matrix_ext.SubVector):\n    """"""Single precision vector view.""""""\n\n    def __init__(self, obj, start=0, length=None):\n        """"""Creates a new vector view from a vector like object.\n\n        If possible the new vector view will share its data with the `obj`,\n        i.e. no copy will be made. A copy will only be made if `obj.__array__`\n        returns a copy, if `obj` is a sequence, or if a copy is needed to\n        satisfy any of the other requirements (data type, order, etc.).\n        Regardless of whether a copy is made or not, the new vector view will\n        not own the memory buffer backing it, i.e. it will not support vector\n        operations that reallocate memory.\n\n        Args:\n            obj (vector_like): A vector, a 1-D numpy array, any object exposing\n                a 1-D array interface, an object whose __array__ method returns\n                a 1-D numpy array, or any sequence that can be interpreted as a\n                vector.\n            start (int): The index of the view start. Defaults to 0.\n            length (int): The length of the view. If None, it is set to\n                len(obj) - start. Defaults to None.\n        """"""\n        if not isinstance(obj, _kaldi_vector.VectorBase):\n            obj = numpy.array(obj, dtype=numpy.float32, copy=False, order=\'C\')\n            if obj.ndim != 1:\n                raise ValueError(""obj should be a 1-D vector like object."")\n        obj_len = len(obj)\n        if not (0 <= start <= obj_len):\n            raise IndexError(""start={0} should be in the range [0,{1}] ""\n                             ""when len(obj)={1}."".format(start, obj_len))\n        max_len = obj_len - start\n        if length is None:\n            length = max_len\n        if not (0 <= length <= max_len):\n            raise IndexError(""length={} should be in the range [0,{}] when ""\n                             ""start={} and len(obj)={}.""\n                             .format(length, max_len, start, obj_len))\n        super(SubVector, self).__init__(obj, start, length)\n\n\nclass _MatrixBase(object):\n    """"""Base class defining the additional API for single precision matrices.\n\n    No constructor.\n    """"""\n\n    def copy_(self, src, trans=_matrix_common.MatrixTransposeType.NO_TRANS):\n        """"""Copies the elements from another matrix.\n\n        Args:\n            src(Matrix or SpMatrix or TpMatrix or DoubleMatrix or DoubleSpMatrix or DoubleTpMatrix or CompressedMatrix):\n                The input matrix.\n            trans (MatrixTransposeType): Whether to use **src** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``. Not active if\n                input is a compressed matrix.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.size() != src.size():\n            raise ValueError(""Cannot copy matrix with dimensions {s[0]}x{s[1]} ""\n                             ""into matrix with dimensions {d[0]}x{d[1]}""\n                             .format(s=src.size(), d=self.size()))\n        if isinstance(src, _kaldi_matrix.MatrixBase):\n            self._copy_from_mat_(src, trans)\n        elif isinstance(src, _sp_matrix.SpMatrix):\n            _kaldi_matrix_ext._copy_from_sp(self, src)\n        elif isinstance(src, _tp_matrix.TpMatrix):\n            _kaldi_matrix_ext._copy_from_tp(self, src, trans)\n        elif isinstance(src, _kaldi_matrix.DoubleMatrixBase):\n            _kaldi_matrix_ext._copy_from_double_mat(self, src, trans)\n        elif isinstance(src, _sp_matrix.SpMatrix):\n            _kaldi_matrix_ext._copy_from_double_sp(self, src)\n        elif isinstance(src, _tp_matrix.TpMatrix):\n            _kaldi_matrix_ext._copy_from_double_tp(self, src, trans)\n        elif isinstance(src, _compressed_matrix.CompressedMatrix):\n            _kaldi_matrix_ext._copy_from_cmat(self, src)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def clone(self):\n        """"""Clones the matrix.\n\n        The clone allocates new memory for its contents and supports matrix\n        operations that reallocate memory, i.e. it is not a view.\n\n        Returns:\n            Matrix: A copy of the matrix.\n        """"""\n        return Matrix(self)\n\n    def size(self):\n        """"""Returns the size of the matrix.\n\n        Returns:\n            A tuple (num_rows, num_cols) of integers.\n        """"""\n        return self.num_rows, self.num_cols\n\n    @property\n    def shape(self):\n        """"""Two element tuple representing the size of the matrix.""""""\n        return self.size()\n\n    def approx_equal(self, other, tol=0.01):\n        """"""Checks if matrices are approximately equal.\n\n        Args:\n            other (Matrix): The matrix to compare against.\n            tol (float): The tolerance for the equality check.\n                Defaults to ``0.01``.\n\n        Returns:\n            True if `self.size() == other.size()` and\n            `||self-other|| <= tol*||self||`. False otherwise.\n        """"""\n        if not isinstance(other, _kaldi_matrix.MatrixBase):\n            return False\n        if self.num_rows != other.num_rows or self.num_cols != other.num_cols:\n            return False\n        return self._approx_equal(other, tol)\n\n    def __eq__(self, other):\n        return self.approx_equal(other, 1e-16)\n\n    def numpy(self):\n        """"""Converts the matrix to a 2-D NumPy array.\n\n        The NumPy array is a view into the matrix, i.e. no data is copied.\n\n        Returns:\n            numpy.ndarray: A NumPy array sharing data with this matrix.\n        """"""\n        return _matrix_ext.matrix_to_numpy(self)\n\n    @property\n    def data(self):\n        """"""Matrix data as a memoryview.""""""\n        return self.numpy().data\n\n    def row_data(self, index):\n        """"""Returns row data as a memoryview.""""""\n        return self[index].data\n\n    def row(self, index):\n        """"""Returns the given row as a new vector view.\n\n        Args:\n            index (int): The row index.\n\n        Returns:\n            SubVector: A vector view representing the given row.\n        """"""\n        return self[index]\n\n    def range(self, row_start, num_rows, col_start, num_cols):\n        """"""Returns the given range of elements as a new matrix view.\n\n        Args:\n            row_start (int): The start row index.\n            num_rows (int): The number of rows.\n            col_start (int): The start column index.\n            num_cols (int): The number of columns.\n\n        Returns:\n            SubMatrix: A matrix view representing the given range.\n        """"""\n        return SubMatrix(self, row_start, num_rows, col_start, num_cols)\n\n    def row_range(self, row_start, num_rows):\n        """"""Returns the given range of rows as a new matrix view.\n\n        Args:\n            row_start (int): The start row index.\n            num_rows (int): The number of rows.\n\n        Returns:\n            SubMatrix: A matrix view representing the given row range.\n        """"""\n        return SubMatrix(self, row_start, num_rows, 0, self.num_cols)\n\n    def col_range(self, col_start, num_cols):\n        """"""Returns the given range of columns as a new matrix view.\n\n        Args:\n            col_start (int): The start column index.\n            num_cols (int): The number of columns.\n\n        Returns:\n            SubMatrix: A matrix view representing the given column range.\n        """"""\n        return SubMatrix(self, 0, self.num_rows, col_start, num_cols)\n\n    def eig(self):\n        """"""Computes eigendecomposition.\n\n        Factorizes a square matrix into :math:`P\\\\ D\\\\ P^{-1}`.\n\n        The relationship of :math:`D` to the eigenvalues is slightly\n        complicated, due to the need for :math:`P` to be real. In the symmetric\n        case, :math:`D` is diagonal and real, but in the non-symmetric case\n        there may be complex-conjugate pairs of eigenvalues. In this case, for\n        the equation :math:`y = P\\\\ D\\\\ P^{-1}` to hold, :math:`D` must actually\n        be block diagonal, with 2x2 blocks corresponding to any such pairs. If a\n        pair is :math:`\\\\lambda +- i\\\\mu`, :math:`D` will have a corresponding\n        2x2 block :math:`[\\\\lambda, \\\\mu; -\\\\mu, \\\\lambda]`. Note that if the\n        matrix is not invertible, :math:`P` may not be invertible so in this\n        case instead of the equation :math:`y = P\\\\ D\\\\ P^{-1}` holding, we have\n        :math:`y\\\\ P = P\\\\ D`.\n\n        Returns:\n            3-element tuple containing\n\n            - **P** (:class:`Matrix`): The eigenvector matrix, where ith column\n              corresponds to the ith eigenvector.\n            - **r** (:class:`Vector`): The vector with real components of the\n              eigenvalues.\n            - **i** (:class:`Vector`): The vector with imaginary components of\n              the eigenvalues.\n\n        Raises:\n            ValueError: If the matrix is not square.\n        """"""\n        m, n = self.size()\n        if m != n:\n            raise ValueError(""eig method cannot be called on a non-square ""\n                             ""matrix."")\n        P = Matrix(n, n)\n        r, i = Vector(n), Vector(n)\n        self._eig(P, r, i)\n        return P, r, i\n\n    def svd(self, destructive=False):\n        """"""Computes singular-value decomposition.\n\n        Factorizes a matrix into :math:`U\\\\ diag(s)\\\\ V^T`.\n\n        For non-square matrices, requires `self.num_rows >= self.num_cols`.\n\n        Args:\n            destructive (bool): Whether to use the destructive operation which\n                avoids a copy but mutates self. Defaults to ``False``.\n\n        Returns:\n            3-element tuple containing\n\n            - **s** (:class:`Vector`): The vector of singular values.\n            - **U** (:class:`Matrix`): The left orthonormal matrix.\n            - **Vt** (:class:`Matrix`): The right orthonormal matrix.\n\n        Raises:\n            ValueError: If `self.num_rows < self.num_cols`.\n\n        Note:\n          **Vt** in the output is already transposed.\n          The singular values in **s** are not sorted.\n\n        See Also:\n          :meth:`singular_values`\n          :meth:`sort_svd`\n        """"""\n        m, n = self.size()\n        if m < n:\n            raise ValueError(""svd for non-square matrices requires ""\n                             ""self.num_rows >= self.num_cols."")\n        U, Vt = Matrix(m, n), Matrix(n, n)\n        s = Vector(n)\n        if destructive:\n            self._destructive_svd_(s, U, Vt)\n        else:\n            self._svd(s, U, Vt)\n        return s, U, Vt\n\n    def singular_values(self):\n        """"""Computes singular values.\n\n        Returns:\n            Vector: The vector of singular values.\n        """"""\n        res = Vector(self.num_cols)\n        self._singular_values(res)\n        return res\n\n    def add_mat_(self, alpha, M,\n                 trans=_matrix_common.MatrixTransposeType.NO_TRANS):\n        """"""Adds another matrix to this one.\n\n        Performs the operation :math:`S = \\\\alpha\\\\ M + S`.\n\n        Args:\n            alpha (float): The scalar multiplier.\n            M (Matrix or SpMatrix or DoubleSpMatrix): The input matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``.\n\n        Raises:\n          RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(M, _kaldi_matrix.MatrixBase):\n            self._add_mat_(alpha, M, trans)\n        elif isinstance(M, _sp_matrix.SpMatrix):\n            _kaldi_matrix_ext.add_sp(self, alpha, M)\n        elif isinstance(M, _sp_matrix.DoubleSpMatrix):\n            _kaldi_matrix_ext.add_double_sp(self, alpha, M)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def add_mat_mat_(self, A, B,\n                     transA=_matrix_common.MatrixTransposeType.NO_TRANS,\n                     transB=_matrix_common.MatrixTransposeType.NO_TRANS,\n                     alpha=1.0, beta=1.0, sparseA=False, sparseB=False):\n        """"""Adds the product of given matrices.\n\n        Performs the operation :math:`M = \\\\alpha\\\\ A\\\\ B + \\\\beta\\\\ M`.\n\n        Args:\n            A (Matrix or TpMatrix or SpMatrix):\n                The first input matrix.\n            B (Matrix or TpMatrix or SpMatrix):\n                The second input matrix.\n            transA (MatrixTransposeType): Whether to use **A** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``.\n            transB (MatrixTransposeType): Whether to use **B** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``.\n            alpha (float): The scalar multiplier for the product.\n                Defaults to ``1.0``.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n            sparseA (bool): Whether to use the algorithm that is faster when\n                **A** is sparse. Defaults to ``False``.\n            sparseA (bool): Whether to use the algorithm that is faster when\n                **B** is sparse. Defaults to ``False``.\n\n        Raises:\n            RuntimeError: In case of size mismatch.\n            TypeError: If matrices of given types can not be multiplied.\n        """"""\n        if isinstance(A, _kaldi_matrix.MatrixBase):\n            if isinstance(B, _kaldi_matrix.MatrixBase):\n                if sparseA:\n                    self._add_smat_mat_(alpha, A, transA, B, transB, beta)\n                elif sparseB:\n                    self._add_mat_smat_(alpha, A, transA, B, transB, beta)\n                else:\n                    self._add_mat_mat_(alpha, A, transA, B, transB, beta)\n            elif isinstance(B, _sp_matrix.SpMatrix):\n                _kaldi_matrix_ext._add_mat_sp(self, alpha, A, transA, B, beta)\n            elif isinstance(B, _tp_matrix.TpMatrix):\n                _kaldi_matrix_ext._add_mat_tp(self, alpha, A, transA, B, transB,\n                                              beta)\n            else:\n                raise TypeError(""Cannot multiply matrix A with matrix B of ""\n                                ""type {}"".format(type(B)))\n        elif isinstance(A, _sp_matrix.SpMatrix):\n            if isinstance(B, _kaldi_matrix.MatrixBase):\n                _kaldi_matrix_ext._add_sp_mat(self, alpha, A, B, transB, beta)\n            elif isinstance(B, _sp_matrix.SpMatrix):\n                _kaldi_matrix_ext._add_sp_sp(self, alpha, A, transA, B, beta)\n            else:\n                raise TypeError(""Cannot multiply symmetric matrix A with ""\n                                ""matrix B of type {}"".format(type(B)))\n        elif isinstance(A, _tp_matrix.SpMatrix):\n            if isinstance(B, _kaldi_matrix.MatrixBase):\n                _kaldi_matrix_ext._add_tp_mat(self, alpha, transA, B, transB,\n                                              beta)\n            elif isinstance(B, _tp_matrix.TpMatrix):\n                _kaldi_matrix_ext._add_tp_tp(self, alpha, A, transA, B, transB,\n                                             beta)\n            else:\n                raise TypeError(""Cannot multiply triangular matrix A with ""\n                                ""matrix B of type {}"".format(type(B)))\n        return self\n\n    def invert_(self, in_double_precision=False):\n        """"""Inverts the matrix.\n\n        Args:\n            in_double_precision (bool): Whether to do the inversion in double\n                precision. Defaults to ``False``.\n\n        Returns:\n            2-element tuple containing\n\n            - **log_det** (:class:`float`): The log determinant.\n            - **det_sign** (:class:`float`): The sign of the determinant, 1 or -1.\n\n        Raises:\n            RuntimeError: If matrix is not square.\n        """"""\n        if in_double_precision:\n            return _kaldi_matrix_ext._invert_in_double(self)\n        else:\n            return _kaldi_matrix_ext._invert(self)\n\n    def copy_cols_(self, src, indices):\n        """"""Copies columns from another matrix.\n\n        Copies column `r` from column `indices[r]` of `src`. As a special case,\n        if `indexes[i] == -1`, sets column `i` to zero. All elements of indices\n        must be in `[-1, src.num_cols-1]`, and `src.num_rows` must equal\n        `self.num_rows`.\n\n        Args:\n            src (Matrix): The input matrix.\n            indices (List[int]): The list of column indices.\n        """"""\n        _kaldi_matrix_ext._copy_cols(self, src, indices)\n        return self\n\n    def copy_rows_(self, src, indices):\n        """"""Copies rows from another matrix.\n\n        Copies row `r` from row `indices[r]` of `src`. As a special case, if\n        `indexes[i] == -1`, sets row `i` to zero. All elements of indices must\n        be in `[-1, src.num_rows-1]`, and `src.num_cols` must equal\n        `self.num_cols`.\n\n        Args:\n            src (Matrix): The input matrix.\n            indices (List[int]): The list of row indices.\n        """"""\n        _kaldi_matrix_ext._copy_rows(self, src, indices)\n        return self\n\n    def add_cols_(self, src, indices):\n        """"""Adds columns from another matrix.\n\n        Adds column `indices[r]` of `src` to column `r`. As a special case, if\n        `indexes[i] == -1`, skips column `i`. All elements of indices must be in\n        `[-1, src.num_cols-1]`, and `src.num_rows` must equal `self.num_rows`.\n\n        Args:\n            src (Matrix): The input matrix.\n            indices (List[int]): The list of column indices.\n        """"""\n        _kaldi_matrix_ext._add_cols(self, src, indices)\n        return self\n\n    def add_rows_(self, src, indices, alpha=1.0):\n        """"""Adds rows from another matrix.\n\n        Scales row `indices[r]` of `src` with `alpha` and adds it to row `r`. As\n        a special case, if `indexes[i] == -1`, skips row `i`. All elements of\n        indices must be in `[-1, src.num_rows-1]`, and `src.num_cols` must equal\n        `self.num_cols`.\n\n        Args:\n            src (Matrix): The input matrix.\n            indices (List[int]): The list of row indices.\n            alpha (float): The scalar multiplier. Defaults to `1.0`.\n        """"""\n        _kaldi_matrix_ext._add_rows(self, alpha, src, indices)\n        return self\n\n    def __getitem__(self, index):\n        """"""Implements self[index].\n\n        This operation is offloaded to NumPy. Hence, it supports all NumPy array\n        indexing schemes: field access, basic slicing and advanced indexing.\n        For details see `NumPy Array Indexing`_.\n\n        Slicing shares data with the source matrix when possible (see Caveats).\n\n        Returns:\n            - a float if the result of numpy indexing is a scalar\n            - a SubVector if the result of numpy indexing is 1 dimensional\n            - a SubMatrix if the result of numpy indexing is 2 dimensional\n\n        Caveats:\n            - Kaldi vector and matrix types do not support non-contiguous memory\n              layouts for the last dimension, i.e. the stride for the last\n              dimension should be the size of a float. If the result of numpy\n              slicing operation has an unsupported stride value for the last\n              dimension, the return value will not share any data with the\n              source matrix, i.e. a copy will be made. Consider the following:\n                >>> m = Matrix(3, 5)\n                >>> s = m[:,0:4:2]     # s does not share data with m\n                >>> s[:] = m[:,1:4:2]  # changing s will not change m\n              Since the slicing operation requires a copy of the data to be\n              made, the source matrix m will not be updated. On the other hand,\n              the following assignment operation will work as expected since\n              __setitem__ method does not create a new scalar/vector/matrix for\n              representing the left hand side:\n                >>> m[:,0:4:2] = m[:,1:4:2]\n\n        .. _NumPy Array Indexing:\n            https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n        """"""\n        ret = self.numpy().__getitem__(index)\n        if isinstance(ret, numpy.float32):\n            return float(ret)\n        elif isinstance(ret, numpy.ndarray):\n            if ret.ndim == 2:\n                return SubMatrix(ret)\n            elif ret.ndim == 1:\n                return SubVector(ret)\n            else:\n                raise ValueError(""indexing operation returned a numpy array ""\n                                 "" with {} dimensions."".format(ret.ndim))\n        raise TypeError(""indexing operation returned an invalid type {}.""\n                        .format(type(ret)))\n\n    def __setitem__(self, index, value):\n        """"""Implements self[index] = value.\n\n        This operation is offloaded to NumPy. Hence, it supports all NumPy array\n        indexing schemes: field access, basic slicing and advanced indexing.\n        For details see `NumPy Array Indexing`_.\n\n        .. _NumPy Array Indexing:\n            https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n        """"""\n        self.numpy().__setitem__(index, value)\n\n    def __contains__(self, value):\n        """"""Implements value in self.""""""\n        return value in self.numpy()\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        # All strings are unicode in Python 3, while we have to encode unicode\n        # strings in Python2. If we can\'t, let python decide the best\n        # characters to replace unicode characters with.\n        # Below implementation was taken from\n        # https://github.com/pytorch/pytorch/blob/master/torch/tensor.py\n        if sys.version_info > (3,):\n            return _str._matrix_str(self)\n        else:\n            if hasattr(sys.stdout, \'encoding\'):\n                return _str._matrix_str(self).encode(\n                    sys.stdout.encoding or \'UTF-8\', \'replace\')\n            else:\n                return _str._matrix_str(self).encode(\'UTF-8\', \'replace\')\n\n    # Numpy array interface methods were adapted from PyTorch.\n    # https://github.com/pytorch/pytorch/commit/c488a9e9bf9eddca6d55957304612b88f4638ca7\n\n    # Numpy array interface, to support `numpy.asarray(vector) -> ndarray`\n    def __array__(self, dtype=None):\n        if dtype is None:\n            return self.numpy()\n        else:\n            return self.numpy().astype(dtype, copy=False)\n\n    # Wrap Numpy array in a vector or matrix when done, to support e.g.\n    # `numpy.sin(vector) -> vector` or `numpy.greater(vector, 0) -> vector`\n    def __array_wrap__(self, array):\n        if array.ndim == 0:\n            if array.dtype.kind == \'b\':\n                return bool(array)\n            elif array.dtype.kind in (\'i\', \'u\'):\n                return int(array)\n            elif array.dtype.kind == \'f\':\n                return float(array)\n            elif array.dtype.kind == \'c\':\n                return complex(array)\n            else:\n                raise RuntimeError(\'bad scalar {!r}\'.format(array))\n        elif array.ndim == 1:\n            if array.dtype != numpy.float32:\n                # Vector stores single precision floats.\n                array = array.astype(\'float32\')\n            return SubVector(array)\n        elif array.ndim == 2:\n            if array.dtype != numpy.float32:\n                # Matrix stores single precision floats.\n                array = array.astype(\'float32\')\n            return SubMatrix(array)\n        else:\n            raise RuntimeError(\'{} dimensional array cannot be converted to a \'\n                               \'Kaldi vector or matrix type\'.format(array.ndim))\n\n\nclass Matrix(_MatrixBase, _kaldi_matrix.Matrix):\n    """"""Single precision matrix.""""""\n\n    def __init__(self, *args):\n        """"""\n        Matrix():\n            Creates an empty matrix.\n\n        Matrix(num_rows: int, num_cols: int):\n            Creates a new matrix of given size and fills it with zeros.\n\n        Args:\n            num_rows (int): Number of rows of the new matrix.\n            num_cols (int): Number of cols of the new matrix.\n\n        Matrix(obj: matrix_like):\n            Creates a new matrix with the elements in obj.\n\n        Args:\n            obj (matrix_like): A matrix, a 2-D numpy array, any object exposing\n                a 2-D array interface, an object with an __array__ method\n                returning a 2-D numpy array, or any (nested) sequence that can\n                be interpreted as a matrix.\n        """"""\n        if len(args) > 2:\n            raise TypeError(""__init__() takes 1 to 3 positional arguments but ""\n                            ""{} were given"".format(len(args) + 1))\n        super(Matrix, self).__init__()\n        if len(args) == 0:\n            return\n        if len(args) == 2:\n            num_rows, num_cols = args\n            if not (isinstance(num_rows, int) and isinstance(num_cols, int)):\n                raise TypeError(""num_rows and num_cols should be integers"")\n            if not (num_rows > 0 and num_cols > 0):\n                if not (num_rows == 0 and num_cols == 0):\n                    raise IndexError(""num_rows and num_cols should both be ""\n                                     ""positive or they should both be 0."")\n            self.resize_(num_rows, num_cols)\n            return\n        obj = args[0]\n        if not isinstance(obj, (_kaldi_matrix.MatrixBase,\n                                _packed_matrix.PackedMatrix,\n                                _kaldi_matrix.DoubleMatrixBase,\n                                _packed_matrix.DoublePackedMatrix,\n                                _compressed_matrix.CompressedMatrix)):\n            obj = numpy.array(obj, dtype=numpy.float32, copy=False, order=\'C\')\n            if obj.ndim != 2:\n                raise ValueError(""obj should be a 2-D matrix like object."")\n            obj = SubMatrix(obj)\n        self.resize_(obj.num_rows, obj.num_cols,\n                     _matrix_common.MatrixResizeType.UNDEFINED)\n        self.copy_(obj)\n\n    def __delitem__(self, index):\n        """"""Removes a row from the matrix.""""""\n        if not (0 <= index < self.num_rows):\n            raise IndexError(""index={} should be in the range [0,{}).""\n                             .format(index, self.num_rows))\n        self._remove_row_(index)\n\n\nclass SubMatrix(_MatrixBase, _matrix_ext.SubMatrix):\n    """"""Single precision matrix view.""""""\n\n    def __init__(self, obj, row_start=0, num_rows=None, col_start=0,\n                 num_cols=None):\n        """"""Creates a new matrix view from a matrix like object.\n\n        If possible the new matrix view will share its data with the `obj`,\n        i.e. no copy will be made. A copy will only be made if `obj.__array__`\n        returns a copy, if `obj` is a sequence, or if a copy is needed to\n        satisfy any of the other requirements (data type, order, etc.).\n        Regardless of whether a copy is made or not, the new matrix view will\n        not own the memory buffer backing it, i.e. it will not support matrix\n        operations that reallocate memory.\n\n        Args:\n            obj (matrix_like): A matrix, a 2-D numpy array, any object exposing\n                a 2-D array interface, an object with an __array__ method\n                returning a 2-D numpy array, or any sequence that can be\n                interpreted as a matrix.\n            row_start (int): The start row index. Defaults to ``0``.\n            num_rows (int): The number of rows. If ``None``, it is set to\n                `self.num_rows - row_start`. Defaults to ``None``.\n            col_start (int): The start column index. Defaults to ``0``.\n            num_cols (int): The number of columns. If ``None``, it is set to\n                `self.num_cols - col_start`. Defaults to ``None``.\n        """"""\n        if not isinstance(obj, _kaldi_matrix.MatrixBase):\n            obj = numpy.array(obj, dtype=numpy.float32, copy=False, order=\'C\')\n            if obj.ndim != 2:\n                raise ValueError(""obj should be a 2-D matrix like object."")\n            obj_num_rows, obj_num_cols = obj.shape\n        else:\n            obj_num_rows, obj_num_cols = obj.num_rows, obj.num_cols\n        if not (0 <= row_start <= obj_num_rows):\n            raise IndexError(""row_start={0} should be in the range [0,{1}] ""\n                             ""when obj.num_rows={1}.""\n                             .format(row_start, obj_num_rows))\n        if not (0 <= col_start <= obj_num_cols):\n            raise IndexError(""col_start={0} should be in the range [0,{1}] ""\n                             ""when obj.num_cols={1}.""\n                             .format(col_offset, obj_num_cols))\n        max_rows, max_cols = obj_num_rows - row_start, obj_num_cols - col_start\n        if num_rows is None:\n            num_rows = max_rows\n        if num_cols is None:\n            num_cols = max_cols\n        if not (0 <= num_rows <= max_rows):\n            raise IndexError(""num_rows={} should be in the range [0,{}] ""\n                             ""when row_start={} and obj.num_rows={}.""\n                             .format(num_rows, max_rows,\n                                     row_start, obj_num_rows))\n        if not (0 <= num_cols <= max_cols):\n            raise IndexError(""num_cols={} should be in the range [0,{}] ""\n                             ""when col_start={} and obj.num_cols={}.""\n                             .format(num_cols, max_cols,\n                                     col_start, obj_num_cols))\n        if not (num_rows > 0 and num_cols > 0):\n            if not (num_rows == 0 and num_cols == 0):\n                raise IndexError(""num_rows and num_cols should both be ""\n                                 ""positive or they should both be 0."")\n        super(SubMatrix, self).__init__(obj, row_start, num_rows,\n                                        col_start, num_cols)\n\n\n################################################################################\n# double precision vector/matrix types\n################################################################################\n\n\nclass _DoubleVectorBase(object):\n    """"""Base class defining the additional API for double precision vectors.\n\n    No constructor.\n    """"""\n\n    def copy_(self, src):\n        """"""Copies the elements from another vector.\n\n        Args:\n            src (Vector or DoubleVector): The input vector.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != src.dim:\n            raise ValueError(""Vector of size {} cannot be copied into vector ""\n                             ""of size {}."".format(src.dim, self.dim))\n        if isinstance(src, _kaldi_vector.DoubleVectorBase):\n            return self._copy_from_vec_(src)\n        elif isinstance(src, _kaldi_vector.VectorBase):\n            _kaldi_vector_ext._copy_from_single_vec_double(self, src)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def clone(self):\n        """"""Clones the vector.\n\n        The clone allocates new memory for its contents and supports vector\n        operations that reallocate memory, i.e. it is not a view.\n\n        Returns:\n            DoubleVector: A copy of the vector.\n        """"""\n        return DoubleVector(self)\n\n    def size(self):\n        """"""Returns the size of the vector as a single element tuple.""""""\n        return (self.dim,)\n\n    @property\n    def shape(self):\n        """"""Single element tuple representing the size of the vector.""""""\n        return self.size()\n\n    def approx_equal(self, other, tol=0.01):\n        """"""Checks if vectors are approximately equal.\n\n        Args:\n            other (DoubleVector): The vector to compare against.\n            tol (float): The tolerance for the equality check.\n                Defaults to ``0.01``.\n\n        Returns:\n            True if `self.dim == other.dim` and\n            `||self-other|| <= tol*||self||`. False otherwise.\n        """"""\n        if not isinstance(other, _kaldi_vector.DoubleVectorBase):\n            return False\n        if self.dim != other.dim:\n            return False\n        return self._approx_equal(other, tol)\n\n    def __eq__(self, other):\n        return self.approx_equal(other, 1e-16)\n\n    def numpy(self):\n        """"""Converts the vector to a 1-D NumPy array.\n\n        The NumPy array is a view into the vector, i.e. no data is copied.\n\n        Returns:\n            numpy.ndarray: A NumPy array sharing data with this vector.\n        """"""\n        return _matrix_ext.double_vector_to_numpy(self)\n\n    @property\n    def data(self):\n        """"""Vector data as a memoryview.""""""\n        return self.numpy().data\n\n    def range(self, start, length):\n        """"""Returns the given range of elements as a new vector view.\n\n        Args:\n            start (int): The start index.\n            length (int): The length.\n\n        Returns:\n            DoubleSubVector: A vector view representing the given range.\n        """"""\n        return DoubleSubVector(self, start, length)\n\n    def add_vec_(self, alpha, v):\n        """"""Adds another vector.\n\n        Performs the operation :math:`y = y + \\\\alpha\\\\ v`.\n\n        Args:\n            alpha (float): The scalar multiplier.\n            v (Vector or DoubleVector): The input vector.\n\n        Raises:\n          RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(v, _kaldi_vector.DoubleVectorBase):\n            return self._add_vec_(alpha, v)\n        elif isinstance(v, _kaldi_vector.VectorBase):\n            _kaldi_vector_ext._add_single_vec_double(self, alpha, v)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def add_vec2_(self, alpha, v):\n        """"""Adds the squares of elements from another vector.\n\n        Performs the operation :math:`y = y + \\\\alpha\\\\ v\\\\odot v`.\n\n        Args:\n            alpha (float): The scalar multiplier.\n            v (Vector or DoubleVector): The input vector.\n\n        Raises:\n          RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(v, _kaldi_vector.DoubleVectorBase):\n            return self._add_vec2_(alpha, v)\n        elif isinstance(v, _kaldi_vector.VectorBase):\n            _kaldi_vector_ext._add_single_vec2_double(self, alpha, v)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def add_mat_vec_(self, alpha, M, trans, v, beta, sparse=False):\n        """"""Computes a matrix-vector product.\n\n        Performs the operation :math:`y = \\\\alpha\\\\ M\\\\ v + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the matrix-vector product.\n            M (DoubleMatrix or DoubleSpMatrix or DoubleTpMatrix): The input matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n            v (DoubleVector): The input vector.\n            beta (float): The scalar multiplier for the destination vector.\n            sparse (bool): Whether to use the algorithm that is faster when\n                **v** is sparse. Defaults to ``False``.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if v.dim != M.num_cols:\n            raise ValueError(""Matrix of size {}x{} cannot be multiplied with ""\n                             ""vector of size {}.""\n                             .format(M.num_rows, M.num_cols, v.dim))\n        if self.dim != M.num_rows:\n            raise ValueError(""Vector of size {} cannot be added to vector of ""\n                             ""size {}."".format(M.num_rows, self.dim))\n        if isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            if sparse:\n                _kaldi_vector_ext._add_mat_svec_double(self, alpha, M, trans, v, beta)\n            else:\n                _kaldi_vector_ext._add_mat_vec_double(self, alpha, M, trans, v, beta)\n        elif isinstance(M, _sp_matrix.DoubleSpMatrix):\n            _kaldi_vector_ext._add_sp_vec_double(self, alpha, M, v, beta)\n        elif isinstance(M, _tp_matrix.DoubleTpMatrix):\n            _kaldi_vector_ext._add_tp_vec_double(self, alpha, M, trans, v, beta)\n        return self\n\n    def mul_tp_(self, M, trans):\n        """"""Multiplies the vector with a lower-triangular matrix.\n\n        Performs the operation :math:`y = M\\\\ y`.\n\n        Args:\n            M (DoubleTpMatrix): The input lower-triangular matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""Matrix with size {}x{} cannot be multiplied ""\n                             ""with vector of size {}.""\n                             .format(M.num_rows, M.num_cols, self.dim))\n        _kaldi_vector_ext._mul_tp_double(self, M, trans)\n        return self\n\n    def solve_(self, M, trans):\n        """"""Solves a linear system.\n\n        The linear system is defined as :math:`M\\\\ x = b`, where :math:`b` and\n        :math:`x` are the initial and final values of the vector, respectively.\n\n        Warning:\n            Does not test for :math:`M` being singular or near-singular.\n\n        Args:\n            M (DoubleTpMatrix): The input lower-triangular matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""The number of rows of the input matrix ({}) ""\n                             ""should match the size of the vector ({}).""\n                             .format(M.num_rows, self.dim))\n        _kaldi_vector_ext._solve_double(self, M, trans)\n        return self\n\n    def copy_rows_from_mat_(self, M):\n        """"""Copies the elements from a matrix row-by-row.\n\n        Args:\n            M (Matrix or DoubleMatrix): The input matrix.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows * M.num_cols:\n            raise ValueError(""The number of elements of the input matrix ({}) ""\n                             ""should match the size of the vector ({}).""\n                             .format(M.num_rows * M.num_cols, self.dim))\n        if isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            _kaldi_vector_ext._copy_rows_from_mat_double(self, M)\n        if isinstance(M, _kaldi_matrix.MatrixBase):\n            _kaldi_vector_ext._copy_rows_from_single_mat_double(self, M)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def copy_cols_from_mat_(self, M):\n        """"""Copies the elements from a matrix column-by-columm.\n\n        Args:\n            M (DoubleMatrix): The input matrix.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows * M.num_cols:\n            raise ValueError(""The number of elements of the input matrix ({}) ""\n                             ""should match the size of the vector ({}).""\n                             .format(M.num_rows * M.num_cols, self.dim))\n        _kaldi_vector_ext._copy_cols_from_mat_double(self, M)\n        return self\n\n    def copy_row_from_mat_(self, M, row):\n        """"""Copies the elements from a matrix row.\n\n        Args:\n            M (Matrix or DoubleMatrix or SpMatrix or DoubleSpMatrix):\n                The input matrix.\n            row (int): The row index.\n\n        Raises:\n            ValueError: In case of size mismatch.\n            IndexError: If the row index is out-of-bounds.\n        """"""\n        if self.dim != M.num_cols:\n            raise ValueError(""The number of columns of the input matrix ({})""\n                             ""should match the size of the vector ({}).""\n                             .format(M.num_cols, self.dim))\n        if not isinstance(row, int) or not (0 <= row < M.num_rows):\n            raise IndexError()\n        if isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            _kaldi_vector_ext._copy_row_from_mat_double(self, M, row)\n        elif isinstance(M, _kaldi_matrix.MatrixBase):\n            _kaldi_vector_ext._copy_row_from_single_mat_double(self, M, row)\n        elif isinstance(M, _sp_matrix.DoubleSpMatrix):\n            _kaldi_vector_ext._copy_row_from_sp_double(self, M, row)\n        elif isinstance(M, _sp_matrix.SpMatrix):\n            _kaldi_vector_ext._copy_row_from_single_sp_double(self, M, row)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def copy_col_from_mat_(self, M, col):\n        """"""Copies the elements from a matrix column.\n\n        Args:\n            M (Matrix or DoubleMatrix): The input matrix.\n            col (int): The column index.\n\n        Raises:\n            ValueError: In case of size mismatch.\n            IndexError: If the column index is out-of-bounds.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""The number of rows of the input matrix ({})""\n                             ""should match the size of this vector ({}).""\n                             .format(M.num_rows, self.dim))\n        if not isinstance(col, int) or not (0 <= col < M.num_cols):\n            raise IndexError()\n        if isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            _kaldi_vector_ext._copy_col_from_mat_double(self, M, col)\n        elif isinstance(M, _kaldi_matrix.MatrixBase):\n            _kaldi_vector_ext._copy_col_from_single_mat_double(self, M, col)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def copy_diag_from_mat_(self, M):\n        """"""Copies the digonal elements from a matrix.\n\n        Args:\n            M (Matrix or DoubleSpMatrix or DoubleTpMatrix): The input matrix.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != min(M.num_rows, M.num_cols):\n            raise ValueError(""The size of the matrix diagonal ({}) should ""\n                             ""match the size of the vector ({}).""\n                             .format(min(M.size()), self.dim))\n        if isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            _kaldi_vector_ext._copy_diag_from_mat_double(self, M)\n        elif isinstance(M, _sp_matrix.DoubleSpMatrix):\n            _kaldi_vector_ext._copy_diag_from_sp_double(self, M)\n        elif isinstance(M, _tp_matrix.DoubleTpMatrix):\n            _kaldi_vector_ext._copy_diag_from_tp_double(self, M)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def copy_from_packed_(self, M):\n        """"""Copies the elements from a packed matrix.\n\n        Args:\n            M (SpMatrix or TpMatrix or DoubleSpMatrix or DoubleTpMatrix):\n                The input packed matrix.\n\n        Raises:\n            ValueError: If `self.dim !=  M.num_rows * (M.num_rows + 1) / 2`.\n        """"""\n        numel = M.num_rows * (M.num_rows + 1) / 2\n        if self.dim != numel:\n            raise ValueError(""The number of elements of the input packed matrix""\n                             "" ({}) should match the size of the vector ({}).""\n                             .format(numel, self.dim))\n        if isinstance(M, _packed_matrix.DoublePackedMatrix):\n            _kaldi_vector_ext._copy_from_packed_double(self, M)\n        elif isinstance(M, _packed_matrix.PackedMatrix):\n            _kaldi_vector_ext._copy_from_single_packed_double(self, M)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def add_row_sum_mat_(self, alpha, M, beta=1.0):\n        """"""Adds the sum of matrix rows.\n\n        Performs the operation :math:`y = \\\\alpha\\\\ \\\\sum_i M[i] + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the row sum.\n            M (DoubleMatrix): The input matrix.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n\n        Raises:\n            ValueError: If `self.dim != M.num_cols`.\n        """"""\n        if self.dim != M.num_cols:\n            raise ValueError(""Cannot add sum of rows with size {} to ""\n                             ""vector of size {}"".format(M.num_cols, self.dim))\n        _kaldi_vector_ext._add_row_sum_mat_double(self, alpha, M, beta)\n        return self\n\n    def add_col_sum_mat_(self, alpha, M, beta=1.0):\n        """"""Adds the sum of matrix columns.\n\n        Performs the operation\n        :math:`y = \\\\alpha\\\\ \\\\sum_i M[:,i] + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the column sum.\n            M (DoubleMatrix): The input matrix.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n\n        Raises:\n            ValueError: If `self.dim != M.num_rows`.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""Cannot add sum of columns with size {} to ""\n                             ""vector of size {}"".format(M.num_rows, self.dim))\n        _kaldi_vector_ext._add_col_sum_mat_double(self, alpha, M, beta)\n        return self\n\n    def add_diag_mat2_(self, alpha, M,\n                       trans=_matrix_common.MatrixTransposeType.NO_TRANS,\n                       beta=1.0):\n        """"""Adds the diagonal of a matrix multiplied with its transpose.\n\n        Performs the operation :math:`y = \\\\alpha\\\\ diag(M M^T) + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the diagonal.\n            M (DoubleMatrix): The input matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.dim != M.num_rows:\n            raise ValueError(""Cannot add diagonal with size {} to ""\n                             ""vector of size {}"".format(M.num_rows, self.dim))\n        _kaldi_vector_ext._add_diag_mat2_double(self, alpha, M, trans, beta)\n        return self\n\n    def add_diag_mat_mat_(self, alpha, M, transM, N, transN, beta=1.0):\n        """"""Adds the diagonal of a matrix-matrix product.\n\n        Performs the operation :math:`y = \\\\alpha\\\\ diag(M N) + \\\\beta\\\\ y`.\n\n        Args:\n            alpha (float): The scalar multiplier for the diagonal.\n            M (DoubleMatrix): The first input matrix.\n            transM (MatrixTransposeType): Whether to use **M** or its transpose.\n            N (DoubleMatrix): The second input matrix.\n            transN (MatrixTransposeType): Whether to use **N** or its transpose.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        m, n = M.size()\n        p, q = N.size()\n\n        if transM == _matrix_common.MatrixTransposeType.NO_TRANS:\n            if transN == _matrix_common.MatrixTransposeType.NO_TRANS:\n                if n != p:\n                    raise ValueError(""Cannot multiply M ({} by {}) with ""\n                                     ""N ({} by {})"".format(m, n, p, q))\n            else:\n                if n != q:\n                    raise ValueError(""Cannot multiply M ({} by {}) with ""\n                                     ""N^T ({} by {})"".format(m, n, q, p))\n        else:\n            if transN == _matrix_common.MatrixTransposeType.NO_TRANS:\n                if m != p:\n                    raise ValueError(""Cannot multiply M ({} by {}) with ""\n                                     ""N ({} by {})"".format(n, m, p, q))\n            else:\n                if m != q:\n                    raise ValueError(""Cannot multiply M ({} by {}) with ""\n                                     ""N ({} by {})"".format(n, m, q, p))\n        _kaldi_vector_ext._add_diag_mat_mat_double(self, alpha, M, transM,\n                                                   N, transN, beta)\n\n    def mul_elements_(self, v):\n        """"""Multiplies the elements with the elements of another vector.\n\n        Performs the operation `y[i] *= v[i]`.\n\n        Args:\n            v (Vector or DoubleVector): The input vector.\n\n        Raises:\n            RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(v, _kaldi_vector.DoubleVectorBase):\n            return self._mul_elements_(v)\n        elif isinstance(v, _kaldi_vector.VectorBase):\n            _kaldi_vector_ext._mul_single_elements_double(self, v)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def div_elements_(self, v):\n        """"""Divides the elements with the elements of another vector.\n\n        Performs the operation `y[i] /= v[i]`.\n\n        Args:\n            v (Vector or DoubleVector): The input vector.\n\n        Raises:\n            RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(v, _kaldi_vector.DoubleVectorBase):\n            return self._div_elements_(v)\n        elif isinstance(v, _kaldi_vector.VectorBase):\n            _kaldi_vector_ext._div_single_elements_double(self, v)\n            return self\n        else:\n            raise TypeError(""input vector type is not supported."")\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        # All strings are unicode in Python 3, while we have to encode unicode\n        # strings in Python2. If we can\'t, let python decide the best\n        # characters to replace unicode characters with.\n        # Below implementation was taken from\n        # https://github.com/pytorch/pytorch/blob/master/torch/tensor.py\n        if sys.version_info > (3,):\n            return _str._vector_str(self)\n        else:\n            if hasattr(sys.stdout, \'encoding\'):\n                return _str._vector_str(self).encode(\n                    sys.stdout.encoding or \'UTF-8\', \'replace\')\n            else:\n                return _str._vector_str(self).encode(\'UTF-8\', \'replace\')\n\n    def __getitem__(self, index):\n        """"""Implements self[index].\n\n        This operation is offloaded to numpy. Hence, it supports all numpy array\n        indexing schemes: field access, basic slicing and advanced indexing.\n        For details see `NumPy Array Indexing`_.\n\n        Slicing shares data with the source vector when possible (see Caveats).\n\n        Returns:\n            - a float if the result of numpy indexing is a scalar\n            - a DoubleSubVector if the result of numpy indexing is 1 dimensional\n            - a DoubleSubMatrix if the result of numpy indexing is 2 dimensional\n\n        Caveats:\n            - Kaldi vector and matrix types do not support non-contiguous memory\n              layouts for the last dimension, i.e. the stride for the last\n              dimension should be the size of a float. If the result of numpy\n              slicing operation has an unsupported stride value for the last\n              dimension, the return value will not share any data with the\n              source vector, i.e. a copy will be made. Consider the following:\n                >>> v = DoubleVector(5)\n                >>> s = v[0:4:2]     # s does not share data with v\n                >>> s[:] = v[1:4:2]  # changing s will not change v\n              Since the slicing operation requires a copy of the data to be\n              made, the source vector v will not be updated. On the other hand,\n              the following assignment operation will work as expected since\n              __setitem__ method does not create a new vector for representing\n              the left hand side:\n                >>> v[0:4:2] = v[1:4:2]\n\n        .. _NumPy Array Indexing:\n            https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n        """"""\n        ret = self.numpy().__getitem__(index)\n        if isinstance(ret, numpy.float64):\n            return float(ret)\n        elif isinstance(ret, numpy.ndarray):\n            if ret.ndim == 1:\n                return DoubleSubVector(ret)\n            elif ret.ndim == 2:\n                return DoubleSubMatrix(ret)\n            else:\n                raise ValueError(""indexing operation returned a numpy array ""\n                                 "" with {} dimensions."".format(ret.ndim))\n        raise TypeError(""indexing operation returned an invalid type {}.""\n                        .format(type(ret)))\n\n    def __setitem__(self, index, value):\n        """"""Implements self[index] = value.\n\n        This operation is offloaded to NumPy. Hence, it supports all NumPy array\n        indexing schemes: field access, basic slicing and advanced indexing.\n        For details see `NumPy Array Indexing`_.\n\n        .. _NumPy Array Indexing:\n            https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n        """"""\n        self.numpy().__setitem__(index, value)\n\n    # Numpy array interface methods were adapted from PyTorch.\n    # https://github.com/pytorch/pytorch/commit/c488a9e9bf9eddca6d55957304612b88f4638ca7\n\n    # Numpy array interface, to support `numpy.asarray(vector) -> ndarray`\n    def __array__(self, dtype=None):\n        if dtype is None:\n            return self.numpy()\n        else:\n            return self.numpy().astype(dtype, copy=False)\n\n    # Wrap Numpy array in a vector or matrix when done, to support e.g.\n    # `numpy.sin(vector) -> vector` or `numpy.greater(vector, 0) -> vector`\n    def __array_wrap__(self, array):\n        if array.ndim == 0:\n            if array.dtype.kind == \'b\':\n                return bool(array)\n            elif array.dtype.kind in (\'i\', \'u\'):\n                return int(array)\n            elif array.dtype.kind == \'f\':\n                return float(array)\n            elif array.dtype.kind == \'c\':\n                return complex(array)\n            else:\n                raise RuntimeError(\'bad scalar {!r}\'.format(array))\n        elif array.ndim == 1:\n            if array.dtype != numpy.float64:\n                # DoubleVector stores double precision floats.\n                array = array.astype(\'float64\')\n            return DoubleSubVector(array)\n        elif array.ndim == 2:\n            if array.dtype != numpy.float64:\n                # DoubleMatrix stores double precision floats.\n                array = array.astype(\'float64\')\n            return DoubleSubMatrix(array)\n        else:\n            raise RuntimeError(\'{} dimensional array cannot be converted to a \'\n                               \'Kaldi vector or matrix type\'.format(array.ndim))\n\n\nclass DoubleVector(_DoubleVectorBase, _kaldi_vector.DoubleVector):\n    """"""Double precision vector.""""""\n\n    def __init__(self, *args):\n        """"""\n        DoubleVector():\n            Creates an empty vector.\n\n        DoubleVector(size: int):\n            Creates a new vector of given size and fills it with zeros.\n\n        Args:\n            size (int): Size of the new vector.\n\n        DoubleVector(obj: vector_like):\n            Creates a new vector with the elements in obj.\n\n        Args:\n            obj (vector_like): A vector, a 1-D numpy array, any object exposing\n                a 1-D array interface, an object with an __array__ method\n                returning a 1-D numpy array, or any sequence that can be\n                interpreted as a vector.\n        """"""\n        if len(args) > 1:\n            raise TypeError(""__init__() takes 1 to 2 positional arguments but ""\n                            ""{} were given"".format(len(args) + 1))\n        super(DoubleVector, self).__init__()\n        if len(args) == 0:\n            return\n        if isinstance(args[0], int):\n            size = args[0]\n            if size < 0:\n                raise ValueError(""size should non-negative"")\n            self.resize_(size)\n            return\n        obj = args[0]\n        if not isinstance(obj, (_kaldi_vector.DoubleVectorBase,\n                                _kaldi_vector.VectorBase)):\n            obj = numpy.array(obj, dtype=numpy.float64, copy=False, order=\'C\')\n            if obj.ndim != 1:\n                raise TypeError(""obj should be a 1-D vector like object."")\n            obj = DoubleSubVector(obj)\n        self.resize_(len(obj), _matrix_common.MatrixResizeType.UNDEFINED)\n        self.copy_(obj)\n\n    def __delitem__(self, index):\n        """"""Removes an element from the vector.""""""\n        if not (0 <= index < self.dim):\n            raise IndexError(""index={} should be in the range [0,{}).""\n                             .format(index, self.dim))\n        self._remove_element_(index)\n\n\nclass DoubleSubVector(_DoubleVectorBase, _matrix_ext.DoubleSubVector):\n    """"""Double precision vector view.""""""\n\n    def __init__(self, obj, start=0, length=None):\n        """"""Creates a new vector view from a vector like object.\n\n        If possible the new vector view will share its data with the `obj`,\n        i.e. no copy will be made. A copy will only be made if `obj.__array__`\n        returns a copy, if `obj` is a sequence, or if a copy is needed to\n        satisfy any of the other requirements (data type, order, etc.).\n        Regardless of whether a copy is made or not, the new vector view will\n        not own the memory buffer backing it, i.e. it will not support vector\n        operations that reallocate memory.\n\n        Args:\n            obj (vector_like): A vector, a 1-D numpy array, any object exposing\n                a 1-D array interface, an object whose __array__ method returns\n                a 1-D numpy array, or any sequence that can be interpreted as a\n                vector.\n            start (int): The index of the view start. Defaults to 0.\n            length (int): The length of the view. If None, it is set to\n                len(obj) - start. Defaults to None.\n        """"""\n        if not isinstance(obj, _kaldi_vector.DoubleVectorBase):\n            obj = numpy.array(obj, dtype=numpy.float64, copy=False, order=\'C\')\n            if obj.ndim != 1:\n                raise ValueError(""obj should be a 1-D vector like object."")\n        obj_len = len(obj)\n        if not (0 <= start <= obj_len):\n            raise IndexError(""start={0} should be in the range [0,{1}] ""\n                             ""when len(obj)={1}."".format(start, obj_len))\n        max_len = obj_len - start\n        if length is None:\n            length = max_len\n        if not (0 <= length <= max_len):\n            raise IndexError(""length={} should be in the range [0,{}] when ""\n                             ""start={} and len(obj)={}.""\n                             .format(length, max_len, start, obj_len))\n        super(DoubleSubVector, self).__init__(obj, start, length)\n\n\nclass _DoubleMatrixBase(object):\n    """"""Base class defining the additional API for single precision matrices.\n\n    No constructor.\n    """"""\n\n    def copy_(self, src, trans=_matrix_common.MatrixTransposeType.NO_TRANS):\n        """"""Copies the elements from another matrix.\n\n        Args:\n            src (Matrix or SpMatrix or TpMatrix or DoubleMatrix or DoubleSpMatrix or DoubleTpMatrix or CompressedMatrix):\n                The input matrix.\n            trans (MatrixTransposeType): Whether to use **src** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``. Not active if\n                input is a compressed matrix.\n\n        Raises:\n            ValueError: In case of size mismatch.\n        """"""\n        if self.size() != src.size():\n            raise ValueError(""Cannot copy matrix with dimensions {s[0]}x{s[1]} ""\n                             ""into matrix with dimensions {d[0]}x{d[1]}""\n                             .format(s=src.size(), d=self.size()))\n        if isinstance(src, _kaldi_matrix.DoubleMatrixBase):\n            self._copy_from_mat_(src, trans)\n        elif isinstance(src, _sp_matrix.DoubleSpMatrix):\n            _kaldi_matrix_ext._copy_from_sp_double(self, src)\n        elif isinstance(src, _tp_matrix.DoubleTpMatrix):\n            _kaldi_matrix_ext._copy_from_tp_double(self, src, trans)\n        elif isinstance(src, _kaldi_matrix.MatrixBase):\n            _kaldi_matrix_ext._copy_from_single_mat_double(self, src, trans)\n        elif isinstance(src, _sp_matrix.SpMatrix):\n            _kaldi_matrix_ext._copy_from_single_sp_double(self, src)\n        elif isinstance(src, _tp_matrix.TpMatrix):\n            _kaldi_matrix_ext._copy_from_single_tp_double(self, src, trans)\n        elif isinstance(src, _compressed_matrix.CompressedMatrix):\n            _kaldi_matrix_ext._copy_from_cmat_double(self, src)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def clone(self):\n        """"""Clones the matrix.\n\n        The clone allocates new memory for its contents and supports matrix\n        operations that reallocate memory, i.e. it is not a view.\n\n        Returns:\n            DoubleMatrix: A copy of the matrix.\n        """"""\n        return DoubleMatrix(self)\n\n    def size(self):\n        """"""Returns the size of the matrix.\n\n        Returns:\n            A tuple (num_rows, num_cols) of integers.\n        """"""\n        return self.num_rows, self.num_cols\n\n    @property\n    def shape(self):\n        """"""Two element tuple representing the size of the matrix.""""""\n        return self.size()\n\n    def approx_equal(self, other, tol=0.01):\n        """"""Checks if matrices are approximately equal.\n\n        Args:\n            other (DoubleMatrix): The matrix to compare against.\n            tol (float): The tolerance for the equality check.\n                Defaults to ``0.01``.\n\n        Returns:\n            True if `self.size() == other.size()` and\n            `||self-other|| <= tol*||self||`. False otherwise.\n        """"""\n        if not isinstance(other, _kaldi_matrix.DoubleMatrixBase):\n            return False\n        if self.num_rows != other.num_rows or self.num_cols != other.num_cols:\n            return False\n        return self._approx_equal(other, tol)\n\n    def __eq__(self, other):\n        return self.approx_equal(other, 1e-16)\n\n    def numpy(self):\n        """"""Converts the matrix to a 2-D NumPy array.\n\n        The NumPy array is a view into the matrix, i.e. no data is copied.\n\n        Returns:\n            numpy.ndarray: A NumPy array sharing data with this matrix.\n        """"""\n        return _matrix_ext.double_matrix_to_numpy(self)\n\n    @property\n    def data(self):\n        """"""Matrix data as a memoryview.""""""\n        return self.numpy().data\n\n    def row_data(self, index):\n        """"""Returns row data as a memoryview.""""""\n        return self[index].data\n\n    def row(self, index):\n        """"""Returns the given row as a new vector view.\n\n        Args:\n            index (int): The row index.\n\n        Returns:\n            DoubleSubVector: A vector view representing the given row.\n        """"""\n        return self[index]\n\n    def range(self, row_start, num_rows, col_start, num_cols):\n        """"""Returns the given range of elements as a new matrix view.\n\n        Args:\n            row_start (int): The start row index.\n            num_rows (int): The number of rows.\n            col_start (int): The start column index.\n            num_cols (int): The number of columns.\n\n        Returns:\n            DoubleSubMatrix: A matrix view representing the given range.\n        """"""\n        return DoubleSubMatrix(self, row_start, num_rows, col_start, num_cols)\n\n    def row_range(self, row_start, num_rows):\n        """"""Returns the given range of rows as a new matrix view.\n\n        Args:\n            row_start (int): The start row index.\n            num_rows (int): The number of rows.\n\n        Returns:\n            DoubleSubMatrix: A matrix view representing the given row range.\n        """"""\n        return DoubleSubMatrix(self, row_start, num_rows, 0, self.num_cols)\n\n    def col_range(self, col_start, num_cols):\n        """"""Returns the given range of columns as a new matrix view.\n\n        Args:\n            col_start (int): The start column index.\n            num_cols (int): The number of columns.\n\n        Returns:\n            DoubleSubMatrix: A matrix view representing the given column range.\n        """"""\n        return DoubleSubMatrix(self, 0, self.num_rows, col_start, num_cols)\n\n    def eig(self):\n        """"""Computes eigendecomposition.\n\n        Factorizes a square matrix into :math:`P\\\\ D\\\\ P^{-1}`.\n\n        The relationship of :math:`D` to the eigenvalues is slightly\n        complicated, due to the need for :math:`P` to be real. In the symmetric\n        case, :math:`D` is diagonal and real, but in the non-symmetric case\n        there may be complex-conjugate pairs of eigenvalues. In this case, for\n        the equation :math:`y = P\\\\ D\\\\ P^{-1}` to hold, :math:`D` must actually\n        be block diagonal, with 2x2 blocks corresponding to any such pairs. If a\n        pair is :math:`\\\\lambda +- i\\\\mu`, :math:`D` will have a corresponding\n        2x2 block :math:`[\\\\lambda, \\\\mu; -\\\\mu, \\\\lambda]`. Note that if the\n        matrix is not invertible, :math:`P` may not be invertible so in this\n        case instead of the equation :math:`y = P\\\\ D\\\\ P^{-1}` holding, we have\n        :math:`y\\\\ P = P\\\\ D`.\n\n        Returns:\n            3-element tuple containing\n\n            - **P** (:class:`DoubleMatrix`): The eigenvector matrix, where ith\n              column corresponds to the ith eigenvector.\n            - **r** (:class:`DoubleVector`): The vector with real components of\n              the eigenvalues.\n            - **i** (:class:`DoubleVector`): The vector with imaginary\n              components of the eigenvalues.\n\n        Raises:\n            ValueError: If the matrix is not square.\n        """"""\n        m, n = self.size()\n        if m != n:\n            raise ValueError(""eig method cannot be called on a non-square ""\n                             ""matrix."")\n        P = DoubleMatrix(n, n)\n        r, i = DoubleVector(n), DoubleVector(n)\n        self._eig(P, r, i)\n        return P, r, i\n\n    def svd(self, destructive=False):\n        """"""Computes singular-value decomposition.\n\n        Factorizes a matrix into :math:`U\\\\ diag(s)\\\\ V^T`.\n\n        For non-square matrices, requires `self.num_rows >= self.num_cols`.\n\n        Args:\n            destructive (bool): Whether to use the destructive operation which\n                avoids a copy but mutates self. Defaults to ``False``.\n\n        Returns:\n            3-element tuple containing\n\n            - **s** (:class:`DoubleVector`): The vector of singular values.\n            - **U** (:class:`DoubleMatrix`): The left orthonormal matrix.\n            - **Vt** (:class:`DoubleMatrix`): The right orthonormal matrix.\n\n        Raises:\n            ValueError: If `self.num_rows < self.num_cols`.\n\n        Note:\n          **Vt** in the output is already transposed.\n          The singular values in **s** are not sorted.\n\n        See Also:\n          :meth:`singular_values`\n          :meth:`sort_svd`\n        """"""\n        m, n = self.size()\n        if m < n:\n            raise ValueError(""svd for non-square matrices requires ""\n                             ""self.num_rows >= self.num_cols."")\n        U, Vt = DoubleMatrix(m, n), DoubleMatrix(n, n)\n        s = DoubleVector(n)\n        if destructive:\n            self._destructive_svd_(s, U, Vt)\n        else:\n            self._svd(s, U, Vt)\n        return s, U, Vt\n\n    def singular_values(self):\n        """"""Computes singular values.\n\n        Returns:\n            DoubleVector: The vector of singular values.\n        """"""\n        res = DoubleVector(self.num_cols)\n        self._singular_values(res)\n        return res\n\n    def add_mat_(self, alpha, M,\n                 trans=_matrix_common.MatrixTransposeType.NO_TRANS):\n        """"""Adds another matrix to this one.\n\n        Performs the operation :math:`S = \\\\alpha\\\\ M + S`.\n\n        Args:\n            alpha (float): The scalar multiplier.\n            M (DoubleMatrix or SpMatrix or DoubleSpMatrix): The input matrix.\n            trans (MatrixTransposeType): Whether to use **M** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``.\n\n        Raises:\n          RuntimeError: In case of size mismatch.\n        """"""\n        if isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            self._add_mat_(alpha, M, trans)\n        elif isinstance(M, _sp_matrix.DoubleSpMatrix):\n            _kaldi_matrix_ext.add_sp_double(self, alpha, M)\n        elif isinstance(M, _sp_matrix.SpMatrix):\n            _kaldi_matrix_ext.add_single_sp_double(self, alpha, M)\n        else:\n            raise TypeError(""input matrix type is not supported."")\n        return self\n\n    def add_mat_mat_(self, A, B,\n                     transA=_matrix_common.MatrixTransposeType.NO_TRANS,\n                     transB=_matrix_common.MatrixTransposeType.NO_TRANS,\n                     alpha=1.0, beta=1.0, sparseA=False, sparseB=False):\n        """"""Adds the product of given matrices.\n\n        Performs the operation :math:`M = \\\\alpha\\\\ A\\\\ B + \\\\beta\\\\ M`.\n\n        Args:\n            A (DoubleMatrix or DoubleTpMatrix or DoubleSpMatrix):\n                The first input matrix.\n            B (DoubleMatrix or DoubleTpMatrix or DoubleSpMatrix):\n                The second input matrix.\n            transA (MatrixTransposeType): Whether to use **A** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``.\n            transB (MatrixTransposeType): Whether to use **B** or its transpose.\n                Defaults to ``MatrixTransposeType.NO_TRANS``.\n            alpha (float): The scalar multiplier for the product.\n                Defaults to ``1.0``.\n            beta (float): The scalar multiplier for the destination vector.\n                Defaults to ``1.0``.\n            sparseA (bool): Whether to use the algorithm that is faster when\n                **A** is sparse. Defaults to ``False``.\n            sparseA (bool): Whether to use the algorithm that is faster when\n                **B** is sparse. Defaults to ``False``.\n\n        Raises:\n            RuntimeError: In case of size mismatch.\n            TypeError: If matrices of given types can not be multiplied.\n        """"""\n        if isinstance(A, _kaldi_matrix.DoubleMatrixBase):\n            if isinstance(B, _kaldi_matrix.DoubleMatrixBase):\n                if sparseA:\n                    self._add_smat_mat_(alpha, A, transA, B, transB, beta)\n                elif sparseB:\n                    self._add_mat_smat_(alpha, A, transA, B, transB, beta)\n                else:\n                    self._add_mat_mat_(alpha, A, transA, B, transB, beta)\n            elif isinstance(B, _sp_matrix.DoubleSpMatrix):\n                _kaldi_matrix_ext._add_mat_sp_double(self, alpha, A, transA,\n                                                     B, beta)\n            elif isinstance(B, _tp_matrix.DoubleTpMatrix):\n                _kaldi_matrix_ext._add_mat_tp_double(self, alpha, A, transA,\n                                                     B, transB, beta)\n            else:\n                raise TypeError(""Cannot multiply matrix A with matrix B of ""\n                                ""type {}"".format(type(B)))\n        elif isinstance(A, _sp_matrix.DoubleSpMatrix):\n            if isinstance(B, _kaldi_matrix.DoubleMatrixBase):\n                _kaldi_matrix_ext._add_sp_mat_double(self, alpha, A, B, transB,\n                                                     beta)\n            elif isinstance(B, _sp_matrix.DoubleSpMatrix):\n                _kaldi_matrix_ext._add_sp_sp_double(self, alpha, A, transA, B,\n                                                    beta)\n            else:\n                raise TypeError(""Cannot multiply symmetric matrix A with ""\n                                ""matrix B of type {}"".format(type(B)))\n        elif isinstance(A, _tp_matrix.DoubleSpMatrix):\n            if isinstance(B, _kaldi_matrix.DoubleMatrixBase):\n                _kaldi_matrix_ext._add_tp_mat_double(self, alpha, transA,\n                                                     B, transB, beta)\n            elif isinstance(B, _tp_matrix.DoubleTpMatrix):\n                _kaldi_matrix_ext._add_tp_tp_double(self, alpha, A, transA,\n                                                    B, transB, beta)\n            else:\n                raise TypeError(""Cannot multiply triangular matrix A with ""\n                                ""matrix B of type {}"".format(type(B)))\n        return self\n\n    def invert_(self):\n        """"""Inverts the matrix.\n\n        Returns:\n            2-element tuple containing\n\n            - **log_det** (:class:`float`): The log determinant.\n            - **det_sign** (:class:`float`): The sign of the determinant, 1 or -1.\n\n        Raises:\n            RuntimeError: If matrix is not square.\n        """"""\n        return _kaldi_matrix_ext._invert_double(self)\n\n    def copy_cols_(self, src, indices):\n        """"""Copies columns from another matrix.\n\n        Copies column `r` from column `indices[r]` of `src`. As a special case,\n        if `indexes[i] == -1`, sets column `i` to zero. All elements of indices\n        must be in `[-1, src.num_cols-1]`, and `src.num_rows` must equal\n        `self.num_rows`.\n\n        Args:\n            src (DoubleMatrix): The input matrix.\n            indices (List[int]): The list of column indices.\n        """"""\n        _kaldi_matrix_ext._copy_cols_double(self, src, indices)\n        return self\n\n    def copy_rows_(self, src, indices):\n        """"""Copies rows from another matrix.\n\n        Copies row `r` from row `indices[r]` of `src`. As a special case, if\n        `indexes[i] == -1`, sets row `i` to zero. All elements of indices must\n        be in `[-1, src.num_rows-1]`, and `src.num_cols` must equal\n        `self.num_cols`.\n\n        Args:\n            src (DoubleMatrix): The input matrix.\n            indices (List[int]): The list of row indices.\n        """"""\n        _kaldi_matrix_ext._copy_rows_double(self, src, indices)\n        return self\n\n    def add_cols_(self, src, indices):\n        """"""Adds columns from another matrix.\n\n        Adds column `indices[r]` of `src` to column `r`. As a special case, if\n        `indexes[i] == -1`, skips column `i`. All elements of indices must be in\n        `[-1, src.num_cols-1]`, and `src.num_rows` must equal `self.num_rows`.\n\n        Args:\n            src (DoubleMatrix): The input matrix.\n            indices (List[int]): The list of column indices.\n        """"""\n        _kaldi_matrix_ext._add_cols_double(self, src, indices)\n        return self\n\n    def add_rows_(self, src, indices, alpha=1.0):\n        """"""Adds rows from another matrix.\n\n        Scales row `indices[r]` of `src` with `alpha` and adds it to row `r`. As\n        a special case, if `indexes[i] == -1`, skips row `i`. All elements of\n        indices must be in `[-1, src.num_rows-1]`, and `src.num_cols` must equal\n        `self.num_cols`.\n\n        Args:\n            src (DoubleMatrix): The input matrix.\n            indices (List[int]): The list of row indices.\n            alpha (float): The scalar multiplier. Defaults to `1.0`.\n        """"""\n        _kaldi_matrix_ext._add_rows_double(self, alpha, src, indices)\n        return self\n\n    def __getitem__(self, index):\n        """"""Implements self[index].\n\n        This operation is offloaded to NumPy. Hence, it supports all NumPy array\n        indexing schemes: field access, basic slicing and advanced indexing.\n        For details see `NumPy Array Indexing`_.\n\n        Slicing shares data with the source matrix when possible (see Caveats).\n\n        Returns:\n            - a float if the result of numpy indexing is a scalar\n            - a DoubleSubVector if the result of numpy indexing is 1 dimensional\n            - a DoubleSubMatrix if the result of numpy indexing is 2 dimensional\n\n        Caveats:\n            - Kaldi vector and matrix types do not support non-contiguous memory\n              layouts for the last dimension, i.e. the stride for the last\n              dimension should be the size of a float. If the result of numpy\n              slicing operation has an unsupported stride value for the last\n              dimension, the return value will not share any data with the\n              source matrix, i.e. a copy will be made. Consider the following:\n                >>> m = DoubleMatrix(3, 5)\n                >>> s = m[:,0:4:2]     # s does not share data with m\n                >>> s[:] = m[:,1:4:2]  # changing s will not change m\n              Since the slicing operation requires a copy of the data to be\n              made, the source matrix m will not be updated. On the other hand,\n              the following assignment operation will work as expected since\n              __setitem__ method does not create a new scalar/vector/matrix for\n              representing the left hand side:\n                >>> m[:,0:4:2] = m[:,1:4:2]\n\n        .. _NumPy Array Indexing:\n            https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n        """"""\n        ret = self.numpy().__getitem__(index)\n        if isinstance(ret, numpy.float64):\n            return float(ret)\n        elif isinstance(ret, numpy.ndarray):\n            if ret.ndim == 2:\n                return DoubleSubMatrix(ret)\n            elif ret.ndim == 1:\n                return DoubleSubVector(ret)\n            else:\n                raise ValueError(""indexing operation returned a numpy array ""\n                                 "" with {} dimensions."".format(ret.ndim))\n        raise TypeError(""indexing operation returned an invalid type {}.""\n                        .format(type(ret)))\n\n    def __setitem__(self, index, value):\n        """"""Implements self[index] = value.\n\n        This operation is offloaded to NumPy. Hence, it supports all NumPy array\n        indexing schemes: field access, basic slicing and advanced indexing.\n        For details see `NumPy Array Indexing`_.\n\n        .. _NumPy Array Indexing:\n            https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html\n        """"""\n        self.numpy().__setitem__(index, value)\n\n    def __contains__(self, value):\n        """"""Implements value in self.""""""\n        return value in self.numpy()\n\n    def __repr__(self):\n        return str(self)\n\n    def __str__(self):\n        # All strings are unicode in Python 3, while we have to encode unicode\n        # strings in Python2. If we can\'t, let python decide the best\n        # characters to replace unicode characters with.\n        # Below implementation was taken from\n        # https://github.com/pytorch/pytorch/blob/master/torch/tensor.py\n        if sys.version_info > (3,):\n            return _str._matrix_str(self)\n        else:\n            if hasattr(sys.stdout, \'encoding\'):\n                return _str._matrix_str(self).encode(\n                    sys.stdout.encoding or \'UTF-8\', \'replace\')\n            else:\n                return _str._matrix_str(self).encode(\'UTF-8\', \'replace\')\n\n    # Numpy array interface methods were adapted from PyTorch.\n    # https://github.com/pytorch/pytorch/commit/c488a9e9bf9eddca6d55957304612b88f4638ca7\n\n    # Numpy array interface, to support `numpy.asarray(vector) -> ndarray`\n    def __array__(self, dtype=None):\n        if dtype is None:\n            return self.numpy()\n        else:\n            return self.numpy().astype(dtype, copy=False)\n\n    # Wrap Numpy array in a vector or matrix when done, to support e.g.\n    # `numpy.sin(vector) -> vector` or `numpy.greater(vector, 0) -> vector`\n    def __array_wrap__(self, array):\n        if array.ndim == 0:\n            if array.dtype.kind == \'b\':\n                return bool(array)\n            elif array.dtype.kind in (\'i\', \'u\'):\n                return int(array)\n            elif array.dtype.kind == \'f\':\n                return float(array)\n            elif array.dtype.kind == \'c\':\n                return complex(array)\n            else:\n                raise RuntimeError(\'bad scalar {!r}\'.format(array))\n        elif array.ndim == 1:\n            if array.dtype != numpy.float64:\n                # DoubleVector stores single precision floats.\n                array = array.astype(\'float64\')\n            return DoubleSubVector(array)\n        elif array.ndim == 2:\n            if array.dtype != numpy.float64:\n                # DoubleMatrix stores single precision floats.\n                array = array.astype(\'float64\')\n            return DoubleSubMatrix(array)\n        else:\n            raise RuntimeError(\'{} dimensional array cannot be converted to a \'\n                               \'Kaldi vector or matrix type\'.format(array.ndim))\n\n\nclass DoubleMatrix(_DoubleMatrixBase, _kaldi_matrix.DoubleMatrix):\n    """"""Double precision matrix.""""""\n\n    def __init__(self, *args):\n        """"""\n        DoubleMatrix():\n            Creates an empty matrix.\n\n        DoubleMatrix(num_rows: int, num_cols: int):\n            Creates a new matrix of given size and fills it with zeros.\n\n        Args:\n            num_rows (int): Number of rows of the new matrix.\n            num_cols (int): Number of cols of the new matrix.\n\n        DoubleMatrix(obj: matrix_like):\n            Creates a new matrix with the elements in obj.\n\n        Args:\n            obj (matrix_like): A matrix, a 2-D numpy array, any object exposing\n                a 2-D array interface, an object with an __array__ method\n                returning a 2-D numpy array, or any (nested) sequence that can\n                be interpreted as a matrix.\n        """"""\n        if len(args) > 2:\n            raise TypeError(""__init__() takes 1 to 3 positional arguments but ""\n                            ""{} were given"".format(len(args) + 1))\n        super(DoubleMatrix, self).__init__()\n        if len(args) == 0:\n            return\n        if len(args) == 2:\n            num_rows, num_cols = args\n            if not (isinstance(num_rows, int) and isinstance(num_cols, int)):\n                raise TypeError(""num_rows and num_cols should be integers"")\n            if not (num_rows > 0 and num_cols > 0):\n                if not (num_rows == 0 and num_cols == 0):\n                    raise IndexError(""num_rows and num_cols should both be ""\n                                     ""positive or they should both be 0."")\n            self.resize_(num_rows, num_cols)\n            return\n        obj = args[0]\n        if not isinstance(obj, (_kaldi_matrix.MatrixBase,\n                                _packed_matrix.PackedMatrix,\n                                _kaldi_matrix.DoubleMatrixBase,\n                                _packed_matrix.DoublePackedMatrix,\n                                _compressed_matrix.CompressedMatrix)):\n            obj = numpy.array(obj, dtype=numpy.float64, copy=False, order=\'C\')\n            if obj.ndim != 2:\n                raise ValueError(""obj should be a 2-D matrix like object."")\n            obj = DoubleSubMatrix(obj)\n        self.resize_(obj.num_rows, obj.num_cols,\n                     _matrix_common.MatrixResizeType.UNDEFINED)\n        self.copy_(obj)\n\n    def __delitem__(self, index):\n        """"""Removes a row from the matrix.""""""\n        if not (0 <= index < self.num_rows):\n            raise IndexError(""index={} should be in the range [0,{}).""\n                             .format(index, self.num_rows))\n        self._remove_row_(index)\n\n\nclass DoubleSubMatrix(_DoubleMatrixBase, _matrix_ext.DoubleSubMatrix):\n    """"""Double precision matrix view.""""""\n\n    def __init__(self, obj, row_start=0, num_rows=None, col_start=0,\n                 num_cols=None):\n        """"""Creates a new matrix view from a matrix like object.\n\n        If possible the new matrix view will share its data with the `obj`,\n        i.e. no copy will be made. A copy will only be made if `obj.__array__`\n        returns a copy, if `obj` is a sequence, or if a copy is needed to\n        satisfy any of the other requirements (data type, order, etc.).\n        Regardless of whether a copy is made or not, the new matrix view will\n        not own the memory buffer backing it, i.e. it will not support matrix\n        operations that reallocate memory.\n\n        Args:\n            obj (matrix_like): A matrix, a 2-D numpy array, any object exposing\n                a 2-D array interface, an object with an __array__ method\n                returning a 2-D numpy array, or any sequence that can be\n                interpreted as a matrix.\n            row_start (int): The start row index. Defaults to ``0``.\n            num_rows (int): The number of rows. If ``None``, it is set to\n                `self.num_rows - row_start`. Defaults to ``None``.\n            col_start (int): The start column index. Defaults to ``0``.\n            num_cols (int): The number of columns. If ``None``, it is set to\n                `self.num_cols - col_start`. Defaults to ``None``.\n        """"""\n        if not isinstance(obj, _kaldi_matrix.DoubleMatrixBase):\n            obj = numpy.array(obj, dtype=numpy.float64, copy=False, order=\'C\')\n            if obj.ndim != 2:\n                raise ValueError(""obj should be a 2-D matrix like object."")\n            obj_num_rows, obj_num_cols = obj.shape\n        else:\n            obj_num_rows, obj_num_cols = obj.num_rows, obj.num_cols\n        if not (0 <= row_start <= obj_num_rows):\n            raise IndexError(""row_start={0} should be in the range [0,{1}] ""\n                             ""when obj.num_rows={1}.""\n                             .format(row_start, obj_num_rows))\n        if not (0 <= col_start <= obj_num_cols):\n            raise IndexError(""col_start={0} should be in the range [0,{1}] ""\n                             ""when obj.num_cols={1}.""\n                             .format(col_offset, obj_num_cols))\n        max_rows, max_cols = obj_num_rows - row_start, obj_num_cols - col_start\n        if num_rows is None:\n            num_rows = max_rows\n        if num_cols is None:\n            num_cols = max_cols\n        if not (0 <= num_rows <= max_rows):\n            raise IndexError(""num_rows={} should be in the range [0,{}] ""\n                             ""when row_start={} and obj.num_rows={}.""\n                             .format(num_rows, max_rows,\n                                     row_start, obj_num_rows))\n        if not (0 <= num_cols <= max_cols):\n            raise IndexError(""num_cols={} should be in the range [0,{}] ""\n                             ""when col_start={} and obj.num_cols={}.""\n                             .format(num_cols, max_cols,\n                                     col_start, obj_num_cols))\n        if not (num_rows > 0 and num_cols > 0):\n            if not (num_rows == 0 and num_cols == 0):\n                raise IndexError(""num_rows and num_cols should both be ""\n                                 ""positive or they should both be 0."")\n        super(DoubleSubMatrix, self).__init__(obj, row_start, num_rows,\n                                        col_start, num_cols)\n\n\n################################################################################\n# vector/matrix wrappers\n################################################################################\n\n\ndef _vector_wrapper(vector):\n    """"""Constructs a new vector instance by swapping contents.\n\n    This function is used for converting `kaldi.matrix._kaldi_vector.Vector`\n    (or `kaldi.matrix._kaldi_vector.DoubleVector`) instances into `Vector`\n    (or `DoubleVector`) instances without copying the contents.\n\n    This is a destructive operation. Contents of the input vector are moved to\n    the newly constructed vector by swapping data pointers.\n\n    Args:\n        vector (`Vector` or `DoubleVector`): The input vector.\n\n    Returns:\n        Vector or DoubleVector: The new vector instance.\n    """"""\n    if isinstance(vector, _kaldi_vector.Vector):\n        return Vector().swap_(vector)\n    elif isinstance(vector, _kaldi_vector.DoubleVector):\n        return DoubleVector().swap_(vector)\n    else:\n        raise TypeError(""unrecognized input type"")\n\n\ndef _matrix_wrapper(matrix):\n    """"""Constructs a new matrix instance by swapping contents.\n\n    This function is used for converting `kaldi.matrix._kaldi_matrix.Matrix`\n    (or `kaldi.matrix._kaldi_matrix.DoubleMatrix`) instances into `Matrix`\n    (or `DoubleMatrix`) instances without copying the contents.\n\n    This is a destructive operation. Contents of the input matrix are moved to\n    the newly constructed matrix by swapping data pointers.\n\n    Args:\n        matrix (`Matrix` or `DoubleMatrix`): The input matrix.\n\n    Returns:\n        Matrix or DoubleMatrix: The new matrix instance.\n    """"""\n    if isinstance(matrix, _kaldi_matrix.Matrix):\n        return Matrix().swap_(matrix)\n    elif isinstance(matrix, _kaldi_matrix.DoubleMatrix):\n        return DoubleMatrix().swap_(matrix)\n    else:\n        raise TypeError(""unrecognized input type"")\n\n################################################################################\n\n_exclude_list = [\'sys\', \'numpy\']\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')\n           and not name in _exclude_list]\n'"
kaldi/matrix/_str.py,0,"b'# Adapted from pytorch tensor printing\n# https://github.com/pytorch/pytorch/blob/master/torch/_tensor_str.py\n\nimport math\nimport numpy\nfrom functools import reduce\n\n\nclass __PrinterOptions(object):\n    precision = 4\n    threshold = 1000\n    edgeitems = 3\n    linewidth = 80\n\n\nPRINT_OPTS = __PrinterOptions()\nSCALE_FORMAT = \'{:.5e} *\\n\'\n\n\n# We could use **kwargs, but this will give better docs\ndef set_printoptions(\n        precision=None,\n        threshold=None,\n        edgeitems=None,\n        linewidth=None,\n        profile=None,\n):\n    """"""Set options for printing. Items shamelessly taken from Numpy\n\n    Args:\n        precision: Number of digits of precision for floating point output\n            (default 8).\n        threshold: Total number of array elements which trigger summarization\n            rather than full repr (default 1000).\n        edgeitems: Number of array items in summary at beginning and end of\n            each dimension (default 3).\n        linewidth: The number of characters per line for the purpose of\n            inserting line breaks (default 80). Thresholded matricies will\n            ignore this parameter.\n        profile: Sane defaults for pretty printing. Can override with any of\n            the above options. (default, short, full)\n    """"""\n    if profile is not None:\n        if profile == ""default"":\n            PRINT_OPTS.precision = 4\n            PRINT_OPTS.threshold = 1000\n            PRINT_OPTS.edgeitems = 3\n            PRINT_OPTS.linewidth = 80\n        elif profile == ""short"":\n            PRINT_OPTS.precision = 2\n            PRINT_OPTS.threshold = 1000\n            PRINT_OPTS.edgeitems = 2\n            PRINT_OPTS.linewidth = 80\n        elif profile == ""full"":\n            PRINT_OPTS.precision = 4\n            PRINT_OPTS.threshold = float(\'inf\')\n            PRINT_OPTS.edgeitems = 3\n            PRINT_OPTS.linewidth = 80\n\n    if precision is not None:\n        PRINT_OPTS.precision = precision\n    if threshold is not None:\n        PRINT_OPTS.threshold = threshold\n    if edgeitems is not None:\n        PRINT_OPTS.edgeitems = edgeitems\n    if linewidth is not None:\n        PRINT_OPTS.linewidth = linewidth\n\n\ndef _range(*args, **kwargs):\n    return __builtins__[\'range\'](*args, **kwargs)\n\n\ndef _number_format(self, min_sz=-1):\n    min_sz = max(min_sz, 2)\n    temp = numpy.abs(self.reshape(self.size), dtype=float)\n\n    invalid_value_mask = ~numpy.isfinite(temp)\n    if invalid_value_mask.all():\n        example_value = 0\n    else:\n        example_value = temp[invalid_value_mask == 0][0]\n    temp[invalid_value_mask] = example_value\n    if invalid_value_mask.any():\n        min_sz = max(min_sz, 3)\n\n    int_mode = True\n    # TODO: use fmod?\n    for value in temp:\n        if value != math.ceil(value):\n            int_mode = False\n            break\n\n    exp_min = temp.min()\n    if exp_min != 0:\n        exp_min = math.floor(math.log10(exp_min)) + 1\n    else:\n        exp_min = 1\n    exp_max = temp.max()\n    if exp_max != 0:\n        exp_max = math.floor(math.log10(exp_max)) + 1\n    else:\n        exp_max = 1\n\n    scale = 1\n    exp_max = int(exp_max)\n    prec = PRINT_OPTS.precision\n    if int_mode:\n        if exp_max > prec + 1:\n            format = \'{{:11.{}e}}\'.format(prec)\n            sz = max(min_sz, 7 + prec)\n        else:\n            sz = max(min_sz, exp_max + 1)\n            format = \'{:\' + str(sz) + \'.0f}\'\n    else:\n        if exp_max - exp_min > prec:\n            sz = 7 + prec\n            if abs(exp_max) > 99 or abs(exp_min) > 99:\n                sz = sz + 1\n            sz = max(min_sz, sz)\n            format = \'{{:{}.{}e}}\'.format(sz, prec)\n        else:\n            if exp_max > prec + 1 or exp_max < 0:\n                sz = max(min_sz, 7)\n                scale = math.pow(10, exp_max - 1)\n            else:\n                if exp_max == 0:\n                    sz = 7\n                else:\n                    sz = exp_max + 6\n                sz = max(min_sz, sz)\n            format = \'{{:{}.{}f}}\'.format(sz, prec)\n    return format, scale, sz\n\n\ndef __repr_row(row, indent, fmt, scale, sz, truncate=None):\n    if truncate is not None:\n        dotfmt = "" {:^5} ""\n        return (indent +\n                \' \'.join(fmt.format(val / scale) for val in row[:truncate]) +\n                dotfmt.format(\'...\') +\n                \' \'.join(fmt.format(val / scale) for val in row[-truncate:]) +\n                \'\\n\')\n    else:\n        return indent + \' \'.join(fmt.format(val / scale) for val in row) + \'\\n\'\n\n\ndef _matrix_str(self, indent=\'\', formatter=None, force_truncate=False):\n    type_str = self.__module__ + \'.\' + self.__class__.__name__\n    self = self.numpy()\n    if self.size == 0:\n        return \'[{} with no elements]\\n\'.format(type_str)\n    n = PRINT_OPTS.edgeitems\n    has_hdots = self.shape[1] > 2 * n\n    has_vdots = self.shape[0] > 2 * n\n    print_full_mat = not has_hdots and not has_vdots\n\n    if formatter is None:\n        fmt, scale, sz = _number_format(self,\n                                        min_sz=5 if not print_full_mat else 0)\n    else:\n        fmt, scale, sz = formatter\n    nColumnPerLine = int(math.floor((PRINT_OPTS.linewidth - len(indent)) / (sz + 1)))\n    strt = \'\'\n    firstColumn = 0\n\n    if not force_truncate and \\\n       (self.size < PRINT_OPTS.threshold or print_full_mat):\n        while firstColumn < self.shape[1]:\n            lastColumn = min(firstColumn + nColumnPerLine - 1, self.shape[1] - 1)\n            if nColumnPerLine < self.shape[1]:\n                strt += \'\\n\' if firstColumn != 1 else \'\'\n                strt += \'Columns {} to {} \\n{}\'.format(\n                    firstColumn, lastColumn, indent)\n            if scale != 1:\n                strt += SCALE_FORMAT.format(scale)\n            for l in _range(self.shape[0]):\n                strt += indent + (\' \' if scale != 1 else \'\')\n                row_slice = self[l, firstColumn:lastColumn + 1]\n                strt += \' \'.join(fmt.format(val / scale) for val in row_slice)\n                strt += \'\\n\'\n            firstColumn = lastColumn + 1\n    else:\n        if scale != 1:\n            strt += SCALE_FORMAT.format(scale)\n        if has_vdots and has_hdots:\n            vdotfmt = ""{:^"" + str((sz + 1) * n - 1) + ""}""\n            ddotfmt = u""{:^5}""\n            for row in self[:n]:\n                strt += __repr_row(row, indent, fmt, scale, sz, n)\n            strt += indent + \' \'.join([vdotfmt.format(\'...\'),\n                                       ddotfmt.format(u\'\\u22F1\'),\n                                       vdotfmt.format(\'...\')]) + ""\\n""\n            for row in self[-n:]:\n                strt += __repr_row(row, indent, fmt, scale, sz, n)\n        elif not has_vdots and has_hdots:\n            for row in self:\n                strt += __repr_row(row, indent, fmt, scale, sz, n)\n        elif has_vdots and not has_hdots:\n            vdotfmt = u""{:^"" + \\\n                str(len(__repr_row(self[0], \'\', fmt, scale, sz))) + \\\n                ""}\\n""\n            for row in self[:n]:\n                strt += __repr_row(row, indent, fmt, scale, sz)\n            strt += vdotfmt.format(u\'\\u22EE\')\n            for row in self[-n:]:\n                strt += __repr_row(row, indent, fmt, scale, sz)\n        else:\n            for row in self:\n                strt += __repr_row(row, indent, fmt, scale, sz)\n    size_str = \'x\'.join(str(size) for size in self.shape)\n    strt += \'[{} of size {}]\\n\'.format(type_str, size_str)\n    return \'\\n\' + strt\n\n\ndef _vector_str(self):\n    type_str = self.__module__ + \'.\' + self.__class__.__name__\n    self = self.numpy()\n    if self.size == 0:\n        return \'[{} with no elements]\\n\'.format(type_str)\n    fmt, scale, sz = _number_format(self)\n    strt = \'\'\n    ident = \'\'\n    n = PRINT_OPTS.edgeitems\n    dotfmt = u""{:^"" + str(sz) + ""}\\n""\n    if scale != 1:\n        strt += SCALE_FORMAT.format(scale)\n        ident = \' \'\n    if self.size < PRINT_OPTS.threshold:\n        strt = (strt +\n                \'\\n\'.join(ident + fmt.format(val / scale) for val in self) +\n                \'\\n\')\n    else:\n        strt = (strt +\n                \'\\n\'.join(ident + fmt.format(val / scale) for val in self[:n]) +\n                \'\\n\' + (ident + dotfmt.format(u""\\u22EE"")) +\n                \'\\n\'.join(ident + fmt.format(val / scale) for val in self[-n:]) +\n                \'\\n\')\n    strt += \'[{} of size {}]\\n\'.format(type_str, self.size)\n    return \'\\n\' + strt\n'"
kaldi/matrix/common.py,0,"b""# FIXME: Relative/absolute import is buggy in Python 3.\nfrom _matrix_common import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/matrix/compressed.py,0,"b""from ._compressed_matrix import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/matrix/functions.py,0,"b'\nfrom . import _compressed_matrix\nfrom . import _kaldi_matrix\nfrom . import _kaldi_matrix_ext\nfrom . import _kaldi_vector\nfrom . import _kaldi_vector_ext\nimport _matrix_common # FIXME: Relative/absolute import is buggy in Python 3.\nfrom . import _sparse_matrix\nfrom . import _sp_matrix\nfrom . import _tp_matrix\n\nfrom ._matrix_functions import *\nfrom ._sp_matrix import SolverOptions\nfrom ._sp_matrix import solve_quadratic_problem\nfrom ._sp_matrix import solve_quadratic_matrix_problem\nfrom ._sp_matrix import solve_double_quadratic_matrix_problem\n\n\ndef approx_equal(a, b, tol=0.01):\n    """"""Checks if given vectors (or matrices) are approximately equal.\n\n    Args:\n        a (Vector or Matrix or SpMatrix or DoubleVector or DoubleMatrix or DoubleSpMatrix):\n            The first object.\n        b (Vector or Matrix or SpMatrix or DoubleVector or DoubleMatrix or DoubleSpMatrix):\n            The second object.\n        tol (float): The tolerance for the equality check. Defaults to ``0.01``.\n\n    Returns:\n        True if input objects have the same type and size, and\n        :math:`\\\\Vert a-b \\\\Vert \\\\leq \\\\mathrm{tol} \\\\times \\\\Vert a \\\\Vert`.\n\n    Raises:\n        TypeError: If the first object is not a vector or matrix instance.\n    """"""\n    if isinstance(a, (_kaldi_vector.VectorBase, _kaldi_vector.DoubleVectorBase,\n                      _kaldi_matrix.MatrixBase, _kaldi_matrix.DoubleMatrixBase,\n                      _sp_matrix.SpMatrix, _sp_matrix.DoubleSpMatrix)):\n        return a.approx_equal(b, tol)\n    raise TypeError(""a is not a vector or matrix instance"")\n\n\ndef assert_equal(a, b, tol=0.01):\n    """"""Asserts given vectors (or matrices) are approximately equal.\n\n    Args:\n        a (Vector or Matrix or SpMatrix or DoubleVector or DoubleMatrix or DoubleSpMatrix):\n            The first object.\n        b (Vector or Matrix or SpMatrix or DoubleVector or DoubleMatrix or DoubleSpMatrix):\n            The second object.\n        tol (float): The tolerance for the equality check. Defaults to ``0.01``.\n\n    Raises:\n        TypeError: If the first object is not a vector or matrix instance.\n        AssertionError: If input objects do not have the same type or size, or\n        :math:`\\\\Vert a-b \\\\Vert > \\\\mathrm{tol} \\\\times \\\\Vert a \\\\Vert`.\n    """"""\n    assert(approx_equal(a, b, tol))\n\n\ndef create_eigenvalue_matrix(real, imag, D=None):\n    """"""Creates the eigenvalue matrix.\n\n    Eigenvalue matrix :math:`D` is part of the decomposition used in eig.\n    :math:`D` will be block-diagonal with blocks of size 1 (for real\n    eigenvalues) or 2x2 for complex pairs. If a complex pair is :math:`\\\\lambda\n    +- i\\\\mu`, :math:`D` will have a corresponding 2x2 block :math:`[\\\\lambda,\n    \\\\mu; -\\\\mu, \\\\lambda]`. This function will throw if any complex eigenvalues\n    are not in complex conjugate pairs (or the members of such pairs are not\n    consecutively numbered). The D you supply must has correct dimensions.\n\n    Args:\n        real (Vector or DoubleVector): The real part of the eigenvalues.\n        imag (Vector or DoubleVector): The imaginary part of the eigenvalues.\n        D (Matrix or DoubleMatrix or None): The output matrix.\n            If provided, the eigenvalue matrix is written into this matrix.\n            If ``None``, the eigenvalue matrix is returned.\n            Defaults to ``None``.\n\n    Returns:\n        Matrix or DoubleMatrix: The eigenvalue matrix if **D** is ``None``.\n\n    Raises:\n        RuntimeError: If `real.dim != imag.dim`\n        TypeError: If input types are not supported.\n    """"""\n    if (isinstance(real, _kaldi_vector.VectorBase) and\n        isinstance(imag, _kaldi_vector.VectorBase)):\n        if D is None:\n            D = Matrix(real.dim, real.dim)\n            _kaldi_matrix._create_eigenvalue_matrix(real, imag, D)\n            return D\n        else:\n            _kaldi_matrix._create_eigenvalue_matrix(real, imag, D)\n    if (isinstance(real, _kaldi_vector.DoubleVectorBase) and\n        isinstance(imag, _kaldi_vector.DoubleVectorBase)):\n        if D is None:\n            D = DoubleMatrix(real.dim, real.dim)\n            _kaldi_matrix._create_eigenvalue_double_matrix(real, imag, D)\n            return D\n        else:\n            _kaldi_matrix._create_eigenvalue_double_matrix(real, imag, D)\n    raise TypeError(""real and imag should be vectors with the same data type."")\n\n\ndef sort_svd(s, U, Vt=None, sort_on_absolute_value=True):\n    """"""Sorts singular-value decomposition in-place.\n\n    SVD is :math:`U\\\\ diag(s)\\\\ V^T`.\n\n    This function is as generic as possible, to be applicable to other\n    types of problems. Requires `s.dim == U.num_cols`, and sorts from\n    greatest to least absolute value, moving the columns of **U**,\n    and the rows of **Vt**, if provided, around in the same way.\n\n    Note:\n        The ``absolute value\'\' part won\'t matter if this is an actual SVD,\n        since singular values are non-negative.\n\n    Args:\n        s (Vector): The singular values.\n        U (Matrix): The :math:`U` part of SVD.\n        Vt (Matrix): The :math:`V^T` part of SVD. Defaults to ``None``.\n        sort_on_absolute_value (bool): How to sort **s**.\n            If True, sort from greatest to least absolute value. Otherwise,\n            sort from greatest to least value. Defaults to ``True``.\n\n    Raises:\n        RuntimeError: If `s.dim != U.num_cols`.\n        TypeError: If input types are not supported.\n    """"""\n    if (isinstance(s, _kaldi_vector.VectorBase) and\n        isinstance(U, _kaldi_matrix.MatrixBase)):\n        _kaldi_matrix._sort_svd(s, U, Vt, sort_on_absolute_value)\n    if (isinstance(s, _kaldi_vector.DoubleVectorBase) and\n        isinstance(U, _kaldi_matrix.DoubleMatrixBase)):\n        _kaldi_matrix._sort_double_svd(s, U, Vt, sort_on_absolute_value)\n    raise TypeError(""s and U should respectively be a vector and matrix with ""\n                    ""matching data types."")\n\n\ndef filter_matrix_rows(matrix, keep_rows):\n    """"""Filters matrix rows.\n\n    The output is a matrix containing only the rows `r` of **in** such that\n    `keep_rows[r] == True`.\n\n    Args:\n        matrix (Matrix or SparseMatrix or CompressedMatrix or GeneralMatrix or DoubleMatrix or DoubleSparseMatrix):\n            The input matrix.\n        keep_rows (List[bool]): The list that determines which rows to keep.\n\n    Returns:\n        A new matrix constructed with the rows to keep.\n\n    Raises:\n        RuntimeError: If `matrix.num_rows != keep_rows.length`.\n        TypeError: If input matrix type is not supported.\n    """"""\n    if isinstance(matrix, _kaldi_matrix.Matrix):\n        return _sparse_matrix._filter_matrix_rows(matrix, keep_rows)\n    if isinstance(matrix, _sparse_matrix.SparseMatrix):\n        return _sparse_matrix._filter_sparse_matrix_rows(matrix, keep_rows)\n    if isinstance(matrix, _compressed_matrix.CompressedMatrix):\n        return _sparse_matrix._filter_compressed_matrix_rows(matrix, keep_rows)\n    if isinstance(matrix, _sparse_matrix.GeneralMatrix):\n        return _sparse_matrix._filter_general_matrix_rows(matrix, keep_rows)\n    if isinstance(matrix, _kaldi_matrix.DoubleMatrix):\n        return _sparse_matrix._filter_matrix_rows_double(matrix, keep_rows)\n    if isinstance(matrix, _sparse_matrix.DoubleSparseMatrix):\n        return _sparse_matrix._filter_sparse_matrix_rows_double(matrix, keep_rows)\n\n    raise TypeError(""input matrix type is not supported."")\n\n\ndef vec_vec(v1, v2):\n    """"""Returns the dot product of vectors.\n\n    Args:\n        v1 (Vector or DoubleVector): The first vector.\n        v2 (Vector or DoubleVector or SparseVector or DoubleSparseVector):\n            The second vector.\n\n    Returns:\n        The dot product of v1 and v2.\n\n    Raises:\n        RuntimeError: In case of size mismatch.\n        TypeError: If input types are not supported.\n    """"""\n    if isinstance(v1, _kaldi_vector.VectorBase):\n        if isinstance(v2, _kaldi_vector.VectorBase):\n            return _kaldi_vector._vec_vec(v1, v2)\n        elif isinstance(v2, _sparse_matrix.SparseVector):\n            return _sparse_matrix._vec_svec(v1, v2)\n    elif isinstance(v1, _kaldi_vector.DoubleVectorBase):\n        if isinstance(v2, _kaldi_vector.DoubleVectorBase):\n            return _kaldi_vector._vec_vec_double(v1, v2)\n        elif isinstance(v2, _sparse_matrix.DoubleSparseVector):\n            return _sparse_matrix._vec_svec_double(v1, v2)\n\n    raise TypeError(""v1 and v2 should be vectors with the same data type."")\n\n\ndef vec_mat_vec(v1, M, v2):\n    """"""Computes a vector-matrix-vector product.\n\n    Performs the operation :math:`v_1\\\\ M\\\\ v_2`.\n\n    Precision of input matrices should match.\n\n    Args:\n        v1 (Vector or DoubleVector): The first input vector.\n        M (Matrix or DoubleMatrix or SpMatrix): The input matrix.\n        v2 (Vector or DoubleVector): The second input vector.\n\n    Returns:\n       The vector-matrix-vector product.\n\n    Raises:\n       RuntimeError: In case of size mismatch.\n    """"""\n    if (isinstance(v1, _kaldi_vector.VectorBase) and\n        isinstance(v2, _kaldi_vector.VectorBase)):\n        if isinstance(M, _kaldi_matrix.MatrixBase):\n            return _kaldi_vector_ext._vec_mat_vec(v1, M, v2)\n        if isinstance(M, _sp_matrix.SpMatrix):\n            return _sp_matrix._vec_sp_vec(v1, M, v2)\n    elif (isinstance(v1, _kaldi_vector.DoubleVectorBase) and\n          isinstance(v2, _kaldi_vector.DoubleVectorBase)):\n        if isinstance(M, _kaldi_matrix.DoubleMatrixBase):\n            return _kaldi_vector_ext._vec_mat_vec_double(v1, M, v2)\n        if isinstance(M, _sp_matrix.DoubleSpMatrix):\n            return _sp_matrix._vec_sp_vec_double(v1, M, v2)\n\n    raise TypeError(""given combination of input types is not supported"")\n\ndef trace_mat(A):\n    """"""Returns the trace of :math:`A`.\n\n    Args:\n        A (Matrix or DoubleMatrix): The input matrix.\n    """"""\n    if isinstance(A, _kaldi_matrix.MatrixBase):\n        return _kaldi_matrix._trace_mat(A)\n    if isinstance(A, _kaldi_matrix.DoubleMatrixBase):\n        return _kaldi_matrix._trace_double_mat(A)\n    raise TypeError(""input matrix type is not supported"")\n\n\ndef trace_mat_mat(A, B, transA=_matrix_common.MatrixTransposeType.NO_TRANS):\n    """"""Returns the trace of :math:`A\\\\ B`.\n\n    Precision of input matrices should match.\n\n    Args:\n        A (Matrix or DoubleMatrix or SpMatrix or DoubleSpMatrix or SparseMatrix or DoubleSparseMatrix):\n            The first input matrix.\n        B (Matrix or DoubleMatrix or SpMatrix or DoubleSpMatrix or SparseMatrix or DoubleSparseMatrix):\n            The second input matrix.\n        transA (_matrix_common.MatrixTransposeType):\n            Whether to use **A** or its transpose.\n            Defaults to ``MatrixTransposeType.NO_TRANS``.\n        lower (bool): Whether to count lower-triangular elements only once.\n            Active only if both inputs are symmetric matrices.\n            Defaults to ``False``.\n    """"""\n    if isinstance(A, _kaldi_matrix.MatrixBase):\n        if isinstance(B, _kaldi_matrix.MatrixBase):\n            return _kaldi_matrix._trace_mat_mat(A, B, transA)\n        elif isinstance(B, _sparse_matrix.SparseMatrix):\n            return _sparse_matrix._trace_mat_smat(A, B, transA)\n    elif isinstance(A, _sp_matrix.SpMatrix):\n        if isinstance(B, _kaldi_matrix.MatrixBase):\n            return _sp_matrix._trace_sp_mat(A, B)\n        elif isinstance(B, _sp_matrix.SpMatrix):\n            if lower:\n                return _sp_matrix._trace_sp_sp_lower(A, B)\n            else:\n                return _sp_matrix._trace_sp_sp(A, B)\n    elif isinstance(A, _kaldi_matrix.DoubleMatrixBase):\n        if isinstance(B, _kaldi_matrix.DoubleMatrixBase):\n            return _kaldi_matrix._trace_double_mat_mat(A, B, transA)\n        elif isinstance(B, _sparse_matrix.DoubleSparseMatrix):\n            return _sparse_matrix._trace_double_mat_smat(A, B, transA)\n    elif isinstance(A, _sp_matrix.DoubleSpMatrix):\n        if isinstance(B, _sp_matrix.DoubleSpMatrix):\n            if lower:\n                return _sp_matrix._trace_double_sp_sp_lower(A, B)\n            else:\n                return _sp_matrix._trace_double_sp_sp(A, B)\n\n    raise TypeError(""given combination of matrix types is not supported"")\n\n\ndef trace_mat_mat_mat(A, B, C,\n                      transA=_matrix_common.MatrixTransposeType.NO_TRANS,\n                      transB=_matrix_common.MatrixTransposeType.NO_TRANS,\n                      transC=_matrix_common.MatrixTransposeType.NO_TRANS):\n    """"""Returns the trace of :math:`A\\\\ B\\\\ C`.\n\n    Precision of input matrices should match.\n\n    Args:\n        A (Matrix or DoubleMatrix): The first input matrix.\n        B (Matrix or DoubleMatrix or SpMatrix or DoubleSpMatrix):\n            The second input matrix.\n        C (Matrix or DoubleMatrix): The third input matrix.\n        transA (_matrix_common.MatrixTransposeType):\n            Whether to use **A** or its transpose.\n            Defaults to ``MatrixTransposeType.NO_TRANS``.\n        transB (_matrix_common.MatrixTransposeType):\n            Whether to use **B** or its transpose.\n            Defaults to ``MatrixTransposeType.NO_TRANS``.\n        transC (_matrix_common.MatrixTransposeType):\n            Whether to use **C** or its transpose.\n            Defaults to ``MatrixTransposeType.NO_TRANS``.\n    """"""\n    if isinstance(A, _kaldi_matrix.MatrixBase):\n        if (isinstance(B, _kaldi_matrix.MatrixBase) and\n            isinstance(C, _kaldi_matrix.MatrixBase)):\n            return _kaldi_matrix._trace_mat_mat_mat(A, transA, B, transB,\n                                                    C, transC)\n        elif (isinstance(B, _sp_matrix.SpMatrix) and\n              isinstance(C, _kaldi_matrix.MatrixBase)):\n            return _sp_matrix._trace_mat_sp_mat(A, transA, B, C, transC)\n    elif isinstance(A, _kaldi_matrix.DoubleMatrixBase):\n        if (isinstance(B, _kaldi_matrix.DoubleMatrixBase) and\n            isinstance(C, _kaldi_matrix.DoubleMatrixBase)):\n            return _kaldi_matrix._trace_double_mat_mat_mat(A, transA, B, transB,\n                                                           C, transC)\n        elif (isinstance(B, _sp_matrix.DoubleSpMatrix) and\n              isinstance(C, _kaldi_matrix.DoubleMatrixBase)):\n            return _sp_matrix._trace_double_mat_sp_mat(A, transA, B, C, transC)\n\n    raise TypeError(""given combination of matrix types is not supported"")\n\n\ndef trace_mat_mat_mat_mat(A, B, C, D,\n                          transA=_matrix_common.MatrixTransposeType.NO_TRANS,\n                          transB=_matrix_common.MatrixTransposeType.NO_TRANS,\n                          transC=_matrix_common.MatrixTransposeType.NO_TRANS,\n                          transD=_matrix_common.MatrixTransposeType.NO_TRANS):\n    """"""Returns the trace of :math:`A\\\\ B\\\\ C\\\\ D`.\n\n    Precision of input matrices should match.\n\n    Args:\n        A (Matrix or DoubleMatrix): The first input matrix.\n        B (Matrix or DoubleMatrix or SpMatrix or DoubleSpMatrix):\n            The second input matrix.\n        C (Matrix or DoubleMatrix): The third input matrix.\n        D (Matrix or DoubleMatrix or SpMatrix or DoubleSpMatrix):\n            The fourth input matrix.\n        transA (_matrix_common.MatrixTransposeType):\n            Whether to use **A** or its transpose.\n            Defaults to ``MatrixTransposeType.NO_TRANS``.\n        transB (_matrix_common.MatrixTransposeType):\n            Whether to use **B** or its transpose.\n            Defaults to ``MatrixTransposeType.NO_TRANS``.\n        transC (_matrix_common.MatrixTransposeType):\n            Whether to use **C** or its transpose.\n            Defaults to ``MatrixTransposeType.NO_TRANS``.\n        transD (_matrix_common.MatrixTransposeType):\n            Whether to use **D** or its transpose.\n            Defaults to ``MatrixTransposeType.NO_TRANS``.\n    """"""\n    if isinstance(A, _kaldi_matrix.MatrixBase):\n        if (isinstance(B, _kaldi_matrix.MatrixBase) and\n            isinstance(C, _kaldi_matrix.MatrixBase) and\n            isinstance(D, _kaldi_matrix.MatrixBase)):\n            return _kaldi_matrix._trace_mat_mat_mat_mat(A, transA, B, transB,\n                                                        C, transC, D, transD)\n        elif (isinstance(B, _sp_matrix.SpMatrix) and\n              isinstance(C, _kaldi_matrix.MatrixBase) and\n              isinstance(D, _sp_matrix.SpMatrix)):\n            return _sp_matrix._trace_mat_sp_mat_sp(A, transA, B, C, transC, D)\n    elif isinstance(A, _kaldi_matrix.DoubleMatrixBase):\n        if (isinstance(B, _kaldi_matrix.DoubleMatrixBase) and\n            isinstance(C, _kaldi_matrix.DoubleMatrixBase) and\n            isinstance(D, _kaldi_matrix.DoubleMatrixBase)):\n            return _kaldi_matrix._trace_double_mat_mat_mat_mat(\n                A, transA, B, transB, C, transC, D, transD)\n        elif (isinstance(B, _sp_matrix.DoubleSpMatrix) and\n              isinstance(C, _kaldi_matrix.DoubleMatrixBase) and\n              isinstance(D, _sp_matrix.DoubleSpMatrix)):\n            return _sp_matrix._trace_double_mat_sp_mat_sp(A, transA, B,\n                                                          C, transC, D)\n\n    raise TypeError(""given combination of matrix types is not supported"")\n\n################################################################################\n\n__all__ = [name for name in dir() if name[0] != \'_\']\n'"
kaldi/matrix/htk.py,0,"b""from ._kaldi_matrix import HtkHeader, read_htk, write_htk, write_sphinx\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/matrix/optimization.py,0,"b""from ._optimization import *\n\n__all__ = [name for name in dir() if name[0] != '_']\n"""
kaldi/matrix/packed.py,0,"b'from . import _kaldi_matrix\nimport _matrix_common # FIXME: Relative/absolute import is buggy in Python 3.\nfrom . import _packed_matrix\nfrom . import _sp_matrix\nfrom . import _tp_matrix\n\n################################################################################\n# single precision packed matrix types\n################################################################################\n\nclass _PackedMatrixBase(object):\n    """"""Base class defining the extra API for single precision packed matrices.\n\n    No constructor.\n    """"""\n    def size(self):\n        """"""Returns size as a tuple.\n\n        Returns:\n            A tuple (num_rows, num_cols) of integers.\n        """"""\n        return self.num_rows, self.num_cols\n\n    def swap_(self, other):\n        """"""Swaps the contents with another matrix.\n\n        Shallow swap.\n\n        Args:\n            other (Matrix or SpMatrix or TpMatrix): The input matrix.\n\n        Raises:\n            ValueError: If **other** is not a square matrix.\n        """"""\n        # Using the native code instead of size()\n        # prevents an exception for the case when \n        # other is not a python object\n        m, n = other.num_rows, other.num_cols \n        if m != n:\n            raise ValueError(""other is not a square matrix."")\n        if isinstance(other, _kaldi_matrix.Matrix):\n            return self.swap_with_matrix_(other)\n        elif isinstance(other, _packed_matrix.PackedMatrix):\n            return self.swap_with_packed_(other)\n        else:\n            raise ValueError(""other must be a Matrix or SpMatrix or TpMatrix."")\n\n\nclass SpMatrix(_PackedMatrixBase, _sp_matrix.SpMatrix):\n    """"""Single precision symmetric matrix.""""""\n\n    def __init__(self, num_rows = None,\n                resize_type=_matrix_common.MatrixResizeType.SET_ZERO):\n        """"""Creates a new symmetric matrix.\n\n        If `num_rows` is not ``None``, initializes the symmetric matrix to the\n        specified size. Otherwise, initializes an empty symmetric matrix.\n\n        Args:\n            num_rows (int): The number of rows. Defaults to ``None``.\n            resize_type (MatrixResizeType): How to initialize the elements.\n                If ``MatrixResizeType.SET_ZERO`` or\n                ``MatrixResizeType.COPY_DATA``, they are set to zero.\n                If ``MatrixResizeType.UNDEFINED``, they are left uninitialized.\n                Defaults to ``MatrixResizeType.SET_ZERO``.\n        """"""\n        super(SpMatrix, self).__init__()\n        if num_rows is not None:\n            if isinstance(num_rows, int) and num_rows >= 0:\n                self.resize_(num_rows, resize_type)\n            else:\n                raise ValueError(""num_rows should be a non-negative integer."")\n\n    def clone(self):\n        """"""Clones the symmetric matrix.\n\n        Returns:\n            SpMatrix: A copy of the symmetric matrix.\n        """"""\n        return SpMatrix(len(self)).copy_from_sp_(self)\n\n\nclass TpMatrix(_PackedMatrixBase, _tp_matrix.TpMatrix):\n    """"""Single precision triangular matrix.""""""\n\n    def __init__(self, num_rows = None,\n                 resize_type=_matrix_common.MatrixResizeType.SET_ZERO):\n        """"""Initializes a new triangular matrix.\n\n        If `num_rows` is not ``None``, initializes the triangular matrix to the\n        specified size. Otherwise, initializes an empty triangular matrix.\n\n        Args:\n            num_rows (int): Number of rows. Defaults to None.\n            resize_type (MatrixResizeType): How to initialize the elements.\n                If ``MatrixResizeType.SET_ZERO`` or\n                ``MatrixResizeType.COPY_DATA``, they are set to zero.\n                If ``MatrixResizeType.UNDEFINED``, they are left uninitialized.\n                Defaults to ``MatrixResizeType.SET_ZERO``.\n        """"""\n        super(TpMatrix, self).__init__()\n        if num_rows is not None:\n            if isinstance(num_rows, int) and num_rows >= 0:\n                self.resize_(num_rows, resize_type)\n            else:\n                raise ValueError(""num_rows should be a non-negative integer."")\n\n    def clone(self):\n        """"""Clones the triangular matrix.\n\n        Returns:\n            TpMatrix: A copy of the triangular matrix.\n        """"""\n        return TpMatrix(len(self)).copy_from_tp_(self)\n\n################################################################################\n# double precision packed matrix types\n################################################################################\n\nclass _DoublePackedMatrixBase(object):\n    """"""Base class defining the extra API for double precision packed matrices.\n\n    No constructor.\n    """"""\n    def size(self):\n        """"""Returns size as a tuple.\n\n        Returns:\n            A tuple (num_rows, num_cols) of integers.\n        """"""\n        return self.num_rows, self.num_cols\n\n    def swap_(self, other):\n        """"""Swaps the contents with another matrix.\n\n        Shallow swap.\n\n        Args:\n            other (DoubleMatrix or DoubleSpMatrix or DoubleTpMatrix):\n                The input matrix.\n\n        Raises:\n            ValueError: If **other** is not a square matrix.\n        """"""\n        m, n = other.num_rows, other.num_cols\n        if m != n:\n            raise ValueError(""other is not a square matrix."")\n        if isinstance(other, _kaldi_matrix.DoubleMatrix):\n            return self.swap_with_matrix_(other)\n        elif isinstance(other, _packed_matrix.DoublePackedMatrix):\n            return self.swap_with_packed_(other)\n        else:\n            raise ValueError(""other must be a DoubleMatrix or DoubleSpMatrix ""\n                             ""or DoubleTpMatrix."")\n\n\nclass DoubleSpMatrix(_DoublePackedMatrixBase, _sp_matrix.DoubleSpMatrix):\n    """"""Double precision symmetric matrix.""""""\n\n    def __init__(self, num_rows = None,\n                 resize_type=_matrix_common.MatrixResizeType.SET_ZERO):\n        """"""Creates a new symmetric matrix.\n\n        If `num_rows` is not ``None``, initializes the symmetric matrix to the\n        specified size. Otherwise, initializes an empty symmetric matrix.\n\n        Args:\n            num_rows (int): Number of rows. Defaults to None.\n            resize_type (MatrixResizeType): How to initialize the elements.\n                If ``MatrixResizeType.SET_ZERO`` or\n                ``MatrixResizeType.COPY_DATA``, they are set to zero.\n                If ``MatrixResizeType.UNDEFINED``, they are left uninitialized.\n                Defaults to ``MatrixResizeType.SET_ZERO``.\n        """"""\n        super(DoubleSpMatrix, self).__init__()\n        if num_rows is not None:\n            if isinstance(num_rows, int) and num_rows >= 0:\n                self.resize_(num_rows, resize_type)\n            else:\n                raise ValueError(""num_rows should be a non-negative integer."")\n\n    def clone(self):\n        """"""Clones the symmetric matrix.\n\n        Returns:\n            DoubleSpMatrix: A copy of the symmetric matrix.\n        """"""\n        return DoubleSpMatrix(len(self)).copy_from_sp_(self)\n\n\nclass DoubleTpMatrix(_DoublePackedMatrixBase, _tp_matrix.DoubleTpMatrix):\n    """"""Double precision triangular matrix.""""""\n\n    def __init__(self, num_rows = None,\n                 resize_type=_matrix_common.MatrixResizeType.SET_ZERO):\n        """"""Initializes a new triangular matrix.\n\n        If `num_rows` is not ``None``, initializes the triangular matrix to the\n        specified size. Otherwise, initializes an empty triangular matrix.\n\n        Args:\n            num_rows (int): Number of rows. Defaults to None.\n            resize_type (MatrixResizeType): How to initialize the elements.\n                If ``MatrixResizeType.SET_ZERO`` or\n                ``MatrixResizeType.COPY_DATA``, they are set to zero.\n                If ``MatrixResizeType.UNDEFINED``, they are left uninitialized.\n                Defaults to ``MatrixResizeType.SET_ZERO``.\n        """"""\n        super(DoubleTpMatrix, self).__init__()\n        if num_rows is not None:\n            if isinstance(num_rows, int) and num_rows >= 0:\n                self.resize_(num_rows, resize_type)\n            else:\n                raise ValueError(""num_rows should be a non-negative integer."")\n\n    def clone(self):\n        """"""Clones the triangular matrix.\n\n        Returns:\n            DoubleTpMatrix: A copy of the triangular matrix.\n        """"""\n        return DoubleTpMatrix(len(self)).copy_from_tp_(self)\n\n################################################################################\n\ndef _sp_matrix_wrapper(matrix):\n    """"""Constructs a new matrix instance by swapping contents.\n    This function is used for converting `kaldi.matrix._sp_matrix.SpMatrix`\n    instances into `SpMatrix` instances without copying the contents.\n    This is a destructive operation. Contents of the input matrix are moved to\n    the newly contstructed matrix by swapping data pointers.\n    Args:\n        matrix (`Matrix`): The input matrix.\n    Returns:\n        SpMatrix: The new matrix instance.\n    """"""\n    if isinstance(matrix, _sp_matrix.SpMatrix):\n        return SpMatrix().swap_(matrix)\n    elif isinstance(matrix, _sp_matrix.DoubleSpMatrix):\n        return DoubleSpMatrix().swap_(matrix)\n    else:\n        raise TypeError(""unrecognized input type"")\n\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/matrix/sparse.py,0,"b""from ._sparse_matrix import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/nnet3/__init__.py,0,"b""from ._natural_gradient_online import *\nfrom ._nnet_common import *\nfrom ._nnet_example import *\nfrom ._nnet_parse import *\nfrom ._nnet_computation_graph_ext import *\nfrom ._nnet_misc_computation_info import *\nfrom ._nnet_component_itf import *\nfrom ._nnet_simple_component import *\nfrom ._nnet_combined_component import *\nfrom ._nnet_normalize_component import *\nfrom ._convolution import *\nfrom ._nnet_convolutional_component import *\nfrom ._attention import *\nfrom ._nnet_attention_component import *\nfrom ._nnet_general_component import *\nfrom ._nnet_descriptor import *\nfrom ._nnet_nnet import *\nfrom ._nnet_computation import *\nfrom ._nnet_test_utils import *\nfrom ._nnet_graph import *\nfrom ._nnet_compile import *\nfrom ._nnet_compile_utils import *\nfrom ._nnet_compile_looped import *\nfrom ._nnet_analyze import *\nfrom ._nnet_compute import *\nfrom ._nnet_batch_compute import *\nfrom ._nnet_optimize import *\nfrom ._nnet_optimize_utils import *\nfrom ._nnet_computation_graph import *\nfrom ._nnet_example_utils import *\nfrom ._nnet_utils import *\nfrom ._nnet_diagnostics import *\nfrom ._nnet_training import *\nfrom ._nnet_chain_example import *\nfrom ._nnet_chain_example_ext import *\nfrom ._nnet_chain_example_merger import *\nfrom ._nnet_chain_training import *\nfrom ._nnet_chain_diagnostics import *\nfrom ._am_nnet_simple import *\nfrom ._nnet_am_decodable_simple import *\nfrom ._decodable_simple_looped import *\nfrom ._decodable_online_looped import *\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/online2/__init__.py,0,"b'""""""\n------------------\nOnline Endpointing\n------------------\n\nThis module contains a simple facility for endpointing, that should be used in\nconjunction with the online decoding code.  By endpointing in this context we\nmean ""deciding when to stop decoding"", and not generic speech/silence\nsegmentation.  The use-case that we have in mind is some kind of dialog system\nwhere, as more speech data comes in, we decode more and more, and we have to\ndecide when to stop decoding.\n\nThe endpointing rule is a disjunction of conjunctions.  The way we have\nit configured, it\'s an OR of five rules, and each rule has the following form::\n\n  (<contains-nonsilence> || !rule.must_contain_nonsilence)\n  && <length-of-trailing-silence> >= rule.min_trailing_silence\n  && <relative-cost> <= rule.max_relative_cost\n  && <utterance-length> >= rule.min_utterance_length\n\nwhere:\n\n<contains-nonsilence>\n    is true if the best traceback contains any nonsilence phone;\n<length-of-trailing-silence>\n    is the length in seconds of silence phones at the end of the best traceback\n    (we stop counting when we hit non-silence),\n<relative-cost>\n    is a value >= 0 extracted from the decoder, that is zero if a final-state of\n    the grammar FST had the best cost at the final frame, and infinity if no\n    final-state was active (and >0 for in-between cases).\n<utterance-length>\n    is the number of seconds of the utterance that we have decoded so far.\n\nAll of these pieces of information are obtained from the best-path traceback\nfrom the decoder, which is output by the function :meth:`get_best_path`. We do\nthis every time we\'re finished processing a chunk of data.\n\nFor details of the default rules, see `OnlineEndpointConfig`.\n\nIt\'s up to the caller whether to use final-probs or not when generating the\nbest-path, i.e. ``decoder.get_best_path(use_final_probs=True|False)``, but we\nrecommend not using them.  If you do use them, then depending on the grammar,\nyou may force the best-path to decode non-silence even though that was not what\nit really preferred to decode.\n""""""\n\nfrom ._online_ivector import *\n\nfrom ._online_endpoint import *\nfrom ._online_feature_pipeline import *\nfrom ._online_gmm_decodable import *\nfrom ._online_gmm_decoding import *\nfrom ._online_nnet2_feature_pipeline import *\nfrom ._online_nnet3_decoding import *\nfrom ._online_nnet3_decoding_ext import *\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/online2/_online_ivector.py,0,"b'from .. import matrix as _matrix\n\nfrom ._online_ivector_feature import *\nfrom ._online_ivector_feature import _OnlineIvectorExtractionInfo\n\nclass OnlineIvectorExtractionInfo(_OnlineIvectorExtractionInfo):\n    """"""Configuration options for online iVector extraction.""""""\n    @property\n    def lda_mat(self):\n        return _matrix.SubMatrix(self._lda_mat)\n\n    @lda_mat.setter\n    def lda_mat(self, value):\n        self._lda_mat = value\n\n    @property\n    def global_cmvn_stats(self):\n        return _matrix.DoubleSubMatrix(self._global_cmvn_stats)\n\n    @global_cmvn_stats.setter\n    def global_cmvn_stats(self, value):\n        self._global_cmvn_stats = value\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/rnnlm/__init__.py,0,"b""from ._compute_state import *\n\nfrom ._sampler import *\nfrom ._sampling_lm_estimate import *\nfrom ._sampling_lm import *\nfrom ._rnnlm_utils import *\nfrom ._rnnlm_example import *\nfrom ._rnnlm_example_creator import *\nfrom ._rnnlm_example_utils import *\nfrom ._rnnlm_core_training import *\nfrom ._rnnlm_core_compute import *\nfrom ._rnnlm_lattice_rescoring import *\nfrom ._rnnlm_embedding_training import *\nfrom ._rnnlm_training import *\n\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/rnnlm/_compute_state.py,0,"b'from ._rnnlm_compute_state import *\n\nclass RnnlmComputeStateInfo(RnnlmComputeStateInfo):\n    """"""State information for RNNLM computation.\n\n    This class keeps references to the word-embedding, nnet3 part of RNNLM\n    and the RnnlmComputeStateComputationOptions. It handles the computation\n    of the nnet3 object.\n\n    Args:\n        opts (RnnlmComputeStateComputationOptions): Options for RNNLM compute\n            state.\n        rnnlm (Nnet): The nnet part of the RNNLM.\n        word_embedding_mat (CuMatrix): The word embedding matrix.\n    """"""\n    def __init__(self, opts, rnnlm, word_embedding_mat):\n        super(RnnlmComputeStateInfo, self).__init__(opts, rnnlm,\n                                                    word_embedding_mat)\n        self._opts = opts\n        self._rnnlm = rnnlm\n        self._word_embedding_mat = word_embedding_mat\n\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/sgmm2/__init__.py,0,"b""from ._am_sgmm2 import *\nfrom ._decodable_am_sgmm2 import *\nfrom ._estimate_am_sgmm2 import *\nfrom ._fmllr_sgmm2 import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/tfrnnlm/__init__.py,0,"b'try:\n    from ._tensorflow_rnnlm import *\nexcept:\n    import logging\n    logging.error(""Cannot import the Python module for TensorFlow RNNLM."")\n    logging.error(""This module depends on kaldi-tensorflow-rnnlm library."")\n    logging.error(""See kaldi/src/tfrnnlm/Makefile for details."")\n    raise\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/transform/__init__.py,0,"b""from ._transform_common import *\nfrom ._compressed_transform_stats import *\nfrom ._regression_tree import *\n\nfrom . import cmvn\nfrom . import fmpe\nfrom . import lda\nfrom . import lvtln\nfrom . import mllr\nfrom . import mllt\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/transform/cmvn.py,0,"b'from . import _cmvn\nfrom .. import matrix\nfrom ..matrix import _kaldi_matrix\nfrom ..matrix import _kaldi_vector\nfrom ..util import io\n\nclass Cmvn(object):\n    """"""Cepstral mean variance normalization (CMVN).\n\n    This class is used for accumulating CMVN statistics and applying CMVN using\n    accumulated statistics. Global CMVN can be computed and applied as follows::\n\n        cmvn = Cmvn(40)  # 40 dimensional features\n\n        for key, feats in SequentialMatrixReader(""ark:feats.ark""):\n            cmvn.accumulate(feats)\n\n        with MatrixWriter(""ark:feats_norm.ark"") as writer:\n            for key, feats in SequentialMatrixReader(""ark:feats.ark""):\n                cmvn.apply(feats, norm_vars=True)\n                writer[key] = feats\n\n    Attributes:\n        stats (DoubleMatrix or None): Accumulated mean/variance statistics\n            matrix of size `2 x dim+1`. This matrix is initialized when a CMVN\n            object is instantiated by specifying a feature dimension or when the\n            :meth:`init` method is explicitly called. It is ``None`` otherwise.\n            `stats[0,:-1]` represents the sum of accumulated feature vectors.\n            `stats[1,:-1]` represents the sum of element-wise squares of\n            accumulated feature vectors. `stats[0,-1]` represents the total\n            count of accumulated feature vectors. `stats[1,-1]` is initialized\n            to zero but otherwise is not used.\n    """"""\n    def __init__(self, dim=None):\n        """"""\n        Args:\n            dim (int): The feature dimension. If specified, :attr:`stats` matrix\n                is initialized with the given dimension.\n        """"""\n        self.init(dim)\n\n    def accumulate(self, feats, weights=None):\n        """"""Accumulates CMVN statistics.\n\n        Computes the CMVN statistics for given features and adds them to the\n        :attr:`stats` matrix.\n\n        Args:\n            feats (VectorBase or MatrixBase): The input features.\n            weights (float or VectorBase): The frame weights. If `feats` is a\n                single feature vector, then `weights` should be a single number.\n                If `feats` is a feature matrix, then `weights` should be a\n                vector of size `feats.num_rows`. If `weights` is not specified\n                or ``None``, then no frame weighting is done.\n        """"""\n        if not self.stats:\n            raise ValueError(""CMVN stats matrix is not initialized. Initialize ""\n                             ""it either by reading it from file or by calling ""\n                             ""the init method to accumulate new statistics or ""\n                             ""by directly setting the stats attribute."")\n        if isinstance(feats, _kaldi_matrix.MatrixBase):\n            _cmvn.acc_cmvn_stats(feats, weights, self.stats)\n        elif isinstance(feats, _kaldi_vector.VectorBase):\n            if weights is None:\n                weights = 1.0\n            _cmvn.acc_cmvn_stats_single_frame(feats, weights, self.stats)\n        else:\n            raise TypeError(""Input feature should be a matrix or vector."")\n\n    def apply(self, feats, norm_vars=False, reverse=False):\n        """"""Applies CMVN to the given feature matrix.\n\n        Args:\n            feats (Matrix): The feature matrix to normalize.\n            norm_vars (bool): Whether to apply variance normalization.\n            reverse (bool): Whether to apply CMVN in a reverse sense, so as to\n                transform zero-mean, unit-variance features into features with\n                the desired mean and variance.\n        """"""\n        if not self.stats:\n            raise ValueError(""CMVN stats matrix is not initialized. Initialize ""\n                             ""it either by reading it from file or by calling ""\n                             ""the init method and accumulating new statistics ""\n                             ""or by directly setting the stats attribute."")\n        if reverse:\n            _cmvn.apply_cmvn_reverse(self.stats, norm_vars, feats)\n        else:\n            _cmvn.apply_cmvn(self.stats, norm_vars, feats)\n\n    def init(self, dim):\n        """"""Initializes the CMVN statistics matrix.\n\n        This method is called during object initialization. It can also be\n        called at a later time to initialize or reset the internal statistics\n        matrix.\n\n        Args:\n            dim (int or None): The feature dimension. If ``None``, then\n                :attr:`stats` attribute is set to ``None``. Otherwise, it is\n                initialized as a `2 x dim+1` matrix of zeros.\n    """"""\n        if dim is None:\n            self.stats = None\n        else:\n            assert(dim > 0)\n            self.stats = matrix.DoubleMatrix(2, dim + 1)\n\n    def read_stats(self, rxfilename, binary=True):\n        """"""Reads CMVN statistics from file.\n\n        Args:\n            rxfilename (str): Extended filename for reading CMVN statistics.\n            binary (bool): Whether to open the file in binary mode.\n        """"""\n        with io.Input(rxfilename, binary=binary) as ki:\n            self.stats = matrix.DoubleMatrix().read_(ki.stream(), ki.binary)\n\n    def skip_dims(self, dims):\n        """"""Modifies the stats to skip normalization for given dimensions.\n\n        This is a destructive operation. The statistics for given dimensions are\n        replaced with fake values that effectively disable normalization in\n        those dimensions. This method should only be called after statistics\n        accumulation is finished since accumulation modifies the :attr:`stats`\n        matrix.\n\n        Args:\n            dims(List[int]): Dimensions for which to skip normalization.\n        """"""\n        _cmvn.fake_stats_for_some_dims(dims, self.stats)\n\n    def write_stats(self, wxfilename, binary=True):\n        """"""Writes CMVN statistics to file.\n\n        Args:\n            wxfilename (str): Extended filename for writing CMVN statistics.\n            binary (bool): Whether to open the file in binary mode.\n        """"""\n        with io.Output(wxfilename, binary=binary) as ko:\n            self.stats.write(ko.stream(), binary)\n\n\n__all__ = [\'Cmvn\']\n'"
kaldi/transform/fmpe.py,0,"b""from ._fmpe import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/transform/lda.py,0,"b""from ._lda_estimate import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/transform/lvtln.py,0,"b""from ._lvtln import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/transform/mllr.py,0,"b""from ._fmllr_diag_gmm import *\nfrom ._fmllr_raw import *\nfrom ._basis_fmllr_diag_gmm import *\nfrom ._regtree_fmllr_diag_gmm import *\nfrom ._regtree_mllr_diag_gmm import *\nfrom ._decodable_am_diag_gmm_regtree import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/transform/mllt.py,0,"b""from ._mllt import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/tree/__init__.py,0,"b""from ._build_tree import *\nfrom ._build_tree_questions import *\nfrom ._build_tree_utils import *\nfrom ._clusterable_classes import *\nfrom ._cluster_utils import *\nfrom ._context_dep import *\nfrom ._event_map import *\nfrom ._tree_renderer import *\n\n__all__ = [name for name in dir()\n           if name[0] != '_'\n           and not name.endswith('Base')]\n"""
kaldi/util/__init__.py,0,"b'from ._kaldi_thread import *\nfrom ._const_integer_set import *\nfrom ._edit_distance import *\n\n# This was adapted from CLIF to make sure it is available even if pyclif is not.\ndef _value_error_on_false(ok, *args):\n    """"""Returns None / args[0] / args if ok.""""""\n    if not isinstance(ok, bool):\n        raise TypeError(""first argument should be a bool"")\n    if not ok:\n        raise ValueError(""call failed"")\n    if args:\n        return args if len(args) > 1 else args[0]\n    return None\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/util/io.py,0,"b'""""""\nFor detailed documentation of Kaldi input/output streams and extended filenames,\nsee `Kaldi I/O mechanisms`_ and `Kaldi I/O from a command-line perspective`_.\n\n.. _Kaldi I/O mechanisms:\n   http://kaldi-asr.org/doc/io.html\n.. _Kaldi I/O from a command-line perspective:\n   http://kaldi-asr.org/doc/io_tut.html\n""""""\n\n\nfrom ..base import io as _base_io\nfrom . import _kaldi_io\nfrom ._kaldi_io import *\n\n\nclass Input(_kaldi_io.Input):\n    """"""Input stream for reading from extended filenames.\n\n    If **rxfilename** is provided, it is opened for reading.\n\n    If **binary** is ``True``, the input stream is opened in binary mode.\n    Otherwise, it is opened in text mode. If the stream is opened in binary mode\n    and it has Kaldi binary mode header, `self.binary` attribute is set to\n    ``True``. Similar to how files are handled in Python, PyKaldi distinguishes\n    between input streams opened in binary and text modes, even when the\n    underlying operating system doesn\'t. If the input stream is opened in binary\n    mode, `read` and `readline` methods return contents as `bytes` objects\n    without any decoding. In text mode, these methods return contents of the\n    input stream as `unicode` strings, the bytes having been first decoded using\n    the platform-dependent default encoding.\n\n    This class implements the iterator and context manager protocols.\n\n    Args:\n        rxfilename (str): Extended filename to open for reading.\n        binary (bool): Whether to open the stream in binary mode.\n\n    Attributes:\n        binary (bool): Whether the contents of the input stream are binary.\n            This attribute is set to ``True`` if the stream is opened in binary\n            mode and it has Kaldi binary mode header. Its value is valid only\n            if the stream is still open.\n    """"""\n\n    def __init__(self, rxfilename=None, binary=True):\n        super(Input, self).__init__()\n        self.binary = False\n        if rxfilename is not None:\n            self.open(rxfilename, binary)\n\n    def __del__(self):\n        if self.is_open():\n            self.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        if self.is_open():\n            self.close()\n\n    def __iter__(self):\n        while True:\n            line = self.readline()\n            if not line:\n                break\n            yield line\n\n    def open(self, rxfilename, binary=True):\n        """"""Opens the stream for reading.\n\n        Args:\n            rxfilename (str): Extended filename to open for reading.\n            binary (bool): Whether to open the stream in binary mode.\n        """"""\n        if binary:\n            success, self.binary = super(Input, self).open(rxfilename)\n        else:\n            success = self._open_text_mode(rxfilename)\n        if not success:\n            raise IOError(""Could not open stream for reading."")\n        self._read = _base_io.read if binary else _base_io.read_text\n        self._readline = _base_io.readline if binary else _base_io.readline_text\n\n    def read(self):\n        """"""Reads and returns the contents of the stream.\n\n        If stream was opened in binary mode, returns a `bytes` object.\n        Otherwise, returns a `unicode` string.\n        """"""\n        if not self.is_open():\n            raise ValueError(""I/O operation on closed stream."")\n        return self._read(self.stream())\n\n    def readline(self):\n        """"""Reads and returns a line from the stream.\n\n        If stream was opened in binary mode, returns a `bytes` object.\n        Otherwise, returns a `unicode` string. If the stream is at EOF,\n        an empty object is returned.\n        """"""\n        if not self.is_open():\n            raise ValueError(""I/O operation on closed stream."")\n        return self._readline(self.stream())\n\n    def readlines(self):\n        """"""Reads and returns the contents of the stream as a list of lines.\n\n        If stream was opened in binary mode, returns a list of `bytes` objects.\n        Otherwise, returns a list of `unicode` strings.\n        """"""\n        return list(self)\n\n\nclass Output(_kaldi_io.Output):\n    """"""Output stream for writing to extended filenames.\n\n    If **wxfilename** is provided, it is opened for writing.\n\n    If **binary** is ``True``, the output stream is opened in binary mode.\n    Otherwise it is opened in text mode. Similar to how files are handled in\n    Python, PyKaldi distinguishes between output streams opened in binary and\n    text modes, even when the underlying operating system doesn\'t. If the output\n    stream is opened in binary mode, `write` and `writelines` methods accept\n    `bytes` objects. Otherwise, they accept `unicode` strings.\n\n    If **write_header** is ``True`` and the stream was opened in binary mode,\n    then Kaldi binary mode header (`\\\\\\\\0` then `B`) is written to the beginning\n    of the stream. This header is checked by PyKaldi input streams opened in\n    binary mode to set the `~Input.binary` attribute.\n\n    This class implements the context manager protocol.\n\n    Args:\n        wxfilename (str): Extended filename to open for writing.\n        binary (bool): Whether to open the stream in binary mode.\n        write_header (bool): Whether to write Kaldi binary mode header in\n            binary mode.\n    """"""\n\n    def __init__(self, wxfilename=None, binary=True, write_header=True):\n        super(Output, self).__init__()\n        if wxfilename is not None:\n            self.open(wxfilename, binary, write_header)\n\n    def __del__(self):\n        if self.is_open():\n            self.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        if self.is_open():\n            self.close()\n\n    def flush(self):\n        """"""Flushes the stream.""""""\n        if not self.is_open():\n            raise ValueError(""I/O operation on closed stream."")\n        _base_io.flush(self.stream())\n\n    def open(self, wxfilename, binary, write_header):\n        """"""Opens the stream for writing.\n\n        Args:\n            wxfilename (str): Extended filename to open for writing.\n            binary (bool): Whether to open the stream in binary mode.\n            write_header (bool): Whether to write Kaldi binary mode header in\n                binary mode.\n        """"""\n        if not super(Output, self).open(wxfilename, binary, write_header):\n            raise IOError(""Could not open stream for writing."")\n        self._write = _base_io.write if binary else _base_io.write_text\n\n    def write(self, s):\n        """"""Writes s to the stream.\n\n        Returns the number of bytes/characters written.\n        """"""\n        if not self.is_open():\n            raise ValueError(""I/O operation on closed stream."")\n        if not self.stream().good():\n            raise IOError(""I/O error."")\n        self._write(self.stream(), s)\n        return len(s)\n\n    def writelines(self, lines):\n        """"""Writes a list of lines to the stream.\n\n        Line separators are not added, so it is usual for each of the lines\n        provided to have a line separator at the end.\n        """"""\n        for line in lines:\n            self.write(line)\n\n\ndef xopen(xfilename, mode=""r"", write_header=True):\n    """"""Opens an extended filename and returns the stream.\n\n    The **mode** defaults to ""r"" which means open for reading in binary mode.\n    The available modes are:\n\n    ========= ===============================================================\n    Character Meaning\n    --------- ---------------------------------------------------------------\n    \'r\'       open for reading (default)\n    \'w\'       open for writing\n    \'b\'       binary mode (default)\n    \'t\'       text mode\n    ========= ===============================================================\n\n    xopen() returns a stream object whose type depends on the mode, and\n    through which the standard I/O operations such as reading and writing\n    are performed. When xopen() is used to open a stream for reading (\'r\', \'rb\',\n    \'rt\'), it returns an `Input`. When used to open a stream for writing (\'w\',\n    \'wb\', \'wt\'), it return an `Output`.\n\n    Similar to how files are handled in Python, PyKaldi distinguishes between\n    streams opened in binary and text modes, even when the underlying operating\n    system doesn\'t. If the stream is opened in binary mode, its I/O methods\n    accept and return `bytes` objects. Otherwise, they accept and return\n    `unicode` strings.\n\n    Args:\n        xfilename (str): Extended filename to open.\n        mode (str): Optional string specifying the mode stream is opened.\n        write_header (str): Whether streams opened for writing in binary mode\n            write Kaldi binary mode header (`\\\\\\\\0` then `B`) to the beginning\n            of the stream. This header is checked by streams opened for reading\n            in binary mode to set `~Input.binary` attribute.\n    """"""\n    if not isinstance(xfilename, str):\n        raise TypeError(""invalid xfilename: %r"" % xfilename)\n    if not isinstance(mode, str):\n        raise TypeError(""invalid mode: %r"" % mode)\n    modes = set(mode)\n    if modes - set(""rwbt"") or len(mode) > len(modes):\n        raise ValueError(""invalid mode: %r"" % mode)\n    reading = ""r"" in modes\n    writing = ""w"" in modes\n    text = ""t"" in modes\n    binary = ""b"" in modes\n    if text and binary:\n        raise ValueError(""can\'t have text and binary mode at once"")\n    if reading + writing != 1:\n        raise ValueError(""must have exactly one of read/write mode"")\n    if not text:\n        binary = True\n    if reading:\n        return Input(xfilename, binary)\n    else:\n        return Output(xfilename, binary, write_header)\n\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
kaldi/util/options.py,0,"b'import argparse\nimport sys\n\nfrom . import _options_ext\n\nclass ParseOptions(_options_ext.ParseOptions):\n    """"""Command line option parser.\n\n    Args:\n        usage (str): Usage string.\n    """"""\n\n    def parse_args(self, args=None):\n        """"""Parses arguments.\n\n        This method is used for parsing command line options. It fills the\n        options objects registered with the parser. Parsed values for options\n        that are directly registered with the parser, i.e. not via an options\n        object, are returned as attributes of a `Namespace` object.\n\n        Args:\n            args (list): List of argument strings. If not provided, the argument\n                strings are taken from `sys.argv`.\n\n        Returns:\n            A new `Namespace` object populated with the parsed values for\n            options that are directly registered with the parser.\n        """"""\n        self._read(args if args else sys.argv)\n        opts = self._get_options()\n        arg_dict = {}\n        arg_dict.update(opts.bool_map)\n        arg_dict.update(opts.int_map)\n        arg_dict.update(opts.uint_map)\n        arg_dict.update(opts.float_map)\n        arg_dict.update(opts.double_map)\n        arg_dict.update(opts.str_map)\n        return argparse.Namespace(**arg_dict)\n\n################################################################################\n\n_exclude_list = [\'argparse\', \'sys\']\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')\n           and not name in _exclude_list]\n'"
kaldi/util/table.py,0,"b'""""""\nFor detailed documentation of Kaldi tables, table readers/writers, table\nread/write specifiers, see `Kaldi I/O mechanisms`_ and\n`Kaldi I/O from a command-line perspective`_.\n\n.. _Kaldi I/O mechanisms:\n   http://kaldi-asr.org/doc/io.html\n.. _Kaldi I/O from a command-line perspective:\n   http://kaldi-asr.org/doc/io_tut.html\n""""""\n\nfrom . import _kaldi_table\nfrom ._kaldi_table import (read_script_file, write_script_file,\n                           classify_wspecifier, classify_rspecifier,\n                           WspecifierType, RspecifierType,\n                           WspecifierOptions, RspecifierOptions)\nfrom . import _kaldi_table_ext\nimport kaldi.matrix as _matrix\n\n################################################################################\n# Sequential Readers\n################################################################################\n\nclass _SequentialReaderBase(object):\n    """"""Base class defining the Python API for sequential table readers.""""""\n    def __init__(self, rspecifier=""""):\n        """"""\n        This class is used for reading objects sequentially from an archive or\n        script file. It implements the iterator protocol similar to how Python\n        implements iteration over dictionaries. Each iteration returns a `(key,\n        value)` pair from the table in sequential order.\n\n        Args:\n            rspecifier(str): Kaldi rspecifier for reading the table.\n                If provided, the table is opened for reading.\n\n        Raises:\n            IOError: If opening the table for reading fails.\n        """"""\n        super(_SequentialReaderBase, self).__init__()\n        if rspecifier != """":\n            if not self.open(rspecifier):\n                raise IOError(""Error opening sequential table reader with ""\n                              ""rspecifier: {}"".format(rspecifier))\n\n    def __enter__(self):\n        return self\n\n    def __iter__(self):\n        while not self.done():\n            key = self.key()\n            value = self.value()\n            self.next()\n            yield key, value\n\n    def open(self, rspecifier):\n        """"""Opens the table for reading.\n\n        Args:\n            rspecifier(str): Kaldi rspecifier for reading the table.\n                If provided, the table is opened for reading.\n\n        Returns:\n            True if table is opened successfully, False otherwise.\n\n        Raises:\n            IOError: If opening the table for reading fails.\n        """"""\n        return super(_SequentialReaderBase, self).open(rspecifier)\n\n    def done(self):\n        """"""Indicates whether the table reader is exhausted or not.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          True if the table reader is exhausted, False otherwise.\n        """"""\n        return super(_SequentialReaderBase, self).done()\n\n    def key(self):\n        """"""Returns the current key.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n            str: The current key.\n        """"""\n        return super(_SequentialReaderBase, self).key()\n\n    def free_current(self):\n        """"""Deallocates the current value.\n\n        This method is provided as an optimization to save memory, for large\n        objects.\n        """"""\n        super(_SequentialReaderBase, self).free_current()\n\n    def value(self):\n        """"""Returns the current value.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n            The current value.\n        """"""\n        return super(_SequentialReaderBase, self).value()\n\n    def next(self):\n        """"""Advances the table reader.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n        """"""\n        super(_SequentialReaderBase, self).next()\n\n    def is_open(self):\n        """"""Indicates whether the table reader is open or not.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          True if the table reader is open, False otherwise.\n        """"""\n        return super(_SequentialReaderBase, self).is_open()\n\n    def close(self):\n        """"""Closes the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n            True if table is closed successfully, False otherwise.\n        """"""\n        return super(_SequentialReaderBase, self).close()\n\n\nclass SequentialVectorReader(_SequentialReaderBase,\n                             _kaldi_table.SequentialVectorReader):\n    """"""Sequential table reader for single precision vectors.""""""\n    pass\n\n\nclass SequentialDoubleVectorReader(_SequentialReaderBase,\n                                   _kaldi_table.SequentialDoubleVectorReader):\n    """"""Sequential table reader for double precision vectors.""""""\n    pass\n\n\nclass SequentialMatrixReader(_SequentialReaderBase,\n                             _kaldi_table.SequentialMatrixReader):\n    """"""Sequential table reader for single precision matrices.""""""\n    pass\n\n\nclass SequentialDoubleMatrixReader(_SequentialReaderBase,\n                                   _kaldi_table.SequentialDoubleMatrixReader):\n    """"""Sequential table reader for double precision matrices.""""""\n    pass\n\n\nclass SequentialWaveReader(_SequentialReaderBase,\n                           _kaldi_table.SequentialWaveReader):\n    """"""Sequential table reader for wave files.""""""\n    pass\n\n\nclass SequentialWaveInfoReader(_SequentialReaderBase,\n                               _kaldi_table.SequentialWaveInfoReader):\n    """"""Sequential table reader for wave file headers.""""""\n    pass\n\n\nclass SequentialPosteriorReader(_SequentialReaderBase,\n                                _kaldi_table.SequentialPosteriorReader):\n    """"""Sequential table reader for frame posteriors.""""""\n    pass\n\n\nclass SequentialGaussPostReader(_SequentialReaderBase,\n                                     _kaldi_table.SequentialGaussPostReader):\n    """"""Sequential table reader for Gaussian-level frame posteriors.""""""\n    pass\n\n\nclass SequentialFstReader(_SequentialReaderBase,\n                          _kaldi_table_ext.SequentialFstReader):\n    """"""Sequential table reader for FSTs over the tropical semiring.""""""\n    pass\n\n\nclass SequentialLogFstReader(_SequentialReaderBase,\n                             _kaldi_table_ext.SequentialLogFstReader):\n    """"""Sequential table reader for FSTs over the log semiring.""""""\n    pass\n\n\nclass SequentialKwsIndexFstReader(_SequentialReaderBase,\n                             _kaldi_table_ext.SequentialKwsIndexFstReader):\n    """"""Sequential table reader for FSTs over the KWS index semiring.""""""\n    pass\n\n\nclass SequentialLatticeReader(_SequentialReaderBase,\n                              _kaldi_table.SequentialLatticeReader):\n    """"""Sequential table reader for lattices.""""""\n    pass\n\n\nclass SequentialCompactLatticeReader(\n        _SequentialReaderBase,\n        _kaldi_table.SequentialCompactLatticeReader):\n    """"""Sequential table reader for compact lattices.""""""\n    pass\n\n\nclass SequentialNnetExampleReader(_SequentialReaderBase,\n                                  _kaldi_table.SequentialNnetExampleReader):\n    """"""Sequential table reader for nnet examples.""""""\n    pass\n\n\nclass SequentialNnetChainExampleReader(\n        _SequentialReaderBase,\n        _kaldi_table.SequentialNnetChainExampleReader):\n    """"""Sequential table reader for nnet chain examples.""""""\n    pass\n\n\nclass SequentialRnnlmExampleReader(_SequentialReaderBase,\n                                  _kaldi_table.SequentialRnnlmExampleReader):\n    """"""Sequential table reader for RNNLM examples.""""""\n    pass\n\n\nclass SequentialIntReader(_SequentialReaderBase,\n                          _kaldi_table.SequentialIntReader):\n    """"""Sequential table reader for integers.""""""\n    pass\n\n\nclass SequentialFloatReader(_SequentialReaderBase,\n                            _kaldi_table.SequentialFloatReader):\n    """"""Sequential table reader for single precision floats.""""""\n    pass\n\n\nclass SequentialDoubleReader(_SequentialReaderBase,\n                             _kaldi_table.SequentialDoubleReader):\n    """"""Sequential table reader for double precision floats.""""""\n    pass\n\n\nclass SequentialBoolReader(_SequentialReaderBase,\n                          _kaldi_table.SequentialBoolReader):\n    """"""Sequential table reader for Booleans.""""""\n    pass\n\n\nclass SequentialIntVectorReader(_SequentialReaderBase,\n                                _kaldi_table.SequentialIntVectorReader):\n    """"""Sequential table reader for integer sequences.""""""\n    pass\n\n\nclass SequentialIntVectorVectorReader(\n        _SequentialReaderBase,\n        _kaldi_table.SequentialIntVectorVectorReader):\n    """"""Sequential table reader for sequences of integer sequences.""""""\n    pass\n\n\nclass SequentialIntPairVectorReader(\n        _SequentialReaderBase,\n        _kaldi_table.SequentialIntPairVectorReader):\n    """"""Sequential table reader for sequences of integer pairs.""""""\n    pass\n\n\nclass SequentialFloatPairVectorReader(\n        _SequentialReaderBase,\n        _kaldi_table.SequentialFloatPairVectorReader):\n    """"""Sequential table reader for sequences of single precision float pairs.""""""\n    pass\n\n################################################################################\n# Random Access Readers\n################################################################################\n\nclass _RandomAccessReaderBase(object):\n    """"""Base class defining the Python API for random access table readers.""""""\n    def __init__(self, rspecifier=""""):\n        """"""\n        This class is used for randomly accessing objects in an archive or\n        script file. It implements `__contains__` and `__getitem__` methods to\n        provide a dictionary-like interface for accessing table entries. e.g.\n        `reader[key]` returns the `value` associated with the `key`.\n\n        Args:\n            rspecifier(str): Kaldi rspecifier for reading the table.\n                If provided, the table is opened for reading.\n\n        Raises:\n            IOError: If opening the table for reading fails.\n        """"""\n        super(_RandomAccessReaderBase, self).__init__()\n        if rspecifier != """":\n            if not self.open(rspecifier):\n                raise IOError(""Error opening random access table reader with ""\n                              ""rspecifier: {}"".format(rspecifier))\n\n    def __enter__(self):\n        return self\n\n    def __contains__(self, key):\n        return self.has_key(key)\n\n    def __getitem__(self, key):\n        if self.has_key(key):\n            return self.value(key)\n        else:\n            raise KeyError(key)\n\n    def open(self, rspecifier):\n        """"""Opens the table for reading.\n\n        Args:\n            rspecifier(str): Kaldi rspecifier for reading the table.\n                If provided, the table is opened for reading.\n\n        Returns:\n            True if table is opened successfully, False otherwise.\n\n        Raises:\n            IOError: If opening the table for reading fails.\n        """"""\n        return super(_RandomAccessReaderBase, self).open(rspecifier)\n\n    def has_key(self, key):\n        """"""Checks whether the table has the key.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Args:\n            key (str): The key.\n\n        Returns:\n          True if the table has the key, False otherwise.\n        """"""\n        return super(_RandomAccessReaderBase, self).has_key(key)\n\n    def value(self, key):\n        """"""Returns the value associated with the key.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Args:\n            key (str): The key.\n\n        Returns:\n            The value associated with the key.\n        """"""\n        return super(_RandomAccessReaderBase, self).value(key)\n\n    def is_open(self):\n        """"""Indicates whether the table reader is open or not.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          True if the table reader is open, False otherwise.\n        """"""\n        return super(_RandomAccessReaderBase, self).is_open()\n\n    def close(self):\n        """"""Closes the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n            True if table is closed successfully, False otherwise.\n        """"""\n        return super(_RandomAccessReaderBase, self).close()\n\n\nclass RandomAccessVectorReader(_RandomAccessReaderBase,\n                               _kaldi_table.RandomAccessVectorReader):\n    """"""Random access table reader for single precision vectors.""""""\n    pass\n\n\nclass RandomAccessDoubleVectorReader(\n        _RandomAccessReaderBase,\n        _kaldi_table.RandomAccessDoubleVectorReader):\n    """"""Random access table reader for double precision vectors.""""""\n    pass\n\n\nclass RandomAccessMatrixReader(_RandomAccessReaderBase,\n                               _kaldi_table.RandomAccessMatrixReader):\n    """"""Random access table reader for single precision matrices.""""""\n    pass\n\n\nclass RandomAccessDoubleMatrixReader(\n        _RandomAccessReaderBase,\n        _kaldi_table.RandomAccessDoubleMatrixReader):\n    """"""Random access table reader for double precision matrices.""""""\n    pass\n\n\nclass RandomAccessWaveReader(_RandomAccessReaderBase,\n                             _kaldi_table.RandomAccessWaveReader):\n    """"""Random access table reader for wave files.""""""\n    pass\n\n\nclass RandomAccessWaveInfoReader(_RandomAccessReaderBase,\n                                 _kaldi_table.RandomAccessWaveInfoReader):\n    """"""Random access table reader for wave file headers.""""""\n    pass\n\n\nclass RandomAccessPosteriorReader(_RandomAccessReaderBase,\n                                  _kaldi_table.RandomAccessPosteriorReader):\n    """"""Random access table reader for frame posteriors.""""""\n    pass\n\n\nclass RandomAccessGaussPostReader(_RandomAccessReaderBase,\n                                       _kaldi_table.RandomAccessGaussPostReader):\n    """"""Random access table reader for Gaussian-level frame posteriors.""""""\n    pass\n\n\nclass RandomAccessFstReader(_RandomAccessReaderBase,\n                            _kaldi_table_ext.RandomAccessFstReader):\n    """"""Random access table reader for FSTs over the tropical semiring.""""""\n    pass\n\n\nclass RandomAccessLogFstReader(_RandomAccessReaderBase,\n                               _kaldi_table_ext.RandomAccessLogFstReader):\n    """"""Random access table reader for FSTs over the log semiring.""""""\n    pass\n\n\nclass RandomAccessKwsIndexFstReader(_RandomAccessReaderBase,\n                               _kaldi_table_ext.RandomAccessKwsIndexFstReader):\n    """"""Random access table reader for FSTs over the KWS index semiring.""""""\n    pass\n\n\nclass RandomAccessLatticeReader(_RandomAccessReaderBase,\n                                _kaldi_table.RandomAccessLatticeReader):\n    """"""Random access table reader for lattices.""""""\n    pass\n\n\nclass RandomAccessCompactLatticeReader(\n        _RandomAccessReaderBase,\n        _kaldi_table.RandomAccessCompactLatticeReader):\n    """"""Random access table reader for compact lattices.""""""\n    pass\n\n\nclass RandomAccessNnetExampleReader(_RandomAccessReaderBase,\n                                    _kaldi_table.RandomAccessNnetExampleReader):\n    """"""Random access table reader for nnet examples.""""""\n    pass\n\n\nclass RandomAccessNnetChainExampleReader(\n        _RandomAccessReaderBase,\n        _kaldi_table.RandomAccessNnetChainExampleReader):\n    """"""Random access table reader for nnet chain examples.""""""\n    pass\n\n\nclass RandomAccessIntReader(_RandomAccessReaderBase,\n                            _kaldi_table.RandomAccessIntReader):\n    """"""Random access table reader for integers.""""""\n    pass\n\n\nclass RandomAccessFloatReader(_RandomAccessReaderBase,\n                              _kaldi_table.RandomAccessFloatReader):\n    """"""Random access table reader for single precision floats.""""""\n    pass\n\n\nclass RandomAccessDoubleReader(_RandomAccessReaderBase,\n                               _kaldi_table.RandomAccessDoubleReader):\n    """"""Random access table reader for double precision floats.""""""\n    pass\n\n\nclass RandomAccessBoolReader(_RandomAccessReaderBase,\n                             _kaldi_table.RandomAccessBoolReader):\n    """"""Random access table reader for Booleans.""""""\n    pass\n\n\nclass RandomAccessIntVectorReader(_RandomAccessReaderBase,\n                                  _kaldi_table.RandomAccessIntVectorReader):\n    """"""Random access table reader for integer sequences.""""""\n    pass\n\n\nclass RandomAccessIntVectorVectorReader(\n        _RandomAccessReaderBase,\n        _kaldi_table.RandomAccessIntVectorVectorReader):\n    """"""Random access table reader for sequences of integer sequences.""""""\n    pass\n\n\nclass RandomAccessIntPairVectorReader(\n        _RandomAccessReaderBase,\n        _kaldi_table.RandomAccessIntPairVectorReader):\n    """"""Random access table reader for sequences of integer pairs.""""""\n    pass\n\n\nclass RandomAccessFloatPairVectorReader(\n        _RandomAccessReaderBase,\n        _kaldi_table.RandomAccessFloatPairVectorReader):\n    """"""\n    Random access table reader for sequences of single precision float pairs.\n    """"""\n    pass\n\n################################################################################\n# Mapped Random Access Readers\n################################################################################\n\nclass _RandomAccessReaderMappedBase(object):\n    """"""\n    Base class defining the Python API for mapped random access table readers.\n    """"""\n    def __init__(self, table_rspecifier="""", map_rspecifier=""""):\n        """"""\n        This class is used for randomly accessing objects in an archive or\n        script file. It implements `__contains__` and `__getitem__` methods to\n        provide a dictionary-like interface for accessing table entries. If a\n        **map_rspecifier** is provided, the map is used for converting the keys\n        to the actual keys used to query the table, e.g. `reader[key]` returns\n        the `value` associated with the key `map[key]`. Otherwise, it works like\n        a random access table reader.\n\n        Args:\n            table_rspecifier(str): Kaldi rspecifier for reading the table.\n                If provided, the table is opened for reading.\n            map_rspecifier (str): Kaldi rspecifier for reading the map.\n                If provided, the map is opened for reading.\n\n        Raises:\n            IOError: If opening the table or map for reading fails.\n        """"""\n        super(_RandomAccessReaderMappedBase, self).__init__()\n        if table_rspecifier != """" and map_rspecifier != """":\n            if not self.open(table_rspecifier, map_rspecifier):\n                raise IOError(""Error opening mapped random access table reader ""\n                              ""with table_rspecifier: {}, map_rspecifier: {}""\n                              .format(table_rspecifier, map_rspecifier))\n\n    def __enter__(self):\n        return self\n\n    def __contains__(self, key):\n        return self.has_key(key)\n\n    def __getitem__(self, key):\n        if self.has_key(key):\n            return self.value(key)\n        else:\n            raise KeyError(key)\n\n    def open(self, table_rspecifier, map_rspecifier):\n        """"""Opens the table for reading.\n\n        Args:\n            table_rspecifier(str): Kaldi rspecifier for reading the table.\n                If provided, the table is opened for reading.\n            map_rspecifier (str): Kaldi rspecifier for reading the map.\n                If provided, the map is opened for reading.\n\n        Returns:\n            True if table is opened successfully, False otherwise.\n\n        Raises:\n            IOError: If opening the table or map for reading fails.\n        """"""\n        return super(_RandomAccessReaderMappedBase, self).open(table_rspecifier,\n                                                               map_rspecifier)\n\n    def has_key(self, key):\n        """"""Checks whether the table has the key.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Args:\n            key (str): The key.\n\n        Returns:\n          True if the table has the key, False otherwise.\n        """"""\n        return super(_RandomAccessReaderMappedBase, self).has_key(key)\n\n    def value(self, key):\n        """"""Returns the value associated with the key.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Args:\n            key (str): The key.\n\n        Returns:\n            The value associated with the key.\n        """"""\n        return super(_RandomAccessReaderMappedBase, self).value(key)\n\n    def is_open(self):\n        """"""Indicates whether the table reader is open or not.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          True if the table reader is open, False otherwise.\n        """"""\n        return super(_RandomAccessReaderMappedBase, self).is_open()\n\n    def close(self):\n        """"""Closes the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n            True if table is closed successfully, False otherwise.\n        """"""\n        return super(_RandomAccessReaderMappedBase, self).close()\n\n\nclass RandomAccessVectorReaderMapped(\n        _RandomAccessReaderMappedBase,\n        _kaldi_table.RandomAccessVectorReaderMapped):\n    """"""Mapped random access table reader for single precision vectors.""""""\n    pass\n\n\nclass RandomAccessDoubleVectorReaderMapped(\n        _RandomAccessReaderMappedBase,\n        _kaldi_table.RandomAccessDoubleVectorReaderMapped):\n    """"""Mapped random access table reader for double precision vectors.""""""\n    pass\n\n\nclass RandomAccessMatrixReaderMapped(\n        _RandomAccessReaderMappedBase,\n        _kaldi_table.RandomAccessMatrixReaderMapped):\n    """"""Mapped random access table reader for single precision matrices.""""""\n    pass\n\n\nclass RandomAccessDoubleMatrixReaderMapped(\n        _RandomAccessReaderMappedBase,\n        _kaldi_table.RandomAccessDoubleMatrixReaderMapped):\n    """"""Mapped random access table reader for double precision matrices.""""""\n    pass\n\n\nclass RandomAccessFloatReaderMapped(\n        _RandomAccessReaderMappedBase,\n        _kaldi_table.RandomAccessFloatReaderMapped):\n    """"""Mapped random access table reader for single precision floats.""""""\n    pass\n\n################################################################################\n# Writers\n################################################################################\n\nclass _WriterBase(object):\n    """"""Base class defining the additional Python API for table writers.""""""\n    def __init__(self, wspecifier=""""):\n        """"""\n\n        This class is used for writing objects to an archive or script file. It\n        implements the `__setitem__` method to provide a dictionary-like\n        interface for writing table entries, e.g. `writer[key] = value` writes\n        the pair `(key, value)` to the table.\n\n        Args:\n            wspecifier (str): Kaldi wspecifier for writing the table.\n                If provided, the table is opened for writing.\n\n        Raises:\n            IOError: If opening the table for writing fails.\n        """"""\n        super(_WriterBase, self).__init__()\n        if wspecifier != """":\n            if not self.open(wspecifier):\n                raise IOError(""Error opening table writer with wspecifier: {}""\n                              .format(wspecifier))\n\n    def __enter__(self):\n        return self\n\n    def __setitem__(self, key, value):\n        self.write(key, value)\n\n    def open(self, wspecifier):\n        """"""Opens the table for writing.\n\n        Args:\n            wspecifier(str): Kaldi wspecifier for writing the table.\n                If provided, the table is opened for writing.\n\n        Returns:\n            True if table is opened successfully, False otherwise.\n\n        Raises:\n            IOError: If opening the table for writing fails.\n        """"""\n        return super(_WriterBase, self).open(wspecifier)\n\n    def flush(self):\n        """"""Flushes the table contents to disk/pipe.""""""\n        super(_WriterBase, self).flush()\n\n    def write(self, key, value):\n        """"""Writes the `(key, value)` pair to the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Args:\n            key (str): The key.\n            value: The value.\n        """"""\n        super(_WriterBase, self).write(key, value)\n\n    def is_open(self):\n        """"""Indicates whether the table writer is open or not.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n          True if the table writer is open, False otherwise.\n        """"""\n        return super(_WriterBase, self).is_open()\n\n    def close(self):\n        """"""Closes the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Returns:\n            True if table is closed successfully, False otherwise.\n        """"""\n        return super(_WriterBase, self).close()\n\n\nclass VectorWriter(_WriterBase, _kaldi_table.VectorWriter):\n    """"""Table writer for single precision vectors.""""""\n    def write(self, key, value):\n        """"""Writes the `(key, value)` pair to the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Overrides write to accept both Vector and SubVector.\n\n        Args:\n            key (str): The key.\n            value: The value.\n        """"""\n        super(VectorWriter, self).write(key, _matrix.Vector(value))\n\n\nclass DoubleVectorWriter(_WriterBase, _kaldi_table.DoubleVectorWriter):\n    """"""Table writer for double precision vectors.""""""\n    def write(self, key, value):\n        """"""Writes the `(key, value)` pair to the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Overrides write to accept both DoubleVector and DoubleSubVector.\n\n        Args:\n            key (str): The key.\n            value: The value.\n        """"""\n        super(DoubleVectorWriter, self).write(key, _matrix.DoubleVector(value))\n\n\nclass MatrixWriter(_WriterBase, _kaldi_table.MatrixWriter):\n    """"""Table writer for single precision matrices.""""""\n    def write(self, key, value):\n        """"""Writes the `(key, value)` pair to the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Overrides write to accept both Matrix and SubMatrix.\n\n        Args:\n            key (str): The key.\n            value: The value.\n        """"""\n        super(MatrixWriter, self).write(key, _matrix.Matrix(value))\n\n\nclass DoubleMatrixWriter(_WriterBase, _kaldi_table.DoubleMatrixWriter):\n    """"""Table writer for double precision matrices.""""""\n    def write(self, key, value):\n        """"""Writes the `(key, value)` pair to the table.\n\n        This method is provided for compatibility with the C++ API only;\n        most users should use the Pythonic API.\n\n        Overrides write to accept both DoubleMatrix and DoubleSubMatrix.\n\n        Args:\n            key (str): The key.\n            value: The value.\n        """"""\n        super(DoubleMatrixWriter, self).write(key, _matrix.DoubleMatrix(value))\n\n\nclass WaveWriter(_WriterBase, _kaldi_table.WaveWriter):\n    """"""Table writer for wave files.""""""\n    pass\n\n\nclass PosteriorWriter(_WriterBase, _kaldi_table.PosteriorWriter):\n    """"""Table writer for frame posteriors.""""""\n    pass\n\n\nclass GaussPostWriter(_WriterBase, _kaldi_table.GaussPostWriter):\n    """"""Table writer for Gaussian-level frame posteriors.""""""\n    pass\n\n\nclass FstWriter(_WriterBase, _kaldi_table_ext.FstWriter):\n    """"""Table writer for FSTs over the tropical semiring.""""""\n    pass\n\n\nclass LogFstWriter(_WriterBase, _kaldi_table_ext.LogFstWriter):\n    """"""Table writer for FSTs over the log semiring.""""""\n    pass\n\n\nclass KwsIndexFstWriter(_WriterBase, _kaldi_table_ext.KwsIndexFstWriter):\n    """"""Table writer for FSTs over the KWS index semiring.""""""\n    pass\n\n\nclass LatticeWriter(_WriterBase, _kaldi_table.LatticeWriter):\n    """"""Table writer for lattices.""""""\n    pass\n\n\nclass CompactLatticeWriter(_WriterBase, _kaldi_table.CompactLatticeWriter):\n    """"""Table writer for compact lattices.""""""\n    pass\n\n\nclass NnetExampleWriter(_WriterBase, _kaldi_table.NnetExampleWriter):\n    """"""Table writer for nnet examples.""""""\n    pass\n\n\nclass NnetChainExampleWriter(_WriterBase, _kaldi_table.NnetChainExampleWriter):\n    """"""Table writer for nnet chain examples.""""""\n    pass\n\n\nclass RnnlmExampleWriter(_WriterBase, _kaldi_table.RnnlmExampleWriter):\n    """"""Table writer for RNNLM examples.""""""\n    pass\n\n\nclass IntWriter(_WriterBase, _kaldi_table.IntWriter):\n    """"""Table writer for integers.""""""\n    pass\n\n\nclass FloatWriter(_WriterBase, _kaldi_table.FloatWriter):\n    """"""Table writer for single precision floats.""""""\n    pass\n\n\nclass DoubleWriter(_WriterBase, _kaldi_table.DoubleWriter):\n    """"""Table writer for double precision floats.""""""\n    pass\n\n\nclass BoolWriter(_WriterBase, _kaldi_table.BoolWriter):\n    """"""Table writer for Booleans.""""""\n    pass\n\n\nclass IntVectorWriter(_WriterBase, _kaldi_table.IntVectorWriter):\n    """"""Table writer for integer sequences.""""""\n    pass\n\n\nclass IntVectorVectorWriter(_WriterBase, _kaldi_table.IntVectorVectorWriter):\n    """"""Table writer for sequences of integer sequences.""""""\n    pass\n\n\nclass IntPairVectorWriter(_WriterBase, _kaldi_table.IntPairVectorWriter):\n    """"""Table writer for sequences of integer pairs.""""""\n    pass\n\n\nclass FloatPairVectorWriter(_WriterBase, _kaldi_table.FloatPairVectorWriter):\n    """"""Table writer for sequences of single precision float pairs.""""""\n    pass\n\n################################################################################\n\n__all__ = [name for name in dir()\n           if name[0] != \'_\'\n           and not name.endswith(\'Base\')]\n'"
tests/base/__init__.py,0,b''
tests/base/kaldi-math-test.py,0,"b'from kaldi.base import math as kaldi_math\nimport math\nimport unittest\n\n\nclass testKaldiMath(unittest.TestCase):\n\n    def testCrashes(self):\n        """""" Try to crash the interpreter """"""\n        # Negative log?\n        kaldi_math.log(-1.0)\n\n        # Really small log\n        kaldi_math.log(kaldi_math.DBL_EPSILON)\n\n        # Negative rands\n        with self.assertRaises(ValueError):\n            kaldi_math.rand_int(-1, -50)\n            kaldi_math.rand_int(10, -50)\n\n        kaldi_math.rand_int(-100, 50)\n\n\n    def testGcdLcmTpl(self):\n        for a in range(1, 15):\n            b = int(kaldi_math.rand() % 10)\n            c = int(kaldi_math.rand() % 10)\n\n            if kaldi_math.rand() % 2 == 0:\n                b = -b\n            if kaldi_math.rand() % 2 == 0:\n                c = -c\n\n            if b == 0 and c == 0:\n                continue\n\n            g = kaldi_math.gcd(b * a, c * a)\n\n            self.assertTrue(g >= a)\n            self.assertEqual(0, (b * a) % g)\n            self.assertEqual(0, (c * a) % g)\n\n            if b <= 0 or c <= 0:\n                with self.assertRaises(ValueError):\n                    kaldi_math.lcm(b * a, c * a)\n            else:\n                h = kaldi_math.lcm(b * a, c * a)\n                self.assertNotEqual(0, h)\n                self.assertEqual(0, (h % (b * a)))\n                self.assertEqual(0, (h % (c * a)))\n\n    def testRoundUpToNearestPowerOfTwo(self):\n        self.assertEqual(1, kaldi_math.round_up_to_nearest_power_of_two(1))\n        self.assertEqual(2, kaldi_math.round_up_to_nearest_power_of_two(2))\n        self.assertEqual(4, kaldi_math.round_up_to_nearest_power_of_two(3))\n        self.assertEqual(4, kaldi_math.round_up_to_nearest_power_of_two(4))\n        self.assertEqual(8, kaldi_math.round_up_to_nearest_power_of_two(7))\n        self.assertEqual(8, kaldi_math.round_up_to_nearest_power_of_two(8))\n        self.assertEqual(256, kaldi_math.round_up_to_nearest_power_of_two(255))\n        self.assertEqual(256, kaldi_math.round_up_to_nearest_power_of_two(256))\n        self.assertEqual(512, kaldi_math.round_up_to_nearest_power_of_two(257))\n        self.assertEqual(1073741824, kaldi_math.round_up_to_nearest_power_of_two(1073700000))\n\n    def testDivideRoundingDown(self):\n        for i in range(100):\n            a = kaldi_math.rand_int(-100, 100)\n            b = 0\n            while b == 0:\n                b = kaldi_math.rand_int(-100, 100)\n            self.assertEqual(int(math.floor(float(a) / float(b))), kaldi_math.divide_rounding_down(a, b))\n\n    @unittest.skip(""TODO"")\n    def testRand(self):\n        pass\n\n    def testLogAddSub(self):\n        for i in range(100):\n            f1 = kaldi_math.rand() % 10000\n            f2 = kaldi_math.rand() % 20\n            add1 = kaldi_math.exp(kaldi_math.log_add(kaldi_math.log(f1), kaldi_math.log(f2)))\n            add2 = kaldi_math.exp(kaldi_math.log_add(kaldi_math.log(f2), kaldi_math.log(f1)))\n            add = f1 + f2\n            thresh = add*0.00001\n\n            self.assertAlmostEqual(add, add1, delta = thresh)\n            self.assertAlmostEqual(add, add2, delta = thresh)\n\n            try:\n                f2_check = kaldi_math.exp(kaldi_math.log_sub(kaldi_math.log(add), kaldi_math.log(f1)))\n                self.assertAlmostEqual(f2, f2_check, delta = thresh)\n            except:\n                # self.assertEqual(0, f2)\n                pass\n\n    def testDefines(self):\n        self.assertAlmostEqual(0.0, kaldi_math.exp(kaldi_math.LOG_ZERO_FLOAT))\n        self.assertAlmostEqual(0.0, kaldi_math.exp(kaldi_math.LOG_ZERO_DOUBLE))\n\n        # TODO:\n        # How to test these in Python?\n\n        # den = 0.0\n        # self.assertTrue(kaldi_math.KALDI_ISNAN(0.0 / den))\n        # self.assertFalse(kaldi_math.KALDI_ISINF(0.0 / den))\n        # self.assertFalse(kaldi_math.KALDI_ISFINITE(0.0 / den))\n        # self.assertFalse(kaldi_math.KALDI_ISNAN(1.0 / den))\n        # self.assertTrue(kaldi_math.KALDI_ISINF(1.0 / den))\n        # self.assertFalse(kaldi_math.KALDI_ISFINITE(1.0 / den))\n        # self.assertTrue(kaldi_math.KALDI_ISFINITE(0.0))\n        # self.assertFalse(kaldi_math.KALDI_ISINF(0.0))\n        # self.assertFalse(kaldi_math.KALDI_ISNAN(0.0))\n\n\n        self.assertTrue(1.0 != 1.0 + kaldi_math.DBL_EPSILON)\n        self.assertTrue(1.0 == 1.0 + 0.5 * kaldi_math.DBL_EPSILON)\n        # self.assertNotAlmostEqual(1.0, 1.0 + kaldi_math.FLT_EPSILON, places = 7)\n        # self.assertAlmostEqual(1.0, 1.0 + 0.5 * kaldi_math.FLT_EPSILON, places = 6)\n\n        self.assertAlmostEqual(0.0, math.fabs(math.sin(kaldi_math.M_PI)))\n        self.assertAlmostEqual(1.0, math.fabs(math.cos(kaldi_math.M_PI)))\n        self.assertAlmostEqual(0.0, math.fabs(math.sin(kaldi_math.M_2PI)))\n        self.assertAlmostEqual(1.0, math.fabs(math.cos(kaldi_math.M_2PI)))\n\n        self.assertAlmostEqual(0.0, math.fabs(math.sin(kaldi_math.exp(kaldi_math.M_LOG_2PI))), places = 5)\n        self.assertAlmostEqual(1.0, math.fabs(math.cos(kaldi_math.exp(kaldi_math.M_LOG_2PI))), places = 5)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/chain/__init__.py,0,b''
tests/chain/languge-model-test.py,0,"b'from __future__ import division\nimport unittest\n\nfrom kaldi.base import math as kaldi_math\nfrom kaldi.fstext import properties, StdVectorFst, compose, shortestdistance\nfrom kaldi.fstext.utils import is_stochastic_fst_in_log, make_linear_acceptor\nfrom kaldi.chain import LanguageModelEstimator, LanguageModelOptions\n\ndef getTestingData():\n    data = []\n    with open(__file__) as inpt: # what a nice quine!\n        for line in inpt:\n            int_line = []\n            for char in line:\n                int_line.append(min(127, ord(char)))\n            data.append(int_line)\n\n    return 127, data\n\ndef showPerplexity(fst, data):\n    num_phones = 0\n    tot_loglike = 0.0\n    for i in data:\n        num_phones += len(i)\n        linear_fst = StdVectorFst()\n        make_linear_acceptor(i, linear_fst)\n        composed_fst = compose(linear_fst, fst)\n        weight = shortestdistance(composed_fst)[-1]\n        tot_loglike -= weight.value\n\n    perplexity = kaldi_math.exp(-(tot_loglike / num_phones))\n    print(""Perplexity over {} phones (of training data) is {}"".format(num_phones, perplexity))\n\nclass TestLanguageModel(unittest.TestCase):\n\n    def testLanguageModelTest(self):\n        vocab_size, data = getTestingData()\n\n        opts = LanguageModelOptions()\n        opts.no_prune_ngram_order = kaldi_math.rand_int(1, 3)\n        opts.ngram_order = opts.no_prune_ngram_order + kaldi_math.rand_int(0, 3)\n        opts.num_extra_lm_states = kaldi_math.rand_int(1, 200)\n\n        if opts.ngram_order < 2:\n            opts.ngram_order = 2\n\n        if kaldi_math.rand_int(1, 2) == 1:\n            opts.num_extra_lm_states *= 10\n\n        estimator = LanguageModelEstimator(opts)\n        for sentence in data:\n            estimator.add_counts(sentence)\n\n        fst = estimator.estimate()\n\n        self.assertTrue(is_stochastic_fst_in_log(fst))\n        self.assertEqual(properties.ACCEPTOR, fst.properties(properties.ACCEPTOR, True))\n        self.assertEqual(properties.I_DETERMINISTIC, fst.properties(properties.I_DETERMINISTIC, True))\n        self.assertEqual(0, fst.properties(properties.I_EPSILONS, True))\n\n        showPerplexity(fst, data)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/cudamatrix/__init__.py,0,b''
tests/cudamatrix/cu-device-test.py,0,"b'import sys\nimport unittest\n\nfrom kaldi.base import math as kaldi_math\nfrom kaldi.base import Timer\nfrom kaldi.cudamatrix import cuda_available, CuMatrix\nfrom kaldi.matrix import *\nfrom kaldi.matrix.common import *\n\n\nclass TestCuDevice(unittest.TestCase):\n\n    def testCudaMatrixResize(self):\n        size_multiples = [1, 2, 4, 8, 16, 32]\n        num_matrices = 256\n        time_in_secs = 0.2\n\n        for size_multiple in size_multiples:\n            sizes = []\n\n            for i in range(num_matrices):\n                num_rows = kaldi_math.rand_int(1, 10)\n                num_rows *= num_rows * size_multiple\n                num_cols = kaldi_math.rand_int(1, 10)\n                num_cols *= num_cols * size_multiple\n                sizes.append((num_rows, num_cols))\n\n            matrices = [CuMatrix() for _ in range(num_matrices)]\n\n            tim = Timer()\n            num_floats_processed = 0\n            while tim.elapsed() < time_in_secs:\n                matrix = kaldi_math.rand_int(0, num_matrices - 1)\n                if matrices[matrix].num_rows() == 0:\n                    num_rows, num_cols = sizes[matrix]\n                    matrices[matrix].resize(num_rows, num_cols,\n                                            MatrixResizeType.UNDEFINED)\n                    num_floats_processed += num_rows * num_cols\n                else:\n                    matrices[matrix].resize(0, 0)\n\n            gflops = num_floats_processed / (tim.elapsed() * 1.0e9)\n            print(""CuMatrix.resize: size multiple {}, speed was {} gigaflops""\n                  .format(size_multiple, gflops))\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/cudamatrix/cu-matrix-test.py,2,"b'from kaldi.base import math as kaldi_math\nfrom kaldi.matrix import Vector, Matrix\n\nfrom kaldi.cudamatrix import (CuMatrix, CuVector,\n                              approx_equal_cu_matrix, same_dim_cu_matrix)\n\nimport unittest\nimport numpy as np\n\nclass TestCuMatrix(unittest.TestCase):\n\n    def testNew(self):\n        A = CuMatrix()\n        self.assertIsNotNone(A)\n        self.assertEqual(0, A.num_rows())\n        self.assertEqual(0, A.num_cols())\n\n        dim = A.dim()\n        self.assertEqual(0, dim.rows)\n        self.assertEqual(0, dim.cols)\n\n        A = CuMatrix.from_size(10, 10)\n        self.assertIsNotNone(A)\n        self.assertEqual(10, A.num_rows())\n        self.assertEqual(10, A.num_cols())\n\n        dim = A.dim()\n        self.assertEqual(10, dim.rows)\n        self.assertEqual(10, dim.cols)\n\n        A = CuMatrix.from_matrix(Matrix([[2, 3], [5, 7]]))\n        self.assertIsNotNone(A)\n        self.assertEqual(2, A.num_rows())\n        self.assertEqual(2, A.num_cols())\n\n        B = CuMatrix.from_other(A)\n        self.assertIsNotNone(B)\n        self.assertEqual(2, B.num_rows())\n        self.assertEqual(2, B.num_cols())\n\n    def testResize(self):\n        A = CuMatrix()\n        A.resize(10, 10)\n        self.assertEqual(10, A.num_rows())\n        self.assertEqual(10, A.num_cols())\n\n        # A.resize(-1, -1) #This hard-crashes\n        A.resize(0, 0)\n\n        # TODO:\n        # A = CuMatrix.from_matrix(Matrix.new([[1, 2], [3, 4], [5, 6]])) #A is 3x2\n        # with self.assertRaises(Exception):\n        #     A.resize(2, 2) #Try to resize to something invalid\n\n    # FIXME:\n    # Hard crashing...\n    @unittest.skip(""hard-crashes"")\n    def testSwap(self):\n        for i in range(10):\n            dim = (10 * i, 4 * i)\n            M = Matrix(np.random.random(dim))\n            A = CuMatrix.from_matrix(M)\n            B = CuMatrix.from_size(A.num_rows(), A.num_cols())\n            B.Swap(A)\n            self.assertAlmostEqual(A.sum(), B.sum(), places = 4) #Kaldi\'s precision is aweful\n            self.assertAlmostEqual(M.sum(), B.sum(), places = 4) #Kaldi\'s precision is aweful\n\n            C = CuMatrix.from_size(M.shape[0], M.shape[1])\n            C.SwapWithMatrix(M)\n            self.assertAlmostEqual(B.sum(), C.sum(), places = 4) #Kaldi\'s precision is aweful\n\n    def testcopy_from_mat(self):\n        for i in range(1, 10):\n            rows, cols = 10*i, 5*i\n            A = Matrix(rows, cols)\n            A.set_randn_()\n            B = CuMatrix.from_size(*A.shape)\n            B.copy_from_mat(A)\n            self.assertAlmostEqual(A.sum(), B.sum(), places = 4)\n\n            A = CuMatrix.from_size(rows, cols)\n            A.set_randn()\n            B = CuMatrix.from_size(rows, cols)\n            B.copy_from_cu_mat(A)\n            self.assertAlmostEqual(A.sum(), B.sum(), places = 4)\n\n    @unittest.skip(""hard-crashes"")\n    def test__getitem(self):\n        A = CuMatrix.from_matrix(Matrix.new(np.arange(10).reshape((5, 2))))\n        self.assertEqual(0.0, A.__getitem(0, 0))\n        self.assertEqual(1.0, A.__getitem(0, 1))\n        self.assertEqual(2.0, A.__getitem(1, 0))\n        self.assertEqual(3.0, A.__getitem(1, 1))\n        self.assertEqual(4.0, A.__getitem(2, 0))\n\n        # This should hard crash\n        with self.assertRaises(IndexError):\n            self.assertEqual(0.0, A.__getitem(0, 2))\n\n    def testSameDim(self):\n        A = CuMatrix()\n        B = CuMatrix()\n        self.assertTrue(same_dim_cu_matrix(A, B))\n\n        A = CuMatrix.from_size(10, 10)\n        B = CuMatrix.from_size(10, 9)\n        self.assertFalse(same_dim_cu_matrix(A, B))\n\n    @unittest.skip(""FIXME"")\n    def testApproxEqual(self):\n        A = CuMatrix()\n        B = CuMatrix()\n        self.assertTrue(approx_equal_cu_matrix(A, B))\n\n        A.SetZero()\n        B.SetZero()\n        self.assertTrue(approx_equal_cu_matrix(A, B))\n\n        B.set_randn()\n        B.Scale(10.0)\n        self.assertFalse(approx_equal_cu_matrix(A, B))\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/cudamatrix/cu-vector-test.py,0,"b'import numpy as np\nimport sys\nimport unittest\n\nfrom kaldi.base import math as kaldi_math\nfrom kaldi.cudamatrix import CuVector, CuSubVector\nfrom kaldi.matrix import Vector\nfrom kaldi.matrix.packed import TpMatrix\n\n\nclass TestCuVector(unittest.TestCase):\n    def testCuVectorNewFromSize(self):\n        vec = CuVector()\n        self.assertIsNotNone(vec)\n        self.assertEqual(0, vec.dim())\n\n        for i in range(10):\n            dim = 10 * i\n            vec = CuVector.from_size(dim)\n            self.assertIsNotNone(vec)\n            self.assertEqual(dim, vec.dim())\n\n    def testCuVectorResize(self):\n        for i in range(10):\n            dim = 10 * i\n            vec = CuVector()\n            vec.resize(dim)\n            self.assertEqual(dim, vec.dim())\n\n    @unittest.skip(""TODO"")\n    def testCuVectorRead(self):\n        pass\n\n    @unittest.skip(""TODO"")\n    def testCuVectorWrite(self):\n        pass\n\n    def testCuVectorSwap(self):\n        N = [2, 3, 5, 7, 13]\n        A = Vector(N).clone()\n        C = CuVector.from_size(5)\n        C.swap(A) #Swap *is* destructive\n\n        self.assertEqual(16.0, C.norm(2))\n\n        A = Vector()\n        C = CuVector.from_size(0)\n        C.swap(A)\n        self.assertEqual(0.0, C.norm(2))\n\n    def testCuVectorCopyFromVec(self):\n\n        # Shouldnt crash\n        A = Vector()\n        C = CuVector.from_size(0)\n        C.copy_from_vec(A)\n\n        # What if dims not match?\n        # HARD-CRASH\n        # FIXME\n        # A = Vector.random(10)\n        # C = CuVector.from_size(0)\n        # C.CopyFromVec(A)\n\n        for i in range(10):\n            dim = 10 * i\n            A = Vector(dim)\n            A.set_randn_()\n            D = CuVector.from_size(dim)\n            D.copy_from_vec(A)\n            self.assertEqual(A.sum(), D.sum())\n\n    @unittest.skip(""Not sequential object"")\n    def testCuSubVector(self):\n        for iteration in range(10):\n            M1 = 1 + kaldi_math.rand() % 10\n            M2 = 1 + kaldi_math.rand() % 1\n            M3 = 1 + kaldi_math.rand() % 10\n            M = M1 + M2 + M3\n\n            m = kaldi_math.rand() % M2\n\n            vec = CuVector.from_size(M)\n            vec.set_randn()\n\n            subvec1 = CuSubVector(vec, M1, M2)\n            # subvec2 = vec.range(M1, M2)\n\n            f1, f2, f3 = vec[M1 + m], subvec1[m], subvec2[m]\n            self.assertEqual(f1, f2)\n            self.assertEqual(f3, f2)\n\n    def testCuVectorInvertElements(self):\n        # Test that this doesnt crash. This crashes when CUDA is enabled.\n        # C = CuVector.from_size(0)\n        # C.invert_elements()\n\n        C = CuVector.from_size(10)\n        C.set_randn()\n        C.invert_elements()\n\n        # Geometric series r = 1/2, a = 1/2\n        A = Vector([2, 4, 8, 16, 32, 64])\n        C = CuVector.from_size(len(A))\n        C.swap(A)\n        C.invert_elements()\n\n        f1 = C.sum()\n        self.assertAlmostEqual(0.5 * (1 - 0.5**len(A))/(1 - 0.5), f1)\n\n    @unittest.skip(""Not a sequential object"")\n    def testCuVectorGetItem(self):\n        v = CuVector()\n        with self.assertRaises(IndexError):\n            v[0]\n\n        v = CuVector.new([3, 5, 7, 11, 13])\n        self.assertAlmostEqual(3.0, v[0])\n        self.assertAlmostEqual(7.0, v[2])\n        self.assertAlmostEqual(13.0, v[4])\n\n        with self.assertRaises(IndexError):\n            v[5]\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/gmm/__init__.py,0,b''
tests/gmm/full-gmm-test.py,6,"b'""""""\nPython implementation of full-gmm-test.cc\n""""""\nimport unittest\nimport numpy as np\n\nfrom kaldi.base import math as kaldi_math\nfrom kaldi.matrix import Matrix, Vector\nfrom kaldi.matrix.common import MatrixTransposeType\nfrom kaldi.matrix.functions import vec_mat_vec\nfrom kaldi.matrix.packed import SpMatrix, TpMatrix\nfrom kaldi.gmm import *\nfrom kaldi.gmm._model_test_common import *\n\ndef RandPosdefSpMatrix(dim):\n    """"""\n    Generate random (non-singular) matrix\n    Arguments:\n        dim - int\n        matrix_sqrt - TpMatrix\n        logdet - float\n    Outputs:\n        matrix - SpMatrix\n    """"""\n    while True:\n        tmp = Matrix(dim, dim)\n        tmp.set_randn_()\n        if tmp.cond() < 100: break\n        print(""Condition number of random matrix large {}, trying again (this is normal)"".format(tmp.cond()))\n\n    # tmp * tmp^T will give positive definite matrix\n    matrix = SpMatrix(dim)\n    matrix.add_mat2_(1.0, tmp, MatrixTransposeType.NO_TRANS, 0.0)\n\n    matrix_sqrt = TpMatrix(len(matrix))\n    matrix_sqrt = matrix_sqrt.cholesky_(matrix)\n    logdet_out = matrix.log_pos_def_det()\n\n    return matrix, matrix_sqrt, logdet_out\n\ndef init_rand_diag_gmm(gmm):\n    num_comp, dim = gmm.num_gauss(), gmm.dim()\n    weights = Vector([kaldi_math.rand_uniform() for _ in range(num_comp)])\n    tot_weight = weights.sum()\n\n    for i, m in enumerate(weights):\n        weights[i] = m / tot_weight\n\n    means = Matrix([[kaldi_math.rand_gauss() for _ in range(dim)] for _ in range(num_comp)])\n    vars_ = Matrix([[kaldi_math.exp(kaldi_math.rand_gauss()) for _ in range(dim)] for _ in range(num_comp)])\n    vars_.invert_elements_()\n    gmm.set_weights(weights)\n    gmm.set_inv_vars_and_means(vars_, means)\n    gmm.perturb(0.5 * kaldi_math.rand_uniform())\n    gmm.compute_gconsts()\n\nclass TestFullGmm(unittest.TestCase):\n    def setUp(self):\n        np.random.seed(12345)\n\n    def testFullGmmEst(self):\n        fgmm = FullGmm()\n        dim = 10 + np.random.randint(low = 0, high = 10)\n        num_comp = 1 + np.random.randint(low = 0, high = 10)\n        num_frames = 5000\n        feats = Matrix(num_frames, dim)\n\n        init_rand_full(dim, num_comp, fgmm)\n        fgmm_normal = FullGmmNormal.new_with_other(fgmm)\n        fgmm_normal.rand(feats)\n\n        acc = AccumFullGmm.new_with_full(fgmm, GmmUpdateFlags.ALL)\n        for t in range(num_frames):\n            acc.accumulate_from_full(fgmm, feats[t,:], 1.0)\n\n        opts = MleFullGmmOptions()\n\n        objf_change, count = mle_full_gmm_update(opts, acc, GmmUpdateFlags.ALL, fgmm)\n        change = objf_change / count\n        num_params = num_comp * (dim + 1 + (dim * (dim + 1)/2))\n        predicted_change = 0.5 * num_params / num_frames\n\n        print(""Objf change per frame was {} vs. predicted {}"".format(change, predicted_change))\n        self.assertTrue(change < 2.0 * predicted_change)\n        self.assertTrue(change > 0.0)\n\n    def testFullGmm(self):\n        dim = 1 + np.random.randint(low = 0, high = 9)\n        nMix = 1 + np.random.randint(low = 0, high = 9)\n\n        print(""Testing NumGauss: {}, Dim: {}"".format(nMix, dim))\n\n        feat = Vector([kaldi_math.rand_gauss() for _ in range(dim)])\n        weights = Vector([kaldi_math.rand_uniform() for _ in range(nMix)])\n        tot_weigth = weights.sum()\n\n        for i, m in enumerate(weights):\n            weights[i] = m / tot_weigth\n\n        means = Matrix([[kaldi_math.rand_gauss() for _ in range(dim)] for _ in range(nMix)])\n\n        invcovars = [SpMatrix(dim) for _ in range(nMix)]\n        covars_logdet = []\n        for _ in range(nMix):\n            c, matrix_sqrt, logdet_out = RandPosdefSpMatrix(dim)\n            invcovars[_].copy_from_sp_(c)\n            invcovars[_].invert_double_()\n            covars_logdet.append(logdet_out)\n\n        # Calculate loglike for feature Vector\n        def auxLogLike(w, logdet, mean_row, invcovar):\n            return -0.5 * ( kaldi_math.M_LOG_2PI * dim \\\n                          + logdet \\\n                          + vec_mat_vec(mean_row, invcovar, mean_row) \\\n                          + vec_mat_vec(feat, invcovar, feat)) \\\n                    + vec_mat_vec(mean_row, invcovar, feat) \\\n                    + np.log(w)\n\n        loglikes = [auxLogLike(weights[m], covars_logdet[m], means[m, :], invcovars[m]) for m in range(nMix)]\n        loglike = Vector(loglikes).log_sum_exp()\n\n        # new Gmm\n        gmm = FullGmm(nMix, dim)\n        gmm.set_weights(weights)\n        gmm.set_inv_covars_and_means(invcovars, means)\n        gmm.compute_gconsts()\n\n        loglike1, posterior1 = gmm.component_posteriors(feat)\n\n        self.assertAlmostEqual(loglike, loglike1, delta = 0.01)\n        self.assertAlmostEqual(1.0, posterior1.sum(), delta = 0.01)\n\n        weights_bak = gmm.weights()\n        means_bak = gmm.get_means()\n        invcovars_bak = gmm.get_covars()\n        for i in range(nMix):\n            invcovars_bak[i].invert_double_()\n\n        # Set all params one-by-one to new model\n        gmm2 = FullGmm(gmm.num_gauss(), gmm.dim())\n        gmm2.set_weights(weights_bak)\n        gmm2.set_means(means_bak)\n        gmm2.set_inv_covars(invcovars_bak)\n        gmm2.compute_gconsts()\n\n        loglike_gmm2 = gmm2.log_likelihood(feat)\n        self.assertAlmostEqual(loglike1, loglike_gmm2, delta = 0.01)\n\n        loglikes = gmm2.log_likelihoods(feat)\n        self.assertAlmostEqual(loglikes.log_sum_exp(), loglike_gmm2)\n\n        indices = list(range(gmm2.num_gauss()))\n        loglikes = gmm2.log_likelihoods_preselect(feat, indices)\n        self.assertAlmostEqual(loglikes.log_sum_exp(), loglike_gmm2)\n\n        # Simple component mean accessor + mutator\n        gmm3 = FullGmm(gmm.num_gauss(), gmm.dim())\n        gmm3.set_weights(weights_bak)\n        means_bak.set_zero_()\n        for i in range(nMix):\n            gmm.get_component_mean(i, means_bak[i,:])\n        gmm3.set_means(means_bak)\n        gmm3.set_inv_covars(invcovars_bak)\n        gmm3.compute_gconsts()\n\n        loglike_gmm3 = gmm3.log_likelihood(feat)\n        self.assertAlmostEqual(loglike1, loglike_gmm3, delta = 0.01)\n\n        gmm4 = FullGmm(gmm.num_gauss(), gmm.dim())\n        gmm4.set_weights(weights_bak)\n        invcovars_bak, means_bak = gmm.get_covars_and_means()\n        for i in range(nMix):\n            invcovars_bak[i].invert_double_()\n        gmm4.set_inv_covars_and_means(invcovars_bak, means_bak)\n        gmm4.compute_gconsts()\n        loglike_gmm4 = gmm4.log_likelihood(feat)\n        self.assertAlmostEqual(loglike1, loglike_gmm4, delta = 0.01)\n\n        # TODO: I/O tests\n\n        # CopyFromFullGmm\n        gmm4 = FullGmm()\n        gmm4.copy_from_full(gmm)\n        loglike5, _ = gmm4.component_posteriors(feat)\n        self.assertAlmostEqual(loglike, loglike5, delta = 0.01)\n\n        # CopyFromDiag\n        gmm_diag = DiagGmm(nMix, dim)\n        init_rand_diag_gmm(gmm_diag)\n        loglike_diag = gmm_diag.log_likelihood(feat)\n\n        gmm_full = FullGmm().copy(gmm_diag)\n        loglike_full = gmm_full.log_likelihood(feat)\n\n        gmm_diag2 = DiagGmm().copy(gmm_full)\n        loglike_diag2 = gmm_diag2.log_likelihood(feat)\n\n        self.assertAlmostEqual(loglike_diag, loglike_full, delta = 0.01)\n        self.assertAlmostEqual(loglike_diag, loglike_diag2, delta = 0.01)\n\n\n        # Split and merge test for 1 component\n        # TODO: Implement split\n        weights1 = Vector([1.0])\n        means1 = Matrix(means[0:1,:])\n        invcovars1 = [invcovars[0]]\n        gmm1 = FullGmm(1, dim)\n        gmm1.set_weights(weights1)\n        gmm1.set_inv_covars_and_means(invcovars1, means1)\n        gmm1.compute_gconsts()\n\n        gmm2 = FullGmm()\n        gmm2.copy(gmm1)\n        gmm2.split(2, 0.001)\n        gmm2.merge(1)\n        loglike1 = gmm1.log_likelihood(feat)\n        loglike2 = gmm2.log_likelihood(feat)\n        self.assertAlmostEqual(loglike1, loglike2, delta = 0.01)\n\n    def testCovars(self):\n        gmm = FullGmm()\n        init_rand_full(10, 4, gmm)\n        covars = gmm.get_covars()[0]\n        self.assertTupleEqual((10, 10), covars.size())\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/hmm/__init__.py,0,b''
tests/hmm/hmm-topology-test.py,0,"b'import random\nimport unittest\n\nfrom kaldi.base.io import istringstream, ostringstream\nfrom kaldi.hmm import HmmTopology\n\ndef get_default_topology(phones):\n    phones = sorted(phones)\n\n    topo_string = """"""<Topology>\n    <TopologyEntry>\n    <ForPhones>\n    """"""\n    for i in phones:\n        topo_string += str(i) + "" ""\n\n    topo_string += """"""</ForPhones>\n    <State> 0 <PdfClass> 0\n    <Transition> 0 0.5\n    <Transition> 1 0.5\n    </State>\n    <State> 1 <PdfClass> 1\n    <Transition> 1 0.5\n    <Transition> 2 0.5\n    </State>\n    <State> 2 <PdfClass> 2\n    <Transition> 2 0.5\n    <Transition> 3 0.5\n    </State>\n    <State> 3 </State>\n    </TopologyEntry>\n    </Topology>\n    """"""\n\n    topo_string = ostringstream.from_str(topo_string)\n\n    topo = HmmTopology()\n    iss = istringstream.from_str(topo_string.to_str())\n    topo.read(iss, False)\n    return topo\n\nclass TestHMMTopology(unittest.TestCase):\n\n    def testHmmTopology(self):\n\n        input_str = """"""<Topology>\n        <TopologyEntry>\n        <ForPhones> 1 2 3 4 5 6 7 8 9 </ForPhones>\n        <State> 0 <PdfClass> 0\n        <Transition> 0 0.5\n        <Transition> 1 0.5\n        </State>\n        <State> 1 <PdfClass> 1\n        <Transition> 1 0.5\n        <Transition> 2 0.5\n        </State>\n        <State> 2 <PdfClass> 2\n        <Transition> 2 0.5\n        <Transition> 3 0.5\n        </State>\n        <State> 3 </State>\n        </TopologyEntry>\n        <TopologyEntry>\n        <ForPhones> 10 11 13 </ForPhones>\n        <State> 0 <PdfClass> 0\n        <Transition> 0 0.5\n        <Transition> 1 0.5\n        </State>\n        <State> 1 <PdfClass> 1\n        <Transition> 1 0.5\n        <Transition> 2 0.5\n        </State>\n        <State> 2 </State>\n        </TopologyEntry>\n        </Topology>""""""\n\n        chain_input_str = """"""<Topology>\n        <TopologyEntry>\n        <ForPhones> 1 2 3 4 5 6 7 8 9 </ForPhones>\n        <State> 0 <ForwardPdfClass> 0 <SelfLoopPdfClass> 1\n        <Transition> 0 0.5\n        <Transition> 1 0.5\n        </State>\n        <State> 1 </State>\n        </TopologyEntry>\n        </Topology>\n        """"""\n\n        for i in range(10):\n            binary = random.choice([True, False])\n\n            topo = HmmTopology()\n\n            iss = istringstream.from_str(input_str)\n            topo.read(iss, False)\n            self.assertEqual(3, topo.min_length(3))\n            self.assertEqual(2, topo.min_length(11))\n\n            oss = ostringstream()\n            topo.write(oss, binary)\n\n            topo2 = HmmTopology()\n            iss2 = istringstream.from_str(oss.to_bytes())\n            topo2.read(iss2, binary)\n\n            # Test equality\n            oss1 = ostringstream()\n            oss2 = ostringstream()\n            topo.write(oss1, False)\n            topo2.write(oss2, False)\n            self.assertEqual(oss1.to_str(), oss2.to_str())\n\n            # Test chain topology\n            chain_topo = HmmTopology()\n            chain_iss = istringstream.from_str(chain_input_str)\n            chain_topo.read(chain_iss, False)\n            self.assertEqual(1, chain_topo.min_length(3))\n\n            # make sure get_default_topology doesnt crash\n            phones = [1, 2]\n            get_default_topology(phones)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/matrix/__init__.py,0,b''
tests/matrix/matrix-test.py,14,"b""from __future__ import division\nimport unittest\nimport numpy as np\n\nfrom kaldi.matrix import Matrix, SubMatrix, SubVector\nfrom kaldi.matrix import DoubleMatrix, DoubleSubMatrix, DoubleSubVector\nfrom kaldi.matrix.packed import SpMatrix, TpMatrix\n\nclass _Tests(object):\n\n    def test_copy(self):\n        m = self.matrix_class()\n        m1 = self.matrix_class().copy_(m)\n        self.assertTupleEqual((0, 0), m1.shape)\n\n        m = self.matrix_class(5, 5)\n        m1 = self.matrix_class(5, 5).copy_(m)\n        self.assertTupleEqual((5, 5), m1.shape)\n\n        m = self.matrix_class([[1., 2.], [3., 4.]])\n        m1 = self.matrix_class(2, 2).copy_(m)\n        self.assertEqual(m[0, 0], m1[0, 0])\n        self.assertEqual(m[0, 1], m1[0, 1])\n        self.assertEqual(m[1, 1], m1[1, 1])\n\n        m[1, 1] = 5.0\n        self.assertNotEqual(m[1, 1], m1[1, 1])\n\n        with self.assertRaises(ValueError):\n            m = self.matrix_class(5, 5)\n            m1 = self.matrix_class(2, 2).copy_(m)\n\n    def test_clone(self):\n        m = self.matrix_class()\n        m1 = m.clone()\n        self.assertTupleEqual((0, 0), m1.shape)\n\n        m = self.matrix_class(5, 5)\n        m1 = m.clone()\n        self.assertTupleEqual((5, 5), m1.shape)\n\n        m = self.matrix_class([[1., 2.], [3., 4.]])\n        m1 = m.clone()\n\n        self.assertEqual(m[0, 0], m1[0, 0])\n        self.assertEqual(m[0, 1], m1[0, 1])\n        self.assertEqual(m[1, 1], m1[1, 1])\n\n        m[1, 1] = 5.0\n        self.assertNotEqual(m[1, 1], m1[1, 1])\n\n    def test_size(self):\n        m = self.matrix_class()\n        self.assertTupleEqual((0, 0), m.size())\n\n        m = self.matrix_class(10, 10)\n        self.assertTupleEqual((10, 10), m.size())\n\n    def test_equal(self):\n        m = self.matrix_class()\n        self.assertTrue(m == m.clone())\n\n        m = self.matrix_class(4, 4)\n        m.set_zero_()\n        m1 = self.matrix_class(4, 4)\n        m1.set_zero_()\n        self.assertTrue(m == m1)\n\n        m = self.matrix_class(4, 4)\n        m1 = SpMatrix(4)\n        self.assertFalse(m == m1)\n\n    def test_numpy(self):\n        m = self.matrix_class()\n        n = m.numpy()\n        self.assertTupleEqual((0, 0), n.shape)\n\n        m = self.matrix_class(5, 5)\n        n = m.numpy()\n        self.assertTupleEqual((5, 5), n.shape)\n\n        m = self.matrix_class([[1.0, -2.0], [3.0, -4.0]])\n        n = m.numpy()\n        self.assertTupleEqual((2, 2), n.shape)\n        self.assertEqual(1.0, n[0, 0])\n        self.assertEqual(-2.0, n[0, 1])\n        self.assertEqual(-4.0, n[1, 1])\n\n        # Test __array__\n        n = np.asarray(m)\n        self.assertIsInstance(n, np.ndarray)\n        # self.assertEqual(n.dtype, np.float32)\n        for i in range(m.num_rows):\n            for j in range(m.num_cols):\n                self.assertEqual(m[i,j], n[i,j])\n\n        # Test __array__wrap__\n        abs_m = np.abs(m)\n        abs_n = np.abs(n)\n        self.assertIsInstance(abs_m, (SubMatrix, DoubleSubMatrix))\n        for i in range(m.num_rows):\n            for j in range(m.num_cols):\n                self.assertEqual(abs_m[i,j], abs_n[i,j])\n\n        # Test some ufuncs\n        for func in ['sin', 'exp', 'square']:\n            ufunc = getattr(np, func)\n            res_m = ufunc(m)\n            res_n = ufunc(n)\n            self.assertIsInstance(res_m, (SubMatrix, DoubleSubMatrix))\n            for i in range(m.num_rows):\n                for j in range(m.num_cols):\n                    self.assertEqual(res_m[i,j], res_n[i,j])\n\n        # Test a ufunc with boolean return value\n        geq0_m = np.greater_equal(m, 0)\n        geq0_n = np.greater_equal(n, 0).astype('float32')\n        self.assertIsInstance(geq0_m, (SubMatrix, DoubleSubMatrix))\n        for i in range(m.num_rows):\n            for j in range(m.num_cols):\n                self.assertEqual(geq0_m[i,j], geq0_n[i,j])\n\n        # Test a ufunc method that change the number of dimensions\n        max0_m = np.maximum.reduce(m)\n        max0_n = np.maximum.reduce(n)\n        self.assertIsInstance(max0_m, (SubVector, DoubleSubVector))\n        for i in range(len(max0_m)):\n            self.assertEqual(max0_m[i], max0_n[i])\n\n    def test_range(self):\n        m = self.matrix_class()\n\n        self.assertTupleEqual((0, 0), m.range(0, 0, 0, 0).size())\n\n        with self.assertRaises(IndexError):\n            m.range(0, 1, 0, 0)\n\n        with self.assertRaises(IndexError):\n            m.range(0, 0, 0, 1)\n\n        m = self.matrix_class([[1.0, 2.0], [3.0, 4.0]])\n        s = m.range(0, None, 0, None)\n        self.assertTupleEqual((2, 2), s.shape)\n\n    def test__getitem__(self):\n        m = self.matrix_class([[3, 5], [7, 11]])\n        self.assertAlmostEqual(3.0, m[0, 0])\n        self.assertAlmostEqual(5.0, m[0, 1])\n        self.assertAlmostEqual(7.0, m[1, 0])\n        self.assertAlmostEqual(11.0, m[1, 1])\n\n        with self.assertRaises(IndexError):\n            m[3, 0]\n\n        with self.assertRaises(IndexError):\n            m[0, 3]\n\n        self.assertAlmostEqual(15.0, m[0, :].numpy().prod())\n        self.assertAlmostEqual(77.0, m[1, :].numpy().prod())\n        self.assertAlmostEqual(21.0, m[:, 0].numpy().prod())\n        self.assertAlmostEqual(55.0, m[:, 1].numpy().prod())\n\n    def test__setitem__(self):\n        m = self.matrix_class()\n        with self.assertRaises(IndexError):\n            m[0] = 1.0\n\n        m = self.matrix_class(2, 2)\n        m[0, 0] = 3.0\n        m[0, 1] = 5.0\n        m[1, 0] = 7.0\n        m[1, 1] = 11.0\n        self.assertAlmostEqual(3.0, m[0, 0])\n        self.assertAlmostEqual(5.0, m[0, 1])\n        self.assertAlmostEqual(7.0, m[1, 0])\n        self.assertAlmostEqual(11.0, m[1, 1])\n\n        with self.assertRaises(IndexError):\n            m[2, 0] = 10.0\n\n        with self.assertRaises(IndexError):\n            m[0, 2] = 10.0\n\n        m = self.matrix_class([[3, 5], [7, 11]])\n        m[0, 0] = 13.0\n\n        self.assertAlmostEqual(65.0, m[0, :].numpy().prod())\n        self.assertAlmostEqual(77.0, m[1, :].numpy().prod())\n        self.assertAlmostEqual(91.0, m[:, 0].numpy().prod())\n        self.assertAlmostEqual(55.0, m[:, 1].numpy().prod())\n\n\n        m = self.matrix_class([[3, 5], [7, 11]])\n        m[0, :] = 3.0\n\n        self.assertAlmostEqual(9.0, m[0, :].numpy().prod())\n        self.assertAlmostEqual(77.0, m[1, :].numpy().prod())\n        self.assertAlmostEqual(21.0, m[:, 0].numpy().prod())\n        self.assertAlmostEqual(33.0, m[:, 1].numpy().prod())\n\n        m = self.matrix_class([[3, 5], [7, 11]])\n        m[:, 0] = 3.0\n\n        self.assertAlmostEqual(15.0, m[0, :].numpy().prod())\n        self.assertAlmostEqual(33.0, m[1, :].numpy().prod())\n        self.assertAlmostEqual(9.0, m[:, 0].numpy().prod())\n        self.assertAlmostEqual(55.0, m[:, 1].numpy().prod())\n\n    def test__init__(self):\n        # Test Empty\n        m = self.matrix_class()\n        self.assertIsNotNone(m)\n        self.assertTupleEqual((0, 0), m.size())\n\n        # Missing dim\n        with self.assertRaises(IndexError):\n            m = self.matrix_class([[]])\n\n        # num_cols == 0 but num_rows != 0\n        with self.assertRaises(IndexError):\n            m = self.matrix_class([[], []])\n\n        # Construct with size\n        m = self.matrix_class(100, 100)\n        self.assertIsNotNone(m)\n        self.assertTupleEqual((100, 100), m.size())\n\n        # Construct with list\n        m = self.matrix_class([[3, 5], [7, 11]])\n        self.assertIsNotNone(m)\n        self.assertTupleEqual((2, 2), m.size())\n\n        # Construct with np.array\n        m2 = self.matrix_class(np.array([[3, 5], [7, 11]]))\n        self.assertIsNotNone(m2)\n        self.assertTupleEqual((2, 2), m2.size())\n\n        self.assertTrue(m.equal(m2))\n        self.assertTrue(m2.equal(m))\n\n        # Construct with other np.array\n        m = self.matrix_class(np.arange(10).reshape(-1, 2))\n        self.assertTupleEqual((5, 2), m.size())\n\n        with self.assertRaises(ValueError):\n            m = self.matrix_class(np.empty((1, 1, 1)))\n\n        # Construct with other matrix\n        m2 = self.matrix_class(m)\n        self.assertTupleEqual((5, 2), m2.size())\n\n        # Construct with TpMatrix\n        m2 = self.matrix_class(TpMatrix(10))\n        self.assertTupleEqual((10, 10), m2.size())\n\n    def test__delitem__(self):\n        m = self.matrix_class()\n        with self.assertRaises(IndexError):\n            del m[0]\n\n        m = self.matrix_class([[3, 5], [7, 11]])\n\n        del m[0] #deletes row 0\n\n        self.assertTupleEqual((1, 2), m.size())\n        self.assertAlmostEqual(7, m[0,0])\n        self.assertAlmostEqual(11, m[0,1])\n\nclass testSubMatrix(unittest.TestCase):\n\n    def test__init__(self):\n        m = Matrix()\n        sb = SubMatrix(m)\n\n        m = Matrix(5, 5)\n        sb = SubMatrix(m)\n\n        for i in range(100):\n            m.set_randn_()\n            self.assertAlmostEqual(m.sum(), sb.sum())\n\n        m = DoubleMatrix()\n        sb = SubMatrix(m)\n\nclass testSingleMatrix(unittest.TestCase, _Tests):\n    matrix_class = Matrix\n\nclass testDoubleMatrix(unittest.TestCase, _Tests):\n    matrix_class = DoubleMatrix\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/matrix/vector-test.py,17,"b""from __future__ import division\nimport unittest\nimport numpy as np\nfrom kaldi.matrix import Vector, SubVector\nfrom kaldi.matrix import DoubleVector, DoubleSubVector\nfrom kaldi.matrix import Matrix, DoubleMatrix\n\nclass _Tests(object):\n\n    def test_copy(self):\n        v = self.vector_class(5)\n        with self.assertRaises(ValueError):\n            v1 = self.vector_class().copy_(v)\n\n        v1 = self.vector_class(len(v)).copy_(v)\n        self.assertEqual(len(v), len(v1))\n\n        # Make sure modifying original\n        # doesn't break new one\n        v[0] = 1.0\n        v[1] = 2.0\n        v[2] = 3.0\n\n        self.assertNotEqual(v[0], v1[0])\n        self.assertNotEqual(v[1], v1[1])\n        self.assertNotEqual(v[2], v1[2])\n\n        # Check copy works with data\n        v1 = self.vector_class(len(v)).copy_(v)\n\n        self.assertEqual(v[0], v1[0])\n        self.assertEqual(v[1], v1[1])\n        self.assertEqual(v[2], v1[2])\n\n        for i in range(10):\n            v = self.vector_class(i).set_randn_()\n            v1 = self.vector_class(i).copy_(v)\n            self.assertEqual(v, v1)\n\n        v = DoubleVector(5).set_randn_()\n        v1 = self.vector_class(5).copy_(v)\n        # self.assertEqual(v, v1) #This fails due to precision/type\n\n    def test_clone(self):\n\n        # Empty clone\n        v = self.vector_class()\n        v2 = v.clone()\n\n        # Clone with data\n        v = self.vector_class(np.array([3, 5, 7]))\n        v2 = v.clone()\n\n        self.assertEqual(v[0], v2[0])\n        self.assertEqual(v[1], v2[1])\n        self.assertEqual(v[2], v2[2])\n\n        # Make sure modifying original\n        # doesn't break new one\n        v.set_zero_()\n        self.assertNotEqual(v[0], v2[0])\n        self.assertNotEqual(v[1], v2[1])\n        self.assertNotEqual(v[2], v2[2])\n\n    def test_shape(self):\n        v = self.vector_class()\n        self.assertTupleEqual((0,), v.shape)\n\n        v = self.vector_class(5)\n        self.assertTupleEqual((5,), v.shape)\n\n    def test_equal(self):\n        v = self.vector_class()\n        v1 = self.vector_class()\n        self.assertEqual(v, v1)\n\n        v = self.vector_class(5)\n        v1 = self.vector_class(4)\n        self.assertNotEqual(v, v1)\n\n        v = self.vector_class(5)\n        v1 = self.vector_class(5)\n\n        v[0] = 10.0\n        v1[0] = 11.0\n        self.assertNotEqual(v, v1)\n\n        v[0] = v1[0]\n        self.assertTrue(v == v1)\n\n    def test_numpy(self):\n        v = self.vector_class()\n        v1 = v.numpy()\n        self.assertTupleEqual((0, ), v1.shape)\n\n        v = self.vector_class(5)\n        v1 = v.numpy()\n        self.assertTupleEqual((5, ), v1.shape)\n\n        v = self.vector_class([1.0, -2.0, 3.0])\n        v1 = v.numpy()\n        self.assertTrue(np.all(np.array([1.0, -2.0, 3.0]) == v1))\n\n        # Test __array__\n        n = np.asarray(v)\n        self.assertIsInstance(n, np.ndarray)\n        # self.assertEqual(n.dtype, np.float32)\n        for i in range(len(v)):\n            self.assertEqual(v[i], n[i])\n\n        # Test __array__wrap__\n        abs_v = np.abs(v)\n        abs_n = np.abs(n)\n        self.assertIsInstance(abs_v, (SubVector, DoubleSubVector))\n        for i in range(len(v)):\n            self.assertEqual(abs_v[i], abs_n[i])\n\n        # Test some ufuncs\n        for func in ['sin', 'exp', 'square']:\n            ufunc = getattr(np, func)\n            res_v = ufunc(v)\n            res_n = ufunc(n)\n            self.assertIsInstance(res_v, (SubVector, DoubleSubVector))\n            for i in range(len(v)):\n                self.assertEqual(res_v[i], res_n[i])\n\n        # Test a ufunc with boolean return value\n        geq0_v = np.greater_equal(v, 0)\n        geq0_n = np.greater_equal(n, 0).astype('float32')\n        self.assertIsInstance(geq0_v, (SubVector, DoubleSubVector))\n        for i in range(len(v)):\n            self.assertEqual(geq0_v[i], geq0_n[i])\n\n    def test_range(self):\n        v = self.vector_class(np.array([3, 5, 7, 11, 13]))\n        v1 = v.range(1, 2)\n\n        self.assertTrue(isinstance(v1, (SubVector, DoubleSubVector)))\n        self.assertTrue(2, len(v1))\n        v2 = self.vector_class([5, 7])\n        self.assertEqual(v2, v1)\n\n        # What happens if we modify v?\n        v[0] = -1.0\n        self.assertEqual(v2, v1)\n        self.assertEqual(v.range(1, 2), v2)\n\n    def add_vec_(self):\n        v = self.vector_class(5).set_randn_()\n        v1 = self.vector_class(5).set_zero_()\n        self.assertEqual(v, v.add_vec_(v1))\n\n        v1 = v1.set_randn_()\n        self.assertNotEqual(v, v.add_vec_(v1))\n\n        v1 = v.clone()\n        v1 = v1.scale_(-1.0)\n        self.assertEqual(Vector(5), v.add_vec_(1.0, v1))\n\n    def test_copy_row_from_mat(self):\n        \n        with self.assertRaises(IndexError):\n            M = Matrix(0, 0).set_randn_()\n            v = self.vector_class(0).copy_row_from_mat_(M, 0)\n\n        for i in range(1, 11):\n            M = Matrix(i, i).set_randn_()\n            v = self.vector_class(i).copy_row_from_mat_(M, 0)\n            for m, e in zip(M[0], v):\n                self.assertEqual(m, e)\n\n    def test_init(self):\n\n        # Construct empty\n        v = self.vector_class()\n        self.assertIsNotNone(v)\n        self.assertEqual(0, len(v))\n\n        v = self.vector_class([])\n\n        # Construct with size\n        v = self.vector_class(100)\n        self.assertIsNotNone(v)\n        self.assertEqual(100, len(v))\n\n        with self.assertRaises(ValueError):\n            v = self.vector_class(-100)\n\n        # Construct with list\n        v = self.vector_class([3, 5, 7, 11, 13])\n        self.assertEqual(5, len(v))\n        self.assertAlmostEqual(15015.0, v.numpy().prod())\n\n        # Construct with np.array\n        v2 = self.vector_class(np.array([3, 5, 7, 11, 13]))\n        self.assertEqual(5, len(v2))\n        self.assertAlmostEqual(15015.0, v2.numpy().prod())\n        self.assertEqual(v, v2)\n\n        # Construct with other np.array\n        v = self.vector_class(np.arange(10))\n        self.assertEqual(10, v.dim)\n\n        with self.assertRaises(TypeError):\n            v = self.vector_class(np.arange(10).reshape(-1, 2))\n\n        # New with double\n        v = self.vector_class(DoubleVector(10).set_randn_())\n        self.assertEqual(10, v.dim)\n\n    def test__getitem__(self):\n        v = self.vector_class()\n        with self.assertRaises(IndexError):\n            v[0]\n\n        v = self.vector_class([3, 5, 7, 11, 13])\n        self.assertAlmostEqual(3.0, v[0])\n        self.assertAlmostEqual(7.0, v[2])\n        self.assertAlmostEqual(13.0, v[4])\n\n        with self.assertRaises(IndexError):\n            v[5]\n\n        self.assertAlmostEqual(15015.0, v[:10].numpy().prod())\n        self.assertAlmostEqual(15015.0, v[::-1].numpy().prod())\n        self.assertAlmostEqual(1001.0, v[2:5].numpy().prod())\n\n    def test__setitem__(self):\n        v = self.vector_class()\n        with self.assertRaises(IndexError):\n            v[0] = 1.0\n\n        v = self.vector_class([3, 5, 7, 11, 13])\n        v[0] = 15.0\n        self.assertAlmostEqual(75075.0, v[:10].numpy().prod())\n\n        with self.assertRaises(ValueError):\n            v[0:3] = np.array([3, 5, 7, 11, 13])\n\n        v[0:5] = np.array([3, 5, 7, 11, 13])\n        self.assertAlmostEqual(15015.0, v.numpy().prod())\n\n    def test__delitem__(self):\n        v = self.vector_class()\n        with self.assertRaises(IndexError):\n            del v[0]\n\n        v = self.vector_class([3, 5, 7, 11, 13])\n        del v[0]\n        self.assertAlmostEqual(5005.0, v.numpy().prod())\n\n\nclass SingleVectorTest(unittest.TestCase, _Tests):\n    vector_class = Vector\n\nclass DoubleVectorTest(unittest.TestCase, _Tests):\n    vector_class = DoubleVector\n\nif __name__ == '__main__':\n    unittest.main()"""
tests/nnet3/__init__.py,0,b''
tests/nnet3/nnet-compute-test.py,0,"b'#!/usr/bin/env python\n\nimport random\nimport unittest\n\nfrom kaldi.base.io import istringstream, ostringstream\nfrom kaldi.cudamatrix import cuda_available, approx_equal_cu_matrix, CuMatrix\nfrom kaldi.matrix import Matrix, Vector\nfrom kaldi.matrix.functions import approx_equal\nfrom kaldi.nnet3 import *\n\nclass TestNnetCompute(unittest.TestCase):\n\n    def test_nnet_compute(self):\n        gen_config = NnetGenerationOptions()\n        test_collapse_model = random.choice([True, False])\n\n        configs = generate_config_sequence(gen_config)\n        nnet = Nnet()\n        for j, config in enumerate(configs):\n            # print(""Input config[{}]:"".format(j))\n            # print(config)\n            istrm = istringstream.from_str(config)\n            nnet.read_config(istrm)\n\n        request = ComputationRequest()\n        inputs = compute_example_computation_request_simple(nnet, request)\n        if test_collapse_model:\n            set_batchnorm_test_mode(True, nnet)\n            set_dropout_test_mode(True, nnet)\n\n        compiler = Compiler(request, nnet)\n        opts = CompilerOptions()\n        computation = compiler.create_computation(opts)\n\n        nnet_collapsed = Nnet.from_other(nnet)\n        if test_collapse_model:\n            collapse_config = CollapseModelConfig()\n            collapse_model(collapse_config, nnet_collapsed)\n            compiler_collapsed = Compiler(request, nnet_collapsed)\n            computation_collapsed = compiler_collapsed.create_computation(opts)\n            computation_collapsed.compute_cuda_indexes()\n\n        ostrm = ostringstream()\n        computation.print_computation(ostrm, nnet)\n        # print(""Generated computation:"")\n        # print(ostrm.to_str())\n\n        check_config = CheckComputationOptions()\n        check_config.check_rewrite = True\n        checker = ComputationChecker(check_config, nnet, computation)\n        checker.check()\n\n        if random.choice([True, False]):\n            opt_config = NnetOptimizeOptions()\n            optimize(opt_config, nnet, max_output_time_in_request(request),\n                     computation)\n            ostrm = ostringstream()\n            computation.print_computation(ostrm, nnet)\n            # print(""Optimized computation:"")\n            # print(ostrm.to_str())\n\n        compute_opts = NnetComputeOptions()\n        compute_opts.debug = random.choice([True, False])\n        computation.compute_cuda_indexes()\n        computer = NnetComputer(compute_opts, computation, nnet, nnet)\n\n        for i, ispec in enumerate(request.inputs):\n            temp = CuMatrix.from_matrix(inputs[i])\n            print(""Input sum:"", temp.sum())\n            computer.accept_input(ispec.name, temp)\n        computer.run()\n\n        output = computer.get_output_destructive(""output"")\n        print(""Output sum:"", output.sum())\n\n        if test_collapse_model:\n            computer_collapsed = NnetComputer(compute_opts,\n                                              computation_collapsed,\n                                              nnet_collapsed, nnet_collapsed)\n            for i, ispec in enumerate(request.inputs):\n                temp = CuMatrix.from_matrix(inputs[i])\n                computer_collapsed.accept_input(ispec.name, temp)\n            computer_collapsed.run()\n            output_collapsed = computer_collapsed.get_output_destructive(""output"")\n            print(""Output sum [collapsed]:"", output_collapsed.sum())\n            self.assertTrue(approx_equal_cu_matrix(output, output_collapsed),\n                            ""Regular and collapsed computation outputs differ."")\n\n        output_deriv = CuMatrix.from_size(output.num_rows(), output.num_cols())\n        output_deriv.set_randn()\n        if request.outputs[0].has_deriv:\n            computer.accept_input(""output"", output_deriv)\n            computer.run()\n            for i, ispec in enumerate(request.inputs):\n                if ispec.has_deriv:\n                    in_deriv = computer.get_output_destructive(ispec.name)\n                    print(""Input-deriv sum for input {} is:"".format(ispec.name),\n                          in_deriv.sum())\n\n    def test_nnet_decodable(self):\n        gen_config = NnetGenerationOptions()\n        configs = generate_config_sequence(gen_config)\n        nnet = Nnet()\n        for j, config in enumerate(configs):\n            # print(""Input config[{}]:"".format(j))\n            # print(config)\n            istrm = istringstream.from_str(config)\n            nnet.read_config(istrm)\n\n        num_frames = 5 + random.randint(1, 100)\n        input_dim = nnet.input_dim(""input"")\n        output_dim = nnet.output_dim(""output"")\n        ivector_dim = max(0, nnet.input_dim(""ivector""))\n        input = Matrix(num_frames, input_dim)\n\n        set_batchnorm_test_mode(True, nnet)\n        set_dropout_test_mode(True, nnet)\n\n        input.set_randn_()\n        ivector = Vector(ivector_dim)\n        ivector.set_randn_()\n\n        priors = Vector(output_dim if random.choice([True, False]) else 0)\n        if len(priors) != 0:\n            priors.set_randn_()\n            priors.apply_exp_()\n\n        output1 = Matrix(num_frames, output_dim)\n        output2 = Matrix(num_frames, output_dim)\n\n        opts = NnetSimpleComputationOptions()\n        opts.frames_per_chunk = random.randint(5, 25)\n        compiler = CachingOptimizingCompiler(nnet)\n        decodable = DecodableNnetSimple(opts, nnet, priors, input, compiler,\n                                        ivector if ivector_dim else None)\n        for t in range(num_frames):\n            decodable.get_output_for_frame(t, output1[t])\n\n        opts = NnetSimpleLoopedComputationOptions()\n        info = DecodableNnetSimpleLoopedInfo.from_priors(opts, priors, nnet)\n        decodable = DecodableNnetSimpleLooped(info, input,\n                                              ivector if ivector_dim else None)\n        for t in range(num_frames):\n            decodable.get_output_for_frame(t, output2[t])\n\n        if (not nnet_is_recurrent(nnet)\n            and nnet.info().find(""statistics-extraction"") == -1\n            and nnet.info().find(""TimeHeightConvolutionComponent"") == -1\n            and nnet.info().find(""RestrictedAttentionComponent"") == -1):\n            for t in range(num_frames):\n                self.assertTrue(approx_equal(output1[t], output2[t]))\n\n\nif __name__ == \'__main__\':\n    for i in range(2):\n        if cuda_available():\n            from kaldi.cudamatrix import CuDevice\n            CuDevice.instantiate().set_debug_stride_mode(True)\n            if i == 0:\n                CuDevice.instantiate().select_gpu_id(""no"")\n            else:\n                CuDevice.instantiate().select_gpu_id(""yes"")\n        unittest.main(exit=False)\n'"
tests/util/__init__.py,0,b''
tests/util/kaldi-io-test.py,0,"b'from __future__ import print_function\n\nimport os\nimport unittest\n\nfrom kaldi.util.io import *\n\n\nclass TestKaldiIO(unittest.TestCase):\n\n    def testClassifyRxfilename(self):\n        self.assertEqual(InputType.STANDARD_INPUT, classify_rxfilename(""""))\n        self.assertEqual(InputType.NO_INPUT, classify_rxfilename("" ""))\n        self.assertEqual(InputType.NO_INPUT, classify_rxfilename("" a ""))\n        self.assertEqual(InputType.NO_INPUT, classify_rxfilename(""a ""))\n        self.assertEqual(InputType.FILE_INPUT, classify_rxfilename(""a""))\n        self.assertEqual(InputType.STANDARD_INPUT, classify_rxfilename(""-""))\n        self.assertEqual(InputType.PIPE_INPUT, classify_rxfilename(""b|""))\n        self.assertEqual(InputType.NO_INPUT, classify_rxfilename(""|b""))\n        self.assertEqual(InputType.PIPE_INPUT, classify_rxfilename(""b c|""))\n        self.assertEqual(InputType.OFFSET_FILE_INPUT, classify_rxfilename(""a b c:123""))\n        self.assertEqual(InputType.OFFSET_FILE_INPUT, classify_rxfilename(""a b c:3""))\n        self.assertEqual(InputType.FILE_INPUT, classify_rxfilename(""a b c:""))\n        self.assertEqual(InputType.FILE_INPUT, classify_rxfilename(""a b c/3""))\n\n    def testClassifyWxfilename(self):\n        self.assertEqual(OutputType.STANDARD_OUTPUT, classify_wxfilename(""""))\n        self.assertEqual(OutputType.NO_OUTPUT, classify_wxfilename("" ""))\n        self.assertEqual(OutputType.NO_OUTPUT, classify_wxfilename("" a ""))\n        self.assertEqual(OutputType.NO_OUTPUT, classify_wxfilename(""a ""))\n        self.assertEqual(OutputType.FILE_OUTPUT, classify_wxfilename(""a""))\n        self.assertEqual(OutputType.STANDARD_OUTPUT, classify_wxfilename(""-""))\n        self.assertEqual(OutputType.NO_OUTPUT, classify_wxfilename(""b|""))\n        self.assertEqual(OutputType.PIPE_OUTPUT, classify_wxfilename(""|b""))\n        self.assertEqual(OutputType.NO_OUTPUT, classify_wxfilename(""b c|""))\n        self.assertEqual(OutputType.NO_OUTPUT, classify_wxfilename(""a b c:123""))\n        self.assertEqual(OutputType.NO_OUTPUT, classify_wxfilename(""a b c:3""))\n        self.assertEqual(OutputType.FILE_OUTPUT, classify_wxfilename(""a b c:""))\n        self.assertEqual(OutputType.FILE_OUTPUT, classify_wxfilename(""a b c/3""))\n\n    def test_text_io(self):\n        filename = ""tmpf""\n        lines = [""400\\t500\\t600"", ""700\\td""]\n        with Output(filename, False) as ko:\n            for line in lines:\n                print(line, file=ko)\n        with Input(filename, False) as ki:\n            for i, line in enumerate(ki):\n                self.assertEqual(line.strip(), lines[i])\n        os.remove(filename)\n\n    def test_binary_io(self):\n        filename = ""tmpf""\n        lines = [b""\\t500\\t600\\n"", b""700\\td\\n""]\n        with Output(filename) as ko:\n            for line in lines:\n                ko.write(line)\n        with Input(filename) as ki:\n            self.assertTrue(ki.binary)\n            for i, line in enumerate(ki):\n                self.assertEqual(line, lines[i])\n        os.remove(filename)\n\n    def test_xopen(self):\n        filename = ""tmpf""\n        lines = [b""\\t500\\t600\\n"", b""700\\td\\n""]\n        with xopen(filename, ""w"") as ko:\n            ko.writelines(lines)\n        with xopen(filename) as ki:\n            self.assertTrue(ki.binary)\n            for i, line in enumerate(ki):\n                self.assertEqual(line, lines[i])\n        os.remove(filename)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/util/kaldi-table-test.py,0,"b'from kaldi.util.table import *\n\nimport unittest\n\nclass TestKaldiTable(unittest.TestCase):\n    def testClassifyWspecifier(self):\n        a = ""b,ark:foo|""\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.ARCHIVE_SPECIFIER, ans)\n        self.assertEqual(""foo|"", ark)\n        self.assertEqual("""", scp)\n        self.assertTrue(opts.binary)\n\n        a = ""t,ark:foo|""\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.ARCHIVE_SPECIFIER, ans)\n        self.assertEqual(""foo|"", ark)\n        self.assertEqual("""", scp)\n        self.assertFalse(opts.binary)\n\n        a = ""t,scp:a b c d""\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual("""", ark)\n        self.assertEqual(""a b c d"", scp)\n        self.assertFalse(opts.binary)\n\n        a = ""t,ark,scp:a b,c,d""\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.BOTH_SPECIFIER, ans)\n        self.assertEqual(""a b"", ark)\n        self.assertEqual(""c,d"", scp)\n        self.assertFalse(opts.binary)\n\n        a = """"\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.NO_SPECIFIER, ans)\n\n        a = "" t,ark:boo"" #leading space not allowed.\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.NO_SPECIFIER, ans)\n\n        a = "" t,ark:boo""\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.NO_SPECIFIER, ans)\n\n        a = ""t,ark:boo "" #trailing space not allowed.\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.NO_SPECIFIER, ans)\n\n        a = ""b,ark,scp:,"" #empty ark, scp fnames valid.\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.BOTH_SPECIFIER, ans)\n        self.assertEqual("""", ark)\n        self.assertEqual("""", scp)\n        self.assertTrue(opts.binary)\n\n        a = ""f,b,ark,scp:,"" #empty ark, scp fnames valid.\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.BOTH_SPECIFIER, ans)\n        self.assertEqual("""", ark)\n        self.assertEqual("""", scp)\n        self.assertTrue(opts.binary)\n        self.assertTrue(opts.flush)\n\n        a = ""nf,b,ark,scp:,"" #empty ark, scp fnames valid.\n        ans, ark, scp, opts = classify_wspecifier(a)\n        self.assertEqual(WspecifierType.BOTH_SPECIFIER, ans)\n        self.assertEqual("""", ark)\n        self.assertEqual("""", scp)\n        self.assertTrue(opts.binary)\n        self.assertFalse(opts.flush)\n\n    def testClassifyRspecifier(self):\n        a = ""ark:foo|""\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.ARCHIVE_SPECIFIER, ans)\n        self.assertEqual(""foo|"", fname)\n\n        a = ""b,ark:foo|"" #b, is ignored\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.ARCHIVE_SPECIFIER, ans)\n        self.assertEqual(""foo|"", fname)\n\n        a = ""ark,b:foo|"" #,b is ignored\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.ARCHIVE_SPECIFIER, ans)\n        self.assertEqual(""foo|"", fname)\n\n        a = ""scp,b:foo|""\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual(""foo|"", fname)\n\n        a = ""scp,scp,b:foo|"";  #invalid as repeated.\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.NO_SPECIFIER, ans)\n        self.assertEqual("""", fname)\n\n        a = ""ark,scp,b:foo|"" #invalid as combined\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.NO_SPECIFIER, ans)\n        self.assertEqual("""", fname)\n\n        a = ""scp,o:foo|""\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual(""foo|"", fname)\n        self.assertTrue(opts.once)\n\n        a = ""scp,no:foo|""\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual(""foo|"", fname)\n        self.assertFalse(opts.once)\n\n        a = ""s,scp,no:foo|""\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual(""foo|"", fname)\n        self.assertFalse(opts.once)\n        self.assertTrue(opts.sorted)\n\n        a = ""scp:foo|""\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual(""foo|"", fname)\n\n        a = ""scp:"" #empty fname valid\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual("""", fname)\n\n        a = """"\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.NO_SPECIFIER, ans)\n\n        a = ""scp""\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.NO_SPECIFIER, ans)\n\n        a = ""ark""\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.NO_SPECIFIER, ans)\n\n        a = ""ark:foo "" #trailing space not allowed\n        ans, fname, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.NO_SPECIFIER, ans)\n\n        # Testing it accepts the meaningless t, and b, prefixes\n        a = ""b,scp:a""\n        ans, b, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual(""a"", b)\n\n        a = ""t,scp:a""\n        ans, b, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.SCRIPT_SPECIFIER, ans)\n        self.assertEqual(""a"", b)\n\n        a = ""b,ark:a""\n        ans, b, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.ARCHIVE_SPECIFIER, ans)\n        self.assertEqual(""a"", b)\n\n        a = ""t,ark:a""\n        ans, b, opts = classify_rspecifier(a)\n        self.assertEqual(RspecifierType.ARCHIVE_SPECIFIER, ans)\n        self.assertEqual(""a"", b)\n\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/util/mixins.py,1,"b'import re\nimport os\n\n\nimport kaldi.util\nfrom kaldi.matrix import Matrix\nfrom kaldi.util.table import WaveWriter\nfrom kaldi.feat.wave import WaveData\n\nimport numpy as np\n\n################################################################################################################\n# Auxiliary class that provides classname instanciation\n################################################################################################################\nclass AuxMixin:\n    def getClassname(self):\n        """""" Infers the name of the object to construct from the classname. """"""\n        return self._test_replacer.sub(\'\', self.__class__.__name__)\n\n    def setUp(self):\n        """""" Sets up the inference for classname so that getImpl constructs the correct object """"""\n        self._test_replacer = re.compile(re.escape(""test""), re.IGNORECASE)\n        self.classname = self.getClassname()\n        self.filename = \'/tmp/temp.ark\'\n        self.rspecifier = \'ark,t:{}\'.format(self.filename)\n\n    def getImpl(self, *args):\n        """""" Returns an instance of the self.classname class passing along the arguments for construction """"""\n        if args:\n            return getattr(kaldi.util.table, self.classname)(args[0])\n        else:\n            return getattr(kaldi.util.table, self.classname)()\n\n    def tearDown(self):\n        if os.path.exists(self.filename):\n            os.remove(self.filename)\n\n\n################################################################################################################\n# Examples mixins\n################################################################################################################\nclass VectorExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""""""one  [ 3 5 7 ]\\n"""""" +\\\n                    """"""two  [ 1 2 3 ]\\n"""""" +\\\n                    """"""three  [ ]\\n"""""")\nclass MatrixExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""""""one  [\\n"""""" +\\\n                    """"""  0 1 2\\n"""""" +\\\n                    """"""  3 4 5\\n"""""" +\\\n                    """"""  6 7 8 ]\\n"""""" +\\\n                    """"""two  [\\n"""""" +\\\n                    """"""  1\\n"""""" +\\\n                    """"""  2\\n"""""" +\\\n                    """"""  3 ]\\n"""""" +\\\n                    """"""three  [ ]\\n"""""")\n\nclass WaveExampleMixin:\n    def writeExample(self, outpt):\n        m = Matrix(np.arange(9).reshape((3, 3)))\n        with WaveWriter(\'ark:/tmp/temp.ark\') as writer:\n            writer[\'one\'] = WaveData.from_data(1.0, m)\n\nclass IntExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""one 1\\ntwo 2\\nthree 3\\n"")\n\nclass FloatExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""one 1.0\\ntwo 2.0\\nthree 3.0\\n"")\n\nclass BoolExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""one T\\ntwo T\\nthree F\\n"")\n\nclass IntVectorExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""""""one 1\\n"""""" +\\\n                    """"""two 2 3\\n"""""" +\\\n                    """"""three\\n"""""")\n\nclass IntVectorVectorExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""""""one 1 ;\\n"""""" +\\\n                    """"""two 1 2 ; 3 4 ;\\n"""""" +\\\n                    """"""three\\n"""""")\n\nclass IntPairVectorExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""""""one 1 1\\n"""""" +\\\n                    """"""two 2 3 ; 4 5\\n"""""" +\\\n                    """"""three\\n"""""")\n\nclass FloatPairVectorExampleMixin:\n    def writeExample(self, outpt):\n        outpt.write(""""""one 1.0 1.0\\n"""""" +\\\n                    """"""two 2.0 3.0 ; 4.0 5.0\\n"""""" +\\\n                    """"""three\\n"""""")\n'"
tests/util/readers-test.py,10,"b'from __future__ import division\nimport numpy as np\nimport os\nimport unittest\n\nfrom kaldi.matrix import *\nimport kaldi.util\n\nfrom .mixins import *\n\n################################################################################################################\n# Sequential Readers\n################################################################################################################\nclass _TestSequentialReaders(AuxMixin):\n    def test__init__(self):\n\n        # Empty reader\n        reader = self.getImpl()\n        self.assertIsNotNone(reader)\n        with self.assertRaises(RuntimeError):\n            self.assertFalse(reader.is_open())\n            self.assertFalse(reader.done())\n\n        with self.assertRaises(RuntimeError):\n            reader.close()\n\n        # Delete file in case it exists\n        if os.path.exists(self.filename):\n            os.remove(self.filename)\n\n        # Reader into a file that does not exists\n        with self.assertRaises(IOError):\n            reader = self.getImpl(self.rspecifier)\n\n        # Touch file\n        open(self.filename, \'w\').close()\n\n        reader = self.getImpl(self.rspecifier)\n        self.assertTrue(reader.is_open())\n        self.assertTrue(reader.done())\n\n    def testContextManager(self):\n        # Delete file in case it exists\n        if os.path.exists(self.filename):\n            os.remove(self.filename)\n\n        # Empty reader via CM\n        with self.assertRaises(RuntimeError):\n            with self.getImpl() as reader:\n                self.assertFalse(reader.is_open())\n                self.assertFalse(reader.done())\n\n            self.assertFalse(reader.is_open())\n            self.assertFalse(reader.done())\n\n        # Reset reader so that it doesnt by default pass the next ones\n        reader = None\n\n        # Touch file\n        open(self.filename, \'w\').close()\n\n        with self.getImpl(self.rspecifier) as reader:\n            self.assertTrue(reader.is_open())\n            self.assertTrue(reader.done())\n\n        with self.assertRaises(RuntimeError):\n            self.assertFalse(reader.is_open())\n            self.assertTrue(reader.done())\n\n    def test__iter__(self):\n        # Create a file and write an example to it\n        with open(self.filename, \'w\') as outpt:\n            self.writeExample(outpt)\n\n        # Iterate over the file\n        with self.getImpl(self.rspecifier) as reader:\n            for idx, (k, v) in enumerate(reader):\n                self.checkRead(idx, (k, v))\n\n        # Check iteration is done\n        # FIXME:\n        # This raises C++ exception instead of a simple True\n        # self.assertTrue(reader.done())\n\n        # Check iterator is closed\n        self.assertFalse(reader.is_open())\n\nclass TestSequentialVectorReader(_TestSequentialReaders, unittest.TestCase, VectorExampleMixin):\n    def checkRead(self, idx, pair):\n        k, v = pair\n        if idx == 0:\n            self.assertEqual(""one"", k)\n            self.assertTrue(np.array_equal([3.0, 5.0, 7.0], v.numpy()))\n        elif idx == 1:\n            self.assertEqual(""two"", k)\n            self.assertTrue(np.array_equal([1.0, 2.0, 3.0], v.numpy()))\n        elif idx == 2:\n            self.assertEqual(""three"", k)\n            self.assertEqual(0, len(v.numpy()))\n        else:\n            self.fail(""shouldn\'t happen"")\n\nclass TestSequentialMatrixReader(_TestSequentialReaders, unittest.TestCase, MatrixExampleMixin):\n    def checkRead(self, idx, pair):\n        k, m = pair\n        if idx == 0:\n            self.assertEqual(""one"", k)\n            self.assertTrue(np.array_equal(np.arange(9).reshape((3, 3)), m.numpy()))\n        elif idx == 1:\n            self.assertEqual(""two"", k)\n            self.assertTrue(np.array_equal([[1.0], [2.0], [3.0]], m.numpy()))\n        elif idx == 2:\n            self.assertEqual(""three"", k)\n            self.assertEqual(0, len(m.numpy()))\n        else:\n            self.fail(""shouldn\'t happen"")\n\nclass TestSequentialWaveReader(_TestSequentialReaders, unittest.TestCase, WaveExampleMixin):\n    def checkRead(self, idx, pair):\n        k, m = pair\n        if idx == 0:\n            self.assertTrue(""one"", k)\n            self.assertTrue(np.array_equal(np.arange(9).reshape((3, 3)), m.data().numpy()))\n        else:\n            self.fail(""shouldn\'t happen"")\n\nclass TestSequentialFloatReader(_TestSequentialReaders, unittest.TestCase, FloatExampleMixin):\n    def checkRead(self, idx, pair):\n        if idx == 0:\n            self.assertTupleEqual((""one"", 1.0), pair)\n        elif idx == 1:\n            self.assertTupleEqual((""two"", 2.0), pair)\n        elif idx == 2:\n            self.assertTupleEqual((""three"", 3.0), pair)\n        elif idx < 0 or idx > 2:\n            self.fail(""shouldn\'t happen"")\n\nclass TestSequentialBoolReader(_TestSequentialReaders, unittest.TestCase, BoolExampleMixin):\n    def checkRead(self, idx, pair):\n        if idx == 0:\n            self.assertTupleEqual((""one"", True), pair)\n        elif idx == 1:\n            self.assertTupleEqual((""two"", True), pair)\n        elif idx == 2:\n            self.assertTupleEqual((""three"", False), pair)\n        elif idx < 0 or idx > 2:\n            self.fail(""shouldn\'t happen"")\n\nclass TestSequentialIntVectorReader(_TestSequentialReaders, unittest.TestCase, IntVectorExampleMixin):\n    def checkRead(self, idx, pair):\n        if idx == 0:\n            self.assertTupleEqual((""one"", [1]), pair)\n        elif idx == 1:\n            self.assertTupleEqual((""two"", [2, 3]), pair)\n        elif idx == 2:\n            self.assertTupleEqual((""three"", []), pair)\n        elif idx < 0 or idx > 2:\n            self.fail(""shouldn\'t happen"")\n\nclass TestSequentialIntVectorVectorReader(_TestSequentialReaders, unittest.TestCase, IntVectorVectorExampleMixin):\n    def checkRead(self, idx, pair):\n        if idx == 0:\n            self.assertTupleEqual((""one"", [[1]]), pair)\n        elif idx == 1:\n            self.assertTupleEqual((""two"", [[1, 2], [3, 4]]), pair)\n        elif idx == 2:\n            self.assertTupleEqual((""three"", []), pair)\n        elif idx < 0 or idx > 2:\n            self.fail(""shouldn\'t happen"")\n\nclass TestSequentialIntPairVectorReader(_TestSequentialReaders, unittest.TestCase, IntPairVectorExampleMixin):\n    def checkRead(self, idx, pair):\n        if idx == 0:\n            self.assertTupleEqual((""one"", [(1, 1)]), pair)\n        elif idx == 1:\n            self.assertTupleEqual((""two"", [(2, 3), (4, 5)]), pair)\n        elif idx == 2:\n            self.assertTupleEqual((""three"", []), pair)\n        elif idx < 0 or idx > 2:\n            self.fail(""shouldn\'t happen"")\n\nclass TestSequentialFloatPairVectorReader(_TestSequentialReaders, unittest.TestCase, FloatPairVectorExampleMixin):\n    def checkRead(self, idx, pair):\n        if idx == 0:\n            self.assertTupleEqual((""one"", [(1.0, 1.0)]), pair)\n        elif idx == 1:\n            self.assertTupleEqual((""two"", [(2.0, 3.0), (4.0, 5.0)]), pair)\n        elif idx == 2:\n            self.assertTupleEqual((""three"", []), pair)\n        elif idx < 0 or idx > 2:\n            self.fail(""shouldn\'t happen"")\n\n################################################################################################################\n# Random Access Readers\n################################################################################################################\nclass _TestRandomAccessReaders(AuxMixin):\n\n    def test__init__(self):\n        reader = self.getImpl()\n        self.assertIsNotNone(reader)\n        self.assertFalse(reader.is_open())\n\n        with self.assertRaises(RuntimeError):\n            reader.close()\n\n        # Delete file in case it exists\n        if os.path.exists(self.filename):\n            os.remove(self.filename)\n\n        # Reading a non-existant file raises an exception\n        with self.assertRaises(IOError):\n            reader = self.getImpl(self.rspecifier)\n            self.assertIsNotNone(reader)\n\n        # Note (VM): If the file does not exist, this is false\n        self.assertFalse(reader.is_open())\n\n        # Touch file\n        open(self.filename, \'w\').close()\n\n        reader = self.getImpl(self.rspecifier)\n        self.assertTrue(reader.is_open())\n\n    def testContextManager(self):\n        with self.assertRaises(RuntimeError):\n            with self.getImpl() as reader:\n                self.assertFalse(reader.is_open())\n\n            self.assertFalse(reader.is_open())\n\n        # Reset reader so that it doesnt by default pass the next ones\n        reader = None\n\n        # Touch file\n        open(self.filename, \'w\').close()\n\n        with self.getImpl(self.rspecifier) as reader:\n            self.assertTrue(reader.is_open())\n\n        self.assertFalse(reader.is_open())\n\n    def getValidKey(self):\n        return ""one""\n\n    def getNotValidKey(self):\n        return ""four""\n\n    def test__contains__(self):\n        # Create a file and write an example to it\n        with open(self.filename, \'w\') as outpt:\n            self.writeExample(outpt)\n\n        # Empty impl\n        with self.assertRaises(RuntimeError):\n            self.assertFalse(self.getImpl().__contains__(self.getValidKey()))\n            self.assertFalse(self.getImpl().__contains__(self.getNotValidKey()))\n\n        # Check keys in example\n        self.assertTrue(self.getImpl(self.rspecifier).__contains__(self.getValidKey()))\n        self.assertFalse(self.getImpl(self.rspecifier).__contains__(self.getNotValidKey()))\n\n        # Also check for the *in* operator\n        self.assertTrue(self.getValidKey() in self.getImpl(self.rspecifier))\n        self.assertFalse(self.getNotValidKey() in self.getImpl(self.rspecifier))\n\n    def test__getitem__(self):\n        # Create a file and write an example to it\n        with open(self.filename, \'w\') as outpt:\n            self.writeExample(outpt)\n\n        #\n        self.checkRead(self.getImpl(self.rspecifier))\n\n        # Check that the keys are strings\n        with self.assertRaises(TypeError):\n            self.getImpl(self.rspecifier)[1]\n\nclass TestRandomAccessVectorReader(_TestRandomAccessReaders, unittest.TestCase, VectorExampleMixin):\n    def checkRead(self, reader):\n        self.assertTrue(np.array_equal([3.0, 5.0, 7.0], reader[""one""].numpy()))\n        self.assertTrue(np.array_equal([1.0, 2.0, 3.0], reader[""two""].numpy()))\n        self.assertEqual(0, len(reader[""three""].numpy()))\n\nclass TestRandomAccessMatrixReader(_TestRandomAccessReaders, unittest.TestCase, MatrixExampleMixin):\n    def checkRead(self, reader):\n        self.assertTrue(np.array_equal(np.arange(9).reshape((3, 3)), reader[""one""].numpy()))\n        self.assertTrue(np.array_equal([[1.0], [2.0], [3.0]], reader[""two""].numpy()))\n        self.assertEqual(0, len(reader[""three""].numpy()))\n\nclass TestRandomAccessWaveReader(_TestRandomAccessReaders, unittest.TestCase, WaveExampleMixin):\n    def checkRead(self, reader):\n        self.assertTrue(np.array_equal(np.arange(9).reshape((3, 3)), reader[""one""].data().numpy()))\n\nclass TestRandomAccessIntReader(_TestRandomAccessReaders, unittest.TestCase, IntExampleMixin):\n    def checkRead(self, reader):\n        self.assertEqual(1, reader[\'one\'])\n        self.assertEqual(3, reader[\'three\'])\n        self.assertEqual(2, reader[\'two\'])\n\n        with self.assertRaises(KeyError):\n            reader[\'four\']\n\nclass TestRandomAccessFloatReader(_TestRandomAccessReaders, unittest.TestCase, IntExampleMixin):\n    def checkRead(self, reader):\n        self.assertEqual(1.0, reader[\'one\'])\n        self.assertEqual(3.0, reader[\'three\'])\n        self.assertEqual(2.0, reader[\'two\'])\n\n        with self.assertRaises(KeyError):\n            reader[\'four\']\n\nclass TestRandomAccessBoolReader(_TestRandomAccessReaders, unittest.TestCase, BoolExampleMixin):\n    def checkRead(self, reader):\n        self.assertEqual(True, reader[\'one\'])\n        self.assertEqual(False, reader[\'three\'])\n        self.assertEqual(True, reader[\'two\'])\n\n        with self.assertRaises(KeyError):\n            reader[\'four\']\n\nclass TestRandomAccessIntVectorReader(_TestRandomAccessReaders, unittest.TestCase, IntVectorExampleMixin):\n    def checkRead(self, reader):\n        self.assertEqual([1], reader[\'one\'])\n        self.assertEqual([], reader[\'three\'])\n        self.assertEqual([2, 3], reader[\'two\'])\n\n        with self.assertRaises(KeyError):\n            reader[\'four\']\n\nclass TestRandomAccessIntVectorVectorReader(_TestRandomAccessReaders, unittest.TestCase, IntVectorVectorExampleMixin):\n    def checkRead(self, reader):\n        self.assertEqual([[1]], reader[\'one\'])\n        self.assertEqual([], reader[\'three\'])\n        self.assertEqual([[1, 2], [3, 4]], reader[\'two\'])\n\n        with self.assertRaises(KeyError):\n            reader[\'four\']\n\nclass TestRandomAccessIntPairVectorReader(_TestRandomAccessReaders, unittest.TestCase, IntPairVectorExampleMixin):\n    def checkRead(self, reader):\n        self.assertEqual([(1, 1)], reader[""one""])\n        self.assertEqual([], reader[""three""])\n        self.assertEqual([(2, 3), (4, 5)], reader[""two""])\n\n        with self.assertRaises(KeyError):\n            reader[\'four\']\n\nclass TestRandomAccessFloatPairVectorReader(_TestRandomAccessReaders, unittest.TestCase, FloatPairVectorExampleMixin):\n    def checkRead(self, reader):\n        self.assertEqual([(1.0, 1.0)], reader[\'one\'])\n        self.assertEqual([], reader[\'three\'])\n        self.assertEqual([(2.0, 3.0), (4.0, 5.0)], reader[\'two\'])\n\n        with self.assertRaises(KeyError):\n            reader[\'four\']\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/util/writers-test.py,0,"b'from __future__ import division\nimport os\nimport unittest\n\nfrom kaldi.matrix import Vector, Matrix, SubMatrix, SubVector\nfrom kaldi.util import *\n\nfrom .mixins import *\n\n# TODO (VM):\n# This tests make use of the filesystem\n# However at the moment I am assuming a\n# Unix architecture (e.g., writing to /tmp).\n# How to do a multi-platform setup?\nclass _TestWriters(AuxMixin):\n\n    def test__init__(self):\n        writer = self.getImpl() # call factory method\n        self.assertIsNotNone(writer)\n        self.assertFalse(writer.is_open())\n\n        with self.assertRaises(Exception):\n            writer.close()\n\n        writer = self.getImpl(self.rspecifier)\n        self.assertIsNotNone(writer)\n        self.assertTrue(writer.is_open())\n        self.assertTrue(writer.close())\n\n        with self.assertRaises(RuntimeError):\n            writer.close()\n\n        # Check that the file exists after closing the writer\n        self.assertTrue(os.path.exists(self.filename))\n\n    def testContextManager(self):\n        obj = self.getExampleObj()\n        with self.getImpl(self.rspecifier) as writer:\n            for o in obj:\n                writer.write(""myobj"", o)\n\n        # Check writer is closed\n        self.assertFalse(writer.is_open())\n\n        # Check that the file exists after closing the writer\n        self.assertTrue(os.path.exists(self.filename))\n\n    def test__setitem__(self):\n        obj = self.getExampleObj()\n        with self.getImpl(self.rspecifier) as writer:\n            for o in obj:\n                writer[""myobj""] = o\n\n        # Check writer is closed\n        self.assertFalse(writer.is_open())\n\n        # Check that the file exists after closing the writer\n        self.assertTrue(os.path.exists(self.filename))\n\nclass TestVectorWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [Vector([1, 2, 3, 4, 5]),\n                SubVector(Vector([1, 2, 3, 4, 5]))]\n\nclass TestMatrixWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [Matrix([[3, 5], [7, 11]]),\n                SubMatrix(Matrix([[3, 5], [7, 11]]))]\n\nclass TestIntWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [3]\n\nclass TestFloatWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [5.0]\n\nclass TestBoolWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [True]\n\nclass TestIntVectorWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [[5, 6, 7, 8]]\n\nclass TestIntVectorVectorWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [[[5, 6, 7, 8], [9, 10, 11]]]\n\nclass TestIntPairVectorWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [[(1, 2), (3, 4), (5, 6)]]\n\nclass TestFloatPairVectorWriter(_TestWriters, unittest.TestCase):\n    def getExampleObj(self):\n        return [[(1.0, 2.0), (3.0, 4.0), (5.0, 6.0)]]\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
examples/scripts/alignment/gmm-aligner.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.alignment import GmmAligner\nfrom kaldi.fstext import SymbolTable\nfrom kaldi.lat.align import WordBoundaryInfoNewOpts, WordBoundaryInfo\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct aligner\naligner = GmmAligner.from_files(""gmm-boost-silence --boost=1.0 1 final.mdl - |"",\n                                ""tree"", ""L.fst"", ""words.txt"", ""disambig.int"",\n                                self_loop_scale=0.1)\nphones = SymbolTable.read_text(""phones.txt"")\nwb_info = WordBoundaryInfo.from_file(WordBoundaryInfoNewOpts(),\n                                     ""word_boundary.int"")\n\n# Define feature pipeline as a Kaldi rspecifier\nfeats_rspecifier = (\n    ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:-""\n    "" | apply-cmvn-sliding --cmn-window=10000 --center=true ark:- ark:-""\n    "" | add-deltas ark:- ark:- |""\n    )\n\n# Align\nwith SequentialMatrixReader(feats_rspecifier) as f, open(""text"") as t:\n    for (fkey, feats), line in zip(f, t):\n        tkey, text = line.strip().split(None, 1)\n        assert(fkey == tkey)\n        out = aligner.align(feats, text)\n        print(fkey, out[""alignment""], flush=True)\n        phone_alignment = aligner.to_phone_alignment(out[""alignment""], phones)\n        print(fkey, phone_alignment, flush=True)\n        word_alignment = aligner.to_word_alignment(out[""best_path""], wb_info)\n        print(fkey, word_alignment, flush=True)\n'"
examples/scripts/alignment/nnet3-aligner.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.alignment import NnetAligner\nfrom kaldi.fstext import SymbolTable\nfrom kaldi.lat.align import WordBoundaryInfoNewOpts, WordBoundaryInfo\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct aligner\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\naligner = NnetAligner.from_files(""final.mdl"", ""tree"", ""L.fst"", ""words.txt"",\n                                 ""disambig.int"", decodable_opts=decodable_opts)\nphones = SymbolTable.read_text(""phones.txt"")\nwb_info = WordBoundaryInfo.from_file(WordBoundaryInfoNewOpts(),\n                                     ""word_boundary.int"")\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:- |""\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:-""\n    "" | ivector-extract-online2 --config=ivector.conf ark:spk2utt ark:- ark:- |""\n    )\n\n# Align wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i, open(""text"") as t:\n    for (fkey, feats), (ikey, ivectors), line in zip(f, i, t):\n        tkey, text = line.strip().split(None, 1)\n        assert(fkey == ikey == tkey)\n        out = aligner.align((feats, ivectors), text)\n        print(fkey, out[""alignment""], flush=True)\n        phone_alignment = aligner.to_phone_alignment(out[""alignment""], phones)\n        print(fkey, phone_alignment, flush=True)\n        word_alignment = aligner.to_word_alignment(out[""best_path""], wb_info)\n        print(fkey, word_alignment, flush=True)\n'"
examples/scripts/asr/gmm-recognizer.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import GmmLatticeFasterRecognizer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.feat.mfcc import Mfcc, MfccOptions\nfrom kaldi.feat.functions import compute_deltas, DeltaFeaturesOptions\nfrom kaldi.feat.window import FrameExtractionOptions\nfrom kaldi.transform.cmvn import Cmvn\nfrom kaldi.util.table import SequentialMatrixReader, SequentialWaveReader\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 11.0\ndecoder_opts.max_active = 7000\nasr = GmmLatticeFasterRecognizer.from_files(\n    ""final.mdl"", ""HCLG.fst"", ""words.txt"", decoder_opts=decoder_opts)\n\n# Define feature pipeline as a Kaldi rspecifier\nfeats_rspecifier = (\n    ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:-""\n    "" | apply-cmvn-sliding --cmn-window=10000 --center=true ark:- ark:-""\n    "" | add-deltas ark:- ark:- |""\n    )\n\n# Decode\nfor key, feats in SequentialMatrixReader(feats_rspecifier):\n    out = asr.decode(feats)\n    print(key, out[""text""], flush=True)\n\nprint(""-"" * 80, flush=True)\n\n# Define feature pipeline in code\ndef make_feat_pipeline(base, opts=DeltaFeaturesOptions()):\n    def feat_pipeline(wav):\n        feats = base.compute_features(wav.data()[0], wav.samp_freq, 1.0)\n        cmvn = Cmvn(base.dim())\n        cmvn.accumulate(feats)\n        cmvn.apply(feats)\n        return compute_deltas(opts, feats)\n    return feat_pipeline\n\nframe_opts = FrameExtractionOptions()\nframe_opts.samp_freq = 16000\nframe_opts.allow_downsample = True\nmfcc_opts = MfccOptions()\nmfcc_opts.use_energy = False\nmfcc_opts.frame_opts = frame_opts\nfeat_pipeline = make_feat_pipeline(Mfcc(mfcc_opts))\n\n# Decode\nfor key, wav in SequentialWaveReader(""scp:wav.scp""):\n    feats = feat_pipeline(wav)\n    out = asr.decode(feats)\n    print(key, out[""text""], flush=True)\n'"
examples/scripts/asr/mapped-loglikes-recognizer.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import MappedLatticeFasterRecognizer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.itf import DecodableInterface\nfrom kaldi.matrix import Matrix\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 13\ndecoder_opts.max_active = 7000\nasr = MappedLatticeFasterRecognizer.from_files(\n    ""final.mdl"", ""HCLG.fst"", ""words.txt"",\n    acoustic_scale=1.0, decoder_opts=decoder_opts)\n\n# Decode log-likelihoods stored as kaldi matrices.\nwith SequentialMatrixReader(""ark:loglikes.ark"") as l:\n    for key, loglikes in l:\n        out = asr.decode(loglikes)\n        print(key, out[""text""], flush=True)\n\n# Decode log-likelihoods represented as numpy ndarrays.\n# Useful for decoding with non-kaldi acoustic models.\nmodel = lambda x: x\nwith SequentialMatrixReader(""ark:loglikes.ark"") as l:\n    for key, feats in l:\n        loglikes = model(feats.numpy())\n        out = asr.decode(Matrix(loglikes))\n        print(key, out[""text""], flush=True)\n'"
examples/scripts/asr/nnet3-batch-recognizer.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import NnetLatticeFasterBatchRecognizer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.nnet3 import NnetBatchComputerOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\nfrom kaldi.cudamatrix import cuda_available\nif cuda_available():\n    from kaldi.cudamatrix import CuDevice\n    CuDevice.instantiate().select_gpu_id(\'yes\')\n    CuDevice.instantiate().allow_multithreading()\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 13\ndecoder_opts.max_active = 7000\ncompute_opts = NnetBatchComputerOptions()\ncompute_opts.acoustic_scale = 1.0\ncompute_opts.frame_subsampling_factor = 3\ncompute_opts.frames_per_chunk = 150\nasr = NnetLatticeFasterBatchRecognizer.from_files(\n    ""final.mdl"", ""HCLG.fst"", ""words.txt"",\n    decoder_opts=decoder_opts, compute_opts=compute_opts, num_threads=4)\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:- |""\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:-""\n    "" | ivector-extract-online2 --config=ivector.conf ark:spk2utt ark:- ark:- |""\n    )\n\n# Decode wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i:\n    for (fkey, feats), (ikey, ivectors) in zip(f, i):\n        assert(fkey == ikey)\n        asr.accept_input(fkey, (feats, ivectors))\n        for out in asr.get_outputs():\n            print(out[""key""], out[""text""], flush=True)\n    asr.finished()\n    for out in asr.get_outputs():\n        print(out[""key""], out[""text""], flush=True)\n'"
examples/scripts/asr/nnet3-grammar-recognizer.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import NnetLatticeFasterGrammarRecognizer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 13\ndecoder_opts.max_active = 7000\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 150\nasr = NnetLatticeFasterGrammarRecognizer.from_files(\n    ""final.mdl"", ""HCLG.grammar.fst"", ""words.txt"",\n    decoder_opts=decoder_opts, decodable_opts=decodable_opts)\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:- |""\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:-""\n    "" | ivector-extract-online2 --config=ivector.conf ark:spk2utt ark:- ark:- |""\n    )\n\n# Decode wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i:\n    for (fkey, feats), (ikey, ivectors) in zip(f, i):\n        assert(fkey == ikey)\n        out = asr.decode((feats, ivectors))\n        print(fkey, out[""text""], flush=True)\n'"
examples/scripts/asr/nnet3-keep-loglikes.py,0,"b'#!/usr/bin/env python\n\n## This script is very similar to the second part in ./nnet3-online-recognizer.py,\n## but it has additional code to extract the log_likelihoods from the nnet\n## during decoding.  Instead of dumping to stdout, the numpy arrays could be saved\n## to disc for later recognition using a script similar to ./mapped-loglikes-recognizer.py. \n\nfrom __future__ import print_function\n\nimport numpy\n\nfrom kaldi.asr import NnetLatticeFasterOnlineRecognizer, MappedLatticeFasterRecognizer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.nnet3 import NnetSimpleLoopedComputationOptions\nfrom kaldi.online2 import (OnlineEndpointConfig,\n                           OnlineIvectorExtractorAdaptationState,\n                           OnlineNnetFeaturePipelineConfig,\n                           OnlineNnetFeaturePipelineInfo,\n                           OnlineNnetFeaturePipeline,\n                           OnlineSilenceWeighting)\nfrom kaldi.util.options import ParseOptions\nfrom kaldi.util.table import SequentialWaveReader, MatrixWriter, SequentialMatrixReader\n\nchunk_size = 1440\n\n# Define online feature pipeline\nfeat_opts = OnlineNnetFeaturePipelineConfig()\nendpoint_opts = OnlineEndpointConfig()\npo = ParseOptions("""")\nfeat_opts.register(po)\nendpoint_opts.register(po)\npo.read_config_file(""online.conf"")\nfeat_info = OnlineNnetFeaturePipelineInfo.from_config(feat_opts)\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 23\ndecoder_opts.max_active = 7000\ndecodable_opts = NnetSimpleLoopedComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 50 ## smallish to force many updates\nasr = NnetLatticeFasterOnlineRecognizer.from_files(\n    ""final.mdl"", ""HCLG.fst"", ""words.txt"",\n    decoder_opts=decoder_opts,\n    decodable_opts=decodable_opts,\n    endpoint_opts=endpoint_opts)\n\n# Decode (chunked + partial output + log_likelihoods)\nwith MatrixWriter(""ark:loglikes.ark"") as llout:\n    for key, wav in SequentialWaveReader(""scp:wav.scp""):\n        feat_pipeline = OnlineNnetFeaturePipeline(feat_info)\n        asr.set_input_pipeline(feat_pipeline)\n        d = asr._decodable\n        asr.init_decoding()\n        data = wav.data()[0]\n        last_chunk = False\n        part = 1\n        prev_num_frames_decoded = 0\n        prev_num_frames_computed = 0\n        llhs = list()\n        for i in range(0, len(data), chunk_size):\n            if i + chunk_size >= len(data):\n                last_chunk = True\n            feat_pipeline.accept_waveform(wav.samp_freq, data[i:i + chunk_size])\n            if last_chunk:\n                feat_pipeline.input_finished()\n            nr = d.num_frames_ready()\n            if nr > prev_num_frames_computed:\n                x = d.log_likelihoods(prev_num_frames_computed, nr - prev_num_frames_computed).numpy()\n                llhs.append(x)\n                prev_num_frames_computed = nr\n            asr.advance_decoding()\n            num_frames_decoded = asr.decoder.num_frames_decoded()\n            if not last_chunk:\n                if num_frames_decoded > prev_num_frames_decoded:\n                    prev_num_frames_decoded = num_frames_decoded\n                    out = asr.get_partial_output()\n                    print(key + ""-part%d"" % part, out[""text""], flush=True)\n                    part += 1\n        asr.finalize_decoding()\n        out = asr.get_output()\n        print(key + ""-final"", out[""text""], flush=True)\n\n        llout[key] = numpy.concatenate(llhs, axis=0)\n\n# Do it again, Sam, but perhaps with a different HCLG.fst\n\n# Decode log-likelihoods stored as kaldi matrices.\nasr = MappedLatticeFasterRecognizer.from_files(\n    ""final.mdl"", ""HCLG.fst"", ""words.txt"",\n    acoustic_scale=1.0, decoder_opts=decoder_opts)\n\nwith SequentialMatrixReader(""ark:loglikes.ark"") as llin:\n    for key, loglikes in llin:\n        out = asr.decode(loglikes)\n        print(key + \'-fromllhs\', out[""text""], flush=True)\n\n\n'"
examples/scripts/asr/nnet3-online-recognizer.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import NnetLatticeFasterOnlineRecognizer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.nnet3 import NnetSimpleLoopedComputationOptions\nfrom kaldi.online2 import (OnlineEndpointConfig,\n                           OnlineIvectorExtractorAdaptationState,\n                           OnlineNnetFeaturePipelineConfig,\n                           OnlineNnetFeaturePipelineInfo,\n                           OnlineNnetFeaturePipeline,\n                           OnlineSilenceWeighting)\nfrom kaldi.util.options import ParseOptions\nfrom kaldi.util.table import SequentialWaveReader\n\nchunk_size = 1440\n\n# Define online feature pipeline\nfeat_opts = OnlineNnetFeaturePipelineConfig()\nendpoint_opts = OnlineEndpointConfig()\npo = ParseOptions("""")\nfeat_opts.register(po)\nendpoint_opts.register(po)\npo.read_config_file(""online.conf"")\nfeat_info = OnlineNnetFeaturePipelineInfo.from_config(feat_opts)\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 13\ndecoder_opts.max_active = 7000\ndecodable_opts = NnetSimpleLoopedComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 150\nasr = NnetLatticeFasterOnlineRecognizer.from_files(\n    ""final.mdl"", ""HCLG.fst"", ""words.txt"",\n    decoder_opts=decoder_opts,\n    decodable_opts=decodable_opts,\n    endpoint_opts=endpoint_opts)\n\n# Decode (whole utterance)\nfor key, wav in SequentialWaveReader(""scp:wav.scp""):\n    feat_pipeline = OnlineNnetFeaturePipeline(feat_info)\n    asr.set_input_pipeline(feat_pipeline)\n    feat_pipeline.accept_waveform(wav.samp_freq, wav.data()[0])\n    feat_pipeline.input_finished()\n    out = asr.decode()\n    print(key, out[""text""], flush=True)\n\n# Decode (chunked + partial output)\nfor key, wav in SequentialWaveReader(""scp:wav.scp""):\n    feat_pipeline = OnlineNnetFeaturePipeline(feat_info)\n    asr.set_input_pipeline(feat_pipeline)\n    asr.init_decoding()\n    data = wav.data()[0]\n    last_chunk = False\n    part = 1\n    prev_num_frames_decoded = 0\n    for i in range(0, len(data), chunk_size):\n        if i + chunk_size >= len(data):\n            last_chunk = True\n        feat_pipeline.accept_waveform(wav.samp_freq, data[i:i + chunk_size])\n        if last_chunk:\n            feat_pipeline.input_finished()\n        asr.advance_decoding()\n        num_frames_decoded = asr.decoder.num_frames_decoded()\n        if not last_chunk:\n            if num_frames_decoded > prev_num_frames_decoded:\n                prev_num_frames_decoded = num_frames_decoded\n                out = asr.get_partial_output()\n                print(key + ""-part%d"" % part, out[""text""], flush=True)\n                part += 1\n    asr.finalize_decoding()\n    out = asr.get_output()\n    print(key + ""-final"", out[""text""], flush=True)\n\n# Decode (chunked + partial output + endpointing\n#         + ivector adaptation + silence weighting)\nadaptation_state = OnlineIvectorExtractorAdaptationState.from_info(\n    feat_info.ivector_extractor_info)\nfor key, wav in SequentialWaveReader(""scp:wav.scp""):\n    feat_pipeline = OnlineNnetFeaturePipeline(feat_info)\n    feat_pipeline.set_adaptation_state(adaptation_state)\n    asr.set_input_pipeline(feat_pipeline)\n    asr.init_decoding()\n    sil_weighting = OnlineSilenceWeighting(\n        asr.transition_model, feat_info.silence_weighting_config,\n        decodable_opts.frame_subsampling_factor)\n    data = wav.data()[0]\n    last_chunk = False\n    utt, part = 1, 1\n    prev_num_frames_decoded, offset = 0, 0\n    for i in range(0, len(data), chunk_size):\n        if i + chunk_size >= len(data):\n            last_chunk = True\n        feat_pipeline.accept_waveform(wav.samp_freq, data[i:i + chunk_size])\n        if last_chunk:\n            feat_pipeline.input_finished()\n        if sil_weighting.active():\n            sil_weighting.compute_current_traceback(asr.decoder)\n            feat_pipeline.ivector_feature().update_frame_weights(\n                sil_weighting.get_delta_weights(\n                    feat_pipeline.num_frames_ready()))\n        asr.advance_decoding()\n        num_frames_decoded = asr.decoder.num_frames_decoded()\n        if not last_chunk:\n            if asr.endpoint_detected():\n                asr.finalize_decoding()\n                out = asr.get_output()\n                print(key + ""-utt%d-final"" % utt, out[""text""], flush=True)\n                offset += int(num_frames_decoded\n                              * decodable_opts.frame_subsampling_factor\n                              * feat_pipeline.frame_shift_in_seconds()\n                              * wav.samp_freq)\n                feat_pipeline.get_adaptation_state(adaptation_state)\n                feat_pipeline = OnlineNnetFeaturePipeline(feat_info)\n                feat_pipeline.set_adaptation_state(adaptation_state)\n                asr.set_input_pipeline(feat_pipeline)\n                asr.init_decoding()\n                sil_weighting = OnlineSilenceWeighting(\n                    asr.transition_model, feat_info.silence_weighting_config,\n                    decodable_opts.frame_subsampling_factor)\n                remainder = data[offset:i + chunk_size]\n                feat_pipeline.accept_waveform(wav.samp_freq, remainder)\n                utt += 1\n                part = 1\n                prev_num_frames_decoded = 0\n            elif num_frames_decoded > prev_num_frames_decoded:\n                prev_num_frames_decoded = num_frames_decoded\n                out = asr.get_partial_output()\n                print(key + ""-utt%d-part%d"" % (utt, part),\n                      out[""text""], flush=True)\n                part += 1\n    asr.finalize_decoding()\n    out = asr.get_output()\n    print(key + ""-utt%d-final"" % utt, out[""text""], flush=True)\n    feat_pipeline.get_adaptation_state(adaptation_state)\n'"
examples/scripts/asr/nnet3-recognizer.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import NnetLatticeFasterRecognizer, LatticeLmRescorer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.fstext import SymbolTable, shortestpath, indices_to_symbols\nfrom kaldi.fstext.utils import get_linear_symbol_sequence\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 13\ndecoder_opts.max_active = 7000\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 150\nasr = NnetLatticeFasterRecognizer.from_files(\n    ""final.mdl"", ""HCLG.fst"",\n    decoder_opts=decoder_opts, decodable_opts=decodable_opts)\n\n# Construct symbol table\nsymbols = SymbolTable.read_text(""words.txt"")\nphi_label = symbols.find_index(""#0"")\n\n# Construct LM rescorer\nrescorer = LatticeLmRescorer.from_files(""G.fst"", ""G.rescore.fst"", phi_label)\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:- |""\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:-""\n    "" | ivector-extract-online2 --config=ivector.conf ark:spk2utt ark:- ark:- |""\n    )\n\n# Decode wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i:\n    for (fkey, feats), (ikey, ivectors) in zip(f, i):\n        assert(fkey == ikey)\n        out = asr.decode((feats, ivectors))\n        rescored_lat = rescorer.rescore(out[""lattice""])\n        words, _, _ = get_linear_symbol_sequence(shortestpath(rescored_lat))\n        print(fkey, "" "".join(indices_to_symbols(symbols, words)), flush=True)\n'"
examples/scripts/asr/nnet3-rnnlm-recognizer.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import NnetLatticeFasterRecognizer, LatticeRnnlmPrunedRescorer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.fstext import SymbolTable, shortestpath, indices_to_symbols\nfrom kaldi.fstext.utils import get_linear_symbol_sequence\nfrom kaldi.lat.functions import ComposeLatticePrunedOptions\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.rnnlm import RnnlmComputeStateComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 13\ndecoder_opts.max_active = 7000\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 150\nasr = NnetLatticeFasterRecognizer.from_files(""final.mdl"", ""HCLG.fst"",\n                                             decoder_opts=decoder_opts,\n                                             decodable_opts=decodable_opts)\n\n# Construct RNNLM rescorer\nsymbols = SymbolTable.read_text(""lm/words.txt"")\nrnnlm_opts = RnnlmComputeStateComputationOptions()\nrnnlm_opts.bos_index = symbols.find_index(""<s>"")\nrnnlm_opts.eos_index = symbols.find_index(""</s>"")\nrnnlm_opts.brk_index = symbols.find_index(""<brk>"")\ncompose_opts = ComposeLatticePrunedOptions()\ncompose_opts.lattice_compose_beam = 4\nrescorer = LatticeRnnlmPrunedRescorer.from_files(\n    ""lm/G.carpa"",\n    ""rnnlm-get-word-embedding lm/word_feats.txt lm/feat_embedding.final.mat -|"",\n    ""lm/final.raw"", acoustic_scale=1.0, max_ngram_order=4, use_const_arpa=True,\n    opts=rnnlm_opts, compose_opts=compose_opts)\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:- |""\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:-""\n    "" | ivector-extract-online2 --config=ivector.conf ark:spk2utt ark:- ark:- |""\n    )\n\n# Decode wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i:\n    for (fkey, feats), (ikey, ivectors) in zip(f, i):\n        assert(fkey == ikey)\n        out = asr.decode((feats, ivectors))\n        rescored_lat = rescorer.rescore(out[""lattice""])\n        words, _, _ = get_linear_symbol_sequence(shortestpath(rescored_lat))\n        print(fkey, "" "".join(indices_to_symbols(symbols, words)), flush=True)\n'"
examples/scripts/segmentation/nnet3-segmenter.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.segmentation import NnetSAD, SegmentationProcessor\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct SAD\nmodel = NnetSAD.read_model(""final.raw"")\npost = NnetSAD.read_average_posteriors(""post_output.vec"")\ntransform = NnetSAD.make_sad_transform(post)\ngraph = NnetSAD.make_sad_graph()\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.extra_left_context = 79\ndecodable_opts.extra_right_context = 21\ndecodable_opts.extra_left_context_initial = 0\ndecodable_opts.extra_right_context_final = 0\ndecodable_opts.frames_per_chunk = 150\ndecodable_opts.acoustic_scale = 0.3\nsad = NnetSAD(model, transform, graph, decodable_opts=decodable_opts)\nseg = SegmentationProcessor(target_labels=[2])\n\n# Define feature pipeline as a Kaldi rspecifier\nfeats_rspec = ""ark:compute-mfcc-feats --config=mfcc.conf scp:wav.scp ark:- |""\n\n# Segment\nwith SequentialMatrixReader(feats_rspec) as f, open (""segments"", ""w"") as s:\n    for key, feats in f:\n        out = sad.segment(feats)\n        segments, stats = seg.process(out[""alignment""])\n        seg.write(key, segments, s)\n        print(""segments:"", segments, flush=True)\n        print(""stats:"", stats, flush=True)\nprint(""global stats:"", seg.stats, flush=True)\n'"
examples/setups/aspire/align.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.alignment import NnetAligner\nfrom kaldi.fstext import SymbolTable\nfrom kaldi.lat.align import WordBoundaryInfoNewOpts, WordBoundaryInfo\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct aligner\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 150\naligner = NnetAligner.from_files(\n    ""exp/tdnn_7b_chain_online/final.mdl"",\n    ""exp/tdnn_7b_chain_online/tree"",\n    ""data/lang/L.fst"",\n    ""data/lang/words.txt"",\n    ""data/lang/phones/disambig.int"",\n    decodable_opts=decodable_opts)\nphones = SymbolTable.read_text(""data/lang/phones.txt"")\nwb_info = WordBoundaryInfo.from_file(WordBoundaryInfoNewOpts(),\n                                     ""data/lang/phones/word_boundary.int"")\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = (\n    ""ark:compute-mfcc-feats --config=conf/mfcc_hires.conf scp:data/test/wav.scp ark:- |""\n)\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=conf/mfcc_hires.conf scp:data/test/wav.scp ark:- |""\n    ""ivector-extract-online2 --config=conf/ivector_extractor.conf ark:data/test/spk2utt ark:- ark:- |""\n)\n\n# Align wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i, \\\n     open(""data/test/text"") as t, \\\n     open(""out/test/align.out"", ""w"") as a, \\\n     open(""out/test/phone_align.out"", ""w"") as p, \\\n     open(""out/test/word_align.out"", ""w"") as w:\n    for (fkey, feats), (ikey, ivectors), line in zip(f, i, t):\n        tkey, text = line.strip().split(None, 1)\n        assert(fkey == ikey == tkey)\n        out = aligner.align((feats, ivectors), text)\n        print(fkey, out[""alignment""], file=a)\n        phone_alignment = aligner.to_phone_alignment(out[""alignment""], phones)\n        print(fkey, phone_alignment, file=p)\n        word_alignment = aligner.to_word_alignment(out[""best_path""], wb_info)\n        print(fkey, word_alignment, file=w)\n'"
examples/setups/aspire/decode.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import NnetLatticeFasterRecognizer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 13\ndecoder_opts.max_active = 7000\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 150\nasr = NnetLatticeFasterRecognizer.from_files(\n    ""exp/tdnn_7b_chain_online/final.mdl"",\n    ""exp/tdnn_7b_chain_online/graph_pp/HCLG.fst"",\n    ""data/lang/words.txt"",\n    decoder_opts=decoder_opts,\n    decodable_opts=decodable_opts)\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = (\n    ""ark:compute-mfcc-feats --config=conf/mfcc_hires.conf scp:data/test/wav.scp ark:- |""\n)\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=conf/mfcc_hires.conf scp:data/test/wav.scp ark:- |""\n    ""ivector-extract-online2 --config=conf/ivector_extractor.conf ark:data/test/spk2utt ark:- ark:- |""\n)\n\n# Decode wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i, \\\n     open(""out/test/decode.out"", ""w"") as o:\n    for (key, feats), (_, ivectors) in zip(f, i):\n        out = asr.decode((feats, ivectors))\n        print(key, out[""text""], file=o)\n'"
examples/setups/ltsv-based-vad/ARMA.py,1,"b'from __future__ import division\nimport numpy as np\n\ndef ApplyARMA(in_matrix, order):\n  """"""This function applies Autoregressive Moving Averarge(ARMA)\n     on the rows of the input matrix.\n     \n  Args:\n      in_matrix: the input matrix\n      order: the order of the ARMA process\n\n  Returns:\n      out_matrix: the matrix after the ARMA process\n  """"""\n\n  nframes, nfeats = in_matrix.shape\n  out_matrix = np.copy(in_matrix)\n\n  for c in range(nfeats):\n    tmp1 = 0\n    tmp2 = 0\n    for r in range(nframes - order):\n      if r < order:\n        out_matrix[r, c] = 0.01 * in_matrix[r, c]\n      elif r == order:\n        for cnt in range(order):\n          tmp1 = tmp1 + out_matrix[r-1-cnt, c]\n          tmp2 = tmp2 + in_matrix[r+cnt, c]\n        tmp2 = tmp2 + in_matrix[r+order ,c]\n        out_matrix[r, c] = (tmp1+tmp2)/(2*order + 1)\n      else:\n        tmp1 = tmp1 + out_matrix[r-1, c] - out_matrix[r-1-order, c]\n        tmp2 = tmp2 + in_matrix[r+order, c] - in_matrix[r-1, c]\n        out_matrix[r, c] = (tmp1+tmp2)/(2*order+1)\n  \n  return out_matrix\n'"
examples/setups/ltsv-based-vad/DCTF.py,3,"b'from __future__ import division\nimport numpy as np\nfrom scipy.io import wavfile\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlb\nfrom kaldi.matrix.functions import compute_dct_matrix\nfrom kaldi.matrix import Matrix\n\n\ndef DCTFCompute(input_s, input_dct_matrix, ctx_win, num_ceps):\n  """"""This function computes the Discrete Cosine Transform of a signal.\n\n  Args:\n      input_s: the input signal\n      input_dct_matrix: The DCT matrix\n      ctx_win: the context window\n      num_ceps: the number of coefficients\n          \n  Returns:\n      dctf_output: The transformed singal\n  """"""\n  input_row_n = input_s.shape[0]\n  fr_len = ctx_win\n  fr_sh = 1;\n\n  num_samples = input_s.shape[0]\n  n_fr = 1 + np.floor( (num_samples-fr_len)/fr_sh ).astype(\'int\')\n  \n  transformed_input = np.zeros((n_fr, num_ceps))\n\n  for r_ind in range(n_fr):\n    start = r_ind*fr_sh\n    end = start+fr_len  \n    window = input_s[start:end]\n    transformed_input[r_ind,:] = input_dct_matrix.dot(window)\n  \n  dctf_output = np.zeros((input_row_n, num_ceps))\n  dctf_output[0:n_fr,:] = transformed_input\n  \n  for ctx_index in range(ctx_win-1):\n    dctf_output[n_fr+ctx_index,:] = transformed_input[-1,:]\n  return dctf_output\n\n\ndef ApplyDCT(num_cep, context_window, feature):\n  """"""This function applies the Discrete Cosine Transform to a feature.\n\n  Args:\n      num_cep: the number of DCT coefficients.\n      context_window: window over which we will calculate the DCT. \n      feature: the input feature\n          \n  Returns:\n      ltsv: The LTSV features\n  """"""\n  dct_matrix_full = Matrix(context_window, context_window)\n  compute_dct_matrix(dct_matrix_full)\n  dct_matrix_full = dct_matrix_full.numpy()\n  dct_matrix = dct_matrix_full[0:num_cep,:]\n \n  final_out = DCTFCompute(feature, dct_matrix, context_window, num_cep)\n  final_out = final_out[:,0];\n \n  return final_out\n\n'"
examples/setups/ltsv-based-vad/LTSV.py,14,"b'from __future__ import division\nimport numpy as np\nfrom scipy.io import wavfile\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlb\n\ndef ApplySigmoidScale (ltsv_sigmoidThr, ltsv_sigmoidSlope, ltsv_input):\n  """"""This function applies sigmoid scale on the input\n  \n  Args:\n      ltsv_sigmoidThr: the threshold of the sigmoid\n      ltsv_sigmoidSlople: the slope of the sigmoid\n      ltsv_input: the input vector\n\n  Returns:\n      ltsv_input: the transformed input vector\n  \n  """"""\n  ltsv_input = 1/( np.exp( (-1/ltsv_sigmoidSlope)*(ltsv_input-ltsv_sigmoidThr) )+1 )\n  return ltsv_input\n\n\ndef ApplyLTSV(spec, ctx_win, sigThresh, sigSlope, sigmoidscale):\n  """"""This function computes the long term signal variability(LTSV)\n  from a spectrogram.\n\n  Args:\n      spec: the input spectrogram\n      ctx_win: the context window over which we will compute LTSV\n      sigThresh: parameter for sigmoid scaling\n      sigSlope: parameter for sigmoid scaling\n      sigmoidscale: Flag that determines if sigmoid scaling is going to be applied\n          \n  Returns:\n      ltsv: The LTSV features\n  """"""\n  nframes, nfeats = spec.shape\n  if nframes < ctx_win+1:\n    ctx_win = num_frames-1\n  \n  featsin = np.zeros((nframes+ctx_win, nfeats))\n  featsin[0:nframes, :] = spec\n  featsin[nframes:, :] = spec[nframes-ctx_win:,:]\n  \n  ltsv = np.zeros(nframes)\n  ltsv_bins = np.zeros([ctx_win,nfeats])\n  ltsv_bins_log = np.zeros([ctx_win,nfeats])\n  entropy_vec = np.zeros(nfeats)\n\n  ltsv_val=0\n  \n  for k in range(nframes):\n    if k < int(round(ctx_win/2)):\n      ltsv_bins[0:int(round(ctx_win/2))-k,:] = np.array([featsin[0,:],] * int(round(ctx_win/2)-k))\n      ltsv_bins[int(round(ctx_win/2))-1-k:ctx_win,:] = featsin[0:int(round(ctx_win/2))+k+1,:]\n    else:\n      ltsv_bins = featsin[k-int(round(ctx_win/2))+1:k+int(round(ctx_win/2))+1,:];\n  \n    # this should never happen after ARMA\n    if np.any(ltsv_bins[ltsv_bins<0]): \n      ltsv_bins[ltsv_bins<0] = 1/100 \n\n    moving_context = np.sum(ltsv_bins,axis=0)\n    ltsv_bins = (ltsv_bins / moving_context[None,:])\n\n    # entropy\n    ltsv_bins_log = np.log(ltsv_bins)\n    ltsv_bins =  ltsv_bins*ltsv_bins_log*(-1) \n    entropy_vec = np.sum(ltsv_bins,axis=0)\n  \n    #variance\n    entropy_vec = entropy_vec - (np.sum(entropy_vec)/nfeats)\n    entropy_vec = np.power(entropy_vec, 2) \n \n    if k  < nframes - int(round(ctx_win/2)):\n      ltsv_val = np.sum(entropy_vec)/nfeats\n    ltsv[k] = ltsv_val\n  \n  if sigmoidscale:\n    ltsv = ApplySigmoidScale(sigThresh, sigSlope, ltsv)\n\n  return ltsv\n\n'"
examples/setups/ltsv-based-vad/compute-vad.py,6,"b'# LTSV features for VAD\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport sys\n\nimport numpy as np\nfrom scipy import signal\nfrom scipy import version\n\nfrom kaldi.feat.window import FrameExtractionOptions\nfrom kaldi.matrix import Vector\nfrom kaldi.util.options import ParseOptions\nfrom kaldi.util.table import (\n    MatrixWriter,\n    RandomAccessFloatReaderMapped,\n    SequentialWaveReader,\n    VectorWriter,\n)\n\nimport ARMA, LTSV, DCTF\n\n\ndef show_plot(\n    key, segment_times, sample_freqs, spec, duration, wav_data, vad_feat\n):\n    """"""This function plots the vad against the signal and the spectrogram.\n\n    Args:\n        segment_times: the time intervals acting as the x axis\n        sample_freqs: the frequency bins acting as the y axis\n        spec: the spectrogram\n        duration: duration of the wave file\n        wav_data: the wave data\n        vad_feat: VAD features\n    """"""\n\n    import matplotlib.pyplot as plt\n    import matplotlib.mlab as mlb\n\n    plt.subplot(3, 1, 1)\n    plt.pcolormesh(segment_times, sample_freqs, 10 * np.log10(spec), cmap=""jet"")\n    plt.ylabel(""Frequency [Hz]"")\n    plt.xlabel(""Time [sec]"")\n\n    plt.subplot(3, 1, 2)\n    axes = plt.gca()\n    axes.set_xlim([0, duration])\n    tmp_axis = np.linspace(0, duration, wav_data.shape[0])\n    plt.plot(tmp_axis, wav_data / np.abs(np.max(wav_data)))\n    plt.xlabel(""Time [sec]"")\n\n    plt.subplot(3, 1, 3)\n    axes = plt.gca()\n    axes.set_xlim([0, duration])\n    tmp_axis = np.linspace(0, duration, vad_feat.shape[0])\n    plt.plot(tmp_axis, vad_feat)\n    plt.xlabel(""Time [sec]"")\n\n    plt.savefig(""plots/"" + key, bbox_inches=""tight"")\n\n\ndef compute_vad(wav_rspecifier, feats_wspecifier, opts):\n    """"""This function computes the vad based on ltsv features.\n\n    The output is written in the file denoted by feats_wspecifier,\n    and if the test_plot flag is set, it produces a plot.\n\n    Args:\n        wav_rspecifier: Kaldi specifier for reading wav files.\n        feats_wspecifier:  Kaldi wpscifier for writing feature files.\n        opts: Options. See main function for list of options\n\n    Returns:\n        True if computation was successful for at least one file.\n        False otherwise.\n    """"""\n\n    num_utts, num_success = 0, 0\n    with SequentialWaveReader(wav_rspecifier) as reader, \\\n         VectorWriter(feats_wspecifier) as writer:\n\n        for num_utts, (key, wave) in enumerate(reader, 1):\n            if wave.duration < opts.min_duration:\n                print(\n                    ""File: {} is too short ({} sec): ""\n                    ""producing no output."".format(key, wave.duration),\n                    file=sys.stderr,\n                )\n                continue\n\n            num_chan = wave.data().num_rows\n            if opts.channel >= num_chan:\n                print(\n                    ""File with id {} has {} channels but you specified ""\n                    ""channel {}, producing no output."",\n                    file=sys.stderr,\n                )\n                continue\n\n            channel = 0 if opts.channel == -1 else opts.channel\n\n            fr_length_samples = int(\n                opts.frame_window * wave.samp_freq * (10 ** (-3))\n            )\n            fr_shift_samples = int(\n                opts.frame_shift * wave.samp_freq * (10 ** (-3))\n            )\n\n            assert opts.nfft >= fr_length_samples\n\n            wav_data = np.squeeze(wave.data()[channel].numpy())\n\n            sample_freqs, segment_times, spec = signal.spectrogram(\n                wav_data,\n                fs=wave.samp_freq,\n                nperseg=fr_length_samples,\n                nfft=opts.nfft,\n                noverlap=fr_length_samples - fr_shift_samples,\n                scaling=""spectrum"",\n                mode=""psd"",\n            )\n\n            specT = np.transpose(spec)\n\n            spect_n = ARMA.ApplyARMA(specT, opts.arma_order)\n\n            ltsv_f = LTSV.ApplyLTSV(\n                spect_n,\n                opts.ltsv_ctx_window,\n                opts.threshold,\n                opts.slope,\n                opts.sigmoid_scale,\n            )\n\n            vad_feat = DCTF.ApplyDCT(\n                opts.dct_num_cep, opts.dct_ctx_window, ltsv_f\n            )\n\n            if opts.test_plot:\n                show_plot(\n                    key,\n                    segment_times,\n                    sample_freqs,\n                    spec,\n                    wave.duration,\n                    wav_data,\n                    vad_feat,\n                )\n\n            writer[key] = Vector(vad_feat)\n            num_success += 1\n\n            if num_utts % 10 == 0:\n                print(\n                    ""Processed {} utterances"".format(num_utts), file=sys.stderr\n                )\n\n    print(\n        ""Done {} out of {} utterances"".format(num_success, num_utts),\n        file=sys.stderr,\n    )\n\n    return num_success != 0\n\n\nif __name__ == ""__main__"":\n\n    usage = """"""Compute VAD.\n\n    Usage:  compute-vad [options...] <wav-rspecifier> <feats-wspecifier>\n    """"""\n\n    po = ParseOptions(usage)\n\n    po.register_float(\n        ""min-duration"",\n        0.0,\n        ""Minimum duration of segments to process in seconds (default: 0.0)."",\n    )\n    po.register_int(\n        ""channel"",\n        -1,\n        ""Channel to extract (-1 -> mono (default), 0 -> left, 1 -> right)"",\n    )\n    po.register_int(\n        ""frame-window"", 25, ""Length of frame window in ms (default: 25)""\n    )\n    po.register_int(\n        ""frame-shift"", 10, ""Length of frame shift in ms (default: 10)""\n    )\n    po.register_int(""nfft"", 512, ""Number of DFT points (default: 256)"")\n    po.register_int(\n        ""arma-order"",\n        5,\n        ""Length of ARMA window that will be applied to the spectrogram"",\n    )\n    po.register_int(\n        ""ltsv-ctx-window"",\n        50,\n        ""Context window for LTSV computation (default: 50)"",\n    )\n    po.register_float(\n        ""threshold"",\n        0.01,\n        ""Parameter for sigmoid scaling in LTSV (default: 0.01)"",\n    )\n    po.register_float(\n        ""slope"", 0.001, ""Parameter for sigmoid scaling in LTSV (default: 0.001)""\n    )\n    po.register_bool(\n        ""sigmoid-scale"", True, ""Apply sigmoid scaling in LTSV (default: True)""\n    )\n    po.register_int(\n        ""dct-num-cep"", 5, ""DCT number of coefficitents (default: 5)""\n    )\n    po.register_int(""dct-ctx-window"", 30, ""DCT context window (default: 30)"")\n    po.register_bool(\n        ""test-plot"", False, ""Produces a plot for testing (default: False)""\n    )\n\n    opts = po.parse_args()\n\n    if po.num_args() != 2:\n        po.print_usage()\n        sys.exit()\n\n    wav_rspecifier = po.get_arg(1)\n    feats_wspecifier = po.get_arg(2)\n\n    compute_vad(wav_rspecifier, feats_wspecifier, opts)\n'"
examples/setups/zamia/align.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.alignment import NnetAligner\nfrom kaldi.fstext import SymbolTable\nfrom kaldi.lat.align import WordBoundaryInfoNewOpts, WordBoundaryInfo\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct aligner\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 150\naligner = NnetAligner.from_files(\n    ""exp/nnet3_chain/tdnn_f/final.mdl"",\n    ""exp/nnet3_chain/tdnn_f/tree"",\n    ""data/lang/L.fst"",\n    ""data/lang/words.txt"",\n    ""data/lang/phones/disambig.int"",\n    decodable_opts=decodable_opts)\nphones = SymbolTable.read_text(""data/lang/phones.txt"")\nwb_info = WordBoundaryInfo.from_file(WordBoundaryInfoNewOpts(),\n                                     ""data/lang/phones/word_boundary.int"")\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = (\n    ""ark:compute-mfcc-feats --config=conf/mfcc_hires.conf scp:data/test/wav.scp ark:- |""\n)\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=conf/mfcc_hires.conf scp:data/test/wav.scp ark:- |""\n    ""ivector-extract-online2 --config=conf/ivector_extractor.conf ark:data/test/spk2utt ark:- ark:- |""\n)\n\n# Align wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i, \\\n     open(""data/test/text"") as t, \\\n     open(""out/test/align.out"", ""w"") as a, \\\n     open(""out/test/phone_align.out"", ""w"") as p, \\\n     open(""out/test/word_align.out"", ""w"") as w:\n    for (fkey, feats), (ikey, ivectors), line in zip(f, i, t):\n        tkey, text = line.strip().split(None, 1)\n        assert(fkey == ikey == tkey)\n        out = aligner.align((feats, ivectors), text)\n        print(fkey, out[""alignment""], file=a)\n        phone_alignment = aligner.to_phone_alignment(out[""alignment""], phones)\n        print(fkey, phone_alignment, file=p)\n        word_alignment = aligner.to_word_alignment(out[""best_path""], wb_info)\n        print(fkey, word_alignment, file=w)\n'"
examples/setups/zamia/decode.py,0,"b'#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nfrom kaldi.asr import NnetLatticeFasterRecognizer\nfrom kaldi.decoder import LatticeFasterDecoderOptions\nfrom kaldi.nnet3 import NnetSimpleComputationOptions\nfrom kaldi.util.table import SequentialMatrixReader\n\n# Construct recognizer\ndecoder_opts = LatticeFasterDecoderOptions()\ndecoder_opts.beam = 13\ndecoder_opts.max_active = 7000\ndecodable_opts = NnetSimpleComputationOptions()\ndecodable_opts.acoustic_scale = 1.0\ndecodable_opts.frame_subsampling_factor = 3\ndecodable_opts.frames_per_chunk = 150\nasr = NnetLatticeFasterRecognizer.from_files(\n    ""exp/nnet3_chain/tdnn_f/final.mdl"",\n    ""exp/nnet3_chain/tdnn_f/graph/HCLG.fst"",\n    ""data/lang/words.txt"",\n    decoder_opts=decoder_opts,\n    decodable_opts=decodable_opts)\n\n# Define feature pipelines as Kaldi rspecifiers\nfeats_rspec = (\n    ""ark:compute-mfcc-feats --config=conf/mfcc_hires.conf scp:data/test/wav.scp ark:- |""\n)\nivectors_rspec = (\n    ""ark:compute-mfcc-feats --config=conf/mfcc_hires.conf scp:data/test/wav.scp ark:- |""\n    ""ivector-extract-online2 --config=conf/ivector_extractor.conf ark:data/test/spk2utt ark:- ark:- |""\n)\n\n# Decode wav files\nwith SequentialMatrixReader(feats_rspec) as f, \\\n     SequentialMatrixReader(ivectors_rspec) as i, \\\n     open(""out/test/decode.out"", ""w"") as o:\n    for (key, feats), (_, ivectors) in zip(f, i):\n        out = asr.decode((feats, ivectors))\n        print(key, out[""text""], file=o)\n'"
examples/scripts/kaldi-style-executables/compute-cmvn-stats/compute-cmvn-stats-two-channel.py,0,"b'#!/usr/bin/env python\nfrom __future__ import division, print_function\n\nimport sys\nfrom collections import defaultdict\n\nfrom kaldi.matrix import DoubleMatrix\nfrom kaldi.transform.cmvn import Cmvn  # acc_cmvn_stats, acc_cmvn_stats_single_frame\nfrom kaldi.util.io import xopen, printable_rxfilename\nfrom kaldi.util.options import ParseOptions\nfrom kaldi.util.table import RandomAccessMatrixReader, DoubleMatrixWriter\n\ndef get_utterance_pairs(reco2file_and_channel_rxfilename):\n    utt_pairs = []\n    call_to_uttlist = defaultdict(list)\n    for line in xopen(reco2file_and_channel_rxfilename, ""rt""):\n        try:\n            utt, call, _ = line.split()  # lines like: sw02001-A sw02001 A\n        except:\n            filename = printable_rxfilename(reco2file_and_channel_rxfilename)\n            raise ValueError(""Expecting 3 fields per line of ""\n                             ""reco2file_and_channel file {}, got: {}""\n                             .format(filename, len(line.split())))\n        call_to_uttlist[call].append(utt)\n    for key, uttlist in call_to_uttlist.items():\n        if len(uttlist) == 2:\n            utt_pairs.append(uttlist)\n        else:\n            print(""Call {} has {} utterances, expected two; treating them ""\n                  ""singly."".format(key, len(uttlist)), file=sys.stderr)\n            utt_pairs.extend([x] for x in uttlist)\n    return utt_pairs\n\n\ndef acc_cmvn_stats_for_pair(utt1, utt2, feats1, feats2, quieter_channel_weight):\n    assert(feats1.num_cols == feats2.num_cols)\n    cmvn1 = Cmvn(feats1.num_cols)\n    cmvn2 = Cmvn(feats2.num_cols)\n    if feats1.num_rows != feats2.num_rows:\n        print(""Number of frames differ between {} and {}: {} vs. {}, treating ""\n              ""them separately."".format(utt1, utt2,\n                                        feats1.num_rows, feats2.num_rows))\n        cmvn1.accumulate(feats1)\n        cmvn2.accumulate(feats2)\n    else:\n        for v1, v2 in zip(feats1, feats2):\n            if v1[0] > v2[0]:\n                w1, w2 = 1.0, quieter_channel_weight\n            else:\n                w1, w2 = quieter_channel_weight, 1.0\n            cmvn1.accumulate(v1, w1)\n            cmvn2.accumulate(v2, w2)\n    return cmvn1, cmvn2\n\ndef compute_cmvn_stats_two_channel(reco2file_and_channel_rxfilename,\n                                   feats_rspecifier, stats_wspecifier, opts):\n    utt_pairs = get_utterance_pairs(reco2file_and_channel_rxfilename)\n\n    num_done, num_err = 0, 0\n    with RandomAccessMatrixReader(feats_rspecifier) as feat_reader, \\\n         DoubleMatrixWriter(stats_wspecifier) as writer:\n            for pair in utt_pairs:\n                if len(pair) == 2:\n                    utt1, utt2 = pair\n                    if utt1 not in feat_reader:\n                        print(""No feature data for utterance {}"".format(utt1),\n                              file=sys.stderr)\n                        num_err += 1\n                        pair = utt2, utt1\n                        # and fall through to the singleton code below.\n                    elif utt2 not in feat_reader:\n                        print(""No feature data for utterance {}"".format(utt2),\n                              file=sys.stderr)\n                        num_err += 1\n                        # and fall through to the singleton code below.\n                    else:\n                        feats1 = feat_reader[utt1]\n                        feats2 = feat_reader[utt2]\n                        cmvn1, cmvn2 = acc_cmvn_stats_for_pair(\n                            utt1, utt2, feats1, feats2,\n                            opts.quieter_channel_weight)\n                        writer[utt1] = cmvn1.stats\n                        writer[utt2] = cmvn2.stats\n                        num_done += 2\n                        continue\n                # process singletons\n                utt = pair[0]\n                if utt not in feat_reader:\n                    print(""No feature data for utterance {}"".format(utt))\n                    num_err += 1\n                    continue\n                feats = feat_reader[utt]\n                cmvn = Cmvn(feats.num_cols)\n                cmvn.accumulate(feats)\n                writer[utt] = cmvn.stats\n                num_done += 1\n    print(""Done accumulating CMVN stats for {} utterances; {} had errors.""\n          .format(num_done, num_err), file=sys.stderr)\n    return True if num_done != 0 else False\n\nif __name__ == \'__main__\':\n    usage = """"""Compute cepstral mean and variance normalization statistics.\n\n    Specialized for two-sided telephone data where we only accumulate\n    the louder of the two channels at each frame (and add it to that\n    side\'s stats).  Reads a \'reco2file_and_channel\' file, normally like\n    sw02001-A sw02001 A\n    sw02001-B sw02001 B\n    sw02005-A sw02005 A\n    sw02005-B sw02005 B\n    interpreted as <utterance-id> <call-id> <side> and for each <call-id>\n    that has two sides, does the \'only-the-louder\' computation, else does\n    per-utterance stats in the normal way.\n    Note: loudness is judged by the first feature component, either energy or c0\n    only applicable to MFCCs or PLPs (this code could be modified to handle filterbanks).\n\n    Usage: compute-cmvn-stats-two-channel [options] <reco2file-and-channel> <feats-rspecifier> <stats-wspecifier>\n    e.g.: compute-cmvn-stats-two-channel data/train_unseg/reco2file_and_channel scp:data/train_unseg/feats.scp ark,t:-\n    """"""\n\n    po = ParseOptions(usage)\n\n    po.register_float(""quieter_channel_weight"", 0.01, ""For the quieter channel,""\n                      "" apply this weight to the stats, so that we still get ""\n                      ""stats if one channel always dominates."")\n\n    opts = po.parse_args()\n\n    if po.num_args() != 3:\n        po.print_usage()\n        sys.exit(1)\n\n    reco2file_and_channel_rxfilename = po.get_arg(1)\n    feats_rspecifier = po.get_arg(2)\n    stats_wspecifier = po.get_arg(3)\n\n    compute_cmvn_stats_two_channel(reco2file_and_channel_rxfilename,\n                                   feats_rspecifier, stats_wspecifier, opts)\n'"
examples/scripts/kaldi-style-executables/compute-mfcc-feats/compute-mfcc-feats.py,0,"b'#!/usr/bin/env python\nfrom __future__ import division, print_function\n\nimport sys\n\nfrom kaldi.feat.mfcc import Mfcc, MfccOptions\nfrom kaldi.matrix import Vector\nfrom kaldi.util.options import ParseOptions\nfrom kaldi.util.table import (MatrixWriter, RandomAccessFloatReaderMapped,\n                              SequentialWaveReader)\n\n\ndef compute_mfcc_feats(wav_rspecifier, feats_wspecifier, opts, mfcc_opts):\n    mfcc = Mfcc(mfcc_opts)\n\n    if opts.vtln_map:\n        vtln_map_reader = RandomAccessFloatReaderMapped(opts.vtln_map,\n                                                        opts.utt2spk)\n    elif opts.utt2spk:\n        print(""utt2spk option is needed only if vtln-map option is specified."",\n              file=sys.stderr)\n\n    num_utts, num_success = 0, 0\n    with SequentialWaveReader(wav_rspecifier) as reader, \\\n         MatrixWriter(feats_wspecifier) as writer:\n        for num_utts, (key, wave) in enumerate(reader, 1):\n            if wave.duration < opts.min_duration:\n                print(""File: {} is too short ({} sec): producing no output.""\n                      .format(key, wave.duration), file=sys.stderr)\n                continue\n\n            num_chan = wave.data().num_rows\n            if opts.channel >= num_chan:\n                print(""File with id {} has {} channels but you specified ""\n                      ""channel {}, producing no output."", file=sys.stderr)\n                continue\n            channel = 0 if opts.channel == -1 else opts.channel\n\n            if opts.vtln_map:\n                if key not in vtln_map_reader:\n                    print(""No vtln-map entry for utterance-id (or speaker-id)"",\n                          key, file=sys.stderr)\n                    continue\n                vtln_warp = vtln_map_reader[key]\n            else:\n                vtln_warp = opts.vtln_warp\n\n            try:\n                feats = mfcc.compute_features(wave.data()[channel],\n                                              wave.samp_freq, vtln_warp)\n            except:\n                print(""Failed to compute features for utterance"", key,\n                      file=sys.stderr)\n                continue\n\n            if opts.subtract_mean:\n                mean = Vector(feats.num_cols)\n                mean.add_row_sum_mat_(1.0, feats)\n                mean.scale_(1.0 / feats.num_rows)\n                for i in range(feats.num_rows):\n                    feats[i].add_vec_(-1.0, mean)\n\n            writer[key] = feats\n            num_success += 1\n\n            if num_utts % 10 == 0:\n                print(""Processed {} utterances"".format(num_utts),\n                      file=sys.stderr)\n\n    print(""Done {} out of {} utterances"".format(num_success, num_utts),\n          file=sys.stderr)\n\n    if opts.vtln_map:\n        vtln_map_reader.close()\n\n    return num_success != 0\n\n\nif __name__ == \'__main__\':\n    usage = """"""Create MFCC feature files.\n\n    Usage:  compute-mfcc-feats [options...] <wav-rspecifier> <feats-wspecifier>\n    """"""\n    po = ParseOptions(usage)\n\n    mfcc_opts = MfccOptions()\n    mfcc_opts.register(po)\n\n    po.register_bool(""subtract-mean"", False, ""Subtract mean of each feature""\n                     ""file [CMS]; not recommended to do it this way."")\n    po.register_float(""vtln-warp"", 1.0, ""Vtln warp factor (only applicable ""\n                      ""if vtln-map not specified)"")\n    po.register_str(""vtln-map"", """", ""Map from utterance or speaker-id to ""\n                    ""vtln warp factor (rspecifier)"")\n    po.register_str(""utt2spk"", """", ""Utterance to speaker-id map rspecifier""\n                    ""(if doing VTLN and you have warps per speaker)"")\n    po.register_int(""channel"", -1, ""Channel to extract (-1 -> expect mono, ""\n                    ""0 -> left, 1 -> right)"")\n    po.register_float(""min-duration"", 0.0, ""Minimum duration of segments ""\n                      ""to process (in seconds)."")\n\n    opts = po.parse_args()\n\n    if (po.num_args() != 2):\n      po.print_usage()\n      sys.exit()\n\n    wav_rspecifier = po.get_arg(1)\n    feats_wspecifier = po.get_arg(2)\n\n    compute_mfcc_feats(wav_rspecifier, feats_wspecifier, opts, mfcc_opts)\n'"
examples/scripts/kaldi-style-executables/copy-matrix/copy-matrix.py,0,"b'#!/usr/bin/env python\nfrom __future__ import division, print_function\n\nimport sys\nfrom kaldi.util.options import ParseOptions\nfrom kaldi.util.table import classify_rspecifier, RspecifierType,\\\n                             classify_wspecifier, WspecifierType,\\\n                             MatrixWriter, SequentialMatrixReader\nfrom kaldi.util.io import read_matrix, Output\nfrom kaldi.matrix import Matrix\nimport logging\n\ndef apply_softmax_per_row(mat):\n    for i in range(mat.num_rows):\n        mat[i].apply_softmax_()\n\nif __name__ == \'__main__\':\n    usage = """"""Copy matrices, or archives of matrices (e.g. features or transforms)\n    Also see copy-feats which has other format options\n\n\n    Usage: copy-matrix [options] <matrix-in-rspecifier> <matrix-out-wspecifier>\n    or     copy-matrix [options] <matrix-in-rxfilename> <matrix-out-wxfilename>\n\n    e.g.\n        copy-matrix --binary=false 1.mat -\n        copy-matrix ark:2.trans ark,t:-\n    """"""\n\n    po = ParseOptions(usage)\n\n    po.register_bool(""binary"", True, ""Write in binary mode (only relevant if output is a wxfilename)"")\n    po.register_float(""scale"", 1.0, ""This option can be used to scale the matrices being copied."")\n    po.register_bool(""apply-log"", False, ""This option can be used to apply log on the matrices. Must be avoided if matrix has negative quantities."")\n    po.register_bool(""apply-exp"", False, ""This option can be used to apply exp on the matrices"")\n    po.register_float(""apply-power"", 1.0, ""This option can be used to apply a power on the matrices"")\n    po.register_bool(""apply-softmax-per-row"", False, ""This option can be used to apply softmax per row of the matrices"")\n\n    opts = po.parse_args()\n\n    if po.num_args() != 2:\n        po.print_usage()\n        sys.exit(1)\n\n    if (opts.apply_log and opts.apply_exp) or (opts.apply_softmax_per_row and opts.apply_exp) or (opts.apply_softmax_per_row and opts.apply_log):\n        print(""Only one of apply-log, apply-exp and apply-softmax-per-row can be given"", file=sys.stderr)\n        sys.exit(1)\n\n    matrix_in_fn = po.get_arg(1)\n    matrix_out_fn = po.get_arg(2)\n\n    in_is_rspecifier = classify_rspecifier(matrix_in_fn)[0] != RspecifierType.NO_SPECIFIER\n    out_is_wspecifier = classify_wspecifier(matrix_out_fn)[0] != WspecifierType.NO_SPECIFIER\n\n    if in_is_rspecifier != out_is_wspecifier:\n        print(""Cannot mix archives with regular files (copying matrices)"", file=sys.stderr)\n        sys.exit(1)\n\n    if not in_is_rspecifier:\n        mat = read_matrix(matrix_in_fn)\n        if opts.scale != 1.0:\n            mat.scale_(opts.scale)\n\n        if opts.apply_log:\n            mat.apply_floor_(1.0e-20)\n            mat.apply_log_()\n\n        if opts.apply_exp:\n            mat.apply_exp_()\n\n        if opts.apply_softmax_per_row:\n            apply_softmax_per_row(mat)\n\n        if opts.apply_power != 1.0:\n            mat.apply_power_(opts.apply_power)\n\n        with Output(matrix_out_fn, opts.binary) as ko:\n            mat.write(ko.stream(), opts.binary)\n\n        logging.info(""Copied matrix to {}"".format(matrix_out_fn))\n\n    else:\n        with MatrixWriter(matrix_out_fn) as writer, \\\n             SequentialMatrixReader(matrix_in_fn) as reader:\n            for num_done, (key, mat) in enumerate(reader):\n\n                if opts.scale != 1.0 or\\\n                   opts.apply_log or\\\n                   opts.apply_exp or\\\n                   opts.apply_power != 1.0 or\\\n                   opts.apply_softmax_per_row:\n\n                    if opts.scale != 1.0:\n                        mat.scale_(opts.scale)\n\n                    if opts.apply_log:\n                        mat.apply_floor_(1.0e-20)\n                        mat.apply_log_()\n\n                    if opts.apply_power != 1.0:\n                        mat.apply_power_(opts.apply_power)\n\n                    writer[key] = mat\n\n                else:\n                    writer[key] = mat\n\n        logging.info(""Copied {} matrices"".format(num_done+1))\n'"
examples/scripts/kaldi-style-executables/extract-segments/extract-segments.py,0,"b'#!/usr/bin/env python\n\nimport logging\nimport sys\n\nfrom kaldi.matrix import SubMatrix\nfrom kaldi.feat.wave import WaveData\nfrom kaldi.util.options import ParseOptions\nfrom kaldi.util.table import RandomAccessWaveReader, WaveWriter\nfrom kaldi.util.io import xopen\n\ndef extract_segments(wav_rspecifier, segments_rxfilename, wav_wspecifier, opts):\n    with RandomAccessWaveReader(wav_rspecifier) as reader, \\\n         WaveWriter(wav_wspecifier) as writer:\n        num_success, num_lines = 0, 0\n        for num_lines, line in enumerate(xopen(segments_rxfilename, ""rt""), 1):\n            # segments file format:\n            #   segment-name wav-name start-time end-time [channel]\n            try:\n                segment, recording, start, end = line.split()\n                channel = None\n            except:\n                try:\n                    segment, recording, start, end, channel = line.split()\n                except:\n                    logging.warning(""Invalid line in segments file: {}""\n                                    .format(line))\n                    continue\n\n            try:\n                start = float(start)\n            except:\n                logging.warning(""Invalid line in segments file [bad start]: {}""\n                                .format(line))\n                continue\n\n            try:\n                end = float(end)\n            except:\n                logging.warning(""Invalid line in segments file [bad end]: {}""\n                                .format(line))\n                continue\n\n            if ((start < 0 or (end != -1.0 and end <= 0))\n                or (start >= end and end > 0)):\n                logging.warning(""Invalid line in segments file [empty or ""\n                                ""invalid segment]: {}"".format(line))\n                continue\n\n            try:\n                if channel:\n                    channel = int(channel)\n            except:\n                logging.warning(""Invalid line in segments file ""\n                                ""[bad channel]: {}"".format(line))\n                continue\n\n            if not recording in reader:\n                logging.warning(""Could not find recording {}, skipping ""\n                                ""segment {}"".format(recording, segment))\n                continue\n\n            wave = reader[recording]\n            wave_data = wave.data()\n            samp_freq = wave.samp_freq\n            num_chan, num_samp = wave_data.shape\n\n            # Convert starting time of the segment to corresponding sample\n            # number. If end time is -1 then use the whole file starting\n            # from start time.\n            start_samp = start * samp_freq\n            end_samp = end * samp_freq if end != -1 else num_samp\n            assert start_samp >= 0 and end_samp > 0, ""Invalid start or end.""\n\n            # start sample must be less than total number of samples,\n            # otherwise skip the segment\n            if start_samp < 0 or start_samp >= num_samp:\n                logging.warning(""Start sample out of range {} [length:] {}, ""\n                                ""skipping segment {}""\n                                .format(start_samp, num_samp, segment))\n                continue\n\n            # end sample must be less than total number samples\n            # otherwise skip the segment\n            if end_samp > num_samp:\n                if end_samp >= num_samp + int(opts.max_overshoot * samp_freq):\n                    logging.warning(""End sample too far out of range {} ""\n                                    ""[length:] {}, skipping segment {}""\n                                    .format(end_samp, num_samp, segment))\n                    continue\n                end_samp = num_samp #for small differences, just truncate.\n\n            # Skip if segment size is less than minimum segment length\n            # (default 0.1s)\n            min_samp = int(opts.min_segment_length * samp_freq)\n            if end_samp <= start_samp + min_samp:\n                logging.warning(""Segment {} too short, skipping it!""\n                                .format(segment))\n                continue\n\n            # check whether the wav file has more than one channel\n            # if yes, specify the channel info in segments file\n            # otherwise skips the segment\n            if channel is None:\n                if num_chan == 1:\n                    channel = 0\n                else:\n                    raise ValuError(""If your data has multiple channels, you ""\n                                    ""must specify the channel in the segments ""\n                                    ""file. Processing segment {}""\n                                    .format(segment))\n            else:\n                if channel >= num_chan:\n                    logging.warning(""Invalid channel {} >= {}, skipping segment""\n                                    "" {}"".format(channel, num_chan, segment))\n                    continue\n\n            segment_matrix = SubMatrix(wave_data, channel, 1,\n                                       int(start_samp),\n                                       int(end_samp - start_samp))\n            segment_wave = WaveData.new(samp_freq, segment_matrix)\n            writer[segment] = segment_wave  # write segment in wave format\n            num_success += 1\n\n        logging.info(""Succesfully processed {} lines out of {} in the ""\n                     ""segments file"".format(num_success, num_lines))\n\nif __name__ == \'__main__\':\n    # Configure log messages to look like Kaldi messages\n    from kaldi import __version__\n    logging.addLevelName(20, \'LOG\')\n    logging.basicConfig(format=\'%(levelname)s (%(module)s[{}]:%(funcName)s():\'\n                               \'%(filename)s:%(lineno)s) %(message)s\'\n                               .format(__version__), level=logging.INFO)\n\n    usage = """"""Extract segments from a large audio file in WAV format.\n    Usage:\n        extract-segments [options] <wav-rspecifier> <segments-file> <wav-wspecifier>\n    """"""\n    po = ParseOptions(usage)\n    po.register_float(""min-segment-length"", 0.1, ""Minimum segment length ""\n                      ""in seconds (reject shorter segments)"")\n    po.register_float(""max_overshoot"", 0.5, ""End segments overshooting audio ""\n                      ""by less than this (in seconds) are truncated, ""\n                      ""else rejected."")\n\n    opts = po.parse_args()\n    if po.num_args() != 3:\n        po.print_usage()\n        sys.exit()\n\n    wav_rspecifier = po.get_arg(1)\n    segments_rxfilename = po.get_arg(2)\n    wav_wspecifier = po.get_arg(3)\n\n    extract_segments(wav_rspecifier, segments_rxfilename, wav_wspecifier, opts)\n'"
examples/scripts/kaldi-style-executables/gmm-decode-faster/gmm-decode-faster.py,0,"b'#!/usr/bin/env python\nfrom __future__ import print_function, division\n\nimport logging\nimport sys\nimport time\n\nfrom kaldi.asr import convert_indices_to_symbols\nfrom kaldi.fstext import read_fst_kaldi\nfrom kaldi.decoder import FasterDecoderOptions, FasterDecoder\nfrom kaldi.fstext import (SymbolTable, FstHeader, FstReadOptions,\n                          StdArc, StdVectorFst, StdConstFst)\nfrom kaldi.fstext.utils import (get_linear_symbol_sequence,\n                                acoustic_lattice_scale, scale_lattice,\n                                convert_lattice_to_compact_lattice)\nfrom kaldi.gmm.am import AmDiagGmm, DecodableAmDiagGmmScaled\nfrom kaldi.hmm import TransitionModel\nfrom kaldi.util.io import xopen\nfrom kaldi.util.options import ParseOptions\nfrom kaldi.util.table import (SequentialMatrixReader, IntVectorWriter,\n                              CompactLatticeWriter)\n\n\ndef gmm_decode_faster(model_rxfilename, fst_rxfilename,\n                      feature_rspecifier, words_wspecifier,\n                      alignment_wspecifier="""", lattice_wspecifier="""",\n                      word_symbol_table="""", acoustic_scale=0.1,\n                      allow_partial=True, decoder_opts=FasterDecoderOptions()):\n    # Read model.\n    trans_model = TransitionModel()\n    am_gmm = AmDiagGmm()\n    with xopen(model_rxfilename) as ki:\n        trans_model.read(ki.stream(), ki.binary)\n        am_gmm.read(ki.stream(), ki.binary)\n\n    # Open table readers/writers.\n    feature_reader = SequentialMatrixReader(feature_rspecifier)\n    words_writer = IntVectorWriter(words_wspecifier)\n    alignment_writer = IntVectorWriter(alignment_wspecifier)\n    clat_writer = CompactLatticeWriter(lattice_wspecifier)\n\n    # Read symbol table.\n    word_syms = None\n    if word_symbol_table != """":\n        word_syms = SymbolTable.read_text(word_symbol_table)\n        if not word_syms:\n            raise RuntimeError(""Could not read symbol table from file {}""\n                               .format(word_symbol_table))\n\n    # NOTE:\n    # It is important to read decode_fst after opening feature reader as\n    # it can prevent crashes on systems without enough virtual memory.\n\n    # Read decoding graph and instantiate decoder.\n    decode_fst = read_fst_kaldi(fst_rxfilename)\n    decoder = FasterDecoder(decode_fst, decoder_opts)\n\n    tot_like = 0.0\n    frame_count = 0\n    num_success, num_fail = 0, 0\n    start = time.time()\n\n    for key, features in feature_reader:\n        if features.num_rows == 0:\n            num_fail += 1\n            logging.warning(""Zero-length utterance: {}"".format(key))\n            continue\n\n        gmm_decodable = DecodableAmDiagGmmScaled(am_gmm, trans_model,\n                                                 features, acoustic_scale)\n        decoder.decode(gmm_decodable)\n\n        if not (allow_partial or decoder.reached_final()):\n            num_fail += 1\n            logging.warning(""Did not successfully decode utterance {}, len = {}""\n                            .format(key, features.num_rows))\n            continue\n\n        try:\n            best_path = decoder.get_best_path()\n        except RuntimeError:\n            num_fail += 1\n            logging.warning(""Did not successfully decode utterance {}, len = {}""\n                            .format(key, features.num_rows))\n            continue\n\n        if not decoder.reached_final():\n            logging.warning(""Decoder did not reach end-state, outputting ""\n                            ""partial traceback since --allow-partial=true"")\n\n        ali, words, weight = get_linear_symbol_sequence(best_path)\n\n        words_writer[key] = words\n\n        if alignment_writer.is_open():\n            alignment_writer[key] = ali\n\n        if clat_writer.is_open():\n            if acoustic_scale != 0.0:\n                scale = acoustic_lattice_scale(1.0 / acoustic_scale)\n                scale_lattice(scale, best_path)\n            best_path = convert_lattice_to_compact_lattice(best_path)\n            clat_writer[key] = best_path\n\n        if word_syms:\n            syms = convert_indices_to_symbols(word_syms, words)\n            print(key, "" "".join(syms), file=sys.stderr)\n\n        num_success += 1\n        frame_count += features.num_rows\n        like = - (weight.value1 + weight.value2);\n        tot_like += like\n        logging.info(""Log-like per frame for utterance {} is {} over {} ""\n                     ""frames."".format(key, like / features.num_rows,\n                                      features.num_rows))\n        logging.debug(""Cost for utterance {} is {} + {}""\n                      .format(key, weight.value1, weight.value2))\n\n    elapsed = time.time() - start\n    logging.info(""Time taken [excluding initialization] {}s: real-time factor ""\n                 ""assuming 100 frames/sec is {}""\n                 .format(elapsed, elapsed * 100 / frame_count))\n    logging.info(""Done {} utterances, failed for {}""\n                 .format(num_success, num_fail))\n    logging.info(""Overall log-likelihood per frame is {} over {} frames.""\n                 .format(tot_like / frame_count, frame_count))\n\n    feature_reader.close()\n    words_writer.close()\n    if alignment_writer.is_open():\n        alignment_writer.close()\n    if clat_writer.is_open():\n        clat_writer.close()\n\n    return True if num_success != 0 else False\n\n\nif __name__ == \'__main__\':\n    # Configure log messages to look like Kaldi messages\n    from kaldi import __version__\n    logging.addLevelName(20, ""LOG"")\n    logging.basicConfig(format=""%(levelname)s (%(module)s[{}]:%(funcName)s():""\n                               ""%(filename)s:%(lineno)s) %(message)s""\n                               .format(__version__), level=logging.INFO)\n\n    usage = """"""Decode features using GMM-based model.\n\n    Usage:  gmm-decode-faster.py [options] model-in fst-in features-rspecifier\n                words-wspecifier [alignments-wspecifier [lattice-wspecifier]]\n\n    Note: lattices, if output, will just be linear sequences;\n          use gmm-latgen-faster if you want ""real"" lattices.\n    """"""\n    po = ParseOptions(usage)\n    decoder_opts = FasterDecoderOptions()\n    decoder_opts.register(po, True)\n    po.register_float(""acoustic-scale"", 0.1,\n                      ""Scaling factor for acoustic likelihoods"")\n    po.register_bool(""allow-partial"", True,\n                     ""Produce output even when final state was not reached"")\n    po.register_str(""word-symbol-table"", """",\n                    ""Symbol table for words [for debug output]"");\n    opts = po.parse_args()\n\n    if po.num_args() < 4 or po.num_args() > 6:\n        po.print_usage()\n        sys.exit()\n\n    model_rxfilename = po.get_arg(1)\n    fst_rxfilename = po.get_arg(2)\n    feature_rspecifier = po.get_arg(3)\n    words_wspecifier = po.get_arg(4)\n    alignment_wspecifier = po.get_opt_arg(5)\n    lattice_wspecifier = po.get_opt_arg(6)\n\n    gmm_decode_faster(model_rxfilename, fst_rxfilename,\n                      feature_rspecifier, words_wspecifier,\n                      alignment_wspecifier, lattice_wspecifier,\n                      opts.word_symbol_table, opts.acoustic_scale,\n                      opts.allow_partial, decoder_opts)\n'"
