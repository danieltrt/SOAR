file_path,api_count,code
setup.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport sys\n\nfrom setuptools import find_packages, setup\n\n# READ README.md for long description on PyPi.\n# This requires uploading via twine, e.g.:\n# $ python setup.py sdist bdist_wheel\n# $ twine upload --repository-url https://test.pypi.org/legacy/ dist/*  # test version\n# $ twine upload dist/*\ntry:\n    long_description = open(\'README.md\', encoding=\'utf-8\').read()\nexcept Exception as e:\n    sys.stderr.write(f\'Failed to convert README.md to rst:\\n  {e}\\n\')\n    sys.stderr.flush()\n    long_description = \'\'\n\n# Remove badges since they will always be obsolete.\n# This assumes the first 4 lines contain badge info.\nlong_description = \'\\n\'.join(line for line in long_description.split(\'\\n\')[4:])\n\nsetup(\n    name=\'funsor\',\n    version=\'0.1.2\',\n    description=\'A tensor-like library for functions and distributions\',\n    packages=find_packages(include=[\'funsor\', \'funsor.*\']),\n    url=\'https://github.com/pyro-ppl/funsor\',\n    project_urls={\n        ""Documentation"": ""https://funsor.pyro.ai"",\n    },\n    author=\'Uber AI Labs\',\n    author_email=\'fritzo@uber.com\',\n    python_requires="">=3.6"",\n    install_requires=[\n        \'makefun\',\n        \'multipledispatch\',\n        \'numpy>=1.7\',\n        \'opt_einsum>=2.3.2\',\n        \'pytest>=4.1\',\n    ],\n    extras_require={\n        \'torch\': [\n            \'pyro-ppl>=0.5\',\n            \'torch>=1.3.0\',\n        ],\n        \'jax\': [\n            \'jax>=0.1.65\',\n            \'jaxlib>=0.1.45\',\n            \'numpyro @ git+https://github.com/pyro-ppl/numpyro.git@e3accb5c71c8e96991abe9dd1f823e4435a40618#egg=numpyro\'\n        ],\n        \'test\': [\n            \'flake8\',\n            \'pandas\',\n            \'pyro-api>=0.1.2\',\n            \'pytest-xdist==1.27.0\',\n            \'pillow-simd\',\n            \'scipy\',\n            \'torchvision\',\n        ],\n        \'dev\': [\n            \'flake8\',\n            \'isort\',\n            \'pandas\',\n            \'pytest-xdist==1.27.0\',\n            \'scipy\',\n            \'sphinx>=2.0\',\n            \'sphinx_rtd_theme\',\n            \'pillow-simd\',\n            \'torchvision\',\n        ],\n    },\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    keywords=\'probabilistic machine learning bayesian statistics pytorch\',\n    classifiers=[\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache License 2.0\',\n        \'Operating System :: POSIX :: Linux\',\n        \'Operating System :: MacOS :: MacOS X\',\n        \'Programming Language :: Python :: 3.6\',\n    ],\n)\n'"
examples/discrete_hmm.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nfrom collections import OrderedDict\n\nimport torch\n\nimport funsor\nimport funsor.torch.distributions as dist\nimport funsor.ops as ops\nfrom funsor.interpreter import interpretation, reinterpret\nfrom funsor.optimizer import apply_optimizer\nfrom funsor.terms import lazy\n\n\ndef main(args):\n    # Declare parameters.\n    trans_probs = torch.tensor([[0.2, 0.8], [0.7, 0.3]], requires_grad=True)\n    emit_probs = torch.tensor([[0.4, 0.6], [0.1, 0.9]], requires_grad=True)\n    params = [trans_probs, emit_probs]\n\n    # A discrete HMM model.\n    def model(data):\n        log_prob = funsor.to_funsor(0.)\n\n        trans = dist.Categorical(probs=funsor.Tensor(\n            trans_probs,\n            inputs=OrderedDict([(\'prev\', funsor.bint(args.hidden_dim))]),\n        ))\n\n        emit = dist.Categorical(probs=funsor.Tensor(\n            emit_probs,\n            inputs=OrderedDict([(\'latent\', funsor.bint(args.hidden_dim))]),\n        ))\n\n        x_curr = funsor.Number(0, args.hidden_dim)\n        for t, y in enumerate(data):\n            x_prev = x_curr\n\n            # A delayed sample statement.\n            x_curr = funsor.Variable(\'x_{}\'.format(t), funsor.bint(args.hidden_dim))\n            log_prob += trans(prev=x_prev, value=x_curr)\n\n            if not args.lazy and isinstance(x_prev, funsor.Variable):\n                log_prob = log_prob.reduce(ops.logaddexp, x_prev.name)\n\n            log_prob += emit(latent=x_curr, value=funsor.Tensor(y, dtype=2))\n\n        log_prob = log_prob.reduce(ops.logaddexp)\n        return log_prob\n\n    # Train model parameters.\n    data = torch.ones(args.time_steps, dtype=torch.long)\n    optim = torch.optim.Adam(params, lr=args.learning_rate)\n    for step in range(args.train_steps):\n        optim.zero_grad()\n        if args.lazy:\n            with interpretation(lazy):\n                log_prob = apply_optimizer(model(data))\n            log_prob = reinterpret(log_prob)\n        else:\n            log_prob = model(data)\n        assert not log_prob.inputs, \'free variables remain\'\n        loss = -log_prob.data\n        loss.backward()\n        optim.step()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""Kalman filter example"")\n    parser.add_argument(""-t"", ""--time-steps"", default=10, type=int)\n    parser.add_argument(""-n"", ""--train-steps"", default=101, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.05, type=float)\n    parser.add_argument(""-d"", ""--hidden-dim"", default=2, type=int)\n    parser.add_argument(""--lazy"", action=\'store_true\')\n    parser.add_argument(""--filter"", action=\'store_true\')\n    parser.add_argument(""--xfail-if-not-implemented"", action=\'store_true\')\n    args = parser.parse_args()\n\n    if args.xfail_if_not_implemented:\n        try:\n            main(args)\n        except NotImplementedError:\n            print(\'XFAIL\')\n    else:\n        main(args)\n'"
examples/eeg_slds.py,11,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nWe use a switching linear dynamical system [1] to model a EEG time series dataset.\nFor inference we use a moment-matching approximation enabled by\n`funsor.interpreter.interpretation(funsor.terms.moment_matching)`.\n\nReferences\n\n[1] Anderson, B., and J. Moore. ""Optimal filtering. Prentice-Hall, Englewood Cliffs."" New Jersey (1979).\n""""""\nimport argparse\nimport time\nfrom collections import OrderedDict\nfrom os.path import exists\nfrom urllib.request import urlopen\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport funsor\nimport funsor.torch.distributions as dist\nimport funsor.ops as ops\nfrom funsor.pyro.convert import funsor_to_cat_and_mvn, funsor_to_mvn, matrix_and_mvn_to_funsor, mvn_to_funsor\n\n\n# download dataset from UCI archive\ndef download_data():\n    if not exists(""eeg.dat""):\n        url = ""http://archive.ics.uci.edu/ml/machine-learning-databases/00264/EEG%20Eye%20State.arff""\n        with open(""eeg.dat"", ""wb"") as f:\n            f.write(urlopen(url).read())\n\n\nclass SLDS(nn.Module):\n    def __init__(self,\n                 num_components,   # the number of switching states K\n                 hidden_dim,       # the dimension of the continuous latent space\n                 obs_dim,          # the dimension of the continuous outputs\n                 fine_transition_matrix=True,    # controls whether the transition matrix depends on s_t\n                 fine_transition_noise=False,    # controls whether the transition noise depends on s_t\n                 fine_observation_matrix=False,  # controls whether the observation matrix depends on s_t\n                 fine_observation_noise=False,   # controls whether the observation noise depends on s_t\n                 moment_matching_lag=1):         # controls the expense of the moment matching approximation\n\n        self.num_components = num_components\n        self.hidden_dim = hidden_dim\n        self.obs_dim = obs_dim\n        self.moment_matching_lag = moment_matching_lag\n        self.fine_transition_noise = fine_transition_noise\n        self.fine_observation_matrix = fine_observation_matrix\n        self.fine_observation_noise = fine_observation_noise\n        self.fine_transition_matrix = fine_transition_matrix\n\n        assert moment_matching_lag > 0\n        assert fine_transition_noise or fine_observation_matrix or fine_observation_noise or fine_transition_matrix, \\\n            ""The continuous dynamics need to be coupled to the discrete dynamics in at least one way [use at "" + \\\n            ""least one of the arguments --ftn --ftm --fon --fom]""\n\n        super(SLDS, self).__init__()\n\n        # initialize the various parameters of the model\n        self.transition_logits = nn.Parameter(0.1 * torch.randn(num_components, num_components))\n        if fine_transition_matrix:\n            transition_matrix = torch.eye(hidden_dim) + 0.05 * torch.randn(num_components, hidden_dim, hidden_dim)\n        else:\n            transition_matrix = torch.eye(hidden_dim) + 0.05 * torch.randn(hidden_dim, hidden_dim)\n        self.transition_matrix = nn.Parameter(transition_matrix)\n        if fine_transition_noise:\n            self.log_transition_noise = nn.Parameter(0.1 * torch.randn(num_components, hidden_dim))\n        else:\n            self.log_transition_noise = nn.Parameter(0.1 * torch.randn(hidden_dim))\n        if fine_observation_matrix:\n            self.observation_matrix = nn.Parameter(0.3 * torch.randn(num_components, hidden_dim, obs_dim))\n        else:\n            self.observation_matrix = nn.Parameter(0.3 * torch.randn(hidden_dim, obs_dim))\n        if fine_observation_noise:\n            self.log_obs_noise = nn.Parameter(0.1 * torch.randn(num_components, obs_dim))\n        else:\n            self.log_obs_noise = nn.Parameter(0.1 * torch.randn(obs_dim))\n\n        # define the prior distribution p(x_0) over the continuous latent at the initial time step t=0\n        x_init_mvn = torch.distributions.MultivariateNormal(torch.zeros(self.hidden_dim), torch.eye(self.hidden_dim))\n        self.x_init_mvn = mvn_to_funsor(x_init_mvn, real_inputs=OrderedDict([(\'x_0\', funsor.reals(self.hidden_dim))]))\n\n    # we construct the various funsors used to compute the marginal log probability and other model quantities.\n    # these funsors depend on the various model parameters.\n    def get_tensors_and_dists(self):\n        # normalize the transition probabilities\n        trans_logits = self.transition_logits - self.transition_logits.logsumexp(dim=-1, keepdim=True)\n        trans_probs = funsor.Tensor(trans_logits, OrderedDict([(""s"", funsor.bint(self.num_components))]))\n\n        trans_mvn = torch.distributions.MultivariateNormal(torch.zeros(self.hidden_dim),\n                                                           self.log_transition_noise.exp().diag_embed())\n        obs_mvn = torch.distributions.MultivariateNormal(torch.zeros(self.obs_dim),\n                                                         self.log_obs_noise.exp().diag_embed())\n\n        event_dims = (""s"",) if self.fine_transition_matrix or self.fine_transition_noise else ()\n        x_trans_dist = matrix_and_mvn_to_funsor(self.transition_matrix, trans_mvn, event_dims, ""x"", ""y"")\n        event_dims = (""s"",) if self.fine_observation_matrix or self.fine_observation_noise else ()\n        y_dist = matrix_and_mvn_to_funsor(self.observation_matrix, obs_mvn, event_dims, ""x"", ""y"")\n\n        return trans_logits, trans_probs, trans_mvn, obs_mvn, x_trans_dist, y_dist\n\n    # compute the marginal log probability of the observed data using a moment-matching approximation\n    @funsor.interpreter.interpretation(funsor.terms.moment_matching)\n    def log_prob(self, data):\n        trans_logits, trans_probs, trans_mvn, obs_mvn, x_trans_dist, y_dist = self.get_tensors_and_dists()\n\n        log_prob = funsor.Number(0.)\n\n        s_vars = {-1: funsor.Tensor(torch.tensor(0), dtype=self.num_components)}\n        x_vars = {}\n\n        for t, y in enumerate(data):\n            # construct free variables for s_t and x_t\n            s_vars[t] = funsor.Variable(f\'s_{t}\', funsor.bint(self.num_components))\n            x_vars[t] = funsor.Variable(f\'x_{t}\', funsor.reals(self.hidden_dim))\n\n            # incorporate the discrete switching dynamics\n            log_prob += dist.Categorical(trans_probs(s=s_vars[t - 1]), value=s_vars[t])\n\n            # incorporate the prior term p(x_t | x_{t-1})\n            if t == 0:\n                log_prob += self.x_init_mvn(value=x_vars[t])\n            else:\n                log_prob += x_trans_dist(s=s_vars[t], x=x_vars[t - 1], y=x_vars[t])\n\n            # do a moment-matching reduction. at this point log_prob depends on (moment_matching_lag + 1)-many\n            # pairs of free variables.\n            if t > self.moment_matching_lag - 1:\n                log_prob = log_prob.reduce(ops.logaddexp, frozenset([s_vars[t - self.moment_matching_lag].name,\n                                                                     x_vars[t - self.moment_matching_lag].name]))\n\n            # incorporate the observation p(y_t | x_t, s_t)\n            log_prob += y_dist(s=s_vars[t], x=x_vars[t], y=y)\n\n        T = data.shape[0]\n        # reduce any remaining free variables\n        for t in range(self.moment_matching_lag):\n            log_prob = log_prob.reduce(ops.logaddexp, frozenset([s_vars[T - self.moment_matching_lag + t].name,\n                                                                 x_vars[T - self.moment_matching_lag + t].name]))\n\n        # assert that we\'ve reduced all the free variables in log_prob\n        assert not log_prob.inputs, \'unexpected free variables remain\'\n\n        # return the PyTorch tensor behind log_prob (which we can directly differentiate)\n        return log_prob.data\n\n    # do filtering, prediction, and smoothing using a moment-matching approximation.\n    # here we implicitly use a moment matching lag of L = 1. the general logic follows\n    # the logic in the log_prob method.\n    @torch.no_grad()\n    @funsor.interpreter.interpretation(funsor.terms.moment_matching)\n    def filter_and_predict(self, data, smoothing=False):\n        trans_logits, trans_probs, trans_mvn, obs_mvn, x_trans_dist, y_dist = self.get_tensors_and_dists()\n\n        log_prob = funsor.Number(0.)\n\n        s_vars = {-1: funsor.Tensor(torch.tensor(0), dtype=self.num_components)}\n        x_vars = {-1: None}\n\n        predictive_x_dists, predictive_y_dists, filtering_dists = [], [], []\n        test_LLs = []\n\n        for t, y in enumerate(data):\n            s_vars[t] = funsor.Variable(f\'s_{t}\', funsor.bint(self.num_components))\n            x_vars[t] = funsor.Variable(f\'x_{t}\', funsor.reals(self.hidden_dim))\n\n            log_prob += dist.Categorical(trans_probs(s=s_vars[t - 1]), value=s_vars[t])\n\n            if t == 0:\n                log_prob += self.x_init_mvn(value=x_vars[t])\n            else:\n                log_prob += x_trans_dist(s=s_vars[t], x=x_vars[t - 1], y=x_vars[t])\n\n            if t > 0:\n                log_prob = log_prob.reduce(ops.logaddexp, frozenset([s_vars[t - 1].name, x_vars[t - 1].name]))\n\n            # do 1-step prediction and compute test LL\n            if t > 0:\n                predictive_x_dists.append(log_prob)\n                _log_prob = log_prob - log_prob.reduce(ops.logaddexp)\n                predictive_y_dist = y_dist(s=s_vars[t], x=x_vars[t]) + _log_prob\n                test_LLs.append(predictive_y_dist(y=y).reduce(ops.logaddexp).data.item())\n                predictive_y_dist = predictive_y_dist.reduce(ops.logaddexp, frozenset([f""x_{t}"", f""s_{t}""]))\n                predictive_y_dists.append(funsor_to_mvn(predictive_y_dist, 0, ()))\n\n            log_prob += y_dist(s=s_vars[t], x=x_vars[t], y=y)\n\n            # save filtering dists for forward-backward smoothing\n            if smoothing:\n                filtering_dists.append(log_prob)\n\n        # do the backward recursion using previously computed ingredients\n        if smoothing:\n            # seed the backward recursion with the filtering distribution at t=T\n            smoothing_dists = [filtering_dists[-1]]\n            T = data.size(0)\n\n            s_vars = {t: funsor.Variable(f\'s_{t}\', funsor.bint(self.num_components)) for t in range(T)}\n            x_vars = {t: funsor.Variable(f\'x_{t}\', funsor.reals(self.hidden_dim)) for t in range(T)}\n\n            # do the backward recursion.\n            # let p[t|t-1] be the predictive distribution at time step t.\n            # let p[t|t] be the filtering distribution at time step t.\n            # let f[t] denote the prior (transition) density at time step t.\n            # then the smoothing distribution p[t|T] at time step t is\n            # given by the following recursion.\n            # p[t-1|T] = p[t-1|t-1] <p[t|T] f[t] / p[t|t-1]>\n            # where <...> denotes integration of the latent variables at time step t.\n            for t in reversed(range(T - 1)):\n                integral = smoothing_dists[-1] - predictive_x_dists[t]\n                integral += dist.Categorical(trans_probs(s=s_vars[t]), value=s_vars[t + 1])\n                integral += x_trans_dist(s=s_vars[t], x=x_vars[t], y=x_vars[t + 1])\n                integral = integral.reduce(ops.logaddexp, frozenset([s_vars[t + 1].name, x_vars[t + 1].name]))\n                smoothing_dists.append(filtering_dists[t] + integral)\n\n        # compute predictive test MSE and predictive variances\n        predictive_means = torch.stack([d.mean for d in predictive_y_dists])  # T-1 ydim\n        predictive_vars = torch.stack([d.covariance_matrix.diagonal(dim1=-1, dim2=-2) for d in predictive_y_dists])\n        predictive_mse = (predictive_means - data[1:, :]).pow(2.0).mean(-1)\n\n        if smoothing:\n            # compute smoothed mean function\n            smoothing_dists = [funsor_to_cat_and_mvn(d, 0, (f""s_{t}"",))\n                               for t, d in enumerate(reversed(smoothing_dists))]\n            means = torch.stack([d[1].mean for d in smoothing_dists])  # T 2 xdim\n            means = torch.matmul(means.unsqueeze(-2), self.observation_matrix).squeeze(-2)  # T 2 ydim\n\n            probs = torch.stack([d[0].logits for d in smoothing_dists]).exp()\n            probs = probs / probs.sum(-1, keepdim=True)  # T 2\n\n            smoothing_means = (probs.unsqueeze(-1) * means).sum(-2)  # T ydim\n            smoothing_probs = probs[:, 1]\n\n            return predictive_mse, torch.tensor(np.array(test_LLs)), predictive_means, predictive_vars, \\\n                smoothing_means, smoothing_probs\n        else:\n            return predictive_mse, torch.tensor(np.array(test_LLs))\n\n\ndef main(args):\n    # download and pre-process EEG data if not in test mode\n    if not args.test:\n        download_data()\n        N_val, N_test = 149, 200\n        data = np.loadtxt(\'eeg.dat\', delimiter=\',\', skiprows=19)\n        print(""[raw data shape] {}"".format(data.shape))\n        data = data[::20, :]\n        print(""[data shape after thinning] {}"".format(data.shape))\n        eye_state = [int(d) for d in data[:, -1].tolist()]\n        data = torch.tensor(data[:, :-1]).float()\n    # in test mode (for continuous integration on github) so create fake data\n    else:\n        data = torch.randn(10, 3)\n        N_val, N_test = 2, 2\n\n    T, obs_dim = data.shape\n    N_train = T - N_test - N_val\n\n    np.random.seed(0)\n    rand_perm = np.random.permutation(N_val + N_test)\n    val_indices = rand_perm[0:N_val]\n    test_indices = rand_perm[N_val:]\n\n    data_mean = data[0:N_train, :].mean(0)\n    data -= data_mean\n    data_std = data[0:N_train, :].std(0)\n    data /= data_std\n\n    print(""Length of time series T: {}   Observation dimension: {}"".format(T, obs_dim))\n    print(""N_train: {}  N_val: {}  N_test: {}"".format(N_train, N_val, N_test))\n\n    torch.manual_seed(args.seed)\n\n    # set up model\n    slds = SLDS(num_components=args.num_components, hidden_dim=args.hidden_dim, obs_dim=obs_dim,\n                fine_observation_noise=args.fon, fine_transition_noise=args.ftn,\n                fine_observation_matrix=args.fom, fine_transition_matrix=args.ftm,\n                moment_matching_lag=args.moment_matching_lag)\n\n    # set up optimizer\n    adam = torch.optim.Adam(slds.parameters(), lr=args.learning_rate, betas=(args.beta1, 0.999), amsgrad=True)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(adam, gamma=args.gamma)\n    ts = [time.time()]\n\n    report_frequency = 1\n\n    # training loop\n    for step in range(args.num_steps):\n        nll = -slds.log_prob(data[0:N_train, :]) / N_train\n        nll.backward()\n\n        if step == 5:\n            scheduler.base_lrs[0] *= 0.20\n\n        adam.step()\n        scheduler.step()\n        adam.zero_grad()\n\n        if step % report_frequency == 0 or step == args.num_steps - 1:\n            step_dt = ts[-1] - ts[-2] if step > 0 else 0.0\n            pred_mse, pred_LLs = slds.filter_and_predict(data[0:N_train + N_val + N_test, :])\n            val_mse = pred_mse[val_indices].mean().item()\n            test_mse = pred_mse[test_indices].mean().item()\n            val_ll = pred_LLs[val_indices].mean().item()\n            test_ll = pred_LLs[test_indices].mean().item()\n\n            stats = ""[step %03d] train_nll: %.5f val_mse: %.5f val_ll: %.5f test_mse: %.5f test_ll: %.5f\\t(dt: %.2f)""\n            print(stats % (step, nll.item(), val_mse, val_ll, test_mse, test_ll, step_dt))\n\n        ts.append(time.time())\n\n    # plot predictions and smoothed means\n    if args.plot:\n        assert not args.test\n        predicted_mse, LLs, pred_means, pred_vars, smooth_means, smooth_probs = \\\n            slds.filter_and_predict(data, smoothing=True)\n\n        pred_means = pred_means.data.numpy()\n        pred_stds = pred_vars.sqrt().data.numpy()\n        smooth_means = smooth_means.data.numpy()\n        smooth_probs = smooth_probs.data.numpy()\n\n        import matplotlib\n        matplotlib.use(\'Agg\')  # noqa: E402\n        import matplotlib.pyplot as plt\n\n        f, axes = plt.subplots(4, 1, figsize=(12, 8), sharex=True)\n        T = data.size(0)\n        N_valtest = N_val + N_test\n        to_seconds = 117.0 / T\n\n        for k, ax in enumerate(axes[:-1]):\n            which = [0, 4, 10][k]\n            ax.plot(to_seconds * np.arange(T), data[:, which], \'ko\', markersize=2)\n            ax.plot(to_seconds * np.arange(N_train), smooth_means[:N_train, which], ls=\'solid\', color=\'r\')\n\n            ax.plot(to_seconds * (N_train + np.arange(N_valtest)),\n                    pred_means[-N_valtest:, which], ls=\'solid\', color=\'b\')\n            ax.fill_between(to_seconds * (N_train + np.arange(N_valtest)),\n                            pred_means[-N_valtest:, which] - 1.645 * pred_stds[-N_valtest:, which],\n                            pred_means[-N_valtest:, which] + 1.645 * pred_stds[-N_valtest:, which],\n                            color=\'lightblue\')\n            ax.set_ylabel(""$y_{%d}$"" % (which + 1), fontsize=20)\n            ax.tick_params(axis=\'both\', which=\'major\', labelsize=14)\n\n        axes[-1].plot(to_seconds * np.arange(T), eye_state, \'k\', ls=\'solid\')\n        axes[-1].plot(to_seconds * np.arange(T), smooth_probs, \'r\', ls=\'solid\')\n        axes[-1].set_xlabel(""Time (s)"", fontsize=20)\n        axes[-1].set_ylabel(""Eye state"", fontsize=20)\n        axes[-1].tick_params(axis=\'both\', which=\'major\', labelsize=14)\n\n        plt.tight_layout(pad=0.7)\n        plt.savefig(\'eeg.pdf\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""Switching linear dynamical system"")\n    parser.add_argument(""-n"", ""--num-steps"", default=3, type=int)\n    parser.add_argument(""-s"", ""--seed"", default=15, type=int)\n    parser.add_argument(""-hd"", ""--hidden-dim"", default=5, type=int)\n    parser.add_argument(""-k"", ""--num-components"", default=2, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.5, type=float)\n    parser.add_argument(""-b1"", ""--beta1"", default=0.75, type=float)\n    parser.add_argument(""-g"", ""--gamma"", default=0.99, type=float)\n    parser.add_argument(""-mml"", ""--moment-matching-lag"", default=1, type=int)\n    parser.add_argument(""--plot"", action=\'store_true\')\n    parser.add_argument(""--fon"", action=\'store_true\')\n    parser.add_argument(""--ftm"", action=\'store_true\')\n    parser.add_argument(""--fom"", action=\'store_true\')\n    parser.add_argument(""--ftn"", action=\'store_true\')\n    parser.add_argument(""--test"", action=\'store_true\')\n    args = parser.parse_args()\n\n    main(args)\n'"
examples/kalman_filter.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport torch\n\nimport funsor\nimport funsor.torch.distributions as dist\nimport funsor.ops as ops\nfrom funsor.interpreter import interpretation, reinterpret\nfrom funsor.optimizer import apply_optimizer\nfrom funsor.terms import lazy\n\n\ndef main(args):\n    # Declare parameters.\n    trans_noise = torch.tensor(0.1, requires_grad=True)\n    emit_noise = torch.tensor(0.5, requires_grad=True)\n    params = [trans_noise, emit_noise]\n\n    # A Gaussian HMM model.\n    def model(data):\n        log_prob = funsor.to_funsor(0.)\n\n        x_curr = funsor.Tensor(torch.tensor(0.))\n        for t, y in enumerate(data):\n            x_prev = x_curr\n\n            # A delayed sample statement.\n            x_curr = funsor.Variable(\'x_{}\'.format(t), funsor.reals())\n            log_prob += dist.Normal(1 + x_prev / 2., trans_noise, value=x_curr)\n\n            # Optionally marginalize out the previous state.\n            if t > 0 and not args.lazy:\n                log_prob = log_prob.reduce(ops.logaddexp, x_prev.name)\n\n            # An observe statement.\n            log_prob += dist.Normal(0.5 + 3 * x_curr, emit_noise, value=y)\n\n        # Marginalize out all remaining delayed variables.\n        log_prob = log_prob.reduce(ops.logaddexp)\n        return log_prob\n\n    # Train model parameters.\n    torch.manual_seed(0)\n    data = torch.randn(args.time_steps)\n    optim = torch.optim.Adam(params, lr=args.learning_rate)\n    for step in range(args.train_steps):\n        optim.zero_grad()\n        if args.lazy:\n            with interpretation(lazy):\n                log_prob = apply_optimizer(model(data))\n            log_prob = reinterpret(log_prob)\n        else:\n            log_prob = model(data)\n        assert not log_prob.inputs, \'free variables remain\'\n        loss = -log_prob.data\n        loss.backward()\n        optim.step()\n        if args.verbose and step % 10 == 0:\n            print(\'step {} loss = {}\'.format(step, loss.item()))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""Kalman filter example"")\n    parser.add_argument(""-t"", ""--time-steps"", default=10, type=int)\n    parser.add_argument(""-n"", ""--train-steps"", default=101, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.05, type=float)\n    parser.add_argument(""--lazy"", action=\'store_true\')\n    parser.add_argument(""--filter"", action=\'store_true\')\n    parser.add_argument(""-v"", ""--verbose"", action=""store_true"")\n    args = parser.parse_args()\n    main(args)\n'"
examples/minipyro.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport torch\nfrom pyro.generic import distributions as dist\nfrom pyro.generic import infer, optim, pyro, pyro_backend\nfrom torch.distributions import constraints\n\nfrom funsor.interpreter import interpretation\nfrom funsor.montecarlo import monte_carlo\n\n\ndef main(args):\n    # Define a basic model with a single Normal latent random variable `loc`\n    # and a batch of Normally distributed observations.\n    def model(data):\n        loc = pyro.sample(""loc"", dist.Normal(0., 1.))\n        with pyro.plate(""data"", len(data), dim=-1):\n            pyro.sample(""obs"", dist.Normal(loc, 1.), obs=data)\n\n    # Define a guide (i.e. variational distribution) with a Normal\n    # distribution over the latent random variable `loc`.\n    def guide(data):\n        guide_loc = pyro.param(""guide_loc"", torch.tensor(0.))\n        guide_scale = pyro.param(""guide_scale"", torch.tensor(1.),\n                                 constraint=constraints.positive)\n        pyro.sample(""loc"", dist.Normal(guide_loc, guide_scale))\n\n    # Generate some data.\n    torch.manual_seed(0)\n    data = torch.randn(100) + 3.0\n\n    # Because the API in minipyro matches that of Pyro proper,\n    # training code works with generic Pyro implementations.\n    with pyro_backend(args.backend), interpretation(monte_carlo):\n        # Construct an SVI object so we can do variational inference on our\n        # model/guide pair.\n        Elbo = infer.JitTrace_ELBO if args.jit else infer.Trace_ELBO\n        elbo = Elbo()\n        adam = optim.Adam({""lr"": args.learning_rate})\n        svi = infer.SVI(model, guide, adam, elbo)\n\n        # Basic training loop\n        pyro.get_param_store().clear()\n        for step in range(args.num_steps):\n            loss = svi.step(data)\n            if args.verbose and step % 100 == 0:\n                print(""step {} loss = {}"".format(step, loss))\n\n        # Report the final values of the variational parameters\n        # in the guide after training.\n        if args.verbose:\n            for name in pyro.get_param_store():\n                value = pyro.param(name).data\n                print(""{} = {}"".format(name, value.detach().cpu().numpy()))\n\n        # For this simple (conjugate) model we know the exact posterior. In\n        # particular we know that the variational distribution should be\n        # centered near 3.0. So let\'s check this explicitly.\n        assert (pyro.param(""guide_loc"") - 3.0).abs() < 0.1\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Minipyro demo"")\n    parser.add_argument(""-b"", ""--backend"", default=""funsor"")\n    parser.add_argument(""-n"", ""--num-steps"", default=1001, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.02, type=float)\n    parser.add_argument(""--jit"", action=""store_true"")\n    parser.add_argument(""-v"", ""--verbose"", action=""store_true"")\n    args = parser.parse_args()\n    main(args)\n'"
examples/pcfg.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport math\nfrom collections import OrderedDict\n\nimport torch\n\nimport funsor.ops as ops\nfrom funsor.delta import Delta\nfrom funsor.domains import bint\nfrom funsor.tensor import Tensor\nfrom funsor.terms import Number, Stack, Variable\n\n\ndef Uniform(components):\n    components = tuple(components)\n    size = len(components)\n    if size == 1:\n        return components[0]\n    var = Variable(\'v\', bint(size))\n    return (Stack(var.name, components).reduce(ops.logaddexp, var.name)\n            - math.log(size))\n\n\n# @of_shape(*([bint(2)] * size))\ndef model(size, position=0):\n    if size == 1:\n        name = str(position)\n        return Uniform((Delta(name, Number(0, 2)),\n                        Delta(name, Number(1, 2))))\n    return Uniform(model(t, position) +\n                   model(size - t, t + position)\n                   for t in range(1, size))\n\n\ndef main(args):\n    torch.manual_seed(args.seed)\n\n    print_ = print if args.verbose else lambda msg: None\n    print_(\'Data:\')\n    data = torch.distributions.Categorical(torch.ones(2)).sample((args.size,))\n    assert data.shape == (args.size,)\n    data = Tensor(data, OrderedDict(i=bint(args.size)), dtype=2)\n    print_(data)\n\n    print_(\'Model:\')\n    m = model(args.size)\n    print_(m.pretty())\n\n    print_(\'Eager log_prob:\')\n    obs = {str(i): data(i) for i in range(args.size)}\n    log_prob = m(**obs)\n    print_(log_prob)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""PCFG example"")\n    parser.add_argument(""-s"", ""--size"", default=3, type=int)\n    parser.add_argument(""--seed"", default=0, type=int)\n    parser.add_argument(""-v"", ""--verbose"", action=\'store_true\')\n    args = parser.parse_args()\n    main(args)\n'"
examples/sensor.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport itertools\nimport math\nimport os\n\nimport pyro.distributions as dist\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\n\nimport funsor.torch.distributions as f_dist\nimport funsor.ops as ops\nfrom funsor.domains import reals\nfrom funsor.pyro.convert import dist_to_funsor, funsor_to_mvn\nfrom funsor.tensor import Tensor, Variable\n\n# We use a 2D continuous-time NCV dynamics model throughout.\n# See http://webee.technion.ac.il/people/shimkin/Estimation09/ch8_target.pdf\nTIME_STEP = 1.\nNCV_PROCESS_NOISE = torch.tensor([[1/3, 0.0, 1/2, 0.0],\n                                  [0.0, 1/3, 0.0, 1/2],\n                                  [1/2, 0.0, 1.0, 0.0],\n                                  [0.0, 1/2, 0.0, 1.0]])\nNCV_TRANSITION_MATRIX = torch.tensor([[1., 0., 0., 0.],\n                                      [0., 1., 0., 0.],\n                                      [1., 0., 1., 0.],\n                                      [0., 1., 0., 1.]])\n\n\n@torch.no_grad()\ndef generate_data(num_frames, num_sensors):\n    """"""\n    Generate data from a damped NCV dynamics model\n    """"""\n    dt = TIME_STEP\n    bias_scale = 4.0\n    obs_noise = 1.0\n    trans_noise = 0.3\n\n    # define dynamics\n    z = torch.cat([10. * torch.randn(2),  # position\n                   torch.rand(2)])  # velocity\n    damp = 0.1  # damp the velocities\n    f = torch.tensor([[1, 0, 0, 0],\n                      [0, 1, 0, 0],\n                      [dt * math.exp(-damp * dt), 0, math.exp(-damp * dt), 0],\n                      [0, dt * math.exp(-damp * dt), 0, math.exp(-damp * dt)]])\n    trans_dist = dist.MultivariateNormal(\n        torch.zeros(4),\n        scale_tril=trans_noise * NCV_PROCESS_NOISE.cholesky())\n\n    # define biased sensors\n    sensor_bias = bias_scale * torch.randn(2 * num_sensors)\n    h = torch.eye(4, 2).unsqueeze(-1).expand(-1, -1, num_sensors).reshape(4, -1)\n    obs_dist = dist.MultivariateNormal(\n        sensor_bias,\n        scale_tril=obs_noise * torch.eye(2 * num_sensors))\n\n    states = []\n    observations = []\n    for t in range(num_frames):\n        z = z @ f + trans_dist.sample()\n        states.append(z)\n\n        x = z @ h + obs_dist.sample()\n        observations.append(x)\n\n    states = torch.stack(states)\n    observations = torch.stack(observations)\n    assert observations.shape == (num_frames, num_sensors * 2)\n    return observations, states, sensor_bias\n\n\nclass Model(nn.Module):\n    def __init__(self, num_sensors):\n        super(Model, self).__init__()\n        self.num_sensors = num_sensors\n\n        # learnable params\n        self.log_bias_scale = nn.Parameter(torch.tensor(0.))\n        self.log_obs_noise = nn.Parameter(torch.tensor(0.))\n        self.log_trans_noise = nn.Parameter(torch.tensor(0.))\n\n    def forward(self, observations, add_bias=True):\n        obs_dim = 2 * self.num_sensors\n        bias_scale = self.log_bias_scale.exp()\n        obs_noise = self.log_obs_noise.exp()\n        trans_noise = self.log_trans_noise.exp()\n\n        # bias distribution\n        bias = Variable(\'bias\', reals(obs_dim))\n        assert not torch.isnan(bias_scale), ""bias scales was nan""\n        bias_dist = dist_to_funsor(\n            dist.MultivariateNormal(\n                torch.zeros(obs_dim),\n                scale_tril=bias_scale * torch.eye(2 * self.num_sensors)\n            )\n        )(value=bias)\n\n        init_dist = torch.distributions.MultivariateNormal(\n            torch.zeros(4), scale_tril=100. * torch.eye(4))\n        self.init = dist_to_funsor(init_dist)(value=""state"")\n\n        # hidden states\n        prev = Variable(""prev"", reals(4))\n        curr = Variable(""curr"", reals(4))\n        self.trans_dist = f_dist.MultivariateNormal(\n            loc=prev @ NCV_TRANSITION_MATRIX,\n            scale_tril=trans_noise * NCV_PROCESS_NOISE.cholesky(),\n            value=curr\n            )\n\n        state = Variable(\'state\', reals(4))\n        obs = Variable(""obs"", reals(obs_dim))\n        observation_matrix = Tensor(torch.eye(4, 2).unsqueeze(-1)\n                                    .expand(-1, -1, self.num_sensors).reshape(4, -1))\n        assert observation_matrix.output.shape == (4, obs_dim), observation_matrix.output.shape\n        obs_loc = state @ observation_matrix\n        if add_bias:\n            obs_loc += bias\n        self.observation_dist = f_dist.MultivariateNormal(\n            loc=obs_loc,\n            scale_tril=obs_noise * torch.eye(obs_dim),\n            value=obs\n        )\n\n        logp = bias_dist\n        curr = ""state_init""\n        logp += self.init(state=curr)\n        for t, x in enumerate(observations):\n            prev, curr = curr, f""state_{t}""\n            logp += self.trans_dist(prev=prev, curr=curr)\n            logp += self.observation_dist(state=curr, obs=x)\n            # marginalize out previous state\n            logp = logp.reduce(ops.logaddexp, prev)\n        # marginalize out bias variable\n        logp = logp.reduce(ops.logaddexp, ""bias"")\n\n        # save posterior over the final state\n        assert set(logp.inputs) == {f\'state_{len(observations) - 1}\'}\n        posterior = funsor_to_mvn(logp, ndims=0)\n\n        # marginalize out remaining variables\n        logp = logp.reduce(ops.logaddexp)\n        assert isinstance(logp, Tensor) and logp.shape == (), logp.pretty()\n        return logp.data, posterior\n\n\ndef track(args):\n    results = {}  # keyed on (seed, bias, num_frames)\n    for seed in args.seed:\n        torch.manual_seed(seed)\n        observations, states, sensor_bias = generate_data(max(args.num_frames), args.num_sensors)\n        for bias, num_frames in itertools.product(args.bias, args.num_frames):\n            print(f\'tracking with seed={seed}, bias={bias}, num_frames={num_frames}\')\n            model = Model(args.num_sensors)\n            optim = Adam(model.parameters(), lr=args.lr, betas=(0.5, 0.8))\n            losses = []\n            for i in range(args.num_epochs):\n                optim.zero_grad()\n                log_prob, posterior = model(observations[:num_frames], add_bias=bias)\n                loss = -log_prob\n                loss.backward()\n                losses.append(loss.item())\n                if i % 10 == 0:\n                    print(loss.item())\n                optim.step()\n\n            # Collect evaluation metrics.\n            final_state_true = states[num_frames - 1]\n            assert final_state_true.shape == (4,)\n            final_pos_true = final_state_true[:2]\n            final_vel_true = final_state_true[2:]\n\n            final_state_est = posterior.loc\n            assert final_state_est.shape == (4,)\n            final_pos_est = final_state_est[:2]\n            final_vel_est = final_state_est[2:]\n            final_pos_error = float(torch.norm(final_pos_true - final_pos_est))\n            final_vel_error = float(torch.norm(final_vel_true - final_vel_est))\n            print(f\'final_pos_error = {final_pos_error}\')\n\n            results[seed, bias, num_frames] = {\n                ""args"": args,\n                ""observations"": observations[:num_frames],\n                ""states"": states[:num_frames],\n                ""sensor_bias"": sensor_bias,\n                ""losses"": losses,\n                ""bias_scale"": float(model.log_bias_scale.exp()),\n                ""obs_noise"": float(model.log_obs_noise.exp()),\n                ""trans_noise"": float(model.log_trans_noise.exp()),\n                ""final_state_estimate"": posterior,\n                ""final_pos_error"": final_pos_error,\n                ""final_vel_error"": final_vel_error,\n            }\n        if args.metrics_filename:\n            print(f\'saving output to: {args.metrics_filename}\')\n            torch.save(results, args.metrics_filename)\n    return results\n\n\ndef main(args):\n    if args.force or not args.metrics_filename or not os.path.exists(args.metrics_filename):\n        results = track(args)\n    else:\n        results = torch.load(args.metrics_filename)\n\n    if args.plot_filename:\n        import matplotlib\n        matplotlib.use(\'Agg\')\n        from matplotlib import pyplot\n        import numpy as np\n        seeds = set(seed for seed, _, _ in results)\n        X = args.num_frames\n        pyplot.figure(figsize=(5, 1.4), dpi=300)\n\n        pos_error = np.array([[results[s, 0, f][\'final_pos_error\'] for s in seeds]\n                              for f in args.num_frames])\n        mse = (pos_error**2).mean(axis=1)\n        std = (pos_error**2).std(axis=1) / len(seeds)**0.5\n        pyplot.plot(X, mse**0.5, \'k--\')\n        pyplot.fill_between(X, (mse - std)**0.5, (mse + std)**0.5, color=\'black\', alpha=0.15, lw=0)\n\n        pos_error = np.array([[results[s, 1, f][\'final_pos_error\'] for s in seeds]\n                              for f in args.num_frames])\n        mse = (pos_error**2).mean(axis=1)\n        std = (pos_error**2).std(axis=1) / len(seeds)**0.5\n        pyplot.plot(X, mse**0.5, \'r-\')\n        pyplot.fill_between(X, (mse - std)**0.5, (mse + std)**0.5, color=\'red\', alpha=0.15, lw=0)\n\n        pyplot.ylabel(\'Position RMSE\')\n        pyplot.xlabel(\'Track Length\')\n        pyplot.xticks((5, 10, 15, 20, 25, 30))\n        pyplot.xlim(5, 30)\n        pyplot.tight_layout(0)\n        pyplot.savefig(args.plot_filename)\n\n\ndef int_list(args):\n    result = []\n    for arg in args.split(\',\'):\n        if \'-\' in arg:\n            beg, end = map(int, arg.split(\'-\'))\n            result.extend(range(beg, 1 + end))\n        else:\n            result.append(int(arg))\n    return result\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Biased Kalman filter"")\n    parser.add_argument(""--seed"", default=""0"", type=int_list,\n                        help=""random seed, comma delimited for multiple runs"")\n    parser.add_argument(""--bias"", default=""0,1"", type=int_list,\n                        help=""whether to model bias, comma deliminted for multiple runs"")\n    parser.add_argument(""-f"", ""--num-frames"", default=""5,10,15,20,25,30"",\n                        type=int_list,\n                        help=""number of sensor frames, comma delimited for multiple runs"")\n    parser.add_argument(""--num-sensors"", default=5, type=int)\n    parser.add_argument(""-n"", ""--num-epochs"", default=50, type=int)\n    parser.add_argument(""--lr"", default=0.1, type=float)\n    parser.add_argument(""--metrics-filename"", default="""", type=str)\n    parser.add_argument(""--plot-filename"", default="""", type=str)\n    parser.add_argument(""--force"", action=""store_true"")\n    args = parser.parse_args()\n    main(args)\n'"
examples/slds.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nimport torch\n\nimport funsor\nimport funsor.torch.distributions as dist\nimport funsor.ops as ops\n\n\ndef main(args):\n    # Declare parameters.\n    trans_probs = funsor.Tensor(torch.tensor([[0.9, 0.1],\n                                              [0.1, 0.9]], requires_grad=True))\n    trans_noise = funsor.Tensor(torch.tensor([\n        0.1,  # low noise component\n        1.0,  # high noisy component\n    ], requires_grad=True))\n    emit_noise = funsor.Tensor(torch.tensor(0.5, requires_grad=True))\n    params = [trans_probs.data,\n              trans_noise.data,\n              emit_noise.data]\n\n    # A Gaussian HMM model.\n    @funsor.interpreter.interpretation(funsor.terms.moment_matching)\n    def model(data):\n        log_prob = funsor.Number(0.)\n\n        # s is the discrete latent state,\n        # x is the continuous latent state,\n        # y is the observed state.\n        s_curr = funsor.Tensor(torch.tensor(0), dtype=2)\n        x_curr = funsor.Tensor(torch.tensor(0.))\n        for t, y in enumerate(data):\n            s_prev = s_curr\n            x_prev = x_curr\n\n            # A delayed sample statement.\n            s_curr = funsor.Variable(\'s_{}\'.format(t), funsor.bint(2))\n            log_prob += dist.Categorical(trans_probs[s_prev], value=s_curr)\n\n            # A delayed sample statement.\n            x_curr = funsor.Variable(\'x_{}\'.format(t), funsor.reals())\n            log_prob += dist.Normal(x_prev, trans_noise[s_curr], value=x_curr)\n\n            # Marginalize out previous delayed sample statements.\n            if t > 0:\n                log_prob = log_prob.reduce(ops.logaddexp, {s_prev.name, x_prev.name})\n\n            # An observe statement.\n            log_prob += dist.Normal(x_curr, emit_noise, value=y)\n\n        log_prob = log_prob.reduce(ops.logaddexp)\n        return log_prob\n\n    # Train model parameters.\n    torch.manual_seed(0)\n    data = torch.randn(args.time_steps)\n    optim = torch.optim.Adam(params, lr=args.learning_rate)\n    for step in range(args.train_steps):\n        optim.zero_grad()\n        log_prob = model(data)\n        assert not log_prob.inputs, \'free variables remain\'\n        loss = -log_prob.data\n        loss.backward()\n        optim.step()\n        if args.verbose and step % 10 == 0:\n            print(\'step {} loss = {}\'.format(step, loss.item()))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""Switching linear dynamical system"")\n    parser.add_argument(""-t"", ""--time-steps"", default=10, type=int)\n    parser.add_argument(""-n"", ""--train-steps"", default=101, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.01, type=float)\n    parser.add_argument(""--filter"", action=\'store_true\')\n    parser.add_argument(""-v"", ""--verbose"", action=""store_true"")\n    args = parser.parse_args()\n    main(args)\n'"
examples/vae.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport os\nfrom collections import OrderedDict\n\nimport torch\nimport torch.utils.data\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import datasets, transforms\n\nimport funsor\nimport funsor.torch.distributions as dist\nimport funsor.ops as ops\nfrom funsor.domains import bint, reals\n\nREPO_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nDATA_PATH = os.path.join(REPO_PATH, 'data')\n\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder, self).__init__()\n        self.fc1 = nn.Linear(784, 400)\n        self.fc21 = nn.Linear(400, 20)\n        self.fc22 = nn.Linear(400, 20)\n\n    def forward(self, image):\n        image = image.reshape(image.shape[:-2] + (-1,))\n        h1 = F.relu(self.fc1(image))\n        loc = self.fc21(h1)\n        scale = self.fc22(h1).exp()\n        return loc, scale\n\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.fc3 = nn.Linear(20, 400)\n        self.fc4 = nn.Linear(400, 784)\n\n    def forward(self, z):\n        h3 = F.relu(self.fc3(z))\n        out = torch.sigmoid(self.fc4(h3))\n        return out.reshape(out.shape[:-1] + (28, 28))\n\n\ndef main(args):\n    encoder = Encoder()\n    decoder = Decoder()\n\n    encode = funsor.function(reals(28, 28), (reals(20), reals(20)))(encoder)\n    decode = funsor.function(reals(20), reals(28, 28))(decoder)\n\n    @funsor.interpreter.interpretation(funsor.montecarlo.monte_carlo)\n    def loss_function(data, subsample_scale):\n        # Lazily sample from the guide.\n        loc, scale = encode(data)\n        q = funsor.Independent(\n            dist.Normal(loc['i'], scale['i'], value='z_i'),\n            'z', 'i', 'z_i')\n\n        # Evaluate the model likelihood at the lazy value z.\n        probs = decode('z')\n        p = dist.Bernoulli(probs['x', 'y'], value=data['x', 'y'])\n        p = p.reduce(ops.add, {'x', 'y'})\n\n        # Construct an elbo. This is where sampling happens.\n        elbo = funsor.Integrate(q, p - q, 'z')\n        elbo = elbo.reduce(ops.add, 'batch') * subsample_scale\n        loss = -elbo\n        return loss\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(DATA_PATH, train=True, download=True,\n                       transform=transforms.ToTensor()),\n        batch_size=args.batch_size, shuffle=True)\n\n    encoder.train()\n    decoder.train()\n    optimizer = optim.Adam(list(encoder.parameters()) +\n                           list(decoder.parameters()), lr=1e-3)\n    for epoch in range(args.num_epochs):\n        train_loss = 0\n        for batch_idx, (data, _) in enumerate(train_loader):\n            subsample_scale = float(len(train_loader.dataset) / len(data))\n            data = data[:, 0, :, :]\n            data = funsor.Tensor(data, OrderedDict(batch=bint(len(data))))\n\n            optimizer.zero_grad()\n            loss = loss_function(data, subsample_scale)\n            assert isinstance(loss, funsor.Tensor), loss.pretty()\n            loss.data.backward()\n            train_loss += loss.item()\n            optimizer.step()\n            if batch_idx % 50 == 0:\n                print('  loss = {}'.format(loss.item()))\n                if batch_idx and args.smoke_test:\n                    return\n        print('epoch {} train_loss = {}'.format(epoch, train_loss))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='VAE MNIST Example')\n    parser.add_argument('-n', '--num-epochs', type=int, default=10)\n    parser.add_argument('--batch-size', type=int, default=8)\n    parser.add_argument('--smoke-test', action='store_true')\n    args = parser.parse_args()\n    main(args)\n"""
funsor/__init__.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom funsor.domains import Domain, bint, find_domain, reals\nfrom funsor.integrate import Integrate\nfrom funsor.interpreter import reinterpret\nfrom funsor.sum_product import MarkovProduct\nfrom funsor.terms import Cat, Funsor, Independent, Lambda, Number, Slice, Stack, Variable, of_shape, to_data, to_funsor\nfrom funsor.tensor import Tensor, function\nfrom funsor.util import set_backend, get_backend, pretty, quote\n\nfrom . import (\n    adjoint,\n    affine,\n    cnf,\n    delta,\n    distribution,\n    domains,\n    einsum,\n    gaussian,\n    integrate,\n    interpreter,\n    joint,\n    memoize,\n    # minipyro,  # TODO: enable when minipyro is backend-agnostic\n    montecarlo,\n    ops,\n    sum_product,\n    terms,\n    testing,\n)\n\n# TODO: move to `funsor.util` when the following circular import issue is resolved\n# funsor.domains -> funsor.util -> set_backend -> funsor.torch -> funsor.domains\nset_backend(get_backend())\n\n\n__all__ = [\n    'Cat',\n    'Domain',\n    'Funsor',\n    'Independent',\n    'Integrate',\n    'Lambda',\n    'MarkovProduct',\n    'Number',\n    'Slice',\n    'Stack',\n    'Tensor',\n    'Variable',\n    'adjoint',\n    'affine',\n    'backward',\n    'bint',\n    'cnf',\n    'delta',\n    'distribution',\n    'domains',\n    'einsum',\n    'find_domain',\n    'function',\n    'gaussian',\n    'get_backend',\n    'integrate',\n    'interpreter',\n    'joint',\n    'memoize',\n    # 'minipyro',  # TODO: enable when minipyro is backend-agnostic\n    'montecarlo',\n    'of_shape',\n    'ops',\n    'pretty',\n    'quote',\n    'reals',\n    'reinterpret',\n    'set_backend',\n    'sum_product',\n    'terms',\n    'testing',\n    'to_data',\n    'to_funsor',\n]\n"""
funsor/adjoint.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict, defaultdict\n\nimport numpy as np\n\nimport funsor.interpreter as interpreter\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction, GaussianMixture, nullop\nfrom funsor.domains import bint\nfrom funsor.gaussian import Gaussian, align_gaussian\nfrom funsor.interpreter import interpretation\nfrom funsor.ops import AssociativeOp\nfrom funsor.registry import KeyedRegistry\nfrom funsor.terms import Binary, Cat, Funsor, Number, Reduce, Slice, Subs, Variable, reflect, substitute, to_funsor\nfrom funsor.tensor import Tensor\n\n\ndef _alpha_unmangle(expr):\n    alpha_subs = {name: name.split(""__BOUND"")[0]\n                  for name in expr.bound if ""__BOUND"" in name}\n    if not alpha_subs:\n        return tuple(expr._ast_values)\n\n    return expr._alpha_convert(alpha_subs)\n\n\nclass AdjointTape(object):\n\n    def __init__(self):\n        self.tape = []\n        self._old_interpretation = None\n\n    def __call__(self, cls, *args):\n        if cls in adjoint_ops:  # atomic op, don\'t trace internals\n            with interpretation(self._old_interpretation):\n                result = cls(*args)\n            self.tape.append((result, cls, args))\n        else:\n            result = self._old_interpretation(cls, *args)\n        return result\n\n    def __enter__(self):\n        self.tape = []\n        self._old_interpretation = interpreter._INTERPRETATION\n        interpreter.set_interpretation(self)\n        return self\n\n    def __exit__(self, *args):\n        interpreter.set_interpretation(self._old_interpretation)\n        self._old_interpretation = None\n\n    def adjoint(self, red_op, bin_op, root, targets):\n\n        bin_unit = to_funsor(ops.UNITS[bin_op])\n        adjoint_values = defaultdict(lambda: bin_unit)\n\n        reached_root = False\n        while self.tape:\n            output, fn, inputs = self.tape.pop()\n            if not reached_root:\n                if output is root:\n                    reached_root = True\n                else:\n                    continue\n\n            # reverse the effects of alpha-renaming\n            with interpretation(reflect):\n                other_subs = tuple((name, to_funsor(name.split(""__BOUND"")[0], domain))\n                                   for name, domain in output.inputs.items() if ""__BOUND"" in name)\n                inputs = _alpha_unmangle(substitute(fn(*inputs), other_subs))\n                output = type(output)(*_alpha_unmangle(substitute(output, other_subs)))\n\n            in_adjs = adjoint_ops(fn, red_op, bin_op, adjoint_values[output], *inputs)\n            for v, adjv in in_adjs.items():\n                adjoint_values[v] = bin_op(adjoint_values[v], adjv)\n\n        target_adjs = {}\n        for v in targets:\n            target_adjs[v] = adjoint_values[v]\n            if not isinstance(v, Variable):\n                target_adjs[v] = bin_op(target_adjs[v], v)\n\n        return target_adjs\n\n\n# logaddexp/add\ndef _fail_default(*args):\n    raise NotImplementedError(""Should not be here! {}"".format(args))\n\n\nadjoint_ops = KeyedRegistry(default=_fail_default)\nif interpreter._DEBUG:\n    adjoint_ops_register = adjoint_ops.register\n    adjoint_ops.register = lambda *args: lambda fn: adjoint_ops_register(*args)(interpreter.debug_logged(fn))\n\n\n@adjoint_ops.register(Tensor, AssociativeOp, AssociativeOp, Funsor, (np.ndarray, np.generic), tuple, object)\ndef adjoint_tensor(adj_redop, adj_binop, out_adj, data, inputs, dtype):\n    return {}\n\n\n@adjoint_ops.register(Binary, AssociativeOp, AssociativeOp, Funsor, AssociativeOp, Funsor, Funsor)\ndef adjoint_binary(adj_redop, adj_binop, out_adj, op, lhs, rhs):\n    assert (adj_redop, op) in ops.DISTRIBUTIVE_OPS\n\n    lhs_reduced_vars = frozenset(rhs.inputs) - frozenset(lhs.inputs)\n    lhs_adj = op(out_adj, rhs).reduce(adj_redop, lhs_reduced_vars)\n\n    rhs_reduced_vars = frozenset(lhs.inputs) - frozenset(rhs.inputs)\n    rhs_adj = op(out_adj, lhs).reduce(adj_redop, rhs_reduced_vars)\n\n    return {lhs: lhs_adj, rhs: rhs_adj}\n\n\n@adjoint_ops.register(Reduce, AssociativeOp, AssociativeOp, Funsor, AssociativeOp, Funsor, frozenset)\ndef adjoint_reduce(adj_redop, adj_binop, out_adj, op, arg, reduced_vars):\n    assert adj_binop is op or (op, adj_binop) in ops.DISTRIBUTIVE_OPS\n\n    if op is adj_redop:\n        # XXX using a hack to simulate ""expand""\n        return {arg: adj_binop(out_adj, Binary(ops.PRODUCT_INVERSES[adj_binop], arg, arg))}\n    elif op is adj_binop:  # plate!\n        out = arg.reduce(op, reduced_vars)\n        return {arg: adj_binop(out_adj, Binary(ops.PRODUCT_INVERSES[op], out, arg))}\n\n\n@adjoint_ops.register(Contraction, AssociativeOp, AssociativeOp, Funsor,\n                      AssociativeOp, AssociativeOp, frozenset, Funsor)\ndef adjoint_contract_unary(adj_redop, adj_binop, out_adj, sum_op, prod_op, reduced_vars, arg):\n    return adjoint_reduce(adj_redop, adj_binop, out_adj, sum_op, arg, reduced_vars)\n\n\n@adjoint_ops.register(Contraction, AssociativeOp, AssociativeOp, Funsor,\n                      AssociativeOp, AssociativeOp, frozenset, tuple)\ndef adjoint_contract_generic(adj_redop, adj_binop, out_adj, sum_op, prod_op, reduced_vars, terms):\n    assert len(terms) == 1 or len(terms) == 2\n    return adjoint_ops(Contraction, adj_redop, adj_binop, out_adj, sum_op, prod_op, reduced_vars, *terms)\n\n\n@adjoint_ops.register(Contraction, AssociativeOp, AssociativeOp, Funsor,\n                      AssociativeOp, AssociativeOp, frozenset, Funsor, Funsor)\ndef adjoint_contract(adj_redop, adj_binop, out_adj, sum_op, prod_op, reduced_vars, lhs, rhs):\n    assert sum_op is nullop or (sum_op, prod_op) in ops.DISTRIBUTIVE_OPS\n\n    lhs_reduced_vars = frozenset(rhs.inputs) - frozenset(lhs.inputs)\n    lhs_adj = Contraction(sum_op if sum_op is not nullop else adj_redop, prod_op, lhs_reduced_vars, out_adj, rhs)\n\n    rhs_reduced_vars = frozenset(lhs.inputs) - frozenset(rhs.inputs)\n    rhs_adj = Contraction(sum_op if sum_op is not nullop else adj_redop, prod_op, rhs_reduced_vars, out_adj, lhs)\n\n    return {lhs: lhs_adj, rhs: rhs_adj}\n\n\n@adjoint_ops.register(Cat, AssociativeOp, AssociativeOp, Funsor, str, tuple, str)\ndef adjoint_cat(adj_redop, adj_binop, out_adj, name, parts, part_name):\n    in_adjs = {}\n    start = 0\n    size = sum(part.inputs[part_name].dtype for part in parts)\n    for i, part in enumerate(parts):\n        if part_name in out_adj.inputs:\n            in_adjs[part] = out_adj(**{name: Slice(name, start, start + part.inputs[part_name].dtype, 1, size)})\n            start += part.inputs[part_name].dtype\n        else:\n            in_adjs[part] = adj_binop(out_adj, Binary(ops.PRODUCT_INVERSES[adj_binop], part, part))\n    return in_adjs\n\n\n@adjoint_ops.register(Subs, AssociativeOp, AssociativeOp, (Number, Tensor), Tensor, tuple)\ndef adjoint_subs_tensor(adj_redop, adj_binop, out_adj, arg, subs):\n\n    assert all(isinstance(v, Funsor) for k, v in subs)\n\n    # invert renaming\n    renames = tuple((v.name, k) for k, v in subs if isinstance(v, Variable))\n    out_adj = Subs(out_adj, renames)\n\n    # inverting advanced indexing\n    slices = tuple((k, v) for k, v in subs if not isinstance(v, Variable))\n\n    # TODO avoid reifying these zero/one tensors by using symbolic constants\n    # ones for things that weren\'t sliced away\n    ones_like_out = Subs(Tensor(ops.full_like(arg.data, ops.UNITS[adj_binop]),\n                                arg.inputs.copy(), arg.output.dtype),\n                         slices)\n    arg_adj = adj_binop(out_adj, ones_like_out)\n\n    # ones for things that were sliced away\n    ones_like_arg = Tensor(ops.full_like(arg.data, ops.UNITS[adj_binop]),\n                           arg.inputs.copy(), arg.output.dtype)\n    arg_adj = _scatter(arg_adj, ones_like_arg, slices)\n\n    return {arg: arg_adj}\n\n\ndef _scatter(src, res, subs):\n    # inverse of advanced indexing\n    # TODO check types of subs, in case some logic from eager_subs was accidentally left out?\n\n    # use advanced indexing logic copied from Tensor.eager_subs:\n\n    # materialize after checking for renaming case\n    subs = OrderedDict((k, res.materialize(v)) for k, v in subs)\n\n    # Compute result shapes.\n    inputs = OrderedDict()\n    for k, domain in res.inputs.items():\n        inputs[k] = domain\n\n    # Construct a dict with each input\'s positional dim,\n    # counting from the right so as to support broadcasting.\n    total_size = len(inputs) + len(res.output.shape)  # Assumes only scalar indices.\n    new_dims = {}\n    for k, domain in inputs.items():\n        assert not domain.shape\n        new_dims[k] = len(new_dims) - total_size\n\n    # Use advanced indexing to construct a simultaneous substitution.\n    index = []\n    for k, domain in res.inputs.items():\n        if k in subs:\n            v = subs.get(k)\n            if isinstance(v, Number):\n                index.append(int(v.data))\n            else:\n                # Permute and expand v.data to end up at new_dims.\n                assert isinstance(v, Tensor)\n                v = v.align(tuple(k2 for k2 in inputs if k2 in v.inputs))\n                assert isinstance(v, Tensor)\n                v_shape = [1] * total_size\n                for k2, size in zip(v.inputs, v.data.shape):\n                    v_shape[new_dims[k2]] = size\n                index.append(v.data.reshape(tuple(v_shape)))\n        else:\n            # Construct a [:] slice for this preserved input.\n            offset_from_right = -1 - new_dims[k]\n            index.append(ops.new_arange(res.data, domain.dtype).reshape(\n                (-1,) + (1,) * offset_from_right))\n\n    # Construct a [:] slice for the output.\n    for i, size in enumerate(res.output.shape):\n        offset_from_right = len(res.output.shape) - i - 1\n        index.append(ops.new_arange(res.data, size).reshape(\n            (-1,) + (1,) * offset_from_right))\n\n    # the only difference from Tensor.eager_subs is here:\n    # instead of indexing the rhs (lhs = rhs[index]), we index the lhs (lhs[index] = rhs)\n\n    # unsqueeze to make broadcasting work\n    src_inputs, src_data = src.inputs.copy(), src.data\n    for k, v in res.inputs.items():\n        if k not in src.inputs and isinstance(subs[k], Number):\n            src_inputs[k] = bint(1)\n            src_data = src_data.unsqueeze(-1 - len(src.output.shape))\n    src = Tensor(src_data, src_inputs, src.output.dtype).align(tuple(res.inputs.keys()))\n\n    data = res.data\n    data[tuple(index)] = src.data\n    return Tensor(data, inputs, res.dtype)\n\n\n@adjoint_ops.register(Subs, ops.LogAddExpOp, ops.AddOp, GaussianMixture, GaussianMixture, tuple)\ndef adjoint_subs_gaussianmixture_gaussianmixture(adj_redop, adj_binop, out_adj, arg, subs):\n\n    if any(v.dtype == \'real\' and not isinstance(v, Variable) for k, v in subs):\n        raise NotImplementedError(""TODO implement adjoint for substitution into Gaussian real variable"")\n\n    # invert renaming\n    renames = tuple((v.name, k) for k, v in subs if isinstance(v, Variable))\n    out_adj = Subs(out_adj, renames)\n\n    # inverting advanced indexing\n    slices = tuple((k, v) for k, v in subs if not isinstance(v, Variable))\n\n    assert len(slices + renames) == len(subs)\n\n    in_adj_discrete = adjoint_ops(Subs, adj_redop, adj_binop, out_adj.terms[0], arg.terms[0], subs)[arg.terms[0]]\n\n    arg_int_inputs = OrderedDict((k, v) for k, v in arg.inputs.items() if v.dtype != \'real\')\n    out_adj_int_inputs = OrderedDict((k, v) for k, v in out_adj.inputs.items() if v.dtype != \'real\')\n\n    arg_real_inputs = OrderedDict((k, v) for k, v in arg.inputs.items() if v.dtype == \'real\')\n\n    align_inputs = OrderedDict((k, v) for k, v in out_adj.terms[1].inputs.items() if v.dtype != \'real\')\n    align_inputs.update(arg_real_inputs)\n    out_adj_info_vec, out_adj_precision = align_gaussian(align_inputs, out_adj.terms[1])\n\n    in_adj_info_vec = list(adjoint_ops(Subs, adj_redop, adj_binop,  # ops.add, ops.mul,\n                                       Tensor(out_adj_info_vec, out_adj_int_inputs),\n                                       Tensor(arg.terms[1].info_vec, arg_int_inputs),\n                                       slices).values())[0]\n\n    in_adj_precision = list(adjoint_ops(Subs, adj_redop, adj_binop,  # ops.add, ops.mul,\n                                        Tensor(out_adj_precision, out_adj_int_inputs),\n                                        Tensor(arg.terms[1].precision, arg_int_inputs),\n                                        slices).values())[0]\n\n    assert isinstance(in_adj_info_vec, Tensor)\n    assert isinstance(in_adj_precision, Tensor)\n\n    in_adj_gaussian = Gaussian(in_adj_info_vec.data, in_adj_precision.data, arg.inputs.copy())\n\n    in_adj = in_adj_gaussian + in_adj_discrete\n    return {arg: in_adj}\n\n\n@adjoint_ops.register(Subs, ops.LogAddExpOp, ops.AddOp, Gaussian, GaussianMixture, tuple)\ndef adjoint_subs_gaussianmixture_discrete(adj_redop, adj_binop, out_adj, arg, subs):\n\n    if any(v.dtype == \'real\' and not isinstance(v, Variable) for k, v in subs):\n        raise NotImplementedError(""TODO implement adjoint for substitution into Gaussian real variable"")\n\n    out_adj_int_inputs = OrderedDict((k, v) for k, v in out_adj.inputs.items() if v.dtype != \'real\')\n    out_adj_ = out_adj + Tensor(out_adj.info_vec.new_zeros(out_adj.info_vec.shape[:-1]), out_adj_int_inputs)\n    return {arg: adjoint_ops(Subs, adj_redop, adj_binop, out_adj_, arg, subs)[arg]}\n\n\n@adjoint_ops.register(Subs, ops.LogAddExpOp, ops.AddOp, (GaussianMixture, Gaussian), Gaussian, tuple)\ndef adjoint_subs_gaussian_gaussian(adj_redop, adj_binop, out_adj, arg, subs):\n\n    if any(v.dtype == \'real\' and not isinstance(v, Variable) for k, v in subs):\n        raise NotImplementedError(""TODO implement adjoint for substitution into Gaussian real variable"")\n\n    arg_int_inputs = OrderedDict((k, v) for k, v in arg.inputs.items() if v.dtype != \'real\')\n    arg_ = arg + Tensor(arg.info_vec.new_zeros(arg.info_vec.shape[:-1]), arg_int_inputs)\n    return {arg: adjoint_ops(Subs, adj_redop, adj_binop, out_adj, arg_, subs)[arg_]}\n\n\n@adjoint_ops.register(Subs, ops.LogAddExpOp, ops.AddOp, (Number, Tensor), GaussianMixture, tuple)\ndef adjoint_subs_gaussianmixture_discrete(adj_redop, adj_binop, out_adj, arg, subs):\n\n    if any(v.dtype == \'real\' and not isinstance(v, Variable) for k, v in subs):\n        raise NotImplementedError(""TODO implement adjoint for substitution into Gaussian real variable"")\n\n    # invert renaming\n    renames = tuple((v.name, k) for k, v in subs if isinstance(v, Variable))\n    out_adj = Subs(out_adj, renames)\n\n    # inverting advanced indexing\n    slices = tuple((k, v) for k, v in subs if not isinstance(v, Variable))\n\n    arg_int_inputs = OrderedDict((k, v) for k, v in arg.inputs.items() if v.dtype != \'real\')\n\n    zeros_like_out = Subs(Tensor(arg.terms[1].info_vec.new_full(arg.terms[1].info_vec.shape[:-1], ops.UNITS[adj_binop]),\n                                 arg_int_inputs),\n                          slices)\n    out_adj = adj_binop(out_adj, zeros_like_out)\n\n    in_adj_discrete = adjoint_ops(Subs, adj_redop, adj_binop, out_adj, arg.terms[0], subs)[arg.terms[0]]\n\n    # invert the slicing for the Gaussian term even though the message does not affect the values\n    in_adj_info_vec = list(adjoint_ops(Subs, adj_redop, adj_binop,  # ops.add, ops.mul,\n                                       zeros_like_out,\n                                       Tensor(arg.terms[1].info_vec, arg_int_inputs),\n                                       slices).values())[0]\n\n    in_adj_precision = list(adjoint_ops(Subs, adj_redop, adj_binop,  # ops.add, ops.mul,\n                                        zeros_like_out,\n                                        Tensor(arg.terms[1].precision, arg_int_inputs),\n                                        slices).values())[0]\n\n    assert isinstance(in_adj_info_vec, Tensor)\n    assert isinstance(in_adj_precision, Tensor)\n\n    in_adj_gaussian = Gaussian(in_adj_info_vec.data, in_adj_precision.data, arg.inputs.copy())\n\n    in_adj = in_adj_gaussian + in_adj_discrete\n    return {arg: in_adj}\n'"
funsor/affine.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\nfrom functools import reduce, singledispatch\n\nimport opt_einsum\n\nfrom funsor.interpreter import gensym\nfrom funsor.tensor import Einsum, Tensor, get_default_prototype\nfrom funsor.terms import Binary, Funsor, Lambda, Reduce, Unary, Variable, bint\n\nfrom . import ops\n\n\ndef is_affine(fn):\n    """"""\n    A sound but incomplete test to determine whether a funsor is affine with\n    respect to all of its real inputs.\n\n    :param Funsor fn: A funsor.\n    :rtype: bool\n    """"""\n    return affine_inputs(fn) == _real_inputs(fn)\n\n\ndef _real_inputs(fn):\n    return frozenset(k for k, d in fn.inputs.items() if d.dtype == ""real"")\n\n\ndef affine_inputs(fn):\n    """"""\n    Returns a [sound sub]set of real inputs of ``fn``\n    wrt which ``fn`` is known to be affine.\n\n    :param Funsor fn: A funsor.\n    :return: A set of input names wrt which ``fn`` is affine.\n    :rtype: frozenset\n    """"""\n    result = getattr(fn, \'_affine_inputs\', None)\n    if result is None:\n        result = fn._affine_inputs = _affine_inputs(fn)\n    return result\n\n\n@singledispatch\ndef _affine_inputs(fn):\n    assert isinstance(fn, Funsor)\n    return frozenset()\n\n\n# Make registration public.\naffine_inputs.register = _affine_inputs.register\n\n\n@affine_inputs.register(Variable)\ndef _(fn):\n    return _real_inputs(fn)\n\n\n@affine_inputs.register(Unary)\ndef _(fn):\n    if fn.op in (ops.neg, ops.add) or isinstance(fn.op, ops.ReshapeOp):\n        return affine_inputs(fn.arg)\n    return frozenset()\n\n\n@affine_inputs.register(Binary)\ndef _(fn):\n    if fn.op in (ops.add, ops.sub):\n        return affine_inputs(fn.lhs) | affine_inputs(fn.rhs)\n    if fn.op is ops.truediv:\n        return affine_inputs(fn.lhs) - _real_inputs(fn.rhs)\n    if isinstance(fn.op, ops.GetitemOp):\n        return affine_inputs(fn.lhs)\n    if fn.op in (ops.mul, ops.matmul):\n        lhs_affine = affine_inputs(fn.lhs) - _real_inputs(fn.rhs)\n        rhs_affine = affine_inputs(fn.rhs) - _real_inputs(fn.lhs)\n        if not lhs_affine:\n            return rhs_affine\n        if not rhs_affine:\n            return lhs_affine\n        # This multilinear case introduces incompleteness, since some vars\n        # could later be reduced, making remaining vars affine.\n        return frozenset()\n    return frozenset()\n\n\n@affine_inputs.register(Reduce)\ndef _(fn):\n    return affine_inputs(fn.arg) - fn.reduced_vars\n\n\n@affine_inputs.register(Einsum)\ndef _(fn):\n    # This is simply a multiary version of the above Binary(ops.mul, ...) case.\n    results = []\n    for i, x in enumerate(fn.operands):\n        others = fn.operands[:i] + fn.operands[i+1:]\n        other_inputs = reduce(ops.or_, map(_real_inputs, others), frozenset())\n        results.append(affine_inputs(x) - other_inputs)\n    # This multilinear case introduces incompleteness, since some vars\n    # could later be reduced, making remaining vars affine.\n    if sum(map(bool, results)) == 1:\n        for result in results:\n            if result:\n                return result\n    return frozenset()\n\n\ndef extract_affine(fn):\n    """"""\n    Extracts an affine representation of a funsor, satisfying::\n\n        x = ...\n        const, coeffs = extract_affine(x)\n        y = sum(Einsum(eqn, (coeff, Variable(var, coeff.output)))\n                for var, (coeff, eqn) in coeffs.items())\n        assert_close(y, x)\n        assert frozenset(coeffs) == affine_inputs(x)\n\n    The ``coeffs`` will have one key per input wrt which ``fn`` is known to be\n    affine (via :func:`affine_inputs` ), and ``const`` and ``coeffs.values``\n    will all be constant wrt these inputs.\n\n    The affine approximation is computed by ev evaluating ``fn`` at\n    zero and each basis vector. To improve performance, users may want to run\n    under the :func:`~funsor.memoize.memoize` interpretation.\n\n    :param Funsor fn: A funsor that is affine wrt the (add,mul) semiring in\n        some subset of its inputs.\n    :return: A pair ``(const, coeffs)`` where const is a funsor with no real\n        inputs and ``coeffs`` is an OrderedDict mapping input name to a\n        ``(coefficient, eqn)`` pair in einsum form.\n    :rtype: tuple\n    """"""\n    # NB: this depends on the global default backend.\n    prototype = get_default_prototype()\n    # Determine constant part by evaluating fn at zero.\n    inputs = affine_inputs(fn)\n    inputs = OrderedDict((k, v) for k, v in fn.inputs.items() if k in inputs)\n    zeros = {k: Tensor(ops.new_zeros(prototype, v.shape)) for k, v in inputs.items()}\n    const = fn(**zeros)\n\n    # Determine linear coefficients by evaluating fn on basis vectors.\n    name = gensym(\'probe\')\n    coeffs = OrderedDict()\n    for k, v in inputs.items():\n        dim = v.num_elements\n        var = Variable(name, bint(dim))\n        subs = zeros.copy()\n        subs[k] = Tensor(ops.new_eye(prototype, (dim,)).reshape((dim,) + v.shape))[var]\n        coeff = Lambda(var, fn(**subs) - const).reshape(v.shape + const.shape)\n        inputs1 = \'\'.join(map(opt_einsum.get_symbol, range(len(coeff.shape))))\n        inputs2 = inputs1[:len(v.shape)]\n        output = inputs1[len(v.shape):]\n        eqn = f\'{inputs1},{inputs2}->{output}\'\n        coeffs[k] = coeff, eqn\n    return const, coeffs\n\n\n__all__ = [\n    ""affine_inputs"",\n    ""extract_affine"",\n    ""is_affine"",\n]\n'"
funsor/cnf.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport itertools\nfrom collections import OrderedDict, defaultdict\nfrom functools import reduce\nfrom typing import Tuple, Union\n\nimport opt_einsum\nfrom multipledispatch.variadic import Variadic\n\nimport funsor.ops as ops\nfrom funsor.affine import affine_inputs\nfrom funsor.delta import Delta\nfrom funsor.domains import find_domain\nfrom funsor.gaussian import Gaussian\nfrom funsor.interpreter import interpretation, recursion_reinterpret\nfrom funsor.ops import DISTRIBUTIVE_OPS, AssociativeOp, NullOp, nullop\nfrom funsor.tensor import Tensor\nfrom funsor.terms import (\n    Align,\n    Binary,\n    Funsor,\n    Number,\n    Reduce,\n    Subs,\n    Unary,\n    Variable,\n    eager,\n    normalize,\n    reflect,\n    to_funsor\n)\nfrom funsor.util import broadcast_shape, get_backend, quote\n\n\nclass Contraction(Funsor):\n    """"""\n    Declarative representation of a finitary sum-product operation.\n\n    After normalization via the :func:`~funsor.terms.normalize` interpretation\n    contractions will canonically order their terms by type::\n\n        Delta, Number, Tensor, Gaussian\n    """"""\n    def __init__(self, red_op, bin_op, reduced_vars, terms):\n        terms = (terms,) if isinstance(terms, Funsor) else terms\n        assert isinstance(red_op, AssociativeOp)\n        assert isinstance(bin_op, AssociativeOp)\n        assert all(isinstance(v, Funsor) for v in terms)\n        assert isinstance(reduced_vars, frozenset)\n        assert all(isinstance(v, str) for v in reduced_vars)\n        assert isinstance(terms, tuple) and len(terms) > 0\n\n        assert not (isinstance(red_op, NullOp) and isinstance(bin_op, NullOp))\n        if isinstance(red_op, NullOp):\n            assert not reduced_vars\n        elif isinstance(bin_op, NullOp):\n            assert len(terms) == 1\n        else:\n            assert reduced_vars and len(terms) > 1\n            assert (red_op, bin_op) in DISTRIBUTIVE_OPS\n\n        inputs = OrderedDict()\n        for v in terms:\n            inputs.update((k, d) for k, d in v.inputs.items() if k not in reduced_vars)\n\n        if bin_op is nullop:\n            output = terms[0].output\n        else:\n            output = reduce(lambda lhs, rhs: find_domain(bin_op, lhs, rhs),\n                            [v.output for v in reversed(terms)])\n        fresh = frozenset()\n        bound = reduced_vars\n        super(Contraction, self).__init__(inputs, output, fresh, bound)\n        self.red_op = red_op\n        self.bin_op = bin_op\n        self.terms = terms\n        self.reduced_vars = reduced_vars\n\n    def unscaled_sample(self, sampled_vars, sample_inputs, rng_key=None):\n        sampled_vars = sampled_vars.intersection(self.inputs)\n        if not sampled_vars:\n            return self\n\n        if self.red_op in (ops.logaddexp, nullop):\n            if self.bin_op in (ops.nullop, ops.logaddexp):\n                if rng_key is not None:\n                    import jax\n                    rng_keys = jax.random.split(rng_key, len(self.terms))\n                else:\n                    rng_keys = [None] * len(self.terms)\n\n                # Design choice: we sample over logaddexp reductions, but leave logaddexp\n                # binary choices symbolic.\n                terms = [\n                    term.unscaled_sample(sampled_vars.intersection(term.inputs), sample_inputs)\n                    for term, rng_key in zip(self.terms, rng_keys)]\n                return Contraction(self.red_op, self.bin_op, self.reduced_vars, *terms)\n\n            if self.bin_op is ops.add:\n                if rng_key is not None:\n                    import jax\n                    rng_keys = jax.random.split(rng_key)\n                else:\n                    rng_keys = [None] * 2\n\n                # Sample variables greedily in order of the terms in which they appear.\n                for term in self.terms:\n                    greedy_vars = sampled_vars.intersection(term.inputs)\n                    if greedy_vars:\n                        break\n                greedy_terms, terms = [], []\n                for term in self.terms:\n                    (terms if greedy_vars.isdisjoint(term.inputs) else greedy_terms).append(term)\n                if len(greedy_terms) == 1:\n                    term = greedy_terms[0]\n                    terms.append(term.unscaled_sample(greedy_vars, sample_inputs, rng_keys[0]))\n                    result = Contraction(self.red_op, self.bin_op, self.reduced_vars, *terms)\n                elif (len(greedy_terms) == 2 and\n                        isinstance(greedy_terms[0], Tensor) and\n                        isinstance(greedy_terms[1], Gaussian)):\n                    discrete, gaussian = greedy_terms\n                    term = discrete + gaussian.log_normalizer\n                    terms.append(gaussian)\n                    terms.append(-gaussian.log_normalizer)\n                    terms.append(term.unscaled_sample(greedy_vars, sample_inputs, rng_keys[0]))\n                    result = Contraction(self.red_op, self.bin_op, self.reduced_vars, *terms)\n                else:\n                    raise NotImplementedError(\'Unhandled case: {}\'.format(\n                        \', \'.join(str(type(t)) for t in greedy_terms)))\n                return result.unscaled_sample(sampled_vars - greedy_vars, sample_inputs, rng_keys[1])\n\n        raise TypeError(""Cannot sample through ops ({}, {})"".format(self.red_op, self.bin_op))\n\n    def align(self, names):\n        assert isinstance(names, tuple)\n        assert all(name in self.inputs for name in names)\n        new_terms = tuple(t.align(tuple(n for n in names if n in t.inputs)) for t in self.terms)\n        result = Contraction(self.red_op, self.bin_op, self.reduced_vars, *new_terms)\n        if not names == tuple(result.inputs):\n            return Align(result, names)  # raise NotImplementedError(""TODO align all terms"")\n        return result\n\n    def _alpha_convert(self, alpha_subs):\n        reduced_vars = frozenset(alpha_subs.get(k, k) for k in self.reduced_vars)\n        bound_types = {}\n        for term in self.terms:\n            bound_types.update({k: term.inputs[k] for k in self.bound.intersection(term.inputs)})\n        alpha_subs = {k: to_funsor(v, bound_types[k]) for k, v in alpha_subs.items()}\n        red_op, bin_op, _, terms = super()._alpha_convert(alpha_subs)\n        return red_op, bin_op, reduced_vars, terms\n\n\nGaussianMixture = Contraction[Union[ops.LogAddExpOp, NullOp], ops.AddOp, frozenset,\n                              Tuple[Union[Tensor, Number], Gaussian]]\n\n\n@quote.register(Contraction)\ndef _(arg, indent, out):\n    line = f""{type(arg).__name__}({repr(arg.red_op)}, {repr(arg.bin_op)},""\n    out.append((indent, line))\n    quote.inplace(arg.reduced_vars, indent + 1, out)\n    i, line = out[-1]\n    out[-1] = i, line + "",""\n    quote.inplace(arg.terms, indent + 1, out)\n    i, line = out[-1]\n    out[-1] = i, line + "")""\n\n\n@recursion_reinterpret.register(Contraction)\ndef recursion_reinterpret_contraction(x):\n    return type(x)(*map(recursion_reinterpret, (x.red_op, x.bin_op, x.reduced_vars) + x.terms))\n\n\n@eager.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Variadic[Funsor])\ndef eager_contraction_generic_to_tuple(red_op, bin_op, reduced_vars, *terms):\n    return eager(Contraction, red_op, bin_op, reduced_vars, terms)\n\n\n@eager.register(Contraction, AssociativeOp, AssociativeOp, frozenset, tuple)\ndef eager_contraction_generic_recursive(red_op, bin_op, reduced_vars, terms):\n    # push down leaf reductions\n    terms, reduced_vars, leaf_reduced = list(terms), frozenset(reduced_vars), False\n    for i, v in enumerate(terms):\n        unique_vars = reduced_vars.intersection(v.inputs) - \\\n            frozenset().union(*(reduced_vars.intersection(vv.inputs) for vv in terms if vv is not v))\n        if unique_vars:\n            result = v.reduce(red_op, unique_vars)\n            if result is not normalize(Contraction, red_op, nullop, unique_vars, (v,)):\n                terms[i] = result\n                reduced_vars -= unique_vars\n                leaf_reduced = True\n\n    if leaf_reduced:\n        return Contraction(red_op, bin_op, reduced_vars, *terms)\n\n    # exploit associativity to recursively evaluate this contraction\n    # a bit expensive, but handles interpreter-imposed directionality constraints\n    terms = tuple(terms)\n    for i, lhs in enumerate(terms[0:-1]):\n        for j_, rhs in enumerate(terms[i+1:]):\n            j = i + j_ + 1\n            unique_vars = reduced_vars.intersection(lhs.inputs, rhs.inputs) - \\\n                frozenset().union(*(reduced_vars.intersection(vv.inputs)\n                                    for vv in terms[:i] + terms[i+1:j] + terms[j+1:]))\n            result = Contraction(red_op, bin_op, unique_vars, lhs, rhs)\n            if result is not normalize(Contraction, red_op, bin_op, unique_vars, (lhs, rhs)):  # did we make progress?\n                # pick the first evaluable pair\n                reduced_vars -= unique_vars\n                new_terms = terms[:i] + (result,) + terms[i+1:j] + terms[j+1:]\n                return Contraction(red_op, bin_op, reduced_vars, *new_terms)\n\n    return None\n\n\n@eager.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Funsor)\ndef eager_contraction_to_reduce(red_op, bin_op, reduced_vars, term):\n    args = red_op, term, reduced_vars\n    return eager.dispatch(Reduce, *args)(*args)\n\n\n@eager.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Funsor, Funsor)\ndef eager_contraction_to_binary(red_op, bin_op, reduced_vars, lhs, rhs):\n\n    if reduced_vars - (reduced_vars.intersection(lhs.inputs, rhs.inputs)):\n        args = red_op, bin_op, reduced_vars, (lhs, rhs)\n        result = eager.dispatch(Contraction, *args)(*args)\n        if result is not None:\n            return result\n\n    args = bin_op, lhs, rhs\n    result = eager.dispatch(Binary, *args)(*args)\n    if result is not None and reduced_vars:\n        args = red_op, result, reduced_vars\n        result = eager.dispatch(Reduce, *args)(*args)\n    return result\n\n\n@eager.register(Contraction, ops.AddOp, ops.MulOp, frozenset, Tensor, Tensor)\ndef eager_contraction_tensor(red_op, bin_op, reduced_vars, *terms):\n    if not all(term.dtype == ""real"" for term in terms):\n        raise NotImplementedError(\'TODO\')\n    backend = BACKEND_TO_EINSUM_BACKEND[get_backend()]\n    return _eager_contract_tensors(reduced_vars, terms, backend=backend)\n\n\n@eager.register(Contraction, ops.LogAddExpOp, ops.AddOp, frozenset, Tensor, Tensor)\ndef eager_contraction_tensor(red_op, bin_op, reduced_vars, *terms):\n    if not all(term.dtype == ""real"" for term in terms):\n        raise NotImplementedError(\'TODO\')\n    backend = BACKEND_TO_LOGSUMEXP_BACKEND[get_backend()]\n    return _eager_contract_tensors(reduced_vars, terms, backend=backend)\n\n\n# TODO Consider using this for more than binary contractions.\ndef _eager_contract_tensors(reduced_vars, terms, backend):\n    iter_symbols = map(opt_einsum.get_symbol, itertools.count())\n    symbols = defaultdict(functools.partial(next, iter_symbols))\n\n    inputs = OrderedDict()\n    einsum_inputs = []\n    operands = []\n    for term in terms:\n        inputs.update(term.inputs)\n        einsum_inputs.append("""".join(symbols[k] for k in term.inputs) +\n                             """".join(symbols[i - len(term.shape)]\n                                     for i, size in enumerate(term.shape)\n                                     if size != 1))\n\n        # Squeeze absent event dims to be compatible with einsum.\n        data = term.data\n        batch_shape = data.shape[:len(data.shape) - len(term.shape)]\n        event_shape = tuple(size for size in term.shape if size != 1)\n        data = data.reshape(batch_shape + event_shape)\n        operands.append(data)\n\n    for k in reduced_vars:\n        del inputs[k]\n    batch_shape = tuple(v.size for v in inputs.values())\n    event_shape = broadcast_shape(*(term.shape for term in terms))\n    einsum_output = ("""".join(symbols[k] for k in inputs) +\n                     """".join(symbols[dim]\n                             for dim in range(-len(event_shape), 0)\n                             if dim in symbols))\n    equation = "","".join(einsum_inputs) + ""->"" + einsum_output\n    data = opt_einsum.contract(equation, *operands, backend=backend)\n    data = data.reshape(batch_shape + event_shape)\n    return Tensor(data, inputs)\n\n\n# TODO(https://github.com/pyro-ppl/funsor/issues/238) Use a port of\n# Pyro\'s gaussian_tensordot() here. Until then we must eagerly add the\n# possibly-rank-deficient terms before reducing to avoid Cholesky errors.\n@eager.register(Contraction, ops.LogAddExpOp, ops.AddOp, frozenset,\n                GaussianMixture, GaussianMixture)\ndef eager_contraction_gaussian(red_op, bin_op, reduced_vars, x, y):\n    return (x + y).reduce(red_op, reduced_vars)\n\n\n@affine_inputs.register(Contraction)\ndef _(fn):\n    with interpretation(reflect):\n        flat = reduce(fn.bin_op, fn.terms).reduce(fn.red_op, fn.reduced_vars)\n    return affine_inputs(flat)\n\n\n##########################################\n# Normalizing Contractions\n##########################################\n\nORDERING = {Delta: 1, Number: 2, Tensor: 3, Gaussian: 4}\nGROUND_TERMS = tuple(ORDERING)\n\n\n@normalize.register(Contraction, AssociativeOp, ops.AddOp, frozenset, GROUND_TERMS, GROUND_TERMS)\ndef normalize_contraction_commutative_canonical_order(red_op, bin_op, reduced_vars, *terms):\n    # when bin_op is commutative, put terms into a canonical order for pattern matching\n    new_terms = tuple(\n        v for i, v in sorted(enumerate(terms),\n                             key=lambda t: (ORDERING.get(type(t[1]).__origin__, -1), t[0]))\n    )\n    if any(v is not vv for v, vv in zip(terms, new_terms)):\n        return Contraction(red_op, bin_op, reduced_vars, *new_terms)\n    return normalize(Contraction, red_op, bin_op, reduced_vars, new_terms)\n\n\n@normalize.register(Contraction, AssociativeOp, ops.AddOp, frozenset, GaussianMixture, GROUND_TERMS)\ndef normalize_contraction_commute_joint(red_op, bin_op, reduced_vars, mixture, other):\n    return Contraction(mixture.red_op if red_op is nullop else red_op, bin_op,\n                       reduced_vars | mixture.reduced_vars, *(mixture.terms + (other,)))\n\n\n@normalize.register(Contraction, AssociativeOp, ops.AddOp, frozenset, GROUND_TERMS, GaussianMixture)\ndef normalize_contraction_commute_joint(red_op, bin_op, reduced_vars, other, mixture):\n    return Contraction(mixture.red_op if red_op is nullop else red_op, bin_op,\n                       reduced_vars | mixture.reduced_vars, *(mixture.terms + (other,)))\n\n\n@normalize.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Variadic[Funsor])\ndef normalize_contraction_generic_args(red_op, bin_op, reduced_vars, *terms):\n    return normalize(Contraction, red_op, bin_op, reduced_vars, tuple(terms))\n\n\n@normalize.register(Contraction, NullOp, NullOp, frozenset, Funsor)\ndef normalize_trivial(red_op, bin_op, reduced_vars, term):\n    assert not reduced_vars\n    return term\n\n\n@normalize.register(Contraction, AssociativeOp, AssociativeOp, frozenset, tuple)\ndef normalize_contraction_generic_tuple(red_op, bin_op, reduced_vars, terms):\n\n    if not reduced_vars and red_op is not nullop:\n        return Contraction(nullop, bin_op, reduced_vars, *terms)\n\n    if len(terms) == 1 and bin_op is not nullop:\n        return Contraction(red_op, nullop, reduced_vars, *terms)\n\n    if red_op is nullop and bin_op is nullop:\n        return terms[0]\n\n    if red_op is bin_op:\n        new_terms = tuple(v.reduce(red_op, reduced_vars) for v in terms)\n        return Contraction(red_op, bin_op, frozenset(), *new_terms)\n\n    if bin_op in ops.UNITS and any(isinstance(t, Number) and t.data == ops.UNITS[bin_op] for t in terms):\n        new_terms = tuple(t for t in terms if not (isinstance(t, Number) and t.data == ops.UNITS[bin_op]))\n        if not new_terms:  # everything was a unit\n            new_terms = (terms[0],)\n        return Contraction(red_op, bin_op, reduced_vars, *new_terms)\n\n    for i, v in enumerate(terms):\n\n        if not isinstance(v, Contraction):\n            continue\n\n        # fuse operations without distributing\n        if (v.red_op is nullop and bin_op is v.bin_op) or \\\n                (bin_op is nullop and v.red_op in (red_op, nullop)):\n            red_op = v.red_op if red_op is nullop else red_op\n            bin_op = v.bin_op if bin_op is nullop else bin_op\n            new_terms = terms[:i] + v.terms + terms[i+1:]\n            return Contraction(red_op, bin_op, reduced_vars | v.reduced_vars, *new_terms)\n\n    # nothing more to do, reflect\n    return None\n\n\n#########################################\n# Creating Contractions from other terms\n#########################################\n\n@normalize.register(Binary, AssociativeOp, Funsor, Funsor)\ndef binary_to_contract(op, lhs, rhs):\n    return Contraction(nullop, op, frozenset(), lhs, rhs)\n\n\n@normalize.register(Reduce, AssociativeOp, Funsor, frozenset)\ndef reduce_funsor(op, arg, reduced_vars):\n    return Contraction(op, nullop, reduced_vars, arg)\n\n\n@normalize.register(Unary, ops.NegOp, (Variable, Contraction[ops.AssociativeOp, ops.MulOp, frozenset, tuple]))\ndef unary_neg_variable(op, arg):\n    return arg * -1\n\n\n#######################################################################\n# Distributing Unary transformations (Subs, log, exp, neg, reciprocal)\n#######################################################################\n\n@normalize.register(Subs, Funsor, tuple)\ndef do_fresh_subs(arg, subs):\n    if not subs:\n        return arg\n    if all(name in arg.fresh for name, sub in subs):\n        return arg.eager_subs(subs)\n    return None\n\n\n@normalize.register(Subs, Contraction, tuple)\ndef distribute_subs_contraction(arg, subs):\n    new_terms = tuple(Subs(v, tuple((name, sub) for name, sub in subs if name in v.inputs))\n                      if any(name in v.inputs for name, sub in subs)\n                      else v\n                      for v in arg.terms)\n    return Contraction(arg.red_op, arg.bin_op, arg.reduced_vars, *new_terms)\n\n\n@normalize.register(Subs, Subs, tuple)\ndef normalize_fuse_subs(arg, subs):\n    # a(b)(c) -> a(b(c), c)\n    arg_subs = tuple(arg.subs.items()) if isinstance(arg.subs, OrderedDict) else arg.subs\n    new_subs = subs + tuple((k, Subs(v, subs)) for k, v in arg_subs)\n    return Subs(arg.arg, new_subs)\n\n\n@normalize.register(Binary, ops.SubOp, Funsor, Funsor)\ndef binary_subtract(op, lhs, rhs):\n    return lhs + -rhs\n\n\n@normalize.register(Binary, ops.DivOp, Funsor, Funsor)\ndef binary_divide(op, lhs, rhs):\n    return lhs * Unary(ops.reciprocal, rhs)\n\n\n@normalize.register(Unary, ops.ExpOp, Unary[ops.LogOp, Funsor])\n@normalize.register(Unary, ops.LogOp, Unary[ops.ExpOp, Funsor])\n@normalize.register(Unary, ops.NegOp, Unary[ops.NegOp, Funsor])\n@normalize.register(Unary, ops.ReciprocalOp, Unary[ops.ReciprocalOp, Funsor])\ndef unary_log_exp(op, arg):\n    return arg.arg\n\n\n@normalize.register(Unary, ops.ReciprocalOp, Contraction[NullOp, ops.MulOp, frozenset, tuple])\n@normalize.register(Unary, ops.NegOp, Contraction[NullOp, ops.AddOp, frozenset, tuple])\ndef unary_contract(op, arg):\n    return Contraction(arg.red_op, arg.bin_op, arg.reduced_vars, *(op(t) for t in arg.terms))\n\n\nBACKEND_TO_EINSUM_BACKEND = {\n    ""numpy"": ""numpy"",\n    ""torch"": ""torch"",\n    ""jax"": ""jax.numpy"",\n}\n# NB: numpy_log, numpy_map is backend-agnostic so they also work for torch backend;\n# however, we might need to profile to make a switch\nBACKEND_TO_LOGSUMEXP_BACKEND = {\n    ""numpy"": ""funsor.einsum.numpy_log"",\n    ""torch"": ""pyro.ops.einsum.torch_log"",\n    ""jax"": ""funsor.einsum.numpy_log"",\n}\nBACKEND_TO_MAP_BACKEND = {\n    ""numpy"": ""funsor.einsum.numpy_map"",\n    ""torch"": ""pyro.ops.einsum.torch_map"",\n    ""jax"": ""funsor.einsum.numpy_map"",\n}\n'"
funsor/delta.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport funsor.ops as ops\nfrom funsor.domains import Domain, reals\nfrom funsor.interpreter import debug_logged\nfrom funsor.ops import AddOp, SubOp, TransformOp\nfrom funsor.registry import KeyedRegistry\nfrom funsor.terms import (\n    Align,\n    Binary,\n    Funsor,\n    FunsorMeta,\n    Independent,\n    Lambda,\n    Number,\n    Unary,\n    Variable,\n    eager,\n    to_funsor\n)\n\n\ndef solve(expr, value):\n    """"""\n    Tries to solve for free inputs of an ``expr`` such that ``expr == value``,\n    and computes the log-abs-det-Jacobian of the resulting substitution.\n\n    :param Funsor expr: An expression with a free variable.\n    :param Funsor value: A target value.\n    :return: A tuple ``(name, point, log_abs_det_jacobian)``\n    :rtype: tuple\n    :raises: ValueError\n    """"""\n    assert isinstance(expr, Funsor)\n    assert isinstance(value, Funsor)\n    result = solve.dispatch(type(expr), *(expr._ast_values + (value,)))\n    if result is None:\n        raise ValueError(""Cannot substitute into a Delta: {}"".format(value))\n    return result\n\n\n_solve = KeyedRegistry(lambda *args: None)\nsolve.dispatch = _solve.__call__\nsolve.register = _solve.register\n\n\n@solve.register(Variable, str, Domain, Funsor)\n@debug_logged\ndef solve_variable(name, output, y):\n    assert y.output == output\n    point = y\n    log_density = Number(0)\n    return name, point, log_density\n\n\n@solve.register(Unary, TransformOp, Funsor, Funsor)\n@debug_logged\ndef solve_unary(op, arg, y):\n    x = op.inv(y)\n    name, point, log_density = solve(arg, x)\n    log_density += op.log_abs_det_jacobian(x, y)\n    return name, point, log_density\n\n\nclass DeltaMeta(FunsorMeta):\n    """"""\n    Makes Delta less of a pain to use by supporting Delta(name, point, log_density)\n    """"""\n    def __call__(cls, *args):\n        if len(args) > 1:\n            assert len(args) == 2 or len(args) == 3\n            assert isinstance(args[0], str) and isinstance(args[1], Funsor)\n            args = args + (Number(0.),) if len(args) == 2 else args\n            args = (((args[0], (to_funsor(args[1]), to_funsor(args[2]))),),)\n        assert isinstance(args[0], tuple)\n        return super().__call__(args[0])\n\n\nclass Delta(Funsor, metaclass=DeltaMeta):\n    """"""\n    Normalized delta distribution binding multiple variables.\n    """"""\n    def __init__(self, terms):\n        assert isinstance(terms, tuple) and len(terms) > 0\n        inputs = OrderedDict()\n        for name, (point, log_density) in terms:\n            assert isinstance(name, str)\n            assert isinstance(point, Funsor)\n            assert isinstance(log_density, Funsor)\n            assert log_density.output == reals()\n            assert name not in inputs\n            assert name not in point.inputs\n            inputs.update({name: point.output})\n            inputs.update(point.inputs)\n\n        output = reals()\n        fresh = frozenset(name for name, term in terms)\n        bound = frozenset()\n        super(Delta, self).__init__(inputs, output, fresh, bound)\n        self.terms = terms\n\n    def align(self, names):\n        assert isinstance(names, tuple)\n        assert all(name in self.fresh for name in names)\n        if not names or names == tuple(n for n, p in self.terms):\n            return self\n\n        new_terms = sorted(self.terms, key=lambda t: names.index(t[0]))\n        return Delta(new_terms)\n\n    def eager_subs(self, subs):\n        terms = OrderedDict(self.terms)\n        new_terms = terms.copy()\n        log_density = Number(0)\n        for name, value in subs:\n            if isinstance(value, Variable):\n                new_terms[value.name] = new_terms.pop(name)\n                continue\n\n            if not any(d.dtype == \'real\' for side in (value, terms[name][0])\n                       for d in side.inputs.values()):\n                point, point_log_density = new_terms.pop(name)\n                log_density += (value == point).all().log() + point_log_density\n                continue\n\n            # Try to invert the substitution.\n            soln = solve(value, terms[name][0])\n            if soln is None:\n                return None  # lazily substitute\n            new_name, new_point, point_log_density = soln\n            old_point, old_point_density = new_terms.pop(name)\n            new_terms[new_name] = (new_point, old_point_density + point_log_density)\n\n        return Delta(tuple(new_terms.items())) + log_density if new_terms else log_density\n\n    def eager_reduce(self, op, reduced_vars):\n        if op is ops.logaddexp:\n            if reduced_vars - self.fresh and self.fresh - reduced_vars:\n                result = self.eager_reduce(op, reduced_vars & self.fresh) if reduced_vars & self.fresh else self\n                if result is not self:\n                    result = result.eager_reduce(op, reduced_vars - self.fresh) if reduced_vars - self.fresh else self\n                    return result if result is not self else None\n                return None\n\n            result_terms = [(name, (point, log_density)) for name, (point, log_density) in self.terms\n                            if name not in reduced_vars]\n\n            result_terms, scale = [], Number(0)\n            for name, (point, log_density) in self.terms:\n                if name in reduced_vars:\n                    # XXX obscenely wasteful - need a lazy Zero term\n                    if point.inputs:\n                        scale += (point == point).all().log()\n                    if log_density.inputs:\n                        scale += log_density * 0.\n                else:\n                    result_terms.append((name, (point, log_density)))\n\n            result = Delta(tuple(result_terms)) + scale if result_terms else scale\n            return result.reduce(op, reduced_vars - self.fresh)\n\n        if op is ops.add:\n            raise NotImplementedError(""TODO Implement ops.add to simulate .to_event()."")\n\n        return None  # defer to default implementation\n\n    def unscaled_sample(self, sampled_vars, sample_inputs, rng_key=None):\n        return self\n\n\n@eager.register(Binary, AddOp, Delta, Delta)\ndef eager_add_multidelta(op, lhs, rhs):\n    if lhs.fresh.intersection(rhs.inputs):\n        return eager_add_delta_funsor(op, lhs, rhs)\n\n    if rhs.fresh.intersection(lhs.inputs):\n        return eager_add_funsor_delta(op, lhs, rhs)\n\n    return Delta(lhs.terms + rhs.terms)\n\n\n@eager.register(Binary, (AddOp, SubOp), Delta, (Funsor, Align))\ndef eager_add_delta_funsor(op, lhs, rhs):\n    if lhs.fresh.intersection(rhs.inputs):\n        rhs = rhs(**{name: point for name, (point, log_density) in lhs.terms if name in rhs.inputs})\n        return op(lhs, rhs)\n\n    return None  # defer to default implementation\n\n\n@eager.register(Binary, AddOp, (Funsor, Align), Delta)\ndef eager_add_funsor_delta(op, lhs, rhs):\n    if rhs.fresh.intersection(lhs.inputs):\n        lhs = lhs(**{name: point for name, (point, log_density) in rhs.terms if name in lhs.inputs})\n        return op(lhs, rhs)\n\n    return None\n\n\n@eager.register(Independent, Delta, str, str, str)\ndef eager_independent_delta(delta, reals_var, bint_var, diag_var):\n    for i, (name, (point, log_density)) in enumerate(delta.terms):\n        if name == diag_var:\n            bv = Variable(bint_var, delta.inputs[bint_var])\n            point = Lambda(bv, point)\n            if bint_var in log_density.inputs:\n                log_density = log_density.reduce(ops.add, bint_var)\n            else:\n                log_density = log_density * delta.inputs[bint_var].dtype\n            new_terms = delta.terms[:i] + ((reals_var, (point, log_density)),) + delta.terms[i+1:]\n            return Delta(new_terms)\n\n    return None\n\n\n__all__ = [\n    \'Delta\',\n    \'solve\',\n]\n'"
funsor/distribution.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport inspect\nimport math\nimport typing\nfrom collections import OrderedDict\nfrom importlib import import_module\n\nimport makefun\n\nimport funsor.delta\nimport funsor.ops as ops\nfrom funsor.affine import is_affine\nfrom funsor.cnf import GaussianMixture\nfrom funsor.domains import Domain, reals\nfrom funsor.gaussian import Gaussian\nfrom funsor.interpreter import gensym\nfrom funsor.tensor import (Tensor, align_tensors, dummy_numeric_array, get_default_prototype,\n                           ignore_jit_warnings, numeric_array, stack)\nfrom funsor.terms import Funsor, FunsorMeta, Independent, Number, Variable, eager, to_data, to_funsor\nfrom funsor.util import broadcast_shape, get_backend\n\n\nBACKEND_TO_DISTRIBUTIONS_BACKEND = {\n    ""torch"": ""funsor.torch.distributions"",\n    ""jax"": ""funsor.jax.distributions"",\n}\n\n\ndef numbers_to_tensors(*args):\n    """"""\n    Convert :class:`~funsor.terms.Number`s to :class:`funsor.tensor.Tensor`s,\n    using any provided tensor as a prototype, if available.\n    """"""\n    if any(isinstance(x, Number) for x in args):\n        prototype = get_default_prototype()\n        options = dict(dtype=prototype.dtype)\n        for x in args:\n            if isinstance(x, Tensor):\n                options = dict(dtype=x.data.dtype, device=getattr(x.data, ""device"", None))\n                break\n        with ignore_jit_warnings():\n            args = tuple(Tensor(numeric_array(x.data, **options), dtype=x.dtype)\n                         if isinstance(x, Number) else x\n                         for x in args)\n    return args\n\n\nclass DistributionMeta(FunsorMeta):\n    """"""\n    Wrapper to fill in default values and convert Numbers to Tensors.\n    """"""\n    def __call__(cls, *args, **kwargs):\n        kwargs.update(zip(cls._ast_fields, args))\n        value = kwargs.pop(\'value\', \'value\')\n        kwargs = OrderedDict(\n            (k, to_funsor(kwargs[k], output=cls._infer_param_domain(k, getattr(kwargs[k], ""shape"", ()))))\n            for k in cls._ast_fields if k != \'value\')\n        value = to_funsor(value, output=cls._infer_value_domain(**{k: v.output for k, v in kwargs.items()}))\n        args = numbers_to_tensors(*(tuple(kwargs.values()) + (value,)))\n        return super(DistributionMeta, cls).__call__(*args)\n\n\nclass Distribution(Funsor, metaclass=DistributionMeta):\n    r""""""\n    Funsor backed by a PyTorch/JAX distribution object.\n\n    :param \\*args: Distribution-dependent parameters.  These can be either\n        funsors or objects that can be coerced to funsors via\n        :func:`~funsor.terms.to_funsor` . See derived classes for details.\n    """"""\n    dist_class = ""defined by derived classes""\n\n    def __init__(self, *args):\n        params = tuple(zip(self._ast_fields, args))\n        assert any(k == \'value\' for k, v in params)\n        inputs = OrderedDict()\n        for name, value in params:\n            assert isinstance(name, str)\n            assert isinstance(value, Funsor)\n            inputs.update(value.inputs)\n        inputs = OrderedDict(inputs)\n        output = reals()\n        super(Distribution, self).__init__(inputs, output)\n        self.params = OrderedDict(params)\n\n    def __repr__(self):\n        return \'{}({})\'.format(type(self).__name__,\n                               \', \'.join(\'{}={}\'.format(*kv) for kv in self.params.items()))\n\n    def eager_reduce(self, op, reduced_vars):\n        if op is ops.logaddexp and isinstance(self.value, Variable) and self.value.name in reduced_vars:\n            return Number(0.)  # distributions are normalized\n        return super(Distribution, self).eager_reduce(op, reduced_vars)\n\n    @classmethod\n    def eager_log_prob(cls, *params):\n        inputs, tensors = align_tensors(*params)\n        params = dict(zip(cls._ast_fields, tensors))\n        value = params.pop(\'value\')\n        data = cls.dist_class(**params).log_prob(value)\n        return Tensor(data, inputs)\n\n    def unscaled_sample(self, sampled_vars, sample_inputs, rng_key=None):\n        params = OrderedDict(self.params)\n        value = params.pop(""value"")\n        assert all(isinstance(v, (Number, Tensor)) for v in params.values())\n        assert isinstance(value, Variable) and value.name in sampled_vars\n        inputs_, tensors = align_tensors(*params.values())\n        inputs = OrderedDict(sample_inputs.items())\n        inputs.update(inputs_)\n        sample_shape = tuple(v.size for v in sample_inputs.values())\n\n        raw_dist = self.dist_class(**dict(zip(self._ast_fields[:-1], tensors)))\n        sample_args = (sample_shape,) if rng_key is None else (rng_key, sample_shape)\n        if getattr(raw_dist, ""has_rsample"", False):\n            raw_sample = raw_dist.rsample(*sample_args)\n        else:\n            raw_sample = ops.detach(raw_dist.sample(*sample_args))\n\n        result = funsor.delta.Delta(value.name, Tensor(raw_sample, inputs, value.output.dtype))\n        if not getattr(raw_dist, ""has_rsample"", False):\n            # scaling of dice_factor by num samples should already be handled by Funsor.sample\n            raw_log_prob = raw_dist.log_prob(raw_sample)\n            dice_factor = Tensor(raw_log_prob - ops.detach(raw_log_prob), inputs)\n            result = result + dice_factor\n        return result\n\n    def __getattribute__(self, attr):\n        if attr in type(self)._ast_fields and attr != \'name\':\n            return self.params[attr]\n        return super().__getattribute__(attr)\n\n    @classmethod\n    @functools.lru_cache(maxsize=5000)\n    def _infer_value_domain(cls, **kwargs):\n        # rely on the underlying distribution\'s logic to infer the event_shape given param domains\n        instance = cls.dist_class(**{k: dummy_numeric_array(domain) for k, domain in kwargs.items()},\n                                  validate_args=False)\n        out_shape = instance.event_shape\n        if type(instance.support).__name__ == ""_IntegerInterval"":\n            out_dtype = int(instance.support.upper_bound + 1)\n        else:\n            out_dtype = \'real\'\n        return Domain(dtype=out_dtype, shape=out_shape)\n\n    @classmethod\n    @functools.lru_cache(maxsize=5000)\n    def _infer_param_domain(cls, name, raw_shape):\n        support = cls.dist_class.arg_constraints.get(name, None)\n        # XXX: if the backend does not have the same definition of constraints, we should\n        # define backend-specific distributions and overide these `infer_value_domain`,\n        # `infer_param_domain` methods.\n        # Because NumPyro and Pyro have the same pattern, we use name check for simplicity.\n        support_name = type(support).__name__\n        if support_name == ""_Simplex"":\n            output = reals(raw_shape[-1])\n        elif support_name == ""_RealVector"":\n            output = reals(raw_shape[-1])\n        elif support_name in [""_LowerCholesky"", ""_PositiveDefinite""]:\n            output = reals(*raw_shape[-2:])\n        # resolve the issue: logits\'s constraints are real (instead of real_vector)\n        # for discrete multivariate distributions in Pyro\n        elif support_name == ""_Real"" and name == ""logits"" and (\n                ""probs"" in cls.dist_class.arg_constraints\n                and type(cls.dist_class.arg_constraints[""probs""]).__name__ == ""_Simplex""):\n            output = reals(raw_shape[-1])\n        else:\n            output = None\n        return output\n\n\n################################################################################\n# Distribution Wrappers\n################################################################################\n\n\ndef make_dist(backend_dist_class, param_names=()):\n    if not param_names:\n        param_names = tuple(name for name in inspect.getfullargspec(backend_dist_class.__init__)[0][1:]\n                            if name in backend_dist_class.arg_constraints)\n\n    @makefun.with_signature(f""__init__(self, {\', \'.join(param_names)}, value=\'value\')"")\n    def dist_init(self, **kwargs):\n        return Distribution.__init__(self, *tuple(kwargs[k] for k in self._ast_fields))\n\n    dist_class = DistributionMeta(backend_dist_class.__name__.split(""Wrapper_"")[-1], (Distribution,), {\n        \'dist_class\': backend_dist_class,\n        \'__init__\': dist_init,\n    })\n\n    eager.register(dist_class, *((Tensor,) * (len(param_names) + 1)))(dist_class.eager_log_prob)\n\n    return dist_class\n\n\nFUNSOR_DIST_NAMES = [\n    (\'Beta\', (\'concentration1\', \'concentration0\')),\n    (\'BernoulliProbs\', (\'probs\',)),\n    (\'BernoulliLogits\', (\'logits\',)),\n    (\'Binomial\', (\'total_count\', \'probs\')),\n    (\'Categorical\', (\'probs\',)),\n    (\'CategoricalLogits\', (\'logits\',)),\n    (\'Delta\', (\'v\', \'log_density\')),\n    (\'Dirichlet\', (\'concentration\',)),\n    (\'Gamma\', (\'concentration\', \'rate\')),\n    (\'Multinomial\', (\'total_count\', \'probs\')),\n    (\'MultivariateNormal\', (\'loc\', \'scale_tril\')),\n    (\'NonreparameterizedBeta\', (\'concentration1\', \'concentration0\')),\n    (\'NonreparameterizedDirichlet\', (\'concentration\',)),\n    (\'NonreparameterizedGamma\', (\'concentration\', \'rate\')),\n    (\'NonreparameterizedNormal\', (\'loc\', \'scale\')),\n    (\'Normal\', (\'loc\', \'scale\')),\n    (\'Poisson\', (\'rate\',))\n]\n\n\n###############################################\n# Converting backend Distributions to funsors\n###############################################\n\ndef backenddist_to_funsor(backend_dist, output=None, dim_to_name=None):\n    funsor_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    funsor_dist_class = getattr(funsor_dist, type(backend_dist).__name__.split(""Wrapper_"")[-1])\n    params = [to_funsor(\n            getattr(backend_dist, param_name),\n            output=funsor_dist_class._infer_param_domain(\n                param_name, getattr(getattr(backend_dist, param_name), ""shape"", ())),\n            dim_to_name=dim_to_name)\n        for param_name in funsor_dist_class._ast_fields if param_name != \'value\']\n    return funsor_dist_class(*params)\n\n\ndef indepdist_to_funsor(backend_dist, output=None, dim_to_name=None):\n    dim_to_name = OrderedDict((dim - backend_dist.reinterpreted_batch_ndims, name)\n                              for dim, name in dim_to_name.items())\n    dim_to_name.update(OrderedDict((i, f""_pyro_event_dim_{i}"")\n                                   for i in range(-backend_dist.reinterpreted_batch_ndims, 0)))\n    result = to_funsor(backend_dist.base_dist, dim_to_name=dim_to_name)\n    for i in reversed(range(-backend_dist.reinterpreted_batch_ndims, 0)):\n        name = f""_pyro_event_dim_{i}""\n        result = funsor.terms.Independent(result, ""value"", name, ""value"")\n    return result\n\n\ndef maskeddist_to_funsor(backend_dist, output=None, dim_to_name=None):\n    mask = to_funsor(ops.astype(backend_dist._mask, \'float32\'), output=output, dim_to_name=dim_to_name)\n    funsor_base_dist = to_funsor(backend_dist.base_dist, output=output, dim_to_name=dim_to_name)\n    return mask * funsor_base_dist\n\n\ndef transformeddist_to_funsor(backend_dist, output=None, dim_to_name=None):\n    raise NotImplementedError(""TODO implement conversion of TransformedDistribution"")\n\n\ndef mvndist_to_funsor(backend_dist, output=None, dim_to_name=None, real_inputs=OrderedDict()):\n    funsor_dist = backenddist_to_funsor(backend_dist, output=output, dim_to_name=dim_to_name)\n    if len(real_inputs) == 0:\n        return funsor_dist\n    discrete, gaussian = funsor_dist(value=""value"").terms\n    inputs = OrderedDict((k, v) for k, v in gaussian.inputs.items() if v.dtype != \'real\')\n    inputs.update(real_inputs)\n    return discrete + Gaussian(gaussian.info_vec, gaussian.precision, inputs)\n\n\n###############################################################\n# Converting distribution funsors to backend distributions\n###############################################################\n\n@to_data.register(Distribution)\ndef distribution_to_data(funsor_dist, name_to_dim=None):\n    pyro_dist_class = funsor_dist.dist_class\n    params = [to_data(getattr(funsor_dist, param_name), name_to_dim=name_to_dim)\n              for param_name in funsor_dist._ast_fields if param_name != \'value\']\n    pyro_dist = pyro_dist_class(**dict(zip(funsor_dist._ast_fields[:-1], params)))\n    funsor_event_shape = funsor_dist.value.output.shape\n    pyro_dist = pyro_dist.to_event(max(len(funsor_event_shape) - len(pyro_dist.event_shape), 0))\n    if pyro_dist.event_shape != funsor_event_shape:\n        raise ValueError(""Event shapes don\'t match, something went wrong"")\n    return pyro_dist\n\n\n@to_data.register(Independent[typing.Union[Independent, Distribution], str, str, str])\ndef indep_to_data(funsor_dist, name_to_dim=None):\n    raise NotImplementedError(""TODO implement conversion of Independent"")\n\n\n@to_data.register(Gaussian)\ndef gaussian_to_data(funsor_dist, name_to_dim=None, normalized=False):\n    if normalized:\n        return to_data(funsor_dist.log_normalizer + funsor_dist, name_to_dim=name_to_dim)\n    loc = ops.cholesky_solve(ops.unsqueeze(funsor_dist.info_vec, -1),\n                             ops.cholesky(funsor_dist.precision)).squeeze(-1)\n    int_inputs = OrderedDict((k, d) for k, d in funsor_dist.inputs.items() if d.dtype != ""real"")\n    loc = to_data(Tensor(loc, int_inputs), name_to_dim)\n    precision = to_data(Tensor(funsor_dist.precision, int_inputs), name_to_dim)\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    return backend_dist.MultivariateNormal.dist_class(loc, precision_matrix=precision)\n\n\n@to_data.register(GaussianMixture)\ndef gaussianmixture_to_data(funsor_dist, name_to_dim=None):\n    discrete, gaussian = funsor_dist.terms\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    cat = backend_dist.CategoricalLogits.dist_class(logits=to_data(\n        discrete + gaussian.log_normalizer, name_to_dim=name_to_dim))\n    mvn = to_data(gaussian, name_to_dim=name_to_dim)\n    return cat, mvn\n\n\n################################################\n# Backend-agnostic distribution patterns\n################################################\n\ndef Bernoulli(probs=None, logits=None, value=\'value\'):\n    """"""\n    Wraps backend `Bernoulli` distributions.\n\n    This dispatches to either `BernoulliProbs` or `BernoulliLogits`\n    to accept either ``probs`` or ``logits`` args.\n\n    :param Funsor probs: Probability of 1.\n    :param Funsor value: Optional observation in ``{0,1}``.\n    """"""\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    if probs is not None:\n        return backend_dist.BernoulliProbs(probs, value)  # noqa: F821\n    if logits is not None:\n        return backend_dist.BernoulliLogits(logits, value)  # noqa: F821\n    raise ValueError(\'Either probs or logits must be specified\')\n\n\ndef LogNormal(loc, scale, value=\'value\'):\n    """"""\n    Wraps backend `LogNormal` distributions.\n\n    :param Funsor loc: Mean of the untransformed Normal distribution.\n    :param Funsor scale: Standard deviation of the untransformed Normal\n        distribution.\n    :param Funsor value: Optional real observation.\n    """"""\n    loc, scale = to_funsor(loc), to_funsor(scale)\n    y = to_funsor(value, output=loc.output)\n    t = ops.exp\n    x = t.inv(y)\n    log_abs_det_jacobian = t.log_abs_det_jacobian(x, y)\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    return backend_dist.Normal(loc, scale, x) - log_abs_det_jacobian  # noqa: F821\n\n\ndef eager_beta(concentration1, concentration0, value):\n    concentration = stack((concentration0, concentration1))\n    value = stack((1 - value, value))\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    return backend_dist.Dirichlet(concentration, value=value)  # noqa: F821\n\n\ndef eager_binomial(total_count, probs, value):\n    probs = stack((1 - probs, probs))\n    value = stack((total_count - value, value))\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    return backend_dist.Multinomial(total_count, probs, value=value)  # noqa: F821\n\n\ndef eager_multinomial(total_count, probs, value):\n    # Multinomial.log_prob() supports inhomogeneous total_count only by\n    # avoiding passing total_count to the constructor.\n    inputs, (total_count, probs, value) = align_tensors(total_count, probs, value)\n    shape = broadcast_shape(total_count.shape + (1,), probs.shape, value.shape)\n    probs = Tensor(ops.expand(probs, shape), inputs)\n    value = Tensor(ops.expand(value, shape), inputs)\n    if get_backend() == ""torch"":\n        total_count = Number(ops.amax(total_count, None).item())  # Used by distributions validation code.\n    else:\n        total_count = Tensor(ops.expand(total_count, shape[:-1]), inputs)\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    return backend_dist.Multinomial.eager_log_prob(total_count, probs, value)  # noqa: F821\n\n\ndef eager_categorical_funsor(probs, value):\n    return probs[value].log()\n\n\ndef eager_categorical_tensor(probs, value):\n    value = probs.materialize(value)\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    return backend_dist.Categorical(probs=probs, value=value)  # noqa: F821\n\n\ndef eager_delta_tensor(v, log_density, value):\n    # This handles event_dim specially, and hence cannot use the\n    # generic Delta.eager_log_prob() method.\n    assert v.output == value.output\n    event_dim = len(v.output.shape)\n    inputs, (v, log_density, value) = align_tensors(v, log_density, value)\n    backend_dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    data = backend_dist.Delta.dist_class(v, log_density, event_dim).log_prob(value)  # noqa: F821\n    return Tensor(data, inputs)\n\n\ndef eager_delta_funsor_variable(v, log_density, value):\n    assert v.output == value.output\n    return funsor.delta.Delta(value.name, v, log_density)\n\n\ndef eager_delta_funsor_funsor(v, log_density, value):\n    assert v.output == value.output\n    return funsor.delta.Delta(v.name, value, log_density)\n\n\ndef eager_delta_variable_variable(v, log_density, value):\n    return None\n\n\ndef eager_normal(loc, scale, value):\n    assert loc.output == reals()\n    assert scale.output == reals()\n    assert value.output == reals()\n    if not is_affine(loc) or not is_affine(value):\n        return None  # lazy\n\n    info_vec = ops.new_zeros(scale.data, scale.data.shape + (1,))\n    precision = ops.pow(scale.data, -2).reshape(scale.data.shape + (1, 1))\n    log_prob = -0.5 * math.log(2 * math.pi) - ops.log(scale).sum()\n    inputs = scale.inputs.copy()\n    var = gensym(\'value\')\n    inputs[var] = reals()\n    gaussian = log_prob + Gaussian(info_vec, precision, inputs)\n    return gaussian(**{var: value - loc})\n\n\ndef eager_mvn(loc, scale_tril, value):\n    assert len(loc.shape) == 1\n    assert len(scale_tril.shape) == 2\n    assert value.output == loc.output\n    if not is_affine(loc) or not is_affine(value):\n        return None  # lazy\n\n    info_vec = ops.new_zeros(scale_tril.data, scale_tril.data.shape[:-1])\n    precision = ops.cholesky_inverse(scale_tril.data)\n    scale_diag = Tensor(ops.diagonal(scale_tril.data, -1, -2), scale_tril.inputs)\n    log_prob = -0.5 * scale_diag.shape[0] * math.log(2 * math.pi) - ops.log(scale_diag).sum()\n    inputs = scale_tril.inputs.copy()\n    var = gensym(\'value\')\n    inputs[var] = reals(scale_diag.shape[0])\n    gaussian = log_prob + Gaussian(info_vec, precision, inputs)\n    return gaussian(**{var: value - loc})\n'"
funsor/domains.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport operator\nfrom collections import namedtuple\nfrom functools import reduce\n\nimport funsor.ops as ops\nfrom funsor.util import broadcast_shape, get_tracing_state, lazy_property, quote\n\n\nclass Domain(namedtuple(\'Domain\', [\'shape\', \'dtype\'])):\n    """"""\n    An object representing the type and shape of a :class:`Funsor` input or\n    output.\n    """"""\n    def __new__(cls, shape, dtype):\n        assert isinstance(shape, tuple)\n        if get_tracing_state():\n            shape = tuple(map(int, shape))\n        assert all(isinstance(size, int) for size in shape), shape\n        if isinstance(dtype, int):\n            assert not shape\n        elif isinstance(dtype, str):\n            assert dtype == \'real\'\n        else:\n            raise ValueError(repr(dtype))\n        return super(Domain, cls).__new__(cls, shape, dtype)\n\n    def __repr__(self):\n        shape = tuple(self.shape)\n        if isinstance(self.dtype, int):\n            if not shape:\n                return \'bint({})\'.format(self.dtype)\n            return \'bint({}, {})\'.format(self.dtype, shape)\n        if not shape:\n            return \'reals()\'\n        return \'reals{}\'.format(shape)\n\n    def __iter__(self):\n        if isinstance(self.dtype, int) and not self.shape:\n            from funsor.terms import Number\n            return (Number(i, self.dtype) for i in range(self.dtype))\n        raise NotImplementedError\n\n    @lazy_property\n    def num_elements(self):\n        return reduce(operator.mul, self.shape, 1)\n\n    @property\n    def size(self):\n        assert isinstance(self.dtype, int)\n        return self.dtype\n\n\n@quote.register(Domain)\ndef _(arg, indent, out):\n    out.append((indent, repr(arg)))\n\n\ndef reals(*shape):\n    """"""\n    Construct a real domain of given shape.\n    """"""\n    return Domain(shape, \'real\')\n\n\ndef bint(size):\n    """"""\n    Construct a bounded integer domain of scalar shape.\n    """"""\n    if get_tracing_state():\n        size = int(size)\n    assert isinstance(size, int) and size >= 0\n    return Domain((), size)\n\n\ndef find_domain(op, *domains):\n    r""""""\n    Finds the :class:`Domain` resulting when applying ``op`` to ``domains``.\n    :param callable op: An operation.\n    :param Domain \\*domains: One or more input domains.\n    """"""\n    assert callable(op), op\n    assert all(isinstance(arg, Domain) for arg in domains)\n    if len(domains) == 1:\n        dtype = domains[0].dtype\n        shape = domains[0].shape\n        if op is ops.log or op is ops.exp:\n            dtype = \'real\'\n        elif isinstance(op, ops.ReshapeOp):\n            shape = op.shape\n        elif isinstance(op, ops.AssociativeOp):\n            shape = ()\n        return Domain(shape, dtype)\n\n    lhs, rhs = domains\n    if isinstance(op, ops.GetitemOp):\n        dtype = lhs.dtype\n        shape = lhs.shape[:op.offset] + lhs.shape[1 + op.offset:]\n        return Domain(shape, dtype)\n    elif op == ops.matmul:\n        assert lhs.shape and rhs.shape\n        if len(rhs.shape) == 1:\n            assert lhs.shape[-1] == rhs.shape[-1]\n            shape = lhs.shape[:-1]\n        elif len(lhs.shape) == 1:\n            assert lhs.shape[-1] == rhs.shape[-2]\n            shape = rhs.shape[:-2] + rhs.shape[-1:]\n        else:\n            assert lhs.shape[-1] == rhs.shape[-2]\n            shape = broadcast_shape(lhs.shape[:-1], rhs.shape[:-2] + (1,)) + rhs.shape[-1:]\n        return Domain(shape, \'real\')\n\n    if lhs.dtype == \'real\' or rhs.dtype == \'real\':\n        dtype = \'real\'\n    elif op in (ops.add, ops.mul, ops.pow, ops.max, ops.min):\n        dtype = op(lhs.dtype - 1, rhs.dtype - 1) + 1\n    elif op in (ops.and_, ops.or_, ops.xor):\n        dtype = 2\n    elif lhs.dtype == rhs.dtype:\n        dtype = lhs.dtype\n    else:\n        raise NotImplementedError(\'TODO\')\n\n    if lhs.shape == rhs.shape:\n        shape = lhs.shape\n    else:\n        shape = broadcast_shape(lhs.shape, rhs.shape)\n    return Domain(shape, dtype)\n\n\n__all__ = [\n    \'Domain\',\n    \'find_domain\',\n    \'bint\',\n    \'reals\',\n]\n'"
funsor/gaussian.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nfrom collections import OrderedDict, defaultdict\nfrom functools import reduce\n\nimport numpy as np\n\nimport funsor\nimport funsor.ops as ops\nfrom funsor.affine import affine_inputs, extract_affine, is_affine\nfrom funsor.delta import Delta\nfrom funsor.domains import reals\nfrom funsor.ops import AddOp, NegOp, SubOp\nfrom funsor.tensor import Tensor, align_tensor, align_tensors\nfrom funsor.terms import Align, Binary, Funsor, FunsorMeta, Number, Slice, Subs, Unary, Variable, eager, reflect\nfrom funsor.util import broadcast_shape, get_backend, get_tracing_state, lazy_property\n\n\ndef _log_det_tri(x):\n    return ops.log(ops.diagonal(x, -1, -2)).sum(-1)\n\n\ndef _vv(vec1, vec2):\n    """"""\n    Computes the inner product ``< vec1 | vec 2 >``.\n    """"""\n    return ops.matmul(ops.unsqueeze(vec1, -2), ops.unsqueeze(vec2, -1)).squeeze(-1).squeeze(-1)\n\n\ndef _mv(mat, vec):\n    return ops.matmul(mat, ops.unsqueeze(vec, -1)).squeeze(-1)\n\n\ndef _trace_mm(x, y):\n    """"""\n    Computes ``trace(x.T @ y)``.\n    """"""\n    assert len(x.shape) >= 2\n    assert len(y.shape) >= 2\n    return (x * y).sum((-1, -2))\n\n\ndef _compute_offsets(inputs):\n    """"""\n    Compute offsets of real inputs into the concatenated Gaussian dims.\n    This ignores all int inputs.\n\n    :param OrderedDict inputs: A schema mapping variable name to domain.\n    :return: a pair ``(offsets, total)``, where ``offsets`` is an OrderedDict\n        mapping input name to integer offset, and ``total`` is the total event\n        size.\n    :rtype: tuple\n    """"""\n    assert isinstance(inputs, OrderedDict)\n    offsets = OrderedDict()\n    total = 0\n    for key, domain in inputs.items():\n        if domain.dtype == \'real\':\n            offsets[key] = total\n            total += domain.num_elements\n    return offsets, total\n\n\ndef _find_intervals(intervals, end):\n    """"""\n    Finds a complete set of intervals partitioning [0, end), given a partial\n    set of non-overlapping intervals.\n    """"""\n    cuts = list(sorted({0, end}.union(*intervals)))\n    return list(zip(cuts[:-1], cuts[1:]))\n\n\ndef _parse_slices(index, value):\n    if not isinstance(index, tuple):\n        index = (index,)\n    if index[0] is Ellipsis:\n        index = index[1:]\n    start_stops = []\n    for pos, i in reversed(list(enumerate(index))):\n        if isinstance(i, slice):\n            start_stops.append((i.start, i.stop))\n        elif isinstance(i, int):\n            start_stops.append((i, i + 1))\n            value = ops.unsqueeze(value, pos - len(index))\n        else:\n            raise ValueError(""invalid index: {}"".format(i))\n    start_stops.reverse()\n    return start_stops, value\n\n\nclass BlockVector(object):\n    """"""\n    Jit-compatible helper to build blockwise vectors.\n    Syntax is similar to :func:`torch.zeros` ::\n\n        x = BlockVector((100, 20))\n        x[..., 0:4] = x1\n        x[..., 6:10] = x2\n        x = x.as_tensor()\n        assert x.shape == (100, 20)\n    """"""\n    def __init__(self, shape):\n        self.shape = shape\n        self.parts = {}\n\n    def __setitem__(self, index, value):\n        (i,), value = _parse_slices(index, value)\n        self.parts[i] = value\n\n    def as_tensor(self):\n        # Fill gaps with zeros.\n        prototype = next(iter(self.parts.values()))\n        for i in _find_intervals(self.parts.keys(), self.shape[-1]):\n            if i not in self.parts:\n                self.parts[i] = ops.new_zeros(prototype, self.shape[:-1] + (i[1] - i[0],))\n\n        # Concatenate parts.\n        parts = [v for k, v in sorted(self.parts.items())]\n        result = ops.cat(-1, *parts)\n        if not get_tracing_state():\n            assert result.shape == self.shape\n        return result\n\n\nclass BlockMatrix(object):\n    """"""\n    Jit-compatible helper to build blockwise matrices.\n    Syntax is similar to :func:`torch.zeros` ::\n\n        x = BlockMatrix((100, 20, 20))\n        x[..., 0:4, 0:4] = x11\n        x[..., 0:4, 6:10] = x12\n        x[..., 6:10, 0:4] = x12.transpose(-1, -2)\n        x[..., 6:10, 6:10] = x22\n        x = x.as_tensor()\n        assert x.shape == (100, 20, 20)\n    """"""\n    def __init__(self, shape):\n        self.shape = shape\n        self.parts = defaultdict(dict)\n\n    def __setitem__(self, index, value):\n        (i, j), value = _parse_slices(index, value)\n        self.parts[i][j] = value\n\n    def as_tensor(self):\n        # Fill gaps with zeros.\n        arbitrary_row = next(iter(self.parts.values()))\n        prototype = next(iter(arbitrary_row.values()))\n        js = set().union(*(part.keys() for part in self.parts.values()))\n        rows = _find_intervals(self.parts.keys(), self.shape[-2])\n        cols = _find_intervals(js, self.shape[-1])\n        for i in rows:\n            for j in cols:\n                if j not in self.parts[i]:\n                    shape = self.shape[:-2] + (i[1] - i[0], j[1] - j[0])\n                    self.parts[i][j] = ops.new_zeros(prototype, shape)\n\n        # Concatenate parts.\n        # TODO This could be optimized into a single .reshape().cat().reshape() if\n        #   all inputs are contiguous, thereby saving a memcopy.\n        columns = {i: ops.cat(-1, *[v for j, v in sorted(part.items())])\n                   for i, part in self.parts.items()}\n        result = ops.cat(-2, *[v for i, v in sorted(columns.items())])\n        if not get_tracing_state():\n            assert result.shape == self.shape\n        return result\n\n\ndef align_gaussian(new_inputs, old):\n    """"""\n    Align data of a Gaussian distribution to a new ``inputs`` shape.\n    """"""\n    assert isinstance(new_inputs, OrderedDict)\n    assert isinstance(old, Gaussian)\n    info_vec = old.info_vec\n    precision = old.precision\n\n    # Align int inputs.\n    # Since these are are managed as in Tensor, we can defer to align_tensor().\n    new_ints = OrderedDict((k, d) for k, d in new_inputs.items() if d.dtype != \'real\')\n    old_ints = OrderedDict((k, d) for k, d in old.inputs.items() if d.dtype != \'real\')\n    if new_ints != old_ints:\n        info_vec = align_tensor(new_ints, Tensor(info_vec, old_ints))\n        precision = align_tensor(new_ints, Tensor(precision, old_ints))\n\n    # Align real inputs, which are all concatenated in the rightmost dims.\n    new_offsets, new_dim = _compute_offsets(new_inputs)\n    old_offsets, old_dim = _compute_offsets(old.inputs)\n    assert info_vec.shape[-1:] == (old_dim,)\n    assert precision.shape[-2:] == (old_dim, old_dim)\n    if new_offsets != old_offsets:\n        old_info_vec = info_vec\n        old_precision = precision\n        info_vec = BlockVector(old_info_vec.shape[:-1] + (new_dim,))\n        precision = BlockMatrix(old_info_vec.shape[:-1] + (new_dim, new_dim))\n        for k1, new_offset1 in new_offsets.items():\n            if k1 not in old_offsets:\n                continue\n            offset1 = old_offsets[k1]\n            num_elements1 = old.inputs[k1].num_elements\n            old_slice1 = slice(offset1, offset1 + num_elements1)\n            new_slice1 = slice(new_offset1, new_offset1 + num_elements1)\n            info_vec[..., new_slice1] = old_info_vec[..., old_slice1]\n            for k2, new_offset2 in new_offsets.items():\n                if k2 not in old_offsets:\n                    continue\n                offset2 = old_offsets[k2]\n                num_elements2 = old.inputs[k2].num_elements\n                old_slice2 = slice(offset2, offset2 + num_elements2)\n                new_slice2 = slice(new_offset2, new_offset2 + num_elements2)\n                precision[..., new_slice1, new_slice2] = old_precision[..., old_slice1, old_slice2]\n        info_vec = info_vec.as_tensor()\n        precision = precision.as_tensor()\n\n    return info_vec, precision\n\n\nclass GaussianMeta(FunsorMeta):\n    """"""\n    Wrapper to convert between OrderedDict and tuple.\n    """"""\n    def __call__(cls, info_vec, precision, inputs):\n        if isinstance(inputs, OrderedDict):\n            inputs = tuple(inputs.items())\n        assert isinstance(inputs, tuple)\n        return super(GaussianMeta, cls).__call__(info_vec, precision, inputs)\n\n\nclass Gaussian(Funsor, metaclass=GaussianMeta):\n    """"""\n    Funsor representing a batched joint Gaussian distribution as a log-density\n    function.\n\n    Mathematically, a Gaussian represents the density function::\n\n        f(x) = < x | info_vec > - 0.5 * < x | precision | x >\n             = < x | info_vec - 0.5 * precision @ x >\n\n    Note that :class:`Gaussian` s are not normalized, rather they are\n    canonicalized to evaluate to zero log density at the origin: ``f(0) = 0``.\n    This canonical form is useful in combination with the information filter\n    representation because it allows :class:`Gaussian` s with incomplete\n    information, i.e.  zero eigenvalues in the precision matrix.  These\n    incomplete distributions arise when making low-dimensional observations on\n    higher dimensional hidden state.\n\n    :param torch.Tensor info_vec: An optional batched information vector,\n        where ``info_vec = precision @ mean``.\n    :param torch.Tensor precision: A batched positive semidefinite precision\n        matrix.\n    :param OrderedDict inputs: Mapping from name to\n        :class:`~funsor.domains.Domain` .\n    """"""\n    def __init__(self, info_vec, precision, inputs):\n        assert ops.is_numeric_array(info_vec) and ops.is_numeric_array(precision)\n        assert isinstance(inputs, tuple)\n        inputs = OrderedDict(inputs)\n\n        # Compute total dimension of all real inputs.\n        dim = sum(d.num_elements for d in inputs.values() if d.dtype == \'real\')\n        if not get_tracing_state():\n            assert dim\n            assert len(precision.shape) >= 2 and precision.shape[-2:] == (dim, dim)\n            assert len(info_vec.shape) >= 1 and info_vec.shape[-1] == dim\n\n        # Compute total shape of all bint inputs.\n        batch_shape = tuple(d.dtype for d in inputs.values()\n                            if isinstance(d.dtype, int))\n        if not get_tracing_state():\n            assert precision.shape == batch_shape + (dim, dim)\n            assert info_vec.shape == batch_shape + (dim,)\n\n        output = reals()\n        fresh = frozenset(inputs.keys())\n        bound = frozenset()\n        super(Gaussian, self).__init__(inputs, output, fresh, bound)\n        self.info_vec = info_vec\n        self.precision = precision\n        self.batch_shape = batch_shape\n        self.event_shape = (dim,)\n\n    @lazy_property\n    def _precision_chol(self):\n        return ops.cholesky(self.precision)\n\n    @lazy_property\n    def log_normalizer(self):\n        dim = self.precision.shape[-1]\n        log_det_term = _log_det_tri(self._precision_chol)\n        loc_info_vec_term = 0.5 * (ops.triangular_solve(\n            self.info_vec[..., None], self._precision_chol)[..., 0] ** 2).sum(-1)\n        data = 0.5 * dim * math.log(2 * math.pi) - log_det_term + loc_info_vec_term\n        inputs = OrderedDict((k, v) for k, v in self.inputs.items() if v.dtype != \'real\')\n        return Tensor(data, inputs)\n\n    def __repr__(self):\n        return \'Gaussian(..., ({}))\'.format(\' \'.join(\n            \'({}, {}),\'.format(*kv) for kv in self.inputs.items()))\n\n    def align(self, names):\n        assert isinstance(names, tuple)\n        assert all(name in self.inputs for name in names)\n        if not names or names == tuple(self.inputs):\n            return self\n\n        inputs = OrderedDict((name, self.inputs[name]) for name in names)\n        inputs.update(self.inputs)\n        info_vec, precision = align_gaussian(inputs, self)\n        return Gaussian(info_vec, precision, inputs)\n\n    def eager_subs(self, subs):\n        assert isinstance(subs, tuple)\n        prototype = Tensor(self.info_vec)\n        subs = tuple((k, v if isinstance(v, (Variable, Slice))\n                      else prototype.materialize(v))\n                     for k, v in subs if k in self.inputs)\n        if not subs:\n            return self\n\n        # Constants and Affine funsors are eagerly substituted;\n        # everything else is lazily substituted.\n        lazy_subs = tuple((k, v) for k, v in subs\n                          if not isinstance(v, (Number, Tensor, Variable, Slice))\n                          and not (is_affine(v) and affine_inputs(v)))\n        var_subs = tuple((k, v) for k, v in subs if isinstance(v, Variable))\n        int_subs = tuple((k, v) for k, v in subs if isinstance(v, (Number, Tensor, Slice))\n                         if v.dtype != \'real\')\n        real_subs = tuple((k, v) for k, v in subs if isinstance(v, (Number, Tensor))\n                          if v.dtype == \'real\')\n        affine_subs = tuple((k, v) for k, v in subs\n                            if is_affine(v) and affine_inputs(v) and not isinstance(v, Variable))\n        if var_subs:\n            return self._eager_subs_var(var_subs, int_subs + real_subs + affine_subs + lazy_subs)\n        if int_subs:\n            return self._eager_subs_int(int_subs, real_subs + affine_subs + lazy_subs)\n        if real_subs:\n            return self._eager_subs_real(real_subs, affine_subs + lazy_subs)\n        if affine_subs:\n            return self._eager_subs_affine(affine_subs, lazy_subs)\n        return reflect(Subs, self, lazy_subs)\n\n    def _eager_subs_var(self, subs, remaining_subs):\n        # Perform variable substitution, i.e. renaming of inputs.\n        rename = {k: v.name for k, v in subs}\n        inputs = OrderedDict((rename.get(k, k), d) for k, d in self.inputs.items())\n        if len(inputs) != len(self.inputs):\n            raise ValueError(""Variable substitution name conflict"")\n        var_result = Gaussian(self.info_vec, self.precision, inputs)\n        return Subs(var_result, remaining_subs) if remaining_subs else var_result\n\n    def _eager_subs_int(self, subs, remaining_subs):\n        # Perform integer substitution, i.e. slicing into a batch.\n        int_inputs = OrderedDict((k, d) for k, d in self.inputs.items() if d.dtype != \'real\')\n        real_inputs = OrderedDict((k, d) for k, d in self.inputs.items() if d.dtype == \'real\')\n        tensors = [self.info_vec, self.precision]\n        funsors = [Subs(Tensor(x, int_inputs), subs) for x in tensors]\n        inputs = funsors[0].inputs.copy()\n        inputs.update(real_inputs)\n        int_result = Gaussian(funsors[0].data, funsors[1].data, inputs)\n        return Subs(int_result, remaining_subs) if remaining_subs else int_result\n\n    def _eager_subs_real(self, subs, remaining_subs):\n        # Broadcast all component tensors.\n        subs = OrderedDict(subs)\n        int_inputs = OrderedDict((k, d) for k, d in self.inputs.items() if d.dtype != \'real\')\n        tensors = [Tensor(self.info_vec, int_inputs),\n                   Tensor(self.precision, int_inputs)]\n        tensors.extend(subs.values())\n        int_inputs, tensors = align_tensors(*tensors)\n        batch_dim = len(tensors[0].shape) - 1\n        batch_shape = broadcast_shape(*(x.shape[:batch_dim] for x in tensors))\n        (info_vec, precision), values = tensors[:2], tensors[2:]\n        offsets, event_size = _compute_offsets(self.inputs)\n        slices = [(k, slice(offset, offset + self.inputs[k].num_elements))\n                  for k, offset in offsets.items()]\n\n        # Expand all substituted values.\n        values = OrderedDict(zip(subs, values))\n        for k, value in values.items():\n            value = value.reshape(value.shape[:batch_dim] + (-1,))\n            if not get_tracing_state():\n                assert value.shape[-1] == self.inputs[k].num_elements\n            values[k] = ops.expand(value, batch_shape + value.shape[-1:])\n\n        # Try to perform a complete substitution of all real variables, resulting in a Tensor.\n        if all(k in subs for k, d in self.inputs.items() if d.dtype == \'real\'):\n            # Form the concatenated value.\n            value = BlockVector(batch_shape + (event_size,))\n            for k, i in slices:\n                if k in values:\n                    value[..., i] = values[k]\n            value = value.as_tensor()\n\n            # Evaluate the non-normalized log density.\n            result = _vv(value, info_vec - 0.5 * _mv(precision, value))\n\n            result = Tensor(result, int_inputs)\n            assert result.output == reals()\n            return Subs(result, remaining_subs) if remaining_subs else result\n\n        # Perform a partial substution of a subset of real variables, resulting in a Joint.\n        # We split real inputs into two sets: a for the preserved and b for the substituted.\n        b = frozenset(k for k, v in subs.items())\n        a = frozenset(k for k, d in self.inputs.items() if d.dtype == \'real\' and k not in b)\n        prec_aa = ops.cat(-2, *[ops.cat(-1, *[\n            precision[..., i1, i2]\n            for k2, i2 in slices if k2 in a])\n            for k1, i1 in slices if k1 in a])\n        prec_ab = ops.cat(-2, *[ops.cat(-1, *[\n            precision[..., i1, i2]\n            for k2, i2 in slices if k2 in b])\n            for k1, i1 in slices if k1 in a])\n        prec_bb = ops.cat(-2, *[ops.cat(-1, *[\n            precision[..., i1, i2]\n            for k2, i2 in slices if k2 in b])\n            for k1, i1 in slices if k1 in b])\n        info_a = ops.cat(-1, *[info_vec[..., i] for k, i in slices if k in a])\n        info_b = ops.cat(-1, *[info_vec[..., i] for k, i in slices if k in b])\n        value_b = ops.cat(-1, *[values[k] for k, i in slices if k in b])\n        info_vec = info_a - _mv(prec_ab, value_b)\n        log_scale = _vv(value_b, info_b - 0.5 * _mv(prec_bb, value_b))\n        precision = ops.expand(prec_aa, info_vec.shape + info_vec.shape[-1:])\n        inputs = int_inputs.copy()\n        for k, d in self.inputs.items():\n            if k not in subs:\n                inputs[k] = d\n        result = Gaussian(info_vec, precision, inputs) + Tensor(log_scale, int_inputs)\n        return Subs(result, remaining_subs) if remaining_subs else result\n\n    def _eager_subs_affine(self, subs, remaining_subs):\n        # Extract an affine representation.\n        affine = OrderedDict()\n        for k, v in subs:\n            const, coeffs = extract_affine(v)\n            if (isinstance(const, Tensor) and\n                    all(isinstance(coeff, Tensor) for coeff, _ in coeffs.values())):\n                affine[k] = const, coeffs\n            else:\n                remaining_subs += (k, v),\n        if not affine:\n            return reflect(Subs, self, remaining_subs)\n\n        # Align integer dimensions.\n        old_int_inputs = OrderedDict((k, v) for k, v in self.inputs.items() if v.dtype != \'real\')\n        tensors = [Tensor(self.info_vec, old_int_inputs),\n                   Tensor(self.precision, old_int_inputs)]\n        for const, coeffs in affine.values():\n            tensors.append(const)\n            tensors.extend(coeff for coeff, _ in coeffs.values())\n        new_int_inputs, tensors = align_tensors(*tensors, expand=True)\n        tensors = (Tensor(x, new_int_inputs) for x in tensors)\n        old_info_vec = next(tensors).data\n        old_precision = next(tensors).data\n        for old_k, (const, coeffs) in affine.items():\n            const = next(tensors)\n            for new_k, (coeff, eqn) in coeffs.items():\n                coeff = next(tensors)\n                coeffs[new_k] = coeff, eqn\n            affine[old_k] = const, coeffs\n        batch_shape = old_info_vec.shape[:-1]\n\n        # Align real dimensions.\n        old_real_inputs = OrderedDict((k, v) for k, v in self.inputs.items() if v.dtype == \'real\')\n        new_real_inputs = old_real_inputs.copy()\n        for old_k, (const, coeffs) in affine.items():\n            del new_real_inputs[old_k]\n            for new_k, (coeff, eqn) in coeffs.items():\n                new_shape = coeff.shape[:len(eqn.split(\'->\')[0].split(\',\')[1])]\n                new_real_inputs[new_k] = reals(*new_shape)\n        old_offsets, old_dim = _compute_offsets(old_real_inputs)\n        new_offsets, new_dim = _compute_offsets(new_real_inputs)\n        new_inputs = new_int_inputs.copy()\n        new_inputs.update(new_real_inputs)\n\n        # Construct a blockwise affine representation of the substitution.\n        subs_vector = BlockVector(batch_shape + (old_dim,))\n        subs_matrix = BlockMatrix(batch_shape + (new_dim, old_dim))\n        for old_k, old_offset in old_offsets.items():\n            old_size = old_real_inputs[old_k].num_elements\n            old_slice = slice(old_offset, old_offset + old_size)\n            if old_k in new_real_inputs:\n                new_offset = new_offsets[old_k]\n                new_slice = slice(new_offset, new_offset + old_size)\n                subs_matrix[..., new_slice, old_slice] = \\\n                    ops.new_eye(self.info_vec, batch_shape + (old_size,))\n                continue\n            const, coeffs = affine[old_k]\n            old_shape = old_real_inputs[old_k].shape\n            assert const.data.shape == batch_shape + old_shape\n            subs_vector[..., old_slice] = const.data.reshape(batch_shape + (old_size,))\n            for new_k, new_offset in new_offsets.items():\n                if new_k in coeffs:\n                    coeff, eqn = coeffs[new_k]\n                    new_size = new_real_inputs[new_k].num_elements\n                    new_slice = slice(new_offset, new_offset + new_size)\n                    assert coeff.shape == new_real_inputs[new_k].shape + old_shape\n                    subs_matrix[..., new_slice, old_slice] = \\\n                        coeff.data.reshape(batch_shape + (new_size, old_size))\n        subs_vector = subs_vector.as_tensor()\n        subs_matrix = subs_matrix.as_tensor()\n        subs_matrix_t = ops.transpose(subs_matrix, -1, -2)\n\n        # Construct the new funsor. Suppose the old Gaussian funsor g has density\n        #   g(x) = < x | i - 1/2 P x>\n        # Now define a new funsor f by substituting x = A y + B:\n        #   f(y) = g(A y + B)\n        #        = < A y + B | i - 1/2 P (A y + B) >\n        #        = < y | At (i - P B) - 1/2 At P A y > + < B | i - 1/2 P B >\n        #        =: < y | i\' - 1/2 P\' y > + C\n        # where  P\' = At P A  and  i\' = At (i - P B)  parametrize a new Gaussian\n        # and  C = < B | i - 1/2 P B >  parametrize a new Tensor.\n        precision = subs_matrix @ old_precision @ subs_matrix_t\n        info_vec = _mv(subs_matrix, old_info_vec - _mv(old_precision, subs_vector))\n        const = _vv(subs_vector, old_info_vec - 0.5 * _mv(old_precision, subs_vector))\n        result = Gaussian(info_vec, precision, new_inputs) + Tensor(const, new_int_inputs)\n        return Subs(result, remaining_subs) if remaining_subs else result\n\n    def eager_reduce(self, op, reduced_vars):\n        if op is ops.logaddexp:\n            # Marginalize out real variables, but keep mixtures lazy.\n            assert all(v in self.inputs for v in reduced_vars)\n            real_vars = frozenset(k for k, d in self.inputs.items() if d.dtype == ""real"")\n            reduced_reals = reduced_vars & real_vars\n            reduced_ints = reduced_vars - real_vars\n            if not reduced_reals:\n                return None  # defer to default implementation\n\n            inputs = OrderedDict((k, d) for k, d in self.inputs.items() if k not in reduced_reals)\n            if reduced_reals == real_vars:\n                result = self.log_normalizer\n            else:\n                int_inputs = OrderedDict((k, v) for k, v in inputs.items() if v.dtype != \'real\')\n                offsets, _ = _compute_offsets(self.inputs)\n                a = []\n                b = []\n                for key, domain in self.inputs.items():\n                    if domain.dtype == \'real\':\n                        block = ops.new_arange(self.info_vec, offsets[key], offsets[key] + domain.num_elements, 1)\n                        (b if key in reduced_vars else a).append(block)\n                a = ops.cat(-1, *a)\n                b = ops.cat(-1, *b)\n                prec_aa = self.precision[..., a[..., None], a]\n                prec_ba = self.precision[..., b[..., None], a]\n                prec_bb = self.precision[..., b[..., None], b]\n                prec_b = ops.cholesky(prec_bb)\n                prec_a = ops.triangular_solve(prec_ba, prec_b)\n                prec_at = ops.transpose(prec_a, -1, -2)\n                precision = prec_aa - ops.matmul(prec_at, prec_a)\n\n                info_a = self.info_vec[..., a]\n                info_b = self.info_vec[..., b]\n                b_tmp = ops.triangular_solve(info_b[..., None], prec_b)\n                info_vec = info_a - ops.matmul(prec_at, b_tmp)[..., 0]\n\n                log_prob = Tensor(0.5 * len(b) * math.log(2 * math.pi) - _log_det_tri(prec_b) +\n                                  0.5 * (b_tmp[..., 0] ** 2).sum(-1),\n                                  int_inputs)\n                result = log_prob + Gaussian(info_vec, precision, inputs)\n\n            return result.reduce(ops.logaddexp, reduced_ints)\n\n        elif op is ops.add:\n            for v in reduced_vars:\n                if self.inputs[v].dtype == \'real\':\n                    raise ValueError(""Cannot sum along a real dimension: {}"".format(repr(v)))\n\n            # Fuse Gaussians along a plate. Compare to eager_add_gaussian_gaussian().\n            old_ints = OrderedDict((k, v) for k, v in self.inputs.items() if v.dtype != \'real\')\n            new_ints = OrderedDict((k, v) for k, v in old_ints.items() if k not in reduced_vars)\n            inputs = OrderedDict((k, v) for k, v in self.inputs.items() if k not in reduced_vars)\n\n            info_vec = Tensor(self.info_vec, old_ints).reduce(ops.add, reduced_vars)\n            precision = Tensor(self.precision, old_ints).reduce(ops.add, reduced_vars)\n            assert info_vec.inputs == new_ints\n            assert precision.inputs == new_ints\n            return Gaussian(info_vec.data, precision.data, inputs)\n\n        return None  # defer to default implementation\n\n    def unscaled_sample(self, sampled_vars, sample_inputs, rng_key=None):\n        sampled_vars = sampled_vars.intersection(self.inputs)\n        if not sampled_vars:\n            return self\n        if any(self.inputs[k].dtype != \'real\' for k in sampled_vars):\n            raise ValueError(\'Sampling from non-normalized Gaussian mixtures is intentionally \'\n                             \'not implemented. You probably want to normalize. To work around, \'\n                             \'add a zero Tensor/Array with given inputs.\')\n\n        # Partition inputs into sample_inputs + int_inputs + real_inputs.\n        sample_inputs = OrderedDict((k, d) for k, d in sample_inputs.items()\n                                    if k not in self.inputs)\n        sample_shape = tuple(int(d.dtype) for d in sample_inputs.values())\n        int_inputs = OrderedDict((k, d) for k, d in self.inputs.items() if d.dtype != \'real\')\n        real_inputs = OrderedDict((k, d) for k, d in self.inputs.items() if d.dtype == \'real\')\n        inputs = sample_inputs.copy()\n        inputs.update(int_inputs)\n\n        if sampled_vars == frozenset(real_inputs):\n            shape = sample_shape + self.info_vec.shape\n            backend = get_backend()\n            if backend != ""numpy"":\n                from importlib import import_module\n                dist = import_module(funsor.distribution.BACKEND_TO_DISTRIBUTIONS_BACKEND[backend])\n                sample_args = (shape,) if rng_key is None else (rng_key, shape)\n                white_noise = dist.Normal.dist_class(0, 1).sample(*sample_args)\n            else:\n                white_noise = np.random.randn(*shape)\n            white_noise = ops.unsqueeze(white_noise, -1)\n\n            white_vec = ops.triangular_solve(self.info_vec[..., None], self._precision_chol)\n            sample = ops.triangular_solve(white_noise + white_vec, self._precision_chol, transpose=True)[..., 0]\n            offsets, _ = _compute_offsets(real_inputs)\n            results = []\n            for key, domain in real_inputs.items():\n                data = sample[..., offsets[key]: offsets[key] + domain.num_elements]\n                data = data.reshape(shape[:-1] + domain.shape)\n                point = Tensor(data, inputs)\n                assert point.output == domain\n                results.append(Delta(key, point))\n            results.append(self.log_normalizer)\n            return reduce(ops.add, results)\n\n        raise NotImplementedError(\'TODO implement partial sampling of real variables\')\n\n\n@eager.register(Binary, AddOp, Gaussian, Gaussian)\ndef eager_add_gaussian_gaussian(op, lhs, rhs):\n    # Fuse two Gaussians by adding their log-densities pointwise.\n    # This is similar to a Kalman filter update, but also keeps track of\n    # the marginal likelihood which accumulates into a Tensor.\n\n    # Align data.\n    inputs = lhs.inputs.copy()\n    inputs.update(rhs.inputs)\n    lhs_info_vec, lhs_precision = align_gaussian(inputs, lhs)\n    rhs_info_vec, rhs_precision = align_gaussian(inputs, rhs)\n\n    # Fuse aligned Gaussians.\n    info_vec = lhs_info_vec + rhs_info_vec\n    precision = lhs_precision + rhs_precision\n    return Gaussian(info_vec, precision, inputs)\n\n\n@eager.register(Binary, SubOp, Gaussian, (Funsor, Align, Gaussian))\n@eager.register(Binary, SubOp, (Funsor, Align, Delta), Gaussian)\ndef eager_sub(op, lhs, rhs):\n    return lhs + -rhs\n\n\n@eager.register(Unary, NegOp, Gaussian)\ndef eager_neg(op, arg):\n    info_vec = -arg.info_vec\n    precision = -arg.precision\n    return Gaussian(info_vec, precision, arg.inputs)\n\n\n__all__ = [\n    \'BlockMatrix\',\n    \'BlockVector\',\n    \'Gaussian\',\n    \'align_gaussian\',\n]\n'"
funsor/integrate.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\nfrom typing import Union\n\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction, GaussianMixture\nfrom funsor.delta import Delta\nfrom funsor.gaussian import Gaussian, align_gaussian, _mv, _trace_mm, _vv\nfrom funsor.tensor import Tensor\nfrom funsor.terms import (\n    Funsor,\n    FunsorMeta,\n    Number,\n    Subs,\n    Unary,\n    Variable,\n    _convert_reduced_vars,\n    eager,\n    normalize,\n    substitute,\n    to_funsor\n)\n\n\nclass IntegrateMeta(FunsorMeta):\n    """"""\n    Wrapper to convert reduced_vars arg to a frozenset of str.\n    """"""\n    def __call__(cls, log_measure, integrand, reduced_vars):\n        reduced_vars = _convert_reduced_vars(reduced_vars)\n        return super().__call__(log_measure, integrand, reduced_vars)\n\n\nclass Integrate(Funsor, metaclass=IntegrateMeta):\n    """"""\n    Funsor representing an integral wrt a log density funsor.\n\n    :param Funsor log_measure: A log density funsor treated as a measure.\n    :param Funsor integrand: An integrand funsor.\n    :param reduced_vars: An input name or set of names to reduce.\n    :type reduced_vars: str, Variable, or set or frozenset thereof.\n    """"""\n    def __init__(self, log_measure, integrand, reduced_vars):\n        assert isinstance(log_measure, Funsor)\n        assert isinstance(integrand, Funsor)\n        assert isinstance(reduced_vars, frozenset)\n        assert all(isinstance(v, str) for v in reduced_vars)\n        inputs = OrderedDict((k, d) for term in (log_measure, integrand)\n                             for (k, d) in term.inputs.items()\n                             if k not in reduced_vars)\n        output = integrand.output\n        fresh = frozenset()\n        bound = reduced_vars\n        super(Integrate, self).__init__(inputs, output, fresh, bound)\n        self.log_measure = log_measure\n        self.integrand = integrand\n        self.reduced_vars = reduced_vars\n\n    def _alpha_convert(self, alpha_subs):\n        assert self.bound.issuperset(alpha_subs)\n        reduced_vars = frozenset(alpha_subs.get(k, k) for k in self.reduced_vars)\n        alpha_subs = {k: to_funsor(v, self.integrand.inputs.get(k, self.log_measure.inputs.get(k)))\n                      for k, v in alpha_subs.items()}\n        log_measure = substitute(self.log_measure, alpha_subs)\n        integrand = substitute(self.integrand, alpha_subs)\n        return log_measure, integrand, reduced_vars\n\n\n@normalize.register(Integrate, Funsor, Funsor, frozenset)\ndef normalize_integrate(log_measure, integrand, reduced_vars):\n    return Contraction(ops.add, ops.mul, reduced_vars, log_measure.exp(), integrand)\n\n\n@normalize.register(Integrate,\n                    Contraction[Union[ops.NullOp, ops.LogAddExpOp], ops.AddOp, frozenset, tuple],\n                    Funsor, frozenset)\ndef normalize_integrate_contraction(log_measure, integrand, reduced_vars):\n    delta_terms = [t for t in log_measure.terms if isinstance(t, Delta)\n                   and t.fresh.intersection(reduced_vars, integrand.inputs)]\n    for delta in delta_terms:\n        integrand = integrand(**{name: point for name, (point, log_density) in delta.terms\n                                 if name in reduced_vars.intersection(integrand.inputs)})\n    return normalize_integrate(log_measure, integrand, reduced_vars)\n\n\n@eager.register(Contraction, ops.AddOp, ops.MulOp, frozenset,\n                Unary[ops.ExpOp, Union[GaussianMixture, Delta, Gaussian, Number, Tensor]],\n                (Variable, Delta, Gaussian, Number, Tensor, GaussianMixture))\ndef eager_contraction_binary_to_integrate(red_op, bin_op, reduced_vars, lhs, rhs):\n\n    if reduced_vars - reduced_vars.intersection(lhs.inputs, rhs.inputs):\n        args = red_op, bin_op, reduced_vars, (lhs, rhs)\n        result = eager.dispatch(Contraction, *args)(*args)\n        if result is not None:\n            return result\n\n    args = lhs.log(), rhs, reduced_vars\n    result = eager.dispatch(Integrate, *args)(*args)\n    if result is not None:\n        return result\n\n    return None\n\n\n@eager.register(Integrate, GaussianMixture, Funsor, frozenset)\ndef eager_integrate_gaussianmixture(log_measure, integrand, reduced_vars):\n    real_vars = frozenset(k for k in reduced_vars if log_measure.inputs[k].dtype == \'real\')\n    if reduced_vars <= real_vars:\n        discrete, gaussian = log_measure.terms\n        return discrete.exp() * Integrate(gaussian, integrand, reduced_vars)\n    return None\n\n\n########################################\n# Delta patterns\n########################################\n\n@eager.register(Integrate, Delta, Funsor, frozenset)\ndef eager_integrate(delta, integrand, reduced_vars):\n    if not reduced_vars & delta.fresh:\n        return None\n    subs = tuple((name, point) for name, (point, log_density) in delta.terms\n                 if name in reduced_vars)\n    new_integrand = Subs(integrand, subs)\n    new_log_measure = Subs(delta, subs)\n    result = Integrate(new_log_measure, new_integrand, reduced_vars - delta.fresh)\n    return result\n\n\n########################################\n# Gaussian patterns\n########################################\n\n@eager.register(Integrate, Gaussian, Variable, frozenset)\ndef eager_integrate(log_measure, integrand, reduced_vars):\n    real_vars = frozenset(k for k in reduced_vars if log_measure.inputs[k].dtype == \'real\')\n    if real_vars == frozenset([integrand.name]):\n        loc = ops.cholesky_solve(ops.unsqueeze(log_measure.info_vec, -1), log_measure._precision_chol).squeeze(-1)\n        data = loc * ops.unsqueeze(ops.exp(log_measure.log_normalizer.data), -1)\n        data = data.reshape(loc.shape[:-1] + integrand.output.shape)\n        inputs = OrderedDict((k, d) for k, d in log_measure.inputs.items() if d.dtype != \'real\')\n        result = Tensor(data, inputs)\n        return result.reduce(ops.add, reduced_vars - real_vars)\n    return None  # defer to default implementation\n\n\n@eager.register(Integrate, Gaussian, Gaussian, frozenset)\ndef eager_integrate(log_measure, integrand, reduced_vars):\n    real_vars = frozenset(k for k in reduced_vars if log_measure.inputs[k].dtype == \'real\')\n    if real_vars:\n\n        lhs_reals = frozenset(k for k, d in log_measure.inputs.items() if d.dtype == \'real\')\n        rhs_reals = frozenset(k for k, d in integrand.inputs.items() if d.dtype == \'real\')\n        if lhs_reals == real_vars and rhs_reals <= real_vars:\n            inputs = OrderedDict((k, d) for t in (log_measure, integrand)\n                                 for k, d in t.inputs.items())\n            lhs_info_vec, lhs_precision = align_gaussian(inputs, log_measure)\n            rhs_info_vec, rhs_precision = align_gaussian(inputs, integrand)\n            lhs = Gaussian(lhs_info_vec, lhs_precision, inputs)\n\n            # Compute the expectation of a non-normalized quadratic form.\n            # See ""The Matrix Cookbook"" (November 15, 2012) ss. 8.2.2 eq. 380.\n            # http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf\n            norm = ops.exp(lhs.log_normalizer.data)\n            lhs_cov = ops.cholesky_inverse(lhs._precision_chol)\n            lhs_loc = ops.cholesky_solve(ops.unsqueeze(lhs.info_vec, -1), lhs._precision_chol).squeeze(-1)\n            vmv_term = _vv(lhs_loc, rhs_info_vec - 0.5 * _mv(rhs_precision, lhs_loc))\n            data = norm * (vmv_term - 0.5 * _trace_mm(rhs_precision, lhs_cov))\n            inputs = OrderedDict((k, d) for k, d in inputs.items() if k not in reduced_vars)\n            result = Tensor(data, inputs)\n            return result.reduce(ops.add, reduced_vars - real_vars)\n\n        raise NotImplementedError(\'TODO implement partial integration\')\n\n    return None  # defer to default implementation\n\n\n__all__ = [\n    \'Integrate\',\n]\n'"
funsor/interpreter.py,3,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport inspect\nimport os\nimport re\nimport types\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\nfrom functools import singledispatch\n\nimport numpy as np\n\nfrom funsor.domains import Domain\nfrom funsor.ops import Op, is_numeric_array\nfrom funsor.registry import KeyedRegistry\nfrom funsor.util import is_nn_module\n\n_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n_DEBUG = int(os.environ.get(""FUNSOR_DEBUG"", 0))\n_STACK_SIZE = 0\n\n_INTERPRETATION = None  # To be set later in funsor.terms\n_USE_TCO = int(os.environ.get(""FUNSOR_USE_TCO"", 0))\n\n_GENSYM_COUNTER = 0\n\n\ndef _indent():\n    result = u\'    \\u2502\' * (_STACK_SIZE // 4 + 3)\n    return result[:_STACK_SIZE]\n\n\nif _DEBUG:\n    class DebugLogged(object):\n        def __init__(self, fn):\n            self.fn = fn\n            while isinstance(fn, functools.partial):\n                fn = fn.func\n            path = inspect.getabsfile(fn)\n            lineno = inspect.getsourcelines(fn)[1]\n            self._message = ""{} file://{} {}"".format(fn.__name__, path, lineno)\n\n        def __call__(self, *args, **kwargs):\n            global _STACK_SIZE\n            print(_indent() + self._message)\n            _STACK_SIZE += 1\n            try:\n                return self.fn(*args, **kwargs)\n            finally:\n                _STACK_SIZE -= 1\n\n        @property\n        def register(self):\n            return self.fn.register\n\n    def debug_logged(fn):\n        if isinstance(fn, DebugLogged):\n            return fn\n        return DebugLogged(fn)\nelse:\n    def debug_logged(fn):\n        return fn\n\n\ndef _classname(cls):\n    return getattr(cls, ""classname"", cls.__name__)\n\n\nclass Interpreter:\n    @property\n    def __call__(self):\n        return _INTERPRETATION\n\n\ndef debug_interpret(cls, *args):\n    global _STACK_SIZE\n    indent = _indent()\n    if _DEBUG > 1:\n        typenames = [_classname(cls)] + [_classname(type(arg)) for arg in args]\n    else:\n        typenames = [cls.__name__] + [type(arg).__name__ for arg in args]\n    print(indent + \' \'.join(typenames))\n\n    _STACK_SIZE += 1\n    try:\n        result = _INTERPRETATION(cls, *args)\n    finally:\n        _STACK_SIZE -= 1\n\n    if _DEBUG > 1:\n        result_str = re.sub(\'\\n\', \'\\n          \' + indent, str(result))\n    else:\n        result_str = type(result).__name__\n    print(indent + \'-> \' + result_str)\n    return result\n\n\ninterpret = debug_interpret if _DEBUG else Interpreter()\n\n\ndef set_interpretation(new):\n    assert callable(new)\n    global _INTERPRETATION\n    _INTERPRETATION = new\n\n\n@contextmanager\ndef interpretation(new):\n    assert callable(new)\n    global _INTERPRETATION\n    old = _INTERPRETATION\n    try:\n        _INTERPRETATION = new\n        yield\n    finally:\n        _INTERPRETATION = old\n\n\n@singledispatch\ndef recursion_reinterpret(x):\n    r""""""\n    Overloaded reinterpretation of a deferred expression.\n    This interpreter uses the Python stack and is subject to the recursion limit.\n\n    This handles a limited class of expressions, raising\n    ``ValueError`` in unhandled cases.\n\n    :param x: An input, typically involving deferred\n        :class:`~funsor.terms.Funsor` s.\n    :type x: A funsor or data structure holding funsors.\n    :return: A reinterpreted version of the input.\n    :raises: ValueError\n    """"""\n    raise ValueError(type(x))\n\n\n# We need to register this later in terms.py after declaring Funsor.\n# reinterpret.register(Funsor)\n@debug_logged\ndef reinterpret_funsor(x):\n    return _INTERPRETATION(type(x), *map(recursion_reinterpret, x._ast_values))\n\n\n_ground_types = (\n    str,\n    int,\n    float,\n    type,\n    functools.partial,\n    types.FunctionType,\n    types.BuiltinFunctionType,\n    Domain,\n    Op,\n    np.generic,\n    np.ndarray,\n    np.ufunc,\n)\n\n\nfor t in _ground_types:\n    @recursion_reinterpret.register(t)\n    def recursion_reinterpret_ground(x):\n        return x\n\n\n@recursion_reinterpret.register(tuple)\n@debug_logged\ndef recursion_reinterpret_tuple(x):\n    return tuple(map(recursion_reinterpret, x))\n\n\n@recursion_reinterpret.register(frozenset)\n@debug_logged\ndef recursion_reinterpret_frozenset(x):\n    return frozenset(map(recursion_reinterpret, x))\n\n\n@recursion_reinterpret.register(dict)\n@debug_logged\ndef recursion_reinterpret_dict(x):\n    return {key: recursion_reinterpret(value) for key, value in x.items()}\n\n\n@recursion_reinterpret.register(OrderedDict)\n@debug_logged\ndef recursion_reinterpret_ordereddict(x):\n    return OrderedDict((key, recursion_reinterpret(value)) for key, value in x.items())\n\n\n@singledispatch\ndef children(x):\n    raise ValueError(type(x))\n\n\n# has to be registered in terms.py\ndef children_funsor(x):\n    return x._ast_values\n\n\n@children.register(tuple)\n@children.register(frozenset)\ndef _children_tuple(x):\n    return x\n\n\n@children.register(dict)\n@children.register(OrderedDict)\ndef _children_tuple(x):\n    return x.values()\n\n\nfor t in _ground_types:\n    @children.register(t)\n    def _children_ground(x):\n        return ()\n\n\ndef is_atom(x):\n    if isinstance(x, (tuple, frozenset)) and not isinstance(x, Domain):\n        return len(x) == 0 or all(is_atom(c) for c in x)\n    return isinstance(x, _ground_types) or is_numeric_array(x) or is_nn_module(x)\n\n\ndef gensym(x=None):\n    global _GENSYM_COUNTER\n    _GENSYM_COUNTER += 1\n    sym = _GENSYM_COUNTER\n    if x is not None:\n        if isinstance(x, str):\n            return x + ""_"" + str(sym)\n        return id(x)\n    return ""V"" + str(sym)\n\n\ndef stack_reinterpret(x):\n    r""""""\n    Overloaded reinterpretation of a deferred expression.\n    This interpreter uses an explicit stack and no recursion but is much slower.\n\n    This handles a limited class of expressions, raising\n    ``ValueError`` in unhandled cases.\n\n    :param x: An input, typically involving deferred\n        :class:`~funsor.terms.Funsor` s.\n    :type x: A funsor or data structure holding funsors.\n    :return: A reinterpreted version of the input.\n    :raises: ValueError\n    """"""\n    x_name = gensym(x)\n    node_vars = {x_name: x}\n    node_names = {x: x_name}\n    env = {}\n    stack = [(x_name, x)]\n    parent_to_children = OrderedDict()\n    child_to_parents = OrderedDict()\n    while stack:\n        h_name, h = stack.pop(0)\n        parent_to_children[h_name] = []\n        for c in children(h):\n            if c in node_names:\n                c_name = node_names[c]\n            else:\n                c_name = gensym(c)\n                node_names[c] = c_name\n                node_vars[c_name] = c\n                stack.append((c_name, c))\n            parent_to_children.setdefault(h_name, []).append(c_name)\n            child_to_parents.setdefault(c_name, []).append(h_name)\n\n    children_counts = OrderedDict((k, len(v)) for k, v in parent_to_children.items())\n    leaves = [name for name, count in children_counts.items() if count == 0]\n    while leaves:\n        h_name = leaves.pop(0)\n        if h_name in child_to_parents:\n            for parent in child_to_parents[h_name]:\n                children_counts[parent] -= 1\n                if children_counts[parent] == 0:\n                    leaves.append(parent)\n\n        h = node_vars[h_name]\n        if is_atom(h):\n            env[h_name] = h\n        elif isinstance(h, (tuple, frozenset)):\n            env[h_name] = type(h)(\n                env[c_name] for c_name in parent_to_children[h_name])\n        else:\n            env[h_name] = _INTERPRETATION(\n                type(h), *(env[c_name] for c_name in parent_to_children[h_name]))\n\n    return env[x_name]\n\n\ndef reinterpret(x):\n    r""""""\n    Overloaded reinterpretation of a deferred expression.\n\n    This handles a limited class of expressions, raising\n    ``ValueError`` in unhandled cases.\n\n    :param x: An input, typically involving deferred\n        :class:`~funsor.terms.Funsor` s.\n    :type x: A funsor or data structure holding funsors.\n    :return: A reinterpreted version of the input.\n    :raises: ValueError\n    """"""\n    if _USE_TCO:\n        return stack_reinterpret(x)\n    else:\n        return recursion_reinterpret(x)\n\n\ndef dispatched_interpretation(fn):\n    """"""\n    Decorator to create a dispatched interpretation function.\n    """"""\n    registry = KeyedRegistry(default=lambda *args: None)\n    if _DEBUG:\n        fn.register = lambda *args: lambda fn: registry.register(*args)(debug_logged(fn))\n    else:\n        fn.register = registry.register\n    fn.dispatch = registry.dispatch\n    return fn\n\n\nclass PatternMissingError(NotImplementedError):\n    def __str__(self):\n        return f""{super().__str__()}\\nThis is most likely due to a missing pattern.""\n\n\n__all__ = [\n    \'PatternMissingError\',\n    \'dispatched_interpretation\',\n    \'interpret\',\n    \'interpretation\',\n    \'reinterpret\',\n    \'set_interpretation\',\n]\n'"
funsor/joint.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nfrom collections import OrderedDict\nfrom functools import reduce\nfrom typing import Tuple, Union\n\nfrom multipledispatch import dispatch\nfrom multipledispatch.variadic import Variadic\n\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction, GaussianMixture\nfrom funsor.delta import Delta\nfrom funsor.domains import bint\nfrom funsor.gaussian import Gaussian, align_gaussian\nfrom funsor.ops import AssociativeOp\nfrom funsor.tensor import Tensor, align_tensor\nfrom funsor.terms import Funsor, Independent, Number, Reduce, Unary, eager, moment_matching, normalize\n\n\n@dispatch(str, str, Variadic[(Gaussian, GaussianMixture)])\ndef eager_cat_homogeneous(name, part_name, *parts):\n    assert parts\n    output = parts[0].output\n    inputs = OrderedDict([(part_name, None)])\n    for part in parts:\n        assert part.output == output\n        assert part_name in part.inputs\n        inputs.update(part.inputs)\n\n    int_inputs = OrderedDict((k, v) for k, v in inputs.items() if v.dtype != ""real"")\n    real_inputs = OrderedDict((k, v) for k, v in inputs.items() if v.dtype == ""real"")\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n    discretes = []\n    info_vecs = []\n    precisions = []\n    for part in parts:\n        inputs[part_name] = part.inputs[part_name]\n        int_inputs[part_name] = inputs[part_name]\n        shape = tuple(d.size for d in int_inputs.values())\n        if isinstance(part, Gaussian):\n            discrete = None\n            gaussian = part\n        elif issubclass(type(part), GaussianMixture):  # TODO figure out why isinstance isn\'t working\n            discrete, gaussian = part.terms[0], part.terms[1]\n            discrete = ops.expand(align_tensor(int_inputs, discrete), shape)\n        else:\n            raise NotImplementedError(""TODO"")\n        discretes.append(discrete)\n        info_vec, precision = align_gaussian(inputs, gaussian)\n        info_vecs.append(ops.expand(info_vec, shape + (-1,)))\n        precisions.append(ops.expand(precision, shape + (-1, -1)))\n    if part_name != name:\n        del inputs[part_name]\n        del int_inputs[part_name]\n\n    dim = 0\n    info_vec = ops.cat(dim, *info_vecs)\n    precision = ops.cat(dim, *precisions)\n    inputs[name] = bint(info_vec.shape[dim])\n    int_inputs[name] = inputs[name]\n    result = Gaussian(info_vec, precision, inputs)\n    if any(d is not None for d in discretes):\n        for i, d in enumerate(discretes):\n            if d is None:\n                discretes[i] = ops.new_zeros(info_vecs[i], info_vecs[i].shape[:-1])\n        discrete = ops.cat(dim, *discretes)\n        result = result + Tensor(discrete, int_inputs)\n    return result\n\n\n#################################\n# patterns for moment-matching\n#################################\n\n@moment_matching.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Variadic[object])\ndef moment_matching_contract_default(*args):\n    return None\n\n\n@moment_matching.register(Contraction, ops.LogAddExpOp, ops.AddOp, frozenset, (Number, Tensor), Gaussian)\ndef moment_matching_contract_joint(red_op, bin_op, reduced_vars, discrete, gaussian):\n\n    approx_vars = frozenset(k for k in reduced_vars if k in gaussian.inputs\n                            and gaussian.inputs[k].dtype != \'real\')\n    exact_vars = reduced_vars - approx_vars\n\n    if exact_vars and approx_vars:\n        return Contraction(red_op, bin_op, exact_vars, discrete, gaussian).reduce(red_op, approx_vars)\n\n    if approx_vars and not exact_vars:\n        discrete += gaussian.log_normalizer\n        new_discrete = discrete.reduce(ops.logaddexp, approx_vars.intersection(discrete.inputs))\n        new_discrete = discrete.reduce(ops.logaddexp, approx_vars.intersection(discrete.inputs))\n        num_elements = reduce(ops.mul, [\n            gaussian.inputs[k].num_elements for k in approx_vars.difference(discrete.inputs)], 1)\n        if num_elements != 1:\n            new_discrete -= math.log(num_elements)\n\n        int_inputs = OrderedDict((k, d) for k, d in gaussian.inputs.items() if d.dtype != \'real\')\n        probs = (discrete - new_discrete.clamp_finite()).exp()\n\n        old_loc = Tensor(ops.cholesky_solve(ops.unsqueeze(gaussian.info_vec, -1), gaussian._precision_chol).squeeze(-1),\n                         int_inputs)\n        new_loc = (probs * old_loc).reduce(ops.add, approx_vars)\n        old_cov = Tensor(ops.cholesky_inverse(gaussian._precision_chol), int_inputs)\n        diff = old_loc - new_loc\n        outers = Tensor(ops.unsqueeze(diff.data, -1) * ops.unsqueeze(diff.data, -2), diff.inputs)\n        new_cov = ((probs * old_cov).reduce(ops.add, approx_vars) +\n                   (probs * outers).reduce(ops.add, approx_vars))\n\n        # Numerically stabilize by adding bogus precision to empty components.\n        total = probs.reduce(ops.add, approx_vars)\n        mask = ops.unsqueeze(ops.unsqueeze((total.data == 0), -1), -1)\n        new_cov.data = new_cov.data + mask * ops.new_eye(new_cov.data, new_cov.data.shape[-1:])\n\n        new_precision = Tensor(ops.cholesky_inverse(ops.cholesky(new_cov.data)), new_cov.inputs)\n        new_info_vec = (new_precision.data @ ops.unsqueeze(new_loc.data, -1)).squeeze(-1)\n        new_inputs = new_loc.inputs.copy()\n        new_inputs.update((k, d) for k, d in gaussian.inputs.items() if d.dtype == \'real\')\n        new_gaussian = Gaussian(new_info_vec, new_precision.data, new_inputs)\n        new_discrete -= new_gaussian.log_normalizer\n\n        return new_discrete + new_gaussian\n\n    return None\n\n\n####################################################\n# Patterns for normalizing\n####################################################\n\n\n@eager.register(Reduce, ops.AddOp, Unary[ops.ExpOp, Funsor], frozenset)\ndef eager_reduce_exp(op, arg, reduced_vars):\n    # x.exp().reduce(ops.add) == x.reduce(ops.logaddexp).exp()\n    log_result = arg.arg.reduce(ops.logaddexp, reduced_vars)\n    if log_result is not normalize(Reduce, ops.logaddexp, arg.arg, reduced_vars):\n        return log_result.exp()\n    return None\n\n\n@eager.register(Independent,\n                (Contraction[ops.NullOp, ops.AddOp, frozenset, Tuple[Delta, Union[Number, Tensor], Gaussian]],\n                 Contraction[ops.NullOp, ops.AddOp, frozenset, Tuple[Delta, Union[Number, Tensor, Gaussian]]]),\n                str, str, str)\ndef eager_independent_joint(joint, reals_var, bint_var, diag_var):\n    if diag_var not in joint.terms[0].fresh:\n        return None\n\n    delta = Independent(joint.terms[0], reals_var, bint_var, diag_var)\n    new_terms = (delta,) + tuple(t.reduce(ops.add, bint_var) for t in joint.terms[1:])\n    return reduce(joint.bin_op, new_terms)\n'"
funsor/memoize.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import Hashable\nfrom contextlib import contextmanager\n\nimport funsor.interpreter as interpreter\n\n\n@contextmanager\ndef memoize(cache=None):\n    """"""\n    Exploit cons-hashing to do implicit common subexpression elimination\n    """"""\n    if cache is None:\n        cache = {}\n\n    @interpreter.interpretation(interpreter._INTERPRETATION)  # use base\n    def memoize_interpretation(cls, *args):\n        key = (cls,) + tuple(id(arg) if (type(arg).__name__ == ""DeviceArray"") or not isinstance(arg, Hashable)\n                             else arg for arg in args)\n        if key not in cache:\n            cache[key] = cls(*args)\n        return cache[key]\n\n    with interpreter.interpretation(memoize_interpretation):\n        yield cache\n'"
funsor/minipyro.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nMini Pyro\n---------\n\nThis file contains a minimal implementation of the Pyro Probabilistic\nProgramming Language. The API (method signatures, etc.) match that of\nthe full implementation as closely as possible. This file is independent\nof the rest of Pyro, with the exception of the :mod:`pyro.distributions`\nmodule.\n\nAn accompanying example that makes use of this implementation can be\nfound at examples/minipyro.py.\n""""""\nimport functools\nimport warnings\nimport weakref\nfrom collections import OrderedDict, namedtuple\n\nimport torch\nfrom pyro.distributions import validation_enabled\nfrom pyro.optim.clipped_adam import ClippedAdam as _ClippedAdam\n\nimport funsor\n\n\n# Funsor repreresents distributions in a fundamentally different way from\n# torch.Distributions and Pyro: funsor distributions are densities whereas\n# torch Distributions are samplers. This class is a compatibility wrapper\n# between the two. It is used only internally in the sample() function.\nclass Distribution(object):\n    def __init__(self, funsor_dist, sample_inputs=None):\n        assert isinstance(funsor_dist, funsor.Funsor)\n        assert not sample_inputs or all(isinstance(inp.dtype, int) for inp in sample_inputs.values())\n        self.funsor_dist = funsor_dist\n        self.output = self.funsor_dist.inputs[""value""]\n        self.sample_inputs = sample_inputs\n\n    def log_prob(self, value):\n        result = self.funsor_dist(value=value)\n        if self.sample_inputs:\n            result = result + funsor.tensor.Tensor(\n                torch.zeros(*(size.dtype for size in self.sample_inputs.values())),\n                self.sample_inputs\n            )\n        return result\n\n    # Draw a sample.\n    def __call__(self):\n        with funsor.interpreter.interpretation(funsor.terms.eager):\n            dist = self.funsor_dist(value=\'value\')\n            delta = dist.sample(frozenset([\'value\']), sample_inputs=self.sample_inputs)\n        if isinstance(delta, funsor.cnf.Contraction):\n            assert len(delta.terms) == 2\n            assert any(isinstance(t, funsor.delta.Delta) for t in delta.terms)\n            delta = [t for t in delta.terms if isinstance(t, funsor.delta.Delta)][0]\n        assert isinstance(delta, funsor.delta.Delta)\n        return delta.terms[0][1][0]\n\n    # Similar to torch.distributions.Distribution.expand().\n    def expand_inputs(self, name, size):\n        if name in self.funsor_dist.inputs:\n            assert self.funsor_dist.inputs[name] == funsor.bint(int(size))\n            return self\n        inputs = OrderedDict([(name, funsor.bint(int(size)))])\n        if self.sample_inputs:\n            inputs.update(self.sample_inputs)\n        return Distribution(self.funsor_dist, sample_inputs=inputs)\n\n\n# Pyro keeps track of two kinds of global state:\n# i)  The effect handler stack, which enables non-standard interpretations of\n#     Pyro primitives like sample();\n#     See http://docs.pyro.ai/en/0.3.1/poutine.html\n# ii) Trainable parameters in the Pyro ParamStore;\n#     See http://docs.pyro.ai/en/0.3.1/parameters.html\n\nPYRO_STACK = []\nPARAM_STORE = {}  # maps name -> (unconstrained_value, constraint)\n\n\ndef get_param_store():\n    return PARAM_STORE\n\n\n# The base effect handler class (called Messenger here for consistency with Pyro).\nclass Messenger(object):\n    def __init__(self, fn=None):\n        self.fn = fn\n\n    # Effect handlers push themselves onto the PYRO_STACK.\n    # Handlers earlier in the PYRO_STACK are applied first.\n    def __enter__(self):\n        PYRO_STACK.append(self)\n\n    def __exit__(self, *args, **kwargs):\n        assert PYRO_STACK[-1] is self\n        PYRO_STACK.pop()\n\n    def process_message(self, msg):\n        pass\n\n    def postprocess_message(self, msg):\n        pass\n\n    def __call__(self, *args, **kwargs):\n        with self:\n            return self.fn(*args, **kwargs)\n\n\n# A first useful example of an effect handler.\n# trace records the inputs and outputs of any primitive site it encloses,\n# and returns a dictionary containing that data to the user.\nclass trace(Messenger):\n    def __enter__(self):\n        super(trace, self).__enter__()\n        self.trace = OrderedDict()\n        return self.trace\n\n    # trace illustrates why we need postprocess_message in addition to process_message:\n    # We only want to record a value after all other effects have been applied\n    def postprocess_message(self, msg):\n        assert msg[""type""] != ""sample"" or msg[""name""] not in self.trace, \\\n            ""sample sites must have unique names""\n        self.trace[msg[""name""]] = msg.copy()\n\n    def get_trace(self, *args, **kwargs):\n        self(*args, **kwargs)\n        return self.trace\n\n\n# A second example of an effect handler for setting the value at a sample site.\n# This illustrates why effect handlers are a useful PPL implementation technique:\n# We can compose trace and replay to replace values but preserve distributions,\n# allowing us to compute the joint probability density of samples under a model.\n# See the definition of elbo(...) below for an example of this pattern.\nclass replay(Messenger):\n    def __init__(self, fn, guide_trace):\n        self.guide_trace = guide_trace\n        super(replay, self).__init__(fn)\n\n    def process_message(self, msg):\n        if msg[""name""] in self.guide_trace:\n            msg[""value""] = self.guide_trace[msg[""name""]][""value""]\n\n\n# block allows the selective application of effect handlers to different parts of a model.\n# Sites hidden by block will only have the handlers below block on the PYRO_STACK applied,\n# allowing inference or other effectful computations to be nested inside models.\nclass block(Messenger):\n    def __init__(self, fn=None, hide_fn=lambda msg: True):\n        self.hide_fn = hide_fn\n        super(block, self).__init__(fn)\n\n    def process_message(self, msg):\n        if self.hide_fn(msg):\n            msg[""stop""] = True\n\n\n# seed is used to fix the RNG state when calling a model.\nclass seed(Messenger):\n    def __init__(self, fn=None, rng_seed=None):\n        self.rng_seed = rng_seed\n        super(seed, self).__init__(fn)\n\n    def __enter__(self):\n        self.old_rng_state = torch.get_rng_state()\n        torch.manual_seed(self.rng_seed)\n\n    def __exit__(self, type, value, traceback):\n        torch.set_rng_state(self.old_rng_state)\n\n\n# Conditional independence is recorded as a plate context at each site.\nCondIndepStackFrame = namedtuple(""CondIndepStackFrame"", [""name"", ""size"", ""dim""])\n\n\n# This implementation of vectorized PlateMessenger broadcasts and\n# records a cond_indep_stack which is later used to convert\n# torch.Tensors to funsor.tensor.Tensors.\nclass PlateMessenger(Messenger):\n    def __init__(self, fn, name, size, dim):\n        assert dim < 0\n        self.frame = CondIndepStackFrame(name, size, dim)\n        super(PlateMessenger, self).__init__(fn)\n\n    def process_message(self, msg):\n        if msg[""type""] in (""sample"", ""param""):\n            assert self.frame.dim not in msg[""cond_indep_stack""]\n            msg[""cond_indep_stack""][self.frame.dim] = self.frame\n        if msg[""type""] == ""sample"":\n            msg[""fn""] = msg[""fn""].expand_inputs(self.frame.name, self.frame.size)\n\n\n# This converts raw tensor.Tensors to funsor.Funsors with .inputs and .output\n# based on information in msg[""cond_indep_stack""] and msg[""fn""].\ndef tensor_to_funsor(value, cond_indep_stack, output):\n    assert isinstance(value, torch.Tensor)\n    event_shape = output.shape\n    batch_shape = value.shape[:value.dim() - len(event_shape)]\n    if torch._C._get_tracing_state():\n        with funsor.tensor.ignore_jit_warnings():\n            batch_shape = tuple(map(int, batch_shape))\n    inputs = OrderedDict()\n    data = value\n    for dim, size in enumerate(batch_shape):\n        if size == 1:\n            data = data.squeeze(dim - value.dim())\n        else:\n            frame = cond_indep_stack[dim - len(batch_shape)]\n            assert size == frame.size, (size, frame)\n            inputs[frame.name] = funsor.bint(int(size))\n    value = funsor.tensor.Tensor(data, inputs, output.dtype)\n    assert value.output == output\n    return value\n\n\n# The log_joint messenger is the main way of recording log probabilities.\n# This is roughly the Funsor equivalent to pyro.poutine.trace.\nclass log_joint(Messenger):\n    def __enter__(self):\n        super(log_joint, self).__enter__()\n        self.log_factors = OrderedDict()  # maps site name to log_prob factor\n        self.plates = set()\n        return self\n\n    def process_message(self, msg):\n        if msg[""type""] == ""sample"":\n            if msg[""value""] is None:\n                # Create a delayed sample.\n                msg[""value""] = funsor.Variable(msg[""name""], msg[""fn""].output)\n\n    def postprocess_message(self, msg):\n        if msg[""type""] == ""sample"":\n            assert msg[""name""] not in self.log_factors, ""all sites must have unique names""\n            log_prob = msg[""fn""].log_prob(msg[""value""])\n            self.log_factors[msg[""name""]] = log_prob\n            self.plates.update(f.name for f in msg[""cond_indep_stack""].values())\n\n\n# apply_stack is called by pyro.sample and pyro.param.\n# It is responsible for applying each Messenger to each effectful operation.\ndef apply_stack(msg):\n    for pointer, handler in enumerate(reversed(PYRO_STACK)):\n        handler.process_message(msg)\n        # When a Messenger sets the ""stop"" field of a message,\n        # it prevents any Messengers above it on the stack from being applied.\n        if msg.get(""stop""):\n            break\n    if msg[""value""] is None:\n        msg[""value""] = msg[""fn""](*msg[""args""])\n    if isinstance(msg[""value""], torch.Tensor):\n        msg[""value""] = tensor_to_funsor(msg[""value""], msg[""cond_indep_stack""], msg[""output""])\n\n    # A Messenger that sets msg[""stop""] == True also prevents application\n    # of postprocess_message by Messengers above it on the stack\n    # via the pointer variable from the process_message loop\n    for handler in PYRO_STACK[-pointer-1:]:\n        handler.postprocess_message(msg)\n    return msg\n\n\n# sample is an effectful version of Distribution.sample(...)\n# When any effect handlers are active, it constructs an initial message and calls apply_stack.\ndef sample(name, fn, obs=None, infer=None):\n    # Wrap the funsor distribution in a Pyro-compatible way.\n    fn = Distribution(fn)\n\n    # if there are no active Messengers, we just draw a sample and return it as expected:\n    if not PYRO_STACK:\n        return fn()\n\n    # Otherwise, we initialize a message...\n    initial_msg = {\n        ""type"": ""sample"",\n        ""name"": name,\n        ""fn"": fn,\n        ""args"": (),\n        ""value"": obs,\n        ""cond_indep_stack"": {},  # maps dim to CondIndepStackFrame\n        ""output"": fn.output,\n        ""infer"": {} if infer is None else infer,\n    }\n\n    # ...and use apply_stack to send it to the Messengers\n    msg = apply_stack(initial_msg)\n    assert isinstance(msg[""value""], funsor.Funsor)\n    return msg[""value""]\n\n\n# param is an effectful version of PARAM_STORE.setdefault that also handles constraints.\n# When any effect handlers are active, it constructs an initial message and calls apply_stack.\ndef param(name, init_value=None, constraint=torch.distributions.constraints.real, event_dim=None):\n    cond_indep_stack = {}\n    output = None\n    if init_value is not None:\n        if event_dim is None:\n            event_dim = init_value.dim()\n        output = funsor.reals(*init_value.shape[init_value.dim() - event_dim:])\n\n    def fn(init_value, constraint):\n        if name in PARAM_STORE:\n            unconstrained_value, constraint = PARAM_STORE[name]\n        else:\n            # Initialize with a constrained value.\n            assert init_value is not None\n            with torch.no_grad():\n                constrained_value = init_value.detach()\n                unconstrained_value = torch.distributions.transform_to(constraint).inv(constrained_value)\n            unconstrained_value.requires_grad_()\n            unconstrained_value._funsor_metadata = (cond_indep_stack, output)\n            PARAM_STORE[name] = unconstrained_value, constraint\n\n        # Transform from unconstrained space to constrained space.\n        constrained_value = torch.distributions.transform_to(constraint)(unconstrained_value)\n        constrained_value.unconstrained = weakref.ref(unconstrained_value)\n        return tensor_to_funsor(constrained_value, *unconstrained_value._funsor_metadata)\n\n    # if there are no active Messengers, we just draw a sample and return it as expected:\n    if not PYRO_STACK:\n        return fn(init_value, constraint)\n\n    # Otherwise, we initialize a message...\n    initial_msg = {\n        ""type"": ""param"",\n        ""name"": name,\n        ""fn"": fn,\n        ""args"": (init_value, constraint),\n        ""value"": None,\n        ""cond_indep_stack"": cond_indep_stack,  # maps dim to CondIndepStackFrame\n        ""output"": output,\n    }\n\n    # ...and use apply_stack to send it to the Messengers\n    msg = apply_stack(initial_msg)\n    assert isinstance(msg[""value""], funsor.Funsor)\n    return msg[""value""]\n\n\n# boilerplate to match the syntax of actual pyro.plate:\ndef plate(name, size, dim):\n    return PlateMessenger(fn=None, name=name, size=size, dim=dim)\n\n\n# This is a thin wrapper around the `torch.optim.Optimizer` class that\n# dynamically generates optimizers for dynamically generated parameters.\n# See http://docs.pyro.ai/en/0.3.1/optimization.html\nclass PyroOptim(object):\n    def __init__(self, optim_args):\n        self.optim_args = optim_args\n        # Each parameter will get its own optimizer, which we keep track\n        # of using this dictionary keyed on parameters.\n        self.optim_objs = {}\n\n    def __call__(self, params):\n        for param in params:\n            # If we\'ve seen this parameter before, use the previously\n            # constructed optimizer.\n            if param in self.optim_objs:\n                optim = self.optim_objs[param]\n            # If we\'ve never seen this parameter before, construct\n            # an Adam optimizer and keep track of it.\n            else:\n                optim = self.TorchOptimizer([param], **self.optim_args)\n                self.optim_objs[param] = optim\n            # Take a gradient step for the parameter param.\n            optim.step()\n\n\n# We wrap some commonly used PyTorch optimizers.\nclass Adam(PyroOptim):\n    TorchOptimizer = torch.optim.Adam\n\n\nclass ClippedAdam(PyroOptim):\n    TorchOptimizer = _ClippedAdam\n\n\n# This is a unified interface for stochastic variational inference in Pyro.\n# The actual construction of the loss is taken care of by `loss`.\n# See http://docs.pyro.ai/en/0.3.1/inference_algos.html\nclass SVI(object):\n    def __init__(self, model, guide, optim, loss):\n        self.model = model\n        self.guide = guide\n        self.optim = optim\n        self.loss = loss\n\n    # This method handles running the model and guide, constructing the loss\n    # function, and taking a gradient step.\n    def step(self, *args, **kwargs):\n        # This wraps both the call to `model` and `guide` in a `trace` so that\n        # we can record all the parameters that are encountered. Note that\n        # further tracing occurs inside of `loss`.\n        with trace() as param_capture:\n            # We use block here to allow tracing to record parameters only.\n            with block(hide_fn=lambda msg: msg[""type""] != ""param""):\n                loss = self.loss(self.model, self.guide, *args, **kwargs)\n        # Differentiate the loss.\n        funsor.to_data(loss).backward()\n        # Grab all the parameters from the trace.\n        params = [site[""value""].data.unconstrained()\n                  for site in param_capture.values()]\n        # Take a step w.r.t. each parameter in params.\n        self.optim(params)\n        # Zero out the gradients so that they don\'t accumulate.\n        for p in params:\n            p.grad = torch.zeros_like(p.grad)\n        return loss.item()\n\n\n# TODO(eb8680) Replace this with funsor.Expectation.\ndef Expectation(log_probs, costs, sum_vars, prod_vars):\n    result = 0\n    for cost in costs:\n        log_prob = funsor.sum_product.sum_product(\n            sum_op=funsor.ops.logaddexp,\n            prod_op=funsor.ops.add,\n            factors=log_probs,\n            plates=prod_vars,\n            eliminate=(prod_vars | sum_vars) - frozenset(cost.inputs)\n        )\n        term = funsor.Integrate(log_prob, cost, sum_vars & frozenset(cost.inputs))\n        term = term.reduce(funsor.ops.add, prod_vars & frozenset(cost.inputs))\n        result += term\n    return result\n\n\n# This is a basic implementation of the Evidence Lower Bound, which is the\n# fundamental objective in Variational Inference.\n# See http://pyro.ai/examples/svi_part_i.html for details.\n# This implementation uses a Dice estimator similar to TraceEnum_ELBO.\ndef elbo(model, guide, *args, **kwargs):\n    with log_joint() as guide_log_joint:\n        guide(*args, **kwargs)\n    with log_joint() as model_log_joint:\n        model(*args, **kwargs)\n\n    # contract out auxiliary variables in the guide\n    guide_log_probs = list(guide_log_joint.log_factors.values())\n    guide_aux_vars = frozenset().union(*(f.inputs for f in guide_log_probs)) - \\\n        frozenset(guide_log_joint.plates) - \\\n        frozenset(model_log_joint.log_factors)\n    if guide_aux_vars:\n        guide_log_probs = funsor.sum_product.partial_sum_product(\n            funsor.ops.logaddexp,\n            funsor.ops.add,\n            guide_log_probs,\n            plates=frozenset(guide_log_joint.plates),\n            eliminate=guide_aux_vars)\n\n    # contract out auxiliary variables in the model\n    model_log_probs = list(model_log_joint.log_factors.values())\n    model_aux_vars = frozenset().union(*(f.inputs for f in model_log_probs)) - \\\n        frozenset(model_log_joint.plates) - \\\n        frozenset(guide_log_joint.log_factors)\n    if model_aux_vars:\n        model_log_probs = funsor.sum_product.partial_sum_product(\n            funsor.ops.logaddexp,\n            funsor.ops.add,\n            model_log_probs,\n            plates=frozenset(model_log_joint.plates),\n            eliminate=model_aux_vars)\n\n    # compute remaining plates and sum_dims\n    plates = frozenset().union(\n        *(model_log_joint.plates.intersection(f.inputs) for f in model_log_probs))\n    plates = plates | frozenset().union(\n        *(guide_log_joint.plates.intersection(f.inputs) for f in guide_log_probs))\n    sum_vars = frozenset().union(model_log_joint.log_factors, guide_log_joint.log_factors) - \\\n        frozenset(model_aux_vars | guide_aux_vars)\n\n    # Accumulate costs from model and guide and log_probs from guide.\n    # Cf. pyro.infer.traceenum_elbo._compute_dice_elbo()\n    # https://github.com/pyro-ppl/pyro/blob/0.3.0/pyro/infer/traceenum_elbo.py#L119\n    costs = []\n    log_probs = []\n    for p in model_log_probs:\n        costs.append(p)\n    for q in guide_log_probs:\n        costs.append(-q)\n        log_probs.append(q)\n\n    # Compute expected cost.\n    # Cf. pyro.infer.util.Dice.compute_expectation()\n    # https://github.com/pyro-ppl/pyro/blob/0.3.0/pyro/infer/util.py#L212\n    elbo = Expectation(tuple(log_probs),\n                       tuple(costs),\n                       sum_vars=sum_vars,\n                       prod_vars=plates)\n\n    loss = -elbo\n    assert not loss.inputs\n    return loss\n\n\n# Base class for elbo implementations.\nclass ELBO(object):\n    def __init__(self, **kwargs):\n        self.options = kwargs\n\n    def __call__(self, model, guide, *args, **kwargs):\n        return elbo(model, guide, *args, **kwargs)\n\n\n# This is a wrapper for compatibility with full Pyro.\nclass Trace_ELBO(ELBO):\n    def __call__(self, model, guide, *args, **kwargs):\n        with funsor.montecarlo.monte_carlo_interpretation():\n            return elbo(model, guide, *args, **kwargs)\n\n\nclass TraceMeanField_ELBO(ELBO):\n    # TODO Use exact KLs where possible.\n    pass\n\n\nclass TraceEnum_ELBO(ELBO):\n    # TODO allow mixing of sampling and exact integration\n    def __call__(self, model, guide, *args, **kwargs):\n        if self.options.get(""optimize"", None):\n            with funsor.interpreter.interpretation(funsor.optimizer.optimize):\n                elbo_expr = elbo(model, guide, *args, **kwargs)\n            return funsor.reinterpret(elbo_expr)\n        return elbo(model, guide, *args, **kwargs)\n\n\n# This is a PyTorch jit wrapper that (1) delays tracing until the first\n# invocation, and (2) registers pyro.param() statements with torch.jit.trace.\n# This version does not support variable number of args or non-tensor kwargs.\nclass Jit(object):\n    def __init__(self, fn, **kwargs):\n        self.fn = fn\n        self.ignore_jit_warnings = kwargs.get(""ignore_jit_warnings"", False)\n        self._compiled = None\n        self._param_trace = None\n\n    def __call__(self, *args):\n        # On first call, initialize params and save their names.\n        if self._param_trace is None:\n            with block(), trace() as tr, block(hide_fn=lambda m: m[""type""] != ""param""):\n                self.fn(*args)\n            self._param_trace = tr\n\n        # Augment args with reads from the global param store.\n        unconstrained_params = tuple(param(name).data.unconstrained()\n                                     for name in self._param_trace)\n        params_and_args = unconstrained_params + args\n\n        # On first call, create a compiled elbo.\n        if self._compiled is None:\n\n            def compiled(*params_and_args):\n                unconstrained_params = params_and_args[:len(self._param_trace)]\n                args = params_and_args[len(self._param_trace):]\n                for name, unconstrained_param in zip(self._param_trace, unconstrained_params):\n                    constrained_param = param(name)  # assume param has been initialized\n                    assert constrained_param.data.unconstrained() is unconstrained_param\n                    self._param_trace[name][""value""] = constrained_param\n                result = replay(self.fn, guide_trace=self._param_trace)(*args)\n                assert not result.inputs\n                assert result.output == funsor.reals()\n                return funsor.to_data(result)\n\n            with validation_enabled(False), warnings.catch_warnings():\n                if self.ignore_jit_warnings:\n                    warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n                self._compiled = torch.jit.trace(compiled, params_and_args, check_trace=False)\n\n        data = self._compiled(*params_and_args)\n        return funsor.tensor.Tensor(data)\n\n\n# This is a jit wrapper for ELBO implementations.\nclass Jit_ELBO(ELBO):\n    def __init__(self, elbo, **kwargs):\n        super(Jit_ELBO, self).__init__(**kwargs)\n        self._elbo = elbo(**kwargs)\n        self._compiled = {}  # maps (model,guide) -> Jit instances\n\n    def __call__(self, model, guide, *args):\n        if (model, guide) not in self._compiled:\n            elbo = functools.partial(self._elbo, model, guide)\n            self._compiled[model, guide] = Jit(elbo, **self.options)\n        return self._compiled[model, guide](*args)\n\n\ndef JitTrace_ELBO(**kwargs):\n    return Jit_ELBO(Trace_ELBO, **kwargs)\n\n\ndef JitTraceMeanField_ELBO(**kwargs):\n    return Jit_ELBO(TraceMeanField_ELBO, **kwargs)\n\n\ndef JitTraceEnum_ELBO(**kwargs):\n    return Jit_ELBO(TraceEnum_ELBO, **kwargs)\n'"
funsor/montecarlo.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\n\nfrom funsor.integrate import Integrate\nfrom funsor.interpreter import dispatched_interpretation, interpretation\nfrom funsor.terms import Funsor, eager\n\n\n@dispatched_interpretation\ndef monte_carlo(cls, *args):\n    """"""\n    A Monte Carlo interpretation of :class:`~funsor.integrate.Integrate`\n    expressions. This falls back to :class:`~funsor.terms.eager` in other\n    cases.\n    """"""\n    # TODO Memoize sample statements in a context manager.\n    result = monte_carlo.dispatch(cls, *args)(*args)\n    if result is None:\n        result = eager(cls, *args)\n    return result\n\n\n# This is a globally configurable parameter to draw multiple samples.\nmonte_carlo.sample_inputs = OrderedDict()\n\n\n@contextmanager\ndef monte_carlo_interpretation(**sample_inputs):\n    """"""\n    Context manager to set ``monte_carlo.sample_inputs`` and\n    install the :func:`monte_carlo` interpretation.\n    """"""\n    old = monte_carlo.sample_inputs\n    monte_carlo.sample_inputs = OrderedDict(sample_inputs)\n    try:\n        with interpretation(monte_carlo):\n            yield\n    finally:\n        monte_carlo.sample_inputs = old\n\n\n@monte_carlo.register(Integrate, Funsor, Funsor, frozenset)\ndef monte_carlo_integrate(log_measure, integrand, reduced_vars):\n    # FIXME: how to pass rng_key to here?\n    sample = log_measure.sample(reduced_vars, monte_carlo.sample_inputs)\n    if sample is log_measure:\n        return None  # cannot progress\n    reduced_vars |= frozenset(monte_carlo.sample_inputs).intersection(sample.inputs)\n    return Integrate(sample, integrand, reduced_vars)\n\n\n__all__ = [\n    \'monte_carlo\',\n    \'monte_carlo_interpretation\'\n]\n'"
funsor/ops.py,55,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport operator\nfrom numbers import Number\n\nimport numpy as np\nfrom multipledispatch import Dispatcher\n\n_builtin_abs = abs\n_builtin_all = all\n_builtin_any = any\n_builtin_max = max\n_builtin_min = min\n_builtin_pow = pow\n_builtin_sum = sum\n\n\nclass Op(Dispatcher):\n    def __init__(self, fn):\n        super(Op, self).__init__(fn.__name__)\n        # register as default operation\n        for nargs in (1, 2):\n            default_signature = (object,) * nargs\n            self.add(default_signature, fn)\n\n    def __repr__(self):\n        return ""ops."" + self.__name__\n\n    def __str__(self):\n        return self.__name__\n\n\nclass TransformOp(Op):\n    def set_inv(self, fn):\n        """"""\n        :param callable fn: A function that inputs an arg ``y`` and outputs a\n            value ``x`` such that ``y=self(x)``.\n        """"""\n        assert callable(fn)\n        self.inv = fn\n        return fn\n\n    def set_log_abs_det_jacobian(self, fn):\n        """"""\n        :param callable fn: A function that inputs two args ``x, y``, where\n            ``y=self(x)``, and returns ``log(abs(det(dy/dx)))``.\n        """"""\n        assert callable(fn)\n        self.log_abs_det_jacobian = fn\n        return fn\n\n    @staticmethod\n    def inv(x):\n        raise NotImplementedError\n\n    @staticmethod\n    def log_abs_det_jacobian(x, y):\n        raise NotImplementedError\n\n\n# FIXME Most code assumes this is an AssociativeCommutativeOp.\nclass AssociativeOp(Op):\n    pass\n\n\nclass AddOp(AssociativeOp):\n    pass\n\n\nclass MulOp(AssociativeOp):\n    pass\n\n\nclass MatmulOp(Op):  # Associtive but not commutative.\n    pass\n\n\nclass LogAddExpOp(AssociativeOp):\n    pass\n\n\nclass SampleOp(LogAddExpOp):\n    pass\n\n\nclass SubOp(Op):\n    pass\n\n\nclass NegOp(Op):\n    pass\n\n\nclass DivOp(Op):\n    pass\n\n\nclass NullOp(AssociativeOp):\n    """"""Placeholder associative op that unifies with any other op""""""\n    pass\n\n\n@NullOp\ndef nullop(x, y):\n    raise ValueError(""should never actually evaluate this!"")\n\n\nclass ReshapeMeta(type):\n    _cache = {}\n\n    def __call__(cls, shape):\n        shape = tuple(shape)\n        try:\n            return ReshapeMeta._cache[shape]\n        except KeyError:\n            instance = super().__call__(shape)\n            ReshapeMeta._cache[shape] = instance\n            return instance\n\n\nclass ReshapeOp(Op, metaclass=ReshapeMeta):\n    def __init__(self, shape):\n        self.shape = shape\n        super().__init__(self._default)\n\n    def _default(self, x):\n        return x.reshape(self.shape)\n\n\nclass GetitemMeta(type):\n    _cache = {}\n\n    def __call__(cls, offset):\n        try:\n            return GetitemMeta._cache[offset]\n        except KeyError:\n            instance = super(GetitemMeta, cls).__call__(offset)\n            GetitemMeta._cache[offset] = instance\n            return instance\n\n\nclass GetitemOp(Op, metaclass=GetitemMeta):\n    """"""\n    Op encoding an index into one dimension, e.g. ``x[:,:,y]`` for offset of 2.\n    """"""\n    def __init__(self, offset):\n        assert isinstance(offset, int)\n        assert offset >= 0\n        self.offset = offset\n        self._prefix = (slice(None),) * offset\n        super(GetitemOp, self).__init__(self._default)\n        self.__name__ = \'GetitemOp({})\'.format(offset)\n\n    def _default(self, x, y):\n        return x[self._prefix + (y,)] if self.offset else x[y]\n\n\ngetitem = GetitemOp(0)\nabs = Op(_builtin_abs)\neq = Op(operator.eq)\nge = Op(operator.ge)\ngt = Op(operator.gt)\ninvert = Op(operator.invert)\nle = Op(operator.le)\nlt = Op(operator.lt)\nne = Op(operator.ne)\nneg = NegOp(operator.neg)\nsub = SubOp(operator.sub)\ntruediv = DivOp(operator.truediv)\n\nadd = AddOp(operator.add)\nand_ = AssociativeOp(operator.and_)\nmul = MulOp(operator.mul)\nmatmul = MatmulOp(operator.matmul)\nor_ = AssociativeOp(operator.or_)\nxor = AssociativeOp(operator.xor)\n\n\n@add.register(object)\ndef _unary_add(x):\n    return x.sum()\n\n\n@Op\ndef sqrt(x):\n    return np.sqrt(x)\n\n\nclass ExpOp(TransformOp):\n    pass\n\n\n@ExpOp\ndef exp(x):\n    return np.exp(x)\n\n\n@exp.set_log_abs_det_jacobian\ndef log_abs_det_jacobian(x, y):\n    return add(x)\n\n\nclass LogOp(TransformOp):\n    pass\n\n\n@LogOp\ndef log(x):\n    if isinstance(x, bool) or (isinstance(x, np.ndarray) and x.dtype == \'bool\'):\n        return np.where(x, 0., float(\'-inf\'))\n    with np.errstate(divide=\'ignore\'):  # skip the warning of log(0.)\n        return np.log(x)\n\n\n@log.set_log_abs_det_jacobian\ndef log_abs_det_jacobian(x, y):\n    return -add(y)\n\n\nexp.set_inv(log)\nlog.set_inv(exp)\n\n\n@Op\ndef log1p(x):\n    return np.log1p(x)\n\n\n@Op\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\n@Op\ndef pow(x, y):\n    return x ** y\n\n\n@AssociativeOp\ndef min(x, y):\n    if hasattr(x, \'__min__\'):\n        return x.__min__(y)\n    if hasattr(y, \'__min__\'):\n        return y.__min__(x)\n    return _builtin_min(x, y)\n\n\n@AssociativeOp\ndef max(x, y):\n    if hasattr(x, \'__max__\'):\n        return x.__max__(y)\n    if hasattr(y, \'__max__\'):\n        return y.__max__(x)\n    return _builtin_max(x, y)\n\n\ndef _logaddexp(x, y):\n    if hasattr(x, ""__logaddexp__""):\n        return x.__logaddexp__(y)\n    if hasattr(y, ""__rlogaddexp__""):\n        return y.__logaddexp__(x)\n    shift = max(x, y)\n    return log(exp(x - shift) + exp(y - shift)) + shift\n\n\nlogaddexp = LogAddExpOp(_logaddexp)\nsample = SampleOp(_logaddexp)\n\n\n@SubOp\ndef safesub(x, y):\n    if isinstance(y, Number):\n        return sub(x, y)\n\n\n@DivOp\ndef safediv(x, y):\n    if isinstance(y, Number):\n        return truediv(x, y)\n\n\nclass ReciprocalOp(Op):\n    pass\n\n\n@ReciprocalOp\ndef reciprocal(x):\n    if isinstance(x, Number):\n        return 1. / x\n    raise ValueError(""No reciprocal for type {}"".format(type(x)))\n\n\nDISTRIBUTIVE_OPS = frozenset([\n    (logaddexp, add),\n    (add, mul),\n    (max, mul),\n    (min, mul),\n    (max, add),\n    (min, add),\n    (sample, add),\n])\n\n\nUNITS = {\n    mul: 1.,\n    add: 0.,\n}\n\n\nPRODUCT_INVERSES = {\n    mul: safediv,\n    add: safesub,\n}\n\n\n######################\n# Numeric Array Ops\n######################\n\n\nall = Op(np.all)\namax = Op(np.amax)\namin = Op(np.amin)\nany = Op(np.any)\nastype = Dispatcher(""ops.astype"")\ncat = Dispatcher(""ops.cat"")\nclamp = Dispatcher(""ops.clamp"")\ndiagonal = Dispatcher(""ops.diagonal"")\neinsum = Dispatcher(""ops.einsum"")\nfull_like = Op(np.full_like)\nprod = Op(np.prod)\nstack = Dispatcher(""ops.stack"")\nsum = Op(np.sum)\ntranspose = Dispatcher(""ops.transpose"")\n\narray = (np.ndarray, np.generic)\n\n\n@astype.register(array, str)\ndef _astype(x, dtype):\n    return x.astype(dtype)\n\n\n@cat.register(int, [array])\ndef _cat(dim, *x):\n    return np.concatenate(x, axis=dim)\n\n\n@clamp.register(array, object, object)\ndef _clamp(x, min, max):\n    return np.clip(x, a_min=min, a_max=max)\n\n\n@Op\ndef cholesky(x):\n    """"""\n    Like :func:`numpy.linalg.cholesky` but uses sqrt for scalar matrices.\n    """"""\n    if x.shape[-1] == 1:\n        return np.sqrt(x)\n    return np.linalg.cholesky(x)\n\n\n@Op\ndef cholesky_inverse(x):\n    """"""\n    Like :func:`torch.cholesky_inverse` but supports batching and gradients.\n    """"""\n    return cholesky_solve(new_eye(x, x.shape[:-1]), x)\n\n\n@Op\ndef cholesky_solve(x, y):\n    y_inv = np.linalg.inv(y)\n    A = np.swapaxes(y_inv, -2, -1) @ y_inv\n    return A @ x\n\n\n@Op\ndef detach(x):\n    return x\n\n\n@diagonal.register(array, int, int)\ndef _diagonal(x, dim1, dim2):\n    return np.diagonal(x, axis1=dim1, axis2=dim2)\n\n\n@einsum.register(str, [array])\ndef _einsum(x, *operand):\n    return np.einsum(x, *operand)\n\n\n@Op\ndef expand(x, shape):\n    prepend_dim = len(shape) - np.ndim(x)\n    assert prepend_dim >= 0\n    shape = shape[:prepend_dim] + tuple(dx if size == -1 else size\n                                        for dx, size in zip(np.shape(x), shape[prepend_dim:]))\n    return np.broadcast_to(x, shape)\n    return np.broadcast_to(x, shape)\n\n\n@Op\ndef finfo(x):\n    return np.finfo(x.dtype)\n\n\n@Op\ndef is_numeric_array(x):\n    return True if isinstance(x, array) else False\n\n\n@Op\ndef logsumexp(x, dim):\n    amax = np.amax(x, axis=dim, keepdims=True)\n    # treat the case x = -inf\n    amax = np.where(np.isfinite(amax), amax, 0.)\n    return log(np.sum(np.exp(x - amax), axis=dim)) + amax.squeeze(axis=dim)\n\n\n@max.register(array, array)\ndef _max(x, y):\n    return np.maximum(x, y)\n\n\n@max.register((int, float), array)\ndef _max(x, y):\n    return np.clip(y, a_min=x, a_max=None)\n\n\n@max.register(array, (int, float))\ndef _max(x, y):\n    return np.clip(x, a_min=y, a_max=None)\n\n\n@min.register(array, array)\ndef _min(x, y):\n    return np.minimum(x, y)\n\n\n@min.register((int, float), array)\ndef _min(x, y):\n    return np.clip(y, a_min=None, a_max=x)\n\n\n@min.register(array, (int, float))\ndef _min(x, y):\n    return np.clip(x, a_min=None, a_max=y)\n\n\n@Op\ndef new_arange(x, stop):\n    return np.arange(stop)\n\n\n@new_arange.register(array, int, int, int)\ndef _new_arange(x, start, stop, step):\n    return np.arange(start, stop, step)\n\n\n@Op\ndef new_zeros(x, shape):\n    return np.zeros(shape, dtype=x.dtype)\n\n\n@Op\ndef new_eye(x, shape):\n    n = shape[-1]\n    return np.broadcast_to(np.eye(n), shape + (n,))\n\n\n@Op\ndef permute(x, dims):\n    return np.transpose(x, axes=dims)\n\n\n@reciprocal.register(array)\ndef _reciprocal(x):\n    result = np.clip(np.reciprocal(x), a_max=np.finfo(x.dtype).max)\n    return result\n\n\n@safediv.register(object, array)\ndef _safediv(x, y):\n    try:\n        finfo = np.finfo(y.dtype)\n    except ValueError:\n        finfo = np.iinfo(y.dtype)\n    return x * np.clip(np.reciprocal(y), a_min=None, a_max=finfo.max)\n\n\n@safesub.register(object, array)\ndef _safesub(x, y):\n    try:\n        finfo = np.finfo(y.dtype)\n    except ValueError:\n        finfo = np.iinfo(y.dtype)\n    return x + np.clip(-y, a_min=None, a_max=finfo.max)\n\n\n@stack.register(int, [array])\ndef _stack(dim, *x):\n    return np.stack(x, axis=dim)\n\n\n@transpose.register(array, int, int)\ndef _transpose(x, dim1, dim2):\n    return np.swapaxes(x, dim1, dim2)\n\n\n@Op\ndef triangular_solve(x, y, upper=False, transpose=False):\n    if transpose:\n        y = np.swapaxes(y, -2, -1)\n    return np.linalg.inv(y) @ x\n\n\n@Op\ndef unsqueeze(x, dim):\n    return np.expand_dims(x, axis=dim)\n\n\n__all__ = [\n    \'AddOp\',\n    \'AssociativeOp\',\n    \'DISTRIBUTIVE_OPS\',\n    \'ExpOp\',\n    \'GetitemOp\',\n    \'LogAddExpOp\',\n    \'LogOp\',\n    \'NegOp\',\n    \'Op\',\n    \'PRODUCT_INVERSES\',\n    \'ReciprocalOp\',\n    \'SampleOp\',\n    \'SubOp\',\n    \'ReshapeOp\',\n    \'UNITS\',\n    \'abs\',\n    \'add\',\n    \'all\',\n    \'amax\',\n    \'amin\',\n    \'and_\',\n    \'any\',\n    \'astype\',\n    \'cat\',\n    \'cholesky\',\n    \'cholesky_inverse\',\n    \'cholesky_solve\',\n    \'clamp\',\n    \'detach\',\n    \'diagonal\',\n    \'einsum\',\n    \'eq\',\n    \'exp\',\n    \'expand\',\n    \'finfo\',\n    \'full_like\',\n    \'ge\',\n    \'getitem\',\n    \'gt\',\n    \'invert\',\n    \'is_numeric_array\',\n    \'le\',\n    \'log\',\n    \'log1p\',\n    \'logaddexp\',\n    \'logsumexp\',\n    \'lt\',\n    \'matmul\',\n    \'max\',\n    \'min\',\n    \'mul\',\n    \'ne\',\n    \'neg\',\n    \'new_arange\',\n    \'new_eye\',\n    \'new_zeros\',\n    \'or_\',\n    \'pow\',\n    \'prod\',\n    \'reciprocal\',\n    \'safediv\',\n    \'safesub\',\n    \'sample\',\n    \'sigmoid\',\n    \'sqrt\',\n    \'stack\',\n    \'sub\',\n    \'sum\',\n    \'transpose\',\n    \'triangular_solve\',\n    \'truediv\',\n    \'unsqueeze\',\n    \'xor\',\n]\n\n__doc__ = """"""\nBuilt-in operations\n-------------------\n\n{}\n\nOperation classes\n-----------------\n"""""".format(""\\n"".join(f"".. autodata:: {_name}\\n""\n                     for _name in __all__ if isinstance(globals()[_name], Op)))\n'"
funsor/optimizer.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport collections\n\nfrom multipledispatch.variadic import Variadic\nfrom opt_einsum.paths import greedy\n\nimport funsor.interpreter as interpreter\nfrom funsor.cnf import Contraction, nullop\nfrom funsor.ops import DISTRIBUTIVE_OPS, AssociativeOp\nfrom funsor.terms import Funsor, eager, lazy, normalize\n\n\n@interpreter.dispatched_interpretation\ndef unfold(cls, *args):\n    result = unfold.dispatch(cls, *args)(*args)\n    if result is None:\n        result = normalize.dispatch(cls, *args)(*args)\n    if result is None:\n        result = lazy(cls, *args)\n    return result\n\n\n@unfold.register(Contraction, AssociativeOp, AssociativeOp, frozenset, tuple)\ndef unfold_contraction_generic_tuple(red_op, bin_op, reduced_vars, terms):\n\n    for i, v in enumerate(terms):\n\n        if not isinstance(v, Contraction):\n            continue\n\n        if v.red_op is nullop and (v.bin_op, bin_op) in DISTRIBUTIVE_OPS:\n            # a * e * (b + c + d) -> (a * e * b) + (a * e * c) + (a * e * d)\n            new_terms = tuple(\n                Contraction(v.red_op, bin_op, v.reduced_vars, *(terms[:i] + (vt,) + terms[i+1:]))\n                for vt in v.terms)\n            return Contraction(red_op, v.bin_op, reduced_vars, *new_terms)\n\n        if red_op in (v.red_op, nullop) and (v.red_op, bin_op) in DISTRIBUTIVE_OPS:\n            new_terms = terms[:i] + (Contraction(v.red_op, v.bin_op, frozenset(), *v.terms),) + terms[i+1:]\n            return Contraction(v.red_op, bin_op, v.reduced_vars, *new_terms).reduce(red_op, reduced_vars)\n\n        if v.red_op in (red_op, nullop) and bin_op in (v.bin_op, nullop):\n            red_op = v.red_op if red_op is nullop else red_op\n            bin_op = v.bin_op if bin_op is nullop else bin_op\n            new_terms = terms[:i] + v.terms + terms[i+1:]\n            return Contraction(red_op, bin_op, reduced_vars | v.reduced_vars, *new_terms)\n\n    return None\n\n\nunfold.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Variadic[Funsor])(\n    lambda r, b, v, *ts: unfold(Contraction, r, b, v, tuple(ts)))\n\n\n@interpreter.dispatched_interpretation\ndef optimize(cls, *args):\n    result = optimize.dispatch(cls, *args)(*args)\n    if result is None:\n        result = eager(cls, *args)\n    return result\n\n\n# TODO set a better value for this\nREAL_SIZE = 3  # the ""size"" of a real-valued dimension passed to the path optimizer\n\n\noptimize.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Variadic[Funsor])(\n    lambda r, b, v, *ts: optimize(Contraction, r, b, v, tuple(ts)))\n\n\n@optimize.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Funsor, Funsor)\n@optimize.register(Contraction, AssociativeOp, AssociativeOp, frozenset, Funsor)\ndef eager_contract_base(red_op, bin_op, reduced_vars, *terms):\n    return None\n\n\n@optimize.register(Contraction, AssociativeOp, AssociativeOp, frozenset, tuple)\ndef optimize_contract_finitary_funsor(red_op, bin_op, reduced_vars, terms):\n\n    if red_op is nullop or bin_op is nullop or not (red_op, bin_op) in DISTRIBUTIVE_OPS:\n        return None\n\n    # build opt_einsum optimizer IR\n    inputs = [frozenset(term.inputs) for term in terms]\n    size_dict = {k: ((REAL_SIZE * v.num_elements) if v.dtype == \'real\' else v.dtype)\n                 for term in terms for k, v in term.inputs.items()}\n    outputs = frozenset().union(*inputs) - reduced_vars\n\n    # optimize path with greedy opt_einsum optimizer\n    # TODO switch to new \'auto\' strategy\n    path = greedy(inputs, outputs, size_dict)\n\n    # first prepare a reduce_dim counter to avoid early reduction\n    reduce_dim_counter = collections.Counter()\n    for input in inputs:\n        reduce_dim_counter.update({d: 1 for d in input})\n\n    operands = list(terms)\n    for (a, b) in path:\n        b, a = tuple(sorted((a, b), reverse=True))\n        tb = operands.pop(b)\n        ta = operands.pop(a)\n\n        # don\'t reduce a dimension too early - keep a collections.Counter\n        # and only reduce when the dimension is removed from all lhs terms in path\n        reduce_dim_counter.subtract({d: 1 for d in reduced_vars & frozenset(ta.inputs.keys())})\n        reduce_dim_counter.subtract({d: 1 for d in reduced_vars & frozenset(tb.inputs.keys())})\n\n        # reduce variables that don\'t appear in other terms\n        both_vars = frozenset(ta.inputs.keys()) | frozenset(tb.inputs.keys())\n        path_end_reduced_vars = frozenset(d for d in reduced_vars & both_vars\n                                          if reduce_dim_counter[d] == 0)\n\n        # count new appearance of variables that aren\'t reduced\n        reduce_dim_counter.update({d: 1 for d in reduced_vars & (both_vars - path_end_reduced_vars)})\n\n        path_end = Contraction(red_op if path_end_reduced_vars else nullop, bin_op, path_end_reduced_vars, ta, tb)\n        operands.append(path_end)\n\n    # reduce any remaining dims, if necessary\n    final_reduced_vars = frozenset(d for (d, count) in reduce_dim_counter.items()\n                                   if count > 0) & reduced_vars\n    if final_reduced_vars:\n        path_end = path_end.reduce(red_op, final_reduced_vars)\n    return path_end\n\n\ndef apply_optimizer(x):\n\n    @interpreter.interpretation(interpreter._INTERPRETATION)\n    def nested_optimize_interpreter(cls, *args):\n        result = optimize.dispatch(cls, *args)(*args)\n        if result is None:\n            result = cls(*args)\n        return result\n\n    with interpreter.interpretation(unfold):\n        expr = interpreter.reinterpret(x)\n\n    with interpreter.interpretation(nested_optimize_interpreter):\n        return interpreter.reinterpret(expr)\n'"
funsor/registry.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import defaultdict\n\nfrom multipledispatch import Dispatcher\n\n\nclass PartialDispatcher(Dispatcher):\n    """"""\n    Wrapper to avoid appearance in stack traces.\n    """"""\n    def partial_call(self, *args):\n        """"""\n        Likde :meth:`__call__` but avoids calling ``func()``.\n        """"""\n        types = tuple(map(type, args))\n        try:\n            func = self._cache[types]\n        except KeyError:\n            func = self.dispatch(*types)\n            if func is None:\n                raise NotImplementedError(\n                    \'Could not find signature for %s: <%s>\' %\n                    (self.name, \', \'.join(cls.__name__ for cls in types)))\n            self._cache[types] = func\n        return func\n\n\nclass PartialDefault:\n    def __init__(self, default):\n        self.default = default\n\n    @property\n    def __call__(self):\n        return self.default\n\n    def partial_call(self, *args):\n        return self.default\n\n\nclass KeyedRegistry(object):\n\n    def __init__(self, default=None):\n        self.default = default if default is None else PartialDefault(default)\n        self.registry = defaultdict(lambda: PartialDispatcher(\'f\'))\n\n    def register(self, key, *types):\n        key = getattr(key, ""__origin__"", key)\n        register = self.registry[key].register\n        if self.default:\n            objects = (object,) * len(types)\n            if objects != types:\n                register(*objects)(self.default)\n\n        # This decorator supports stacking multiple decorators, which is not\n        # supported by multipledipatch (which returns a Dispatch object rather\n        # than the original function).\n        def decorator(fn):\n            register(*types)(fn)\n            return fn\n\n        return decorator\n\n    def __contains__(self, key):\n        return key in self.registry\n\n    def __getitem__(self, key):\n        key = getattr(key, ""__origin__"", key)\n        if self.default is None:\n            return self.registry[key]\n        return self.registry.get(key, self.default)\n\n    def __call__(self, key, *args):\n        return self[key](*args)\n\n    def dispatch(self, key, *args):\n        return self[key].partial_call(*args)\n\n\n__all__ = [\n    \'KeyedRegistry\',\n]\n'"
funsor/sum_product.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport re\nfrom collections import OrderedDict, defaultdict\nfrom functools import reduce\n\nimport numpy as np\n\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction\nfrom funsor.domains import bint\nfrom funsor.ops import UNITS, AssociativeOp\nfrom funsor.terms import Cat, Funsor, FunsorMeta, Number, Slice, Stack, Subs, Variable, eager, substitute, to_funsor\nfrom funsor.util import quote\n\n\ndef _partition(terms, sum_vars):\n    # Construct a bipartite graph between terms and the vars\n    neighbors = OrderedDict([(t, []) for t in terms])\n    for term in terms:\n        for dim in term.inputs.keys():\n            if dim in sum_vars:\n                neighbors[term].append(dim)\n                neighbors.setdefault(dim, []).append(term)\n\n    # Partition the bipartite graph into connected components for contraction.\n    components = []\n    while neighbors:\n        v, pending = neighbors.popitem()\n        component = OrderedDict([(v, None)])  # used as an OrderedSet\n        for v in pending:\n            component[v] = None\n        while pending:\n            v = pending.pop()\n            for v in neighbors.pop(v):\n                if v not in component:\n                    component[v] = None\n                    pending.append(v)\n\n        # Split this connected component into tensors and dims.\n        component_terms = tuple(v for v in component if isinstance(v, Funsor))\n        if component_terms:\n            component_dims = frozenset(v for v in component if not isinstance(v, Funsor))\n            components.append((component_terms, component_dims))\n    return components\n\n\ndef partial_sum_product(sum_op, prod_op, factors, eliminate=frozenset(), plates=frozenset()):\n    """"""\n    Performs partial sum-product contraction of a collection of factors.\n\n    :return: a list of partially contracted Funsors.\n    :rtype: list\n    """"""\n    assert callable(sum_op)\n    assert callable(prod_op)\n    assert isinstance(factors, (tuple, list))\n    assert all(isinstance(f, Funsor) for f in factors)\n    assert isinstance(eliminate, frozenset)\n    assert isinstance(plates, frozenset)\n    sum_vars = eliminate - plates\n\n    var_to_ordinal = {}\n    ordinal_to_factors = defaultdict(list)\n    for f in factors:\n        ordinal = plates.intersection(f.inputs)\n        ordinal_to_factors[ordinal].append(f)\n        for var in sum_vars.intersection(f.inputs):\n            var_to_ordinal[var] = var_to_ordinal.get(var, ordinal) & ordinal\n\n    ordinal_to_vars = defaultdict(set)\n    for var, ordinal in var_to_ordinal.items():\n        ordinal_to_vars[ordinal].add(var)\n\n    results = []\n    while ordinal_to_factors:\n        leaf = max(ordinal_to_factors, key=len)\n        leaf_factors = ordinal_to_factors.pop(leaf)\n        leaf_reduce_vars = ordinal_to_vars[leaf]\n        for (group_factors, group_vars) in _partition(leaf_factors, leaf_reduce_vars):\n            f = reduce(prod_op, group_factors).reduce(sum_op, group_vars)\n            remaining_sum_vars = sum_vars.intersection(f.inputs)\n            if not remaining_sum_vars:\n                results.append(f.reduce(prod_op, leaf & eliminate))\n            else:\n                new_plates = frozenset().union(\n                    *(var_to_ordinal[v] for v in remaining_sum_vars))\n                if new_plates == leaf:\n                    raise ValueError(""intractable!"")\n                f = f.reduce(prod_op, leaf - new_plates)\n                ordinal_to_factors[new_plates].append(f)\n\n    return results\n\n\ndef sum_product(sum_op, prod_op, factors, eliminate=frozenset(), plates=frozenset()):\n    """"""\n    Performs sum-product contraction of a collection of factors.\n\n    :return: a single contracted Funsor.\n    :rtype: :class:`~funsor.terms.Funsor`\n    """"""\n    factors = partial_sum_product(sum_op, prod_op, factors, eliminate, plates)\n    return reduce(prod_op, factors, Number(UNITS[prod_op]))\n\n\ndef naive_sequential_sum_product(sum_op, prod_op, trans, time, step):\n    assert isinstance(sum_op, AssociativeOp)\n    assert isinstance(prod_op, AssociativeOp)\n    assert isinstance(trans, Funsor)\n    assert isinstance(time, Variable)\n    assert isinstance(step, dict)\n    assert all(isinstance(k, str) for k in step.keys())\n    assert all(isinstance(v, str) for v in step.values())\n    if time.name in trans.inputs:\n        assert time.output == trans.inputs[time.name]\n\n    step = OrderedDict(sorted(step.items()))\n    drop = tuple(""_drop_{}"".format(i) for i in range(len(step)))\n    prev_to_drop = dict(zip(step.keys(), drop))\n    curr_to_drop = dict(zip(step.values(), drop))\n    drop = frozenset(drop)\n\n    time, duration = time.name, time.output.size\n    factors = [trans(**{time: t}) for t in range(duration)]\n    while len(factors) > 1:\n        y = factors.pop()(**prev_to_drop)\n        x = factors.pop()(**curr_to_drop)\n        xy = prod_op(x, y).reduce(sum_op, drop)\n        factors.append(xy)\n    return factors[0]\n\n\ndef sequential_sum_product(sum_op, prod_op, trans, time, step):\n    """"""\n    For a funsor ``trans`` with dimensions ``time``, ``prev`` and ``curr``,\n    computes a recursion equivalent to::\n\n        tail_time = 1 + arange(""time"", trans.inputs[""time""].size - 1)\n        tail = sequential_sum_product(sum_op, prod_op,\n                                      trans(time=tail_time),\n                                      time, {""prev"": ""curr""})\n        return prod_op(trans(time=0)(curr=""drop""), tail(prev=""drop"")) \\\n           .reduce(sum_op, ""drop"")\n\n    but does so efficiently in parallel in O(log(time)).\n\n    :param ~funsor.ops.AssociativeOp sum_op: A semiring sum operation.\n    :param ~funsor.ops.AssociativeOp prod_op: A semiring product operation.\n    :param ~funsor.terms.Funsor trans: A transition funsor.\n    :param Variable time: The time input dimension.\n    :param dict step: A dict mapping previous variables to current variables.\n        This can contain multiple pairs of prev->curr variable names.\n    """"""\n    assert isinstance(sum_op, AssociativeOp)\n    assert isinstance(prod_op, AssociativeOp)\n    assert isinstance(trans, Funsor)\n    assert isinstance(time, Variable)\n    assert isinstance(step, dict)\n    assert all(isinstance(k, str) for k in step.keys())\n    assert all(isinstance(v, str) for v in step.values())\n    if time.name in trans.inputs:\n        assert time.output == trans.inputs[time.name]\n\n    step = OrderedDict(sorted(step.items()))\n    drop = tuple(""_drop_{}"".format(i) for i in range(len(step)))\n    prev_to_drop = dict(zip(step.keys(), drop))\n    curr_to_drop = dict(zip(step.values(), drop))\n    drop = frozenset(drop)\n\n    time, duration = time.name, time.output.size\n    while duration > 1:\n        even_duration = duration // 2 * 2\n        x = trans(**{time: Slice(time, 0, even_duration, 2, duration)}, **curr_to_drop)\n        y = trans(**{time: Slice(time, 1, even_duration, 2, duration)}, **prev_to_drop)\n        contracted = Contraction(sum_op, prod_op, drop, x, y)\n\n        if duration > even_duration:\n            extra = trans(**{time: Slice(time, duration - 1, duration)})\n            contracted = Cat(time, (contracted, extra))\n        trans = contracted\n        duration = (duration + 1) // 2\n    return trans(**{time: 0})\n\n\ndef mixed_sequential_sum_product(sum_op, prod_op, trans, time, step, num_segments=None):\n    """"""\n    For a funsor ``trans`` with dimensions ``time``, ``prev`` and ``curr``,\n    computes a recursion equivalent to::\n\n        tail_time = 1 + arange(""time"", trans.inputs[""time""].size - 1)\n        tail = sequential_sum_product(sum_op, prod_op,\n                                      trans(time=tail_time),\n                                      time, {""prev"": ""curr""})\n        return prod_op(trans(time=0)(curr=""drop""), tail(prev=""drop"")) \\\n           .reduce(sum_op, ""drop"")\n\n    by mixing parallel and serial scan algorithms over ``num_segments`` segments.\n\n    :param ~funsor.ops.AssociativeOp sum_op: A semiring sum operation.\n    :param ~funsor.ops.AssociativeOp prod_op: A semiring product operation.\n    :param ~funsor.terms.Funsor trans: A transition funsor.\n    :param Variable time: The time input dimension.\n    :param dict step: A dict mapping previous variables to current variables.\n        This can contain multiple pairs of prev->curr variable names.\n    :param int num_segments: number of segments for the first stage\n    """"""\n    time_var, time, duration = time, time.name, time.output.size\n    num_segments = duration if num_segments is None else num_segments\n    assert num_segments > 0 and duration > 0\n\n    # handle unevenly sized segments by chopping off the final segment and calling mixed_sequential_sum_product again\n    if duration % num_segments and duration - duration % num_segments > 0:\n        remainder = trans(**{time: Slice(time, duration - duration % num_segments, duration, 1, duration)})\n        initial = trans(**{time: Slice(time, 0, duration - duration % num_segments, 1, duration)})\n        initial_eliminated = mixed_sequential_sum_product(\n            sum_op, prod_op, initial, Variable(time, bint(duration - duration % num_segments)), step,\n            num_segments=num_segments)\n        final = Cat(time, (Stack(time, (initial_eliminated,)), remainder))\n        final_eliminated = naive_sequential_sum_product(\n            sum_op, prod_op, final, Variable(time, bint(1 + duration % num_segments)), step)\n        return final_eliminated\n\n    # handle degenerate cases that reduce to a single stage\n    if num_segments == 1:\n        return naive_sequential_sum_product(sum_op, prod_op, trans, time_var, step)\n    if num_segments >= duration:\n        return sequential_sum_product(sum_op, prod_op, trans, time_var, step)\n\n    # break trans into num_segments segments of equal length\n    segment_length = duration // num_segments\n    segments = [trans(**{time: Slice(time, i * segment_length, (i + 1) * segment_length, 1, duration)})\n                for i in range(num_segments)]\n\n    first_stage_result = naive_sequential_sum_product(\n        sum_op, prod_op, Stack(time + ""__SEGMENTED"", tuple(segments)),\n        Variable(time, bint(segment_length)), step)\n\n    second_stage_result = sequential_sum_product(\n        sum_op, prod_op, first_stage_result,\n        Variable(time + ""__SEGMENTED"", bint(num_segments)), step)\n\n    return second_stage_result\n\n\ndef naive_sarkka_bilmes_product(sum_op, prod_op, trans, time_var, global_vars=frozenset()):\n\n    assert isinstance(global_vars, frozenset)\n\n    time = time_var.name\n\n    def get_shift(name):\n        return len(re.search(""^P*"", name).group(0))\n\n    def shift_name(name, t):\n        return t * ""P"" + name\n\n    def shift_funsor(f, t):\n        if t == 0:\n            return f\n        return f(**{name: shift_name(name, t) for name in f.inputs\n                    if name != time and name not in global_vars})\n\n    lags = {get_shift(name) for name in trans.inputs if name != time}\n    lags.discard(0)\n    if not lags:\n        return naive_sequential_sum_product(sum_op, prod_op, trans, time_var, {})\n\n    period = int(np.lcm.reduce(list(lags)))\n\n    duration = trans.inputs[time].size\n    if duration % period:\n        raise NotImplementedError(""TODO handle partial windows"")\n\n    result = trans(**{time: duration - 1})\n    original_names = frozenset(name for name in trans.inputs\n                               if name != time and name not in global_vars\n                               and not name.startswith(""P""))\n    for t in range(trans.inputs[time].size - 2, -1, -1):\n        result = prod_op(shift_funsor(trans(**{time: t}), duration - t - 1), result)\n        sum_vars = frozenset(shift_name(name, duration - t - 1) for name in original_names)\n        result = result.reduce(sum_op, sum_vars)\n\n    result = result(**{name: name.replace(""P"" * duration, ""P"") for name in result.inputs})\n    return result\n\n\ndef sarkka_bilmes_product(sum_op, prod_op, trans, time_var, global_vars=frozenset(), num_periods=1):\n\n    assert isinstance(global_vars, frozenset)\n\n    time = time_var.name\n\n    def get_shift(name):\n        return len(re.search(""^P*"", name).group(0))\n\n    def shift_name(name, t):\n        return t * ""P"" + name\n\n    def shift_funsor(f, t):\n        if t == 0:\n            return f\n        return f(**{name: shift_name(name, t) for name in f.inputs\n                    if name != time and name not in global_vars})\n\n    lags = {get_shift(name) for name in trans.inputs if name != time}\n    lags.discard(0)\n    if not lags:\n        return sequential_sum_product(sum_op, prod_op, trans, time_var, {})\n\n    period = int(np.lcm.reduce(list(lags)))\n    original_names = frozenset(name for name in trans.inputs\n                               if name != time and name not in global_vars\n                               and not name.startswith(""P""))\n    renamed_factors = []\n    duration = trans.inputs[time].size\n    if duration % period:\n        raise NotImplementedError(""TODO handle partial windows"")\n\n    for t in range(period):\n        slice_t = Slice(time, t, duration - period + t + 1, period, duration)\n        factor = shift_funsor(trans, period - t - 1)\n        factor = factor(**{time: slice_t})\n        renamed_factors.append(factor)\n\n    block_trans = reduce(prod_op, renamed_factors)\n    block_step = {shift_name(name, period): name for name in block_trans.inputs\n                  if name != time and name not in global_vars and get_shift(name) < period}\n    block_time_var = Variable(time_var.name, bint(duration // period))\n    final_chunk = mixed_sequential_sum_product(\n        sum_op, prod_op, block_trans, block_time_var, block_step,\n        num_segments=max(1, duration // (period * num_periods)))\n    final_sum_vars = frozenset(\n        shift_name(name, t) for name in original_names for t in range(1, period))\n    result = final_chunk.reduce(sum_op, final_sum_vars)\n    result = result(**{name: name.replace(""P"" * period, ""P"") for name in result.inputs})\n    return result\n\n\nclass MarkovProductMeta(FunsorMeta):\n    """"""\n    Wrapper to convert ``step`` to a tuple and fill in default ``step_names``.\n    """"""\n    def __call__(cls, sum_op, prod_op, trans, time, step, step_names=None):\n        if isinstance(time, str):\n            assert time in trans.inputs, ""please pass Variable(time, ...)""\n            time = Variable(time, trans.inputs[time])\n        if isinstance(step, dict):\n            step = frozenset(step.items())\n        if step_names is None:\n            step_names = frozenset((k, k) for pair in step for k in pair)\n        if isinstance(step_names, dict):\n            step_names = frozenset(step_names.items())\n        return super().__call__(sum_op, prod_op, trans, time, step, step_names)\n\n\nclass MarkovProduct(Funsor, metaclass=MarkovProductMeta):\n    """"""\n    Lazy representation of :func:`sequential_sum_product` .\n\n    :param AssociativeOp sum_op: A marginalization op.\n    :param AssociativeOp prod_op: A Bayesian fusion op.\n    :param Funsor trans: A sequence of transition factors,\n        usually varying along the ``time`` input.\n    :param time: A time dimension.\n    :type time: str or Variable\n    :param dict step: A str-to-str mapping of ""previous"" inputs of ``trans``\n        to ""current"" inputs of ``trans``.\n    :param dict step_names: Optional, for internal use by alpha conversion.\n    """"""\n    def __init__(self, sum_op, prod_op, trans, time, step, step_names):\n        assert isinstance(sum_op, AssociativeOp)\n        assert isinstance(prod_op, AssociativeOp)\n        assert isinstance(trans, Funsor)\n        assert isinstance(time, Variable)\n        assert isinstance(step, frozenset)\n        assert isinstance(step_names, frozenset)\n        step = dict(step)\n        step_names = dict(step_names)\n        assert all(isinstance(k, str) for k in step_names.keys())\n        assert all(isinstance(v, str) for v in step_names.values())\n        assert set(step_names) == set(step).union(step.values())\n        inputs = OrderedDict((step_names.get(k, k), v)\n                             for k, v in trans.inputs.items()\n                             if k != time.name)\n        output = trans.output\n        fresh = frozenset(step_names.values())\n        bound = frozenset(step_names.keys()) | {time.name}\n        super().__init__(inputs, output, fresh, bound)\n        self.sum_op = sum_op\n        self.prod_op = prod_op\n        self.trans = trans\n        self.time = time\n        self.step = step\n        self.step_names = step_names\n\n    def _alpha_convert(self, alpha_subs):\n        assert self.bound.issuperset(alpha_subs)\n        time = Variable(alpha_subs.get(self.time.name, self.time.name),\n                        self.time.output)\n        step = frozenset((alpha_subs.get(k, k), alpha_subs.get(v, v))\n                         for k, v in self.step.items())\n        step_names = frozenset((alpha_subs.get(k, k), v)\n                               for k, v in self.step_names.items())\n        alpha_subs = {k: to_funsor(v, self.trans.inputs[k])\n                      for k, v in alpha_subs.items()\n                      if k in self.trans.inputs}\n        trans = substitute(self.trans, alpha_subs)\n        return self.sum_op, self.prod_op, trans, time, step, step_names\n\n    def eager_subs(self, subs):\n        assert isinstance(subs, tuple)\n        # Eagerly rename variables.\n        rename = {k: v.name for k, v in subs if isinstance(v, Variable)}\n        if not rename:\n            return None\n        step_names = frozenset((k, rename.get(v, v))\n                               for k, v in self.step_names.items())\n        result = MarkovProduct(self.sum_op, self.prod_op,\n                               self.trans, self.time, self.step, step_names)\n        lazy = tuple((k, v) for k, v in subs if not isinstance(v, Variable))\n        if lazy:\n            result = Subs(result, lazy)\n        return result\n\n\n@quote.register(MarkovProduct)\ndef _(arg, indent, out):\n    line = f""{type(arg).__name__}({repr(arg.sum_op)}, {repr(arg.prod_op)},""\n    out.append((indent, line))\n    for value in arg._ast_values[2:]:\n        quote.inplace(value, indent + 1, out)\n        i, line = out[-1]\n        out[-1] = i, line + "",""\n    i, line = out[-1]\n    out[-1] = i, line[:-1] + "")""\n\n\n@eager.register(MarkovProduct, AssociativeOp, AssociativeOp,\n                Funsor, Variable, frozenset, frozenset)\ndef eager_markov_product(sum_op, prod_op, trans, time, step, step_names):\n    if step:\n        result = sequential_sum_product(sum_op, prod_op, trans, time, dict(step))\n    elif time.name in trans.inputs:\n        result = trans.reduce(prod_op, time.name)\n    elif prod_op is ops.add:\n        result = trans * time.size\n    elif prod_op is ops.mul:\n        result = trans ** time.size\n    else:\n        raise NotImplementedError(\'https://github.com/pyro-ppl/funsor/issues/233\')\n\n    return Subs(result, step_names)\n'"
funsor/tensor.py,20,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport itertools\nimport warnings\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\nfrom functools import reduce\n\nimport numpy as np\nimport opt_einsum\nfrom multipledispatch import dispatch\nfrom multipledispatch.variadic import Variadic\n\nimport funsor\nimport funsor.ops as ops\nfrom funsor.delta import Delta\nfrom funsor.domains import Domain, bint, find_domain, reals\nfrom funsor.ops import GetitemOp, MatmulOp, Op, ReshapeOp\nfrom funsor.terms import (\n    Binary,\n    Funsor,\n    FunsorMeta,\n    Lambda,\n    Number,\n    Slice,\n    Unary,\n    Variable,\n    eager,\n    substitute,\n    to_data,\n    to_funsor\n)\nfrom funsor.util import getargspec, get_backend, get_tracing_state, is_nn_module, quote\n\n\ndef get_default_prototype():\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        return torch.tensor([])\n    else:\n        return np.array([])\n\n\ndef numeric_array(x, dtype=None, device=None):\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        return torch.tensor(x, dtype=dtype, device=device)\n    else:\n        return np.array(x, dtype=dtype)\n\n\ndef dummy_numeric_array(domain):\n    value = 0.1 if domain.dtype == \'real\' else 1\n    return ops.expand(numeric_array(value), domain.shape) if domain.shape else value\n\n\ndef _nameof(fn):\n    return getattr(fn, \'__name__\', type(fn).__name__)\n\n\n@contextmanager\ndef ignore_jit_warnings():\n    with warnings.catch_warnings():\n        if get_backend() == ""torch"":\n            import torch\n\n            warnings.filterwarnings(""ignore"", category=torch.jit.TracerWarning)\n        yield\n\n\nclass TensorMeta(FunsorMeta):\n    """"""\n    Wrapper to fill in default args and convert between OrderedDict and tuple.\n    """"""\n    def __call__(cls, data, inputs=None, dtype=""real""):\n        if inputs is None:\n            inputs = tuple()\n        elif isinstance(inputs, OrderedDict):\n            inputs = tuple(inputs.items())\n        # XXX: memoize tests fail for np.generic because those scalar values are hashable?\n        # it seems that there is no harm with the conversion generic -> ndarray here\n        if isinstance(data, np.generic):\n            data = data.__array__()\n        return super(TensorMeta, cls).__call__(data, inputs, dtype)\n\n\nclass Tensor(Funsor, metaclass=TensorMeta):\n    """"""\n    Funsor backed by a PyTorch Tensor or a NumPy ndarray.\n\n    This follows the :mod:`torch.distributions` convention of arranging\n    named ""batch"" dimensions on the left and remaining ""event"" dimensions\n    on the right. The output shape is determined by all remaining dims.\n    For example::\n\n        data = torch.zeros(5,4,3,2)\n        x = Tensor(data, OrderedDict([(""i"", bint(5)), (""j"", bint(4))]))\n        assert x.output == reals(3, 2)\n\n    Operators like ``matmul`` and ``.sum()`` operate only on the output shape,\n    and will not change the named inputs.\n\n   :param numeric_array data: A PyTorch tensor or NumPy ndarray.\n    :param OrderedDict inputs: An optional mapping from input name (str) to\n        datatype (:class:`~funsor.domains.Domain` ). Defaults to empty.\n    :param dtype: optional output datatype. Defaults to ""real"".\n    :type dtype: int or the string ""real"".\n    """"""\n    def __init__(self, data, inputs=None, dtype=""real""):\n        assert ops.is_numeric_array(data)\n        assert isinstance(inputs, tuple)\n        if not get_tracing_state():\n            assert len(inputs) <= len(data.shape)\n            for (k, d), size in zip(inputs, data.shape):\n                assert d.dtype == size\n        inputs = OrderedDict(inputs)\n        output = Domain(data.shape[len(inputs):], dtype)\n        fresh = frozenset(inputs.keys())\n        bound = frozenset()\n        super(Tensor, self).__init__(inputs, output, fresh, bound)\n        self.data = data\n\n    def __repr__(self):\n        if self.output != ""real"":\n            return \'Tensor({}, {}, {})\'.format(self.data, self.inputs, repr(self.dtype))\n        elif self.inputs:\n            return \'Tensor({}, {})\'.format(self.data, self.inputs)\n        else:\n            return \'Tensor({})\'.format(self.data)\n\n    def __str__(self):\n        if self.dtype != ""real"":\n            return \'Tensor({}, {}, {})\'.format(self.data, self.inputs, repr(self.dtype))\n        elif self.inputs:\n            return \'Tensor({}, {})\'.format(self.data, self.inputs)\n        else:\n            return str(self.data)\n\n    def __int__(self):\n        return int(self.data)\n\n    def __float__(self):\n        return float(self.data)\n\n    def __bool__(self):\n        return bool(self.data)\n\n    def item(self):\n        return self.data.item()\n\n    def clamp_finite(self):\n        finfo = ops.finfo(self.data)\n        data = ops.clamp(self.data, finfo.min, finfo.max)\n        return Tensor(data, self.inputs, self.dtype)\n\n    @property\n    def requires_grad(self):\n        # NB: numpy does not have attribute requires_grad\n        return getattr(self.data, ""requires_grad"", None)\n\n    def align(self, names):\n        assert isinstance(names, tuple)\n        assert all(name in self.inputs for name in names)\n        if not names or names == tuple(self.inputs):\n            return self\n\n        inputs = OrderedDict((name, self.inputs[name]) for name in names)\n        inputs.update(self.inputs)\n        old_dims = tuple(self.inputs)\n        new_dims = tuple(inputs)\n        permutation = tuple(old_dims.index(d) for d in new_dims)\n        permutation = permutation + tuple(range(len(permutation), len(permutation) + len(self.output.shape)))\n        data = ops.permute(self.data, permutation)\n        return Tensor(data, inputs, self.dtype)\n\n    def eager_subs(self, subs):\n        assert isinstance(subs, tuple)\n        subs = OrderedDict((k, to_funsor(v, self.inputs[k]))\n                           for k, v in subs if k in self.inputs)\n        if not subs:\n            return self\n\n        # Handle renaming to enable cons hashing, and\n        # handle slicing to avoid copying data.\n        if any(isinstance(v, (Variable, Slice)) for v in subs.values()):\n            slices = None\n            inputs = OrderedDict()\n            for i, (k, d) in enumerate(self.inputs.items()):\n                if k in subs:\n                    v = subs[k]\n                    if isinstance(v, Variable):\n                        del subs[k]\n                        k = v.name\n                    elif isinstance(v, Slice):\n                        del subs[k]\n                        k = v.name\n                        d = v.inputs[v.name]\n                        if slices is None:\n                            slices = [slice(None)] * len(self.data.shape)\n                        slices[i] = v.slice\n                inputs[k] = d\n            data = self.data[tuple(slices)] if slices else self.data\n            result = Tensor(data, inputs, self.dtype)\n            return result.eager_subs(tuple(subs.items()))\n\n        # materialize after checking for renaming case\n        subs = OrderedDict((k, self.materialize(v)) for k, v in subs.items())\n\n        # Compute result shapes.\n        inputs = OrderedDict()\n        for k, domain in self.inputs.items():\n            if k in subs:\n                inputs.update(subs[k].inputs)\n            else:\n                inputs[k] = domain\n\n        # Construct a dict with each input\'s positional dim,\n        # counting from the right so as to support broadcasting.\n        total_size = len(inputs) + len(self.output.shape)  # Assumes only scalar indices.\n        new_dims = {}\n        for k, domain in inputs.items():\n            assert not domain.shape\n            new_dims[k] = len(new_dims) - total_size\n\n        # Use advanced indexing to construct a simultaneous substitution.\n        index = []\n        for k, domain in self.inputs.items():\n            if k in subs:\n                v = subs.get(k)\n                if isinstance(v, Number):\n                    index.append(int(v.data))\n                else:\n                    # Permute and expand v.data to end up at new_dims.\n                    assert isinstance(v, Tensor)\n                    v = v.align(tuple(k2 for k2 in inputs if k2 in v.inputs))\n                    assert isinstance(v, Tensor)\n                    v_shape = [1] * total_size\n                    for k2, size in zip(v.inputs, v.data.shape):\n                        v_shape[new_dims[k2]] = size\n                    index.append(v.data.reshape(tuple(v_shape)))\n            else:\n                # Construct a [:] slice for this preserved input.\n                offset_from_right = -1 - new_dims[k]\n                index.append(ops.new_arange(self.data, domain.dtype).reshape(\n                    (-1,) + (1,) * offset_from_right))\n\n        # Construct a [:] slice for the output.\n        for i, size in enumerate(self.output.shape):\n            offset_from_right = len(self.output.shape) - i - 1\n            index.append(ops.new_arange(self.data, size).reshape(\n                (-1,) + (1,) * offset_from_right))\n\n        data = self.data[tuple(index)]\n        return Tensor(data, inputs, self.dtype)\n\n    def eager_unary(self, op):\n        dtype = find_domain(op, self.output).dtype\n        if op in REDUCE_OP_TO_NUMERIC:\n            batch_dim = len(self.data.shape) - len(self.output.shape)\n            data = self.data.reshape(self.data.shape[:batch_dim] + (-1,))\n            data = REDUCE_OP_TO_NUMERIC[op](data, -1)\n            return Tensor(data, self.inputs, dtype)\n        return Tensor(op(self.data), self.inputs, dtype)\n\n    def eager_reduce(self, op, reduced_vars):\n        if op in REDUCE_OP_TO_NUMERIC:\n            numeric_op = REDUCE_OP_TO_NUMERIC[op]\n            assert isinstance(reduced_vars, frozenset)\n            self_vars = frozenset(self.inputs)\n            reduced_vars = reduced_vars & self_vars\n            if reduced_vars == self_vars and not self.output.shape:\n                return Tensor(numeric_op(self.data, None), dtype=self.dtype)\n\n            # Reduce one dim at a time.\n            data = self.data\n            offset = 0\n            for k, domain in self.inputs.items():\n                if k in reduced_vars:\n                    assert not domain.shape\n                    data = numeric_op(data, offset)\n                else:\n                    offset += 1\n            inputs = OrderedDict((k, v) for k, v in self.inputs.items()\n                                 if k not in reduced_vars)\n            return Tensor(data, inputs, self.dtype)\n        return super(Tensor, self).eager_reduce(op, reduced_vars)\n\n    def unscaled_sample(self, sampled_vars, sample_inputs, rng_key=None):\n        assert self.output == reals()\n        sampled_vars = sampled_vars.intersection(self.inputs)\n        if not sampled_vars:\n            return self\n\n        # Partition inputs into sample_inputs + batch_inputs + event_inputs.\n        sample_inputs = OrderedDict((k, d) for k, d in sample_inputs.items()\n                                    if k not in self.inputs)\n        sample_shape = tuple(int(d.dtype) for d in sample_inputs.values())\n        batch_inputs = OrderedDict((k, d) for k, d in self.inputs.items() if k not in sampled_vars)\n        event_inputs = OrderedDict((k, d) for k, d in self.inputs.items() if k in sampled_vars)\n        be_inputs = batch_inputs.copy()\n        be_inputs.update(event_inputs)\n        sb_inputs = sample_inputs.copy()\n        sb_inputs.update(batch_inputs)\n\n        # Sample all variables in a single Categorical call.\n        logits = align_tensor(be_inputs, self)\n        batch_shape = logits.shape[:len(batch_inputs)]\n        flat_logits = logits.reshape(batch_shape + (-1,))\n        sample_shape = tuple(d.dtype for d in sample_inputs.values())\n\n        backend = get_backend()\n        if backend != ""numpy"":\n            from importlib import import_module\n            dist = import_module(funsor.distribution.BACKEND_TO_DISTRIBUTIONS_BACKEND[backend])\n            sample_args = (sample_shape,) if rng_key is None else (rng_key, sample_shape)\n            flat_sample = dist.CategoricalLogits.dist_class(logits=flat_logits).sample(*sample_args)\n        else:  # default numpy backend\n            assert backend == ""numpy""\n            shape = sample_shape + flat_logits.shape[:-1]\n            logit_max = np.amax(flat_logits, -1, keepdims=True)\n            probs = np.exp(flat_logits - logit_max)\n            probs = probs / np.sum(probs, -1, keepdims=True)\n            s = np.cumsum(probs, -1)\n            r = np.random.rand(*shape)\n            flat_sample = np.sum(s < np.expand_dims(r, -1), axis=-1)\n\n        assert flat_sample.shape == sample_shape + batch_shape\n        results = []\n        mod_sample = flat_sample\n        for name, domain in reversed(list(event_inputs.items())):\n            size = domain.dtype\n            point = Tensor(mod_sample % size, sb_inputs, size)\n            mod_sample = mod_sample // size\n            results.append(Delta(name, point))\n\n        # Account for the log normalizer factor.\n        # Derivation: Let f be a nonnormalized distribution (a funsor), and\n        #   consider operations in linear space (source code is in log space).\n        #   Let x0 ~ f/|f| be a monte carlo sample from a normalized f/|f|.\n        #                              f(x0) / |f|      # dice numerator\n        #   Let g = delta(x=x0) |f| -----------------\n        #                           detach(f(x0)/|f|)   # dice denominator\n        #                       |detach(f)| f(x0)\n        #         = delta(x=x0) -----------------  be a dice approximation of f.\n        #                         detach(f(x0))\n        #   Then g is an unbiased estimator of f in value and all derivatives.\n        #   In the special case f = detach(f), we can simplify to\n        #       g = delta(x=x0) |f|.\n        if (backend == ""torch"" and flat_logits.requires_grad) or backend == ""jax"":\n            # Apply a dice factor to preserve differentiability.\n            index = [ops.new_arange(self.data, n).reshape((n,) + (1,) * (len(flat_logits.shape) - i - 2))\n                     for i, n in enumerate(flat_logits.shape[:-1])]\n            index.append(flat_sample)\n            log_prob = flat_logits[tuple(index)]\n            assert log_prob.shape == flat_sample.shape\n            results.append(Tensor(ops.logsumexp(ops.detach(flat_logits), -1) +\n                                  (log_prob - ops.detach(log_prob)), sb_inputs))\n        else:\n            # This is the special case f = detach(f).\n            results.append(Tensor(ops.logsumexp(flat_logits, -1), batch_inputs))\n\n        return reduce(ops.add, results)\n\n    def new_arange(self, name, *args, **kwargs):\n        """"""\n        Helper to create a named :func:`torch.arange` or :func:`np.arange` funsor.\n        In some cases this can be replaced by a symbolic\n        :class:`~funsor.terms.Slice` .\n\n        :param str name: A variable name.\n        :param int start:\n        :param int stop:\n        :param int step: Three args following :py:class:`slice` semantics.\n        :param int dtype: An optional bounded integer type of this slice.\n        :rtype: Tensor\n        """"""\n        start = 0\n        step = 1\n        dtype = None\n        if len(args) == 1:\n            stop = args[0]\n            dtype = kwargs.pop(""dtype"", stop)\n        elif len(args) == 2:\n            start, stop = args\n            dtype = kwargs.pop(""dtype"", stop)\n        elif len(args) == 3:\n            start, stop, step = args\n            dtype = kwargs.pop(""dtype"", stop)\n        elif len(args) == 4:\n            start, stop, step, dtype = args\n        else:\n            raise ValueError\n        if step <= 0:\n            raise ValueError\n        stop = min(dtype, max(start, stop))\n        data = ops.new_arange(self.data, start, stop, step)\n        inputs = OrderedDict([(name, bint(len(data)))])\n        return Tensor(data, inputs, dtype=dtype)\n\n    def materialize(self, x):\n        """"""\n        Attempt to convert a Funsor to a :class:`~funsor.terms.Number` or\n        :class:`Tensor` by substituting :func:`arange` s into its free variables.\n\n        :arg Funsor x: A funsor.\n        :rtype: Funsor\n        """"""\n        assert isinstance(x, Funsor)\n        if isinstance(x, (Number, Tensor)):\n            return x\n        subs = []\n        for name, domain in x.inputs.items():\n            if isinstance(domain.dtype, int):\n                subs.append((name, self.new_arange(name, domain.dtype)))\n        subs = tuple(subs)\n        return substitute(x, subs)\n\n\n@to_funsor.register(np.ndarray)\n@to_funsor.register(np.generic)\ndef tensor_to_funsor(x, output=None, dim_to_name=None):\n    if not dim_to_name:\n        output = output if output is not None else reals(*x.shape)\n        result = Tensor(x, dtype=output.dtype)\n        if result.output != output:\n            raise ValueError(""Invalid shape: expected {}, actual {}""\n                             .format(output.shape, result.output.shape))\n        return result\n    else:\n        assert all(isinstance(k, int) and k < 0 and isinstance(v, str)\n                   for k, v in dim_to_name.items())\n\n        if output is None:\n            # Assume the leftmost dim_to_name key refers to the leftmost dim of x\n            # when there is ambiguity about event shape\n            batch_ndims = min(-min(dim_to_name.keys()), len(x.shape))\n            output = reals(*x.shape[batch_ndims:])\n\n        # logic very similar to pyro.ops.packed.pack\n        # this should not touch memory, only reshape\n        # pack the tensor according to the dim => name mapping in inputs\n        packed_inputs = OrderedDict()\n        for dim, size in zip(range(len(x.shape) - len(output.shape)), x.shape):\n            name = dim_to_name.get(dim + len(output.shape) - len(x.shape), None)\n            if name is not None and size > 1:\n                packed_inputs[name] = bint(size)\n        shape = tuple(d.size for d in packed_inputs.values()) + output.shape\n        if x.shape != shape:\n            x = x.reshape(shape)\n        return Tensor(x, packed_inputs, dtype=output.dtype)\n\n\ndef align_tensor(new_inputs, x, expand=False):\n    r""""""\n    Permute and add dims to a tensor to match desired ``new_inputs``.\n\n    :param OrderedDict new_inputs: A target set of inputs.\n    :param funsor.terms.Funsor x: A :class:`Tensor` or\n        :class:`~funsor.terms.Number` .\n    :param bool expand: If False (default), set result size to 1 for any input\n        of ``x`` not in ``new_inputs``; if True expand to ``new_inputs`` size.\n    :return: a number or :class:`torch.Tensor` or :class:`np.ndarray` that can be broadcast to other\n        tensors with inputs ``new_inputs``.\n    :rtype: int or float or torch.Tensor or np.ndarray\n    """"""\n    assert isinstance(new_inputs, OrderedDict)\n    assert isinstance(x, (Number, Tensor))\n    assert all(isinstance(d.dtype, int) for d in x.inputs.values())\n\n    data = x.data\n    if isinstance(x, Number):\n        return data\n\n    old_inputs = x.inputs\n    if old_inputs == new_inputs:\n        return data\n\n    # Permute squashed input dims.\n    x_keys = tuple(old_inputs)\n    data = ops.permute(data, tuple(x_keys.index(k) for k in new_inputs if k in old_inputs) +\n                       tuple(range(len(old_inputs), len(data.shape))))\n\n    # Unsquash multivariate input dims by filling in ones.\n    data = data.reshape(tuple(old_inputs[k].dtype if k in old_inputs else 1 for k in new_inputs) +\n                        x.output.shape)\n\n    # Optionally expand new dims.\n    if expand:\n        data = ops.expand(data, tuple(d.dtype for d in new_inputs.values()) + x.output.shape)\n    return data\n\n\ndef align_tensors(*args, **kwargs):\n    r""""""\n    Permute multiple tensors before applying a broadcasted op.\n\n    This is mainly useful for implementing eager funsor operations.\n\n    :param funsor.terms.Funsor \\*args: Multiple :class:`Tensor` s and\n        :class:`~funsor.terms.Number` s.\n    :param bool expand: Whether to expand input tensors. Defaults to False.\n    :return: a pair ``(inputs, tensors)`` where tensors are all\n        :class:`torch.Tensor` s or :class:`np.ndarray` s\n        that can be broadcast together to a single data\n        with given ``inputs``.\n    :rtype: tuple\n    """"""\n    expand = kwargs.pop(\'expand\', False)\n    assert not kwargs\n    inputs = OrderedDict()\n    for x in args:\n        inputs.update(x.inputs)\n    tensors = [align_tensor(inputs, x, expand=expand) for x in args]\n    return inputs, tensors\n\n\n@to_data.register(Tensor)\ndef tensor_to_data(x, name_to_dim=None):\n    if not name_to_dim or not x.inputs:\n        if x.inputs:\n            raise ValueError(f""cannot convert Tensor to data due to lazy inputs: {set(x.inputs)}"")\n        return x.data\n    else:\n        assert all(isinstance(k, str) and isinstance(v, int) and v < 0\n                   for k, v in name_to_dim.items())\n        # logic very similar to pyro.ops.packed.unpack\n        # first collapse input domains into single dimensions\n        data = x.data.reshape(tuple(d.dtype for d in x.inputs.values()) + x.output.shape)\n        # permute packed dimensions to correct order\n        unsorted_dims = [name_to_dim[name] for name in x.inputs]\n        dims = sorted(unsorted_dims)\n        permutation = [unsorted_dims.index(dim) for dim in dims] + \\\n            list(range(len(dims), len(dims) + len(x.output.shape)))\n        data = ops.permute(data, permutation)\n        # expand\n        batch_shape = [1] * -min(dims)\n        for dim, size in zip(dims, data.shape):\n            batch_shape[dim] = size\n        return data.reshape(tuple(batch_shape) + x.output.shape)\n\n\n@eager.register(Binary, Op, Tensor, Number)\ndef eager_binary_tensor_number(op, lhs, rhs):\n    data = op(lhs.data, rhs.data)\n    return Tensor(data, lhs.inputs, lhs.dtype)\n\n\n@eager.register(Binary, Op, Number, Tensor)\ndef eager_binary_number_tensor(op, lhs, rhs):\n    data = op(lhs.data, rhs.data)\n    return Tensor(data, rhs.inputs, rhs.dtype)\n\n\n@eager.register(Binary, Op, Tensor, Tensor)\ndef eager_binary_tensor_tensor(op, lhs, rhs):\n    # Compute inputs and outputs.\n    dtype = find_domain(op, lhs.output, rhs.output).dtype\n    if lhs.inputs == rhs.inputs:\n        inputs = lhs.inputs\n        lhs_data, rhs_data = lhs.data, rhs.data\n    else:\n        inputs, (lhs_data, rhs_data) = align_tensors(lhs, rhs)\n\n    # Reshape to support broadcasting of output shape.\n    if inputs:\n        lhs_dim = len(lhs.shape)\n        rhs_dim = len(rhs.shape)\n        if lhs_dim < rhs_dim:\n            cut = len(lhs_data.shape) - lhs_dim\n            shape = lhs_data.shape\n            shape = shape[:cut] + (1,) * (rhs_dim - lhs_dim) + shape[cut:]\n            lhs_data = lhs_data.reshape(shape)\n        elif rhs_dim < lhs_dim:\n            cut = len(rhs_data.shape) - rhs_dim\n            shape = rhs_data.shape\n            shape = shape[:cut] + (1,) * (lhs_dim - rhs_dim) + shape[cut:]\n            rhs_data = rhs_data.reshape(shape)\n\n    data = op(lhs_data, rhs_data)\n    return Tensor(data, inputs, dtype)\n\n\n@eager.register(Binary, MatmulOp, Tensor, Tensor)\ndef eager_binary_tensor_tensor(op, lhs, rhs):\n    # Compute inputs and outputs.\n    dtype = find_domain(op, lhs.output, rhs.output).dtype\n    if lhs.inputs == rhs.inputs:\n        inputs = lhs.inputs\n        lhs_data, rhs_data = lhs.data, rhs.data\n    else:\n        inputs, (lhs_data, rhs_data) = align_tensors(lhs, rhs)\n    if len(lhs.shape) == 1:\n        lhs_data = ops.unsqueeze(lhs_data, -2)\n    if len(rhs.shape) == 1:\n        rhs_data = ops.unsqueeze(rhs_data, -1)\n\n    # Reshape to support broadcasting of output shape.\n    if inputs:\n        lhs_dim = max(2, len(lhs.shape))\n        rhs_dim = max(2, len(rhs.shape))\n        if lhs_dim < rhs_dim:\n            cut = len(lhs_data.shape) - lhs_dim\n            shape = lhs_data.shape\n            shape = shape[:cut] + (1,) * (rhs_dim - lhs_dim) + shape[cut:]\n            lhs_data = lhs_data.reshape(shape)\n        elif rhs_dim < lhs_dim:\n            cut = len(rhs_data.shape) - rhs_dim\n            shape = rhs_data.shape\n            shape = shape[:cut] + (1,) * (lhs_dim - rhs_dim) + shape[cut:]\n            rhs_data = rhs_data.reshape(shape)\n\n    data = op(lhs_data, rhs_data)\n    if len(lhs.shape) == 1:\n        data = data.squeeze(-2)\n    if len(rhs.shape) == 1:\n        data = data.squeeze(-1)\n    return Tensor(data, inputs, dtype)\n\n\n@eager.register(Unary, ReshapeOp, Tensor)\ndef eager_reshape_tensor(op, arg):\n    if arg.shape == op.shape:\n        return arg\n    batch_shape = arg.data.shape[:len(arg.data.shape) - len(arg.shape)]\n    data = arg.data.reshape(batch_shape + op.shape)\n    return Tensor(data, arg.inputs, arg.dtype)\n\n\n@eager.register(Binary, GetitemOp, Tensor, Number)\ndef eager_getitem_tensor_number(op, lhs, rhs):\n    index = [slice(None)] * (len(lhs.inputs) + op.offset)\n    index.append(rhs.data)\n    index = tuple(index)\n    data = lhs.data[index]\n    return Tensor(data, lhs.inputs, lhs.dtype)\n\n\n@eager.register(Binary, GetitemOp, Tensor, Variable)\ndef eager_getitem_tensor_variable(op, lhs, rhs):\n    assert op.offset < len(lhs.output.shape)\n    assert rhs.output == bint(lhs.output.shape[op.offset])\n    assert rhs.name not in lhs.inputs\n\n    # Convert a positional event dimension to a named batch dimension.\n    inputs = lhs.inputs.copy()\n    inputs[rhs.name] = rhs.output\n    data = lhs.data\n    target_dim = len(lhs.inputs)\n    source_dim = target_dim + op.offset\n    if target_dim != source_dim:\n        perm = list(range(len(data.shape)))\n        del perm[source_dim]\n        perm.insert(target_dim, source_dim)\n        data = ops.permute(data, perm)\n    return Tensor(data, inputs, lhs.dtype)\n\n\n@eager.register(Binary, GetitemOp, Tensor, Tensor)\ndef eager_getitem_tensor_tensor(op, lhs, rhs):\n    assert op.offset < len(lhs.output.shape)\n    assert rhs.output == bint(lhs.output.shape[op.offset])\n\n    # Compute inputs and outputs.\n    if lhs.inputs == rhs.inputs:\n        inputs, lhs_data, rhs_data = lhs.inputs, lhs.data, rhs.data\n    else:\n        inputs, (lhs_data, rhs_data) = align_tensors(lhs, rhs)\n    if len(lhs.output.shape) > 1:\n        rhs_data = rhs_data.reshape(rhs_data.shape + (1,) * (len(lhs.output.shape) - 1))\n\n    # Perform advanced indexing.\n    lhs_data_dim = len(lhs_data.shape)\n    target_dim = lhs_data_dim - len(lhs.output.shape) + op.offset\n    index = [None] * lhs_data_dim\n    for i in range(target_dim):\n        index[i] = ops.new_arange(lhs_data, lhs_data.shape[i]).reshape((-1,) + (1,) * (lhs_data_dim - i - 2))\n    index[target_dim] = rhs_data\n    for i in range(1 + target_dim, lhs_data_dim):\n        index[i] = ops.new_arange(lhs_data, lhs_data.shape[i]).reshape((-1,) + (1,) * (lhs_data_dim - i - 1))\n    data = lhs_data[tuple(index)]\n    return Tensor(data, inputs, lhs.dtype)\n\n\n@eager.register(Lambda, Variable, Tensor)\ndef eager_lambda(var, expr):\n    inputs = expr.inputs.copy()\n    if var.name in inputs:\n        inputs.pop(var.name)\n        inputs[var.name] = var.output\n        data = align_tensor(inputs, expr)\n        inputs.pop(var.name)\n    else:\n        data = expr.data\n        shape = data.shape\n        dim = len(shape) - len(expr.output.shape)\n        data = data.reshape(shape[:dim] + (1,) + shape[dim:])\n        data = ops.expand(data, shape[:dim] + (var.dtype,) + shape[dim:])\n    return Tensor(data, inputs, expr.dtype)\n\n\n@dispatch(str, Variadic[Tensor])\ndef eager_stack_homogeneous(name, *parts):\n    assert parts\n    output = parts[0].output\n    part_inputs = OrderedDict()\n    for part in parts:\n        assert part.output == output\n        assert name not in part.inputs\n        part_inputs.update(part.inputs)\n\n    shape = tuple(d.size for d in part_inputs.values()) + output.shape\n    data = ops.stack(0, *[ops.expand(align_tensor(part_inputs, part), shape)\n                          for part in parts])\n    inputs = OrderedDict([(name, bint(len(parts)))])\n    inputs.update(part_inputs)\n    return Tensor(data, inputs, dtype=output.dtype)\n\n\n@dispatch(str, str, Variadic[Tensor])\ndef eager_cat_homogeneous(name, part_name, *parts):\n    assert parts\n    output = parts[0].output\n    inputs = OrderedDict([(part_name, None)])\n    for part in parts:\n        assert part.output == output\n        assert part_name in part.inputs\n        inputs.update(part.inputs)\n\n    tensors = []\n    for part in parts:\n        inputs[part_name] = part.inputs[part_name]\n        shape = tuple(d.size for d in inputs.values()) + output.shape\n        tensors.append(ops.expand(align_tensor(inputs, part), shape))\n    del inputs[part_name]\n\n    dim = 0\n    tensor = ops.cat(dim, *tensors)\n    inputs = OrderedDict([(name, bint(tensor.shape[dim]))] + list(inputs.items()))\n    return Tensor(tensor, inputs, dtype=output.dtype)\n\n\nclass LazyTuple(tuple):\n    def __call__(self, *args, **kwargs):\n        return LazyTuple(x(*args, **kwargs) for x in self)\n\n\nclass Function(Funsor):\n    r""""""\n    Funsor wrapped by a native PyTorch or NumPy function.\n\n    Functions are assumed to support broadcasting and can be eagerly evaluated\n    on funsors with free variables of int type (i.e. batch dimensions).\n\n    :class:`Function` s are usually created via the :func:`function` decorator.\n\n    :param callable fn: A native PyTorch or NumPy function to wrap.\n    :param funsor.domains.Domain output: An output domain.\n    :param Funsor args: Funsor arguments.\n    """"""\n    def __init__(self, fn, output, args):\n        assert callable(fn)\n        assert not isinstance(fn, Function)\n        assert isinstance(args, tuple)\n        inputs = OrderedDict()\n        for arg in args:\n            assert isinstance(arg, Funsor)\n            inputs.update(arg.inputs)\n        super(Function, self).__init__(inputs, output)\n        self.fn = fn\n        self.args = args\n\n    def __repr__(self):\n        return \'{}({}, {}, {})\'.format(type(self).__name__, _nameof(self.fn),\n                                       repr(self.output), repr(self.args))\n\n    def __str__(self):\n        return \'{}({}, {}, {})\'.format(type(self).__name__, _nameof(self.fn),\n                                       str(self.output), str(self.args))\n\n\n@quote.register(Function)\ndef _(arg, indent, out):\n    out.append((indent, f""Function({_nameof(arg.fn)},""))\n    quote.inplace(arg.output, indent + 1, out)\n    i, line = out[-1]\n    out[-1] = i, line + "",""\n    quote.inplace(arg.args, indent + 1, out)\n    i, line = out[-1]\n    out[-1] = i, line + "")""\n\n\n@eager.register(Function, object, Domain, tuple)\ndef eager_function(fn, output, args):\n    if not all(isinstance(arg, (Number, Tensor)) for arg in args):\n        return None  # defer to default implementation\n    inputs, tensors = align_tensors(*args)\n    data = fn(*tensors)\n    result = Tensor(data, inputs, dtype=output.dtype)\n    assert result.output == output\n    return result\n\n\ndef _select(fn, i, *args):\n    result = fn(*args)\n    assert isinstance(result, tuple)\n    return result[i]\n\n\ndef _nested_function(fn, args, output):\n    if isinstance(output, Domain):\n        return Function(fn, output, args)\n    elif isinstance(output, tuple):\n        result = []\n        for i, output_i in enumerate(output):\n            fn_i = functools.partial(_select, fn, i)\n            fn_i.__name__ = f""{_nameof(fn)}_{i}""\n            result.append(_nested_function(fn_i, args, output_i))\n        return LazyTuple(result)\n    raise ValueError(""Invalid output: {}"".format(output))\n\n\nclass _Memoized(object):\n    def __init__(self, fn):\n        self.fn = fn\n        self._cache = None\n\n    def __call__(self, *args):\n        if self._cache is not None:\n            old_args, old_result = self._cache\n            if all(x is y for x, y in zip(args, old_args)):\n                return old_result\n        result = self.fn(*args)\n        self._cache = args, result\n        return result\n\n    @property\n    def __name__(self):\n        return _nameof(self.fn)\n\n\ndef _function(inputs, output, fn):\n    if is_nn_module(fn):\n        names = getargspec(fn.forward)[0][1:]\n    else:\n        names = getargspec(fn)[0]\n    args = tuple(Variable(name, domain) for (name, domain) in zip(names, inputs))\n    assert len(args) == len(inputs)\n    if not isinstance(output, Domain):\n        assert isinstance(output, tuple)\n        # Memoize multiple-output functions so that invocations can be shared among\n        # all outputs. This is not foolproof, but does work in simple situations.\n        fn = _Memoized(fn)\n    return _nested_function(fn, args, output)\n\n\ndef function(*signature):\n    r""""""\n    Decorator to wrap a PyTorch/NumPy function.\n\n    Example::\n\n        @funsor.torch.function(reals(3,4), reals(4,5), reals(3,5))\n        def matmul(x, y):\n            return torch.matmul(x, y)\n\n        @funsor.torch.function(reals(10), reals(10, 10), reals())\n        def mvn_log_prob(loc, scale_tril, x):\n            d = torch.distributions.MultivariateNormal(loc, scale_tril)\n            return d.log_prob(x)\n\n    To support functions that output nested tuples of tensors, specify a nested\n    tuple of output types, for example::\n\n        @funsor.torch.function(reals(8), (reals(), bint(8)))\n        def max_and_argmax(x):\n            return torch.max(x, dim=-1)\n\n    :param \\*signature: A sequence if input domains followed by a final output\n        domain or nested tuple of output domains.\n    """"""\n    assert signature\n    inputs, output = signature[:-1], signature[-1]\n    assert all(isinstance(d, Domain) for d in inputs)\n    assert isinstance(output, (Domain, tuple))\n    return functools.partial(_function, inputs, output)\n\n\nclass Einsum(Funsor):\n    """"""\n    Wrapper around :func:`torch.einsum` or :func:`np.einsum` to operate on real-valued Funsors.\n\n    Note this operates only on the ``output`` tensor. To perform sum-product\n    contractions on named dimensions, instead use ``+`` and\n    :class:`~funsor.terms.Reduce`.\n\n    :param str equation: An :func:`torch.einsum` or :func:`np.einsum` equation.\n    :param tuple operands: A tuple of input funsors.\n    """"""\n    def __init__(self, equation, operands):\n        assert isinstance(equation, str)\n        assert isinstance(operands, tuple)\n        assert all(isinstance(x, Funsor) for x in operands)\n        ein_inputs, ein_output = equation.split(\'->\')\n        ein_inputs = ein_inputs.split(\',\')\n        size_dict = {}\n        inputs = OrderedDict()\n        assert len(ein_inputs) == len(operands)\n        for ein_input, x in zip(ein_inputs, operands):\n            assert x.dtype == \'real\'\n            inputs.update(x.inputs)\n            assert len(ein_input) == len(x.output.shape)\n            for name, size in zip(ein_input, x.output.shape):\n                other_size = size_dict.setdefault(name, size)\n                if other_size != size:\n                    raise ValueError(""Size mismatch at {}: {} vs {}""\n                                     .format(name, size, other_size))\n        output = reals(*(size_dict[d] for d in ein_output))\n        super(Einsum, self).__init__(inputs, output)\n        self.equation = equation\n        self.operands = operands\n\n    def __repr__(self):\n        return \'Einsum({}, {})\'.format(repr(self.equation), repr(self.operands))\n\n    def __str__(self):\n        return \'Einsum({}, {})\'.format(repr(self.equation), str(self.operands))\n\n\n@eager.register(Einsum, str, tuple)\ndef eager_einsum(equation, operands):\n    if all(isinstance(x, Tensor) for x in operands):\n        # Make new symbols for inputs of operands.\n        inputs = OrderedDict()\n        for x in operands:\n            inputs.update(x.inputs)\n        symbols = set(equation)\n        get_symbol = iter(map(opt_einsum.get_symbol, itertools.count()))\n        new_symbols = {}\n        for k in inputs:\n            symbol = next(get_symbol)\n            while symbol in symbols:\n                symbol = next(get_symbol)\n            symbols.add(symbol)\n            new_symbols[k] = symbol\n\n        # Manually broadcast using einsum symbols.\n        assert \'.\' not in equation\n        ins, out = equation.split(\'->\')\n        ins = ins.split(\',\')\n        ins = [\'\'.join(new_symbols[k] for k in x.inputs) + x_out\n               for x, x_out in zip(operands, ins)]\n        out = \'\'.join(new_symbols[k] for k in inputs) + out\n        equation = \',\'.join(ins) + \'->\' + out\n\n        data = ops.einsum(equation, *[x.data for x in operands])\n        return Tensor(data, inputs)\n\n    return None  # defer to default implementation\n\n\ndef tensordot(x, y, dims):\n    """"""\n    Wrapper around :func:`torch.tensordot` or :func:`np.tensordot`\n    to operate on real-valued Funsors.\n\n    Note this operates only on the ``output`` tensor. To perform sum-product\n    contractions on named dimensions, instead use ``+`` and\n    :class:`~funsor.terms.Reduce`.\n\n    Arguments should satisfy::\n\n        len(x.shape) >= dims\n        len(y.shape) >= dims\n        dims == 0 or x.shape[-dims:] == y.shape[:dims]\n\n    :param Funsor x: A left hand argument.\n    :param Funsor y: A y hand argument.\n    :param int dims: The number of dimension of overlap of output shape.\n    :rtype: Funsor\n    """"""\n    assert dims >= 0\n    assert len(x.shape) >= dims\n    assert len(y.shape) >= dims\n    assert dims == 0 or x.shape[-dims:] == y.shape[:dims]\n    x_start, x_end = 0, len(x.output.shape)\n    y_start = x_end - dims\n    y_end = y_start + len(y.output.shape)\n    symbols = \'abcdefghijklmnopqrstuvwxyz\'\n    equation = \'{},{}->{}\'.format(symbols[x_start:x_end],\n                                  symbols[y_start:y_end],\n                                  symbols[x_start:y_start] + symbols[x_end:y_end])\n    return Einsum(equation, (x, y))\n\n\ndef stack(parts, dim=0):\n    """"""\n    Wrapper around :func:`torch.stack` or :func:`np.stack` to operate on real-valued Funsors.\n\n    Note this operates only on the ``output`` tensor. To stack funsors in a\n    new named dim, instead use :class:`~funsor.terms.Stack`.\n\n    :param tuple parts: A tuple of funsors.\n    :param int dim: A torch dim along which to stack.\n    :rtype: Funsor\n    """"""\n    assert isinstance(dim, int)\n    assert isinstance(parts, tuple)\n    assert len(set(x.output for x in parts)) == 1\n    shape = parts[0].output.shape\n    if dim >= 0:\n        dim = dim - len(shape) - 1\n    assert dim < 0\n    split = dim + len(shape) + 1\n    shape = shape[:split] + (len(parts),) + shape[split:]\n    output = Domain(shape, parts[0].dtype)\n    fn = functools.partial(ops.stack, dim)\n    return Function(fn, output, parts)\n\n\nREDUCE_OP_TO_NUMERIC = {\n    ops.add: ops.sum,\n    ops.mul: ops.prod,\n    ops.and_: ops.all,\n    ops.or_: ops.any,\n    ops.logaddexp: ops.logsumexp,\n    ops.sample: ops.logsumexp,\n    ops.min: ops.amin,\n    ops.max: ops.amax,\n}\n\n\n__all__ = [\n    \'Einsum\',\n    \'Function\',\n    \'REDUCE_OP_TO_NUMERIC\',\n    \'Tensor\',\n    \'align_tensor\',\n    \'align_tensors\',\n    \'function\',\n    \'ignore_jit_warnings\',\n    \'stack\',\n    \'tensordot\',\n]\n'"
funsor/terms.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport itertools\nimport math\nimport numbers\nimport typing\nfrom collections import Hashable, OrderedDict\nfrom functools import reduce, singledispatch\nfrom weakref import WeakValueDictionary\n\nfrom multipledispatch import dispatch\nfrom multipledispatch.variadic import Variadic, isvariadic\n\nimport funsor.interpreter as interpreter\nimport funsor.ops as ops\nfrom funsor.domains import Domain, bint, find_domain, reals\nfrom funsor.interpreter import PatternMissingError, dispatched_interpretation, interpret\nfrom funsor.ops import AssociativeOp, GetitemOp, Op\nfrom funsor.util import getargspec, lazy_property, pretty, quote\n\n\ndef substitute(expr, subs):\n    if isinstance(subs, (dict, OrderedDict)):\n        subs = tuple(subs.items())\n    assert isinstance(subs, tuple)\n    assert all(isinstance(v, Funsor) for k, v in subs)\n\n    @interpreter.interpretation(interpreter._INTERPRETATION)  # use base\n    def subs_interpreter(cls, *args):\n        expr = cls(*args)\n        fresh_subs = tuple((k, v) for k, v in subs if k in expr.fresh)\n        if fresh_subs:\n            expr = interpreter.debug_logged(expr.eager_subs)(fresh_subs)\n        return expr\n\n    with interpreter.interpretation(subs_interpreter):\n        return interpreter.reinterpret(expr)\n\n\ndef _alpha_mangle(expr):\n    """"""\n    Rename bound variables in expr to avoid conflict with any free variables.\n\n    FIXME this does not avoid conflict with other bound variables.\n    """"""\n    alpha_subs = {name: interpreter.gensym(name + ""__BOUND"")\n                  for name in expr.bound if ""__BOUND"" not in name}\n    if not alpha_subs:\n        return expr\n\n    ast_values = expr._alpha_convert(alpha_subs)\n    return reflect(type(expr), *ast_values)\n\n\ndef reflect(cls, *args, **kwargs):\n    """"""\n    Construct a funsor, populate ``._ast_values``, and cons hash.\n    This is the only interpretation allowed to construct funsors.\n    """"""\n    if len(args) > len(cls._ast_fields):\n        # handle varargs\n        new_args = tuple(args[:len(cls._ast_fields) - 1]) + (args[len(cls._ast_fields) - 1 - len(args):],)\n        assert len(new_args) == len(cls._ast_fields)\n        _, args = args, new_args\n\n    # JAX DeviceArray has .__hash__ method but raise the unhashable error there.\n    cache_key = tuple(id(arg) if type(arg).__name__ == ""DeviceArray"" or not isinstance(arg, Hashable)\n                      else arg for arg in args)\n    if cache_key in cls._cons_cache:\n        return cls._cons_cache[cache_key]\n\n    arg_types = tuple(typing.Tuple[tuple(map(type, arg))]\n                      if (type(arg) is tuple and all(isinstance(a, Funsor) for a in arg))\n                      else typing.Tuple if (type(arg) is tuple and not arg)\n                      else type(arg) for arg in args)\n    cls_specific = (cls.__origin__ if cls.__args__ else cls)[arg_types]\n    result = super(FunsorMeta, cls_specific).__call__(*args)\n    result._ast_values = args\n\n    # alpha-convert eagerly upon binding any variable\n    result = _alpha_mangle(result)\n\n    cls._cons_cache[cache_key] = result\n    return result\n\n\n@dispatched_interpretation\ndef normalize(cls, *args):\n\n    result = normalize.dispatch(cls, *args)(*args)\n    if result is None:\n        result = reflect(cls, *args)\n\n    return result\n\n\n@dispatched_interpretation\ndef lazy(cls, *args):\n    """"""\n    Substitute eagerly but perform ops lazily.\n    """"""\n    result = lazy.dispatch(cls, *args)(*args)\n    if result is None:\n        result = reflect(cls, *args)\n    return result\n\n\n@dispatched_interpretation\ndef eager(cls, *args):\n    """"""\n    Eagerly execute ops with known implementations.\n    """"""\n    result = eager.dispatch(cls, *args)(*args)\n    if result is None:\n        result = normalize.dispatch(cls, *args)(*args)\n    if result is None:\n        result = reflect(cls, *args)\n    return result\n\n\n@dispatched_interpretation\ndef eager_or_die(cls, *args):\n    """"""\n    Eagerly execute ops with known implementations.\n    Disallows lazy :class:`Subs` , :class:`Unary` , :class:`Binary` , and\n    :class:`Reduce` .\n\n    :raises: :py:class:`NotImplementedError` no pattern is found.\n    """"""\n    result = eager.dispatch(cls, *args)(*args)\n    if result is None:\n        if cls in (Subs, Unary, Binary, Reduce):\n            raise NotImplementedError(""Missing pattern for {}({})"".format(\n                cls.__name__, "", "".join(map(str, args))))\n        result = reflect(cls, *args)\n    return result\n\n\n@dispatched_interpretation\ndef sequential(cls, *args):\n    """"""\n    Eagerly execute ops with known implementations; additonally execute\n    vectorized ops sequentially if no known vectorized implementation exists.\n    """"""\n    result = sequential.dispatch(cls, *args)(*args)\n    if result is None:\n        result = eager.dispatch(cls, *args)(*args)\n    if result is None:\n        result = normalize.dispatch(cls, *args)(*args)\n    if result is None:\n        result = reflect(cls, *args)\n    return result\n\n\n@dispatched_interpretation\ndef moment_matching(cls, *args):\n    """"""\n    A moment matching interpretation of :class:`Reduce` expressions. This falls\n    back to :class:`eager` in other cases.\n    """"""\n    result = moment_matching.dispatch(cls, *args)(*args)\n    if result is None:\n        result = eager.dispatch(cls, *args)(*args)\n    if result is None:\n        result = normalize.dispatch(cls, *args)(*args)\n    if result is None:\n        result = reflect(cls, *args)\n    return result\n\n\ninterpreter.set_interpretation(eager)  # Use eager interpretation by default.\n\n\nclass FunsorMeta(type):\n    """"""\n    Metaclass for Funsors to perform four independent tasks:\n\n    1.  Fill in default kwargs and convert kwargs to args before deferring to a\n        nonstandard interpretation. This allows derived metaclasses to fill in\n        defaults and do type conversion, thereby simplifying logic of\n        interpretations.\n    2.  Ensure each Funsor class has an attribute ``._ast_fields`` describing\n        its input args and each Funsor instance has an attribute ``._ast_args``\n        with values corresponding to its input args. This allows the instance\n        to be reflectively reconstructed under a different interpretation, and\n        is used by :func:`funsor.interpreter.reinterpret`.\n    3.  Cons-hash construction, so that repeatedly calling the constructor\n        with identical args will product the same object. This enables cheap\n        syntactic equality testing using the ``is`` operator, which is\n        is important both for hashing (e.g. for memoizing funsor functions)\n        and for unit testing, since ``.__eq__()`` is overloaded with\n        elementwise semantics. Cons hashing differs from memoization in that\n        it incurs no memory overhead beyond the cons hash dict.\n    4.  Support subtyping with parameters for pattern matching, e.g. Number[int, int].\n    """"""\n    def __init__(cls, name, bases, dct):\n        super(FunsorMeta, cls).__init__(name, bases, dct)\n        if not hasattr(cls, ""__args__""):\n            cls.__args__ = ()\n        if cls.__args__:\n            base, = bases\n            cls.__origin__ = base\n        else:\n            cls._ast_fields = getargspec(cls.__init__)[0][1:]\n            cls._cons_cache = WeakValueDictionary()\n            cls._type_cache = WeakValueDictionary()\n\n    def __call__(cls, *args, **kwargs):\n        if cls.__args__:\n            cls = cls.__origin__\n\n        # Convert kwargs to args.\n        if kwargs:\n            args = list(args)\n            for name in cls._ast_fields[len(args):]:\n                args.append(kwargs.pop(name))\n            assert not kwargs, kwargs\n            args = tuple(args)\n\n        return interpret(cls, *args)\n\n    def __getitem__(cls, arg_types):\n        if not isinstance(arg_types, tuple):\n            arg_types = (arg_types,)\n        assert not any(isvariadic(arg_type) for arg_type in arg_types), ""nested variadic types not supported""\n        # switch tuple to typing.Tuple\n        arg_types = tuple(typing.Tuple if arg_type is tuple else arg_type for arg_type in arg_types)\n        if arg_types not in cls._type_cache:\n            assert not cls.__args__, ""cannot subscript a subscripted type {}"".format(cls)\n            assert len(arg_types) == len(cls._ast_fields), ""must provide types for all params""\n            new_dct = cls.__dict__.copy()\n            new_dct.update({""__args__"": arg_types})\n            # type(cls) to handle FunsorMeta subclasses\n            cls._type_cache[arg_types] = type(cls)(cls.__name__, (cls,), new_dct)\n        return cls._type_cache[arg_types]\n\n    def __subclasscheck__(cls, subcls):  # issubclass(subcls, cls)\n        if cls is subcls:\n            return True\n        if not isinstance(subcls, FunsorMeta):\n            return super(FunsorMeta, getattr(cls, ""__origin__"", cls)).__subclasscheck__(subcls)\n\n        cls_origin = getattr(cls, ""__origin__"", cls)\n        subcls_origin = getattr(subcls, ""__origin__"", subcls)\n        if not super(FunsorMeta, cls_origin).__subclasscheck__(subcls_origin):\n            return False\n\n        if cls.__args__:\n            if not subcls.__args__:\n                return False\n            if len(cls.__args__) != len(subcls.__args__):\n                return False\n            for subcls_param, param in zip(subcls.__args__, cls.__args__):\n                if not _issubclass_tuple(subcls_param, param):\n                    return False\n        return True\n\n    @lazy_property\n    def classname(cls):\n        return cls.__name__ + ""[{}]"".format("", "".join(\n            str(getattr(t, ""classname"", t))  # Tuple doesn\'t have __name__\n            for t in cls.__args__))\n\n\ndef _issubclass_tuple(subcls, cls):\n    """"""\n    utility for pattern matching with tuple subexpressions\n    """"""\n    # so much boilerplate...\n    cls_is_union = hasattr(cls, ""__origin__"") and (cls.__origin__ or cls) is typing.Union\n    if isinstance(cls, tuple) or cls_is_union:\n        return any(_issubclass_tuple(subcls, option)\n                   for option in (getattr(cls, ""__args__"", []) if cls_is_union else cls))\n\n    subcls_is_union = hasattr(subcls, ""__origin__"") and (subcls.__origin__ or subcls) is typing.Union\n    if isinstance(subcls, tuple) or subcls_is_union:\n        return any(_issubclass_tuple(option, cls)\n                   for option in (getattr(subcls, ""__args__"", []) if subcls_is_union else subcls))\n\n    subcls_is_tuple = hasattr(subcls, ""__origin__"") and (subcls.__origin__ or subcls) in (tuple, typing.Tuple)\n    cls_is_tuple = hasattr(cls, ""__origin__"") and (cls.__origin__ or cls) in (tuple, typing.Tuple)\n    if subcls_is_tuple != cls_is_tuple:\n        return False\n    if not cls_is_tuple:\n        return issubclass(subcls, cls)\n    if not cls.__args__:\n        return True\n    if not subcls.__args__ or len(subcls.__args__) != len(cls.__args__):\n        return False\n\n    return all(_issubclass_tuple(a, b) for a, b in zip(subcls.__args__, cls.__args__))\n\n\ndef _convert_reduced_vars(reduced_vars):\n    """"""\n    Helper to convert the reduced_vars arg of ``.reduce()`` and friends.\n\n    :param reduced_vars:\n    :type reduced_vars: str, Variable, or set or frozenset thereof.\n    :rtype: frozenset of str\n    """"""\n    # Avoid copying if arg is of correct type.\n    if (isinstance(reduced_vars, frozenset) and\n            all(isinstance(var, str) for var in reduced_vars)):\n        return reduced_vars\n\n    if isinstance(reduced_vars, (str, Variable)):\n        reduced_vars = {reduced_vars}\n    assert isinstance(reduced_vars, (frozenset, set))\n    assert all(isinstance(var, (str, Variable)) for var in reduced_vars)\n    return frozenset(var if isinstance(var, str) else var.name\n                     for var in reduced_vars)\n\n\nclass Funsor(object, metaclass=FunsorMeta):\n    """"""\n    Abstract base class for immutable functional tensors.\n\n    Concrete derived classes must implement ``__init__()`` methods taking\n    hashable ``*args`` and no optional ``**kwargs`` so as to support cons\n    hashing.\n\n    Derived classes with ``.fresh`` variables must implement an\n    :meth:`eager_subs` method. Derived classes with ``.bound`` variables must\n    implement an :meth:`_alpha_convert` method.\n\n    :param OrderedDict inputs: A mapping from input name to domain.\n        This can be viewed as a typed context or a mapping from\n        free variables to domains.\n    :param Domain output: An output domain.\n    """"""\n    def __init__(self, inputs, output, fresh=None, bound=None):\n        fresh = frozenset() if fresh is None else fresh\n        bound = frozenset() if bound is None else bound\n        assert isinstance(inputs, OrderedDict)\n        for name, input_ in inputs.items():\n            assert isinstance(name, str)\n            assert isinstance(input_, Domain)\n        assert isinstance(output, Domain)\n        assert isinstance(fresh, frozenset)\n        assert isinstance(bound, frozenset)\n        super(Funsor, self).__init__()\n        self.inputs = inputs\n        self.output = output\n        self.fresh = fresh\n        self.bound = bound\n\n    @property\n    def dtype(self):\n        return self.output.dtype\n\n    @property\n    def shape(self):\n        return self.output.shape\n\n    def __hash__(self):\n        return id(self)\n\n    def __repr__(self):\n        return \'{}({})\'.format(type(self).__name__, \', \'.join(map(repr, self._ast_values)))\n\n    def __str__(self):\n        return \'{}({})\'.format(type(self).__name__, \', \'.join(map(str, self._ast_values)))\n\n    def quote(self):\n        return quote(self)\n\n    def pretty(self, maxlen=40):\n        return pretty(self, maxlen=maxlen)\n\n    def __contains__(self, item):\n        raise TypeError\n\n    def _alpha_convert(self, alpha_subs):\n        """"""\n        Rename bound variables while preserving all free variables.\n        """"""\n        # Substitute all funsor values.\n        # Subclasses must handle string conversion.\n        assert self.bound.issuperset(alpha_subs)\n        return tuple(substitute(v, alpha_subs) for v in self._ast_values)\n\n    def __call__(self, *args, **kwargs):\n        """"""\n        Partially evaluates this funsor by substituting dimensions.\n        """"""\n        # Eagerly restrict to this funsor\'s inputs.\n        subs = OrderedDict(zip(self.inputs, args))\n        for k in self.inputs:\n            if k in kwargs:\n                subs[k] = kwargs[k]\n        return Subs(self, tuple(subs.items()))\n\n    def __bool__(self):\n        if self.inputs or self.output.shape:\n            raise ValueError(\n                ""bool value of Funsor with more than one value is ambiguous"")\n        raise NotImplementedError\n\n    def __nonzero__(self):\n        return self.__bool__()\n\n    def __len__(self):\n        if not self.output.shape:\n            raise ValueError(\'Funsor with empty shape has no len()\')\n        return self.output.shape[0]\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n    def item(self):\n        if self.inputs or self.output.shape:\n            raise ValueError(\n                ""only one element Funsors can be converted to Python scalars"")\n        raise NotImplementedError\n\n    @property\n    def requires_grad(self):\n        return False\n\n    def reduce(self, op, reduced_vars=None):\n        """"""\n        Reduce along all or a subset of inputs.\n\n        :param callable op: A reduction operation.\n        :param reduced_vars: An optional input name or set of names to reduce.\n            If unspecified, all inputs will be reduced.\n        :type reduced_vars: str, Variable, or set or frozenset thereof.\n        """"""\n        assert isinstance(op, AssociativeOp)\n        # Eagerly convert reduced_vars to appropriate things.\n        if reduced_vars is None:\n            # Empty reduced_vars means ""reduce over everything"".\n            reduced_vars = frozenset(self.inputs)\n        else:\n            reduced_vars = _convert_reduced_vars(reduced_vars)\n        assert isinstance(reduced_vars, frozenset), reduced_vars\n        if not reduced_vars:\n            return self\n        assert reduced_vars.issubset(self.inputs)\n        return Reduce(op, self, reduced_vars)\n\n    def sample(self, sampled_vars, sample_inputs=None, rng_key=None):\n        """"""\n        Create a Monte Carlo approximation to this funsor by replacing\n        functions of ``sampled_vars`` with :class:`~funsor.delta.Delta` s.\n\n        The result is a :class:`Funsor` with the same ``.inputs`` and\n        ``.output`` as the original funsor (plus ``sample_inputs`` if\n        provided), so that self can be replaced by the sample in expectation\n        computations::\n\n            y = x.sample(sampled_vars)\n            assert y.inputs == x.inputs\n            assert y.output == x.output\n            exact = (x.exp() * integrand).reduce(ops.add)\n            approx = (y.exp() * integrand).reduce(ops.add)\n\n        If ``sample_inputs`` is provided, this creates a batch of samples\n        scaled samples.\n\n        :param sampled_vars: A set of input variables to sample.\n        :type sampled_vars: str, Variable, or set or frozenset thereof.\n        :param OrderedDict sample_inputs: An optional mapping from variable\n            name to :class:`~funsor.domains.Domain` over which samples will\n            be batched.\n        :param rng_key: a PRNG state to be used by JAX backend to generate random samples\n        :type rng_key: None or JAX\'s random.PRNGKey\n        """"""\n        assert self.output == reals()\n        sampled_vars = _convert_reduced_vars(sampled_vars)\n        assert isinstance(sampled_vars, frozenset)\n        if sample_inputs is None:\n            sample_inputs = OrderedDict()\n        assert isinstance(sample_inputs, OrderedDict)\n        if sampled_vars.isdisjoint(self.inputs):\n            return self\n\n        result = interpreter.debug_logged(self.unscaled_sample)(sampled_vars, sample_inputs, rng_key)\n        if sample_inputs is not None:\n            log_scale = 0\n            for var, domain in sample_inputs.items():\n                if var in result.inputs and var not in self.inputs:\n                    log_scale -= math.log(domain.dtype)\n            if log_scale != 0:\n                result += log_scale\n        return result\n\n    def unscaled_sample(self, sampled_vars, sample_inputs, rng_key=None):\n        """"""\n        Internal method to draw an unscaled sample.\n        This should be overridden by subclasses.\n        """"""\n        assert self.output == reals()\n        assert isinstance(sampled_vars, frozenset)\n        assert isinstance(sample_inputs, OrderedDict)\n        if sampled_vars.isdisjoint(self.inputs):\n            return self\n        raise ValueError(""Cannot sample from a {}"".format(type(self).__name__))\n\n    def align(self, names):\n        """"""\n        Align this funsor to match given ``names``.\n        This is mainly useful in preparation for extracting ``.data``\n        of a :class:`funsor.tensor.Tensor`.\n\n        :param tuple names: A tuple of strings representing all names\n            but in a new order.\n        :return: A permuted funsor equivalent to self.\n        :rtype: Funsor\n        """"""\n        assert isinstance(names, tuple)\n        if not names or names == tuple(self.inputs):\n            return self\n        return Align(self, names)\n\n    def eager_subs(self, subs):\n        """"""\n        Internal substitution function. This relies on the user-facing\n        :meth:`__call__` method to coerce non-Funsors to Funsors. Once all\n        inputs are Funsors, :meth:`eager_subs` implementations can recurse to\n        call :class:`Subs`.\n        """"""\n        return None  # defer to default implementation\n\n    def eager_unary(self, op):\n        return None  # defer to default implementation\n\n    def eager_reduce(self, op, reduced_vars):\n        assert reduced_vars.issubset(self.inputs)  # FIXME Is this valid?\n        if not reduced_vars:\n            return self\n\n        return None  # defer to default implementation\n\n    def sequential_reduce(self, op, reduced_vars):\n        assert reduced_vars.issubset(self.inputs)  # FIXME Is this valid?\n        if not reduced_vars:\n            return self\n\n        # Try to sum out integer scalars. This is mainly useful for testing,\n        # since reduction is more efficiently implemented by Tensor.\n        eager_vars = []\n        lazy_vars = []\n        for k in reduced_vars:\n            if isinstance(self.inputs[k].dtype, int) and not self.inputs[k].shape:\n                eager_vars.append(k)\n            else:\n                lazy_vars.append(k)\n        if eager_vars:\n            result = None\n            for values in itertools.product(*(self.inputs[k] for k in eager_vars)):\n                subs = dict(zip(eager_vars, values))\n                result = self(**subs) if result is None else op(result, self(**subs))\n            if lazy_vars:\n                result = Reduce(op, result, frozenset(lazy_vars))\n            return result\n\n        return None  # defer to default implementation\n\n    def moment_matching_reduce(self, op, reduced_vars):\n        assert reduced_vars.issubset(self.inputs)  # FIXME Is this valid?\n        if not reduced_vars:\n            return self\n\n        return None  # defer to default implementation\n\n    # The following methods conform to a standard array/tensor interface.\n\n    def __invert__(self):\n        return Unary(ops.invert, self)\n\n    def __neg__(self):\n        return Unary(ops.neg, self)\n\n    def abs(self):\n        return Unary(ops.abs, self)\n\n    def sqrt(self):\n        return Unary(ops.sqrt, self)\n\n    def exp(self):\n        return Unary(ops.exp, self)\n\n    def log(self):\n        return Unary(ops.log, self)\n\n    def log1p(self):\n        return Unary(ops.log1p, self)\n\n    def sigmoid(self):\n        return Unary(ops.sigmoid, self)\n\n    def reshape(self, shape):\n        return Unary(ops.ReshapeOp(shape), self)\n\n    # The following reductions are treated as Unary ops because they\n    # reduce over output shape while preserving all inputs.\n    # To reduce over inputs, instead call .reduce(op, reduced_vars).\n\n    def sum(self):\n        return Unary(ops.add, self)\n\n    def prod(self):\n        return Unary(ops.mul, self)\n\n    def logsumexp(self):\n        return Unary(ops.logaddexp, self)\n\n    def all(self):\n        return Unary(ops.and_, self)\n\n    def any(self):\n        return Unary(ops.or_, self)\n\n    def min(self):\n        return Unary(ops.min, self)\n\n    def max(self):\n        return Unary(ops.max, self)\n\n    def __add__(self, other):\n        return Binary(ops.add, self, to_funsor(other))\n\n    def __radd__(self, other):\n        return Binary(ops.add, self, to_funsor(other))\n\n    def __sub__(self, other):\n        return Binary(ops.sub, self, to_funsor(other))\n\n    def __rsub__(self, other):\n        return Binary(ops.sub, to_funsor(other), self)\n\n    def __logaddexp__(self, other):\n        return Binary(ops.logaddexp, self, to_funsor(other))\n\n    def __rlogaddexp__(self, other):\n        return Binary(ops.logaddexp, to_funsor(other), self)\n\n    def __mul__(self, other):\n        return Binary(ops.mul, self, to_funsor(other))\n\n    def __rmul__(self, other):\n        return Binary(ops.mul, self, to_funsor(other))\n\n    def __truediv__(self, other):\n        return Binary(ops.truediv, self, to_funsor(other))\n\n    def __rtruediv__(self, other):\n        return Binary(ops.truediv, to_funsor(other), self)\n\n    def __matmul__(self, other):\n        return Binary(ops.matmul, self, to_funsor(other))\n\n    def __rmatmul__(self, other):\n        return Binary(ops.matmul, to_funsor(other), self)\n\n    def __pow__(self, other):\n        return Binary(ops.pow, self, to_funsor(other))\n\n    def __rpow__(self, other):\n        return Binary(ops.pow, to_funsor(other), self)\n\n    def __and__(self, other):\n        return Binary(ops.and_, self, to_funsor(other))\n\n    def __rand__(self, other):\n        return Binary(ops.and_, self, to_funsor(other))\n\n    def __or__(self, other):\n        return Binary(ops.or_, self, to_funsor(other))\n\n    def __ror__(self, other):\n        return Binary(ops.or_, self, to_funsor(other))\n\n    def __xor__(self, other):\n        return Binary(ops.xor, self, to_funsor(other))\n\n    def __eq__(self, other):\n        return Binary(ops.eq, self, to_funsor(other))\n\n    def __ne__(self, other):\n        return Binary(ops.ne, self, to_funsor(other))\n\n    def __lt__(self, other):\n        return Binary(ops.lt, self, to_funsor(other))\n\n    def __le__(self, other):\n        return Binary(ops.le, self, to_funsor(other))\n\n    def __gt__(self, other):\n        return Binary(ops.gt, self, to_funsor(other))\n\n    def __ge__(self, other):\n        return Binary(ops.ge, self, to_funsor(other))\n\n    def __min__(self, other):\n        return Binary(ops.min, self, to_funsor(other))\n\n    def __max__(self, other):\n        return Binary(ops.max, self, to_funsor(other))\n\n    def __getitem__(self, other):\n        if type(other) is not tuple:\n            other = to_funsor(other, bint(self.output.shape[0]))\n            return Binary(ops.getitem, self, other)\n\n        # Handle Ellipsis slicing.\n        if any(part is Ellipsis for part in other):\n            left = []\n            for part in other:\n                if part is Ellipsis:\n                    break\n                left.append(part)\n            right = []\n            for part in reversed(other):\n                if part is Ellipsis:\n                    break\n                right.append(part)\n            right.reverse()\n            missing = len(self.output.shape) - len(left) - len(right)\n            assert missing >= 0\n            middle = [slice(None)] * missing\n            other = tuple(left + middle + right)\n\n        # Handle each slice separately.\n        result = self\n        offset = 0\n        for part in other:\n            if isinstance(part, slice):\n                if part != slice(None):\n                    raise NotImplementedError(\'TODO support nontrivial slicing\')\n                offset += 1\n            else:\n                part = to_funsor(part, bint(result.output.shape[offset]))\n                result = Binary(GetitemOp(offset), result, part)\n        return result\n\n\n@quote.register(Funsor)\ndef _(arg, indent, out):\n    name = type(arg).__name__\n    if type(arg).__module__ in [\'funsor.torch.distributions\', \'funsor.jax.distributions\']:\n        name = \'dist.\' + name\n    out.append((indent, name + ""(""))\n    for value in arg._ast_values[:-1]:\n        quote.inplace(value, indent + 1, out)\n        i, line = out[-1]\n        out[-1] = i, line + "",""\n    for value in arg._ast_values[-1:]:\n        quote.inplace(value, indent + 1, out)\n        i, line = out[-1]\n        out[-1] = i, line + "")""\n\n\ninterpreter.recursion_reinterpret.register(Funsor)(interpreter.reinterpret_funsor)\ninterpreter.children.register(Funsor)(interpreter.children_funsor)\n\n\n@singledispatch\ndef to_funsor(x, output=None, dim_to_name=None, **kwargs):\n    """"""\n    Convert to a :class:`Funsor` .\n    Only :class:`Funsor` s and scalars are accepted.\n\n    :param x: An object.\n    :param funsor.domains.Domain output: An optional output hint.\n    :param OrderedDict dim_to_name: An optional mapping from negative batch dimensions to name strings.\n    :return: A Funsor equivalent to ``x``.\n    :rtype: Funsor\n    :raises: ValueError\n    """"""\n    raise ValueError(""Cannot convert to Funsor: {}"".format(repr(x)))\n\n\n@to_funsor.register(Funsor)\ndef funsor_to_funsor(x, output=None, dim_to_name=None):\n    if output is not None and x.output != output:\n        raise ValueError(""Output mismatch: {} vs {}"".format(x.output, output))\n    if dim_to_name is not None and list(x.inputs.keys()) != list(dim_to_name.values()):\n        raise ValueError(""Inputs mismatch: {} vs {}"".format(x.inputs, dim_to_name))\n    return x\n\n\n@singledispatch\ndef to_data(x, name_to_dim=None, **kwargs):\n    """"""\n    Extract a python object from a :class:`Funsor`.\n\n    Raises a ``ValueError`` if free variables remain or if the funsor is lazy.\n\n    :param x: An object, possibly a :class:`Funsor`.\n    :param OrderedDict name_to_dim: An optional inputs hint.\n    :return: A non-funsor equivalent to ``x``.\n    :raises: ValueError if any free variables remain.\n    :raises: PatternMissingError if funsor is not fully evaluated.\n    """"""\n    return x\n\n\n@to_data.register(Funsor)\ndef _to_data_funsor(x, name_to_dim=None):\n    if name_to_dim is None and x.inputs:\n        raise ValueError(f""cannot convert {type(x)} to data due to lazy inputs: {set(x.inputs)}"")\n    raise PatternMissingError(r""cannot convert to a non-Funsor: {repr(x)}"")\n\n\nclass Variable(Funsor):\n    """"""\n    Funsor representing a single free variable.\n\n    :param str name: A variable name.\n    :param funsor.domains.Domain output: A domain.\n    """"""\n    def __init__(self, name, output):\n        inputs = OrderedDict([(name, output)])\n        fresh = frozenset({name})\n        super(Variable, self).__init__(inputs, output, fresh)\n        self.name = name\n\n    def __repr__(self):\n        return ""Variable({}, {})"".format(repr(self.name), repr(self.output))\n\n    def __str__(self):\n        return self.name\n\n    def eager_subs(self, subs):\n        assert len(subs) == 1 and subs[0][0] == self.name\n        return subs[0][1]\n\n\n@to_funsor.register(str)\ndef name_to_funsor(name, output=None):\n    if output is None:\n        raise ValueError(f""Missing output: {name}"")\n    return Variable(name, output)\n\n\nclass SubsMeta(FunsorMeta):\n    """"""\n    Wrapper to call :func:`to_funsor` and check types.\n    """"""\n    def __call__(cls, arg, subs):\n        subs = tuple((k, to_funsor(v, arg.inputs[k]))\n                     for k, v in subs if k in arg.inputs)\n        return super().__call__(arg, subs)\n\n\nclass Subs(Funsor, metaclass=SubsMeta):\n    """"""\n    Lazy substitution of the form ``x(u=y, v=z)``.\n\n    :param Funsor arg: A funsor being substituted into.\n    :param tuple subs: A tuple of ``(name, value)`` pairs, where ``name`` is a\n        string and ``value`` can be coerced to a :class:`Funsor` via\n        :func:`to_funsor`.\n    """"""\n    def __init__(self, arg, subs):\n        assert isinstance(arg, Funsor)\n        assert isinstance(subs, tuple)\n        for key, value in subs:\n            assert isinstance(key, str)\n            assert key in arg.inputs\n            assert isinstance(value, Funsor)\n        inputs = arg.inputs.copy()\n        for key, value in subs:\n            del inputs[key]\n        for key, value in subs:\n            inputs.update(value.inputs)\n        fresh = frozenset()\n        bound = frozenset(key for key, value in subs)\n        super(Subs, self).__init__(inputs, arg.output, fresh, bound)\n        self.arg = arg\n        self.subs = OrderedDict(subs)\n\n    def __repr__(self):\n        return \'Subs({}, {})\'.format(self.arg, self.subs)\n\n    def _alpha_convert(self, alpha_subs):\n        assert self.bound.issuperset(alpha_subs)\n        alpha_subs = {k: to_funsor(v, self.subs[k].output)\n                      for k, v in alpha_subs.items()}\n        arg, subs = self._ast_values\n        arg = substitute(arg, alpha_subs)\n        subs = tuple((str(alpha_subs.get(k, k)), v) for k, v in subs)\n        return arg, subs\n\n    def unscaled_sample(self, sampled_vars, sample_inputs, rng_key=None):\n        if any(k in sample_inputs for k, v in self.subs.items()):\n            raise NotImplementedError(\'TODO alpha-convert\')\n        subs_sampled_vars = set()\n        for name in sampled_vars:\n            if name in self.arg.inputs:\n                if any(name in v.inputs for k, v in self.subs.items()):\n                    raise ValueError(""Cannot sample"")\n                subs_sampled_vars.add(name)\n            else:\n                for k, v in self.subs.items():\n                    if name in v.inputs:\n                        subs_sampled_vars.add(k)\n        subs_sampled_vars = frozenset(subs_sampled_vars)\n        arg = self.arg.unscaled_sample(subs_sampled_vars, sample_inputs, rng_key)\n        return Subs(arg, tuple(self.subs.items()))\n\n\n@lazy.register(Subs, Funsor, object)\n@eager.register(Subs, Funsor, object)\ndef eager_subs(arg, subs):\n    assert isinstance(subs, tuple)\n    if not any(k in arg.inputs for k, v in subs):\n        return arg\n    return substitute(arg, subs)\n\n\n_PREFIX = {\n    ops.neg: \'-\',\n    ops.invert: \'~\',\n}\n\n\nclass Unary(Funsor):\n    """"""\n    Lazy unary operation.\n\n    :param ~funsor.ops.Op op: A unary operator.\n    :param Funsor arg: An argument.\n    """"""\n    def __init__(self, op, arg):\n        assert callable(op)\n        assert isinstance(arg, Funsor)\n        output = find_domain(op, arg.output)\n        super(Unary, self).__init__(arg.inputs, output)\n        self.op = op\n        self.arg = arg\n\n    def __repr__(self):\n        if self.op in _PREFIX:\n            return \'{}{}\'.format(_PREFIX[self.op], self.arg)\n        return \'Unary({}, {})\'.format(self.op.__name__, self.arg)\n\n\n@eager.register(Unary, Op, Funsor)\ndef eager_unary(op, arg):\n    return interpreter.debug_logged(arg.eager_unary)(op)\n\n\n@eager.register(Unary, AssociativeOp, Funsor)\ndef eager_unary(op, arg):\n    if not arg.output.shape:\n        return arg\n    return interpreter.debug_logged(arg.eager_unary)(op)\n\n\n_INFIX = {\n    ops.add: \'+\',\n    ops.sub: \'-\',\n    ops.mul: \'*\',\n    ops.truediv: \'/\',\n    ops.pow: \'**\',\n}\n\n\nclass Binary(Funsor):\n    """"""\n    Lazy binary operation.\n\n    :param ~funsor.ops.Op op: A binary operator.\n    :param Funsor lhs: A left hand side argument.\n    :param Funsor rhs: A right hand side argument.\n    """"""\n    def __init__(self, op, lhs, rhs):\n        assert callable(op)\n        assert isinstance(lhs, Funsor)\n        assert isinstance(rhs, Funsor)\n        inputs = lhs.inputs.copy()\n        inputs.update(rhs.inputs)\n        output = find_domain(op, lhs.output, rhs.output)\n        super(Binary, self).__init__(inputs, output)\n        self.op = op\n        self.lhs = lhs\n        self.rhs = rhs\n\n    def __repr__(self):\n        if self.op in _INFIX:\n            return \'({} {} {})\'.format(self.lhs, _INFIX[self.op], self.rhs)\n        return \'Binary({}, {}, {})\'.format(self.op.__name__, self.lhs, self.rhs)\n\n\nclass Reduce(Funsor):\n    """"""\n    Lazy reduction over multiple variables.\n\n    :param ~funsor.ops.Op op: A binary operator.\n    :param funsor arg: An argument to be reduced.\n    :param frozenset reduced_vars: A set of variable names over which to reduce.\n    """"""\n    def __init__(self, op, arg, reduced_vars):\n        assert callable(op)\n        assert isinstance(arg, Funsor)\n        assert isinstance(reduced_vars, frozenset)\n        inputs = OrderedDict((k, v) for k, v in arg.inputs.items() if k not in reduced_vars)\n        output = arg.output\n        fresh = frozenset()\n        bound = reduced_vars\n        super(Reduce, self).__init__(inputs, output, fresh, bound)\n        self.op = op\n        self.arg = arg\n        self.reduced_vars = reduced_vars\n\n    def __repr__(self):\n        return \'Reduce({}, {}, {})\'.format(\n            self.op.__name__, self.arg, self.reduced_vars)\n\n    def _alpha_convert(self, alpha_subs):\n        alpha_subs = {k: to_funsor(v, self.arg.inputs[k])\n                      for k, v in alpha_subs.items()}\n        op, arg, reduced_vars = super()._alpha_convert(alpha_subs)\n        reduced_vars = frozenset(str(alpha_subs.get(k, k)) for k in reduced_vars)\n        return op, arg, reduced_vars\n\n\n@eager.register(Reduce, AssociativeOp, Funsor, frozenset)\ndef eager_reduce(op, arg, reduced_vars):\n    return interpreter.debug_logged(arg.eager_reduce)(op, reduced_vars)\n\n\n@sequential.register(Reduce, AssociativeOp, Funsor, frozenset)\ndef sequential_reduce(op, arg, reduced_vars):\n    return interpreter.debug_logged(arg.sequential_reduce)(op, reduced_vars)\n\n\n@moment_matching.register(Reduce, AssociativeOp, Funsor, frozenset)\ndef moment_matching_reduce(op, arg, reduced_vars):\n    return interpreter.debug_logged(arg.moment_matching_reduce)(op, reduced_vars)\n\n\nclass NumberMeta(FunsorMeta):\n    """"""\n    Wrapper to fill in default ``dtype``.\n    """"""\n    def __call__(cls, data, dtype=None):\n        if dtype is None:\n            dtype = ""real""\n        return super(NumberMeta, cls).__call__(data, dtype)\n\n\nclass Number(Funsor, metaclass=NumberMeta):\n    """"""\n    Funsor backed by a Python number.\n\n    :param numbers.Number data: A python number.\n    :param dtype: A nonnegative integer or the string ""real"".\n    """"""\n    def __init__(self, data, dtype=None):\n        assert isinstance(data, numbers.Number)\n        if isinstance(dtype, int):\n            data = type(dtype)(data)\n            if dtype != 2:  # booleans have bitwise interpretation\n                assert 0 <= data and data < dtype\n        else:\n            assert isinstance(dtype, str) and dtype == ""real""\n            data = float(data)\n        inputs = OrderedDict()\n        output = Domain((), dtype)\n        super(Number, self).__init__(inputs, output)\n        self.data = data\n\n    def __repr__(self):\n        if self.dtype == ""real"":\n            return \'Number({}, ""real"")\'.format(repr(self.data))\n        else:\n            return \'Number({}, {})\'.format(repr(self.data), self.dtype)\n\n    def __str__(self):\n        return str(self.data)\n\n    def __int__(self):\n        return int(self.data)\n\n    def __float__(self):\n        return float(self.data)\n\n    def __bool__(self):\n        return bool(self.data)\n\n    def item(self):\n        return self.data\n\n    def eager_unary(self, op):\n        dtype = find_domain(op, self.output).dtype\n        return Number(op(self.data), dtype)\n\n\n@to_funsor.register(numbers.Number)\ndef number_to_funsor(x, output=None, dim_to_name=None):\n    if output is None:\n        return Number(x)\n    if output.shape:\n        raise ValueError(""Cannot create Number with shape {}"".format(output.shape))\n    return Number(x, output.dtype)\n\n\n@to_data.register(Number)\ndef _to_data_number(x, name_to_dim=None):\n    return x.data\n\n\n@eager.register(Binary, Op, Number, Number)\ndef eager_binary_number_number(op, lhs, rhs):\n    data = op(lhs.data, rhs.data)\n    output = find_domain(op, lhs.output, rhs.output)\n    dtype = output.dtype\n    return Number(data, dtype)\n\n\nclass SliceMeta(FunsorMeta):\n    """"""\n    Wrapper to fill in ``start``, ``stop``, ``step``, ``dtype`` following\n    Python conventions.\n    """"""\n    def __call__(cls, name, *args, **kwargs):\n        start = 0\n        step = 1\n        dtype = None\n        if len(args) == 1:\n            stop = args[0]\n            dtype = kwargs.pop(""dtype"", stop)\n        elif len(args) == 2:\n            start, stop = args\n            dtype = kwargs.pop(""dtype"", stop)\n        elif len(args) == 3:\n            start, stop, step = args\n            dtype = kwargs.pop(""dtype"", stop)\n        elif len(args) == 4:\n            start, stop, step, dtype = args\n        else:\n            raise ValueError\n        if step <= 0:\n            raise ValueError\n        stop = min(dtype, max(start, stop))\n        return super().__call__(name, start, stop, step, dtype)\n\n\nclass Slice(Funsor, metaclass=SliceMeta):\n    """"""\n    Symbolic representation of a Python :py:class:`slice` object.\n\n    :param str name: A name for the new slice dimension.\n    :param int start:\n    :param int stop:\n    :param int step: Three args following :py:class:`slice` semantics.\n    :param int dtype: An optional bounded integer type of this slice.\n    """"""\n    def __init__(self, name, start, stop, step, dtype):\n        assert isinstance(name, str)\n        assert isinstance(start, int) and start >= 0\n        assert isinstance(stop, int) and stop >= start\n        assert isinstance(step, int) and step > 0\n        assert isinstance(dtype, int)\n        size = max(0, (stop + step - 1 - start) // step)\n        inputs = OrderedDict([(name, bint(size))])\n        output = bint(dtype)\n        fresh = frozenset({name})\n        super().__init__(inputs, output, fresh)\n        self.name = name\n        self.slice = slice(start, stop, step)\n\n    def __repr__(self):\n        return ""Slice({})"".format("", "".join(map(repr, self._ast_values)))\n\n    def eager_subs(self, subs):\n        assert len(subs) == 1 and subs[0][0] == self.name\n        index = subs[0][1]\n\n        if isinstance(index, Variable):\n            name = index.name\n            return Slice(name, self.slice.start, self.slice.stop, self.slice.step, self.dtype)\n        elif isinstance(index, Number):\n            data = self.slice.start + self.slice.step * index.data\n            return Number(data, self.output.dtype)\n        elif type(index).__name__ == ""Tensor"":  # avoid importing funsor.tensor.Tensor\n            data = self.slice.start + self.slice.step * index.data\n            return type(index)(data, index.inputs, self.output.dtype)\n        elif isinstance(index, Slice):\n            name = index.name\n            start = self.slice.start + self.slice.step * index.slice.start\n            step = self.slice.step * index.slice.step\n            return Slice(name, start, self.slice.stop, step, self.dtype)\n        else:\n            raise NotImplementedError(\'TODO support substitution of {} into Slice\'.format(type(index)))\n\n\nclass Align(Funsor):\n    """"""\n    Lazy call to ``.align(...)``.\n\n    :param Funsor arg: A funsor to align.\n    :param tuple names: A tuple of input names whose order to follow.\n    """"""\n    def __init__(self, arg, names):\n        assert isinstance(arg, Funsor)\n        assert isinstance(names, tuple)\n        assert all(isinstance(name, str) for name in names)\n        assert all(name in arg.inputs for name in names)\n        inputs = OrderedDict((name, arg.inputs[name]) for name in names)\n        inputs.update(arg.inputs)\n        output = arg.output\n        fresh = frozenset()  # TODO get this right\n        bound = frozenset()\n        super(Align, self).__init__(inputs, output, fresh, bound)\n        self.arg = arg\n\n    def align(self, names):\n        return self.arg.align(names)\n\n    def eager_unary(self, op):\n        return Unary(op, self.arg)\n\n    def eager_reduce(self, op, reduced_vars):\n        return self.arg.reduce(op, reduced_vars)\n\n\n@eager.register(Align, Funsor, tuple)\ndef eager_align(arg, names):\n    if not frozenset(names) == frozenset(arg.inputs.keys()):\n        # assume there\'s been a substitution and this align is no longer valid\n        return arg\n    return None\n\n\n@eager.register(Binary, Op, Align, Funsor)\ndef eager_binary_align_funsor(op, lhs, rhs):\n    return Binary(op, lhs.arg, rhs)\n\n\n@eager.register(Binary, Op, Funsor, Align)\ndef eager_binary_funsor_align(op, lhs, rhs):\n    return Binary(op, lhs, rhs.arg)\n\n\n@eager.register(Binary, Op, Align, Align)\ndef eager_binary_align_align(op, lhs, rhs):\n    return Binary(op, lhs.arg, rhs.arg)\n\n\nclass Stack(Funsor):\n    """"""\n    Stack of funsors along a new input dimension.\n\n    :param str name: The name of the new input variable along which to stack.\n    :param tuple parts: A tuple of Funsors of homogenous output domain.\n    """"""\n    def __init__(self, name, parts):\n        assert isinstance(name, str)\n        assert isinstance(parts, tuple)\n        assert parts\n        assert not any(name in x.inputs for x in parts)\n        assert len(set(x.output for x in parts)) == 1\n        output = parts[0].output\n        domain = bint(len(parts))\n        inputs = OrderedDict([(name, domain)])\n        for x in parts:\n            inputs.update(x.inputs)\n        fresh = frozenset({name})\n        super().__init__(inputs, output, fresh)\n        self.name = name\n        self.parts = parts\n\n    def eager_subs(self, subs):\n        assert isinstance(subs, tuple) and len(subs) == 1 and subs[0][0] == self.name\n        index = subs[0][1]\n\n        # Try to eagerly select an index.\n        assert index.output == bint(len(self.parts))\n\n        if isinstance(index, Number):\n            # Select a single part.\n            return self.parts[index.data]\n        elif isinstance(index, Variable):\n            # Rename the stacking dimension.\n            parts = self.parts\n            return Stack(index.name, parts)\n        elif isinstance(index, Slice):\n            parts = self.parts[index.slice]\n            return Stack(index.name, parts)\n        else:\n            raise NotImplementedError(\'TODO support advanced indexing in Stack\')\n\n    def eager_reduce(self, op, reduced_vars):\n        parts = self.parts\n        if self.name in reduced_vars:\n            reduced_vars -= frozenset([self.name])\n            if reduced_vars:\n                parts = tuple(x.reduce(op, reduced_vars) for x in parts)\n            return reduce(op, parts)\n        parts = tuple(x.reduce(op, reduced_vars) for x in parts)\n        return Stack(self.name, parts)\n\n\n@eager.register(Stack, str, tuple)\ndef eager_stack(name, parts):\n    return eager_stack_homogeneous(name, *parts)\n\n\n@dispatch(str, Variadic[Funsor])\ndef eager_stack_homogeneous(name, *parts):\n    return None  # defer to default implementation\n\n\nclass CatMeta(FunsorMeta):\n    """"""\n    Wrapper to fill in default value for ``part_name``.\n    """"""\n    def __call__(cls, name, parts, part_name=None):\n        if part_name is None:\n            part_name = name\n        return super().__call__(name, parts, part_name)\n\n\nclass Cat(Funsor, metaclass=CatMeta):\n    """"""\n    Concatenate funsors along an existing input dimension.\n\n    :param str name: The name of the input variable along which to concatenate.\n    :param tuple parts: A tuple of Funsors of homogenous output domain.\n    """"""\n    def __init__(self, name, parts, part_name=None):\n        assert isinstance(name, str)\n        assert isinstance(parts, tuple)\n        assert isinstance(part_name, str)\n        assert parts\n        assert all(part_name in x.inputs for x in parts)\n        if part_name != name:\n            assert not any(name in x.inputs for x in parts)\n        assert len(set(x.output for x in parts)) == 1\n        output = parts[0].output\n        inputs = OrderedDict()\n        for x in parts:\n            inputs.update(x.inputs)\n        del inputs[part_name]\n        inputs[name] = bint(sum(x.inputs[part_name].size for x in parts))\n        fresh = frozenset({name})\n        bound = frozenset({part_name})\n        super().__init__(inputs, output, fresh, bound)\n        self.name = name\n        self.parts = parts\n        self.part_name = part_name\n\n    def _alpha_convert(self, alpha_subs):\n        assert len(alpha_subs) == 1\n        part_name = alpha_subs[self.part_name]\n        parts = tuple(\n            substitute(p, {self.part_name:\n                           to_funsor(part_name, p.inputs[self.part_name])})\n            for p in self.parts)\n        return self.name, parts, part_name\n\n    def eager_subs(self, subs):\n        assert len(subs) == 1 and subs[0][0] == self.name\n        value = subs[0][1]\n\n        if isinstance(value, Variable):\n            return Cat(value.name, self.parts, self.part_name)\n        elif isinstance(value, Number):\n            n = value.data\n            for part in self.parts:\n                size = part.inputs[self.part_name].size\n                if n < size:\n                    return part(**{self.part_name: n})\n                n -= size\n            assert False\n        elif isinstance(value, Slice):\n            start, stop, step = \\\n                value.slice.start, value.slice.stop, value.slice.step\n            new_parts = []\n            pos = 0\n            for part in self.parts:\n                psize = part.inputs[self.part_name].size\n                if step > 1:\n                    pstart = ((pos - start) // step) * step - (pos - start)\n                    pstart = pstart + step if pstart < 0 else pstart\n                else:\n                    pstart = max(start - pos, 0)\n                pstop = min(pos + psize, stop) - pos\n\n                if not (pstart >= pstop or pos >= stop or pos + psize <= start):\n                    pslice = Slice(self.part_name, pstart, pstop, step, psize)\n                    part = part(**{self.part_name: pslice})\n                    new_parts.append(part)\n\n                pos += psize\n\n            return Cat(self.name, tuple(new_parts), self.part_name)\n        else:\n            raise NotImplementedError(""TODO implement Cat.eager_subs for {}""\n                                      .format(type(value)))\n\n\n@eager.register(Cat, str, tuple, str)\ndef eager_cat(name, parts, part_name):\n    if len(parts) == 1:\n        return parts[0](**{part_name: name})\n    return eager_cat_homogeneous(name, part_name, *parts)\n\n\n@dispatch(str, str, Variadic[Funsor])\ndef eager_cat_homogeneous(name, part_name, *parts):\n    return None  # defer to default implementation\n\n\nclass Lambda(Funsor):\n    """"""\n    Lazy inverse to ``ops.getitem``.\n\n    This is useful to simulate higher-order functions of integers\n    by representing those functions as arrays.\n\n    :param Variable var: A variable to bind.\n    :param funsor expr: A funsor.\n    """"""\n    def __init__(self, var, expr):\n        assert isinstance(var, Variable)\n        assert isinstance(var.dtype, int)\n        assert isinstance(expr, Funsor)\n        inputs = expr.inputs.copy()\n        inputs.pop(var.name, None)\n        shape = (var.dtype,) + expr.output.shape\n        output = Domain(shape, expr.dtype)\n        fresh = frozenset()\n        bound = frozenset({var.name})\n        super(Lambda, self).__init__(inputs, output, fresh, bound)\n        self.var = var\n        self.expr = expr\n\n    def _alpha_convert(self, alpha_subs):\n        alpha_subs = {k: to_funsor(v, self.var.inputs[k])\n                      for k, v in alpha_subs.items()}\n        return super()._alpha_convert(alpha_subs)\n\n\n@eager.register(Binary, GetitemOp, Lambda, (Funsor, Align))\ndef eager_getitem_lambda(op, lhs, rhs):\n    if op.offset == 0:\n        return Subs(lhs.expr, ((lhs.var.name, rhs),))\n    expr = GetitemOp(op.offset - 1)(lhs.expr, rhs)\n    return Lambda(lhs.var, expr)\n\n\nclass Independent(Funsor):\n    """"""\n    Creates an independent diagonal distribution.\n\n    This is equivalent to substitution followed by reduction::\n\n        f = ...  # a batched distribution\n        assert f.inputs[\'x_i\'] == reals(4, 5)\n        assert f.inputs[\'i\'] == bint(3)\n\n        g = Independent(f, \'x\', \'i\', \'x_i\')\n        assert g.inputs[\'x\'] == reals(3, 4, 5)\n        assert \'x_i\' not in g.inputs\n        assert \'i\' not in g.inputs\n\n        x = Variable(\'x\', reals(3, 4, 5))\n        g == f(x_i=x[\'i\']).reduce(ops.logaddexp, \'i\')\n\n    :param Funsor fn: A funsor.\n    :param str reals_var: The name of a real-tensor input.\n    :param str bint_var: The name of a new batch input of ``fn``.\n    :param diag_var: The name of a smaller-shape real input of ``fn``.\n    """"""\n    def __init__(self, fn, reals_var, bint_var, diag_var):\n        assert isinstance(fn, Funsor)\n        assert isinstance(reals_var, str)\n        assert isinstance(bint_var, str)\n        assert bint_var in fn.inputs\n        assert isinstance(fn.inputs[bint_var].dtype, int)\n        assert isinstance(diag_var, str)\n        assert diag_var in fn.inputs\n        assert fn.inputs[diag_var].dtype == \'real\'\n        inputs = fn.inputs.copy()\n        shape = (inputs.pop(bint_var).dtype,) + inputs.pop(diag_var).shape\n        assert reals_var not in inputs\n        inputs[reals_var] = reals(*shape)\n        fresh = frozenset({reals_var})\n        bound = frozenset({bint_var, diag_var})\n        super(Independent, self).__init__(inputs, fn.output, fresh, bound)\n        self.fn = fn\n        self.reals_var = reals_var\n        self.bint_var = bint_var\n        self.diag_var = diag_var\n\n    def _alpha_convert(self, alpha_subs):\n        alpha_subs = {k: to_funsor(v, self.fn.inputs[k])\n                      for k, v in alpha_subs.items()}\n        fn, reals_var, bint_var, diag_var = super()._alpha_convert(alpha_subs)\n        bint_var = str(alpha_subs.get(bint_var, bint_var))\n        diag_var = str(alpha_subs.get(diag_var, diag_var))\n        return fn, reals_var, bint_var, diag_var\n\n    def unscaled_sample(self, sampled_vars, sample_inputs, rng_key=None):\n        if self.bint_var in sampled_vars or self.bint_var in sample_inputs:\n            raise NotImplementedError(\'TODO alpha-convert\')\n        sampled_vars = frozenset(self.diag_var if v == self.reals_var else v\n                                 for v in sampled_vars)\n        fn = self.fn.unscaled_sample(sampled_vars, sample_inputs, rng_key)\n        return Independent(fn, self.reals_var, self.bint_var, self.diag_var)\n\n    def eager_subs(self, subs):\n        assert len(subs) == 1 and subs[0][0] == self.reals_var\n        value = subs[0][1]\n\n        # Handle simple renaming to preserve Independent.\n        if isinstance(value, Variable):\n            return Independent(self.fn, value.name, self.bint_var, self.diag_var)\n\n        # Otherwise convert to a Reduce.\n        result = Subs(self.fn, ((self.diag_var, value[self.bint_var]),))\n        result = result.reduce(ops.add, self.bint_var)\n        return result\n\n\n@eager.register(Independent, Funsor, str, str, str)\ndef eager_independent_trivial(fn, reals_var, bint_var, diag_var):\n    # compare to Independent.eager_subs\n    if diag_var not in fn.inputs:\n        return fn.reduce(ops.add, bint_var)\n    return None\n\n\ndef _of_shape(fn, shape):\n    args, vargs, kwargs, defaults = getargspec(fn)\n    assert not vargs\n    assert not kwargs\n    names = tuple(args)\n    args = [Variable(name, size) for name, size in zip(names, shape)]\n    return to_funsor(fn(*args)).align(names)\n\n\ndef of_shape(*shape):\n    """"""\n    Decorator to construct a :class:`Funsor` with one free :class:`Variable`\n    per function arg.\n    """"""\n    return functools.partial(_of_shape, shape=shape)\n\n\n################################################################################\n# Register Ops\n################################################################################\n\n\n@quote.register(Variable)\n@quote.register(Number)\n@quote.register(Slice)\ndef quote_inplace_oneline(arg, indent, out):\n    out.append((indent, repr(arg)))\n\n\n@quote.register(Unary)\n@quote.register(Binary)\n@quote.register(Reduce)\n@quote.register(Stack)\n@quote.register(Cat)\n@quote.register(Lambda)\ndef quote_inplace_first_arg_on_first_line(arg, indent, out):\n    line = f""{type(arg).__name__}({repr(arg._ast_values[0])},""\n    out.append((indent, line))\n    for value in arg._ast_values[1:-1]:\n        quote.inplace(value, indent + 1, out)\n        i, line = out[-1]\n        out[-1] = i, line + "",""\n    for value in arg._ast_values[-1:]:\n        quote.inplace(value, indent + 1, out)\n        i, line = out[-1]\n        out[-1] = i, line + "")""\n\n\n@ops.abs.register(Funsor)\ndef _abs(x):\n    return Unary(ops.abs, x)\n\n\n@ops.sqrt.register(Funsor)\ndef _sqrt(x):\n    return Unary(ops.sqrt, x)\n\n\n@ops.exp.register(Funsor)\ndef _exp(x):\n    return Unary(ops.exp, x)\n\n\n@ops.log.register(Funsor)\ndef _log(x):\n    return Unary(ops.log, x)\n\n\n@ops.log1p.register(Funsor)\ndef _log1p(x):\n    return Unary(ops.log1p, x)\n\n\n@ops.reciprocal.register(Funsor)\ndef _reciprocal(x):\n    return Unary(ops.reciprocal, x)\n\n\n@ops.sigmoid.register(Funsor)\ndef _sigmoid(x):\n    return Unary(ops.sigmoid, x)\n\n\n__all__ = [\n    \'Binary\',\n    \'Cat\',\n    \'Funsor\',\n    \'Independent\',\n    \'Lambda\',\n    \'Number\',\n    \'Reduce\',\n    \'Stack\',\n    \'Slice\',\n    \'Subs\',\n    \'Unary\',\n    \'Variable\',\n    \'eager\',\n    \'eager_or_die\',\n    \'lazy\',\n    \'moment_matching\',\n    \'of_shape\',\n    \'reflect\',\n    \'sequential\',\n    \'to_data\',\n    \'to_funsor\',\n]\n'"
funsor/testing.py,15,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport contextlib\nimport itertools\nimport numbers\nimport operator\nfrom collections import OrderedDict, namedtuple\nfrom functools import reduce\n\nimport numpy as np\nimport opt_einsum\nimport pytest\nfrom multipledispatch import dispatch\nfrom multipledispatch.variadic import Variadic\n\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction\nfrom funsor.delta import Delta\nfrom funsor.domains import Domain, bint, reals\nfrom funsor.gaussian import Gaussian\nfrom funsor.terms import Funsor, Number\nfrom funsor.tensor import Tensor\nfrom funsor.util import get_backend\n\n\n@contextlib.contextmanager\ndef xfail_if_not_implemented(msg=""Not implemented""):\n    try:\n        yield\n    except NotImplementedError as e:\n        pytest.xfail(reason=\'{}:\\n{}\'.format(msg, e))\n\n\nclass ActualExpected(namedtuple(\'LazyComparison\', [\'actual\', \'expected\'])):\n    """"""\n    Lazy string formatter for test assertions.\n    """"""\n    def __repr__(self):\n        return \'\\n\'.join([\'Expected:\', str(self.expected), \'Actual:\', str(self.actual)])\n\n\ndef id_from_inputs(inputs):\n    if isinstance(inputs, (dict, OrderedDict)):\n        inputs = inputs.items()\n    if not inputs:\n        return \'()\'\n    return \',\'.join(k + \'\'.join(map(str, d.shape)) for k, d in inputs)\n\n\n@dispatch(object, object, Variadic[float])\ndef allclose(a, b, rtol=1e-05, atol=1e-08):\n    if type(a) != type(b):\n        return False\n    return ops.abs(a - b) < rtol + atol * ops.abs(b)\n\n\ndispatch(np.ndarray, np.ndarray, Variadic[float])(np.allclose)\n\n\n@dispatch(Tensor, Tensor, Variadic[float])\ndef allclose(a, b, rtol=1e-05, atol=1e-08):\n    if a.inputs != b.inputs or a.output != b.output:\n        return False\n    return allclose(a.data, b.data, rtol=rtol, atol=atol)\n\n\ndef is_array(x):\n    return isinstance(x, (np.ndarray, np.generic)) or type(x).__name__ == ""DeviceArray""\n\n\ndef assert_close(actual, expected, atol=1e-6, rtol=1e-6):\n    msg = ActualExpected(actual, expected)\n    if is_array(actual):\n        assert is_array(expected), msg\n    elif isinstance(actual, Tensor) and is_array(actual.data):\n        assert isinstance(expected, Tensor) and is_array(expected.data)\n    elif isinstance(actual, Contraction) and isinstance(actual.terms[0], Tensor) \\\n            and is_array(actual.terms[0].data):\n        assert isinstance(expected, Contraction) and is_array(expected.terms[0].data)\n    elif isinstance(actual, Gaussian) and is_array(actual.info_vec):\n        assert isinstance(expected, Gaussian) and is_array(expected.info_vec)\n    else:\n        assert type(actual) == type(expected), msg\n\n    if isinstance(actual, Funsor):\n        assert isinstance(actual, Funsor)\n        assert isinstance(expected, Funsor)\n        assert actual.inputs == expected.inputs, (actual.inputs, expected.inputs)\n        assert actual.output == expected.output, (actual.output, expected.output)\n\n    if isinstance(actual, (Number, Tensor)):\n        assert_close(actual.data, expected.data, atol=atol, rtol=rtol)\n    elif isinstance(actual, Delta):\n        assert frozenset(n for n, p in actual.terms) == frozenset(n for n, p in expected.terms)\n        actual = actual.align(tuple(n for n, p in expected.terms))\n        for (actual_name, (actual_point, actual_log_density)), \\\n                (expected_name, (expected_point, expected_log_density)) in \\\n                zip(actual.terms, expected.terms):\n            assert actual_name == expected_name\n            assert_close(actual_point, expected_point, atol=atol, rtol=rtol)\n            assert_close(actual_log_density, expected_log_density, atol=atol, rtol=rtol)\n    elif isinstance(actual, Gaussian):\n        assert_close(actual.info_vec, expected.info_vec, atol=atol, rtol=rtol)\n        assert_close(actual.precision, expected.precision, atol=atol, rtol=rtol)\n    elif isinstance(actual, Contraction):\n        assert actual.red_op == expected.red_op\n        assert actual.bin_op == expected.bin_op\n        assert actual.reduced_vars == expected.reduced_vars\n        assert len(actual.terms) == len(expected.terms)\n        for ta, te in zip(actual.terms, expected.terms):\n            assert_close(ta, te, atol, rtol)\n    elif type(actual).__name__ == ""Tensor"":\n        assert get_backend() == ""torch""\n        import torch\n\n        assert actual.dtype == expected.dtype, msg\n        assert actual.shape == expected.shape, msg\n        if actual.dtype in (torch.long, torch.uint8, torch.bool):\n            assert (actual == expected).all(), msg\n        else:\n            eq = (actual == expected)\n            if eq.all():\n                return\n            if eq.any():\n                actual = actual[~eq]\n                expected = expected[~eq]\n            diff = (actual.detach() - expected.detach()).abs()\n            if rtol is not None:\n                assert (diff / (atol + expected.detach().abs())).max() < rtol, msg\n            elif atol is not None:\n                assert diff.max() < atol, msg\n    elif is_array(actual):\n        if isinstance(actual, (np.ndarray, np.generic)):\n            assert actual.dtype == expected.dtype, msg\n        else:\n            assert get_backend() == ""jax""\n            import jax\n\n            assert actual.dtype == jax.dtypes.canonicalize_dtype(expected.dtype), msg\n\n        assert actual.shape == expected.shape, msg\n        if actual.dtype in (np.int32, np.int64, np.uint8, np.bool):\n            assert (actual == expected).all(), msg\n        else:\n            actual, expected = np.asarray(actual), np.asarray(expected)\n            eq = (actual == expected)\n            if eq.all():\n                return\n            if eq.any():\n                actual = actual[~eq]\n                expected = expected[~eq]\n            diff = abs(actual - expected)\n            if rtol is not None:\n                assert (diff / (atol + abs(expected))).max() < rtol, msg\n            elif atol is not None:\n                assert diff.max() < atol, msg\n    elif isinstance(actual, numbers.Number):\n        diff = abs(actual - expected)\n        if rtol is not None:\n            assert diff < (atol + abs(expected)) * rtol, msg\n        elif atol is not None:\n            assert diff < atol, msg\n    else:\n        raise ValueError(\'cannot compare objects of type {}\'.format(type(actual)))\n\n\ndef check_funsor(x, inputs, output, data=None):\n    """"""\n    Check dims and shape modulo reordering.\n    """"""\n    assert isinstance(x, Funsor)\n    assert dict(x.inputs) == dict(inputs)\n    if output is not None:\n        assert x.output == output\n    if data is not None:\n        if x.inputs == inputs:\n            x_data = x.data\n        else:\n            x_data = x.align(tuple(inputs)).data\n        if inputs or output.shape:\n            assert (x_data == data).all()\n        else:\n            assert x_data == data\n\n\ndef xfail_param(*args, **kwargs):\n    return pytest.param(*args, marks=[pytest.mark.xfail(**kwargs)])\n\n\ndef make_einsum_example(equation, fill=None, sizes=(2, 3)):\n    symbols = sorted(set(equation) - set(\',->\'))\n    sizes = {dim: size for dim, size in zip(symbols, itertools.cycle(sizes))}\n    inputs, outputs = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n    outputs = outputs.split(\',\')\n    operands = []\n    for dims in inputs:\n        shape = tuple(sizes[dim] for dim in dims)\n        x = randn(shape)\n        operand = x if fill is None else (x - x + fill)\n        # no need to use pyro_dims for numpy backend\n        if not isinstance(operand, np.ndarray):\n            operand._pyro_dims = dims\n        operands.append(operand)\n    funsor_operands = [\n        Tensor(operand, OrderedDict([(d, bint(sizes[d])) for d in inp]))\n        for inp, operand in zip(inputs, operands)\n    ]\n\n    assert equation == \\\n        "","".join(["""".join(operand.inputs.keys()) for operand in funsor_operands]) + ""->"" + "","".join(outputs)\n    return inputs, outputs, sizes, operands, funsor_operands\n\n\ndef assert_equiv(x, y):\n    """"""\n    Check that two funsors are equivalent up to permutation of inputs.\n    """"""\n    check_funsor(x, y.inputs, y.output, y.data)\n\n\ndef rand(*args):\n    if isinstance(args[0], tuple):\n        assert len(args) == 1\n        shape = args[0]\n    else:\n        shape = args\n\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        return torch.rand(shape)\n    else:\n        # work around numpy random returns float object instead of np.ndarray object when shape == ()\n        return np.array(np.random.rand(*shape))\n\n\ndef randint(low, high, size):\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        return torch.randint(low, high, size=size)\n    else:\n        return np.random.randint(low, high, size=size)\n\n\ndef randn(*args):\n    if isinstance(args[0], tuple):\n        assert len(args) == 1\n        shape = args[0]\n    else:\n        shape = args\n\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        return torch.randn(shape)\n    else:\n        # work around numpy random returns float object instead of np.ndarray object when shape == ()\n        return np.array(np.random.randn(*shape))\n\n\ndef zeros(*args):\n    if isinstance(args[0], tuple):\n        assert len(args) == 1\n        shape = args[0]\n    else:\n        shape = args\n\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        return torch.zeros(shape)\n    else:\n        return np.zeros(shape)\n\n\ndef ones(*args):\n    if isinstance(args[0], tuple):\n        assert len(args) == 1\n        shape = args[0]\n    else:\n        shape = args\n\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        return torch.ones(shape)\n    else:\n        return np.ones(shape)\n\n\ndef empty(*args):\n    if isinstance(args[0], tuple):\n        assert len(args) == 1\n        shape = args[0]\n    else:\n        shape = args\n\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        return torch.empty(shape)\n    else:\n        return np.empty(shape)\n\n\ndef random_tensor(inputs, output=reals()):\n    """"""\n    Creates a random :class:`funsor.tensor.Tensor` with given inputs and output.\n    """"""\n    backend = get_backend()\n    assert isinstance(inputs, OrderedDict)\n    assert isinstance(output, Domain)\n    shape = tuple(d.dtype for d in inputs.values()) + output.shape\n    if output.dtype == \'real\':\n        data = randn(shape)\n    else:\n        num_elements = reduce(operator.mul, shape, 1)\n        if backend == ""torch"":\n            import torch\n\n            data = torch.multinomial(torch.ones(output.dtype), num_elements, replacement=True)\n        else:\n            data = np.random.choice(output.dtype, num_elements, replace=True)\n        data = data.reshape(shape)\n    return Tensor(data, inputs, output.dtype)\n\n\ndef random_gaussian(inputs):\n    """"""\n    Creates a random :class:`funsor.gaussian.Gaussian` with given inputs.\n    """"""\n    assert isinstance(inputs, OrderedDict)\n    batch_shape = tuple(d.dtype for d in inputs.values() if d.dtype != \'real\')\n    event_shape = (sum(d.num_elements for d in inputs.values() if d.dtype == \'real\'),)\n    prec_sqrt = randn(batch_shape + event_shape + event_shape)\n    precision = ops.matmul(prec_sqrt, ops.transpose(prec_sqrt, -1, -2))\n    precision = precision + 0.5 * ops.new_eye(precision, event_shape[:1])\n    loc = randn(batch_shape + event_shape)\n    info_vec = ops.matmul(precision, ops.unsqueeze(loc, -1)).squeeze(-1)\n    return Gaussian(info_vec, precision, inputs)\n\n\ndef random_mvn(batch_shape, dim, diag=False):\n    """"""\n    Generate a random :class:`torch.distributions.MultivariateNormal` with given shape.\n    """"""\n    backend = get_backend()\n    rank = dim + dim\n    loc = randn(batch_shape + (dim,))\n    cov = randn(batch_shape + (dim, rank))\n    cov = cov @ ops.transpose(cov, -1, -2)\n    if diag:\n        cov = cov * ops.new_eye(cov, (dim,))\n    if backend == ""torch"":\n        import pyro\n\n        return pyro.distributions.MultivariateNormal(loc, cov)\n    elif backend == ""jax"":\n        import numpyro\n\n        return numpyro.distributions.MultivariateNormal(loc, cov)\n\n\ndef make_plated_hmm_einsum(num_steps, num_obs_plates=1, num_hidden_plates=0):\n\n    assert num_obs_plates >= num_hidden_plates\n    t0 = num_obs_plates + 1\n\n    obs_plates = \'\'.join(opt_einsum.get_symbol(i) for i in range(num_obs_plates))\n    hidden_plates = \'\'.join(opt_einsum.get_symbol(i) for i in range(num_hidden_plates))\n\n    inputs = [str(opt_einsum.get_symbol(t0))]\n    for t in range(t0, num_steps+t0):\n        inputs.append(str(opt_einsum.get_symbol(t)) + str(opt_einsum.get_symbol(t+1)) + hidden_plates)\n        inputs.append(str(opt_einsum.get_symbol(t+1)) + obs_plates)\n    equation = "","".join(inputs) + ""->""\n    return (equation, \'\'.join(sorted(tuple(set(obs_plates + hidden_plates)))))\n\n\ndef make_chain_einsum(num_steps):\n    inputs = [str(opt_einsum.get_symbol(0))]\n    for t in range(num_steps):\n        inputs.append(str(opt_einsum.get_symbol(t)) + str(opt_einsum.get_symbol(t+1)))\n    equation = "","".join(inputs) + ""->""\n    return equation\n\n\ndef make_hmm_einsum(num_steps):\n    inputs = [str(opt_einsum.get_symbol(0))]\n    for t in range(num_steps):\n        inputs.append(str(opt_einsum.get_symbol(t)) + str(opt_einsum.get_symbol(t+1)))\n        inputs.append(str(opt_einsum.get_symbol(t+1)))\n    equation = "","".join(inputs) + ""->""\n    return equation\n'"
funsor/util.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport inspect\nimport re\nimport os\n\nimport numpy as np\n\n_FUNSOR_BACKEND = os.environ.get(""FUNSOR_BACKEND"", ""numpy"")\n_JAX_LOADED = True if _FUNSOR_BACKEND == ""jax"" else False\n\n\nclass lazy_property(object):\n    def __init__(self, fn):\n        self.fn = fn\n        functools.update_wrapper(self, fn)\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self\n        value = self.fn(obj)\n        setattr(obj, self.fn.__name__, value)\n        return value\n\n\ndef getargspec(fn):\n    """"""\n    Similar to Python 2\'s :py:func:`inspect.getargspec` but:\n    - In Python 3 uses ``getfullargspec`` to avoid ``DeprecationWarning``.\n    - For builtin functions like ``torch.matmul`` or ``numpy.matmul``, falls back to\n      attempting to parse the function docstring, assuming torch-style or numpy-style.\n    """"""\n    assert callable(fn)\n    try:\n        args, vargs, kwargs, defaults, _, _, _ = inspect.getfullargspec(fn)\n    except TypeError:\n        # Fall back to attempting to parse a PyTorch/NumPy-style docstring.\n        match = re.match(r""\\s*{}\\(([^)]*)\\)"".format(fn.__name__), fn.__doc__)\n        if match is None:\n            raise\n        parts = re.sub(r""[[\\]]"", """", match.group(1)).split("", "")\n        args = [a.split(""="")[0] for a in parts if a not in [""/"", ""*""]]\n        if not all(re.match(r""^[^\\d\\W]\\w*\\Z"", arg) for arg in args):\n            raise\n        vargs = None\n        kwargs = None\n        defaults = ()  # Ignore defaults.\n    return args, vargs, kwargs, defaults\n\n\ndef quote(arg):\n    """"""\n    Serialize an object to text that can be parsed by Python.\n\n    This is useful to save intermediate funsors to add to tests.\n    """"""\n    out = []\n    _quote_inplace(arg, 0, out)\n    lines = []\n    for indent, line in out:\n        if indent + len(line) >= 80:\n            line += ""  # noqa""\n        lines.append(\' \' * indent + line)\n    return \'\\n\'.join(lines)\n\n\ndef pretty(arg, maxlen=40):\n    """"""\n    Pretty print an expression. This is useful for debugging.\n    """"""\n    out = []\n    _quote_inplace(arg, 0, out)\n    fill = u\'   \\u2502\' * 100\n    lines = []\n    for indent, line in out:\n        if len(line) > maxlen:\n            line = line[:maxlen] + ""...""\n        lines.append(fill[:indent] + line)\n    return \'\\n\'.join(lines)\n\n\n@functools.singledispatch\ndef _quote_inplace(arg, indent, out):\n    line = re.sub(\'\\n\\\\s*\', \' \', repr(arg))\n    out.append((indent, line))\n\n\nquote.inplace = _quote_inplace\nquote.register = _quote_inplace.register\n\n\n@quote.register(tuple)\ndef _(arg, indent, out):\n    if not arg:\n        out.append((indent, ""()""))\n        return\n    for value in arg[:1]:\n        temp = []\n        quote.inplace(value, indent + 1, temp)\n        i, line = temp[0]\n        temp[0] = i - 1, ""("" + line\n        out.extend(temp)\n        i, line = out[-1]\n        out[-1] = i, line + \',\'\n    for value in arg[1:]:\n        quote.inplace(value, indent + 1, out)\n        i, line = out[-1]\n        out[-1] = i, line + \',\'\n    i, line = out[-1]\n    out[-1] = i, line + \')\'\n\n\n@quote.register(np.ndarray)\ndef _quote(arg, indent, out):\n    """"""\n    Work around NumPy ndarray not supporting reproducible repr.\n    """"""\n    out.append((indent, f""np.array({repr(arg.tolist())}, dtype=np.{arg.dtype})""))\n\n\ndef broadcast_shape(*shapes, **kwargs):\n    """"""\n    Similar to ``np.broadcast()`` but for shapes.\n    Equivalent to ``np.broadcast(*map(np.empty, shapes)).shape``.\n    :param tuple shapes: shapes of tensors.\n    :param bool strict: whether to use extend-but-not-resize broadcasting.\n    :returns: broadcasted shape\n    :rtype: tuple\n    :raises: ValueError\n    """"""\n    strict = kwargs.pop(\'strict\', False)\n    reversed_shape = []\n    for shape in shapes:\n        for i, size in enumerate(reversed(shape)):\n            if i >= len(reversed_shape):\n                reversed_shape.append(size)\n            elif reversed_shape[i] == 1 and not strict:\n                reversed_shape[i] = size\n            elif reversed_shape[i] != size and (size != 1 or strict):\n                raise ValueError(\'shape mismatch: objects cannot be broadcast to a single shape: {}\'.format(\n                    \' vs \'.join(map(str, shapes))))\n    return tuple(reversed(reversed_shape))\n\n\ndef set_backend(backend):\n    """"""\n    Set backend for Funsor. Currently, only three backends are supported:\n    ""numpy"", ""torch"", and ""jax"". And Funsor only runs with one backend\n    at a time.\n\n    The default backend will be ""numpy"". We can change the default backend\n    by specifying a new one in the environment variable `FUNSOR_BACKEND`,\n    e.g. `FUNSOR_BACKEND=torch`.\n\n    .. note: When `jax` backend is set, we cannot revert back to the default\n    `numpy` backend because we dispatch to using `jax.numpy` all ops with\n    `numpy.ndarray` or `numpy.generic` inputs.\n\n    :param str backend: either ""numpy"", ""torch"", or ""jax"".\n    """"""\n    global _FUNSOR_BACKEND, _JAX_LOADED\n\n    if backend == ""numpy"":\n        if _JAX_LOADED:\n            raise ValueError(""Cannot revert back to NumPy backend when JAX backend has been set."")\n        else:\n            _FUNSOR_BACKEND = ""numpy""\n    elif backend == ""torch"":\n        _FUNSOR_BACKEND = ""torch""\n\n        import torch  # noqa: F401\n        import funsor.torch  # noqa: F401\n    elif backend == ""jax"":\n        _FUNSOR_BACKEND = ""jax""\n        _JAX_LOADED = True\n\n        import jax  # noqa: F401\n        import funsor.jax  # noqa: F401\n    else:\n        raise ValueError(""backend should be either \'numpy\', \'torch\', or \'jax\'""\n                         "", got {}"".format(backend))\n\n\ndef get_backend():\n    """"""\n    Get the current backend of Funsor.\n\n    :return: either ""numpy"", ""torch"", or ""jax"".\n    :rtype: str\n    """"""\n    return _FUNSOR_BACKEND\n\n\ndef get_tracing_state():\n    if _FUNSOR_BACKEND == ""torch"":\n        import torch\n\n        return torch._C._get_tracing_state()\n    else:\n        return None\n\n\ndef is_nn_module(x):\n    if _FUNSOR_BACKEND == ""torch"":\n        import torch\n\n        return isinstance(x, torch.nn.Module)\n    return False\n'"
scripts/update_headers.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport glob\n\nroot = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nblacklist = [""/build/"", ""/dist/""]\nfile_types = [\n    (""*.py"", ""# {}""),\n    (""*.cpp"", ""// {}""),\n]\n\nfor basename, comment in file_types:\n    copyright_line = comment.format(""Copyright Contributors to the Pyro project.\\n"")\n    # See https://spdx.org/ids-how\n    spdx_line = comment.format(""SPDX-License-Identifier: Apache-2.0\\n"")\n\n    filenames = glob.glob(os.path.join(root, ""**"", basename), recursive=True)\n    filenames.sort()\n    filenames = [\n        filename\n        for filename in filenames\n        if not any(word in filename for word in blacklist)\n    ]\n    for filename in filenames:\n        with open(filename) as f:\n            lines = f.readlines()\n\n        # Ignore empty files like __init__.py\n        if all(line.isspace() for line in lines):\n            continue\n\n        # Ensure first few line are copyright notices.\n        changed = False\n        lineno = 0\n        if not lines[lineno].startswith(comment.format(""Copyright"")):\n            lines.insert(lineno, copyright_line)\n            changed = True\n        lineno += 1\n        while lines[lineno].startswith(comment.format(""Copyright"")):\n            lineno += 1\n            changed = True\n\n        # Ensure next line is an SPDX short identifier.\n        if not lines[lineno].startswith(comment.format(""SPDX-License-Identifier"")):\n            lines.insert(lineno, spdx_line)\n            changed = True\n        lineno += 1\n\n        # Ensure next line is blank.\n        if not lines[lineno].isspace():\n            lines.insert(lineno, ""\\n"")\n            changed = True\n\n        if not changed:\n            continue\n\n        with open(filename, ""w"") as f:\n            f.write("""".join(lines))\n\n        print(""updated {}"".format(filename[len(root) + 1:]))\n'"
test/__init__.py,0,b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n'
test/conftest.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as np\n\nimport funsor.util\n\n\ndef _disallow_set_backend(*args):\n    raise ValueError(""set_backend() cannot be called during tests"")\n\n\ndef pytest_runtest_setup(item):\n    np.random.seed(0)\n    backend = funsor.util.get_backend()\n    if backend == ""torch"":\n        import pyro\n\n        pyro.set_rng_seed(0)\n        pyro.enable_validation(True)\n    elif backend == ""jax"":\n        from jax.config import config\n\n        config.update(\'jax_platform_name\', \'cpu\')\n\n    funsor.util.set_backend = _disallow_set_backend\n'"
test/test_adjoint.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport pytest\n\nimport funsor\nimport funsor.ops as ops\nfrom funsor.adjoint import AdjointTape\nfrom funsor.domains import bint, reals\nfrom funsor.einsum import BACKEND_ADJOINT_OPS, einsum, naive_einsum, naive_plated_einsum\nfrom funsor.interpreter import interpretation\nfrom funsor.optimizer import apply_optimizer\nfrom funsor.sum_product import MarkovProduct, naive_sequential_sum_product, sequential_sum_product, sum_product\nfrom funsor.terms import Variable, reflect\nfrom funsor.testing import (\n    assert_close,\n    check_funsor,\n    make_einsum_example,\n    make_plated_hmm_einsum,\n    random_gaussian,\n    random_tensor,\n    xfail_param\n)\nfrom funsor.util import get_backend\n\npytestmark = pytest.mark.skipif(get_backend() != ""torch"",\n                                reason=""numpy/jax backend requires porting pyro.ops.einsum"")\nif get_backend() == ""torch"":\n    import torch\n    from pyro.ops.contract import einsum as pyro_einsum\n    from pyro.ops.einsum.adjoint import require_backward as pyro_require_backward\n\nEINSUM_EXAMPLES = [\n    ""a->"",\n    ""ab->"",\n    "",->"",\n    "",,->"",\n    ""a,a->a"",\n    ""a,a,a->a"",\n    ""a,b->"",\n    ""ab,a->"",\n    ""a,b,c->"",\n    ""a,a->"",\n    ""a,a,a,ab->"",\n    ""abc,bcd,cde->"",\n    ""ab,bc,cd->"",\n    ""ab,b,bc,c,cd,d->"",\n]\n\n\n@pytest.mark.parametrize(\'einsum_impl\', [naive_einsum, einsum])\n@pytest.mark.parametrize(\'equation\', EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', [\n    \'pyro.ops.einsum.torch_marginal\',\n    xfail_param(\'pyro.ops.einsum.torch_map\', reason=""wrong adjoint""),\n])\ndef test_einsum_adjoint(einsum_impl, equation, backend):\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n    sum_op, prod_op = BACKEND_ADJOINT_OPS[backend]\n\n    with AdjointTape() as tape:  # interpretation(reflect):\n        fwd_expr = einsum_impl(equation, *funsor_operands, backend=backend)\n    actuals = tape.adjoint(sum_op, prod_op, fwd_expr, funsor_operands)\n\n    for operand in operands:\n        pyro_require_backward(operand)\n    expected_out = pyro_einsum(equation, *operands,\n                               modulo_total=True,\n                               backend=backend)[0]\n    expected_out._pyro_backward()\n\n    for i, (inp, tv, fv) in enumerate(zip(inputs, operands, funsor_operands)):\n        actual = actuals[fv]\n        expected = tv._pyro_backward_result\n        if inp:\n            actual = actual.align(tuple(inp))\n        assert isinstance(actual, funsor.Tensor)\n        assert expected.shape == actual.data.shape\n        assert torch.allclose(expected, actual.data, atol=1e-7)\n\n\nPLATED_EINSUM_EXAMPLES = [\n    (\',i->\', \'i\'),\n    (\'i->\', \'i\'),\n    (\'ai->\', \'i\'),\n    (\',ai,abij->\', \'ij\'),\n    (\'a,ai,bij->\', \'ij\'),\n    (\'ai,abi,bci,cdi->\', \'i\'),\n    (\'aij,abij,bcij->\', \'ij\'),\n    (\'a,abi,bcij,cdij->\', \'ij\'),\n]\n\n\n@pytest.mark.parametrize(\'einsum_impl\', [naive_plated_einsum, einsum])\n@pytest.mark.parametrize(\'equation,plates\', PLATED_EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', [\n    \'pyro.ops.einsum.torch_marginal\',\n    xfail_param(\'pyro.ops.einsum.torch_map\', reason=""wrong adjoint""),\n])\ndef test_plated_einsum_adjoint(einsum_impl, equation, plates, backend):\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n    sum_op, prod_op = BACKEND_ADJOINT_OPS[backend]\n\n    with AdjointTape() as tape:  # interpretation(reflect):\n        fwd_expr = einsum_impl(equation, *funsor_operands, plates=plates, backend=backend)\n    actuals = tape.adjoint(sum_op, prod_op, fwd_expr, funsor_operands)\n\n    for operand in operands:\n        pyro_require_backward(operand)\n    expected_out = pyro_einsum(equation, *operands,\n                               modulo_total=False,\n                               plates=plates,\n                               backend=backend)[0]\n    expected_out._pyro_backward()\n\n    for i, (inp, tv, fv) in enumerate(zip(inputs, operands, funsor_operands)):\n        actual = actuals[fv]\n        expected = tv._pyro_backward_result\n        if inp:\n            actual = actual.align(tuple(inp))\n        assert isinstance(actual, funsor.Tensor)\n        assert expected.shape == actual.data.shape\n        assert torch.allclose(expected, actual.data, atol=1e-7)\n\n\nOPTIMIZED_PLATED_EINSUM_EXAMPLES = [\n    make_plated_hmm_einsum(num_steps, num_obs_plates=b, num_hidden_plates=a)\n    for num_steps in [20, 30, 50]\n    for (a, b) in [(0, 0), (0, 1), (0, 2), (1, 1), (1, 2)]\n]\n\n\n@pytest.mark.parametrize(\'equation,plates\', OPTIMIZED_PLATED_EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', [\n    \'pyro.ops.einsum.torch_marginal\',\n    xfail_param(\'pyro.ops.einsum.torch_map\', reason=""wrong adjoint""),\n])\ndef test_optimized_plated_einsum_adjoint(equation, plates, backend):\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n    sum_op, prod_op = BACKEND_ADJOINT_OPS[backend]\n\n    with AdjointTape() as tape:  # interpretation(reflect):\n        fwd_expr = einsum(equation, *funsor_operands, plates=plates, backend=backend)\n    actuals = tape.adjoint(sum_op, prod_op, fwd_expr, funsor_operands)\n\n    for operand in operands:\n        pyro_require_backward(operand)\n    expected_out = pyro_einsum(equation, *operands,\n                               modulo_total=False,\n                               plates=plates,\n                               backend=backend)[0]\n    expected_out._pyro_backward()\n\n    for i, (inp, tv, fv) in enumerate(zip(inputs, operands, funsor_operands)):\n        actual = actuals[fv]\n        expected = tv._pyro_backward_result\n        if inp:\n            actual = actual.align(tuple(inp))\n        assert isinstance(actual, funsor.Tensor)\n        assert expected.shape == actual.data.shape\n        assert torch.allclose(expected, actual.data, atol=1e-7)\n\n\n@pytest.mark.parametrize(\'num_steps\', list(range(3, 13)))\n@pytest.mark.parametrize(\'sum_op,prod_op,state_domain\', [\n    (ops.add, ops.mul, bint(2)),\n    (ops.add, ops.mul, bint(3)),\n    (ops.logaddexp, ops.add, bint(2)),\n    (ops.logaddexp, ops.add, bint(3)),\n    (ops.logaddexp, ops.add, reals()),\n    (ops.logaddexp, ops.add, reals(2)),\n], ids=str)\n@pytest.mark.parametrize(\'batch_inputs\', [\n    {},\n    {""foo"": bint(5)},\n    {""foo"": bint(2), ""bar"": bint(4)},\n], ids=lambda d: "","".join(d.keys()))\n@pytest.mark.parametrize(\'impl\', [\n    sequential_sum_product,\n    naive_sequential_sum_product,\n    MarkovProduct,\n])\ndef test_sequential_sum_product_adjoint(impl, sum_op, prod_op, batch_inputs, state_domain, num_steps):\n    # test mostly copied from test_sum_product.py\n    inputs = OrderedDict(batch_inputs)\n    inputs.update(prev=state_domain, curr=state_domain)\n    inputs[""time""] = bint(num_steps)\n    if state_domain.dtype == ""real"":\n        trans = random_gaussian(inputs)\n    else:\n        trans = random_tensor(inputs)\n    time = Variable(""time"", bint(num_steps))\n\n    with AdjointTape() as actual_tape:\n        actual = impl(sum_op, prod_op, trans, time, {""prev"": ""curr""})\n\n    expected_inputs = batch_inputs.copy()\n    expected_inputs.update(prev=state_domain, curr=state_domain)\n    assert dict(actual.inputs) == expected_inputs\n\n    # Check against contract.\n    operands = tuple(trans(time=t, prev=""t_{}"".format(t), curr=""t_{}"".format(t+1))\n                     for t in range(num_steps))\n    reduce_vars = frozenset(""t_{}"".format(t) for t in range(1, num_steps))\n    with AdjointTape() as expected_tape:\n        with interpretation(reflect):\n            expected = sum_product(sum_op, prod_op, operands, reduce_vars)\n        expected = apply_optimizer(expected)\n        expected = expected(**{""t_0"": ""prev"", ""t_{}"".format(num_steps): ""curr""})\n        expected = expected.align(tuple(actual.inputs.keys()))\n\n    # check forward pass (sanity check)\n    assert_close(actual, expected, rtol=5e-4 * num_steps)\n\n    # perform backward passes only after the sanity check\n    expected_bwds = expected_tape.adjoint(sum_op, prod_op, expected, operands)\n    actual_bwd = actual_tape.adjoint(sum_op, prod_op, actual, (trans,))[trans]\n\n    # check backward pass\n    for t, operand in enumerate(operands):\n        actual_bwd_t = actual_bwd(time=t, prev=""t_{}"".format(t), curr=""t_{}"".format(t+1))\n        expected_bwd = expected_bwds[operand].align(tuple(actual_bwd_t.inputs.keys()))\n        check_funsor(actual_bwd_t, expected_bwd.inputs, expected_bwd.output)\n        assert_close(actual_bwd_t, expected_bwd, rtol=5e-4 * num_steps)\n'"
test/test_affine.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport pytest\n\nfrom funsor.affine import extract_affine, is_affine\nfrom funsor.cnf import Contraction\nfrom funsor.domains import bint, reals\nfrom funsor.terms import Number, Unary, Variable\nfrom funsor.testing import assert_close, check_funsor, ones, randn, random_gaussian, random_tensor  # noqa: F401\nfrom funsor.tensor import Einsum, Tensor\n\nassert random_gaussian  # flake8\n\nSMOKE_TESTS = [\n    (\'t+x\', Contraction),\n    (\'x+t\', Contraction),\n    (\'n+x\', Contraction),\n    (\'n*x\', Contraction),\n    (\'t*x\', Contraction),\n    (\'x*t\', Contraction),\n    (\'-x\', Contraction),\n    (\'t-x\', Contraction),\n]\n\n\n@pytest.mark.parametrize(\'expr,expected_type\', SMOKE_TESTS)\ndef test_smoke(expr, expected_type):\n\n    t = Tensor(randn(2, 3), OrderedDict([(\'i\', bint(2)), (\'j\', bint(3))]))\n    assert isinstance(t, Tensor)\n\n    n = Number(2.)\n    assert isinstance(n, Number)\n\n    x = Variable(\'x\', reals())\n    assert isinstance(x, Variable)\n\n    y = Variable(\'y\', reals())\n    assert isinstance(y, Variable)\n\n    result = eval(expr)\n    assert isinstance(result, expected_type)\n    assert is_affine(result)\n\n\nSUBS_TESTS = [\n    (""(t * x)(i=1)"", Contraction, {""j"": bint(3), ""x"": reals()}),\n    (""(t * x)(i=1, x=y)"", Contraction, {""j"": bint(3), ""y"": reals()}),\n    (""(t * x + n)(x=y)"", Contraction, {""y"": reals(), ""i"": bint(2), ""j"": bint(3)}),\n    (""(x + y)(y=z)"", Contraction, {""x"": reals(), ""z"": reals()}),\n    (""(-x)(x=y+z)"", Contraction, {""y"": reals(), ""z"": reals()}),\n    (""(t * x + t * y)(x=z)"", Contraction, {""y"": reals(), ""z"": reals(), ""i"": bint(2), ""j"": bint(3)}),\n]\n\n\n@pytest.mark.parametrize(""expr,expected_type,expected_inputs"", SUBS_TESTS)\ndef test_affine_subs(expr, expected_type, expected_inputs):\n\n    expected_output = reals()\n\n    t = Tensor(randn(2, 3), OrderedDict([(\'i\', bint(2)), (\'j\', bint(3))]))\n    assert isinstance(t, Tensor)\n\n    n = Number(2.)\n    assert isinstance(n, Number)\n\n    x = Variable(\'x\', reals())\n    assert isinstance(x, Variable)\n\n    y = Variable(\'y\', reals())\n    assert isinstance(y, Variable)\n\n    z = Variable(\'z\', reals())\n    assert isinstance(z, Variable)\n\n    result = eval(expr)\n    assert isinstance(result, expected_type)\n    check_funsor(result, expected_inputs, expected_output)\n    assert is_affine(result)\n\n\n@pytest.mark.parametrize(\'expr\', [\n    ""-Variable(\'x\', reals())"",\n    ""Variable(\'x\', reals(2)).sum()"",\n    ""Variable(\'x\', reals()) + 0.5"",\n    ""Variable(\'x\', reals(2, 3)) + Variable(\'y\', reals(2, 3))"",\n    ""Variable(\'x\', reals(2)) + Variable(\'y\', reals(2))"",\n    ""Variable(\'x\', reals(2)) + ones(2)"",\n    ""Variable(\'x\', reals(2)) * randn(2)"",\n    ""Variable(\'x\', reals(2)) * randn(2) + ones(2)"",\n    ""Variable(\'x\', reals(2)) + Tensor(randn(3, 2), OrderedDict(i=bint(3)))"",\n    ""Einsum(\'abcd,ac->bd\',""\n    "" (Tensor(randn(2, 3, 4, 5)), Variable(\'x\', reals(2, 4))))"",\n    ""Tensor(randn(3, 5)) + Einsum(\'abcd,ac->bd\',""\n    "" (Tensor(randn(2, 3, 4, 5)), Variable(\'x\', reals(2, 4))))"",\n    ""Variable(\'x\', reals(2, 8))[0] + randn(8)"",\n    ""Variable(\'x\', reals(2, 8))[Variable(\'i\', bint(2))] / 4 - 3.5"",\n])\ndef test_extract_affine(expr):\n    x = eval(expr)\n    assert is_affine(x)\n    assert isinstance(x, (Unary, Contraction, Einsum))\n    real_inputs = OrderedDict((k, d) for k, d in x.inputs.items()\n                              if d.dtype == \'real\')\n\n    const, coeffs = extract_affine(x)\n    assert isinstance(const, Tensor)\n    assert const.shape == x.shape\n    assert list(coeffs) == list(real_inputs)\n    for name, (coeff, eqn) in coeffs.items():\n        assert isinstance(name, str)\n        assert isinstance(coeff, Tensor)\n        assert isinstance(eqn, str)\n\n    subs = {k: random_tensor(OrderedDict(), d) for k, d in real_inputs.items()}\n    expected = x(**subs)\n    assert isinstance(expected, Tensor)\n\n    actual = const + sum(Einsum(eqn, (coeff, subs[k]))\n                         for k, (coeff, eqn) in coeffs.items())\n    assert isinstance(actual, Tensor)\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(""expr"", [\n    ""Variable(\'x\', reals()).log()"",\n    ""Variable(\'x\', reals()).exp()"",\n    ""Variable(\'x\', reals()).sigmoid()"",\n    ""Variable(\'x\', reals(2)).prod()"",\n    ""Variable(\'x\', reals()) ** 2"",\n    ""Variable(\'x\', reals()) ** 2"",\n    ""2 ** Variable(\'x\', reals())"",\n    ""Variable(\'x\', reals()) * Variable(\'x\', reals())"",\n    ""Variable(\'x\', reals()) * Variable(\'y\', reals())"",\n    ""Variable(\'x\', reals()) / Variable(\'y\', reals())"",\n    ""Variable(\'x\', reals()) / Variable(\'y\', reals())"",\n    ""Variable(\'x\', reals(2,3)) @ Variable(\'y\', reals(3,4))"",\n    ""random_gaussian(OrderedDict(x=reals()))"",\n    ""Einsum(\'abcd,ac->bd\',""\n    "" (Variable(\'y\', reals(2, 3, 4, 5)), Variable(\'x\', reals(2, 4))))"",\n])\ndef test_not_is_affine(expr):\n    x = eval(expr)\n    assert not is_affine(x)\n'"
test/test_alpha_conversion.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pytest\n\nimport funsor.ops as ops\nfrom funsor.domains import bint, reals\nfrom funsor.interpreter import gensym, interpretation, reinterpret\nfrom funsor.terms import Cat, Independent, Lambda, Number, Slice, Stack, Variable, reflect\nfrom funsor.testing import assert_close, check_funsor, random_tensor\nfrom funsor.util import get_backend\n\n\ndef test_sample_subs_smoke():\n    x = random_tensor(OrderedDict([(\'i\', bint(3)), (\'j\', bint(2))]), reals())\n    with interpretation(reflect):\n        z = x(i=1)\n    rng_key = None if get_backend() == ""torch"" else np.array([0, 1], dtype=np.uint32)\n    actual = z.sample(frozenset({""j""}), OrderedDict({""i"": bint(4)}), rng_key=rng_key)\n    check_funsor(actual, {""j"": bint(2), ""i"": bint(4)}, reals())\n\n\ndef test_subs_reduce():\n    x = random_tensor(OrderedDict([(\'i\', bint(3)), (\'j\', bint(2))]), reals())\n    ix = random_tensor(OrderedDict([(\'i\', bint(3))]), bint(2))\n    ix2 = ix(i=\'i2\')\n    with interpretation(reflect):\n        actual = x.reduce(ops.add, frozenset({""i""}))\n    actual = actual(j=ix)\n    expected = x(j=ix2).reduce(ops.add, frozenset({""i""}))(i2=\'i\')\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'lhs_vars\', [(), (\'i\',), (\'j\',), (\'i\', \'j\')])\n@pytest.mark.parametrize(\'rhs_vars\', [(), (\'i\',), (\'j\',), (\'i\', \'j\')])\ndef test_distribute_reduce(lhs_vars, rhs_vars):\n\n    lhs_vars, rhs_vars = frozenset(lhs_vars), frozenset(rhs_vars)\n    lhs = random_tensor(OrderedDict([(\'i\', bint(3)), (\'j\', bint(2))]), reals())\n    rhs = random_tensor(OrderedDict([(\'i\', bint(3)), (\'j\', bint(2))]), reals())\n\n    with interpretation(reflect):\n        actual_lhs = lhs.reduce(ops.add, lhs_vars) if lhs_vars else lhs\n        actual_rhs = rhs.reduce(ops.add, rhs_vars) if rhs_vars else rhs\n\n    actual = reinterpret(actual_lhs * actual_rhs)\n\n    lhs_subs = {v: gensym(v) for v in lhs_vars}\n    rhs_subs = {v: gensym(v) for v in rhs_vars}\n    expected = (lhs(**lhs_subs) * rhs(**rhs_subs)).reduce(\n        ops.add, frozenset(lhs_subs.values()) | frozenset(rhs_subs.values()))\n\n    assert_close(actual, expected)\n\n\ndef test_lazy_subs_type_clash():\n    with interpretation(reflect):\n        Slice(\'t\', 3)(t=Slice(\'t\', 2, dtype=3)).reduce(ops.add)\n\n\n@pytest.mark.parametrize(""name"", [""s"", ""t""])\ndef test_cat(name):\n    with interpretation(reflect):\n        x = Stack(""t"", (Number(1), Number(2)))\n        y = Stack(""t"", (Number(4), Number(8), Number(16)))\n        xy = Cat(name, (x, y), ""t"")\n        xy.reduce(ops.add)\n\n\ndef test_subs_lambda():\n    z = Variable(\'z\', reals())\n    i = Variable(\'i\', bint(5))\n    ix = random_tensor(OrderedDict([(\'i\', bint(5))]), reals())\n    actual = Lambda(i, z)(z=ix)\n    expected = Lambda(i(i=\'j\'), z(z=ix))\n    check_funsor(actual, expected.inputs, expected.output)\n    assert_close(actual, expected)\n\n\ndef test_slice_lambda():\n    z = Variable(\'z\', reals())\n    i = Variable(\'i\', bint(5))\n    j = Variable(\'j\', bint(7))\n    zi = Lambda(i, z)\n    zj = Lambda(j, z)\n    zij = Lambda(j, zi)\n    zj2 = zij[:, i]\n    check_funsor(zj2, zj.inputs, zj.output)\n\n\ndef test_subs_independent():\n    f = Variable(\'x_i\', reals(4, 5)) + random_tensor(OrderedDict(i=bint(3)))\n\n    actual = Independent(f, \'x\', \'i\', \'x_i\')\n    assert \'i\' not in actual.inputs\n    assert \'x_i\' not in actual.inputs\n\n    y = Variable(\'y\', reals(3, 4, 5))\n    fsub = y + (0. * random_tensor(OrderedDict(i=bint(7))))\n    actual = actual(x=fsub)\n    assert actual.inputs[\'i\'] == bint(7)\n\n    expected = f(x_i=y[\'i\']).reduce(ops.add, \'i\')\n\n    data = random_tensor(OrderedDict(i=bint(7)), y.output)\n    assert_close(actual(y=data), expected(y=data))\n\n\n@pytest.mark.xfail(reason=""Independent not quite compatible with sample"")\ndef test_sample_independent():\n    f = Variable(\'x_i\', reals(4, 5)) + random_tensor(OrderedDict(i=bint(3)))\n    actual = Independent(f, \'x\', \'i\', \'x_i\')\n    assert actual.sample(\'i\')\n    assert actual.sample(\'j\', {\'i\': 2})\n'"
test/test_cnf.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nfrom collections import OrderedDict\n\nimport numpy as np  # noqa: F401\nimport pytest\n\nfrom funsor import ops\nfrom funsor.cnf import Contraction, BACKEND_TO_EINSUM_BACKEND, BACKEND_TO_LOGSUMEXP_BACKEND\nfrom funsor.domains import bint  # noqa F403\nfrom funsor.domains import reals\nfrom funsor.einsum import einsum, naive_plated_einsum\nfrom funsor.interpreter import interpretation, reinterpret\nfrom funsor.tensor import Tensor\nfrom funsor.terms import Number, eager, normalize, reflect\nfrom funsor.testing import assert_close, check_funsor, make_einsum_example, random_tensor\nfrom funsor.util import get_backend, quote\n\nEINSUM_EXAMPLES = [\n    (""a,b->"", \'\'),\n    (""ab,a->"", \'\'),\n    (""a,a->"", \'\'),\n    (""a,a->a"", \'\'),\n    (""ab,bc,cd->da"", \'\'),\n    (""ab,cd,bc->da"", \'\'),\n    (""a,a,a,ab->ab"", \'\'),\n    (\'i->\', \'i\'),\n    (\',i->\', \'i\'),\n    (\'ai->\', \'i\'),\n    (\',ai,abij->\', \'ij\'),\n    (\'a,ai,bij->\', \'ij\'),\n    (\'ai,abi,bci,cdi->\', \'i\'),\n    (\'aij,abij,bcij->\', \'ij\'),\n    (\'a,abi,bcij,cdij->\', \'ij\'),\n]\n\n\n@pytest.mark.parametrize(\'equation,plates\', EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', [\n    BACKEND_TO_EINSUM_BACKEND[get_backend()],\n    BACKEND_TO_LOGSUMEXP_BACKEND[get_backend()]])\n@pytest.mark.parametrize(\'einsum_impl\', [einsum, naive_plated_einsum])\ndef test_normalize_einsum(equation, plates, backend, einsum_impl):\n    if get_backend() == ""torch"":\n        import torch  # noqa: F401\n\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n\n    with interpretation(reflect):\n        expr = einsum_impl(equation, *funsor_operands, backend=backend, plates=plates)\n\n    with interpretation(normalize):\n        transformed_expr = reinterpret(expr)\n\n    assert isinstance(transformed_expr, Contraction)\n    check_funsor(transformed_expr, expr.inputs, expr.output)\n\n    assert all(isinstance(v, (Number, Tensor, Contraction)) for v in transformed_expr.terms)\n\n    with interpretation(normalize):\n        transformed_expr2 = reinterpret(transformed_expr)\n\n    assert transformed_expr2 is transformed_expr  # check normalization\n\n    with interpretation(eager):\n        actual = reinterpret(transformed_expr)\n        expected = reinterpret(expr)\n\n    assert_close(actual, expected, rtol=1e-4)\n\n    actual = eval(quote(expected))  # requires torch, bint\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(""x_shape"", [(), (1,), (3,), (1, 1), (1, 3), (2, 1), (2, 3)], ids=str)\n@pytest.mark.parametrize(""y_shape"", [(), (1,), (3,), (1, 1), (1, 3), (2, 1), (2, 3)], ids=str)\n@pytest.mark.parametrize(""x_inputs,y_inputs"", [\n    ("""", """"),\n    (""i"", """"),\n    ("""", ""i""),\n    (""j"", ""i""),\n    (""ij"", ""i""),\n    (""i"", ""ik""),\n    (""ij"", ""ik""),\n])\n@pytest.mark.parametrize(""red_op,bin_op"", [(ops.add, ops.mul), (ops.logaddexp, ops.add)], ids=str)\ndef test_eager_contract_tensor_tensor(red_op, bin_op, x_inputs, x_shape, y_inputs, y_shape):\n    backend = get_backend()\n    inputs = OrderedDict([(""i"", bint(4)), (""j"", bint(5)), (""k"", bint(6))])\n    x_inputs = OrderedDict((k, v) for k, v in inputs.items() if k in x_inputs)\n    y_inputs = OrderedDict((k, v) for k, v in inputs.items() if k in y_inputs)\n    x = random_tensor(x_inputs, reals(*x_shape))\n    y = random_tensor(y_inputs, reals(*y_shape))\n\n    xy = bin_op(x, y)\n    all_vars = frozenset(x.inputs).union(y.inputs)\n    for n in range(len(all_vars)):\n        for reduced_vars in map(frozenset, itertools.combinations(all_vars, n)):\n            print(f""reduced_vars = {reduced_vars}"")\n            expected = xy.reduce(red_op, reduced_vars)\n            actual = Contraction(red_op, bin_op, reduced_vars, (x, y))\n            assert_close(actual, expected, atol=1e-4, rtol=1e-3 if backend == ""jax"" else 1e-4)\n'"
test/test_delta.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\nimport funsor.ops as ops\nfrom funsor.delta import Delta\nfrom funsor.domains import reals\nfrom funsor.tensor import Tensor, numeric_array\nfrom funsor.terms import Number, Variable\nfrom funsor.testing import assert_close, check_funsor, randn\n\n\ndef test_eager_subs_variable():\n    v = Variable('v', reals(3))\n    point = Tensor(randn(3))\n    d = Delta('foo', v)\n    assert d(v=point) is Delta('foo', point)\n\n\n@pytest.mark.parametrize('log_density', [0, 1.234])\ndef test_eager_subs_ground(log_density):\n    point1 = Tensor(randn(3))\n    point2 = Tensor(randn(3))\n    d = Delta('foo', point1, log_density)\n    check_funsor(d(foo=point1), {}, reals(), numeric_array(float(log_density)))\n    check_funsor(d(foo=point2), {}, reals(), numeric_array(float('-inf')))\n\n\ndef test_add_delta_funsor():\n    x = Variable('x', reals(3))\n    y = Variable('y', reals(3))\n    d = Delta('x', y)\n\n    expr = -(1 + x ** 2).log()\n    assert d + expr is d + expr(x=y)\n    assert expr + d is expr(x=y) + d\n\n\ndef test_reduce():\n    point = Tensor(randn(3))\n    d = Delta('foo', point)\n    assert d.reduce(ops.logaddexp, frozenset(['foo'])) is Number(0)\n\n\n@pytest.mark.parametrize('log_density', [0, 1.234])\ndef test_reduce_density(log_density):\n    point = Tensor(randn(3))\n    d = Delta('foo', point, log_density)\n    # Note that log_density affects ground substitution but does not affect reduction.\n    assert d.reduce(ops.logaddexp, frozenset(['foo'])) is Number(0)\n\n\n@pytest.mark.parametrize('shape', [(), (4,), (2, 3)], ids=str)\ndef test_transform_exp(shape):\n    point = Tensor(ops.abs(randn(shape)))\n    x = Variable('x', reals(*shape))\n    actual = Delta('y', point)(y=ops.exp(x))\n    expected = Delta('x', point.log(), point.log().sum())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize('shape', [(), (4,), (2, 3)], ids=str)\ndef test_transform_log(shape):\n    point = Tensor(randn(shape))\n    x = Variable('x', reals(*shape))\n    actual = Delta('y', point)(y=ops.log(x))\n    expected = Delta('x', point.exp(), -point.sum())\n    assert_close(actual, expected)\n"""
test/test_distribution.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\nimport math\nfrom collections import OrderedDict\nfrom importlib import import_module\n\nimport numpy as np\nimport pytest\n\nimport funsor\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction, GaussianMixture\nfrom funsor.delta import Delta\nfrom funsor.distribution import BACKEND_TO_DISTRIBUTIONS_BACKEND\nfrom funsor.domains import bint, reals\nfrom funsor.interpreter import interpretation, reinterpret\nfrom funsor.integrate import Integrate\nfrom funsor.tensor import Einsum, Tensor, align_tensors, numeric_array\nfrom funsor.terms import Independent, Variable, eager, lazy\nfrom funsor.testing import assert_close, check_funsor, rand, randint, randn, random_mvn, random_tensor, xfail_param\nfrom funsor.util import get_backend\n\npytestmark = pytest.mark.skipif(get_backend() == ""numpy"",\n                                reason=""numpy does not have distributions backend"")\nif get_backend() != ""numpy"":\n    dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    backend_dist = dist.dist\n\nif get_backend() == ""torch"":\n    from funsor.pyro.convert import dist_to_funsor\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'eager\', [False, True])\ndef test_beta_density(batch_shape, eager):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(), reals(), reals(), reals())\n    def beta(concentration1, concentration0, value):\n        return backend_dist.Beta(concentration1, concentration0).log_prob(value)\n\n    check_funsor(beta, {\'concentration1\': reals(), \'concentration0\': reals(), \'value\': reals()}, reals())\n\n    concentration1 = Tensor(ops.exp(randn(batch_shape)), inputs)\n    concentration0 = Tensor(ops.exp(randn(batch_shape)), inputs)\n    value = Tensor(rand(batch_shape), inputs)\n    expected = beta(concentration1, concentration0, value)\n    check_funsor(expected, inputs, reals())\n\n    d = Variable(\'value\', reals())\n    actual = dist.Beta(concentration1, concentration0, value) if eager else \\\n        dist.Beta(concentration1, concentration0, d)(value=value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'syntax\', [\'eager\', \'lazy\', \'generic\'])\ndef test_bernoulli_probs_density(batch_shape, syntax):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(), reals(), reals())\n    def bernoulli(probs, value):\n        return backend_dist.Bernoulli(probs).log_prob(value)\n\n    check_funsor(bernoulli, {\'probs\': reals(), \'value\': reals()}, reals())\n\n    probs = Tensor(rand(batch_shape), inputs)\n    value = Tensor(rand(batch_shape).round(), inputs)\n    expected = bernoulli(probs, value)\n    check_funsor(expected, inputs, reals())\n\n    d = Variable(\'value\', reals())\n    if syntax == \'eager\':\n        actual = dist.BernoulliProbs(probs, value)\n    elif syntax == \'lazy\':\n        actual = dist.BernoulliProbs(probs, d)(value=value)\n    elif syntax == \'generic\':\n        actual = dist.Bernoulli(probs=probs)(value=value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'syntax\', [\'eager\', \'lazy\', \'generic\'])\ndef test_bernoulli_logits_density(batch_shape, syntax):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(), reals(), reals())\n    def bernoulli(logits, value):\n        return backend_dist.Bernoulli(logits=logits).log_prob(value)\n\n    check_funsor(bernoulli, {\'logits\': reals(), \'value\': reals()}, reals())\n\n    logits = Tensor(rand(batch_shape), inputs)\n    value = Tensor(ops.astype(rand(batch_shape) >= 0.5, \'float\'), inputs)\n    expected = bernoulli(logits, value)\n    check_funsor(expected, inputs, reals())\n\n    d = Variable(\'value\', reals())\n    if syntax == \'eager\':\n        actual = dist.BernoulliLogits(logits, value)\n    elif syntax == \'lazy\':\n        actual = dist.BernoulliLogits(logits, d)(value=value)\n    elif syntax == \'generic\':\n        actual = dist.Bernoulli(logits=logits)(value=value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'eager\', [False, True])\ndef test_binomial_density(batch_shape, eager):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n    max_count = 10\n\n    @funsor.function(reals(), reals(), reals(), reals())\n    def binomial(total_count, probs, value):\n        return backend_dist.Binomial(total_count, probs).log_prob(value)\n\n    check_funsor(binomial, {\'total_count\': reals(), \'probs\': reals(), \'value\': reals()}, reals())\n\n    value_data = ops.astype(random_tensor(inputs, bint(max_count)).data, \'float\')\n    total_count_data = value_data + ops.astype(random_tensor(inputs, bint(max_count)).data, \'float\')\n    value = Tensor(value_data, inputs)\n    total_count = Tensor(total_count_data, inputs)\n    probs = Tensor(rand(batch_shape), inputs)\n    expected = binomial(total_count, probs, value)\n    check_funsor(expected, inputs, reals())\n\n    m = Variable(\'value\', reals())\n    actual = dist.Binomial(total_count, probs, value) if eager else \\\n        dist.Binomial(total_count, probs, m)(value=value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected, rtol=1e-5)\n\n\ndef test_categorical_defaults():\n    probs = Variable(\'probs\', reals(3))\n    value = Variable(\'value\', bint(3))\n    assert dist.Categorical(probs) is dist.Categorical(probs, value)\n\n\n@pytest.mark.parametrize(\'size\', [4])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_categorical_density(size, batch_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.of_shape(reals(size), bint(size))\n    def categorical(probs, value):\n        return probs[value].log()\n\n    check_funsor(categorical, {\'probs\': reals(size), \'value\': bint(size)}, reals())\n\n    probs_data = ops.exp(randn(batch_shape + (size,)))\n    probs_data /= probs_data.sum(-1)[..., None]\n    probs = Tensor(probs_data, inputs)\n    value = random_tensor(inputs, bint(size))\n    expected = categorical(probs, value)\n    check_funsor(expected, inputs, reals())\n\n    actual = dist.Categorical(probs, value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\ndef test_delta_defaults():\n    v = Variable(\'v\', reals())\n    log_density = Variable(\'log_density\', reals())\n    backend_dist_module = BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()]\n    assert isinstance(dist.Delta(v, log_density), import_module(backend_dist_module).Delta)\n    value = Variable(\'value\', reals())\n    assert dist.Delta(v, log_density, \'value\') is dist.Delta(v, log_density, value)\n\n\n@pytest.mark.parametrize(\'event_shape\', [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_delta_density(batch_shape, event_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(*event_shape), reals(), reals(*event_shape), reals())\n    def delta(v, log_density, value):\n        eq = (v == value)\n        for _ in range(len(event_shape)):\n            eq = ops.all(eq, -1)\n        return ops.log(ops.astype(eq, \'float32\')) + log_density\n\n    check_funsor(delta, {\'v\': reals(*event_shape),\n                         \'log_density\': reals(),\n                         \'value\': reals(*event_shape)}, reals())\n\n    v = Tensor(randn(batch_shape + event_shape), inputs)\n    log_density = Tensor(ops.exp(randn(batch_shape)), inputs)\n    for value in [v, Tensor(randn(batch_shape + event_shape), inputs)]:\n        expected = delta(v, log_density, value)\n        check_funsor(expected, inputs, reals())\n\n        actual = dist.Delta(v, log_density, value)\n        check_funsor(actual, inputs, reals())\n        assert_close(actual, expected)\n\n\ndef test_delta_delta():\n    v = Variable(\'v\', reals(2))\n    point = Tensor(randn(2))\n    log_density = Tensor(numeric_array(0.5))\n    d = dist.Delta(point, log_density, v)\n    assert d is Delta(\'v\', point, log_density)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'event_shape\', [(1,), (4,), (5,)], ids=str)\ndef test_dirichlet_density(batch_shape, event_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(*event_shape), reals(*event_shape), reals())\n    def dirichlet(concentration, value):\n        return backend_dist.Dirichlet(concentration).log_prob(value)\n\n    check_funsor(dirichlet, {\'concentration\': reals(*event_shape), \'value\': reals(*event_shape)}, reals())\n\n    concentration = Tensor(ops.exp(randn(batch_shape + event_shape)), inputs)\n    value_data = rand(batch_shape + event_shape)\n    value_data = value_data / value_data.sum(-1)[..., None]\n    value = Tensor(value_data, inputs)\n    expected = dirichlet(concentration, value)\n    check_funsor(expected, inputs, reals())\n    actual = dist.Dirichlet(concentration, value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'event_shape\', [(1,), (4,), (5,)], ids=str)\n@pytest.mark.xfail(get_backend() != \'torch\', reason=""DirichletMultinomial is not implemented yet in NumPyro"")\ndef test_dirichlet_multinomial_density(batch_shape, event_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n    max_count = 10\n\n    @funsor.function(reals(*event_shape), reals(), reals(*event_shape), reals())\n    def dirichlet_multinomial(concentration, total_count, value):\n        return backend_dist.DirichletMultinomial(concentration, total_count).log_prob(value)\n\n    check_funsor(dirichlet_multinomial, {\'concentration\': reals(*event_shape),\n                                         \'total_count\': reals(),\n                                         \'value\': reals(*event_shape)},\n                 reals())\n\n    concentration = Tensor(ops.exp(randn(batch_shape + event_shape)), inputs)\n    value_data = ops.astype(randint(0, max_count, size=batch_shape + event_shape), \'float32\')\n    total_count_data = value_data.sum(-1) + ops.astype(randint(0, max_count, size=batch_shape), \'float32\')\n    value = Tensor(value_data, inputs)\n    total_count = Tensor(total_count_data, inputs)\n    expected = dirichlet_multinomial(concentration, total_count, value)\n    check_funsor(expected, inputs, reals())\n    actual = dist.DirichletMultinomial(concentration, total_count, value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_lognormal_density(batch_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(), reals(), reals(), reals())\n    def log_normal(loc, scale, value):\n        return backend_dist.LogNormal(loc, scale).log_prob(value)\n\n    check_funsor(log_normal, {\'loc\': reals(), \'scale\': reals(), \'value\': reals()}, reals())\n\n    loc = Tensor(randn(batch_shape), inputs)\n    scale = Tensor(ops.exp(randn(batch_shape)), inputs)\n    value = Tensor(ops.exp(randn(batch_shape)), inputs)\n    expected = log_normal(loc, scale, value)\n    check_funsor(expected, inputs, reals())\n\n    actual = dist.LogNormal(loc, scale, value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'event_shape\', [(1,), (4,), (5,)], ids=str)\ndef test_multinomial_density(batch_shape, event_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n    max_count = 10\n\n    @funsor.function(reals(), reals(*event_shape), reals(*event_shape), reals())\n    def multinomial(total_count, probs, value):\n        if get_backend() == ""torch"":\n            total_count = total_count.max().item()\n        return backend_dist.Multinomial(total_count, probs).log_prob(value)\n\n    check_funsor(multinomial, {\'total_count\': reals(), \'probs\': reals(*event_shape), \'value\': reals(*event_shape)},\n                 reals())\n\n    probs_data = rand(batch_shape + event_shape)\n    probs_data = probs_data / probs_data.sum(-1)[..., None]\n    probs = Tensor(probs_data, inputs)\n    value_data = ops.astype(randint(0, max_count, size=batch_shape + event_shape), \'float\')\n    total_count_data = value_data.sum(-1)\n    value = Tensor(value_data, inputs)\n    total_count = Tensor(total_count_data, inputs)\n    expected = multinomial(total_count, probs, value)\n    check_funsor(expected, inputs, reals())\n    actual = dist.Multinomial(total_count, probs, value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\ndef test_normal_defaults():\n    loc = Variable(\'loc\', reals())\n    scale = Variable(\'scale\', reals())\n    value = Variable(\'value\', reals())\n    assert dist.Normal(loc, scale) is dist.Normal(loc, scale, value)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_normal_density(batch_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.of_shape(reals(), reals(), reals())\n    def normal(loc, scale, value):\n        return -((value - loc) ** 2) / (2 * scale ** 2) - scale.log() - math.log(math.sqrt(2 * math.pi))\n\n    check_funsor(normal, {\'loc\': reals(), \'scale\': reals(), \'value\': reals()}, reals())\n\n    loc = Tensor(randn(batch_shape), inputs)\n    scale = Tensor(ops.exp(randn(batch_shape)), inputs)\n    value = Tensor(randn(batch_shape), inputs)\n    expected = normal(loc, scale, value)\n    check_funsor(expected, inputs, reals())\n\n    actual = dist.Normal(loc, scale, value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_normal_gaussian_1(batch_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    loc = Tensor(randn(batch_shape), inputs)\n    scale = Tensor(ops.exp(randn(batch_shape)), inputs)\n    value = Tensor(randn(batch_shape), inputs)\n\n    expected = dist.Normal(loc, scale, value)\n    assert isinstance(expected, Tensor)\n    check_funsor(expected, inputs, reals())\n\n    g = dist.Normal(loc, scale, \'value\')\n    assert isinstance(g, Contraction)\n    actual = g(value=value)\n    check_funsor(actual, inputs, reals())\n\n    assert_close(actual, expected, atol=1e-4)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_normal_gaussian_2(batch_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    loc = Tensor(randn(batch_shape), inputs)\n    scale = Tensor(ops.exp(randn(batch_shape)), inputs)\n    value = Tensor(randn(batch_shape), inputs)\n\n    expected = dist.Normal(loc, scale, value)\n    assert isinstance(expected, Tensor)\n    check_funsor(expected, inputs, reals())\n\n    g = dist.Normal(Variable(\'value\', reals()), scale, loc)\n    assert isinstance(g, Contraction)\n    actual = g(value=value)\n    check_funsor(actual, inputs, reals())\n\n    assert_close(actual, expected, atol=1e-4)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_normal_gaussian_3(batch_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    loc = Tensor(randn(batch_shape), inputs)\n    scale = Tensor(ops.exp(randn(batch_shape)), inputs)\n    value = Tensor(randn(batch_shape), inputs)\n\n    expected = dist.Normal(loc, scale, value)\n    assert isinstance(expected, Tensor)\n    check_funsor(expected, inputs, reals())\n\n    g = dist.Normal(Variable(\'loc\', reals()), scale, \'value\')\n    assert isinstance(g, Contraction)\n    actual = g(loc=loc, value=value)\n    check_funsor(actual, inputs, reals())\n\n    assert_close(actual, expected, atol=1e-4)\n\n\nNORMAL_AFFINE_TESTS = [\n    \'dist.Normal(x+2, scale, y+2)\',\n    \'dist.Normal(y, scale, x)\',\n    \'dist.Normal(x - y, scale, 0)\',\n    \'dist.Normal(0, scale, y - x)\',\n    \'dist.Normal(2 * x - y, scale, x)\',\n    \'dist.Normal(0, 1, (x - y) / scale) - scale.log()\',\n    \'dist.Normal(2 * y, 2 * scale, 2 * x) + math.log(2)\',\n]\n\n\n@pytest.mark.parametrize(\'expr\', NORMAL_AFFINE_TESTS)\ndef test_normal_affine(expr):\n\n    scale = Tensor(numeric_array(0.3), OrderedDict())\n    x = Variable(\'x\', reals())\n    y = Variable(\'y\', reals())\n\n    expected = dist.Normal(x, scale, y)\n    actual = eval(expr)\n\n    assert isinstance(actual, Contraction)\n    assert dict(actual.inputs) == dict(expected.inputs), (actual.inputs, expected.inputs)\n\n    for ta, te in zip(actual.terms, expected.terms):\n        assert_close(ta.align(tuple(te.inputs)), te)\n\n\ndef test_normal_independent():\n    loc = random_tensor(OrderedDict(), reals(2))\n    scale = ops.exp(random_tensor(OrderedDict(), reals(2)))\n    fn = dist.Normal(loc[\'i\'], scale[\'i\'], value=\'z_i\')\n    assert fn.inputs[\'z_i\'] == reals()\n    d = Independent(fn, \'z\', \'i\', \'z_i\')\n    assert d.inputs[\'z\'] == reals(2)\n    rng_key = None if get_backend() == ""torch"" else np.array([0, 0], dtype=np.uint32)\n    sample = d.sample(frozenset([\'z\']), rng_key=rng_key)\n    assert isinstance(sample, Contraction)\n    assert sample.inputs[\'z\'] == reals(2)\n\n\ndef test_mvn_defaults():\n    loc = Variable(\'loc\', reals(3))\n    scale_tril = Variable(\'scale\', reals(3, 3))\n    value = Variable(\'value\', reals(3))\n    assert dist.MultivariateNormal(loc, scale_tril) is dist.MultivariateNormal(loc, scale_tril, value)\n\n\ndef _random_scale_tril(shape):\n    if get_backend() == ""torch"":\n        data = randn(shape)\n        return backend_dist.transforms.transform_to(backend_dist.constraints.lower_cholesky)(data)\n    else:\n        data = randn(shape[:-2] + (shape[-1] * (shape[-1] + 1) // 2,))\n        return backend_dist.biject_to(backend_dist.constraints.lower_cholesky)(data)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_mvn_density(batch_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(3), reals(3, 3), reals(3), reals())\n    def mvn(loc, scale_tril, value):\n        return backend_dist.MultivariateNormal(loc, scale_tril=scale_tril).log_prob(value)\n\n    check_funsor(mvn, {\'loc\': reals(3), \'scale_tril\': reals(3, 3), \'value\': reals(3)}, reals())\n\n    loc = Tensor(randn(batch_shape + (3,)), inputs)\n    scale_tril = Tensor(_random_scale_tril(batch_shape + (3, 3)), inputs)\n    value = Tensor(randn(batch_shape + (3,)), inputs)\n    expected = mvn(loc, scale_tril, value)\n    check_funsor(expected, inputs, reals())\n\n    actual = dist.MultivariateNormal(loc, scale_tril, value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_mvn_gaussian(batch_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    loc = Tensor(randn(batch_shape + (3,)), inputs)\n    scale_tril = Tensor(_random_scale_tril(batch_shape + (3, 3)), inputs)\n    value = Tensor(randn(batch_shape + (3,)), inputs)\n\n    expected = dist.MultivariateNormal(loc, scale_tril, value)\n    assert isinstance(expected, Tensor)\n    check_funsor(expected, inputs, reals())\n\n    g = dist.MultivariateNormal(loc, scale_tril, \'value\')\n    assert isinstance(g, Contraction)\n    actual = g(value=value)\n    check_funsor(actual, inputs, reals())\n\n    assert_close(actual, expected, atol=1e-3, rtol=1e-4)\n\n\ndef _check_mvn_affine(d1, data):\n    backend_module = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    assert isinstance(d1, backend_module.MultivariateNormal)\n    d2 = reinterpret(d1)\n    assert issubclass(type(d2), GaussianMixture)\n    actual = d2(**data)\n    expected = d1(**data)\n    assert_close(actual, expected)\n\n\n@pytest.mark.xfail(get_backend() == \'jax\', reason=\'dist_to_funsor for jax backend is not available yet\')\ndef test_mvn_affine_one_var():\n    x = Variable(\'x\', reals(2))\n    data = dict(x=Tensor(randn(2)))\n    with interpretation(lazy):\n        d = dist_to_funsor(random_mvn((), 2))\n        d = d(value=2 * x + 1)\n    _check_mvn_affine(d, data)\n\n\n@pytest.mark.xfail(get_backend() == \'jax\', reason=\'dist_to_funsor for jax backend is not available yet\')\ndef test_mvn_affine_two_vars():\n    x = Variable(\'x\', reals(2))\n    y = Variable(\'y\', reals(2))\n    data = dict(x=Tensor(randn(2)), y=Tensor(randn(2)))\n    with interpretation(lazy):\n        d = dist_to_funsor(random_mvn((), 2))\n        d = d(value=x - y)\n    _check_mvn_affine(d, data)\n\n\ndef test_mvn_affine_matmul():\n    x = Variable(\'x\', reals(2))\n    y = Variable(\'y\', reals(3))\n    m = Tensor(randn(2, 3))\n    data = dict(x=Tensor(randn(2)), y=Tensor(randn(3)))\n    with interpretation(lazy):\n        d = random_mvn((), 3)\n        d = dist.MultivariateNormal(loc=y, scale_tril=d.scale_tril, value=x @ m)\n    _check_mvn_affine(d, data)\n\n\n@pytest.mark.xfail(get_backend() == \'jax\', reason=\'dist_to_funsor for jax backend is not available yet\')\ndef test_mvn_affine_matmul_sub():\n    x = Variable(\'x\', reals(2))\n    y = Variable(\'y\', reals(3))\n    m = Tensor(randn(2, 3))\n    data = dict(x=Tensor(randn(2)), y=Tensor(randn(3)))\n    with interpretation(lazy):\n        d = dist_to_funsor(random_mvn((), 3))\n        d = d(value=x @ m - y)\n    _check_mvn_affine(d, data)\n\n\n@pytest.mark.xfail(get_backend() == \'jax\', reason=\'dist_to_funsor for jax backend is not available yet\')\ndef test_mvn_affine_einsum():\n    c = Tensor(randn(3, 2, 2))\n    x = Variable(\'x\', reals(2, 2))\n    y = Variable(\'y\', reals())\n    data = dict(x=Tensor(randn(2, 2)), y=Tensor(randn(())))\n    with interpretation(lazy):\n        d = dist_to_funsor(random_mvn((), 3))\n        d = d(value=Einsum(""abc,bc->a"", c, x) + y)\n    _check_mvn_affine(d, data)\n\n\n@pytest.mark.xfail(get_backend() == \'jax\', reason=\'dist_to_funsor for jax backend is not available yet\')\ndef test_mvn_affine_getitem():\n    x = Variable(\'x\', reals(2, 2))\n    data = dict(x=Tensor(randn(2, 2)))\n    with interpretation(lazy):\n        d = dist_to_funsor(random_mvn((), 2))\n        d = d(value=x[0] - x[1])\n    _check_mvn_affine(d, data)\n\n\n@pytest.mark.xfail(get_backend() == \'jax\', reason=\'dist_to_funsor for jax backend is not available yet\')\ndef test_mvn_affine_reshape():\n    x = Variable(\'x\', reals(2, 2))\n    y = Variable(\'y\', reals(4))\n    data = dict(x=Tensor(randn(2, 2)), y=Tensor(randn(4)))\n    with interpretation(lazy):\n        d = dist_to_funsor(random_mvn((), 4))\n        d = d(value=x.reshape((4,)) - y)\n    _check_mvn_affine(d, data)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'syntax\', [\'eager\', \'lazy\'])\ndef test_poisson_probs_density(batch_shape, syntax):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(), reals(), reals())\n    def poisson(rate, value):\n        return backend_dist.Poisson(rate).log_prob(value)\n\n    check_funsor(poisson, {\'rate\': reals(), \'value\': reals()}, reals())\n\n    rate = Tensor(rand(batch_shape), inputs)\n    value = Tensor(ops.astype(ops.astype(ops.exp(randn(batch_shape)), \'int32\'), \'float32\'), inputs)\n    expected = poisson(rate, value)\n    check_funsor(expected, inputs, reals())\n\n    d = Variable(\'value\', reals())\n    if syntax == \'eager\':\n        actual = dist.Poisson(rate, value)\n    elif syntax == \'lazy\':\n        actual = dist.Poisson(rate, d)(value=value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'syntax\', [\'eager\', \'lazy\'])\ndef test_gamma_probs_density(batch_shape, syntax):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(), reals(), reals(), reals())\n    def gamma(concentration, rate, value):\n        return backend_dist.Gamma(concentration, rate).log_prob(value)\n\n    check_funsor(gamma, {\'concentration\': reals(), \'rate\': reals(), \'value\': reals()}, reals())\n\n    concentration = Tensor(rand(batch_shape), inputs)\n    rate = Tensor(rand(batch_shape), inputs)\n    value = Tensor(ops.exp(randn(batch_shape)), inputs)\n    expected = gamma(concentration, rate, value)\n    check_funsor(expected, inputs, reals())\n\n    d = Variable(\'value\', reals())\n    if syntax == \'eager\':\n        actual = dist.Gamma(concentration, rate, value)\n    elif syntax == \'lazy\':\n        actual = dist.Gamma(concentration, rate, d)(value=value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'syntax\', [\'eager\', \'lazy\'])\n@pytest.mark.xfail(get_backend() != \'torch\', reason=""VonMises is not implemented yet in NumPyro"")\ndef test_von_mises_probs_density(batch_shape, syntax):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    @funsor.function(reals(), reals(), reals(), reals())\n    def von_mises(loc, concentration, value):\n        return backend_dist.VonMises(loc, concentration).log_prob(value)\n\n    check_funsor(von_mises, {\'concentration\': reals(), \'loc\': reals(), \'value\': reals()}, reals())\n\n    concentration = Tensor(rand(batch_shape), inputs)\n    loc = Tensor(rand(batch_shape), inputs)\n    value = Tensor(ops.abs(randn(batch_shape)), inputs)\n    expected = von_mises(loc, concentration, value)\n    check_funsor(expected, inputs, reals())\n\n    d = Variable(\'value\', reals())\n    if syntax == \'eager\':\n        actual = dist.VonMises(loc, concentration, value)\n    elif syntax == \'lazy\':\n        actual = dist.VonMises(loc, concentration, d)(value=value)\n    check_funsor(actual, inputs, reals())\n    assert_close(actual, expected)\n\n\ndef _get_stat_diff(funsor_dist_class, sample_inputs, inputs, num_samples, statistic, with_lazy, params):\n    params = [Tensor(p, inputs) for p in params]\n    if isinstance(with_lazy, bool):\n        with interpretation(lazy if with_lazy else eager):\n            funsor_dist = funsor_dist_class(*params)\n    else:\n        funsor_dist = funsor_dist_class(*params)\n\n    rng_key = None if get_backend() == ""torch"" else np.array([0, 0], dtype=np.uint32)\n    sample_value = funsor_dist.sample(frozenset([\'value\']), sample_inputs, rng_key=rng_key)\n    expected_inputs = OrderedDict(\n        tuple(sample_inputs.items()) + tuple(inputs.items()) + ((\'value\', funsor_dist.inputs[\'value\']),)\n    )\n    check_funsor(sample_value, expected_inputs, reals())\n\n    if sample_inputs:\n\n        actual_mean = Integrate(\n            sample_value, Variable(\'value\', funsor_dist.inputs[\'value\']), frozenset([\'value\'])\n        ).reduce(ops.add, frozenset(sample_inputs))\n\n        inputs, tensors = align_tensors(*list(funsor_dist.params.values())[:-1])\n        raw_dist = funsor_dist.dist_class(**dict(zip(funsor_dist._ast_fields[:-1], tensors)))\n        expected_mean = Tensor(raw_dist.mean, inputs)\n\n        if statistic == ""mean"":\n            actual_stat, expected_stat = actual_mean, expected_mean\n        elif statistic == ""variance"":\n            actual_stat = Integrate(\n                sample_value,\n                (Variable(\'value\', funsor_dist.inputs[\'value\']) - actual_mean) ** 2,\n                frozenset([\'value\'])\n            ).reduce(ops.add, frozenset(sample_inputs))\n            expected_stat = Tensor(raw_dist.variance, inputs)\n        elif statistic == ""entropy"":\n            actual_stat = -Integrate(\n                sample_value, funsor_dist, frozenset([\'value\'])\n            ).reduce(ops.add, frozenset(sample_inputs))\n            expected_stat = Tensor(raw_dist.entropy(), inputs)\n        else:\n            raise ValueError(""invalid test statistic"")\n\n        diff = actual_stat.reduce(ops.add).data - expected_stat.reduce(ops.add).data\n        return diff.sum(), diff\n\n\ndef _check_sample(funsor_dist_class, params, sample_inputs, inputs, atol=1e-2,\n                  num_samples=100000, statistic=""mean"", skip_grad=False, with_lazy=None):\n    """"""utility that compares a Monte Carlo estimate of a distribution mean with the true mean""""""\n    samples_per_dim = int(num_samples ** (1./max(1, len(sample_inputs))))\n    sample_inputs = OrderedDict((k, bint(samples_per_dim)) for k in sample_inputs)\n    _get_stat_diff_fn = functools.partial(\n        _get_stat_diff, funsor_dist_class, sample_inputs, inputs, num_samples, statistic, with_lazy)\n\n    if get_backend() == ""torch"":\n        import torch\n\n        for param in params:\n            param.requires_grad_()\n\n        res = _get_stat_diff_fn(params)\n        if sample_inputs:\n            diff_sum, diff = res\n            assert_close(diff, ops.new_zeros(diff, diff.shape), atol=atol, rtol=None)\n            if not skip_grad:\n                diff_grads = torch.autograd.grad(diff_sum, params, allow_unused=True)\n                for diff_grad in diff_grads:\n                    assert_close(diff_grad, ops.new_zeros(diff_grad, diff_grad.shape), atol=atol, rtol=None)\n    elif get_backend() == ""jax"":\n        import jax\n\n        if sample_inputs:\n            if skip_grad:\n                _, diff = _get_stat_diff_fn(params)\n                assert_close(diff, ops.new_zeros(diff, diff.shape), atol=atol, rtol=None)\n            else:\n                (_, diff), diff_grads = jax.value_and_grad(_get_stat_diff_fn, has_aux=True)(params)\n                assert_close(diff, ops.new_zeros(diff, diff.shape), atol=atol, rtol=None)\n                for diff_grad in diff_grads:\n                    assert_close(diff_grad, ops.new_zeros(diff_grad, diff_grad.shape), atol=atol, rtol=None)\n        else:\n            _get_stat_diff_fn(params)\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'reparametrized\', [True, False])\ndef test_gamma_sample(batch_shape, sample_inputs, reparametrized):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    concentration = rand(batch_shape)\n    rate = rand(batch_shape)\n    funsor_dist_class = (dist.Gamma if reparametrized else dist.NonreparameterizedGamma)\n    params = (concentration, rate)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, num_samples=200000,\n                  atol=5e-2 if reparametrized else 1e-1)\n\n\n@pytest.mark.parametrize(""with_lazy"", [True, xfail_param(False, reason=""missing pattern"")])\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'reparametrized\', [True, False])\ndef test_normal_sample(with_lazy, batch_shape, sample_inputs, reparametrized):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    loc = randn(batch_shape)\n    scale = rand(batch_shape)\n    funsor_dist_class = (dist.Normal if reparametrized else dist.NonreparameterizedNormal)\n    params = (loc, scale)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, num_samples=200000,\n                  atol=1e-2 if reparametrized else 1e-1, with_lazy=with_lazy)\n\n\n@pytest.mark.parametrize(""with_lazy"", [True, xfail_param(False, reason=""missing pattern"")])\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'event_shape\', [(1,), (4,), (5,)], ids=str)\ndef test_mvn_sample(with_lazy, batch_shape, sample_inputs, event_shape):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    loc = randn(batch_shape + event_shape)\n    scale_tril = _random_scale_tril(batch_shape + event_shape * 2)\n    funsor_dist_class = dist.MultivariateNormal\n    params = (loc, scale_tril)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, atol=7e-2, num_samples=200000, with_lazy=with_lazy)\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'event_shape\', [(1,), (4,), (5,)], ids=str)\n@pytest.mark.parametrize(\'reparametrized\', [True, False])\ndef test_dirichlet_sample(batch_shape, sample_inputs, event_shape, reparametrized):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    concentration = ops.exp(randn(batch_shape + event_shape))\n    funsor_dist_class = (dist.Dirichlet if reparametrized else dist.NonreparameterizedDirichlet)\n    params = (concentration,)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, atol=1e-2 if reparametrized else 1e-1)\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_bernoullilogits_sample(batch_shape, sample_inputs):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    logits = rand(batch_shape)\n    funsor_dist_class = dist.BernoulliLogits\n    params = (logits,)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, atol=5e-2, num_samples=100000)\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_bernoulliprobs_sample(batch_shape, sample_inputs):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    probs = rand(batch_shape)\n    funsor_dist_class = dist.BernoulliProbs\n    params = (probs,)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, atol=5e-2, num_samples=100000)\n\n\n@pytest.mark.parametrize(""with_lazy"", [True, xfail_param(False, reason=""missing pattern"")])\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\n@pytest.mark.parametrize(\'reparametrized\', [True, False])\ndef test_beta_sample(with_lazy, batch_shape, sample_inputs, reparametrized):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    concentration1 = ops.exp(randn(batch_shape))\n    concentration0 = ops.exp(randn(batch_shape))\n    funsor_dist_class = (dist.Beta if reparametrized else dist.NonreparameterizedBeta)\n    params = (concentration1, concentration0)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, atol=1e-2 if reparametrized else 1e-1,\n                  statistic=""variance"", num_samples=100000, with_lazy=with_lazy)\n\n\n@pytest.mark.parametrize(""with_lazy"", [True, xfail_param(False, reason=""missing pattern"")])\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_binomial_sample(with_lazy, batch_shape, sample_inputs):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    max_count = 10\n    total_count_data = random_tensor(inputs, bint(max_count)).data\n    if get_backend() == ""torch"":\n        total_count_data = ops.astype(total_count_data, \'float\')\n    total_count = total_count_data\n    probs = rand(batch_shape)\n    funsor_dist_class = dist.Binomial\n    params = (total_count, probs)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, atol=2e-2, skip_grad=True, with_lazy=with_lazy)\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [(), (\'ii\',), (\'ii\', \'jj\'), (\'ii\', \'jj\', \'kk\')])\n@pytest.mark.parametrize(\'batch_shape\', [(), (5,), (2, 3)], ids=str)\ndef test_poisson_sample(batch_shape, sample_inputs):\n    batch_dims = (\'i\', \'j\', \'k\')[:len(batch_shape)]\n    inputs = OrderedDict((k, bint(v)) for k, v in zip(batch_dims, batch_shape))\n\n    rate = rand(batch_shape)\n    funsor_dist_class = dist.Poisson\n    params = (rate,)\n\n    _check_sample(funsor_dist_class, params, sample_inputs, inputs, skip_grad=True)\n'"
test/test_einsum.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport opt_einsum\nimport pytest\n\nimport funsor\nimport funsor.ops as ops\nfrom funsor.cnf import BACKEND_TO_EINSUM_BACKEND, BACKEND_TO_LOGSUMEXP_BACKEND, BACKEND_TO_MAP_BACKEND\nfrom funsor.domains import bint\nfrom funsor.einsum import naive_einsum, naive_plated_einsum\nfrom funsor.interpreter import interpretation, reinterpret\nfrom funsor.optimizer import apply_optimizer\nfrom funsor.tensor import Tensor\nfrom funsor.terms import Variable, reflect\nfrom funsor.testing import assert_close, make_einsum_example\nfrom funsor.util import get_backend\n\nEINSUM_EXAMPLES = [\n    ""a,b->"",\n    ""ab,a->"",\n    ""a,a->"",\n    ""a,a->a"",\n    ""a,a,a,ab->ab"",\n    ""ab->ba"",\n    ""ab,bc,cd->da"",\n]\n\n\ndef backend_to_einsum_backends(backend):\n    backends = [BACKEND_TO_EINSUM_BACKEND[get_backend()],\n                BACKEND_TO_LOGSUMEXP_BACKEND[get_backend()]]\n    map_backend = BACKEND_TO_MAP_BACKEND[get_backend()]\n    backends.append(map_backend)\n    return backends\n\n\n@pytest.mark.parametrize(\'equation\', EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', backend_to_einsum_backends(get_backend()))\ndef test_einsum(equation, backend):\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n    expected = opt_einsum.contract(equation, *operands, backend=backend)\n\n    with interpretation(reflect):\n        naive_ast = naive_einsum(equation, *funsor_operands, backend=backend)\n        optimized_ast = apply_optimizer(naive_ast)\n    print(""Naive expression: {}"".format(naive_ast))\n    print(""Optimized expression: {}"".format(optimized_ast))\n    actual_optimized = reinterpret(optimized_ast)  # eager by default\n    actual = naive_einsum(equation, *funsor_operands, backend=backend)\n\n    assert isinstance(actual, funsor.Tensor) and len(outputs) == 1\n    if len(outputs[0]) > 0:\n        actual = actual.align(tuple(outputs[0]))\n        actual_optimized = actual_optimized.align(tuple(outputs[0]))\n\n    assert_close(actual, actual_optimized, atol=1e-4)\n    assert expected.shape == actual.data.shape\n    assert_close(expected, actual.data, rtol=1e-5, atol=1e-8)\n    for output in outputs:\n        for i, output_dim in enumerate(output):\n            assert output_dim in actual.inputs\n            assert actual.inputs[output_dim].dtype == sizes[output_dim]\n\n\n@pytest.mark.parametrize(\'equation\', EINSUM_EXAMPLES)\n@pytest.mark.skipif(get_backend() == ""numpy"",\n                    reason=""funsor.distribution does not support numpy backend"")\ndef test_einsum_categorical(equation):\n    if get_backend() == ""jax"":\n        from funsor.jax.distributions import Categorical\n    else:\n        from funsor.torch.distributions import Categorical\n\n    inputs, outputs, sizes, operands, _ = make_einsum_example(equation)\n    operands = [ops.abs(operand) / ops.abs(operand).sum(-1)[..., None]\n                for operand in operands]\n\n    expected = opt_einsum.contract(equation, *operands,\n                                   backend=BACKEND_TO_EINSUM_BACKEND[get_backend()])\n\n    with interpretation(reflect):\n        funsor_operands = [\n            Categorical(probs=Tensor(\n                operand,\n                inputs=OrderedDict([(d, bint(sizes[d])) for d in inp[:-1]])\n            ))(value=Variable(inp[-1], bint(sizes[inp[-1]]))).exp()\n            for inp, operand in zip(inputs, operands)\n        ]\n\n        naive_ast = naive_einsum(equation, *funsor_operands)\n        optimized_ast = apply_optimizer(naive_ast)\n\n    print(""Naive expression: {}"".format(naive_ast))\n    print(""Optimized expression: {}"".format(optimized_ast))\n    actual_optimized = reinterpret(optimized_ast)  # eager by default\n    actual = naive_einsum(equation, *map(reinterpret, funsor_operands))\n\n    if len(outputs[0]) > 0:\n        actual = actual.align(tuple(outputs[0]))\n        actual_optimized = actual_optimized.align(tuple(outputs[0]))\n\n    assert_close(actual, actual_optimized, atol=1e-4)\n\n    assert expected.shape == actual.data.shape\n    assert_close(expected, actual.data)\n    for output in outputs:\n        for i, output_dim in enumerate(output):\n            assert output_dim in actual.inputs\n            assert actual.inputs[output_dim].dtype == sizes[output_dim]\n\n\nPLATED_EINSUM_EXAMPLES = [\n    (\'i->\', \'i\'),\n    (\',i->\', \'i\'),\n    (\'ai->\', \'i\'),\n    (\',ai,abij->\', \'ij\'),\n    (\'a,ai,bij->\', \'ij\'),\n    (\'ai,abi,bci,cdi->\', \'i\'),\n    (\'aij,abij,bcij->\', \'ij\'),\n    (\'a,abi,bcij,cdij->\', \'ij\'),\n]\n\n\n@pytest.mark.parametrize(\'equation,plates\', PLATED_EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', [\n    BACKEND_TO_EINSUM_BACKEND[get_backend()],\n    BACKEND_TO_LOGSUMEXP_BACKEND[get_backend()],\n    BACKEND_TO_MAP_BACKEND[get_backend()]\n])\n@pytest.mark.skipif(get_backend() != ""torch"",\n                    reason=""pyro.ops.contract.einsum does not work with numpy/jax backend."")\ndef test_plated_einsum(equation, plates, backend):\n    from pyro.ops.contract import einsum as pyro_einsum\n\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n    expected = pyro_einsum(equation, *operands, plates=plates, backend=backend, modulo_total=False)[0]\n    with interpretation(reflect):\n        naive_ast = naive_plated_einsum(equation, *funsor_operands, plates=plates, backend=backend)\n        optimized_ast = apply_optimizer(naive_ast)\n    actual_optimized = reinterpret(optimized_ast)  # eager by default\n    actual = naive_plated_einsum(equation, *funsor_operands, plates=plates, backend=backend)\n\n    if len(outputs[0]) > 0:\n        actual = actual.align(tuple(outputs[0]))\n        actual_optimized = actual_optimized.align(tuple(outputs[0]))\n\n    assert_close(actual, actual_optimized, atol=1e-3 if backend == \'torch\' else 1e-4)\n\n    assert expected.shape == actual.data.shape\n    assert_close(expected, actual.data)\n    for output in outputs:\n        for i, output_dim in enumerate(output):\n            assert output_dim in actual.inputs\n            assert actual.inputs[output_dim].dtype == sizes[output_dim]\n'"
test/test_gaussian.py,3,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nfrom collections import OrderedDict\nfrom functools import reduce\n\nimport numpy as np\nimport pytest\n\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction, GaussianMixture\nfrom funsor.domains import bint, reals\nfrom funsor.gaussian import BlockMatrix, BlockVector, Gaussian\nfrom funsor.integrate import Integrate\nfrom funsor.tensor import Einsum, Tensor, numeric_array\nfrom funsor.terms import Number, Variable\nfrom funsor.testing import (assert_close, id_from_inputs, ones, randn, random_gaussian,\n                            random_tensor, zeros)\nfrom funsor.util import get_backend\n\nassert Einsum  # flake8\n\n\n@pytest.mark.parametrize(""size"", [1, 2, 3], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (5,), (2, 3)], ids=str)\n@pytest.mark.skipif(get_backend() != ""torch"", reason=""The test is specific to \'torch\' backend"")\ndef test_cholesky_solve(batch_shape, size):\n    import torch\n\n    b = torch.randn(batch_shape + (size, 5))\n    x = torch.randn(batch_shape + (size, size))\n    x = x.transpose(-1, -2).matmul(x)\n    u = x.cholesky()\n    expected = torch.cholesky_solve(b, u)\n    assert not expected.requires_grad\n    actual = torch.cholesky_solve(b.requires_grad_(), u.requires_grad_())\n    assert actual.requires_grad\n    assert_close(expected, actual)\n\n\ndef naive_cholesky_inverse(u):\n    import torch\n\n    shape = u.shape\n    return torch.stack([\n        part.cholesky_inverse()\n        for part in u.reshape((-1,) + u.shape[-2:])\n    ]).reshape(shape)\n\n\n@pytest.mark.parametrize(""requires_grad"", [False, True])\n@pytest.mark.parametrize(""size"", [1, 2, 3], ids=str)\n@pytest.mark.parametrize(""batch_shape"", [(), (5,), (2, 3)], ids=str)\n@pytest.mark.skipif(get_backend() != ""torch"", reason=""The test is specific to \'torch\' backend"")\ndef test_cholesky_inverse(batch_shape, size, requires_grad):\n    import torch\n\n    x = torch.randn(batch_shape + (size, size))\n    x = x.transpose(-1, -2).matmul(x)\n    u = x.cholesky()\n    if requires_grad:\n        u.requires_grad_()\n    assert_close(ops.cholesky_inverse(u), naive_cholesky_inverse(u))\n    if requires_grad:\n        ops.cholesky_inverse(u).sum().backward()\n\n\ndef test_block_vector():\n    shape = (10,)\n    expected = zeros(shape)\n    actual = BlockVector(shape)\n\n    expected[1] = randn(())\n    actual[1] = expected[1]\n\n    expected[3:5] = randn((2,))\n    actual[3:5] = expected[3:5]\n\n    assert_close(actual.as_tensor(), expected)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (3, 2)])\ndef test_block_vector_batched(batch_shape):\n    shape = batch_shape + (10,)\n    expected = zeros(shape)\n    actual = BlockVector(shape)\n\n    expected[..., 1] = randn(batch_shape)\n    actual[..., 1] = expected[..., 1]\n\n    expected[..., 3:5] = randn(batch_shape + (2,))\n    actual[..., 3:5] = expected[..., 3:5]\n\n    assert_close(actual.as_tensor(), expected)\n\n\n@pytest.mark.parametrize(\'sparse\', [False, True])\ndef test_block_matrix(sparse):\n    shape = (10, 10)\n    expected = zeros(shape)\n    actual = BlockMatrix(shape)\n\n    expected[1, 1] = randn(())\n    actual[1, 1] = expected[1, 1]\n\n    if not sparse:\n        expected[1, 3:5] = randn((2,))\n        actual[1, 3:5] = expected[1, 3:5]\n\n        expected[3:5, 1] = randn((2,))\n        actual[3:5, 1] = expected[3:5, 1]\n\n    expected[3:5, 3:5] = randn((2, 2))\n    actual[3:5, 3:5] = expected[3:5, 3:5]\n\n    assert_close(actual.as_tensor(), expected)\n\n\n@pytest.mark.parametrize(\'sparse\', [False, True])\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (3, 2)])\ndef test_block_matrix_batched(batch_shape, sparse):\n    shape = batch_shape + (10, 10)\n    expected = zeros(shape)\n    actual = BlockMatrix(shape)\n\n    expected[..., 1, 1] = randn(batch_shape)\n    actual[..., 1, 1] = expected[..., 1, 1]\n\n    if not sparse:\n        expected[..., 1, 3:5] = randn(batch_shape + (2,))\n        actual[..., 1, 3:5] = expected[..., 1, 3:5]\n\n        expected[..., 3:5, 1] = randn(batch_shape + (2,))\n        actual[..., 3:5, 1] = expected[..., 3:5, 1]\n\n    expected[..., 3:5, 3:5] = randn(batch_shape + (2, 2))\n    actual[..., 3:5, 3:5] = expected[..., 3:5, 3:5]\n\n    assert_close(actual.as_tensor(), expected)\n\n\n@pytest.mark.parametrize(\'expr,expected_type\', [\n    (\'-g1\', Gaussian),\n    (\'g1 + 1\', Contraction),\n    (\'g1 - 1\', Contraction),\n    (\'1 + g1\', Contraction),\n    (\'g1 + shift\', Contraction),\n    (\'g1 + shift\', Contraction),\n    (\'shift + g1\', Contraction),\n    (\'shift - g1\', Contraction),\n    (\'g1 + g1\', Gaussian),\n    (\'(g1 + g2 + g2) - g2\', Gaussian),\n    (\'g1(i=i0)\', Gaussian),\n    (\'g2(i=i0)\', Gaussian),\n    (\'g1(i=i0) + g2(i=i0)\', Gaussian),\n    (\'g1(i=i0) + g2\', Gaussian),\n    (\'g1(x=x0)\', Tensor),\n    (\'g2(y=y0)\', Tensor),\n    (\'(g1 + g2)(i=i0)\', Gaussian),\n    (\'(g1 + g2)(x=x0, y=y0)\', Tensor),\n    (\'(g2 + g1)(x=x0, y=y0)\', Tensor),\n    (\'g1.reduce(ops.logaddexp, ""x"")\', Tensor),\n    (\'(g1 + g2).reduce(ops.logaddexp, ""x"")\', Contraction),\n    (\'(g1 + g2).reduce(ops.logaddexp, ""y"")\', Contraction),\n    (\'(g1 + g2).reduce(ops.logaddexp, frozenset([""x"", ""y""]))\', Tensor),\n])\ndef test_smoke(expr, expected_type):\n    g1 = Gaussian(\n        info_vec=numeric_array([[0.0, 0.1, 0.2],\n                               [2.0, 3.0, 4.0]]),\n        precision=numeric_array([[[1.0, 0.1, 0.2],\n                                  [0.1, 1.0, 0.3],\n                                  [0.2, 0.3, 1.0]],\n                                 [[1.0, 0.1, 0.2],\n                                 [0.1, 1.0, 0.3],\n                                 [0.2, 0.3, 1.0]]]),\n        inputs=OrderedDict([(\'i\', bint(2)), (\'x\', reals(3))]))\n    assert isinstance(g1, Gaussian)\n\n    g2 = Gaussian(\n        info_vec=numeric_array([[0.0, 0.1],\n                                [2.0, 3.0]]),\n        precision=numeric_array([[[1.0, 0.2],\n                                  [0.2, 1.0]],\n                                 [[1.0, 0.2],\n                                  [0.2, 1.0]]]),\n        inputs=OrderedDict([(\'i\', bint(2)), (\'y\', reals(2))]))\n    assert isinstance(g2, Gaussian)\n\n    shift = Tensor(numeric_array([-1., 1.]), OrderedDict([(\'i\', bint(2))]))\n    assert isinstance(shift, Tensor)\n\n    i0 = Number(1, 2)\n    assert isinstance(i0, Number)\n\n    x0 = Tensor(numeric_array([0.5, 0.6, 0.7]))\n    assert isinstance(x0, Tensor)\n\n    y0 = Tensor(numeric_array([[0.2, 0.3],\n                               [0.8, 0.9]]),\n                inputs=OrderedDict([(\'i\', bint(2))]))\n    assert isinstance(y0, Tensor)\n\n    result = eval(expr)\n    assert isinstance(result, expected_type)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n    {\'i\': bint(2), \'j\': bint(3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals()},\n    {\'x\': reals(4)},\n    {\'x\': reals(2, 3)},\n    {\'x\': reals(), \'y\': reals()},\n    {\'x\': reals(2), \'y\': reals(3)},\n    {\'x\': reals(4), \'y\': reals(2, 3), \'z\': reals()},\n], ids=id_from_inputs)\ndef test_align(int_inputs, real_inputs):\n    inputs1 = OrderedDict(list(sorted(int_inputs.items())) +\n                          list(sorted(real_inputs.items())))\n    inputs2 = OrderedDict(reversed(inputs1.items()))\n    g1 = random_gaussian(inputs1)\n    g2 = g1.align(tuple(inputs2))\n    assert g2.inputs == inputs2\n    g3 = g2.align(tuple(inputs1))\n    assert_close(g3, g1)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n    {\'i\': bint(2), \'j\': bint(3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals()},\n    {\'x\': reals(4)},\n    {\'x\': reals(2, 3)},\n    {\'x\': reals(), \'y\': reals()},\n    {\'x\': reals(2), \'y\': reals(3)},\n    {\'x\': reals(4), \'y\': reals(2, 3), \'z\': reals()},\n], ids=id_from_inputs)\ndef test_eager_subs_origin(int_inputs, real_inputs):\n    int_inputs = OrderedDict(sorted(int_inputs.items()))\n    real_inputs = OrderedDict(sorted(real_inputs.items()))\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n    g = random_gaussian(inputs)\n\n    # Check that Gaussian log density at origin is zero.\n    origin = {k: zeros(d.shape) for k, d in real_inputs.items()}\n    actual = g(**origin)\n    expected_data = zeros(tuple(d.size for d in int_inputs.values()))\n    expected = Tensor(expected_data, int_inputs)\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n    {\'i\': bint(2), \'j\': bint(3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals()},\n    {\'x\': reals(4)},\n    {\'x\': reals(2, 3)},\n    {\'x\': reals(), \'y\': reals()},\n    {\'x\': reals(2), \'y\': reals(3)},\n    {\'x\': reals(4), \'y\': reals(2, 3), \'z\': reals()},\n], ids=id_from_inputs)\ndef test_eager_subs(int_inputs, real_inputs):\n    int_inputs = OrderedDict(sorted(int_inputs.items()))\n    real_inputs = OrderedDict(sorted(real_inputs.items()))\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n\n    g = random_gaussian(inputs)\n\n    for order in itertools.permutations(inputs):\n        ground_values = {}\n        dependent_values = {}\n        for i, name in enumerate(order):\n            upstream = OrderedDict([(k, inputs[k]) for k in order[:i] if k in int_inputs])\n            value = random_tensor(upstream, inputs[name])\n            ground_values[name] = value(**ground_values)\n            dependent_values[name] = value\n\n        expected = g(**ground_values)\n        actual = g\n        for k in reversed(order):\n            actual = actual(**{k: dependent_values[k]})\n        assert_close(actual, expected, atol=1e-5, rtol=1e-4)\n\n\ndef test_eager_subs_variable():\n    inputs = OrderedDict([(\'i\', bint(2)), (\'x\', reals()), (\'y\', reals(2))])\n    g1 = random_gaussian(inputs)\n\n    g2 = g1(x=\'z\')\n    assert set(g2.inputs) == {\'i\', \'y\', \'z\'}\n    assert g2.info_vec is g1.info_vec\n    assert g2.precision is g1.precision\n\n    g2 = g1(x=\'y\', y=\'x\')\n    assert set(g2.inputs) == {\'i\', \'x\', \'y\'}\n    assert g2.inputs[\'x\'] == reals(2)\n    assert g2.info_vec is g1.info_vec\n    assert g2.precision is g1.precision\n\n    g2 = g1(i=\'j\')\n    assert set(g2.inputs) == {\'j\', \'x\', \'y\'}\n    assert g2.info_vec is g1.info_vec\n    assert g2.precision is g1.precision\n\n\n@pytest.mark.parametrize(\'subs\', [\n    ((\'x\', \'Variable(""u"", reals()) * 2\'),),\n    ((\'y\', \'Variable(""v"", reals(4)) + 1\'),),\n    ((\'z\', \'Variable(""w"", reals(6)).reshape((2,3))\'),),\n    ((\'x\', \'Variable(""v"", reals(4)).sum()\'),\n     (\'y\', \'Variable(""v"", reals(4)) - 1\')),\n    ((\'x\', \'Variable(""u"", reals()) * 2 + 1\'),\n     (\'y\', \'Variable(""u"", reals()) * Tensor(ones((4,)))\'),\n     (\'z\', \'Variable(""u"", reals()) * Tensor(ones((2, 3)))\')),\n    ((\'y\', \'Einsum(""abc,bc->a"", (Tensor(randn((4, 3, 5))), Variable(""v"", reals(3, 5))))\'),),\n])\n@pytest.mark.parametrize(\'g_ints\', ["""", ""i"", ""j"", ""ij""])\n@pytest.mark.parametrize(\'subs_ints\', ["""", ""i"", ""j"", ""ji""])\ndef test_eager_subs_affine(subs, g_ints, subs_ints):\n    sizes = {\'i\': 5, \'j\': 6}\n    subs_inputs = OrderedDict((k, bint(sizes[k])) for k in subs_ints)\n    g_inputs = OrderedDict((k, bint(sizes[k])) for k in g_ints)\n    g_inputs[\'x\'] = reals()\n    g_inputs[\'y\'] = reals(4)\n    g_inputs[\'z\'] = reals(2, 3)\n    g = random_gaussian(g_inputs)\n    subs = {k: eval(v) + random_tensor(subs_inputs) for k, v in subs}\n\n    inputs = g.inputs.copy()\n    for v in subs.values():\n        inputs.update(v.inputs)\n    grounding_subs = {k: random_tensor(OrderedDict(), d) for k, d in inputs.items()}\n    ground_subs = {k: v(**grounding_subs) for k, v in subs.items()}\n\n    g_subs = g(**subs)\n    assert issubclass(type(g_subs), GaussianMixture)\n    actual = g_subs(**grounding_subs)\n    expected = g(**ground_subs)(**grounding_subs)\n    assert_close(actual, expected, atol=1e-3, rtol=2e-4)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n    {\'i\': bint(2), \'j\': bint(3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals()},\n    {\'x\': reals(4)},\n    {\'x\': reals(2, 3)},\n    {\'x\': reals(), \'y\': reals()},\n    {\'x\': reals(2), \'y\': reals(3)},\n    {\'x\': reals(4), \'y\': reals(2, 3), \'z\': reals()},\n], ids=id_from_inputs)\ndef test_add_gaussian_number(int_inputs, real_inputs):\n    int_inputs = OrderedDict(sorted(int_inputs.items()))\n    real_inputs = OrderedDict(sorted(real_inputs.items()))\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n\n    g = random_gaussian(inputs)\n    n = Number(1.234)\n    values = {name: random_tensor(int_inputs, domain)\n              for name, domain in real_inputs.items()}\n\n    assert_close((g + n)(**values), g(**values) + n, atol=1e-5, rtol=1e-5)\n    assert_close((n + g)(**values), n + g(**values), atol=1e-5, rtol=1e-5)\n    assert_close((g - n)(**values), g(**values) - n, atol=1e-5, rtol=1e-5)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n    {\'i\': bint(2), \'j\': bint(3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals()},\n    {\'x\': reals(4)},\n    {\'x\': reals(2, 3)},\n    {\'x\': reals(), \'y\': reals()},\n    {\'x\': reals(2), \'y\': reals(3)},\n    {\'x\': reals(4), \'y\': reals(2, 3), \'z\': reals()},\n], ids=id_from_inputs)\ndef test_add_gaussian_tensor(int_inputs, real_inputs):\n    int_inputs = OrderedDict(sorted(int_inputs.items()))\n    real_inputs = OrderedDict(sorted(real_inputs.items()))\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n\n    g = random_gaussian(inputs)\n    t = random_tensor(int_inputs, reals())\n    values = {name: random_tensor(int_inputs, domain)\n              for name, domain in real_inputs.items()}\n\n    assert_close((g + t)(**values), g(**values) + t, atol=1e-5, rtol=1e-5)\n    assert_close((t + g)(**values), t + g(**values), atol=1e-5, rtol=1e-5)\n    assert_close((g - t)(**values), g(**values) - t, atol=1e-5, rtol=1e-5)\n\n\n@pytest.mark.parametrize(\'lhs_inputs\', [\n    {\'x\': reals()},\n    {\'y\': reals(4)},\n    {\'z\': reals(2, 3)},\n    {\'x\': reals(), \'y\': reals(4)},\n    {\'y\': reals(4), \'z\': reals(2, 3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'rhs_inputs\', [\n    {\'x\': reals()},\n    {\'y\': reals(4)},\n    {\'z\': reals(2, 3)},\n    {\'x\': reals(), \'y\': reals(4)},\n    {\'y\': reals(4), \'z\': reals(2, 3)},\n], ids=id_from_inputs)\ndef test_add_gaussian_gaussian(lhs_inputs, rhs_inputs):\n    lhs_inputs = OrderedDict(sorted(lhs_inputs.items()))\n    rhs_inputs = OrderedDict(sorted(rhs_inputs.items()))\n    inputs = lhs_inputs.copy()\n    inputs.update(rhs_inputs)\n    int_inputs = OrderedDict((k, d) for k, d in inputs.items() if d.dtype != \'real\')\n    real_inputs = OrderedDict((k, d) for k, d in inputs.items() if d.dtype == \'real\')\n\n    g1 = random_gaussian(lhs_inputs)\n    g2 = random_gaussian(rhs_inputs)\n    values = {name: random_tensor(int_inputs, domain)\n              for name, domain in real_inputs.items()}\n\n    assert_close((g1 + g2)(**values), g1(**values) + g2(**values), atol=1e-4, rtol=None)\n\n\n@pytest.mark.parametrize(\'inputs\', [\n    OrderedDict([(\'i\', bint(2)), (\'x\', reals())]),\n    OrderedDict([(\'i\', bint(3)), (\'x\', reals())]),\n    OrderedDict([(\'i\', bint(2)), (\'x\', reals(2))]),\n    OrderedDict([(\'i\', bint(2)), (\'x\', reals()), (\'y\', reals())]),\n    OrderedDict([(\'i\', bint(3)), (\'j\', bint(4)), (\'x\', reals(2))]),\n], ids=id_from_inputs)\ndef test_reduce_add(inputs):\n    g = random_gaussian(inputs)\n    actual = g.reduce(ops.add, \'i\')\n\n    gs = [g(i=i) for i in range(g.inputs[\'i\'].dtype)]\n    expected = reduce(ops.add, gs)\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n    {\'i\': bint(2), \'j\': bint(3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals(), \'y\': reals()},\n    {\'x\': reals(2), \'y\': reals(3)},\n    {\'x\': reals(4), \'y\': reals(2, 3), \'z\': reals()},\n    {\'w\': reals(5), \'x\': reals(4), \'y\': reals(2, 3), \'z\': reals()},\n], ids=id_from_inputs)\ndef test_reduce_logsumexp(int_inputs, real_inputs):\n    int_inputs = OrderedDict(sorted(int_inputs.items()))\n    real_inputs = OrderedDict(sorted(real_inputs.items()))\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n\n    g = random_gaussian(inputs)\n    g_xy = g.reduce(ops.logaddexp, frozenset([\'x\', \'y\']))\n    assert_close(g_xy, g.reduce(ops.logaddexp, \'x\').reduce(ops.logaddexp, \'y\'), atol=1e-3, rtol=None)\n    assert_close(g_xy, g.reduce(ops.logaddexp, \'y\').reduce(ops.logaddexp, \'x\'), atol=1e-3, rtol=None)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals()},\n    {\'x\': reals(4)},\n    {\'x\': reals(2, 3)},\n], ids=id_from_inputs)\ndef test_integrate_variable(int_inputs, real_inputs):\n    int_inputs = OrderedDict(sorted(int_inputs.items()))\n    real_inputs = OrderedDict(sorted(real_inputs.items()))\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n\n    log_measure = random_gaussian(inputs)\n    integrand = reduce(ops.add, [Variable(k, d) for k, d in real_inputs.items()])\n    reduced_vars = frozenset(real_inputs)\n\n    rng_key = None if get_backend() != \'jax\' else np.array([0, 0], dtype=np.uint32)\n    sampled_log_measure = log_measure.sample(reduced_vars, OrderedDict(particle=bint(100000)), rng_key=rng_key)\n    approx = Integrate(sampled_log_measure, integrand, reduced_vars | {\'particle\'})\n    assert isinstance(approx, Tensor)\n\n    exact = Integrate(log_measure, integrand, reduced_vars)\n    assert isinstance(exact, Tensor)\n    assert_close(approx, exact, atol=0.1, rtol=0.1)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n    {\'i\': bint(2), \'j\': bint(3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals()},\n    {\'x\': reals(2)},\n    {\'x\': reals(), \'y\': reals()},\n    {\'x\': reals(2), \'y\': reals(3)},\n    {\'x\': reals(4), \'y\': reals(2, 3)},\n], ids=id_from_inputs)\ndef test_integrate_gaussian(int_inputs, real_inputs):\n    int_inputs = OrderedDict(sorted(int_inputs.items()))\n    real_inputs = OrderedDict(sorted(real_inputs.items()))\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n\n    log_measure = random_gaussian(inputs)\n    integrand = random_gaussian(inputs)\n    reduced_vars = frozenset(real_inputs)\n\n    rng_key = None if get_backend() != \'jax\' else np.array([0, 0], dtype=np.uint32)\n    sampled_log_measure = log_measure.sample(reduced_vars, OrderedDict(particle=bint(100000)), rng_key=rng_key)\n    approx = Integrate(sampled_log_measure, integrand, reduced_vars | {\'particle\'})\n    assert isinstance(approx, Tensor)\n\n    exact = Integrate(log_measure, integrand, reduced_vars)\n    assert isinstance(exact, Tensor)\n    assert_close(approx, exact, atol=0.1, rtol=0.1)\n\n\n@pytest.mark.xfail(get_backend() == \'torch\', reason=""numerically unstable in torch backend"")\ndef test_mc_plate_gaussian():\n    log_measure = Gaussian(numeric_array([0.]), numeric_array([[1.]]),\n                           ((\'loc\', reals()),)) + numeric_array(-0.9189)\n    integrand = Gaussian(randn((100, 1)) + 3., ones((100, 1, 1)),\n                         ((\'data\', bint(100)), (\'loc\', reals())))\n\n    rng_key = None if get_backend() != \'jax\' else np.array([0, 0], dtype=np.uint32)\n    res = Integrate(log_measure.sample(\'loc\', rng_key=rng_key), integrand, \'loc\')\n    res = res.reduce(ops.mul, \'data\')\n    assert not ((res == float(\'inf\')) | (res == float(\'-inf\'))).any()\n'"
test/test_import.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport glob\nimport os\nfrom importlib import import_module\n\nfrom funsor import get_backend\n\n\ndef test_all_modules_are_imported():\n    root = os.path.join(os.path.dirname(os.path.dirname(__file__)), \'funsor\')\n    for path in glob.glob(os.path.join(root, \'*.py\')):\n        name = os.path.basename(path)[:-3]\n        if name in ({\'torch\', \'jax\'} - {get_backend()}):\n            assert not hasattr(import_module(\'funsor\'), name)\n            continue\n        if name.startswith(\'__\'):\n            continue\n        if name == ""minipyro"":\n            continue  # TODO: enable when minipyro is backend-agnostic\n        assert hasattr(import_module(\'funsor\'), name), f\'funsor/__init__.py does not import {name}\'\n        actual = getattr(import_module(\'funsor\'), name)\n        expected = import_module(f\'funsor.{name}\')\n        assert actual == expected\n'"
test/test_integrate.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport pytest\n\nfrom funsor import ops\nfrom funsor.domains import bint\nfrom funsor.integrate import Integrate\nfrom funsor.interpreter import interpretation\nfrom funsor.montecarlo import monte_carlo\nfrom funsor.terms import Variable, eager, lazy, moment_matching, normalize, reflect\nfrom funsor.testing import assert_close, random_tensor\nfrom funsor.util import get_backend\n\n\n@pytest.mark.parametrize(\'interp\', [\n    reflect, lazy, normalize, eager, moment_matching,\n    pytest.param(monte_carlo, marks=pytest.mark.xfail(\n        get_backend() == ""jax"", reason=""Lacking pattern to pass rng_key""))\n])\ndef test_integrate(interp):\n    log_measure = random_tensor(OrderedDict([(\'i\', bint(2)), (\'j\', bint(3))]))\n    integrand = random_tensor(OrderedDict([(\'j\', bint(3)), (\'k\', bint(4))]))\n    with interpretation(interp):\n        Integrate(log_measure, integrand, {\'i\', \'j\', \'k\'})\n\n\ndef test_syntactic_sugar():\n    i = Variable(""i"", bint(3))\n    log_measure = random_tensor(OrderedDict(i=bint(3)))\n    integrand = random_tensor(OrderedDict(i=bint(3)))\n    expected = (log_measure.exp() * integrand).reduce(ops.add, ""i"")\n    assert_close(Integrate(log_measure, integrand, ""i""), expected)\n    assert_close(Integrate(log_measure, integrand, {""i""}), expected)\n    assert_close(Integrate(log_measure, integrand, frozenset([""i""])), expected)\n    assert_close(Integrate(log_measure, integrand, i), expected)\n    assert_close(Integrate(log_measure, integrand, {i}), expected)\n    assert_close(Integrate(log_measure, integrand, frozenset([i])), expected)\n'"
test/test_joint.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\nfrom functools import reduce\n\nimport numpy as np\nimport pytest\n\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction\nfrom funsor.delta import Delta\nfrom funsor.domains import bint, reals\nfrom funsor.gaussian import Gaussian\nfrom funsor.integrate import Integrate\nfrom funsor.interpreter import interpretation\nfrom funsor.montecarlo import monte_carlo_interpretation\nfrom funsor.tensor import Tensor, numeric_array\nfrom funsor.terms import Number, Variable, eager, moment_matching\nfrom funsor.testing import (assert_close, randn, random_gaussian, random_tensor,\n                            zeros, xfail_if_not_implemented)\nfrom funsor.util import get_backend\n\n\ndef id_from_inputs(inputs):\n    if not inputs:\n        return \'()\'\n    return \',\'.join(k + \'\'.join(map(str, d.shape)) for k, d in inputs.items())\n\n\nSMOKE_TESTS = [\n    (\'dx + dy\', Delta),\n    (\'dx + g\', Contraction),\n    (\'dy + g\', Contraction),\n    (\'g + dx\', Contraction),\n    (\'g + dy\', Contraction),\n    (\'dx + t\', Contraction),\n    (\'dy + t\', Contraction),\n    (\'dx - t\', Contraction),\n    (\'dy - t\', Contraction),\n    (\'t + dx\', Contraction),\n    (\'t + dy\', Contraction),\n    (\'g + 1\', Contraction),\n    (\'g - 1\', Contraction),\n    (\'1 + g\', Contraction),\n    (\'g + t\', Contraction),\n    (\'g - t\', Contraction),\n    (\'t + g\', Contraction),\n    (\'t - g\', Contraction),\n    (\'g + g\', Gaussian),\n    (\'-(g + g)\', Gaussian),\n    (\'(dx + dy)(i=i0)\', Delta),\n    (\'(dx + g)(i=i0)\', Contraction),\n    (\'(dy + g)(i=i0)\', Contraction),\n    (\'(g + dx)(i=i0)\', Contraction),\n    (\'(g + dy)(i=i0)\', Contraction),\n    (\'(dx + t)(i=i0)\', Contraction),\n    (\'(dy + t)(i=i0)\', Contraction),\n    (\'(dx - t)(i=i0)\', Contraction),\n    (\'(dy - t)(i=i0)\', Contraction),\n    (\'(t + dx)(i=i0)\', Contraction),\n    (\'(t + dy)(i=i0)\', Contraction),\n    (\'(g + 1)(i=i0)\', Contraction),\n    (\'(g - 1)(i=i0)\', Contraction),\n    (\'(1 + g)(i=i0)\', Contraction),\n    (\'(g + t)(i=i0)\', Contraction),\n    (\'(g - t)(i=i0)\', Contraction),\n    (\'(t + g)(i=i0)\', Contraction),\n    (\'(g + g)(i=i0)\', Gaussian),\n    (\'(dx + dy)(x=x0)\', Contraction),\n    (\'(dx + g)(x=x0)\', Tensor),\n    (\'(dy + g)(x=x0)\', Contraction),\n    (\'(g + dx)(x=x0)\', Tensor),\n    (\'(g + dy)(x=x0)\', Contraction),\n    (\'(dx + t)(x=x0)\', Tensor),\n    (\'(dy + t)(x=x0)\', Contraction),\n    (\'(dx - t)(x=x0)\', Tensor),\n    (\'(dy - t)(x=x0)\', Contraction),\n    (\'(t + dx)(x=x0)\', Tensor),\n    (\'(t + dy)(x=x0)\', Contraction),\n    (\'(g + 1)(x=x0)\', Tensor),\n    (\'(g - 1)(x=x0)\', Tensor),\n    (\'(1 + g)(x=x0)\', Tensor),\n    (\'(g + t)(x=x0)\', Tensor),\n    (\'(g - t)(x=x0)\', Tensor),\n    (\'(t + g)(x=x0)\', Tensor),\n    (\'(g + g)(x=x0)\', Tensor),\n    (\'(g + dy).reduce(ops.logaddexp, ""x"")\', Contraction),\n    (\'(g + dy).reduce(ops.logaddexp, ""y"")\', Contraction),\n    (\'(t + g + dy).reduce(ops.logaddexp, ""x"")\', Contraction),\n    (\'(t + g + dy).reduce(ops.logaddexp, ""y"")\', Contraction),\n    (\'(t + g).reduce(ops.logaddexp, ""x"")\', Tensor),\n]\n\n\n@pytest.mark.parametrize(\'expr,expected_type\', SMOKE_TESTS)\ndef test_smoke(expr, expected_type):\n    dx = Delta(\'x\', Tensor(randn(2, 3), OrderedDict([(\'i\', bint(2))])))\n    assert isinstance(dx, Delta)\n\n    dy = Delta(\'y\', Tensor(randn(3, 4), OrderedDict([(\'j\', bint(3))])))\n    assert isinstance(dy, Delta)\n\n    t = Tensor(randn(2, 3), OrderedDict([(\'i\', bint(2)), (\'j\', bint(3))]))\n    assert isinstance(t, Tensor)\n\n    g = Gaussian(\n        info_vec=numeric_array([[0.0, 0.1, 0.2],\n                                [2.0, 3.0, 4.0]]),\n        precision=numeric_array([[[1.0, 0.1, 0.2],\n                                  [0.1, 1.0, 0.3],\n                                  [0.2, 0.3, 1.0]],\n                                 [[1.0, 0.1, 0.2],\n                                  [0.1, 1.0, 0.3],\n                                  [0.2, 0.3, 1.0]]]),\n        inputs=OrderedDict([(\'i\', bint(2)), (\'x\', reals(3))]))\n    assert isinstance(g, Gaussian)\n\n    i0 = Number(1, 2)\n    assert isinstance(i0, Number)\n\n    x0 = Tensor(numeric_array([0.5, 0.6, 0.7]))\n    assert isinstance(x0, Tensor)\n\n    result = eval(expr)\n    assert isinstance(result, expected_type)\n\n\n@pytest.mark.parametrize(\'int_inputs\', [\n    {},\n    {\'i\': bint(2)},\n    {\'i\': bint(2), \'j\': bint(3)},\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_inputs\', [\n    {\'x\': reals()},\n    {\'x\': reals(4)},\n    {\'x\': reals(2, 3)},\n    {\'x\': reals(), \'y\': reals()},\n    {\'x\': reals(2), \'y\': reals(3)},\n    {\'x\': reals(4), \'y\': reals(2, 3), \'z\': reals()},\n], ids=id_from_inputs)\ndef test_reduce_logaddexp(int_inputs, real_inputs):\n    int_inputs = OrderedDict(sorted(int_inputs.items()))\n    real_inputs = OrderedDict(sorted(real_inputs.items()))\n    inputs = int_inputs.copy()\n    inputs.update(real_inputs)\n\n    t = random_tensor(int_inputs)\n    g = random_gaussian(inputs)\n    truth = {name: random_tensor(int_inputs, domain) for name, domain in real_inputs.items()}\n\n    state = 0\n    state += g\n    state += t\n    for name, point in truth.items():\n        with xfail_if_not_implemented():\n            state += Delta(name, point)\n    actual = state.reduce(ops.logaddexp, frozenset(truth))\n\n    expected = t + g(**truth)\n    assert_close(actual, expected, atol=1e-5, rtol=1e-4 if get_backend() == ""jax"" else 1e-5)\n\n\ndef test_reduce_logaddexp_deltas_lazy():\n    a = Delta(\'a\', Tensor(randn(3, 2), OrderedDict(i=bint(3))))\n    b = Delta(\'b\', Tensor(randn(3), OrderedDict(i=bint(3))))\n    x = a + b\n    assert isinstance(x, Delta)\n    assert set(x.inputs) == {\'a\', \'b\', \'i\'}\n\n    y = x.reduce(ops.logaddexp, \'i\')\n    # assert isinstance(y, Reduce)\n    assert set(y.inputs) == {\'a\', \'b\'}\n    assert_close(x.reduce(ops.logaddexp), y.reduce(ops.logaddexp))\n\n\ndef test_reduce_logaddexp_deltas_discrete_lazy():\n    a = Delta(\'a\', Tensor(randn(3, 2), OrderedDict(i=bint(3))))\n    b = Delta(\'b\', Tensor(randn(3), OrderedDict(i=bint(3))))\n    c = Tensor(randn(3), OrderedDict(i=bint(3)))\n    x = a + b + c\n    assert isinstance(x, Contraction)\n    assert set(x.inputs) == {\'a\', \'b\', \'i\'}\n\n    y = x.reduce(ops.logaddexp, \'i\')\n    # assert isinstance(y, Reduce)\n    assert set(y.inputs) == {\'a\', \'b\'}\n    assert_close(x.reduce(ops.logaddexp), y.reduce(ops.logaddexp))\n\n\ndef test_reduce_logaddexp_gaussian_lazy():\n    a = random_gaussian(OrderedDict(i=bint(3), a=reals(2)))\n    b = random_tensor(OrderedDict(i=bint(3), b=bint(2)))\n    x = a + b\n    assert isinstance(x, Contraction)\n    assert set(x.inputs) == {\'a\', \'b\', \'i\'}\n\n    y = x.reduce(ops.logaddexp, \'i\')\n    # assert isinstance(y, Reduce)\n    assert set(y.inputs) == {\'a\', \'b\'}\n    assert_close(x.reduce(ops.logaddexp), y.reduce(ops.logaddexp))\n\n\n@pytest.mark.parametrize(\'inputs\', [\n    OrderedDict([(\'i\', bint(2)), (\'x\', reals())]),\n    OrderedDict([(\'i\', bint(3)), (\'x\', reals())]),\n    OrderedDict([(\'i\', bint(2)), (\'x\', reals(2))]),\n    OrderedDict([(\'i\', bint(2)), (\'x\', reals()), (\'y\', reals())]),\n    OrderedDict([(\'i\', bint(3)), (\'j\', bint(4)), (\'x\', reals(2))]),\n    OrderedDict([(\'j\', bint(2)), (\'i\', bint(3)), (\'k\', bint(2)), (\'x\', reals(2))]),\n], ids=id_from_inputs)\ndef test_reduce_add(inputs):\n    int_inputs = OrderedDict((k, d) for k, d in inputs.items() if d.dtype != \'real\')\n    x = random_gaussian(inputs) + random_tensor(int_inputs)\n    assert isinstance(x, Contraction)\n    actual = x.reduce(ops.add, \'i\')\n\n    xs = [x(i=i) for i in range(x.inputs[\'i\'].dtype)]\n    expected = reduce(ops.add, xs)\n    assert_close(actual, expected, atol=1e-3, rtol=1e-4)\n\n\ndef test_reduce_moment_matching_univariate():\n    int_inputs = [(\'i\', bint(2))]\n    real_inputs = [(\'x\', reals())]\n    inputs = OrderedDict(int_inputs + real_inputs)\n    int_inputs = OrderedDict(int_inputs)\n    real_inputs = OrderedDict(real_inputs)\n\n    p = 0.8\n    t = 1.234\n    s1, s2, s3 = 2.0, 3.0, 4.0\n    loc = numeric_array([[-s1], [s1]])\n    precision = numeric_array([[[s2 ** -2]], [[s3 ** -2]]])\n    info_vec = (precision @ ops.unsqueeze(loc, -1)).squeeze(-1)\n    discrete = Tensor(ops.log(numeric_array([1 - p, p])) + t, int_inputs)\n    gaussian = Gaussian(info_vec, precision, inputs)\n    gaussian -= gaussian.log_normalizer\n    joint = discrete + gaussian\n    with interpretation(moment_matching):\n        actual = joint.reduce(ops.logaddexp, \'i\')\n    assert_close(actual.reduce(ops.logaddexp), joint.reduce(ops.logaddexp))\n\n    expected_loc = numeric_array([(2 * p - 1) * s1])\n    expected_variance = (4 * p * (1 - p) * s1 ** 2\n                         + (1 - p) * s2 ** 2\n                         + p * s3 ** 2)\n    expected_precision = numeric_array([[1 / expected_variance]])\n    expected_info_vec = (expected_precision @ ops.unsqueeze(expected_loc, -1)).squeeze(-1)\n    expected_gaussian = Gaussian(expected_info_vec, expected_precision, real_inputs)\n    expected_gaussian -= expected_gaussian.log_normalizer\n    expected_discrete = Tensor(numeric_array(t))\n    expected = expected_discrete + expected_gaussian\n    assert_close(actual, expected, atol=1e-5, rtol=None)\n\n\ndef _inverse(x):\n    if get_backend() == ""torch"":\n        return x.inverse()\n    else:\n        return np.linalg.inv(x)\n\n\ndef test_reduce_moment_matching_multivariate():\n    int_inputs = [(\'i\', bint(4))]\n    real_inputs = [(\'x\', reals(2))]\n    inputs = OrderedDict(int_inputs + real_inputs)\n    int_inputs = OrderedDict(int_inputs)\n    real_inputs = OrderedDict(real_inputs)\n\n    loc = numeric_array([[-10., -1.],\n                         [+10., -1.],\n                         [+10., +1.],\n                         [-10., +1.]])\n    precision = zeros(4, 1, 1) + ops.new_eye(loc, (2,))\n    discrete = Tensor(zeros(4), int_inputs)\n    gaussian = Gaussian(loc, precision, inputs)\n    gaussian -= gaussian.log_normalizer\n    joint = discrete + gaussian\n    with interpretation(moment_matching):\n        actual = joint.reduce(ops.logaddexp, \'i\')\n    assert_close(actual.reduce(ops.logaddexp), joint.reduce(ops.logaddexp))\n\n    expected_loc = zeros(2)\n    expected_covariance = numeric_array([[101., 0.], [0., 2.]])\n    expected_precision = _inverse(expected_covariance)\n    expected_gaussian = Gaussian(expected_loc, expected_precision, real_inputs)\n    expected_gaussian -= expected_gaussian.log_normalizer\n    expected_discrete = Tensor(ops.log(numeric_array(4.)))\n    expected = expected_discrete + expected_gaussian\n    assert_close(actual, expected, atol=1e-5, rtol=None)\n\n\n@pytest.mark.parametrize(\'interp\', [eager, moment_matching],\n                         ids=lambda f: f.__name__)\ndef test_reduce_moment_matching_shape(interp):\n    delta = Delta(\'x\', random_tensor(OrderedDict([(\'h\', bint(7))])))\n    discrete = random_tensor(OrderedDict(\n        [(\'h\', bint(7)), (\'i\', bint(6)), (\'j\', bint(5)), (\'k\', bint(4))]))\n    gaussian = random_gaussian(OrderedDict(\n        [(\'k\', bint(4)), (\'l\', bint(3)), (\'m\', bint(2)), (\'y\', reals()), (\'z\', reals(2))]))\n    reduced_vars = frozenset([\'i\', \'k\', \'l\'])\n    real_vars = frozenset(k for k, d in gaussian.inputs.items() if d.dtype == ""real"")\n    joint = delta + discrete + gaussian\n    with interpretation(interp):\n        actual = joint.reduce(ops.logaddexp, reduced_vars)\n    assert set(actual.inputs) == set(joint.inputs) - reduced_vars\n    assert_close(actual.reduce(ops.logaddexp, real_vars),\n                 joint.reduce(ops.logaddexp, real_vars | reduced_vars))\n\n\n@pytest.mark.xfail(reason=""missing pattern"")\ndef test_reduce_moment_matching_moments():\n    x = Variable(\'x\', reals(2))\n    gaussian = random_gaussian(OrderedDict(\n        [(\'i\', bint(2)), (\'j\', bint(3)), (\'x\', reals(2))]))\n    with interpretation(moment_matching):\n        approx = gaussian.reduce(ops.logaddexp, \'j\')\n    with monte_carlo_interpretation(s=bint(100000)):\n        actual = Integrate(approx, Number(1.), \'x\')\n        expected = Integrate(gaussian, Number(1.), {\'j\', \'x\'})\n        assert_close(actual, expected, atol=1e-3, rtol=1e-3)\n\n        actual = Integrate(approx, x, \'x\')\n        expected = Integrate(gaussian, x, {\'j\', \'x\'})\n        assert_close(actual, expected, atol=1e-2, rtol=1e-2)\n\n        actual = Integrate(approx, x * x, \'x\')\n        expected = Integrate(gaussian, x * x, {\'j\', \'x\'})\n        assert_close(actual, expected, atol=1e-2, rtol=1e-2)\n\n\ndef test_reduce_moment_matching_finite():\n    delta = Delta(\'x\', random_tensor(OrderedDict([(\'h\', bint(7))])))\n    discrete = random_tensor(OrderedDict(\n        [(\'i\', bint(6)), (\'j\', bint(5)), (\'k\', bint(3))]))\n    gaussian = random_gaussian(OrderedDict(\n        [(\'k\', bint(3)), (\'l\', bint(2)), (\'y\', reals()), (\'z\', reals(2))]))\n\n    discrete.data[1:, :] = -float(\'inf\')\n    discrete.data[:, 1:] = -float(\'inf\')\n\n    reduced_vars = frozenset([\'j\', \'k\'])\n    joint = delta + discrete + gaussian\n    with interpretation(moment_matching):\n        joint.reduce(ops.logaddexp, reduced_vars)\n'"
test/test_memoize.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as np\nimport pytest\n\nimport funsor.ops as ops\nfrom funsor.cnf import BACKEND_TO_EINSUM_BACKEND, BACKEND_TO_LOGSUMEXP_BACKEND\nfrom funsor.einsum import einsum, naive_plated_einsum\nfrom funsor.interpreter import interpretation, reinterpret\nfrom funsor.memoize import memoize\nfrom funsor.tensor import numeric_array\nfrom funsor.terms import reflect\nfrom funsor.testing import make_einsum_example, xfail_param\nfrom funsor.util import get_backend\n\n\nEINSUM_EXAMPLES = [\n    (""a,b->"", \'\'),\n    (""ab,a->"", \'\'),\n    (""a,a->"", \'\'),\n    (""a,a->a"", \'\'),\n    (""ab,bc,cd->da"", \'\'),\n    (""ab,cd,bc->da"", \'\'),\n    (""a,a,a,ab->ab"", \'\'),\n    (\'i->\', \'i\'),\n    (\',i->\', \'i\'),\n    (\'ai->\', \'i\'),\n    (\',ai,abij->\', \'ij\'),\n    (\'a,ai,bij->\', \'ij\'),\n    (\'ai,abi,bci,cdi->\', \'i\'),\n    (\'aij,abij,bcij->\', \'ij\'),\n    (\'a,abi,bcij,cdij->\', \'ij\'),\n]\n\n\ndef backend_to_einsum_backends(backend):\n    backends = [BACKEND_TO_EINSUM_BACKEND[get_backend()],\n                BACKEND_TO_LOGSUMEXP_BACKEND[get_backend()]]\n    return backends[:1]\n\n\n@pytest.mark.parametrize(\'equation,plates\', EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', backend_to_einsum_backends(get_backend()))\n@pytest.mark.parametrize(\'einsum_impl,same_lazy\', [\n    (einsum, True),\n    (einsum, xfail_param(False, reason=""nested interpreters?"")),\n    (naive_plated_einsum, True),\n    (naive_plated_einsum, False)\n])\ndef test_einsum_complete_sharing(equation, plates, backend, einsum_impl, same_lazy):\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n\n    with interpretation(reflect):\n        lazy_expr1 = einsum_impl(equation, *funsor_operands, backend=backend, plates=plates)\n        lazy_expr2 = lazy_expr1 if same_lazy else \\\n            einsum_impl(equation, *funsor_operands, backend=backend, plates=plates)\n\n    with memoize():\n        expr1 = reinterpret(lazy_expr1)\n        expr2 = reinterpret(lazy_expr2)\n    expr3 = reinterpret(lazy_expr1)\n\n    assert expr1 is expr2\n    assert expr1 is not expr3\n\n\n@pytest.mark.parametrize(\'equation,plates\', EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', backend_to_einsum_backends(get_backend()))\n@pytest.mark.parametrize(\'einsum_impl,same_lazy\', [\n    (einsum, True),\n    (einsum, xfail_param(False, reason=""nested interpreters?"")),\n    (naive_plated_einsum, True),\n    (naive_plated_einsum, False)\n])\ndef test_einsum_complete_sharing_reuse_cache(equation, plates, backend, einsum_impl, same_lazy):\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n\n    with interpretation(reflect):\n        lazy_expr1 = einsum_impl(equation, *funsor_operands, backend=backend, plates=plates)\n        lazy_expr2 = lazy_expr1 if same_lazy else \\\n            einsum_impl(equation, *funsor_operands, backend=backend, plates=plates)\n\n    cache = {}\n    with memoize(cache) as cache:\n        expr1 = reinterpret(lazy_expr1)\n\n    with memoize(cache):\n        expr2 = reinterpret(lazy_expr2)\n\n    expr3 = reinterpret(lazy_expr1)\n\n    assert expr1 is expr2\n    assert expr1 is not expr3\n\n\n@pytest.mark.parametrize(\'check_sample\', [\n    False, xfail_param(True, reason=""Joint.sample cannot directly be memoized in this way yet"")])\n@pytest.mark.skipif(get_backend() == ""numpy"", reason=""there is no numpy distributions backend"")\ndef test_memoize_sample(check_sample):\n    if get_backend() == ""jax"":\n        from funsor.jax.distributions import Normal\n    else:\n        from funsor.torch.distributions import Normal\n\n    rng_keys = (None, None, None) if get_backend() == ""torch"" \\\n        else np.array([[0, 1], [0, 2], [0, 3]], dtype=np.uint32)\n\n    with memoize():\n        m, s = numeric_array(0.), numeric_array(1.)\n        j1 = Normal(m, s, \'x\')\n        j2 = Normal(m, s, \'x\')\n        x1 = j1.sample(frozenset({\'x\'}), rng_key=rng_keys[0])\n        x12 = j1.sample(frozenset({\'x\'}), rng_key=rng_keys[1])\n        x2 = j2.sample(frozenset({\'x\'}), rng_key=rng_keys[2])\n\n    # this assertion now passes\n    assert j1 is j2\n\n    # these assertions fail because sample is not memoized\n    if check_sample:\n        assert x1 is x12\n        assert x1 is x2\n\n\n@pytest.mark.parametrize(""eqn1,eqn2"", [(""ab,bc,cd->d"", ""de,ef,fg->"")])\n@pytest.mark.parametrize(""einsum_impl1"", [naive_plated_einsum, xfail_param(einsum, reason=""nested interpreters?"")])\n@pytest.mark.parametrize(""einsum_impl2"", [naive_plated_einsum, xfail_param(einsum, reason=""nested interpreters?"")])\n@pytest.mark.parametrize(\'backend1\', backend_to_einsum_backends(get_backend()))\n@pytest.mark.parametrize(\'backend2\', backend_to_einsum_backends(get_backend()))\ndef test_nested_einsum_complete_sharing(eqn1, eqn2, einsum_impl1, einsum_impl2, backend1, backend2):\n\n    inputs1, outputs1, sizes1, operands1, funsor_operands1 = make_einsum_example(eqn1, sizes=(3,))\n    inputs2, outputs2, sizes2, operands2, funsor_operands2 = make_einsum_example(eqn2, sizes=(3,))\n\n    with memoize():\n        output1_1 = einsum_impl1(eqn1, *funsor_operands1, backend=backend1)\n        output2_1 = einsum_impl2(outputs1[0] + "","" + eqn2, *([output1_1] + funsor_operands2), backend=backend2)\n\n        output1_2 = einsum_impl1(eqn1, *funsor_operands1, backend=backend1)\n        output2_2 = einsum_impl2(outputs1[0] + "","" + eqn2, *([output1_2] + funsor_operands2), backend=backend2)\n\n    assert output1_1 is output1_2\n    assert output2_1 is output2_2\n\n\ndef test_nested_complete_sharing_direct():\n\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(""ab,bc,cd->d"")\n    ab, bc, cd = funsor_operands\n\n    # avoids the complicated internal interpreter usage of the nested optimized einsum tests above\n    with interpretation(reflect):\n        c1 = (ab * bc).reduce(ops.add, frozenset({""a"", ""b""}))\n        d1 = (c1 * cd).reduce(ops.add, frozenset({""c""}))\n\n        # this does not trigger a second alpha-renaming\n        c2 = (ab * bc).reduce(ops.add, frozenset({""a"", ""b""}))\n        d2 = (c2 * cd).reduce(ops.add, frozenset({""c""}))\n\n    with memoize():\n        assert reinterpret(c1) is reinterpret(c2)\n        assert reinterpret(d1) is reinterpret(d2)\n'"
test/test_minipyro.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nimport pytest\n\nimport funsor\nfrom funsor.testing import xfail_param\nfrom funsor.util import get_backend\n\npytestmark = pytest.mark.skipif(get_backend() != ""torch"",\n                                reason=""numpy/jax backend requires porting pyro.ops.einsum"")\nif get_backend() == ""torch"":\n    import torch\n    from pyro.ops.indexing import Vindex as _Vindex\n    from pyroapi import distributions as dist\n    from pyroapi import handlers, infer, optim, pyro, pyro_backend\n    from torch.autograd import grad\n    from torch.distributions import constraints, kl_divergence\n\n    import funsor.compat.ops as ops\n\n\n# This file tests a variety of model,guide pairs with valid and invalid structure.\n# See https://github.com/pyro-ppl/pyro/blob/0.3.1/tests/infer/test_valid_models.py\n\n\ndef Vindex(x):\n    if isinstance(x, funsor.Funsor):\n        return x\n    return _Vindex(x)\n\n\ndef _check_loss_and_grads(expected_loss, actual_loss, atol=1e-4, rtol=1e-4):\n    # copied from pyro\n    expected_loss, actual_loss = funsor.to_data(expected_loss), funsor.to_data(actual_loss)\n    assert ops.allclose(actual_loss, expected_loss, atol=atol, rtol=rtol)\n    names = pyro.get_param_store().keys()\n    params = []\n    for name in names:\n        params.append(funsor.to_data(pyro.param(name)).unconstrained())\n    actual_grads = grad(actual_loss, params, allow_unused=True, retain_graph=True)\n    expected_grads = grad(expected_loss, params, allow_unused=True, retain_graph=True)\n    for name, actual_grad, expected_grad in zip(names, actual_grads, expected_grads):\n        if actual_grad is None or expected_grad is None:\n            continue\n        assert ops.allclose(actual_grad, expected_grad, atol=atol, rtol=rtol)\n\n\ndef assert_ok(model, guide, elbo, *args, **kwargs):\n    """"""\n    Assert that inference works without warnings or errors.\n    """"""\n    pyro.get_param_store().clear()\n    adam = optim.Adam({""lr"": 1e-6})\n    inference = infer.SVI(model, guide, adam, elbo)\n    for i in range(2):\n        inference.step(*args, **kwargs)\n\n\ndef assert_error(model, guide, elbo, match=None):\n    """"""\n    Assert that inference fails with an error.\n    """"""\n    pyro.get_param_store().clear()\n    adam = optim.Adam({""lr"": 1e-6})\n    inference = infer.SVI(model,  guide, adam, elbo)\n    with pytest.raises((NotImplementedError, UserWarning, KeyError, ValueError, RuntimeError),\n                       match=match):\n        inference.step()\n\n\ndef assert_warning(model, guide, elbo):\n    """"""\n    Assert that inference works but with a warning.\n    """"""\n    pyro.get_param_store().clear()\n    adam = optim.Adam({""lr"": 1e-6})\n    inference = infer.SVI(model, guide, adam, elbo)\n    with warnings.catch_warnings(record=True) as w:\n        warnings.simplefilter(""always"")\n        inference.step()\n        assert len(w), \'No warnings were raised\'\n        for warning in w:\n            print(warning)\n\n\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro"", ""funsor""])\ndef test_generate_data(backend):\n\n    def model():\n        loc = pyro.param(""loc"", torch.tensor(2.0))\n        scale = pyro.param(""scale"", torch.tensor(1.0))\n        x = pyro.sample(""x"", dist.Normal(loc, scale))\n        return x\n\n    with pyro_backend(backend):\n        data = model()\n        data = data.data\n        assert data.shape == ()\n\n\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro"", ""funsor""])\ndef test_rng_seed(backend):\n\n    def model():\n        return pyro.sample(""x"", dist.Normal(0, 1))\n\n    with pyro_backend(backend):\n        with handlers.seed(rng_seed=0):\n            expected = model()\n        with handlers.seed(rng_seed=0):\n            actual = model()\n        assert ops.allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro"", ""funsor""])\ndef test_rng_state(backend):\n\n    def model():\n        return pyro.sample(""x"", dist.Normal(0, 1))\n\n    with pyro_backend(backend):\n        with handlers.seed(rng_seed=0):\n            model()\n            expected = model()\n        with handlers.seed(rng_seed=0):\n            model()\n            with handlers.seed(rng_seed=0):\n                model()\n            actual = model()\n        assert ops.allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro"", ""funsor""])\ndef test_generate_data_plate(backend):\n    num_points = 1000\n\n    def model(data=None):\n        loc = pyro.param(""loc"", torch.tensor(2.0))\n        scale = pyro.param(""scale"", torch.tensor(1.0))\n        with pyro.plate(""data"", 1000, dim=-1):\n            x = pyro.sample(""x"", dist.Normal(loc, scale), obs=data)\n        return x\n\n    with pyro_backend(backend):\n        data = model().data\n        assert data.shape == (num_points,)\n        mean = data.sum().item() / num_points\n        assert 1.9 <= mean <= 2.1\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend,optim_name"", [\n    (""pyro"", ""Adam""),\n    (""pyro"", ""ClippedAdam""),\n    (""minipyro"", ""Adam""),\n    (""funsor"", ""Adam""),\n    (""funsor"", ""ClippedAdam""),\n])\ndef test_optimizer(backend, optim_name, jit):\n\n    def model(data):\n        p = pyro.param(""p"", torch.tensor(0.5))\n        pyro.sample(""x"", dist.Bernoulli(p), obs=data)\n\n    def guide(data):\n        pass\n\n    data = torch.tensor(0.)\n    with pyro_backend(backend):\n        pyro.get_param_store().clear()\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        optimizer = getattr(optim, optim_name)({""lr"": 1e-6})\n        inference = infer.SVI(model, guide, optimizer, elbo)\n        for i in range(2):\n            inference.step(data)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro"", ""funsor""])\ndef test_nonempty_model_empty_guide_ok(backend, jit):\n\n    def model(data):\n        loc = pyro.param(""loc"", torch.tensor(0.0))\n        pyro.sample(""x"", dist.Normal(loc, 1.), obs=data)\n\n    def guide(data):\n        pass\n\n    data = torch.tensor(2.)\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo, data)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro"", ""funsor""])\ndef test_plate_ok(backend, jit):\n    data = torch.randn(10)\n\n    def model():\n        locs = pyro.param(""locs"", torch.tensor([0.2, 0.3, 0.5]))\n        p = torch.tensor([0.2, 0.3, 0.5])\n        with pyro.plate(""plate"", len(data), dim=-1):\n            x = pyro.sample(""x"", dist.Categorical(p))\n            pyro.sample(""obs"", dist.Normal(locs[x], 1.), obs=data)\n\n    def guide():\n        p = pyro.param(""p"", torch.tensor([0.5, 0.3, 0.2]))\n        with pyro.plate(""plate"", len(data), dim=-1):\n            pyro.sample(""x"", dist.Categorical(p))\n\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro"", ""funsor""])\ndef test_nested_plate_plate_ok(backend, jit):\n    data = torch.randn(2, 3)\n\n    def model():\n        loc = torch.tensor(3.0)\n        with pyro.plate(""plate_outer"", data.size(-1), dim=-1):\n            x = pyro.sample(""x"", dist.Normal(loc, 1.))\n            with pyro.plate(""plate_inner"", data.size(-2), dim=-2):\n                pyro.sample(""y"", dist.Normal(x, 1.), obs=data)\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        scale = pyro.param(""scale"", torch.tensor(1.))\n        with pyro.plate(""plate_outer"", data.size(-1), dim=-1):\n            pyro.sample(""x"", dist.Normal(loc, scale))\n\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""funsor""])\ndef test_local_param_ok(backend, jit):\n    data = torch.randn(10)\n\n    def model():\n        locs = pyro.param(""locs"", torch.tensor([-1., 0., 1.]))\n        with pyro.plate(""plate"", len(data), dim=-1):\n            x = pyro.sample(""x"", dist.Categorical(torch.ones(3) / 3))\n            pyro.sample(""obs"", dist.Normal(locs[x], 1.), obs=data)\n\n    def guide():\n        with pyro.plate(""plate"", len(data), dim=-1):\n            p = pyro.param(""p"", torch.ones(len(data), 3) / 3, event_dim=1)\n            pyro.sample(""x"", dist.Categorical(p))\n        return p\n\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo)\n\n        # Check that pyro.param() can be called without init_value.\n        expected = guide()\n        actual = pyro.param(""p"")\n        assert ops.allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""backend"", [""pyro"", ""minipyro"", ""funsor""])\ndef test_constraints(backend, jit):\n    data = torch.tensor(0.5)\n\n    def model():\n        locs = pyro.param(""locs"", torch.randn(3), constraint=constraints.real)\n        scales = pyro.param(""scales"", torch.randn(3).exp(), constraint=constraints.positive)\n        p = torch.tensor([0.5, 0.3, 0.2])\n        x = pyro.sample(""x"", dist.Categorical(p))\n        pyro.sample(""obs"", dist.Normal(locs[x], scales[x]), obs=data)\n\n    def guide():\n        q = pyro.param(""q"", torch.randn(3).exp(), constraint=constraints.simplex)\n        pyro.sample(""x"", dist.Categorical(q))\n\n    with pyro_backend(backend):\n        Elbo = infer.JitTrace_ELBO if jit else infer.Trace_ELBO\n        elbo = Elbo(ignore_jit_warnings=True)\n        assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""backend"", [\n    ""pyro"",\n    xfail_param(""funsor"", reason=""missing patterns""),\n])\ndef test_mean_field_ok(backend):\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.Normal(x, 1.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        x = pyro.sample(""x"", dist.Normal(loc, 1.))\n        pyro.sample(""y"", dist.Normal(x, 1.))\n\n    with pyro_backend(backend):\n        elbo = infer.TraceMeanField_ELBO()\n        assert_ok(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""backend"", [\n    ""pyro"",\n    xfail_param(""funsor"", reason=""missing patterns""),\n])\ndef test_mean_field_warn(backend):\n\n    def model():\n        x = pyro.sample(""x"", dist.Normal(0., 1.))\n        pyro.sample(""y"", dist.Normal(x, 1.))\n\n    def guide():\n        loc = pyro.param(""loc"", torch.tensor(0.))\n        y = pyro.sample(""y"", dist.Normal(loc, 1.))\n        pyro.sample(""x"", dist.Normal(y, 1.))\n\n    with pyro_backend(backend):\n        elbo = infer.TraceMeanField_ELBO()\n        assert_warning(model, guide, elbo)\n\n\n@pytest.mark.parametrize(""backend"", [""pyro"", ""funsor""])\n@pytest.mark.parametrize(""inner_dim"", [2])\n@pytest.mark.parametrize(""outer_dim"", [2])\ndef test_elbo_plate_plate(backend, outer_dim, inner_dim):\n    with pyro_backend(backend):\n        pyro.get_param_store().clear()\n        num_particles = 1\n        q = pyro.param(""q"", torch.tensor([0.75, 0.25], requires_grad=True))\n        p = 0.2693204236205713  # for which kl(Categorical(q), Categorical(p)) = 0.5\n        p = torch.tensor([p, 1-p])\n\n        def model():\n            d = dist.Categorical(p)\n            context1 = pyro.plate(""outer"", outer_dim, dim=-1)\n            context2 = pyro.plate(""inner"", inner_dim, dim=-2)\n            pyro.sample(""w"", d)\n            with context1:\n                pyro.sample(""x"", d)\n            with context2:\n                pyro.sample(""y"", d)\n            with context1, context2:\n                pyro.sample(""z"", d)\n\n        def guide():\n            d = dist.Categorical(pyro.param(""q""))\n            context1 = pyro.plate(""outer"", outer_dim, dim=-1)\n            context2 = pyro.plate(""inner"", inner_dim, dim=-2)\n            pyro.sample(""w"", d, infer={""enumerate"": ""parallel""})\n            with context1:\n                pyro.sample(""x"", d, infer={""enumerate"": ""parallel""})\n            with context2:\n                pyro.sample(""y"", d, infer={""enumerate"": ""parallel""})\n            with context1, context2:\n                pyro.sample(""z"", d, infer={""enumerate"": ""parallel""})\n\n        kl_node = kl_divergence(torch.distributions.Categorical(funsor.to_data(q)),\n                                torch.distributions.Categorical(funsor.to_data(p)))\n        kl = (1 + outer_dim + inner_dim + outer_dim * inner_dim) * kl_node\n        expected_loss = kl\n        expected_grad = grad(kl, [funsor.to_data(q)])[0]\n\n        elbo = infer.TraceEnum_ELBO(num_particles=num_particles,\n                                    vectorize_particles=True,\n                                    strict_enumeration_warning=True)\n        elbo = elbo.differentiable_loss if backend == ""pyro"" else elbo\n        actual_loss = funsor.to_data(elbo(model, guide))\n        actual_loss.backward()\n        actual_grad = funsor.to_data(pyro.param(\'q\')).grad\n\n        assert ops.allclose(actual_loss, expected_loss, atol=1e-5)\n        assert ops.allclose(actual_grad, expected_grad, atol=1e-5)\n\n\n@pytest.mark.parametrize(\'backend\', [""pyro"", ""funsor""])\ndef test_elbo_enumerate_plates_1(backend):\n    #  +-----------------+\n    #  | a ----> b   M=2 |\n    #  +-----------------+\n    #  +-----------------+\n    #  | c ----> d   N=3 |\n    #  +-----------------+\n    # This tests two unrelated plates.\n    # Each should remain uncontracted.\n    with pyro_backend(backend):\n        pyro.param(""probs_a"",\n                   torch.tensor([0.45, 0.55]),\n                   constraint=constraints.simplex)\n        pyro.param(""probs_b"",\n                   torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n                   constraint=constraints.simplex)\n        pyro.param(""probs_c"",\n                   torch.tensor([0.75, 0.25]),\n                   constraint=constraints.simplex)\n        pyro.param(""probs_d"",\n                   torch.tensor([[0.4, 0.6], [0.3, 0.7]]),\n                   constraint=constraints.simplex)\n        b_data = torch.tensor([0, 1])\n        d_data = torch.tensor([0, 0, 1])\n\n        def auto_model():\n            probs_a = pyro.param(""probs_a"")\n            probs_b = pyro.param(""probs_b"")\n            probs_c = pyro.param(""probs_c"")\n            probs_d = pyro.param(""probs_d"")\n            with pyro.plate(""a_axis"", 2, dim=-1):\n                a = pyro.sample(""a"", dist.Categorical(probs_a),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""b"", dist.Categorical(probs_b[a]), obs=b_data)\n            with pyro.plate(""c_axis"", 3, dim=-1):\n                c = pyro.sample(""c"", dist.Categorical(probs_c),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""d"", dist.Categorical(probs_d[c]), obs=d_data)\n\n        def hand_model():\n            probs_a = pyro.param(""probs_a"")\n            probs_b = pyro.param(""probs_b"")\n            probs_c = pyro.param(""probs_c"")\n            probs_d = pyro.param(""probs_d"")\n            for i in range(2):\n                a = pyro.sample(""a_{}"".format(i), dist.Categorical(probs_a),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""b_{}"".format(i), dist.Categorical(probs_b[a]), obs=b_data[i])\n            for j in range(3):\n                c = pyro.sample(""c_{}"".format(j), dist.Categorical(probs_c),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""d_{}"".format(j), dist.Categorical(probs_d[c]), obs=d_data[j])\n\n        def guide():\n            pass\n\n        elbo = infer.TraceEnum_ELBO(max_plate_nesting=1)\n        elbo = elbo.differentiable_loss if backend == ""pyro"" else elbo\n        auto_loss = elbo(auto_model, guide)\n        elbo = infer.TraceEnum_ELBO(max_plate_nesting=0)\n        elbo = elbo.differentiable_loss if backend == ""pyro"" else elbo\n        hand_loss = elbo(hand_model, guide)\n        _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.parametrize(\'backend\', [""pyro"", ""funsor""])\ndef test_elbo_enumerate_plate_7(backend):\n    #  Guide    Model\n    #    a -----> b\n    #    |        |\n    #  +-|--------|----------------+\n    #  | V        V                |\n    #  | c -----> d -----> e   N=2 |\n    #  +---------------------------+\n    # This tests a mixture of model and guide enumeration.\n    with pyro_backend(backend):\n        pyro.param(""model_probs_a"",\n                   torch.tensor([0.45, 0.55]),\n                   constraint=constraints.simplex)\n        pyro.param(""model_probs_b"",\n                   torch.tensor([[0.6, 0.4], [0.4, 0.6]]),\n                   constraint=constraints.simplex)\n        pyro.param(""model_probs_c"",\n                   torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n                   constraint=constraints.simplex)\n        pyro.param(""model_probs_d"",\n                   torch.tensor([[[0.4, 0.6], [0.3, 0.7]], [[0.3, 0.7], [0.2, 0.8]]]),\n                   constraint=constraints.simplex)\n        pyro.param(""model_probs_e"",\n                   torch.tensor([[0.75, 0.25], [0.55, 0.45]]),\n                   constraint=constraints.simplex)\n        pyro.param(""guide_probs_a"",\n                   torch.tensor([0.35, 0.64]),\n                   constraint=constraints.simplex)\n        pyro.param(""guide_probs_c"",\n                   torch.tensor([[0., 1.], [1., 0.]]),  # deterministic\n                   constraint=constraints.simplex)\n\n        def auto_model(data):\n            probs_a = pyro.param(""model_probs_a"")\n            probs_b = pyro.param(""model_probs_b"")\n            probs_c = pyro.param(""model_probs_c"")\n            probs_d = pyro.param(""model_probs_d"")\n            probs_e = pyro.param(""model_probs_e"")\n            a = pyro.sample(""a"", dist.Categorical(probs_a))\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]),\n                            infer={""enumerate"": ""parallel""})\n            with pyro.plate(""data"", 2, dim=-1):\n                c = pyro.sample(""c"", dist.Categorical(probs_c[a]))\n                d = pyro.sample(""d"", dist.Categorical(Vindex(probs_d)[b, c]),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""obs"", dist.Categorical(probs_e[d]), obs=data)\n\n        def auto_guide(data):\n            probs_a = pyro.param(""guide_probs_a"")\n            probs_c = pyro.param(""guide_probs_c"")\n            a = pyro.sample(""a"", dist.Categorical(probs_a),\n                            infer={""enumerate"": ""parallel""})\n            with pyro.plate(""data"", 2, dim=-1):\n                pyro.sample(""c"", dist.Categorical(probs_c[a]))\n\n        def hand_model(data):\n            probs_a = pyro.param(""model_probs_a"")\n            probs_b = pyro.param(""model_probs_b"")\n            probs_c = pyro.param(""model_probs_c"")\n            probs_d = pyro.param(""model_probs_d"")\n            probs_e = pyro.param(""model_probs_e"")\n            a = pyro.sample(""a"", dist.Categorical(probs_a))\n            b = pyro.sample(""b"", dist.Categorical(probs_b[a]),\n                            infer={""enumerate"": ""parallel""})\n            for i in range(2):\n                c = pyro.sample(""c_{}"".format(i), dist.Categorical(probs_c[a]))\n                d = pyro.sample(""d_{}"".format(i),\n                                dist.Categorical(Vindex(probs_d)[b, c]),\n                                infer={""enumerate"": ""parallel""})\n                pyro.sample(""obs_{}"".format(i), dist.Categorical(probs_e[d]), obs=data[i])\n\n        def hand_guide(data):\n            probs_a = pyro.param(""guide_probs_a"")\n            probs_c = pyro.param(""guide_probs_c"")\n            a = pyro.sample(""a"", dist.Categorical(probs_a),\n                            infer={""enumerate"": ""parallel""})\n            for i in range(2):\n                pyro.sample(""c_{}"".format(i), dist.Categorical(probs_c[a]))\n\n        data = torch.tensor([0, 0])\n        elbo = infer.TraceEnum_ELBO(max_plate_nesting=1)\n        elbo = elbo.differentiable_loss if backend == ""pyro"" else elbo\n        auto_loss = elbo(auto_model, auto_guide, data)\n        elbo = infer.TraceEnum_ELBO(max_plate_nesting=0)\n        elbo = elbo.differentiable_loss if backend == ""pyro"" else elbo\n        hand_loss = elbo(hand_model, hand_guide, data)\n        _check_loss_and_grads(hand_loss, auto_loss)\n\n\n@pytest.mark.xfail(reason=""missing patterns"")\n@pytest.mark.parametrize(""jit"", [False, True], ids=[""py"", ""jit""])\n@pytest.mark.parametrize(""exact"", [\n    True,\n    xfail_param(False, reason=""mixed sampling and exact not implemented yet"")\n], ids=[""exact"", ""monte-carlo""])\ndef test_gaussian_probit_hmm_smoke(exact, jit):\n\n    def model(data):\n        T, N, D = data.shape  # time steps, individuals, features\n\n        # Gaussian initial distribution.\n        init_loc = pyro.param(""init_loc"", torch.zeros(D))\n        init_scale = pyro.param(""init_scale"", 1e-2 * torch.eye(D),\n                                constraint=constraints.lower_cholesky)\n\n        # Linear dynamics with Gaussian noise.\n        trans_const = pyro.param(""trans_const"", torch.zeros(D))\n        trans_coeff = pyro.param(""trans_coeff"", torch.eye(D))\n        noise = pyro.param(""noise"", 1e-2 * torch.eye(D),\n                           constraint=constraints.lower_cholesky)\n\n        obs_plate = pyro.plate(""channel"", D, dim=-1)\n        with pyro.plate(""data"", N, dim=-2):\n            state = None\n            for t in range(T):\n                # Transition.\n                if t == 0:\n                    loc = init_loc\n                    scale_tril = init_scale\n                else:\n                    loc = trans_const + funsor.torch.torch_tensordot(trans_coeff, state, 1)\n                    scale_tril = noise\n                state = pyro.sample(""state_{}"".format(t),\n                                    dist.MultivariateNormal(loc, scale_tril),\n                                    infer={""exact"": exact})\n\n                # Factorial probit likelihood model.\n                with obs_plate:\n                    pyro.sample(""obs_{}"".format(t),\n                                dist.Bernoulli(logits=state[""channel""]),\n                                obs=data[t])\n\n    def guide(data):\n        pass\n\n    data = torch.distributions.Bernoulli(0.5).sample((3, 4, 2))\n\n    with pyro_backend(""funsor""):\n        Elbo = infer.JitTraceEnum_ELBO if jit else infer.TraceEnum_ELBO\n        elbo = Elbo()\n        adam = optim.Adam({""lr"": 1e-3})\n        svi = infer.SVI(model, guide, adam, elbo)\n        svi.step(data)\n'"
test/test_optimizer.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport pytest\n\nimport funsor\nfrom funsor.domains import bint\nfrom funsor.einsum import einsum, naive_contract_einsum, naive_einsum, naive_plated_einsum\nfrom funsor.interpreter import interpretation, reinterpret\nfrom funsor.optimizer import apply_optimizer\nfrom funsor.tensor import Tensor\nfrom funsor.terms import Variable, normalize, reflect\nfrom funsor.testing import assert_close, make_chain_einsum, make_einsum_example, make_hmm_einsum, make_plated_hmm_einsum\nfrom funsor.util import get_backend\n\n# TODO: make this file backend agnostic\npytestmark = pytest.mark.skipif(get_backend() != ""torch"",\n                                reason=""jax backend does not have pyro.ops.contract.einsum equivalent"")\nif get_backend() == ""torch"":\n    import torch\n    from funsor.torch.distributions import Categorical\n\n    from pyro.ops.contract import einsum as pyro_einsum\n\nOPTIMIZED_EINSUM_EXAMPLES = [\n    make_chain_einsum(t) for t in range(2, 50, 10)\n] + [\n    make_hmm_einsum(t) for t in range(2, 50, 10)\n]\n\n\n@pytest.mark.parametrize(\'equation\', OPTIMIZED_EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', [\n    \'pyro.ops.einsum.torch_log\',\n    \'pyro.ops.einsum.torch_map\',\n])\n@pytest.mark.parametrize(""einsum_impl"", [\n    naive_einsum,\n    naive_contract_einsum,\n])\ndef test_optimized_einsum(equation, backend, einsum_impl):\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n    expected = pyro_einsum(equation, *operands, backend=backend)[0]\n    with interpretation(normalize):\n        naive_ast = einsum_impl(equation, *funsor_operands, backend=backend)\n    optimized_ast = apply_optimizer(naive_ast)\n    actual = reinterpret(optimized_ast)  # eager by default\n\n    assert isinstance(actual, funsor.Tensor) and len(outputs) == 1\n    if len(outputs[0]) > 0:\n        actual = actual.align(tuple(outputs[0]))\n\n    assert expected.shape == actual.data.shape\n    assert torch.allclose(expected, actual.data)\n    for output in outputs:\n        for i, output_dim in enumerate(output):\n            assert output_dim in actual.inputs\n            assert actual.inputs[output_dim].dtype == sizes[output_dim]\n\n\n@pytest.mark.parametrize(""eqn1,eqn2"", [\n    (""a,ab->b"", ""bc->""),\n    (""ab,bc,cd->d"", ""de,ef,fg->""),\n])\n@pytest.mark.parametrize(""optimize1"", [False, True])\n@pytest.mark.parametrize(""optimize2"", [False, True])\n@pytest.mark.parametrize(""backend1"", [\n    \'torch\', \'pyro.ops.einsum.torch_log\', \'pyro.ops.einsum.torch_map\'])\n@pytest.mark.parametrize(""backend2"", [\n    \'torch\', \'pyro.ops.einsum.torch_log\', \'pyro.ops.einsum.torch_map\'])\n@pytest.mark.parametrize(""einsum_impl"", [naive_einsum, naive_contract_einsum])\ndef test_nested_einsum(eqn1, eqn2, optimize1, optimize2, backend1, backend2, einsum_impl):\n    inputs1, outputs1, sizes1, operands1, _ = make_einsum_example(eqn1, sizes=(3,))\n    inputs2, outputs2, sizes2, operands2, funsor_operands2 = make_einsum_example(eqn2, sizes=(3,))\n\n    # normalize the probs for ground-truth comparison\n    operands1 = [operand.abs() / operand.abs().sum(-1, keepdim=True)\n                 for operand in operands1]\n\n    expected1 = pyro_einsum(eqn1, *operands1, backend=backend1, modulo_total=True)[0]\n    expected2 = pyro_einsum(outputs1[0] + "","" + eqn2, *([expected1] + operands2),\n                            backend=backend2, modulo_total=True)[0]\n\n    with interpretation(normalize):\n        funsor_operands1 = [\n            Categorical(probs=Tensor(\n                operand,\n                inputs=OrderedDict([(d, bint(sizes1[d])) for d in inp[:-1]])\n            ))(value=Variable(inp[-1], bint(sizes1[inp[-1]]))).exp()\n            for inp, operand in zip(inputs1, operands1)\n        ]\n\n        output1_naive = einsum_impl(eqn1, *funsor_operands1, backend=backend1)\n        with interpretation(reflect):\n            output1 = apply_optimizer(output1_naive) if optimize1 else output1_naive\n        output2_naive = einsum_impl(outputs1[0] + "","" + eqn2, *([output1] + funsor_operands2), backend=backend2)\n        with interpretation(reflect):\n            output2 = apply_optimizer(output2_naive) if optimize2 else output2_naive\n\n    actual1 = reinterpret(output1)\n    actual2 = reinterpret(output2)\n\n    assert torch.allclose(expected1, actual1.data)\n    assert torch.allclose(expected2, actual2.data)\n\n\nPLATED_EINSUM_EXAMPLES = [\n    make_plated_hmm_einsum(num_steps, num_obs_plates=b, num_hidden_plates=a)\n    for num_steps in range(3, 50, 6)\n    for (a, b) in [(0, 1), (0, 2), (0, 0), (1, 1), (1, 2)]\n]\n\n\n@pytest.mark.parametrize(\'equation,plates\', PLATED_EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'backend\', [\n    \'pyro.ops.einsum.torch_log\',\n    \'pyro.ops.einsum.torch_map\',\n])\ndef test_optimized_plated_einsum(equation, plates, backend):\n    inputs, outputs, sizes, operands, funsor_operands = make_einsum_example(equation)\n    expected = pyro_einsum(equation, *operands, plates=plates, backend=backend)[0]\n    actual = einsum(equation, *funsor_operands, plates=plates, backend=backend)\n\n    if len(equation) < 10:\n        actual_naive = naive_plated_einsum(equation, *funsor_operands, plates=plates, backend=backend)\n        assert_close(actual, actual_naive)\n\n    assert isinstance(actual, funsor.Tensor) and len(outputs) == 1\n    if len(outputs[0]) > 0:\n        actual = actual.align(tuple(outputs[0]))\n\n    assert expected.shape == actual.data.shape\n    assert torch.allclose(expected, actual.data)\n    for output in outputs:\n        for i, output_dim in enumerate(output):\n            assert output_dim in actual.inputs\n            assert actual.inputs[output_dim].dtype == sizes[output_dim]\n'"
test/test_samplers.py,7,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nfrom collections import OrderedDict\nfrom importlib import import_module\n\nimport numpy as np\nimport pytest\n\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction\nfrom funsor.delta import Delta\nfrom funsor.distribution import BACKEND_TO_DISTRIBUTIONS_BACKEND\nfrom funsor.domains import bint, reals\nfrom funsor.integrate import Integrate\nfrom funsor.montecarlo import monte_carlo_interpretation\nfrom funsor.tensor import Tensor, align_tensors\nfrom funsor.terms import Variable\nfrom funsor.testing import assert_close, id_from_inputs, randn, random_gaussian, random_tensor, xfail_if_not_implemented\nfrom funsor.util import get_backend\n\npytestmark = pytest.mark.skipif(get_backend() == ""numpy"",\n                                reason=""numpy does not have distributions backend"")\nif get_backend() != ""numpy"":\n    dist = import_module(BACKEND_TO_DISTRIBUTIONS_BACKEND[get_backend()])\n    backend_dist = dist.dist\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [\n    (),\n    ((\'s\', bint(6)),),\n    ((\'s\', bint(6)), (\'t\', bint(7))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'batch_inputs\', [\n    (),\n    ((\'b\', bint(4)),),\n    ((\'b\', bint(4)), (\'c\', bint(5))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'event_inputs\', [\n    ((\'e\', bint(2)),),\n    ((\'e\', bint(2)), (\'f\', bint(3))),\n], ids=id_from_inputs)\ndef test_tensor_shape(sample_inputs, batch_inputs, event_inputs):\n    be_inputs = OrderedDict(batch_inputs + event_inputs)\n    expected_inputs = OrderedDict(sample_inputs + batch_inputs + event_inputs)\n    sample_inputs = OrderedDict(sample_inputs)\n    batch_inputs = OrderedDict(batch_inputs)\n    event_inputs = OrderedDict(event_inputs)\n    x = random_tensor(be_inputs)\n    rng_key = subkey = None if get_backend() == ""torch"" else np.array([0, 0], dtype=np.uint32)\n\n    for num_sampled in range(len(event_inputs) + 1):\n        for sampled_vars in itertools.combinations(list(event_inputs), num_sampled):\n            sampled_vars = frozenset(sampled_vars)\n            print(\'sampled_vars: {}\'.format(\', \'.join(sampled_vars)))\n            if rng_key is not None:\n                import jax\n                rng_key, subkey = jax.random.split(rng_key)\n\n            y = x.sample(sampled_vars, sample_inputs, rng_key=subkey)\n            if num_sampled == len(event_inputs):\n                assert isinstance(y, (Delta, Contraction))\n            if sampled_vars:\n                assert dict(y.inputs) == dict(expected_inputs), sampled_vars\n            else:\n                assert y is x\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [\n    (),\n    ((\'s\', bint(3)),),\n    ((\'s\', bint(3)), (\'t\', bint(4))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'batch_inputs\', [\n    (),\n    ((\'b\', bint(2)),),\n    ((\'c\', reals()),),\n    ((\'b\', bint(2)), (\'c\', reals())),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'event_inputs\', [\n    ((\'e\', reals()),),\n    ((\'e\', reals()), (\'f\', reals(2))),\n], ids=id_from_inputs)\ndef test_gaussian_shape(sample_inputs, batch_inputs, event_inputs):\n    be_inputs = OrderedDict(batch_inputs + event_inputs)\n    expected_inputs = OrderedDict(sample_inputs + batch_inputs + event_inputs)\n    sample_inputs = OrderedDict(sample_inputs)\n    batch_inputs = OrderedDict(batch_inputs)\n    event_inputs = OrderedDict(event_inputs)\n    x = random_gaussian(be_inputs)\n    rng_key = subkey = None if get_backend() == ""torch"" else np.array([0, 0], dtype=np.uint32)\n\n    xfail = False\n    for num_sampled in range(len(event_inputs) + 1):\n        for sampled_vars in itertools.combinations(list(event_inputs), num_sampled):\n            sampled_vars = frozenset(sampled_vars)\n            print(\'sampled_vars: {}\'.format(\', \'.join(sampled_vars)))\n            try:\n                if rng_key is not None:\n                    import jax\n                    rng_key, subkey = jax.random.split(rng_key)\n\n                y = x.sample(sampled_vars, sample_inputs, rng_key=subkey)\n            except NotImplementedError:\n                xfail = True\n                continue\n            if num_sampled == len(event_inputs):\n                assert isinstance(y, (Delta, Contraction))\n            if sampled_vars:\n                assert dict(y.inputs) == dict(expected_inputs), sampled_vars\n            else:\n                assert y is x\n    if xfail:\n        pytest.xfail(reason=\'Not implemented\')\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [\n    (),\n    ((\'s\', bint(3)),),\n    ((\'s\', bint(3)), (\'t\', bint(4))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'batch_inputs\', [\n    (),\n    ((\'b\', bint(2)),),\n    ((\'c\', reals()),),\n    ((\'b\', bint(2)), (\'c\', reals())),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'event_inputs\', [\n    ((\'e\', reals()),),\n    ((\'e\', reals()), (\'f\', reals(2))),\n], ids=id_from_inputs)\ndef test_transformed_gaussian_shape(sample_inputs, batch_inputs, event_inputs):\n    be_inputs = OrderedDict(batch_inputs + event_inputs)\n    expected_inputs = OrderedDict(sample_inputs + batch_inputs + event_inputs)\n    sample_inputs = OrderedDict(sample_inputs)\n    batch_inputs = OrderedDict(batch_inputs)\n    event_inputs = OrderedDict(event_inputs)\n\n    x = random_gaussian(be_inputs)\n    x = x(**{name: name + \'_\' for name, domain in event_inputs.items()})\n    x = x(**{name + \'_\': Variable(name, domain).log()\n             for name, domain in event_inputs.items()})\n\n    rng_key = subkey = None if get_backend() == ""torch"" else np.array([0, 0], dtype=np.uint32)\n    xfail = False\n    for num_sampled in range(len(event_inputs) + 1):\n        for sampled_vars in itertools.combinations(list(event_inputs), num_sampled):\n            sampled_vars = frozenset(sampled_vars)\n            print(\'sampled_vars: {}\'.format(\', \'.join(sampled_vars)))\n            try:\n                if rng_key is not None:\n                    import jax\n                    rng_key, subkey = jax.random.split(rng_key)\n\n                y = x.sample(sampled_vars, sample_inputs, rng_key=subkey)\n            except NotImplementedError:\n                xfail = True\n                continue\n            if num_sampled == len(event_inputs):\n                assert isinstance(y, (Delta, Contraction))\n            if sampled_vars:\n                assert dict(y.inputs) == dict(expected_inputs), sampled_vars\n            else:\n                assert y is x\n    if xfail:\n        pytest.xfail(reason=\'Not implemented\')\n\n\n@pytest.mark.parametrize(\'sample_inputs\', [\n    (),\n    ((\'s\', bint(6)),),\n    ((\'s\', bint(6)), (\'t\', bint(7))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'int_event_inputs\', [\n    (),\n    ((\'d\', bint(2)),),\n    ((\'d\', bint(2)), (\'e\', bint(3))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'real_event_inputs\', [\n    ((\'g\', reals()),),\n    ((\'g\', reals()), (\'h\', reals(4))),\n], ids=id_from_inputs)\ndef test_joint_shape(sample_inputs, int_event_inputs, real_event_inputs):\n    event_inputs = int_event_inputs + real_event_inputs\n    discrete_inputs = OrderedDict(int_event_inputs)\n    gaussian_inputs = OrderedDict(event_inputs)\n    expected_inputs = OrderedDict(sample_inputs + event_inputs)\n    sample_inputs = OrderedDict(sample_inputs)\n    event_inputs = OrderedDict(event_inputs)\n    t = random_tensor(discrete_inputs)\n    g = random_gaussian(gaussian_inputs)\n    x = t + g  # Joint(discrete=t, gaussian=g)\n\n    rng_key = subkey = None if get_backend() == ""torch"" else np.array([0, 0], dtype=np.uint32)\n    xfail = False\n    for num_sampled in range(len(event_inputs)):\n        for sampled_vars in itertools.combinations(list(event_inputs), num_sampled):\n            sampled_vars = frozenset(sampled_vars)\n            print(\'sampled_vars: {}\'.format(\', \'.join(sampled_vars)))\n            try:\n                if rng_key is not None:\n                    import jax\n                    rng_key, subkey = jax.random.split(rng_key)\n\n                y = x.sample(sampled_vars, sample_inputs, rng_key=subkey)\n            except NotImplementedError:\n                xfail = True\n                continue\n            if sampled_vars:\n                assert dict(y.inputs) == dict(expected_inputs), sampled_vars\n            else:\n                assert y is x\n    if xfail:\n        pytest.xfail(reason=\'Not implemented\')\n\n\n@pytest.mark.parametrize(\'batch_inputs\', [\n    (),\n    ((\'b\', bint(4)),),\n    ((\'b\', bint(2)), (\'c\', bint(2))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'event_inputs\', [\n    ((\'e\', bint(3)),),\n    ((\'e\', bint(2)), (\'f\', bint(2))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'test_grad\', [False, True], ids=[\'value\', \'grad\'])\ndef test_tensor_distribution(event_inputs, batch_inputs, test_grad):\n    num_samples = 50000\n    sample_inputs = OrderedDict(n=bint(num_samples))\n    be_inputs = OrderedDict(batch_inputs + event_inputs)\n    batch_inputs = OrderedDict(batch_inputs)\n    event_inputs = OrderedDict(event_inputs)\n    sampled_vars = frozenset(event_inputs)\n    p_data = random_tensor(be_inputs).data\n    rng_key = None if get_backend() == ""torch"" else np.array([0, 0], dtype=np.uint32)\n    probe = randn(p_data.shape)\n\n    def diff_fn(p_data):\n        p = Tensor(p_data, be_inputs)\n        q = p.sample(sampled_vars, sample_inputs, rng_key=rng_key)\n        mq = p.materialize(q).reduce(ops.logaddexp, \'n\')\n        mq = mq.align(tuple(p.inputs))\n\n        _, (p_data, mq_data) = align_tensors(p, mq)\n        assert p_data.shape == mq_data.shape\n        return (ops.exp(mq_data) * probe).sum() - (ops.exp(p_data) * probe).sum(), mq\n\n    if test_grad:\n        if get_backend() == ""jax"":\n            import jax\n\n            diff_grad, mq = jax.grad(diff_fn, has_aux=True)(p_data)\n        else:\n            import torch\n\n            p_data.requires_grad_(True)\n            diff_grad = torch.autograd.grad(diff_fn(p_data)[0], [p_data])[0]\n\n        assert_close(diff_grad, ops.new_zeros(diff_grad, diff_grad.shape), atol=0.1, rtol=None)\n    else:\n        _, mq = diff_fn(p_data)\n        assert_close(mq, Tensor(p_data, be_inputs), atol=0.1, rtol=None)\n\n\n@pytest.mark.parametrize(\'batch_inputs\', [\n    (),\n    ((\'b\', bint(3)),),\n    ((\'b\', bint(3)), (\'c\', bint(4))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'event_inputs\', [\n    ((\'e\', reals()),),\n    ((\'e\', reals()), (\'f\', reals(2))),\n], ids=id_from_inputs)\ndef test_gaussian_distribution(event_inputs, batch_inputs):\n    num_samples = 100000\n    sample_inputs = OrderedDict(particle=bint(num_samples))\n    be_inputs = OrderedDict(batch_inputs + event_inputs)\n    batch_inputs = OrderedDict(batch_inputs)\n    event_inputs = OrderedDict(event_inputs)\n    sampled_vars = frozenset(event_inputs)\n    p = random_gaussian(be_inputs)\n\n    rng_key = None if get_backend() == ""torch"" else np.array([0, 0], dtype=np.uint32)\n    q = p.sample(sampled_vars, sample_inputs, rng_key=rng_key)\n    p_vars = sampled_vars\n    q_vars = sampled_vars | frozenset([\'particle\'])\n    # Check zeroth moment.\n    assert_close(q.reduce(ops.logaddexp, q_vars),\n                 p.reduce(ops.logaddexp, p_vars), atol=1e-6)\n    for k1, d1 in event_inputs.items():\n        x = Variable(k1, d1)\n        # Check first moments.\n        assert_close(Integrate(q, x, q_vars),\n                     Integrate(p, x, p_vars), atol=0.5, rtol=0.2)\n        for k2, d2 in event_inputs.items():\n            y = Variable(k2, d2)\n            # Check second moments.\n            continue  # FIXME: Quadratic integration is not supported:\n            assert_close(Integrate(q, x * y, q_vars),\n                         Integrate(p, x * y, p_vars), atol=1e-2)\n\n\n@pytest.mark.parametrize(\'batch_inputs\', [\n    (),\n    ((\'b\', bint(3)),),\n    ((\'b\', bint(3)), (\'c\', bint(2))),\n], ids=id_from_inputs)\n@pytest.mark.parametrize(\'event_inputs\', [\n    ((\'e\', reals()), (\'f\', bint(3))),\n    ((\'e\', reals(2)), (\'f\', bint(2))),\n], ids=id_from_inputs)\ndef test_gaussian_mixture_distribution(batch_inputs, event_inputs):\n    num_samples = 100000\n    sample_inputs = OrderedDict(particle=bint(num_samples))\n    be_inputs = OrderedDict(batch_inputs + event_inputs)\n    int_inputs = OrderedDict((k, d) for k, d in be_inputs.items()\n                             if d.dtype != \'real\')\n    batch_inputs = OrderedDict(batch_inputs)\n    event_inputs = OrderedDict(event_inputs)\n    sampled_vars = frozenset([\'f\'])\n    p = random_gaussian(be_inputs) + 0.5 * random_tensor(int_inputs)\n    p_marginal = p.reduce(ops.logaddexp, \'e\')\n    assert isinstance(p_marginal, Tensor)\n\n    rng_key = None if get_backend() == ""torch"" else np.array([0, 1], dtype=np.uint32)\n    q = p.sample(sampled_vars, sample_inputs, rng_key=rng_key)\n    q_marginal = q.reduce(ops.logaddexp, \'e\')\n    q_marginal = p_marginal.materialize(q_marginal).reduce(ops.logaddexp, \'particle\')\n    assert isinstance(q_marginal, Tensor)\n    q_marginal = q_marginal.align(tuple(p_marginal.inputs))\n    assert_close(q_marginal, p_marginal, atol=0.15, rtol=None)\n\n\n@pytest.mark.parametrize(\'moment\', [0, 1, 2, 3])\ndef test_lognormal_distribution(moment):\n    num_samples = 100000\n    inputs = OrderedDict(batch=bint(10))\n    loc = random_tensor(inputs)\n    scale = random_tensor(inputs).exp()\n\n    log_measure = dist.LogNormal(loc, scale)(value=\'x\')\n    probe = Variable(\'x\', reals()) ** moment\n    with monte_carlo_interpretation(particle=bint(num_samples)):\n        with xfail_if_not_implemented():\n            actual = Integrate(log_measure, probe, frozenset([\'x\']))\n\n    samples = backend_dist.LogNormal(loc, scale).sample((num_samples,))\n    expected = (samples ** moment).mean(0)\n    assert_close(actual.data, expected, atol=1e-2, rtol=1e-2)\n'"
test/test_sum_product.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport re\nimport os\nfrom collections import OrderedDict\nfrom functools import partial, reduce\n\nimport pytest\n\nimport funsor.ops as ops\nfrom funsor.domains import bint, reals\nfrom funsor.interpreter import interpretation\nfrom funsor.optimizer import apply_optimizer\nfrom funsor.sum_product import (\n    MarkovProduct,\n    _partition,\n    mixed_sequential_sum_product,\n    naive_sarkka_bilmes_product,\n    naive_sequential_sum_product,\n    partial_sum_product,\n    sarkka_bilmes_product,\n    sequential_sum_product,\n    sum_product\n)\nfrom funsor.tensor import Tensor, get_default_prototype\nfrom funsor.terms import Variable, eager_or_die, moment_matching, reflect\nfrom funsor.testing import assert_close, random_gaussian, random_tensor\nfrom funsor.util import get_backend\n\npytestmark = pytest.mark.skipif((get_backend() == \'jax\') and (\'CI\' in os.environ), reason=\'slow tests\')\n\n\n@pytest.mark.parametrize(\'inputs,dims,expected_num_components\', [\n    ([\'\'], set(), 1),\n    ([\'a\'], set(), 1),\n    ([\'a\'], set(\'a\'), 1),\n    ([\'a\', \'a\'], set(), 2),\n    ([\'a\', \'a\'], set(\'a\'), 1),\n    ([\'a\', \'a\', \'b\', \'b\'], set(), 4),\n    ([\'a\', \'a\', \'b\', \'b\'], set(\'a\'), 3),\n    ([\'a\', \'a\', \'b\', \'b\'], set(\'b\'), 3),\n    ([\'a\', \'a\', \'b\', \'b\'], set(\'ab\'), 2),\n    ([\'a\', \'ab\', \'b\'], set(), 3),\n    ([\'a\', \'ab\', \'b\'], set(\'a\'), 2),\n    ([\'a\', \'ab\', \'b\'], set(\'b\'), 2),\n    ([\'a\', \'ab\', \'b\'], set(\'ab\'), 1),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(), 4),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'c\'), 3),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'b\'), 3),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'a\'), 3),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'ac\'), 2),\n    ([\'a\', \'ab\', \'bc\', \'c\'], set(\'abc\'), 1),\n])\ndef test_partition(inputs, dims, expected_num_components):\n    sizes = dict(zip(\'abc\', [2, 3, 4]))\n    terms = [random_tensor(OrderedDict((s, bint(sizes[s])) for s in input_))\n             for input_ in inputs]\n    components = list(_partition(terms, dims))\n\n    # Check that result is a partition.\n    expected_terms = sorted(terms, key=id)\n    actual_terms = sorted((x for c in components for x in c[0]), key=id)\n    assert actual_terms == expected_terms\n    assert dims == set.union(set(), *(c[1] for c in components))\n\n    # Check that the partition is not too coarse.\n    assert len(components) == expected_num_components\n\n    # Check that partition is not too fine.\n    component_dict = {x: i for i, (terms, _) in enumerate(components) for x in terms}\n    for x in terms:\n        for y in terms:\n            if x is not y:\n                if dims.intersection(x.inputs, y.inputs):\n                    assert component_dict[x] == component_dict[y]\n\n\n@pytest.mark.parametrize(\'sum_op,prod_op\', [(ops.add, ops.mul), (ops.logaddexp, ops.add)])\n@pytest.mark.parametrize(\'inputs,plates\', [(\'a,abi,bcij\', \'ij\')])\n@pytest.mark.parametrize(\'vars1,vars2\', [\n    (\'\', \'abcij\'),\n    (\'c\', \'abij\'),\n    (\'cj\', \'abi\'),\n    (\'bcj\', \'ai\'),\n    (\'bcij\', \'a\'),\n    (\'abcij\', \'\'),\n])\ndef test_partial_sum_product(sum_op, prod_op, inputs, plates, vars1, vars2):\n    inputs = inputs.split(\',\')\n    factors = [random_tensor(OrderedDict((d, bint(2)) for d in ds)) for ds in inputs]\n    plates = frozenset(plates)\n    vars1 = frozenset(vars1)\n    vars2 = frozenset(vars2)\n\n    factors1 = partial_sum_product(sum_op, prod_op, factors, vars1, plates)\n    factors2 = partial_sum_product(sum_op, prod_op, factors1, vars2, plates)\n    actual = reduce(prod_op, factors2)\n\n    expected = sum_product(sum_op, prod_op, factors, vars1 | vars2, plates)\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'num_steps\', [None] + list(range(1, 13)))\n@pytest.mark.parametrize(\'sum_op,prod_op,state_domain\', [\n    (ops.add, ops.mul, bint(2)),\n    (ops.add, ops.mul, bint(3)),\n    (ops.logaddexp, ops.add, bint(2)),\n    (ops.logaddexp, ops.add, bint(3)),\n    (ops.logaddexp, ops.add, reals()),\n    (ops.logaddexp, ops.add, reals(2)),\n], ids=str)\n@pytest.mark.parametrize(\'batch_inputs\', [\n    {},\n    {""foo"": bint(5)},\n    {""foo"": bint(2), ""bar"": bint(4)},\n], ids=lambda d: "","".join(d.keys()))\n@pytest.mark.parametrize(\'impl\', [\n    sequential_sum_product,\n    naive_sequential_sum_product,\n    MarkovProduct,\n    partial(mixed_sequential_sum_product, num_segments=2),\n    partial(mixed_sequential_sum_product, num_segments=3),\n])\ndef test_sequential_sum_product(impl, sum_op, prod_op, batch_inputs, state_domain, num_steps):\n    inputs = OrderedDict(batch_inputs)\n    inputs.update(prev=state_domain, curr=state_domain)\n    if num_steps is None:\n        num_steps = 1\n    else:\n        inputs[""time""] = bint(num_steps)\n    if state_domain.dtype == ""real"":\n        trans = random_gaussian(inputs)\n    else:\n        trans = random_tensor(inputs)\n    time = Variable(""time"", bint(num_steps))\n\n    actual = impl(sum_op, prod_op, trans, time, {""prev"": ""curr""})\n    expected_inputs = batch_inputs.copy()\n    expected_inputs.update(prev=state_domain, curr=state_domain)\n    assert dict(actual.inputs) == expected_inputs\n\n    # Check against contract.\n    operands = tuple(trans(time=t, prev=""t_{}"".format(t), curr=""t_{}"".format(t+1))\n                     for t in range(num_steps))\n    reduce_vars = frozenset(""t_{}"".format(t) for t in range(1, num_steps))\n    with interpretation(reflect):\n        expected = sum_product(sum_op, prod_op, operands, reduce_vars)\n    expected = apply_optimizer(expected)\n    expected = expected(**{""t_0"": ""prev"", ""t_{}"".format(num_steps): ""curr""})\n    expected = expected.align(tuple(actual.inputs.keys()))\n    assert_close(actual, expected, rtol=5e-4 * num_steps)\n\n\n@pytest.mark.parametrize(\'num_steps\', [None] + list(range(1, 6)))\n@pytest.mark.parametrize(\'batch_inputs\', [\n    {},\n    {""foo"": bint(5)},\n    {""foo"": bint(2), ""bar"": bint(4)},\n], ids=lambda d: "","".join(d.keys()))\n@pytest.mark.parametrize(\'x_domain,y_domain\', [\n    (bint(2), bint(3)),\n    (reals(), reals(2, 2)),\n    (bint(2), reals(2)),\n], ids=str)\n@pytest.mark.parametrize(\'impl\', [\n    sequential_sum_product,\n    naive_sequential_sum_product,\n    MarkovProduct,\n    partial(mixed_sequential_sum_product, num_segments=2),\n    partial(mixed_sequential_sum_product, num_segments=3),\n])\ndef test_sequential_sum_product_multi(impl, x_domain, y_domain, batch_inputs, num_steps):\n    sum_op = ops.logaddexp\n    prod_op = ops.add\n    inputs = OrderedDict(batch_inputs)\n    inputs.update(x_prev=x_domain, x_curr=x_domain,\n                  y_prev=y_domain, y_curr=y_domain)\n    if num_steps is None:\n        num_steps = 1\n    else:\n        inputs[""time""] = bint(num_steps)\n    if any(v.dtype == ""real"" for v in inputs.values()):\n        trans = random_gaussian(inputs)\n    else:\n        trans = random_tensor(inputs)\n    time = Variable(""time"", bint(num_steps))\n    step = {""x_prev"": ""x_curr"", ""y_prev"": ""y_curr""}\n\n    with interpretation(moment_matching):\n        actual = impl(sum_op, prod_op, trans, time, step)\n        expected_inputs = batch_inputs.copy()\n        expected_inputs.update(x_prev=x_domain, x_curr=x_domain,\n                               y_prev=y_domain, y_curr=y_domain)\n        assert dict(actual.inputs) == expected_inputs\n\n        # Check against contract.\n        operands = tuple(trans(time=t,\n                               x_prev=""x_{}"".format(t), x_curr=""x_{}"".format(t+1),\n                               y_prev=""y_{}"".format(t), y_curr=""y_{}"".format(t+1))\n                         for t in range(num_steps))\n        reduce_vars = frozenset(""x_{}"".format(t) for t in range(1, num_steps)).union(\n                                ""y_{}"".format(t) for t in range(1, num_steps))\n        expected = sum_product(sum_op, prod_op, operands, reduce_vars)\n        expected = expected(**{""x_0"": ""x_prev"", ""x_{}"".format(num_steps): ""x_curr"",\n                               ""y_0"": ""y_prev"", ""y_{}"".format(num_steps): ""y_curr""})\n        expected = expected.align(tuple(actual.inputs.keys()))\n\n\n@pytest.mark.parametrize(""num_steps"", [1, 2, 3, 10])\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_sequential_sum_product_bias_1(num_steps, dim):\n    time = Variable(""time"", bint(num_steps))\n    bias_dist = random_gaussian(OrderedDict([\n        (""bias"", reals(dim)),\n    ]))\n    trans = random_gaussian(OrderedDict([\n        (""time"", bint(num_steps)),\n        (""x_prev"", reals(dim)),\n        (""x_curr"", reals(dim)),\n    ]))\n    obs = random_gaussian(OrderedDict([\n        (""time"", bint(num_steps)),\n        (""x_curr"", reals(dim)),\n        (""bias"", reals(dim)),\n    ]))\n    factor = trans + obs + bias_dist\n    assert set(factor.inputs) == {""time"", ""bias"", ""x_prev"", ""x_curr""}\n\n    result = sequential_sum_product(ops.logaddexp, ops.add, factor, time, {""x_prev"": ""x_curr""})\n    assert set(result.inputs) == {""bias"", ""x_prev"", ""x_curr""}\n\n\n@pytest.mark.xfail(reason=""missing pattern for Gaussian(x=y[z]) for real x"")\n@pytest.mark.parametrize(""num_steps"", [1, 2, 3, 10])\n@pytest.mark.parametrize(""num_sensors"", [2])\n@pytest.mark.parametrize(""dim"", [1, 2, 3])\ndef test_sequential_sum_product_bias_2(num_steps, num_sensors, dim):\n    time = Variable(""time"", bint(num_steps))\n    bias = Variable(""bias"", reals(num_sensors, dim))\n    bias_dist = random_gaussian(OrderedDict([\n        (""bias"", reals(num_sensors, dim)),\n    ]))\n    trans = random_gaussian(OrderedDict([\n        (""time"", bint(num_steps)),\n        (""x_prev"", reals(dim)),\n        (""x_curr"", reals(dim)),\n    ]))\n    obs = random_gaussian(OrderedDict([\n        (""time"", bint(num_steps)),\n        (""x_curr"", reals(dim)),\n        (""bias"", reals(dim)),\n    ]))\n\n    # Each time step only a single sensor observes x,\n    # and each sensor has a different bias.\n    sensor_id = Tensor(ops.new_arange(get_default_prototype(), num_steps) % 2,\n                       OrderedDict(time=bint(num_steps)), dtype=2)\n    with interpretation(eager_or_die):\n        factor = trans + obs(bias=bias[sensor_id]) + bias_dist\n    assert set(factor.inputs) == {""time"", ""bias"", ""x_prev"", ""x_curr""}\n\n    result = sequential_sum_product(ops.logaddexp, ops.add, factor, time, {""x_prev"": ""x_curr""})\n    assert set(result.inputs) == {""bias"", ""x_prev"", ""x_curr""}\n\n\ndef _check_sarkka_bilmes(trans, expected_inputs, global_vars, num_periods=1):\n\n    sum_op, prod_op = ops.logaddexp, ops.add\n\n    assert ""time"" in trans.inputs\n    duration = trans.inputs[""time""].dtype\n    time_var = Variable(""time"", bint(duration))\n\n    expected = naive_sarkka_bilmes_product(sum_op, prod_op, trans, time_var, global_vars)\n    assert dict(expected.inputs) == expected_inputs\n\n    actual = sarkka_bilmes_product(sum_op, prod_op, trans, time_var, global_vars,\n                                   num_periods=num_periods)\n    assert dict(actual.inputs) == expected_inputs\n\n    actual = actual.align(tuple(expected.inputs.keys()))\n    assert_close(actual, expected, atol=5e-4, rtol=5e-4)\n\n\n@pytest.mark.parametrize(""duration"", [2, 3, 4, 5, 6])\ndef test_sarkka_bilmes_example_0(duration):\n\n    trans = random_tensor(OrderedDict({\n        ""time"": bint(duration),\n        ""a"": bint(3),\n    }))\n\n    expected_inputs = {\n        ""a"": bint(3),\n    }\n\n    _check_sarkka_bilmes(trans, expected_inputs, frozenset())\n\n\n@pytest.mark.parametrize(""duration"", [2, 3, 4, 5, 6])\ndef test_sarkka_bilmes_example_1(duration):\n\n    trans = random_tensor(OrderedDict({\n        ""time"": bint(duration),\n        ""a"": bint(3),\n        ""b"": bint(2),\n        ""Pb"": bint(2),\n    }))\n\n    expected_inputs = {\n        ""a"": bint(3),\n        ""b"": bint(2),\n        ""Pb"": bint(2),\n    }\n\n    _check_sarkka_bilmes(trans, expected_inputs, frozenset())\n\n\n@pytest.mark.parametrize(""duration"", [2, 4, 6, 8])\ndef test_sarkka_bilmes_example_2(duration):\n\n    trans = random_tensor(OrderedDict({\n        ""time"": bint(duration),\n        ""a"": bint(4),\n        ""b"": bint(3),\n        ""Pb"": bint(3),\n        ""c"": bint(2),\n        ""PPc"": bint(2),\n    }))\n\n    expected_inputs = {\n        ""a"": bint(4),\n        ""b"": bint(3),\n        ""Pb"": bint(3),\n        ""c"": bint(2),\n        ""PPc"": bint(2),\n        ""Pc"": bint(2),\n    }\n\n    _check_sarkka_bilmes(trans, expected_inputs, frozenset())\n\n\n@pytest.mark.parametrize(""duration"", [2, 4, 6, 8])\ndef test_sarkka_bilmes_example_3(duration):\n\n    trans = random_tensor(OrderedDict({\n        ""time"": bint(duration),\n        ""a"": bint(4),\n        ""c"": bint(2),\n        ""PPc"": bint(2),\n    }))\n\n    expected_inputs = {\n        ""a"": bint(4),\n        ""c"": bint(2),\n        ""PPc"": bint(2),\n        ""Pc"": bint(2),\n    }\n\n    _check_sarkka_bilmes(trans, expected_inputs, frozenset())\n\n\n@pytest.mark.parametrize(""duration"", [3, 6, 9])\ndef test_sarkka_bilmes_example_4(duration):\n\n    trans = random_tensor(OrderedDict({\n        ""time"": bint(duration),\n        ""a"": bint(2),\n        ""Pa"": bint(2),\n        ""PPPa"": bint(2),\n    }))\n\n    expected_inputs = {\n        ""a"": bint(2),\n        ""PPa"": bint(2),\n        ""PPPa"": bint(2),\n        ""Pa"": bint(2),\n    }\n\n    _check_sarkka_bilmes(trans, expected_inputs, frozenset())\n\n\n@pytest.mark.parametrize(""duration"", [2, 3, 4, 5, 6])\ndef test_sarkka_bilmes_example_5(duration):\n\n    trans = random_tensor(OrderedDict({\n        ""time"": bint(duration),\n        ""a"": bint(3),\n        ""Pa"": bint(3),\n        ""x"": bint(2),\n    }))\n\n    expected_inputs = {\n        ""a"": bint(3),\n        ""Pa"": bint(3),\n        ""x"": bint(2),\n    }\n\n    global_vars = frozenset([""x""])\n\n    _check_sarkka_bilmes(trans, expected_inputs, global_vars)\n\n\n@pytest.mark.parametrize(""duration"", [3, 6, 9])\ndef test_sarkka_bilmes_example_6(duration):\n\n    trans = random_tensor(OrderedDict({\n        ""time"": bint(duration),\n        ""a"": bint(2),\n        ""Pa"": bint(2),\n        ""PPPa"": bint(2),\n        ""x"": bint(3),\n    }))\n\n    expected_inputs = {\n        ""a"": bint(2),\n        ""PPa"": bint(2),\n        ""PPPa"": bint(2),\n        ""Pa"": bint(2),\n        ""x"": bint(3),\n    }\n\n    global_vars = frozenset([""x""])\n\n    _check_sarkka_bilmes(trans, expected_inputs, global_vars)\n\n\n@pytest.mark.parametrize(""time_input"", [(""time"", bint(t)) for t in range(2, 10)])\n@pytest.mark.parametrize(""global_inputs"", [\n    (),\n    ((""x"", bint(2)),),\n])\n@pytest.mark.parametrize(""local_inputs"", [\n    # tensor\n    ((""a"", bint(2)),),\n    ((""a"", bint(2)), (""Pa"", bint(2))),\n    ((""a"", bint(2)), (""b"", bint(2)), (""Pb"", bint(2))),\n    ((""a"", bint(2)), (""b"", bint(2)), (""PPb"", bint(2))),\n    ((""a"", bint(2)), (""b"", bint(2)), (""Pb"", bint(2)), (""c"", bint(2)), (""PPc"", bint(2))),\n    ((""a"", bint(2)), (""Pa"", bint(2)), (""PPPa"", bint(2))),\n    ((""a"", bint(2)), (""b"", bint(2)), (""PPb"", bint(2)), (""PPPa"", bint(2))),\n    # gaussian\n    ((""a"", reals()),),\n    ((""a"", reals()), (""Pa"", reals())),\n    ((""a"", reals()), (""b"", reals()), (""Pb"", reals())),\n    ((""a"", reals()), (""b"", reals()), (""PPb"", reals())),\n    ((""a"", reals()), (""b"", reals()), (""Pb"", reals()), (""c"", reals()), (""PPc"", reals())),\n    ((""a"", reals()), (""Pa"", reals()), (""PPPa"", reals())),\n    ((""a"", reals()), (""b"", reals()), (""PPb"", reals()), (""PPPa"", reals())),\n    # mv gaussian\n    ((""a"", reals(2)), (""b"", reals(2)), (""Pb"", reals(2))),\n    ((""a"", reals(2)), (""b"", reals(2)), (""PPb"", reals(2))),\n])\n@pytest.mark.parametrize(""num_periods"", [1, 2])\ndef test_sarkka_bilmes_generic(time_input, global_inputs, local_inputs, num_periods):\n\n    lags = {\n        kk: reduce(max, [\n            len(re.search(""^P*"", k).group(0)) for k, v in local_inputs\n            if k.strip(""P"") == kk], 0)\n        for kk, vv in local_inputs if not kk.startswith(""P"")\n    }\n    expected_inputs = dict(global_inputs + tuple(set(\n        ((t * ""P"" + k), v)\n        for k, v in local_inputs if not k.startswith(""P"")\n        for t in range(0, lags[k] + 1))))\n\n    trans_inputs = OrderedDict(global_inputs + (time_input,) + local_inputs)\n    global_vars = frozenset(k for k, v in global_inputs)\n\n    if any(v.dtype == ""real"" for v in trans_inputs.values()):\n        trans = random_gaussian(trans_inputs)\n    else:\n        trans = random_tensor(trans_inputs)\n\n    try:\n        _check_sarkka_bilmes(trans, expected_inputs, global_vars, num_periods)\n    except NotImplementedError as e:\n        partial_reasons = (\n            \'TODO handle partial windows\',\n        )\n        if any(reason in e.args[0] for reason in partial_reasons):\n            pytest.xfail(reason=e.args[0])\n        else:\n            raise\n\n\n@pytest.mark.parametrize(""duration,num_segments"", [(12, 1), (12, 2), (12, 3), (12, 4), (12, 6)])\ndef test_mixed_sequential_sum_product(duration, num_segments):\n\n    sum_op, prod_op = ops.logaddexp, ops.add\n    time_var = Variable(""time"", bint(duration))\n    step = {""Px"": ""x""}\n\n    trans_inputs = ((time_var.name, bint(duration)),) + \\\n        tuple((k, bint(2)) for k in step.keys()) + \\\n        tuple((v, bint(2)) for v in step.values())\n\n    trans = random_tensor(OrderedDict(trans_inputs))\n\n    expected = sequential_sum_product(sum_op, prod_op, trans, time_var, step)\n    actual = mixed_sequential_sum_product(sum_op, prod_op, trans, time_var, step,\n                                          num_segments=num_segments)\n\n    assert_close(actual, expected)\n'"
test/test_tensor.py,3,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pytest\n\nimport funsor\nimport funsor.ops as ops\nfrom funsor.domains import Domain, bint, find_domain, reals\nfrom funsor.interpreter import interpretation\nfrom funsor.terms import Cat, Lambda, Number, Slice, Stack, Variable, lazy\nfrom funsor.testing import (assert_close, assert_equiv, check_funsor, empty,\n                            rand, randn, random_tensor, zeros)\nfrom funsor.tensor import REDUCE_OP_TO_NUMERIC, Einsum, Tensor, align_tensors, numeric_array, stack, tensordot\nfrom funsor.util import get_backend\n\n\n@pytest.mark.parametrize(\'output_shape\', [(), (2,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'inputs\', [(), (\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')], ids=str)\ndef test_quote(output_shape, inputs):\n    if get_backend() == ""torch"":\n        import torch  # noqa: F401\n\n    sizes = {\'a\': 4, \'b\': 5, \'c\': 6}\n    inputs = OrderedDict((k, bint(sizes[k])) for k in inputs)\n    x = random_tensor(inputs, reals(*output_shape))\n    s = funsor.quote(x)\n    assert isinstance(s, str)\n    assert_close(eval(s), x)\n\n\n@pytest.mark.parametrize(\'shape\', [(), (4,), (3, 2)])\n@pytest.mark.parametrize(\'dtype\', [\'float32\', \'float64\', \'int32\', \'int64\', \'uint8\', \'bool\'])\ndef test_to_funsor(shape, dtype):\n    t = ops.astype(randn(shape), dtype)\n    f = funsor.to_funsor(t)\n    assert isinstance(f, Tensor)\n    assert funsor.to_funsor(t, reals(*shape)) is f\n    with pytest.raises(ValueError):\n        funsor.to_funsor(t, reals(5, *shape))\n\n\ndef test_to_data():\n    data = zeros((3, 3))\n    x = Tensor(data)\n    assert funsor.to_data(x) is data\n\n\ndef test_to_data_error():\n    data = zeros((3, 3))\n    x = Tensor(data, OrderedDict(i=bint(3)))\n    with pytest.raises(ValueError):\n        funsor.to_data(x)\n\n\ndef test_cons_hash():\n    x = randn((3, 3))\n    assert Tensor(x) is Tensor(x)\n\n\ndef test_indexing():\n    data = randn((4, 5))\n    inputs = OrderedDict([(\'i\', bint(4)),\n                          (\'j\', bint(5))])\n    x = Tensor(data, inputs)\n    check_funsor(x, inputs, reals(), data)\n\n    assert x() is x\n    assert x(k=3) is x\n    check_funsor(x(1), {\'j\': bint(5)}, reals(), data[1])\n    check_funsor(x(1, 2), {}, reals(), data[1, 2])\n    check_funsor(x(1, 2, k=3), {}, reals(), data[1, 2])\n    check_funsor(x(1, j=2), {}, reals(), data[1, 2])\n    check_funsor(x(1, j=2, k=3), (), reals(), data[1, 2])\n    check_funsor(x(1, k=3), {\'j\': bint(5)}, reals(), data[1])\n    check_funsor(x(i=1), {\'j\': bint(5)}, reals(), data[1])\n    check_funsor(x(i=1, j=2), (), reals(), data[1, 2])\n    check_funsor(x(i=1, j=2, k=3), (), reals(), data[1, 2])\n    check_funsor(x(i=1, k=3), {\'j\': bint(5)}, reals(), data[1])\n    check_funsor(x(j=2), {\'i\': bint(4)}, reals(), data[:, 2])\n    check_funsor(x(j=2, k=3), {\'i\': bint(4)}, reals(), data[:, 2])\n\n\ndef test_advanced_indexing_shape():\n    I, J, M, N = 4, 4, 2, 3\n    x = Tensor(randn((I, J)), OrderedDict([\n        (\'i\', bint(I)),\n        (\'j\', bint(J)),\n    ]))\n    m = Tensor(numeric_array([2, 3]), OrderedDict([(\'m\', bint(M))]), I)\n    n = Tensor(numeric_array([0, 1, 1]), OrderedDict([(\'n\', bint(N))]), J)\n    assert x.data.shape == (I, J)\n\n    check_funsor(x(i=m), {\'j\': bint(J), \'m\': bint(M)}, reals())\n    check_funsor(x(i=m, j=n), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(i=m, j=n, k=m), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(i=m, k=m), {\'j\': bint(J), \'m\': bint(M)}, reals())\n    check_funsor(x(i=n), {\'j\': bint(J), \'n\': bint(N)}, reals())\n    check_funsor(x(i=n, k=m), {\'j\': bint(J), \'n\': bint(N)}, reals())\n    check_funsor(x(j=m), {\'i\': bint(I), \'m\': bint(M)}, reals())\n    check_funsor(x(j=m, i=n), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(j=m, i=n, k=m), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(j=m, k=m), {\'i\': bint(I), \'m\': bint(M)}, reals())\n    check_funsor(x(j=n), {\'i\': bint(I), \'n\': bint(N)}, reals())\n    check_funsor(x(j=n, k=m), {\'i\': bint(I), \'n\': bint(N)}, reals())\n    check_funsor(x(m), {\'j\': bint(J), \'m\': bint(M)}, reals())\n    check_funsor(x(m, j=n), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(m, j=n, k=m), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(m, k=m), {\'j\': bint(J), \'m\': bint(M)}, reals())\n    check_funsor(x(m, n), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(m, n, k=m), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(n), {\'j\': bint(J), \'n\': bint(N)}, reals())\n    check_funsor(x(n, k=m), {\'j\': bint(J), \'n\': bint(N)}, reals())\n    check_funsor(x(n, m), {\'m\': bint(M), \'n\': bint(N)}, reals())\n    check_funsor(x(n, m, k=m), {\'m\': bint(M), \'n\': bint(N)}, reals())\n\n\ndef test_slice_simple():\n    t = randn((3, 4, 5))\n    f = Tensor(t)[""i"", ""j""]\n    assert_close(f, f(i=Slice(""i"", 3)))\n    assert_close(f, f(j=Slice(""j"", 4)))\n    assert_close(f, f(i=Slice(""i"", 3), j=Slice(""j"", 4)))\n    assert_close(f, f(i=Slice(""i"", 3), j=""j""))\n    assert_close(f, f(i=""i"", j=Slice(""j"", 4)))\n\n\n@pytest.mark.parametrize(""stop"", [0, 1, 2, 10])\ndef test_slice_1(stop):\n    t = randn((10, 2))\n    actual = Tensor(t)[""i""](i=Slice(""j"", stop, dtype=10))\n    expected = Tensor(t[:stop])[""j""]\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(""start"", [0, 1, 2, 10])\n@pytest.mark.parametrize(""stop"", [0, 1, 2, 10])\n@pytest.mark.parametrize(""step"", [1, 2, 5, 10])\ndef test_slice_2(start, stop, step):\n    t = randn((10, 2))\n    actual = Tensor(t)[""i""](i=Slice(""j"", start, stop, step, dtype=10))\n    expected = Tensor(t[start: stop: step])[""j""]\n    assert_close(actual, expected)\n\n\ndef test_arange_simple():\n    t = randn((3, 4, 5))\n    f = Tensor(t)[""i"", ""j""]\n    assert_close(f, f(i=f.new_arange(""i"", 3)))\n    assert_close(f, f(j=f.new_arange(""j"", 4)))\n    assert_close(f, f(i=f.new_arange(""i"", 3), j=f.new_arange(""j"", 4)))\n    assert_close(f, f(i=f.new_arange(""i"", 3), j=""j""))\n    assert_close(f, f(i=""i"", j=f.new_arange(""j"", 4)))\n\n\n@pytest.mark.parametrize(""stop"", [0, 1, 2, 10])\ndef test_arange_1(stop):\n    t = randn((10, 2))\n    f = Tensor(t)[""i""]\n    actual = f(i=f.new_arange(""j"", stop, dtype=10))\n    expected = Tensor(t[:stop])[""j""]\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(""start"", [0, 1, 2, 10])\n@pytest.mark.parametrize(""stop"", [0, 1, 2, 10])\n@pytest.mark.parametrize(""step"", [1, 2, 5, 10])\ndef test_arange_2(start, stop, step):\n    t = randn((10, 2))\n    f = Tensor(t)[""i""]\n    actual = f(i=f.new_arange(""j"", start, stop, step, dtype=10))\n    expected = Tensor(t[start: stop: step])[""j""]\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'output_shape\', [(), (7,), (3, 2)])\ndef test_advanced_indexing_tensor(output_shape):\n    #      u   v\n    #     / \\ / \\\n    #    i   j   k\n    #     \\  |  /\n    #      \\ | /\n    #        x\n    output = reals(*output_shape)\n    x = random_tensor(OrderedDict([\n        (\'i\', bint(2)),\n        (\'j\', bint(3)),\n        (\'k\', bint(4)),\n    ]), output)\n    i = random_tensor(OrderedDict([\n        (\'u\', bint(5)),\n    ]), bint(2))\n    j = random_tensor(OrderedDict([\n        (\'v\', bint(6)),\n        (\'u\', bint(5)),\n    ]), bint(3))\n    k = random_tensor(OrderedDict([\n        (\'v\', bint(6)),\n    ]), bint(4))\n\n    expected_data = empty((5, 6) + output_shape)\n    for u in range(5):\n        for v in range(6):\n            expected_data[u, v] = x.data[i.data[u], j.data[v, u], k.data[v]]\n    expected = Tensor(expected_data, OrderedDict([\n        (\'u\', bint(5)),\n        (\'v\', bint(6)),\n    ]))\n\n    assert_equiv(expected, x(i, j, k))\n    assert_equiv(expected, x(i=i, j=j, k=k))\n\n    assert_equiv(expected, x(i=i, j=j)(k=k))\n    assert_equiv(expected, x(j=j, k=k)(i=i))\n    assert_equiv(expected, x(k=k, i=i)(j=j))\n\n    assert_equiv(expected, x(i=i)(j=j, k=k))\n    assert_equiv(expected, x(j=j)(k=k, i=i))\n    assert_equiv(expected, x(k=k)(i=i, j=j))\n\n    assert_equiv(expected, x(i=i)(j=j)(k=k))\n    assert_equiv(expected, x(i=i)(k=k)(j=j))\n    assert_equiv(expected, x(j=j)(i=i)(k=k))\n    assert_equiv(expected, x(j=j)(k=k)(i=i))\n    assert_equiv(expected, x(k=k)(i=i)(j=j))\n    assert_equiv(expected, x(k=k)(j=j)(i=i))\n\n\n@pytest.mark.parametrize(\'output_shape\', [(), (7,), (3, 2)])\ndef test_advanced_indexing_lazy(output_shape):\n    x = Tensor(randn((2, 3, 4) + output_shape), OrderedDict([\n        (\'i\', bint(2)),\n        (\'j\', bint(3)),\n        (\'k\', bint(4)),\n    ]))\n    u = Variable(\'u\', bint(2))\n    v = Variable(\'v\', bint(3))\n    with interpretation(lazy):\n        i = Number(1, 2) - u\n        j = Number(2, 3) - v\n        k = u + v\n\n    expected_data = empty((2, 3) + output_shape)\n    i_data = x.materialize(i).data\n    j_data = x.materialize(j).data\n    k_data = x.materialize(k).data\n    for u in range(2):\n        for v in range(3):\n            expected_data[u, v] = x.data[i_data[u], j_data[v], k_data[u, v]]\n    expected = Tensor(expected_data, OrderedDict([\n        (\'u\', bint(2)),\n        (\'v\', bint(3)),\n    ]))\n\n    assert_equiv(expected, x(i, j, k))\n    assert_equiv(expected, x(i=i, j=j, k=k))\n\n    assert_equiv(expected, x(i=i, j=j)(k=k))\n    assert_equiv(expected, x(j=j, k=k)(i=i))\n    assert_equiv(expected, x(k=k, i=i)(j=j))\n\n    assert_equiv(expected, x(i=i)(j=j, k=k))\n    assert_equiv(expected, x(j=j)(k=k, i=i))\n    assert_equiv(expected, x(k=k)(i=i, j=j))\n\n    assert_equiv(expected, x(i=i)(j=j)(k=k))\n    assert_equiv(expected, x(i=i)(k=k)(j=j))\n    assert_equiv(expected, x(j=j)(i=i)(k=k))\n    assert_equiv(expected, x(j=j)(k=k)(i=i))\n    assert_equiv(expected, x(k=k)(i=i)(j=j))\n    assert_equiv(expected, x(k=k)(j=j)(i=i))\n\n\ndef unary_eval(symbol, x):\n    if symbol in [\'~\', \'-\']:\n        return eval(\'{} x\'.format(symbol))\n    return getattr(x, symbol)()\n\n\n@pytest.mark.parametrize(\'dims\', [(), (\'a\',), (\'a\', \'b\')])\n@pytest.mark.parametrize(\'symbol\', [\n    \'~\', \'-\', \'abs\', \'sqrt\', \'exp\', \'log\', \'log1p\', \'sigmoid\',\n])\ndef test_unary(symbol, dims):\n    sizes = {\'a\': 3, \'b\': 4}\n    shape = tuple(sizes[d] for d in dims)\n    inputs = OrderedDict((d, bint(sizes[d])) for d in dims)\n    dtype = \'real\'\n    data = rand(shape) + 0.5\n    if symbol == \'~\':\n        data = ops.astype(data, \'uint8\')\n        dtype = 2\n    if get_backend() != ""torch"" and symbol in [""abs"", ""sqrt"", ""exp"", ""log"", ""log1p"", ""sigmoid""]:\n        expected_data = getattr(ops, symbol)(data)\n    else:\n        expected_data = unary_eval(symbol, data)\n\n    x = Tensor(data, inputs, dtype)\n    actual = unary_eval(symbol, x)\n    check_funsor(actual, inputs, funsor.Domain((), dtype), expected_data)\n\n\nBINARY_OPS = [\n    \'+\', \'-\', \'*\', \'/\', \'**\', \'==\', \'!=\', \'<\', \'<=\', \'>\', \'>=\',\n    \'min\', \'max\',\n]\nBOOLEAN_OPS = [\'&\', \'|\', \'^\']\n\n\ndef binary_eval(symbol, x, y):\n    if symbol == \'min\':\n        return funsor.ops.min(x, y)\n    if symbol == \'max\':\n        return funsor.ops.max(x, y)\n    return eval(\'x {} y\'.format(symbol))\n\n\n@pytest.mark.parametrize(\'dims2\', [(), (\'a\',), (\'b\', \'a\'), (\'b\', \'c\', \'a\')])\n@pytest.mark.parametrize(\'dims1\', [(), (\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')])\n@pytest.mark.parametrize(\'symbol\', BINARY_OPS + BOOLEAN_OPS)\ndef test_binary_funsor_funsor(symbol, dims1, dims2):\n    sizes = {\'a\': 3, \'b\': 4, \'c\': 5}\n    shape1 = tuple(sizes[d] for d in dims1)\n    shape2 = tuple(sizes[d] for d in dims2)\n    inputs1 = OrderedDict((d, bint(sizes[d])) for d in dims1)\n    inputs2 = OrderedDict((d, bint(sizes[d])) for d in dims2)\n    data1 = rand(shape1) + 0.5\n    data2 = rand(shape2) + 0.5\n    dtype = \'real\'\n    if symbol in BOOLEAN_OPS:\n        dtype = 2\n        data1 = ops.astype(data1, \'uint8\')\n        data2 = ops.astype(data2, \'uint8\')\n    x1 = Tensor(data1, inputs1, dtype)\n    x2 = Tensor(data2, inputs2, dtype)\n    inputs, aligned = align_tensors(x1, x2)\n    expected_data = binary_eval(symbol, aligned[0], aligned[1])\n\n    actual = binary_eval(symbol, x1, x2)\n    check_funsor(actual, inputs, Domain((), dtype), expected_data)\n\n\n@pytest.mark.parametrize(\'output_shape2\', [(), (2,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'output_shape1\', [(), (2,), (3, 2)], ids=str)\n@pytest.mark.parametrize(\'inputs2\', [(), (\'a\',), (\'b\', \'a\'), (\'b\', \'c\', \'a\')], ids=str)\n@pytest.mark.parametrize(\'inputs1\', [(), (\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')], ids=str)\ndef test_binary_broadcast(inputs1, inputs2, output_shape1, output_shape2):\n    sizes = {\'a\': 4, \'b\': 5, \'c\': 6}\n    inputs1 = OrderedDict((k, bint(sizes[k])) for k in inputs1)\n    inputs2 = OrderedDict((k, bint(sizes[k])) for k in inputs2)\n    x1 = random_tensor(inputs1, reals(*output_shape1))\n    x2 = random_tensor(inputs1, reals(*output_shape2))\n\n    actual = x1 + x2\n    assert actual.output == find_domain(ops.add, x1.output, x2.output)\n\n    block = {\'a\': 1, \'b\': 2, \'c\': 3}\n    actual_block = actual(**block)\n    expected_block = Tensor(x1(**block).data + x2(**block).data)\n    assert_close(actual_block, expected_block)\n\n\n@pytest.mark.parametrize(\'output_shape2\', [(2,), (2, 5), (4, 2, 5)], ids=str)\n@pytest.mark.parametrize(\'output_shape1\', [(2,), (3, 2), (4, 3, 2)], ids=str)\n@pytest.mark.parametrize(\'inputs2\', [(), (\'a\',), (\'b\', \'a\'), (\'b\', \'c\', \'a\')], ids=str)\n@pytest.mark.parametrize(\'inputs1\', [(), (\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')], ids=str)\ndef test_matmul(inputs1, inputs2, output_shape1, output_shape2):\n    sizes = {\'a\': 6, \'b\': 7, \'c\': 8}\n    inputs1 = OrderedDict((k, bint(sizes[k])) for k in inputs1)\n    inputs2 = OrderedDict((k, bint(sizes[k])) for k in inputs2)\n    x1 = random_tensor(inputs1, reals(*output_shape1))\n    x2 = random_tensor(inputs1, reals(*output_shape2))\n\n    actual = x1 @ x2\n    assert actual.output == find_domain(ops.matmul, x1.output, x2.output)\n\n    block = {\'a\': 1, \'b\': 2, \'c\': 3}\n    actual_block = actual(**block)\n    expected_block = Tensor(x1(**block).data @ x2(**block).data)\n    assert_close(actual_block, expected_block, atol=1e-5, rtol=1e-5)\n\n\n@pytest.mark.parametrize(\'scalar\', [0.5])\n@pytest.mark.parametrize(\'dims\', [(), (\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')])\n@pytest.mark.parametrize(\'symbol\', BINARY_OPS)\ndef test_binary_funsor_scalar(symbol, dims, scalar):\n    sizes = {\'a\': 3, \'b\': 4, \'c\': 5}\n    shape = tuple(sizes[d] for d in dims)\n    inputs = OrderedDict((d, bint(sizes[d])) for d in dims)\n    data1 = rand(shape) + 0.5\n    expected_data = binary_eval(symbol, data1, scalar)\n\n    x1 = Tensor(data1, inputs)\n    actual = binary_eval(symbol, x1, scalar)\n    check_funsor(actual, inputs, reals(), expected_data)\n\n\n@pytest.mark.parametrize(\'scalar\', [0.5])\n@pytest.mark.parametrize(\'dims\', [(), (\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')])\n@pytest.mark.parametrize(\'symbol\', BINARY_OPS)\ndef test_binary_scalar_funsor(symbol, dims, scalar):\n    sizes = {\'a\': 3, \'b\': 4, \'c\': 5}\n    shape = tuple(sizes[d] for d in dims)\n    inputs = OrderedDict((d, bint(sizes[d])) for d in dims)\n    data1 = rand(shape) + 0.5\n    expected_data = binary_eval(symbol, scalar, data1)\n\n    x1 = Tensor(data1, inputs)\n    actual = binary_eval(symbol, scalar, x1)\n    check_funsor(actual, inputs, reals(), expected_data)\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (5,), (4, 3)])\n@pytest.mark.parametrize(""old_shape,new_shape"", [\n    ((), ()),\n    ((), (1,)),\n    ((2,), (2, 1)),\n    ((2,), (1, 2)),\n    ((6,), (2, 3)),\n    ((6,), (2, 1, 3)),\n    ((2, 3, 2), (3, 2, 2)),\n    ((2, 3, 2), (2, 2, 3)),\n])\ndef test_reshape(batch_shape, old_shape, new_shape):\n    inputs = OrderedDict(zip(""abc"", map(bint, batch_shape)))\n    old = random_tensor(inputs, reals(*old_shape))\n    assert old.reshape(old.shape) is old\n\n    new = old.reshape(new_shape)\n    assert new.inputs == inputs\n    assert new.shape == new_shape\n    assert new.dtype == old.dtype\n\n    old2 = new.reshape(old_shape)\n    assert_close(old2, old)\n\n\ndef test_getitem_number_0_inputs():\n    data = randn((5, 4, 3, 2))\n    x = Tensor(data)\n    assert_close(x[2], Tensor(data[2]))\n    assert_close(x[:, 1], Tensor(data[:, 1]))\n    assert_close(x[2, 1], Tensor(data[2, 1]))\n    assert_close(x[2, :, 1], Tensor(data[2, :, 1]))\n    assert_close(x[3, ...], Tensor(data[3, ...]))\n    assert_close(x[3, 2, ...], Tensor(data[3, 2, ...]))\n    assert_close(x[..., 1], Tensor(data[..., 1]))\n    assert_close(x[..., 2, 1], Tensor(data[..., 2, 1]))\n    assert_close(x[3, ..., 1], Tensor(data[3, ..., 1]))\n\n\ndef test_getitem_number_1_inputs():\n    data = randn((3, 5, 4, 3, 2))\n    inputs = OrderedDict([(\'i\', bint(3))])\n    x = Tensor(data, inputs)\n    assert_close(x[2], Tensor(data[:, 2], inputs))\n    assert_close(x[:, 1], Tensor(data[:, :, 1], inputs))\n    assert_close(x[2, 1], Tensor(data[:, 2, 1], inputs))\n    assert_close(x[2, :, 1], Tensor(data[:, 2, :, 1], inputs))\n    assert_close(x[3, ...], Tensor(data[:, 3, ...], inputs))\n    assert_close(x[3, 2, ...], Tensor(data[:, 3, 2, ...], inputs))\n    assert_close(x[..., 1], Tensor(data[..., 1], inputs))\n    assert_close(x[..., 2, 1], Tensor(data[..., 2, 1], inputs))\n    assert_close(x[3, ..., 1], Tensor(data[:, 3, ..., 1], inputs))\n\n\ndef test_getitem_number_2_inputs():\n    data = randn((3, 4, 5, 4, 3, 2))\n    inputs = OrderedDict([(\'i\', bint(3)), (\'j\', bint(4))])\n    x = Tensor(data, inputs)\n    assert_close(x[2], Tensor(data[:, :, 2], inputs))\n    assert_close(x[:, 1], Tensor(data[:, :, :, 1], inputs))\n    assert_close(x[2, 1], Tensor(data[:, :, 2, 1], inputs))\n    assert_close(x[2, :, 1], Tensor(data[:, :, 2, :, 1], inputs))\n    assert_close(x[3, ...], Tensor(data[:, :, 3, ...], inputs))\n    assert_close(x[3, 2, ...], Tensor(data[:, :, 3, 2, ...], inputs))\n    assert_close(x[..., 1], Tensor(data[..., 1], inputs))\n    assert_close(x[..., 2, 1], Tensor(data[..., 2, 1], inputs))\n    assert_close(x[3, ..., 1], Tensor(data[:, :, 3, ..., 1], inputs))\n\n\ndef test_getitem_variable():\n    data = randn((5, 4, 3, 2))\n    x = Tensor(data)\n    i = Variable(\'i\', bint(5))\n    j = Variable(\'j\', bint(4))\n    assert x[i] is Tensor(data, OrderedDict([(\'i\', bint(5))]))\n    assert x[i, j] is Tensor(data, OrderedDict([(\'i\', bint(5)), (\'j\', bint(4))]))\n\n\ndef test_getitem_string():\n    data = randn((5, 4, 3, 2))\n    x = Tensor(data)\n    assert x[\'i\'] is Tensor(data, OrderedDict([(\'i\', bint(5))]))\n    assert x[\'i\', \'j\'] is Tensor(data, OrderedDict([(\'i\', bint(5)), (\'j\', bint(4))]))\n\n\ndef test_getitem_tensor():\n    data = randn((5, 4, 3, 2))\n    x = Tensor(data)\n    i = Variable(\'i\', bint(5))\n    j = Variable(\'j\', bint(4))\n    k = Variable(\'k\', bint(3))\n    m = Variable(\'m\', bint(2))\n\n    y = random_tensor(OrderedDict(), bint(5))\n    assert_close(x[i](i=y), x[y])\n\n    y = random_tensor(OrderedDict(), bint(4))\n    assert_close(x[:, j](j=y), x[:, y])\n\n    y = random_tensor(OrderedDict(), bint(3))\n    assert_close(x[:, :, k](k=y), x[:, :, y])\n\n    y = random_tensor(OrderedDict(), bint(2))\n    assert_close(x[:, :, :, m](m=y), x[:, :, :, y])\n\n    y = random_tensor(OrderedDict([(\'i\', i.output)]),\n                      bint(j.dtype))\n    assert_close(x[i, j](j=y), x[i, y])\n\n    y = random_tensor(OrderedDict([(\'i\', i.output), (\'j\', j.output)]),\n                      bint(k.dtype))\n    assert_close(x[i, j, k](k=y), x[i, j, y])\n\n\ndef test_lambda_getitem():\n    data = randn((2,))\n    x = Tensor(data)\n    y = Tensor(data, OrderedDict(i=bint(2)))\n    i = Variable(\'i\', bint(2))\n    assert x[i] is y\n    assert Lambda(i, y) is x\n\n\nREDUCE_OPS = [\n    ops.add,\n    ops.mul,\n    ops.and_,\n    ops.or_,\n    ops.logaddexp,\n    ops.sample,\n    ops.min,\n    ops.max,\n]\n\n\n@pytest.mark.parametrize(\'dims\', [(), (\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')])\n@pytest.mark.parametrize(\'op\', REDUCE_OPS, ids=str)\ndef test_reduce_all(dims, op):\n    sizes = {\'a\': 3, \'b\': 4, \'c\': 5}\n    shape = tuple(sizes[d] for d in dims)\n    inputs = OrderedDict((d, bint(sizes[d])) for d in dims)\n    data = rand(shape) + 0.5\n    if op in [ops.and_, ops.or_]:\n        data = ops.astype(data, \'uint8\')\n    expected_data = REDUCE_OP_TO_NUMERIC[op](data, None)\n\n    x = Tensor(data, inputs)\n    actual = x.reduce(op)\n    check_funsor(actual, {}, reals(), expected_data)\n\n\n@pytest.mark.parametrize(\'dims,reduced_vars\', [\n    (dims, reduced_vars)\n    for dims in [(\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')]\n    for num_reduced in range(len(dims) + 2)\n    for reduced_vars in itertools.combinations(dims, num_reduced)\n])\n@pytest.mark.parametrize(\'op\', REDUCE_OPS)\ndef test_reduce_subset(dims, reduced_vars, op):\n    reduced_vars = frozenset(reduced_vars)\n    sizes = {\'a\': 3, \'b\': 4, \'c\': 5}\n    shape = tuple(sizes[d] for d in dims)\n    inputs = OrderedDict((d, bint(sizes[d])) for d in dims)\n    data = rand(shape) + 0.5\n    dtype = \'real\'\n    if op in [ops.and_, ops.or_]:\n        data = ops.astype(data, \'uint8\')\n        dtype = 2\n    x = Tensor(data, inputs, dtype)\n    actual = x.reduce(op, reduced_vars)\n    expected_inputs = OrderedDict(\n        (d, bint(sizes[d])) for d in dims if d not in reduced_vars)\n\n    reduced_vars &= frozenset(dims)\n    if not reduced_vars:\n        assert actual is x\n    else:\n        if reduced_vars == frozenset(dims):\n            data = REDUCE_OP_TO_NUMERIC[op](data, None)\n        else:\n            for pos in reversed(sorted(map(dims.index, reduced_vars))):\n                data = REDUCE_OP_TO_NUMERIC[op](data, pos)\n        check_funsor(actual, expected_inputs, Domain((), dtype))\n        assert_close(actual, Tensor(data, expected_inputs, dtype),\n                     atol=1e-5, rtol=1e-5)\n\n\n@pytest.mark.parametrize(\'dims\', [(), (\'a\',), (\'a\', \'b\'), (\'b\', \'a\', \'c\')])\n@pytest.mark.parametrize(\'event_shape\', [(), (4,), (2, 3)])\n@pytest.mark.parametrize(\'op\', REDUCE_OPS, ids=str)\ndef test_reduce_event(op, event_shape, dims):\n    sizes = {\'a\': 3, \'b\': 4, \'c\': 5}\n    batch_shape = tuple(sizes[d] for d in dims)\n    shape = batch_shape + event_shape\n    inputs = OrderedDict((d, bint(sizes[d])) for d in dims)\n    numeric_op = REDUCE_OP_TO_NUMERIC[op]\n    data = rand(shape) + 0.5\n    dtype = \'real\'\n    if op in [ops.and_, ops.or_]:\n        data = ops.astype(data, \'uint8\')\n    expected_data = numeric_op(data.reshape(batch_shape + (-1,)), -1)\n\n    x = Tensor(data, inputs, dtype=dtype)\n    op_name = numeric_op.__name__[1:] if op in [ops.min, ops.max] else numeric_op.__name__\n    actual = getattr(x, op_name)()\n    check_funsor(actual, inputs, Domain((), dtype), expected_data)\n\n\n@pytest.mark.parametrize(\'shape\', [(), (4,), (2, 3)])\ndef test_all_equal(shape):\n    inputs = OrderedDict()\n    data1 = rand(shape) + 0.5\n    data2 = rand(shape) + 0.5\n    dtype = \'real\'\n\n    x1 = Tensor(data1, inputs, dtype=dtype)\n    x2 = Tensor(data2, inputs, dtype=dtype)\n    assert (x1 == x1).all()\n    assert (x2 == x2).all()\n    assert not (x1 == x2).all()\n    assert not (x1 != x1).any()\n    assert not (x2 != x2).any()\n    assert (x1 != x2).any()\n\n\ndef test_function_matmul():\n    @funsor.function(reals(3, 4), reals(4, 5), reals(3, 5))\n    def matmul(x, y):\n        return x @ y\n\n    check_funsor(matmul, {\'x\': reals(3, 4), \'y\': reals(4, 5)}, reals(3, 5))\n\n    x = Tensor(randn((3, 4)))\n    y = Tensor(randn((4, 5)))\n    actual = matmul(x, y)\n    expected_data = x.data @ y.data\n    check_funsor(actual, {}, reals(3, 5), expected_data)\n\n\ndef test_function_lazy_matmul():\n\n    @funsor.function(reals(3, 4), reals(4, 5), reals(3, 5))\n    def matmul(x, y):\n        return x @ y\n\n    x_lazy = Variable(\'x\', reals(3, 4))\n    y = Tensor(randn((4, 5)))\n    actual_lazy = matmul(x_lazy, y)\n    check_funsor(actual_lazy, {\'x\': reals(3, 4)}, reals(3, 5))\n    assert isinstance(actual_lazy, funsor.tensor.Function)\n\n    x = Tensor(randn((3, 4)))\n    actual = actual_lazy(x=x)\n    expected_data = x.data @ y.data\n    check_funsor(actual, {}, reals(3, 5), expected_data)\n\n\ndef _numeric_max_and_argmax(x):\n    if get_backend() == ""torch"":\n        import torch\n\n        return torch.max(x, dim=-1)\n    else:\n        return np.max(x, axis=-1), np.argmax(x, axis=-1)\n\n\ndef test_function_nested_eager():\n\n    @funsor.function(reals(8), (reals(), bint(8)))\n    def max_and_argmax(x):\n        return tuple(_numeric_max_and_argmax(x))\n\n    inputs = OrderedDict([(\'i\', bint(2)), (\'j\', bint(3))])\n    x = Tensor(randn((2, 3, 8)), inputs)\n    m, a = _numeric_max_and_argmax(x.data)\n    expected_max = Tensor(m, inputs, \'real\')\n    expected_argmax = Tensor(a, inputs, 8)\n\n    actual_max, actual_argmax = max_and_argmax(x)\n    assert_close(actual_max, expected_max)\n    assert_close(actual_argmax, expected_argmax)\n\n\ndef test_function_nested_lazy():\n\n    @funsor.function(reals(8), (reals(), bint(8)))\n    def max_and_argmax(x):\n        return tuple(_numeric_max_and_argmax(x))\n\n    x_lazy = Variable(\'x\', reals(8))\n    lazy_max, lazy_argmax = max_and_argmax(x_lazy)\n    assert isinstance(lazy_max, funsor.tensor.Function)\n    assert isinstance(lazy_argmax, funsor.tensor.Function)\n    check_funsor(lazy_max, {\'x\': reals(8)}, reals())\n    check_funsor(lazy_argmax, {\'x\': reals(8)}, bint(8))\n\n    inputs = OrderedDict([(\'i\', bint(2)), (\'j\', bint(3))])\n    y = Tensor(randn((2, 3, 8)), inputs)\n    actual_max = lazy_max(x=y)\n    actual_argmax = lazy_argmax(x=y)\n    expected_max, expected_argmax = max_and_argmax(y)\n    assert_close(actual_max, expected_max)\n    assert_close(actual_argmax, expected_argmax)\n\n\ndef test_function_of_numeric_array():\n    backend = get_backend()\n    if backend == ""torch"":\n        import torch\n\n        matmul = torch.matmul\n    elif backend == ""jax"":\n        import jax\n\n        matmul = jax.numpy.matmul\n    else:\n        matmul = np.matmul\n    x = randn((4, 3))\n    y = randn((3, 2))\n    f = funsor.function(reals(4, 3), reals(3, 2), reals(4, 2))(matmul)\n    actual = f(x, y)\n    expected = f(Tensor(x), Tensor(y))\n    assert_close(actual, expected)\n\n\ndef test_align():\n    x = Tensor(randn((2, 3, 4)), OrderedDict([\n        (\'i\', bint(2)),\n        (\'j\', bint(3)),\n        (\'k\', bint(4)),\n    ]))\n    y = x.align((\'j\', \'k\', \'i\'))\n    assert isinstance(y, Tensor)\n    assert tuple(y.inputs) == (\'j\', \'k\', \'i\')\n    for i in range(2):\n        for j in range(3):\n            for k in range(4):\n                assert x(i=i, j=j, k=k) == y(i=i, j=j, k=k)\n\n\nEINSUM_EXAMPLES = [\n    \'a->a\',\n    \'a,a->a\',\n    \'a,b->\',\n    \'a,b->a\',\n    \'a,b->b\',\n    \'a,b->ab\',\n    \'a,b->ba\',\n    \'ab,ba->\',\n    \'ab,ba->a\',\n    \'ab,ba->b\',\n    \'ab,ba->ab\',\n    \'ab,ba->ba\',\n    \'ab,bc->ac\',\n]\n\n\n@pytest.mark.parametrize(\'equation\', EINSUM_EXAMPLES)\ndef test_einsum(equation):\n    sizes = dict(a=2, b=3, c=4)\n    inputs, outputs = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n    tensors = [randn(tuple(sizes[d] for d in dims)) for dims in inputs]\n    funsors = [Tensor(x) for x in tensors]\n    expected = Tensor(ops.einsum(equation, *tensors))\n    actual = Einsum(equation, tuple(funsors))\n    assert_close(actual, expected, atol=1e-5, rtol=None)\n\n\n@pytest.mark.parametrize(\'equation\', EINSUM_EXAMPLES)\n@pytest.mark.parametrize(\'batch1\', [\'\'])\n@pytest.mark.parametrize(\'batch2\', [\'\'])\ndef test_batched_einsum(equation, batch1, batch2):\n    inputs, output = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n\n    sizes = dict(a=2, b=3, c=4, i=5, j=6)\n    batch1 = OrderedDict([(k, bint(sizes[k])) for k in batch1])\n    batch2 = OrderedDict([(k, bint(sizes[k])) for k in batch2])\n    funsors = [random_tensor(batch, reals(*(sizes[d] for d in dims)))\n               for batch, dims in zip([batch1, batch2], inputs)]\n    actual = Einsum(equation, tuple(funsors))\n\n    _equation = \',\'.join(\'...\' + i for i in inputs) + \'->...\' + output\n    inputs, tensors = align_tensors(*funsors)\n    batch = tuple(v.size for v in inputs.values())\n    tensors = [ops.expand(x, batch + f.shape) for (x, f) in zip(tensors, funsors)]\n    expected = Tensor(ops.einsum(_equation, *tensors), inputs)\n    assert_close(actual, expected, atol=1e-5, rtol=None)\n\n\ndef _numeric_tensordot(x, y, dim):\n    if get_backend() == ""torch"":\n        import torch\n\n        return torch.tensordot(x, y, dim)\n    else:\n        return np.tensordot(x, y, axes=dim)\n\n\n@pytest.mark.parametrize(\'y_shape\', [(), (4,), (4, 5)], ids=str)\n@pytest.mark.parametrize(\'xy_shape\', [(), (6,), (6, 7)], ids=str)\n@pytest.mark.parametrize(\'x_shape\', [(), (2,), (2, 3)], ids=str)\ndef test_tensor_tensordot(x_shape, xy_shape, y_shape):\n    x = randn(x_shape + xy_shape)\n    y = randn(xy_shape + y_shape)\n    dim = len(xy_shape)\n    actual = tensordot(Tensor(x), Tensor(y), dim)\n    expected = Tensor(_numeric_tensordot(x, y, dim))\n    assert_close(actual, expected, atol=1e-5, rtol=None)\n\n\n@pytest.mark.parametrize(\'n\', [1, 2, 5])\n@pytest.mark.parametrize(\'shape,dim\', [\n    ((), 0),\n    ((), -1),\n    ((1,), 0),\n    ((1,), 1),\n    ((1,), -1),\n    ((1,), -2),\n    ((2, 3), 0),\n    ((2, 3), 1),\n    ((2, 3), 2),\n    ((2, 3), -1),\n    ((2, 3), -2),\n    ((2, 3), -3),\n], ids=str)\ndef test_tensor_stack(n, shape, dim):\n    tensors = [randn(shape) for _ in range(n)]\n    actual = stack(tuple(Tensor(t) for t in tensors), dim=dim)\n    expected = Tensor(ops.stack(dim, *tensors))\n    assert_close(actual, expected)\n\n\n@pytest.mark.parametrize(\'output\', [bint(2), reals(), reals(4), reals(2, 3)], ids=str)\ndef test_funsor_stack(output):\n    x = random_tensor(OrderedDict([\n        (\'i\', bint(2)),\n    ]), output)\n    y = random_tensor(OrderedDict([\n        (\'j\', bint(3)),\n    ]), output)\n    z = random_tensor(OrderedDict([\n        (\'i\', bint(2)),\n        (\'k\', bint(4)),\n    ]), output)\n\n    xy = Stack(\'t\', (x, y))\n    assert isinstance(xy, Tensor)\n    assert xy.inputs == OrderedDict([\n        (\'t\', bint(2)),\n        (\'i\', bint(2)),\n        (\'j\', bint(3)),\n    ])\n    assert xy.output == output\n    for j in range(3):\n        assert_close(xy(t=0, j=j), x)\n    for i in range(2):\n        assert_close(xy(t=1, i=i), y)\n\n    xyz = Stack(\'t\', (x, y, z))\n    assert isinstance(xyz, Tensor)\n    assert xyz.inputs == OrderedDict([\n        (\'t\', bint(3)),\n        (\'i\', bint(2)),\n        (\'j\', bint(3)),\n        (\'k\', bint(4)),\n    ])\n    assert xy.output == output\n    for j in range(3):\n        for k in range(4):\n            assert_close(xyz(t=0, j=j, k=k), x)\n    for i in range(2):\n        for k in range(4):\n            assert_close(xyz(t=1, i=i, k=k), y)\n    for j in range(3):\n        assert_close(xyz(t=2, j=j), z)\n\n\n@pytest.mark.parametrize(\'output\', [bint(2), reals(), reals(4), reals(2, 3)], ids=str)\ndef test_cat_simple(output):\n    x = random_tensor(OrderedDict([\n        (\'i\', bint(2)),\n    ]), output)\n    y = random_tensor(OrderedDict([\n        (\'i\', bint(3)),\n        (\'j\', bint(4)),\n    ]), output)\n    z = random_tensor(OrderedDict([\n        (\'i\', bint(5)),\n        (\'k\', bint(6)),\n    ]), output)\n\n    assert Cat(\'i\', (x,)) is x\n    assert Cat(\'i\', (y,)) is y\n    assert Cat(\'i\', (z,)) is z\n\n    xy = Cat(\'i\', (x, y))\n    assert isinstance(xy, Tensor)\n    assert xy.inputs == OrderedDict([\n        (\'i\', bint(2 + 3)),\n        (\'j\', bint(4)),\n    ])\n    assert xy.output == output\n\n    xyz = Cat(\'i\', (x, y, z))\n    assert isinstance(xyz, Tensor)\n    assert xyz.inputs == OrderedDict([\n        (\'i\', bint(2 + 3 + 5)),\n        (\'j\', bint(4)),\n        (\'k\', bint(6)),\n    ])\n    assert xy.output == output\n\n\n@pytest.mark.parametrize(""expand_shape"", [(4, 3, 2), (4, -1, 2), (4, 3, -1), (4, -1, -1)])\ndef test_ops_expand(expand_shape):\n    x = randn((3, 2))\n    actual = ops.expand(x, expand_shape)\n    assert actual.shape == (4, 3, 2)\n\n\ndef test_tensor_to_funsor_ambiguous_output():\n    x = randn((2, 1))\n    f = funsor.to_funsor(x, output=None, dim_to_name=OrderedDict({-2: \'a\'}))\n    f2 = funsor.to_funsor(x, output=reals(), dim_to_name=OrderedDict({-2: \'a\'}))\n    assert f.inputs == f2.inputs == OrderedDict(a=bint(2))\n    assert f.output.shape == () == f2.output.shape\n'"
test/test_terms.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport typing\nfrom collections import OrderedDict\nfrom functools import reduce\n\nimport numpy as np\nimport pytest\n\nimport funsor\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction\nfrom funsor.domains import Domain, bint, reals\nfrom funsor.interpreter import interpretation, reinterpret\nfrom funsor.tensor import REDUCE_OP_TO_NUMERIC\nfrom funsor.terms import (\n    Binary,\n    Cat,\n    Funsor,\n    Independent,\n    Lambda,\n    Number,\n    Reduce,\n    Slice,\n    Stack,\n    Subs,\n    Variable,\n    eager,\n    eager_or_die,\n    lazy,\n    normalize,\n    reflect,\n    sequential,\n    to_data,\n    to_funsor\n)\nfrom funsor.testing import assert_close, check_funsor, random_tensor\n\nassert Binary  # flake8\nassert Subs  # flake8\nassert Contraction  # flake8\n\nnp.seterr(all=\'ignore\')\n\n\ndef test_to_funsor():\n    assert to_funsor(0) is Number(0)\n\n\n@pytest.mark.parametrize(\'x\', [""foo"", list(), tuple(), set(), dict()])\ndef test_to_funsor_error(x):\n    with pytest.raises(ValueError):\n        to_funsor(x)\n\n\ndef test_to_data():\n    actual = to_data(Number(0.))\n    expected = 0.\n    assert type(actual) == type(expected)\n    assert actual == expected\n\n\ndef test_to_data_error():\n    with pytest.raises(ValueError):\n        to_data(Variable(\'x\', reals()))\n    with pytest.raises(ValueError):\n        to_data(Variable(\'y\', bint(12)))\n\n\ndef test_cons_hash():\n    assert Variable(\'x\', bint(3)) is Variable(\'x\', bint(3))\n    assert Variable(\'x\', reals()) is Variable(\'x\', reals())\n    assert Variable(\'x\', reals()) is not Variable(\'x\', bint(3))\n    assert Number(0, 3) is Number(0, 3)\n    assert Number(0.) is Number(0.)\n    assert Number(0.) is not Number(0, 3)\n    assert Slice(\'x\', 10) is Slice(\'x\', 10)\n    assert Slice(\'x\', 10) is Slice(\'x\', 0, 10)\n    assert Slice(\'x\', 10, 10) is not Slice(\'x\', 0, 10)\n    assert Slice(\'x\', 2, 10, 1) is Slice(\'x\', 2, 10)\n\n\ndef check_quote(x):\n    s = funsor.quote(x)\n    assert isinstance(s, str)\n    y = eval(s)\n    assert x is y\n\n\n@pytest.mark.parametrize(\'interp\', [reflect, lazy, normalize, eager], ids=lambda i: i.__name__)\ndef test_quote(interp):\n    with interpretation(interp):\n        x = Variable(\'x\', bint(8))\n        check_quote(x)\n\n        y = Variable(\'y\', reals(8, 3, 3))\n        check_quote(y)\n        check_quote(y[x])\n\n        z = Stack(\'i\', (Number(0), Variable(\'z\', reals())))\n        check_quote(z)\n        check_quote(z(i=0))\n        check_quote(z(i=Slice(\'i\', 0, 1, 1, 2)))\n        check_quote(z.reduce(ops.add, \'i\'))\n        check_quote(Cat(\'i\', (z, z, z)))\n        check_quote(Lambda(Variable(\'i\', bint(2)), z))\n\n\n@pytest.mark.parametrize(\'expr\', [\n    ""Variable(\'x\', bint(3))"",\n    ""Variable(\'x\', reals())"",\n    ""Number(0.)"",\n    ""Number(1, dtype=10)"",\n    ""-Variable(\'x\', reals())"",\n    ""Variable(\'x\', reals()) + Variable(\'y\', reals())"",\n    ""Variable(\'x\', reals())(x=Number(0.))"",\n])\ndef test_reinterpret(expr):\n    x = eval(expr)\n    assert funsor.reinterpret(x) is x\n\n\n@pytest.mark.parametrize(""expr"", [\n    ""Variable(\'x\', reals())"",\n    ""Number(1)"",\n    ""Number(1).log()"",\n    ""Number(1) + Number(2)"",\n    ""Stack(\'t\', (Number(1), Number(2)))"",\n    ""Stack(\'t\', (Number(1), Number(2))).reduce(ops.add, \'t\')"",\n])\ndef test_eager_or_die_ok(expr):\n    with interpretation(eager_or_die):\n        eval(expr)\n\n\n@pytest.mark.parametrize(""expr"", [\n    ""Variable(\'x\', reals()).log()"",\n    ""Number(1) / Variable(\'x\', reals())"",\n    ""Variable(\'x\', reals()) ** Number(2)"",\n    ""Stack(\'t\', (Number(1), Variable(\'x\', reals()))).reduce(ops.logaddexp, \'t\')"",\n])\ndef test_eager_or_die_error(expr):\n    with interpretation(eager_or_die):\n        with pytest.raises(NotImplementedError):\n            eval(expr)\n\n\n@pytest.mark.parametrize(\'domain\', [bint(3), reals()])\ndef test_variable(domain):\n    x = Variable(\'x\', domain)\n    check_funsor(x, {\'x\': domain}, domain)\n    assert Variable(\'x\', domain) is x\n    assert x(\'x\') is x\n    y = Variable(\'y\', domain)\n    assert x(\'y\') is y\n    assert x(x=\'y\') is y\n    assert x(x=y) is y\n    x4 = Variable(\'x\', bint(4))\n    assert x4 is not x\n    assert x4(\'x\') is x4\n    assert x(y=x4) is x\n\n    xp1 = x + 1.\n    assert xp1(x=2.) == 3.\n\n\ndef test_substitute():\n    x = Variable(\'x\', reals())\n    y = Variable(\'y\', reals())\n    z = Variable(\'z\', reals())\n\n    f = x * y + x * z\n\n    assert f(y=2) is x * 2 + x * z\n    assert f(z=2) is x * y + x * 2\n    assert f(y=x) is x * x + x * z\n    assert f(x=y) is y * y + y * z\n    assert f(y=z, z=y) is x * z + x * y\n    assert f(x=y, y=z, z=x) is y * z + y * x\n\n\ndef unary_eval(symbol, x):\n    if symbol in [\'~\', \'-\']:\n        return eval(\'{} x\'.format(symbol))\n    return getattr(ops, symbol)(x)\n\n\n@pytest.mark.parametrize(\'data\', [0, 0.5, 1])\n@pytest.mark.parametrize(\'symbol\', [\n    \'~\', \'-\', \'abs\', \'sqrt\', \'exp\', \'log\', \'log1p\', \'sigmoid\',\n])\ndef test_unary(symbol, data):\n    dtype = \'real\'\n    if symbol == \'~\':\n        data = bool(data)\n        dtype = 2\n    expected_data = unary_eval(symbol, data)\n\n    x = Number(data, dtype)\n    actual = unary_eval(symbol, x)\n    check_funsor(actual, {}, Domain((), dtype), expected_data)\n\n\nBINARY_OPS = [\n    \'+\', \'-\', \'*\', \'/\', \'**\', \'==\', \'!=\', \'<\', \'<=\', \'>\', \'>=\',\n    \'min\', \'max\',\n]\nBOOLEAN_OPS = [\'&\', \'|\', \'^\']\n\n\ndef binary_eval(symbol, x, y):\n    if symbol == \'min\':\n        return ops.min(x, y)\n    if symbol == \'max\':\n        return ops.max(x, y)\n    return eval(\'x {} y\'.format(symbol))\n\n\n@pytest.mark.parametrize(\'data1\', [0, 0.2, 1])\n@pytest.mark.parametrize(\'data2\', [0, 0.8, 1])\n@pytest.mark.parametrize(\'symbol\', BINARY_OPS + BOOLEAN_OPS)\ndef test_binary(symbol, data1, data2):\n    dtype = \'real\'\n    if symbol in BOOLEAN_OPS:\n        dtype = 2\n        data1 = bool(data1)\n        data2 = bool(data2)\n    try:\n        expected_data = binary_eval(symbol, data1, data2)\n    except ZeroDivisionError:\n        return\n\n    x1 = Number(data1, dtype)\n    x2 = Number(data2, dtype)\n    actual = binary_eval(symbol, x1, x2)\n    check_funsor(actual, {}, Domain((), dtype), expected_data)\n    with interpretation(normalize):\n        actual_reflect = binary_eval(symbol, x1, x2)\n    assert actual.output == actual_reflect.output\n\n\n@pytest.mark.parametrize(\'op\', REDUCE_OP_TO_NUMERIC,\n                         ids=[op.__name__ for op in REDUCE_OP_TO_NUMERIC])\ndef test_reduce_all(op):\n    x = Variable(\'x\', bint(2))\n    y = Variable(\'y\', bint(3))\n    z = Variable(\'z\', bint(4))\n    if isinstance(op, ops.LogAddExpOp):\n        pytest.skip()  # not defined for integers\n\n    with interpretation(sequential):\n        f = x * y + z\n        dtype = f.dtype\n        check_funsor(f, {\'x\': bint(2), \'y\': bint(3), \'z\': bint(4)}, Domain((), dtype))\n        actual = f.reduce(op)\n\n    with interpretation(sequential):\n        values = [f(x=i, y=j, z=k)\n                  for i in x.output\n                  for j in y.output\n                  for k in z.output]\n        expected = reduce(op, values)\n\n    assert actual == expected\n\n\n@pytest.mark.parametrize(\'reduced_vars\', [\n    reduced_vars\n    for num_reduced in range(3 + 1)\n    for reduced_vars in itertools.combinations(\'xyz\', num_reduced)\n])\n@pytest.mark.parametrize(\'op\', REDUCE_OP_TO_NUMERIC,\n                         ids=[op.__name__ for op in REDUCE_OP_TO_NUMERIC])\ndef test_reduce_subset(op, reduced_vars):\n    reduced_vars = frozenset(reduced_vars)\n    x = Variable(\'x\', bint(2))\n    y = Variable(\'y\', bint(3))\n    z = Variable(\'z\', bint(4))\n    f = x * y + z\n    dtype = f.dtype\n    check_funsor(f, {\'x\': bint(2), \'y\': bint(3), \'z\': bint(4)}, Domain((), dtype))\n    if isinstance(op, ops.LogAddExpOp):\n        pytest.skip()  # not defined for integers\n\n    with interpretation(sequential):\n        actual = f.reduce(op, reduced_vars)\n        expected = f\n        for v in [x, y, z]:\n            if v.name in reduced_vars:\n                expected = reduce(op, [expected(**{v.name: i}) for i in v.output])\n\n    try:\n        check_funsor(actual, expected.inputs, expected.output)\n    except AssertionError:\n        assert type(actual).__origin__ == type(expected).__origin__\n        assert actual.inputs == expected.inputs\n        assert actual.output.dtype != \'real\' and expected.output.dtype != \'real\'\n        pytest.xfail(reason=""bound inference not quite right"")\n\n    # TODO check data\n    if not reduced_vars:\n        assert actual is f\n\n\ndef test_reduce_syntactic_sugar():\n    i = Variable(""i"", bint(3))\n    x = Stack(""i"", (Number(1), Number(2), Number(3)))\n    expected = Number(1 + 2 + 3)\n    assert x.reduce(ops.add) is expected\n    assert x.reduce(ops.add, ""i"") is expected\n    assert x.reduce(ops.add, {""i""}) is expected\n    assert x.reduce(ops.add, frozenset([""i""])) is expected\n    assert x.reduce(ops.add, i) is expected\n    assert x.reduce(ops.add, {i}) is expected\n    assert x.reduce(ops.add, frozenset([i])) is expected\n\n\ndef test_slice():\n    t_slice = Slice(""t"", 10)\n\n    s_slice = t_slice(t=""s"")\n    assert isinstance(s_slice, Slice)\n    assert s_slice.slice == t_slice.slice\n    assert s_slice(s=""t"") is t_slice\n\n    assert t_slice(t=0) is Number(0, 10)\n    assert t_slice(t=1) is Number(1, 10)\n    assert t_slice(t=2) is Number(2, 10)\n    assert t_slice(t=t_slice) is t_slice\n\n\n@pytest.mark.parametrize(\'base_shape\', [(), (4,), (3, 2)], ids=str)\ndef test_lambda(base_shape):\n    z = Variable(\'z\', reals(*base_shape))\n    i = Variable(\'i\', bint(5))\n    j = Variable(\'j\', bint(7))\n\n    zi = Lambda(i, z)\n    assert zi.output.shape == (5,) + base_shape\n    assert zi[i] is z\n\n    zj = Lambda(j, z)\n    assert zj.output.shape == (7,) + base_shape\n    assert zj[j] is z\n\n    zij = Lambda(j, zi)\n    assert zij.output.shape == (7, 5) + base_shape\n    assert zij[j] is zi\n    assert zij[j, i] is z\n    # assert zij[:, i] is zj  # XXX this was disabled by alpha-renaming\n    check_funsor(zij[:, i], zj.inputs, zj.output)\n\n\ndef test_independent():\n    f = Variable(\'x_i\', reals(4, 5)) + random_tensor(OrderedDict(i=bint(3)))\n    assert f.inputs[\'x_i\'] == reals(4, 5)\n    assert f.inputs[\'i\'] == bint(3)\n\n    actual = Independent(f, \'x\', \'i\', \'x_i\')\n    assert actual.inputs[\'x\'] == reals(3, 4, 5)\n    assert \'i\' not in actual.inputs\n\n    x = Variable(\'x\', reals(3, 4, 5))\n    expected = f(x_i=x[\'i\']).reduce(ops.add, \'i\')\n    assert actual.inputs == expected.inputs\n    assert actual.output == expected.output\n\n    data = random_tensor(OrderedDict(), x.output)\n    assert_close(actual(data), expected(data), atol=1e-5, rtol=1e-5)\n\n    renamed = actual(x=\'y\')\n    assert isinstance(renamed, Independent)\n    assert_close(renamed(y=data), expected(x=data), atol=1e-5, rtol=1e-5)\n\n    # Ensure it\'s ok for .reals_var and .diag_var to be the same.\n    renamed = actual(x=\'x_i\')\n    assert isinstance(renamed, Independent)\n    assert_close(renamed(x_i=data), expected(x=data), atol=1e-5, rtol=1e-5)\n\n\ndef test_stack_simple():\n    x = Number(0.)\n    y = Number(1.)\n    z = Number(4.)\n\n    xyz = Stack(\'i\', (x, y, z))\n    check_funsor(xyz, {\'i\': bint(3)}, reals())\n\n    assert xyz(i=Number(0, 3)) is x\n    assert xyz(i=Number(1, 3)) is y\n    assert xyz(i=Number(2, 3)) is z\n    assert xyz.reduce(ops.add, \'i\') == 5.\n\n\ndef test_stack_subs():\n    x = Variable(\'x\', reals())\n    y = Variable(\'y\', reals())\n    z = Variable(\'z\', reals())\n    j = Variable(\'j\', bint(3))\n\n    f = Stack(\'i\', (Number(0), x, y * z))\n    check_funsor(f, {\'i\': bint(3), \'x\': reals(), \'y\': reals(), \'z\': reals()},\n                 reals())\n\n    assert f(i=Number(0, 3)) is Number(0)\n    assert f(i=Number(1, 3)) is x\n    assert f(i=Number(2, 3)) is y * z\n    assert f(i=j) is Stack(\'j\', (Number(0), x, y * z))\n    assert f(i=\'j\') is Stack(\'j\', (Number(0), x, y * z))\n    assert f.reduce(ops.add, \'i\') is Number(0) + x + (y * z)\n\n    assert f(x=0) is Stack(\'i\', (Number(0), Number(0), y * z))\n    assert f(y=x) is Stack(\'i\', (Number(0), x, x * z))\n    assert f(x=0, y=x) is Stack(\'i\', (Number(0), Number(0), x * z))\n    assert f(x=0, y=x, i=Number(2, 3)) is x * z\n    assert f(x=0, i=j) is Stack(\'j\', (Number(0), Number(0), y * z))\n    assert f(x=0, i=\'j\') is Stack(\'j\', (Number(0), Number(0), y * z))\n\n\n@pytest.mark.parametrize(""start,stop"", [(0, 1), (0, 2), (0, 10), (1, 2), (1, 10), (2, 10)])\n@pytest.mark.parametrize(""step"", [1, 2, 5, 10])\ndef test_stack_slice(start, stop, step):\n    xs = tuple(map(Number, range(10)))\n    actual = Stack(\'i\', xs)(i=Slice(\'j\', start, stop, step, dtype=10))\n    expected = Stack(\'j\', xs[start: stop: step])\n    assert type(actual) == type(expected)\n    assert actual.name == expected.name\n    assert actual.parts == expected.parts\n\n\ndef test_cat_simple():\n    x = Stack(\'i\', (Number(0), Number(1), Number(2)))\n    y = Stack(\'i\', (Number(3), Number(4)))\n\n    assert Cat(\'i\', (x,)) is x\n    assert Cat(\'i\', (y,)) is y\n\n    xy = Cat(\'i\', (x, y))\n    assert xy.inputs == OrderedDict(i=bint(5))\n    assert xy.name == \'i\'\n    for i in range(5):\n        assert xy(i=i) is Number(i)\n\n\ndef test_align_simple():\n    x = Variable(\'x\', reals())\n    y = Variable(\'y\', reals())\n    z = Variable(\'z\', reals())\n    f = z + y * x\n    assert tuple(f.inputs) == (\'z\', \'y\', \'x\')\n    g = f.align((\'x\', \'y\', \'z\'))\n    assert tuple(g.inputs) == (\'x\', \'y\', \'z\')\n    for k, v in f.inputs.items():\n        assert g.inputs[k] == v\n    assert f(x=1, y=2, z=3) == g(x=1, y=2, z=3)\n\n\n@pytest.mark.parametrize(""subcls_expr,cls_expr"", [\n    (""Reduce"", ""Reduce""),\n    (""Reduce[ops.AssociativeOp, Funsor, frozenset]"", ""Funsor""),\n    (""Reduce[ops.AssociativeOp, Funsor, frozenset]"", ""Reduce""),\n    (""Reduce[ops.AssociativeOp, Funsor, frozenset]"", ""Reduce[ops.Op, Funsor, frozenset]""),\n    (""Reduce[ops.AssociativeOp, Reduce[ops.AssociativeOp, Funsor, frozenset], frozenset]"",\n     ""Reduce[ops.Op, Funsor, frozenset]""),\n    (""Reduce[ops.AssociativeOp, Reduce[ops.AssociativeOp, Funsor, frozenset], frozenset]"",\n     ""Reduce[ops.AssociativeOp, Reduce, frozenset]""),\n    (""Stack[str, typing.Tuple[Number, Number, Number]]"", ""Stack""),\n    (""Stack[str, typing.Tuple[Number, Number, Number]]"", ""Stack[str, tuple]""),\n    # Unions\n    (""Reduce[ops.AssociativeOp, (Number, Stack[str, (tuple, typing.Tuple[Number, Number])]), frozenset]"", ""Funsor""),\n    (""Reduce[ops.AssociativeOp, (Number, Stack), frozenset]"", ""Reduce[ops.Op, Funsor, frozenset]""),\n    (""Reduce[ops.AssociativeOp, (Stack, Reduce[ops.AssociativeOp, (Number, Stack), frozenset]), frozenset]"",\n     ""Reduce[(ops.Op, ops.AssociativeOp), Stack, frozenset]""),\n])\ndef test_parametric_subclass(subcls_expr, cls_expr):\n    subcls = eval(subcls_expr)\n    cls = eval(cls_expr)\n    print(subcls.classname)\n    print(cls.classname)\n    assert issubclass(cls, (Funsor, Reduce)) and not issubclass(subcls, typing.Tuple)  # appease flake8\n    assert issubclass(subcls, cls)\n\n\n@pytest.mark.parametrize(""subcls_expr,cls_expr"", [\n    (""Funsor"", ""Reduce[ops.AssociativeOp, Funsor, frozenset]""),\n    (""Reduce"", ""Reduce[ops.AssociativeOp, Funsor, frozenset]""),\n    (""Reduce[ops.Op, Funsor, frozenset]"", ""Reduce[ops.AssociativeOp, Funsor, frozenset]""),\n    (""Reduce[ops.AssociativeOp, Reduce[ops.AssociativeOp, Funsor, frozenset], frozenset]"",\n     ""Reduce[ops.Op, Variable, frozenset]""),\n    (""Reduce[ops.AssociativeOp, Reduce[ops.AssociativeOp, Funsor, frozenset], frozenset]"",\n     ""Reduce[ops.AssociativeOp, Reduce[ops.AddOp, Funsor, frozenset], frozenset]""),\n    (""Stack"", ""Stack[str, typing.Tuple[Number, Number, Number]]""),\n    (""Stack[str, tuple]"", ""Stack[str, typing.Tuple[Number, Number, Number]]""),\n    (""Stack[str, typing.Tuple[Number, Number]]"", ""Stack[str, typing.Tuple[Number, Reduce]]""),\n    (""Stack[str, typing.Tuple[Number, Reduce]]"", ""Stack[str, typing.Tuple[Number, Number]]""),\n    # Unions\n    (""Funsor"", ""Reduce[ops.AssociativeOp, (Number, Funsor), frozenset]""),\n    (""Reduce[ops.Op, Funsor, frozenset]"", ""Reduce[ops.AssociativeOp, (Number, Stack), frozenset]""),\n    (""Reduce[(ops.Op, ops.AssociativeOp), Stack, frozenset]"",\n     ""Reduce[ops.AssociativeOp, (Stack[str, tuple], Reduce[ops.AssociativeOp, (Cat, Stack), frozenset]), frozenset]""),\n])\ndef test_not_parametric_subclass(subcls_expr, cls_expr):\n    subcls = eval(subcls_expr)\n    cls = eval(cls_expr)\n    print(subcls.classname)\n    print(cls.classname)\n    assert issubclass(cls, (Funsor, Reduce)) and not issubclass(subcls, typing.Tuple)  # appease flake8\n    assert not issubclass(subcls, cls)\n\n\n@pytest.mark.parametrize(""start,stop"", [\n    (1, 3), (0, 1), (3, 7), (4, 8), (0, 2), (0, 10), (1, 2), (1, 10), (2, 10)])\n@pytest.mark.parametrize(""step"", [1, 2, 3, 5, 10])\ndef test_cat_slice_tensor(start, stop, step):\n\n    terms = tuple(\n        random_tensor(OrderedDict(t=bint(t), a=bint(2)))\n        for t in [2, 1, 3, 4, 1, 3])\n    dtype = sum(term.inputs[\'t\'].dtype for term in terms)\n    sub = Slice(\'t\', start, stop, step, dtype)\n\n    # eager\n    expected = Cat(\'t\', terms)(t=sub)\n\n    # lazy - exercise Cat.eager_subs\n    with interpretation(lazy):\n        actual = Cat(\'t\', terms)(t=sub)\n    actual = reinterpret(actual)\n\n    assert_close(actual, expected)\n'"
docs/source/conf.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport sys\n\nimport sphinx_rtd_theme\n\n# import pkg_resources\n\n# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n# -- Project information -----------------------------------------------------\n\nproject = u\'Funsor\'\ncopyright = u\'2019, Uber Technologies, Inc\'\nauthor = u\'Uber AI Labs\'\n\n# The short X.Y version\nversion = u\'0.0\'\n# The full version, including alpha/beta/rc tags\nrelease = u\'0.0\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Disable documentation inheritance so as to avoid inheriting\n# docstrings in a different format, e.g. when the parent class\n# is a PyTorch class.\n\nautodoc_inherit_docstrings = False\n\n# FIXME the sphinx version on readthedocs does not\n# support this option. These must be manually added.\n# autodoc_default_options = {\n#     \'member-order\': \'bysource\',\n#     \'show-inheritance\': True,\n#     \'special-members\': True,\n#     \'undoc-members\': True,\n#     \'exclude-members\': \'__dict__,__module__,__weakref__\',\n# }\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# do not prepend module name to functions\nadd_module_names = False\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'funsordoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'Funsor.tex\', u\'Funsor Documentation\', u\'Uber AI Labs\', \'manual\'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'Funsor\', u\'Funsor Documentation\',\n     [author], 1)\n]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Funsor\', u\'Funsor Documentation\',\n     author, \'Funsor\', \'Functional analysis + tensors + symbolic algebra.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'torch\': (\'http://pytorch.org/docs/master/\', None),\n    \'pyro\': (\'http://docs.pyro.ai/en/stable/\', None),\n    \'opt_einsum\': (\'https://optimized-einsum.readthedocs.io/en/stable/\', None)\n}\n\n# @jpchen\'s hack to get rtd builder to install latest pytorch\nif \'READTHEDOCS\' in os.environ:\n    os.system(\'pip install torch==1.3.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\')\n    # pyro needs to be installed after torch so pyro doesnt install the bloated torch-1.0 wheel\n    os.system(\'pip install pyro-ppl\')\n'"
examples/mixed_hmm/__init__.py,0,b''
examples/mixed_hmm/experiment.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport functools\nimport json\nimport os\nimport uuid\n\nimport pyro\nimport pyro.poutine as poutine\nimport torch\n\nimport funsor.ops as ops\nfrom funsor.interpreter import interpretation\nfrom funsor.optimizer import apply_optimizer\nfrom funsor.sum_product import MarkovProduct, naive_sequential_sum_product, sum_product\nfrom funsor.terms import lazy, to_funsor\nfrom model import Guide, Model\nfrom seal_data import prepare_fake, prepare_seal\n\n\ndef aic_num_parameters(model, guide=None):\n    """"""\n    hacky AIC param count that includes all parameters in the model and guide\n    """"""\n    with poutine.block(), poutine.trace(param_only=True) as param_capture:\n        model()\n        if guide is not None:\n            guide()\n\n    return sum(node[""value""].numel() for node in param_capture.trace.nodes.values())\n\n\ndef parallel_loss_fn(model, guide, parallel=True):\n    # We\'re doing exact inference, so we don\'t use the guide here.\n    factors = model()\n    t_term, new_factors = factors[0], factors[1:]\n    t = to_funsor(""t"", t_term.inputs[""t""])\n    if parallel:\n        result = MarkovProduct(ops.logaddexp, ops.add, t_term, t, {""y(t=1)"": ""y""})\n    else:\n        result = naive_sequential_sum_product(ops.logaddexp, ops.add, t_term, t, {""y(t=1)"": ""y""})\n    new_factors = [result] + new_factors\n\n    plates = frozenset([\'g\', \'i\'])\n    eliminate = frozenset().union(*(f.inputs for f in new_factors))\n    with interpretation(lazy):\n        loss = sum_product(ops.logaddexp, ops.add, new_factors, eliminate, plates)\n    loss = apply_optimizer(loss)\n    assert not loss.inputs\n    return -loss.data\n\n\ndef run_expt(args):\n\n    optim = args[""optim""]\n    lr = args[""learnrate""]\n    schedule = [] if not args[""schedule""] else [int(i) for i in args[""schedule""].split("","")]\n    # default these to ""none"" instead of None, which argparse does for some reason\n    args[""group""] = ""none"" if args[""group""] is None else args[""group""]\n    args[""individual""] = ""none"" if args[""individual""] is None else args[""individual""]\n    random_effects = {""group"": args[""group""], ""individual"": args[""individual""]}\n\n    pyro.enable_validation(args[""validation""])\n    pyro.set_rng_seed(args[""seed""])  # reproducible random effect parameter init\n    if args[""cuda""]:\n        torch.set_default_tensor_type(torch.cuda.FloatTensor)\n\n    if args[""dataset""] == ""seal"":\n        filename = os.path.join(args[""folder""], ""prep_seal_data.csv"")\n        config = prepare_seal(filename, random_effects)\n    elif args[""dataset""] == ""fake"":\n        fake_sizes = {\n            ""state"": args[""size_state""],\n            ""random"": args[""size_random""],\n            ""group"": args[""size_group""],\n            ""individual"": args[""size_individual""],\n            ""timesteps"": args[""size_timesteps""],\n        }\n        config = prepare_fake(fake_sizes, random_effects)\n    else:\n        raise ValueError(""Dataset {} not yet included"".format(args[""dataset""]))\n\n    if args[""smoke""]:\n        args[""timesteps""] = 2\n        config[""sizes""][""timesteps""] = 3\n\n    if args[""truncate""] > 0:\n        config[""sizes""][""timesteps""] = args[""truncate""]\n\n    config[""zeroinflation""] = args[""zeroinflation""]\n\n    model = Model(config)\n    guide = Guide(config)\n    loss_fn = parallel_loss_fn\n\n    if args[""jit""]:\n        loss_fn = torch.jit.trace(lambda: loss_fn(model, guide, args[""parallel""]), ())\n    else:\n        loss_fn = functools.partial(loss_fn, model, guide, args[""parallel""])\n\n    # count the number of parameters once\n    num_parameters = aic_num_parameters(model, guide)\n\n    losses = []\n\n    # TODO support continuous random effects with monte carlo\n    assert random_effects[""group""] != ""continuous""\n    assert random_effects[""individual""] != ""continuous""\n\n    with pyro.poutine.trace(param_only=True) as param_capture:\n        loss_fn()\n    params = [site[""value""].unconstrained() for site in param_capture.trace.nodes.values()]\n    if optim == ""sgd"":\n        optimizer = torch.optim.Adam(params, lr=lr)\n    elif optim == ""lbfgs"":\n        optimizer = torch.optim.LBFGS(params, lr=lr)\n    else:\n        raise ValueError(""{} not supported optimizer"".format(optim))\n\n    if schedule:\n        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=schedule, gamma=0.5)\n        schedule_step_loss = False\n    else:\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \'min\')\n        schedule_step_loss = True\n\n    for t in range(args[""timesteps""]):\n        def closure():\n            optimizer.zero_grad()\n            loss = loss_fn()\n            loss.backward()\n            return loss\n        loss = optimizer.step(closure)\n        scheduler.step(loss.item() if schedule_step_loss else t)\n        losses.append(loss.item())\n        print(""Loss: {}, AIC[{}]: "".format(loss.item(), t),\n              2. * loss + 2. * num_parameters)\n\n    aic_final = 2. * losses[-1] + 2. * num_parameters\n    print(""AIC final: {}"".format(aic_final))\n\n    results = {}\n    results[""args""] = args\n    results[""sizes""] = config[""sizes""]\n    results[""likelihoods""] = losses\n    results[""likelihood_final""] = losses[-1]\n    results[""aic_final""] = aic_final\n    results[""aic_num_parameters""] = num_parameters\n\n    if args[""resultsdir""] is not None and os.path.exists(args[""resultsdir""]):\n        re_str = ""g"" + (""n"" if args[""group""] is None else ""d"" if args[""group""] == ""discrete"" else ""c"")\n        re_str += ""i"" + (""n"" if args[""individual""] is None else ""d"" if args[""individual""] == ""discrete"" else ""c"")\n        results_filename = ""expt_{}_{}_{}.json"".format(args[""dataset""], re_str, str(uuid.uuid4().hex)[0:5])\n        with open(os.path.join(args[""resultsdir""], results_filename), ""w"") as f:\n            json.dump(results, f)\n\n    return results\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-d"", ""--dataset"", default=""seal"", type=str)\n    parser.add_argument(""-g"", ""--group"", default=""none"", type=str)\n    parser.add_argument(""-i"", ""--individual"", default=""none"", type=str)\n    parser.add_argument(""-f"", ""--folder"", default=""./"", type=str)\n    parser.add_argument(""-o"", ""--optim"", default=""sgd"", type=str)\n    parser.add_argument(""-lr"", ""--learnrate"", default=0.05, type=float)\n    parser.add_argument(""-t"", ""--timesteps"", default=1000, type=int)\n    parser.add_argument(""-r"", ""--resultsdir"", default=""./results"", type=str)\n    parser.add_argument(""-zi"", ""--zeroinflation"", action=""store_true"")\n    parser.add_argument(""--seed"", default=101, type=int)\n    parser.add_argument(""--truncate"", default=-1, type=int)\n    parser.add_argument(""--jit"", action=""store_true"")\n    parser.add_argument(""--cuda"", action=""store_true"")\n    parser.add_argument(""--parallel"", action=""store_true"")\n    parser.add_argument(""--smoke"", action=""store_true"")\n    parser.add_argument(""--schedule"", default="""", type=str)\n    parser.add_argument(\'--validation\', action=\'store_true\')\n\n    # sizes for generating fake data\n    parser.add_argument(""-ss"", ""--size-state"", default=3, type=int)\n    parser.add_argument(""-sr"", ""--size-random"", default=4, type=int)\n    parser.add_argument(""-sg"", ""--size-group"", default=2, type=int)\n    parser.add_argument(""-si"", ""--size-individual"", default=20, type=int)\n    parser.add_argument(""-st"", ""--size-timesteps"", default=1800, type=int)\n    args = parser.parse_args()\n\n    if args.group == ""none"":\n        args.group = None\n    if args.individual == ""none"":\n        args.individual = None\n\n    run_expt(vars(args))\n'"
examples/mixed_hmm/model.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport pyro\nimport torch\nfrom torch.distributions import constraints\n\nimport funsor.torch.distributions as dist\nimport funsor.ops as ops\nfrom funsor.domains import bint, reals\nfrom funsor.tensor import Tensor\nfrom funsor.terms import Stack, Variable, to_funsor\n\n\nclass Guide(object):\n    """"""generic mean-field guide for continuous random effects""""""\n\n    def __init__(self, config):\n        self.config = config\n        self.params = self.initialize_params()\n\n    def initialize_params(self):\n\n        # dictionary of guide random effect parameters\n        params = {\n            ""eps_g"": {},\n            ""eps_i"": {},\n        }\n\n        N_state = self.config[""sizes""][""state""]\n\n        # initialize group-level parameters\n        if self.config[""group""][""random""] == ""continuous"":\n\n            params[""eps_g""][""loc""] = Tensor(\n                pyro.param(""loc_group"",\n                           lambda: torch.zeros((N_state, N_state))),\n                OrderedDict([(""y_prev"", bint(N_state))]),\n            )\n\n            params[""eps_g""][""scale""] = Tensor(\n                pyro.param(""scale_group"",\n                           lambda: torch.ones((N_state, N_state)),\n                           constraint=constraints.positive),\n                OrderedDict([(""y_prev"", bint(N_state))]),\n            )\n\n        # initialize individual-level random effect parameters\n        N_c = self.config[""sizes""][""group""]\n        if self.config[""individual""][""random""] == ""continuous"":\n\n            params[""eps_i""][""loc""] = Tensor(\n                pyro.param(""loc_individual"",\n                           lambda: torch.zeros((N_c, N_state, N_state))),\n                OrderedDict([(""g"", bint(N_c)), (""y_prev"", bint(N_state))]),\n            )\n\n            params[""eps_i""][""scale""] = Tensor(\n                pyro.param(""scale_individual"",\n                           lambda: torch.ones((N_c, N_state, N_state)),\n                           constraint=constraints.positive),\n                OrderedDict([(""g"", bint(N_c)), (""y_prev"", bint(N_state))]),\n            )\n\n        self.params = params\n        return self.params\n\n    def __call__(self):\n\n        # calls pyro.param so that params are exposed and constraints applied\n        # should not create any new torch.Tensors after __init__\n        self.initialize_params()\n\n        N_c = self.config[""sizes""][""group""]\n        N_s = self.config[""sizes""][""individual""]\n\n        log_prob = Tensor(torch.tensor(0.), OrderedDict())\n\n        plate_g = Tensor(torch.zeros(N_c), OrderedDict([(""g"", bint(N_c))]))\n        plate_i = Tensor(torch.zeros(N_s), OrderedDict([(""i"", bint(N_s))]))\n\n        if self.config[""group""][""random""] == ""continuous"":\n            eps_g_dist = plate_g + dist.Normal(**self.params[""eps_g""])(value=""eps_g"")\n            log_prob += eps_g_dist\n\n        # individual-level random effects\n        if self.config[""individual""][""random""] == ""continuous"":\n            eps_i_dist = plate_g + plate_i + dist.Normal(**self.params[""eps_i""])(value=""eps_i"")\n            log_prob += eps_i_dist\n\n        return log_prob\n\n\nclass Model(object):\n\n    def __init__(self, config):\n        self.config = config\n        self.params = self.initialize_params()\n        self.raggedness_masks = self.initialize_raggedness_masks()\n        self.observations = self.initialize_observations()\n\n    def initialize_params(self):\n\n        # return a dict of per-site params as funsor.tensor.Tensors\n        params = {\n            ""e_g"": {},\n            ""theta_g"": {},\n            ""eps_g"": {},\n            ""e_i"": {},\n            ""theta_i"": {},\n            ""eps_i"": {},\n            ""zi_step"": {},\n            ""step"": {},\n            ""angle"": {},\n            ""zi_omega"": {},\n            ""omega"": {},\n        }\n\n        # size parameters\n        N_v = self.config[""sizes""][""random""]\n        N_state = self.config[""sizes""][""state""]\n\n        # initialize group-level random effect parameters\n        if self.config[""group""][""random""] == ""discrete"":\n\n            params[""e_g""][""probs""] = Tensor(\n                pyro.param(""probs_e_g"",\n                           lambda: torch.randn((N_v,)).abs(),\n                           constraint=constraints.simplex),\n                OrderedDict(),\n            )\n\n            params[""eps_g""][""theta""] = Tensor(\n                pyro.param(""theta_g"",\n                           lambda: torch.randn((N_v, N_state, N_state))),\n                OrderedDict([(""e_g"", bint(N_v)), (""y_prev"", bint(N_state))]),\n            )\n\n        elif self.config[""group""][""random""] == ""continuous"":\n\n            # note these are prior values, trainable versions live in guide\n            params[""eps_g""][""loc""] = Tensor(\n                torch.zeros((N_state, N_state)),\n                OrderedDict([(""y_prev"", bint(N_state))]),\n            )\n\n            params[""eps_g""][""scale""] = Tensor(\n                torch.ones((N_state, N_state)),\n                OrderedDict([(""y_prev"", bint(N_state))]),\n            )\n\n        # initialize individual-level random effect parameters\n        N_c = self.config[""sizes""][""group""]\n        if self.config[""individual""][""random""] == ""discrete"":\n\n            params[""e_i""][""probs""] = Tensor(\n                pyro.param(""probs_e_i"",\n                           lambda: torch.randn((N_c, N_v,)).abs(),\n                           constraint=constraints.simplex),\n                OrderedDict([(""g"", bint(N_c))]),  # different value per group\n            )\n\n            params[""eps_i""][""theta""] = Tensor(\n                pyro.param(""theta_i"",\n                           lambda: torch.randn((N_c, N_v, N_state, N_state))),\n                OrderedDict([(""g"", bint(N_c)), (""e_i"", bint(N_v)), (""y_prev"", bint(N_state))]),\n            )\n\n        elif self.config[""individual""][""random""] == ""continuous"":\n\n            params[""eps_i""][""loc""] = Tensor(\n                torch.zeros((N_c, N_state, N_state)),\n                OrderedDict([(""g"", bint(N_c)), (""y_prev"", bint(N_state))]),\n            )\n\n            params[""eps_i""][""scale""] = Tensor(\n                torch.ones((N_c, N_state, N_state)),\n                OrderedDict([(""g"", bint(N_c)), (""y_prev"", bint(N_state))]),\n            )\n\n        # initialize likelihood parameters\n        # observation 1: step size (step ~ Gamma)\n        params[""zi_step""][""zi_param""] = Tensor(\n            pyro.param(""step_zi_param"",\n                       lambda: torch.ones((N_state, 2)),\n                       constraint=constraints.simplex),\n            OrderedDict([(""y_curr"", bint(N_state))]),\n        )\n\n        params[""step""][""concentration""] = Tensor(\n            pyro.param(""step_param_concentration"",\n                       lambda: torch.randn((N_state,)).abs(),\n                       constraint=constraints.positive),\n            OrderedDict([(""y_curr"", bint(N_state))]),\n        )\n\n        params[""step""][""rate""] = Tensor(\n            pyro.param(""step_param_rate"",\n                       lambda: torch.randn((N_state,)).abs(),\n                       constraint=constraints.positive),\n            OrderedDict([(""y_curr"", bint(N_state))]),\n        )\n\n        # observation 2: step angle (angle ~ VonMises)\n        params[""angle""][""concentration""] = Tensor(\n            pyro.param(""angle_param_concentration"",\n                       lambda: torch.randn((N_state,)).abs(),\n                       constraint=constraints.positive),\n            OrderedDict([(""y_curr"", bint(N_state))]),\n        )\n\n        params[""angle""][""loc""] = Tensor(\n            pyro.param(""angle_param_loc"",\n                       lambda: torch.randn((N_state,)).abs()),\n            OrderedDict([(""y_curr"", bint(N_state))]),\n        )\n\n        # observation 3: dive activity (omega ~ Beta)\n        params[""zi_omega""][""zi_param""] = Tensor(\n            pyro.param(""omega_zi_param"",\n                       lambda: torch.ones((N_state, 2)),\n                       constraint=constraints.simplex),\n            OrderedDict([(""y_curr"", bint(N_state))]),\n        )\n\n        params[""omega""][""concentration0""] = Tensor(\n            pyro.param(""omega_param_concentration0"",\n                       lambda: torch.randn((N_state,)).abs(),\n                       constraint=constraints.positive),\n            OrderedDict([(""y_curr"", bint(N_state))]),\n        )\n\n        params[""omega""][""concentration1""] = Tensor(\n            pyro.param(""omega_param_concentration1"",\n                       lambda: torch.randn((N_state,)).abs(),\n                       constraint=constraints.positive),\n            OrderedDict([(""y_curr"", bint(N_state))]),\n        )\n\n        self.params = params\n        return self.params\n\n    def initialize_observations(self):\n        """"""\n        Convert raw observation tensors into funsor.tensor.Tensors\n        """"""\n        batch_inputs = OrderedDict([\n            (""i"", bint(self.config[""sizes""][""individual""])),\n            (""g"", bint(self.config[""sizes""][""group""])),\n            (""t"", bint(self.config[""sizes""][""timesteps""])),\n        ])\n\n        observations = {}\n        for name, data in self.config[""observations""].items():\n            observations[name] = Tensor(data[..., :self.config[""sizes""][""timesteps""]], batch_inputs)\n\n        self.observations = observations\n        return self.observations\n\n    def initialize_raggedness_masks(self):\n        """"""\n        Convert raw raggedness tensors into funsor.tensor.Tensors\n        """"""\n        batch_inputs = OrderedDict([\n            (""i"", bint(self.config[""sizes""][""individual""])),\n            (""g"", bint(self.config[""sizes""][""group""])),\n            (""t"", bint(self.config[""sizes""][""timesteps""])),\n        ])\n\n        raggedness_masks = {}\n        for name in (""individual"", ""timestep""):\n            data = self.config[name][""mask""]\n            if len(data.shape) < len(batch_inputs):\n                while len(data.shape) < len(batch_inputs):\n                    data = data.unsqueeze(-1)\n                data = data.expand(tuple(v.dtype for v in batch_inputs.values()))\n            data = data.to(self.config[""observations""][""step""].dtype)\n            raggedness_masks[name] = Tensor(data[..., :self.config[""sizes""][""timesteps""]],\n                                            batch_inputs)\n\n        self.raggedness_masks = raggedness_masks\n        return self.raggedness_masks\n\n    def __call__(self):\n\n        # calls pyro.param so that params are exposed and constraints applied\n        # should not create any new torch.Tensors after __init__\n        self.initialize_params()\n\n        N_state = self.config[""sizes""][""state""]\n\n        # initialize gamma to uniform\n        gamma = Tensor(\n            torch.zeros((N_state, N_state)),\n            OrderedDict([(""y_prev"", bint(N_state))]),\n        )\n\n        N_v = self.config[""sizes""][""random""]\n        N_c = self.config[""sizes""][""group""]\n        log_prob = []\n\n        plate_g = Tensor(torch.zeros(N_c), OrderedDict([(""g"", bint(N_c))]))\n\n        # group-level random effects\n        if self.config[""group""][""random""] == ""discrete"":\n            # group-level discrete effect\n            e_g = Variable(""e_g"", bint(N_v))\n            e_g_dist = plate_g + dist.Categorical(**self.params[""e_g""])(value=e_g)\n\n            log_prob.append(e_g_dist)\n\n            eps_g = (plate_g + self.params[""eps_g""][""theta""])(e_g=e_g)\n\n        elif self.config[""group""][""random""] == ""continuous"":\n            eps_g = Variable(""eps_g"", reals(N_state))\n            eps_g_dist = plate_g + dist.Normal(**self.params[""eps_g""])(value=eps_g)\n\n            log_prob.append(eps_g_dist)\n        else:\n            eps_g = to_funsor(0.)\n\n        N_s = self.config[""sizes""][""individual""]\n\n        plate_i = Tensor(torch.zeros(N_s), OrderedDict([(""i"", bint(N_s))]))\n        # individual-level random effects\n        if self.config[""individual""][""random""] == ""discrete"":\n            # individual-level discrete effect\n            e_i = Variable(""e_i"", bint(N_v))\n            e_i_dist = plate_g + plate_i + dist.Categorical(\n                **self.params[""e_i""]\n            )(value=e_i) * self.raggedness_masks[""individual""](t=0)\n\n            log_prob.append(e_i_dist)\n\n            eps_i = (plate_i + plate_g + self.params[""eps_i""][""theta""](e_i=e_i))\n\n        elif self.config[""individual""][""random""] == ""continuous"":\n            eps_i = Variable(""eps_i"", reals(N_state))\n            eps_i_dist = plate_g + plate_i + dist.Normal(**self.params[""eps_i""])(value=eps_i)\n\n            log_prob.append(eps_i_dist)\n        else:\n            eps_i = to_funsor(0.)\n\n        # add group-level and individual-level random effects to gamma\n        gamma = gamma + eps_g + eps_i\n\n        N_state = self.config[""sizes""][""state""]\n\n        # we\'ve accounted for all effects, now actually compute gamma_y\n        gamma_y = gamma(y_prev=""y(t=1)"")\n\n        y = Variable(""y"", bint(N_state))\n        y_dist = plate_g + plate_i + dist.Categorical(\n            probs=gamma_y.exp() / gamma_y.exp().sum()\n        )(value=y)\n\n        # observation 1: step size\n        step_dist = plate_g + plate_i + dist.Gamma(\n            **{k: v(y_curr=y) for k, v in self.params[""step""].items()}\n        )(value=self.observations[""step""])\n\n        # step size zero-inflation\n        if self.config[""zeroinflation""]:\n            step_zi = dist.Categorical(probs=self.params[""zi_step""][""zi_param""](y_curr=y))(\n                value=""zi_step"")\n            step_zi_dist = plate_g + plate_i + dist.Delta(self.config[""MISSING""], 0.)(\n                value=self.observations[""step""])\n            step_dist = (step_zi + Stack(""zi_step"", (step_dist, step_zi_dist))).reduce(ops.logaddexp, ""zi_step"")\n\n        # observation 2: step angle\n        angle_dist = plate_g + plate_i + dist.VonMises(\n            **{k: v(y_curr=y) for k, v in self.params[""angle""].items()}\n        )(value=self.observations[""angle""])\n\n        # observation 3: dive activity\n        omega_dist = plate_g + plate_i + dist.Beta(\n            **{k: v(y_curr=y) for k, v in self.params[""omega""].items()}\n        )(value=self.observations[""omega""])\n\n        # dive activity zero-inflation\n        if self.config[""zeroinflation""]:\n            omega_zi = dist.Categorical(probs=self.params[""zi_omega""][""zi_param""](y_curr=y))(\n                value=""zi_omega"")\n            omega_zi_dist = plate_g + plate_i + dist.Delta(self.config[""MISSING""], 0.)(\n                value=self.observations[""omega""])\n            omega_dist = (omega_zi + Stack(""zi_omega"", (omega_dist, omega_zi_dist))).reduce(ops.logaddexp, ""zi_omega"")\n\n        # finally, construct the term for parallel scan reduction\n        hmm_factor = step_dist + angle_dist + omega_dist\n        hmm_factor = hmm_factor * self.raggedness_masks[""individual""]\n        hmm_factor = hmm_factor * self.raggedness_masks[""timestep""]\n        # copy masking behavior of pyro.infer.TraceEnum_ELBO._compute_model_factors\n        hmm_factor = hmm_factor + y_dist\n        log_prob.insert(0, hmm_factor)\n\n        return log_prob\n'"
examples/mixed_hmm/seal_data.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nfrom urllib.request import urlopen\n\nimport pandas as pd\nimport torch\n\nMISSING = 1e-6\n\n\ndef download_seal_data(filename):\n    """"""download the preprocessed seal data and save it to filename""""""\n    url = ""https://d2hg8soec8ck9v.cloudfront.net/datasets/prep_seal_data.csv""\n    with open(filename, ""wb"") as f:\n        f.write(urlopen(url).read())\n\n\ndef prepare_seal(filename, random_effects):\n\n    if not os.path.exists(filename):\n        download_seal_data(filename)\n\n    seal_df = pd.read_csv(filename)\n    obs_keys = [""step"", ""angle"", ""omega""]\n    # data format for z1, z2:\n    # single tensor with shape (individual, group, time, coords)\n    observations = torch.zeros((20, 2, 1800, len(obs_keys))).fill_(float(""-inf""))\n    for g, (group, group_df) in enumerate(seal_df.groupby(""sex"")):\n        for i, (ind, ind_df) in enumerate(group_df.groupby(""ID"")):\n            for o, obs_key in enumerate(obs_keys):\n                observations[i, g, 0:len(ind_df), o] = torch.tensor(ind_df[obs_key].values)\n\n    observations[torch.isnan(observations)] = float(""-inf"")\n\n    # make masks\n    # mask_i should mask out individuals, it applies at all timesteps\n    mask_i = (observations > float(""-inf"")).any(dim=-1).any(dim=-1)  # time nonempty\n\n    # mask_t handles padding for time series of different length\n    mask_t = (observations > float(""-inf"")).all(dim=-1)   # include non-inf\n\n    # temporary hack to avoid zero-inflation issues\n    # observations[observations == 0.] = MISSING\n    observations[(observations == 0.) | (observations == float(""-inf""))] = MISSING\n    assert not torch.isnan(observations).any()\n\n    # observations = observations[..., 5:11, :]  # truncate for testing\n\n    config = {\n        ""MISSING"": MISSING,\n        ""sizes"": {\n            ""state"": 3,\n            ""random"": 4,\n            ""group"": observations.shape[1],\n            ""individual"": observations.shape[0],\n            ""timesteps"": observations.shape[2],\n        },\n        ""group"": {""random"": random_effects[""group""], ""fixed"": None},\n        ""individual"": {""random"": random_effects[""individual""], ""fixed"": None, ""mask"": mask_i},\n        ""timestep"": {""random"": None, ""fixed"": None, ""mask"": mask_t},\n        ""observations"": {\n            ""step"": observations[..., 0],\n            ""angle"": observations[..., 1],\n            ""omega"": observations[..., 2],\n        },\n    }\n\n    return config\n\n\ndef prepare_fake(sizes, random_effects):\n    """"""\n    Generate fake datasets of varying size. Used for evaluating computational performance.\n    """"""\n    obs_keys = [""step"", ""angle"", ""omega""]\n    # data format for z1, z2:\n    # single tensor with shape (individual, group, time, coords)\n    observations = torch.randn((\n        sizes[""individual""], sizes[""group""], sizes[""timesteps""], len(obs_keys))).abs()\n    observations[torch.isnan(observations)] = float(""-inf"")\n    observations[observations >= 1.] = 0.5\n\n    # make masks\n    # mask_i should mask out individuals, it applies at all timesteps\n    mask_i = (observations > float(""-inf"")).any(dim=-1).any(dim=-1)  # time nonempty\n\n    # mask_t handles padding for time series of different length\n    mask_t = (observations > float(""-inf"")).all(dim=-1)   # include non-inf\n\n    # temporary hack to avoid zero-inflation issues\n    # observations[observations == 0.] = MISSING\n    observations[(observations == 0.) | (observations == float(""-inf""))] = MISSING\n    assert not torch.isnan(observations).any()\n\n    # observations = observations[..., 5:11, :]  # truncate for testing\n\n    config = {\n        ""MISSING"": MISSING,\n        ""sizes"": sizes.copy(),\n        ""group"": {""random"": random_effects[""group""], ""fixed"": None},\n        ""individual"": {""random"": random_effects[""individual""], ""fixed"": None, ""mask"": mask_i},\n        ""timestep"": {""random"": None, ""fixed"": None, ""mask"": mask_t},\n        ""observations"": {\n            ""step"": observations[..., 0],\n            ""angle"": observations[..., 1],\n            ""omega"": observations[..., 2],\n        },\n    }\n\n    return config\n'"
funsor/compat/__init__.py,0,b''
funsor/compat/ops.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom torch import ones, randn, tensor, zeros  # noqa F401\n\nfrom funsor.testing import allclose  # noqa F401\n\nfrom .ops import *  # noqa F401\n'"
funsor/einsum/__init__.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import reduce\n\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction\nfrom funsor.interpreter import interpretation\nfrom funsor.optimizer import apply_optimizer\nfrom funsor.sum_product import sum_product\nfrom funsor.terms import Funsor, lazy\n\n# TODO: add numpy einsum here\nBACKEND_OPS = {\n    ""torch"": (ops.add, ops.mul),\n    ""pyro.ops.einsum.torch_log"": (ops.logaddexp, ops.add),\n    ""pyro.ops.einsum.torch_marginal"": (ops.logaddexp, ops.add),\n    ""pyro.ops.einsum.torch_map"": (ops.max, ops.add),\n    ""pyro.ops.einsum.torch_sample"": (ops.logaddexp, ops.add),\n    ""numpy"": (ops.add, ops.mul),\n    ""funsor.einsum.numpy_log"": (ops.logaddexp, ops.add),\n    ""funsor.einsum.numpy_map"": (ops.max, ops.add),\n    ""jax.numpy"": (ops.add, ops.mul),\n}\n\nBACKEND_ADJOINT_OPS = {\n    ""pyro.ops.einsum.torch_marginal"": (ops.logaddexp, ops.add),\n    ""pyro.ops.einsum.torch_map"": (ops.max, ops.add),\n}\n\n\ndef naive_contract_einsum(eqn, *terms, **kwargs):\n    """"""\n    Use for testing Contract against einsum\n    """"""\n    assert ""plates"" not in kwargs\n\n    backend = kwargs.pop(\'backend\', \'torch\')\n    if backend in BACKEND_OPS:\n        sum_op, prod_op = BACKEND_OPS[backend]\n    else:\n        raise ValueError(""{} backend not implemented"".format(backend))\n\n    assert isinstance(eqn, str)\n    assert all(isinstance(term, Funsor) for term in terms)\n    inputs, output = eqn.split(\'->\')\n    inputs = inputs.split(\',\')\n    assert len(inputs) == len(terms)\n    assert len(output.split(\',\')) == 1\n    input_dims = frozenset(d for inp in inputs for d in inp)\n    output_dims = frozenset(d for d in output)\n    reduced_vars = input_dims - output_dims\n    return Contraction(sum_op, prod_op, reduced_vars, *terms)\n\n\ndef naive_einsum(eqn, *terms, **kwargs):\n    """"""\n    Implements standard variable elimination.\n    """"""\n    backend = kwargs.pop(\'backend\', \'torch\')\n    if backend in BACKEND_OPS:\n        sum_op, prod_op = BACKEND_OPS[backend]\n    else:\n        raise ValueError(""{} backend not implemented"".format(backend))\n\n    assert isinstance(eqn, str)\n    assert all(isinstance(term, Funsor) for term in terms)\n    inputs, output = eqn.split(\'->\')\n    assert len(output.split(\',\')) == 1\n    input_dims = frozenset(d for inp in inputs.split(\',\') for d in inp)\n    output_dims = frozenset(output)\n    reduce_dims = input_dims - output_dims\n    return reduce(prod_op, terms).reduce(sum_op, reduce_dims)\n\n\ndef naive_plated_einsum(eqn, *terms, **kwargs):\n    """"""\n    Implements Tensor Variable Elimination (Algorithm 1 in [Obermeyer et al 2019])\n\n    [Obermeyer et al 2019] Obermeyer, F., Bingham, E., Jankowiak, M., Chiu, J.,\n        Pradhan, N., Rush, A., and Goodman, N.  Tensor Variable Elimination for\n        Plated Factor Graphs, 2019\n    """"""\n    plates = kwargs.pop(\'plates\', \'\')\n    if not plates:\n        return naive_einsum(eqn, *terms, **kwargs)\n\n    backend = kwargs.pop(\'backend\', \'torch\')\n    if backend in BACKEND_OPS:\n        sum_op, prod_op = BACKEND_OPS[backend]\n    else:\n        raise ValueError(""{} backend not implemented"".format(backend))\n\n    assert isinstance(eqn, str)\n    assert all(isinstance(term, Funsor) for term in terms)\n    inputs, output = eqn.split(\'->\')\n    inputs = inputs.split(\',\')\n    assert len(inputs) == len(terms)\n    assert len(output.split(\',\')) == 1\n    input_dims = frozenset(d for inp in inputs for d in inp)\n    output_dims = frozenset(d for d in output)\n    plate_dims = frozenset(plates) - output_dims\n    reduce_vars = input_dims - output_dims - frozenset(plates)\n\n    output_plates = output_dims & frozenset(plates)\n    if not all(output_plates.issubset(inp) for inp in inputs):\n        raise NotImplementedError(""TODO"")\n\n    eliminate = plate_dims | reduce_vars\n    return sum_product(sum_op, prod_op, terms, eliminate, frozenset(plates))\n\n\ndef einsum(eqn, *terms, **kwargs):\n    r""""""\n    Top-level interface for optimized tensor variable elimination.\n\n    :param str equation: An einsum equation.\n    :param funsor.terms.Funsor \\*terms: One or more operands.\n    :param set plates: Optional keyword argument denoting which funsor\n        dimensions are plate dimensions. Among all input dimensions (from\n        terms): dimensions in plates but not in outputs are product-reduced;\n        dimensions in neither plates nor outputs are sum-reduced.\n    """"""\n    with interpretation(lazy):\n        naive_ast = naive_plated_einsum(eqn, *terms, **kwargs)\n    return apply_optimizer(naive_ast)\n'"
funsor/einsum/numpy_log.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport funsor.ops as ops\nfrom funsor.einsum.util import Tensordot\nfrom funsor.util import get_backend\n\n\ndef einsum(equation, *operands):\n    """"""\n    Log-sum-exp implementation of einsum.\n    """"""\n    if get_backend() != ""jax"":\n        # NB: rename symbols to support NumPy, which allow only symbols a-z.\n        symbols = sorted(set(equation) - set(\',->\'))\n        rename = dict(zip(symbols, \'abcdefghijklmnopqrstuvwxyz\'))\n        equation = \'\'.join(rename.get(s, s) for s in equation)\n\n    inputs, output = equation.split(\'->\')\n    if inputs == output:\n        return operands[0][...]  # create a new object\n    inputs = inputs.split(\',\')\n\n    shifts = []\n    exp_operands = []\n    for dims, operand in zip(inputs, operands):\n        shift = operand\n        for i, dim in enumerate(dims):\n            if dim not in output:\n                shift = ops.amax(shift, i, keepdims=True)\n        # avoid nan due to -inf - -inf\n        shift = ops.clamp(shift, ops.finfo(shift).min, None)\n        exp_operands.append(ops.exp(operand - shift))\n\n        # permute shift to match output\n        shift = shift.reshape([size for size, dim in zip(operand.shape, dims) if dim in output])\n        if len(shift.shape) > 0:\n            shift = shift.reshape((1,) * (len(output) - shift.ndim) + shift.shape)\n            dims = [dim for dim in dims if dim in output]\n            dims = [dim for dim in output if dim not in dims] + dims\n            shift = ops.permute(shift, [dims.index(dim) for dim in output])\n        shifts.append(shift)\n\n    result = ops.log(ops.einsum(equation, *exp_operands))\n    return sum(shifts + [result])\n\n\ntensordot = Tensordot(einsum)\ntranspose = ops.permute\n\n__all__ = [""einsum"", ""tensordot"", ""transpose""]\n'"
funsor/einsum/numpy_map.py,0,"b'# Copyright (c) 2017-2019 Uber Technologies, Inc.\n# SPDX-License-Identifier: Apache-2.0\n\nimport operator\n\nfrom functools import reduce\n\nimport funsor.ops as ops\nfrom funsor.einsum.util import Tensordot, broadcast_all\n\n\ndef einsum(equation, *operands):\n    """"""\n    Forward-max-sum backward-argmax implementation of einsum.\n    This assumes all operands have a ``._pyro_dims`` attribute set.\n    """"""\n    inputs, output = equation.split(\'->\')\n    inputs = inputs.split(\',\')\n\n    contract_dims = \'\'.join(sorted(set().union(*inputs) - set(output)))\n    dims = output + contract_dims\n    result = reduce(operator.add, broadcast_all(*operands, inputs=inputs, dims=dims))\n    if contract_dims:\n        output_shape = result.shape[:len(output)]\n        result = ops.amax(result.reshape(output_shape + (-1,)), -1)\n    elif result is operands[0]:\n        result = result[...]  # create a new object\n    assert len(result.shape) == len(output)\n\n    return result\n\n\ntensordot = Tensordot(einsum)\ntranspose = ops.permute\n\n__all__ = [""transpose"", ""einsum"", ""tensordot""]\n'"
funsor/einsum/util.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom funsor import ops\n\nEINSUM_SYMBOLS_BASE = \'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\'\n\n\nclass Tensordot:\n    """"""\n    Creates a tensordot implementation from an einsum implementation.\n    """"""\n    def __init__(self, einsum):\n        self.einsum = einsum\n\n    # Copyright (c) 2014 Daniel Smith\n    # SPDX-License-Identifier: MIT\n    # This function is copied and adapted from:\n    # https://github.com/dgasmith/opt_einsum/blob/a6dd686/opt_einsum/backends/torch.py\n    def __call__(self, x, y, axes=2):\n        xnd = len(x.shape)\n        ynd = len(y.shape)\n\n        # convert int argument to (list[int], list[int])\n        if isinstance(axes, int):\n            axes = range(xnd - axes, xnd), range(axes)\n\n        # convert (int, int) to (list[int], list[int])\n        if isinstance(axes[0], int):\n            axes = (axes[0],), axes[1]\n        if isinstance(axes[1], int):\n            axes = axes[0], (axes[1],)\n\n        # initialize empty indices\n        x_ix = [None] * xnd\n        y_ix = [None] * ynd\n        out_ix = []\n\n        # fill in repeated indices\n        available_ix = iter(EINSUM_SYMBOLS_BASE)\n        for ax1, ax2 in zip(*axes):\n            repeat = next(available_ix)\n            x_ix[ax1] = repeat\n            y_ix[ax2] = repeat\n\n        # fill in the rest, and maintain output order\n        for i in range(xnd):\n            if x_ix[i] is None:\n                leave = next(available_ix)\n                x_ix[i] = leave\n                out_ix.append(leave)\n        for i in range(ynd):\n            if y_ix[i] is None:\n                leave = next(available_ix)\n                y_ix[i] = leave\n                out_ix.append(leave)\n\n        # form full string and contract!\n        einsum_str = ""{},{}->{}"".format(*map("""".join, (x_ix, y_ix, out_ix)))\n        return self.einsum(einsum_str, x, y)\n\n\ndef broadcast_all(*values, **kwargs):\n    """"""\n    Packed broadcasting of multiple tensors.\n    """"""\n    inputs = kwargs.get(\'inputs\')\n    dims = kwargs.get(\'dims\')\n    sizes = {dim: size for value, old_dims in zip(values, inputs)\n             for dim, size in zip(old_dims, value.shape)}\n    if dims is None:\n        dims = \'\'.join(sorted(sizes))\n    else:\n        assert set(dims) == set(sizes)\n    shape = tuple(sizes[dim] for dim in dims)\n    values = list(values)\n    for i, (x, old_dims) in enumerate(zip(values, inputs)):\n        if old_dims != dims:\n            x = ops.permute(x, tuple(old_dims.index(dim) for dim in dims if dim in old_dims))\n            x = x.reshape(tuple(sizes[dim] if dim in old_dims else 1 for dim in dims))\n            x = ops.expand(x, shape)\n            assert len(x.shape) == len(dims)\n            values[i] = x\n    return tuple(values)\n'"
funsor/jax/__init__.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom jax.core import Tracer\nfrom jax.interpreters.xla import DeviceArray\n\nimport funsor.jax.distributions  # noqa: F401\nimport funsor.jax.ops  # noqa: F401\nimport funsor.ops as ops\nfrom funsor.adjoint import adjoint_ops\nfrom funsor.interpreter import children, recursion_reinterpret\nfrom funsor.terms import Funsor, to_funsor\nfrom funsor.tensor import Tensor, tensor_to_funsor\nfrom funsor.util import quote\n\n\n@adjoint_ops.register(Tensor, ops.AssociativeOp, ops.AssociativeOp, Funsor, (DeviceArray, Tracer), tuple, object)\ndef adjoint_tensor(adj_redop, adj_binop, out_adj, data, inputs, dtype):\n    return {}\n\n\n@recursion_reinterpret.register(DeviceArray)\n@recursion_reinterpret.register(Tracer)\ndef _recursion_reinterpret_ground(x):\n    return x\n\n\n@children.register(DeviceArray)\n@children.register(Tracer)\ndef _children_ground(x):\n    return ()\n\n\nto_funsor.register(DeviceArray)(tensor_to_funsor)\nto_funsor.register(Tracer)(tensor_to_funsor)\n\n\n@quote.register(DeviceArray)\ndef _quote(x, indent, out):\n    """"""\n    Work around JAX\'s DeviceArray not supporting reproducible repr.\n    """"""\n    out.append((indent, f""np.array({repr(x.copy().tolist())}, dtype=np.{x.dtype})""))\n'"
funsor/jax/distributions.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\n\nimport numpyro.distributions as dist\n\nfrom funsor.distribution import (  # noqa: F401\n    Bernoulli,\n    FUNSOR_DIST_NAMES,\n    LogNormal,\n    backenddist_to_funsor,\n    eager_beta,\n    eager_binomial,\n    eager_categorical_funsor,\n    eager_categorical_tensor,\n    eager_delta_funsor_funsor,\n    eager_delta_funsor_variable,\n    eager_delta_tensor,\n    eager_delta_variable_variable,\n    eager_multinomial,\n    eager_mvn,\n    eager_normal,\n    indepdist_to_funsor,\n    make_dist,\n    maskeddist_to_funsor,\n    mvndist_to_funsor,\n    transformeddist_to_funsor,\n)\nfrom funsor.domains import reals\nfrom funsor.tensor import Tensor, dummy_numeric_array\nfrom funsor.terms import Funsor, Variable, eager, to_funsor\n\n\n################################################################################\n# Distribution Wrappers\n################################################################################\n\n\nclass _NumPyroWrapper_Binomial(dist.BinomialProbs):\n    pass\n\n\nclass _NumPyroWrapper_Categorical(dist.CategoricalProbs):\n    pass\n\n\nclass _NumPyroWrapper_Multinomial(dist.MultinomialProbs):\n    pass\n\n\nclass _NumPyroWrapper_NonreparameterizedBeta(dist.Beta):\n    has_rsample = False\n\n\nclass _NumPyroWrapper_NonreparameterizedDirichlet(dist.Dirichlet):\n    has_rsample = False\n\n\nclass _NumPyroWrapper_NonreparameterizedGamma(dist.Gamma):\n    has_rsample = False\n\n\nclass _NumPyroWrapper_NonreparameterizedNormal(dist.Normal):\n    has_rsample = False\n\n\ndef _get_numpyro_dist(dist_name):\n    if dist_name in ['Binomial', 'Categorical', 'Multinomial'] or dist_name.startswith('Nonreparameterized'):\n        return globals().get('_NumPyroWrapper_' + dist_name)\n    else:\n        return getattr(dist, dist_name, None)\n\n\nNUMPYRO_DIST_NAMES = FUNSOR_DIST_NAMES\n\n\nfor dist_name, param_names in NUMPYRO_DIST_NAMES:\n    numpyro_dist = _get_numpyro_dist(dist_name)\n    if numpyro_dist is not None:\n        # resolve numpyro distributions do not have `has_rsample` attributes\n        has_rsample = getattr(numpyro_dist, 'has_rsample', not numpyro_dist.is_discrete)\n        if has_rsample:\n            numpyro_dist.has_rsample = True\n            numpyro_dist.rsample = numpyro_dist.sample\n        locals()[dist_name] = make_dist(numpyro_dist, param_names)\n\n# Delta has to be treated specially because of its weird shape inference semantics\nDelta._infer_value_domain = classmethod(lambda cls, **kwargs: kwargs['v'])  # noqa: F821\n\n\n# Multinomial and related dists have dependent bint dtypes, so we just make them 'real'\n# See issue: https://github.com/pyro-ppl/funsor/issues/322\n@functools.lru_cache(maxsize=5000)\ndef _multinomial_infer_value_domain(cls, **kwargs):\n    instance = cls.dist_class(**{k: dummy_numeric_array(domain) for k, domain in kwargs.items()}, validate_args=False)\n    return reals(*instance.event_shape)\n\n\nBinomial._infer_value_domain = classmethod(_multinomial_infer_value_domain)  # noqa: F821\nMultinomial._infer_value_domain = classmethod(_multinomial_infer_value_domain)  # noqa: F821\n\n\n###############################################\n# Converting PyTorch Distributions to funsors\n###############################################\n\nto_funsor.register(dist.Distribution)(backenddist_to_funsor)\nto_funsor.register(dist.Independent)(indepdist_to_funsor)\nto_funsor.register(dist.MaskedDistribution)(maskeddist_to_funsor)\nto_funsor.register(dist.TransformedDistribution)(transformeddist_to_funsor)\nto_funsor.register(dist.MultivariateNormal)(mvndist_to_funsor)\n\n\n@to_funsor.register(dist.BinomialProbs)\n@to_funsor.register(dist.BinomialLogits)\ndef categorical_to_funsor(numpyro_dist, output=None, dim_to_name=None):\n    new_pyro_dist = _NumPyroWrapper_Binomial(probs=numpyro_dist.probs)\n    return backenddist_to_funsor(new_pyro_dist, output, dim_to_name)\n\n\n@to_funsor.register(dist.CategoricalProbs)\n# XXX: in Pyro backend, we always convert pyro.distributions.Categorical\n# to funsor.torch.distributions.Categorical\n@to_funsor.register(dist.CategoricalLogits)\ndef categorical_to_funsor(numpyro_dist, output=None, dim_to_name=None):\n    new_pyro_dist = _NumPyroWrapper_Categorical(probs=numpyro_dist.probs)\n    return backenddist_to_funsor(new_pyro_dist, output, dim_to_name)\n\n\n@to_funsor.register(dist.MultinomialProbs)\n@to_funsor.register(dist.MultinomialLogits)\ndef categorical_to_funsor(numpyro_dist, output=None, dim_to_name=None):\n    new_pyro_dist = _NumPyroWrapper_Multinomial(probs=numpyro_dist.probs)\n    return backenddist_to_funsor(new_pyro_dist, output, dim_to_name)\n\n\neager.register(Beta, Funsor, Funsor, Funsor)(eager_beta)  # noqa: F821)\neager.register(Binomial, Funsor, Funsor, Funsor)(eager_binomial)  # noqa: F821\neager.register(Multinomial, Tensor, Tensor, Tensor)(eager_multinomial)  # noqa: F821)\neager.register(Categorical, Funsor, Tensor)(eager_categorical_funsor)  # noqa: F821)\neager.register(Categorical, Tensor, Variable)(eager_categorical_tensor)  # noqa: F821)\neager.register(Delta, Tensor, Tensor, Tensor)(eager_delta_tensor)  # noqa: F821\neager.register(Delta, Funsor, Funsor, Variable)(eager_delta_funsor_variable)  # noqa: F821\neager.register(Delta, Variable, Funsor, Variable)(eager_delta_funsor_variable)  # noqa: F821\neager.register(Delta, Variable, Funsor, Funsor)(eager_delta_funsor_funsor)  # noqa: F821\neager.register(Delta, Variable, Variable, Variable)(eager_delta_variable_variable)  # noqa: F821\neager.register(Normal, Funsor, Tensor, Funsor)(eager_normal)  # noqa: F821\neager.register(MultivariateNormal, Funsor, Tensor, Funsor)(eager_mvn)  # noqa: F821\n"""
funsor/jax/ops.py,54,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport jax.numpy as np\nimport numpy as onp\nfrom jax import lax\nfrom jax.core import Tracer\nfrom jax.interpreters.xla import DeviceArray\nfrom jax.scipy.linalg import cho_solve, solve_triangular\nfrom jax.scipy.special import expit, logsumexp\n\nimport funsor.ops as ops\n\n\n################################################################################\n# Register Ops\n################################################################################\n\narray = (onp.generic, onp.ndarray, DeviceArray, Tracer)\nops.clamp.register(array, object, object)(np.clip)\nops.exp.register(array)(np.exp)\nops.full_like.register(array, object)(np.full_like)\nops.log1p.register(array)(np.log1p)\nops.max.register(array)(np.maximum)\nops.min.register(array)(np.minimum)\nops.permute.register(array, (tuple, list))(np.transpose)\nops.sigmoid.register(array)(expit)\nops.sqrt.register(array)(np.sqrt)\nops.transpose.register(array, int, int)(np.swapaxes)\nops.unsqueeze.register(array, int)(np.expand_dims)\n\n\n@ops.all.register(array, (int, type(None)))\ndef _all(x, dim):\n    return np.all(x, axis=dim)\n\n\n@ops.amax.register(array, (int, type(None)))\ndef _amax(x, dim, keepdims=False):\n    return np.amax(x, axis=dim, keepdims=keepdims)\n\n\n@ops.amin.register(array, (int, type(None)))\ndef _amin(x, dim, keepdims=False):\n    return np.amin(x, axis=dim, keepdims=keepdims)\n\n\n@ops.any.register(array, (int, type(None)))\ndef _any(x, dim):\n    return np.any(x, axis=dim)\n\n\n@ops.astype.register(array, str)\ndef _astype(x, dtype):\n    return x.astype(dtype)\n\n\n@ops.cat.register(int, [array])\ndef _cat(dim, *x):\n    if len(x) == 1:\n        return x[0]\n    return np.concatenate(x, axis=dim)\n\n\n@ops.cholesky.register(array)\ndef _cholesky(x):\n    """"""\n    Like :func:`numpy.linalg.cholesky` but uses sqrt for scalar matrices.\n    """"""\n    if x.shape[-1] == 1:\n        return np.sqrt(x)\n    return np.linalg.cholesky(x)\n\n\n@ops.cholesky_inverse.register(array)\ndef _cholesky_inverse(x):\n    """"""\n    Like :func:`torch.cholesky_inverse` but supports batching and gradients.\n    """"""\n    return _cholesky_solve(_new_eye(x, x.shape[:-1]), x)\n\n\n@ops.cholesky_solve.register(array, array)\ndef _cholesky_solve(x, y):\n    return cho_solve((y, True), x)\n\n\n@ops.detach.register(array)\ndef _detach(x):\n    return lax.stop_gradient(x)\n\n\n@ops.diagonal.register(array, int, int)\ndef _diagonal(x, dim1, dim2):\n    return np.diagonal(x, axis1=dim1, axis2=dim2)\n\n\n@ops.einsum.register(str, [array])\ndef _einsum(equation, *operands):\n    return np.einsum(equation, *operands)\n\n\n@ops.expand.register(array, tuple)\ndef _expand(x, shape):\n    prepend_dim = len(shape) - np.ndim(x)\n    assert prepend_dim >= 0\n    shape = shape[:prepend_dim] + tuple(dx if size == -1 else size\n                                        for dx, size in zip(np.shape(x), shape[prepend_dim:]))\n    return np.broadcast_to(x, shape)\n\n\n@ops.finfo.register(array)\ndef _finfo(x):\n    return np.finfo(x.dtype)\n\n\n@ops.is_numeric_array.register(array)\ndef _is_numeric_array(x):\n    return True\n\n\n@ops.log.register(array)\ndef _log(x):\n    return np.log(x)\n\n\n@ops.logsumexp.register(array, (int, type(None)))\ndef _logsumexp(x, dim):\n    return logsumexp(x, axis=dim)\n\n\n@ops.max.register(array, array)\ndef _max(x, y):\n    return np.maximum(x, y)\n\n\n@ops.max.register((int, float), array)\ndef _max(x, y):\n    return np.clip(y, a_min=x, a_max=None)\n\n\n@ops.max.register(array, (int, float))\ndef _max(x, y):\n    return np.clip(x, a_min=y, a_max=None)\n\n\n@ops.min.register(array, array)\ndef _min(x, y):\n    return np.minimum(x, y)\n\n\n# TODO: replace (int, float) by object\n@ops.min.register((int, float), array)\ndef _min(x, y):\n    return np.clip(y, a_min=None, a_max=x)\n\n\n@ops.min.register(array, (int, float))\ndef _min(x, y):\n    return np.clip(x, a_min=None, a_max=y)\n\n\n@ops.new_arange.register(array, int, int, int)\ndef _new_arange(x, start, stop, step):\n    return np.arange(start, stop, step)\n\n\n@ops.new_arange.register(array, int)\ndef _new_arange(x, stop):\n    return np.arange(stop)\n\n\n@ops.new_eye.register(array, tuple)\ndef _new_eye(x, shape):\n    n = shape[-1]\n    return np.broadcast_to(np.eye(n), shape + (n,))\n\n\n@ops.new_zeros.register(array, tuple)\ndef _new_zeros(x, shape):\n    return onp.zeros(shape, dtype=x.dtype)\n\n\n@ops.prod.register(array, (int, type(None)))\ndef _prod(x, dim):\n    return np.prod(x, axis=dim)\n\n\n@ops.reciprocal.register(array)\ndef _reciprocal(x):\n    result = np.clip(np.reciprocal(x), a_max=np.finfo(x.dtype).max)\n    return result\n\n\n@ops.safediv.register((int, float), array)\ndef _safediv(x, y):\n    try:\n        finfo = np.finfo(y.dtype)\n    except ValueError:\n        finfo = np.iinfo(y.dtype)\n    return x * np.clip(np.reciprocal(y), a_min=None, a_max=finfo.max)\n\n\n@ops.safesub.register((int, float), array)\ndef _safesub(x, y):\n    try:\n        finfo = np.finfo(y.dtype)\n    except ValueError:\n        finfo = np.iinfo(y.dtype)\n    return x + np.clip(-y, a_min=None, a_max=finfo.max)\n\n\n@ops.stack.register(int, [array])\ndef _stack(dim, *x):\n    return np.stack(x, axis=dim)\n\n\n@ops.sum.register(array, (int, type(None)))\ndef _sum(x, dim):\n    return np.sum(x, axis=dim)\n\n\n@ops.triangular_solve.register(array, array)\ndef _triangular_solve(x, y, upper=False, transpose=False):\n    assert np.ndim(x) >= 2 and np.ndim(y) >= 2\n    n, m = x.shape[-2:]\n    assert y.shape[-2:] == (n, n)\n    # NB: JAX requires x and y have the same batch_shape\n    batch_shape = lax.broadcast_shapes(x.shape[:-2], y.shape[:-2])\n    x = np.broadcast_to(x, batch_shape + (n, m))\n    if y.shape[:-2] == batch_shape:\n        return solve_triangular(y, x, trans=int(transpose), lower=not upper)\n\n    # The following procedure handles the case: y.shape = (i, 1, n, n), x.shape = (..., i, j, n, m)\n    # because we don\'t want to broadcast y to the shape (i, j, n, n).\n    # We are going to make x have shape (..., 1, j,  i, 1, n) to apply batched triangular_solve\n    dx = x.ndim\n    prepend_ndim = dx - y.ndim  # ndim of ... part\n    # Reshape x with the shape (..., 1, i, j, 1, n, m)\n    x_new_shape = batch_shape[:prepend_ndim]\n    for (sy, sx) in zip(y.shape[:-2], batch_shape[prepend_ndim:]):\n        x_new_shape += (sx // sy, sy)\n    x_new_shape += (n, m,)\n    x = np.reshape(x, x_new_shape)\n    # Permute y to make it have shape (..., 1, j, m, i, 1, n)\n    batch_ndim = x.ndim - 2\n    permute_dims = (tuple(range(prepend_ndim))\n                    + tuple(range(prepend_ndim, batch_ndim, 2))\n                    + (batch_ndim + 1,)\n                    + tuple(range(prepend_ndim + 1, batch_ndim, 2))\n                    + (batch_ndim,))\n    x = np.transpose(x, permute_dims)\n    x_permute_shape = x.shape\n\n    # reshape to (-1, i, 1, n)\n    x = np.reshape(x, (-1,) + y.shape[:-1])\n    # permute to (i, 1, n, -1)\n    x = np.moveaxis(x, 0, -1)\n\n    sol = solve_triangular(y, x, trans=int(transpose), lower=not upper)  # shape: (i, 1, n, -1)\n    sol = np.moveaxis(sol, -1, 0)  # shape: (-1, i, 1, n)\n    sol = np.reshape(sol, x_permute_shape)  # shape: (..., 1, j, m, i, 1, n)\n\n    # now we permute back to x_new_shape = (..., 1, i, j, 1, n, m)\n    permute_inv_dims = tuple(range(prepend_ndim))\n    for i in range(y.ndim - 2):\n        permute_inv_dims += (prepend_ndim + i, dx + i - 1)\n    permute_inv_dims += (sol.ndim - 1, prepend_ndim + y.ndim - 2)\n    sol = np.transpose(sol, permute_inv_dims)\n    return sol.reshape(batch_shape + (n, m))\n'"
funsor/pyro/__init__.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom funsor.pyro.distribution import FunsorDistribution\nfrom funsor.pyro.hmm import DiscreteHMM, GaussianHMM, GaussianMRF, SwitchingLinearHMM\n\n__all__ = [\n    ""DiscreteHMM"",\n    ""FunsorDistribution"",\n    ""GaussianHMM"",\n    ""GaussianMRF"",\n    ""SwitchingLinearHMM"",\n]\n'"
funsor/pyro/convert.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis module follows a convention for converting between funsors and PyTorch\ndistribution objects. This convention is compatible with NumPy/PyTorch-style\nbroadcasting. Following PyTorch distributions (and Tensorflow distributions),\nwe consider ""event shapes"" to be on the right and broadcast-compatible ""batch\nshapes"" to be on the left.\n\nThis module also aims to be forgiving in inputs and pedantic in outputs:\nmethods accept either the superclass :class:`torch.distributions.Distribution`\nobjects or the subclass :class:`pyro.distributions.TorchDistribution` objects.\nMethods return only the narrower subclass\n:class:`pyro.distributions.TorchDistribution` objects.\n""""""\n\nimport math\nfrom collections import OrderedDict\n\nimport torch\nfrom pyro.distributions.util import broadcast_shape\n\nfrom funsor.cnf import Contraction\nfrom funsor.delta import Delta\nfrom funsor.domains import Domain, bint, reals\nfrom funsor.gaussian import Gaussian\nfrom funsor.interpreter import gensym\nfrom funsor.tensor import Tensor, align_tensors\nfrom funsor.terms import Funsor, Independent, Variable, eager, to_data, to_funsor\nfrom funsor.torch.distributions import Normal\n\n# Conversion functions use fixed names for Pyro batch dims, but\n# accept an event_inputs tuple for custom event dim names.\nDIM_TO_NAME = tuple(map(""_pyro_dim_{}"".format, range(-100, 0)))\nNAME_TO_DIM = dict(zip(DIM_TO_NAME, range(-100, 0)))\n\n\ndef default_dim_to_name(inputs_shape, event_inputs):\n    dim_to_name_list = DIM_TO_NAME + event_inputs if event_inputs else DIM_TO_NAME\n    return OrderedDict(zip(\n        range(-len(inputs_shape), 0),\n        dim_to_name_list[len(dim_to_name_list) - len(inputs_shape):]))\n\n\ndef default_name_to_dim(event_inputs):\n    if not event_inputs:\n        return NAME_TO_DIM\n    dim_to_name = DIM_TO_NAME + event_inputs\n    return dict(zip(dim_to_name, range(-len(dim_to_name), 0)))\n\n\ndef tensor_to_funsor(tensor, event_inputs=(), event_output=0, dtype=""real""):\n    """"""\n    Convert a :class:`torch.Tensor` to a :class:`funsor.tensor.Tensor` .\n\n    Note this should not touch data, but may trigger a\n    :meth:`torch.Tensor.reshape` op.\n\n    :param torch.Tensor tensor: A PyTorch tensor.\n    :param tuple event_inputs: A tuple of names for rightmost tensor\n        dimensions.  If ``tensor`` has these names, they will be converted to\n        ``result.inputs``.\n    :param int event_output: The number of tensor dimensions assigned to\n        ``result.output``. These must be on the right of any ``event_input``\n        dimensions.\n    :return: A funsor.\n    :rtype: funsor.tensor.Tensor\n    """"""\n    assert isinstance(tensor, torch.Tensor)\n    assert isinstance(event_inputs, tuple)\n    assert isinstance(event_output, int) and event_output >= 0\n    inputs_shape = tensor.shape[:tensor.dim() - event_output]\n    output = Domain(dtype=dtype, shape=tensor.shape[tensor.dim() - event_output:])\n    dim_to_name = default_dim_to_name(inputs_shape, event_inputs)\n    return to_funsor(tensor, output, dim_to_name)\n\n\ndef funsor_to_tensor(funsor_, ndims, event_inputs=()):\n    """"""\n    Convert a :class:`funsor.tensor.Tensor` to a :class:`torch.Tensor` .\n\n    Note this should not touch data, but may trigger a\n    :meth:`torch.Tensor.reshape` op.\n\n    :param funsor.tensor.Tensor funsor_: A funsor.\n    :param int ndims: The number of result dims, ``== result.dim()``.\n    :param tuple event_inputs: Names assigned to rightmost dimensions.\n    :return: A PyTorch tensor.\n    :rtype: torch.Tensor\n    """"""\n    assert isinstance(funsor_, Tensor)\n    assert all(k.startswith(""_pyro_dim_"") or k in event_inputs for k in funsor_.inputs)\n\n    tensor = to_data(funsor_, default_name_to_dim(event_inputs))\n\n    if ndims != tensor.dim():\n        tensor = tensor.reshape((1,) * (ndims - tensor.dim()) + tensor.shape)\n    assert tensor.dim() == ndims\n    return tensor\n\n\ndef dist_to_funsor(pyro_dist, event_inputs=()):\n    """"""\n    Convert a PyTorch distribution to a Funsor.\n\n    :param torch.distribution.Distribution: A PyTorch distribution.\n    :return: A funsor.\n    :rtype: funsor.terms.Funsor\n    """"""\n    assert isinstance(pyro_dist, torch.distributions.Distribution)\n    assert isinstance(event_inputs, tuple)\n    return to_funsor(pyro_dist, reals(), default_dim_to_name(pyro_dist.batch_shape, event_inputs))\n\n\ndef mvn_to_funsor(pyro_dist, event_inputs=(), real_inputs=OrderedDict()):\n    """"""\n    Convert a joint :class:`torch.distributions.MultivariateNormal`\n    distribution into a :class:`~funsor.terms.Funsor` with multiple real\n    inputs.\n\n    This should satisfy::\n\n        sum(d.num_elements for d in real_inputs.values())\n          == pyro_dist.event_shape[0]\n\n    :param torch.distributions.MultivariateNormal pyro_dist: A\n        multivariate normal distribution over one or more variables\n        of real or vector or tensor type.\n    :param tuple event_inputs: A tuple of names for rightmost dimensions.\n        These will be assigned to ``result.inputs`` of type ``bint``.\n    :param OrderedDict real_inputs: A dict mapping real variable name\n        to appropriately sized ``reals()``. The sum of all ``.numel()``\n        of all real inputs should be equal to the ``pyro_dist`` dimension.\n    :return: A funsor with given ``real_inputs`` and possibly additional\n        bint inputs.\n    :rtype: funsor.terms.Funsor\n    """"""\n    assert isinstance(pyro_dist, torch.distributions.MultivariateNormal)\n    assert isinstance(event_inputs, tuple)\n    assert isinstance(real_inputs, OrderedDict)\n    dim_to_name = default_dim_to_name(pyro_dist.batch_shape, event_inputs)\n    return to_funsor(pyro_dist, reals(), dim_to_name, real_inputs=real_inputs)\n\n\ndef funsor_to_mvn(gaussian, ndims, event_inputs=()):\n    """"""\n    Convert a :class:`~funsor.terms.Funsor` to a\n    :class:`pyro.distributions.MultivariateNormal` , dropping the normalization\n    constant.\n\n    :param gaussian: A Gaussian funsor.\n    :type gaussian: funsor.gaussian.Gaussian or funsor.joint.Joint\n    :param int ndims: The number of batch dimensions in the result.\n    :param tuple event_inputs: A tuple of names to assign to rightmost\n        dimensions.\n    :return: a multivariate normal distribution.\n    :rtype: pyro.distributions.MultivariateNormal\n    """"""\n    assert sum(1 for d in gaussian.inputs.values() if d.dtype == ""real"") == 1\n    if isinstance(gaussian, Contraction):\n        gaussian = [v for v in gaussian.terms if isinstance(v, Gaussian)][0]\n    assert isinstance(gaussian, Gaussian)\n    result = to_data(gaussian, name_to_dim=default_name_to_dim(event_inputs))\n    if ndims != len(result.batch_shape):\n        result = result.expand((1,) * (ndims - len(result.batch_shape)) + result.batch_shape)\n    return result\n\n\ndef funsor_to_cat_and_mvn(funsor_, ndims, event_inputs):\n    """"""\n    Converts a labeled gaussian mixture model to a pair of distributions.\n\n    :param funsor.joint.Joint funsor_: A Gaussian mixture funsor.\n    :param int ndims: The number of batch dimensions in the result.\n    :return: A pair ``(cat, mvn)``, where ``cat`` is a\n        :class:`~pyro.distributions.Categorical` distribution over mixture\n        components and ``mvn`` is a\n        :class:`~pyro.distributions.MultivariateNormal` with rightmost batch\n        dimension ranging over mixture components.\n    """"""\n    assert isinstance(funsor_, Contraction), funsor_\n    assert sum(1 for d in funsor_.inputs.values() if d.dtype == ""real"") == 1\n    assert event_inputs, ""no components name found""\n    assert not any(isinstance(v, Delta) for v in funsor_.terms)\n    cat, mvn = to_data(funsor_, name_to_dim=default_name_to_dim(event_inputs))\n    if ndims != len(cat.batch_shape):\n        cat = cat.expand((1,) * (ndims - len(cat.batch_shape)) + cat.batch_shape)\n    if ndims + 1 != len(mvn.batch_shape):\n        mvn = mvn.expand((1,) * (ndims + 1 - len(mvn.batch_shape)) + mvn.batch_shape)\n    return cat, mvn\n\n\nclass AffineNormal(Funsor):\n    """"""\n    Represents a conditional diagonal normal distribution over a random\n    variable ``Y`` whose mean is an affine function of a random variable ``X``.\n    The likelihood of ``X`` is thus::\n\n        AffineNormal(matrix, loc, scale).condition(y).log_density(x)\n\n    which is equivalent to::\n\n        Normal(x @ matrix + loc, scale).to_event(1).log_prob(y)\n\n    :param ~funsor.terms.Funsor matrix: A transformation from ``X`` to ``Y``.\n        Should have rightmost shape ``(x_dim, y_dim)``.\n    :param ~funsor.terms.Funsor loc: A constant offset for ``Y``\'s mean.\n        Should have output shape ``(y_dim,)``.\n    :param ~funsor.terms.Funsor scale: Standard deviation for ``Y``.\n        Should have output shape ``(y_dim,)``.\n    :param ~funsor.terms.Funsor value_x: A value ``X``.\n    :param ~funsor.terms.Funsor value_y: A value ``Y``.\n    """"""\n    def __init__(self, matrix, loc, scale, value_x, value_y):\n        assert len(matrix.output.shape) == 2\n        assert value_x.output == reals(matrix.output.shape[0])\n        assert value_y.output == reals(matrix.output.shape[1])\n        inputs = OrderedDict()\n        for f in (matrix, loc, scale, value_x, value_y):\n            inputs.update(f.inputs)\n        output = reals()\n        super().__init__(inputs, output)\n        self.matrix = matrix\n        self.loc = loc\n        self.scale = scale\n        self.value_x = value_x\n        self.value_y = value_y\n\n\n@eager.register(AffineNormal, Tensor, Tensor, Tensor, Tensor, (Funsor, Tensor))\ndef eager_affine_normal(matrix, loc, scale, value_x, value_y):\n    assert len(matrix.output.shape) == 2\n    assert value_x.output == reals(matrix.output.shape[0])\n    assert value_y.output == reals(matrix.output.shape[1])\n    loc += value_x @ matrix\n    int_inputs, (loc, scale) = align_tensors(loc, scale, expand=True)\n\n    i_name = gensym(""i"")\n    y_name = gensym(""y"")\n    y_i_name = gensym(""y_i"")\n    int_inputs[i_name] = bint(value_y.output.shape[0])\n    loc = Tensor(loc, int_inputs)\n    scale = Tensor(scale, int_inputs)\n    y_dist = Independent(Normal(loc, scale, y_i_name), y_name, i_name, y_i_name)\n    return y_dist(**{y_name: value_y})\n\n\n@eager.register(AffineNormal, Tensor, Tensor, Tensor, Funsor, Tensor)\ndef eager_affine_normal(matrix, loc, scale, value_x, value_y):\n    assert len(matrix.output.shape) == 2\n    assert value_x.output == reals(matrix.output.shape[0])\n    assert value_y.output == reals(matrix.output.shape[1])\n    tensors = (matrix, loc, scale, value_y)\n    int_inputs, tensors = align_tensors(*tensors)\n    matrix, loc, scale, value_y = tensors\n\n    assert value_y.size(-1) == loc.size(-1)\n    prec_sqrt = matrix / scale.unsqueeze(-2)\n    precision = prec_sqrt.matmul(prec_sqrt.transpose(-1, -2))\n    delta = (value_y - loc) / scale\n    info_vec = prec_sqrt.matmul(delta.unsqueeze(-1)).squeeze(-1)\n    log_normalizer = (-0.5 * loc.size(-1) * math.log(2 * math.pi)\n                      - 0.5 * delta.pow(2).sum(-1) - scale.log().sum(-1))\n    precision = precision.expand(info_vec.shape + (-1,))\n    log_normalizer = log_normalizer.expand(info_vec.shape[:-1])\n    inputs = int_inputs.copy()\n    x_name = gensym(""x"")\n    inputs[x_name] = value_x.output\n    x_dist = Tensor(log_normalizer, int_inputs) + Gaussian(info_vec, precision, inputs)\n    return x_dist(**{x_name: value_x})\n\n\ndef matrix_and_mvn_to_funsor(matrix, mvn, event_dims=(), x_name=""value_x"", y_name=""value_y""):\n    """"""\n    Convert a noisy affine function to a Gaussian. The noisy affine function is\n    defined as::\n\n        y = x @ matrix + mvn.sample()\n\n    The result is a non-normalized Gaussian funsor with two real inputs,\n    ``x_name`` and ``y_name``, corresponding to a conditional distribution of\n    real vector ``y` given real vector ``x``.\n\n    :param torch.Tensor matrix: A matrix with rightmost shape ``(x_size, y_size)``.\n    :param mvn: A multivariate normal distribution with\n        ``event_shape == (y_size,)``.\n    :type mvn: torch.distributions.MultivariateNormal or\n        torch.distributions.Independent of torch.distributions.Normal\n    :param tuple event_dims: A tuple of names for rightmost dimensions.\n        These will be assigned to ``result.inputs`` of type ``bint``.\n    :param str x_name: The name of the ``x`` random variable.\n    :param str y_name: The name of the ``y`` random variable.\n    :return: A funsor with given ``real_inputs`` and possibly additional\n        bint inputs.\n    :rtype: funsor.terms.Funsor\n    """"""\n    assert (isinstance(mvn, torch.distributions.MultivariateNormal) or\n            (isinstance(mvn, torch.distributions.Independent) and\n             isinstance(mvn.base_dist, torch.distributions.Normal)))\n    assert isinstance(matrix, torch.Tensor)\n    x_size, y_size = matrix.shape[-2:]\n    assert mvn.event_shape == (y_size,)\n\n    # Handle diagonal normal distributions as an efficient special case.\n    if isinstance(mvn, torch.distributions.Independent):\n        return AffineNormal(tensor_to_funsor(matrix, event_dims, 2),\n                            tensor_to_funsor(mvn.base_dist.loc, event_dims, 1),\n                            tensor_to_funsor(mvn.base_dist.scale, event_dims, 1),\n                            Variable(x_name, reals(x_size)),\n                            Variable(y_name, reals(y_size)))\n\n    info_vec = mvn.loc.unsqueeze(-1).cholesky_solve(mvn.scale_tril).squeeze(-1)\n    log_prob = (-0.5 * y_size * math.log(2 * math.pi)\n                - mvn.scale_tril.diagonal(dim1=-1, dim2=-2).log().sum(-1)\n                - 0.5 * (info_vec * mvn.loc).sum(-1))\n\n    batch_shape = broadcast_shape(matrix.shape[:-2], mvn.batch_shape)\n    P_yy = mvn.precision_matrix.expand(batch_shape + (y_size, y_size))\n    neg_P_xy = matrix.matmul(P_yy)\n    P_xy = -neg_P_xy\n    P_yx = P_xy.transpose(-1, -2)\n    P_xx = neg_P_xy.matmul(matrix.transpose(-1, -2))\n    precision = torch.cat([torch.cat([P_xx, P_xy], -1),\n                           torch.cat([P_yx, P_yy], -1)], -2)\n    info_y = info_vec.expand(batch_shape + (y_size,))\n    info_x = -matrix.matmul(info_y.unsqueeze(-1)).squeeze(-1)\n    info_vec = torch.cat([info_x, info_y], -1)\n\n    info_vec = tensor_to_funsor(info_vec, event_dims, 1)\n    precision = tensor_to_funsor(precision, event_dims, 2)\n    inputs = info_vec.inputs.copy()\n    inputs[x_name] = reals(x_size)\n    inputs[y_name] = reals(y_size)\n    return tensor_to_funsor(log_prob, event_dims) + Gaussian(info_vec.data, precision.data, inputs)\n'"
funsor/pyro/distribution.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport pyro.distributions as dist\nimport torch\nfrom torch.distributions import constraints\n\nfrom funsor.cnf import Contraction\nfrom funsor.delta import Delta\nfrom funsor.domains import bint\nfrom funsor.pyro.convert import DIM_TO_NAME, funsor_to_tensor, tensor_to_funsor\nfrom funsor.terms import Funsor, to_funsor\n\n\nclass FunsorDistribution(dist.TorchDistribution):\n    """"""\n    :class:`~torch.distributions.Distribution` wrapper around a\n    :class:`~funsor.terms.Funsor` for use in Pyro code. This is typically used\n    as a base class for specific funsor inference algorithms wrapped in a\n    distribution interface.\n\n    :param funsor.terms.Funsor funsor_dist: A funsor with an input named\n        ""value"" that is treated as a random variable. The distribution should\n        be normalized over ""value"".\n    :param torch.Size batch_shape: The distribution\'s batch shape. This must\n        be in the same order as the input of the ``funsor_dist``, but may\n        contain extra dims of size 1.\n    :param event_shape: The distribution\'s event shape.\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, funsor_dist, batch_shape=torch.Size(), event_shape=torch.Size(),\n                 dtype=""real"", validate_args=None):\n        assert isinstance(funsor_dist, Funsor)\n        assert isinstance(batch_shape, tuple)\n        assert isinstance(event_shape, tuple)\n        assert ""value"" in funsor_dist.inputs\n        super(FunsorDistribution, self).__init__(batch_shape, event_shape, validate_args)\n        self.funsor_dist = funsor_dist\n        self.dtype = dtype\n\n    @constraints.dependent_property\n    def support(self):\n        if self.dtype == ""real"":\n            return constraints.real\n        else:\n            return constraints.integer_interval(0, self.dtype - 1)\n\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        ndims = max(len(self.batch_shape), value.dim() - self.event_dim)\n        value = tensor_to_funsor(value, event_output=self.event_dim, dtype=self.dtype)\n        log_prob = self.funsor_dist(value=value)\n        log_prob = funsor_to_tensor(log_prob, ndims=ndims)\n        return log_prob\n\n    def _sample_delta(self, sample_shape):\n        sample_inputs = None\n        if sample_shape:\n            sample_inputs = OrderedDict()\n            shape = sample_shape + self.batch_shape\n            for dim in range(-len(shape), -len(self.batch_shape)):\n                if shape[dim] > 1:\n                    sample_inputs[DIM_TO_NAME[dim]] = bint(shape[dim])\n        delta = self.funsor_dist.sample(frozenset({""value""}), sample_inputs)\n        if isinstance(delta, Contraction):\n            assert len([d for d in delta.terms if isinstance(d, Delta)]) == 1\n            delta = delta.terms[0]\n        assert isinstance(delta, Delta)\n        return delta\n\n    @torch.no_grad()\n    def sample(self, sample_shape=torch.Size()):\n        delta = self._sample_delta(sample_shape)\n        ndims = len(sample_shape) + len(self.batch_shape) + len(self.event_shape)\n        value = funsor_to_tensor(delta.terms[0][1][0], ndims=ndims)\n        return value.detach()\n\n    def rsample(self, sample_shape=torch.Size()):\n        delta = self._sample_delta(sample_shape)\n        assert not delta.log_density.requires_grad, ""distribution is not fully reparametrized""\n        ndims = len(sample_shape) + len(self.batch_shape) + len(self.event_shape)\n        value = funsor_to_tensor(delta.terms[0][1][0], ndims=ndims)\n        return value\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(type(self), _instance)\n        batch_shape = torch.Size(batch_shape)\n        funsor_dist = self.funsor_dist + tensor_to_funsor(torch.zeros(batch_shape))\n        super(type(self), new).__init__(\n            funsor_dist, batch_shape, self.event_shape, self.dtype, validate_args=False)\n        new.validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n\n@to_funsor.register(FunsorDistribution)\ndef funsordistribution_to_funsor(pyro_dist, output=None, dim_to_name=None):\n    raise NotImplementedError(""TODO implement conversion for FunsorDistribution"")\n'"
funsor/pyro/hmm.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport torch\nfrom pyro.distributions.util import broadcast_shape\n\nimport funsor.ops as ops\nfrom funsor.domains import bint, reals\nfrom funsor.interpreter import interpretation\nfrom funsor.pyro.convert import (\n    dist_to_funsor,\n    funsor_to_cat_and_mvn,\n    funsor_to_tensor,\n    matrix_and_mvn_to_funsor,\n    mvn_to_funsor,\n    tensor_to_funsor\n)\nfrom funsor.pyro.distribution import FunsorDistribution\nfrom funsor.sum_product import MarkovProduct, naive_sequential_sum_product, sequential_sum_product\nfrom funsor.terms import Variable, eager, lazy, moment_matching\n\n\nclass DiscreteHMM(FunsorDistribution):\n    r""""""\n    Hidden Markov Model with discrete latent state and arbitrary observation\n    distribution. This uses [1] to parallelize over time, achieving\n    O(log(time)) parallel complexity.\n\n    The event_shape of this distribution includes time on the left::\n\n        event_shape = (num_steps,) + observation_dist.event_shape\n\n    This distribution supports any combination of homogeneous/heterogeneous\n    time dependency of ``transition_logits`` and ``observation_dist``. However,\n    because time is included in this distribution\'s event_shape, the\n    homogeneous+homogeneous case will have a broadcastable event_shape with\n    ``num_steps = 1``, allowing :meth:`log_prob` to work with arbitrary length\n    data::\n\n        # homogeneous + homogeneous case:\n        event_shape = (1,) + observation_dist.event_shape\n\n    This class should be interchangeable with\n    :class:`pyro.distributions.hmm.DiscreteHMM` .\n\n    **References:**\n\n    [1] Simo Sarkka, Angel F. Garcia-Fernandez (2019)\n        ""Temporal Parallelization of Bayesian Filters and Smoothers""\n        https://arxiv.org/pdf/1905.13002.pdf\n\n    :param ~torch.Tensor initial_logits: A logits tensor for an initial\n        categorical distribution over latent states. Should have rightmost size\n        ``state_dim`` and be broadcastable to ``batch_shape + (state_dim,)``.\n    :param ~torch.Tensor transition_logits: A logits tensor for transition\n        conditional distributions between latent states. Should have rightmost\n        shape ``(state_dim, state_dim)`` (old, new), and be broadcastable to\n        ``batch_shape + (num_steps, state_dim, state_dim)``.\n    :param ~torch.distributions.Distribution observation_dist: A conditional\n        distribution of observed data conditioned on latent state. The\n        ``.batch_shape`` should have rightmost size ``state_dim`` and be\n        broadcastable to ``batch_shape + (num_steps, state_dim)``. The\n        ``.event_shape`` may be arbitrary.\n    """"""\n    def __init__(self, initial_logits, transition_logits, observation_dist, validate_args=None):\n        assert isinstance(initial_logits, torch.Tensor)\n        assert isinstance(transition_logits, torch.Tensor)\n        assert isinstance(observation_dist, torch.distributions.Distribution)\n        assert initial_logits.dim() >= 1\n        assert transition_logits.dim() >= 2\n        assert len(observation_dist.batch_shape) >= 1\n        shape = broadcast_shape(initial_logits.shape[:-1] + (1,),\n                                transition_logits.shape[:-2],\n                                observation_dist.batch_shape[:-1])\n        batch_shape, time_shape = shape[:-1], shape[-1:]\n        event_shape = time_shape + observation_dist.event_shape\n        self._has_rsample = observation_dist.has_rsample\n\n        # Normalize.\n        initial_logits = initial_logits - initial_logits.logsumexp(-1, True)\n        transition_logits = transition_logits - transition_logits.logsumexp(-1, True)\n\n        # Convert tensors and distributions to funsors.\n        init = tensor_to_funsor(initial_logits, (""state"",))\n        trans = tensor_to_funsor(transition_logits, (""time"", ""state"", ""state(time=1)""))\n        obs = dist_to_funsor(observation_dist, (""time"", ""state(time=1)""))\n        dtype = obs.inputs[""value""].dtype\n\n        # Construct the joint funsor.\n        with interpretation(lazy):\n            # TODO perform math here once sequential_sum_product has been\n            #   implemented as a first-class funsor.\n            funsor_dist = Variable(""value"", obs.inputs[""value""])  # a bogus value\n            # Until funsor_dist is defined, we save factors for hand-computation in .log_prob().\n            self._init = init\n            self._trans = trans\n            self._obs = obs\n\n        super(DiscreteHMM, self).__init__(funsor_dist, batch_shape, event_shape, dtype, validate_args)\n\n    @torch.distributions.constraints.dependent_property\n    def has_rsample(self):\n        return self._has_rsample\n\n    # TODO remove this once self.funsor_dist is defined.\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        ndims = max(len(self.batch_shape), value.dim() - self.event_dim)\n        time = Variable(""time"", bint(self.event_shape[0]))\n        value = tensor_to_funsor(value, (""time"",), event_output=self.event_dim - 1,\n                                 dtype=self.dtype)\n\n        # Compare with pyro.distributions.hmm.DiscreteHMM.log_prob().\n        obs = self._obs(value=value)\n        result = self._trans + obs\n        result = sequential_sum_product(ops.logaddexp, ops.add,\n                                        result, time, {""state"": ""state(time=1)""})\n        result = self._init + result.reduce(ops.logaddexp, ""state(time=1)"")\n        result = result.reduce(ops.logaddexp, ""state"")\n\n        result = funsor_to_tensor(result, ndims=ndims)\n        return result\n\n    # TODO remove this once self.funsor_dist is defined.\n    def _sample_delta(self, sample_shape):\n        raise NotImplementedError(""TODO"")\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(DiscreteHMM, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new._has_rsample = self._has_rsample\n        new._init = self._init + tensor_to_funsor(torch.zeros(batch_shape))\n        new._trans = self._trans\n        new._obs = self._obs\n        super(DiscreteHMM, new).__init__(\n            self.funsor_dist, batch_shape, self.event_shape, self.dtype, validate_args=False)\n        new.validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n\nclass GaussianHMM(FunsorDistribution):\n    r""""""\n    Hidden Markov Model with Gaussians for initial, transition, and observation\n    distributions. This adapts [1] to parallelize over time to achieve\n    O(log(time)) parallel complexity, however it differs in that it tracks the\n    log normalizer to ensure :meth:`log_prob` is differentiable.\n\n    This corresponds to the generative model::\n\n        z = initial_distribution.sample()\n        x = []\n        for t in range(num_steps):\n            z = z @ transition_matrix + transition_dist.sample()\n            x.append(z @ observation_matrix + observation_dist.sample())\n\n    The event_shape of this distribution includes time on the left::\n\n        event_shape = (num_steps,) + observation_dist.event_shape\n\n    This distribution supports any combination of homogeneous/heterogeneous\n    time dependency of ``transition_dist`` and ``observation_dist``. However,\n    because time is included in this distribution\'s event_shape, the\n    homogeneous+homogeneous case will have a broadcastable event_shape with\n    ``num_steps = 1``, allowing :meth:`log_prob` to work with arbitrary length\n    data::\n\n        event_shape = (1, obs_dim)  # homogeneous + homogeneous case\n\n    This class should be compatible with\n    :class:`pyro.distributions.hmm.GaussianHMM` , but additionally supports\n    funsor :mod:`~funsor.adjoint` algorithms.\n\n    **References:**\n\n    [1] Simo Sarkka, Angel F. Garcia-Fernandez (2019)\n        ""Temporal Parallelization of Bayesian Filters and Smoothers""\n        https://arxiv.org/pdf/1905.13002.pdf\n\n    :ivar int hidden_dim: The dimension of the hidden state.\n    :ivar int obs_dim: The dimension of the observed state.\n    :param ~torch.distributions.MultivariateNormal initial_dist: A distribution\n        over initial states. This should have batch_shape broadcastable to\n        ``self.batch_shape``.  This should have event_shape ``(hidden_dim,)``.\n    :param ~torch.Tensor transition_matrix: A linear transformation of hidden\n        state. This should have shape broadcastable to\n        ``self.batch_shape + (num_steps, hidden_dim, hidden_dim)`` where the\n        rightmost dims are ordered ``(old, new)``.\n    :param ~torch.distributions.MultivariateNormal transition_dist: A process\n        noise distribution. This should have batch_shape broadcastable to\n        ``self.batch_shape + (num_steps,)``.  This should have event_shape\n        ``(hidden_dim,)``.\n    :param ~torch.Tensor transition_matrix: A linear transformation from hidden\n        to observed state. This should have shape broadcastable to\n        ``self.batch_shape + (num_steps, hidden_dim, obs_dim)``.\n    :param observation_dist: An observation noise distribution. This should\n        have batch_shape broadcastable to ``self.batch_shape + (num_steps,)``.\n        This should have event_shape ``(obs_dim,)``.\n    :type observation_dist: ~torch.distributions.MultivariateNormal or\n        ~torch.distributions.Independent of ~torch.distributions.Normal\n    """"""\n    has_rsample = True\n    arg_constraints = {}\n\n    def __init__(self, initial_dist, transition_matrix, transition_dist,\n                 observation_matrix, observation_dist, validate_args=None):\n        assert isinstance(initial_dist, torch.distributions.MultivariateNormal)\n        assert isinstance(transition_matrix, torch.Tensor)\n        assert isinstance(transition_dist, torch.distributions.MultivariateNormal)\n        assert isinstance(observation_matrix, torch.Tensor)\n        assert isinstance(observation_dist, torch.distributions.MultivariateNormal)\n        hidden_dim, obs_dim = observation_matrix.shape[-2:]\n        assert obs_dim >= hidden_dim // 2, ""obs_dim must be at least half of hidden_dim""\n        assert initial_dist.event_shape == (hidden_dim,)\n        assert transition_matrix.shape[-2:] == (hidden_dim, hidden_dim)\n        assert transition_dist.event_shape == (hidden_dim,)\n        assert observation_dist.event_shape == (obs_dim,)\n        shape = broadcast_shape(initial_dist.batch_shape + (1,),\n                                transition_matrix.shape[:-2],\n                                transition_dist.batch_shape,\n                                observation_matrix.shape[:-2],\n                                observation_dist.batch_shape)\n        batch_shape, time_shape = shape[:-1], shape[-1:]\n        event_shape = time_shape + (obs_dim,)\n\n        # Convert distributions to funsors.\n        init = dist_to_funsor(initial_dist)(value=""state"")\n        trans = matrix_and_mvn_to_funsor(transition_matrix, transition_dist,\n                                         (""time"",), ""state"", ""state(time=1)"")\n        obs = matrix_and_mvn_to_funsor(observation_matrix, observation_dist,\n                                       (""time"",), ""state(time=1)"", ""value"")\n        dtype = ""real""\n\n        # Construct the joint funsor.\n        with interpretation(lazy):\n            value = Variable(""value"", reals(time_shape[0], obs_dim))\n            result = trans + obs(value=value[""time""])\n            result = MarkovProduct(ops.logaddexp, ops.add,\n                                   result, ""time"", {""state"": ""state(time=1)""})\n            result = init + result.reduce(ops.logaddexp, ""state(time=1)"")\n            funsor_dist = result.reduce(ops.logaddexp, ""state"")\n\n        super(GaussianHMM, self).__init__(\n            funsor_dist, batch_shape, event_shape, dtype, validate_args)\n        self.hidden_dim = hidden_dim\n        self.obs_dim = obs_dim\n\n\nclass GaussianMRF(FunsorDistribution):\n    r""""""\n    Temporal Markov Random Field with Gaussian factors for initial, transition,\n    and observation distributions. This adapts [1] to parallelize over time to\n    achieve O(log(time)) parallel complexity, however it differs in that it\n    tracks the log normalizer to ensure :meth:`log_prob` is differentiable.\n\n    The event_shape of this distribution includes time on the left::\n\n        event_shape = (num_steps,) + observation_dist.event_shape\n\n    This distribution supports any combination of homogeneous/heterogeneous\n    time dependency of ``transition_dist`` and ``observation_dist``. However,\n    because time is included in this distribution\'s event_shape, the\n    homogeneous+homogeneous case will have a broadcastable event_shape with\n    ``num_steps = 1``, allowing :meth:`log_prob` to work with arbitrary length\n    data::\n\n        event_shape = (1, obs_dim)  # homogeneous + homogeneous case\n\n    This class should be compatible with\n    :class:`pyro.distributions.hmm.GaussianMRF` , but additionally supports\n    funsor :mod:`~funsor.adjoint` algorithms.\n\n    **References:**\n\n    [1] Simo Sarkka, Angel F. Garcia-Fernandez (2019)\n        ""Temporal Parallelization of Bayesian Filters and Smoothers""\n        https://arxiv.org/pdf/1905.13002.pdf\n\n    :ivar int hidden_dim: The dimension of the hidden state.\n    :ivar int obs_dim: The dimension of the observed state.\n    :param ~torch.distributions.MultivariateNormal initial_dist: A distribution\n        over initial states. This should have batch_shape broadcastable to\n        ``self.batch_shape``.  This should have event_shape ``(hidden_dim,)``.\n    :param ~torch.distributions.MultivariateNormal transition_dist: A joint\n        distribution factor over a pair of successive time steps. This should\n        have batch_shape broadcastable to ``self.batch_shape + (num_steps,)``.\n        This should have event_shape ``(hidden_dim + hidden_dim,)`` (old+new).\n    :param ~torch.distributions.MultivariateNormal observation_dist: A joint\n        distribution factor over a hidden and an observed state. This should\n        have batch_shape broadcastable to ``self.batch_shape + (num_steps,)``.\n        This should have event_shape ``(hidden_dim + obs_dim,)``.\n    """"""\n    has_rsample = True\n\n    def __init__(self, initial_dist, transition_dist, observation_dist, validate_args=None):\n        assert isinstance(initial_dist, torch.distributions.MultivariateNormal)\n        assert isinstance(transition_dist, torch.distributions.MultivariateNormal)\n        assert isinstance(observation_dist, torch.distributions.MultivariateNormal)\n        hidden_dim = initial_dist.event_shape[0]\n        assert transition_dist.event_shape[0] == hidden_dim + hidden_dim\n        obs_dim = observation_dist.event_shape[0] - hidden_dim\n        shape = broadcast_shape(initial_dist.batch_shape + (1,),\n                                transition_dist.batch_shape,\n                                observation_dist.batch_shape)\n        batch_shape, time_shape = shape[:-1], shape[-1:]\n        event_shape = time_shape + (obs_dim,)\n\n        # Convert distributions to funsors.\n        init = dist_to_funsor(initial_dist)(value=""state"")\n        trans = mvn_to_funsor(transition_dist, (""time"",),\n                              OrderedDict([(""state"", reals(hidden_dim)),\n                                           (""state(time=1)"", reals(hidden_dim))]))\n        obs = mvn_to_funsor(observation_dist, (""time"",),\n                            OrderedDict([(""state(time=1)"", reals(hidden_dim)),\n                                         (""value"", reals(obs_dim))]))\n\n        # Construct the joint funsor.\n        # Compare with pyro.distributions.hmm.GaussianMRF.log_prob().\n        with interpretation(lazy):\n            time = Variable(""time"", bint(time_shape[0]))\n            value = Variable(""value"", reals(time_shape[0], obs_dim))\n            logp_oh = trans + obs(value=value[""time""])\n            logp_oh = MarkovProduct(ops.logaddexp, ops.add,\n                                    logp_oh, time, {""state"": ""state(time=1)""})\n            logp_oh += init\n            logp_oh = logp_oh.reduce(ops.logaddexp, frozenset({""state"", ""state(time=1)""}))\n            logp_h = trans + obs.reduce(ops.logaddexp, ""value"")\n            logp_h = MarkovProduct(ops.logaddexp, ops.add,\n                                   logp_h, time, {""state"": ""state(time=1)""})\n            logp_h += init\n            logp_h = logp_h.reduce(ops.logaddexp, frozenset({""state"", ""state(time=1)""}))\n            funsor_dist = logp_oh - logp_h\n\n        dtype = ""real""\n        super(GaussianMRF, self).__init__(funsor_dist, batch_shape, event_shape, dtype, validate_args)\n        self.hidden_dim = hidden_dim\n        self.obs_dim = obs_dim\n\n\nclass SwitchingLinearHMM(FunsorDistribution):\n    r""""""\n    Switching Linear Dynamical System represented as a Hidden Markov Model.\n\n    This corresponds to the generative model::\n\n        z = Categorical(logits=initial_logits).sample()\n        y = initial_mvn[z].sample()\n        x = []\n        for t in range(num_steps):\n            z = Categorical(logits=transition_logits[t, z]).sample()\n            y = y @ transition_matrix[t, z] + transition_mvn[t, z].sample()\n            x.append(y @ observation_matrix[t, z] + observation_mvn[t, z].sample())\n\n    Viewed as a dynamic Bayesian network::\n\n        z[t-1] ----> z[t] ---> z[t+1]         Discrete latent class\n           |  \\       |  \\       |   \\\n           | y[t-1] ----> y[t] ----> y[t+1]   Gaussian latent state\n           |   /      |   /      |   /\n           V  /       V  /       V  /\n        x[t-1]       x[t]      x[t+1]         Gaussian observation\n\n    Let ``class`` be the latent class, ``state`` be the latent multivariate\n    normal state, and ``value`` be the observed multivariate normal value.\n\n    :param ~torch.Tensor initial_logits: Represents ``p(class[0])``.\n    :param ~torch.distributions.MultivariateNormal initial_mvn: Represents\n        ``p(state[0] | class[0])``.\n    :param ~torch.Tensor transition_logits: Represents\n        ``p(class[t+1] | class[t])``.\n    :param ~torch.Tensor transition_matrix:\n    :param ~torch.distributions.MultivariateNormal transition_mvn: Together\n        with ``transition_matrix``, this represents\n        ``p(state[t], state[t+1] | class[t])``.\n    :param ~torch.Tensor observation_matrix:\n    :param ~torch.distributions.MultivariateNormal observation_mvn: Together\n        with ``observation_matrix``, this represents\n        ``p(value[t+1], state[t+1] | class[t+1])``.\n    :param bool exact: If True, perform exact inference at cost exponential in\n        ``num_steps``. If False, use a :func:`~funsor.terms.moment_matching`\n        approximation and use parallel scan algorithm to reduce parallel\n        complexity to logarithmic in ``num_steps``. Defaults to False.\n    """"""\n    has_rsample = True\n    arg_constraints = {}\n\n    def __init__(self, initial_logits, initial_mvn,\n                 transition_logits, transition_matrix, transition_mvn,\n                 observation_matrix, observation_mvn, exact=False, validate_args=None):\n        assert isinstance(initial_logits, torch.Tensor)\n        assert isinstance(initial_mvn, torch.distributions.MultivariateNormal)\n        assert isinstance(transition_logits, torch.Tensor)\n        assert isinstance(transition_matrix, torch.Tensor)\n        assert isinstance(transition_mvn, torch.distributions.MultivariateNormal)\n        assert isinstance(observation_matrix, torch.Tensor)\n        assert isinstance(observation_mvn, torch.distributions.MultivariateNormal)\n        hidden_cardinality = initial_logits.size(-1)\n        hidden_dim, obs_dim = observation_matrix.shape[-2:]\n        assert obs_dim >= hidden_dim // 2, ""obs_dim must be at least half of hidden_dim""\n        assert initial_mvn.event_shape[0] == hidden_dim\n        assert transition_logits.size(-1) == hidden_cardinality\n        assert transition_matrix.shape[-2:] == (hidden_dim, hidden_dim)\n        assert transition_mvn.event_shape[0] == hidden_dim\n        assert observation_mvn.event_shape[0] == obs_dim\n        init_shape = broadcast_shape(initial_logits.shape, initial_mvn.batch_shape)\n        shape = broadcast_shape(init_shape[:-1] + (1, init_shape[-1]),\n                                transition_logits.shape[:-1],\n                                transition_matrix.shape[:-2],\n                                transition_mvn.batch_shape,\n                                observation_matrix.shape[:-2],\n                                observation_mvn.batch_shape)\n        assert shape[-1] == hidden_cardinality\n        batch_shape, time_shape = shape[:-2], shape[-2:-1]\n        event_shape = time_shape + (obs_dim,)\n\n        # Normalize.\n        initial_logits = initial_logits - initial_logits.logsumexp(-1, True)\n        transition_logits = transition_logits - transition_logits.logsumexp(-1, True)\n\n        # Convert tensors and distributions to funsors.\n        init = (tensor_to_funsor(initial_logits, (""class"",)) +\n                dist_to_funsor(initial_mvn, (""class"",))(value=""state""))\n        trans = (tensor_to_funsor(transition_logits, (""time"", ""class"", ""class(time=1)"")) +\n                 matrix_and_mvn_to_funsor(transition_matrix, transition_mvn,\n                                          (""time"", ""class(time=1)""), ""state"", ""state(time=1)""))\n        obs = matrix_and_mvn_to_funsor(observation_matrix, observation_mvn,\n                                       (""time"", ""class(time=1)""), ""state(time=1)"", ""value"")\n        if ""class(time=1)"" not in set(trans.inputs).union(obs.inputs):\n            raise ValueError(""neither transition nor observation depend on discrete state"")\n        dtype = ""real""\n\n        # Construct the joint funsor.\n        with interpretation(lazy):\n            # TODO perform math here once sequential_sum_product has been\n            #   implemented as a first-class funsor.\n            funsor_dist = Variable(""value"", obs.inputs[""value""])  # a bogus value\n            # Until funsor_dist is defined, we save factors for hand-computation in .log_prob().\n            self._init = init\n            self._trans = trans\n            self._obs = obs\n\n        super(SwitchingLinearHMM, self).__init__(\n            funsor_dist, batch_shape, event_shape, dtype, validate_args)\n        self.exact = exact\n\n    # TODO remove this once self.funsor_dist is defined.\n    def log_prob(self, value):\n        ndims = max(len(self.batch_shape), value.dim() - 2)\n        time = Variable(""time"", bint(self.event_shape[0]))\n        value = tensor_to_funsor(value, (""time"",), 1)\n\n        seq_sum_prod = naive_sequential_sum_product if self.exact else sequential_sum_product\n        with interpretation(eager if self.exact else moment_matching):\n            result = self._trans + self._obs(value=value)\n            result = seq_sum_prod(ops.logaddexp, ops.add, result, time,\n                                  {""class"": ""class(time=1)"", ""state"": ""state(time=1)""})\n            result += self._init\n            result = result.reduce(\n                ops.logaddexp, frozenset([""class"", ""state"", ""class(time=1)"", ""state(time=1)""]))\n\n            result = funsor_to_tensor(result, ndims=ndims)\n            return result\n\n    # TODO remove this once self.funsor_dist is defined.\n    def _sample_delta(self, sample_shape):\n        raise NotImplementedError(""TODO"")\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(SwitchingLinearHMM, _instance)\n        batch_shape = torch.Size(batch_shape)\n        new._init = self._init + tensor_to_funsor(torch.zeros(batch_shape))\n        new._trans = self._trans\n        new._obs = self._obs\n        new.exact = self.exact\n        super(SwitchingLinearHMM, new).__init__(\n            self.funsor_dist, batch_shape, self.event_shape, self.dtype, validate_args=False)\n        new.validate_args = self.__dict__.get(\'_validate_args\')\n        return new\n\n    def filter(self, value):\n        """"""\n        Compute posterior over final state given a sequence of observations.\n\n        :param ~torch.Tensor value: A sequence of observations.\n        :return: A posterior distribution over latent states at the final time\n            step, represented as a pair ``(cat, mvn)``, where\n            :class:`~pyro.distributions.Categorical` distribution over mixture\n            components and ``mvn`` is a\n            :class:`~pyro.distributions.MultivariateNormal` with rightmost\n            batch dimension ranging over mixture components. This can then be\n            used to initialize a sequential Pyro model for prediction.\n        :rtype: tuple\n        """"""\n        ndims = max(len(self.batch_shape), value.dim() - 2)\n        time = Variable(""time"", bint(self.event_shape[0]))\n        value = tensor_to_funsor(value, (""time"",), 1)\n\n        seq_sum_prod = naive_sequential_sum_product if self.exact else sequential_sum_product\n        with interpretation(eager if self.exact else moment_matching):\n            logp = self._trans + self._obs(value=value)\n            logp = seq_sum_prod(ops.logaddexp, ops.add, logp, time,\n                                {""class"": ""class(time=1)"", ""state"": ""state(time=1)""})\n            logp += self._init\n            logp = logp.reduce(ops.logaddexp, frozenset([""class"", ""state""]))\n\n        cat, mvn = funsor_to_cat_and_mvn(logp, ndims, (""class(time=1)"",))\n        cat = cat.expand(self.batch_shape)\n        mvn = mvn.expand(self.batch_shape + cat.logits.shape[-1:])\n        return cat, mvn\n'"
funsor/torch/__init__.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\nfrom multipledispatch import dispatch\n\nimport funsor.torch.distributions  # noqa: F401\nimport funsor.torch.ops  # noqa: F401\nimport funsor.ops as ops\nfrom funsor.adjoint import adjoint_ops\nfrom funsor.interpreter import children, recursion_reinterpret\nfrom funsor.terms import Funsor, to_funsor\nfrom funsor.tensor import Tensor, tensor_to_funsor\nfrom funsor.util import quote\n\n\n@adjoint_ops.register(Tensor, ops.AssociativeOp, ops.AssociativeOp, Funsor, torch.Tensor, tuple, object)\ndef adjoint_tensor(adj_redop, adj_binop, out_adj, data, inputs, dtype):\n    return {}\n\n\n@recursion_reinterpret.register(torch.Tensor)\n@recursion_reinterpret.register(torch.nn.Module)\ndef recursion_reinterpret_ground(x):\n    return x\n\n\n@children.register(torch.Tensor)\n@children.register(torch.nn.Module)\ndef _children_ground(x):\n    return ()\n\n\n@quote.register(torch.Tensor)\ndef _quote(x, indent, out):\n    """"""\n    Work around PyTorch not supporting reproducible repr.\n    """"""\n    out.append((indent, f""torch.tensor({repr(x.tolist())}, dtype={x.dtype})""))\n\n\nto_funsor.register(torch.Tensor)(tensor_to_funsor)\n\n\n@dispatch(torch.Tensor, torch.Tensor, [float])\ndef allclose(a, b, rtol=1e-05, atol=1e-08):\n    return torch.allclose(a, b, rtol=rtol, atol=atol)\n'"
funsor/torch/distributions.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport functools\n\nimport pyro.distributions as dist\nimport pyro.distributions.testing.fakes as fakes\nfrom pyro.distributions.torch_distribution import MaskedDistribution\nimport torch\n\nfrom funsor.distribution import (  # noqa: F401\n    Bernoulli,\n    FUNSOR_DIST_NAMES,\n    LogNormal,\n    backenddist_to_funsor,\n    eager_beta,\n    eager_binomial,\n    eager_categorical_funsor,\n    eager_categorical_tensor,\n    eager_delta_funsor_funsor,\n    eager_delta_funsor_variable,\n    eager_delta_tensor,\n    eager_delta_variable_variable,\n    eager_multinomial,\n    eager_mvn,\n    eager_normal,\n    indepdist_to_funsor,\n    make_dist,\n    maskeddist_to_funsor,\n    mvndist_to_funsor,\n    transformeddist_to_funsor,\n)\nfrom funsor.domains import reals\nfrom funsor.tensor import Tensor, dummy_numeric_array\nfrom funsor.terms import Funsor, Variable, eager, to_funsor\n\n\n################################################################################\n# Distribution Wrappers\n################################################################################\n\n\nclass _PyroWrapper_BernoulliProbs(dist.Bernoulli):\n    def __init__(self, probs, validate_args=None):\n        return super().__init__(probs=probs, validate_args=validate_args)\n\n    # XXX: subclasses of Pyro distribution which defines a custom __init__ method\n    # should also have `expand` implemented.\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(_PyroWrapper_BernoulliProbs, _instance)\n        return super().expand(batch_shape, _instance=new)\n\n\nclass _PyroWrapper_BernoulliLogits(dist.Bernoulli):\n    def __init__(self, logits, validate_args=None):\n        return super().__init__(logits=logits, validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(_PyroWrapper_BernoulliLogits, _instance)\n        return super().expand(batch_shape, _instance=new)\n\n\nclass _PyroWrapper_CategoricalLogits(dist.Categorical):\n    def __init__(self, logits, validate_args=None):\n        return super().__init__(logits=logits, validate_args=validate_args)\n\n    def expand(self, batch_shape, _instance=None):\n        new = self._get_checked_instance(_PyroWrapper_CategoricalLogits, _instance)\n        return super().expand(batch_shape, _instance=new)\n\n\ndef _get_pyro_dist(dist_name):\n    if dist_name in ['BernoulliProbs', 'BernoulliLogits', 'CategoricalLogits']:\n        return globals().get('_PyroWrapper_' + dist_name)\n    elif dist_name.startswith('Nonreparameterized'):\n        return getattr(fakes, dist_name)\n    else:\n        return getattr(dist, dist_name)\n\n\nPYRO_DIST_NAMES = FUNSOR_DIST_NAMES + [\n    ('DirichletMultinomial', ('concentration', 'total_count')),\n    ('VonMises', ('loc', 'concentration')),\n]\n\n\nfor dist_name, param_names in PYRO_DIST_NAMES:\n    locals()[dist_name] = make_dist(_get_pyro_dist(dist_name), param_names)\n\n\n# Delta has to be treated specially because of its weird shape inference semantics\nDelta._infer_value_domain = classmethod(lambda cls, **kwargs: kwargs['v'])  # noqa: F821\n\n\n# Multinomial and related dists have dependent bint dtypes, so we just make them 'real'\n# See issue: https://github.com/pyro-ppl/funsor/issues/322\n@functools.lru_cache(maxsize=5000)\ndef _multinomial_infer_value_domain(cls, **kwargs):\n    instance = cls.dist_class(**{k: dummy_numeric_array(domain) for k, domain in kwargs.items()}, validate_args=False)\n    return reals(*instance.event_shape)\n\n\nBinomial._infer_value_domain = classmethod(_multinomial_infer_value_domain)  # noqa: F821\nMultinomial._infer_value_domain = classmethod(_multinomial_infer_value_domain)  # noqa: F821\nDirichletMultinomial._infer_value_domain = classmethod(_multinomial_infer_value_domain)  # noqa: F821\n\n\n###############################################\n# Converting PyTorch Distributions to funsors\n###############################################\n\nto_funsor.register(torch.distributions.Distribution)(backenddist_to_funsor)\nto_funsor.register(torch.distributions.Independent)(indepdist_to_funsor)\nto_funsor.register(MaskedDistribution)(maskeddist_to_funsor)\nto_funsor.register(torch.distributions.TransformedDistribution)(transformeddist_to_funsor)\nto_funsor.register(torch.distributions.MultivariateNormal)(mvndist_to_funsor)\n\n\n@to_funsor.register(torch.distributions.Bernoulli)\ndef bernoulli_to_funsor(pyro_dist, output=None, dim_to_name=None):\n    new_pyro_dist = _PyroWrapper_BernoulliLogits(logits=pyro_dist.logits)\n    return backenddist_to_funsor(new_pyro_dist, output, dim_to_name)\n\n\neager.register(Beta, Funsor, Funsor, Funsor)(eager_beta)  # noqa: F821)\neager.register(Binomial, Funsor, Funsor, Funsor)(eager_binomial)  # noqa: F821\neager.register(Multinomial, Tensor, Tensor, Tensor)(eager_multinomial)  # noqa: F821)\neager.register(Categorical, Funsor, Tensor)(eager_categorical_funsor)  # noqa: F821)\neager.register(Categorical, Tensor, Variable)(eager_categorical_tensor)  # noqa: F821)\neager.register(Delta, Tensor, Tensor, Tensor)(eager_delta_tensor)  # noqa: F821\neager.register(Delta, Funsor, Funsor, Variable)(eager_delta_funsor_variable)  # noqa: F821\neager.register(Delta, Variable, Funsor, Variable)(eager_delta_funsor_variable)  # noqa: F821\neager.register(Delta, Variable, Funsor, Funsor)(eager_delta_funsor_funsor)  # noqa: F821\neager.register(Delta, Variable, Variable, Variable)(eager_delta_variable_variable)  # noqa: F821\neager.register(Normal, Funsor, Tensor, Funsor)(eager_normal)  # noqa: F821\neager.register(MultivariateNormal, Funsor, Tensor, Funsor)(eager_mvn)  # noqa: F821\n"""
funsor/torch/ops.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport torch\n\nimport funsor.ops as ops\n\n\n################################################################################\n# Register Ops\n################################################################################\n\nops.abs.register(torch.Tensor)(torch.abs)\nops.cholesky_solve.register(torch.Tensor, torch.Tensor)(torch.cholesky_solve)\nops.clamp.register(torch.Tensor, object, object)(torch.clamp)\nops.exp.register(torch.Tensor)(torch.exp)\nops.full_like.register(torch.Tensor, object)(torch.full_like)\nops.log1p.register(torch.Tensor)(torch.log1p)\nops.sqrt.register(torch.Tensor)(torch.sqrt)\nops.transpose.register(torch.Tensor, int, int)(torch.transpose)\nops.unsqueeze.register(torch.Tensor, int)(torch.unsqueeze)\n\n\n@ops.all.register(torch.Tensor, (int, type(None)))\ndef _all(x, dim):\n    return x.all() if dim is None else x.all(dim=dim)\n\n\n@ops.amax.register(torch.Tensor, (int, type(None)))\ndef _amax(x, dim, keepdims=False):\n    return x.max() if dim is None else x.max(dim, keepdims)[0]\n\n\n@ops.amin.register(torch.Tensor, (int, type(None)))\ndef _amin(x, dim, keepdims=False):\n    return x.min() if dim is None else x.min(dim, keepdims)[0]\n\n\n@ops.any.register(torch.Tensor, (int, type(None)))\ndef _any(x, dim):\n    return x.any() if dim is None else x.any(dim=dim)\n\n\n@ops.astype.register(torch.Tensor, str)\ndef _astype(x, dtype):\n    return x.type(getattr(torch, dtype))\n\n\n@ops.cat.register(int, [torch.Tensor])\ndef _cat(dim, *x):\n    if len(x) == 1:\n        return x[0]\n    return torch.cat(x, dim=dim)\n\n\n@ops.cholesky.register(torch.Tensor)\ndef _cholesky(x):\n    """"""\n    Like :func:`torch.cholesky` but uses sqrt for scalar matrices.\n    Works around https://github.com/pytorch/pytorch/issues/24403 often.\n    """"""\n    if x.size(-1) == 1:\n        return x.sqrt()\n    return x.cholesky()\n\n\n@ops.cholesky_inverse.register(torch.Tensor)\ndef _cholesky_inverse(x):\n    """"""\n    Like :func:`torch.cholesky_inverse` but supports batching and gradients.\n    """"""\n    if x.dim() == 2:\n        return x.cholesky_inverse()\n    return torch.eye(x.size(-1)).cholesky_solve(x)\n\n\n@ops.detach.register(torch.Tensor)\ndef _detach(x):\n    return x.detach()\n\n\n@ops.diagonal.register(torch.Tensor, int, int)\ndef _diagonal(x, dim1, dim2):\n    return x.diagonal(dim1=dim1, dim2=dim2)\n\n\n@ops.einsum.register(str, [torch.Tensor])\ndef _einsum(equation, *operands):\n    return torch.einsum(equation, *operands)\n\n\n@ops.expand.register(torch.Tensor, tuple)\ndef _expand(x, shape):\n    return x.expand(shape)\n\n\n@ops.finfo.register(torch.Tensor)\ndef _finfo(x):\n    return torch.finfo(x.dtype)\n\n\n@ops.is_numeric_array.register(torch.Tensor)\ndef _is_numeric_array(x):\n    return True\n\n\n@ops.log.register(torch.Tensor)\ndef _log(x):\n    if x.dtype in (torch.bool, torch.uint8, torch.long):\n        x = x.float()\n    return x.log()\n\n\n@ops.logsumexp.register(torch.Tensor, (int, type(None)))\ndef _logsumexp(x, dim):\n    return x.reshape(-1).logsumexp(0) if dim is None else x.logsumexp(dim)\n\n\n@ops.max.register(torch.Tensor, torch.Tensor)\ndef _max(x, y):\n    return torch.max(x, y)\n\n\n@ops.max.register(object, torch.Tensor)\ndef _max(x, y):\n    return y.clamp(min=x)\n\n\n@ops.max.register(torch.Tensor, object)\ndef _max(x, y):\n    return x.clamp(min=y)\n\n\n@ops.min.register(torch.Tensor, torch.Tensor)\ndef _min(x, y):\n    return torch.min(x, y)\n\n\n@ops.min.register(object, torch.Tensor)\ndef _min(x, y):\n    return y.clamp(max=x)\n\n\n@ops.min.register(torch.Tensor, object)\ndef _min(x, y):\n    return x.clamp(max=y)\n\n\n@ops.new_arange.register(torch.Tensor, int, int, int)\ndef _new_arange(x, start, stop, step):\n    return torch.arange(start, stop, step)\n\n\n@ops.new_arange.register(torch.Tensor, (int, torch.Tensor))\ndef _new_arange(x, stop):\n    return torch.arange(stop)\n\n\n@ops.new_eye.register(torch.Tensor, tuple)\ndef _new_eye(x, shape):\n    return torch.eye(shape[-1]).expand(shape + (-1,))\n\n\n@ops.new_zeros.register(torch.Tensor, tuple)\ndef _new_zeros(x, shape):\n    return x.new_zeros(shape)\n\n\n@ops.permute.register(torch.Tensor, (tuple, list))\ndef _permute(x, dims):\n    return x.permute(dims)\n\n\n@ops.pow.register(object, torch.Tensor)\ndef _pow(x, y):\n    result = x ** y\n    # work around shape bug https://github.com/pytorch/pytorch/issues/16685\n    return result.reshape(y.shape)\n\n\n@ops.pow.register(torch.Tensor, (object, torch.Tensor))\ndef _pow(x, y):\n    return x ** y\n\n\n@ops.prod.register(torch.Tensor, (int, type(None)))\ndef _prod(x, dim):\n    return x.prod() if dim is None else x.prod(dim=dim)\n\n\n@ops.reciprocal.register(torch.Tensor)\ndef _reciprocal(x):\n    result = x.reciprocal().clamp(max=torch.finfo(x.dtype).max)\n    return result\n\n\n@ops.safediv.register(object, torch.Tensor)\ndef _safediv(x, y):\n    try:\n        finfo = torch.finfo(y.dtype)\n    except TypeError:\n        finfo = torch.iinfo(y.dtype)\n    return x * y.reciprocal().clamp(max=finfo.max)\n\n\n@ops.safesub.register(object, torch.Tensor)\ndef _safesub(x, y):\n    try:\n        finfo = torch.finfo(y.dtype)\n    except TypeError:\n        finfo = torch.iinfo(y.dtype)\n    return x + (-y).clamp(max=finfo.max)\n\n\n@ops.stack.register(int, [torch.Tensor])\ndef _stack(dim, *x):\n    return torch.stack(x, dim=dim)\n\n\n@ops.sum.register(torch.Tensor, (int, type(None)))\ndef _sum(x, dim):\n    return x.sum() if dim is None else x.sum(dim)\n\n\n@ops.triangular_solve.register(torch.Tensor, torch.Tensor)\ndef _triangular_solve(x, y, upper=False, transpose=False):\n    return x.triangular_solve(y, upper, transpose).solution\n'"
test/examples/conftest.py,0,b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro\n\n\ndef pytest_runtest_setup(item):\n    pyro.set_rng_seed(0)\n    pyro.enable_validation(True)\n'
test/examples/test_bart.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nimport pytest\nimport torch\n\nimport funsor\nimport funsor.torch.distributions as dist\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction\nfrom funsor.domains import bint, reals\nfrom funsor.gaussian import Gaussian\nfrom funsor.integrate import Integrate\nfrom funsor.interpreter import interpretation\nfrom funsor.montecarlo import monte_carlo\nfrom funsor.pyro.convert import AffineNormal\nfrom funsor.sum_product import MarkovProduct\nfrom funsor.tensor import Function, Tensor\nfrom funsor.terms import Binary, Independent, Stack, Subs, Variable, reflect\nfrom funsor.testing import xfail_param\n\nnum_origins = 2\nnum_destins = 2\n\n\ndef bounded_exp(x, bound):\n    return (x - math.log(bound)).sigmoid() * bound\n\n\ncall_count = 0\n\n\n@funsor.function(reals(2 * num_origins * num_destins),\n                 (reals(num_origins, num_destins, 2),\n                 reals(num_origins, num_destins)))\ndef unpack_gate_rate(gate_rate):\n    global call_count\n    call_count += 1\n    batch_shape = gate_rate.shape[:-1]\n    event_shape = (2, num_origins, num_destins)\n    gate, rate = gate_rate.reshape(batch_shape + event_shape).unbind(-3)\n    rate = bounded_exp(rate, bound=1e4)\n    gate = torch.stack((torch.zeros_like(gate), gate), dim=-1)\n    return gate, rate\n\n\nunpack_gate_rate_0 = unpack_gate_rate[0].fn\nunpack_gate_rate_1 = unpack_gate_rate[1].fn\n\n\n@pytest.mark.parametrize(\'analytic_kl\', [\n    False,\n    xfail_param(True, reason=""missing pattern""),\n], ids=[\'monte-carlo-kl\', \'analytic-kl\'])\ndef test_bart(analytic_kl):\n    global call_count\n    call_count = 0\n\n    with interpretation(reflect):\n        q = Independent(\n         Independent(\n          Contraction(ops.nullop, ops.add,\n           frozenset(),\n           (Tensor(\n             torch.tensor([[-0.6077086925506592, -1.1546266078948975, -0.7021151781082153, -0.5303535461425781, -0.6365622282028198, -1.2423288822174072, -0.9941254258155823, -0.6287292242050171], [-0.6987162828445435, -1.0875964164733887, -0.7337473630905151, -0.4713417589664459, -0.6674002408981323, -1.2478348016738892, -0.8939017057418823, -0.5238542556762695]], dtype=torch.float32),  # noqa\n             ((\'time_b4\',\n               bint(2),),\n              (\'_event_1_b2\',\n               bint(8),),),\n             \'real\'),\n            Gaussian(\n             torch.tensor([[[-0.3536059558391571], [-0.21779225766658783], [0.2840439975261688], [0.4531521499156952], [-0.1220812276005745], [-0.05519985035061836], [0.10932210087776184], [0.6656699776649475]], [[-0.39107921719551086], [-0.20241987705230713], [0.2170514464378357], [0.4500560462474823], [0.27945515513420105], [-0.0490039587020874], [-0.06399798393249512], [0.846565842628479]]], dtype=torch.float32),  # noqa\n             torch.tensor([[[[1.984686255455017]], [[0.6699360013008118]], [[1.6215802431106567]], [[2.372016668319702]], [[1.77385413646698]], [[0.526767373085022]], [[0.8722561597824097]], [[2.1879124641418457]]], [[[1.6996612548828125]], [[0.7535632252693176]], [[1.4946647882461548]], [[2.642792224884033]], [[1.7301604747772217]], [[0.5203893780708313]], [[1.055436372756958]], [[2.8370864391326904]]]], dtype=torch.float32),  # noqa\n             ((\'time_b4\',\n               bint(2),),\n              (\'_event_1_b2\',\n               bint(8),),\n              (\'value_b1\',\n               reals(),),)),)),\n          \'gate_rate_b3\',\n          \'_event_1_b2\',\n          \'value_b1\'),\n         \'gate_rate_t\',\n         \'time_b4\',\n         \'gate_rate_b3\')\n        p_prior = Contraction(ops.logaddexp, ops.add,\n         frozenset({\'state(time=1)_b11\', \'state_b10\'}),\n         (MarkovProduct(ops.logaddexp, ops.add,\n           Contraction(ops.nullop, ops.add,\n            frozenset(),\n            (Tensor(\n              torch.tensor(2.7672932147979736, dtype=torch.float32),\n              (),\n              \'real\'),\n             Gaussian(\n              torch.tensor([-0.0, -0.0, 0.0, 0.0], dtype=torch.float32),\n              torch.tensor([[98.01002502441406, 0.0, -99.0000228881836, -0.0], [0.0, 98.01002502441406, -0.0, -99.0000228881836], [-99.0000228881836, -0.0, 100.0000228881836, 0.0], [-0.0, -99.0000228881836, 0.0, 100.0000228881836]], dtype=torch.float32),  # noqa\n              ((\'state_b7\',\n                reals(2,),),\n               (\'state(time=1)_b8\',\n                reals(2,),),)),\n             Subs(\n              AffineNormal(\n               Tensor(\n                torch.tensor([[0.03488487750291824, 0.07356668263673782, 0.19946961104869843, 0.5386509299278259, -0.708323061466217, 0.24411526322364807, -0.20855577290058136, -0.2421337217092514], [0.41762110590934753, 0.5272183418273926, -0.49835553765296936, -0.0363837406039238, -0.0005282597267068923, 0.2704298794269562, -0.155222088098526, -0.44802337884902954]], dtype=torch.float32),  # noqa\n                (),\n                \'real\'),\n               Tensor(\n                torch.tensor([[-0.003566693514585495, -0.2848514914512634, 0.037103548645973206, 0.12648648023605347, -0.18501518666744232, -0.20899859070777893, 0.04121830314397812, 0.0054807960987091064], [0.0021788496524095535, -0.18700894713401794, 0.08187370002269745, 0.13554862141609192, -0.10477752983570099, -0.20848378539085388, -0.01393645629286766, 0.011670656502246857]], dtype=torch.float32),  # noqa\n                ((\'time_b9\',\n                  bint(2),),),\n                \'real\'),\n               Tensor(\n                torch.tensor([[0.5974780917167664, 0.864071786403656, 1.0236268043518066, 0.7147538065910339, 0.7423890233039856, 0.9462157487869263, 1.2132389545440674, 1.0596832036972046], [0.5787821412086487, 0.9178534150123596, 0.9074794054031372, 0.6600189208984375, 0.8473222255706787, 0.8426999449729919, 1.194266438484192, 1.0471148490905762]], dtype=torch.float32),  # noqa\n                ((\'time_b9\',\n                  bint(2),),),\n                \'real\'),\n               Variable(\'state(time=1)_b8\', reals(2,)),\n               Variable(\'gate_rate_b6\', reals(8,))),\n              ((\'gate_rate_b6\',\n                Binary(ops.GetitemOp(0),\n                 Variable(\'gate_rate_t\', reals(2, 8)),\n                 Variable(\'time_b9\', bint(2))),),)),)),\n           Variable(\'time_b9\', bint(2)),\n           frozenset({(\'state_b7\', \'state(time=1)_b8\')}),\n           frozenset({(\'state(time=1)_b8\', \'state(time=1)_b11\'), (\'state_b7\', \'state_b10\')})),  # noqa\n          Subs(\n           dist.MultivariateNormal(\n            Tensor(\n             torch.tensor([0.0, 0.0], dtype=torch.float32),\n             (),\n             \'real\'),\n            Tensor(\n             torch.tensor([[10.0, 0.0], [0.0, 10.0]], dtype=torch.float32),\n             (),\n             \'real\'),\n            Variable(\'value_b5\', reals(2,))),\n           ((\'value_b5\',\n             Variable(\'state_b10\', reals(2,)),),)),))\n        p_likelihood = Contraction(ops.add, ops.nullop,\n         frozenset({\'time_b17\', \'destin_b16\', \'origin_b15\'}),\n         (Contraction(ops.logaddexp, ops.add,\n           frozenset({\'gated_b14\'}),\n           (dist.Categorical(\n             Binary(ops.GetitemOp(0),\n              Binary(ops.GetitemOp(0),\n               Subs(\n                Function(unpack_gate_rate_0,\n                 reals(2, 2, 2),\n                 (Variable(\'gate_rate_b12\', reals(8,)),)),\n                ((\'gate_rate_b12\',\n                  Binary(ops.GetitemOp(0),\n                   Variable(\'gate_rate_t\', reals(2, 8)),\n                   Variable(\'time_b17\', bint(2))),),)),\n               Variable(\'origin_b15\', bint(2))),\n              Variable(\'destin_b16\', bint(2))),\n             Variable(\'gated_b14\', bint(2))),\n            Stack(\'gated_b14\',\n             (dist.Poisson(\n               Binary(ops.GetitemOp(0),\n                Binary(ops.GetitemOp(0),\n                 Subs(\n                  Function(unpack_gate_rate_1,\n                   reals(2, 2),\n                   (Variable(\'gate_rate_b13\', reals(8,)),)),\n                  ((\'gate_rate_b13\',\n                    Binary(ops.GetitemOp(0),\n                     Variable(\'gate_rate_t\', reals(2, 8)),\n                     Variable(\'time_b17\', bint(2))),),)),\n                 Variable(\'origin_b15\', bint(2))),\n                Variable(\'destin_b16\', bint(2))),\n               Tensor(\n                torch.tensor([[[1.0, 1.0], [5.0, 0.0]], [[0.0, 6.0], [19.0, 3.0]]], dtype=torch.float32),  # noqa\n                ((\'time_b17\',\n                  bint(2),),\n                 (\'origin_b15\',\n                  bint(2),),\n                 (\'destin_b16\',\n                  bint(2),),),\n                \'real\')),\n              dist.Delta(\n               Tensor(\n                torch.tensor(0.0, dtype=torch.float32),\n                (),\n                \'real\'),\n               Tensor(\n                torch.tensor(0.0, dtype=torch.float32),\n                (),\n                \'real\'),\n               Tensor(\n                torch.tensor([[[1.0, 1.0], [5.0, 0.0]], [[0.0, 6.0], [19.0, 3.0]]], dtype=torch.float32),  # noqa\n                ((\'time_b17\',\n                  bint(2),),\n                 (\'origin_b15\',\n                  bint(2),),\n                 (\'destin_b16\',\n                  bint(2),),),\n                \'real\')),)),)),))\n\n    if analytic_kl:\n        exact_part = funsor.Integrate(q, p_prior - q, ""gate_rate_t"")\n        with interpretation(monte_carlo):\n            approx_part = funsor.Integrate(q, p_likelihood, ""gate_rate_t"")\n        elbo = exact_part + approx_part\n    else:\n        p = p_prior + p_likelihood\n        with interpretation(monte_carlo):\n            elbo = Integrate(q, p - q, ""gate_rate_t"")\n\n    assert isinstance(elbo, Tensor), elbo.pretty()\n    assert call_count == 1\n'"
test/examples/test_sensor_fusion.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport pytest\nimport torch\n\nimport funsor.torch.distributions as dist\nimport funsor.ops as ops\nfrom funsor.cnf import Contraction\nfrom funsor.domains import bint, reals\nfrom funsor.gaussian import Gaussian\nfrom funsor.pyro.convert import dist_to_funsor, matrix_and_mvn_to_funsor\nfrom funsor.tensor import Tensor\nfrom funsor.terms import Subs, Variable\nfrom funsor.testing import random_mvn\n\n\n# This version constructs factors using funsor.torch.distributions.\n@pytest.mark.parametrize(\'state_dim,obs_dim\', [(3, 2), (2, 3)])\ndef test_distributions(state_dim, obs_dim):\n    data = Tensor(torch.randn(2, obs_dim))[""time""]\n\n    bias = Variable(""bias"", reals(obs_dim))\n    bias_dist = dist_to_funsor(random_mvn((), obs_dim))(value=bias)\n\n    prev = Variable(""prev"", reals(state_dim))\n    curr = Variable(""curr"", reals(state_dim))\n    trans_mat = Tensor(torch.eye(state_dim) + 0.1 * torch.randn(state_dim, state_dim))\n    trans_mvn = random_mvn((), state_dim)\n    trans_dist = dist.MultivariateNormal(\n        loc=trans_mvn.loc,\n        scale_tril=trans_mvn.scale_tril,\n        value=curr - prev @ trans_mat)\n\n    state = Variable(""state"", reals(state_dim))\n    obs = Variable(""obs"", reals(obs_dim))\n    obs_mat = Tensor(torch.randn(state_dim, obs_dim))\n    obs_mvn = random_mvn((), obs_dim)\n    obs_dist = dist.MultivariateNormal(\n        loc=obs_mvn.loc,\n        scale_tril=obs_mvn.scale_tril,\n        value=state @ obs_mat + bias - obs)\n\n    log_prob = 0\n    log_prob += bias_dist\n\n    state_0 = Variable(""state_0"", reals(state_dim))\n    log_prob += obs_dist(state=state_0, obs=data(time=0))\n\n    state_1 = Variable(""state_1"", reals(state_dim))\n    log_prob += trans_dist(prev=state_0, curr=state_1)\n    log_prob += obs_dist(state=state_1, obs=data(time=1))\n\n    log_prob = log_prob.reduce(ops.logaddexp)\n    assert isinstance(log_prob, Tensor), log_prob.pretty()\n\n\n# This version constructs factors using funsor.pyro.convert.\ndef test_pyro_convert():\n    data = Tensor(torch.randn(2, 2), OrderedDict([(""time"", bint(2))]))\n\n    bias_dist = dist_to_funsor(random_mvn((), 2))\n\n    trans_mat = torch.randn(3, 3)\n    trans_mvn = random_mvn((), 3)\n    trans = matrix_and_mvn_to_funsor(trans_mat, trans_mvn, (), ""prev"", ""curr"")\n\n    obs_mat = torch.randn(3, 2)\n    obs_mvn = random_mvn((), 2)\n    obs = matrix_and_mvn_to_funsor(obs_mat, obs_mvn, (), ""state"", ""obs"")\n\n    log_prob = 0\n    bias = Variable(""bias"", reals(2))\n    log_prob += bias_dist(value=bias)\n\n    state_0 = Variable(""state_0"", reals(3))\n    log_prob += obs(state=state_0, obs=bias + data(time=0))\n\n    state_1 = Variable(""state_1"", reals(3))\n    log_prob += trans(prev=state_0, curr=state_1)\n    log_prob += obs(state=state_1, obs=bias + data(time=1))\n\n    log_prob = log_prob.reduce(ops.logaddexp)\n    assert isinstance(log_prob, Tensor), log_prob.pretty()\n\n\ndef test_affine_subs():\n    # This was recorded from test_pyro_convert.\n    x = Subs(\n     Gaussian(\n      torch.tensor([1.3027106523513794, 1.4167094230651855, -0.9750942587852478, 0.5321089029312134, -0.9039931297302246], dtype=torch.float32),  # noqa\n      torch.tensor([[1.0199567079544067, 0.9840421676635742, -0.473368763923645, 0.34206756949424744, -0.7562517523765564], [0.9840421676635742, 1.511502742767334, -1.7593903541564941, 0.6647964119911194, -0.5119513273239136], [-0.4733688533306122, -1.7593903541564941, 3.2386727333068848, -0.9345928430557251, -0.1534711718559265], [0.34206756949424744, 0.6647964119911194, -0.9345928430557251, 0.3141004145145416, -0.12399007380008698], [-0.7562517523765564, -0.5119513273239136, -0.1534711718559265, -0.12399007380008698, 0.6450173854827881]], dtype=torch.float32),  # noqa\n      ((\'state_1_b6\',\n        reals(3,),),\n       (\'obs_b2\',\n        reals(2,),),)),\n     ((\'obs_b2\',\n       Contraction(ops.nullop, ops.add,\n        frozenset(),\n        (Variable(\'bias_b5\', reals(2,)),\n         Tensor(\n          torch.tensor([-2.1787893772125244, 0.5684312582015991], dtype=torch.float32),  # noqa\n          (),\n          \'real\'),)),),))\n    assert isinstance(x, (Gaussian, Contraction)), x.pretty()\n'"
test/pyro/conftest.py,0,b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro\n\n\ndef pytest_runtest_setup(item):\n    pyro.set_rng_seed(0)\n    pyro.enable_validation(True)\n'
test/pyro/test_convert.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import OrderedDict\n\nimport pyro.distributions as dist\nimport pytest\nimport torch\nfrom pyro.distributions.torch_distribution import MaskedDistribution\n\nfrom funsor.domains import bint, reals\nfrom funsor.pyro.convert import (\n    AffineNormal,\n    dist_to_funsor,\n    funsor_to_cat_and_mvn,\n    funsor_to_mvn,\n    funsor_to_tensor,\n    matrix_and_mvn_to_funsor,\n    mvn_to_funsor,\n    tensor_to_funsor\n)\nfrom funsor.tensor import Tensor\nfrom funsor.terms import Funsor, Variable\nfrom funsor.testing import assert_close, random_mvn, random_tensor\n\nEVENT_SHAPES = [(), (1,), (5,), (4, 3)]\nBATCH_SHAPES = [(), (1,), (4,), (2, 3), (1, 2, 1, 3, 1)]\nREAL_SIZES = [(1,), (1, 1), (1, 1, 1), (1, 2), (2, 1), (2, 3), (3, 1, 2)]\n\n\n@pytest.mark.parametrize(""event_shape,event_output"", [\n    (shape, size)\n    for shape in EVENT_SHAPES\n    for size in range(len(shape))\n], ids=str)\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_tensor_funsor_tensor(batch_shape, event_shape, event_output):\n    event_inputs = (""foo"", ""bar"", ""baz"")[:len(event_shape) - event_output]\n    t = torch.randn(batch_shape + event_shape)\n    f = tensor_to_funsor(t, event_inputs, event_output)\n    t2 = funsor_to_tensor(f, t.dim(), event_inputs)\n    assert_close(t2, t)\n\n\n@pytest.mark.parametrize(""event_sizes"", REAL_SIZES, ids=str)\n@pytest.mark.parametrize(""event_shape"", EVENT_SHAPES, ids=str)\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_mvn_to_funsor(batch_shape, event_shape, event_sizes):\n    event_size = sum(event_sizes)\n    mvn = random_mvn(batch_shape + event_shape, event_size)\n    int_inputs = OrderedDict((k, bint(size)) for k, size in zip(""abc"", event_shape))\n    real_inputs = OrderedDict((k, reals(size)) for k, size in zip(""xyz"", event_sizes))\n\n    f = mvn_to_funsor(mvn, tuple(int_inputs), real_inputs)\n    assert isinstance(f, Funsor)\n    for k, d in int_inputs.items():\n        if d.num_elements == 1:\n            assert d not in f.inputs\n        else:\n            assert k in f.inputs\n            assert f.inputs[k] == d\n    for k, d in real_inputs.items():\n        assert k in f.inputs\n        assert f.inputs[k] == d\n\n    value = mvn.sample()\n    subs = {}\n    beg = 0\n    for k, d in real_inputs.items():\n        end = beg + d.num_elements\n        subs[k] = tensor_to_funsor(value[..., beg:end], tuple(int_inputs), 1)\n        beg = end\n    actual_log_prob = f(**subs)\n    expected_log_prob = tensor_to_funsor(mvn.log_prob(value), tuple(int_inputs))\n    assert_close(actual_log_prob, expected_log_prob, atol=1e-5, rtol=1e-5)\n\n\n@pytest.mark.parametrize(""x_size"", [1, 2])\n@pytest.mark.parametrize(""y_size"", [1, 3])\n@pytest.mark.parametrize(""matrix_shape,loc_shape,scale_shape,x_shape,y_shape"", [\n    ((), (), (), (), ()),\n    ((4,), (4,), (4,), (4,), (4,)),\n    ((4, 5), (4, 5), (4, 5), (4, 5), (4, 5)),\n    ((4,), (), (), (), ()),\n    ((), (4,), (), (), ()),\n    ((), (), (4,), (), ()),\n    ((), (), (), (4,), ()),\n    ((), (), (), (), (4,)),\n], ids=str)\ndef test_affine_normal(matrix_shape, loc_shape, scale_shape, x_shape, y_shape,\n                       x_size, y_size):\n\n    def _rand(batch_shape, *event_shape):\n        inputs = OrderedDict(zip(""abcdef"", map(bint, reversed(batch_shape))))\n        return random_tensor(inputs, reals(*event_shape))\n\n    matrix = _rand(matrix_shape, x_size, y_size)\n    loc = _rand(loc_shape, y_size)\n    scale = _rand(scale_shape, y_size).exp()\n    value_x = _rand(x_shape, x_size)\n    value_y = _rand(y_shape, y_size)\n\n    f = AffineNormal(matrix, loc, scale,\n                     Variable(""x"", reals(x_size)),\n                     Variable(""y"", reals(y_size)))\n    assert isinstance(f, AffineNormal)\n\n    # Evaluate via two different patterns.\n    expected = f(x=value_x)(y=value_y)\n    actual = f(y=value_y)(x=value_x)\n    assert_close(actual, expected, atol=1e-5, rtol=2e-4)\n\n\n@pytest.mark.parametrize(""x_size"", [1, 2, 3])\n@pytest.mark.parametrize(""y_size"", [1, 2, 3])\n@pytest.mark.parametrize(""event_shape"", EVENT_SHAPES, ids=str)\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_matrix_and_mvn_to_funsor(batch_shape, event_shape, x_size, y_size):\n    matrix = torch.randn(batch_shape + event_shape + (x_size, y_size))\n    y_mvn = random_mvn(batch_shape + event_shape, y_size)\n    xy_mvn = random_mvn(batch_shape + event_shape, x_size + y_size)\n    int_inputs = OrderedDict((k, bint(size)) for k, size in zip(""abc"", event_shape))\n    real_inputs = OrderedDict([(""x"", reals(x_size)), (""y"", reals(y_size))])\n\n    f = (matrix_and_mvn_to_funsor(matrix, y_mvn, tuple(int_inputs), ""x"", ""y"") +\n         mvn_to_funsor(xy_mvn, tuple(int_inputs), real_inputs))\n    assert isinstance(f, Funsor)\n    for k, d in int_inputs.items():\n        if d.num_elements == 1:\n            assert d not in f.inputs\n        else:\n            assert k in f.inputs\n            assert f.inputs[k] == d\n    assert f.inputs[""x""] == reals(x_size)\n    assert f.inputs[""y""] == reals(y_size)\n\n    xy = torch.randn(x_size + y_size)\n    x, y = xy[:x_size], xy[x_size:]\n    y_pred = x.unsqueeze(-2).matmul(matrix).squeeze(-2)\n    actual_log_prob = f(x=x, y=y)\n    expected_log_prob = tensor_to_funsor(\n        xy_mvn.log_prob(xy) + y_mvn.log_prob(y - y_pred), tuple(int_inputs))\n    assert_close(actual_log_prob, expected_log_prob, atol=1e-4, rtol=1e-4)\n\n\n@pytest.mark.parametrize(""batch_shape"", [(), (4,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""x_size"", [1, 2, 3])\n@pytest.mark.parametrize(""y_size"", [1, 2, 3])\ndef test_matrix_and_mvn_to_funsor_diag(batch_shape, x_size, y_size):\n    matrix = torch.randn(batch_shape + (x_size, y_size))\n    loc = torch.randn(batch_shape + (y_size,))\n    scale = torch.randn(batch_shape + (y_size,)).exp()\n\n    normal = dist.Normal(loc, scale).to_event(1)\n    actual = matrix_and_mvn_to_funsor(matrix, normal)\n    assert isinstance(actual, AffineNormal)\n\n    mvn = dist.MultivariateNormal(loc, scale_tril=scale.diag_embed())\n    expected = matrix_and_mvn_to_funsor(matrix, mvn)\n\n    y = tensor_to_funsor(torch.randn(batch_shape + (y_size,)), (), 1)\n    actual_like = actual(value_y=y)\n    expected_like = expected(value_y=y)\n    assert_close(actual_like, expected_like, atol=1e-4, rtol=1e-4)\n\n    x = tensor_to_funsor(torch.randn(batch_shape + (x_size,)), (), 1)\n    actual_norm = actual_like(value_x=x)\n    expected_norm = expected_like(value_x=x)\n    assert_close(actual_norm, expected_norm, atol=1e-4, rtol=1e-4)\n\n\n@pytest.mark.parametrize(""real_size"", [1, 2, 3])\n@pytest.mark.parametrize(""event_shape"", EVENT_SHAPES, ids=str)\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_funsor_to_mvn(batch_shape, event_shape, real_size):\n    expected = random_mvn(batch_shape + event_shape, real_size)\n    event_dims = tuple(""abc""[:len(event_shape)])\n    ndims = len(expected.batch_shape)\n\n    funsor_ = dist_to_funsor(expected, event_dims)(value=""value"")\n    assert isinstance(funsor_, Funsor)\n\n    actual = funsor_to_mvn(funsor_, ndims, event_dims)\n    assert isinstance(actual, dist.MultivariateNormal)\n    assert actual.batch_shape == expected.batch_shape\n    assert_close(actual.loc, expected.loc, atol=1e-3, rtol=None)\n    assert_close(actual.precision_matrix,\n                 expected.precision_matrix, atol=1e-3, rtol=None)\n\n\n@pytest.mark.parametrize(""int_size"", [2, 3])\n@pytest.mark.parametrize(""real_size"", [1, 2, 3])\n@pytest.mark.parametrize(""event_shape"", EVENT_SHAPES, ids=str)\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_funsor_to_cat_and_mvn(batch_shape, event_shape, int_size, real_size):\n    logits = torch.randn(batch_shape + event_shape + (int_size,))\n    expected_cat = dist.Categorical(logits=logits)\n    expected_mvn = random_mvn(batch_shape + event_shape + (int_size,), real_size)\n    event_dims = tuple(""abc""[:len(event_shape)]) + (""component"",)\n    ndims = len(expected_cat.batch_shape)\n\n    funsor_ = (tensor_to_funsor(logits, event_dims) +\n               dist_to_funsor(expected_mvn, event_dims)(value=""value""))\n    assert isinstance(funsor_, Funsor)\n\n    actual_cat, actual_mvn = funsor_to_cat_and_mvn(funsor_, ndims, event_dims)\n    assert isinstance(actual_cat, dist.Categorical)\n    assert isinstance(actual_mvn, dist.MultivariateNormal)\n    assert actual_cat.batch_shape == expected_cat.batch_shape\n    assert actual_mvn.batch_shape == expected_mvn.batch_shape\n    assert_close(actual_cat.logits, expected_cat.logits, atol=1e-4, rtol=None)\n    assert_close(actual_mvn.loc, expected_mvn.loc, atol=1e-4, rtol=None)\n    assert_close(actual_mvn.precision_matrix,\n                 expected_mvn.precision_matrix, atol=1e-4, rtol=None)\n\n\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\n@pytest.mark.parametrize(""cardinality"", [2, 3, 5])\ndef test_dist_to_funsor_categorical(batch_shape, cardinality):\n    logits = torch.randn(batch_shape + (cardinality,))\n    logits -= logits.logsumexp(dim=-1, keepdim=True)\n    d = dist.Categorical(logits=logits)\n    f = dist_to_funsor(d)\n    assert isinstance(f, Tensor)\n    expected = tensor_to_funsor(logits, (""value"",))\n    assert_close(f, expected)\n\n\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_dist_to_funsor_bernoulli(batch_shape):\n    logits = torch.randn(batch_shape)\n    d = dist.Bernoulli(logits=logits)\n    f = dist_to_funsor(d)\n    assert isinstance(f, Funsor)\n\n    value = d.sample()\n    actual_log_prob = f(value=tensor_to_funsor(value))\n    expected_log_prob = tensor_to_funsor(d.log_prob(value))\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_dist_to_funsor_normal(batch_shape):\n    loc = torch.randn(batch_shape)\n    scale = torch.randn(batch_shape).exp()\n    d = dist.Normal(loc, scale)\n    f = dist_to_funsor(d)\n    assert isinstance(f, Funsor)\n\n    value = d.sample()\n    actual_log_prob = f(value=tensor_to_funsor(value))\n    expected_log_prob = tensor_to_funsor(d.log_prob(value))\n    assert_close(actual_log_prob, expected_log_prob, rtol=1e-5)\n\n\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\n@pytest.mark.parametrize(""event_size"", [2, 3, 5])\ndef test_dist_to_funsor_mvn(batch_shape, event_size):\n    loc = torch.randn(batch_shape + (event_size,))\n    cov = torch.randn(batch_shape + (event_size, 2 * event_size))\n    cov = cov.matmul(cov.transpose(-1, -2))\n    scale_tril = torch.cholesky(cov)\n    d = dist.MultivariateNormal(loc, scale_tril=scale_tril)\n    f = dist_to_funsor(d)\n    assert isinstance(f, Funsor)\n\n    value = d.sample()\n    actual_log_prob = f(value=tensor_to_funsor(value, event_output=1))\n    expected_log_prob = tensor_to_funsor(d.log_prob(value))\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(""event_shape"", [(), (6,), (3, 2)], ids=str)\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_dist_to_funsor_independent(batch_shape, event_shape):\n    loc = torch.randn(batch_shape + event_shape)\n    scale = torch.randn(batch_shape + event_shape).exp()\n    d = dist.Normal(loc, scale).to_event(len(event_shape))\n    f = dist_to_funsor(d)\n    assert isinstance(f, Funsor)\n\n    value = d.sample()\n    funsor_value = tensor_to_funsor(value, event_output=len(event_shape))\n    actual_log_prob = f(value=funsor_value)\n    expected_log_prob = tensor_to_funsor(d.log_prob(value))\n    assert_close(actual_log_prob, expected_log_prob, rtol=1e-5)\n\n\n@pytest.mark.parametrize(""batch_shape"", BATCH_SHAPES, ids=str)\ndef test_dist_to_funsor_masked(batch_shape):\n    loc = torch.randn(batch_shape)\n    scale = torch.randn(batch_shape).exp()\n    mask = torch.bernoulli(torch.full(batch_shape, 0.5)).byte()\n    d = dist.Normal(loc, scale).mask(mask)\n    assert isinstance(d, MaskedDistribution)\n    f = dist_to_funsor(d)\n    assert isinstance(f, Funsor)\n\n    value = d.sample()\n    actual_log_prob = f(value=tensor_to_funsor(value))\n    expected_log_prob = tensor_to_funsor(d.log_prob(value))\n    assert_close(actual_log_prob, expected_log_prob)\n'"
test/pyro/test_distribution.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro.distributions as dist\nimport pytest\nimport torch\n\nfrom funsor.pyro.convert import tensor_to_funsor\nfrom funsor.pyro.distribution import FunsorDistribution\nfrom funsor.testing import assert_close\n\nSHAPES = [(), (1,), (4,), (2, 3), (1, 2, 1, 3, 1)]\n\n\nclass Categorical(FunsorDistribution):\n    def __init__(self, logits, validate_args=None):\n        batch_shape = logits.shape[:-1]\n        event_shape = torch.Size()\n        funsor_dist = tensor_to_funsor(logits, (""value"",))\n        dtype = int(logits.size(-1))\n        super(Categorical, self).__init__(\n            funsor_dist, batch_shape, event_shape, dtype, validate_args)\n\n\n@pytest.mark.parametrize(""cardinality"", [2, 3])\n@pytest.mark.parametrize(""sample_shape"", SHAPES, ids=str)\n@pytest.mark.parametrize(""batch_shape"", SHAPES, ids=str)\ndef test_categorical_log_prob(sample_shape, batch_shape, cardinality):\n    logits = torch.randn(batch_shape + (cardinality,))\n    logits -= logits.logsumexp(dim=-1, keepdim=True)\n    actual = Categorical(logits=logits)\n    expected = dist.Categorical(logits=logits)\n    assert actual.batch_shape == expected.batch_shape\n    assert actual.event_shape == expected.event_shape\n\n    value = expected.sample(sample_shape)\n    actual_log_prob = actual.log_prob(value)\n    expected_log_prob = expected.log_prob(value)\n    assert_close(actual_log_prob, expected_log_prob)\n\n\n@pytest.mark.parametrize(""cardinality"", [2, 3])\n@pytest.mark.parametrize(""sample_shape"", SHAPES, ids=str)\n@pytest.mark.parametrize(""batch_shape"", SHAPES, ids=str)\ndef test_categorical_sample(sample_shape, batch_shape, cardinality):\n    logits = torch.randn(batch_shape + (cardinality,))\n    logits -= logits.logsumexp(dim=-1, keepdim=True)\n    actual = Categorical(logits=logits)\n    expected = dist.Categorical(logits=logits)\n    assert actual.batch_shape == expected.batch_shape\n    assert actual.event_shape == expected.event_shape\n\n    actual_sample = actual.sample(sample_shape)\n    expected_sample = expected.sample(sample_shape)\n    assert actual_sample.dtype == expected_sample.dtype\n    assert actual_sample.shape == expected_sample.shape\n    expected.log_prob(actual_sample)  # validates sample\n'"
test/pyro/test_hmm.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pyro.distributions as dist\nimport pytest\nimport torch\nfrom pyro.distributions.util import broadcast_shape\n\nfrom funsor.pyro.hmm import DiscreteHMM, GaussianHMM, GaussianMRF, SwitchingLinearHMM\nfrom funsor.testing import assert_close, random_mvn\n\n\ndef check_expand(old_dist, old_data):\n    new_batch_shape = (2,) + old_dist.batch_shape\n    new_dist = old_dist.expand(new_batch_shape)\n    assert new_dist.batch_shape == new_batch_shape\n\n    old_log_prob = new_dist.log_prob(old_data)\n    assert old_log_prob.shape == new_batch_shape\n\n    new_data = old_data.expand(new_batch_shape + new_dist.event_shape)\n    new_log_prob = new_dist.log_prob(new_data)\n    assert_close(old_log_prob, new_log_prob)\n    assert new_dist.log_prob(new_data).shape == new_batch_shape\n\n\nDISCRETE_HMM_SHAPES = [\n    # init_shape, trans_shape, obs_shape\n    ((), (1,), ()),\n    ((), (), (1,)),\n    ((), (2,), ()),\n    ((), (7,), ()),\n    ((), (), (7,)),\n    ((), (7,), (1,)),\n    ((), (1,), (7,)),\n    ((), (7,), (5, 7)),\n    ((), (5, 7), (7,)),\n    ((), (5, 7), (5, 7)),\n    ((5,), (7,), (7,)),\n    ((5,), (7,), (5, 7)),\n    ((5,), (5, 7), (7,)),\n    ((5,), (5, 7), (5, 7)),\n    ((4, 1, 1), (3, 1, 7), (2, 7)),\n]\n\n\n@pytest.mark.parametrize(""state_dim"", [2, 3])\n@pytest.mark.parametrize(""init_shape,trans_shape,obs_shape"", DISCRETE_HMM_SHAPES, ids=str)\ndef test_discrete_categorical_log_prob(init_shape, trans_shape, obs_shape, state_dim):\n    obs_dim = 4\n    init_logits = torch.randn(init_shape + (state_dim,))\n    trans_logits = torch.randn(trans_shape + (state_dim, state_dim))\n    obs_logits = torch.randn(obs_shape + (state_dim, obs_dim))\n    obs_dist = dist.Categorical(logits=obs_logits)\n\n    actual_dist = DiscreteHMM(init_logits, trans_logits, obs_dist)\n    expected_dist = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n    assert actual_dist.event_shape == expected_dist.event_shape\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n\n    batch_shape = broadcast_shape(init_shape + (1,), trans_shape, obs_shape)\n    data = obs_dist.expand(batch_shape + (state_dim,)).sample()\n    data = data[(slice(None),) * len(batch_shape) + (0,)]\n    actual_log_prob = actual_dist.log_prob(data)\n    expected_log_prob = expected_dist.log_prob(data)\n    assert_close(actual_log_prob, expected_log_prob)\n    check_expand(actual_dist, data)\n\n\n@pytest.mark.parametrize(""state_dim"", [2, 3])\n@pytest.mark.parametrize(""init_shape,trans_shape,obs_shape"", DISCRETE_HMM_SHAPES, ids=str)\ndef test_discrete_normal_log_prob(init_shape, trans_shape, obs_shape, state_dim):\n    init_logits = torch.randn(init_shape + (state_dim,))\n    trans_logits = torch.randn(trans_shape + (state_dim, state_dim))\n    loc = torch.randn(obs_shape + (state_dim,))\n    scale = torch.randn(obs_shape + (state_dim,)).exp()\n    obs_dist = dist.Normal(loc, scale)\n\n    actual_dist = DiscreteHMM(init_logits, trans_logits, obs_dist)\n    expected_dist = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n    assert actual_dist.event_shape == expected_dist.event_shape\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n\n    batch_shape = broadcast_shape(init_shape + (1,), trans_shape, obs_shape)\n    data = obs_dist.expand(batch_shape + (state_dim,)).sample()\n    data = data[(slice(None),) * len(batch_shape) + (0,)]\n    actual_log_prob = actual_dist.log_prob(data)\n    expected_log_prob = expected_dist.log_prob(data)\n    assert_close(actual_log_prob, expected_log_prob, rtol=5e-5)\n    check_expand(actual_dist, data)\n\n\n@pytest.mark.parametrize(""state_dim"", [2, 3])\n@pytest.mark.parametrize(""init_shape,trans_shape,obs_shape"", DISCRETE_HMM_SHAPES, ids=str)\ndef test_discrete_mvn_log_prob(init_shape, trans_shape, obs_shape, state_dim):\n    event_size = 4\n    init_logits = torch.randn(init_shape + (state_dim,))\n    trans_logits = torch.randn(trans_shape + (state_dim, state_dim))\n    loc = torch.randn(obs_shape + (state_dim, event_size))\n    cov = torch.randn(obs_shape + (state_dim, event_size, 2 * event_size))\n    cov = cov.matmul(cov.transpose(-1, -2))\n    scale_tril = torch.cholesky(cov)\n    obs_dist = dist.MultivariateNormal(loc, scale_tril=scale_tril)\n\n    actual_dist = DiscreteHMM(init_logits, trans_logits, obs_dist)\n    expected_dist = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n    assert actual_dist.event_shape == expected_dist.event_shape\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n\n    batch_shape = broadcast_shape(init_shape + (1,), trans_shape, obs_shape)\n    data = obs_dist.expand(batch_shape + (state_dim,)).sample()\n    data = data[(slice(None),) * len(batch_shape) + (0,)]\n    actual_log_prob = actual_dist.log_prob(data)\n    expected_log_prob = expected_dist.log_prob(data)\n    assert_close(actual_log_prob, expected_log_prob)\n    check_expand(actual_dist, data)\n\n\n@pytest.mark.parametrize(""state_dim"", [2, 3])\n@pytest.mark.parametrize(""init_shape,trans_shape,obs_shape"", DISCRETE_HMM_SHAPES, ids=str)\ndef test_discrete_diag_normal_log_prob(init_shape, trans_shape, obs_shape, state_dim):\n    event_size = 4\n    init_logits = torch.randn(init_shape + (state_dim,))\n    trans_logits = torch.randn(trans_shape + (state_dim, state_dim))\n    loc = torch.randn(obs_shape + (state_dim, event_size))\n    scale = torch.randn(obs_shape + (state_dim, event_size)).exp()\n    obs_dist = dist.Normal(loc, scale).to_event(1)\n\n    actual_dist = DiscreteHMM(init_logits, trans_logits, obs_dist)\n    expected_dist = dist.DiscreteHMM(init_logits, trans_logits, obs_dist)\n    assert actual_dist.event_shape == expected_dist.event_shape\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n\n    batch_shape = broadcast_shape(init_shape + (1,), trans_shape, obs_shape)\n    data = obs_dist.expand(batch_shape + (state_dim,)).sample()\n    data = data[(slice(None),) * len(batch_shape) + (0,)]\n    actual_log_prob = actual_dist.log_prob(data)\n    expected_log_prob = expected_dist.log_prob(data)\n    assert_close(actual_log_prob, expected_log_prob, atol=1e-5, rtol=1e-5)\n    check_expand(actual_dist, data)\n\n\n@pytest.mark.parametrize(""obs_dim,hidden_dim"",\n                         [(1, 1), (1, 2), (2, 1), (2, 2), (2, 3), (3, 2)])\n@pytest.mark.parametrize(""init_shape,trans_mat_shape,trans_mvn_shape,obs_mat_shape,obs_mvn_shape"", [\n    ((), (), (), (), ()),\n    ((), (6,), (), (), ()),\n    ((), (), (6,), (), ()),\n    ((), (), (), (6,), ()),\n    ((), (), (), (), (6,)),\n    ((), (6,), (6,), (6,), (6,)),\n    ((5,), (6,), (), (), ()),\n    ((), (5, 1), (6,), (), ()),\n    ((), (), (5, 1), (6,), ()),\n    ((), (), (), (5, 1), (6,)),\n    ((), (6,), (5, 1), (), ()),\n    ((), (), (6,), (5, 1), ()),\n    ((), (), (), (6,), (5, 1)),\n    ((5,), (), (), (), (6,)),\n    ((5,), (5, 6), (5, 6), (5, 6), (5, 6)),\n], ids=str)\ndef test_gaussian_hmm_log_prob(init_shape, trans_mat_shape, trans_mvn_shape,\n                               obs_mat_shape, obs_mvn_shape, hidden_dim, obs_dim):\n    init_dist = random_mvn(init_shape, hidden_dim)\n    trans_mat = torch.randn(trans_mat_shape + (hidden_dim, hidden_dim))\n    trans_dist = random_mvn(trans_mvn_shape, hidden_dim)\n    obs_mat = torch.randn(obs_mat_shape + (hidden_dim, obs_dim))\n    obs_dist = random_mvn(obs_mvn_shape, obs_dim)\n\n    actual_dist = GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n    expected_dist = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n    assert actual_dist.event_shape == expected_dist.event_shape\n\n    shape = broadcast_shape(init_shape + (1,),\n                            trans_mat_shape, trans_mvn_shape,\n                            obs_mat_shape, obs_mvn_shape)\n    data = obs_dist.expand(shape).sample()\n    assert data.shape == actual_dist.shape()\n\n    actual_log_prob = actual_dist.log_prob(data)\n    expected_log_prob = expected_dist.log_prob(data)\n    assert_close(actual_log_prob, expected_log_prob, atol=1e-5, rtol=1e-5)\n    check_expand(actual_dist, data)\n\n\n@pytest.mark.parametrize(""obs_dim"", [1, 2, 3])\n@pytest.mark.parametrize(""hidden_dim"", [1, 2, 3])\n@pytest.mark.parametrize(""init_shape,trans_shape,obs_shape"", DISCRETE_HMM_SHAPES, ids=str)\ndef test_gaussian_mrf_log_prob(init_shape, trans_shape, obs_shape, hidden_dim, obs_dim):\n    init_dist = random_mvn(init_shape, hidden_dim)\n    trans_dist = random_mvn(trans_shape, hidden_dim + hidden_dim)\n    obs_dist = random_mvn(obs_shape, hidden_dim + obs_dim)\n\n    actual_dist = GaussianMRF(init_dist, trans_dist, obs_dist)\n    expected_dist = dist.GaussianMRF(init_dist, trans_dist, obs_dist)\n    assert actual_dist.event_shape == expected_dist.event_shape\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n\n    batch_shape = broadcast_shape(init_shape + (1,), trans_shape, obs_shape)\n    data = obs_dist.expand(batch_shape).sample()[..., hidden_dim:]\n    actual_log_prob = actual_dist.log_prob(data)\n    expected_log_prob = expected_dist.log_prob(data)\n    assert_close(actual_log_prob, expected_log_prob, atol=1e-4, rtol=1e-4)\n    check_expand(actual_dist, data)\n\n\nSLHMM_SCHEMA = "","".join([\n    ""init_cat_shape"", ""init_mvn_shape"",\n    ""trans_cat_shape"", ""trans_mat_shape"", ""trans_mvn_shape"",\n    ""obs_mat_shape"", ""obs_mvn_shape"",\n])\nSLHMM_SHAPES = [\n    ((2,), (), (1, 2,), (1, 3, 3), (1,), (1, 3, 4), (1,)),\n    ((2,), (), (5, 1, 2,), (1, 3, 3), (1,), (1, 3, 4), (1,)),\n    ((2,), (), (1, 2,), (5, 1, 3, 3), (1,), (1, 3, 4), (1,)),\n    ((2,), (), (1, 2,), (1, 3, 3), (5, 1), (1, 3, 4), (1,)),\n    ((2,), (), (1, 2,), (1, 3, 3), (1,), (5, 1, 3, 4), (1,)),\n    ((2,), (), (1, 2,), (1, 3, 3), (1,), (1, 3, 4), (5, 1)),\n    ((2,), (), (5, 1, 2,), (5, 1, 3, 3), (5, 1), (5, 1, 3, 4), (5, 1)),\n    ((2,), (2,), (5, 2, 2,), (5, 2, 3, 3), (5, 2), (5, 2, 3, 4), (5, 2)),\n    ((7, 2,), (), (7, 5, 1, 2,), (7, 5, 1, 3, 3), (7, 5, 1), (7, 5, 1, 3, 4), (7, 5, 1)),\n    ((7, 2,), (7, 2), (7, 5, 2, 2,), (7, 5, 2, 3, 3), (7, 5, 2), (7, 5, 2, 3, 4), (7, 5, 2)),\n]\n\n\n@pytest.mark.parametrize(SLHMM_SCHEMA, SLHMM_SHAPES, ids=str)\ndef test_switching_linear_hmm_shape(init_cat_shape, init_mvn_shape,\n                                    trans_cat_shape, trans_mat_shape, trans_mvn_shape,\n                                    obs_mat_shape, obs_mvn_shape):\n    hidden_dim, obs_dim = obs_mat_shape[-2:]\n    assert trans_mat_shape[-2:] == (hidden_dim, hidden_dim)\n\n    init_logits = torch.randn(init_cat_shape)\n    init_mvn = random_mvn(init_mvn_shape, hidden_dim)\n    trans_logits = torch.randn(trans_cat_shape)\n    trans_matrix = torch.randn(trans_mat_shape)\n    trans_mvn = random_mvn(trans_mvn_shape, hidden_dim)\n    obs_matrix = torch.randn(obs_mat_shape)\n    obs_mvn = random_mvn(obs_mvn_shape, obs_dim)\n\n    init_shape = broadcast_shape(init_cat_shape, init_mvn_shape)\n    shape = broadcast_shape(init_shape[:-1] + (1, init_shape[-1]),\n                            trans_cat_shape[:-1],\n                            trans_mat_shape[:-2],\n                            trans_mvn_shape,\n                            obs_mat_shape[:-2],\n                            obs_mvn_shape)\n    expected_batch_shape, time_shape = shape[:-2], shape[-2:-1]\n    expected_event_shape = time_shape + (obs_dim,)\n\n    actual_dist = SwitchingLinearHMM(init_logits, init_mvn,\n                                     trans_logits, trans_matrix, trans_mvn,\n                                     obs_matrix, obs_mvn)\n    assert actual_dist.event_shape == expected_event_shape\n    assert actual_dist.batch_shape == expected_batch_shape\n\n    data = obs_mvn.expand(shape).sample()[..., 0, :]\n    actual_log_prob = actual_dist.log_prob(data)\n    assert actual_log_prob.shape == expected_batch_shape\n    check_expand(actual_dist, data)\n\n    final_cat, final_mvn = actual_dist.filter(data)\n    assert isinstance(final_cat, dist.Categorical)\n    assert isinstance(final_mvn, dist.MultivariateNormal)\n    assert final_cat.batch_shape == actual_dist.batch_shape\n    assert final_mvn.batch_shape == actual_dist.batch_shape + final_cat.logits.shape[-1:]\n\n\n@pytest.mark.parametrize(""num_components"", [2, 3])\n@pytest.mark.parametrize(""obs_dim,hidden_dim"",\n                         [(1, 1), (1, 2), (2, 1), (2, 2), (2, 3), (3, 2)])\n@pytest.mark.parametrize(""num_steps"", [1, 2, 3, 4, 5, 6])\n@pytest.mark.parametrize(""exact"", [True, False], ids=[""exact"", ""approx""])\ndef test_switching_linear_hmm_log_prob(exact, num_steps, hidden_dim, obs_dim, num_components):\n    # This tests agreement between an SLDS and an HMM when all components\n    # are identical, i.e. so latent can be marginalized out.\n    torch.manual_seed(2)\n    init_logits = torch.rand(num_components)\n    init_mvn = random_mvn((), hidden_dim)\n    trans_logits = torch.rand(num_components)\n    trans_matrix = torch.randn(hidden_dim, hidden_dim)\n    trans_mvn = random_mvn((), hidden_dim)\n    obs_matrix = torch.randn(hidden_dim, obs_dim)\n    obs_mvn = random_mvn((), obs_dim)\n\n    expected_dist = GaussianHMM(init_mvn,\n                                trans_matrix.expand(num_steps, -1, -1),\n                                trans_mvn, obs_matrix, obs_mvn)\n    actual_dist = SwitchingLinearHMM(init_logits, init_mvn, trans_logits,\n                                     trans_matrix.expand(num_steps, num_components, -1, -1),\n                                     trans_mvn, obs_matrix, obs_mvn,\n                                     exact=exact)\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n    assert actual_dist.event_shape == expected_dist.event_shape\n\n    data = obs_mvn.sample(expected_dist.batch_shape + (num_steps,))\n    assert data.shape == expected_dist.shape()\n    expected_log_prob = expected_dist.log_prob(data)\n    assert expected_log_prob.shape == expected_dist.batch_shape\n    actual_log_prob = actual_dist.log_prob(data)\n    assert_close(actual_log_prob, expected_log_prob, atol=1e-4, rtol=None)\n\n\n@pytest.mark.parametrize(""num_steps"", [2, 3])\n@pytest.mark.parametrize(""num_components"", [2, 3])\n@pytest.mark.parametrize(""exact"", [True, False], ids=[""exact"", ""approx""])\ndef test_switching_linear_hmm_log_prob_alternating(exact, num_steps, num_components):\n    # This tests agreement between an SLDS and an HMM in the case that the two\n    # SLDS discrete states alternate back and forth between 0 and 1 deterministically\n\n    torch.manual_seed(0)\n\n    hidden_dim = 4\n    obs_dim = 3\n    extra_components = num_components - 2\n\n    init_logits = torch.tensor([float(""-inf""), 0.0] + extra_components * [float(""-inf"")])\n    init_mvn = random_mvn((num_components,), hidden_dim)\n\n    left_logits = torch.tensor([0.0, float(""-inf"")] + extra_components * [float(""-inf"")])\n    right_logits = torch.tensor([float(""-inf""), 0.0] + extra_components * [float(""-inf"")])\n    trans_logits = torch.stack([left_logits if t % 2 == 0 else right_logits for t in range(num_steps)])\n    trans_logits = trans_logits.unsqueeze(-2)\n\n    hmm_trans_matrix = torch.randn(num_steps, hidden_dim, hidden_dim)\n    switching_trans_matrix = hmm_trans_matrix.unsqueeze(-3).expand(-1, num_components, -1, -1)\n\n    trans_mvn = random_mvn((num_steps, num_components,), hidden_dim)\n    hmm_obs_matrix = torch.randn(num_steps, hidden_dim, obs_dim)\n    switching_obs_matrix = hmm_obs_matrix.unsqueeze(-3).expand(-1, num_components, -1, -1)\n    obs_mvn = random_mvn((num_steps, num_components), obs_dim)\n\n    hmm_trans_mvn_loc = torch.empty(num_steps, hidden_dim)\n    hmm_trans_mvn_cov = torch.empty(num_steps, hidden_dim, hidden_dim)\n    hmm_obs_mvn_loc = torch.empty(num_steps, obs_dim)\n    hmm_obs_mvn_cov = torch.empty(num_steps, obs_dim, obs_dim)\n\n    for t in range(num_steps):\n        # select relevant bits for hmm given deterministic dynamics in discrete space\n        s = t % 2  # 0, 1, 0, 1, ...\n        hmm_trans_mvn_loc[t] = trans_mvn.loc[t, s]\n        hmm_trans_mvn_cov[t] = trans_mvn.covariance_matrix[t, s]\n        hmm_obs_mvn_loc[t] = obs_mvn.loc[t, s]\n        hmm_obs_mvn_cov[t] = obs_mvn.covariance_matrix[t, s]\n\n        # scramble matrices in places that should never be accessed given deterministic dynamics in discrete space\n        s = 1 - (t % 2)  # 1, 0, 1, 0, ...\n        switching_trans_matrix[t, s, :, :] = torch.rand(hidden_dim, hidden_dim)\n        switching_obs_matrix[t, s, :, :] = torch.rand(hidden_dim, obs_dim)\n\n    expected_dist = GaussianHMM(dist.MultivariateNormal(init_mvn.loc[1], init_mvn.covariance_matrix[1]),\n                                hmm_trans_matrix,\n                                dist.MultivariateNormal(hmm_trans_mvn_loc, hmm_trans_mvn_cov),\n                                hmm_obs_matrix,\n                                dist.MultivariateNormal(hmm_obs_mvn_loc, hmm_obs_mvn_cov))\n\n    actual_dist = SwitchingLinearHMM(init_logits, init_mvn, trans_logits,\n                                     switching_trans_matrix,\n                                     trans_mvn, switching_obs_matrix, obs_mvn,\n                                     exact=exact)\n\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n    assert actual_dist.event_shape == expected_dist.event_shape\n\n    data = obs_mvn.sample()[:, 0, :]\n    assert data.shape == expected_dist.shape()\n    expected_log_prob = expected_dist.log_prob(data)\n    assert expected_log_prob.shape == expected_dist.batch_shape\n    actual_log_prob = actual_dist.log_prob(data)\n    assert_close(actual_log_prob, expected_log_prob, atol=1e-2, rtol=None)\n\n\n@pytest.mark.parametrize(""hidden_dim"", [2, 3])\n@pytest.mark.parametrize(""init_shape,trans_mat_shape,trans_mvn_shape,obs_mvn_shape"", [\n    ((), (), (), ()),\n    ((), (6,), (), ()),\n    ((), (), (6,), ()),\n    ((), (), (), ()),\n    ((), (), (), (6,)),\n    ((5,), (6,), (), ()),\n    ((), (5, 1), (6,), ()),\n    ((), (), (), (6,)),\n    ((), (6,), (5, 1), ()),\n    ((5,), (), (), (6,)),\n], ids=str)\ndef test_gaussian_hmm_log_prob_null_dynamics(init_shape, trans_mat_shape, trans_mvn_shape,\n                                             obs_mvn_shape, hidden_dim):\n    obs_dim = hidden_dim\n    init_dist = random_mvn(init_shape, hidden_dim)\n\n    # impose null dynamics\n    trans_mat = torch.zeros(trans_mat_shape + (hidden_dim, hidden_dim))\n    trans_dist = random_mvn(trans_mvn_shape, hidden_dim, diag=True)\n\n    # trivial observation matrix (hidden_dim = obs_dim)\n    obs_mat = torch.eye(hidden_dim)\n    obs_dist = random_mvn(obs_mvn_shape, obs_dim, diag=True)\n\n    actual_dist = GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n    expected_dist = dist.GaussianHMM(init_dist, trans_mat, trans_dist, obs_mat, obs_dist)\n    assert actual_dist.batch_shape == expected_dist.batch_shape\n    assert actual_dist.event_shape == expected_dist.event_shape\n\n    shape = broadcast_shape(init_shape + (1,),\n                            trans_mat_shape, trans_mvn_shape,\n                            obs_mvn_shape)\n    data = obs_dist.expand(shape).sample()\n    assert data.shape == actual_dist.shape()\n\n    actual_log_prob = actual_dist.log_prob(data)\n    expected_log_prob = expected_dist.log_prob(data)\n\n    assert_close(actual_log_prob, expected_log_prob, atol=1e-5, rtol=1e-5)\n    check_expand(actual_dist, data)\n\n    obs_cov = obs_dist.covariance_matrix.diagonal(dim1=-1, dim2=-2)\n    trans_cov = trans_dist.covariance_matrix.diagonal(dim1=-1, dim2=-2)\n    sum_scale = (obs_cov + trans_cov).sqrt()\n    sum_loc = trans_dist.loc + obs_dist.loc\n\n    analytic_log_prob = dist.Normal(sum_loc, sum_scale).log_prob(data).sum(-1).sum(-1)\n    assert_close(analytic_log_prob, actual_log_prob, atol=1.0e-5)\n'"
test/pyroapi/conftest.py,0,b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_runtest_call(item):\n    try:\n        item.runtest()\n    except NotImplementedError as e:\n        pytest.xfail(str(e))\n'
test/pyroapi/test_pyroapi.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\nfrom pyroapi import pyro_backend\nfrom pyroapi.tests import *  # noqa F401\n\n\n@pytest.yield_fixture\ndef backend():\n    with pyro_backend(""funsor""):\n        yield\n'"
