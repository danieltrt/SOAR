file_path,api_count,code
setup.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport sys\n\nfrom setuptools import find_packages, setup\n\nPROJECT_PATH = os.path.dirname(os.path.abspath(__file__))\n\n# Find version\nfor line in open(os.path.join(PROJECT_PATH, 'numpyro', 'version.py')):\n    if line.startswith('__version__ = '):\n        version = line.strip().split()[2][1:-1]\n\n# READ README.md for long description on PyPi.\ntry:\n    long_description = open('README.md', encoding='utf-8').read()\nexcept Exception as e:\n    sys.stderr.write('Failed to read README.md:\\n  {}\\n'.format(e))\n    sys.stderr.flush()\n    long_description = ''\n\n\nsetup(\n    name='numpyro',\n    version=version,\n    description='Pyro PPL on NumPy',\n    packages=find_packages(include=['numpyro', 'numpyro.*']),\n    url='https://github.com/pyro-ppl/numpyro',\n    author='Uber AI Labs',\n    author_email='npradhan@uber.com',\n    install_requires=[\n        # TODO: pin to a specific version for the release (until JAX's API becomes stable)\n        'jax==0.1.67',\n        # check min version here: https://github.com/google/jax/blob/master/jax/lib/__init__.py#L20\n        'jaxlib>=0.1.47',\n        'tqdm',\n    ],\n    extras_require={\n        'doc': ['sphinx', 'sphinx_rtd_theme', 'sphinx-gallery'],\n        'test': [\n            'flake8',\n            'pytest>=4.1',\n            'pyro-api>=0.1.1'\n        ],\n        'dev': ['ipython', 'isort'],\n        'examples': ['matplotlib', 'seaborn'],\n    },\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    keywords='probabilistic machine learning bayesian statistics',\n    license='Apache License 2.0',\n    classifiers=[\n        'Intended Audience :: Developers',\n        'Intended Audience :: Education',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: Apache Software License',\n        'Operating System :: POSIX :: Linux',\n        'Operating System :: MacOS :: MacOS X',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n    ],\n)\n"""
examples/__init__.py,0,b''
examples/baseball.py,10,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nBaseball\n========\n\nOriginal example from Pyro:\nhttps://github.com/pyro-ppl/pyro/blob/dev/examples/baseball.py\n\nExample has been adapted from [1]. It demonstrates how to do Bayesian inference using\nNUTS (or, HMC) in Pyro, and use of some common inference utilities.\n\nAs in the Stan tutorial, this uses the small baseball dataset of Efron and Morris [2]\nto estimate players\' batting average which is the fraction of times a player got a\nbase hit out of the number of times they went up at bat.\n\nThe dataset separates the initial 45 at-bats statistics from the remaining season.\nWe use the hits data from the initial 45 at-bats to estimate the batting average\nfor each player. We then use the remaining season\'s data to validate the predictions\nfrom our models.\n\nThree models are evaluated:\n\n    - Complete pooling model: The success probability of scoring a hit is shared\n      amongst all players.\n    - No pooling model: Each individual player\'s success probability is distinct and\n      there is no data sharing amongst players.\n    - Partial pooling model: A hierarchical model with partial data sharing.\n\nWe recommend Radford Neal\'s tutorial on HMC ([3]) to users who would like to get a\nmore comprehensive understanding of HMC and its variants, and to [4] for details on\nthe No U-Turn Sampler, which provides an efficient and automated way (i.e. limited\nhyper-parameters) of running HMC on different problems.\n\n**References:**\n\n    1. Carpenter B. (2016), `""Hierarchical Partial Pooling for Repeated Binary Trials""\n       <http://mc-stan.org/users/documentation/case-studies/pool-binary-trials.html/>`_.\n    2. Efron B., Morris C. (1975), ""Data analysis using Stein\'s estimator and its\n       generalizations"", J. Amer. Statist. Assoc., 70, 311-319.\n    3. Neal, R. (2012), ""MCMC using Hamiltonian Dynamics"",\n       (https://arxiv.org/pdf/1206.1901.pdf)\n    4. Hoffman, M. D. and Gelman, A. (2014), ""The No-U-turn sampler: Adaptively setting\n       path lengths in Hamiltonian Monte Carlo"", (https://arxiv.org/abs/1111.4246)\n""""""\n\nimport argparse\nimport os\n\nimport numpy as onp\n\nimport jax.numpy as np\nimport jax.random as random\nfrom jax.scipy.special import logsumexp\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.examples.datasets import BASEBALL, load_dataset\nfrom numpyro.infer import MCMC, NUTS, Predictive, log_likelihood\n\n\ndef fully_pooled(at_bats, hits=None):\n    r""""""\n    Number of hits in $K$ at bats for each player has a Binomial\n    distribution with a common probability of success, $\\phi$.\n\n    :param (np.DeviceArray) at_bats: Number of at bats for each player.\n    :param (np.DeviceArray) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    """"""\n    phi_prior = dist.Uniform(0, 1)\n    phi = numpyro.sample(""phi"", phi_prior)\n    num_players = at_bats.shape[0]\n    with numpyro.plate(""num_players"", num_players):\n        return numpyro.sample(""obs"", dist.Binomial(at_bats, probs=phi), obs=hits)\n\n\ndef not_pooled(at_bats, hits=None):\n    r""""""\n    Number of hits in $K$ at bats for each player has a Binomial\n    distribution with independent probability of success, $\\phi_i$.\n\n    :param (np.DeviceArray) at_bats: Number of at bats for each player.\n    :param (np.DeviceArray) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    """"""\n    num_players = at_bats.shape[0]\n    with numpyro.plate(""num_players"", num_players):\n        phi_prior = dist.Uniform(0, 1)\n        phi = numpyro.sample(""phi"", phi_prior)\n        return numpyro.sample(""obs"", dist.Binomial(at_bats, probs=phi), obs=hits)\n\n\ndef partially_pooled(at_bats, hits=None):\n    r""""""\n    Number of hits has a Binomial distribution with independent\n    probability of success, $\\phi_i$. Each $\\phi_i$ follows a Beta\n    distribution with concentration parameters $c_1$ and $c_2$, where\n    $c_1 = m * kappa$, $c_2 = (1 - m) * kappa$, $m ~ Uniform(0, 1)$,\n    and $kappa ~ Pareto(1, 1.5)$.\n\n    :param (np.DeviceArray) at_bats: Number of at bats for each player.\n    :param (np.DeviceArray) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    """"""\n    m = numpyro.sample(""m"", dist.Uniform(0, 1))\n    kappa = numpyro.sample(""kappa"", dist.Pareto(1.5))\n    num_players = at_bats.shape[0]\n    with numpyro.plate(""num_players"", num_players):\n        phi_prior = dist.Beta(m * kappa, (1 - m) * kappa)\n        phi = numpyro.sample(""phi"", phi_prior)\n        return numpyro.sample(""obs"", dist.Binomial(at_bats, probs=phi), obs=hits)\n\n\ndef partially_pooled_with_logit(at_bats, hits=None):\n    r""""""\n    Number of hits has a Binomial distribution with a logit link function.\n    The logits $\\alpha$ for each player is normally distributed with the\n    mean and scale parameters sharing a common prior.\n\n    :param (np.DeviceArray) at_bats: Number of at bats for each player.\n    :param (np.DeviceArray) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    """"""\n    loc = numpyro.sample(""loc"", dist.Normal(-1, 1))\n    scale = numpyro.sample(""scale"", dist.HalfCauchy(1))\n    num_players = at_bats.shape[0]\n    with numpyro.plate(""num_players"", num_players):\n        alpha = numpyro.sample(""alpha"", dist.Normal(loc, scale))\n        return numpyro.sample(""obs"", dist.Binomial(at_bats, logits=alpha), obs=hits)\n\n\ndef run_inference(model, at_bats, hits, rng_key, args):\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(rng_key, at_bats, hits)\n    return mcmc.get_samples()\n\n\ndef predict(model, at_bats, hits, z, rng_key, player_names, train=True):\n    header = model.__name__ + (\' - TRAIN\' if train else \' - TEST\')\n    predictions = Predictive(model, posterior_samples=z)(rng_key, at_bats)[\'obs\']\n    print_results(\'=\' * 30 + header + \'=\' * 30,\n                  predictions,\n                  player_names,\n                  at_bats,\n                  hits)\n    if not train:\n        post_loglik = log_likelihood(model, z, at_bats, hits)[\'obs\']\n        # computes expected log predictive density at each data point\n        exp_log_density = logsumexp(post_loglik, axis=0) - np.log(np.shape(post_loglik)[0])\n        # reports log predictive density of all test points\n        print(\'\\nLog pointwise predictive density: {:.2f}\\n\'.format(exp_log_density.sum()))\n\n\ndef print_results(header, preds, player_names, at_bats, hits):\n    columns = [\'\', \'At-bats\', \'ActualHits\', \'Pred(p25)\', \'Pred(p50)\', \'Pred(p75)\']\n    header_format = \'{:>20} {:>10} {:>10} {:>10} {:>10} {:>10}\'\n    row_format = \'{:>20} {:>10.0f} {:>10.0f} {:>10.2f} {:>10.2f} {:>10.2f}\'\n    quantiles = onp.quantile(preds, [0.25, 0.5, 0.75], axis=0)\n    print(\'\\n\', header, \'\\n\')\n    print(header_format.format(*columns))\n    for i, p in enumerate(player_names):\n        print(row_format.format(p, at_bats[i], hits[i], *quantiles[:, i]), \'\\n\')\n\n\ndef main(args):\n    _, fetch_train = load_dataset(BASEBALL, split=\'train\', shuffle=False)\n    train, player_names = fetch_train()\n    _, fetch_test = load_dataset(BASEBALL, split=\'test\', shuffle=False)\n    test, _ = fetch_test()\n    at_bats, hits = train[:, 0], train[:, 1]\n    season_at_bats, season_hits = test[:, 0], test[:, 1]\n    for i, model in enumerate((fully_pooled,\n                               not_pooled,\n                               partially_pooled,\n                               partially_pooled_with_logit,\n                               )):\n        rng_key, rng_key_predict = random.split(random.PRNGKey(i + 1))\n        zs = run_inference(model, at_bats, hits, rng_key, args)\n        predict(model, at_bats, hits, zs, rng_key_predict, player_names)\n        predict(model, season_at_bats, season_hits, zs, rng_key_predict, player_names, train=False)\n\n\nif __name__ == ""__main__"":\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""Baseball batting average using HMC"")\n    parser.add_argument(""-n"", ""--num-samples"", nargs=""?"", default=3000, type=int)\n    parser.add_argument(""--num-warmup"", nargs=\'?\', default=1500, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=1, type=int)\n    parser.add_argument(\'--device\', default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/bnn.py,21,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nBayesian Neural Network\n=======================\n\nWe demonstrate how to use NUTS to do inference on a simple (small)\nBayesian neural network with two hidden layers.\n""""""\n\nimport argparse\nimport os\nimport time\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as onp\n\nfrom jax import vmap\nimport jax.numpy as np\nimport jax.random as random\n\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\n\nmatplotlib.use(\'Agg\')  # noqa: E402\n\n\n# the non-linearity we use in our neural network\ndef nonlin(x):\n    return np.tanh(x)\n\n\n# a two-layer bayesian neural network with computational flow\n# given by D_X => D_H => D_H => D_Y where D_H is the number of\n# hidden units. (note we indicate tensor dimensions in the comments)\ndef model(X, Y, D_H):\n\n    D_X, D_Y = X.shape[1], 1\n\n    # sample first layer (we put unit normal priors on all weights)\n    w1 = numpyro.sample(""w1"", dist.Normal(np.zeros((D_X, D_H)), np.ones((D_X, D_H))))  # D_X D_H\n    z1 = nonlin(np.matmul(X, w1))   # N D_H  <= first layer of activations\n\n    # sample second layer\n    w2 = numpyro.sample(""w2"", dist.Normal(np.zeros((D_H, D_H)), np.ones((D_H, D_H))))  # D_H D_H\n    z2 = nonlin(np.matmul(z1, w2))  # N D_H  <= second layer of activations\n\n    # sample final layer of weights and neural network output\n    w3 = numpyro.sample(""w3"", dist.Normal(np.zeros((D_H, D_Y)), np.ones((D_H, D_Y))))  # D_H D_Y\n    z3 = np.matmul(z2, w3)  # N D_Y  <= output of the neural network\n\n    # we put a prior on the observation noise\n    prec_obs = numpyro.sample(""prec_obs"", dist.Gamma(3.0, 1.0))\n    sigma_obs = 1.0 / np.sqrt(prec_obs)\n\n    # observe data\n    numpyro.sample(""Y"", dist.Normal(z3, sigma_obs), obs=Y)\n\n\n# helper function for HMC inference\ndef run_inference(model, args, rng_key, X, Y, D_H):\n    start = time.time()\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(rng_key, X, Y, D_H)\n    mcmc.print_summary()\n    print(\'\\nMCMC elapsed time:\', time.time() - start)\n    return mcmc.get_samples()\n\n\n# helper function for prediction\ndef predict(model, rng_key, samples, X, D_H):\n    model = handlers.substitute(handlers.seed(model, rng_key), samples)\n    # note that Y will be sampled in the model because we pass Y=None here\n    model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H)\n    return model_trace[\'Y\'][\'value\']\n\n\n# create artificial regression dataset\ndef get_data(N=50, D_X=3, sigma_obs=0.05, N_test=500):\n    D_Y = 1  # create 1d outputs\n    onp.random.seed(0)\n    X = np.linspace(-1, 1, N)\n    X = np.power(X[:, onp.newaxis], np.arange(D_X))\n    W = 0.5 * onp.random.randn(D_X)\n    Y = np.dot(X, W) + 0.5 * np.power(0.5 + X[:, 1], 2.0) * np.sin(4.0 * X[:, 1])\n    Y += sigma_obs * onp.random.randn(N)\n    Y = Y[:, onp.newaxis]\n    Y -= np.mean(Y)\n    Y /= np.std(Y)\n\n    assert X.shape == (N, D_X)\n    assert Y.shape == (N, D_Y)\n\n    X_test = np.linspace(-1.3, 1.3, N_test)\n    X_test = np.power(X_test[:, onp.newaxis], np.arange(D_X))\n\n    return X, Y, X_test\n\n\ndef main(args):\n    N, D_X, D_H = args.num_data, 3, args.num_hidden\n    X, Y, X_test = get_data(N=N, D_X=D_X)\n\n    # do inference\n    rng_key, rng_key_predict = random.split(random.PRNGKey(0))\n    samples = run_inference(model, args, rng_key, X, Y, D_H)\n\n    # predict Y_test at inputs X_test\n    vmap_args = (samples, random.split(rng_key_predict, args.num_samples * args.num_chains))\n    predictions = vmap(lambda samples, rng_key: predict(model, rng_key, samples, X_test, D_H))(*vmap_args)\n    predictions = predictions[..., 0]\n\n    # compute mean prediction and confidence interval around median\n    mean_prediction = np.mean(predictions, axis=0)\n    percentiles = onp.percentile(predictions, [5.0, 95.0], axis=0)\n\n    # make plots\n    fig, ax = plt.subplots(1, 1)\n\n    # plot training data\n    ax.plot(X[:, 1], Y[:, 0], \'kx\')\n    # plot 90% confidence level of predictions\n    ax.fill_between(X_test[:, 1], percentiles[0, :], percentiles[1, :], color=\'lightblue\')\n    # plot mean prediction\n    ax.plot(X_test[:, 1], mean_prediction, \'blue\', ls=\'solid\', lw=2.0)\n    ax.set(xlabel=""X"", ylabel=""Y"", title=""Mean predictions with 90% CI"")\n\n    plt.savefig(\'bnn_plot.pdf\')\n    plt.tight_layout()\n\n\nif __name__ == ""__main__"":\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""Bayesian neural network example"")\n    parser.add_argument(""-n"", ""--num-samples"", nargs=""?"", default=2000, type=int)\n    parser.add_argument(""--num-warmup"", nargs=\'?\', default=1000, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=1, type=int)\n    parser.add_argument(""--num-data"", nargs=\'?\', default=100, type=int)\n    parser.add_argument(""--num-hidden"", nargs=\'?\', default=5, type=int)\n    parser.add_argument(""--device"", default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/covtype.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\nimport time\n\nimport numpy as onp\n\nfrom jax import random\nimport jax.numpy as np\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.examples.datasets import COVTYPE, load_dataset\nfrom numpyro.infer import MCMC, NUTS\n\n\ndef _load_dataset():\n    _, fetch = load_dataset(COVTYPE, shuffle=False)\n    features, labels = fetch()\n\n    # normalize features and add intercept\n    features = (features - features.mean(0)) / features.std(0)\n    features = np.hstack([features, np.ones((features.shape[0], 1))])\n\n    # make binary feature\n    _, counts = onp.unique(labels, return_counts=True)\n    specific_category = np.argmax(counts)\n    labels = (labels == specific_category)\n\n    N, dim = features.shape\n    print(""Data shape:"", features.shape)\n    print(""Label distribution: {} has label 1, {} has label 0""\n          .format(labels.sum(), N - labels.sum()))\n    return features, labels\n\n\ndef model(data, labels):\n    dim = data.shape[1]\n    coefs = numpyro.sample(\'coefs\', dist.Normal(np.zeros(dim), np.ones(dim)))\n    logits = np.dot(data, coefs)\n    return numpyro.sample(\'obs\', dist.Bernoulli(logits=logits), obs=labels)\n\n\ndef benchmark_hmc(args, features, labels):\n    step_size = np.sqrt(0.5 / features.shape[0])\n    trajectory_length = step_size * args.num_steps\n    rng_key = random.PRNGKey(1)\n    start = time.time()\n    kernel = NUTS(model, trajectory_length=trajectory_length)\n    mcmc = MCMC(kernel, 0, args.num_samples)\n    mcmc.run(rng_key, features, labels)\n    mcmc.print_summary()\n    print(\'\\nMCMC elapsed time:\', time.time() - start)\n\n\ndef main(args):\n    features, labels = _load_dataset()\n    benchmark_hmc(args, features, labels)\n\n\nif __name__ == \'__main__\':\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-samples\', default=100, type=int, help=\'number of samples\')\n    parser.add_argument(\'--num-steps\', default=10, type=int, help=\'number of steps (for ""HMC"")\')\n    parser.add_argument(\'--num-chains\', nargs=\'?\', default=1, type=int)\n    parser.add_argument(\'--algo\', default=\'NUTS\', type=str, help=\'whether to run ""HMC"" or ""NUTS""\')\n    parser.add_argument(\'--device\', default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/funnel.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nNeal\'s Funnel\n=============\n\nThis example, which is adapted from [1], illustrates how to leverage non-centered\nparameterization using the class :class:`numpyro.distributions.TransformedDistribution`.\nWe will examine the difference between two types of parameterizations on the\n10-dimensional Neal\'s funnel distribution. As we will see, HMC gets trouble at\nthe neck of the funnel if centered parameterization is used. On the contrary,\nthe problem can be solved by using non-centered parameterization.\n\nUsing non-centered parameterization through TransformedDistribution in NumPyro\nhas the same effect as the automatic reparameterisation technique introduced in\n[2]. However, in [2], users need to implement a (non-trivial) reparameterization\nrule for each type of transform. Instead, in NumPyro the only requirement to let\ninference algorithms know to do reparameterization automatically is to declare\nthe random variable as a transformed distribution.\n\n**References:**\n\n    1. *Stan User\'s Guide*, https://mc-stan.org/docs/2_19/stan-users-guide/reparameterization-section.html\n    2. Maria I. Gorinova, Dave Moore, Matthew D. Hoffman (2019), ""Automatic\n       Reparameterisation of Probabilistic Programs"", (https://arxiv.org/abs/1906.03028)\n""""""\n\nimport argparse\nimport os\n\nimport matplotlib.pyplot as plt\n\nfrom jax import random\nimport jax.numpy as np\n\nimport numpyro\nfrom numpyro.contrib.reparam import reparam, TransformReparam\nimport numpyro.distributions as dist\nfrom numpyro.distributions.transforms import AffineTransform\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\n\ndef model(dim=10):\n    y = numpyro.sample(\'y\', dist.Normal(0, 3))\n    numpyro.sample(\'x\', dist.Normal(np.zeros(dim - 1), np.exp(y / 2)))\n\n\ndef reparam_model(dim=10):\n    y = numpyro.sample(\'y\', dist.Normal(0, 3))\n    with reparam(config={\'x\': TransformReparam()}):\n        numpyro.sample(\'x\', dist.TransformedDistribution(\n            dist.Normal(np.zeros(dim - 1), 1), AffineTransform(0, np.exp(y / 2))))\n\n\ndef run_inference(model, args, rng_key):\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(rng_key)\n    mcmc.print_summary()\n    return mcmc.get_samples()\n\n\ndef main(args):\n    rng_key = random.PRNGKey(0)\n\n    # do inference with centered parameterization\n    print(""============================= Centered Parameterization =============================="")\n    samples = run_inference(model, args, rng_key)\n\n    # do inference with non-centered parameterization\n    print(""\\n=========================== Non-centered Parameterization ============================"")\n    reparam_samples = run_inference(reparam_model, args, rng_key)\n    # collect deterministic sites\n    reparam_samples = Predictive(reparam_model, reparam_samples, return_sites=[\'x\', \'y\'])(\n        random.PRNGKey(1))\n\n    # make plots\n    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(8, 8))\n\n    ax1.plot(samples[\'x\'][:, 0], samples[\'y\'], ""go"", alpha=0.3)\n    ax1.set(xlim=(-20, 20), ylim=(-9, 9), ylabel=\'y\',\n            title=\'Funnel samples with centered parameterization\')\n\n    ax2.plot(reparam_samples[\'x\'][:, 0], reparam_samples[\'y\'], ""go"", alpha=0.3)\n    ax2.set(xlim=(-20, 20), ylim=(-9, 9), xlabel=\'x[0]\', ylabel=\'y\',\n            title=\'Funnel samples with non-centered parameterization\')\n\n    plt.savefig(\'funnel_plot.pdf\')\n    plt.tight_layout()\n\n\nif __name__ == ""__main__"":\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""Non-centered reparameterization example"")\n    parser.add_argument(""-n"", ""--num-samples"", nargs=""?"", default=1000, type=int)\n    parser.add_argument(""--num-warmup"", nargs=\'?\', default=1000, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=1, type=int)\n    parser.add_argument(""--device"", default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/gp.py,17,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nGaussian Process\n================\n\nIn this example we show how to use NUTS to sample from the posterior\nover the hyperparameters of a gaussian process.\n""""""\n\nimport argparse\nimport os\nimport time\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as onp\n\nimport jax\nfrom jax import vmap\nimport jax.numpy as np\nimport jax.random as random\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\n\nmatplotlib.use(\'Agg\')  # noqa: E402\n\n\n# squared exponential kernel with diagonal noise term\ndef kernel(X, Z, var, length, noise, jitter=1.0e-6, include_noise=True):\n    deltaXsq = np.power((X[:, None] - Z) / length, 2.0)\n    k = var * np.exp(-0.5 * deltaXsq)\n    if include_noise:\n        k += (noise + jitter) * np.eye(X.shape[0])\n    return k\n\n\ndef model(X, Y):\n    # set uninformative log-normal priors on our three kernel hyperparameters\n    var = numpyro.sample(""kernel_var"", dist.LogNormal(0.0, 10.0))\n    noise = numpyro.sample(""kernel_noise"", dist.LogNormal(0.0, 10.0))\n    length = numpyro.sample(""kernel_length"", dist.LogNormal(0.0, 10.0))\n\n    # compute kernel\n    k = kernel(X, X, var, length, noise)\n\n    # sample Y according to the standard gaussian process formula\n    numpyro.sample(""Y"", dist.MultivariateNormal(loc=np.zeros(X.shape[0]), covariance_matrix=k),\n                   obs=Y)\n\n\n# helper function for doing hmc inference\ndef run_inference(model, args, rng_key, X, Y):\n    start = time.time()\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(rng_key, X, Y)\n    mcmc.print_summary()\n    print(\'\\nMCMC elapsed time:\', time.time() - start)\n    return mcmc.get_samples()\n\n\n# do GP prediction for a given set of hyperparameters. this makes use of the well-known\n# formula for gaussian process predictions\ndef predict(rng_key, X, Y, X_test, var, length, noise):\n    # compute kernels between train and test data, etc.\n    k_pp = kernel(X_test, X_test, var, length, noise, include_noise=True)\n    k_pX = kernel(X_test, X, var, length, noise, include_noise=False)\n    k_XX = kernel(X, X, var, length, noise, include_noise=True)\n    K_xx_inv = np.linalg.inv(k_XX)\n    K = k_pp - np.matmul(k_pX, np.matmul(K_xx_inv, np.transpose(k_pX)))\n    sigma_noise = np.sqrt(np.clip(np.diag(K), a_min=0.)) * jax.random.normal(rng_key, X_test.shape[:1])\n    mean = np.matmul(k_pX, np.matmul(K_xx_inv, Y))\n    # we return both the mean function and a sample from the posterior predictive for the\n    # given set of hyperparameters\n    return mean, mean + sigma_noise\n\n\n# create artificial regression dataset\ndef get_data(N=30, sigma_obs=0.15, N_test=400):\n    onp.random.seed(0)\n    X = np.linspace(-1, 1, N)\n    Y = X + 0.2 * np.power(X, 3.0) + 0.5 * np.power(0.5 + X, 2.0) * np.sin(4.0 * X)\n    Y += sigma_obs * onp.random.randn(N)\n    Y -= np.mean(Y)\n    Y /= np.std(Y)\n\n    assert X.shape == (N,)\n    assert Y.shape == (N,)\n\n    X_test = np.linspace(-1.3, 1.3, N_test)\n\n    return X, Y, X_test\n\n\ndef main(args):\n    X, Y, X_test = get_data(N=args.num_data)\n\n    # do inference\n    rng_key, rng_key_predict = random.split(random.PRNGKey(0))\n    samples = run_inference(model, args, rng_key, X, Y)\n\n    # do prediction\n    vmap_args = (random.split(rng_key_predict, args.num_samples * args.num_chains), samples[\'kernel_var\'],\n                 samples[\'kernel_length\'], samples[\'kernel_noise\'])\n    means, predictions = vmap(lambda rng_key, var, length, noise:\n                              predict(rng_key, X, Y, X_test, var, length, noise))(*vmap_args)\n\n    mean_prediction = onp.mean(means, axis=0)\n    percentiles = onp.percentile(predictions, [5.0, 95.0], axis=0)\n\n    # make plots\n    fig, ax = plt.subplots(1, 1)\n\n    # plot training data\n    ax.plot(X, Y, \'kx\')\n    # plot 90% confidence level of predictions\n    ax.fill_between(X_test, percentiles[0, :], percentiles[1, :], color=\'lightblue\')\n    # plot mean prediction\n    ax.plot(X_test, mean_prediction, \'blue\', ls=\'solid\', lw=2.0)\n    ax.set(xlabel=""X"", ylabel=""Y"", title=""Mean predictions with 90% CI"")\n\n    plt.savefig(""gp_plot.pdf"")\n    plt.tight_layout()\n\n\nif __name__ == ""__main__"":\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""Gaussian Process example"")\n    parser.add_argument(""-n"", ""--num-samples"", nargs=""?"", default=1000, type=int)\n    parser.add_argument(""--num-warmup"", nargs=\'?\', default=1000, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=1, type=int)\n    parser.add_argument(""--num-data"", nargs=\'?\', default=25, type=int)\n    parser.add_argument(""--device"", default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/hmm.py,13,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nHidden Markov Model\n===================\n\nIn this example, we will follow [1] to construct a semi-supervised Hidden Markov\nModel for a generative model with observations are words and latent variables\nare categories. Instead of automatically marginalizing all discrete latent\nvariables (as in [2]), we will use the ""forward algorithm"" (which exploits the\nconditional independent of a Markov model - see [3]) to iteratively do this\nmarginalization.\n\nThe semi-supervised problem is chosen instead of an unsupervised one because it\nis hard to make the inference works for an unsupervised model (see the\ndiscussion [4]). On the other hand, this example also illustrates the usage of\nJAX\'s `lax.scan` primitive. The primitive will greatly improve compiling for the\nmodel.\n\n**References:**\n\n    1. https://mc-stan.org/docs/2_19/stan-users-guide/hmms-section.html\n    2. http://pyro.ai/examples/hmm.html\n    3. https://en.wikipedia.org/wiki/Forward_algorithm\n    4. https://discourse.pymc.io/t/how-to-marginalized-markov-chain-with-categorical/2230\n""""""\n\nimport argparse\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as onp\nfrom scipy.stats import gaussian_kde\n\nfrom jax import lax, random\nimport jax.numpy as np\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.distributions.util import logsumexp\nfrom numpyro.infer import MCMC, NUTS\n\n\ndef simulate_data(rng_key, num_categories, num_words, num_supervised_data, num_unsupervised_data):\n    rng_key, rng_key_transition, rng_key_emission = random.split(rng_key, 3)\n\n    transition_prior = np.ones(num_categories)\n    emission_prior = np.repeat(0.1, num_words)\n\n    transition_prob = dist.Dirichlet(transition_prior).sample(key=rng_key_transition,\n                                                              sample_shape=(num_categories,))\n    emission_prob = dist.Dirichlet(emission_prior).sample(key=rng_key_emission,\n                                                          sample_shape=(num_categories,))\n\n    start_prob = np.repeat(1. / num_categories, num_categories)\n    categories, words = [], []\n    for t in range(num_supervised_data + num_unsupervised_data):\n        rng_key, rng_key_transition, rng_key_emission = random.split(rng_key, 3)\n        if t == 0 or t == num_supervised_data:\n            category = dist.Categorical(start_prob).sample(key=rng_key_transition)\n        else:\n            category = dist.Categorical(transition_prob[category]).sample(key=rng_key_transition)\n        word = dist.Categorical(emission_prob[category]).sample(key=rng_key_emission)\n        categories.append(category)\n        words.append(word)\n\n    # split into supervised data and unsupervised data\n    categories, words = np.stack(categories), np.stack(words)\n    supervised_categories = categories[:num_supervised_data]\n    supervised_words = words[:num_supervised_data]\n    unsupervised_words = words[num_supervised_data:]\n    return (transition_prior, emission_prior, transition_prob, emission_prob,\n            supervised_categories, supervised_words, unsupervised_words)\n\n\ndef forward_one_step(prev_log_prob, curr_word, transition_log_prob, emission_log_prob):\n    log_prob_tmp = np.expand_dims(prev_log_prob, axis=1) + transition_log_prob\n    log_prob = log_prob_tmp + emission_log_prob[:, curr_word]\n    return logsumexp(log_prob, axis=0)\n\n\ndef forward_log_prob(init_log_prob, words, transition_log_prob, emission_log_prob, unroll_loop=False):\n    # Note: The following naive implementation will make it very slow to compile\n    # and do inference. So we use lax.scan instead.\n    #\n    # >>> log_prob = init_log_prob\n    # >>> for word in words:\n    # ...     log_prob = forward_one_step(log_prob, word, transition_log_prob, emission_log_prob)\n    def scan_fn(log_prob, word):\n        return forward_one_step(log_prob, word, transition_log_prob, emission_log_prob), np.zeros((0,))\n\n    if unroll_loop:\n        log_prob = init_log_prob\n        for word in words:\n            log_prob = forward_one_step(log_prob, word, transition_log_prob, emission_log_prob)\n    else:\n        log_prob, _ = lax.scan(scan_fn, init_log_prob, words)\n    return log_prob\n\n\ndef semi_supervised_hmm(transition_prior, emission_prior,\n                        supervised_categories, supervised_words,\n                        unsupervised_words, unroll_loop=False):\n    num_categories, num_words = transition_prior.shape[0], emission_prior.shape[0]\n    transition_prob = numpyro.sample(\'transition_prob\', dist.Dirichlet(\n        np.broadcast_to(transition_prior, (num_categories, num_categories))))\n    emission_prob = numpyro.sample(\'emission_prob\', dist.Dirichlet(\n        np.broadcast_to(emission_prior, (num_categories, num_words))))\n\n    # models supervised data;\n    # here we don\'t make any assumption about the first supervised category, in other words,\n    # we place a flat/uniform prior on it.\n    numpyro.sample(\'supervised_categories\', dist.Categorical(transition_prob[supervised_categories[:-1]]),\n                   obs=supervised_categories[1:])\n    numpyro.sample(\'supervised_words\', dist.Categorical(emission_prob[supervised_categories]),\n                   obs=supervised_words)\n\n    # computes log prob of unsupervised data\n    transition_log_prob = np.log(transition_prob)\n    emission_log_prob = np.log(emission_prob)\n    init_log_prob = emission_log_prob[:, unsupervised_words[0]]\n    log_prob = forward_log_prob(init_log_prob, unsupervised_words[1:],\n                                transition_log_prob, emission_log_prob, unroll_loop)\n    log_prob = logsumexp(log_prob, axis=0, keepdims=True)\n    # inject log_prob to potential function\n    numpyro.factor(\'forward_log_prob\', log_prob)\n\n\ndef print_results(posterior, transition_prob, emission_prob):\n    header = semi_supervised_hmm.__name__ + \' - TRAIN\'\n    columns = [\'\', \'ActualProb\', \'Pred(p25)\', \'Pred(p50)\', \'Pred(p75)\']\n    header_format = \'{:>20} {:>10} {:>10} {:>10} {:>10}\'\n    row_format = \'{:>20} {:>10.2f} {:>10.2f} {:>10.2f} {:>10.2f}\'\n    print(\'\\n\', \'=\' * 20 + header + \'=\' * 20, \'\\n\')\n    print(header_format.format(*columns))\n\n    quantiles = onp.quantile(posterior[\'transition_prob\'], [0.25, 0.5, 0.75], axis=0)\n    for i in range(transition_prob.shape[0]):\n        for j in range(transition_prob.shape[1]):\n            idx = \'transition[{},{}]\'.format(i, j)\n            print(row_format.format(idx, transition_prob[i, j], *quantiles[:, i, j]), \'\\n\')\n\n    quantiles = onp.quantile(posterior[\'emission_prob\'], [0.25, 0.5, 0.75], axis=0)\n    for i in range(emission_prob.shape[0]):\n        for j in range(emission_prob.shape[1]):\n            idx = \'emission[{},{}]\'.format(i, j)\n            print(row_format.format(idx, emission_prob[i, j], *quantiles[:, i, j]), \'\\n\')\n\n\ndef main(args):\n    print(\'Simulating data...\')\n    (transition_prior, emission_prior, transition_prob, emission_prob,\n     supervised_categories, supervised_words, unsupervised_words) = simulate_data(\n        random.PRNGKey(1),\n        num_categories=args.num_categories,\n        num_words=args.num_words,\n        num_supervised_data=args.num_supervised,\n        num_unsupervised_data=args.num_unsupervised,\n    )\n    print(\'Starting inference...\')\n    rng_key = random.PRNGKey(2)\n    start = time.time()\n    kernel = NUTS(semi_supervised_hmm)\n    mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(rng_key, transition_prior, emission_prior, supervised_categories,\n             supervised_words, unsupervised_words, args.unroll_loop)\n    samples = mcmc.get_samples()\n    print_results(samples, transition_prob, emission_prob)\n    print(\'\\nMCMC elapsed time:\', time.time() - start)\n\n    # make plots\n    fig, ax = plt.subplots(1, 1)\n\n    x = onp.linspace(0, 1, 101)\n    for i in range(transition_prob.shape[0]):\n        for j in range(transition_prob.shape[1]):\n            ax.plot(x, gaussian_kde(samples[\'transition_prob\'][:, i, j])(x),\n                    label=""trans_prob[{}, {}], true value = {:.2f}""\n                    .format(i, j, transition_prob[i, j]))\n    ax.set(xlabel=""Probability"", ylabel=""Frequency"",\n           title=""Transition probability posterior"")\n    ax.legend()\n\n    plt.savefig(""hmm_plot.pdf"")\n    plt.tight_layout()\n\n\nif __name__ == \'__main__\':\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=\'Semi-supervised Hidden Markov Model\')\n    parser.add_argument(\'--num-categories\', default=3, type=int)\n    parser.add_argument(\'--num-words\', default=10, type=int)\n    parser.add_argument(\'--num-supervised\', default=100, type=int)\n    parser.add_argument(\'--num-unsupervised\', default=500, type=int)\n    parser.add_argument(\'-n\', \'--num-samples\', nargs=\'?\', default=1000, type=int)\n    parser.add_argument(\'--num-warmup\', nargs=\'?\', default=500, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=1, type=int)\n    parser.add_argument(""--unroll-loop"", action=\'store_true\')\n    parser.add_argument(\'--device\', default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/minipyro.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport argparse\n\nfrom jax import random\nimport jax.numpy as np\nfrom jax.random import PRNGKey\n\nimport numpyro\nfrom numpyro import optim\nimport numpyro.distributions as dist\nfrom numpyro.infer import ELBO, SVI\nfrom numpyro.util import fori_loop\n\n\ndef model(data):\n    loc = numpyro.sample(""loc"", dist.Normal(0., 1.))\n    numpyro.sample(""obs"", dist.Normal(loc, 1.), obs=data)\n\n\n# Define a guide (i.e. variational distribution) with a Normal\n# distribution over the latent random variable `loc`.\ndef guide(data):\n    guide_loc = numpyro.param(""guide_loc"", 0.)\n    guide_scale = np.exp(numpyro.param(""guide_scale_log"", 0.))\n    numpyro.sample(""loc"", dist.Normal(guide_loc, guide_scale))\n\n\ndef main(args):\n    # Generate some data.\n    data = random.normal(PRNGKey(0), shape=(100,)) + 3.0\n\n    # Construct an SVI object so we can do variational inference on our\n    # model/guide pair.\n    adam = optim.Adam(args.learning_rate)\n\n    svi = SVI(model, guide, adam, ELBO(num_particles=100))\n    svi_state = svi.init(PRNGKey(0), data)\n\n    # Training loop\n    def body_fn(i, val):\n        svi_state, loss = svi.update(val, data)\n        return svi_state\n\n    svi_state = fori_loop(0, args.num_steps, body_fn, svi_state)\n\n    # Report the final values of the variational parameters\n    # in the guide after training.\n    params = svi.get_params(svi_state)\n    for name, value in params.items():\n        print(""{} = {}"".format(name, value))\n\n    # For this simple (conjugate) model we know the exact posterior. In\n    # particular we know that the variational distribution should be\n    # centered near 3.0. So let\'s check this explicitly.\n    assert np.abs(params[""guide_loc""] - 3.0) < 0.1\n\n\nif __name__ == ""__main__"":\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""Mini Pyro demo"")\n    parser.add_argument(""-f"", ""--full-pyro"", action=""store_true"", default=False)\n    parser.add_argument(""-n"", ""--num-steps"", default=1001, type=int)\n    parser.add_argument(""-lr"", ""--learning-rate"", default=0.02, type=float)\n    args = parser.parse_args()\n    main(args)\n'"
examples/neutra.py,9,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nNeural Transport\n================\n\nThis example illustrates how to use a trained AutoBNAFNormal autoguide to transform a posterior to a\nGaussian-like one. The transform will be used to get better mixing rate for NUTS sampler.\n\n**References:**\n\n    1. Hoffman, M. et al. (2019), ""NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport"",\n       (https://arxiv.org/abs/1903.03704)\n""""""\n\nimport argparse\nimport os\n\nfrom matplotlib.gridspec import GridSpec\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom jax import lax, random\nimport jax.numpy as np\n\nimport numpyro\nfrom numpyro import optim\nfrom numpyro.contrib.autoguide import AutoBNAFNormal\nfrom numpyro.contrib.reparam import NeuTraReparam\nfrom numpyro.diagnostics import print_summary\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.util import logsumexp\nfrom numpyro.infer import ELBO, MCMC, NUTS, SVI\n\n\nclass DualMoonDistribution(dist.Distribution):\n    support = constraints.real_vector\n\n    def __init__(self):\n        super(DualMoonDistribution, self).__init__(event_shape=(2,))\n\n    def sample(self, key, sample_shape=()):\n        # it is enough to return an arbitrary sample with correct shape\n        return np.zeros(sample_shape + self.event_shape)\n\n    def log_prob(self, x):\n        term1 = 0.5 * ((np.linalg.norm(x, axis=-1) - 2) / 0.4) ** 2\n        term2 = -0.5 * ((x[..., :1] + np.array([-2., 2.])) / 0.6) ** 2\n        pe = term1 - logsumexp(term2, axis=-1)\n        return -pe\n\n\ndef dual_moon_model():\n    numpyro.sample(\'x\', DualMoonDistribution())\n\n\ndef main(args):\n    print(""Start vanilla HMC..."")\n    nuts_kernel = NUTS(dual_moon_model)\n    mcmc = MCMC(nuts_kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(random.PRNGKey(0))\n    mcmc.print_summary()\n    vanilla_samples = mcmc.get_samples()[\'x\'].copy()\n\n    guide = AutoBNAFNormal(dual_moon_model, hidden_factors=[args.hidden_factor, args.hidden_factor])\n    svi = SVI(dual_moon_model, guide, optim.Adam(0.003), ELBO())\n    svi_state = svi.init(random.PRNGKey(1))\n\n    print(""Start training guide..."")\n    last_state, losses = lax.scan(lambda state, i: svi.update(state), svi_state, np.zeros(args.num_iters))\n    params = svi.get_params(last_state)\n    print(""Finish training guide. Extract samples..."")\n    guide_samples = guide.sample_posterior(random.PRNGKey(2), params,\n                                           sample_shape=(args.num_samples,))[\'x\'].copy()\n\n    print(""\\nStart NeuTra HMC..."")\n    neutra = NeuTraReparam(guide, params)\n    neutra_model = neutra.reparam(dual_moon_model)\n    nuts_kernel = NUTS(neutra_model)\n    mcmc = MCMC(nuts_kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(random.PRNGKey(3))\n    mcmc.print_summary()\n    zs = mcmc.get_samples(group_by_chain=True)[""auto_shared_latent""]\n    print(""Transform samples into unwarped space..."")\n    samples = neutra.transform_sample(zs)\n    print_summary(samples)\n    zs = zs.reshape(-1, 2)\n    samples = samples[\'x\'].reshape(-1, 2).copy()\n\n    # make plots\n\n    # guide samples (for plotting)\n    guide_base_samples = dist.Normal(np.zeros(2), 1.).sample(random.PRNGKey(4), (1000,))\n    guide_trans_samples = neutra.transform_sample(guide_base_samples)[\'x\']\n\n    x1 = np.linspace(-3, 3, 100)\n    x2 = np.linspace(-3, 3, 100)\n    X1, X2 = np.meshgrid(x1, x2)\n    P = np.exp(DualMoonDistribution().log_prob(np.stack([X1, X2], axis=-1)))\n\n    fig = plt.figure(figsize=(12, 8), constrained_layout=True)\n    gs = GridSpec(2, 3, figure=fig)\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax2 = fig.add_subplot(gs[1, 0])\n    ax3 = fig.add_subplot(gs[0, 1])\n    ax4 = fig.add_subplot(gs[1, 1])\n    ax5 = fig.add_subplot(gs[0, 2])\n    ax6 = fig.add_subplot(gs[1, 2])\n\n    ax1.plot(losses[1000:])\n    ax1.set_title(\'Autoguide training loss\\n(after 1000 steps)\')\n\n    ax2.contourf(X1, X2, P, cmap=\'OrRd\')\n    sns.kdeplot(guide_samples[:, 0], guide_samples[:, 1], n_levels=30, ax=ax2)\n    ax2.set(xlim=[-3, 3], ylim=[-3, 3],\n            xlabel=\'x0\', ylabel=\'x1\', title=\'Posterior using\\nAutoBNAFNormal guide\')\n\n    sns.scatterplot(guide_base_samples[:, 0], guide_base_samples[:, 1], ax=ax3,\n                    hue=guide_trans_samples[:, 0] < 0.)\n    ax3.set(xlim=[-3, 3], ylim=[-3, 3],\n            xlabel=\'x0\', ylabel=\'x1\', title=\'AutoBNAFNormal base samples\\n(True=left moon; False=right moon)\')\n\n    ax4.contourf(X1, X2, P, cmap=\'OrRd\')\n    sns.kdeplot(vanilla_samples[:, 0], vanilla_samples[:, 1], n_levels=30, ax=ax4)\n    ax4.plot(vanilla_samples[-50:, 0], vanilla_samples[-50:, 1], \'bo-\', alpha=0.5)\n    ax4.set(xlim=[-3, 3], ylim=[-3, 3],\n            xlabel=\'x0\', ylabel=\'x1\', title=\'Posterior using\\nvanilla HMC sampler\')\n\n    sns.scatterplot(zs[:, 0], zs[:, 1], ax=ax5, hue=samples[:, 0] < 0.,\n                    s=30, alpha=0.5, edgecolor=""none"")\n    ax5.set(xlim=[-5, 5], ylim=[-5, 5],\n            xlabel=\'x0\', ylabel=\'x1\', title=\'Samples from the\\nwarped posterior - p(z)\')\n\n    ax6.contourf(X1, X2, P, cmap=\'OrRd\')\n    sns.kdeplot(samples[:, 0], samples[:, 1], n_levels=30, ax=ax6)\n    ax6.plot(samples[-50:, 0], samples[-50:, 1], \'bo-\', alpha=0.2)\n    ax6.set(xlim=[-3, 3], ylim=[-3, 3],\n            xlabel=\'x0\', ylabel=\'x1\', title=\'Posterior using\\nNeuTra HMC sampler\')\n\n    plt.savefig(""neutra.pdf"")\n\n\nif __name__ == ""__main__"":\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""NeuTra HMC"")\n    parser.add_argument(\'-n\', \'--num-samples\', nargs=\'?\', default=4000, type=int)\n    parser.add_argument(\'--num-warmup\', nargs=\'?\', default=1000, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=1, type=int)\n    parser.add_argument(\'--hidden-factor\', nargs=\'?\', default=8, type=int)\n    parser.add_argument(\'--num-iters\', nargs=\'?\', default=10000, type=int)\n    parser.add_argument(\'--device\', default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/ode.py,10,"b'""""""\nPredator-Prey Model\n===================\n\nThis example replicates the great case study [1], which leverages the Lotka-Volterra\nequation [2] to describe the dynamics of Canada lynx (predator) and snowshoe hare\n(prey) populations. We will use the dataset obtained from [3] and run MCMC to get\ninferences about parameters of the differential equation governing the dynamics.\n\n**References:**\n\n    1. Bob Carpenter (2018), `""Predator-Prey Population Dynamics: the Lotka-Volterra model in Stan""\n       <https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html/>`_.\n    2. https://en.wikipedia.org/wiki/Lotka\xe2\x80\x93Volterra_equations\n    3. http://people.whitman.edu/~hundledr/courses/M250F03/M250.html\n""""""\n\nimport argparse\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom jax.experimental.ode import odeint\nimport jax.numpy as np\nfrom jax.random import PRNGKey\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.examples.datasets import LYNXHARE, load_dataset\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\nmatplotlib.use(\'Agg\')  # noqa: E402\n\n\ndef dz_dt(z, t, theta):\n    """"""\n    Lotka\xe2\x80\x93Volterra equations. Real positive parameters `alpha`, `beta`, `gamma`, `delta`\n    describes the interaction of two species.\n    """"""\n    u = z[0]\n    v = z[1]\n    alpha, beta, gamma, delta = theta[..., 0], theta[..., 1], theta[..., 2], theta[..., 3]\n    du_dt = (alpha - beta * v) * u\n    dv_dt = (-gamma + delta * u) * v\n    return np.stack([du_dt, dv_dt])\n\n\ndef model(N, y=None):\n    """"""\n    :param int N: number of measurement times\n    :param numpy.ndarray y: measured populations with shape (N, 2)\n    """"""\n    # initial population\n    z_init = numpyro.sample(""z_init"", dist.LogNormal(np.log(10), 1), sample_shape=(2,))\n    # measurement times\n    ts = np.arange(float(N))\n    # parameters alpha, beta, gamma, delta of dz_dt\n    theta = numpyro.sample(\n        ""theta"",\n        dist.TruncatedNormal(low=0., loc=np.array([0.5, 0.05, 1.5, 0.05]),\n                             scale=np.array([0.5, 0.05, 0.5, 0.05])))\n    # integrate dz/dt, the result will have shape N x 2\n    z = odeint(dz_dt, z_init, ts, theta, rtol=1e-5, atol=1e-3, mxstep=500)\n    # measurement errors, we expect that measured hare has larger error than measured lynx\n    sigma = numpyro.sample(""sigma"", dist.Exponential(np.array([1, 2])))\n    # measured populations (in log scale)\n    numpyro.sample(""y"", dist.Normal(np.log(z), sigma), obs=y)\n\n\ndef main(args):\n    _, fetch = load_dataset(LYNXHARE, shuffle=False)\n    year, data = fetch()  # data is in hare -> lynx order\n\n    # use dense_mass for better mixing rate\n    mcmc = MCMC(NUTS(model, dense_mass=True),\n                args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(PRNGKey(1), N=data.shape[0], y=np.log(data))\n    mcmc.print_summary()\n\n    # predict populations\n    y_pred = Predictive(model, mcmc.get_samples())(PRNGKey(2), data.shape[0])[""y""]\n    pop_pred = np.exp(y_pred)\n    mu, pi = np.mean(pop_pred, 0), np.percentile(pop_pred, (10, 90), 0)\n    plt.plot(year, data[:, 0], ""ko"", mfc=""none"", ms=4, label=""true hare"", alpha=0.67)\n    plt.plot(year, data[:, 1], ""bx"", label=""true lynx"")\n    plt.plot(year, mu[:, 0], ""k-."", label=""pred hare"", lw=1, alpha=0.67)\n    plt.plot(year, mu[:, 1], ""b--"", label=""pred lynx"")\n    plt.fill_between(year, pi[0, :, 0], pi[1, :, 0], color=""k"", alpha=0.2)\n    plt.fill_between(year, pi[0, :, 1], pi[1, :, 1], color=""b"", alpha=0.3)\n    plt.gca().set(ylim=(0, 160), xlabel=""year"", ylabel=""population (in thousands)"")\n    plt.title(""Posterior predictive (80% CI) with predator-prey pattern."")\n    plt.legend()\n\n    plt.savefig(""ode_plot.pdf"")\n    plt.tight_layout()\n\n\nif __name__ == \'__main__\':\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=\'Predator-Prey Model\')\n    parser.add_argument(\'-n\', \'--num-samples\', nargs=\'?\', default=1000, type=int)\n    parser.add_argument(\'--num-warmup\', nargs=\'?\', default=1000, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=1, type=int)\n    parser.add_argument(\'--device\', default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/proportion_test.py,9,"b'""""""\nProportion Test\n===============\nYou are managing a business and want to test if calling your customers will\nincrease their chance of making a purchase. You get 100,000 customer records and call\nroughly half of them and record if they make a purchase in the next three months.\nYou do the same for the half that did not get called. After three months, the data is in -\ndid calling help?\n\nThis example answers this question by estimating a logistic regression model where the\ncovariates are whether the customer got called and their gender. We place a multivariate\nnormal prior on the regression coefficients. We report the 95% highest posterior\ndensity interval for the effect of making a call.\n""""""\n\n\nimport argparse\nimport os\nfrom typing import Tuple\n\nimport jax.numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nfrom jax import random\nfrom jax.scipy.special import expit\nfrom numpyro.diagnostics import hpdi\nfrom numpyro.infer import MCMC, NUTS\n\n\ndef make_dataset(rng_key) -> Tuple[np.ndarray, np.ndarray]:\n    """"""\n    Make simulated dataset where potential customers who get a\n    sales calls have ~2% higher chance of making another purchase.\n    """"""\n    key1, key2, key3 = random.split(rng_key, 3)\n\n    num_calls = 51342\n    num_no_calls = 48658\n\n    made_purchase_got_called = dist.Bernoulli(0.084).sample(key1, sample_shape=(num_calls,))\n    made_purchase_no_calls = dist.Bernoulli(0.061).sample(key2, sample_shape=(num_no_calls,))\n\n    made_purchase = np.concatenate([made_purchase_got_called, made_purchase_no_calls])\n\n    is_female = dist.Bernoulli(0.5).sample(key3, sample_shape=(num_calls + num_no_calls,))\n    got_called = np.concatenate([np.ones(num_calls), np.zeros(num_no_calls)])\n    design_matrix = np.hstack([np.ones((num_no_calls + num_calls, 1)),\n                               got_called.reshape(-1, 1),\n                               is_female.reshape(-1, 1)])\n\n    return design_matrix, made_purchase\n\n\ndef model(design_matrix: np.ndarray, outcome: np.ndarray = None) -> None:\n    """"""\n    Model definition: Log odds of making a purchase is a linear combination\n    of covariates. Specify a Normal prior over regression coefficients.\n    :param design_matrix: Covariates. All categorical variables have been one-hot\n        encoded.\n    :param outcome: Binary response variable. In this case, whether or not the\n        customer made a purchase.\n    """"""\n\n    beta = numpyro.sample(\'coefficients\', dist.MultivariateNormal(loc=0.,\n                                                                  covariance_matrix=np.eye(design_matrix.shape[1])))\n    logits = design_matrix.dot(beta)\n\n    with numpyro.plate(\'data\', design_matrix.shape[0]):\n        numpyro.sample(\'obs\', dist.Bernoulli(logits=logits), obs=outcome)\n\n\ndef print_results(coef: np.ndarray, interval_size: float = 0.95) -> None:\n    """"""\n    Print the confidence interval for the effect size with interval_size\n    probability mass.\n    """"""\n\n    baseline_response = expit(coef[:, 0])\n    response_with_calls = expit(coef[:, 0] + coef[:, 1])\n\n    impact_on_probability = hpdi(response_with_calls - baseline_response, prob=interval_size)\n\n    effect_of_gender = hpdi(coef[:, 2], prob=interval_size)\n\n    print(f""There is a {interval_size * 100}% probability that calling customers ""\n          ""increases the chance they\'ll make a purchase by ""\n          f""{(100 * impact_on_probability[0]):.2} to {(100 * impact_on_probability[1]):.2} percentage points.""\n          )\n\n    print(f""There is a {interval_size * 100}% probability the effect of gender on the log odds of conversion ""\n          f""lies in the interval ({effect_of_gender[0]:.2}, {effect_of_gender[1]:.2f}).""\n          "" Since this interval contains 0, we can conclude gender does not impact the conversion rate."")\n\n\ndef run_inference(design_matrix: np.ndarray, outcome: np.ndarray,\n                  rng_key: np.ndarray,\n                  num_warmup: int,\n                  num_samples: int, num_chains: int,\n                  interval_size: float = 0.95) -> None:\n    """"""\n    Estimate the effect size.\n    """"""\n\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, num_warmup, num_samples, num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(rng_key, design_matrix, outcome)\n\n    # 0th column is intercept (not getting called)\n    # 1st column is effect of getting called\n    # 2nd column is effect of gender (should be none since assigned at random)\n    coef = mcmc.get_samples()[\'coefficients\']\n    print_results(coef, interval_size)\n\n\ndef main(args):\n    rng_key, _ = random.split(random.PRNGKey(3))\n    design_matrix, response = make_dataset(rng_key)\n    run_inference(design_matrix, response, rng_key,\n                  args.num_warmup,\n                  args.num_samples,\n                  args.num_chains,\n                  args.interval_size)\n\n\nif __name__ == \'__main__\':\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=\'Testing whether  \')\n    parser.add_argument(\'-n\', \'--num-samples\', nargs=\'?\', default=500, type=int)\n    parser.add_argument(\'--num-warmup\', nargs=\'?\', default=1500, type=int)\n    parser.add_argument(\'--num-chains\', nargs=\'?\', default=1, type=int)\n    parser.add_argument(\'--interval-size\', nargs=\'?\', default=0.95, type=float)\n    parser.add_argument(\'--device\', default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/sparse_regression.py,67,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nSparse Regression\n=================\n\nWe demonstrate how to do (fully Bayesian) sparse linear regression using the\napproach described in [1]. This approach is particularly suitable for situations\nwith many feature dimensions (large P) but not too many datapoints (small N).\nIn particular we consider a quadratic regressor of the form:\n\n.. math::\n\n    f(X) = \\\\text{constant} + \\\\sum_i \\\\theta_i X_i + \\\\sum_{i<j} \\\\theta_{ij} X_i X_j + \\\\text{observation noise}\n\n**References:**\n\n    1. Raj Agrawal, Jonathan H. Huggins, Brian Trippe, Tamara Broderick (2019),\n       ""The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions"",\n       (https://arxiv.org/abs/1905.06501)\n""""""\n\nimport argparse\nimport itertools\nimport os\nimport time\n\nimport numpy as onp\n\nimport jax\nfrom jax import vmap\nimport jax.numpy as np\nimport jax.random as random\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS\n\n\ndef dot(X, Z):\n    return np.dot(X, Z[..., None])[..., 0]\n\n\n# The kernel that corresponds to our quadratic regressor.\ndef kernel(X, Z, eta1, eta2, c, jitter=1.0e-6):\n    eta1sq = np.square(eta1)\n    eta2sq = np.square(eta2)\n    k1 = 0.5 * eta2sq * np.square(1.0 + dot(X, Z))\n    k2 = -0.5 * eta2sq * dot(np.square(X), np.square(Z))\n    k3 = (eta1sq - eta2sq) * dot(X, Z)\n    k4 = np.square(c) - 0.5 * eta2sq\n    if X.shape == Z.shape:\n        k4 += jitter * np.eye(X.shape[0])\n    return k1 + k2 + k3 + k4\n\n\n# Most of the model code is concerned with constructing the sparsity inducing prior.\ndef model(X, Y, hypers):\n    S, P, N = hypers[\'expected_sparsity\'], X.shape[1], X.shape[0]\n\n    sigma = numpyro.sample(""sigma"", dist.HalfNormal(hypers[\'alpha3\']))\n    phi = sigma * (S / np.sqrt(N)) / (P - S)\n    eta1 = numpyro.sample(""eta1"", dist.HalfCauchy(phi))\n\n    msq = numpyro.sample(""msq"", dist.InverseGamma(hypers[\'alpha1\'], hypers[\'beta1\']))\n    xisq = numpyro.sample(""xisq"", dist.InverseGamma(hypers[\'alpha2\'], hypers[\'beta2\']))\n\n    eta2 = np.square(eta1) * np.sqrt(xisq) / msq\n\n    lam = numpyro.sample(""lambda"", dist.HalfCauchy(np.ones(P)))\n    kappa = np.sqrt(msq) * lam / np.sqrt(msq + np.square(eta1 * lam))\n\n    # sample observation noise\n    var_obs = numpyro.sample(""var_obs"", dist.InverseGamma(hypers[\'alpha_obs\'], hypers[\'beta_obs\']))\n\n    # compute kernel\n    kX = kappa * X\n    k = kernel(kX, kX, eta1, eta2, hypers[\'c\']) + var_obs * np.eye(N)\n    assert k.shape == (N, N)\n\n    # sample Y according to the standard gaussian process formula\n    numpyro.sample(""Y"", dist.MultivariateNormal(loc=np.zeros(X.shape[0]), covariance_matrix=k),\n                   obs=Y)\n\n\n# Compute the mean and variance of coefficient theta_i (where i = dimension) for a\n# MCMC sample of the kernel hyperparameters (eta1, xisq, ...).\n# Compare to theorem 5.1 in reference [1].\ndef compute_singleton_mean_variance(X, Y, dimension, msq, lam, eta1, xisq, c, var_obs):\n    P, N = X.shape[1], X.shape[0]\n\n    probe = np.zeros((2, P))\n    probe = jax.ops.index_update(probe, jax.ops.index[:, dimension], np.array([1.0, -1.0]))\n\n    eta2 = np.square(eta1) * np.sqrt(xisq) / msq\n    kappa = np.sqrt(msq) * lam / np.sqrt(msq + np.square(eta1 * lam))\n\n    kX = kappa * X\n    kprobe = kappa * probe\n\n    k_xx = kernel(kX, kX, eta1, eta2, c) + var_obs * np.eye(N)\n    k_xx_inv = np.linalg.inv(k_xx)\n    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n\n    vec = np.array([0.50, -0.50])\n    mu = np.matmul(k_probeX, np.matmul(k_xx_inv, Y))\n    mu = np.dot(mu, vec)\n\n    var = k_prbprb - np.matmul(k_probeX, np.matmul(k_xx_inv, np.transpose(k_probeX)))\n    var = np.matmul(var, vec)\n    var = np.dot(var, vec)\n\n    return mu, var\n\n\n# Compute the mean and variance of coefficient theta_ij for a MCMC sample of the\n# kernel hyperparameters (eta1, xisq, ...). Compare to theorem 5.1 in reference [1].\ndef compute_pairwise_mean_variance(X, Y, dim1, dim2, msq, lam, eta1, xisq, c, var_obs):\n    P, N = X.shape[1], X.shape[0]\n\n    probe = np.zeros((4, P))\n    probe = jax.ops.index_update(probe, jax.ops.index[:, dim1], np.array([1.0, 1.0, -1.0, -1.0]))\n    probe = jax.ops.index_update(probe, jax.ops.index[:, dim2], np.array([1.0, -1.0, 1.0, -1.0]))\n\n    eta2 = np.square(eta1) * np.sqrt(xisq) / msq\n    kappa = np.sqrt(msq) * lam / np.sqrt(msq + np.square(eta1 * lam))\n\n    kX = kappa * X\n    kprobe = kappa * probe\n\n    k_xx = kernel(kX, kX, eta1, eta2, c) + var_obs * np.eye(N)\n    k_xx_inv = np.linalg.inv(k_xx)\n    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n\n    vec = np.array([0.25, -0.25, -0.25, 0.25])\n    mu = np.matmul(k_probeX, np.matmul(k_xx_inv, Y))\n    mu = np.dot(mu, vec)\n\n    var = k_prbprb - np.matmul(k_probeX, np.matmul(k_xx_inv, np.transpose(k_probeX)))\n    var = np.matmul(var, vec)\n    var = np.dot(var, vec)\n\n    return mu, var\n\n\n# Sample coefficients theta from the posterior for a given MCMC sample.\n# The first P returned values are {theta_1, theta_2, ...., theta_P}, while\n# the remaining values are {theta_ij} for i,j in the list `active_dims`,\n# sorted so that i < j.\ndef sample_theta_space(X, Y, active_dims, msq, lam, eta1, xisq, c, var_obs):\n    P, N, M = X.shape[1], X.shape[0], len(active_dims)\n    # the total number of coefficients we return\n    num_coefficients = P + M * (M - 1) // 2\n\n    probe = np.zeros((2 * P + 2 * M * (M - 1), P))\n    vec = np.zeros((num_coefficients, 2 * P + 2 * M * (M - 1)))\n    start1 = 0\n    start2 = 0\n\n    for dim in range(P):\n        probe = jax.ops.index_update(probe, jax.ops.index[start1:start1 + 2, dim], np.array([1.0, -1.0]))\n        vec = jax.ops.index_update(vec, jax.ops.index[start2, start1:start1 + 2], np.array([0.5, -0.5]))\n        start1 += 2\n        start2 += 1\n\n    for dim1 in active_dims:\n        for dim2 in active_dims:\n            if dim1 >= dim2:\n                continue\n            probe = jax.ops.index_update(probe, jax.ops.index[start1:start1 + 4, dim1],\n                                         np.array([1.0, 1.0, -1.0, -1.0]))\n            probe = jax.ops.index_update(probe, jax.ops.index[start1:start1 + 4, dim2],\n                                         np.array([1.0, -1.0, 1.0, -1.0]))\n            vec = jax.ops.index_update(vec, jax.ops.index[start2, start1:start1 + 4],\n                                       np.array([0.25, -0.25, -0.25, 0.25]))\n            start1 += 4\n            start2 += 1\n\n    eta2 = np.square(eta1) * np.sqrt(xisq) / msq\n    kappa = np.sqrt(msq) * lam / np.sqrt(msq + np.square(eta1 * lam))\n\n    kX = kappa * X\n    kprobe = kappa * probe\n\n    k_xx = kernel(kX, kX, eta1, eta2, c) + var_obs * np.eye(N)\n    k_xx_inv = np.linalg.inv(k_xx)\n    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n\n    mu = np.matmul(k_probeX, np.matmul(k_xx_inv, Y))\n    mu = np.sum(mu * vec, axis=-1)\n\n    covar = k_prbprb - np.matmul(k_probeX, np.matmul(k_xx_inv, np.transpose(k_probeX)))\n    covar = np.matmul(vec, np.matmul(covar, np.transpose(vec)))\n    L = np.linalg.cholesky(covar)\n\n    # sample from N(mu, covar)\n    sample = mu + np.matmul(L, onp.random.randn(num_coefficients))\n\n    return sample\n\n\n# Helper function for doing HMC inference\ndef run_inference(model, args, rng_key, X, Y, hypers):\n    start = time.time()\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, args.num_warmup, args.num_samples, num_chains=args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(rng_key, X, Y, hypers)\n    mcmc.print_summary()\n    print(\'\\nMCMC elapsed time:\', time.time() - start)\n    return mcmc.get_samples()\n\n\n# Get the mean and variance of a gaussian mixture\ndef gaussian_mixture_stats(mus, variances):\n    mean_mu = np.mean(mus)\n    mean_var = np.mean(variances) + np.mean(np.square(mus)) - np.square(mean_mu)\n    return mean_mu, mean_var\n\n\n# Create artificial regression dataset where only S out of P feature\n# dimensions contain signal and where there is a single pairwise interaction\n# between the first and second dimensions.\ndef get_data(N=20, S=2, P=10, sigma_obs=0.05):\n    assert S < P and P > 1 and S > 0\n    onp.random.seed(0)\n\n    X = onp.random.randn(N, P)\n    # generate S coefficients with non-negligible magnitude\n    W = 0.5 + 2.5 * onp.random.rand(S)\n    # generate data using the S coefficients and a single pairwise interaction\n    Y = onp.sum(X[:, 0:S] * W, axis=-1) + X[:, 0] * X[:, 1] + sigma_obs * onp.random.randn(N)\n    Y -= np.mean(Y)\n    Y_std = np.std(Y)\n\n    assert X.shape == (N, P)\n    assert Y.shape == (N,)\n\n    return X, Y / Y_std, W / Y_std, 1.0 / Y_std\n\n\n# Helper function for analyzing the posterior statistics for coefficient theta_i\ndef analyze_dimension(samples, X, Y, dimension, hypers):\n    vmap_args = (samples[\'msq\'], samples[\'lambda\'], samples[\'eta1\'], samples[\'xisq\'], samples[\'var_obs\'])\n    mus, variances = vmap(lambda msq, lam, eta1, xisq, var_obs:\n                          compute_singleton_mean_variance(X, Y, dimension, msq, lam,\n                                                          eta1, xisq, hypers[\'c\'], var_obs))(*vmap_args)\n    mean, variance = gaussian_mixture_stats(mus, variances)\n    std = np.sqrt(variance)\n    return mean, std\n\n\n# Helper function for analyzing the posterior statistics for coefficient theta_ij\ndef analyze_pair_of_dimensions(samples, X, Y, dim1, dim2, hypers):\n    vmap_args = (samples[\'msq\'], samples[\'lambda\'], samples[\'eta1\'], samples[\'xisq\'], samples[\'var_obs\'])\n    mus, variances = vmap(lambda msq, lam, eta1, xisq, var_obs:\n                          compute_pairwise_mean_variance(X, Y, dim1, dim2, msq, lam,\n                                                         eta1, xisq, hypers[\'c\'], var_obs))(*vmap_args)\n    mean, variance = gaussian_mixture_stats(mus, variances)\n    std = np.sqrt(variance)\n    return mean, std\n\n\ndef main(args):\n    X, Y, expected_thetas, expected_pairwise = get_data(N=args.num_data, P=args.num_dimensions,\n                                                        S=args.active_dimensions)\n\n    # setup hyperparameters\n    hypers = {\'expected_sparsity\': max(1.0, args.num_dimensions / 10),\n              \'alpha1\': 3.0, \'beta1\': 1.0,\n              \'alpha2\': 3.0, \'beta2\': 1.0,\n              \'alpha3\': 1.0, \'c\': 1.0,\n              \'alpha_obs\': 3.0, \'beta_obs\': 1.0}\n\n    # do inference\n    rng_key = random.PRNGKey(0)\n    samples = run_inference(model, args, rng_key, X, Y, hypers)\n\n    # compute the mean and square root variance of each coefficient theta_i\n    means, stds = vmap(lambda dim: analyze_dimension(samples, X, Y, dim, hypers))(np.arange(args.num_dimensions))\n\n    print(""Coefficients theta_1 to theta_%d used to generate the data:"" % args.active_dimensions, expected_thetas)\n    print(""The single quadratic coefficient theta_{1,2} used to generate the data:"", expected_pairwise)\n    active_dimensions = []\n\n    for dim, (mean, std) in enumerate(zip(means, stds)):\n        # we mark the dimension as inactive if the interval [mean - 3 * std, mean + 3 * std] contains zero\n        lower, upper = mean - 3.0 * std, mean + 3.0 * std\n        inactive = ""inactive"" if lower < 0.0 and upper > 0.0 else ""active""\n        if inactive == ""active"":\n            active_dimensions.append(dim)\n        print(""[dimension %02d/%02d]  %s:\\t%.2e +- %.2e"" % (dim + 1, args.num_dimensions, inactive, mean, std))\n\n    print(""Identified a total of %d active dimensions; expected %d."" % (len(active_dimensions),\n                                                                        args.active_dimensions))\n\n    # Compute the mean and square root variance of coefficients theta_ij for i,j active dimensions.\n    # Note that the resulting numbers are only meaningful for i != j.\n    if len(active_dimensions) > 0:\n        dim_pairs = np.array(list(itertools.product(active_dimensions, active_dimensions)))\n        means, stds = vmap(lambda dim_pair: analyze_pair_of_dimensions(samples, X, Y,\n                                                                       dim_pair[0], dim_pair[1], hypers))(dim_pairs)\n        for dim_pair, mean, std in zip(dim_pairs, means, stds):\n            dim1, dim2 = dim_pair\n            if dim1 >= dim2:\n                continue\n            lower, upper = mean - 3.0 * std, mean + 3.0 * std\n            if not (lower < 0.0 and upper > 0.0):\n                format_str = ""Identified pairwise interaction between dimensions %d and %d: %.2e +- %.2e""\n                print(format_str % (dim1 + 1, dim2 + 1, mean, std))\n\n        # Draw a single sample of coefficients theta from the posterior, where we return all singleton\n        # coefficients theta_i and pairwise coefficients theta_ij for i, j active dimensions. We use the\n        # final MCMC sample obtained from the HMC sampler.\n        thetas = sample_theta_space(X, Y, active_dimensions, samples[\'msq\'][-1], samples[\'lambda\'][-1],\n                                    samples[\'eta1\'][-1], samples[\'xisq\'][-1], hypers[\'c\'], samples[\'var_obs\'][-1])\n        print(""Single posterior sample theta:\\n"", thetas)\n\n\nif __name__ == ""__main__"":\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""Gaussian Process example"")\n    parser.add_argument(""-n"", ""--num-samples"", nargs=""?"", default=1000, type=int)\n    parser.add_argument(""--num-warmup"", nargs=\'?\', default=500, type=int)\n    parser.add_argument(""--num-chains"", nargs=\'?\', default=1, type=int)\n    parser.add_argument(""--num-data"", nargs=\'?\', default=100, type=int)\n    parser.add_argument(""--num-dimensions"", nargs=\'?\', default=20, type=int)\n    parser.add_argument(""--active-dimensions"", nargs=\'?\', default=3, type=int)\n    parser.add_argument(""--device"", default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/stochastic_volatility.py,5,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nStochastic Volatility\n=====================\n\nGenerative model:\n\n.. math::\n\n    \\\\begin{align}\n        \\\\sigma & \\\\sim \\\\text{Exponential}(50) \\\\\\\\\n        \\\\nu & \\\\sim \\\\text{Exponential}(.1) \\\\\\\\\n        s_i & \\\\sim \\\\text{Normal}(s_{i-1}, \\\\sigma^{- 2}) \\\\\\\\\n        r_i & \\\\sim \\\\text{StudentT}(\\\\nu, 0, \\\\exp(s_i))\n    \\\\end{align}\n\nThis example is from PyMC3 [1], which itself is adapted from the original experiment\nfrom [2]. A discussion about translating this in Pyro appears in [3].\n\nWe take this example to illustrate how to use the functional interface `hmc`. However,\nwe recommend readers to use `MCMC` class as in other examples because it is more stable\nand has more features supported.\n\n**References:**\n\n    1. *Stochastic Volatility Model*, https://docs.pymc.io/notebooks/stochastic_volatility.html\n    2. *The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo*,\n       https://arxiv.org/pdf/1111.4246.pdf\n    3. Pyro forum discussion, https://forum.pyro.ai/t/problems-transforming-a-pymc3-model-to-pyro-mcmc/208/14\n""""""\n\nimport argparse\nimport os\n\nimport matplotlib\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\nimport numpy as onp\n\nimport jax.numpy as np\nimport jax.random as random\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.examples.datasets import SP500, load_dataset\nfrom numpyro.infer.mcmc import hmc\nfrom numpyro.infer.util import initialize_model\nfrom numpyro.util import fori_collect\n\nmatplotlib.use(\'Agg\')  # noqa: E402\n\n\ndef model(returns):\n    step_size = numpyro.sample(\'sigma\', dist.Exponential(50.))\n    s = numpyro.sample(\'s\', dist.GaussianRandomWalk(scale=step_size, num_steps=np.shape(returns)[0]))\n    nu = numpyro.sample(\'nu\', dist.Exponential(.1))\n    return numpyro.sample(\'r\', dist.StudentT(df=nu, loc=0., scale=np.exp(s)),\n                          obs=returns)\n\n\ndef print_results(posterior, dates):\n    def _print_row(values, row_name=\'\'):\n        quantiles = [0.2, 0.4, 0.5, 0.6, 0.8]\n        row_name_fmt = \'{:>8}\'\n        header_format = row_name_fmt + \'{:>12}\' * 5\n        row_format = row_name_fmt + \'{:>12.3f}\' * 5\n        columns = [\'(p{})\'.format(q * 100) for q in quantiles]\n        q_values = onp.quantile(values, quantiles, axis=0)\n        print(header_format.format(\'\', *columns))\n        print(row_format.format(row_name, *q_values))\n        print(\'\\n\')\n\n    print(\'=\' * 20, \'sigma\', \'=\' * 20)\n    _print_row(posterior[\'sigma\'])\n    print(\'=\' * 20, \'nu\', \'=\' * 20)\n    _print_row(posterior[\'nu\'])\n    print(\'=\' * 20, \'volatility\', \'=\' * 20)\n    for i in range(0, len(dates), 180):\n        _print_row(np.exp(posterior[\'s\'][:, i]), dates[i])\n\n\ndef main(args):\n    _, fetch = load_dataset(SP500, shuffle=False)\n    dates, returns = fetch()\n    init_rng_key, sample_rng_key = random.split(random.PRNGKey(args.rng_seed))\n    model_info = initialize_model(init_rng_key, model, model_args=(returns,))\n    init_kernel, sample_kernel = hmc(model_info.potential_fn, algo=\'NUTS\')\n    hmc_state = init_kernel(model_info.param_info, args.num_warmup, rng_key=sample_rng_key)\n    hmc_states = fori_collect(args.num_warmup, args.num_warmup + args.num_samples, sample_kernel, hmc_state,\n                              transform=lambda hmc_state: model_info.postprocess_fn(hmc_state.z),\n                              progbar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    print_results(hmc_states, dates)\n\n    fig, ax = plt.subplots(1, 1)\n    dates = mdates.num2date(mdates.datestr2num(dates))\n    ax.plot(dates, returns, lw=0.5)\n    # format the ticks\n    ax.xaxis.set_major_locator(mdates.YearLocator())\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\'%Y\'))\n    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n\n    ax.plot(dates, np.exp(hmc_states[\'s\'].T), \'r\', alpha=0.01)\n    legend = ax.legend([\'returns\', \'volatility\'], loc=\'upper right\')\n    legend.legendHandles[1].set_alpha(0.6)\n    ax.set(xlabel=\'time\', ylabel=\'returns\', title=\'Volatility of S&P500 over time\')\n\n    plt.savefig(""stochastic_volatility_plot.pdf"")\n    plt.tight_layout()\n\n\nif __name__ == ""__main__"":\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""Stochastic Volatility Model"")\n    parser.add_argument(\'-n\', \'--num-samples\', nargs=\'?\', default=600, type=int)\n    parser.add_argument(\'--num-warmup\', nargs=\'?\', default=600, type=int)\n    parser.add_argument(\'--device\', default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    parser.add_argument(\'--rng_seed\', default=21, type=int, help=\'random number generator seed\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n\n    main(args)\n'"
examples/ucbadmit.py,10,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nGeneralized Linear Mixed Models\n===============================\n\nThe UCBadmit data is sourced from the study [1] of gender biased in graduate admissions at\nUC Berkeley in Fall 1973:\n\n.. table:: UCBadmit dataset\n   :align: center\n\n   ====== ====== ============== =======\n    dept   male   applications   admit\n   ====== ====== ============== =======\n     0      1         825         512\n     0      0         108          89\n     1      1         560         353\n     1      0          25          17\n     2      1         325         120\n     2      0         593         202\n     3      1         417         138\n     3      0         375         131\n     4      1         191          53\n     4      0         393          94\n     5      1         373          22\n     5      0         341          24\n   ====== ====== ============== =======\n\nThis example replicates the multilevel model `m_glmm5` at [3], which is used to evaluate whether\nthe data contain evidence of gender biased in admissions accross departments. This is a form of\nGeneralized Linear Mixed Models for binomial regression problem, which models\n\n    - varying intercepts accross departments,\n    - varying slopes (or the effects of being male) accross departments,\n    - correlation between intercepts and slopes,\n\nand uses non-centered parameterization (or whitening).\n\nA more comprehensive explanation for binomial regression and non-centered parameterization can be\nfound in Chapter 10 (Counting and Classification) and Chapter 13 (Adventures in Covariance) of [2].\n\n**References:**\n\n    1. Bickel, P. J., Hammel, E. A., and O\'Connell, J. W. (1975), ""Sex Bias in Graduate Admissions:\n       Data from Berkeley"", Science, 187(4175), 398-404.\n    2. McElreath, R. (2018), ""Statistical Rethinking: A Bayesian Course with Examples in R and Stan"",\n       Chapman and Hall/CRC.\n    3. https://github.com/rmcelreath/rethinking/tree/Experimental#multilevel-model-formulas\n""""""\n\nimport argparse\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as onp\n\nfrom jax import random\nimport jax.numpy as np\nfrom jax.scipy.special import expit\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.examples.datasets import UCBADMIT, load_dataset\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\n\ndef glmm(dept, male, applications, admit=None):\n    v_mu = numpyro.sample(\'v_mu\', dist.Normal(0, np.array([4., 1.])))\n\n    sigma = numpyro.sample(\'sigma\', dist.HalfNormal(np.ones(2)))\n    L_Rho = numpyro.sample(\'L_Rho\', dist.LKJCholesky(2, concentration=2))\n    scale_tril = sigma[..., np.newaxis] * L_Rho\n    # non-centered parameterization\n    num_dept = len(onp.unique(dept))\n    z = numpyro.sample(\'z\', dist.Normal(np.zeros((num_dept, 2)), 1))\n    v = np.dot(scale_tril, z.T).T\n\n    logits = v_mu[0] + v[dept, 0] + (v_mu[1] + v[dept, 1]) * male\n    if admit is None:\n        # we use a Delta site to record probs for predictive distribution\n        probs = expit(logits)\n        numpyro.sample(\'probs\', dist.Delta(probs), obs=probs)\n    numpyro.sample(\'admit\', dist.Binomial(applications, logits=logits), obs=admit)\n\n\ndef run_inference(dept, male, applications, admit, rng_key, args):\n    kernel = NUTS(glmm)\n    mcmc = MCMC(kernel, args.num_warmup, args.num_samples, args.num_chains,\n                progress_bar=False if ""NUMPYRO_SPHINXBUILD"" in os.environ else True)\n    mcmc.run(rng_key, dept, male, applications, admit)\n    return mcmc.get_samples()\n\n\ndef print_results(header, preds, dept, male, probs):\n    columns = [\'Dept\', \'Male\', \'ActualProb\', \'Pred(p25)\', \'Pred(p50)\', \'Pred(p75)\']\n    header_format = \'{:>10} {:>10} {:>10} {:>10} {:>10} {:>10}\'\n    row_format = \'{:>10.0f} {:>10.0f} {:>10.2f} {:>10.2f} {:>10.2f} {:>10.2f}\'\n    quantiles = onp.quantile(preds, [0.25, 0.5, 0.75], axis=0)\n    print(\'\\n\', header, \'\\n\')\n    print(header_format.format(*columns))\n    for i in range(len(dept)):\n        print(row_format.format(dept[i], male[i], probs[i], *quantiles[:, i]), \'\\n\')\n\n\ndef main(args):\n    _, fetch_train = load_dataset(UCBADMIT, split=\'train\', shuffle=False)\n    dept, male, applications, admit = fetch_train()\n    rng_key, rng_key_predict = random.split(random.PRNGKey(1))\n    zs = run_inference(dept, male, applications, admit, rng_key, args)\n    pred_probs = Predictive(glmm, zs)(rng_key_predict, dept, male, applications)[\'probs\']\n    header = \'=\' * 30 + \'glmm - TRAIN\' + \'=\' * 30\n    print_results(header, pred_probs, dept, male, admit / applications)\n\n    # make plots\n    fig, ax = plt.subplots(1, 1)\n\n    ax.plot(range(1, 13), admit / applications, ""o"", ms=7, label=""actual rate"")\n    ax.errorbar(range(1, 13), np.mean(pred_probs, 0), np.std(pred_probs, 0),\n                fmt=""o"", c=""k"", mfc=""none"", ms=7, elinewidth=1, label=r""mean $\\pm$ std"")\n    ax.plot(range(1, 13), np.percentile(pred_probs, 5, 0), ""k+"")\n    ax.plot(range(1, 13), np.percentile(pred_probs, 95, 0), ""k+"")\n    ax.set(xlabel=""cases"", ylabel=""admit rate"", title=""Posterior Predictive Check with 90% CI"")\n    ax.legend()\n\n    plt.savefig(""ucbadmit_plot.pdf"")\n    plt.tight_layout()\n\n\nif __name__ == \'__main__\':\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=\'UCBadmit gender discrimination using HMC\')\n    parser.add_argument(\'-n\', \'--num-samples\', nargs=\'?\', default=2000, type=int)\n    parser.add_argument(\'--num-warmup\', nargs=\'?\', default=500, type=int)\n    parser.add_argument(\'--num-chains\', nargs=\'?\', default=1, type=int)\n    parser.add_argument(\'--device\', default=\'cpu\', type=str, help=\'use ""cpu"" or ""gpu"".\')\n    args = parser.parse_args()\n\n    numpyro.set_platform(args.device)\n    numpyro.set_host_device_count(args.num_chains)\n\n    main(args)\n'"
examples/vae.py,5,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nVariational Autoencoder\n=======================\n""""""\n\nimport argparse\nimport inspect\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\n\nfrom jax import jit, lax, random\nfrom jax.experimental import stax\nimport jax.numpy as np\nfrom jax.random import PRNGKey\n\nimport numpyro\nfrom numpyro import optim\nimport numpyro.distributions as dist\nfrom numpyro.examples.datasets import MNIST, load_dataset\nfrom numpyro.infer import ELBO, SVI\n\nRESULTS_DIR = os.path.abspath(os.path.join(os.path.dirname(inspect.getfile(lambda: None)),\n                              \'.results\'))\nos.makedirs(RESULTS_DIR, exist_ok=True)\n\n\ndef encoder(hidden_dim, z_dim):\n    return stax.serial(\n        stax.Dense(hidden_dim, W_init=stax.randn()), stax.Softplus,\n        stax.FanOut(2),\n        stax.parallel(stax.Dense(z_dim, W_init=stax.randn()),\n                      stax.serial(stax.Dense(z_dim, W_init=stax.randn()), stax.Exp)),\n    )\n\n\ndef decoder(hidden_dim, out_dim):\n    return stax.serial(\n        stax.Dense(hidden_dim, W_init=stax.randn()), stax.Softplus,\n        stax.Dense(out_dim, W_init=stax.randn()), stax.Sigmoid,\n    )\n\n\ndef model(batch, hidden_dim=400, z_dim=100):\n    batch = np.reshape(batch, (batch.shape[0], -1))\n    batch_dim, out_dim = np.shape(batch)\n    decode = numpyro.module(\'decoder\', decoder(hidden_dim, out_dim), (batch_dim, z_dim))\n    z = numpyro.sample(\'z\', dist.Normal(np.zeros((z_dim,)), np.ones((z_dim,))))\n    img_loc = decode(z)\n    return numpyro.sample(\'obs\', dist.Bernoulli(img_loc), obs=batch)\n\n\ndef guide(batch, hidden_dim=400, z_dim=100):\n    batch = np.reshape(batch, (batch.shape[0], -1))\n    batch_dim, out_dim = np.shape(batch)\n    encode = numpyro.module(\'encoder\', encoder(hidden_dim, z_dim), (batch_dim, out_dim))\n    z_loc, z_std = encode(batch)\n    z = numpyro.sample(\'z\', dist.Normal(z_loc, z_std))\n    return z\n\n\n@jit\ndef binarize(rng_key, batch):\n    return random.bernoulli(rng_key, batch).astype(batch.dtype)\n\n\ndef main(args):\n    encoder_nn = encoder(args.hidden_dim, args.z_dim)\n    decoder_nn = decoder(args.hidden_dim, 28 * 28)\n    adam = optim.Adam(args.learning_rate)\n    svi = SVI(model, guide, adam, ELBO(), hidden_dim=args.hidden_dim, z_dim=args.z_dim)\n    rng_key = PRNGKey(0)\n    train_init, train_fetch = load_dataset(MNIST, batch_size=args.batch_size, split=\'train\')\n    test_init, test_fetch = load_dataset(MNIST, batch_size=args.batch_size, split=\'test\')\n    num_train, train_idx = train_init()\n    rng_key, rng_key_binarize, rng_key_init = random.split(rng_key, 3)\n    sample_batch = binarize(rng_key_binarize, train_fetch(0, train_idx)[0])\n    svi_state = svi.init(rng_key_init, sample_batch)\n\n    @jit\n    def epoch_train(svi_state, rng_key):\n        def body_fn(i, val):\n            loss_sum, svi_state = val\n            rng_key_binarize = random.fold_in(rng_key, i)\n            batch = binarize(rng_key_binarize, train_fetch(i, train_idx)[0])\n            svi_state, loss = svi.update(svi_state, batch)\n            loss_sum += loss\n            return loss_sum, svi_state\n\n        return lax.fori_loop(0, num_train, body_fn, (0., svi_state))\n\n    @jit\n    def eval_test(svi_state, rng_key):\n        def body_fun(i, loss_sum):\n            rng_key_binarize = random.fold_in(rng_key, i)\n            batch = binarize(rng_key_binarize, test_fetch(i, test_idx)[0])\n            # FIXME: does this lead to a requirement for an rng_key arg in svi_eval?\n            loss = svi.evaluate(svi_state, batch) / len(batch)\n            loss_sum += loss\n            return loss_sum\n\n        loss = lax.fori_loop(0, num_test, body_fun, 0.)\n        loss = loss / num_test\n        return loss\n\n    def reconstruct_img(epoch, rng_key):\n        img = test_fetch(0, test_idx)[0][0]\n        plt.imsave(os.path.join(RESULTS_DIR, \'original_epoch={}.png\'.format(epoch)), img, cmap=\'gray\')\n        rng_key_binarize, rng_key_sample = random.split(rng_key)\n        test_sample = binarize(rng_key_binarize, img)\n        params = svi.get_params(svi_state)\n        z_mean, z_var = encoder_nn[1](params[\'encoder$params\'], test_sample.reshape([1, -1]))\n        z = dist.Normal(z_mean, z_var).sample(rng_key_sample)\n        img_loc = decoder_nn[1](params[\'decoder$params\'], z).reshape([28, 28])\n        plt.imsave(os.path.join(RESULTS_DIR, \'recons_epoch={}.png\'.format(epoch)), img_loc, cmap=\'gray\')\n\n    for i in range(args.num_epochs):\n        rng_key, rng_key_train, rng_key_test, rng_key_reconstruct = random.split(rng_key, 4)\n        t_start = time.time()\n        num_train, train_idx = train_init()\n        _, svi_state = epoch_train(svi_state, rng_key_train)\n        rng_key, rng_key_test, rng_key_reconstruct = random.split(rng_key, 3)\n        num_test, test_idx = test_init()\n        test_loss = eval_test(svi_state, rng_key_test)\n        reconstruct_img(i, rng_key_reconstruct)\n        print(""Epoch {}: loss = {} ({:.2f} s.)"".format(i, test_loss, time.time() - t_start))\n\n\nif __name__ == \'__main__\':\n    assert numpyro.__version__.startswith(\'0.2.4\')\n    parser = argparse.ArgumentParser(description=""parse args"")\n    parser.add_argument(\'-n\', \'--num-epochs\', default=15, type=int, help=\'number of training epochs\')\n    parser.add_argument(\'-lr\', \'--learning-rate\', default=1.0e-3, type=float, help=\'learning rate\')\n    parser.add_argument(\'-batch-size\', default=128, type=int, help=\'batch size\')\n    parser.add_argument(\'-z-dim\', default=50, type=int, help=\'size of latent\')\n    parser.add_argument(\'-hidden-dim\', default=400, type=int, help=\'size of hidden layer in encoder/decoder networks\')\n    args = parser.parse_args()\n    main(args)\n'"
numpyro/__init__.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpyro import compat, diagnostics, distributions, handlers, infer, optim\nfrom numpyro.distributions.distribution import enable_validation, validation_enabled\nimport numpyro.patch  # noqa: F401\nfrom numpyro.primitives import deterministic, factor, module, param, plate, plate_stack, sample\nfrom numpyro.util import enable_x64, set_host_device_count, set_platform\nfrom numpyro.version import __version__\n\nset_platform('cpu')\n\n\n__all__ = [\n    '__version__',\n    'compat',\n    'deterministic',\n    'diagnostics',\n    'distributions',\n    'enable_x64',\n    'enable_validation',\n    'factor',\n    'handlers',\n    'infer',\n    'module',\n    'optim',\n    'param',\n    'plate',\n    'plate_stack',\n    'sample',\n    'set_host_device_count',\n    'set_platform',\n    'validation_enabled',\n]\n"""
numpyro/diagnostics.py,22,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis provides a small set of utilities in NumPyro that are used to diagnose posterior samples.\n""""""\n\nfrom collections import OrderedDict\nfrom itertools import product\n\nimport numpy as onp\n\nfrom jax import device_get\nfrom jax.tree_util import tree_flatten, tree_map\n\n__all__ = [\n    \'autocorrelation\',\n    \'autocovariance\',\n    \'effective_sample_size\',\n    \'gelman_rubin\',\n    \'hpdi\',\n    \'split_gelman_rubin\',\n    \'print_summary\',\n]\n\n\ndef _compute_chain_variance_stats(x):\n    # compute within-chain variance and variance estimator\n    # input has shape C x N x sample_shape\n    C, N = x.shape[:2]\n    chain_var = x.var(axis=1, ddof=1)\n    var_within = chain_var.mean(axis=0)\n    var_estimator = var_within * (N - 1) / N\n    if x.shape[0] > 1:\n        chain_mean = x.mean(axis=1)\n        var_between = chain_mean.var(axis=0, ddof=1)\n        var_estimator = var_estimator + var_between\n    else:\n        var_within = var_estimator\n    return var_within, var_estimator\n\n\ndef gelman_rubin(x):\n    """"""\n    Computes R-hat over chains of samples ``x``, where the first dimension of\n    ``x`` is chain dimension and the second dimension of ``x`` is draw dimension.\n    It is required that ``x.shape[0] >= 2`` and ``x.shape[1] >= 2``.\n\n    :param numpy.ndarray x: the input array.\n    :return: R-hat of ``x``.\n    :rtype: numpy.ndarray\n    """"""\n    assert x.ndim >= 2\n    assert x.shape[0] >= 2\n    assert x.shape[1] >= 2\n    var_within, var_estimator = _compute_chain_variance_stats(x)\n    with onp.errstate(invalid=""ignore"", divide=""ignore""):\n        rhat = onp.sqrt(var_estimator / var_within)\n    return rhat\n\n\ndef split_gelman_rubin(x):\n    """"""\n    Computes split R-hat over chains of samples ``x``, where the first dimension\n    of ``x`` is chain dimension and the second dimension of ``x`` is draw dimension.\n    It is required that ``x.shape[1] >= 4``.\n\n    :param numpy.ndarray x: the input array.\n    :return: split R-hat of ``x``.\n    :rtype: numpy.ndarray\n    """"""\n    assert x.ndim >= 2\n    assert x.shape[1] >= 4\n\n    N_half = x.shape[1] // 2\n    new_input = onp.concatenate([x[:, :N_half], x[:, -N_half:]], axis=0)\n    split_rhat = gelman_rubin(new_input)\n    return split_rhat\n\n\ndef _fft_next_fast_len(target):\n    # find the smallest number >= N such that the only divisors are 2, 3, 5\n    # works just like scipy.fftpack.next_fast_len\n    if target <= 2:\n        return target\n    while True:\n        m = target\n        while m % 2 == 0:\n            m //= 2\n        while m % 3 == 0:\n            m //= 3\n        while m % 5 == 0:\n            m //= 5\n        if m == 1:\n            return target\n        target += 1\n\n\ndef autocorrelation(x, axis=0):\n    """"""\n    Computes the autocorrelation of samples at dimension ``axis``.\n\n    :param numpy.ndarray x: the input array.\n    :param int axis: the dimension to calculate autocorrelation.\n    :return: autocorrelation of ``x``.\n    :rtype: numpy.ndarray\n    """"""\n    # Ref: https://en.wikipedia.org/wiki/Autocorrelation#Efficient_computation\n    # Adapted from Stan implementation\n    # https://github.com/stan-dev/math/blob/develop/stan/math/prim/mat/fun/autocorrelation.hpp\n    N = x.shape[axis]\n    M = _fft_next_fast_len(N)\n    M2 = 2 * M\n\n    # transpose axis with -1 for Fourier transform\n    x = onp.swapaxes(x, axis, -1)\n\n    # centering x\n    centered_signal = x - x.mean(axis=-1, keepdims=True)\n\n    # Fourier transform\n    freqvec = onp.fft.rfft(centered_signal, n=M2, axis=-1)\n    # take square of magnitude of freqvec (or freqvec x freqvec*)\n    freqvec_gram = freqvec * onp.conjugate(freqvec)\n    # inverse Fourier transform\n    autocorr = onp.fft.irfft(freqvec_gram, n=M2, axis=-1)\n\n    # truncate and normalize the result, then transpose back to original shape\n    autocorr = autocorr[..., :N]\n    autocorr = autocorr / onp.arange(N, 0., -1)\n    with onp.errstate(invalid=""ignore"", divide=""ignore""):\n        autocorr = autocorr / autocorr[..., :1]\n    return onp.swapaxes(autocorr, axis, -1)\n\n\ndef autocovariance(x, axis=0):\n    """"""\n    Computes the autocovariance of samples at dimension ``axis``.\n\n    :param numpy.ndarray x: the input array.\n    :param int axis: the dimension to calculate autocovariance.\n    :return: autocovariance of ``x``.\n    :rtype: numpy.ndarray\n    """"""\n    return autocorrelation(x, axis) * x.var(axis=axis, keepdims=True)\n\n\ndef effective_sample_size(x):\n    """"""\n    Computes effective sample size of input ``x``, where the first dimension of\n    ``x`` is chain dimension and the second dimension of ``x`` is draw dimension.\n\n    **References:**\n\n    1. *Introduction to Markov Chain Monte Carlo*,\n       Charles J. Geyer\n    2. *Stan Reference Manual version 2.18*,\n       Stan Development Team\n\n    :param numpy.ndarray x: the input array.\n    :return: effective sample size of ``x``.\n    :rtype: numpy.ndarray\n    """"""\n    assert x.ndim >= 2\n    assert x.shape[1] >= 2\n\n    # find autocovariance for each chain at lag k\n    gamma_k_c = autocovariance(x, axis=1)\n\n    # find autocorrelation at lag k (from Stan reference)\n    var_within, var_estimator = _compute_chain_variance_stats(x)\n    rho_k = 1. - (var_within - gamma_k_c.mean(axis=0)) / var_estimator\n    # correlation at lag 0 is always 1\n    rho_k[0] = 1.\n\n    # initial positive sequence (formula 1.18 in [1]) applied for autocorrelation\n    Rho_k = rho_k[:-1:2, ...] + rho_k[1::2, ...]\n\n    # initial monotone (decreasing) sequence\n    Rho_init = Rho_k[:1]\n    Rho_k = onp.concatenate(\n        [Rho_init, onp.minimum.accumulate(onp.clip(Rho_k[1:, ...], a_min=0, a_max=None), axis=0)],\n        axis=0\n    )\n\n    tau = -1. + 2. * Rho_k.sum(axis=0)\n    n_eff = onp.prod(x.shape[:2]) / tau\n    return n_eff\n\n\ndef hpdi(x, prob=0.90, axis=0):\n    """"""\n    Computes ""highest posterior density interval"" (HPDI) which is the narrowest\n    interval with probability mass ``prob``.\n\n    :param numpy.ndarray x: the input array.\n    :param float prob: the probability mass of samples within the interval.\n    :param int axis: the dimension to calculate hpdi.\n    :return: quantiles of ``x`` at ``(1 - prob) / 2`` and\n        ``(1 + prob) / 2``.\n    :rtype: numpy.ndarray\n    """"""\n    x = onp.swapaxes(x, axis, 0)\n    sorted_x = onp.sort(x, axis=0)\n    mass = x.shape[0]\n    index_length = int(prob * mass)\n    intervals_left = sorted_x[:(mass - index_length)]\n    intervals_right = sorted_x[index_length:]\n    intervals_length = intervals_right - intervals_left\n    index_start = intervals_length.argmin(axis=0)\n    index_end = index_start + index_length\n    hpd_left = onp.take_along_axis(sorted_x, index_start[None, ...], axis=0)\n    hpd_left = onp.swapaxes(hpd_left, axis, 0)\n    hpd_right = onp.take_along_axis(sorted_x, index_end[None, ...], axis=0)\n    hpd_right = onp.swapaxes(hpd_right, axis, 0)\n    return onp.concatenate([hpd_left, hpd_right], axis=axis)\n\n\ndef summary(samples, prob=0.90, group_by_chain=True):\n    """"""\n    Returns a summary table displaying diagnostics of ``samples`` from the\n    posterior. The diagnostics displayed are mean, standard deviation, median,\n    the 90% Credibility Interval :func:`~numpyro.diagnostics.hpdi`,\n    :func:`~numpyro.diagnostics.effective_sample_size`, and\n    :func:`~numpyro.diagnostics.split_gelman_rubin`.\n\n    :param samples: a collection of input samples with left most dimension is chain\n        dimension and second to left most dimension is draw dimension.\n    :type samples: dict or numpy.ndarray\n    :param float prob: the probability mass of samples within the HPDI interval.\n    :param bool group_by_chain: If True, each variable in `samples` will be treated\n        as having shape `num_chains x num_samples x sample_shape`. Otherwise, the\n        corresponding shape will be `num_samples x sample_shape` (i.e. without\n        chain dimension).\n    """"""\n    if not group_by_chain:\n        samples = tree_map(lambda x: x[None, ...], samples)\n    if not isinstance(samples, dict):\n        samples = {\'Param:{}\'.format(i): v for i, v in enumerate(tree_flatten(samples)[0])}\n\n    summary_dict = {}\n    for name, value in samples.items():\n        value = device_get(value)\n        value_flat = onp.reshape(value, (-1,) + value.shape[2:])\n        mean = value_flat.mean(axis=0)\n        std = value_flat.std(axis=0, ddof=1)\n        median = onp.median(value_flat, axis=0)\n        hpd = hpdi(value_flat, prob=prob)\n        n_eff = effective_sample_size(value)\n        r_hat = split_gelman_rubin(value)\n        hpd_lower = \'{:.1f}%\'.format(50 * (1 - prob))\n        hpd_upper = \'{:.1f}%\'.format(50 * (1 + prob))\n        summary_dict[name] = OrderedDict([(""mean"", mean), (""std"", std), (""median"", median),\n                                          (hpd_lower, hpd[0]), (hpd_upper, hpd[1]),\n                                          (""n_eff"", n_eff), (""r_hat"", r_hat)])\n    return summary_dict\n\n\ndef print_summary(samples, prob=0.90, group_by_chain=True):\n    """"""\n    Prints a summary table displaying diagnostics of ``samples`` from the\n    posterior. The diagnostics displayed are mean, standard deviation, median,\n    the 90% Credibility Interval :func:`~numpyro.diagnostics.hpdi`,\n    :func:`~numpyro.diagnostics.effective_sample_size`, and\n    :func:`~numpyro.diagnostics.split_gelman_rubin`.\n\n    :param samples: a collection of input samples with left most dimension is chain\n        dimension and second to left most dimension is draw dimension.\n    :type samples: dict or numpy.ndarray\n    :param float prob: the probability mass of samples within the HPDI interval.\n    :param bool group_by_chain: If True, each variable in `samples` will be treated\n        as having shape `num_chains x num_samples x sample_shape`. Otherwise, the\n        corresponding shape will be `num_samples x sample_shape` (i.e. without\n        chain dimension).\n    """"""\n    if not group_by_chain:\n        samples = tree_map(lambda x: x[None, ...], samples)\n    if not isinstance(samples, dict):\n        samples = {\'Param:{}\'.format(i): v for i, v in enumerate(tree_flatten(samples)[0])}\n    summary_dict = summary(samples, prob, group_by_chain=True)\n\n    row_names = {k: k + \'[\' + \',\'.join(map(lambda x: str(x - 1), v.shape[2:])) + \']\'\n                 for k, v in samples.items()}\n    max_len = max(max(map(lambda x: len(x), row_names.values())), 10)\n    name_format = \'{:>\' + str(max_len) + \'}\'\n    header_format = name_format + \' {:>9}\' * 7\n    columns = [\'\'] + list(list(summary_dict.values())[0].keys())\n\n    print()\n    print(header_format.format(*columns))\n\n    row_format = name_format + \' {:>9.2f}\' * 7\n    for name, stats_dict in summary_dict.items():\n        shape = stats_dict[""mean""].shape\n        if len(shape) == 0:\n            print(row_format.format(name, *stats_dict.values()))\n        else:\n            for idx in product(*map(range, shape)):\n                idx_str = \'[{}]\'.format(\',\'.join(map(str, idx)))\n                print(row_format.format(name + idx_str, *[v[idx] for v in stats_dict.values()]))\n    print()\n'"
numpyro/handlers.py,8,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nThis provides a small set of effect handlers in NumPyro that are modeled\nafter Pyro\'s `poutine <http://docs.pyro.ai/en/stable/poutine.html>`_ module.\nFor a tutorial on effect handlers more generally, readers are encouraged to\nread `Poutine: A Guide to Programming with Effect Handlers in Pyro\n<http://pyro.ai/examples/effect_handlers.html>`_. These simple effect handlers\ncan be composed together or new ones added to enable implementation of custom\ninference utilities and algorithms.\n\n**Example**\n\nAs an example, we are using :class:`~numpyro.handlers.seed`, :class:`~numpyro.handlers.trace`\nand :class:`~numpyro.handlers.substitute` handlers to define the `log_likelihood` function below.\nWe first create a logistic regression model and sample from the posterior distribution over\nthe regression parameters using :func:`~numpyro.infer.MCMC`. The `log_likelihood` function\nuses effect handlers to run the model by substituting sample sites with values from the posterior\ndistribution and computes the log density for a single data point. The `log_predictive_density`\nfunction computes the log likelihood for each draw from the joint posterior and aggregates the\nresults for all the data points, but does so by using JAX\'s auto-vectorize transform called\n`vmap` so that we do not need to loop over all the data points.\n\n\n\n.. doctest::\n\n   >>> import jax.numpy as np\n   >>> from jax import random, vmap\n   >>> from jax.scipy.special import logsumexp\n   >>> import numpyro\n   >>> import numpyro.distributions as dist\n   >>> from numpyro import handlers\n   >>> from numpyro.infer import MCMC, NUTS\n\n   >>> N, D = 3000, 3\n   >>> def logistic_regression(data, labels):\n   ...     coefs = numpyro.sample(\'coefs\', dist.Normal(np.zeros(D), np.ones(D)))\n   ...     intercept = numpyro.sample(\'intercept\', dist.Normal(0., 10.))\n   ...     logits = np.sum(coefs * data + intercept, axis=-1)\n   ...     return numpyro.sample(\'obs\', dist.Bernoulli(logits=logits), obs=labels)\n\n   >>> data = random.normal(random.PRNGKey(0), (N, D))\n   >>> true_coefs = np.arange(1., D + 1.)\n   >>> logits = np.sum(true_coefs * data, axis=-1)\n   >>> labels = dist.Bernoulli(logits=logits).sample(random.PRNGKey(1))\n\n   >>> num_warmup, num_samples = 1000, 1000\n   >>> mcmc = MCMC(NUTS(model=logistic_regression), num_warmup, num_samples)\n   >>> mcmc.run(random.PRNGKey(2), data, labels)  # doctest: +SKIP\n   sample: 100%|\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88\xe2\x96\x88| 1000/1000 [00:00<00:00, 1252.39it/s, 1 steps of size 5.83e-01. acc. prob=0.85]\n   >>> mcmc.print_summary()  # doctest: +SKIP\n\n\n                      mean         sd       5.5%      94.5%      n_eff       Rhat\n       coefs[0]       0.96       0.07       0.85       1.07     455.35       1.01\n       coefs[1]       2.05       0.09       1.91       2.20     332.00       1.01\n       coefs[2]       3.18       0.13       2.96       3.37     320.27       1.00\n      intercept      -0.03       0.02      -0.06       0.00     402.53       1.00\n\n   >>> def log_likelihood(rng_key, params, model, *args, **kwargs):\n   ...     model = handlers.substitute(handlers.seed(model, rng_key), params)\n   ...     model_trace = handlers.trace(model).get_trace(*args, **kwargs)\n   ...     obs_node = model_trace[\'obs\']\n   ...     return obs_node[\'fn\'].log_prob(obs_node[\'value\'])\n\n   >>> def log_predictive_density(rng_key, params, model, *args, **kwargs):\n   ...     n = list(params.values())[0].shape[0]\n   ...     log_lk_fn = vmap(lambda rng_key, params: log_likelihood(rng_key, params, model, *args, **kwargs))\n   ...     log_lk_vals = log_lk_fn(random.split(rng_key, n), params)\n   ...     return np.sum(logsumexp(log_lk_vals, 0) - np.log(n))\n\n   >>> print(log_predictive_density(random.PRNGKey(2), mcmc.get_samples(),\n   ...       logistic_regression, data, labels))  # doctest: +SKIP\n   -874.89813\n""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom collections import OrderedDict\nimport warnings\n\nfrom jax import lax, random\nimport jax.numpy as np\n\nfrom numpyro.primitives import Messenger\nfrom numpyro.util import not_jax_tracer\n\n__all__ = [\n    \'block\',\n    \'condition\',\n    \'replay\',\n    \'scale\',\n    \'seed\',\n    \'substitute\',\n    \'trace\',\n]\n\n\nclass trace(Messenger):\n    """"""\n    Returns a handler that records the inputs and outputs at primitive calls\n    inside `fn`.\n\n    **Example**\n\n    .. doctest::\n\n       >>> from jax import random\n       >>> import numpyro\n       >>> import numpyro.distributions as dist\n       >>> from numpyro.handlers import seed, trace\n       >>> import pprint as pp\n\n       >>> def model():\n       ...     numpyro.sample(\'a\', dist.Normal(0., 1.))\n\n       >>> exec_trace = trace(seed(model, random.PRNGKey(0))).get_trace()\n       >>> pp.pprint(exec_trace)  # doctest: +SKIP\n       OrderedDict([(\'a\',\n                     {\'args\': (),\n                      \'fn\': <numpyro.distributions.continuous.Normal object at 0x7f9e689b1eb8>,\n                      \'is_observed\': False,\n                      \'kwargs\': {\'rng_key\': DeviceArray([0, 0], dtype=uint32)},\n                      \'name\': \'a\',\n                      \'type\': \'sample\',\n                      \'value\': DeviceArray(-0.20584235, dtype=float32)})])\n    """"""\n    def __enter__(self):\n        super(trace, self).__enter__()\n        self.trace = OrderedDict()\n        return self.trace\n\n    def postprocess_message(self, msg):\n        assert not(msg[\'type\'] == \'sample\' and msg[\'name\'] in self.trace), \'all sites must have unique names\'\n        self.trace[msg[\'name\']] = msg.copy()\n\n    def get_trace(self, *args, **kwargs):\n        """"""\n        Run the wrapped callable and return the recorded trace.\n\n        :param `*args`: arguments to the callable.\n        :param `**kwargs`: keyword arguments to the callable.\n        :return: `OrderedDict` containing the execution trace.\n        """"""\n        self(*args, **kwargs)\n        return self.trace\n\n\nclass replay(Messenger):\n    """"""\n    Given a callable `fn` and an execution trace `guide_trace`,\n    return a callable which substitutes `sample` calls in `fn` with\n    values from the corresponding site names in `guide_trace`.\n\n    :param fn: Python callable with NumPyro primitives.\n    :param guide_trace: an OrderedDict containing execution metadata.\n\n    **Example**\n\n    .. doctest::\n\n       >>> from jax import random\n       >>> import numpyro\n       >>> import numpyro.distributions as dist\n       >>> from numpyro.handlers import replay, seed, trace\n\n       >>> def model():\n       ...     numpyro.sample(\'a\', dist.Normal(0., 1.))\n\n       >>> exec_trace = trace(seed(model, random.PRNGKey(0))).get_trace()\n       >>> print(exec_trace[\'a\'][\'value\'])  # doctest: +SKIP\n       -0.20584235\n       >>> replayed_trace = trace(replay(model, exec_trace)).get_trace()\n       >>> print(exec_trace[\'a\'][\'value\'])  # doctest: +SKIP\n       -0.20584235\n       >>> assert replayed_trace[\'a\'][\'value\'] == exec_trace[\'a\'][\'value\']\n    """"""\n    def __init__(self, fn, guide_trace):\n        self.guide_trace = guide_trace\n        super(replay, self).__init__(fn)\n\n    def process_message(self, msg):\n        if msg[\'name\'] in self.guide_trace and msg[\'type\'] in (\'sample\', \'plate\'):\n            msg[\'value\'] = self.guide_trace[msg[\'name\']][\'value\']\n\n\nclass block(Messenger):\n    """"""\n    Given a callable `fn`, return another callable that selectively hides\n    primitive sites  where `hide_fn` returns True from other effect handlers\n    on the stack.\n\n    :param fn: Python callable with NumPyro primitives.\n    :param hide_fn: function which when given a dictionary containing\n        site-level metadata returns whether it should be blocked.\n\n    **Example:**\n\n    .. doctest::\n\n       >>> from jax import random\n       >>> import numpyro\n       >>> from numpyro.handlers import block, seed, trace\n       >>> import numpyro.distributions as dist\n\n       >>> def model():\n       ...     a = numpyro.sample(\'a\', dist.Normal(0., 1.))\n       ...     return numpyro.sample(\'b\', dist.Normal(a, 1.))\n\n       >>> model = seed(model, random.PRNGKey(0))\n       >>> block_all = block(model)\n       >>> block_a = block(model, lambda site: site[\'name\'] == \'a\')\n       >>> trace_block_all = trace(block_all).get_trace()\n       >>> assert not {\'a\', \'b\'}.intersection(trace_block_all.keys())\n       >>> trace_block_a =  trace(block_a).get_trace()\n       >>> assert \'a\' not in trace_block_a\n       >>> assert \'b\' in trace_block_a\n    """"""\n    def __init__(self, fn=None, hide_fn=lambda msg: True):\n        self.hide_fn = hide_fn\n        super(block, self).__init__(fn)\n\n    def process_message(self, msg):\n        if self.hide_fn(msg):\n            msg[\'stop\'] = True\n\n\nclass condition(Messenger):\n    """"""\n    Conditions unobserved sample sites to values from `param_map` or `condition_fn`.\n    Similar to :class:`~numpyro.handlers.substitute` except that it only affects\n    `sample` sites and changes the `is_observed` property to `True`.\n\n    :param fn: Python callable with NumPyro primitives.\n    :param dict param_map: dictionary of `numpy.ndarray` values keyed by\n       site names.\n    :param condition_fn: callable that takes in a site dict and returns\n       a numpy array or `None` (in which case the handler has no side\n       effect).\n\n    **Example:**\n\n    .. doctest::\n\n       >>> from jax import random\n       >>> import numpyro\n       >>> from numpyro.handlers import condition, seed, substitute, trace\n       >>> import numpyro.distributions as dist\n\n       >>> def model():\n       ...     numpyro.sample(\'a\', dist.Normal(0., 1.))\n\n       >>> model = seed(model, random.PRNGKey(0))\n       >>> exec_trace = trace(condition(model, {\'a\': -1})).get_trace()\n       >>> assert exec_trace[\'a\'][\'value\'] == -1\n       >>> assert exec_trace[\'a\'][\'is_observed\']\n    """"""\n    def __init__(self, fn=None, param_map=None, condition_fn=None):\n        self.condition_fn = condition_fn\n        self.param_map = param_map\n        if sum((x is not None for x in (param_map, condition_fn))) != 1:\n            raise ValueError(\'Only one of `param_map` or `condition_fn` \'\n                             \'should be provided.\')\n        super(condition, self).__init__(fn)\n\n    def process_message(self, msg):\n        if msg[\'type\'] != \'sample\':\n            return\n\n        if self.param_map is not None:\n            value = self.param_map.get(msg[\'name\'])\n        else:\n            value = self.condition_fn(msg)\n\n        if value is not None:\n            msg[\'value\'] = value\n            if msg[\'is_observed\']:\n                raise ValueError(""Cannot condition an already observed site: {}."".format(msg[\'name\']))\n            msg[\'is_observed\'] = True\n\n\nclass mask(Messenger):\n    """"""\n    This messenger masks out some of the sample statements elementwise.\n\n    :param mask_array: a DeviceArray with `bool` dtype for masking elementwise masking\n        of sample sites.\n    """"""\n    def __init__(self, fn=None, mask_array=True):\n        if lax.dtype(mask_array) != \'bool\':\n            raise ValueError(""`mask` should be a bool array."")\n        self.mask = mask_array\n        super(mask, self).__init__(fn)\n\n    def process_message(self, msg):\n        if msg[\'type\'] != \'sample\':\n            return\n\n        msg[\'mask\'] = self.mask if msg[\'mask\'] is None else self.mask & msg[\'mask\']\n\n\nclass scale(Messenger):\n    """"""\n    This messenger rescales the log probability score.\n\n    This is typically used for data subsampling or for stratified sampling of data\n    (e.g. in fraud detection where negatives vastly outnumber positives).\n\n    :param float scale_factor: a positive scaling factor\n    """"""\n    def __init__(self, fn=None, scale_factor=1.):\n        if not_jax_tracer(scale_factor):\n            if scale_factor <= 0:\n                raise ValueError(""scale factor should be a positive number."")\n        self.scale = scale_factor\n        super(scale, self).__init__(fn)\n\n    def process_message(self, msg):\n        if msg[\'type\'] not in (\'sample\', \'plate\'):\n            return\n\n        msg[""scale""] = self.scale if msg.get(\'scale\') is None else self.scale * msg[\'scale\']\n\n\nclass seed(Messenger):\n    """"""\n    JAX uses a functional pseudo random number generator that requires passing\n    in a seed :func:`~jax.random.PRNGKey` to every stochastic function. The\n    `seed` handler allows us to initially seed a stochastic function with a\n    :func:`~jax.random.PRNGKey`. Every call to the :func:`~numpyro.handlers.sample`\n    primitive inside the function results in a splitting of this initial seed\n    so that we use a fresh seed for each subsequent call without having to\n    explicitly pass in a `PRNGKey` to each `sample` call.\n\n    :param fn: Python callable with NumPyro primitives.\n    :param rng_seed: a random number generator seed.\n    :type rng_seed: int, np.ndarray scalar, or jax.random.PRNGKey\n\n    .. note::\n\n        Unlike in Pyro, `numpyro.sample` primitive cannot be used without wrapping\n        it in seed handler since there is no global random state. As such,\n        users need to use `seed` as a contextmanager to generate samples from\n        distributions or as a decorator for their model callable (See below).\n\n    **Example:**\n\n    .. doctest::\n\n       >>> from jax import random\n       >>> import numpyro\n       >>> import numpyro.handlers\n       >>> import numpyro.distributions as dist\n\n       >>> # as context manager\n       >>> with handlers.seed(rng_seed=1):\n       ...     x = numpyro.sample(\'x\', dist.Normal(0., 1.))\n\n       >>> def model():\n       ...     return numpyro.sample(\'y\', dist.Normal(0., 1.))\n\n       >>> # as function decorator (/modifier)\n       >>> y = handlers.seed(model, rng_seed=1)()\n       >>> assert x == y\n    """"""\n    def __init__(self, fn=None, rng_seed=None, rng=None):\n        if rng is not None:\n            warnings.warn(\'`rng` argument is deprecated and renamed to `rng_seed` instead.\', DeprecationWarning)\n            rng_seed = rng\n        if isinstance(rng_seed, int) or (isinstance(rng_seed, np.ndarray) and not np.shape(rng_seed)):\n            rng_seed = random.PRNGKey(rng_seed)\n        if not (isinstance(rng_seed, np.ndarray) and rng_seed.dtype == np.uint32 and rng_seed.shape == (2,)):\n            raise TypeError(\'Incorrect type for rng_seed: {}\'.format(type(rng_seed)))\n        self.rng_key = rng_seed\n        super(seed, self).__init__(fn)\n\n    def process_message(self, msg):\n        if msg[\'type\'] == \'sample\' and not msg[\'is_observed\'] and \\\n                msg[\'kwargs\'][\'rng_key\'] is None:\n            self.rng_key, rng_key_sample = random.split(self.rng_key)\n            msg[\'kwargs\'][\'rng_key\'] = rng_key_sample\n\n\nclass substitute(Messenger):\n    """"""\n    Given a callable `fn` and a dict `param_map` keyed by site names\n    (alternatively, a callable `substitute_fn`), return a callable\n    which substitutes all primitive calls in `fn` with values from\n    `param_map` whose key matches the site name. If the site name\n    is not present in `param_map`, there is no side effect.\n\n    If a `substitute_fn` is provided, then the value at the site is\n    replaced by the value returned from the call to `substitute_fn`\n    for the given site.\n\n    :param fn: Python callable with NumPyro primitives.\n    :param dict param_map: dictionary of `numpy.ndarray` values keyed by\n        site names.\n    :param substitute_fn: callable that takes in a site dict and returns\n        a numpy array or `None` (in which case the handler has no side\n        effect).\n\n    **Example:**\n\n    .. doctest::\n\n       >>> from jax import random\n       >>> import numpyro\n       >>> from numpyro.handlers import seed, substitute, trace\n       >>> import numpyro.distributions as dist\n\n       >>> def model():\n       ...     numpyro.sample(\'a\', dist.Normal(0., 1.))\n\n       >>> model = seed(model, random.PRNGKey(0))\n       >>> exec_trace = trace(substitute(model, {\'a\': -1})).get_trace()\n       >>> assert exec_trace[\'a\'][\'value\'] == -1\n    """"""\n    def __init__(self, fn=None, param_map=None, substitute_fn=None):\n        self.substitute_fn = substitute_fn\n        self.param_map = param_map\n        if sum((x is not None for x in (param_map, substitute_fn))) != 1:\n            raise ValueError(\'Only one of `param_map` or `substitute_fn` \'\n                             \'should be provided.\')\n        super(substitute, self).__init__(fn)\n\n    def process_message(self, msg):\n        if msg[\'type\'] not in (\'sample\', \'param\'):\n            return\n\n        if self.param_map is not None:\n            value = self.param_map.get(msg[\'name\'])\n        else:\n            value = self.substitute_fn(msg)\n\n        if value is not None:\n            msg[\'value\'] = value\n'"
numpyro/optim.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\nOptimizer classes defined here are light wrappers over the corresponding optimizers\nsourced from :mod:`jax.experimental.optimizers` with an interface that is better\nsuited for working with NumPyro inference algorithms.\n""""""\n\nfrom typing import Callable, Tuple, TypeVar\n\nfrom jax.experimental import optimizers\nimport jax.numpy as np\nfrom jax.tree_util import tree_map\n\n__all__ = [\n    \'Adam\',\n    \'Adagrad\',\n    \'ClippedAdam\',\n    \'Momentum\',\n    \'RMSProp\',\n    \'RMSPropMomentum\',\n    \'SGD\',\n    \'SM3\',\n]\n\n_Params = TypeVar(\'_Params\')\n_OptState = TypeVar(\'_OptState\')\n_IterOptState = Tuple[int, _OptState]\n\n\nclass _NumpyroOptim(object):\n    def __init__(self, optim_fn: Callable, *args, **kwargs) -> None:\n        self.init_fn, self.update_fn, self.get_params_fn = optim_fn(*args, **kwargs)\n\n    def init(self, params: _Params) -> _IterOptState:\n        """"""\n        Initialize the optimizer with parameters designated to be optimized.\n\n        :param params: a collection of numpy arrays.\n        :return: initial optimizer state.\n        """"""\n        opt_state = self.init_fn(params)\n        return np.array(0), opt_state\n\n    def update(self, g: _Params, state: _IterOptState) -> _IterOptState:\n        """"""\n        Gradient update for the optimizer.\n\n        :param g: gradient information for parameters.\n        :param state: current optimizer state.\n        :return: new optimizer state after the update.\n        """"""\n        i, opt_state = state\n        opt_state = self.update_fn(i, g, opt_state)\n        return i + 1, opt_state\n\n    def get_params(self, state: _IterOptState) -> _Params:\n        """"""\n        Get current parameter values.\n\n        :param state: current optimizer state.\n        :return: collection with current value for parameters.\n        """"""\n        _, opt_state = state\n        return self.get_params_fn(opt_state)\n\n\ndef _add_doc(fn):\n    def _wrapped(cls):\n        cls.__doc__ = \'Wrapper class for the JAX optimizer: :func:`~jax.experimental.optimizers.{}`\'\\\n            .format(fn.__name__)\n        return cls\n\n    return _wrapped\n\n\n@_add_doc(optimizers.adam)\nclass Adam(_NumpyroOptim):\n    def __init__(self, *args, **kwargs):\n        super(Adam, self).__init__(optimizers.adam, *args, **kwargs)\n\n\nclass ClippedAdam(_NumpyroOptim):\n    """"""\n    :class:`~numpyro.optim.Adam` optimizer with gradient clipping.\n\n    :param float clip_norm: All gradient values will be clipped between\n        `[-clip_norm, clip_norm]`.\n\n    **Reference:**\n\n    `A Method for Stochastic Optimization`, Diederik P. Kingma, Jimmy Ba\n    https://arxiv.org/abs/1412.6980\n    """"""\n    def __init__(self, *args, clip_norm=10., **kwargs):\n        self.clip_norm = clip_norm\n        super(ClippedAdam, self).__init__(optimizers.adam, *args, **kwargs)\n\n    def update(self, g, state):\n        i, opt_state = state\n        # clip norm\n        g = tree_map(lambda g_: np.clip(g_, a_min=-self.clip_norm, a_max=self.clip_norm), g)\n        opt_state = self.update_fn(i, g, opt_state)\n        return i + 1, opt_state\n\n\n@_add_doc(optimizers.adagrad)\nclass Adagrad(_NumpyroOptim):\n    def __init__(self, *args, **kwargs):\n        super(Adagrad, self).__init__(optimizers.adagrad, *args, **kwargs)\n\n\n@_add_doc(optimizers.momentum)\nclass Momentum(_NumpyroOptim):\n    def __init__(self, *args, **kwargs):\n        super(Momentum, self).__init__(optimizers.momentum, *args, **kwargs)\n\n\n@_add_doc(optimizers.rmsprop)\nclass RMSProp(_NumpyroOptim):\n    def __init__(self, *args, **kwargs):\n        super(RMSProp, self).__init__(optimizers.rmsprop, *args, **kwargs)\n\n\n@_add_doc(optimizers.rmsprop_momentum)\nclass RMSPropMomentum(_NumpyroOptim):\n    def __init__(self, *args, **kwargs):\n        super(RMSPropMomentum, self).__init__(optimizers.rmsprop_momentum, *args, **kwargs)\n\n\n@_add_doc(optimizers.sgd)\nclass SGD(_NumpyroOptim):\n    def __init__(self, *args, **kwargs):\n        super(SGD, self).__init__(optimizers.sgd, *args, **kwargs)\n\n\n@_add_doc(optimizers.sm3)\nclass SM3(_NumpyroOptim):\n    def __init__(self, *args, **kwargs):\n        super(SM3, self).__init__(optimizers.sm3, *args, **kwargs)\n'"
numpyro/patch.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n\ndef patch_dependency(target, root_module):\n    parts = target.split('.')\n    assert parts[0] == root_module.__name__\n    module = root_module\n    for part in parts[1:-1]:\n        module = getattr(module, part)\n    name = parts[-1]\n    old_fn = getattr(module, name)\n    old_fn = getattr(old_fn, '_pyro_unpatched', old_fn)  # ensure patching is idempotent\n\n    def decorator(new_fn):\n        new_fn.__name__ = name\n        new_fn._pyro_unpatched = old_fn\n        setattr(module, name, new_fn)\n        return new_fn\n\n    return decorator\n"""
numpyro/primitives.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\nfrom contextlib import contextmanager, ExitStack\nimport functools\n\nfrom jax import lax\n\nimport numpyro\nfrom numpyro.distributions.discrete import PRNGIdentity\nfrom numpyro.util import identity\n\n_PYRO_STACK = []\n\n\nCondIndepStackFrame = namedtuple(\'CondIndepStackFrame\', [\'name\', \'dim\', \'size\'])\n\n\ndef apply_stack(msg):\n    pointer = 0\n    for pointer, handler in enumerate(reversed(_PYRO_STACK)):\n        handler.process_message(msg)\n        # When a Messenger sets the ""stop"" field of a message,\n        # it prevents any Messengers above it on the stack from being applied.\n        if msg.get(""stop""):\n            break\n    if msg[\'value\'] is None:\n        if msg[\'type\'] == \'sample\':\n            msg[\'value\'], msg[\'intermediates\'] = msg[\'fn\'](*msg[\'args\'],\n                                                           sample_intermediates=True,\n                                                           **msg[\'kwargs\'])\n        else:\n            msg[\'value\'] = msg[\'fn\'](*msg[\'args\'], **msg[\'kwargs\'])\n\n    # A Messenger that sets msg[""stop""] == True also prevents application\n    # of postprocess_message by Messengers above it on the stack\n    # via the pointer variable from the process_message loop\n    for handler in _PYRO_STACK[-pointer-1:]:\n        handler.postprocess_message(msg)\n    return msg\n\n\nclass Messenger(object):\n    def __init__(self, fn=None):\n        if fn is not None and not callable(fn):\n            raise ValueError(""Expected `fn` to be a Python callable object; ""\n                             ""instead found type(fn) = {}."".format(type(fn)))\n        self.fn = fn\n        functools.update_wrapper(self, fn, updated=[])\n\n    def __enter__(self):\n        _PYRO_STACK.append(self)\n\n    def __exit__(self, *args, **kwargs):\n        assert _PYRO_STACK[-1] is self\n        _PYRO_STACK.pop()\n\n    def process_message(self, msg):\n        pass\n\n    def postprocess_message(self, msg):\n        pass\n\n    def __call__(self, *args, **kwargs):\n        with self:\n            return self.fn(*args, **kwargs)\n\n\ndef sample(name, fn, obs=None, rng_key=None, sample_shape=()):\n    """"""\n    Returns a random sample from the stochastic function `fn`. This can have\n    additional side effects when wrapped inside effect handlers like\n    :class:`~numpyro.handlers.substitute`.\n\n    .. note::\n        By design, `sample` primitive is meant to be used inside a NumPyro model.\n        Then :class:`~numpyro.handlers.seed` handler is used to inject a random\n        state to `fn`. In those situations, `rng_key` keyword will take no\n        effect.\n\n    :param str name: name of the sample site.\n    :param fn: a stochastic function that returns a sample.\n    :param numpy.ndarray obs: observed value\n    :param jax.random.PRNGKey rng_key: an optional random key for `fn`.\n    :param sample_shape: Shape of samples to be drawn.\n    :return: sample from the stochastic `fn`.\n    """"""\n    # if there are no active Messengers, we just draw a sample and return it as expected:\n    if not _PYRO_STACK:\n        return fn(rng_key=rng_key, sample_shape=sample_shape)\n\n    # Otherwise, we initialize a message...\n    initial_msg = {\n        \'type\': \'sample\',\n        \'name\': name,\n        \'fn\': fn,\n        \'args\': (),\n        \'kwargs\': {\'rng_key\': rng_key, \'sample_shape\': sample_shape},\n        \'value\': obs,\n        \'mask\': None,\n        \'scale\': None,\n        \'is_observed\': obs is not None,\n        \'intermediates\': [],\n        \'cond_indep_stack\': [],\n    }\n\n    # ...and use apply_stack to send it to the Messengers\n    msg = apply_stack(initial_msg)\n    return msg[\'value\']\n\n\ndef param(name, init_value=None, **kwargs):\n    """"""\n    Annotate the given site as an optimizable parameter for use with\n    :mod:`jax.experimental.optimizers`. For an example of how `param` statements\n    can be used in inference algorithms, refer to :func:`~numpyro.svi.svi`.\n\n    :param str name: name of site.\n    :param numpy.ndarray init_value: initial value specified by the user. Note that\n        the onus of using this to initialize the optimizer is on the user /\n        inference algorithm, since there is no global parameter store in\n        NumPyro.\n    :return: value for the parameter. Unless wrapped inside a\n        handler like :class:`~numpyro.handlers.substitute`, this will simply\n        return the initial value.\n    """"""\n    # if there are no active Messengers, we just draw a sample and return it as expected:\n    if not _PYRO_STACK:\n        return init_value\n\n    # Otherwise, we initialize a message...\n    initial_msg = {\n        \'type\': \'param\',\n        \'name\': name,\n        \'fn\': identity,\n        \'args\': (init_value,),\n        \'kwargs\': kwargs,\n        \'value\': None,\n        \'cond_indep_stack\': [],\n    }\n\n    # ...and use apply_stack to send it to the Messengers\n    msg = apply_stack(initial_msg)\n    return msg[\'value\']\n\n\ndef deterministic(name, value):\n    """"""\n    Used to designate deterministic sites in the model. Note that most effect\n    handlers will not operate on deterministic sites (except\n    :func:`~numpyro.handlers.trace`), so deterministic sites should be\n    side-effect free. The use case for deterministic nodes is to record any\n    values in the model execution trace.\n\n    :param str name: name of the deterministic site.\n    :param numpy.ndarray value: deterministic value to record in the trace.\n    """"""\n    if not _PYRO_STACK:\n        return value\n\n    initial_msg = {\n        \'type\': \'deterministic\',\n        \'name\': name,\n        \'value\': value,\n    }\n\n    # ...and use apply_stack to send it to the Messengers\n    msg = apply_stack(initial_msg)\n    return msg[\'value\']\n\n\ndef module(name, nn, input_shape=None):\n    """"""\n    Declare a :mod:`~jax.experimental.stax` style neural network inside a\n    model so that its parameters are registered for optimization via\n    :func:`~numpyro.primitives.param` statements.\n\n    :param str name: name of the module to be registered.\n    :param tuple nn: a tuple of `(init_fn, apply_fn)` obtained by a :mod:`~jax.experimental.stax`\n        constructor function.\n    :param tuple input_shape: shape of the input taken by the\n        neural network.\n    :return: a `apply_fn` with bound parameters that takes an array\n        as an input and returns the neural network transformed output\n        array.\n    """"""\n    module_key = name + \'$params\'\n    nn_init, nn_apply = nn\n    nn_params = param(module_key)\n    if nn_params is None:\n        if input_shape is None:\n            raise ValueError(\'Valid value for `input_size` needed to initialize.\')\n        rng_key = numpyro.sample(name + \'$rng_key\', PRNGIdentity())\n        _, nn_params = nn_init(rng_key, input_shape)\n        param(module_key, nn_params)\n    return functools.partial(nn_apply, nn_params)\n\n\nclass plate(Messenger):\n    """"""\n    Construct for annotating conditionally independent variables. Within a\n    `plate` context manager, `sample` sites will be automatically broadcasted to\n    the size of the plate. Additionally, a scale factor might be applied by\n    certain inference algorithms if `subsample_size` is specified.\n\n    :param str name: Name of the plate.\n    :param int size: Size of the plate.\n    :param int subsample_size: Optional argument denoting the size of the mini-batch.\n        This can be used to apply a scaling factor by inference algorithms. e.g.\n        when computing ELBO using a mini-batch.\n    :param int dim: Optional argument to specify which dimension in the tensor\n        is used as the plate dim. If `None` (default), the leftmost available dim\n        is allocated.\n    """"""\n    def __init__(self, name, size, subsample_size=None, dim=None):\n        self.name = name\n        self.size = size\n        self.subsample_size = size if subsample_size is None else subsample_size\n        if dim is not None and dim >= 0:\n            raise ValueError(\'dim arg must be negative.\')\n        self.dim = dim\n        self._validate_and_set_dim()\n        super(plate, self).__init__()\n\n    def _validate_and_set_dim(self):\n        msg = {\n            \'type\': \'plate\',\n            \'fn\': identity,\n            \'name\': self.name,\n            \'args\': (None,),\n            \'kwargs\': {},\n            \'value\': None,\n            \'scale\': 1.0,\n            \'cond_indep_stack\': [],\n        }\n        apply_stack(msg)\n        cond_indep_stack = msg[\'cond_indep_stack\']\n        occupied_dims = {f.dim for f in cond_indep_stack}\n        dim = -1\n        while True:\n            if dim not in occupied_dims:\n                break\n            dim -= 1\n        if self.dim is None:\n            self.dim = dim\n        else:\n            assert self.dim not in occupied_dims\n\n    @staticmethod\n    def _get_batch_shape(cond_indep_stack):\n        n_dims = max(-f.dim for f in cond_indep_stack)\n        batch_shape = [1] * n_dims\n        for f in cond_indep_stack:\n            batch_shape[f.dim] = f.size\n        return tuple(batch_shape)\n\n    def process_message(self, msg):\n        if msg[\'type\'] not in (\'sample\', \'plate\'):\n            return\n\n        cond_indep_stack = msg[\'cond_indep_stack\']\n        frame = CondIndepStackFrame(self.name, self.dim, self.subsample_size)\n        cond_indep_stack.append(frame)\n        expected_shape = self._get_batch_shape(cond_indep_stack)\n        if msg[\'type\'] == \'sample\':\n            dist_batch_shape = msg[\'fn\'].batch_shape\n            if \'sample_shape\' in msg[\'kwargs\']:\n                dist_batch_shape = msg[\'kwargs\'][\'sample_shape\'] + dist_batch_shape\n                msg[\'kwargs\'][\'sample_shape\'] = ()\n            overlap_idx = max(len(expected_shape) - len(dist_batch_shape), 0)\n            trailing_shape = expected_shape[overlap_idx:]\n            broadcast_shape = lax.broadcast_shapes(trailing_shape, dist_batch_shape)\n            batch_shape = expected_shape[:overlap_idx] + broadcast_shape\n            msg[\'fn\'] = msg[\'fn\'].expand(batch_shape)\n        if self.size != self.subsample_size:\n            scale = 1. if msg[\'scale\'] is None else msg[\'scale\']\n            msg[\'scale\'] = scale * self.size / self.subsample_size\n\n\n@contextmanager\ndef plate_stack(prefix, sizes, rightmost_dim=-1):\n    """"""\n    Create a contiguous stack of :class:`plate` s with dimensions::\n\n        rightmost_dim - len(sizes), ..., rightmost_dim\n\n    :param str prefix: Name prefix for plates.\n    :param iterable sizes: An iterable of plate sizes.\n    :param int rightmost_dim: The rightmost dim, counting from the right.\n    """"""\n    assert rightmost_dim < 0\n    with ExitStack() as stack:\n        for i, size in enumerate(reversed(sizes)):\n            plate_i = plate(""{}_{}"".format(prefix, i), size, dim=rightmost_dim - i)\n            stack.enter_context(plate_i)\n        yield\n\n\ndef factor(name, log_factor):\n    """"""\n    Factor statement to add arbitrary log probability factor to a\n    probabilistic model.\n\n    :param str name: Name of the trivial sample.\n    :param numpy.ndarray log_factor: A possibly batched log probability factor.\n    """"""\n    unit_dist = numpyro.distributions.distribution.Unit(log_factor)\n    unit_value = unit_dist.sample(None)\n    sample(name, unit_dist, obs=unit_value)\n'"
numpyro/util.py,10,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple, OrderedDict\nfrom contextlib import contextmanager\nimport os\nimport random\nimport re\n\nimport numpy as onp\nimport tqdm\n\nimport jax\nfrom jax import device_put, jit, lax, ops, vmap\nfrom jax.core import Tracer\nfrom jax.dtypes import canonicalize_dtype\nimport jax.numpy as np\nfrom jax.tree_util import tree_flatten, tree_map, tree_unflatten\n\n_DATA_TYPES = {}\n_DISABLE_CONTROL_FLOW_PRIM = False\n\n\ndef set_rng_seed(rng_seed):\n    """"""\n    Initializes internal state for the Python and NumPy random number generators.\n\n    :param int rng_seed: seed for Python and NumPy random states.\n    """"""\n    random.seed(rng_seed)\n    onp.random.seed(rng_seed)\n\n\ndef enable_x64(use_x64=True):\n    """"""\n    Changes the default array type to use 64 bit precision as in NumPy.\n\n    :param bool use_x64: when `True`, JAX arrays will use 64 bits by default;\n        else 32 bits.\n    """"""\n    if not use_x64:\n        use_x64 = os.getenv(\'JAX_ENABLE_X64\', 0)\n    jax.config.update(\'jax_enable_x64\', use_x64)\n\n\ndef set_platform(platform=None):\n    """"""\n    Changes platform to CPU, GPU, or TPU. This utility only takes\n    effect at the beginning of your program.\n\n    :param str platform: either \'cpu\', \'gpu\', or \'tpu\'.\n    """"""\n    if platform is None:\n        platform = os.getenv(\'JAX_PLATFORM_NAME\', \'cpu\')\n    jax.config.update(\'jax_platform_name\', platform)\n\n\ndef set_host_device_count(n):\n    """"""\n    By default, XLA considers all CPU cores as one device. This utility tells XLA\n    that there are `n` host (CPU) devices available to use. As a consequence, this\n    allows parallel mapping in JAX :func:`jax.pmap` to work in CPU platform.\n\n    .. note:: This utility only takes effect at the beginning of your program.\n        Under the hood, this sets the environment variable\n        `XLA_FLAGS=--xla_force_host_platform_device_count=[num_devices]`, where\n        `[num_device]` is the desired number of CPU devices `n`.\n\n    .. warning:: Our understanding of the side effects of using the\n        `xla_force_host_platform_device_count` flag in XLA is incomplete. If you\n        observe some strange phenomenon when using this utility, please let us\n        know through our issue or forum page. More information is available in this\n        `JAX issue <https://github.com/google/jax/issues/1408>`_.\n\n    :param int n: number of CPU devices to use.\n    """"""\n    xla_flags = os.getenv(\'XLA_FLAGS\', \'\').lstrip(\'--\')\n    xla_flags = re.sub(r\'xla_force_host_platform_device_count=.+\\s\', \'\', xla_flags).split()\n    os.environ[\'XLA_FLAGS\'] = \' \'.join([\'--xla_force_host_platform_device_count={}\'.format(n)]\n                                       + xla_flags)\n\n\n@contextmanager\ndef optional(condition, context_manager):\n    """"""\n    Optionally wrap inside `context_manager` if condition is `True`.\n    """"""\n    if condition:\n        with context_manager:\n            yield\n    else:\n        yield\n\n\n@contextmanager\ndef control_flow_prims_disabled():\n    global _DISABLE_CONTROL_FLOW_PRIM\n    stored_flag = _DISABLE_CONTROL_FLOW_PRIM\n    try:\n        _DISABLE_CONTROL_FLOW_PRIM = True\n        yield\n    finally:\n        _DISABLE_CONTROL_FLOW_PRIM = stored_flag\n\n\ndef cond(pred, true_operand, true_fun, false_operand, false_fun):\n    if _DISABLE_CONTROL_FLOW_PRIM:\n        if pred:\n            return true_fun(true_operand)\n        else:\n            return false_fun(false_operand)\n    else:\n        return lax.cond(pred, true_operand, true_fun, false_operand, false_fun)\n\n\ndef while_loop(cond_fun, body_fun, init_val):\n    if _DISABLE_CONTROL_FLOW_PRIM:\n        val = init_val\n        while cond_fun(val):\n            val = body_fun(val)\n        return val\n    else:\n        return lax.while_loop(cond_fun, body_fun, init_val)\n\n\ndef fori_loop(lower, upper, body_fun, init_val):\n    if _DISABLE_CONTROL_FLOW_PRIM:\n        val = init_val\n        for i in range(int(lower), int(upper)):\n            val = body_fun(i, val)\n        return val\n    else:\n        return lax.fori_loop(lower, upper, body_fun, init_val)\n\n\ndef not_jax_tracer(x):\n    """"""\n    Checks if `x` is not an array generated inside `jit`, `pmap`, `vmap`, or `lax_control_flow`.\n    """"""\n    return not isinstance(x, Tracer)\n\n\ndef identity(x, *args, **kwargs):\n    return x\n\n\ndef cached_by(outer_fn, *keys):\n    # Restrict cache size to prevent ref cycles.\n    max_size = 8\n    outer_fn._cache = getattr(outer_fn, \'_cache\', OrderedDict())\n\n    def _wrapped(fn):\n        fn_cache = outer_fn._cache\n        if keys in fn_cache:\n            fn = fn_cache[keys]\n            # update position\n            del fn_cache[keys]\n            fn_cache[keys] = fn\n        else:\n            fn_cache[keys] = fn\n        if len(fn_cache) > max_size:\n            fn_cache.popitem(last=False)\n        return fn\n\n    return _wrapped\n\n\ndef fori_collect(lower, upper, body_fun, init_val, transform=identity,\n                 progbar=True, return_last_val=False, collection_size=None, **progbar_opts):\n    """"""\n    This looping construct works like :func:`~jax.lax.fori_loop` but with the additional\n    effect of collecting values from the loop body. In addition, this allows for\n    post-processing of these samples via `transform`, and progress bar updates.\n    Note that, `progbar=False` will be faster, especially when collecting a\n    lot of samples. Refer to example usage in :func:`~numpyro.infer.mcmc.hmc`.\n\n    :param int lower: the index to start the collective work. In other words,\n        we will skip collecting the first `lower` values.\n    :param int upper: number of times to run the loop body.\n    :param body_fun: a callable that takes a collection of\n        `np.ndarray` and returns a collection with the same shape and\n        `dtype`.\n    :param init_val: initial value to pass as argument to `body_fun`. Can\n        be any Python collection type containing `np.ndarray` objects.\n    :param transform: a callable to post-process the values returned by `body_fn`.\n    :param progbar: whether to post progress bar updates.\n    :param bool return_last_val: If `True`, the last value is also returned.\n        This has the same type as `init_val`.\n    :param int collection_size: Size of the returned collection. If not specified,\n        the size will be ``upper - lower``. If the size is larger than\n        ``upper - lower``, only the top ``upper - lower`` entries will be non-zero.\n    :param `**progbar_opts`: optional additional progress bar arguments. A\n        `diagnostics_fn` can be supplied which when passed the current value\n        from `body_fun` returns a string that is used to update the progress\n        bar postfix. Also a `progbar_desc` keyword argument can be supplied\n        which is used to label the progress bar.\n    :return: collection with the same type as `init_val` with values\n        collected along the leading axis of `np.ndarray` objects.\n    """"""\n    assert lower <= upper\n    collection_size = upper - lower if collection_size is None else collection_size\n    assert collection_size >= upper - lower\n    init_val_flat, unravel_fn = ravel_pytree(transform(init_val))\n\n    @cached_by(fori_collect, body_fun, transform)\n    def _body_fn(i, vals):\n        val, collection, lower_idx = vals\n        val = body_fun(val)\n        i = np.where(i >= lower_idx, i - lower_idx, 0)\n        collection = ops.index_update(collection, i, ravel_pytree(transform(val))[0])\n        return val, collection, lower_idx\n\n    collection = np.zeros((collection_size,) + init_val_flat.shape)\n    if not progbar:\n        last_val, collection, _ = fori_loop(0, upper, _body_fn, (init_val, collection, lower))\n    else:\n        diagnostics_fn = progbar_opts.pop(\'diagnostics_fn\', None)\n        progbar_desc = progbar_opts.pop(\'progbar_desc\', lambda x: \'\')\n\n        vals = (init_val, collection, device_put(lower))\n        if upper == 0:\n            # special case, only compiling\n            jit(_body_fn)(0, vals)\n        else:\n            with tqdm.trange(upper) as t:\n                for i in t:\n                    vals = jit(_body_fn)(i, vals)\n                    t.set_description(progbar_desc(i), refresh=False)\n                    if diagnostics_fn:\n                        t.set_postfix_str(diagnostics_fn(vals[0]), refresh=False)\n\n        last_val, collection, _ = vals\n\n    unravel_collection = vmap(unravel_fn)(collection)\n    return (unravel_collection, last_val) if return_last_val else unravel_collection\n\n\ndef copy_docs_from(source_class, full_text=False):\n    """"""\n    Decorator to copy class and method docs from source to destin class.\n    """"""\n\n    def decorator(destin_class):\n        # This works only in python 3.3+:\n        # if not destin_class.__doc__:\n        #     destin_class.__doc__ = source_class.__doc__\n        for name in dir(destin_class):\n            if name.startswith(\'_\'):\n                continue\n            destin_attr = getattr(destin_class, name)\n            destin_attr = getattr(destin_attr, \'__func__\', destin_attr)\n            source_attr = getattr(source_class, name, None)\n            source_doc = getattr(source_attr, \'__doc__\', None)\n            if source_doc and not getattr(destin_attr, \'__doc__\', None):\n                if full_text or source_doc.startswith(\'See \'):\n                    destin_doc = source_doc\n                else:\n                    destin_doc = \'See :meth:`{}.{}.{}`\'.format(\n                        source_class.__module__, source_class.__name__, name)\n                if isinstance(destin_attr, property):\n                    # Set docs for object properties.\n                    # Since __doc__ is read-only, we need to reset the property\n                    # with the updated doc.\n                    updated_property = property(destin_attr.fget,\n                                                destin_attr.fset,\n                                                destin_attr.fdel,\n                                                destin_doc)\n                    setattr(destin_class, name, updated_property)\n                else:\n                    destin_attr.__doc__ = destin_doc\n        return destin_class\n\n    return decorator\n\n\npytree_metadata = namedtuple(\'pytree_metadata\', [\'flat\', \'shape\', \'size\', \'dtype\'])\n\n\ndef _ravel_list(*leaves):\n    leaves_metadata = tree_map(lambda l: pytree_metadata(\n        np.ravel(l), np.shape(l), np.size(l), canonicalize_dtype(lax.dtype(l))), leaves)\n    leaves_idx = np.cumsum(np.array((0,) + tuple(d.size for d in leaves_metadata)))\n\n    def unravel_list(arr):\n        return [np.reshape(lax.dynamic_slice_in_dim(arr, leaves_idx[i], m.size),\n                           m.shape).astype(m.dtype)\n                for i, m in enumerate(leaves_metadata)]\n\n    flat = np.concatenate([m.flat for m in leaves_metadata]) if leaves_metadata else np.array([])\n    return flat, unravel_list\n\n\ndef ravel_pytree(pytree):\n    leaves, treedef = tree_flatten(pytree)\n    flat, unravel_list = _ravel_list(*leaves)\n\n    def unravel_pytree(arr):\n        return tree_unflatten(treedef, unravel_list(arr))\n\n    return flat, unravel_pytree\n'"
numpyro/version.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n__version__ = '0.2.4'\n"""
scripts/update_headers.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport glob\n\nroot = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\nblacklist = [""/build/"", ""/dist/"", ""/pyro_api.egg""]\nfile_types = [\n    (""*.py"", ""# {}""),\n    (""*.cpp"", ""// {}""),\n]\n\nfor basename, comment in file_types:\n    copyright_line = comment.format(""Copyright Contributors to the Pyro project.\\n"")\n    # See https://spdx.org/ids-how\n    spdx_line = comment.format(""SPDX-License-Identifier: Apache-2.0\\n"")\n\n    filenames = glob.glob(os.path.join(root, ""**"", basename), recursive=True)\n    filenames.sort()\n    filenames = [\n        filename\n        for filename in filenames\n        if not any(word in filename for word in blacklist)\n    ]\n    for filename in filenames:\n        with open(filename) as f:\n            lines = f.readlines()\n\n        # Ignore empty files like __init__.py\n        if all(line.isspace() for line in lines):\n            continue\n\n        # Ensure first few line are copyright notices.\n        lineno = 0\n        if not lines[lineno].startswith(comment.format(""Copyright"")):\n            lines.insert(lineno, copyright_line)\n        else:\n            lines[lineno] = copyright_line\n        lineno += 1\n        while lines[lineno].startswith(comment.format(""Copyright"")):\n            lineno += 1\n\n        # Ensure next line is an SPDX short identifier.\n        if not lines[lineno].startswith(comment.format(""SPDX-License-Identifier"")):\n            lines.insert(lineno, spdx_line)\n        else:\n            lines[lineno] = spdx_line\n        lineno += 1\n\n        # Ensure next line is blank.\n        if not lines[lineno].isspace():\n            lines.insert(lineno, ""\\n"")\n\n        with open(filename, ""w"") as f:\n            f.write("""".join(lines))\n\n        print(""updated {}"".format(filename[len(root) + 1:]))\n'"
test/conftest.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\n\nfrom numpyro.util import set_rng_seed\n\nfrom jax.config import config; config.update('jax_platform_name', 'cpu')  # noqa: E702\n\n\ndef pytest_runtest_setup(item):\n    if 'JAX_ENABLE_x64' in os.environ:\n        config.update('jax_enable_x64', True)\n    set_rng_seed(0)\n"""
test/test_compile.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\nfrom jax import lax, random\nfrom jax.lib import xla_bridge\n\nimport numpyro\nfrom numpyro.contrib.autoguide import AutoDiagonalNormal\nimport numpyro.distributions as dist\nfrom numpyro.infer import ELBO, MCMC, NUTS, SVI\nimport numpyro.optim as optim\n\n\nGLOBAL = {""count"": 0}\n\n\ndef model(deterministic=True):\n    GLOBAL[""count""] += 1\n    x = numpyro.sample(""x"", dist.Normal())\n    if deterministic:\n        numpyro.deterministic(""x_copy"", x)\n\n\n@pytest.mark.parametrize(\'deterministic\', [True, False])\ndef test_mcmc_one_chain(deterministic):\n    GLOBAL[""count""] = 0\n    mcmc = MCMC(NUTS(model), 100, 100)\n    mcmc.run(random.PRNGKey(0), deterministic=deterministic)\n    mcmc.get_samples()\n\n    if deterministic:\n        assert GLOBAL[""count""] == 4\n    else:\n        assert GLOBAL[""count""] == 3\n\n\n@pytest.mark.parametrize(\'deterministic\', [True, False])\n@pytest.mark.skipif(xla_bridge.device_count() < 2, reason=""only one device is available"")\ndef test_mcmc_parallel_chain(deterministic):\n    GLOBAL[""count""] = 0\n    mcmc = MCMC(NUTS(model), 100, 100, num_chains=2)\n    mcmc.run(random.PRNGKey(0), deterministic=deterministic)\n    mcmc.get_samples()\n\n    if deterministic:\n        assert GLOBAL[""count""] == 4\n    else:\n        assert GLOBAL[""count""] == 3\n\n\n@pytest.mark.parametrize(\'deterministic\', [True, False])\ndef test_autoguide(deterministic):\n    GLOBAL[""count""] = 0\n    guide = AutoDiagonalNormal(model)\n    svi = SVI(model, guide, optim.Adam(0.1), ELBO(), deterministic=deterministic)\n    svi_state = svi.init(random.PRNGKey(0))\n    svi_state = lax.fori_loop(0, 100, lambda i, val: svi.update(val)[0], svi_state)\n    params = svi.get_params(svi_state)\n    guide.sample_posterior(random.PRNGKey(1), params, sample_shape=(100,))\n\n    if deterministic:\n        assert GLOBAL[""count""] == 5\n    else:\n        assert GLOBAL[""count""] == 4\n'"
test/test_diagnostics.py,14,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as onp\nfrom numpy.testing import assert_allclose\nimport pytest\nfrom scipy.fftpack import next_fast_len\n\nfrom numpyro.diagnostics import (\n    _fft_next_fast_len,\n    autocorrelation,\n    autocovariance,\n    effective_sample_size,\n    gelman_rubin,\n    hpdi,\n    split_gelman_rubin\n)\n\n\n@pytest.mark.parametrize('statistics, input_shape, output_shape', [\n    (autocorrelation, (10,), (10,)),\n    (autocorrelation, (10, 3), (10, 3)),\n    (autocovariance, (10,), (10,)),\n    (autocovariance, (10, 3), (10, 3)),\n    (hpdi, (10,), (2,)),\n    (hpdi, (10, 3), (2, 3)),\n    (gelman_rubin, (4, 10), ()),\n    (gelman_rubin, (4, 10, 3), (3,)),\n    (split_gelman_rubin, (4, 10), ()),\n    (split_gelman_rubin, (4, 10, 3), (3,)),\n    (effective_sample_size, (4, 10), ()),\n    (effective_sample_size, (4, 10, 3), (3,)),\n])\ndef test_shape(statistics, input_shape, output_shape):\n    x = onp.random.normal(size=input_shape)\n    y = statistics(x)\n    assert y.shape == output_shape\n\n    # test correct batch calculation\n    if x.shape[-1] == 3:\n        for i in range(3):\n            assert_allclose(statistics(x[..., i]), y[..., i])\n\n\n@pytest.mark.parametrize('target', [433, 124, 25, 300, 1, 3, 7])\ndef test_fft_next_fast_len(target):\n    assert _fft_next_fast_len(target) == next_fast_len(target)\n\n\ndef test_hpdi():\n    x = onp.random.normal(size=20000)\n    assert_allclose(hpdi(x, prob=0.8), onp.quantile(x, [0.1, 0.9]), atol=0.01)\n\n    x = onp.random.exponential(size=20000)\n    assert_allclose(hpdi(x, prob=0.2), onp.array([0., 0.22]), atol=0.01)\n\n\ndef test_autocorrelation():\n    x = onp.arange(10.)\n    actual = autocorrelation(x)\n    expected = onp.array([1, 0.78, 0.52, 0.21, -0.13, -0.52, -0.94, -1.4, -1.91, -2.45])\n    assert_allclose(actual, expected, atol=0.01)\n\n\ndef test_autocovariance():\n    x = onp.arange(10.)\n    actual = autocovariance(x)\n    expected = onp.array([8.25, 6.42, 4.25, 1.75, -1.08, -4.25, -7.75, -11.58, -15.75, -20.25])\n    assert_allclose(actual, expected, atol=0.01)\n\n\ndef test_gelman_rubin():\n    # only need to test precision for small data\n    x = onp.empty((2, 10))\n    x[0, :] = onp.arange(10.)\n    x[1, :] = onp.arange(10.) + 1\n\n    r_hat = gelman_rubin(x)\n    assert_allclose(r_hat, 0.98, atol=0.01)\n\n\ndef test_split_gelman_rubin_agree_with_gelman_rubin():\n    x = onp.random.normal(size=(2, 10))\n    r_hat1 = gelman_rubin(x.reshape(2, 2, 5).reshape(4, 5))\n    r_hat2 = split_gelman_rubin(x)\n    assert_allclose(r_hat1, r_hat2)\n\n\ndef test_effective_sample_size():\n    x = onp.arange(1000.).reshape(100, 10)\n    assert_allclose(effective_sample_size(x), 52.64, atol=0.01)\n"""
test/test_distributions.py,233,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\nfrom functools import partial\nimport inspect\nimport os\n\nimport numpy as onp\nfrom numpy.testing import assert_allclose, assert_array_equal\nimport pytest\nimport scipy.stats as osp\n\nimport jax\nfrom jax import grad, jacfwd, lax, vmap\nimport jax.numpy as np\nimport jax.random as random\nfrom jax.scipy.special import logsumexp\n\nfrom numpyro.contrib.nn import AutoregressiveNN\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints, transforms\nfrom numpyro.distributions.discrete import _to_probs_bernoulli, _to_probs_multinom\nfrom numpyro.distributions.flows import InverseAutoregressiveTransform\nfrom numpyro.distributions.transforms import MultivariateAffineTransform, PermuteTransform, PowerTransform, biject_to\nfrom numpyro.distributions.util import (\n    matrix_to_tril_vec,\n    multinomial,\n    poisson,\n    signed_stick_breaking_tril,\n    vec_to_tril_matrix\n)\n\n\ndef _identity(x): return x\n\n\nclass T(namedtuple(\'TestCase\', [\'jax_dist\', \'sp_dist\', \'params\'])):\n    def __new__(cls, jax_dist, *params):\n        sp_dist = None\n        if jax_dist in _DIST_MAP:\n            sp_dist = _DIST_MAP[jax_dist]\n        return super(cls, T).__new__(cls, jax_dist, sp_dist, params)\n\n\ndef _mvn_to_scipy(loc, cov, prec, tril):\n    jax_dist = dist.MultivariateNormal(loc, cov, prec, tril)\n    mean = jax_dist.mean\n    cov = jax_dist.covariance_matrix\n    return osp.multivariate_normal(mean=mean, cov=cov)\n\n\ndef _lowrank_mvn_to_scipy(loc, cov_fac, cov_diag):\n    jax_dist = dist.LowRankMultivariateNormal(loc, cov_fac, cov_diag)\n    mean = jax_dist.mean\n    cov = jax_dist.covariance_matrix\n    return osp.multivariate_normal(mean=mean, cov=cov)\n\n\nclass _ImproperWrapper(dist.ImproperUniform):\n    def sample(self, key, sample_shape=()):\n        transform = biject_to(self.support)\n        prototype_value = np.zeros(self.event_shape)\n        unconstrained_event_shape = np.shape(transform.inv(prototype_value))\n        shape = sample_shape + self.batch_shape + unconstrained_event_shape\n        unconstrained_samples = random.uniform(key, shape,\n                                               minval=-2,\n                                               maxval=2)\n        return transform(unconstrained_samples)\n\n\n_DIST_MAP = {\n    dist.BernoulliProbs: lambda probs: osp.bernoulli(p=probs),\n    dist.BernoulliLogits: lambda logits: osp.bernoulli(p=_to_probs_bernoulli(logits)),\n    dist.Beta: lambda con1, con0: osp.beta(con1, con0),\n    dist.BinomialProbs: lambda probs, total_count: osp.binom(n=total_count, p=probs),\n    dist.BinomialLogits: lambda logits, total_count: osp.binom(n=total_count, p=_to_probs_bernoulli(logits)),\n    dist.Cauchy: lambda loc, scale: osp.cauchy(loc=loc, scale=scale),\n    dist.Chi2: lambda df: osp.chi2(df),\n    dist.Dirichlet: lambda conc: osp.dirichlet(conc),\n    dist.Exponential: lambda rate: osp.expon(scale=np.reciprocal(rate)),\n    dist.Gamma: lambda conc, rate: osp.gamma(conc, scale=1./rate),\n    dist.Gumbel: lambda loc, scale: osp.gumbel_r(loc=loc, scale=scale),\n    dist.HalfCauchy: lambda scale: osp.halfcauchy(scale=scale),\n    dist.HalfNormal: lambda scale: osp.halfnorm(scale=scale),\n    dist.InverseGamma: lambda conc, rate: osp.invgamma(conc, scale=rate),\n    dist.LogNormal: lambda loc, scale: osp.lognorm(s=scale, scale=np.exp(loc)),\n    dist.MultinomialProbs: lambda probs, total_count: osp.multinomial(n=total_count, p=probs),\n    dist.MultinomialLogits: lambda logits, total_count: osp.multinomial(n=total_count,\n                                                                        p=_to_probs_multinom(logits)),\n    dist.MultivariateNormal: _mvn_to_scipy,\n    dist.LowRankMultivariateNormal: _lowrank_mvn_to_scipy,\n    dist.Normal: lambda loc, scale: osp.norm(loc=loc, scale=scale),\n    dist.Pareto: lambda alpha, scale: osp.pareto(alpha, scale=scale),\n    dist.Poisson: lambda rate: osp.poisson(rate),\n    dist.StudentT: lambda df, loc, scale: osp.t(df=df, loc=loc, scale=scale),\n    dist.Uniform: lambda a, b: osp.uniform(a, b - a),\n    dist.Logistic: lambda loc, scale: osp.logistic(loc=loc, scale=scale)\n}\n\n\nCONTINUOUS = [\n    T(dist.Beta, 1., 2.),\n    T(dist.Beta, 1., np.array([2., 2.])),\n    T(dist.Beta, 1., np.array([[1., 1.], [2., 2.]])),\n    T(dist.Chi2, 2.),\n    T(dist.Chi2, np.array([0.3, 1.3])),\n    T(dist.Cauchy, 0., 1.),\n    T(dist.Cauchy, 0., np.array([1., 2.])),\n    T(dist.Cauchy, np.array([0., 1.]), np.array([[1.], [2.]])),\n    T(dist.Dirichlet, np.array([1.7])),\n    T(dist.Dirichlet, np.array([0.2, 1.1])),\n    T(dist.Dirichlet, np.array([[0.2, 1.1], [2., 2.]])),\n    T(dist.Exponential, 2.),\n    T(dist.Exponential, np.array([4., 2.])),\n    T(dist.Gamma, np.array([1.7]), np.array([[2.], [3.]])),\n    T(dist.Gamma, np.array([0.5, 1.3]), np.array([[1.], [3.]])),\n    T(dist.GaussianRandomWalk, 0.1, 10),\n    T(dist.GaussianRandomWalk, np.array([0.1, 0.3, 0.25]), 10),\n    T(dist.Gumbel, 0., 1.),\n    T(dist.Gumbel, 0.5, 2.),\n    T(dist.Gumbel, np.array([0., 0.5]), np.array([1., 2.])),\n    T(dist.HalfCauchy, 1.),\n    T(dist.HalfCauchy, np.array([1., 2.])),\n    T(dist.HalfNormal, 1.),\n    T(dist.HalfNormal, np.array([1., 2.])),\n    T(_ImproperWrapper, constraints.positive, (), (3,)),\n    T(dist.InverseGamma, np.array([1.7]), np.array([[2.], [3.]])),\n    T(dist.InverseGamma, np.array([0.5, 1.3]), np.array([[1.], [3.]])),\n    T(dist.LKJ, 2, 0.5, ""onion""),\n    T(dist.LKJ, 5, np.array([0.5, 1., 2.]), ""cvine""),\n    T(dist.LKJCholesky, 2, 0.5, ""onion""),\n    T(dist.LKJCholesky, 2, 0.5, ""cvine""),\n    T(dist.LKJCholesky, 5, np.array([0.5, 1., 2.]), ""onion""),\n    pytest.param(*T(dist.LKJCholesky, 5, np.array([0.5, 1., 2.]), ""cvine""),\n                 marks=pytest.mark.skipif(\'CI\' in os.environ, reason=""reduce time for Travis"")),\n    pytest.param(*T(dist.LKJCholesky, 3, np.array([[3., 0.6], [0.2, 5.]]), ""onion""),\n                 marks=pytest.mark.skipif(\'CI\' in os.environ, reason=""reduce time for Travis"")),\n    T(dist.LKJCholesky, 3, np.array([[3., 0.6], [0.2, 5.]]), ""cvine""),\n    T(dist.Logistic, 0., 1.),\n    T(dist.Logistic, 1., np.array([1., 2.])),\n    T(dist.Logistic, np.array([0., 1.]), np.array([[1.], [2.]])),\n    T(dist.LogNormal, 1., 0.2),\n    T(dist.LogNormal, -1., np.array([0.5, 1.3])),\n    T(dist.LogNormal, np.array([0.5, -0.7]), np.array([[0.1, 0.4], [0.5, 0.1]])),\n    T(dist.MultivariateNormal, 0., np.array([[1., 0.5], [0.5, 1.]]), None, None),\n    T(dist.MultivariateNormal, np.array([1., 3.]), None, np.array([[1., 0.5], [0.5, 1.]]), None),\n    T(dist.MultivariateNormal, np.array([1., 3.]), None, np.array([[[1., 0.5], [0.5, 1.]]]), None),\n    T(dist.MultivariateNormal, np.array([2.]), None, None, np.array([[1., 0.], [0.5, 1.]])),\n    T(dist.MultivariateNormal, np.arange(6, dtype=np.float32).reshape((3, 2)), None, None,\n      np.array([[1., 0.], [0., 1.]])),\n    T(dist.MultivariateNormal, 0., None, np.broadcast_to(np.identity(3), (2, 3, 3)), None),\n    T(dist.LowRankMultivariateNormal, np.zeros(2), np.array([[1], [0]]), np.array([1, 1])),\n    T(dist.LowRankMultivariateNormal, np.arange(6, dtype=np.float32).reshape((2, 3)),\n      np.arange(6, dtype=np.float32).reshape((3, 2)), np.array([1, 2, 3])),\n    T(dist.Normal, 0., 1.),\n    T(dist.Normal, 1., np.array([1., 2.])),\n    T(dist.Normal, np.array([0., 1.]), np.array([[1.], [2.]])),\n    T(dist.Pareto, 2., 1.),\n    T(dist.Pareto, np.array([0.3, 2.]), np.array([1., 0.5])),\n    T(dist.Pareto, np.array([1., 0.5]), np.array([[1.], [3.]])),\n    T(dist.StudentT, 1., 1., 0.5),\n    T(dist.StudentT, 2., np.array([1., 2.]), 2.),\n    T(dist.StudentT, np.array([3, 5]), np.array([[1.], [2.]]), 2.),\n    T(dist.TruncatedCauchy, -1., 0., 1.),\n    T(dist.TruncatedCauchy, 1., 0., np.array([1., 2.])),\n    T(dist.TruncatedCauchy, np.array([-2., 2.]), np.array([0., 1.]), np.array([[1.], [2.]])),\n    T(dist.TruncatedNormal, -1., 0., 1.),\n    T(dist.TruncatedNormal, 1., -1., np.array([1., 2.])),\n    T(dist.TruncatedNormal, np.array([-2., 2.]), np.array([0., 1.]), np.array([[1.], [2.]])),\n    T(dist.Uniform, 0., 2.),\n    T(dist.Uniform, 1., np.array([2., 3.])),\n    T(dist.Uniform, np.array([0., 0.]), np.array([[2.], [3.]])),\n]\n\n\nDISCRETE = [\n    T(dist.BetaBinomial, 2., 5., 10),\n    T(dist.BetaBinomial, np.array([2., 4.]), np.array([5., 3.]), np.array([10, 12])),\n    T(dist.BernoulliProbs, 0.2),\n    T(dist.BernoulliProbs, np.array([0.2, 0.7])),\n    T(dist.BernoulliLogits, np.array([-1., 3.])),\n    T(dist.BinomialProbs, np.array([0.2, 0.7]), np.array([10, 2])),\n    T(dist.BinomialProbs, np.array([0.2, 0.7]), np.array([5, 8])),\n    T(dist.BinomialLogits, np.array([-1., 3.]), np.array([5, 8])),\n    T(dist.CategoricalProbs, np.array([1.])),\n    T(dist.CategoricalProbs, np.array([0.1, 0.5, 0.4])),\n    T(dist.CategoricalProbs, np.array([[0.1, 0.5, 0.4], [0.4, 0.4, 0.2]])),\n    T(dist.CategoricalLogits, np.array([-5.])),\n    T(dist.CategoricalLogits, np.array([1., 2., -2.])),\n    T(dist.Delta, 1),\n    T(dist.Delta, np.array([0., 2.])),\n    T(dist.Delta, np.array([0., 2.]), np.array([-2., -4.])),\n    T(dist.CategoricalLogits, np.array([[-1, 2., 3.], [3., -4., -2.]])),\n    T(dist.GammaPoisson, 2., 2.),\n    T(dist.GammaPoisson, np.array([6., 2]), np.array([2., 8.])),\n    T(dist.MultinomialProbs, np.array([0.2, 0.7, 0.1]), 10),\n    T(dist.MultinomialProbs, np.array([0.2, 0.7, 0.1]), np.array([5, 8])),\n    T(dist.MultinomialLogits, np.array([-1., 3.]), np.array([[5], [8]])),\n    T(dist.OrderedLogistic, -2, np.array([-10., 4., 9.])),\n    T(dist.OrderedLogistic, np.array([-4, 3, 4, 5]), np.array([-1.5])),\n    T(dist.Poisson, 2.),\n    T(dist.Poisson, np.array([2., 3., 5.])),\n    T(dist.ZeroInflatedPoisson, 0.6, 2.),\n    T(dist.ZeroInflatedPoisson, np.array([0.2, 0.7, 0.3]), np.array([2., 3., 5.])),\n]\n\n\ndef _is_batched_multivariate(jax_dist):\n    return len(jax_dist.event_shape) > 0 and len(jax_dist.batch_shape) > 0\n\n\ndef gen_values_within_bounds(constraint, size, key=random.PRNGKey(11)):\n    eps = 1e-6\n\n    if isinstance(constraint, constraints._Boolean):\n        return random.bernoulli(key, shape=size)\n    elif isinstance(constraint, constraints._GreaterThan):\n        return np.exp(random.normal(key, size)) + constraint.lower_bound + eps\n    elif isinstance(constraint, constraints._IntegerInterval):\n        lower_bound = np.broadcast_to(constraint.lower_bound, size)\n        upper_bound = np.broadcast_to(constraint.upper_bound, size)\n        return random.randint(key, size, lower_bound, upper_bound + 1)\n    elif isinstance(constraint, constraints._IntegerGreaterThan):\n        return constraint.lower_bound + poisson(key, 5, shape=size)\n    elif isinstance(constraint, constraints._Interval):\n        lower_bound = np.broadcast_to(constraint.lower_bound, size)\n        upper_bound = np.broadcast_to(constraint.upper_bound, size)\n        return random.uniform(key, size, minval=lower_bound, maxval=upper_bound)\n    elif isinstance(constraint, (constraints._Real, constraints._RealVector)):\n        return random.normal(key, size)\n    elif isinstance(constraint, constraints._Simplex):\n        return osp.dirichlet.rvs(alpha=np.ones((size[-1],)), size=size[:-1])\n    elif isinstance(constraint, constraints._Multinomial):\n        n = size[-1]\n        return multinomial(key, p=np.ones((n,)) / n, n=constraint.upper_bound, shape=size[:-1])\n    elif isinstance(constraint, constraints._CorrCholesky):\n        return signed_stick_breaking_tril(\n            random.uniform(key, size[:-2] + (size[-1] * (size[-1] - 1) // 2,), minval=-1, maxval=1))\n    elif isinstance(constraint, constraints._CorrMatrix):\n        cholesky = signed_stick_breaking_tril(\n            random.uniform(key, size[:-2] + (size[-1] * (size[-1] - 1) // 2,), minval=-1, maxval=1))\n        return np.matmul(cholesky, np.swapaxes(cholesky, -2, -1))\n    elif isinstance(constraint, constraints._LowerCholesky):\n        return np.tril(random.uniform(key, size))\n    elif isinstance(constraint, constraints._PositiveDefinite):\n        x = random.normal(key, size)\n        return np.matmul(x, np.swapaxes(x, -2, -1))\n    elif isinstance(constraint, constraints._OrderedVector):\n        x = np.cumsum(random.exponential(key, size), -1)\n        return x - random.normal(key, size[:-1])\n    else:\n        raise NotImplementedError(\'{} not implemented.\'.format(constraint))\n\n\ndef gen_values_outside_bounds(constraint, size, key=random.PRNGKey(11)):\n    if isinstance(constraint, constraints._Boolean):\n        return random.bernoulli(key, shape=size) - 2\n    elif isinstance(constraint, constraints._GreaterThan):\n        return constraint.lower_bound - np.exp(random.normal(key, size))\n    elif isinstance(constraint, constraints._IntegerInterval):\n        lower_bound = np.broadcast_to(constraint.lower_bound, size)\n        return random.randint(key, size, lower_bound - 1, lower_bound)\n    elif isinstance(constraint, constraints._IntegerGreaterThan):\n        return constraint.lower_bound - poisson(key, 5, shape=size)\n    elif isinstance(constraint, constraints._Interval):\n        upper_bound = np.broadcast_to(constraint.upper_bound, size)\n        return random.uniform(key, size, minval=upper_bound, maxval=upper_bound + 1.)\n    elif isinstance(constraint, (constraints._Real, constraints._RealVector)):\n        return lax.full(size, np.nan)\n    elif isinstance(constraint, constraints._Simplex):\n        return osp.dirichlet.rvs(alpha=np.ones((size[-1],)), size=size[:-1]) + 1e-2\n    elif isinstance(constraint, constraints._Multinomial):\n        n = size[-1]\n        return multinomial(key, p=np.ones((n,)) / n, n=constraint.upper_bound, shape=size[:-1]) + 1\n    elif isinstance(constraint, constraints._CorrCholesky):\n        return signed_stick_breaking_tril(\n            random.uniform(key, size[:-2] + (size[-1] * (size[-1] - 1) // 2,),\n                           minval=-1, maxval=1)) + 1e-2\n    elif isinstance(constraint, constraints._CorrMatrix):\n        cholesky = 1e-2 + signed_stick_breaking_tril(\n            random.uniform(key, size[:-2] + (size[-1] * (size[-1] - 1) // 2,), minval=-1, maxval=1))\n        return np.matmul(cholesky, np.swapaxes(cholesky, -2, -1))\n    elif isinstance(constraint, constraints._LowerCholesky):\n        return random.uniform(key, size)\n    elif isinstance(constraint, constraints._PositiveDefinite):\n        return random.normal(key, size)\n    elif isinstance(constraint, constraints._OrderedVector):\n        x = np.cumsum(random.exponential(key, size), -1)\n        return x[..., ::-1]\n    else:\n        raise NotImplementedError(\'{} not implemented.\'.format(constraint))\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS + DISCRETE)\n@pytest.mark.parametrize(\'prepend_shape\', [\n    (),\n    (2,),\n    (2, 3),\n])\ndef test_dist_shape(jax_dist, sp_dist, params, prepend_shape):\n    jax_dist = jax_dist(*params)\n    rng_key = random.PRNGKey(0)\n    expected_shape = prepend_shape + jax_dist.batch_shape + jax_dist.event_shape\n    samples = jax_dist.sample(key=rng_key, sample_shape=prepend_shape)\n    assert isinstance(samples, jax.interpreters.xla.DeviceArray)\n    assert np.shape(samples) == expected_shape\n    if sp_dist and not _is_batched_multivariate(jax_dist):\n        sp_dist = sp_dist(*params)\n        sp_samples = sp_dist.rvs(size=prepend_shape + jax_dist.batch_shape)\n        assert np.shape(sp_samples) == expected_shape\n    if isinstance(jax_dist, dist.MultivariateNormal):\n        assert jax_dist.covariance_matrix.ndim == len(jax_dist.batch_shape) + 2\n        assert_allclose(jax_dist.precision_matrix, np.linalg.inv(jax_dist.covariance_matrix), rtol=1e-6)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [(), (4,), (3, 2)])\ndef test_unit(batch_shape):\n    log_factor = random.normal(random.PRNGKey(0), batch_shape)\n\n    d = dist.Unit(log_factor=log_factor)\n    x = d.sample(random.PRNGKey(1))\n    assert x.shape == batch_shape + (0,)\n    assert (d.log_prob(x) == log_factor).all()\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS)\ndef test_sample_gradient(jax_dist, sp_dist, params):\n    if not jax_dist.reparametrized_params:\n        pytest.skip(\'{} not reparametrized.\'.format(jax_dist.__name__))\n\n    dist_args = [p.name for p in inspect.signature(jax_dist).parameters.values()]\n    params_dict = dict(zip(dist_args[:len(params)], params))\n    nonrepara_params_dict = {k: v for k, v in params_dict.items()\n                             if k not in jax_dist.reparametrized_params}\n    repara_params = tuple(v for k, v in params_dict.items()\n                          if k in jax_dist.reparametrized_params)\n\n    rng_key = random.PRNGKey(0)\n\n    def fn(args):\n        args_dict = dict(zip(jax_dist.reparametrized_params, args))\n        return np.sum(jax_dist(**args_dict, **nonrepara_params_dict).sample(key=rng_key))\n\n    actual_grad = jax.grad(fn)(repara_params)\n    assert len(actual_grad) == len(repara_params)\n\n    eps = 1e-3\n    for i in range(len(repara_params)):\n        if repara_params[i] is None:\n            continue\n        args_lhs = [p if j != i else p - eps for j, p in enumerate(repara_params)]\n        args_rhs = [p if j != i else p + eps for j, p in enumerate(repara_params)]\n        fn_lhs = fn(args_lhs)\n        fn_rhs = fn(args_rhs)\n        # finite diff approximation\n        expected_grad = (fn_rhs - fn_lhs) / (2. * eps)\n        assert np.shape(actual_grad[i]) == np.shape(repara_params[i])\n        assert_allclose(np.sum(actual_grad[i]), expected_grad, rtol=0.02)\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', [\n    (dist.Gamma, osp.gamma, (1.,)),\n    (dist.Gamma, osp.gamma, (0.1,)),\n    (dist.Gamma, osp.gamma, (10.,)),\n    # TODO: add more test cases for Beta/StudentT (and Dirichlet too) when\n    # their pathwise grad (independent of standard_gamma grad) is implemented.\n    pytest.param(dist.Beta, osp.beta, (1., 1.), marks=pytest.mark.xfail(\n        reason=\'currently, variance of grad of beta sampler is large\')),\n    pytest.param(dist.StudentT, osp.t, (1.,), marks=pytest.mark.xfail(\n        reason=\'currently, variance of grad of t sampler is large\')),\n])\ndef test_pathwise_gradient(jax_dist, sp_dist, params):\n    rng_key = random.PRNGKey(0)\n    N = 100\n    z = jax_dist(*params).sample(key=rng_key, sample_shape=(N,))\n    actual_grad = jacfwd(lambda x: jax_dist(*x).sample(key=rng_key, sample_shape=(N,)))(params)\n    eps = 1e-3\n    for i in range(len(params)):\n        args_lhs = [p if j != i else p - eps for j, p in enumerate(params)]\n        args_rhs = [p if j != i else p + eps for j, p in enumerate(params)]\n        cdf_dot = (sp_dist(*args_rhs).cdf(z) - sp_dist(*args_lhs).cdf(z)) / (2 * eps)\n        expected_grad = -cdf_dot / sp_dist(*params).pdf(z)\n        assert_allclose(actual_grad[i], expected_grad, rtol=0.005)\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS + DISCRETE)\n@pytest.mark.parametrize(\'prepend_shape\', [\n    (),\n    (2,),\n    (2, 3),\n])\n@pytest.mark.parametrize(\'jit\', [False, True])\ndef test_log_prob(jax_dist, sp_dist, params, prepend_shape, jit):\n    jit_fn = _identity if not jit else jax.jit\n    jax_dist = jax_dist(*params)\n    rng_key = random.PRNGKey(0)\n    samples = jax_dist.sample(key=rng_key, sample_shape=prepend_shape)\n    assert jax_dist.log_prob(samples).shape == prepend_shape + jax_dist.batch_shape\n    if not sp_dist:\n        if isinstance(jax_dist, dist.TruncatedCauchy) or isinstance(jax_dist, dist.TruncatedNormal):\n            low, loc, scale = params\n            high = np.inf\n            sp_dist = osp.cauchy if isinstance(jax_dist, dist.TruncatedCauchy) else osp.norm\n            sp_dist = sp_dist(loc, scale)\n            expected = sp_dist.logpdf(samples) - np.log(sp_dist.cdf(high) - sp_dist.cdf(low))\n            assert_allclose(jit_fn(jax_dist.log_prob)(samples), expected, atol=1e-5)\n            return\n        pytest.skip(\'no corresponding scipy distn.\')\n    if _is_batched_multivariate(jax_dist):\n        pytest.skip(\'batching not allowed in multivariate distns.\')\n    if jax_dist.event_shape and prepend_shape:\n        # >>> d = sp.dirichlet([1.1, 1.1])\n        # >>> samples = d.rvs(size=(2,))\n        # >>> d.logpdf(samples)\n        # ValueError: The input vector \'x\' must lie within the normal simplex ...\n        pytest.skip(\'batched samples cannot be scored by multivariate distributions.\')\n    sp_dist = sp_dist(*params)\n    try:\n        expected = sp_dist.logpdf(samples)\n    except AttributeError:\n        expected = sp_dist.logpmf(samples)\n    except ValueError as e:\n        # precision issue: np.sum(x / np.sum(x)) = 0.99999994 != 1\n        if ""The input vector \'x\' must lie within the normal simplex."" in str(e):\n            samples = samples.copy().astype(\'float64\')\n            samples = samples / samples.sum(axis=-1, keepdims=True)\n            expected = sp_dist.logpdf(samples)\n        else:\n            raise e\n    assert_allclose(jit_fn(jax_dist.log_prob)(samples), expected, atol=1e-5)\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS + DISCRETE)\ndef test_independent_shape(jax_dist, sp_dist, params):\n    d = jax_dist(*params)\n    batch_shape, event_shape = d.batch_shape, d.event_shape\n    shape = batch_shape + event_shape\n    for i in range(len(batch_shape)):\n        indep = dist.Independent(d, reinterpreted_batch_ndims=i)\n        sample = indep.sample(random.PRNGKey(0))\n        event_boundary = len(shape) - len(event_shape) - i\n        assert indep.batch_shape == shape[:event_boundary]\n        assert indep.event_shape == shape[event_boundary:]\n        assert np.shape(indep.log_prob(sample)) == shape[:event_boundary]\n\n\ndef _tril_cholesky_to_tril_corr(x):\n    w = vec_to_tril_matrix(x, diagonal=-1)\n    diag = np.sqrt(1 - np.sum(w ** 2, axis=-1))\n    cholesky = w + np.expand_dims(diag, axis=-1) * np.identity(w.shape[-1])\n    corr = np.matmul(cholesky, cholesky.T)\n    return matrix_to_tril_vec(corr, diagonal=-1)\n\n\n@pytest.mark.parametrize(\'dimension\', [2, 3, 5])\ndef test_log_prob_LKJCholesky_uniform(dimension):\n    # When concentration=1, the distribution of correlation matrices is uniform.\n    # We will test that fact here.\n    d = dist.LKJCholesky(dimension=dimension, concentration=1)\n    N = 5\n    corr_log_prob = []\n    for i in range(N):\n        sample = d.sample(random.PRNGKey(i))\n        log_prob = d.log_prob(sample)\n        sample_tril = matrix_to_tril_vec(sample, diagonal=-1)\n        cholesky_to_corr_jac = onp.linalg.slogdet(\n            jax.jacobian(_tril_cholesky_to_tril_corr)(sample_tril))[1]\n        corr_log_prob.append(log_prob - cholesky_to_corr_jac)\n\n    corr_log_prob = np.array(corr_log_prob)\n    # test if they are constant\n    assert_allclose(corr_log_prob, np.broadcast_to(corr_log_prob[0], corr_log_prob.shape),\n                    rtol=1e-6)\n\n    if dimension == 2:\n        # when concentration = 1, LKJ gives a uniform distribution over correlation matrix,\n        # hence for the case dimension = 2,\n        # density of a correlation matrix will be Uniform(-1, 1) = 0.5.\n        # In addition, jacobian of the transformation from cholesky -> corr is 1 (hence its\n        # log value is 0) because the off-diagonal lower triangular element does not change\n        # in the transform.\n        # So target_log_prob = log(0.5)\n        assert_allclose(corr_log_prob[0], np.log(0.5), rtol=1e-6)\n\n\n@pytest.mark.parametrize(""dimension"", [2, 3, 5])\n@pytest.mark.parametrize(""concentration"", [0.6, 2.2])\ndef test_log_prob_LKJCholesky(dimension, concentration):\n    # We will test against the fact that LKJCorrCholesky can be seen as a\n    # TransformedDistribution with base distribution is a distribution of partial\n    # correlations in C-vine method (modulo an affine transform to change domain from (0, 1)\n    # to (1, 0)) and transform is a signed stick-breaking process.\n    d = dist.LKJCholesky(dimension, concentration, sample_method=""cvine"")\n\n    beta_sample = d._beta.sample(random.PRNGKey(0))\n    beta_log_prob = np.sum(d._beta.log_prob(beta_sample))\n    partial_correlation = 2 * beta_sample - 1\n    affine_logdet = beta_sample.shape[-1] * np.log(2)\n    sample = signed_stick_breaking_tril(partial_correlation)\n\n    # compute signed stick breaking logdet\n    inv_tanh = lambda t: np.log((1 + t) / (1 - t)) / 2  # noqa: E731\n    inv_tanh_logdet = np.sum(np.log(vmap(grad(inv_tanh))(partial_correlation)))\n    unconstrained = inv_tanh(partial_correlation)\n    corr_cholesky_logdet = biject_to(constraints.corr_cholesky).log_abs_det_jacobian(\n        unconstrained,\n        sample,\n    )\n    signed_stick_breaking_logdet = corr_cholesky_logdet + inv_tanh_logdet\n\n    actual_log_prob = d.log_prob(sample)\n    expected_log_prob = beta_log_prob - affine_logdet - signed_stick_breaking_logdet\n    assert_allclose(actual_log_prob, expected_log_prob, rtol=1e-5)\n\n    assert_allclose(jax.jit(d.log_prob)(sample), d.log_prob(sample), atol=1e-7)\n\n\n@pytest.mark.parametrize(\'rate\', [0.1, 0.5, 0.9, 1.0, 1.1, 2.0, 10.0])\ndef test_ZIP_log_prob(rate):\n    # if gate is 0 ZIP is Poisson\n    zip_ = dist.ZeroInflatedPoisson(0., rate)\n    pois = dist.Poisson(rate)\n    s = zip_.sample(random.PRNGKey(0), (20,))\n    zip_prob = zip_.log_prob(s)\n    pois_prob = pois.log_prob(s)\n    assert_allclose(zip_prob, pois_prob)\n\n    # if gate is 1 ZIP is Delta(0)\n    zip_ = dist.ZeroInflatedPoisson(1., rate)\n    delta = dist.Delta(0.)\n    s = np.array([0., 1.])\n    zip_prob = zip_.log_prob(s)\n    delta_prob = delta.log_prob(s)\n    assert_allclose(zip_prob, delta_prob)\n\n\n@pytest.mark.parametrize(""total_count"", [1, 2, 3, 10])\n@pytest.mark.parametrize(""shape"", [(1,), (3, 1), (2, 3, 1)])\ndef test_beta_binomial_log_prob(total_count, shape):\n    concentration0 = onp.exp(onp.random.normal(size=shape))\n    concentration1 = onp.exp(onp.random.normal(size=shape))\n    value = np.arange(1 + total_count)\n\n    num_samples = 100000\n    probs = onp.random.beta(concentration1, concentration0, size=(num_samples,) + shape)\n    log_probs = dist.Binomial(total_count, probs).log_prob(value)\n    expected = logsumexp(log_probs, 0) - np.log(num_samples)\n\n    actual = dist.BetaBinomial(concentration1, concentration0, total_count).log_prob(value)\n    assert_allclose(actual, expected, rtol=0.02)\n\n\n@pytest.mark.parametrize(""shape"", [(1,), (3, 1), (2, 3, 1)])\ndef test_gamma_poisson_log_prob(shape):\n    gamma_conc = onp.exp(onp.random.normal(size=shape))\n    gamma_rate = onp.exp(onp.random.normal(size=shape))\n    value = np.arange(15)\n\n    num_samples = 300000\n    poisson_rate = onp.random.gamma(gamma_conc, 1 / gamma_rate, size=(num_samples,) + shape)\n    log_probs = dist.Poisson(poisson_rate).log_prob(value)\n    expected = logsumexp(log_probs, 0) - np.log(num_samples)\n    actual = dist.GammaPoisson(gamma_conc, gamma_rate).log_prob(value)\n    assert_allclose(actual, expected, rtol=0.05)\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS + DISCRETE)\ndef test_log_prob_gradient(jax_dist, sp_dist, params):\n    if jax_dist in [dist.LKJ, dist.LKJCholesky]:\n        pytest.skip(\'we have separated tests for LKJCholesky distribution\')\n    if jax_dist is _ImproperWrapper:\n        pytest.skip(\'no param for ImproperUniform to test for log_prob gradient\')\n\n    rng_key = random.PRNGKey(0)\n    value = jax_dist(*params).sample(rng_key)\n\n    def fn(*args):\n        return np.sum(jax_dist(*args).log_prob(value))\n\n    eps = 1e-3\n    for i in range(len(params)):\n        if params[i] is None or np.result_type(params[i]) in (np.int32, np.int64):\n            continue\n        actual_grad = jax.grad(fn, i)(*params)\n        args_lhs = [p if j != i else p - eps for j, p in enumerate(params)]\n        args_rhs = [p if j != i else p + eps for j, p in enumerate(params)]\n        fn_lhs = fn(*args_lhs)\n        fn_rhs = fn(*args_rhs)\n        # finite diff approximation\n        expected_grad = (fn_rhs - fn_lhs) / (2. * eps)\n        assert np.shape(actual_grad) == np.shape(params[i])\n        if i == 0 and jax_dist is dist.Delta:\n            # grad w.r.t. `value` of Delta distribution will be 0\n            # but numerical value will give nan (= inf - inf)\n            expected_grad = 0.\n        assert_allclose(np.sum(actual_grad), expected_grad, rtol=0.01, atol=0.01)\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS + DISCRETE)\ndef test_mean_var(jax_dist, sp_dist, params):\n    if jax_dist is _ImproperWrapper:\n        pytest.skip(""Improper distribution does not has mean/var implemented"")\n\n    n = 20000 if jax_dist in [dist.LKJ, dist.LKJCholesky] else 200000\n    d_jax = jax_dist(*params)\n    k = random.PRNGKey(0)\n    samples = d_jax.sample(k, sample_shape=(n,))\n    # check with suitable scipy implementation if available\n    if sp_dist and not _is_batched_multivariate(d_jax):\n        d_sp = sp_dist(*params)\n        try:\n            sp_mean = d_sp.mean()\n        except TypeError:  # mvn does not have .mean() method\n            sp_mean = d_sp.mean\n        # for multivariate distns try .cov first\n        if d_jax.event_shape:\n            try:\n                sp_var = np.diag(d_sp.cov())\n            except TypeError:  # mvn does not have .cov() method\n                sp_var = np.diag(d_sp.cov)\n            except AttributeError:\n                sp_var = d_sp.var()\n        else:\n            sp_var = d_sp.var()\n        assert_allclose(d_jax.mean, sp_mean, rtol=0.01, atol=1e-7)\n        assert_allclose(d_jax.variance, sp_var, rtol=0.01, atol=1e-7)\n        if np.all(np.isfinite(sp_mean)):\n            assert_allclose(np.mean(samples, 0), d_jax.mean, rtol=0.05, atol=1e-2)\n        if np.all(np.isfinite(sp_var)):\n            assert_allclose(np.std(samples, 0), np.sqrt(d_jax.variance), rtol=0.05, atol=1e-2)\n    elif jax_dist in [dist.LKJ, dist.LKJCholesky]:\n        if jax_dist is dist.LKJCholesky:\n            corr_samples = np.matmul(samples, np.swapaxes(samples, -2, -1))\n        else:\n            corr_samples = samples\n        dimension, concentration, _ = params\n        # marginal of off-diagonal entries\n        marginal = dist.Beta(concentration + 0.5 * (dimension - 2),\n                             concentration + 0.5 * (dimension - 2))\n        # scale statistics due to linear mapping\n        marginal_mean = 2 * marginal.mean - 1\n        marginal_std = 2 * np.sqrt(marginal.variance)\n        expected_mean = np.broadcast_to(np.reshape(marginal_mean, np.shape(marginal_mean) + (1, 1)),\n                                        np.shape(marginal_mean) + d_jax.event_shape)\n        expected_std = np.broadcast_to(np.reshape(marginal_std, np.shape(marginal_std) + (1, 1)),\n                                       np.shape(marginal_std) + d_jax.event_shape)\n        # diagonal elements of correlation matrices are 1\n        expected_mean = expected_mean * (1 - np.identity(dimension)) + np.identity(dimension)\n        expected_std = expected_std * (1 - np.identity(dimension))\n\n        assert_allclose(np.mean(corr_samples, axis=0), expected_mean, atol=0.01)\n        assert_allclose(np.std(corr_samples, axis=0), expected_std, atol=0.01)\n    else:\n        if np.all(np.isfinite(d_jax.mean)):\n            assert_allclose(np.mean(samples, 0), d_jax.mean, rtol=0.05, atol=1e-2)\n        if np.all(np.isfinite(d_jax.variance)):\n            assert_allclose(np.std(samples, 0), np.sqrt(d_jax.variance), rtol=0.05, atol=1e-2)\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS + DISCRETE)\n@pytest.mark.parametrize(\'prepend_shape\', [\n    (),\n    (2,),\n    (2, 3),\n])\ndef test_distribution_constraints(jax_dist, sp_dist, params, prepend_shape):\n    dist_args = [p.name for p in inspect.signature(jax_dist).parameters.values()]\n\n    valid_params, oob_params = list(params), list(params)\n    key = random.PRNGKey(1)\n    dependent_constraint = False\n    for i in range(len(params)):\n        if jax_dist in (_ImproperWrapper, dist.LKJ, dist.LKJCholesky) and dist_args[i] != ""concentration"":\n            continue\n        if params[i] is None:\n            oob_params[i] = None\n            valid_params[i] = None\n            continue\n        constraint = jax_dist.arg_constraints[dist_args[i]]\n        if isinstance(constraint, constraints._Dependent):\n            dependent_constraint = True\n            break\n        key, key_gen = random.split(key)\n        oob_params[i] = gen_values_outside_bounds(constraint, np.shape(params[i]), key_gen)\n        valid_params[i] = gen_values_within_bounds(constraint, np.shape(params[i]), key_gen)\n\n    assert jax_dist(*oob_params)\n\n    # Invalid parameter values throw ValueError\n    if not dependent_constraint and jax_dist is not _ImproperWrapper:\n        with pytest.raises(ValueError):\n            jax_dist(*oob_params, validate_args=True)\n\n    d = jax_dist(*valid_params, validate_args=True)\n\n    # Test agreement of log density evaluation on randomly generated samples\n    # with scipy\'s implementation when available.\n    if sp_dist and \\\n            not _is_batched_multivariate(d) and \\\n            not (d.event_shape and prepend_shape):\n        valid_samples = gen_values_within_bounds(d.support, size=prepend_shape + d.batch_shape + d.event_shape)\n        try:\n            expected = sp_dist(*valid_params).logpdf(valid_samples)\n        except AttributeError:\n            expected = sp_dist(*valid_params).logpmf(valid_samples)\n        assert_allclose(d.log_prob(valid_samples), expected, atol=1e-5, rtol=1e-5)\n\n    # Out of support samples throw ValueError\n    oob_samples = gen_values_outside_bounds(d.support, size=prepend_shape + d.batch_shape + d.event_shape)\n    with pytest.warns(UserWarning):\n        d.log_prob(oob_samples)\n\n\ndef test_categorical_log_prob_grad():\n    data = np.repeat(np.arange(3), 10)\n\n    def f(x):\n        return dist.Categorical(jax.nn.softmax(x * np.arange(1, 4))).log_prob(data).sum()\n\n    def g(x):\n        return dist.Categorical(logits=x * np.arange(1, 4)).log_prob(data).sum()\n\n    x = 0.5\n    fx, grad_fx = jax.value_and_grad(f)(x)\n    gx, grad_gx = jax.value_and_grad(g)(x)\n    assert_allclose(fx, gx)\n    assert_allclose(grad_fx, grad_gx, atol=1e-4)\n\n\n########################################\n# Tests for constraints and transforms #\n########################################\n\n\n@pytest.mark.parametrize(\'constraint, x, expected\', [\n    (constraints.boolean, np.array([True, False]), np.array([True, True])),\n    (constraints.boolean, np.array([1, 1]), np.array([True, True])),\n    (constraints.boolean, np.array([-1, 1]), np.array([False, True])),\n    (constraints.corr_cholesky, np.array([[[1, 0], [0, 1]], [[1, 0.1], [0, 1]]]),\n     np.array([True, False])),  # NB: not lower_triangular\n    (constraints.corr_cholesky, np.array([[[1, 0], [1, 0]], [[1, 0], [0.5, 0.5]]]),\n     np.array([False, False])),  # NB: not positive_diagonal & not unit_norm_row\n    (constraints.corr_matrix, np.array([[[1, 0], [0, 1]], [[1, 0.1], [0, 1]]]),\n     np.array([True, False])),  # NB: not lower_triangular\n    (constraints.corr_matrix, np.array([[[1, 0], [1, 0]], [[1, 0], [0.5, 0.5]]]),\n     np.array([False, False])),  # NB: not unit diagonal\n    (constraints.greater_than(1), 3, True),\n    (constraints.greater_than(1), np.array([-1, 1, 5]), np.array([False, False, True])),\n    (constraints.integer_interval(-3, 5), 0, True),\n    (constraints.integer_interval(-3, 5), np.array([-5, -3, 0, 1.1, 5, 7]),\n     np.array([False, True, True, False, True, False])),\n    (constraints.interval(-3, 5), 0, True),\n    (constraints.interval(-3, 5), np.array([-5, -3, 0, 5, 7]),\n     np.array([False, False, True, False, False])),\n    (constraints.lower_cholesky, np.array([[1., 0.], [-2., 0.1]]), True),\n    (constraints.lower_cholesky, np.array([[[1., 0.], [-2., -0.1]], [[1., 0.1], [2., 0.2]]]),\n     np.array([False, False])),\n    (constraints.nonnegative_integer, 3, True),\n    (constraints.nonnegative_integer, np.array([-1., 0., 5.]), np.array([False, True, True])),\n    (constraints.positive, 3, True),\n    (constraints.positive, np.array([-1, 0, 5]), np.array([False, False, True])),\n    (constraints.positive_definite, np.array([[1., 0.3], [0.3, 1.]]), True),\n    (constraints.positive_definite, np.array([[[2., 0.4], [0.3, 2.]], [[1., 0.1], [0.1, 0.]]]),\n     np.array([False, False])),\n    (constraints.positive_integer, 3, True),\n    (constraints.positive_integer, np.array([-1., 0., 5.]), np.array([False, False, True])),\n    (constraints.real, -1, True),\n    (constraints.real, np.array([np.inf, np.NINF, np.nan, np.pi]),\n     np.array([False, False, False, True])),\n    (constraints.simplex, np.array([0.1, 0.3, 0.6]), True),\n    (constraints.simplex, np.array([[0.1, 0.3, 0.6], [-0.1, 0.6, 0.5], [0.1, 0.6, 0.5]]),\n     np.array([True, False, False])),\n    (constraints.unit_interval, 0.1, True),\n    (constraints.unit_interval, np.array([-5, 0, 0.5, 1, 7]),\n     np.array([False, False, True, False, False])),\n])\ndef test_constraints(constraint, x, expected):\n    assert_array_equal(constraint(x), expected)\n\n\n@pytest.mark.parametrize(\'constraint\', [\n    constraints.corr_cholesky,\n    constraints.corr_matrix,\n    constraints.greater_than(2),\n    constraints.interval(-3, 5),\n    constraints.lower_cholesky,\n    constraints.ordered_vector,\n    constraints.positive,\n    constraints.positive_definite,\n    constraints.real,\n    constraints.simplex,\n    constraints.unit_interval,\n], ids=lambda x: x.__class__)\n@pytest.mark.parametrize(\'shape\', [(), (1,), (3,), (6,), (3, 1), (1, 3), (5, 3)])\ndef test_biject_to(constraint, shape):\n    transform = biject_to(constraint)\n    if transform.event_dim == 2:\n        event_dim = 1  # actual dim of unconstrained domain\n    else:\n        event_dim = transform.event_dim\n    if isinstance(constraint, constraints._Interval):\n        assert transform.codomain.upper_bound == constraint.upper_bound\n        assert transform.codomain.lower_bound == constraint.lower_bound\n    elif isinstance(constraint, constraints._GreaterThan):\n        assert transform.codomain.lower_bound == constraint.lower_bound\n    if len(shape) < event_dim:\n        return\n    rng_key = random.PRNGKey(0)\n    x = random.normal(rng_key, shape)\n    y = transform(x)\n\n    # test inv work for NaN arrays:\n    x_nan = transform.inv(np.full(np.shape(y), np.nan))\n    assert (x_nan.shape == x.shape)\n\n    # test codomain\n    batch_shape = shape if event_dim == 0 else shape[:-1]\n    assert_array_equal(transform.codomain(y), np.ones(batch_shape, dtype=np.bool_))\n\n    # test inv\n    z = transform.inv(y)\n    assert_allclose(x, z, atol=1e-6, rtol=1e-6)\n\n    # test domain, currently all is constraints.real or constraints.real_vector\n    assert_array_equal(transform.domain(z), np.ones(batch_shape))\n\n    # test log_abs_det_jacobian\n    actual = transform.log_abs_det_jacobian(x, y)\n    assert np.shape(actual) == batch_shape\n    if len(shape) == event_dim:\n        if constraint is constraints.simplex:\n            expected = onp.linalg.slogdet(jax.jacobian(transform)(x)[:-1, :])[1]\n            inv_expected = onp.linalg.slogdet(jax.jacobian(transform.inv)(y)[:, :-1])[1]\n        elif constraint is constraints.ordered_vector:\n            expected = onp.linalg.slogdet(jax.jacobian(transform)(x))[1]\n            inv_expected = onp.linalg.slogdet(jax.jacobian(transform.inv)(y))[1]\n        elif constraint in [constraints.corr_cholesky, constraints.corr_matrix]:\n            vec_transform = lambda x: matrix_to_tril_vec(transform(x), diagonal=-1)  # noqa: E731\n            y_tril = matrix_to_tril_vec(y, diagonal=-1)\n\n            def inv_vec_transform(y):\n                matrix = vec_to_tril_matrix(y, diagonal=-1)\n                if constraint is constraints.corr_matrix:\n                    # fill the upper triangular part\n                    matrix = matrix + np.swapaxes(matrix, -2, -1) + np.identity(matrix.shape[-1])\n                return transform.inv(matrix)\n\n            expected = onp.linalg.slogdet(jax.jacobian(vec_transform)(x))[1]\n            inv_expected = onp.linalg.slogdet(jax.jacobian(inv_vec_transform)(y_tril))[1]\n        elif constraint in [constraints.lower_cholesky, constraints.positive_definite]:\n            vec_transform = lambda x: matrix_to_tril_vec(transform(x))  # noqa: E731\n            y_tril = matrix_to_tril_vec(y)\n\n            def inv_vec_transform(y):\n                matrix = vec_to_tril_matrix(y)\n                if constraint is constraints.positive_definite:\n                    # fill the upper triangular part\n                    matrix = matrix + np.swapaxes(matrix, -2, -1) - np.diag(np.diag(matrix))\n                return transform.inv(matrix)\n\n            expected = onp.linalg.slogdet(jax.jacobian(vec_transform)(x))[1]\n            inv_expected = onp.linalg.slogdet(jax.jacobian(inv_vec_transform)(y_tril))[1]\n        else:\n            expected = np.log(np.abs(grad(transform)(x)))\n            inv_expected = np.log(np.abs(grad(transform.inv)(y)))\n\n        assert_allclose(actual, expected, atol=1e-6, rtol=1e-6)\n        assert_allclose(actual, -inv_expected, atol=1e-6, rtol=1e-6)\n\n\n# NB: skip transforms which are tested in `test_biject_to`\n@pytest.mark.parametrize(\'transform, event_shape\', [\n    (PermuteTransform(np.array([3, 0, 4, 1, 2])), (5,)),\n    (PowerTransform(2.), ()),\n    (MultivariateAffineTransform(np.array([1., 2.]), np.array([[0.6, 0.], [1.5, 0.4]])), (2,))\n])\n@pytest.mark.parametrize(\'batch_shape\', [(), (1,), (3,), (6,), (3, 1), (1, 3), (5, 3)])\ndef test_bijective_transforms(transform, event_shape, batch_shape):\n    shape = batch_shape + event_shape\n    rng_key = random.PRNGKey(0)\n    x = biject_to(transform.domain)(random.normal(rng_key, shape))\n    y = transform(x)\n\n    # test codomain\n    assert_array_equal(transform.codomain(y), np.ones(batch_shape))\n\n    # test inv\n    z = transform.inv(y)\n    assert_allclose(x, z, atol=1e-6, rtol=1e-6)\n\n    # test domain\n    assert_array_equal(transform.domain(z), np.ones(batch_shape))\n\n    # test log_abs_det_jacobian\n    actual = transform.log_abs_det_jacobian(x, y)\n    assert np.shape(actual) == batch_shape\n    if len(shape) == transform.event_dim:\n        if len(event_shape) == 1:\n            expected = onp.linalg.slogdet(jax.jacobian(transform)(x))[1]\n            inv_expected = onp.linalg.slogdet(jax.jacobian(transform.inv)(y))[1]\n        else:\n            expected = np.log(np.abs(grad(transform)(x)))\n            inv_expected = np.log(np.abs(grad(transform.inv)(y)))\n\n        assert_allclose(actual, expected, atol=1e-6)\n        assert_allclose(actual, -inv_expected, atol=1e-6)\n\n\n@pytest.mark.parametrize(\'transformed_dist\', [\n    dist.TransformedDistribution(dist.Normal(np.array([2., 3.]), 1.), transforms.ExpTransform()),\n    dist.TransformedDistribution(dist.Exponential(np.ones(2)), [\n        transforms.PowerTransform(0.7),\n        transforms.AffineTransform(0., np.ones(2) * 3)\n    ]),\n])\ndef test_transformed_distribution_intermediates(transformed_dist):\n    sample, intermediates = transformed_dist.sample_with_intermediates(random.PRNGKey(1))\n    assert_allclose(transformed_dist.log_prob(sample, intermediates), transformed_dist.log_prob(sample))\n\n\ndef test_transformed_transformed_distribution():\n    loc, scale = -2, 3\n    dist1 = dist.TransformedDistribution(dist.Normal(2, 3), transforms.PowerTransform(2.))\n    dist2 = dist.TransformedDistribution(dist1, transforms.AffineTransform(-2, 3))\n    assert isinstance(dist2.base_dist, dist.Normal)\n    assert len(dist2.transforms) == 2\n    assert isinstance(dist2.transforms[0], transforms.PowerTransform)\n    assert isinstance(dist2.transforms[1], transforms.AffineTransform)\n\n    rng_key = random.PRNGKey(0)\n    assert_allclose(loc + scale * dist1.sample(rng_key), dist2.sample(rng_key))\n    intermediates = dist2.sample_with_intermediates(rng_key)\n    assert len(intermediates) == 2\n\n\ndef _make_iaf(input_dim, hidden_dims, rng_key):\n    arn_init, arn = AutoregressiveNN(input_dim, hidden_dims, param_dims=[1, 1])\n    _, init_params = arn_init(rng_key, (input_dim,))\n    return InverseAutoregressiveTransform(partial(arn, init_params))\n\n\n@pytest.mark.parametrize(\'ts\', [\n    [transforms.PowerTransform(0.7), transforms.AffineTransform(2., 3.)],\n    [transforms.ExpTransform()],\n    [transforms.ComposeTransform([transforms.AffineTransform(-2, 3),\n                                  transforms.ExpTransform()]),\n     transforms.PowerTransform(3.)],\n    [_make_iaf(5, hidden_dims=[10], rng_key=random.PRNGKey(0)),\n     transforms.PermuteTransform(np.arange(5)[::-1]),\n     _make_iaf(5, hidden_dims=[10], rng_key=random.PRNGKey(1))]\n])\ndef test_compose_transform_with_intermediates(ts):\n    transform = transforms.ComposeTransform(ts)\n    x = random.normal(random.PRNGKey(2), (7, 5))\n    y, intermediates = transform.call_with_intermediates(x)\n    logdet = transform.log_abs_det_jacobian(x, y, intermediates)\n    assert_allclose(y, transform(x))\n    assert_allclose(logdet, transform.log_abs_det_jacobian(x, y))\n\n\n@pytest.mark.parametrize(\'x_dim, y_dim\', [(3, 3), (3, 4)])\ndef test_unpack_transform(x_dim, y_dim):\n    xy = onp.random.randn(x_dim + y_dim)\n    unpack_fn = lambda xy: {\'x\': xy[:x_dim], \'y\': xy[x_dim:]}  # noqa: E731\n    transform = transforms.UnpackTransform(unpack_fn)\n    z = transform(xy)\n    if x_dim == y_dim:\n        with pytest.warns(UserWarning, match=""UnpackTransform.inv""):\n            t = transform.inv(z)\n    else:\n        t = transform.inv(z)\n\n    assert_allclose(t, xy)\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS)\ndef test_generated_sample_distribution(jax_dist, sp_dist, params,\n                                       N_sample=100_000,\n                                       key=random.PRNGKey(11)):\n    """""" On samplers that we do not get directly from JAX, (e.g. we only get\n    Gumbel(0,1) but also provide samplers for Gumbel(loc, scale)), also test\n    agreement in the empirical distribution of generated samples between our\n    samplers and those from SciPy.\n    """"""\n\n    if jax_dist not in [dist.Gumbel]:\n        pytest.skip(""{} sampling method taken from upstream, no need to""\n                    ""test generated samples."".format(jax_dist.__name__))\n\n    jax_dist = jax_dist(*params)\n    if sp_dist and not jax_dist.event_shape and not jax_dist.batch_shape:\n        our_samples = jax_dist.sample(key, (N_sample,))\n        ks_result = osp.kstest(our_samples, sp_dist(*params).cdf)\n        assert ks_result.pvalue > 0.05\n\n\n@pytest.mark.parametrize(\'jax_dist, params, support\', [\n    (dist.BernoulliLogits, (5.,), np.arange(2)),\n    (dist.BernoulliProbs, (0.5,), np.arange(2)),\n    (dist.BinomialLogits, (4.5, 10), np.arange(11)),\n    (dist.BinomialProbs, (0.5, 11), np.arange(12)),\n    (dist.BetaBinomial, (2., 0.5, 12), np.arange(13)),\n    (dist.CategoricalLogits, (np.array([3., 4., 5.]),), np.arange(3)),\n    (dist.CategoricalProbs, (np.array([0.1, 0.5, 0.4]),), np.arange(3)),\n])\n@pytest.mark.parametrize(\'batch_shape\', [(5,), ()])\n@pytest.mark.parametrize(\'expand\', [False, True])\ndef test_enumerate_support_smoke(jax_dist, params, support, batch_shape, expand):\n    p0 = np.broadcast_to(params[0], batch_shape + np.shape(params[0]))\n    actual = jax_dist(p0, *params[1:]).enumerate_support(expand=expand)\n    expected = support.reshape((-1,) + (1,) * len(batch_shape))\n    if expand:\n        expected = np.broadcast_to(expected, support.shape + batch_shape)\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(\'jax_dist, sp_dist, params\', CONTINUOUS + DISCRETE)\n@pytest.mark.parametrize(\'prepend_shape\', [\n    (),\n    (2, 3),\n])\n@pytest.mark.parametrize(\'sample_shape\', [\n    (),\n    (4,),\n])\ndef test_expand(jax_dist, sp_dist, params, prepend_shape, sample_shape):\n    jax_dist = jax_dist(*params)\n    new_batch_shape = prepend_shape + jax_dist.batch_shape\n    expanded_dist = jax_dist.expand(new_batch_shape)\n    rng_key = random.PRNGKey(0)\n    samples = expanded_dist.sample(rng_key, sample_shape)\n    assert expanded_dist.batch_shape == new_batch_shape\n    assert samples.shape == sample_shape + new_batch_shape + jax_dist.event_shape\n    assert expanded_dist.log_prob(samples).shape == sample_shape + new_batch_shape\n    # test expand of expand\n    assert expanded_dist.expand((3,) + new_batch_shape).batch_shape == (3,) + new_batch_shape\n    # test expand error\n    if prepend_shape:\n        with pytest.raises(ValueError, match=""Cannot broadcast distribution of shape""):\n            assert expanded_dist.expand((3,) + jax_dist.batch_shape)\n\n\n@pytest.mark.parametrize(\'batch_shape\', [\n    (),\n    (4,),\n])\ndef test_polya_gamma(batch_shape, num_points=20000):\n    d = dist.TruncatedPolyaGamma(batch_shape=batch_shape)\n    rng_key = random.PRNGKey(0)\n\n    # test density approximately normalized\n    x = np.linspace(1.0e-6, d.truncation_point, num_points)\n    prob = (d.truncation_point / num_points) * np.exp(logsumexp(d.log_prob(x), axis=-1))\n    assert_allclose(prob, np.ones(batch_shape), rtol=1.0e-4)\n\n    # test mean of approximate sampler\n    z = d.sample(rng_key, sample_shape=(3000,))\n    mean = np.mean(z, axis=-1)\n    assert_allclose(mean, 0.25 * np.ones(batch_shape), rtol=0.07)\n\n\n@pytest.mark.parametrize(""extra_event_dims,expand_shape"", [\n    (0, (4, 3, 2, 1)),\n    (0, (4, 3, 2, 2)),\n    (1, (5, 4, 3, 2)),\n    (2, (5, 4, 3)),\n])\ndef test_expand_reshaped_distribution(extra_event_dims, expand_shape):\n    loc = np.zeros((1, 6))\n    scale_tril = np.eye(6)\n    d = dist.MultivariateNormal(loc, scale_tril=scale_tril)\n    full_shape = (4, 1, 1, 1, 6)\n    reshaped_dist = d.expand([4, 1, 1, 1]).to_event(extra_event_dims)\n    cut = 4 - extra_event_dims\n    batch_shape, event_shape = full_shape[:cut], full_shape[cut:]\n    assert reshaped_dist.batch_shape == batch_shape\n    assert reshaped_dist.event_shape == event_shape\n    large = reshaped_dist.expand(expand_shape)\n    assert large.batch_shape == expand_shape\n    assert large.event_shape == event_shape\n\n    # Throws error when batch shape cannot be broadcasted\n    with pytest.raises((RuntimeError, ValueError)):\n        reshaped_dist.expand(expand_shape + (3,))\n\n    # Throws error when trying to shrink existing batch shape\n    with pytest.raises((RuntimeError, ValueError)):\n        large.expand(expand_shape[1:])\n\n\n@pytest.mark.parametrize(\'batch_shape, mask_shape\', [\n    ((), ()),\n    ((2,), ()),\n    ((), (2,)),\n    ((2,), (2,)),\n    ((4, 2), (1, 2)),\n    ((2,), (4, 2)),\n])\n@pytest.mark.parametrize(\'event_shape\', [\n    (),\n    (3,)\n])\ndef test_mask(batch_shape, event_shape, mask_shape):\n    jax_dist = dist.Normal().expand(batch_shape + event_shape).to_event(len(event_shape))\n    mask = dist.Bernoulli(0.5).sample(random.PRNGKey(0), mask_shape)\n    if mask_shape == ():\n        mask = bool(mask)\n    samples = jax_dist.sample(random.PRNGKey(1))\n    actual = jax_dist.mask(mask).log_prob(samples)\n    assert_allclose(actual != 0, np.broadcast_to(mask, lax.broadcast_shapes(batch_shape, mask_shape)))\n'"
test/test_distributions_util.py,29,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numbers import Number\n\nimport numpy as onp\nfrom numpy.testing import assert_allclose\nimport pytest\n\nfrom jax import lax, random, vmap\nimport jax.numpy as np\nfrom jax.scipy.special import expit, xlog1py, xlogy\n\nfrom numpyro.distributions.util import (\n    binary_cross_entropy_with_logits,\n    categorical,\n    cholesky_update,\n    multinomial,\n    poisson,\n    vec_to_tril_matrix,\n    binomial)\n\n\n@pytest.mark.parametrize(\'x, y\', [\n    (0.2, 10.),\n    (0.6, -10.),\n])\ndef test_binary_cross_entropy_with_logits(x, y):\n    actual = -y * np.log(expit(x)) - (1 - y) * np.log(expit(-x))\n    expect = binary_cross_entropy_with_logits(x, y)\n    assert_allclose(actual, expect, rtol=1e-6)\n\n\n@pytest.mark.parametrize(\'prim\', [\n    xlogy,\n    xlog1py,\n])\ndef test_binop_batch_rule(prim):\n    bx = np.array([1., 2., 3.])\n    by = np.array([2., 3., 4.])\n    x = np.array(1.)\n    y = np.array(2.)\n\n    actual_bx_by = vmap(lambda x, y: prim(x, y))(bx, by)\n    for i in range(3):\n        assert_allclose(actual_bx_by[i], prim(bx[i], by[i]))\n\n    actual_x_by = vmap(lambda y: prim(x, y))(by)\n    for i in range(3):\n        assert_allclose(actual_x_by[i], prim(x, by[i]))\n\n    actual_bx_y = vmap(lambda x: prim(x, y))(bx)\n    for i in range(3):\n        assert_allclose(actual_bx_y[i], prim(bx[i], y))\n\n\n@pytest.mark.parametrize(\'p, shape\', [\n    (np.array([0.1, 0.9]), ()),\n    (np.array([0.2, 0.8]), (2,)),\n    (np.array([[0.1, 0.9], [0.2, 0.8]]), ()),\n    (np.array([[0.1, 0.9], [0.2, 0.8]]), (3, 2)),\n])\ndef test_categorical_shape(p, shape):\n    rng_key = random.PRNGKey(0)\n    expected_shape = lax.broadcast_shapes(p.shape[:-1], shape)\n    assert np.shape(categorical(rng_key, p, shape)) == expected_shape\n\n\n@pytest.mark.parametrize(""p"", [\n    np.array([0.2, 0.3, 0.5]),\n    np.array([0.8, 0.1, 0.1]),\n])\ndef test_categorical_stats(p):\n    rng_key = random.PRNGKey(0)\n    n = 10000\n    z = categorical(rng_key, p, (n,))\n    _, counts = onp.unique(z, return_counts=True)\n    assert_allclose(counts / float(n), p, atol=0.01)\n\n\n@pytest.mark.parametrize(\'p, shape\', [\n    (np.array([0.1, 0.9]), ()),\n    (np.array([0.2, 0.8]), (2,)),\n    (np.array([[0.1, 0.9], [0.2, 0.8]]), ()),\n    (np.array([[0.1, 0.9], [0.2, 0.8]]), (3, 2)),\n])\ndef test_multinomial_shape(p, shape):\n    rng_key = random.PRNGKey(0)\n    n = 10000\n    expected_shape = lax.broadcast_shapes(p.shape[:-1], shape) + p.shape[-1:]\n    assert np.shape(multinomial(rng_key, p, n, shape)) == expected_shape\n\n\n@pytest.mark.parametrize(""p"", [\n    np.array([0.2, 0.3, 0.5]),\n    np.array([0.8, 0.1, 0.1]),\n])\n@pytest.mark.parametrize(""n"", [\n    10000,\n    np.array([10000, 20000]),\n])\ndef test_multinomial_stats(p, n):\n    rng_key = random.PRNGKey(0)\n    z = multinomial(rng_key, p, n)\n    n = float(n) if isinstance(n, Number) else np.expand_dims(n.astype(p.dtype), -1)\n    p = np.broadcast_to(p, z.shape)\n    assert_allclose(z / n, p, atol=0.01)\n\n\ndef test_poisson():\n    mu = rate = 1000\n    N = 2 ** 18\n\n    key = random.PRNGKey(64)\n    B = poisson(key, rate=rate, shape=(N,))\n    assert_allclose(B.mean(), mu, rtol=0.001)\n\n\n@pytest.mark.parametrize(""shape"", [\n    (6,),\n    (5, 10),\n    (3, 4, 3),\n])\n@pytest.mark.parametrize(""diagonal"", [\n    0,\n    -1,\n    -2,\n])\ndef test_vec_to_tril_matrix(shape, diagonal):\n    rng_key = random.PRNGKey(0)\n    x = random.normal(rng_key, shape)\n    actual = vec_to_tril_matrix(x, diagonal)\n    expected = onp.zeros(shape[:-1] + actual.shape[-2:])\n    tril_idxs = onp.tril_indices(expected.shape[-1], diagonal)\n    expected[..., tril_idxs[0], tril_idxs[1]] = x\n    assert_allclose(actual, expected)\n\n\n@pytest.mark.parametrize(""chol_batch_shape"", [(), (3,)])\n@pytest.mark.parametrize(""vec_batch_shape"", [(), (3,)])\n@pytest.mark.parametrize(""dim"", [1, 4])\n@pytest.mark.parametrize(""coef"", [1, -1])\ndef test_cholesky_update(chol_batch_shape, vec_batch_shape, dim, coef):\n    A = random.normal(random.PRNGKey(0), chol_batch_shape + (dim, dim))\n    A = A @ np.swapaxes(A, -2, -1) + np.eye(dim)\n    x = random.normal(random.PRNGKey(0), vec_batch_shape + (dim,)) * 0.1\n    xxt = x[..., None] @ x[..., None, :]\n    expected = np.linalg.cholesky(A + coef * xxt)\n    actual = cholesky_update(np.linalg.cholesky(A), x, coef)\n    assert_allclose(actual, expected, atol=1e-4, rtol=1e-4)\n\n\n@pytest.mark.parametrize(""n"", [10, 100, 1000])\n@pytest.mark.parametrize(""p"", [0., 0.01, 0.05, 0.3, 0.5, 0.7, 0.95, 1.])\ndef test_binomial_mean(n, p):\n    samples = binomial(random.PRNGKey(1), p, n, shape=(100, 100))\n    expected_mean = n * p\n    assert_allclose(np.mean(samples), expected_mean, rtol=0.05)\n'"
test/test_example_utils.py,7,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport jax.numpy as np\n\nfrom numpyro.examples.datasets import BASEBALL, COVTYPE, MNIST, SP500, load_dataset\nfrom numpyro.util import fori_loop\n\n\ndef test_baseball_data_load():\n    init, fetch = load_dataset(BASEBALL, split='train', shuffle=False)\n    num_batches, idx = init()\n    dataset = fetch(0, idx)\n    assert np.shape(dataset[0]) == (18, 2)\n    assert np.shape(dataset[1]) == (18,)\n\n\ndef test_covtype_data_load():\n    _, fetch = load_dataset(COVTYPE, shuffle=False)\n    x, y = fetch()\n    assert np.shape(x) == (581012, 54)\n    assert np.shape(y) == (581012,)\n\n\ndef test_mnist_data_load():\n    def mean_pixels(i, mean_pix):\n        batch, _ = fetch(i, idx)\n        return mean_pix + np.sum(batch) / batch.size\n\n    init, fetch = load_dataset(MNIST, batch_size=128, split='train')\n    num_batches, idx = init()\n    assert fori_loop(0, num_batches, mean_pixels, np.float32(0.)) / num_batches < 0.15\n\n\ndef test_sp500_data_load():\n    _, fetch = load_dataset(SP500, split='train', shuffle=False)\n    date, value = fetch()\n    assert np.shape(date) == np.shape(date) == (2427,)\n"""
test/test_examples.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nfrom subprocess import check_call\nimport sys\n\nimport pytest\n\nTESTS_DIR = os.path.dirname(os.path.abspath(__file__))\nEXAMPLES_DIR = os.path.join(os.path.dirname(TESTS_DIR), \'examples\')\n\n\nEXAMPLES = [\n    \'baseball.py --num-samples 100 --num-warmup 100 --num-chains 2\',\n    \'bnn.py --num-samples 10 --num-warmup 10 --num-data 7 --num-chains 2\',\n    \'covtype.py --algo HMC --num-samples 10\',\n    \'gp.py --num-samples 10 --num-warmup 10 --num-chains 2\',\n    \'hmm.py --num-samples 100 --num-warmup 100 --num-chains 2\',\n    \'minipyro.py\',\n    \'neutra.py --num-samples 100 --num-warmup 100\',\n    \'ode.py --num-samples 100 --num-warmup 100 --num-chains 1\',\n    \'sparse_regression.py --num-samples 10 --num-warmup 10 --num-data 10 --num-dimensions 10\',\n    \'stochastic_volatility.py --num-samples 100 --num-warmup 100\',\n    \'ucbadmit.py --num-chains 2\',\n    \'vae.py -n 1\',\n]\n\n\n@pytest.mark.parametrize(\'example\', EXAMPLES)\n@pytest.mark.filterwarnings(""ignore:There are not enough devices:UserWarning"")\ndef test_cpu(example):\n    print(\'Running:\\npython examples/{}\'.format(example))\n    example = example.split()\n    filename, args = example[0], example[1:]\n    filename = os.path.join(EXAMPLES_DIR, filename)\n    check_call([sys.executable, filename] + args)\n'"
test/test_flows.py,7,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nimport numpy as onp\nfrom numpy.testing import assert_allclose\nimport pytest\n\nfrom jax import jacfwd, random\nfrom jax.experimental import stax\n\nfrom numpyro.contrib.nn import AutoregressiveNN, BlockNeuralAutoregressiveNN\nfrom numpyro.distributions.flows import BlockNeuralAutoregressiveTransform, InverseAutoregressiveTransform\nfrom numpyro.distributions.util import matrix_to_tril_vec\n\n\ndef _make_iaf_args(input_dim, hidden_dims):\n    _, rng_perm = random.split(random.PRNGKey(0))\n    perm = random.permutation(rng_perm, onp.arange(input_dim))\n    # we use Elu nonlinearity because the default one, Relu, masks out negative hidden values,\n    # which in turn create some zero entries in the lower triangular part of Jacobian.\n    arn_init, arn = AutoregressiveNN(input_dim, hidden_dims, param_dims=[1, 1],\n                                     permutation=perm, nonlinearity=stax.Elu)\n    _, init_params = arn_init(random.PRNGKey(0), (input_dim,))\n    return partial(arn, init_params),\n\n\ndef _make_bnaf_args(input_dim, hidden_factors):\n    arn_init, arn = BlockNeuralAutoregressiveNN(input_dim, hidden_factors)\n    _, rng_key_perm = random.split(random.PRNGKey(0))\n    _, init_params = arn_init(random.PRNGKey(0), (input_dim,))\n    return partial(arn, init_params),\n\n\n@pytest.mark.parametrize('flow_class, flow_args, input_dim', [\n    (InverseAutoregressiveTransform, _make_iaf_args(5, hidden_dims=[10]), 5),\n    (InverseAutoregressiveTransform, _make_iaf_args(7, hidden_dims=[8, 9]), 7),\n    (BlockNeuralAutoregressiveTransform, _make_bnaf_args(7, hidden_factors=[4]), 7),\n    (BlockNeuralAutoregressiveTransform, _make_bnaf_args(7, hidden_factors=[2, 3]), 7),\n])\n@pytest.mark.parametrize('batch_shape', [(), (1,), (4,), (2, 3)])\ndef test_flows(flow_class, flow_args, input_dim, batch_shape):\n    transform = flow_class(*flow_args)\n    x = random.normal(random.PRNGKey(0), batch_shape + (input_dim,))\n\n    # test inverse is correct\n    y = transform(x)\n    try:\n        inv = transform.inv(y)\n        assert_allclose(x, inv, atol=1e-5)\n    except NotImplementedError:\n        pass\n\n    # test jacobian shape\n    actual = transform.log_abs_det_jacobian(x, y)\n    assert onp.shape(actual) == batch_shape\n\n    if batch_shape == ():\n        # make sure transform.log_abs_det_jacobian is correct\n        jac = jacfwd(transform)(x)\n        expected = onp.linalg.slogdet(jac)[1]\n        assert_allclose(actual, expected, atol=1e-5)\n\n        # make sure jacobian is triangular, first permute jacobian as necessary\n        if isinstance(transform, InverseAutoregressiveTransform):\n            permuted_jac = onp.zeros(jac.shape)\n            _, rng_key_perm = random.split(random.PRNGKey(0))\n            perm = random.permutation(rng_key_perm, onp.arange(input_dim))\n\n            for j in range(input_dim):\n                for k in range(input_dim):\n                    permuted_jac[j, k] = jac[perm[j], perm[k]]\n\n            jac = permuted_jac\n\n        assert onp.sum(onp.abs(onp.triu(jac, 1))) == 0.00\n        assert onp.all(onp.abs(matrix_to_tril_vec(jac)) > 0)\n"""
test/test_handlers.py,10,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as onp\nfrom numpy.testing import assert_allclose, assert_raises\nimport pytest\n\nfrom jax import jit\nfrom jax import numpy as np\nfrom jax import random, vmap\n\nimport numpyro\nfrom numpyro import handlers\nimport numpyro.distributions as dist\nfrom numpyro.infer.util import log_density\nfrom numpyro.util import optional\n\n\n@pytest.mark.parametrize(\'mask_last\', [1, 5, 10])\n@pytest.mark.parametrize(\'use_jit\', [False, True])\ndef test_mask(mask_last, use_jit):\n    N = 10\n    mask = onp.ones(N, dtype=onp.bool)\n    mask[-mask_last] = 0\n\n    def model(data, mask):\n        with numpyro.plate(\'N\', N):\n            x = numpyro.sample(\'x\', dist.Normal(0, 1))\n            with handlers.mask(mask_array=mask):\n                numpyro.sample(\'y\', dist.Delta(x, log_density=1.))\n                with handlers.scale(scale_factor=2):\n                    numpyro.sample(\'obs\', dist.Normal(x, 1), obs=data)\n\n    data = random.normal(random.PRNGKey(0), (N,))\n    x = random.normal(random.PRNGKey(1), (N,))\n    if use_jit:\n        log_joint = jit(log_density, static_argnums=(0,))(model, (data, mask), {}, {\'x\': x, \'y\': x})[0]\n    else:\n        log_joint = log_density(model, (data, mask), {}, {\'x\': x, \'y\': x})[0]\n    log_prob_x = dist.Normal(0, 1).log_prob(x)\n    log_prob_y = mask\n    log_prob_z = dist.Normal(x, 1).log_prob(data)\n    expected = (log_prob_x + np.where(mask,  log_prob_y + 2 * log_prob_z, 0.)).sum()\n    assert_allclose(log_joint, expected, atol=1e-4)\n\n\n@pytest.mark.parametrize(\'use_context_manager\', [True, False])\ndef test_scale(use_context_manager):\n    def model(data):\n        x = numpyro.sample(\'x\', dist.Normal(0, 1))\n        with optional(use_context_manager, handlers.scale(scale_factor=10)):\n            numpyro.sample(\'obs\', dist.Normal(x, 1), obs=data)\n\n    model = model if use_context_manager else handlers.scale(model, 10.)\n    data = random.normal(random.PRNGKey(0), (3,))\n    x = random.normal(random.PRNGKey(1))\n    log_joint = log_density(model, (data,), {}, {\'x\': x})[0]\n    log_prob1, log_prob2 = dist.Normal(0, 1).log_prob(x), dist.Normal(x, 1).log_prob(data).sum()\n    expected = log_prob1 + 10 * log_prob2 if use_context_manager else 10 * (log_prob1 + log_prob2)\n    assert_allclose(log_joint, expected)\n\n\ndef test_substitute():\n    def model():\n        x = numpyro.param(\'x\', None)\n        y = handlers.substitute(lambda: numpyro.param(\'y\', None) * numpyro.param(\'x\', None), {\'y\': x})()\n        return x + y\n\n    assert handlers.substitute(model, {\'x\': 3.})() == 12.\n\n\ndef test_seed():\n    def _sample():\n        x = numpyro.sample(\'x\', dist.Normal(0., 1.))\n        y = numpyro.sample(\'y\', dist.Normal(1., 2.))\n        return np.stack([x, y])\n\n    xs = []\n    for i in range(100):\n        with handlers.seed(rng_seed=i):\n            xs.append(_sample())\n    xs = np.stack(xs)\n\n    ys = vmap(lambda rng_key: handlers.seed(lambda: _sample(), rng_key)())(np.arange(100))\n    assert_allclose(xs, ys, atol=1e-6)\n\n\ndef test_nested_seeding():\n    def fn(rng_key_1, rng_key_2, rng_key_3):\n        xs = []\n        with handlers.seed(rng_seed=rng_key_1):\n            with handlers.seed(rng_seed=rng_key_2):\n                xs.append(numpyro.sample(\'x\', dist.Normal(0., 1.)))\n                with handlers.seed(rng_seed=rng_key_3):\n                    xs.append(numpyro.sample(\'y\', dist.Normal(0., 1.)))\n        return np.stack(xs)\n\n    s1, s2 = fn(0, 1, 2), fn(3, 1, 2)\n    assert_allclose(s1, s2)\n    s1, s2 = fn(0, 1, 2), fn(3, 1, 4)\n    assert_allclose(s1[0], s2[0])\n    assert_raises(AssertionError, assert_allclose, s1[1], s2[1])\n\n\ndef test_condition():\n    def model():\n        x = numpyro.sample(\'x\', dist.Delta(0.))\n        y = numpyro.sample(\'y\', dist.Normal(0., 1.))\n        return x + y\n\n    model = handlers.condition(handlers.seed(model, random.PRNGKey(1)), {\'y\': 2.})\n    model_trace = handlers.trace(model).get_trace()\n    assert model_trace[\'y\'][\'value\'] == 2.\n    assert model_trace[\'y\'][\'is_observed\']\n    # Raise ValueError when site is already observed.\n    with pytest.raises(ValueError):\n        handlers.condition(model, {\'y\': 3.})()\n\n\ndef test_no_split_deterministic():\n    def model():\n        x = numpyro.sample(\'x\', dist.Normal(0., 1.))\n        y = numpyro.sample(\'y\', dist.Normal(0., 1.))\n        return x + y\n\n    model = handlers.condition(model, {\'x\': 1., \'y\': 2.})\n    assert model() == 3.\n\n\ndef model_nested_plates_0():\n    with numpyro.plate(\'outer\', 10):\n        x = numpyro.sample(\'y\', dist.Normal(0., 1.))\n        assert x.shape == (10,)\n        with numpyro.plate(\'inner\', 5):\n            y = numpyro.sample(\'x\', dist.Normal(0., 1.))\n            assert y.shape == (5, 10)\n            z = numpyro.deterministic(\'z\', x ** 2)\n            assert z.shape == (10,)\n\n\ndef model_nested_plates_1():\n    with numpyro.plate(\'outer\', 10, dim=-2):\n        x = numpyro.sample(\'y\', dist.Normal(0., 1.))\n        assert x.shape == (10, 1)\n        with numpyro.plate(\'inner\', 5):\n            y = numpyro.sample(\'x\', dist.Normal(0., 1.))\n            assert y.shape == (10, 5)\n            z = numpyro.deterministic(\'z\', x ** 2)\n            assert z.shape == (10, 1)\n\n\ndef model_nested_plates_2():\n    outer = numpyro.plate(\'outer\', 10)\n    inner = numpyro.plate(\'inner\', 5, dim=-3)\n    with outer:\n        x = numpyro.sample(\'x\', dist.Normal(0., 1.))\n        assert x.shape == (10,)\n    with inner:\n        y = numpyro.sample(\'y\', dist.Normal(0., 1.))\n        assert y.shape == (5, 1, 1)\n        z = numpyro.deterministic(\'z\', x ** 2)\n        assert z.shape == (10,)\n\n    with outer, inner:\n        xy = numpyro.sample(\'xy\', dist.Normal(0., 1.), sample_shape=(10,))\n        assert xy.shape == (5, 1, 10)\n\n\ndef model_nested_plates_3():\n    outer = numpyro.plate(\'outer\', 10, dim=-1)\n    inner = numpyro.plate(\'inner\', 5, dim=-2)\n    numpyro.deterministic(\'z\', 1.)\n\n    with inner, outer:\n        xy = numpyro.sample(\'xy\', dist.Normal(np.zeros((5, 10)), 1.))\n        assert xy.shape == (5, 10)\n\n\ndef model_dist_batch_shape():\n    outer = numpyro.plate(\'outer\', 10)\n    inner = numpyro.plate(\'inner\', 5, dim=-3)\n    with outer:\n        x = numpyro.sample(\'x\', dist.Normal(np.zeros(10), 1.))\n        assert x.shape == (10,)\n    with inner:\n        y = numpyro.sample(\'y\', dist.Normal(0., np.ones(10)))\n        assert y.shape == (5, 1, 10)\n        z = numpyro.deterministic(\'z\', x ** 2)\n        assert z.shape == (10,)\n\n    with outer, inner:\n        xy = numpyro.sample(\'xy\', dist.Normal(0., np.ones(10)), sample_shape=(10,))\n        assert xy.shape == (5, 10, 10)\n\n\ndef model_subsample_1():\n    outer = numpyro.plate(\'outer\', 20, subsample_size=10)\n    inner = numpyro.plate(\'inner\', 10, subsample_size=5, dim=-3)\n    with outer:\n        x = numpyro.sample(\'x\', dist.Normal(0., 1.))\n        assert x.shape == (10,)\n    with inner:\n        y = numpyro.sample(\'y\', dist.Normal(0., 1.))\n        assert y.shape == (5, 1, 1)\n        z = numpyro.deterministic(\'z\', x ** 2)\n        assert z.shape == (10,)\n\n    with outer, inner:\n        xy = numpyro.sample(\'xy\', dist.Normal(0., 1.))\n        assert xy.shape == (5, 1, 10)\n\n\n@pytest.mark.parametrize(\'model\', [\n    model_nested_plates_0,\n    model_nested_plates_1,\n    model_nested_plates_2,\n    model_nested_plates_3,\n    model_dist_batch_shape,\n    model_subsample_1,\n])\ndef test_plate(model):\n    trace = handlers.trace(handlers.seed(model, random.PRNGKey(1))).get_trace()\n    jit_trace = handlers.trace(jit(handlers.seed(model, random.PRNGKey(1)))).get_trace()\n    assert \'z\' in trace\n    for name, site in trace.items():\n        if site[\'type\'] == \'sample\':\n            assert_allclose(jit_trace[name][\'value\'], site[\'value\'])\n\n\ndef test_messenger_fn_invalid():\n    with pytest.raises(ValueError, match=""to be a Python callable object""):\n        with numpyro.handlers.mask(False):\n            pass\n\n\n@pytest.mark.parametrize(\'shape\', [(), (5,), (2, 3)])\ndef test_plate_stack(shape):\n    def guide():\n        with numpyro.plate_stack(""plates"", shape):\n            return numpyro.sample(""x"", dist.Normal(0, 1))\n\n    x = handlers.seed(guide, 0)()\n    assert x.shape == shape\n'"
test/test_hmc_util.py,38,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\nimport logging\nimport os\n\nimport numpy as onp\nfrom numpy.testing import assert_allclose\nimport pytest\n\nfrom jax import device_put, disable_jit, grad, jit, random, tree_map\nimport jax.numpy as np\n\nimport numpyro.distributions as dist\nfrom numpyro.infer.hmc_util import (\n    AdaptWindow,\n    _is_iterative_turning,\n    _leaf_idx_to_ckpt_idxs,\n    build_adaptation_schedule,\n    build_tree,\n    consensus,\n    dual_averaging,\n    find_reasonable_step_size,\n    parametric_draws,\n    velocity_verlet,\n    warmup_adapter,\n    welford_covariance\n)\nfrom numpyro.util import control_flow_prims_disabled, fori_loop, optional\n\nlogger = logging.getLogger(__name__)\n\n\n@pytest.mark.parametrize(\'jitted\', [True, False])\ndef test_dual_averaging(jitted):\n    def optimize(f):\n        da_init, da_update = dual_averaging(gamma=0.5)\n        da_state = da_init()\n        for i in range(10):\n            x = da_state[0]\n            g = grad(f)(x)\n            da_state = da_update(g, da_state)\n        x_avg = da_state[1]\n        return x_avg\n\n    f = lambda x: (x + 1) ** 2  # noqa: E731\n    fn = jit(optimize, static_argnums=(0,)) if jitted else optimize\n    x_opt = fn(f)\n\n    assert_allclose(x_opt, -1., atol=1e-3)\n\n\n@pytest.mark.parametrize(\'jitted\', [True, False])\n@pytest.mark.parametrize(\'diagonal\', [True, False])\n@pytest.mark.parametrize(\'regularize\', [True, False])\n@pytest.mark.filterwarnings(\'ignore:numpy.linalg support is experimental:UserWarning\')\ndef test_welford_covariance(jitted, diagonal, regularize):\n    with optional(jitted, disable_jit()), optional(jitted, control_flow_prims_disabled()):\n        onp.random.seed(0)\n        loc = onp.random.randn(3)\n        a = onp.random.randn(3, 3)\n        target_cov = onp.matmul(a, a.T)\n        x = onp.random.multivariate_normal(loc, target_cov, size=(2000,))\n        x = device_put(x)\n\n        @jit\n        def get_cov(x):\n            wc_init, wc_update, wc_final = welford_covariance(diagonal=diagonal)\n            wc_state = wc_init(3)\n            wc_state = fori_loop(0, 2000, lambda i, val: wc_update(x[i], val), wc_state)\n            cov, cov_inv_sqrt = wc_final(wc_state, regularize=regularize)\n            return cov, cov_inv_sqrt\n\n        cov, cov_inv_sqrt = get_cov(x)\n\n        if diagonal:\n            diag_cov = np.diagonal(target_cov)\n            assert_allclose(cov, diag_cov, rtol=0.06)\n            assert_allclose(cov_inv_sqrt, np.sqrt(np.reciprocal(diag_cov)), rtol=0.06)\n        else:\n            assert_allclose(cov, target_cov, rtol=0.06)\n            assert_allclose(cov_inv_sqrt, np.linalg.cholesky(np.linalg.inv(cov)), rtol=0.06)\n\n\n########################################\n# verlocity_verlet Test\n########################################\n\nTEST_EXAMPLES = []\nEXAMPLE_IDS = []\n\nModelArgs = namedtuple(\'model_args\', [\'step_size\', \'num_steps\', \'q_i\', \'p_i\', \'q_f\', \'p_f\', \'m_inv\', \'prec\'])\nExample = namedtuple(\'test_case\', [\'model\', \'args\'])\n\n\ndef register_model(init_args):\n    """"""\n    Register the model along with each of the model arguments\n    as test examples.\n    """"""\n    def register_fn(model):\n        for args in init_args:\n            test_example = Example(model, args)\n            TEST_EXAMPLES.append(test_example)\n            EXAMPLE_IDS.append(model.__name__)\n    return register_fn\n\n\n@register_model([\n    ModelArgs(\n        step_size=0.01,\n        num_steps=100,\n        q_i={\'x\': 0.0},\n        p_i={\'x\': 1.0},\n        q_f={\'x\': np.sin(1.0)},\n        p_f={\'x\': np.cos(1.0)},\n        m_inv=np.array([1.]),\n        prec=1e-4\n    )\n])\nclass HarmonicOscillator(object):\n    @staticmethod\n    def kinetic_fn(m_inv, p):\n        return 0.5 * np.sum(m_inv * p[\'x\'] ** 2)\n\n    @staticmethod\n    def potential_fn(q):\n        return 0.5 * q[\'x\'] ** 2\n\n\n@register_model([\n    ModelArgs(\n        step_size=0.01,\n        num_steps=628,\n        q_i={\'x\': 1.0, \'y\': 0.0},\n        p_i={\'x\': 0.0, \'y\': 1.0},\n        q_f={\'x\': 1.0, \'y\': 0.0},\n        p_f={\'x\': 0.0, \'y\': 1.0},\n        m_inv=np.array([1., 1.]),\n        prec=5.0e-3\n    )\n])\nclass CircularPlanetaryMotion(object):\n    @staticmethod\n    def kinetic_fn(m_inv, p):\n        z = np.stack([p[\'x\'], p[\'y\']], axis=-1)\n        return 0.5 * np.dot(m_inv, z**2)\n\n    @staticmethod\n    def potential_fn(q):\n        return - 1.0 / np.power(q[\'x\'] ** 2 + q[\'y\'] ** 2, 0.5)\n\n\n@register_model([\n    ModelArgs(\n        step_size=0.1,\n        num_steps=1810,\n        q_i={\'x\': 0.02},\n        p_i={\'x\': 0.0},\n        q_f={\'x\': -0.02},\n        p_f={\'x\': 0.0},\n        m_inv=np.array([1.]),\n        prec=1.0e-4\n    )\n])\nclass QuarticOscillator(object):\n    @staticmethod\n    def kinetic_fn(m_inv, p):\n        return 0.5 * np.sum(m_inv * p[\'x\'] ** 2)\n\n    @staticmethod\n    def potential_fn(q):\n        return 0.25 * np.power(q[\'x\'], 4.0)\n\n\n@pytest.mark.parametrize(\'jitted\', [True, False])\n@pytest.mark.parametrize(\'example\', TEST_EXAMPLES, ids=EXAMPLE_IDS)\ndef test_velocity_verlet(jitted, example):\n    def get_final_state(model, step_size, num_steps, q_i, p_i):\n        vv_init, vv_update = velocity_verlet(model.potential_fn, model.kinetic_fn)\n        vv_state = vv_init(q_i, p_i)\n        q_f, p_f, _, _ = fori_loop(0, num_steps,\n                                   lambda i, val: vv_update(step_size, args.m_inv, val),\n                                   vv_state)\n        return (q_f, p_f)\n\n    model, args = example\n    fn = jit(get_final_state, static_argnums=(0,)) if jitted else get_final_state\n    q_f, p_f = fn(model, args.step_size, args.num_steps, args.q_i, args.p_i)\n\n    logger.info(\'Test trajectory:\')\n    logger.info(\'initial q: {}\'.format(args.q_i))\n    logger.info(\'final q: {}\'.format(q_f))\n    for node in args.q_f:\n        assert_allclose(q_f[node], args.q_f[node], atol=args.prec)\n        assert_allclose(p_f[node], args.p_f[node], atol=args.prec)\n\n    logger.info(\'Test energy conservation:\')\n    energy_initial = model.kinetic_fn(args.m_inv, args.p_i) + model.potential_fn(args.q_i)\n    energy_final = model.kinetic_fn(args.m_inv, p_f) + model.potential_fn(q_f)\n    logger.info(\'initial energy: {}\'.format(energy_initial))\n    logger.info(\'final energy: {}\'.format(energy_final))\n    assert_allclose(energy_initial, energy_final, atol=1e-5)\n\n    logger.info(\'Test time reversibility:\')\n    p_reverse = tree_map(lambda x: -x, p_f)\n    q_i, p_i = get_final_state(model, args.step_size, args.num_steps, q_f, p_reverse)\n    for node in args.q_i:\n        assert_allclose(q_i[node], args.q_i[node], atol=1e-4)\n\n\n@pytest.mark.parametrize(\'jitted\', [True, False])\n@pytest.mark.parametrize(\'init_step_size\', [0.1, 10.0])\ndef test_find_reasonable_step_size(jitted, init_step_size):\n    def kinetic_fn(m_inv, p):\n        return 0.5 * np.sum(m_inv * p ** 2)\n\n    def potential_fn(q):\n        return 0.5 * q ** 2\n\n    p_generator = lambda prototype, m_inv, rng_key: 1.0  # noqa: E731\n    q = 0.0\n    m_inv = np.array([1.])\n\n    fn = (jit(find_reasonable_step_size, static_argnums=(0, 1, 2))\n          if jitted else find_reasonable_step_size)\n    rng_key = random.PRNGKey(0)\n    step_size = fn(potential_fn, kinetic_fn, p_generator, init_step_size, m_inv, q, rng_key)\n\n    # Apply 1 velocity verlet step with step_size=eps, we have\n    # z_new = eps, r_new = 1 - eps^2 / 2, hence energy_new = 0.5 + eps^4 / 8,\n    # hence delta_energy = energy_new - energy_init = eps^4 / 8.\n    # We want to find a reasonable step_size such that delta_energy ~ -log(0.8),\n    # hence that step_size ~ the following threshold\n    threshold = np.power(-np.log(0.8) * 8, 0.25)\n\n    # Confirm that given init_step_size, we will doubly increase/decrease it\n    # until it passes threshold.\n    if init_step_size < threshold:\n        assert step_size / 2 < threshold\n        assert step_size > threshold\n    else:\n        assert step_size * 2 > threshold\n        assert step_size < threshold\n\n\n@pytest.mark.parametrize(\'num_steps, expected\', [\n    (18, [(0, 17)]),\n    (50, [(0, 6), (7, 44), (45, 49)]),\n    (100, [(0, 14), (15, 89), (90, 99)]),\n    (150, [(0, 74), (75, 99), (100, 149)]),\n    (200, [(0, 74), (75, 99), (100, 149), (150, 199)]),\n    (280, [(0, 74), (75, 99), (100, 229), (230, 279)]),\n])\ndef test_build_adaptation_schedule(num_steps, expected):\n    adaptation_schedule = build_adaptation_schedule(num_steps)\n    expected_schedule = [AdaptWindow(i, j) for i, j in expected]\n    assert adaptation_schedule == expected_schedule\n\n\n@pytest.mark.parametrize(\'jitted\', [\n    True,\n    pytest.param(False, marks=pytest.mark.skipif(""CI"" in os.environ, reason=""slow in Travis""))\n])\ndef test_warmup_adapter(jitted):\n    def find_reasonable_step_size(step_size, m_inv, z, rng_key):\n        return np.where(step_size < 1, step_size * 4, step_size / 4)\n\n    num_steps = 150\n    adaptation_schedule = build_adaptation_schedule(num_steps)\n    init_step_size = 1.\n    mass_matrix_size = 3\n\n    wa_init, wa_update = warmup_adapter(num_steps, find_reasonable_step_size)\n    wa_update = jit(wa_update) if jitted else wa_update\n\n    rng_key = random.PRNGKey(0)\n    z = np.ones(3)\n    wa_state = wa_init(z, rng_key, init_step_size, mass_matrix_size=mass_matrix_size)\n    step_size, inverse_mass_matrix, _, _, _, window_idx, _ = wa_state\n    assert step_size == find_reasonable_step_size(init_step_size, inverse_mass_matrix, z, rng_key)\n    assert_allclose(inverse_mass_matrix, np.ones(mass_matrix_size))\n    assert window_idx == 0\n\n    window = adaptation_schedule[0]\n    for t in range(window.start, window.end + 1):\n        wa_state = wa_update(t, 0.7 + 0.1 * t / (window.end - window.start), z, wa_state)\n    last_step_size = step_size\n    step_size, inverse_mass_matrix, _, _, _, window_idx, _ = wa_state\n    assert window_idx == 1\n    # step_size is decreased because accept_prob < target_accept_prob\n    assert step_size < last_step_size\n    # inverse_mass_matrix does not change at the end of the first window\n    assert_allclose(inverse_mass_matrix, np.ones(mass_matrix_size))\n\n    window = adaptation_schedule[1]\n    window_len = window.end - window.start\n    for t in range(window.start, window.end + 1):\n        wa_state = wa_update(t, 0.8 + 0.1 * (t - window.start) / window_len, 2 * z, wa_state)\n    last_step_size = step_size\n    step_size, inverse_mass_matrix, _, _, _, window_idx, _ = wa_state\n    assert window_idx == 2\n    # step_size is increased because accept_prob > target_accept_prob\n    assert step_size > last_step_size\n    # Verifies that inverse_mass_matrix changes at the end of the second window.\n    # Because z_flat is constant during the second window, covariance will be 0\n    # and only regularize_term of welford scheme is involved.\n    # This also verifies that z_flat terms in the first window does not affect\n    # the second window.\n    welford_regularize_term = 1e-3 * (5 / (window.end + 1 - window.start + 5))\n    assert_allclose(inverse_mass_matrix,\n                    np.full((mass_matrix_size,), welford_regularize_term),\n                    atol=1e-7)\n\n    window = adaptation_schedule[2]\n    for t in range(window.start, window.end + 1):\n        wa_state = wa_update(t, 0.8, t * z, wa_state)\n    last_step_size = step_size\n    step_size, final_inverse_mass_matrix, _, _, _, window_idx, _ = wa_state\n    assert window_idx == 3\n    # during the last window, because target_accept_prob=0.8,\n    # log_step_size will be equal to the constant prox_center=log(10*last_step_size)\n    assert_allclose(step_size, last_step_size * 10)\n    # Verifies that inverse_mass_matrix does not change during the last window\n    # despite z_flat changes w.r.t time t,\n    assert_allclose(final_inverse_mass_matrix, inverse_mass_matrix)\n\n\n@pytest.mark.parametrize(\'leaf_idx, ckpt_idxs\', [\n    (0, (1, 0)),\n    (6, (3, 2)),\n    (7, (0, 2)),\n    (13, (2, 2)),\n    (15, (0, 3)),\n])\ndef test_leaf_idx_to_ckpt_idx(leaf_idx, ckpt_idxs):\n    assert _leaf_idx_to_ckpt_idxs(leaf_idx) == ckpt_idxs\n\n\n@pytest.mark.parametrize(\'ckpt_idxs, expected_turning\', [\n    ((3, 2), False),\n    ((3, 3), True),\n    ((0, 0), False),\n    ((0, 1), True),\n    ((1, 3), True),\n])\ndef test_is_iterative_turning(ckpt_idxs, expected_turning):\n    inverse_mass_matrix = np.ones(1)\n    r = 1.\n    r_sum = 3.\n    r_ckpts = np.array([1., 2., 3., -2.])\n    r_sum_ckpts = np.array([2., 4., 4., -1.])\n\n    actual_turning = _is_iterative_turning(inverse_mass_matrix, r, r_sum, r_ckpts, r_sum_ckpts,\n                                           *ckpt_idxs)\n    assert expected_turning == actual_turning\n\n\n@pytest.mark.parametrize(\'step_size\', [0.01, 1., 100.])\ndef test_build_tree(step_size):\n    def kinetic_fn(m_inv, p):\n        return 0.5 * np.sum(m_inv * p ** 2)\n\n    def potential_fn(q):\n        return 0.5 * q ** 2\n\n    vv_init, vv_update = velocity_verlet(potential_fn, kinetic_fn)\n    vv_state = vv_init(0.0, 1.0)\n    inverse_mass_matrix = np.array([1.])\n    rng_key = random.PRNGKey(0)\n\n    @jit\n    def fn(vv_state):\n        tree = build_tree(vv_update, kinetic_fn, vv_state, inverse_mass_matrix,\n                          step_size, rng_key)\n        return tree\n\n    tree = fn(vv_state)\n\n    assert tree.num_proposals >= 2 ** (tree.depth - 1)\n\n    assert tree.sum_accept_probs <= tree.num_proposals\n\n    if tree.depth < 10:\n        assert tree.turning | tree.diverging\n\n    # for large step_size, assert that diverging will happen in 1 step\n    if step_size > 10:\n        assert tree.diverging\n        assert tree.num_proposals == 1\n\n    # for small step_size, assert that it should take a while to meet the terminate condition\n    if step_size < 0.1:\n        assert tree.num_proposals > 10\n\n\n@pytest.mark.parametrize(\'method\', [consensus, parametric_draws])\n@pytest.mark.parametrize(\'diagonal\', [True, False])\ndef test_gaussian_subposterior(method, diagonal):\n    D = 10\n    n_samples = 10000\n    n_draws = 9000\n    n_subs = 8\n\n    mean = np.arange(D)\n    cov = np.ones((D, D)) * 0.9 + np.identity(D) * 0.1\n    subcov = n_subs * cov  # subposterior\'s covariance\n    subposteriors = list(dist.MultivariateNormal(mean, subcov).sample(\n        random.PRNGKey(1), (n_subs, n_samples)))\n\n    draws = method(subposteriors, n_draws, diagonal=diagonal)\n    assert draws.shape == (n_draws, D)\n    assert_allclose(np.mean(draws, axis=0), mean, atol=0.03)\n    if diagonal:\n        assert_allclose(np.var(draws, axis=0), np.diag(cov), atol=0.05)\n    else:\n        assert_allclose(np.cov(draws.T), cov, atol=0.05)\n\n\n@pytest.mark.parametrize(\'method\', [consensus, parametric_draws])\ndef test_subposterior_structure(method):\n    subposteriors = [{\'x\': np.ones((100, 3)), \'y\': np.zeros((100,))} for i in range(10)]\n    draws = method(subposteriors, num_draws=9)\n    assert draws[\'x\'].shape == (9, 3)\n    assert draws[\'y\'].shape == (9,)\n'"
test/test_infer_util.py,11,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nfrom numpy.testing import assert_allclose\nimport pytest\n\nfrom jax import lax, random\nimport jax.numpy as np\n\nimport numpyro\nfrom numpyro import handlers\nfrom numpyro.contrib.reparam import TransformReparam, reparam\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.transforms import AffineTransform, biject_to\nfrom numpyro.infer import ELBO, MCMC, NUTS, SVI\nfrom numpyro.infer.initialization import (\n    init_to_feasible,\n    init_to_median,\n    init_to_prior,\n    init_to_uniform,\n    init_to_value,\n)\nfrom numpyro.infer.util import (\n    Predictive,\n    constrain_fn,\n    initialize_model,\n    log_likelihood,\n    potential_energy,\n    transform_fn,\n)\nimport numpyro.optim as optim\n\n\ndef beta_bernoulli():\n    N = 800\n    true_probs = np.array([0.2, 0.7])\n    data = dist.Bernoulli(true_probs).sample(random.PRNGKey(0), (N,))\n\n    def model(data=None):\n        with numpyro.plate(""dim"", 2):\n            beta = numpyro.sample(""beta"", dist.Beta(1., 1.))\n        with numpyro.plate(""plate"", N, dim=-2):\n            numpyro.deterministic(""beta_sq"", beta ** 2)\n            numpyro.sample(""obs"", dist.Bernoulli(beta), obs=data)\n\n    return model, data, true_probs\n\n\n@pytest.mark.parametrize(\'parallel\', [True, False])\ndef test_predictive(parallel):\n    model, data, true_probs = beta_bernoulli()\n    mcmc = MCMC(NUTS(model), num_warmup=100, num_samples=100)\n    mcmc.run(random.PRNGKey(0), data)\n    samples = mcmc.get_samples()\n    predictive = Predictive(model, samples, parallel=parallel)\n    predictive_samples = predictive(random.PRNGKey(1))\n    assert predictive_samples.keys() == {""beta_sq"", ""obs""}\n\n    predictive.return_sites = [""beta"", ""beta_sq"", ""obs""]\n    predictive_samples = predictive(random.PRNGKey(1))\n    # check shapes\n    assert predictive_samples[""beta""].shape == (100,) + true_probs.shape\n    assert predictive_samples[""beta_sq""].shape == (100,) + true_probs.shape\n    assert predictive_samples[""obs""].shape == (100,) + data.shape\n    # check sample mean\n    assert_allclose(predictive_samples[""obs""].reshape((-1,) + true_probs.shape).mean(0), true_probs, rtol=0.1)\n\n\ndef test_predictive_with_guide():\n    data = np.array([1] * 8 + [0] * 2)\n\n    def model(data):\n        f = numpyro.sample(""beta"", dist.Beta(1., 1.))\n        with numpyro.plate(""plate"", 10):\n            numpyro.deterministic(""beta_sq"", f ** 2)\n            numpyro.sample(""obs"", dist.Bernoulli(f), obs=data)\n\n    def guide(data):\n        alpha_q = numpyro.param(""alpha_q"", 1.0,\n                                constraint=constraints.positive)\n        beta_q = numpyro.param(""beta_q"", 1.0,\n                               constraint=constraints.positive)\n        numpyro.sample(""beta"", dist.Beta(alpha_q, beta_q))\n\n    svi = SVI(model, guide, optim.Adam(0.1), ELBO())\n    svi_state = svi.init(random.PRNGKey(1), data)\n\n    def body_fn(i, val):\n        svi_state, _ = svi.update(val, data)\n        return svi_state\n\n    svi_state = lax.fori_loop(0, 1000, body_fn, svi_state)\n    params = svi.get_params(svi_state)\n    predictive = Predictive(model, guide=guide, params=params, num_samples=1000)(random.PRNGKey(2), data=None)\n    assert predictive[""beta_sq""].shape == (1000,)\n    obs_pred = predictive[""obs""]\n    assert_allclose(np.mean(obs_pred), 0.8, atol=0.05)\n\n\ndef test_predictive_with_improper():\n    true_coef = 0.9\n\n    def model(data):\n        alpha = numpyro.sample(\'alpha\', dist.Uniform(0, 1))\n        with reparam(config={\'loc\': TransformReparam()}):\n            loc = numpyro.sample(\'loc\', dist.TransformedDistribution(\n                dist.Uniform(0, 1).mask(False),\n                AffineTransform(0, alpha)))\n        numpyro.sample(\'obs\', dist.Normal(loc, 0.1), obs=data)\n\n    data = true_coef + random.normal(random.PRNGKey(0), (1000,))\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000)\n    mcmc.run(random.PRNGKey(0), data)\n    samples = mcmc.get_samples()\n    obs_pred = Predictive(model, samples)(random.PRNGKey(1), data=None)[""obs""]\n    assert_allclose(np.mean(obs_pred), true_coef, atol=0.05)\n\n\ndef test_prior_predictive():\n    model, data, true_probs = beta_bernoulli()\n    predictive_samples = Predictive(model, num_samples=100)(random.PRNGKey(1))\n    assert predictive_samples.keys() == {""beta"", ""beta_sq"", ""obs""}\n\n    # check shapes\n    assert predictive_samples[""beta""].shape == (100,) + true_probs.shape\n    assert predictive_samples[""obs""].shape == (100,) + data.shape\n\n\ndef test_log_likelihood():\n    model, data, _ = beta_bernoulli()\n    samples = Predictive(model, return_sites=[""beta""], num_samples=100)(random.PRNGKey(1))\n    loglik = log_likelihood(model, samples, data)\n    assert loglik.keys() == {""obs""}\n    # check shapes\n    assert loglik[""obs""].shape == (100,) + data.shape\n    assert_allclose(loglik[""obs""], dist.Bernoulli(samples[""beta""].reshape((100, 1, -1))).log_prob(data))\n\n\ndef test_model_with_transformed_distribution():\n    x_prior = dist.HalfNormal(2)\n    y_prior = dist.LogNormal(scale=3.)  # transformed distribution\n\n    def model():\n        numpyro.sample(\'x\', x_prior)\n        numpyro.sample(\'y\', y_prior)\n\n    params = {\'x\': np.array(-5.), \'y\': np.array(7.)}\n    model = handlers.seed(model, random.PRNGKey(0))\n    inv_transforms = {\'x\': biject_to(x_prior.support), \'y\': biject_to(y_prior.support)}\n    expected_samples = partial(transform_fn, inv_transforms)(params)\n    expected_potential_energy = (\n        - x_prior.log_prob(expected_samples[\'x\']) -\n        y_prior.log_prob(expected_samples[\'y\']) -\n        inv_transforms[\'x\'].log_abs_det_jacobian(params[\'x\'], expected_samples[\'x\']) -\n        inv_transforms[\'y\'].log_abs_det_jacobian(params[\'y\'], expected_samples[\'y\'])\n    )\n\n    reparam_model = reparam(model, {\'y\': TransformReparam()})\n    base_params = {\'x\': params[\'x\'], \'y_base\': params[\'y\']}\n    actual_samples = constrain_fn(\n        handlers.seed(reparam_model, random.PRNGKey(0)),\n        (), {}, base_params, return_deterministic=True)\n    actual_potential_energy = potential_energy(reparam_model, (), {}, base_params)\n\n    assert_allclose(expected_samples[\'x\'], actual_samples[\'x\'])\n    assert_allclose(expected_samples[\'y\'], actual_samples[\'y\'])\n    assert_allclose(actual_potential_energy, expected_potential_energy)\n\n\ndef test_model_with_mask_false():\n    def model():\n        x = numpyro.sample(""x"", dist.Normal())\n        with numpyro.handlers.mask(mask_array=False):\n            numpyro.sample(""y"", dist.Normal(x), obs=1)\n\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, num_warmup=500, num_samples=500, num_chains=1)\n    mcmc.run(random.PRNGKey(1))\n    assert_allclose(mcmc.get_samples()[\'x\'].mean(), 0., atol=0.1)\n\n\n@pytest.mark.parametrize(\'init_strategy\', [\n    init_to_feasible(),\n    init_to_median(num_samples=2),\n    init_to_prior(),\n    init_to_uniform(radius=3),\n    init_to_value(values={\'tau\': 0.7}),\n    init_to_feasible,\n    init_to_median,\n    init_to_prior,\n    init_to_uniform,\n    init_to_value,\n])\ndef test_initialize_model_change_point(init_strategy):\n    def model(data):\n        alpha = 1 / np.mean(data)\n        lambda1 = numpyro.sample(\'lambda1\', dist.Exponential(alpha))\n        lambda2 = numpyro.sample(\'lambda2\', dist.Exponential(alpha))\n        tau = numpyro.sample(\'tau\', dist.Uniform(0, 1))\n        lambda12 = np.where(np.arange(len(data)) < tau * len(data), lambda1, lambda2)\n        numpyro.sample(\'obs\', dist.Poisson(lambda12), obs=data)\n\n    count_data = np.array([\n        13,  24,   8,  24,   7,  35,  14,  11,  15,  11,  22,  22,  11,  57,\n        11,  19,  29,   6,  19,  12,  22,  12,  18,  72,  32,   9,   7,  13,\n        19,  23,  27,  20,   6,  17,  13,  10,  14,   6,  16,  15,   7,   2,\n        15,  15,  19,  70,  49,   7,  53,  22,  21,  31,  19,  11,  18,  20,\n        12,  35,  17,  23,  17,   4,   2,  31,  30,  13,  27,   0,  39,  37,\n        5,  14,  13,  22,\n    ])\n\n    rng_keys = random.split(random.PRNGKey(1), 2)\n    init_params, _, _, _ = initialize_model(rng_keys, model,\n                                            init_strategy=init_strategy,\n                                            model_args=(count_data,))\n    for i in range(2):\n        init_params_i, _, _, _ = initialize_model(rng_keys[i], model,\n                                                  init_strategy=init_strategy,\n                                                  model_args=(count_data,))\n        for name, p in init_params[0].items():\n            # XXX: the result is equal if we disable fast-math-mode\n            assert_allclose(p[i], init_params_i[0][name], atol=1e-6)\n\n\n@pytest.mark.parametrize(\'init_strategy\', [\n    init_to_feasible(),\n    init_to_median(num_samples=2),\n    init_to_prior(),\n    init_to_uniform(),\n])\ndef test_initialize_model_dirichlet_categorical(init_strategy):\n    def model(data):\n        concentration = np.array([1.0, 1.0, 1.0])\n        p_latent = numpyro.sample(\'p_latent\', dist.Dirichlet(concentration))\n        numpyro.sample(\'obs\', dist.Categorical(p_latent), obs=data)\n        return p_latent\n\n    true_probs = np.array([0.1, 0.6, 0.3])\n    data = dist.Categorical(true_probs).sample(random.PRNGKey(1), (2000,))\n\n    rng_keys = random.split(random.PRNGKey(1), 2)\n    init_params, _, _, _ = initialize_model(rng_keys, model,\n                                            init_strategy=init_strategy,\n                                            model_args=(data,))\n    for i in range(2):\n        init_params_i, _, _, _ = initialize_model(rng_keys[i], model,\n                                                  init_strategy=init_strategy,\n                                                  model_args=(data,))\n        for name, p in init_params[0].items():\n            # XXX: the result is equal if we disable fast-math-mode\n            assert_allclose(p[i], init_params_i[0][name], atol=1e-6)\n\n\n@pytest.mark.parametrize(\'event_shape\', [(3,), ()])\ndef test_improper_expand(event_shape):\n\n    def model():\n        population = np.array([1000., 2000., 3000.])\n        with numpyro.plate(""region"", 3):\n            d = dist.ImproperUniform(support=constraints.interval(0, population),\n                                     batch_shape=(3,),\n                                     event_shape=event_shape)\n            incidence = numpyro.sample(""incidence"", d)\n            assert d.log_prob(incidence).shape == (3,)\n\n    model_info = initialize_model(random.PRNGKey(0), model)\n    assert model_info.param_info.z[\'incidence\'].shape == (3,) + event_shape\n'"
test/test_mcmc.py,79,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\n\nimport numpy as onp\nfrom jax.test_util import check_close\nfrom numpy.testing import assert_allclose\nimport pytest\n\nfrom jax import jit, pmap, random, vmap\nfrom jax.lib import xla_bridge\nimport jax.numpy as np\nfrom jax.scipy.special import logit\n\nimport numpyro\nfrom numpyro.contrib.reparam import TransformReparam, reparam\nimport numpyro.distributions as dist\nfrom numpyro.distributions.transforms import AffineTransform\nfrom numpyro.infer import HMC, MCMC, NUTS, SA\nfrom numpyro.infer.mcmc import hmc, _get_proposal_loc_and_scale, _numpy_delete\nfrom numpyro.infer.util import initialize_model\nfrom numpyro.util import fori_collect\n\n\n@pytest.mark.parametrize(\'kernel_cls\', [HMC, NUTS, SA])\n@pytest.mark.parametrize(\'dense_mass\', [False, True])\ndef test_unnormalized_normal_x64(kernel_cls, dense_mass):\n    true_mean, true_std = 1., 0.5\n    warmup_steps, num_samples = (100000, 100000) if kernel_cls is SA else (1000, 8000)\n\n    def potential_fn(z):\n        return 0.5 * np.sum(((z - true_mean) / true_std) ** 2)\n\n    init_params = np.array(0.)\n    if kernel_cls is SA:\n        kernel = SA(potential_fn=potential_fn, dense_mass=dense_mass)\n    else:\n        kernel = kernel_cls(potential_fn=potential_fn, trajectory_length=8, dense_mass=dense_mass)\n    mcmc = MCMC(kernel, warmup_steps, num_samples, progress_bar=False)\n    mcmc.run(random.PRNGKey(0), init_params=init_params)\n    mcmc.print_summary()\n    hmc_states = mcmc.get_samples()\n    assert_allclose(np.mean(hmc_states), true_mean, rtol=0.05)\n    assert_allclose(np.std(hmc_states), true_std, rtol=0.05)\n\n    if \'JAX_ENABLE_X64\' in os.environ:\n        assert hmc_states.dtype == np.float64\n\n\ndef test_correlated_mvn():\n    # This requires dense mass matrix estimation.\n    D = 5\n\n    warmup_steps, num_samples = 5000, 8000\n\n    true_mean = 0.\n    a = np.tril(0.5 * np.fliplr(np.eye(D)) + 0.1 * np.exp(random.normal(random.PRNGKey(0), shape=(D, D))))\n    true_cov = np.dot(a, a.T)\n    true_prec = np.linalg.inv(true_cov)\n\n    def potential_fn(z):\n        return 0.5 * np.dot(z.T, np.dot(true_prec, z))\n\n    init_params = np.zeros(D)\n    kernel = NUTS(potential_fn=potential_fn, dense_mass=True)\n    mcmc = MCMC(kernel, warmup_steps, num_samples)\n    mcmc.run(random.PRNGKey(0), init_params=init_params)\n    samples = mcmc.get_samples()\n    assert_allclose(np.mean(samples), true_mean, atol=0.02)\n    assert onp.sum(onp.abs(onp.cov(samples.T) - true_cov)) / D**2 < 0.02\n\n\n@pytest.mark.parametrize(\'kernel_cls\', [HMC, NUTS, SA])\ndef test_logistic_regression_x64(kernel_cls):\n    N, dim = 3000, 3\n    warmup_steps, num_samples = (100000, 100000) if kernel_cls is SA else (1000, 8000)\n    data = random.normal(random.PRNGKey(0), (N, dim))\n    true_coefs = np.arange(1., dim + 1.)\n    logits = np.sum(true_coefs * data, axis=-1)\n    labels = dist.Bernoulli(logits=logits).sample(random.PRNGKey(1))\n\n    def model(labels):\n        coefs = numpyro.sample(\'coefs\', dist.Normal(np.zeros(dim), np.ones(dim)))\n        logits = numpyro.deterministic(\'logits\', np.sum(coefs * data, axis=-1))\n        return numpyro.sample(\'obs\', dist.Bernoulli(logits=logits), obs=labels)\n\n    if kernel_cls is SA:\n        kernel = SA(model=model, adapt_state_size=9)\n    else:\n        kernel = kernel_cls(model=model, trajectory_length=8, find_heuristic_step_size=True)\n    mcmc = MCMC(kernel, warmup_steps, num_samples, progress_bar=False)\n    mcmc.run(random.PRNGKey(2), labels)\n    mcmc.print_summary()\n    samples = mcmc.get_samples()\n    assert samples[\'logits\'].shape == (num_samples, N)\n    assert_allclose(np.mean(samples[\'coefs\'], 0), true_coefs, atol=0.22)\n\n    if \'JAX_ENABLE_X64\' in os.environ:\n        assert samples[\'coefs\'].dtype == np.float64\n\n\ndef test_uniform_normal():\n    true_coef = 0.9\n    num_warmup, num_samples = 1000, 1000\n\n    def model(data):\n        alpha = numpyro.sample(\'alpha\', dist.Uniform(0, 1))\n        with reparam(config={\'loc\': TransformReparam()}):\n            loc = numpyro.sample(\'loc\', dist.Uniform(0, alpha))\n        numpyro.sample(\'obs\', dist.Normal(loc, 0.1), obs=data)\n\n    data = true_coef + random.normal(random.PRNGKey(0), (1000,))\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples)\n    mcmc.warmup(random.PRNGKey(2), data, collect_warmup=True)\n    warmup_samples = mcmc.get_samples()\n    mcmc.run(random.PRNGKey(3), data)\n    samples = mcmc.get_samples()\n    assert len(warmup_samples[\'loc\']) == num_warmup\n    assert len(samples[\'loc\']) == num_samples\n    assert_allclose(np.mean(samples[\'loc\'], 0), true_coef, atol=0.05)\n\n\ndef test_improper_normal():\n    true_coef = 0.9\n\n    def model(data):\n        alpha = numpyro.sample(\'alpha\', dist.Uniform(0, 1))\n        with reparam(config={\'loc\': TransformReparam()}):\n            loc = numpyro.sample(\'loc\', dist.TransformedDistribution(\n                dist.Uniform(0, 1).mask(False),\n                AffineTransform(0, alpha)))\n        numpyro.sample(\'obs\', dist.Normal(loc, 0.1), obs=data)\n\n    data = true_coef + random.normal(random.PRNGKey(0), (1000,))\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, num_warmup=1000, num_samples=1000)\n    mcmc.run(random.PRNGKey(0), data)\n    samples = mcmc.get_samples()\n    assert_allclose(np.mean(samples[\'loc\'], 0), true_coef, atol=0.05)\n\n\n@pytest.mark.parametrize(\'kernel_cls\', [HMC, NUTS, SA])\ndef test_beta_bernoulli_x64(kernel_cls):\n    warmup_steps, num_samples = (100000, 100000) if kernel_cls is SA else (500, 20000)\n\n    def model(data):\n        alpha = np.array([1.1, 1.1])\n        beta = np.array([1.1, 1.1])\n        p_latent = numpyro.sample(\'p_latent\', dist.Beta(alpha, beta))\n        numpyro.sample(\'obs\', dist.Bernoulli(p_latent), obs=data)\n        return p_latent\n\n    true_probs = np.array([0.9, 0.1])\n    data = dist.Bernoulli(true_probs).sample(random.PRNGKey(1), (1000, 2))\n    if kernel_cls is SA:\n        kernel = SA(model=model)\n    else:\n        kernel = kernel_cls(model=model, trajectory_length=0.1)\n    mcmc = MCMC(kernel, num_warmup=warmup_steps, num_samples=num_samples, progress_bar=False)\n    mcmc.run(random.PRNGKey(2), data)\n    mcmc.print_summary()\n    samples = mcmc.get_samples()\n    assert_allclose(np.mean(samples[\'p_latent\'], 0), true_probs, atol=0.05)\n\n    if \'JAX_ENABLE_X64\' in os.environ:\n        assert samples[\'p_latent\'].dtype == np.float64\n\n\n@pytest.mark.parametrize(\'kernel_cls\', [HMC, NUTS])\n@pytest.mark.parametrize(\'dense_mass\', [False, True])\ndef test_dirichlet_categorical_x64(kernel_cls, dense_mass):\n    warmup_steps, num_samples = 100, 20000\n\n    def model(data):\n        concentration = np.array([1.0, 1.0, 1.0])\n        p_latent = numpyro.sample(\'p_latent\', dist.Dirichlet(concentration))\n        numpyro.sample(\'obs\', dist.Categorical(p_latent), obs=data)\n        return p_latent\n\n    true_probs = np.array([0.1, 0.6, 0.3])\n    data = dist.Categorical(true_probs).sample(random.PRNGKey(1), (2000,))\n    kernel = kernel_cls(model, trajectory_length=1., dense_mass=dense_mass)\n    mcmc = MCMC(kernel, warmup_steps, num_samples, progress_bar=False)\n    mcmc.run(random.PRNGKey(2), data)\n    samples = mcmc.get_samples()\n    assert_allclose(np.mean(samples[\'p_latent\'], 0), true_probs, atol=0.02)\n\n    if \'JAX_ENABLE_X64\' in os.environ:\n        assert samples[\'p_latent\'].dtype == np.float64\n\n\ndef test_change_point_x64():\n    # Ref: https://forum.pyro.ai/t/i-dont-understand-why-nuts-code-is-not-working-bayesian-hackers-mail/696\n    warmup_steps, num_samples = 500, 3000\n\n    def model(data):\n        alpha = 1 / np.mean(data)\n        lambda1 = numpyro.sample(\'lambda1\', dist.Exponential(alpha))\n        lambda2 = numpyro.sample(\'lambda2\', dist.Exponential(alpha))\n        tau = numpyro.sample(\'tau\', dist.Uniform(0, 1))\n        lambda12 = np.where(np.arange(len(data)) < tau * len(data), lambda1, lambda2)\n        numpyro.sample(\'obs\', dist.Poisson(lambda12), obs=data)\n\n    count_data = np.array([\n        13,  24,   8,  24,   7,  35,  14,  11,  15,  11,  22,  22,  11,  57,\n        11,  19,  29,   6,  19,  12,  22,  12,  18,  72,  32,   9,   7,  13,\n        19,  23,  27,  20,   6,  17,  13,  10,  14,   6,  16,  15,   7,   2,\n        15,  15,  19,  70,  49,   7,  53,  22,  21,  31,  19,  11,  18,  20,\n        12,  35,  17,  23,  17,   4,   2,  31,  30,  13,  27,   0,  39,  37,\n        5,  14,  13,  22,\n    ])\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, warmup_steps, num_samples)\n    mcmc.run(random.PRNGKey(4), count_data)\n    samples = mcmc.get_samples()\n    tau_posterior = (samples[\'tau\'] * len(count_data)).astype(np.int32)\n    tau_values, counts = onp.unique(tau_posterior, return_counts=True)\n    mode_ind = np.argmax(counts)\n    mode = tau_values[mode_ind]\n    assert mode == 44\n\n    if \'JAX_ENABLE_X64\' in os.environ:\n        assert samples[\'lambda1\'].dtype == np.float64\n        assert samples[\'lambda2\'].dtype == np.float64\n        assert samples[\'tau\'].dtype == np.float64\n\n\n@pytest.mark.parametrize(\'with_logits\', [\'True\', \'False\'])\ndef test_binomial_stable_x64(with_logits):\n    # Ref: https://github.com/pyro-ppl/pyro/issues/1706\n    warmup_steps, num_samples = 200, 200\n\n    def model(data):\n        p = numpyro.sample(\'p\', dist.Beta(1., 1.))\n        if with_logits:\n            logits = logit(p)\n            numpyro.sample(\'obs\', dist.Binomial(data[\'n\'], logits=logits), obs=data[\'x\'])\n        else:\n            numpyro.sample(\'obs\', dist.Binomial(data[\'n\'], probs=p), obs=data[\'x\'])\n\n    data = {\'n\': 5000000, \'x\': 3849}\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, warmup_steps, num_samples)\n    mcmc.run(random.PRNGKey(2), data)\n    samples = mcmc.get_samples()\n    assert_allclose(np.mean(samples[\'p\'], 0), data[\'x\'] / data[\'n\'], rtol=0.05)\n\n    if \'JAX_ENABLE_X64\' in os.environ:\n        assert samples[\'p\'].dtype == np.float64\n\n\ndef test_improper_prior():\n    true_mean, true_std = 1., 2.\n    num_warmup, num_samples = 1000, 8000\n\n    def model(data):\n        mean = numpyro.sample(\'mean\', dist.Normal(0, 1).mask(False))\n        std = numpyro.sample(\'std\', dist.ImproperUniform(dist.constraints.positive, (), ()))\n        return numpyro.sample(\'obs\', dist.Normal(mean, std), obs=data)\n\n    data = dist.Normal(true_mean, true_std).sample(random.PRNGKey(1), (2000,))\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, num_warmup, num_samples)\n    mcmc.warmup(random.PRNGKey(2), data)\n    mcmc.run(random.PRNGKey(2), data)\n    samples = mcmc.get_samples()\n    assert_allclose(np.mean(samples[\'mean\']), true_mean, rtol=0.05)\n    assert_allclose(np.mean(samples[\'std\']), true_std, rtol=0.05)\n\n\ndef test_mcmc_progbar():\n    true_mean, true_std = 1., 2.\n    num_warmup, num_samples = 10, 10\n\n    def model(data):\n        mean = numpyro.sample(\'mean\', dist.Normal(0, 1).mask(False))\n        std = numpyro.sample(\'std\', dist.LogNormal(0, 1).mask(False))\n        return numpyro.sample(\'obs\', dist.Normal(mean, std), obs=data)\n\n    data = dist.Normal(true_mean, true_std).sample(random.PRNGKey(1), (2000,))\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, num_warmup, num_samples)\n    mcmc.warmup(random.PRNGKey(2), data)\n    mcmc.run(random.PRNGKey(3), data)\n    mcmc1 = MCMC(kernel, num_warmup, num_samples, progress_bar=False)\n    mcmc1.run(random.PRNGKey(2), data)\n\n    with pytest.raises(AssertionError):\n        check_close(mcmc1.get_samples(), mcmc.get_samples(), atol=1e-4, rtol=1e-4)\n    mcmc1.warmup(random.PRNGKey(2), data)\n    mcmc1.run(random.PRNGKey(3), data)\n    check_close(mcmc1.get_samples(), mcmc.get_samples(), atol=1e-4, rtol=1e-4)\n    check_close(mcmc1._warmup_state, mcmc._warmup_state, atol=1e-4, rtol=1e-4)\n\n\n@pytest.mark.parametrize(\'kernel_cls\', [HMC, NUTS])\n@pytest.mark.parametrize(\'adapt_step_size\', [True, False])\ndef test_diverging(kernel_cls, adapt_step_size):\n    data = random.normal(random.PRNGKey(0), (1000,))\n\n    def model(data):\n        loc = numpyro.sample(\'loc\', dist.Normal(0., 1.))\n        numpyro.sample(\'obs\', dist.Normal(loc, 1), obs=data)\n\n    kernel = kernel_cls(model, step_size=10., adapt_step_size=adapt_step_size, adapt_mass_matrix=False)\n    num_warmup = num_samples = 1000\n    mcmc = MCMC(kernel, num_warmup, num_samples)\n    mcmc.warmup(random.PRNGKey(1), data, extra_fields=[\'diverging\'], collect_warmup=True)\n    warmup_divergences = mcmc.get_extra_fields()[\'diverging\'].sum()\n    mcmc.run(random.PRNGKey(2), data, extra_fields=[\'diverging\'])\n    num_divergences = warmup_divergences + mcmc.get_extra_fields()[\'diverging\'].sum()\n    if adapt_step_size:\n        assert num_divergences <= num_warmup\n    else:\n        assert_allclose(num_divergences, num_warmup + num_samples)\n\n\ndef test_prior_with_sample_shape():\n    data = {\n        ""J"": 8,\n        ""y"": np.array([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0]),\n        ""sigma"": np.array([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0]),\n    }\n\n    def schools_model():\n        mu = numpyro.sample(\'mu\', dist.Normal(0, 5))\n        tau = numpyro.sample(\'tau\', dist.HalfCauchy(5))\n        theta = numpyro.sample(\'theta\', dist.Normal(mu, tau), sample_shape=(data[\'J\'],))\n        numpyro.sample(\'obs\', dist.Normal(theta, data[\'sigma\']), obs=data[\'y\'])\n\n    num_samples = 500\n    mcmc = MCMC(NUTS(schools_model), num_warmup=500, num_samples=num_samples)\n    mcmc.run(random.PRNGKey(0))\n    assert mcmc.get_samples()[\'theta\'].shape == (num_samples, data[\'J\'])\n\n\n@pytest.mark.parametrize(\'num_chains\', [1, 2])\n@pytest.mark.parametrize(\'chain_method\', [\'parallel\', \'sequential\', \'vectorized\'])\n@pytest.mark.parametrize(\'progress_bar\', [True, False])\n@pytest.mark.filterwarnings(""ignore:There are not enough devices:UserWarning"")\ndef test_empty_model(num_chains, chain_method, progress_bar):\n    def model():\n        pass\n\n    mcmc = MCMC(NUTS(model), num_warmup=10, num_samples=10, num_chains=num_chains,\n                chain_method=chain_method, progress_bar=progress_bar)\n    mcmc.run(random.PRNGKey(0))\n    assert mcmc.get_samples() == {}\n\n\n@pytest.mark.parametrize(\'use_init_params\', [False, True])\n@pytest.mark.parametrize(\'chain_method\', [\'parallel\', \'sequential\', \'vectorized\'])\n@pytest.mark.skipif(\'XLA_FLAGS\' not in os.environ, reason=\'without this mark, we have duplicated tests in Travis\')\ndef test_chain(use_init_params, chain_method):\n    N, dim = 3000, 3\n    num_chains = 2\n    num_warmup, num_samples = 5000, 5000\n    data = random.normal(random.PRNGKey(0), (N, dim))\n    true_coefs = np.arange(1., dim + 1.)\n    logits = np.sum(true_coefs * data, axis=-1)\n    labels = dist.Bernoulli(logits=logits).sample(random.PRNGKey(1))\n\n    def model(labels):\n        coefs = numpyro.sample(\'coefs\', dist.Normal(np.zeros(dim), np.ones(dim)))\n        logits = np.sum(coefs * data, axis=-1)\n        return numpyro.sample(\'obs\', dist.Bernoulli(logits=logits), obs=labels)\n\n    kernel = NUTS(model=model)\n    mcmc = MCMC(kernel, num_warmup, num_samples, num_chains=num_chains)\n    mcmc.chain_method = chain_method\n    init_params = None if not use_init_params else \\\n        {\'coefs\': np.tile(np.ones(dim), num_chains).reshape(num_chains, dim)}\n    mcmc.run(random.PRNGKey(2), labels, init_params=init_params)\n    samples_flat = mcmc.get_samples()\n    assert samples_flat[\'coefs\'].shape[0] == num_chains * num_samples\n    samples = mcmc.get_samples(group_by_chain=True)\n    assert samples[\'coefs\'].shape[:2] == (num_chains, num_samples)\n    assert_allclose(np.mean(samples_flat[\'coefs\'], 0), true_coefs, atol=0.21)\n\n\n@pytest.mark.parametrize(\'kernel_cls\', [HMC, NUTS])\n@pytest.mark.parametrize(\'chain_method\', [\n    pytest.param(\'parallel\', marks=pytest.mark.xfail(\n        reason=\'jit+pmap does not work in CPU yet\')),\n    \'sequential\',\n    \'vectorized\',\n])\n@pytest.mark.skipif(\'CI\' in os.environ, reason=""Compiling time the whole sampling process is slow."")\ndef test_chain_inside_jit(kernel_cls, chain_method):\n    # NB: this feature is useful for consensus MC.\n    # Caution: compiling time will be slow (~ 90s)\n    if chain_method == \'parallel\' and xla_bridge.device_count() == 1:\n        pytest.skip(\'parallel method requires device_count greater than 1.\')\n    warmup_steps, num_samples = 100, 2000\n    # Here are settings which is currently supported.\n    rng_key = random.PRNGKey(2)\n    step_size = 1.\n    target_accept_prob = 0.8\n    trajectory_length = 1.\n    # Not supported yet:\n    #   + adapt_step_size\n    #   + adapt_mass_matrix\n    #   + max_tree_depth\n    #   + num_warmup\n    #   + num_samples\n\n    def model(data):\n        concentration = np.array([1.0, 1.0, 1.0])\n        p_latent = numpyro.sample(\'p_latent\', dist.Dirichlet(concentration))\n        numpyro.sample(\'obs\', dist.Categorical(p_latent), obs=data)\n        return p_latent\n\n    @jit\n    def get_samples(rng_key, data, step_size, trajectory_length, target_accept_prob):\n        kernel = kernel_cls(model, step_size=step_size, trajectory_length=trajectory_length,\n                            target_accept_prob=target_accept_prob)\n        mcmc = MCMC(kernel, warmup_steps, num_samples, num_chains=2, chain_method=chain_method,\n                    progress_bar=False)\n        mcmc.run(rng_key, data)\n        return mcmc.get_samples()\n\n    true_probs = np.array([0.1, 0.6, 0.3])\n    data = dist.Categorical(true_probs).sample(random.PRNGKey(1), (2000,))\n    samples = get_samples(rng_key, data, step_size, trajectory_length, target_accept_prob)\n    assert_allclose(np.mean(samples[\'p_latent\'], 0), true_probs, atol=0.02)\n\n\n@pytest.mark.parametrize(\'chain_method\', [\n    \'sequential\',\n    \'parallel\',\n    \'vectorized\',\n])\n@pytest.mark.parametrize(\'compile_args\', [\n    False,\n    True\n])\n@pytest.mark.skipif(\'CI\' in os.environ, reason=""Compiling time the whole sampling process is slow."")\ndef test_chain_smoke(chain_method, compile_args):\n    def model(data):\n        concentration = np.array([1.0, 1.0, 1.0])\n        p_latent = numpyro.sample(\'p_latent\', dist.Dirichlet(concentration))\n        numpyro.sample(\'obs\', dist.Categorical(p_latent), obs=data)\n        return p_latent\n\n    data = dist.Categorical(np.array([0.1, 0.6, 0.3])).sample(random.PRNGKey(1), (2000,))\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, 2, 5, num_chains=2, chain_method=chain_method, jit_model_args=compile_args)\n    mcmc.warmup(random.PRNGKey(0), data)\n    mcmc.run(random.PRNGKey(1), data)\n\n\ndef test_extra_fields():\n    def model():\n        numpyro.sample(\'x\', dist.Normal(0, 1), sample_shape=(5,))\n\n    mcmc = MCMC(NUTS(model), 1000, 1000)\n    mcmc.run(random.PRNGKey(0), extra_fields=(\'num_steps\', \'adapt_state.step_size\'))\n    samples = mcmc.get_samples(group_by_chain=True)\n    assert samples[\'x\'].shape == (1, 1000, 5)\n    stats = mcmc.get_extra_fields(group_by_chain=True)\n    assert \'num_steps\' in stats\n    assert stats[\'num_steps\'].shape == (1, 1000)\n    assert \'adapt_state.step_size\' in stats\n    assert stats[\'adapt_state.step_size\'].shape == (1, 1000)\n\n\n@pytest.mark.parametrize(\'algo\', [\'HMC\', \'NUTS\'])\ndef test_functional_beta_bernoulli_x64(algo):\n    warmup_steps, num_samples = 500, 20000\n\n    def model(data):\n        alpha = np.array([1.1, 1.1])\n        beta = np.array([1.1, 1.1])\n        p_latent = numpyro.sample(\'p_latent\', dist.Beta(alpha, beta))\n        numpyro.sample(\'obs\', dist.Bernoulli(p_latent), obs=data)\n        return p_latent\n\n    true_probs = np.array([0.9, 0.1])\n    data = dist.Bernoulli(true_probs).sample(random.PRNGKey(1), (1000, 2))\n    init_params, potential_fn, constrain_fn, _ = initialize_model(random.PRNGKey(2), model, model_args=(data,))\n    init_kernel, sample_kernel = hmc(potential_fn, algo=algo)\n    hmc_state = init_kernel(init_params,\n                            trajectory_length=1.,\n                            num_warmup=warmup_steps)\n    samples = fori_collect(0, num_samples, sample_kernel, hmc_state,\n                           transform=lambda x: constrain_fn(x.z))\n    assert_allclose(np.mean(samples[\'p_latent\'], 0), true_probs, atol=0.05)\n\n    if \'JAX_ENABLE_X64\' in os.environ:\n        assert samples[\'p_latent\'].dtype == np.float64\n\n\n@pytest.mark.parametrize(\'algo\', [\'HMC\', \'NUTS\'])\n@pytest.mark.parametrize(\'map_fn\', [vmap, pmap])\n@pytest.mark.skipif(\'XLA_FLAGS\' not in os.environ, reason=\'without this mark, we have duplicated tests in Travis\')\ndef test_functional_map(algo, map_fn):\n    if map_fn is pmap and xla_bridge.device_count() == 1:\n        pytest.skip(\'pmap test requires device_count greater than 1.\')\n\n    true_mean, true_std = 1., 2.\n    warmup_steps, num_samples = 1000, 8000\n\n    def potential_fn(z):\n        return 0.5 * np.sum(((z - true_mean) / true_std) ** 2)\n\n    init_kernel, sample_kernel = hmc(potential_fn, algo=algo)\n    init_params = np.array([0., -1.])\n    rng_keys = random.split(random.PRNGKey(0), 2)\n\n    init_kernel_map = map_fn(lambda init_param, rng_key: init_kernel(\n        init_param, trajectory_length=9, num_warmup=warmup_steps, rng_key=rng_key))\n    init_states = init_kernel_map(init_params, rng_keys)\n\n    fori_collect_map = map_fn(lambda hmc_state: fori_collect(0, num_samples, sample_kernel, hmc_state,\n                                                             transform=lambda x: x.z, progbar=False))\n    chain_samples = fori_collect_map(init_states)\n\n    assert_allclose(np.mean(chain_samples, axis=1), np.repeat(true_mean, 2), rtol=0.06)\n    assert_allclose(np.std(chain_samples, axis=1), np.repeat(true_std, 2), rtol=0.06)\n\n\n@pytest.mark.parametrize(\'jit_args\', [False, True])\n@pytest.mark.parametrize(\'shape\', [50, 100])\ndef test_reuse_mcmc_run(jit_args, shape):\n    y1 = onp.random.normal(3, 0.1, (100,))\n    y2 = onp.random.normal(-3, 0.1, (shape,))\n\n    def model(y_obs):\n        mu = numpyro.sample(\'mu\', dist.Normal(0., 1.))\n        sigma = numpyro.sample(""sigma"", dist.HalfCauchy(3.))\n        numpyro.sample(""y"", dist.Normal(mu, sigma), obs=y_obs)\n\n    # Run MCMC on zero observations.\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, 300, 500, jit_model_args=jit_args)\n    mcmc.run(random.PRNGKey(32), y1)\n\n    # Re-run on new data - should be much faster.\n    mcmc.run(random.PRNGKey(32), y2)\n    assert_allclose(mcmc.get_samples()[\'mu\'].mean(), -3., atol=0.1)\n\n\n@pytest.mark.parametrize(\'jit_args\', [False, True])\ndef test_model_with_multiple_exec_paths(jit_args):\n    def model(a=None, b=None, z=None):\n        int_term = numpyro.sample(\'a\', dist.Normal(0., 0.2))\n        x_term, y_term = 0., 0.\n        if a is not None:\n            x = numpyro.sample(\'x\', dist.HalfNormal(0.5))\n            x_term = a * x\n        if b is not None:\n            y = numpyro.sample(\'y\', dist.HalfNormal(0.5))\n            y_term = b * y\n        sigma = numpyro.sample(\'sigma\', dist.Exponential(1.))\n        mu = int_term + x_term + y_term\n        numpyro.sample(\'obs\', dist.Normal(mu, sigma), obs=z)\n\n    a = np.exp(onp.random.randn(10))\n    b = np.exp(onp.random.randn(10))\n    z = onp.random.randn(10)\n\n    # Run MCMC on zero observations.\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, 20, 10, jit_model_args=jit_args)\n    mcmc.run(random.PRNGKey(1), a, b=None, z=z)\n    assert set(mcmc.get_samples()) == {\'a\', \'x\', \'sigma\'}\n    mcmc.run(random.PRNGKey(2), a=None, b=b, z=z)\n    assert set(mcmc.get_samples()) == {\'a\', \'y\', \'sigma\'}\n    mcmc.run(random.PRNGKey(3), a=a, b=b, z=z)\n    assert set(mcmc.get_samples()) == {\'a\', \'x\', \'y\', \'sigma\'}\n\n\n@pytest.mark.parametrize(\'num_chains\', [1, 2])\n@pytest.mark.parametrize(\'chain_method\', [\'parallel\', \'sequential\', \'vectorized\'])\n@pytest.mark.parametrize(\'progress_bar\', [True, False])\ndef test_compile_warmup_run(num_chains, chain_method, progress_bar):\n    def model():\n        numpyro.sample(""x"", dist.Normal(0, 1))\n\n    if num_chains == 1 and chain_method in [\'sequential\', \'vectorized\']:\n        pytest.skip(\'duplicated test\')\n    if num_chains > 1 and chain_method == \'parallel\':\n        pytest.skip(\'duplicated test\')\n\n    rng_key = random.PRNGKey(0)\n    num_samples = 10\n    mcmc = MCMC(NUTS(model), 10, num_samples, num_chains,\n                chain_method=chain_method, progress_bar=progress_bar)\n\n    mcmc.run(rng_key)\n    expected_samples = mcmc.get_samples()[""x""]\n\n    mcmc._compile(rng_key)\n    # no delay after compiling\n    mcmc.warmup(rng_key)\n    mcmc.run(mcmc._warmup_state.rng_key)\n    actual_samples = mcmc.get_samples()[""x""]\n\n    assert_allclose(actual_samples, expected_samples)\n\n    # test for reproducible\n    if num_chains > 1:\n        mcmc = MCMC(NUTS(model), 10, num_samples, 1, progress_bar=progress_bar)\n        rng_key = random.split(rng_key)[0]\n        mcmc.run(rng_key)\n        first_chain_samples = mcmc.get_samples()[""x""]\n        assert_allclose(actual_samples[:num_samples], first_chain_samples, atol=1e-5)\n\n\n@pytest.mark.parametrize(\'dense_mass\', [True, False])\ndef test_get_proposal_loc_and_scale(dense_mass):\n    N = 10\n    dim = 3\n    samples = random.normal(random.PRNGKey(0), (N, dim))\n    loc = np.mean(samples[:-1], 0)\n    if dense_mass:\n        scale = np.linalg.cholesky(np.cov(samples[:-1], rowvar=False, bias=True))\n    else:\n        scale = np.std(samples[:-1], 0)\n    actual_loc, actual_scale = _get_proposal_loc_and_scale(samples[:-1], loc, scale, samples[-1])\n    expected_loc, expected_scale = [], []\n    for i in range(N - 1):\n        samples_i = onp.delete(samples, i, axis=0)\n        expected_loc.append(np.mean(samples_i, 0))\n        if dense_mass:\n            expected_scale.append(np.linalg.cholesky(np.cov(samples_i, rowvar=False, bias=True)))\n        else:\n            expected_scale.append(np.std(samples_i, 0))\n    expected_loc = np.stack(expected_loc)\n    expected_scale = np.stack(expected_scale)\n    assert_allclose(actual_loc, expected_loc, rtol=1e-4)\n    assert_allclose(actual_scale, expected_scale, atol=1e-6, rtol=0.05)\n\n\n@pytest.mark.parametrize(\'shape\', [(4,), (3, 2)])\n@pytest.mark.parametrize(\'idx\', [0, 1, 2])\ndef test_numpy_delete(shape, idx):\n    x = random.normal(random.PRNGKey(0), shape)\n    expected = onp.delete(x, idx, axis=0)\n    actual = _numpy_delete(x, idx)\n    assert_allclose(actual, expected)\n'"
test/test_optimizers.py,7,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\nfrom jax import grad, jit, partial\nimport jax.numpy as np\n\nfrom numpyro import optim\n\n\ndef loss(params):\n    return np.sum(params['x'] ** 2 + params['y'] ** 2)\n\n\n@partial(jit, static_argnums=(1,))\ndef step(opt_state, optim):\n    params = optim.get_params(opt_state)\n    g = grad(loss)(params)\n    return optim.update(g, opt_state)\n\n\n@pytest.mark.parametrize('optim_class, args', [\n    (optim.Adam, (1e-2,)),\n    (optim.ClippedAdam, (1e-2,)),\n    (optim.Adagrad, (1e-1,)),\n    (optim.Momentum, (1e-2, 0.5,)),\n    (optim.RMSProp, (1e-2, 0.95)),\n    (optim.RMSPropMomentum, (1e-4,)),\n    (optim.SGD, (1e-2,))\n])\ndef test_optim_multi_params(optim_class, args):\n    params = {'x': np.array([1., 1., 1.]), 'y': np.array([-1, -1., -1.])}\n    opt = optim_class(*args)\n    opt_state = opt.init(params)\n    for i in range(2000):\n        opt_state = step(opt_state, opt)\n    for _, param in opt.get_params(opt_state).items():\n        assert np.allclose(param, np.zeros(3))\n\n\n# note: this is somewhat of a bruteforce test. testing directly from\n# _NumpyroOptim would probably be better\n@pytest.mark.parametrize('optim_class, args', [\n    (optim.Adam, (1e-2,)),\n    (optim.ClippedAdam, (1e-2,)),\n    (optim.Adagrad, (1e-1,)),\n    (optim.Momentum, (1e-2, 0.5,)),\n    (optim.RMSProp, (1e-2, 0.95)),\n    (optim.RMSPropMomentum, (1e-4,)),\n    (optim.SGD, (1e-2,))\n])\ndef test_numpyrooptim_no_double_jit(optim_class, args):\n\n    opt = optim_class(*args)\n    state = opt.init(np.zeros(10))\n\n    my_fn_calls = 0\n\n    @jit\n    def my_fn(state, g):\n        nonlocal my_fn_calls\n        my_fn_calls += 1\n\n        state = opt.update(g, state)\n        return state\n\n    state = my_fn(state, np.ones(10)*1.)\n    state = my_fn(state, np.ones(10)*2.)\n    state = my_fn(state, np.ones(10)*3.)\n\n    assert my_fn_calls == 1\n"""
test/test_svi.py,6,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpy.testing import assert_allclose\nimport pytest\n\nfrom jax import jit, random, value_and_grad\nimport jax.numpy as np\nfrom jax.test_util import check_close\n\nimport numpyro\nfrom numpyro import optim\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.transforms import AffineTransform, SigmoidTransform\nfrom numpyro.handlers import substitute\nfrom numpyro.infer import ELBO, SVI, RenyiELBO\nfrom numpyro.util import fori_loop\n\n\n@pytest.mark.parametrize(\'alpha\', [0., 2.])\ndef test_renyi_elbo(alpha):\n    def model(x):\n        numpyro.sample(""obs"", dist.Normal(0, 1), obs=x)\n\n    def guide(x):\n        pass\n\n    def elbo_loss_fn(x):\n        return ELBO().loss(random.PRNGKey(0), {}, model, guide, x)\n\n    def renyi_loss_fn(x):\n        return RenyiELBO(alpha=alpha, num_particles=10).loss(random.PRNGKey(0), {}, model, guide, x)\n\n    elbo_loss, elbo_grad = value_and_grad(elbo_loss_fn)(2.)\n    renyi_loss, renyi_grad = value_and_grad(renyi_loss_fn)(2.)\n    assert_allclose(elbo_loss, renyi_loss, rtol=1e-6)\n    assert_allclose(elbo_grad, renyi_grad, rtol=1e-6)\n\n\n@pytest.mark.parametrize(\'elbo\', [\n    ELBO(),\n    RenyiELBO(num_particles=10),\n])\ndef test_beta_bernoulli(elbo):\n    data = np.array([1.0] * 8 + [0.0] * 2)\n\n    def model(data):\n        f = numpyro.sample(""beta"", dist.Beta(1., 1.))\n        numpyro.sample(""obs"", dist.Bernoulli(f), obs=data)\n\n    def guide(data):\n        alpha_q = numpyro.param(""alpha_q"", 1.0,\n                                constraint=constraints.positive)\n        beta_q = numpyro.param(""beta_q"", 1.0,\n                               constraint=constraints.positive)\n        numpyro.sample(""beta"", dist.Beta(alpha_q, beta_q))\n\n    adam = optim.Adam(0.05)\n    svi = SVI(model, guide, adam, elbo)\n    svi_state = svi.init(random.PRNGKey(1), data)\n    assert_allclose(adam.get_params(svi_state.optim_state)[\'alpha_q\'], 0.)\n\n    def body_fn(i, val):\n        svi_state, _ = svi.update(val, data)\n        return svi_state\n\n    svi_state = fori_loop(0, 1000, body_fn, svi_state)\n    params = svi.get_params(svi_state)\n    assert_allclose(params[\'alpha_q\'] / (params[\'alpha_q\'] + params[\'beta_q\']), 0.8, atol=0.05, rtol=0.05)\n\n\ndef test_jitted_update_fn():\n    data = np.array([1.0] * 8 + [0.0] * 2)\n\n    def model(data):\n        f = numpyro.sample(""beta"", dist.Beta(1., 1.))\n        numpyro.sample(""obs"", dist.Bernoulli(f), obs=data)\n\n    def guide(data):\n        alpha_q = numpyro.param(""alpha_q"", 1.0,\n                                constraint=constraints.positive)\n        beta_q = numpyro.param(""beta_q"", 1.0,\n                               constraint=constraints.positive)\n        numpyro.sample(""beta"", dist.Beta(alpha_q, beta_q))\n\n    adam = optim.Adam(0.05)\n    svi = SVI(model, guide, adam, ELBO())\n    svi_state = svi.init(random.PRNGKey(1), data)\n    expected = svi.get_params(svi.update(svi_state, data)[0])\n\n    actual = svi.get_params(jit(svi.update)(svi_state, data=data)[0])\n    check_close(actual, expected, atol=1e-5)\n\n\ndef test_param():\n    # this test the validity of model/guide sites having\n    # param constraints contain composed transformed\n    rng_keys = random.split(random.PRNGKey(0), 5)\n    a_minval = 1\n    c_minval = -2\n    c_maxval = -1\n    a_init = np.exp(random.normal(rng_keys[0])) + a_minval\n    b_init = np.exp(random.normal(rng_keys[1]))\n    c_init = random.uniform(rng_keys[2], minval=c_minval, maxval=c_maxval)\n    d_init = random.uniform(rng_keys[3])\n    obs = random.normal(rng_keys[4])\n\n    def model():\n        a = numpyro.param(\'a\', a_init, constraint=constraints.greater_than(a_minval))\n        b = numpyro.param(\'b\', b_init, constraint=constraints.positive)\n        numpyro.sample(\'x\', dist.Normal(a, b), obs=obs)\n\n    def guide():\n        c = numpyro.param(\'c\', c_init, constraint=constraints.interval(c_minval, c_maxval))\n        d = numpyro.param(\'d\', d_init, constraint=constraints.unit_interval)\n        numpyro.sample(\'y\', dist.Normal(c, d), obs=obs)\n\n    adam = optim.Adam(0.01)\n    svi = SVI(model, guide, adam, ELBO())\n    svi_state = svi.init(random.PRNGKey(0))\n\n    params = svi.get_params(svi_state)\n    assert_allclose(params[\'a\'], a_init)\n    assert_allclose(params[\'b\'], b_init)\n    assert_allclose(params[\'c\'], c_init)\n    assert_allclose(params[\'d\'], d_init)\n\n    actual_loss = svi.evaluate(svi_state)\n    assert np.isfinite(actual_loss)\n    expected_loss = dist.Normal(c_init, d_init).log_prob(obs) - dist.Normal(a_init, b_init).log_prob(obs)\n    # not so precisely because we do transform / inverse transform stuffs\n    assert_allclose(actual_loss, expected_loss, rtol=1e-6)\n\n\ndef test_elbo_dynamic_support():\n    x_prior = dist.TransformedDistribution(\n        dist.Normal(), [AffineTransform(0, 2), SigmoidTransform(), AffineTransform(0, 3)])\n    x_guide = dist.Uniform(0, 3)\n\n    def model():\n        numpyro.sample(\'x\', x_prior)\n\n    def guide():\n        numpyro.sample(\'x\', x_guide)\n\n    adam = optim.Adam(0.01)\n    x = 2.\n    guide = substitute(guide, param_map={\'x\': x})\n    svi = SVI(model, guide, adam, ELBO())\n    svi_state = svi.init(random.PRNGKey(0))\n    actual_loss = svi.evaluate(svi_state)\n    assert np.isfinite(actual_loss)\n    expected_loss = x_guide.log_prob(x) - x_prior.log_prob(x)\n    assert_allclose(actual_loss, expected_loss)\n'"
test/test_util.py,10,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpy.testing import assert_allclose\nimport pytest\n\nfrom jax import lax\nfrom jax.dtypes import canonicalize_dtype\nimport jax.numpy as np\nfrom jax.test_util import check_eq\nfrom jax.tree_util import tree_flatten, tree_multimap\n\nfrom numpyro.util import fori_collect, ravel_pytree\n\n\ndef test_fori_collect():\n    def f(x):\n        return {'i': x['i'] + x['j'], 'j': x['i'] - x['j']}\n\n    a = {'i': np.array([0.]), 'j': np.array([1.])}\n    expected_tree = {'i': np.array([[0.], [2.]])}\n    actual_tree = fori_collect(1, 3, f, a, transform=lambda a: {'i': a['i']})\n    check_eq(actual_tree, expected_tree)\n\n\n@pytest.mark.parametrize('progbar', [False, True])\ndef test_fori_collect_return_last(progbar):\n    def f(x):\n        x['i'] = x['i'] + 1\n        return x\n\n    tree, init_state = fori_collect(2, 4, f, {'i': 0},\n                                    transform=lambda a: {'i': a['i']},\n                                    return_last_val=True,\n                                    progbar=progbar)\n    expected_tree = {'i': np.array([3, 4])}\n    expected_last_state = {'i': np.array(4)}\n    check_eq(init_state, expected_last_state)\n    check_eq(tree, expected_tree)\n\n\n@pytest.mark.parametrize('pytree', [\n    {'a': np.array(0.), 'b': np.array([[1., 2.], [3., 4.]])},\n    {'a': np.array(0), 'b': np.array([[1, 2], [3, 4]])},\n    {'a': np.array(0), 'b': np.array([[1., 2.], [3., 4.]])},\n    {'a': 0., 'b': np.array([[1., 2.], [3., 4.]])},\n    {'a': False, 'b': np.array([[1., 2.], [3., 4.]])},\n    [False, True, 0., np.array([[1., 2.], [3., 4.]])],\n])\ndef test_ravel_pytree(pytree):\n    flat, unravel_fn = ravel_pytree(pytree)\n    unravel = unravel_fn(flat)\n    tree_flatten(tree_multimap(lambda x, y: assert_allclose(x, y), unravel, pytree))\n    assert all(tree_flatten(tree_multimap(lambda x, y:\n                                          canonicalize_dtype(lax.dtype(x)) == canonicalize_dtype(lax.dtype(y)),\n                                          unravel, pytree))[0])\n"""
docs/source/conf.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport sys\n\nimport sphinx_rtd_theme\n\n\n# import pkg_resources\n\n# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n\nos.environ[\'SPHINX_BUILD\'] = \'1\'\n\n# HACK: This is to ensure that local functions are documented by sphinx.\nfrom numpyro.infer.mcmc import hmc  # noqa: E402\nhmc(None, None)\n\n# -- Project information -----------------------------------------------------\n\nproject = u\'NumPyro\'\ncopyright = u\'2019, Uber Technologies, Inc\'\nauthor = u\'Uber AI Labs\'\n\nversion = \'\'\n\nif \'READTHEDOCS\' not in os.environ:\n    # if developing locally, use pyro.__version__ as version\n    from numpyro import __version__  # noqaE402\n    version = __version__\n\n# release version\nrelease = version\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Disable documentation inheritance so as to avoid inheriting\n# docstrings in a different format, e.g. when the parent class\n# is a PyTorch class.\n\nautodoc_inherit_docstrings = False\n\n# autodoc_default_options = {\n#     \'member-order\': \'bysource\',\n#     \'show-inheritance\': True,\n#     \'special-members\': True,\n#     \'undoc-members\': True,\n#     \'exclude-members\': \'__dict__,__module__,__weakref__\',\n# }\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n\n# do not prepend module name to functions\nadd_module_names = False\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'numpyrodoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NumPyro.tex\', u\'NumPyro Documentation\', u\'Uber AI Labs\', \'manual\'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'NumPyro\', u\'NumPyro Documentation\',\n     [author], 1)\n]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NumPyro\', u\'NumPyro Documentation\',\n     author, \'NumPyro\', \'Pyro PPL on Numpy\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'jax\': (\'https://jax.readthedocs.io/en/latest/\', None),\n    \'pyro\': (\'http://docs.pyro.ai/en/stable/\', None),\n}\n'"
notebooks/source/conf.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport glob\nimport shutil\n\nfrom sphinx_gallery.scrapers import figure_rst\nfrom sphinx_gallery.sorting import FileNameSortKey\nimport sphinx_rtd_theme\n\n# -*- coding: utf-8 -*-\n#\n# NumPyro Tutorials documentation build configuration file, created by\n# sphinx-quickstart on Tue Oct 31 11:33:17 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx.ext.intersphinx\',\n              \'sphinx.ext.todo\',\n              \'sphinx.ext.mathjax\',\n              \'sphinx.ext.githubpages\',\n              \'nbsphinx\',\n              \'sphinx.ext.autodoc\',\n              \'IPython.sphinxext.ipython_console_highlighting\',\n              \'sphinx_gallery.gen_gallery\',\n              ]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = [\'.rst\', \'.ipynb\']\n\n# do not execute cells\nnbsphinx_execute = \'never\'\n\n# allow errors because not all tutorials build\nnbsphinx_allow_errors = True\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'NumPyro Tutorials\'\ncopyright = u\'2019, Uber Technologies, Inc\'\nauthor = u\'Uber AI Labs\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n\nversion = \'\'\n\nif \'READTHEDOCS\' not in os.environ:\n    # if developing locally, use numpyro.__version__ as version\n    from numpyro import __version__  # noqaE402\n    version = __version__\n\n# release version\nrelease = version\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'.ipynb_checkpoints\', \'logistic_regression.ipynb\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# extend timeout\nnbsphinx_timeout = 120\n\n# -- Options for gallery --------------------------------------------------\n\n# examples with order\nEXAMPLES = [\n   \'baseball.py\',\n   \'bnn.py\',\n   \'funnel.py\',\n   \'gp.py\',\n   \'ucbadmit.py\',\n   \'hmm.py\',\n   \'neutra.py\',\n   \'ode.py\',\n   \'sparse_regression.py\',\n   \'stochastic_volatility.py\',\n   \'vae.py\',\n]\n\n\nclass GalleryFileNameSortKey(FileNameSortKey):\n    def __call__(self, filename):\n        if filename in EXAMPLES:\n            return ""{:02d}"".format(EXAMPLES.index(filename))\n        else:  # not in examples list, sort by name\n            return ""99"" + filename\n\n\n# Adapted from https://sphinx-gallery.github.io/stable/advanced.html#example-2-detecting-image-files-on-disk\n#\n# Custom images can be put in _static/img folder, with the pattern\n#   sphx_glr_[name_of_example]_1.png\n# Note that this also displays the image in the example page.\n# To not display the image, we can add the following lines\n# at the end of __call__ method:\n#   if ""sparse_regression"" in images_rst:\n#       images_rst = """"\n#   return images_rst\n#\n# If there are several images for an example, we can select\n# which one to be the thumbnail image by adding a comment\n# in the example script\n#   # sphinx_gallery_thumbnail_number = 2\nclass PNGScraper(object):\n    def __init__(self):\n        self.seen = set()\n\n    def __repr__(self):\n        return \'PNGScraper\'\n\n    def __call__(self, block, block_vars, gallery_conf):\n        # Find all PNG files in the directory of this example.\n        pngs = sorted(glob.glob(os.path.join(os.path.dirname(__file__), \'_static/img/sphx_glr_*.png\')))\n\n        # Iterate through PNGs, copy them to the sphinx-gallery output directory\n        image_names = list()\n        image_path_iterator = block_vars[\'image_path_iterator\']\n        for png in pngs:\n            if png not in self.seen:\n                self.seen |= set(png)\n                this_image_path = image_path_iterator.next()\n                image_names.append(this_image_path)\n                shutil.copy(png, this_image_path)\n        # Use the `figure_rst` helper function to generate rST for image files\n        images_rst = figure_rst(image_names, gallery_conf[\'src_dir\'])\n        return images_rst\n\n\nsphinx_gallery_conf = {\n    \'examples_dirs\': [\'../../examples\'],\n    \'gallery_dirs\': \'examples\',\n    \'filename_pattern\': \'.py\',\n    \'ignore_pattern\': \'(minipyro|covtype|__init__)\',\n    \'within_subsection_order\': GalleryFileNameSortKey,\n    \'image_scrapers\': (\'matplotlib\', PNGScraper()),\n    \'default_thumb_file\': \'source/_static/img/pyro_logo_wide.png\',\n}\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n# logo\nhtml_logo = \'_static/img/pyro_logo_wide.png\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    # \'logo_only\': True\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\nhtml_style = \'css/pyro.css\'\n\n# html_favicon = \'../img/favicon/favicon.ico\'\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NumPyroTutorialsDoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NumPyroTutorials.tex\', u\'Numpyro Examples and Tutorials\',\n     u\'Uber AI Labs\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'NumPyroTutorials\', u\'Numpyro Examples and Tutorials\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NumPyroTutorials\', u\'NumPyro Examples and Tutorials\',\n     author, \'NumPyroTutorials\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
numpyro/compat/__init__.py,0,b''
numpyro/compat/distributions.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpyro.distributions import *  # noqa: F401, F403\n'"
numpyro/compat/handlers.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpyro.handlers import *  # noqa: F401, F403\n'"
numpyro/compat/infer.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\n\nfrom jax import jit\n\nimport numpyro\nfrom numpyro.compat.pyro import get_param_store\nimport numpyro.distributions as dist\nfrom numpyro.infer import elbo, mcmc, svi\n\n\nclass HMC(mcmc.HMC):\n    def __init__(self,\n                 model=None,\n                 potential_fn=None,\n                 step_size=1,\n                 adapt_step_size=True,\n                 adapt_mass_matrix=True,\n                 full_mass=False,\n                 use_multinomial_sampling=True,\n                 transforms=None,\n                 max_plate_nesting=None,\n                 jit_compile=False,\n                 jit_options=None,\n                 ignore_jit_warnings=False,\n                 trajectory_length=2 * math.pi,\n                 target_accept_prob=0.8):\n        super(HMC, self).__init__(model=model,\n                                  potential_fn=potential_fn,\n                                  step_size=step_size,\n                                  adapt_step_size=adapt_step_size,\n                                  adapt_mass_matrix=adapt_mass_matrix,\n                                  dense_mass=full_mass,\n                                  target_accept_prob=target_accept_prob,\n                                  trajectory_length=trajectory_length)\n\n\nclass NUTS(mcmc.NUTS):\n    def __init__(self,\n                 model=None,\n                 potential_fn=None,\n                 step_size=1,\n                 adapt_step_size=True,\n                 adapt_mass_matrix=True,\n                 full_mass=False,\n                 use_multinomial_sampling=True,\n                 transforms=None,\n                 max_plate_nesting=None,\n                 jit_compile=False,\n                 jit_options=None,\n                 ignore_jit_warnings=False,\n                 trajectory_length=2 * math.pi,\n                 target_accept_prob=0.8,\n                 max_tree_depth=10):\n        if potential_fn is not None:\n            raise ValueError('Only `model` argument is supported in generic module;'\n                             ' `potential_fn` is not supported.')\n        super(NUTS, self).__init__(model=model,\n                                   potential_fn=potential_fn,\n                                   step_size=step_size,\n                                   adapt_step_size=adapt_step_size,\n                                   adapt_mass_matrix=adapt_mass_matrix,\n                                   dense_mass=full_mass,\n                                   target_accept_prob=target_accept_prob,\n                                   trajectory_length=trajectory_length,\n                                   max_tree_depth=max_tree_depth)\n\n\nclass MCMC(object):\n    def __init__(self,\n                 kernel,\n                 num_samples,\n                 warmup_steps=None,\n                 initial_params=None,\n                 num_chains=1,\n                 hook_fn=None,\n                 mp_context=None,\n                 disable_progbar=False,\n                 disable_validation=True,\n                 transforms=None):\n        if warmup_steps is None:\n            warmup_steps = num_samples\n        self._initial_params = initial_params\n        self._mcmc = mcmc.MCMC(kernel,\n                               warmup_steps,\n                               num_samples,\n                               num_chains=num_chains,\n                               progress_bar=(not disable_progbar))\n\n    def run(self, *args, rng_key=None, **kwargs):\n        if rng_key is None:\n            rng_key = numpyro.sample('mcmc.run', dist.PRNGIdentity())\n        self._mcmc.run(rng_key, *args, init_params=self._initial_params, **kwargs)\n\n    def get_samples(self, num_samples=None, group_by_chain=False):\n        if num_samples is not None:\n            raise ValueError('`num_samples` arg unsupported in NumPyro.')\n        return self._mcmc.get_samples(group_by_chain=group_by_chain)\n\n    def summary(self, prob=0.9):\n        self._mcmc.print_summary()\n\n\nclass SVI(svi.SVI):\n    def __init__(self,\n                 model,\n                 guide,\n                 optim,\n                 loss,\n                 loss_and_grads=None,\n                 num_samples=10,\n                 num_steps=0,\n                 **kwargs):\n        super(SVI, self).__init__(model=model,\n                                  guide=guide,\n                                  optim=optim,\n                                  loss=loss)\n        self.svi_state = None\n\n    def evaluate_loss(self, *args, **kwargs):\n        return self.evaluate(self.svi_state, *args, **kwargs)\n\n    def step(self, *args, rng_key=None, **kwargs):\n        if self.svi_state is None:\n            if rng_key is None:\n                rng_key = numpyro.sample('svi.init', dist.PRNGIdentity())\n            self.svi_state = self.init(rng_key, *args, **kwargs)\n        try:\n            self.svi_state, loss = jit(self.update)(self.svi_state, *args, **kwargs)\n        except TypeError as e:\n            if 'not a valid JAX type' in str(e):\n                raise TypeError('NumPyro backend requires args, kwargs to be arrays or tuples, '\n                                'dicts of arrays.')\n            else:\n                raise e\n        params = jit(super(SVI, self).get_params)(self.svi_state)\n        get_param_store().update(params)\n        return loss\n\n    def get_params(self):\n        return super(SVI, self).get_params(self.svi_state)\n\n\nclass Trace_ELBO(elbo.ELBO):\n    def __init__(self,\n                 num_particles=1,\n                 max_plate_nesting=float('inf'),\n                 max_iarange_nesting=None,  # DEPRECATED\n                 vectorize_particles=False,\n                 strict_enumeration_warning=True,\n                 ignore_jit_warnings=False,\n                 jit_options=None,\n                 retain_graph=None,\n                 tail_adaptive_beta=-1.0):\n        super(Trace_ELBO, self).__init__(num_particles=num_particles)\n\n\n# JIT is enabled by default\nJitTrace_ELBO = Trace_ELBO\n"""
numpyro/compat/ops.py,3,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport numpy as onp\n\nimport jax.numpy as np\nfrom jax.numpy import *  # noqa: F401, F403\n\ntensor = array  # noqa: F405\n\nrandn = onp.random.randn\n\n\n# Provide wrappers to initialize ones/zeros using the pytorch convention\n# of using *sizes. e.g. ops.ones(2, 3) as well as ops.ones((2, 3)) can\n# be used to initialize an array of ones with shape (2, 3).\n\ndef ones(*sizes, **kwargs):\n    if len(sizes) == 0:\n        raise ValueError('Positional `size` argument not provided.')\n    elif len(sizes) == 1:\n        if isinstance(sizes[0], (tuple, list)):\n            sizes = sizes[0]\n    if not all([isinstance(s, int) for s in sizes]):\n        raise ValueError('Invalid data type for `size` provided.')\n    return np.ones(sizes, **kwargs)\n\n\ndef zeros(*sizes, **kwargs):\n    if len(sizes) == 0:\n        raise ValueError('Positional `size` argument not provided.')\n    elif len(sizes) == 1:\n        if isinstance(sizes[0], (tuple, list)):\n            sizes = sizes[0]\n    if not all([isinstance(s, int) for s in sizes]):\n        raise ValueError('Invalid data type for `size` provided.')\n    return np.ones(sizes, **kwargs)\n"""
numpyro/compat/optim.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpyro import optim\n\n\ndef Adam(kwargs):\n    step_size = kwargs.pop('lr')\n    b1, b2 = kwargs.pop('betas', (0.9, 0.999))\n    eps = kwargs.pop('eps', 1.e-8)\n    return optim.Adam(step_size=step_size, b1=b1, b2=b2, eps=eps)\n\n\ndef ClippedAdam(kwargs):\n    step_size = kwargs.pop('lr')\n    b1, b2 = kwargs.pop('betas', (0.9, 0.999))\n    eps = kwargs.pop('eps', 1.e-8)\n    clip_norm = kwargs.pop('clip_norm', 10.)\n    lrd = kwargs.pop('lrd', None)\n    init_lr = step_size\n    if lrd is not None:\n        def step_size(i):\n            return init_lr * lrd ** i\n    return optim.ClippedAdam(step_size=step_size, b1=b1, b2=b2, eps=eps, clip_norm=clip_norm)\n"""
numpyro/compat/pyro.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport warnings\n\nfrom numpyro.compat.util import UnsupportedAPIWarning\nfrom numpyro.primitives import module, param as _param, plate, sample  # noqa: F401\n\n\n_PARAM_STORE = {}\n\n\ndef get_param_store():\n    warnings.warn('A limited parameter store is provided for compatibility with Pyro. '\n                  'Value of SVI parameters should be obtained via SVI.get_params() method.',\n                  category=UnsupportedAPIWarning)\n    # Return an empty dict for compatibility\n    return _PARAM_STORE\n\n\ndef clear_param_store():\n    return _PARAM_STORE.clear()\n\n\ndef param(name, *args, **kwargs):\n    # Get value assuming statement is wrapped in a substitute handler.\n    val = _param(name, *args, **kwargs)\n    # If no value is found, check param store.\n    if val is None:\n        # If other arguments are provided, e.g. for initialization, raise error.\n        if args or kwargs:\n            raise NotImplementedError\n        param_store = get_param_store()\n        if name in param_store:\n            val = param_store[name]\n    return val\n"""
numpyro/compat/util.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n\nclass UnsupportedAPIWarning(Warning):\n    """"""\n    Warn users on Pyro operations that do not have a meaningful interpretation\n    in NumPyro. Unlike raising NotImplementedError, it might be possible in\n    such cases to return a dummy object and recover.\n    """"""\n    pass\n'"
numpyro/contrib/__init__.py,0,b''
numpyro/contrib/indexing.py,11,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport jax.numpy as np\n\n\ndef _is_batched(arg):\n    return np.ndim(arg) > 0\n\n\ndef vindex(tensor, args):\n    """"""\n    Vectorized advanced indexing with broadcasting semantics.\n\n    See also the convenience wrapper :class:`Vindex`.\n\n    This is useful for writing indexing code that is compatible with batching\n    and enumeration, especially for selecting mixture components with discrete\n    random variables.\n\n    For example suppose ``x`` is a parameter with ``len(x.shape) == 3`` and we wish\n    to generalize the expression ``x[i, :, j]`` from integer ``i,j`` to tensors\n    ``i,j`` with batch dims and enum dims (but no event dims). Then we can\n    write the generalize version using :class:`Vindex` ::\n\n        xij = Vindex(x)[i, :, j]\n\n        batch_shape = broadcast_shape(i.shape, j.shape)\n        event_shape = (x.size(1),)\n        assert xij.shape == batch_shape + event_shape\n\n    To handle the case when ``x`` may also contain batch dimensions (e.g. if\n    ``x`` was sampled in a plated context as when using vectorized particles),\n    :func:`vindex` uses the special convention that ``Ellipsis`` denotes batch\n    dimensions (hence ``...`` can appear only on the left, never in the middle\n    or in the right). Suppose ``x`` has event dim 3. Then we can write::\n\n        old_batch_shape = x.shape[:-3]\n        old_event_shape = x.shape[-3:]\n\n        xij = Vindex(x)[..., i, :, j]   # The ... denotes unknown batch shape.\n\n        new_batch_shape = broadcast_shape(old_batch_shape, i.shape, j.shape)\n        new_event_shape = (x.size(1),)\n        assert xij.shape = new_batch_shape + new_event_shape\n\n    Note that this special handling of ``Ellipsis`` differs from the NEP [1].\n\n    Formally, this function assumes:\n\n    1.  Each arg is either ``Ellipsis``, ``slice(None)``, an integer, or a\n        batched integer tensor (i.e. with empty event shape). This\n        function does not support Nontrivial slices or boolean tensor\n        masks. ``Ellipsis`` can only appear on the left as ``args[0]``.\n    2.  If ``args[0] is not Ellipsis`` then ``tensor`` is not\n        batched, and its event dim is equal to ``len(args)``.\n    3.  If ``args[0] is Ellipsis`` then ``tensor`` is batched and\n        its event dim is equal to ``len(args[1:])``. Dims of ``tensor``\n        to the left of the event dims are considered batch dims and will be\n        broadcasted with dims of tensor args.\n\n    Note that if none of the args is a tensor with ``len(shape) > 0``, then this\n    function behaves like standard indexing::\n\n        if not any(isinstance(a, np.ndarray) and len(a.shape) > 0 for a in args):\n            assert Vindex(x)[args] == x[args]\n\n    **References**\n\n    [1] https://www.numpy.org/neps/nep-0021-advanced-indexing.html\n        introduces ``vindex`` as a helper for vectorized indexing.\n        This implementation is similar to the proposed notation\n        ``x.vindex[]`` except for slightly different handling of ``Ellipsis``.\n\n    :param np.ndarray tensor: A tensor to be indexed.\n    :param tuple args: An index, as args to ``__getitem__``.\n    :returns: A nonstandard interpetation of ``tensor[args]``.\n    :rtype: np.ndarray\n    """"""\n    if not isinstance(args, tuple):\n        return tensor[args]\n    if not args:\n        return tensor\n\n    assert np.ndim(tensor) > 0\n    # Compute event dim before and after indexing.\n    if args[0] is Ellipsis:\n        args = args[1:]\n        if not args:\n            return tensor\n        old_event_dim = len(args)\n        args = (slice(None),) * (np.ndim(tensor) - len(args)) + args\n    else:\n        args = args + (slice(None),) * (np.ndim(tensor) - len(args))\n        old_event_dim = len(args)\n    assert len(args) == np.ndim(tensor)\n    if any(a is Ellipsis for a in args):\n        raise NotImplementedError(""Non-leading Ellipsis is not supported"")\n\n    # In simple cases, standard advanced indexing broadcasts correctly.\n    is_standard = True\n    if np.ndim(tensor) > old_event_dim and _is_batched(args[0]):\n        is_standard = False\n    elif any(_is_batched(a) for a in args[1:]):\n        is_standard = False\n    if is_standard:\n        return tensor[args]\n\n    # Convert args to use broadcasting semantics.\n    new_event_dim = sum(isinstance(a, slice) for a in args[-old_event_dim:])\n    new_dim = 0\n    args = list(args)\n    for i, arg in reversed(list(enumerate(args))):\n        if isinstance(arg, slice):\n            # Convert slices to arange()s.\n            if arg != slice(None):\n                raise NotImplementedError(""Nontrivial slices are not supported"")\n            arg = np.arange(tensor.shape[i], dtype=np.int32)\n            arg = arg.reshape((-1,) + (1,) * new_dim)\n            new_dim += 1\n        elif _is_batched(arg):\n            # Reshape nontrivial tensors.\n            arg = arg.reshape(arg.shape + (1,) * new_event_dim)\n        args[i] = arg\n    args = tuple(args)\n\n    return tensor[args]\n\n\nclass Vindex:\n    """"""\n    Convenience wrapper around :func:`vindex`.\n\n    The following are equivalent::\n\n        Vindex(x)[..., i, j, :]\n        vindex(x, (Ellipsis, i, j, slice(None)))\n\n    :param np.ndarray tensor: A tensor to be indexed.\n    :return: An object with a special :meth:`__getitem__` method.\n    """"""\n    def __init__(self, tensor):\n        self._tensor = tensor\n\n    def __getitem__(self, args):\n        return vindex(self._tensor, args)\n'"
numpyro/contrib/reparam.py,1,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABC, abstractmethod\n\nimport jax.numpy as jnp\n\nimport numpyro\nfrom numpyro.contrib.autoguide import AutoContinuous\nimport numpyro.distributions as dist\nfrom numpyro.distributions import biject_to\nfrom numpyro.distributions.util import sum_rightmost\nfrom numpyro.handlers import Messenger\n\n\nclass reparam(Messenger):\n    """"""\n    Reparametrizes each affected sample site into one or more auxiliary sample\n    sites followed by a deterministic transformation [1].\n\n    To specify reparameterizers, pass a ``config`` dict or callable to the\n    constructor.  See the :mod:`numpyro.contrib.reparam` module for available\n    reparameterizers.\n\n    Note some reparameterizers can examine the ``*args,**kwargs`` inputs of\n    functions they affect; these reparameterizers require using\n    ``poutine.reparam`` as a decorator rather than as a context manager.\n\n    [1] Maria I. Gorinova, Dave Moore, Matthew D. Hoffman (2019)\n        ""Automatic Reparameterisation of Probabilistic Programs""\n        https://arxiv.org/pdf/1906.03028.pdf\n\n    :param config: Configuration, either a dict mapping site name to\n        :class:`~numpyro.contrib.reparam.Reparam` ,\n        or a function mapping site to\n        :class:`~numpyro.contrib.reparam.Reparam` or None.\n    :type config: dict or callable\n    """"""\n    def __init__(self, fn=None, config=None):\n        assert isinstance(config, dict) or callable(config)\n        self.config = config\n        super().__init__(fn)\n\n    def process_message(self, msg):\n        if msg[""type""] != ""sample"":\n            return\n\n        if isinstance(self.config, dict):\n            reparam = self.config.get(msg[""name""])\n        else:\n            reparam = self.config(msg)\n        if reparam is None:\n            return\n\n        new_fn, value = reparam(msg[""name""], msg[""fn""], msg[""value""])\n\n        if value is not None:\n            if new_fn is None:\n                msg[\'type\'] = \'deterministic\'\n                msg[\'value\'] = value\n                for key in list(msg.keys()):\n                    if key not in (\'type\', \'name\', \'value\'):\n                        del msg[key]\n                return\n\n            if msg[""value""] is None:\n                msg[""is_observed""] = True\n            msg[""value""] = value\n        msg[""fn""] = new_fn\n\n\nclass Reparam(ABC):\n    """"""\n    Base class for reparameterizers.\n    """"""\n    @abstractmethod\n    def __call__(self, name, fn, obs):\n        """"""\n        :param str name: A sample site name.\n        :param ~numpyro.distributions.Distribution fn: A distribution.\n        :param numpy.ndarray obs: Observed value or None.\n        :return: A pair (``new_fn``, ``value``).\n        """"""\n        return fn, obs\n\n\nclass TransformReparam(Reparam):\n    """"""\n    Reparameterizer for\n    :class:`~numpyro.distributions.TransformedDistribution` latent variables.\n\n    This is useful for transformed distributions with complex,\n    geometry-changing transforms, where the posterior has simple shape in\n    the space of ``base_dist``.\n\n    This reparameterization works only for latent variables, not likelihoods.\n    """"""\n    def __call__(self, name, fn, obs):\n        assert obs is None, ""TransformReparam does not support observe statements""\n        batch_shape = fn.batch_shape\n        if isinstance(fn, dist.ExpandedDistribution):\n            fn = fn.base_dist\n        assert isinstance(fn, dist.TransformedDistribution)\n\n        # Draw noise from the base distribution.\n        # We need to make sure that we have the same batch_shape\n        reinterpreted_batch_ndims = fn.event_dim - fn.base_dist.event_dim\n        x = numpyro.sample(""{}_base"".format(name),\n                           fn.base_dist.to_event(reinterpreted_batch_ndims).expand(batch_shape))\n\n        # Differentiably transform.\n        for t in fn.transforms:\n            x = t(x)\n\n        # Simulate a pyro.deterministic() site.\n        return None, x\n\n\nclass NeuTraReparam(Reparam):\n    """"""\n    Neural Transport reparameterizer [1] of multiple latent variables.\n\n    This uses a trained :class:`~pyro.contrib.autoguide.AutoContinuous`\n    guide to alter the geometry of a model, typically for use e.g. in MCMC.\n    Example usage::\n\n        # Step 1. Train a guide\n        guide = AutoIAFNormal(model)\n        svi = SVI(model, guide, ...)\n        # ...train the guide...\n\n        # Step 2. Use trained guide in NeuTra MCMC\n        neutra = NeuTraReparam(guide)\n        model = netra.reparam(model)\n        nuts = NUTS(model)\n        # ...now use the model in HMC or NUTS...\n\n    This reparameterization works only for latent variables, not likelihoods.\n    Note that all sites must share a single common :class:`NeuTraReparam`\n    instance, and that the model must have static structure.\n\n    [1] Hoffman, M. et al. (2019)\n        ""NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport""\n        https://arxiv.org/abs/1903.03704\n\n    :param ~numpyro.contrib.autoguide.AutoContinuous guide: A guide.\n    :param params: trained parameters of the guide.\n    """"""\n    def __init__(self, guide, params):\n        if not isinstance(guide, AutoContinuous):\n            raise TypeError(""NeuTraReparam expected an AutoContinuous guide, but got {}""\n                            .format(type(guide)))\n        self.guide = guide\n        self.params = params\n        try:\n            self.transform = self.guide.get_transform(params)\n        except (NotImplementedError, TypeError):\n            raise ValueError(""NeuTraReparam only supports guides that implement ""\n                             ""`get_transform` method that does not depend on the ""\n                             ""model\'s `*args, **kwargs`"")\n        self._x_unconstrained = {}\n\n    def _reparam_config(self, site):\n        if site[""name""] in self.guide.prototype_trace and not site.get(""is_observed"", False):\n            return self\n\n    def reparam(self, fn=None):\n        return reparam(fn, config=self._reparam_config)\n\n    def __call__(self, name, fn, obs):\n        if name not in self.guide.prototype_trace:\n            return fn, obs\n        assert obs is None, ""NeuTraReparam does not support observe statements""\n\n        log_density = 0.\n        if not self._x_unconstrained:  # On first sample site.\n            # Sample a shared latent.\n            z_unconstrained = numpyro.sample(""{}_shared_latent"".format(self.guide.prefix),\n                                             self.guide.get_base_dist().mask(False))\n\n            # Differentiably transform.\n            x_unconstrained = self.transform(z_unconstrained)\n            # TODO: find a way to only compute those log_prob terms when needed\n            log_density = self.transform.log_abs_det_jacobian(z_unconstrained, x_unconstrained)\n            self._x_unconstrained = self.guide._unpack_latent(x_unconstrained)\n\n        # Extract a single site\'s value from the shared latent.\n        unconstrained_value = self._x_unconstrained.pop(name)\n        transform = biject_to(fn.support)\n        value = transform(unconstrained_value)\n        logdet = transform.log_abs_det_jacobian(unconstrained_value, value)\n        logdet = sum_rightmost(logdet, jnp.ndim(logdet) - jnp.ndim(value) + len(fn.event_shape))\n        log_density = log_density + fn.log_prob(value) + logdet\n        numpyro.factor(""_{}_log_prob"".format(name), log_density)\n        return None, value\n\n    def transform_sample(self, latent):\n        """"""\n        Given latent samples from the warped posterior (with possible batch dimensions),\n        return a `dict` of samples from the latent sites in the model.\n\n        :param latent: sample from the warped posterior (possibly batched).\n        :return: a `dict` of samples keyed by latent sites in the model.\n        :rtype: dict\n        """"""\n        x_unconstrained = self.transform(latent)\n        return self.guide._unpack_and_constrain(x_unconstrained, self.params)\n'"
numpyro/distributions/__init__.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpyro.distributions.conjugate import BetaBinomial, GammaPoisson\nimport numpyro.distributions.constraints  # noqa: F401\nfrom numpyro.distributions.continuous import (\n    LKJ,\n    Beta,\n    Cauchy,\n    Chi2,\n    Dirichlet,\n    Exponential,\n    Gamma,\n    GaussianRandomWalk,\n    Gumbel,\n    HalfCauchy,\n    HalfNormal,\n    InverseGamma,\n    LKJCholesky,\n    Logistic,\n    LogNormal,\n    LowRankMultivariateNormal,\n    MultivariateNormal,\n    Normal,\n    Pareto,\n    StudentT,\n    TruncatedCauchy,\n    TruncatedNormal,\n    TruncatedPolyaGamma,\n    Uniform\n)\nfrom numpyro.distributions.discrete import (\n    Bernoulli,\n    BernoulliLogits,\n    BernoulliProbs,\n    Binomial,\n    BinomialLogits,\n    BinomialProbs,\n    Categorical,\n    CategoricalLogits,\n    CategoricalProbs,\n    Delta,\n    Multinomial,\n    MultinomialLogits,\n    MultinomialProbs,\n    OrderedLogistic,\n    Poisson,\n    PRNGIdentity,\n    ZeroInflatedPoisson\n)\nfrom numpyro.distributions.distribution import (\n    Distribution,\n    ExpandedDistribution,\n    ImproperUniform,\n    Independent,\n    MaskedDistribution,\n    TransformedDistribution,\n    Unit\n)\nimport numpyro.distributions.transforms  # noqa: F401\nfrom numpyro.distributions.transforms import biject_to\n\n__all__ = [\n    'biject_to',\n    'constraints',\n    'transforms',\n    'Bernoulli',\n    'BernoulliLogits',\n    'BernoulliProbs',\n    'Beta',\n    'BetaBinomial',\n    'Binomial',\n    'BinomialLogits',\n    'BinomialProbs',\n    'Categorical',\n    'CategoricalLogits',\n    'CategoricalProbs',\n    'Cauchy',\n    'Chi2',\n    'Delta',\n    'Dirichlet',\n    'Distribution',\n    'Exponential',\n    'ExpandedDistribution',\n    'Gamma',\n    'GammaPoisson',\n    'GaussianRandomWalk',\n    'Gumbel',\n    'HalfCauchy',\n    'HalfNormal',\n    'ImproperUniform',\n    'Independent',\n    'InverseGamma',\n    'LKJ',\n    'LKJCholesky',\n    'Logistic',\n    'LogNormal',\n    'MaskedDistribution',\n    'Multinomial',\n    'MultinomialLogits',\n    'MultinomialProbs',\n    'MultivariateNormal',\n    'LowRankMultivariateNormal',\n    'Normal',\n    'OrderedLogistic',\n    'Pareto',\n    'Poisson',\n    'PRNGIdentity',\n    'StudentT',\n    'TransformedDistribution',\n    'TruncatedCauchy',\n    'TruncatedNormal',\n    'TruncatedPolyaGamma',\n    'Uniform',\n    'Unit',\n    'ZeroInflatedPoisson',\n\n]\n"""
numpyro/distributions/conjugate.py,11,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom jax import lax, random\nimport jax.numpy as np\nfrom jax.scipy.special import betaln, gammaln\n\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.continuous import Beta, Gamma\nfrom numpyro.distributions.discrete import Binomial, Poisson\nfrom numpyro.distributions.distribution import Distribution\nfrom numpyro.distributions.util import promote_shapes, validate_sample\nfrom numpyro.util import not_jax_tracer\n\n\nclass BetaBinomial(Distribution):\n    r""""""\n    Compound distribution comprising of a beta-binomial pair. The probability of\n    success (``probs`` for the :class:`~numpyro.distributions.Binomial` distribution)\n    is unknown and randomly drawn from a :class:`~numpyro.distributions.Beta` distribution\n    prior to a certain number of Bernoulli trials given by ``total_count``.\n\n    :param numpy.ndarray concentration1: 1st concentration parameter (alpha) for the\n        Beta distribution.\n    :param numpy.ndarray concentration0: 2nd concentration parameter (beta) for the\n        Beta distribution.\n    :param numpy.ndarray total_count: number of Bernoulli trials.\n    """"""\n    arg_constraints = {\'concentration1\': constraints.positive, \'concentration0\': constraints.positive,\n                       \'total_count\': constraints.nonnegative_integer}\n    has_enumerate_support = True\n    is_discrete = True\n\n    def __init__(self, concentration1, concentration0, total_count=1, validate_args=None):\n        batch_shape = lax.broadcast_shapes(np.shape(concentration1), np.shape(concentration0),\n                                           np.shape(total_count))\n        self.concentration1 = np.broadcast_to(concentration1, batch_shape)\n        self.concentration0 = np.broadcast_to(concentration0, batch_shape)\n        self.total_count, = promote_shapes(total_count, shape=batch_shape)\n        self._beta = Beta(self.concentration1, self.concentration0)\n        super(BetaBinomial, self).__init__(batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        key_beta, key_binom = random.split(key)\n        probs = self._beta.sample(key_beta, sample_shape)\n        return Binomial(self.total_count, probs).sample(key_binom)\n\n    @validate_sample\n    def log_prob(self, value):\n        log_factorial_n = gammaln(self.total_count + 1)\n        log_factorial_k = gammaln(value + 1)\n        log_factorial_nmk = gammaln(self.total_count - value + 1)\n        return (log_factorial_n - log_factorial_k - log_factorial_nmk +\n                betaln(value + self.concentration1, self.total_count - value + self.concentration0) -\n                betaln(self.concentration0, self.concentration1))\n\n    @property\n    def mean(self):\n        return self._beta.mean * self.total_count\n\n    @property\n    def variance(self):\n        return self._beta.variance * self.total_count * (self.concentration0 + self.concentration1 + self.total_count)\n\n    @property\n    def support(self):\n        return constraints.integer_interval(0, self.total_count)\n\n    def enumerate_support(self, expand=True):\n        total_count = np.amax(self.total_count)\n        if not_jax_tracer(total_count):\n            # NB: the error can\'t be raised if inhomogeneous issue happens when tracing\n            if np.amin(self.total_count) != total_count:\n                raise NotImplementedError(""Inhomogeneous total count not supported""\n                                          "" by `enumerate_support`."")\n        values = np.arange(total_count + 1).reshape((-1,) + (1,) * len(self.batch_shape))\n        if expand:\n            values = np.broadcast_to(values, values.shape[:1] + self.batch_shape)\n        return values\n\n\nclass GammaPoisson(Distribution):\n    r""""""\n    Compound distribution comprising of a gamma-poisson pair, also referred to as\n    a gamma-poisson mixture. The ``rate`` parameter for the\n    :class:`~numpyro.distributions.Poisson` distribution is unknown and randomly\n    drawn from a :class:`~numpyro.distributions.Gamma` distribution.\n\n    :param numpy.ndarray concentration: shape parameter (alpha) of the Gamma distribution.\n    :param numpy.ndarray rate: rate parameter (beta) for the Gamma distribution.\n    """"""\n    arg_constraints = {\'concentration\': constraints.positive, \'rate\': constraints.positive}\n    support = constraints.nonnegative_integer\n    is_discrete = True\n\n    def __init__(self, concentration, rate=1., validate_args=None):\n        self._gamma = Gamma(concentration, rate)\n        self.concentration = self._gamma.concentration\n        self.rate = self._gamma.rate\n        super(GammaPoisson, self).__init__(self._gamma.batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        key_gamma, key_poisson = random.split(key)\n        rate = self._gamma.sample(key_gamma, sample_shape)\n        return Poisson(rate).sample(key_poisson)\n\n    @validate_sample\n    def log_prob(self, value):\n        post_value = self.concentration + value\n        return -betaln(self.concentration, value + 1) - np.log(post_value) + \\\n            self.concentration * np.log(self.rate) - post_value * np.log1p(self.rate)\n\n    @property\n    def mean(self):\n        return self.concentration / self.rate\n\n    @property\n    def variance(self):\n        return self.concentration / np.square(self.rate) * (1 + self.rate)\n'"
numpyro/distributions/constraints.py,20,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# The implementation follows the design in PyTorch: torch.distributions.constraints.py\n#\n# Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)\n# Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\n# Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n# Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n# Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n# Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n# Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n# Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n# Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n\n__all__ = [\n    \'boolean\',\n    \'corr_cholesky\',\n    \'corr_matrix\',\n    \'dependent\',\n    \'greater_than\',\n    \'integer_interval\',\n    \'integer_greater_than\',\n    \'interval\',\n    \'is_dependent\',\n    \'lower_cholesky\',\n    \'multinomial\',\n    \'nonnegative_integer\',\n    \'positive\',\n    \'positive_definite\',\n    \'positive_integer\',\n    \'real\',\n    \'real_vector\',\n    \'simplex\',\n    \'unit_interval\',\n    \'Constraint\',\n]\n\n\nimport jax.numpy as np\n\n\nclass Constraint(object):\n    """"""\n    Abstract base class for constraints.\n\n    A constraint object represents a region over which a variable is valid,\n    e.g. within which a variable can be optimized.\n    """"""\n    def __call__(self, x):\n        raise NotImplementedError\n\n    def check(self, value):\n        """"""\n        Returns a byte tensor of `sample_shape + batch_shape` indicating\n        whether each event in value satisfies this constraint.\n        """"""\n        return self(value)\n\n\nclass _Boolean(Constraint):\n    def __call__(self, x):\n        return (x == 0) | (x == 1)\n\n\nclass _CorrCholesky(Constraint):\n    def __call__(self, x):\n        tril = np.tril(x)\n        lower_triangular = np.all(np.reshape(tril == x, x.shape[:-2] + (-1,)), axis=-1)\n        positive_diagonal = np.all(np.diagonal(x, axis1=-2, axis2=-1) > 0, axis=-1)\n        x_norm = np.linalg.norm(x, axis=-1)\n        unit_norm_row = np.all((x_norm <= 1) & (x_norm > 1 - 1e-6), axis=-1)\n        return lower_triangular & positive_diagonal & unit_norm_row\n\n\nclass _CorrMatrix(Constraint):\n    def __call__(self, x):\n        # check for symmetric\n        symmetric = np.all(np.all(x == np.swapaxes(x, -2, -1), axis=-1), axis=-1)\n        # check for the smallest eigenvalue is positive\n        positive = np.linalg.eigh(x)[0][..., 0] > 0\n        # check for diagonal equal to 1\n        unit_variance = np.all(np.abs(np.diagonal(x, axis1=-2, axis2=-1) - 1) < 1e-6, axis=-1)\n        return symmetric & positive & unit_variance\n\n\nclass _Dependent(Constraint):\n    def __call__(self, x):\n        raise ValueError(\'Cannot determine validity of dependent constraint\')\n\n\ndef is_dependent(constraint):\n    return isinstance(constraint, _Dependent)\n\n\nclass _GreaterThan(Constraint):\n    def __init__(self, lower_bound):\n        self.lower_bound = lower_bound\n\n    def __call__(self, x):\n        return x > self.lower_bound\n\n\nclass _IntegerInterval(Constraint):\n    def __init__(self, lower_bound, upper_bound):\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n\n    def __call__(self, x):\n        return (x >= self.lower_bound) & (x <= self.upper_bound) & (x == np.floor(x))\n\n\nclass _IntegerGreaterThan(Constraint):\n    def __init__(self, lower_bound):\n        self.lower_bound = lower_bound\n\n    def __call__(self, x):\n        return (x % 1 == 0) & (x >= self.lower_bound)\n\n\nclass _Interval(Constraint):\n    def __init__(self, lower_bound, upper_bound):\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n\n    def __call__(self, x):\n        return (x > self.lower_bound) & (x < self.upper_bound)\n\n\nclass _LowerCholesky(Constraint):\n    def __call__(self, x):\n        tril = np.tril(x)\n        lower_triangular = np.all(np.reshape(tril == x, x.shape[:-2] + (-1,)), axis=-1)\n        positive_diagonal = np.all(np.diagonal(x, axis1=-2, axis2=-1) > 0, axis=-1)\n        return lower_triangular & positive_diagonal\n\n\nclass _Multinomial(Constraint):\n    def __init__(self, upper_bound):\n        self.upper_bound = upper_bound\n\n    def __call__(self, x):\n        return np.all(x >= 0, axis=-1) & (np.sum(x, -1) == self.upper_bound)\n\n\nclass _OrderedVector(Constraint):\n    def __call__(self, x):\n        return np.all(x[..., 1:] > x[..., :-1], axis=-1)\n\n\nclass _PositiveDefinite(Constraint):\n    def __call__(self, x):\n        # check for symmetric\n        symmetric = np.all(np.all(x == np.swapaxes(x, -2, -1), axis=-1), axis=-1)\n        # check for the smallest eigenvalue is positive\n        positive = np.linalg.eigh(x)[0][..., 0] > 0\n        return symmetric & positive\n\n\nclass _Real(Constraint):\n    def __call__(self, x):\n        return np.isfinite(x)\n\n\nclass _RealVector(Constraint):\n    def __call__(self, x):\n        return np.all(np.isfinite(x), axis=-1)\n\n\nclass _Simplex(Constraint):\n    def __call__(self, x):\n        x_sum = np.sum(x, axis=-1)\n        return np.all(x > 0, axis=-1) & (x_sum <= 1) & (x_sum > 1 - 1e-6)\n\n\n# TODO: Make types consistent\n\nboolean = _Boolean()\ncorr_cholesky = _CorrCholesky()\ncorr_matrix = _CorrMatrix()\ndependent = _Dependent()\ngreater_than = _GreaterThan\ninteger_interval = _IntegerInterval\ninteger_greater_than = _IntegerGreaterThan\ninterval = _Interval\nlower_cholesky = _LowerCholesky()\nmultinomial = _Multinomial\nnonnegative_integer = _IntegerGreaterThan(0)\nordered_vector = _OrderedVector()\npositive = _GreaterThan(0.)\npositive_definite = _PositiveDefinite()\npositive_integer = _IntegerGreaterThan(1)\nreal = _Real()\nreal_vector = _RealVector()\nsimplex = _Simplex()\nunit_interval = _Interval(0., 1.)\n'"
numpyro/distributions/continuous.py,170,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# The implementation largely follows the design in PyTorch\'s `torch.distributions`\n#\n# Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)\n# Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\n# Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n# Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n# Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n# Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n# Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n# Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n# Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n\nfrom jax import lax, ops\nimport jax.numpy as np\nimport jax.random as random\nimport jax.nn as nn\nfrom jax.scipy.linalg import cho_solve, solve_triangular\nfrom jax.scipy.special import gammaln, log_ndtr, multigammaln, ndtr, ndtri, logsumexp\n\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.distribution import Distribution, TransformedDistribution\nfrom numpyro.distributions.transforms import AffineTransform, ExpTransform, InvCholeskyTransform, PowerTransform\nfrom numpyro.distributions.util import (\n    cholesky_of_inverse,\n    lazy_property,\n    matrix_to_tril_vec,\n    promote_shapes,\n    signed_stick_breaking_tril,\n    validate_sample,\n    vec_to_tril_matrix\n)\nfrom numpyro.util import copy_docs_from\n\n\nEULER_MASCHERONI = 0.5772156649015328606065120900824024310421\n\n\n@copy_docs_from(Distribution)\nclass Beta(Distribution):\n    arg_constraints = {\'concentration1\': constraints.positive, \'concentration0\': constraints.positive}\n    support = constraints.unit_interval\n\n    def __init__(self, concentration1, concentration0, validate_args=None):\n        batch_shape = lax.broadcast_shapes(np.shape(concentration1), np.shape(concentration0))\n        self.concentration1 = np.broadcast_to(concentration1, batch_shape)\n        self.concentration0 = np.broadcast_to(concentration0, batch_shape)\n        self._dirichlet = Dirichlet(np.stack([self.concentration1, self.concentration0],\n                                             axis=-1))\n        super(Beta, self).__init__(batch_shape=batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return self._dirichlet.sample(key, sample_shape)[..., 0]\n\n    @validate_sample\n    def log_prob(self, value):\n        return self._dirichlet.log_prob(np.stack([value, 1. - value], -1))\n\n    @property\n    def mean(self):\n        return self.concentration1 / (self.concentration1 + self.concentration0)\n\n    @property\n    def variance(self):\n        total = self.concentration1 + self.concentration0\n        return self.concentration1 * self.concentration0 / (total ** 2 * (total + 1))\n\n\n@copy_docs_from(Distribution)\nclass Cauchy(Distribution):\n    arg_constraints = {\'loc\': constraints.real, \'scale\': constraints.positive}\n    support = constraints.real\n    reparametrized_params = [\'loc\', \'scale\']\n\n    def __init__(self, loc=0., scale=1., validate_args=None):\n        self.loc, self.scale = promote_shapes(loc, scale)\n        batch_shape = lax.broadcast_shapes(np.shape(loc), np.shape(scale))\n        super(Cauchy, self).__init__(batch_shape=batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        eps = random.cauchy(key, shape=sample_shape + self.batch_shape)\n        return self.loc + eps * self.scale\n\n    @validate_sample\n    def log_prob(self, value):\n        return - np.log(np.pi) - np.log(self.scale) - np.log1p(((value - self.loc) / self.scale) ** 2)\n\n    @property\n    def mean(self):\n        return np.full(self.batch_shape, np.nan)\n\n    @property\n    def variance(self):\n        return np.full(self.batch_shape, np.nan)\n\n\n@copy_docs_from(Distribution)\nclass Dirichlet(Distribution):\n    arg_constraints = {\'concentration\': constraints.positive}\n    support = constraints.simplex\n\n    def __init__(self, concentration, validate_args=None):\n        if np.ndim(concentration) < 1:\n            raise ValueError(""`concentration` parameter must be at least one-dimensional."")\n        self.concentration = concentration\n        batch_shape, event_shape = concentration.shape[:-1], concentration.shape[-1:]\n        super(Dirichlet, self).__init__(batch_shape=batch_shape,\n                                        event_shape=event_shape,\n                                        validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        shape = sample_shape + self.batch_shape + self.event_shape\n        gamma_samples = random.gamma(key, self.concentration, shape=shape)\n        samples = gamma_samples / np.sum(gamma_samples, axis=-1, keepdims=True)\n        return np.clip(samples, a_min=np.finfo(samples).tiny, a_max=1 - np.finfo(samples).eps)\n\n    @validate_sample\n    def log_prob(self, value):\n        normalize_term = (np.sum(gammaln(self.concentration), axis=-1) -\n                          gammaln(np.sum(self.concentration, axis=-1)))\n        return np.sum(np.log(value) * (self.concentration - 1.), axis=-1) - normalize_term\n\n    @property\n    def mean(self):\n        return self.concentration / np.sum(self.concentration, axis=-1, keepdims=True)\n\n    @property\n    def variance(self):\n        con0 = np.sum(self.concentration, axis=-1, keepdims=True)\n        return self.concentration * (con0 - self.concentration) / (con0 ** 2 * (con0 + 1))\n\n\n@copy_docs_from(Distribution)\nclass Exponential(Distribution):\n    reparametrized_params = [\'rate\']\n    arg_constraints = {\'rate\': constraints.positive}\n    support = constraints.positive\n\n    def __init__(self, rate=1., validate_args=None):\n        self.rate = rate\n        super(Exponential, self).__init__(batch_shape=np.shape(rate), validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return random.exponential(key, shape=sample_shape + self.batch_shape) / self.rate\n\n    @validate_sample\n    def log_prob(self, value):\n        return np.log(self.rate) - self.rate * value\n\n    @property\n    def mean(self):\n        return np.reciprocal(self.rate)\n\n    @property\n    def variance(self):\n        return np.reciprocal(self.rate ** 2)\n\n\n@copy_docs_from(Distribution)\nclass Gamma(Distribution):\n    arg_constraints = {\'concentration\': constraints.positive,\n                       \'rate\': constraints.positive}\n    support = constraints.positive\n    reparametrized_params = [\'rate\']\n\n    def __init__(self, concentration, rate=1., validate_args=None):\n        self.concentration, self.rate = promote_shapes(concentration, rate)\n        batch_shape = lax.broadcast_shapes(np.shape(concentration), np.shape(rate))\n        super(Gamma, self).__init__(batch_shape=batch_shape,\n                                    validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        shape = sample_shape + self.batch_shape + self.event_shape\n        return random.gamma(key, self.concentration, shape=shape) / self.rate\n\n    @validate_sample\n    def log_prob(self, value):\n        normalize_term = (gammaln(self.concentration) -\n                          self.concentration * np.log(self.rate))\n        return (self.concentration - 1) * np.log(value) - self.rate * value - normalize_term\n\n    @property\n    def mean(self):\n        return self.concentration / self.rate\n\n    @property\n    def variance(self):\n        return self.concentration / np.power(self.rate, 2)\n\n\n@copy_docs_from(Distribution)\nclass Chi2(Gamma):\n    arg_constraints = {\'df\': constraints.positive}\n\n    def __init__(self, df, validate_args=None):\n        self.df = df\n        super(Chi2, self).__init__(0.5 * df, 0.5, validate_args=validate_args)\n\n\n@copy_docs_from(Distribution)\nclass GaussianRandomWalk(Distribution):\n    arg_constraints = {\'num_steps\': constraints.positive_integer, \'scale\': constraints.positive}\n    support = constraints.real_vector\n    reparametrized_params = [\'scale\']\n\n    def __init__(self, scale=1., num_steps=1, validate_args=None):\n        assert np.shape(num_steps) == ()\n        self.scale = scale\n        self.num_steps = num_steps\n        batch_shape, event_shape = np.shape(scale), (num_steps,)\n        super(GaussianRandomWalk, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        shape = sample_shape + self.batch_shape + self.event_shape\n        walks = random.normal(key, shape=shape)\n        return np.cumsum(walks, axis=-1) * np.expand_dims(self.scale, axis=-1)\n\n    @validate_sample\n    def log_prob(self, value):\n        init_prob = Normal(0., self.scale).log_prob(value[..., 0])\n        scale = np.expand_dims(self.scale, -1)\n        step_probs = Normal(value[..., :-1], scale).log_prob(value[..., 1:])\n        return init_prob + np.sum(step_probs, axis=-1)\n\n    @property\n    def mean(self):\n        return np.zeros(self.batch_shape + self.event_shape)\n\n    @property\n    def variance(self):\n        return np.broadcast_to(np.expand_dims(self.scale, -1) ** 2 * np.arange(1, self.num_steps + 1),\n                               self.batch_shape + self.event_shape)\n\n\n@copy_docs_from(Distribution)\nclass HalfCauchy(Distribution):\n    reparametrized_params = [\'scale\']\n    support = constraints.positive\n    arg_constraints = {\'scale\': constraints.positive}\n\n    def __init__(self, scale=1., validate_args=None):\n        self._cauchy = Cauchy(0., scale)\n        self.scale = scale\n        super(HalfCauchy, self).__init__(batch_shape=np.shape(scale), validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return np.abs(self._cauchy.sample(key, sample_shape))\n\n    @validate_sample\n    def log_prob(self, value):\n        return self._cauchy.log_prob(value) + np.log(2)\n\n    @property\n    def mean(self):\n        return np.full(self.batch_shape, np.inf)\n\n    @property\n    def variance(self):\n        return np.full(self.batch_shape, np.inf)\n\n\n@copy_docs_from(Distribution)\nclass HalfNormal(Distribution):\n    reparametrized_params = [\'scale\']\n    support = constraints.positive\n    arg_constraints = {\'scale\': constraints.positive}\n\n    def __init__(self, scale=1., validate_args=None):\n        self._normal = Normal(0., scale)\n        self.scale = scale\n        super(HalfNormal, self).__init__(batch_shape=np.shape(scale), validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return np.abs(self._normal.sample(key, sample_shape))\n\n    @validate_sample\n    def log_prob(self, value):\n        return self._normal.log_prob(value) + np.log(2)\n\n    @property\n    def mean(self):\n        return np.sqrt(2 / np.pi) * self.scale\n\n    @property\n    def variance(self):\n        return (1 - 2 / np.pi) * self.scale ** 2\n\n\n@copy_docs_from(Distribution)\nclass InverseGamma(TransformedDistribution):\n    arg_constraints = {\'concentration\': constraints.positive, \'rate\': constraints.positive}\n    support = constraints.positive\n    reparametrized_params = [\'rate\']\n\n    def __init__(self, concentration, rate=1., validate_args=None):\n        # NB: we keep the same notation `rate` as in Pyro and tensorflow but\n        # it plays the role of scale parameter of InverseGamma in literatures\n        # (e.g. wikipedia: https://en.wikipedia.org/wiki/Inverse-gamma_distribution)\n        base_dist = Gamma(concentration, rate)\n        self.concentration = concentration\n        self.rate = rate\n        super(InverseGamma, self).__init__(base_dist, PowerTransform(-1.0),\n                                           validate_args=validate_args)\n\n    @property\n    def mean(self):\n        # mean is inf for alpha <= 1\n        a = self.rate / (self.concentration - 1)\n        return np.where(self.concentration <= 1, np.inf, a)\n\n    @property\n    def variance(self):\n        # var is inf for alpha <= 2\n        a = (self.rate / (self.concentration - 1)) ** 2 / (self.concentration - 2)\n        return np.where(self.concentration <= 2, np.inf, a)\n\n\n@copy_docs_from(Distribution)\nclass Gumbel(Distribution):\n    arg_constraints = {\'loc\': constraints.real, \'scale\': constraints.positive}\n    support = constraints.real\n    reparametrized_params = [\'loc\', \'scale\']\n\n    def __init__(self, loc=0., scale=1., validate_args=None):\n        self.loc, self.scale = promote_shapes(loc, scale)\n        batch_shape = lax.broadcast_shapes(np.shape(loc), np.shape(scale))\n\n        super(Gumbel, self).__init__(batch_shape=batch_shape,\n                                     validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        standard_gumbel_sample = random.gumbel(key, shape=sample_shape + self.batch_shape + self.event_shape)\n        return self.loc + self.scale * standard_gumbel_sample\n\n    @validate_sample\n    def log_prob(self, value):\n        z = (value - self.loc) / self.scale\n        return -(z + np.exp(-z)) - np.log(self.scale)\n\n    @property\n    def mean(self):\n        return np.broadcast_to(self.loc + self.scale * EULER_MASCHERONI,\n                               self.batch_shape)\n\n    @property\n    def variance(self):\n        return np.broadcast_to(np.pi**2 / 6. * self.scale**2,\n                               self.batch_shape)\n\n\n@copy_docs_from(Distribution)\nclass LKJ(TransformedDistribution):\n    r""""""\n    LKJ distribution for correlation matrices. The distribution is controlled by ``concentration``\n    parameter :math:`\\eta` to make the probability of the correlation matrix :math:`M` propotional\n    to :math:`\\det(M)^{\\eta - 1}`. Because of that, when ``concentration == 1``, we have a\n    uniform distribution over correlation matrices.\n\n    When ``concentration > 1``, the distribution favors samples with large large determinent. This\n    is useful when we know a priori that the underlying variables are not correlated.\n\n    When ``concentration < 1``, the distribution favors samples with small determinent. This is\n    useful when we know a priori that some underlying variables are correlated.\n\n    :param int dimension: dimension of the matrices\n    :param ndarray concentration: concentration/shape parameter of the\n        distribution (often referred to as eta)\n    :param str sample_method: Either ""cvine"" or ""onion"". Both methods are proposed in [1] and\n        offer the same distribution over correlation matrices. But they are different in how\n        to generate samples. Defaults to ""onion"".\n\n    **References**\n\n    [1] `Generating random correlation matrices based on vines and extended onion method`,\n    Daniel Lewandowski, Dorota Kurowicka, Harry Joe\n    """"""\n    arg_constraints = {\'concentration\': constraints.positive}\n    support = constraints.corr_matrix\n\n    def __init__(self, dimension, concentration=1., sample_method=\'onion\', validate_args=None):\n        base_dist = LKJCholesky(dimension, concentration, sample_method)\n        self.dimension, self.concentration = base_dist.dimension, base_dist.concentration\n        self.sample_method = sample_method\n        super(LKJ, self).__init__(base_dist, InvCholeskyTransform(domain=constraints.corr_cholesky),\n                                  validate_args=validate_args)\n\n    @property\n    def mean(self):\n        return np.broadcast_to(np.identity(self.dimension), self.batch_shape + (self.dimension, self.dimension))\n\n\n@copy_docs_from(Distribution)\nclass LKJCholesky(Distribution):\n    r""""""\n    LKJ distribution for lower Cholesky factors of correlation matrices. The distribution is\n    controlled by ``concentration`` parameter :math:`\\eta` to make the probability of the\n    correlation matrix :math:`M` generated from a Cholesky factor propotional to\n    :math:`\\det(M)^{\\eta - 1}`. Because of that, when ``concentration == 1``, we have a\n    uniform distribution over Cholesky factors of correlation matrices.\n\n    When ``concentration > 1``, the distribution favors samples with large diagonal entries\n    (hence large determinent). This is useful when we know a priori that the underlying\n    variables are not correlated.\n\n    When ``concentration < 1``, the distribution favors samples with small diagonal entries\n    (hence small determinent). This is useful when we know a priori that some underlying\n    variables are correlated.\n\n    :param int dimension: dimension of the matrices\n    :param ndarray concentration: concentration/shape parameter of the\n        distribution (often referred to as eta)\n    :param str sample_method: Either ""cvine"" or ""onion"". Both methods are proposed in [1] and\n        offer the same distribution over correlation matrices. But they are different in how\n        to generate samples. Defaults to ""onion"".\n\n    **References**\n\n    [1] `Generating random correlation matrices based on vines and extended onion method`,\n    Daniel Lewandowski, Dorota Kurowicka, Harry Joe\n    """"""\n    arg_constraints = {\'concentration\': constraints.positive}\n    support = constraints.corr_cholesky\n\n    def __init__(self, dimension, concentration=1., sample_method=\'onion\', validate_args=None):\n        if dimension < 2:\n            raise ValueError(""Dimension must be greater than or equal to 2."")\n        self.dimension = dimension\n        self.concentration = concentration\n        batch_shape = np.shape(concentration)\n        event_shape = (dimension, dimension)\n\n        # We construct base distributions to generate samples for each method.\n        # The purpose of this base distribution is to generate a distribution for\n        # correlation matrices which is propotional to `det(M)^{\\eta - 1}`.\n        # (note that this is not a unique way to define base distribution)\n        # Both of the following methods have marginal distribution of each off-diagonal\n        # element of sampled correlation matrices is Beta(eta + (D-2) / 2, eta + (D-2) / 2)\n        # (up to a linear transform: x -> 2x - 1)\n        Dm1 = self.dimension - 1\n        marginal_concentration = concentration + 0.5 * (self.dimension - 2)\n        offset = 0.5 * np.arange(Dm1)\n        if sample_method == \'onion\':\n            # The following construction follows from the algorithm in Section 3.2 of [1]:\n            # NB: in [1], the method for case k > 1 can also work for the case k = 1.\n            beta_concentration0 = np.expand_dims(marginal_concentration, axis=-1) - offset\n            beta_concentration1 = offset + 0.5\n            self._beta = Beta(beta_concentration1, beta_concentration0)\n        elif sample_method == \'cvine\':\n            # The following construction follows from the algorithm in Section 2.4 of [1]:\n            # offset_tril is [0, 1, 1, 2, 2, 2,...] / 2\n            offset_tril = matrix_to_tril_vec(np.broadcast_to(offset, (Dm1, Dm1)))\n            beta_concentration = np.expand_dims(marginal_concentration, axis=-1) - offset_tril\n            self._beta = Beta(beta_concentration, beta_concentration)\n        else:\n            raise ValueError(""`method` should be one of \'cvine\' or \'onion\'."")\n        self.sample_method = sample_method\n\n        super(LKJCholesky, self).__init__(batch_shape=batch_shape,\n                                          event_shape=event_shape,\n                                          validate_args=validate_args)\n\n    def _cvine(self, key, size):\n        # C-vine method first uses beta_dist to generate partial correlations,\n        # then apply signed stick breaking to transform to cholesky factor.\n        # Here is an attempt to prove that using signed stick breaking to\n        # generate correlation matrices is the same as the C-vine method in [1]\n        # for the entry r_32.\n        #\n        # With notations follow from [1], we define\n        #   p: partial correlation matrix,\n        #   c: cholesky factor,\n        #   r: correlation matrix.\n        # From recursive formula (2) in [1], we have\n        #   r_32 = p_32 * sqrt{(1 - p_21^2)*(1 - p_31^2)} + p_21 * p_31 =: I\n        # On the other hand, signed stick breaking process gives:\n        #   l_21 = p_21, l_31 = p_31, l_22 = sqrt(1 - p_21^2), l_32 = p_32 * sqrt(1 - p_31^2)\n        #   r_32 = l_21 * l_31 + l_22 * l_32\n        #        = p_21 * p_31 + p_32 * sqrt{(1 - p_21^2)*(1 - p_31^2)} = I\n        beta_sample = self._beta.sample(key, size)\n        partial_correlation = 2 * beta_sample - 1  # scale to domain to (-1, 1)\n        return signed_stick_breaking_tril(partial_correlation)\n\n    def _onion(self, key, size):\n        key_beta, key_normal = random.split(key)\n        # Now we generate w term in Algorithm 3.2 of [1].\n        beta_sample = self._beta.sample(key_beta, size)\n        # The following Normal distribution is used to create a uniform distribution on\n        # a hypershere (ref: http://mathworld.wolfram.com/HyperspherePointPicking.html)\n        normal_sample = random.normal(\n            key_normal,\n            shape=size + self.batch_shape + (self.dimension * (self.dimension - 1) // 2,)\n        )\n        normal_sample = vec_to_tril_matrix(normal_sample, diagonal=0)\n        u_hypershere = normal_sample / np.linalg.norm(normal_sample, axis=-1, keepdims=True)\n        w = np.expand_dims(np.sqrt(beta_sample), axis=-1) * u_hypershere\n\n        # put w into the off-diagonal triangular part\n        cholesky = ops.index_add(np.zeros(size + self.batch_shape + self.event_shape),\n                                 ops.index[..., 1:, :-1], w)\n        # correct the diagonal\n        # NB: we clip due to numerical precision\n        diag = np.sqrt(np.clip(1 - np.sum(cholesky ** 2, axis=-1), a_min=0.))\n        cholesky = cholesky + np.expand_dims(diag, axis=-1) * np.identity(self.dimension)\n        return cholesky\n\n    def sample(self, key, sample_shape=()):\n        if self.sample_method == ""onion"":\n            return self._onion(key, sample_shape)\n        else:\n            return self._cvine(key, sample_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        # Note about computing Jacobian of the transformation from Cholesky factor to\n        # correlation matrix:\n        #\n        #   Assume C = L@Lt and L = (1 0 0; a \\sqrt(1-a^2) 0; b c \\sqrt(1-b^2-c^2)), we have\n        #   Then off-diagonal lower triangular vector of L is transformed to the off-diagonal\n        #   lower triangular vector of C by the transform:\n        #       (a, b, c) -> (a, b, ab + c\\sqrt(1-a^2))\n        #   Hence, Jacobian = 1 * 1 * \\sqrt(1 - a^2) = \\sqrt(1 - a^2) = L22, where L22\n        #       is the 2th diagonal element of L\n        #   Generally, for a D dimensional matrix, we have:\n        #       Jacobian = L22^(D-2) * L33^(D-3) * ... * Ldd^0\n        #\n        # From [1], we know that probability of a correlation matrix is propotional to\n        #   determinant ** (concentration - 1) = prod(L_ii ^ 2(concentration - 1))\n        # On the other hand, Jabobian of the transformation from Cholesky factor to\n        # correlation matrix is:\n        #   prod(L_ii ^ (D - i))\n        # So the probability of a Cholesky factor is propotional to\n        #   prod(L_ii ^ (2 * concentration - 2 + D - i)) =: prod(L_ii ^ order_i)\n        # with order_i = 2 * concentration - 2 + D - i,\n        # i = 2..D (we omit the element i = 1 because L_11 = 1)\n\n        # Compute `order` vector (note that we need to reindex i -> i-2):\n        one_to_D = np.arange(1, self.dimension)\n        order_offset = (3 - self.dimension) + one_to_D\n        order = 2 * np.expand_dims(self.concentration, axis=-1) - order_offset\n\n        # Compute unnormalized log_prob:\n        value_diag = value[..., one_to_D, one_to_D]\n        unnormalized = np.sum(order * np.log(value_diag), axis=-1)\n\n        # Compute normalization constant (on the first proof of page 1999 of [1])\n        Dm1 = self.dimension - 1\n        alpha = self.concentration + 0.5 * Dm1\n        denominator = gammaln(alpha) * Dm1\n        numerator = multigammaln(alpha - 0.5, Dm1)\n        # pi_constant in [1] is D * (D - 1) / 4 * log(pi)\n        # pi_constant in multigammaln is (D - 1) * (D - 2) / 4 * log(pi)\n        # hence, we need to add a pi_constant = (D - 1) * log(pi) / 2\n        pi_constant = 0.5 * Dm1 * np.log(np.pi)\n        normalize_term = pi_constant + numerator - denominator\n        return unnormalized - normalize_term\n\n\n@copy_docs_from(Distribution)\nclass LogNormal(TransformedDistribution):\n    arg_constraints = {\'loc\': constraints.real, \'scale\': constraints.positive}\n    reparametrized_params = [\'loc\', \'scale\']\n\n    def __init__(self, loc=0., scale=1., validate_args=None):\n        base_dist = Normal(loc, scale)\n        self.loc, self.scale = base_dist.loc, base_dist.scale\n        super(LogNormal, self).__init__(base_dist, ExpTransform(), validate_args=validate_args)\n\n    @property\n    def mean(self):\n        return np.exp(self.loc + self.scale ** 2 / 2)\n\n    @property\n    def variance(self):\n        return (np.exp(self.scale ** 2) - 1) * np.exp(2 * self.loc + self.scale ** 2)\n\n\ndef _batch_mahalanobis(bL, bx):\n    if bL.shape[:-1] == bx.shape:\n        # no need to use the below optimization procedure\n        solve_bL_bx = solve_triangular(bL, bx[..., None], lower=True).squeeze(-1)\n        return np.sum(np.square(solve_bL_bx), -1)\n\n    # NB: The following procedure handles the case: bL.shape = (i, 1, n, n), bx.shape = (i, j, n)\n    # because we don\'t want to broadcast bL to the shape (i, j, n, n).\n\n    # Assume that bL.shape = (i, 1, n, n), bx.shape = (..., i, j, n),\n    # we are going to make bx have shape (..., 1, j,  i, 1, n) to apply batched tril_solve\n    sample_ndim = bx.ndim - bL.ndim + 1  # size of sample_shape\n    out_shape = np.shape(bx)[:-1]  # shape of output\n    # Reshape bx with the shape (..., 1, i, j, 1, n)\n    bx_new_shape = out_shape[:sample_ndim]\n    for (sL, sx) in zip(bL.shape[:-2], out_shape[sample_ndim:]):\n        bx_new_shape += (sx // sL, sL)\n    bx_new_shape += (-1,)\n    bx = np.reshape(bx, bx_new_shape)\n    # Permute bx to make it have shape (..., 1, j, i, 1, n)\n    permute_dims = (tuple(range(sample_ndim))\n                    + tuple(range(sample_ndim, bx.ndim - 1, 2))\n                    + tuple(range(sample_ndim + 1, bx.ndim - 1, 2))\n                    + (bx.ndim - 1,))\n    bx = np.transpose(bx, permute_dims)\n\n    # reshape to (-1, i, 1, n)\n    xt = np.reshape(bx, (-1,) + bL.shape[:-1])\n    # permute to (i, 1, n, -1)\n    xt = np.moveaxis(xt, 0, -1)\n    solve_bL_bx = solve_triangular(bL, xt, lower=True)  # shape: (i, 1, n, -1)\n    M = np.sum(solve_bL_bx ** 2, axis=-2)  # shape: (i, 1, -1)\n    # permute back to (-1, i, 1)\n    M = np.moveaxis(M, -1, 0)\n    # reshape back to (..., 1, j, i, 1)\n    M = np.reshape(M, bx.shape[:-1])\n    # permute back to (..., 1, i, j, 1)\n    permute_inv_dims = tuple(range(sample_ndim))\n    for i in range(bL.ndim - 2):\n        permute_inv_dims += (sample_ndim + i, len(out_shape) + i)\n    M = np.transpose(M, permute_inv_dims)\n    return np.reshape(M, out_shape)\n\n\n@copy_docs_from(Distribution)\nclass MultivariateNormal(Distribution):\n    arg_constraints = {\'loc\': constraints.real_vector,\n                       \'covariance_matrix\': constraints.positive_definite,\n                       \'precision_matrix\': constraints.positive_definite,\n                       \'scale_tril\': constraints.lower_cholesky}\n    support = constraints.real_vector\n    reparametrized_params = [\'loc\', \'covariance_matrix\', \'precision_matrix\', \'scale_tril\']\n\n    def __init__(self, loc=0., covariance_matrix=None, precision_matrix=None, scale_tril=None,\n                 validate_args=None):\n        if np.isscalar(loc):\n            loc = np.expand_dims(loc, axis=-1)\n        # temporary append a new axis to loc\n        loc = loc[..., np.newaxis]\n        if covariance_matrix is not None:\n            loc, self.covariance_matrix = promote_shapes(loc, covariance_matrix)\n            self.scale_tril = np.linalg.cholesky(self.covariance_matrix)\n        elif precision_matrix is not None:\n            loc, self.precision_matrix = promote_shapes(loc, precision_matrix)\n            self.scale_tril = cholesky_of_inverse(self.precision_matrix)\n        elif scale_tril is not None:\n            loc, self.scale_tril = promote_shapes(loc, scale_tril)\n        else:\n            raise ValueError(\'One of `covariance_matrix`, `precision_matrix`, `scale_tril`\'\n                             \' must be specified.\')\n        batch_shape = lax.broadcast_shapes(np.shape(loc)[:-2], np.shape(self.scale_tril)[:-2])\n        event_shape = np.shape(self.scale_tril)[-1:]\n        self.loc = np.broadcast_to(np.squeeze(loc, axis=-1), batch_shape + event_shape)\n        super(MultivariateNormal, self).__init__(batch_shape=batch_shape,\n                                                 event_shape=event_shape,\n                                                 validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        eps = random.normal(key, shape=sample_shape + self.batch_shape + self.event_shape)\n        return self.loc + np.squeeze(np.matmul(self.scale_tril, eps[..., np.newaxis]), axis=-1)\n\n    @validate_sample\n    def log_prob(self, value):\n        M = _batch_mahalanobis(self.scale_tril, value - self.loc)\n        half_log_det = np.log(np.diagonal(self.scale_tril, axis1=-2, axis2=-1)).sum(-1)\n        normalize_term = half_log_det + 0.5 * self.scale_tril.shape[-1] * np.log(2 * np.pi)\n        return - 0.5 * M - normalize_term\n\n    @lazy_property\n    def covariance_matrix(self):\n        return np.matmul(self.scale_tril, np.swapaxes(self.scale_tril, -1, -2))\n\n    @lazy_property\n    def precision_matrix(self):\n        identity = np.broadcast_to(np.eye(self.scale_tril.shape[-1]), self.scale_tril.shape)\n        return cho_solve((self.scale_tril, True), identity)\n\n    @property\n    def mean(self):\n        return self.loc\n\n    @property\n    def variance(self):\n        return np.broadcast_to(np.sum(self.scale_tril ** 2, axis=-1),\n                               self.batch_shape + self.event_shape)\n\n\ndef _batch_mv(bmat, bvec):\n    r""""""\n    Performs a batched matrix-vector product, with compatible but different batch shapes.\n    This function takes as input `bmat`, containing :math:`n \\times n` matrices, and\n    `bvec`, containing length :math:`n` vectors.\n    Both `bmat` and `bvec` may have any number of leading dimensions, which correspond\n    to a batch shape. They are not necessarily assumed to have the same batch shape,\n    just ones which can be broadcasted.\n    """"""\n    return np.squeeze(np.matmul(bmat, np.expand_dims(bvec, axis=-1)), axis=-1)\n\n\ndef _batch_capacitance_tril(W, D):\n    r""""""\n    Computes Cholesky of :math:`I + W.T @ inv(D) @ W` for a batch of matrices :math:`W`\n    and a batch of vectors :math:`D`.\n    """"""\n    Wt_Dinv = np.swapaxes(W, -1, -2) / np.expand_dims(D, -2)\n    K = np.matmul(Wt_Dinv, W)\n    # could be inefficient\n    return np.linalg.cholesky(np.add(K, np.identity(K.shape[-1])))\n\n\ndef _batch_lowrank_logdet(W, D, capacitance_tril):\n    r""""""\n    Uses ""matrix determinant lemma""::\n        log|W @ W.T + D| = log|C| + log|D|,\n    where :math:`C` is the capacitance matrix :math:`I + W.T @ inv(D) @ W`, to compute\n    the log determinant.\n    """"""\n    return 2 * np.sum(np.log(np.diagonal(capacitance_tril, axis1=-2, axis2=-1)), axis=-1) + np.log(D).sum(-1)\n\n\ndef _batch_lowrank_mahalanobis(W, D, x, capacitance_tril):\n    r""""""\n    Uses ""Woodbury matrix identity""::\n        inv(W @ W.T + D) = inv(D) - inv(D) @ W @ inv(C) @ W.T @ inv(D),\n    where :math:`C` is the capacitance matrix :math:`I + W.T @ inv(D) @ W`, to compute the squared\n    Mahalanobis distance :math:`x.T @ inv(W @ W.T + D) @ x`.\n    """"""\n    Wt_Dinv = np.swapaxes(W, -1, -2) / np.expand_dims(D, -2)\n    Wt_Dinv_x = _batch_mv(Wt_Dinv, x)\n    mahalanobis_term1 = np.sum(np.square(x) / D, axis=-1)\n    mahalanobis_term2 = _batch_mahalanobis(capacitance_tril, Wt_Dinv_x)\n    return mahalanobis_term1 - mahalanobis_term2\n\n\n@copy_docs_from(Distribution)\nclass LowRankMultivariateNormal(Distribution):\n    arg_constraints = {\n        ""loc"": constraints.real_vector,\n        ""cov_factor"": constraints.real,\n        ""cov_diag"": constraints.positive\n        }\n    support = constraints.real_vector\n\n    def __init__(self, loc, cov_factor, cov_diag, validate_args=None):\n        if np.ndim(loc) < 1:\n            raise ValueError(""`loc` must be at least one-dimensional."")\n        event_shape = np.shape(loc)[-1:]\n        if np.ndim(cov_factor) < 2:\n            raise ValueError(""`cov_factor` must be at least two-dimensional, ""\n                             ""with optional leading batch dimensions"")\n        if np.shape(cov_factor)[-2:-1] != event_shape:\n            raise ValueError(""`cov_factor` must be a batch of matrices with shape {} x m""\n                             .format(event_shape[0]))\n        if np.shape(cov_diag)[-1:] != event_shape:\n            raise ValueError(""`cov_diag` must be a batch of vectors with shape {}"".format(self.event_shape))\n\n        loc, cov_factor, cov_diag = promote_shapes(loc[..., np.newaxis], cov_factor, cov_diag[..., np.newaxis])\n        batch_shape = lax.broadcast_shapes(np.shape(loc), np.shape(cov_factor), np.shape(cov_diag))[:-2]\n        self.loc = np.broadcast_to(loc[..., 0], batch_shape + event_shape)\n        self.cov_factor = cov_factor\n        cov_diag = cov_diag[..., 0]\n        self.cov_diag = cov_diag\n        self._capacitance_tril = _batch_capacitance_tril(cov_factor, cov_diag)\n        super(LowRankMultivariateNormal, self).__init__(\n            batch_shape=batch_shape, event_shape=event_shape, validate_args=validate_args\n            )\n\n    @property\n    def mean(self):\n        return self.loc\n\n    @lazy_property\n    def variance(self):\n        raw_variance = np.square(self.cov_factor).sum(-1) + self.cov_diag\n        return np.broadcast_to(raw_variance, self.batch_shape + self.event_shape)\n\n    @lazy_property\n    def scale_tril(self):\n        # The following identity is used to increase the numerically computation stability\n        # for Cholesky decomposition (see http://www.gaussianprocess.org/gpml/, Section 3.4.3):\n        #     W @ W.T + D = D1/2 @ (I + D-1/2 @ W @ W.T @ D-1/2) @ D1/2\n        # The matrix ""I + D-1/2 @ W @ W.T @ D-1/2"" has eigenvalues bounded from below by 1,\n        # hence it is well-conditioned and safe to take Cholesky decomposition.\n        cov_diag_sqrt_unsqueeze = np.expand_dims(np.sqrt(self.cov_diag), axis=-1)\n        Dinvsqrt_W = self.cov_factor / cov_diag_sqrt_unsqueeze\n        K = np.matmul(Dinvsqrt_W, np.swapaxes(Dinvsqrt_W, -1, -2))\n        K = np.add(K, np.identity(K.shape[-1]))\n        scale_tril = cov_diag_sqrt_unsqueeze * np.linalg.cholesky(K)\n        return scale_tril\n\n    @lazy_property\n    def covariance_matrix(self):\n        # TODO: find a better solution to create a diagonal matrix\n        new_diag = self.cov_diag[..., np.newaxis] * np.identity(self.loc.shape[-1])\n        covariance_matrix = new_diag + np.matmul(\n            self.cov_factor, np.swapaxes(self.cov_factor, -1, -2)\n            )\n        return covariance_matrix\n\n    @lazy_property\n    def precision_matrix(self):\n        # We use ""Woodbury matrix identity"" to take advantage of low rank form::\n        #     inv(W @ W.T + D) = inv(D) - inv(D) @ W @ inv(C) @ W.T @ inv(D)\n        # where :math:`C` is the capacitance matrix.\n        Wt_Dinv = (np.swapaxes(self.cov_factor, -1, -2)\n                   / np.expand_dims(self.cov_diag, axis=-2))\n        A = solve_triangular(Wt_Dinv, self._capacitance_tril, lower=True)\n        # TODO: find a better solution to create a diagonal matrix\n        inverse_cov_diag = np.reciprocal(self.cov_diag)\n        diag_embed = inverse_cov_diag[..., np.newaxis] * np.identity(self.loc.shape[-1])\n        return diag_embed - np.matmul(np.swapaxes(A, -1, -2), A)\n\n    def sample(self, key, sample_shape=()):\n        key_W, key_D = random.split(key)\n        batch_shape = sample_shape + self.batch_shape\n        W_shape = batch_shape + self.cov_factor.shape[-1:]\n        D_shape = batch_shape + self.cov_diag.shape[-1:]\n        eps_W = random.normal(key_W, W_shape)\n        eps_D = random.normal(key_D, D_shape)\n        return (self.loc + _batch_mv(self.cov_factor, eps_W)\n                + np.sqrt(self.cov_diag) * eps_D)\n\n    @validate_sample\n    def log_prob(self, value):\n        diff = value - self.loc\n        M = _batch_lowrank_mahalanobis(self.cov_factor,\n                                       self.cov_diag,\n                                       diff,\n                                       self._capacitance_tril)\n        log_det = _batch_lowrank_logdet(self.cov_factor,\n                                        self.cov_diag,\n                                        self._capacitance_tril)\n        return -0.5 * (self.loc.shape[-1] * np.log(2 * np.pi) + log_det + M)\n\n    def entropy(self):\n        log_det = _batch_lowrank_logdet(self.cov_factor,\n                                        self.cov_diag,\n                                        self._capacitance_tril)\n        H = 0.5 * (self.loc.shape[-1] * (1.0 + np.log(2 * np.pi)) + log_det)\n        return np.broadcast_to(H, self.batch_shape)\n\n\n@copy_docs_from(Distribution)\nclass Normal(Distribution):\n    arg_constraints = {\'loc\': constraints.real, \'scale\': constraints.positive}\n    support = constraints.real\n    reparametrized_params = [\'loc\', \'scale\']\n\n    def __init__(self, loc=0., scale=1., validate_args=None):\n        self.loc, self.scale = promote_shapes(loc, scale)\n        batch_shape = lax.broadcast_shapes(np.shape(loc), np.shape(scale))\n        super(Normal, self).__init__(batch_shape=batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        eps = random.normal(key, shape=sample_shape + self.batch_shape + self.event_shape)\n        return self.loc + eps * self.scale\n\n    @validate_sample\n    def log_prob(self, value):\n        normalize_term = np.log(np.sqrt(2 * np.pi) * self.scale)\n        value_scaled = (value - self.loc) / self.scale\n        return -0.5 * value_scaled ** 2 - normalize_term\n\n    def icdf(self, q):\n        return self.loc + self.scale * ndtri(q)\n\n    @property\n    def mean(self):\n        return np.broadcast_to(self.loc, self.batch_shape)\n\n    @property\n    def variance(self):\n        return np.broadcast_to(self.scale ** 2, self.batch_shape)\n\n\n@copy_docs_from(Distribution)\nclass Pareto(TransformedDistribution):\n    arg_constraints = {\'alpha\': constraints.positive, \'scale\': constraints.positive}\n\n    def __init__(self, alpha, scale=1., validate_args=None):\n        batch_shape = lax.broadcast_shapes(np.shape(scale), np.shape(alpha))\n        self.scale, self.alpha = np.broadcast_to(scale, batch_shape), np.broadcast_to(alpha, batch_shape)\n        base_dist = Exponential(self.alpha)\n        transforms = [ExpTransform(), AffineTransform(loc=0, scale=self.scale)]\n        super(Pareto, self).__init__(base_dist, transforms, validate_args=validate_args)\n\n    @property\n    def mean(self):\n        # mean is inf for alpha <= 1\n        a = lax.div(self.alpha * self.scale, (self.alpha - 1))\n        return np.where(self.alpha <= 1, np.inf, a)\n\n    @property\n    def variance(self):\n        # var is inf for alpha <= 2\n        a = lax.div((self.scale ** 2) * self.alpha, (self.alpha - 1) ** 2 * (self.alpha - 2))\n        return np.where(self.alpha <= 2, np.inf, a)\n\n    # override the default behaviour to save computations\n    @property\n    def support(self):\n        return constraints.greater_than(self.scale)\n\n\n@copy_docs_from(Distribution)\nclass StudentT(Distribution):\n    arg_constraints = {\'df\': constraints.positive, \'loc\': constraints.real, \'scale\': constraints.positive}\n    support = constraints.real\n    reparametrized_params = [\'loc\', \'scale\']\n\n    def __init__(self, df, loc=0., scale=1., validate_args=None):\n        batch_shape = lax.broadcast_shapes(np.shape(df), np.shape(loc), np.shape(scale))\n        self.df = np.broadcast_to(df, batch_shape)\n        self.loc, self.scale = promote_shapes(loc, scale, shape=batch_shape)\n        self._chi2 = Chi2(self.df)\n        super(StudentT, self).__init__(batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        key_normal, key_chi2 = random.split(key)\n        std_normal = random.normal(key_normal, shape=sample_shape + self.batch_shape)\n        z = self._chi2.sample(key_chi2, sample_shape)\n        y = std_normal * np.sqrt(self.df / z)\n        return self.loc + self.scale * y\n\n    @validate_sample\n    def log_prob(self, value):\n        y = (value - self.loc) / self.scale\n        z = (np.log(self.scale) + 0.5 * np.log(self.df) + 0.5 * np.log(np.pi) +\n             gammaln(0.5 * self.df) - gammaln(0.5 * (self.df + 1.)))\n        return -0.5 * (self.df + 1.) * np.log1p(y ** 2. / self.df) - z\n\n    @property\n    def mean(self):\n        # for df <= 1. should be np.nan (keeping np.inf for consistency with scipy)\n        return np.broadcast_to(np.where(self.df <= 1, np.inf, self.loc), self.batch_shape)\n\n    @property\n    def variance(self):\n        var = np.where(self.df > 2, self.scale ** 2 * self.df / (self.df - 2.0), np.inf)\n        var = np.where(self.df <= 1, np.nan, var)\n        return np.broadcast_to(var, self.batch_shape)\n\n\nclass _BaseTruncatedCauchy(Distribution):\n    # NB: this is a truncated cauchy with low=0, scale=1\n    support = constraints.positive\n\n    def __init__(self, base_loc):\n        self.base_loc = base_loc\n        super(_BaseTruncatedCauchy, self).__init__(batch_shape=np.shape(base_loc))\n\n    def sample(self, key, sample_shape=()):\n        # We use inverse transform method:\n        # z ~ inv_cdf(U), where U ~ Uniform(cdf(low), cdf(high)).\n        #                         ~ Uniform(arctan(low), arctan(high)) / pi + 1/2\n        size = sample_shape + self.batch_shape\n        minval = -np.arctan(self.base_loc)\n        maxval = np.pi / 2\n        u = minval + random.uniform(key, shape=size) * (maxval - minval)\n        return self.base_loc + np.tan(u)\n\n    @validate_sample\n    def log_prob(self, value):\n        # pi / 2 is arctan of self.high when that arg is supported\n        normalize_term = np.log(np.pi / 2 + np.arctan(self.base_loc))\n        return - np.log1p((value - self.base_loc) ** 2) - normalize_term\n\n\n@copy_docs_from(Distribution)\nclass TruncatedCauchy(TransformedDistribution):\n    arg_constraints = {\'low\': constraints.real, \'loc\': constraints.real,\n                       \'scale\': constraints.positive}\n    reparametrized_params = [\'low\', \'loc\', \'scale\']\n\n    def __init__(self, low=0., loc=0., scale=1., validate_args=None):\n        self.low, self.loc, self.scale = promote_shapes(low, loc, scale)\n        base_loc = (loc - low) / scale\n        base_dist = _BaseTruncatedCauchy(base_loc)\n        self._support = constraints.greater_than(low)\n        super(TruncatedCauchy, self).__init__(base_dist, AffineTransform(low, scale),\n                                              validate_args=validate_args)\n\n    @property\n    def support(self):\n        return self._support\n\n    # NB: these stats do not apply when arg `high` is supported\n    @property\n    def mean(self):\n        return np.full(self.batch_shape, np.nan)\n\n    @property\n    def variance(self):\n        return np.full(self.batch_shape, np.nan)\n\n\nclass _BaseTruncatedNormal(Distribution):\n    # NB: this is a truncated normal with low=0, scale=1\n    support = constraints.positive\n\n    def __init__(self, base_loc):\n        self.base_loc = base_loc\n        self._normal = Normal(base_loc, 1.)\n        super(_BaseTruncatedNormal, self).__init__(batch_shape=np.shape(base_loc))\n\n    def sample(self, key, sample_shape=()):\n        size = sample_shape + self.batch_shape\n        # We use inverse transform method:\n        # z ~ icdf(U), where U ~ Uniform(0, 1).\n        u = random.uniform(key, shape=size)\n        # Ref: https://en.wikipedia.org/wiki/Truncated_normal_distribution#Simulating\n        # icdf[cdf_a + u * (1 - cdf_a)] = icdf[1 - (1 - cdf_a)(1 - u)]\n        #                                 = - icdf[(1 - cdf_a)(1 - u)]\n        return self.base_loc - ndtri(ndtr(self.base_loc) * (1 - u))\n\n    @validate_sample\n    def log_prob(self, value):\n        # log(cdf(high) - cdf(low)) = log(1 - cdf(low)) = log(cdf(-low))\n        return self._normal.log_prob(value) - log_ndtr(self.base_loc)\n\n\n@copy_docs_from(Distribution)\nclass TruncatedNormal(TransformedDistribution):\n    arg_constraints = {\'low\': constraints.real, \'loc\': constraints.real,\n                       \'scale\': constraints.positive}\n    reparametrized_params = [\'low\', \'loc\', \'scale\']\n\n    # TODO: support `high` arg\n    def __init__(self, low=0., loc=0., scale=1., validate_args=None):\n        self.low, self.loc, self.scale = promote_shapes(low, loc, scale)\n        base_loc = (loc - low) / scale\n        base_dist = _BaseTruncatedNormal(base_loc)\n        self._support = constraints.greater_than(low)\n        super(TruncatedNormal, self).__init__(base_dist, AffineTransform(low, scale),\n                                              validate_args=validate_args)\n\n    @property\n    def support(self):\n        return self._support\n\n    @property\n    def mean(self):\n        low_prob_scaled = np.exp(self.base_dist.log_prob(0.))\n        return self.loc + low_prob_scaled * self.scale\n\n    @property\n    def variance(self):\n        low_prob_scaled = np.exp(self.base_dist.log_prob(0.))\n        return (self.scale ** 2) * (1 - self.base_dist.base_loc * low_prob_scaled - low_prob_scaled ** 2)\n\n\nclass _BaseUniform(Distribution):\n    support = constraints.unit_interval\n\n    def __init__(self, batch_shape=()):\n        super(_BaseUniform, self).__init__(batch_shape=batch_shape)\n\n    def sample(self, key, sample_shape=()):\n        size = sample_shape + self.batch_shape\n        return random.uniform(key, shape=size)\n\n    @validate_sample\n    def log_prob(self, value):\n        batch_shape = lax.broadcast_shapes(self.batch_shape, np.shape(value))\n        return - np.zeros(batch_shape)\n\n\n@copy_docs_from(Distribution)\nclass Uniform(TransformedDistribution):\n    arg_constraints = {\'low\': constraints.dependent, \'high\': constraints.dependent}\n    reparametrized_params = [\'low\', \'high\']\n\n    def __init__(self, low=0., high=1., validate_args=None):\n        self.low, self.high = promote_shapes(low, high)\n        batch_shape = lax.broadcast_shapes(np.shape(low), np.shape(high))\n        base_dist = _BaseUniform(batch_shape)\n        self._support = constraints.interval(low, high)\n        super(Uniform, self).__init__(base_dist, AffineTransform(low, high - low), validate_args=validate_args)\n\n    @property\n    def support(self):\n        return self._support\n\n    @property\n    def mean(self):\n        return self.low + (self.high - self.low) / 2.\n\n    @property\n    def variance(self):\n        return (self.high - self.low) ** 2 / 12.\n\n\n@copy_docs_from(Distribution)\nclass Logistic(Distribution):\n    arg_constraints = {\'loc\': constraints.real, \'scale\': constraints.positive}\n    support = constraints.real\n    reparametrized_params = [\'loc\', \'real\']\n\n    def __init__(self, loc=0., scale=1., validate_args=None):\n        self.loc, self.scale = promote_shapes(loc, scale)\n        batch_shape = lax.broadcast_shapes(np.shape(loc), np.shape(scale))\n        super(Logistic, self).__init__(batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        z = random.logistic(key, shape=sample_shape + self.batch_shape + self.event_shape)\n        return self.loc + z * self.scale\n\n    @validate_sample\n    def log_prob(self, value):\n        log_exponent = (self.loc - value) / self.scale\n        log_denominator = np.log(self.scale) + 2 * nn.softplus(log_exponent)\n        return log_exponent - log_denominator\n\n    @property\n    def mean(self):\n        return np.broadcast_to(self.loc, self.batch_shape)\n\n    @property\n    def variance(self):\n        var = (self.scale ** 2) * (np.pi ** 2) / 3\n        return np.broadcast_to(var, self.batch_shape)\n\n\n@copy_docs_from(Distribution)\nclass TruncatedPolyaGamma(Distribution):\n    truncation_point = 2.5\n    num_log_prob_terms = 7\n    num_gamma_variates = 8\n    assert num_log_prob_terms % 2 == 1\n\n    arg_constraints = {}\n    support = constraints.interval(0.0, truncation_point)\n\n    def __init__(self, batch_shape=(), validate_args=None):\n        super(TruncatedPolyaGamma, self).__init__(batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        denom = np.square(np.arange(0.5, self.num_gamma_variates))\n        x = random.gamma(key, np.ones(self.batch_shape + sample_shape + (self.num_gamma_variates,)))\n        x = np.sum(x / denom, axis=-1)\n        return np.clip(x * (0.5 / np.pi ** 2), a_max=self.truncation_point)\n\n    @validate_sample\n    def log_prob(self, value):\n        value = value[..., None]\n        all_indices = np.arange(0, self.num_log_prob_terms)\n        two_n_plus_one = 2.0 * all_indices + 1.0\n        log_terms = np.log(two_n_plus_one) - 1.5 * np.log(value) - 0.125 * np.square(two_n_plus_one) / value\n        even_terms = np.take(log_terms, all_indices[::2], axis=-1)\n        odd_terms = np.take(log_terms, all_indices[1::2], axis=-1)\n        sum_even = np.exp(logsumexp(even_terms, axis=-1))\n        sum_odd = np.exp(logsumexp(odd_terms, axis=-1))\n        return np.log(sum_even - sum_odd) - 0.5 * np.log(2.0 * np.pi)\n'"
numpyro/distributions/discrete.py,83,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# The implementation largely follows the design in PyTorch\'s `torch.distributions`\n#\n# Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)\n# Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\n# Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n# Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n# Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n# Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n# Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n# Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n# Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\nfrom jax import device_put, lax\nfrom jax.dtypes import canonicalize_dtype\nfrom jax.nn import softmax\nimport jax.numpy as np\nimport jax.random as random\nfrom jax.scipy.special import expit, gammaln, logsumexp, xlog1py, xlogy\n\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.distribution import Distribution\nfrom numpyro.distributions.util import (\n    binary_cross_entropy_with_logits,\n    binomial,\n    categorical,\n    categorical_logits,\n    clamp_probs,\n    get_dtype,\n    lazy_property,\n    multinomial,\n    poisson,\n    promote_shapes,\n    sum_rightmost,\n    validate_sample,\n)\nfrom numpyro.util import copy_docs_from, not_jax_tracer\n\n\ndef _to_probs_bernoulli(logits):\n    return 1 / (1 + np.exp(-logits))\n\n\ndef _to_logits_bernoulli(probs):\n    ps_clamped = clamp_probs(probs)\n    return np.log(ps_clamped) - np.log1p(-ps_clamped)\n\n\ndef _to_probs_multinom(logits):\n    return softmax(logits, axis=-1)\n\n\ndef _to_logits_multinom(probs):\n    minval = np.finfo(get_dtype(probs)).min\n    return np.clip(np.log(probs), a_min=minval)\n\n\n@copy_docs_from(Distribution)\nclass BernoulliProbs(Distribution):\n    arg_constraints = {\'probs\': constraints.unit_interval}\n    support = constraints.boolean\n    has_enumerate_support = True\n    is_discrete = True\n\n    def __init__(self, probs, validate_args=None):\n        self.probs = probs\n        super(BernoulliProbs, self).__init__(batch_shape=np.shape(self.probs), validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return random.bernoulli(key, self.probs, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        return xlogy(value, self.probs) + xlog1py(1 - value, -self.probs)\n\n    @property\n    def mean(self):\n        return self.probs\n\n    @property\n    def variance(self):\n        return self.probs * (1 - self.probs)\n\n    def enumerate_support(self, expand=True):\n        values = np.arange(2).reshape((-1,) + (1,) * len(self.batch_shape))\n        if expand:\n            values = np.broadcast_to(values, values.shape[:1] + self.batch_shape)\n        return values\n\n\n@copy_docs_from(Distribution)\nclass BernoulliLogits(Distribution):\n    arg_constraints = {\'logits\': constraints.real}\n    support = constraints.boolean\n    has_enumerate_support = True\n    is_discrete = True\n\n    def __init__(self, logits=None, validate_args=None):\n        self.logits = logits\n        super(BernoulliLogits, self).__init__(batch_shape=np.shape(self.logits), validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return random.bernoulli(key, self.probs, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        return -binary_cross_entropy_with_logits(self.logits, value)\n\n    @lazy_property\n    def probs(self):\n        return _to_probs_bernoulli(self.logits)\n\n    @property\n    def mean(self):\n        return self.probs\n\n    @property\n    def variance(self):\n        return self.probs * (1 - self.probs)\n\n    def enumerate_support(self, expand=True):\n        values = np.arange(2).reshape((-1,) + (1,) * len(self.batch_shape))\n        if expand:\n            values = np.broadcast_to(values, values.shape[:1] + self.batch_shape)\n        return values\n\n\ndef Bernoulli(probs=None, logits=None, validate_args=None):\n    if probs is not None:\n        return BernoulliProbs(probs, validate_args=validate_args)\n    elif logits is not None:\n        return BernoulliLogits(logits, validate_args=validate_args)\n    else:\n        raise ValueError(\'One of `probs` or `logits` must be specified.\')\n\n\n@copy_docs_from(Distribution)\nclass BinomialProbs(Distribution):\n    arg_constraints = {\'total_count\': constraints.nonnegative_integer,\n                       \'probs\': constraints.unit_interval}\n    has_enumerate_support = True\n    is_discrete = True\n\n    def __init__(self, probs, total_count=1, validate_args=None):\n        self.probs, self.total_count = promote_shapes(probs, total_count)\n        batch_shape = lax.broadcast_shapes(np.shape(probs), np.shape(total_count))\n        super(BinomialProbs, self).__init__(batch_shape=batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return binomial(key, self.probs, n=self.total_count, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        log_factorial_n = gammaln(self.total_count + 1)\n        log_factorial_k = gammaln(value + 1)\n        log_factorial_nmk = gammaln(self.total_count - value + 1)\n        return (log_factorial_n - log_factorial_k - log_factorial_nmk +\n                xlogy(value, self.probs) + xlog1py(self.total_count - value, -self.probs))\n\n    @property\n    def mean(self):\n        return np.broadcast_to(self.total_count * self.probs, self.batch_shape)\n\n    @property\n    def variance(self):\n        return np.broadcast_to(self.total_count * self.probs * (1 - self.probs), self.batch_shape)\n\n    @property\n    def support(self):\n        return constraints.integer_interval(0, self.total_count)\n\n    def enumerate_support(self, expand=True):\n        total_count = np.amax(self.total_count)\n        if not_jax_tracer(total_count):\n            # NB: the error can\'t be raised if inhomogeneous issue happens when tracing\n            if np.amin(self.total_count) != total_count:\n                raise NotImplementedError(""Inhomogeneous total count not supported""\n                                          "" by `enumerate_support`."")\n        values = np.arange(total_count + 1).reshape((-1,) + (1,) * len(self.batch_shape))\n        if expand:\n            values = np.broadcast_to(values, values.shape[:1] + self.batch_shape)\n        return values\n\n\n@copy_docs_from(Distribution)\nclass BinomialLogits(Distribution):\n    arg_constraints = {\'total_count\': constraints.nonnegative_integer,\n                       \'logits\': constraints.real}\n    has_enumerate_support = True\n    is_discrete = True\n\n    def __init__(self, logits, total_count=1, validate_args=None):\n        self.logits, self.total_count = promote_shapes(logits, total_count)\n        batch_shape = lax.broadcast_shapes(np.shape(logits), np.shape(total_count))\n        super(BinomialLogits, self).__init__(batch_shape=batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return binomial(key, self.probs, n=self.total_count, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        log_factorial_n = gammaln(self.total_count + 1)\n        log_factorial_k = gammaln(value + 1)\n        log_factorial_nmk = gammaln(self.total_count - value + 1)\n        normalize_term = (self.total_count * np.clip(self.logits, 0) +\n                          xlog1py(self.total_count, np.exp(-np.abs(self.logits))) -\n                          log_factorial_n)\n        return value * self.logits - log_factorial_k - log_factorial_nmk - normalize_term\n\n    @lazy_property\n    def probs(self):\n        return _to_probs_bernoulli(self.logits)\n\n    @property\n    def mean(self):\n        return np.broadcast_to(self.total_count * self.probs, self.batch_shape)\n\n    @property\n    def variance(self):\n        return np.broadcast_to(self.total_count * self.probs * (1 - self.probs), self.batch_shape)\n\n    @property\n    def support(self):\n        return constraints.integer_interval(0, self.total_count)\n\n    def enumerate_support(self, expand=True):\n        total_count = np.amax(self.total_count)\n        if not_jax_tracer(total_count):\n            # NB: the error can\'t be raised if inhomogeneous issue happens when tracing\n            if np.amin(self.total_count) != total_count:\n                raise NotImplementedError(""Inhomogeneous total count not supported""\n                                          "" by `enumerate_support`."")\n        values = np.arange(total_count + 1).reshape((-1,) + (1,) * len(self.batch_shape))\n        if expand:\n            values = np.broadcast_to(values, values.shape[:1] + self.batch_shape)\n        return values\n\n\ndef Binomial(total_count=1, probs=None, logits=None, validate_args=None):\n    if probs is not None:\n        return BinomialProbs(probs, total_count, validate_args=validate_args)\n    elif logits is not None:\n        return BinomialLogits(logits, total_count, validate_args=validate_args)\n    else:\n        raise ValueError(\'One of `probs` or `logits` must be specified.\')\n\n\n@copy_docs_from(Distribution)\nclass CategoricalProbs(Distribution):\n    arg_constraints = {\'probs\': constraints.simplex}\n    has_enumerate_support = True\n    is_discrete = True\n\n    def __init__(self, probs, validate_args=None):\n        if np.ndim(probs) < 1:\n            raise ValueError(""`probs` parameter must be at least one-dimensional."")\n        self.probs = probs\n        super(CategoricalProbs, self).__init__(batch_shape=np.shape(self.probs)[:-1],\n                                               validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return categorical(key, self.probs, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        batch_shape = lax.broadcast_shapes(np.shape(value), self.batch_shape)\n        value = np.expand_dims(value, axis=-1)\n        value = np.broadcast_to(value, batch_shape + (1,))\n        logits = _to_logits_multinom(self.probs)\n        log_pmf = np.broadcast_to(logits, batch_shape + np.shape(logits)[-1:])\n        return np.take_along_axis(log_pmf, value, axis=-1)[..., 0]\n\n    @property\n    def mean(self):\n        return np.full(self.batch_shape, np.nan, dtype=get_dtype(self.probs))\n\n    @property\n    def variance(self):\n        return np.full(self.batch_shape, np.nan, dtype=get_dtype(self.probs))\n\n    @property\n    def support(self):\n        return constraints.integer_interval(0, np.shape(self.probs)[-1] - 1)\n\n    def enumerate_support(self, expand=True):\n        values = np.arange(self.probs.shape[-1]).reshape((-1,) + (1,) * len(self.batch_shape))\n        if expand:\n            values = np.broadcast_to(values, values.shape[:1] + self.batch_shape)\n        return values\n\n\n@copy_docs_from(Distribution)\nclass CategoricalLogits(Distribution):\n    arg_constraints = {\'logits\': constraints.real_vector}\n    has_enumerate_support = True\n    is_discrete = True\n\n    def __init__(self, logits, validate_args=None):\n        if np.ndim(logits) < 1:\n            raise ValueError(""`logits` parameter must be at least one-dimensional."")\n        self.logits = logits\n        super(CategoricalLogits, self).__init__(batch_shape=np.shape(logits)[:-1],\n                                                validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return categorical_logits(key, self.logits, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        batch_shape = lax.broadcast_shapes(np.shape(value), self.batch_shape)\n        value = np.expand_dims(value, -1)\n        value = np.broadcast_to(value, batch_shape + (1,))\n        log_pmf = self.logits - logsumexp(self.logits, axis=-1, keepdims=True)\n        log_pmf = np.broadcast_to(log_pmf, batch_shape + np.shape(log_pmf)[-1:])\n        return np.take_along_axis(log_pmf, value, -1)[..., 0]\n\n    @lazy_property\n    def probs(self):\n        return _to_probs_multinom(self.logits)\n\n    @property\n    def mean(self):\n        return np.full(self.batch_shape, np.nan, dtype=get_dtype(self.logits))\n\n    @property\n    def variance(self):\n        return np.full(self.batch_shape, np.nan, dtype=get_dtype(self.logits))\n\n    @property\n    def support(self):\n        return constraints.integer_interval(0, np.shape(self.logits)[-1] - 1)\n\n    def enumerate_support(self, expand=True):\n        values = np.arange(self.logits.shape[-1]).reshape((-1,) + (1,) * len(self.batch_shape))\n        if expand:\n            values = np.broadcast_to(values, values.shape[:1] + self.batch_shape)\n        return values\n\n\ndef Categorical(probs=None, logits=None, validate_args=None):\n    if probs is not None:\n        return CategoricalProbs(probs, validate_args=validate_args)\n    elif logits is not None:\n        return CategoricalLogits(logits, validate_args=validate_args)\n    else:\n        raise ValueError(\'One of `probs` or `logits` must be specified.\')\n\n\n@copy_docs_from(Distribution)\nclass Delta(Distribution):\n    arg_constraints = {\'value\': constraints.real, \'log_density\': constraints.real}\n    support = constraints.real\n    is_discrete = True\n\n    def __init__(self, value=0., log_density=0., event_dim=0, validate_args=None):\n        if event_dim > np.ndim(value):\n            raise ValueError(\'Expected event_dim <= v.dim(), actual {} vs {}\'\n                             .format(event_dim, np.ndim(value)))\n        batch_dim = np.ndim(value) - event_dim\n        batch_shape = np.shape(value)[:batch_dim]\n        event_shape = np.shape(value)[batch_dim:]\n        self.value = lax.convert_element_type(value, canonicalize_dtype(np.float64))\n        # NB: following Pyro implementation, log_density should be broadcasted to batch_shape\n        self.log_density = promote_shapes(log_density, shape=batch_shape)[0]\n        super(Delta, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        shape = sample_shape + self.batch_shape + self.event_shape\n        return np.broadcast_to(device_put(self.value), shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        log_prob = np.log(value == self.value)\n        log_prob = sum_rightmost(log_prob, len(self.event_shape))\n        return log_prob + self.log_density\n\n    @property\n    def mean(self):\n        return self.value\n\n    @property\n    def variance(self):\n        return np.zeros(self.batch_shape + self.event_shape)\n\n\nclass OrderedLogistic(CategoricalProbs):\n    """"""\n    A categorical distribution with ordered outcomes.\n\n    **References:**\n\n    1. *Stan Functions Reference, v2.20 section 12.6*,\n       Stan Development Team\n\n    :param numpy.ndarray predictor: prediction in real domain; typically this is output\n        of a linear model.\n    :param numpy.ndarray cutpoints: positions in real domain to separate categories.\n    """"""\n    arg_constraints = {\'predictor\': constraints.real,\n                       \'cutpoints\': constraints.ordered_vector}\n\n    def __init__(self, predictor, cutpoints, validate_args=None):\n        predictor, self.cutpoints = promote_shapes(np.expand_dims(predictor, -1), cutpoints)\n        self.predictor = predictor[..., 0]\n        cumulative_probs = expit(cutpoints - predictor)\n        # add two boundary points 0 and 1\n        pad_width = [(0, 0)] * (np.ndim(cumulative_probs) - 1) + [(1, 1)]\n        cumulative_probs = np.pad(cumulative_probs, pad_width, constant_values=(0, 1))\n        probs = cumulative_probs[..., 1:] - cumulative_probs[..., :-1]\n        super(OrderedLogistic, self).__init__(probs, validate_args=validate_args)\n\n\nclass PRNGIdentity(Distribution):\n    """"""\n    Distribution over :func:`~jax.random.PRNGKey`. This can be used to\n    draw a batch of :func:`~jax.random.PRNGKey` using the :class:`~numpyro.handlers.seed`\n    handler. Only `sample` method is supported.\n    """"""\n    is_discrete = True\n\n    def __init__(self):\n        super(PRNGIdentity, self).__init__(event_shape=(2,))\n\n    def sample(self, key, sample_shape=()):\n        return np.reshape(random.split(key, np.product(sample_shape).astype(np.int32)),\n                          sample_shape + self.event_shape)\n\n\n@copy_docs_from(Distribution)\nclass MultinomialProbs(Distribution):\n    arg_constraints = {\'total_count\': constraints.nonnegative_integer,\n                       \'probs\': constraints.simplex}\n    is_discrete = True\n\n    def __init__(self, probs, total_count=1, validate_args=None):\n        if np.ndim(probs) < 1:\n            raise ValueError(""`probs` parameter must be at least one-dimensional."")\n        batch_shape = lax.broadcast_shapes(np.shape(probs)[:-1], np.shape(total_count))\n        self.probs = promote_shapes(probs, shape=batch_shape + np.shape(probs)[-1:])[0]\n        self.total_count = promote_shapes(total_count, shape=batch_shape)[0]\n        super(MultinomialProbs, self).__init__(batch_shape=batch_shape,\n                                               event_shape=np.shape(self.probs)[-1:],\n                                               validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return multinomial(key, self.probs, self.total_count, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        return gammaln(self.total_count + 1) \\\n            + np.sum(xlogy(value, self.probs) - gammaln(value + 1), axis=-1)\n\n    @property\n    def mean(self):\n        return self.probs * np.expand_dims(self.total_count, -1)\n\n    @property\n    def variance(self):\n        return np.expand_dims(self.total_count, -1) * self.probs * (1 - self.probs)\n\n    @property\n    def support(self):\n        return constraints.multinomial(self.total_count)\n\n\n@copy_docs_from(Distribution)\nclass MultinomialLogits(Distribution):\n    arg_constraints = {\'total_count\': constraints.nonnegative_integer,\n                       \'logits\': constraints.real_vector}\n    is_discrete = True\n\n    def __init__(self, logits, total_count=1, validate_args=None):\n        if np.ndim(logits) < 1:\n            raise ValueError(""`logits` parameter must be at least one-dimensional."")\n        batch_shape = lax.broadcast_shapes(np.shape(logits)[:-1], np.shape(total_count))\n        self.logits = promote_shapes(logits, shape=batch_shape + np.shape(logits)[-1:])[0]\n        self.total_count = promote_shapes(total_count, shape=batch_shape)[0]\n        super(MultinomialLogits, self).__init__(batch_shape=batch_shape,\n                                                event_shape=np.shape(self.logits)[-1:],\n                                                validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return multinomial(key, self.probs, self.total_count, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        normalize_term = self.total_count * logsumexp(self.logits, axis=-1) \\\n            - gammaln(self.total_count + 1)\n        return np.sum(value * self.logits - gammaln(value + 1), axis=-1) - normalize_term\n\n    @lazy_property\n    def probs(self):\n        return _to_probs_multinom(self.logits)\n\n    @property\n    def mean(self):\n        return np.expand_dims(self.total_count, -1) * self.probs\n\n    @property\n    def variance(self):\n        return np.expand_dims(self.total_count, -1) * self.probs * (1 - self.probs)\n\n    @property\n    def support(self):\n        return constraints.multinomial(self.total_count)\n\n\ndef Multinomial(total_count=1, probs=None, logits=None, validate_args=None):\n    if probs is not None:\n        return MultinomialProbs(probs, total_count, validate_args=validate_args)\n    elif logits is not None:\n        return MultinomialLogits(logits, total_count, validate_args=validate_args)\n    else:\n        raise ValueError(\'One of `probs` or `logits` must be specified.\')\n\n\n@copy_docs_from(Distribution)\nclass Poisson(Distribution):\n    arg_constraints = {\'rate\': constraints.positive}\n    support = constraints.nonnegative_integer\n    is_discrete = True\n\n    def __init__(self, rate, validate_args=None):\n        self.rate = rate\n        super(Poisson, self).__init__(np.shape(rate), validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return poisson(key, self.rate, shape=sample_shape + self.batch_shape)\n\n    @validate_sample\n    def log_prob(self, value):\n        if self._validate_args:\n            self._validate_sample(value)\n        return (np.log(self.rate) * value) - gammaln(value + 1) - self.rate\n\n    @property\n    def mean(self):\n        return self.rate\n\n    @property\n    def variance(self):\n        return self.rate\n\n\nclass ZeroInflatedPoisson(Distribution):\n    """"""\n    A Zero Inflated Poisson distribution.\n\n    :param numpy.ndarray gate: probability of extra zeros.\n    :param numpy.ndarray rate: rate of Poisson distribution.\n    """"""\n    arg_constraints = {\'gate\': constraints.unit_interval, \'rate\': constraints.positive}\n    support = constraints.nonnegative_integer\n    is_discrete = True\n\n    def __init__(self, gate, rate=1., validate_args=None):\n        batch_shape = lax.broadcast_shapes(np.shape(gate), np.shape(rate))\n        self.gate, self.rate = promote_shapes(gate, rate)\n        super(ZeroInflatedPoisson, self).__init__(batch_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        key_bern, key_poisson = random.split(key)\n        shape = sample_shape + self.batch_shape\n        mask = random.bernoulli(key_bern, self.gate, shape)\n        samples = poisson(key_poisson, self.rate, shape)\n        return np.where(mask, 0, samples)\n\n    @validate_sample\n    def log_prob(self, value):\n        log_prob = np.log(self.rate) * value - gammaln(value + 1) + (np.log1p(-self.gate) - self.rate)\n        return np.where(value == 0, np.logaddexp(np.log(self.gate), log_prob), log_prob)\n\n    @lazy_property\n    def mean(self):\n        return (1 - self.gate) * self.rate\n\n    @lazy_property\n    def variance(self):\n        return (1 - self.gate) * self.rate * (1 + self.rate * self.gate)\n'"
numpyro/distributions/distribution.py,24,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# The implementation follows the design in PyTorch: torch.distributions.distribution.py\n#\n# Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)\n# Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\n# Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n# Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n# Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n# Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n# Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n# Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n# Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\nfrom collections import OrderedDict\nfrom contextlib import contextmanager\nimport warnings\n\nimport jax.numpy as np\nfrom jax import lax\n\nfrom numpyro.distributions.constraints import is_dependent, real\nfrom numpyro.distributions.transforms import Transform\nfrom numpyro.distributions.util import lazy_property, sum_rightmost, validate_sample\nfrom numpyro.util import not_jax_tracer\n\n_VALIDATION_ENABLED = False\n\n\ndef enable_validation(is_validate=True):\n    """"""\n    Enable or disable validation checks in NumPyro. Validation checks provide useful warnings and\n    errors, e.g. NaN checks, validating distribution arguments and support values, etc. which is\n    useful for debugging.\n\n    .. note:: This utility does not take effect under JAX\'s JIT compilation or vectorized\n        transformation :func:`jax.vmap`.\n\n    :param bool is_validate: whether to enable validation checks.\n    """"""\n    global _VALIDATION_ENABLED\n    _VALIDATION_ENABLED = is_validate\n    Distribution.set_default_validate_args(is_validate)\n\n\n@contextmanager\ndef validation_enabled(is_validate=True):\n    """"""\n    Context manager that is useful when temporarily enabling/disabling validation checks.\n\n    :param bool is_validate: whether to enable validation checks.\n    """"""\n    distribution_validation_status = _VALIDATION_ENABLED\n    try:\n        enable_validation(is_validate)\n        yield\n    finally:\n        enable_validation(distribution_validation_status)\n\n\nclass Distribution(object):\n    """"""\n    Base class for probability distributions in NumPyro. The design largely\n    follows from :mod:`torch.distributions`.\n\n    :param batch_shape: The batch shape for the distribution. This designates\n        independent (possibly non-identical) dimensions of a sample from the\n        distribution. This is fixed for a distribution instance and is inferred\n        from the shape of the distribution parameters.\n    :param event_shape: The event shape for the distribution. This designates\n        the dependent dimensions of a sample from the distribution. These are\n        collapsed when we evaluate the log probability density of a batch of\n        samples using `.log_prob`.\n    :param validate_args: Whether to enable validation of distribution\n        parameters and arguments to `.log_prob` method.\n\n    As an example:\n\n    .. doctest::\n\n       >>> import jax.numpy as np\n       >>> import numpyro.distributions as dist\n       >>> d = dist.Dirichlet(np.ones((2, 3, 4)))\n       >>> d.batch_shape\n       (2, 3)\n       >>> d.event_shape\n       (4,)\n    """"""\n    arg_constraints = {}\n    support = None\n    has_enumerate_support = False\n    is_discrete = False\n    reparametrized_params = []\n    _validate_args = False\n\n    @staticmethod\n    def set_default_validate_args(value):\n        if value not in [True, False]:\n            raise ValueError\n        Distribution._validate_args = value\n\n    def __init__(self, batch_shape=(), event_shape=(), validate_args=None):\n        self._batch_shape = batch_shape\n        self._event_shape = event_shape\n        if validate_args is not None:\n            self._validate_args = validate_args\n        if self._validate_args:\n            for param, constraint in self.arg_constraints.items():\n                if param not in self.__dict__ and isinstance(getattr(type(self), param), lazy_property):\n                    continue\n                if is_dependent(constraint):\n                    continue  # skip constraints that cannot be checked\n                is_valid = np.all(constraint(getattr(self, param)))\n                if not_jax_tracer(is_valid):\n                    if not is_valid:\n                        raise ValueError(""The parameter {} has invalid values"".format(param))\n        super(Distribution, self).__init__()\n\n    @property\n    def batch_shape(self):\n        """"""\n        Returns the shape over which the distribution parameters are batched.\n\n        :return: batch shape of the distribution.\n        :rtype: tuple\n        """"""\n        return self._batch_shape\n\n    @property\n    def event_shape(self):\n        """"""\n        Returns the shape of a single sample from the distribution without\n        batching.\n\n        :return: event shape of the distribution.\n        :rtype: tuple\n        """"""\n        return self._event_shape\n\n    @property\n    def event_dim(self):\n        """"""\n        :return: Number of dimensions of individual events.\n        :rtype: int\n        """"""\n        return len(self.event_shape)\n\n    def shape(self, sample_shape=()):\n        """"""\n        The tensor shape of samples from this distribution.\n\n        Samples are of shape::\n\n            d.shape(sample_shape) == sample_shape + d.batch_shape + d.event_shape\n\n        :param tuple sample_shape: the size of the iid batch to be drawn from the\n            distribution.\n        :return: shape of samples.\n        :rtype: tuple\n        """"""\n        return sample_shape + self.batch_shape + self.event_shape\n\n    def sample(self, key, sample_shape=()):\n        """"""\n        Returns a sample from the distribution having shape given by\n        `sample_shape + batch_shape + event_shape`. Note that when `sample_shape` is non-empty,\n        leading dimensions (of size `sample_shape`) of the returned sample will\n        be filled with iid draws from the distribution instance.\n\n        :param jax.random.PRNGKey key: the rng_key key to be used for the distribution.\n        :param tuple sample_shape: the sample shape for the distribution.\n        :return: an array of shape `sample_shape + batch_shape + event_shape`\n        :rtype: numpy.ndarray\n        """"""\n        raise NotImplementedError\n\n    def sample_with_intermediates(self, key, sample_shape=()):\n        """"""\n        Same as ``sample`` except that any intermediate computations are\n        returned (useful for `TransformedDistribution`).\n\n        :param jax.random.PRNGKey key: the rng_key key to be used for the distribution.\n        :param tuple sample_shape: the sample shape for the distribution.\n        :return: an array of shape `sample_shape + batch_shape + event_shape`\n        :rtype: numpy.ndarray\n        """"""\n        return self.sample(key, sample_shape=sample_shape), []\n\n    def log_prob(self, value):\n        """"""\n        Evaluates the log probability density for a batch of samples given by\n        `value`.\n\n        :param value: A batch of samples from the distribution.\n        :return: an array with shape `value.shape[:-self.event_shape]`\n        :rtype: numpy.ndarray\n        """"""\n        raise NotImplementedError\n\n    @property\n    def mean(self):\n        """"""\n        Mean of the distribution.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def variance(self):\n        """"""\n        Variance of the distribution.\n        """"""\n        raise NotImplementedError\n\n    def _validate_sample(self, value):\n        mask = self.support(value)\n        if not_jax_tracer(mask):\n            if not np.all(mask):\n                warnings.warn(\'Out-of-support values provided to log prob method. \'\n                              \'The value argument should be within the support.\')\n        return mask\n\n    def __call__(self, *args, **kwargs):\n        key = kwargs.pop(\'rng_key\')\n        sample_intermediates = kwargs.pop(\'sample_intermediates\', False)\n        if sample_intermediates:\n            return self.sample_with_intermediates(key, *args, **kwargs)\n        return self.sample(key, *args, **kwargs)\n\n    def to_event(self, reinterpreted_batch_ndims=None):\n        """"""\n        Interpret the rightmost `reinterpreted_batch_ndims` batch dimensions as\n        dependent event dimensions.\n\n        :param reinterpreted_batch_ndims: Number of rightmost batch dims to\n            interpret as event dims.\n        :return: An instance of `Independent` distribution.\n        :rtype: Independent\n        """"""\n        if reinterpreted_batch_ndims is None:\n            reinterpreted_batch_ndims = len(self.batch_shape)\n        elif reinterpreted_batch_ndims == 0:\n            return self\n        return Independent(self, reinterpreted_batch_ndims)\n\n    def enumerate_support(self, expand=True):\n        """"""\n        Returns an array with shape `len(support) x batch_shape`\n        containing all values in the support.\n        """"""\n        raise NotImplementedError\n\n    def expand(self, batch_shape):\n        """"""\n        Returns a new :class:`ExpandedDistribution` instance with batch\n        dimensions expanded to `batch_shape`.\n\n        :param tuple batch_shape: batch shape to expand to.\n        :return: an instance of `ExpandedDistribution`.\n        :rtype: :class:`ExpandedDistribution`\n        """"""\n        batch_shape = tuple(batch_shape)\n        if batch_shape == self.batch_shape:\n            return self\n        return ExpandedDistribution(self, batch_shape)\n\n    def expand_by(self, sample_shape):\n        """"""\n        Expands a distribution by adding ``sample_shape`` to the left side of\n        its :attr:`~numpyro.distributions.distribution.Distribution.batch_shape`.\n        To expand internal dims of ``self.batch_shape`` from 1 to something\n        larger, use :meth:`expand` instead.\n\n        :param tuple sample_shape: The size of the iid batch to be drawn\n            from the distribution.\n        :return: An expanded version of this distribution.\n        :rtype: :class:`ExpandedDistribution`\n        """"""\n        return self.expand(tuple(sample_shape) + self._batch_shape)\n\n    def mask(self, mask):\n        """"""\n        Masks a distribution by a boolean or boolean-valued array that is\n        broadcastable to the distributions\n        :attr:`Distribution.batch_shape` .\n\n        :param mask: A boolean or boolean valued array.\n        :type mask: bool or np.ndarray\n        :return: A masked copy of this distribution.\n        :rtype: :class:`MaskedDistribution`\n        """"""\n        if mask is True:\n            return self\n        return MaskedDistribution(self, mask)\n\n\nclass ExpandedDistribution(Distribution):\n    arg_constraints = {}\n\n    def __init__(self, base_dist, batch_shape=()):\n        if isinstance(base_dist, ExpandedDistribution):\n            batch_shape = self._broadcast_shape(base_dist.batch_shape, batch_shape)\n            base_dist = base_dist.base_dist\n        self.base_dist = base_dist\n        super().__init__(base_dist.batch_shape, base_dist.event_shape)\n        # adjust batch shape\n        self.expand(batch_shape)\n\n    def expand(self, batch_shape):\n        # Do basic validation. e.g. we should not ""unexpand"" distributions even if that is possible.\n        new_shape, _, _ = self._broadcast_shape(self.batch_shape, batch_shape)\n        # Record interstitial and expanded dims/sizes w.r.t. the base distribution\n        new_shape, expanded_sizes, interstitial_sizes = self._broadcast_shape(self.base_dist.batch_shape,\n                                                                              new_shape)\n        self._batch_shape = new_shape\n        self._expanded_sizes = expanded_sizes\n        self._interstitial_sizes = interstitial_sizes\n        return self\n\n    @staticmethod\n    def _broadcast_shape(existing_shape, new_shape):\n        if len(new_shape) < len(existing_shape):\n            raise ValueError(""Cannot broadcast distribution of shape {} to shape {}""\n                             .format(existing_shape, new_shape))\n        reversed_shape = list(reversed(existing_shape))\n        expanded_sizes, interstitial_sizes = [], []\n        for i, size in enumerate(reversed(new_shape)):\n            if i >= len(reversed_shape):\n                reversed_shape.append(size)\n                expanded_sizes.append((-i - 1, size))\n            elif reversed_shape[i] == 1:\n                if size != 1:\n                    reversed_shape[i] = size\n                    interstitial_sizes.append((-i - 1, size))\n            elif reversed_shape[i] != size:\n                raise ValueError(""Cannot broadcast distribution of shape {} to shape {}""\n                                 .format(existing_shape, new_shape))\n        return tuple(reversed(reversed_shape)), OrderedDict(expanded_sizes), OrderedDict(interstitial_sizes)\n\n    @property\n    def has_enumerate_support(self):\n        return self.base_dist.has_enumerate_support\n\n    @property\n    def is_discrete(self):\n        return self.base_dist.is_discrete\n\n    @property\n    def support(self):\n        return self.base_dist.support\n\n    def sample(self, key, sample_shape=()):\n        interstitial_dims = tuple(self._interstitial_sizes.keys())\n        event_dim = len(self.event_shape)\n        interstitial_dims = tuple(i - event_dim for i in interstitial_dims)\n        interstitial_sizes = tuple(self._interstitial_sizes.values())\n        expanded_sizes = tuple(self._expanded_sizes.values())\n        batch_shape = expanded_sizes + interstitial_sizes\n        samples = self.base_dist.sample(key, sample_shape + batch_shape)\n        interstitial_idx = len(sample_shape) + len(expanded_sizes)\n        interstitial_sample_dims = tuple(range(interstitial_idx, interstitial_idx + len(interstitial_sizes)))\n        for dim1, dim2 in zip(interstitial_dims, interstitial_sample_dims):\n            samples = np.swapaxes(samples, dim1, dim2)\n        return samples.reshape(sample_shape + self.batch_shape + self.event_shape)\n\n    def log_prob(self, value):\n        shape = lax.broadcast_shapes(self.batch_shape, np.shape(value)[:max(np.ndim(value) - len(self.event_shape), 0)])\n        log_prob = self.base_dist.log_prob(value)\n        return np.broadcast_to(log_prob, shape)\n\n    def enumerate_support(self, expand=True):\n        samples = self.base_dist.enumerate_support(expand=False)\n        enum_shape = samples.shape[:1]\n        samples = samples.reshape(enum_shape + (1,) * len(self.batch_shape))\n        if expand:\n            samples = samples.expand(enum_shape + self.batch_shape)\n        return samples\n\n    @property\n    def mean(self):\n        return np.broadcast_to(self.base_dist.mean, self.batch_shape + self.event_shape)\n\n    @property\n    def variance(self):\n        return np.broadcast_to(self.base_dist.variance, self.batch_shape + self.event_shape)\n\n\nclass ImproperUniform(Distribution):\n    """"""\n    A helper distribution with zero :meth:`log_prob` over the `support` domain.\n\n    .. note:: `sample` method is not implemented for this distribution. In autoguide and mcmc,\n        initial parameters for improper sites are derived from `init_to_uniform` or `init_to_value`\n        strategies.\n\n    **Usage:**\n\n    .. doctest::\n\n       >>> from numpyro import sample\n       >>> from numpyro.distributions import ImproperUniform, Normal, constraints\n       >>>\n       >>> def model():\n       ...     # ordered vector with length 10\n       ...     x = sample(\'x\', ImproperUniform(constraints.ordered_vector, (), event_shape=(10,)))\n       ...\n       ...     # real matrix with shape (3, 4)\n       ...     y = sample(\'y\', ImproperUniform(constraints.real, (), event_shape=(3, 4)))\n       ...\n       ...     # a shape-(6, 8) batch of length-5 vectors greater than 3\n       ...     z = sample(\'z\', ImproperUniform(constraints.greater_than(3), (6, 8), event_shape=(5,)))\n\n    If you want to set improper prior over all values greater than `a`, where `a` is\n    another random variable, you might use\n\n       >>> def model():\n       ...     a = sample(\'a\', Normal(0, 1))\n       ...     x = sample(\'x\', ImproperUniform(constraints.greater_than(a), (), event_shape=()))\n\n    or if you want to reparameterize it\n\n       >>> from numpyro.distributions import TransformedDistribution, transforms\n       >>> from numpyro.contrib.reparam import reparam, TransformReparam\n       >>>\n       >>> def model():\n       ...     a = sample(\'a\', Normal(0, 1))\n       ...     with reparam(config={\'x\': TransformReparam()}):\n       ...         x = sample(\'x\',\n       ...                    TransformedDistribution(ImproperUniform(constraints.positive, (), ()),\n       ...                                            transforms.AffineTransform(a, 1)))\n\n    :param ~numpyro.distributions.constraints.Constraint support: the support of this distribution.\n    :param tuple batch_shape: batch shape of this distribution. It is usually safe to\n        set `batch_shape=()`.\n    :param tuple event_shape: event shape of this distribution.\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, support, batch_shape, event_shape, validate_args=None):\n        self.support = support\n        super().__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    @validate_sample\n    def log_prob(self, value):\n        batch_shape = np.shape(value)[:np.ndim(value) - len(self.event_shape)]\n        batch_shape = lax.broadcast_shapes(batch_shape, self.batch_shape)\n        return np.zeros(batch_shape)\n\n    def _validate_sample(self, value):\n        mask = super(ImproperUniform, self)._validate_sample(value)\n        batch_dim = np.ndim(value) - len(self.event_shape)\n        if batch_dim < np.ndim(mask):\n            mask = np.all(np.reshape(mask, np.shape(mask)[:batch_dim] + (-1,)), -1)\n        return mask\n\n\nclass Independent(Distribution):\n    """"""\n    Reinterprets batch dimensions of a distribution as event dims by shifting\n    the batch-event dim boundary further to the left.\n\n    From a practical standpoint, this is useful when changing the result of\n    :meth:`log_prob`. For example, a univariate Normal distribution can be\n    interpreted as a multivariate Normal with diagonal covariance:\n\n    .. doctest::\n\n        >>> import numpyro.distributions as dist\n        >>> normal = dist.Normal(np.zeros(3), np.ones(3))\n        >>> [normal.batch_shape, normal.event_shape]\n        [(3,), ()]\n        >>> diag_normal = dist.Independent(normal, 1)\n        >>> [diag_normal.batch_shape, diag_normal.event_shape]\n        [(), (3,)]\n\n    :param numpyro.distribution.Distribution base_distribution: a distribution instance.\n    :param int reinterpreted_batch_ndims: the number of batch dims to reinterpret as event dims.\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, base_dist, reinterpreted_batch_ndims, validate_args=None):\n        if reinterpreted_batch_ndims > len(base_dist.batch_shape):\n            raise ValueError(""Expected reinterpreted_batch_ndims <= len(base_distribution.batch_shape), ""\n                             ""actual {} vs {}"".format(reinterpreted_batch_ndims,\n                                                      len(base_dist.batch_shape)))\n        shape = base_dist.batch_shape + base_dist.event_shape\n        event_dim = reinterpreted_batch_ndims + len(base_dist.event_shape)\n        batch_shape = shape[:len(shape) - event_dim]\n        event_shape = shape[len(shape) - event_dim:]\n        self.base_dist = base_dist\n        self.reinterpreted_batch_ndims = reinterpreted_batch_ndims\n        super(Independent, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    @property\n    def support(self):\n        return self.base_dist.support\n\n    @property\n    def has_enumerate_support(self):\n        return self.base_dist.has_enumerate_support\n\n    @property\n    def is_discrete(self):\n        return self.base_dist.is_discrete\n\n    @property\n    def reparameterized_params(self):\n        return self.base_dist.reparameterized_params\n\n    @property\n    def mean(self):\n        return self.base_dist.mean\n\n    @property\n    def variance(self):\n        return self.base_dist.variance\n\n    def sample(self, key, sample_shape=()):\n        return self.base_dist.sample(key, sample_shape=sample_shape)\n\n    def log_prob(self, value):\n        log_prob = self.base_dist.log_prob(value)\n        return sum_rightmost(log_prob, self.reinterpreted_batch_ndims)\n\n\nclass MaskedDistribution(Distribution):\n    """"""\n    Masks a distribution by a boolean array that is broadcastable to the\n    distribution\'s :attr:`Distribution.batch_shape`.\n    In the special case ``mask is False``, computation of :meth:`log_prob` , is skipped,\n    and constant zero values are returned instead.\n\n    :param mask: A boolean or boolean-valued array.\n    :type mask: np.ndarray or bool\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, base_dist, mask):\n        if isinstance(mask, bool):\n            self._mask = mask\n        else:\n            batch_shape = lax.broadcast_shapes(np.shape(mask), base_dist.batch_shape)\n            if mask.shape != batch_shape:\n                mask = np.broadcast_to(mask, batch_shape)\n            if base_dist.batch_shape != batch_shape:\n                base_dist = base_dist.expand(batch_shape)\n            self._mask = mask.astype(\'bool\')\n        self.base_dist = base_dist\n        super().__init__(base_dist.batch_shape, base_dist.event_shape)\n\n    @property\n    def has_enumerate_support(self):\n        return self.base_dist.has_enumerate_support\n\n    @property\n    def is_discrete(self):\n        return self.base_dist.is_discrete\n\n    @property\n    def support(self):\n        return self.base_dist.support\n\n    def sample(self, key, sample_shape=()):\n        return self.base_dist.sample(key, sample_shape)\n\n    def log_prob(self, value):\n        if self._mask is False:\n            shape = lax.broadcast_shapes(self.base_dist.batch_shape,\n                                         np.shape(value)[:max(np.ndim(value) - len(self.event_shape), 0)])\n            return np.zeros(shape)\n        if self._mask is True:\n            return self.base_dist.log_prob(value)\n        return self.base_dist.log_prob(value) * self._mask\n\n    def enumerate_support(self, expand=True):\n        return self.base_dist.enumerate_support(expand=expand)\n\n    @property\n    def mean(self):\n        return self.base_dist.mean\n\n    @property\n    def variance(self):\n        return self.base_dist.variance\n\n\nclass TransformedDistribution(Distribution):\n    """"""\n    Returns a distribution instance obtained as a result of applying\n    a sequence of transforms to a base distribution. For an example,\n    see :class:`~numpyro.distributions.LogNormal` and\n    :class:`~numpyro.distributions.HalfNormal`.\n\n    :param base_distribution: the base distribution over which to apply transforms.\n    :param transforms: a single transform or a list of transforms.\n    :param validate_args: Whether to enable validation of distribution\n        parameters and arguments to `.log_prob` method.\n    """"""\n    arg_constraints = {}\n\n    def __init__(self, base_distribution, transforms, validate_args=None):\n        if isinstance(transforms, Transform):\n            transforms = [transforms, ]\n        elif isinstance(transforms, list):\n            if not all(isinstance(t, Transform) for t in transforms):\n                raise ValueError(""transforms must be a Transform or a list of Transforms"")\n        else:\n            raise ValueError(""transforms must be a Transform or list, but was {}"".format(transforms))\n        # XXX: this logic will not be valid when IndependentDistribution is support;\n        # in that case, it is more involved to support Transform(Indep(Transform));\n        # however, we might not need to support such kind of distribution\n        # and should raise an error if base_distribution is an Indep one\n        if isinstance(base_distribution, TransformedDistribution):\n            self.base_dist = base_distribution.base_dist\n            self.transforms = base_distribution.transforms + transforms\n        else:\n            self.base_dist = base_distribution\n            self.transforms = transforms\n        # NB: here we assume that base_dist.shape == transformed_dist.shape\n        # but that might not be True for some transforms such as StickBreakingTransform\n        # because the event dimension is transformed from (n - 1,) to (n,).\n        # Currently, we have no mechanism to fix this issue. Given that\n        # this is just an edge case, we might skip this issue but need\n        # to pay attention to any inference function that inspects\n        # transformed distribution\'s shape.\n        shape = base_distribution.batch_shape + base_distribution.event_shape\n        event_dim = max([len(base_distribution.event_shape)] + [t.event_dim for t in transforms])\n        batch_shape = shape[:len(shape) - event_dim]\n        event_shape = shape[len(shape) - event_dim:]\n        super(TransformedDistribution, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    @property\n    def support(self):\n        domain = self.base_dist.support\n        for t in self.transforms:\n            t.domain = domain\n            domain = t.codomain\n        return domain\n\n    def sample(self, key, sample_shape=()):\n        x = self.base_dist.sample(key, sample_shape)\n        for transform in self.transforms:\n            x = transform(x)\n        return x\n\n    def sample_with_intermediates(self, key, sample_shape=()):\n        x = self.base_dist.sample(key, sample_shape)\n        intermediates = []\n        for transform in self.transforms:\n            x_tmp = x\n            x, t_inter = transform.call_with_intermediates(x)\n            intermediates.append([x_tmp, t_inter])\n        return x, intermediates\n\n    @validate_sample\n    def log_prob(self, value, intermediates=None):\n        if intermediates is not None:\n            if len(intermediates) != len(self.transforms):\n                raise ValueError(\'Intermediates array has length = {}. Expected = {}.\'\n                                 .format(len(intermediates), len(self.transforms)))\n        event_dim = len(self.event_shape)\n        log_prob = 0.0\n        y = value\n        for i, transform in enumerate(reversed(self.transforms)):\n            x = transform.inv(y) if intermediates is None else intermediates[-i - 1][0]\n            t_inter = None if intermediates is None else intermediates[-i - 1][1]\n            t_log_det = transform.log_abs_det_jacobian(x, y, t_inter)\n            log_prob = log_prob - sum_rightmost(t_log_det, event_dim - transform.event_dim)\n            y = x\n\n        log_prob = log_prob + sum_rightmost(self.base_dist.log_prob(y),\n                                            event_dim - len(self.base_dist.event_shape))\n        return log_prob\n\n    @property\n    def mean(self):\n        raise NotImplementedError\n\n    @property\n    def variance(self):\n        raise NotImplementedError\n\n\nclass Unit(Distribution):\n    """"""\n    Trivial nonnormalized distribution representing the unit type.\n\n    The unit type has a single value with no data, i.e. ``value.size == 0``.\n\n    This is used for :func:`numpyro.factor` statements.\n    """"""\n    arg_constraints = {\'log_factor\': real}\n    support = real\n\n    def __init__(self, log_factor, validate_args=None):\n        batch_shape = np.shape(log_factor)\n        event_shape = (0,)  # This satisfies .size == 0.\n        self.log_factor = log_factor\n        super(Unit, self).__init__(batch_shape, event_shape, validate_args=validate_args)\n\n    def sample(self, key, sample_shape=()):\n        return np.empty(sample_shape + self.batch_shape + self.event_shape)\n\n    def log_prob(self, value):\n        shape = lax.broadcast_shapes(self.batch_shape, np.shape(value)[:-1])\n        return np.broadcast_to(self.log_factor, shape)\n'"
numpyro/distributions/flows.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom jax import lax\nimport jax.numpy as np\n\nfrom numpyro.distributions.constraints import real_vector\nfrom numpyro.distributions.transforms import Transform\nfrom numpyro.util import fori_loop\n\n\ndef _clamp_preserve_gradients(x, min, max):\n    return x + lax.stop_gradient(np.clip(x, a_min=min, a_max=max) - x)\n\n\n# adapted from https://github.com/pyro-ppl/pyro/blob/dev/pyro/distributions/transforms/iaf.py\nclass InverseAutoregressiveTransform(Transform):\n    """"""\n    An implementation of Inverse Autoregressive Flow, using Eq (10) from Kingma et al., 2016,\n\n        :math:`\\\\mathbf{y} = \\\\mu_t + \\\\sigma_t\\\\odot\\\\mathbf{x}`\n\n    where :math:`\\\\mathbf{x}` are the inputs, :math:`\\\\mathbf{y}` are the outputs, :math:`\\\\mu_t,\\\\sigma_t`\n    are calculated from an autoregressive network on :math:`\\\\mathbf{x}`, and :math:`\\\\sigma_t>0`.\n\n    **References**\n\n    1. *Improving Variational Inference with Inverse Autoregressive Flow* [arXiv:1606.04934],\n       Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, Max Welling\n    """"""\n    domain = real_vector\n    codomain = real_vector\n    event_dim = 1\n\n    def __init__(self, autoregressive_nn, log_scale_min_clip=-5., log_scale_max_clip=3.):\n        """"""\n        :param autoregressive_nn: an autoregressive neural network whose forward call returns a real-valued\n            mean and log scale as a tuple\n        """"""\n        self.arn = autoregressive_nn\n        self.log_scale_min_clip = log_scale_min_clip\n        self.log_scale_max_clip = log_scale_max_clip\n\n    def __call__(self, x):\n        """"""\n        :param numpy.ndarray x: the input into the transform\n        """"""\n        return self.call_with_intermediates(x)[0]\n\n    def call_with_intermediates(self, x):\n        mean, log_scale = self.arn(x)\n        log_scale = _clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)\n        scale = np.exp(log_scale)\n        return scale * x + mean, log_scale\n\n    def inv(self, y):\n        """"""\n        :param numpy.ndarray y: the output of the transform to be inverted\n        """"""\n        # NOTE: Inversion is an expensive operation that scales in the dimension of the input\n        def _update_x(i, x):\n            mean, log_scale = self.arn(x)\n            inverse_scale = np.exp(-_clamp_preserve_gradients(\n                log_scale, min=self.log_scale_min_clip, max=self.log_scale_max_clip))\n            x = (y - mean) * inverse_scale\n            return x\n\n        x = fori_loop(0, y.shape[-1], _update_x, np.zeros(y.shape))\n        return x\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        """"""\n        Calculates the elementwise determinant of the log jacobian.\n\n        :param numpy.ndarray x: the input to the transform\n        :param numpy.ndarray y: the output of the transform\n        """"""\n        if intermediates is None:\n            log_scale = self.arn(x)[1]\n            log_scale = _clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)\n            return log_scale.sum(-1)\n        else:\n            log_scale = intermediates\n            return log_scale.sum(-1)\n\n\nclass BlockNeuralAutoregressiveTransform(Transform):\n    """"""\n    An implementation of Block Neural Autoregressive flow.\n\n    **References**\n\n    1. *Block Neural Autoregressive Flow*,\n       Nicola De Cao, Ivan Titov, Wilker Aziz\n    """"""\n    event_dim = 1\n\n    def __init__(self, bn_arn):\n        self.bn_arn = bn_arn\n\n    def __call__(self, x):\n        """"""\n        :param numpy.ndarray x: the input into the transform\n        """"""\n        return self.call_with_intermediates(x)[0]\n\n    def call_with_intermediates(self, x):\n        y, logdet = self.bn_arn(x)\n        return y, logdet\n\n    def inv(self, y):\n        raise NotImplementedError(""Block neural autoregressive transform does not have an analytic""\n                                  "" inverse implemented."")\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        """"""\n        Calculates the elementwise determinant of the log jacobian.\n\n        :param numpy.ndarray x: the input to the transform\n        :param numpy.ndarray y: the output of the transform\n        """"""\n        if intermediates is None:\n            logdet = self.bn_arn(x)[1]\n            return logdet.sum(-1)\n        else:\n            logdet = intermediates\n            return logdet.sum(-1)\n'"
numpyro/distributions/transforms.py,59,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport math\nimport warnings\n\nfrom jax import ops, tree_flatten, tree_map, vmap\nfrom jax.dtypes import canonicalize_dtype\nfrom jax.flatten_util import ravel_pytree\nfrom jax.nn import softplus\nimport jax.numpy as np\nfrom jax.scipy.linalg import solve_triangular\nfrom jax.scipy.special import expit, logit\n\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.util import (\n    get_dtype,\n    matrix_to_tril_vec,\n    signed_stick_breaking_tril,\n    sum_rightmost,\n    vec_to_tril_matrix\n)\n\n__all__ = [\n    \'biject_to\',\n    \'AbsTransform\',\n    \'AffineTransform\',\n    \'ComposeTransform\',\n    \'CorrCholeskyTransform\',\n    \'ExpTransform\',\n    \'IdentityTransform\',\n    \'InvCholeskyTransform\',\n    \'LowerCholeskyTransform\',\n    \'MultivariateAffineTransform\',\n    \'PermuteTransform\',\n    \'PowerTransform\',\n    \'SigmoidTransform\',\n    \'StickBreakingTransform\',\n    \'Transform\',\n    \'UnpackTransform\',\n]\n\n\ndef _clipped_expit(x):\n    finfo = np.finfo(get_dtype(x))\n    return np.clip(expit(x), a_min=finfo.tiny, a_max=1. - finfo.eps)\n\n\nclass Transform(object):\n    domain = constraints.real\n    codomain = constraints.real\n    event_dim = 0\n\n    def __call__(self, x):\n        return NotImplementedError\n\n    def inv(self, y):\n        raise NotImplementedError\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        raise NotImplementedError\n\n    def call_with_intermediates(self, x):\n        return self(x), None\n\n\nclass AbsTransform(Transform):\n    domain = constraints.real\n    codomain = constraints.positive\n\n    def __eq__(self, other):\n        return isinstance(other, AbsTransform)\n\n    def __call__(self, x):\n        return np.abs(x)\n\n    def inv(self, y):\n        return y\n\n\nclass AffineTransform(Transform):\n    # TODO: currently, just support scale > 0\n    def __init__(self, loc, scale, domain=constraints.real):\n        self.loc = loc\n        self.scale = scale\n        self.domain = domain\n\n    @property\n    def codomain(self):\n        if self.domain is constraints.real:\n            return constraints.real\n        elif self.domain is constraints.real_vector:\n            return constraints.real_vector\n        elif isinstance(self.domain, constraints.greater_than):\n            return constraints.greater_than(self.__call__(self.domain.lower_bound))\n        elif isinstance(self.domain, constraints.interval):\n            return constraints.interval(self.__call__(self.domain.lower_bound),\n                                        self.__call__(self.domain.upper_bound))\n        else:\n            raise NotImplementedError\n\n    @property\n    def event_dim(self):\n        return 1 if self.domain is constraints.real_vector else 0\n\n    def __call__(self, x):\n        return self.loc + self.scale * x\n\n    def inv(self, y):\n        return (y - self.loc) / self.scale\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        return sum_rightmost(np.broadcast_to(np.log(np.abs(self.scale)), np.shape(x)), self.event_dim)\n\n\nclass ComposeTransform(Transform):\n    def __init__(self, parts):\n        self.parts = parts\n\n    @property\n    def domain(self):\n        return self.parts[0].domain\n\n    @property\n    def codomain(self):\n        return self.parts[-1].codomain\n\n    @property\n    def event_dim(self):\n        return max(p.event_dim for p in self.parts)\n\n    def __call__(self, x):\n        for part in self.parts:\n            x = part(x)\n        return x\n\n    def inv(self, y):\n        for part in self.parts[::-1]:\n            y = part.inv(y)\n        return y\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        if intermediates is not None:\n            if len(intermediates) != len(self.parts):\n                raise ValueError(\'Intermediates array has length = {}. Expected = {}.\'\n                                 .format(len(intermediates), len(self.parts)))\n\n        result = 0.\n        for i, part in enumerate(self.parts[:-1]):\n            y_tmp = part(x) if intermediates is None else intermediates[i][0]\n            inter = None if intermediates is None else intermediates[i][1]\n            logdet = part.log_abs_det_jacobian(x, y_tmp, intermediates=inter)\n            result = result + sum_rightmost(logdet, self.event_dim - part.event_dim)\n            x = y_tmp\n        # account the the last transform, where y is available\n        inter = None if intermediates is None else intermediates[-1]\n        logdet = self.parts[-1].log_abs_det_jacobian(x, y, intermediates=inter)\n        result = result + sum_rightmost(logdet, self.event_dim - self.parts[-1].event_dim)\n        return result\n\n    def call_with_intermediates(self, x):\n        intermediates = []\n        for part in self.parts[:-1]:\n            x, inter = part.call_with_intermediates(x)\n            intermediates.append([x, inter])\n        # NB: we don\'t need to hold the last output value in `intermediates`\n        x, inter = self.parts[-1].call_with_intermediates(x)\n        intermediates.append(inter)\n        return x, intermediates\n\n\nclass CorrCholeskyTransform(Transform):\n    r""""""\n    Transforms a uncontrained real vector :math:`x` with length :math:`D*(D-1)/2` into the\n    Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower\n    triangular matrix with positive diagonals and unit Euclidean norm for each row.\n    The transform is processed as follows:\n\n        1. First we convert :math:`x` into a lower triangular matrix with the following order:\n\n        .. math::\n            \\begin{bmatrix}\n                1   & 0 & 0 & 0 \\\\\n                x_0 & 1 & 0 & 0 \\\\\n                x_1 & x_2 & 1 & 0 \\\\\n                x_3 & x_4 & x_5 & 1\n            \\end{bmatrix}\n\n        2. For each row :math:`X_i` of the lower triangular part, we apply a *signed* version of\n        class :class:`StickBreakingTransform` to transform :math:`X_i` into a\n        unit Euclidean length vector using the following steps:\n\n            a. Scales into the interval :math:`(-1, 1)` domain: :math:`r_i = \\tanh(X_i)`.\n            b. Transforms into an unsigned domain: :math:`z_i = r_i^2`.\n            c. Applies :math:`s_i = StickBreakingTransform(z_i)`.\n            d. Transforms back into signed domain: :math:`y_i = (sign(r_i), 1) * \\sqrt{s_i}`.\n    """"""\n    domain = constraints.real_vector\n    codomain = constraints.corr_cholesky\n    event_dim = 2\n\n    def __call__(self, x):\n        # we interchange step 1 and step 2.a for a better performance\n        t = np.tanh(x)\n        return signed_stick_breaking_tril(t)\n\n    def inv(self, y):\n        # inverse stick-breaking\n        z1m_cumprod = 1 - np.cumsum(y * y, axis=-1)\n        pad_width = [(0, 0)] * y.ndim\n        pad_width[-1] = (1, 0)\n        z1m_cumprod_shifted = np.pad(z1m_cumprod[..., :-1], pad_width,\n                                     mode=""constant"", constant_values=1.)\n        t = matrix_to_tril_vec(y, diagonal=-1) / np.sqrt(\n            matrix_to_tril_vec(z1m_cumprod_shifted, diagonal=-1))\n        # inverse of tanh\n        x = np.log((1 + t) / (1 - t)) / 2\n        return x\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        # NB: because domain and codomain are two spaces with different dimensions, determinant of\n        # Jacobian is not well-defined. Here we return `log_abs_det_jacobian` of `x` and the\n        # flatten lower triangular part of `y`.\n\n        # stick_breaking_logdet = log(y / r) = log(z_cumprod)  (modulo right shifted)\n        z1m_cumprod = 1 - np.cumsum(y * y, axis=-1)\n        # by taking diagonal=-2, we don\'t need to shift z_cumprod to the right\n        # NB: diagonal=-2 works fine for (2 x 2) matrix, where we get an empty array\n        z1m_cumprod_tril = matrix_to_tril_vec(z1m_cumprod, diagonal=-2)\n        stick_breaking_logdet = 0.5 * np.sum(np.log(z1m_cumprod_tril), axis=-1)\n\n        tanh_logdet = -2 * np.sum(x + softplus(-2 * x) - np.log(2.), axis=-1)\n        return stick_breaking_logdet + tanh_logdet\n\n\nclass ExpTransform(Transform):\n    # TODO: refine domain/codomain logic through setters, especially when\n    # transforms for inverses are supported\n    def __init__(self, domain=constraints.real):\n        self.domain = domain\n\n    @property\n    def codomain(self):\n        if self.domain is constraints.real:\n            return constraints.positive\n        elif isinstance(self.domain, constraints.greater_than):\n            return constraints.greater_than(self.__call__(self.domain.lower_bound))\n        elif isinstance(self.domain, constraints.interval):\n            return constraints.interval(self.__call__(self.domain.lower_bound),\n                                        self.__call__(self.domain.upper_bound))\n        else:\n            raise NotImplementedError\n\n    def __call__(self, x):\n        # XXX consider to clamp from below for stability if necessary\n        return np.exp(x)\n\n    def inv(self, y):\n        return np.log(y)\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        return x\n\n\nclass IdentityTransform(Transform):\n\n    def __init__(self, event_dim=0):\n        self.event_dim = event_dim\n\n    def __call__(self, x):\n        return x\n\n    def inv(self, y):\n        return y\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        return np.full(np.shape(x) if self.event_dim == 0 else np.shape(x)[:-1], 0.)\n\n\nclass InvCholeskyTransform(Transform):\n    r""""""\n    Transform via the mapping :math:`y = x @ x.T`, where `x` is a lower\n    triangular matrix with positive diagonal.\n    """"""\n    event_dim = 2\n\n    def __init__(self, domain=constraints.lower_cholesky):\n        assert domain in [constraints.lower_cholesky, constraints.corr_cholesky]\n        self.domain = domain\n\n    @property\n    def codomain(self):\n        if self.domain is constraints.lower_cholesky:\n            return constraints.positive_definite\n        elif self.domain:\n            return constraints.corr_matrix\n\n    def __call__(self, x):\n        return np.matmul(x, np.swapaxes(x, -2, -1))\n\n    def inv(self, y):\n        return np.linalg.cholesky(y)\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        if self.domain is constraints.lower_cholesky:\n            # Ref: http://web.mit.edu/18.325/www/handouts/handout2.pdf page 13\n            n = np.shape(x)[-1]\n            order = np.arange(n, 0, -1)\n            return n * np.log(2) + np.sum(order * np.log(np.diagonal(x, axis1=-2, axis2=-1)), axis=-1)\n        else:\n            # NB: see derivation in LKJCholesky implementation\n            n = np.shape(x)[-1]\n            order = np.arange(n - 1, -1, -1)\n            return np.sum(order * np.log(np.diagonal(x, axis1=-2, axis2=-1)), axis=-1)\n\n\nclass LowerCholeskyTransform(Transform):\n    domain = constraints.real_vector\n    codomain = constraints.lower_cholesky\n    event_dim = 2\n\n    def __call__(self, x):\n        n = round((math.sqrt(1 + 8 * x.shape[-1]) - 1) / 2)\n        z = vec_to_tril_matrix(x[..., :-n], diagonal=-1)\n        diag = np.exp(x[..., -n:])\n        return z + np.expand_dims(diag, axis=-1) * np.identity(n)\n\n    def inv(self, y):\n        z = matrix_to_tril_vec(y, diagonal=-1)\n        return np.concatenate([z, np.log(np.diagonal(y, axis1=-2, axis2=-1))], axis=-1)\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        # the jacobian is diagonal, so logdet is the sum of diagonal `exp` transform\n        n = round((math.sqrt(1 + 8 * x.shape[-1]) - 1) / 2)\n        return x[..., -n:].sum(-1)\n\n\nclass MultivariateAffineTransform(Transform):\n    r""""""\n    Transform via the mapping :math:`y = loc + scale\\_tril\\ @\\ x`.\n\n    :param loc: a real vector.\n    :param scale_tril: a lower triangular matrix with positive diagonal.\n    """"""\n    domain = constraints.real_vector\n    codomain = constraints.real_vector\n    event_dim = 1\n\n    def __init__(self, loc, scale_tril):\n        # TODO: relax this condition per user request\n        if np.ndim(scale_tril) != 2:\n            raise ValueError(""Only support 2-dimensional scale_tril matrix."")\n        self.loc = loc\n        self.scale_tril = scale_tril\n\n    def __call__(self, x):\n        return self.loc + np.squeeze(np.matmul(self.scale_tril, x[..., np.newaxis]), axis=-1)\n\n    def inv(self, y):\n        y = y - self.loc\n        original_shape = np.shape(y)\n        yt = np.reshape(y, (-1, original_shape[-1])).T\n        xt = solve_triangular(self.scale_tril, yt, lower=True)\n        return np.reshape(xt.T, original_shape)\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        return np.broadcast_to(np.log(np.diagonal(self.scale_tril, axis1=-2, axis2=-1)).sum(-1),\n                               np.shape(x)[:-1])\n\n\nclass OrderedTransform(Transform):\n    """"""\n    Transform a real vector to an ordered vector.\n\n    **References:**\n\n    1. *Stan Reference Manual v2.20, section 10.6*,\n       Stan Development Team\n    """"""\n    domain = constraints.real_vector\n    codomain = constraints.ordered_vector\n    event_dim = 1\n\n    def __call__(self, x):\n        z = np.concatenate([x[..., :1], np.exp(x[..., 1:])], axis=-1)\n        return np.cumsum(z, axis=-1)\n\n    def inv(self, y):\n        x = np.log(y[..., 1:] - y[..., :-1])\n        return np.concatenate([y[..., :1], x], axis=-1)\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        return np.sum(x[..., 1:], -1)\n\n\nclass PermuteTransform(Transform):\n    domain = constraints.real_vector\n    codomain = constraints.real_vector\n    event_dim = 1\n\n    def __init__(self, permutation):\n        self.permutation = permutation\n\n    def __call__(self, x):\n        return x[..., self.permutation]\n\n    def inv(self, y):\n        size = self.permutation.size\n        permutation_inv = ops.index_update(np.zeros(size, dtype=canonicalize_dtype(np.int64)),\n                                           self.permutation,\n                                           np.arange(size))\n        return y[..., permutation_inv]\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        return np.full(np.shape(x)[:-1], 0.)\n\n\nclass PowerTransform(Transform):\n    domain = constraints.positive\n    codomain = constraints.positive\n\n    def __init__(self, exponent):\n        self.exponent = exponent\n\n    def __call__(self, x):\n        return np.power(x, self.exponent)\n\n    def inv(self, y):\n        return np.power(y, 1 / self.exponent)\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        return np.log(np.abs(self.exponent * y / x))\n\n\nclass SigmoidTransform(Transform):\n    codomain = constraints.unit_interval\n\n    def __call__(self, x):\n        return _clipped_expit(x)\n\n    def inv(self, y):\n        return logit(y)\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        x_abs = np.abs(x)\n        return -x_abs - 2 * np.log1p(np.exp(-x_abs))\n\n\nclass StickBreakingTransform(Transform):\n    domain = constraints.real_vector\n    codomain = constraints.simplex\n    event_dim = 1\n\n    def __call__(self, x):\n        # we shift x to obtain a balanced mapping (0, 0, ..., 0) -> (1/K, 1/K, ..., 1/K)\n        x = x - np.log(x.shape[-1] - np.arange(x.shape[-1]))\n        # convert to probabilities (relative to the remaining) of each fraction of the stick\n        z = _clipped_expit(x)\n        z1m_cumprod = np.cumprod(1 - z, axis=-1)\n        pad_width = [(0, 0)] * x.ndim\n        pad_width[-1] = (0, 1)\n        z_padded = np.pad(z, pad_width, mode=""constant"", constant_values=1.)\n        pad_width = [(0, 0)] * x.ndim\n        pad_width[-1] = (1, 0)\n        z1m_cumprod_shifted = np.pad(z1m_cumprod, pad_width, mode=""constant"", constant_values=1.)\n        return z_padded * z1m_cumprod_shifted\n\n    def inv(self, y):\n        y_crop = y[..., :-1]\n        z1m_cumprod = np.clip(1 - np.cumsum(y_crop, axis=-1), a_min=np.finfo(y.dtype).tiny)\n        # hence x = logit(z) = log(z / (1 - z)) = y[::-1] / z1m_cumprod\n        x = np.log(y_crop / z1m_cumprod)\n        return x + np.log(x.shape[-1] - np.arange(x.shape[-1]))\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        # Ref: https://mc-stan.org/docs/2_19/reference-manual/simplex-transform-section.html\n        # |det|(J) = Product(y * (1 - z))\n        x = x - np.log(x.shape[-1] - np.arange(x.shape[-1]))\n        z = np.clip(expit(x), a_min=np.finfo(x.dtype).tiny)\n        # XXX we use the identity 1 - z = z * exp(-x) to not worry about\n        # the case z ~ 1\n        return np.sum(np.log(y[..., :-1] * z) - x, axis=-1)\n\n\nclass UnpackTransform(Transform):\n    """"""\n    Transforms a contiguous array to a pytree of subarrays.\n\n    :param unpack_fn: callable used to unpack a contiguous array.\n    """"""\n    domain = constraints.real_vector\n    event_dim = 1\n\n    def __init__(self, unpack_fn):\n        self.unpack_fn = unpack_fn\n\n    def __call__(self, x):\n        batch_shape = x.shape[:-1]\n        if batch_shape:\n            unpacked = vmap(self.unpack_fn)(x.reshape((-1,) + x.shape[-1:]))\n            return tree_map(lambda z: np.reshape(z, batch_shape + z.shape[1:]), unpacked)\n        else:\n            return self.unpack_fn(x)\n\n    def inv(self, y):\n        leading_dims = [v.shape[0] if np.ndim(v) > 0 else 0\n                        for v in tree_flatten(y)[0]]\n        d0 = leading_dims[0]\n        not_scalar = d0 > 0 or len(leading_dims) > 1\n        if not_scalar and all(d == d0 for d in leading_dims[1:]):\n            warnings.warn(""UnpackTransform.inv might lead to an unexpected behavior because it""\n                          "" cannot transform a batch of unpacked arrays."")\n        return ravel_pytree(y)[0]\n\n    def log_abs_det_jacobian(self, x, y, intermediates=None):\n        return np.zeros(np.shape(x)[:-1])\n\n\n##########################################################\n# CONSTRAINT_REGISTRY\n##########################################################\n\nclass ConstraintRegistry(object):\n    def __init__(self):\n        self._registry = {}\n\n    def register(self, constraint, factory=None):\n        if factory is None:\n            return lambda factory: self.register(constraint, factory)\n\n        if isinstance(constraint, constraints.Constraint):\n            constraint = type(constraint)\n\n        self._registry[constraint] = factory\n\n    def __call__(self, constraint):\n        try:\n            factory = self._registry[type(constraint)]\n        except KeyError:\n            raise NotImplementedError\n\n        return factory(constraint)\n\n\nbiject_to = ConstraintRegistry()\n\n\n@biject_to.register(constraints.corr_cholesky)\ndef _transform_to_corr_cholesky(constraint):\n    return CorrCholeskyTransform()\n\n\n@biject_to.register(constraints.corr_matrix)\ndef _transform_to_corr_matrix(constraint):\n    return ComposeTransform([CorrCholeskyTransform(), InvCholeskyTransform(domain=constraints.corr_cholesky)])\n\n\n@biject_to.register(constraints.greater_than)\ndef _transform_to_greater_than(constraint):\n    if constraint is constraints.positive:\n        return ExpTransform()\n    return ComposeTransform([ExpTransform(),\n                             AffineTransform(constraint.lower_bound, 1,\n                                             domain=constraints.positive)])\n\n\n@biject_to.register(constraints.interval)\ndef _transform_to_interval(constraint):\n    if constraint is constraints.unit_interval:\n        return SigmoidTransform()\n    scale = constraint.upper_bound - constraint.lower_bound\n    return ComposeTransform([SigmoidTransform(),\n                             AffineTransform(constraint.lower_bound, scale,\n                                             domain=constraints.unit_interval)])\n\n\n@biject_to.register(constraints.lower_cholesky)\ndef _transform_to_lower_cholesky(constraint):\n    return LowerCholeskyTransform()\n\n\n@biject_to.register(constraints.ordered_vector)\ndef _transform_to_ordered_vector(constraint):\n    return OrderedTransform()\n\n\n@biject_to.register(constraints.positive_definite)\ndef _transform_to_positive_definite(constraint):\n    return ComposeTransform([LowerCholeskyTransform(), InvCholeskyTransform()])\n\n\n@biject_to.register(constraints.real)\ndef _transform_to_real(constraint):\n    return IdentityTransform()\n\n\n@biject_to.register(constraints.real_vector)\ndef _transform_to_real_vector(constraint):\n    return IdentityTransform(event_dim=1)\n\n\n@biject_to.register(constraints.simplex)\ndef _transform_to_simplex(constraint):\n    return StickBreakingTransform()\n'"
numpyro/distributions/util.py,90,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\nfrom collections import namedtuple\nfrom functools import update_wrapper\nimport math\n\nfrom jax import jit, lax, random, vmap\nfrom jax.dtypes import canonicalize_dtype\nfrom jax.lib import xla_bridge\nimport jax.numpy as np\nfrom jax.scipy.linalg import solve_triangular\nfrom jax.scipy.special import gammaln\nfrom jax.util import partial\n\n\n# Parameters for Transformed Rejection with Squeeze (TRS) algorithm - page 3.\n_tr_params = namedtuple(\'tr_params\', [\'c\', \'b\', \'a\', \'alpha\', \'u_r\', \'v_r\', \'m\', \'log_p\', \'log1_p\', \'log_h\'])\n\n\ndef _get_tr_params(n, p):\n    # See Table 1. Additionally, we pre-compute log(p), log1(-p) and the\n    # constant terms, that depend only on (n, p, m) in log(f(k)) (bottom of page 5).\n    mu = n * p\n    spq = np.sqrt(mu * (1 - p))\n    c = mu + 0.5\n    b = 1.15 + 2.53 * spq\n    a = -0.0873 + 0.0248 * b + 0.01 * p\n    alpha = (2.83 + 5.1 / b) * spq\n    u_r = 0.43\n    v_r = 0.92 - 4.2 / b\n    m = np.floor((n + 1) * p).astype(n.dtype)\n    log_p = np.log(p)\n    log1_p = np.log1p(-p)\n    log_h = (m + 0.5) * (np.log((m + 1.) / (n - m + 1.)) + log1_p - log_p) + \\\n            (stirling_approx_tail(m) + stirling_approx_tail(n - m))\n    return _tr_params(c, b, a, alpha, u_r, v_r, m, log_p, log1_p, log_h)\n\n\ndef stirling_approx_tail(k):\n    precomputed = np.array([\n        0.08106146679532726,\n        0.04134069595540929,\n        0.02767792568499834,\n        0.02079067210376509,\n        0.01664469118982119,\n        0.01387612882307075,\n        0.01189670994589177,\n        0.01041126526197209,\n        0.009255462182712733,\n        0.008330563433362871,\n    ])\n    kp1 = k + 1\n    kp1sq = (k + 1) ** 2\n    return np.where(k < 10,\n                    precomputed[k],\n                    (1. / 12 - (1. / 360 - (1. / 1260) / kp1sq) / kp1sq) / kp1)\n\n\ndef _binomial_btrs(key, p, n):\n    """"""\n    Based on the transformed rejection sampling algorithm (BTRS) from the\n    following reference:\n\n    Hormann, ""The Generation of Binonmial Random Variates""\n    (https://core.ac.uk/download/pdf/11007254.pdf)\n    """"""\n    def _btrs_body_fn(val):\n        _, key, _, _ = val\n        key, key_u, key_v = random.split(key, 3)\n        u = random.uniform(key_u)\n        v = random.uniform(key_v)\n        u = u - 0.5\n        k = np.floor((2 * tr_params.a / (0.5 - np.abs(u)) + tr_params.b) * u + tr_params.c).astype(n.dtype)\n        return k, key, u, v\n\n    def _btrs_cond_fn(val):\n        def accept_fn(k, u, v):\n            # See acceptance condition in Step 3. (Page 3) of TRS algorithm\n            # v <= f(k) * g_grad(u) / alpha\n\n            m = tr_params.m\n            log_p = tr_params.log_p\n            log1_p = tr_params.log1_p\n            # See: formula for log(f(k)) at bottom of Page 5.\n            log_f = (n + 1.) * np.log((n - m + 1.) / (n - k + 1.)) + \\\n                    (k + 0.5) * (np.log((n - k + 1.) / (k + 1.)) + log_p - log1_p) + \\\n                    (stirling_approx_tail(k) - stirling_approx_tail(n - k)) + tr_params.log_h\n            g = (tr_params.a / (0.5 - np.abs(u)) ** 2) + tr_params.b\n            return np.log((v * tr_params.alpha) / g) <= log_f\n\n        k, key, u, v = val\n        early_accept = (np.abs(u) <= tr_params.u_r) & (v <= tr_params.v_r)\n        early_reject = (k < 0) | (k > n)\n        return lax.cond(early_accept | early_reject,\n                        (),\n                        lambda _: ~early_accept,\n                        (k, u, v),\n                        lambda x: ~accept_fn(*x))\n\n    tr_params = _get_tr_params(n, p)\n    ret = lax.while_loop(_btrs_cond_fn, _btrs_body_fn,\n                         (-1, key, 1., 1.))  # use k=-1 initially so that cond_fn returns True\n    return ret[0]\n\n\ndef _binomial_inversion(key, p, n):\n    def _binom_inv_body_fn(val):\n        i, key, geom_acc = val\n        key, key_u = random.split(key)\n        u = random.uniform(key_u)\n        geom = np.floor(np.log1p(-u) / log1_p) + 1\n        geom_acc = geom_acc + geom\n        return i + 1, key, geom_acc\n\n    def _binom_inv_cond_fn(val):\n        i, _, geom_acc = val\n        return geom_acc <= n\n\n    log1_p = np.log1p(-p)\n    ret = lax.while_loop(_binom_inv_cond_fn, _binom_inv_body_fn,\n                         (-1, key, 0.))\n    return ret[0]\n\n\ndef _binomial_dispatch(key, p, n):\n    def dispatch(key, p, n):\n        is_le_mid = p <= 0.5\n        pq = np.where(is_le_mid, p, 1 - p)\n        mu = n * pq\n        k = lax.cond(mu < 10,\n                     (key, pq, n),\n                     lambda x: _binomial_inversion(*x),\n                     (key, pq, n),\n                     lambda x: _binomial_btrs(*x))\n        return np.where(is_le_mid, k, n - k)\n\n    # Return 0 for nan `p` or negative `n`, since nan values are not allowed for integer types\n    cond0 = np.isfinite(p) & (n > 0) & (p > 0)\n    return lax.cond(cond0 & (p < 1),\n                    (key, p, n),\n                    lambda x: dispatch(*x),\n                    (),\n                    lambda _: np.where(cond0, n, 0))\n\n\n@partial(jit, static_argnums=(3,))\ndef _binomial(key, p, n, shape):\n    shape = shape or lax.broadcast_shapes(np.shape(p), np.shape(n))\n    # reshape to map over axis 0\n    p = np.reshape(np.broadcast_to(p, shape), -1)\n    n = np.reshape(np.broadcast_to(n, shape), -1)\n    key = random.split(key, np.size(p))\n    if xla_bridge.get_backend().platform == \'cpu\':\n        ret = lax.map(lambda x: _binomial_dispatch(*x),\n                      (key, p, n))\n    else:\n        ret = vmap(lambda *x: _binomial_dispatch(*x))(key, p, n)\n    return np.reshape(ret, shape)\n\n\ndef binomial(key, p, n=1, shape=()):\n    return _binomial(key, p, n, shape)\n\n\n@partial(jit, static_argnums=(2,))\ndef _categorical(key, p, shape):\n    # this implementation is fast when event shape is small, and slow otherwise\n    # Ref: https://stackoverflow.com/a/34190035\n    shape = shape or p.shape[:-1]\n    s = np.cumsum(p, axis=-1)\n    r = random.uniform(key, shape=shape + (1,))\n    # FIXME: replace this computation by using binary search as suggested in the above\n    # reference. A while_loop + vmap for a reshaped 2D array would be enough.\n    return np.sum(s < r, axis=-1)\n\n\ndef categorical(key, p, shape=()):\n    return _categorical(key, p, shape)\n\n\n# TODO: drop this for the next JAX release, see https://github.com/google/jax/pull/1855\ndef categorical_logits(key, logits, shape=()):\n    shape = shape or logits.shape[:-1]\n    return np.argmax(random.gumbel(key, shape + logits.shape[-1:], logits.dtype)\n                     + logits, axis=-1)\n\n\n# Ref https://github.com/numpy/numpy/blob/8a0858f3903e488495a56b4a6d19bbefabc97dca/\n# numpy/random/src/distributions/distributions.c#L574\ndef _poisson_large(val):\n    rng_key, lam = val\n    slam = np.sqrt(lam)\n    loglam = np.log(lam)\n    b = 0.931 + 2.53 * slam\n    a = -0.059 + 0.02483 * b\n    invalpha = 1.1239 + 1.1328 / (b - 3.4)\n    vr = 0.9277 - 3.6224 / (b - 2)\n\n    def cond_fn(val):\n        _, V, us, k = val\n        cond1 = (us >= 0.07) & (V <= vr)\n        cond2 = (k < 0) | ((us < 0.013) & (V > us))\n        cond3 = ((np.log(V) + np.log(invalpha) - np.log(a / (us * us) + b))\n                 <= (-lam + k * loglam - gammaln(k + 1)))\n\n        # lax.cond in _poisson_one apparently may still\n        # execute _poisson_large for small lam:\n        # additional condition to not iterate if that is the case\n        cond4 = lam >= 10\n        return (~cond1) & (cond2 | (~cond3)) & cond4\n\n    def body_fn(val):\n        rng_key, *_ = val\n        rng_key, key_U, key_V = random.split(rng_key, 3)\n        U = random.uniform(key_U) - 0.5\n        V = random.uniform(key_V)\n        us = 0.5 - np.abs(U)\n        k = np.floor((2 * a / us + b) * U + lam + 0.43)\n        return rng_key, V, us, k\n\n    *_, k = lax.while_loop(cond_fn, body_fn, (rng_key, 0., 0., -1.))\n    return k\n\n\ndef _poisson_small(val):\n    rng_key, lam = val\n    enlam = np.exp(-lam)\n\n    def cond_fn(val):\n        cond1 = val[1] > enlam\n\n        # lax.cond in _poisson_one apparently may still\n        # execute _poisson_small for large lam:\n        # additional condition to not iterate if that is the case\n        cond2 = lam < 10\n        return cond1 & cond2\n\n    def body_fn(val):\n        rng_key, prod, k = val\n        rng_key, key_U = random.split(rng_key)\n        U = random.uniform(key_U)\n        prod = prod * U\n        return rng_key, prod, k + 1\n\n    init = np.where(lam == 0., 0., -1.)\n    *_, k = lax.while_loop(cond_fn, body_fn, (rng_key, 1., init))\n    return k\n\n\ndef _poisson_one(val):\n    return lax.cond(val[1] >= 10, val, _poisson_large, val, _poisson_small)\n\n\n@partial(jit, static_argnums=(2, 3))\ndef _poisson(key, rate, shape, dtype):\n    # Ref: https://en.wikipedia.org/wiki/Poisson_distribution#Generating_Poisson-distributed_random_variables\n    shape = shape or np.shape(rate)\n    rate = lax.convert_element_type(rate, canonicalize_dtype(np.float64))\n    rate = np.broadcast_to(rate, shape)\n    rng_keys = random.split(key, np.size(rate))\n    if xla_bridge.get_backend().platform == \'cpu\':\n        k = lax.map(_poisson_one, (rng_keys, np.reshape(rate, -1)))\n    else:\n        k = vmap(_poisson_one)((rng_keys, np.reshape(rate, -1)))\n    k = lax.convert_element_type(k, dtype)\n    return np.reshape(k, shape)\n\n\ndef poisson(key, rate, shape=(), dtype=np.int64):\n    dtype = canonicalize_dtype(dtype)\n    return _poisson(key, rate, shape, dtype)\n\n\ndef _scatter_add_one(operand, indices, updates):\n    return lax.scatter_add(operand, indices, updates,\n                           lax.ScatterDimensionNumbers(update_window_dims=(),\n                                                       inserted_window_dims=(0,),\n                                                       scatter_dims_to_operand_dims=(0,)))\n\n\n@partial(jit, static_argnums=(3, 4))\ndef _multinomial(key, p, n, n_max, shape=()):\n    if np.shape(n) != np.shape(p)[:-1]:\n        broadcast_shape = lax.broadcast_shapes(np.shape(n), np.shape(p)[:-1])\n        n = np.broadcast_to(n, broadcast_shape)\n        p = np.broadcast_to(p, broadcast_shape + np.shape(p)[-1:])\n    shape = shape or p.shape[:-1]\n    # get indices from categorical distribution then gather the result\n    indices = categorical(key, p, (n_max,) + shape)\n    # mask out values when counts is heterogeneous\n    if np.ndim(n) > 0:\n        mask = promote_shapes(np.arange(n_max) < np.expand_dims(n, -1), shape=shape + (n_max,))[0]\n        mask = np.moveaxis(mask, -1, 0).astype(indices.dtype)\n        excess = np.concatenate([np.expand_dims(n_max - n, -1), np.zeros(np.shape(n) + (p.shape[-1] - 1,))], -1)\n    else:\n        mask = 1\n        excess = 0\n    # NB: we transpose to move batch shape to the front\n    indices_2D = (np.reshape(indices * mask, (n_max, -1,))).T\n    samples_2D = vmap(_scatter_add_one, (0, 0, 0))(np.zeros((indices_2D.shape[0], p.shape[-1]),\n                                                            dtype=indices.dtype),\n                                                   np.expand_dims(indices_2D, axis=-1),\n                                                   np.ones(indices_2D.shape, dtype=indices.dtype))\n    return np.reshape(samples_2D, shape + p.shape[-1:]) - excess\n\n\ndef multinomial(key, p, n, shape=()):\n    n_max = int(np.max(n))\n    return _multinomial(key, p, n, n_max, shape)\n\n\ndef cholesky_of_inverse(matrix):\n    # This formulation only takes the inverse of a triangular matrix\n    # which is more numerically stable.\n    # Refer to:\n    # https://nbviewer.jupyter.org/gist/fehiepsi/5ef8e09e61604f10607380467eb82006#Precision-to-scale_tril\n    tril_inv = np.swapaxes(np.linalg.cholesky(matrix[..., ::-1, ::-1])[..., ::-1, ::-1], -2, -1)\n    identity = np.broadcast_to(np.identity(matrix.shape[-1]), tril_inv.shape)\n    return solve_triangular(tril_inv, identity, lower=True)\n\n\n# TODO: move upstream to jax.nn\ndef binary_cross_entropy_with_logits(x, y):\n    # compute -y * log(sigmoid(x)) - (1 - y) * log(1 - sigmoid(x))\n    # Ref: https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\n    return np.clip(x, 0) + np.log1p(np.exp(-np.abs(x))) - x * y\n\n\ndef promote_shapes(*args, shape=()):\n    # adapted from lax.lax_numpy\n    if len(args) < 2 and not shape:\n        return args\n    else:\n        shapes = [np.shape(arg) for arg in args]\n        num_dims = len(lax.broadcast_shapes(shape, *shapes))\n        return [lax.reshape(arg, (1,) * (num_dims - len(s)) + s)\n                if len(s) < num_dims else arg for arg, s in zip(args, shapes)]\n\n\ndef get_dtype(x):\n    return canonicalize_dtype(lax.dtype(x))\n\n\ndef sum_rightmost(x, dim):\n    """"""\n    Sum out ``dim`` many rightmost dimensions of a given tensor.\n    """"""\n    out_dim = np.ndim(x) - dim\n    x = np.reshape(x[..., np.newaxis], np.shape(x)[:out_dim] + (-1,))\n    return np.sum(x, axis=-1)\n\n\ndef matrix_to_tril_vec(x, diagonal=0):\n    idxs = np.tril_indices(x.shape[-1], diagonal)\n    return x[..., idxs[0], idxs[1]]\n\n\ndef vec_to_tril_matrix(t, diagonal=0):\n    # NB: the following formula only works for diagonal <= 0\n    n = round((math.sqrt(1 + 8 * t.shape[-1]) - 1) / 2) - diagonal\n    n2 = n * n\n    idx = np.reshape(np.arange(n2), (n, n))[np.tril_indices(n, diagonal)]\n    x = lax.scatter_add(np.zeros(t.shape[:-1] + (n2,)), np.expand_dims(idx, axis=-1), t,\n                        lax.ScatterDimensionNumbers(update_window_dims=range(t.ndim - 1),\n                                                    inserted_window_dims=(t.ndim - 1,),\n                                                    scatter_dims_to_operand_dims=(t.ndim - 1,)))\n    return np.reshape(x, x.shape[:-1] + (n, n))\n\n\ndef cholesky_update(L, x, coef=1):\n    """"""\n    Finds cholesky of L @ L.T + coef * x @ x.T.\n\n    **References;**\n\n        1. A more efficient rank-one covariance matrix update for evolution strategies,\n           Oswin Krause and Christian Igel\n    """"""\n    batch_shape = lax.broadcast_shapes(L.shape[:-2], x.shape[:-1])\n    L = np.broadcast_to(L, batch_shape + L.shape[-2:])\n    x = np.broadcast_to(x, batch_shape + x.shape[-1:])\n    diag = np.diagonal(L, axis1=-2, axis2=-1)\n    # convert to unit diagonal triangular matrix: L @ D @ T.t\n    L = L / diag[..., None, :]\n    D = np.square(diag)\n\n    def scan_fn(carry, val):\n        b, w = carry\n        j, Dj, L_j = val\n        wj = w[..., j]\n        gamma = b * Dj + coef * np.square(wj)\n        Dj_new = gamma / b\n        b = gamma / Dj_new\n\n        # update vectors w and L_j\n        w = w - wj[..., None] * L_j\n        L_j = L_j + (coef * wj / gamma)[..., None] * w\n        return (b, w), (Dj_new, L_j)\n\n    D, L = np.moveaxis(D, -1, 0), np.moveaxis(L, -1, 0)  # move scan dim to front\n    _, (D, L) = lax.scan(scan_fn, (np.ones(batch_shape), x), (np.arange(D.shape[0]), D, L))\n    D, L = np.moveaxis(D, 0, -1), np.moveaxis(L, 0, -1)  # move scan dim back\n    return L * np.sqrt(D)[..., None, :]\n\n\ndef signed_stick_breaking_tril(t):\n    # make sure that t in (-1, 1)\n    eps = np.finfo(t.dtype).eps\n    t = np.clip(t, a_min=(-1 + eps), a_max=(1 - eps))\n    # transform t to tril matrix with identity diagonal\n    r = vec_to_tril_matrix(t, diagonal=-1)\n\n    # apply stick-breaking on the squared values;\n    # we omit the step of computing s = z * z_cumprod by using the fact:\n    #     y = sign(r) * s = sign(r) * sqrt(z * z_cumprod) = r * sqrt(z_cumprod)\n    z = r ** 2\n    z1m_cumprod = np.cumprod(1 - z, axis=-1)\n    z1m_cumprod_sqrt = np.sqrt(z1m_cumprod)\n\n    pad_width = [(0, 0)] * z.ndim\n    pad_width[-1] = (1, 0)\n    z1m_cumprod_sqrt_shifted = np.pad(z1m_cumprod_sqrt[..., :-1], pad_width,\n                                      mode=""constant"", constant_values=1.)\n    y = (r + np.identity(r.shape[-1])) * z1m_cumprod_sqrt_shifted\n    return y\n\n\ndef logmatmulexp(x, y):\n    """"""\n    Numerically stable version of ``(x.log() @ y.log()).exp()``.\n    """"""\n    x_shift = lax.stop_gradient(np.amax(x, -1, keepdims=True))\n    y_shift = lax.stop_gradient(np.amax(y, -2, keepdims=True))\n    xy = np.log(np.matmul(np.exp(x - x_shift), np.exp(y - y_shift)))\n    return xy + x_shift + y_shift\n\n\ndef logsumexp(x, axis=0, keepdims=False):\n    # TODO: remove when https://github.com/google/jax/pull/2260 merged upstream\n    x_max = lax.stop_gradient(np.amax(x, axis=axis, keepdims=True))\n    y = np.log(np.sum(np.exp(x - x_max), axis=axis, keepdims=True)) + x_max\n    return y if keepdims else y.squeeze(axis=axis)\n\n\ndef clamp_probs(probs):\n    finfo = np.finfo(get_dtype(probs))\n    return np.clip(probs, a_min=finfo.tiny, a_max=1. - finfo.eps)\n\n\n# The is sourced from: torch.distributions.util.py\n#\n# Copyright (c) 2016-     Facebook, Inc            (Adam Paszke)\n# Copyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\n# Copyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\n# Copyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\n# Copyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\n# Copyright (c) 2011-2013 NYU                      (Clement Farabet)\n# Copyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\n# Copyright (c) 2006      Idiap Research Institute (Samy Bengio)\n# Copyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\nclass lazy_property(object):\n    r""""""\n    Used as a decorator for lazy loading of class attributes. This uses a\n    non-data descriptor that calls the wrapped method to compute the property on\n    first call; thereafter replacing the wrapped method into an instance\n    attribute.\n    """"""\n    def __init__(self, wrapped):\n        self.wrapped = wrapped\n        update_wrapper(self, wrapped)\n\n    def __get__(self, instance, obj_type=None):\n        if instance is None:\n            return self\n        value = self.wrapped(instance)\n        setattr(instance, self.wrapped.__name__, value)\n        return value\n\n\ndef validate_sample(log_prob_fn):\n    def wrapper(self, *args, **kwargs):\n        log_prob = log_prob_fn(self, *args, *kwargs)\n        if self._validate_args:\n            value = kwargs[\'value\'] if \'value\' in kwargs else args[0]\n            mask = self._validate_sample(value)\n            log_prob = np.where(mask, log_prob, -np.inf)\n        return log_prob\n\n    return wrapper\n'"
numpyro/examples/__init__.py,0,b''
numpyro/examples/datasets.py,15,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\nimport csv\nimport gzip\nimport os\nimport struct\nfrom urllib.parse import urlparse\nfrom urllib.request import urlretrieve\n\nimport numpy as np\n\nfrom jax import device_put, lax\nfrom jax.interpreters.xla import DeviceArray\n\nif \'CI\' in os.environ:\n    DATA_DIR = os.path.expanduser(\'~/.data\')\nelse:\n    DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__),\n                                            \'.data\'))\nos.makedirs(DATA_DIR, exist_ok=True)\n\n\ndset = namedtuple(\'dset\', [\'name\', \'urls\'])\n\n\nBASEBALL = dset(\'baseball\', [\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/EfronMorrisBB.txt\',\n])\n\n\nCOVTYPE = dset(\'covtype\', [\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/covtype.zip\',\n])\n\n\nMNIST = dset(\'mnist\', [\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/mnist/train-images-idx3-ubyte.gz\',\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/mnist/train-labels-idx1-ubyte.gz\',\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/mnist/t10k-images-idx3-ubyte.gz\',\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/mnist/t10k-labels-idx1-ubyte.gz\',\n])\n\n\nSP500 = dset(\'SP500\', [\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/SP500.csv\',\n])\n\n\nUCBADMIT = dset(\'ucbadmit\', [\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/UCBadmit.csv\',\n])\n\n\nLYNXHARE = dset(\'lynxhare\', [\n    \'https://d2hg8soec8ck9v.cloudfront.net/datasets/LynxHare.txt\',\n])\n\n\ndef _download(dset):\n    for url in dset.urls:\n        file = os.path.basename(urlparse(url).path)\n        out_path = os.path.join(DATA_DIR, file)\n        if not os.path.exists(out_path):\n            print(\'Downloading - {}.\'.format(url))\n            urlretrieve(url, out_path)\n            print(\'Download complete.\')\n\n\ndef _load_baseball():\n    _download(BASEBALL)\n\n    def train_test_split(file):\n        train, test, player_names = [], [], []\n        with open(file, \'r\') as f:\n            csv_reader = csv.DictReader(f, delimiter=\'\\t\', quoting=csv.QUOTE_NONE)\n            for row in csv_reader:\n                player_names.append(row[\'FirstName\'] + \' \' + row[\'LastName\'])\n                at_bats, hits = row[\'At-Bats\'], row[\'Hits\']\n                train.append(np.array([int(at_bats), int(hits)]))\n                season_at_bats, season_hits = row[\'SeasonAt-Bats\'], row[\'SeasonHits\']\n                test.append(np.array([int(season_at_bats), int(season_hits)]))\n        return np.stack(train), np.stack(test), np.array(player_names)\n\n    train, test, player_names = train_test_split(os.path.join(DATA_DIR, \'EfronMorrisBB.txt\'))\n    return {\'train\': (train, player_names),\n            \'test\': (test, player_names)}\n\n\ndef _load_covtype():\n    _download(COVTYPE)\n\n    file_path = os.path.join(DATA_DIR, \'covtype.zip\')\n    data = np.load(file_path)\n\n    return {\n        \'train\': (data[\'data\'], data[\'target\'])\n    }\n\n\ndef _load_mnist():\n    _download(MNIST)\n\n    def read_label(file):\n        with gzip.open(file, \'rb\') as f:\n            f.read(8)\n            data = np.frombuffer(f.read(), dtype=np.int8) / np.float32(255.)\n            return device_put(data)\n\n    def read_img(file):\n        with gzip.open(file, \'rb\') as f:\n            _, _, nrows, ncols = struct.unpack("">IIII"", f.read(16))\n            data = np.frombuffer(f.read(), dtype=np.uint8) / np.float32(255.)\n            return device_put(data.reshape(-1, nrows, ncols))\n\n    files = [os.path.join(DATA_DIR, os.path.basename(urlparse(url).path))\n             for url in MNIST.urls]\n    return {\'train\': (read_img(files[0]), read_label(files[1])),\n            \'test\': (read_img(files[2]), read_label(files[3]))}\n\n\ndef _load_sp500():\n    _download(SP500)\n\n    date, value = [], []\n    with open(os.path.join(DATA_DIR, \'SP500.csv\'), \'r\') as f:\n        csv_reader = csv.DictReader(f, quoting=csv.QUOTE_NONE)\n        for row in csv_reader:\n            date.append(row[\'DATE\'])\n            value.append(float(row[\'VALUE\']))\n    date = np.stack(date)\n    value = np.stack(value)\n\n    return {\'train\': (date, value)}\n\n\ndef _load_ucbadmit():\n    _download(UCBADMIT)\n\n    dept, male, applications, admit = [], [], [], []\n    with open(os.path.join(DATA_DIR, \'UCBadmit.csv\')) as f:\n        csv_reader = csv.DictReader(\n            f,\n            delimiter=\';\',\n            fieldnames=[\'index\', \'dept\', \'gender\', \'admit\', \'reject\', \'applications\']\n        )\n        next(csv_reader)  # skip the first row\n        for row in csv_reader:\n            dept.append(ord(row[\'dept\']) - ord(\'A\'))\n            male.append(row[\'gender\'] == \'male\')\n            applications.append(int(row[\'applications\']))\n            admit.append(int(row[\'admit\']))\n\n    return {\'train\': (np.stack(dept), np.stack(male), np.stack(applications), np.stack(admit))}\n\n\ndef _load_lynxhare():\n    _download(LYNXHARE)\n\n    file_path = os.path.join(DATA_DIR, \'LynxHare.txt\')\n    data = np.loadtxt(file_path)\n\n    return {\n        \'train\': (data[:, 0].astype(int), data[:, 1:])\n    }\n\n\ndef _load(dset):\n    if dset == BASEBALL:\n        return _load_baseball()\n    elif dset == COVTYPE:\n        return _load_covtype()\n    elif dset == MNIST:\n        return _load_mnist()\n    elif dset == SP500:\n        return _load_sp500()\n    elif dset == UCBADMIT:\n        return _load_ucbadmit()\n    elif dset == LYNXHARE:\n        return _load_lynxhare()\n    raise ValueError(\'Dataset - {} not found.\'.format(dset.name))\n\n\ndef iter_dataset(dset, batch_size=None, split=\'train\', shuffle=True):\n    arrays = _load(dset)[split]\n    num_records = len(arrays[0])\n    idxs = np.arange(num_records)\n    if not batch_size:\n        batch_size = num_records\n    if shuffle:\n        idxs = np.random.permutation(idxs)\n    for i in range(num_records // batch_size):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, num_records)\n        yield tuple(a[idxs[start_idx:end_idx]] for a in arrays)\n\n\ndef load_dataset(dset, batch_size=None, split=\'train\', shuffle=True):\n    arrays = _load(dset)[split]\n    num_records = len(arrays[0])\n    idxs = np.arange(num_records)\n    if not batch_size:\n        batch_size = num_records\n\n    def init():\n        return num_records // batch_size, np.random.permutation(idxs) if shuffle else idxs\n\n    def get_batch(i=0, idxs=idxs):\n        ret_idx = lax.dynamic_slice_in_dim(idxs, i * batch_size, batch_size)\n        return tuple(lax.index_take(a, (ret_idx,), axes=(0,)) if isinstance(a, DeviceArray)\n                     else np.take(a, ret_idx, axis=0) for a in arrays)\n\n    return init, get_batch\n'"
numpyro/infer/__init__.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpyro.infer.elbo import ELBO, RenyiELBO\nfrom numpyro.infer.initialization import (\n    init_to_feasible,\n    init_to_median,\n    init_to_prior,\n    init_to_uniform,\n    init_to_value,\n)\nfrom numpyro.infer.mcmc import HMC, MCMC, NUTS, SA\nfrom numpyro.infer.svi import SVI\nfrom numpyro.infer.util import (\n    Predictive,\n    log_likelihood\n)\n\n__all__ = [\n    'init_to_feasible',\n    'init_to_median',\n    'init_to_prior',\n    'init_to_uniform',\n    'init_to_value',\n    'log_likelihood',\n    'ELBO',\n    'RenyiELBO',\n    'HMC',\n    'MCMC',\n    'NUTS',\n    'Predictive',\n    'SA',\n    'SVI',\n]\n"""
numpyro/infer/elbo.py,4,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom jax import random, vmap\nfrom jax.lax import stop_gradient\nimport jax.numpy as np\nfrom jax.scipy.special import logsumexp\n\nfrom numpyro.handlers import replay, seed\nfrom numpyro.infer.util import log_density\n\n\nclass ELBO(object):\n    """"""\n    A trace implementation of ELBO-based SVI. The estimator is constructed\n    along the lines of references [1] and [2]. There are no restrictions on the\n    dependency structure of the model or the guide.\n\n    This is the most basic implementation of the Evidence Lower Bound, which is the\n    fundamental objective in Variational Inference. This implementation has various\n    limitations (for example it only supports random variables with reparameterized\n    samplers) but can be used as a template to build more sophisticated loss\n    objectives.\n\n    For more details, refer to http://pyro.ai/examples/svi_part_i.html.\n\n    **References:**\n\n    1. *Automated Variational Inference in Probabilistic Programming*,\n       David Wingate, Theo Weber\n    2. *Black Box Variational Inference*,\n       Rajesh Ranganath, Sean Gerrish, David M. Blei\n\n    :param num_particles: The number of particles/samples used to form the ELBO\n        (gradient) estimators.\n    """"""\n    def __init__(self, num_particles=1):\n        self.num_particles = num_particles\n\n    def loss(self, rng_key, param_map, model, guide, *args, **kwargs):\n        """"""\n        Evaluates the ELBO with an estimator that uses num_particles many samples/particles.\n\n        :param jax.random.PRNGKey rng_key: random number generator seed.\n        :param dict param_map: dictionary of current parameter values keyed by site\n            name.\n        :param model: Python callable with NumPyro primitives for the model.\n        :param guide: Python callable with NumPyro primitives for the guide.\n        :param args: arguments to the model / guide (these can possibly vary during\n            the course of fitting).\n        :param kwargs: keyword arguments to the model / guide (these can possibly vary\n            during the course of fitting).\n        :return: negative of the Evidence Lower Bound (ELBO) to be minimized.\n        """"""\n        def single_particle_elbo(rng_key):\n            model_seed, guide_seed = random.split(rng_key)\n            seeded_model = seed(model, model_seed)\n            seeded_guide = seed(guide, guide_seed)\n            guide_log_density, guide_trace = log_density(seeded_guide, args, kwargs, param_map)\n            seeded_model = replay(seeded_model, guide_trace)\n            model_log_density, _ = log_density(seeded_model, args, kwargs, param_map)\n\n            # log p(z) - log q(z)\n            elbo = model_log_density - guide_log_density\n            return elbo\n\n        # Return (-elbo) since by convention we do gradient descent on a loss and\n        # the ELBO is a lower bound that needs to be maximized.\n        if self.num_particles == 1:\n            return - single_particle_elbo(rng_key)\n        else:\n            rng_keys = random.split(rng_key, self.num_particles)\n            return - np.mean(vmap(single_particle_elbo)(rng_keys))\n\n\nclass RenyiELBO(ELBO):\n    r""""""\n    An implementation of Renyi\'s :math:`\\alpha`-divergence\n    variational inference following reference [1].\n    In order for the objective to be a strict lower bound, we require\n    :math:`\\alpha \\ge 0`. Note, however, that according to reference [1], depending\n    on the dataset :math:`\\alpha < 0` might give better results. In the special case\n    :math:`\\alpha = 0`, the objective function is that of the important weighted\n    autoencoder derived in reference [2].\n\n    .. note:: Setting :math:`\\alpha < 1` gives a better bound than the usual ELBO.\n\n    :param float alpha: The order of :math:`\\alpha`-divergence.\n        Here :math:`\\alpha \\neq 1`. Default is 0.\n    :param num_particles: The number of particles/samples\n        used to form the objective (gradient) estimator. Default is 2.\n\n    **References:**\n\n    1. *Renyi Divergence Variational Inference*, Yingzhen Li, Richard E. Turner\n    2. *Importance Weighted Autoencoders*, Yuri Burda, Roger Grosse, Ruslan Salakhutdinov\n    """"""\n    def __init__(self, alpha=0, num_particles=2):\n        if alpha == 1:\n            raise ValueError(""The order alpha should not be equal to 1. Please use ELBO class""\n                             ""for the case alpha = 1."")\n        self.alpha = alpha\n        super(RenyiELBO, self).__init__(num_particles=num_particles)\n\n    def loss(self, rng_key, param_map, model, guide, *args, **kwargs):\n        """"""\n        Evaluates the Renyi ELBO with an estimator that uses num_particles many samples/particles.\n\n        :param jax.random.PRNGKey rng_key: random number generator seed.\n        :param dict param_map: dictionary of current parameter values keyed by site\n            name.\n        :param model: Python callable with NumPyro primitives for the model.\n        :param guide: Python callable with NumPyro primitives for the guide.\n        :param args: arguments to the model / guide (these can possibly vary during\n            the course of fitting).\n        :param kwargs: keyword arguments to the model / guide (these can possibly vary\n            during the course of fitting).\n        :returns: negative of the Renyi Evidence Lower Bound (ELBO) to be minimized.\n        """"""\n        def single_particle_elbo(rng_key):\n            model_seed, guide_seed = random.split(rng_key)\n            seeded_model = seed(model, model_seed)\n            seeded_guide = seed(guide, guide_seed)\n            guide_log_density, guide_trace = log_density(seeded_guide, args, kwargs, param_map)\n            # NB: we only want to substitute params not available in guide_trace\n            model_param_map = {k: v for k, v in param_map.items() if k not in guide_trace}\n            seeded_model = replay(seeded_model, guide_trace)\n            model_log_density, _ = log_density(seeded_model, args, kwargs, model_param_map)\n\n            # log p(z) - log q(z)\n            elbo = model_log_density - guide_log_density\n            return elbo\n\n        rng_keys = random.split(rng_key, self.num_particles)\n        elbos = vmap(single_particle_elbo)(rng_keys)\n        scaled_elbos = (1. - self.alpha) * elbos\n        avg_log_exp = logsumexp(scaled_elbos) - np.log(self.num_particles)\n        weights = np.exp(scaled_elbos - avg_log_exp)\n        renyi_elbo = avg_log_exp / (1. - self.alpha)\n        weighted_elbo = np.dot(stop_gradient(weights), elbos) / self.num_particles\n        return - (stop_gradient(renyi_elbo - weighted_elbo) + weighted_elbo)\n'"
numpyro/infer/hmc_util.py,58,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\n\nfrom jax import grad, random, value_and_grad, vmap\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as np\nfrom jax.ops import index_update\nfrom jax.scipy.special import expit\nfrom jax.tree_util import tree_flatten, tree_map, tree_multimap\n\nimport numpyro.distributions as dist\nfrom numpyro.distributions.util import cholesky_of_inverse, get_dtype\nfrom numpyro.util import cond, identity, while_loop\n\nAdaptWindow = namedtuple(\'AdaptWindow\', [\'start\', \'end\'])\nHMCAdaptState = namedtuple(\'HMCAdaptState\', [\'step_size\', \'inverse_mass_matrix\', \'mass_matrix_sqrt\',\n                                             \'ss_state\', \'mm_state\', \'window_idx\', \'rng_key\'])\nIntegratorState = namedtuple(\'IntegratorState\', [\'z\', \'r\', \'potential_energy\', \'z_grad\'])\n\nTreeInfo = namedtuple(\'TreeInfo\', [\'z_left\', \'r_left\', \'z_left_grad\',\n                                   \'z_right\', \'r_right\', \'z_right_grad\',\n                                   \'z_proposal\', \'z_proposal_pe\', \'z_proposal_grad\', \'z_proposal_energy\',\n                                   \'depth\', \'weight\', \'r_sum\', \'turning\', \'diverging\',\n                                   \'sum_accept_probs\', \'num_proposals\'])\n\n\ndef dual_averaging(t0=10, kappa=0.75, gamma=0.05):\n    """"""\n    Dual Averaging is a scheme to solve convex optimization problems. It\n    belongs to a class of subgradient methods which uses subgradients (which\n    lie in a dual space) to update states (in primal space) of a model. Under\n    some conditions, the averages of generated parameters during the scheme are\n    guaranteed to converge to an optimal value. However, a counter-intuitive\n    aspect of traditional subgradient methods is ""new subgradients enter the\n    model with decreasing weights"" (see reference [1]). Dual Averaging scheme\n    resolves that issue by updating parameters using weights equally for\n    subgradients, hence we have the name ""dual averaging"".\n\n    This class implements a dual averaging scheme which is adapted for Markov\n    chain Monte Carlo (MCMC) algorithms. To be more precise, we will replace\n    subgradients by some statistics calculated at the end of MCMC trajectories.\n    Following [2], we introduce some free parameters such as ``t0`` and\n    ``kappa``, which is helpful and still guarantees the convergence of the\n    scheme.\n\n    **References:**\n\n    1. *Primal-dual subgradient methods for convex problems*,\n       Yurii Nesterov\n    2. *The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo*,\n       Matthew D. Hoffman, Andrew Gelman\n\n    :param int t0: A free parameter introduced in reference [2] that stabilizes\n        the initial steps of the scheme. Defaults to 10.\n    :param float kappa: A free parameter introduced in reference [2] that\n        controls the weights of steps of the scheme. For a small ``kappa``, the\n        scheme will quickly forget states from early steps. This should be a\n        number in :math:`(0.5, 1]`. Defaults to 0.75.\n    :param float gamma: A free parameter introduced in reference [1] which\n        controls the speed of the convergence of the scheme. Defaults to 0.05.\n    :return: a (`init_fn`, `update_fn`) pair.\n    """"""\n    def init_fn(prox_center=0.):\n        """"""\n        :param float prox_center: A parameter introduced in reference [1] which\n            pulls the primal sequence towards it. Defaults to 0.\n        :return: initial state for the scheme.\n        """"""\n        x_t = 0.\n        x_avg = 0.  # average of primal sequence\n        g_avg = 0.  # average of dual sequence\n        t = 0\n        return x_t, x_avg, g_avg, t, prox_center\n\n    def update_fn(g, state):\n        """"""\n        :param float g: The current subgradient or statistics calculated during\n            an MCMC trajectory.\n        :param state: Current state of the scheme.\n        :return: new state for the scheme.\n        """"""\n        x_t, x_avg, g_avg, t, prox_center = state\n        t = t + 1\n        # g_avg = (g_1 + ... + g_t) / t\n        g_avg = (1 - 1 / (t + t0)) * g_avg + g / (t + t0)\n        # According to formula (3.4) of [1], we have\n        #     x_t = argmin{ g_avg . x + loc_t . |x - x0|^2 },\n        # hence x_t = x0 - g_avg / (2 * loc_t),\n        # where loc_t := beta_t / t, beta_t := (gamma/2) * sqrt(t).\n        x_t = prox_center - (t ** 0.5) / gamma * g_avg\n        # weight for the new x_t\n        weight_t = t ** (-kappa)\n        x_avg = (1 - weight_t) * x_avg + weight_t * x_t\n        return x_t, x_avg, g_avg, t, prox_center\n\n    return init_fn, update_fn\n\n\ndef welford_covariance(diagonal=True):\n    """"""\n    Implements Welford\'s online method for estimating (co)variance. Useful for\n    adapting diagonal and dense mass structures for HMC. It is required that\n    each sample is a 1-dimensional array.\n\n    **References:**\n\n    1. *The Art of Computer Programming*,\n       Donald E. Knuth\n\n    :param bool diagonal: If True, we estimate the variance of samples.\n        Otherwise, we estimate the covariance of the samples. Defaults to True.\n    :return: a (`init_fn`, `update_fn`, `final_fn`) triple.\n    """"""\n    def init_fn(size):\n        """"""\n        :param int size: size of each sample.\n        :return: initial state for the scheme.\n        """"""\n        mean = np.zeros(size)\n        if diagonal:\n            m2 = np.zeros(size)\n        else:\n            m2 = np.zeros((size, size))\n        n = 0\n        return mean, m2, n\n\n    def update_fn(sample, state):\n        """"""\n        :param sample: A new sample.\n        :param state: Current state of the scheme.\n        :return: new state for the scheme.\n        """"""\n        mean, m2, n = state\n        n = n + 1\n        delta_pre = sample - mean\n        mean = mean + delta_pre / n\n        delta_post = sample - mean\n        if diagonal:\n            m2 = m2 + delta_pre * delta_post\n        else:\n            m2 = m2 + np.outer(delta_post, delta_pre)\n        return mean, m2, n\n\n    def final_fn(state, regularize=False):\n        """"""\n        :param state: Current state of the scheme.\n        :param bool regularize: Whether to adjust diagonal for numerical stability.\n        :return: a pair of estimated covariance and the square root of precision.\n        """"""\n        mean, m2, n = state\n        # XXX it is not necessary to check for the case n=1\n        cov = m2 / (n - 1)\n        if regularize:\n            # Regularization from Stan\n            scaled_cov = (n / (n + 5)) * cov\n            shrinkage = 1e-3 * (5 / (n + 5))\n            if diagonal:\n                cov = scaled_cov + shrinkage\n            else:\n                cov = scaled_cov + shrinkage * np.identity(mean.shape[0])\n        if np.ndim(cov) == 2:\n            cov_inv_sqrt = cholesky_of_inverse(cov)\n        else:\n            cov_inv_sqrt = np.sqrt(np.reciprocal(cov))\n        return cov, cov_inv_sqrt\n\n    return init_fn, update_fn, final_fn\n\n\ndef velocity_verlet(potential_fn, kinetic_fn):\n    r""""""\n    Second order symplectic integrator that uses the velocity verlet algorithm\n    for position `z` and momentum `r`.\n\n    :param potential_fn: Python callable that computes the potential energy\n        given input parameters. The input parameters to `potential_fn` can be\n        any python collection type.\n    :param kinetic_fn: Python callable that returns the kinetic energy given\n        inverse mass matrix and momentum.\n    :return: a pair of (`init_fn`, `update_fn`).\n    """"""\n    def init_fn(z, r, potential_energy=None, z_grad=None):\n        """"""\n        :param z: Position of the particle.\n        :param r: Momentum of the particle.\n        :param potential_energy: Potential energy at `z`.\n        :param z_grad: gradient of potential energy at `z`.\n        :return: initial state for the integrator.\n        """"""\n        if potential_energy is None or z_grad is None:\n            potential_energy, z_grad = value_and_grad(potential_fn)(z)\n        return IntegratorState(z, r, potential_energy, z_grad)\n\n    def update_fn(step_size, inverse_mass_matrix, state):\n        """"""\n        :param float step_size: Size of a single step.\n        :param inverse_mass_matrix: Inverse of mass matrix, which is used to\n            calculate kinetic energy.\n        :param state: Current state of the integrator.\n        :return: new state for the integrator.\n        """"""\n        z, r, _, z_grad = state\n        r = tree_multimap(lambda r, z_grad: r - 0.5 * step_size * z_grad, r, z_grad)  # r(n+1/2)\n        r_grad = grad(kinetic_fn, argnums=1)(inverse_mass_matrix, r)\n        z = tree_multimap(lambda z, r_grad: z + step_size * r_grad, z, r_grad)  # z(n+1)\n        potential_energy, z_grad = value_and_grad(potential_fn)(z)\n        r = tree_multimap(lambda r, z_grad: r - 0.5 * step_size * z_grad, r, z_grad)  # r(n+1)\n        return IntegratorState(z, r, potential_energy, z_grad)\n\n    return init_fn, update_fn\n\n\ndef find_reasonable_step_size(potential_fn, kinetic_fn, momentum_generator,\n                              init_step_size, inverse_mass_matrix, position, rng_key):\n    """"""\n    Finds a reasonable step size by tuning `init_step_size`. This function is used\n    to avoid working with a too large or too small step size in HMC.\n\n    **References:**\n\n    1. *The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo*,\n       Matthew D. Hoffman, Andrew Gelman\n\n    :param potential_fn: A callable to compute potential energy.\n    :param kinetic_fn: A callable to compute kinetic energy.\n    :param momentum_generator: A generator to get a random momentum variable.\n    :param float init_step_size: Initial step size to be tuned.\n    :param inverse_mass_matrix: Inverse of mass matrix.\n    :param position: Current position of the particle.\n    :param jax.random.PRNGKey rng_key: Random key to be used as the source of randomness.\n    :return: a reasonable value for step size.\n    :rtype: float\n    """"""\n    # We are going to find a step_size which make accept_prob (Metropolis correction)\n    # near the target_accept_prob. If accept_prob:=exp(-delta_energy) is small,\n    # then we have to decrease step_size; otherwise, increase step_size.\n    target_accept_prob = np.log(0.8)\n\n    _, vv_update = velocity_verlet(potential_fn, kinetic_fn)\n    z = position\n    potential_energy, z_grad = value_and_grad(potential_fn)(z)\n    finfo = np.finfo(get_dtype(init_step_size))\n\n    def _body_fn(state):\n        step_size, _, direction, rng_key = state\n        rng_key, rng_key_momentum = random.split(rng_key)\n        # scale step_size: increase 2x or decrease 2x depends on direction;\n        # direction=1 means keep increasing step_size, otherwise decreasing step_size.\n        # Note that the direction is -1 if delta_energy is `NaN`, which may be the\n        # case for a diverging trajectory (e.g. in the case of evaluating log prob\n        # of a value simulated using a large step size for a constrained sample site).\n        step_size = (2.0 ** direction) * step_size\n        r = momentum_generator(position, inverse_mass_matrix, rng_key_momentum)\n        _, r_new, potential_energy_new, _ = vv_update(step_size,\n                                                      inverse_mass_matrix,\n                                                      (z, r, potential_energy, z_grad))\n        energy_current = kinetic_fn(inverse_mass_matrix, r) + potential_energy\n        energy_new = kinetic_fn(inverse_mass_matrix, r_new) + potential_energy_new\n        delta_energy = energy_new - energy_current\n        direction_new = np.where(target_accept_prob < -delta_energy, 1, -1)\n        return step_size, direction, direction_new, rng_key\n\n    def _cond_fn(state):\n        step_size, last_direction, direction, _ = state\n        # condition to run only if step_size is not too small or we are not decreasing step_size\n        not_small_step_size_cond = (step_size > finfo.tiny) | (direction >= 0)\n        # condition to run only if step_size is not too large or we are not increasing step_size\n        not_large_step_size_cond = (step_size < finfo.max) | (direction <= 0)\n        not_extreme_cond = not_small_step_size_cond & not_large_step_size_cond\n        return not_extreme_cond & ((last_direction == 0) | (direction == last_direction))\n\n    step_size, _, _, _ = while_loop(_cond_fn, _body_fn, (init_step_size, 0, 0, rng_key))\n    return step_size\n\n\ndef build_adaptation_schedule(num_steps):\n    """"""\n    Builds a window adaptation schedule to be used during warmup phase of HMC.\n\n    :param int num_steps: Number of warmup steps.\n    :return: a list of contiguous windows, each has attributes `start` and `end`,\n        where `start` is the starting index and `end` is the ending index of the window.\n\n    **References:**\n\n    1. *Stan Reference Manual version 2.18*,\n       Stan Development Team\n    """"""\n    adaptation_schedule = []\n    # from Stan, for small num_steps\n    if num_steps < 20:\n        adaptation_schedule.append(AdaptWindow(0, num_steps - 1))\n        return adaptation_schedule\n\n    # We separate num_steps into windows:\n    #   start_buffer + window 1 + window 2 + window 3 + ... + end_buffer\n    # where the length of each window will be doubled for the next window.\n    # We won\'t adapt mass matrix during start and end buffers; and mass\n    # matrix will be updated at the end of each window. This is helpful\n    # for dealing with the intense computation of sampling momentum from the\n    # inverse of mass matrix.\n    start_buffer_size = 75  # from Stan\n    end_buffer_size = 50  # from Stan\n    init_window_size = 25  # from Stan\n    if (start_buffer_size + end_buffer_size + init_window_size) > num_steps:\n        start_buffer_size = int(0.15 * num_steps)\n        end_buffer_size = int(0.1 * num_steps)\n        init_window_size = num_steps - start_buffer_size - end_buffer_size\n\n    adaptation_schedule.append(AdaptWindow(start=0, end=start_buffer_size - 1))\n    end_window_start = num_steps - end_buffer_size\n\n    next_window_size = init_window_size\n    next_window_start = start_buffer_size\n    while next_window_start < end_window_start:\n        cur_window_start, cur_window_size = next_window_start, next_window_size\n        # Ensure that slow adaptation windows are monotonically increasing\n        if 3 * cur_window_size <= end_window_start - cur_window_start:\n            next_window_size = 2 * cur_window_size\n        else:\n            cur_window_size = end_window_start - cur_window_start\n        next_window_start = cur_window_start + cur_window_size\n        adaptation_schedule.append(AdaptWindow(cur_window_start, next_window_start - 1))\n    adaptation_schedule.append(AdaptWindow(end_window_start, num_steps - 1))\n    return adaptation_schedule\n\n\ndef warmup_adapter(num_adapt_steps, find_reasonable_step_size=None,\n                   adapt_step_size=True, adapt_mass_matrix=True,\n                   dense_mass=False, target_accept_prob=0.8):\n    """"""\n    A scheme to adapt tunable parameters, namely step size and mass matrix, during\n    the warmup phase of HMC.\n\n    :param int num_adapt_steps: Number of warmup steps.\n    :param find_reasonable_step_size: A callable to find a reasonable step size\n        at the beginning of each adaptation window.\n    :param bool adapt_step_size: A flag to decide if we want to adapt step_size\n        during warm-up phase using Dual Averaging scheme (defaults to ``True``).\n    :param bool adapt_mass_matrix: A flag to decide if we want to adapt mass\n        matrix during warm-up phase using Welford scheme (defaults to ``True``).\n    :param bool dense_mass: A flag to decide if mass matrix is dense or\n        diagonal (defaults to ``False``).\n    :param float target_accept_prob: Target acceptance probability for step size\n        adaptation using Dual Averaging. Increasing this value will lead to a smaller\n        step size, hence the sampling will be slower but more robust. Default to 0.8.\n    :return: a pair of (`init_fn`, `update_fn`).\n    """"""\n    if find_reasonable_step_size is None:\n        find_reasonable_step_size = identity\n    ss_init, ss_update = dual_averaging()\n    mm_init, mm_update, mm_final = welford_covariance(diagonal=not dense_mass)\n    adaptation_schedule = np.array(build_adaptation_schedule(num_adapt_steps))\n    num_windows = len(adaptation_schedule)\n\n    def init_fn(z, rng_key, step_size=1.0, inverse_mass_matrix=None, mass_matrix_size=None):\n        """"""\n        :param z: Initial position of the integrator.\n        :param jax.random.PRNGKey rng_key: Random key to be used as the source of randomness.\n        :param float step_size: Initial step size.\n        :param inverse_mass_matrix: Inverse of the initial mass matrix. If ``None``,\n            inverse of mass matrix will be an identity matrix with size is decided\n            by the argument `mass_matrix_size`.\n        :param int mass_matrix_size: Size of the mass matrix.\n        :return: initial state of the adapt scheme.\n        """"""\n        rng_key, rng_key_ss = random.split(rng_key)\n        if inverse_mass_matrix is None:\n            assert mass_matrix_size is not None\n            if dense_mass:\n                inverse_mass_matrix = np.identity(mass_matrix_size)\n            else:\n                inverse_mass_matrix = np.ones(mass_matrix_size)\n            mass_matrix_sqrt = inverse_mass_matrix\n        else:\n            if dense_mass:\n                mass_matrix_sqrt = cholesky_of_inverse(inverse_mass_matrix)\n            else:\n                mass_matrix_sqrt = np.sqrt(np.reciprocal(inverse_mass_matrix))\n\n        if adapt_step_size:\n            step_size = find_reasonable_step_size(step_size, inverse_mass_matrix, z, rng_key_ss)\n        ss_state = ss_init(np.log(10 * step_size))\n\n        mm_state = mm_init(inverse_mass_matrix.shape[-1])\n\n        window_idx = 0\n        return HMCAdaptState(step_size, inverse_mass_matrix, mass_matrix_sqrt,\n                             ss_state, mm_state, window_idx, rng_key)\n\n    def _update_at_window_end(z, rng_key_ss, state):\n        step_size, inverse_mass_matrix, mass_matrix_sqrt, ss_state, mm_state, window_idx, rng_key = state\n\n        if adapt_mass_matrix:\n            inverse_mass_matrix, mass_matrix_sqrt = mm_final(mm_state, regularize=True)\n            mm_state = mm_init(inverse_mass_matrix.shape[-1])\n\n        if adapt_step_size:\n            step_size = find_reasonable_step_size(step_size, inverse_mass_matrix, z, rng_key_ss)\n            ss_state = ss_init(np.log(10 * step_size))\n\n        return HMCAdaptState(step_size, inverse_mass_matrix, mass_matrix_sqrt,\n                             ss_state, mm_state, window_idx, rng_key)\n\n    def update_fn(t, accept_prob, z, state):\n        """"""\n        :param int t: The current time step.\n        :param float accept_prob: Acceptance probability of the current trajectory.\n        :param z: New position drawn at the end of the current trajectory.\n        :param state: Current state of the adapt scheme.\n        :return: new state of the adapt scheme.\n        """"""\n        step_size, inverse_mass_matrix, mass_matrix_sqrt, ss_state, mm_state, window_idx, rng_key = state\n        rng_key, rng_key_ss = random.split(rng_key)\n\n        # update step size state\n        if adapt_step_size:\n            ss_state = ss_update(target_accept_prob - accept_prob, ss_state)\n            # note: at the end of warmup phase, use average of log step_size\n            log_step_size, log_step_size_avg, *_ = ss_state\n            step_size = np.where(t == (num_adapt_steps - 1),\n                                 np.exp(log_step_size_avg),\n                                 np.exp(log_step_size))\n            # account the the case log_step_size is an extreme number\n            finfo = np.finfo(get_dtype(step_size))\n            step_size = np.clip(step_size, a_min=finfo.tiny, a_max=finfo.max)\n\n        # update mass matrix state\n        is_middle_window = (0 < window_idx) & (window_idx < (num_windows - 1))\n        if adapt_mass_matrix:\n            z_flat, _ = ravel_pytree(z)\n            mm_state = cond(is_middle_window,\n                            (z_flat, mm_state), lambda args: mm_update(*args),\n                            mm_state, identity)\n\n        t_at_window_end = t == adaptation_schedule[window_idx, 1]\n        window_idx = np.where(t_at_window_end, window_idx + 1, window_idx)\n        state = HMCAdaptState(step_size, inverse_mass_matrix, mass_matrix_sqrt,\n                              ss_state, mm_state, window_idx, rng_key)\n        state = cond(t_at_window_end & is_middle_window,\n                     (z, rng_key_ss, state), lambda args: _update_at_window_end(*args),\n                     state, identity)\n        return state\n\n    return init_fn, update_fn\n\n\ndef _is_turning(inverse_mass_matrix, r_left, r_right, r_sum):\n    r_left, _ = ravel_pytree(r_left)\n    r_right, _ = ravel_pytree(r_right)\n    r_sum, _ = ravel_pytree(r_sum)\n\n    if inverse_mass_matrix.ndim == 2:\n        v_left = np.matmul(inverse_mass_matrix, r_left)\n        v_right = np.matmul(inverse_mass_matrix, r_right)\n    elif inverse_mass_matrix.ndim == 1:\n        v_left = np.multiply(inverse_mass_matrix, r_left)\n        v_right = np.multiply(inverse_mass_matrix, r_right)\n\n    # This implements dynamic termination criterion (ref [2], section A.4.2).\n    r_sum = r_sum - (r_left + r_right) / 2\n    turning_at_left = np.dot(v_left, r_sum) <= 0\n    turning_at_right = np.dot(v_right, r_sum) <= 0\n    return turning_at_left | turning_at_right\n\n\ndef _uniform_transition_kernel(current_tree, new_tree):\n    # This function computes transition prob for subtrees (ref [2], section A.3.1).\n    # e^new_weight / (e^new_weight + e^current_weight)\n    transition_prob = expit(new_tree.weight - current_tree.weight)\n    return transition_prob\n\n\ndef _biased_transition_kernel(current_tree, new_tree):\n    # This function computes transition prob for main trees (ref [2], section A.3.2).\n    transition_prob = np.exp(new_tree.weight - current_tree.weight)\n    # If new tree is turning or diverging, we won\'t move the proposal\n    # to the new tree.\n    transition_prob = np.where(new_tree.turning | new_tree.diverging,\n                               0.0, np.clip(transition_prob, a_max=1.0))\n    return transition_prob\n\n\ndef _combine_tree(current_tree, new_tree, inverse_mass_matrix, going_right, rng_key, biased_transition):\n    # Now we combine the current tree and the new tree. Note that outside\n    # leaves of the combined tree are determined by the direction.\n    z_left, r_left, z_left_grad, z_right, r_right, r_right_grad = cond(\n        going_right,\n        (current_tree, new_tree),\n        lambda trees: (trees[0].z_left, trees[0].r_left,\n                       trees[0].z_left_grad, trees[1].z_right,\n                       trees[1].r_right, trees[1].z_right_grad),\n        (new_tree, current_tree),\n        lambda trees: (trees[0].z_left, trees[0].r_left,\n                       trees[0].z_left_grad, trees[1].z_right,\n                       trees[1].r_right, trees[1].z_right_grad)\n    )\n    r_sum = tree_multimap(np.add, current_tree.r_sum, new_tree.r_sum)\n\n    if biased_transition:\n        transition_prob = _biased_transition_kernel(current_tree, new_tree)\n        turning = new_tree.turning | _is_turning(inverse_mass_matrix, r_left, r_right, r_sum)\n    else:\n        transition_prob = _uniform_transition_kernel(current_tree, new_tree)\n        turning = current_tree.turning\n\n    transition = random.bernoulli(rng_key, transition_prob)\n    z_proposal, z_proposal_pe, z_proposal_grad, z_proposal_energy = cond(\n        transition,\n        new_tree, lambda tree: (tree.z_proposal, tree.z_proposal_pe, tree.z_proposal_grad, tree.z_proposal_energy),\n        current_tree, lambda tree: (tree.z_proposal, tree.z_proposal_pe, tree.z_proposal_grad, tree.z_proposal_energy)\n    )\n\n    tree_depth = current_tree.depth + 1\n    tree_weight = np.logaddexp(current_tree.weight, new_tree.weight)\n    diverging = new_tree.diverging\n\n    sum_accept_probs = current_tree.sum_accept_probs + new_tree.sum_accept_probs\n    num_proposals = current_tree.num_proposals + new_tree.num_proposals\n\n    return TreeInfo(z_left, r_left, z_left_grad, z_right, r_right, r_right_grad,\n                    z_proposal, z_proposal_pe, z_proposal_grad, z_proposal_energy,\n                    tree_depth, tree_weight, r_sum, turning, diverging,\n                    sum_accept_probs, num_proposals)\n\n\ndef _build_basetree(vv_update, kinetic_fn, z, r, z_grad, inverse_mass_matrix, step_size, going_right,\n                    energy_current, max_delta_energy):\n    step_size = np.where(going_right, step_size, -step_size)\n    z_new, r_new, potential_energy_new, z_new_grad = vv_update(\n        step_size,\n        inverse_mass_matrix,\n        (z, r, energy_current, z_grad),\n    )\n\n    energy_new = potential_energy_new + kinetic_fn(inverse_mass_matrix, r_new)\n    delta_energy = energy_new - energy_current\n    # Handles the NaN case.\n    delta_energy = np.where(np.isnan(delta_energy), np.inf, delta_energy)\n    tree_weight = -delta_energy\n\n    diverging = delta_energy > max_delta_energy\n    accept_prob = np.clip(np.exp(-delta_energy), a_max=1.0)\n    return TreeInfo(z_new, r_new, z_new_grad, z_new, r_new, z_new_grad,\n                    z_new, potential_energy_new, z_new_grad, energy_new,\n                    depth=0, weight=tree_weight, r_sum=r_new, turning=False,\n                    diverging=diverging, sum_accept_probs=accept_prob, num_proposals=1)\n\n\ndef _get_leaf(tree, going_right):\n    return cond(going_right,\n                tree,\n                lambda tree: (tree.z_right, tree.r_right, tree.z_right_grad),\n                tree,\n                lambda tree: (tree.z_left, tree.r_left, tree.z_left_grad))\n\n\ndef _double_tree(current_tree, vv_update, kinetic_fn, inverse_mass_matrix, step_size,\n                 going_right, rng_key, energy_current, max_delta_energy, r_ckpts, r_sum_ckpts):\n    key, transition_key = random.split(rng_key)\n\n    new_tree = _iterative_build_subtree(current_tree, vv_update, kinetic_fn,\n                                        inverse_mass_matrix, step_size,\n                                        going_right, key, energy_current, max_delta_energy,\n                                        r_ckpts, r_sum_ckpts)\n\n    return _combine_tree(current_tree, new_tree, inverse_mass_matrix, going_right, transition_key,\n                         True)\n\n\ndef _leaf_idx_to_ckpt_idxs(n):\n    # computes the number of non-zero bits except the last bit\n    # e.g. 6 -> 2, 7 -> 2, 13 -> 2\n    _, idx_max = while_loop(lambda nc: nc[0] > 0,\n                            lambda nc: (nc[0] >> 1, nc[1] + (nc[0] & 1)),\n                            (n >> 1, 0))\n    # computes the number of contiguous last non-zero bits\n    # e.g. 6 -> 0, 7 -> 3, 13 -> 1\n    _, num_subtrees = while_loop(lambda nc: (nc[0] & 1) != 0,\n                                 lambda nc: (nc[0] >> 1, nc[1] + 1),\n                                 (n, 0))\n    # TODO: explore the potential of setting idx_min=0 to allow more turning checks\n    # It will be useful in case: e.g. assume a tree 0 -> 7 is a circle,\n    # subtrees 0 -> 3, 4 -> 7 are half-circles, which two leaves might not\n    # satisfy turning condition;\n    # the full tree 0 -> 7 is a circle, which two leaves might also not satisfy\n    # turning condition;\n    # however, we can check the turning condition of the subtree 0 -> 5, which\n    # likely satisfies turning condition because its trajectory 3/4 of a circle.\n    # XXX: make sure that detailed balance is satisfied if we follow this direction\n    idx_min = idx_max - num_subtrees + 1\n    return idx_min, idx_max\n\n\ndef _is_iterative_turning(inverse_mass_matrix, r, r_sum, r_ckpts, r_sum_ckpts, idx_min, idx_max):\n    def _body_fn(state):\n        i, _ = state\n        subtree_r_sum = r_sum - r_sum_ckpts[i] + r_ckpts[i]\n        return i - 1, _is_turning(inverse_mass_matrix, r_ckpts[i], r, subtree_r_sum)\n\n    _, turning = while_loop(lambda it: (it[0] >= idx_min) & ~it[1],\n                            _body_fn,\n                            (idx_max, False))\n    return turning\n\n\ndef _iterative_build_subtree(prototype_tree, vv_update, kinetic_fn,\n                             inverse_mass_matrix, step_size, going_right, rng_key,\n                             energy_current, max_delta_energy, r_ckpts, r_sum_ckpts):\n    max_num_proposals = 2 ** prototype_tree.depth\n\n    def _cond_fn(state):\n        tree, turning, _, _, _ = state\n        return (tree.num_proposals < max_num_proposals) & ~turning & ~tree.diverging\n\n    def _body_fn(state):\n        current_tree, _, r_ckpts, r_sum_ckpts, rng_key = state\n        rng_key, transition_rng_key = random.split(rng_key)\n        # If we are going to the right, start from the right leaf of the current tree.\n        z, r, z_grad = _get_leaf(current_tree, going_right)\n        new_leaf = _build_basetree(vv_update, kinetic_fn, z, r, z_grad, inverse_mass_matrix, step_size,\n                                   going_right, energy_current, max_delta_energy)\n        new_tree = cond(current_tree.num_proposals == 0,\n                        new_leaf,\n                        identity,\n                        (current_tree, new_leaf, inverse_mass_matrix, going_right, transition_rng_key),\n                        lambda x: _combine_tree(*x, False))\n\n        leaf_idx = current_tree.num_proposals\n        # NB: in the special case leaf_idx=0, ckpt_idx_min=1 and ckpt_idx_max=0,\n        # the following logic is still valid for that case\n        ckpt_idx_min, ckpt_idx_max = _leaf_idx_to_ckpt_idxs(leaf_idx)\n        r, _ = ravel_pytree(new_leaf.r_right)\n        r_sum, _ = ravel_pytree(new_tree.r_sum)\n        # we update checkpoints when leaf_idx is even\n        r_ckpts, r_sum_ckpts = cond(leaf_idx % 2 == 0,\n                                    (r_ckpts, r_sum_ckpts),\n                                    lambda x: (index_update(x[0], ckpt_idx_max, r),\n                                               index_update(x[1], ckpt_idx_max, r_sum)),\n                                    (r_ckpts, r_sum_ckpts),\n                                    identity)\n\n        turning = _is_iterative_turning(inverse_mass_matrix, r, r_sum, r_ckpts, r_sum_ckpts,\n                                        ckpt_idx_min, ckpt_idx_max)\n        return new_tree, turning, r_ckpts, r_sum_ckpts, rng_key\n\n    basetree = prototype_tree._replace(num_proposals=0)\n\n    tree, turning, _, _, _ = while_loop(\n        _cond_fn,\n        _body_fn,\n        (basetree, False, r_ckpts, r_sum_ckpts, rng_key)\n    )\n    # update depth and turning condition\n    return TreeInfo(tree.z_left, tree.r_left, tree.z_left_grad,\n                    tree.z_right, tree.r_right, tree.z_right_grad,\n                    tree.z_proposal, tree.z_proposal_pe, tree.z_proposal_grad, tree.z_proposal_energy,\n                    prototype_tree.depth, tree.weight, tree.r_sum, turning, tree.diverging,\n                    tree.sum_accept_probs, tree.num_proposals)\n\n\ndef build_tree(verlet_update, kinetic_fn, verlet_state, inverse_mass_matrix, step_size, rng_key,\n               max_delta_energy=1000., max_tree_depth=10):\n    """"""\n    Builds a binary tree from the `verlet_state`. This is used in NUTS sampler.\n\n    **References:**\n\n    1. *The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo*,\n       Matthew D. Hoffman, Andrew Gelman\n    2. *A Conceptual Introduction to Hamiltonian Monte Carlo*,\n       Michael Betancourt\n\n    :param verlet_update: A callable to get a new integrator state given a current\n        integrator state.\n    :param kinetic_fn: A callable to compute kinetic energy.\n    :param verlet_state: Initial integrator state.\n    :param inverse_mass_matrix: Inverse of the mass matrix.\n    :param float step_size: Step size for the current trajectory.\n    :param jax.random.PRNGKey rng_key: random key to be used as the source of\n        randomness.\n    :param float max_delta_energy: A threshold to decide if the new state diverges\n        (based on the energy difference) too much from the initial integrator state.\n    :return: information of the tree.\n    :rtype: :data:`TreeInfo`\n    """"""\n    z, r, potential_energy, z_grad = verlet_state\n    energy_current = potential_energy + kinetic_fn(inverse_mass_matrix, r)\n    r_ckpts = np.zeros((max_tree_depth, inverse_mass_matrix.shape[-1]))\n    r_sum_ckpts = np.zeros((max_tree_depth, inverse_mass_matrix.shape[-1]))\n\n    tree = TreeInfo(z, r, z_grad, z, r, z_grad, z, potential_energy, z_grad, energy_current,\n                    depth=0, weight=0., r_sum=r, turning=False, diverging=False,\n                    sum_accept_probs=0., num_proposals=0)\n\n    def _cond_fn(state):\n        tree, _ = state\n        return (tree.depth < max_tree_depth) & ~tree.turning & ~tree.diverging\n\n    def _body_fn(state):\n        tree, key = state\n        key, direction_key, doubling_key = random.split(key, 3)\n        going_right = random.bernoulli(direction_key)\n        tree = _double_tree(tree, verlet_update, kinetic_fn, inverse_mass_matrix, step_size,\n                            going_right, doubling_key, energy_current, max_delta_energy,\n                            r_ckpts, r_sum_ckpts)\n        return tree, key\n\n    state = (tree, rng_key)\n    tree, _ = while_loop(_cond_fn, _body_fn, state)\n    return tree\n\n\ndef euclidean_kinetic_energy(inverse_mass_matrix, r):\n    r, _ = ravel_pytree(r)\n\n    if inverse_mass_matrix.ndim == 2:\n        v = np.matmul(inverse_mass_matrix, r)\n    elif inverse_mass_matrix.ndim == 1:\n        v = np.multiply(inverse_mass_matrix, r)\n\n    return 0.5 * np.dot(v, r)\n\n\ndef consensus(subposteriors, num_draws=None, diagonal=False, rng_key=None):\n    """"""\n    Merges subposteriors following consensus Monte Carlo algorithm.\n\n    **References:**\n\n    1. *Bayes and big data: The consensus Monte Carlo algorithm*,\n       Steven L. Scott, Alexander W. Blocker, Fernando V. Bonassi, Hugh A. Chipman,\n       Edward I. George, Robert E. McCulloch\n\n    :param list subposteriors: a list in which each element is a collection of samples.\n    :param int num_draws: number of draws from the merged posterior.\n    :param bool diagonal: whether to compute weights using variance or covariance, defaults to\n        `False` (using covariance).\n    :param jax.random.PRNGKey rng_key: source of the randomness, defaults to `jax.random.PRNGKey(0)`.\n    :return: if `num_draws` is None, merges subposteriors without resampling; otherwise, returns\n        a collection of `num_draws` samples with the same data structure as each subposterior.\n    """"""\n    # stack subposteriors\n    joined_subposteriors = tree_multimap(lambda *args: np.stack(args), *subposteriors)\n    # shape of joined_subposteriors: n_subs x n_samples x sample_shape\n    joined_subposteriors = vmap(vmap(lambda sample: ravel_pytree(sample)[0]))(joined_subposteriors)\n\n    if num_draws is not None:\n        rng_key = random.PRNGKey(0) if rng_key is None else rng_key\n        # randomly gets num_draws from subposteriors\n        n_subs = len(subposteriors)\n        n_samples = tree_flatten(subposteriors[0])[0][0].shape[0]\n        # shape of draw_idxs: n_subs x num_draws x sample_shape\n        draw_idxs = random.randint(rng_key, shape=(n_subs, num_draws), minval=0, maxval=n_samples)\n        joined_subposteriors = vmap(lambda x, idx: x[idx])(joined_subposteriors, draw_idxs)\n\n    if diagonal:\n        # compute weights for each subposterior (ref: Section 3.1 of [1])\n        weights = vmap(lambda x: 1 / np.var(x, ddof=1, axis=0))(joined_subposteriors)\n        normalized_weights = weights / np.sum(weights, axis=0)\n        # get weighted samples\n        samples_flat = np.einsum(\'ij,ikj->kj\', normalized_weights, joined_subposteriors)\n    else:\n        weights = vmap(lambda x: np.linalg.inv(np.cov(x.T)))(joined_subposteriors)\n        normalized_weights = np.matmul(np.linalg.inv(np.sum(weights, axis=0)), weights)\n        samples_flat = np.einsum(\'ijk,ilk->lj\', normalized_weights, joined_subposteriors)\n\n    # unravel_fn acts on 1 sample of a subposterior\n    _, unravel_fn = ravel_pytree(tree_map(lambda x: x[0], subposteriors[0]))\n    return vmap(lambda x: unravel_fn(x))(samples_flat)\n\n\ndef parametric(subposteriors, diagonal=False):\n    """"""\n    Merges subposteriors following (embarrassingly parallel) parametric Monte Carlo algorithm.\n\n    **References:**\n\n    1. *Asymptotically Exact, Embarrassingly Parallel MCMC*,\n       Willie Neiswanger, Chong Wang, Eric Xing\n\n    :param list subposteriors: a list in which each element is a collection of samples.\n    :param bool diagonal: whether to compute weights using variance or covariance, defaults to\n        `False` (using covariance).\n    :return: the estimated mean and variance/covariance parameters of the joined posterior\n    """"""\n    joined_subposteriors = tree_multimap(lambda *args: np.stack(args), *subposteriors)\n    joined_subposteriors = vmap(vmap(lambda sample: ravel_pytree(sample)[0]))(joined_subposteriors)\n\n    submeans = np.mean(joined_subposteriors, axis=1)\n    if diagonal:\n        weights = vmap(lambda x: 1 / np.var(x, ddof=1, axis=0))(joined_subposteriors)\n        var = 1 / np.sum(weights, axis=0)\n        normalized_weights = var * weights\n\n        # comparing to consensus implementation, we compute weighted mean here\n        mean = np.einsum(\'ij,ij->j\', normalized_weights, submeans)\n        return mean, var\n    else:\n        weights = vmap(lambda x: np.linalg.inv(np.cov(x.T)))(joined_subposteriors)\n        cov = np.linalg.inv(np.sum(weights, axis=0))\n        normalized_weights = np.matmul(cov, weights)\n\n        # comparing to consensus implementation, we compute weighted mean here\n        mean = np.einsum(\'ijk,ik->j\', normalized_weights, submeans)\n        return mean, cov\n\n\ndef parametric_draws(subposteriors, num_draws, diagonal=False, rng_key=None):\n    """"""\n    Merges subposteriors following (embarrassingly parallel) parametric Monte Carlo algorithm.\n\n    **References:**\n\n    1. *Asymptotically Exact, Embarrassingly Parallel MCMC*,\n       Willie Neiswanger, Chong Wang, Eric Xing\n\n    :param list subposteriors: a list in which each element is a collection of samples.\n    :param int num_draws: number of draws from the merged posterior.\n    :param bool diagonal: whether to compute weights using variance or covariance, defaults to\n        `False` (using covariance).\n    :param jax.random.PRNGKey rng_key: source of the randomness, defaults to `jax.random.PRNGKey(0)`.\n    :return: a collection of `num_draws` samples with the same data structure as each subposterior.\n    """"""\n    rng_key = random.PRNGKey(0) if rng_key is None else rng_key\n    if diagonal:\n        mean, var = parametric(subposteriors, diagonal=True)\n        samples_flat = dist.Normal(mean, np.sqrt(var)).sample(rng_key, (num_draws,))\n    else:\n        mean, cov = parametric(subposteriors, diagonal=False)\n        samples_flat = dist.MultivariateNormal(mean, cov).sample(rng_key, (num_draws,))\n\n    _, unravel_fn = ravel_pytree(tree_map(lambda x: x[0], subposteriors[0]))\n    return vmap(lambda x: unravel_fn(x))(samples_flat)\n'"
numpyro/infer/initialization.py,3,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nfrom jax import random\nimport jax.numpy as np\n\nimport numpyro.distributions as dist\nfrom numpyro.distributions import biject_to\n\n\ndef init_to_median(site=None, num_samples=15):\n    """"""\n    Initialize to the prior median. For priors with no `.sample` method implemented,\n    we defer to the :func:`init_to_uniform` strategy.\n\n    :param int num_samples: number of prior points to calculate median.\n    """"""\n    if site is None:\n        return partial(init_to_median, num_samples=num_samples)\n\n    if site[\'type\'] == \'sample\' and not site[\'is_observed\'] and not site[\'fn\'].is_discrete:\n        rng_key = site[\'kwargs\'].get(\'rng_key\')\n        sample_shape = site[\'kwargs\'].get(\'sample_shape\')\n        try:\n            samples = site[\'fn\'].sample(rng_key, sample_shape=(num_samples,) + sample_shape)\n            return np.median(samples, axis=0)\n        except NotImplementedError:\n            return init_to_uniform(site)\n\n\ndef init_to_prior(site=None):\n    """"""\n    Initialize to a prior sample. For priors with no `.sample` method implemented,\n    we defer to the :func:`init_to_uniform` strategy.\n    """"""\n    return init_to_median(site, num_samples=1)\n\n\ndef init_to_uniform(site=None, radius=2):\n    """"""\n    Initialize to a random point in the area `(-radius, radius)` of unconstrained domain.\n\n    :param float radius: specifies the range to draw an initial point in the unconstrained domain.\n    """"""\n    if site is None:\n        return partial(init_to_uniform, radius=radius)\n\n    if site[\'type\'] == \'sample\' and not site[\'is_observed\'] and not site[\'fn\'].is_discrete:\n        rng_key = site[\'kwargs\'].get(\'rng_key\')\n        sample_shape = site[\'kwargs\'].get(\'sample_shape\')\n        rng_key, subkey = random.split(rng_key)\n\n        # this is used to interpret the changes of event_shape in\n        # domain and codomain spaces\n        try:\n            prototype_value = site[\'fn\'].sample(subkey, sample_shape=())\n        except NotImplementedError:\n            # XXX: this works for ImproperUniform prior,\n            # we can\'t use this logic for general priors\n            # because some distributions such as TransformedDistribution might\n            # have wrong event_shape.\n            prototype_value = np.full(site[\'fn\'].shape(), np.nan)\n\n        transform = biject_to(site[\'fn\'].support)\n        unconstrained_shape = np.shape(transform.inv(prototype_value))\n        unconstrained_samples = dist.Uniform(-radius, radius).sample(\n            rng_key, sample_shape=sample_shape + unconstrained_shape)\n        return transform(unconstrained_samples)\n\n\ndef init_to_feasible(site=None):\n    """"""\n    Initialize to an arbitrary feasible point, ignoring distribution\n    parameters.\n    """"""\n    return init_to_uniform(site, radius=0)\n\n\ndef init_to_value(site=None, values={}):\n    """"""\n    Initialize to the value specified in `values`. We defer to\n    :func:`init_to_uniform` strategy for sites which do not appear in `values`.\n\n    :param dict values: dictionary of initial values keyed by site name.\n    """"""\n    if site is None:\n        return partial(init_to_value, values=values)\n\n    if site[\'type\'] == \'sample\' and not site[\'is_observed\']:\n        if site[\'name\'] in values:\n            return values[site[\'name\']]\n        else:  # defer to default strategy\n            return init_to_uniform(site)\n'"
numpyro/infer/mcmc.py,50,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom abc import ABC, abstractmethod\nfrom collections import namedtuple\nimport functools\nimport math\nfrom operator import attrgetter\nimport os\nimport warnings\n\nfrom jax import jit, lax, partial, pmap, random, vmap, device_put\nfrom jax.core import Tracer\nfrom jax.dtypes import canonicalize_dtype\nfrom jax.flatten_util import ravel_pytree\nfrom jax.interpreters.xla import DeviceArray\nfrom jax.lib import xla_bridge\nimport jax.numpy as np\nfrom jax.random import PRNGKey\nfrom jax.scipy.special import logsumexp\nfrom jax.tree_util import tree_flatten, tree_map, tree_multimap\n\nfrom numpyro.diagnostics import print_summary\nimport numpyro.distributions as dist\nfrom numpyro.distributions.util import categorical_logits, cholesky_update\nfrom numpyro.infer.hmc_util import (\n    IntegratorState,\n    build_tree,\n    euclidean_kinetic_energy,\n    find_reasonable_step_size,\n    velocity_verlet,\n    warmup_adapter\n)\nfrom numpyro.infer.util import ParamInfo, init_to_uniform, initialize_model\nfrom numpyro.util import cond, copy_docs_from, fori_collect, fori_loop, identity, cached_by\n\nHMCState = namedtuple(\'HMCState\', [\'i\', \'z\', \'z_grad\', \'potential_energy\', \'energy\', \'num_steps\', \'accept_prob\',\n                                   \'mean_accept_prob\', \'diverging\', \'adapt_state\', \'rng_key\'])\n""""""\nA :func:`~collections.namedtuple` consisting of the following fields:\n\n - **i** - iteration. This is reset to 0 after warmup.\n - **z** - Python collection representing values (unconstrained samples from\n   the posterior) at latent sites.\n - **z_grad** - Gradient of potential energy w.r.t. latent sample sites.\n - **potential_energy** - Potential energy computed at the given value of ``z``.\n - **energy** - Sum of potential energy and kinetic energy of the current state.\n - **num_steps** - Number of steps in the Hamiltonian trajectory (for diagnostics).\n - **accept_prob** - Acceptance probability of the proposal. Note that ``z``\n   does not correspond to the proposal if it is rejected.\n - **mean_accept_prob** - Mean acceptance probability until current iteration\n   during warmup adaptation or sampling (for diagnostics).\n - **diverging** - A boolean value to indicate whether the current trajectory is diverging.\n - **adapt_state** - A ``HMCAdaptState`` namedtuple which contains adaptation information\n   during warmup:\n\n   + **step_size** - Step size to be used by the integrator in the next iteration.\n   + **inverse_mass_matrix** - The inverse mass matrix to be used for the next\n     iteration.\n   + **mass_matrix_sqrt** - The square root of mass matrix to be used for the next\n     iteration. In case of dense mass, this is the Cholesky factorization of the\n     mass matrix.\n\n - **rng_key** - random number generator seed used for the iteration.\n""""""\n\n\ndef _get_num_steps(step_size, trajectory_length):\n    num_steps = np.clip(trajectory_length / step_size, a_min=1)\n    # NB: casting to np.int64 does not take effect (returns np.int32 instead)\n    # if jax_enable_x64 is False\n    return num_steps.astype(canonicalize_dtype(np.int64))\n\n\ndef momentum_generator(prototype_r, mass_matrix_sqrt, rng_key):\n    _, unpack_fn = ravel_pytree(prototype_r)\n    eps = random.normal(rng_key, np.shape(mass_matrix_sqrt)[:1])\n    if mass_matrix_sqrt.ndim == 1:\n        r = np.multiply(mass_matrix_sqrt, eps)\n        return unpack_fn(r)\n    elif mass_matrix_sqrt.ndim == 2:\n        r = np.dot(mass_matrix_sqrt, eps)\n        return unpack_fn(r)\n    else:\n        raise ValueError(""Mass matrix has incorrect number of dims."")\n\n\ndef get_diagnostics_str(mcmc_state):\n    if isinstance(mcmc_state, HMCState):\n        return \'{} steps of size {:.2e}. acc. prob={:.2f}\'.format(mcmc_state.num_steps,\n                                                                  mcmc_state.adapt_state.step_size,\n                                                                  mcmc_state.mean_accept_prob)\n    else:\n        return \'acc. prob={:.2f}\'.format(mcmc_state.mean_accept_prob)\n\n\ndef get_progbar_desc_str(num_warmup, i):\n    if i < num_warmup:\n        return \'warmup\'\n    return \'sample\'\n\n\ndef hmc(potential_fn=None, potential_fn_gen=None, kinetic_fn=None, algo=\'NUTS\'):\n    r""""""\n    Hamiltonian Monte Carlo inference, using either fixed number of\n    steps or the No U-Turn Sampler (NUTS) with adaptive path length.\n\n    **References:**\n\n    1. *MCMC Using Hamiltonian Dynamics*,\n       Radford M. Neal\n    2. *The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo*,\n       Matthew D. Hoffman, and Andrew Gelman.\n    3. *A Conceptual Introduction to Hamiltonian Monte Carlo`*,\n       Michael Betancourt\n\n    :param potential_fn: Python callable that computes the potential energy\n        given input parameters. The input parameters to `potential_fn` can be\n        any python collection type, provided that `init_params` argument to\n        `init_kernel` has the same type.\n    :param potential_fn_gen: Python callable that when provided with model\n        arguments / keyword arguments returns `potential_fn`. This\n        may be provided to do inference on the same model with changing data.\n        If the data shape remains the same, we can compile `sample_kernel`\n        once, and use the same for multiple inference runs.\n    :param kinetic_fn: Python callable that returns the kinetic energy given\n        inverse mass matrix and momentum. If not provided, the default is\n        euclidean kinetic energy.\n    :param str algo: Whether to run ``HMC`` with fixed number of steps or ``NUTS``\n        with adaptive path length. Default is ``NUTS``.\n    :return: a tuple of callables (`init_kernel`, `sample_kernel`), the first\n        one to initialize the sampler, and the second one to generate samples\n        given an existing one.\n\n    .. warning::\n        Instead of using this interface directly, we would highly recommend you\n        to use the higher level :class:`numpyro.infer.MCMC` API instead.\n\n    **Example**\n\n    .. doctest::\n\n        >>> import jax\n        >>> from jax import random\n        >>> import jax.numpy as np\n        >>> import numpyro\n        >>> import numpyro.distributions as dist\n        >>> from numpyro.infer.mcmc import hmc\n        >>> from numpyro.infer.util import initialize_model\n        >>> from numpyro.util import fori_collect\n\n        >>> true_coefs = np.array([1., 2., 3.])\n        >>> data = random.normal(random.PRNGKey(2), (2000, 3))\n        >>> dim = 3\n        >>> labels = dist.Bernoulli(logits=(true_coefs * data).sum(-1)).sample(random.PRNGKey(3))\n        >>>\n        >>> def model(data, labels):\n        ...     coefs_mean = np.zeros(dim)\n        ...     coefs = numpyro.sample(\'beta\', dist.Normal(coefs_mean, np.ones(3)))\n        ...     intercept = numpyro.sample(\'intercept\', dist.Normal(0., 10.))\n        ...     return numpyro.sample(\'y\', dist.Bernoulli(logits=(coefs * data + intercept).sum(-1)), obs=labels)\n        >>>\n        >>> model_info = initialize_model(random.PRNGKey(0), model, model_args=(data, labels,))\n        >>> init_kernel, sample_kernel = hmc(model_info.potential_fn, algo=\'NUTS\')\n        >>> hmc_state = init_kernel(model_info.param_info,\n        ...                         trajectory_length=10,\n        ...                         num_warmup=300)\n        >>> samples = fori_collect(0, 500, sample_kernel, hmc_state,\n        ...                        transform=lambda state: model_info.postprocess_fn(state.z))\n        >>> print(np.mean(samples[\'beta\'], axis=0))  # doctest: +SKIP\n        [0.9153987 2.0754058 2.9621222]\n    """"""\n    if kinetic_fn is None:\n        kinetic_fn = euclidean_kinetic_energy\n    vv_update = None\n    trajectory_len = None\n    max_treedepth = None\n    wa_update = None\n    wa_steps = None\n    max_delta_energy = 1000.\n    if algo not in {\'HMC\', \'NUTS\'}:\n        raise ValueError(\'`algo` must be one of `HMC` or `NUTS`.\')\n\n    def init_kernel(init_params,\n                    num_warmup,\n                    step_size=1.0,\n                    inverse_mass_matrix=None,\n                    adapt_step_size=True,\n                    adapt_mass_matrix=True,\n                    dense_mass=False,\n                    target_accept_prob=0.8,\n                    trajectory_length=2*math.pi,\n                    max_tree_depth=10,\n                    find_heuristic_step_size=False,\n                    model_args=(),\n                    model_kwargs=None,\n                    rng_key=PRNGKey(0)):\n        """"""\n        Initializes the HMC sampler.\n\n        :param init_params: Initial parameters to begin sampling. The type must\n            be consistent with the input type to `potential_fn`.\n        :param int num_warmup: Number of warmup steps; samples generated\n            during warmup are discarded.\n        :param float step_size: Determines the size of a single step taken by the\n            verlet integrator while computing the trajectory using Hamiltonian\n            dynamics. If not specified, it will be set to 1.\n        :param numpy.ndarray inverse_mass_matrix: Initial value for inverse mass matrix.\n            This may be adapted during warmup if adapt_mass_matrix = True.\n            If no value is specified, then it is initialized to the identity matrix.\n        :param bool adapt_step_size: A flag to decide if we want to adapt step_size\n            during warm-up phase using Dual Averaging scheme.\n        :param bool adapt_mass_matrix: A flag to decide if we want to adapt mass\n            matrix during warm-up phase using Welford scheme.\n        :param bool dense_mass: A flag to decide if mass matrix is dense or\n            diagonal (default when ``dense_mass=False``)\n        :param float target_accept_prob: Target acceptance probability for step size\n            adaptation using Dual Averaging. Increasing this value will lead to a smaller\n            step size, hence the sampling will be slower but more robust. Default to 0.8.\n        :param float trajectory_length: Length of a MCMC trajectory for HMC. Default\n            value is :math:`2\\\\pi`.\n        :param int max_tree_depth: Max depth of the binary tree created during the doubling\n            scheme of NUTS sampler. Defaults to 10.\n        :param bool find_heuristic_step_size: whether to a heuristic function to adjust the\n            step size at the beginning of each adaptation window. Defaults to False.\n        :param tuple model_args: Model arguments if `potential_fn_gen` is specified.\n        :param dict model_kwargs: Model keyword arguments if `potential_fn_gen` is specified.\n        :param jax.random.PRNGKey rng_key: random key to be used as the source of\n            randomness.\n\n        """"""\n        step_size = lax.convert_element_type(step_size, canonicalize_dtype(np.float64))\n        nonlocal wa_update, trajectory_len, max_treedepth, vv_update, wa_steps\n        wa_steps = num_warmup\n        trajectory_len = trajectory_length\n        max_treedepth = max_tree_depth\n        if isinstance(init_params, ParamInfo):\n            z, pe, z_grad = init_params\n        else:\n            z, pe, z_grad = init_params, None, None\n        pe_fn = potential_fn\n        if potential_fn_gen:\n            if pe_fn is not None:\n                raise ValueError(\'Only one of `potential_fn` or `potential_fn_gen` must be provided.\')\n            else:\n                kwargs = {} if model_kwargs is None else model_kwargs\n                pe_fn = potential_fn_gen(*model_args, **kwargs)\n\n        find_reasonable_ss = None\n        if find_heuristic_step_size:\n            find_reasonable_ss = partial(find_reasonable_step_size,\n                                         pe_fn,\n                                         kinetic_fn,\n                                         momentum_generator)\n\n        wa_init, wa_update = warmup_adapter(num_warmup,\n                                            adapt_step_size=adapt_step_size,\n                                            adapt_mass_matrix=adapt_mass_matrix,\n                                            dense_mass=dense_mass,\n                                            target_accept_prob=target_accept_prob,\n                                            find_reasonable_step_size=find_reasonable_ss)\n\n        rng_key_hmc, rng_key_wa, rng_key_momentum = random.split(rng_key, 3)\n        wa_state = wa_init(z, rng_key_wa, step_size,\n                           inverse_mass_matrix=inverse_mass_matrix,\n                           mass_matrix_size=np.size(ravel_pytree(z)[0]))\n        r = momentum_generator(z, wa_state.mass_matrix_sqrt, rng_key_momentum)\n        vv_init, vv_update = velocity_verlet(pe_fn, kinetic_fn)\n        vv_state = vv_init(z, r, potential_energy=pe, z_grad=z_grad)\n        energy = kinetic_fn(wa_state.inverse_mass_matrix, vv_state.r)\n        hmc_state = HMCState(0, vv_state.z, vv_state.z_grad, vv_state.potential_energy, energy,\n                             0, 0., 0., False, wa_state, rng_key_hmc)\n        return device_put(hmc_state)\n\n    def _hmc_next(step_size, inverse_mass_matrix, vv_state,\n                  model_args, model_kwargs, rng_key):\n        if potential_fn_gen:\n            nonlocal vv_update\n            pe_fn = potential_fn_gen(*model_args, **model_kwargs)\n            _, vv_update = velocity_verlet(pe_fn, kinetic_fn)\n\n        num_steps = _get_num_steps(step_size, trajectory_len)\n        vv_state_new = fori_loop(0, num_steps,\n                                 lambda i, val: vv_update(step_size, inverse_mass_matrix, val),\n                                 vv_state)\n        energy_old = vv_state.potential_energy + kinetic_fn(inverse_mass_matrix, vv_state.r)\n        energy_new = vv_state_new.potential_energy + kinetic_fn(inverse_mass_matrix, vv_state_new.r)\n        delta_energy = energy_new - energy_old\n        delta_energy = np.where(np.isnan(delta_energy), np.inf, delta_energy)\n        accept_prob = np.clip(np.exp(-delta_energy), a_max=1.0)\n        diverging = delta_energy > max_delta_energy\n        transition = random.bernoulli(rng_key, accept_prob)\n        vv_state, energy = cond(transition,\n                                (vv_state_new, energy_new), identity,\n                                (vv_state, energy_old), identity)\n        return vv_state, energy, num_steps, accept_prob, diverging\n\n    def _nuts_next(step_size, inverse_mass_matrix, vv_state,\n                   model_args, model_kwargs, rng_key):\n        if potential_fn_gen:\n            nonlocal vv_update\n            pe_fn = potential_fn_gen(*model_args, **model_kwargs)\n            _, vv_update = velocity_verlet(pe_fn, kinetic_fn)\n\n        binary_tree = build_tree(vv_update, kinetic_fn, vv_state,\n                                 inverse_mass_matrix, step_size, rng_key,\n                                 max_delta_energy=max_delta_energy,\n                                 max_tree_depth=max_treedepth)\n        accept_prob = binary_tree.sum_accept_probs / binary_tree.num_proposals\n        num_steps = binary_tree.num_proposals\n        vv_state = IntegratorState(z=binary_tree.z_proposal,\n                                   r=vv_state.r,\n                                   potential_energy=binary_tree.z_proposal_pe,\n                                   z_grad=binary_tree.z_proposal_grad)\n        return vv_state, binary_tree.z_proposal_energy, num_steps, accept_prob, binary_tree.diverging\n\n    _next = _nuts_next if algo == \'NUTS\' else _hmc_next\n\n    def sample_kernel(hmc_state, model_args=(), model_kwargs=None):\n        """"""\n        Given an existing :data:`~numpyro.infer.mcmc.HMCState`, run HMC with fixed (possibly adapted)\n        step size and return a new :data:`~numpyro.infer.mcmc.HMCState`.\n\n        :param hmc_state: Current sample (and associated state).\n        :param tuple model_args: Model arguments if `potential_fn_gen` is specified.\n        :param dict model_kwargs: Model keyword arguments if `potential_fn_gen` is specified.\n        :return: new proposed :data:`~numpyro.infer.mcmc.HMCState` from simulating\n            Hamiltonian dynamics given existing state.\n\n        """"""\n        model_kwargs = {} if model_kwargs is None else model_kwargs\n        rng_key, rng_key_momentum, rng_key_transition = random.split(hmc_state.rng_key, 3)\n        r = momentum_generator(hmc_state.z, hmc_state.adapt_state.mass_matrix_sqrt, rng_key_momentum)\n        vv_state = IntegratorState(hmc_state.z, r, hmc_state.potential_energy, hmc_state.z_grad)\n        vv_state, energy, num_steps, accept_prob, diverging = _next(hmc_state.adapt_state.step_size,\n                                                                    hmc_state.adapt_state.inverse_mass_matrix,\n                                                                    vv_state,\n                                                                    model_args,\n                                                                    model_kwargs,\n                                                                    rng_key_transition)\n        # not update adapt_state after warmup phase\n        adapt_state = cond(hmc_state.i < wa_steps,\n                           (hmc_state.i, accept_prob, vv_state.z, hmc_state.adapt_state),\n                           lambda args: wa_update(*args),\n                           hmc_state.adapt_state,\n                           identity)\n\n        itr = hmc_state.i + 1\n        n = np.where(hmc_state.i < wa_steps, itr, itr - wa_steps)\n        mean_accept_prob = hmc_state.mean_accept_prob + (accept_prob - hmc_state.mean_accept_prob) / n\n\n        return HMCState(itr, vv_state.z, vv_state.z_grad, vv_state.potential_energy, energy, num_steps,\n                        accept_prob, mean_accept_prob, diverging, adapt_state, rng_key)\n\n    # Make `init_kernel` and `sample_kernel` visible from the global scope once\n    # `hmc` is called for sphinx doc generation.\n    if \'SPHINX_BUILD\' in os.environ:\n        hmc.init_kernel = init_kernel\n        hmc.sample_kernel = sample_kernel\n\n    return init_kernel, sample_kernel\n\n\nclass MCMCKernel(ABC):\n    """"""\n    Defines the interface for the Markov transition kernel that is\n    used for :class:`~numpyro.infer.MCMC` inference.\n    """"""\n    def postprocess_fn(self, model_args, model_kwargs):\n        """"""\n        Function that transforms unconstrained values at sample sites to values\n        constrained to the site\'s support, in addition to returning deterministic\n        sites in the model.\n\n        :param model_args: Arguments to the model.\n        :param model_kwargs: Keyword arguments to the model.\n        """"""\n        return identity\n\n    @abstractmethod\n    def init(self, rng_key, num_warmup, init_params, model_args, model_kwargs):\n        """"""\n        Initialize the `MCMCKernel` and return an initial state to begin sampling\n        from.\n\n        :param random.PRNGKey rng_key: Random number generator key to initialize\n            the kernel.\n        :param int num_warmup: Number of warmup steps. This can be useful\n            when doing adaptation during warmup.\n        :param tuple init_params: Initial parameters to begin sampling. The type must\n            be consistent with the input type to `potential_fn`.\n        :param model_args: Arguments provided to the model.\n        :param model_kwargs: Keyword arguments provided to the model.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def sample(self, state, model_args, model_kwargs):\n        """"""\n        Given the current `state`, return the next `state` using the given\n        transition kernel.\n\n        :param state: Arbitrary data structure representing the state for the\n            kernel. For HMC, this is given by :data:`~numpyro.infer.mcmc.HMCState`.\n        :param model_args: Arguments provided to the model.\n        :param model_kwargs: Keyword arguments provided to the model.\n        :return: Next `state`.\n        """"""\n        raise NotImplementedError\n\n\nclass HMC(MCMCKernel):\n    """"""\n    Hamiltonian Monte Carlo inference, using fixed trajectory length, with\n    provision for step size and mass matrix adaptation.\n\n    **References:**\n\n    1. *MCMC Using Hamiltonian Dynamics*,\n       Radford M. Neal\n\n    :param model: Python callable containing Pyro :mod:`~numpyro.primitives`.\n        If model is provided, `potential_fn` will be inferred using the model.\n    :param potential_fn: Python callable that computes the potential energy\n        given input parameters. The input parameters to `potential_fn` can be\n        any python collection type, provided that `init_params` argument to\n        `init_kernel` has the same type.\n    :param kinetic_fn: Python callable that returns the kinetic energy given\n        inverse mass matrix and momentum. If not provided, the default is\n        euclidean kinetic energy.\n    :param float step_size: Determines the size of a single step taken by the\n        verlet integrator while computing the trajectory using Hamiltonian\n        dynamics. If not specified, it will be set to 1.\n    :param bool adapt_step_size: A flag to decide if we want to adapt step_size\n        during warm-up phase using Dual Averaging scheme.\n    :param bool adapt_mass_matrix: A flag to decide if we want to adapt mass\n        matrix during warm-up phase using Welford scheme.\n    :param bool dense_mass:  A flag to decide if mass matrix is dense or\n        diagonal (default when ``dense_mass=False``)\n    :param float target_accept_prob: Target acceptance probability for step size\n        adaptation using Dual Averaging. Increasing this value will lead to a smaller\n        step size, hence the sampling will be slower but more robust. Default to 0.8.\n    :param float trajectory_length: Length of a MCMC trajectory for HMC. Default\n        value is :math:`2\\\\pi`.\n    :param callable init_strategy: a per-site initialization function.\n        See :ref:`init_strategy` section for available functions.\n    :param bool find_heuristic_step_size: whether to a heuristic function to adjust the\n        step size at the beginning of each adaptation window. Defaults to False.\n    """"""\n    def __init__(self,\n                 model=None,\n                 potential_fn=None,\n                 kinetic_fn=None,\n                 step_size=1.0,\n                 adapt_step_size=True,\n                 adapt_mass_matrix=True,\n                 dense_mass=False,\n                 target_accept_prob=0.8,\n                 trajectory_length=2 * math.pi,\n                 init_strategy=init_to_uniform,\n                 find_heuristic_step_size=False):\n        if not (model is None) ^ (potential_fn is None):\n            raise ValueError(\'Only one of `model` or `potential_fn` must be specified.\')\n        self._model = model\n        self._potential_fn = potential_fn\n        self._kinetic_fn = kinetic_fn if kinetic_fn is not None else euclidean_kinetic_energy\n        self._step_size = step_size\n        self._adapt_step_size = adapt_step_size\n        self._adapt_mass_matrix = adapt_mass_matrix\n        self._dense_mass = dense_mass\n        self._target_accept_prob = target_accept_prob\n        self._trajectory_length = trajectory_length\n        self._algo = \'HMC\'\n        self._max_tree_depth = 10\n        self._init_strategy = init_strategy\n        self._find_heuristic_step_size = find_heuristic_step_size\n        # Set on first call to init\n        self._init_fn = None\n        self._postprocess_fn = None\n        self._sample_fn = None\n\n    def _init_state(self, rng_key, model_args, model_kwargs, init_params):\n        if self._model is not None:\n            init_params, potential_fn, postprocess_fn, model_trace = initialize_model(\n                rng_key,\n                self._model,\n                dynamic_args=True,\n                model_args=model_args,\n                model_kwargs=model_kwargs)\n            if any(v[\'type\'] == \'param\' for v in model_trace.values()):\n                warnings.warn(""\'param\' sites will be treated as constants during inference. To define ""\n                              ""an improper variable, please use a \'sample\' site with log probability ""\n                              ""masked out. For example, `sample(\'x\', dist.LogNormal(0, 1).mask(False)` ""\n                              ""means that `x` has improper distribution over the positive domain."")\n            if self._init_fn is None:\n                self._init_fn, self._sample_fn = hmc(potential_fn_gen=potential_fn,\n                                                     kinetic_fn=self._kinetic_fn,\n                                                     algo=self._algo)\n            self._postprocess_fn = postprocess_fn\n        elif self._init_fn is None:\n            self._init_fn, self._sample_fn = hmc(potential_fn=self._potential_fn,\n                                                 kinetic_fn=self._kinetic_fn,\n                                                 algo=self._algo)\n\n        return init_params\n\n    @property\n    def model(self):\n        return self._model\n\n    @copy_docs_from(MCMCKernel.init)\n    def init(self, rng_key, num_warmup, init_params=None, model_args=(), model_kwargs={}):\n        # non-vectorized\n        if rng_key.ndim == 1:\n            rng_key, rng_key_init_model = random.split(rng_key)\n        # vectorized\n        else:\n            rng_key, rng_key_init_model = np.swapaxes(vmap(random.split)(rng_key), 0, 1)\n        init_params = self._init_state(rng_key_init_model, model_args, model_kwargs, init_params)\n        if self._potential_fn and init_params is None:\n            raise ValueError(\'Valid value of `init_params` must be provided with\'\n                             \' `potential_fn`.\')\n\n        hmc_init_fn = lambda init_params, rng_key: self._init_fn(  # noqa: E731\n            init_params,\n            num_warmup=num_warmup,\n            step_size=self._step_size,\n            adapt_step_size=self._adapt_step_size,\n            adapt_mass_matrix=self._adapt_mass_matrix,\n            dense_mass=self._dense_mass,\n            target_accept_prob=self._target_accept_prob,\n            trajectory_length=self._trajectory_length,\n            max_tree_depth=self._max_tree_depth,\n            find_heuristic_step_size=self._find_heuristic_step_size,\n            model_args=model_args,\n            model_kwargs=model_kwargs,\n            rng_key=rng_key,\n        )\n        if rng_key.ndim == 1:\n            init_state = hmc_init_fn(init_params, rng_key)\n        else:\n            # XXX it is safe to run hmc_init_fn under vmap despite that hmc_init_fn changes some\n            # nonlocal variables: momentum_generator, wa_update, trajectory_len, max_treedepth,\n            # wa_steps because those variables do not depend on traced args: init_params, rng_key.\n            init_state = vmap(hmc_init_fn)(init_params, rng_key)\n            sample_fn = vmap(self._sample_fn, in_axes=(0, None, None))\n            self._sample_fn = sample_fn\n        return init_state\n\n    @copy_docs_from(MCMCKernel.postprocess_fn)\n    def postprocess_fn(self, args, kwargs):\n        if self._postprocess_fn is None:\n            return identity\n        return self._postprocess_fn(*args, **kwargs)\n\n    def sample(self, state, model_args, model_kwargs):\n        """"""\n        Run HMC from the given :data:`~numpyro.infer.mcmc.HMCState` and return the resulting\n        :data:`~numpyro.infer.mcmc.HMCState`.\n\n        :param HMCState state: Represents the current state.\n        :param model_args: Arguments provided to the model.\n        :param model_kwargs: Keyword arguments provided to the model.\n        :return: Next `state` after running HMC.\n        """"""\n        return self._sample_fn(state, model_args, model_kwargs)\n\n\nclass NUTS(HMC):\n    """"""\n    Hamiltonian Monte Carlo inference, using the No U-Turn Sampler (NUTS)\n    with adaptive path length and mass matrix adaptation.\n\n    **References:**\n\n    1. *MCMC Using Hamiltonian Dynamics*,\n       Radford M. Neal\n    2. *The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo*,\n       Matthew D. Hoffman, and Andrew Gelman.\n    3. *A Conceptual Introduction to Hamiltonian Monte Carlo`*,\n       Michael Betancourt\n\n    :param model: Python callable containing Pyro :mod:`~numpyro.primitives`.\n        If model is provided, `potential_fn` will be inferred using the model.\n    :param potential_fn: Python callable that computes the potential energy\n        given input parameters. The input parameters to `potential_fn` can be\n        any python collection type, provided that `init_params` argument to\n        `init_kernel` has the same type.\n    :param kinetic_fn: Python callable that returns the kinetic energy given\n        inverse mass matrix and momentum. If not provided, the default is\n        euclidean kinetic energy.\n    :param float step_size: Determines the size of a single step taken by the\n        verlet integrator while computing the trajectory using Hamiltonian\n        dynamics. If not specified, it will be set to 1.\n    :param bool adapt_step_size: A flag to decide if we want to adapt step_size\n        during warm-up phase using Dual Averaging scheme.\n    :param bool adapt_mass_matrix: A flag to decide if we want to adapt mass\n        matrix during warm-up phase using Welford scheme.\n    :param bool dense_mass:  A flag to decide if mass matrix is dense or\n        diagonal (default when ``dense_mass=False``)\n    :param float target_accept_prob: Target acceptance probability for step size\n        adaptation using Dual Averaging. Increasing this value will lead to a smaller\n        step size, hence the sampling will be slower but more robust. Default to 0.8.\n    :param float trajectory_length: Length of a MCMC trajectory for HMC. This arg has\n        no effect in NUTS sampler.\n    :param int max_tree_depth: Max depth of the binary tree created during the doubling\n        scheme of NUTS sampler. Defaults to 10.\n    :param callable init_strategy: a per-site initialization function.\n        See :ref:`init_strategy` section for available functions.\n    :param bool find_heuristic_step_size: whether to a heuristic function to adjust the\n        step size at the beginning of each adaptation window. Defaults to False.\n    """"""\n    def __init__(self,\n                 model=None,\n                 potential_fn=None,\n                 kinetic_fn=None,\n                 step_size=1.0,\n                 adapt_step_size=True,\n                 adapt_mass_matrix=True,\n                 dense_mass=False,\n                 target_accept_prob=0.8,\n                 trajectory_length=None,\n                 max_tree_depth=10,\n                 init_strategy=init_to_uniform,\n                 find_heuristic_step_size=False):\n        super(NUTS, self).__init__(potential_fn=potential_fn, model=model, kinetic_fn=kinetic_fn,\n                                   step_size=step_size, adapt_step_size=adapt_step_size,\n                                   adapt_mass_matrix=adapt_mass_matrix, dense_mass=dense_mass,\n                                   target_accept_prob=target_accept_prob,\n                                   trajectory_length=trajectory_length,\n                                   init_strategy=init_strategy,\n                                   find_heuristic_step_size=find_heuristic_step_size)\n        self._max_tree_depth = max_tree_depth\n        self._algo = \'NUTS\'\n\n\ndef _get_proposal_loc_and_scale(samples, loc, scale, new_sample):\n    # get loc/scale of q_{-n} (Algorithm 1, line 5 of ref [1]) for n from 1 -> N\n    # these loc/scale will be stacked to the first dim; so\n    #   proposal_loc.shape[0] = proposal_loc.shape[0] = N\n    # Here, we use the numerical stability procedure in Appendix 6 of [1].\n    weight = 1 / samples.shape[0]\n    if scale.ndim > loc.ndim:\n        new_scale = cholesky_update(scale, new_sample - loc, weight)\n        proposal_scale = cholesky_update(new_scale, samples - loc, -weight)\n        proposal_scale = cholesky_update(proposal_scale, new_sample - samples, - (weight ** 2))\n    else:\n        var = np.square(scale) + weight * np.square(new_sample - loc)\n        proposal_var = var - weight * np.square(samples - loc)\n        proposal_var = proposal_var - weight ** 2 * np.square(new_sample - samples)\n        proposal_scale = np.sqrt(proposal_var)\n\n    proposal_loc = loc + weight * (new_sample - samples)\n    return proposal_loc, proposal_scale\n\n\ndef _sample_proposal(inv_mass_matrix_sqrt, rng_key, batch_shape=()):\n    eps = random.normal(rng_key, batch_shape + np.shape(inv_mass_matrix_sqrt)[:1])\n    if inv_mass_matrix_sqrt.ndim == 1:\n        r = np.multiply(inv_mass_matrix_sqrt, eps)\n    elif inv_mass_matrix_sqrt.ndim == 2:\n        r = np.matmul(inv_mass_matrix_sqrt, eps[..., None])[..., 0]\n    else:\n        raise ValueError(""Mass matrix has incorrect number of dims."")\n    return r\n\n\n# XXX: probably we need to recompute `loc`, `inv_mass_matrix_sqrt` from `zs`\n# because we might lose precision after many iterations of using _get_proposal_loc_and_scale;\n# If we recompute, we don\'t need to store `loc` and `inv_mass_matrix_sqrt` here.\n# We may also update those values every 10D iterations...\nSAAdaptState = namedtuple(\'SAAdaptState\', [\'zs\', \'pes\', \'loc\', \'inv_mass_matrix_sqrt\'])\nSAState = namedtuple(\'SAState\', [\'i\', \'z\', \'potential_energy\', \'accept_prob\',\n                                 \'mean_accept_prob\', \'diverging\', \'adapt_state\', \'rng_key\'])\n""""""\nA :func:`~collections.namedtuple` used in Sample Adaptive MCMC.\nThis consists of the following fields:\n\n - **i** - iteration. This is reset to 0 after warmup.\n - **z** - Python collection representing values (unconstrained samples from\n   the posterior) at latent sites.\n - **potential_energy** - Potential energy computed at the given value of ``z``.\n - **accept_prob** - Acceptance probability of the proposal. Note that ``z``\n   does not correspond to the proposal if it is rejected.\n - **mean_accept_prob** - Mean acceptance probability until current iteration\n   during warmup or sampling (for diagnostics).\n - **diverging** - A boolean value to indicate whether the new sample potential energy\n   is diverging from the current one.\n - **adapt_state** - A ``SAAdaptState`` namedtuple which contains adaptation information:\n\n   + **zs** - Step size to be used by the integrator in the next iteration.\n   + **pes** - Potential energies of `zs`.\n   + **loc** - Mean of those `zs`.\n   + **inv_mass_matrix_sqrt** - If using dense mass matrix, this is Cholesky of the\n     covariance of `zs`. Otherwise, this is standard deviation of those `zs`.\n\n - **rng_key** - random number generator seed used for the iteration.\n""""""\n\n\ndef _numpy_delete(x, idx):\n    """"""\n    Gets the subarray from `x` where data from index `idx` on the first axis is removed.\n    """"""\n    # NB: numpy.delete is not yet available in JAX\n    mask = np.arange(x.shape[0] - 1) < idx\n    return np.where(mask.reshape((-1,) + (1,) * (x.ndim - 1)), x[:-1], x[1:])\n\n\n# TODO: consider to expose this functional style\ndef _sa(potential_fn=None, potential_fn_gen=None):\n    wa_steps = None\n    max_delta_energy = 1000.\n\n    def init_kernel(init_params,\n                    num_warmup,\n                    adapt_state_size=None,\n                    inverse_mass_matrix=None,\n                    dense_mass=False,\n                    model_args=(),\n                    model_kwargs=None,\n                    rng_key=PRNGKey(0)):\n        nonlocal wa_steps\n        wa_steps = num_warmup\n        pe_fn = potential_fn\n        if potential_fn_gen:\n            if pe_fn is not None:\n                raise ValueError(\'Only one of `potential_fn` or `potential_fn_gen` must be provided.\')\n            else:\n                kwargs = {} if model_kwargs is None else model_kwargs\n                pe_fn = potential_fn_gen(*model_args, **kwargs)\n        rng_key_sa, rng_key_zs, rng_key_z = random.split(rng_key, 3)\n        z = init_params\n        z_flat, unravel_fn = ravel_pytree(z)\n        if inverse_mass_matrix is None:\n            inverse_mass_matrix = np.identity(z_flat.shape[-1]) if dense_mass else np.ones(z_flat.shape[-1])\n        inv_mass_matrix_sqrt = np.linalg.cholesky(inverse_mass_matrix) if dense_mass \\\n            else np.sqrt(inverse_mass_matrix)\n        if adapt_state_size is None:\n            # XXX: heuristic choice\n            adapt_state_size = 2 * z_flat.shape[-1]\n        else:\n            assert adapt_state_size > 1, \'adapt_state_size should be greater than 1.\'\n        # NB: mean is init_params\n        zs = z_flat + _sample_proposal(inv_mass_matrix_sqrt, rng_key_zs, (adapt_state_size,))\n        # compute potential energies\n        pes = lax.map(lambda z: pe_fn(unravel_fn(z)), zs)\n        if dense_mass:\n            cov = np.cov(zs, rowvar=False, bias=True)\n            if cov.shape == ():  # JAX returns scalar for 1D input\n                cov = cov.reshape((1, 1))\n            inv_mass_matrix_sqrt = np.linalg.cholesky(cov)\n        else:\n            inv_mass_matrix_sqrt = np.std(zs, 0)\n        adapt_state = SAAdaptState(zs, pes, np.mean(zs, 0), inv_mass_matrix_sqrt)\n        k = categorical_logits(rng_key_z, np.zeros(zs.shape[0]))\n        z = unravel_fn(zs[k])\n        pe = pes[k]\n        sa_state = SAState(0, z, pe, 0., 0., False, adapt_state, rng_key_sa)\n        return device_put(sa_state)\n\n    def sample_kernel(sa_state, model_args=(), model_kwargs=None):\n        pe_fn = potential_fn\n        if potential_fn_gen:\n            pe_fn = potential_fn_gen(*model_args, **model_kwargs)\n        zs, pes, loc, scale = sa_state.adapt_state\n        rng_key, rng_key_z, rng_key_reject, rng_key_accept = random.split(sa_state.rng_key, 4)\n        _, unravel_fn = ravel_pytree(sa_state.z)\n\n        z = loc + _sample_proposal(scale, rng_key_z)\n        pe = pe_fn(unravel_fn(z))\n        pe = np.where(np.isnan(pe), np.inf, pe)\n        diverging = (pe - sa_state.potential_energy) > max_delta_energy\n\n        # NB: all terms having the pattern *s will have shape N x ...\n        # and all terms having the pattern *s_ will have shape (N + 1) x ...\n        locs, scales = _get_proposal_loc_and_scale(zs, loc, scale, z)\n        zs_ = np.concatenate([zs, z[None, :]])\n        pes_ = np.concatenate([pes, pe[None]])\n        locs_ = np.concatenate([locs, loc[None, :]])\n        scales_ = np.concatenate([scales, scale[None, ...]])\n        if scale.ndim == 2:  # dense_mass\n            log_weights_ = dist.MultivariateNormal(locs_, scale_tril=scales_).log_prob(zs_) + pes_\n        else:\n            log_weights_ = dist.Normal(locs_, scales_).log_prob(zs_).sum(-1) + pes_\n        log_weights_ = np.where(np.isnan(log_weights_), -np.inf, log_weights_)\n        # get rejecting index\n        j = categorical_logits(rng_key_reject, log_weights_)\n        zs = _numpy_delete(zs_, j)\n        pes = _numpy_delete(pes_, j)\n        loc = locs_[j]\n        scale = scales_[j]\n        adapt_state = SAAdaptState(zs, pes, loc, scale)\n\n        # NB: weights[-1] / sum(weights) is the probability of rejecting the new sample `z`.\n        accept_prob = 1 - np.exp(log_weights_[-1] - logsumexp(log_weights_))\n        itr = sa_state.i + 1\n        n = np.where(sa_state.i < wa_steps, itr, itr - wa_steps)\n        mean_accept_prob = sa_state.mean_accept_prob + (accept_prob - sa_state.mean_accept_prob) / n\n\n        # XXX: we make a modification of SA sampler in [1]\n        # in [1], each MCMC state contains N points `zs`\n        # here we do resampling to pick randomly a point from those N points\n        k = categorical_logits(rng_key_accept, np.zeros(zs.shape[0]))\n        z = unravel_fn(zs[k])\n        pe = pes[k]\n        return SAState(itr, z, pe, accept_prob, mean_accept_prob, diverging, adapt_state, rng_key)\n\n    return init_kernel, sample_kernel\n\n\n# TODO: this shares almost the same code as HMC, so we can abstract out much of the implementation\nclass SA(MCMCKernel):\n    """"""\n    Sample Adaptive MCMC, a gradient-free sampler.\n\n    This is a very fast (in term of n_eff / s) sampler but requires\n    many warmup (burn-in) steps. In each MCMC step, we only need to\n    evaluate potential function at one point.\n\n    Note that unlike in reference [1], we return a randomly selected (i.e. thinned)\n    subset of approximate posterior samples of size num_chains x num_samples\n    instead of num_chains x num_samples x adapt_state_size.\n\n    .. note:: We recommend to use this kernel with `progress_bar=False` in :class:`MCMC`\n        to reduce JAX\'s dispatch overhead.\n\n    **References:**\n\n    1. *Sample Adaptive MCMC* (https://papers.nips.cc/paper/9107-sample-adaptive-mcmc),\n       Michael Zhu\n\n    :param model: Python callable containing Pyro :mod:`~numpyro.primitives`.\n        If model is provided, `potential_fn` will be inferred using the model.\n    :param potential_fn: Python callable that computes the potential energy\n        given input parameters. The input parameters to `potential_fn` can be\n        any python collection type, provided that `init_params` argument to\n        `init_kernel` has the same type.\n    :param int adapt_state_size: The number of points to generate proposal\n        distribution. Defaults to 2 times latent size.\n    :param bool dense_mass:  A flag to decide if mass matrix is dense or\n        diagonal (default to ``dense_mass=True``)\n    :param callable init_strategy: a per-site initialization function.\n        See :ref:`init_strategy` section for available functions.\n    """"""\n    def __init__(self, model=None, potential_fn=None, adapt_state_size=None,\n                 dense_mass=True, init_strategy=init_to_uniform):\n        if not (model is None) ^ (potential_fn is None):\n            raise ValueError(\'Only one of `model` or `potential_fn` must be specified.\')\n        self._model = model\n        self._potential_fn = potential_fn\n        self._adapt_state_size = adapt_state_size\n        self._dense_mass = dense_mass\n        self._init_strategy = init_strategy\n        self._init_fn = None\n        self._postprocess_fn = None\n        self._sample_fn = None\n\n    def _init_state(self, rng_key, model_args, model_kwargs, init_params):\n        if self._model is not None:\n            init_params, potential_fn, postprocess_fn, _ = initialize_model(\n                rng_key,\n                self._model,\n                dynamic_args=True,\n                model_args=model_args,\n                model_kwargs=model_kwargs)\n            init_params = init_params[0]\n            # NB: init args is different from HMC\n            self._init_fn, sample_fn = _sa(potential_fn_gen=potential_fn)\n            if self._postprocess_fn is None:\n                self._postprocess_fn = postprocess_fn\n        else:\n            self._init_fn, sample_fn = _sa(potential_fn=self._potential_fn)\n\n        if self._sample_fn is None:\n            self._sample_fn = sample_fn\n        return init_params\n\n    @copy_docs_from(MCMCKernel.init)\n    def init(self, rng_key, num_warmup, init_params=None, model_args=(), model_kwargs={}):\n        # non-vectorized\n        if rng_key.ndim == 1:\n            rng_key, rng_key_init_model = random.split(rng_key)\n        # vectorized\n        else:\n            rng_key, rng_key_init_model = np.swapaxes(vmap(random.split)(rng_key), 0, 1)\n            # we need only a single key for initializing PE / constraints fn\n            rng_key_init_model = rng_key_init_model[0]\n        init_params = self._init_state(rng_key_init_model, model_args, model_kwargs, init_params)\n        if self._potential_fn and init_params is None:\n            raise ValueError(\'Valid value of `init_params` must be provided with\'\n                             \' `potential_fn`.\')\n\n        # NB: init args is different from HMC\n        sa_init_fn = lambda init_params, rng_key: self._init_fn(  # noqa: E731\n            init_params,\n            num_warmup=num_warmup,\n            adapt_state_size=self._adapt_state_size,\n            dense_mass=self._dense_mass,\n            rng_key=rng_key,\n            model_args=model_args,\n            model_kwargs=model_kwargs,\n        )\n        if rng_key.ndim == 1:\n            init_state = sa_init_fn(init_params, rng_key)\n        else:\n            init_state = vmap(sa_init_fn)(init_params, rng_key)\n            sample_fn = vmap(self._sample_fn, in_axes=(0, None, None))\n            self._sample_fn = sample_fn\n        return init_state\n\n    @copy_docs_from(MCMCKernel.postprocess_fn)\n    def postprocess_fn(self, args, kwargs):\n        if self._postprocess_fn is None:\n            return identity\n        return self._postprocess_fn(*args, **kwargs)\n\n    def sample(self, state, model_args, model_kwargs):\n        """"""\n        Run SA from the given :data:`~numpyro.infer.mcmc.SAState` and return the resulting\n        :data:`~numpyro.infer.mcmc.SAState`.\n\n        :param SAState state: Represents the current state.\n        :param model_args: Arguments provided to the model.\n        :param model_kwargs: Keyword arguments provided to the model.\n        :return: Next `state` after running SA.\n        """"""\n        return self._sample_fn(state, model_args, model_kwargs)\n\n\ndef _get_value_from_index(xs, i):\n    return tree_map(lambda x: x[i], xs)\n\n\ndef _laxmap(f, xs):\n    n = tree_flatten(xs)[0][0].shape[0]\n\n    ys = []\n    for i in range(n):\n        x = jit(_get_value_from_index)(xs, i)\n        ys.append(f(x))\n\n    return tree_multimap(lambda *args: np.stack(args), *ys)\n\n\ndef _sample_fn_jit_args(state, sampler):\n    hmc_state, args, kwargs = state\n    return sampler.sample(hmc_state, args, kwargs), args, kwargs\n\n\ndef _sample_fn_nojit_args(state, sampler, args, kwargs):\n    # state is a tuple of size 1 - containing HMCState\n    return sampler.sample(state[0], args, kwargs),\n\n\ndef _collect_fn(collect_fields):\n    @cached_by(_collect_fn, collect_fields)\n    def collect(x):\n        return attrgetter(*collect_fields)(x[0])\n\n    return collect\n\n\n# XXX: Is there a better hash key that we can use?\ndef _hashable(x):\n    # When the arguments are JITed, ShapedArray is hashable.\n    if isinstance(x, Tracer):\n        return x\n    elif isinstance(x, DeviceArray):\n        return x.copy().tobytes()\n    elif isinstance(x, np.ndarray):\n        return x.tobytes()\n    return x\n\n\nclass MCMC(object):\n    """"""\n    Provides access to Markov Chain Monte Carlo inference algorithms in NumPyro.\n\n    .. note:: `chain_method` is an experimental arg, which might be removed in a future version.\n\n    .. note:: Setting `progress_bar=False` will improve the speed for many cases.\n\n    :param MCMCKernel sampler: an instance of :class:`~numpyro.infer.mcmc.MCMCKernel` that\n        determines the sampler for running MCMC. Currently, only :class:`~numpyro.infer.mcmc.HMC`\n        and :class:`~numpyro.infer.mcmc.NUTS` are available.\n    :param int num_warmup: Number of warmup steps.\n    :param int num_samples: Number of samples to generate from the Markov chain.\n    :param int num_chains: Number of Number of MCMC chains to run. By default,\n        chains will be run in parallel using :func:`jax.pmap`, failing which,\n        chains will be run in sequence.\n    :param postprocess_fn: Post-processing callable - used to convert a collection of unconstrained\n        sample values returned from the sampler to constrained values that lie within the support\n        of the sample sites. Additionally, this is used to return values at deterministic sites in\n        the model.\n    :param str chain_method: One of \'parallel\' (default), \'sequential\', \'vectorized\'. The method\n        \'parallel\' is used to execute the drawing process in parallel on XLA devices (CPUs/GPUs/TPUs),\n        If there are not enough devices for \'parallel\', we fall back to \'sequential\' method to draw\n        chains sequentially. \'vectorized\' method is an experimental feature which vectorizes the\n        drawing method, hence allowing us to collect samples in parallel on a single device.\n    :param bool progress_bar: Whether to enable progress bar updates. Defaults to\n        ``True``.\n    :param bool jit_model_args: If set to `True`, this will compile the potential energy\n        computation as a function of model arguments. As such, calling `MCMC.run` again\n        on a same sized but different dataset will not result in additional compilation cost.\n    """"""\n    def __init__(self,\n                 sampler,\n                 num_warmup,\n                 num_samples,\n                 num_chains=1,\n                 postprocess_fn=None,\n                 chain_method=\'parallel\',\n                 progress_bar=True,\n                 jit_model_args=False):\n        self.sampler = sampler\n        self.num_warmup = num_warmup\n        self.num_samples = num_samples\n        self.num_chains = num_chains\n        self.postprocess_fn = postprocess_fn\n        self.chain_method = chain_method\n        self.progress_bar = progress_bar\n        # TODO: We should have progress bars (maybe without diagnostics) for num_chains > 1\n        if (chain_method == \'parallel\' and num_chains > 1) or (\n                ""CI"" in os.environ or ""PYTEST_XDIST_WORKER"" in os.environ):\n            self.progress_bar = False\n        self._jit_model_args = jit_model_args\n        self._states = None\n        self._states_flat = None\n        # HMCState returned by last run\n        self._last_state = None\n        # HMCState returned by last warmup\n        self._warmup_state = None\n        # HMCState returned by hmc.init_kernel\n        self._init_state_cache = {}\n        self._cache = {}\n        self._collection_params = {}\n        self._set_collection_params()\n\n    def _get_cached_fn(self):\n        if self._jit_model_args:\n            args, kwargs = (None,), (None,)\n        else:\n            args = tree_map(lambda x: _hashable(x), self._args)\n            kwargs = tree_map(lambda x: _hashable(x), tuple(sorted(self._kwargs.items())))\n        key = args + kwargs\n        try:\n            fn = self._cache.get(key, None)\n        # If unhashable arguments are provided, proceed normally\n        # without caching\n        except TypeError:\n            fn, key = None, None\n        if fn is None:\n            if self._jit_model_args:\n                fn = partial(_sample_fn_jit_args, sampler=self.sampler)\n            else:\n                fn = partial(_sample_fn_nojit_args, sampler=self.sampler,\n                             args=self._args, kwargs=self._kwargs)\n            if key is not None:\n                self._cache[key] = fn\n        return fn\n\n    def _get_cached_init_state(self, rng_key, args, kwargs):\n        rng_key = (_hashable(rng_key),)\n        args = tree_map(lambda x: _hashable(x), args)\n        kwargs = tree_map(lambda x: _hashable(x), tuple(sorted(kwargs.items())))\n        key = rng_key + args + kwargs\n        try:\n            return self._init_state_cache.get(key, None)\n        # If unhashable arguments are provided, return None\n        except TypeError:\n            return None\n\n    def _single_chain_mcmc(self, rng_key, init_state, init_params, args, kwargs, collect_fields=(\'z\',)):\n        if init_state is None:\n            init_state = self.sampler.init(rng_key, self.num_warmup, init_params,\n                                           model_args=args, model_kwargs=kwargs)\n        if self.postprocess_fn is None:\n            postprocess_fn = self.sampler.postprocess_fn(args, kwargs)\n        else:\n            postprocess_fn = self.postprocess_fn\n        diagnostics = lambda x: get_diagnostics_str(x[0]) if rng_key.ndim == 1 else None   # noqa: E731\n        init_val = (init_state, args, kwargs) if self._jit_model_args else (init_state,)\n        lower_idx = self._collection_params[""lower""]\n        upper_idx = self._collection_params[""upper""]\n\n        collect_vals = fori_collect(lower_idx,\n                                    upper_idx,\n                                    self._get_cached_fn(),\n                                    init_val,\n                                    transform=_collect_fn(collect_fields),\n                                    progbar=self.progress_bar,\n                                    return_last_val=True,\n                                    collection_size=self._collection_params[""collection_size""],\n                                    progbar_desc=functools.partial(get_progbar_desc_str,\n                                                                   lower_idx),\n                                    diagnostics_fn=diagnostics)\n        states, last_val = collect_vals\n        # Get first argument of type `HMCState`\n        last_state = last_val[0]\n        if len(collect_fields) == 1:\n            states = (states,)\n        states = dict(zip(collect_fields, states))\n        # Apply constraints if number of samples is non-zero\n        site_values = tree_flatten(states[\'z\'])[0]\n        if len(site_values) > 0 and site_values[0].size > 0:\n            states[\'z\'] = lax.map(postprocess_fn, states[\'z\'])\n        return states, last_state\n\n    def _single_chain_jit_args(self, init, collect_fields=(\'z\',)):\n        return self._single_chain_mcmc(*init, collect_fields=collect_fields)\n\n    def _single_chain_nojit_args(self, init, model_args, model_kwargs, collect_fields=(\'z\',)):\n        return self._single_chain_mcmc(*init, model_args, model_kwargs, collect_fields=collect_fields)\n\n    def _set_collection_params(self, lower=None, upper=None, collection_size=None):\n        self._collection_params[""lower""] = self.num_warmup if lower is None else lower\n        self._collection_params[""upper""] = self.num_warmup + self.num_samples if upper is None else upper\n        self._collection_params[""collection_size""] = collection_size\n\n    def _compile(self, rng_key, *args, extra_fields=(), init_params=None, **kwargs):\n        self._set_collection_params(0, 0, self.num_samples)\n        self.run(rng_key, *args, extra_fields=extra_fields, init_params=init_params, **kwargs)\n        rng_key = (_hashable(rng_key),)\n        args = tree_map(lambda x: _hashable(x), args)\n        kwargs = tree_map(lambda x: _hashable(x), tuple(sorted(kwargs.items())))\n        key = rng_key + args + kwargs\n        try:\n            self._init_state_cache[key] = self._last_state\n        # If unhashable arguments are provided, return None\n        except TypeError:\n            pass\n\n    def warmup(self, rng_key, *args, extra_fields=(), collect_warmup=False, init_params=None, **kwargs):\n        """"""\n        Run the MCMC warmup adaptation phase. After this call, the :meth:`run` method\n        will skip the warmup adaptation phase. To run `warmup` again for the new data,\n        it is required to run :meth:`warmup` again.\n\n        :param random.PRNGKey rng_key: Random number generator key to be used for the sampling.\n        :param args: Arguments to be provided to the :meth:`numpyro.infer.mcmc.MCMCKernel.init` method.\n            These are typically the arguments needed by the `model`.\n        :param extra_fields: Extra fields (aside from `z`, `diverging`) from :data:`numpyro.infer.mcmc.HMCState`\n            to collect during the MCMC run.\n        :type extra_fields: tuple or list\n        :param bool collect_warmup: Whether to collect samples from the warmup phase. Defaults\n            to `False`.\n        :param init_params: Initial parameters to begin sampling. The type must be consistent\n            with the input type to `potential_fn`.\n        :param kwargs: Keyword arguments to be provided to the :meth:`numpyro.infer.mcmc.MCMCKernel.init`\n            method. These are typically the keyword arguments needed by the `model`.\n        """"""\n        self._warmup_state = None\n        if collect_warmup:\n            self._set_collection_params(0, self.num_warmup, self.num_warmup)\n        else:\n            self._set_collection_params(self.num_warmup, self.num_warmup, self.num_samples)\n        self.run(rng_key, *args, extra_fields=extra_fields, init_params=init_params, **kwargs)\n        self._warmup_state = self._last_state\n\n    def run(self, rng_key, *args, extra_fields=(), init_params=None, **kwargs):\n        """"""\n        Run the MCMC samplers and collect samples.\n\n        :param random.PRNGKey rng_key: Random number generator key to be used for the sampling.\n            For multi-chains, a batch of `num_chains` keys can be supplied. If `rng_key`\n            does not have batch_size, it will be split in to a batch of `num_chains` keys.\n        :param args: Arguments to be provided to the :meth:`numpyro.infer.mcmc.MCMCKernel.init` method.\n            These are typically the arguments needed by the `model`.\n        :param extra_fields: Extra fields (aside from `z`, `diverging`) from :data:`numpyro.infer.mcmc.HMCState`\n            to collect during the MCMC run.\n        :type extra_fields: tuple or list\n        :param init_params: Initial parameters to begin sampling. The type must be consistent\n            with the input type to `potential_fn`.\n        :param kwargs: Keyword arguments to be provided to the :meth:`numpyro.infer.mcmc.MCMCKernel.init`\n            method. These are typically the keyword arguments needed by the `model`.\n\n        .. note:: jax allows python code to continue even when the compiled code has not finished yet.\n            This can cause troubles when trying to profile the code for speed.\n            See https://jax.readthedocs.io/en/latest/async_dispatch.html and\n            https://jax.readthedocs.io/en/latest/profiling.html for pointers on profiling jax programs.\n        """"""\n        self._args = args\n        self._kwargs = kwargs\n        init_state = self._get_cached_init_state(rng_key, args, kwargs)\n        if self.num_chains > 1 and rng_key.ndim == 1:\n            rng_key = random.split(rng_key, self.num_chains)\n\n        if self._warmup_state is not None:\n            self._set_collection_params(0, self.num_samples, self.num_samples)\n            init_state = self._warmup_state._replace(rng_key=rng_key)\n\n        chain_method = self.chain_method\n        if chain_method == \'parallel\' and xla_bridge.device_count() < self.num_chains:\n            chain_method = \'sequential\'\n            warnings.warn(\'There are not enough devices to run parallel chains: expected {} but got {}.\'\n                          \' Chains will be drawn sequentially. If you are running MCMC in CPU,\'\n                          \' consider to use `numpyro.set_host_device_count({})` at the beginning\'\n                          \' of your program.\'\n                          .format(self.num_chains, xla_bridge.device_count(), self.num_chains))\n\n        if init_params is not None and self.num_chains > 1:\n            prototype_init_val = tree_flatten(init_params)[0][0]\n            if np.shape(prototype_init_val)[0] != self.num_chains:\n                raise ValueError(\'`init_params` must have the same leading dimension\'\n                                 \' as `num_chains`.\')\n        assert isinstance(extra_fields, (tuple, list))\n        collect_fields = tuple(set((\'z\', \'diverging\') + tuple(extra_fields)))\n        if self.num_chains == 1:\n            states_flat, last_state = self._single_chain_mcmc(rng_key, init_state, init_params,\n                                                              args, kwargs, collect_fields)\n            states = tree_map(lambda x: x[np.newaxis, ...], states_flat)\n        else:\n            if self._jit_model_args:\n                partial_map_fn = partial(self._single_chain_jit_args,\n                                         collect_fields=collect_fields)\n            else:\n                partial_map_fn = partial(self._single_chain_nojit_args,\n                                         model_args=args,\n                                         model_kwargs=kwargs,\n                                         collect_fields=collect_fields)\n            if chain_method == \'sequential\':\n                if self.progress_bar:\n                    map_fn = partial(_laxmap, partial_map_fn)\n                else:\n                    map_fn = partial(lax.map, partial_map_fn)\n            elif chain_method == \'parallel\':\n                map_fn = pmap(partial_map_fn)\n            elif chain_method == \'vectorized\':\n                map_fn = partial_map_fn\n            else:\n                raise ValueError(\'Only supporting the following methods to draw chains:\'\n                                 \' ""sequential"", ""parallel"", or ""vectorized""\')\n            if self._jit_model_args:\n                states, last_state = map_fn((rng_key, init_state, init_params, args, kwargs))\n            else:\n                states, last_state = map_fn((rng_key, init_state, init_params))\n            if chain_method == \'vectorized\':\n                # swap num_samples x num_chains to num_chains x num_samples\n                states = tree_map(lambda x: np.swapaxes(x, 0, 1), states)\n            states_flat = tree_map(lambda x: np.reshape(x, (-1,) + x.shape[2:]), states)\n        self._last_state = last_state\n        self._states = states\n        self._states_flat = states_flat\n        self._set_collection_params()\n\n    def get_samples(self, group_by_chain=False):\n        """"""\n        Get samples from the MCMC run.\n\n        :param bool group_by_chain: Whether to preserve the chain dimension. If True,\n            all samples will have num_chains as the size of their leading dimension.\n        :return: Samples having the same data type as `init_params`. The data type is a\n            `dict` keyed on site names if a model containing Pyro primitives is used,\n            but can be any :func:`jaxlib.pytree`, more generally (e.g. when defining a\n            `potential_fn` for HMC that takes `list` args).\n        """"""\n        return self._states[\'z\'] if group_by_chain else self._states_flat[\'z\']\n\n    def get_extra_fields(self, group_by_chain=False):\n        """"""\n        Get extra fields from the MCMC run.\n\n        :param bool group_by_chain: Whether to preserve the chain dimension. If True,\n            all samples will have num_chains as the size of their leading dimension.\n        :return: Extra fields keyed by field names which are specified in the\n            `extra_fields` keyword of :meth:`run`.\n        """"""\n        states = self._states if group_by_chain else self._states_flat\n        return {k: v for k, v in states.items() if k != \'z\'}\n\n    def print_summary(self, prob=0.9, exclude_deterministic=True):\n        # Exclude deterministic sites by default\n        sites = self._states[\'z\']\n        if isinstance(sites, dict) and exclude_deterministic:\n            sites = {k: v for k, v in self._states[\'z\'].items() if k in self._last_state.z}\n        print_summary(sites, prob=prob)\n        extra_fields = self.get_extra_fields()\n        if \'diverging\' in extra_fields:\n            print(""Number of divergences: {}"".format(np.sum(extra_fields[\'diverging\'])))\n'"
numpyro/infer/svi.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import namedtuple, partial\n\nfrom jax import random, value_and_grad\n\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.transforms import biject_to\nfrom numpyro.handlers import seed, trace\nfrom numpyro.infer.util import transform_fn\n\nSVIState = namedtuple(\'SVIState\', [\'optim_state\', \'rng_key\'])\n""""""\nA :func:`~collections.namedtuple` consisting of the following fields:\n - **optim_state** - current optimizer\'s state.\n - **rng_key** - random number generator seed used for the iteration.\n""""""\n\n\nclass SVI(object):\n    """"""\n    Stochastic Variational Inference given an ELBO loss objective.\n\n    :param model: Python callable with Pyro primitives for the model.\n    :param guide: Python callable with Pyro primitives for the guide\n        (recognition network).\n    :param optim: an instance of :class:`~numpyro.optim._NumpyroOptim`.\n    :param loss: ELBO loss, i.e. negative Evidence Lower Bound, to minimize.\n    :param static_kwargs: static arguments for the model / guide, i.e. arguments\n        that remain constant during fitting.\n    :return: tuple of `(init_fn, update_fn, evaluate)`.\n    """"""\n    def __init__(self, model, guide, optim, loss, **static_kwargs):\n        self.model = model\n        self.guide = guide\n        self.loss = loss\n        self.optim = optim\n        self.static_kwargs = static_kwargs\n        self.constrain_fn = None\n\n    def init(self, rng_key, *args, **kwargs):\n        """"""\n\n        :param jax.random.PRNGKey rng_key: random number generator seed.\n        :param args: arguments to the model / guide (these can possibly vary during\n            the course of fitting).\n        :param kwargs: keyword arguments to the model / guide (these can possibly vary\n            during the course of fitting).\n        :return: tuple containing initial :data:`SVIState`, and `get_params`, a callable\n            that transforms unconstrained parameter values from the optimizer to the\n            specified constrained domain\n        """"""\n        rng_key, model_seed, guide_seed = random.split(rng_key, 3)\n        model_init = seed(self.model, model_seed)\n        guide_init = seed(self.guide, guide_seed)\n        guide_trace = trace(guide_init).get_trace(*args, **kwargs, **self.static_kwargs)\n        model_trace = trace(model_init).get_trace(*args, **kwargs, **self.static_kwargs)\n        params = {}\n        inv_transforms = {}\n        # NB: params in model_trace will be overwritten by params in guide_trace\n        for site in list(model_trace.values()) + list(guide_trace.values()):\n            if site[\'type\'] == \'param\':\n                constraint = site[\'kwargs\'].pop(\'constraint\', constraints.real)\n                transform = biject_to(constraint)\n                inv_transforms[site[\'name\']] = transform\n                params[site[\'name\']] = transform.inv(site[\'value\'])\n\n        self.constrain_fn = partial(transform_fn, inv_transforms)\n        return SVIState(self.optim.init(params), rng_key)\n\n    def get_params(self, svi_state):\n        """"""\n        Gets values at `param` sites of the `model` and `guide`.\n\n        :param svi_state: current state of the optimizer.\n        """"""\n        params = self.constrain_fn(self.optim.get_params(svi_state.optim_state))\n        return params\n\n    def update(self, svi_state, *args, **kwargs):\n        """"""\n        Take a single step of SVI (possibly on a batch / minibatch of data),\n        using the optimizer.\n\n        :param svi_state: current state of SVI.\n        :param args: arguments to the model / guide (these can possibly vary during\n            the course of fitting).\n        :param kwargs: keyword arguments to the model / guide (these can possibly vary\n            during the course of fitting).\n        :return: tuple of `(svi_state, loss)`.\n        """"""\n        rng_key, rng_key_step = random.split(svi_state.rng_key)\n        params = self.optim.get_params(svi_state.optim_state)\n        loss_val, grads = value_and_grad(\n            lambda x: self.loss.loss(rng_key_step, self.constrain_fn(x), self.model, self.guide,\n                                     *args, **kwargs, **self.static_kwargs))(params)\n        optim_state = self.optim.update(grads, svi_state.optim_state)\n        return SVIState(optim_state, rng_key), loss_val\n\n    def evaluate(self, svi_state, *args, **kwargs):\n        """"""\n        Take a single step of SVI (possibly on a batch / minibatch of data).\n\n        :param svi_state: current state of SVI.\n        :param args: arguments to the model / guide (these can possibly vary during\n            the course of fitting).\n        :param kwargs: keyword arguments to the model / guide.\n        :return: evaluate ELBO loss given the current parameter values\n            (held within `svi_state.optim_state`).\n        """"""\n        # we split to have the same seed as `update_fn` given an svi_state\n        _, rng_key_eval = random.split(svi_state.rng_key)\n        params = self.get_params(svi_state)\n        return self.loss.loss(rng_key_eval, params, self.model, self.guide,\n                              *args, **kwargs, **self.static_kwargs)\n'"
numpyro/infer/util.py,10,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom collections import namedtuple\nfrom functools import partial\nimport warnings\n\nfrom jax import device_get, lax, random, value_and_grad, vmap\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as np\n\nimport numpyro\nfrom numpyro.distributions.constraints import _GreaterThan, _Interval\nfrom numpyro.distributions.transforms import biject_to\nfrom numpyro.distributions.util import sum_rightmost\nfrom numpyro.handlers import seed, substitute, trace\nfrom numpyro.infer.initialization import init_to_uniform, init_to_value\nfrom numpyro.util import not_jax_tracer, while_loop\n\n__all__ = [\n    \'find_valid_initial_params\',\n    \'get_potential_fn\',\n    \'log_density\',\n    \'log_likelihood\',\n    \'potential_energy\',\n    \'initialize_model\',\n    \'Predictive\',\n]\n\nModelInfo = namedtuple(\'ModelInfo\', [\'param_info\', \'potential_fn\', \'postprocess_fn\', \'model_trace\'])\nParamInfo = namedtuple(\'ParamInfo\', [\'z\', \'potential_energy\', \'z_grad\'])\n\n\ndef log_density(model, model_args, model_kwargs, params):\n    """"""\n    (EXPERIMENTAL INTERFACE) Computes log of joint density for the model given\n    latent values ``params``.\n\n    :param model: Python callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: dictionary of current parameter values keyed by site\n        name.\n    :return: log of joint density and a corresponding model trace\n    """"""\n    model = substitute(model, param_map=params)\n    model_trace = trace(model).get_trace(*model_args, **model_kwargs)\n    log_joint = np.array(0.)\n    for site in model_trace.values():\n        if site[\'type\'] == \'sample\':\n            value = site[\'value\']\n            intermediates = site[\'intermediates\']\n            mask = site[\'mask\']\n            scale = site[\'scale\']\n            # Early exit when all elements are masked\n            if not_jax_tracer(mask) and mask is not None and not np.any(mask):\n                continue\n            if intermediates:\n                log_prob = site[\'fn\'].log_prob(value, intermediates)\n            else:\n                log_prob = site[\'fn\'].log_prob(value)\n\n            # Minor optimizations\n            # XXX: note that this may not work correctly for dynamic masks, provide\n            # explicit jax.DeviceArray for masking.\n            if mask is not None:\n                if scale is not None:\n                    log_prob = np.where(mask, scale * log_prob, 0.)\n                else:\n                    log_prob = np.where(mask, log_prob, 0.)\n            else:\n                if scale is not None:\n                    log_prob = scale * log_prob\n            log_prob = np.sum(log_prob)\n            log_joint = log_joint + log_prob\n    return log_joint, model_trace\n\n\ndef transform_fn(transforms, params, invert=False):\n    """"""\n    (EXPERIMENTAL INTERFACE) Callable that applies a transformation from the `transforms`\n    dict to values in the `params` dict and returns the transformed values keyed on\n    the same names.\n\n    :param transforms: Dictionary of transforms keyed by names. Names in\n        `transforms` and `params` should align.\n    :param params: Dictionary of arrays keyed by names.\n    :param invert: Whether to apply the inverse of the transforms.\n    :return: `dict` of transformed params.\n    """"""\n    if invert:\n        transforms = {k: v.inv for k, v in transforms.items()}\n    return {k: transforms[k](v) if k in transforms else v\n            for k, v in params.items()}\n\n\ndef constrain_fn(model, model_args, model_kwargs, params, return_deterministic=False):\n    """"""\n    (EXPERIMENTAL INTERFACE) Gets value at each latent site in `model` given\n    unconstrained parameters `params`. The `transforms` is used to transform these\n    unconstrained parameters to base values of the corresponding priors in `model`.\n    If a prior is a transformed distribution, the corresponding base value lies in\n    the support of base distribution. Otherwise, the base value lies in the support\n    of the distribution.\n\n    :param model: a callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: dictionary of unconstrained values keyed by site\n        names.\n    :param bool return_deterministic: whether to return the value of `deterministic`\n        sites from the model. Defaults to `False`.\n    :return: `dict` of transformed params.\n    """"""\n    def substitute_fn(site):\n        if site[\'name\'] in params:\n            return biject_to(site[\'fn\'].support)(params[site[\'name\']])\n\n    substituted_model = substitute(model, substitute_fn=substitute_fn)\n    model_trace = trace(substituted_model).get_trace(*model_args, **model_kwargs)\n    return {k: v[\'value\'] for k, v in model_trace.items() if (k in params) or\n            (return_deterministic and (v[\'type\'] == \'deterministic\'))}\n\n\ndef _unconstrain_reparam(params, site):\n    name = site[\'name\']\n    if name in params:\n        p = params[name]\n        t = biject_to(site[\'fn\'].support)\n        value = t(p)\n\n        log_det = t.log_abs_det_jacobian(p, value)\n        log_det = sum_rightmost(log_det, np.ndim(log_det) - np.ndim(value) + len(site[\'fn\'].event_shape))\n        if site[\'scale\'] is not None:\n            log_det = site[\'scale\'] * log_det\n        numpyro.factor(\'_{}_log_det\'.format(name), log_det)\n        return value\n\n\ndef potential_energy(model, model_args, model_kwargs, params):\n    """"""\n    (EXPERIMENTAL INTERFACE) Computes potential energy of a model given unconstrained params.\n    The `inv_transforms` is used to transform these unconstrained parameters to base values\n    of the corresponding priors in `model`. If a prior is a transformed distribution,\n    the corresponding base value lies in the support of base distribution. Otherwise,\n    the base value lies in the support of the distribution.\n\n    :param model: a callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: unconstrained parameters of `model`.\n    :return: potential energy given unconstrained parameters.\n    """"""\n    substituted_model = substitute(model, substitute_fn=partial(_unconstrain_reparam, params))\n    # no param is needed for log_density computation because we already substitute\n    log_joint, model_trace = log_density(substituted_model, model_args, model_kwargs, {})\n    return - log_joint\n\n\ndef _init_to_unconstrained_value(site=None, values={}):\n    if site is None:\n        return partial(init_to_value, values=values)\n\n\ndef find_valid_initial_params(rng_key, model,\n                              init_strategy=init_to_uniform,\n                              model_args=(),\n                              model_kwargs=None,\n                              prototype_params=None):\n    """"""\n    (EXPERIMENTAL INTERFACE) Given a model with Pyro primitives, returns an initial\n    valid unconstrained value for all the parameters. This function also returns\n    the corresponding potential energy, the gradients, and an\n    `is_valid` flag to say whether the initial parameters are valid. Parameter values\n    are considered valid if the values and the gradients for the log density have\n    finite values.\n\n    :param jax.random.PRNGKey rng_key: random number generator seed to\n        sample from the prior. The returned `init_params` will have the\n        batch shape ``rng_key.shape[:-1]``.\n    :param model: Python callable containing Pyro primitives.\n    :param callable init_strategy: a per-site initialization function.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict prototype_params: an optional prototype parameters, which is used\n        to define the shape for initial parameters.\n    :return: tuple of `init_params_info` and `is_valid`, where `init_params_info` is the tuple\n        containing the initial params, their potential energy, and their gradients.\n    """"""\n    model_kwargs = {} if model_kwargs is None else model_kwargs\n    init_strategy = init_strategy if isinstance(init_strategy, partial) else init_strategy()\n    # handle those init strategies differently to save computation\n    if init_strategy.func is init_to_uniform:\n        radius = init_strategy.keywords.get(""radius"")\n        init_values = {}\n    elif init_strategy.func is _init_to_unconstrained_value:\n        radius = 2\n        init_values = init_strategy.keywords.get(""values"")\n    else:\n        radius = None\n\n    def cond_fn(state):\n        i, _, _, is_valid = state\n        return (i < 100) & (~is_valid)\n\n    def body_fn(state):\n        i, key, _, _ = state\n        key, subkey = random.split(key)\n\n        if radius is None or prototype_params is None:\n            # Wrap model in a `substitute` handler to initialize from `init_loc_fn`.\n            seeded_model = substitute(seed(model, subkey), substitute_fn=init_strategy)\n            model_trace = trace(seeded_model).get_trace(*model_args, **model_kwargs)\n            constrained_values, inv_transforms = {}, {}\n            for k, v in model_trace.items():\n                if v[\'type\'] == \'sample\' and not v[\'is_observed\'] and not v[\'fn\'].is_discrete:\n                    constrained_values[k] = v[\'value\']\n                    inv_transforms[k] = biject_to(v[\'fn\'].support)\n            params = transform_fn(inv_transforms,\n                                  {k: v for k, v in constrained_values.items()},\n                                  invert=True)\n        else:  # this branch doesn\'t require tracing the model\n            params = {}\n            for k, v in prototype_params.items():\n                if k in init_values:\n                    params[k] = init_values[k]\n                else:\n                    params[k] = random.uniform(subkey, np.shape(v), minval=-radius, maxval=radius)\n                    key, subkey = random.split(key)\n\n        potential_fn = partial(potential_energy, model, model_args, model_kwargs)\n        pe, z_grad = value_and_grad(potential_fn)(params)\n        z_grad_flat = ravel_pytree(z_grad)[0]\n        is_valid = np.isfinite(pe) & np.all(np.isfinite(z_grad_flat))\n        return i + 1, key, (params, pe, z_grad), is_valid\n\n    def _find_valid_params(rng_key, exit_early=False):\n        init_state = (0, rng_key, (prototype_params, 0., prototype_params), False)\n        if exit_early and not_jax_tracer(rng_key):\n            # Early return if valid params found. This is only helpful for single chain,\n            # where we can avoid compiling body_fn in while_loop.\n            _, _, (init_params, pe, z_grad), is_valid = init_state = body_fn(init_state)\n            if not_jax_tracer(is_valid):\n                if device_get(is_valid):\n                    return (init_params, pe, z_grad), is_valid\n\n        # XXX: this requires compiling the model, so for multi-chain, we trace the model 2-times\n        # even if the init_state is a valid result\n        _, _, (init_params, pe, z_grad), is_valid = while_loop(cond_fn, body_fn, init_state)\n        return (init_params, pe, z_grad), is_valid\n\n    # Handle possible vectorization\n    if rng_key.ndim == 1:\n        (init_params, pe, z_grad), is_valid = _find_valid_params(rng_key, exit_early=True)\n    else:\n        (init_params, pe, z_grad), is_valid = lax.map(_find_valid_params, rng_key)\n    return (init_params, pe, z_grad), is_valid\n\n\ndef _get_model_transforms(model, model_args=(), model_kwargs=None):\n    model_kwargs = {} if model_kwargs is None else model_kwargs\n    model_trace = trace(model).get_trace(*model_args, **model_kwargs)\n    inv_transforms = {}\n    # model code may need to be replayed in the presence of deterministic sites\n    replay_model = False\n    for k, v in model_trace.items():\n        if v[\'type\'] == \'sample\' and not v[\'is_observed\'] and not v[\'fn\'].is_discrete:\n            support = v[\'fn\'].support\n            inv_transforms[k] = biject_to(support)\n            # XXX: the following code filters out most situations with dynamic supports\n            args = ()\n            if isinstance(support, _GreaterThan):\n                args = (\'lower_bound\',)\n            elif isinstance(support, _Interval):\n                args = (\'lower_bound\', \'upper_bound\')\n            for arg in args:\n                if not isinstance(getattr(support, arg), (int, float)):\n                    replay_model = True\n        elif v[\'type\'] == \'deterministic\':\n            replay_model = True\n    return inv_transforms, replay_model, model_trace\n\n\ndef get_potential_fn(model, inv_transforms, replay_model=False, dynamic_args=False, model_args=(), model_kwargs=None):\n    """"""\n    (EXPERIMENTAL INTERFACE) Given a model with Pyro primitives, returns a\n    function which, given unconstrained parameters, evaluates the potential\n    energy (negative log joint density). In addition, this returns a\n    function to transform unconstrained values at sample sites to constrained\n    values within their respective support.\n\n    :param model: Python callable containing Pyro primitives.\n    :param dict inv_transforms: dictionary of transforms keyed by names.\n    :param bool replay_model: whether we need to replay model in\n        `postprocess_fn` to obtain `deterministic` sites.\n    :param bool dynamic_args: if `True`, the `potential_fn` and\n        `constraints_fn` are themselves dependent on model arguments.\n        When provided a `*model_args, **model_kwargs`, they return\n        `potential_fn` and `constraints_fn` callables, respectively.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :return: tuple of (`potential_fn`, `postprocess_fn`). The latter is used\n        to constrain unconstrained samples (e.g. those returned by HMC)\n        to values that lie within the site\'s support, and return values at\n        `deterministic` sites in the model.\n    """"""\n    if dynamic_args:\n        def potential_fn(*args, **kwargs):\n            return partial(potential_energy, model, args, kwargs)\n\n        def postprocess_fn(*args, **kwargs):\n            if replay_model:\n                return partial(constrain_fn, model, args, kwargs, return_deterministic=True)\n            else:\n                return partial(transform_fn, inv_transforms)\n    else:\n        model_kwargs = {} if model_kwargs is None else model_kwargs\n        potential_fn = partial(potential_energy, model, model_args, model_kwargs)\n        if replay_model:\n            postprocess_fn = partial(constrain_fn, model, model_args, model_kwargs,\n                                     return_deterministic=True)\n        else:\n            postprocess_fn = partial(transform_fn, inv_transforms)\n\n    return potential_fn, postprocess_fn\n\n\ndef initialize_model(rng_key, model,\n                     init_strategy=init_to_uniform,\n                     dynamic_args=False,\n                     model_args=(),\n                     model_kwargs=None):\n    """"""\n    (EXPERIMENTAL INTERFACE) Helper function that calls :func:`~numpyro.infer.util.get_potential_fn`\n    and :func:`~numpyro.infer.util.find_valid_initial_params` under the hood\n    to return a tuple of (`init_params_info`, `potential_fn`, `postprocess_fn`, `model_trace`).\n\n    :param jax.random.PRNGKey rng_key: random number generator seed to\n        sample from the prior. The returned `init_params` will have the\n        batch shape ``rng_key.shape[:-1]``.\n    :param model: Python callable containing Pyro primitives.\n    :param callable init_strategy: a per-site initialization function.\n        See :ref:`init_strategy` section for available functions.\n    :param bool dynamic_args: if `True`, the `potential_fn` and\n        `constraints_fn` are themselves dependent on model arguments.\n        When provided a `*model_args, **model_kwargs`, they return\n        `potential_fn` and `constraints_fn` callables, respectively.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :return: a namedtupe `ModelInfo` which contains the fields\n        (`param_info`, `potential_fn`, `postprocess_fn`, `model_trace`), where\n        `param_info` is a namedtuple `ParamInfo` containing values from the prior\n        used to initiate MCMC, their corresponding potential energy, and their gradients;\n        `postprocess_fn` is a callable that uses inverse transforms\n        to convert unconstrained HMC samples to constrained values that\n        lie within the site\'s support, in addition to returning values\n        at `deterministic` sites in the model.\n    """"""\n    model_kwargs = {} if model_kwargs is None else model_kwargs\n    substituted_model = substitute(seed(model, rng_key if np.ndim(rng_key) == 1 else rng_key[0]),\n                                   substitute_fn=init_strategy)\n    inv_transforms, replay_model, model_trace = _get_model_transforms(\n        substituted_model, model_args, model_kwargs)\n    constrained_values = {k: v[\'value\'] for k, v in model_trace.items()\n                          if v[\'type\'] == \'sample\' and not v[\'is_observed\']\n                          and not v[\'fn\'].is_discrete}\n\n    potential_fn, postprocess_fn = get_potential_fn(model,\n                                                    inv_transforms,\n                                                    replay_model=replay_model,\n                                                    dynamic_args=dynamic_args,\n                                                    model_args=model_args,\n                                                    model_kwargs=model_kwargs)\n\n    init_strategy = init_strategy if isinstance(init_strategy, partial) else init_strategy()\n    if init_strategy.func is init_to_value:\n        init_values = init_strategy.keywords.get(""values"")\n        unconstrained_values = transform_fn(inv_transforms, init_values, invert=True)\n        init_strategy = _init_to_unconstrained_value(values=unconstrained_values)\n    prototype_params = transform_fn(inv_transforms, constrained_values, invert=True)\n    (init_params, pe, grad), is_valid = find_valid_initial_params(rng_key, model,\n                                                                  init_strategy=init_strategy,\n                                                                  model_args=model_args,\n                                                                  model_kwargs=model_kwargs,\n                                                                  prototype_params=prototype_params)\n\n    if not_jax_tracer(is_valid):\n        if device_get(~np.all(is_valid)):\n            raise RuntimeError(""Cannot find valid initial parameters. Please check your model again."")\n    return ModelInfo(ParamInfo(init_params, pe, grad), potential_fn, postprocess_fn, model_trace)\n\n\ndef _predictive(rng_key, model, posterior_samples, num_samples, return_sites=None,\n                parallel=True, model_args=(), model_kwargs={}):\n    rng_keys = random.split(rng_key, num_samples)\n\n    def single_prediction(val):\n        rng_key, samples = val\n        model_trace = trace(seed(substitute(model, samples), rng_key)).get_trace(\n            *model_args, **model_kwargs)\n        if return_sites is not None:\n            if return_sites == \'\':\n                sites = {k for k, site in model_trace.items() if site[\'type\'] != \'plate\'}\n            else:\n                sites = return_sites\n        else:\n            sites = {k for k, site in model_trace.items()\n                     if (site[\'type\'] == \'sample\' and k not in samples) or (site[\'type\'] == \'deterministic\')}\n        return {name: site[\'value\'] for name, site in model_trace.items() if name in sites}\n\n    if parallel:\n        return vmap(single_prediction)((rng_keys, posterior_samples))\n    else:\n        return lax.map(single_prediction, (rng_keys, posterior_samples))\n\n\nclass Predictive(object):\n    """"""\n    This class is used to construct predictive distribution. The predictive distribution is obtained\n    by running model conditioned on latent samples from `posterior_samples`.\n\n    .. warning::\n        The interface for the `Predictive` class is experimental, and\n        might change in the future.\n\n    :param model: Python callable containing Pyro primitives.\n    :param dict posterior_samples: dictionary of samples from the posterior.\n    :param callable guide: optional guide to get posterior samples of sites not present\n        in `posterior_samples`.\n    :param dict params: dictionary of values for param sites of model/guide.\n    :param int num_samples: number of samples\n    :param list return_sites: sites to return; by default only sample sites not present\n        in `posterior_samples` are returned.\n    :param bool parallel: whether to predict in parallel using JAX vectorized map :func:`jax.vmap`.\n        Defaults to False.\n\n    :return: dict of samples from the predictive distribution.\n    """"""\n    def __init__(self, model, posterior_samples=None, guide=None, params=None, num_samples=None,\n                 return_sites=None, parallel=False):\n        if posterior_samples is None and num_samples is None:\n            raise ValueError(""Either posterior_samples or num_samples must be specified."")\n\n        posterior_samples = {} if posterior_samples is None else posterior_samples\n\n        for name, sample in posterior_samples.items():\n            batch_size = sample.shape[0]\n            if (num_samples is not None) and (num_samples != batch_size):\n                warnings.warn(""Sample\'s leading dimension size {} is different from the ""\n                              ""provided {} num_samples argument. Defaulting to {}.""\n                              .format(batch_size, num_samples, batch_size), UserWarning)\n            num_samples = batch_size\n\n        if num_samples is None:\n            raise ValueError(""No sample sites in posterior samples to infer `num_samples`."")\n\n        if return_sites is not None:\n            assert isinstance(return_sites, (list, tuple, set))\n\n        self.model = model\n        self.posterior_samples = {} if posterior_samples is None else posterior_samples\n        self.num_samples = num_samples\n        self.guide = guide\n        self.params = {} if params is None else params\n        self.return_sites = return_sites\n        self.parallel = parallel\n\n    def __call__(self, rng_key, *args, **kwargs):\n        """"""\n        Returns dict of samples from the predictive distribution. By default, only sample sites not\n        contained in `posterior_samples` are returned. This can be modified by changing the\n        `return_sites` keyword argument of this :class:`Predictive` instance.\n\n        :param jax.random.PRNGKey rng_key: random key to draw samples.\n        :param args: model arguments.\n        :param kwargs: model kwargs.\n        """"""\n        posterior_samples = self.posterior_samples\n        if self.guide is not None:\n            rng_key, guide_rng_key = random.split(rng_key)\n            # use return_sites=\'\' as a special signal to return all sites\n            guide = substitute(self.guide, self.params)\n            posterior_samples = _predictive(guide_rng_key, guide, posterior_samples,\n                                            self.num_samples, return_sites=\'\', parallel=self.parallel,\n                                            model_args=args, model_kwargs=kwargs)\n        model = substitute(self.model, self.params)\n        return _predictive(rng_key, model, posterior_samples, self.num_samples,\n                           return_sites=self.return_sites, parallel=self.parallel,\n                           model_args=args, model_kwargs=kwargs)\n\n    def get_samples(self, rng_key, *args, **kwargs):\n        warnings.warn(""The method `.get_samples` has been deprecated in favor of `.__call__`."",\n                      DeprecationWarning)\n        return self.__call__(rng_key, *args, **kwargs)\n\n\ndef log_likelihood(model, posterior_samples, *args, **kwargs):\n    """"""\n    (EXPERIMENTAL INTERFACE) Returns log likelihood at observation nodes of model,\n    given samples of all latent variables.\n\n    :param model: Python callable containing Pyro primitives.\n    :param dict posterior_samples: dictionary of samples from the posterior.\n    :param args: model arguments.\n    :param kwargs: model kwargs.\n    :return: dict of log likelihoods at observation sites.\n    """"""\n    def single_loglik(samples):\n        model_trace = trace(substitute(model, samples)).get_trace(*args, **kwargs)\n        return {name: site[\'fn\'].log_prob(site[\'value\']) for name, site in model_trace.items()\n                if site[\'type\'] == \'sample\' and site[\'is_observed\']}\n\n    return vmap(single_loglik)(posterior_samples)\n'"
test/contrib/test_autoguide.py,20,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import partial\n\nfrom numpy.testing import assert_allclose\nimport pytest\n\nfrom jax import random\nimport jax.numpy as np\nfrom jax.test_util import check_eq\n\nimport numpyro\nfrom numpyro import optim\nfrom numpyro.contrib.autoguide import (\n    AutoDiagonalNormal,\n    AutoIAFNormal,\n    AutoBNAFNormal,\n    AutoLaplaceApproximation,\n    AutoLowRankMultivariateNormal,\n    AutoMultivariateNormal\n)\nfrom numpyro.contrib.nn.auto_reg_nn import AutoregressiveNN\nfrom numpyro.contrib.reparam import TransformReparam, reparam\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints, transforms\nfrom numpyro.distributions.flows import InverseAutoregressiveTransform\nfrom numpyro.handlers import substitute\nfrom numpyro.infer import ELBO, SVI\nfrom numpyro.infer.initialization import init_to_median\nfrom numpyro.util import fori_loop\n\ninit_strategy = init_to_median(num_samples=2)\n\n\n@pytest.mark.parametrize(\'auto_class\', [\n    AutoDiagonalNormal,\n    AutoIAFNormal,\n    AutoBNAFNormal,\n    AutoMultivariateNormal,\n    AutoLaplaceApproximation,\n    AutoLowRankMultivariateNormal,\n])\ndef test_beta_bernoulli(auto_class):\n    data = np.array([[1.0] * 8 + [0.0] * 2,\n                     [1.0] * 4 + [0.0] * 6]).T\n\n    def model(data):\n        f = numpyro.sample(\'beta\', dist.Beta(np.ones(2), np.ones(2)))\n        numpyro.sample(\'obs\', dist.Bernoulli(f), obs=data)\n\n    adam = optim.Adam(0.01)\n    guide = auto_class(model, init_strategy=init_strategy)\n    svi = SVI(model, guide, adam, ELBO())\n    svi_state = svi.init(random.PRNGKey(1), data)\n\n    def body_fn(i, val):\n        svi_state, loss = svi.update(val, data)\n        return svi_state\n\n    svi_state = fori_loop(0, 3000, body_fn, svi_state)\n    params = svi.get_params(svi_state)\n    true_coefs = (np.sum(data, axis=0) + 1) / (data.shape[0] + 2)\n    # test .sample_posterior method\n    posterior_samples = guide.sample_posterior(random.PRNGKey(1), params, sample_shape=(1000,))\n    assert_allclose(np.mean(posterior_samples[\'beta\'], 0), true_coefs, atol=0.05)\n\n\n@pytest.mark.parametrize(\'auto_class\', [\n    AutoDiagonalNormal,\n    AutoIAFNormal,\n    AutoBNAFNormal,\n    AutoMultivariateNormal,\n    AutoLaplaceApproximation,\n    AutoLowRankMultivariateNormal,\n])\ndef test_logistic_regression(auto_class):\n    N, dim = 3000, 3\n    data = random.normal(random.PRNGKey(0), (N, dim))\n    true_coefs = np.arange(1., dim + 1.)\n    logits = np.sum(true_coefs * data, axis=-1)\n    labels = dist.Bernoulli(logits=logits).sample(random.PRNGKey(1))\n\n    def model(data, labels):\n        coefs = numpyro.sample(\'coefs\', dist.Normal(np.zeros(dim), np.ones(dim)))\n        logits = np.sum(coefs * data, axis=-1)\n        return numpyro.sample(\'obs\', dist.Bernoulli(logits=logits), obs=labels)\n\n    adam = optim.Adam(0.01)\n    rng_key_init = random.PRNGKey(1)\n    guide = auto_class(model, init_strategy=init_strategy)\n    svi = SVI(model, guide, adam, ELBO())\n    svi_state = svi.init(rng_key_init, data, labels)\n\n    def body_fn(i, val):\n        svi_state, loss = svi.update(val, data, labels)\n        return svi_state\n\n    svi_state = fori_loop(0, 2000, body_fn, svi_state)\n    params = svi.get_params(svi_state)\n    if auto_class not in (AutoIAFNormal, AutoBNAFNormal):\n        median = guide.median(params)\n        assert_allclose(median[\'coefs\'], true_coefs, rtol=0.1)\n        # test .quantile method\n        median = guide.quantiles(params, [0.2, 0.5])\n        assert_allclose(median[\'coefs\'][1], true_coefs, rtol=0.1)\n    # test .sample_posterior method\n    posterior_samples = guide.sample_posterior(random.PRNGKey(1), params, sample_shape=(1000,))\n    assert_allclose(np.mean(posterior_samples[\'coefs\'], 0), true_coefs, rtol=0.1)\n\n\ndef test_iaf():\n    # test for substitute logic for exposed methods `sample_posterior` and `get_transforms`\n    N, dim = 3000, 3\n    data = random.normal(random.PRNGKey(0), (N, dim))\n    true_coefs = np.arange(1., dim + 1.)\n    logits = np.sum(true_coefs * data, axis=-1)\n    labels = dist.Bernoulli(logits=logits).sample(random.PRNGKey(1))\n\n    def model(data, labels):\n        coefs = numpyro.sample(\'coefs\', dist.Normal(np.zeros(dim), np.ones(dim)))\n        offset = numpyro.sample(\'offset\', dist.Uniform(-1, 1))\n        logits = offset + np.sum(coefs * data, axis=-1)\n        return numpyro.sample(\'obs\', dist.Bernoulli(logits=logits), obs=labels)\n\n    adam = optim.Adam(0.01)\n    rng_key_init = random.PRNGKey(1)\n    guide = AutoIAFNormal(model)\n    svi = SVI(model, guide, adam, ELBO())\n    svi_state = svi.init(rng_key_init, data, labels)\n    params = svi.get_params(svi_state)\n\n    x = random.normal(random.PRNGKey(0), (dim + 1,))\n    rng_key = random.PRNGKey(1)\n    actual_sample = guide.sample_posterior(rng_key, params)\n    actual_output = guide._unpack_latent(guide.get_transform(params)(x))\n\n    flows = []\n    for i in range(guide.num_flows):\n        if i > 0:\n            flows.append(transforms.PermuteTransform(np.arange(dim + 1)[::-1]))\n        arn_init, arn_apply = AutoregressiveNN(dim + 1, [dim + 1, dim + 1],\n                                               permutation=np.arange(dim + 1),\n                                               skip_connections=guide._skip_connections,\n                                               nonlinearity=guide._nonlinearity)\n        arn = partial(arn_apply, params[\'auto_arn__{}$params\'.format(i)])\n        flows.append(InverseAutoregressiveTransform(arn))\n    flows.append(guide._unpack_latent)\n\n    transform = transforms.ComposeTransform(flows)\n    _, rng_key_sample = random.split(rng_key)\n    expected_sample = transform(dist.Normal(np.zeros(dim + 1), 1).sample(rng_key_sample))\n    expected_output = transform(x)\n    assert_allclose(actual_sample[\'coefs\'], expected_sample[\'coefs\'])\n    assert_allclose(actual_sample[\'offset\'],\n                    transforms.biject_to(constraints.interval(-1, 1))(expected_sample[\'offset\']))\n    check_eq(actual_output, expected_output)\n\n\ndef test_uniform_normal():\n    true_coef = 0.9\n    data = true_coef + random.normal(random.PRNGKey(0), (1000,))\n\n    def model(data):\n        alpha = numpyro.sample(\'alpha\', dist.Uniform(0, 1))\n        with reparam(config={\'loc\': TransformReparam()}):\n            loc = numpyro.sample(\'loc\', dist.Uniform(0, alpha))\n        numpyro.sample(\'obs\', dist.Normal(loc, 0.1), obs=data)\n\n    adam = optim.Adam(0.01)\n    rng_key_init = random.PRNGKey(1)\n    guide = AutoDiagonalNormal(model)\n    svi = SVI(model, guide, adam, ELBO())\n    svi_state = svi.init(rng_key_init, data)\n\n    def body_fn(i, val):\n        svi_state, loss = svi.update(val, data)\n        return svi_state\n\n    svi_state = fori_loop(0, 1000, body_fn, svi_state)\n    params = svi.get_params(svi_state)\n    median = guide.median(params)\n    assert_allclose(median[\'loc\'], true_coef, rtol=0.05)\n    # test .quantile method\n    median = guide.quantiles(params, [0.2, 0.5])\n    assert_allclose(median[\'loc\'][1], true_coef, rtol=0.1)\n\n\ndef test_param():\n    # this test the validity of model having\n    # param sites contain composed transformed constraints\n    rng_keys = random.split(random.PRNGKey(0), 3)\n    a_minval = 1\n    a_init = np.exp(random.normal(rng_keys[0])) + a_minval\n    b_init = np.exp(random.normal(rng_keys[1]))\n    x_init = random.normal(rng_keys[2])\n\n    def model():\n        a = numpyro.param(\'a\', a_init, constraint=constraints.greater_than(a_minval))\n        b = numpyro.param(\'b\', b_init, constraint=constraints.positive)\n        numpyro.sample(\'x\', dist.Normal(a, b))\n\n    # this class is used to force init value of `x` to x_init\n    class _AutoGuide(AutoDiagonalNormal):\n        def __call__(self, *args, **kwargs):\n            return substitute(super(_AutoGuide, self).__call__,\n                              {\'_auto_latent\': x_init})(*args, **kwargs)\n\n    adam = optim.Adam(0.01)\n    rng_key_init = random.PRNGKey(1)\n    guide = _AutoGuide(model)\n    svi = SVI(model, guide, adam, ELBO())\n    svi_state = svi.init(rng_key_init)\n\n    params = svi.get_params(svi_state)\n    assert_allclose(params[\'a\'], a_init)\n    assert_allclose(params[\'b\'], b_init)\n    assert_allclose(params[\'auto_loc\'], guide._init_latent)\n    assert_allclose(params[\'auto_scale\'], np.ones(1) * guide._init_scale)\n\n    actual_loss = svi.evaluate(svi_state)\n    assert np.isfinite(actual_loss)\n    expected_loss = dist.Normal(guide._init_latent, guide._init_scale).log_prob(x_init) \\\n        - dist.Normal(a_init, b_init).log_prob(x_init)\n    assert_allclose(actual_loss, expected_loss, rtol=1e-6)\n\n\ndef test_dynamic_supports():\n    true_coef = 0.9\n    data = true_coef + random.normal(random.PRNGKey(0), (1000,))\n\n    def actual_model(data):\n        alpha = numpyro.sample(\'alpha\', dist.Uniform(0, 1))\n        with reparam(config={\'loc\': TransformReparam()}):\n            loc = numpyro.sample(\'loc\', dist.Uniform(0, alpha))\n        numpyro.sample(\'obs\', dist.Normal(loc, 0.1), obs=data)\n\n    def expected_model(data):\n        alpha = numpyro.sample(\'alpha\', dist.Uniform(0, 1))\n        loc = numpyro.sample(\'loc\', dist.Uniform(0, 1)) * alpha\n        numpyro.sample(\'obs\', dist.Normal(loc, 0.1), obs=data)\n\n    adam = optim.Adam(0.01)\n    rng_key_init = random.PRNGKey(1)\n\n    guide = AutoDiagonalNormal(actual_model)\n    svi = SVI(actual_model, guide, adam, ELBO())\n    svi_state = svi.init(rng_key_init, data)\n    actual_opt_params = adam.get_params(svi_state.optim_state)\n    actual_params = svi.get_params(svi_state)\n    actual_values = guide.median(actual_params)\n    actual_loss = svi.evaluate(svi_state, data)\n\n    guide = AutoDiagonalNormal(expected_model)\n    svi = SVI(expected_model, guide, adam, ELBO())\n    svi_state = svi.init(rng_key_init, data)\n    expected_opt_params = adam.get_params(svi_state.optim_state)\n    expected_params = svi.get_params(svi_state)\n    expected_values = guide.median(expected_params)\n    expected_loss = svi.evaluate(svi_state, data)\n\n    # test auto_loc, auto_scale\n    check_eq(actual_opt_params, expected_opt_params)\n    check_eq(actual_params, expected_params)\n    # test latent values\n    assert_allclose(actual_values[\'alpha\'], expected_values[\'alpha\'])\n    assert_allclose(actual_values[\'loc_base\'], expected_values[\'loc\'])\n    assert_allclose(actual_loss, expected_loss)\n\n\ndef test_laplace_approximation_warning():\n    def model(x, y):\n        a = numpyro.sample(""a"", dist.Normal(0, 10))\n        b = numpyro.sample(""b"", dist.Normal(0, 10), sample_shape=(3,))\n        mu = a + b[0] * x + b[1] * x ** 2 + b[2] * x ** 3\n        numpyro.sample(""y"", dist.Normal(mu, 0.001), obs=y)\n\n    x = random.normal(random.PRNGKey(0), (3,))\n    y = 1 + 2 * x + 3 * x ** 2 + 4 * x ** 3\n    guide = AutoLaplaceApproximation(model)\n    svi = SVI(model, guide, optim.Adam(0.1), ELBO(), x=x, y=y)\n    init_state = svi.init(random.PRNGKey(0))\n    svi_state = fori_loop(0, 10000, lambda i, val: svi.update(val)[0], init_state)\n    params = svi.get_params(svi_state)\n    with pytest.warns(UserWarning, match=""Hessian of log posterior""):\n        guide.sample_posterior(random.PRNGKey(1), params)\n'"
test/contrib/test_distributions_contrib.py,75,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom functools import reduce\nfrom operator import mul\n\nimport numpy as onp\nfrom numpy.testing import assert_allclose\nimport pytest\nimport scipy.stats as osp_stats\n\nimport jax\nfrom jax import grad, lax, random\nimport jax.numpy as np\nfrom jax.scipy.special import logit\n\nimport numpyro.contrib.distributions as dist\nfrom numpyro.contrib.distributions import jax_multivariate, validation_enabled\nfrom numpyro.distributions import constraints\n\n\ndef idfn(param):\n    if isinstance(param, (osp_stats._distn_infrastructure.rv_generic,\n                          osp_stats._multivariate.multi_rv_generic)):\n        return param.name\n    elif isinstance(param, constraints.Constraint):\n        return param.__class__.__name__\n    return repr(param)\n\n\n@pytest.mark.parametrize('jax_dist', [\n    dist.beta,\n    dist.cauchy,\n    dist.expon,\n    dist.gamma,\n    dist.halfcauchy,\n    dist.halfnorm,\n    dist.lognorm,\n    dist.pareto,\n    dist.trunccauchy,\n    dist.truncnorm,\n    dist.norm,\n    dist.t,\n    dist.uniform,\n], ids=idfn)\n@pytest.mark.parametrize('loc, scale', [\n    (1, 1),\n    (1., np.array([1., 2.])),\n])\n@pytest.mark.parametrize('prepend_shape', [\n    None,\n    (),\n    (2,),\n    (2, 3),\n])\ndef test_continuous_shape(jax_dist, loc, scale, prepend_shape):\n    rng_key = random.PRNGKey(0)\n    args = [i + 1 for i in range(jax_dist.numargs)]\n    expected_shape = lax.broadcast_shapes(*[np.shape(loc), np.shape(scale)])\n    samples = jax_dist.rvs(*args, loc=loc, scale=scale, random_state=rng_key)\n    assert isinstance(samples, jax.interpreters.xla.DeviceArray)\n    assert np.shape(samples) == expected_shape\n    assert np.shape(jax_dist(*args, loc=loc, scale=scale).rvs(random_state=rng_key)) == expected_shape\n    if prepend_shape is not None:\n        expected_shape = prepend_shape + lax.broadcast_shapes(*[np.shape(loc), np.shape(scale)])\n        assert np.shape(jax_dist.rvs(*args, loc=loc, scale=scale,\n                                     size=expected_shape, random_state=rng_key)) == expected_shape\n        assert np.shape(jax_dist(*args, loc=loc, scale=scale)\n                        .rvs(random_state=rng_key, size=expected_shape)) == expected_shape\n\n\n@pytest.mark.parametrize('jax_dist, dist_args, sample', [\n    (dist.beta, (-1, 1), -1),\n    (dist.beta, (2, np.array([1., -3])), np.array([1., -2])),\n    (dist.cauchy, (), np.inf),\n    (dist.cauchy, (), np.array([1., np.nan])),\n    (dist.expon, (), -1),\n    (dist.expon, (), np.array([1., -2])),\n    (dist.gamma, (-1,), -1),\n    (dist.gamma, (np.array([-2., 3]),), np.array([1., -2])),\n    (dist.halfcauchy, (), -1),\n    (dist.halfcauchy, (), np.array([1., -2])),\n    (dist.halfnorm, (), -1),\n    (dist.halfnorm, (), np.array([1., -2])),\n    (dist.lognorm, (-1,), -1),\n    (dist.lognorm, (np.array([-2., 3]),), np.array([1., -2])),\n    (dist.norm, (), np.inf),\n    (dist.norm, (), np.array([1., np.nan])),\n    (dist.pareto, (-1,), -1),\n    (dist.pareto, (np.array([-2., 3]),), np.array([1., -2])),\n    (dist.t, (-1,), np.inf),\n    (dist.t, (np.array([-2., 3]),), np.array([1., np.nan])),\n    (dist.trunccauchy, (), -1),\n    (dist.trunccauchy, (), np.array([1., -2])),\n    (dist.truncnorm, (), -1),\n    (dist.truncnorm, (), np.array([1., -2])),\n    (dist.uniform, (), -1),\n    (dist.uniform, (), np.array([0.5, -2])),\n], ids=idfn)\ndef test_continuous_validate_args(jax_dist, dist_args, sample):\n    valid_args = [i + 1 for i in range(jax_dist.numargs)]\n    with validation_enabled():\n        if dist_args:\n            with pytest.raises(ValueError, match='Invalid parameters'):\n                jax_dist(*dist_args)\n\n        with pytest.raises(ValueError, match='Invalid scale parameter'):\n            jax_dist(*valid_args, scale=-1)\n\n        frozen_dist = jax_dist(*valid_args)\n        with pytest.raises(ValueError, match='Invalid values'):\n            frozen_dist.logpdf(sample)\n\n\n@pytest.mark.parametrize('jax_dist, dist_args', [\n    (dist.categorical, (np.array([0.1, 0.9]),)),\n    (dist.categorical, (np.array([[0.1, 0.9], [0.2, 0.8]]),)),\n    (dist.dirichlet, (np.ones(3),)),\n    (dist.dirichlet, (np.ones((2, 3)),)),\n    (dist.multinomial, (10, np.array([0.1, 0.9]),)),\n    (dist.multinomial, (10, np.array([[0.1, 0.9], [0.2, 0.8]]),)),\n], ids=idfn)\n@pytest.mark.parametrize('prepend_shape', [\n    None,\n    (),\n    (2,),\n    (2, 3),\n])\ndef test_multivariate_shape(jax_dist, dist_args, prepend_shape):\n    rng_key = random.PRNGKey(0)\n    expected_shape = jax_dist._batch_shape(*dist_args) + jax_dist._event_shape(*dist_args)\n    samples = jax_dist.rvs(*dist_args, random_state=rng_key)\n    assert isinstance(samples, jax.interpreters.xla.DeviceArray)\n    assert np.shape(samples) == expected_shape\n    assert np.shape(jax_dist(*dist_args).rvs(random_state=rng_key)) == expected_shape\n    if prepend_shape is not None:\n        size = prepend_shape + jax_dist._batch_shape(*dist_args)\n        expected_shape = size + jax_dist._event_shape(*dist_args)\n        samples = jax_dist.rvs(*dist_args, size=size, random_state=rng_key)\n        assert np.shape(samples) == expected_shape\n        samples = jax_dist(*dist_args).rvs(random_state=rng_key, size=size)\n        assert np.shape(samples) == expected_shape\n\n\n@pytest.mark.parametrize('jax_dist, valid_args, invalid_args, invalid_sample', [\n    (dist.categorical, (np.array([0.1, 0.9]),), (np.array([0.1, 0.8]),), np.array([1, 4])),\n    (dist.dirichlet, (np.ones(3),), (np.array([-1., 2., 3.]),), np.array([0.1, 0.7, 0.1])),\n    (dist.multinomial, (10, np.array([0.1, 0.9]),), (10, np.array([0.2, 0.9]),), np.array([-1, 9])),\n], ids=idfn)\ndef test_multivariate_validate_args(jax_dist, valid_args, invalid_args, invalid_sample):\n    with validation_enabled():\n        with pytest.raises(ValueError, match='Invalid parameters'):\n            jax_dist(*invalid_args)\n\n        frozen_dist = jax_dist(*valid_args)\n        with pytest.raises(ValueError, match='Invalid values'):\n            frozen_dist.logpmf(invalid_sample)\n\n\n@pytest.mark.parametrize('jax_dist, dist_args', [\n    (dist.bernoulli, (0.1,)),\n    (dist.bernoulli, (np.array([0.3, 0.5]),)),\n    (dist.binom, (10, 0.4)),\n    (dist.binom, (np.array([10]), np.array([0.4, 0.3]))),\n    (dist.poisson, (1.,)),\n    (dist.poisson, (np.array([1., 4., 10.]),)),\n], ids=idfn)\n@pytest.mark.parametrize('prepend_shape', [\n    None,\n    (),\n    (2,),\n    (2, 3),\n])\ndef test_discrete_shape(jax_dist, dist_args, prepend_shape):\n    rng_key = random.PRNGKey(0)\n    sp_dist = getattr(osp_stats, jax_dist.name)\n    expected_shape = np.shape(sp_dist.rvs(*dist_args))\n    samples = jax_dist.rvs(*dist_args, random_state=rng_key)\n    assert isinstance(samples, jax.interpreters.xla.DeviceArray)\n    assert np.shape(samples) == expected_shape\n    if prepend_shape is not None:\n        shape = prepend_shape + lax.broadcast_shapes(*[np.shape(arg) for arg in dist_args])\n        expected_shape = np.shape(sp_dist.rvs(*dist_args, size=shape))\n        assert np.shape(jax_dist.rvs(*dist_args, size=shape, random_state=rng_key)) == expected_shape\n\n\n@pytest.mark.parametrize('jax_dist, valid_args, invalid_args, invalid_sample', [\n    (dist.bernoulli, (0.8,), (np.nan,), 2),\n    (dist.binom, (10, 0.8), (-10, 0.8), -10),\n    (dist.binom, (10, 0.8), (10, 1.1), -1),\n    (dist.poisson, (4.,), (-1.,), -1),\n], ids=idfn)\ndef test_discrete_validate_args(jax_dist, valid_args, invalid_args, invalid_sample):\n    with validation_enabled():\n        with pytest.raises(ValueError, match='Invalid parameters'):\n            jax_dist(*invalid_args)\n\n        frozen_dist = jax_dist(*valid_args)\n        with pytest.raises(ValueError, match='Invalid values'):\n            frozen_dist.logpmf(invalid_sample)\n\n\n@pytest.mark.parametrize('jax_dist', [\n    dist.beta,\n    dist.cauchy,\n    dist.expon,\n    dist.gamma,\n    dist.halfcauchy,\n    dist.halfnorm,\n    dist.lognorm,\n    dist.norm,\n    dist.pareto,\n    dist.t,\n    dist.trunccauchy,\n    dist.truncnorm,\n    dist.uniform,\n], ids=idfn)\n@pytest.mark.parametrize('loc, scale', [\n    (1., 1.),\n    (1., np.array([1., 2.])),\n])\ndef test_sample_gradient(jax_dist, loc, scale):\n    rng_key = random.PRNGKey(0)\n    args = [i + 1. for i in range(jax_dist.numargs)]\n    expected_shape = lax.broadcast_shapes(*[np.shape(loc), np.shape(scale)])\n\n    def fn(args, loc, scale):\n        return jax_dist.rvs(*args, loc=loc, scale=scale, random_state=rng_key).sum()\n\n    # FIXME: find a proper test for gradients of arg parameters\n    assert len(grad(fn)(args, loc, scale)) == jax_dist.numargs\n    assert_allclose(grad(fn, 1)(args, loc, scale),\n                    loc * reduce(mul, expected_shape[:len(expected_shape) - np.ndim(loc)], 1.))\n    assert_allclose(grad(fn, 2)(args, loc, scale),\n                    jax_dist.rvs(*args, size=expected_shape, random_state=rng_key))\n\n\n@pytest.mark.parametrize('jax_dist, dist_args', [\n    (dist.dirichlet, (np.ones(3),)),\n    (dist.dirichlet, (np.ones((2, 3)),)),\n], ids=idfn)\ndef test_mvsample_gradient(jax_dist, dist_args):\n    rng_key = random.PRNGKey(0)\n\n    def fn(args):\n        return jax_dist.rvs(*args, random_state=rng_key).sum()\n\n    # FIXME: find a proper test for gradients of arg parameters\n    assert len(grad(fn)(dist_args)) == jax_dist.numargs\n\n\n@pytest.mark.parametrize('jax_dist', [\n    dist.beta,\n    dist.cauchy,\n    dist.expon,\n    dist.gamma,\n    dist.halfcauchy,\n    dist.halfnorm,\n    dist.lognorm,\n    dist.norm,\n    dist.pareto,\n    dist.t,\n    dist.trunccauchy,\n    dist.truncnorm,\n    dist.uniform,\n], ids=idfn)\n@pytest.mark.parametrize('loc_scale', [\n    (),\n    (1,),\n    (1, 1),\n    (1., np.array([1., 2.])),\n])\ndef test_continuous_logpdf(jax_dist, loc_scale):\n    rng_key = random.PRNGKey(0)\n    args = [i + 1 for i in range(jax_dist.numargs)] + list(loc_scale)\n    samples = jax_dist.rvs(*args, random_state=rng_key)\n    if jax_dist is dist.trunccauchy:\n        sp_dist = osp_stats.cauchy\n        assert_allclose(jax_dist.logpdf(samples, args[0], args[1]),\n                        sp_dist.logpdf(samples) - np.log(sp_dist.cdf(args[1]) - sp_dist.cdf(args[0])),\n                        atol=1e-6)\n    else:\n        sp_dist = getattr(osp_stats, jax_dist.name)\n        assert_allclose(jax_dist.logpdf(samples, *args), sp_dist.logpdf(samples, *args), atol=1.3e-6)\n\n\n@pytest.mark.parametrize('jax_dist, dist_args', [\n    (dist.dirichlet, (np.array([1., 2., 3.]),)),\n], ids=idfn)\n@pytest.mark.parametrize('shape', [\n    None,\n    (),\n    (2,),\n    (2, 3),\n])\ndef test_multivariate_continuous_logpdf(jax_dist, dist_args, shape):\n    rng_key = random.PRNGKey(0)\n    samples = jax_dist.rvs(*dist_args, size=shape, random_state=rng_key)\n    # XXX scipy.stats.dirichlet does not work with batch\n    if samples.ndim == 1:\n        sp_dist = getattr(osp_stats, jax_dist.name)\n        assert_allclose(jax_dist.logpdf(samples, *dist_args),\n                        sp_dist.logpdf(samples, *dist_args), atol=1e-6)\n\n    event_dim = len(jax_dist._event_shape(*dist_args))\n    batch_shape = samples.shape if event_dim == 0 else samples.shape[:-1]\n    assert jax_dist.logpdf(samples, *dist_args).shape == batch_shape\n\n\n@pytest.mark.parametrize('jax_dist, dist_args', [\n    (dist.categorical, (np.array([0.7, 0.3]),)),\n    (dist.multinomial, (10, np.array([0.3, 0.7]),)),\n], ids=idfn)\n@pytest.mark.parametrize('shape', [\n    None,\n    (),\n    (2,),\n    (2, 3),\n])\ndef test_multivariate_discrete_logpmf(jax_dist, dist_args, shape):\n    rng_key = random.PRNGKey(0)\n    samples = jax_dist.rvs(*dist_args, size=shape, random_state=rng_key)\n    # XXX scipy.stats.multinomial does not work with batch\n    if samples.ndim == 1:\n        if jax_dist is dist.categorical:\n            # test against PyTorch\n            assert_allclose(jax_dist.logpmf(np.array([1, 0]), *dist_args),\n                            np.array([-1.2040, -0.3567]), atol=1e-4)\n        else:\n            sp_dist = getattr(osp_stats, jax_dist.name)\n            assert_allclose(jax_dist.logpmf(samples, *dist_args),\n                            sp_dist.logpmf(samples, *dist_args), atol=1e-5)\n\n    event_dim = len(jax_dist._event_shape(*dist_args))\n    batch_shape = samples.shape if event_dim == 0 else samples.shape[:-1]\n    assert jax_dist.logpmf(samples, *dist_args).shape == batch_shape\n\n\n@pytest.mark.parametrize('jax_dist, dist_args', [\n    (dist.bernoulli, (0.1,)),\n    (dist.bernoulli, (np.array([0.3, 0.5]),)),\n    (dist.binom, (10, 0.4)),\n    (dist.binom, (np.array([10]), np.array([0.4, 0.3]))),\n    (dist.binom, (np.array([2, 5]), np.array([[0.4], [0.5]]))),\n    (dist.poisson, (4.,)),\n    (dist.poisson, (np.array([1., 4., 10.]),)),\n], ids=idfn)\n@pytest.mark.parametrize('shape', [\n    None,\n    (),\n    (2,),\n    (2, 3),\n])\ndef test_discrete_logpmf(jax_dist, dist_args, shape):\n    rng_key = random.PRNGKey(0)\n    sp_dist = getattr(osp_stats, jax_dist.name)\n    samples = jax_dist.rvs(*dist_args, random_state=rng_key)\n    assert_allclose(jax_dist.logpmf(samples, *dist_args),\n                    sp_dist.logpmf(onp.asarray(samples), *dist_args),\n                    rtol=1e-5)\n    if shape is not None:\n        shape = shape + lax.broadcast_shapes(*[np.shape(arg) for arg in dist_args])\n        samples = jax_dist.rvs(*dist_args, size=shape, random_state=rng_key)\n        assert_allclose(jax_dist.logpmf(samples, *dist_args),\n                        sp_dist.logpmf(onp.asarray(samples), *dist_args),\n                        rtol=1e-5)\n\n        def fn(sample, *args):\n            return np.sum(jax_dist.logpmf(sample, *args))\n\n        for i in range(len(dist_args)):\n            if np.result_type(dist_args[i]) in (np.int32, np.int64):\n                continue\n            logpmf_grad = grad(fn, i + 1)(samples, *dist_args)\n            assert np.all(np.isfinite(logpmf_grad))\n\n\n@pytest.mark.parametrize('jax_dist, dist_args', [\n    (dist.bernoulli, (0.1,)),\n    (dist.bernoulli, (np.array([0.3, 0.5]),)),\n    (dist.binom, (10, 0.4)),\n    (dist.binom, (np.array([10]), np.array([0.4, 0.3]))),\n    (dist.binom, (np.array([2, 5]), np.array([[0.4], [0.5]]))),\n    (dist.categorical, (np.array([0.1, 0.9]),)),\n    (dist.categorical, (np.array([[0.1, 0.9], [0.2, 0.8]]),)),\n    (dist.multinomial, (10, np.array([0.1, 0.9]),)),\n    (dist.multinomial, (10, np.array([[0.1, 0.9], [0.2, 0.8]]),)),\n], ids=idfn)\ndef test_discrete_with_logits(jax_dist, dist_args):\n    rng_key = random.PRNGKey(0)\n    logit_to_prob = np.log if isinstance(jax_dist, jax_multivariate) else logit\n    logit_args = dist_args[:-1] + (logit_to_prob(dist_args[-1]),)\n\n    actual_sample = jax_dist.rvs(*dist_args, random_state=rng_key)\n    expected_sample = jax_dist(*logit_args, is_logits=True).rvs(random_state=rng_key)\n    assert_allclose(actual_sample, expected_sample)\n\n    actual_pmf = jax_dist.logpmf(actual_sample, *dist_args)\n    expected_pmf = jax_dist(*logit_args, is_logits=True).logpmf(actual_sample)\n    assert_allclose(actual_pmf, expected_pmf, rtol=1e-6)\n"""
test/contrib/test_indexing.py,14,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\n\nimport jax.lax as lax\nimport jax.numpy as np\nimport jax.random as random\nimport numpy as onp\nimport pytest\n\nimport numpyro.distributions as dist\nfrom numpyro.contrib.indexing import Vindex\n\n\ndef z(*shape):\n    return np.zeros(shape, dtype=np.int32)\n\n\nSHAPE_EXAMPLES = [\n    ('Vindex(z())[...]', ()),\n    ('Vindex(z(2))[...]', (2,)),\n    ('Vindex(z(2))[...,0]', ()),\n    ('Vindex(z(2))[...,:]', (2,)),\n    ('Vindex(z(2))[...,z(3)]', (3,)),\n    ('Vindex(z(2))[0]', ()),\n    ('Vindex(z(2))[:]', (2,)),\n    ('Vindex(z(2))[z(3)]', (3,)),\n    ('Vindex(z(2,3))[...]', (2, 3)),\n    ('Vindex(z(2,3))[...,0]', (2,)),\n    ('Vindex(z(2,3))[...,:]', (2, 3)),\n    ('Vindex(z(2,3))[...,z(2)]', (2,)),\n    ('Vindex(z(2,3))[...,z(4,1)]', (4, 2)),\n    ('Vindex(z(2,3))[...,0,0]', ()),\n    ('Vindex(z(2,3))[...,0,:]', (3,)),\n    ('Vindex(z(2,3))[...,0,z(4)]', (4,)),\n    ('Vindex(z(2,3))[...,:,0]', (2,)),\n    ('Vindex(z(2,3))[...,:,:]', (2, 3)),\n    ('Vindex(z(2,3))[...,:,z(4)]', (4, 2)),\n    ('Vindex(z(2,3))[...,z(4),0]', (4,)),\n    ('Vindex(z(2,3))[...,z(4),:]', (4, 3)),\n    ('Vindex(z(2,3))[...,z(4),z(4)]', (4,)),\n    ('Vindex(z(2,3))[...,z(5,1),z(4)]', (5, 4)),\n    ('Vindex(z(2,3))[...,z(4),z(5,1)]', (5, 4)),\n    ('Vindex(z(2,3))[0,0]', ()),\n    ('Vindex(z(2,3))[0,:]', (3,)),\n    ('Vindex(z(2,3))[0,z(4)]', (4,)),\n    ('Vindex(z(2,3))[:,0]', (2,)),\n    ('Vindex(z(2,3))[:,:]', (2, 3)),\n    ('Vindex(z(2,3))[:,z(4)]', (4, 2)),\n    ('Vindex(z(2,3))[z(4),0]', (4,)),\n    ('Vindex(z(2,3))[z(4),:]', (4, 3)),\n    ('Vindex(z(2,3))[z(4)]', (4, 3)),\n    ('Vindex(z(2,3))[z(4),z(4)]', (4,)),\n    ('Vindex(z(2,3))[z(5,1),z(4)]', (5, 4)),\n    ('Vindex(z(2,3))[z(4),z(5,1)]', (5, 4)),\n    ('Vindex(z(2,3,4))[...]', (2, 3, 4)),\n    ('Vindex(z(2,3,4))[...,z(3)]', (2, 3)),\n    ('Vindex(z(2,3,4))[...,z(2,1)]', (2, 3)),\n    ('Vindex(z(2,3,4))[...,z(2,3)]', (2, 3)),\n    ('Vindex(z(2,3,4))[...,z(5,1,1)]', (5, 2, 3)),\n    ('Vindex(z(2,3,4))[...,z(2),0]', (2,)),\n    ('Vindex(z(2,3,4))[...,z(5,1),0]', (5, 2)),\n    ('Vindex(z(2,3,4))[...,z(2),:]', (2, 4)),\n    ('Vindex(z(2,3,4))[...,z(5,1),:]', (5, 2, 4)),\n    ('Vindex(z(2,3,4))[...,z(5),0,0]', (5,)),\n    ('Vindex(z(2,3,4))[...,z(5),0,:]', (5, 4)),\n    ('Vindex(z(2,3,4))[...,z(5),:,0]', (5, 3)),\n    ('Vindex(z(2,3,4))[...,z(5),:,:]', (5, 3, 4)),\n    ('Vindex(z(2,3,4))[0,0,z(5)]', (5,)),\n    ('Vindex(z(2,3,4))[0,:,z(5)]', (5, 3)),\n    ('Vindex(z(2,3,4))[0,z(5),0]', (5,)),\n    ('Vindex(z(2,3,4))[0,z(5),:]', (5, 4)),\n    ('Vindex(z(2,3,4))[0,z(5),z(5)]', (5,)),\n    ('Vindex(z(2,3,4))[0,z(5,1),z(6)]', (5, 6)),\n    ('Vindex(z(2,3,4))[0,z(6),z(5,1)]', (5, 6)),\n    ('Vindex(z(2,3,4))[:,0,z(5)]', (5, 2)),\n    ('Vindex(z(2,3,4))[:,:,z(5)]', (5, 2, 3)),\n    ('Vindex(z(2,3,4))[:,z(5),0]', (5, 2)),\n    ('Vindex(z(2,3,4))[:,z(5),:]', (5, 2, 4)),\n    ('Vindex(z(2,3,4))[:,z(5),z(5)]', (5, 2)),\n    ('Vindex(z(2,3,4))[:,z(5,1),z(6)]', (5, 6, 2)),\n    ('Vindex(z(2,3,4))[:,z(6),z(5,1)]', (5, 6, 2)),\n    ('Vindex(z(2,3,4))[z(5),0,0]', (5,)),\n    ('Vindex(z(2,3,4))[z(5),0,:]', (5, 4)),\n    ('Vindex(z(2,3,4))[z(5),:,0]', (5, 3)),\n    ('Vindex(z(2,3,4))[z(5),:,:]', (5, 3, 4)),\n    ('Vindex(z(2,3,4))[z(5),0,z(5)]', (5,)),\n    ('Vindex(z(2,3,4))[z(5,1),0,z(6)]', (5, 6)),\n    ('Vindex(z(2,3,4))[z(6),0,z(5,1)]', (5, 6)),\n    ('Vindex(z(2,3,4))[z(5),:,z(5)]', (5, 3)),\n    ('Vindex(z(2,3,4))[z(5,1),:,z(6)]', (5, 6, 3)),\n    ('Vindex(z(2,3,4))[z(6),:,z(5,1)]', (5, 6, 3)),\n]\n\n\n@pytest.mark.parametrize('expression,expected_shape', SHAPE_EXAMPLES, ids=str)\ndef test_shape(expression, expected_shape):\n    result = eval(expression)\n    assert result.shape == expected_shape\n\n\n@pytest.mark.parametrize('event_shape', [(), (7,)], ids=str)\n@pytest.mark.parametrize('j_shape', [(), (2,), (3, 1), (4, 1, 1), (4, 3, 2)], ids=str)\n@pytest.mark.parametrize('i_shape', [(), (2,), (3, 1), (4, 1, 1), (4, 3, 2)], ids=str)\n@pytest.mark.parametrize('x_shape', [(), (2,), (3, 1), (4, 1, 1), (4, 3, 2)], ids=str)\ndef test_value(x_shape, i_shape, j_shape, event_shape):\n    x = np.array(onp.random.rand(*(x_shape + (5, 6) + event_shape)))\n    i = dist.Categorical(np.ones((5,))).sample(random.PRNGKey(1), i_shape)\n    j = dist.Categorical(np.ones((6,))).sample(random.PRNGKey(2), j_shape)\n    if event_shape:\n        actual = Vindex(x)[..., i, j, :]\n    else:\n        actual = Vindex(x)[..., i, j]\n\n    shape = lax.broadcast_shapes(x_shape, i_shape, j_shape)\n    x = np.broadcast_to(x, shape + (5, 6) + event_shape)\n    i = np.broadcast_to(i, shape)\n    j = np.broadcast_to(j, shape)\n    expected = onp.empty(shape + event_shape, dtype=x.dtype)\n    for ind in (itertools.product(*map(range, shape)) if shape else [()]):\n        expected[ind] = x[ind + (i[ind].item(), j[ind].item())]\n    assert np.all(actual == np.array(expected, dtype=x.dtype))\n\n\n@pytest.mark.parametrize('prev_enum_dim,curr_enum_dim', [(-3, -4), (-4, -5), (-5, -3)])\ndef test_hmm_example(prev_enum_dim, curr_enum_dim):\n    hidden_dim = 8\n    probs_x = np.array(onp.random.rand(hidden_dim, hidden_dim, hidden_dim))\n    x_prev = np.arange(hidden_dim).reshape((-1,) + (1,) * (-1 - prev_enum_dim))\n    x_curr = np.arange(hidden_dim).reshape((-1,) + (1,) * (-1 - curr_enum_dim))\n\n    expected = probs_x[x_prev.reshape(x_prev.shape + (1,)),\n                       x_curr.reshape(x_curr.shape + (1,)),\n                       np.arange(hidden_dim)]\n\n    actual = Vindex(probs_x)[x_prev, x_curr, :]\n    assert np.all(actual == expected)\n"""
test/contrib/test_nn.py,17,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# lightly adapted from https://github.com/pyro-ppl/pyro/blob/dev/tests/nn/\n\nimport numpy as onp\nfrom numpy.testing import assert_allclose, assert_array_equal\nimport pytest\n\nfrom jax import jacfwd, random, vmap\nimport jax.numpy as np\nfrom jax.experimental.stax import serial\n\nfrom numpyro.contrib.nn import AutoregressiveNN, MaskedDense\nfrom numpyro.contrib.nn.auto_reg_nn import create_mask\nfrom numpyro.contrib.nn.block_neural_arn import BlockNeuralAutoregressiveNN\nfrom numpyro.distributions.util import matrix_to_tril_vec\n\n\n@pytest.mark.parametrize(\'input_dim\', [5])\n@pytest.mark.parametrize(\'param_dims\', [[1], [1, 1], [2], [2, 3]])\n@pytest.mark.parametrize(\'hidden_dims\', [[8], [6, 7]])\n@pytest.mark.parametrize(\'skip_connections\', [True, False])\ndef test_auto_reg_nn(input_dim, hidden_dims, param_dims, skip_connections):\n    rng_key, rng_key_perm = random.split(random.PRNGKey(0))\n    perm = random.permutation(rng_key_perm, onp.arange(input_dim))\n    arn_init, arn = AutoregressiveNN(input_dim, hidden_dims, param_dims=param_dims,\n                                     skip_connections=skip_connections, permutation=perm)\n\n    batch_size = 4\n    input_shape = (batch_size, input_dim)\n    _, init_params = arn_init(rng_key, input_shape)\n\n    output = arn(init_params, onp.random.rand(*input_shape))\n\n    if param_dims == [1]:\n        assert output.shape == (batch_size, input_dim)\n        jac = jacfwd(lambda x: arn(init_params, x))(onp.random.rand(input_dim))\n    elif param_dims == [1, 1]:\n        assert output[0].shape == (batch_size, input_dim)\n        assert output[1].shape == (batch_size, input_dim)\n        jac = jacfwd(lambda x: arn(init_params, x)[0])(onp.random.rand(input_dim))\n    elif param_dims == [2]:\n        assert output.shape == (2, batch_size, input_dim)\n        jac = jacfwd(lambda x: arn(init_params, x))(onp.random.rand(input_dim))\n    elif param_dims == [2, 3]:\n        assert output[0].shape == (2, batch_size, input_dim)\n        assert output[1].shape == (3, batch_size, input_dim)\n        jac = jacfwd(lambda x: arn(init_params, x)[0])(onp.random.rand(input_dim))\n\n    # permute jacobian as necessary\n    permuted_jac = onp.zeros(jac.shape)\n\n    for j in range(input_dim):\n        for k in range(input_dim):\n            permuted_jac[..., j, k] = jac[..., perm[j], perm[k]]\n\n    # make sure jacobians are triangular\n    assert onp.sum(onp.abs(onp.triu(permuted_jac))) == 0.0\n\n\n@pytest.mark.parametrize(\'input_dim\', [2, 6])\n@pytest.mark.parametrize(\'n_layers\', [1, 2])\n@pytest.mark.parametrize(\'output_dim_multiplier\', [1, 2])\ndef test_masks(input_dim, n_layers, output_dim_multiplier):\n    hidden_dim = input_dim * 3\n    hidden_dims = [hidden_dim] * n_layers\n    permutation = onp.random.permutation(input_dim)\n    masks, mask_skip = create_mask(input_dim, hidden_dims, permutation, output_dim_multiplier)\n    masks = [onp.transpose(m) for m in masks]\n    mask_skip = onp.transpose(mask_skip)\n\n    # First test that hidden layer masks are adequately connected\n    # Tracing backwards, works out what inputs each output is connected to\n    # It\'s a dictionary of sets indexed by a tuple (input_dim, param_dim)\n    _permutation = list(permutation)\n\n    # Loop over variables\n    for idx in range(input_dim):\n        # Calculate correct answer\n        correct = onp.array(sorted(_permutation[0:onp.where(permutation == idx)[0][0]]))\n\n        # Loop over parameters for each variable\n        for jdx in range(output_dim_multiplier):\n            prev_connections = set()\n            # Do output-to-penultimate hidden layer mask\n            for kdx in range(masks[-1].shape[1]):\n                if masks[-1][idx + jdx * input_dim, kdx]:\n                    prev_connections.add(kdx)\n\n            # Do hidden-to-hidden, and hidden-to-input layer masks\n            for m in reversed(masks[:-1]):\n                this_connections = set()\n                for kdx in prev_connections:\n                    for ldx in range(m.shape[1]):\n                        if m[kdx, ldx]:\n                            this_connections.add(ldx)\n                prev_connections = this_connections\n\n            assert_array_equal(list(sorted(prev_connections)), correct)\n\n            # Test the skip-connections mask\n            skip_connections = set()\n            for kdx in range(mask_skip.shape[1]):\n                if mask_skip[idx + jdx * input_dim, kdx]:\n                    skip_connections.add(kdx)\n            assert_array_equal(list(sorted(skip_connections)), correct)\n\n\n@pytest.mark.parametrize(\'input_dim\', [5, 7])\ndef test_masked_dense(input_dim):\n    hidden_dim = input_dim * 3\n    output_dim_multiplier = input_dim - 4\n    mask, _ = create_mask(input_dim, [hidden_dim], onp.random.permutation(input_dim), output_dim_multiplier)\n    init_random_params, masked_dense = serial(MaskedDense(mask[0]))\n\n    rng_key = random.PRNGKey(0)\n    batch_size = 4\n    input_shape = (batch_size, input_dim)\n    _, init_params = init_random_params(rng_key, input_shape)\n    output = masked_dense(init_params, onp.random.rand(*input_shape))\n    assert output.shape == (batch_size, hidden_dim)\n\n\n@pytest.mark.parametrize(\'input_dim\', [5])\n@pytest.mark.parametrize(\'hidden_factors\', [[4], [2, 3]])\n@pytest.mark.parametrize(\'residual\', [None, ""normal"", ""gated""])\n@pytest.mark.parametrize(\'batch_shape\', [(3,), ()])\ndef test_block_neural_arn(input_dim, hidden_factors, residual, batch_shape):\n    arn_init, arn = BlockNeuralAutoregressiveNN(input_dim, hidden_factors, residual)\n\n    rng = random.PRNGKey(0)\n    input_shape = batch_shape + (input_dim,)\n    out_shape, init_params = arn_init(rng, input_shape)\n    assert out_shape == input_shape\n\n    x = random.normal(random.PRNGKey(1), input_shape)\n    output, logdet = arn(init_params, x)\n    assert output.shape == input_shape\n    assert logdet.shape == input_shape\n\n    if len(batch_shape) == 1:\n        jac = vmap(jacfwd(lambda x: arn(init_params, x)[0]))(x)\n    else:\n        jac = jacfwd(lambda x: arn(init_params, x)[0])(x)\n    assert_allclose(logdet.sum(-1), np.linalg.slogdet(jac)[1], rtol=1e-6)\n\n    # make sure jacobians are lower triangular\n    assert onp.sum(onp.abs(onp.triu(jac, k=1))) == 0.0\n    assert onp.all(onp.abs(matrix_to_tril_vec(jac)) > 0)\n'"
test/contrib/test_reparam.py,13,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom jax import lax, numpy as np, random\nimport numpy as onp\nfrom numpy.testing import assert_allclose\nimport pytest\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.distributions.transforms import AffineTransform, ExpTransform\nimport numpyro.handlers as handlers\nfrom numpyro.contrib.autoguide import AutoIAFNormal\nfrom numpyro.contrib.reparam import NeuTraReparam, TransformReparam, reparam\nfrom numpyro.infer.util import initialize_model\nfrom numpyro.infer import MCMC, NUTS, SVI, ELBO\nfrom numpyro.optim import Adam\n\n\n# Test helper to extract a few log central moments from samples.\ndef get_moments(x):\n    assert (x > 0).all()\n    x = np.log(x)\n    m1 = np.mean(x, axis=0)\n    x = x - m1\n    xx = x * x\n    xxx = x * xx\n    xxxx = xx * xx\n    m2 = np.mean(xx, axis=0)\n    m3 = np.mean(xxx, axis=0) / m2 ** 1.5\n    m4 = np.mean(xxxx, axis=0) / m2 ** 2\n    return np.stack([m1, m2, m3, m4])\n\n\n@pytest.mark.parametrize(""shape"", [(), (4,), (2, 3)], ids=str)\ndef test_log_normal(shape):\n    loc = onp.random.rand(*shape) * 2 - 1\n    scale = onp.random.rand(*shape) + 0.5\n\n    def model():\n        with numpyro.plate_stack(""plates"", shape):\n            with numpyro.plate(""particles"", 100000):\n                return numpyro.sample(""x"",\n                                      dist.TransformedDistribution(\n                                          dist.Normal(np.zeros_like(loc),\n                                                      np.ones_like(scale)),\n                                          [AffineTransform(loc, scale),\n                                           ExpTransform()]).expand_by([100000]))\n\n    with handlers.trace() as tr:\n        value = handlers.seed(model, 0)()\n    expected_moments = get_moments(value)\n\n    with reparam(config={""x"": TransformReparam()}):\n        with handlers.trace() as tr:\n            value = handlers.seed(model, 0)()\n    assert tr[""x""][""type""] == ""deterministic""\n    actual_moments = get_moments(value)\n    assert_allclose(actual_moments, expected_moments, atol=0.05)\n\n\ndef neals_funnel(dim):\n    y = numpyro.sample(\'y\', dist.Normal(0, 3))\n    with numpyro.plate(\'D\', dim):\n        numpyro.sample(\'x\', dist.Normal(0, np.exp(y / 2)))\n\n\ndef dirichlet_categorical(data):\n    concentration = np.array([1.0, 1.0, 1.0])\n    p_latent = numpyro.sample(\'p\', dist.Dirichlet(concentration))\n    with numpyro.plate(\'N\', data.shape[0]):\n        numpyro.sample(\'obs\', dist.Categorical(p_latent), obs=data)\n    return p_latent\n\n\ndef test_neals_funnel_smoke():\n    dim = 10\n\n    guide = AutoIAFNormal(neals_funnel)\n    svi = SVI(neals_funnel, guide, Adam(1e-10), ELBO())\n    svi_state = svi.init(random.PRNGKey(0), dim)\n\n    def body_fn(i, val):\n        svi_state, loss = svi.update(val, dim)\n        return svi_state\n\n    svi_state = lax.fori_loop(0, 1000, body_fn, svi_state)\n    params = svi.get_params(svi_state)\n\n    neutra = NeuTraReparam(guide, params)\n    model = neutra.reparam(neals_funnel)\n    nuts = NUTS(model)\n    mcmc = MCMC(nuts, num_warmup=50, num_samples=50)\n    mcmc.run(random.PRNGKey(1), dim)\n    samples = mcmc.get_samples()\n    transformed_samples = neutra.transform_sample(samples[\'auto_shared_latent\'])\n    assert \'x\' in transformed_samples\n    assert \'y\' in transformed_samples\n\n\n@pytest.mark.parametrize(\'model, kwargs\', [\n    (neals_funnel, {\'dim\': 10}),\n    (dirichlet_categorical, {\'data\': np.ones(10, dtype=np.int32)})\n])\ndef test_reparam_log_joint(model, kwargs):\n    guide = AutoIAFNormal(model)\n    svi = SVI(model, guide, Adam(1e-10), ELBO(), **kwargs)\n    svi_state = svi.init(random.PRNGKey(0))\n    params = svi.get_params(svi_state)\n    neutra = NeuTraReparam(guide, params)\n    reparam_model = neutra.reparam(model)\n    _, pe_fn, _, _ = initialize_model(random.PRNGKey(1), model, model_kwargs=kwargs)\n    init_params, pe_fn_neutra, _, _ = initialize_model(random.PRNGKey(2), reparam_model, model_kwargs=kwargs)\n    latent_x = list(init_params[0].values())[0]\n    pe_transformed = pe_fn_neutra(init_params[0])\n    latent_y = neutra.transform(latent_x)\n    log_det_jacobian = neutra.transform.log_abs_det_jacobian(latent_x, latent_y)\n    pe = pe_fn(guide._unpack_latent(latent_y))\n    assert_allclose(pe_transformed, pe - log_det_jacobian)\n'"
test/pyroapi/conftest.py,0,b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nimport pytest\n\n\ndef pytest_runtest_call(item):\n    try:\n        item.runtest()\n    except NotImplementedError as e:\n        pytest.xfail(str(e))\n'
test/pyroapi/test_pyroapi.py,0,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom pyroapi import pyro_backend\nfrom pyroapi.tests import *  # noqa F401\nimport pytest\n\npytestmark = pytest.mark.filterwarnings(""ignore::numpyro.compat.util.UnsupportedAPIWarning"")\n\n\n@pytest.yield_fixture\ndef backend():\n    with pyro_backend(\'numpy\'):\n        yield\n'"
numpyro/contrib/autoguide/__init__.py,27,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n""""""\n.. warning::\n    The interface for the `contrib.autoguide` module is experimental, and\n    subject to frequent revisions.\n""""""\n# Adapted from pyro.contrib.autoguide\nfrom abc import ABC, abstractmethod\nimport warnings\n\nfrom jax import hessian, lax, random, tree_map\nfrom jax.experimental import stax\nfrom jax.flatten_util import ravel_pytree\nimport jax.numpy as np\n\nimport numpyro\nfrom numpyro import handlers\nfrom numpyro.contrib.nn.auto_reg_nn import AutoregressiveNN\nfrom numpyro.contrib.nn.block_neural_arn import BlockNeuralAutoregressiveNN\nimport numpyro.distributions as dist\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.flows import BlockNeuralAutoregressiveTransform, InverseAutoregressiveTransform\nfrom numpyro.distributions.transforms import (\n    AffineTransform,\n    ComposeTransform,\n    MultivariateAffineTransform,\n    PermuteTransform,\n    UnpackTransform,\n    biject_to\n)\nfrom numpyro.distributions.util import cholesky_of_inverse, sum_rightmost\nfrom numpyro.infer.elbo import ELBO\nfrom numpyro.infer.util import initialize_model, init_to_uniform\nfrom numpyro.util import not_jax_tracer\n\n__all__ = [\n    \'AutoContinuous\',\n    \'AutoGuide\',\n    \'AutoDiagonalNormal\',\n    \'AutoMultivariateNormal\',\n    \'AutoIAFNormal\',\n]\n\n\nclass AutoGuide(ABC):\n    """"""\n    Base class for automatic guides.\n\n    Derived classes must implement the :meth:`__call__` method.\n\n    :param callable model: a pyro model\n    :param str prefix: a prefix that will be prefixed to all param internal sites\n    """"""\n\n    def __init__(self, model, prefix=\'auto\'):\n        assert isinstance(prefix, str)\n        self.model = model\n        self.prefix = prefix\n        self.prototype_trace = None\n\n    @abstractmethod\n    def __call__(self, *args, **kwargs):\n        """"""\n        A guide with the same ``*args, **kwargs`` as the base ``model``.\n\n        :return: A dict mapping sample site name to sampled value.\n        :rtype: dict\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def sample_posterior(self, rng_key, params, *args, **kwargs):\n        """"""\n        Generate samples from the approximate posterior over the latent\n        sites in the model.\n\n        :param jax.random.PRNGKey rng_key: PRNG seed.\n        :param params: Current parameters of model and autoguide.\n        :param sample_shape: (keyword argument) shape of samples to be drawn.\n        :return: batch of samples from the approximate posterior.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def _sample_latent(self, *args, **kwargs):\n        """"""\n        Samples an encoded latent given the same ``*args, **kwargs`` as the\n        base ``model``.\n        """"""\n        raise NotImplementedError\n\n    def _setup_prototype(self, *args, **kwargs):\n        # run the model so we can inspect its structure\n        rng_key = numpyro.sample(""_{}_rng_key_setup"".format(self.prefix), dist.PRNGIdentity())\n        model = handlers.seed(self.model, rng_key)\n        self.prototype_trace = handlers.block(handlers.trace(model).get_trace)(*args, **kwargs)\n\n\nclass AutoContinuous(AutoGuide):\n    """"""\n    Base class for implementations of continuous-valued Automatic\n    Differentiation Variational Inference [1].\n\n    Each derived class implements its own :meth:`_get_transform` method.\n\n    Assumes model structure and latent dimension are fixed, and all latent\n    variables are continuous.\n\n    **Reference:**\n\n    1. *Automatic Differentiation Variational Inference*,\n       Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M.\n       Blei\n\n    :param callable model: A NumPyro model.\n    :param str prefix: a prefix that will be prefixed to all param internal sites.\n    :param callable init_strategy: A per-site initialization function.\n        See :ref:`init_strategy` section for available functions.\n    """"""\n    def __init__(self, model, prefix=""auto"", init_strategy=init_to_uniform):\n        self.init_strategy = init_strategy\n        super(AutoContinuous, self).__init__(model, prefix=prefix)\n\n    def _setup_prototype(self, *args, **kwargs):\n        rng_key = numpyro.sample(""_{}_rng_key_setup"".format(self.prefix), dist.PRNGIdentity())\n        with handlers.block():\n            init_params, _, self._postprocess_fn, self.prototype_trace = initialize_model(\n                rng_key, self.model,\n                init_strategy=self.init_strategy,\n                dynamic_args=False,\n                model_args=args,\n                model_kwargs=kwargs)\n\n        self._init_latent, unpack_latent = ravel_pytree(init_params[0])\n        # this is to match the behavior of Pyro, where we can apply\n        # unpack_latent for a batch of samples\n        self._unpack_latent = UnpackTransform(unpack_latent)\n        # FIXME: in Pyro this is latent_dim, should we use latent_dim here?\n        self.latent_size = np.size(self._init_latent)\n        if self.latent_size == 0:\n            raise RuntimeError(\'{} found no latent variables; Use an empty guide instead\'\n                               .format(type(self).__name__))\n\n    @abstractmethod\n    def _get_posterior(self):\n        raise NotImplementedError\n\n    def _sample_latent(self, *args, **kwargs):\n        sample_shape = kwargs.pop(\'sample_shape\', ())\n        posterior = self._get_posterior()\n        return numpyro.sample(""_{}_latent"".format(self.prefix), posterior, sample_shape=sample_shape)\n\n    def __call__(self, *args, **kwargs):\n        """"""\n        An automatic guide with the same ``*args, **kwargs`` as the base ``model``.\n\n        :return: A dict mapping sample site name to sampled value.\n        :rtype: dict\n        """"""\n        if self.prototype_trace is None:\n            # run model to inspect the model structure\n            self._setup_prototype(*args, **kwargs)\n\n        latent = self._sample_latent(*args, **kwargs)\n\n        # unpack continuous latent samples\n        result = {}\n\n        for name, unconstrained_value in self._unpack_latent(latent).items():\n            site = self.prototype_trace[name]\n            transform = biject_to(site[\'fn\'].support)\n            value = transform(unconstrained_value)\n            log_density = - transform.log_abs_det_jacobian(unconstrained_value, value)\n            event_ndim = len(site[\'fn\'].event_shape)\n            log_density = sum_rightmost(log_density,\n                                        np.ndim(log_density) - np.ndim(value) + event_ndim)\n            delta_dist = dist.Delta(value, log_density=log_density, event_dim=event_ndim)\n            result[name] = numpyro.sample(name, delta_dist)\n\n        return result\n\n    def _unpack_and_constrain(self, latent_sample, params):\n        def unpack_single_latent(latent):\n            unpacked_samples = self._unpack_latent(latent)\n            # add param sites in model\n            unpacked_samples.update({k: v for k, v in params.items() if k in self.prototype_trace\n                                     and v[\'type\'] == \'param\'})\n            return self._postprocess_fn(unpacked_samples)\n\n        sample_shape = np.shape(latent_sample)[:-1]\n        if sample_shape:\n            latent_sample = np.reshape(latent_sample, (-1, np.shape(latent_sample)[-1]))\n            unpacked_samples = lax.map(unpack_single_latent, latent_sample)\n            return tree_map(lambda x: np.reshape(x, sample_shape + np.shape(x)[1:]),\n                            unpacked_samples)\n        else:\n            return unpack_single_latent(latent_sample)\n\n    @property\n    def get_base_dist(self):\n        """"""\n        Returns the base distribution of the posterior when reparameterized\n        as a :class:`~numpyro.distributions.TransformedDistribution`. This\n        should not depend on the model\'s `*args, **kwargs`.\n        """"""\n        raise NotImplementedError\n\n    def get_transform(self, params):\n        """"""\n        Returns the transformation learned by the guide to generate samples from the unconstrained\n        (approximate) posterior.\n\n        :param dict params: Current parameters of model and autoguide.\n        :return: the transform of posterior distribution\n        :rtype: :class:`~numpyro.distributions.transforms.Transform`\n        """"""\n        posterior = handlers.substitute(self._get_posterior, params)()\n        assert isinstance(posterior, dist.TransformedDistribution), \\\n            ""posterior is not a transformed distribution""\n        if len(posterior.transforms) > 0:\n            return ComposeTransform(posterior.transforms)\n        else:\n            return posterior.transforms[0]\n\n    def get_posterior(self, params):\n        """"""\n        Returns the posterior distribution.\n\n        :param dict params: Current parameters of model and autoguide.\n        """"""\n        base_dist = self.get_base_dist()\n        transform = self.get_transform(params)\n        return dist.TransformedDistribution(base_dist, transform)\n\n    def sample_posterior(self, rng_key, params, sample_shape=()):\n        """"""\n        Get samples from the learned posterior.\n\n        :param jax.random.PRNGKey rng_key: random key to be used draw samples.\n        :param dict params: Current parameters of model and autoguide.\n        :param tuple sample_shape: batch shape of each latent sample, defaults to ().\n        :return: a dict containing samples drawn the this guide.\n        :rtype: dict\n        """"""\n        latent_sample = handlers.substitute(\n            handlers.seed(self._sample_latent, rng_key), params)(sample_shape=sample_shape)\n        return self._unpack_and_constrain(latent_sample, params)\n\n    def median(self, params):\n        """"""\n        Returns the posterior median value of each latent variable.\n\n        :param dict params: A dict containing parameter values.\n        :return: A dict mapping sample site name to median tensor.\n        :rtype: dict\n        """"""\n        raise NotImplementedError\n\n    def quantiles(self, params, quantiles):\n        """"""\n        Returns posterior quantiles each latent variable. Example::\n\n            print(guide.quantiles(opt_state, [0.05, 0.5, 0.95]))\n\n        :param dict params: A dict containing parameter values.\n        :param list quantiles: A list of requested quantiles between 0 and 1.\n        :return: A dict mapping sample site name to a list of quantile values.\n        :rtype: dict\n        """"""\n        raise NotImplementedError\n\n\nclass AutoDiagonalNormal(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a Normal distribution\n    with a diagonal covariance matrix to construct a guide over the entire\n    latent space. The guide does not depend on the model\'s ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoDiagonalNormal(model, ...)\n        svi = SVI(model, guide, ...)\n    """"""\n    def __init__(self, model, prefix=""auto"", init_strategy=init_to_uniform, init_scale=0.1):\n        if init_scale <= 0:\n            raise ValueError(""Expected init_scale > 0. but got {}"".format(init_scale))\n        self._init_scale = init_scale\n        super().__init__(model, prefix, init_strategy)\n\n    def _get_posterior(self):\n        loc = numpyro.param(\'{}_loc\'.format(self.prefix), self._init_latent)\n        scale = numpyro.param(\'{}_scale\'.format(self.prefix),\n                              np.full(self.latent_size, self._init_scale),\n                              constraint=constraints.positive)\n        return dist.Normal(loc, scale)\n\n    def get_base_dist(self):\n        return dist.Normal(np.zeros(self.latent_size), 1).to_event(1)\n\n    def get_transform(self, params):\n        loc = params[\'{}_loc\'.format(self.prefix)]\n        scale = params[\'{}_scale\'.format(self.prefix)]\n        return AffineTransform(loc, scale, domain=constraints.real_vector)\n\n    def get_posterior(self, params):\n        """"""\n        Returns a diagonal Normal posterior distribution.\n        """"""\n        transform = self.get_transform(params)\n        return dist.Normal(transform.loc, transform.scale)\n\n    def median(self, params):\n        loc = params[\'{}_loc\'.format(self.prefix)]\n        return self._unpack_and_constrain(loc, params)\n\n    def quantiles(self, params, quantiles):\n        quantiles = np.array(quantiles)[..., None]\n        latent = self.get_posterior(params).icdf(quantiles)\n        return self._unpack_and_constrain(latent, params)\n\n\nclass AutoMultivariateNormal(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a MultivariateNormal\n    distribution to construct a guide over the entire latent space.\n    The guide does not depend on the model\'s ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoMultivariateNormal(model, ...)\n        svi = SVI(model, guide, ...)\n    """"""\n    def __init__(self, model, prefix=""auto"", init_strategy=init_to_uniform, init_scale=0.1):\n        if init_scale <= 0:\n            raise ValueError(""Expected init_scale > 0. but got {}"".format(init_scale))\n        self._init_scale = init_scale\n        super().__init__(model, prefix, init_strategy)\n\n    def _get_posterior(self):\n        loc = numpyro.param(\'{}_loc\'.format(self.prefix), self._init_latent)\n        scale_tril = numpyro.param(\'{}_scale_tril\'.format(self.prefix),\n                                   np.identity(self.latent_size) * self._init_scale,\n                                   constraint=constraints.lower_cholesky)\n        return dist.MultivariateNormal(loc, scale_tril=scale_tril)\n\n    def get_base_dist(self):\n        return dist.Normal(np.zeros(self.latent_size), 1).to_event(1)\n\n    def get_transform(self, params):\n        loc = params[\'{}_loc\'.format(self.prefix)]\n        scale_tril = params[\'{}_scale_tril\'.format(self.prefix)]\n        return MultivariateAffineTransform(loc, scale_tril)\n\n    def get_posterior(self, params):\n        """"""\n        Returns a multivariate Normal posterior distribution.\n        """"""\n        transform = self.get_transform(params)\n        return dist.MultivariateNormal(transform.loc, transform.scale_tril)\n\n    def median(self, params):\n        loc = params[\'{}_loc\'.format(self.prefix)]\n        return self._unpack_and_constrain(loc, params)\n\n    def quantiles(self, params, quantiles):\n        transform = self.get_transform(params)\n        quantiles = np.array(quantiles)[..., None]\n        latent = dist.Normal(transform.loc, np.diagonal(transform.scale_tril)).icdf(quantiles)\n        return self._unpack_and_constrain(latent, params)\n\n\nclass AutoLowRankMultivariateNormal(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a LowRankMultivariateNormal\n    distribution to construct a guide over the entire latent space.\n    The guide does not depend on the model\'s ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoLowRankMultivariateNormal(model, rank=2, ...)\n        svi = SVI(model, guide, ...)\n    """"""\n    def __init__(self, model, prefix=""auto"", init_strategy=init_to_uniform, init_scale=0.1, rank=None):\n        if init_scale <= 0:\n            raise ValueError(""Expected init_scale > 0. but got {}"".format(init_scale))\n        self._init_scale = init_scale\n        self.rank = rank\n        super(AutoLowRankMultivariateNormal, self).__init__(\n            model, prefix=prefix, init_strategy=init_strategy)\n\n    def _get_posterior(self, *args, **kwargs):\n        rank = int(round(self.latent_size ** 0.5)) if self.rank is None else self.rank\n        loc = numpyro.param(\'{}_loc\'.format(self.prefix), self._init_latent)\n        cov_factor = numpyro.param(\'{}_cov_factor\'.format(self.prefix), np.zeros((self.latent_size, rank)))\n        scale = numpyro.param(\'{}_scale\'.format(self.prefix),\n                              np.full(self.latent_size, self._init_scale),\n                              constraint=constraints.positive)\n        cov_diag = scale * scale\n        cov_factor = cov_factor * scale[..., None]\n        return dist.LowRankMultivariateNormal(loc, cov_factor, cov_diag)\n\n    def get_base_dist(self):\n        return dist.Normal(np.zeros(self.latent_size), 1).to_event(1)\n\n    def get_transform(self, params):\n        posterior = self.get_posterior(params)\n        return MultivariateAffineTransform(posterior.loc, posterior.scale_tril)\n\n    def get_posterior(self, params):\n        """"""\n        Returns a lowrank multivariate Normal posterior distribution.\n        """"""\n        loc = params[\'{}_loc\'.format(self.prefix)]\n        cov_factor = params[\'{}_cov_factor\'.format(self.prefix)]\n        scale = params[\'{}_scale\'.format(self.prefix)]\n        cov_diag = scale * scale\n        cov_factor = cov_factor * scale[..., None]\n        return dist.LowRankMultivariateNormal(loc, cov_factor, cov_diag)\n\n    def median(self, params):\n        loc = params[\'{}_loc\'.format(self.prefix)]\n        return self._unpack_and_constrain(loc, params)\n\n    def quantiles(self, params, quantiles):\n        transform = self.get_transform(params)\n        quantiles = np.array(quantiles)[..., None]\n        latent = dist.Normal(transform.loc, np.diagonal(transform.scale_tril)).icdf(quantiles)\n        return self._unpack_and_constrain(latent, params)\n\n\nclass AutoLaplaceApproximation(AutoContinuous):\n    r""""""\n    Laplace approximation (quadratic approximation) approximates the posterior\n    :math:`\\log p(z | x)` by a multivariate normal distribution in the\n    unconstrained space. Under the hood, it uses Delta distributions to\n    construct a MAP guide over the entire (unconstrained) latent space. Its\n    covariance is given by the inverse of the hessian of :math:`-\\log p(x, z)`\n    at the MAP point of `z`.\n\n    Usage::\n\n        guide = AutoLaplaceApproximation(model, ...)\n        svi = SVI(model, guide, ...)\n    """"""\n    def _setup_prototype(self, *args, **kwargs):\n        super(AutoLaplaceApproximation, self)._setup_prototype(*args, **kwargs)\n\n        def loss_fn(params):\n            # we are doing maximum likelihood, so only require `num_particles=1` and an arbitrary rng_key.\n            return ELBO().loss(random.PRNGKey(0), params, self.model, self, *args, **kwargs)\n\n        self._loss_fn = loss_fn\n\n    def _get_posterior(self, *args, **kwargs):\n        # sample from Delta guide\n        loc = numpyro.param(\'{}_loc\'.format(self.prefix), self._init_latent)\n        return dist.Delta(loc, event_dim=1)\n\n    def get_base_dist(self):\n        return dist.Normal(np.zeros(self.latent_size), 1).to_event(1)\n\n    def get_transform(self, params):\n        def loss_fn(z):\n            params1 = params.copy()\n            params1[\'{}_loc\'.format(self.prefix)] = z\n            return self._loss_fn(params1)\n\n        loc = params[\'{}_loc\'.format(self.prefix)]\n        precision = hessian(loss_fn)(loc)\n        scale_tril = cholesky_of_inverse(precision)\n        if not_jax_tracer(scale_tril):\n            if np.any(np.isnan(scale_tril)):\n                warnings.warn(""Hessian of log posterior at the MAP point is singular. Posterior""\n                              "" samples from AutoLaplaceApproxmiation will be constant (equal to""\n                              "" the MAP point)."")\n        scale_tril = np.where(np.isnan(scale_tril), 0., scale_tril)\n        return MultivariateAffineTransform(loc, scale_tril)\n\n    def get_posterior(self, params):\n        """"""\n        Returns a multivariate Normal posterior distribution.\n        """"""\n        transform = self.get_transform(params)\n        return dist.MultivariateNormal(transform.loc, scale_tril=transform.scale_tril)\n\n    def sample_posterior(self, rng_key, params, sample_shape=()):\n        latent_sample = self.get_posterior(params).sample(rng_key, sample_shape)\n        return self._unpack_and_constrain(latent_sample, params)\n\n    def median(self, params):\n        loc = params[\'{}_loc\'.format(self.prefix)]\n        return self._unpack_and_constrain(loc, params)\n\n    def quantiles(self, params, quantiles):\n        transform = self.get_transform(params)\n        quantiles = np.array(quantiles)[..., None]\n        latent = dist.Normal(transform.loc, np.diagonal(transform.scale_tril)).icdf(quantiles)\n        return self._unpack_and_constrain(latent, params)\n\n\nclass AutoIAFNormal(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a Diagonal Normal\n    distribution transformed via a\n    :class:`~numpyro.distributions.flows.InverseAutoregressiveTransform`\n    to construct a guide over the entire latent space. The guide does not\n    depend on the model\'s ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoIAFNormal(model, hidden_dims=[20], skip_connections=True, ...)\n        svi = SVI(model, guide, ...)\n\n    :param callable model: a generative model.\n    :param str prefix: a prefix that will be prefixed to all param internal sites.\n    :param callable init_strategy: A per-site initialization function.\n    :param int num_flows: the number of flows to be used, defaults to 3.\n    :param list hidden_dims: the dimensionality of the hidden units per layer.\n        Defaults to ``[latent_size, latent_size]``.\n    :param bool skip_connections: whether to add skip connections from the input to the\n        output of each flow. Defaults to False.\n    :param callable nonlinearity: the nonlinearity to use in the feedforward network.\n        Defaults to :func:`jax.experimental.stax.Elu`.\n    """"""\n    def __init__(self, model, prefix=""auto"", init_strategy=init_to_uniform,\n                 num_flows=3, hidden_dims=None, skip_connections=False, nonlinearity=stax.Elu):\n        self.num_flows = num_flows\n        # 2-layer, stax.Elu, skip_connections=False by default following the experiments in\n        # IAF paper (https://arxiv.org/abs/1606.04934)\n        # and Neutra paper (https://arxiv.org/abs/1903.03704)\n        self._hidden_dims = hidden_dims\n        self._skip_connections = skip_connections\n        self._nonlinearity = nonlinearity\n        super(AutoIAFNormal, self).__init__(model, prefix=prefix, init_strategy=init_strategy)\n\n    def _get_posterior(self):\n        if self.latent_size == 1:\n            raise ValueError(\'latent dim = 1. Consider using AutoDiagonalNormal instead\')\n        hidden_dims = [self.latent_size, self.latent_size] if self._hidden_dims is None else self._hidden_dims\n        flows = []\n        for i in range(self.num_flows):\n            if i > 0:\n                flows.append(PermuteTransform(np.arange(self.latent_size)[::-1]))\n            arn = AutoregressiveNN(self.latent_size, hidden_dims,\n                                   permutation=np.arange(self.latent_size),\n                                   skip_connections=self._skip_connections,\n                                   nonlinearity=self._nonlinearity)\n            arnn = numpyro.module(\'{}_arn__{}\'.format(self.prefix, i), arn, (self.latent_size,))\n            flows.append(InverseAutoregressiveTransform(arnn))\n        return dist.TransformedDistribution(self.get_base_dist(), flows)\n\n    def get_base_dist(self):\n        return dist.Normal(np.zeros(self.latent_size), 1).to_event(1)\n\n\nclass AutoBNAFNormal(AutoContinuous):\n    """"""\n    This implementation of :class:`AutoContinuous` uses a Diagonal Normal\n    distribution transformed via a\n    :class:`~numpyro.distributions.flows.BlockNeuralAutoregressiveTransform`\n    to construct a guide over the entire latent space. The guide does not\n    depend on the model\'s ``*args, **kwargs``.\n\n    Usage::\n\n        guide = AutoBNAFNormal(model, num_flows=1, hidden_factors=[50, 50], ...)\n        svi = SVI(model, guide, ...)\n\n    **References**\n\n    1. *Block Neural Autoregressive Flow*,\n       Nicola De Cao, Ivan Titov, Wilker Aziz\n\n    :param callable model: a generative model.\n    :param str prefix: a prefix that will be prefixed to all param internal sites.\n    :param callable init_strategy: A per-site initialization function.\n    :param int num_flows: the number of flows to be used, defaults to 3.\n    :param list hidden_factors: Hidden layer i has ``hidden_factors[i]`` hidden units per\n        input dimension. This corresponds to both :math:`a` and :math:`b` in reference [1].\n        The elements of hidden_factors must be integers.\n    """"""\n    def __init__(self, model, prefix=""auto"", init_strategy=init_to_uniform, num_flows=1,\n                 hidden_factors=[8, 8]):\n        self.num_flows = num_flows\n        self._hidden_factors = hidden_factors\n        super(AutoBNAFNormal, self).__init__(model, prefix=prefix, init_strategy=init_strategy)\n\n    def _get_posterior(self):\n        if self.latent_size == 1:\n            raise ValueError(\'latent dim = 1. Consider using AutoDiagonalNormal instead\')\n        flows = []\n        for i in range(self.num_flows):\n            if i > 0:\n                flows.append(PermuteTransform(np.arange(self.latent_size)[::-1]))\n            residual = ""gated"" if i < (self.num_flows - 1) else None\n            arn = BlockNeuralAutoregressiveNN(self.latent_size, self._hidden_factors, residual)\n            arnn = numpyro.module(\'{}_arn__{}\'.format(self.prefix, i), arn, (self.latent_size,))\n            flows.append(BlockNeuralAutoregressiveTransform(arnn))\n        return dist.TransformedDistribution(self.get_base_dist(), flows)\n\n    def get_base_dist(self):\n        return dist.Normal(np.zeros(self.latent_size), 1).to_event(1)\n'"
numpyro/contrib/distributions/__init__.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpyro.contrib.distributions.continuous import (\n    beta,\n    cauchy,\n    expon,\n    gamma,\n    halfcauchy,\n    halfnorm,\n    lognorm,\n    norm,\n    pareto,\n    t,\n    trunccauchy,\n    truncnorm,\n    uniform\n)\nfrom numpyro.contrib.distributions.discrete import bernoulli, binom, poisson\nfrom numpyro.contrib.distributions.distribution import (\n    jax_continuous,\n    jax_discrete,\n    jax_multivariate,\n    validation_enabled\n)\nfrom numpyro.contrib.distributions.multivariate import categorical, dirichlet, multinomial\n\n__all__ = [\n    'beta',\n    'bernoulli',\n    'binom',\n    'cauchy',\n    'categorical',\n    'dirichlet',\n    'expon',\n    'gamma',\n    'halfcauchy',\n    'halfnorm',\n    'jax_continuous',\n    'jax_discrete',\n    'jax_multivariate',\n    'lognorm',\n    'multinomial',\n    'norm',\n    'pareto',\n    'poisson',\n    't',\n    'trunccauchy',\n    'truncnorm',\n    'uniform',\n    'validation_enabled',\n]\n"""
numpyro/contrib/distributions/continuous.py,73,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# Source code modified from scipy.stats._continous_distns.py\n#\n# Copyright (c) 2001, 2002 Enthought, Inc.\n# All rights reserved.\n#\n# Copyright (c) 2003-2019 SciPy Developers.\n# All rights reserved.\n\nimport jax.numpy as np\nfrom jax.numpy.lax_numpy import _promote_dtypes\nimport jax.random as random\nfrom jax.scipy.special import digamma, gammaln, log_ndtr, ndtr, ndtri\n\nfrom numpyro.contrib.distributions.distribution import jax_continuous\nfrom numpyro.distributions import constraints\n\n\nclass beta_gen(jax_continuous):\n    arg_constraints = {\'a\': constraints.positive, \'b\': constraints.positive}\n    _support_mask = constraints.unit_interval\n\n    def _rvs(self, a, b):\n        # TODO: use upstream implementation when available\n        # XXX the implementation is different from PyTorch\'s one\n        # in PyTorch, a sample is generated from dirichlet distribution\n        key_a, key_b = random.split(self._random_state)\n        gamma_a = random.gamma(key_a, a, shape=self._size)\n        gamma_b = random.gamma(key_b, b, shape=self._size)\n        return gamma_a / (gamma_a + gamma_b)\n\n    def _cdf(self, x, a, b):\n        raise NotImplementedError(\'Missing jax.scipy.special.btdtr\')\n\n    def _ppf(self, q, a, b):\n        raise NotImplementedError(\'Missing jax.scipy.special.btdtri\')\n\n    def _stats(self, a, b):\n        mn = a * 1.0 / (a + b)\n        var = (a * b * 1.0) / (a + b + 1.0) / (a + b) ** 2.0\n        g1 = 2.0 * (b - a) * np.sqrt((1.0 + a + b) / (a * b)) / (2 + a + b)\n        g2 = 6.0 * (a ** 3 + a ** 2 * (1 - 2 * b) + b ** 2 * (1 + b) - 2 * a * b * (2 + b))\n        g2 = g2 / (a * b * (a + b + 2) * (a + b + 3))\n        return mn, var, g1, g2\n\n\nclass cauchy_gen(jax_continuous):\n    _support_mask = constraints.real\n\n    def _rvs(self):\n        return random.cauchy(self._random_state, shape=self._size)\n\n    def _cdf(self, x):\n        return 0.5 + 1.0 / np.pi * np.arctan(x)\n\n    def _ppf(self, q):\n        return np.tan(np.pi * q - np.pi / 2.0)\n\n    def _sf(self, x):\n        return 0.5 - 1.0 / np.pi * np.arctan(x)\n\n    def _isf(self, q):\n        return np.tan(np.pi / 2.0 - np.pi * q)\n\n    def _stats(self):\n        return np.nan, np.nan, np.nan, np.nan\n\n    def _entropy(self):\n        return np.log(4 * np.pi)\n\n\nclass expon_gen(jax_continuous):\n    _support_mask = constraints.positive\n\n    def _rvs(self):\n        return random.exponential(self._random_state, shape=self._size)\n\n    def _cdf(self, x):\n        return -np.expm1(-x)\n\n    def _ppf(self, q):\n        return -np.log1p(-q)\n\n    def _sf(self, x):\n        return np.exp(-x)\n\n    def _logsf(self, x):\n        return -x\n\n    def _isf(self, q):\n        return -np.log(q)\n\n    def _stats(self):\n        return 1.0, 1.0, 2.0, 6.0\n\n    def _entropy(self):\n        return 1.0\n\n\nclass gamma_gen(jax_continuous):\n    arg_constraints = {\'a\': constraints.positive}\n    _support_mask = constraints.positive\n\n    def _rvs(self, a):\n        return random.gamma(self._random_state, a, shape=self._size)\n\n    def _cdf(self, x, a):\n        raise NotImplementedError(\'Missing jax.scipy.special.gammainc\')\n\n    def _sf(self, x, a):\n        raise NotImplementedError(\'Missing jax.scipy.special.gammainc\')\n\n    def _ppf(self, q, a):\n        raise NotImplementedError(\'Missing jax.scipy.special.gammaincinv\')\n\n    def _stats(self, a):\n        return a, a, 2.0 / np.sqrt(a), 6.0 / a\n\n    def _entropy(self, a):\n        return digamma(a) * (1 - a) + a + gammaln(a)\n\n\nclass halfcauchy_gen(jax_continuous):\n    _support_mask = constraints.positive\n\n    def _rvs(self):\n        return np.abs(random.cauchy(self._random_state, shape=self._size))\n\n    def _pdf(self, x):\n        return 2.0 / np.pi / (1.0 + x * x)\n\n    def _logpdf(self, x):\n        return np.log(2.0 / np.pi) - np.log1p(x * x)\n\n    def _cdf(self, x):\n        return 2.0 / np.pi * np.arctan(x)\n\n    def _ppf(self, q):\n        return np.tan(np.pi / 2 * q)\n\n    def _stats(self):\n        return np.inf, np.inf, np.nan, np.nan\n\n    def _entropy(self):\n        return np.log(2 * np.pi)\n\n\nclass halfnorm_gen(jax_continuous):\n    _support_mask = constraints.positive\n\n    def _rvs(self):\n        return np.abs(random.normal(self._random_state, shape=self._size))\n\n    def _pdf(self, x):\n        # halfnorm.pdf(x) = sqrt(2/pi) * exp(-x**2/2)\n        return np.sqrt(2.0 / np.pi) * np.exp(-x * x / 2.0)\n\n    def _logpdf(self, x):\n        return 0.5 * np.log(2.0 / np.pi) - x * x / 2.0\n\n    def _cdf(self, x):\n        return norm._cdf(x) * 2 - 1.0\n\n    def _ppf(self, q):\n        return norm._ppf((1 + q) / 2.0)\n\n    def _stats(self):\n        return (np.sqrt(2.0 / np.pi), 1 - 2.0 / np.pi,\n                np.sqrt(2) * (4 - np.pi) / (np.pi - 2) ** 1.5, 8 * (np.pi - 3) / (np.pi - 2) ** 2)\n\n    def _entropy(self):\n        return 0.5 * np.log(np.pi / 2.0) + 0.5\n\n\nclass lognorm_gen(jax_continuous):\n    arg_constraints = {\'s\': constraints.positive}\n    _support_mask = constraints.positive\n\n    def _rvs(self, s):\n        # TODO: use upstream implementation when available\n        return np.exp(s * random.normal(self._random_state, shape=self._size))\n\n    def _pdf(self, x, s):\n        # lognorm.pdf(x, s) = 1 / (s*x*sqrt(2*pi)) * exp(-1/2*(log(x)/s)**2)\n        return np.exp(self._logpdf(x, s))\n\n    def _logpdf(self, x, s):\n        return np.where(x != 0,\n                        -np.log(x) ** 2 / (2 * s ** 2) - np.log(s * x * np.sqrt(2 * np.pi)),\n                        -np.inf)\n\n    def _cdf(self, x, s):\n        return norm._cdf(np.log(x) / s)\n\n    def _logcdf(self, x, s):\n        return norm._logcdf(np.log(x) / s)\n\n    def _ppf(self, q, s):\n        return np.exp(s * norm._ppf(q))\n\n    def _sf(self, x, s):\n        return norm._sf(np.log(x) / s)\n\n    def _logsf(self, x, s):\n        return norm._logsf(-np.log(x) / s)\n\n    def _stats(self, s):\n        p = np.exp(s * s)\n        mu = np.sqrt(p)\n        mu2 = p * (p - 1)\n        g1 = np.sqrt(p - 1) * (2 + p)\n        g2 = np.polyval([1, 2, 3, 0, -6.0], p)\n        return mu, mu2, g1, g2\n\n    def _entropy(self, s):\n        return 0.5 * (1 + np.log(2 * np.pi) + 2 * np.log(s))\n\n\nclass norm_gen(jax_continuous):\n    _support_mask = constraints.real\n\n    def _rvs(self):\n        return random.normal(self._random_state, shape=self._size)\n\n    def _pdf(self, x):\n        # norm.pdf(x) = exp(-x**2/2)/sqrt(2*pi)\n        return np.exp(-x**2 / 2.0) / np.sqrt(2 * np.pi)\n\n    def _logpdf(self, x):\n        return -(x ** 2 + np.log(2 * np.pi)) / 2.0\n\n    def _cdf(self, x):\n        return ndtr(x)\n\n    def _logcdf(self, x):\n        return log_ndtr(x)\n\n    def _sf(self, x):\n        return ndtr(-x)\n\n    def _logsf(self, x):\n        return log_ndtr(-x)\n\n    def _ppf(self, q):\n        return ndtri(q)\n\n    def _isf(self, q):\n        return -ndtri(q)\n\n    def _stats(self):\n        return 0.0, 1.0, 0.0, 0.0\n\n    def _entropy(self):\n        return 0.5 * (np.log(2 * np.pi) + 1)\n\n\nclass pareto_gen(jax_continuous):\n    arg_constraints = {\'b\': constraints.positive}\n    _support_mask = constraints.greater_than(1)\n\n    def _rvs(self, b):\n        return random.pareto(self._random_state, b, shape=self._size)\n\n    def _cdf(self, x, b):\n        return 1 - x ** (-b)\n\n    def _ppf(self, q, b):\n        return np.pow(1 - q, -1.0 / b)\n\n    def _sf(self, x, b):\n        return x ** (-b)\n\n    def _stats(self, b, moments=\'mv\'):\n        mu, mu2, g1, g2 = None, None, None, None\n        if \'m\' in moments:\n            mask = b > 1\n            bt = np.extract(mask, b)\n            mu = np.where(mask, bt / (bt - 1.0), np.inf)\n        if \'v\' in moments:\n            mask = b > 2\n            bt = np.extract(mask, b)\n            mu2 = np.where(mask, bt / (bt - 2.0) / (bt - 1.0) ** 2, np.inf)\n        if \'s\' in moments:\n            mask = b > 3\n            bt = np.extract(mask, b)\n            vals = 2 * (bt + 1.0) * np.sqrt(bt - 2.0) / ((bt - 3.0) * np.sqrt(bt))\n            g1 = np.where(mask, vals, np.nan)\n        if \'k\' in moments:\n            mask = b > 4\n            bt = np.extract(mask, b)\n            vals = (6.0 * np.polyval([1.0, 1.0, -6, -2], bt)\n                    / np.polyval([1.0, -7.0, 12.0, 0.0], bt))\n            g2 = np.where(mask, vals, np.nan)\n        return mu, mu2, g1, g2\n\n    def _entropy(self, c):\n        return 1 + 1.0 / c - np.log(c)\n\n\nclass t_gen(jax_continuous):\n    arg_constraints = {\'df\': constraints.positive}\n    _support_mask = constraints.real\n\n    def _rvs(self, df):\n        # TODO: use upstream implementation when available\n        key_n, key_g = random.split(self._random_state)\n        normal = random.normal(key_n, shape=self._size)\n        half_df = df / 2.0\n        gamma = random.gamma(key_n, half_df, shape=self._size)\n        return normal * np.sqrt(half_df / gamma)\n\n    def _cdf(self, x, df):\n        raise NotImplementedError(\'Missing jax.scipy.special.stdtr\')\n\n    def _sf(self, x, df):\n        raise NotImplementedError(\'Missing jax.scipy.special.stdtr\')\n\n    def _ppf(self, q, df):\n        raise NotImplementedError(\'Missing jax.scipy.special.stdtrit\')\n\n    def _isf(self, q, df):\n        raise NotImplementedError(\'Missing jax.scipy.special.stdtrit\')\n\n    def _stats(self, df):\n        mu = np.where(df > 1, 0.0, np.inf)\n        mu2 = np.where(df > 2, df / (df - 2.0), np.inf)\n        mu2 = np.where(df <= 1, np.nan, mu2)\n        g1 = np.where(df > 3, 0.0, np.nan)\n        g2 = np.where(df > 4, 6.0 / (df - 4.0), np.inf)\n        g2 = np.where(df <= 2, np.nan, g2)\n        return mu, mu2, g1, g2\n\n\nclass trunccauchy_gen(jax_continuous):\n    # TODO: override _argcheck with the constraint that a < b\n\n    def _support(self, *args, **kwargs):\n        (a, b), loc, scale = self._parse_args(*args, **kwargs)\n        # TODO: make constraints.less_than and support a == -np.inf\n        if b == np.inf:\n            return constraints.greater_than((a - loc) * scale)\n        else:\n            return constraints.interval((a - loc) * scale, (b - loc) * scale)\n\n    def _rvs(self, a, b):\n        # We use inverse transform method:\n        # z ~ ppf(U), where U ~ Uniform(cdf(a), cdf(b)).\n        #                     ~ Uniform(arctan(a), arctan(b)) / pi + 1/2\n        u = random.uniform(self._random_state, shape=self._size,\n                           minval=np.arctan(a), maxval=np.arctan(b))\n        return np.tan(u)\n\n    def _pdf(self, x, a, b):\n        return np.reciprocal((1 + x * x) * (np.arctan(b) - np.arctan(a)))\n\n    def _logpdf(self, x, a, b):\n        # trunc_pdf(x) = pdf(x) / (cdf(b) - cdf(a))\n        #              = 1 / (1 + x^2) / (arctan(b) - arctan(a))\n        normalizer = np.log(np.arctan(b) - np.arctan(a))\n        return -(np.log(1 + x * x) + normalizer)\n\n\nclass truncnorm_gen(jax_continuous):\n    # TODO: override _argcheck with the constraint that a < b\n\n    def _support(self, *args, **kwargs):\n        (a, b), loc, scale = self._parse_args(*args, **kwargs)\n        # TODO: make constraints.less_than and support a == -np.inf\n        if b == np.inf:\n            return constraints.greater_than((a - loc) * scale)\n        else:\n            return constraints.interval((a - loc) * scale, (b - loc) * scale)\n\n    def _rvs(self, a, b):\n        # We use inverse transform method:\n        # z ~ ppf(U), where U ~ Uniform(0, 1).\n        u = random.uniform(self._random_state, shape=self._size)\n        return self._ppf(u, a, b)\n\n    def _pdf(self, x, a, b):\n        delta = norm._sf(a) - norm._sf(b)\n        return norm._pdf(x) / delta\n\n    def _logpdf(self, x, a, b):\n        x, a, b = _promote_dtypes(x, a, b)\n        # XXX: consider to use norm._cdf(b) - norm._cdf(a) when a, b < 0\n        delta = norm._sf(a) - norm._sf(b)\n        return norm._logpdf(x) - np.log(delta)\n\n    def _cdf(self, x, a, b):\n        delta = norm._sf(a) - norm._sf(b)\n        return (norm._cdf(x) - norm._cdf(a)) / delta\n\n    def _ppf(self, q, a, b):\n        q, a, b = _promote_dtypes(q, a, b)\n        # XXX: consider to use norm._ppf(q * norm._cdf(b) + norm._cdf(a) * (1.0 - q))\n        # when a, b < 0\n        ppf = norm._isf(q * norm._sf(b) + norm._sf(a) * (1.0 - q))\n        return ppf\n\n    def _stats(self, a, b):\n        nA, nB = norm._cdf(a), norm._cdf(b)\n        d = nB - nA\n        pA, pB = norm._pdf(a), norm._pdf(b)\n        mu = (pA - pB) / d   # correction sign\n        mu2 = 1 + (a * pA - b * pB) / d - mu * mu\n        return mu, mu2, None, None\n\n\nclass uniform_gen(jax_continuous):\n    _support_mask = constraints.unit_interval\n\n    def _rvs(self):\n        return random.uniform(self._random_state, shape=self._size)\n\n    def _cdf(self, x):\n        return x\n\n    def _ppf(self, q):\n        return q\n\n    def _stats(self):\n        return 0.5, 1.0 / 12, 0, -1.2\n\n    def _entropy(self):\n        return 0.0\n\n\nbeta = beta_gen(a=0.0, b=1.0, name=\'beta\')\ncauchy = cauchy_gen(name=\'cauchy\')\nexpon = expon_gen(a=0.0, name=\'expon\')\ngamma = gamma_gen(a=0.0, name=\'gamma\')\nhalfcauchy = halfcauchy_gen(a=0.0, name=\'halfcauchy\')\nhalfnorm = halfnorm_gen(a=0.0, name=\'halfnorm\')\nlognorm = lognorm_gen(a=0.0, name=\'lognorm\')\nnorm = norm_gen(name=\'norm\')\npareto = pareto_gen(a=1.0, name=""pareto"")\nt = t_gen(name=\'t\')\ntrunccauchy = trunccauchy_gen(name=\'trunccauchy\')\ntruncnorm = truncnorm_gen(name=\'truncnorm\')\nuniform = uniform_gen(a=0.0, b=1.0, name=\'uniform\')\n'"
numpyro/contrib/distributions/discrete.py,12,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# Source code modified from scipy.stats._discrete_distns.py\n#\n# Copyright (c) 2001, 2002 Enthought, Inc.\n# All rights reserved.\n#\n# Copyright (c) 2003-2019 SciPy Developers.\n# All rights reserved.\n\nimport numpy as onp\n\nfrom jax import device_put, random\nimport jax.numpy as np\nfrom jax.numpy.lax_numpy import _promote_dtypes\nfrom jax.scipy.special import entr, expit, gammaln, xlog1py, xlogy\n\nfrom numpyro.contrib.distributions.distribution import jax_discrete\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.util import binary_cross_entropy_with_logits\n\n\nclass bernoulli_gen(jax_discrete):\n    _support_mask = constraints.integer_interval(0, 1)\n\n    @property\n    def arg_constraints(self):\n        if self.is_logits:\n            return {'p': constraints.real}\n        else:\n            return {'p': constraints.unit_interval}\n\n    def _rvs(self, p):\n        if self.is_logits:\n            p = expit(p)\n        return random.bernoulli(self._random_state, p, self._size)\n\n    def _logpmf(self, x, p):\n        if self.is_logits:\n            return -binary_cross_entropy_with_logits(p, x)\n        else:\n            # TODO: consider always clamp and convert probs to logits\n            return xlogy(x, p) + xlog1py(1 - x, -p)\n\n    def _pmf(self, x, p):\n        return np.exp(self._logpmf(x, p))\n\n    def _cdf(self, x, p):\n        return binom._cdf(x, 1, p)\n\n    def _sf(self, x, p):\n        return binom._sf(x, 1, p)\n\n    def _ppf(self, q, p):\n        return binom._ppf(q, 1, p)\n\n    def _stats(self, p):\n        return binom._stats(1, p)\n\n    def _entropy(self, p):\n        # TODO: use logits and binary_cross_entropy_with_logits for more stable\n        if self.is_logits:\n            p = expit(p)\n        return entr(p) + entr(1 - p)\n\n\nclass binom_gen(jax_discrete):\n    @property\n    def arg_constraints(self):\n        if self.is_logits:\n            return {'n': constraints.nonnegative_integer,\n                    'p': constraints.real}\n        else:\n            return {'n': constraints.nonnegative_integer,\n                    'p': constraints.unit_interval}\n\n    def _support(self, *args, **kwargs):\n        (n, p), loc, _ = self._parse_args(*args, **kwargs)\n        return constraints.integer_interval(loc, loc + n)\n\n    def _rvs(self, n, p):\n        if self.is_logits:\n            p = expit(p)\n        # use scipy samplers directly and put the samples on device later.\n        # TODO: use util.binomial instead\n        random_state = onp.random.RandomState(self._random_state)\n        sample = random_state.binomial(n, p, self._size)\n        return device_put(sample)\n\n    def _logpmf(self, x, n, p):\n        x, n, p = _promote_dtypes(x, n, p)\n        combiln = gammaln(n + 1) - (gammaln(x + 1) + gammaln(n - x + 1))\n        if self.is_logits:\n            # TODO: move this implementation to PyTorch if it does not get non-continuous problem\n            # In PyTorch, k * logit - n * log1p(e^logit) get overflow when logit is a large\n            # positive number. In that case, we can reformulate into\n            # k * logit - n * log1p(e^logit) = k * logit - n * (log1p(e^-logit) + logit)\n            #                                = k * logit - n * logit - n * log1p(e^-logit)\n            # More context: https://github.com/pytorch/pytorch/pull/15962/\n            return combiln + x * p - (n * np.clip(p, 0) + xlog1py(n, np.exp(-np.abs(p))))\n        else:\n            return combiln + xlogy(x, p) + xlog1py(n - x, -p)\n\n    def _pmf(self, x, n, p):\n        return np.exp(self._logpmf(x, n, p))\n\n    def _cdf(self, x, n, p):\n        raise NotImplementedError('Missing jax.scipy.special.bdtr')\n\n    def _sf(self, x, n, p):\n        raise NotImplementedError('Missing jax.scipy.special.bdtrc')\n\n    def _ppf(self, q, n, p):\n        raise NotImplementedError('Missing jax.scipy.special.bdtrk')\n\n    def _stats(self, n, p, moments='mv'):\n        q = 1.0 - p\n        mu = n * p\n        var = n * p * q\n        g1, g2 = None, None\n        if 's' in moments:\n            g1 = (q - p) / np.sqrt(var)\n        if 'k' in moments:\n            g2 = (1.0 - 6 * p * q) / var\n        return mu, var, g1, g2\n\n    def _entropy(self, n, p):\n        if self.is_logits:\n            p = expit(p)\n        k = np.arange(n + 1)\n        vals = self._pmf(k, n, p)\n        return np.sum(entr(vals), axis=0)\n\n\nclass poisson_gen(jax_discrete):\n    arg_constraints = {'mu': constraints.positive}\n    _support_mask = constraints.nonnegative_integer\n\n    def _rvs(self, mu):\n        random_state = onp.random.RandomState(self._random_state)\n        sample = random_state.poisson(mu, self._size)\n        return device_put(sample)\n\n    def _logpmf(self, x, mu):\n        x, mu = _promote_dtypes(x, mu)\n        Pk = xlogy(x, mu) - gammaln(x + 1) - mu\n        return Pk\n\n    def _pmf(self, x, mu):\n        # poisson.pmf(k) = exp(-mu) * mu**k / k!\n        return np.exp(self._logpmf(x, mu))\n\n    def _cdf(self, x, mu):\n        raise NotImplementedError('Missing jax.scipy.special.pdtr')\n\n    def _sf(self, x, mu):\n        raise NotImplementedError('Missing jax.scipy.special.pdtrc')\n\n    def _ppf(self, q, mu):\n        raise NotImplementedError('Missing jax.scipy.special.pdtrk')\n\n    def _stats(self, mu):\n        var = mu\n        tmp = np.asarray(mu)\n        mu_nonzero = tmp > 0\n        g1 = np.where(mu_nonzero, np.sqrt(1.0 / tmp), np.inf)\n        g2 = np.where(mu_nonzero, 1.0 / tmp, np.inf)\n        return mu, var, g1, g2\n\n\nbernoulli = bernoulli_gen(b=1, name='bernoulli')\nbinom = binom_gen(name='binom')\npoisson = poisson_gen(name='poisson')\n"""
numpyro/contrib/distributions/distribution.py,7,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# Source code modified from scipy.stats._distn_infrastructure.py\n#\n# Copyright (c) 2001, 2002 Enthought, Inc.\n# All rights reserved.\n#\n# Copyright (c) 2003-2019 SciPy Developers.\n# All rights reserved.\nfrom contextlib import contextmanager\n\nfrom scipy._lib._util import getargspec_no_self\nimport scipy.stats as osp_stats\nfrom scipy.stats._distn_infrastructure import instancemethod, rv_frozen, rv_generic\n\nfrom jax import lax\nimport jax.numpy as np\nfrom jax.random import _is_prng_key\nfrom jax.scipy import stats\n\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.transforms import AffineTransform\n\n\nclass jax_frozen(rv_frozen):\n    _validate_args = False\n\n    def __init__(self, dist, *args, **kwargs):\n        self.args = args\n        self.kwds = kwargs\n\n        # create a new instance\n        self.dist = dist.__class__(**dist._updated_ctor_param())\n\n        shapes, _, scale = self.dist._parse_args(*args, **kwargs)\n        if self._validate_args:\n            # TODO: check more concretely for each parameter\n            if not np.all(self.dist._argcheck(*shapes)):\n                raise ValueError(\'Invalid parameters provided to the distribution.\')\n            if not np.all(scale > 0):\n                raise ValueError(\'Invalid scale parameter provided to the distribution.\')\n\n        self.a, self.b = self.dist.a, self.dist.b\n\n    @property\n    def support(self):\n        return self.dist._support(*self.args, **self.kwds)\n\n    def __call__(self, size=None, random_state=None):\n        return self.rvs(size, random_state)\n\n    def log_prob(self, x):\n        if isinstance(self.dist, jax_continuous):\n            return self.logpdf(x)\n        else:\n            return self.logpmf(x)\n\n    def logpdf(self, x):\n        if self._validate_args:\n            self._validate_sample(x)\n        return self.dist.logpdf(x, *self.args, **self.kwds)\n\n    def logpmf(self, k):\n        if self._validate_args:\n            self._validate_sample(k)\n        return self.dist.logpmf(k, *self.args, **self.kwds)\n\n    def _validate_sample(self, x):\n        if not np.all(self.support(x)):\n            raise ValueError(\'Invalid values provided to log prob method. \'\n                             \'The value argument must be within the support.\')\n\n\nclass jax_generic(rv_generic):\n    arg_constraints = {}\n\n    def freeze(self, *args, **kwargs):\n        return jax_frozen(self, *args, **kwargs)\n\n    def _argcheck(self, *args):\n        cond = 1\n        constraints = self.arg_constraints\n        if args:\n            for arg, arg_name in zip(args, self.shapes.split(\', \')):\n                if arg_name in constraints:\n                    cond = np.logical_and(cond, constraints[arg_name](arg))\n        return cond\n\n\nclass jax_continuous(jax_generic, osp_stats.rv_continuous):\n    def _support(self, *args, **kwargs):\n        # support of the transformed distribution\n        _, loc, scale = self._parse_args(*args, **kwargs)\n        return AffineTransform(loc, scale, domain=self._support_mask).codomain\n\n    def rvs(self, *args, **kwargs):\n        rng_key = kwargs.pop(\'random_state\')\n        if rng_key is None:\n            rng_key = self.random_state\n        # assert that rng_key is PRNGKey and not mtrand.RandomState object from numpy.\n        assert _is_prng_key(rng_key)\n\n        args = list(args)\n        # If \'size\' is not in kwargs, then it is either the last element of args\n        # or it will take default value (which is None).\n        # Note: self.numargs is the number of shape parameters.\n        size = kwargs.pop(\'size\', args.pop() if len(args) > (self.numargs + 2) else None)\n        # XXX when args is not empty, parse_args requires either _pdf or _cdf method is implemented\n        # to recognize valid arg signatures (e.g. `a` in `gamma` or `s` in lognormal)\n        args, loc, scale = self._parse_args(*args, **kwargs)\n        if not size:\n            shapes = [np.shape(arg) for arg in args] + [np.shape(loc), np.shape(scale)]\n            size = lax.broadcast_shapes(*shapes)\n        elif isinstance(size, int):\n            size = (size,)\n\n        self._random_state = rng_key\n        self._size = size\n        vals = self._rvs(*args)\n        return vals * scale + loc\n\n    def pdf(self, x, *args, **kwargs):\n        if hasattr(stats, self.name):\n            return getattr(stats, self.name).pdf(x, *args, **kwargs)\n        else:\n            return super(jax_continuous, self).pdf(x, *args, **kwargs)\n\n    def logpdf(self, x, *args, **kwargs):\n        if hasattr(stats, self.name):\n            return getattr(stats, self.name).logpdf(x, *args, **kwargs)\n        else:\n            args, loc, scale = self._parse_args(*args, **kwargs)\n            y = (x - loc) / scale\n            return self._logpdf(y, *args) - np.log(scale)\n\n\nclass jax_discrete(jax_generic, osp_stats.rv_discrete):\n    def __new__(cls, *args, **kwargs):\n        return super(jax_discrete, cls).__new__(cls)\n\n    def __init__(self, *args, **kwargs):\n        self.is_logits = kwargs.pop(""is_logits"", False)\n        super(jax_discrete, self).__init__(*args, **kwargs)\n\n    def freeze(self, *args, **kwargs):\n        self._ctor_param.update(is_logits=kwargs.pop(""is_logits"", False))\n        return super(jax_discrete, self).freeze(*args, **kwargs)\n\n    def _support(self, *args, **kwargs):\n        args, loc, _ = self._parse_args(*args, **kwargs)\n        support_mask = self._support_mask\n        if isinstance(support_mask, constraints.integer_interval):\n            return constraints.integer_interval(loc + support_mask.lower_bound,\n                                                loc + support_mask.upper_bound)\n        elif isinstance(support_mask, constraints.integer_greater_than):\n            return constraints.integer_greater_than(loc + support_mask.lower_bound)\n        else:\n            raise NotImplementedError\n\n    def rvs(self, *args, **kwargs):\n        rng_key = kwargs.pop(\'random_state\')\n        if rng_key is None:\n            rng_key = self.random_state\n        # assert that rng_key is PRNGKey and not mtrand.RandomState object from numpy.\n        assert _is_prng_key(rng_key)\n\n        args = list(args)\n        size = kwargs.pop(\'size\', args.pop() if len(args) > (self.numargs + 1) else None)\n        args, loc, _ = self._parse_args(*args, **kwargs)\n        if not size:\n            shapes = [np.shape(arg) for arg in args] + [np.shape(loc)]\n            size = lax.broadcast_shapes(*shapes)\n        elif isinstance(size, int):\n            size = (size,)\n\n        self._random_state = rng_key\n        self._size = size\n        vals = self._rvs(*args)\n        return vals + loc\n\n    def logpmf(self, k, *args, **kwargs):\n        args, loc, _ = self._parse_args(*args, **kwargs)\n        k = k - loc\n        return self._logpmf(k, *args)\n\n\n_parse_arg_template = """"""\ndef _parse_args(self, {shape_arg_str}):\n    return ({shape_arg_str}), 0, 1\n""""""\n\n\nclass jax_multivariate(jax_generic):\n    def _construct_argparser(self, *args, **kwargs):\n        if self.shapes:\n            shapes = self.shapes.replace(\',\', \' \').split()\n        else:\n            shapes = getargspec_no_self(self._rvs).args\n\n        # have the arguments, construct the method from template\n        shapes_str = \', \'.join(shapes) + \', \' if shapes else \'\'  # NB: not None\n        dct = dict(shape_arg_str=shapes_str)\n        ns = {}\n        exec(_parse_arg_template.format(**dct), ns)\n        # NB: attach to the instance, not class\n        for name in [\'_parse_args\']:\n            setattr(self, name, instancemethod(ns[name], self, self.__class__))\n\n        self.shapes = \', \'.join(shapes) if shapes else None\n        if not hasattr(self, \'numargs\'):\n            self.numargs = len(shapes)\n\n    def _support(self, *args, **kwargs):\n        return self._support_mask\n\n    def rvs(self, *args, **kwargs):\n        rng_key = kwargs.pop(\'random_state\')\n        if rng_key is None:\n            rng_key = self.random_state\n        # assert that rng_key is PRNGKey and not mtrand.RandomState object from numpy.\n        assert _is_prng_key(rng_key)\n\n        args = list(args)\n        size = kwargs.pop(\'size\', args.pop() if len(args) > self.numargs else None)\n        args, _, _ = self._parse_args(*args, **kwargs)\n        if not size:\n            size = self._batch_shape(*args)\n        elif isinstance(size, int):\n            size = (size,)\n\n        self._random_state = rng_key\n        self._size = size\n        return self._rvs(*args)\n\n\n@contextmanager\ndef validation_enabled():\n    jax_frozen_flag = jax_frozen._validate_args\n    try:\n        jax_frozen._validate_args = True\n        yield\n    finally:\n        jax_frozen._validate_args = jax_frozen_flag\n'"
numpyro/contrib/distributions/multivariate.py,21,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# Source code modified from scipy.stats._multivariate.py\n#\n# Copyright (c) 2001, 2002 Enthought, Inc.\n# All rights reserved.\n#\n# Copyright (c) 2003-2019 SciPy Developers.\n# All rights reserved.\n\nfrom jax import lax, random\nfrom jax.nn import softmax\nimport jax.numpy as np\nfrom jax.numpy.lax_numpy import _promote_dtypes\nfrom jax.scipy.special import digamma, entr, gammaln, logsumexp, xlogy\n\nfrom numpyro.contrib.distributions.discrete import binom\nfrom numpyro.contrib.distributions.distribution import jax_continuous, jax_discrete, jax_multivariate\nfrom numpyro.distributions import constraints\nfrom numpyro.distributions.util import categorical as categorical_rvs\nfrom numpyro.distributions.util import multinomial as multinomial_rvs\n\n\ndef _lnB(alpha):\n    return np.sum(gammaln(alpha), axis=-1) - gammaln(np.sum(alpha, axis=-1))\n\n\n# TODO: either use multivariate docs instead of the default one of\n# rv_continuous/rv_discrete\n\nclass categorical_gen(jax_multivariate, jax_discrete):\n    @property\n    def arg_constraints(self):\n        if self.is_logits:\n            return {\'p\': constraints.real}\n        else:\n            return {\'p\': constraints.simplex}\n\n    def _support(self, *args, **kwargs):\n        (p,), _, _ = self._parse_args(*args, **kwargs)\n        return constraints.integer_interval(0, p.shape[-1] - 1)\n\n    def _batch_shape(self, p):\n        return p.shape[:-1]\n\n    def _event_shape(self, p):\n        return ()\n\n    def logpmf(self, x, p):\n        batch_shape = lax.broadcast_shapes(x.shape, p.shape[:-1])\n        # append a dimension to x\n        # TODO: consider to convert x.dtype to int\n        x = np.expand_dims(x, axis=-1)\n        x = np.broadcast_to(x, batch_shape + (1,))\n        p = np.broadcast_to(p, batch_shape + p.shape[-1:])\n        if self.is_logits:\n            # normalize log prob\n            p = p - logsumexp(p, axis=-1, keepdims=True)\n            # gather and remove the trailing dimension\n            return np.take_along_axis(p, x, axis=-1)[..., 0]\n        else:\n            return np.take_along_axis(np.log(p), x, axis=-1)[..., 0]\n\n    def pmf(self, x, p):\n        return np.exp(self.logpmf(x, p))\n\n    def _rvs(self, p):\n        if self.is_logits:\n            p = softmax(p)\n        return categorical_rvs(self._random_state, p, self._size)\n\n\nclass dirichlet_gen(jax_multivariate, jax_continuous):\n    arg_constraints = {""alpha"": constraints.positive}\n    _support_mask = constraints.simplex\n\n    def _batch_shape(self, alpha):\n        return alpha.shape[:-1]\n\n    def _event_shape(self, alpha):\n        return alpha.shape[-1:]\n\n    def logpdf(self, x, alpha):\n        lnB = _lnB(alpha)\n        return -lnB + np.sum(xlogy(alpha - 1, x), axis=-1)\n\n    def pdf(self, x, alpha):\n        return np.exp(self.logpdf(x, alpha))\n\n    def mean(self, alpha):\n        return alpha / np.sum(alpha, axis=-1, keepdims=True)\n\n    def var(self, alpha):\n        alpha0 = np.sum(alpha, axis=-1, keepdims=True)\n        return (alpha * (alpha0 - alpha)) / ((alpha0 * alpha0) * (alpha0 + 1))\n\n    def entropy(self, alpha):\n        alpha0 = np.sum(alpha, axis=-1)\n        lnB = _lnB(alpha)\n        K = alpha.shape[-1]\n        return lnB + (alpha0 - K) * digamma(alpha0) - np.inner((alpha - 1) * digamma(alpha))\n\n    def _rvs(self, alpha):\n        K = alpha.shape[-1]\n        gamma_samples = random.gamma(self._random_state, alpha, shape=self._size + (K,))\n        return gamma_samples / np.sum(gamma_samples, axis=-1, keepdims=True)\n\n\nclass multinomial_gen(jax_multivariate, jax_discrete):\n    def _support(self, *args, **kwargs):\n        (n, p), _, _ = self._parse_args(*args, **kwargs)\n        return constraints.integer_interval(0, n)\n\n    @property\n    def arg_constraints(self):\n        if self.is_logits:\n            return {\'n\': constraints.nonnegative_integer,\n                    \'p\': constraints.real}\n        else:\n            return {\'n\': constraints.nonnegative_integer,\n                    \'p\': constraints.simplex}\n\n    def _batch_shape(self, n, p):\n        return p.shape[:-1]\n\n    def _event_shape(self, n, p):\n        return p.shape[-1:]\n\n    def logpmf(self, x, n, p):\n        x, n, p = _promote_dtypes(x, n, p)\n        if self.is_logits:\n            return gammaln(n + 1) + np.sum(x * p - gammaln(x + 1), axis=-1) - n * logsumexp(p, axis=-1)\n        else:\n            return gammaln(n + 1) + np.sum(xlogy(x, p) - gammaln(x + 1), axis=-1)\n\n    def pmf(self, x, n, p):\n        return np.exp(self.logpmf(x, n, p))\n\n    def entropy(self, n, p):\n        x = np.arange(1, np.max(n) + 1)\n\n        term1 = n * np.sum(entr(p), axis=-1) - gammaln(n + 1)\n\n        n = n[..., np.newaxis]\n        new_axes_needed = max(p.ndim, n.ndim) - x.ndim + 1\n        x.shape += (1,) * new_axes_needed\n\n        term2 = np.sum(binom.pmf(x, n, p) * gammaln(x + 1),\n                       axis=(-1, -1 - new_axes_needed))\n\n        return term1 + term2\n\n    def _rvs(self, n, p):\n        if self.is_logits:\n            p = softmax(p)\n        return multinomial_rvs(self._random_state, p, n, self._size)\n\n\ncategorical = categorical_gen(name=\'categorical\')\ndirichlet = dirichlet_gen(name=\'dirichlet\')\nmultinomial = multinomial_gen(name=\'multinomial\')\n'"
numpyro/contrib/nn/__init__.py,0,"b""# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom numpyro.contrib.nn.auto_reg_nn import AutoregressiveNN\nfrom numpyro.contrib.nn.block_neural_arn import BlockNeuralAutoregressiveNN\nfrom numpyro.contrib.nn.masked_dense import MaskedDense\n\n__all__ = ['MaskedDense', 'AutoregressiveNN', 'BlockNeuralAutoregressiveNN']\n"""
numpyro/contrib/nn/auto_reg_nn.py,11,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\n# lightly adapted from https://github.com/pyro-ppl/pyro/blob/dev/pyro/nn/auto_reg_nn.py\n\nfrom jax import ops\nfrom jax.experimental import stax\nimport jax.numpy as np\n\nfrom numpyro.contrib.nn.masked_dense import MaskedDense\n\n\ndef sample_mask_indices(input_dim, hidden_dim):\n    """"""\n    Samples the indices assigned to hidden units during the construction of MADE masks\n\n    :param input_dim: the dimensionality of the input variable\n    :type input_dim: int\n    :param hidden_dim: the dimensionality of the hidden layer\n    :type hidden_dim: int\n    """"""\n    indices = np.linspace(1, input_dim, num=hidden_dim)\n    # Simple procedure tries to space fractional indices evenly by rounding to nearest int\n    return np.round(indices)\n\n\ndef create_mask(input_dim, hidden_dims, permutation, output_dim_multiplier):\n    """"""\n    Creates (non-conditional) MADE masks\n\n    :param input_dim: the dimensionality of the input variable\n    :type input_dim: int\n    :param hidden_dims: the dimensionality of the hidden layers(s)\n    :type hidden_dims: list[int]\n    :param permutation: the order of the input variables\n    :type permutation: numpy array of integers of length `input_dim`\n    :param output_dim_multiplier: tiles the output (e.g. for when a separate mean and scale parameter are desired)\n    :type output_dim_multiplier: int\n    """"""\n    # Create mask indices for input, hidden layers, and final layer\n    var_index = np.zeros(permutation.shape[0])\n    var_index = ops.index_update(var_index, permutation, np.arange(input_dim))\n\n    # Create the indices that are assigned to the neurons\n    input_indices = 1 + var_index\n    hidden_indices = [sample_mask_indices(input_dim - 1, h) for h in hidden_dims]\n    output_indices = np.tile(var_index + 1, output_dim_multiplier)\n\n    # Create mask from input to output for the skips connections\n    mask_skip = output_indices[None, :] > input_indices[:, None]\n\n    # Create mask from input to first hidden layer, and between subsequent hidden layers\n    # NB: these masks are transposed version of Pyro ones\n    masks = [hidden_indices[0][None, :] >= input_indices[:, None]]\n    for i in range(1, len(hidden_dims)):\n        masks.append(hidden_indices[i][None, :] >= hidden_indices[i - 1][:, None])\n\n    # Create mask from last hidden layer to output layer\n    masks.append(output_indices[None, :] > hidden_indices[-1][:, None])\n\n    return masks, mask_skip\n\n\ndef AutoregressiveNN(input_dim, hidden_dims, param_dims=[1, 1], permutation=None,\n                     skip_connections=False, nonlinearity=stax.Relu):\n    """"""\n    An implementation of a MADE-like auto-regressive neural network.\n\n    Similar to the purely functional layer implemented in jax.experimental.stax,\n    the `AutoregressiveNN` class has `init_fun` and `apply_fun` methods,\n    where `init_fun` takes an rng_key key and an input shape and returns an\n    (output_shape, params) pair, and `apply_fun` takes params and inputs\n    and applies the layer.\n\n    :param input_dim: the dimensionality of the input\n    :type input_dim: int\n    :param hidden_dims: the dimensionality of the hidden units per layer\n    :type hidden_dims: list[int]\n    :param param_dims: shape the output into parameters of dimension (p_n, input_dim) for p_n in param_dims\n        when p_n > 1 and dimension (input_dim) when p_n == 1. The default is [1, 1], i.e. output two parameters\n        of dimension (input_dim), which is useful for inverse autoregressive flow.\n    :type param_dims: list[int]\n    :param permutation: an optional permutation that is applied to the inputs and controls the order of the\n        autoregressive factorization. in particular for the identity permutation the autoregressive structure\n        is such that the Jacobian is triangular. Defaults to identity permutation.\n    :type permutation: array of ints\n    :param bool skip_connection: whether to add skip connections from the input to the output.\n    :type skip_connections: bool\n    :param nonlinearity: The nonlinearity to use in the feedforward network such as ReLU. Note that no\n        nonlinearity is applied to the final network output, so the output is an unbounded real number.\n        defaults to ReLU.\n    :type nonlinearity: callable.\n    :return: a tuple (init_fun, apply_fun)\n\n    Reference:\n\n    MADE: Masked Autoencoder for Distribution Estimation [arXiv:1502.03509]\n    Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle\n    """"""\n    output_multiplier = sum(param_dims)\n    all_ones = (np.array(param_dims) == 1).all()\n\n    # Calculate the indices on the output corresponding to each parameter\n    ends = np.cumsum(np.array(param_dims), axis=0)\n    starts = np.concatenate((np.zeros(1), ends[:-1]))\n    param_slices = [slice(int(s), int(e)) for s, e in zip(starts, ends)]\n\n    # Hidden dimension must be not less than the input otherwise it isn\'t\n    # possible to connect to the outputs correctly\n    for h in hidden_dims:\n        if h < input_dim:\n            raise ValueError(\'Hidden dimension must not be less than input dimension.\')\n\n    if permutation is None:\n        permutation = np.arange(input_dim)\n\n    # Create masks\n    masks, mask_skip = create_mask(input_dim=input_dim, hidden_dims=hidden_dims,\n                                   permutation=permutation,\n                                   output_dim_multiplier=output_multiplier)\n\n    main_layers = []\n    # Create masked layers\n    for i, mask in enumerate(masks):\n        main_layers.append(MaskedDense(mask))\n        if i < len(masks) - 1:\n            main_layers.append(nonlinearity)\n\n    if skip_connections:\n        net_init, net = stax.serial(stax.FanOut(2),\n                                    stax.parallel(stax.serial(*main_layers),\n                                                  MaskedDense(mask_skip, bias=False)),\n                                    stax.FanInSum)\n    else:\n        net_init, net = stax.serial(*main_layers)\n\n    def init_fun(rng_key, input_shape):\n        """"""\n        :param rng_key: rng_key used to initialize parameters\n        :param input_shape: input shape\n        """"""\n        assert input_dim == input_shape[-1]\n        return net_init(rng_key, input_shape)\n\n    def apply_fun(params, inputs, **kwargs):\n        """"""\n        :param params: layer parameters\n        :param inputs: layer inputs\n        """"""\n        out = net(params, inputs, **kwargs)\n\n        # reshape output as necessary\n        out = np.reshape(out, inputs.shape[:-1] + (output_multiplier, input_dim))\n        # move param dims to the first dimension\n        out = np.moveaxis(out, -2, 0)\n\n        if all_ones:\n            # Squeeze dimension if all parameters are one dimensional\n            out = tuple([out[i] for i in range(output_multiplier)])\n        else:\n            # If not all ones, then probably don\'t want to squeeze a single dimension parameter\n            out = tuple([out[s] for s in param_slices])\n\n        # if len(param_dims) == 1, we return the array instead of a tuple of arrays\n        return out[0] if len(param_dims) == 1 else out\n\n    return init_fun, apply_fun\n'"
numpyro/contrib/nn/block_neural_arn.py,15,"b'from jax import ops, random\nfrom jax.experimental import stax\nfrom jax.nn import sigmoid, softplus\nfrom jax.nn.initializers import glorot_uniform, normal, uniform\nimport jax.numpy as np\n\nfrom numpyro.distributions.util import logmatmulexp, vec_to_tril_matrix\n\n\ndef BlockMaskedDense(num_blocks, in_factor, out_factor, bias=True, W_init=glorot_uniform()):\n    """"""\n    Module that implements a linear layer with block matrices with positive diagonal blocks.\n    Moreover, it uses Weight Normalization (https://arxiv.org/abs/1602.07868) for stability.\n\n    :param int num_blocks: Number of block matrices.\n    :param int in_factor: number of rows in each block.\n    :param int out_factor: number of columns in each block.\n    :param W_init: initialization method for the weights.\n    :return: an (`init_fn`, `update_fn`) pair.\n    """"""\n    input_dim, out_dim = num_blocks * in_factor, num_blocks * out_factor\n    # construct mask_d, mask_o for formula (8) of Ref [1]\n    # Diagonal block mask\n    mask_d = np.identity(num_blocks)[..., None]\n    mask_d = np.tile(mask_d, (1, in_factor, out_factor)).reshape(input_dim, out_dim)\n    # Off-diagonal block mask for upper triangular weight matrix\n    mask_o = vec_to_tril_matrix(np.ones(num_blocks * (num_blocks - 1) // 2), diagonal=-1).T[..., None]\n    mask_o = np.tile(mask_o, (1, in_factor, out_factor)).reshape(input_dim, out_dim)\n\n    def init_fun(rng, input_shape):\n        assert input_dim == input_shape[-1]\n        *k1, k2, k3 = random.split(rng, num_blocks + 2)\n\n        # Initialize each column block using W_init\n        W = np.zeros((input_dim, out_dim))\n        for i in range(num_blocks):\n            W = ops.index_add(\n                W,\n                ops.index[:(i + 1) * in_factor, i * out_factor:(i + 1) * out_factor],\n                W_init(k1[i], ((i + 1) * in_factor, out_factor))\n            )\n\n        # initialize weight scale\n        ws = np.log(uniform(1.)(k2, (out_dim,)))\n\n        if bias:\n            b = (uniform(1.)(k3, (out_dim,)) - 0.5) * (2 / np.sqrt(out_dim))\n            params = (W, ws, b)\n        else:\n            params = (W, ws)\n        return input_shape[:-1] + (out_dim,), params\n\n    def apply_fun(params, inputs, **kwargs):\n        x, logdet = inputs\n        if bias:\n            W, ws, b = params\n        else:\n            W, ws = params\n\n        # Form block weight matrix, making sure it\'s positive on diagonal!\n        w = np.exp(W) * mask_d + W * mask_o\n\n        # Compute norm of each column (i.e. each output features)\n        w_norm = np.linalg.norm(w, axis=-2, keepdims=True)\n\n        # Normalize weight and rescale\n        w = np.exp(ws) * w / w_norm\n\n        out = np.dot(x, w)\n        if bias:\n            out = out + b\n\n        dense_logdet = ws + W - np.log(w_norm)\n        # logdet of block diagonal\n        dense_logdet = dense_logdet[mask_d.astype(bool)].reshape(num_blocks, in_factor, out_factor)\n        if logdet is None:\n            logdet = np.broadcast_to(dense_logdet, x.shape[:-1] + dense_logdet.shape)\n        else:\n            logdet = logmatmulexp(logdet, dense_logdet)\n        return out, logdet\n\n    return init_fun, apply_fun\n\n\ndef Tanh():\n    """"""\n    Tanh nonlinearity with its log jacobian.\n\n    :return: an (`init_fn`, `update_fn`) pair.\n    """"""\n    def init_fun(rng, input_shape):\n        return input_shape, ()\n\n    def apply_fun(params, inputs, **kwargs):\n        x, logdet = inputs\n        out = np.tanh(x)\n        tanh_logdet = -2 * (x + softplus(-2 * x) - np.log(2.))\n        # logdet.shape = batch_shape + (num_blocks, in_factor, out_factor)\n        # tanh_logdet.shape = batch_shape + (num_blocks x out_factor,)\n        # so we need to reshape tanh_logdet to: batch_shape + (num_blocks, 1, out_factor)\n        tanh_logdet = tanh_logdet.reshape(logdet.shape[:-2] + (1, logdet.shape[-1]))\n        return out, logdet + tanh_logdet\n\n    return init_fun, apply_fun\n\n\ndef FanInResidualNormal():\n    """"""\n    Similar to stax.FanInSum but also keeps track of log determinant of Jacobian.\n    It is required that the second fan-in branch is identity.\n\n    :return: an (`init_fn`, `update_fn`) pair.\n    """"""\n    def init_fun(rng, input_shape):\n        return input_shape[0], ()\n\n    def apply_fun(params, inputs, **kwargs):\n        # f(x) + x\n        (fx, logdet), (x, _) = inputs\n        return fx + x, softplus(logdet)\n\n    return init_fun, apply_fun\n\n\ndef FanInResidualGated(gate_init=normal(1.)):\n    """"""\n    Similar to FanInNormal uses a learnable parameter `gate` to interpolate two fan-in branches.\n    It is required that the second fan-in branch is identity.\n\n    :param gate_init: initialization method for the gate.\n    :return: an (`init_fn`, `update_fn`) pair.\n    """"""\n    def init_fun(rng, input_shape):\n        return input_shape[0], gate_init(rng, ())\n\n    def apply_fun(params, inputs, **kwargs):\n        # a * f(x) + (1 - a) * x\n        (fx, logdet), (x, _) = inputs\n        gate = sigmoid(params)\n        out = gate * fx + (1 - gate) * x\n        logdet = softplus(logdet + params) - softplus(params)\n        return out, logdet\n\n    return init_fun, apply_fun\n\n\ndef BlockNeuralAutoregressiveNN(input_dim, hidden_factors=[8, 8], residual=None):\n    """"""\n    An implementation of Block Neural Autoregressive neural network.\n\n    **References**\n\n    1. *Block Neural Autoregressive Flow*,\n       Nicola De Cao, Ivan Titov, Wilker Aziz\n\n    :param int input_dim: The dimensionality of the input.\n    :param list hidden_factors: Hidden layer i has ``hidden_factors[i]`` hidden units per\n        input dimension. This corresponds to both :math:`a` and :math:`b` in reference [1].\n        The elements of hidden_factors must be integers.\n    :param str residual: Type of residual connections to use. One of `None`, `""normal""`, `""gated""`.\n    :return: an (`init_fn`, `update_fn`) pair.\n    """"""\n    layers = []\n    in_factor = 1\n    for hidden_factor in hidden_factors:\n        layers.append(BlockMaskedDense(input_dim, in_factor, hidden_factor))\n        layers.append(Tanh())\n        in_factor = hidden_factor\n    layers.append(BlockMaskedDense(input_dim, in_factor, 1))\n    arn = stax.serial(*layers)\n    if residual is not None:\n        FanInResidual = FanInResidualGated if residual == ""gated"" else FanInResidualNormal\n        arn = stax.serial(stax.FanOut(2), stax.parallel(arn, stax.Identity), FanInResidual())\n\n    def init_fun(rng, input_shape):\n        return arn[0](rng, input_shape)\n\n    def apply_fun(params, inputs, **kwargs):\n        out, logdet = arn[1](params, (inputs, None), **kwargs)\n        return out, logdet.reshape(inputs.shape)\n\n    return init_fun, apply_fun\n'"
numpyro/contrib/nn/masked_dense.py,2,"b'# Copyright Contributors to the Pyro project.\n# SPDX-License-Identifier: Apache-2.0\n\nfrom jax import random\nfrom jax.nn.initializers import glorot_normal, normal\nimport jax.numpy as np\n\n\ndef MaskedDense(mask, bias=True, W_init=glorot_normal(), b_init=normal()):\n    """"""\n    As in jax.experimental.stax, each layer constructor function returns\n    an (init_fun, apply_fun) pair, where `init_fun` takes an rng_key key and\n    an input shape and returns an (output_shape, params) pair, and\n    `apply_fun` takes params, inputs, and an rng_key key and applies the layer.\n\n    :param array mask: Mask of shape (input_dim, out_dim) applied to the weights of the layer.\n    :param bool bias: whether to include bias term.\n    :param array W_init: initialization method for the weights.\n    :param array b_init: initialization method for the bias terms.\n    :return: a (`init_fn`, `update_fn`) pair.\n    """"""\n    def init_fun(rng_key, input_shape):\n        k1, k2 = random.split(rng_key)\n        W = W_init(k1, mask.shape)\n        if bias:\n            b = b_init(k2, mask.shape[-1:])\n            params = (W, b)\n        else:\n            params = W\n        return input_shape[:-1] + mask.shape[-1:], params\n\n    def apply_fun(params, inputs, **kwargs):\n        if bias:\n            W, b = params\n            return np.dot(inputs, W * mask) + b\n        else:\n            W = params\n            return np.dot(inputs, W * mask)\n\n    return init_fun, apply_fun\n'"
