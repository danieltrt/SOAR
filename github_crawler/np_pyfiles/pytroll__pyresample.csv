file_path,api_count,code
setup.py,0,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2012, 2014, 2015  Esben S. Nielsen\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# workaround python bug: http://bugs.python.org/issue15881#msg170215\n# remove when python 2 support is dropped\n""""""The setup module.""""""\nimport multiprocessing  # noqa: F401\nimport versioneer\nimport os\nimport sys\n\nfrom setuptools import Extension, find_packages, setup\nfrom setuptools.command.build_ext import build_ext as _build_ext\n\nrequirements = [\'setuptools>=3.2\', \'pyproj>=1.9.5.1\', \'configobj\',\n                \'pykdtree>=1.3.1\', \'pyyaml\', \'numpy>=1.10.0\']\nextras_require = {\'numexpr\': [\'numexpr\'],\n                  \'quicklook\': [\'matplotlib\', \'cartopy\', \'pillow\'],\n                  \'rasterio\': [\'rasterio\'],\n                  \'dask\': [\'dask>=0.16.1\'],\n                  \'cf\': [\'xarray\'],\n                  \'gradient_search\': [\'shapely\']}\n\nsetup_requires = [\'numpy>=1.10.0\']\ntest_requires = [\'rasterio\', \'dask\', \'xarray\', \'cartopy\', \'pillow\', \'matplotlib\', \'scipy\', ]\n\nif sys.platform.startswith(""win""):\n    extra_compile_args = []\nelse:\n    extra_compile_args = [""-O3"", ""-Wno-unused-function""]\n\nextensions = [\n    Extension(""pyresample.ewa._ll2cr"", sources=[""pyresample/ewa/_ll2cr.pyx""],\n              extra_compile_args=extra_compile_args),\n    Extension(""pyresample.ewa._fornav"", sources=[""pyresample/ewa/_fornav.pyx"",\n                                                 ""pyresample/ewa/_fornav_templates.cpp""],\n              language=""c++"", extra_compile_args=extra_compile_args,\n              depends=[""pyresample/ewa/_fornav_templates.h""]),\n    Extension(""pyresample.gradient._gradient_search"", sources=[""pyresample/gradient/_gradient_search.pyx""],\n              extra_compile_args=extra_compile_args),\n]\n\ntry:\n    from Cython.Build import cythonize\nexcept ImportError:\n    cythonize = None\n\n\ndef set_builtin(name, value):\n    """"""Set builtin.""""""\n    if isinstance(__builtins__, dict):\n        __builtins__[name] = value\n    else:\n        setattr(__builtins__, name, value)\n\n\ncmdclass = versioneer.get_cmdclass()\nversioneer_build_ext = cmdclass.get(\'build_ext\', _build_ext)\n\n\nclass build_ext(_build_ext):\n    """"""Work around to bootstrap numpy includes in to extensions.\n\n    Copied from:\n\n        http://stackoverflow.com/questions/19919905/how-to-bootstrap-numpy-installation-in-setup-py\n\n    """"""\n\n    def finalize_options(self):\n        """"""Finalize options.""""""\n        versioneer_build_ext.finalize_options(self)\n        # Prevent numpy from thinking it is still in its setup process:\n        set_builtin(\'__NUMPY_SETUP__\', False)\n        import numpy\n        self.include_dirs.append(numpy.get_include())\n\n\ncmdclass[\'build_ext\'] = build_ext\n\nif __name__ == ""__main__"":\n    if not os.getenv(""USE_CYTHON"", False) or cythonize is None:\n        print(\n            ""Cython will not be used. Use environment variable \'USE_CYTHON=True\' to use it"")\n\n        def cythonize(extensions, **_ignore):\n            """"""Fake function to compile from C/C++ files instead of compiling .pyx files with cython.""""""\n            for extension in extensions:\n                sources = []\n                for sfile in extension.sources:\n                    path, ext = os.path.splitext(sfile)\n                    if ext in (\'.pyx\', \'.py\'):\n                        if extension.language == \'c++\':\n                            ext = \'.cpp\'\n                        else:\n                            ext = \'.c\'\n                        sfile = path + ext\n                    sources.append(sfile)\n                extension.sources[:] = sources\n            return extensions\n\n    README = open(\'README.md\', \'r\').read()\n    setup(name=\'pyresample\',\n          version=versioneer.get_version(),\n          cmdclass=cmdclass,\n          description=\'Geospatial image resampling in Python\',\n          long_description=README,\n          long_description_content_type=\'text/markdown\',\n          author=\'Thomas Lavergne\',\n          author_email=\'t.lavergne@met.no\',\n          package_dir={\'pyresample\': \'pyresample\'},\n          packages=find_packages(),\n          python_requires=\'>=3.4\',\n          setup_requires=setup_requires,\n          install_requires=requirements,\n          extras_require=extras_require,\n          tests_require=test_requires,\n          ext_modules=cythonize(extensions),\n          test_suite=\'pyresample.test.suite\',\n          zip_safe=False,\n          classifiers=[\n              \'Development Status :: 5 - Production/Stable\',\n              \'License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)\',\n              \'Programming Language :: Python\',\n              \'Operating System :: OS Independent\',\n              \'Intended Audience :: Science/Research\',\n              \'Topic :: Scientific/Engineering\'\n          ]\n          )\n'"
versioneer.py,0,"b'\n# Version: 0.18\n\n""""""The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'date\']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  ""master"" and ""slave"" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other langauges) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n""Entry-point scripts"" (`setup(entry_points={""console_scripts"": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It\'s not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons ""Public Domain\nDedication"" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n""""""\n\nfrom __future__ import print_function\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_root():\n    """"""Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    """"""\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (""Versioneer was unable to run the project root directory. ""\n               ""Versioneer requires setup.py to be executed from ""\n               ""its immediate directory (like \'python setup.py COMMAND\'), ""\n               ""or in a way that lets it use sys.argv[0] to find the root ""\n               ""(like \'python path/to/setup.py COMMAND\')."")\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(""Warning: build in %s is using versioneer.py from %s""\n                  % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    """"""Read the project setup.cfg file to determine Versioneer config.""""""\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    if cfg.tag_prefix in (""\'\'"", \'""""\'):\n        cfg.tag_prefix = """"\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\'git\'] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    git_date = ""%(DOLLAR)sFormat:%%ci%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n            print(""stdout was %%s"" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %%s but none started with prefix %%s"" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %%s not under git control"" %% root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%%s*"" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%s*"" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    """"""Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    """"""Try to determine the version from _version.py if present.""""""\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"",\n                   contents, re.M | re.S)\n    if not mo:\n        mo = re.search(r""version_json = \'\'\'\\r\\n(.*)\'\'\'  # END VERSION_JSON"",\n                       contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    """"""Write the given version number to the given _version.py file.""""""\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True,\n                          indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\nclass VersioneerBadRootError(Exception):\n    """"""The project root directory is unknown or missing key files.""""""\n\n\ndef get_versions(verbose=False):\n    """"""Get the project version from whatever source is available.\n\n    Returns dict with two keys: \'version\' and \'full\'.\n    """"""\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \\\n        ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None, ""error"": ""unable to compute version"",\n            ""date"": None}\n\n\ndef get_version():\n    """"""Get the short version string for this project.""""""\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    """"""Get the custom setuptools/distutils subclasses used by Versioneer.""""""\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            print("" date: %s"" % vers.get(""date""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn\'t copied too, \'git describe\' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different ""build_py"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib,\n                                                  cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n        # nczeczulin reports that py2exe won\'t like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   ""version"": versioneer.get_version().split(""+"", 1)[0], # FILEVERSION\n        #   ""product_version"": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    if \'py2exe\' in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""py2exe""] = cmd_py2exe\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(target_versionfile,\n                                  self._versioneer_generated_versions)\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    """"""Main VCS-independent setup function for installing Versioneer.""""""\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError,\n            configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"",\n                  file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(LONG % {""DOLLAR"": ""$"",\n                        ""STYLE"": cfg.style,\n                        ""TAG_PREFIX"": cfg.tag_prefix,\n                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        })\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                       ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print("" appending versionfile_source (\'%s\') to MANIFEST.in"" %\n              cfg.versionfile_source)\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    """"""Validate the contents of setup.py against Versioneer\'s expectations.""""""\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
pyresample/__init__.py,0,"b""# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010, 2014, 2015  Esben S. Nielsen\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport os\n\nCHUNK_SIZE = int(os.getenv('PYTROLL_CHUNK_SIZE', 4096))\n\n# Backwards compatibility\nfrom pyresample import geometry  # noqa\nfrom pyresample import grid  # noqa\nfrom pyresample import image  # noqa\nfrom pyresample import kd_tree  # noqa\nfrom pyresample import utils  # noqa\nfrom pyresample import plot  # noqa\n# Easy access\nfrom pyresample.geometry import (SwathDefinition,  # noqa\n                                 AreaDefinition,  # noqa\n                                 DynamicAreaDefinition)  # noqa\nfrom pyresample.area_config import load_area, create_area_def, get_area_def, \\\n                                   parse_area_file, convert_def_to_yaml  # noqa\nfrom pyresample.kd_tree import XArrayResamplerNN  # noqa\nfrom pyresample.plot import save_quicklook, area_def2basemap  # noqa\nfrom .version import get_versions  # noqa\n\n__all__ = ['grid', 'image', 'kd_tree', 'utils', 'plot', 'geo_filter', 'geometry', 'CHUNK_SIZE',\n           'load_area', 'create_area_def', 'get_area_def', 'parse_area_file', 'convert_def_to_yaml']\n\n__version__ = get_versions()['version']\ndel get_versions\n"""
pyresample/_multi_proc.py,14,"b""# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010, 2015  Esben S. Nielsen\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import absolute_import\n\nimport ctypes\nimport multiprocessing as mp\n\nimport numpy as np\n\n\nclass Scheduler(object):\n\n    def __init__(self, ndata, nprocs, chunk=None, schedule='guided'):\n        if not schedule in ['guided', 'dynamic', 'static']:\n            raise ValueError('unknown scheduling strategy')\n        self._ndata = mp.RawValue(ctypes.c_int, ndata)\n        self._start = mp.RawValue(ctypes.c_int, 0)\n        self._lock = mp.Lock()\n        self._schedule = schedule\n        self._nprocs = nprocs\n        if schedule == 'guided' or schedule == 'dynamic':\n            min_chunk = ndata // (10 * nprocs)\n            if chunk:\n                min_chunk = chunk\n            min_chunk = max(min_chunk, 1)\n            self._chunk = min_chunk\n        elif schedule == 'static':\n            min_chunk = ndata // nprocs\n            if chunk:\n                min_chunk = max(chunk, min_chunk)\n            min_chunk = max(min_chunk, 1)\n            self._chunk = min_chunk\n\n    def __iter__(self):\n        while True:\n            self._lock.acquire()\n            ndata = self._ndata.value\n            nprocs = self._nprocs\n            start = self._start.value\n            if self._schedule == 'guided':\n                _chunk = ndata // nprocs\n                chunk = max(self._chunk, _chunk)\n            else:\n                chunk = self._chunk\n            if ndata:\n                if chunk > ndata:\n                    s0 = start\n                    s1 = start + ndata\n                    self._ndata.value = 0\n                else:\n                    s0 = start\n                    s1 = start + chunk\n                    self._ndata.value = ndata - chunk\n                    self._start.value = start + chunk\n                self._lock.release()\n                yield slice(s0, s1)\n            else:\n                self._lock.release()\n                return\n\n\ndef shmem_as_ndarray(raw_array):\n    _ctypes_to_numpy = {\n        ctypes.c_char: np.int8,\n        ctypes.c_wchar: np.int16,\n        ctypes.c_byte: np.int8,\n        ctypes.c_ubyte: np.uint8,\n        ctypes.c_short: np.int16,\n        ctypes.c_ushort: np.uint16,\n        ctypes.c_int: np.int32,\n        ctypes.c_uint: np.int32,\n        ctypes.c_long: np.int32,\n        ctypes.c_ulong: np.int32,\n        ctypes.c_float: np.float32,\n        ctypes.c_double: np.float64\n    }\n    dtype = _ctypes_to_numpy[raw_array._type_]\n\n    # The following works too, but occasionally raises\n    # RuntimeWarning: Item size computed from the PEP 3118 buffer format string does not match the actual item size.\n    # and appears to be slower.\n    # return np.ctypeslib.as_array(raw_array)\n\n    return np.frombuffer(raw_array, dtype=dtype)\n"""
pyresample/_spatial_mp.py,11,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010, 2013, 2015  Esben S. Nielsen, Martin Raspaud\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import absolute_import\n\nimport ctypes\n\nimport numpy as np\nimport pyproj\nimport multiprocessing as mp\nimport warnings\n\nfrom pyresample.utils import proj4_str_to_dict, is_pyproj2\n\n\ntry:\n    import numexpr as ne\nexcept ImportError:\n    ne = None\n\nfrom ._multi_proc import shmem_as_ndarray, Scheduler\n\n# Earth radius\nR = 6370997.0\n\n\nclass cKDTree_MP(object):\n\n    \'\'\' Multiprocessing cKDTree subclass, shared memory \'\'\'\n\n    def __init__(self, data, leafsize=10, nprocs=2, chunk=None,\n                 schedule=\'guided\'):\n        \'\'\'\n        Same as cKDTree.__init__ except that an internal copy\n        of data to shared memory is made.\n        Extra keyword arguments:\n        chunk : Minimum chunk size for the load balancer.\n        schedule: Strategy for balancing work load\n        (\'static\', \'dynamic\' or \'guided\').\n        \'\'\'\n\n        self.n, self.m = data.shape\n        # Allocate shared memory for data\n        self.shmem_data = mp.RawArray(ctypes.c_double, self.n * self.m)\n\n        # View shared memory as ndarray, and copy over the data.\n        # The RawArray objects have information about the dtype and\n        # buffer size.\n        _data = shmem_as_ndarray(self.shmem_data).reshape((self.n, self.m))\n        _data[:,:] = data\n\n        # Initialize parent, we must do this last because\n        # cKDTree stores a reference to the data array. We pass in\n        # the copy in shared memory rather than the origial data.\n        self.leafsize = leafsize\n        self._nprocs = nprocs\n        self._chunk = chunk\n        self._schedule = schedule\n\n    def query(self, x, k=1, eps=0, p=2, distance_upper_bound=np.inf):\n        \'\'\'\n        Same as cKDTree.query except parallelized with multiple\n        processes and shared memory.        \n        \'\'\'\n\n        # allocate shared memory for x and result\n        nx = x.shape[0]\n        shmem_x = mp.RawArray(ctypes.c_double, nx * self.m)\n        shmem_d = mp.RawArray(ctypes.c_double, nx * k)\n        shmem_i = mp.RawArray(ctypes.c_int, nx * k)\n\n        # view shared memory as ndarrays\n        _x = shmem_as_ndarray(shmem_x).reshape((nx, self.m))\n        if k == 1:\n            _d = shmem_as_ndarray(shmem_d)\n            _i = shmem_as_ndarray(shmem_i)\n        else:\n            _d = shmem_as_ndarray(shmem_d).reshape((nx, k))\n            _i = shmem_as_ndarray(shmem_i).reshape((nx, k))\n\n        # copy x to shared memory\n        _x[:] = x\n\n        # set up a scheduler to load balance the query\n        scheduler = Scheduler(nx, self._nprocs, chunk=self._chunk,\n                              schedule=self._schedule)\n\n        # query with multiple processes\n        query_args = [scheduler, self.shmem_data, self.n, self.m,\n                      self.leafsize, shmem_x, nx, shmem_d, shmem_i,\n                      k, eps, p, distance_upper_bound]\n\n        _run_jobs(_parallel_query, query_args, self._nprocs)\n        # return results (private memory)\n        return _d.copy(), _i.copy()\n\n\nclass BaseProj(pyproj.Proj):\n    """"""Helper class for easier backwards compatibility.""""""\n\n    def __init__(self, projparams=None, preserve_units=True, **kwargs):\n        if is_pyproj2():\n            # have to have this because pyproj uses __new__\n            # subclasses would fail when calling __init__ otherwise\n            super(BaseProj, self).__init__(projparams=projparams,\n                                           preserve_units=preserve_units,\n                                           **kwargs)\n\n    def is_latlong(self):\n        if is_pyproj2():\n            return self.crs.is_geographic\n        return super(BaseProj, self).is_latlong()\n\n\nclass Proj(BaseProj):\n    """"""Helper class to skip transforming lon/lat projection coordinates.""""""\n\n    def __call__(self, data1, data2, inverse=False, radians=False,\n                 errcheck=False, nprocs=1):\n        if self.is_latlong():\n            return data1, data2\n        return super(Proj, self).__call__(data1, data2, inverse=inverse,\n                                          radians=radians, errcheck=errcheck)\n\n\nclass Proj_MP(BaseProj):\n\n    def __init__(self, *args, **kwargs):\n        self._args = args\n        self._kwargs = kwargs\n        super(Proj_MP, self).__init__(*args, **kwargs)\n\n    def __call__(self, data1, data2, inverse=False, radians=False,\n                 errcheck=False, nprocs=2, chunk=None, schedule=\'guided\'):\n        if self.is_latlong():\n            return data1, data2\n\n        grid_shape = data1.shape\n        n = data1.size\n\n        # Create shared memory\n        shmem_data1 = mp.RawArray(ctypes.c_double, n)\n        shmem_data2 = mp.RawArray(ctypes.c_double, n)\n        shmem_res1 = mp.RawArray(ctypes.c_double, n)\n        shmem_res2 = mp.RawArray(ctypes.c_double, n)\n\n        # view shared memory as ndarrays\n        _data1 = shmem_as_ndarray(shmem_data1)\n        _data2 = shmem_as_ndarray(shmem_data2)\n        _res1 = shmem_as_ndarray(shmem_res1)\n        _res2 = shmem_as_ndarray(shmem_res2)\n\n        # copy input data to shared memory\n        _data1[:] = data1.ravel()\n        _data2[:] = data2.ravel()\n\n        # set up a scheduler to load balance the query\n        scheduler = Scheduler(n, nprocs, chunk=chunk, schedule=schedule)\n\n        # Projection with multiple processes\n        proj_call_args = [scheduler, shmem_data1, shmem_data2, shmem_res1,\n                          shmem_res2, self._args, self._kwargs, inverse,\n                          radians, errcheck]\n\n        _run_jobs(_parallel_proj, proj_call_args, nprocs)\n        return _res1.copy().reshape(grid_shape), _res2.copy().reshape(grid_shape)\n\n\nclass Cartesian(object):\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def transform_lonlats(self, lons, lats):\n        """"""Transform longitudes and latitues to cartesian coordinates.""""""\n        if np.issubdtype(lons.dtype, np.integer):\n            lons = lons.astype(np.float)\n        coords = np.zeros((lons.size, 3), dtype=lons.dtype)\n        if ne:\n            deg2rad = np.pi / 180  # noqa: F841\n            coords[:, 0] = ne.evaluate(""R*cos(lats*deg2rad)*cos(lons*deg2rad)"")\n            coords[:, 1] = ne.evaluate(""R*cos(lats*deg2rad)*sin(lons*deg2rad)"")\n            coords[:, 2] = ne.evaluate(""R*sin(lats*deg2rad)"")\n        else:\n            coords[:, 0] = R * np.cos(np.deg2rad(lats)) * np.cos(np.deg2rad(lons))\n            coords[:, 1] = R * np.cos(np.deg2rad(lats)) * np.sin(np.deg2rad(lons))\n            coords[:, 2] = R * np.sin(np.deg2rad(lats))\n        return coords\n\n\nCartesian_MP = Cartesian\n\n\ndef _run_jobs(target, args, nprocs):\n    """"""Run process pool\n    """"""\n\n    # return status in shared memory\n    # access to these values are serialized automatically\n    ierr = mp.Value(ctypes.c_int, 0)\n    warn_msg = mp.Array(ctypes.c_char, 1024)\n\n    args.extend((ierr, warn_msg))\n\n    pool = [mp.Process(target=target, args=args) for n in range(nprocs)]\n    for p in pool:\n        p.start()\n    for p in pool:\n        p.join()\n    if ierr.value != 0:\n        raise RuntimeError(\'%d errors in worker processes. Last one reported:\\n%s\' %\n                           (ierr.value, warn_msg.value.decode()))\n\n# This is executed in an external process:\n\n\ndef _parallel_query(scheduler,  # scheduler for load balancing\n                    # data needed to reconstruct the kd-tree\n                    data, ndata, ndim, leafsize,\n                    x, nx, d, i,  # query data and results\n                    k, eps, p, dub,  # auxillary query parameters\n                    ierr, warn_msg):  # return values (0 on success)\n\n    try:\n        # View shared memory as ndarrays.\n        _data = shmem_as_ndarray(data).reshape((ndata, ndim))\n        _x = shmem_as_ndarray(x).reshape((nx, ndim))\n        if k == 1:\n            _d = shmem_as_ndarray(d)\n            _i = shmem_as_ndarray(i)\n        else:\n            _d = shmem_as_ndarray(d).reshape((nx, k))\n            _i = shmem_as_ndarray(i).reshape((nx, k))\n\n        # Reconstruct the kd-tree from the data.\n        import scipy.spatial as sp\n        kdtree = sp.cKDTree(_data, leafsize=leafsize)\n\n        # Query for nearest neighbours, using slice ranges,\n        # from the load balancer.\n        for s in scheduler:\n            if k == 1:\n                _d[s], _i[s] = kdtree.query(_x[s,:], k=1, eps=eps, p=p,\\\n                                            distance_upper_bound=dub)\n            else:\n                _d[s,:], _i[s,:] = kdtree.query(_x[s,:], k=k, eps=eps, p=p,\\\n                                                distance_upper_bound=dub)\n    # An error occured, increment the return value ierr.\n    # Access to ierr is serialized by multiprocessing.\n    except Exception as e:\n        ierr.value += 1\n        warn_msg.value = str(e).encode()\n\n\ndef _parallel_proj(scheduler, data1, data2, res1, res2, proj_args, proj_kwargs,\n                   inverse, radians, errcheck, ierr, warn_msg):\n    try:\n        # View shared memory as ndarrays.\n        _data1 = shmem_as_ndarray(data1)\n        _data2 = shmem_as_ndarray(data2)\n        _res1 = shmem_as_ndarray(res1)\n        _res2 = shmem_as_ndarray(res2)\n\n        # Initialise pyproj\n        proj = pyproj.Proj(*proj_args, **proj_kwargs)\n\n        # Reproject data segment\n        for s in scheduler:\n            _res1[s], _res2[s] = proj(_data1[s], _data2[s], inverse=inverse,\n                                      radians=radians, errcheck=errcheck)\n\n    # An error occured, increment the return value ierr.\n    # Access to ierr is serialized by multiprocessing.\n    except Exception as e:\n        ierr.value += 1\n        warn_msg.value = str(e).encode()\n\n\ndef _parallel_transform(scheduler, lons, lats, n, coords, ierr, warn_msg):\n    try:\n        # View shared memory as ndarrays.\n        _lons = shmem_as_ndarray(lons)\n        _lats = shmem_as_ndarray(lats)\n        _coords = shmem_as_ndarray(coords).reshape((n, 3))\n\n        # Transform to cartesian coordinates\n        for s in scheduler:\n            _coords[s, 0] = R * \\\n                np.cos(np.radians(_lats[s])) * np.cos(np.radians(_lons[s]))\n            _coords[s, 1] = R * \\\n                np.cos(np.radians(_lats[s])) * np.sin(np.radians(_lons[s]))\n            _coords[s, 2] = R * np.sin(np.radians(_lats[s]))\n    # An error occured, increment the return value ierr.\n    # Access to ierr is serialized by multiprocessing.\n    except Exception as e:\n        ierr.value += 1\n        warn_msg.value = str(e).encode()\n'"
pyresample/area_config.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2019 Pyresample developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\nimport logging\nimport math\nimport os\nimport io\nimport warnings\nimport pathlib\n\nimport numpy as np\nimport yaml\nfrom pyresample.utils import proj4_str_to_dict\n\n\ntry:\n    from xarray import DataArray\nexcept ImportError:\n    class DataArray(object):\n        """"""Stand-in for DataArray for holding units information.""""""\n\n        def __init__(self, data, attrs=None):\n            self.attrs = attrs or {}\n            self.data = np.array(data)\n\n        def __getitem__(self, item):\n            return DataArray(self.data[item], attrs=self.attrs)\n\n        def __getattr__(self, item):\n            return self.attrs[item]\n\n        def __len__(self):\n            return len(self.data)\n\n\nclass AreaNotFound(KeyError):\n    """"""Exception raised when specified are is no found in file.""""""\n    pass\n\n\ndef load_area(area_file_name, *regions):\n    """"""Load area(s) from area file.\n\n    Parameters\n    ----------\n    area_file_name : str, pathlib.Path, stream, or list thereof\n        List of paths or streams.  Any str or pathlib.Path will be\n        interpreted as a path to a file.  Any stream will be interpreted\n        as containing a yaml definition.  To read directly from a string,\n        use :func:`load_area_from_string`.\n    regions : str argument list\n        Regions to parse. If no regions are specified all\n        regions in the file are returned\n\n    Returns\n    -------\n    area_defs : AreaDefinition or list\n        If one area name is specified a single AreaDefinition object is returned.\n        If several area names are specified a list of AreaDefinition objects is returned\n\n    Raises\n    ------\n    AreaNotFound:\n        If a specified area name is not found\n    """"""\n\n    area_list = parse_area_file(area_file_name, *regions)\n    if len(area_list) == 1:\n        return area_list[0]\n    return area_list\n\n\ndef load_area_from_string(area_strs, *regions):\n    """"""Load area(s) from area strings.\n\n    Like :func:`~pyresample.area_config.load_area`, but load from string\n    directly.\n\n    Parameters\n    ----------\n    area_strs : str or List[str]\n        Strings containing yaml definitions.\n    regions : str\n        Regions to parse.\n\n    Returns\n    -------\n    area_defs : AreaDefinition or list\n        If one area name is specified a single AreaDefinition object is returned.\n        If several area names are specified a list of AreaDefinition objects is returned\n    """"""\n    if isinstance(area_strs, str):\n        area_strs = [area_strs]\n    return load_area([io.StringIO(area_str) for area_str in area_strs],\n                     *regions)\n\n\ndef parse_area_file(area_file_name, *regions):\n    """"""Parse area information from area file\n\n    Parameters\n    -----------\n    area_file_name : str or list\n        One or more paths to area definition files\n    regions : str argument list\n        Regions to parse. If no regions are specified all\n        regions in the file are returned\n\n    Returns\n    -------\n    area_defs : list\n        List of AreaDefinition objects\n\n    Raises\n    ------\n    AreaNotFound:\n        If a specified area is not found\n    """"""\n\n    try:\n        return _parse_yaml_area_file(area_file_name, *regions)\n    except (yaml.scanner.ScannerError, yaml.parser.ParserError):\n        return _parse_legacy_area_file(area_file_name, *regions)\n\n\ndef _read_yaml_area_file_content(area_file_name):\n    """"""Read one or more area files in to a single dict object.""""""\n    from pyresample.utils import recursive_dict_update\n\n    if isinstance(area_file_name, (str, pathlib.Path)):\n        area_file_name = [area_file_name]\n\n    area_dict = {}\n    for area_file_obj in area_file_name:\n        if isinstance(area_file_obj, io.IOBase):\n            # already a stream\n            tmp_dict = yaml.safe_load(area_file_obj)\n        else:\n            # hopefully a path to a file, but in the past a yaml string could\n            # be passed directly, assume any string with a newline must be\n            # a yaml file and not a path\n            if isinstance(area_file_obj, str) and ""\\n"" in area_file_obj:\n                warnings.warn(""It looks like you passed a YAML string ""\n                              ""directly.  This is deprecated since pyresample ""\n                              ""1.14.1, please use load_area_from_string or ""\n                              ""pass a stream or a path to a file instead"",\n                              DeprecationWarning)\n                tmp_dict = yaml.safe_load(area_file_obj)\n            else:\n                with open(area_file_obj) as area_file_obj:\n                    tmp_dict = yaml.safe_load(area_file_obj)\n        area_dict = recursive_dict_update(area_dict, tmp_dict)\n\n    return area_dict\n\n\ndef _parse_yaml_area_file(area_file_name, *regions):\n    """"""Parse area information from a yaml area file.\n\n    Args:\n        area_file_name: filename, file-like object, yaml string, or list of\n                        these.\n\n    The result of loading multiple area files is the combination of all\n    the files, using the first file as the ""base"", replacing things after\n    that.\n    """"""\n    area_dict = _read_yaml_area_file_content(area_file_name)\n    area_list = regions or area_dict.keys()\n    res = []\n    for area_name in area_list:\n        params = area_dict.get(area_name)\n        if params is None:\n            raise AreaNotFound(\'Area ""{0}"" not found in file ""{1}""\'.format(area_name, area_file_name))\n        params.setdefault(\'area_id\', area_name)\n        # Optional arguments.\n        params[\'shape\'] = _capture_subarguments(params, \'shape\', [\'height\', \'width\'])\n        params[\'upper_left_extent\'] = _capture_subarguments(params, \'upper_left_extent\', [\'upper_left_extent\', \'x\', \'y\',\n                                                                                          \'units\'])\n        params[\'center\'] = _capture_subarguments(params, \'center\', [\'center\', \'x\', \'y\', \'units\'])\n        params[\'area_extent\'] = _capture_subarguments(params, \'area_extent\', [\'area_extent\', \'lower_left_xy\',\n                                                                              \'upper_right_xy\', \'units\'])\n        params[\'resolution\'] = _capture_subarguments(params, \'resolution\', [\'resolution\', \'dx\', \'dy\', \'units\'])\n        params[\'radius\'] = _capture_subarguments(params, \'radius\', [\'radius\', \'dx\', \'dy\', \'units\'])\n        params[\'rotation\'] = _capture_subarguments(params, \'rotation\', [\'rotation\', \'units\'])\n        res.append(create_area_def(**params))\n    return res\n\n\ndef _capture_subarguments(params, arg_name, sub_arg_list):\n    """"""Captures :func:`~pyresample.utils.create_area_def` sub-arguments (i.e. units, height, dx, etc) from a yaml file.\n\n    Example:\n        resolution:\n          dx: 11\n          dy: 22\n          units: meters\n        # returns DataArray((11, 22), attrs={\'units\': \'meters})\n    """"""\n    # Check if argument is in yaml.\n    argument = params.get(arg_name)\n    if not isinstance(argument, dict):\n        return argument\n    argument_keys = argument.keys()\n    for sub_arg in argument_keys:\n        # Verify that provided sub-arguments are valid.\n        if sub_arg not in sub_arg_list:\n            raise ValueError(\'Invalid area definition: {0} is not a valid sub-argument for {1}\'.format(sub_arg,\n                                                                                                       arg_name))\n        elif arg_name in argument_keys:\n            # If the arg_name is provided as a sub_arg, then it contains all the data and does not need other sub_args.\n            if sub_arg != arg_name and sub_arg != \'units\':\n                raise ValueError(\'Invalid area definition: {0} has too many sub-arguments: Both {0} and {1} were \'\n                                 \'specified.\'.\n                                 format(arg_name, sub_arg))\n            # If the arg_name is provided, it\'s expected that units is also provided.\n            elif \'units\' not in argument_keys:\n                raise ValueError(\'Invalid area definition: {0} has the sub-argument {0} without units\'.format(arg_name))\n    units = argument.pop(\'units\', None)\n    list_of_values = argument.pop(arg_name, [])\n    for sub_arg in sub_arg_list:\n        sub_arg_value = argument.get(sub_arg)\n        # Don\'t append units to the argument.\n        if sub_arg_value is not None:\n            if sub_arg in (\'lower_left_xy\', \'upper_right_xy\') and isinstance(sub_arg_value, list):\n                list_of_values.extend(sub_arg_value)\n            else:\n                list_of_values.append(sub_arg_value)\n    # If units are provided, convert to xarray.\n    if units is not None:\n        return DataArray(list_of_values, attrs={\'units\': units})\n    return list_of_values\n\n\ndef _read_legacy_area_file_lines(area_file_name):\n    if isinstance(area_file_name, str):\n        area_file_name = [area_file_name]\n\n    for area_file_obj in area_file_name:\n        if (isinstance(area_file_obj, str) and\n           not os.path.isfile(area_file_obj)):\n            # file content string\n            for line in area_file_obj.splitlines():\n                yield line\n            continue\n        elif isinstance(area_file_obj, str):\n            # filename\n            with open(area_file_obj, \'r\') as area_file:\n                for line in area_file.readlines():\n                    yield line\n\n\ndef _parse_legacy_area_file(area_file_name, *regions):\n    """"""Parse area information from a legacy area file.""""""\n    area_file = _read_legacy_area_file_lines(area_file_name)\n    area_list = list(regions)\n    if not area_list:\n        select_all_areas = True\n        area_defs = []\n    else:\n        select_all_areas = False\n        area_defs = [None for i in area_list]\n\n    # Extract area from file\n    in_area = False\n    for line in area_file:\n        if not in_area:\n            if \'REGION\' in line and not line.strip().startswith(\'#\'):\n                area_id = line.replace(\'REGION:\', \'\'). \\\n                    replace(\'{\', \'\').strip()\n                if area_id in area_list or select_all_areas:\n                    in_area = True\n                    area_content = \'\'\n        elif \'};\' in line:\n            in_area = False\n            try:\n                if select_all_areas:\n                    area_defs.append(_create_area(area_id, area_content))\n                else:\n                    area_defs[area_list.index(area_id)] = _create_area(area_id,\n                                                                       area_content)\n            except KeyError:\n                raise ValueError(\'Invalid area definition: %s, %s\' % (area_id, area_content))\n        else:\n            area_content += line\n\n    # Check if all specified areas were found\n    if not select_all_areas:\n        for i, area in enumerate(area_defs):\n            if area is None:\n                raise AreaNotFound(\'Area ""%s"" not found in file ""%s""\' %\n                                   (area_list[i], area_file_name))\n    return area_defs\n\n\ndef _create_area(area_id, area_content):\n    """"""Parse area configuration""""""\n    from configobj import ConfigObj\n    config_obj = area_content.replace(\'{\', \'\').replace(\'};\', \'\')\n    config_obj = ConfigObj([line.replace(\':\', \'=\', 1)\n                            for line in config_obj.splitlines()])\n    config = config_obj.dict()\n    config[\'REGION\'] = area_id\n\n    try:\n        string_types = basestring\n    except NameError:\n        string_types = str\n    if not isinstance(config[\'NAME\'], string_types):\n        config[\'NAME\'] = \', \'.join(config[\'NAME\'])\n\n    config[\'XSIZE\'] = int(config[\'XSIZE\'])\n    config[\'YSIZE\'] = int(config[\'YSIZE\'])\n    if \'ROTATION\' in config.keys():\n        config[\'ROTATION\'] = float(config[\'ROTATION\'])\n    else:\n        config[\'ROTATION\'] = 0\n    config[\'AREA_EXTENT\'][0] = config[\'AREA_EXTENT\'][0].replace(\'(\', \'\')\n    config[\'AREA_EXTENT\'][3] = config[\'AREA_EXTENT\'][3].replace(\')\', \'\')\n\n    for i, val in enumerate(config[\'AREA_EXTENT\']):\n        config[\'AREA_EXTENT\'][i] = float(val)\n\n    config[\'PCS_DEF\'] = _get_proj4_args(config[\'PCS_DEF\'])\n    return create_area_def(config[\'REGION\'], config[\'PCS_DEF\'], description=config[\'NAME\'], proj_id=config[\'PCS_ID\'],\n                           shape=(config[\'YSIZE\'], config[\'XSIZE\']), area_extent=config[\'AREA_EXTENT\'],\n                           rotation=config[\'ROTATION\'])\n\n\ndef get_area_def(area_id, area_name, proj_id, proj4_args, width, height, area_extent, rotation=0):\n    """"""Construct AreaDefinition object from arguments\n\n    Parameters\n    -----------\n    area_id : str\n        ID of area\n    area_name :str\n        Description of area\n    proj_id : str\n        ID of projection\n    proj4_args : list, dict, or str\n        Proj4 arguments as list of arguments or string\n    width : int\n        Number of pixel in x dimension\n    height : int\n        Number of pixel in y dimension\n    rotation: float\n        Rotation in degrees (negative is cw)\n    area_extent : list\n        Area extent as a list of ints (LL_x, LL_y, UR_x, UR_y)\n\n    Returns\n    -------\n    area_def : object\n        AreaDefinition object\n    """"""\n\n    proj_dict = _get_proj4_args(proj4_args)\n    return create_area_def(area_id, proj_dict, description=area_name, proj_id=proj_id,\n                           shape=(height, width), area_extent=area_extent)\n\n\ndef _get_proj4_args(proj4_args):\n    """"""Create dict from proj4 args.""""""\n    from pyresample.utils.proj4 import convert_proj_floats\n    if isinstance(proj4_args, str):\n        # float conversion is done in `proj4_str_to_dict` already\n        return proj4_str_to_dict(str(proj4_args))\n\n    from configobj import ConfigObj\n    proj_config = ConfigObj(proj4_args)\n    return convert_proj_floats(proj_config.items())\n\n\ndef create_area_def(area_id, projection, width=None, height=None, area_extent=None, shape=None, upper_left_extent=None,\n                    center=None, resolution=None, radius=None, units=None, **kwargs):\n    """"""Takes data the user knows and tries to make an area definition from what can be found.\n\n    Parameters\n    ----------\n    area_id : str\n        ID of area\n    projection : dict or str\n        Projection parameters as a proj4_dict or proj4_string\n    description : str, optional\n        Description/name of area. Defaults to area_id\n    proj_id : str, optional\n        ID of projection (deprecated)\n    units : str, optional\n        Units that provided arguments should be interpreted as. This can be\n        one of \'deg\', \'degrees\', \'meters\', \'metres\', and any\n        parameter supported by the\n        `cs2cs -lu <https://proj4.org/apps/cs2cs.html#cmdoption-cs2cs-lu>`_\n        command. Units are determined in the following priority:\n\n        1. units expressed with each variable through a DataArray\'s attrs attribute.\n        2. units passed to ``units``\n        3. units used in ``projection``\n        4. meters\n\n    width : str, optional\n        Number of pixels in the x direction\n    height : str, optional\n        Number of pixels in the y direction\n    area_extent : list, optional\n        Area extent as a list (lower_left_x, lower_left_y, upper_right_x, upper_right_y)\n    shape : list, optional\n        Number of pixels in the y and x direction (height, width)\n    upper_left_extent : list, optional\n        Upper left corner of upper left pixel (x, y)\n    center : list, optional\n        Center of projection (x, y)\n    resolution : list or float, optional\n        Size of pixels: (dx, dy)\n    radius : list or float, optional\n        Length from the center to the edges of the projection (dx, dy)\n    rotation: float, optional\n        rotation in degrees(negative is cw)\n    nprocs : int, optional\n        Number of processor cores to be used\n    lons : numpy array, optional\n        Grid lons\n    lats : numpy array, optional\n        Grid lats\n    optimize_projection:\n        Whether the projection parameters have to be optimized for a DynamicAreaDefinition.\n\n    Returns\n    -------\n    AreaDefinition or DynamicAreaDefinition : AreaDefinition or DynamicAreaDefinition\n        If shape and area_extent are found, an AreaDefinition object is returned.\n        If only shape or area_extent can be found, a DynamicAreaDefinition object is returned\n\n    Raises\n    ------\n    ValueError:\n        If neither shape nor area_extent could be found\n\n    Notes\n    -----\n    * ``resolution`` and ``radius`` can be specified with one value if dx == dy\n    * If ``resolution`` and ``radius`` are provided as angles, center must be given or findable. In such a case,\n      they represent [projection x distance from center[0] to center[0]+dx, projection y distance from center[1] to\n      center[1]+dy]\n    """"""\n    from pyresample._spatial_mp import Proj\n    description = kwargs.pop(\'description\', area_id)\n    proj_id = kwargs.pop(\'proj_id\', None)\n\n    # convert EPSG dictionaries to projection string\n    # (hold on to EPSG code as much as possible)\n    if isinstance(projection, dict) and \'EPSG\' in projection:\n        projection = ""EPSG:{}"".format(projection[\'EPSG\'])\n\n    # Get a proj4_dict from either a proj4_dict or a proj4_string.\n    proj_dict = _get_proj_data(projection)\n    try:\n        p = Proj(projection, preserve_units=True)\n    except RuntimeError:\n        return _make_area(area_id, description, proj_id, projection, shape, area_extent, **kwargs)\n\n    # If no units are provided, try to get units used in proj_dict. If still none are provided, use meters.\n    if units is None:\n        units = proj_dict.get(\'units\', \'m\' if not p.is_latlong() else \'degrees\')\n\n    # Allow height and width to be provided for more consistency across functions in pyresample.\n    if height is not None or width is not None:\n        shape = _validate_variable(shape, (height, width), \'shape\', [\'height\', \'width\'])\n\n    # Makes sure list-like objects are list-like, have the right shape, and contain only numbers.\n    center = _verify_list(\'center\', center, 2)\n    radius = _verify_list(\'radius\', radius, 2)\n    upper_left_extent = _verify_list(\'upper_left_extent\', upper_left_extent, 2)\n    resolution = _verify_list(\'resolution\', resolution, 2)\n    shape = _verify_list(\'shape\', shape, 2)\n    area_extent = _verify_list(\'area_extent\', area_extent, 4)\n\n    # Converts from lat/lon to projection coordinates (x,y) if not in projection coordinates. Returns tuples.\n    center = _convert_units(center, \'center\', units, p, proj_dict)\n    upper_left_extent = _convert_units(upper_left_extent, \'upper_left_extent\', units, p, proj_dict)\n    if area_extent is not None:\n        # convert area extent, pass as (X, Y)\n        area_extent_ll = area_extent[:2]\n        area_extent_ur = area_extent[2:]\n        area_extent_ll = _convert_units(area_extent_ll, \'area_extent\', units, p, proj_dict)\n        area_extent_ur = _convert_units(area_extent_ur, \'area_extent\', units, p, proj_dict)\n        area_extent = area_extent_ll + area_extent_ur\n\n    # Fills in missing information to attempt to create an area definition.\n    if area_extent is None or shape is None:\n        area_extent, shape, resolution = \\\n            _extrapolate_information(area_extent, shape, center, radius,\n                                     resolution, upper_left_extent, units,\n                                     p, proj_dict)\n    return _make_area(area_id, description, proj_id, projection, shape,\n                      area_extent, resolution=resolution, **kwargs)\n\n\ndef _make_area(area_id, description, proj_id, proj_dict, shape, area_extent, **kwargs):\n    """"""Handles the creation of an area definition for create_area_def.""""""\n    from pyresample.geometry import AreaDefinition\n    from pyresample.geometry import DynamicAreaDefinition\n    # Remove arguments that are only for DynamicAreaDefinition.\n    optimize_projection = kwargs.pop(\'optimize_projection\', False)\n    resolution = kwargs.pop(\'resolution\', None)\n    # If enough data is provided, create an AreaDefinition. If only shape or area_extent are found, make a\n    # DynamicAreaDefinition. If not enough information was provided, raise a ValueError.\n    height, width = (None, None)\n    if shape is not None:\n        height, width = shape\n    if None not in (area_extent, shape):\n        return AreaDefinition(area_id, description, proj_id, proj_dict, width, height, area_extent, **kwargs)\n\n    return DynamicAreaDefinition(area_id=area_id, description=description, projection=proj_dict, width=width,\n                                 height=height, area_extent=area_extent, rotation=kwargs.get(\'rotation\'),\n                                 resolution=resolution, optimize_projection=optimize_projection)\n\n\ndef _get_proj_data(projection):\n    """"""Takes a proj4_dict or proj4_string and returns a proj4_dict and a Proj function.\n\n    There is special handling for the ""EPSG:XXXX"" case where ""XXXX"" is an\n    EPSG number code. It can be provided as a string `""EPSG:XXXX""` or as a\n    dictionary (when provided via YAML) as `{\'EPSG\': XXXX}`.\n    If it is passed as a string (""EPSG:XXXX"") then the rules of\n    :func:`~pyresample.utils._proj.proj4_str_to_dict` are followed.\n    If a dictionary and pyproj 2.0+ is installed then the string\n    `""EPSG:XXXX""` is passed to ``proj4_str_to_dict``. If pyproj<2.0\n    is installed then the string ``+init=EPSG:XXXX`` is passed to\n    ``proj4_str_to_dict`` which provides limited information to area\n    config operations.\n\n    """"""\n    if isinstance(projection, dict) and \'EPSG\' in projection:\n        projection = ""EPSG:{}"".format(projection[\'EPSG\'])\n\n    if isinstance(projection, str):\n        proj_dict = proj4_str_to_dict(projection)\n    elif isinstance(projection, dict):\n        proj_dict = projection\n    else:\n        raise TypeError(\'Wrong type for projection: {0}. Expected dict or string.\'.format(type(projection)))\n    return proj_dict\n\n\ndef _sign(num):\n    """"""Return the sign of the number provided.\n\n    Returns:\n        1 if number is greater than 0, -1 otherwise\n\n    """"""\n    return -1 if num < 0 else 1\n\n\ndef _round_poles(center, units, p):\n    """"""Round center to the nearest pole if it is extremely close to said pole.\n\n    Used to work around floating point precision issues .\n\n    """"""\n    # For a laea projection, this allows for an error of 11 meters around the pole.\n    error = .0001\n    if \'deg\' in units:\n        if abs(abs(center[1]) - 90) < error:\n            center = (center[0], _sign(center[1]) * 90)\n    else:\n        center = p(*center, inverse=True, errcheck=True)\n        if abs(abs(center[1]) - 90) < error:\n            center = (center[0], _sign(center[1]) * 90)\n        center = p(*center, errcheck=True)\n    return center\n\n\ndef _distance_from_center_forward(var, center, p):\n    """"""Convert distances in degrees to projection units.""""""\n    # Interprets radius and resolution as distances between latitudes/longitudes.\n    # Since the distance between longitudes and latitudes is not constant in\n    # most projections, there must be reference point to start from.\n    if center is None:\n        center = (0, 0)\n\n    center_as_angle = p(*center, inverse=True, errcheck=True)\n    pole = 90\n    # If on a pole, use northern/southern latitude for both height and width.\n    if abs(abs(center_as_angle[1]) - pole) < 1e-8:\n        direction_of_poles = _sign(center_as_angle[1])\n        var = (center[1] - p(0, center_as_angle[1] - direction_of_poles * abs(var[0]),\n                             errcheck=True)[1],\n               center[1] - p(0, center_as_angle[1] - direction_of_poles * abs(var[1]),\n                             errcheck=True)[1])\n    # Uses southern latitude and western longitude if radius is positive. Uses northern latitude and\n    # eastern longitude if radius is negative.\n    else:\n        var = (center[0] - p(center_as_angle[0] - var[0], center_as_angle[1], errcheck=True)[0],\n               center[1] - p(center_as_angle[0], center_as_angle[1] - var[1], errcheck=True)[1])\n    return var\n\n\ndef _convert_units(var, name, units, p, proj_dict, inverse=False, center=None):\n    """"""Converts units from lon/lat to projection coordinates (meters).\n\n    If `inverse` it True then the inverse calculation is done.\n\n    """"""\n    from pyproj import transform\n    from pyresample._spatial_mp import Proj\n    if var is None:\n        return None\n    if isinstance(var, DataArray):\n        units = var.units\n        var = tuple(var.data.tolist())\n    if p.is_latlong() and not (\'deg\' == units or \'degrees\' == units):\n        raise ValueError(\'latlon/latlong projection cannot take {0} as units: {1}\'.format(units, name))\n    # Check if units are an angle.\n    is_angle = (\'deg\' == units or \'degrees\' == units)\n    if (\'deg\' in units) and not is_angle:\n        logging.warning(\'units provided to {0} are incorrect: {1}\'.format(name, units))\n    # Convert from var projection units to projection units given by projection from user.\n    if not is_angle:\n        if units == \'meters\' or units == \'metres\':\n            units = \'m\'\n        if proj_dict.get(\'units\', \'m\') != units:\n            tmp_proj_dict = proj_dict.copy()\n            tmp_proj_dict[\'units\'] = units\n            var = transform(Proj(tmp_proj_dict, preserve_units=True), p, *var)\n    if name == \'center\':\n        var = _round_poles(var, units, p)\n    # Return either degrees or meters depending on if the inverse is true or not.\n    # Don\'t convert if inverse is True: Want degrees.\n    # Converts list-like from degrees to meters.\n    if is_angle and not inverse:\n        if name in (\'radius\', \'resolution\'):\n            var = _distance_from_center_forward(var, center, p)\n        else:\n            var = p(*var, errcheck=True)\n    # Don\'t convert if inverse is False: Want meters.\n    elif not is_angle and inverse:\n        # Converts list-like from meters to degrees.\n        var = p(*var, inverse=True, errcheck=True)\n    if name in [\'radius\', \'resolution\']:\n        var = (abs(var[0]), abs(var[1]))\n    return var\n\n\ndef _round_shape(shape, radius=None, resolution=None):\n    """"""Make sure shape is an integer.\n\n    Rounds down if shape is less than .01 above nearest whole number to\n    handle floating point precision issues. Otherwise the number is round\n    up.\n\n    """"""\n    # Used for area definition to prevent indexing None.\n    if shape is None:\n        return None\n    incorrect_shape = False\n    height, width = shape\n    if abs(width - round(width)) > 1e-8:\n        incorrect_shape = True\n        if width - math.floor(width) >= .01:\n            width = math.ceil(width)\n    width = int(round(width))\n    if abs(height - round(height)) > 1e-8:\n        incorrect_shape = True\n        if height - math.floor(height) >= .01:\n            height = math.ceil(height)\n    height = int(round(height))\n    if incorrect_shape:\n        if radius is not None and resolution is not None:\n            new_resolution = (2 * radius[0] / width, 2 * radius[1] / height)\n            logging.warning(\'shape found from radius and resolution does not contain only \'\n                            \'integers: {0}\\nRounding shape to {1} and resolution from {2} meters to \'\n                            \'{3} meters\'.format(shape, (height, width), resolution, new_resolution))\n        else:\n            logging.warning(\'shape provided does not contain only integers: {0}\\n\'\n                            \'Rounding shape to {1}\'.format(shape, (height, width)))\n    return height, width\n\n\ndef _validate_variable(var, new_var, var_name, input_list):\n    """"""Makes sure data given by the user does not conflict with itself.\n\n    If a variable that was given by the user contradicts other data provided, an exception is raised.\n    Example: upper_left_extent is (-10, 10), but area_extent is (-20, -20, 20, 20).\n\n    """"""\n    if var is not None and not np.allclose(np.array(var, dtype=float), np.array(new_var, dtype=float), equal_nan=True):\n        raise ValueError(\'CONFLICTING DATA: {0} given does not match {0} found from {1}\'.format(\n            var_name, \', \'.join(input_list)) + \':\\ngiven: {0}\\nvs\\nfound: {1}\'.format(var, new_var, var_name,\n                                                                                      input_list))\n    return new_var\n\n\ndef _extrapolate_information(area_extent, shape, center, radius, resolution, upper_left_extent, units, p, proj_dict):\n    """"""Attempts to find shape and area_extent based on data provided.\n\n    Parameters are used in a specific order to determine area_extent and shape.\n    The area_extent and shape are later used to create an `AreaDefinition`.\n    Providing some parameters may have no effect if other parameters could be\n    to used determine area_extent and shape. The order of the parameters used\n    is:\n\n    1. area_extent\n    2. upper_left_extent and center\n    3. radius and resolution\n    4. resolution and shape\n    5. radius and center\n    6. upper_left_extent and radius\n\n    """"""\n    # Input unaffected by data below: When area extent is calculated, it\'s either with\n    # shape (giving you an area definition) or with center/radius/upper_left_extent (which this produces).\n    # Yet output (center/radius/upper_left_extent) is essential for data below.\n    if area_extent is not None:\n        # Function 1-A\n        new_center = ((area_extent[2] + area_extent[0]) / 2, (area_extent[3] + area_extent[1]) / 2)\n        center = _validate_variable(center, new_center, \'center\', [\'area_extent\'])\n        # If radius is given in an angle without center it will raise an exception, and to verify, it must be in meters.\n        radius = _convert_units(radius, \'radius\', units, p, proj_dict, center=center)\n        new_radius = ((area_extent[2] - area_extent[0]) / 2, (area_extent[3] - area_extent[1]) / 2)\n        radius = _validate_variable(radius, new_radius, \'radius\', [\'area_extent\'])\n        new_upper_left_extent = (area_extent[0], area_extent[3])\n        upper_left_extent = _validate_variable(\n            upper_left_extent, new_upper_left_extent, \'upper_left_extent\', [\'area_extent\'])\n    # Output used below, but nowhere else is upper_left_extent made. Thus it should go as early as possible.\n    elif None not in (upper_left_extent, center):\n        # Function 1-B\n        radius = _convert_units(radius, \'radius\', units, p, proj_dict, center=center)\n        new_radius = (center[0] - upper_left_extent[0], upper_left_extent[1] - center[1])\n        radius = _validate_variable(radius, new_radius, \'radius\', [\'upper_left_extent\', \'center\'])\n    else:\n        radius = _convert_units(radius, \'radius\', units, p, proj_dict, center=center)\n    # Convert resolution to meters if given as an angle. If center is not found, an exception is raised.\n    resolution = _convert_units(resolution, \'resolution\', units, p, proj_dict, center=center)\n    # Inputs unaffected by data below: area_extent is not an input. However, output is used below.\n    if radius is not None and resolution is not None:\n        # Function 2-A\n        new_shape = _round_shape((2 * radius[1] / resolution[1], 2 * radius[0] / resolution[0]), radius=radius,\n                                 resolution=resolution)\n        shape = _validate_variable(shape, new_shape, \'shape\', [\'radius\', \'resolution\'])\n    elif resolution is not None and shape is not None:\n        # Function 2-B\n        new_radius = (resolution[0] * shape[1] / 2, resolution[1] * shape[0] / 2)\n        radius = _validate_variable(radius, new_radius, \'radius\', [\'shape\', \'resolution\'])\n    # Input determined from above functions, but output does not affect above functions: area_extent can be\n    # used to find center/upper_left_extent which are used to find each other, which is redundant.\n    if center is not None and radius is not None:\n        # Function 1-C\n        new_area_extent = (center[0] - radius[0], center[1] - radius[1], center[0] + radius[0], center[1] + radius[1])\n        area_extent = _validate_variable(area_extent, new_area_extent, \'area_extent\', [\'center\', \'radius\'])\n    elif upper_left_extent is not None and radius is not None:\n        # Function 1-D\n        new_area_extent = (\n            upper_left_extent[0], upper_left_extent[1] - 2 * radius[1], upper_left_extent[0] + 2 * radius[0],\n            upper_left_extent[1])\n        area_extent = _validate_variable(area_extent, new_area_extent, \'area_extent\', [\'upper_left_extent\', \'radius\'])\n    return area_extent, shape, resolution\n\n\ndef _format_list(var, name):\n    """"""Used to let resolution and radius be single numbers if their elements are equal.\n\n    Also makes sure that data is list-like and contains only numbers.\n    """"""\n    # Single-number format.\n    if not isinstance(var, (list, tuple)) and name in (\'resolution\', \'radius\'):\n        var = (float(var), float(var))\n    elif name == \'shape\':\n        var = _round_shape(var)\n    else:\n        var = tuple(float(num) for num in var)\n    return var\n\n\ndef _verify_list(name, var, length):\n    """"""Checks that list-like variables are list-like, shapes are accurate, and values are numbers.""""""\n    # Make list-like data into tuples (or leave as xarrays). If not list-like, throw a ValueError unless it is None.\n    if var is None:\n        return None\n    # Verify that list is made of numbers and is list-like.\n    try:\n        if \'units\' in getattr(var, \'attrs\', {}) and name != \'shape\':\n            # For len(var) to work, DataArray must contain a list, not a tuple\n            var = DataArray(list(_format_list(var.data.tolist(), name)), attrs=var.attrs)\n        elif isinstance(var, DataArray):\n            if name == \'shape\':\n                logging.warning(""{0} is unitless, but was passed as a DataArray"".format(name, var.attrs))\n            else:\n                logging.warning(""{0} is a DataArray but does not have the attribute \'units\',""\n                                ""but instead has attribute(s): {1}"".format(name, var.attrs))\n            var = _format_list(var.data.tolist(), name)\n        else:\n            var = _format_list(var, name)\n    except TypeError:\n        raise ValueError(\'{0} is not list-like:\\n{1}\'.format(name, var))\n    except ValueError:\n        raise ValueError(\'{0} is not composed purely of numbers:\\n{1}\'.format(name, var))\n    # Confirm correct shape\n    if len(var) != length:\n        raise ValueError(\'{0} should have length {1}, but instead has length {2}:\\n{3}\'.format(name, length,\n                                                                                               len(var), var))\n    return var\n\n\ndef convert_def_to_yaml(def_area_file, yaml_area_file):\n    """"""Convert a legacy area def file to the yaml counter partself.\n\n    *yaml_area_file* will be overwritten by the operation.\n    """"""\n    areas = _parse_legacy_area_file(def_area_file)\n    with open(yaml_area_file, \'w\') as yaml_file:\n        for area in areas:\n            yaml_file.write(area.create_areas_def())\n'"
pyresample/boundary.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2014, 2015, 2017 Martin Raspaud\n\n# Author(s):\n\n#   Martin Raspaud <martin.raspaud@smhi.se>\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""The Boundary classes.\n""""""\n\n\nimport logging\nimport logging.handlers\n\nimport numpy as np\n\nfrom pyresample.spherical import SphPolygon\n\nlogger = logging.getLogger(__name__)\n\n\nclass Boundary(object):\n\n    """"""Boundary objects.\n    """"""\n\n    def __init__(self, lons=None, lats=None, frequency=1):\n        self._contour_poly = None\n        if lons is not None:\n            self.lons = lons[::frequency]\n        if lats is not None:\n            self.lats = lats[::frequency]\n\n    def contour(self):\n        return self.lons, self.lats\n\n    @property\n    def contour_poly(self):\n        """"""Get the Spherical polygon corresponding to the Boundary\n        """"""\n        if self._contour_poly is None:\n            self._contour_poly = SphPolygon(\n                np.deg2rad(np.vstack(self.contour()).T))\n        return self._contour_poly\n\n    def draw(self, mapper, options, **more_options):\n        """"""Draw the current boundary on the *mapper*\n        """"""\n        self.contour_poly.draw(mapper, options, **more_options)\n\n\nclass AreaBoundary(Boundary):\n\n    """"""Area boundary objects.\n    """"""\n\n    def __init__(self, *sides):\n        Boundary.__init__(self)\n        self.sides_lons, self.sides_lats = zip(*sides)\n        self.sides_lons = list(self.sides_lons)\n        self.sides_lats = list(self.sides_lats)\n\n    def decimate(self, ratio):\n        """"""Remove some points in the boundaries, but never the corners.\n        """"""\n        for i in range(len(self.sides_lons)):\n            length = len(self.sides_lons[i])\n            start = int((length % ratio) / 2)\n            points = np.concatenate(([0], np.arange(start, length, ratio),\n                                     [length - 1]))\n            if points[1] == 0:\n                points = points[1:]\n            if points[-2] == (length - 1):\n                points = points[:-1]\n            self.sides_lons[i] = self.sides_lons[i][points]\n            self.sides_lats[i] = self.sides_lats[i][points]\n\n    def contour(self):\n        """"""Get the (lons, lats) tuple of the boundary object.\n        """"""\n        lons = np.concatenate([lns[:-1] for lns in self.sides_lons])\n        lats = np.concatenate([lts[:-1] for lts in self.sides_lats])\n\n        return lons, lats\n\n\nclass AreaDefBoundary(AreaBoundary):\n    """"""Boundaries for area definitions (pyresample).\n    """"""\n\n    def __init__(self, area, frequency=1):\n        lons, lats = area.get_bbox_lonlats()\n        AreaBoundary.__init__(self,\n                              *zip(lons, lats))\n\n        if frequency != 1:\n            self.decimate(frequency)\n\n\nclass SimpleBoundary(object):\n    """"""Container for geometry boundary.\n    Labelling starts in upper left corner and proceeds clockwise""""""\n\n    def __init__(self, side1, side2, side3, side4):\n        self.side1 = side1\n        self.side2 = side2\n        self.side3 = side3\n        self.side4 = side4\n'"
pyresample/data_reduce.py,10,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010, 2015  Esben S. Nielsen\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Reduce data sets based on geographical information""""""\n\nfrom __future__ import absolute_import\n\nimport numpy as np\n\n# Earth radius\nR = 6370997.0\n\n\ndef swath_from_cartesian_grid(cart_grid, lons, lats, data,\n                              radius_of_influence):\n    """"""Makes coarse data reduction of swath data by comparison with\n    cartesian grid\n\n    Parameters\n    ----------\n    chart_grid : numpy array\n        Grid of area cartesian coordinates\n    lons : numpy array\n        Swath lons\n    lats : numpy array\n        Swath lats\n    data : numpy array\n        Swath data\n    radius_of_influence : float\n        Cut off distance in meters\n\n    Returns\n    -------\n    (lons, lats, data) : list of numpy arrays\n        Reduced swath data and coordinate set\n    """"""\n\n    valid_index = get_valid_index_from_cartesian_grid(cart_grid, lons, lats,\n                                                      radius_of_influence)\n\n    lons = lons[valid_index]\n    lats = lats[valid_index]\n    data = data[valid_index]\n\n    return lons, lats, data\n\n\ndef get_valid_index_from_cartesian_grid(cart_grid, lons, lats,\n                                        radius_of_influence):\n    """"""Calculates relevant data indices using coarse data reduction of swath\n    data by comparison with cartesian grid\n\n    Parameters\n    ----------\n    chart_grid : numpy array\n        Grid of area cartesian coordinates\n    lons : numpy array\n        Swath lons\n    lats : numpy array\n        Swath lats\n    data : numpy array\n        Swath data\n    radius_of_influence : float\n        Cut off distance in meters\n\n    Returns\n    -------\n    valid_index : numpy array\n        Boolean array of same size as lons and lats indicating relevant indices\n    """"""\n\n    def _get_lons(x, y):\n        return np.rad2deg(np.arccos(x / np.sqrt(x ** 2 + y ** 2))) * np.sign(y)\n\n    def _get_lats(z):\n        return 90 - np.rad2deg(np.arccos(z / R))\n\n    # Get sides of target grid and transform to lon lats\n    lons_side1 = _get_lons(cart_grid[0, :, 0], cart_grid[0, :, 1])\n    lons_side2 = _get_lons(cart_grid[:, -1, 0], cart_grid[:, -1, 1])\n    lons_side3 = _get_lons(cart_grid[-1, ::-1, 0], cart_grid[-1, ::-1, 1])\n    lons_side4 = _get_lons(cart_grid[::-1, 0, 0], cart_grid[::-1, 0, 1])\n\n    lats_side1 = _get_lats(cart_grid[0, :, 2])\n    lats_side2 = _get_lats(cart_grid[:, -1, 2])\n    lats_side3 = _get_lats(cart_grid[-1, ::-1, 2])\n    lats_side4 = _get_lats(cart_grid[::-1, 0, 2])\n\n    valid_index = _get_valid_index(lons_side1, lons_side2, lons_side3, lons_side4,\n                                   lats_side1, lats_side2, lats_side3, lats_side4,\n                                   lons, lats, radius_of_influence)\n\n    return valid_index\n\n\ndef swath_from_lonlat_grid(grid_lons, grid_lats, lons, lats, data,\n                           radius_of_influence):\n    """"""Makes coarse data reduction of swath data by comparison with\n    lon lat grid\n\n    Parameters\n    ----------\n    grid_lons : numpy array\n        Grid of area lons\n    grid_lats : numpy array\n        Grid of area lats\n    lons : numpy array\n        Swath lons\n    lats : numpy array\n        Swath lats\n    data : numpy array\n        Swath data\n    radius_of_influence : float\n        Cut off distance in meters\n\n    Returns\n    -------\n    (lons, lats, data) : list of numpy arrays\n        Reduced swath data and coordinate set\n    """"""\n\n    valid_index = get_valid_index_from_lonlat_grid(\n        grid_lons, grid_lats, lons, lats, radius_of_influence)\n\n    lons = lons[valid_index]\n    lats = lats[valid_index]\n    data = data[valid_index]\n\n    return lons, lats, data\n\n\ndef swath_from_lonlat_boundaries(boundary_lons, boundary_lats, lons, lats, data,\n                                 radius_of_influence):\n    """"""Makes coarse data reduction of swath data by comparison with\n    lon lat boundary\n\n    Parameters\n    ----------\n    boundary_lons : numpy array\n        Grid of area lons\n    boundary_lats : numpy array\n        Grid of area lats\n    lons : numpy array\n        Swath lons\n    lats : numpy array\n        Swath lats\n    data : numpy array\n        Swath data\n    radius_of_influence : float\n        Cut off distance in meters\n\n    Returns\n    -------\n    (lons, lats, data) : list of numpy arrays\n        Reduced swath data and coordinate set\n    """"""\n\n    valid_index = get_valid_index_from_lonlat_boundaries(boundary_lons,\n                                                         boundary_lats, lons, lats, radius_of_influence)\n\n    lons = lons[valid_index]\n    lats = lats[valid_index]\n    data = data[valid_index]\n\n    return lons, lats, data\n\n\ndef get_valid_index_from_lonlat_grid(grid_lons, grid_lats, lons, lats, radius_of_influence):\n    """"""Calculates relevant data indices using coarse data reduction of swath\n    data by comparison with lon lat grid\n\n    Parameters\n    ----------\n    chart_grid : numpy array\n        Grid of area cartesian coordinates\n    lons : numpy array\n        Swath lons\n    lats : numpy array\n        Swath lats\n    data : numpy array\n        Swath data\n    radius_of_influence : float\n        Cut off distance in meters\n\n    Returns\n    -------\n    valid_index : numpy array\n        Boolean array of same size as lon and lat indicating relevant indices\n    """"""\n\n    # Get sides of target grid\n    lons_side1 = grid_lons[0, :]\n    lons_side2 = grid_lons[:, -1]\n    lons_side3 = grid_lons[-1, ::-1]\n    lons_side4 = grid_lons[::-1, 0]\n\n    lats_side1 = grid_lats[0, :]\n    lats_side2 = grid_lats[:, -1]\n    lats_side3 = grid_lats[-1, :]\n    lats_side4 = grid_lats[:, 0]\n\n    valid_index = _get_valid_index(lons_side1, lons_side2, lons_side3, lons_side4,\n                                   lats_side1, lats_side2, lats_side3, lats_side4,\n                                   lons, lats, radius_of_influence)\n\n    return valid_index\n\n\ndef get_valid_index_from_lonlat_boundaries(boundary_lons, boundary_lats, lons, lats, radius_of_influence):\n    """"""Find relevant indices from grid boundaries using the\n    winding number theorem""""""\n\n    valid_index = _get_valid_index(boundary_lons.side1, boundary_lons.side2,\n                                   boundary_lons.side3, boundary_lons.side4,\n                                   boundary_lats.side1, boundary_lats.side2,\n                                   boundary_lats.side3, boundary_lats.side4,\n                                   lons, lats, radius_of_influence)\n\n    return valid_index\n\n\ndef _get_valid_index(lons_side1, lons_side2, lons_side3, lons_side4,\n                     lats_side1, lats_side2, lats_side3, lats_side4,\n                     lons, lats, radius_of_influence):\n    """"""Find relevant indices from grid boundaries using the\n    winding number theorem""""""\n\n    # Coarse reduction of data based on extrema analysis of the boundary\n    # lon lat values of the target grid\n    illegal_lons = (((lons_side1 < -180) | (lons_side1 > 180)).any() or\n                    ((lons_side2 < -180) | (lons_side2 > 180)).any() or\n                    ((lons_side3 < -180) | (lons_side3 > 180)).any() or\n                    ((lons_side4 < -180) | (lons_side4 > 180)).any())\n\n    illegal_lats = (((lats_side1 < -90) | (lats_side1 > 90)).any() or\n                    ((lats_side2 < -90) | (lats_side2 > 90)).any() or\n                    ((lats_side3 < -90) | (lats_side3 > 90)).any() or\n                    ((lats_side4 < -90) | (lats_side4 > 90)).any())\n\n    if illegal_lons or illegal_lats:\n        # Grid boundaries are not safe to operate on\n        return np.ones(lons.size, dtype=np.bool)\n\n    # Find sum angle sum of grid boundary\n    angle_sum = 0\n    for side in (lons_side1, lons_side2, lons_side3, lons_side4):\n        prev = None\n        side_sum = 0\n        for lon in side:\n            if prev:\n                delta = lon - prev\n                if abs(delta) > 180:\n                    delta = (abs(delta) - 360) * (delta // abs(delta))\n                angle_sum += delta\n                side_sum += delta\n            prev = lon\n\n    # Buffer min and max lon and lat of interest with radius of interest\n    lat_min = min(lats_side1.min(), lats_side2.min(), lats_side3.min(),\n                  lats_side4.min())\n    lat_min_buffered = lat_min - np.degrees(float(radius_of_influence) / R)\n    lat_max = max(lats_side1.max(), lats_side2.max(), lats_side3.max(),\n                  lats_side4.max())\n    lat_max_buffered = lat_max + np.degrees(float(radius_of_influence) / R)\n\n    max_angle_s2 = max(abs(lats_side2.max()), abs(lats_side2.min()))\n    max_angle_s4 = max(abs(lats_side4.max()), abs(lats_side4.min()))\n    lon_min_buffered = (lons_side4.min() -\n                        np.degrees(float(radius_of_influence) /\n                                   (np.sin(np.radians(max_angle_s4)) * R)))\n\n    lon_max_buffered = (lons_side2.max() +\n                        np.degrees(float(radius_of_influence) /\n                                   (np.sin(np.radians(max_angle_s2)) * R)))\n\n    # From the winding number theorem follows:\n    # angle_sum possiblilities:\n    # -360: area covers north pole\n    # 360: area covers south pole\n    #   0: area covers no poles\n    # else: area covers both poles\n    if round(angle_sum) == -360:\n        # Covers NP\n        valid_index = (lats >= lat_min_buffered)\n    elif round(angle_sum) == 360:\n        # Covers SP\n        valid_index = (lats <= lat_max_buffered)\n    elif round(angle_sum) == 0:\n        # Covers no poles\n        valid_lats = (lats >= lat_min_buffered) * (lats <= lat_max_buffered)\n\n        if lons_side2.min() > lons_side4.max():\n            # No date line crossing\n            valid_lons = (lons >= lon_min_buffered) * \\\n                (lons <= lon_max_buffered)\n        else:\n            # Date line crossing\n            seg1 = (lons >= lon_min_buffered) * (lons <= 180)\n            seg2 = (lons <= lon_max_buffered) * (lons >= -180)\n            valid_lons = seg1 + seg2\n\n        valid_index = valid_lats * valid_lons\n    else:\n        # Covers both poles don\'t reduce\n        valid_index = np.ones(lons.size, dtype=np.bool)\n\n    return valid_index\n'"
pyresample/geo_filter.py,6,"b'import numpy as np\n\nfrom . import _spatial_mp\nfrom . import geometry\n\n\nclass GridFilter(object):\n    """"""Geographic filter from a grid.\n\n    Args:\n        grid_ll_x (float):\n            Projection x coordinate of lower left corner of lower left pixel\n        grid_ll_y (float):\n            Projection y coordinate of lower left corner of lower left pixel\n        grid_ur_x (float):\n            Projection x coordinate of upper right corner of upper right pixel\n        grid_ur_y (float):\n            Projection y coordinate of upper right corner of upper right pixel\n        proj4_string (str):\n            Projection definition as a PROJ.4 string.\n        mask (numpy array):\n            Mask as boolean numpy array\n\n    """"""\n\n    def __init__(self, area_def, filter, nprocs=1):\n        self.area_def = area_def\n        self._filter = filter.astype(np.bool)\n        self.nprocs = nprocs\n\n    def get_valid_index(self, geometry_def):\n        """"""Calculates valid_index array  based on lons and lats\n\n        Args:\n            lons (numpy array): Longitude degrees array\n            lats (numpy array): Latitude degrees array\n\n        Returns:\n            Boolean numpy array of same shape as lons and lats\n\n        """"""\n\n        lons = geometry_def.lons[:]\n        lats = geometry_def.lats[:]\n\n        # Get projection coords\n        if self.nprocs > 1:\n            proj = _spatial_mp.Proj_MP(**self.area_def.proj_dict)\n        else:\n            proj = _spatial_mp.Proj(**self.area_def.proj_dict)\n\n        x_coord, y_coord = proj(lons, lats, nprocs=self.nprocs)\n\n        # Find array indices of coordinates\n        target_x = ((x_coord / self.area_def.pixel_size_x) +\n                    self.area_def.pixel_offset_x).astype(np.int32)\n        target_y = (self.area_def.pixel_offset_y -\n                    (y_coord / self.area_def.pixel_size_y)).astype(np.int32)\n\n        # Create mask for pixels outside array (invalid pixels)\n        target_x_valid = (target_x >= 0) & (target_x < self.area_def.width)\n        target_y_valid = (target_y >= 0) & (target_y < self.area_def.height)\n\n        # Set index of invalid pixels to 0\n        target_x[np.invert(target_x_valid)] = 0\n        target_y[np.invert(target_y_valid)] = 0\n\n        # Find mask\n        filter = self._filter[target_y, target_x]\n\n        # Remove invalid pixels\n        filter = (filter & target_x_valid & target_y_valid).astype(np.bool)\n\n        return filter\n\n    def filter(self, geometry_def, data):\n        lons = geometry_def.lons[:]\n        lats = geometry_def.lats[:]\n        valid_index = self.get_valid_index(geometry_def)\n        lons_f = lons[valid_index]\n        lats_f = lats[valid_index]\n        data_f = data[valid_index]\n        geometry_def_f = \\\n            geometry.CoordinateDefinition(lons_f, lats_f,\n                                          nprocs=geometry_def.nprocs)\n        return geometry_def_f, data_f\n'"
pyresample/geometry.py,104,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010-2016\n#\n# Authors:\n#    Esben S. Nielsen\n#    Thomas Lavergne\n#    Adam Dybbroe\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Classes for geometry operations.""""""\n\nimport hashlib\nimport warnings\nfrom collections import OrderedDict\nfrom logging import getLogger\n\nimport numpy as np\nimport yaml\nfrom pyproj import Geod, transform\n\nfrom pyresample import CHUNK_SIZE\nfrom pyresample._spatial_mp import Cartesian, Cartesian_MP, Proj, Proj_MP\nfrom pyresample.boundary import AreaDefBoundary, Boundary, SimpleBoundary\nfrom pyresample.utils import (proj4_str_to_dict, proj4_dict_to_str,\n                              convert_proj_floats, proj4_radius_parameters,\n                              check_slice_orientation, load_cf_area)\nfrom pyresample.area_config import create_area_def\n\ntry:\n    from xarray import DataArray\nexcept ImportError:\n    DataArray = np.ndarray\n\ntry:\n    from pyproj import CRS\nexcept ImportError:\n    CRS = None\n\nlogger = getLogger(__name__)\n\n\nclass DimensionError(ValueError):\n    """"""Wrap ValueError.""""""\n\n    pass\n\n\nclass IncompatibleAreas(ValueError):\n    """"""Error when the areas to combine are not compatible.""""""\n\n    pass\n\n\nclass BaseDefinition(object):\n    """"""Base class for geometry definitions.\n\n    .. versionchanged:: 1.8.0\n\n        `BaseDefinition` no longer checks the validity of the provided\n        longitude and latitude coordinates to improve performance. Longitude\n        arrays are expected to be between -180 and 180 degrees, latitude -90\n        to 90 degrees. Use :func:`~pyresample.utils.check_and_wrap` to preprocess\n        your arrays.\n\n    """"""\n\n    def __init__(self, lons=None, lats=None, nprocs=1):\n        """"""Initialize BaseDefinition.""""""\n        if type(lons) != type(lats):\n            raise TypeError(\'lons and lats must be of same type\')\n        elif lons is not None:\n            if not isinstance(lons, (np.ndarray, DataArray)):\n                lons = np.asanyarray(lons)\n                lats = np.asanyarray(lats)\n            if lons.shape != lats.shape:\n                raise ValueError(\'lons and lats must have same shape\')\n\n        self.nprocs = nprocs\n        self.lats = lats\n        self.lons = lons\n        self.ndim = None\n        self.cartesian_coords = None\n        self.hash = None\n\n    def __getitem__(self, key):\n        """"""Slice a 2D geographic definition.""""""\n        y_slice, x_slice = key\n        return self.__class__(\n            lons=self.lons[y_slice, x_slice],\n            lats=self.lats[y_slice, x_slice],\n            nprocs=self.nprocs\n        )\n\n    def __hash__(self):\n        """"""Compute the hash of this object.""""""\n        if self.hash is None:\n            self.hash = int(self.update_hash().hexdigest(), 16)\n        return self.hash\n\n    def __eq__(self, other):\n        """"""Test for approximate equality.""""""\n        if self is other:\n            return True\n        if other.lons is None or other.lats is None:\n            other_lons, other_lats = other.get_lonlats()\n        else:\n            other_lons = other.lons\n            other_lats = other.lats\n\n        if self.lons is None or self.lats is None:\n            self_lons, self_lats = self.get_lonlats()\n        else:\n            self_lons = self.lons\n            self_lats = self.lats\n\n        if self_lons is other_lons and self_lats is other_lats:\n            return True\n        if isinstance(self_lons, DataArray) and np.ndarray is not DataArray:\n            self_lons = self_lons.data\n            self_lats = self_lats.data\n        if isinstance(other_lons, DataArray) and np.ndarray is not DataArray:\n            other_lons = other_lons.data\n            other_lats = other_lats.data\n        try:\n            from dask.array import allclose\n        except ImportError:\n            from numpy import allclose\n        try:\n            return (allclose(self_lons, other_lons, atol=1e-6, rtol=5e-9, equal_nan=True) and\n                    allclose(self_lats, other_lats, atol=1e-6, rtol=5e-9, equal_nan=True))\n        except (AttributeError, ValueError):\n            return False\n\n    def __ne__(self, other):\n        """"""Test for approximate equality.""""""\n        return not self.__eq__(other)\n\n    def get_area_extent_for_subset(self, row_LR, col_LR, row_UL, col_UL):\n        """"""Calculate extent for a subdomain of this area.\n\n        Rows are counted from upper left to lower left and columns are\n        counted from upper left to upper right.\n\n        Args:\n            row_LR (int): row of the lower right pixel\n            col_LR (int): col of the lower right pixel\n            row_UL (int): row of the upper left pixel\n            col_UL (int): col of the upper left pixel\n\n        Returns:\n            area_extent (tuple):\n                Area extent (LL_x, LL_y, UR_x, UR_y) of the subset\n\n        Author:\n            Ulrich Hamann\n\n        """"""\n        (a, b) = self.get_proj_coords(data_slice=(row_LR, col_LR))\n        a = a - 0.5 * self.pixel_size_x\n        b = b - 0.5 * self.pixel_size_y\n        (c, d) = self.get_proj_coords(data_slice=(row_UL, col_UL))\n        c = c + 0.5 * self.pixel_size_x\n        d = d + 0.5 * self.pixel_size_y\n\n        return a, b, c, d\n\n    def get_lonlat(self, row, col):\n        """"""Retrieve lon and lat of single pixel.\n\n        Parameters\n        ----------\n        row : int\n        col : int\n\n        Returns\n        -------\n        (lon, lat) : tuple of floats\n\n        """"""\n        if self.ndim != 2:\n            raise DimensionError((\'operation undefined \'\n                                  \'for %sD geometry \') % self.ndim)\n        elif self.lons is None or self.lats is None:\n            raise ValueError(\'lon/lat values are not defined\')\n        return self.lons[row, col], self.lats[row, col]\n\n    def get_lonlats(self, data_slice=None, chunks=None, **kwargs):\n        """"""Get longitude and latitude arrays representing this geometry.\n\n        Returns\n        -------\n        (lon, lat) : tuple of numpy arrays\n            If `chunks` is provided then the arrays will be dask arrays\n            with the provided chunk size. If `chunks` is not provided then\n            the returned arrays are the same as the internal data types\n            of this geometry object (numpy or dask).\n\n        """"""\n        lons = self.lons\n        lats = self.lats\n        if lons is None or lats is None:\n            raise ValueError(\'lon/lat values are not defined\')\n        elif DataArray is not np.ndarray and isinstance(lons, DataArray):\n            # lons/lats are xarray DataArray objects, use numpy/dask array underneath\n            lons = lons.data\n            lats = lats.data\n\n        if chunks is not None:\n            import dask.array as da\n            if isinstance(lons, da.Array):\n                # rechunk to this specific chunk size\n                lons = lons.rechunk(chunks)\n                lats = lats.rechunk(chunks)\n            elif not isinstance(lons, da.Array):\n                # convert numpy array to dask array\n                lons = da.from_array(np.asanyarray(lons), chunks=chunks)\n                lats = da.from_array(np.asanyarray(lats), chunks=chunks)\n        if data_slice is not None:\n            lons, lats = lons[data_slice], lats[data_slice]\n        return lons, lats\n\n    def get_lonlats_dask(self, chunks=None):\n        """"""Get the lon lats as a single dask array.""""""\n        warnings.warn(""\'get_lonlats_dask\' is deprecated, please use ""\n                      ""\'get_lonlats\' with the \'chunks\' keyword argument specified."", DeprecationWarning)\n        if chunks is None:\n            chunks = CHUNK_SIZE  # FUTURE: Use a global config object instead\n        return self.get_lonlats(chunks=chunks)\n\n    def get_boundary_lonlats(self):\n        """"""Return Boundary objects.""""""\n        s1_lon, s1_lat = self.get_lonlats(data_slice=(0, slice(None)))\n        s2_lon, s2_lat = self.get_lonlats(data_slice=(slice(None), -1))\n        s3_lon, s3_lat = self.get_lonlats(data_slice=(-1, slice(None, None, -1)))\n        s4_lon, s4_lat = self.get_lonlats(data_slice=(slice(None, None, -1), 0))\n        return (SimpleBoundary(s1_lon.squeeze(), s2_lon.squeeze(), s3_lon.squeeze(), s4_lon.squeeze()),\n                SimpleBoundary(s1_lat.squeeze(), s2_lat.squeeze(), s3_lat.squeeze(), s4_lat.squeeze()))\n\n    def get_bbox_lonlats(self):\n        """"""Return the bounding box lons and lats.""""""\n        s1_lon, s1_lat = self.get_lonlats(data_slice=(0, slice(None)))\n        s2_lon, s2_lat = self.get_lonlats(data_slice=(slice(None), -1))\n        s3_lon, s3_lat = self.get_lonlats(data_slice=(-1, slice(None, None, -1)))\n        s4_lon, s4_lat = self.get_lonlats(data_slice=(slice(None, None, -1), 0))\n        return zip(*[(s1_lon.squeeze(), s1_lat.squeeze()),\n                     (s2_lon.squeeze(), s2_lat.squeeze()),\n                     (s3_lon.squeeze(), s3_lat.squeeze()),\n                     (s4_lon.squeeze(), s4_lat.squeeze())])\n\n    def get_cartesian_coords(self, nprocs=None, data_slice=None, cache=False):\n        """"""Retrieve cartesian coordinates of geometry definition.\n\n        Parameters\n        ----------\n        nprocs : int, optional\n            Number of processor cores to be used.\n            Defaults to the nprocs set when instantiating object\n        data_slice : slice object, optional\n            Calculate only cartesian coordnates for the defined slice\n        cache : bool, optional\n            Store result the result. Requires data_slice to be None\n\n        Returns\n        -------\n        cartesian_coords : numpy array\n\n        """"""\n        if cache:\n            warnings.warn(""\'cache\' keyword argument will be removed in the ""\n                          ""future and data will not be cached."", PendingDeprecationWarning)\n\n        if self.cartesian_coords is None:\n            # Coordinates are not cached\n            if nprocs is None:\n                nprocs = self.nprocs\n\n            if data_slice is None:\n                # Use full slice\n                data_slice = slice(None)\n\n            lons, lats = self.get_lonlats(nprocs=nprocs, data_slice=data_slice)\n\n            if nprocs > 1:\n                cartesian = Cartesian_MP(nprocs)\n            else:\n                cartesian = Cartesian()\n\n            cartesian_coords = cartesian.transform_lonlats(np.ravel(lons), np.ravel(lats))\n            if isinstance(lons, np.ndarray) and lons.ndim > 1:\n                # Reshape to correct shape\n                cartesian_coords = cartesian_coords.reshape(lons.shape[0], lons.shape[1], 3)\n\n            if cache and data_slice is None:\n                self.cartesian_coords = cartesian_coords\n        else:\n            # Coordinates are cached\n            if data_slice is None:\n                cartesian_coords = self.cartesian_coords\n            else:\n                cartesian_coords = self.cartesian_coords[data_slice]\n\n        return cartesian_coords\n\n    @property\n    def corners(self):\n        """"""Return the corners of the current area.""""""\n        from pyresample.spherical_geometry import Coordinate\n        return [Coordinate(*self.get_lonlat(0, 0)),\n                Coordinate(*self.get_lonlat(0, -1)),\n                Coordinate(*self.get_lonlat(-1, -1)),\n                Coordinate(*self.get_lonlat(-1, 0))]\n\n    def __contains__(self, point):\n        """"""Check if a point is inside the 4 corners of the current area.\n\n        This uses great circle arcs as area boundaries.\n\n        """"""\n        from pyresample.spherical_geometry import point_inside, Coordinate\n        corners = self.corners\n\n        if isinstance(point, tuple):\n            return point_inside(Coordinate(*point), corners)\n        else:\n            return point_inside(point, corners)\n\n    def overlaps(self, other):\n        """"""Test if the current area overlaps the *other* area.\n\n        This is based solely on the corners of areas, assuming the\n        boundaries to be great circles.\n\n        Parameters\n        ----------\n        other : object\n            Instance of subclass of BaseDefinition\n\n        Returns\n        -------\n        overlaps : bool\n\n        """"""\n        from pyresample.spherical_geometry import Arc\n\n        self_corners = self.corners\n\n        other_corners = other.corners\n\n        for i in self_corners:\n            if i in other:\n                return True\n        for i in other_corners:\n            if i in self:\n                return True\n\n        self_arc1 = Arc(self_corners[0], self_corners[1])\n        self_arc2 = Arc(self_corners[1], self_corners[2])\n        self_arc3 = Arc(self_corners[2], self_corners[3])\n        self_arc4 = Arc(self_corners[3], self_corners[0])\n\n        other_arc1 = Arc(other_corners[0], other_corners[1])\n        other_arc2 = Arc(other_corners[1], other_corners[2])\n        other_arc3 = Arc(other_corners[2], other_corners[3])\n        other_arc4 = Arc(other_corners[3], other_corners[0])\n\n        for i in (self_arc1, self_arc2, self_arc3, self_arc4):\n            for j in (other_arc1, other_arc2, other_arc3, other_arc4):\n                if i.intersects(j):\n                    return True\n        return False\n\n    def get_area(self):\n        """"""Get the area of the convex area defined by the corners of the curren area.""""""\n        from pyresample.spherical_geometry import get_polygon_area\n\n        return get_polygon_area(self.corners)\n\n    def intersection(self, other):\n        """"""Return the corners of the intersection polygon of the current area with *other*.\n\n        Parameters\n        ----------\n        other : object\n            Instance of subclass of BaseDefinition\n\n        Returns\n        -------\n        (corner1, corner2, corner3, corner4) : tuple of points\n\n        """"""\n        from pyresample.spherical_geometry import intersection_polygon\n        return intersection_polygon(self.corners, other.corners)\n\n    def overlap_rate(self, other):\n        """"""Get how much the current area overlaps an *other* area.\n\n        Parameters\n        ----------\n        other : object\n            Instance of subclass of BaseDefinition\n\n        Returns\n        -------\n        overlap_rate : float\n\n        """"""\n        from pyresample.spherical_geometry import get_polygon_area\n        other_area = other.get_area()\n        inter_area = get_polygon_area(self.intersection(other))\n        return inter_area / other_area\n\n    def get_area_slices(self, area_to_cover):\n        """"""Compute the slice to read based on an `area_to_cover`.""""""\n        raise NotImplementedError\n\n\nclass CoordinateDefinition(BaseDefinition):\n    """"""Base class for geometry definitions defined by lons and lats only.""""""\n\n    def __init__(self, lons, lats, nprocs=1):\n        """"""Initialize CoordinateDefinition.""""""\n        if not isinstance(lons, (np.ndarray, DataArray)):\n            lons = np.asanyarray(lons)\n            lats = np.asanyarray(lats)\n        super(CoordinateDefinition, self).__init__(lons, lats, nprocs)\n        if lons.shape == lats.shape and lons.dtype == lats.dtype:\n            self.shape = lons.shape\n            self.size = lons.size\n            self.ndim = lons.ndim\n            self.dtype = lons.dtype\n        else:\n            raise ValueError((\'%s must be created with either \'\n                              \'lon/lats of the same shape with same dtype\') %\n                             self.__class__.__name__)\n\n    def concatenate(self, other):\n        """"""Concatenate coordinate definitions.""""""\n        if self.ndim != other.ndim:\n            raise DimensionError((\'Unable to concatenate %sD and %sD \'\n                                  \'geometries\') % (self.ndim, other.ndim))\n        klass = _get_highest_level_class(self, other)\n        lons = np.concatenate((self.lons, other.lons))\n        lats = np.concatenate((self.lats, other.lats))\n        nprocs = min(self.nprocs, other.nprocs)\n        return klass(lons, lats, nprocs=nprocs)\n\n    def append(self, other):\n        """"""Append another coordinate definition to existing one.""""""\n        if self.ndim != other.ndim:\n            raise DimensionError((\'Unable to append %sD and %sD \'\n                                  \'geometries\') % (self.ndim, other.ndim))\n        self.lons = np.concatenate((self.lons, other.lons))\n        self.lats = np.concatenate((self.lats, other.lats))\n        self.shape = self.lons.shape\n        self.size = self.lons.size\n\n    def __str__(self):\n        """"""Return string representation of the coordinate definition.""""""\n        # Rely on numpy\'s object printing\n        return (\'Shape: %s\\nLons: %s\\nLats: %s\') % (str(self.shape),\n                                                    str(self.lons),\n                                                    str(self.lats))\n\n    def geocentric_resolution(self, ellps=\'WGS84\', radius=None, nadir_factor=2):\n        """"""Calculate maximum geocentric pixel resolution.\n\n        If `lons` is a :class:`xarray.DataArray` object with a `resolution`\n        attribute, this will be used instead of loading the longitude and\n        latitude data. In this case the resolution attribute is assumed to\n        mean the nadir resolution of a swath and will be multiplied by the\n        `nadir_factor` to adjust for increases in the spatial resolution\n        towards the limb of the swath.\n\n        Args:\n            ellps (str): PROJ Ellipsoid for the Cartographic projection\n                used as the target geocentric coordinate reference system.\n                Default: \'WGS84\'. Ignored if `radius` is provided.\n            radius (float): Spherical radius of the Earth to use instead of\n                the definitions in `ellps`.\n            nadir_factor (int): Number to multiply the nadir resolution\n                attribute by to reflect pixel size on the limb of the swath.\n\n        Returns: Estimated maximum pixel size in meters on a geocentric\n            coordinate system (X, Y, Z) representing the Earth.\n\n        Raises: RuntimeError if a simple search for valid longitude/latitude\n            data points found no valid data points.\n\n        """"""\n        if hasattr(self.lons, \'attrs\') and \\\n                self.lons.attrs.get(\'resolution\') is not None:\n            return self.lons.attrs[\'resolution\'] * nadir_factor\n        if self.ndim == 1:\n            raise RuntimeError(""Can\'t confidently determine geocentric ""\n                               ""resolution for 1D swath."")\n        from pyproj import transform\n        rows = self.shape[0]\n        start_row = rows // 2  # middle row\n        src = Proj(\'+proj=latlong +datum=WGS84\')\n        if radius:\n            dst = Proj(""+proj=cart +a={} +b={}"".format(radius, radius))\n        else:\n            dst = Proj(""+proj=cart +ellps={}"".format(ellps))\n        # simply take the first two columns of the middle of the swath\n        lons = self.lons[start_row: start_row + 1, :2]\n        lats = self.lats[start_row: start_row + 1, :2]\n        if hasattr(lons.data, \'compute\'):\n            # dask arrays, compute them together\n            import dask.array as da\n            lons, lats = da.compute(lons, lats)\n        if hasattr(lons, \'values\'):\n            # convert xarray to numpy array\n            lons = lons.values\n            lats = lats.values\n        lons = lons.ravel()\n        lats = lats.ravel()\n        alt = np.zeros_like(lons)\n\n        xyz = np.stack(transform(src, dst, lons, lats, alt), axis=1)\n        dist = np.linalg.norm(xyz[1] - xyz[0])\n        dist = dist[np.isfinite(dist)]\n        if not dist.size:\n            raise RuntimeError(""Could not calculate geocentric resolution"")\n        return dist[0]\n\n\nclass GridDefinition(CoordinateDefinition):\n    """"""Grid defined by lons and lats.\n\n    Parameters\n    ----------\n    lons : numpy array\n    lats : numpy array\n    nprocs : int, optional\n        Number of processor cores to be used for calculations.\n\n    Attributes\n    ----------\n    shape : tuple\n        Grid shape as (rows, cols)\n    size : int\n        Number of elements in grid\n    lons : object\n        Grid lons\n    lats : object\n        Grid lats\n    cartesian_coords : object\n        Grid cartesian coordinates\n\n    """"""\n\n    def __init__(self, lons, lats, nprocs=1):\n        """"""Initialize GridDefinition.""""""\n        super(GridDefinition, self).__init__(lons, lats, nprocs)\n        if lons.shape != lats.shape:\n            raise ValueError(\'lon and lat grid must have same shape\')\n        elif lons.ndim != 2:\n            raise ValueError(\'2 dimensional lon lat grid expected\')\n\n\ndef get_array_hashable(arr):\n    """"""Compute a hashable form of the array `arr`.\n\n    Works with numpy arrays, dask.array.Array, and xarray.DataArray.\n    """"""\n    # look for precomputed value\n    if isinstance(arr, DataArray) and np.ndarray is not DataArray:\n        return arr.attrs.get(\'hash\', get_array_hashable(arr.data))\n    else:\n        try:\n            return arr.name.encode(\'utf-8\')  # dask array\n        except AttributeError:\n            return np.asarray(arr).view(np.uint8)  # np array\n\n\nclass SwathDefinition(CoordinateDefinition):\n    """"""Swath defined by lons and lats.\n\n    Parameters\n    ----------\n    lons : numpy array\n    lats : numpy array\n    nprocs : int, optional\n        Number of processor cores to be used for calculations.\n\n    Attributes\n    ----------\n    shape : tuple\n        Swath shape\n    size : int\n        Number of elements in swath\n    ndims : int\n        Swath dimensions\n    lons : object\n        Swath lons\n    lats : object\n        Swath lats\n    cartesian_coords : object\n        Swath cartesian coordinates\n\n    """"""\n\n    def __init__(self, lons, lats, nprocs=1):\n        """"""Initialize SwathDefinition.""""""\n        if not isinstance(lons, (np.ndarray, DataArray)):\n            lons = np.asanyarray(lons)\n            lats = np.asanyarray(lats)\n        super(SwathDefinition, self).__init__(lons, lats, nprocs)\n        if lons.shape != lats.shape:\n            raise ValueError(\'lon and lat arrays must have same shape\')\n        elif lons.ndim > 2:\n            raise ValueError(\'Only 1 and 2 dimensional swaths are allowed\')\n\n    def copy(self):\n        """"""Copy the current swath.""""""\n        return SwathDefinition(self.lons, self.lats)\n\n    @staticmethod\n    def _do_transform(src, dst, lons, lats, alt):\n        """"""Run pyproj.transform and stack the results.""""""\n        x, y, z = transform(src, dst, lons, lats, alt)\n        return np.dstack((x, y, z))\n\n    def aggregate(self, **dims):\n        """"""Aggregate the current swath definition by averaging.\n\n        For example, averaging over 2x2 windows:\n        `sd.aggregate(x=2, y=2)`\n        """"""\n        import pyproj\n        import dask.array as da\n\n        geocent = pyproj.Proj(proj=\'geocent\')\n        latlong = pyproj.Proj(proj=\'latlong\')\n        res = da.map_blocks(self._do_transform, latlong, geocent,\n                            self.lons.data, self.lats.data,\n                            da.zeros_like(self.lons.data), new_axis=[2],\n                            chunks=(self.lons.chunks[0], self.lons.chunks[1], 3))\n        res = DataArray(res, dims=[\'y\', \'x\', \'coord\'], coords=self.lons.coords)\n        res = res.coarsen(**dims).mean()\n        lonlatalt = da.map_blocks(self._do_transform, geocent, latlong,\n                                  res[:, :, 0].data, res[:, :, 1].data,\n                                  res[:, :, 2].data, new_axis=[2],\n                                  chunks=res.data.chunks)\n        lons = DataArray(lonlatalt[:, :, 0], dims=self.lons.dims,\n                         coords=res.coords, attrs=self.lons.attrs.copy())\n        lats = DataArray(lonlatalt[:, :, 1], dims=self.lons.dims,\n                         coords=res.coords, attrs=self.lons.attrs.copy())\n        try:\n            resolution = lons.attrs[\'resolution\'] * ((dims.get(\'x\', 1) + dims.get(\'y\', 1)) / 2)\n            lons.attrs[\'resolution\'] = resolution\n            lats.attrs[\'resolution\'] = resolution\n        except KeyError:\n            pass\n        return SwathDefinition(lons, lats)\n\n    def __hash__(self):\n        """"""Compute the hash of this object.""""""\n        if self.hash is None:\n            self.hash = int(self.update_hash().hexdigest(), 16)\n        return self.hash\n\n    def update_hash(self, the_hash=None):\n        """"""Update the hash.""""""\n        if the_hash is None:\n            the_hash = hashlib.sha1()\n        the_hash.update(get_array_hashable(self.lons))\n        the_hash.update(get_array_hashable(self.lats))\n        try:\n            if self.lons.mask is not np.bool_(False):\n                the_hash.update(get_array_hashable(self.lons.mask))\n        except AttributeError:\n            pass\n        return the_hash\n\n    def _compute_omerc_parameters(self, ellipsoid):\n        """"""Compute the oblique mercator projection bouding box parameters.""""""\n        lines, cols = self.lons.shape\n        lon1, lon2 = np.asanyarray(self.lons[[0, -1], int(cols / 2)])\n        lat1, lat, lat2 = np.asanyarray(\n            self.lats[[0, int(lines / 2), -1], int(cols / 2)])\n        if any(np.isnan((lon1, lon2, lat1, lat, lat2))):\n            thelons = self.lons[:, int(cols / 2)]\n            thelons = thelons.where(thelons.notnull(), drop=True)\n            thelats = self.lats[:, int(cols / 2)]\n            thelats = thelats.where(thelats.notnull(), drop=True)\n            lon1, lon2 = np.asanyarray(thelons[[0, -1]])\n            lines = len(thelats)\n            lat1, lat, lat2 = np.asanyarray(thelats[[0, int(lines / 2), -1]])\n\n        proj_dict2points = {\'proj\': \'omerc\', \'lat_0\': lat, \'ellps\': ellipsoid,\n                            \'lat_1\': lat1, \'lon_1\': lon1,\n                            \'lat_2\': lat2, \'lon_2\': lon2,\n                            \'no_rot\': True\n                            }\n\n        # We need to compute alpha-based omerc for geotiff support\n        lonc, lat0 = Proj(**proj_dict2points)(0, 0, inverse=True)\n        az1, az2, _ = Geod(**proj_dict2points).inv(lonc, lat0, lon2, lat2)\n        azimuth = az1\n        az1, az2, _ = Geod(**proj_dict2points).inv(lonc, lat0, lon1, lat1)\n        if abs(az1 - azimuth) > 1:\n            if abs(az2 - azimuth) > 1:\n                logger.warning(""Can\'t find appropriate azimuth."")\n            else:\n                azimuth += az2\n                azimuth /= 2\n        else:\n            azimuth += az1\n            azimuth /= 2\n        if abs(azimuth) > 90:\n            azimuth = 180 + azimuth\n\n        prj_params = {\'proj\': \'omerc\', \'alpha\': float(azimuth), \'lat_0\': float(lat0), \'lonc\': float(lonc),\n                      \'gamma\': 0,\n                      \'ellps\': ellipsoid}\n\n        return prj_params\n\n    def _compute_generic_parameters(self, projection, ellipsoid):\n        """"""Compute the projection bb parameters for most projections.""""""\n        lines, cols = self.lons.shape\n        lat_0 = self.lats[int(lines / 2), int(cols / 2)]\n        lon_0 = self.lons[int(lines / 2), int(cols / 2)]\n        return {\'proj\': projection, \'ellps\': ellipsoid,\n                \'lat_0\': lat_0, \'lon_0\': lon_0}\n\n    def get_edge_lonlats(self):\n        """"""Get the concatenated boundary of the current swath.""""""\n        lons, lats = self.get_bbox_lonlats()\n        blons = np.ma.concatenate(lons)\n        blats = np.ma.concatenate(lats)\n        return blons, blats\n\n    def compute_bb_proj_params(self, proj_dict):\n        """"""Compute BB projection parameters.""""""\n        projection = proj_dict[\'proj\']\n        if projection == \'omerc\':\n            ellipsoid = proj_dict.get(\'ellps\', \'sphere\')\n            return self._compute_omerc_parameters(ellipsoid)\n        else:\n            ellipsoid = proj_dict.get(\'ellps\', \'WGS84\')\n            new_proj = self._compute_generic_parameters(projection, ellipsoid)\n            new_proj.update(proj_dict)\n            return new_proj\n\n    def _compute_uniform_shape(self):\n        """"""Compute the height and width of a domain to have uniform resolution across dimensions.""""""\n        g = Geod(ellps=\'WGS84\')\n\n        def notnull(arr):\n            try:\n                return arr.where(arr.notnull(), drop=True)\n            except AttributeError:\n                return arr[np.isfinite(arr)]\n        leftlons = self.lons[:, 0]\n        rightlons = self.lons[:, -1]\n        middlelons = self.lons[:, int(self.lons.shape[1] / 2)]\n        leftlats = self.lats[:, 0]\n        rightlats = self.lats[:, -1]\n        middlelats = self.lats[:, int(self.lats.shape[1] / 2)]\n        try:\n            import dask.array as da\n        except ImportError:\n            pass\n        else:\n            leftlons, rightlons, middlelons, leftlats, rightlats, middlelats = da.compute(leftlons, rightlons,\n                                                                                          middlelons, leftlats,\n                                                                                          rightlats, middlelats)\n        leftlons = notnull(leftlons)\n        rightlons = notnull(rightlons)\n        middlelons = notnull(middlelons)\n        leftlats = notnull(leftlats)\n        rightlats = notnull(rightlats)\n        middlelats = notnull(middlelats)\n\n        az1, az2, width1 = g.inv(leftlons[0], leftlats[0], rightlons[0], rightlats[0])\n        az1, az2, width2 = g.inv(leftlons[-1], leftlats[-1], rightlons[-1], rightlats[-1])\n        az1, az2, height = g.inv(middlelons[0], middlelats[0], middlelons[-1], middlelats[-1])\n        width = min(width1, width2)\n        vresolution = height * 1.0 / self.lons.shape[0]\n        hresolution = width * 1.0 / self.lons.shape[1]\n        resolution = min(vresolution, hresolution)\n        width = int(width * 1.1 / resolution)\n        height = int(height * 1.1 / resolution)\n        return height, width\n\n    def compute_optimal_bb_area(self, proj_dict=None):\n        """"""Compute the ""best"" bounding box area for this swath with `proj_dict`.\n\n        By default, the projection is Oblique Mercator (`omerc` in proj.4), in\n        which case the right projection angle `alpha` is computed from the\n        swath centerline. For other projections, only the appropriate center of\n        projection and area extents are computed.\n\n        The height and width are computed so that the resolution is\n        approximately the same across dimensions.\n        """"""\n        if proj_dict is None:\n            proj_dict = {}\n        projection = proj_dict.setdefault(\'proj\', \'omerc\')\n        area_id = projection + \'_otf\'\n        description = \'On-the-fly \' + projection + \' area\'\n        height, width = self._compute_uniform_shape()\n        proj_dict = self.compute_bb_proj_params(proj_dict)\n\n        area = DynamicAreaDefinition(area_id, description, proj_dict)\n        lons, lats = self.get_edge_lonlats()\n        return area.freeze((lons, lats), shape=(height, width))\n\n\nclass DynamicAreaDefinition(object):\n    """"""An AreaDefintion containing just a subset of the needed parameters.\n\n    The purpose of this class is to be able to adapt the area extent and shape\n    of the area to a given set of longitudes and latitudes, such that e.g.\n    polar satellite granules can be resampled optimally to a given projection.\n\n    Parameters\n    ----------\n    area_id:\n        The name of the area.\n    description:\n        The description of the area.\n    projection:\n        The dictionary or string of projection parameters. Doesn\'t have to\n        be complete. If not complete, ``proj_info`` must be provided to\n        ``freeze`` to ""fill in"" any missing parameters.\n    width:\n        x dimension in number of pixels, aka number of grid columns\n    height:\n        y dimension in number of pixels, aka number of grid rows\n    shape:\n        Corresponding array shape as (height, width)\n    area_extent:\n        The area extent of the area.\n    pixel_size_x:\n        Pixel width in projection units\n    pixel_size_y:\n        Pixel height in projection units\n    resolution:\n        Resolution of the resulting area as (pixel_size_x, pixel_size_y) or a scalar if pixel_size_x == pixel_size_y.\n    optimize_projection:\n        Whether the projection parameters have to be optimized.\n    rotation:\n        Rotation in degrees (negative is cw)\n\n    """"""\n\n    def __init__(self, area_id=None, description=None, projection=None,\n                 width=None, height=None, area_extent=None,\n                 resolution=None, optimize_projection=False, rotation=None):\n        """"""Initialize the DynamicAreaDefinition.""""""\n        self.area_id = area_id\n        self.description = description\n        self.width = width\n        self.height = height\n        self.shape = (self.height, self.width)\n        self.area_extent = area_extent\n        self.optimize_projection = optimize_projection\n        if isinstance(resolution, (int, float)):\n            resolution = (resolution, resolution)\n        self.resolution = resolution\n        self.rotation = rotation\n        self._projection = projection\n\n        # check if non-dict projections are valid\n        # dicts may be updated later\n        if not isinstance(self._projection, dict):\n            Proj(projection)\n\n    def _get_proj_dict(self):\n        projection = self._projection\n        if CRS is not None:\n            try:\n                crs = CRS(projection)\n            except RuntimeError:\n                # could be incomplete dictionary\n                return projection\n            if hasattr(crs, \'to_dict\'):\n                # pyproj 2.2+\n                proj_dict = crs.to_dict()\n            else:\n                proj_dict = proj4_str_to_dict(crs.to_proj4())\n        else:\n            if isinstance(projection, str):\n                proj_dict = proj4_str_to_dict(projection)\n            elif isinstance(projection, dict):\n                proj_dict = projection.copy()\n            else:\n                raise TypeError(\'Wrong type for projection: {0}. Expected \'\n                                \'dict or string.\'.format(type(projection)))\n\n        return proj_dict\n\n    @property\n    def pixel_size_x(self):\n        """"""Return pixel size in X direction.""""""\n        if self.resolution is None:\n            return None\n        return self.resolution[0]\n\n    @property\n    def pixel_size_y(self):\n        """"""Return pixel size in Y direction.""""""\n        if self.resolution is None:\n            return None\n        return self.resolution[1]\n\n    def compute_domain(self, corners, resolution=None, shape=None):\n        """"""Compute shape and area_extent from corners and [shape or resolution] info.\n\n        Corners represents the center of pixels, while area_extent represents the edge of pixels.\n\n        Note that ``shape`` is (rows, columns) and ``resolution`` is\n        (x_size, y_size); the dimensions are flipped.\n\n        """"""\n        if resolution is not None and shape is not None:\n            raise ValueError(""Both resolution and shape can\'t be provided."")\n        elif resolution is None and shape is None:\n            raise ValueError(""Either resolution or shape must be provided."")\n\n        if shape:\n            height, width = shape\n            x_resolution = (corners[2] - corners[0]) * 1.0 / (width - 1)\n            y_resolution = (corners[3] - corners[1]) * 1.0 / (height - 1)\n        else:\n            if isinstance(resolution, (int, float)):\n                resolution = (resolution, resolution)\n            x_resolution, y_resolution = resolution\n            width = int(np.rint((corners[2] - corners[0]) * 1.0\n                                / x_resolution + 1))\n            height = int(np.rint((corners[3] - corners[1]) * 1.0\n                                 / y_resolution + 1))\n\n        area_extent = (corners[0] - x_resolution / 2,\n                       corners[1] - y_resolution / 2,\n                       corners[2] + x_resolution / 2,\n                       corners[3] + y_resolution / 2)\n        return area_extent, width, height\n\n    def freeze(self, lonslats=None, resolution=None, shape=None, proj_info=None):\n        """"""Create an AreaDefinition from this area with help of some extra info.\n\n        Parameters\n        ----------\n        lonlats : SwathDefinition or tuple\n          The geographical coordinates to contain in the resulting area.\n          A tuple should be ``(lons, lats)``.\n        resolution:\n          the resolution of the resulting area.\n        shape:\n          the shape of the resulting area.\n        proj_info:\n          complementing parameters to the projection info.\n\n        Resolution and shape parameters are ignored if the instance is created\n        with the `optimize_projection` flag set to True.\n\n        """"""\n        proj_dict = self._get_proj_dict()\n        projection = self._projection\n        if proj_info is not None:\n            # this is now our complete projection information\n            proj_dict.update(proj_info)\n            projection = proj_dict\n\n        if self.optimize_projection:\n            return lonslats.compute_optimal_bb_area(proj_dict)\n        if resolution is None:\n            resolution = self.resolution\n        if shape is None:\n            shape = self.shape\n        height, width = shape\n        shape = None if None in shape else shape\n        area_extent = self.area_extent\n        if not area_extent or not width or not height:\n            proj4 = Proj(proj_dict)\n            try:\n                lons, lats = lonslats\n            except (TypeError, ValueError):\n                lons, lats = lonslats.get_lonlats()\n            xarr, yarr = proj4(np.asarray(lons), np.asarray(lats))\n            xarr[xarr > 9e29] = np.nan\n            yarr[yarr > 9e29] = np.nan\n            corners = [np.nanmin(xarr), np.nanmin(yarr),\n                       np.nanmax(xarr), np.nanmax(yarr)]\n            area_extent, width, height = self.compute_domain(corners, resolution, shape)\n        return AreaDefinition(self.area_id, self.description, \'\',\n                              projection, width, height,\n                              area_extent, self.rotation)\n\n\ndef invproj(data_x, data_y, proj_dict):\n    """"""Perform inverse projection.""""""\n    # XXX: does pyproj copy arrays? What can we do so it doesn\'t?\n    target_proj = Proj(proj_dict)\n    return np.dstack(target_proj(data_x, data_y, inverse=True))\n\n\nclass AreaDefinition(BaseDefinition):\n    """"""Holds definition of an area.\n\n    Parameters\n    ----------\n    area_id : str\n        Identifier for the area\n    description : str\n        Human-readable description of the area\n    proj_id : str\n        ID of projection\n    projection: dict or str or pyproj.crs.CRS\n        Dictionary of PROJ parameters or string of PROJ or WKT parameters.\n        Can also be a :class:`pyproj.crs.CRS` object.\n    width : int\n        x dimension in number of pixels, aka number of grid columns\n    height : int\n        y dimension in number of pixels, aka number of grid rows\n    area_extent : list\n        Area extent as a list (lower_left_x, lower_left_y, upper_right_x, upper_right_y)\n    rotation: float, optional\n        rotation in degrees (negative is clockwise)\n    nprocs : int, optional\n        Number of processor cores to be used for certain calculations\n\n    Attributes\n    ----------\n    area_id : str\n        Identifier for the area\n    description : str\n        Human-readable description of the area\n    proj_id : str\n        ID of projection\n    projection : dict or str\n        Dictionary or string with Proj.4 parameters\n    width : int\n        x dimension in number of pixels, aka number of grid columns\n    height : int\n        y dimension in number of pixels, aka number of grid rows\n    rotation: float\n        rotation in degrees (negative is cw)\n    size : int\n        Number of points in grid\n    area_extent : tuple\n        Area extent as a tuple (lower_left_x, lower_left_y, upper_right_x, upper_right_y)\n    area_extent_ll : tuple\n        Area extent in lons lats as a tuple (lower_left_lon, lower_left_lat, upper_right_lon, upper_right_lat)\n    pixel_size_x : float\n        Pixel width in projection units\n    pixel_size_y : float\n        Pixel height in projection units\n    upper_left_extent : tuple\n        Coordinates (x, y) of upper left corner of upper left pixel in projection units\n    pixel_upper_left : tuple\n        Coordinates (x, y) of center of upper left pixel in projection units\n    pixel_offset_x : float\n        x offset between projection center and upper left corner of upper\n        left pixel in units of pixels.\n    pixel_offset_y : float\n        y offset between projection center and upper left corner of upper\n        left pixel in units of pixels..\n    crs : pyproj.crs.CRS\n        Coordinate reference system object similar to the PROJ parameters in\n        `proj_dict` and `proj_str`. This is the preferred attribute to use\n        when working with the `pyproj` library. Note, however, that this\n        object is not thread-safe and should not be passed between threads.\n    crs_wkt : str\n        WellKnownText version of the CRS object. This is the preferred\n        way of describing CRS information as a string.\n    cartesian_coords : object\n        Grid cartesian coordinates\n\n    """"""\n\n    def __init__(self, area_id, description, proj_id, projection, width, height,\n                 area_extent, rotation=None, nprocs=1, lons=None, lats=None,\n                 dtype=np.float64):\n        """"""Initialize AreaDefinition.""""""\n        super(AreaDefinition, self).__init__(lons, lats, nprocs)\n        self.area_id = area_id\n        self.description = description\n        self.proj_id = proj_id\n        self.width = int(width)\n        self.height = int(height)\n        self.crop_offset = (0, 0)\n        try:\n            self.rotation = float(rotation)\n        except TypeError:\n            self.rotation = 0\n        if lons is not None:\n            if lons.shape != self.shape:\n                raise ValueError(\'Shape of lon lat grid must match \'\n                                 \'area definition\')\n        self.size = height * width\n        self.ndim = 2\n        self.pixel_size_x = (area_extent[2] - area_extent[0]) / float(width)\n        self.pixel_size_y = (area_extent[3] - area_extent[1]) / float(height)\n        self.area_extent = tuple(area_extent)\n        if CRS is not None:\n            self.crs_wkt = CRS(projection).to_wkt()\n            self._proj_dict = None\n            self.crs = self._crs  # see _crs property for details\n        else:\n            if isinstance(projection, str):\n                proj_dict = proj4_str_to_dict(projection)\n            elif isinstance(projection, dict):\n                # use the float-converted dict to pass to Proj\n                projection = convert_proj_floats(projection.items())\n                proj_dict = projection\n            else:\n                raise TypeError(\'Wrong type for projection: {0}. Expected dict or string.\'.format(type(projection)))\n            self._proj_dict = proj_dict\n\n        # Calculate area_extent in lon lat\n        proj = Proj(projection)\n        corner_lons, corner_lats = proj((area_extent[0], area_extent[2]),\n                                        (area_extent[1], area_extent[3]),\n                                        inverse=True)\n        self.area_extent_ll = (corner_lons[0], corner_lats[0],\n                               corner_lons[1], corner_lats[1])\n\n        # Calculate projection coordinates of extent of upper left pixel\n        self.upper_left_extent = (float(area_extent[0]), float(area_extent[3]))\n        self.pixel_upper_left = (float(area_extent[0]) + float(self.pixel_size_x) / 2,\n                                 float(area_extent[3]) - float(self.pixel_size_y) / 2)\n\n        # Pixel_offset defines the distance to projection center from origin\n        # (UL) of image in units of pixels.\n        self.pixel_offset_x = -self.area_extent[0] / self.pixel_size_x\n        self.pixel_offset_y = self.area_extent[3] / self.pixel_size_y\n\n        self._projection_x_coords = None\n        self._projection_y_coords = None\n\n        self.dtype = dtype\n\n    @property\n    def _crs(self):\n        """"""Wrap the `crs` property in a helper property.\n\n        The :class:`pyproj.crs.CRS` object is not thread-safe. To avoid\n        accidentally passing it between threads, we only create it when it\n        is requested (the `self.crs` property). The alternative of storing it\n        as a normal instance attribute could cause issues between threads.\n\n        For backwards compatibility, we only create the `.crs` property if\n        pyproj 2.0+ is installed. Users can then check\n        `hasattr(area_def, \'crs\')` to easily support older versions of\n        pyresample and pyproj.\n\n        """"""\n        return CRS.from_wkt(self.crs_wkt)\n\n    @property\n    def proj_dict(self):\n        """"""Return the PROJ projection dictionary.\n\n        This is no longer the preferred way of describing CRS information.\n        Switch to the `crs` or `crs_wkt` properties for the most flexibility.\n        """"""\n        if self._proj_dict is None and hasattr(self, \'crs\'):\n            if hasattr(self.crs, \'to_dict\'):\n                # pyproj 2.2+\n                self._proj_dict = self.crs.to_dict()\n            else:\n                self._proj_dict = proj4_str_to_dict(self.crs.to_proj4())\n        return self._proj_dict\n\n    def copy(self, **override_kwargs):\n        """"""Make a copy of the current area.\n\n        This replaces the current values with anything in *override_kwargs*.\n        """"""\n        kwargs = {\'area_id\': self.area_id,\n                  \'description\': self.description,\n                  \'proj_id\': self.proj_id,\n                  \'projection\': self.proj_dict,\n                  \'width\': self.width,\n                  \'height\': self.height,\n                  \'area_extent\': self.area_extent,\n                  \'rotation\': self.rotation}\n        kwargs.update(override_kwargs)\n        return AreaDefinition(**kwargs)\n\n    def aggregate(self, **dims):\n        """"""Return an aggregated version of the area.""""""\n        width = int(self.width / dims.get(\'x\', 1))\n        height = int(self.height / dims.get(\'y\', 1))\n        return self.copy(height=height, width=width)\n\n    @property\n    def shape(self):\n        """"""Return area shape.""""""\n        return self.height, self.width\n\n    @property\n    def resolution(self):\n        """"""Return area resolution in X and Y direction.""""""\n        return self.pixel_size_x, self.pixel_size_y\n\n    @property\n    def name(self):\n        """"""Return area name.""""""\n        warnings.warn(""\'name\' is deprecated, use \'description\' instead."", PendingDeprecationWarning)\n        return self.description\n\n    @property\n    def x_size(self):\n        """"""Return area width.""""""\n        warnings.warn(""\'x_size\' is deprecated, use \'width\' instead."", PendingDeprecationWarning)\n        return self.width\n\n    @property\n    def y_size(self):\n        """"""Return area height.""""""\n        warnings.warn(""\'y_size\' is deprecated, use \'height\' instead."", PendingDeprecationWarning)\n        return self.height\n\n    @classmethod\n    def from_epsg(cls, code, resolution):\n        """"""Create an AreaDefinition object from an epsg code (string or int) and a resolution.""""""\n        if CRS is None:\n            raise NotImplementedError\n        crs = CRS(\'EPSG:\' + str(code))\n        bounds = crs.area_of_use.bounds\n        proj = Proj(crs)\n        left1, low1 = proj(bounds[0], bounds[1])\n        right1, up1 = proj(bounds[2], bounds[3])\n        left2, up2 = proj(bounds[0], bounds[3])\n        right2, low2 = proj(bounds[2], bounds[1])\n        left = min(left1, left2)\n        right = max(right1, right2)\n        up = max(up1, up2)\n        low = min(low1, low2)\n        area_extent = (left, low, right, up)\n        return create_area_def(crs.name, crs.to_dict(), area_extent=area_extent, resolution=resolution)\n\n    @classmethod\n    def from_extent(cls, area_id, projection, shape, area_extent, units=None, **kwargs):\n        """"""Create an AreaDefinition object from area_extent and shape.\n\n        Parameters\n        ----------\n        area_id : str\n            ID of area\n        projection : dict or str\n            Projection parameters as a proj4_dict or proj4_string\n        shape : list\n            Number of pixels in the y and x direction (height, width)\n        area_extent : list\n            Area extent as a list (lower_left_x, lower_left_y, upper_right_x, upper_right_y)\n        units : str, optional\n            Units that provided arguments should be interpreted as. This can be\n            one of \'deg\', \'degrees\', \'meters\', \'metres\', and any\n            parameter supported by the\n            `cs2cs -lu <https://proj4.org/apps/cs2cs.html#cmdoption-cs2cs-lu>`_\n            command. Units are determined in the following priority:\n\n            1. units expressed with each variable through a DataArray\'s attrs attribute.\n            2. units passed to ``units``\n            3. units used in ``projection``\n            4. meters\n\n        description : str, optional\n            Description/name of area. Defaults to area_id\n        proj_id : str, optional\n            ID of projection\n        rotation: float, optional\n            rotation in degrees (negative is cw)\n        nprocs : int, optional\n            Number of processor cores to be used\n        lons : numpy array, optional\n            Grid lons\n        lats : numpy array, optional\n            Grid lats\n\n        Returns\n        -------\n        AreaDefinition : AreaDefinition\n\n        """"""\n        return create_area_def(area_id, projection, shape=shape, area_extent=area_extent, units=units, **kwargs)\n\n    @classmethod\n    def from_circle(cls, area_id, projection, center, radius, shape=None, resolution=None, units=None, **kwargs):\n        """"""Create an AreaDefinition from center, radius, and shape or from center, radius, and resolution.\n\n        Parameters\n        ----------\n        area_id : str\n            ID of area\n        projection : dict or str\n            Projection parameters as a proj4_dict or proj4_string\n        center : list\n            Center of projection (x, y)\n        radius : list or float\n            Length from the center to the edges of the projection (dx, dy)\n        shape : list, optional\n            Number of pixels in the y and x direction (height, width)\n        resolution : list or float, optional\n            Size of pixels: (dx, dy)\n        units : str, optional\n            Units that provided arguments should be interpreted as. This can be\n            one of \'deg\', \'degrees\', \'meters\', \'metres\', and any\n            parameter supported by the\n            `cs2cs -lu <https://proj4.org/apps/cs2cs.html#cmdoption-cs2cs-lu>`_\n            command. Units are determined in the following priority:\n\n            1. units expressed with each variable through a DataArray\'s attrs attribute.\n            2. units passed to ``units``\n            3. units used in ``projection``\n            4. meters\n\n        description : str, optional\n            Description/name of area. Defaults to area_id\n        proj_id : str, optional\n            ID of projection\n        rotation: float, optional\n            rotation in degrees (negative is cw)\n        nprocs : int, optional\n            Number of processor cores to be used\n        lons : numpy array, optional\n            Grid lons\n        lats : numpy array, optional\n            Grid lats\n        optimize_projection:\n            Whether the projection parameters have to be optimized for a DynamicAreaDefinition.\n\n        Returns\n        -------\n        AreaDefinition or DynamicAreaDefinition : AreaDefinition or DynamicAreaDefinition\n            If shape or resolution are provided, an AreaDefinition object is returned.\n            Else a DynamicAreaDefinition object is returned\n\n        Notes\n        -----\n        * ``resolution`` and ``radius`` can be specified with one value if dx == dy\n\n        """"""\n        return create_area_def(area_id, projection, shape=shape, center=center, radius=radius,\n                               resolution=resolution, units=units, **kwargs)\n\n    @classmethod\n    def from_area_of_interest(cls, area_id, projection, shape, center, resolution, units=None, **kwargs):\n        """"""Create an AreaDefinition from center, resolution, and shape.\n\n        Parameters\n        ----------\n        area_id : str\n            ID of area\n        projection : dict or str\n            Projection parameters as a proj4_dict or proj4_string\n        shape : list\n            Number of pixels in the y and x direction (height, width)\n        center : list\n            Center of projection (x, y)\n        resolution : list or float\n            Size of pixels: (dx, dy). Can be specified with one value if dx == dy\n        units : str, optional\n            Units that provided arguments should be interpreted as. This can be\n            one of \'deg\', \'degrees\', \'meters\', \'metres\', and any\n            parameter supported by the\n            `cs2cs -lu <https://proj4.org/apps/cs2cs.html#cmdoption-cs2cs-lu>`_\n            command. Units are determined in the following priority:\n\n            1. units expressed with each variable through a DataArray\'s attrs attribute.\n            2. units passed to ``units``\n            3. units used in ``projection``\n            4. meters\n\n        description : str, optional\n            Description/name of area. Defaults to area_id\n        proj_id : str, optional\n            ID of projection\n        rotation: float, optional\n            rotation in degrees (negative is cw)\n        nprocs : int, optional\n            Number of processor cores to be used\n        lons : numpy array, optional\n            Grid lons\n        lats : numpy array, optional\n            Grid lats\n\n        Returns\n        -------\n        AreaDefinition : AreaDefinition\n\n        """"""\n        return create_area_def(area_id, projection, shape=shape, center=center,\n                               resolution=resolution, units=units, **kwargs)\n\n    @classmethod\n    def from_ul_corner(cls, area_id, projection, shape, upper_left_extent, resolution, units=None, **kwargs):\n        """"""Create an AreaDefinition object from upper_left_extent, resolution, and shape.\n\n        Parameters\n        ----------\n        area_id : str\n            ID of area\n        projection : dict or str\n            Projection parameters as a proj4_dict or proj4_string\n        shape : list\n            Number of pixels in the y and x direction (height, width)\n        upper_left_extent : list\n            Upper left corner of upper left pixel (x, y)\n        resolution : list or float\n            Size of pixels in **meters**: (dx, dy). Can be specified with one value if dx == dy\n        units : str, optional\n            Units that provided arguments should be interpreted as. This can be\n            one of \'deg\', \'degrees\', \'meters\', \'metres\', and any\n            parameter supported by the\n            `cs2cs -lu <https://proj4.org/apps/cs2cs.html#cmdoption-cs2cs-lu>`_\n            command. Units are determined in the following priority:\n\n            1. units expressed with each variable through a DataArray\'s attrs attribute.\n            2. units passed to ``units``\n            3. units used in ``projection``\n            4. meters\n\n        description : str, optional\n            Description/name of area. Defaults to area_id\n        proj_id : str, optional\n            ID of projection\n        rotation: float, optional\n            rotation in degrees (negative is cw)\n        nprocs : int, optional\n            Number of processor cores to be used\n        lons : numpy array, optional\n            Grid lons\n        lats : numpy array, optional\n            Grid lats\n\n        Returns\n        -------\n        AreaDefinition : AreaDefinition\n\n        """"""\n        return create_area_def(area_id, projection, shape=shape, upper_left_extent=upper_left_extent,\n                               resolution=resolution, units=units, **kwargs)\n\n    @classmethod\n    def from_cf(cls, cf_file, variable=None, y=None, x=None):\n        """"""Create an AreaDefinition object from a netCDF/CF file.\n\n        Parameters\n        ----------\n        nc_file : string or object\n            path to a netCDF/CF file, or opened xarray.Dataset object\n        variable : string, optional\n            name of the variable to load the AreaDefinition from\n            If variable is None the file will be searched for valid CF\n            area definitions\n        y : string, optional\n            name of the variable to use as \'y\' axis of the CF area definition\n            If y is None an appropriate \'y\' axis will be deduced from the CF file\n        x : string, optional\n            name of the variable to use as \'x\' axis of the CF area definition\n            If x is None an appropriate \'x\' axis will be deduced from the CF file\n\n        Returns\n        -------\n        AreaDefinition : AreaDefinition\n\n        """"""\n        return load_cf_area(cf_file, variable=variable, y=y, x=x)[0]\n\n    def __hash__(self):\n        """"""Compute the hash of this object.""""""\n        if self.hash is None:\n            self.hash = int(self.update_hash().hexdigest(), 16)\n        return self.hash\n\n    @property\n    def proj_str(self):\n        """"""Return PROJ projection string.\n\n        This is no longer the preferred way of describing CRS information.\n        Switch to the `crs` or `crs_wkt` properties for the most flexibility.\n\n        """"""\n        proj_dict = self.proj_dict.copy()\n        if \'towgs84\' in proj_dict and isinstance(proj_dict[\'towgs84\'], list):\n            # pyproj 2+ creates a list in the dictionary\n            # but the string should be comma-separated\n            if all(x == 0 for x in proj_dict[\'towgs84\']):\n                # all 0s in towgs84 are technically equal to not having them\n                # specified, but PROJ considers them different\n                proj_dict.pop(\'towgs84\')\n            else:\n                proj_dict[\'towgs84\'] = \',\'.join(str(x) for x in proj_dict[\'towgs84\'])\n        return proj4_dict_to_str(proj_dict, sort=True)\n\n    def __str__(self):\n        """"""Return string representation of the AreaDefinition.""""""\n        # We need a sorted dictionary for a unique hash of str(self)\n        proj_dict = self.proj_dict\n        proj_str = (\'{\' +\n                    \', \'.join([""\'%s\': \'%s\'"" % (str(k), str(proj_dict[k]))\n                               for k in sorted(proj_dict.keys())]) +\n                    \'}\')\n        if not self.proj_id:\n            third_line = """"\n        else:\n            third_line = ""Projection ID: {0}\\n"".format(self.proj_id)\n        return (\'Area ID: {0}\\nDescription: {1}\\n{2}\'\n                \'Projection: {3}\\nNumber of columns: {4}\\nNumber of rows: {5}\\n\'\n                \'Area extent: {6}\').format(self.area_id, self.description, third_line,\n                                           proj_str, self.width, self.height,\n                                           tuple(round(x, 4) for x in self.area_extent))\n\n    __repr__ = __str__\n\n    def to_cartopy_crs(self):\n        """"""Convert projection to cartopy CRS object.""""""\n        from pyresample.utils.cartopy import from_proj\n        bounds = (self.area_extent[0],\n                  self.area_extent[2],\n                  self.area_extent[1],\n                  self.area_extent[3])\n        if hasattr(self, \'crs\') and self.crs.to_epsg() is not None:\n            proj_params = ""EPSG:{}"".format(self.crs.to_epsg())\n        else:\n            proj_params = self.proj_str\n        if Proj(proj_params).is_latlong():\n            # Convert area extent from degrees to radians\n            bounds = np.deg2rad(bounds)\n        crs = from_proj(proj_params, bounds=bounds)\n        return crs\n\n    def create_areas_def(self):\n        """"""Generate YAML formatted representation of this area.""""""\n        if hasattr(self, \'crs\') and self.crs.to_epsg() is not None:\n            proj_dict = {\'EPSG\': self.crs.to_epsg()}\n        else:\n            proj_dict = self.proj_dict\n            # pyproj 2.0+ adds a \'+type=crs\' parameter\n            proj_dict.pop(\'type\', None)\n\n        res = OrderedDict(description=self.description,\n                          projection=OrderedDict(proj_dict),\n                          shape=OrderedDict([(\'height\', self.height), (\'width\', self.width)]))\n        units = res[\'projection\'].pop(\'units\', None)\n        extent = OrderedDict([(\'lower_left_xy\', list(self.area_extent[:2])),\n                              (\'upper_right_xy\', list(self.area_extent[2:]))])\n        if units is not None:\n            extent[\'units\'] = units\n        res[\'area_extent\'] = extent\n\n        return ordered_dump(OrderedDict([(self.area_id, res)]))\n\n    def create_areas_def_legacy(self):\n        """"""Create area definition in legacy format.""""""\n        proj_dict = self.proj_dict\n        proj_str = \',\'.join([""%s=%s"" % (str(k), str(proj_dict[k]))\n                             for k in sorted(proj_dict.keys())])\n\n        fmt = ""REGION: {name} {{\\n""\n        fmt += ""\\tNAME:\\t{name}\\n""\n        fmt += ""\\tPCS_ID:\\t{area_id}\\n""\n        fmt += ""\\tPCS_DEF:\\t{proj_str}\\n""\n        fmt += ""\\tXSIZE:\\t{x_size}\\n""\n        fmt += ""\\tYSIZE:\\t{y_size}\\n""\n        # fmt += ""\\tROTATION:\\t{rotation}\\n""\n        fmt += ""\\tAREA_EXTENT: {area_extent}\\n}};\\n""\n        area_def_str = fmt.format(name=self.description, area_id=self.area_id,\n                                  proj_str=proj_str, x_size=self.width,\n                                  y_size=self.height,\n                                  area_extent=self.area_extent)\n        return area_def_str\n\n    def __eq__(self, other):\n        """"""Test for equality.""""""\n        try:\n            return ((self.proj_str == other.proj_str) and\n                    (self.shape == other.shape) and\n                    (np.allclose(self.area_extent, other.area_extent)))\n        except AttributeError:\n            return super(AreaDefinition, self).__eq__(other)\n\n    def __ne__(self, other):\n        """"""Test for equality.""""""\n        return not self.__eq__(other)\n\n    def update_hash(self, the_hash=None):\n        """"""Update a hash, or return a new one if needed.""""""\n        if the_hash is None:\n            the_hash = hashlib.sha1()\n        the_hash.update(self.proj_str.encode(\'utf-8\'))\n        the_hash.update(np.array(self.shape))\n        the_hash.update(np.array(self.area_extent))\n        return the_hash\n\n    def colrow2lonlat(self, cols, rows):\n        """"""Return lons and lats for the given image columns and rows.\n\n        Both scalars and arrays are supported. To be used with scarse\n        data points instead of slices (see get_lonlats).\n\n        """"""\n        p = Proj(self.proj_str)\n        x = self.projection_x_coords\n        y = self.projection_y_coords\n        return p(y[y.size - cols], x[x.size - rows], inverse=True)\n\n    def lonlat2colrow(self, lons, lats):\n        """"""Return image columns and rows for the given lons and lats.\n\n        Both scalars and arrays are supported.  Same as\n        get_xy_from_lonlat, renamed for convenience.\n\n        """"""\n        return self.get_xy_from_lonlat(lons, lats)\n\n    def get_xy_from_lonlat(self, lon, lat):\n        """"""Retrieve closest x and y coordinates.\n\n        Retrieve closest x and y coordinates (column, row indices) for the\n        specified geolocation (lon,lat) if inside area. If lon,lat is a point a\n        ValueError is raised if the return point is outside the area domain. If\n        lon,lat is a tuple of sequences of longitudes and latitudes, a tuple of\n        masked arrays are returned.\n\n        :Input:\n\n        lon : point or sequence (list or array) of longitudes\n        lat : point or sequence (list or array) of latitudes\n\n        :Returns:\n\n        (x, y) : tuple of integer points/arrays\n\n        """"""\n        if isinstance(lon, list):\n            lon = np.array(lon)\n        if isinstance(lat, list):\n            lat = np.array(lat)\n\n        if ((isinstance(lon, np.ndarray) and\n             not isinstance(lat, np.ndarray)) or (not isinstance(lon, np.ndarray) and isinstance(lat, np.ndarray))):\n            raise ValueError(""Both lon and lat needs to be of "" +\n                             ""the same type and have the same dimensions!"")\n\n        if isinstance(lon, np.ndarray) and isinstance(lat, np.ndarray):\n            if lon.shape != lat.shape:\n                raise ValueError(""lon and lat is not of the same shape!"")\n\n        pobj = Proj(self.proj_str)\n        xm_, ym_ = pobj(lon, lat)\n\n        return self.get_xy_from_proj_coords(xm_, ym_)\n\n    def get_xy_from_proj_coords(self, xm, ym):\n        """"""Find closest grid cell index for a specified projection coordinate.\n\n        If xm, ym is a tuple of sequences of projection coordinates, a tuple\n        of masked arrays are returned.\n\n        Args:\n            xm (list or array): point or sequence of x-coordinates in\n                                 meters (map projection)\n            ym (list or array): point or sequence of y-coordinates in\n                                 meters (map projection)\n\n        Returns:\n            x, y : column and row grid cell indexes as 2 scalars or arrays\n\n        Raises:\n            ValueError: if the return point is outside the area domain\n\n        """"""\n        if isinstance(xm, list):\n            xm = np.array(xm)\n        if isinstance(ym, list):\n            ym = np.array(ym)\n\n        if ((isinstance(xm, np.ndarray) and\n             not isinstance(ym, np.ndarray)) or (not isinstance(xm, np.ndarray) and isinstance(ym, np.ndarray))):\n            raise ValueError(""Both projection coordinates xm and ym needs to be of "" +\n                             ""the same type and have the same dimensions!"")\n\n        if isinstance(xm, np.ndarray) and isinstance(ym, np.ndarray):\n            if xm.shape != ym.shape:\n                raise ValueError(\n                    ""projection coordinates xm and ym is not of the same shape!"")\n\n        upl_x = self.area_extent[0]\n        upl_y = self.area_extent[3]\n        xscale = (self.area_extent[2] -\n                  self.area_extent[0]) / float(self.width)\n        # because rows direction is the opposite of y\'s\n        yscale = (self.area_extent[1] -\n                  self.area_extent[3]) / float(self.height)\n\n        x__ = (xm - upl_x) / xscale\n        y__ = (ym - upl_y) / yscale\n\n        if isinstance(x__, np.ndarray) and isinstance(y__, np.ndarray):\n            mask = (((x__ < 0) | (x__ >= self.width)) |\n                    ((y__ < 0) | (y__ >= self.height)))\n            return (np.ma.masked_array(x__.astype(\'int\'), mask=mask,\n                                       fill_value=-1, copy=False),\n                    np.ma.masked_array(y__.astype(\'int\'), mask=mask,\n                                       fill_value=-1, copy=False))\n        else:\n            if ((x__ < 0 or x__ >= self.width) or\n                    (y__ < 0 or y__ >= self.height)):\n                raise ValueError(\'Point outside area:( %f %f)\' % (x__, y__))\n            return int(x__), int(y__)\n\n    def get_lonlat(self, row, col):\n        """"""Retrieve lon and lat values of single point in area grid.\n\n        Parameters\n        ----------\n        row : int\n        col : int\n\n        Returns\n        -------\n        (lon, lat) : tuple of floats\n\n        """"""\n        lon, lat = self.get_lonlats(nprocs=None, data_slice=(row, col))\n        return lon.item(), lat.item()\n\n    @staticmethod\n    def _do_rotation(xspan, yspan, rot_deg=0):\n        """"""Apply a rotation factor to a matrix of points.""""""\n        if hasattr(xspan, \'chunks\'):\n            # we were given dask arrays, use dask functions\n            import dask.array as numpy\n        else:\n            numpy = np\n        rot_rad = numpy.radians(rot_deg)\n        rot_mat = numpy.array([[np.cos(rot_rad),  np.sin(rot_rad)], [-np.sin(rot_rad), np.cos(rot_rad)]])\n        x, y = numpy.meshgrid(xspan, yspan)\n        return numpy.einsum(\'ji, mni -> jmn\', rot_mat, numpy.dstack([x, y]))\n\n    def get_proj_vectors_dask(self, chunks=None, dtype=None):\n        """"""Get projection vectors.""""""\n        warnings.warn(""\'get_proj_vectors_dask\' is deprecated, please use ""\n                      ""\'get_proj_vectors\' with the \'chunks\' keyword argument specified."", DeprecationWarning)\n        if chunks is None:\n            chunks = CHUNK_SIZE  # FUTURE: Use a global config object instead\n        return self.get_proj_vectors(dtype=dtype, chunks=chunks)\n\n    def _get_proj_vectors(self, dtype=None, check_rotation=True, chunks=None):\n        """"""Get 1D projection coordinates.""""""\n        x_kwargs = {}\n        y_kwargs = {}\n\n        if chunks is not None and not isinstance(chunks, int):\n            y_chunks = chunks[0]\n            x_chunks = chunks[1]\n        else:\n            y_chunks = x_chunks = chunks\n\n        if x_chunks is not None or y_chunks is not None:\n            # use dask functions instead of numpy\n            from dask.array import arange\n            x_kwargs = {\'chunks\': x_chunks}\n            y_kwargs = {\'chunks\': y_chunks}\n        else:\n            arange = np.arange\n        if check_rotation and self.rotation != 0:\n            warnings.warn(""Projection vectors will not be accurate because rotation is not 0"", RuntimeWarning)\n        if dtype is None:\n            dtype = self.dtype\n        x_kwargs[\'dtype\'] = dtype\n        y_kwargs[\'dtype\'] = dtype\n\n        target_x = arange(self.width, **x_kwargs) * self.pixel_size_x + self.pixel_upper_left[0]\n        target_y = arange(self.height, **y_kwargs) * -self.pixel_size_y + self.pixel_upper_left[1]\n        return target_x, target_y\n\n    def get_proj_vectors(self, dtype=None, chunks=None):\n        """"""Calculate 1D projection coordinates for the X and Y dimension.\n\n        Parameters\n        ----------\n        dtype : numpy.dtype\n            Numpy data type for the returned arrays\n        chunks : int or tuple\n            Return dask arrays with the chunk size specified. If this is a\n            tuple then the first element is the Y array\'s chunk size and the\n            second is the X array\'s chunk size.\n\n        Returns\n        -------\n        tuple: (X, Y) where X and Y are 1-dimensional numpy arrays\n\n        The data type of the returned arrays can be controlled with the\n        `dtype` keyword argument. If `chunks` is provided then dask arrays\n        are returned instead.\n\n        """"""\n        return self._get_proj_vectors(dtype=dtype, chunks=chunks)\n\n    def get_proj_coords_dask(self, chunks=None, dtype=None):\n        """"""Get projection coordinates.""""""\n        warnings.warn(""\'get_proj_coords_dask\' is deprecated, please use ""\n                      ""\'get_proj_coords\' with the \'chunks\' keyword argument specified."", DeprecationWarning)\n        if chunks is None:\n            chunks = CHUNK_SIZE  # FUTURE: Use a global config object instead\n        return self.get_proj_coords(chunks=chunks, dtype=dtype)\n\n    def get_proj_coords(self, data_slice=None, dtype=None, chunks=None):\n        """"""Get projection coordinates of grid.\n\n        Parameters\n        ----------\n        data_slice : slice object, optional\n            Calculate only coordinates for specified slice\n        dtype : numpy.dtype, optional\n            Data type of the returned arrays\n        chunks: int or tuple, optional\n            Create dask arrays and use this chunk size\n\n        Returns\n        -------\n        (target_x, target_y) : tuple of numpy arrays\n            Grids of area x- and y-coordinates in projection units\n\n        .. versionchanged:: 1.11.0\n\n            Removed \'cache\' keyword argument and add \'chunks\' for creating\n            dask arrays.\n\n        """"""\n        target_x, target_y = self._get_proj_vectors(dtype=dtype, check_rotation=False, chunks=chunks)\n        if data_slice is not None and isinstance(data_slice, slice):\n            target_y = target_y[data_slice]\n        elif data_slice is not None:\n            target_y = target_y[data_slice[0]]\n            target_x = target_x[data_slice[1]]\n\n        if self.rotation != 0:\n            res = self._do_rotation(target_x, target_y, self.rotation)\n            target_x, target_y = res[0, :, :], res[1, :, :]\n        elif chunks is not None:\n            import dask.array as da\n            target_x, target_y = da.meshgrid(target_x, target_y)\n        else:\n            target_x, target_y = np.meshgrid(target_x, target_y)\n\n        return target_x, target_y\n\n    @property\n    def projection_x_coords(self):\n        """"""Return projection X coordinates.""""""\n        if self.rotation != 0:\n            # rotation is only supported in \'get_proj_coords\' right now\n            return self.get_proj_coords(data_slice=(0, slice(None)))[0].squeeze()\n        return self.get_proj_vectors()[0]\n\n    @property\n    def projection_y_coords(self):\n        """"""Return projection Y coordinates.""""""\n        if self.rotation != 0:\n            # rotation is only supported in \'get_proj_coords\' right now\n            return self.get_proj_coords(data_slice=(slice(None), 0))[1].squeeze()\n        return self.get_proj_vectors()[1]\n\n    @property\n    def outer_boundary_corners(self):\n        """"""Return the lon,lat of the outer edges of the corner points.""""""\n        from pyresample.spherical_geometry import Coordinate\n        proj = Proj(**self.proj_dict)\n\n        corner_lons, corner_lats = proj((self.area_extent[0], self.area_extent[2],\n                                         self.area_extent[2], self.area_extent[0]),\n                                        (self.area_extent[3], self.area_extent[3],\n                                         self.area_extent[1], self.area_extent[1]),\n                                        inverse=True)\n        return [Coordinate(corner_lons[0], corner_lats[0]),\n                Coordinate(corner_lons[1], corner_lats[1]),\n                Coordinate(corner_lons[2], corner_lats[2]),\n                Coordinate(corner_lons[3], corner_lats[3])]\n\n    def get_lonlats_dask(self, chunks=None, dtype=None):\n        """"""Get longitudes and latitudes.""""""\n        warnings.warn(""\'get_lonlats_dask\' is deprecated, please use ""\n                      ""\'get_lonlats\' with the \'chunks\' keyword argument specified."", DeprecationWarning)\n        if chunks is None:\n            chunks = CHUNK_SIZE  # FUTURE: Use a global config object instead\n        return self.get_lonlats(chunks=chunks, dtype=dtype)\n\n    def get_lonlats(self, nprocs=None, data_slice=None, cache=False, dtype=None, chunks=None):\n        """"""Return lon and lat arrays of area.\n\n        Parameters\n        ----------\n        nprocs : int, optional\n            Number of processor cores to be used.\n            Defaults to the nprocs set when instantiating object\n        data_slice : slice object, optional\n            Calculate only coordinates for specified slice\n        cache : bool, optional\n            Store result the result. Requires data_slice to be None\n        dtype : numpy.dtype, optional\n            Data type of the returned arrays\n        chunks: int or tuple, optional\n            Create dask arrays and use this chunk size\n\n        Returns\n        -------\n        (lons, lats) : tuple of numpy arrays\n            Grids of area lons and and lats\n\n        """"""\n        if cache:\n            warnings.warn(""\'cache\' keyword argument will be removed in the ""\n                          ""future and data will not be cached."", PendingDeprecationWarning)\n        if dtype is None:\n            dtype = self.dtype\n\n        if self.lons is not None:\n            # Data is cache already\n            lons = self.lons\n            lats = self.lats\n            if data_slice is not None:\n                lons = lons[data_slice]\n                lats = lats[data_slice]\n            return lons, lats\n\n        # Get X/Y coordinates for the whole area\n        target_x, target_y = self.get_proj_coords(data_slice=data_slice, chunks=chunks, dtype=dtype)\n        if nprocs is None and not hasattr(target_x, \'chunks\'):\n            nprocs = self.nprocs\n        if nprocs is not None and hasattr(target_x, \'chunks\'):\n            # we let \'get_proj_coords\' decide if dask arrays should be made\n            # but if the user provided nprocs then this doesn\'t make sense\n            raise ValueError(""Can\'t specify \'nprocs\' and \'chunks\' at the same time"")\n\n        # Proj.4 definition of target area projection\n        proj_def = self.crs_wkt if hasattr(self, \'crs_wkt\') else self.proj_dict\n        if hasattr(target_x, \'chunks\'):\n            # we are using dask arrays, map blocks to th\n            from dask.array import map_blocks\n            res = map_blocks(invproj, target_x, target_y,\n                             chunks=(target_x.chunks[0], target_x.chunks[1], 2),\n                             new_axis=[2], proj_dict=proj_def).astype(dtype)\n            return res[:, :, 0], res[:, :, 1]\n\n        if nprocs > 1:\n            target_proj = Proj_MP(proj_def)\n        else:\n            target_proj = Proj(proj_def)\n\n        # Get corresponding longitude and latitude values\n        lons, lats = target_proj(target_x, target_y, inverse=True, nprocs=nprocs)\n        lons = np.asanyarray(lons, dtype=dtype)\n        lats = np.asanyarray(lats, dtype=dtype)\n\n        if cache and data_slice is None:\n            # Cache the result if requested\n            self.lons = lons\n            self.lats = lats\n\n        return lons, lats\n\n    @property\n    def proj4_string(self):\n        """"""Return projection definition as Proj.4 string.""""""\n        warnings.warn(""\'proj4_string\' is deprecated, please use \'proj_str\' ""\n                      ""instead."", DeprecationWarning)\n        return proj4_dict_to_str(self.proj_dict)\n\n    def _get_slice_starts_stops(self, area_to_cover):\n        """"""Get x and y start and stop points for slicing.""""""\n        llx, lly, urx, ury = area_to_cover.area_extent\n        x, y = self.get_xy_from_proj_coords([llx, urx], [lly, ury])\n\n        if self.area_extent[0] > self.area_extent[2]:\n            xstart = 0 if x[1] is np.ma.masked else x[1]\n            xstop = self.width if x[0] is np.ma.masked else x[0] + 1\n        else:\n            xstart = 0 if x[0] is np.ma.masked else x[0]\n            xstop = self.width if x[1] is np.ma.masked else x[1] + 1\n        if self.area_extent[1] > self.area_extent[3]:\n            ystart = 0 if y[0] is np.ma.masked else y[0]\n            ystop = self.height if y[1] is np.ma.masked else y[1] + 1\n        else:\n            ystart = 0 if y[1] is np.ma.masked else y[1]\n            ystop = self.height if y[0] is np.ma.masked else y[0] + 1\n\n        return xstart, xstop, ystart, ystop\n\n    def get_area_slices(self, area_to_cover, shape_divisible_by=None):\n        """"""Compute the slice to read based on an `area_to_cover`.""""""\n        if not isinstance(area_to_cover, AreaDefinition):\n            raise NotImplementedError(\'Only AreaDefinitions can be used\')\n\n        # Intersection only required for two different projections\n        proj_def_to_cover = area_to_cover.crs if hasattr(area_to_cover, \'crs\') else area_to_cover.proj_str\n        proj_def = self.crs if hasattr(self, \'crs\') else self.proj_str\n        if proj_def_to_cover == proj_def:\n            logger.debug(\'Projections for data and slice areas are\'\n                         \' identical: %s\',\n                         proj_def_to_cover)\n            # Get slice parameters\n            xstart, xstop, ystart, ystop = self._get_slice_starts_stops(\n                area_to_cover)\n\n            return (check_slice_orientation(slice(xstart, xstop)),\n                    check_slice_orientation(slice(ystart, ystop)))\n\n        if self.proj_dict.get(\'proj\') != \'geos\':\n            raise NotImplementedError(""Source projection must be \'geos\' if ""\n                                      ""source/target projections are not ""\n                                      ""equal."")\n\n        data_boundary = Boundary(*get_geostationary_bounding_box(self))\n        if area_to_cover.proj_dict.get(\'proj\') == \'geos\':\n            area_boundary = Boundary(\n                *get_geostationary_bounding_box(area_to_cover))\n        else:\n            area_boundary = AreaDefBoundary(area_to_cover, 100)\n\n        intersection = data_boundary.contour_poly.intersection(\n            area_boundary.contour_poly)\n        if intersection is None:\n            logger.debug(\'Cannot determine appropriate slicing. \'\n                         ""Data and projection area do not overlap."")\n            raise NotImplementedError\n        x, y = self.get_xy_from_lonlat(np.rad2deg(intersection.lon),\n                                       np.rad2deg(intersection.lat))\n        x_slice = slice(np.ma.min(x), np.ma.max(x) + 1)\n        y_slice = slice(np.ma.min(y), np.ma.max(y) + 1)\n        if shape_divisible_by is not None:\n            x_slice = _make_slice_divisible(x_slice, self.width,\n                                            factor=shape_divisible_by)\n            y_slice = _make_slice_divisible(y_slice, self.height,\n                                            factor=shape_divisible_by)\n\n        return (check_slice_orientation(x_slice),\n                check_slice_orientation(y_slice))\n\n    def crop_around(self, other_area):\n        """"""Crop this area around `other_area`.""""""\n        xslice, yslice = self.get_area_slices(other_area)\n        return self[yslice, xslice]\n\n    def __getitem__(self, key):\n        """"""Apply slices to the area_extent and size of the area.""""""\n        yslice, xslice = key\n        # Get actual values, replace Nones\n        yindices = yslice.indices(self.height)\n        total_rows = int((yindices[1] - yindices[0]) / yindices[2])\n        ystopactual = yindices[1] - (yindices[1] - 1) % yindices[2]\n        xindices = xslice.indices(self.width)\n        total_cols = int((xindices[1] - xindices[0]) / xindices[2])\n        xstopactual = xindices[1] - (xindices[1] - 1) % xindices[2]\n        yslice = slice(yindices[0], ystopactual, yindices[2])\n        xslice = slice(xindices[0], xstopactual, xindices[2])\n\n        new_area_extent = ((self.pixel_upper_left[0] + (xslice.start - 0.5) * self.pixel_size_x),\n                           (self.pixel_upper_left[1] - (yslice.stop - 0.5) * self.pixel_size_y),\n                           (self.pixel_upper_left[0] + (xslice.stop - 0.5) * self.pixel_size_x),\n                           (self.pixel_upper_left[1] - (yslice.start - 0.5) * self.pixel_size_y))\n\n        new_area = AreaDefinition(self.area_id, self.description,\n                                  self.proj_id, self.proj_dict,\n                                  total_cols,\n                                  total_rows,\n                                  new_area_extent)\n        new_area.crop_offset = (self.crop_offset[0] + yslice.start,\n                                self.crop_offset[1] + xslice.start)\n        return new_area\n\n    def geocentric_resolution(self, ellps=\'WGS84\', radius=None):\n        """"""Find best estimate for overall geocentric resolution.\n\n        This method is extremely important to the results of KDTree-based\n        resamplers like the nearest neighbor resampling. This is used to\n        determine how far the KDTree should be queried for valid pixels\n        before giving up (`radius_of_influence`). This method attempts to\n        make a best guess at what geocentric resolution (the units used by\n        the KDTree) represents the majority of an area.\n\n        To do this this method will:\n\n        1. Create a vertical mid-line and a horizontal mid-line.\n        2. Convert these coordinates to geocentric coordinates.\n        3. Compute the distance between points along these lines.\n        4. Take the histogram of each set of distances and find the\n           bin with the most points.\n        5. Take the average of the edges of that bin.\n        6. Return the maximum of the vertical and horizontal bin\n           edge averages.\n\n        """"""\n        from pyproj import transform\n        rows, cols = self.shape\n        mid_row = rows // 2\n        mid_col = cols // 2\n        x, y = self.get_proj_vectors()\n        mid_col_x = np.repeat(x[mid_col], y.size)\n        mid_row_y = np.repeat(y[mid_row], x.size)\n        src = Proj(getattr(self, \'crs\', self.proj_dict))\n        if radius:\n            dst = Proj(""+proj=cart +a={} +b={}"".format(radius, radius))\n        else:\n            dst = Proj(""+proj=cart +ellps={}"".format(ellps))\n        # need some altitude, go with the surface (0)\n        alt_x = np.zeros(x.size)\n        alt_y = np.zeros(y.size)\n        # convert our midlines to (X, Y, Z) geocentric coordinates\n        hor_xyz = np.stack(transform(src, dst, x, mid_row_y, alt_x), axis=1)\n        vert_xyz = np.stack(transform(src, dst, mid_col_x, y, alt_y), axis=1)\n        # Find the distance in meters along our midlines\n        hor_dist = np.linalg.norm(np.diff(hor_xyz, axis=0), axis=1)\n        vert_dist = np.linalg.norm(np.diff(vert_xyz, axis=0), axis=1)\n        # Get rid of any NaNs or infinite values\n        hor_dist = hor_dist[np.isfinite(hor_dist)]\n        vert_dist = vert_dist[np.isfinite(vert_dist)]\n        # use the average of the largest histogram bin to avoid\n        # outliers and really large values.\n        # Very useful near edge of disk geostationary areas.\n        hor_res = vert_res = 0\n        if hor_dist.size:\n            hor_res = np.mean(np.histogram_bin_edges(hor_dist)[:2])\n        if vert_dist.size:\n            vert_res = np.mean(np.histogram_bin_edges(vert_dist)[:2])\n        # Use the maximum distance between the two midlines instead of\n        # binning both of them together. If we binned them together then\n        # we are highly dependent on the shape of the area (more rows in\n        # the area would almost always mean that we resulted in the vertical\n        # midline\'s distance).\n        res = max(hor_res, vert_res)\n        if not res:\n            raise RuntimeError(""Could not calculate geocentric resolution"")\n        # return np.max(np.concatenate(vert_dist, hor_dist))  # alternative to histogram\n        return res\n\n\ndef _make_slice_divisible(sli, max_size, factor=2):\n    """"""Make the given slice even in size.""""""\n    rem = (sli.stop - sli.start) % factor\n    if rem != 0:\n        adj = factor - rem\n        if sli.stop + 1 + rem < max_size:\n            sli = slice(sli.start, sli.stop + adj)\n        elif sli.start > 0:\n            sli = slice(sli.start - adj, sli.stop)\n        else:\n            sli = slice(sli.start, sli.stop - rem)\n\n    return sli\n\n\ndef get_geostationary_angle_extent(geos_area):\n    """"""Get the max earth (vs space) viewing angles in x and y.""""""\n    # get some projection parameters\n    a, b = proj4_radius_parameters(geos_area.proj_dict)\n    req = a / 1000.0\n    rp = b / 1000.0\n    h = geos_area.proj_dict[\'h\'] / 1000.0 + req\n\n    # compute some constants\n    aeq = 1 - req ** 2 / (h ** 2)\n    ap_ = 1 - rp ** 2 / (h ** 2)\n\n    # generate points around the north hemisphere in satellite projection\n    # make it a bit smaller so that we stay inside the valid area\n    xmax = np.arccos(np.sqrt(aeq))\n    ymax = np.arccos(np.sqrt(ap_))\n    return xmax, ymax\n\n\ndef get_geostationary_bounding_box(geos_area, nb_points=50):\n    """"""Get the bbox in lon/lats of the valid pixels inside `geos_area`.\n\n    Args:\n      nb_points: Number of points on the polygon\n\n    """"""\n    xmax, ymax = get_geostationary_angle_extent(geos_area)\n\n    # generate points around the north hemisphere in satellite projection\n    # make it a bit smaller so that we stay inside the valid area\n    x = np.cos(np.linspace(-np.pi, 0, int(nb_points / 2.0))) * (xmax - 0.0001)\n    y = -np.sin(np.linspace(-np.pi, 0, int(nb_points / 2.0))) * (ymax - 0.0001)\n\n    ll_x, ll_y, ur_x, ur_y = geos_area.area_extent\n\n    x *= geos_area.proj_dict[\'h\']\n    y *= geos_area.proj_dict[\'h\']\n\n    x = np.clip(np.concatenate([x, x[::-1]]), min(ll_x, ur_x), max(ll_x, ur_x))\n    y = np.clip(np.concatenate([y, -y]), min(ll_y, ur_y), max(ll_y, ur_y))\n\n    return Proj(**geos_area.proj_dict)(x, y, inverse=True)\n\n\ndef combine_area_extents_vertical(area1, area2):\n    """"""Combine the area extents of areas 1 and 2.""""""\n    if (area1.area_extent[0] == area2.area_extent[0]\n            and area1.area_extent[2] == area2.area_extent[2]):\n        current_extent = list(area1.area_extent)\n        if np.isclose(area1.area_extent[1], area2.area_extent[3]):\n            current_extent[1] = area2.area_extent[1]\n        elif np.isclose(area1.area_extent[3], area2.area_extent[1]):\n            current_extent[3] = area2.area_extent[3]\n        else:\n            raise IncompatibleAreas(\n                ""Can\'t concatenate non-contiguous area definitions: ""\n                ""{0} and {1}"".format(area1, area2))\n    else:\n        raise IncompatibleAreas(\n            ""Can\'t concatenate area definitions with ""\n            ""incompatible area extents: ""\n            ""{0} and {1}"".format(area1, area2))\n    return current_extent\n\n\ndef concatenate_area_defs(area1, area2, axis=0):\n    """"""Append *area2* to *area1* and return the results.""""""\n    different_items = (set(area1.proj_dict.items()) ^\n                       set(area2.proj_dict.items()))\n    if axis == 0:\n        same_size = area1.width == area2.width\n    else:\n        raise NotImplementedError(\'Only vertical contatenation is supported.\')\n    if different_items or not same_size:\n        raise IncompatibleAreas(""Can\'t concatenate area definitions with ""\n                                ""different projections: ""\n                                ""{0} and {1}"".format(area1, area2))\n\n    if axis == 0:\n        area_extent = combine_area_extents_vertical(area1, area2)\n        x_size = int(area1.width)\n        y_size = int(area1.height + area2.height)\n    else:\n        raise NotImplementedError(\'Only vertical contatenation is supported.\')\n    return AreaDefinition(area1.area_id, area1.description, area1.proj_id,\n                          area1.proj_dict, x_size, y_size,\n                          area_extent)\n\n\nclass StackedAreaDefinition(BaseDefinition):\n    """"""Definition based on muliple vertically stacked AreaDefinitions.""""""\n\n    def __init__(self, *definitions, **kwargs):\n        """"""Initialize StackedAreaDefinition based on *definitions*.\n\n        *kwargs* used here are `nprocs` and `dtype` (see AreaDefinition).\n\n        """"""\n        nprocs = kwargs.get(\'nprocs\', 1)\n        super(StackedAreaDefinition, self).__init__(nprocs=nprocs)\n        self.dtype = kwargs.get(\'dtype\', np.float64)\n        self.defs = []\n        self.proj_dict = {}\n        for definition in definitions:\n            self.append(definition)\n\n    @property\n    def width(self):\n        """"""Return width of the area definition.""""""\n        return self.defs[0].width\n\n    @property\n    def x_size(self):\n        """"""Return width of the area definition.""""""\n        warnings.warn(""\'x_size\' is deprecated, use \'width\' instead."", PendingDeprecationWarning)\n        return self.width\n\n    @property\n    def height(self):\n        """"""Return height of the area definition.""""""\n        return sum(definition.height for definition in self.defs)\n\n    @property\n    def y_size(self):\n        """"""Return height of the area definition.""""""\n        warnings.warn(""\'y_size\' is deprecated, use \'height\' instead."", PendingDeprecationWarning)\n        return self.height\n\n    @property\n    def size(self):\n        """"""Return size of the area definition.""""""\n        return self.height * self.width\n\n    @property\n    def shape(self):\n        """"""Return shape of the area definition.""""""\n        return (self.height, self.width)\n\n    def append(self, definition):\n        """"""Append another definition to the area.""""""\n        if isinstance(definition, StackedAreaDefinition):\n            for area in definition.defs:\n                self.append(area)\n            return\n        if definition.height == 0:\n            return\n        if not self.defs:\n            self.proj_dict = definition.proj_dict\n        elif self.proj_dict != definition.proj_dict:\n            raise NotImplementedError(\'Cannot append areas:\'\n                                      \' Proj.4 dict mismatch\')\n        try:\n            self.defs[-1] = concatenate_area_defs(self.defs[-1], definition)\n        except (IncompatibleAreas, IndexError):\n            self.defs.append(definition)\n\n    def get_lonlats(self, nprocs=None, data_slice=None, cache=False, dtype=None, chunks=None):\n        """"""Return lon and lat arrays of the area.""""""\n        if chunks is not None:\n            from dask.array import vstack\n        else:\n            vstack = np.vstack\n\n        llons = []\n        llats = []\n        try:\n            row_slice, col_slice = data_slice\n        except TypeError:\n            row_slice = slice(0, self.height)\n            col_slice = slice(0, self.width)\n        offset = 0\n        for definition in self.defs:\n            local_row_slice = slice(max(row_slice.start - offset, 0),\n                                    min(max(row_slice.stop - offset, 0), definition.height),\n                                    row_slice.step)\n            lons, lats = definition.get_lonlats(nprocs=nprocs, data_slice=(local_row_slice, col_slice),\n                                                cache=cache, dtype=dtype, chunks=chunks)\n\n            llons.append(lons)\n            llats.append(lats)\n            offset += lons.shape[0]\n\n        self.lons = vstack(llons)\n        self.lats = vstack(llats)\n\n        return self.lons, self.lats\n\n    def get_lonlats_dask(self, chunks=None, dtype=None):\n        """"""Return lon and lat dask arrays of the area.""""""\n        warnings.warn(""\'get_lonlats_dask\' is deprecated, please use ""\n                      ""\'get_lonlats\' with the \'chunks\' keyword argument specified."",\n                      DeprecationWarning)\n        if chunks is None:\n            chunks = CHUNK_SIZE  # FUTURE: Use a global config object instead\n        return self.get_lonlats(chunks=chunks, dtype=dtype)\n\n    def squeeze(self):\n        """"""Generate a single AreaDefinition if possible.""""""\n        if len(self.defs) == 1:\n            return self.defs[0]\n        else:\n            return self\n\n    @property\n    def proj4_string(self):\n        """"""Return projection definition as Proj.4 string.""""""\n        warnings.warn(""\'proj4_string\' is deprecated, please use \'proj_str\' ""\n                      ""instead."", DeprecationWarning)\n        return self.defs[0].proj_str\n\n    @property\n    def proj_str(self):\n        """"""Return projection definition as Proj.4 string.""""""\n        return self.defs[0].proj_str\n\n    def update_hash(self, the_hash=None):\n        """"""Update the hash.""""""\n        for areadef in self.defs:\n            the_hash = areadef.update_hash(the_hash)\n        return the_hash\n\n\ndef _get_slice(segments, shape):\n    """"""Segment a 1D or 2D array.""""""\n    if not (1 <= len(shape) <= 2):\n        raise ValueError(\'Cannot segment array of shape: %s\' % str(shape))\n    else:\n        size = shape[0]\n        slice_length = int(np.ceil(float(size) / segments))\n        start_idx = 0\n        end_idx = slice_length\n        while start_idx < size:\n            if len(shape) == 1:\n                yield slice(start_idx, end_idx)\n            else:\n                yield (slice(start_idx, end_idx), slice(None))\n            start_idx = end_idx\n            end_idx = min(start_idx + slice_length, size)\n\n\ndef _flatten_cartesian_coords(cartesian_coords):\n    """"""Flatten array to (n, 3) shape.""""""\n    shape = cartesian_coords.shape\n    if len(shape) > 2:\n        cartesian_coords = cartesian_coords.reshape(shape[0] *\n                                                    shape[1], 3)\n    return cartesian_coords\n\n\ndef _get_highest_level_class(obj1, obj2):\n    if (not issubclass(obj1.__class__, obj2.__class__) or\n            not issubclass(obj2.__class__, obj1.__class__)):\n        raise TypeError(\'No common superclass for %s and %s\' %\n                        (obj1.__class__, obj2.__class__))\n\n    if obj1.__class__ == obj2.__class__:\n        klass = obj1.__class__\n    elif issubclass(obj1.__class__, obj2.__class__):\n        klass = obj2.__class__\n    else:\n        klass = obj1.__class__\n    return klass\n\n\ndef ordered_dump(data, stream=None, Dumper=yaml.Dumper, **kwds):\n    """"""Dump the data to YAML in ordered fashion.""""""\n    class OrderedDumper(Dumper):\n        pass\n\n    def _dict_representer(dumper, data):\n        return dumper.represent_mapping(\n            yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,\n            data.items(), flow_style=False)\n\n    OrderedDumper.add_representer(OrderedDict, _dict_representer)\n    return yaml.dump(data, stream, OrderedDumper, **kwds)\n'"
pyresample/grid.py,10,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010, 2014, 2015  Esben S. Nielsen\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Resample image from one projection to another\nusing nearest neighbour method in cartesian projection coordinate systems""""""\n\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom pyresample import geometry, _spatial_mp\n\ntry:\n    range = xrange\nexcept NameError:\n    pass\n\n\ndef get_image_from_linesample(row_indices, col_indices, source_image,\n                              fill_value=0):\n    """"""Samples from image based on index arrays.\n\n    Parameters\n    ----------\n    row_indices : numpy array\n        Row indices. Dimensions must match col_indices\n    col_indices : numpy array\n        Col indices. Dimensions must match row_indices\n    source_image : numpy array\n        Source image\n    fill_value : int or None, optional\n            Set undetermined pixels to this value.\n            If fill_value is None a masked array is returned\n            with undetermined pixels masked\n\n    Returns\n    -------\n    image_data : numpy array\n        Resampled image\n    """"""\n\n    # mask out non valid row and col indices\n    row_mask = (row_indices >= 0) * (row_indices < source_image.shape[0])\n    col_mask = (col_indices >= 0) * (col_indices < source_image.shape[1])\n    valid_rows = row_indices * row_mask\n    valid_cols = col_indices * col_mask\n\n    # free memory\n    del(row_indices)\n    del(col_indices)\n\n    # get valid part of image\n    target_image = source_image[valid_rows, valid_cols]\n\n    # free memory\n    del(valid_rows)\n    del(valid_cols)\n\n    # create mask for valid data points\n    valid_data = row_mask * col_mask\n    if valid_data.ndim != target_image.ndim:\n        for i in range(target_image.ndim - valid_data.ndim):\n            valid_data = np.expand_dims(valid_data, axis=valid_data.ndim)\n\n    # free memory\n    del(row_mask)\n    del(col_mask)\n\n    # fill the non valid part of the image\n    if fill_value is not None:\n        target_filled = (target_image * valid_data +\n                         (1 - valid_data) * fill_value)\n    else:\n        if np.ma.is_masked(target_image):\n            mask = ((1 - valid_data) | target_image.mask)\n        else:\n            mask = (1 - valid_data)\n        target_filled = np.ma.array(target_image, mask=mask)\n\n    return target_filled.astype(target_image.dtype)\n\n\ndef get_linesample(lons, lats, source_area_def, nprocs=1):\n    """"""Returns index row and col arrays for resampling\n\n    Parameters\n    ----------\n    lons : numpy array\n        Lons. Dimensions must match lats\n    lats : numpy array\n        Lats. Dimensions must match lons\n    source_area_def : object\n        Source definition as AreaDefinition object\n    nprocs : int, optional\n        Number of processor cores to be used\n\n    Returns\n    -------\n    (row_indices, col_indices) : tuple of numpy arrays\n        Arrays for resampling area by array indexing\n    """"""\n\n    # Proj.4 definition of source area projection\n    if nprocs > 1:\n        source_proj = _spatial_mp.Proj_MP(**source_area_def.proj_dict)\n    else:\n        source_proj = _spatial_mp.Proj(**source_area_def.proj_dict)\n\n    # get cartesian projection values from longitude and latitude\n    source_x, source_y = source_proj(lons, lats, nprocs=nprocs)\n\n    # Find corresponding pixels (element by element conversion of ndarrays)\n    source_pixel_x = (source_area_def.pixel_offset_x +\n                      source_x / source_area_def.pixel_size_x).astype(np.int32)\n\n    source_pixel_y = (source_area_def.pixel_offset_y -\n                      source_y / source_area_def.pixel_size_y).astype(np.int32)\n\n    return source_pixel_y, source_pixel_x\n\n\ndef get_image_from_lonlats(lons, lats, source_area_def, source_image_data,\n                           fill_value=0, nprocs=1):\n    """"""Samples from image based on lon lat arrays\n    using nearest neighbour method in cartesian projection coordinate systems.\n\n    Parameters\n    ----------\n    lons : numpy array\n        Lons. Dimensions must match lats\n    lats : numpy array\n        Lats. Dimensions must match lons\n    source_area_def : object\n        Source definition as AreaDefinition object\n    source_image_data : numpy array\n        Source image data\n    fill_value : int or None, optional\n            Set undetermined pixels to this value.\n            If fill_value is None a masked array is returned\n            with undetermined pixels masked\n    nprocs : int, optional\n        Number of processor cores to be used\n\n    Returns\n    -------\n    image_data : numpy array\n        Resampled image data\n    """"""\n\n    source_pixel_y, source_pixel_x = get_linesample(lons, lats,\n                                                    source_area_def,\n                                                    nprocs=nprocs)\n\n    # Return target image\n    return get_image_from_linesample(source_pixel_y, source_pixel_x,\n                                     source_image_data, fill_value)\n\n\ndef get_resampled_image(target_area_def, source_area_def, source_image_data,\n                        fill_value=0, nprocs=1, segments=None):\n    """"""Resamples image using nearest neighbour method in cartesian\n    projection coordinate systems.\n\n    Parameters\n    ----------\n    target_area_def : object\n        Target definition as AreaDefinition object\n    source_area_def : object\n        Source definition as AreaDefinition object\n    source_image_data : numpy array\n        Source image data\n    fill_value : {int, None} optional\n        Set undetermined pixels to this value.\n        If fill_value is None a masked array is returned\n        with undetermined pixels masked\n    nprocs : int, optional\n        Number of processor cores to be used\n    segments : {int, None} optional\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated.\n\n    Returns\n    -------\n    image_data : numpy array\n        Resampled image data\n    """"""\n\n    if not isinstance(target_area_def, geometry.AreaDefinition):\n        raise TypeError(\'target_area_def must be of type AreaDefinition\')\n    if not isinstance(source_area_def, geometry.AreaDefinition):\n        raise TypeError(\'source_area_def must be of type AreaDefinition\')\n    if not isinstance(source_image_data, (np.ndarray,\n                                          np.ma.core.MaskedArray)):\n        raise TypeError(\'source_image must be of type ndarray\'\n                        \' or a masked array.\')\n\n    # Calculate number of segments if needed\n    if segments is None:\n        rows = target_area_def.height\n        cut_off = 500\n        if rows > cut_off:\n            segments = int(rows / cut_off)\n        else:\n            segments = 1\n\n    if segments > 1:\n        # Iterate through segments\n        for i, target_slice in enumerate(geometry._get_slice(segments,\n                                                             target_area_def.shape)):\n\n            # Select data from segment with slice\n            lons, lats = target_area_def.get_lonlats(\n                nprocs=nprocs, data_slice=target_slice)\n\n            # Calculate partial result\n            next_result = get_image_from_lonlats(lons, lats, source_area_def,\n                                                 source_image_data,\n                                                 fill_value, nprocs)\n\n            # Build result iteratively\n            if i == 0:\n                # First iteration\n                result = next_result\n            else:\n                if isinstance(next_result, np.ma.core.MaskedArray):\n                    stack = np.ma.row_stack\n                else:\n                    stack = np.row_stack\n                result = stack((result, next_result))\n\n        return result\n    else:\n        # Get lon lat arrays of target area\n        lons, lats = target_area_def.get_lonlats(nprocs)\n        # Get target image\n        return get_image_from_lonlats(lons, lats, source_area_def,\n                                      source_image_data, fill_value, nprocs)\n'"
pyresample/image.py,2,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010, 2015  Esben S. Nielsen\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Handles resampling of images with assigned geometry definitions""""""\n\nfrom __future__ import absolute_import\n\nimport numpy as np\n\nfrom pyresample import geometry, grid, kd_tree, bilinear\n\n\nclass ImageContainer(object):\n\n    """"""Holds image with geometry definition.\n    Allows indexing with linesample arrays.\n\n    Parameters\n    ----------\n    image_data : numpy array\n        Image data\n    geo_def : object\n        Geometry definition\n    fill_value : int or None, optional\n        Set undetermined pixels to this value.\n        If fill_value is None a masked array is returned\n        with undetermined pixels masked\n    nprocs : int, optional\n        Number of processor cores to be used\n\n    Attributes\n    ----------\n    image_data : numpy array\n        Image data\n    geo_def : object\n        Geometry definition\n    fill_value : int or None\n        Resample result fill value\n    nprocs : int\n        Number of processor cores to be used for geometry operations\n    """"""\n\n    def __init__(self, image_data, geo_def, fill_value=0, nprocs=1):\n        if type(geo_def).__name__ == ""DynamicAreaDefinition"":\n            geo_def = geo_def.freeze()\n        if not isinstance(image_data, (np.ndarray, np.ma.core.MaskedArray)):\n            raise TypeError(\'image_data must be either an ndarray\'\n                            \' or a masked array\')\n        elif ((image_data.ndim > geo_def.ndim + 1) or\n              (image_data.ndim < geo_def.ndim)):\n            raise ValueError((\'Unexpected number of dimensions for \'\n                              \'image_data: %s\') % image_data.ndim)\n        for i, size in enumerate(geo_def.shape):\n            if image_data.shape[i] != size:\n                raise ValueError((\'Size mismatch for image_data. Expected \'\n                                  \'size %s for dimension %s and got %s\') %\n                                 (size, i, image_data.shape[i]))\n\n        self.shape = geo_def.shape\n        self.size = geo_def.size\n        self.ndim = geo_def.ndim\n        self.image_data = image_data\n        if image_data.ndim > geo_def.ndim:\n            self.channels = image_data.shape[-1]\n        else:\n            self.channels = 1\n        self.geo_def = geo_def\n        self.fill_value = fill_value\n        self.nprocs = nprocs\n\n    def __str__(self):\n        return \'Image:\\n %s\' % self.image_data.__str__()\n\n    def __repr__(self):\n        return self.image_data.__repr__()\n\n    def resample(self, target_geo_def):\n        """"""Base method for resampling""""""\n\n        raise NotImplementedError(\'Method ""resample"" is not implemented \'\n                                  \'in class %s\' % self.__class__.__name__)\n\n    def get_array_from_linesample(self, row_indices, col_indices):\n        """"""Samples from image based on index arrays.\n\n        Parameters\n        ----------\n        row_indices : numpy array\n            Row indices. Dimensions must match col_indices\n        col_indices : numpy array\n            Col indices. Dimensions must match row_indices\n\n        Returns\n        -------\n        image_data : numpy_array\n            Resampled image data\n        """"""\n\n        if self.geo_def.ndim != 2:\n            raise TypeError(\'Resampling from linesamples only makes sense \'\n                            \'on 2D data\')\n\n        return grid.get_image_from_linesample(row_indices, col_indices,\n                                              self.image_data,\n                                              self.fill_value)\n\n    def get_array_from_neighbour_info(self, *args, **kwargs):\n        """"""Base method for resampling from preprocessed data.""""""\n\n        raise NotImplementedError(\'Method ""get_array_from_neighbour_info"" is \'\n                                  \'not implemented in class %s\' %\n                                  self.__class__.__name__)\n\n\nclass ImageContainerQuick(ImageContainer):\n\n    """"""Holds image with area definition. \'\n    Allows quick resampling within area.\n\n    Parameters\n    ----------\n    image_data : numpy array\n        Image data\n    geo_def : object\n        Area definition as AreaDefinition object\n    fill_value : int or None, optional\n        Set undetermined pixels to this value.\n        If fill_value is None a masked array is returned\n        with undetermined pixels masked\n    nprocs : int, optional\n        Number of processor cores to be used for geometry operations\n    segments : int or None\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n\n    Attributes\n    ----------\n    image_data : numpy array\n        Image data\n    geo_def : object\n        Area definition as AreaDefinition object\n    fill_value : int or None\n        Resample result fill value\n        If fill_value is None a masked array is returned\n        with undetermined pixels masked\n    nprocs : int\n        Number of processor cores to be used\n    segments : int or None\n        Number of segments to use when resampling\n    """"""\n\n    def __init__(self, image_data, geo_def, fill_value=0, nprocs=1,\n                 segments=None):\n        if not isinstance(geo_def, geometry.AreaDefinition):\n            raise TypeError(\'area_def must be of type \'\n                            \'geometry.AreaDefinition\')\n        super(ImageContainerQuick, self).__init__(image_data, geo_def,\n                                                  fill_value=fill_value,\n                                                  nprocs=nprocs)\n        self.segments = segments\n\n    def resample(self, target_area_def):\n        """"""Resamples image to area definition using nearest neighbour\n        approach in projection coordinates.\n\n        Parameters\n        ----------\n        target_area_def : object\n            Target area definition as AreaDefinition object\n\n        Returns\n        -------\n        image_container : object\n            ImageContainerQuick object of resampled area\n        """"""\n\n        resampled_image = grid.get_resampled_image(target_area_def,\n                                                   self.geo_def,\n                                                   self.image_data,\n                                                   fill_value=self.fill_value,\n                                                   nprocs=self.nprocs,\n                                                   segments=self.segments)\n\n        return ImageContainerQuick(resampled_image, target_area_def,\n                                   fill_value=self.fill_value,\n                                   nprocs=self.nprocs, segments=self.segments)\n\n\nclass ImageContainerNearest(ImageContainer):\n\n    """"""Holds image with geometry definition.\n    Allows nearest neighbour to new geometry definition.\n\n    Parameters\n    ----------\n    image_data : numpy array\n        Image data\n    geo_def : object\n        Geometry definition\n    radius_of_influence : float\n        Cut off distance in meters\n    epsilon : float, optional\n        Allowed uncertainty in meters. Increasing uncertainty\n        reduces execution time\n    fill_value : int or None, optional\n        Set undetermined pixels to this value.\n        If fill_value is None a masked array is returned\n        with undetermined pixels masked\n    reduce_data : bool, optional\n        Perform coarse data reduction before resampling in order\n        to reduce execution time\n    nprocs : int, optional\n        Number of processor cores to be used for geometry operations\n    segments : int or None\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n\n    Attributes\n    ----------\n\n    image_data : numpy array\n        Image data\n    geo_def : object\n        Geometry definition\n    radius_of_influence : float\n        Cut off distance in meters\n    epsilon : float\n        Allowed uncertainty in meters\n    fill_value : int or None\n        Resample result fill value\n    reduce_data : bool\n        Perform coarse data reduction before resampling\n    nprocs : int\n        Number of processor cores to be used\n    segments : int or None\n        Number of segments to use when resampling\n    """"""\n\n    def __init__(self, image_data, geo_def, radius_of_influence, epsilon=0,\n                 fill_value=0, reduce_data=True, nprocs=1, segments=None):\n        super(ImageContainerNearest, self).__init__(image_data, geo_def,\n                                                    fill_value=fill_value,\n                                                    nprocs=nprocs)\n        self.radius_of_influence = radius_of_influence\n        self.epsilon = epsilon\n        self.reduce_data = reduce_data\n        self.segments = segments\n\n    def resample(self, target_geo_def):\n        """"""Resamples image to area definition using nearest neighbour\n        approach\n\n        Parameters\n        ----------\n        target_geo_def : object\n            Target geometry definition\n\n        Returns\n        -------\n        image_container : object\n            ImageContainerNearest object of resampled geometry\n        """"""\n\n        if self.image_data.ndim > 2 and self.ndim > 1:\n            image_data = self.image_data.reshape(self.image_data.shape[0] *\n                                                 self.image_data.shape[1],\n                                                 self.image_data.shape[2])\n        else:\n            image_data = self.image_data.ravel()\n\n        resampled_image = \\\n            kd_tree.resample_nearest(self.geo_def,\n                                     image_data,\n                                     target_geo_def,\n                                     self.radius_of_influence,\n                                     epsilon=self.epsilon,\n                                     fill_value=self.fill_value,\n                                     nprocs=self.nprocs,\n                                     reduce_data=self.reduce_data,\n                                     segments=self.segments)\n        return ImageContainerNearest(resampled_image, target_geo_def,\n                                     self.radius_of_influence,\n                                     epsilon=self.epsilon,\n                                     fill_value=self.fill_value,\n                                     reduce_data=self.reduce_data,\n                                     nprocs=self.nprocs,\n                                     segments=self.segments)\n\n\nclass ImageContainerBilinear(ImageContainer):\n\n    """"""Holds image with geometry definition.\n    Allows bilinear to new geometry definition.\n\n    Parameters\n    ----------\n    image_data : numpy array\n        Image data\n    geo_def : object\n        Geometry definition\n    radius_of_influence : float\n        Cut off distance in meters\n    epsilon : float, optional\n        Allowed uncertainty in meters. Increasing uncertainty\n        reduces execution time\n    fill_value : int or None, optional\n        Set undetermined pixels to this value.\n        If fill_value is None a masked array is returned\n        with undetermined pixels masked\n    reduce_data : bool, optional\n        Perform coarse data reduction before resampling in order\n        to reduce execution time\n    nprocs : int, optional\n        Number of processor cores to be used for geometry operations\n    segments : int or None\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n\n    Attributes\n    ----------\n\n    image_data : numpy array\n        Image data\n    geo_def : object\n        Geometry definition\n    radius_of_influence : float\n        Cut off distance in meters\n    epsilon : float\n        Allowed uncertainty in meters\n    fill_value : int or None\n        Resample result fill value\n    reduce_data : bool\n        Perform coarse data reduction before resampling\n    nprocs : int\n        Number of processor cores to be used\n    segments : int or None\n        Number of segments to use when resampling\n    """"""\n\n    def __init__(self, image_data, geo_def, radius_of_influence, epsilon=0,\n                 fill_value=0, reduce_data=False, nprocs=1, segments=None,\n                 neighbours=32):\n        super(ImageContainerBilinear, self).__init__(image_data, geo_def,\n                                                     fill_value=fill_value,\n                                                     nprocs=nprocs)\n        self.radius_of_influence = radius_of_influence\n        self.epsilon = epsilon\n        self.reduce_data = reduce_data\n        self.segments = segments\n        self.neighbours = neighbours\n\n    def resample(self, target_geo_def):\n        """"""Resamples image to area definition using bilinear approach\n\n        Parameters\n        ----------\n        target_geo_def : object\n            Target geometry definition\n\n        Returns\n        -------\n        image_container : object\n            ImageContainerBilinear object of resampled geometry\n        """"""\n\n        if self.image_data.ndim > 2 and self.ndim > 1:\n            image_data = self.image_data.reshape(self.image_data.shape[0] *\n                                                 self.image_data.shape[1],\n                                                 self.image_data.shape[2])\n        else:\n            image_data = self.image_data.ravel()\n\n        try:\n            mask = image_data.mask.copy()\n            image_data = image_data.data.copy()\n            image_data[mask] = np.nan\n        except AttributeError:\n            pass\n\n        resampled_image = \\\n            bilinear.resample_bilinear(image_data,\n                                       self.geo_def,\n                                       target_geo_def,\n                                       radius=self.radius_of_influence,\n                                       neighbours=self.neighbours,\n                                       epsilon=self.epsilon,\n                                       fill_value=self.fill_value,\n                                       nprocs=self.nprocs,\n                                       reduce_data=self.reduce_data,\n                                       segments=self.segments)\n        try:\n            resampled_image = resampled_image.reshape(target_geo_def.shape)\n        except ValueError:\n            # The input data was 3D\n            shp = target_geo_def.shape\n            new_shp = [shp[0], shp[1], image_data.shape[-1]]\n            resampled_image = resampled_image.reshape(new_shp)\n\n        return ImageContainerBilinear(resampled_image, target_geo_def,\n                                      self.radius_of_influence,\n                                      epsilon=self.epsilon,\n                                      fill_value=self.fill_value,\n                                      reduce_data=self.reduce_data,\n                                      nprocs=self.nprocs,\n                                      segments=self.segments)\n'"
pyresample/kd_tree.py,70,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010, 2014, 2015  Esben S. Nielsen\n#                           Adam.Dybbroe\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Handles reprojection of geolocated data. Several types of resampling are\nsupported""""""\n\nfrom __future__ import absolute_import\n\nimport sys\nimport types\nimport warnings\nfrom copy import deepcopy\nfrom logging import getLogger\n\nimport numpy as np\n\nfrom pykdtree.kdtree import KDTree\nfrom pyresample import CHUNK_SIZE, _spatial_mp, data_reduce, geometry\n\nlogger = getLogger(__name__)\n\ntry:\n    from xarray import DataArray\n    import dask.array as da\n    import dask\n    if hasattr(dask, \'blockwise\'):\n        blockwise = da.blockwise\n    else:\n        blockwise = da.atop\nexcept ImportError:\n    DataArray = None\n    da = None\n    dask = None\n\nif sys.version >= \'3\':\n    long = int\n\n\nclass EmptyResult(ValueError):\n    pass\n\n\ndef resample_nearest(source_geo_def,\n                     data,\n                     target_geo_def,\n                     radius_of_influence,\n                     epsilon=0,\n                     fill_value=0,\n                     reduce_data=True,\n                     nprocs=1,\n                     segments=None):\n    """"""Resamples data using kd-tree nearest neighbour approach\n\n    Parameters\n    ----------\n    source_geo_def : object\n        Geometry definition of source\n    data : numpy array\n        1d array of single channel data points or\n        (source_size, k) array of k channels of datapoints\n    target_geo_def : object\n        Geometry definition of target\n    radius_of_influence : float\n        Cut off distance in meters\n    epsilon : float, optional\n        Allowed uncertainty in meters. Increasing uncertainty\n        reduces execution time\n    fill_value : int, float, numpy floating, numpy integer or None, optional\n            Set undetermined pixels to this value.\n            If fill_value is None a masked array is returned\n            with undetermined pixels masked\n    reduce_data : bool, optional\n        Perform initial coarse reduction of source dataset in order\n        to reduce execution time\n    nprocs : int, optional\n        Number of processor cores to be used\n    segments : int or None\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n\n    Returns\n    -------\n    data : numpy array\n        Source data resampled to target geometry\n    """"""\n\n    return _resample(source_geo_def, data, target_geo_def, \'nn\',\n                     radius_of_influence, neighbours=1,\n                     epsilon=epsilon, fill_value=fill_value,\n                     reduce_data=reduce_data, nprocs=nprocs, segments=segments)\n\n\ndef resample_gauss(source_geo_def, data, target_geo_def,\n                   radius_of_influence, sigmas, neighbours=8, epsilon=0,\n                   fill_value=0, reduce_data=True, nprocs=1, segments=None,\n                   with_uncert=False):\n    """"""Resamples data using kd-tree gaussian weighting neighbour approach.\n\n    Parameters\n    ----------\n    source_geo_def : object\n        Geometry definition of source\n    data : numpy array\n        Array of single channel data points or\n        (source_geo_def.shape, k) array of k channels of datapoints\n    target_geo_def : object\n        Geometry definition of target\n    radius_of_influence : float\n        Cut off distance in meters\n    sigmas : list of floats or float\n        List of sigmas to use for the gauss weighting of each\n        channel 1 to k, w_k = exp(-dist^2/sigma_k^2).\n        If only one channel is resampled sigmas is a single float value.\n    neighbours : int, optional\n        The number of neigbours to consider for each grid point\n    epsilon : float, optional\n        Allowed uncertainty in meters. Increasing uncertainty\n        reduces execution time\n    fill_value : {int, None}, optional\n            Set undetermined pixels to this value.\n            If fill_value is None a masked array is returned\n            with undetermined pixels masked\n    reduce_data : bool, optional\n        Perform initial coarse reduction of source dataset in order\n        to reduce execution time\n    nprocs : int, optional\n        Number of processor cores to be used\n    segments : int or None\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n    with_uncert : bool, optional\n        Calculate uncertainty estimates\n\n    Returns\n    -------\n    data : numpy array (default)\n        Source data resampled to target geometry\n    data, stddev, counts : numpy array, numpy array, numpy array (if with_uncert == True)\n        Source data resampled to target geometry.\n        Weighted standard devaition for all pixels having more than one source value\n        Counts of number of source values used in weighting per pixel\n\n    """"""\n    def gauss(sigma):\n        # Return gauss function object\n        return lambda r: np.exp(-r ** 2 / float(sigma) ** 2)\n\n    # Build correct sigma argument\n    is_multi_channel = False\n    try:\n        sigmas.__iter__()\n        sigma_list = sigmas\n        is_multi_channel = True\n    except AttributeError:\n        sigma_list = [sigmas]\n\n    for sigma in sigma_list:\n        if not isinstance(sigma, (long, int, float)):\n            raise TypeError(\'sigma must be number\')\n\n    # Get gauss function objects\n    if is_multi_channel:\n        weight_funcs = list(map(gauss, sigma_list))\n    else:\n        weight_funcs = gauss(sigmas)\n\n    return _resample(source_geo_def, data, target_geo_def, \'custom\',\n                     radius_of_influence, neighbours=neighbours,\n                     epsilon=epsilon, weight_funcs=weight_funcs, fill_value=fill_value,\n                     reduce_data=reduce_data, nprocs=nprocs, segments=segments, with_uncert=with_uncert)\n\n\ndef resample_custom(source_geo_def, data, target_geo_def,\n                    radius_of_influence, weight_funcs, neighbours=8,\n                    epsilon=0, fill_value=0, reduce_data=True, nprocs=1,\n                    segments=None, with_uncert=False):\n    """"""Resamples data using kd-tree custom radial weighting neighbour approach\n\n    Parameters\n    ----------\n    source_geo_def : object\n        Geometry definition of source\n    data : numpy array\n        Array of single channel data points or\n        (source_geo_def.shape, k) array of k channels of datapoints\n    target_geo_def : object\n        Geometry definition of target\n    radius_of_influence : float\n        Cut off distance in meters\n    weight_funcs : list of function objects or function object\n        List of weight functions f(dist) to use for the weighting\n        of each channel 1 to k.\n        If only one channel is resampled weight_funcs is\n        a single function object.\n    neighbours : int, optional\n        The number of neigbours to consider for each grid point\n    epsilon : float, optional\n        Allowed uncertainty in meters. Increasing uncertainty\n        reduces execution time\n    fill_value : {int, None}, optional\n            Set undetermined pixels to this value.\n            If fill_value is None a masked array is returned\n            with undetermined pixels masked\n    reduce_data : bool, optional\n        Perform initial coarse reduction of source dataset in order\n        to reduce execution time\n    nprocs : int, optional\n        Number of processor cores to be used\n    segments : {int, None}\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n\n    Returns\n    -------\n    data : numpy array (default)\n        Source data resampled to target geometry\n    data, stddev, counts : numpy array, numpy array, numpy array (if with_uncert == True)\n        Source data resampled to target geometry.\n        Weighted standard devaition for all pixels having more than one source value\n        Counts of number of source values used in weighting per pixel\n    """"""\n\n    if not isinstance(weight_funcs, (list, tuple)):\n        if not isinstance(weight_funcs, types.FunctionType):\n            raise TypeError(\'weight_func must be function object\')\n    else:\n        for weight_func in weight_funcs:\n            if not isinstance(weight_func, types.FunctionType):\n                raise TypeError(\'weight_func must be function object\')\n\n    return _resample(source_geo_def, data, target_geo_def, \'custom\',\n                     radius_of_influence, neighbours=neighbours,\n                     epsilon=epsilon, weight_funcs=weight_funcs,\n                     fill_value=fill_value, reduce_data=reduce_data,\n                     nprocs=nprocs, segments=segments, with_uncert=with_uncert)\n\n\ndef _resample(source_geo_def, data, target_geo_def, resample_type,\n              radius_of_influence, neighbours=8, epsilon=0, weight_funcs=None,\n              fill_value=0, reduce_data=True, nprocs=1, segments=None, with_uncert=False):\n    """"""Resamples swath using kd-tree approach""""""\n\n    valid_input_index, valid_output_index, index_array, distance_array = \\\n        get_neighbour_info(source_geo_def,\n                           target_geo_def,\n                           radius_of_influence,\n                           neighbours=neighbours,\n                           epsilon=epsilon,\n                           reduce_data=reduce_data,\n                           nprocs=nprocs,\n                           segments=segments)\n\n    return get_sample_from_neighbour_info(resample_type,\n                                          target_geo_def.shape,\n                                          data, valid_input_index,\n                                          valid_output_index,\n                                          index_array,\n                                          distance_array=distance_array,\n                                          weight_funcs=weight_funcs,\n                                          fill_value=fill_value,\n                                          with_uncert=with_uncert)\n\n\ndef get_neighbour_info(source_geo_def, target_geo_def, radius_of_influence,\n                       neighbours=8, epsilon=0, reduce_data=True,\n                       nprocs=1, segments=None):\n    """"""Returns neighbour info\n\n    Parameters\n    ----------\n    source_geo_def : object\n        Geometry definition of source\n    target_geo_def : object\n        Geometry definition of target\n    radius_of_influence : float\n        Cut off distance in meters\n    neighbours : int, optional\n        The number of neigbours to consider for each grid point\n    epsilon : float, optional\n        Allowed uncertainty in meters. Increasing uncertainty\n        reduces execution time\n    reduce_data : bool, optional\n        Perform initial coarse reduction of source dataset in order\n        to reduce execution time\n    nprocs : int, optional\n        Number of processor cores to be used\n    segments : int or None\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n\n    Returns\n    -------\n    (valid_input_index, valid_output_index,\n    index_array, distance_array) : tuple of numpy arrays\n        Neighbour resampling info\n    """"""\n\n    if source_geo_def.size < neighbours:\n        warnings.warn(\'Searching for %s neighbours in %s data points\' %\n                      (neighbours, source_geo_def.size))\n\n    if segments is None:\n        cut_off = 3000000\n        if target_geo_def.size > cut_off:\n            segments = int(target_geo_def.size / cut_off)\n        else:\n            segments = 1\n\n    # Find reduced input coordinate set\n    valid_input_index, source_lons, source_lats = _get_valid_input_index(source_geo_def, target_geo_def,\n                                                                         reduce_data,\n                                                                         radius_of_influence,\n                                                                         nprocs=nprocs)\n\n    # Create kd-tree\n    try:\n        resample_kdtree = _create_resample_kdtree(source_lons, source_lats,\n                                                  valid_input_index,\n                                                  nprocs=nprocs)\n    except EmptyResult:\n        # Handle if all input data is reduced away\n        valid_output_index, index_array, distance_array = \\\n            _create_empty_info(source_geo_def, target_geo_def, neighbours)\n        return (valid_input_index, valid_output_index, index_array,\n                distance_array)\n\n    if segments > 1:\n        # Iterate through segments\n        for i, target_slice in enumerate(geometry._get_slice(segments,\n                                                             target_geo_def.shape)):\n\n            # Query on slice of target coordinates\n            next_voi, next_ia, next_da = \\\n                _query_resample_kdtree(resample_kdtree, source_geo_def,\n                                       target_geo_def,\n                                       radius_of_influence, target_slice,\n                                       neighbours=neighbours,\n                                       epsilon=epsilon,\n                                       reduce_data=reduce_data,\n                                       nprocs=nprocs)\n\n            # Build result iteratively\n            if i == 0:\n                # First iteration\n                valid_output_index = next_voi\n                index_array = next_ia\n                distance_array = next_da\n            else:\n                valid_output_index = np.append(valid_output_index, next_voi)\n                if neighbours > 1:\n                    index_array = np.row_stack((index_array, next_ia))\n                    distance_array = np.row_stack((distance_array, next_da))\n                else:\n                    index_array = np.append(index_array, next_ia)\n                    distance_array = np.append(distance_array, next_da)\n    else:\n        # Query kd-tree with full target coordinate set\n        full_slice = slice(None)\n        valid_output_index, index_array, distance_array = \\\n            _query_resample_kdtree(resample_kdtree, source_geo_def,\n                                   target_geo_def,\n                                   radius_of_influence, full_slice,\n                                   neighbours=neighbours,\n                                   epsilon=epsilon,\n                                   reduce_data=reduce_data,\n                                   nprocs=nprocs)\n\n    # Check if number of neighbours is potentially too low\n    if neighbours > 1:\n        if not np.all(np.isinf(distance_array[:, -1])):\n            warnings.warn((\'Possible more than %s neighbours \'\n                           \'within %s m for some data points\') %\n                          (neighbours, radius_of_influence))\n\n    return valid_input_index, valid_output_index, index_array, distance_array\n\n\ndef _get_valid_input_index(source_geo_def,\n                           target_geo_def,\n                           reduce_data,\n                           radius_of_influence,\n                           nprocs=1):\n    """"""Find indices of reduced inputput data""""""\n\n    source_lons, source_lats = source_geo_def.get_lonlats(nprocs=nprocs)\n    source_lons = np.asanyarray(source_lons).ravel()\n    source_lats = np.asanyarray(source_lats).ravel()\n\n    if source_lons.size == 0 or source_lats.size == 0:\n        raise ValueError(\'Cannot resample empty data set\')\n    elif source_lons.size != source_lats.size or \\\n            source_lons.shape != source_lats.shape:\n        raise ValueError(\'Mismatch between lons and lats\')\n\n    # Remove illegal values\n    valid_input_index = ((source_lons >= -180) & (source_lons <= 180) &\n                         (source_lats <= 90) & (source_lats >= -90))\n\n    if reduce_data:\n        # Reduce dataset\n        if (isinstance(source_geo_def, geometry.CoordinateDefinition) and\n            isinstance(target_geo_def, (geometry.GridDefinition,\n                                        geometry.AreaDefinition))) or \\\n           (isinstance(source_geo_def, (geometry.GridDefinition,\n                                        geometry.AreaDefinition)) and\n            isinstance(target_geo_def, (geometry.GridDefinition,\n                                        geometry.AreaDefinition))):\n            # Resampling from swath to grid or from grid to grid\n            lonlat_boundary = target_geo_def.get_boundary_lonlats()\n\n            # Combine reduced and legal values\n            valid_input_index &= \\\n                data_reduce.get_valid_index_from_lonlat_boundaries(\n                    lonlat_boundary[0],\n                    lonlat_boundary[1],\n                    source_lons, source_lats,\n                    radius_of_influence)\n\n    if isinstance(valid_input_index, np.ma.core.MaskedArray):\n        # Make sure valid_input_index is not a masked array\n        valid_input_index = valid_input_index.filled(False)\n\n    return valid_input_index, source_lons, source_lats\n\n\ndef _get_valid_output_index(source_geo_def, target_geo_def, target_lons,\n                            target_lats, reduce_data, radius_of_influence):\n    """"""Find indices of reduced output data""""""\n\n    valid_output_index = np.ones(target_lons.size, dtype=np.bool)\n\n    if reduce_data:\n        if isinstance(source_geo_def, (geometry.GridDefinition,\n                                       geometry.AreaDefinition)) and \\\n                isinstance(target_geo_def, geometry.CoordinateDefinition):\n            # Resampling from grid to swath\n            lonlat_boundary = source_geo_def.get_boundary_lonlats()\n            valid_output_index = \\\n                data_reduce.get_valid_index_from_lonlat_boundaries(\n                    lonlat_boundary[0],\n                    lonlat_boundary[1],\n                    target_lons,\n                    target_lats,\n                    radius_of_influence)\n            valid_output_index = valid_output_index.astype(np.bool)\n\n    # Remove illegal values\n    valid_out = ((target_lons >= -180) & (target_lons <= 180) &\n                 (target_lats <= 90) & (target_lats >= -90))\n\n    # Combine reduced and legal values\n    valid_output_index = (valid_output_index & valid_out)\n    if isinstance(valid_output_index, np.ma.MaskedArray):\n        valid_output_index = valid_output_index.filled(False)\n\n    return valid_output_index\n\n\ndef _create_resample_kdtree(source_lons,\n                            source_lats,\n                            valid_input_index,\n                            nprocs=1):\n    """"""Set up kd tree on input""""""\n    """"""\n    if not isinstance(source_geo_def, geometry.BaseDefinition):\n        raise TypeError(\'source_geo_def must be of geometry type\')\n\n    #Get reduced cartesian coordinates and flatten them\n    source_cartesian_coords = source_geo_def.get_cartesian_coords(nprocs=nprocs)\n    input_coords = geometry._flatten_cartesian_coords(source_cartesian_coords)\n    input_coords = input_coords[valid_input_index]\n    """"""\n\n    source_lons_valid = source_lons[valid_input_index]\n    source_lats_valid = source_lats[valid_input_index]\n\n    if nprocs > 1:\n        cartesian = _spatial_mp.Cartesian_MP(nprocs)\n    else:\n        cartesian = _spatial_mp.Cartesian()\n\n    input_coords = cartesian.transform_lonlats(source_lons_valid,\n                                               source_lats_valid)\n\n    if input_coords.size == 0:\n        raise EmptyResult(\'No valid data points in input data\')\n\n    # Build kd-tree on input\n    if nprocs > 1:\n        resample_kdtree = _spatial_mp.cKDTree_MP(input_coords, nprocs=nprocs)\n    else:\n        resample_kdtree = KDTree(input_coords)\n\n    return resample_kdtree\n\n\ndef _query_resample_kdtree(resample_kdtree,\n                           source_geo_def,\n                           target_geo_def,\n                           radius_of_influence,\n                           data_slice,\n                           neighbours=8,\n                           epsilon=0,\n                           reduce_data=True,\n                           nprocs=1):\n    """"""Query kd-tree on slice of target coordinates""""""\n\n    # Check validity of input\n    if not isinstance(target_geo_def, geometry.BaseDefinition):\n        raise TypeError(\'target_geo_def must be of geometry type\')\n    elif not isinstance(radius_of_influence, (long, int, float)):\n        raise TypeError(\'radius_of_influence must be number\')\n    elif not isinstance(neighbours, int):\n        raise TypeError(\'neighbours must be integer\')\n    elif not isinstance(epsilon, (long, int, float)):\n        raise TypeError(\'epsilon must be number\')\n\n    # Get sliced target coordinates\n    target_lons, target_lats = target_geo_def.get_lonlats(nprocs=nprocs,\n                                                          data_slice=data_slice, dtype=source_geo_def.dtype)\n\n    # Find indiced of reduced target coordinates\n    valid_output_index = _get_valid_output_index(source_geo_def,\n                                                 target_geo_def,\n                                                 target_lons.ravel(),\n                                                 target_lats.ravel(),\n                                                 reduce_data,\n                                                 radius_of_influence)\n\n    # Get cartesian target coordinates and select reduced set\n    if nprocs > 1:\n        cartesian = _spatial_mp.Cartesian_MP(nprocs)\n    else:\n        cartesian = _spatial_mp.Cartesian()\n\n    target_lons_valid = target_lons.ravel()[valid_output_index]\n    target_lats_valid = target_lats.ravel()[valid_output_index]\n\n    output_coords = cartesian.transform_lonlats(target_lons_valid,\n                                                target_lats_valid)\n\n    # pykdtree requires query points have same data type as kdtree.\n    try:\n        dt = resample_kdtree.data.dtype\n    except AttributeError:\n        # use a sensible default\n        dt = np.dtype(\'d\')\n    output_coords = np.asarray(output_coords, dtype=dt)\n\n    # Query kd-tree\n    distance_array, index_array = resample_kdtree.query(output_coords,\n                                                        k=neighbours,\n                                                        eps=epsilon,\n                                                        distance_upper_bound=radius_of_influence)\n\n    return valid_output_index, index_array, distance_array\n\n\ndef _create_empty_info(source_geo_def, target_geo_def, neighbours):\n    """"""Creates dummy info for empty result set""""""\n\n    valid_output_index = np.ones(target_geo_def.size, dtype=np.bool)\n    if neighbours > 1:\n        index_array = (np.ones((target_geo_def.size, neighbours),\n                               dtype=np.int32) * source_geo_def.size)\n        distance_array = np.ones((target_geo_def.size, neighbours))\n    else:\n        index_array = (np.ones(target_geo_def.size, dtype=np.int32) *\n                       source_geo_def.size)\n        distance_array = np.ones(target_geo_def.size)\n\n    return valid_output_index, index_array, distance_array\n\n\ndef get_sample_from_neighbour_info(resample_type, output_shape, data,\n                                   valid_input_index, valid_output_index,\n                                   index_array, distance_array=None,\n                                   weight_funcs=None, fill_value=0,\n                                   with_uncert=False):\n    """"""Resamples swath based on neighbour info\n\n    Parameters\n    ----------\n    resample_type : {\'nn\', \'custom\'}\n        \'nn\': Use nearest neighbour resampling\n        \'custom\': Resample based on weight_funcs\n    output_shape : (int, int)\n        Shape of output as (rows, cols)\n    data : numpy array\n        Source data\n    valid_input_index : numpy array\n        valid_input_index from get_neighbour_info\n    valid_output_index : numpy array\n        valid_output_index from get_neighbour_info\n    index_array : numpy array\n        index_array from get_neighbour_info\n    distance_array : numpy array, optional\n        distance_array from get_neighbour_info\n        Not needed for \'nn\' resample type\n    weight_funcs : list of function objects or function object, optional\n        List of weight functions f(dist) to use for the weighting\n        of each channel 1 to k.\n        If only one channel is resampled weight_funcs is\n        a single function object.\n        Must be supplied when using \'custom\' resample type\n    fill_value : int, float, numpy floating, numpy integer or None, optional\n        Set undetermined pixels to this value.\n        If fill_value is None a masked array is returned\n        with undetermined pixels masked\n\n    Returns\n    -------\n    result : numpy array\n        Source data resampled to target geometry\n    """"""\n\n    if data.ndim > 2 and data.shape[0] * data.shape[1] == valid_input_index.size:\n        data = data.reshape(data.shape[0] * data.shape[1], data.shape[2])\n    elif data.shape[0] != valid_input_index.size:\n        data = data.ravel()\n\n    if valid_input_index.size != data.shape[0]:\n        raise ValueError(\'Mismatch between geometry and dataset\')\n\n    is_multi_channel = (data.ndim > 1)\n    valid_input_size = valid_input_index.sum()\n    valid_output_size = valid_output_index.sum()\n\n    # Handle empty result set\n    if valid_input_size == 0 or valid_output_size == 0:\n        if is_multi_channel:\n            output_shape = list(output_shape)\n            output_shape.append(data.shape[1])\n\n        if fill_value is None:\n            # Use masked array for fill values\n            return np.ma.array(np.zeros(output_shape, data.dtype),\n                               mask=np.ones(output_shape, dtype=np.bool))\n        else:\n            # Return fill vaues for all pixels\n            return np.ones(output_shape, dtype=data.dtype) * fill_value\n\n    # Get size of output and reduced input\n    input_size = valid_input_size\n    if len(output_shape) > 1:\n        output_size = output_shape[0] * output_shape[1]\n    else:\n        output_size = output_shape[0]\n\n    # Check validity of input\n    if not isinstance(data, np.ndarray):\n        raise TypeError(\'data must be numpy array\')\n    elif valid_input_index.ndim != 1:\n        raise TypeError(\'valid_index must be one dimensional array\')\n    elif data.shape[0] != valid_input_index.size:\n        raise TypeError(\'Not the same number of datapoints in \'\n                        \'valid_input_index and data\')\n\n    valid_types = (\'nn\', \'custom\')\n    if resample_type not in valid_types:\n        raise TypeError(\'Invalid resampling type: %s\' % resample_type)\n\n    if resample_type == \'custom\' and weight_funcs is None:\n        raise ValueError(\'weight_funcs must be supplied when using \'\n                         \'custom resampling\')\n\n    if index_array.ndim == 1:\n        neighbours = 1\n    else:\n        neighbours = index_array.shape[1]\n        if resample_type == \'nn\':\n            raise ValueError(\'index_array contains more neighbours than \'\n                             \'just the nearest\')\n\n    # Reduce data\n    new_data = data[valid_input_index]\n\n    # Nearest neighbour resampling should conserve data type\n    # Get data type\n    conserve_input_data_type = False\n    if resample_type == \'nn\':\n        conserve_input_data_type = True\n        input_data_type = new_data.dtype\n\n    # Handle masked array input\n    is_masked_data = False\n    if np.ma.is_masked(new_data):\n        # Add the mask as channels to the dataset\n        is_masked_data = True\n        new_data = np.column_stack((new_data.data, new_data.mask))\n\n    if new_data.ndim > 1:  # Multiple channels or masked input\n        output_shape = list(output_shape)\n        output_shape.append(new_data.shape[1])\n\n    # Prepare weight_funcs argument for handeling mask data\n    if weight_funcs is not None and is_masked_data:\n        if is_multi_channel:\n            weight_funcs = weight_funcs * 2\n        else:\n            weight_funcs = (weight_funcs,) * 2\n\n    # Handle request for masking intead of using fill values\n    use_masked_fill_value = False\n    if fill_value is None:\n        use_masked_fill_value = True\n        fill_value = _get_fill_mask_value(new_data.dtype)\n\n    # Resample based on kd-tree query result\n    if resample_type == \'nn\' or neighbours == 1:\n        # Get nearest neighbour using array indexing\n        index_mask = (index_array == input_size)\n        new_index_array = np.where(index_mask, 0, index_array)\n        result = new_data[new_index_array].copy()\n        result[index_mask] = fill_value\n    else:\n        # Calculate result using weighting.\n        # Note: the code below has low readability in order\n        #       to avoid looping over numpy arrays\n\n        # Get neighbours and masks of valid indices\n        ch_neighbour_list = []\n        index_mask_list = []\n        for i in range(neighbours):  # Iterate over number of neighbours\n            # Make working copy neighbour index and\n            # set out of bounds indices to zero\n            index_ni = index_array[:, i].copy()\n            index_mask_ni = (index_ni == input_size)\n            index_ni[index_mask_ni] = 0\n\n            # Get channel data for the corresponing indices\n            ch_ni = new_data[index_ni]\n            ch_neighbour_list.append(ch_ni)\n            index_mask_list.append(index_mask_ni)\n\n        # Calculate weights\n        weight_list = []\n        for i in range(neighbours):  # Iterate over number of neighbours\n            # Make working copy of neighbour distances and\n            # set out of bounds distance to 1 in order to avoid numerical Inf\n            distance = distance_array[:, i].copy()\n            distance[index_mask_list[i]] = 1\n\n            if new_data.ndim > 1:  # More than one channel in data set.\n                # Calculate weights for each channel\n                weights = []\n                num_weights = valid_output_index.sum()\n                num_channels = new_data.shape[1]\n                for j in range(num_channels):\n                    calc_weight = weight_funcs[j](distance)\n                    # Turn a scalar weight into a numpy array\n                    # (no effect if calc_weight already is an array)\n                    expanded_calc_weight = np.ones(num_weights) * calc_weight\n                    weights.append(expanded_calc_weight)\n\n                # Collect weights for all channels for neighbour number\n                weight_list.append(np.column_stack(weights))\n            else:  # Only one channel\n                weights = weight_funcs(distance)\n                weight_list.append(weights)\n\n        result = 0\n        norm = 0\n        count = 0\n        norm_sqr = 0\n        stddev = 0\n\n        # Calculate result\n        for i in range(neighbours):  # Iterate over number of neighbours\n            # Find invalid indices to be masked of from calculation\n            if new_data.ndim > 1:  # More than one channel in data set.\n                inv_index_mask = np.expand_dims(\n                    np.invert(index_mask_list[i]), axis=1)\n            else:  # Only one channel\n                inv_index_mask = np.invert(index_mask_list[i])\n\n            # Aggregate result and norm\n            weights_tmp = inv_index_mask * weight_list[i]\n            result += weights_tmp * ch_neighbour_list[i]\n            norm += weights_tmp\n\n        # Normalize result and set fillvalue\n        result_valid_index = (norm > 0)\n        result[result_valid_index] /= norm[result_valid_index]\n\n        if with_uncert:  # Calculate uncertainties\n            # 2. pass to calculate standard deviation\n            for i in range(neighbours):  # Iterate over number of neighbours\n                # Find invalid indices to be masked of from calculation\n                if new_data.ndim > 1:  # More than one channel in data set.\n                    inv_index_mask = np.expand_dims(\n                        np.invert(index_mask_list[i]), axis=1)\n                else:  # Only one channel\n                    inv_index_mask = np.invert(index_mask_list[i])\n\n                # Aggregate stddev information\n                weights_tmp = inv_index_mask * weight_list[i]\n                count += inv_index_mask\n                norm_sqr += weights_tmp ** 2\n                values = inv_index_mask * ch_neighbour_list[i]\n                stddev += weights_tmp * (values - result) ** 2\n\n            # Calculate final stddev\n            new_valid_index = (count > 1)\n            if stddev.ndim >= 2:\n                # If given more than 1 input data array\n                new_valid_index = new_valid_index[:, 0]\n                for i in range(stddev.shape[-1]):\n                    v1 = norm[new_valid_index, i]\n                    v2 = norm_sqr[new_valid_index, i]\n                    stddev[new_valid_index, i] = np.sqrt(\n                        (v1 / (v1 ** 2 - v2)) * stddev[new_valid_index, i])\n                    stddev[~new_valid_index, i] = np.NaN\n            else:\n                # If given single input data array\n                v1 = norm[new_valid_index]\n                v2 = norm_sqr[new_valid_index]\n                stddev[new_valid_index] = np.sqrt(\n                    (v1 / (v1 ** 2 - v2)) * stddev[new_valid_index])\n                stddev[~new_valid_index] = np.NaN\n\n        # Add fill values\n        result[np.invert(result_valid_index)] = fill_value\n\n    # Create full result\n    if new_data.ndim > 1:  # More than one channel\n        output_raw_shape = ((output_size, new_data.shape[1]))\n    else:  # One channel\n        output_raw_shape = output_size\n\n    full_result = np.ones(output_raw_shape) * fill_value\n    full_result[valid_output_index] = result\n    result = full_result\n\n    if with_uncert:  # Add fill values for uncertainty\n        full_stddev = np.ones(output_raw_shape) * np.nan\n        full_count = np.zeros(output_raw_shape)\n        full_stddev[valid_output_index] = stddev\n        full_count[valid_output_index] = count\n        stddev = full_stddev\n        count = full_count\n\n        stddev = stddev.reshape(output_shape)\n        count = count.reshape(output_shape)\n\n        if is_masked_data:  # Ignore uncert computation of masks\n            stddev = _remask_data(stddev, is_to_be_masked=False)\n            count = _remask_data(count, is_to_be_masked=False)\n\n        # Set masks for invalid stddev\n        stddev = np.ma.array(stddev, mask=np.isnan(stddev))\n\n    # Reshape resampled data to correct shape\n    result = result.reshape(output_shape)\n\n    # Remap mask channels to create masked output\n    if is_masked_data:\n        result = _remask_data(result)\n\n    # Create masking of fill values\n    if use_masked_fill_value:\n        result = np.ma.masked_equal(result, fill_value)\n\n    # Set output data type to input data type if relevant\n    if conserve_input_data_type:\n        result = result.astype(input_data_type)\n\n    if with_uncert:\n        if np.ma.isMA(result):\n            stddev = np.ma.array(stddev, mask=(result.mask | stddev.mask))\n            count = np.ma.array(count, mask=result.mask)\n        return result, stddev, count\n    else:\n        return result\n\n\ndef lonlat2xyz(lons, lats):\n    R = 6370997.0\n    x_coords = R * np.cos(np.deg2rad(lats)) * np.cos(np.deg2rad(lons))\n    y_coords = R * np.cos(np.deg2rad(lats)) * np.sin(np.deg2rad(lons))\n    z_coords = R * np.sin(np.deg2rad(lats))\n\n    stack = np.stack if isinstance(lons, np.ndarray) else da.stack\n    return stack(\n        (x_coords.ravel(), y_coords.ravel(), z_coords.ravel()), axis=-1)\n\n\ndef query_no_distance(target_lons, target_lats, valid_output_index,\n                      mask=None, valid_input_index=None,\n                      neighbours=None, epsilon=None, radius=None,\n                      kdtree=None):\n    """"""Query the kdtree. No distances are returned.\n\n    NOTE: Dask array arguments must always come before other keyword arguments\n          for `da.blockwise` arguments to work.\n\n    """"""\n    voi = valid_output_index\n    shape = voi.shape + (neighbours,)\n    voir = voi.ravel()\n    if mask is not None:\n        mask = mask.ravel()[valid_input_index.ravel()]\n    target_lons_valid = target_lons.ravel()[voir]\n    target_lats_valid = target_lats.ravel()[voir]\n\n    coords = lonlat2xyz(target_lons_valid, target_lats_valid)\n    distance_array, index_array = kdtree.query(\n        coords,\n        k=neighbours,\n        eps=epsilon,\n        distance_upper_bound=radius,\n        mask=mask)\n\n    if index_array.ndim == 1:\n        index_array = index_array[:, None]\n\n    # KDTree query returns out-of-bounds neighbors as `len(arr)`\n    # which is an invalid index, we mask those out so -1 represents\n    # invalid values\n    # voi is 2D (trows, tcols)\n    # index_array is 2D (valid output pixels, neighbors)\n    # there are as many Trues in voi as rows in index_array\n    good_pixels = index_array < kdtree.n\n    res_ia = np.empty(shape, dtype=np.int)\n    mask = np.zeros(shape, dtype=np.bool)\n    mask[voi, :] = good_pixels\n    res_ia[mask] = index_array[good_pixels]\n    res_ia[~mask] = -1\n    return res_ia\n\n\ndef _my_index(index_arr, vii, data_arr, vii_slices=None, ia_slices=None,\n              fill_value=np.nan):\n    """"""Helper function for \'get_sample_from_neighbour_info\'.""""""\n    vii_slices = tuple(\n        x if x is not None else vii.ravel() for x in vii_slices)\n    mask_slices = tuple(\n        x if x is not None else (index_arr == -1) for x in ia_slices)\n    ia_slices = tuple(\n        x if x is not None else index_arr for x in ia_slices)\n    res = data_arr[vii_slices][ia_slices]\n    res[mask_slices] = fill_value\n    return res\n\n\nclass XArrayResamplerNN(object):\n    def __init__(self,\n                 source_geo_def,\n                 target_geo_def,\n                 radius_of_influence=None,\n                 neighbours=1,\n                 epsilon=0):\n        """"""\n\n        Parameters\n        ----------\n        source_geo_def : object\n            Geometry definition of source\n        target_geo_def : object\n            Geometry definition of target\n        radius_of_influence : float, optional\n            Cut off distance in geocentric meters.\n            If not provided this will be estimated based on the source\n            and target geometry definition.\n        neighbours : int, optional\n            The number of neigbours to consider for each grid point.\n            Default 1. Currently 1 is the only supported number.\n        epsilon : float, optional\n            Allowed uncertainty in meters. Increasing uncertainty\n            reduces execution time\n\n        """"""\n        if DataArray is None:\n            raise ImportError(""Missing \'xarray\' and \'dask\' dependencies"")\n\n        self.valid_input_index = None\n        self.valid_output_index = None\n        self.index_array = None\n        self.distance_array = None\n        self.delayed_kdtree = None\n        self.neighbours = neighbours\n        self.epsilon = epsilon\n        self.source_geo_def = source_geo_def\n        self.target_geo_def = target_geo_def\n        if radius_of_influence is None:\n            radius_of_influence = self._compute_radius_of_influence()\n        self.radius_of_influence = radius_of_influence\n        assert (self.target_geo_def.ndim == 2), \\\n            ""Target area definition must be 2 dimensions""\n\n    def _compute_radius_of_influence(self):\n        """"""Estimate a good default radius_of_influence.""""""\n        try:\n            src_res = self.source_geo_def.geocentric_resolution()\n        except RuntimeError:\n            logger.warning(""Could not calculate source definition resolution"")\n            src_res = np.nan\n        try:\n            dst_res = self.target_geo_def.geocentric_resolution()\n        except RuntimeError:\n            logger.warning(""Could not calculate destination definition ""\n                           ""resolution"")\n            dst_res = np.nan\n        radius_of_influence = np.nanmax([src_res, dst_res])\n        if np.isnan(radius_of_influence):\n            logger.warning(""Could not calculate radius_of_influence, falling ""\n                           ""back to 10000 meters. This may produce lower ""\n                           ""quality results than expected."")\n            radius_of_influence = 10000\n        return radius_of_influence\n\n    def _create_resample_kdtree(self, chunks=CHUNK_SIZE):\n        """"""Set up kd tree on input""""""\n        source_lons, source_lats = self.source_geo_def.get_lonlats(\n            chunks=chunks)\n        valid_input_idx = ((source_lons >= -180) & (source_lons <= 180) &\n                           (source_lats <= 90) & (source_lats >= -90))\n        input_coords = lonlat2xyz(source_lons, source_lats)\n        input_coords = input_coords[valid_input_idx.ravel(), :]\n\n        # Build kd-tree on input\n        input_coords = input_coords.astype(np.float)\n        delayed_kdtree = dask.delayed(KDTree, pure=True)(input_coords)\n        return valid_input_idx, delayed_kdtree\n\n    def query_resample_kdtree(self,\n                              resample_kdtree,\n                              tlons,\n                              tlats,\n                              valid_oi,\n                              mask):\n        """"""Query kd-tree on slice of target coordinates.""""""\n        if mask is None:\n            args = tuple()\n        else:\n            ndims = self.source_geo_def.ndim\n            dims = \'mn\'[:ndims]\n            args = (mask, dims, self.valid_input_index, dims)\n        # res.shape = rows, cols, neighbors\n        # j=rows, i=cols, k=neighbors, m=source rows, n=source cols\n        res = blockwise(query_no_distance, \'jik\', tlons, \'ji\', tlats, \'ji\',\n                        valid_oi, \'ji\', *args, kdtree=resample_kdtree,\n                        neighbours=self.neighbours, epsilon=self.epsilon,\n                        radius=self.radius_of_influence, dtype=np.int,\n                        new_axes={\'k\': self.neighbours}, concatenate=True)\n        return res, None\n\n    def get_neighbour_info(self, mask=None):\n        """"""Return neighbour info.\n\n        Returns\n        -------\n        (valid_input_index, valid_output_index,\n        index_array, distance_array) : tuple of numpy arrays\n            Neighbour resampling info\n\n        """"""\n        if self.source_geo_def.size < self.neighbours:\n            warnings.warn(\'Searching for %s neighbours in %s data points\' %\n                          (self.neighbours, self.source_geo_def.size))\n\n        # Create kd-tree\n        chunks = mask.chunks if mask is not None else CHUNK_SIZE\n        valid_input_idx, resample_kdtree = self._create_resample_kdtree(\n            chunks=chunks)\n        self.valid_input_index = valid_input_idx\n        self.delayed_kdtree = resample_kdtree\n\n        # TODO: Add \'chunks\' keyword argument to this method and use it\n        target_lons, target_lats = self.target_geo_def.get_lonlats(chunks=CHUNK_SIZE)\n        valid_output_idx = ((target_lons >= -180) & (target_lons <= 180) &\n                            (target_lats <= 90) & (target_lats >= -90))\n\n        if mask is not None:\n            assert (mask.shape == self.source_geo_def.shape), \\\n                ""\'mask\' must be the same shape as the source geo definition""\n            mask = mask.data\n        index_arr, distance_arr = self.query_resample_kdtree(\n            resample_kdtree, target_lons, target_lats, valid_output_idx, mask)\n\n        self.valid_output_index, self.index_array = valid_output_idx, index_arr\n        self.distance_array = distance_arr\n\n        return (self.valid_input_index,\n                self.valid_output_index,\n                self.index_array,\n                self.distance_array)\n\n    def get_sample_from_neighbour_info(self, data, fill_value=np.nan):\n        """"""Get the pixels matching the target area.\n\n        This method should work for any dimensionality of the provided data\n        array as long as the geolocation dimensions match in size and name in\n        ``data.dims``. Where source area definition are `AreaDefinition`\n        objects the corresponding dimensions in the data should be\n        ``(\'y\', \'x\')``.\n\n        This method also attempts to preserve chunk sizes of dask arrays,\n        but does require loading/sharing the fully computed source data before\n        it can actually compute the values to write to the destination array.\n        This can result in large memory usage for large source data arrays,\n        but is a necessary evil until fancier indexing is supported by dask\n        and/or pykdtree.\n\n        Args:\n            data (xarray.DataArray): Source data pixels to sample\n            fill_value (float): Output fill value when no source data is\n                near the target pixel. When omitted, if the input data is an\n                integer array then the maximum value for that integer type is\n                used, but otherwise, NaN is used and can be detected in the\n                result with ``res.isnull()``.\n\n        Returns:\n            dask.array.Array: The resampled array. The dtype of the array will\n                be the same as the input data. Pixels with no matching data from\n                the input array will be filled (see the `fill_value` parameter\n                description above).\n        """"""\n        if fill_value is not None and np.isnan(fill_value) and \\\n                np.issubdtype(data.dtype, np.integer):\n            fill_value = _get_fill_mask_value(data.dtype)\n            logger.warning(""Fill value incompatible with integer data ""\n                           ""using {:d} instead."".format(fill_value))\n\n        # Convert back to 1 neighbor\n        if self.neighbours > 1:\n            raise NotImplementedError(""Nearest neighbor resampling can not ""\n                                      ""handle more than 1 neighbor yet."")\n        # Convert from multiple neighbor shape to 1 neighbor\n        ia = self.index_array[:, :, 0]\n        vii = self.valid_input_index\n\n        if isinstance(self.source_geo_def, geometry.SwathDefinition):\n            # could be 1D or 2D\n            src_geo_dims = self.source_geo_def.lons.dims\n        else:\n            # assume AreaDefinitions and everything else are 2D with \'y\', \'x\'\n            src_geo_dims = (\'y\', \'x\')\n        dst_geo_dims = (\'y\', \'x\')\n        # verify that source dims are the same between geo and data\n        data_geo_dims = tuple(d for d in data.dims if d in src_geo_dims)\n        assert (data_geo_dims == src_geo_dims), \\\n            ""Data dimensions do not match source area dimensions""\n        # verify that the dims are next to each other\n        first_dim_idx = data.dims.index(src_geo_dims[0])\n        num_dims = len(src_geo_dims)\n        assert (data.dims[first_dim_idx:first_dim_idx + num_dims] ==\n                data_geo_dims), ""Data\'s geolocation dimensions are not "" \\\n                                ""consecutive.""\n\n        # FIXME: Can\'t include coordinates whose dimensions depend on the geo\n        #        dims either\n        def contain_coords(var, coord_list):\n            return bool(set(coord_list).intersection(set(var.dims)))\n\n        coords = {c: c_var for c, c_var in data.coords.items()\n                  if not contain_coords(c_var, src_geo_dims + dst_geo_dims)}\n        try:\n            # TODO: Add \'chunks\' kwarg\n            coord_x, coord_y = self.target_geo_def.get_proj_vectors(chunks=CHUNK_SIZE)\n            coords[\'y\'] = coord_y\n            coords[\'x\'] = coord_x\n        except AttributeError:\n            logger.debug(""No geo coordinates created"")\n\n        # shape of the source data after we flatten the geo dimensions\n        flat_src_shape = []\n        # slice objects to index in to the source data\n        vii_slices = []\n        ia_slices = []\n        # whether we have seen the geo dims in our analysis\n        geo_handled = False\n        # dimension indexes for da.blockwise\n        src_adims = []\n        flat_adim = []\n        # map source dimension name to dimension number for da.blockwise\n        src_dim_to_ind = {}\n        # destination array dimension indexes for da.blockwise\n        dst_dims = []\n        for i, dim in enumerate(data.dims):\n            src_dim_to_ind[dim] = i\n            if dim in src_geo_dims and not geo_handled:\n                flat_src_shape.append(-1)\n                vii_slices.append(None)  # mark for replacement\n                ia_slices.append(None)  # mark for replacement\n                flat_adim.append(i)\n                src_adims.append(i)\n                dst_dims.extend(dst_geo_dims)\n                geo_handled = True\n            elif dim not in src_geo_dims:\n                flat_src_shape.append(data.sizes[dim])\n                vii_slices.append(slice(None))\n                ia_slices.append(slice(None))\n                src_adims.append(i)\n                dst_dims.append(dim)\n        # map destination dimension names to blockwise dimension indexes\n        dst_dim_to_ind = src_dim_to_ind.copy()\n        dst_dim_to_ind[\'y\'] = i + 1\n        dst_dim_to_ind[\'x\'] = i + 2\n        # FUTURE: when we allow more than one neighbor\n        # neighbors_dim = i + 3\n\n        new_data = data.data.reshape(flat_src_shape)\n        vii = vii.ravel()\n        dst_adims = [dst_dim_to_ind[dim] for dim in dst_dims]\n        ia_adims = [dst_dim_to_ind[dim] for dim in dst_geo_dims]\n        # FUTURE: when we allow more than one neighbor add neighbors dimension\n        # dst_adims.append(neighbors_dim)\n        # ia_adims.append(neighbors_dim)\n        # FUTURE: when we allow more than one neighbor we need to add\n        #         the new axis to blockwise:\n        #         `new_axes={neighbor_dim: self.neighbors}`\n        # FUTURE: if/when dask can handle index arrays that are dask arrays\n        #         then we can avoid all of this complicated blockwise stuff\n        res = blockwise(_my_index, dst_adims,\n                        ia, ia_adims,\n                        vii, flat_adim,\n                        new_data, src_adims,\n                        vii_slices=vii_slices, ia_slices=ia_slices,\n                        fill_value=fill_value,\n                        dtype=new_data.dtype, concatenate=True)\n        res = DataArray(res, dims=dst_dims, coords=coords,\n                        attrs=deepcopy(data.attrs))\n\n        return res\n\n\ndef _get_fill_mask_value(data_dtype):\n    """"""Return the maximum value of dtype.""""""\n    if issubclass(data_dtype.type, np.floating):\n        fill_value = np.finfo(data_dtype.type).max\n    elif issubclass(data_dtype.type, np.integer):\n        fill_value = np.iinfo(data_dtype.type).max\n    else:\n        raise TypeError(\'Type %s is unsupported for masked fill values\' %\n                        data_dtype.type)\n    return fill_value\n\n\ndef _remask_data(data, is_to_be_masked=True):\n    """"""Interprets half the array as mask for the other half""""""\n\n    channels = data.shape[-1]\n    if is_to_be_masked:\n        mask = data[..., (channels // 2):]\n        # All pixels affected by masked pixels are masked out\n        mask = (mask != 0)\n        data = np.ma.array(data[..., :(channels // 2)], mask=mask)\n    else:\n        data = data[..., :(channels // 2)]\n\n    if data.shape[-1] == 1:\n        data = data.reshape(data.shape[:-1])\n    return data\n'"
pyresample/plot.py,6,"b'#!/usr/bin/env python\n# encoding: utf8\n#\n# Copyright (C) 2010-2019 Pytroll\n#\n# Authors:\n#    Esben S. Nielsen\n#    Thomas Lavergne\n#    Adam Dybbroe <adam.dybbroe@smhi.se>\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Utility functions for quick and easy display.""""""\n\nfrom __future__ import absolute_import\nimport numpy as np\n\ntry:\n    from pyresample.utils import cartopy  # noqa\n\n    BASEMAP_NOT_CARTOPY = False\nexcept ImportError:\n    BASEMAP_NOT_CARTOPY = True\n\n\ndef ellps2axis(ellps_name):\n    """"""Get semi-major and semi-minor axis from ellipsis definition.\n\n    Parameters\n    ---------\n    ellps_name : str\n        Standard name of ellipsis\n\n    Returns\n    -------\n    (a, b) : semi-major and semi-minor axis\n\n    """"""\n    ellps = {\'helmert\': {\'a\': 6378200.0, \'b\': 6356818.1696278909},\n             \'intl\': {\'a\': 6378388.0, \'b\': 6356911.9461279465},\n             \'merit\': {\'a\': 6378137.0, \'b\': 6356752.2982159676},\n             \'wgs72\': {\'a\': 6378135.0, \'b\': 6356750.5200160937},\n             \'sphere\': {\'a\': 6370997.0, \'b\': 6370997.0},\n             \'clrk66\': {\'a\': 6378206.4000000004, \'b\': 6356583.7999999998},\n             \'nwl9d\': {\'a\': 6378145.0, \'b\': 6356759.7694886839},\n             \'lerch\': {\'a\': 6378139.0, \'b\': 6356754.2915103417},\n             \'evrstss\': {\'a\': 6377298.5559999999, \'b\': 6356097.5503008962},\n             \'evrst30\': {\'a\': 6377276.3449999997, \'b\': 6356075.4131402401},\n             \'mprts\': {\'a\': 6397300.0, \'b\': 6363806.2827225132},\n             \'krass\': {\'a\': 6378245.0, \'b\': 6356863.0187730473},\n             \'walbeck\': {\'a\': 6376896.0, \'b\': 6355834.8466999996},\n             \'kaula\': {\'a\': 6378163.0, \'b\': 6356776.9920869097},\n             \'wgs66\': {\'a\': 6378145.0, \'b\': 6356759.7694886839},\n             \'evrst56\': {\'a\': 6377301.2429999998, \'b\': 6356100.2283681016},\n             \'new_intl\': {\'a\': 6378157.5, \'b\': 6356772.2000000002},\n             \'airy\': {\'a\': 6377563.3959999997, \'b\': 6356256.9100000001},\n             \'bessel\': {\'a\': 6377397.1550000003, \'b\': 6356078.9628181886},\n             \'seasia\': {\'a\': 6378155.0, \'b\': 6356773.3205000004},\n             \'aust_sa\': {\'a\': 6378160.0, \'b\': 6356774.7191953054},\n             \'wgs84\': {\'a\': 6378137.0, \'b\': 6356752.3142451793},\n             \'hough\': {\'a\': 6378270.0, \'b\': 6356794.3434343431},\n             \'wgs60\': {\'a\': 6378165.0, \'b\': 6356783.2869594367},\n             \'engelis\': {\'a\': 6378136.0499999998, \'b\': 6356751.3227215428},\n             \'apl4.9\': {\'a\': 6378137.0, \'b\': 6356751.796311819},\n             \'andrae\': {\'a\': 6377104.4299999997, \'b\': 6355847.4152333336},\n             \'sgs85\': {\'a\': 6378136.0, \'b\': 6356751.301568781},\n             \'delmbr\': {\'a\': 6376428.0, \'b\': 6355957.9261637237},\n             \'fschr60m\': {\'a\': 6378155.0, \'b\': 6356773.3204827355},\n             \'iau76\': {\'a\': 6378140.0, \'b\': 6356755.2881575283},\n             \'plessis\': {\'a\': 6376523.0, \'b\': 6355863.0},\n             \'cpm\': {\'a\': 6375738.7000000002, \'b\': 6356666.221912113},\n             \'fschr68\': {\'a\': 6378150.0, \'b\': 6356768.3372443849},\n             \'mod_airy\': {\'a\': 6377340.1890000002, \'b\': 6356034.4460000005},\n             \'grs80\': {\'a\': 6378137.0, \'b\': 6356752.3141403561},\n             \'bess_nam\': {\'a\': 6377483.8650000002, \'b\': 6356165.3829663256},\n             \'fschr60\': {\'a\': 6378166.0, \'b\': 6356784.2836071067},\n             \'clrk80\': {\'a\': 6378249.1449999996, \'b\': 6356514.9658284895},\n             \'evrst69\': {\'a\': 6377295.6639999999, \'b\': 6356094.6679152036},\n             \'grs67\': {\'a\': 6378160.0, \'b\': 6356774.5160907144},\n             \'evrst48\': {\'a\': 6377304.0630000001, \'b\': 6356103.0389931547}}\n    try:\n        ellps_axis = ellps[ellps_name.lower()]\n        a = ellps_axis[\'a\']\n        b = ellps_axis[\'b\']\n    except KeyError:\n        raise ValueError((\'Could not determine semi-major and semi-minor axis \'\n                          \'of specified ellipsis %s\') % ellps_name)\n    return a, b\n\n\ndef area_def2basemap(area_def, **kwargs):\n    """"""Get Basemap object from an AreaDefinition object.\n\n    Parameters\n    ---------\n    area_def : object\n        geometry.AreaDefinition object\n    **kwargs: Keyword arguments\n        Additional initialization arguments for Basemap\n\n    Returns\n    -------\n    bmap : Basemap object\n\n    """"""\n    import warnings\n    warnings.warn(""Basemap is no longer maintained. Please switch to cartopy ""\n                  ""by using \'area_def.to_cartopy_crs()\'. See the pyresample ""\n                  ""documentation for more details."", DeprecationWarning)\n\n    from mpl_toolkits.basemap import Basemap\n    try:\n        a, b = ellps2axis(area_def.proj_dict[\'ellps\'])\n        rsphere = (a, b)\n    except KeyError:\n        try:\n            a = float(area_def.proj_dict[\'a\'])\n            try:\n                b = float(area_def.proj_dict[\'b\'])\n                rsphere = (a, b)\n            except KeyError:\n                rsphere = a\n        except KeyError:\n            # Default to WGS84 ellipsoid\n            a, b = ellps2axis(\'wgs84\')\n            rsphere = (a, b)\n\n    # Add projection specific basemap args to args passed to function\n    basemap_args = kwargs\n    basemap_args[\'rsphere\'] = rsphere\n\n    if area_def.proj_dict[\'proj\'] in (\'ortho\', \'geos\', \'nsper\'):\n        llcrnrx, llcrnry, urcrnrx, urcrnry = area_def.area_extent\n        basemap_args[\'llcrnrx\'] = llcrnrx\n        basemap_args[\'llcrnry\'] = llcrnry\n        basemap_args[\'urcrnrx\'] = urcrnrx\n        basemap_args[\'urcrnry\'] = urcrnry\n    else:\n        llcrnrlon, llcrnrlat, urcrnrlon, urcrnrlat = area_def.area_extent_ll\n        basemap_args[\'llcrnrlon\'] = llcrnrlon\n        basemap_args[\'llcrnrlat\'] = llcrnrlat\n        basemap_args[\'urcrnrlon\'] = urcrnrlon\n        basemap_args[\'urcrnrlat\'] = urcrnrlat\n\n    if area_def.proj_dict[\'proj\'] == \'eqc\':\n        basemap_args[\'projection\'] = \'cyl\'\n    else:\n        basemap_args[\'projection\'] = area_def.proj_dict[\'proj\']\n\n    # Try adding potentially remaining args\n    for key in (\'lon_0\', \'lat_0\', \'lon_1\', \'lat_1\', \'lon_2\', \'lat_2\',\n                \'lat_ts\'):\n        try:\n            basemap_args[key] = float(area_def.proj_dict[key])\n        except KeyError:\n            pass\n\n    return Basemap(**basemap_args)\n\n\ndef _basemap_get_quicklook(area_def, data, vmin=None, vmax=None,\n                           label=\'Variable (units)\', num_meridians=45,\n                           num_parallels=10, coast_res=\'110m\', cmap=\'RdBu_r\'):\n    """"""Doing quicklook image plots with Basemap.""""""\n    if area_def.shape != data.shape:\n        raise ValueError(\'area_def shape %s does not match data shape %s\' %\n                         (list(area_def.shape), list(data.shape)))\n    import matplotlib.pyplot as plt\n    bmap = area_def2basemap(area_def, resolution=coast_res)\n    bmap.drawcoastlines()\n    if num_meridians > 0:\n        bmap.drawmeridians(np.arange(-180, 180, num_meridians))\n    if num_parallels > 0:\n        bmap.drawparallels(np.arange(-90, 90, num_parallels))\n    if not (np.ma.isMaskedArray(data) and data.mask.all()):\n        col = bmap.imshow(data, origin=\'upper\', vmin=vmin, vmax=vmax, cmap=cmap)\n        plt.colorbar(col, shrink=0.5, pad=0.05).set_label(label)\n    return plt\n\n\ndef _translate_coast_resolution_to_cartopy(coast_res):\n    """"""Translate the coast resolution argument between cartopy and basemap notation.""""""\n    bmap_to_cartopy_res = {\n        \'c\': \'110m\',\n        \'l\': \'110m\',\n        \'i\': \'50m\',\n        \'h\': \'10m\',\n        \'f\': \'10m\'\n    }\n\n    if BASEMAP_NOT_CARTOPY:\n        if coast_res.endswith(\'m\'):\n            _rev_map = {v: k for k, v in bmap_to_cartopy_res.items()}\n            coast_res = _rev_map[coast_res]\n        return coast_res, False\n\n    if coast_res and coast_res not in [\'110m\', \'50m\', \'10m\']:\n        import warnings\n        warnings.warn(""\'coast_res\' should be either \'110m\', \'50m\', \'10m\'."")\n        coast_res = {\n            \'c\': \'110m\',\n            \'l\': \'110m\',\n            \'i\': \'50m\',\n            \'h\': \'10m\',\n            \'f\': \'10m\'\n        }[coast_res]\n\n    return coast_res, True\n\n\ndef _add_gridlines(axes, nmeridians, nparallels):\n    """"""Add gridlines: meridians and parallels onto the plot.""""""\n    from matplotlib import ticker as mticker\n\n    gl = axes.gridlines()\n    if nmeridians:\n        gl.xlocator = mticker.FixedLocator(np.arange(-180, 180+nmeridians, nmeridians))\n    else:\n        gl.xlines = False\n    if nparallels:\n        gl.ylocator = mticker.FixedLocator(np.arange(-90, 90+nparallels, nparallels))\n    else:\n        gl.ylines = False\n\n    return gl\n\n\ndef _get_quicklook(area_def, data, vmin=None, vmax=None,\n                   label=\'Variable (units)\', num_meridians=45,\n                   num_parallels=10, coast_res=\'110m\', cmap=\'RdBu_r\'):\n    """"""Get default cartopy matplotlib plot.""""""\n    import matplotlib.pyplot as plt\n\n    coast_res, is_cartopy = _translate_coast_resolution_to_cartopy(coast_res)\n    if not is_cartopy:\n        return _basemap_get_quicklook(\n            area_def, data, vmin, vmax, label, num_meridians,\n            num_parallels, coast_res=coast_res, cmap=cmap)\n\n    if area_def.shape != data.shape:\n        raise ValueError(\'area_def shape %s does not match data shape %s\' %\n                         (list(area_def.shape), list(data.shape)))\n\n    crs = area_def.to_cartopy_crs()\n    ax = plt.axes(projection=crs)\n    ax.coastlines(resolution=coast_res)\n    ax.set_global()\n\n    if num_meridians or num_parallels:\n        _ = _add_gridlines(ax, num_meridians, num_parallels)\n\n    if not (np.ma.isMaskedArray(data) and data.mask.all()):\n        col = ax.imshow(data, transform=crs, extent=crs.bounds,\n                        origin=\'upper\', vmin=vmin, vmax=vmax, cmap=cmap)\n        plt.colorbar(col, shrink=0.5, pad=0.05).set_label(label)\n    return plt\n\n\ndef show_quicklook(area_def, data, vmin=None, vmax=None,\n                   label=\'Variable (units)\', num_meridians=45,\n                   num_parallels=10, coast_res=\'110m\', cmap=\'RdBu_r\'):\n    """"""Display default quicklook plot.\n\n    Parameters\n    ---------\n    area_def : object\n        geometry.AreaDefinition object\n    data : numpy array | numpy masked array\n        2D array matching area_def. Use masked array for transparent values\n    vmin : float, optional\n        Min value for luminescence scaling\n    vmax : float, optional\n        Max value for luminescence scaling\n    label : str, optional\n        Label for data\n    num_meridians : int, optional\n        Number of meridians to plot on the globe\n    num_parallels : int, optional\n        Number of parallels to plot on the globe\n    coast_res : {\'c\', \'l\', \'i\', \'h\', \'f\'}, optional\n        Resolution of coastlines\n\n    Returns\n    -------\n    bmap : Basemap object\n\n    """"""\n    plt = _get_quicklook(area_def, data, vmin=vmin, vmax=vmax,\n                         label=label, num_meridians=num_meridians,\n                         num_parallels=num_parallels, coast_res=coast_res,\n                         cmap=cmap)\n    plt.show()\n    plt.close()\n\n\ndef save_quicklook(filename, area_def, data, vmin=None, vmax=None,\n                   label=\'Variable (units)\', num_meridians=45,\n                   num_parallels=10, coast_res=\'110m\',\n                   cmap=\'RdBu_r\'):\n    """"""Display and save default quicklook plot.\n\n    Parameters\n    ----------\n    filename : str\n        path to output file\n    area_def : object\n        geometry.AreaDefinition object\n    data : numpy array | numpy masked array\n        2D array matching area_def. Use masked array for transparent values\n    vmin : float, optional\n        Min value for luminescence scaling\n    vmax : float, optional\n        Max value for luminescence scaling\n    label : str, optional\n        Label for data\n    num_meridians : int, optional\n        Number of meridians to plot on the globe\n    num_parallels : int, optional\n        Number of parallels to plot on the globe\n    coast_res : {\'c\', \'l\', \'i\', \'h\', \'f\'}, optional\n        Resolution of coastlines\n\n    """"""\n    plt = _get_quicklook(area_def, data, vmin=vmin, vmax=vmax,\n                         label=label, num_meridians=num_meridians,\n                         num_parallels=num_parallels, coast_res=coast_res,\n                         cmap=cmap)\n    plt.savefig(filename, bbox_inches=\'tight\')\n    plt.close()\n'"
pyresample/resampler.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2019\n\n# Author(s):\n\n#   Martin Raspaud <martin.raspaud@smhi.se>\n\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Base resampler class made for subclassing.""""""\n\nimport hashlib\nimport json\nimport os\n\nimport numpy as np\n\nfrom pyresample.geometry import SwathDefinition\n\n\ndef hash_dict(the_dict, the_hash=None):\n    """"""Calculate a hash for a dictionary.""""""\n    if the_hash is None:\n        the_hash = hashlib.sha1()\n    the_hash.update(json.dumps(the_dict, sort_keys=True).encode(\'utf-8\'))\n    return the_hash\n\n\nclass BaseResampler(object):\n    """"""Base abstract resampler class.""""""\n\n    def __init__(self, source_geo_def, target_geo_def):\n        """"""Initialize resampler with geolocation information.\n\n        Args:\n            source_geo_def (SwathDefinition, AreaDefinition):\n                Geolocation definition for the data to be resampled\n            target_geo_def (CoordinateDefinition, AreaDefinition):\n                Geolocation definition for the area to resample data to.\n\n        """"""\n        self.source_geo_def = source_geo_def\n        self.target_geo_def = target_geo_def\n\n    def get_hash(self, source_geo_def=None, target_geo_def=None, **kwargs):\n        """"""Get hash for the current resample with the given *kwargs*.""""""\n        if source_geo_def is None:\n            source_geo_def = self.source_geo_def\n        if target_geo_def is None:\n            target_geo_def = self.target_geo_def\n        the_hash = source_geo_def.update_hash()\n        target_geo_def.update_hash(the_hash)\n        hash_dict(kwargs, the_hash)\n        return the_hash.hexdigest()\n\n    def precompute(self, **kwargs):\n        """"""Do the precomputation.\n\n        This is an optional step if the subclass wants to implement more\n        complex features like caching or can share some calculations\n        between multiple datasets to be processed.\n\n        """"""\n        return None\n\n    def compute(self, data, **kwargs):\n        """"""Do the actual resampling.\n\n        This must be implemented by subclasses.\n\n        """"""\n        raise NotImplementedError\n\n    def resample(self, data, cache_dir=None, mask_area=None, **kwargs):\n        """"""Resample `data` by calling `precompute` and `compute` methods.\n\n        Only certain resampling classes may use `cache_dir` and the `mask`\n        provided when `mask_area` is True. The return value of calling the\n        `precompute` method is passed as the `cache_id` keyword argument\n        of the `compute` method, but may not be used directly for caching. It\n        is up to the individual resampler subclasses to determine how this\n        is used.\n\n        Args:\n            data (xarray.DataArray): Data to be resampled\n            cache_dir (str): directory to cache precomputed results\n                             (default False, optional)\n            mask_area (bool): Mask geolocation data where data values are\n                              invalid. This should be used when data values\n                              may affect what neighbors are considered valid.\n\n        Returns (xarray.DataArray): Data resampled to the target area\n\n        """"""\n        # default is to mask areas for SwathDefinitions\n        if mask_area is None and isinstance(\n                self.source_geo_def, SwathDefinition):\n            mask_area = True\n\n        if mask_area:\n            if isinstance(self.source_geo_def, SwathDefinition):\n                geo_dims = self.source_geo_def.lons.dims\n            else:\n                geo_dims = (\'y\', \'x\')\n            flat_dims = [dim for dim in data.dims if dim not in geo_dims]\n            if np.issubdtype(data.dtype, np.integer):\n                kwargs[\'mask\'] = data == data.attrs.get(\'_FillValue\', np.iinfo(data.dtype.type).max)\n            else:\n                kwargs[\'mask\'] = data.isnull()\n            kwargs[\'mask\'] = kwargs[\'mask\'].all(dim=flat_dims)\n\n        cache_id = self.precompute(cache_dir=cache_dir, **kwargs)\n        return self.compute(data, cache_id=cache_id, **kwargs)\n\n    def _create_cache_filename(self, cache_dir=None, prefix=\'\',\n                               fmt=\'.zarr\', **kwargs):\n        """"""Create filename for the cached resampling parameters.""""""\n        cache_dir = cache_dir or \'.\'\n        hash_str = self.get_hash(**kwargs)\n\n        return os.path.join(cache_dir, prefix + hash_str + fmt)\n'"
pyresample/spherical.py,79,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013, 2014, 2015 Martin Raspaud\n\n# Author(s):\n\n#   Martin Raspaud <martin.raspaud@smhi.se>\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Some generalized spherical functions.\n\nbase type is a numpy array of size (n, 2) (2 for lon and lats)\n\n""""""\n\nimport numpy as np\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass SCoordinate(object):\n\n    """"""Spherical coordinates.""""""\n\n    def __init__(self, lon, lat):\n        self.lon = lon\n        self.lat = lat\n\n    def cross2cart(self, point):\n        """"""Compute the cross product, and convert to cartesian coordinates.""""""\n\n        lat1 = self.lat\n        lon1 = self.lon\n        lat2 = point.lat\n        lon2 = point.lon\n\n        ad = np.sin(lat1 - lat2) * np.cos((lon1 - lon2) / 2.0)\n        be = np.sin(lat1 + lat2) * np.sin((lon1 - lon2) / 2.0)\n        c = np.sin((lon1 + lon2) / 2.0)\n        f = np.cos((lon1 + lon2) / 2.0)\n        g = np.cos(lat1)\n        h = np.cos(lat2)\n        i = np.sin(lon2 - lon1)\n        res = CCoordinate(np.array([-ad * c + be * f,\n                                    ad * f + be * c,\n                                    g * h * i]))\n\n        return res\n\n    def to_cart(self):\n        """"""Convert to cartesian.""""""\n        return CCoordinate(np.array([np.cos(self.lat) * np.cos(self.lon),\n                                     np.cos(self.lat) * np.sin(self.lon),\n                                     np.sin(self.lat)]))\n\n    def distance(self, point):\n        """"""Vincenty formula.""""""\n\n        dlambda = self.lon - point.lon\n        num = ((np.cos(point.lat) * np.sin(dlambda)) ** 2 +\n               (np.cos(self.lat) * np.sin(point.lat) -\n                np.sin(self.lat) * np.cos(point.lat) *\n                np.cos(dlambda)) ** 2)\n        den = (np.sin(self.lat) * np.sin(point.lat) +\n               np.cos(self.lat) * np.cos(point.lat) * np.cos(dlambda))\n\n        return np.arctan2(num ** .5, den)\n\n    def hdistance(self, point):\n        """"""Haversine formula.""""""\n\n        return 2 * np.arcsin((np.sin((point.lat - self.lat) / 2.0) ** 2.0 +\n                              np.cos(point.lat) * np.cos(self.lat) *\n                              np.sin((point.lon - self.lon) / 2.0) ** 2.0) ** .5)\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __eq__(self, other):\n        return np.allclose((self.lon, self.lat), (other.lon, other.lat))\n\n    def __str__(self):\n        return str((np.rad2deg(self.lon), np.rad2deg(self.lat)))\n\n    def __repr__(self):\n        return str((np.rad2deg(self.lon), np.rad2deg(self.lat)))\n\n    def __iter__(self):\n        return zip([self.lon, self.lat]).__iter__()\n\n\nclass CCoordinate(object):\n\n    """"""Cartesian coordinates\n    """"""\n\n    def __init__(self, cart):\n        self.cart = np.array(cart)\n\n    def norm(self):\n        """"""Euclidean norm of the vector.\n        """"""\n        return np.sqrt(np.einsum(\'...i, ...i\', self.cart, self.cart))\n\n    def normalize(self):\n        """"""normalize the vector.\n        """"""\n\n        self.cart /= np.sqrt(np.einsum(\'...i, ...i\', self.cart, self.cart))\n\n        return self\n\n    def cross(self, point):\n        """"""cross product with another vector.\n        """"""\n        return CCoordinate(np.cross(self.cart, point.cart))\n\n    def dot(self, point):\n        """"""dot product with another vector.\n        """"""\n        return np.inner(self.cart, point.cart)\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __eq__(self, other):\n        return np.allclose(self.cart, other.cart)\n\n    def __str__(self):\n        return str(self.cart)\n\n    def __repr__(self):\n        return str(self.cart)\n\n    def __add__(self, other):\n        try:\n            return CCoordinate(self.cart + other.cart)\n        except AttributeError:\n            return CCoordinate(self.cart + np.array(other))\n\n    def __radd__(self, other):\n        return self.__add__(other)\n\n    def __mul__(self, other):\n        try:\n            return CCoordinate(self.cart * other.cart)\n        except AttributeError:\n            return CCoordinate(self.cart * np.array(other))\n\n    def __rmul__(self, other):\n        return self.__mul__(other)\n\n    def to_spherical(self):\n        return SCoordinate(np.arctan2(self.cart[1], self.cart[0]),\n                           np.arcsin(self.cart[2]))\n\n\nEPSILON = 0.0000001\n\n\ndef modpi(val, mod=np.pi):\n    """"""Puts *val* between -*mod* and *mod*.\n    """"""\n    return (val + mod) % (2 * mod) - mod\n\n\nclass Arc(object):\n\n    """"""An arc of the great circle between two points.""""""\n\n    def __init__(self, start, end):\n        self.start, self.end = start, end\n\n    def __eq__(self, other):\n        if(self.start == other.start and self.end == other.end):\n            return 1\n        return 0\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __str__(self):\n        return (str(self.start) + "" -> "" + str(self.end))\n\n    def __repr__(self):\n        return (str(self.start) + "" -> "" + str(self.end))\n\n    def angle(self, other_arc):\n        """"""Oriented angle between two arcs.\n        """"""\n        if self.start == other_arc.start:\n            a__ = self.start\n            b__ = self.end\n            c__ = other_arc.end\n        elif self.start == other_arc.end:\n            a__ = self.start\n            b__ = self.end\n            c__ = other_arc.start\n        elif self.end == other_arc.end:\n            a__ = self.end\n            b__ = self.start\n            c__ = other_arc.start\n        elif self.end == other_arc.start:\n            a__ = self.end\n            b__ = self.start\n            c__ = other_arc.end\n        else:\n            raise ValueError(""No common point in angle computation."")\n\n        ua_ = a__.cross2cart(b__)\n        ub_ = a__.cross2cart(c__)\n\n        val = ua_.dot(ub_) / (ua_.norm() * ub_.norm())\n        if abs(val - 1) < EPSILON:\n            angle = 0\n        elif abs(val + 1) < EPSILON:\n            angle = np.pi\n        else:\n            angle = np.arccos(val)\n\n        n__ = ua_.normalize()\n        if n__.dot(c__.to_cart()) > 0:\n            return -angle\n        else:\n            return angle\n\n    def intersections(self, other_arc):\n        """"""Gives the two intersections of the greats circles defined by the\n       current arc and *other_arc*.\n       From http://williams.best.vwh.net/intersect.htm\n        """"""\n\n        if self.end.lon - self.start.lon > np.pi:\n            self.end.lon -= 2 * np.pi\n        if other_arc.end.lon - other_arc.start.lon > np.pi:\n            other_arc.end.lon -= 2 * np.pi\n        if self.end.lon - self.start.lon < -np.pi:\n            self.end.lon += 2 * np.pi\n        if other_arc.end.lon - other_arc.start.lon < -np.pi:\n            other_arc.end.lon += 2 * np.pi\n\n        ea_ = self.start.cross2cart(self.end).normalize()\n        eb_ = other_arc.start.cross2cart(other_arc.end).normalize()\n\n        cross = ea_.cross(eb_)\n        lat = np.arctan2(cross.cart[2],\n                         np.sqrt(cross.cart[0] ** 2 + cross.cart[1] ** 2))\n        lon = np.arctan2(cross.cart[1], cross.cart[0])\n\n        return (SCoordinate(lon, lat),\n                SCoordinate(modpi(lon + np.pi), -lat))\n\n    def intersects(self, other_arc):\n        """"""Check if the current arc and the *other_arc* intersect.\n\n        An arc is defined as the shortest tracks between two points.\n        """"""\n\n        return bool(self.intersection(other_arc))\n\n    def intersection(self, other_arc):\n        """"""Return where, if the current arc and the *other_arc* intersect.\n\n        None is returned if there is not intersection.\n        An arc is defined as the shortest tracks between two points.\n        """"""\n        if self == other_arc:\n            return None\n\n        for i in self.intersections(other_arc):\n            a__ = self.start\n            b__ = self.end\n            c__ = other_arc.start\n            d__ = other_arc.end\n\n            ab_ = a__.hdistance(b__)\n            cd_ = c__.hdistance(d__)\n\n            if(((i in (a__, b__)) or\n                (abs(a__.hdistance(i) + b__.hdistance(i) - ab_) < EPSILON)) and\n               ((i in (c__, d__)) or\n                    (abs(c__.hdistance(i) + d__.hdistance(i) - cd_) < EPSILON))):\n                return i\n        return None\n\n    def get_next_intersection(self, arcs, known_inter=None):\n        """"""Get the next intersection between the current arc and *arcs*\n        """"""\n        res = []\n        for arc in arcs:\n            inter = self.intersection(arc)\n            if (inter is not None and\n                    inter != arc.end and\n                    inter != self.end):\n                res.append((inter, arc))\n\n        def dist(args):\n            """"""distance key.\n            """"""\n            return self.start.distance(args[0])\n\n        take_next = False\n        for inter, arc in sorted(res, key=dist):\n            if known_inter is not None:\n                if known_inter == inter:\n                    take_next = True\n                elif take_next:\n                    return inter, arc\n            else:\n                return inter, arc\n\n        return None, None\n\n\nclass SphPolygon(object):\n    """"""Spherical polygon.\n\n    Vertices as a 2-column array of (col 1) lons and (col 2) lats is given in\n    radians.\n    The inside of the polygon is defined by the vertices being defined\n    clockwise around it.\n    """"""\n\n    def __init__(self, vertices, radius=1):\n        self.vertices = vertices\n        self.lon = self.vertices[:, 0]\n        self.lat = self.vertices[:, 1]\n        self.radius = radius\n        self.cvertices = np.array([np.cos(self.lat) * np.cos(self.lon),\n                                   np.cos(self.lat) * np.sin(self.lon),\n                                   np.sin(self.lat)]).T * radius\n        self.x__ = self.cvertices[:, 0]\n        self.y__ = self.cvertices[:, 1]\n        self.z__ = self.cvertices[:, 2]\n\n    def invert(self):\n        """"""Invert the polygon.""""""\n        self.vertices = np.flipud(self.vertices)\n        self.cvertices = np.flipud(self.cvertices)\n        self.lon = self.vertices[:, 0]\n        self.lat = self.vertices[:, 1]\n        self.x__ = self.cvertices[:, 0]\n        self.y__ = self.cvertices[:, 1]\n        self.z__ = self.cvertices[:, 2]\n\n    def inverse(self):\n        """"""Return an inverse of the polygon.""""""\n        return SphPolygon(np.flipud(self.vertices), radius=self.radius)\n\n    def aedges(self):\n        """"""Iterator over the edges, in arcs of Coordinates.""""""\n        for (lon_start, lat_start), (lon_stop, lat_stop) in self.edges():\n            yield Arc(SCoordinate(lon_start, lat_start),\n                      SCoordinate(lon_stop, lat_stop))\n\n    def edges(self):\n        """"""Iterator over the edges, in geographical coordinates.""""""\n        for i in range(len(self.lon) - 1):\n            yield (self.lon[i], self.lat[i]), (self.lon[i + 1], self.lat[i + 1])\n        yield (self.lon[i + 1], self.lat[i + 1]), (self.lon[0], self.lat[0])\n\n    def area(self):\n        """"""Find the area of a polygon.\n\n        The inside of the polygon is defined by having the vertices enumerated\n        clockwise around it.\n\n        Uses the algorithm described in [bev1987]_.\n\n        .. [bev1987] , Michael Bevis and Greg Cambareri,\n           ""Computing the area of a spherical polygon of arbitrary shape"",\n           in *Mathematical Geology*, May 1987, Volume 19, Issue 4, pp 335-346.\n\n        Note: The article mixes up longitudes and latitudes in equation 3! Look\n        at the fortran code appendix for the correct version.\n        """"""\n\n        phi_a = self.lat\n        phi_p = self.lat.take(np.arange(len(self.lat)) + 1, mode=""wrap"")\n        phi_b = self.lat.take(np.arange(len(self.lat)) + 2, mode=""wrap"")\n        lam_a = self.lon\n        lam_p = self.lon.take(np.arange(len(self.lon)) + 1, mode=""wrap"")\n        lam_b = self.lon.take(np.arange(len(self.lon)) + 2, mode=""wrap"")\n\n        new_lons_a = np.arctan2(np.sin(lam_a - lam_p) * np.cos(phi_a),\n                                np.sin(phi_a) * np.cos(phi_p)\n                                - np.cos(phi_a) * np.sin(phi_p)\n                                * np.cos(lam_a - lam_p))\n\n        new_lons_b = np.arctan2(np.sin(lam_b - lam_p) * np.cos(phi_b),\n                                np.sin(phi_b) * np.cos(phi_p)\n                                - np.cos(phi_b) * np.sin(phi_p)\n                                * np.cos(lam_b - lam_p))\n\n        alpha = new_lons_a - new_lons_b\n        alpha[alpha < 0] += 2 * np.pi\n\n        return (sum(alpha) - (len(self.lon) - 2) * np.pi) * self.radius ** 2\n\n    def _bool_oper(self, other, sign=1):\n        """"""Performs a boolean operation on this and *other* polygons.abs\n\n        By default, or when sign is 1, the union is perfomed. If sign is -1,\n        the intersection of the polygons is returned.\n\n        The algorithm works this way: find an intersection between the two\n        polygons. If none can be found, then the two polygons are either not\n        overlapping, or one is entirely included in the other. Otherwise,\n        follow the edges of a polygon until another intersection is\n        encountered, at which point you start following the edges of the other\n        polygon, and so on until you come back to the first intersection. In\n        which direction to follow the edges of the polygons depends if you are\n        interested in the union or the intersection of the two polygons.\n        """"""\n        def rotate_arcs(start_arc, arcs):\n            idx = arcs.index(start_arc)\n            return arcs[idx:] + arcs[:idx]\n\n        arcs1 = [edge for edge in self.aedges()]\n        arcs2 = [edge for edge in other.aedges()]\n\n        nodes = []\n\n        # find the first intersection, to start from.\n        for edge1 in arcs1:\n            inter, edge2 = edge1.get_next_intersection(arcs2)\n            if inter is not None and inter != edge1.end and inter != edge2.end:\n                break\n\n        # if no intersection is found, find out if the one poly is included in\n        # the other.\n        if inter is None:\n            polys = [0, self, other]\n            if self._is_inside(other):\n                return polys[-sign]\n            if other._is_inside(self):\n                return polys[sign]\n\n            return None\n\n        # starting from the intersection, follow the edges of one of the\n        # polygons.\n\n        while True:\n            arcs1 = rotate_arcs(edge1, arcs1)\n            arcs2 = rotate_arcs(edge2, arcs2)\n\n            narcs1 = arcs1 + [edge1]\n            narcs2 = arcs2 + [edge2]\n\n            arc1 = Arc(inter, edge1.end)\n            arc2 = Arc(inter, edge2.end)\n\n            if np.sign(arc1.angle(arc2)) != sign:\n                arcs1, arcs2 = arcs2, arcs1\n                narcs1, narcs2 = narcs2, narcs1\n\n            nodes.append(inter)\n\n            for edge1 in narcs1:\n                inter, edge2 = edge1.get_next_intersection(narcs2, inter)\n                if inter is not None:\n                    break\n                elif len(nodes) > 0 and edge1.end not in [nodes[-1], nodes[0]]:\n                    nodes.append(edge1.end)\n\n            if inter is None and len(nodes) > 2 and nodes[-1] == nodes[0]:\n                nodes = nodes[:-1]\n                break\n            if inter == nodes[0]:\n                break\n        return SphPolygon(np.array([(node.lon, node.lat) for node in nodes]), radius=self.radius)\n\n    def union(self, other):\n        """"""Return the union of this and `other` polygon.""""""\n        return self._bool_oper(other, 1)\n\n    def intersection(self, other):\n        """"""Return the intersection of this and `other` polygon.""""""\n        return self._bool_oper(other, -1)\n\n    def _is_inside(self, other):\n        """"""Checks if the polygon is entirely inside the other. Should be used\n        with :meth:`inter` first to check if the is a known intersection.\n        """"""\n\n        anti_lon_0 = self.lon[0] + np.pi\n        if anti_lon_0 > np.pi:\n            anti_lon_0 -= np.pi * 2\n\n        anti_lon_1 = self.lon[1] + np.pi\n        if anti_lon_1 > np.pi:\n            anti_lon_1 -= np.pi * 2\n\n        arc1 = Arc(SCoordinate(self.lon[1],\n                               self.lat[1]),\n                   SCoordinate(anti_lon_0,\n                               -self.lat[0]))\n\n        arc2 = Arc(SCoordinate(anti_lon_0,\n                               -self.lat[0]),\n                   SCoordinate(anti_lon_1,\n                               -self.lat[1]))\n\n        arc3 = Arc(SCoordinate(anti_lon_1,\n                               -self.lat[1]),\n                   SCoordinate(self.lon[0],\n                               self.lat[0]))\n\n        other_arcs = [edge for edge in other.aedges()]\n        for arc in [arc1, arc2, arc3]:\n            inter, other_arc = arc.get_next_intersection(other_arcs)\n            if inter is not None:\n                sarc = Arc(arc.start, inter)\n                earc = Arc(inter, other_arc.end)\n                return sarc.angle(earc) < 0\n        return other.area() > (2 * np.pi * other.radius ** 2)\n\n    def __str__(self):\n        return str(np.rad2deg(self.vertices))\n'"
pyresample/spherical_geometry.py,5,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010, 2015  Martin Raspaud\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Classes for spherical geometry operations""""""\n\nfrom __future__ import absolute_import\n\nimport math\nimport numpy as np\nimport warnings\n\nwarnings.warn(""This module will be removed in pyresample 2.0, please use the ""\n              ""`pyresample.spherical` module functions and class instead."",\n              DeprecationWarning)\n\ntry:\n    range = xrange\nexcept NameError:\n    pass\n\nEPSILON = 0.0000001\n\n# FIXME: this has not been tested with R != 1\n\n\nclass Coordinate(object):\n\n    """"""Point on earth in terms of lat and lon.\n    """"""\n    lat = None\n    lon = None\n    x__ = None\n    y__ = None\n    z__ = None\n\n    def __init__(self, lon=None, lat=None,\n                 x__=None, y__=None, z__=None, R__=1):\n        self.R__ = R__\n        if lat is not None and lon is not None:\n            if not(-180 <= lon <= 180 and -90 <= lat <= 90):\n                raise ValueError(\'Illegal (lon, lat) coordinates: (%s, %s)\'\n                                 % (lon, lat))\n            self.lat = math.radians(lat)\n            self.lon = math.radians(lon)\n            self._update_cart()\n        else:\n            self.x__ = x__\n            self.y__ = y__\n            self.z__ = z__\n            self._update_lonlat()\n\n    def _update_cart(self):\n        """"""Convert lon/lat to cartesian coordinates.\n        """"""\n\n        self.x__ = math.cos(self.lat) * math.cos(self.lon)\n        self.y__ = math.cos(self.lat) * math.sin(self.lon)\n        self.z__ = math.sin(self.lat)\n\n    def _update_lonlat(self):\n        """"""Convert cartesian to lon/lat.\n        """"""\n\n        self.lat = math.degrees(math.asin(self.z__ / self.R__))\n        self.lon = math.degrees(math.atan2(self.y__, self.x__))\n\n    def __ne__(self, other):\n        if(abs(self.lat - other.lat) < EPSILON and\n           abs(self.lon - other.lon) < EPSILON):\n            return 0\n        else:\n            return 1\n\n    def __eq__(self, other):\n        return not self.__ne__(other)\n\n    def __str__(self):\n        return str((math.degrees(self.lon), math.degrees(self.lat)))\n\n    def __repr__(self):\n        return str((math.degrees(self.lon), math.degrees(self.lat)))\n\n    def cross2cart(self, point):\n        """"""Compute the cross product, and convert to cartesian coordinates\n        (assuming radius 1).\n        """"""\n        lat1 = self.lat\n        lon1 = self.lon\n        lat2 = point.lat\n        lon2 = point.lon\n\n        res = Coordinate(\n            x__=(math.sin(lat1 - lat2) * math.sin((lon1 + lon2) / 2) *\n                 math.cos((lon1 - lon2) / 2) - math.sin(lat1 + lat2) *\n                 math.cos((lon1 + lon2) / 2) * math.sin((lon1 - lon2) / 2)),\n            y__=(math.sin(lat1 - lat2) * math.cos((lon1 + lon2) / 2) *\n                 math.cos((lon1 - lon2) / 2) + math.sin(lat1 + lat2) *\n                 math.sin((lon1 + lon2) / 2) * math.sin((lon1 - lon2) / 2)),\n            z__=(math.cos(lat1) * math.cos(lat2) * math.sin(lon1 - lon2)))\n\n        return res\n\n    def distance(self, point):\n        """"""Vincenty formula.\n        """"""\n        dlambda = self.lon - point.lon\n        num = ((math.cos(point.lat) * math.sin(dlambda)) ** 2 +\n               (math.cos(self.lat) * math.sin(point.lat) -\n                math.sin(self.lat) * math.cos(point.lat) *\n                math.cos(dlambda)) ** 2)\n        den = (math.sin(self.lat) * math.sin(point.lat) +\n               math.cos(self.lat) * math.cos(point.lat) * math.cos(dlambda))\n\n        return math.atan2(math.sqrt(num), den)\n\n    def norm(self):\n        """"""Return the norm of the vector.\n        """"""\n        return math.sqrt(self.x__ ** 2 + self.y__ ** 2 + self.z__ ** 2)\n\n    def normalize(self):\n        """"""normalize the vector.\n        """"""\n\n        norm = self.norm()\n        self.x__ /= norm\n        self.y__ /= norm\n        self.z__ /= norm\n\n        return self\n\n    def cross(self, point):\n        """"""cross product with another vector.\n        """"""\n        x__ = self.y__ * point.z__ - self.z__ * point.y__\n        y__ = self.z__ * point.x__ - self.x__ * point.z__\n        z__ = self.x__ * point.y__ - self.y__ * point.x__\n\n        return Coordinate(x__=x__, y__=y__, z__=z__)\n\n    def dot(self, point):\n        """"""dot product with another vector.\n        """"""\n        return (self.x__ * point.x__ +\n                self.y__ * point.y__ +\n                self.z__ * point.z__)\n\n\nclass Arc(object):\n\n    """"""An arc of the great circle between two points.\n    """"""\n    start = None\n    end = None\n\n    def __init__(self, start, end):\n        self.start, self.end = start, end\n\n    def center_angle(self):\n        """"""Angle of an arc at the center of the sphere.\n        """"""\n        val = (math.cos(self.start.lat - self.end.lat) +\n               math.cos(self.start.lon - self.end.lon) - 1)\n\n        if val > 1:\n            val = 1\n        elif val < -1:\n            val = -1\n\n        return math.acos(val)\n\n    def __eq__(self, other):\n        if(self.start == other.start and self.end == other.end):\n            return 1\n        return 0\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __str__(self):\n        return str((str(self.start), str(self.end)))\n\n    def angle(self, other_arc, snap=True):\n        """"""Oriented angle between two arcs.\n\n        Parameters\n        ----------\n        other_arc : pyresample.spherical_geometry.Arc\n        snap : boolean\n            Snap small angles to 0. Allows for detecting colinearity. Disable\n            snapping when calculating polygon areas as it might lead to\n            negative area values.\n        """"""\n        if self.start == other_arc.start:\n            a__ = self.start\n            b__ = self.end\n            c__ = other_arc.end\n        elif self.start == other_arc.end:\n            a__ = self.start\n            b__ = self.end\n            c__ = other_arc.start\n        elif self.end == other_arc.end:\n            a__ = self.end\n            b__ = self.start\n            c__ = other_arc.start\n        elif self.end == other_arc.start:\n            a__ = self.end\n            b__ = self.start\n            c__ = other_arc.end\n        else:\n            raise ValueError(""No common point in angle computation."")\n\n        ua_ = a__.cross(b__)\n        ub_ = a__.cross(c__)\n\n        val = ua_.dot(ub_) / (ua_.norm() * ub_.norm())\n        if snap:\n            if abs(val - 1) < EPSILON:\n                angle = 0\n            elif abs(val + 1) < EPSILON:\n                angle = math.pi\n            else:\n                angle = math.acos(val)\n        else:\n            if 0 <= val - 1 < EPSILON:\n                angle = 0\n            elif -EPSILON < val + 1 <= 0:\n                angle = math.pi\n            else:\n                angle = math.acos(val)\n\n        n__ = ua_.normalize()\n        if n__.dot(c__) > 0:\n            return -angle\n        else:\n            return angle\n\n    def intersections(self, other_arc):\n        """"""Gives the two intersections of the greats circles defined by the\n       current arc and *other_arc*.\n        """"""\n        if self.end.lon - self.start.lon > math.pi:\n            self.end.lon -= 2 * math.pi\n        if other_arc.end.lon - other_arc.start.lon > math.pi:\n            other_arc.end.lon -= 2 * math.pi\n        if self.end.lon - self.start.lon < -math.pi:\n            self.end.lon += 2 * math.pi\n        if other_arc.end.lon - other_arc.start.lon < -math.pi:\n            other_arc.end.lon += 2 * math.pi\n\n        ea_ = self.start.cross2cart(self.end).normalize()\n        eb_ = other_arc.start.cross2cart(other_arc.end).normalize()\n\n        cross = ea_.cross(eb_)\n        lat = math.atan2(cross.z__, math.sqrt(cross.x__ ** 2 + cross.y__ ** 2))\n        lon = math.atan2(-cross.y__, cross.x__)\n\n        return (Coordinate(math.degrees(lon), math.degrees(lat)),\n                Coordinate(math.degrees(modpi(lon + math.pi)),\n                           math.degrees(-lat)))\n\n    def intersects(self, other_arc):\n        """"""Says if two arcs defined by the current arc and the *other_arc*\n        intersect. An arc is defined as the shortest tracks between two points.\n        """"""\n        return bool(self.intersection(other_arc))\n\n    def intersection(self, other_arc):\n        """"""Says where, if two arcs defined by the current arc and the\n        *other_arc* intersect. An arc is defined as the shortest tracks between\n        two points.\n        """"""\n        for i in self.intersections(other_arc):\n            a__ = self.start\n            b__ = self.end\n            c__ = other_arc.start\n            d__ = other_arc.end\n\n            ab_ = a__.distance(b__)\n            cd_ = c__.distance(d__)\n\n            if(abs(a__.distance(i) + b__.distance(i) - ab_) < EPSILON and\n               abs(c__.distance(i) + d__.distance(i) - cd_) < EPSILON):\n                return i\n        return None\n\n\ndef modpi(val):\n    """"""Puts *val* between -pi and pi.\n    """"""\n    return (val + math.pi) % (2 * math.pi) - math.pi\n\n\ndef get_polygon_area(corners):\n    """"""Get the area of the convex area defined by *corners*.\n    """"""\n    # We assume the earth is spherical !!!\n    # Should be the radius of the earth at the observed position\n    R = 1\n\n    c1_ = corners[0]\n    area = 0\n\n    for idx in range(1, len(corners) - 1):\n        b1_ = Arc(c1_, corners[idx])\n        b2_ = Arc(c1_, corners[idx + 1])\n        b3_ = Arc(corners[idx], corners[idx + 1])\n        e__ = (abs(b1_.angle(b2_, snap=False)) +\n               abs(b2_.angle(b3_, snap=False)) +\n               abs(b3_.angle(b1_, snap=False)))\n        area += e__ - math.pi\n    return R ** 2 * area\n\n\ndef get_intersections(b__, boundaries):\n    """"""Get the intersections of *b__* with *boundaries*.\n    Returns both the intersection coordinates and the concerned boundaries.\n    """"""\n    intersections = []\n    bounds = []\n    for other_b in boundaries:\n        inter = b__.intersection(other_b)\n        if inter is not None:\n            intersections.append(inter)\n            bounds.append(other_b)\n    return intersections, bounds\n\n\ndef get_first_intersection(b__, boundaries):\n    """"""Get the first intersection on *b__* with *boundaries*.\n    """"""\n    intersections, bounds = get_intersections(b__, boundaries)\n    del bounds\n    dists = np.array([b__.start.distance(p__) for p__ in intersections])\n    indices = dists.argsort()\n    if len(intersections) > 0:\n        return intersections[indices[0]]\n    return None\n\n\ndef get_next_intersection(p__, b__, boundaries):\n    """"""Get the next intersection from the intersection of arcs *p__* and *b__*\n    along segment *b__* with *boundaries*.\n    """"""\n    new_b = Arc(p__, b__.end)\n    intersections, bounds = get_intersections(new_b, boundaries)\n    dists = np.array([b__.start.distance(p2) for p2 in intersections])\n    indices = dists.argsort()\n    if len(intersections) > 0 and intersections[indices[0]] != p__:\n        return intersections[indices[0]], bounds[indices[0]]\n    elif len(intersections) > 1:\n        return intersections[indices[1]], bounds[indices[1]]\n    return None, None\n\n\ndef point_inside(point, corners):\n    """"""Is a point inside the 4 corners ? This uses great circle arcs as area\n    boundaries.\n    """"""\n    arc1 = Arc(corners[0], corners[1])\n    arc2 = Arc(corners[1], corners[2])\n    arc3 = Arc(corners[2], corners[3])\n    arc4 = Arc(corners[3], corners[0])\n\n    arc5 = Arc(corners[1], point)\n    arc6 = Arc(corners[3], point)\n\n    angle1 = modpi(arc1.angle(arc2))\n    angle1bis = modpi(arc1.angle(arc5))\n\n    angle2 = modpi(arc3.angle(arc4))\n    angle2bis = modpi(arc3.angle(arc6))\n\n    return (np.sign(angle1) == np.sign(angle1bis) and\n            abs(angle1) > abs(angle1bis) and\n            np.sign(angle2) == np.sign(angle2bis) and\n            abs(angle2) > abs(angle2bis))\n\n\ndef intersection_polygon(area_corners, segment_corners):\n    """"""Get the intersection polygon between two areas.\n    """"""\n    area_boundaries = [Arc(area_corners[0], area_corners[1]),\n                       Arc(area_corners[1], area_corners[2]),\n                       Arc(area_corners[2], area_corners[3]),\n                       Arc(area_corners[3], area_corners[0])]\n    segment_boundaries = [Arc(segment_corners[0], segment_corners[1]),\n                          Arc(segment_corners[1], segment_corners[2]),\n                          Arc(segment_corners[2], segment_corners[3]),\n                          Arc(segment_corners[3], segment_corners[0])]\n\n    angle1 = area_boundaries[0].angle(area_boundaries[1])\n    angle2 = segment_boundaries[0].angle(segment_boundaries[1])\n    if np.sign(angle1) != np.sign(angle2):\n        segment_corners.reverse()\n        segment_boundaries = [Arc(segment_corners[0], segment_corners[1]),\n                              Arc(segment_corners[1], segment_corners[2]),\n                              Arc(segment_corners[2], segment_corners[3]),\n                              Arc(segment_corners[3], segment_corners[0])]\n    poly = []\n\n    boundaries = area_boundaries\n    other_boundaries = segment_boundaries\n\n    b__ = None\n\n    for b__ in boundaries:\n        if point_inside(b__.start, segment_corners):\n            poly.append(b__.start)\n            break\n        else:\n            inter = get_first_intersection(b__, other_boundaries)\n            if inter is not None:\n                poly.append(inter)\n                break\n    if len(poly) == 0:\n        return None\n    while len(poly) < 2 or poly[0] != poly[-1]:\n        inter, b2_ = get_next_intersection(poly[-1], b__, other_boundaries)\n        if inter is None:\n            poly.append(b__.end)\n            idx = (boundaries.index(b__) + 1) % len(boundaries)\n            b__ = boundaries[idx]\n        else:\n            poly.append(inter)\n            b__ = b2_\n            boundaries, other_boundaries = other_boundaries, boundaries\n    return poly[:-1]\n'"
pyresample/version.py,0,"b'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""$Format:%d$""\n    git_full = ""$Format:%H$""\n    git_date = ""$Format:%ci$""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""pep440""\n    cfg.tag_prefix = ""v""\n    cfg.parentdir_prefix = ""None""\n    cfg.versionfile_source = ""pyresample/version.py""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%s*"" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n'"
docs/source/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# pyresample documentation build configuration file, created by\n# sphinx-quickstart on Tue Jan  5 13:01:32 2010.\n#\n# This file is execfile()d with the current directory set to its containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\nclass Mock(object):\n\n    def __init__(self, *args, **kwargs):\n        pass\n\n    def __call__(self, *args, **kwargs):\n        return Mock()\n\n    @classmethod\n    def __getattr__(cls, name):\n        if name in (\'__file__\', \'__path__\'):\n            return \'/dev/null\'\n        elif name[0] == name[0].upper():\n            mockType = type(name, (), {})\n            mockType.__module__ = __name__\n            return mockType\n        elif name == ""inf"":\n            return 0\n        else:\n            return Mock()\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'../../\'))\nfrom pyresample import __version__  # noqa\n\n# -- General configuration -----------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be extensions\n# coming with Sphinx (named \'sphinx.ext.*\') or your custom ones.\nextensions = [\n    \'sphinx.ext.doctest\', \'sphinx.ext.autodoc\', \'sphinx.ext.napoleon\', \'sphinx.ext.intersphinx\']\n\n# DocTest Settings\n# don\'t run regular >>> code blocks\ndoctest_test_doctest_blocks = \'\'\n# setup imports so we can skip certain doctests\ndoctest_global_setup = \'\'\'\ntry:\n    import matplotlib.pyplot as plt\nexcept ImportError:\n    plt = None\n\ntry:\n    import cartopy\nexcept ImportError:\n    cartopy = None\n\ntry:\n    from mpl_toolkits.basemap import Basemap\nexcept ImportError:\n    Basemap = None\n\'\'\'\n\n# Napoleon Settings (to support numpy style docs)\nnapoleon_numpy_docstring = True\nnapoleon_use_admonition_for_examples = True\nnapoleon_use_admonition_for_notes = True\nnapoleon_use_admonition_for_references = True\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix of source filenames.\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n#source_encoding = \'utf-8\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'pyresample\'\ncopyright = u\'2013, Esben S. Nielsen\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n\nversion = __version__.split(\'+\')[0]\n# The full version, including alpha/beta/rc tags.\nrelease = __version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of documents that shouldn\'t be included in the build.\n#unused_docs = []\n\n# List of directories, relative to source directory, that shouldn\'t be searched\n# for source files.\nexclude_trees = []\n\n# The reST default role (used for this markup: `text`) to use for all documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n\n# -- Options for HTML output ---------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  Major themes that come with\n# Sphinx are currently \'default\' and \'sphinxdoc\'.\nhtml_theme = \'default\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# ""<project> v<release> documentation"".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon of the\n# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# If not \'\', a \'Last updated on:\' timestamp is inserted at every page bottom,\n# using the given strftime format.\n#html_last_updated_fmt = \'%b %d, %Y\'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_use_modindex = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# If nonempty, this is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = \'\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'pyresampledoc\'\n\n\n# -- Options for LaTeX output --------------------------------------------\n\n# The paper size (\'letter\' or \'a4\').\n#latex_paper_size = \'letter\'\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#latex_font_size = \'10pt\'\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass [howto/manual]).\nlatex_documents = [\n    (\'index\', \'pyresample.tex\', u\'pyresample Documentation\',\n     u\'Esben S. Nielsen\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n# latex_use_parts = False\n\n# Additional stuff for the LaTeX preamble.\n# latex_preamble = \'\'\n\n# Documents to append as an appendix to all manuals.\n# latex_appendices = []\n\n# If false, no module index is generated.\n# latex_use_modindex = True\n\n# Intersphinx extention\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3\', None),\n    \'numpy\': (\'https://docs.scipy.org/doc/numpy\', None),\n    \'scipy\': (\'https://docs.scipy.org/doc/scipy/reference\', None),\n    \'xarray\': (\'https://xarray.pydata.org/en/stable\', None),\n    \'dask\': (\'https://docs.dask.org/en/latest\', None),\n    \'pyresample\': (\'https://pyresample.readthedocs.io/en/stable\', None),\n    \'trollsift\': (\'https://trollsift.readthedocs.io/en/stable\', None),\n    \'trollimage\': (\'https://trollimage.readthedocs.io/en/stable\', None),\n    \'pyproj\': (\'https://pyproj4.github.io/pyproj/dev/\', None),\n    \'proj4\': (\'https://proj.org\', None),\n}\n'"
pyresample/bilinear/__init__.py,55,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2017\n\n# Author(s):\n\n#   Panu Lahtinen <panu.lahtinen@fmi.fi>\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Code for resampling using bilinear algorithm for irregular grids.\n\nThe algorithm is taken from\n\nhttp://www.ahinson.com/algorithms_general/Sections/InterpolationRegression/InterpolationIrregularBilinear.pdf\n\n""""""\n\nimport numpy as np\nfrom pyresample._spatial_mp import Proj\nimport warnings\n\nfrom pyresample import kd_tree\n\n\ndef resample_bilinear(data, source_geo_def, target_area_def, radius=50e3,\n                      neighbours=32, nprocs=1, fill_value=0,\n                      reduce_data=True, segments=None, epsilon=0):\n    """"""Resample using bilinear interpolation.\n\n    data : numpy array\n        Array of single channel data points or\n        (source_geo_def.shape, k) array of k channels of datapoints\n    source_geo_def : object\n        Geometry definition of source data\n    target_area_def : object\n        Geometry definition of target area\n    radius : float, optional\n        Cut-off distance in meters\n    neighbours : int, optional\n        Number of neighbours to consider for each grid point when\n        searching the closest corner points\n    nprocs : int, optional\n        Number of processor cores to be used for getting neighbour info\n    fill_value : {int, None}, optional\n        Set undetermined pixels to this value.\n        If fill_value is None a masked array is returned with undetermined\n        pixels masked\n    reduce_data : bool, optional\n        Perform initial coarse reduction of source dataset in order\n        to reduce execution time\n    segments : int or None\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n    epsilon : float, optional\n        Allowed uncertainty in meters. Increasing uncertainty\n        reduces execution time\n\n    Returns\n    -------\n    data : numpy array\n        Source data resampled to target geometry\n    """"""\n\n    # Calculate the resampling information\n    t__, s__, input_idxs, idx_ref = get_bil_info(source_geo_def,\n                                                 target_area_def,\n                                                 radius=radius,\n                                                 neighbours=neighbours,\n                                                 nprocs=nprocs,\n                                                 masked=False,\n                                                 reduce_data=reduce_data,\n                                                 segments=segments,\n                                                 epsilon=epsilon)\n\n    data = _check_data_shape(data, input_idxs)\n\n    result = np.nan * np.zeros((target_area_def.size, data.shape[1]))\n    for i in range(data.shape[1]):\n        result[:, i] = get_sample_from_bil_info(data[:, i], t__, s__,\n                                                input_idxs, idx_ref,\n                                                output_shape=None)\n\n    if fill_value is None:\n        result = np.ma.masked_invalid(result)\n    else:\n        result[np.isnan(result)] = fill_value\n\n    # Reshape to target area shape\n    shp = target_area_def.shape\n    result = result.reshape((shp[0], shp[1], data.shape[1]))\n    # Remove extra dimensions\n    result = np.squeeze(result)\n\n    return result\n\n\ndef get_sample_from_bil_info(data, t__, s__, input_idxs, idx_arr,\n                             output_shape=None):\n    """"""Resample data using bilinear interpolation.\n\n    Parameters\n    ----------\n    data : numpy array\n        1d array to be resampled\n    t__ : numpy array\n        Vertical fractional distances from corner to the new points\n    s__ : numpy array\n        Horizontal fractional distances from corner to the new points\n    input_idxs : numpy array\n        Valid indices in the input data\n    idx_arr : numpy array\n        Mapping array from valid source points to target points\n    output_shape : tuple, optional\n        Tuple of (y, x) dimension for the target projection.\n        If None (default), do not reshape data.\n\n    Returns\n    -------\n    result : numpy array\n        Source data resampled to target geometry\n    """"""\n\n    # Reduce data\n    new_data = data[input_idxs]\n    # Add a small ""machine epsilon"" so that tiny variations are not discarded\n    epsilon = 1e-6\n    data_min = np.nanmin(new_data) - epsilon\n    data_max = np.nanmax(new_data) + epsilon\n\n    new_data = new_data[idx_arr]\n\n    # Get neighbour data to separate variables\n    p_1 = new_data[:, 0]\n    p_2 = new_data[:, 1]\n    p_3 = new_data[:, 2]\n    p_4 = new_data[:, 3]\n\n    result = (p_1 * (1 - s__) * (1 - t__) +\n              p_2 * s__ * (1 - t__) +\n              p_3 * (1 - s__) * t__ +\n              p_4 * s__ * t__)\n\n    if hasattr(result, \'mask\'):\n        mask = result.mask\n        result = result.data\n        result[mask] = np.nan\n\n    try:\n        with np.errstate(invalid=\'ignore\'):\n            idxs = (result > data_max) | (result < data_min)\n        result[idxs] = np.nan\n    except TypeError:\n        pass\n\n    if output_shape is not None:\n        result = result.reshape(output_shape)\n\n    return result\n\n\ndef get_bil_info(source_geo_def, target_area_def, radius=50e3, neighbours=32,\n                 nprocs=1, masked=False, reduce_data=True, segments=None,\n                 epsilon=0):\n    """"""Calculate information needed for bilinear resampling.\n\n    source_geo_def : object\n        Geometry definition of source data\n    target_area_def : object\n        Geometry definition of target area\n    radius : float, optional\n        Cut-off distance in meters\n    neighbours : int, optional\n        Number of neighbours to consider for each grid point when\n        searching the closest corner points\n    nprocs : int, optional\n        Number of processor cores to be used for getting neighbour info\n    masked : bool, optional\n        If true, return masked arrays, else return np.nan values for\n        invalid points (default)\n    reduce_data : bool, optional\n        Perform initial coarse reduction of source dataset in order\n        to reduce execution time\n    segments : int or None\n        Number of segments to use when resampling.\n        If set to None an estimate will be calculated\n    epsilon : float, optional\n        Allowed uncertainty in meters. Increasing uncertainty\n        reduces execution time\n\n    Returns\n    -------\n    t__ : numpy array\n        Vertical fractional distances from corner to the new points\n    s__ : numpy array\n        Horizontal fractional distances from corner to the new points\n    input_idxs : numpy array\n        Valid indices in the input data\n    idx_arr : numpy array\n        Mapping array from valid source points to target points\n    """"""\n\n    # Check source_geo_def\n    # if isinstance(source_geo_def, tuple):\n    #     from pyresample.geometry import SwathDefinition\n    #     lons, lats = _mask_coordinates(source_geo_def[0], source_geo_def[1])\n    #     source_geo_def = SwathDefinition(lons, lats)\n\n    # Calculate neighbour information\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        (input_idxs, output_idxs, idx_ref, dists) = \\\n            kd_tree.get_neighbour_info(source_geo_def, target_area_def,\n                                       radius, neighbours=neighbours,\n                                       nprocs=nprocs, reduce_data=reduce_data,\n                                       segments=segments, epsilon=epsilon)\n\n    del output_idxs, dists\n\n    # Reduce index reference\n    input_size = input_idxs.sum()\n    index_mask = (idx_ref == input_size)\n    idx_ref = np.where(index_mask, 0, idx_ref)\n\n    # Get output projection as pyproj object\n    proj = Proj(target_area_def.proj_str)\n\n    # Get output x/y coordinates\n    out_x, out_y = _get_output_xy(target_area_def, proj)\n\n    # Get input x/y coordinates\n    in_x, in_y = _get_input_xy(source_geo_def, proj, input_idxs, idx_ref)\n\n    # Get the four closest corner points around each output location\n    pt_1, pt_2, pt_3, pt_4, idx_ref = \\\n        _get_bounding_corners(in_x, in_y, out_x, out_y, neighbours, idx_ref)\n\n    # Calculate vertical and horizontal fractional distances t and s\n    t__, s__ = _get_ts(pt_1, pt_2, pt_3, pt_4, out_x, out_y)\n\n    # Mask NaN values\n    if masked:\n        mask = np.isnan(t__) | np.isnan(s__)\n        t__ = np.ma.masked_where(mask, t__)\n        s__ = np.ma.masked_where(mask, s__)\n\n    return t__, s__, input_idxs, idx_ref\n\n\ndef _get_ts(pt_1, pt_2, pt_3, pt_4, out_x, out_y):\n    """"""Calculate vertical and horizontal fractional distances t and s""""""\n\n    # General case, ie. where the the corners form an irregular rectangle\n    t__, s__ = _get_ts_irregular(pt_1, pt_2, pt_3, pt_4, out_y, out_x)\n\n    # Cases where verticals are parallel\n    idxs = np.isnan(t__) | np.isnan(s__)\n    # Remove extra dimensions\n    idxs = idxs.ravel()\n\n    if np.any(idxs):\n        t__[idxs], s__[idxs] = \\\n            _get_ts_uprights_parallel(pt_1[idxs, :], pt_2[idxs, :],\n                                      pt_3[idxs, :], pt_4[idxs, :],\n                                      out_y[idxs], out_x[idxs])\n\n    # Cases where both verticals and horizontals are parallel\n    idxs = np.isnan(t__) | np.isnan(s__)\n    # Remove extra dimensions\n    idxs = idxs.ravel()\n    if np.any(idxs):\n        t__[idxs], s__[idxs] = \\\n            _get_ts_parallellogram(pt_1[idxs, :], pt_2[idxs, :], pt_3[idxs, :],\n                                   out_y[idxs], out_x[idxs])\n\n    with np.errstate(invalid=\'ignore\'):\n        idxs = (t__ < 0) | (t__ > 1) | (s__ < 0) | (s__ > 1)\n    t__[idxs] = np.nan\n    s__[idxs] = np.nan\n\n    return t__, s__\n\n\ndef _get_ts_irregular(pt_1, pt_2, pt_3, pt_4, out_y, out_x):\n    """"""Get parameters for the case where none of the sides are parallel.""""""\n\n    # Get parameters for the quadratic equation\n    a__, b__, c__ = _calc_abc(pt_1, pt_2, pt_3, pt_4, out_y, out_x)\n\n    # Get the valid roots from interval [0, 1]\n    t__ = _solve_quadratic(a__, b__, c__, min_val=0., max_val=1.)\n\n    # Calculate parameter s\n    s__ = _solve_another_fractional_distance(t__, pt_1[:, 1], pt_3[:, 1],\n                                             pt_2[:, 1], pt_4[:, 1], out_y)\n\n    return t__, s__\n\n\ndef _get_ts_uprights_parallel(pt_1, pt_2, pt_3, pt_4, out_y, out_x):\n    """"""Get parameters for the case where uprights are parallel""""""\n\n    # Get parameters for the quadratic equation\n    a__, b__, c__ = _calc_abc(pt_1, pt_3, pt_2, pt_4, out_y, out_x)\n\n    # Get the valid roots from interval [0, 1]\n    s__ = _solve_quadratic(a__, b__, c__, min_val=0., max_val=1.)\n\n    # Calculate parameter t\n    t__ = _solve_another_fractional_distance(s__, pt_1[:, 1], pt_2[:, 1],\n                                             pt_3[:, 1], pt_4[:, 1], out_y)\n\n    return t__, s__\n\n\ndef _get_ts_parallellogram(pt_1, pt_2, pt_3, out_y, out_x):\n    """"""Get parameters for the case where uprights are parallel""""""\n\n    # Pairwise longitudal separations between reference points\n    x_21 = pt_2[:, 0] - pt_1[:, 0]\n    x_31 = pt_3[:, 0] - pt_1[:, 0]\n\n    # Pairwise latitudal separations between reference points\n    y_21 = pt_2[:, 1] - pt_1[:, 1]\n    y_31 = pt_3[:, 1] - pt_1[:, 1]\n\n    t__ = (x_21 * (out_y - pt_1[:, 1]) - y_21 * (out_x - pt_1[:, 0])) / \\\n          (x_21 * y_31 - y_21 * x_31)\n    with np.errstate(invalid=\'ignore\'):\n        idxs = (t__ < 0.) | (t__ > 1.)\n    t__[idxs] = np.nan\n\n    s__ = (out_x - pt_1[:, 0] + x_31 * t__) / x_21\n\n    with np.errstate(invalid=\'ignore\'):\n        idxs = (s__ < 0.) | (s__ > 1.)\n    s__[idxs] = np.nan\n\n    return t__, s__\n\n\ndef _solve_another_fractional_distance(f__, y_1, y_2, y_3, y_4, out_y):\n    """"""Solve parameter t__ from s__, or vice versa.  For solving s__,\n    switch order of y_2 and y_3.""""""\n    y_21 = y_2 - y_1\n    y_43 = y_4 - y_3\n\n    with np.errstate(divide=\'ignore\'):\n        g__ = ((out_y - y_1 - y_21 * f__) /\n               (y_3 + y_43 * f__ - y_1 - y_21 * f__))\n\n    # Limit values to interval [0, 1]\n    with np.errstate(invalid=\'ignore\'):\n        idxs = (g__ < 0) | (g__ > 1)\n    g__[idxs] = np.nan\n\n    return g__\n\n\ndef _calc_abc(pt_1, pt_2, pt_3, pt_4, out_y, out_x):\n    """"""Calculate coefficients for quadratic equation for\n    _get_ts_irregular() and _get_ts_uprights().  For _get_ts_uprights\n    switch order of pt_2 and pt_3.\n    """"""\n    # Pairwise longitudal separations between reference points\n    x_21 = pt_2[:, 0] - pt_1[:, 0]\n    x_31 = pt_3[:, 0] - pt_1[:, 0]\n    x_42 = pt_4[:, 0] - pt_2[:, 0]\n\n    # Pairwise latitudal separations between reference points\n    y_21 = pt_2[:, 1] - pt_1[:, 1]\n    y_31 = pt_3[:, 1] - pt_1[:, 1]\n    y_42 = pt_4[:, 1] - pt_2[:, 1]\n\n    a__ = x_31 * y_42 - y_31 * x_42\n    b__ = out_y * (x_42 - x_31) - out_x * (y_42 - y_31) + \\\n        x_31 * pt_2[:, 1] - y_31 * pt_2[:, 0] + \\\n        y_42 * pt_1[:, 0] - x_42 * pt_1[:, 1]\n    c__ = out_y * x_21 - out_x * y_21 + pt_1[:, 0] * pt_2[:, 1] - \\\n        pt_2[:, 0] * pt_1[:, 1]\n\n    return a__, b__, c__\n\n\ndef _mask_coordinates(lons, lats):\n    """"""Mask invalid coordinate values""""""\n    lons = lons.ravel()\n    lats = lats.ravel()\n    idxs = ((lons < -180.) | (lons > 180.) |\n            (lats < -90.) | (lats > 90.))\n    if hasattr(lons, \'mask\'):\n        lons = np.ma.masked_where(idxs | lons.mask, lons)\n    else:\n        lons[idxs] = np.nan\n    if hasattr(lats, \'mask\'):\n        lats = np.ma.masked_where(idxs | lats.mask, lats)\n    else:\n        lats[idxs] = np.nan\n\n    return lons, lats\n\n\ndef _get_corner(stride, valid, in_x, in_y, idx_ref):\n    """"""Get closest set of coordinates from the *valid* locations""""""\n    # Find the closest valid pixels, if any\n    idxs = np.argmax(valid, axis=1)\n    # Check which of these were actually valid\n    invalid = np.invert(np.max(valid, axis=1))\n\n    # Replace invalid points with np.nan\n    x__ = in_x[stride, idxs]\n    x__[invalid] = np.nan\n    y__ = in_y[stride, idxs]\n    y__[invalid] = np.nan\n    idx = idx_ref[stride, idxs]\n\n    return x__, y__, idx\n\n\ndef _get_bounding_corners(in_x, in_y, out_x, out_y, neighbours, idx_ref):\n    """"""Get four closest locations from (in_x, in_y) so that they form a\n    bounding rectangle around the requested location given by (out_x,\n    out_y).\n    """"""\n\n    # Find four closest pixels around the target location\n\n    # Tile output coordinates to same shape as neighbour info\n    out_x_tile = np.tile(out_x, (neighbours, 1)).T\n    out_y_tile = np.tile(out_y, (neighbours, 1)).T\n\n    # Get differences in both directions\n    x_diff = out_x_tile - in_x\n    y_diff = out_y_tile - in_y\n\n    stride = np.arange(x_diff.shape[0])\n\n    # Upper left source pixel\n    valid = (x_diff > 0) & (y_diff < 0)\n    x_1, y_1, idx_1 = _get_corner(stride, valid, in_x, in_y, idx_ref)\n\n    # Upper right source pixel\n    valid = (x_diff < 0) & (y_diff < 0)\n    x_2, y_2, idx_2 = _get_corner(stride, valid, in_x, in_y, idx_ref)\n\n    # Lower left source pixel\n    valid = (x_diff > 0) & (y_diff > 0)\n    x_3, y_3, idx_3 = _get_corner(stride, valid, in_x, in_y, idx_ref)\n\n    # Lower right source pixel\n    valid = (x_diff < 0) & (y_diff > 0)\n    x_4, y_4, idx_4 = _get_corner(stride, valid, in_x, in_y, idx_ref)\n\n    # Combine sorted indices to idx_ref\n    idx_ref = np.vstack((idx_1, idx_2, idx_3, idx_4)).T\n\n    return (np.vstack((x_1, y_1)).T, np.vstack((x_2, y_2)).T,\n            np.vstack((x_3, y_3)).T, np.vstack((x_4, y_4)).T, idx_ref)\n\n\ndef _solve_quadratic(a__, b__, c__, min_val=0.0, max_val=1.0):\n    """"""Solve quadratic equation and return the valid roots from interval\n    [*min_val*, *max_val*]\n\n    """"""\n\n    def int_and_float_to_numpy(val):\n        if not isinstance(val, np.ndarray):\n            if isinstance(val, (int, float)):\n                val = [val]\n            val = np.array(val)\n        return val\n\n    a__ = int_and_float_to_numpy(a__)\n    b__ = int_and_float_to_numpy(b__)\n    c__ = int_and_float_to_numpy(c__)\n\n    discriminant = b__ * b__ - 4 * a__ * c__\n\n    # Solve the quadratic polynomial\n    with np.errstate(invalid=\'ignore\', divide=\'ignore\'):\n        x_1 = (-b__ + np.sqrt(discriminant)) / (2 * a__)\n        x_2 = (-b__ - np.sqrt(discriminant)) / (2 * a__)\n\n    # Find valid solutions, ie. 0 <= t <= 1\n    x__ = x_1.copy()\n    with np.errstate(invalid=\'ignore\'):\n        idxs = (x_1 < min_val) | (x_1 > max_val)\n    x__[idxs] = x_2[idxs]\n\n    with np.errstate(invalid=\'ignore\'):\n        idxs = (x__ < min_val) | (x__ > max_val)\n    x__[idxs] = np.nan\n\n    return x__\n\n\ndef _get_output_xy(target_area_def, proj):\n    """"""Get x/y coordinates of the target grid.""""""\n    # Read output coordinates\n    out_lons, out_lats = target_area_def.get_lonlats()\n\n    # Replace masked arrays with np.nan\'d ndarrays\n    out_lons = _convert_masks_to_nans(out_lons)\n    out_lats = _convert_masks_to_nans(out_lats)\n\n    # Mask invalid coordinates\n    out_lons, out_lats = _mask_coordinates(out_lons, out_lats)\n\n    # Convert coordinates to output projection x/y space\n    out_x, out_y = proj(out_lons, out_lats)\n\n    return out_x, out_y\n\n\ndef _get_input_xy(source_geo_def, proj, input_idxs, idx_ref):\n    """"""Get x/y coordinates for the input area and reduce the data.""""""\n    in_lons, in_lats = source_geo_def.get_lonlats()\n\n    # Select valid locations\n    in_lons = in_lons.ravel()[input_idxs]\n    in_lats = in_lats.ravel()[input_idxs]\n\n    # Mask invalid values\n    in_lons, in_lats = _mask_coordinates(in_lons, in_lats)\n\n    # Expand input coordinates for each output location\n    in_lons = in_lons[idx_ref]\n    in_lats = in_lats[idx_ref]\n\n    # Replace masked arrays with np.nan\'d ndarrays\n    in_lons = _convert_masks_to_nans(in_lons)\n    in_lats = _convert_masks_to_nans(in_lats)\n\n    # Convert coordinates to output projection x/y space\n    in_x, in_y = proj(in_lons, in_lats)\n\n    return in_x, in_y\n\n\ndef _convert_masks_to_nans(arr):\n    """"""Remove masked array masks and replace corresponding values with nans""""""\n    if hasattr(arr, \'mask\'):\n        mask = arr.mask\n        arr = arr.data\n        arr[mask] = np.nan\n    return arr\n\n\ndef _check_data_shape(data, input_idxs):\n    """"""Check data shape and adjust if necessary.""""""\n    # Handle multiple datasets\n    if data.ndim > 2 and data.shape[0] * data.shape[1] == input_idxs.shape[0]:\n        data = data.reshape(data.shape[0] * data.shape[1], data.shape[2])\n    # Also ravel single dataset\n    elif data.shape[0] != input_idxs.size:\n        data = data.ravel()\n\n    # Ensure two dimensions\n    if data.ndim == 1:\n        data = np.expand_dims(data, 1)\n\n    return data\n'"
pyresample/bilinear/xarr.py,34,"b'""""""XArray version of bilinear interpolation.""""""\n\nimport warnings\n\ntry:\n    from xarray import DataArray\n    import dask.array as da\nexcept ImportError:\n    DataArray = None\n    da = None\n\nimport numpy as np\n\nfrom pyresample._spatial_mp import Proj\n\nfrom pykdtree.kdtree import KDTree\nfrom pyresample import data_reduce, geometry, CHUNK_SIZE\n\nCACHE_INDICES = [\'bilinear_s\',\n                 \'bilinear_t\',\n                 \'slices_x\',\n                 \'slices_y\',\n                 \'mask_slices\',\n                 \'out_coords_x\',\n                 \'out_coords_y\']\n\n\nclass XArrayResamplerBilinear(object):\n    """"""Bilinear interpolation using XArray.""""""\n\n    def __init__(self,\n                 source_geo_def,\n                 target_geo_def,\n                 radius_of_influence,\n                 neighbours=32,\n                 epsilon=0,\n                 reduce_data=True):\n        """"""\n        Initialize resampler.\n\n        Parameters\n        ----------\n        source_geo_def : object\n            Geometry definition of source\n        target_geo_def : object\n            Geometry definition of target\n        radius_of_influence : float\n            Cut off distance in meters\n        neighbours : int, optional\n            The number of neigbours to consider for each grid point\n        epsilon : float, optional\n            Allowed uncertainty in meters. Increasing uncertainty\n            reduces execution time\n        reduce_data : bool, optional\n            Perform initial coarse reduction of source dataset in order\n            to reduce execution time\n\n        """"""\n        if da is None:\n            raise ImportError(""Missing \'xarray\' and \'dask\' dependencies"")\n\n        self.valid_input_index = None\n        self.valid_output_index = None\n        self.index_array = None\n        self.distance_array = None\n        self.bilinear_t = None\n        self.bilinear_s = None\n        self.slices_x = None\n        self.slices_y = None\n        self.slices = {\'x\': self.slices_x, \'y\': self.slices_y}\n        self.mask_slices = None\n        self.out_coords_x = None\n        self.out_coords_y = None\n        self.out_coords = {\'x\': self.out_coords_x, \'y\': self.out_coords_y}\n        self.neighbours = neighbours\n        self.epsilon = epsilon\n        self.reduce_data = reduce_data\n        self.source_geo_def = source_geo_def\n        self.target_geo_def = target_geo_def\n        self.radius_of_influence = radius_of_influence\n\n    def get_bil_info(self):\n        """"""Return neighbour info.\n\n        Returns\n        -------\n        t__ : numpy array\n            Vertical fractional distances from corner to the new points\n        s__ : numpy array\n            Horizontal fractional distances from corner to the new points\n        valid_input_index : numpy array\n            Valid indices in the input data\n        index_array : numpy array\n            Mapping array from valid source points to target points\n\n        """"""\n        if self.source_geo_def.size < self.neighbours:\n            warnings.warn(\'Searching for %s neighbours in %s data points\' %\n                          (self.neighbours, self.source_geo_def.size))\n\n        # Create kd-tree\n        valid_input_index, resample_kdtree = self._create_resample_kdtree()\n        # This is a numpy array\n        self.valid_input_index = valid_input_index\n\n        if resample_kdtree.n == 0:\n            # Handle if all input data is reduced away\n            bilinear_t, bilinear_s, valid_input_index, index_array = \\\n                _create_empty_bil_info(self.source_geo_def,\n                                       self.target_geo_def)\n            self.bilinear_t = bilinear_t\n            self.bilinear_s = bilinear_s\n            self.valid_input_index = valid_input_index\n            self.index_array = index_array\n\n            return bilinear_t, bilinear_s, valid_input_index, index_array\n\n        target_lons, target_lats = self.target_geo_def.get_lonlats()\n        valid_output_idx = ((target_lons >= -180) & (target_lons <= 180) &\n                            (target_lats <= 90) & (target_lats >= -90))\n\n        index_array, distance_array = self._query_resample_kdtree(\n            resample_kdtree, target_lons, target_lats, valid_output_idx)\n\n        # Reduce index reference\n        input_size = da.sum(self.valid_input_index)\n        index_mask = index_array == input_size\n        index_array = da.where(index_mask, 0, index_array)\n\n        # Get output projection as pyproj object\n        proj = Proj(self.target_geo_def.proj_str)\n\n        # Get output x/y coordinates\n        out_x, out_y = self.target_geo_def.get_proj_coords(chunks=CHUNK_SIZE)\n        out_x = da.ravel(out_x)\n        out_y = da.ravel(out_y)\n\n        # Get input x/y coordinates\n        in_x, in_y = _get_input_xy_dask(self.source_geo_def, proj,\n                                        self.valid_input_index, index_array)\n\n        # Get the four closest corner points around each output location\n        pt_1, pt_2, pt_3, pt_4, index_array = \\\n            _get_bounding_corners_dask(in_x, in_y, out_x, out_y,\n                                       self.neighbours, index_array)\n\n        # Calculate vertical and horizontal fractional distances t and s\n        t__, s__ = _get_ts_dask(pt_1, pt_2, pt_3, pt_4, out_x, out_y)\n        self.bilinear_t, self.bilinear_s = t__, s__\n\n        self.valid_output_index = valid_output_idx\n        self.index_array = index_array\n        self.distance_array = distance_array\n\n        self._get_slices()\n\n        return (self.bilinear_t, self.bilinear_s,\n                self.slices, self.mask_slices,\n                self.out_coords)\n\n    def get_sample_from_bil_info(self, data, fill_value=None,\n                                 output_shape=None):\n        """"""Resample using pre-computed resampling LUTs.""""""\n        del output_shape\n        if fill_value is None:\n            if np.issubdtype(data.dtype, np.integer):\n                fill_value = 0\n            else:\n                fill_value = np.nan\n\n        p_1, p_2, p_3, p_4 = self._slice_data(data, fill_value)\n        s__, t__ = self.bilinear_s, self.bilinear_t\n\n        res = (p_1 * (1 - s__) * (1 - t__) +\n               p_2 * s__ * (1 - t__) +\n               p_3 * (1 - s__) * t__ +\n               p_4 * s__ * t__)\n\n        epsilon = 1e-6\n        data_min = da.nanmin(data) - epsilon\n        data_max = da.nanmax(data) + epsilon\n\n        idxs = (res > data_max) | (res < data_min)\n        res = da.where(idxs, fill_value, res)\n        res = da.where(np.isnan(res), fill_value, res)\n        shp = self.target_geo_def.shape\n        if data.ndim == 3:\n            res = da.reshape(res, (res.shape[0], shp[0], shp[1]))\n        else:\n            res = da.reshape(res, (shp[0], shp[1]))\n\n        # Add missing coordinates\n        self._add_missing_coordinates(data)\n\n        res = DataArray(res, dims=data.dims, coords=self.out_coords)\n\n        return res\n\n    def _compute_indices(self):\n        for idx in CACHE_INDICES:\n            var = getattr(self, idx)\n            try:\n                var = var.compute()\n                setattr(self, idx, var)\n            except AttributeError:\n                continue\n\n    def _add_missing_coordinates(self, data):\n        if self.out_coords[\'x\'] is None and self.out_coords_x is not None:\n            self.out_coords[\'x\'] = self.out_coords_x\n            self.out_coords[\'y\'] = self.out_coords_y\n        for _, dim in enumerate(data.dims):\n            if dim not in self.out_coords:\n                try:\n                    self.out_coords[dim] = data.coords[dim]\n                except KeyError:\n                    pass\n\n    def _slice_data(self, data, fill_value):\n\n        def _slicer(values, sl_x, sl_y, mask, fill_value):\n            if values.ndim == 2:\n                arr = values[(sl_y, sl_x)]\n                arr[(mask, )] = fill_value\n                p_1 = arr[:, 0]\n                p_2 = arr[:, 1]\n                p_3 = arr[:, 2]\n                p_4 = arr[:, 3]\n            elif values.ndim == 3:\n                arr = values[(slice(None), sl_y, sl_x)]\n                arr[(slice(None), mask)] = fill_value\n                p_1 = arr[:, :, 0]\n                p_2 = arr[:, :, 1]\n                p_3 = arr[:, :, 2]\n                p_4 = arr[:, :, 3]\n            else:\n                raise ValueError\n\n            return p_1, p_2, p_3, p_4\n\n        values = data.values\n        sl_y = self.slices_y\n        sl_x = self.slices_x\n        mask = self.mask_slices\n\n        return _slicer(values, sl_x, sl_y, mask, fill_value)\n\n    def _get_slices(self):\n        shp = self.source_geo_def.shape\n        cols, lines = np.meshgrid(np.arange(shp[1]),\n                                  np.arange(shp[0]))\n        cols = np.ravel(cols)\n        lines = np.ravel(lines)\n\n        vii = self.valid_input_index\n        ia_ = self.index_array\n\n        # ia_ contains reduced (valid) indices of the source array, and has the\n        # shape of the destination array\n        rlines = lines[vii][ia_]\n        rcols = cols[vii][ia_]\n\n        try:\n            coord_x, coord_y = self.target_geo_def.get_proj_vectors()\n            self.out_coords[\'y\'] = coord_y\n            self.out_coords[\'x\'] = coord_x\n            self.out_coords_y = self.out_coords[\'y\']\n            self.out_coords_x = self.out_coords[\'x\']\n        except AttributeError:\n            pass\n\n        self.mask_slices = ia_ >= self.source_geo_def.size\n        self.slices[\'y\'] = rlines\n        self.slices[\'x\'] = rcols\n        self.slices_y = self.slices[\'y\']\n        self.slices_x = self.slices[\'x\']\n\n    def _create_resample_kdtree(self):\n        """"""Set up kd tree on input.""""""\n        # Get input information\n        valid_input_index, source_lons, source_lats = \\\n            _get_valid_input_index_dask(self.source_geo_def,\n                                        self.target_geo_def,\n                                        self.reduce_data,\n                                        self.radius_of_influence)\n\n        # FIXME: Is dask smart enough to only compute the pixels we end up\n        #        using even with this complicated indexing\n        input_coords = lonlat2xyz(source_lons, source_lats)\n        valid_input_index = da.ravel(valid_input_index)\n        input_coords = input_coords[valid_input_index, :]\n        input_coords = input_coords.compute()\n        # Build kd-tree on input\n        input_coords = input_coords.astype(np.float)\n        valid_input_index, input_coords = da.compute(valid_input_index,\n                                                     input_coords)\n        return valid_input_index, KDTree(input_coords)\n\n    def _query_resample_kdtree(self,\n                               resample_kdtree,\n                               tlons,\n                               tlats,\n                               valid_oi,\n                               reduce_data=True):\n        """"""Query kd-tree on slice of target coordinates.""""""\n        res = query_no_distance(tlons, tlats,\n                                valid_oi, resample_kdtree,\n                                self.neighbours, self.epsilon,\n                                self.radius_of_influence)\n        return res, None\n\n\ndef _get_input_xy_dask(source_geo_def, proj, valid_input_index, index_array):\n    """"""Get x/y coordinates for the input area and reduce the data.""""""\n    in_lons, in_lats = source_geo_def.get_lonlats(chunks=CHUNK_SIZE)\n\n    # Mask invalid values\n    in_lons, in_lats = _mask_coordinates_dask(in_lons, in_lats)\n\n    # Select valid locations\n    # TODO: direct indexing w/o .compute() results in\n    # ""ValueError: object too deep for desired array\n\n    in_lons = da.ravel(in_lons)\n    in_lons = in_lons.compute()\n    in_lons = in_lons[valid_input_index]\n    in_lats = da.ravel(in_lats)\n    in_lats = in_lats.compute()\n    in_lats = in_lats[valid_input_index]\n    index_array = index_array.compute()\n\n    # Expand input coordinates for each output location\n    in_lons = in_lons[index_array]\n    in_lats = in_lats[index_array]\n\n    # Convert coordinates to output projection x/y space\n    in_x, in_y = proj(in_lons, in_lats)\n\n    return in_x, in_y\n\n\ndef _mask_coordinates_dask(lons, lats):\n    """"""Mask invalid coordinate values.""""""\n    idxs = ((lons < -180.) | (lons > 180.) |\n            (lats < -90.) | (lats > 90.))\n    lons = da.where(idxs, np.nan, lons)\n    lats = da.where(idxs, np.nan, lats)\n\n    return lons, lats\n\n\ndef _get_bounding_corners_dask(in_x, in_y, out_x, out_y, neighbours, index_array):\n    """"""Get bounding corners.\n\n    Get four closest locations from (in_x, in_y) so that they form a\n    bounding rectangle around the requested location given by (out_x,\n    out_y).\n\n    """"""\n    # Find four closest pixels around the target location\n\n    # FIXME: how to daskify?\n    # Tile output coordinates to same shape as neighbour info\n    # Replacing with da.transpose and da.tile doesn\'t work\n    out_x_tile = np.reshape(np.tile(out_x, neighbours),\n                            (neighbours, out_x.size)).T\n    out_y_tile = np.reshape(np.tile(out_y, neighbours),\n                            (neighbours, out_y.size)).T\n\n    # Get differences in both directions\n    x_diff = out_x_tile - in_x\n    y_diff = out_y_tile - in_y\n\n    stride = np.arange(x_diff.shape[0])\n\n    # Upper left source pixel\n    valid = (x_diff > 0) & (y_diff < 0)\n    x_1, y_1, idx_1 = _get_corner_dask(stride, valid, in_x, in_y, index_array)\n\n    # Upper right source pixel\n    valid = (x_diff < 0) & (y_diff < 0)\n    x_2, y_2, idx_2 = _get_corner_dask(stride, valid, in_x, in_y, index_array)\n\n    # Lower left source pixel\n    valid = (x_diff > 0) & (y_diff > 0)\n    x_3, y_3, idx_3 = _get_corner_dask(stride, valid, in_x, in_y, index_array)\n\n    # Lower right source pixel\n    valid = (x_diff < 0) & (y_diff > 0)\n    x_4, y_4, idx_4 = _get_corner_dask(stride, valid, in_x, in_y, index_array)\n\n    # Combine sorted indices to index_array\n    index_array = np.transpose(np.vstack((idx_1, idx_2, idx_3, idx_4)))\n\n    return (np.transpose(np.vstack((x_1, y_1))),\n            np.transpose(np.vstack((x_2, y_2))),\n            np.transpose(np.vstack((x_3, y_3))),\n            np.transpose(np.vstack((x_4, y_4))),\n            index_array)\n\n\ndef _get_corner_dask(stride, valid, in_x, in_y, index_array):\n    """"""Get closest set of coordinates from the *valid* locations.""""""\n    # Find the closest valid pixels, if any\n    idxs = np.argmax(valid, axis=1)\n    # Check which of these were actually valid\n    invalid = np.invert(np.max(valid, axis=1))\n\n    # idxs = idxs.compute()\n    index_array = index_array.compute()\n\n    # Replace invalid points with np.nan\n    x__ = in_x[stride, idxs]  # TODO: daskify\n    x__ = da.where(invalid, np.nan, x__)\n    y__ = in_y[stride, idxs]  # TODO: daskify\n    y__ = da.where(invalid, np.nan, y__)\n\n    idx = index_array[stride, idxs]  # TODO: daskify\n\n    return x__, y__, idx\n\n\ndef _get_ts_dask(pt_1, pt_2, pt_3, pt_4, out_x, out_y):\n    """"""Calculate vertical and horizontal fractional distances t and s.""""""\n    def invalid_to_nan(t__, s__):\n        idxs = (t__ < 0) | (t__ > 1) | (s__ < 0) | (s__ > 1)\n        t__ = da.where(idxs, np.nan, t__)\n        s__ = da.where(idxs, np.nan, s__)\n        return t__, s__\n\n    # General case, ie. where the the corners form an irregular rectangle\n    t__, s__ = _get_ts_irregular_dask(pt_1, pt_2, pt_3, pt_4, out_y, out_x)\n\n    # Replace invalid values with NaNs\n    t__, s__ = invalid_to_nan(t__, s__)\n\n    # Cases where verticals are parallel\n    idxs = da.isnan(t__) | da.isnan(s__)\n    # Remove extra dimensions\n    idxs = da.ravel(idxs)\n\n    if da.any(idxs):\n        t_new, s_new = _get_ts_uprights_parallel_dask(pt_1, pt_2,\n                                                      pt_3, pt_4,\n                                                      out_y, out_x)\n        t__ = da.where(idxs, t_new, t__)\n        s__ = da.where(idxs, s_new, s__)\n\n    # Replace invalid values with NaNs\n    t__, s__ = invalid_to_nan(t__, s__)\n\n    # Cases where both verticals and horizontals are parallel\n    idxs = da.isnan(t__) | da.isnan(s__)\n    # Remove extra dimensions\n    idxs = da.ravel(idxs)\n    if da.any(idxs):\n        t_new, s_new = _get_ts_parallellogram_dask(pt_1, pt_2, pt_3,\n                                                   out_y, out_x)\n        t__ = da.where(idxs, t_new, t__)\n        s__ = da.where(idxs, s_new, s__)\n\n    # Replace invalid values with NaNs\n    t__, s__ = invalid_to_nan(t__, s__)\n\n    return t__, s__\n\n\ndef _get_ts_irregular_dask(pt_1, pt_2, pt_3, pt_4, out_y, out_x):\n    """"""Get parameters for the case where none of the sides are parallel.""""""\n    # Get parameters for the quadratic equation\n    # TODO: check if needs daskifying\n    a__, b__, c__ = _calc_abc_dask(pt_1, pt_2, pt_3, pt_4, out_y, out_x)\n\n    # Get the valid roots from interval [0, 1]\n    t__ = _solve_quadratic_dask(a__, b__, c__, min_val=0., max_val=1.)\n\n    # Calculate parameter s\n    s__ = _solve_another_fractional_distance_dask(t__, pt_1[:, 1], pt_3[:, 1],\n                                                  pt_2[:, 1], pt_4[:, 1], out_y)\n\n    return t__, s__\n\n\n# Might not need daskifying\ndef _calc_abc_dask(pt_1, pt_2, pt_3, pt_4, out_y, out_x):\n    """"""Calculate coefficients for quadratic equation.\n\n    In this order of arguments used for _get_ts_irregular() and\n    _get_ts_uprights().  For _get_ts_uprights switch order of pt_2 and\n    pt_3.\n\n    """"""\n    # Pairwise longitudal separations between reference points\n    x_21 = pt_2[:, 0] - pt_1[:, 0]\n    x_31 = pt_3[:, 0] - pt_1[:, 0]\n    x_42 = pt_4[:, 0] - pt_2[:, 0]\n\n    # Pairwise latitudal separations between reference points\n    y_21 = pt_2[:, 1] - pt_1[:, 1]\n    y_31 = pt_3[:, 1] - pt_1[:, 1]\n    y_42 = pt_4[:, 1] - pt_2[:, 1]\n\n    a__ = x_31 * y_42 - y_31 * x_42\n    b__ = out_y * (x_42 - x_31) - out_x * (y_42 - y_31) + \\\n        x_31 * pt_2[:, 1] - y_31 * pt_2[:, 0] + \\\n        y_42 * pt_1[:, 0] - x_42 * pt_1[:, 1]\n    c__ = out_y * x_21 - out_x * y_21 + pt_1[:, 0] * pt_2[:, 1] - \\\n        pt_2[:, 0] * pt_1[:, 1]\n\n    return a__, b__, c__\n\n\ndef _solve_quadratic_dask(a__, b__, c__, min_val=0.0, max_val=1.0):\n    """"""Solve quadratic equation.\n\n    Solve quadratic equation and return the valid roots from interval\n    [*min_val*, *max_val*].\n\n    """"""\n    discriminant = b__ * b__ - 4 * a__ * c__\n\n    # Solve the quadratic polynomial\n    x_1 = (-b__ + da.sqrt(discriminant)) / (2 * a__)\n    x_2 = (-b__ - da.sqrt(discriminant)) / (2 * a__)\n\n    # Find valid solutions, ie. 0 <= t <= 1\n    idxs = (x_1 < min_val) | (x_1 > max_val)\n    x__ = da.where(idxs, x_2, x_1)\n\n    idxs = (x__ < min_val) | (x__ > max_val)\n    x__ = da.where(idxs, np.nan, x__)\n\n    return x__\n\n\ndef _solve_another_fractional_distance_dask(f__, y_1, y_2, y_3, y_4, out_y):\n    """"""Solve parameter t__ from s__, or vice versa.\n\n    For solving s__, switch order of y_2 and y_3.\n    """"""\n    y_21 = y_2 - y_1\n    y_43 = y_4 - y_3\n\n    g__ = ((out_y - y_1 - y_21 * f__) /\n           (y_3 + y_43 * f__ - y_1 - y_21 * f__))\n\n    # Limit values to interval [0, 1]\n    idxs = (g__ < 0) | (g__ > 1)\n    g__ = da.where(idxs, np.nan, g__)\n\n    return g__\n\n\ndef _get_ts_uprights_parallel_dask(pt_1, pt_2, pt_3, pt_4, out_y, out_x):\n    """"""Get parameters for the case where uprights are parallel.""""""\n    # Get parameters for the quadratic equation\n    a__, b__, c__ = _calc_abc_dask(pt_1, pt_3, pt_2, pt_4, out_y, out_x)\n\n    # Get the valid roots from interval [0, 1]\n    s__ = _solve_quadratic_dask(a__, b__, c__, min_val=0., max_val=1.)\n\n    # Calculate parameter t\n    t__ = _solve_another_fractional_distance_dask(s__, pt_1[:, 1], pt_2[:, 1],\n                                                  pt_3[:, 1], pt_4[:, 1], out_y)\n\n    return t__, s__\n\n\ndef _get_ts_parallellogram_dask(pt_1, pt_2, pt_3, out_y, out_x):\n    """"""Get parameters for the case where uprights are parallel.""""""\n    # Pairwise longitudal separations between reference points\n    x_21 = pt_2[:, 0] - pt_1[:, 0]\n    x_31 = pt_3[:, 0] - pt_1[:, 0]\n\n    # Pairwise latitudal separations between reference points\n    y_21 = pt_2[:, 1] - pt_1[:, 1]\n    y_31 = pt_3[:, 1] - pt_1[:, 1]\n\n    t__ = (x_21 * (out_y - pt_1[:, 1]) - y_21 * (out_x - pt_1[:, 0])) / \\\n          (x_21 * y_31 - y_21 * x_31)\n    idxs = (t__ < 0.) | (t__ > 1.)\n    t__ = da.where(idxs, np.nan, t__)\n\n    s__ = (out_x - pt_1[:, 0] + x_31 * t__) / x_21\n    idxs = (s__ < 0.) | (s__ > 1.)\n    s__ = da.where(idxs, np.nan, s__)\n\n    return t__, s__\n\n\ndef query_no_distance(target_lons, target_lats,\n                      valid_output_index, kdtree, neighbours, epsilon, radius):\n    """"""Query the kdtree. No distances are returned.""""""\n    voi = valid_output_index\n    voir = da.ravel(voi)\n    target_lons_valid = da.ravel(target_lons)[voir]\n    target_lats_valid = da.ravel(target_lats)[voir]\n\n    coords = lonlat2xyz(target_lons_valid, target_lats_valid)\n    distance_array, index_array = kdtree.query(\n        coords.compute(),\n        k=neighbours,\n        eps=epsilon,\n        distance_upper_bound=radius)\n\n    return index_array\n\n\ndef _get_valid_input_index_dask(source_geo_def,\n                                target_geo_def,\n                                reduce_data,\n                                radius_of_influence):\n    """"""Find indices of reduced input data.""""""\n    source_lons, source_lats = source_geo_def.get_lonlats(chunks=CHUNK_SIZE)\n    source_lons = da.ravel(source_lons)\n    source_lats = da.ravel(source_lats)\n\n    if source_lons.size == 0 or source_lats.size == 0:\n        raise ValueError(\'Cannot resample empty data set\')\n    elif source_lons.size != source_lats.size or \\\n            source_lons.shape != source_lats.shape:\n        raise ValueError(\'Mismatch between lons and lats\')\n\n    # Remove illegal values\n    valid_input_index = ((source_lons >= -180) & (source_lons <= 180) &\n                         (source_lats <= 90) & (source_lats >= -90))\n\n    if reduce_data:\n        # Reduce dataset\n        if (isinstance(source_geo_def, geometry.CoordinateDefinition) and\n            isinstance(target_geo_def, (geometry.GridDefinition,\n                                        geometry.AreaDefinition))) or \\\n           (isinstance(source_geo_def, (geometry.GridDefinition,\n                                        geometry.AreaDefinition)) and\n            isinstance(target_geo_def, (geometry.GridDefinition,\n                                        geometry.AreaDefinition))):\n            # Resampling from swath to grid or from grid to grid\n            lonlat_boundary = target_geo_def.get_boundary_lonlats()\n\n            # Combine reduced and legal values\n            valid_input_index &= \\\n                data_reduce.get_valid_index_from_lonlat_boundaries(\n                    lonlat_boundary[0],\n                    lonlat_boundary[1],\n                    source_lons, source_lats,\n                    radius_of_influence)\n\n    if (isinstance(valid_input_index, np.ma.core.MaskedArray)):\n        # Make sure valid_input_index is not a masked array\n        valid_input_index = valid_input_index.filled(False)\n\n    return valid_input_index, source_lons, source_lats\n\n\ndef lonlat2xyz(lons, lats):\n    """"""Convert geographic coordinates to cartesian 3D coordinates.""""""\n    R = 6370997.0\n    x_coords = R * da.cos(da.deg2rad(lats)) * da.cos(da.deg2rad(lons))\n    y_coords = R * da.cos(da.deg2rad(lats)) * da.sin(da.deg2rad(lons))\n    z_coords = R * da.sin(da.deg2rad(lats))\n\n    return da.stack(\n        (x_coords.ravel(), y_coords.ravel(), z_coords.ravel()), axis=-1)\n\n\ndef _create_empty_bil_info(source_geo_def, target_geo_def):\n    """"""Create dummy info for empty result set.""""""\n    valid_input_index = np.ones(source_geo_def.size, dtype=np.bool)\n    index_array = np.ones((target_geo_def.size, 4), dtype=np.int32)\n    bilinear_s = np.nan * np.zeros(target_geo_def.size)\n    bilinear_t = np.nan * np.zeros(target_geo_def.size)\n\n    return bilinear_t, bilinear_s, valid_input_index, index_array\n'"
pyresample/bucket/__init__.py,16,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2019  Pyresample developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Code for resampling using bucket resampling.""""""\n\nimport dask.array as da\nimport xarray as xr\nimport numpy as np\nimport logging\nfrom pyresample._spatial_mp import Proj\n\nLOG = logging.getLogger(__name__)\n\n\nclass BucketResampler(object):\n\n    """"""Class for bucket resampling.\n\n    Bucket resampling is useful for calculating averages and hit-counts\n    when aggregating data to coarser scale grids.\n\n    Below are examples how to use the resampler.\n\n    Read data using Satpy.  The resampling can also be done (apart from\n    fractions) directly from Satpy, but this demonstrates the direct\n    low-level usage.\n\n    >>> from pyresample.bucket import BucketResampler\n    >>> from satpy import Scene\n    >>> from satpy.resample import get_area_def\n    >>> fname = ""hrpt_noaa19_20170519_1214_42635.l1b""\n    >>> glbl = Scene(filenames=[fname])\n    >>> glbl.load([\'4\'])\n    >>> data = glbl[\'4\']\n    >>> lons, lats = data.area.get_lonlats()\n    >>> target_area = get_area_def(\'euro4\')\n\n    Initialize the resampler\n\n    >>> resampler = BucketResampler(adef, lons, lats)\n\n    Calculate the sum of all the data in each grid location:\n\n    >>> sums = resampler.get_sum(data)\n\n    Calculate how many values were collected at each grid location:\n\n    >>> counts = resampler.get_count()\n\n    The average can be calculated from the above two results, or directly\n    using the helper method:\n\n    >>> average = resampler.get_average(data)\n\n    Calculate fractions of occurrences of different values in each grid\n    location.  The data needs to be categorical (in integers), so\n    we\'ll create some categorical data from the brightness temperature\n    data that were read earlier.  The data are returned in a\n    dictionary with the categories as keys.\n\n    >>> data = da.where(data > 250, 1, 0)\n    >>> fractions = resampler.get_fractions(data, categories=[0, 1])\n    >>> import matplotlib.pyplot as plt\n    >>> plt.imshow(fractions[0]); plt.show()\n    """"""\n\n    def __init__(self, target_area, source_lons, source_lats):\n\n        self.target_area = target_area\n        self.source_lons = source_lons\n        self.source_lats = source_lats\n        self.prj = Proj(self.target_area.proj_dict)\n        self.x_idxs = None\n        self.y_idxs = None\n        self.idxs = None\n        self._get_indices()\n        self.counts = None\n\n    def _get_proj_coordinates(self, lons, lats):\n        """"""Calculate projection coordinates.\n\n        Parameters\n        ----------\n        lons : Numpy or Dask array\n            Longitude coordinates\n        lats : Numpy or Dask array\n            Latitude coordinates\n        """"""\n        proj_x, proj_y = self.prj(lons, lats)\n        return np.stack((proj_x, proj_y))\n\n    def _get_indices(self):\n        """"""Calculate projection indices.\n\n        Returns\n        -------\n        x_idxs : Dask array\n            X indices of the target grid where the data are put\n        y_idxs : Dask array\n            Y indices of the target grid where the data are put\n        """"""\n        LOG.info(""Determine bucket resampling indices"")\n\n        # Transform source lons/lats to target projection coordinates x/y\n        lons = self.source_lons.ravel()\n        lats = self.source_lats.ravel()\n        result = da.map_blocks(self._get_proj_coordinates, lons, lats,\n                               new_axis=0, chunks=(2,) + lons.chunks)\n        proj_x = result[0, :]\n        proj_y = result[1, :]\n\n        # Calculate array indices. Orient so that 0-meridian is pointing down.\n        adef = self.target_area\n        x_res, y_res = adef.resolution\n        x_idxs = da.floor((proj_x - adef.area_extent[0]) / x_res).astype(np.int)\n        y_idxs = da.floor((adef.area_extent[3] - proj_y) / y_res).astype(np.int)\n\n        # Get valid index locations\n        mask = ((x_idxs >= 0) & (x_idxs < adef.width) &\n                (y_idxs >= 0) & (y_idxs < adef.height))\n        self.y_idxs = da.where(mask, y_idxs, -1)\n        self.x_idxs = da.where(mask, x_idxs, -1)\n\n        # Convert X- and Y-indices to raveled indexing\n        target_shape = self.target_area.shape\n        self.idxs = self.y_idxs * target_shape[1] + self.x_idxs\n\n    def get_sum(self, data, mask_all_nan=False):\n        """"""Calculate sums for each bin with drop-in-a-bucket resampling.\n\n        Parameters\n        ----------\n        data : Numpy or Dask array\n        mask_all_nan : boolean (optional)\n            Mask bins that have only NaN results, default: False\n\n        Returns\n        -------\n        data : Numpy or Dask array\n            Bin-wise sums in the target grid\n        """"""\n        LOG.info(""Get sum of values in each location"")\n        if isinstance(data, xr.DataArray):\n            data = data.data\n        data = data.ravel()\n        # Remove NaN values from the data when used as weights\n        weights = da.where(np.isnan(data), 0, data)\n\n        # Rechunk indices to match the data chunking\n        if weights.chunks != self.idxs.chunks:\n            self.idxs = da.rechunk(self.idxs, weights.chunks)\n\n        # Calculate the sum of the data falling to each bin\n        out_size = self.target_area.size\n        sums, _ = da.histogram(self.idxs, bins=out_size, range=(0, out_size),\n                               weights=weights, density=False)\n\n        if mask_all_nan:\n            nans = np.isnan(data)\n            nan_sums, _ = da.histogram(self.idxs[nans], bins=out_size,\n                                       range=(0, out_size))\n            counts = self.get_count().ravel()\n            sums = da.where(nan_sums == counts, np.nan, sums)\n\n        return sums.reshape(self.target_area.shape)\n\n    def get_count(self):\n        """"""Count the number of occurrences for each bin using drop-in-a-bucket\n        resampling.\n\n        Returns\n        -------\n        data : Dask array\n            Bin-wise count of hits for each target grid location\n        """"""\n        LOG.info(""Get number of values in each location"")\n\n        out_size = self.target_area.size\n\n        # Calculate the sum of the data falling to each bin\n        if self.counts is None:\n            counts, _ = da.histogram(self.idxs, bins=out_size,\n                                     range=(0, out_size))\n            self.counts = counts.reshape(self.target_area.shape)\n\n        return self.counts\n\n    def get_average(self, data, fill_value=np.nan, mask_all_nan=False):\n        """"""Calculate bin-averages using bucket resampling.\n\n        Parameters\n        ----------\n        data : Numpy or Dask array\n            Data to be binned and averaged\n        fill_value : float\n            Fill value to replace missing values.  Default: np.nan\n\n        Returns\n        -------\n        average : Dask array\n            Binned and averaged data.\n        """"""\n        LOG.info(""Get average value for each location"")\n\n        sums = self.get_sum(data, mask_all_nan=mask_all_nan)\n        counts = self.get_sum(np.logical_not(np.isnan(data)).astype(int),\n                              mask_all_nan=False)\n\n        average = sums / da.where(counts == 0, np.nan, counts)\n        average = da.where(np.isnan(average), fill_value, average)\n\n        return average\n\n    def get_fractions(self, data, categories=None, fill_value=np.nan):\n        """"""Get fraction of occurrences for each given categorical value.\n\n        Parameters\n        ----------\n        data : Numpy or Dask array\n            Categorical data to be processed\n        categories : iterable or None\n            One dimensional list of categories in the data, or None.  If None,\n            categories are determined from the data by fully processing the\n            data and finding the unique category values.\n        fill_value : float\n            Fill value to replace missing values.  Default: np.nan\n        """"""\n        if categories is None:\n            LOG.warning(""No categories given, need to compute the data."")\n            # compute any dask arrays by converting to numpy\n            categories = np.asarray(np.unique(data))\n        try:\n            num = categories.size\n        except AttributeError:\n            num = len(categories)\n        LOG.info(""Get fractions for %d categories"", num)\n        results = {}\n        counts = self.get_count()\n        counts = counts.astype(float)\n        # Disable logging for calls to get_sum()\n        LOG.disabled = True\n        for cat in categories:\n            cat_data = da.where(data == cat, 1.0, 0.0)\n\n            sums = self.get_sum(cat_data)\n            result = sums.astype(float) / counts\n            result = da.where(counts == 0.0, fill_value, result)\n            results[cat] = result\n        # Re-enable logging\n        LOG.disabled = False\n\n        return results\n\n\ndef round_to_resolution(arr, resolution):\n    """"""Round the values in *arr* to closest resolution element.\n\n    Parameters\n    ----------\n    arr : list, tuple, Numpy or Dask array\n        Array to be rounded\n    resolution : float\n        Resolution unit to which data are rounded\n\n    Returns\n    -------\n    data : Numpy or Dask array\n        Source data rounded to the closest resolution unit\n    """"""\n    if isinstance(arr, (list, tuple)):\n        arr = np.array(arr)\n    return resolution * np.round(arr / resolution)\n'"
pyresample/ewa/__init__.py,17,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2016\n\n# Author(s):\n\n#   David Hoese <david.hoese@ssec.wisc.edu>\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Code for resampling using the Elliptical Weighted Averaging (EWA) algorithm.\n\nThe logic and original code for this algorithm were translated from the\nsoftware package ""MODIS Swath 2 Grid Toolbox"" or ""ms2gt"" created by the\nNASA National Snow & Ice Data Center (NSIDC):\n\n    https://nsidc.org/data/modis/ms2gt/index.html\n\nSince the project has slowed down, Terry Haran has maintained the package\nand made updates available:\n\n    http://cires1.colorado.edu/~tharan/ms2gt/\n\nThe ms2gt C executables ""ll2cr"" and ""fornav"" were rewritten for the\nPolar2Grid software package created by the Space Science Engineering Center\n(SSEC)/Cooperative Institute for Meteorological Satellite Studies. They were\nrewritten as a combination of C++ and Cython to make them more python friendly\nby David Hoese and were then copied and modified here in pyresample. The\nrewrite of ""ll2cr"" also included an important switch from using the ""mapx""\nlibrary to using the more popular and capable pyproj (PROJ.4) library.\n\nThe EWA algorithm consists of two parts ""ll2cr"" and ""fornav"" and are described\nbelow.\n\nll2cr\n-----\n\nThe ""ll2cr"" process is the first step in the EWA algorithm. It stands for\n""latitude/longitude to column/row"". Its main purpose is to convert\ninput longitude and latitude coordinates to column and row coordinates\nof the destination grid. These coordinates are then used in the next step\n""fornav"".\n\nfornav\n------\n\nThe ""fornav"" or ""Forward Navigation"" step of the EWA algorithm is where\nthe actual Elliptical Weighted Averaging algorithm is run. The algorithm\nmaps input swath pixels to output grid pixels by averaging multiple input\npixels based on an elliptical region and other coefficients, some of which\nare determined at run time.\n\nFor more information on these steps see the documentation for the\ncorresponding modules.\n\n""""""\n\nimport logging\nimport numpy as np\nfrom pyresample.ewa import _ll2cr, _fornav\n\nLOG = logging.getLogger(__name__)\n\n\ndef ll2cr(swath_def, area_def, fill=np.nan, copy=True):\n    """"""Map input swath pixels to output grid column and rows.\n\n    Parameters\n    ----------\n\n    swath_def : SwathDefinition\n        Navigation definition for swath data to remap\n    area_def : AreaDefinition\n        Grid definition to be mapped to\n    fill : float, optional\n        Fill value used in longitude and latitude arrays\n    copy : bool, optional\n        Create a copy of the longitude and latitude arrays (default: True)\n\n    Returns\n    -------\n\n    (swath_points_in_grid, cols, rows) : tuple of integer, numpy array, numpy array\n        Number of points from the input swath overlapping the destination\n        area and the column and row arrays to pass to `fornav`.\n\n\n    .. note::\n\n        ll2cr uses the pyproj library which is limited to 64-bit float\n        navigation arrays in order to not do additional copying or casting\n        of data types.\n\n    """"""\n    lons, lats = swath_def.get_lonlats()\n    # ll2cr requires 64-bit floats due to pyproj limitations\n    # also need a copy of lons, lats since they are written to in-place\n    try:\n        lons = lons.astype(np.float64, copy=copy)\n        lats = lats.astype(np.float64, copy=copy)\n    except TypeError:\n        lons = lons.astype(np.float64)\n        lats = lats.astype(np.float64)\n\n    # Break the input area up in to the expected parameters for ll2cr\n    p = area_def.proj_str\n    cw = area_def.pixel_size_x\n    # cell height must be negative for this to work as expected\n    ch = -abs(area_def.pixel_size_y)\n    w = area_def.width\n    h = area_def.height\n    ox = area_def.area_extent[0] + cw / 2.\n    oy = area_def.area_extent[3] + ch / 2.\n    swath_points_in_grid = _ll2cr.ll2cr_static(lons, lats, fill,\n                                               p, cw, ch, w, h, ox, oy)\n    return swath_points_in_grid, lons, lats\n\n\ndef fornav(cols, rows, area_def, data_in,\n           rows_per_scan=None, fill=None, out=None,\n           weight_count=10000, weight_min=0.01, weight_distance_max=1.0,\n           weight_delta_max=10.0, weight_sum_min=-1.0,\n           maximum_weight_mode=False):\n    """"""Remap data in to output grid using elliptical weighted averaging.\n\n    This algorithm works under the assumption that the data is observed\n    one scan line at a time. However, good results can still be achieved\n    for non-scan based data is provided if `rows_per_scan` is set to the\n    number of rows in the entire swath or by setting it to `None`.\n\n    Parameters\n    ----------\n\n    cols : numpy array\n        Column location for each input swath pixel (from `ll2cr`)\n    rows : numpy array\n        Row location for each input swath pixel (from `ll2cr`)\n    area_def : AreaDefinition\n        Grid definition to be mapped to\n    data_in : numpy array or tuple of numpy arrays\n        Swath data to be remapped to output grid\n    rows_per_scan : int or None, optional\n        Number of data rows for every observed scanline. If None then the\n        entire swath is treated as one large scanline.\n    fill : float/int or None, optional\n        If `data_in` is made of numpy arrays then this represents the fill\n        value used to mark invalid data pixels. This value will also be\n        used in the output array(s). If None, then np.nan will be used\n        for float arrays and -999 will be used for integer arrays.\n    out : numpy array or tuple of numpy arrays, optional\n        Specify a numpy array to be written to for each input array. This can\n        be used as an optimization by providing `np.memmap` arrays or other\n        array-like objects.\n    weight_count : int, optional\n        number of elements to create in the gaussian weight table.\n        Default is 10000. Must be at least 2\n    weight_min : float, optional\n        the minimum value to store in the last position of the\n        weight table. Default is 0.01, which, with a\n        `weight_distance_max` of 1.0 produces a weight of 0.01\n        at a grid cell distance of 1.0. Must be greater than 0.\n    weight_distance_max : float, optional\n        distance in grid cell units at which to\n        apply a weight of `weight_min`. Default is\n        1.0. Must be greater than 0.\n    weight_delta_max : float, optional\n        maximum distance in grid cells in each grid\n        dimension over which to distribute a single swath cell.\n        Default is 10.0.\n    weight_sum_min : float, optional\n        minimum weight sum value. Cells whose weight sums\n        are less than `weight_sum_min` are set to the grid fill value.\n        Default is EPSILON.\n    maximum_weight_mode : bool, optional\n        If False (default), a weighted average of\n        all swath cells that map to a particular grid cell is used.\n        If True, the swath cell having the maximum weight of all\n        swath cells that map to a particular grid cell is used. This\n        option should be used for coded/category data, i.e. snow cover.\n\n    Returns\n    -------\n\n    (valid grid points, output arrays): tuple of integer tuples and numpy array tuples\n        The valid_grid_points tuple holds the number of output grid pixels that\n        were written with valid data. The second element in the tuple is a tuple of\n        output grid numpy arrays for each input array. If there was only one input\n        array provided then the returned tuple is simply the singe points integer\n        and single output grid array.\n    """"""\n    if isinstance(data_in, (tuple, list)):\n        # we can only support one data type per call at this time\n        assert(in_arr.dtype == data_in[0].dtype for in_arr in data_in[1:])\n    else:\n        # assume they gave us a single numpy array-like object\n        data_in = [data_in]\n\n    # need a list for replacing these arrays later\n    data_in = [np.ascontiguousarray(d) for d in data_in]\n    # determine a fill value if they didn\'t tell us what they have as a\n    # fill value in the numpy arrays\n    if ""fill"" is None:\n        if np.issubdtype(data_in[0].dtype, np.floating):\n            fill = np.nan\n        elif np.issubdtype(data_in[0].dtype, np.integer):\n            fill = -999\n        else:\n            raise ValueError(\n                ""Unsupported input data type for EWA Resampling: {}"".format(data_in[0].dtype))\n\n    convert_to_masked = False\n    for idx, in_arr in enumerate(data_in):\n        if isinstance(in_arr, np.ma.MaskedArray):\n            convert_to_masked = True\n            # convert masked arrays to single numpy arrays\n            data_in[idx] = in_arr.filled(fill)\n    data_in = tuple(data_in)\n\n    if out is not None:\n        # the user may have provided memmapped arrays or other array-like\n        # objects\n        if isinstance(out, (tuple, list)):\n            out = tuple(out)\n        else:\n            out = (out,)\n    else:\n        # create a place for output data to be written\n        out = tuple(np.empty(area_def.shape, dtype=in_arr.dtype)\n                    for in_arr in data_in)\n\n    # see if the user specified rows per scan\n    # otherwise, use the entire swath as one ""scanline""\n    rows_per_scan = rows_per_scan or data_in[0].shape[0]\n\n    results = _fornav.fornav_wrapper(cols, rows, data_in, out,\n                                     np.nan, np.nan, rows_per_scan,\n                                     weight_count=weight_count,\n                                     weight_min=weight_min,\n                                     weight_distance_max=weight_distance_max,\n                                     weight_delta_max=weight_delta_max,\n                                     weight_sum_min=weight_sum_min,\n                                     maximum_weight_mode=maximum_weight_mode)\n\n    def _mask_helper(data, fill):\n        if np.isnan(fill):\n            return np.isnan(data)\n        else:\n            return data == fill\n\n    if convert_to_masked:\n        # they gave us masked arrays so give them masked arrays back\n        out = [np.ma.masked_where(_mask_helper(out_arr, fill), out_arr)\n               for out_arr in out]\n    if len(out) == 1:\n        # they only gave us one data array as input, so give them one back\n        out = out[0]\n        results = results[0]\n\n    return results, out\n'"
pyresample/gradient/__init__.py,14,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2013-2019\n#\n# Author(s):\n#\n#   Martin Raspaud <martin.raspaud@smhi.se>\n#   Panu Lahtinen <panu.lahtinen@fmi.fi>\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Implementation of the gradient search algorithm as described by Trishchenko.""""""\n\nimport logging\n\nimport dask.array as da\nimport dask\nimport numpy as np\nimport pyproj\nimport xarray as xr\nfrom shapely.geometry import Polygon\n\nfrom pyresample import CHUNK_SIZE\nfrom pyresample.gradient._gradient_search import one_step_gradient_search\nfrom pyresample.resampler import BaseResampler\nfrom pyresample.geometry import get_geostationary_bounding_box\n\nlogger = logging.getLogger(__name__)\n\n\n@da.as_gufunc(signature=\'(),()->(),()\')\ndef transform(x_coords, y_coords, src_prj=None, dst_prj=None):\n    """"""Calculate projection coordinates.""""""\n    return pyproj.transform(src_prj, dst_prj, x_coords, y_coords)\n\n\nclass GradientSearchResampler(BaseResampler):\n    """"""Resample using gradient search based bilinear interpolation.""""""\n\n    def __init__(self, source_geo_def, target_geo_def):\n        """"""Init GradientResampler.""""""\n        super(GradientSearchResampler, self).__init__(source_geo_def, target_geo_def)\n        import warnings\n        warnings.warn(""You are using the Gradient Search Resampler, which is still EXPERIMENTAL."")\n        self.use_input_coords = None\n        self._src_dst_filtered = False\n        self.prj = None\n        self.src_x = None\n        self.src_y = None\n        self.src_slices = None\n        self.dst_x = None\n        self.dst_y = None\n        self.dst_slices = None\n        self.src_gradient_xl = None\n        self.src_gradient_xp = None\n        self.src_gradient_yl = None\n        self.src_gradient_yp = None\n        self.dst_polys = {}\n        self.dst_mosaic_locations = None\n        self.coverage_status = None\n\n    def _get_projection_coordinates(self, datachunks):\n        """"""Get projection coordinates.""""""\n        if self.use_input_coords is None:\n            try:\n                self.src_x, self.src_y = self.source_geo_def.get_proj_coords(\n                    chunks=datachunks)\n                src_prj = pyproj.Proj(**self.source_geo_def.proj_dict)\n                self.use_input_coords = True\n            except AttributeError:\n                self.src_x, self.src_y = self.source_geo_def.get_lonlats(\n                    chunks=datachunks)\n                src_prj = pyproj.Proj(""+proj=longlat"")\n                self.use_input_coords = False\n            try:\n                self.dst_x, self.dst_y = self.target_geo_def.get_proj_coords(\n                    chunks=CHUNK_SIZE)\n                dst_prj = pyproj.Proj(**self.target_geo_def.proj_dict)\n            except AttributeError:\n                if self.use_input_coords is False:\n                    raise NotImplementedError(\'Cannot resample lon/lat to lon/lat with gradient search.\')\n                self.dst_x, self.dst_y = self.target_geo_def.get_lonlats(\n                    chunks=CHUNK_SIZE)\n                dst_prj = pyproj.Proj(""+proj=longlat"")\n            if self.use_input_coords:\n                self.dst_x, self.dst_y = transform(\n                    self.dst_x, self.dst_y,\n                    src_prj=dst_prj, dst_prj=src_prj)\n                self.prj = pyproj.Proj(**self.source_geo_def.proj_dict)\n            else:\n                self.src_x, self.src_y = transform(\n                    self.src_x, self.src_y,\n                    src_prj=src_prj, dst_prj=dst_prj)\n                self.prj = pyproj.Proj(**self.target_geo_def.proj_dict)\n\n    def _get_src_poly(self, src_y_start, src_y_end, src_x_start, src_x_end):\n        """"""Get bounding polygon for source chunk.""""""\n        geo_def = self.source_geo_def[src_y_start:src_y_end,\n                                      src_x_start:src_x_end]\n        try:\n            src_poly = get_polygon(self.prj, geo_def)\n        except AttributeError:\n            # Can\'t create polygons for SwathDefinition\n            src_poly = False\n\n        return src_poly\n\n    def _get_dst_poly(self, idx, dst_x_start, dst_x_end,\n                      dst_y_start, dst_y_end):\n        """"""Get target chunk polygon.""""""\n        dst_poly = self.dst_polys.get(idx, None)\n        if dst_poly is None:\n            geo_def = self.target_geo_def[dst_y_start:dst_y_end,\n                                          dst_x_start:dst_x_end]\n            try:\n                dst_poly = get_polygon(self.prj, geo_def)\n            except AttributeError:\n                # Can\'t create polygons for SwathDefinition\n                dst_poly = False\n            self.dst_polys[idx] = dst_poly\n\n        return dst_poly\n\n    def get_chunk_mappings(self):\n        """"""Map source and target chunks together if they overlap.""""""\n        src_y_chunks, src_x_chunks = self.src_x.chunks\n        dst_y_chunks, dst_x_chunks = self.dst_x.chunks\n\n        coverage_status = []\n        src_slices, dst_slices = [], []\n        dst_mosaic_locations = []\n\n        src_x_start = 0\n        for src_x_step in src_x_chunks:\n            src_x_end = src_x_start + src_x_step\n            src_y_start = 0\n            for src_y_step in src_y_chunks:\n                src_y_end = src_y_start + src_y_step\n                # Get source chunk polygon\n                src_poly = self._get_src_poly(src_y_start, src_y_end,\n                                              src_x_start, src_x_end)\n\n                dst_x_start = 0\n                for k, dst_x_step in enumerate(dst_x_chunks):\n                    dst_x_end = dst_x_start + dst_x_step\n                    dst_y_start = 0\n                    for l, dst_y_step in enumerate(dst_y_chunks):\n                        dst_y_end = dst_y_start + dst_y_step\n                        # Get destination chunk polygon\n                        dst_poly = self._get_dst_poly((k, l),\n                                                      dst_x_start, dst_x_end,\n                                                      dst_y_start, dst_y_end)\n\n                        covers = check_overlap(src_poly, dst_poly)\n\n                        coverage_status.append(covers)\n                        src_slices.append((src_y_start, src_y_end,\n                                           src_x_start, src_x_end))\n                        dst_slices.append((dst_y_start, dst_y_end,\n                                           dst_x_start, dst_x_end))\n                        dst_mosaic_locations.append((k, l))\n\n                        dst_y_start = dst_y_end\n                    dst_x_start = dst_x_end\n                src_y_start = src_y_end\n            src_x_start = src_x_end\n\n        self.src_slices = src_slices\n        self.dst_slices = dst_slices\n        self.dst_mosaic_locations = dst_mosaic_locations\n        self.coverage_status = coverage_status\n\n    def _filter_data(self, data, is_src=True, add_dim=False):\n        """"""Filter unused chunks from the given array.""""""\n        if add_dim:\n            if data.ndim not in [2, 3]:\n                raise NotImplementedError(\'Gradient search resampling only \'\n                                          \'supports 2D or 3D arrays.\')\n            if data.ndim == 2:\n                data = data[np.newaxis, :, :]\n\n        data_out = []\n        for i, covers in enumerate(self.coverage_status):\n            if covers:\n                if is_src:\n                    y_start, y_end, x_start, x_end = self.src_slices[i]\n                else:\n                    y_start, y_end, x_start, x_end = self.dst_slices[i]\n                try:\n                    val = data[:, y_start:y_end, x_start:x_end]\n                except IndexError:\n                    val = data[y_start:y_end, x_start:x_end]\n            else:\n                val = None\n            data_out.append(val)\n\n        return data_out\n\n    def _get_gradients(self):\n        """"""Get gradients in X and Y directions.""""""\n        self.src_gradient_xl, self.src_gradient_xp = np.gradient(\n            self.src_x, axis=[0, 1])\n        self.src_gradient_yl, self.src_gradient_yp = np.gradient(\n            self.src_y, axis=[0, 1])\n\n    def _filter_src_dst(self):\n        """"""Filter source and target chunks.""""""\n        self.src_x = self._filter_data(self.src_x)\n        self.src_y = self._filter_data(self.src_y)\n        self.src_gradient_yl = self._filter_data(self.src_gradient_yl)\n        self.src_gradient_yp = self._filter_data(self.src_gradient_yp)\n        self.src_gradient_xl = self._filter_data(self.src_gradient_xl)\n        self.src_gradient_xp = self._filter_data(self.src_gradient_xp)\n        self.dst_x = self._filter_data(self.dst_x, is_src=False)\n        self.dst_y = self._filter_data(self.dst_y, is_src=False)\n        self._src_dst_filtered = True\n\n    def compute(self, data, fill_value=None, **kwargs):\n        """"""Resample the given data using gradient search algorithm.""""""\n        if \'bands\' in data.dims:\n            datachunks = data.sel(bands=data.coords[\'bands\'][0]).chunks\n        else:\n            datachunks = data.chunks\n        data_dims = data.dims\n        data_coords = data.coords\n\n        self._get_projection_coordinates(datachunks)\n\n        if self.src_gradient_xl is None:\n            self._get_gradients()\n        if self.coverage_status is None:\n            self.get_chunk_mappings()\n        if not self._src_dst_filtered:\n            self._filter_src_dst()\n\n        data = self._filter_data(data.data, add_dim=True)\n\n        res = parallel_gradient_search(data,\n                                       self.src_x, self.src_y,\n                                       self.dst_x, self.dst_y,\n                                       self.src_gradient_xl,\n                                       self.src_gradient_xp,\n                                       self.src_gradient_yl,\n                                       self.src_gradient_yp,\n                                       self.dst_mosaic_locations,\n                                       self.dst_slices,\n                                       **kwargs)\n\n        # TODO: this will crash wen the target geo definition is a swath def.\n        x_coord, y_coord = self.target_geo_def.get_proj_vectors()\n        coords = []\n        for key in data_dims:\n            if key == \'x\':\n                coords.append(x_coord)\n            elif key == \'y\':\n                coords.append(y_coord)\n            else:\n                coords.append(data_coords[key])\n\n        if fill_value is not None:\n            res = da.where(np.isnan(res), fill_value, res)\n        res = xr.DataArray(res, dims=data_dims, coords=coords)\n\n        return res\n\n\ndef check_overlap(src_poly, dst_poly):\n    """"""Check if the two polygons overlap.""""""\n    if dst_poly is False or src_poly is False:\n        covers = True\n    elif dst_poly is not None and src_poly is not None:\n        covers = src_poly.intersects(dst_poly)\n    else:\n        covers = False\n\n    return covers\n\n\ndef _gradient_resample_data(src_data, src_x, src_y,\n                            src_gradient_xl, src_gradient_xp,\n                            src_gradient_yl, src_gradient_yp,\n                            dst_x, dst_y,\n                            method=\'bilinear\'):\n    """"""Resample using gradient search.""""""\n    assert src_data.ndim == 3\n    assert src_x.ndim == 2\n    assert src_y.ndim == 2\n    assert src_gradient_xl.ndim == 2\n    assert src_gradient_xp.ndim == 2\n    assert src_gradient_yl.ndim == 2\n    assert src_gradient_yp.ndim == 2\n    assert dst_x.ndim == 2\n    assert dst_y.ndim == 2\n    assert (src_data.shape[1:] == src_x.shape == src_y.shape ==\n            src_gradient_xl.shape == src_gradient_xp.shape ==\n            src_gradient_yl.shape == src_gradient_yp.shape)\n    assert dst_x.shape == dst_y.shape\n\n    image = one_step_gradient_search(src_data, src_x, src_y,\n                                     src_gradient_xl, src_gradient_xp,\n                                     src_gradient_yl, src_gradient_yp,\n                                     dst_x, dst_y,\n                                     method=method)\n\n    return image\n\n\ndef get_border_lonlats(geo_def):\n    """"""Get the border x- and y-coordinates.""""""\n    if geo_def.proj_dict[\'proj\'] == \'geos\':\n        lon_b, lat_b = get_geostationary_bounding_box(geo_def, 3600)\n    else:\n        lons, lats = geo_def.get_boundary_lonlats()\n        lon_b = np.concatenate((lons.side1, lons.side2, lons.side3, lons.side4))\n        lat_b = np.concatenate((lats.side1, lats.side2, lats.side3, lats.side4))\n\n    return lon_b, lat_b\n\n\ndef get_polygon(prj, geo_def):\n    """"""Get border polygon from area definition in projection *prj*.""""""\n    lon_b, lat_b = get_border_lonlats(geo_def)\n    x_borders, y_borders = prj(lon_b, lat_b)\n    boundary = [(x_borders[i], y_borders[i]) for i in range(len(x_borders))\n                if np.isfinite(x_borders[i]) and np.isfinite(y_borders[i])]\n    poly = Polygon(boundary)\n    if np.isfinite(poly.area) and poly.area > 0.0:\n        return poly\n    return None\n\n\ndef parallel_gradient_search(data, src_x, src_y, dst_x, dst_y,\n                             src_gradient_xl, src_gradient_xp,\n                             src_gradient_yl, src_gradient_yp,\n                             dst_mosaic_locations, dst_slices,\n                             **kwargs):\n    """"""Run gradient search in parallel in input area coordinates.""""""\n    method = kwargs.get(\'method\', \'bilinear\')\n    # Determine the number of bands\n    bands = np.array([arr.shape[0] for arr in data if arr is not None])\n    num_bands = np.max(bands)\n    if np.any(bands != num_bands):\n        raise ValueError(""All source data chunks have to have the same number of bands"")\n    chunks = {}\n    is_pad = False\n    # Collect co-located target chunks\n    for i, arr in enumerate(data):\n        if arr is None:\n            is_pad = True\n            res = da.full((num_bands, dst_slices[i][1] - dst_slices[i][0],\n                           dst_slices[i][3] - dst_slices[i][2]), np.nan)\n        else:\n            is_pad = False\n            res = dask.delayed(_gradient_resample_data)(\n                arr.astype(np.float64),\n                src_x[i], src_y[i],\n                src_gradient_xl[i], src_gradient_xp[i],\n                src_gradient_yl[i], src_gradient_yp[i],\n                dst_x[i], dst_y[i],\n                method=method)\n            res = da.from_delayed(res, (num_bands, ) + dst_x[i].shape,\n                                  dtype=np.float64)\n        if dst_mosaic_locations[i] in chunks:\n            if not is_pad:\n                chunks[dst_mosaic_locations[i]].append(res)\n        else:\n            chunks[dst_mosaic_locations[i]] = [res, ]\n\n    return _concatenate_chunks(chunks)\n\n\ndef _concatenate_chunks(chunks):\n    """"""Concatenate chunks to full output array.""""""\n    # Form the full array\n    col, res = [], []\n    prev_y = 0\n    for y, x in sorted(chunks):\n        if len(chunks[(y, x)]) > 1:\n            chunk = da.nanmax(da.stack(chunks[(y, x)], axis=-1), axis=-1)\n        else:\n            chunk = chunks[(y, x)][0]\n        if y == prev_y:\n            col.append(chunk)\n            continue\n        res.append(da.concatenate(col, axis=1))\n        col = [chunk]\n        prev_y = y\n    res.append(da.concatenate(col, axis=1))\n\n    res = da.concatenate(res, axis=2).squeeze()\n\n    return res\n'"
pyresample/test/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2014-2020 Martin Raspaud\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""The test base.""""""\n'"
pyresample/test/test_bilinear.py,153,"b'""""""Test bilinear interpolation.""""""\nimport unittest\nimport numpy as np\nfrom unittest import mock\n\n\nclass TestNumpyBilinear(unittest.TestCase):\n    """"""Test Numpy-based bilinear interpolation.""""""\n\n    @classmethod\n    def setUpClass(cls):\n        """"""Do some setup for the test class.""""""\n        from pyresample import geometry, kd_tree\n\n        cls.pts_irregular = (np.array([[-1., 1.], ]),\n                             np.array([[1., 2.], ]),\n                             np.array([[-2., -1.], ]),\n                             np.array([[2., -4.], ]))\n        cls.pts_vert_parallel = (np.array([[-1., 1.], ]),\n                                 np.array([[1., 2.], ]),\n                                 np.array([[-1., -1.], ]),\n                                 np.array([[1., -2.], ]))\n        cls.pts_both_parallel = (np.array([[-1., 1.], ]),\n                                 np.array([[1., 1.], ]),\n                                 np.array([[-1., -1.], ]),\n                                 np.array([[1., -1.], ]))\n\n        # Area definition with four pixels\n        target_def = geometry.AreaDefinition(\'areaD\',\n                                             \'Europe (3km, HRV, VTC)\',\n                                             \'areaD\',\n                                             {\'a\': \'6378144.0\',\n                                              \'b\': \'6356759.0\',\n                                              \'lat_0\': \'50.00\',\n                                              \'lat_ts\': \'50.00\',\n                                              \'lon_0\': \'8.00\',\n                                              \'proj\': \'stere\'},\n                                             4, 4,\n                                             [-1370912.72,\n                                              -909968.64000000001,\n                                              1029087.28,\n                                              1490031.3600000001])\n\n        # Input data around the target pixel at 0.63388324, 55.08234642,\n        in_shape = (100, 100)\n        cls.data1 = np.ones((in_shape[0], in_shape[1]))\n        cls.data2 = 2. * cls.data1\n        cls.data3 = cls.data1 + 9.5\n        lons, lats = np.meshgrid(np.linspace(-25., 40., num=in_shape[0]),\n                                 np.linspace(45., 75., num=in_shape[1]))\n        cls.swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n\n        radius = 50e3\n        cls.neighbours = 32\n        input_idxs, output_idxs, idx_ref, dists = \\\n            kd_tree.get_neighbour_info(cls.swath_def, target_def,\n                                       radius, neighbours=cls.neighbours,\n                                       nprocs=1)\n        input_size = input_idxs.sum()\n        index_mask = (idx_ref == input_size)\n        idx_ref = np.where(index_mask, 0, idx_ref)\n\n        cls.input_idxs = input_idxs\n        cls.target_def = target_def\n        cls.idx_ref = idx_ref\n\n    def test_calc_abc(self):\n        """"""Test calculation of quadratic coefficients.""""""\n        from pyresample.bilinear import _calc_abc\n\n        # No np.nan inputs\n        pt_1, pt_2, pt_3, pt_4 = self.pts_irregular\n        res = _calc_abc(pt_1, pt_2, pt_3, pt_4, 0.0, 0.0)\n        self.assertFalse(np.isnan(res[0]))\n        self.assertFalse(np.isnan(res[1]))\n        self.assertFalse(np.isnan(res[2]))\n        # np.nan input -> np.nan output\n        res = _calc_abc(np.array([[np.nan, np.nan]]),\n                        pt_2, pt_3, pt_4, 0.0, 0.0)\n        self.assertTrue(np.isnan(res[0]))\n        self.assertTrue(np.isnan(res[1]))\n        self.assertTrue(np.isnan(res[2]))\n\n    def test_get_ts_irregular(self):\n        """"""Test calculations for irregular corner locations.""""""\n        from pyresample.bilinear import _get_ts_irregular\n\n        res = _get_ts_irregular(self.pts_irregular[0],\n                                self.pts_irregular[1],\n                                self.pts_irregular[2],\n                                self.pts_irregular[3],\n                                0., 0.)\n        self.assertEqual(res[0], 0.375)\n        self.assertEqual(res[1], 0.5)\n        res = _get_ts_irregular(self.pts_vert_parallel[0],\n                                self.pts_vert_parallel[1],\n                                self.pts_vert_parallel[2],\n                                self.pts_vert_parallel[3],\n                                0., 0.)\n        self.assertTrue(np.isnan(res[0]))\n        self.assertTrue(np.isnan(res[1]))\n\n    def test_get_ts_uprights_parallel(self):\n        """"""Test calculation when uprights are parallel.""""""\n        from pyresample.bilinear import _get_ts_uprights_parallel\n\n        res = _get_ts_uprights_parallel(self.pts_vert_parallel[0],\n                                        self.pts_vert_parallel[1],\n                                        self.pts_vert_parallel[2],\n                                        self.pts_vert_parallel[3],\n                                        0., 0.)\n        self.assertEqual(res[0], 0.5)\n        self.assertEqual(res[1], 0.5)\n\n    def test_get_ts_parallellogram(self):\n        """"""Test calculation when the corners form a parallellogram.""""""\n        from pyresample.bilinear import _get_ts_parallellogram\n\n        res = _get_ts_parallellogram(self.pts_both_parallel[0],\n                                     self.pts_both_parallel[1],\n                                     self.pts_both_parallel[2],\n                                     0., 0.)\n        self.assertEqual(res[0], 0.5)\n        self.assertEqual(res[1], 0.5)\n\n    def test_get_ts(self):\n        """"""Test get_ts().""""""\n        from pyresample.bilinear import _get_ts\n\n        out_x = np.array([[0.]])\n        out_y = np.array([[0.]])\n        res = _get_ts(self.pts_irregular[0],\n                      self.pts_irregular[1],\n                      self.pts_irregular[2],\n                      self.pts_irregular[3],\n                      out_x, out_y)\n        self.assertEqual(res[0], 0.375)\n        self.assertEqual(res[1], 0.5)\n        res = _get_ts(self.pts_both_parallel[0],\n                      self.pts_both_parallel[1],\n                      self.pts_both_parallel[2],\n                      self.pts_both_parallel[3],\n                      out_x, out_y)\n        self.assertEqual(res[0], 0.5)\n        self.assertEqual(res[1], 0.5)\n        res = _get_ts(self.pts_vert_parallel[0],\n                      self.pts_vert_parallel[1],\n                      self.pts_vert_parallel[2],\n                      self.pts_vert_parallel[3],\n                      out_x, out_y)\n        self.assertEqual(res[0], 0.5)\n        self.assertEqual(res[1], 0.5)\n\n    def test_solve_quadratic(self):\n        """"""Test solving quadratic equation.""""""\n        from pyresample.bilinear import (_solve_quadratic, _calc_abc)\n\n        res = _solve_quadratic(1, 0, 0)\n        self.assertEqual(res[0], 0.0)\n        res = _solve_quadratic(1, 2, 1)\n        self.assertTrue(np.isnan(res[0]))\n        res = _solve_quadratic(1, 2, 1, min_val=-2.)\n        self.assertEqual(res[0], -1.0)\n        # Test that small adjustments work\n        pt_1, pt_2, pt_3, pt_4 = self.pts_vert_parallel\n        pt_1 = self.pts_vert_parallel[0].copy()\n        pt_1[0][0] += 1e-7\n        res = _calc_abc(pt_1, pt_2, pt_3, pt_4, 0.0, 0.0)\n        res = _solve_quadratic(res[0], res[1], res[2])\n        self.assertAlmostEqual(res[0], 0.5, 5)\n        res = _calc_abc(pt_1, pt_3, pt_2, pt_4, 0.0, 0.0)\n        res = _solve_quadratic(res[0], res[1], res[2])\n        self.assertAlmostEqual(res[0], 0.5, 5)\n\n    def test_get_output_xy(self):\n        """"""Test calculation of output xy-coordinates.""""""\n        from pyresample.bilinear import _get_output_xy\n        from pyresample._spatial_mp import Proj\n\n        proj = Proj(self.target_def.proj_str)\n        out_x, out_y = _get_output_xy(self.target_def, proj)\n        self.assertTrue(out_x.all())\n        self.assertTrue(out_y.all())\n\n    def test_get_input_xy(self):\n        """"""Test calculation of input xy-coordinates.""""""\n        from pyresample.bilinear import _get_input_xy\n        from pyresample._spatial_mp import Proj\n\n        proj = Proj(self.target_def.proj_str)\n        in_x, in_y = _get_input_xy(self.swath_def, proj,\n                                   self.input_idxs, self.idx_ref)\n        self.assertTrue(in_x.all())\n        self.assertTrue(in_y.all())\n\n    def test_get_bounding_corners(self):\n        """"""Test calculation of bounding corners.""""""\n        from pyresample.bilinear import (_get_output_xy,\n                                         _get_input_xy,\n                                         _get_bounding_corners)\n        from pyresample._spatial_mp import Proj\n\n        proj = Proj(self.target_def.proj_str)\n        out_x, out_y = _get_output_xy(self.target_def, proj)\n        in_x, in_y = _get_input_xy(self.swath_def, proj,\n                                   self.input_idxs, self.idx_ref)\n        res = _get_bounding_corners(in_x, in_y, out_x, out_y,\n                                    self.neighbours, self.idx_ref)\n        for i in range(len(res) - 1):\n            pt_ = res[i]\n            for j in range(2):\n                # Only the sixth output location has four valid corners\n                self.assertTrue(np.isfinite(pt_[5, j]))\n\n    def test_get_bil_info(self):\n        """"""Test calculation of bilinear resampling indices.""""""\n        from pyresample.bilinear import get_bil_info\n\n        def _check_ts(t__, s__):\n            for i in range(len(t__)):\n                # Just check the exact value for one pixel\n                if i == 5:\n                    self.assertAlmostEqual(t__[i], 0.730659147133, 5)\n                    self.assertAlmostEqual(s__[i], 0.310314173004, 5)\n                # These pixels are outside the area\n                elif i in [12, 13, 14, 15]:\n                    self.assertTrue(np.isnan(t__[i]))\n                    self.assertTrue(np.isnan(s__[i]))\n                # All the others should have values between 0.0 and 1.0\n                else:\n                    self.assertTrue(t__[i] >= 0.0)\n                    self.assertTrue(s__[i] >= 0.0)\n                    self.assertTrue(t__[i] <= 1.0)\n                    self.assertTrue(s__[i] <= 1.0)\n\n        t__, s__, input_idxs, idx_arr = get_bil_info(self.swath_def,\n                                                     self.target_def,\n                                                     50e5, neighbours=32,\n                                                     nprocs=1,\n                                                     reduce_data=False)\n        _check_ts(t__, s__)\n\n        t__, s__, input_idxs, idx_arr = get_bil_info(self.swath_def,\n                                                     self.target_def,\n                                                     50e5, neighbours=32,\n                                                     nprocs=1,\n                                                     reduce_data=True)\n        _check_ts(t__, s__)\n\n    def test_get_sample_from_bil_info(self):\n        """"""Test resampling using resampling indices.""""""\n        from pyresample.bilinear import get_bil_info, get_sample_from_bil_info\n\n        t__, s__, input_idxs, idx_arr = get_bil_info(self.swath_def,\n                                                     self.target_def,\n                                                     50e5, neighbours=32,\n                                                     nprocs=1)\n        # Sample from data1\n        res = get_sample_from_bil_info(self.data1.ravel(), t__, s__,\n                                       input_idxs, idx_arr)\n        self.assertEqual(res[5], 1.)\n        # Sample from data2\n        res = get_sample_from_bil_info(self.data2.ravel(), t__, s__,\n                                       input_idxs, idx_arr)\n        self.assertEqual(res[5], 2.)\n        # Reshaping\n        res = get_sample_from_bil_info(self.data2.ravel(), t__, s__,\n                                       input_idxs, idx_arr,\n                                       output_shape=self.target_def.shape)\n        res = res.shape\n        self.assertEqual(res[0], self.target_def.shape[0])\n        self.assertEqual(res[1], self.target_def.shape[1])\n\n        # Test rounding that is happening for certain values\n        res = get_sample_from_bil_info(self.data3.ravel(), t__, s__,\n                                       input_idxs, idx_arr,\n                                       output_shape=self.target_def.shape)\n        # Four pixels are outside of the data\n        self.assertEqual(np.isnan(res).sum(), 4)\n\n    def test_resample_bilinear(self):\n        """"""Test whole bilinear resampling.""""""\n        from pyresample.bilinear import resample_bilinear\n\n        # Single array\n        res = resample_bilinear(self.data1,\n                                self.swath_def,\n                                self.target_def,\n                                50e5, neighbours=32,\n                                nprocs=1)\n        self.assertEqual(res.shape, self.target_def.shape)\n        # There are 12 pixels with value 1, all others are zero\n        self.assertEqual(res.sum(), 12)\n        self.assertEqual((res == 0).sum(), 4)\n\n        # Single array with masked output\n        res = resample_bilinear(self.data1,\n                                self.swath_def,\n                                self.target_def,\n                                50e5, neighbours=32,\n                                nprocs=1, fill_value=None)\n        self.assertTrue(hasattr(res, \'mask\'))\n        # There should be 12 valid pixels\n        self.assertEqual(self.target_def.size - res.mask.sum(), 12)\n\n        # Two stacked arrays\n        data = np.dstack((self.data1, self.data2))\n        res = resample_bilinear(data,\n                                self.swath_def,\n                                self.target_def)\n        shp = res.shape\n        self.assertEqual(shp[0:2], self.target_def.shape)\n        self.assertEqual(shp[-1], 2)\n\n\nclass TestXarrayBilinear(unittest.TestCase):\n    """"""Test Xarra/Dask -based bilinear interpolation.""""""\n\n    def setUp(self):\n        """"""Do some setup for common things.""""""\n        import dask.array as da\n        from xarray import DataArray\n        from pyresample import geometry, kd_tree\n\n        self.pts_irregular = (np.array([[-1., 1.], ]),\n                              np.array([[1., 2.], ]),\n                              np.array([[-2., -1.], ]),\n                              np.array([[2., -4.], ]))\n        self.pts_vert_parallel = (np.array([[-1., 1.], ]),\n                                  np.array([[1., 2.], ]),\n                                  np.array([[-1., -1.], ]),\n                                  np.array([[1., -2.], ]))\n        self.pts_both_parallel = (np.array([[-1., 1.], ]),\n                                  np.array([[1., 1.], ]),\n                                  np.array([[-1., -1.], ]),\n                                  np.array([[1., -1.], ]))\n\n        # Area definition with four pixels\n        self.target_def = geometry.AreaDefinition(\'areaD\',\n                                                  \'Europe (3km, HRV, VTC)\',\n                                                  \'areaD\',\n                                                  {\'a\': \'6378144.0\',\n                                                   \'b\': \'6356759.0\',\n                                                   \'lat_0\': \'50.00\',\n                                                   \'lat_ts\': \'50.00\',\n                                                   \'lon_0\': \'8.00\',\n                                                   \'proj\': \'stere\'},\n                                                  4, 4,\n                                                  [-1370912.72,\n                                                   -909968.64000000001,\n                                                   1029087.28,\n                                                   1490031.3600000001])\n\n        # Input data around the target pixel at 0.63388324, 55.08234642,\n        in_shape = (100, 100)\n        self.data1 = DataArray(da.ones((in_shape[0], in_shape[1])), dims=(\'y\', \'x\'))\n        self.data2 = 2. * self.data1\n        self.data3 = self.data1 + 9.5\n        lons, lats = np.meshgrid(np.linspace(-25., 40., num=in_shape[0]),\n                                 np.linspace(45., 75., num=in_shape[1]))\n        self.source_def = geometry.SwathDefinition(lons=lons, lats=lats)\n\n        self.radius = 50e3\n        self.neighbours = 32\n        valid_input_index, output_idxs, index_array, dists = \\\n            kd_tree.get_neighbour_info(self.source_def, self.target_def,\n                                       self.radius, neighbours=self.neighbours,\n                                       nprocs=1)\n        input_size = valid_input_index.sum()\n        index_mask = (index_array == input_size)\n        index_array = np.where(index_mask, 0, index_array)\n\n        self.valid_input_index = valid_input_index\n        self.index_array = index_array\n\n        shp = self.source_def.shape\n        self.cols, self.lines = np.meshgrid(np.arange(shp[1]),\n                                            np.arange(shp[0]))\n\n    def test_init(self):\n        """"""Test that the resampler has been initialized correctly.""""""\n        from pyresample.bilinear.xarr import XArrayResamplerBilinear\n\n        # With defaults\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius)\n        self.assertTrue(resampler.source_geo_def == self.source_def)\n        self.assertTrue(resampler.target_geo_def == self.target_def)\n        self.assertEqual(resampler.radius_of_influence, self.radius)\n        self.assertEqual(resampler.neighbours, 32)\n        self.assertEqual(resampler.epsilon, 0)\n        self.assertTrue(resampler.reduce_data)\n        # These should be None\n        self.assertIsNone(resampler.valid_input_index)\n        self.assertIsNone(resampler.valid_output_index)\n        self.assertIsNone(resampler.index_array)\n        self.assertIsNone(resampler.distance_array)\n        self.assertIsNone(resampler.bilinear_t)\n        self.assertIsNone(resampler.bilinear_s)\n        self.assertIsNone(resampler.slices_x)\n        self.assertIsNone(resampler.slices_y)\n        self.assertIsNone(resampler.mask_slices)\n        self.assertIsNone(resampler.out_coords_x)\n        self.assertIsNone(resampler.out_coords_y)\n        # self.slices_{x,y} are used in self.slices dict\n        self.assertTrue(resampler.slices[\'x\'] is resampler.slices_x)\n        self.assertTrue(resampler.slices[\'y\'] is resampler.slices_y)\n        # self.out_coords_{x,y} are used in self.out_coords dict\n        self.assertTrue(resampler.out_coords[\'x\'] is resampler.out_coords_x)\n        self.assertTrue(resampler.out_coords[\'y\'] is resampler.out_coords_y)\n\n        # Override defaults\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius, neighbours=16,\n                                            epsilon=0.1, reduce_data=False)\n        self.assertEqual(resampler.neighbours, 16)\n        self.assertEqual(resampler.epsilon, 0.1)\n        self.assertFalse(resampler.reduce_data)\n\n    def test_get_bil_info(self):\n        """"""Test calculation of bilinear info.""""""\n        from pyresample.bilinear.xarr import XArrayResamplerBilinear\n\n        def _check_ts(t__, s__, nans):\n            for i, _ in enumerate(t__):\n                # Just check the exact value for one pixel\n                if i == 5:\n                    self.assertAlmostEqual(t__[i], 0.730659147133, 5)\n                    self.assertAlmostEqual(s__[i], 0.310314173004, 5)\n                # These pixels are outside the area\n                elif i in nans:\n                    self.assertTrue(np.isnan(t__[i]))\n                    self.assertTrue(np.isnan(s__[i]))\n                # All the others should have values between 0.0 and 1.0\n                else:\n                    self.assertTrue(t__[i] >= 0.0)\n                    self.assertTrue(s__[i] >= 0.0)\n                    self.assertTrue(t__[i] <= 1.0)\n                    self.assertTrue(s__[i] <= 1.0)\n\n        # Data reduction enabled (default)\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius, reduce_data=True)\n        (t__, s__, slices, mask_slices, out_coords) = resampler.get_bil_info()\n        _check_ts(t__.compute(), s__.compute(), [3, 10, 12, 13, 14, 15])\n\n        # Nothing should be masked based on coordinates\n        self.assertTrue(np.all(~mask_slices))\n        # Four values per output location\n        self.assertEqual(mask_slices.shape, (self.target_def.size, 4))\n\n        # self.slices_{x,y} are used in self.slices dict so they\n        # should be the same (object)\n        self.assertTrue(isinstance(slices, dict))\n        self.assertTrue(resampler.slices[\'x\'] is resampler.slices_x)\n        self.assertTrue(np.all(resampler.slices[\'x\'] == slices[\'x\']))\n        self.assertTrue(resampler.slices[\'y\'] is resampler.slices_y)\n        self.assertTrue(np.all(resampler.slices[\'y\'] == slices[\'y\']))\n\n        # self.slices_{x,y} are used in self.slices dict so they\n        # should be the same (object)\n        self.assertTrue(isinstance(out_coords, dict))\n        self.assertTrue(resampler.out_coords[\'x\'] is resampler.out_coords_x)\n        self.assertTrue(np.all(resampler.out_coords[\'x\'] == out_coords[\'x\']))\n        self.assertTrue(resampler.out_coords[\'y\'] is resampler.out_coords_y)\n        self.assertTrue(np.all(resampler.out_coords[\'y\'] == out_coords[\'y\']))\n\n        # Also some other attributes should have been set\n        self.assertTrue(t__ is resampler.bilinear_t)\n        self.assertTrue(s__ is resampler.bilinear_s)\n        self.assertIsNotNone(resampler.valid_output_index)\n        self.assertIsNotNone(resampler.index_array)\n        self.assertIsNotNone(resampler.valid_input_index)\n\n        # Data reduction disabled\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius, reduce_data=False)\n        (t__, s__, slices, mask_slices, out_coords) = resampler.get_bil_info()\n        _check_ts(t__.compute(), s__.compute(), [10, 12, 13, 14, 15])\n\n    def test_get_sample_from_bil_info(self):\n        """"""Test bilinear interpolation as a whole.""""""\n        from pyresample.bilinear.xarr import XArrayResamplerBilinear\n\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius)\n        _ = resampler.get_bil_info()\n\n        # Sample from data1\n        res = resampler.get_sample_from_bil_info(self.data1)\n        res = res.compute()\n        # Check couple of values\n        self.assertEqual(res.values[1, 1], 1.)\n        self.assertTrue(np.isnan(res.values[0, 3]))\n        # Check that the values haven\'t gone down or up a lot\n        self.assertAlmostEqual(np.nanmin(res.values), 1.)\n        self.assertAlmostEqual(np.nanmax(res.values), 1.)\n        # Check that dimensions are the same\n        self.assertEqual(res.dims, self.data1.dims)\n\n        # Sample from data1, custom fill value\n        res = resampler.get_sample_from_bil_info(self.data1, fill_value=-1.0)\n        res = res.compute()\n        self.assertEqual(np.nanmin(res.values), -1.)\n\n        # Sample from integer data\n        res = resampler.get_sample_from_bil_info(self.data1.astype(np.uint8),\n                                                 fill_value=None)\n        res = res.compute()\n        # Five values should be filled with zeros, which is the\n        # default fill_value for integer data\n        self.assertEqual(np.sum(res == 0), 6)\n\n    @mock.patch(\'pyresample.bilinear.xarr.setattr\')\n    def test_compute_indices(self, mock_setattr):\n        """"""Test running .compute() for indices.""""""\n        from pyresample.bilinear.xarr import (XArrayResamplerBilinear,\n                                              CACHE_INDICES)\n\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius)\n\n        # Set indices to Numpy arrays\n        for idx in CACHE_INDICES:\n            setattr(resampler, idx, np.array([]))\n        resampler._compute_indices()\n        # None of the indices shouldn\'t have been reassigned\n        mock_setattr.assert_not_called()\n\n        # Set indices to a Mock object\n        arr = mock.MagicMock()\n        for idx in CACHE_INDICES:\n            setattr(resampler, idx, arr)\n        resampler._compute_indices()\n        # All the indices should have been reassigned\n        self.assertEqual(mock_setattr.call_count, len(CACHE_INDICES))\n        # The compute should have been called the same amount of times\n        self.assertEqual(arr.compute.call_count, len(CACHE_INDICES))\n\n    def test_add_missing_coordinates(self):\n        """"""Test coordinate updating.""""""\n        import dask.array as da\n        from xarray import DataArray\n        from pyresample.bilinear.xarr import XArrayResamplerBilinear\n\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius)\n        bands = [\'R\', \'G\', \'B\']\n        data = DataArray(da.ones((3, 10, 10)), dims=(\'bands\', \'y\', \'x\'),\n                         coords={\'bands\': bands,\n                                 \'y\': np.arange(10), \'x\': np.arange(10)})\n        resampler._add_missing_coordinates(data)\n        # X and Y coordinates should not change\n        self.assertIsNone(resampler.out_coords_x)\n        self.assertIsNone(resampler.out_coords_y)\n        self.assertIsNone(resampler.out_coords[\'x\'])\n        self.assertIsNone(resampler.out_coords[\'y\'])\n        self.assertTrue(\'bands\' in resampler.out_coords)\n        self.assertTrue(np.all(resampler.out_coords[\'bands\'] == bands))\n\n    def test_slice_data(self):\n        """"""Test slicing the data.""""""\n        import dask.array as da\n        from xarray import DataArray\n        from pyresample.bilinear.xarr import XArrayResamplerBilinear\n\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius)\n\n        # Too many dimensions\n        data = DataArray(da.ones((1, 3, 10, 10)))\n        with self.assertRaises(ValueError):\n            _ = resampler._slice_data(data, np.nan)\n\n        # 2D data\n        data = DataArray(da.ones((10, 10)))\n        resampler.slices_x = np.random.randint(0, 10, (100, 4))\n        resampler.slices_y = np.random.randint(0, 10, (100, 4))\n        resampler.mask_slices = np.zeros((100, 4), dtype=np.bool)\n        p_1, p_2, p_3, p_4 = resampler._slice_data(data, np.nan)\n        self.assertEqual(p_1.shape, (100, ))\n        self.assertTrue(p_1.shape == p_2.shape == p_3.shape == p_4.shape)\n        self.assertTrue(np.all(p_1 == 1.0) and np.all(p_2 == 1.0) and\n                        np.all(p_3 == 1.0) and np.all(p_4 == 1.0))\n\n        # 2D data with masking\n        resampler.mask_slices = np.ones((100, 4), dtype=np.bool)\n        p_1, p_2, p_3, p_4 = resampler._slice_data(data, np.nan)\n        self.assertTrue(np.all(np.isnan(p_1)) and np.all(np.isnan(p_2)) and\n                        np.all(np.isnan(p_3)) and np.all(np.isnan(p_4)))\n\n        # 3D data\n        data = DataArray(da.ones((3, 10, 10)))\n        resampler.slices_x = np.random.randint(0, 10, (100, 4))\n        resampler.slices_y = np.random.randint(0, 10, (100, 4))\n        resampler.mask_slices = np.zeros((100, 4), dtype=np.bool)\n        p_1, p_2, p_3, p_4 = resampler._slice_data(data, np.nan)\n        self.assertEqual(p_1.shape, (3, 100))\n        self.assertTrue(p_1.shape == p_2.shape == p_3.shape == p_4.shape)\n\n        # 3D data with masking\n        resampler.mask_slices = np.ones((100, 4), dtype=np.bool)\n        p_1, p_2, p_3, p_4 = resampler._slice_data(data, np.nan)\n        self.assertTrue(np.all(np.isnan(p_1)) and np.all(np.isnan(p_2)) and\n                        np.all(np.isnan(p_3)) and np.all(np.isnan(p_4)))\n\n    @mock.patch(\'pyresample.bilinear.xarr.np.meshgrid\')\n    def test_get_slices(self, meshgrid):\n        """"""Test slice array creation.""""""\n        from pyresample.bilinear.xarr import XArrayResamplerBilinear\n\n        meshgrid.return_value = (self.cols, self.lines)\n\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius)\n        resampler.valid_input_index = self.valid_input_index\n        resampler.index_array = self.index_array\n\n        resampler._get_slices()\n        self.assertIsNotNone(resampler.out_coords_x)\n        self.assertIsNotNone(resampler.out_coords_y)\n        self.assertTrue(resampler.out_coords_x is resampler.out_coords[\'x\'])\n        self.assertTrue(resampler.out_coords_y is resampler.out_coords[\'y\'])\n        self.assertTrue(np.allclose(\n            resampler.out_coords_x,\n            [-1070912.72, -470912.72, 129087.28, 729087.28]))\n        self.assertTrue(np.allclose(\n            resampler.out_coords_y,\n            [1190031.36,  590031.36,   -9968.64, -609968.64]))\n\n        self.assertIsNotNone(resampler.slices_x)\n        self.assertIsNotNone(resampler.slices_y)\n        self.assertTrue(resampler.slices_x is resampler.slices[\'x\'])\n        self.assertTrue(resampler.slices_y is resampler.slices[\'y\'])\n        self.assertTrue(resampler.slices_x.shape == (self.target_def.size, 32))\n        self.assertTrue(resampler.slices_y.shape == (self.target_def.size, 32))\n        self.assertEqual(np.sum(resampler.slices_x), 12471)\n        self.assertEqual(np.sum(resampler.slices_y), 2223)\n\n        self.assertFalse(np.any(resampler.mask_slices))\n\n        # Ensure that source geo def is used in masking\n        # Setting target_geo_def to 0-size shouldn\'t cause any masked values\n        resampler.target_geo_def = np.array([])\n        resampler._get_slices()\n        self.assertFalse(np.any(resampler.mask_slices))\n        # Setting source area def to 0-size should mask all values\n        resampler.source_geo_def = np.array([[]])\n        resampler._get_slices()\n        self.assertTrue(np.all(resampler.mask_slices))\n\n    @mock.patch(\'pyresample.bilinear.xarr.KDTree\')\n    def test_create_resample_kdtree(self, KDTree):\n        """"""Test that KDTree creation is called.""""""\n        from pyresample.bilinear.xarr import XArrayResamplerBilinear\n\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius)\n\n        vii, kdtree = resampler._create_resample_kdtree()\n        self.assertEqual(np.sum(vii), 2700)\n        self.assertEqual(vii.size, self.source_def.size)\n        KDTree.assert_called_once()\n\n    @mock.patch(\'pyresample.bilinear.xarr.query_no_distance\')\n    def test_query_resample_kdtree(self, qnd):\n        """"""Test that query_no_distance is called in _query_resample_kdtree().""""""\n        from pyresample.bilinear.xarr import XArrayResamplerBilinear\n\n        resampler = XArrayResamplerBilinear(self.source_def, self.target_def,\n                                            self.radius)\n        res, none = resampler._query_resample_kdtree(1, 2, 3, 4,\n                                                     reduce_data=5)\n        qnd.assert_called_with(2, 3, 4, 1, resampler.neighbours,\n                               resampler.epsilon,\n                               resampler.radius_of_influence)\n\n    def test_get_input_xy_dask(self):\n        """"""Test computation of input X and Y coordinates in target proj.""""""\n        import dask.array as da\n        from pyresample.bilinear.xarr import _get_input_xy_dask\n        from pyresample._spatial_mp import Proj\n\n        proj = Proj(self.target_def.proj_str)\n        in_x, in_y = _get_input_xy_dask(self.source_def, proj,\n                                        da.from_array(self.valid_input_index),\n                                        da.from_array(self.index_array))\n\n        self.assertTrue(in_x.shape, (self.target_def.size, 32))\n        self.assertTrue(in_y.shape, (self.target_def.size, 32))\n        self.assertTrue(in_x.all())\n        self.assertTrue(in_y.all())\n\n    def test_mask_coordinates_dask(self):\n        """"""Test masking of invalid coordinates.""""""\n        import dask.array as da\n        from pyresample.bilinear.xarr import _mask_coordinates_dask\n\n        lons, lats = _mask_coordinates_dask(\n            da.from_array([-200., 0., 0., 0., 200.]),\n            da.from_array([0., -100., 0, 100., 0.]))\n        lons, lats = da.compute(lons, lats)\n        self.assertTrue(lons[2] == lats[2] == 0.0)\n        self.assertEqual(np.sum(np.isnan(lons)), 4)\n        self.assertEqual(np.sum(np.isnan(lats)), 4)\n\n    def test_get_bounding_corners_dask(self):\n        """"""Test finding surrounding bounding corners.""""""\n        import dask.array as da\n        from pyresample.bilinear.xarr import (_get_input_xy_dask,\n                                              _get_bounding_corners_dask)\n        from pyresample._spatial_mp import Proj\n        from pyresample import CHUNK_SIZE\n\n        proj = Proj(self.target_def.proj_str)\n        out_x, out_y = self.target_def.get_proj_coords(chunks=CHUNK_SIZE)\n        out_x = da.ravel(out_x)\n        out_y = da.ravel(out_y)\n        in_x, in_y = _get_input_xy_dask(self.source_def, proj,\n                                        da.from_array(self.valid_input_index),\n                                        da.from_array(self.index_array))\n        pt_1, pt_2, pt_3, pt_4, ia_ = _get_bounding_corners_dask(\n            in_x, in_y, out_x, out_y,\n            self.neighbours,\n            da.from_array(self.index_array))\n\n        self.assertTrue(pt_1.shape == pt_2.shape ==\n                        pt_3.shape == pt_4.shape ==\n                        (self.target_def.size, 2))\n        self.assertTrue(ia_.shape == (self.target_def.size, 4))\n\n        # Check which of the locations has four valid X/Y pairs by\n        # finding where there are non-NaN values\n        res = da.sum(pt_1 + pt_2 + pt_3 + pt_4, axis=1).compute()\n        self.assertEqual(np.sum(~np.isnan(res)), 10)\n\n    def test_get_corner_dask(self):\n        """"""Test finding the closest corners.""""""\n        import dask.array as da\n        from pyresample.bilinear.xarr import (_get_corner_dask,\n                                              _get_input_xy_dask)\n        from pyresample import CHUNK_SIZE\n        from pyresample._spatial_mp import Proj\n\n        proj = Proj(self.target_def.proj_str)\n        in_x, in_y = _get_input_xy_dask(self.source_def, proj,\n                                        da.from_array(self.valid_input_index),\n                                        da.from_array(self.index_array))\n        out_x, out_y = self.target_def.get_proj_coords(chunks=CHUNK_SIZE)\n        out_x = da.ravel(out_x)\n        out_y = da.ravel(out_y)\n\n        # Some copy&paste from the code to get the input\n        out_x_tile = np.reshape(np.tile(out_x, self.neighbours),\n                                (self.neighbours, out_x.size)).T\n        out_y_tile = np.reshape(np.tile(out_y, self.neighbours),\n                                (self.neighbours, out_y.size)).T\n        x_diff = out_x_tile - in_x\n        y_diff = out_y_tile - in_y\n        stride = np.arange(x_diff.shape[0])\n\n        # Use lower left source pixels for testing\n        valid = (x_diff > 0) & (y_diff > 0)\n        x_3, y_3, idx_3 = _get_corner_dask(stride, valid, in_x, in_y,\n                                           da.from_array(self.index_array))\n\n        self.assertTrue(x_3.shape == y_3.shape == idx_3.shape ==\n                        (self.target_def.size, ))\n        # Four locations have no data to the lower left of them (the\n        # bottom row of the area\n        self.assertEqual(np.sum(np.isnan(x_3.compute())), 4)\n\n    @mock.patch(\'pyresample.bilinear.xarr._get_ts_parallellogram_dask\')\n    @mock.patch(\'pyresample.bilinear.xarr._get_ts_uprights_parallel_dask\')\n    @mock.patch(\'pyresample.bilinear.xarr._get_ts_irregular_dask\')\n    def test_get_ts_dask(self, irregular, uprights, parallellogram):\n        """"""Test that the three separate functions are called.""""""\n        from pyresample.bilinear.xarr import _get_ts_dask\n\n        # All valid values\n        t_irr = np.array([0.1, 0.2, 0.3])\n        s_irr = np.array([0.1, 0.2, 0.3])\n        irregular.return_value = (t_irr, s_irr)\n        t__, s__ = _get_ts_dask(1, 2, 3, 4, 5, 6)\n        irregular.assert_called_once()\n        uprights.assert_not_called()\n        parallellogram.assert_not_called()\n        self.assertTrue(np.allclose(t__.compute(), t_irr))\n        self.assertTrue(np.allclose(s__.compute(), s_irr))\n\n        # NaN in the first step, good value for that location from the\n        # second step\n        t_irr = np.array([0.1, 0.2, np.nan])\n        s_irr = np.array([0.1, 0.2, np.nan])\n        irregular.return_value = (t_irr, s_irr)\n        t_upr = np.array([3, 3, 0.3])\n        s_upr = np.array([3, 3, 0.3])\n        uprights.return_value = (t_upr, s_upr)\n        t__, s__ = _get_ts_dask(1, 2, 3, 4, 5, 6)\n        self.assertEqual(irregular.call_count, 2)\n        uprights.assert_called_once()\n        parallellogram.assert_not_called()\n        # Only the last value of the first step should have been replaced\n        t_res = np.array([0.1, 0.2, 0.3])\n        s_res = np.array([0.1, 0.2, 0.3])\n        self.assertTrue(np.allclose(t__.compute(), t_res))\n        self.assertTrue(np.allclose(s__.compute(), s_res))\n\n        # Two NaNs in the first step, one of which are found by the\n        # second, and the last bad value is replaced by the third step\n        t_irr = np.array([0.1, np.nan, np.nan])\n        s_irr = np.array([0.1, np.nan, np.nan])\n        irregular.return_value = (t_irr, s_irr)\n        t_upr = np.array([3, np.nan, 0.3])\n        s_upr = np.array([3, np.nan, 0.3])\n        uprights.return_value = (t_upr, s_upr)\n        t_par = np.array([4, 0.2, 0.3])\n        s_par = np.array([4, 0.2, 0.3])\n        parallellogram.return_value = (t_par, s_par)\n        t__, s__ = _get_ts_dask(1, 2, 3, 4, 5, 6)\n        self.assertEqual(irregular.call_count, 3)\n        self.assertEqual(uprights.call_count, 2)\n        parallellogram.assert_called_once()\n        # Only the last two values should have been replaced\n        t_res = np.array([0.1, 0.2, 0.3])\n        s_res = np.array([0.1, 0.2, 0.3])\n        self.assertTrue(np.allclose(t__.compute(), t_res))\n        self.assertTrue(np.allclose(s__.compute(), s_res))\n\n        # Too large and small values should be set to NaN\n        t_irr = np.array([1.00001, -0.00001, 1e6])\n        s_irr = np.array([1.00001, -0.00001, -1e6])\n        irregular.return_value = (t_irr, s_irr)\n        # Second step also returns invalid values\n        t_upr = np.array([1.00001, 0.2, np.nan])\n        s_upr = np.array([-0.00001, 0.2, np.nan])\n        uprights.return_value = (t_upr, s_upr)\n        # Third step has one new valid value, the last will stay invalid\n        t_par = np.array([0.1, 0.2, 4.0])\n        s_par = np.array([0.1, 0.2, 4.0])\n        parallellogram.return_value = (t_par, s_par)\n        t__, s__ = _get_ts_dask(1, 2, 3, 4, 5, 6)\n\n        t_res = np.array([0.1, 0.2, np.nan])\n        s_res = np.array([0.1, 0.2, np.nan])\n        self.assertTrue(np.allclose(t__.compute(), t_res, equal_nan=True))\n        self.assertTrue(np.allclose(s__.compute(), s_res, equal_nan=True))\n\n    def test_get_ts_irregular_dask(self):\n        """"""Test calculations for irregular corner locations.""""""\n        from pyresample.bilinear.xarr import _get_ts_irregular_dask\n\n        res = _get_ts_irregular_dask(self.pts_irregular[0],\n                                     self.pts_irregular[1],\n                                     self.pts_irregular[2],\n                                     self.pts_irregular[3],\n                                     0., 0.)\n        self.assertEqual(res[0], 0.375)\n        self.assertEqual(res[1], 0.5)\n        res = _get_ts_irregular_dask(self.pts_vert_parallel[0],\n                                     self.pts_vert_parallel[1],\n                                     self.pts_vert_parallel[2],\n                                     self.pts_vert_parallel[3],\n                                     0., 0.)\n        self.assertTrue(np.isnan(res[0]))\n        self.assertTrue(np.isnan(res[1]))\n\n    def test_get_ts_uprights_parallel(self):\n        """"""Test calculation when uprights are parallel.""""""\n        from pyresample.bilinear import _get_ts_uprights_parallel\n\n        res = _get_ts_uprights_parallel(self.pts_vert_parallel[0],\n                                        self.pts_vert_parallel[1],\n                                        self.pts_vert_parallel[2],\n                                        self.pts_vert_parallel[3],\n                                        0., 0.)\n        self.assertEqual(res[0], 0.5)\n        self.assertEqual(res[1], 0.5)\n\n    def test_get_ts_parallellogram(self):\n        """"""Test calculation when the corners form a parallellogram.""""""\n        from pyresample.bilinear import _get_ts_parallellogram\n\n        res = _get_ts_parallellogram(self.pts_both_parallel[0],\n                                     self.pts_both_parallel[1],\n                                     self.pts_both_parallel[2],\n                                     0., 0.)\n        self.assertEqual(res[0], 0.5)\n        self.assertEqual(res[1], 0.5)\n\n    def test_calc_abc(self):\n        """"""Test calculation of quadratic coefficients.""""""\n        from pyresample.bilinear.xarr import _calc_abc_dask\n\n        # No np.nan inputs\n        pt_1, pt_2, pt_3, pt_4 = self.pts_irregular\n        res = _calc_abc_dask(pt_1, pt_2, pt_3, pt_4, 0.0, 0.0)\n        self.assertFalse(np.isnan(res[0]))\n        self.assertFalse(np.isnan(res[1]))\n        self.assertFalse(np.isnan(res[2]))\n        # np.nan input -> np.nan output\n        res = _calc_abc_dask(np.array([[np.nan, np.nan]]),\n                             pt_2, pt_3, pt_4, 0.0, 0.0)\n        self.assertTrue(np.isnan(res[0]))\n        self.assertTrue(np.isnan(res[1]))\n        self.assertTrue(np.isnan(res[2]))\n\n    def test_solve_quadratic(self):\n        """"""Test solving quadratic equation.""""""\n        from pyresample.bilinear.xarr import (_solve_quadratic_dask,\n                                              _calc_abc_dask)\n\n        res = _solve_quadratic_dask(1, 0, 0).compute()\n        self.assertEqual(res, 0.0)\n        res = _solve_quadratic_dask(1, 2, 1).compute()\n        self.assertTrue(np.isnan(res))\n        res = _solve_quadratic_dask(1, 2, 1, min_val=-2.).compute()\n        self.assertEqual(res, -1.0)\n        # Test that small adjustments work\n        pt_1, pt_2, pt_3, pt_4 = self.pts_vert_parallel\n        pt_1 = self.pts_vert_parallel[0].copy()\n        pt_1[0][0] += 1e-7\n        res = _calc_abc_dask(pt_1, pt_2, pt_3, pt_4, 0.0, 0.0)\n        res = _solve_quadratic_dask(res[0], res[1], res[2]).compute()\n        self.assertAlmostEqual(res[0], 0.5, 5)\n        res = _calc_abc_dask(pt_1, pt_3, pt_2, pt_4, 0.0, 0.0)\n        res = _solve_quadratic_dask(res[0], res[1], res[2]).compute()\n        self.assertAlmostEqual(res[0], 0.5, 5)\n\n    def test_query_no_distance(self):\n        """"""Test KDTree querying.""""""\n        from pyresample.bilinear.xarr import query_no_distance\n\n        kdtree = mock.MagicMock()\n        kdtree.query.return_value = (1, 2)\n        lons, lats = self.target_def.get_lonlats()\n        voi = (lons >= -180) & (lons <= 180) & (lats <= 90) & (lats >= -90)\n        res = query_no_distance(lons, lats, voi, kdtree, self.neighbours,\n                                0., self.radius)\n        # Only the second value from the query is returned\n        self.assertEqual(res, 2)\n        kdtree.query.assert_called_once()\n\n    def test_get_valid_input_index_dask(self):\n        """"""Test finding valid indices for reduced input data.""""""\n        from pyresample.bilinear.xarr import _get_valid_input_index_dask\n\n        # Do not reduce data\n        vii, lons, lats = _get_valid_input_index_dask(self.source_def,\n                                                      self.target_def,\n                                                      False, self.radius)\n        self.assertEqual(vii.shape, (self.source_def.size, ))\n        self.assertTrue(vii.dtype == np.bool)\n        # No data has been reduced, whole input is used\n        self.assertTrue(vii.compute().all())\n\n        # Reduce data\n        vii, lons, lats = _get_valid_input_index_dask(self.source_def,\n                                                      self.target_def,\n                                                      True, self.radius)\n        # 2700 valid input points\n        self.assertEqual(vii.compute().sum(), 2700)\n\n    def test_create_empty_bil_info(self):\n        """"""Test creation of empty bilinear info.""""""\n        from pyresample.bilinear.xarr import _create_empty_bil_info\n\n        t__, s__, vii, ia_ = _create_empty_bil_info(self.source_def,\n                                                    self.target_def)\n        self.assertEqual(t__.shape, (self.target_def.size,))\n        self.assertEqual(s__.shape, (self.target_def.size,))\n        self.assertEqual(ia_.shape, (self.target_def.size, 4))\n        self.assertTrue(ia_.dtype == np.int32)\n        self.assertEqual(vii.shape, (self.source_def.size,))\n        self.assertTrue(vii.dtype == np.bool)\n\n    def test_lonlat2xyz(self):\n        """"""Test conversion from geographic to cartesian 3D coordinates.""""""\n        from pyresample.bilinear.xarr import lonlat2xyz\n        from pyresample import CHUNK_SIZE\n\n        lons, lats = self.target_def.get_lonlats(chunks=CHUNK_SIZE)\n        res = lonlat2xyz(lons, lats)\n        self.assertEqual(res.shape, (self.target_def.size, 3))\n        vals = [3188578.91069278, -612099.36103276, 5481596.63569999]\n        self.assertTrue(np.allclose(res.compute()[0, :], vals))\n'"
pyresample/test/test_bucket.py,49,"b'import unittest\nimport numpy as np\nimport dask.array as da\nimport dask\nimport xarray as xr\nfrom unittest.mock import MagicMock, patch\n\nfrom pyresample import create_area_def\nfrom pyresample.geometry import AreaDefinition\nfrom pyresample import bucket\nfrom pyresample.test.utils import CustomScheduler\n\n\nclass Test(unittest.TestCase):\n\n    adef = AreaDefinition(\'eurol\', \'description\', \'\',\n                          {\'ellps\': \'WGS84\',\n                           \'lat_0\': \'90.0\',\n                           \'lat_ts\': \'60.0\',\n                           \'lon_0\': \'0.0\',\n                           \'proj\': \'stere\'}, 2560, 2048,\n                          (-3780000.0, -7644000.0, 3900000.0, -1500000.0))\n    chunks = 2\n    lons = da.from_array(np.array([[25., 25.], [25., 25.]]),\n                         chunks=chunks)\n    lats = da.from_array(np.array([[60., 60.00001], [60.2, 60.3]]),\n                         chunks=chunks)\n\n    def setUp(self):\n        self.resampler = bucket.BucketResampler(self.adef, self.lons, self.lats)\n\n    @patch(\'pyresample.bucket.Proj\')\n    @patch(\'pyresample.bucket.BucketResampler._get_indices\')\n    def test_init(self, get_indices, prj):\n        resampler = bucket.BucketResampler(self.adef, self.lons, self.lats)\n        get_indices.assert_called_once()\n        prj.assert_called_once_with(self.adef.proj_dict)\n        self.assertTrue(hasattr(resampler, \'target_area\'))\n        self.assertTrue(hasattr(resampler, \'source_lons\'))\n        self.assertTrue(hasattr(resampler, \'source_lats\'))\n        self.assertTrue(hasattr(resampler, \'x_idxs\'))\n        self.assertTrue(hasattr(resampler, \'y_idxs\'))\n        self.assertTrue(hasattr(resampler, \'idxs\'))\n        self.assertTrue(hasattr(resampler, \'get_sum\'))\n        self.assertTrue(hasattr(resampler, \'get_count\'))\n        self.assertTrue(hasattr(resampler, \'get_average\'))\n        self.assertTrue(hasattr(resampler, \'get_fractions\'))\n        self.assertIsNone(resampler.counts)\n\n    def test_round_to_resolution(self):\n        """"""Test rounding to given resolution""""""\n        # Scalar, integer resolution\n        self.assertEqual(bucket.round_to_resolution(5.5, 2.), 6)\n        # Scalar, non-integer resolution\n        self.assertEqual(bucket.round_to_resolution(5.5, 1.7), 5.1)\n        # List\n        self.assertTrue(np.all(bucket.round_to_resolution([4.2, 5.6], 2) ==\n                               np.array([4., 6.])))\n        # Numpy array\n        self.assertTrue(np.all(bucket.round_to_resolution(np.array([4.2, 5.6]), 2) ==\n                               np.array([4., 6.])))\n        # Dask array\n        self.assertTrue(\n            np.all(bucket.round_to_resolution(da.array([4.2, 5.6]), 2) ==\n                   np.array([4., 6.])))\n\n    def test_get_proj_coordinates(self):\n        """"""Test calculation of projection coordinates.""""""\n        prj = MagicMock()\n        prj.return_value = ([3.1, 3.1, 3.1], [4.8, 4.8, 4.8])\n        lons = [1., 1., 1.]\n        lats = [2., 2., 2.]\n        self.resampler.prj = prj\n        result = self.resampler._get_proj_coordinates(lons, lats)\n        prj.assert_called_once_with(lons, lats)\n        self.assertTrue(isinstance(result, np.ndarray))\n        np.testing.assert_equal(result, np.array([[3.1, 3.1, 3.1],\n                                                  [4.8, 4.8, 4.8]]))\n\n    def test_get_bucket_indices(self):\n        """"""Test calculation of array indices.""""""\n        # Ensure nothing is calculated\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            self.resampler._get_indices()\n        x_idxs, y_idxs = da.compute(self.resampler.x_idxs,\n                                    self.resampler.y_idxs)\n        np.testing.assert_equal(x_idxs, np.array([1710, 1710, 1707, 1705]))\n        np.testing.assert_equal(y_idxs, np.array([465, 465, 459, 455]))\n\n        # Additional small test case\n        adef = create_area_def(\n            area_id=\'test\',\n            projection={\'proj\': \'latlong\'},\n            width=2, height=2,\n            center=(0, 0),\n            resolution=10)\n        lons = da.from_array(\n            np.array([-10.0, -9.9, -0.1, 0, 0.1, 9.9, 10.0, -10.1, 0]),\n            chunks=2)\n        lats = da.from_array(\n            np.array([-10.0, -9.9, -0.1, 0, 0.1, 9.9, 10.0, 0, 10.1]),\n            chunks=2)\n        resampler = bucket.BucketResampler(source_lats=lats,\n                                           source_lons=lons,\n                                           target_area=adef)\n        resampler._get_indices()\n        np.testing.assert_equal(resampler.x_idxs, np.array([-1, 0, 0, 1, 1, 1, -1, -1, -1]))\n        np.testing.assert_equal(resampler.y_idxs, np.array([-1, 1, 1, 1, 0, 0, -1, -1, -1]))\n\n    def test_get_sum(self):\n        """"""Test drop-in-a-bucket sum.""""""\n        data = da.from_array(np.array([[2., 2.], [2., 2.]]),\n                             chunks=self.chunks)\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_sum(data)\n\n        result = result.compute()\n        # One bin with two hits, so max value is 2.0\n        self.assertTrue(np.max(result) == 4.)\n        # Two bins with the same value\n        self.assertEqual(np.sum(result == 2.), 2)\n        # One bin with double the value\n        self.assertEqual(np.sum(result == 4.), 1)\n        self.assertEqual(result.shape, self.adef.shape)\n\n        # Test that also Xarray.DataArrays work\n        data = xr.DataArray(data)\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_sum(data)\n        # One bin with two hits, so max value is 2.0\n        self.assertTrue(np.max(result) == 4.)\n        # Two bins with the same value\n        self.assertEqual(np.sum(result == 2.), 2)\n        # One bin with double the value\n        self.assertEqual(np.sum(result == 4.), 1)\n        self.assertEqual(result.shape, self.adef.shape)\n\n        # Test masking all-NaN bins\n        data = da.from_array(np.array([[np.nan, np.nan], [np.nan, np.nan]]),\n                             chunks=self.chunks)\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_sum(data, mask_all_nan=True)\n        self.assertTrue(np.all(np.isnan(result)))\n        # By default all-NaN bins have a value of 0.0\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_sum(data)\n        self.assertEqual(np.nanmax(result), 0.0)\n\n    def test_get_count(self):\n        """"""Test drop-in-a-bucket sum.""""""\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_count()\n        result = result.compute()\n        self.assertTrue(np.max(result) == 2)\n        self.assertEqual(np.sum(result == 1), 2)\n        self.assertEqual(np.sum(result == 2), 1)\n        self.assertTrue(self.resampler.counts is not None)\n\n    def test_get_average(self):\n        """"""Test averaging bucket resampling.""""""\n        data = da.from_array(np.array([[2., 4.], [3., np.nan]]),\n                             chunks=self.chunks)\n        # Without pre-calculated indices\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_average(data)\n        result = result.compute()\n        self.assertEqual(np.nanmax(result), 3.)\n        self.assertTrue(np.any(np.isnan(result)))\n        # Use a fill value other than np.nan\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_average(data, fill_value=-1)\n        result = result.compute()\n        self.assertEqual(np.max(result), 3.)\n        self.assertEqual(np.min(result), -1)\n        self.assertFalse(np.any(np.isnan(result)))\n\n        # Test masking all-NaN bins\n        data = da.from_array(np.array([[np.nan, np.nan], [np.nan, np.nan]]),\n                             chunks=self.chunks)\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_average(data, mask_all_nan=True)\n        self.assertTrue(np.all(np.isnan(result)))\n        # By default all-NaN bins have a value of NaN\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_average(data)\n        self.assertTrue(np.all(np.isnan(result)))\n\n    def test_resample_bucket_fractions(self):\n        """"""Test fraction calculations for categorical data.""""""\n        data = da.from_array(np.array([[2, 4], [2, 2]]),\n                             chunks=self.chunks)\n        categories = [1, 2, 3, 4]\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_fractions(data, categories=categories)\n        self.assertEqual(set(categories), set(result.keys()))\n        res = result[1].compute()\n        self.assertTrue(np.nanmax(res) == 0.)\n        res = result[2].compute()\n        self.assertTrue(np.nanmax(res) == 1.)\n        self.assertTrue(np.nanmin(res) == 0.5)\n        res = result[3].compute()\n        self.assertTrue(np.nanmax(res) == 0.)\n        res = result[4].compute()\n        self.assertTrue(np.nanmax(res) == 0.5)\n        self.assertTrue(np.nanmin(res) == 0.)\n        # There should be NaN values\n        self.assertTrue(np.any(np.isnan(res)))\n\n        # Use a fill value\n        with dask.config.set(scheduler=CustomScheduler(max_computes=0)):\n            result = self.resampler.get_fractions(data, categories=categories,\n                                                  fill_value=-1)\n\n        # There should not be any NaN values\n        for i in categories:\n            res = result[i].compute()\n            self.assertFalse(np.any(np.isnan(res)))\n            self.assertTrue(np.min(res) == -1)\n\n        # No categories given, need to compute the data once to get\n        # the categories\n        with dask.config.set(scheduler=CustomScheduler(max_computes=1)):\n            result = self.resampler.get_fractions(data, categories=None)\n'"
pyresample/test/test_data_reduce.py,16,"b'# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2018  Pytroll Developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Testing the data_reduce module.""""""\n\nimport unittest\nimport numpy as np\nfrom pyresample import geometry\nfrom pyresample.data_reduce import (get_valid_index_from_cartesian_grid,\n                                    swath_from_lonlat_grid,\n                                    swath_from_lonlat_boundaries,\n                                    swath_from_cartesian_grid,\n                                    get_valid_index_from_lonlat_grid)\n\n\nclass Test(unittest.TestCase):\n\n    """"""Unit testing the data_reduce module.""""""\n\n    @classmethod\n    def setUpClass(cls):\n        """"""Get ready for testing.""""""\n        cls.area_def = geometry.AreaDefinition(\'areaD\',\n                                               \'Europe (3km, HRV, VTC)\',\n                                               \'areaD\',\n                                               {\'a\': \'6378144.0\',\n                                                \'b\': \'6356759.0\',\n                                                \'lat_0\': \'50.00\',\n                                                \'lat_ts\': \'50.00\',\n                                                \'lon_0\': \'8.00\',\n                                                \'proj\': \'stere\'},\n                                               800,\n                                               800,\n                                               [-1370912.72,\n                                                   -909968.64000000001,\n                                                   1029087.28,\n                                                   1490031.3600000001])\n\n    def test_reduce(self):\n        data = np.fromfunction(lambda y, x: (y + x), (1000, 1000))\n        lons = np.fromfunction(\n            lambda y, x: -180 + (360.0 / 1000) * x, (1000, 1000))\n        lats = np.fromfunction(\n            lambda y, x: -90 + (180.0 / 1000) * y, (1000, 1000))\n        grid_lons, grid_lats = self.area_def.get_lonlats()\n        lons, lats, data = swath_from_lonlat_grid(grid_lons, grid_lats,\n                                                  lons, lats, data,\n                                                  7000)\n        cross_sum = data.sum()\n        expected = 20685125.0\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_reduce_boundary(self):\n        data = np.fromfunction(lambda y, x: (y + x), (1000, 1000))\n        lons = np.fromfunction(\n            lambda y, x: -180 + (360.0 / 1000) * x, (1000, 1000))\n        lats = np.fromfunction(\n            lambda y, x: -90 + (180.0 / 1000) * y, (1000, 1000))\n        boundary_lonlats = self.area_def.get_boundary_lonlats()\n        lons, lats, data = swath_from_lonlat_boundaries(boundary_lonlats[0],\n                                                        boundary_lonlats[1],\n                                                        lons, lats, data, 7000)\n        cross_sum = data.sum()\n        expected = 20685125.0\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_cartesian_reduce(self):\n        data = np.fromfunction(lambda y, x: (y + x), (1000, 1000))\n        lons = np.fromfunction(\n            lambda y, x: -180 + (360.0 / 1000) * x, (1000, 1000))\n        lats = np.fromfunction(\n            lambda y, x: -90 + (180.0 / 1000) * y, (1000, 1000))\n        grid = self.area_def.get_cartesian_coords()\n        lons, lats, data = swath_from_cartesian_grid(grid, lons, lats, data,\n                                                     7000)\n        cross_sum = data.sum()\n        expected = 20685125.0\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_area_con_reduce(self):\n        data = np.fromfunction(lambda y, x: (y + x), (1000, 1000))\n        lons = np.fromfunction(\n            lambda y, x: -180 + (360.0 / 1000) * x, (1000, 1000))\n        lats = np.fromfunction(\n            lambda y, x: -90 + (180.0 / 1000) * y, (1000, 1000))\n        grid_lons, grid_lats = self.area_def.get_lonlats()\n        valid_index = get_valid_index_from_lonlat_grid(grid_lons, grid_lats,\n                                                       lons, lats, 7000)\n        data = data[valid_index]\n        cross_sum = data.sum()\n        expected = 20685125.0\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_area_con_cartesian_reduce(self):\n        data = np.fromfunction(lambda y, x: (y + x), (1000, 1000))\n        lons = np.fromfunction(\n            lambda y, x: -180 + (360.0 / 1000) * x, (1000, 1000))\n        lats = np.fromfunction(\n            lambda y, x: -90 + (180.0 / 1000) * y, (1000, 1000))\n        cart_grid = self.area_def.get_cartesian_coords()\n        valid_index = get_valid_index_from_cartesian_grid(cart_grid,\n                                                          lons, lats, 7000)\n        data = data[valid_index]\n        cross_sum = data.sum()\n        expected = 20685125.0\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_reduce_north_pole(self):\n        """"""Test reducing around the poles.""""""\n\n        from pyresample import utils\n        area_id = \'ease_sh\'\n        description = \'Antarctic EASE grid\'\n        proj_id = \'ease_sh\'\n        projection = \'+proj=laea +lat_0=-90 +lon_0=0 +a=6371228.0 +units=m\'\n        x_size = 425\n        y_size = 425\n        area_extent = (-5326849.0625, -5326849.0625,\n                       5326849.0625, 5326849.0625)\n        area_def = utils.get_area_def(area_id, description, proj_id,\n                                      projection, x_size, y_size, area_extent)\n\n        grid_lons, grid_lats = area_def.get_lonlats()\n\n        area_id = \'ease_sh\'\n        description = \'Antarctic EASE grid\'\n        proj_id = \'ease_sh\'\n        projection = \'+proj=laea +lat_0=-90 +lon_0=0 +a=6371228.0 +units=m\'\n        x_size = 1000\n        y_size = 1000\n        area_extent = (-532684.0625, -532684.0625, 532684.0625, 532684.0625)\n        smaller_area_def = utils.get_area_def(area_id, description, proj_id,\n                                              projection, x_size, y_size,\n                                              area_extent)\n\n        data = np.fromfunction(lambda y, x: (y + x), (1000, 1000))\n        lons, lats = smaller_area_def.get_lonlats()\n\n        lons, lats, data = swath_from_lonlat_grid(grid_lons, grid_lats,\n                                                  lons, lats, data, 7000)\n\n        cross_sum = data.sum()\n        expected = 999000000.0\n        self.assertAlmostEqual(cross_sum, expected)\n'"
pyresample/test/test_ewa_fornav.py,44,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2016-2019\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Test EWA fornav module.""""""\nimport logging\nimport numpy as np\nimport unittest\n\nLOG = logging.getLogger(__name__)\n\n\nclass TestFornav(unittest.TestCase):\n    """"""Test fornav resampling functions.""""""\n\n    def test_fornav_swath_larger(self):\n        """"""Test that a swath larger than the output grid fills the entire grid.""""""\n        from pyresample.ewa import _fornav\n        swath_shape = (1600, 3200)\n        data_type = np.float32\n        # Create a fake row and cols array\n        rows = np.empty(swath_shape, dtype=np.float32)\n        rows[:] = np.linspace(-500, 2500, 1600)[:, None]\n        cols = np.empty(swath_shape, dtype=np.float32)\n        cols[:] = np.linspace(-2500, 1500, 3200)\n        rows_per_scan = 16\n        # Create a fake data swath\n        data = np.ones(swath_shape, dtype=data_type)\n        out = np.empty((1000, 1000), dtype=data_type)\n\n        grid_points_covered = _fornav.fornav_wrapper(cols, rows, (data,), (out,),\n                                                     np.nan, np.nan, rows_per_scan)\n        one_grid_points_covered = grid_points_covered[0]\n        # The swath was larger than the grid, all of the grid should have\n        # been covered by swath pixels\n        self.assertEqual(one_grid_points_covered, out.size,\n                         msg=""Not all grid pixels were filled"")\n        # The swath was all 1s so there shouldn\'t be any non-1 values in the\n        # output except outside the swath\n        self.assertTrue(((out == 1) | np.isnan(out)).all(),\n                         msg=""Unexpected interpolation values were returned"")\n\n    def test_fornav_swath_smaller(self):\n        """"""Test that a swath smaller than the output grid is entirely used.""""""\n        from pyresample.ewa import _fornav\n        swath_shape = (1600, 3200)\n        data_type = np.float32\n        # Create a fake row and cols array\n        rows = np.empty(swath_shape, dtype=np.float32)\n        rows[:] = np.linspace(500, 800, 1600)[:, None]\n        cols = np.empty(swath_shape, dtype=np.float32)\n        cols[:] = np.linspace(200, 600, 3200)\n        rows_per_scan = 16\n        # Create a fake data swath\n        data = np.ones(swath_shape, dtype=data_type)\n        out = np.empty((1000, 1000), dtype=data_type)\n\n        grid_points_covered = _fornav.fornav_wrapper(cols, rows, (data,), (out,),\n                                                     np.nan, np.nan, rows_per_scan)\n        one_grid_points_covered = grid_points_covered[0]\n        # The swath was smaller than the grid, make sure its whole area\n        # was covered (percentage of grid rows/cols to overall size)\n        self.assertAlmostEqual(one_grid_points_covered / float(out.size), 0.12, 2,\n                               msg=""Not all input swath pixels were used"")\n        # The swath was all 1s so there shouldn\'t be any non-1 values in the\n        # output except outside the swath\n        self.assertTrue(((out == 1) | np.isnan(out)).all(),\n                        msg=""Unexpected interpolation values were returned"")\n\n    def test_fornav_swath_smaller_int8(self):\n        """"""Test that a swath smaller than the output grid is entirely used.""""""\n        from pyresample.ewa import _fornav\n        swath_shape = (1600, 3200)\n        data_type = np.int8\n        # Create a fake row and cols array\n        rows = np.empty(swath_shape, dtype=np.float32)\n        rows[:] = np.linspace(500, 800, 1600)[:, None]\n        cols = np.empty(swath_shape, dtype=np.float32)\n        cols[:] = np.linspace(200, 600, 3200)\n        rows_per_scan = 16\n        # Create a fake data swath\n        data = np.ones(swath_shape, dtype=data_type)\n        out = np.empty((1000, 1000), dtype=data_type)\n\n        grid_points_covered = _fornav.fornav_wrapper(cols, rows, (data,), (out,),\n                                                     -128, -128, rows_per_scan)\n        one_grid_points_covered = grid_points_covered[0]\n        # The swath was smaller than the grid, make sure its whole area\n        # was covered (percentage of grid rows/cols to overall size)\n        self.assertAlmostEqual(one_grid_points_covered / float(out.size), 0.12, 2,\n                               msg=""Not all input swath pixels were used"")\n        # The swath was all 1s so there shouldn\'t be any non-1 values in the\n        # output except outside the swath\n        # import ipdb; ipdb.set_trace()\n        self.assertTrue(((out == 1) | (out == -128)).all(),\n                        msg=""Unexpected interpolation values were returned"")\n\n    def test_fornav_swath_one_scan_geo_nans(self):\n        """"""Test that a swath treated as one large scan with NaNs in geolocation still succeeds.""""""\n        from pyresample.ewa import _fornav\n        swath_shape = (1600, 3200)\n        data_type = np.float32\n        # Create a fake row and cols array\n        rows = np.empty(swath_shape, dtype=np.float32)\n        rows[:] = np.linspace(500, 800, 1600)[:, None]\n        cols = np.empty(swath_shape, dtype=np.float32)\n        cols[:] = np.linspace(200, 600, 3200)\n        rows[:10, :] = np.nan\n        cols[:10, :] = np.nan\n        rows_per_scan = rows.shape[0]\n        # Create a fake data swath\n        data = np.ones(swath_shape, dtype=data_type)\n        out = np.empty((1000, 1000), dtype=data_type)\n\n        grid_points_covered = _fornav.fornav_wrapper(cols, rows, (data,), (out,),\n                                                     np.nan, np.nan, rows_per_scan)\n        one_grid_points_covered = grid_points_covered[0]\n        # The swath was smaller than the grid, make sure its whole area\n        # was covered (percentage of grid rows/cols to overall size)\n        self.assertAlmostEqual(one_grid_points_covered / float(out.size), 0.12, 2,\n                               msg=""Not all input swath pixels were used"")\n        # The swath was all 1s so there shouldn\'t be any non-1 values in the\n        # output except outside the swath\n        self.assertTrue(((out == 1) | np.isnan(out)).all(),\n                        msg=""Unexpected interpolation values were returned"")\n\n\nclass TestFornavWrapper(unittest.TestCase):\n    """"""Test the function wrapping the lower-level fornav code.""""""\n\n    def test_fornav_swath_larger_float32(self):\n        """"""Test that a swath larger than the output grid fills the entire grid.\n        """"""\n        from pyresample.ewa import fornav\n        swath_shape = (1600, 3200)\n        data_type = np.float32\n        # Create a fake row and cols array\n        rows = np.empty(swath_shape, dtype=np.float32)\n        rows[:] = np.linspace(-500, 2500, 1600)[:, None]\n        cols = np.empty(swath_shape, dtype=np.float32)\n        cols[:] = np.linspace(-2500, 1500, 3200)\n        # Create a fake data swath\n        data = np.ones(swath_shape, dtype=data_type)\n        out = np.empty((1000, 1000), dtype=data_type)\n        # area can be None because `out` is specified\n        area = None\n\n        grid_points_covered, out_res = fornav(cols, rows, area, data,\n                                              rows_per_scan=16, out=out)\n        self.assertIs(out, out_res)\n        # The swath was larger than the grid, all of the grid should have\n        # been covered by swath pixels\n        self.assertEqual(grid_points_covered, out.size,\n                         msg=""Not all grid pixels were filled"")\n        # The swath was all 1s so there shouldn\'t be any non-1 values in the\n        # output except outside the swath\n        self.assertTrue(((out == 1) | np.isnan(out)).all(),\n                        msg=""Unexpected interpolation values were returned"")\n'"
pyresample/test/test_ewa_ll2cr.py,19,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2016\n\n# Author(s):\n\n#   David Hoese <david.hoese@ssec.wisc.edu>\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Test the EWA ll2cr code.\n""""""\nimport logging\nimport numpy as np\nfrom pyresample.test.utils import create_test_longitude, create_test_latitude\nimport unittest\n\nLOG = logging.getLogger(__name__)\n\n\ndynamic_wgs84 = {\n    ""grid_name"": ""test_wgs84_fit"",\n    ""origin_x"": None,\n    ""origin_y"": None,\n    ""width"": None,\n    ""height"": None,\n    ""cell_width"": 0.0057,\n    ""cell_height"": -0.0057,\n    ""proj4_definition"": ""+proj=latlong +datum=WGS84 +ellps=WGS84 +no_defs"",\n}\n\nstatic_lcc = {\n    ""grid_name"": ""test_lcc"",\n    ""origin_x"": -1950510.636800,\n    ""origin_y"": 4368587.226913,\n    ""width"": 5120,\n    ""height"": 5120,\n    ""cell_width"": 1015.9,\n    ""cell_height"": -1015.9,\n    ""proj4_definition"": ""+proj=lcc +a=6371200 +b=6371200 +lat_0=25 +lat_1=25 +lon_0=-95 +units=m +no_defs"",\n}\n\n\nclass TestLL2CRStatic(unittest.TestCase):\n    def test_lcc_basic1(self):\n        from pyresample.ewa import _ll2cr\n        lon_arr = create_test_longitude(-95.0, -75.0, (50, 100), dtype=np.float64)\n        lat_arr = create_test_latitude(18.0, 40.0, (50, 100), dtype=np.float64)\n        grid_info = static_lcc.copy()\n        fill_in = np.nan\n        proj_str = grid_info[""proj4_definition""]\n        cw = grid_info[""cell_width""]\n        ch = grid_info[""cell_height""]\n        ox = grid_info[""origin_x""]\n        oy = grid_info[""origin_y""]\n        w = grid_info[""width""]\n        h = grid_info[""height""]\n        points_in_grid = _ll2cr.ll2cr_static(lon_arr, lat_arr, fill_in, proj_str,\n                                                               cw, ch, w, h, ox, oy)\n        self.assertEqual(points_in_grid, lon_arr.size, ""all these test points should fall in this grid"")\n\n    def test_lcc_fail1(self):\n        from pyresample.ewa import _ll2cr\n        lon_arr = create_test_longitude(-15.0, 15.0, (50, 100), dtype=np.float64)\n        lat_arr = create_test_latitude(18.0, 40.0, (50, 100), dtype=np.float64)\n        grid_info = static_lcc.copy()\n        fill_in = np.nan\n        proj_str = grid_info[""proj4_definition""]\n        cw = grid_info[""cell_width""]\n        ch = grid_info[""cell_height""]\n        ox = grid_info[""origin_x""]\n        oy = grid_info[""origin_y""]\n        w = grid_info[""width""]\n        h = grid_info[""height""]\n        points_in_grid = _ll2cr.ll2cr_static(lon_arr, lat_arr, fill_in, proj_str,\n                                             cw, ch, w, h, ox, oy)\n        self.assertEqual(points_in_grid, 0, ""none of these test points should fall in this grid"")\n\n\nclass TestLL2CRDynamic(unittest.TestCase):\n    def test_latlong_basic1(self):\n        from pyresample.ewa import _ll2cr\n        lon_arr = create_test_longitude(-95.0, -75.0, (50, 100), dtype=np.float64)\n        lat_arr = create_test_latitude(15.0, 30.0, (50, 100), dtype=np.float64)\n        grid_info = dynamic_wgs84.copy()\n        fill_in = np.nan\n        proj_str = grid_info[""proj4_definition""]\n        cw = grid_info[""cell_width""]\n        ch = grid_info[""cell_height""]\n        ox = grid_info[""origin_x""]\n        oy = grid_info[""origin_y""]\n        w = grid_info[""width""]\n        h = grid_info[""height""]\n        points_in_grid, lon_res, lat_res, ox, oy, w, h = _ll2cr.ll2cr_dynamic(lon_arr, lat_arr, fill_in, proj_str,\n                                                                              cw, ch, w, h, ox, oy)\n        self.assertEqual(points_in_grid, lon_arr.size, ""all points should be contained in a dynamic grid"")\n        self.assertIs(lon_arr, lon_res)\n        self.assertIs(lat_arr, lat_res)\n        self.assertEqual(lon_arr[0, 0], 0, ""ll2cr returned the wrong result for a dynamic latlong grid"")\n        self.assertEqual(lat_arr[-1, 0], 0, ""ll2cr returned the wrong result for a dynamic latlong grid"")\n\n    def test_latlong_basic2(self):\n        from pyresample.ewa import _ll2cr\n        lon_arr = create_test_longitude(-95.0, -75.0, (50, 100), twist_factor=0.6, dtype=np.float64)\n        lat_arr = create_test_latitude(15.0, 30.0, (50, 100), twist_factor=-0.1, dtype=np.float64)\n        grid_info = dynamic_wgs84.copy()\n        fill_in = np.nan\n        proj_str = grid_info[""proj4_definition""]\n        cw = grid_info[""cell_width""]\n        ch = grid_info[""cell_height""]\n        ox = grid_info[""origin_x""]\n        oy = grid_info[""origin_y""]\n        w = grid_info[""width""]\n        h = grid_info[""height""]\n        points_in_grid, lon_res, lat_res, ox, oy, w, h = _ll2cr.ll2cr_dynamic(lon_arr, lat_arr, fill_in, proj_str,\n                                                                              cw, ch, w, h, ox, oy)\n        self.assertEqual(points_in_grid, lon_arr.size, ""all points should be contained in a dynamic grid"")\n        self.assertIs(lon_arr, lon_res)\n        self.assertIs(lat_arr, lat_res)\n        self.assertEqual(lon_arr[0, 0], 0, ""ll2cr returned the wrong result for a dynamic latlong grid"")\n        self.assertEqual(lat_arr[-1, 0], 0, ""ll2cr returned the wrong result for a dynamic latlong grid"")\n\n    def test_latlong_dateline1(self):\n        from pyresample.ewa import _ll2cr\n        lon_arr = create_test_longitude(165.0, -165.0, (50, 100), twist_factor=0.6, dtype=np.float64)\n        lat_arr = create_test_latitude(15.0, 30.0, (50, 100), twist_factor=-0.1, dtype=np.float64)\n        grid_info = dynamic_wgs84.copy()\n        fill_in = np.nan\n        proj_str = grid_info[""proj4_definition""]\n        cw = grid_info[""cell_width""]\n        ch = grid_info[""cell_height""]\n        ox = grid_info[""origin_x""]\n        oy = grid_info[""origin_y""]\n        w = grid_info[""width""]\n        h = grid_info[""height""]\n        points_in_grid, lon_res, lat_res, ox, oy, w, h = _ll2cr.ll2cr_dynamic(lon_arr, lat_arr, fill_in, proj_str,\n                                                                              cw, ch, w, h, ox, oy)\n        self.assertEqual(points_in_grid, lon_arr.size, ""all points should be contained in a dynamic grid"")\n        self.assertIs(lon_arr, lon_res)\n        self.assertIs(lat_arr, lat_res)\n        self.assertEqual(lon_arr[0, 0], 0, ""ll2cr returned the wrong result for a dynamic latlong grid"")\n        self.assertEqual(lat_arr[-1, 0], 0, ""ll2cr returned the wrong result for a dynamic latlong grid"")\n        self.assertTrue(np.all(np.diff(lon_arr[0]) >= 0), ""ll2cr didn\'t return monotonic columns over the dateline"")\n\n\nclass TestLL2CRWrapper(unittest.TestCase):\n    def test_basic1(self):\n        from pyresample.ewa import ll2cr\n        from pyresample.geometry import SwathDefinition, AreaDefinition\n        from pyresample.utils import proj4_str_to_dict\n        lon_arr = create_test_longitude(-95.0, -75.0, (50, 100), dtype=np.float64)\n        lat_arr = create_test_latitude(18.0, 40.0, (50, 100), dtype=np.float64)\n        swath_def = SwathDefinition(lon_arr, lat_arr)\n        grid_info = static_lcc.copy()\n        cw = grid_info[""cell_width""]\n        ch = grid_info[""cell_height""]\n        ox = grid_info[""origin_x""]\n        oy = grid_info[""origin_y""]\n        w = grid_info[""width""]\n        h = grid_info[""height""]\n        half_w = abs(cw / 2.)\n        half_h = abs(ch / 2.)\n        extents = [\n            ox - half_w, oy - h * abs(ch) - half_h,\n            ox + w * abs(cw) + half_w, oy + half_h\n        ]\n        area = AreaDefinition(\'test_area\', \'test_area\', \'test_area\',\n                              proj4_str_to_dict(grid_info[\'proj4_definition\']),\n                              w, h, extents)\n        points_in_grid, lon_res, lat_res, = ll2cr(swath_def, area,\n                                                  fill=np.nan, copy=False)\n        self.assertEqual(points_in_grid, lon_arr.size, ""all points should be contained in a dynamic grid"")\n        self.assertIs(lon_arr, lon_res)\n        self.assertIs(lat_arr, lat_res)\n        self.assertEqual(points_in_grid, lon_arr.size, ""all these test points should fall in this grid"")\n'"
pyresample/test/test_geometry.py,211,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2010-2016\n#\n# Authors:\n#    Esben S. Nielsen\n#    Thomas Lavergne\n#    Adam Dybbroe\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Test the geometry objects.""""""\nimport random\nimport sys\n\nimport numpy as np\n\nfrom pyresample import geo_filter, geometry, parse_area_file\nfrom pyresample.geometry import (IncompatibleAreas,\n                                 combine_area_extents_vertical,\n                                 concatenate_area_defs)\nfrom pyresample.test.utils import catch_warnings\n\nfrom unittest.mock import MagicMock, patch\nimport unittest\n\ntry:\n    from pyproj import CRS\nexcept ImportError:\n    CRS = None\n\n\nclass Test(unittest.TestCase):\n    """"""Unit testing the geometry and geo_filter modules.""""""\n\n    def test_lonlat_precomp(self):\n        """"""Test the lonlat precomputation.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n        lons, lats = area_def.get_lonlats()\n        lon, lat = area_def.get_lonlat(400, 400)\n        self.assertAlmostEqual(lon, 5.5028467120975835,\n                               msg=\'lon retrieval from precomputated grid failed\')\n        self.assertAlmostEqual(lat, 52.566998432390619,\n                               msg=\'lat retrieval from precomputated grid failed\')\n\n    def test_cartesian(self):\n        """"""Test getting the cartesian coordinates.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n        cart_coords = area_def.get_cartesian_coords()\n        exp = 5872039989466.8457031\n        self.assertTrue((cart_coords.sum() - exp) < 1e-7 * exp,\n                        msg=\'Calculation of cartesian coordinates failed\')\n\n    def test_cartopy_crs(self):\n        """"""Test conversion from area definition to cartopy crs.""""""\n        from pyresample import utils\n\n        europe = geometry.AreaDefinition(area_id=\'areaD\',\n                                         description=\'Europe (3km, HRV, VTC)\',\n                                         proj_id=\'areaD\',\n                                         projection={\'a\': \'6378144.0\',\n                                                     \'b\': \'6356759.0\',\n                                                     \'lat_0\': \'50.00\',\n                                                     \'lat_ts\': \'50.00\',\n                                                     \'lon_0\': \'8.00\',\n                                                     \'proj\': \'stere\'},\n                                         width=800, height=800,\n                                         area_extent=[-1370912.72,\n                                                      -909968.64000000001,\n                                                      1029087.28,\n                                                      1490031.3600000001])\n        seviri = geometry.AreaDefinition(area_id=\'seviri\',\n                                         description=\'SEVIRI HRIT like (flipped, south up)\',\n                                         proj_id=\'seviri\',\n                                         projection={\'proj\': \'geos\',\n                                                     \'lon_0\': 0.0,\n                                                     \'a\': 6378169.00,\n                                                     \'b\': 6356583.80,\n                                                     \'h\': 35785831.00,\n                                                     \'units\': \'m\'},\n                                         width=123, height=123,\n                                         area_extent=[5500000, 5500000, -5500000, -5500000])\n\n        for area_def in [europe, seviri]:\n            crs = area_def.to_cartopy_crs()\n\n            # Bounds\n            self.assertEqual(crs.bounds,\n                             (area_def.area_extent[0],\n                              area_def.area_extent[2],\n                              area_def.area_extent[1],\n                              area_def.area_extent[3]))\n\n            # Threshold\n            thresh_exp = min(np.fabs(area_def.area_extent[2] - area_def.area_extent[0]),\n                             np.fabs(area_def.area_extent[3] - area_def.area_extent[1])) / 100.\n            self.assertEqual(crs.threshold, thresh_exp)\n\n        # EPSG projection\n        projections = [\'+init=EPSG:6932\']\n        if utils.is_pyproj2():\n            projections.append(\'EPSG:6932\')\n\n        for projection in projections:\n            area = geometry.AreaDefinition(\n                area_id=\'ease-sh-2.0\',\n                description=\'25km EASE Grid 2.0 (Southern Hemisphere)\',\n                proj_id=\'ease-sh-2.0\',\n                projection=projection,\n                width=123, height=123,\n                area_extent=[-40000., -40000., 40000., 40000.])\n            with patch(\'pyresample.utils.cartopy.warnings.warn\') as warn:\n                # Test that user warning has been issued (EPSG to proj4 string is potentially lossy)\n                area.to_cartopy_crs()\n                if projection.startswith(\'EPSG\'):\n                    # we\'ll only get this for the new EPSG:XXXX syntax\n                    warn.assert_called()\n\n        # Bounds for latlong projection must be specified in radians\n        latlong_crs = geometry.AreaDefinition(area_id=\'latlong\',\n                                              description=\'Global regular lat-lon grid\',\n                                              proj_id=\'latlong\',\n                                              projection={\'proj\': \'latlong\', \'lon0\': 0},\n                                              width=360,\n                                              height=180,\n                                              area_extent=(-180, -90, 180, 90)).to_cartopy_crs()\n        self.assertTrue(np.allclose(latlong_crs.bounds, [-np.pi, np.pi, -np.pi/2, np.pi/2]))\n\n    def test_create_areas_def(self):\n        """"""Test exporting area defs.""""""\n        from pyresample import utils\n        import yaml\n\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\',\n                                           \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'90.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                            -909968.64000000001,\n                                            1029087.28,\n                                            1490031.3600000001])\n        res = yaml.safe_load(area_def.create_areas_def())\n        expected = yaml.safe_load((\'areaD:\\n  description: Europe (3km, HRV, VTC)\\n\'\n                                   \'  projection:\\n    a: 6378144.0\\n    b: 6356759.0\\n\'\n                                   \'    lat_0: 90.0\\n    lat_ts: 50.0\\n    lon_0: 8.0\\n\'\n                                   \'    proj: stere\\n  shape:\\n    height: 800\\n\'\n                                   \'    width: 800\\n  area_extent:\\n\'\n                                   \'    lower_left_xy: [-1370912.72, -909968.64]\\n\'\n                                   \'    upper_right_xy: [1029087.28, 1490031.36]\\n\'))\n\n        self.assertEqual(set(res.keys()), set(expected.keys()))\n        res = res[\'areaD\']\n        expected = expected[\'areaD\']\n        self.assertEqual(set(res.keys()), set(expected.keys()))\n        self.assertEqual(res[\'description\'], expected[\'description\'])\n        self.assertEqual(res[\'shape\'], expected[\'shape\'])\n        self.assertEqual(res[\'area_extent\'][\'lower_left_xy\'],\n                         expected[\'area_extent\'][\'lower_left_xy\'])\n        # pyproj versions may effect how the PROJ is formatted\n        for proj_key in [\'a\', \'lat_0\', \'lon_0\', \'proj\', \'lat_ts\']:\n            self.assertEqual(res[\'projection\'][proj_key],\n                             expected[\'projection\'][proj_key])\n\n        # EPSG\n        projections = {\'+init=epsg:3006\': \'init: epsg:3006\'}\n        if utils.is_pyproj2():\n            projections[\'EPSG:3006\'] = \'EPSG: 3006\'\n\n        for projection, epsg_yaml in projections.items():\n            area_def = geometry.AreaDefinition(\'baws300_sweref99tm\', \'BAWS, 300m resolution, sweref99tm\',\n                                               \'sweref99tm\',\n                                               projection,\n                                               4667,\n                                               4667,\n                                               [-49739, 5954123, 1350361, 7354223])\n            res = yaml.safe_load(area_def.create_areas_def())\n            expected = yaml.safe_load((\'baws300_sweref99tm:\\n\'\n                                       \'  description: BAWS, 300m resolution, sweref99tm\\n\'\n                                       \'  projection:\\n\'\n                                       \'    {epsg}\\n\'\n                                       \'  shape:\\n\'\n                                       \'    height: 4667\\n\'\n                                       \'    width: 4667\\n\'\n                                       \'  area_extent:\\n\'\n                                       \'    lower_left_xy: [-49739, 5954123]\\n\'\n                                       \'    upper_right_xy: [1350361, 7354223]\'.format(epsg=epsg_yaml)))\n        self.assertDictEqual(res, expected)\n\n    def test_parse_area_file(self):\n        """"""Test parsing the are file.""""""\n        from pyresample import utils\n\n        expected = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\',\n                                           \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                            -909968.64000000001,\n                                            1029087.28,\n                                            1490031.3600000001])\n        yaml_str = (\'areaD:\\n  description: Europe (3km, HRV, VTC)\\n\'\n                    \'  projection:\\n    a: 6378144.0\\n    b: 6356759.0\\n\'\n                    \'    lat_0: 50.0\\n    lat_ts: 50.0\\n    lon_0: 8.0\\n\'\n                    \'    proj: stere\\n  shape:\\n    height: 800\\n\'\n                    \'    width: 800\\n  area_extent:\\n\'\n                    \'    lower_left_xy: [-1370912.72, -909968.64]\\n\'\n                    \'    upper_right_xy: [1029087.28, 1490031.36]\\n\')\n        area_def = parse_area_file(yaml_str, \'areaD\')[0]\n        self.assertEqual(area_def, expected)\n\n        # EPSG\n        projections = {\'+init=epsg:3006\': \'init: epsg:3006\'}\n        if utils.is_pyproj2():\n            projections[\'EPSG:3006\'] = \'EPSG: 3006\'\n        for projection, epsg_yaml in projections.items():\n            expected = geometry.AreaDefinition(\'baws300_sweref99tm\', \'BAWS, 300m resolution, sweref99tm\',\n                                               \'sweref99tm\',\n                                               projection,\n                                               4667,\n                                               4667,\n                                               [-49739, 5954123, 1350361, 7354223])\n            yaml_str = (\'baws300_sweref99tm:\\n\'\n                        \'  description: BAWS, 300m resolution, sweref99tm\\n\'\n                        \'  projection:\\n\'\n                        \'    {epsg}\\n\'\n                        \'  shape:\\n\'\n                        \'    height: 4667\\n\'\n                        \'    width: 4667\\n\'\n                        \'  area_extent:\\n\'\n                        \'    lower_left_xy: [-49739, 5954123]\\n\'\n                        \'    upper_right_xy: [1350361, 7354223]\'.format(epsg=epsg_yaml))\n            area_def = parse_area_file(yaml_str, \'baws300_sweref99tm\')[0]\n            self.assertEqual(area_def, expected)\n\n    def test_base_type(self):\n        """"""Test the base type.""""""\n        lons1 = np.arange(-135., +135, 50.)\n        lats = np.ones_like(lons1) * 70.\n\n        # Test dtype is preserved without longitude wrapping\n        basedef = geometry.BaseDefinition(lons1, lats)\n        lons, _ = basedef.get_lonlats()\n        self.assertEqual(lons.dtype, lons1.dtype,\n                         ""BaseDefinition did not maintain dtype of longitudes (in:%s out:%s)"" %\n                         (lons1.dtype, lons.dtype,))\n\n        lons1_ints = lons1.astype(\'int\')\n        basedef = geometry.BaseDefinition(lons1_ints, lats)\n        lons, _ = basedef.get_lonlats()\n        self.assertEqual(lons.dtype, lons1_ints.dtype,\n                         ""BaseDefinition did not maintain dtype of longitudes (in:%s out:%s)"" %\n                         (lons1_ints.dtype, lons.dtype,))\n\n        # Test dtype is preserved with automatic longitude wrapping\n        lons2 = np.where(lons1 < 0, lons1 + 360, lons1)\n        with catch_warnings():\n            basedef = geometry.BaseDefinition(lons2, lats)\n\n        lons, _ = basedef.get_lonlats()\n        self.assertEqual(lons.dtype, lons2.dtype,\n                         ""BaseDefinition did not maintain dtype of longitudes (in:%s out:%s)"" %\n                         (lons2.dtype, lons.dtype,))\n\n        lons2_ints = lons2.astype(\'int\')\n        with catch_warnings():\n            basedef = geometry.BaseDefinition(lons2_ints, lats)\n\n        lons, _ = basedef.get_lonlats()\n        self.assertEqual(lons.dtype, lons2_ints.dtype,\n                         ""BaseDefinition did not maintain dtype of longitudes (in:%s out:%s)"" %\n                         (lons2_ints.dtype, lons.dtype,))\n\n    def test_area_hash(self):\n        """"""Test the area hash.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n\n        self.assertIsInstance(hash(area_def), int)\n\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'lat_0\': \'50.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n\n        self.assertIsInstance(hash(area_def), int)\n\n        area_def = geometry.AreaDefinition(\'New area\', \'Europe\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'lat_0\': \'50.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n\n        self.assertIsInstance(hash(area_def), int)\n\n    def test_get_array_hashable(self):\n        """"""Test making the array hashable.""""""\n        arr = np.array([1.2, 1.3, 1.4, 1.5])\n        if sys.byteorder == \'little\':\n            # arr.view(np.uint8)\n            reference = np.array([51,  51,  51,  51,  51,  51, 243,\n                                  63, 205, 204, 204, 204, 204,\n                                  204, 244,  63, 102, 102, 102, 102,\n                                  102, 102, 246,  63,   0,   0,\n                                  0,   0,   0,   0, 248,  63],\n                                 dtype=np.uint8)\n        else:\n            # on le machines use arr.byteswap().view(np.uint8)\n            reference = np.array([63, 243,  51,  51,  51,  51,  51,\n                                  51,  63, 244, 204, 204, 204,\n                                  204, 204, 205,  63, 246, 102, 102,\n                                  102, 102, 102, 102,  63, 248,\n                                  0,   0,   0,   0,   0,   0],\n                                 dtype=np.uint8)\n\n        np.testing.assert_allclose(reference,\n                                   geometry.get_array_hashable(arr))\n\n        try:\n            import xarray as xr\n        except ImportError:\n            pass\n        else:\n            xrarr = xr.DataArray(arr)\n            np.testing.assert_allclose(reference,\n                                       geometry.get_array_hashable(arr))\n\n            xrarr.attrs[\'hash\'] = 42\n            self.assertEqual(geometry.get_array_hashable(xrarr),\n                             xrarr.attrs[\'hash\'])\n\n    def test_swath_hash(self):\n        """"""Test swath hash.""""""\n        lons = np.array([1.2, 1.3, 1.4, 1.5])\n        lats = np.array([65.9, 65.86, 65.82, 65.78])\n        swath_def = geometry.SwathDefinition(lons, lats)\n\n        self.assertIsInstance(hash(swath_def), int)\n\n        try:\n            import dask.array as da\n        except ImportError:\n            print(""Not testing with dask arrays"")\n        else:\n            dalons = da.from_array(lons, chunks=1000)\n            dalats = da.from_array(lats, chunks=1000)\n            swath_def = geometry.SwathDefinition(dalons, dalats)\n\n            self.assertIsInstance(hash(swath_def), int)\n\n        try:\n            import xarray as xr\n        except ImportError:\n            print(""Not testing with xarray"")\n        else:\n            xrlons = xr.DataArray(lons)\n            xrlats = xr.DataArray(lats)\n            swath_def = geometry.SwathDefinition(xrlons, xrlats)\n\n            self.assertIsInstance(hash(swath_def), int)\n\n        try:\n            import xarray as xr\n            import dask.array as da\n        except ImportError:\n            print(""Not testing with xarrays and dask arrays"")\n        else:\n            xrlons = xr.DataArray(da.from_array(lons, chunks=1000))\n            xrlats = xr.DataArray(da.from_array(lats, chunks=1000))\n            swath_def = geometry.SwathDefinition(xrlons, xrlats)\n\n            self.assertIsInstance(hash(swath_def), int)\n\n        lons = np.ma.array([1.2, 1.3, 1.4, 1.5])\n        lats = np.ma.array([65.9, 65.86, 65.82, 65.78])\n        swath_def = geometry.SwathDefinition(lons, lats)\n\n        self.assertIsInstance(hash(swath_def), int)\n\n    def test_area_equal(self):\n        """"""Test areas equality.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n        area_def2 = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                            {\'a\': \'6378144.0\',\n                                             \'b\': \'6356759.0\',\n                                             \'lat_0\': \'50.00\',\n                                             \'lat_ts\': \'50.00\',\n                                             \'lon_0\': \'8.00\',\n                                             \'proj\': \'stere\'},\n                                            800,\n                                            800,\n                                            [-1370912.72,\n                                                -909968.64000000001,\n                                                1029087.28,\n                                                1490031.3600000001])\n        self.assertFalse(\n            area_def != area_def2, \'area_defs are not equal as expected\')\n\n    def test_not_area_equal(self):\n        """"""Test areas inequality.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n\n        msg_area = geometry.AreaDefinition(\'msg_full\', \'Full globe MSG image 0 degrees\',\n                                           \'msg_full\',\n                                           {\'a\': \'6378169.0\',\n                                            \'b\': \'6356584.0\',\n                                            \'h\': \'35785831.0\',\n                                            \'lon_0\': \'0\',\n                                            \'proj\': \'geos\'},\n                                           3712,\n                                           3712,\n                                           [-5568742.4000000004,\n                                               -5568742.4000000004,\n                                               5568742.4000000004,\n                                               5568742.4000000004]\n                                           )\n        self.assertFalse(\n            area_def == msg_area, \'area_defs are not expected to be equal\')\n\n    def test_swath_equal_area(self):\n        """"""Test equality swath area.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n\n        swath_def = geometry.SwathDefinition(*area_def.get_lonlats())\n\n        self.assertFalse(\n            swath_def != area_def, ""swath_def and area_def should be equal"")\n\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n\n        self.assertFalse(\n            area_def != swath_def, ""swath_def and area_def should be equal"")\n\n    def test_swath_not_equal_area(self):\n        """"""Test inequality swath area.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n\n        lons = np.array([1.2, 1.3, 1.4, 1.5])\n        lats = np.array([65.9, 65.86, 65.82, 65.78])\n        swath_def = geometry.SwathDefinition(lons, lats)\n\n        self.assertFalse(\n            swath_def == area_def, ""swath_def and area_def should be different"")\n\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n\n        self.assertFalse(\n            area_def == swath_def, ""swath_def and area_def should be different"")\n\n    def test_grid_filter_valid(self):\n        """"""Test valid grid filtering.""""""\n        lons = np.array([-170, -30, 30, 170])\n        lats = np.array([20, -40, 50, -80])\n        swath_def = geometry.SwathDefinition(lons, lats)\n        filter_area = geometry.AreaDefinition(\'test\', \'test\', \'test\',\n                                              {\'proj\': \'eqc\', \'lon_0\': 0.0,\n                                                  \'lat_0\': 0.0},\n                                              8, 8,\n                                              (-20037508.34, -10018754.17, 20037508.34, 10018754.17))\n        filter = np.array([[1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           ])\n        grid_filter = geo_filter.GridFilter(filter_area, filter)\n        valid_index = grid_filter.get_valid_index(swath_def)\n        expected = np.array([1, 0, 0, 1])\n        self.assertTrue(\n            np.array_equal(valid_index, expected), \'Failed to find grid filter\')\n\n    def test_grid_filter(self):\n        """"""Test filtering a grid.""""""\n        lons = np.array([-170, -30, 30, 170])\n        lats = np.array([20, -40, 50, -80])\n        swath_def = geometry.SwathDefinition(lons, lats)\n        data = np.array([1, 2, 3, 4])\n        filter_area = geometry.AreaDefinition(\'test\', \'test\', \'test\',\n                                              {\'proj\': \'eqc\', \'lon_0\': 0.0,\n                                                  \'lat_0\': 0.0},\n                                              8, 8,\n                                              (-20037508.34, -10018754.17, 20037508.34, 10018754.17))\n        filter = np.array([[1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           ])\n        grid_filter = geo_filter.GridFilter(filter_area, filter)\n        swath_def_f, data_f = grid_filter.filter(swath_def, data)\n        expected = np.array([1, 4])\n        self.assertTrue(\n            np.array_equal(data_f, expected), \'Failed grid filtering data\')\n        expected_lons = np.array([-170, 170])\n        expected_lats = np.array([20, -80])\n        self.assertTrue(np.array_equal(swath_def_f.lons[:], expected_lons)\n                        and np.array_equal(swath_def_f.lats[:], expected_lats),\n                        \'Failed finding grid filtering lon lats\')\n\n    def test_grid_filter2D(self):\n        """"""Test filtering a 2D grid.""""""\n        lons = np.array([[-170, -30, 30, 170],\n                         [-170, -30, 30, 170]])\n        lats = np.array([[20, -40, 50, -80],\n                         [25, -35, 55, -75]])\n        swath_def = geometry.SwathDefinition(lons, lats)\n        data1 = np.ones((2, 4))\n        data2 = np.ones((2, 4)) * 2\n        data3 = np.ones((2, 4)) * 3\n        data = np.dstack((data1, data2, data3))\n        filter_area = geometry.AreaDefinition(\'test\', \'test\', \'test\',\n                                              {\'proj\': \'eqc\', \'lon_0\': 0.0,\n                                                  \'lat_0\': 0.0},\n                                              8, 8,\n                                              (-20037508.34, -10018754.17, 20037508.34, 10018754.17))\n        filter = np.array([[1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [1, 1, 1, 1, 0, 0, 0, 0],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           [0, 0, 0, 0, 1, 1, 1, 1],\n                           ])\n        grid_filter = geo_filter.GridFilter(filter_area, filter, nprocs=2)\n        swath_def_f, data_f = grid_filter.filter(swath_def, data)\n        expected = np.array([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]])\n        self.assertTrue(\n            np.array_equal(data_f, expected), \'Failed 2D grid filtering data\')\n        expected_lons = np.array([-170, 170, -170, 170])\n        expected_lats = np.array([20, -80, 25, -75])\n        self.assertTrue(np.array_equal(swath_def_f.lons[:], expected_lons)\n                        and np.array_equal(swath_def_f.lats[:], expected_lats),\n                        \'Failed finding 2D grid filtering lon lats\')\n\n    def test_boundary(self):\n        """"""Test getting the boundary.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           10,\n                                           10,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n        proj_x_boundary, proj_y_boundary = area_def.projection_x_coords, area_def.projection_y_coords\n        expected_x = np.array([-1250912.72, -1010912.72, -770912.72,\n                               -530912.72, -290912.72, -50912.72, 189087.28,\n                               429087.28, 669087.28, 909087.28])\n        expected_y = np.array([1370031.36, 1130031.36, 890031.36, 650031.36,\n                               410031.36, 170031.36, -69968.64, -309968.64,\n                               -549968.64, -789968.64])\n        self.assertTrue(np.allclose(proj_x_boundary, expected_x),\n                        \'Failed to find projection x coords\')\n        self.assertTrue(np.allclose(proj_y_boundary, expected_y),\n                        \'Failed to find projection y coords\')\n\n    def test_area_extent_ll(self):\n        """"""Test getting the lower left area extent.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           10,\n                                           10,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n        self.assertAlmostEqual(sum(area_def.area_extent_ll),\n                               122.06448093539757, 5,\n                               \'Failed to get lon and lats of area extent\')\n\n    def test_latlong_area(self):\n        """"""Test getting lons and lats from an area.""""""\n        area_def = geometry.AreaDefinition(\'\', \'\', \'\',\n                                           {\'proj\': \'latlong\'},\n                                           360, 180,\n                                           [-180, -90, 180, 90])\n        lons, lats = area_def.get_lonlats()\n        self.assertEqual(lons[0, 0], -179.5)\n        self.assertEqual(lats[0, 0], 89.5)\n\n    def test_lonlat2colrow(self):\n        """"""Test lonlat2colrow.""""""\n        from pyresample import utils\n        area_id = \'meteosat_0deg\'\n        area_name = \'Meteosat 0 degree Service\'\n        proj_id = \'geos0\'\n        x_size = 3712\n        y_size = 3712\n        area_extent = [-5570248.477339261, -5567248.074173444,\n                       5567248.074173444, 5570248.477339261]\n        proj_dict = {\'a\': \'6378169.00\',\n                     \'b\': \'6356583.80\',\n                     \'h\': \'35785831.0\',\n                     \'lon_0\': \'0.0\',\n                     \'proj\': \'geos\'}\n        area = utils.get_area_def(area_id,\n                                  area_name,\n                                  proj_id,\n                                  proj_dict,\n                                  x_size, y_size,\n                                  area_extent)\n\n        # Imatra, Wiesbaden\n        longitudes = np.array([28.75242, 8.24932])\n        latitudes = np.array([61.17185, 50.08258])\n        cols__, rows__ = area.lonlat2colrow(longitudes, latitudes)\n\n        # test arrays\n        cols_expects = np.array([2304, 2040])\n        rows_expects = np.array([186, 341])\n        self.assertTrue((cols__ == cols_expects).all())\n        self.assertTrue((rows__ == rows_expects).all())\n\n        # test scalars\n        lon, lat = (-8.125547604568746, -14.345524111874646)\n        self.assertTrue(area.lonlat2colrow(lon, lat) == (1567, 2375))\n\n    def test_colrow2lonlat(self):\n        """"""Test colrow2lonlat.""""""\n        from pyresample import utils\n        area_id = \'meteosat_0deg\'\n        area_name = \'Meteosat 0 degree Service\'\n        proj_id = \'geos0\'\n        x_size = 3712\n        y_size = 3712\n        area_extent = [-5570248.477339261, -5567248.074173444,\n                       5567248.074173444, 5570248.477339261]\n        proj_dict = {\'a\': \'6378169.00\',\n                     \'b\': \'6356583.80\',\n                     \'h\': \'35785831.0\',\n                     \'lon_0\': \'0.0\',\n                     \'proj\': \'geos\'}\n        area = utils.get_area_def(area_id,\n                                  area_name,\n                                  proj_id,\n                                  proj_dict,\n                                  x_size, y_size,\n                                  area_extent)\n\n        # Imatra, Wiesbaden\n        cols = np.array([2304, 2040])\n        rows = np.array([186, 341])\n        lons__, lats__ = area.colrow2lonlat(cols, rows)\n\n        # test arrays\n        lon_expects = np.array([28.77763033, 8.23765962])\n        lat_expects = np.array([61.20120556, 50.05836402])\n        self.assertTrue(np.allclose(lons__, lon_expects, rtol=0, atol=1e-7))\n        self.assertTrue(np.allclose(lats__, lat_expects, rtol=0, atol=1e-7))\n\n        # test scalars\n        lon__, lat__ = area.colrow2lonlat(1567, 2375)\n        lon_expect = -8.125547604568746\n        lat_expect = -14.345524111874646\n        self.assertTrue(np.allclose(lon__, lon_expect, rtol=0, atol=1e-7))\n        self.assertTrue(np.allclose(lat__, lat_expect, rtol=0, atol=1e-7))\n\n    def test_get_proj_coords_basic(self):\n        """"""Test basic get_proj_coords usage.""""""\n        from pyresample import utils\n        area_id = \'test\'\n        area_name = \'Test area with 2x2 pixels\'\n        proj_id = \'test\'\n        x_size = 10\n        y_size = 10\n        area_extent = [1000000, 0, 1050000, 50000]\n        proj_dict = {""proj"": \'laea\', \'lat_0\': \'60\', \'lon_0\': \'0\', \'a\': \'6371228.0\', \'units\': \'m\'}\n        area_def = utils.get_area_def(area_id, area_name, proj_id, proj_dict, x_size, y_size, area_extent)\n\n        xcoord, ycoord = area_def.get_proj_coords()\n        self.assertTrue(np.allclose(xcoord[0, :],\n                                    np.array([1002500., 1007500., 1012500.,\n                                              1017500., 1022500., 1027500.,\n                                              1032500., 1037500., 1042500.,\n                                              1047500.])))\n        self.assertTrue(np.allclose(ycoord[:, 0],\n                                    np.array([47500., 42500., 37500., 32500.,\n                                              27500., 22500., 17500., 12500.,\n                                              7500.,  2500.])))\n\n        xcoord, ycoord = area_def.get_proj_coords(data_slice=(slice(None, None, 2),\n                                                              slice(None, None, 2)))\n\n        self.assertTrue(np.allclose(xcoord[0, :],\n                                    np.array([1002500., 1012500., 1022500.,\n                                              1032500., 1042500.])))\n        self.assertTrue(np.allclose(ycoord[:, 0],\n                                    np.array([47500., 37500., 27500., 17500.,\n                                              7500.])))\n\n    def test_get_proj_coords_rotation(self):\n        """"""Test basic get_proj_coords usage with rotation specified.""""""\n        from pyresample.geometry import AreaDefinition\n        area_id = \'test\'\n        area_name = \'Test area with 2x2 pixels\'\n        proj_id = \'test\'\n        x_size = 10\n        y_size = 10\n        area_extent = [1000000, 0, 1050000, 50000]\n        proj_dict = {""proj"": \'laea\', \'lat_0\': \'60\', \'lon_0\': \'0\', \'a\': \'6371228.0\', \'units\': \'m\'}\n        area_def = AreaDefinition(area_id, area_name, proj_id, proj_dict, x_size, y_size, area_extent, rotation=45)\n\n        xcoord, ycoord = area_def.get_proj_coords()\n        np.testing.assert_allclose(xcoord[0, :],\n                                   np.array([742462.120246, 745997.654152, 749533.188058, 753068.721964,\n                                             756604.25587, 760139.789776, 763675.323681, 767210.857587,\n                                             770746.391493, 774281.925399]))\n        np.testing.assert_allclose(ycoord[:, 0],\n                                   np.array([-675286.976033, -678822.509939, -682358.043845, -685893.577751,\n                                             -689429.111657, -692964.645563, -696500.179469, -700035.713375,\n                                             -703571.247281, -707106.781187]))\n\n        xcoord, ycoord = area_def.get_proj_coords(data_slice=(slice(None, None, 2), slice(None, None, 2)))\n        np.testing.assert_allclose(xcoord[0, :],\n                                   np.array([742462.120246, 749533.188058, 756604.25587, 763675.323681,\n                                             770746.391493]))\n        np.testing.assert_allclose(ycoord[:, 0],\n                                   np.array([-675286.976033, -682358.043845, -689429.111657, -696500.179469,\n                                             -703571.247281]))\n\n    def test_get_proj_coords_dask(self):\n        """"""Test get_proj_coords usage with dask arrays.""""""\n        from pyresample import get_area_def\n        area_id = \'test\'\n        area_name = \'Test area with 2x2 pixels\'\n        proj_id = \'test\'\n        x_size = 10\n        y_size = 10\n        area_extent = [1000000, 0, 1050000, 50000]\n        proj_dict = {""proj"": \'laea\', \'lat_0\': \'60\', \'lon_0\': \'0\', \'a\': \'6371228.0\', \'units\': \'m\'}\n        area_def = get_area_def(area_id, area_name, proj_id, proj_dict, x_size, y_size, area_extent)\n\n        xcoord, ycoord = area_def.get_proj_coords(chunks=4096)\n        xcoord = xcoord.compute()\n        ycoord = ycoord.compute()\n        self.assertTrue(np.allclose(xcoord[0, :],\n                                    np.array([1002500., 1007500., 1012500.,\n                                              1017500., 1022500., 1027500.,\n                                              1032500., 1037500., 1042500.,\n                                              1047500.])))\n        self.assertTrue(np.allclose(ycoord[:, 0],\n                                    np.array([47500., 42500., 37500., 32500.,\n                                              27500., 22500., 17500., 12500.,\n                                              7500.,  2500.])))\n\n        # use the shared method and provide chunks and slices\n        xcoord, ycoord = area_def.get_proj_coords(data_slice=(slice(None, None, 2),\n                                                              slice(None, None, 2)),\n                                                  chunks=4096)\n        xcoord = xcoord.compute()\n        ycoord = ycoord.compute()\n        self.assertTrue(np.allclose(xcoord[0, :],\n                                    np.array([1002500., 1012500., 1022500.,\n                                              1032500., 1042500.])))\n        self.assertTrue(np.allclose(ycoord[:, 0],\n                                    np.array([47500., 37500., 27500., 17500.,\n                                              7500.])))\n\n    def test_get_xy_from_lonlat(self):\n        """"""Test the function get_xy_from_lonlat.""""""\n        from pyresample import utils\n        area_id = \'test\'\n        area_name = \'Test area with 2x2 pixels\'\n        proj_id = \'test\'\n        x_size = 2\n        y_size = 2\n        area_extent = [1000000, 0, 1050000, 50000]\n        proj_dict = {""proj"": \'laea\',\n                     \'lat_0\': \'60\',\n                     \'lon_0\': \'0\',\n                     \'a\': \'6371228.0\', \'units\': \'m\'}\n        area_def = utils.get_area_def(area_id,\n                                      area_name,\n                                      proj_id,\n                                      proj_dict,\n                                      x_size, y_size,\n                                      area_extent)\n        from pyresample._spatial_mp import Proj\n        p__ = Proj(proj_dict)\n        lon_ul, lat_ul = p__(1000000, 50000, inverse=True)\n        lon_ur, lat_ur = p__(1050000, 50000, inverse=True)\n        lon_ll, lat_ll = p__(1000000, 0, inverse=True)\n        lon_lr, lat_lr = p__(1050000, 0, inverse=True)\n\n        eps_lonlat = 0.01\n        eps_meters = 100\n        x__, y__ = area_def.get_xy_from_lonlat(lon_ul + eps_lonlat,\n                                               lat_ul - eps_lonlat)\n        x_expect, y_expect = 0, 0\n        self.assertEqual(x__, x_expect)\n        self.assertEqual(y__, y_expect)\n        x__, y__ = area_def.get_xy_from_lonlat(lon_ur - eps_lonlat,\n                                               lat_ur - eps_lonlat)\n        self.assertEqual(x__, 1)\n        self.assertEqual(y__, 0)\n        x__, y__ = area_def.get_xy_from_lonlat(lon_ll + eps_lonlat,\n                                               lat_ll + eps_lonlat)\n        self.assertEqual(x__, 0)\n        self.assertEqual(y__, 1)\n        x__, y__ = area_def.get_xy_from_lonlat(lon_lr - eps_lonlat,\n                                               lat_lr + eps_lonlat)\n        self.assertEqual(x__, 1)\n        self.assertEqual(y__, 1)\n\n        lon, lat = p__(1025000 - eps_meters, 25000 - eps_meters, inverse=True)\n        x__, y__ = area_def.get_xy_from_lonlat(lon, lat)\n        self.assertEqual(x__, 0)\n        self.assertEqual(y__, 1)\n\n        lon, lat = p__(1025000 + eps_meters, 25000 - eps_meters, inverse=True)\n        x__, y__ = area_def.get_xy_from_lonlat(lon, lat)\n        self.assertEqual(x__, 1)\n        self.assertEqual(y__, 1)\n\n        lon, lat = p__(1025000 - eps_meters, 25000 + eps_meters, inverse=True)\n        x__, y__ = area_def.get_xy_from_lonlat(lon, lat)\n        self.assertEqual(x__, 0)\n        self.assertEqual(y__, 0)\n\n        lon, lat = p__(1025000 + eps_meters, 25000 + eps_meters, inverse=True)\n        x__, y__ = area_def.get_xy_from_lonlat(lon, lat)\n        self.assertEqual(x__, 1)\n        self.assertEqual(y__, 0)\n\n        lon, lat = p__(999000, -10, inverse=True)\n        self.assertRaises(ValueError, area_def.get_xy_from_lonlat, lon, lat)\n        self.assertRaises(ValueError, area_def.get_xy_from_lonlat, 0., 0.)\n\n        # Test getting arrays back:\n        lons = [lon_ll + eps_lonlat, lon_ur - eps_lonlat]\n        lats = [lat_ll + eps_lonlat, lat_ur - eps_lonlat]\n        x__, y__ = area_def.get_xy_from_lonlat(lons, lats)\n\n        x_expects = np.array([0, 1])\n        y_expects = np.array([1, 0])\n        self.assertTrue((x__.data == x_expects).all())\n        self.assertTrue((y__.data == y_expects).all())\n\n    def test_get_slice_starts_stops(self):\n        """"""Check area slice end-points.""""""\n        from pyresample import utils\n        area_id = \'orig\'\n        area_name = \'Test area\'\n        proj_id = \'test\'\n        x_size = 3712\n        y_size = 3712\n        area_extent = (-5570248.477339745, -5561247.267842293, 5567248.074173927, 5570248.477339745)\n        proj_dict = {\'a\': 6378169.0, \'b\': 6356583.8, \'h\': 35785831.0,\n                     \'lon_0\': 0.0, \'proj\': \'geos\', \'units\': \'m\'}\n        target_area = utils.get_area_def(area_id,\n                                         area_name,\n                                         proj_id,\n                                         proj_dict,\n                                         x_size, y_size,\n                                         area_extent)\n\n        # Expected result is the same for all cases\n        expected = (3, 3709, 3, 3709)\n\n        # Source and target have the same orientation\n        area_extent = (-5580248.477339745, -5571247.267842293, 5577248.074173927, 5580248.477339745)\n        source_area = utils.get_area_def(area_id,\n                                         area_name,\n                                         proj_id,\n                                         proj_dict,\n                                         x_size, y_size,\n                                         area_extent)\n        res = source_area._get_slice_starts_stops(target_area)\n        assert res == expected\n\n        # Source is flipped in X direction\n        area_extent = (5577248.074173927, -5571247.267842293, -5580248.477339745, 5580248.477339745)\n        source_area = utils.get_area_def(area_id,\n                                         area_name,\n                                         proj_id,\n                                         proj_dict,\n                                         x_size, y_size,\n                                         area_extent)\n        res = source_area._get_slice_starts_stops(target_area)\n        assert res == expected\n\n        # Source is flipped in Y direction\n        area_extent = (-5580248.477339745, 5580248.477339745, 5577248.074173927, -5571247.267842293)\n        source_area = utils.get_area_def(area_id,\n                                         area_name,\n                                         proj_id,\n                                         proj_dict,\n                                         x_size, y_size,\n                                         area_extent)\n        res = source_area._get_slice_starts_stops(target_area)\n        assert res == expected\n\n        # Source is flipped in both X and Y directions\n        area_extent = (5577248.074173927, 5580248.477339745, -5580248.477339745, -5571247.267842293)\n        source_area = utils.get_area_def(area_id,\n                                         area_name,\n                                         proj_id,\n                                         proj_dict,\n                                         x_size, y_size,\n                                         area_extent)\n        res = source_area._get_slice_starts_stops(target_area)\n        assert res == expected\n\n    def test_get_area_slices(self):\n        """"""Check area slicing.""""""\n        from pyresample import utils\n\n        # The area of our source data\n        area_id = \'orig\'\n        area_name = \'Test area\'\n        proj_id = \'test\'\n        x_size = 3712\n        y_size = 3712\n        area_extent = (-5570248.477339745, -5561247.267842293, 5567248.074173927, 5570248.477339745)\n        proj_dict = {\'a\': 6378169.0, \'b\': 6356583.8, \'h\': 35785831.0,\n                     \'lon_0\': 0.0, \'proj\': \'geos\', \'units\': \'m\'}\n        area_def = utils.get_area_def(area_id,\n                                      area_name,\n                                      proj_id,\n                                      proj_dict,\n                                      x_size, y_size,\n                                      area_extent)\n\n        # An area that is a subset of the original one\n        area_to_cover = utils.get_area_def(\n            \'cover_subset\',\n            \'Area to cover\',\n            \'test\',\n            proj_dict,\n            1000, 1000,\n            area_extent=(area_extent[0] + 10000,\n                         area_extent[1] + 10000,\n                         area_extent[2] - 10000,\n                         area_extent[3] - 10000))\n        slice_x, slice_y = area_def.get_area_slices(area_to_cover)\n        self.assertEqual(slice(3, 3709, None), slice_x)\n        self.assertEqual(slice(3, 3709, None), slice_y)\n\n        # An area similar to the source data but not the same\n        area_id = \'cover\'\n        area_name = \'Area to cover\'\n        proj_id = \'test\'\n        x_size = 3712\n        y_size = 3712\n        area_extent = (-5570248.477339261, -5567248.074173444, 5567248.074173444, 5570248.477339261)\n        proj_dict = {\'a\': 6378169.5, \'b\': 6356583.8, \'h\': 35785831.0,\n                     \'lon_0\': 0.0, \'proj\': \'geos\', \'units\': \'m\'}\n\n        area_to_cover = utils.get_area_def(area_id,\n                                           area_name,\n                                           proj_id,\n                                           proj_dict,\n                                           x_size, y_size,\n                                           area_extent)\n        slice_x, slice_y = area_def.get_area_slices(area_to_cover)\n        self.assertEqual(slice(46, 3667, None), slice_x)\n        self.assertEqual(slice(52, 3663, None), slice_y)\n\n        area_to_cover = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                                {\'a\': 6378144.0,\n                                                 \'b\': 6356759.0,\n                                                 \'lat_0\': 50.00,\n                                                 \'lat_ts\': 50.00,\n                                                 \'lon_0\': 8.00,\n                                                 \'proj\': \'stere\'},\n                                                10,\n                                                10,\n                                                [-1370912.72,\n                                                 -909968.64,\n                                                 1029087.28,\n                                                 1490031.36])\n        slice_x, slice_y = area_def.get_area_slices(area_to_cover)\n        self.assertEqual(slice_x, slice(1610, 2343))\n        self.assertEqual(slice_y, slice(158, 515, None))\n\n        # The same as source area, but flipped in X and Y\n        area_id = \'cover\'\n        area_name = \'Area to cover\'\n        proj_id = \'test\'\n        x_size = 3712\n        y_size = 3712\n        area_extent = (5567248.074173927, 5570248.477339745, -5570248.477339745, -5561247.267842293)\n        proj_dict = {\'a\': 6378169.0, \'b\': 6356583.8, \'h\': 35785831.0,\n                     \'lon_0\': 0.0, \'proj\': \'geos\', \'units\': \'m\'}\n\n        area_to_cover = utils.get_area_def(area_id,\n                                           area_name,\n                                           proj_id,\n                                           proj_dict,\n                                           x_size, y_size,\n                                           area_extent)\n        slice_x, slice_y = area_def.get_area_slices(area_to_cover)\n        self.assertEqual(slice(0, x_size, None), slice_x)\n        self.assertEqual(slice(0, y_size, None), slice_y)\n\n        # totally different area\n        projections = [{""init"": \'EPSG:4326\'}]\n        if utils.is_pyproj2():\n            projections.append(\'EPSG:4326\')\n        for projection in projections:\n            area_to_cover = geometry.AreaDefinition(\n                \'epsg4326\', \'Global equal latitude/longitude grid for global sphere\',\n                \'epsg4326\',\n                projection,\n                8192,\n                4096,\n                [-180.0, -90.0, 180.0, 90.0])\n\n            slice_x, slice_y = area_def.get_area_slices(area_to_cover)\n            self.assertEqual(slice_x, slice(46, 3667, None))\n            self.assertEqual(slice_y, slice(52, 3663, None))\n\n    def test_get_area_slices_nongeos(self):\n        """"""Check area slicing for non-geos projections.""""""\n        from pyresample import utils\n\n        # The area of our source data\n        area_id = \'orig\'\n        area_name = \'Test area\'\n        proj_id = \'test\'\n        x_size = 3712\n        y_size = 3712\n        area_extent = (-5570248.477339745, -5561247.267842293, 5567248.074173927, 5570248.477339745)\n        proj_dict = {\'a\': 6378169.0, \'b\': 6356583.8, \'lat_1\': 25.,\n                     \'lat_2\': 25., \'lon_0\': 0.0, \'proj\': \'lcc\', \'units\': \'m\'}\n        area_def = utils.get_area_def(area_id,\n                                      area_name,\n                                      proj_id,\n                                      proj_dict,\n                                      x_size, y_size,\n                                      area_extent)\n\n        # An area that is a subset of the original one\n        area_to_cover = utils.get_area_def(\n            \'cover_subset\',\n            \'Area to cover\',\n            \'test\',\n            proj_dict,\n            1000, 1000,\n            area_extent=(area_extent[0] + 10000,\n                         area_extent[1] + 10000,\n                         area_extent[2] - 10000,\n                         area_extent[3] - 10000))\n        slice_x, slice_y = area_def.get_area_slices(area_to_cover)\n        self.assertEqual(slice(3, 3709, None), slice_x)\n        self.assertEqual(slice(3, 3709, None), slice_y)\n\n    def test_proj_str(self):\n        """"""Test the \'proj_str\' property of AreaDefinition.""""""\n        from collections import OrderedDict\n        from pyresample import utils\n        from pyresample.test.utils import friendly_crs_equal\n\n        # pyproj 2.0+ adds a +type=crs parameter\n        proj_dict = OrderedDict()\n        proj_dict[\'proj\'] = \'stere\'\n        proj_dict[\'a\'] = 6378144.0\n        proj_dict[\'b\'] = 6356759.0\n        proj_dict[\'lat_0\'] = 90.00\n        proj_dict[\'lat_ts\'] = 50.00\n        proj_dict[\'lon_0\'] = 8.00\n        area = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                       proj_dict, 10, 10,\n                                       [-1370912.72, -909968.64, 1029087.28,\n                                        1490031.36])\n        assert friendly_crs_equal(\n            \'+a=6378144.0 +b=6356759.0 +lat_0=90.0 +lat_ts=50.0 \'\n            \'+lon_0=8.0 +proj=stere\',\n            area\n        )\n        # try a omerc projection and no_rot parameters\n        proj_dict[\'proj\'] = \'omerc\'\n        proj_dict[\'lat_0\'] = 50.0\n        proj_dict[\'alpha\'] = proj_dict.pop(\'lat_ts\')\n        proj_dict[\'no_rot\'] = \'\'\n        area = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                       proj_dict, 10, 10,\n                                       [-1370912.72, -909968.64, 1029087.28,\n                                        1490031.36])\n        assert friendly_crs_equal(\n            \'+a=6378144.0 +alpha=50.0 +b=6356759.0 +lat_0=50.0 \'\n            \'+lon_0=8.0 +no_rot +proj=omerc\',\n            area\n        )\n\n        # EPSG\n        if utils.is_pyproj2():\n            # With pyproj 2.0+ we expand EPSG to full parameter list\n            full_proj = (\'+datum=WGS84 +lat_0=-90 +lon_0=0 +no_defs \'\n                         \'+proj=laea +type=crs +units=m +x_0=0 +y_0=0\')\n            projections = [\n                (\'+init=EPSG:6932\', full_proj),\n                (\'EPSG:6932\', full_proj)\n            ]\n        else:\n            projections = [\n                (\'+init=EPSG:6932\', \'+init=EPSG:6932\'),\n            ]\n        for projection, expected_proj in projections:\n            area = geometry.AreaDefinition(\n                area_id=\'ease-sh-2.0\',\n                description=\'25km EASE Grid 2.0 (Southern Hemisphere)\',\n                proj_id=\'ease-sh-2.0\',\n                projection=projection,\n                width=123, height=123,\n                area_extent=[-40000., -40000., 40000., 40000.])\n            self.assertEqual(area.proj_str, expected_proj)\n\n        if utils.is_pyproj2():\n            # CRS with towgs84 in it\n            # we remove towgs84 if they are all 0s\n            projection = {\'proj\': \'laea\', \'lat_0\': 52, \'lon_0\': 10, \'x_0\': 4321000, \'y_0\': 3210000,\n                          \'ellps\': \'GRS80\', \'towgs84\': \'0,0,0,0,0,0,0\', \'units\': \'m\', \'no_defs\': True}\n            area = geometry.AreaDefinition(\n                area_id=\'test_towgs84\',\n                description=\'\',\n                proj_id=\'\',\n                projection=projection,\n                width=123, height=123,\n                area_extent=[-40000., -40000., 40000., 40000.])\n            self.assertEqual(area.proj_str,\n                             \'+ellps=GRS80 +lat_0=52 +lon_0=10 +no_defs +proj=laea \'\n                             # \'+towgs84=0.0,0.0,0.0,0.0,0.0,0.0,0.0 \'\n                             \'+type=crs +units=m \'\n                             \'+x_0=4321000 +y_0=3210000\')\n            projection = {\'proj\': \'laea\', \'lat_0\': 52, \'lon_0\': 10, \'x_0\': 4321000, \'y_0\': 3210000,\n                          \'ellps\': \'GRS80\', \'towgs84\': \'0,5,0,0,0,0,0\', \'units\': \'m\', \'no_defs\': True}\n            area = geometry.AreaDefinition(\n                area_id=\'test_towgs84\',\n                description=\'\',\n                proj_id=\'\',\n                projection=projection,\n                width=123, height=123,\n                area_extent=[-40000., -40000., 40000., 40000.])\n            self.assertEqual(area.proj_str,\n                             \'+ellps=GRS80 +lat_0=52 +lon_0=10 +no_defs +proj=laea \'\n                             \'+towgs84=0.0,5.0,0.0,0.0,0.0,0.0,0.0 \'\n                             \'+type=crs +units=m \'\n                             \'+x_0=4321000 +y_0=3210000\')\n\n    def test_striding(self):\n        """"""Test striding AreaDefinitions.""""""\n        from pyresample import utils\n\n        area_id = \'orig\'\n        area_name = \'Test area\'\n        proj_id = \'test\'\n        x_size = 3712\n        y_size = 3712\n        area_extent = (-5570248.477339745, -5561247.267842293, 5567248.074173927, 5570248.477339745)\n        proj_dict = {\'a\': 6378169.0, \'b\': 6356583.8, \'h\': 35785831.0,\n                     \'lon_0\': 0.0, \'proj\': \'geos\', \'units\': \'m\'}\n        area_def = utils.get_area_def(area_id,\n                                      area_name,\n                                      proj_id,\n                                      proj_dict,\n                                      x_size, y_size,\n                                      area_extent)\n\n        reduced_area = area_def[::4, ::4]\n        np.testing.assert_allclose(reduced_area.area_extent, (area_extent[0],\n                                                              area_extent[1] + 3 * area_def.pixel_size_y,\n                                                              area_extent[2] - 3 * area_def.pixel_size_x,\n                                                              area_extent[3]))\n        self.assertEqual(reduced_area.shape, (928, 928))\n\n    def test_get_lonlats_options(self):\n        """"""Test that lotlat options are respected.""""""\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           {\'a\': \'6378144.0\',\n                                            \'b\': \'6356759.0\',\n                                            \'lat_0\': \'50.00\',\n                                            \'lat_ts\': \'50.00\',\n                                            \'lon_0\': \'8.00\',\n                                            \'proj\': \'stere\'},\n                                           800,\n                                           800,\n                                           [-1370912.72,\n                                               -909968.64000000001,\n                                               1029087.28,\n                                               1490031.3600000001])\n        (lon, _) = area_def.get_lonlats(dtype=""f4"")\n        self.assertEqual(lon.dtype, np.dtype(""f4""))\n\n        (lon, _) = area_def.get_lonlats(dtype=""f8"")\n        self.assertEqual(lon.dtype, np.dtype(""f8""))\n\n        from dask.array.core import Array as dask_array\n        (lon, _) = area_def.get_lonlats(dtype=""f4"", chunks=4)\n        self.assertEqual(lon.dtype, np.dtype(""f4""))\n        self.assertIsInstance(lon, dask_array)\n\n        (lon, _) = area_def.get_lonlats(dtype=""f8"", chunks=4)\n        self.assertEqual(lon.dtype, np.dtype(""f8""))\n        self.assertIsInstance(lon, dask_array)\n\n    def test_area_def_geocentric_resolution(self):\n        """"""Test the AreaDefinition.geocentric_resolution method.""""""\n        from pyresample import get_area_def\n        area_extent = (-5570248.477339745, -5561247.267842293, 5567248.074173927, 5570248.477339745)\n        proj_dict = {\'a\': 6378169.0, \'b\': 6356583.8, \'h\': 35785831.0,\n                     \'lon_0\': 0.0, \'proj\': \'geos\', \'units\': \'m\'}\n        # metered projection\n        area_def = get_area_def(\'orig\', \'Test area\', \'test\',\n                                proj_dict,\n                                3712, 3712,\n                                area_extent)\n        geo_res = area_def.geocentric_resolution()\n        np.testing.assert_allclose(10646.562531, geo_res)\n\n        # non-square area non-space area\n        area_extent = (-4570248.477339745, -3561247.267842293, 0, 3570248.477339745)\n        area_def = get_area_def(\'orig\', \'Test area\', \'test\',\n                                proj_dict,\n                                2000, 5000,\n                                area_extent)\n        geo_res = area_def.geocentric_resolution()\n        np.testing.assert_allclose(2397.687307, geo_res)\n\n        # lon/lat\n        proj_dict = {\'a\': 6378169.0, \'b\': 6356583.8, \'proj\': \'latlong\'}\n        area_def = get_area_def(\'orig\', \'Test area\', \'test\',\n                                proj_dict,\n                                3712, 3712,\n                                [-130, 30, -120, 40])\n        geo_res = area_def.geocentric_resolution()\n        np.testing.assert_allclose(298.647232, geo_res)\n\n    def test_from_epsg(self):\n        """"""Test the from_epsg class method.""""""\n        from pyresample.geometry import AreaDefinition\n        sweref = AreaDefinition.from_epsg(\'3006\', 2000)\n        assert sweref.name == \'SWEREF99 TM\'\n        assert sweref.proj_dict == {\'ellps\': \'GRS80\', \'no_defs\': None,\n                                    \'proj\': \'utm\', \'type\': \'crs\', \'units\': \'m\',\n                                    \'zone\': 33}\n        assert sweref.width == 453\n        assert sweref.height == 794\n        import numpy as np\n        np.testing.assert_allclose(sweref.area_extent,\n                                   (181896.3291, 6101648.0705,\n                                    1086312.942376, 7689478.3056))\n\n    def test_from_cf(self):\n        """"""Test the from_cf class method.""""""\n        from pyresample.geometry import AreaDefinition\n        # prepare a netCDF/CF lookalike with xarray\n        import xarray as xr\n        nlat = 19\n        nlon = 37\n        ds = xr.Dataset({\'temp\': ((\'lat\', \'lon\'), np.ma.masked_all((nlat, nlon)))},\n                        coords={\'lat\': np.linspace(-90., +90., num=nlat),\n                                \'lon\': np.linspace(-180., +180., num=nlon)},)\n        ds[\'lat\'].attrs[\'units\'] = \'degreeN\'\n        ds[\'lat\'].attrs[\'standard_name\'] = \'latitude\'\n        ds[\'lon\'].attrs[\'units\'] = \'degreeE\'\n        ds[\'lon\'].attrs[\'standard_name\'] = \'longitude\'\n\n        # call from_cf() and check the results\n        adef = AreaDefinition.from_cf(ds, )\n\n        self.assertEqual(adef.shape, (19, 37))\n        xc = adef.projection_x_coords\n        yc = adef.projection_y_coords\n        self.assertEqual(xc[0], -180., msg=""Wrong x axis (index 0)"")\n        self.assertEqual(xc[1], -180. + 10.0, msg=""Wrong x axis (index 1)"")\n        self.assertEqual(yc[0], -90., msg=""Wrong y axis (index 0)"")\n        self.assertEqual(yc[1], -90. + 10.0, msg=""Wrong y axis (index 1)"")\n\n    @unittest.skipIf(CRS is None, ""pyproj 2.0+ required"")\n    def test_area_def_init_projection(self):\n        """"""Test AreaDefinition with different projection definitions.""""""\n        proj_dict = {\n            \'a\': \'6378144.0\',\n            \'b\': \'6356759.0\',\n            \'lat_0\': \'90.00\',\n            \'lat_ts\': \'50.00\',\n            \'lon_0\': \'8.00\',\n            \'proj\': \'stere\'\n        }\n        crs = CRS(CRS.from_dict(proj_dict).to_wkt())\n        # pass CRS object directly\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           crs,\n                                           800, 800,\n                                           [-1370912.72, -909968.64000000001,\n                                            1029087.28, 1490031.3600000001])\n        self.assertEqual(crs, area_def.crs)\n        # PROJ dictionary\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           crs.to_dict(),\n                                           800, 800,\n                                           [-1370912.72, -909968.64000000001,\n                                            1029087.28, 1490031.3600000001])\n        self.assertEqual(crs, area_def.crs)\n        # PROJ string\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           crs.to_string(),\n                                           800, 800,\n                                           [-1370912.72, -909968.64000000001,\n                                            1029087.28, 1490031.3600000001])\n        self.assertEqual(crs, area_def.crs)\n        # WKT2\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           crs.to_wkt(),\n                                           800, 800,\n                                           [-1370912.72, -909968.64000000001,\n                                            1029087.28, 1490031.3600000001])\n        self.assertEqual(crs, area_def.crs)\n        # WKT1_ESRI\n        area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                           crs.to_wkt(version=\'WKT1_ESRI\'),\n                                           800, 800,\n                                           [-1370912.72, -909968.64000000001,\n                                            1029087.28, 1490031.3600000001])\n        # WKT1 to WKT2 has some different naming of things so this fails\n        # self.assertEqual(crs, area_def.crs)\n\n\nclass TestMakeSliceDivisible(unittest.TestCase):\n    """"""Test the _make_slice_divisible.""""""\n\n    def test_make_slice_divisible(self):\n        """"""Test that making area shape divisible by a given factor works.""""""\n        from pyresample.geometry import _make_slice_divisible\n\n        # Divisible by 2\n        sli = slice(10, 21)\n        factor = 2\n        self.assertNotEqual((sli.stop - sli.start) % factor, 0)\n        res = _make_slice_divisible(sli, 1000, factor=factor)\n        self.assertEqual((res.stop - res.start) % factor, 0)\n\n        # Divisible by 3\n        sli = slice(10, 23)\n        factor = 3\n        self.assertNotEqual((sli.stop - sli.start) % factor, 0)\n        res = _make_slice_divisible(sli, 1000, factor=factor)\n        self.assertEqual((res.stop - res.start) % factor, 0)\n\n        # Divisible by 5\n        sli = slice(10, 23)\n        factor = 5\n        self.assertNotEqual((sli.stop - sli.start) % factor, 0)\n        res = _make_slice_divisible(sli, 1000, factor=factor)\n        self.assertEqual((res.stop - res.start) % factor, 0)\n\n\ndef assert_np_dict_allclose(dict1, dict2):\n    """"""Check allclose on dicts.""""""\n    assert set(dict1.keys()) == set(dict2.keys())\n    for key, val in dict1.items():\n        try:\n            np.testing.assert_allclose(val, dict2[key])\n        except TypeError:\n            assert(val == dict2[key])\n\n\nclass TestSwathDefinition(unittest.TestCase):\n    """"""Test the SwathDefinition.""""""\n\n    def test_swath(self):\n        """"""Test swath.""""""\n        lons1 = np.fromfunction(lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats1 = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n\n        swath_def = geometry.SwathDefinition(lons1, lats1)\n\n        lons2, lats2 = swath_def.get_lonlats()\n\n        self.assertFalse(id(lons1) != id(lons2) or id(lats1) != id(lats2),\n                         msg=\'Caching of swath coordinates failed\')\n\n    def test_slice(self):\n        """"""Test that SwathDefinitions can be sliced.""""""\n        lons1 = np.fromfunction(lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats1 = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n\n        swath_def = geometry.SwathDefinition(lons1, lats1)\n        new_swath_def = swath_def[1000:4000, 20:40]\n        self.assertTupleEqual(new_swath_def.lons.shape, (3000, 20))\n        self.assertTupleEqual(new_swath_def.lats.shape, (3000, 20))\n\n    def test_concat_1d(self):\n        """"""Test concatenating in 1d.""""""\n        lons1 = np.array([1, 2, 3])\n        lats1 = np.array([1, 2, 3])\n        lons2 = np.array([4, 5, 6])\n        lats2 = np.array([4, 5, 6])\n        swath_def1 = geometry.SwathDefinition(lons1, lats1)\n        swath_def2 = geometry.SwathDefinition(lons2, lats2)\n        swath_def_concat = swath_def1.concatenate(swath_def2)\n        expected = np.array([1, 2, 3, 4, 5, 6])\n        self.assertTrue(np.array_equal(swath_def_concat.lons, expected) and\n                        np.array_equal(swath_def_concat.lons, expected),\n                        \'Failed to concatenate 1D swaths\')\n\n    def test_concat_2d(self):\n        """"""Test concatenating in 2d.""""""\n        lons1 = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n        lats1 = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n        lons2 = np.array([[4, 5, 6], [6, 7, 8]])\n        lats2 = np.array([[4, 5, 6], [6, 7, 8]])\n        swath_def1 = geometry.SwathDefinition(lons1, lats1)\n        swath_def2 = geometry.SwathDefinition(lons2, lats2)\n        swath_def_concat = swath_def1.concatenate(swath_def2)\n        expected = np.array(\n            [[1, 2, 3], [3, 4, 5], [5, 6, 7], [4, 5, 6], [6, 7, 8]])\n        self.assertTrue(np.array_equal(swath_def_concat.lons, expected) and\n                        np.array_equal(swath_def_concat.lons, expected),\n                        \'Failed to concatenate 2D swaths\')\n\n    def test_append_1d(self):\n        """"""Test appending in 1d.""""""\n        lons1 = np.array([1, 2, 3])\n        lats1 = np.array([1, 2, 3])\n        lons2 = np.array([4, 5, 6])\n        lats2 = np.array([4, 5, 6])\n        swath_def1 = geometry.SwathDefinition(lons1, lats1)\n        swath_def2 = geometry.SwathDefinition(lons2, lats2)\n        swath_def1.append(swath_def2)\n        expected = np.array([1, 2, 3, 4, 5, 6])\n        self.assertTrue(np.array_equal(swath_def1.lons, expected) and\n                        np.array_equal(swath_def1.lons, expected),\n                        \'Failed to append 1D swaths\')\n\n    def test_append_2d(self):\n        """"""Test appending in 2d.""""""\n        lons1 = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n        lats1 = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n        lons2 = np.array([[4, 5, 6], [6, 7, 8]])\n        lats2 = np.array([[4, 5, 6], [6, 7, 8]])\n        swath_def1 = geometry.SwathDefinition(lons1, lats1)\n        swath_def2 = geometry.SwathDefinition(lons2, lats2)\n        swath_def1.append(swath_def2)\n        expected = np.array(\n            [[1, 2, 3], [3, 4, 5], [5, 6, 7], [4, 5, 6], [6, 7, 8]])\n        self.assertTrue(np.array_equal(swath_def1.lons, expected) and\n                        np.array_equal(swath_def1.lons, expected),\n                        \'Failed to append 2D swaths\')\n\n    def test_swath_equal(self):\n        """"""Test swath equality.""""""\n        lons = np.array([1.2, 1.3, 1.4, 1.5])\n        lats = np.array([65.9, 65.86, 65.82, 65.78])\n        swath_def = geometry.SwathDefinition(lons, lats)\n        swath_def2 = geometry.SwathDefinition(lons, lats)\n        # Identical lons and lats\n        self.assertFalse(\n            swath_def != swath_def2, \'swath_defs are not equal as expected\')\n        # Identical objects\n        self.assertFalse(\n            swath_def != swath_def, \'swath_defs are not equal as expected\')\n\n        lons = np.array([1.2, 1.3, 1.4, 1.5])\n        lats = np.array([65.9, 65.86, 65.82, 65.78])\n        lons2 = np.array([1.2, 1.3, 1.4, 1.5])\n        lats2 = np.array([65.9, 65.86, 65.82, 65.78])\n        swath_def = geometry.SwathDefinition(lons, lats)\n        swath_def2 = geometry.SwathDefinition(lons2, lats2)\n        # different arrays, same values\n        self.assertFalse(\n            swath_def != swath_def2, \'swath_defs are not equal as expected\')\n\n        lons = np.array([1.2, 1.3, 1.4, np.nan])\n        lats = np.array([65.9, 65.86, 65.82, np.nan])\n        lons2 = np.array([1.2, 1.3, 1.4, np.nan])\n        lats2 = np.array([65.9, 65.86, 65.82, np.nan])\n        swath_def = geometry.SwathDefinition(lons, lats)\n        swath_def2 = geometry.SwathDefinition(lons2, lats2)\n        # different arrays, same values, with nans\n        self.assertFalse(\n            swath_def != swath_def2, \'swath_defs are not equal as expected\')\n\n        try:\n            import dask.array as da\n            lons = da.from_array(np.array([1.2, 1.3, 1.4, np.nan]), chunks=2)\n            lats = da.from_array(np.array([65.9, 65.86, 65.82, np.nan]), chunks=2)\n            lons2 = da.from_array(np.array([1.2, 1.3, 1.4, np.nan]), chunks=2)\n            lats2 = da.from_array(np.array([65.9, 65.86, 65.82, np.nan]), chunks=2)\n            swath_def = geometry.SwathDefinition(lons, lats)\n            swath_def2 = geometry.SwathDefinition(lons2, lats2)\n            # different arrays, same values, with nans\n            self.assertFalse(\n                swath_def != swath_def2, \'swath_defs are not equal as expected\')\n        except ImportError:\n            pass\n\n        try:\n            import xarray as xr\n            lons = xr.DataArray(np.array([1.2, 1.3, 1.4, np.nan]))\n            lats = xr.DataArray(np.array([65.9, 65.86, 65.82, np.nan]))\n            lons2 = xr.DataArray(np.array([1.2, 1.3, 1.4, np.nan]))\n            lats2 = xr.DataArray(np.array([65.9, 65.86, 65.82, np.nan]))\n            swath_def = geometry.SwathDefinition(lons, lats)\n            swath_def2 = geometry.SwathDefinition(lons2, lats2)\n            # different arrays, same values, with nans\n            self.assertFalse(\n                swath_def != swath_def2, \'swath_defs are not equal as expected\')\n\n        except ImportError:\n            pass\n\n    def test_swath_not_equal(self):\n        """"""Test swath inequality.""""""\n        lats1 = np.array([65.9, 65.86, 65.82, 65.78])\n        lons = np.array([1.2, 1.3, 1.4, 1.5])\n        lats2 = np.array([65.91, 65.85, 65.80, 65.75])\n        swath_def = geometry.SwathDefinition(lons, lats1)\n        swath_def2 = geometry.SwathDefinition(lons, lats2)\n        self.assertFalse(\n            swath_def == swath_def2, \'swath_defs are not expected to be equal\')\n\n    def test_compute_omerc_params(self):\n        """"""Test omerc parameters computation.""""""\n        lats = np.array([[85.23900604248047, 62.256004333496094, 35.58000183105469],\n                         [80.84000396728516, 60.74200439453125, 34.08500289916992],\n                         [67.07600402832031, 54.147003173828125, 30.547000885009766]]).T\n\n        lons = np.array([[-90.67900085449219, -21.565000534057617, -21.525001525878906],\n                         [79.11000061035156, 7.284000396728516, -5.107000350952148],\n                         [81.26400756835938, 29.672000885009766, 10.260000228881836]]).T\n\n        area = geometry.SwathDefinition(lons, lats)\n        proj_dict = {\'lonc\': -11.391744043133668, \'ellps\': \'WGS84\',\n                     \'proj\': \'omerc\', \'alpha\': 9.185764390923012,\n                     \'gamma\': 0, \'lat_0\': -0.2821013754097188}\n        assert_np_dict_allclose(area._compute_omerc_parameters(\'WGS84\'),\n                                proj_dict)\n        import xarray as xr\n        lats = xr.DataArray(np.array([[85.23900604248047, 62.256004333496094, 35.58000183105469, np.nan],\n                                      [80.84000396728516, 60.74200439453125, 34.08500289916992, np.nan],\n                                      [67.07600402832031, 54.147003173828125, 30.547000885009766, np.nan]]).T,\n                            dims=[\'y\', \'x\'])\n\n        lons = xr.DataArray(np.array([[-90.67900085449219, -21.565000534057617, -21.525001525878906, np.nan],\n                                      [79.11000061035156, 7.284000396728516, -5.107000350952148, np.nan],\n                                      [81.26400756835938, 29.672000885009766, 10.260000228881836, np.nan]]).T)\n\n        area = geometry.SwathDefinition(lons, lats)\n        proj_dict = {\'lonc\': -11.391744043133668, \'ellps\': \'WGS84\',\n                     \'proj\': \'omerc\', \'alpha\': 9.185764390923012,\n                     \'gamma\': 0, \'lat_0\': -0.2821013754097188}\n        assert_np_dict_allclose(area._compute_omerc_parameters(\'WGS84\'),\n                                proj_dict)\n\n    def test_get_edge_lonlats(self):\n        """"""Test the `get_edge_lonlats` functionality.""""""\n        lats = np.array([[85.23900604248047, 62.256004333496094, 35.58000183105469],\n                         [80.84000396728516, 60.74200439453125, 34.08500289916992],\n                         [67.07600402832031, 54.147003173828125, 30.547000885009766]]).T\n\n        lons = np.array([[-90.67900085449219, -21.565000534057617, -21.525001525878906],\n                         [79.11000061035156, 7.284000396728516, -5.107000350952148],\n                         [81.26400756835938, 29.672000885009766, 10.260000228881836]]).T\n\n        area = geometry.SwathDefinition(lons, lats)\n        lons, lats = area.get_edge_lonlats()\n\n        np.testing.assert_allclose(lons, [-90.67900085, 79.11000061,  81.26400757,\n                                          81.26400757, 29.67200089, 10.26000023,\n                                          10.26000023, -5.10700035, -21.52500153,\n                                          -21.52500153, -21.56500053, -90.67900085])\n        np.testing.assert_allclose(lats, [85.23900604, 80.84000397, 67.07600403,\n                                          67.07600403, 54.14700317, 30.54700089,\n                                          30.54700089, 34.0850029, 35.58000183,\n                                          35.58000183, 62.25600433,  85.23900604])\n\n        lats = np.array([[80., 80., 80.],\n                         [80., 90., 80],\n                         [80., 80., 80.]]).T\n\n        lons = np.array([[-45., 0., 45.],\n                         [-90, 0., 90.],\n                         [-135., -180., 135.]]).T\n\n        area = geometry.SwathDefinition(lons, lats)\n        lons, lats = area.get_edge_lonlats()\n\n        np.testing.assert_allclose(lons, [-45., -90., -135., -135., -180., 135.,\n                                          135., 90., 45., 45., 0., -45.])\n        np.testing.assert_allclose(lats, [80., 80., 80., 80., 80., 80., 80.,\n                                          80., 80., 80., 80., 80.])\n\n    def test_compute_optimal_bb(self):\n        """"""Test computing the bb area.""""""\n        from pyresample.utils import is_pyproj2\n        import xarray as xr\n        nplats = np.array([[85.23900604248047, 62.256004333496094, 35.58000183105469],\n                           [80.84000396728516, 60.74200439453125, 34.08500289916992],\n                           [67.07600402832031, 54.147003173828125, 30.547000885009766]]).T\n        lats = xr.DataArray(nplats)\n        nplons = np.array([[-90.67900085449219, -21.565000534057617, -21.525001525878906],\n                           [79.11000061035156, 7.284000396728516, -5.107000350952148],\n                           [81.26400756835938, 29.672000885009766, 10.260000228881836]]).T\n        lons = xr.DataArray(nplons)\n\n        area = geometry.SwathDefinition(lons, lats)\n\n        res = area.compute_optimal_bb_area({\'proj\': \'omerc\', \'ellps\': \'WGS84\'})\n\n        np.testing.assert_allclose(res.area_extent, [-2348379.728104, 3228086.496211,\n                                                     2432121.058435, 10775774.254169])\n        proj_dict = {\'gamma\': 0.0, \'lonc\': -11.391744043133668,\n                     \'ellps\': \'WGS84\', \'proj\': \'omerc\',\n                     \'alpha\': 9.185764390923012, \'lat_0\': -0.2821013754097188}\n        if is_pyproj2():\n            # pyproj2 adds some extra defaults\n            proj_dict.update({\'x_0\': 0, \'y_0\': 0, \'units\': \'m\',\n                              \'k\': 1, \'gamma\': 0,\n                              \'no_defs\': None, \'type\': \'crs\'})\n        assert_np_dict_allclose(res.proj_dict, proj_dict)\n        self.assertEqual(res.shape, (6, 3))\n\n        area = geometry.SwathDefinition(nplons, nplats)\n\n        res = area.compute_optimal_bb_area({\'proj\': \'omerc\', \'ellps\': \'WGS84\'})\n\n        np.testing.assert_allclose(res.area_extent, [-2348379.728104, 3228086.496211,\n                                                     2432121.058435, 10775774.254169])\n        proj_dict = {\'gamma\': 0.0, \'lonc\': -11.391744043133668,\n                     \'ellps\': \'WGS84\', \'proj\': \'omerc\',\n                     \'alpha\': 9.185764390923012, \'lat_0\': -0.2821013754097188}\n        if is_pyproj2():\n            # pyproj2 adds some extra defaults\n            proj_dict.update({\'x_0\': 0, \'y_0\': 0, \'units\': \'m\',\n                              \'k\': 1, \'gamma\': 0,\n                              \'no_defs\': None, \'type\': \'crs\'})\n        assert_np_dict_allclose(res.proj_dict, proj_dict)\n        self.assertEqual(res.shape, (6, 3))\n\n    def test_aggregation(self):\n        """"""Test aggregation on SwathDefinitions.""""""\n        import dask.array as da\n        import xarray as xr\n        import numpy as np\n        window_size = 2\n        resolution = 3\n        lats = np.array([[0, 0, 0, 0], [1, 1, 1, 1.0]])\n        lons = np.array([[178.5, 179.5, -179.5, -178.5], [178.5, 179.5, -179.5, -178.5]])\n        xlats = xr.DataArray(da.from_array(lats, chunks=2), dims=[\'y\', \'x\'],\n                             attrs={\'resolution\': resolution})\n        xlons = xr.DataArray(da.from_array(lons, chunks=2), dims=[\'y\', \'x\'],\n                             attrs={\'resolution\': resolution})\n        from pyresample.geometry import SwathDefinition\n        sd = SwathDefinition(xlons, xlats)\n        res = sd.aggregate(y=window_size, x=window_size)\n        np.testing.assert_allclose(res.lons, [[179, -179]])\n        np.testing.assert_allclose(res.lats, [[0.5, 0.5]], atol=2e-5)\n        self.assertAlmostEqual(res.lons.resolution, window_size * resolution)\n        self.assertAlmostEqual(res.lats.resolution, window_size * resolution)\n\n    def test_striding(self):\n        """"""Test striding.""""""\n        import dask.array as da\n        import xarray as xr\n        import numpy as np\n        lats = np.array([[0, 0, 0, 0], [1, 1, 1, 1.0]])\n        lons = np.array([[178.5, 179.5, -179.5, -178.5], [178.5, 179.5, -179.5, -178.5]])\n        xlats = xr.DataArray(da.from_array(lats, chunks=2), dims=[\'y\', \'x\'])\n        xlons = xr.DataArray(da.from_array(lons, chunks=2), dims=[\'y\', \'x\'])\n        from pyresample.geometry import SwathDefinition\n        sd = SwathDefinition(xlons, xlats)\n        res = sd[::2, ::2]\n        np.testing.assert_allclose(res.lons, [[178.5, -179.5]])\n        np.testing.assert_allclose(res.lats, [[0, 0]], atol=2e-5)\n\n    def test_swath_def_geocentric_resolution(self):\n        """"""Test the SwathDefinition.geocentric_resolution method.""""""\n        import dask.array as da\n        import xarray as xr\n        import numpy as np\n        from pyresample.geometry import SwathDefinition\n        lats = np.array([[0, 0, 0, 0], [1, 1, 1, 1.0]])\n        lons = np.array([[178.5, 179.5, -179.5, -178.5], [178.5, 179.5, -179.5, -178.5]])\n        xlats = xr.DataArray(da.from_array(lats, chunks=2), dims=[\'y\', \'x\'])\n        xlons = xr.DataArray(da.from_array(lons, chunks=2), dims=[\'y\', \'x\'])\n        sd = SwathDefinition(xlons, xlats)\n        geo_res = sd.geocentric_resolution()\n        # google says 1 degrees of longitude is about ~111.321km\n        # so this seems good\n        np.testing.assert_allclose(111301.237078, geo_res)\n\n        # with a resolution attribute that is None\n        xlons.attrs[\'resolution\'] = None\n        xlats.attrs[\'resolution\'] = None\n        sd = SwathDefinition(xlons, xlats)\n        geo_res = sd.geocentric_resolution()\n        np.testing.assert_allclose(111301.237078, geo_res)\n\n        # with a resolution attribute that is a number\n        xlons.attrs[\'resolution\'] = 111301.237078 / 2\n        xlats.attrs[\'resolution\'] = 111301.237078 / 2\n        sd = SwathDefinition(xlons, xlats)\n        geo_res = sd.geocentric_resolution()\n        np.testing.assert_allclose(111301.237078, geo_res)\n\n        # 1D\n        xlats = xr.DataArray(da.from_array(lats.ravel(), chunks=2), dims=[\'y\'])\n        xlons = xr.DataArray(da.from_array(lons.ravel(), chunks=2), dims=[\'y\'])\n        sd = SwathDefinition(xlons, xlats)\n        self.assertRaises(RuntimeError, sd.geocentric_resolution)\n\n\nclass TestStackedAreaDefinition(unittest.TestCase):\n    """"""Test the StackedAreaDefition.""""""\n\n    def test_append(self):\n        """"""Appending new definitions.""""""\n        area1 = geometry.AreaDefinition(""area1"", \'area1\', ""geosmsg"",\n                                        {\'a\': \'6378169.0\', \'b\': \'6356583.8\',\n                                         \'h\': \'35785831.0\', \'lon_0\': \'0.0\',\n                                         \'proj\': \'geos\', \'units\': \'m\'},\n                                        5568, 464,\n                                        (3738502.0095458371, 3715498.9194295374,\n                                            -1830246.0673044831, 3251436.5796920112)\n                                        )\n\n        area2 = geometry.AreaDefinition(""area2"", \'area2\', ""geosmsg"",\n                                        {\'a\': \'6378169.0\', \'b\': \'6356583.8\',\n                                         \'h\': \'35785831.0\', \'lon_0\': \'0.0\',\n                                         \'proj\': \'geos\', \'units\': \'m\'},\n                                        5568, 464,\n                                        (3738502.0095458371, 4179561.259167064,\n                                            -1830246.0673044831, 3715498.9194295374)\n                                        )\n\n        adef = geometry.StackedAreaDefinition(area1, area2)\n        self.assertEqual(len(adef.defs), 1)\n        self.assertTupleEqual(adef.defs[0].area_extent,\n                              (3738502.0095458371, 4179561.259167064,\n                               -1830246.0673044831, 3251436.5796920112))\n\n        # same\n\n        area3 = geometry.AreaDefinition(""area3"", \'area3\', ""geosmsg"",\n                                        {\'a\': \'6378169.0\', \'b\': \'6356583.8\',\n                                         \'h\': \'35785831.0\', \'lon_0\': \'0.0\',\n                                         \'proj\': \'geos\', \'units\': \'m\'},\n                                        5568, 464,\n                                        (3738502.0095458371, 3251436.5796920112,\n                                         -1830246.0673044831, 2787374.2399544837))\n        adef.append(area3)\n        self.assertEqual(len(adef.defs), 1)\n        self.assertTupleEqual(adef.defs[0].area_extent,\n                              (3738502.0095458371, 4179561.259167064,\n                               -1830246.0673044831, 2787374.2399544837))\n\n        self.assertIsInstance(adef.squeeze(), geometry.AreaDefinition)\n\n        # transition\n        area4 = geometry.AreaDefinition(""area4"", \'area4\', ""geosmsg"",\n                                        {\'a\': \'6378169.0\', \'b\': \'6356583.8\',\n                                         \'h\': \'35785831.0\', \'lon_0\': \'0.0\',\n                                         \'proj\': \'geos\', \'units\': \'m\'},\n                                        5568, 464,\n                                        (5567747.7409681147, 2787374.2399544837,\n                                         -1000.3358822065015, 2323311.9002169576))\n\n        adef.append(area4)\n        self.assertEqual(len(adef.defs), 2)\n        self.assertTupleEqual(adef.defs[-1].area_extent,\n                              (5567747.7409681147, 2787374.2399544837,\n                               -1000.3358822065015, 2323311.9002169576))\n\n        self.assertEqual(adef.height, 4 * 464)\n        self.assertIsInstance(adef.squeeze(), geometry.StackedAreaDefinition)\n\n        adef2 = geometry.StackedAreaDefinition()\n        self.assertEqual(len(adef2.defs), 0)\n\n        adef2.append(adef)\n        self.assertEqual(len(adef2.defs), 2)\n        self.assertTupleEqual(adef2.defs[-1].area_extent,\n                              (5567747.7409681147, 2787374.2399544837,\n                               -1000.3358822065015, 2323311.9002169576))\n\n        self.assertEqual(adef2.height, 4 * 464)\n\n    def test_get_lonlats(self):\n        """"""Test get_lonlats on StackedAreaDefinition.""""""\n        area3 = geometry.AreaDefinition(""area3"", \'area3\', ""geosmsg"",\n                                        {\'a\': \'6378169.0\', \'b\': \'6356583.8\',\n                                         \'h\': \'35785831.0\', \'lon_0\': \'0.0\',\n                                         \'proj\': \'geos\', \'units\': \'m\'},\n                                        5568, 464,\n                                        (3738502.0095458371, 3251436.5796920112,\n                                         -1830246.0673044831, 2787374.2399544837))\n\n        # transition\n        area4 = geometry.AreaDefinition(""area4"", \'area4\', ""geosmsg"",\n                                        {\'a\': \'6378169.0\', \'b\': \'6356583.8\',\n                                         \'h\': \'35785831.0\', \'lon_0\': \'0.0\',\n                                         \'proj\': \'geos\', \'units\': \'m\'},\n                                        5568, 464,\n                                        (5567747.7409681147, 2787374.2399544837,\n                                         -1000.3358822065015, 2323311.9002169576))\n\n        final_area = geometry.StackedAreaDefinition(area3, area4)\n        self.assertEqual(len(final_area.defs), 2)\n        lons, lats = final_area.get_lonlats()\n        lons0, lats0 = final_area.defs[0].get_lonlats()\n        lons1, lats1 = final_area.defs[1].get_lonlats()\n        np.testing.assert_allclose(lons[:464, :], lons0)\n        np.testing.assert_allclose(lons[464:, :], lons1)\n        np.testing.assert_allclose(lats[:464, :], lats0)\n        np.testing.assert_allclose(lats[464:, :], lats1)\n\n    def test_combine_area_extents(self):\n        """"""Test combination of area extents.""""""\n        area1 = MagicMock()\n        area1.area_extent = (1, 2, 3, 4)\n        area2 = MagicMock()\n        area2.area_extent = (1, 6, 3, 2)\n        res = combine_area_extents_vertical(area1, area2)\n        self.assertListEqual(res, [1, 6, 3, 4])\n\n        area1 = MagicMock()\n        area1.area_extent = (1, 2, 3, 4)\n        area2 = MagicMock()\n        area2.area_extent = (1, 4, 3, 6)\n        res = combine_area_extents_vertical(area1, area2)\n        self.assertListEqual(res, [1, 2, 3, 6])\n\n        # Non contiguous area extends shouldn\'t be combinable\n        area1 = MagicMock()\n        area1.area_extent = (1, 2, 3, 4)\n        area2 = MagicMock()\n        area2.area_extent = (1, 5, 3, 7)\n        self.assertRaises(IncompatibleAreas,\n                          combine_area_extents_vertical, area1, area2)\n\n    def test_append_area_defs_fail(self):\n        """"""Fail appending areas.""""""\n        area1 = MagicMock()\n        area1.proj_dict = {""proj"": \'A\'}\n        area1.width = 4\n        area1.height = 5\n        area2 = MagicMock()\n        area2.proj_dict = {\'proj\': \'B\'}\n        area2.width = 4\n        area2.height = 6\n        # res = combine_area_extents_vertical(area1, area2)\n        self.assertRaises(IncompatibleAreas,\n                          concatenate_area_defs, area1, area2)\n\n    @patch(\'pyresample.geometry.AreaDefinition\')\n    def test_append_area_defs(self, adef):\n        """"""Test appending area definitions.""""""\n        x_size = random.randrange(6425)\n        area1 = MagicMock()\n        area1.area_extent = (1, 2, 3, 4)\n        area1.proj_dict = {""proj"": \'A\'}\n        area1.height = random.randrange(6425)\n        area1.width = x_size\n\n        area2 = MagicMock()\n        area2.area_extent = (1, 4, 3, 6)\n        area2.proj_dict = {""proj"": \'A\'}\n        area2.height = random.randrange(6425)\n        area2.width = x_size\n\n        concatenate_area_defs(area1, area2)\n        area_extent = [1, 2, 3, 6]\n        y_size = area1.height + area2.height\n        adef.assert_called_once_with(area1.area_id, area1.description, area1.proj_id,\n                                     area1.proj_dict, area1.width, y_size, area_extent)\n\n    def test_create_area_def(self):\n        """"""Test create_area_def and the four sub-methods that call it in AreaDefinition.""""""\n        from pyresample.geometry import AreaDefinition\n        from pyresample.geometry import DynamicAreaDefinition\n        from pyresample.area_config import DataArray\n        from pyresample.area_config import create_area_def as cad\n        from pyresample import utils\n        import pyproj\n\n        area_id = \'ease_sh\'\n        description = \'Antarctic EASE grid\'\n        projection_list = [{\'proj\': \'laea\', \'lat_0\': -90, \'lon_0\': 0, \'a\': 6371228.0, \'units\': \'m\'},\n                           \'+proj=laea +lat_0=-90 +lon_0=0 +a=6371228.0 +units=m\',\n                           \'+init=EPSG:3409\']\n        if utils.is_pyproj2():\n            projection_list.append(\'EPSG:3409\')\n        proj_id = \'ease_sh\'\n        shape = (425, 850)\n        upper_left_extent = (-5326849.0625, 5326849.0625)\n        center_list = [[0, 0], \'a\', (1, 2, 3)]\n        area_extent = (-5326849.0625, -5326849.0625, 5326849.0625, 5326849.0625)\n        resolution = (12533.7625, 25067.525)\n        radius = [5326849.0625, 5326849.0625]\n        units_list = [\'meters\', \'degrees\']\n        base_def = AreaDefinition(area_id, description, \'\', projection_list[0], shape[1], shape[0], area_extent)\n\n        # Tests that incorrect lists do not create an area definition, that both projection strings and\n        # dicts are accepted, and that degrees and meters both create the same area definition.\n        # area_list used to check that areas are all correct at the end.\n        area_list = []\n        from itertools import product\n        for projection, units, center in product(projection_list, units_list, center_list):\n            # essentials = center, radius, upper_left_extent, resolution, shape.\n            if \'m\' in units:\n                # Meters.\n                essentials = [[0, 0], [5326849.0625, 5326849.0625], (-5326849.0625, 5326849.0625),\n                              (12533.7625, 25067.525), (425, 850)]\n            else:\n                # Degrees.\n                essentials = [(0.0, -90.0), 49.4217406986, (-45.0, -17.516001139327766),\n                              (0.11271481862984278, 0.22542974631297721), (425, 850)]\n            # If center is valid, use it.\n            if len(center) == 2:\n                center = essentials[0]\n            try:\n                area_list.append(cad(area_id, projection, proj_id=proj_id, upper_left_extent=essentials[2],\n                                     center=center, shape=essentials[4], resolution=essentials[3],\n                                     radius=essentials[1], description=description, units=units, rotation=45))\n            except ValueError:\n                pass\n        self.assertEqual(len(area_list), 8 if utils.is_pyproj2() else 6)\n\n        # Tests that specifying units through xarrays works.\n        area_list.append(cad(area_id, projection_list[1], shape=shape,\n                             area_extent=DataArray((-135.0, -17.516001139327766,\n                                                    45.0, -17.516001139327766),\n                                                   attrs={\'units\': \'degrees\'})))\n        # Tests area functions 1-A and 2-A.\n        area_list.append(cad(area_id, projection_list[1], resolution=resolution, area_extent=area_extent))\n        # Tests area function 1-B. Also test that DynamicAreaDefinition arguments don\'t crash AreaDefinition.\n        area_list.append(cad(area_id, projection_list[1], shape=shape, center=center_list[0],\n                             upper_left_extent=upper_left_extent, optimize_projection=None))\n        # Tests area function 1-C.\n        area_list.append(cad(area_id, projection_list[1], shape=shape, center=center_list[0], radius=radius))\n        # Tests area function 1-D.\n        area_list.append(cad(area_id, projection_list[1], shape=shape,\n                             radius=radius, upper_left_extent=upper_left_extent))\n        # Tests all 4 user cases.\n        area_list.append(AreaDefinition.from_extent(area_id, projection_list[1], shape, area_extent))\n        area_list.append(AreaDefinition.from_circle(area_id, projection_list[1], center_list[0], radius,\n                                                    resolution=resolution))\n        area_list.append(AreaDefinition.from_area_of_interest(area_id, projection_list[1], shape, center_list[0],\n                                                              resolution))\n        area_list.append(AreaDefinition.from_ul_corner(area_id, projection_list[1], shape, upper_left_extent,\n                                                       resolution))\n        # Tests non-poles using degrees and mercator.\n        area_def = cad(area_id, \'+a=6371228.0 +units=m +lon_0=0 +proj=merc +lat_0=0\',\n                       center=(0, 0), radius=45, resolution=(1, 0.9999291722135637), units=\'degrees\')\n        self.assertTrue(isinstance(area_def, AreaDefinition))\n        self.assertTrue(np.allclose(area_def.area_extent, (-5003950.7698, -5615432.0761, 5003950.7698, 5615432.0761)))\n        self.assertEqual(area_def.shape, (101, 90))\n        # Checks every area definition made\n        for area_def in area_list:\n            if \'EPSG\' in area_def.proj_dict or \'init\' in area_def.proj_dict:\n                # Use formal definition of EPSG projections to make them comparable to the base definition\n                proj_def = pyproj.Proj(area_def.proj_str).definition_string().strip()\n                area_def = area_def.copy(projection=proj_def)\n\n                # Remove extra attributes from the formal definition\n                if \'R\' in area_def.proj_dict:\n                    # pyproj < 2\n                    area_def.proj_dict[\'a\'] = area_def.proj_dict.pop(\'R\')\n                for key in [\'x_0\', \'y_0\', \'no_defs\', \'b\', \'init\']:\n                    area_def.proj_dict.pop(key, None)\n\n            self.assertEqual(area_def, base_def)\n\n        # Makes sure if shape or area_extent is found/given, a DynamicAreaDefinition is made.\n        self.assertTrue(isinstance(cad(area_id, projection_list[1], shape=shape), DynamicAreaDefinition))\n        self.assertTrue(isinstance(cad(area_id, projection_list[1], area_extent=area_extent), DynamicAreaDefinition))\n\n        area_def = cad(\'omerc_bb\', {\'ellps\': \'WGS84\', \'proj\': \'omerc\'})\n        self.assertTrue(isinstance(area_def, DynamicAreaDefinition))\n\n\nclass TestDynamicAreaDefinition(unittest.TestCase):\n    """"""Test the DynamicAreaDefinition class.""""""\n\n    def test_freeze(self):\n        """"""Test freezing the area.""""""\n        area = geometry.DynamicAreaDefinition(\'test_area\', \'A test area\',\n                                              {\'proj\': \'laea\'})\n        lons = [10, 10, 22, 22]\n        lats = [50, 66, 66, 50]\n        result = area.freeze((lons, lats),\n                             resolution=3000,\n                             proj_info={\'lon_0\': 16, \'lat_0\': 58})\n\n        np.testing.assert_allclose(result.area_extent, (-432079.38952,\n                                                        -872594.690447,\n                                                        432079.38952,\n                                                        904633.303964))\n        self.assertEqual(result.proj_dict[\'lon_0\'], 16)\n        self.assertEqual(result.proj_dict[\'lat_0\'], 58)\n        self.assertEqual(result.width, 288)\n        self.assertEqual(result.height, 592)\n\n        # make sure that setting `proj_info` once doesn\'t\n        # set it in the dynamic area\n        result = area.freeze((lons, lats),\n                             resolution=3000,\n                             proj_info={\'lon_0\': 0})\n        np.testing.assert_allclose(result.area_extent, (538546.7274949469,\n                                                        5380808.879250369,\n                                                        1724415.6519203288,\n                                                        6998895.701001488))\n        self.assertEqual(result.proj_dict[\'lon_0\'], 0)\n        # lat_0 could be provided or not depending on version of pyproj\n        self.assertEqual(result.proj_dict.get(\'lat_0\', 0), 0)\n        self.assertEqual(result.width, 395)\n        self.assertEqual(result.height, 539)\n\n    def test_freeze_with_bb(self):\n        """"""Test freezing the area with bounding box computation.""""""\n        area = geometry.DynamicAreaDefinition(\'test_area\', \'A test area\', {\'proj\': \'omerc\'},\n                                              optimize_projection=True)\n        lons = [[10, 12.1, 14.2, 16.3],\n                [10, 12, 14, 16],\n                [10, 11.9, 13.8, 15.7]]\n        lats = [[66, 67, 68, 69.],\n                [58, 59, 60, 61],\n                [50, 51, 52, 53]]\n        import xarray as xr\n        sdef = geometry.SwathDefinition(xr.DataArray(lons), xr.DataArray(lats))\n        result = area.freeze(sdef, resolution=1000)\n        np.testing.assert_allclose(result.area_extent,\n                                   [-335439.956533, 5502125.451125,\n                                    191991.313351, 7737532.343683])\n\n        self.assertEqual(result.width, 4)\n        self.assertEqual(result.height, 18)\n        # Test for properties and shape usage in freeze.\n        area = geometry.DynamicAreaDefinition(\'test_area\', \'A test area\', {\'proj\': \'merc\'},\n                                              width=4, height=18)\n        self.assertEqual((18, 4), area.shape)\n        result = area.freeze(sdef)\n        np.testing.assert_allclose(result.area_extent,\n                                   (996309.4426, 6287132.757981, 1931393.165263, 10837238.860543))\n        area = geometry.DynamicAreaDefinition(\'test_area\', \'A test area\', {\'proj\': \'merc\'},\n                                              resolution=1000)\n        self.assertEqual(1000, area.pixel_size_x)\n        self.assertEqual(1000, area.pixel_size_y)\n\n    def test_compute_domain(self):\n        """"""Test computing size and area extent.""""""\n        area = geometry.DynamicAreaDefinition(\'test_area\', \'A test area\',\n                                              {\'proj\': \'laea\'})\n        corners = [1, 1, 9, 9]\n        self.assertRaises(ValueError, area.compute_domain, corners, 1, 1)\n\n        area_extent, x_size, y_size = area.compute_domain(corners, shape=(5, 5))\n        self.assertTupleEqual(area_extent, (0, 0, 10, 10))\n        self.assertEqual(x_size, 5)\n        self.assertEqual(y_size, 5)\n\n        area_extent, x_size, y_size = area.compute_domain(corners, resolution=2)\n        self.assertTupleEqual(area_extent, (0, 0, 10, 10))\n        self.assertEqual(x_size, 5)\n        self.assertEqual(y_size, 5)\n\n\nclass TestCrop(unittest.TestCase):\n    """"""Test the area helpers.""""""\n\n    def test_get_geostationary_bbox(self):\n        """"""Get the geostationary bbox.""""""\n        geos_area = MagicMock()\n        lon_0 = 0\n        geos_area.proj_dict = {\'a\': 6378169.00,\n                               \'b\': 6356583.80,\n                               \'h\': 35785831.00,\n                               \'lon_0\': lon_0,\n                               \'proj\': \'geos\'}\n        geos_area.area_extent = [-5500000., -5500000., 5500000., 5500000.]\n\n        lon, lat = geometry.get_geostationary_bounding_box(geos_area, 20)\n        # This musk be equal to lon.\n        elon = np.array([-79.23372832, -77.9694809, -74.55229623, -67.32816598,\n                         -41.45591465, 41.45591465, 67.32816598, 74.55229623,\n                         77.9694809, 79.23372832, 79.23372832, 77.9694809,\n                         74.55229623, 67.32816598, 41.45591465, -41.45591465,\n                         -67.32816598, -74.55229623, -77.9694809, -79.23372832])\n        elat = np.array([6.94302533e-15, 1.97333299e+01, 3.92114217e+01, 5.82244715e+01,\n                         7.52409201e+01, 7.52409201e+01, 5.82244715e+01, 3.92114217e+01,\n                         1.97333299e+01, -0.00000000e+00, -6.94302533e-15, -1.97333299e+01,\n                         -3.92114217e+01, -5.82244715e+01, -7.52409201e+01, -7.52409201e+01,\n                         -5.82244715e+01, -3.92114217e+01, -1.97333299e+01, 0.0])\n\n        np.testing.assert_allclose(lon, elon)\n        np.testing.assert_allclose(lat, elat)\n\n        geos_area = MagicMock()\n        lon_0 = 10\n        geos_area.proj_dict = {\'a\': 6378169.00,\n                               \'b\': 6356583.80,\n                               \'h\': 35785831.00,\n                               \'lon_0\': lon_0,\n                               \'proj\': \'geos\'}\n        geos_area.area_extent = [-5500000., -5500000., 5500000., 5500000.]\n\n        lon, lat = geometry.get_geostationary_bounding_box(geos_area, 20)\n        np.testing.assert_allclose(lon, elon + lon_0)\n\n    def test_get_geostationary_angle_extent(self):\n        """"""Get max geostationary angles.""""""\n        geos_area = MagicMock()\n        del geos_area.crs\n        geos_area.proj_dict = {\n            \'proj\': \'geos\',\n            \'sweep\': \'x\',\n            \'lon_0\': -89.5,\n            \'a\': 6378169.00,\n            \'b\': 6356583.80,\n            \'h\': 35785831.00,\n            \'units\': \'m\'}\n\n        expected = (0.15185342867090912, 0.15133555510297725)\n        np.testing.assert_allclose(expected,\n                                   geometry.get_geostationary_angle_extent(geos_area))\n\n        geos_area.proj_dict[\'a\'] = 1000.0\n        geos_area.proj_dict[\'b\'] = 1000.0\n        geos_area.proj_dict[\'h\'] = np.sqrt(2) * 1000.0 - 1000.0\n\n        expected = (np.deg2rad(45), np.deg2rad(45))\n        np.testing.assert_allclose(expected,\n                                   geometry.get_geostationary_angle_extent(geos_area))\n\n        geos_area.proj_dict = {\n            \'proj\': \'geos\',\n            \'sweep\': \'x\',\n            \'lon_0\': -89.5,\n            \'ellps\': \'GRS80\',\n            \'h\': 35785831.00,\n            \'units\': \'m\'}\n        expected = (0.15185277703584374, 0.15133971368991794)\n        np.testing.assert_allclose(expected,\n                                   geometry.get_geostationary_angle_extent(geos_area))\n\n    def test_sub_area(self):\n        """"""Sub area slicing.""""""\n        area = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                       {\'a\': \'6378144.0\',\n                                        \'b\': \'6356759.0\',\n                                        \'lat_0\': \'50.00\',\n                                        \'lat_ts\': \'50.00\',\n                                        \'lon_0\': \'8.00\',\n                                        \'proj\': \'stere\'},\n                                       800,\n                                       800,\n                                       [-1370912.72,\n                                        -909968.64000000001,\n                                        1029087.28,\n                                        1490031.3600000001])\n        res = area[slice(20, 720), slice(100, 500)]\n        np.testing.assert_allclose((-1070912.72, -669968.6399999999,\n                                    129087.28000000003, 1430031.36),\n                                   res.area_extent)\n        self.assertEqual(res.shape, (700, 400))\n\n    def test_aggregate(self):\n        """"""Test aggregation of AreaDefinitions.""""""\n        area = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                       {\'a\': \'6378144.0\',\n                                        \'b\': \'6356759.0\',\n                                        \'lat_0\': \'50.00\',\n                                        \'lat_ts\': \'50.00\',\n                                        \'lon_0\': \'8.00\',\n                                        \'proj\': \'stere\'},\n                                       800,\n                                       800,\n                                       [-1370912.72,\n                                           -909968.64000000001,\n                                           1029087.28,\n                                           1490031.3600000001])\n        res = area.aggregate(x=4, y=2)\n        self.assertDictEqual(res.proj_dict, area.proj_dict)\n        np.testing.assert_allclose(res.area_extent, area.area_extent)\n        self.assertEqual(res.shape[0], area.shape[0] / 2)\n        self.assertEqual(res.shape[1], area.shape[1] / 4)\n'"
pyresample/test/test_gradient.py,67,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2019\n\n# Author(s):\n\n#   Martin Raspaud <martin.raspaud@smhi.se>\n\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Tests for the gradien search resampling.""""""\n\nimport unittest\nfrom unittest import mock\nfrom pyresample.geometry import AreaDefinition, SwathDefinition\nimport numpy as np\nimport dask.array as da\nimport xarray as xr\n\n\nclass TestGradientResampler(unittest.TestCase):\n    """"""Test case for the gradient resampling.""""""\n\n    def setUp(self):\n        """"""Set up the test case.""""""\n        from pyresample.gradient import GradientSearchResampler\n        self.src_area = AreaDefinition(\'dst\', \'dst area\', None,\n                                       {\'ellps\': \'WGS84\', \'h\': \'35785831\', \'proj\': \'geos\'},\n                                       100, 100,\n                                       (5550000.0, 5550000.0, -5550000.0, -5550000.0))\n        self.src_swath = SwathDefinition(*self.src_area.get_lonlats())\n        self.dst_area = AreaDefinition(\'euro40\', \'euro40\', None,\n                                       {\'proj\': \'stere\', \'lon_0\': 14.0,\n                                        \'lat_0\': 90.0, \'lat_ts\': 60.0,\n                                        \'ellps\': \'bessel\'},\n                                       102, 102,\n                                       (-2717181.7304994687, -5571048.14031214,\n                                        1378818.2695005313, -1475048.1403121399))\n\n        self.resampler = GradientSearchResampler(self.src_area, self.dst_area)\n        self.swath_resampler = GradientSearchResampler(self.src_swath,\n                                                       self.dst_area)\n\n    def test_get_projection_coordinates_area_to_area(self):\n        """"""Check that the coordinates are initialized, for area -> area.""""""\n        assert self.resampler.prj is None\n        self.resampler._get_projection_coordinates((10, 10))\n        cdst_x = self.resampler.dst_x.compute()\n        cdst_y = self.resampler.dst_y.compute()\n        assert np.allclose(np.min(cdst_x), -2022632.1675016289)\n        assert np.allclose(np.max(cdst_x), 2196052.591296284)\n        assert np.allclose(np.min(cdst_y), 3517933.413092212)\n        assert np.allclose(np.max(cdst_y), 5387038.893400168)\n        assert self.resampler.use_input_coords\n        assert self.resampler.prj is not None\n\n    def test_get_projection_coordinates_swath_to_area(self):\n        """"""Check that the coordinates are initialized, for swath -> area.""""""\n        assert self.swath_resampler.prj is None\n        self.swath_resampler._get_projection_coordinates((10, 10))\n        cdst_x = self.swath_resampler.dst_x.compute()\n        cdst_y = self.swath_resampler.dst_y.compute()\n        assert np.allclose(np.min(cdst_x), -2697103.29912692)\n        assert np.allclose(np.max(cdst_x), 1358739.8381279823)\n        assert np.allclose(np.min(cdst_y), -5550969.708939591)\n        assert np.allclose(np.max(cdst_y), -1495126.5716846888)\n        assert self.swath_resampler.use_input_coords is False\n        assert self.swath_resampler.prj is not None\n\n    def test_get_gradients(self):\n        """"""Test that coordinate gradients are computed correctly.""""""\n        self.resampler._get_projection_coordinates((10, 10))\n        assert self.resampler.src_gradient_xl is None\n        self.resampler._get_gradients()\n        assert self.resampler.src_gradient_xl.compute().max() == 0.0\n        assert self.resampler.src_gradient_xp.compute().max() == -111000.0\n        assert self.resampler.src_gradient_yl.compute().max() == 111000.0\n        assert self.resampler.src_gradient_yp.compute().max() == 0.0\n\n    def test_get_chunk_mappings(self):\n        """"""Test that chunk overlap, and source and target slices are correct.""""""\n        chunks = (10, 10)\n        num_chunks = np.product(chunks)\n        self.resampler._get_projection_coordinates(chunks)\n        self.resampler._get_gradients()\n        assert self.resampler.coverage_status is None\n        self.resampler.get_chunk_mappings()\n        # 8 source chunks overlap the target area\n        covered_src_chunks = np.array([38, 39, 48, 49, 58, 59, 68, 69])\n        res = np.where(self.resampler.coverage_status)[0]\n        assert np.all(res == covered_src_chunks)\n        # All *num_chunks* should have values in the lists\n        assert len(self.resampler.coverage_status) == num_chunks\n        assert len(self.resampler.src_slices) == num_chunks\n        assert len(self.resampler.dst_slices) == num_chunks\n        assert len(self.resampler.dst_mosaic_locations) == num_chunks\n        # There\'s only one output chunk, and the covered source chunks\n        # should have destination locations of (0, 0)\n        res = np.array(self.resampler.dst_mosaic_locations)[covered_src_chunks]\n        assert all([all(loc == (0, 0)) for loc in list(res)])\n\n    def test_get_src_poly_area(self):\n        """"""Test defining source chunk polygon for AreaDefinition.""""""\n        chunks = (10, 10)\n        self.resampler._get_projection_coordinates(chunks)\n        self.resampler._get_gradients()\n        poly = self.resampler._get_src_poly(0, 40, 0, 40)\n        assert np.allclose(poly.area, 12365358458842.43)\n\n    def test_get_src_poly_swath(self):\n        """"""Test defining source chunk polygon for SwathDefinition.""""""\n        chunks = (10, 10)\n        self.swath_resampler._get_projection_coordinates(chunks)\n        self.swath_resampler._get_gradients()\n        # Swath area defs can\'t be sliced, so False is returned\n        poly = self.swath_resampler._get_src_poly(0, 40, 0, 40)\n        assert poly is False\n\n    @mock.patch(\'pyresample.gradient.get_polygon\')\n    def test_get_dst_poly(self, get_polygon):\n        """"""Test defining destination chunk polygon.""""""\n        chunks = (10, 10)\n        self.resampler._get_projection_coordinates(chunks)\n        self.resampler._get_gradients()\n        # First call should make a call to get_polygon()\n        self.resampler._get_dst_poly(\'idx1\', 0, 10, 0, 10)\n        assert get_polygon.call_count == 1\n        assert \'idx1\' in self.resampler.dst_polys\n        # The second call to the same index should come from cache\n        self.resampler._get_dst_poly(\'idx1\', 0, 10, 0, 10)\n        assert get_polygon.call_count == 1\n\n        # Swath defs raise AttributeError, and False is returned\n        get_polygon.side_effect = AttributeError\n        self.resampler._get_dst_poly(\'idx2\', 0, 10, 0, 10)\n        assert self.resampler.dst_polys[\'idx2\'] is False\n\n    def test_filter_data(self):\n        """"""Test filtering chunks that do not overlap.""""""\n        chunks = (10, 10)\n        self.resampler._get_projection_coordinates(chunks)\n        self.resampler._get_gradients()\n        self.resampler.get_chunk_mappings()\n\n        # Basic filtering.  There should be 8 dask arrays that each\n        # have a shape of (10, 10)\n        res = self.resampler._filter_data(self.resampler.src_x)\n        valid = [itm for itm in res if itm is not None]\n        assert len(valid) == 8\n        shapes = [arr.shape for arr in valid]\n        for shp in shapes:\n            assert shp == (10, 10)\n\n        # Destination x/y coordinate array filtering.  Again, 8 dask\n        # arrays each with shape (102, 102)\n        res = self.resampler._filter_data(self.resampler.dst_x, is_src=False)\n        valid = [itm for itm in res if itm is not None]\n        assert len(valid) == 8\n        shapes = [arr.shape for arr in valid]\n        for shp in shapes:\n            assert shp == (102, 102)\n\n        # Add a dimension to the given dataset\n        data = da.random.random(self.src_area.shape)\n        res = self.resampler._filter_data(data, add_dim=True)\n        valid = [itm for itm in res if itm is not None]\n        assert len(valid) == 8\n        shapes = [arr.shape for arr in valid]\n        for shp in shapes:\n            assert shp == (1, 10, 10)\n\n        # 1D and 3+D should raise NotImplementedError\n        data = da.random.random((3,))\n        try:\n            res = self.resampler._filter_data(data, add_dim=True)\n            raise IndexError\n        except NotImplementedError:\n            pass\n        data = da.random.random((3, 3, 3, 3))\n        try:\n            res = self.resampler._filter_data(data, add_dim=True)\n            raise IndexError\n        except NotImplementedError:\n            pass\n\n    def test_resample_area_to_area_2d(self):\n        """"""Resample area to area, 2d.""""""\n        data = xr.DataArray(da.ones(self.src_area.shape, dtype=np.float64),\n                            dims=[\'y\', \'x\'])\n        res = self.resampler.compute(\n            data, method=\'bil\').compute(scheduler=\'single-threaded\')\n        assert res.shape == self.dst_area.shape\n        assert np.allclose(res, 1)\n\n    def test_resample_area_to_area_2d_fill_value(self):\n        """"""Resample area to area, 2d, use fill value.""""""\n        data = xr.DataArray(da.full(self.src_area.shape, np.nan,\n                                    dtype=np.float64), dims=[\'y\', \'x\'])\n        res = self.resampler.compute(\n            data, method=\'bil\',\n            fill_value=2.0).compute(scheduler=\'single-threaded\')\n        assert res.shape == self.dst_area.shape\n        assert np.allclose(res, 2.0)\n\n    def test_resample_area_to_area_3d(self):\n        """"""Resample area to area, 3d.""""""\n        data = xr.DataArray(da.ones((3, ) + self.src_area.shape,\n                                    dtype=np.float64) *\n                            np.array([1, 2, 3])[:, np.newaxis, np.newaxis],\n                            dims=[\'bands\', \'y\', \'x\'])\n        res = self.resampler.compute(\n            data, method=\'bil\').compute(scheduler=\'single-threaded\')\n        assert res.shape == (3, ) + self.dst_area.shape\n        assert np.allclose(res[0, :, :], 1.0)\n        assert np.allclose(res[1, :, :], 2.0)\n        assert np.allclose(res[2, :, :], 3.0)\n\n    def test_resample_swath_to_area_2d(self):\n        """"""Resample swath to area, 2d.""""""\n        data = xr.DataArray(da.ones(self.src_swath.shape, dtype=np.float64),\n                            dims=[\'y\', \'x\'])\n        res = self.swath_resampler.compute(\n            data, method=\'bil\').compute(scheduler=\'single-threaded\')\n        assert res.shape == self.dst_area.shape\n        assert not np.all(np.isnan(res))\n\n\n    def test_resample_swath_to_area_3d(self):\n        """"""Resample area to area, 3d.""""""\n        data = xr.DataArray(da.ones((3, ) + self.src_swath.shape,\n                                    dtype=np.float64) *\n                            np.array([1, 2, 3])[:, np.newaxis, np.newaxis],\n                            dims=[\'bands\', \'y\', \'x\'])\n        res = self.swath_resampler.compute(\n            data, method=\'bil\').compute(scheduler=\'single-threaded\')\n        assert res.shape == (3, ) + self.dst_area.shape\n        for i in range(res.shape[0]):\n            arr = np.ravel(res[i, :, :])\n            assert np.allclose(arr[np.isfinite(arr)], float(i + 1))\n\n\ndef test_check_overlap():\n    """"""Test overlap check returning correct results.""""""\n    from shapely.geometry import Polygon\n    from pyresample.gradient import check_overlap\n\n    # If either of the polygons is False, True is returned\n    assert check_overlap(False, 3) is True\n    assert check_overlap(\'eggs\', False) is True\n    assert check_overlap(False, False) is True\n\n    # If either the polygons is None, False is returned\n    assert check_overlap(None, \'bacon\') is False\n    assert check_overlap(\'spam\', None) is False\n    assert check_overlap(None, None) is False\n\n    # If the polygons overlap, True is returned\n    poly1 = Polygon(((0, 0), (0, 1), (1, 1), (1, 0)))\n    poly2 = Polygon(((-1, -1), (-1, 1), (1, 1), (1, -1)))\n    assert check_overlap(poly1, poly2) is True\n\n    # If the polygons do not overlap, False is returned\n    poly2 = Polygon(((5, 5), (6, 5), (6, 6), (5, 6)))\n    assert check_overlap(poly1, poly2) is False\n\n\n@mock.patch(\'pyresample.gradient.get_geostationary_bounding_box\')\ndef test_get_border_lonlats(get_geostationary_bounding_box):\n    """"""Test that correct methods are called in get_border_lonlats().""""""\n    from pyresample.gradient import get_border_lonlats\n    geo_def = mock.MagicMock(proj_dict={\'proj\': \'geos\'})\n    get_geostationary_bounding_box.return_value = 1, 2\n    res = get_border_lonlats(geo_def)\n    assert res == (1, 2)\n    get_geostationary_bounding_box.assert_called_with(geo_def, 3600)\n    geo_def.get_boundary_lonlats.assert_not_called()\n\n    lon_sides = mock.MagicMock(side1=np.array([1]), side2=np.array([2]),\n                               side3=np.array([3]), side4=np.array([4]))\n    lat_sides = mock.MagicMock(side1=np.array([1]), side2=np.array([2]),\n                               side3=np.array([3]), side4=np.array([4]))\n    geo_def = mock.MagicMock()\n    geo_def.get_boundary_lonlats.return_value = lon_sides, lat_sides\n    lon_b, lat_b = get_border_lonlats(geo_def)\n    assert np.all(lon_b == np.array([1, 2, 3, 4]))\n    assert np.all(lat_b == np.array([1, 2, 3, 4]))\n\n\n@mock.patch(\'pyresample.gradient.Polygon\')\n@mock.patch(\'pyresample.gradient.get_border_lonlats\')\ndef test_get_polygon(get_border_lonlats, Polygon):\n    """"""Test polygon creation.""""""\n    from pyresample.gradient import get_polygon\n\n    # Valid polygon\n    get_border_lonlats.return_value = (1, 2)\n    geo_def = mock.MagicMock()\n    prj = mock.MagicMock()\n    x_borders = [0, 0, 1, 1]\n    y_borders = [0, 1, 1, 0]\n    boundary = [(0, 0), (0, 1), (1, 1), (1, 0)]\n    prj.return_value = (x_borders, y_borders)\n    poly = mock.MagicMock(area=2.0)\n    Polygon.return_value = poly\n    res = get_polygon(prj, geo_def)\n    get_border_lonlats.assert_called_with(geo_def)\n    prj.assert_called_with(1, 2)\n    Polygon.assert_called_with(boundary)\n    assert res is poly\n\n    # Some border points are invalid, those should have been removed\n    x_borders = [np.inf, 0, 0, 0, 1, np.nan, 2]\n    y_borders = [-1, 0, np.nan, 1, 1, np.nan, -1]\n    boundary = [(0, 0), (0, 1), (1, 1), (2, -1)]\n    prj.return_value = (x_borders, y_borders)\n    res = get_polygon(prj, geo_def)\n    Polygon.assert_called_with(boundary)\n    assert res is poly\n\n    # Polygon area is NaN\n    poly.area = np.nan\n    res = get_polygon(prj, geo_def)\n    assert res is None\n\n    # Polygon area is 0.0\n    poly.area = 0.0\n    res = get_polygon(prj, geo_def)\n    assert res is None\n\n\n@mock.patch(\'pyresample.gradient.one_step_gradient_search\')\ndef test_gradient_resample_data(one_step_gradient_search):\n    """"""Test that one_step_gradient_search() is called with proper array shapes.""""""\n    from pyresample.gradient import _gradient_resample_data\n\n    ndim_3 = np.zeros((3, 3, 4))\n    ndim_2a = np.zeros((3, 4))\n    ndim_2b = np.zeros((8, 10))\n\n    # One of the source arrays has wrong shape\n    try:\n        _ = _gradient_resample_data(ndim_3, ndim_2a, ndim_2b, ndim_2a, ndim_2a,\n                                    ndim_2a, ndim_2a, ndim_2b, ndim_2b)\n        raise IndexError\n    except AssertionError:\n        pass\n    one_step_gradient_search.assert_not_called()\n\n    # Data array has wrong shape\n    try:\n        _ = _gradient_resample_data(ndim_2a, ndim_2a, ndim_2a, ndim_2a, ndim_2a,\n                                    ndim_2a, ndim_2a, ndim_2b, ndim_2b)\n        raise IndexError\n    except AssertionError:\n        pass\n    one_step_gradient_search.assert_not_called()\n\n    # The destination x and y arrays have different shapes\n    try:\n        _ = _gradient_resample_data(ndim_3, ndim_2a, ndim_2a, ndim_2a, ndim_2a,\n                                    ndim_2a, ndim_2a, ndim_2b, ndim_2a)\n        raise IndexError\n    except AssertionError:\n        pass\n    one_step_gradient_search.assert_not_called()\n\n    # Correct shapes are given\n    _ = _gradient_resample_data(ndim_3, ndim_2a, ndim_2a, ndim_2a, ndim_2a,\n                                ndim_2a, ndim_2a, ndim_2b, ndim_2b)\n    one_step_gradient_search.assert_called_once()\n\n\n@mock.patch(\'pyresample.gradient.dask.delayed\')\n@mock.patch(\'pyresample.gradient._concatenate_chunks\')\n@mock.patch(\'pyresample.gradient.da\')\ndef test_parallel_gradient_search(dask_da, _concatenate_chunks, delayed):\n    """"""Test calling parallel_gradient_search().""""""\n    from pyresample.gradient import parallel_gradient_search\n\n    def mock_cc(chunks):\n        """"""Return the input.""""""\n        return chunks\n\n    _concatenate_chunks.side_effect = mock_cc\n\n    # Mismatch in number of bands raises ValueError\n    data = [np.zeros((1, 5, 5)), np.zeros((2, 5, 5))]\n    try:\n        parallel_gradient_search(data, None, None, None, None,\n                                 None, None, None, None, None, None)\n        raise\n    except ValueError:\n        pass\n\n    data = [np.zeros((1, 5, 4)), np.ones((1, 5, 4)), None, None]\n    src_x, src_y = [1, 2, 3, 4], [4, 5, 6, 4]\n    # dst_x is used to check the target area shape, so needs ""valid""\n    # data.  The last values shouldn\'t matter as data[-2:] are None\n    # and should be skipped.\n    dst_x = [np.zeros((5, 5)), np.zeros((5, 5)), \'foo\', \'bar\']\n    dst_y = [1, 2, 3, 4]\n    src_gradient_xl, src_gradient_xp = [1, 2, None, None], [1, 2, None, None]\n    src_gradient_yl, src_gradient_yp = [1, 2, None, None], [1, 2, None, None]\n    # Destination slices are used only for padding, so the first two\n    # None values shouldn\'t raise errors\n    dst_slices = [None, None, [1, 2, 1, 3], [1, 3, 1, 4]]\n    # The first two chunks have the same target location, same for the two last\n    dst_mosaic_locations = [(0, 0), (0, 0), (0, 1), (0, 1)]\n\n    res = parallel_gradient_search(data, src_x, src_y, dst_x, dst_y,\n                                   src_gradient_xl, src_gradient_xp,\n                                   src_gradient_yl, src_gradient_yp,\n                                   dst_mosaic_locations, dst_slices,\n                                   method=\'foo\')\n    assert len(res[(0, 0)]) == 2\n    # The second padding shouldn\'t be in the chunks[(0, 1)] list\n    assert len(res[(0, 1)]) == 1\n    _concatenate_chunks.assert_called_with(res)\n    # Two padding arrays\n    assert dask_da.full.call_count == 2\n    assert mock.call((1, 1, 2), np.nan) in dask_da.full.mock_calls\n    assert mock.call((1, 2, 3), np.nan) in dask_da.full.mock_calls\n    # Two resample calls\n    assert dask_da.from_delayed.call_count == 2\n    # The _gradient_resample_data() function has been delayed twice\n    assert \'_gradient_resample_data\' in str(delayed.mock_calls[0])\n    assert \'_gradient_resample_data\' in str(delayed.mock_calls[2])\n    assert str(mock.call()(data[0],\n                           src_x[0], src_y[0],\n                           src_gradient_xl[0], src_gradient_xp[0],\n                           src_gradient_yl[0], src_gradient_yp[0],\n                           dst_x[0], dst_y[0],\n                           method=\'foo\')) == str(delayed.mock_calls[1])\n    assert str(mock.call()(data[1],\n                           src_x[1], src_y[1],\n                           src_gradient_xl[1], src_gradient_xp[1],\n                           src_gradient_yl[1], src_gradient_yp[1],\n                           dst_x[1], dst_y[1],\n                           method=\'foo\')) == str(delayed.mock_calls[3])\n\n\ndef test_concatenate_chunks():\n    """"""Test chunk concatenation for correct results.""""""\n    from pyresample.gradient import _concatenate_chunks\n\n    # 1-band image\n    chunks = {(0, 0): [np.ones((1, 5, 4)), np.zeros((1, 5, 4))],\n              (1, 0): [np.zeros((1, 5, 2))],\n              (1, 1): [np.full((1, 3, 2), 0.5)],\n              (0, 1): [np.full((1, 3, 4), -1)]}\n    res = _concatenate_chunks(chunks).compute(scheduler=\'single-threaded\')\n    assert np.all(res[:5, :4] == 1.0)\n    assert np.all(res[:5, 4:] == 0.0)\n    assert np.all(res[5:, :4] == -1.0)\n    assert np.all(res[5:, 4:] == 0.5)\n    assert res.shape == (8, 6)\n\n    # 3-band image\n    chunks = {(0, 0): [np.ones((3, 5, 4)), np.zeros((3, 5, 4))],\n              (1, 0): [np.zeros((3, 5, 2))],\n              (1, 1): [np.full((3, 3, 2), 0.5)],\n              (0, 1): [np.full((3, 3, 4), -1)]}\n    res = _concatenate_chunks(chunks).compute(scheduler=\'single-threaded\')\n    assert np.all(res[:, :5, :4] == 1.0)\n    assert np.all(res[:, :5, 4:] == 0.0)\n    assert np.all(res[:, 5:, :4] == -1.0)\n    assert np.all(res[:, 5:, 4:] == 0.5)\n    assert res.shape == (3, 8, 6)\n\n\n@mock.patch(\'pyresample.gradient.da\')\ndef test_concatenate_chunks_stack_calls(dask_da):\n    """"""Test that stacking is called the correct times in chunk concatenation.""""""\n    from pyresample.gradient import _concatenate_chunks\n\n    chunks = {(0, 0): [np.ones((1, 5, 4)), np.zeros((1, 5, 4))],\n              (1, 0): [np.zeros((1, 5, 2))],\n              (1, 1): [np.full((1, 3, 2), 0.5)],\n              (0, 1): [np.full((1, 3, 4), -1)]}\n    _ = _concatenate_chunks(chunks)\n    dask_da.stack.assert_called_once_with(chunks[(0, 0)], axis=-1)\n    dask_da.nanmax.assert_called_once()\n    assert \'axis=2\' in str(dask_da.concatenate.mock_calls[-2])\n    assert \'squeeze\' in str(dask_da.concatenate.mock_calls[-1])\n'"
pyresample/test/test_grid.py,24,"b'import unittest\n\nimport numpy as np\n\nfrom pyresample import grid, geometry, utils\n\n\ndef mp(f):\n    f.mp = True\n    return f\n\n\ndef tmp(f):\n    f.tmp = True\n    return f\n\n\nclass Test(unittest.TestCase):\n\n    area_def = geometry.AreaDefinition(\'areaD\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n                                       {\'a\': \'6378144.0\',\n                                        \'b\': \'6356759.0\',\n                                        \'lat_0\': \'50.00\',\n                                        \'lat_ts\': \'50.00\',\n                                        \'lon_0\': \'8.00\',\n                                        \'proj\': \'stere\'},\n                                       800,\n                                       800,\n                                       [-1370912.72,\n                                           -909968.64000000001,\n                                           1029087.28,\n                                           1490031.3600000001])\n\n    area_def2 = geometry.AreaDefinition(\'areaD2\', \'Europe (3km, HRV, VTC)\', \'areaD2\',\n                                        {\'a\': \'6378144.0\',\n                                         \'b\': \'6356759.0\',\n                                         \'lat_0\': \'50.00\',\n                                         \'lat_ts\': \'50.00\',\n                                         \'lon_0\': \'8.00\',\n                                         \'proj\': \'stere\'},\n                                        5,\n                                        5,\n                                        [-1370912.72,\n                                            -909968.64000000001,\n                                            1029087.28,\n                                            1490031.3600000001])\n\n    msg_area = geometry.AreaDefinition(\'msg_full\', \'Full globe MSG image 0 degrees\',\n                                       \'msg_full\',\n                                       {\'a\': \'6378169.0\',\n                                        \'b\': \'6356584.0\',\n                                        \'h\': \'35785831.0\',\n                                        \'lon_0\': \'0\',\n                                        \'proj\': \'geos\'},\n                                       3712,\n                                       3712,\n                                       [-5568742.4000000004,\n                                           -5568742.4000000004,\n                                           5568742.4000000004,\n                                           5568742.4000000004]\n                                       )\n\n    def test_linesample(self):\n        data = np.fromfunction(lambda y, x: y * x, (40, 40))\n        rows = np.array([[1, 2], [3, 4]])\n        cols = np.array([[25, 26], [27, 28]])\n        res = grid.get_image_from_linesample(rows, cols, data)\n        expected = np.array([[25., 52.], [81., 112.]])\n        self.assertTrue(np.array_equal(res, expected), \'Linesample failed\')\n\n    def test_linesample_multi(self):\n        data1 = np.fromfunction(lambda y, x: y * x, (40, 40))\n        data2 = np.fromfunction(lambda y, x: 2 * y * x, (40, 40))\n        data3 = np.fromfunction(lambda y, x: 3 * y * x, (40, 40))\n        data = np.zeros((40, 40, 3))\n        data[:, :, 0] = data1\n        data[:, :, 1] = data2\n        data[:, :, 2] = data3\n        rows = np.array([[1, 2], [3, 4]])\n        cols = np.array([[25, 26], [27, 28]])\n        res = grid.get_image_from_linesample(rows, cols, data)\n        expected = np.array([[[25., 50., 75.],\n                              [52., 104., 156.]],\n                             [[81., 162., 243.],\n                              [112.,  224.,  336.]]])\n        self.assertTrue(np.array_equal(res, expected), \'Linesample failed\')\n\n    def test_from_latlon(self):\n        data = np.fromfunction(lambda y, x: y * x, (800, 800))\n        lons = np.fromfunction(lambda y, x: x, (10, 10))\n        lats = np.fromfunction(lambda y, x: 50 - (5.0 / 10) * y, (10, 10))\n        #source_def = grid.AreaDefinition.get_from_area_def(self.area_def)\n        source_def = self.area_def\n        res = grid.get_image_from_lonlats(lons, lats, source_def, data)\n        expected = np.array([[129276.,  141032.,  153370.,  165804.,  178334.,  190575.,\n                              202864.,  214768.,  226176.,  238080.],\n                             [133056.,  146016.,  158808.,  171696.,  184320.,  196992.,\n                              209712.,  222480.,  234840.,  247715.],\n                             [137026.,  150150.,  163370.,  177215.,  190629.,  203756.,\n                              217464.,  230256.,  243048.,  256373.],\n                             [140660.,  154496.,  168714.,  182484.,  196542.,  210650.,\n                              224257.,  238464.,  251712.,  265512.],\n                             [144480.,  158484.,  173148.,  187912.,  202776.,  217358.,\n                              231990.,  246240.,  259920.,  274170.],\n                             [147968.,  163261.,  178398.,  193635.,  208616.,  223647.,\n                              238728.,  253859.,  268584.,  283898.],\n                             [151638.,  167121.,  182704.,  198990.,  214775.,  230280.,\n                              246442.,  261617.,  276792.,  292574.],\n                             [154980.,  171186.,  187860.,  204016.,  220542.,  237120.,\n                              253125.,  269806.,  285456.,  301732.],\n                             [158500.,  175536.,  192038.,  209280.,  226626.,  243697.,\n                              260820.,  277564.,  293664.,  310408.],\n                             [161696.,  179470.,  197100.,  214834.,  232320.,  250236.,\n                              267448.,  285090.,  302328.,  320229.]])\n        self.assertTrue(\n            np.array_equal(res, expected), \'Sampling from lat lon failed\')\n\n    def test_proj_coords(self):\n        #res = grid.get_proj_coords(self.area_def2)\n        res = self.area_def2.get_proj_coords()\n        cross_sum = res[0].sum() + res[1].sum()\n        expected = 2977965.9999999963\n        self.assertAlmostEqual(\n            cross_sum, expected, msg=\'Calculation of proj coords failed\')\n\n    def test_latlons(self):\n        #res = grid.get_lonlats(self.area_def2)\n        res = self.area_def2.get_lonlats()\n        cross_sum = res[0].sum() + res[1].sum()\n        expected = 1440.8280578215431\n        self.assertAlmostEqual(\n            cross_sum, expected, msg=\'Calculation of lat lons failed\')\n\n    @mp\n    def test_latlons_mp(self):\n        #res = grid.get_lonlats(self.area_def2, nprocs=2)\n        res = self.area_def2.get_lonlats(nprocs=2)\n        cross_sum = res[0].sum() + res[1].sum()\n        expected = 1440.8280578215431\n        self.assertAlmostEqual(\n            cross_sum, expected, msg=\'Calculation of lat lons failed\')\n\n    def test_resampled_image(self):\n        data = np.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        target_def = self.area_def\n        source_def = self.msg_area\n        res = grid.get_resampled_image(\n            target_def, source_def, data, segments=1)\n        cross_sum = res.sum()\n        expected = 399936.39392500359\n        self.assertAlmostEqual(\n            cross_sum, expected, msg=\'Resampling of image failed\')\n\n    def test_resampled_image_masked(self):\n        # Generate test image with masked elements\n        data = np.ma.ones(self.msg_area.shape)\n        data.mask = np.zeros(data.shape)\n        data.mask[253:400, 1970:2211] = 1\n\n        # Resample image using multiple segments\n        target_def = self.area_def\n        source_def = self.msg_area\n        res = grid.get_resampled_image(\n            target_def, source_def, data, segments=4, fill_value=None)\n\n        # Make sure the mask has been preserved\n        self.assertGreater(res.mask.sum(), 0,\n                           msg=\'Resampling did not preserve the mask\')\n\n    @tmp\n    def test_generate_linesample(self):\n        data = np.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        row_indices, col_indices = utils.generate_quick_linesample_arrays(self.msg_area,\n                                                                          self.area_def)\n        res = data[row_indices, col_indices]\n        cross_sum = res.sum()\n        expected = 399936.39392500359\n        self.assertAlmostEqual(\n            cross_sum, expected, msg=\'Generate linesample failed\')\n        self.assertFalse(row_indices.dtype != np.uint16 or col_indices.dtype != np.uint16,\n                         \'Generate linesample failed. Downcast to uint16 expected\')\n\n    @mp\n    def test_resampled_image_mp(self):\n        data = np.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        target_def = self.area_def\n        source_def = self.msg_area\n        res = grid.get_resampled_image(\n            target_def, source_def, data, nprocs=2, segments=1)\n        cross_sum = res.sum()\n        expected = 399936.39392500359\n        self.assertAlmostEqual(\n            cross_sum, expected, msg=\'Resampling of image mp failed\')\n\n    def test_single_lonlat(self):\n        lon, lat = self.area_def.get_lonlat(400, 400)\n        self.assertAlmostEqual(\n            lon, 5.5028467120975835, msg=\'Resampling of single lon failed\')\n        self.assertAlmostEqual(\n            lat, 52.566998432390619, msg=\'Resampling of single lat failed\')\n\n    def test_proj4_string(self):\n        """"""Test \'proj_str\' property of AreaDefinition.""""""\n        from pyresample.utils import is_pyproj2\n        proj4_string = self.area_def.proj_str\n        expected_string = \'+a=6378144.0 +b=6356759.0 +lat_ts=50.0 +lon_0=8.0 +proj=stere +lat_0=50.0\'\n        if is_pyproj2():\n            expected_string = \'+a=6378144 +k=1 +lat_0=50 +lon_0=8 \' \\\n                              \'+no_defs +proj=stere +rf=298.253168108487 \' \\\n                              \'+type=crs +units=m +x_0=0 +y_0=0\'\n        self.assertEqual(\n            frozenset(proj4_string.split()), frozenset(expected_string.split()))\n'"
pyresample/test/test_image.py,0,"b""import os\nimport unittest\n\nimport numpy\n\nfrom pyresample import image, geometry, utils\n\n\ndef mask(f):\n    f.mask = True\n    return f\n\n\ndef tmp(f):\n    f.tmp = True\n    return f\n\n\nclass Test(unittest.TestCase):\n\n    area_def = geometry.AreaDefinition('areaD', 'Europe (3km, HRV, VTC)',\n                                       'areaD',\n                                       {'a': '6378144.0',\n                                        'b': '6356759.0',\n                                        'lat_0': '50.00',\n                                        'lat_ts': '50.00',\n                                        'lon_0': '8.00',\n                                        'proj': 'stere'},\n                                       800,\n                                       800,\n                                       [-1370912.72,\n                                        -909968.64000000001,\n                                        1029087.28,\n                                        1490031.3600000001])\n\n    msg_area = geometry.AreaDefinition('msg_full',\n                                       'Full globe MSG image 0 degrees',\n                                       'msg_full',\n                                       {'a': '6378169.0',\n                                        'b': '6356584.0',\n                                        'h': '35785831.0',\n                                        'lon_0': '0',\n                                        'proj': 'geos'},\n                                       3712,\n                                       3712,\n                                       [-5568742.4000000004,\n                                        -5568742.4000000004,\n                                        5568742.4000000004,\n                                        5568742.4000000004])\n\n    msg_area_resize = geometry.AreaDefinition('msg_full',\n                                              'Full globe MSG image 0 degrees',\n                                              'msg_full',\n                                              {'a': '6378169.0',\n                                               'b': '6356584.0',\n                                               'h': '35785831.0',\n                                               'lon_0': '0',\n                                               'proj': 'geos'},\n                                              928,\n                                              928,\n                                              [-5568742.4000000004,\n                                               -5568742.4000000004,\n                                               5568742.4000000004,\n                                               5568742.4000000004])\n\n    @tmp\n    def test_image(self):\n        data = numpy.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        msg_con = image.ImageContainerQuick(data, self.msg_area, segments=1)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum = res.sum()\n        expected = 399936.39392500359\n        self.assertAlmostEqual(cross_sum, expected)\n\n    @tmp\n    def test_image_segments(self):\n        data = numpy.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        msg_con = image.ImageContainerQuick(data, self.msg_area, segments=8)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum = res.sum()\n        expected = 399936.39392500359\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_return_type(self):\n        data = numpy.ones((3712, 3712)).astype('int')\n        msg_con = image.ImageContainerQuick(data, self.msg_area, segments=1)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        self.assertTrue(data.dtype is res.dtype)\n\n    @mask\n    def test_masked_image(self):\n        data = numpy.zeros((3712, 3712))\n        mask = numpy.zeros((3712, 3712))\n        mask[:, 1865:] = 1\n        data_masked = numpy.ma.array(data, mask=mask)\n        msg_con = image.ImageContainerQuick(\n            data_masked, self.msg_area, segments=1)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        resampled_mask = res.mask.astype('int')\n        expected = numpy.fromfile(os.path.join(os.path.dirname(__file__),\n                                               'test_files', 'mask_grid.dat'),\n                                  sep=' ').reshape((800, 800))\n        self.assertTrue(numpy.array_equal(resampled_mask, expected))\n\n    @mask\n    def test_masked_image_fill(self):\n        data = numpy.zeros((3712, 3712))\n        mask = numpy.zeros((3712, 3712))\n        mask[:, 1865:] = 1\n        data_masked = numpy.ma.array(data, mask=mask)\n        msg_con = image.ImageContainerQuick(data_masked, self.msg_area,\n                                            fill_value=None, segments=1)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        resampled_mask = res.mask.astype('int')\n        expected = numpy.fromfile(os.path.join(os.path.dirname(__file__),\n                                               'test_files',\n                                               'mask_grid.dat'),\n                                  sep=' ').reshape((800, 800))\n        self.assertTrue(numpy.array_equal(resampled_mask, expected))\n\n    def test_nearest_neighbour(self):\n        data = numpy.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        msg_con = image.ImageContainerNearest(\n            data, self.msg_area, 50000, segments=1)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum = res.sum()\n        expected = 399936.70287099993\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_nearest_resize(self):\n        data = numpy.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        msg_con = image.ImageContainerNearest(\n            data, self.msg_area, 50000, segments=1)\n        area_con = msg_con.resample(self.msg_area_resize)\n        res = area_con.image_data\n        cross_sum = res.sum()\n        expected = 2212023.0175830\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_nearest_neighbour_multi(self):\n        data1 = numpy.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        data2 = numpy.fromfunction(\n            lambda y, x: y * x * 10 ** -6, (3712, 3712)) * 2\n        data = numpy.dstack((data1, data2))\n        msg_con = image.ImageContainerNearest(\n            data, self.msg_area, 50000, segments=1)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum1 = res[:, :, 0].sum()\n        expected1 = 399936.70287099993\n        self.assertAlmostEqual(cross_sum1, expected1)\n\n        cross_sum2 = res[:, :, 1].sum()\n        expected2 = 399936.70287099993 * 2\n        self.assertAlmostEqual(cross_sum2, expected2)\n\n    def test_nearest_neighbour_multi_preproc(self):\n        data1 = numpy.fromfunction(lambda y, x: y * x * 10 ** -6, (3712, 3712))\n        data2 = numpy.fromfunction(\n            lambda y, x: y * x * 10 ** -6, (3712, 3712)) * 2\n        data = numpy.dstack((data1, data2))\n        msg_con = image.ImageContainer(data, self.msg_area)\n        # area_con = msg_con.resample_area_nearest_neighbour(self.area_def,\n        # 50000)\n        row_indices, col_indices = \\\n            utils.generate_nearest_neighbour_linesample_arrays(self.msg_area,\n                                                               self.area_def,\n                                                               50000)\n        res = msg_con.get_array_from_linesample(row_indices, col_indices)\n        cross_sum1 = res[:, :, 0].sum()\n        expected1 = 399936.70287099993\n        self.assertAlmostEqual(cross_sum1, expected1)\n\n        cross_sum2 = res[:, :, 1].sum()\n        expected2 = 399936.70287099993 * 2\n        self.assertAlmostEqual(cross_sum2, expected2)\n\n    def test_nearest_swath(self):\n        data = numpy.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = numpy.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = numpy.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        swath_con = image.ImageContainerNearest(\n            data, swath_def, 50000, segments=1)\n        area_con = swath_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum = res.sum()\n        expected = 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_swath_segments(self):\n        data = numpy.fromfunction(lambda y, x: y * x, (50, 10))\n        data = numpy.dstack(3 * (data,))\n        lons = numpy.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = numpy.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        swath_con = image.ImageContainerNearest(\n            data, swath_def, 50000, segments=2)\n        area_con = swath_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum = res.sum()\n        expected = 3 * 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_bilinear(self):\n        data = numpy.fromfunction(lambda y, x: y * x * 10 ** -6, (928, 928))\n        msg_con = image.ImageContainerBilinear(data, self.msg_area_resize,\n                                               50000, segments=1,\n                                               neighbours=8)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum = res.sum()\n        expected = 24712.589910252744\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_bilinear_multi(self):\n        data1 = numpy.fromfunction(lambda y, x: y * x * 10 ** -6, (928, 928))\n        data2 = numpy.fromfunction(lambda y, x: y * x * 10 ** -6,\n                                   (928, 928)) * 2\n        data = numpy.dstack((data1, data2))\n        msg_con = image.ImageContainerBilinear(data, self.msg_area_resize,\n                                               50000, segments=1,\n                                               neighbours=8)\n        area_con = msg_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum1 = res[:, :, 0].sum()\n        expected1 = 24712.589910252744\n        self.assertAlmostEqual(cross_sum1, expected1)\n        cross_sum2 = res[:, :, 1].sum()\n        expected2 = 24712.589910252744 * 2\n        self.assertAlmostEqual(cross_sum2, expected2)\n\n    def test_bilinear_swath(self):\n        data = numpy.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = numpy.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = numpy.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        swath_con = image.ImageContainerBilinear(data, swath_def, 500000,\n                                                 segments=1, neighbours=8)\n        area_con = swath_con.resample(self.area_def)\n        res = area_con.image_data\n        cross_sum = res.sum()\n        expected = 16852120.789503865\n        self.assertAlmostEqual(cross_sum, expected)\n"""
pyresample/test/test_kd_tree.py,183,"b'import os\nimport numpy as np\n\nfrom pyresample import geometry, kd_tree, utils\nfrom pyresample.test.utils import catch_warnings\n\nimport unittest\nfrom unittest import mock\n\n\nclass Test(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        cls.area_def = geometry.AreaDefinition(\'areaD\',\n                                               \'Europe (3km, HRV, VTC)\',\n                                               \'areaD\',\n                                               {\'a\': \'6378144.0\',\n                                                \'b\': \'6356759.0\',\n                                                \'lat_0\': \'50.00\',\n                                                \'lat_ts\': \'50.00\',\n                                                \'lon_0\': \'8.00\',\n                                                \'proj\': \'stere\'},\n                                               800,\n                                               800,\n                                               [-1370912.72,\n                                                   -909968.64000000001,\n                                                   1029087.28,\n                                                   1490031.3600000001])\n\n        cls.tdata = np.array([1, 2, 3])\n        cls.tlons = np.array([11.280789, 12.649354, 12.080402])\n        cls.tlats = np.array([56.011037, 55.629675, 55.641535])\n        cls.tswath = geometry.SwathDefinition(lons=cls.tlons, lats=cls.tlats)\n        cls.tgrid = geometry.CoordinateDefinition(\n            lons=np.array([12.562036]), lats=np.array([55.715613]))\n\n    def test_nearest_base(self):\n        res = kd_tree.resample_nearest(self.tswath,\n                                       self.tdata.ravel(), self.tgrid,\n                                       100000, reduce_data=False, segments=1)\n        self.assertTrue(res[0] == 2)\n\n    def test_gauss_base(self):\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_gauss(self.tswath,\n                                         self.tdata.ravel(), self.tgrid,\n                                         50000, 25000, reduce_data=False, segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse((\'Searching\' not in str(w[0].message)))\n        self.assertAlmostEqual(res[0], 2.2020729, 5)\n\n    def test_custom_base(self):\n        def wf(dist):\n            return 1 - dist / 100000.0\n\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_custom(self.tswath,\n                                          self.tdata.ravel(), self.tgrid,\n                                          50000, wf, reduce_data=False, segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse((\'Searching\' not in str(w[0].message)))\n        self.assertAlmostEqual(res[0], 2.4356757, 5)\n\n    def test_gauss_uncert(self):\n        sigma = utils.fwhm2sigma(41627.730557884883)\n        with catch_warnings(UserWarning) as w:\n            res, stddev, count = kd_tree.resample_gauss(self.tswath, self.tdata,\n                                                        self.tgrid, 100000, sigma,\n                                                        with_uncert=True)\n            self.assertTrue(len(w) > 0)\n            self.assertTrue((any(\'Searching\' in str(_w.message) for _w in w)))\n\n        expected_res = 2.20206560694\n        expected_stddev = 0.707115076173\n        expected_count = 3\n        self.assertAlmostEqual(res[0], expected_res, 5)\n        self.assertAlmostEqual(stddev[0], expected_stddev, 5)\n        self.assertEqual(count[0], expected_count)\n\n    def test_custom_uncert(self):\n        def wf(dist):\n            return 1 - dist / 100000.0\n\n        with catch_warnings(UserWarning) as w:\n            res, stddev, counts = kd_tree.resample_custom(self.tswath,\n                                                          self.tdata, self.tgrid,\n                                                          100000, wf, with_uncert=True)\n            self.assertTrue(len(w) > 0)\n            self.assertTrue((any(\'Searching\' in str(_w.message) for _w in w)))\n\n        self.assertAlmostEqual(res[0], 2.32193149, 5)\n        self.assertAlmostEqual(stddev[0], 0.81817972, 5)\n        self.assertEqual(counts[0], 3)\n\n    def test_nearest(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       self.area_def, 50000, segments=1)\n        cross_sum = res.sum()\n        expected = 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_masked_swath_target(self):\n        """"""Test that a masked array works as a target.""""""\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        mask = np.ones_like(lons, dtype=np.bool)\n        mask[::2, ::2] = False\n        swath_def = geometry.SwathDefinition(\n            lons=np.ma.masked_array(lons, mask=mask),\n            lats=np.ma.masked_array(lats, mask=False)\n        )\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       swath_def, 50000, segments=3)\n        cross_sum = res.sum()\n        # expected = 12716  # if masks aren\'t respected\n        expected = 12000\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_1d(self):\n        data = np.fromfunction(lambda x, y: x * y, (800, 800))\n        lons = np.fromfunction(lambda x: 3 + x / 100., (500,))\n        lats = np.fromfunction(lambda x: 75 - x / 10., (500,))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(self.area_def, data.ravel(),\n                                       swath_def, 50000, segments=1)\n        cross_sum = res.sum()\n        expected = 35821299.0\n        self.assertEqual(res.shape, (500,))\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_empty(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 165 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       self.area_def, 50000, segments=1)\n        cross_sum = res.sum()\n        expected = 0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_empty_multi(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 165 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data_multi,\n                                       self.area_def, 50000, segments=1)\n        self.assertEqual(res.shape, (800, 800, 3),\n                         msg=\'Swath resampling nearest empty multi failed\')\n\n    def test_nearest_empty_multi_masked(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 165 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data_multi,\n                                       self.area_def, 50000, segments=1,\n                                       fill_value=None)\n        self.assertEqual(res.shape, (800, 800, 3))\n\n    def test_nearest_empty_masked(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 165 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       self.area_def, 50000, segments=1,\n                                       fill_value=None)\n        cross_sum = res.mask.sum()\n        expected = res.size\n        self.assertTrue(cross_sum == expected)\n\n    def test_nearest_segments(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       self.area_def, 50000, segments=2)\n        cross_sum = res.sum()\n        expected = 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_remap(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       self.area_def, 50000, segments=1)\n        remap = kd_tree.resample_nearest(self.area_def, res.ravel(),\n                                         swath_def, 5000, segments=1)\n        cross_sum = remap.sum()\n        expected = 22275.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_mp(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       self.area_def, 50000, nprocs=2, segments=1)\n        cross_sum = res.sum()\n        expected = 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_multi(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        res = kd_tree.resample_nearest(swath_def, data_multi,\n                                       self.area_def, 50000, segments=1)\n        cross_sum = res.sum()\n        expected = 3 * 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_multi_unraveled(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.dstack((data, data, data))\n        res = kd_tree.resample_nearest(swath_def, data_multi,\n                                       self.area_def, 50000, segments=1)\n        cross_sum = res.sum()\n        expected = 3 * 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_gauss_sparse(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_gauss(swath_def, data.ravel(),\n                                     self.area_def, 50000, 25000, fill_value=-1, segments=1)\n        cross_sum = res.sum()\n        expected = 15387753.9852\n        self.assertAlmostEqual(cross_sum, expected, places=3)\n\n    def test_gauss(self):\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -5, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_gauss(swath_def, data.ravel(),\n                                         self.area_def, 50000, 25000, segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse((\'Possible more\' not in str(w[0].message)))\n        cross_sum = res.sum()\n        expected = 4872.8100353517921\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_gauss_fwhm(self):\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -5, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_gauss(swath_def, data.ravel(),\n                                         self.area_def, 50000, utils.fwhm2sigma(41627.730557884883), segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse((\'Possible more\' not in str(w[0].message)))\n        cross_sum = res.sum()\n        expected = 4872.8100353517921\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_gauss_multi(self):\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -6, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_gauss(swath_def, data_multi,\n                                         self.area_def, 50000, [25000, 15000, 10000], segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse((\'Possible more\' not in str(w[0].message)))\n        cross_sum = res.sum()\n        expected = 1461.8429990248171\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_gauss_multi_uncert(self):\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -6, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        with catch_warnings(UserWarning) as w:\n            # The assertion below checks if there is only one warning raised\n            # and whether it contains a specific message from pyresample\n            # On python 2.7.9+ the resample_gauss method raises multiple deprecation warnings\n            # that cause to fail, so we ignore the unrelated warnings.\n            res, stddev, counts = kd_tree.resample_gauss(swath_def, data_multi,\n                                                         self.area_def, 50000, [\n                                                             25000, 15000, 10000],\n                                                         segments=1, with_uncert=True)\n            self.assertTrue(len(w) >= 1)\n            self.assertTrue(\n                any([\'Possible more\' in str(x.message) for x in w]))\n        cross_sum = res.sum()\n        cross_sum_counts = counts.sum()\n        expected = 1461.8429990248171\n        expected_stddev = [0.44621800779801657, 0.44363137712896705,\n                           0.43861019464274459]\n        expected_counts = 4934802.0\n        self.assertTrue(res.shape == stddev.shape and stddev.shape ==\n                        counts.shape and counts.shape == (800, 800, 3))\n        self.assertAlmostEqual(cross_sum, expected)\n\n        for i, e_stddev in enumerate(expected_stddev):\n            cross_sum_stddev = stddev[:, :, i].sum()\n            self.assertAlmostEqual(cross_sum_stddev, e_stddev)\n        self.assertAlmostEqual(cross_sum_counts, expected_counts)\n\n    def test_gauss_multi_mp(self):\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -6, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_gauss(swath_def, data_multi,\n                                         self.area_def, 50000, [\n                                             25000, 15000, 10000],\n                                         nprocs=2, segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse((\'Possible more\' not in str(w[0].message)))\n        cross_sum = res.sum()\n        expected = 1461.8429990248171\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_gauss_multi_mp_segments(self):\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -6, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_gauss(swath_def, data_multi,\n                                         self.area_def, 50000, [\n                                             25000, 15000, 10000],\n                                         nprocs=2, segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse(\'Possible more\' not in str(w[0].message))\n        cross_sum = res.sum()\n        expected = 1461.8429990248171\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_gauss_multi_mp_segments_empty(self):\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -6, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 165 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        res = kd_tree.resample_gauss(swath_def, data_multi,\n                                     self.area_def, 50000, [\n                                         25000, 15000, 10000],\n                                     nprocs=2, segments=1)\n        cross_sum = res.sum()\n        self.assertTrue(cross_sum == 0)\n\n    def test_custom(self):\n        def wf(dist):\n            return 1 - dist / 100000.0\n\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -5, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_custom(swath_def, data.ravel(),\n                                          self.area_def, 50000, wf, segments=1)\n            # PyProj proj/CRS and ""more than 8 neighbours"" are warned about\n            self.assertFalse(len(w) > 2)\n            neighbour_warn = False\n            for warn in w:\n                if \'Possible more\' in str(warn.message):\n                    neighbour_warn = True\n                    break\n            self.assertTrue(neighbour_warn)\n            if len(w) == 2:\n                proj_crs_warn = False\n                for warn in w:\n                    if \'important projection information\' in str(warn.message):\n                        proj_crs_warn = True\n                        break\n                self.assertTrue(proj_crs_warn)\n\n        cross_sum = res.sum()\n        expected = 4872.8100347930776\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_custom_multi(self):\n        def wf1(dist):\n            return 1 - dist / 100000.0\n\n        def wf2(dist):\n            return 1\n\n        def wf3(dist):\n            return np.cos(dist) ** 2\n\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -6, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n        with catch_warnings(UserWarning) as w:\n            res = kd_tree.resample_custom(swath_def, data_multi,\n                                          self.area_def, 50000, [wf1, wf2, wf3], segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse(\'Possible more\' not in str(w[0].message))\n        cross_sum = res.sum()\n        expected = 1461.8428378742638\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_masked_nearest(self):\n        data = np.ones((50, 10))\n        data[:, 5:] = 2\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        mask = np.ones((50, 10))\n        mask[:, :5] = 0\n        masked_data = np.ma.array(data, mask=mask)\n        res = kd_tree.resample_nearest(swath_def, masked_data.ravel(),\n                                       self.area_def, 50000, segments=1)\n        expected_mask = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                 \'test_files\',\n                                                 \'mask_test_nearest_mask.dat\'),\n                                    sep=\' \').reshape((800, 800))\n        expected_data = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                 \'test_files\',\n                                                 \'mask_test_nearest_data.dat\'),\n                                    sep=\' \').reshape((800, 800))\n        self.assertTrue(np.array_equal(expected_mask, res.mask))\n        self.assertTrue(np.array_equal(expected_data, res.data))\n\n    def test_masked_nearest_1d(self):\n        data = np.ones((800, 800))\n        data[:400, :] = 2\n        lons = np.fromfunction(lambda x: 3 + x / 100., (500,))\n        lats = np.fromfunction(lambda x: 75 - x / 10., (500,))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        mask = np.ones((800, 800))\n        mask[400:, :] = 0\n        masked_data = np.ma.array(data, mask=mask)\n        res = kd_tree.resample_nearest(self.area_def, masked_data.ravel(),\n                                       swath_def, 50000, segments=1)\n        self.assertEqual(res.mask.sum(), 112)\n\n    def test_masked_gauss(self):\n        data = np.ones((50, 10))\n        data[:, 5:] = 2\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        mask = np.ones((50, 10))\n        mask[:, :5] = 0\n        masked_data = np.ma.array(data, mask=mask)\n        res = kd_tree.resample_gauss(swath_def, masked_data.ravel(),\n                                     self.area_def, 50000, 25000, segments=1)\n        expected_mask = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                 \'test_files\',\n                                                 \'mask_test_mask.dat\'),\n                                    sep=\' \').reshape((800, 800))\n        expected_data = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                 \'test_files\',\n                                                 \'mask_test_data.dat\'),\n                                    sep=\' \').reshape((800, 800))\n        expected = expected_data.sum()\n        cross_sum = res.data.sum()\n\n        self.assertTrue(np.array_equal(expected_mask, res.mask))\n        self.assertAlmostEqual(cross_sum, expected, places=3)\n\n    def test_masked_fill_float(self):\n        data = np.ones((50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       self.area_def, 50000, fill_value=None, segments=1)\n        expected_fill_mask = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                      \'test_files\',\n                                                      \'mask_test_fill_value.dat\'),\n                                         sep=\' \').reshape((800, 800))\n        fill_mask = res.mask\n        self.assertTrue(np.array_equal(fill_mask, expected_fill_mask))\n\n    def test_masked_fill_int(self):\n        data = np.ones((50, 10)).astype(\'int\')\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def, data.ravel(),\n                                       self.area_def, 50000, fill_value=None, segments=1)\n        expected_fill_mask = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                      \'test_files\',\n                                                      \'mask_test_fill_value.dat\'),\n                                         sep=\' \').reshape((800, 800))\n        fill_mask = res.mask\n        self.assertTrue(np.array_equal(fill_mask, expected_fill_mask))\n\n    def test_masked_full(self):\n        data = np.ones((50, 10))\n        data[:, 5:] = 2\n        mask = np.ones((50, 10))\n        mask[:, :5] = 0\n        masked_data = np.ma.array(data, mask=mask)\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def,\n                                       masked_data.ravel(\n                                       ), self.area_def, 50000,\n                                       fill_value=None, segments=1)\n        expected_fill_mask = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                      \'test_files\',\n                                                      \'mask_test_full_fill.dat\'),\n                                         sep=\' \').reshape((800, 800))\n        fill_mask = res.mask\n\n        self.assertTrue(np.array_equal(fill_mask, expected_fill_mask))\n\n    def test_masked_full_multi(self):\n        data = np.ones((50, 10))\n        data[:, 5:] = 2\n        mask1 = np.ones((50, 10))\n        mask1[:, :5] = 0\n        mask2 = np.ones((50, 10))\n        mask2[:, 5:] = 0\n        mask3 = np.ones((50, 10))\n        mask3[:25, :] = 0\n        data_multi = np.column_stack(\n            (data.ravel(), data.ravel(), data.ravel()))\n        mask_multi = np.column_stack(\n            (mask1.ravel(), mask2.ravel(), mask3.ravel()))\n        masked_data = np.ma.array(data_multi, mask=mask_multi)\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        res = kd_tree.resample_nearest(swath_def,\n                                       masked_data, self.area_def, 50000,\n                                       fill_value=None, segments=1)\n        expected_fill_mask = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                      \'test_files\',\n                                                      \'mask_test_full_fill_multi.dat\'),\n                                         sep=\' \').reshape((800, 800, 3))\n        fill_mask = res.mask\n        cross_sum = res.sum()\n        expected = 357140.0\n        self.assertAlmostEqual(cross_sum, expected)\n        self.assertTrue(np.array_equal(fill_mask, expected_fill_mask))\n\n    def test_dtype(self):\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        grid_def = geometry.GridDefinition(lons, lats)\n        lons = np.asarray(lons, dtype=\'f4\')\n        lats = np.asarray(lats, dtype=\'f4\')\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        valid_input_index, valid_output_index, index_array, distance_array = \\\n            kd_tree.get_neighbour_info(swath_def,\n                                       grid_def,\n                                       50000, neighbours=1, segments=1)\n\n    def test_nearest_from_sample(self):\n        data = np.fromfunction(lambda y, x: y * x, (50, 10))\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        valid_input_index, valid_output_index, index_array, distance_array = \\\n            kd_tree.get_neighbour_info(swath_def,\n                                       self.area_def,\n                                       50000, neighbours=1, segments=1)\n        res = kd_tree.get_sample_from_neighbour_info(\'nn\', (800, 800), data.ravel(),\n                                                     valid_input_index, valid_output_index,\n                                                     index_array)\n        cross_sum = res.sum()\n        expected = 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_from_sample_np_dtypes(self):\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        valid_input_index, valid_output_index, index_array, distance_array = \\\n            kd_tree.get_neighbour_info(swath_def,\n                                       self.area_def,\n                                       50000, neighbours=1, segments=1)\n\n        for dtype in [np.uint16, np.float32]:\n            with self.subTest(dtype):\n                data = np.fromfunction(lambda y, x: y * x, (50, 10)).astype(dtype)\n                fill_value = dtype(0.0)\n                res = \\\n                    kd_tree.get_sample_from_neighbour_info(\'nn\', (800, 800),\n                                                           data.ravel(),\n                                                           valid_input_index,\n                                                           valid_output_index,\n                                                           index_array,\n                                                           fill_value=fill_value)\n                cross_sum = res.sum()\n                expected = 15874591.0\n                self.assertEqual(cross_sum, expected)\n\n    def test_custom_multi_from_sample(self):\n        def wf1(dist):\n            return 1 - dist / 100000.0\n\n        def wf2(dist):\n            return 1\n\n        def wf3(dist):\n            return np.cos(dist) ** 2\n\n        data = np.fromfunction(lambda y, x: (y + x) * 10 ** -6, (5000, 100))\n        lons = np.fromfunction(\n            lambda y, x: 3 + (10.0 / 100) * x, (5000, 100))\n        lats = np.fromfunction(\n            lambda y, x: 75 - (50.0 / 5000) * y, (5000, 100))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        data_multi = np.column_stack((data.ravel(), data.ravel(),\n                                      data.ravel()))\n\n        with catch_warnings(UserWarning) as w:\n            valid_input_index, valid_output_index, index_array, distance_array = \\\n                kd_tree.get_neighbour_info(swath_def,\n                                           self.area_def,\n                                           50000, segments=1)\n            self.assertFalse(len(w) != 1)\n            self.assertFalse((\'Possible more\' not in str(w[0].message)))\n\n        res = kd_tree.get_sample_from_neighbour_info(\'custom\', (800, 800),\n                                                     data_multi,\n                                                     valid_input_index, valid_output_index,\n                                                     index_array, distance_array,\n                                                     weight_funcs=[wf1, wf2, wf3])\n\n        cross_sum = res.sum()\n\n        expected = 1461.8428378742638\n        self.assertAlmostEqual(cross_sum, expected)\n        res = kd_tree.get_sample_from_neighbour_info(\'custom\', (800, 800),\n                                                     data_multi,\n                                                     valid_input_index, valid_output_index,\n                                                     index_array, distance_array,\n                                                     weight_funcs=[wf1, wf2, wf3])\n\n        # Look for error where input data has been manipulated\n        cross_sum = res.sum()\n        expected = 1461.8428378742638\n        self.assertAlmostEqual(cross_sum, expected)\n\n    def test_masked_multi_from_sample(self):\n        data = np.ones((50, 10))\n        data[:, 5:] = 2\n        mask1 = np.ones((50, 10))\n        mask1[:, :5] = 0\n        mask2 = np.ones((50, 10))\n        mask2[:, 5:] = 0\n        mask3 = np.ones((50, 10))\n        mask3[:25, :] = 0\n        data_multi = np.column_stack(\n            (data.ravel(), data.ravel(), data.ravel()))\n        mask_multi = np.column_stack(\n            (mask1.ravel(), mask2.ravel(), mask3.ravel()))\n        masked_data = np.ma.array(data_multi, mask=mask_multi)\n        lons = np.fromfunction(lambda y, x: 3 + x, (50, 10))\n        lats = np.fromfunction(lambda y, x: 75 - y, (50, 10))\n        swath_def = geometry.SwathDefinition(lons=lons, lats=lats)\n        valid_input_index, valid_output_index, index_array, distance_array = \\\n            kd_tree.get_neighbour_info(swath_def,\n                                       self.area_def,\n                                       50000, neighbours=1, segments=1)\n        res = kd_tree.get_sample_from_neighbour_info(\'nn\', (800, 800),\n                                                     masked_data,\n                                                     valid_input_index,\n                                                     valid_output_index, index_array,\n                                                     fill_value=None)\n        expected_fill_mask = np.fromfile(os.path.join(os.path.dirname(__file__),\n                                                      \'test_files\',\n                                                      \'mask_test_full_fill_multi.dat\'),\n                                         sep=\' \').reshape((800, 800, 3))\n        fill_mask = res.mask\n        self.assertTrue(np.array_equal(fill_mask, expected_fill_mask))\n\n\nclass TestXArrayResamplerNN(unittest.TestCase):\n    """"""Test the XArrayResamplerNN class.""""""\n\n    @classmethod\n    def setUpClass(cls):\n        import xarray as xr\n        import dask.array as da\n        cls.area_def = geometry.AreaDefinition(\'areaD\',\n                                               \'Europe (3km, HRV, VTC)\',\n                                               \'areaD\',\n                                               {\'a\': \'6378144.0\',\n                                                \'b\': \'6356759.0\',\n                                                \'lat_0\': \'50.00\',\n                                                \'lat_ts\': \'50.00\',\n                                                \'lon_0\': \'8.00\',\n                                                \'proj\': \'stere\'},\n                                               800,\n                                               800,\n                                               [-1370912.72,\n                                                -909968.64000000001,\n                                                1029087.28,\n                                                1490031.3600000001])\n\n        dfa = da.from_array  # shortcut\n        cls.chunks = chunks = 5\n        cls.tgrid = geometry.CoordinateDefinition(\n            lons=dfa(np.array([\n                [11.5, 12.562036, 12.9],\n                [11.5, 12.562036, 12.9],\n                [11.5, 12.562036, 12.9],\n                [11.5, 12.562036, 12.9],\n            ]), chunks=chunks),\n            lats=dfa(np.array([\n                [55.715613, 55.715613, 55.715613],\n                [55.715613, 55.715613, 55.715613],\n                [55.715613, np.nan, 55.715613],\n                [55.715613, 55.715613, 55.715613],\n            ]), chunks=chunks))\n\n        cls.tdata_1d = xr.DataArray(\n            dfa(np.array([1., 2., 3.]), chunks=chunks), dims=(\'my_dim1\',))\n        cls.tlons_1d = xr.DataArray(\n            dfa(np.array([11.280789, 12.649354, 12.080402]), chunks=chunks),\n            dims=(\'my_dim1\',))\n        cls.tlats_1d = xr.DataArray(\n            dfa(np.array([56.011037, 55.629675, 55.641535]), chunks=chunks),\n            dims=(\'my_dim1\',))\n        cls.tswath_1d = geometry.SwathDefinition(lons=cls.tlons_1d,\n                                                 lats=cls.tlats_1d)\n\n        cls.data_2d = xr.DataArray(\n            da.from_array(np.fromfunction(lambda y, x: y * x, (50, 10)),\n                          chunks=5),\n            dims=(\'my_dim_y\', \'my_dim_x\'))\n        cls.data_3d = xr.DataArray(\n            da.from_array(np.fromfunction(lambda y, x, b: y * x * b, (50, 10, 3)),\n                          chunks=5),\n            dims=(\'my_dim_y\', \'my_dim_x\', \'bands\'),\n            coords={\'bands\': [\'r\', \'g\', \'b\']})\n        cls.lons_2d = xr.DataArray(\n            da.from_array(np.fromfunction(lambda y, x: 3 + x, (50, 10)),\n                          chunks=5),\n            dims=(\'my_dim_y\', \'my_dim_x\'))\n        cls.lats_2d = xr.DataArray(\n            da.from_array(np.fromfunction(lambda y, x: 75 - y, (50, 10)),\n                          chunks=5),\n            dims=(\'my_dim_y\', \'my_dim_x\'))\n        cls.swath_def_2d = geometry.SwathDefinition(lons=cls.lons_2d,\n                                                    lats=cls.lats_2d)\n        cls.src_area_2d = geometry.AreaDefinition(\n            \'areaD_src\', \'Europe (3km, HRV, VTC)\', \'areaD\',\n            {\'a\': \'6378144.0\', \'b\': \'6356759.0\', \'lat_0\': \'52.00\',\n             \'lat_ts\': \'52.00\', \'lon_0\': \'5.00\', \'proj\': \'stere\'}, 50, 10,\n            [-1370912.72, -909968.64000000001, 1029087.28,\n             1490031.3600000001])\n\n    def test_nearest_swath_1d_mask_to_grid_1n(self):\n        """"""Test 1D swath definition to 2D grid definition; 1 neighbor.""""""\n        from pyresample.kd_tree import XArrayResamplerNN\n        import xarray as xr\n        import dask.array as da\n        resampler = XArrayResamplerNN(self.tswath_1d, self.tgrid,\n                                      radius_of_influence=100000,\n                                      neighbours=1)\n        data = self.tdata_1d\n        ninfo = resampler.get_neighbour_info(mask=data.isnull())\n        for val in ninfo[:3]:\n            # vii, ia, voi\n            self.assertIsInstance(val, da.Array)\n        res = resampler.get_sample_from_neighbour_info(data)\n        self.assertIsInstance(res, xr.DataArray)\n        self.assertIsInstance(res.data, da.Array)\n        actual = res.values\n        expected = np.array([\n            [1., 2., 2.],\n            [1., 2., 2.],\n            [1., np.nan, 2.],\n            [1., 2., 2.],\n        ])\n        np.testing.assert_allclose(actual, expected)\n\n    def test_nearest_type_preserve(self):\n        """"""Test 1D swath definition to 2D grid definition; 1 neighbor.""""""\n        from pyresample.kd_tree import XArrayResamplerNN\n        import xarray as xr\n        import dask.array as da\n        resampler = XArrayResamplerNN(self.tswath_1d, self.tgrid,\n                                      radius_of_influence=100000,\n                                      neighbours=1)\n        data = self.tdata_1d\n        data = xr.DataArray(da.from_array(np.array([1, 2, 3]),\n                                          chunks=5),\n                            dims=(\'my_dim1\',))\n        ninfo = resampler.get_neighbour_info()\n        for val in ninfo[:3]:\n            # vii, ia, voi\n            self.assertIsInstance(val, da.Array)\n        res = resampler.get_sample_from_neighbour_info(data, fill_value=255)\n        self.assertIsInstance(res, xr.DataArray)\n        self.assertIsInstance(res.data, da.Array)\n        actual = res.values\n        expected = np.array([\n            [1, 2, 2],\n            [1, 2, 2],\n            [1, 255, 2],\n            [1, 2, 2],\n        ])\n        np.testing.assert_equal(actual, expected)\n\n    def test_nearest_swath_2d_mask_to_area_1n(self):\n        """"""Test 2D swath definition to 2D area definition; 1 neighbor.""""""\n        from pyresample.kd_tree import XArrayResamplerNN\n        import xarray as xr\n        import dask.array as da\n        swath_def = self.swath_def_2d\n        data = self.data_2d\n        resampler = XArrayResamplerNN(swath_def, self.area_def,\n                                      radius_of_influence=50000,\n                                      neighbours=1)\n        ninfo = resampler.get_neighbour_info(mask=data.isnull())\n        for val in ninfo[:3]:\n            # vii, ia, voi\n            self.assertIsInstance(val, da.Array)\n        res = resampler.get_sample_from_neighbour_info(data)\n        self.assertIsInstance(res, xr.DataArray)\n        self.assertIsInstance(res.data, da.Array)\n        res = res.values\n        cross_sum = np.nansum(res)\n        expected = 15874591.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_area_2d_to_area_1n(self):\n        """"""Test 2D area definition to 2D area definition; 1 neighbor.""""""\n        from pyresample.kd_tree import XArrayResamplerNN\n        import xarray as xr\n        import dask.array as da\n        data = self.data_2d\n        resampler = XArrayResamplerNN(self.src_area_2d, self.area_def,\n                                      radius_of_influence=50000,\n                                      neighbours=1)\n        ninfo = resampler.get_neighbour_info()\n        for val in ninfo[:3]:\n            # vii, ia, voi\n            self.assertIsInstance(val, da.Array)\n        self.assertRaises(AssertionError,\n                          resampler.get_sample_from_neighbour_info, data)\n\n        # rename data dimensions to match the expected area dimensions\n        data = data.rename({\'my_dim_y\': \'y\', \'my_dim_x\': \'x\'})\n        res = resampler.get_sample_from_neighbour_info(data)\n        self.assertIsInstance(res, xr.DataArray)\n        self.assertIsInstance(res.data, da.Array)\n        res = res.values\n        cross_sum = np.nansum(res)\n        expected = 27706753.0\n        self.assertEqual(cross_sum, expected)\n\n    def test_nearest_area_2d_to_area_1n_no_roi(self):\n        """"""Test 2D area definition to 2D area definition; 1 neighbor, no radius of influence.""""""\n        from pyresample.kd_tree import XArrayResamplerNN\n        import xarray as xr\n        import dask.array as da\n        data = self.data_2d\n        resampler = XArrayResamplerNN(self.src_area_2d, self.area_def,\n                                      neighbours=1)\n        ninfo = resampler.get_neighbour_info()\n        for val in ninfo[:3]:\n            # vii, ia, voi\n            self.assertIsInstance(val, da.Array)\n        self.assertRaises(AssertionError,\n                          resampler.get_sample_from_neighbour_info, data)\n\n        # rename data dimensions to match the expected area dimensions\n        data = data.rename({\'my_dim_y\': \'y\', \'my_dim_x\': \'x\'})\n        res = resampler.get_sample_from_neighbour_info(data)\n        self.assertIsInstance(res, xr.DataArray)\n        self.assertIsInstance(res.data, da.Array)\n        res = res.values\n        cross_sum = np.nansum(res)\n        expected = 87281406.0\n        self.assertEqual(cross_sum, expected)\n\n        # pretend the resolutions can\'t be determined\n        with mock.patch.object(self.src_area_2d, \'geocentric_resolution\') as sgr, \\\n                mock.patch.object(self.area_def, \'geocentric_resolution\') as dgr:\n            sgr.side_effect = RuntimeError\n            dgr.side_effect = RuntimeError\n            resampler = XArrayResamplerNN(self.src_area_2d, self.area_def,\n                                          neighbours=1)\n            resampler.get_neighbour_info()\n            res = resampler.get_sample_from_neighbour_info(data)\n            self.assertIsInstance(res, xr.DataArray)\n            self.assertIsInstance(res.data, da.Array)\n            res = res.values\n            cross_sum = np.nansum(res)\n            expected = 1855928.0\n            self.assertEqual(cross_sum, expected)\n\n    def test_nearest_area_2d_to_area_1n_3d_data(self):\n        """"""Test 2D area definition to 2D area definition; 1 neighbor, 3d data.""""""\n        from pyresample.kd_tree import XArrayResamplerNN\n        import xarray as xr\n        import dask.array as da\n        data = self.data_3d\n        resampler = XArrayResamplerNN(self.src_area_2d, self.area_def,\n                                      radius_of_influence=50000,\n                                      neighbours=1)\n        ninfo = resampler.get_neighbour_info()\n        for val in ninfo[:3]:\n            # vii, ia, voi\n            self.assertIsInstance(val, da.Array)\n        self.assertRaises(AssertionError,\n                          resampler.get_sample_from_neighbour_info, data)\n\n        # rename data dimensions to match the expected area dimensions\n        data = data.rename({\'my_dim_y\': \'y\', \'my_dim_x\': \'x\'})\n        res = resampler.get_sample_from_neighbour_info(data)\n        self.assertIsInstance(res, xr.DataArray)\n        self.assertIsInstance(res.data, da.Array)\n        self.assertCountEqual(res.coords[\'bands\'], [\'r\', \'g\', \'b\'])\n        res = res.values\n        cross_sum = np.nansum(res)\n        expected = 83120259.0\n        self.assertEqual(cross_sum, expected)\n\n    @unittest.skipIf(True, ""Multiple neighbors not supported yet"")\n    def test_nearest_swath_1d_mask_to_grid_8n(self):\n        """"""Test 1D swath definition to 2D grid definition; 8 neighbors.""""""\n        from pyresample.kd_tree import XArrayResamplerNN\n        import xarray as xr\n        import dask.array as da\n        resampler = XArrayResamplerNN(self.tswath_1d, self.tgrid,\n                                      radius_of_influence=100000,\n                                      neighbours=8)\n        data = self.tdata_1d\n        ninfo = resampler.get_neighbour_info(mask=data.isnull())\n        for val in ninfo[:3]:\n            # vii, ia, voi\n            self.assertIsInstance(val, da.Array)\n        res = resampler.get_sample_from_neighbour_info(data)\n        self.assertIsInstance(res, xr.DataArray)\n        self.assertIsInstance(res.data, da.Array)\n        # actual = res.values\n        # expected = TODO\n        # np.testing.assert_allclose(actual, expected)\n'"
pyresample/test/test_plot.py,10,"b'#!/usr/bin/env python\n# encoding: utf8\n#\n# Copyright (C) 2014-2020 PyTroll developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Test the quicklook plotting functions.""""""\n\nimport os\nimport numpy as np\nimport sys\nimport unittest\nfrom unittest import mock\n\ntry:\n    import matplotlib\n    matplotlib.use(\'Agg\')\nexcept ImportError:\n    pass  # Postpone fail to individual tests\n\ntry:\n    from mpl_toolkits.basemap import Basemap\nexcept ImportError:\n    Basemap = None\n\n\nMERIDIANS1 = np.array([-180, -170, -160, -150, -140, -130, -120, -110, -100,  -90,  -80,\n                       -70,  -60,  -50,  -40,  -30,  -20,  -10,    0,   10,   20,   30,\n                       40,   50,   60,   70,   80,   90,  100,  110,  120,  130,  140,\n                       150,  160,  170,  180], dtype=np.int64)\n\nPARALLELS1 = np.array([-90, -80, -70, -60, -50, -40, -30, -20, -10,   0,  10,  20,  30,\n                       40,  50,  60,  70,  80,  90], dtype=np.int64)\n\n\nclass Test(unittest.TestCase):\n    """"""Test the plot utilities.""""""\n\n    filename = os.path.abspath(os.path.join(os.path.dirname(__file__),\n                                            \'test_files\', \'ssmis_swath.npz\'))\n    data = np.load(filename)[\'data\']\n    lons = data[:, 0].astype(np.float64)\n    lats = data[:, 1].astype(np.float64)\n    tb37v = data[:, 2].astype(np.float64)\n\n    # screen out the fill values\n    fvalue = -10000000000.0\n    valid_fov = (lons != fvalue) * (lats != fvalue) * (tb37v != fvalue)\n    lons = lons[valid_fov]\n    lats = lats[valid_fov]\n    tb37v = tb37v[valid_fov]\n\n    def setUp(self):\n        """"""Set up the services for the test functions.""""""\n        pass\n\n    def test_ellps2axis(self):\n        """"""Test the ellps2axis function.""""""\n        from pyresample import plot\n        a, b = plot.ellps2axis(\'WGS84\')\n        self.assertAlmostEqual(a, 6378137.0,\n                               msg=\'Failed to get semi-major axis of ellipsis\')\n        self.assertAlmostEqual(b, 6356752.3142451793,\n                               msg=\'Failed to get semi-minor axis of ellipsis\')\n\n    @unittest.skipIf(Basemap is None, ""basemap is not available"")\n    def test_area_def2basemap(self):\n        """"""Test the area to Basemap object conversion function.""""""\n        from pyresample import plot\n        from pyresample import parse_area_file\n        area_def = parse_area_file(os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.yaml\'), \'ease_sh\')[0]\n        bmap = plot.area_def2basemap(area_def)\n        self.assertTrue(bmap.rmajor == bmap.rminor and bmap.rmajor == 6371228.0,\n                        \'Failed to create Basemap object\')\n\n    @mock.patch(\'matplotlib.ticker.FixedLocator\')\n    @mock.patch(\'matplotlib.pyplot.axes\')\n    def test_add_gridlines(self, axes, fx_locator):\n        """"""Test the adding of gridlines to matplotlib plotting.""""""\n        from pyresample.plot import _add_gridlines\n        fx_locator.return_value = mock.MagicMock()\n        axes.return_value = mock.MagicMock()\n\n        retv = _add_gridlines(axes, None, None)\n        fx_locator.assert_not_called()\n        self.assertEqual(retv.xlines, False)\n        self.assertEqual(retv.ylines, False)\n\n        retv = _add_gridlines(axes, 10, None)\n        fx_locator.assert_called_once()\n\n        meridians = fx_locator.call_args_list[0][0][0]\n        np.testing.assert_array_equal(meridians, MERIDIANS1)\n\n        retv = _add_gridlines(axes, 10, 10)\n        parallels = fx_locator.call_args_list[-1][0][0]\n        np.testing.assert_array_equal(parallels, PARALLELS1)\n        ncalls = fx_locator.call_count\n        self.assertEqual(ncalls, 3)\n\n        retv = _add_gridlines(axes, None, 10)\n        ncalls = fx_locator.call_count\n        self.assertEqual(ncalls, 4)\n\n        retv = _add_gridlines(axes, 0, 0)\n        ncalls = fx_locator.call_count\n        self.assertEqual(ncalls, 4)\n\n    def test_translate_coast_res(self):\n        """"""Test the translation of coast resolution arguments from old basemap notation to cartopy.""""""\n        from pyresample.plot import _translate_coast_resolution_to_cartopy\n        from pyresample.plot import BASEMAP_NOT_CARTOPY\n\n        with self.assertRaises(KeyError) as raises:\n            if sys.version_info > (3,):\n                self.assertEqual(raises.msg, None)\n            retv, _ = _translate_coast_resolution_to_cartopy(\'200m\')\n\n        if BASEMAP_NOT_CARTOPY:\n            retv, _ = _translate_coast_resolution_to_cartopy(\'c\')\n            self.assertEqual(retv, \'c\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'110m\')\n            self.assertEqual(retv, \'l\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'10m\')\n            self.assertEqual(retv, \'f\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'50m\')\n            self.assertEqual(retv, \'i\')\n\n        if not BASEMAP_NOT_CARTOPY:\n            retv, _ = _translate_coast_resolution_to_cartopy(\'c\')\n            self.assertEqual(retv, \'110m\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'l\')\n            self.assertEqual(retv, \'110m\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'i\')\n            self.assertEqual(retv, \'50m\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'h\')\n            self.assertEqual(retv, \'10m\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'f\')\n            self.assertEqual(retv, \'10m\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'110m\')\n            self.assertEqual(retv, \'110m\')\n            retv, _ = _translate_coast_resolution_to_cartopy(\'10m\')\n            self.assertEqual(retv, \'10m\')\n\n    def test_plate_carreeplot(self):\n        """"""Test the Plate Caree plotting functionality.""""""\n        from pyresample import plot, kd_tree, geometry\n        from pyresample import parse_area_file\n        area_def = parse_area_file(os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.yaml\'), \'pc_world\')[0]\n        swath_def = geometry.SwathDefinition(self.lons, self.lats)\n        result = kd_tree.resample_nearest(swath_def, self.tb37v, area_def,\n                                          radius_of_influence=20000,\n                                          fill_value=None)\n\n        plot._get_quicklook(area_def, result, num_meridians=0, num_parallels=0)\n        plot._get_quicklook(area_def, result, num_meridians=10, num_parallels=10)\n        plot._get_quicklook(area_def, result, num_meridians=None, num_parallels=None)\n\n    def test_easeplot(self):\n        """"""Test the plotting on the ease grid area.""""""\n        from pyresample import plot, kd_tree, geometry\n        from pyresample import parse_area_file\n        area_def = parse_area_file(os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.yaml\'), \'ease_sh\')[0]\n        swath_def = geometry.SwathDefinition(self.lons, self.lats)\n        result = kd_tree.resample_nearest(swath_def, self.tb37v, area_def,\n                                          radius_of_influence=20000,\n                                          fill_value=None)\n        plot._get_quicklook(area_def, result)\n\n    def test_orthoplot(self):\n        """"""Test the ortho plotting.""""""\n        from pyresample import plot, kd_tree, geometry\n        from pyresample import parse_area_file\n        area_def = parse_area_file(os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.cfg\'), \'ortho\')[0]\n        swath_def = geometry.SwathDefinition(self.lons, self.lats)\n        result = kd_tree.resample_nearest(swath_def, self.tb37v, area_def,\n                                          radius_of_influence=20000,\n                                          fill_value=None)\n        plot._get_quicklook(area_def, result)\n'"
pyresample/test/test_spatial_mp.py,17,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# pyresample, Resampling of remote sensing image data in python\n#\n# Copyright (C) 2014-2019 PyTroll developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Testing the _spatial_mp module.""""""\n\n\n# from unittest import mock\nimport unittest\nimport numpy as np\n\nimport pyresample._spatial_mp as sp\n\n\n# class SpatialMPTest(unittest.TestCase):\n#     @mock.patch(\'pyresample._spatial_mp.pyproj.Proj.__init__\', return_value=None)\n#     def test_base_proj_epsg(self, proj_init):\n#         """"""Test Proj creation with EPSG codes""""""\n#         if pyproj.__version__ < \'2\':\n#             return self.skipTest(reason=\'pyproj 2+ only\')\n#\n#         args = [\n#             [None, {\'init\': \'EPSG:6932\'}],\n#             [{\'init\': \'EPSG:6932\'}, {}],\n#             [None, {\'EPSG\': \'6932\'}],\n#             [{\'EPSG\': \'6932\'}, {}]\n#         ]\n#         for projparams, kwargs in args:\n#             BaseProj(projparams, **kwargs)\n#             proj_init.assert_called_with(projparams=\'EPSG:6932\', preserve_units=mock.ANY)\n#             proj_init.reset_mock()\n\nclass SpatialMPTest(unittest.TestCase):\n    """"""Test of spatial_mp.""""""\n\n    def setUp(self):\n        """"""Set up some variables.""""""\n        self.exp_coords = np.array([[6370997., 0, 0],\n                                    [6178887.9339746, 1089504.6535337, 1106312.0189715],\n                                    [5233097.4664751, 2440233.4244888, 2692499.6776952]])\n        self.lon = np.array([0, 10, 25])\n        self.lat = np.array([0, 10, 25])\n        self.lon32 = self.lon.astype(np.float32)\n        self.lat32 = self.lon.astype(np.float32)\n        self.lon64 = self.lon.astype(np.float64)\n        self.lat64 = self.lon.astype(np.float64)\n\n    def test_cartesian(self):\n        """"""Test the transform_lonlats of class Cartesian.""""""\n        my_cartesian = sp.Cartesian()\n        coords_int = my_cartesian.transform_lonlats(self.lon, self.lat)\n        coords_float = my_cartesian.transform_lonlats(self.lon64, self.lat64)\n        coords_float32 = my_cartesian.transform_lonlats(self.lon32, self.lat32)\n        np.testing.assert_almost_equal(coords_float, self.exp_coords, decimal=3)\n        np.testing.assert_almost_equal(coords_int, coords_float, decimal=0)\n        self.assertIs(type(coords_float32[0, 0]), np.float32)\n        self.assertIs(type(coords_float[0, 0]), np.float64)\n        self.assertTrue(np.issubdtype(coords_int.dtype, np.floating))\n\n    def test_cartesian_without_ne(self):\n        """"""Test the transform_lonlats of class Cartesian without numexpr.""""""\n        sp.ne = False\n        my_cartesian = sp.Cartesian()\n        coords_int = my_cartesian.transform_lonlats(self.lon, self.lat)\n        coords_float = my_cartesian.transform_lonlats(self.lon64, self.lat64)\n        coords_float32 = my_cartesian.transform_lonlats(self.lon32, self.lat32)\n        np.testing.assert_almost_equal(coords_float, self.exp_coords, decimal=3)\n        np.testing.assert_almost_equal(coords_int, coords_float, decimal=0)\n        self.assertIs(type(coords_float32[0, 0]), np.float32)\n        self.assertIs(type(coords_float[0, 0]), np.float64)\n        self.assertTrue(np.issubdtype(coords_int.dtype, np.floating))\n'"
pyresample/test/test_spherical.py,117,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2013-2020 Martin Raspaud\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Test cases for spherical geometry.""""""\n\nfrom pyresample.spherical import SphPolygon, Arc, SCoordinate, CCoordinate\nimport unittest\nimport numpy as np\n\n\nclass TestSCoordinate(unittest.TestCase):\n\n    """"""Test SCoordinates.\n    """"""\n\n    def test_distance(self):\n        """"""Test Vincenty formula\n        """"""\n        d = SCoordinate(0, 0).distance(SCoordinate(1, 1))\n        self.assertEqual(d, 1.2745557823062943)\n\n    def test_hdistance(self):\n        """"""Test Haversine formula\n        """"""\n        d = SCoordinate(0, 0).hdistance(SCoordinate(1, 1))\n        self.assertTrue(np.allclose(d, 1.2745557823062943))\n\n    def test_str(self):\n        """"""Check the string representation\n        """"""\n        d = SCoordinate(0, 0)\n        self.assertEqual(str(d), ""(0.0, 0.0)"")\n\n    def test_repr(self):\n        """"""Check the representation\n        """"""\n        d = SCoordinate(0, 0)\n        self.assertEqual(repr(d), ""(0.0, 0.0)"")\n\n\nclass TestCCoordinate(unittest.TestCase):\n\n    """"""Test SCoordinates.\n    """"""\n\n    def test_str(self):\n        """"""Check the string representation\n        """"""\n        d = CCoordinate((0, 0, 0))\n        self.assertEqual(str(d), ""[0 0 0]"")\n\n    def test_repr(self):\n        """"""Check the representation\n        """"""\n        d = CCoordinate((0, 0, 0))\n        self.assertEqual(repr(d), ""[0 0 0]"")\n\n    def test_norm(self):\n        """"""Euclidean norm of a cartesian vector\n        """"""\n        d = CCoordinate((1, 0, 0))\n        self.assertEqual(d.norm(), 1.0)\n\n    def test_normalize(self):\n        """"""Normalize a cartesian vector\n        """"""\n        d = CCoordinate((2., 0., 0.))\n        self.assertTrue(np.allclose(d.normalize().cart, [1, 0, 0]))\n\n    def test_cross(self):\n        """"""Test cross product in cartesian coordinates\n        """"""\n        d = CCoordinate((1., 0., 0.))\n        c = CCoordinate((0., 1., 0.))\n        self.assertTrue(np.allclose(d.cross(c).cart, [0., 0., 1.]))\n\n    def test_dot(self):\n        """"""Test the dot product of two cartesian vectors.\n        """"""\n        d = CCoordinate((1., 0., 0.))\n        c = CCoordinate((0., 1., 0.))\n        self.assertEqual(d.dot(c), 0)\n\n    def test_ne(self):\n        """"""Test inequality of two cartesian vectors.\n        """"""\n        d = CCoordinate((1., 0., 0.))\n        c = CCoordinate((0., 1., 0.))\n        self.assertTrue(c != d)\n\n    def test_eq(self):\n        """"""Test equality of two cartesian vectors.\n        """"""\n        d = CCoordinate((1., 0., 0.))\n        c = CCoordinate((0., 1., 0.))\n        self.assertFalse(c == d)\n\n    def test_add(self):\n        """"""Test adding cartesian vectors.\n        """"""\n        d = CCoordinate((1., 0., 0.))\n        c = CCoordinate((0., 1., 0.))\n        b = CCoordinate((1., 1., 0.))\n        self.assertTrue(np.allclose((d + c).cart, b.cart))\n\n        self.assertTrue(np.allclose((d + (0, 1, 0)).cart, b.cart))\n\n        self.assertTrue(np.allclose(((0, 1, 0) + d).cart, b.cart))\n\n    def test_mul(self):\n        """"""Test multiplying (element-wise) cartesian vectors.\n        """"""\n        d = CCoordinate((1., 0., 0.))\n        c = CCoordinate((0., 1., 0.))\n        b = CCoordinate((0., 0., 0.))\n        self.assertTrue(np.allclose((d * c).cart, b.cart))\n        self.assertTrue(np.allclose((d * (0, 1, 0)).cart, b.cart))\n\n        self.assertTrue(np.allclose(((0, 1, 0) * d).cart, b.cart))\n\n    def test_to_spherical(self):\n        """"""Test converting to spherical coordinates.\n        """"""\n        d = CCoordinate((1., 0., 0.))\n        c = SCoordinate(0, 0)\n        self.assertEqual(d.to_spherical(), c)\n\n\nclass TestArc(unittest.TestCase):\n\n    """"""Test arcs\n    """"""\n\n    def test_eq(self):\n        arc1 = Arc(SCoordinate(0, 0),\n                   SCoordinate(np.deg2rad(10), np.deg2rad(10)))\n        arc2 = Arc(SCoordinate(0, np.deg2rad(10)),\n                   SCoordinate(np.deg2rad(10), 0))\n\n        self.assertFalse(arc1 == arc2)\n\n        self.assertTrue(arc1 == arc1)\n\n    def test_ne(self):\n        arc1 = Arc(SCoordinate(0, 0),\n                   SCoordinate(np.deg2rad(10), np.deg2rad(10)))\n        arc2 = Arc(SCoordinate(0, np.deg2rad(10)),\n                   SCoordinate(np.deg2rad(10), 0))\n\n        self.assertTrue(arc1 != arc2)\n\n        self.assertFalse(arc1 != arc1)\n\n    def test_str(self):\n        arc1 = Arc(SCoordinate(0, 0),\n                   SCoordinate(np.deg2rad(10), np.deg2rad(10)))\n        self.assertEqual(str(arc1), str(arc1.start) + "" -> "" + str(arc1.end))\n        self.assertEqual(repr(arc1), str(arc1.start) + "" -> "" + str(arc1.end))\n\n    def test_intersection(self):\n        arc1 = Arc(SCoordinate(0, 0),\n                   SCoordinate(np.deg2rad(10), np.deg2rad(10)))\n        arc2 = Arc(SCoordinate(0, np.deg2rad(10)),\n                   SCoordinate(np.deg2rad(10), 0))\n        lon, lat = arc1.intersection(arc2)\n\n        self.assertTrue(np.allclose(np.rad2deg(lon), 5))\n        self.assertEqual(np.rad2deg(lat).round(7), round(5.0575148968282093, 7))\n\n        arc1 = Arc(SCoordinate(0, 0),\n                   SCoordinate(np.deg2rad(10), np.deg2rad(10)))\n\n        self.assertTrue(arc1.intersection(arc1) is None)\n\n        arc1 = Arc(SCoordinate(np.deg2rad(24.341215776575297),\n                               np.deg2rad(44.987819588259327)),\n                   SCoordinate(np.deg2rad(18.842727517611817),\n                               np.deg2rad(46.512483610284178)))\n        arc2 = Arc(SCoordinate(np.deg2rad(20.165961750361905),\n                               np.deg2rad(46.177305385810541)),\n                   SCoordinate(np.deg2rad(20.253297585831707),\n                               np.deg2rad(50.935830837274324)))\n        inter = SCoordinate(np.deg2rad(20.165957021925202),\n                            np.deg2rad(46.177022633103398))\n        self.assertEqual(arc1.intersection(arc2), inter)\n\n        arc1 = Arc(SCoordinate(np.deg2rad(-2.4982818108326734),\n                               np.deg2rad(48.596644847869655)),\n                   SCoordinate(np.deg2rad(-2.9571441235622835),\n                               np.deg2rad(49.165688435261394)))\n        arc2 = Arc(SCoordinate(np.deg2rad(-3.4976667413531688),\n                               np.deg2rad(48.562704872921373)),\n                   SCoordinate(np.deg2rad(-5.893976312685715),\n                               np.deg2rad(48.445795283217116)))\n\n        self.assertTrue(arc1.intersection(arc2) is None)\n\n    def test_angle(self):\n        arc1 = Arc(SCoordinate(np.deg2rad(157.5),\n                               np.deg2rad(89.234600944314138)),\n                   SCoordinate(np.deg2rad(90),\n                               np.deg2rad(89)))\n        arc2 = Arc(SCoordinate(np.deg2rad(157.5),\n                               np.deg2rad(89.234600944314138)),\n                   SCoordinate(np.deg2rad(135),\n                               np.deg2rad(89)))\n\n        self.assertAlmostEqual(np.rad2deg(arc1.angle(arc2)), -44.996385007218926)\n\n        arc1 = Arc(SCoordinate(np.deg2rad(112.5),\n                               np.deg2rad(89.234600944314138)),\n                   SCoordinate(np.deg2rad(90), np.deg2rad(89)))\n        arc2 = Arc(SCoordinate(np.deg2rad(112.5),\n                               np.deg2rad(89.234600944314138)),\n                   SCoordinate(np.deg2rad(45), np.deg2rad(89)))\n\n        self.assertAlmostEqual(np.rad2deg(arc1.angle(arc2)), 44.996385007218883)\n\n        arc1 = Arc(SCoordinate(0, 0), SCoordinate(1, 0))\n        self.assertAlmostEqual(arc1.angle(arc1), 0)\n\n        arc2 = Arc(SCoordinate(1, 0), SCoordinate(0, 0))\n        self.assertAlmostEqual(arc1.angle(arc2), 0)\n\n        arc2 = Arc(SCoordinate(0, 0), SCoordinate(-1, 0))\n        self.assertAlmostEqual(arc1.angle(arc2), np.pi)\n\n        arc2 = Arc(SCoordinate(2, 0), SCoordinate(1, 0))\n        self.assertAlmostEqual(arc1.angle(arc2), np.pi)\n\n        arc2 = Arc(SCoordinate(2, 0), SCoordinate(3, 0))\n        self.assertRaises(ValueError, arc1.angle, arc2)\n\n\nclass TestSphericalPolygon(unittest.TestCase):\n\n    """"""Test the spherical polygon.\n    """"""\n\n    def test_area(self):\n        """"""Test the area function\n        """"""\n        vertices = np.array([[1, 2, 3, 4, 3, 2],\n                             [3, 4, 3, 2, 1, 2]]).T\n        polygon = SphPolygon(np.deg2rad(vertices))\n\n        self.assertAlmostEqual(0.00121732523118, polygon.area())\n\n        vertices = np.array([[1, 2, 3, 2],\n                             [3, 4, 3, 2]]).T\n        polygon = SphPolygon(np.deg2rad(vertices))\n\n        self.assertAlmostEqual(0.000608430665842, polygon.area())\n\n        vertices = np.array([[0, 0, 1, 1],\n                             [0, 1, 1, 0]]).T\n        polygon = SphPolygon(np.deg2rad(vertices))\n\n        self.assertAlmostEqual(0.000304609684862, polygon.area())\n\n        # Across the dateline\n\n        vertices = np.array([[179.5, -179.5, -179.5, 179.5],\n                             [1, 1, 0, 0]]).T\n        polygon = SphPolygon(np.deg2rad(vertices))\n\n        self.assertAlmostEqual(0.000304609684862, polygon.area())\n\n        vertices = np.array([[0, 90, 90, 0],\n                             [1, 1, 0, 0]]).T\n        polygon = SphPolygon(np.deg2rad(vertices))\n\n        self.assertAlmostEqual(0.0349012696772, polygon.area())\n\n        vertices = np.array([[90, 0, 0],\n                             [0, 0, 90]]).T\n        polygon = SphPolygon(np.deg2rad(vertices))\n\n        self.assertAlmostEqual(np.pi / 2, polygon.area())\n\n        # Around the north pole\n\n        vertices = np.array([[0, -90, 180, 90],\n                             [89, 89, 89, 89]]).T\n        polygon = SphPolygon(np.deg2rad(vertices))\n\n        self.assertAlmostEqual(0.000609265770322, polygon.area())\n\n        # Around the south pole\n\n        vertices = np.array([[0, 90, 180, -90],\n                             [-89, -89, -89, -89]]).T\n        polygon = SphPolygon(np.deg2rad(vertices))\n\n        self.assertAlmostEqual(0.000609265770322, polygon.area())\n\n    def test_is_inside(self):\n        """"""Test checking if a polygon is inside of another.\n        """"""\n\n        vertices = np.array([[1, 1, 20, 20],\n                             [1, 20, 20, 1]]).T\n\n        polygon1 = SphPolygon(np.deg2rad(vertices))\n\n        vertices = np.array([[0, 0, 30, 30],\n                             [0, 30, 30, 0]]).T\n\n        polygon2 = SphPolygon(np.deg2rad(vertices))\n\n        self.assertTrue(polygon1._is_inside(polygon2))\n        self.assertFalse(polygon2._is_inside(polygon1))\n        self.assertTrue(polygon2.area() > polygon1.area())\n\n        polygon2.invert()\n        self.assertFalse(polygon1._is_inside(polygon2))\n        self.assertFalse(polygon2._is_inside(polygon1))\n\n        vertices = np.array([[0, 0, 30, 30],\n                             [21, 30, 30, 21]]).T\n\n        polygon2 = SphPolygon(np.deg2rad(vertices))\n        self.assertFalse(polygon1._is_inside(polygon2))\n        self.assertFalse(polygon2._is_inside(polygon1))\n\n        polygon2.invert()\n\n        self.assertTrue(polygon1._is_inside(polygon2))\n        self.assertFalse(polygon2._is_inside(polygon1))\n\n        vertices = np.array([[100, 100, 130, 130],\n                             [41, 50, 50, 41]]).T\n\n        polygon2 = SphPolygon(np.deg2rad(vertices))\n\n        self.assertFalse(polygon1._is_inside(polygon2))\n        self.assertFalse(polygon2._is_inside(polygon1))\n\n        polygon2.invert()\n\n        self.assertTrue(polygon1._is_inside(polygon2))\n        self.assertFalse(polygon2._is_inside(polygon1))\n\n        vertices = np.array([[-1.54009253, 82.62402855],\n                             [3.4804808, 82.8105746],\n                             [20.7214892, 83.00875812],\n                             [32.8857629, 82.7607758],\n                             [41.53844302, 82.36024339],\n                             [47.92062759, 81.91317164],\n                             [52.82785062, 81.45769791],\n                             [56.75107895, 81.00613046],\n                             [59.99843787, 80.56042986],\n                             [62.76998034, 80.11814453],\n                             [65.20076209, 79.67471372],\n                             [67.38577498, 79.22428],\n                             [69.39480149, 78.75981318],\n                             [71.28163984, 78.27283234],\n                             [73.09016378, 77.75277976],\n                             [74.85864685, 77.18594725],\n                             [76.62327682, 76.55367303],\n                             [78.42162204, 75.82918893],\n                             [80.29698409, 74.97171721],\n                             [82.30538638, 73.9143231],\n                             [84.52973107, 72.53535661],\n                             [87.11696138, 70.57600156],\n                             [87.79163209, 69.98712409],\n                             [72.98142447, 67.1760143],\n                             [61.79517279, 63.2846272],\n                             [53.50600609, 58.7098766],\n                             [47.26725347, 53.70533139],\n                             [42.44083259, 48.42199571],\n                             [38.59682041, 42.95008531],\n                             [35.45189206, 37.3452509],\n                             [32.43435578, 30.72373327],\n                             [31.73750748, 30.89485287],\n                             [29.37284023, 31.44344415],\n                             [27.66001308, 31.81016309],\n                             [26.31358296, 32.08057499],\n                             [25.1963477, 32.29313986],\n                             [24.23118049, 32.46821821],\n                             [23.36993508, 32.61780082],\n                             [22.57998837, 32.74952569],\n                             [21.8375532, 32.86857867],\n                             [21.12396693, 32.97868717],\n                             [20.42339605, 33.08268331],\n                             [19.72121983, 33.18284728],\n                             [19.00268283, 33.28113306],\n                             [18.2515215, 33.3793305],\n                             [17.4482606, 33.47919405],\n                             [16.56773514, 33.58255576],\n                             [15.57501961, 33.6914282],\n                             [14.4180087, 33.8080799],\n                             [13.01234319, 33.93498577],\n                             [11.20625437, 34.0742239],\n                             [8.67990371, 34.22415978],\n                             [7.89344478, 34.26018768],\n                             [8.69446485, 41.19823568],\n                             [9.25707165, 47.17351118],\n                             [9.66283477, 53.14128114],\n                             [9.84134875, 59.09937166],\n                             [9.65054241, 65.04458004],\n                             [8.7667375, 70.97023122],\n                             [6.28280904, 76.85731403]])\n        polygon1 = SphPolygon(np.deg2rad(vertices))\n\n        vertices = np.array([[49.94506701, 46.52610743],\n                             [51.04293649, 46.52610743],\n                             [62.02163129, 46.52610743],\n                             [73.0003261, 46.52610743],\n                             [83.9790209, 46.52610743],\n                             [85.05493299, 46.52610743],\n                             [85.05493299, 45.76549301],\n                             [85.05493299, 37.58315571],\n                             [85.05493299, 28.39260587],\n                             [85.05493299, 18.33178739],\n                             [85.05493299, 17.30750918],\n                             [83.95706351, 17.30750918],\n                             [72.97836871, 17.30750918],\n                             [61.9996739, 17.30750918],\n                             [51.0209791, 17.30750918],\n                             [49.94506701, 17.30750918],\n                             [49.94506701, 18.35262921],\n                             [49.94506701, 28.41192025],\n                             [49.94506701, 37.60055422],\n                             [49.94506701, 45.78080831]])\n        polygon2 = SphPolygon(np.deg2rad(vertices))\n\n        self.assertFalse(polygon2._is_inside(polygon1))\n        self.assertFalse(polygon1._is_inside(polygon2))\n\n    def test_bool(self):\n        """"""Test the intersection and union functions.\n        """"""\n        vertices = np.array([[180, 90, 0, -90],\n                             [89, 89, 89, 89]]).T\n        poly1 = SphPolygon(np.deg2rad(vertices))\n        vertices = np.array([[-45, -135, 135, 45],\n                             [89, 89, 89, 89]]).T\n        poly2 = SphPolygon(np.deg2rad(vertices))\n\n        uni = np.array([[157.5,   89.23460094],\n                        [-225.,   89.],\n                        [112.5,   89.23460094],\n                        [90.,   89.],\n                        [67.5,   89.23460094],\n                        [45.,   89.],\n                        [22.5,   89.23460094],\n                        [0.,   89.],\n                        [-22.5,   89.23460094],\n                        [-45.,   89.],\n                        [-67.5,   89.23460094],\n                        [-90.,   89.],\n                        [-112.5,   89.23460094],\n                        [-135.,   89.],\n                        [-157.5,   89.23460094],\n                        [-180.,   89.]])\n        inter = np.array([[157.5,   89.23460094],\n                          [112.5,   89.23460094],\n                          [67.5,   89.23460094],\n                          [22.5,   89.23460094],\n                          [-22.5,   89.23460094],\n                          [-67.5,   89.23460094],\n                          [-112.5,   89.23460094],\n                          [-157.5,   89.23460094]])\n        poly_inter = poly1.intersection(poly2)\n        poly_union = poly1.union(poly2)\n\n        self.assertTrue(poly_inter.area() <= poly_union.area())\n\n        self.assertTrue(np.allclose(poly_inter.vertices,\n                                    np.deg2rad(inter)))\n        self.assertTrue(np.allclose(poly_union.vertices,\n                                    np.deg2rad(uni)))\n\n        # Test 2 polygons sharing 2 contiguous edges.\n\n        vertices1 = np.array([[-10,  10],\n                              [-5,  10],\n                              [0,  10],\n                              [5,  10],\n                              [10,  10],\n                              [10, -10],\n                              [-10, -10]])\n\n        vertices2 = np.array([[-5,  10],\n                              [0,  10],\n                              [5,  10],\n                              [5,  -5],\n                              [-5,  -5]])\n\n        vertices3 = np.array([[5,  10],\n                              [5,  -5],\n                              [-5,  -5],\n                              [-5,  10],\n                              [0,  10]])\n\n        poly1 = SphPolygon(np.deg2rad(vertices1))\n        poly2 = SphPolygon(np.deg2rad(vertices2))\n        poly_inter = poly1.intersection(poly2)\n\n        self.assertTrue(np.allclose(poly_inter.vertices,\n                                    np.deg2rad(vertices3)))\n\n        # Test when last node of the intersection is the last vertice of the\n        # second polygon.\n\n        swath_vertices = np.array([[-115.32268301,   66.32946139],\n                                   [-61.48397172,  58.56799254],\n                                   [-60.25004314, 58.00754686],\n                                   [-71.35057076,   49.60229517],\n                                   [-113.746486,  56.03008985]])\n        area_vertices = np.array([[-68.32812107,  52.3480829],\n                                  [-67.84993896,  53.07015692],\n                                  [-55.54651296,  64.9254637],\n                                  [-24.63341856,  74.24628796],\n                                  [-31.8996363,  27.99907764],\n                                  [-39.581043,  37.0639821],\n                                  [-50.90185988,  45.56296169],\n                                  [-67.43022017,  52.12399581]])\n\n        res = np.array([[-62.77837918,   59.12607053],\n                        [-61.48397172,   58.56799254],\n                        [-60.25004314,   58.00754686],\n                        [-71.35057076,   49.60229517],\n                        [-113.746486,     56.03008985],\n                        [-115.32268301,   66.32946139]])\n\n        poly1 = SphPolygon(np.deg2rad(swath_vertices))\n        poly2 = SphPolygon(np.deg2rad(area_vertices))\n\n        poly_inter = poly1.intersection(poly2)\n        self.assertTrue(np.allclose(poly_inter.vertices,\n                                    np.deg2rad(res)))\n\n        poly_inter = poly2.intersection(poly1)\n        self.assertTrue(np.allclose(poly_inter.vertices,\n                                    np.deg2rad(res)))\n\n    def test_consistent_radius(self):\n        poly1 = np.array([(-50, 69), (-36, 69), (-36, 64), (-50, 64)])\n        poly2 = np.array([(-46, 68), (-40, 68), (-40, 65), (-45, 65)])\n        poly_outer = SphPolygon(np.deg2rad(poly1), radius=6371)\n        poly_inner = SphPolygon(np.deg2rad(poly2), radius=6371)\n        poly_inter = poly_outer.intersection(poly_inner)\n        self.assertAlmostEqual(poly_inter.radius, poly_inner.radius)\n        # Well, now when we are at it.\n        self.assertAlmostEqual(poly_inter.area(), poly_inner.area())\n'"
pyresample/test/test_spherical_geometry.py,34,"b'import numpy as np\nimport unittest\nimport math\n\nfrom pyresample.spherical_geometry import Coordinate, Arc\nfrom pyresample import geometry\n\n\nclass TestOverlap(unittest.TestCase):\n\n    """"""Testing overlapping functions in pyresample.\n    """"""\n\n    def assert_raises(self, exception, call_able, *args):\n        """"""assertRaises() has changed from py2.6 to 2.7! Here is an attempt to\n        cover both""""""\n        import sys\n        if sys.version_info < (2, 7):\n            self.assertRaises(exception, call_able, *args)\n        else:\n            with self.assertRaises(exception):\n                call_able(*args)\n\n    def test_inside(self):\n        """"""Testing if a point is inside an area.\n        """"""\n        lons = np.array([[-11, 11], [-11, 11]])\n        lats = np.array([[11, 11], [-11, -11]])\n        area = geometry.SwathDefinition(lons, lats)\n\n        point = Coordinate(0, 0)\n\n        self.assertTrue(point in area)\n\n        point = Coordinate(0, 12)\n        self.assertFalse(point in area)\n\n        lons = np.array([[-179, 179], [-179, 179]])\n        lats = np.array([[1, 1], [-1, -1]])\n        area = geometry.SwathDefinition(lons, lats)\n\n        point = Coordinate(180, 0)\n        self.assertTrue(point in area)\n\n        point = Coordinate(180, 12)\n        self.assertFalse(point in area)\n\n        point = Coordinate(-180, 12)\n        self.assertFalse(point in area)\n\n        self.assert_raises(ValueError, Coordinate, 0, 192)\n\n        self.assert_raises(ValueError, Coordinate, 15, -91)\n\n        # case of the north pole\n        lons = np.array([[0, 90], [-90, 180]])\n        lats = np.array([[89, 89], [89, 89]])\n        area = geometry.SwathDefinition(lons, lats)\n\n        point = Coordinate(90, 90)\n        self.assertTrue(point in area)\n\n    def test_overlaps(self):\n        """"""Test if two areas overlap.\n        """"""\n        lons1 = np.array([[0, 90], [-90, 180]])\n        lats1 = np.array([[89, 89], [89, 89]])\n        area1 = geometry.SwathDefinition(lons1, lats1)\n\n        lons2 = np.array([[45, 135], [-45, -135]])\n        lats2 = np.array([[89, 89], [89, 89]])\n        area2 = geometry.SwathDefinition(lons2, lats2)\n\n        self.assertTrue(area1.overlaps(area2))\n        self.assertTrue(area2.overlaps(area1))\n\n        lons1 = np.array([[0, 45], [135, 90]])\n        lats1 = np.array([[89, 89], [89, 89]])\n        area1 = geometry.SwathDefinition(lons1, lats1)\n\n        lons2 = np.array([[180, -135], [-45, -90]])\n        lats2 = np.array([[89, 89], [89, 89]])\n        area2 = geometry.SwathDefinition(lons2, lats2)\n\n        self.assertFalse(area1.overlaps(area2))\n        self.assertFalse(area2.overlaps(area1))\n\n        lons1 = np.array([[-1, 1], [-1, 1]])\n        lats1 = np.array([[1, 1], [-1, -1]])\n        area1 = geometry.SwathDefinition(lons1, lats1)\n\n        lons2 = np.array([[0, 2], [0, 2]])\n        lats2 = np.array([[0, 0], [2, 2]])\n        area2 = geometry.SwathDefinition(lons2, lats2)\n\n        self.assertTrue(area1.overlaps(area2))\n        self.assertTrue(area2.overlaps(area1))\n\n        lons1 = np.array([[-1, 0], [-1, 0]])\n        lats1 = np.array([[1, 2], [-1, 0]])\n        area1 = geometry.SwathDefinition(lons1, lats1)\n\n        lons2 = np.array([[1, 2], [1, 2]])\n        lats2 = np.array([[1, 2], [-1, 0]])\n        area2 = geometry.SwathDefinition(lons2, lats2)\n\n        self.assertFalse(area1.overlaps(area2))\n        self.assertFalse(area2.overlaps(area1))\n\n    def test_overlap_rate(self):\n        """"""Test how much two areas overlap.\n        """"""\n\n        lons1 = np.array([[-1, 1], [-1, 1]])\n        lats1 = np.array([[1, 1], [-1, -1]])\n        area1 = geometry.SwathDefinition(lons1, lats1)\n\n        lons2 = np.array([[0, 2], [0, 2]])\n        lats2 = np.array([[0, 0], [2, 2]])\n        area2 = geometry.SwathDefinition(lons2, lats2)\n\n        self.assertAlmostEqual(area1.overlap_rate(area2), 0.25, 3)\n        self.assertAlmostEqual(area2.overlap_rate(area1), 0.25, 3)\n\n        lons1 = np.array([[82.829699999999974, 36.888300000000001],\n                          [98.145499999999984, 2.8773]])\n        lats1 = np.array([[60.5944, 52.859999999999999],\n                          [80.395899999999997, 66.7547]])\n        area1 = geometry.SwathDefinition(lons1, lats1)\n\n        lons2 = np.array([[7.8098183315148422, 26.189349044600252],\n                          [7.8098183315148422, 26.189349044600252]])\n        lats2 = np.array([[62.953206630716465, 62.953206630716465],\n                          [53.301561187195546, 53.301561187195546]])\n        area2 = geometry.SwathDefinition(lons2, lats2)\n\n        self.assertAlmostEqual(area1.overlap_rate(area2), 0.07, 2)\n        self.assertAlmostEqual(area2.overlap_rate(area1), 0.012, 3)\n\n        lons1 = np.array([[82.829699999999974, 36.888300000000001],\n                          [98.145499999999984, 2.8773]])\n        lats1 = np.array([[60.5944, 52.859999999999999],\n                          [80.395899999999997, 66.7547]])\n        area1 = geometry.SwathDefinition(lons1, lats1)\n\n        lons2 = np.array([[12.108984194981202, 30.490647126520301],\n                          [12.108984194981202, 30.490647126520301]])\n        lats2 = np.array([[65.98228561983025, 65.98228561983025],\n                          [57.304862819933433, 57.304862819933433]])\n        area2 = geometry.SwathDefinition(lons2, lats2)\n\n        self.assertAlmostEqual(area1.overlap_rate(area2), 0.509, 2)\n        self.assertAlmostEqual(area2.overlap_rate(area1), 0.0685, 3)\n\n\nclass TestSphereGeometry(unittest.TestCase):\n\n    """"""Testing sphere geometry from this module.\n    """"""\n\n    def test_angle(self):\n        """"""Testing the angle value between two arcs.\n        """"""\n\n        base = 0\n\n        p0_ = Coordinate(base, base)\n        p1_ = Coordinate(base, base + 1)\n        p2_ = Coordinate(base + 1, base)\n        p3_ = Coordinate(base, base - 1)\n        p4_ = Coordinate(base - 1, base)\n\n        arc1 = Arc(p0_, p1_)\n        arc2 = Arc(p0_, p2_)\n        arc3 = Arc(p0_, p3_)\n        arc4 = Arc(p0_, p4_)\n\n        self.assertAlmostEqual(arc1.angle(arc2), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc2.angle(arc3), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc3.angle(arc4), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc4.angle(arc1), math.pi / 2,\n                               msg=""this should be pi/2"")\n\n        self.assertAlmostEqual(arc1.angle(arc4), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc4.angle(arc3), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc3.angle(arc2), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc2.angle(arc1), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n\n        self.assertAlmostEqual(arc1.angle(arc3), math.pi,\n                               msg=""this should be pi"")\n        self.assertAlmostEqual(arc3.angle(arc1), math.pi,\n                               msg=""this should be pi"")\n        self.assertAlmostEqual(arc2.angle(arc4), math.pi,\n                               msg=""this should be pi"")\n        self.assertAlmostEqual(arc4.angle(arc2), math.pi,\n                               msg=""this should be pi"")\n\n        p5_ = Coordinate(base + 1, base + 1)\n        p6_ = Coordinate(base + 1, base - 1)\n        p7_ = Coordinate(base - 1, base - 1)\n        p8_ = Coordinate(base - 1, base + 1)\n\n        arc5 = Arc(p0_, p5_)\n        arc6 = Arc(p0_, p6_)\n        arc7 = Arc(p0_, p7_)\n        arc8 = Arc(p0_, p8_)\n\n        self.assertAlmostEqual(arc1.angle(arc5), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n        self.assertAlmostEqual(arc5.angle(arc2), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n        self.assertAlmostEqual(arc2.angle(arc6), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n        self.assertAlmostEqual(arc6.angle(arc3), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n        self.assertAlmostEqual(arc3.angle(arc7), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n        self.assertAlmostEqual(arc7.angle(arc4), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n        self.assertAlmostEqual(arc4.angle(arc8), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n        self.assertAlmostEqual(arc8.angle(arc1), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n\n        self.assertAlmostEqual(arc1.angle(arc6), 3 * math.pi / 4, 3,\n                               msg=""this should be 3pi/4"")\n\n        c0_ = Coordinate(180, 0)\n        c1_ = Coordinate(180, 1)\n        c2_ = Coordinate(-179, 0)\n        c3_ = Coordinate(-180, -1)\n        c4_ = Coordinate(179, 0)\n\n        arc1 = Arc(c0_, c1_)\n        arc2 = Arc(c0_, c2_)\n        arc3 = Arc(c0_, c3_)\n        arc4 = Arc(c0_, c4_)\n\n        self.assertAlmostEqual(arc1.angle(arc2), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc2.angle(arc3), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc3.angle(arc4), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc4.angle(arc1), math.pi / 2,\n                               msg=""this should be pi/2"")\n\n        self.assertAlmostEqual(arc1.angle(arc4), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc4.angle(arc3), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc3.angle(arc2), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc2.angle(arc1), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n\n        # case of the north pole\n\n        c0_ = Coordinate(0, 90)\n        c1_ = Coordinate(0, 89)\n        c2_ = Coordinate(-90, 89)\n        c3_ = Coordinate(180, 89)\n        c4_ = Coordinate(90, 89)\n\n        arc1 = Arc(c0_, c1_)\n        arc2 = Arc(c0_, c2_)\n        arc3 = Arc(c0_, c3_)\n        arc4 = Arc(c0_, c4_)\n\n        self.assertAlmostEqual(arc1.angle(arc2), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc2.angle(arc3), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc3.angle(arc4), math.pi / 2,\n                               msg=""this should be pi/2"")\n        self.assertAlmostEqual(arc4.angle(arc1), math.pi / 2,\n                               msg=""this should be pi/2"")\n\n        self.assertAlmostEqual(arc1.angle(arc4), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc4.angle(arc3), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc3.angle(arc2), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n        self.assertAlmostEqual(arc2.angle(arc1), -math.pi / 2,\n                               msg=""this should be -pi/2"")\n\n        self.assertAlmostEqual(Arc(c1_, c2_).angle(arc1), math.pi / 4, 3,\n                               msg=""this should be pi/4"")\n\n        self.assertAlmostEqual(Arc(c4_, c3_).angle(arc4), -math.pi / 4, 3,\n                               msg=""this should be -pi/4"")\n\n        self.assertAlmostEqual(Arc(c1_, c4_).angle(arc1), -math.pi / 4, 3,\n                               msg=""this should be -pi/4"")\n\n    def test_intersects(self):\n        """"""Test if two arcs intersect.\n        """"""\n        p0_ = Coordinate(0, 0)\n        p1_ = Coordinate(0, 1)\n        p2_ = Coordinate(1, 0)\n        p3_ = Coordinate(0, -1)\n        p4_ = Coordinate(-1, 0)\n        p5_ = Coordinate(1, 1)\n        p6_ = Coordinate(1, -1)\n\n        arc13 = Arc(p1_, p3_)\n        arc24 = Arc(p2_, p4_)\n\n        arc32 = Arc(p3_, p2_)\n        arc41 = Arc(p4_, p1_)\n\n        arc40 = Arc(p4_, p0_)\n        arc56 = Arc(p5_, p6_)\n\n        arc45 = Arc(p4_, p5_)\n        arc02 = Arc(p0_, p2_)\n\n        arc35 = Arc(p3_, p5_)\n\n        self.assertTrue(arc13.intersects(arc24))\n\n        self.assertFalse(arc32.intersects(arc41))\n\n        self.assertFalse(arc56.intersects(arc40))\n\n        self.assertFalse(arc56.intersects(arc40))\n\n        self.assertFalse(arc45.intersects(arc02))\n\n        self.assertTrue(arc35.intersects(arc24))\n\n        p0_ = Coordinate(180, 0)\n        p1_ = Coordinate(180, 1)\n        p2_ = Coordinate(-179, 0)\n        p3_ = Coordinate(-180, -1)\n        p4_ = Coordinate(179, 0)\n        p5_ = Coordinate(-179, 1)\n        p6_ = Coordinate(-179, -1)\n\n        arc13 = Arc(p1_, p3_)\n        arc24 = Arc(p2_, p4_)\n\n        arc32 = Arc(p3_, p2_)\n        arc41 = Arc(p4_, p1_)\n\n        arc40 = Arc(p4_, p0_)\n        arc56 = Arc(p5_, p6_)\n\n        arc45 = Arc(p4_, p5_)\n        arc02 = Arc(p0_, p2_)\n\n        arc35 = Arc(p3_, p5_)\n\n        self.assertTrue(arc13.intersects(arc24))\n\n        self.assertFalse(arc32.intersects(arc41))\n\n        self.assertFalse(arc56.intersects(arc40))\n\n        self.assertFalse(arc56.intersects(arc40))\n\n        self.assertFalse(arc45.intersects(arc02))\n\n        self.assertTrue(arc35.intersects(arc24))\n\n        # case of the north pole\n\n        p0_ = Coordinate(0, 90)\n        p1_ = Coordinate(0, 89)\n        p2_ = Coordinate(90, 89)\n        p3_ = Coordinate(180, 89)\n        p4_ = Coordinate(-90, 89)\n        p5_ = Coordinate(45, 89)\n        p6_ = Coordinate(135, 89)\n\n        arc13 = Arc(p1_, p3_)\n        arc24 = Arc(p2_, p4_)\n\n        arc32 = Arc(p3_, p2_)\n        arc41 = Arc(p4_, p1_)\n\n        arc40 = Arc(p4_, p0_)\n        arc56 = Arc(p5_, p6_)\n\n        arc45 = Arc(p4_, p5_)\n        arc02 = Arc(p0_, p2_)\n\n        arc35 = Arc(p3_, p5_)\n\n        self.assertTrue(arc13.intersects(arc24))\n\n        self.assertFalse(arc32.intersects(arc41))\n\n        self.assertFalse(arc56.intersects(arc40))\n\n        self.assertFalse(arc56.intersects(arc40))\n\n        self.assertFalse(arc45.intersects(arc02))\n\n        self.assertTrue(arc35.intersects(arc24))\n'"
pyresample/test/test_swath.py,5,"b'import os\nimport sys\nimport unittest\nimport warnings\nwarnings.simplefilter(""always"")\n\nimport numpy as np\nfrom pyresample.test.utils import catch_warnings\nfrom pyresample import kd_tree, geometry\n\n\nclass Test(unittest.TestCase):\n\n    filename = os.path.abspath(os.path.join(os.path.dirname(__file__),\n                                            \'test_files\', \'ssmis_swath.npz\'))\n    data = np.load(filename)[\'data\']\n    lons = data[:, 0].astype(np.float64)\n    lats = data[:, 1].astype(np.float64)\n    tb37v = data[:, 2].astype(np.float64)\n\n    # screen out the fill values\n    fvalue = -10000000000.0\n    valid_fov = (lons != fvalue) * (lats != fvalue) * (tb37v != fvalue)\n    lons = lons[valid_fov]\n    lats = lats[valid_fov]\n    tb37v = tb37v[valid_fov]\n\n    def test_self_map(self):\n        swath_def = geometry.SwathDefinition(lons=self.lons, lats=self.lats)\n        with catch_warnings() as w:\n            res = kd_tree.resample_gauss(swath_def, self.tb37v.copy(), swath_def,\n                                         radius_of_influence=70000, sigmas=56500)\n            self.assertFalse(\n                len(w) != 1, \'Failed to create neighbour radius warning\')\n            self.assertFalse((\'Possible more\' not in str(\n                w[0].message)), \'Failed to create correct neighbour radius warning\')\n\n        if sys.platform == \'darwin\':\n            # OSX seems to get slightly different results for `_spatial_mp.Cartesian`\n            truth_value = 668848.144817\n        else:\n            truth_value = 668848.082208\n        self.assertAlmostEqual(res.sum() / 100., truth_value, 1,\n                               msg=\'Failed self mapping swath for 1 channel\')\n\n    def test_self_map_multi(self):\n        data = np.column_stack((self.tb37v, self.tb37v, self.tb37v))\n        swath_def = geometry.SwathDefinition(lons=self.lons, lats=self.lats)\n\n        with catch_warnings() as w:\n            res = kd_tree.resample_gauss(swath_def, data, swath_def,\n                                         radius_of_influence=70000, sigmas=[56500, 56500, 56500])\n            self.assertFalse(\n                len(w) != 1, \'Failed to create neighbour radius warning\')\n            self.assertFalse((\'Possible more\' not in str(\n                w[0].message)), \'Failed to create correct neighbour radius warning\')\n\n        if sys.platform == \'darwin\':\n            # OSX seems to get slightly different results for `_spatial_mp.Cartesian`\n            truth_value = 668848.144817\n        else:\n            truth_value = 668848.082208\n        self.assertAlmostEqual(res[:, 0].sum() / 100., truth_value, 1,\n                               msg=\'Failed self mapping swath multi for channel 1\')\n        self.assertAlmostEqual(res[:, 1].sum() / 100., truth_value, 1,\n                               msg=\'Failed self mapping swath multi for channel 2\')\n        self.assertAlmostEqual(res[:, 2].sum() / 100., truth_value, 1,\n                               msg=\'Failed self mapping swath multi for channel 3\')\n\n\ndef suite():\n    """"""The test suite.\n    """"""\n    loader = unittest.TestLoader()\n    mysuite = unittest.TestSuite()\n    mysuite.addTest(loader.loadTestsFromTestCase(Test))\n\n    return mysuite\n'"
pyresample/test/test_utils.py,43,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2015-2020 Pyresample developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Test various utility functions.""""""\n\nimport os\nimport unittest\nfrom unittest import mock\nimport io\nimport pathlib\nfrom tempfile import NamedTemporaryFile\n\nimport numpy as np\nimport uuid\n\nfrom pyresample.test.utils import create_test_longitude, create_test_latitude\n\n\ndef tmptiff(width=100, height=100, transform=None, crs=None, dtype=np.uint8):\n    import rasterio\n    array = np.ones((width, height)).astype(dtype)\n    fname = \'/vsimem/%s\' % uuid.uuid4()\n    with rasterio.open(fname, \'w\', driver=\'GTiff\', count=1, transform=transform,\n                       width=width, height=height, crs=crs, dtype=dtype) as dst:\n        dst.write(array, 1)\n    return fname\n\n\nclass TestLegacyAreaParser(unittest.TestCase):\n    def test_area_parser_legacy(self):\n        """"""Test legacy area parser.""""""\n        from pyresample import parse_area_file\n        from pyresample.utils import is_pyproj2\n        ease_nh, ease_sh = parse_area_file(os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.cfg\'),\n                                           \'ease_nh\', \'ease_sh\')\n\n        if is_pyproj2():\n            # pyproj 2.0+ adds some extra parameters\n            projection = (""{\'R\': \'6371228\', \'lat_0\': \'90\', \'lon_0\': \'0\', ""\n                          ""\'no_defs\': \'None\', \'proj\': \'laea\', \'type\': \'crs\', ""\n                          ""\'units\': \'m\', \'x_0\': \'0\', \'y_0\': \'0\'}"")\n        else:\n            projection = (""{\'a\': \'6371228.0\', \'lat_0\': \'90.0\', ""\n                          ""\'lon_0\': \'0.0\', \'proj\': \'laea\', \'units\': \'m\'}"")\n        nh_str = """"""Area ID: ease_nh\nDescription: Arctic EASE grid\nProjection ID: ease_nh\nProjection: {}\nNumber of columns: 425\nNumber of rows: 425\nArea extent: (-5326849.0625, -5326849.0625, 5326849.0625, 5326849.0625)"""""".format(projection)\n        self.assertEqual(ease_nh.__str__(), nh_str)\n        self.assertIsInstance(ease_nh.proj_dict[\'lat_0\'], (int, float))\n\n        if is_pyproj2():\n            projection = (""{\'R\': \'6371228\', \'lat_0\': \'-90\', \'lon_0\': \'0\', ""\n                          ""\'no_defs\': \'None\', \'proj\': \'laea\', \'type\': \'crs\', ""\n                          ""\'units\': \'m\', \'x_0\': \'0\', \'y_0\': \'0\'}"")\n        else:\n            projection = (""{\'a\': \'6371228.0\', \'lat_0\': \'-90.0\', ""\n                          ""\'lon_0\': \'0.0\', \'proj\': \'laea\', \'units\': \'m\'}"")\n        sh_str = """"""Area ID: ease_sh\nDescription: Antarctic EASE grid\nProjection ID: ease_sh\nProjection: {}\nNumber of columns: 425\nNumber of rows: 425\nArea extent: (-5326849.0625, -5326849.0625, 5326849.0625, 5326849.0625)"""""".format(projection)\n        self.assertEqual(ease_sh.__str__(), sh_str)\n        self.assertIsInstance(ease_sh.proj_dict[\'lat_0\'], (int, float))\n\n    def test_load_area(self):\n        from pyresample import load_area\n        from pyresample.utils import is_pyproj2\n        ease_nh = load_area(os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.cfg\'), \'ease_nh\')\n        if is_pyproj2():\n            # pyproj 2.0+ adds some extra parameters\n            projection = (""{\'R\': \'6371228\', \'lat_0\': \'90\', \'lon_0\': \'0\', ""\n                          ""\'no_defs\': \'None\', \'proj\': \'laea\', \'type\': \'crs\', ""\n                          ""\'units\': \'m\', \'x_0\': \'0\', \'y_0\': \'0\'}"")\n        else:\n            projection = (""{\'a\': \'6371228.0\', \'lat_0\': \'90.0\', ""\n                          ""\'lon_0\': \'0.0\', \'proj\': \'laea\', \'units\': \'m\'}"")\n        nh_str = """"""Area ID: ease_nh\nDescription: Arctic EASE grid\nProjection ID: ease_nh\nProjection: {}\nNumber of columns: 425\nNumber of rows: 425\nArea extent: (-5326849.0625, -5326849.0625, 5326849.0625, 5326849.0625)"""""".format(projection)\n        self.assertEqual(nh_str, ease_nh.__str__())\n\n    def test_area_file_not_found_exception(self):\n        from pyresample.area_config import load_area\n        self.assertRaises(FileNotFoundError, load_area,\n                          ""/this/file/does/not/exist.yaml"")\n        self.assertRaises(FileNotFoundError, load_area,\n                          pathlib.Path(""/this/file/does/not/exist.yaml""))\n\n    def test_not_found_exception(self):\n        from pyresample.area_config import AreaNotFound, parse_area_file\n        self.assertRaises(AreaNotFound, parse_area_file,\n                          os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.cfg\'), \'no_area\')\n\n    def test_commented(self):\n        from pyresample import parse_area_file\n        areas = parse_area_file(os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.cfg\'))\n        self.assertNotIn(\'commented\', [area.name for area in areas])\n\n\nclass TestYAMLAreaParser(unittest.TestCase):\n    def test_area_parser_yaml(self):\n        """"""Test YAML area parser.""""""\n        from pyresample import parse_area_file\n        test_area_file = os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.yaml\')\n        test_areas = parse_area_file(test_area_file, \'ease_nh\', \'ease_sh\', \'test_meters\', \'test_degrees\',\n                                     \'test_latlong\')\n        ease_nh, ease_sh, test_m, test_deg, test_latlong = test_areas\n\n        from pyresample.utils import is_pyproj2\n        if is_pyproj2():\n            # pyproj 2.0+ adds some extra parameters\n            projection = (""{\'R\': \'6371228\', \'lat_0\': \'-90\', \'lon_0\': \'0\', ""\n                          ""\'no_defs\': \'None\', \'proj\': \'laea\', \'type\': \'crs\', ""\n                          ""\'units\': \'m\', \'x_0\': \'0\', \'y_0\': \'0\'}"")\n        else:\n            projection = (""{\'a\': \'6371228.0\', \'lat_0\': \'-90.0\', ""\n                          ""\'lon_0\': \'0.0\', \'proj\': \'laea\', \'units\': \'m\'}"")\n        nh_str = """"""Area ID: ease_nh\nDescription: Arctic EASE grid\nProjection: {}\nNumber of columns: 425\nNumber of rows: 425\nArea extent: (-5326849.0625, -5326849.0625, 5326849.0625, 5326849.0625)"""""".format(projection)\n        self.assertEqual(ease_nh.__str__(), nh_str)\n\n        sh_str = """"""Area ID: ease_sh\nDescription: Antarctic EASE grid\nProjection: {}\nNumber of columns: 425\nNumber of rows: 425\nArea extent: (-5326849.0625, -5326849.0625, 5326849.0625, 5326849.0625)"""""".format(projection)\n        self.assertEqual(ease_sh.__str__(), sh_str)\n\n        m_str = """"""Area ID: test_meters\nDescription: test_meters\nProjection: {}\nNumber of columns: 850\nNumber of rows: 425\nArea extent: (-5326849.0625, -5326849.0625, 5326849.0625, 5326849.0625)"""""".format(projection)\n        self.assertEqual(test_m.__str__(), m_str)\n\n        deg_str = """"""Area ID: test_degrees\nDescription: test_degrees\nProjection: {}\nNumber of columns: 850\nNumber of rows: 425\nArea extent: (-5326849.0625, -5326849.0625, 5326849.0625, 5326849.0625)"""""".format(projection)\n        self.assertEqual(test_deg.__str__(), deg_str)\n\n        if is_pyproj2():\n            # pyproj 2.0+ adds some extra parameters\n            projection = (""{\'ellps\': \'WGS84\', \'no_defs\': \'None\', ""\n                          ""\'pm\': \'-81.36\', \'proj\': \'longlat\', ""\n                          ""\'type\': \'crs\'}"")\n        else:\n            projection = (""{\'ellps\': \'WGS84\', \'lat_0\': \'27.12\', ""\n                          ""\'lon_0\': \'-81.36\', \'proj\': \'longlat\'}"")\n        latlong_str = """"""Area ID: test_latlong\nDescription: Basic latlong grid\nProjection: {}\nNumber of columns: 3473\nNumber of rows: 4058\nArea extent: (-0.0812, 0.4039, 0.0812, 0.5428)"""""".format(projection)\n        self.assertEqual(test_latlong.__str__(), latlong_str)\n\n    def test_dynamic_area_parser_yaml(self):\n        """"""Test YAML area parser on dynamic areas.""""""\n        from pyresample import parse_area_file\n        from pyresample.geometry import DynamicAreaDefinition\n        test_area_file = os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.yaml\')\n        test_area = parse_area_file(test_area_file, \'test_dynamic_resolution\')[0]\n\n        self.assertIsInstance(test_area, DynamicAreaDefinition)\n        self.assertTrue(hasattr(test_area, \'resolution\'))\n        self.assertEqual(test_area.resolution, (1000.0, 1000.0))\n\n        # lat/lon\n        from pyresample import parse_area_file\n        from pyresample.geometry import DynamicAreaDefinition\n        test_area_file = os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.yaml\')\n        test_area = parse_area_file(test_area_file, \'test_dynamic_resolution_ll\')[0]\n\n        self.assertIsInstance(test_area, DynamicAreaDefinition)\n        self.assertTrue(hasattr(test_area, \'resolution\'))\n        self.assertEqual(test_area.resolution, (1.0, 1.0))\n\n    def test_multiple_file_content(self):\n        from pyresample import parse_area_file\n        from pyresample.area_config import load_area_from_string\n        area_list = [""""""ease_sh:\n  description: Antarctic EASE grid\n  projection:\n    a: 6371228.0\n    units: m\n    lon_0: 0\n    proj: laea\n    lat_0: -90\n  shape:\n    height: 425\n    width: 425\n  area_extent:\n    lower_left_xy: [-5326849.0625, -5326849.0625]\n    upper_right_xy: [5326849.0625, 5326849.0625]\n    units: m\n"""""",\n                     """"""ease_sh2:\n  description: Antarctic EASE grid\n  projection:\n    a: 6371228.0\n    units: m\n    lon_0: 0\n    proj: laea\n    lat_0: -90\n  shape:\n    height: 425\n    width: 425\n  area_extent:\n    lower_left_xy: [-5326849.0625, -5326849.0625]\n    upper_right_xy: [5326849.0625, 5326849.0625]\n    units: m\n""""""]\n        with self.assertWarns(DeprecationWarning):\n            results = parse_area_file(area_list)\n        self.assertEqual(len(results), 2)\n        self.assertIn(results[0].area_id, (\'ease_sh\', \'ease_sh2\'))\n        self.assertIn(results[1].area_id, (\'ease_sh\', \'ease_sh2\'))\n        results2 = parse_area_file([io.StringIO(ar) for ar in area_list])\n        results3 = load_area_from_string(area_list)\n        self.assertEqual(results, results2)\n        self.assertEqual(results, results3)\n\n\nclass TestPreprocessing(unittest.TestCase):\n    def test_nearest_neighbor_area_area(self):\n        from pyresample import utils, geometry\n        proj_str = ""+proj=lcc +datum=WGS84 +ellps=WGS84 +lat_0=25 +lat_1=25 +lon_0=-95 +units=m +no_defs""\n        proj_dict = utils.proj4.proj4_str_to_dict(proj_str)\n        extents = [0, 0, 1000. * 5000, 1000. * 5000]\n        area_def = geometry.AreaDefinition(\'CONUS\', \'CONUS\', \'CONUS\',\n                                           proj_dict, 400, 500, extents)\n\n        extents2 = [-1000, -1000, 1000. * 4000, 1000. * 4000]\n        area_def2 = geometry.AreaDefinition(\'CONUS\', \'CONUS\', \'CONUS\',\n                                            proj_dict, 600, 700, extents2)\n        rows, cols = utils.generate_nearest_neighbour_linesample_arrays(area_def, area_def2, 12000.)\n\n    def test_nearest_neighbor_area_grid(self):\n        from pyresample import utils, geometry\n        lon_arr = create_test_longitude(-94.9, -90.0, (50, 100), dtype=np.float64)\n        lat_arr = create_test_latitude(25.1, 30.0, (50, 100), dtype=np.float64)\n        grid = geometry.GridDefinition(lons=lon_arr, lats=lat_arr)\n\n        proj_str = ""+proj=lcc +datum=WGS84 +ellps=WGS84 +lat_0=25 +lat_1=25 +lon_0=-95 +units=m +no_defs""\n        proj_dict = utils.proj4.proj4_str_to_dict(proj_str)\n        extents = [0, 0, 1000. * 5000, 1000. * 5000]\n        area_def = geometry.AreaDefinition(\'CONUS\', \'CONUS\', \'CONUS\',\n                                           proj_dict, 400, 500, extents)\n        rows, cols = utils.generate_nearest_neighbour_linesample_arrays(area_def, grid, 12000.)\n\n    def test_nearest_neighbor_grid_area(self):\n        from pyresample import utils, geometry\n        proj_str = ""+proj=lcc +datum=WGS84 +ellps=WGS84 +lat_0=25 +lat_1=25 +lon_0=-95 +units=m +no_defs""\n        proj_dict = utils.proj4.proj4_str_to_dict(proj_str)\n        extents = [0, 0, 1000. * 2500., 1000. * 2000.]\n        area_def = geometry.AreaDefinition(\'CONUS\', \'CONUS\', \'CONUS\',\n                                           proj_dict, 40, 50, extents)\n\n        lon_arr = create_test_longitude(-100.0, -60.0, (550, 500), dtype=np.float64)\n        lat_arr = create_test_latitude(20.0, 45.0, (550, 500), dtype=np.float64)\n        grid = geometry.GridDefinition(lons=lon_arr, lats=lat_arr)\n        rows, cols = utils.generate_nearest_neighbour_linesample_arrays(grid, area_def, 12000.)\n\n    def test_nearest_neighbor_grid_grid(self):\n        from pyresample import utils, geometry\n        lon_arr = create_test_longitude(-95.0, -85.0, (40, 50), dtype=np.float64)\n        lat_arr = create_test_latitude(25.0, 35.0, (40, 50), dtype=np.float64)\n        grid_dst = geometry.GridDefinition(lons=lon_arr, lats=lat_arr)\n\n        lon_arr = create_test_longitude(-100.0, -80.0, (400, 500), dtype=np.float64)\n        lat_arr = create_test_latitude(20.0, 40.0, (400, 500), dtype=np.float64)\n        grid = geometry.GridDefinition(lons=lon_arr, lats=lat_arr)\n        rows, cols = utils.generate_nearest_neighbour_linesample_arrays(grid, grid_dst, 12000.)\n\n\nclass TestMisc(unittest.TestCase):\n    def test_wrap_longitudes(self):\n        # test that we indeed wrap to [-180:+180[\n        from pyresample import utils\n        step = 60\n        lons = np.arange(-360, 360 + step, step)\n        self.assertTrue(\n            (lons.min() < -180) and (lons.max() >= 180) and (+180 in lons))\n        wlons = utils.wrap_longitudes(lons)\n        self.assertFalse(\n            (wlons.min() < -180) or (wlons.max() >= 180) or (+180 in wlons))\n\n    def test_wrap_and_check(self):\n        from pyresample import utils\n\n        lons1 = np.arange(-135., +135, 50.)\n        lats = np.ones_like(lons1) * 70.\n        new_lons, new_lats = utils.check_and_wrap(lons1, lats)\n        self.assertIs(lats, new_lats)\n        self.assertTrue(np.isclose(lons1, new_lons).all())\n\n        lons2 = np.where(lons1 < 0, lons1 + 360, lons1)\n        new_lons, new_lats = utils.check_and_wrap(lons2, lats)\n        self.assertIs(lats, new_lats)\n        # after wrapping lons2 should look like lons1\n        self.assertTrue(np.isclose(lons1, new_lons).all())\n\n        lats2 = lats + 25.\n        self.assertRaises(ValueError, utils.check_and_wrap, lons1, lats2)\n\n    def test_unicode_proj4_string(self):\n        """"""Test that unicode is accepted for area creation.""""""\n        from pyresample import get_area_def\n        get_area_def(u""eurol"", u""eurol"", u""bla"",\n                     u\'+proj=stere +a=6378273 +b=6356889.44891 +lat_0=90 +lat_ts=70 +lon_0=-45\',\n                     1000, 1000, (-1000, -1000, 1000, 1000))\n\n    def test_proj4_radius_parameters_provided(self):\n        """"""Test proj4_radius_parameters with a/b.""""""\n        from pyresample import utils\n        a, b = utils.proj4.proj4_radius_parameters(\n            \'+proj=stere +a=6378273 +b=6356889.44891\',\n        )\n        np.testing.assert_almost_equal(a, 6378273)\n        np.testing.assert_almost_equal(b, 6356889.44891)\n\n        # test again but force pyproj <2 behavior\n        with mock.patch.object(utils.proj4, \'CRS\', None):\n            a, b = utils.proj4.proj4_radius_parameters(\n                \'+proj=stere +a=6378273 +b=6356889.44891\',\n            )\n            np.testing.assert_almost_equal(a, 6378273)\n            np.testing.assert_almost_equal(b, 6356889.44891)\n\n    def test_proj4_radius_parameters_ellps(self):\n        """"""Test proj4_radius_parameters with ellps.""""""\n        from pyresample import utils\n        a, b = utils.proj4.proj4_radius_parameters(\n            \'+proj=stere +ellps=WGS84\',\n        )\n        np.testing.assert_almost_equal(a, 6378137.)\n        np.testing.assert_almost_equal(b, 6356752.314245, decimal=6)\n\n        # test again but force pyproj <2 behavior\n        with mock.patch.object(utils.proj4, \'CRS\', None):\n            a, b = utils.proj4.proj4_radius_parameters(\n                \'+proj=stere +ellps=WGS84\',\n            )\n            np.testing.assert_almost_equal(a, 6378137.)\n            np.testing.assert_almost_equal(b, 6356752.314245, decimal=6)\n\n    def test_proj4_radius_parameters_default(self):\n        """"""Test proj4_radius_parameters with default parameters.""""""\n        from pyresample import utils\n        a, b = utils.proj4.proj4_radius_parameters(\n            \'+proj=lcc +lat_0=10 +lat_1=10\',\n        )\n        # WGS84\n        np.testing.assert_almost_equal(a, 6378137.)\n        np.testing.assert_almost_equal(b, 6356752.314245, decimal=6)\n\n        # test again but force pyproj <2 behavior\n        with mock.patch.object(utils.proj4, \'CRS\', None):\n            a, b = utils.proj4.proj4_radius_parameters(\n                \'+proj=lcc +lat_0=10 +lat_1=10\',\n            )\n            # WGS84\n            np.testing.assert_almost_equal(a, 6378137.)\n            np.testing.assert_almost_equal(b, 6356752.314245, decimal=6)\n\n    def test_proj4_radius_parameters_spherical(self):\n        """"""Test proj4_radius_parameters in case of a spherical earth.""""""\n        from pyresample import utils\n        a, b = utils.proj4.proj4_radius_parameters(\n            \'+proj=stere +R=6378273\',\n        )\n        np.testing.assert_almost_equal(a, 6378273.)\n        np.testing.assert_almost_equal(b, 6378273.)\n\n        # test again but force pyproj <2 behavior\n        with mock.patch.object(utils.proj4, \'CRS\', None):\n            a, b = utils.proj4.proj4_radius_parameters(\n                \'+proj=stere +R=6378273\',\n            )\n            np.testing.assert_almost_equal(a, 6378273.)\n            np.testing.assert_almost_equal(b, 6378273.)\n\n    def test_convert_proj_floats(self):\n        from collections import OrderedDict\n        import pyresample.utils as utils\n\n        pairs = [(\'proj\', \'lcc\'), (\'ellps\', \'WGS84\'), (\'lon_0\', \'-95\'), (\'no_defs\', True)]\n        expected = OrderedDict([(\'proj\', \'lcc\'), (\'ellps\', \'WGS84\'), (\'lon_0\', -95.0), (\'no_defs\', True)])\n        self.assertDictEqual(utils.proj4.convert_proj_floats(pairs), expected)\n\n        # EPSG\n        pairs = [(\'init\', \'EPSG:4326\'), (\'EPSG\', 4326)]\n        for pair in pairs:\n            expected = OrderedDict([pair])\n            self.assertDictEqual(utils.proj4.convert_proj_floats([pair]), expected)\n\n    def test_proj4_str_dict_conversion(self):\n        from pyresample import utils\n\n        proj_str = ""+proj=lcc +ellps=WGS84 +lon_0=-95 +no_defs""\n        proj_dict = utils.proj4.proj4_str_to_dict(proj_str)\n        proj_str2 = utils.proj4.proj4_dict_to_str(proj_dict)\n        proj_dict2 = utils.proj4.proj4_str_to_dict(proj_str2)\n        self.assertDictEqual(proj_dict, proj_dict2)\n        self.assertIsInstance(proj_dict[\'lon_0\'], float)\n        self.assertIsInstance(proj_dict2[\'lon_0\'], float)\n\n        # EPSG\n        proj_str = \'+init=EPSG:4326\'\n        proj_dict_exp = {\'init\': \'EPSG:4326\'}\n        proj_dict = utils.proj4.proj4_str_to_dict(proj_str)\n        self.assertEqual(proj_dict, proj_dict_exp)\n        self.assertEqual(utils.proj4.proj4_dict_to_str(proj_dict), proj_str)  # round-trip\n\n        proj_str = \'EPSG:4326\'\n        proj_dict_exp = {\'init\': \'EPSG:4326\'}\n        proj_dict_exp2 = {\'proj\': \'longlat\', \'datum\': \'WGS84\', \'no_defs\': None, \'type\': \'crs\'}\n        proj_dict = utils.proj4.proj4_str_to_dict(proj_str)\n        if \'init\' in proj_dict:\n            # pyproj <2.0\n            self.assertEqual(proj_dict, proj_dict_exp)\n        else:\n            # pyproj 2.0+\n            self.assertEqual(proj_dict, proj_dict_exp2)\n        # input != output for this style of EPSG code\n        # EPSG to PROJ.4 can be lossy\n        # self.assertEqual(utils._proj4.proj4_dict_to_str(proj_dict), proj_str)  # round-trip\n\n    def test_def2yaml_converter(self):\n        from pyresample import parse_area_file, convert_def_to_yaml\n        from pyresample.utils import is_pyproj2\n        import tempfile\n        def_file = os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.cfg\')\n        filehandle, yaml_file = tempfile.mkstemp()\n        os.close(filehandle)\n        try:\n            convert_def_to_yaml(def_file, yaml_file)\n            areas_new = set(parse_area_file(yaml_file))\n            areas = parse_area_file(def_file)\n            for area in areas:\n                if is_pyproj2():\n                    # pyproj 2.0 adds units back in\n                    # pyproj <2 doesn\'t\n                    continue\n                # initialize _proj_dict\n                area.proj_dict  # noqa\n                area._proj_dict.pop(\'units\', None)\n            areas_old = set(areas)\n            areas_new = {area.area_id: area for area in areas_new}\n            areas_old = {area.area_id: area for area in areas_old}\n            self.assertEqual(areas_new, areas_old)\n        finally:\n            os.remove(yaml_file)\n\n    def test_get_area_def_from_raster(self):\n        from pyresample import utils\n        from rasterio.crs import CRS\n        from affine import Affine\n        x_size = 791\n        y_size = 718\n        transform = Affine(300.0379266750948, 0.0, 101985.0,\n                           0.0, -300.041782729805, 2826915.0)\n        crs = CRS(init=\'epsg:3857\')\n        if utils.is_pyproj2():\n            # pyproj 2.0+ expands CRS parameters\n            from pyproj import CRS\n            proj_dict = CRS(3857).to_dict()\n        else:\n            proj_dict = crs.to_dict()\n        source = tmptiff(x_size, y_size, transform, crs=crs)\n        area_id = \'area_id\'\n        proj_id = \'proj_id\'\n        description = \'name\'\n        area_def = utils.rasterio.get_area_def_from_raster(\n            source, area_id=area_id, name=description, proj_id=proj_id)\n        self.assertEqual(area_def.area_id, area_id)\n        self.assertEqual(area_def.proj_id, proj_id)\n        self.assertEqual(area_def.description, description)\n        self.assertEqual(area_def.width, x_size)\n        self.assertEqual(area_def.height, y_size)\n        self.assertDictEqual(proj_dict, area_def.proj_dict)\n        self.assertTupleEqual(area_def.area_extent, (transform.c, transform.f + transform.e * y_size,\n                                                     transform.c + transform.a * x_size, transform.f))\n\n    def test_get_area_def_from_raster_extracts_proj_id(self):\n        from rasterio.crs import CRS\n        from pyresample import utils\n        crs = CRS(init=\'epsg:3857\')\n        source = tmptiff(crs=crs)\n        area_def = utils.rasterio.get_area_def_from_raster(source)\n        self.assertEqual(area_def.proj_id, \'WGS 84 / Pseudo-Mercator\')\n\n    def test_get_area_def_from_raster_rotated_value_err(self):\n        from pyresample import utils\n        from affine import Affine\n        transform = Affine(300.0379266750948, 0.1, 101985.0,\n                           0.0, -300.041782729805, 2826915.0)\n        source = tmptiff(transform=transform)\n        self.assertRaises(ValueError, utils.rasterio.get_area_def_from_raster, source)\n\n    def test_get_area_def_from_raster_non_georef_value_err(self):\n        from pyresample import utils\n        from affine import Affine\n        transform = Affine(300.0379266750948, 0.0, 101985.0,\n                           0.0, -300.041782729805, 2826915.0)\n        source = tmptiff(transform=transform)\n        self.assertRaises(ValueError, utils.rasterio.get_area_def_from_raster, source)\n\n    def test_get_area_def_from_raster_non_georef_respects_proj_dict(self):\n        from pyresample import utils\n        from affine import Affine\n        transform = Affine(300.0379266750948, 0.0, 101985.0,\n                           0.0, -300.041782729805, 2826915.0)\n        source = tmptiff(transform=transform)\n        proj_dict = {\'init\': \'epsg:3857\'}\n        area_def = utils.rasterio.get_area_def_from_raster(source, proj_dict=proj_dict)\n        if utils.is_pyproj2():\n            from pyproj import CRS\n            proj_dict = CRS(3857).to_dict()\n        self.assertDictEqual(area_def.proj_dict, proj_dict)\n\n\nclass TestProjRotation(unittest.TestCase):\n    """"""Test loading areas with rotation specified.""""""\n\n    def test_rotation_legacy(self):\n        """"""Basic rotation in legacy format.""""""\n        from pyresample.area_config import load_area\n        legacyDef = """"""REGION: regionB {\n        NAME:          regionB\n        PCS_ID:        regionB\n        PCS_DEF:       proj=merc, lon_0=-34, k=1, x_0=0, y_0=0, a=6378137, b=6378137\n        XSIZE:         800\n        YSIZE:         548\n        ROTATION:      -45\n        AREA_EXTENT:   (-7761424.714818418, -4861746.639279127, 11136477.43264252, 8236799.845095873)\n        };""""""\n        with NamedTemporaryFile(mode=""w"", suffix=\'.cfg\', delete=False) as f:\n            f.write(legacyDef)\n        test_area = load_area(f.name, \'regionB\')\n        self.assertEqual(test_area.rotation, -45)\n        os.remove(f.name)\n\n    def test_rotation_yaml(self):\n        """"""Basic rotation in yaml format.""""""\n        from pyresample.area_config import load_area\n        yamlDef = """"""regionB:\n          description: regionB\n          projection:\n            a: 6378137.0\n            b: 6378137.0\n            lon_0: -34\n            proj: merc\n            x_0: 0\n            y_0: 0\n            k_0: 1\n          shape:\n            height: 548\n            width: 800\n          rotation: -45\n          area_extent:\n            lower_left_xy: [-7761424.714818418, -4861746.639279127]\n            upper_right_xy: [11136477.43264252, 8236799.845095873]\n          units: m""""""\n        with NamedTemporaryFile(mode=""w"", suffix=\'.yaml\', delete=False) as f:\n            f.write(yamlDef)\n        test_area = load_area(f.name, \'regionB\')\n        self.assertEqual(test_area.rotation, -45)\n        os.remove(f.name)\n\n    def test_norotation_legacy(self):\n        """"""No rotation specified in legacy format.""""""\n        from pyresample.area_config import load_area\n        legacyDef = """"""REGION: regionB {\n        NAME:          regionB\n        PCS_ID:        regionB\n        PCS_DEF:       proj=merc, lon_0=-34, k=1, x_0=0, y_0=0, a=6378137, b=6378137\n        XSIZE:         800\n        YSIZE:         548\n        AREA_EXTENT:   (-7761424.714818418, -4861746.639279127, 11136477.43264252, 8236799.845095873)\n        };""""""\n        with NamedTemporaryFile(mode=""w"", suffix=\'.cfg\', delete=False) as f:\n            f.write(legacyDef)\n        test_area = load_area(f.name, \'regionB\')\n        self.assertEqual(test_area.rotation, 0)\n        os.remove(f.name)\n\n    def test_norotation_yaml(self):\n        """"""No rotation specified in yaml format.""""""\n        from pyresample.area_config import load_area\n        yamlDef = """"""regionB:\n          description: regionB\n          projection:\n            a: 6378137.0\n            b: 6378137.0\n            lon_0: -34\n            proj: merc\n            x_0: 0\n            y_0: 0\n            k_0: 1\n          shape:\n            height: 548\n            width: 800\n          area_extent:\n            lower_left_xy: [-7761424.714818418, -4861746.639279127]\n            upper_right_xy: [11136477.43264252, 8236799.845095873]\n          units: m""""""\n        with NamedTemporaryFile(mode=""w"", suffix=\'.yaml\', delete=False) as f:\n            f.write(yamlDef)\n        test_area = load_area(f.name, \'regionB\')\n        self.assertEqual(test_area.rotation, 0)\n        os.remove(f.name)\n\n\n# helper routines for the CF test cases\ndef _prepare_cf_nh10km():\n    import xarray as xr\n    nx = 760\n    ny = 1120\n    ds = xr.Dataset({\'ice_conc\': ((\'time\', \'yc\', \'xc\'), np.ma.masked_all((1, ny, nx)),\n                                  {\'grid_mapping\': \'Polar_Stereographic_Grid\'}),\n                     \'xc\': (\'xc\', np.linspace(-3845, 3745, num=nx),\n                            {\'standard_name\': \'projection_x_coordinate\', \'units\': \'km\'}),\n                     \'yc\': (\'yc\', np.linspace(+5845, -5345, num=ny),\n                            {\'standard_name\': \'projection_y_coordinate\', \'units\': \'km\'})},\n                    coords={\'lat\': ((\'yc\', \'xc\'), np.ma.masked_all((ny, nx))),\n                            \'lon\': ((\'yc\', \'xc\'), np.ma.masked_all((ny, nx)))},)\n    ds[\'lat\'].attrs[\'units\'] = \'degrees_north\'\n    ds[\'lat\'].attrs[\'standard_name\'] = \'latitude\'\n    ds[\'lon\'].attrs[\'units\'] = \'degrees_east\'\n    ds[\'lon\'].attrs[\'standard_name\'] = \'longitude\'\n\n    ds[\'Polar_Stereographic_Grid\'] = 0\n    ds[\'Polar_Stereographic_Grid\'].attrs[\'grid_mapping_name\'] = ""polar_stereographic""\n    ds[\'Polar_Stereographic_Grid\'].attrs[\'false_easting\'] = 0.\n    ds[\'Polar_Stereographic_Grid\'].attrs[\'false_northing\'] = 0.\n    ds[\'Polar_Stereographic_Grid\'].attrs[\'semi_major_axis\'] = 6378273.\n    ds[\'Polar_Stereographic_Grid\'].attrs[\'semi_minor_axis\'] = 6356889.44891\n    ds[\'Polar_Stereographic_Grid\'].attrs[\'straight_vertical_longitude_from_pole\'] = -45.\n    ds[\'Polar_Stereographic_Grid\'].attrs[\'latitude_of_projection_origin\'] = 90.\n    ds[\'Polar_Stereographic_Grid\'].attrs[\'standard_parallel\'] = 70.\n\n    return ds\n\n\ndef _prepare_cf_llwgs84():\n    import xarray as xr\n    nlat = 19\n    nlon = 37\n    ds = xr.Dataset({\'temp\': ((\'lat\', \'lon\'), np.ma.masked_all((nlat, nlon)), {\'grid_mapping\': \'crs\'})},\n                    coords={\'lat\': np.linspace(-90., +90., num=nlat),\n                            \'lon\': np.linspace(-180., +180., num=nlon)})\n    ds[\'lat\'].attrs[\'units\'] = \'degreesN\'\n    ds[\'lat\'].attrs[\'standard_name\'] = \'latitude\'\n    ds[\'lon\'].attrs[\'units\'] = \'degreesE\'\n    ds[\'lon\'].attrs[\'standard_name\'] = \'longitude\'\n\n    ds[\'crs\'] = 0\n    ds[\'crs\'].attrs[\'grid_mapping_name\'] = ""latitude_longitude""\n    ds[\'crs\'].attrs[\'longitude_of_prime_meridian\'] = 0.\n    ds[\'crs\'].attrs[\'semi_major_axis\'] = 6378137.\n    ds[\'crs\'].attrs[\'inverse_flattening\'] = 298.257223563\n\n    return ds\n\n\ndef _prepare_cf_llnocrs():\n    import xarray as xr\n    nlat = 19\n    nlon = 37\n    ds = xr.Dataset({\'temp\': ((\'lat\', \'lon\'), np.ma.masked_all((nlat, nlon)))},\n                    coords={\'lat\': np.linspace(-90., +90., num=nlat),\n                            \'lon\': np.linspace(-180., +180., num=nlon)})\n    ds[\'lat\'].attrs[\'units\'] = \'degreeN\'\n    ds[\'lat\'].attrs[\'standard_name\'] = \'latitude\'\n    ds[\'lon\'].attrs[\'units\'] = \'degreeE\'\n    ds[\'lon\'].attrs[\'standard_name\'] = \'longitude\'\n\n    return ds\n\n\nclass TestLoadCFArea_Public(unittest.TestCase):\n    """"""Test public API load_cf_area() for loading an AreaDefinition from netCDF/CF files.""""""\n\n    def test_load_cf_from_wrong_filepath(self):\n        from pyresample.utils import load_cf_area\n\n        # wrong case #1: the path does not exist\n        cf_file = os.path.join(os.path.dirname(__file__), \'test_files\', \'does_not_exist.nc\')\n        self.assertRaises(FileNotFoundError, load_cf_area, cf_file)\n\n        # wrong case #2: the path exists, but is not a netCDF file\n        cf_file = os.path.join(os.path.dirname(__file__), \'test_files\', \'areas.yaml\')\n        self.assertRaises(OSError, load_cf_area, cf_file)\n\n    def test_load_cf_parameters_errors(self):\n        from pyresample.utils import load_cf_area\n\n        # prepare xarray Dataset\n        cf_file = _prepare_cf_nh10km()\n\n        # try to load from a variable= that does not exist\n        self.assertRaises(KeyError, load_cf_area, cf_file, \'doesNotExist\')\n\n        # try to load from a variable= that is itself is a grid_mapping, but without y= or x=\n        self.assertRaises(ValueError, load_cf_area, cf_file, \'Polar_Stereographic_Grid\',)\n\n        # try to load using a variable= that is a valid grid_mapping container, but use wrong x= and y=\n        self.assertRaises(KeyError, load_cf_area, cf_file, \'Polar_Stereographic_Grid\', y=\'doesNotExist\', x=\'xc\',)\n        self.assertRaises(ValueError, load_cf_area, cf_file, \'Polar_Stereographic_Grid\', y=\'time\', x=\'xc\',)\n\n        # try to load using a variable= that does not define a grid mapping\n        self.assertRaises(ValueError, load_cf_area, cf_file, \'lat\',)\n\n    def test_load_cf_nh10km(self):\n        from pyresample.utils import load_cf_area\n\n        def validate_nh10km_adef(adef):\n            self.assertEqual(adef.shape, (1120, 760))\n            xc = adef.projection_x_coords\n            yc = adef.projection_y_coords\n            self.assertEqual(xc[0], -3845000.0, msg=""Wrong x axis (index 0)"")\n            self.assertEqual(xc[1], xc[0] + 10000.0, msg=""Wrong x axis (index 1)"")\n            self.assertEqual(yc[0], 5845000.0, msg=""Wrong y axis (index 0)"")\n            self.assertEqual(yc[1], yc[0] - 10000.0, msg=""Wrong y axis (index 1)"")\n\n        # prepare xarray Dataset\n        cf_file = _prepare_cf_nh10km()\n\n        # load using a variable= that is a valid grid_mapping container\n        adef, _ = load_cf_area(cf_file, \'Polar_Stereographic_Grid\', y=\'yc\', x=\'xc\',)\n        validate_nh10km_adef(adef)\n\n        # load using a variable= that has a :grid_mapping attribute\n        adef, _ = load_cf_area(cf_file, \'ice_conc\')\n        validate_nh10km_adef(adef)\n\n        # load without using a variable=\n        adef, _ = load_cf_area(cf_file)\n        validate_nh10km_adef(adef)\n\n    def test_load_cf_nh10km_cfinfo(self):\n        from pyresample.utils import load_cf_area\n\n        def validate_nh10km_cfinfo(cfinfo, variable=\'ice_conc\', lat=\'lat\', lon=\'lon\'):\n            # test some of the fields\n            self.assertEqual(cf_info[\'variable\'], variable)\n            self.assertEqual(cf_info[\'grid_mapping_variable\'], \'Polar_Stereographic_Grid\')\n            self.assertEqual(cf_info[\'type_of_grid_mapping\'], \'polar_stereographic\')\n            self.assertEqual(cf_info[\'lon\'], lon)\n            self.assertEqual(cf_info[\'lat\'], lat)\n            self.assertEqual(cf_info[\'x\'][\'varname\'], \'xc\')\n            self.assertEqual(cf_info[\'x\'][\'first\'], -3845.0)\n            self.assertEqual(cf_info[\'y\'][\'varname\'], \'yc\')\n            self.assertEqual(cf_info[\'y\'][\'last\'], -5345.0)\n\n        # prepare xarray Dataset\n        cf_file = _prepare_cf_nh10km()\n\n        # load using a variable= that is a valid grid_mapping container\n        _, cf_info = load_cf_area(cf_file, \'Polar_Stereographic_Grid\', y=\'yc\', x=\'xc\')\n        validate_nh10km_cfinfo(cf_info, variable=\'Polar_Stereographic_Grid\', lat=None, lon=None)\n\n        # load using a variable= that has a :grid_mapping attribute\n        _, cf_info = load_cf_area(cf_file, \'ice_conc\')\n        validate_nh10km_cfinfo(cf_info)\n\n        # load without using a variable=\n        _, cf_info = load_cf_area(cf_file)\n        validate_nh10km_cfinfo(cf_info)\n\n    def test_load_cf_llwgs84(self):\n        from pyresample.utils import load_cf_area\n\n        def validate_llwgs84(adef, cfinfo, lat=\'lat\', lon=\'lon\'):\n            self.assertEqual(adef.shape, (19, 37))\n            xc = adef.projection_x_coords\n            yc = adef.projection_y_coords\n            self.assertEqual(xc[0], -180., msg=""Wrong x axis (index 0)"")\n            self.assertEqual(xc[1], -180. + 10.0, msg=""Wrong x axis (index 1)"")\n            self.assertEqual(yc[0], -90., msg=""Wrong y axis (index 0)"")\n            self.assertEqual(yc[1], -90. + 10.0, msg=""Wrong y axis (index 1)"")\n            self.assertEqual(cfinfo[\'lon\'], lon)\n            self.assertEqual(cf_info[\'lat\'], lat)\n            self.assertEqual(cf_info[\'type_of_grid_mapping\'], \'latitude_longitude\')\n            self.assertEqual(cf_info[\'x\'][\'varname\'], \'lon\')\n            self.assertEqual(cf_info[\'x\'][\'first\'], -180.)\n            self.assertEqual(cf_info[\'y\'][\'varname\'], \'lat\')\n            self.assertEqual(cf_info[\'y\'][\'first\'], -90.)\n\n        # prepare xarray Dataset\n        cf_file = _prepare_cf_llwgs84()\n\n        # load using a variable= that is a valid grid_mapping container\n        adef, cf_info = load_cf_area(cf_file, \'crs\', y=\'lat\', x=\'lon\')\n        validate_llwgs84(adef, cf_info, lat=None, lon=None)\n\n        # load using a variable=temp\n        adef, cf_info = load_cf_area(cf_file, \'temp\')\n        validate_llwgs84(adef, cf_info)\n\n        # load using a variable=None\n        adef, cf_info = load_cf_area(cf_file)\n        validate_llwgs84(adef, cf_info)\n\n    def test_load_cf_llnocrs(self):\n        from pyresample.utils import load_cf_area\n\n        def validate_llnocrs(adef, cfinfo, lat=\'lat\', lon=\'lon\'):\n            self.assertEqual(adef.shape, (19, 37))\n            xc = adef.projection_x_coords\n            yc = adef.projection_y_coords\n            self.assertEqual(xc[0], -180., msg=""Wrong x axis (index 0)"")\n            self.assertEqual(xc[1], -180. + 10.0, msg=""Wrong x axis (index 1)"")\n            self.assertEqual(yc[0], -90., msg=""Wrong y axis (index 0)"")\n            self.assertEqual(yc[1], -90. + 10.0, msg=""Wrong y axis (index 1)"")\n            self.assertEqual(cfinfo[\'lon\'], lon)\n            self.assertEqual(cf_info[\'lat\'], lat)\n            self.assertEqual(cf_info[\'type_of_grid_mapping\'], \'latitude_longitude\')\n            self.assertEqual(cf_info[\'x\'][\'varname\'], \'lon\')\n            self.assertEqual(cf_info[\'x\'][\'first\'], -180.)\n            self.assertEqual(cf_info[\'y\'][\'varname\'], \'lat\')\n            self.assertEqual(cf_info[\'y\'][\'first\'], -90.)\n\n        # prepare xarray Dataset\n        cf_file = _prepare_cf_llnocrs()\n\n        # load using a variable=temp\n        adef, cf_info = load_cf_area(cf_file, \'temp\')\n        validate_llnocrs(adef, cf_info)\n\n        # load using a variable=None\n        adef, cf_info = load_cf_area(cf_file)\n        validate_llnocrs(adef, cf_info)\n\n\nclass TestLoadCFArea_Private(unittest.TestCase):\n    """"""Test private routines involved in loading an AreaDefinition from netCDF/CF files.""""""\n\n    def setUp(self):\n        """"""Prepare nc_handles.""""""\n        self.nc_handles = {}\n        self.nc_handles[\'nh10km\'] = _prepare_cf_nh10km()\n        self.nc_handles[\'llwgs84\'] = _prepare_cf_llwgs84()\n        self.nc_handles[\'llnocrs\'] = _prepare_cf_llnocrs()\n\n    def test_cf_guess_lonlat(self):\n        from pyresample.utils.cf import _guess_cf_lonlat_varname\n\n        # nominal\n        self.assertEqual(_guess_cf_lonlat_varname(self.nc_handles[\'nh10km\'], \'ice_conc\', \'lat\'), \'lat\',)\n        self.assertEqual(_guess_cf_lonlat_varname(self.nc_handles[\'nh10km\'], \'ice_conc\', \'lon\'), \'lon\',)\n        self.assertEqual(_guess_cf_lonlat_varname(self.nc_handles[\'llwgs84\'], \'temp\', \'lat\'), \'lat\')\n        self.assertEqual(_guess_cf_lonlat_varname(self.nc_handles[\'llwgs84\'], \'temp\', \'lon\'), \'lon\')\n        self.assertEqual(_guess_cf_lonlat_varname(self.nc_handles[\'llnocrs\'], \'temp\', \'lat\'), \'lat\')\n        self.assertEqual(_guess_cf_lonlat_varname(self.nc_handles[\'llnocrs\'], \'temp\', \'lon\'), \'lon\')\n\n        # error cases\n        self.assertRaises(ValueError, _guess_cf_lonlat_varname, self.nc_handles[\'nh10km\'], \'ice_conc\', \'wrong\',)\n        self.assertRaises(KeyError, _guess_cf_lonlat_varname, self.nc_handles[\'nh10km\'], \'doesNotExist\', \'lat\',)\n\n    def test_cf_guess_axis_varname(self):\n        from pyresample.utils.cf import _guess_cf_axis_varname\n\n        # nominal\n        self.assertEqual(_guess_cf_axis_varname(\n            self.nc_handles[\'nh10km\'], \'ice_conc\', \'x\', \'polar_stereographic\'), \'xc\')\n        self.assertEqual(_guess_cf_axis_varname(\n            self.nc_handles[\'nh10km\'], \'ice_conc\', \'y\', \'polar_stereographic\'), \'yc\')\n        self.assertEqual(_guess_cf_axis_varname(self.nc_handles[\'llwgs84\'], \'temp\', \'x\', \'latitude_longitude\'), \'lon\')\n        self.assertEqual(_guess_cf_axis_varname(self.nc_handles[\'llwgs84\'], \'temp\', \'y\', \'latitude_longitude\'), \'lat\')\n\n        # error cases\n        self.assertRaises(ValueError, _guess_cf_axis_varname,\n                          self.nc_handles[\'nh10km\'], \'ice_conc\', \'wrong\', \'polar_stereographic\')\n        self.assertRaises(KeyError, _guess_cf_axis_varname,\n                          self.nc_handles[\'nh10km\'], \'doesNotExist\', \'x\', \'polar_stereographic\')\n\n    def test_cf_is_valid_coordinate_standardname(self):\n        from pyresample.utils.cf import _is_valid_coordinate_standardname\n        from pyresample.utils.cf import _valid_cf_type_of_grid_mapping\n\n        # nominal\n        for proj_type in _valid_cf_type_of_grid_mapping:\n            if proj_type == \'geostationary\':\n                self.assertTrue(_is_valid_coordinate_standardname(\'projection_x_angular_coordinate\', \'x\', proj_type))\n                self.assertTrue(_is_valid_coordinate_standardname(\'projection_y_angular_coordinate\', \'y\', proj_type))\n                self.assertTrue(_is_valid_coordinate_standardname(\'projection_x_coordinate\', \'x\', proj_type))\n                self.assertTrue(_is_valid_coordinate_standardname(\'projection_y_coordinate\', \'y\', proj_type))\n            elif proj_type == \'latitude_longitude\':\n                self.assertTrue(_is_valid_coordinate_standardname(\'longitude\', \'x\', proj_type))\n                self.assertTrue(_is_valid_coordinate_standardname(\'latitude\', \'y\', proj_type))\n            elif proj_type == \'rotated_latitude_longitude\':\n                self.assertTrue(_is_valid_coordinate_standardname(\'grid_longitude\', \'x\', proj_type))\n                self.assertTrue(_is_valid_coordinate_standardname(\'grid_latitude\', \'y\', proj_type))\n            else:\n                self.assertTrue(_is_valid_coordinate_standardname(\'projection_x_coordinate\', \'x\', \'default\'))\n                self.assertTrue(_is_valid_coordinate_standardname(\'projection_y_coordinate\', \'y\', \'default\'))\n\n        # error cases\n        self.assertRaises(ValueError, _is_valid_coordinate_standardname, \'projection_x_coordinate\', \'x\', \'wrong\')\n        self.assertRaises(ValueError, _is_valid_coordinate_standardname, \'projection_y_coordinate\', \'y\', \'also_wrong\')\n\n    def test_cf_is_valid_coordinate_variable(self):\n        from pyresample.utils.cf import _is_valid_coordinate_variable\n\n        # nominal\n        self.assertTrue(_is_valid_coordinate_variable(self.nc_handles[\'nh10km\'], \'xc\', \'x\', \'polar_stereographic\'))\n        self.assertTrue(_is_valid_coordinate_variable(self.nc_handles[\'nh10km\'], \'yc\', \'y\', \'polar_stereographic\'))\n        self.assertTrue(_is_valid_coordinate_variable(self.nc_handles[\'llwgs84\'], \'lon\', \'x\', \'latitude_longitude\'))\n        self.assertTrue(_is_valid_coordinate_variable(self.nc_handles[\'llwgs84\'], \'lat\', \'y\', \'latitude_longitude\'))\n        self.assertTrue(_is_valid_coordinate_variable(self.nc_handles[\'llnocrs\'], \'lon\', \'x\', \'latitude_longitude\'))\n        self.assertTrue(_is_valid_coordinate_variable(self.nc_handles[\'llnocrs\'], \'lat\', \'y\', \'latitude_longitude\'))\n\n        # error cases\n        self.assertRaises(KeyError, _is_valid_coordinate_variable,\n                          self.nc_handles[\'nh10km\'], \'doesNotExist\', \'x\', \'polar_stereographic\')\n        self.assertRaises(ValueError, _is_valid_coordinate_variable,\n                          self.nc_handles[\'nh10km\'], \'xc\', \'wrong\', \'polar_stereographic\')\n        self.assertRaises(ValueError, _is_valid_coordinate_variable,\n                          self.nc_handles[\'nh10km\'], \'xc\', \'x\', \'wrong\')\n\n    def test_cf_load_crs_from_cf_gridmapping(self):\n        from pyresample.utils.cf import _load_crs_from_cf_gridmapping\n\n        def validate_crs_nh10km(crs):\n            crs_dict = crs.to_dict()\n            self.assertEqual(crs_dict[\'proj\'], \'stere\')\n            self.assertEqual(crs_dict[\'lat_0\'], 90.)\n\n        def validate_crs_llwgs84(crs):\n            crs_dict = crs.to_dict()\n            self.assertEqual(crs_dict[\'proj\'], \'longlat\')\n            self.assertEqual(crs_dict[\'ellps\'], \'WGS84\')\n\n        crs = _load_crs_from_cf_gridmapping(self.nc_handles[\'nh10km\'], \'Polar_Stereographic_Grid\')\n        validate_crs_nh10km(crs)\n        crs = _load_crs_from_cf_gridmapping(self.nc_handles[\'llwgs84\'], \'crs\')\n        validate_crs_llwgs84(crs)\n\n\ndef test_check_slice_orientation():\n    """"""Test that slicing fix is doing what it should.""""""\n    from pyresample.utils import check_slice_orientation\n\n    # Forward slicing should not be changed\n    start, stop, step = 0, 10, None\n    slice_in = slice(start, stop, step)\n    res = check_slice_orientation(slice_in)\n    assert res is slice_in\n\n    # Reverse slicing should not be changed if the step is negative\n    start, stop, step = 10, 0, -1\n    slice_in = slice(start, stop, step)\n    res = check_slice_orientation(slice_in)\n    assert res is slice_in\n\n    # Reverse slicing should be fixed if step is positive\n    start, stop, step = 10, 0, 2\n    slice_in = slice(start, stop, step)\n    res = check_slice_orientation(slice_in)\n    assert res == slice(start, stop, -step)\n\n    # Reverse slicing should be fixed if step is None\n    start, stop, step = 10, 0, None\n    slice_in = slice(start, stop, step)\n    res = check_slice_orientation(slice_in)\n    assert res == slice(start, stop, -1)\n'"
pyresample/test/utils.py,8,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright (c) 2016 David Hoese\n# Author(s):\n#   David Hoese <david.hoese@ssec.wisc.edu>\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Utilities for testing.\n\nThis mostly takes from astropy\'s method for checking warnings during tests.\n""""""\nimport sys\nimport types\nimport warnings\n\nimport numpy as np\n\ntry:\n    from pyproj import CRS\nexcept ImportError:\n    CRS = None\n\n_deprecations_as_exceptions = False\n_include_astropy_deprecations = False\nAstropyDeprecationWarning = None\nAstropyPendingDeprecationWarning = None\n\n\ndef treat_deprecations_as_exceptions():\n    """"""\n    Turn all DeprecationWarnings (which indicate deprecated uses of\n    Python itself or Numpy, but not within Astropy, where we use our\n    own deprecation warning class) into exceptions so that we find\n    out about them early.\n\n    This completely resets the warning filters and any ""already seen""\n    warning state.\n    """"""\n    # First, totally reset the warning state\n    for module in sys.modules.values():\n        # We don\'t want to deal with six.MovedModules, only ""real""\n        # modules.\n        if (isinstance(module, types.ModuleType) and\n                hasattr(module, \'__warningregistry__\')):\n            del module.__warningregistry__\n\n    if not _deprecations_as_exceptions:\n        return\n\n    warnings.resetwarnings()\n\n    # Hide the next couple of DeprecationWarnings\n    warnings.simplefilter(\'ignore\', DeprecationWarning)\n    # Here\'s the wrinkle: a couple of our third-party dependencies\n    # (py.test and scipy) are still using deprecated features\n    # themselves, and we\'d like to ignore those.  Fortunately, those\n    # show up only at import time, so if we import those things *now*,\n    # before we turn the warnings into exceptions, we\'re golden.\n    try:\n        # A deprecated stdlib module used by py.test\n        import compiler  # noqa\n    except ImportError:\n        pass\n\n    try:\n        import scipy  # noqa\n    except ImportError:\n        pass\n\n    # Now, start over again with the warning filters\n    warnings.resetwarnings()\n    # Now, turn DeprecationWarnings into exceptions\n    warnings.filterwarnings(""error"", "".*"", DeprecationWarning)\n\n    # Only turn astropy deprecation warnings into exceptions if requested\n    if _include_astropy_deprecations:\n        warnings.filterwarnings(""error"", "".*"", AstropyDeprecationWarning)\n        warnings.filterwarnings(""error"", "".*"", AstropyPendingDeprecationWarning)\n\n    # py.test reads files with the \'U\' flag, which is now\n    # deprecated in Python 3.4.\n    warnings.filterwarnings(\n        ""ignore"",\n        r""\'U\' mode is deprecated"",\n        DeprecationWarning)\n\n    # BeautifulSoup4 triggers a DeprecationWarning in stdlib\'s\n    # html module.x\n    warnings.filterwarnings(\n        ""ignore"",\n        r""The strict argument and mode are deprecated\\."",\n        DeprecationWarning)\n    warnings.filterwarnings(\n        ""ignore"",\n        r""The value of convert_charrefs will become True in 3\\.5\\. ""\n        r""You are encouraged to set the value explicitly\\."",\n        DeprecationWarning)\n    # Filter out pyresample\'s deprecation warnings.\n    warnings.filterwarnings(\n        ""ignore"",\n        r""This module will be removed in pyresample 2\\.0\\, please use the""\n        r""\\`pyresample.spherical\\` module functions and class instead\\."",\n        DeprecationWarning)\n\n    if sys.version_info[:2] >= (3, 5):\n        # py.test raises this warning on Python 3.5.\n        # This can be removed when fixed in py.test.\n        # See https://github.com/pytest-dev/pytest/pull/1009\n        warnings.filterwarnings(\n            ""ignore"",\n            r""inspect\\.getargspec\\(\\) is deprecated, use ""\n            r""inspect\\.signature\\(\\) instead"",\n            DeprecationWarning)\n\n\nclass catch_warnings(warnings.catch_warnings):\n    """"""\n    A high-powered version of warnings.catch_warnings to use for testing\n    and to make sure that there is no dependence on the order in which\n    the tests are run.\n\n    This completely blitzes any memory of any warnings that have\n    appeared before so that all warnings will be caught and displayed.\n\n    ``*args`` is a set of warning classes to collect.  If no arguments are\n    provided, all warnings are collected.\n\n    Use as follows::\n\n        with catch_warnings(MyCustomWarning) as w:\n            do.something.bad()\n        assert len(w) > 0\n    """"""\n    def __init__(self, *classes):\n        super(catch_warnings, self).__init__(record=True)\n        self.classes = classes\n\n    def __enter__(self):\n        warning_list = super(catch_warnings, self).__enter__()\n        treat_deprecations_as_exceptions()\n        if len(self.classes) == 0:\n            warnings.simplefilter(\'always\')\n        else:\n            warnings.simplefilter(\'ignore\')\n            for cls in self.classes:\n                warnings.simplefilter(\'always\', cls)\n        return warning_list\n\n    def __exit__(self, type, value, traceback):\n        treat_deprecations_as_exceptions()\n\n\ndef create_test_longitude(start, stop, shape, twist_factor=0.0, dtype=np.float32):\n    if start > stop:\n        stop += 360.0\n\n    lon_row = np.linspace(start, stop, num=shape[1]).astype(dtype)\n    twist_array = np.arange(shape[0]).reshape((shape[0], 1)) * twist_factor\n    lon_array = np.repeat([lon_row], shape[0], axis=0)\n    lon_array += twist_array\n\n    if stop > 360.0:\n        lon_array[lon_array > 360.0] -= 360\n    return lon_array\n\n\ndef create_test_latitude(start, stop, shape, twist_factor=0.0, dtype=np.float32):\n    lat_col = np.linspace(start, stop, num=shape[0]).astype(dtype).reshape((shape[0], 1))\n    twist_array = np.arange(shape[1]) * twist_factor\n    lat_array = np.repeat(lat_col, shape[1], axis=1)\n    lat_array += twist_array\n    return lat_array\n\n\nclass CustomScheduler(object):\n    """"""Scheduler raising an exception if data are computed too many times.""""""\n\n    def __init__(self, max_computes=1):\n        """"""Set starting and maximum compute counts.""""""\n        self.max_computes = max_computes\n        self.total_computes = 0\n\n    def __call__(self, dsk, keys, **kwargs):\n        """"""Compute dask task and keep track of number of times we do so.""""""\n        import dask\n        self.total_computes += 1\n        if self.total_computes > self.max_computes:\n            raise RuntimeError(""Too many dask computations were scheduled: ""\n                               ""{}"".format(self.total_computes))\n        return dask.get(dsk, keys, **kwargs)\n\n\ndef friendly_crs_equal(expected, actual, keys=None, use_obj=True, use_wkt=True):\n    """"""Test if two projection definitions are equal.\n\n    The main purpose of this function is to help manage differences\n    between pyproj versions. Depending on the version installed and used\n    pyresample may provide a different `proj_dict` or other similar\n    CRS definition.\n\n    Args:\n        expected (dict, str, pyproj.crs.CRS): Expected CRS definition as\n            a PROJ dictionary or string or CRS object.\n        actual (dict, str, pyproj.crs.CRS): Actual CRS definition\n        keys (list): Specific PROJ parameters to look for. Only takes effect\n            if `use_obj` is `False`.\n        use_obj (bool): Use pyproj\'s CRS object to test equivalence. Default\n            is True.\n        use_wkt (bool): Increase likely hood of making CRS objects equal by\n            converting WellKnownText before converting to the final CRS\n            object. Requires `use_obj`. Defaults to True.\n\n    """"""\n    if CRS is not None and use_obj:\n        if hasattr(expected, \'crs\'):\n            expected = expected.crs\n        if hasattr(actual, \'crs\'):\n            actual = actual.crs\n        expected_crs = CRS(expected)\n        actual_crs = CRS(actual)\n        if use_wkt:\n            expected_crs = CRS(expected_crs.to_wkt())\n            actual_crs = CRS(actual_crs.to_wkt())\n        return expected_crs == actual_crs\n    raise NotImplementedError(""""""TODO"""""")\n'"
pyresample/utils/__init__.py,7,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2019 Pyresample developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Miscellaneous utility functions for pyresample.""""""\ntry:\n    from collections.abc import Mapping\nexcept ImportError:\n    from collections import Mapping\nimport numpy as np\nimport pyproj\nimport warnings\n\nfrom .proj4 import (proj4_dict_to_str, proj4_str_to_dict, convert_proj_floats, proj4_radius_parameters)  # noqa\nfrom .rasterio import get_area_def_from_raster  # noqa\nfrom .cf import load_cf_area  # noqa\n\n\ndef get_area_def(*args, **kwargs):\n    from pyresample.area_config import get_area_def\n    warnings.warn(""\'get_area_def\' has moved, import it with \'from pyresample import get_area_def\'"")\n    return get_area_def(*args, **kwargs)\n\n\ndef create_area_def(*args, **kwargs):\n    from pyresample.area_config import create_area_def\n    warnings.warn(""\'create_area_def\' has moved, import it with \'from pyresample import create_area_def\'"")\n    return create_area_def(*args, **kwargs)\n\n\ndef load_area(*args, **kwargs):\n    from pyresample.area_config import load_area\n    warnings.warn(""\'load_area\' has moved, import it with \'from pyresample import load_area\'"")\n    return load_area(*args, **kwargs)\n\n\ndef convert_def_to_yaml(*args, **kwargs):\n    from pyresample.area_config import convert_def_to_yaml\n    warnings.warn(""\'convert_def_to_yaml\' has moved, import it with \'from pyresample import convert_def_to_yaml\'"")\n    return convert_def_to_yaml(*args, **kwargs)\n\n\ndef parse_area_file(*args, **kwargs):\n    from pyresample.area_config import parse_area_file\n    warnings.warn(""\'parse_area_file\' has moved, import it with \'from pyresample import parse_area_file\'"")\n    return parse_area_file(*args, **kwargs)\n\n\ndef generate_quick_linesample_arrays(source_area_def, target_area_def, nprocs=1):\n    """"""Generate linesample arrays for quick grid resampling\n\n    Parameters\n    -----------\n    source_area_def : object\n        Source area definition as geometry definition object\n    target_area_def : object\n        Target area definition as geometry definition object\n    nprocs : int, optional\n        Number of processor cores to be used\n\n    Returns\n    -------\n    (row_indices, col_indices) : tuple of numpy arrays\n    """"""\n    from pyresample.grid import get_linesample\n    lons, lats = target_area_def.get_lonlats(nprocs)\n\n    source_pixel_y, source_pixel_x = get_linesample(lons, lats,\n                                                    source_area_def,\n                                                    nprocs=nprocs)\n\n    source_pixel_x = _downcast_index_array(source_pixel_x,\n                                           source_area_def.shape[1])\n    source_pixel_y = _downcast_index_array(source_pixel_y,\n                                           source_area_def.shape[0])\n\n    return source_pixel_y, source_pixel_x\n\n\ndef generate_nearest_neighbour_linesample_arrays(source_area_def,\n                                                 target_area_def,\n                                                 radius_of_influence,\n                                                 nprocs=1):\n    """"""Generate linesample arrays for nearest neighbour grid resampling\n\n    Parameters\n    -----------\n    source_area_def : object\n        Source area definition as geometry definition object\n    target_area_def : object\n        Target area definition as geometry definition object\n    radius_of_influence : float\n        Cut off distance in meters\n    nprocs : int, optional\n        Number of processor cores to be used\n\n    Returns\n    -------\n    (row_indices, col_indices) : tuple of numpy arrays\n    """"""\n\n    from pyresample.kd_tree import get_neighbour_info\n    valid_input_index, valid_output_index, index_array, distance_array = \\\n        get_neighbour_info(source_area_def,\n                           target_area_def,\n                           radius_of_influence,\n                           neighbours=1,\n                           nprocs=nprocs)\n    # Enumerate rows and cols\n    rows = np.fromfunction(lambda i, j: i, source_area_def.shape,\n                           dtype=np.int32).ravel()\n    cols = np.fromfunction(lambda i, j: j, source_area_def.shape,\n                           dtype=np.int32).ravel()\n\n    # Reduce to match resampling data set\n    rows_valid = rows[valid_input_index]\n    cols_valid = cols[valid_input_index]\n\n    # Get result using array indexing\n    number_of_valid_points = valid_input_index.sum()\n    index_mask = (index_array == number_of_valid_points)\n    index_array[index_mask] = 0\n    row_sample = rows_valid[index_array]\n    col_sample = cols_valid[index_array]\n    row_sample[index_mask] = -1\n    col_sample[index_mask] = -1\n\n    # Reshape to correct shape\n    row_indices = row_sample.reshape(target_area_def.shape)\n    col_indices = col_sample.reshape(target_area_def.shape)\n\n    row_indices = _downcast_index_array(row_indices,\n                                        source_area_def.shape[0])\n    col_indices = _downcast_index_array(col_indices,\n                                        source_area_def.shape[1])\n\n    return row_indices, col_indices\n\n\ndef fwhm2sigma(fwhm):\n    """"""Calculate sigma for gauss function from FWHM (3 dB level)\n\n    Parameters\n    ----------\n    fwhm : float\n        FWHM of gauss function (3 dB level of beam footprint)\n\n    Returns\n    -------\n    sigma : float\n        sigma for use in resampling gauss function\n\n    """"""\n\n    return fwhm / (2 * np.sqrt(np.log(2)))\n\n\ndef _downcast_index_array(index_array, size):\n    """"""Try to downcast array to uint16\n    """"""\n\n    if size <= np.iinfo(np.uint16).max:\n        mask = (index_array < 0) | (index_array >= size)\n        index_array[mask] = size\n        index_array = index_array.astype(np.uint16)\n    return index_array\n\n\ndef wrap_longitudes(lons):\n    """"""Wrap longitudes to the [-180:+180[ validity range (preserves dtype)\n\n    Parameters\n    ----------\n    lons : numpy array\n        Longitudes in degrees\n\n    Returns\n    -------\n    lons : numpy array\n        Longitudes wrapped into [-180:+180[ validity range\n\n    """"""\n    return (lons + 180) % 360 - 180\n\n\ndef check_and_wrap(lons, lats):\n    """"""Wrap longitude to [-180:+180[ and check latitude for validity.\n\n    Args:\n        lons (ndarray): Longitude degrees\n        lats (ndarray): Latitude degrees\n\n    Returns:\n        lons, lats: Longitude degrees in the range [-180:180[ and the original\n                    latitude array\n\n    Raises:\n        ValueError: If latitude array is not between -90 and 90\n\n    """"""\n    # check the latitutes\n    if lats.min() < -90. or lats.max() > 90.:\n        raise ValueError(\n            \'Some latitudes are outside the [-90.:+90] validity range\')\n\n    # check the longitudes\n    if lons.min() < -180. or lons.max() >= 180.:\n        # wrap longitudes to [-180;+180[\n        lons = wrap_longitudes(lons)\n\n    return lons, lats\n\n\ndef recursive_dict_update(d, u):\n    """"""Recursive dictionary update using\n\n    Copied from:\n\n        http://stackoverflow.com/questions/3232943/update-value-of-a-nested-dictionary-of-varying-depth\n\n    """"""\n    for k, v in u.items():\n        if isinstance(v, Mapping):\n            r = recursive_dict_update(d.get(k, {}), v)\n            d[k] = r\n        else:\n            d[k] = u[k]\n    return d\n\n\ndef is_pyproj2():\n    """"""Determine whether the current pyproj version is >= 2.0""""""\n    return pyproj.__version__ >= \'2\'\n\n\ndef check_slice_orientation(sli):\n    """"""Check that the slice is slicing the right way.""""""\n    if sli.start > sli.stop:\n        if sli.step is None or sli.step > 0:\n            step = -(sli.step or 1)\n            sli = slice(sli.start, sli.stop, step)\n\n    return sli\n'"
pyresample/utils/cartopy.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright (C) 2018 PyTroll developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""Classes for geometry operations""""""\n\nfrom logging import getLogger\nimport numpy as np\nimport pyproj\nimport warnings\n\ntry:\n    from xarray import DataArray\nexcept ImportError:\n    DataArray = np.ndarray\n\nfrom pyresample.utils.proj4 import proj4_str_to_dict\nimport cartopy.crs as ccrs\nimport shapely.geometry as sgeom\n\ntry:\n    from cartopy.crs import from_proj\nexcept ImportError:\n    from_proj = None\n\nlogger = getLogger(__name__)\n\n_GLOBE_PARAMS = {\'datum\': \'datum\',\n                 \'ellps\': \'ellipse\',\n                 \'a\': \'semimajor_axis\',\n                 \'b\': \'semiminor_axis\',\n                 \'f\': \'flattening\',\n                 \'rf\': \'inverse_flattening\',\n                 \'towgs84\': \'towgs84\',\n                 \'nadgrids\': \'nadgrids\'}\n\n\ndef _globe_from_proj4(proj4_terms):\n    """"""Create a `Globe` object from PROJ.4 parameters.""""""\n    globe_terms = filter(lambda term: term[0] in _GLOBE_PARAMS,\n                         proj4_terms.items())\n    globe = ccrs.Globe(**{_GLOBE_PARAMS[name]: value for name, value in\n                          globe_terms})\n    return globe\n\n\n# copy of class in cartopy (before it was released)\nclass _PROJ4Projection(ccrs.Projection):\n\n    def __init__(self, proj4_terms, globe=None, bounds=None):\n        if \'EPSG\' in proj4_terms.upper():\n            warnings.warn(\'Converting EPSG projection to proj4 string, which is a potentially lossy transformation\')\n            proj4_terms = pyproj.Proj(proj4_terms).definition_string().strip()\n        terms = proj4_str_to_dict(proj4_terms)\n        globe = _globe_from_proj4(terms) if globe is None else globe\n\n        other_terms = []\n        for term in terms.items():\n            if term[0] not in _GLOBE_PARAMS:\n                other_terms.append(term)\n        super(_PROJ4Projection, self).__init__(other_terms, globe)\n\n        self.bounds = bounds\n\n    def __repr__(self):\n        return \'_PROJ4Projection({})\'.format(self.proj4_init)\n\n    @property\n    def boundary(self):\n        x0, x1, y0, y1 = self.bounds\n        return sgeom.LineString([(x0, y0), (x0, y1), (x1, y1), (x1, y0),\n                                 (x0, y0)])\n\n    @property\n    def x_limits(self):\n        x0, x1, y0, y1 = self.bounds\n        return (x0, x1)\n\n    @property\n    def y_limits(self):\n        x0, x1, y0, y1 = self.bounds\n        return (y0, y1)\n\n    @property\n    def threshold(self):\n        x0, x1, y0, y1 = self.bounds\n        return min(abs(x1 - x0), abs(y1 - y0)) / 100.\n\n\ndef _lesser_from_proj(proj4_terms, globe=None, bounds=None):\n    """"""Not-as-good version of cartopy\'s \'from_proj\' function.\n\n    The user doesn\'t have a newer version of Cartopy so there is\n    no `from_proj` function to use which does a fancier job of\n    creating CRS objects from PROJ.4 strings than this does.\n\n    """"""\n    return _PROJ4Projection(proj4_terms, globe=globe, bounds=bounds)\n\n\nif from_proj is None:\n    from_proj = _lesser_from_proj\n'"
pyresample/utils/cf.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2020 Pyresample developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""Load an AreaDefinition object from a netCDF/CF file.""""""\n\nimport pyproj\n\n# list of valid CF grid mappings:\n_valid_cf_type_of_grid_mapping = \\\n    (\'albers_conical_equal_area\',\n     \'azimuthal_equidistant\',\n     \'geostationary\',\n     \'lambert_azimuthal_equal_area\',\n     \'lambert_conformal_conic\',\n     \'lambert_cylindrical_equal_area\',\n     \'latitude_longitude\',\n     \'mercator\',\n     \'oblique_mercator\',\n     \'orthographic\',\n     \'polar_stereographic\',\n     \'rotated_latitude_longitude\',\n     \'sinusoidal\',\n     \'stereographic\',\n     \'transverse_mercator\',\n     \'vertical_perspective\')\n\n# dictionnary with the standard_names accepted by CF per projection type\n#   this can be used for reading from and writing to CF files\n_valid_cf_coordinate_standardnames = {}\n# specific name for most grid mappings\n_valid_cf_coordinate_standardnames[\'default\'] = dict()\n_valid_cf_coordinate_standardnames[\'default\'][\'x\'] = (\'projection_x_coordinate\',)\n_valid_cf_coordinate_standardnames[\'default\'][\'y\'] = (\'projection_y_coordinate\',)\n# specific name for the latitude_longitude grid mapping\n_valid_cf_coordinate_standardnames[\'latitude_longitude\'] = dict()\n_valid_cf_coordinate_standardnames[\'latitude_longitude\'][\'x\'] = (\'longitude\',)\n_valid_cf_coordinate_standardnames[\'latitude_longitude\'][\'y\'] = (\'latitude\',)\n# specific name for the rotated_latitude_longitude grid mapping\n_valid_cf_coordinate_standardnames[\'rotated_latitude_longitude\'] = dict()\n_valid_cf_coordinate_standardnames[\'rotated_latitude_longitude\'][\'x\'] = (\'grid_longitude\',)\n_valid_cf_coordinate_standardnames[\'rotated_latitude_longitude\'][\'y\'] = (\'grid_latitude\',)\n# specific name for the geostationary grid mapping (we support two flavors)\n_valid_cf_coordinate_standardnames[\'geostationary\'] = dict()\n_valid_cf_coordinate_standardnames[\'geostationary\'][\'x\'] = (\n    \'projection_x_angular_coordinate\', \'projection_x_coordinate\',)\n_valid_cf_coordinate_standardnames[\'geostationary\'][\'y\'] = (\n    \'projection_y_angular_coordinate\', \'projection_y_coordinate\',)\n\n\ndef _convert_XY_CF_to_Proj(crs, axis_info):\n    """"""Convert XY values from CF to PROJ convention. With CF =< 1.9 only affects geostrationary projection.""""""\n    crs_dict = crs.to_dict()\n    if crs_dict[\'proj\'] == \'geos\':\n        # for geostationary projection, the values stored as x/y in CF are not directly\n        #  the x/y along the projection axes, but are rather the scanning angles from\n        #  the satellite. We must multiply them by the height of the satellite.\n        satellite_height = crs_dict[\'h\']\n        for k in (\'first\', \'last\', \'spacing\'):\n            axis_info[k] *= satellite_height\n        # the unit is now the default (meters)\n        axis_info[\'units\'] = None\n\n    return axis_info\n\n\ndef _load_crs_from_cf_gridmapping(nc_handle, grid_mapping_varname):\n    """"""Initialize a CRS object from a CF grid_mapping variable.""""""\n    # check the variable exists\n    try:\n        v = nc_handle[grid_mapping_varname]\n    except KeyError:\n        raise KeyError(""Variable \'{}\' does not exist in netCDF file"".format(grid_mapping_varname))\n\n    # check this indeed is a supported grid mapping variable\n    try:\n        if v.grid_mapping_name not in _valid_cf_type_of_grid_mapping:\n            raise ValueError(""Not a valid CF grid_mapping variable ({})"".format(grid_mapping_varname))\n    except AttributeError:\n        # no :grid_mapping_name thus it cannot be a valid grid_mapping variable\n        raise ValueError(""Not a valid CF grid_mapping variable ({})"".format(grid_mapping_varname))\n\n    # use pyproj to load the CRS\n    return pyproj.CRS.from_cf(v.attrs)\n\n\ndef _is_valid_coordinate_standardname(coord_standard_name, axis, type_of_grid_mapping):\n    """"""Check that a CF coordinate variable has the expected CF standard_name with regard to the typw of grid mapping.""""""\n    valid = False\n\n    if axis not in (\'x\', \'y\'):\n        raise ValueError(""axis= parameter must be \'x\' or \'y\'"")\n\n    if type_of_grid_mapping != \'default\' and type_of_grid_mapping not in _valid_cf_type_of_grid_mapping:\n        raise ValueError(""grid_mapping_name {} is not a valid CF one"".format(type_of_grid_mapping))\n\n    # access the valid standard_names (also handle the \'default\')\n    try:\n        valid_coord_standard_names = _valid_cf_coordinate_standardnames[type_of_grid_mapping][axis]\n    except KeyError:\n        valid_coord_standard_names = _valid_cf_coordinate_standardnames[\'default\'][axis]\n\n    # test for validity\n    valid = coord_standard_name in valid_coord_standard_names\n\n    return valid\n\n\ndef _is_valid_coordinate_variable(nc_handle, coord_varname, axis, type_of_grid_mapping):\n    """"""Check if a variable is a valid CF coordinate variable.""""""\n    valid = False\n\n    if axis not in (\'x\', \'y\'):\n        raise ValueError(""axis= parameter must be \'x\' or \'y\'"")\n\n    try:\n        coord_var = nc_handle[coord_varname]\n    except KeyError:\n        raise KeyError(""Variable \'{}\' does not exist in netCDF file"".format(coord_varname))\n\n    try:\n        coord_standard_name = getattr(coord_var, \'standard_name\')\n        valid = _is_valid_coordinate_standardname(coord_standard_name, axis, type_of_grid_mapping)\n    except AttributeError:\n        # if the coordinate variable is missing a standard_name, it cannot be a valid CF coordinate axis\n        valid = False\n\n    return valid\n\n\ndef _load_cf_axis_info(nc_handle, coord_varname):\n    """"""Load and compute information for a coordinate axis (e.g. first & last values, spacing, length, etc...).""""""\n    # this requires reading the data, we only read first and last\n    first = (nc_handle[coord_varname][0]).item()\n    last = (nc_handle[coord_varname][-1]).item()\n    nb = len(nc_handle[coord_varname])\n\n    # spacing and sign of the axis\n    delta = float(last - first) / (nb - 1)\n    spacing = abs(delta)\n    sign = delta / spacing\n\n    # get the unit information\n    try:\n        unit = getattr(nc_handle[coord_varname], \'units\')\n    except AttributeError:\n        unit = None\n\n    # some units that are valid in CF are not valid to pass to proj\n    if unit.startswith(\'rad\') or \\\n       unit.startswith(\'deg\'):\n        unit = None\n\n    # return in a dictionnary structure\n    ret = {\'first\': first, \'last\': last, \'spacing\': spacing,\n           \'nb\': nb, \'sign\': sign, \'unit\': unit}\n\n    return ret\n\n\ndef _get_area_extent_from_cf_axis(x, y):\n    """"""Compute the area_extent of the AreaDefinition object from the information on the x and y axes.""""""\n    # find the ll: lower-left and ur: upper-right.\n    # x[\'first\'], y[\'first\'] is always the Upper Left corner\n    #   (think of numpy\'s convention for a 2D image with index 0,0 in top left).\n    ll_x, ll_y = x[\'first\'], y[\'last\']\n    ur_x, ur_y = x[\'last\'], y[\'first\']\n\n    # handle the half-pixel offset between the center of corner cell (what we have in the axis info)\n    #   and the corner of corner cell (what AreaDefinition expects)\n    ll_x -= x[\'sign\'] * 0.5 * x[\'spacing\']\n    ur_x += x[\'sign\'] * 0.5 * x[\'spacing\']\n    ll_y += y[\'sign\'] * 0.5 * y[\'spacing\']\n    ur_y -= y[\'sign\'] * 0.5 * y[\'spacing\']\n\n    # return as tuple\n    ret = (ll_x, ll_y, ur_x, ur_y)\n\n    return ret\n\n\ndef _guess_cf_axis_varname(nc_handle, variable, axis, type_of_grid_mapping):\n    """"""Guess the name of the netCDF variable holding the coordinate axis of a netCDF field.""""""\n    ret = None\n\n    if axis not in (\'x\', \'y\'):\n        raise ValueError(""axis= parameter must be \'x\' or \'y\'"")\n\n    # the name of y and x are in the dimensions of the variable=\n    try:\n        dims = nc_handle[variable].dims\n    except KeyError:\n        raise KeyError(""variable {} not found in file"".format(variable))\n\n    for dim in dims:\n        # test if each dim is a valid CF coordinate variable\n        if _is_valid_coordinate_variable(nc_handle, dim, axis, type_of_grid_mapping):\n            ret = dim\n            break\n\n    return ret\n\n\ndef _guess_cf_lonlat_varname(nc_handle, variable, lonlat):\n    """"""Guess the name of the netCDF variable holding the longitude (or latitude) of a netCDF field.""""""\n    ret = None\n\n    if lonlat not in (\'lon\', \'lat\'):\n        raise ValueError(""lonlat= parameter must be \'lon\' or \'lat\'"")\n\n    # lat/lon are either directly a dimension, or a :coordinates.\n\n    # By default (decode_cf=True) xarray puts all dims and :coordinates in .coords\n    #   and remove the :coordinates attribute\n    try:\n        search_list = list(nc_handle[variable].coords)\n    except KeyError:\n        raise KeyError(""variable {} not found in file"".format(variable))\n\n    # if decode_cf=False was used, the look at the :coordinates attribute\n    if \'coordinates\' in nc_handle[variable].attrs.keys():\n        search_list += (nc_handle[variable].attrs[\'coordinates\']).split()\n\n    # go through the list of variables and check if one of them is lat / lon\n    for v in search_list:\n        try:\n            # this allows for both \'latitude\' and \'rotated_latitude\'...\n            if {\'lat\': \'latitude\', \'lon\': \'longitude\'}[lonlat] in getattr(nc_handle[v], \'standard_name\'):\n                ret = v\n                break\n        except AttributeError:\n            # no \'standard_name\'. this is not what we are looking for.\n            pass\n\n    return ret\n\n\ndef _load_cf_area_one_variable_crs(nc_handle, variable):\n    """"""Load the CRS corresponding to variable.""""""\n    grid_mapping_variable = None\n    variable_is_itself_gridmapping = False\n    # test if the variable has a grid_mapping attribute\n    if hasattr(nc_handle[variable], \'grid_mapping\'):\n        # good. attempt to load the grid_mapping information into a pyproj object\n        crs = _load_crs_from_cf_gridmapping(nc_handle, nc_handle[variable].grid_mapping)\n        grid_mapping_variable = nc_handle[variable].grid_mapping\n    elif hasattr(nc_handle[variable], \'grid_mapping_name\') and \\\n            nc_handle[variable].grid_mapping_name in _valid_cf_type_of_grid_mapping:\n        # this looks like a valid grid_mapping variable\n        try:\n            # try to load it\n            crs = _load_crs_from_cf_gridmapping(nc_handle, variable)\n            grid_mapping_variable = variable\n            variable_is_itself_gridmapping = True\n        except pyproj.exceptions.CRSError as ex:\n            raise ValueError(""ERROR: pyproj didn\'t manage to load the CRS: {}"".format(ex))\n    else:\n        # fallback position: maybe the variable is on a basic lat/lon grid with no\n        #   grid_mapping. Note: there is no default CRS in CF, we choose WGS84\n        grid_mapping_variable = ""latlon_default""\n        crs = pyproj.CRS.from_string(\'+proj=latlon +datum=WGS84 +ellps=WGS84\')\n\n    # return\n    return crs, grid_mapping_variable, variable_is_itself_gridmapping\n\n\ndef _load_cf_area_one_variable_axis(nc_handle, variable, type_of_grid_mapping, y=None, x=None):\n    """"""Identidy and load axis x and y.""""""\n    # if y= or x= are None, guess the variable names for the axis\n    xy = dict()\n    if y is None and x is None:\n        for axis in (\'x\', \'y\'):\n            xy[axis] = _guess_cf_axis_varname(nc_handle, variable, axis, type_of_grid_mapping)\n            if xy[axis] is None:\n                raise ValueError(""Could not guess the name of the \'{}\' axis for {}"".format(\n                    axis, variable))\n    else:\n        # y= and x= are provided by the caller. Check they are valid CF coordinate variables\n        #   The order is always (y,x)\n        xy[\'y\'] = y\n        xy[\'x\'] = x\n        for axis in (\'x\', \'y\'):\n            _valid_axis = _is_valid_coordinate_variable(nc_handle, xy[axis], axis, type_of_grid_mapping)\n            if not _valid_axis:\n                ve = ""Variable x=\'{}\' is not a valid CF coordinate variable for the {} axis"".format(xy[axis], axis)\n                raise ValueError(ve)\n\n    # we now have the names for the x= and y= coordinate variables: load the info of each axis separately\n    axis_info = dict()\n    for axis in (\'x\', \'y\'):\n        axis_info[axis] = _load_cf_axis_info(nc_handle, xy[axis],)\n\n    return xy, axis_info\n\n\ndef _load_cf_area_one_variable_areadef(axis_info, crs, unit, grid_mapping_variable):\n    """"""Prepare the AreaDefinition object.""""""\n    from pyresample import geometry\n    # create shape\n    shape = (axis_info[\'y\'][\'nb\'], axis_info[\'x\'][\'nb\'])\n\n    # get area extent from the x and y info\n    extent = _get_area_extent_from_cf_axis(axis_info[\'x\'], axis_info[\'y\'])\n\n    # transform the crs objecto a proj_dict (might not be needed in future versions of pyresample)\n    proj_dict = crs.to_dict()\n\n    # finally prepare the AreaDefinition object\n    return geometry.AreaDefinition.from_extent(grid_mapping_variable, proj_dict, shape, extent, units=unit)\n\n\ndef _load_cf_area_one_variable(nc_handle, variable, y=None, x=None):\n    """"""Load the AreaDefinition corresponding to one netCDF variable/field.""""""\n    if variable not in nc_handle.variables.keys():\n        raise KeyError(""Variable \'{}\' does not exist in netCDF file"".format(variable))\n\n    # the routine always prepares a cf_info\n    cf_info = dict()\n    cf_info[\'variable\'] = variable\n\n    # Load a CRS object\n    # =================\n    crs, grid_mapping_variable, variable_is_itself_gridmapping = _load_cf_area_one_variable_crs(nc_handle, variable)\n\n    # the type of grid_mapping (its grid_mapping_name) impacts several aspects of the CF reader\n    if grid_mapping_variable == \'latlon_default\':\n        type_of_grid_mapping = \'latitude_longitude\'\n    else:\n        try:\n            type_of_grid_mapping = nc_handle[grid_mapping_variable].grid_mapping_name\n        except AttributeError:\n            raise ValueError(\n                (""Not a valid CF grid_mapping variable ({}):""\n                 ""it lacks a :grid_mapping_name attribute"").format(grid_mapping_variable))\n\n    cf_info[\'grid_mapping_variable\'] = grid_mapping_variable\n    cf_info[\'type_of_grid_mapping\'] = type_of_grid_mapping\n\n    # test if we can allow None for y and x\n    if variable_is_itself_gridmapping and (y is None or x is None):\n        raise ValueError(""When variable= points to the grid_mapping variable itself, y= and x= must be provided"")\n\n    # identify and load the x/y axis\n    # ==============================\n    xy, axis_info = _load_cf_area_one_variable_axis(nc_handle, variable, type_of_grid_mapping, y=y, x=x)\n\n    # there are few cases where the x/y values loaded from the CF files cannot be\n    #   used directly in pyresample. We need a conversion:\n    for axis in (\'x\', \'y\'):\n        axis_info[axis] = _convert_XY_CF_to_Proj(crs, axis_info[axis])\n\n    # transfer information on the axis to the cf_info dict()\n    for axis in (\'x\', \'y\'):\n        cf_info[axis] = dict()\n        cf_info[axis][\'varname\'] = xy[axis]\n        for k in axis_info[axis].keys():\n            cf_info[axis][k] = axis_info[axis][k]\n\n    # sanity check: we cannot have different units for x and y\n    unit = axis_info[\'x\'][\'unit\']\n    if axis_info[\'x\'][\'unit\'] != axis_info[\'y\'][\'unit\']:\n        raise ValueError(""Cannot have different units for \'x\' ({}) and \'y\' ({}) axis."".format(\n            axis_info[\'x\'][\'unit\'], axis_info[\'y\'][\'unit\']))\n\n    # prepare the AreaDefinition object\n    # =================================\n    area_def = _load_cf_area_one_variable_areadef(axis_info, crs, unit, grid_mapping_variable)\n\n    return area_def, cf_info\n\n\ndef _load_cf_area_several_variables(nc_handle):\n    """"""Load the AreaDefinition corresponding to several netCDF variables/fields.""""""\n    def _indices_unique_AreaDefs(adefs):\n        """"""Find the indices of unique AreaDefinitions in a list.""""""\n        uniqs = dict()\n        for i, adef in enumerate(adefs):\n            if adef not in uniqs:  # this uses AreaDefinition.__eq__()\n                uniqs[adef] = i\n\n        # return only the indices\n        return uniqs.values()\n\n    adefs = []\n    infos = []\n\n    # go through all the variables\n    for v in nc_handle.variables.keys():\n\n        # skip variables that are less than 2D: they cannot\n        #   possibly sustain an AreaDefinition\n        if nc_handle[v].ndim < 2:\n            continue\n\n        try:\n            # try and load an AreaDefinition from this variable\n            adef, info = _load_cf_area_one_variable(nc_handle, v)\n            # store\n            adefs.append(adef)\n            infos.append(info)\n            # break the loop, we have all we need\n            break\n        except ValueError:\n            # this is not a problem: variable v simply doesn\'t define an AreaDefinition\n            continue\n\n    # go through the loaded AreaDefinitions and find the unique ones.\n    indices = _indices_unique_AreaDefs(adefs)\n    uniq_adefs = [adefs[ui] for ui in indices]\n    uniq_infos = [infos[ui] for ui in indices]\n\n    return uniq_adefs, uniq_infos\n\n\ndef load_cf_area(nc_file, variable=None, y=None, x=None):\n    """"""Load an AreaDefinition object from a netCDF/CF file.\n\n    Parameters\n    ----------\n    nc_file : string or object\n        path to a netCDF/CF file, or opened xarray.Dataset object\n    variable : string, optional\n        name of the variable to load the AreaDefinition from.\n        If the variable is not a CF grid_mapping container variable,\n        it should be a variable having a :grid_mapping attribute.\n        If variable is None the file will be searched for valid CF\n        area definitions\n    y : string, optional\n        name of the variable to use as \'y\' axis of the CF area definition\n        If y is None an appropriate \'y\' axis will be deduced from the CF file\n    x : string, optional\n        name of the variable to use as \'x\' axis of the CF area definition\n        If x is None an appropriate \'x\' axis will be deduced from the CF file\n\n    Returns\n    -------\n    are_def, cf_info : geometry.AreaDefinition object, dict\n       cf_info holds info about how the AreaDefinition was defined in the CF file.\n\n    """"""\n    import xarray as xr\n    # basic check on the default values of the parameters.\n    if (x is not None and y is None) or (x is None and y is not None):\n        raise ValueError(""You must specify either both or none of x= and y="")\n\n    # the nc_file can be either the path to a netCDF/CF file, or directly an opened xarray.Dataset()\n    if isinstance(nc_file, xr.Dataset):\n        nc_handle = nc_file\n    else:\n        #   if the path to a file, open the Dataset access to it\n        try:\n            nc_handle = xr.open_dataset(nc_file)\n        except FileNotFoundError as ex:\n            raise FileNotFoundError(""This file does not exist ({})"".format(ex))\n        except (OSError, TypeError) as ex:\n            raise OSError(""This file is probably not a valid netCDF file ({})."".format(ex))\n\n    if variable is None:\n        # if the variable=None, we search through all variables\n        area_def, cf_info = _load_cf_area_several_variables(nc_handle)\n        if len(area_def) == 0:\n            raise ValueError(""Found no AreaDefinitions in this netCDF/CF file."")\n        elif len(area_def) > 1:\n            # there were several area_definitions defined in this file. For now bark.\n            raise ValueError(""The CF file holds several different AreaDefinitions. Use the variable= keyword."")\n        else:\n            area_def = area_def[0]\n            cf_info = cf_info[0]\n    else:\n        # the variable= is known, call appropriate routine\n        try:\n            area_def, cf_info = _load_cf_area_one_variable(nc_handle, variable, y=y, x=x, )\n        except ValueError as ve:\n            raise ValueError(""Found no AreaDefinition associated with variable {} ({})"".format(variable, ve))\n\n    # also guess the name of the latitude and longitude variables\n    for ll in (\'lon\', \'lat\'):\n        cf_info[ll] = _guess_cf_lonlat_varname(nc_handle, cf_info[\'variable\'], ll)\n        # this can be None, in which case there was no good lat/lon candidate variable\n        #   in the file.\n\n    return area_def, cf_info\n'"
pyresample/utils/proj4.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2019 Pyresample developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom collections import OrderedDict\n\ntry:\n    from pyproj.crs import CRS\nexcept ImportError:\n    CRS = None\n\n\ndef convert_proj_floats(proj_pairs):\n    """"""Convert PROJ.4 parameters to floats if possible.""""""\n    proj_dict = OrderedDict()\n    for x in proj_pairs:\n        if len(x) == 1 or x[1] is True:\n            proj_dict[x[0]] = True\n            continue\n\n        try:\n            proj_dict[x[0]] = float(x[1])\n        except ValueError:\n            proj_dict[x[0]] = x[1]\n\n    return proj_dict\n\n\ndef proj4_str_to_dict(proj4_str):\n    """"""Convert PROJ.4 compatible string definition to dict\n\n    EPSG codes should be provided as ""EPSG:XXXX"" where ""XXXX""\n    is the EPSG number code. It can also be provided as\n    ``""+init=EPSG:XXXX""`` as long as the underlying PROJ library\n    supports it (deprecated in PROJ 6.0+).\n\n    Note: Key only parameters will be assigned a value of `True`.\n    """"""\n    # convert EPSG codes to equivalent PROJ4 string definition\n    if proj4_str.startswith(\'EPSG:\') and CRS is not None:\n        crs = CRS(proj4_str)\n        if hasattr(crs, \'to_dict\'):\n            # pyproj 2.2+\n            return crs.to_dict()\n        proj4_str = crs.to_proj4()\n    elif proj4_str.startswith(\'EPSG:\'):\n        # legacy +init= PROJ4 string and no pyproj 2.0+ to help convert\n        proj4_str = ""+init={}"".format(proj4_str)\n\n    pairs = (x.split(\'=\', 1) for x in proj4_str.replace(\'+\', \'\').split("" ""))\n    return convert_proj_floats(pairs)\n\n\ndef proj4_dict_to_str(proj4_dict, sort=False):\n    """"""Convert a dictionary of PROJ.4 parameters to a valid PROJ.4 string""""""\n    items = proj4_dict.items()\n    if sort:\n        items = sorted(items)\n    params = []\n    for key, val in items:\n        key = str(key) if key.startswith(\'+\') else \'+\' + str(key)\n        if key in [\'+no_defs\', \'+no_off\', \'+no_rot\']:\n            param = key\n        else:\n            param = \'{}={}\'.format(key, val)\n        params.append(param)\n    return \' \'.join(params)\n\n\ndef proj4_radius_parameters(proj4_dict):\n    """"""Calculate \'a\' and \'b\' radius parameters.\n\n    Arguments:\n        proj4_dict (str or dict): PROJ.4 parameters\n\n    Returns:\n        a (float), b (float): equatorial and polar radius\n    """"""\n    if CRS is not None:\n        import math\n        crs = CRS(proj4_dict)\n        a = crs.ellipsoid.semi_major_metre\n        b = crs.ellipsoid.semi_minor_metre\n        if not math.isnan(b):\n            return a, b\n        # older versions of pyproj didn\'t always have a valid minor radius\n        proj4_dict = crs.to_dict()\n\n    if isinstance(proj4_dict, str):\n        new_info = proj4_str_to_dict(proj4_dict)\n    else:\n        new_info = proj4_dict.copy()\n\n    # load information from PROJ.4 about the ellipsis if possible\n\n    from pyproj import Geod\n\n    if \'ellps\' in new_info:\n        geod = Geod(**new_info)\n        new_info[\'a\'] = geod.a\n        new_info[\'b\'] = geod.b\n    elif \'a\' not in new_info or \'b\' not in new_info:\n\n        if \'rf\' in new_info and \'f\' not in new_info:\n            new_info[\'f\'] = 1. / float(new_info[\'rf\'])\n\n        if \'a\' in new_info and \'f\' in new_info:\n            new_info[\'b\'] = float(new_info[\'a\']) * (1 - float(new_info[\'f\']))\n        elif \'b\' in new_info and \'f\' in new_info:\n            new_info[\'a\'] = float(new_info[\'b\']) / (1 - float(new_info[\'f\']))\n        elif \'R\' in new_info:\n            new_info[\'a\'] = new_info[\'R\']\n            new_info[\'b\'] = new_info[\'R\']\n        else:\n            geod = Geod(**{\'ellps\': \'WGS84\'})\n            new_info[\'a\'] = geod.a\n            new_info[\'b\'] = geod.b\n\n    return float(new_info[\'a\']), float(new_info[\'b\'])\n'"
pyresample/utils/rasterio.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2019 Pyresample developers\n#\n# This program is free software: you can redistribute it and/or modify it under\n# the terms of the GNU Lesser General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option) any\n# later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public License for more\n# details.\n#\n# You should have received a copy of the GNU Lesser General Public License along\n# with this program.  If not, see <http://www.gnu.org/licenses/>.\nfrom . import proj4_str_to_dict\n\n\ndef _get_area_def_from_gdal(dataset, area_id=None, name=None, proj_id=None, proj_dict=None):\n    from pyresample.geometry import AreaDefinition\n\n    # a: width of a pixel\n    # b: row rotation (typically zero)\n    # c: x-coordinate of the upper-left corner of the upper-left pixel\n    # d: column rotation (typically zero)\n    # e: height of a pixel (typically negative)\n    # f: y-coordinate of the of the upper-left corner of the upper-left pixel\n    c, a, b, f, d, e = dataset.GetGeoTransform()\n    if not (b == d == 0):\n        raise ValueError(\'Rotated rasters are not supported at this time.\')\n    area_extent = (c, f + e * dataset.RasterYSize, c + a * dataset.RasterXSize, f)\n\n    if proj_dict is None:\n        from osgeo import osr\n        proj = dataset.GetProjection()\n        if proj != \'\':\n            sref = osr.SpatialReference(wkt=proj)\n            proj_dict = proj4_str_to_dict(sref.ExportToProj4())\n        else:\n            raise ValueError(\'The source raster is not gereferenced, please provide the value of proj_dict\')\n\n        if proj_id is None:\n            proj_id = proj.split(\'""\')[1]\n\n    area_def = AreaDefinition(area_id, name, proj_id, proj_dict,\n                              dataset.RasterXSize, dataset.RasterYSize, area_extent)\n    return area_def\n\n\ndef _get_area_def_from_rasterio(dataset, area_id, name, proj_id=None, proj_dict=None):\n    from pyresample.geometry import AreaDefinition\n\n    a, b, c, d, e, f, _, _, _ = dataset.transform\n    if not (b == d == 0):\n        raise ValueError(\'Rotated rasters are not supported at this time.\')\n\n    if proj_dict is None:\n        crs = dataset.crs\n        if crs is not None:\n            proj_dict = dataset.crs.to_dict()\n        else:\n            raise ValueError(\'The source raster is not gereferenced, please provide the value of proj_dict\')\n\n        if proj_id is None:\n            proj_id = crs.wkt.split(\'""\')[1]\n\n    area_def = AreaDefinition(area_id, name, proj_id, proj_dict,\n                              dataset.width, dataset.height, dataset.bounds)\n    return area_def\n\n\ndef get_area_def_from_raster(source, area_id=None, name=None, proj_id=None, proj_dict=None):\n    """"""Construct AreaDefinition object from raster.\n\n    Parameters\n    ----------\n    source : str, Dataset, DatasetReader or DatasetWriter\n        A file name. Also it can be ``osgeo.gdal.Dataset``,\n        ``rasterio.io.DatasetReader`` or ``rasterio.io.DatasetWriter``\n    area_id : str, optional\n        ID of area\n    name : str, optional\n        Name of area\n    proj_id : str, optional\n        ID of projection\n    proj_dict : dict, optional\n        PROJ.4 parameters\n\n    Returns\n    -------\n    area_def : object\n        AreaDefinition object\n    """"""\n    try:\n        import rasterio\n    except ImportError:\n        rasterio = None\n        try:\n            from osgeo import gdal\n        except ImportError:\n            raise ImportError(\'Either rasterio or gdal must be available\')\n\n    cleanup_gdal = cleanup_rasterio = None\n    if isinstance(source, str):\n        if rasterio is not None:\n            source = rasterio.open(source)\n            cleanup_rasterio = True\n        else:\n            source = gdal.Open(source)\n            cleanup_gdal = True\n\n    try:\n        if rasterio is not None and isinstance(source, (rasterio.io.DatasetReader, rasterio.io.DatasetWriter)):\n            return _get_area_def_from_rasterio(source, area_id, name, proj_id, proj_dict)\n        return _get_area_def_from_gdal(source, area_id, name, proj_id, proj_dict)\n    finally:\n        if cleanup_rasterio:\n            source.close()\n        elif cleanup_gdal:\n            source = None\n'"
