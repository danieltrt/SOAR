file_path,api_count,code
alternate_reality.py,18,"b'# -*- coding: utf-8 -*-\n""""""Alternate_Reality\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1bknp8iZGkXEBz8LQHs2ipneEN4iFHDwb\n\n#**ALTERNATE REALITY**\n\n### **Description**\n\n---\n\nThis is a project that I am working on to apply my newfound skills of Recurrent Neural Networks and Natural Language Processing. This project deals with **text generation** using LSTM RNN\'s.\n\n\n### **Motivation**\n\n\n---\n\nI lived in New York City from 2014 to 2018. In the American education system, students write out different things (essays, poems, research papers, op-eds, etc.) on a daily basis.\n\nThroughout my time at *The Bronx High School of Science*, I was trained in the art of scientific thinking! I had to write daily homeworks for my english class, most of which I typed up in Google docs. I also wrote numerous research papers for my english and history classes, some of which were well above **20 pages**. All of these texts are my creation. My own piece in the world of words and sentences.\n\nWhen I had to leave *New York City* in the middle of my senior year, I was devastated beyond narration. It was the single most horrifying experience of my life. I was detached from everything I belonged to and stood for (kind of like Thor if you think about it) and banished away. \n\nEver since then, I have not been able to think like I used to be able to. I read text, but I am not critically thinking of it, not finding literary and rhetorical devices that the author used to set a scene, or show the development of a character.\n\nAnd so I turned to **Neural Networks** to help me out. By using **Python**, **Keras**, and **LSTM**, I will be able to create a *Recurrent Neural Network* for language modeling and sample new text written in my style.\n""""""\n\n""""""\nmounting the google drive to use text data and to clone GItHub repositories\n""""""\n\nfrom google.colab import drive\ndrive.mount(\'/content/drive/\')\n\n""""""\nimportant libraries imported\n""""""\n\nfrom __future__ import print_function\nimport os, io, sys, random\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import one_hot\nfrom tensorflow.keras.callbacks import LambdaCallback\nfrom tensorflow.keras.models import Model, Sequential, load_model\nfrom tensorflow.keras.layers import Dense, LSTM, Activation, Dropout, Input, Lambda, Reshape, Masking\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.utils import to_categorical, plot_model\n\n# Commented out IPython magic to ensure Python compatibility.\n""""""\nmaking sure tensorflow\'s version2 is used in this notebook\n""""""\n\n# %tensorflow_version 2.x\n\nimport os\n\nif int(tf.__version__[0]) < 2:\n  !pip install tensorflow==2.1\n\nprint(""Tensorflow version: "" + tf.__version__)\n\n""""""\ntesting if connected to TPU and/or GPU\n""""""\n\nimport pprint\n\nif \'COLAB_TPU_ADDR\' not in os.environ:\n  print(\'Not connected to a TPU runtime.\')\nelse:\n  tpu_address = \'grpc://\' + os.environ[\'COLAB_TPU_ADDR\']\n  print (\'Connected to TPU.\\n\\nTPU address is\', tpu_address)\n\n  with tf.compat.v1.Session((tpu_address)) as session:\n    devices = session.list_devices()\n    \n  print(\'TPU devices:\')\n  pprint.pprint(devices)\n\nif tf.test.gpu_device_name() == \'\':\n  print(\'\\n\\nNot connected to a GPU runtime.\')\nelse:\n  print(\'\\n\\nConnected to GPU: \' + tf.test.gpu_device_name())\n\n""""""\ncleaning the data and forming the examples\n""""""\n\nimport numpy as np\nimport io\n\n\npath = ""/content/drive/My Drive/Alternate-Reality/folder/text_data/merged.txt""\n\nwith io.open(path, encoding=\'utf-8\') as corpus:\n    text = corpus.read()\n\nLENGTH = len(text)\nTx = 11 # length of each example (characters)\n\nvocab = sorted(set(list(text))) # list (a set actually) of all the characters in the corpus\nchar_to_indices = dict((ch, idx) for idx, ch in enumerate(vocab))\nindex_to_char = dict((idx, ch) for idx, ch in enumerate(vocab))\n\n# pretty much temporary variables just for the sake of splitting the huge corpus\nsentences = [] # X\nmapped_chars = [] # Y\n\nstep = 3\n\nfor i in range(0, LENGTH - Tx, step):\n    temp_text = text[i: i+Tx]\n    sentences.append(temp_text[:-1])\n    mapped_chars.append(temp_text[-1])\n\nm = len(sentences)\n\nX = np.zeros((m, Tx - 1, len(vocab)))\nY = np.zeros((m, len(vocab)))\n\nfor i, example in enumerate(sentences):\n    X[i, :, :] = one_hot([char_to_indices[ch] for ch in example], depth=len(vocab))\n    Y[i, :] = one_hot(char_to_indices[mapped_chars[i]], depth=len(vocab))\n\n# a nuisance is fixed by turning X and Y into numpy arrays\nX = np.asarray(X)\nY = np.asarray(Y)\n\n#==============printing data dimesions=========================================\nprint(f""Length of corpus: {LENGTH}"")\nprint(f""X.shape = {X.shape}"")\nprint(f""Y.shape = {Y.shape}"")\nprint(f""Number of examples: {m}"")\n\n""""""\nread the function docstring below\n""""""\n\ndef get_example(index=None):\n    """"""\n    retrieves the example at index position in X is index is passed, otherwise random example is obtained\n    :param index: index of example desired to be retrieved\n    :return: string of text\n    """"""\n\n    if index is None:\n        index = np.random.randint(low=0, high=m)\n\n    curr_x = [index_to_char[idx] for idx in np.argmax(X[index, :, :], axis=1)]\n    curr_y = index_to_char[np.argmax(Y[index, :])]\n\n    x_y = (\'\'.join(curr_x), curr_y)\n\n    return x_y\n\n""""""\ntesting a single example and the time it took to retrieve it\n""""""\n\nimport time\n\nstart = time.process_time()\nexample = get_example()\nend = time.process_time()\n\nprint(f""Sample X: {example[0]}\\nCorresponding Y: {example[1]}"")\nprint(f""\\nTime taken for acquiring this example: {end - start} seconds"")\n\n""""""\nnetwork architecture creation\nmodel creation\nplot_model allows me to see what my neural network looks like\n""""""\n\ndef Ram_Says(Tx, vocab, output_length):\n  # network architecture LSTM -> Dropout -> Reshape -> LSTM -> Dropout -> Dense\n\n  # define the initial hidden state a0 and initial cell state c0\n  a0 = Input(shape=(output_length,), name=\'a0\')\n  c0 = Input(shape=(output_length,), name=\'c0\')\n  a = a0\n  c = c0\n\n  X = Input(shape=(Tx, len(vocab)), name=\'X\')\n  \n  a, _, c = LSTM(units=output_length, activation=\'tanh\', return_state=True, dtype=\'float32\', name=f\'lstm_1\')(X, [a, c])\n  a = Dropout(rate=0.2, name=f\'dropout_1\')(a)\n  a = Reshape((1, output_length), name=\'reshape_1\')(a) # needed after a dropout layer for another LSTM layer\n  a = LSTM(units=output_length, activation=\'tanh\', dtype=\'float32\', name=f\'lstm_2\')(a)\n  a = Dropout(rate=0.2, name=f\'dropout_2\')(a)\n  out = Dense(units=len(vocab), activation=\'softmax\', name=f\'dense\')(a)\n    \n  model = Model(inputs=[X, a0, c0], outputs=out, name=\'Ram\')\n\n  return model\n\n""""""\ncreating the model and the summary of it\n""""""\n#====================Creating important variables===============================\nn_a = 256 # number of hidden state dimensions for each LSTM cell\n\na0 = np.zeros((m, n_a))\nc0 = np.zeros((m, n_a))\n#===============================================================================\n\nmodel = Ram_Says(Tx=Tx - 1, vocab=vocab, output_length=n_a)\n\nplot_model(model, to_file=\'/content/drive/My Drive/Alternate-Reality/nn_graph.png\')\n\nmodel.summary()\n\n""""""\nconfiguring optimizations for the model\nfitting the model\n""""""\n\nlearning_rate = 0.01\nlearning_rate_decay = 0.001\n\noptimizer = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, decay=learning_rate_decay)\n\nmodel.compile(optimizer=optimizer, loss=\'categorical_crossentropy\', metrics=[\'accuracy\'])\n\nbatch_size = 8192\n\nmodel.fit([X, a0, c0], Y, batch_size=batch_size, epochs=100)\n\n""""""\nfilepath\n""""""\n\nfilepath = \'/content/drive/My Drive/Alternate-Reality/Ram_Says_Trained_Model.h5\'\n\n""""""\nthis code takes care of saving the new model only if its accuracy is better than\nthat of the last model\n""""""\n\nif os.path.exists(filepath):\n  prev_model = load_model(filepath)\n  prev_acc = prev_model.evaluate([X, a0, c0], Y, verbose=0)[1]\n  curr_acc = model.evaluate([X, a0, c0], Y, verbose=0)[1]\n  if curr_acc > prev_acc:\n    print(""There was a previous model saved."")\n    print(f""Previous accuracy: {round(prev_acc*100, 2)}%"")\n    print(f""Current accuracy: {round(curr_acc*100, 2)}%"")\n    model.save(filepath)\n    print(\'New model is saved.\')\n  else:\n    print(f""Previous accuracy: {round(prev_acc*100, 2)}%"")\n    print(f""Current accuracy: {round(curr_acc*100, 2)}%"")\n    print(\'Old model is kept.\')\nelse: # if this is the first time saving the model\n  model.save(filepath)\n  print(\'First time model is saved.\')\n\n""""""\nloading the model for sampling\n""""""\n\nRam_says = load_model(filepath)\n\n""""""\ntesting the accuracy of the model on X and Y\n""""""\n\naccuracy = Ram_says.evaluate([X, a0, c0], Y, verbose=0)[1]\nprint(f""Accuracy on the training set: {round(accuracy*100, 2)}%"")\n\n""""""\ntext sampling time\n""""""\n\ndef sample(preds, temperature=1.0):\n    preds = np.asarray(preds).astype(\'float64\')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n\ndef generate_output():\n      diversity = random.choice([0.2, 0.5, 0.7, 1.0, 1.2, 1.4])\n      diversity = 0.2\n      print(f\'Diversity: {diversity}\')\n      generated = \'\'\n      sentence = input(\'Your text: \')\n      generated += sentence\n      if len(sentence) > 10:\n        sentence = sentence[:10]\n      elif len(sentence) < 10:\n        rem = 10 - len(sentence)\n        sentence += \' \' * rem\n      sys.stdout.write(generated + \' \')\n      a0 = np.zeros((1, n_a))\n      c0 = np.zeros((1, n_a))\n      sys.stdout.write(sentence)\n      for i in range(1000):\n          x_pred = np.zeros((1, Tx-1, len(vocab)))\n          for t, char in enumerate(sentence):\n              if char != \'0\':\n                  x_pred[0, t, char_to_indices[char]] = 1.\n          preds = Ram_says.predict([x_pred, a0, c0], verbose=0)[0]\n          next_index = sample(preds, temperature = 1.0)\n          next_char = index_to_char[next_index]\n\n          generated += next_char\n          sentence = sentence[1:] + next_char\n\n          sys.stdout.write(next_char)\n          sys.stdout.flush()\n\n          if next_char == \'\\n\':\n              print(\'\\n\')\n          elif next_char == \'\\t\':\n              print(\'\\t\')\n\ngenerate_output()\n\n""""""# I will be placing the model\'s generated text below for record.\n\n> ""My name is Ramansh Sharma My name is the public control considered to fut tell overt of a decide the world to life people real and massive really came to congres to be streated it could have ingarding the world many that estabe by nugues and could have been guvern for the Soligition, purched that remamplogr prices repective independence in 1982. Bech of 1980ided to social and enjoyable capting the sent a see that her seperones in Am"" - Ram_says\n\nPretty good for a first try for a character level model!\n\n> My name is Ram My name is for the fight not started with a being at left the sade but the colonists clear of the Dept-A spondin surmerity, Konusansingtond that in the US and CIA port and scange and industed the Cold War and Congress in Depertmant in reace in the different id on the eneming American relations going to see servions. He also failed in his becuuse to the ready the Ford she end and commercesvestromes to onder the shooked a protects the treaty proved my team America in aid of the new that her strect of phy incrusing and of the pagome, he hading the didrition in 1982. After their visto want in the Cold War which movern of the fen his \xe2\x80\x9cIn and wenter for their marking of confisens than his prorecive in front of a contriluting as a ""confid and the Office of Economic Opportunity. He also failed in his fail w\n\n> Thor is the king Thor is the rare to be some known as the enemonity of Congress, the meint in the Cold War when Chine with them. I was also high domination which dickins of Curness, he will be a beation\xe2\x80\xa6.ther would reduce the decision of appressed nor the increased communist against controlles. Perretimmert. Itay of nuclas they gave he was a tork the side and very impossing this him as a facutive promest it be a stay a and fack as the anities and the EPand and being people white Sinally sochiets. He did not convered congresity. \n\n\n> o show he was the niting to the 1987 in are income taxes. Truean aid to the child of cotting in country, this be mistaky was allice the new that were because Opeaa Act goush in the Cold War and Congress dissided on I came to that felt me to event, and the INation He was was a Soviet troops,\n\n> The The                                 Date-10foce deneed to ave the police of the sent to be some known as \xe2\x80\x9cTo loon duberd many as it is to New York, to his domestic oil for a long of the weap in Congress she had ease and ensore considerable spending neges and cered to confinced the not on that is a senter aftarity, and a communist confidend ming the enemonity of Congress, complelated to make me to ausheres decart did not prife to meacunion of the Indians, if the stopled. And order to be the same that I am good family defense does money starting ut from America. a recatted to the 1972 Nair ance me to eventle, to stay the cares and even Dieanation, even to geter from the British other trages in Robard on New Yer proved a coffitted them in 1975, up they wanted to raised her free that they will be jasunang but was is the mind end hand of the part endere to the money starting under President during the reader to the side because Opea which states who arm diresten to whith with whith security plannerd\n""""""'"
