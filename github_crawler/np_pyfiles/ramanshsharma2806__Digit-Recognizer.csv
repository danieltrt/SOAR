file_path,api_count,code
explore_data.py,2,"b'""""""\nPython file for exploring the data\n""""""\n\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n\ntraining_data = pd.read_csv(""../digit-recognizer/train.csv"", index_col=0) # loading the training training_data\n\n# top of the training_data\nprint(training_data.head())\n\n\nprint(\'42000 examples, each with 784 pixel values\')\n\nprint(f""shape of training data: {training_data.shape}"")\n\nplt.rcParams[\'figure.figsize\'] = (15.0, 15.0)  # set default size of plots\n\nfor i in range(0, 10):\n    # for plotting multiple plots in same cell\n    plt.subplot(3, 4, i + 1)\n\n    plt.title(f""Example of digit: {i}"")\n\n    # acquiring a random digit\'s random example pixel values\n    obj = np.transpose(training_data.loc[i].iloc[np.random.randn(1)])\n\n    # reshaping the training_data type to a square image\n    obj = np.asarray(obj).reshape(28, 28)\n\n    plt.imshow(obj)  # plotting using matplotlib\n\nplt.savefig(\'digits.png\')\n\n\n'"
test_stuff.py,20,"b'\n\n# this file is just for testing out stuff\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\n# training_data = pd.read_csv(""../digit-recognizer/train.csv"", index_col=0)\n# test_data = pd.read_csv(""../digit-recognizer/test.csv"")\n# sample_submission = pd.read_csv(""../digit-recognizer/sample_submission.csv"", index_col=\'ImageId\')\n\n\n# print(training_data.shape) # 42000x784\n#\n#\n# print(test_data.shape) # 28000, 784\n#\n# print(sample_submission.shape) # 28000, 2\n#\n# print(sample_submission.head())\n#\n#\n# print(\'\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\')\n#\n# print(str(0.6 * 70) + "" "" + str(0.2 * 70) + "" "" + str(0.2 * 70))\n#\n#\n# temp = np.zeros((10,), dtype=int)\n# print(temp)\n#\n\n# vars = []\n# L = 10\n#\n# for i in range(1, 10):\n#     print(i)\n#     vars.append(i)\n#\n# print(vars)\n\n\ngrads = [(1, 2), (4, 5), (5, 6)]\n\n\n# print(grads[0][1])\n\n\n# for i in range(20):\n#     d = np.random.randint(5)\n#     print(d)\n\n#\n# f = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 4, 5, 6, 6, 11])\n#\n# d = np.where(f == np.amax(f))\n# print(d[0])\n\n\n# a = np.arange(1, 11)\n# print(f""a: {a}"")\n# b = np.array(a, copy=True)\n#\n# print(f""b: {b}"")\n#\n# a = np.arange(5, 122)\n#\n# print(f""a now: {a}"")\n# print(f""b now: {b}"")\n\n\n\n\n\n# t = np.random.randint(1, 11, size=(3, 6))\n# print(f""t: {t}"")\n# print(f""n: {t.mean()}"")\n# print(f""axis 0: {t.mean(axis=0)}"")\n# print(f""axis 1: {t.mean(axis=1)}"")\n\n\n\n\n\n# pd.DataFrame.from_dict(data=mydict, orient=\'index\').to_csv(\'dict_file.csv\', header=False)\n# nowdict = pd.read_csv(\'dict_file.csv\', header=None, index_col=0, squeeze=True).to_dict()\n# for key in nowdict.keys():\n#     temp = nowdict[key]\n#     nowdict[key] = np.asarray(list(temp))\n#\n# print(nowdict[""W1""].shape)\n\n\n\n\n# a = np.arange(1, 11).reshape(2, 5)\n#\n# print(f""a: {a}"")\n#\n# print(f""a[0][1]: {a[:, 4]}"")\n#\n\n\n\n# a = np.array([1, 2, 3, 4, 5])\n# b = np.array([1, 2, 3, 5, 6])\n# acc = np.sum(a == b)\n# print(int(acc * 100 / 5))\n\n\n# d = np.zeros((10, 30))\n#\n# print(f""d: {d}"")\n#\n# print(f""d[:, 10]: {np.where(d[:, 10] == 0)}"")\n\n\nprac = np.zeros((1, 10))\n#\n# for i in range(10):\n#     print(f""prac[{i}] = {prac[:, i]}"")\n#\n#\n#\n\n\nimport PIL\n\n# from PIL import Image\n# # load the image\n# image = Image.open(\'../digit3.png\')\n# # summarize some details about the image\n# gs_image = image.convert(mode=\'L\')\n# # save in jpeg format\n# gs_image.save(\'opera_house_grayscale.jpg\')\n# # load the image again and show it\n# image2 = Image.open(\'opera_house_grayscale.jpg\')\n# # show the image\n# # image2.show()\n#\n# image3 = image2.resize((28, 28))\n# image3.show()\n# # report the size of the thumbnail\n# print(type(image3))\n#\n#\n# from PIL import Image, ImageFilter\n#\n#\n#\n# def imageprepare(argv):\n#     """"""\n#     This function returns the pixel values.\n#     The imput is a png file location.\n#     """"""\n#     im = Image.open(argv).convert(\'L\')\n#     width = float(im.size[0])\n#     height = float(im.size[1])\n#     newImage = Image.new(\'L\', (28, 28), (255))  # creates white canvas of 28x28 pixels\n#\n#     if width > height:  # check which dimension is bigger\n#         # Width is bigger. Width becomes 20 pixels.\n#         nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n#         if (nheight == 0):  # rare case but minimum is 1 pixel\n#             nheight = 1\n#             # resize and sharpen\n#         img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n#         wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n#         newImage.paste(img, (4, wtop))  # paste resized image on white canvas\n#     else:\n#         # Height is bigger. Heigth becomes 20 pixels.\n#         nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n#         if (nwidth == 0):  # rare case but minimum is 1 pixel\n#             nwidth = 1\n#             # resize and sharpen\n#         img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n#         wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n#         newImage.paste(img, (wleft, 4))  # paste resized image on white canvas\n#\n#     # newImage.save(""sample.png\n#\n#     tv = list(newImage.getdata())  # get pixel values\n#\n#     # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n#     tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n#     # print(tva)\n#     return tva\n#\n# image3=np.asarray(imageprepare(\'../digit3.png\')).reshape(784, 1)#file path here\n# # print(len(image3))# mnist IMAGES are 28x28=784 pixels\n\n\nimport pickle\n# gd_inc = open(""costs_gd.pickle"", ""rb"")\n# gd_cv = open(""cv_gd.pickle"", ""rb"")\n#\n# gd_costs = pickle.load(gd_inc)\n# gd_cv_costs = pickle.load(gd_cv)\n#\n# pickle_inc = open(""costs_place.pickle"", ""rb"")\n# pickle_cv = open(""cv_costs.pickle"", ""rb"")\n#\n# train_costs = pickle.load(pickle_inc)\n# cv_costs = pickle.load(pickle_cv)\n#\n# plt.plot(train_costs, label=""Adam train"")\n# plt.plot(cv_costs, label=""Adam validation"")\n# plt.plot(gd_costs)\n# plt.plot(gd_cv_costs)\n# plt.legend(loc=""upper right"")\n#\n\n\n# def make_batches(X, y, batch_size):\n#     """"""\n#     returns a list of batches of size passed in\n#     :param data: training data passed in\n#     :param batch_size: batch size\n#     :return: list of batches\n#     """"""\n#\n#     total = X.shape[1]\n#\n#     permutation = np.random.permutation(total)\n#\n#     shuffled_x = X[:, permutation]\n#     shuffled_y = y[:, permutation].reshape(1, total)\n#\n#     whole_batches = total // batch_size  # considering data\'s second dimension contains all examples\n#     batches = []\n#\n#     for i in range(whole_batches):\n#         curr_x = X[:, i * batch_size: (i + 1) * batch_size]\n#         curr_y = y[:, i * batch_size: (i + 1) * batch_size]\n#         batch = (curr_x, curr_y)\n#         batches.append(batch)\n#\n#     if total % 2 != 0:\n#         curr_x = X[:, whole_batches * batch_size:]\n#         curr_y = y[:, whole_batches * batch_size]\n#         batch = (curr_x, curr_y)\n#         batches.append(batch)\n#\n#     return batches\n#\n#\n# datas = np.arange(100).reshape(2, 50)\n# y = np.arange(50).reshape(1, 50)\n#\n# print(len(make_batches(datas, y, 10)[0]))\n#\n#\n#\n\n\n\n\n\n#\n#\n# import time\n#\n#\n#\n#\n# start = time.time()\n#\n# for i in range(3):\n#     time.sleep(1)\n#\n# end = time.time()\n#\n# print(end-start)\n#\n#\n#\n\n\n\n\nprint(""\\\\"")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
src/compute_cost.py,2,"b'""""""\nThis module will compute the cost of each iteration of forward prop\n""""""\n\n\nimport numpy as np\nfrom src.prep_data import m_train\n\n\ndef compute_cost(AL, y):\n    """"""\n    computing the cost of the loss function given an iteration\'s output value\n    :param y: label values of the images\n    :param AL: shape (42000, 10), each example\'s prediction being an array of 10 rounded values\n    :return: cost of the function L\n    """"""\n\n    cost = (-1 / m_train) * np.sum(y * np.log(AL + 1e-8)) # 1e-8 added to avoid taking log of 0\n    cost = np.squeeze(cost) # turns [[17]] into 17\n\n    assert (cost.shape == ())\n\n    return cost\n\n\n'"
src/equations.py,3,"b'""""""\nThis file will contain the different mathematical functions that the model will require\nThese functions are: softmax, relu, softmax_backward, relu_backward\n""""""\n\n\nimport numpy as np\n\n\ndef softmax(Z):\n    """"""\n    computing softmax of Z\n    :param Z: input\n    :return: softmax and cache (z)\n    """"""\n\n    e_Z = np.exp(Z - np.max(Z))\n\n    A = e_Z / e_Z.sum(axis=0)  # only difference\n\n    assert (A.shape == Z.shape)\n\n    cache = Z\n\n    return A, cache\n\n\ndef softmax_backward(AL, cache, y):\n    """"""\n    computes dZ for softmax\n    :param cache: Z value\n    :param AL: output layer A value\n    :param y: ground truth\n    :return: dZ\n    """"""\n\n    Z = cache\n\n    dZ = AL - y  # derivative of cost with respect to Z for softmax function\n\n    assert (dZ.shape == Z.shape)\n\n    return dZ\n\n\ndef relu(Z):\n    """"""\n    computing relu of Z\n    :param Z: input\n    :return: relu and cache (Z)\n    """"""\n\n    A = np.maximum(0, Z)\n\n    assert (A.shape == Z.shape)\n\n    cache = Z  # cache is used in backprop\n\n    return A, cache\n\n\ndef relu_backward(dA, cache):\n    """"""\n    computing dZ\n    :param dA: dA of current layer\n    :param cache: Z from relu\n    :return: dZ\n    """"""\n\n    Z = cache\n\n    dZ = np.array(dA, copy=True)  # gradient is 1 for z > 0\n\n    dZ[Z <= 0] = 0  # gradient is 0 for z <= 0 otherwise 1 for rest\n\n    assert (dZ.shape == Z.shape)\n\n    return dZ\n\n\n'"
src/initialize_parameters.py,2,"b'""""""\nThis file will initialize all the required parameters\nThe weights, W, and the biases, b, will be generated for the neural network\'s layers\n""""""\n\n\nimport numpy as np\n\n\nparameters = {}\n\ndef initialize_parameters(layers):\n    """"""\n    initializes the parameters for the layers passed\n    :param layers: a list of layer dimensions\n    :return: dictionary containing different parameters, W1, W2, b1, b2, etc.\n    """"""\n\n    L = len(layers)\n\n    for i in range(1, L):\n        parameters[\'W\' + str(i)] = np.random.randn(layers[i], layers[i - 1]) * 0.01\n        parameters[\'b\' + str(i)] = np.zeros((layers[i], 1))\n\n        assert (parameters[\'W\' + str(i)].shape == (layers[i], layers[i - 1]))\n        assert (parameters[\'b\' + str(i)].shape == (layers[i], 1))\n\n    return parameters\n\n\n'"
src/linear_relu_backward.py,3,"b'""""""\nThis file will implement the backward propagation of the model\nSeparate functions will be defined so that any form of input will work\n""""""\n\n\nimport numpy as np\nfrom src.equations import softmax_backward, relu_backward\nfrom src.prep_data import m_train, y\n\n\ndef backward(dZ, caches):\n    """"""\n    computing the gradients of cost with respect to W, b and A, aka dW, db, and dA[L-1]\n    :param dZ: dZ of current layer\n    :param caches: packed tuple (linear_cache, activated cache)\n    :return: dW, db, and dA_prev\n    """"""\n\n    linear_cache, _ = caches\n    A_prev, W, b = linear_cache\n\n    dW = (1 / m_train) * np.dot(dZ, np.transpose(A_prev))\n    db = (1 / m_train) * np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(np.transpose(W), dZ)\n\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    assert (dA_prev.shape == A_prev.shape)\n\n    return (dW, db), dA_prev\n\ndef linear_backward(input, caches, func, Y=y):\n    """"""\n    computing the entire backward prop gradients based on the activation function\n    :param Y: needs to be passed in to avoid mismatch in mini batch gradient descent\n    :param input: can be dA (for relu) or AL (for softmax)\n    :param caches: packed tuple (linear_cache, activated cache)\n    :param func: activation function (relu or softmax)\n    :return: dW, db, dA_prev\n    """"""\n\n    gradient, dA_prev = None, None\n    _, activated_cache = caches\n    Z = activated_cache\n\n    if func == \'softmax\':\n        dZ = softmax_backward(input, Z, Y) # implementation for softmax_backward handles the dZ calculation\n        gradient, dA_prev = backward(dZ, caches)\n\n    elif func == \'relu\':\n        dZ = relu_backward(input, Z) # implementation for relu_backward handles the dZ calculation\n        gradient, dA_prev = backward(dZ, caches)\n\n    return gradient, dA_prev\n\ndef L_model_backward(AL, caches, Y_param=y):\n    """"""\n    computing the gradients for all the layers\n    :param Y_param: needs to be passed in to avoid mismatch in min batch gradient descent\n    :param AL: output value of final layer after softmax activation\n    :param caches: accumulated caches of all the layers\n    :return: accumulated gradients of all the layers to be used by update_parameters\n    """"""\n\n    gradients = []\n    L = len(caches) - 1\n\n    gradient, dA_prev = linear_backward(AL, caches[L], \'softmax\', Y=Y_param) # for softmax, AL is needed in the backprop\n    gradients.append(gradient)\n\n    dA = dA_prev\n\n    for l in range(L - 1, -1, -1):\n        gradient, dA_prev = linear_backward(dA, caches[l], \'relu\')\n        gradients.append(gradient)\n        dA = dA_prev\n\n    gradients = gradients[::-1] # going from first layer to last layer\n\n    return gradients\n\n\n'"
src/linear_relu_forward.py,1,"b'""""""\nThis file will implement the forward propagation of the model\nSeparate functions will be defined so that any form of input will work\n""""""\n\n\nimport numpy as np\nfrom src.equations import softmax, relu\n\n\ndef forward(A_prev, W, b):\n    """"""\n    simply computing the value of z\n    :param A_prev: activations of previous layer\n    :param W: parameters for this layer\n    :param b: bias for this layer\n    :return: return value of z and cache of linear parameters, A_prev, W, b to be used in backpropagation\n    """"""\n\n    Z = np.dot(W, A_prev) + b\n\n    assert (Z.shape == (W.shape[0], A_prev.shape[1]))\n\n    # linear cache\n    cache = (A_prev, W, b) # will be used later in backpropagation\n\n    return Z, cache\n\ndef linear_forward(A_prev, W, b, func):\n    """"""\n    computing the activations based on the activation function\n    :param A_prev: activations of previous layer\n    :param W: parameters for this layer\n    :param b: bias for this layer\n    :param func: relu or softmax\n    :return: activated values A plus linear cache and activated cache (z values)\n    """"""\n\n    linear_cache, activated_cache, A = None, None, None\n\n    if func == ""softmax"":\n        Z, linear_cache = forward(A_prev, W, b)\n        A, activated_cache = softmax(Z)\n\n    elif func == ""relu"":\n        Z, linear_cache = forward(A_prev, W, b)\n        A, activated_cache = relu(Z)\n\n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n\n    cache = (linear_cache, activated_cache)\n\n    return A, cache\n\ndef L_model_forward(X, parameters):\n    """"""\n    Computing the entire forward propagation for the model\n    :param X: A_prev[0] passed in\n    :param parameters: containing all the initialized (or trained) parameters\n    :return: output value and the accumulated caches\n    """"""\n\n    A = X # X is inserted as A_prev into the loop\n    caches = []\n    L = len(parameters) // 2 # for every layer there are W and b, so half of parameters are total number of layers\n\n    for l in range(1, L):\n        A_prev = A\n        A, cache = linear_forward(A_prev, parameters[\'W\' + str(l)], parameters[\'b\' + str(l)], \'relu\')\n        caches.append(cache) # adding layer l\'s linear and activated caches to be used in backpropagation\n\n    AL, cache = linear_forward(A, parameters[\'W\' + str(L)], parameters[\'b\' + str(L)], \'softmax\')\n    caches.append(cache)\n\n    assert (AL.shape == (10, X.shape[1]))\n\n    return AL, caches\n\n\n'"
src/main.py,7,"b'""""""\nThis module is just to run the model in a clean environment with new data\n""""""\n\nimport pickle\nfrom src.model import VanillaNN, test_accuracy\nfrom src.prep_data import test_data, train_data, m_train, m_test, labels_train, labels_test\nfrom matplotlib import pyplot as plt\nimport numpy as np\nfrom src.new_image import image\nimport time\n\n\nparameters, train_costs = None, None\n\nlayers = [784, 30, 30, 10]\n\nepochs = 15\n\nbatch_size = 1024\n\nmodel = VanillaNN(layer_dims=layers, iterations=epochs, learning_rate=0.0025, mini_batch_size=batch_size,\n                  print_cost=True)\n\n\ndef vector_to_digit(initial_predictions, size=None):\n    """"""\n    converts vectors of length 10 to a single digit where the position is 1\n    :param size: number of examples in data set, either for training or test set\n    :param initial_predictions: matrix of predictions\n    :return: vector of predictions\n    """"""\n\n    # shape of parameter predictions is (10, 32000)\n\n    if size is None:\n        size = m_train  # default is for training set\n\n    pred_updated = np.zeros((1, size))\n\n    for i in range(size):\n        temp_pred = initial_predictions[:, i]\n        pred_updated[:, i] = np.where(temp_pred == np.amax(temp_pred))[0]\n\n    return pred_updated\n\n\ncommand_message = ""\\nList of commands are:"" \\\n                  ""\\ne:          terminate the program"" \\\n                  ""\\nnew :       test the new .png image you have inserted"" \\\n                  ""\\ntrain adam: train the model on adam optimization algorithm"" \\\n                  ""\\ntrain gd:   train the model on gradient descent algorithm"" \\\n                  ""\\nc:          display the cost function of training and validation sets"" \\\n                  ""\\nacc:        print the accuracies for training and test sets"" \\\n                  ""\\ntest:       test a new random image from the test set and classify it"" \\\n                  ""\\ncommands:   print this command list\\n""\n\nprint(command_message)\n\nif __name__ == ""__main__"":\n\n    user = input(""Enter command: "")\n\n    while user != \'e\':\n        if user == \'train adam\':\n            start = time.time()\n            parameters, train_costs, cv_costs = model.train(X=train_data, technique=\'adam\')\n            end = time.time()\n            print(f""\\nTime taken for {epochs} epochs: {end - start} seconds \\n"")\n            pickle_out = open(""dict.pickle"", ""wb"")\n            pickle_cost = open(""costs_place.pickle"", ""wb"")\n            pickle_cvcosts = open(""cv_costs.pickle"", ""wb"")\n            pickle.dump(cv_costs, pickle_cvcosts)\n            pickle.dump(train_costs, pickle_cost)\n            pickle.dump(parameters, pickle_out)\n            pickle_out.close()\n            pickle_cost.close()\n            pickle_cvcosts.close()\n\n        elif user == \'train gd\':\n            start = time.process_time()\n            parameters, train_costs, cv_costs = model.train(X=train_data, technique=\'gd\')\n            print(f""\\nTime taken for {epochs} epochs: {time.process_time() - start} seconds \\n"")\n            pickle_out = open(""dict.pickle"", ""wb"")\n            pickle_cost = open(""costs_place.pickle"", ""wb"")\n            pickle_cvcosts = open(""cv_costs.pickle"", ""wb"")\n            pickle.dump(cv_costs, pickle_cvcosts)\n            pickle.dump(train_costs, pickle_cost)\n            pickle.dump(parameters, pickle_out)\n            pickle_out.close()\n            pickle_cost.close()\n            pickle_cvcosts.close()\n\n        elif user == ""new"":\n            pickle_in = open(""dict.pickle"", ""rb"")\n            parameters = pickle.load(pickle_in)\n            X_test = image\n            output = model.test(parameters, X_test)\n            print(f""\\nOutput probabilities are: \\t\\n{output}\\n"")\n            digit = np.where(output == np.amax(output))[0][0]\n            fig = np.asarray(X_test).reshape(28, 28)\n            plt.title(f""The test example digit is: {digit}"")\n            plt.imshow(fig)\n            plt.show()\n            plt.close()\n\n        elif user == \'test\':\n            pickle_in = open(""dict.pickle"", ""rb"")\n            parameters = pickle.load(pickle_in)\n            X_test = test_data[:, np.random.randint(m_test)].reshape(784, 1)  # random image\n            output = model.test(parameters, X_test)\n            print(f""\\nOutput probabilities are: \\t\\n{output}\\n"")\n            digit = np.where(output == np.amax(output))[0][0]\n            fig = np.asarray(X_test).reshape(28, 28)\n            plt.title(f""The test example digit is: {digit}"")\n            plt.imshow(fig)\n            plt.show()\n            plt.close()\n\n        elif user == \'c\':\n            pickle_inc = open(""costs_place.pickle"", ""rb"")\n            pickle_cv = open(""cv_costs.pickle"", ""rb"")\n\n            train_costs = pickle.load(pickle_inc)\n            cv_costs = pickle.load(pickle_cv)\n\n            width_in_inches = 30\n            height_in_inches = 15\n            dots_per_inch = 50\n\n            plt.figure(\n                figsize=(width_in_inches, height_in_inches),\n                dpi=dots_per_inch)\n\n            plt.plot(train_costs, \'^:r\', label=""Adam train"", mew=7, linewidth=3)\n            plt.plot(cv_costs, \'^:b\', label=""Adam validation"", mew=7, linewidth=3)\n            plt.legend(loc=""upper right"", fontsize=25)\n\n            plt.title(""Cost (train and validation) as the model trains"", fontsize=35, color=\'black\')\n\n            plt.xlabel(\'Epoch\', fontsize=35, color=\'black\')\n            plt.ylabel(""Cost"", fontsize=35, color=\'black\')\n\n            plt.xticks(range(0, len(train_costs) + 1), fontsize=17, color=\'black\')\n            plt.yticks(fontsize=17, color=\'black\')\n\n            plt.show()\n            plt.close()\n\n        elif user == \'acc\':  # for calculating train_accuracy over the training set\n            pickle_in = open(""dict.pickle"", ""rb"")\n            parameters = pickle.load(pickle_in)\n\n            temp_train = model.test(parameters, train_data)  # shape (10, 32000)\n            train_predictions = vector_to_digit(temp_train, size=m_train)  # shape (1, 32000)\n\n            temp_test = model.test(parameters, test_data)  # shape (10, 10000)\n            test_predictions = vector_to_digit(temp_test, size=m_test)  # shape (1, 10000)\n\n            train_accuracy = test_accuracy(train_predictions, ground_truth=labels_train, size=m_train)\n            tests_accuracy = test_accuracy(test_predictions, ground_truth=labels_test, size=m_test)\n\n            print(f""\\nAccuracy on training set is: {train_accuracy}%"")\n            print(f""Accuracy on test set is: {tests_accuracy}%\\n"")\n\n        elif user == \'commands\':\n            print(command_message)\n\n        user = input(""Enter command: "")\n\n    print(""\\nSee you later user."")\n'"
src/model.py,2,"b'""""""\nThe main model file. Here all the different functions will be pieced together to form the multilayer Neural Network.\nIf the size of the NN is desired to be changed, it can be done in the layer_dims array.\n""""""\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom src.prep_data import train_data, test_data, cv_data, y, y_cv, labels_train, m_train\nfrom src.initialize_parameters import initialize_parameters\nfrom src.compute_cost import compute_cost\nfrom src.update_parameters import update_adam_parameters, initialize_adam, update_gd_parameters\nfrom src.linear_relu_forward import L_model_forward\nfrom src.linear_relu_backward import L_model_backward\n\n\nplt.rcParams[\'figure.figsize\'] = (10.0, 7.0)  # set default size of plots\n\nlayer_dimensions = [784, 30, 30, 10]  # 3 layer model, 3rd layer having 10 output units which will be rounded off and\n\n\ndef test_accuracy(predictions, ground_truth=None, size=None):\n    """"""\n    calculates the accuracy of the predictions\n    :param size: size can be passed in, either for training or testing sets\n    :param ground_truth: if required, user can pass the ground truth\n    :param predictions: digit predictions for each example\n    :return: accuracy as a percentage over 100%\n    """"""\n\n    if ground_truth is None:\n        ground_truth = labels_train  # default is over the training set\n\n    if size is None:\n        size = m_train  # default is for training set\n\n    accuracy = round(np.sum(predictions == ground_truth) * 100 / size, 2)\n\n    return accuracy\n\n\ndef make_batches(X_data, y_data, batch_size=512):\n    """"""\n    returns a list of batches of size passed in\n    :param X_data: x data passed in\n    :param y_data: y data passed in\n    :param batch_size: batch size\n    :return: list of batches\n    """"""\n\n    total = X_data.shape[1] # number of total examples\n\n    permutation = np.random.permutation(total) # needs to be same for both x and y\n\n    shuffled_x = X_data[:, permutation]\n    shuffled_y = y_data[:, permutation].reshape(10, total)\n\n    whole_batches = total // batch_size  # considering data\'s second dimension contains all examples\n    batches = []\n\n    for i in range(whole_batches):\n        curr_x = shuffled_x[:, i * batch_size: (i + 1) * batch_size]\n        curr_y = shuffled_y[:, i * batch_size: (i + 1) * batch_size]\n        batch = (curr_x, curr_y)\n        batches.append(batch)\n\n    if total % 2 != 0:\n        curr_x = shuffled_x[:, whole_batches * batch_size:]\n        curr_y = shuffled_y[:, whole_batches * batch_size:]\n        batch = (curr_x, curr_y)\n        batches.append(batch)\n\n    return batches\n\n\nclass VanillaNN:\n    """"""\n    The model object\n    This will have the train and test functions\n    """"""\n\n    def __init__(self, parameters=None, layer_dims=None, iterations=3000, learning_rate=0.075, mini_batch_size=512,\n                 print_cost=False):\n        """"""\n        initiating the model object\n        :param parameters: if passed by user (saw while testing)\n        :param mini_batch_size: mini batch size passed in by the user\n        :param layer_dims: network architecture\n        :param iterations: gradient descent runs for these many iterations\n        :param learning_rate: alpha in gradient descent\n        :param print_cost: if required, cost may be printed\n        """"""\n\n        if layer_dims is None:\n            self.layer_dims = layer_dimensions\n\n        else:\n            self.layer_dims = layer_dims  # user can be allowed to pass in the neural network architecture\n\n        self.parameters = parameters\n        self.iterations = iterations\n        self.learning_rate = learning_rate\n        self.mini_batch_size = mini_batch_size\n        self.print_cost = print_cost\n        self.costs = []\n        self.cv_costs = []\n        self.v, self.s = None, None\n\n    def train(self, X=train_data, Y=y, technique=""adam""):\n        """"""\n        this is the most important function. Here, all the helper functions will be called and model will be trained\n        :param technique: optimization algorithm (adam or gd)\n        :param Y: label values of the images\n        :param X: set of all the images (pixel arrays), in order to train this supervised model\n        :return: None\n        """"""\n\n        # first the parameters need to be initialized\n        self.parameters = initialize_parameters(self.layer_dims)\n        self.v, self.s = initialize_adam(self.parameters)\n\n        t = 0 # initializing for adam\n\n        # now for each cycle of iterations\n        for i in range(1, self.iterations + 1):\n\n            # make new batches\n            batches = make_batches(X, Y, batch_size=self.mini_batch_size) # for training data\n            curr_cost, curr_cv_cost = 0, 0\n\n            for batch in batches:\n\n                curr_X, curr_Y = batch\n\n                # forward propagation run\n                AL, caches = L_model_forward(curr_X, self.parameters)\n                cv_AL, _ = L_model_forward(cv_data, self.parameters)  # validation test\n\n                # cost is stored\n                cost = compute_cost(AL, curr_Y)\n                curr_cost += cost\n                cv_cost = compute_cost(cv_AL, y_cv)\n                curr_cv_cost += cv_cost\n\n                # back propagation will be run\n                gradients = L_model_backward(AL, caches, Y_param=curr_Y)  # gradients\n\n                # parameters will be updated\n                if technique == \'adam\':\n                    t += 1\n                    self.parameters, self.v, self.s = update_adam_parameters(self.parameters, gradients, self.v,\n                                                                             self.s, t, self.learning_rate)\n\n                elif technique == \'gd\':\n                    self.parameters = update_gd_parameters(self.parameters, gradients, self.learning_rate)\n\n            t = 0 # resetting the adam counter\n\n            curr_cost = curr_cost / m_train # average cost\n            curr_cv_cost = curr_cv_cost / len(y_cv) # average cost\n\n            self.costs.append(curr_cost)\n            self.cv_costs.append(curr_cv_cost)\n\n            # printing the cost every 100 iterations\n            if (i == 0 or i % 10 == 0) and self.print_cost:\n                print(f""Cost for iteration # {i}:  {curr_cost}"")\n\n        return self.parameters, self.costs, self.cv_costs\n\n    def test(self, parameters=None, X_test=test_data):\n        """"""\n        computing the predicted digit of an image pixel array\n        :param parameters: user inserted\n        :param X_test: image pixel array\n        :return: output probabilities of softmax\n        """"""\n\n        if parameters is None:\n            parameters = self.parameters\n\n        AL, _ = L_model_forward(X_test, parameters)\n\n        return AL\n'"
src/new_image.py,1,"b'""""""\nthis module converts an image into a (1, 784) numpy array\n""""""\n\nimport numpy as np\nfrom PIL import Image, ImageFilter\n\n\ndef image_prepare(argv):\n    """"""\n    This function returns the pixel values.\n    The input is a png file location.\n    :param argv: image\n    :return: list of pixel values\n    """"""\n\n    im = Image.open(argv).convert(\'L\')\n    width = float(im.size[0])\n    height = float(im.size[1])\n    newImage = Image.new(\'L\', (28, 28), 255)  # creates white canvas of 28x28 pixels\n\n    if width > height:  # check which dimension is bigger\n        # Width is bigger. Width becomes 20 pixels.\n        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n\n        if nheight == 0:  # rare case but minimum is 1 pixel\n            nheight = 1\n            # resize and sharpen\n\n        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n        newImage.paste(img, (4, wtop))  # paste resized image on white canvas\n    else:\n        # Height is bigger. Height becomes 20 pixels.\n        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n\n        if nwidth == 0:  # rare case but minimum is 1 pixel\n            nwidth = 1\n            # resize and sharpen\n\n        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n        wleft = int(round(((28 - nwidth) / 2), 0))  # calculate vertical position\n        newImage.paste(img, (wleft, 4))  # paste resized image on white canvas\n\n    # newImage.save(""sample.png\n\n    tv = list(newImage.getdata())  # get pixel values\n\n    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n    # print(tva)\n    return tva\n\n\nimage = np.asarray(image_prepare(\'../digit3.png\')).reshape(784, 1)  # file path here\n\n\n'"
src/prep_data.py,9,"b'""""""\nThis file will be used to prepare the data to be used by the model.\nData will be divided into train, cross validation, and test\n""""""\n\n\nimport numpy as np\nimport pandas as pd\n\n\ndata = pd.read_csv(\'../digit-recognizer/train.csv\')\n\nlabels = np.asarray(data[\'label\'])  # can be used as y, the true values\n\ndata.set_index(\'label\', inplace=True)  # permanently sets label column to be the index\n\ndata_original = np.asarray(data)\ndata_original = np.transpose(data_original)  # this fixes a small bug, changes shape to (784, 42000)\n\ntrain_data = data_original[:, : 30000]  # (784, 30000) training data\nlabels_train = labels[: 30000] # labels for training set\n\ncv_data = data_original[:, 30000:35000] # validation set will be used for plotting CV costs\nlabels_cv = labels[30000:35000] # labels for validation set\n\ntest_data = data_original[:, 35000:]  # shape (784, 7000)\nlabels_test = labels[35000:] # labels for test set\n\nm_train = len(labels_train)  # number of training examples in training data\nm_cv = len(labels_cv) # number of examples in validation set\nm_test = len(labels_test) # number of examples in test set\n\ny_cv = []\nfor i in range(m_cv):\n    temp = np.zeros((10,), dtype=int)\n    temp[labels_cv[i]] = 1\n    y_cv.append(temp)\n\ny_cv = np.asarray(y_cv)\ny_cv = np.transpose(y_cv) # ground truth for validation test shape (10, 5000)\n\ny = []\nfor i in range(m_train):\n    temp = np.zeros((10,), dtype=int)\n    temp[labels_train[i]] = 1\n    y.append(temp)\n\ny = np.asarray(y)  # shape (29400, 10), a 1 for each label digit\'s position in an empty (10,) zeros array\ny = np.transpose(y)  # fixes a bug and changes shape of y to (10, 30000)\n\n\n'"
src/update_parameters.py,8,"b'""""""\nThis module will update the parameters for gradient descent (adam or otherwise)\n""""""\n\n\nimport numpy as np\n\n\ndef initialize_adam(parameters):\n    """"""\n    initiating the Adam parameters\n    """"""\n\n    L = len(parameters) // 2\n    v = {}\n    s = {}\n\n    for l in range(L):\n        v[""dW"" + str(l + 1)] = np.zeros(parameters[""W"" + str(l + 1)].shape)\n        v[""db"" + str(l + 1)] = np.zeros(parameters[""b"" + str(l + 1)].shape)\n        s[""dW"" + str(l + 1)] = np.zeros(parameters[""W"" + str(l + 1)].shape)\n        s[""db"" + str(l + 1)] = np.zeros(parameters[""b"" + str(l + 1)].shape)\n\n    return v, s\n\n\ndef update_adam_parameters(parameters, gradients, v, s, t, alpha=0.01):\n    """"""\n    updating the parameters with adam optimization\n    :param parameters: passed in parameters\n    :param gradients: derivatives of parameters with respect to cost\n    :param v: momentum estimation velocity\n    :param s: RMSProp estimation\n    :param t: batch\n    :param alpha: learning rate\n    :return: parameters, v, and s\n    """"""\n\n    # predefined constants\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 1e-8\n\n    L = len(parameters) // 2\n    v_corrected = {}\n    s_corrected = {}\n\n    for l in range(L):\n        v[""dW"" + str(l + 1)] = beta1 * v[""dW"" + str(l + 1)] + (1 - beta1) * gradients[l][0]\n        v[""db"" + str(l + 1)] = beta1 * v[""db"" + str(l + 1)] + (1 - beta1) * gradients[l][1]\n\n        v_corrected[""dW"" + str(l + 1)] = v[""dW"" + str(l + 1)] / (1 - beta1 ** t)\n        v_corrected[""db"" + str(l + 1)] = v[""db"" + str(l + 1)] / (1 - beta1 ** t)\n\n        s[""dW"" + str(l + 1)] = beta2 * s[""dW"" + str(l + 1)] + (1 - beta2) * np.square(gradients[l][0])\n        s[""db"" + str(l + 1)] = beta2 * s[""db"" + str(l + 1)] + (1 - beta2) * np.square(gradients[l][1])\n\n        s_corrected[""dW"" + str(l + 1)] = s[""dW"" + str(l + 1)] / (1 - beta2 ** t)\n        s_corrected[""db"" + str(l + 1)] = s[""db"" + str(l + 1)] / (1 - beta2 ** t)\n\n        parameters[""W"" + str(l + 1)] = parameters[""W"" + str(l + 1)] - (\n                (alpha * v_corrected[""dW"" + str(l + 1)]) / np.sqrt(s_corrected[""dW"" + str(l + 1)] + epsilon))\n        parameters[""b"" + str(l + 1)] = parameters[""b"" + str(l + 1)] - (\n                (alpha * v_corrected[""db"" + str(l + 1)]) / np.sqrt(s_corrected[""db"" + str(l + 1)] + epsilon))\n\n    return parameters, v, s\n\n\ndef update_gd_parameters(parameters, gradients, alpha=0.01):\n    """"""\n    subtracting the gradient from the parameters for gradient descent\n    :param parameters: containing all the parameters for all the layers\n    :param gradients: containing all the gradients for all the layers\n    :param alpha: learning rate\n    :return: parameters\n    """"""\n\n    L = len(parameters) // 2\n\n    for l in range(1, L + 1):\n        parameters[\'W\' + str(l)] -= alpha * gradients[l - 1][0]\n        parameters[\'b\' + str(l)] -= alpha * gradients[l - 1][1]\n\n    return parameters\n\n\n'"
