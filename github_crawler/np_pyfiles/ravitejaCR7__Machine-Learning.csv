file_path,api_count,code
Clustering/KmeansPlus.py,9,"b""\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nTrain_data = np.array(pd.read_csv('leaf.data',header = None))\ncols = Train_data.shape[1] - 1\n\ndef splitData(data):\n    data = data[:,1:data.shape[1]]\n    return data\n\ndef preprocessData(data):\n    rows, cols = data.shape\n    for i in range(cols):\n        mean = np.mean(data[:, i])\n        std = np.std(data[:, i])\n        data[:, i] = (data[:, i]-mean)/std\n    return data\n        \n    \n    \ndef getClusterCenters(data, k):\n    rows, cols = data.shape\n    centers = np.zeros((k, cols))\n    r = np.random.choice(rows, 1) #Initial Choice\n    centers[0, :] = data[r, :]\n    k_center = 1\n    while k_center < k:\n        pd = [] #pd represents probability distribution\n        dist_sum = 0\n        for i in range(rows):\n            max_dist = 1000\n            for j in range(k_center):\n                dist = np.linalg.norm(data[i] - centers[j]) ** 2\n                if dist < max_dist:\n                    max_dist = dist\n            d = max_dist*max_dist\n            dist_sum += d\n            pd.append(d)\n        for i in range(rows):\n            pd[i] = pd[i]/dist_sum\n        r = np.random.choice(rows, 1, p=pd) \n        centers[k_center, :] = data[r, :]\n        k_center = k_center + 1\n    return centers\n\n    \ndef clusters_formation(labels, data, total_clusters):\n    clusters = []  #Initilaizing the Clusters \n    m = total_clusters\n    for i in range(m):   #Initilaizing each Cluster Center \n        clusters.append([])\n    n = len(data)\n    for i in range(n):\n        clusters[labels[i]].append(data[i])\n    return clusters\n    \n\ndef loss(clusters, centers):\n    n = len(centers)\n    loss = 0\n    \n    for i in range(n): #for each cluster\n        cluster_center = centers[i]\n        cluster = clusters[i]\n        for j in cluster:\n            k = np.linalg.norm(cluster_center-j)\n            loss += k**2\n    return loss    \n\n\n\ndata = splitData(Train_data) #Split Data sepearting the class labels\ndata = preprocessData(data) #Preprocessing the Data\n\nk = [12, 18, 24, 36, 42]\n\nfor i in k:\n    obj = []\n    for j in range(20):\n        cluster_centers = getClusterCenters(data, i)\n        clustering = KMeans(n_clusters=i , init = cluster_centers).fit(data)\n        clusters = clusters_formation(clustering.labels_, data, i)\n        l = loss(clusters, clustering.cluster_centers_)\n        obj.append(l)\n    print('The mean and variance of the kmeans objective for k = ',i,'is',np.mean(obj),'and ',np.var(obj))\n\n\n\n\n    \n    \n"""
Clustering/Spectral_Algo.py,11,"b'\nimport numpy as np\nimport math\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\ndef circs():\n    X = np.zeros((2, 100))\n    y = 0\n\n    i_s = np.arange(0, 2*np.pi, np.pi/25.0)\n\n    for i in i_s:\n        X[0, y] = np.cos(i)\n        X[1, y] = np.sin(i)\n        y += 1\n\n    for i in i_s:\n        X[0, y] = 2*np.cos(i)\n        X[1, y] = 2*np.sin(i)\n        y += 1\n    return X\n\ndef similarity_matrix(X, sigma):\n    n = len(X)\n    sim_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i+1):\n            if i == j:\n                sim_matrix[i, j] = 1   #Similarity between 2 same rows is 1\n            else:\n                dist = np.linalg.norm(X[i]-X[j])\n                k = (-1 / (2 * (sigma**2))) * (dist**2)\n                sim_matrix[i, j] = sim_matrix[j, i] = math.exp(k)\n    return sim_matrix\n                \n            \n     \ndef Laplacian_matrix(A):\n    n = len(A)\n    D = np.zeros((n, n))  #D is a diagonal matrix with D[i, i]= \xc2\xb4\xe2\x88\x91j A[i, j]\n    for i in range(n):\n        k = 0\n        for j in range(n):\n            k += A[i, j]\n        D[i, i] = k\n    return D - A\n            \n\ndef eigen(L):\n    return np.linalg.eig(L)\n\ndef spectralClustering(sim_matrix):\n    K = 2\n    L  =  Laplacian_matrix(sim_matrix)\n  \n\n    eigenValues, eigenVectors = eigen(L)\n    idx = eigenValues.argsort()[0:K]  \n    eigenValues = eigenValues[idx]\n    V = eigenVectors[:,idx]\n    spectral = KMeans(n_clusters=K).fit(V)\n    labels = spectral.labels_\n    clusters, cluster_centers = clusters_formation(labels, V, K)\n    return clusters, labels\n\n\ndef clusters_formation(labels, data, total_clusters):\n    clusters = []  #Initilaizing the Clusters \n    cluster_centers = [] #Initilaizing the Clusters centers\n    m = total_clusters\n    for i in range(m):   #Initilaizing each Cluster Center \n        clusters.append([])\n        cluster_centers.append(0)\n    n = len(data)\n    for i in range(n):\n        clusters[labels[i]].append(data[i])\n        cluster_centers[labels[i]] = cluster_centers[labels[i]]+data[i]\n    for i in range(m):\n        cluster_centers[i] = cluster_centers[i]/len(clusters[i])\n    return clusters, cluster_centers\n    \n\ndef loss(clusters, centers):\n    n = len(centers)\n    loss = 0\n    \n    for i in range(n): #for each cluster\n        cluster_center = centers[i]\n        cluster = clusters[i]\n        for j in cluster:\n            k = np.linalg.norm(cluster_center-j)\n            loss += k**2\n    return loss\n    \n    \n\n#Set the DataSet\nX = circs() #Concentric Circles dataset is taken as the sample dataset\nX =  X.transpose()\nsigma = 0.1   #Set the Sigma Value\nA = similarity_matrix(X, sigma)\n\n\n""""""\nSpectral Algorithm Starts\nGiven a Similarity Matrix it outputs the clusters by using Spectral Clustering\n""""""\n\n\nfinal_Clusters, labels = spectralClustering(A) #Spectral Clustering(Default Clusters is set to 2)\n\n\n#Scatter Plot\n""""""\nLABEL_COLOR_MAP = {0 : \'r\',\n                   1 : \'k\',\n                   }\nlabel_colors = [LABEL_COLOR_MAP[l] for l in labels]\nplt.scatter(X[:, 0], X[:, 1], c=label_colors)\n\n""""""\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
Clustering/Spectral_Circle.py,11,"b""\nimport numpy as np\nimport math\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\n\n\n\ndef circs():\n    \n    X = np.zeros((2, 100))\n    y = 0\n\n    i_s = np.arange(0, 2*np.pi, np.pi/25.0)\n\n    for i in i_s:\n        X[0, y] = np.cos(i)\n        X[1, y] = np.sin(i)\n        y += 1\n\n    for i in i_s:\n        X[0, y] = 2*np.cos(i)\n        X[1, y] = 2*np.sin(i)\n        y += 1\n    return X\n\n\ndef similarity_matrix(X, sigma):\n    n = len(X)\n    sim_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i+1):\n            if i == j:\n                sim_matrix[i, j] = 1   #Similarity between 2 same rows is 1\n            else:\n                dist = np.linalg.norm(X[i]-X[j])\n                k = (-1 / (2 * (sigma**2))) * (dist**2)\n                sim_matrix[i, j] = sim_matrix[j, i] = math.exp(k)\n    return sim_matrix\n                \n            \n     \ndef Laplacian_matrix(A):\n    n = len(A)\n    D = np.zeros((n, n))  #D is a diagonal matrix with D[i, i]= \xc2\xb4\xe2\x88\x91j A[i, j]\n    for i in range(n):\n        k = 0\n        for j in range(n):\n            k += A[i, j]\n        D[i, i] = k\n    return D - A\n            \n\ndef eigen(L):\n    return np.linalg.eig(L)\n\ndef spectralClustering(X, K, sigma):\n    A  = similarity_matrix(X, sigma)\n    L  =  Laplacian_matrix(A)\n  \n\n    eigenValues, eigenVectors = eigen(L)\n    idx = eigenValues.argsort()[0:K]  \n    eigenValues = eigenValues[idx]\n    V = eigenVectors[:,idx]\n    spectral = KMeans(n_clusters=K).fit(V)\n    labels = spectral.labels_\n    clusters, cluster_centers = clusters_formation(labels, V, 2)\n    return clusters, spectral.cluster_centers_, labels\n\n\ndef clusters_formation(labels, data, total_clusters):\n    clusters = []  #Initilaizing the Clusters \n    cluster_centers = [] #Initilaizing the Clusters centers\n    m = total_clusters\n    for i in range(m):   #Initilaizing each Cluster Center \n        clusters.append([])\n        cluster_centers.append(0)\n    n = len(data)\n    for i in range(n):\n        clusters[labels[i]].append(data[i])\n        cluster_centers[labels[i]] = cluster_centers[labels[i]]+data[i]\n    for i in range(m):\n        cluster_centers[i] = cluster_centers[i]/len(clusters[i])\n    return clusters, cluster_centers\n    \n\ndef loss(clusters, centers):\n    n = len(centers)\n    loss = 0\n    \n    for i in range(n): #for each cluster\n        cluster_center = centers[i]\n        cluster = clusters[i]\n        for j in cluster:\n            k = np.linalg.norm(cluster_center-j)\n            loss += k**2\n    return loss\n    \n    \n        \nK = 2\n\nX = circs()\n\nX =  X.transpose()\nsigma = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10**4] #Sigma Values\n#Kmeans Clustering\nkmeans = KMeans(n_clusters=K).fit(X)\n\nclusters_kmeans = clusters_formation(kmeans.labels_, X, K)[0]\n\nLoss_kmeans = loss(clusters_kmeans, kmeans.cluster_centers_)\n\nLABEL_COLOR_MAP = {0 : 'r',\n                   1 : 'k',\n                   }\nlabel_color = [LABEL_COLOR_MAP[l] for l in kmeans.labels_]\n\nplt.scatter(X[:, 0], X[:, 1], c=label_color)\n\n#Spectral Clustering\nbest_sigma = 0\nbest_loss = 100\nfor i in sigma:\n    clusters, cluster_centers, labels = spectralClustering(X, K, i)\n    LABEL_COLOR_MAP = {0 : 'r',\n                       1 : 'k',\n                       }\n    label_colors = [LABEL_COLOR_MAP[l] for l in labels]\n    plt.scatter(X[:, 0], X[:, 1], c=label_colors)\n    Loss_spectral =  loss(clusters, cluster_centers)\n    if(Loss_spectral < Loss_kmeans and Loss_spectral < best_loss):\n        best_sigma = i\n        best_loss = Loss_spectral\n        \n\nprint('Loss_kmeans', Loss_kmeans)\n    \n    \nprint('Loss_spectral', best_loss)\n\n\nprint('The value of sigma for which Spectral Clustering Outperforms Kmeans is', best_sigma)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
Clustering/partitionImg.py,7,"b'\n\nimport numpy as np\nimport math\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport skimage.io as sk\n\n\n\ndef similarity_matrix(X, sigma):\n    n = len(X)\n    sim_matrix = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i+1):\n            if i == j:\n                sim_matrix[i, j] = 1   #Similarity between 2 same rows is 1\n            else:\n                dist = X[i]-X[j]\n                k = (-1 / (2 * (sigma**2))) * (dist**2)\n                sim_matrix[i, j] = sim_matrix[j, i] = math.exp(k)\n    return sim_matrix\n                \n            \n     \ndef Laplacian_matrix(A):\n    n = len(A)\n    D = np.zeros((n, n))  #D is a diagonal matrix with D[i, i]= \xc2\xb4\xe2\x88\x91j A[i, j]\n    M = A.sum(axis=1)\n    print(M.shape)\n    for i in range(n):\n        D[i, i] = M[i]\n    return D - A\n            \n\ndef eigen(L):\n    return np.linalg.eig(L)\n\ndef spectralClustering(X, K, sigma):\n    A  = similarity_matrix(X, sigma)\n    print(A[0])\n    L  =  Laplacian_matrix(A)\n  \n    print(L[0])\n    eigenValues, eigenVectors = eigen(L)\n    idx = eigenValues.argsort()[0:K]  \n    eigenValues = eigenValues[idx]\n\n    V = eigenVectors[:,idx]\n\n    spectral = KMeans(n_clusters=K).fit(V)\n    labels = spectral.labels_\n    clusters, cluster_centers = clusters_formation(labels, X, K)\n    return clusters, cluster_centers, labels\n\ndef clusters_formation(labels, data, m):\n    clusters = []  #Initilaizing the Clusters \n    cluster_centers = [] #Initilaizing the Clusters centers\n    for i in range(m):   #Initilaizing each Cluster Center \n        clusters.append([])\n        cluster_centers.append(0)\n    n = len(data)\n    for i in range(n):\n        clusters[labels[i]].append(data[i])\n        cluster_centers[labels[i]] = cluster_centers[labels[i]]+data[i]\n    for i in range(m):\n        cluster_centers[i] = cluster_centers[i]/len(clusters[i])\n    return clusters, cluster_centers    \n\ndef loss(clusters, centers):\n    n = len(centers)\n    loss = 0\n    \n    for i in range(n): #for each cluster\n        cluster_center = centers[i]\n        cluster = clusters[i]\n        for j in cluster:\n            k = np.linalg.norm(cluster_center-j)\n            loss += k**2\n    return loss\n    \n    \n        \n\n\n    \n    \n        \nK = 2\n\ndata = plt.imread(""bw.jpg"")  #Converting a grayscale image into a list of pixel values\n\ndata = data.flatten()\nprint(len(data))\n\nsigma = 0.01\nclusters, cluster_centers, labels = spectralClustering(data, K, sigma)\nimage_labels = np.array(labels).astype(np.float)\nimage_labels = np.reshape(image_labels, (75, 100))\nLoss_Spectral = loss(clusters, cluster_centers)\n\nsk.imshow(image_labels)\nsk.imsave(""Spectral.jpg"", image_labels)\nprint(\'End\')\ndata = data.reshape(-1,1)\n\n\n\n\nkmeans = KMeans(n_clusters=2).fit(data)\nclusters_kmeans = clusters_formation(kmeans.labels_, data, K)[0]\n\n\nLoss_kmeans = loss(clusters_kmeans, kmeans.cluster_centers_)\nimage_labels = np.reshape(kmeans.labels_, (75, 100))\n\nsk.imshow(image_labels)\nprint(\'Loss_kmeans\', Loss_kmeans)\n#sk.imsave(""Kmeans.jpg"", image_labels)\nprint(\'Loss_Spectral\', Loss_Spectral)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
Decision Tree/DecisionMushTree.py,0,"b""\nimport pandas as pd\nimport math\nTraining_data = pd.read_csv('mush_train.data',header = None)\nTest_data = pd.read_csv('mush_test.data',header = None)\n\n\nattrLen = len(Training_data.columns)-1\n\n#print(Training_data.loc[Training_data[0] == 'p'] )\n    \n\ndef split(data, dupAttrList):\n    rows = len(data)    \n    maxProb = 1\n    bestAttr = -1\n    for i in dupAttrList:\n        prob = 0 #prob of the target given an attribute\n        for j in attrValues[i]:\n            temp = data.loc[data[i+1] == j, [0,i+1]]\n            tempLen = len(temp)\n            p = len(temp.loc[temp[0] == 'p'])\n            e = len(temp.loc[temp[0] == 'e'])\n            logp = 0\n            loge = 0\n            if(tempLen > 0):\n                if p > 0:\n                    logp = (math.log(p/tempLen))/(math.log(2))\n                if e > 0:\n                    loge = (math.log(e/tempLen))/(math.log(2))\n                prob = prob + -1 * tempLen/rows * ((p/tempLen)*logp+(e/tempLen)*loge)\n        if(prob <= maxProb):\n            maxProb = prob\n            bestAttr = i\n    return bestAttr\n            \n\n            \n            \n            \n            \n\ndef build_tree(Training_data, root):\n    dupAttrList = []\n    for i in range(attrLen):\n        dupAttrList.append(i)\n    max_height = 0;\n    attr = split(Training_data, dupAttrList)\n    root = attr\n    dupAttrList.remove(attr)\n    lst = [] \n    i = len(attrValues[attr]) - 1\n    while i >= 0:\n        lst.append([attrValues[attr][i],attr,0])\n        i = i-1\n    data[attr] = Training_data;\n    i = 0 \n    Node_Count = 0\n    while(len(lst) != 0):\n        entry = lst.pop()\n        Node_Count = Node_Count + 1; \n        attrToSplitOn = entry[1]\n        valueToSplitOn = entry[0]\n        DataToSplitOn = data[attrToSplitOn]\n        SplitData = DataToSplitOn.loc[DataToSplitOn[attrToSplitOn+1] == valueToSplitOn]\n        p = len(SplitData.loc[SplitData[0] == 'p'])\n        e = len(SplitData.loc[SplitData[0] == 'e'])\n        height = entry[2]+1\n        if height > max_height:\n            max_height = height\n        lenSplitData = len(SplitData)\n        if(lenSplitData == 0):\n            p = len(DataToSplitOn.loc[DataToSplitOn[0] == 'p'])\n            e = len(DataToSplitOn.loc[DataToSplitOn[0] == 'e'])\n            if p > e:\n                tree[attrToSplitOn][valueToSplitOn] = 'p'\n            else:\n                tree[attrToSplitOn][valueToSplitOn] = 'e'\n        else:\n            if p == lenSplitData:\n                tree[attrToSplitOn][valueToSplitOn] = 'p'\n            elif e == lenSplitData:\n                tree[attrToSplitOn][valueToSplitOn] = 'e'\n            else:\n                attr = split(SplitData, dupAttrList)\n                dupAttrList.remove(attr)\n                data[attr] = SplitData;\n                tree[attrToSplitOn][valueToSplitOn] = attr\n                i = len(attrValues[attr]) - 1\n                while i >= 0:\n                    lst.append([attrValues[attr][i],attr,height])\n                    i = i-1\n    for i in dupAttrList:\n        del tree[i]\n    return root, max_height+1, Node_Count;\n              \n    \n\nattrValues = []\n\nattrValues.append(['b', 'c', 'x', 'f', 'k', 's'])\nattrValues.append(['f', 'g', 'y', 's'])\nattrValues.append(['n','b','c','g','r','p','u','e','w','y'])\nattrValues.append(['t','f'])\nattrValues.append(['a','l','c','y','f','m','n','p','s'])#4\nattrValues.append(['a','d','f','n'])\nattrValues.append(['c','w','d'])\nattrValues.append(['b','n'])\nattrValues.append(['k','n','b','h','g','r','o','p','u','e','w','y'])\nattrValues.append(['e','t'])\nattrValues.append(['b','c','u','e','z','r','m'])\nattrValues.append(['f','y','k','s'])\nattrValues.append(['f','y','k','s'])\nattrValues.append(['n','b','c','g','o','p','e','w','y'])\nattrValues.append(['n','b','c','g','o','p','e','w','y'])\nattrValues.append(['p','u'])\nattrValues.append(['n','o','w','y'])\nattrValues.append(['n','o','t'])\nattrValues.append(['c','e','f','l','n','p','s','z'])\nattrValues.append(['k','n','b','h','r','o','u','w','y'])\nattrValues.append(['a','c','n','s','v','y'])\nattrValues.append(['g','l','m','p','u','w','d'])\n\n\n\nattrList = []\n\n\ndata = {}\n\ntarget = ['p','e']\n\ndupAttrList = attrList\n\n\ntree = {}\n\nfor i in range(attrLen):\n    attrList.append(i)\n\ndef Intialize(tree):\n    for i in attrList:\n        tree[i] = {}\n        data[i] = {}\n        for j in attrValues[i]:\n            tree[i][j] = -1\n        \n #duplicate Attribute List for not repeating the attributes\n\n\n\nroot = None\nIntialize(tree)\nroot, height, Node_count = build_tree(Training_data, root) #build tree\n\n\n#deleting the entries of the attributes that are not used\n\n    \nprint(tree)\n\nprint('Height of the tree is',height)\n\nprint('Number of the nodes in the tree are',Node_count)\n\nTraining_DataLength = len(Training_data)\ni = 0\n\n\ndef accuracy(tree, data, root):\n    dataLength = len(data)\n    acc = 0\n    i = 0\n    while i < dataLength:\n        result = root\n        while True:\n            result = tree[result][data.iloc[i][result+1]]\n            if result == 'e' or result == 'p':\n                break\n        if data.iloc[i][0] == result:\n            acc = acc + 1\n        i = i+1\n    return (acc/dataLength)*100;\n    \n\ntrainingAccuracy = accuracy(tree, Training_data, root)\ntestingAccuracy = accuracy(tree, Test_data, root)\nprint('Testing accuracy is', testingAccuracy)\n\nprint('Training accuracy is', trainingAccuracy)\n\n#Merging datasets for testing accuracy based on dataset split\n\nmushroom_data = pd.concat([Training_data, Test_data])\nlenMushroom = len(mushroom_data)\n#for problem 2.7\n\ni = 10 #starting with 50% of training data and remaing of test data\nwhile i < 100:\n    trainLength = math.ceil((i/100)*lenMushroom)\n    testLength = lenMushroom - trainLength\n    #print(trainLength, testLength)\n    Training_data = mushroom_data.iloc[0:trainLength]\n    Test_data = mushroom_data.iloc[trainLength+1:lenMushroom]\n    tree = {}\n    root = None\n    Intialize(tree)\n    #print(tree)\n    root, height, Node_count  = build_tree(Training_data, root)\n    #print(tree)\n    trainingAccuracy = accuracy(tree, Training_data, root)\n    #print('height', height)\n    #print('Node_count',Node_count)\n    testingAccuracy = accuracy(tree, Test_data, root)\n    print(i)\n    print('with',i,'% split of training and',100-i,'% split of testing data')\n    print('Testing accuracy is', testingAccuracy)\n    print('Training accuracy is', trainingAccuracy)\n    i = i + 10\n        \n        \n"""
Ensemble Learning/AdaBoost.py,3,"b'import pandas as pd\nimport numpy as np\nimport math\nimport time\nTrain_data = pd.read_csv(\'heart_train.data\',header = None)\nTest_data = pd.read_csv(\'heart_test.data\',header = None)\n\nTrain_data.loc[Train_data[0] == 0, 0] = -1\nTest_data.loc[Test_data[0] == 0, 0] = -1\n\n\n\nattrs = len(Train_data.columns)  #No of Attributes\n\nrows = len(Train_data)   #No of Training Data Points\n\n\nTrain_data = np.array(Train_data.iloc[:,:])\nTest_data = np.array(Test_data.iloc[:,:])\n\n        \n\ndef output(tree, lst):\n    while(True):\n        for key, value in tree.items():\n            #print(\'key\', key)\n            #print(\'value\', value)\n            if value == 1 or value == -1:\n                return value\n            else:\n                k = lst[key]\n                tree = tree[key][k]\n            if tree == 1 or tree == -1:\n                return tree\n\ndef AdaBoost(rounds, model, data):\n    weightedError = 1\n    weight = [1/rows]*rows #Initial weight matrix\n    alpha = []\n    HSpace = [] #Total hypothesis space\n    predictions = []\n    labels = []\n    eps = []    #Weighted errors\n    for r in range(rounds):\n        print(\'round\',r+1)\n        weightedError = 1\n        it = 0\n        for mod in model:\n            w = 0\n            if r == 0:\n                labels.append([])\n            lst = []\n            for i in range(rows):\n                row = data[i, :]\n                if r == 0:\n                    k = output(mod, row)\n                    lst.append(k)\n                    if row[0] != k:\n                        w = w + weight[i]\n                    labels[it].append(k)\n                else:\n                    k = labels[it][i]\n                    lst.append(k)\n                    if row[0] != k:\n                        w = w + weight[i]\n                    if w > weightedError or w > 0.5:#Optimization\n                        break\n            if w < weightedError:\n                weightedError = w\n                bTree = mod #bTree indicates it is a best tree\n                bestP = lst\n            it += 1\n        HSpace.append(bTree)\n        eps.append(weightedError)\n        t = 1/2 * math.log((1-weightedError)/weightedError)\n        alpha.append(t)\n        #Weight Updation\n        sum1 = 0\n        predictions.append(bestP)\n        for i in range(rows):\n            prediction = bestP[i]\n            actual = data[i, 0]\n            #print(prediction, actual)\n            weight[i] = (weight[i] * (np.exp(-1 * prediction * actual * t)))/(2*np.sqrt(weightedError * (1-weightedError)))\n            sum1 += weight[i]\n        print(""sum"", sum1)\n        #print(weight)\n        #print(alpha)\n        print(weightedError)                                  \n    return eps, alpha, HSpace, predictions\n                                        \n                    \n                \n        \n        \ndef Hypothesis():\n    hypo = []\n    lst = [1, -1]\n    for i in range(1, attrs):#attribute iteration\n        for j in range(1, attrs):\n            for k in range(1, attrs):#5 different cases for 3 attribute splits\n                for a in lst:\n                    for b in lst:\n                        for c in lst:\n                            for d in lst:\n                                tree = {}                #case 1 LLL\n                                tree[i] = {}\n                                tree[i][1] = a\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][1] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d\n                                tree1[j][0] = tree2\n                                tree[i][0] = tree1\n                                hypo.append(tree)\n                                \n                                tree = {}                #case 2 LLR\n                                tree[i] = {}\n                                tree[i][1] = a\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][0] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d\n                                tree1[j][1] = tree2\n                                tree[i][0] = tree1\n                                hypo.append(tree)\n                                \n                                tree = {}                #case 3 \n                                tree[i] = {}\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][0] = a\n                                tree1[j][1] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d\n                                tree[i][0] = tree1\n                                tree[i][1] = tree2\n                                hypo.append(tree)\n                                \n                                tree = {}                #case 4\n                                tree[i] = {}\n                                tree[i][0] = a\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][1] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d  \n                                tree1[j][0] = tree2\n                                tree[i][1] = tree1\n                                hypo.append(tree)\n                                \n                                tree = {}                #case 5\n                                tree[i] = {}\n                                tree[i][0] = a\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][0] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d\n                                tree1[j][1] = tree2\n                                tree[i][1] = tree1\n                                hypo.append(tree)  \n    return hypo\n                \n\nrounds = 5\nprint(\'starting time\')\nt1 = time.time()\nThypo = Hypothesis()   #Thypo mean Total hypothesis space\nprint(\'end\')\nt2 = time.time()\nprint(t2-t1, \'secs\')\n#print(hypo)\nprint(\'Generated Hypothesis spaces\')\n\nepsilon, alpha, hypo, predictions = AdaBoost(rounds, Thypo, Train_data) #method for getting Hypothesis space and Alpha values\nt2 = time.time()\nprint(t2-t1, \'secs\')\n\nprint(hypo)\n\n#accuracy calculation\nlength = len(Test_data)\n\naccuracy = 0\nfor i in range(length):\n    p = 0\n    row = Test_data[i, :]\n    y = row[0]\n    for r in range(rounds):\n        k = output(hypo[r], row)\n        p = p + alpha[r] * k\n    if p >= 0:\n        if y == 1:\n            accuracy += 1\n    else:\n        if y == -1:\n            accuracy += 1\n\naccuracy = accuracy/length * 100\nprint(""Accuracy on the test data set is"", accuracy)\n\n\nfor r in range(rounds):\n    print(\'The value of epsilon and alpha for round,\',r+1,\'is\',alpha[r],epsilon[r])\n    \nt2 = time.time()\nprint(t2-t1, \'secs\')'"
Ensemble Learning/Bagging.py,0,"b'import pandas as pd\nimport math\n\nTrain_data = pd.read_csv(\'heart_train.data\',header = None)\nTest_data = pd.read_csv(\'heart_test.data\',header = None)\n\n\nattrs = len(Train_data.columns)  #No of Attributes\n\nrows = len(Train_data)   #No of Training Data Points\n\n\n\ndef splitData(data, attribute):\n    data0 = data.loc[data[attribute] == 0, :]\n    data1 = data.loc[data[attribute] == 1, :]    \n    return data0, data1\n\ndef label(data):\n    p = len(data.loc[data[0] == 1, :])\n    n = len(data.loc[data[0] == 0, :])\n    if p >= n:\n        return 1\n    else:\n        return 0\n\ndef Train(data, attrList):\n    maxProb = 1\n    bestAttr = -1       \n    for i in attrList:\n        prob = 0 #prob of the target given an attribute\n        for j in range(2): #binary values so j is  0, 1 \n            temp = data.loc[data[i] == j, [0, i]]\n            tempLen = len(temp)\n            p = len(temp.loc[temp[0] == 1])\n            e = len(temp.loc[temp[0] == 0])\n            logp = 0\n            loge = 0\n            if(tempLen > 0):\n                if p > 0:\n                    logp = (math.log(p/tempLen))/(math.log(2))\n                if e > 0:\n                    loge = (math.log(e/tempLen))/(math.log(2))\n                prob = prob + -1 * tempLen/rows * ((p/tempLen)*logp+(e/tempLen)*loge)\n        if(prob <= maxProb):\n            maxProb = prob\n            bestAttr = i\n            #print(maxProb)\n            #print(bestAttr)\n    tree = {}\n    tree[bestAttr] = {}\n    data0, data1 = splitData(data, bestAttr)\n    tree[bestAttr][0] = label(data0)\n    tree[bestAttr][1] = label(data1)\n    return tree, bestAttr\n    \n    \n\ndef output(tree, lst):\n    while(True):\n        for key, value in tree.items():\n            #print(\'key\', key)\n            #print(\'value\', value)\n            if value == 1 or value == 0:\n                return value\n            else:\n                k = lst[key]\n                tree = tree[key][k]\n            if tree == 1 or tree == 0:\n                return tree\n            \n    \n\ndef acc(model, data):\n    r = len(data)\n    accuracy = 0\n    for i in range(r):\n        row = data.loc[i, :]\n        lst = [0] * 2\n        for mod in model:\n            lst[output(mod, row)] += 1\n        if lst[0] > lst[1]:\n            if row[0] == 0:\n                accuracy += 1\n        else:\n            if row[0] == 1:\n                accuracy += 1\n    \n    accuracy = accuracy/r * 100\n    return accuracy\n            \n            \n            \n        \n    \n\nit = 25   #Running it for for 25 Iteration to select the best Test accuracy out of 25 Iterations\nTest_accuracy = 0\nwhile(it != 0):\n    n = 20  #20 samplings\n    classifiers = []\n    attributeList = []\n    \n    for i in range(1, attrs):\n            attributeList.append(i)\n    \n    for i in range(20):\n        data = Train_data.sample(n = rows, replace = True)\n        Hspace, attr = Train(data, attributeList)\n        attributeList.remove(attr)\n        classifiers.append(Hspace)\n    accuracy = acc(classifiers, Test_data) \n    if accuracy > Test_accuracy:\n        Test_accuracy = accuracy\n        print(\'Iteration\', it, \'Accuracy\', Test_accuracy)\n    it -= 1 \n\n\n\n    \n    \n    \n\nprint(""Accuracy on test data set is"", Test_accuracy)\n    \nprint(\'Classifiers are\', classifiers)\nprint(\'length\', len(classifiers))\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n\n'"
Ensemble Learning/Boosting_10iterations.py,3,"b'import pandas as pd\nimport numpy as np\nimport math\nimport matplotlib.pyplot as plt\nimport time\nTrain_data = pd.read_csv(\'heart_train.data\',header = None)\nTest_data = pd.read_csv(\'heart_test.data\',header = None)\nTrain_data.loc[Train_data[0] == 0, 0] = -1\nTest_data.loc[Test_data[0] == 0, 0] = -1\n\n\nattrs = len(Train_data.columns)  #No of Attributes\n\nrows = len(Train_data)   #No of Training Data Points\n\n\nTrain_data = np.array(Train_data.iloc[:,:])\nTest_data = np.array(Test_data.iloc[:,:])\n\n        \n    \ndef output(tree, lst):\n    while(True):\n        for key, value in tree.items():\n            #print(\'key\', key)\n            #print(\'value\', value)\n            if value == 1 or value == -1:\n                return value\n            else:\n                k = lst[key]\n                tree = tree[key][k]\n            if tree == 1 or tree == -1:\n                return tree\n        \n            \ndef accur(data, rounds, alpha, hypo):#Accuracy Calculation for a given data, Number of Rounds, Alpha and \n    accuracy = 0                     #Hypothesis space\n    length = len(data)\n    for i in range(length):\n        p = 0\n        row = data[i, :]\n        y = row[0]\n        for r in range(rounds):\n            k = output(hypo[r], row)\n            p = p + alpha[r] * k\n        if p >= 0:\n            if y == 1:\n                accuracy += 1\n        else:\n            if y == -1:\n                accuracy += 1\n    accuracy = accuracy/length * 100\n    return accuracy\n\ndef AdaBoost(rounds, model, data):\n    test = Test_data\n    weightedError = 1\n    weight = [1/rows]*rows #Initial weight matrix\n    alpha = []\n    HSpace = [] #Total hypothesis space\n    predictions = []\n    Train_accuracies = []\n    Test_accuracies = []\n    eps = []    #Weighted errors\n    for r in range(rounds):\n        t1 = time.time()\n        print(\'round\',r+1)\n        weightedError = 1\n        for mod in model:\n            w = 0\n            lst = []\n            for i in range(rows):\n                row = data[i, :]\n                k = output(mod, row)\n                lst.append(k)\n                if row[0] != k:\n                    w = w + weight[i]\n                if w > weightedError or w > 0.51:#Optimization\n                    break\n            if w < weightedError:\n                weightedError = w\n                bTree = mod #bTree indicates it is a best tree\n                bestP = lst\n        HSpace.append(bTree)\n        eps.append(weightedError)\n        t = 1/2 * math.log((1-weightedError)/weightedError)\n        alpha.append(t)\n        #Train and Test Accuracy Calculation\n        acc1 = accur(data, r+1, alpha, HSpace)\n        acc2 = accur(test, r+1, alpha, HSpace)\n        Train_accuracies.append(acc1)\n        Test_accuracies.append(acc2)\n        #Weight Updation\n        sum1 = 0\n        predictions.append(bestP)\n        for i in range(rows):\n            prediction = bestP[i]\n            actual = data[i, 0]\n            #print(prediction, actual)\n            weight[i] = (weight[i] * (np.exp(-1 * prediction * actual * t)))/(2*np.sqrt(weightedError * (1-weightedError)))\n            sum1 += weight[i]\n        #print(weight)\n        #print(alpha)\n        print(weightedError) \n        t2 = time.time()\n        print(\'round\', r+1, \'end\')\n        print(t2-t1, \'secs\')                      \n    return eps, alpha, HSpace, predictions, Train_accuracies, Test_accuracies\n                                        \n                    \n                \n        \n        \ndef Hypothesis():\n    hypo = []\n    lst = [1, -1]\n    for i in range(1, attrs):#attribute iteration\n        for j in range(1, attrs):\n            for k in range(1, attrs):#5 different cases for 3 attribute splits\n                for a in lst:\n                    for b in lst:\n                        for c in lst:\n                            for d in lst:\n                                tree = {}                #case 1 LLL\n                                tree[i] = {}\n                                tree[i][1] = a\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][1] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d\n                                tree1[j][0] = tree2\n                                tree[i][0] = tree1\n                                hypo.append(tree)\n                                \n                                tree = {}                #case 2 LLR\n                                tree[i] = {}\n                                tree[i][1] = a\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][0] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d\n                                tree1[j][1] = tree2\n                                tree[i][0] = tree1\n                                hypo.append(tree)\n                                \n                                tree = {}                #case 3 \n                                tree[i] = {}\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][0] = a\n                                tree1[j][1] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d\n                                tree[i][0] = tree1\n                                tree[i][1] = tree2\n                                hypo.append(tree)\n                                \n                                tree = {}                #case 4\n                                tree[i] = {}\n                                tree[i][0] = a\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][1] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d  \n                                tree1[j][0] = tree2\n                                tree[i][1] = tree1\n                                hypo.append(tree)\n                                \n                                tree = {}                #case 5\n                                tree[i] = {}\n                                tree[i][0] = a\n                                tree1 = {}\n                                tree1[j] = {}\n                                tree1[j][0] = b\n                                tree2 = {}\n                                tree2[k] = {}\n                                tree2[k][0] = c\n                                tree2[k][1] = d\n                                tree1[j][1] = tree2\n                                tree[i][1] = tree1\n                                hypo.append(tree)  \n    return hypo\n                \n\ndata = Train_data\nrounds = 10\nprint(\'starting time\')\nt1 = time.time()\nThypo = Hypothesis()   #Thypo mean Total hypothesis space\nprint(\'end\')\nt2 = time.time()\nprint(t2-t1, \'secs\')\nprint(\'Generated Hypothesis spaces\')\n\nepsilon, alpha, hypo, predictions, Train_accuracies, Test_accuracies = AdaBoost(rounds, Thypo, data) #method for getting Hypothesis space and Alpha values\nt2 = time.time()\nprint(t2-t1, \'secs\')\n\nprint(\'Hypothesis Spaces are\', hypo)\n\nprint(\'Alpha values are\')\nprint(alpha)\n\n\nprint(\'Epsilon values are\')\nprint(epsilon)\n\n\nprint(""Accuracies on the training data set are"", Train_accuracies)\n\n\nprint(""Accuracies on the test data set are"", Test_accuracies)\n\nt2 = time.time()\nprint(t2-t1, \'secs\')\n\n\n#plot reference : https://www.geeksforgeeks.org/graph-plotting-in-python-set-1/\nx1 = []\nfor i in range(rounds):\n    x1.append(i+1)\n\n# plotting the Training accuracies\nplt.plot(x1, Train_accuracies, label = ""Train Accuracies"") \n  \n# plotting the Testing accuracies\nplt.plot(x1, Test_accuracies, label = ""Test Accuracies"") \n  \n# naming the x axis \nplt.xlabel(\'Numbe of Rounds\') \n# naming the y axis \nplt.ylabel(\'Accuracy\') \n# giving a title to my graph \nplt.title(\'Boosting\') \n  \n# show a legend on the plot \nplt.legend() \n  \n# function to show the plot \nplt.show() \n\n\n'"
Ensemble Learning/Boosting_20Iterations.py,3,"b'import pandas as pd\nimport numpy as np\nimport math\nTrain_data = pd.read_csv(\'heart_train.data\',header = None)\nTest_data = pd.read_csv(\'heart_test.data\',header = None)\nTrain_data.loc[Train_data[0] == 0, 0] = -1\nTest_data.loc[Test_data[0] == 0, 0] = -1\n\n\n\nattrs = len(Train_data.columns)  #No of Attributes\n\nrows = len(Train_data)   #No of Training Data Points\n\n\nTrain_data = np.array(Train_data.iloc[:,:])\nTest_data = np.array(Test_data.iloc[:,:])\n        \n\ndef output(tree, lst):\n    while(True):\n        for key, value in tree.items():\n            #print(\'key\', key)\n            #print(\'value\', value)\n            if value == 1 or value == -1:\n                return value\n            else:\n                k = lst[key]\n                tree = tree[key][k]\n            if tree == 1 or tree == -1:\n                return tree\n\ndef AdaBoost(rounds, model, data):\n    weightedError = 1\n    weight = [1/rows]*rows #Initial weight matrix\n    alpha = []\n    HSpace = [] #Total hypothesis space\n    predictions = []\n    \n    for r in range(rounds):\n        weightedError = 1\n        for mod in model:\n            w = 0\n            lst = []\n            for i in range(rows):\n                row = data[i, :]\n                k = output(mod, row)\n                lst.append(k)\n                if row[0] != k:\n                    w = w + weight[i]\n            if w < weightedError:\n                weightedError = w\n                bTree = mod #bTree indicates it is a best tree\n                bestP = lst\n        HSpace.append(bTree)\n        t = 1/2 * math.log((1-weightedError)/weightedError)\n        alpha.append(t)\n        #Weight Updation\n        sum1 = 0\n        predictions.append(bestP)\n        for i in range(rows):\n            prediction = bestP[i]\n            actual = data[i, 0]\n            #print(prediction, actual)\n            weight[i] = (weight[i] * (np.exp(-1 * prediction * actual * t)))/(2*np.sqrt(weightedError * (1-weightedError)))\n            sum1 += weight[i]\n        #print(""sum"", sum1)\n        #print(weight)\n        #print(alpha)\n        print(\'weightedError on round\',r+1,\'is\',weightedError)\n                                        \n    return alpha, HSpace, predictions\n                                        \n                    \n                \n        \n        \ndef Hypothesis():\n    hypo = []\n    lst = [1, -1]\n    for attr in range(1, attrs):#attribute iteration\n        for i in lst:\n            for j in lst: # 4 cases for 2 possible choices of labelling\n                tree = {}\n                tree[attr] = {}\n                tree[attr][0] = i\n                tree[attr][1] = j\n                hypo.append(tree)\n    #print(hypo)\n    print(\'length\', len(hypo))\n    return hypo\n                \n\nrounds = 20\nThypo = Hypothesis()   #Thypo mean Total hypothesis space\nalpha, hypo, predictions = AdaBoost(rounds, Thypo, Train_data) #method for getting Hypothesis space and Alpha values\n\n#accuracy calculation\nlength = len(Test_data)\n\naccuracy = 0\nfor i in range(length):\n    p = 0\n    row = Test_data[i, :]\n    for r in range(rounds):\n        k = output(hypo[r], row)\n        p = p + alpha[r] * k\n    if p > 0:\n        if Test_data[i, 0] == 1:\n            accuracy += 1\n    else:\n        if Test_data[i, 0] == -1:\n            accuracy += 1\n\n\naccuracy = accuracy/length * 100\nprint(\'Accuracy on the test data set after\' ,rounds,\'rounds is\', accuracy)\n        \nprint(\'The values of alpha are\')     \nprint(alpha)'"
Ensemble Learning/Cordinate_Descent.py,4,"b'import pandas as pd\nimport numpy as np\nimport math\nTrain_data = pd.read_csv(\'heart_train.data\',header = None)\nTest_data = pd.read_csv(\'heart_test.data\',header = None)\nTrain_data.loc[Train_data[0] == 0, 0] = -1\nTest_data.loc[Test_data[0] == 0, 0] = -1\n\n\n\nattrs = len(Train_data.columns)  #No of Attributes\n\nrows = len(Train_data)   #No of Training Data Points\n\n\nTrain_data = np.array(Train_data.iloc[:,:])\nTest_data = np.array(Test_data.iloc[:,:])\n\n\ndef output(tree, lst):\n    while(True):\n        for key, value in tree.items():\n            if value == 1 or value == -1:\n                return value\n            else:\n                k = lst[key]\n                tree = tree[key][k]\n            if tree == 1 or tree == -1:\n                return tree\n\ndef Hypothesis():\n    hypo = []\n    lst = [1, -1]\n    for attr in range(1, attrs):#attribute iteration\n        for i in lst:\n            for j in lst: # 4 cases for 2 possible choices of labelling 1, -1\n                tree = {}\n                tree[attr] = {}\n                tree[attr][0] = i\n                tree[attr][1] = j\n                hypo.append(tree)\n    return hypo\n                \n              \ndef Coordinate_Descent(alpha, data, labels):\n    length = len(alpha)\n    it = 0\n    diff = 1\n    while(diff > 0.01):\n        diff = 0\n        for i in range(length):#Updating alphas in Round Robin way\n            n = 0\n            d = 0\n            for r in range(rows):\n                k = data[r, 0]\n                p = 0\n                for j in range(length):\n                    if i != j:\n                        p = p + alpha[j] * labels[j][r]\n                p = np.exp(-1 * k * p)\n                if k == labels[i][r]:\n                    n += p\n                else:\n                    d += p\n            Palpha = alpha[i]\n            alpha[i] = 1/2 * math.log(n/d)\n            diff += abs(alpha[i] - Palpha)\n        #print(alpha)\n        #print(\'diff\',diff)\n        it += 1\n    return alpha\n        \n\nhypo = Hypothesis()   #hypo mean Total hypothesis space\nspace = len(hypo)\nalpha = [.5] * space\n\n\nlabels = []\nfor tree in hypo:\n    lst = []\n    for i in range(rows):\n        row = Train_data[i, :]\n        #print(tree)\n        #print(row)\n        k = output(tree, row)\n        lst.append(k)\n    labels.append(lst)\n        \n#print(labels)     \nalpha = Coordinate_Descent(alpha, Train_data, labels) #method for getting Hypothesis space and Alpha values\n\n\n\n#Exponential Loss on Training set\nexpLoss = 0\nfor r in range(rows):\n    y = Train_data[r, 0]\n    k = 0\n    for h in range(space):\n        k += alpha[h] * labels[h][r]\n    k = np.exp(-1 * y * k)\n    expLoss += k\nprint(\'Exponential Loss on Training data set is\', expLoss)\n\n#accuracy calculation\n\nlength = len(Test_data)\n\naccuracy = 0\nfor i in range(length):\n    p = 0\n    row = Test_data[i, :]\n    for r in range(space):\n        k = output(hypo[r], row)\n        p = p + alpha[r] * k\n    if p >= 0:\n        if row[0] == 1:\n            accuracy += 1\n    else:\n        if row[0] == -1:\n            accuracy += 1\n\naccuracy = accuracy/length * 100\nprint(""Accuracy on the test data set is"", accuracy)\n        \nprint(\'The values of alpha are\')     \nprint(alpha)'"
Feature Selection/PCA.py,3,"b""import pandas as pd\nimport numpy as np\n\n\nTrain_data = pd.read_csv('sonar_train.csv',header = None)\n\n\n\ndef computeCovarianceMatrix(data):\n    mat = data\n    m = len(data)\n    mean = (data.sum(axis=0))/m\n    n = len(data)\n    for i in range(n):\n        mat[i] = mat[i] - mean\n    mat = mat.transpose()\n    mat = np.matmul(mat, mat.transpose())\n    return mat\n    \n    \ndef eigen(matrix):\n    eigenValues = np.linalg.eig(matrix)[0]\n    m = len(eigenValues)\n    idx = eigenValues.argsort()[m-6:m]  \n    print(idx)\n    eigenValues = eigenValues[idx]\n    return eigenValues\n    \n\n\nm = len(Train_data.columns)-1\ndata = np.array(Train_data.iloc[:,0:m])\nCovarianceMatrix = computeCovarianceMatrix(data)\n\n\n\neigenValues = eigen(CovarianceMatrix)\nprint('Top 6 eigen values are', eigenValues)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
Feature Selection/PCA_FeatureSel.py,10,"b""import pandas as pd\nimport numpy as np\nimport math\n\n\nTrain_data = pd.read_csv('sonar_train.csv',header = None)\nTest_data = np.array(pd.read_csv('sonar_test.csv',header = None))\n\ndef calculateProbability(x, mean, stdev):\n    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n\n        \n#Accuracy on Test Data\ndef accuracy(features):\n    n = len(Test_data)\n    count = 0\n    for i in range(n):\n        r = Test_data[i,:]\n        p1 = 1\n        p2 = 1\n        for j in features:\n            mean = data_summary[1][j]['mean']\n            std = data_summary[1][j]['sd']\n            p1 = p1 * calculateProbability(r[j], mean, std)\n            mean = data_summary[2][j]['mean']\n            std = data_summary[2][j]['sd']\n            p2 = p2 * calculateProbability(r[j], mean, std)\n        if p1 > p2:\n            if r[m] == 1:\n                count += 1\n        else:\n            if r[m] == 2:\n                count += 1\n            \n    test_accuracy = count/n * 100\n    return test_accuracy \n\n\ndef computeCovarianceMatrix(data):\n    mat = data\n    m = len(data)\n    mean = (data.sum(axis=0))/m\n    n = len(data)\n    for i in range(n):\n        mat[i] = mat[i] - mean\n    mat = mat.transpose()\n    mat = np.matmul(mat, mat.transpose())\n    return mat\n    \n    \ndef eigen(matrix): #Method for calculating Eigen Values and Eigen Vectors\n    eigenValues, eigenVectors = np.linalg.eig(matrix) \n    idx = eigenValues.argsort()[::-1][0:10]     #[0:6] selects the top 6 eigen values\n    eigenVectors = eigenVectors[:, idx]\n    return eigenVectors\n    \n    \n\ndef ProbDis(vector):   #Define Probability distribution\n    pd = []\n    m, n = vector.shape\n    for i in range(m):\n        v = 0\n        for j in range(n):\n            v = v+(vector[i, j]**2)\n        pd.append(v/n)\n    return pd\n\n\n\n\nm = len(Train_data.columns)-1\ndata = {}\ndata[1] = np.array(Train_data.loc[Train_data[m] == 1])\ndata[2] = np.array(Train_data.loc[Train_data[m] == 2])\n\ndata_summary = {}\nfor key, value in data.items():\n    data_k = value\n    data_summary[key] = {}\n    for i in range(m):\n        tree = {}\n        tree['mean'] = np.mean(data_k[:, i])\n        tree['sd'] = np.std(data_k[:, i])\n        data_summary[key][i] = tree\n\ntrainData = np.array(Train_data.iloc[:,0:m])\nCovarianceMatrix = computeCovarianceMatrix(trainData)\neigenVectors = eigen(CovarianceMatrix)\n\n\n\nk = 10\ns = 20\nfor i in range(k):\n    eg = eigenVectors[:,0:i+1] #Selects the top K eigen Vectors\n    pd = ProbDis(eg) #For Calculating Probability Distribution\n    for j in range(s):\n        sum_acc = 0\n        for it in range(100):  #Running over through 100 iterations to determine average accuracy\n            f = np.random.choice(m, j+1, p=pd) #Sampling with repetition with a given probability\n            features = np.unique(f) #Selecting the unique features\n            test_acc = accuracy(features)\n            sum_acc += test_acc #Summing up to calculate average accuracy\n        avg_testAccuracy = sum_acc/100 #Average accuracy\n        print('Error on test data for value of k =',i+1,'and s = ',j+1,'is',100-avg_testAccuracy)\n    print('\\n')\n            \n            \n            \n\n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
Feature Selection/SVMwithPCA.py,22,"b'import pandas as pd\nimport numpy as np\nimport cvxopt as cpt\n\n\n#Reading the data using pandas\nTrain_data = pd.read_csv(\'sonar_train.csv\',header = None)\nTest_data = pd.read_csv(\'sonar_test.csv\',header = None)\nValidation_data = pd.read_csv(\'sonar_valid.csv\',header = None)\n\n#setting 0 to -1 in the target\nm = len(Train_data.columns)-1\nTrain_data.loc[Train_data[m] == 2, m] = -1\nm = len(Test_data.columns)-1\nTest_data.loc[Test_data[m] == 2, m] = -1\nm = len(Validation_data.columns)-1\nValidation_data.loc[Validation_data[m] == 2, m] = -1\n\ndef splitData(data): #Splits Train data separating labels\n    m = len(data.columns)-1\n    x = np.array(data.iloc[:,0:m])\n    y = np.array(data.iloc[:,m])\n    return x, y    \n\ndef computeCovarianceMatrix(data):\n    mat = data\n    m = len(data)\n    mean = (data.sum(axis=0))/m\n    n = len(data)\n    for i in range(n):\n        mat[i] = mat[i] - mean\n    mat = mat.transpose()\n    mat = np.matmul(mat, mat.transpose())\n    return mat\n \n    \ndef eigen(matrix): #Method for calculating Eigen Values and Eigen Vectors\n    eigenValues, eigenVectors = np.linalg.eig(matrix) \n    idx = eigenValues.argsort()[::-1][0:6]     #[0:6] selects the top 6 eigen values\n    eigenValues = eigenValues[idx]  \n    eigenVectors = eigenVectors[:, idx]\n    return eigenValues, eigenVectors\n    \n\n#Accuracy Computation\ndef computeAccuracy(x, y, w, b):#Given data, w and b computeAccuracy calculates accuracy\n    x = np.asarray(x)\n    count = 0\n    length = len(x)\n    for i in range(length):\n        f = np.dot(w, x[i]) + b\n        if f * y[i] > 0:\n            count += 1\n    accuracy = count/length * 100\n    return accuracy\n\n\n#Methods for calculating P, H, Q AND G \ndef computeP(m, n):\n    P = np.zeros((m+n+1, m+n+1))\n    for i in range(n):\n        P[i][i] = 1\n    return cpt.matrix(P)\n    \ndef computeH(m):\n    H = np.zeros((2*m, 1))\n    for i in range(m):\n        H[i][0] = -1\n    return cpt.matrix(H)\n\ndef computeQ(m, n, c):\n    Q = np.zeros((m+n+1, 1))\n    for i in range(n, m+n):\n        Q[i][0] = c\n    return cpt.matrix(Q)\n\ndef computeG(Train_x, Train_y, m, n):\n    a = np.zeros((m, n))\n    b = np.zeros((m, n))\n    for i in range(m):\n        for j in range(n):\n            a[i][j] = - Train_y[i] * Train_x[i][j]\n    x = np.vstack((a, b)) #hstack\n            \n    c = np.identity(m) * -1\n    d = np.identity(m) * -1\n    y = np.vstack((c, d))\n            \n    e = np.ones((m, 1)) * -1\n    for i in range(m):\n        e[i] = e[i] * Train_y[i]\n    f = np.zeros((m,1))\n    z = np.vstack((e, f))\n    GMat = np.hstack((x, y, z))\n    G = cpt.matrix(GMat)\n    return G\n\ndef ProjectData(data): #Projecting the data using PCA\n    data_x, data_y = splitData(data)\n    CovarianceMatrix = computeCovarianceMatrix(data_x)\n    eigenValues, eigenVectors = eigen(CovarianceMatrix) \n    m = len(data.columns)-1\n    data = np.array(data.iloc[:,0:m])\n    data_x = np.matmul(data, eigenVectors) #Projecting the data into new dimensional place\n    return data_x, data_y, eigenVectors\n\n\ndef ProjectData1(data, eigenVectors):\n    data, data_y = splitData(data)\n    data_x = np.matmul(data, eigenVectors) #Projecting the data into new dimensional place\n    return data_x, data_y\n    \n    \n    \n#Projects the training, testing and validation data into 6 dimensions w.r.to frobenious form\ndata_x, data_y, eigenVectors = ProjectData(Train_data)\nvalid_x, valid_y = ProjectData1(Validation_data, eigenVectors)\ntest_x, test_y = ProjectData1(Test_data, eigenVectors)\n\n\nk = 6 #K values\nC =[1, 10, 100, 1000]   #C values\nm = len(data_x)  #number of rows\nfinalWeight = []\nfinalBias = 0\nbestAccuracy = 0\nbestK = 0\nbestC = 0\nfor i in range(k):\n    data_k = data_x[:, 0:i+1]\n    valid_k = valid_x[:, 0:i+1]\n    n = data_k.shape[1]   #number of columns\n    P = computeP(m, n)\n    G = computeG(data_k, data_y, m, n)\n    h = computeH(m)\n    for j in C:\n        q = computeQ(m, n, j)\n        #Quadratic Programming cvxopt solver\n        result = cpt.solvers.qp(P, q, G, h)\n        lst = result[\'x\']\n        #Weight Calculation\n        weight = []\n        for k in range(n):\n            weight.append(lst[k]) \n        #bias\n        bias = lst[m+n]\n        acc = computeAccuracy(data_k, data_y, weight, bias)\n        acc = computeAccuracy(valid_k, valid_y, weight, bias)\n        print(\'Error on Validation data set is\',100-acc,\' for value of C =\',j,\'and K =\',i+1)\n        if(acc > bestAccuracy): #Tuning the value of c and K from Validation set\n            finalWeight = weight\n            finalBias = bias\n            bestAccuracy = acc\n            bestK = i+1\n            bestC = j\n            \n        \nprint(\'The best pair of k and c value is\',bestK,\'and\',bestC)\n#Accuracy on testing set\naccuracy_proj = computeAccuracy(test_x[:, 0:bestK], test_y, finalWeight, finalBias)\nprint(\'Error on testing set of Projected Data is\', 100-accuracy_proj)\n\n\n\n\n\n""""""\nCalculating the estimator of error on the original data instead of Projection\n""""""\ndata_x, data_y = splitData(Train_data)\nvalid_x, valid_y = splitData(Validation_data)\ntest_x, test_y = splitData(Test_data)\nC =[1, 10, 100, 1000]   #C values\nm = len(data_x)  #number of rows\nfinalWeight = []\nfinalBias = 0\nbestAccuracy = 0\nbestC = 0\nn = data_x.shape[1]  #number of columns\nprint(n)\nP = computeP(m, n)\nG = computeG(data_x, data_y, m, n)\nh = computeH(m)\nfor j in C:\n    print(j)\n    q = computeQ(m, n, j)\n    #Quadratic Programming cvxopt solver\n    result = cpt.solvers.qp(P, q, G, h)\n    lst = result[\'x\']\n    #Weight Calculation\n    weight = []\n    for k in range(n):\n        weight.append(lst[k]) \n    #bias\n    bias = lst[m+n]\n    print(data_x.shape)\n    acc = computeAccuracy(data_x, data_y, weight, bias)\n    acc = computeAccuracy(valid_x, valid_y, weight, bias)\n    if(acc > bestAccuracy): #Tuning the value of c and K from Validation set\n        finalWeight = weight\n        finalBias = bias\n        bestAccuracy = acc\n        bestC = j\n            \n        \nprint(\'The best c value on original data is\',bestC)\n#Accuracy on testing set\nprint(\'Error on testing set on the original data is\', 100-computeAccuracy(test_x, test_y, finalWeight, finalBias))\n\nprint(\'Error on testing set of Projected Data is\', 100-accuracy_proj)\n\n\n\n\n\n\n\n\n\n\n\n\n'"
Feature Selection/naiveBayes.py,5,"b""import pandas as pd\nimport numpy as np\nimport math\n\n#Reading the data using pandas\nTrain_data = pd.read_csv('sonar_train.csv',header = None)\nTest_data = pd.read_csv('sonar_test.csv',header = None)\n\ndef calculateProbability(x, mean, stdev):\n    exponent = math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n    return (1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n\nm = len(Train_data.columns)-1\ndata = {}\ndata[1] = np.array(Train_data.loc[Train_data[m] == 1])\ndata[2] = np.array(Train_data.loc[Train_data[m] == 2])\n\ndata_summary = {}\nfor key, value in data.items():\n    data_k = value\n    data_summary[key] = {}\n    for i in range(m):\n        tree = {}\n        tree['mean'] = np.mean(data_k[:, i])\n        tree['sd'] = np.std(data_k[:, i])\n        data_summary[key][i] = tree\n        \nTest_data = np.array(Test_data)\n#Accuracy on Test Data\nn = len(Test_data)\ncount = 0\nfor i in range(n):\n    r = Test_data[i,:]\n    p1 = 1\n    p2 = 1\n    for j in range(m):\n        mean = data_summary[1][j]['mean']\n        std = data_summary[1][j]['sd']\n        p1 = p1 * calculateProbability(r[j], mean, std)\n        mean = data_summary[2][j]['mean']\n        std = data_summary[2][j]['sd']\n        p2 = p2 * calculateProbability(r[j], mean, std)\n    if p1 > p2:\n        if r[m] == 1:\n            count += 1\n    else:\n        if r[m] == 2:\n            count += 1\n        \n        \ntest_accuracy = count/n * 100\nprint('Accuracy on test data is', test_accuracy)        \n    \n    \n\n\n"""
Linear Regression/GradientDescent.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nSpyder Editor\n\nThis is a temporary script file.\n""""""\n\nimport pandas as pd\n\ndata = pd.read_csv(\'perceptron.data\',header = None)\n\n\n\n\nx = data[[0, 1, 2, 3]]\nprint(x)\nlearningStep = 1\n\nw = [0, 0, 0, 0]\nb = 0\n\n    \n\n\ndef predicted(x):\n    wx = 0\n    for i in range(len(x)):\n        wx = wx + (w[i]*(x.iloc[i]))\n    wx = wx + b\n    return wx\n      \n    \n\ndef perceptronLoss():\n    loss = 0\n    for i in range(len(data)):\n        f = predicted(x.iloc[i])\n        z = -1*(f)*(data.iloc[i][4])\n        loss = loss + max(0, z)\n    return loss\n            \n   \ndef subGradient():\n    lossw = [0, 0, 0, 0]\n    lossb = 0\n    for i in range(len(data)):\n        z = -(data.iloc[i][4])*predicted(x.iloc[i])\n        if(z >= 0):\n            lossw = lossw + data.iloc[i][4]*(x.iloc[i])\n            lossb = lossb + data.iloc[i][4]\n    return (lossw, lossb) \n\n\n\n    \niterations = 10\nit= 1\nprint(""Weight and Bias for the first 3 iterations are "")\nwhile(True):\n    learningStep = 1/(1+it); \n    Deltaw, Deltab = subGradient()\n    #print(""Deltaw "",Deltaw)\n    #print(""Deltab "",Deltab)\n    w = w + learningStep*Deltaw\n    b = b + learningStep*Deltab\n    pLoss = perceptronLoss()\n    print(pLoss)\n    if(it <= 3):\n        print(""Weight\\n"",w)\n        print(""bias\\n"",b)\n     #stopping condition subgradient should be zer0 \n    if(all(v == 0 for v in Deltaw) and Deltab==0):    \n        break\n    print(it)\n    it = it+1\n    \nprint(""Total number of Iterations is "",it)\npLoss = perceptronLoss()\nprint(""Loss "",pLoss)\nprint(""Final weight "",w)\nprint(""Final bias "",b)\n\n'"
Linear Regression/stochastic.py,0,"b'#stochastic\nimport pandas as pd\n\ndata = pd.read_csv(\'perceptron.data\',header = None)\n\n\n\n\nx = data[[0, 1, 2, 3]]\n\nlearningStep = 1\n#[149.277140, 52.533473, 1.67163, -172.891940] w\n#-322.0 b\nw = [0, 0, 0, 0]\nb = 0\n\n    \n\n\ndef predicted(x):\n    wx = 0\n    for i in range(len(x)):\n        wx = wx + (w[i]*(x.iloc[i]))\n    wx = wx + b\n    return wx\n      \n    \n\ndef perceptronLoss():\n    loss = 0\n    for i in range(len(data)):\n        f = predicted(x.iloc[i])\n        z = -1*(f)*(data.iloc[i][4])\n        loss = loss + max(0, z)\n    return loss\n            \n   \ndef subGradient(i):\n    lossw = [0, 0, 0, 0]\n    lossb = 0\n    z = -(data.iloc[i][4])*predicted(x.iloc[i])\n    if(z >= 0):\n        lossw = lossw + data.iloc[i][4]*(x.iloc[i])\n        lossb = lossb + data.iloc[i][4]\n    return (lossw, lossb) \n\n\n\n    \niterations = 1000\ni = 0 \nk= 0\n\nprint(""Weight and Bias for the first 3 iterations are "")\nwhile(True):\n#    print(""i"",i)\n    Deltaw, Deltab = subGradient(i)\n    w = w + learningStep*Deltaw\n    b = b + learningStep*Deltab\n    if(i <= 2 and k<1):\n        print(""Weight\\n"",w)\n        print(""bias\\n"",b)\n    if(i%1000 == 0):\n        pLoss = perceptronLoss()\n        #print(""pLoss \\t"",pLoss)\n        if(pLoss == 0):\n            print(k)\n            print(i)\n            break\n    i = i+1\n    if(i == 1000):\n        i=0\n        k = k + 1\n        print(k)\nprint(""Total number of iterations is "",(k+1)*1000)\nprint(""Loss "",pLoss)\nprint(""Final weight "",w)\nprint(""Final bias "",b)\n\n""""""i = 0\nwhile(i<100):\n    print(predicted(x.iloc[i]))\n    i = i + 1""""""\n'"
Logistic Regression/l2-logistic-reg.py,12,"b'import numpy as np\n\n# Compute the value of the likelihood function\ndef compute_loss(datarow, w, b):\n    wxb = np.dot(w.T, datarow[1:23]) + b\n    return (((datarow[0] + 1)/2) * wxb) - (np.log(1 + np.exp(wxb)))\n\n# Compute w and b gradient values\ndef compute_wb(datarow, w, b):\n    wxb = np.exp(np.dot(w.T, datarow[1:23]) + b)\n    py1 = wxb / (1 + wxb)\n    diff_b = ((datarow[0] + 1) / 2) - py1\n    diff_w = datarow[1:23] * diff_b\n    return diff_w, diff_b\n\n# Predict values\ndef predict(w, b, datarow):\n    wxb = np.exp(np.dot(w.T, datarow) + b)\n    py_positive = wxb / (1 + wxb)\n    py_negative = 1 / (1 + wxb)\n    return 1.0 if py_positive >= py_negative else -1.0\n\n# Accuracy calculation\ndef accuracy(Y1,Y2):\n    counter = 0\n    index = 0\n    for val1 in Y1:\n        if val1 == Y2[index]:\n            counter += 1\n        index += 1\n\n    return counter/len(Y1)\n\n########################## TRAIN ##########################\n# Opening training file and reading contents\npark_train_file = open(\'park_train.data\',\'r\')\nfile_contents = """"\nif park_train_file.mode == \'r\':\n    file_contents = park_train_file.read()\n\npark_train_file.close()\n\n# Read file to parse contents and store in numpy array\ntrain_data = np.genfromtxt(\'park_train.data\', delimiter="","")\ntrain_data_length = len(train_data)\n\n# Gradient ascent parameter initialization\nstep_size = 0.000001\nlambda_array = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n\n# w and b dict for all lambda values\nwb_lambda_dict = {}\ncols = train_data.shape[1] -1\nfor lambda_value in lambda_array:\n\n    # Gradient ascent parameters\n    iteration_counter = 1\n    w_vector = 0.25 * np.ones((22,), dtype=float)\n    b_value = 0.75\n    total_loss = 0.0\n\n    # Iterating till convergence\n    while True:\n\n        # Saving previous loss\n        prev_loss = total_loss\n        total_loss = 0.0\n        grad_sum_w = np.zeros((22,))\n        grad_sum_b = 0.0\n\n        # Iterating all training data\n        for i in range(train_data_length):\n            total_loss += compute_loss(train_data[i], w_vector, b_value)\n            temp_w, temp_b = compute_wb(train_data[i], w_vector, b_value)\n            grad_sum_w += temp_w\n            grad_sum_b += temp_b\n\n        # L2 regularization\n        total_loss -= (lambda_value/2) * (np.linalg.norm(w_vector))\n        grad_sum_w -= lambda_value * np.array([1]*cols)\n\n        print(""Iteration:"", iteration_counter, ""Total loss:"", total_loss)\n\n        # If difference in loss in minimal\n        if total_loss - prev_loss < 0.00015 and iteration_counter >= 2:\n            break\n\n        # Updating w and b values\n        w_vector += step_size * grad_sum_w\n        b_value += step_size * grad_sum_b\n\n        # Increasing count\n        iteration_counter += 1\n\n    wb_lambda_dict[lambda_value] = np.append(w_vector, b_value)\n\n\n############################# VALIDATION #########################################\n# Opening validation file and reading contents\npark_valid_file = open(\'park_validation.data\',\'r\')\nfile_contents = """"\nif park_valid_file.mode == \'r\':\n    file_contents = park_valid_file.read()\n\npark_valid_file.close()\n\n# Creating variable to parse and store validation data\nvalid_data = np.empty((58,23), dtype=float)\n\n# Initialise row and column counters\ni = 0\nj = 0\n\n# Read file to parse contents and store in numpy array\nfor line in file_contents.split(\'\\n\'):\n    for dt in line.split("",""):\n        # If y is 0 change to -1\n        if j == 0 and dt == \'0\':\n            valid_data[i, j] = -1\n        else:\n            valid_data[i, j] = dt\n        j += 1\n    j = 0\n    print(i)\n    i += 1\n\n# Accuracy dictionary for validation data\nacc_dict_valid = {}\n\nbest_valid_lambda = -100\nbest_valid_accuracy = -100\n\n# Calculating accuracy for validation data\nfor lamb, vector in wb_lambda_dict.items():\n    if len(vector) != 0:\n        pred_array = []\n        actual_data = []\n        for i in range(len(valid_data)):\n            slice_data = valid_data[i][1:23]\n            actual_data.append(valid_data[i][0])\n            pred_array.append(predict(vector[0:22], vector[22], slice_data))\n\n        acc_dict_valid[lamb] = accuracy(pred_array, actual_data)\n\n        if acc_dict_valid[lamb] >= best_valid_accuracy:\n            best_valid_lambda = lamb\n            best_valid_accuracy = acc_dict_valid[lamb]\n\nprint(""Accuracy on validation data for each lambda:"", acc_dict_valid)\n\n############################# TEST #########################################\n# Opening test file and reading contents\npark_test_file = open(\'park_test.data\',\'r\')\nfile_contents = """"\nif park_test_file.mode == \'r\':\n    file_contents = park_test_file.read()\n\npark_test_file.close()\n\n# Creating variable to parse and store test data\ntest_data = np.empty((59,23), dtype=float)\n\n# Initialise row and column counters\ni = 0\nj = 0\n\n# Read file to parse contents and store in numpy array\nfor line in file_contents.split(\'\\n\'):\n    for dt in line.split("",""):\n        # If y is 0 change to -1\n        if j == 0 and dt == \'0\':\n            test_data[i, j] = -1\n        else:\n            test_data[i, j] = dt\n        j += 1\n    j = 0\n    i += 1\n\n# Calculating accuracy for test data\npred_array = []\nactual_data = []\n\nfor i in range(len(test_data)):\n    slice_data = test_data[i][1:23]\n    actual_data.append(test_data[i][0])\n    pred_array.append(predict(wb_lambda_dict[best_valid_lambda][0:22], wb_lambda_dict[best_valid_lambda][22], slice_data))\n\nprint(""Best w vector:"", wb_lambda_dict[best_valid_lambda][0:22])\nprint(""Best b value:"", wb_lambda_dict[best_valid_lambda][22])\nprint(""Accuracy on test data:"", accuracy(pred_array, actual_data), ""for best lambda:"", best_valid_lambda)\n\n'"
Logistic Regression/logisticRegression.py,15,"b'import numpy as np\n\nTrain_data = np.genfromtxt(\'park_train.data\', delimiter="","")\nTest_data = np.genfromtxt(\'park_test.data\', delimiter="","")\nValidation_data = np.genfromtxt(\'park_validation.data\', delimiter="","")\n\n#setting 0 to -1 in the target\n\nTrain_data[Train_data[:, 0] == 0, 0] = -1\nTest_data[Test_data[:, 0] == 0, 0] = -1\nValidation_data[Validation_data[:, 0] == 0, 0] = -1\n\n\n\ndef sigmoid_function(theta):\n    s = 1 / (1 + np.exp(-theta))\n    return s\n\ndef compute_loss(data, weight, bias):\n    m = data.shape[0]\n    loss = 0\n    for i in range(m):\n        x = data[i, 1:]\n        y = data[i, 0]\n        k = np.dot(weight.T, x) + bias\n        #print(1 + np.exp(k))\n        loss += y*k - np.log(1 + np.exp(k))\n    return loss\n\ndef logistic_regression(data, weight, bias, step):\n    m = data.shape[0]\n    loss = compute_loss(data, weight, bias)\n    it = 0\n    while True:\n        it += 1\n        gradient_w = 0\n        gradient_b = 0\n        for i in range(m):\n            x = data[i, 1:]\n            k = np.dot(weight.T, x) + bias\n            p = sigmoid_function(k)\n            gradient_w += x * ((data[i, 0]+1)/2 - p)\n            gradient_b += ((data[i, 0]+1)/2 - p)\n        w = weight + step * gradient_w\n        b = bias + step * gradient_b\n        loss1 = compute_loss(data, weight, bias)\n        if loss1 - loss < 0.0001 and it >= 2:\n            break\n        loss = loss1\n        weight = w\n        bias = b\n    print(""Iterations"", it)\n    return w, b\n\ndef logistic_regression_l1(data, weight, bias, step, l1):\n    m, n = data.shape\n    n -= 1\n    wt = np.array([1] * n)\n    loss = compute_loss(data, weight, bias)\n    it = 0\n    while True:\n        it += 1\n        gradient_w = 0\n        gradient_b = 0\n        for i in range(m):\n            x = data[i, 1:]\n            k = np.dot(weight.T, x) + bias\n            p = sigmoid_function(k)\n            gradient_w += x * ((data[i, 0]+1)/2 - p)\n            gradient_b += ((data[i, 0]+1)/2 - p)\n        gradient_w = gradient_w - l1 * wt\n        w = weight + step * gradient_w\n        b = bias + step * gradient_b\n        loss1 = compute_loss(data, weight, bias) - l1 * np.linalg.norm(weight)\n        if loss1 - loss < 0.0001 and it >= 2:\n            break\n        loss = loss1\n        weight = w\n        bias = b\n    return w, b             \n            \ndef logistic_regression_l2(data, weight, bias, step, l2):\n    m = data.shape[0]\n    it = 0\n    loss = compute_loss(data, weight, bias)\n    while True:\n        it += 1\n        gradient_w = 0\n        gradient_b = 0\n        for i in range(m):\n            x = data[i, 1:]\n            k = np.dot(weight.T, x) + bias\n            p = sigmoid_function(k)\n            gradient_w += x * ((data[i, 0]+1)/2 - p)\n            gradient_b += ((data[i, 0]+1)/2 - p)\n        gradient_w = gradient_w - (l2 * weight)\n        w = weight + step * gradient_w\n        b = bias + step * gradient_b\n        loss1 = compute_loss(data, weight, bias) - l2 * (np.linalg.norm(weight) ** 2)\n        if loss1 - loss < 0.0001 and it >= 2:\n            break\n        loss = loss1\n        weight = w\n        bias = b\n    return w, b           \n            \n\n\ndef accuracy(data, weight, bias):\n    m = data.shape[0]\n    cnt = 0\n    for i in range(m):\n        x = data[i, 1:]\n        t = np.dot(weight.T, x) + bias\n        if t > 0:\n            if data[i, 0] > 0:\n                cnt += 1\n        else:\n            if data[i, 0] < 0:\n                cnt += 1\n    return cnt/m * 100\n\n\n\ncols = Train_data.shape[1] - 1\nweight = [0.25] * cols\nweight = np.array(weight)\nbias = 0.75\nlearningRate = [0.000001] #Defines the step size learning rate\n\n\nacc = 0\nfor lr in learningRate:\n    w, b = logistic_regression(Train_data, weight, bias, lr)\n    valid_acc = accuracy(Validation_data, w, b)\n    print(""Validation accuracy is"",valid_acc)\n    if valid_acc >= acc:\n        acc = valid_acc\n        finalWeight = w\n        finalBias = b\n\nprint(finalWeight)  \nprint(\'Accuarcy on test data is\', accuracy(Test_data, finalWeight, finalBias))\n\nacc = 0\nl2 = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 1000]\nfor lr in learningRate:\n    for i in l2:\n        w, b = logistic_regression_l2(Train_data, weight, bias, lr, i)\n        valid_acc = accuracy(Validation_data, w, b)\n        print(""Validation accuracy for l2="",i,""is"",valid_acc)\n        if valid_acc >= acc:\n            bestl2 = i\n            acc = valid_acc\n            finalWeight = w\n            finalBias = b\n\nprint(""Weight vector of l2 regularization"", finalWeight)  \nprint(""Bias of l2 regularization"", finalBias)\nprint(""Best l2 constant"",bestl2)\nprint(\'Accuarcy on test data with l2 penalty is\', accuracy(Test_data, finalWeight, finalBias))\n        \n\n    \n#Logistic Regression \nacc = 0\nl1 = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 1000]\nfor lr in learningRate:\n    for i in l1:\n        w, b = logistic_regression_l1(Train_data, weight, bias, lr, i)\n        valid_acc = accuracy(Validation_data, w, b)\n        print(""Validation accuracy for l1="",i,""is"",valid_acc)\n        if valid_acc >= acc:\n            bestl1 = i\n            acc = valid_acc\n            finalWeight = w\n            finalBias = b\n\nprint(""Weight vector of l1 regularization"", finalWeight)  \nprint(""Bias of l1 regularization"", finalBias)\nprint(""Best l1 constant"",bestl1)\nprint(\'Accuarcy on test data with l1 penalty is\', accuracy(Test_data, finalWeight, finalBias))                \n'"
Mixture Models/GMM.py,18,"b'import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import adjusted_rand_score\nfrom sklearn.metrics import fowlkes_mallows_score\n\n\ndef logLikelihood(data, mean, covariance, lambda1, k, m): # Log Likelihood \n\n    log = 0\n    for i in range(len(data)):\n        sum1 = 0\n        for j in range(k):\n            sum1 += lambda1[j] * guassian(data[i], mean[j], covariance[j], m)\n        log += np.log(sum1)\n    return log\n\ndef gmmPred(data, mean, covariance, lambda1, k, m):\n    pred = []\n    for i in range(len(data)):\n        best_likelihood = None\n        best_cluster = None\n        for j in range(k):\n            likelihood = lambda1[j] * guassian(data[i], mean[j], covariance[j], m)\n            if best_likelihood is None or best_likelihood <= likelihood:\n                best_likelihood = likelihood\n                best_cluster = j\n        pred.append(best_cluster)\n    return pred\n\ndef preprocessData(data): #Preprocessing Data\n    rows, cols = data.shape\n    for i in range(cols):\n        mean = np.mean(data[:, i])\n        std = np.std(data[:, i])\n        data[:, i] = (data[:, i]-mean)/std\n    return data\n\ndef guassian(row, mean, covariance, m): # Multivariate Gaussian\n    diff_data_mean = np.array(row - mean).reshape(1, m)\n    exp = np.exp(-0.5 * np.dot(np.dot(diff_data_mean, np.linalg.inv(covariance)), diff_data_mean.T))\n    return (1 / np.sqrt(((2 * math.pi) ** m) * np.linalg.det(covariance))) * exp\n\n\ntrain_data = np.array(pd.read_csv(\'leaf.data\',header = None)) #Train data\ntrainLength = len(train_data)\ntrain_features = train_data[:, 0]\ntrain_data = train_data[:, 1:]\ncols = len(train_data[0])\nscaledData = preprocessData(train_data) #Scaling data\n\nkArray = [12, 18, 24, 36, 42] # K array\n\n# Get GMM objective loss array and compute mean and variance\nlossArray = []\n\nmeanArray = []\ncovarianceArray = []\nlambdaArray = []\n# For each K\nfor k in kArray:\n    print(\'K-value\',k)\n    for i in range(20): #20 random Intializations\n        print(""Iter"",i)\n        centers = np.empty((k, cols), dtype=np.float64)\n        for j in range(k):\n            centers[j] = np.array(np.random.choice(np.arange(-3, 4, 1), cols)).reshape(1, cols)\n        cov_matrix_arr = np.empty((k, cols, cols))\n        for j in range(k):\n            cov_matrix_arr[j] = np.identity(n=cols, dtype=np.float64)\n\n        lambda_arr = np.empty((k, 1), dtype=np.float64)\n        for j in range(k):\n            lambda_arr[j] = 1/k\n\n        logLikelihoodVal = logLikelihood(scaledData, centers, cov_matrix_arr, lambda_arr, k, cols)\n        iteration_counter = 1\n        while True:\n            #E Step\n            q_array = np.empty((trainLength, k), dtype=np.float64)\n            for x in range(trainLength):\n                den_sum = 0\n                for k_val in range(k):\n                    q_array[x, k_val] = lambda_arr[k_val] * guassian(scaledData[x], centers[k_val], cov_matrix_arr[k_val], cols)\n                    den_sum += q_array[x, k_val]\n\n                q_array[x] = q_array[x] / den_sum\n            #M Step\n            for k_val in range(k):\n                num_total = 0\n                den_total = 0\n                for m in range(trainLength):\n                    num_total += q_array[m, k_val] * scaledData[m]\n                    den_total += q_array[m, k_val]\n                centers[k_val] = num_total / den_total\n\n            for k_val in range(k):\n                num_total = 0.0\n                den_total = 0.0\n\n                for m in range(trainLength):\n                    diff_vector = scaledData[m] - centers[k_val]\n                    diff_vector = np.array(diff_vector).reshape((1, cols))\n                    num_total += q_array[m, k_val] * np.dot(diff_vector.T, diff_vector)\n                    den_total += q_array[m, k_val]\n                cov_matrix_arr[k_val] = num_total / den_total\n                cov_matrix_arr[k_val] += np.identity(n=cols)\n\n            for k_val in range(k):\n                num_total = 0\n                for m in range(trainLength):\n                    num_total += q_array[m, k_val]\n\n            lambda_arr[k_val] = num_total / trainLength\n\n            prevLog = logLikelihoodVal\n            logLikelihoodVal = logLikelihood(scaledData, centers, cov_matrix_arr, lambda_arr, k, cols)\n            if prevLog >= logLikelihoodVal:             # Convergence Check\n                lossArray.append(logLikelihoodVal)\n                meanArray.append(centers)\n                covarianceArray.append(cov_matrix_arr)\n                lambdaArray.append(lambda_arr)\n                break\n\nindex = 0            # Mean and variance of GMM objective    \nwhile index < 5:\n    k = index * 20\n    print(""The mean and Variance of the GMM Objective for k ="",kArray[index],""is"", np.mean(lossArray[k:k+20]), ""and:"", np.var(lossArray[k:k+20]))\n    index += 1\n\n""""""\nUsing 2 metrics to compare clusters against true labels \n""""""\nadjRand = 0\nfms = 0\ntemp_data = np.append(scaledData, np.array(train_features - 1).reshape((trainLength, 1)), axis=1)\nfor i in range(20):\n    predict_array = gmmPred(scaledData, meanArray[60+i], covarianceArray[60+i], lambdaArray[60+i], 36, cols)\n    adjRand += adjusted_rand_score(train_data[:, 0], predict_array) #Adjusted Rand Score\n    fms += fowlkes_mallows_score(train_data[:, 0], predict_array) #Fowlkes Mallows Score\n\nprint(""Adjusted Rand Index of the GMM model with k: 36 is"", adjRand/20)\nprint(""Fowkes Mallows Score of the GMM model with k: 36 is"", fms/20)\n'"
Mixture Models/GMMPlus.py,20,"b'import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.metrics import adjusted_rand_score\nfrom sklearn.metrics import fowlkes_mallows_score\n\n\ndef getClusterCenters(data, k):\n    rows, cols = data.shape\n    centers = np.zeros((k, cols))\n    r = np.random.choice(rows, 1) #Initial Choice\n    centers[0, :] = data[r, :]\n    k_center = 1\n    while k_center < k:\n        pd = [] #pd represents probability distribution\n        dist_sum = 0\n        for i in range(rows):\n            max_dist = 1000\n            for j in range(k_center):\n                dist = np.linalg.norm(data[i] - centers[j]) ** 2\n                if dist < max_dist:\n                    max_dist = dist\n            d = max_dist*max_dist\n            dist_sum += d\n            pd.append(d)\n        for i in range(rows):\n            pd[i] = pd[i]/dist_sum\n        r = np.random.choice(rows, 1, p=pd) \n        centers[k_center, :] = data[r, :]\n        k_center = k_center + 1\n    return centers\n\n# Log Likelihood \ndef logLikelihood(data, mean, covariance, lambda1, k, m):\n    log = 0\n    for i in range(len(data)):\n        sum1 = 0\n        for j in range(k):\n            sum1 += lambda1[j] * guassian(data[i], mean[j], covariance[j], m)\n        log += np.log(sum1)\n    return log\n\ndef gmmPred(data, mean, covariance, lambda1, k, m):\n    pred = []\n    for i in range(len(data)):\n        best_likelihood = None\n        best_cluster = None\n        for j in range(k):\n            likelihood = lambda1[j] * guassian(data[i], mean[j], covariance[j], m)\n            if best_likelihood is None or best_likelihood <= likelihood:\n                best_likelihood = likelihood\n                best_cluster = j\n        pred.append(best_cluster)\n    return pred\n\ndef preprocessData(data): #Preprocessing Data\n    rows, cols = data.shape\n    for i in range(cols):\n        mean = np.mean(data[:, i])\n        std = np.std(data[:, i])\n        data[:, i] = (data[:, i]-mean)/std\n    return data\n\ndef guassian(row, mean, covariance, m): # Multivariate Gaussian\n    diff_data_mean = np.array(row - mean).reshape(1, m)\n    exp = np.exp(-0.5 * np.dot(np.dot(diff_data_mean, np.linalg.inv(covariance)), diff_data_mean.T))\n    return (1 / np.sqrt(((2 * math.pi) ** m) * np.linalg.det(covariance))) * exp\n\n\ntrain_data = np.array(pd.read_csv(\'leaf.data\',header = None)) #Train data\ntrainLength = len(train_data)\ntrain_features = train_data[:, 0]\ntrain_data = train_data[:, 1:]\ncols = len(train_data[0])\n\nscaledData = preprocessData(train_data) #Scaling data\n\n# K array\nkArray = [12, 18, 24, 36, 42]\n\n# Get GMM objective loss array and compute mean and variance\nlossArray = []\n\nmeanArray = []\ncovarianceArray = []\nlambdaArray = []\n\n# For each K\nfor k in kArray:\n    print(\'K-value\',k)\n    for i in range(20): #20 random Intializations\n        print(""Iter"",i)\n        mean_arr = getClusterCenters(train_data, k)\n        covMatrix = np.empty((k, cols, cols))\n        for j in range(k):\n            covMatrix[j] = np.identity(n=cols, dtype=np.float64)\n\n        lambda_arr = np.empty((k, 1), dtype=np.float64)\n        for j in range(k):\n            lambda_arr[j] = 1/k\n\n        log_like_val = logLikelihood(scaledData, mean_arr, covMatrix, lambda_arr, k, cols)\n        iteration_counter = 1\n        while True:\n            q_array = np.empty((trainLength, k), dtype=np.float64)             #E Step\n            for x in range(trainLength):\n                den_sum = 0\n                for k_val in range(k):\n                    q_array[x, k_val] = lambda_arr[k_val] * guassian(scaledData[x], mean_arr[k_val], covMatrix[k_val], cols)\n                    den_sum += q_array[x, k_val]\n\n                q_array[x] = q_array[x] / den_sum\n\n            for k_val in range(k):             #M Step\n                num_total = 0\n                den_total = 0\n                for m in range(trainLength):\n                    num_total += q_array[m, k_val] * scaledData[m]\n                    den_total += q_array[m, k_val]\n\n                mean_arr[k_val] = num_total / den_total\n\n            for k_val in range(k):\n                num_total = 0\n                den_total = 0\n                for m in range(trainLength):\n                    diff_vector = scaledData[m] - mean_arr[k_val]\n                    diff_vector = np.array(diff_vector).reshape((1, cols))\n                    num_total += q_array[m, k_val] * np.dot(diff_vector.T, diff_vector)\n                    den_total += q_array[m, k_val]\n\n                covMatrix[k_val] = num_total / den_total\n                covMatrix[k_val] += np.identity(n=cols)\n\n            for k_val in range(k):\n                num_total = 0\n                for m in range(trainLength):\n                    num_total += q_array[m, k_val]\n\n            lambda_arr[k_val] = num_total / trainLength\n\n            pLogLikelihood = log_like_val\n            log_like_val = logLikelihood(scaledData, mean_arr, covMatrix, lambda_arr, k, cols)\n\n            # Convergence Check\n            if pLogLikelihood >= log_like_val:\n                lossArray.append(log_like_val)\n                meanArray.append(mean_arr)\n                covarianceArray.append(covMatrix)\n                lambdaArray.append(lambda_arr)\n                break\n\n# Mean and variance of GMM objective\nindex = 0                \nwhile index < 5:\n    k = index * 20\n    print(""The mean and Variance of the GMM Objective for k ="",kArray[index],""is"", np.mean(lossArray[k:k+20]), ""and:"", np.var(lossArray[k:k+20]))\n    index += 1\n\n\n# Predict clusters with k = 36\nadjRand = 0\nfms = 0\ntemp_data = np.append(scaledData, np.array(train_features - 1).reshape((trainLength, 1)), axis=1)\nfor i in range(20):\n    predict_array = gmmPred(scaledData, meanArray[60+i], covarianceArray[60+i], lambdaArray[60+i], 36, cols)\n    adjRand += adjusted_rand_score(train_features-1, predict_array)\n    fms += fowlkes_mallows_score(train_features-1, predict_array)\n\nprint(""Adjusted Rand Index of the GMM model with k = 36 is"", adjRand/20)\nprint(""Fowkes Mallows Score of the GMM model with k = 36 is"", fms/20)\n'"
SVM/QuadProg.py,6,"b'import pandas as pd\n\nimport numpy as np\nimport quadprog as quad\n\n ##Quadratic programming Reference as posted in piazza but using quadprog \n #package instead of CVXOPT\n ## https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf\n    \ndata = pd.read_csv(\'mystery.data\',header = None)\nrows = len(data)\nG = np.zeros((rows,16))\n\n\ndef Solve(P, q, G=None, h=None):  #Quad Prog Solving\n    return quad.solve_qp(.5 * P, q, -G.T,  -h, 0)[0]\n\n\n # Feature vector [x0**3, x1**3,x2**3,x3**3,x0 ** 2,x1 ** 2,x2 ** 2,x3 ** 2,x0 * x1,x1 * x2,x2 * x3,x0,x1,x2,x3]\ndef features(train_data, rows):\n    featuredData = np.empty((1000,16), dtype=float)\n    for k in range(rows):\n        x0 = data.iloc[k, 0];\n        x1 = data.iloc[k, 1];\n        x2 = data.iloc[k, 2];\n        x3 = data.iloc[k, 3];\n        y = data.iloc[k, 4]\n        featuredData[k, 0] = x0 ** 3\n        featuredData[k, 1] = x1 ** 3\n        featuredData[k, 2] = x2 ** 3\n        featuredData[k, 3] = x3 ** 3\n        featuredData[k, 4] = x0 ** 2\n        featuredData[k, 5] = x1 ** 2\n        featuredData[k, 6] = x2 ** 2\n        featuredData[k, 7] = x3 ** 2\n        featuredData[k, 8] = x0 * x1\n        featuredData[k, 9] = x1 * x2\n        featuredData[k, 10] = x2 * x3\n        featuredData[k, 11] = x0\n        featuredData[k, 12] = x1\n        featuredData[k, 13] = x2\n        featuredData[k, 14] = x3\n        featuredData[k, 15] = y\n        G[k, 0] = -featuredData[k, 0] * y\n        G[k, 1] = -featuredData[k, 1] * y\n        G[k, 2] = -featuredData[k, 2] * y\n        G[k, 3] = -featuredData[k, 3] * y\n        G[k, 4] = -featuredData[k, 4] * y\n        G[k, 5] = -featuredData[k, 5] * y\n        G[k, 6] = -featuredData[k, 6] * y\n        G[k, 7] = -featuredData[k, 7] * y\n        G[k, 8] = -featuredData[k, 8] * y\n        G[k, 9] = -featuredData[k, 9] * y\n        G[k, 10] = -featuredData[k, 10] * y\n        G[k, 11] = -featuredData[k, 11] * y\n        G[k, 12] = -featuredData[k, 12] * y\n        G[k, 13] = -featuredData[k, 13] * y\n        G[k, 14] = -featuredData[k, 14] * y\n        G[k, 15] = -y\n\n    return featuredData, G\n\ndata, G = features(data, rows)\n#print(data[0,1])\n\nP = np.zeros((16,16))\nfor i in range(16):\n    P[i, i] = 1;\nq = np.zeros((16,1)).reshape((16,))\nh = -np.ones((rows,1)).reshape((len(data),))\n#print(P)\n\n\n   \n\n\nW = Solve(P,q,G,h)   #Quadratic prog solve\n\nprint(""Weight "", W[:15])  #Weight\nprint(""bias "", W[15])   #bias\n\nprint(""Margin "",  (1 / np.sqrt((W[0:15] ** 2).sum())))   #Margin = 1/||W||\n\n\ni = 0\n\n#Test\nsupportVectors = []\npredicted = []\nwhile(i < 1000):\n    j = 0\n    k = 0\n    while(j < 15):\n        k = k + W[j]*data[i, j]\n        j = j+ 1\n    k = k+W[15]\n    if(k > 0 and k <= 1.00000000000002):\n        supportVectors.append(data[i])\n        predicted.append(k)\n    elif(int(k) == -1):\n        predicted.append(k)\n        supportVectors.append(data[i])\n    #print(""predicted "",k,"" actual "", data[i, j])\n    i = i+1\n    \n#print(supportVectors)\n#print(predicted)\nsupportVectorLength = len(supportVectors)\ni = 0\nprint(""\\nThe Support Vectors are "")\nwhile(i < supportVectorLength):\n    print(supportVectors[i],""\\n"")\n    i += 1\n\n\n'"
