file_path,api_count,code
btcjpy/convert.py,0,"b'import csv\nimport re\nfrom create_csv import HEADER\n\nINPUT = \'input.csv\'                                  # input file name\nPATH  = \'input_min.csv\'                              # output file name\nREGEX = \'[\\d]{4}-[\\d]{2}-[\\d]{2} [\\d]{2}:[\\d]{2}:00\' # YYYY-MM-DD hh:mm:00\n\n\nif __name__ == \'__main__\':\n    \'\'\'\n    converting time units of the input data from seconds to minutes.\n    \'\'\'\n\n    # output file is overwritten even if it exists already\n    with open(PATH, \'w\') as f:\n        f.write(HEADER + \'\\n\')\n\n    pattern = re.compile(REGEX)\n    val_per_min = []\n\n    with open(INPUT, newline=\'\') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            if pattern.match(row[0]):                # row[0] equals ""time""\n                val_per_min.append(row)\n\n    with open(PATH, \'a\') as f:\n        writer = csv.writer(f, lineterminator=\'\\n\')\n        writer.writerows(val_per_min)\n\n'"
btcjpy/create_csv.py,0,"b""import json\nimport urllib.request\nfrom datetime import datetime\n\nURL    = 'https://public.bitbank.cc/btc_jpy/ticker'     # Please change this url as you like\nPATH   = 'input.csv'                                    # Output file name\nHEADER = 'time,sell,buy,high,low,last,vol'              # Csv header. After changing above url, you may need to fix this\nFORMAT = '%Y-%m-%d %H:%M:%S'                            # Time column format\n\nreq = urllib.request.Request(URL)\nwritable = True\n\n\ndef write(time, data):\n    header = HEADER.split(',')\n\n    with open(PATH, 'a') as f:\n        f.write(time)\n        for i in range(1, len(header)):\n            f.write(',')\n            f.write(data[header[i]])\n        f.write('\\n')\n\n\nif __name__ == '__main__':\n    '''\n    fetching ticker per 10 seconds\n    '''\n\n    # output file is overwritten even if it exists already\n    with open(PATH, 'w') as f:\n        f.write(HEADER + '\\n')\n\n    # Since consecutive values are required for input data,\n    # this loop stops fetching if some error happens\n    while True:\n        dt = datetime.now()\n        if dt.second % 10 == 0 and writable:\n            writable = False\n            time = dt.strftime(FORMAT)\n            print(time)\n            with urllib.request.urlopen(req) as res:\n                body = json.load(res)\n                data = body['data']                 # After changing above url, you also need to fix this depending on ticker response\n                write(time, data)\n        elif dt.second % 10 == 1 and not writable:\n            writable = True\n\n"""
btcjpy/laplace.py,15,"b'import os\nimport csv\nimport re\nimport joblib\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp.random.seed(0)\ntf.set_random_seed(1234)\n\nINPUT_DATA = \'input_min.csv\'\nLOG_DIR    = \'.tensorboard/logs\'\nMODEL_DIR  = \'.model\'\nCHECKPOINT = \'/model.ckpt\'\nSCALER     = \'/scaler.save\'\n\nMAXLEN           = 41                                     # Time series length of input data\nINTERVAL         = 10                                     # Time interval between the last input value and answer value\nN_IN             = 4                                      # Input dimension\nN_HIDDEN         = 13                                     # Number of hidden layers\nN_OUT            = 4                                      # Output dimension\nLEARNING_RATE    = 0.0015                                 # Optimizer\'s learning rate\nPATIENCE         = 10                                     # Max step of EarlyStopping\nINPUT_VALUE_TYPE = [\'sell\', \'buy\', \'last\', \'vol\']         # Input value type\nEPOCHS           = 1500                                   # Epochs\nBATCH_SIZE       = 50                                     # Batch size\nTESTING_INTERVAL = 10                                     # Test interval\n\nRANDOM_LEARNING_ENABLED = True                            # Index of data determined randomly or not\nEARLY_STOPPING_ENABLED  = False                           # Early Stopping enabled or not\n\n\ndef inference(x, n_in=None, maxlen=None, n_hidden=None, n_out=None):\n    def weight_bariable(shape, name=None):\n        initial = tf.truncated_normal(shape, stddev=0.01)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(shape, name=None):\n        initial = tf.zeros(shape, dtype=tf.float32)\n        return tf.Variable(initial, name=name)\n\n    # In order to adjust to specification of tf.nn.static_bidirectional_rnn,\n    # reshaping format of recurrent data to (batch_size, input_dim)\n    x = tf.transpose(x, [1, 0, 2]) # Tensor: (?, MAXLEN, N_IN) => Tensor: (MAXLEN, ?, N_IN)\n    x = tf.reshape(x, [-1, n_in])  # Tensor: (MAXLEN, ?, N_IN) => Tensor: (?, N_IN)\n    x = tf.split(x, maxlen, 0)     # Tensor: (?, N_IN)         =>   list: len(x): MAXLEN\n\n    cell_forward = layers.LSTMCell(n_hidden, unit_forget_bias=True)\n    cell_backward = layers.LSTMCell(n_hidden, unit_forget_bias=True)\n    outputs, _, _ = tf.nn.static_bidirectional_rnn(cell_forward, cell_backward, x, dtype=tf.float32)\n\n    W = weight_bariable([n_hidden * 2, n_out], name=\'W\')\n    tf.summary.histogram(\'W\', W) # TensorBoard\n    b = bias_variable([n_out], name=\'b\')\n    y = tf.matmul(outputs[-1], W) + b\n    return y\n\n\ndef loss(y, t):\n    with tf.name_scope(\'loss\'):\n        mse = tf.reduce_mean(tf.square(y - t))\n        tf.summary.scalar(\'mse\', mse) # TensorBoard\n        return mse\n\n\ndef training(loss):\n    with tf.name_scope(\'train\'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, beta1=0.9, beta2=0.999)\n        train_step = optimizer.minimize(loss)\n        return train_step\n\n\nclass EarlyStopping():\n    def __init__(self, patience=0, verbose=0):\n        self._step = 0\n        self._loss = float(\'inf\')\n        self.patience = patience\n        self.verbose = verbose\n\n    def validate(self, loss):\n        if self._loss < loss:\n            self._step += 1\n            if self._step > self.patience:\n                if self.verbose:\n                    print(\'early stopping\')\n                return True\n        else:\n            self._step = 0\n            self._loss = loss\n        return False\n\n\ndef get_input_data():\n    # csv check\n    if not os.path.exists(INPUT_DATA):\n        print(\'please prepare input data first:\', INPUT_DATA)\n        exit()\n\n    ticker_data = []\n    with open(INPUT_DATA, newline=\'\') as csvfile:\n\n        # header check\n        with open(INPUT_DATA, newline=\'\') as tmp:\n            header = tmp.readline()\n            if re.match(\'\\D+\', header):\n                print(\'skipping header\')\n                next(csvfile)\n\n        reader = csv.reader(csvfile)\n        for row in reader:\n            del row[0]   # exclude ""time""\n            del row[2:4] # exclude ""high"", ""low""\n            ticker_data.append([float(v) for v in row])\n    return ticker_data\n\n\ndef check_answer(q, a, p, verbose=None):\n    \'\'\'\n    answer checking\n\n    checking correct answer with q(question), p(prediction), and a(answer)\n\n    I. increasing case (a - q[-1] >= 0)\n\n        if p - q[-1] >= 0:\n            correct_counts += 1\n\n    II. decreasing case (a - q[-1] < 0)\n\n        if p - q[-1] < 0:\n            correct_counts += 1\n    \'\'\'\n\n    diff_a_q = a - q[-1]\n    diff_p_q = p - q[-1]\n\n    if verbose:\n        print(\'-\' * 10)\n        print(\'question: \')\n        print(q)\n        print()\n        print(\'prediction:                     \', p)\n        print(\'answer:                         \', a)\n        print()\n        print(\'answer     - question[-1] =     \', diff_a_q)\n        print(\'prediction - question[-1] =     \', diff_p_q)\n        print(\'answer     - prediction   =     \', a - p)\n        print()\n\n    diff_a_q = diff_a_q >= 0\n    diff_p_q = diff_p_q >= 0\n    correct_counts = np.zeros(N_OUT)\n\n    for i in range(len(correct_counts)):\n        if diff_a_q[i]:         # increasing case\n            if diff_p_q[i]:\n                if verbose:\n                    print(INPUT_VALUE_TYPE[i], \'is correct prediction!\')\n                correct_counts[i] += 1\n        else:                   # decreasing case\n            if not diff_p_q[i]:\n                if verbose:\n                    print(INPUT_VALUE_TYPE[i], \'is correct prediction!\')\n                correct_counts[i] += 1\n\n    return correct_counts\n\n\ndef check_accuracy(correct_counts, number_of_tests, epoch):\n    accuracy = correct_counts * 100 / number_of_tests # %\n    if epoch > 0:\n        print()\n        print(\'accuracy per 10 epoch:           {}\'.format(accuracy))\n    return accuracy\n\n\ndef check_accuracy_average(val_acc_ave, acc, length=10):\n    val_acc_ave.insert(0, acc)\n    if len(val_acc_ave) > length:\n        val_acc_ave.pop()\n    if len(val_acc_ave) == length:\n        accuracy_total = np.zeros(N_OUT)\n        for i in range(len(val_acc_ave)):\n            accuracy_total += val_acc_ave[i]\n        accuracy_total /= length\n        print(\'average of the last {} accuracy: {}\'.format(length, accuracy_total))\n\n\ndef predict(arr_f):\n    tf.reset_default_graph()\n\n    # type check\n    if not isinstance(arr_f, np.ndarray):\n        print(\'input data has to be numpy ndarray\')\n        return\n\n    # shape check\n    if arr_f.ndim != 2 or arr_f.shape[0] != MAXLEN or arr_f.shape[1] != N_IN:\n        print(\'input data is numpy array whose shape is (\', MAXLEN, \',\', N_IN, \'),\')\n        print(\'which means values of\', INPUT_VALUE_TYPE, \' x\', MAXLEN, \' rows\')\n        return\n\n    # learning check\n    if not os.path.exists(MODEL_DIR + SCALER):\n        print(\'model is not learned yet. please learn first\')\n        return\n\n    # normalization\n    scaler = joblib.load(MODEL_DIR + SCALER)\n    arr_f = scaler.transform(arr_f)\n\n    \'\'\'\n    model setting\n    \'\'\'\n    x = tf.placeholder(tf.float32, shape=[None, MAXLEN, N_IN])\n    t = tf.placeholder(tf.float32, shape=[None, N_OUT])\n    y = inference(x, n_in=N_IN, maxlen=MAXLEN, n_hidden=N_HIDDEN, n_out=N_OUT)\n\n    \'\'\'\n    prediction\n    \'\'\'\n    # reshaping\n    arr_f = arr_f.reshape(1, MAXLEN, N_IN)\n\n    sess = tf.Session()\n\n    # restoring variables\n    tf.train.Saver().restore(sess, MODEL_DIR + CHECKPOINT)\n\n    prediction = y.eval(session=sess, feed_dict={\n        x: arr_f\n    })\n    np.set_printoptions(suppress=True)\n\n    # restoring values from normalization\n    p = scaler.inverse_transform(prediction)\n\n    return p[0]\n\n\ndef predict_rising_from(arr_f):\n    predicted = predict(arr_f)\n    risingPrediction = (predicted - arr_f[-1]) > 0\n    return risingPrediction\n\n\ndef predict_falling_from(arr_f):\n    risingPrediction = predict_rising_from(arr_f)\n    return np.logical_not(risingPrediction)\n\n\ndef make_input_data():\n    list = get_input_data()\n    tmp_idx = np.random.randint(0, len(list) - MAXLEN + 1)\n    list = list[tmp_idx:tmp_idx + MAXLEN]\n    return np.array(list)\n\n\nif __name__ == \'__main__\':\n    # TensorBoard directory check\n    if not os.path.exists(LOG_DIR):\n        os.makedirs(LOG_DIR)\n\n    # model directory check\n    if not os.path.exists(MODEL_DIR):\n        os.mkdir(MODEL_DIR)\n\n    \'\'\'\n    producing data\n    \'\'\'\n    f = get_input_data() # input data list\n\n    # normalization\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    arr_f = scaler.fit_transform(np.array(f))\n    joblib.dump(scaler, MODEL_DIR + SCALER)\n\n    data = []\n    target = []\n\n    # + 1 is caused by exclusiveness of stop value of range\n    for i in range(0, len(f) - MAXLEN - INTERVAL + 1):\n        data.append(arr_f[i: i + MAXLEN])\n        target.append(arr_f[i + MAXLEN + INTERVAL - 1])\n\n    X = np.array(data).reshape(len(data), MAXLEN, N_IN)\n    Y = np.array(target).reshape(len(data), N_OUT)\n    N_train = int(len(data) * 0.9)\n    N_validation = len(data) - N_train\n    X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=N_validation)\n\n    \'\'\'\n    model setting\n    \'\'\'\n    x = tf.placeholder(tf.float32, shape=[None, MAXLEN, N_IN])\n    t = tf.placeholder(tf.float32, shape=[None, N_OUT])\n    y = inference(x, n_in=N_IN, maxlen=MAXLEN, n_hidden=N_HIDDEN, n_out=N_OUT)\n    loss = loss(y, t)\n    train_step = training(loss)\n    if EARLY_STOPPING_ENABLED:\n        early_stopping = EarlyStopping(patience=PATIENCE, verbose=1)\n\n    \'\'\'\n    learning\n    \'\'\'\n    epochs = EPOCHS\n    batch_size = BATCH_SIZE\n    np.set_printoptions(suppress=True)\n\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver() # for saving model\n    sess = tf.Session()\n\n    # TensorBoard\n    file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n    summaries = tf.summary.merge_all()\n\n    sess.run(init)\n\n    n_batches = N_train // batch_size\n    val_acc_ave = []\n    number_of_tests = N_validation * 10 // epochs\n    validation_indices = [v for v in range(N_validation)]\n\n    for epoch in range(epochs):\n\n        for i in range(n_batches):\n            if RANDOM_LEARNING_ENABLED:\n                i = np.random.randint(0, n_batches)\n            start = i * batch_size\n            end = start + batch_size\n            sess.run(train_step, feed_dict={\n                x: X_train[start:end],\n                t: Y_train[start:end]\n            })\n\n        # TensorBoard\n        summary, val_loss = sess.run([summaries, loss], feed_dict={\n            x: X_validation,\n            t: Y_validation\n        })\n        file_writer.add_summary(summary, epoch)\n\n        if EARLY_STOPPING_ENABLED:\n            if early_stopping.validate(val_loss):\n                break\n\n        \'\'\'\n        testing\n        \'\'\'\n        if epoch % TESTING_INTERVAL == 0:\n            print(\'-\' * 10)\n            correct_counts = np.zeros(N_OUT)\n\n            for i in range(number_of_tests):\n                random_idx = np.random.randint(0, len(validation_indices))\n                index = validation_indices.pop(random_idx)\n\n                question = X_validation\n                answer = Y_validation\n                prediction = y.eval(session=sess, feed_dict={\n                    x: question\n                })\n\n                # restoring values from normalization\n                q = scaler.inverse_transform(question[index])\n                p = scaler.inverse_transform(prediction)[index]\n                a = scaler.inverse_transform(answer)[index]\n\n                correct_counts += check_answer(q=q, a=a, p=p, verbose=1)\n\n            accuracy = check_accuracy(correct_counts, number_of_tests, epoch)\n            check_accuracy_average(val_acc_ave, accuracy)\n\n        print()\n        print(\'epoch:\', epoch, \', validation loss:\', val_loss)\n\n    # model saving\n    model_path = saver.save(sess, MODEL_DIR + CHECKPOINT)\n    print(\'Model saved to:\', model_path)\n\n'"
btcusd/convert.py,0,"b'import csv\nimport re\nfrom create_csv import HEADER\n\nINPUT = \'input.csv\'                                  # input file name\nPATH  = \'input_min.csv\'                              # output file name\nREGEX = \'[\\d]{4}-[\\d]{2}-[\\d]{2} [\\d]{2}:[\\d]{2}:00\' # YYYY-MM-DD hh:mm:00\n\n\nif __name__ == \'__main__\':\n    \'\'\'\n    converting time units of the input data from seconds to minutes.\n    \'\'\'\n\n    # output file is overwritten even if it exists already\n    with open(PATH, \'w\') as f:\n        f.write(HEADER + \'\\n\')\n\n    pattern = re.compile(REGEX)\n    val_per_min = []\n\n    with open(INPUT, newline=\'\') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            if pattern.match(row[0]):                # row[0] equals ""time""\n                val_per_min.append(row)\n\n    with open(PATH, \'a\') as f:\n        writer = csv.writer(f, lineterminator=\'\\n\')\n        writer.writerows(val_per_min)\n\n'"
btcusd/create_csv.py,0,"b""import json\nimport urllib.request\nfrom datetime import datetime\n\nURL    = 'https://api.bitfinex.com/v1/pubticker/btcusd' # Please change this url as you like\nPATH   = 'input.csv'                                    # Output file name\nHEADER = 'time,bid,ask,last_price,volume'               # Csv header. After changing above url, you may need to fix this\nFORMAT = '%Y-%m-%d %H:%M:%S'                            # Time column format\n\nreq = urllib.request.Request(URL)\nwritable = True\n\n\ndef write(time, data):\n    header = HEADER.split(',')\n\n    with open(PATH, 'a') as f:\n        f.write(time)\n        for i in range(1, len(header)):\n            f.write(',')\n            f.write(data[header[i]])\n        f.write('\\n')\n\n\nif __name__ == '__main__':\n    '''\n    fetching ticker per 10 seconds\n    '''\n\n    # output file is overwritten even if it exists already\n    with open(PATH, 'w') as f:\n        f.write(HEADER + '\\n')\n\n    # Since consecutive values are required for input data,\n    # this loop stops fetching if some error happens\n    while True:\n        dt = datetime.now()\n        if dt.second % 10 == 0 and writable:\n            writable = False\n            time = dt.strftime(FORMAT)\n            print(time)\n            with urllib.request.urlopen(req) as res:\n                body = json.load(res)\n                write(time, body)                       # After changing above url, you also need to fix this depending on ticker response\n        elif dt.second % 10 == 1 and not writable:\n            writable = True\n\n"""
btcusd/laplace.py,15,"b'import os\nimport csv\nimport re\nimport joblib\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nnp.random.seed(0)\ntf.set_random_seed(1234)\n\nINPUT_DATA = \'input_min.csv\'\nLOG_DIR    = \'.tensorboard/logs\'\nMODEL_DIR  = \'.model\'\nCHECKPOINT = \'/model.ckpt\'\nSCALER     = \'/scaler.save\'\n\nMAXLEN           = 41                                     # Time series length of input data\nINTERVAL         = 10                                     # Time interval between the last input value and answer value\nN_IN             = 4                                      # Input dimension\nN_HIDDEN         = 13                                     # Number of hidden layers\nN_OUT            = 4                                      # Output dimension\nLEARNING_RATE    = 0.0015                                 # Optimizer\'s learning rate\nPATIENCE         = 10                                     # Max step of EarlyStopping\nINPUT_VALUE_TYPE = [\'bid\', \'ask\', \'last_price\', \'volume\'] # Input value type\nEPOCHS           = 2500                                   # Epochs\nBATCH_SIZE       = 50                                     # Batch size\nTESTING_INTERVAL = 10                                     # Test interval\n\nRANDOM_LEARNING_ENABLED = True                            # Index of data determined randomly or not\nEARLY_STOPPING_ENABLED  = False                           # Early Stopping enabled or not\n\ndef inference(x, n_in=None, maxlen=None, n_hidden=None, n_out=None):\n    def weight_bariable(shape, name=None):\n        initial = tf.truncated_normal(shape, stddev=0.01)\n        return tf.Variable(initial, name=name)\n\n    def bias_variable(shape, name=None):\n        initial = tf.zeros(shape, dtype=tf.float32)\n        return tf.Variable(initial, name=name)\n\n    # In order to adjust to specification of tf.nn.static_bidirectional_rnn,\n    # reshaping format of recurrent data to (batch_size, input_dim)\n    x = tf.transpose(x, [1, 0, 2]) # Tensor: (?, MAXLEN, N_IN) => Tensor: (MAXLEN, ?, N_IN)\n    x = tf.reshape(x, [-1, n_in])  # Tensor: (MAXLEN, ?, N_IN) => Tensor: (?, N_IN)\n    x = tf.split(x, maxlen, 0)     # Tensor: (?, N_IN)         =>   list: len(x): MAXLEN\n\n    cell_forward = layers.LSTMCell(n_hidden, unit_forget_bias=True)\n    cell_backward = layers.LSTMCell(n_hidden, unit_forget_bias=True)\n    outputs, _, _ = tf.nn.static_bidirectional_rnn(cell_forward, cell_backward, x, dtype=tf.float32)\n\n    W = weight_bariable([n_hidden * 2, n_out], name=\'W\')\n    tf.summary.histogram(\'W\', W) # TensorBoard\n    b = bias_variable([n_out], name=\'b\')\n    y = tf.matmul(outputs[-1], W) + b\n    return y\n\n\ndef loss(y, t):\n    with tf.name_scope(\'loss\'):\n        mse = tf.reduce_mean(tf.square(y - t))\n        tf.summary.scalar(\'mse\', mse) # TensorBoard\n        return mse\n\n\ndef training(loss):\n    with tf.name_scope(\'train\'):\n        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, beta1=0.9, beta2=0.999)\n        train_step = optimizer.minimize(loss)\n        return train_step\n\n\nclass EarlyStopping():\n    def __init__(self, patience=0, verbose=0):\n        self._step = 0\n        self._loss = float(\'inf\')\n        self.patience = patience\n        self.verbose = verbose\n\n    def validate(self, loss):\n        if self._loss < loss:\n            self._step += 1\n            if self._step > self.patience:\n                if self.verbose:\n                    print(\'early stopping\')\n                return True\n        else:\n            self._step = 0\n            self._loss = loss\n        return False\n\n\ndef get_input_data():\n    # csv check\n    if not os.path.exists(INPUT_DATA):\n        print(\'please prepare input data first:\', INPUT_DATA)\n        exit()\n\n    ticker_data = []\n    with open(INPUT_DATA, newline=\'\') as csvfile:\n\n        # header check\n        with open(INPUT_DATA, newline=\'\') as tmp:\n            header = tmp.readline()\n            if re.match(\'\\D+\', header):\n                print(\'skipping header\')\n                next(csvfile)\n\n        reader = csv.reader(csvfile)\n        for row in reader:\n            del row[0]   # exclude ""time""\n            ticker_data.append([float(v) for v in row])\n    return ticker_data\n\n\ndef check_answer(q, a, p, verbose=None):\n    \'\'\'\n    answer checking\n\n    checking correct answer with q(question), p(prediction), and a(answer)\n\n    I. increasing case (a - q[-1] >= 0)\n\n        if p - q[-1] >= 0:\n            correct_counts += 1\n\n    II. decreasing case (a - q[-1] < 0)\n\n        if p - q[-1] < 0:\n            correct_counts += 1\n    \'\'\'\n\n    diff_a_q = a - q[-1]\n    diff_p_q = p - q[-1]\n\n    if verbose:\n        print(\'-\' * 10)\n        print(\'question: \')\n        print(q)\n        print()\n        print(\'prediction:                     \', p)\n        print(\'answer:                         \', a)\n        print()\n        print(\'answer     - question[-1] =     \', diff_a_q)\n        print(\'prediction - question[-1] =     \', diff_p_q)\n        print(\'answer     - prediction   =     \', a - p)\n        print()\n\n    diff_a_q = diff_a_q >= 0\n    diff_p_q = diff_p_q >= 0\n    correct_counts = np.zeros(N_OUT)\n\n    for i in range(len(correct_counts)):\n        if diff_a_q[i]:         # increasing case\n            if diff_p_q[i]:\n                if verbose:\n                    print(INPUT_VALUE_TYPE[i], \'is correct prediction!\')\n                correct_counts[i] += 1\n        else:                   # decreasing case\n            if not diff_p_q[i]:\n                if verbose:\n                    print(INPUT_VALUE_TYPE[i], \'is correct prediction!\')\n                correct_counts[i] += 1\n\n    return correct_counts\n\n\ndef check_accuracy(correct_counts, number_of_tests, epoch):\n    accuracy = correct_counts * 100 / number_of_tests # %\n    if epoch > 0:\n        print()\n        print(\'accuracy per 10 epoch:           {}\'.format(accuracy))\n    return accuracy\n\n\ndef check_accuracy_average(val_acc_ave, acc, length=10):\n    val_acc_ave.insert(0, acc)\n    if len(val_acc_ave) > length:\n        val_acc_ave.pop()\n    if len(val_acc_ave) == length:\n        accuracy_total = np.zeros(N_OUT)\n        for i in range(len(val_acc_ave)):\n            accuracy_total += val_acc_ave[i]\n        accuracy_total /= length\n        print(\'average of the last {} accuracy: {}\'.format(length, accuracy_total))\n\n\ndef predict(arr_f):\n    tf.reset_default_graph()\n\n    # type check\n    if not isinstance(arr_f, np.ndarray):\n        print(\'input data has to be numpy ndarray\')\n        return\n\n    # shape check\n    if arr_f.ndim != 2 or arr_f.shape[0] != MAXLEN or arr_f.shape[1] != N_IN:\n        print(\'input data is numpy array whose shape is (\', MAXLEN, \',\', N_IN, \'),\')\n        print(\'which means values of\', INPUT_VALUE_TYPE, \' x\', MAXLEN, \' rows\')\n        return\n\n    # learning check\n    if not os.path.exists(MODEL_DIR + SCALER):\n        print(\'model is not learned yet. please learn first\')\n        return\n\n    # normalization\n    scaler = joblib.load(MODEL_DIR + SCALER)\n    arr_f = scaler.transform(arr_f)\n\n    \'\'\'\n    model setting\n    \'\'\'\n    x = tf.placeholder(tf.float32, shape=[None, MAXLEN, N_IN])\n    t = tf.placeholder(tf.float32, shape=[None, N_OUT])\n    y = inference(x, n_in=N_IN, maxlen=MAXLEN, n_hidden=N_HIDDEN, n_out=N_OUT)\n\n    \'\'\'\n    prediction\n    \'\'\'\n    # reshaping\n    arr_f = arr_f.reshape(1, MAXLEN, N_IN)\n\n    sess = tf.Session()\n\n    # restoring variables\n    tf.train.Saver().restore(sess, MODEL_DIR + CHECKPOINT)\n\n    prediction = y.eval(session=sess, feed_dict={\n        x: arr_f\n    })\n    np.set_printoptions(suppress=True)\n\n    # restoring values from normalization\n    p = scaler.inverse_transform(prediction)\n\n    return p[0]\n\n\ndef predict_rising_from(arr_f):\n    predicted = predict(arr_f)\n    risingPrediction = (predicted - arr_f[-1]) > 0\n    return risingPrediction\n\n\ndef predict_falling_from(arr_f):\n    risingPrediction = predict_rising_from(arr_f)\n    return np.logical_not(risingPrediction)\n\n\ndef make_input_data():\n    list = get_input_data()\n    tmp_idx = np.random.randint(0, len(list) - MAXLEN + 1)\n    list = list[tmp_idx:tmp_idx + MAXLEN]\n    return np.array(list)\n\n\nif __name__ == \'__main__\':\n    # TensorBoard directory check\n    if not os.path.exists(LOG_DIR):\n        os.makedirs(LOG_DIR)\n\n    # model directory check\n    if not os.path.exists(MODEL_DIR):\n        os.mkdir(MODEL_DIR)\n\n    \'\'\'\n    producing data\n    \'\'\'\n    f = get_input_data() # input data list\n\n    # normalization\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    arr_f = scaler.fit_transform(np.array(f))\n    joblib.dump(scaler, MODEL_DIR + SCALER)\n\n    data = []\n    target = []\n\n    # + 1 is caused by exclusiveness of stop value of range\n    for i in range(0, len(f) - MAXLEN - INTERVAL + 1):\n        data.append(arr_f[i: i + MAXLEN])\n        target.append(arr_f[i + MAXLEN + INTERVAL - 1])\n\n    X = np.array(data).reshape(len(data), MAXLEN, N_IN)\n    Y = np.array(target).reshape(len(data), N_OUT)\n    N_train = int(len(data) * 0.9)\n    N_validation = len(data) - N_train\n    X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=N_validation)\n\n    \'\'\'\n    model setting\n    \'\'\'\n    x = tf.placeholder(tf.float32, shape=[None, MAXLEN, N_IN])\n    t = tf.placeholder(tf.float32, shape=[None, N_OUT])\n    y = inference(x, n_in=N_IN, maxlen=MAXLEN, n_hidden=N_HIDDEN, n_out=N_OUT)\n    loss = loss(y, t)\n    train_step = training(loss)\n    if EARLY_STOPPING_ENABLED:\n        early_stopping = EarlyStopping(patience=PATIENCE, verbose=1)\n\n    \'\'\'\n    learning\n    \'\'\'\n    epochs = EPOCHS\n    batch_size = BATCH_SIZE\n    np.set_printoptions(suppress=True)\n\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver() # for saving model\n    sess = tf.Session()\n\n    # TensorBoard\n    file_writer = tf.summary.FileWriter(LOG_DIR, sess.graph)\n    summaries = tf.summary.merge_all()\n\n    sess.run(init)\n\n    n_batches = N_train // batch_size\n    val_acc_ave = []\n    number_of_tests = N_validation * 10 // epochs\n    validation_indices = [v for v in range(N_validation)]\n\n    for epoch in range(epochs):\n\n        for i in range(n_batches):\n            if RANDOM_LEARNING_ENABLED:\n                i = np.random.randint(0, n_batches)\n            start = i * batch_size\n            end = start + batch_size\n            sess.run(train_step, feed_dict={\n                x: X_train[start:end],\n                t: Y_train[start:end]\n            })\n\n        # TensorBoard\n        summary, val_loss = sess.run([summaries, loss], feed_dict={\n            x: X_validation,\n            t: Y_validation\n        })\n        file_writer.add_summary(summary, epoch)\n\n        if EARLY_STOPPING_ENABLED:\n            if early_stopping.validate(val_loss):\n                break\n\n        \'\'\'\n        testing\n        \'\'\'\n        if epoch % TESTING_INTERVAL == 0:\n            print(\'-\' * 10)\n            correct_counts = np.zeros(N_OUT)\n\n            for i in range(number_of_tests):\n                random_idx = np.random.randint(0, len(validation_indices))\n                index = validation_indices.pop(random_idx)\n\n                question = X_validation\n                answer = Y_validation\n                prediction = y.eval(session=sess, feed_dict={\n                    x: question\n                })\n\n                # restoring values from normalization\n                q = scaler.inverse_transform(question[index])\n                p = scaler.inverse_transform(prediction)[index]\n                a = scaler.inverse_transform(answer)[index]\n\n                correct_counts += check_answer(q=q, a=a, p=p, verbose=1)\n\n            accuracy = check_accuracy(correct_counts, number_of_tests, epoch)\n            check_accuracy_average(val_acc_ave, accuracy)\n\n        print()\n        print(\'epoch:\', epoch, \', validation loss:\', val_loss)\n\n    # model saving\n    model_path = saver.save(sess, MODEL_DIR + CHECKPOINT)\n    print(\'Model saved to:\', model_path)\n\n'"
