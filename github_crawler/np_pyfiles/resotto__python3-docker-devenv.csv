file_path,api_count,code
gru_sin_tensorflow.py,8,"b""import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nnp.random.seed(0)\ntf.set_random_seed(1234)\n\ndef inference(x, n_batch, maxlen=None, n_hidden=None, n_out=None):\n    def weight_variable(shape):\n        initial = tf.truncated_normal(shape, stddev=0.01)\n        return tf.Variable(initial)\n\n    def bias_variable(shape):\n        initial = tf.zeros(shape, dtype=tf.float32)\n        return tf.Variable(initial)\n\n    cell = tf.nn.rnn_cell.GRUCell(n_hidden)\n    initial_state = cell.zero_state(n_batch, tf.float32) # zero_state() returns zero-filled state tensor(s)\n\n    state = initial_state\n    outputs = [] # save previous output of hidden layer\n    with tf.variable_scope('GRU'): # define 'GRU' as scope name in order to distinguish variables on TensorFlow\n        for t in range(maxlen):\n            if t > 0:\n                # get_variable_scope() returns the current variable scope\n                # reuse_variables() enables us to create a scope with the same name\n                tf.get_variable_scope().reuse_variables()\n            (cell_output, state) = cell(x[:, t, :], state) # output calculation per time t\n            outputs.append(cell_output)\n\n    output = outputs[-1]\n\n    V = weight_variable([n_hidden, n_out])\n    c = bias_variable([n_out])\n    y = tf.matmul(output, V) + c # linear activation\n\n    return y\n\ndef loss(y, t):\n    mse = tf.reduce_mean(tf.square(y - t)) # squared error function\n    return mse\n\ndef training(loss):\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999)\n    train_step = optimizer.minimize(loss)\n    return train_step\n\n\nclass EarlyStopping():\n    def __init__(self, patience=0, verbose=0):\n        self._step = 0\n        self._loss = float('inf')\n        self.patience = patience\n        self.verbose = verbose\n\n    def validate(self, loss):\n        if self._loss < loss:\n            self._step += 1\n            if self._step > self.patience:\n                if self.verbose:\n                    print('early stopping')\n                return True\n        else:\n            self._step = 0\n            self._loss = loss\n\n        return False\n\n\nif __name__ == '__main__':\n    def sin(x, T=100):\n        return np.sin(2.0 * np.pi * x / T)\n\n    def toy_problem(T=100, ampl=0.05):\n        x = np.arange(0, 2 * T + 1)\n        noise = ampl * np.random.uniform(low=-1.0, high=1.0, size=len(x))\n        return sin(x) + noise\n\n\n    '''\n    producing data\n    '''\n    T = 100\n    f = toy_problem(T)\n\n    length_of_sequences = 2 * T\n    maxlen = 25\n\n    data = [] # group of recurrent data whose length is maxlen used by prediction\n    target = [] # group of desired outputs to be obtained by prediction\n\n    for i in range(0, length_of_sequences - maxlen + 1):\n        data.append(f[i: i + maxlen])\n        target.append(f[i + maxlen])\n\n    X = np.array(data).reshape(len(data), maxlen, 1)\n    Y = np.array(target).reshape(len(data), 1)\n\n    # data setting\n    N_train = int(len(data) * 0.9)\n    N_validation = len(data) - N_train\n\n    X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=N_validation)\n\n\n    '''\n    model setting\n    '''\n    n_in = len(X[0][0]) # 1\n    n_hidden = 30\n    n_out = len(Y[0]) # 1\n\n    x = tf.placeholder(tf.float32, shape=[None, maxlen, n_in])\n    t = tf.placeholder(tf.float32, shape=[None, n_out])\n    n_batch = tf.placeholder(tf.int32, shape=[])\n\n    y = inference(x, n_batch, maxlen=maxlen, n_hidden=n_hidden, n_out=n_out)\n    loss = loss(y, t)\n    train_step = training(loss)\n\n    early_stopping = EarlyStopping(patience=10, verbose=1)\n    history = {\n        'val_loss': []\n    }\n\n\n    '''\n    learning\n    '''\n    epochs = 500\n    batch_size = 10\n\n    init = tf.global_variables_initializer()\n    sess = tf.Session()\n    sess.run(init)\n\n    n_batches = N_train // batch_size\n\n    for epoch in range(epochs):\n        X_, Y_ = shuffle(X_train, Y_train)\n\n        for i in range(n_batches):\n            start = i * batch_size\n            end = start + batch_size\n\n            sess.run(train_step, feed_dict={\n                x: X_[start:end],\n                t: Y_[start:end],\n                n_batch: batch_size\n            })\n\n        # evaluation with validation data\n        val_loss = loss.eval(session=sess, feed_dict={\n            x: X_validation,\n            t: Y_validation,\n            n_batch: N_validation\n        })\n\n        history['val_loss'].append(val_loss)\n        print('epoch:', epoch, ' validation loss:', val_loss)\n\n        # Early Stopping\n        if early_stopping.validate(val_loss):\n            break\n\n\n    '''\n    prediction based on predicated values\n    '''\n    Z = X[:1] # cut off the first part of the original data\n\n    original = [f[i] for i in range(maxlen)]\n    predicted_vals = [None for i in range(maxlen)]\n\n    # reuse the last predicated value for the next input\n    for i in range(length_of_sequences - maxlen + 1):\n        # predict from the last recurrent data\n        z_ = Z[-1:]\n        y_ = y.eval(session=sess, feed_dict={\n            x: Z[-1:],\n            n_batch: 1\n        })\n        # produce new recurrent data with prediction result\n        sequence_ = np.concatenate((z_.reshape(maxlen, n_in)[1:], y_), axis=0).reshape(1, maxlen, n_in)\n        Z = np.append(Z, sequence_, axis=0)\n        predicted_vals.append(y_.reshape(-1))\n\n\n    '''\n    visualization\n    '''\n    plt.figure()\n    plt.ylim([-1.5, 1.5])\n    plt.plot(toy_problem(T, ampl=0), linestyle='dotted', color='#aaaaaa', label='true_distribution')\n    plt.plot(original, linestyle='dashed', color='black', label='toy_problem')\n    plt.plot(predicted_vals, color='black', label='predicted')\n    plt.ylabel('sin(t)')\n    plt.xlabel('t')\n    plt.legend()\n    plt.show()\n\n"""
hello.py,0,"b""print('Hello python3-docker-devenv!')\n"""
