file_path,api_count,code
live_detection.py,2,"b'import numpy as np\r\nimport argparse\r\nimport time\r\nimport cv2\r\n\r\nobjectDetected = \'item\'\r\nwith open(\'yolov3.txt\', \'r\') as f:\r\n    classes = [line.strip() for line in f.readlines()]\r\n\r\nCOLORS = np.random.uniform(0, 255, size=(len(classes), 3))\r\n\r\ndef distance_to_camera(knownWidth, focalLength, perWidth):\r\n    return (knownWidth * focalLength) / perWidth\r\n\r\ndef get_output_layers(net):\r\n    layer_names = net.getLayerNames()\r\n    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\r\n    return output_layers\r\n\r\ndef draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\r\n    label = str(classes[class_id])\r\n    color = COLORS[class_id]\r\n    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\r\n    cv2.putText(img, label+\'(\'+str(int(confidence*100))+\'%)\', (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\r\n\r\nnet = cv2.dnn.readNet(\'yolov3.weights\', \'yolov3.cfg\')\r\nprint(""[INFO] starting video stream..."")\r\nvs = cv2.VideoCapture(0)\r\nstart = time.time()\r\n\r\nframe_count = 0.0\r\n\r\nKNOWN_DISTANCE = 24.0\r\nKNOWN_WIDTH = 11.0\r\n\r\nwhile True:\r\n        ret, frame = vs.read()\r\n        cv2.resize(frame, (600, frame.shape[0]))\r\n\r\n        scale = 0.00392\r\n        (height, width) = frame.shape[:2]\r\n        blob = cv2.dnn.blobFromImage(frame, scale, (416, 416), (0, 0, 0), True, crop=False)\r\n        net.setInput(blob)\r\n        outs = net.forward(get_output_layers(net))\r\n        class_ids = []\r\n        confidences = []\r\n        boxes = []\r\n        conf_threshold = 0.5\r\n        nms_threshold = 0.4\r\n        for out in outs:\r\n            for detection in out:\r\n                scores = detection[5:]\r\n                class_id = np.argmax(scores)\r\n                confidence = scores[class_id]\r\n                if confidence > 0.5:\r\n                    center_x = int(detection[0] * width)\r\n                    center_y = int(detection[1] * height)\r\n                    w = int(detection[2] * width)\r\n                    h = int(detection[3] * height)\r\n                    x = center_x - w / 2\r\n                    y = center_y - h / 2\r\n                    class_ids.append(class_id)\r\n                    confidences.append(float(confidence))\r\n                    boxes.append([x, y, w, h])\r\n\r\n        indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\r\n\r\n        for i in indices:\r\n            i = i[0]\r\n            box = boxes[i]\r\n            x = box[0]\r\n            y = box[1]\r\n            w = box[2]\r\n            h = box[3]\r\n            draw_prediction(frame, class_ids[i], confidences[i], round(x), round(y), round(x + w), round(y + h))\r\n\r\n        frame_count += 1\r\n\r\n        cv2.imshow(""Frame"", frame)\r\n        key = cv2.waitKey(1) & 0xFF\r\n        #if the `q` key was pressed, break from the loop\r\n        if key == ord(""q""):\r\n             break\r\n\r\nend = time.time()\r\n\r\ntime_elapsed = end - start\r\n\r\nprint(""[INFO] elapsed time: {:.2f}"".format(time_elapsed))\r\nprint(""[INFO] approx. FPS: {:.2f}"".format(frame_count / time_elapsed))\r\nvs.release()\r\ncv2.destroyAllWindows()\r\n'"
gait/core/main.py,0,"b'from keras.layers import (\n    Convolution2D,\n    MaxPooling2D,\n    Flatten,\n    Dense\n)\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.models import model_from_json\nimport simplejson as sj\n\ndef create_model():\n    model = Sequential()\n    model.add(Convolution2D(4, (3, 3), input_shape=(240, 320, 3), activation=\'relu\'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Flatten())\n    model.add(Dense(output_dim=560, activation=\'relu\'))\n    model.add(Dense(output_dim=560, activation=\'relu\'))\n    model.add(Dense(output_dim=560, activation=\'relu\'))\n    model.add(Dense(output_dim=560, activation=\'relu\'))\n    model.add(Dense(output_dim=1, activation=\'sigmoid\'))\n    return model\n\ndef train(model, training_set, test_set):\n    model.compile(optimizer=\'adam\', loss=\'binary_crossentropy\', metrics=[\'accuracy\'])\n    model.fit_generator(\n            training_set,\n            steps_per_epoch=250,\n            epochs=25,\n            verbose=1,\n            validation_data=test_set,\n            validation_steps=62.5\n    )\n\ndef save_model(model):\n    print(""Saving..."")\n    model.save_weights(""model.h5"")\n    print("" [*] Weights"")\n    open(""model.json"", ""w"").write(\n            sj.dumps(sj.loads(model.to_json()), indent=4)\n    )\n    print("" [*] Model"")\n\ndef load_model():\n    print(""Loading..."")\n    json_file = open(""model.json"", ""r"")\n    model = model_from_json(json_file.read())\n    print("" [*] Model"")\n    model.load_weights(""model.h5"")\n    print("" [*] Weights"")\n    json_file.close()\n    return model\n\ndef dataset_provider(datagen):\n    return datagen.flow_from_directory(\n        \'imagesrc\',\n        target_size=(240, 320),\n        batch_size=32,\n        class_mode=\'binary\'\n    )\n\n# Primary datagen\ntrain_datagen = ImageDataGenerator(\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n# Validation datagen\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\n# Primary Set for training\ntraining_set = dataset_provider(train_datagen)\n# Secondary / Test set for validation\ntest_set = dataset_provider(test_datagen)\n\nmodel = create_model()\ntrain(model, training_set, test_set)\nsave_model(model)'"
gait/core/model_data.py,0,"b'from keras.models import model_from_json\n\n# load json and create model\njson_file = open(\'model.json\', \'r\')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(""model.h5"")\nprint(""Loaded model from disk"")\n \n# evaluate loaded model on test data\nloaded_model.compile(loss=\'binary_crossentropy\', optimizer=\'rmsprop\', metrics=[\'accuracy\'])\nscore = loaded_model.evaluate(X, Y, verbose=0)\nprint(""%s: %.2f%%"" % (loaded_model.metrics_names[1], score[1]*100))\n\n'"
opencvutils/detection/detection.py,0,"b'from time import sleep\nimport time\nimport logging as log\nimport argparse\nimport cv2\nimport datetime as dt\nimport datetime\n\nclass MotionDetection:\n\n    def __init__(self):\n        print(""Motion Detection Ready to Run!"")\n\n    def run(self):\n        ap = argparse.ArgumentParser()\n        ap.add_argument(""-v"", ""--video"", help=""path to the video file"")\n        ap.add_argument(""-a"", ""--min-area"", type=int, default=500, help=""minimum area size"")\n        args = vars(ap.parse_args())\n\n        # if the video argument is None, then we are reading from webcam\n        if args.get(""video"", None) is None:\n            vs = cv2.VideoCapture(0)\n            time.sleep(2.0)\n\n        # otherwise, we are reading from a video file\n        else:\n            vs = cv2.VideoCapture(args[""video""])\n\n        # initialize the first frame in the video stream\n        firstFrame = None\n\n        # loop over the frames of the video\n        while True:\n            # grab the current frame and initialize the occupied/unoccupied\n            # text\n            ret, frame = vs.read()\n            text = ""Unoccupied""\n\n            # if the frame could not be grabbed, then we have reached the end\n            # of the video\n            if frame is None:\n                break\n\n            # resize the frame, convert it to grayscale, and blur it\n            frame = cv2.resize(frame, (500, frame.shape[0]))\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            gray = cv2.GaussianBlur(gray, (21, 21), 0)\n\n            # if the first frame is None, initialize it\n            if firstFrame is None:\n                firstFrame = gray\n                continue\n\n            # compute the absolute difference between the current frame and\n            # first frame\n            frameDelta = cv2.absdiff(firstFrame, gray)\n            thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\n\n            # dilate the thresholded image to fill in holes, then find contours\n            # on thresholded image\n            thresh = cv2.dilate(thresh, None, iterations=2)\n            cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n                                    cv2.CHAIN_APPROX_SIMPLE)\n            cnts = cnts[0]\n\n            # loop over the contours\n            for c in cnts:\n                # if the contour is too small, ignore it\n                if cv2.contourArea(c) < args[""min_area""]:\n                    continue\n\n                # compute the bounding box for the contour, draw it on the frame,\n                # and update the text\n                (x, y, w, h) = cv2.boundingRect(c)\n                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n                # TODO Occupied by whom? Using GAIT, passing the video argument to gait\n                text = ""Occupied""\n\n            # draw the text and timestamp on the frame\n            cv2.putText(frame, ""Room Status: {}"".format(text), (10, 20),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n            cv2.putText(frame, datetime.datetime.now().strftime(""%A %d %B %Y %I:%M:%S%p""),\n                        (10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n\n            # show the frame and record if the user presses a key\n            cv2.imshow(""Gait Recognition"", frame)\n            cv2.imshow(""Thresh"", thresh)\n            cv2.imshow(""Frame Delta"", frameDelta)\n            key = cv2.waitKey(1) & 0xFF\n\n            # if the `q` key is pressed, break from the lop\n            if key == ord(""q""):\n                break\n\n        # cleanup the camera and close any open windows\n        vs.release()\n        cv2.destroyAllWindows()\n\n\nclass PedestrianDetection:\n\n    def __init__(self):\n        print(""Pedestrian Detection Ready to Run"")\n\n    def run(self):\n        hog = cv2.HOGDescriptor()\n        hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n        cap = cv2.VideoCapture(""/path/to/test/video"")\n        while True:\n            r, frame = cap.read()\n            if r:\n                start_time = time.time()\n                frame = cv2.resize(frame, (1280, 720))  # Downscale to improve frame rate\n                gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # HOG needs a grayscale image\n\n                rects, weights = hog.detectMultiScale(gray_frame)\n\n                # Measure elapsed time for detections\n                end_time = time.time()\n                print(""Elapsed time:"", end_time - start_time)\n\n                for i, (x, y, w, h) in enumerate(rects):\n                    if weights[i] < 0.7:\n                        continue\n                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n                cv2.imshow(""preview"", frame)\n            k = cv2.waitKey(1)\n            if k & 0xFF == ord(""q""):  # Exit condition\n                break\n\n\nclass FaceDetection:\n\n    def __init__(self):\n        print(""Face Detection Ready to run!"")\n\n    def run(self):\n        cascPath = ""haarcascade_frontalface_default.xml""\n        faceCascade = cv2.CascadeClassifier(cascPath)\n        log.basicConfig(filename=\'webcam.log\', level=log.INFO)\n\n        video_capture = cv2.VideoCapture(0)\n        anterior = 0\n\n        while True:\n            if not video_capture.isOpened():\n                print(\'Unable to load camera.\')\n                sleep(5)\n                pass\n\n            # Capture frame-by-frame\n            ret, frame = video_capture.read()\n\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n            faces = faceCascade.detectMultiScale(\n                gray,\n                scaleFactor=1.1,\n                minNeighbors=5,\n                minSize=(30, 30)\n            )\n\n            # Draw a rectangle around the faces\n            for (x, y, w, h) in faces:\n                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n            if anterior != len(faces):\n                anterior = len(faces)\n                log.info(""faces: "" + str(len(faces)) + "" at "" + str(dt.datetime.now()))\n\n            # Display the resulting frame\n            cv2.imshow(\'Video\', frame)\n\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n\n            # Display the resulting frame\n            cv2.imshow(\'Video\', frame)\n\n        # When everything is done, release the capture\n        video_capture.release()\n        cv2.destroyAllWindows()\n'"
opencvutils/detection/test.py,0,"b'from detection import MotionDetection, PedestrianDetection, FaceDetection\n\n\ndef motion_test():\n    motion_obj = MotionDetection()\n    motion_obj.run()\n\n\ndef face_test():\n    face_obj = FaceDetection()\n    face_obj.run()\n\n\ndef ped_test():\n    ped_obj = PedestrianDetection()\n    ped_obj.run()\n\n\n#motion_test()#face_test()\nped_test()\n'"
yoloutils/detection/detection.py,2,"b'import cv2\nimport argparse\nimport numpy as np\n\nap = argparse.ArgumentParser()\nap.add_argument(\'-i\', \'--image\', required=True,\n                help=\'path to input image\')\nap.add_argument(\'-c\', \'--config\', required=True,\n                help=\'path to yolo config file\')\nap.add_argument(\'-w\', \'--weights\', required=True,\n                help=\'path to yolo pre-trained weights\')\nap.add_argument(\'-cl\', \'--classes\', required=True,\n                help=\'path to text file containing class names\')\nargs = ap.parse_args()\n\n\ndef get_output_layers(net):\n\n    layer_names = net.getLayerNames()\n\n    output_layers = [layer_names[i[0] - 1]\n                     for i in net.getUnconnectedOutLayers()]\n\n    return output_layers\n\n\ndef draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n\n    label = str(classes[class_id])\n\n    color = COLORS[class_id]\n\n    cv2.rectangle(img, (x, y), (x_plus_w, y_plus_h), color, 2)\n\n    cv2.putText(img, label, (x-10, y-10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n\nimage = cv2.imread(args.image)\n\nWidth = image.shape[1]\nHeight = image.shape[0]\nscale = 0.00392\n\nclasses = None\n\nwith open(args.classes, \'r\') as f:\n    classes = [line.strip() for line in f.readlines()]\n\nCOLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n\nnet = cv2.dnn.readNet(args.weights, args.config)\n\nblob = cv2.dnn.blobFromImage(\n    image, scale, (416, 416), (0, 0, 0), True, crop=False)\n\nnet.setInput(blob)\n\nouts = net.forward(get_output_layers(net))\n\nclass_ids = []\nconfidences = []\nboxes = []\nconf_threshold = 0.5\nnms_threshold = 0.4\n\n\nfor out in outs:\n    for detection in out:\n        scores = detection[5:]\n        class_id = np.argmax(scores)\n        confidence = scores[class_id]\n        if confidence > 0.5:\n            center_x = int(detection[0] * Width)\n            center_y = int(detection[1] * Height)\n            w = int(detection[2] * Width)\n            h = int(detection[3] * Height)\n            x = center_x - w / 2\n            y = center_y - h / 2\n            class_ids.append(class_id)\n            confidences.append(float(confidence))\n            boxes.append([x, y, w, h])\n\n\nindices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n\nfor i in indices:\n    i = i[0]\n    box = boxes[i]\n    x = box[0]\n    y = box[1]\n    w = box[2]\n    h = box[3]\n    draw_prediction(image, class_ids[i], confidences[i], round(\n        x), round(y), round(x+w), round(y+h))\n\ncv2.imshow(""object detection"", image)\ncv2.waitKey()\n\ncv2.imwrite(""object-detection.jpg"", image)\ncv2.destroyAllWindows()\n'"
yoloutils/detection/general_detection.py,2,"b'import cv2\nimport argparse\nimport numpy as np\n\n\nclass DetectObject:\n\n    def __init__(self, ap):\n\n        self.classes = None\n        self.ap = argparse.ArgumentParser()\n        self.ap.add_argument(\'-i\', \'--image\', required=True,\n                             help=\'path to input image\')\n        self.ap.add_argument(\'-c\', \'--config\', required=True,\n                             help=\'path to yolo config file\')\n        self.ap.add_argument(\'-w\', \'--weights\', required=True,\n                             help=\'path to yolo pre-trained weights\')\n        self.ap.add_argument(\'-cl\', \'--classes\', required=True,\n                             help=\'path to text file containing class names\')\n        self.args = ap.parse_args()\n\n    def get_output_layers(self, net):\n\n        self.layer_names = net.getLayerNames()\n\n        self.output_layers = [self.layer_names[i[0] - 1]\n                              for i in self.net.getUnconnectedOutLayers()]\n\n        return self.output_layers\n\n    def draw_prediction(self, img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n\n        self.label = str(self.classes[self.class_id])\n\n        self.color = COLORS[class_id]\n\n        self.cv2.rectangle(self.img, (self.x, self.y),\n                           (self.x_plus_w, self.y_plus_h), self.scorescolor, 2)\n\n        self.cv2.putText(self.img, self.label, (x-10, y-10),\n                         self.cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.color, 2)\n\n    def read_image(self):\n\n        self.image = cv2.imread(self.args.image)\n\n        self.Width = self.image.shape[1]\n        self.Height = self.image.shape[0]\n        self.scale = 0.00392\n\n        with open(self.args.classes, \'r\') as f:\n            self.classes = [line.strip() for line in f.readlines()]\n\n        self.COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n\n        self.net = self.cv2.dnn.readNet(self.args.weights, self.args.config)\n\n        self.blob = self.cv2.dnn.blobFromImage(\n            self.image, self.scale, (416, 416), (0, 0, 0), True, crop=False)\n\n        self.net.setInput(blob)\n\n        self.outs = net.forward(get_output_layers(net))\n\n        self.class_ids = []\n        self.confidences = []\n        self.boxes = []\n        self.conf_threshold = 0.5\n        self.nms_threshold = 0.4\n\n        for out in self.outs:\n            for detection in self.out:\n                self.scores = detection[5:]\n                self.class_id = np.argmax(self.scores)\n                self.confidence = scores[class_id]\n                if self.confidence > 0.5:\n                    self.center_x = int(detection[0] * Width)\n                    self.center_y = int(detection[1] * Height)\n                    self.w = int(detection[2] * Width)\n                    selfh = int(detection[3] * Height)\n                    self.x = center_x - w / 2\n                    self.y = center_y - h / 2\n                    self.class_ids.append(class_id)\n                    self.confidences.append(float(confidence))\n                    self.boxes.append([x, y, w, h])\n\n        self.indices = cv2.dnn.NMSBoxes(\n            boxes, confidences, conf_threshold, nms_threshold)\n\n        for i in indices:\n            i = i[0]\n            box = boxes[i]\n            x = box[0]\n            y = box[1]\n            w = box[2]\n            h = box[3]\n            draw_prediction(image, class_ids[i], confidences[i], round(\n                x), round(y), round(x+w), round(y+h))\n\n        cv2.imshow(""object detection"", image)\n        cv2.waitKey()\n\n        cv2.imwrite(""object-detection.jpg"", image)\n        cv2.destroyAllWindows()\n'"
yoloutils/detection/testing_gen_detection.py,0,b'from general_detection import DetectObject\n'
opencvutils/detection_tests/frontal_face/object_detection.py,0,"b'import cv2\nimport sys\nimport logging as log\nimport datetime as dt\nfrom time import sleep\n\ncascPath = ""haarcascade_frontalface_default.xml""\nfaceCascade = cv2.CascadeClassifier(cascPath)\nlog.basicConfig(filename=\'webcam.log\',level=log.INFO)\n\nvideo_capture = cv2.VideoCapture(0)\nanterior = 0\n\nwhile True:\n    if not video_capture.isOpened():\n        print(\'Unable to load camera.\')\n        sleep(5)\n        pass\n\n    # Capture frame-by-frame\n    ret, frame = video_capture.read()\n\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    faces = faceCascade.detectMultiScale(\n        gray,\n        scaleFactor=1.1,\n        minNeighbors=5,\n        minSize=(30, 30)\n    )\n\n    # Draw a rectangle around the faces\n    for (x, y, w, h) in faces:\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n\n    if anterior != len(faces):\n        anterior = len(faces)\n        log.info(""faces: ""+str(len(faces))+"" at ""+str(dt.datetime.now()))\n\n\n    # Display the resulting frame\n    cv2.imshow(\'Video\', frame)\n\n\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\n\n    # Display the resulting frame\n    cv2.imshow(\'Video\', frame)\n\n# When everything is done, release the capture\nvideo_capture.release()\ncv2.destroyAllWindows()\n'"
opencvutils/detection_tests/full_body/motion_detector.py,0,"b'# python motion_detector.py\n# python motion_detector.py --video ""path to recorder video""\n\nimport argparse\nimport datetime\nimport time\nimport cv2\n\n# construct the argument parser and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(""-v"", ""--video"", help=""path to the video file"")\nap.add_argument(""-a"", ""--min-area"", type=int, default=500, help=""minimum area size"")\nargs = vars(ap.parse_args())\n\n# if the video argument is None, then we are reading from webcam\nif args.get(""video"", None) is None:\n    vs = VideoCapture(0)\n    time.sleep(2.0)\n\n# otherwise, we are reading from a video file\nelse:\n    vs = cv2.VideoCapture(args[""video""])\n\n# initialize the first frame in the video stream\nfirstFrame = None\n\n# loop over the frames of the video\nwhile True:\n    # grab the current frame and initialize the occupied/unoccupied\n    # text\n    ret, frame = vs.read()\n    text = ""Unoccupied""\n\n    # if the frame could not be grabbed, then we have reached the end\n    # of the video\n    if frame is None:\n        break\n\n    # resize the frame, convert it to grayscale, and blur it\n    frame = cv2.resize(frame, (500, frame.shape[0]))\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (21, 21), 0)\n\n    # if the first frame is None, initialize it\n    if firstFrame is None:\n        firstFrame = gray\n        continue\n\n    # compute the absolute difference between the current frame and\n    # first frame\n    frameDelta = cv2.absdiff(firstFrame, gray)\n    thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\n\n    # dilate the thresholded image to fill in holes, then find contours\n    # on thresholded image\n    thresh = cv2.dilate(thresh, None, iterations=2)\n    cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n                            cv2.CHAIN_APPROX_SIMPLE)\n    cnts = cnts[0]\n\n    # loop over the contours\n    for c in cnts:\n        # if the contour is too small, ignore it\n        if cv2.contourArea(c) < args[""min_area""]:\n            continue\n\n        # compute the bounding box for the contour, draw it on the frame,\n        # and update the text\n        (x, y, w, h) = cv2.boundingRect(c)\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        # TODO Occupied by whom? Using GAIT, passing the video argument to gait\n        text = ""Occupied""\n\n    # draw the text and timestamp on the frame\n    cv2.putText(frame, ""Room Status: {}"".format(text), (10, 20),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n    cv2.putText(frame, datetime.datetime.now().strftime(""%A %d %B %Y %I:%M:%S%p""),\n                (10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0, 0, 255), 1)\n\n    # show the frame and record if the user presses a key\n    cv2.imshow(""Gait Recognition"", frame)\n    cv2.imshow(""Thresh"", thresh)\n    cv2.imshow(""Frame Delta"", frameDelta)\n    key = cv2.waitKey(1) & 0xFF\n\n    # if the `q` key is pressed, break from the lop\n    if key == ord(""q""):\n        break\n\n# cleanup the camera and close any open windows\nvs.release()\ncv2.destroyAllWindows()\n'"
opencvutils/detection_tests/pedestrian/pedestrian_detection.py,0,"b'import cv2\nimport time\n\nhog = cv2.HOGDescriptor()\nhog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\ncap = cv2.VideoCapture(""/path/to/test/video"")\nwhile True:\n    r, frame = cap.read()\n    if r:\n        start_time = time.time()\n        frame = cv2.resize(frame, (1280, 720))  # Downscale to improve frame rate\n        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)  # HOG needs a grayscale image\n\n        rects, weights = hog.detectMultiScale(gray_frame)\n\n        # Measure elapsed time for detections\n        end_time = time.time()\n        print(""Elapsed time:"", end_time - start_time)\n\n        for i, (x, y, w, h) in enumerate(rects):\n            if weights[i] < 0.7:\n                continue\n            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n        cv2.imshow(""preview"", frame)\n    k = cv2.waitKey(1)\n    if k & 0xFF == ord(""q""):  # Exit condition\n        break\n'"
