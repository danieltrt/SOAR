file_path,api_count,code
setup.py,0,"b'from setuptools import find_packages, setup\n\nimport versioneer\n\nDISTNAME = ""mygrad""\nLICENSE = ""MIT""\nAUTHOR = ""Ryan Soklaski""\nAUTHOR_EMAIL = ""rsoklaski@gmail.com""\nURL = ""https://github.com/rsokl/MyGrad""\nCLASSIFIERS = [\n    ""Development Status :: 5 - Production/Stable"",\n    ""License :: OSI Approved :: MIT License"",\n    ""Operating System :: OS Independent"",\n    ""Intended Audience :: Science/Research"",\n    ""Intended Audience :: Education"",\n    ""Programming Language :: Python"",\n    ""Programming Language :: Python :: 3"",\n    ""Programming Language :: Python :: 3.5"",\n    ""Programming Language :: Python :: 3.6"",\n    ""Programming Language :: Python :: 3.7"",\n    ""Programming Language :: Python :: 3.8"",\n    ""Topic :: Scientific/Engineering"",\n]\n\nINSTALL_REQUIRES = [""numpy >= 1.12""]\nTESTS_REQUIRE = [""pytest >= 3.8"", ""hypothesis >= 4.53.2"", ""scipy""]\n\nDESCRIPTION = ""A sleek auto-differentiation library that wraps numpy.""\nLONG_DESCRIPTION = """"""\nmygrad is a simple, NumPy-centric autograd library. An autograd library enables\nyou to automatically compute derivatives of mathematical functions. This library is\ndesigned to serve primarily as an education tool for learning about gradient-based\nmachine learning; it is easy to install, has a readable and easily customizable code base,\nand provides a sleek interface that mimics NumPy. Furthermore, it leverages NumPy\'s\nvectorization to achieve good performance despite the library\'s simplicity.\n\nThis is not meant to be a competitor to libraries like PyTorch (which mygrad most\nclosely resembles) or TensorFlow. Rather, it is meant to serve as a useful tool for\nstudents who are learning about training neural networks using back propagation.\n""""""\n\n\nsetup(\n    name=DISTNAME,\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    license=LICENSE,\n    author=AUTHOR,\n    author_email=AUTHOR_EMAIL,\n    classifiers=CLASSIFIERS,\n    description=DESCRIPTION,\n    long_description=LONG_DESCRIPTION,\n    install_requires=INSTALL_REQUIRES,\n    tests_require=TESTS_REQUIRE,\n    extras_require={\n        ""rnn"": [""numba>=0.34.0""]  # GRU and vanilla RNN require numba-acceleration\n    },\n    url=URL,\n    download_url=""https://github.com/rsokl/mygrad/tarball/"" + versioneer.get_version(),\n    python_requires="">=3.6"",\n    packages=find_packages(where=""src"", exclude=[""tests"", ""tests.*""]),\n    package_dir={"""": ""src""},\n)\n'"
versioneer.py,0,"b'# Version: 0.18\n\n""""""The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'date\']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  ""master"" and ""slave"" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other langauges) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n""Entry-point scripts"" (`setup(entry_points={""console_scripts"": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It\'s not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons ""Public Domain\nDedication"" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n""""""\n\nfrom __future__ import print_function\n\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_root():\n    """"""Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    """"""\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\n            ""Versioneer was unable to run the project root directory. ""\n            ""Versioneer requires setup.py to be executed from ""\n            ""its immediate directory (like \'python setup.py COMMAND\'), ""\n            ""or in a way that lets it use sys.argv[0] to find the root ""\n            ""(like \'python path/to/setup.py COMMAND\').""\n        )\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(\n                ""Warning: build in %s is using versioneer.py from %s""\n                % (os.path.dirname(me), versioneer_py)\n            )\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    """"""Read the project setup.cfg file to determine Versioneer config.""""""\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    if cfg.tag_prefix in (""\'\'"", \'""""\'):\n        cfg.tag_prefix = """"\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\n    ""git""\n] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    git_date = ""%(DOLLAR)sFormat:%%ci%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n            print(""stdout was %%s"" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %%s but none started with prefix %%s"" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %%s not under git control"" %% root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%%s*"" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG) :] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r""\\d"", r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(""picking %s"" % r)\n            return {\n                ""version"": r,\n                ""full-revisionid"": keywords[""full""].strip(),\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": date,\n            }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": keywords[""full""].strip(),\n        ""dirty"": False,\n        ""error"": ""no suitable tags"",\n        ""date"": None,\n    }\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS,\n        [\n            ""describe"",\n            ""--tags"",\n            ""--dirty"",\n            ""--always"",\n            ""--long"",\n            ""--match"",\n            ""%s*"" % tag_prefix,\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r""^(.+)-(\\d+)-g([0-9a-f]+)$"", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = ""unable to parse git-describe output: \'%s\'"" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = ""tag \'%s\' doesn\'t start with prefix \'%s\'"" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""], cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""], cwd=root)[\n        0\n    ].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    """"""Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                ""version"": dirname[len(parentdir_prefix) :],\n                ""full-revisionid"": None,\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            ""Tried directories %s but none started with prefix %s""\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    """"""Try to determine the version from _version.py if present.""""""\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(\n        r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"", contents, re.M | re.S\n    )\n    if not mo:\n        mo = re.search(\n            r""version_json = \'\'\'\\r\\n(.*)\'\'\'  # END VERSION_JSON"", contents, re.M | re.S\n        )\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    """"""Write the given version number to the given _version.py file.""""""\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True, indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""], pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {\n            ""version"": ""unknown"",\n            ""full-revisionid"": pieces.get(""long""),\n            ""dirty"": None,\n            ""error"": pieces[""error""],\n            ""date"": None,\n        }\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {\n        ""version"": rendered,\n        ""full-revisionid"": pieces[""long""],\n        ""dirty"": pieces[""dirty""],\n        ""error"": None,\n        ""date"": pieces.get(""date""),\n    }\n\n\nclass VersioneerBadRootError(Exception):\n    """"""The project root directory is unknown or missing key files.""""""\n\n\ndef get_versions(verbose=False):\n    """"""Get the project version from whatever source is available.\n\n    Returns dict with two keys: \'version\' and \'full\'.\n    """"""\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert (\n        cfg.versionfile_source is not None\n    ), ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": None,\n        ""dirty"": None,\n        ""error"": ""unable to compute version"",\n        ""date"": None,\n    }\n\n\ndef get_version():\n    """"""Get the short version string for this project.""""""\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    """"""Get the custom setuptools/distutils subclasses used by Versioneer.""""""\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            print("" date: %s"" % vers.get(""date""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn\'t copied too, \'git describe\' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different ""build_py"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib, cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        # nczeczulin reports that py2exe won\'t like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   ""version"": versioneer.get_version().split(""+"", 1)[0], # FILEVERSION\n        #   ""product_version"": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            ""DOLLAR"": ""$"",\n                            ""STYLE"": cfg.style,\n                            ""TAG_PREFIX"": cfg.tag_prefix,\n                            ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                            ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    if ""py2exe"" in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(\n                        LONG\n                        % {\n                            ""DOLLAR"": ""$"",\n                            ""STYLE"": cfg.style,\n                            ""TAG_PREFIX"": cfg.tag_prefix,\n                            ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                            ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        }\n                    )\n\n        cmds[""py2exe""] = cmd_py2exe\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(\n                target_versionfile, self._versioneer_generated_versions\n            )\n\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    """"""Main VCS-independent setup function for installing Versioneer.""""""\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (\n        EnvironmentError,\n        configparser.NoSectionError,\n        configparser.NoOptionError,\n    ) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"", file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(\n            LONG\n            % {\n                ""DOLLAR"": ""$"",\n                ""STYLE"": cfg.style,\n                ""TAG_PREFIX"": cfg.tag_prefix,\n                ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n            }\n        )\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source), ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print(\n            "" appending versionfile_source (\'%s\') to MANIFEST.in""\n            % cfg.versionfile_source\n        )\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    """"""Validate the contents of setup.py against Versioneer\'s expectations.""""""\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,"b'import os\n\nfrom hypothesis import Verbosity, settings\n\nsettings.register_profile(""ci"", deadline=1000)\nsettings.register_profile(""intense"", deadline=None, max_examples=1000)\nsettings.register_profile(""dev"", max_examples=10)\nsettings.register_profile(""debug"", max_examples=10, verbosity=Verbosity.verbose)\nsettings.load_profile(os.getenv(""HYPOTHESIS_PROFILE"", ""default""))\n'"
tests/test_indexing_routines.py,10,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given\n\nimport mygrad as mg\nfrom mygrad import where\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef mygrad_where(x, y, condition, constant=False):\n    return where(condition, x, y, constant=constant)\n\n\ndef numpy_where(x, y, condition):\n    return np.where(condition, x, y)\n\n\ndef condition_strat(*arrs):\n    shape = np.broadcast(*arrs).shape\n    return hnp.arrays(shape=hnp.broadcastable_shapes(shape=shape), dtype=bool)\n\n\n@fwdprop_test_factory(\n    mygrad_func=mygrad_where,\n    true_func=numpy_where,\n    kwargs=dict(condition=condition_strat),\n    num_arrays=2,\n)\ndef test_where_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=mygrad_where,\n    true_func=numpy_where,\n    kwargs=dict(condition=condition_strat),\n    num_arrays=2,\n)\ndef test_where_bkwd():\n    pass\n\n\n@given(condition=st.from_type(type) | hnp.arrays(shape=hnp.array_shapes(), dtype=int))\n@pytest.mark.filterwarnings(""ignore: Calling nonzero on 0d arrays is deprecated"")\ndef test_where_condition_only_fwd(condition):\n    """"""mygrad.where should merely mirror numpy.where when only `where(condition)`\n    is specified.""""""\n    tensor_condition = (\n        mg.Tensor(condition) if isinstance(condition, np.ndarray) else condition\n    )\n    assert all(\n        np.all(x == y) for x, y in zip(where(tensor_condition), np.where(condition))\n    )\n\n\n@given(\n    condition=hnp.arrays(shape=hnp.array_shapes(min_dims=1), dtype=bool),\n    x=st.none() | hnp.arrays(shape=hnp.array_shapes(min_dims=1), dtype=int,),\n    y=st.none() | hnp.arrays(shape=hnp.array_shapes(min_dims=1), dtype=int,),\n)\ndef test_clip_input_validation(condition, x, y):\n    args = [i for i in (x, y) if i is not None]\n\n    try:\n        np.where(condition, *args)\n    except Exception as e:\n        with pytest.raises(type(e)):\n            where(condition, *args)\n        return\n'"
tests/test_operation_base.py,4,"b'import hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given\nfrom hypothesis.extra import numpy as hnp\n\nfrom mygrad import Tensor\nfrom mygrad.errors import InvalidGradient\nfrom mygrad.operation_base import Operation\nfrom tests.utils import does_not_raise\n\n\nclass OldOperation(Operation):\n    """"""Implements old version of MyGrad back-propagation""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return a.data\n\n    def backward_var(self, grad, index, **kwargs):\n        self.variables[index].backward(grad)\n\n\ndef old_op(a):\n    return Tensor._op(OldOperation, a)\n\n\n@given(\n    constant=st.booleans(),\n    arr=hnp.arrays(\n        dtype=np.float64,\n        shape=hnp.array_shapes(min_dims=0),\n        elements=st.floats(-1e6, 1e6),\n    )\n    | st.floats(-1e6, 1e6),\n    op_before=st.booleans(),\n    op_after=st.booleans(),\n)\ndef test_backpropping_non_numeric_gradient_raises(\n    constant: bool, arr: np.ndarray, op_before: bool, op_after: bool\n):\n    x = Tensor(arr, constant=constant)\n\n    if op_before:\n        x += 1\n\n    x = old_op(x)\n\n    if op_after:\n        x = x * 2\n\n    # if constant tensor, backprop should not be triggered - no exception raised\n    with (\n        pytest.raises(InvalidGradient) if not constant else does_not_raise()\n    ) as exec_info:\n        x.backward()\n\n    if exec_info is not None:\n        err_msg = str(exec_info.value)\n        assert ""NoneType"" in err_msg\n'"
tests/test_tensor_creation.py,17,"b'import hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import given\nfrom numpy.testing import assert_array_equal\n\nfrom mygrad import Tensor\nfrom mygrad.tensor_creation.funcs import (\n    arange,\n    empty,\n    empty_like,\n    eye,\n    full,\n    full_like,\n    geomspace,\n    identity,\n    linspace,\n    logspace,\n    ones,\n    ones_like,\n    zeros,\n    zeros_like,\n)\n\n\ndef check_tensor_array(tensor, array, constant):\n    assert isinstance(tensor, Tensor)\n    assert_array_equal(tensor.data, array)\n    assert tensor.dtype is array.dtype\n    assert tensor.constant is constant\n\n\n@given(constant=st.booleans(), dtype=st.sampled_from((np.int32, np.float64)))\ndef test_all_tensor_creation(constant, dtype):\n    x = np.array([1, 2, 3])\n\n    e = empty((3, 2), dtype=dtype, constant=constant)\n    assert e.shape == (3, 2)\n    assert e.constant is constant\n\n    e = empty_like(e, dtype=dtype, constant=constant)\n    assert e.shape == (3, 2)\n    assert e.constant is constant\n\n    check_tensor_array(\n        eye(3, dtype=dtype, constant=constant), np.eye(3, dtype=dtype), constant\n    )\n\n    check_tensor_array(\n        identity(3, dtype=dtype, constant=constant),\n        np.identity(3, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        ones((4, 5, 6), dtype=dtype, constant=constant),\n        np.ones((4, 5, 6), dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        ones_like(x, dtype=dtype, constant=constant),\n        np.ones_like(x, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        ones_like(Tensor(x), dtype=dtype, constant=constant),\n        np.ones_like(x, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        zeros((4, 5, 6), dtype=dtype, constant=constant),\n        np.zeros((4, 5, 6), dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        zeros_like(x, dtype=dtype, constant=constant),\n        np.zeros_like(x, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        zeros_like(Tensor(x), dtype=dtype, constant=constant),\n        np.zeros_like(x, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        full((4, 5, 6), 5.0, dtype=dtype, constant=constant),\n        np.full((4, 5, 6), 5.0, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        full_like(x, 5.0, dtype=dtype, constant=constant),\n        np.full_like(x, 5.0, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        full_like(Tensor(x), 5.0, dtype=dtype, constant=constant),\n        np.full_like(x, 5.0, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        arange(3, 7, dtype=dtype, constant=constant),\n        np.arange(3, 7, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        linspace(3, 7, dtype=dtype, constant=constant),\n        np.linspace(3, 7, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        logspace(3, 7, dtype=dtype, constant=constant),\n        np.logspace(3, 7, dtype=dtype),\n        constant,\n    )\n\n    check_tensor_array(\n        geomspace(3, 7, dtype=dtype, constant=constant),\n        np.geomspace(3, 7, dtype=dtype),\n        constant,\n    )\n'"
tests/test_tensor_manip.py,41,"b'from itertools import permutations\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import given, settings\nfrom numpy.testing import assert_allclose\nfrom pytest import raises\n\nfrom mygrad import (\n    Tensor,\n    broadcast_to,\n    expand_dims,\n    moveaxis,\n    ravel,\n    repeat,\n    roll,\n    squeeze,\n    swapaxes,\n    transpose,\n)\n\nfrom .custom_strategies import valid_axes\nfrom .utils.numerical_gradient import numerical_gradient_full\nfrom .wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef test_input_validation():\n    x = Tensor([[1, 2]])\n\n    with raises(TypeError):\n        transpose(x, (0,), 1)\n\n\n@settings(deadline=None)\n@given(\n    x=hnp.arrays(\n        shape=hnp.array_shapes(max_side=4, max_dims=5),\n        dtype=float,\n        elements=st.floats(-10.0, 10.0),\n    ),\n    data=st.data(),\n)\ndef test_transpose(x, data):\n    axes = data.draw(\n        valid_axes(x.ndim, min_dim=x.ndim, max_dim=x.ndim).map(\n            lambda out: (out,) if isinstance(out, int) else out\n        ),\n        label=""axes"",\n    )\n\n    x_arr = Tensor(np.copy(x))\n\n    o = transpose(x_arr, axes, constant=False)\n    grad = data.draw(\n        hnp.arrays(shape=o.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    o.backward(grad)\n\n    def f(x):\n        return np.transpose(x, axes)\n\n    assert_allclose(o.data, f(x))\n\n    (dx,) = numerical_gradient_full(f, x, back_grad=grad)\n\n    assert_allclose(x_arr.grad, dx)\n\n    out = transpose(x, constant=True)\n    assert out.constant and not x_arr.constant\n\n\ndef test_transpose_property():\n    dat = np.arange(6).reshape(2, 3)\n    x = Tensor(dat)\n    f = x.T\n    f.backward(dat.T)\n\n    assert_allclose(f.data, dat.T)\n    assert_allclose(x.grad, dat)\n\n\ndef test_transpose_method():\n    dat = np.arange(24).reshape(2, 3, 4)\n\n    for axes in permutations(range(3)):\n        # passing tuple of integers\n        x = Tensor(dat)\n        f = x.transpose(axes)\n        f.backward(dat.transpose(axes))\n\n        assert_allclose(f.data, dat.transpose(axes))\n        assert_allclose(x.grad, dat)\n\n        # passing integers directly\n        x = Tensor(dat)\n        f = x.transpose(*axes)\n        f.backward(dat.transpose(axes))\n\n        assert_allclose(f.data, dat.transpose(axes), err_msg=""{}"".format(axes))\n        assert_allclose(x.grad, dat, err_msg=""{}"".format(axes))\n\n    # passing integers directly\n    x = Tensor(dat)\n    f = x.transpose()\n    f.backward(dat.transpose())\n\n    assert_allclose(f.data, dat.transpose())\n    assert_allclose(x.grad, dat)\n\n    # check that constant=True works\n    x = Tensor(dat)\n    f = x.transpose(constant=True)\n    assert f.constant and not x.constant\n\n    f = x.transpose(1, 0, 2, constant=True)\n    assert f.constant and not x.constant\n\n\n@given(\n    x=hnp.arrays(\n        shape=hnp.array_shapes(max_side=2, max_dims=3),\n        dtype=float,\n        elements=st.floats(-10.0, 10.0),\n    ),\n    data=st.data(),\n)\ndef test_squeeze(x, data):\n    axes = data.draw(valid_axes(x.ndim), label=""axes"")\n    x_arr = Tensor(np.copy(x))\n    x_arr2 = Tensor(np.copy(x))\n\n    def f(x):\n        return np.squeeze(x, axes)\n\n    try:\n        numpy_out = np.squeeze(x, axes)\n    except ValueError:\n        with raises(ValueError):\n            squeeze(x_arr, axes, constant=False)\n        return\n\n    o = squeeze(x_arr, axes, constant=False)\n    o_method = x_arr2.squeeze(axes)\n    assert_allclose(o.data, numpy_out)\n    assert_allclose(o_method.data, numpy_out)\n\n    grad = data.draw(\n        hnp.arrays(shape=o.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n    o.backward(grad)\n    o_method.backward(grad)\n\n    (dx,) = numerical_gradient_full(f, x, back_grad=grad)\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(x_arr2.grad, dx)\n\n\ndef _expand_dims_axis(arr):\n    return st.integers(-arr.ndim - 1, arr.ndim)\n\n\ndef _swap_axes_axis(arr):\n    return st.integers(-arr.ndim, arr.ndim - 1) if arr.ndim else st.just(0)\n\n\ndef _valid_moveaxis_args(*arrs, **kwargs):\n    return len(kwargs[""source""]) == len(kwargs[""destination""])\n\n\n@fwdprop_test_factory(\n    mygrad_func=expand_dims,\n    true_func=np.expand_dims,\n    num_arrays=1,\n    kwargs=dict(axis=_expand_dims_axis),\n)\ndef test_expand_dims_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=expand_dims,\n    true_func=np.expand_dims,\n    num_arrays=1,\n    kwargs=dict(axis=_expand_dims_axis),\n    vary_each_element=True,\n)\ndef test_expand_dims_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=moveaxis,\n    true_func=np.moveaxis,\n    num_arrays=1,\n    kwargs=dict(\n        source=lambda x: valid_axes(x.ndim, permit_none=False, permit_int=False),\n        destination=lambda x: valid_axes(x.ndim, permit_none=False, permit_int=False),\n    ),\n    assumptions=_valid_moveaxis_args,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=4, max_dims=5)},\n)\ndef test_moveaxis_fwd():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=moveaxis,\n    true_func=np.moveaxis,\n    num_arrays=1,\n    kwargs=dict(\n        source=lambda x: valid_axes(x.ndim, permit_none=False, permit_int=False),\n        destination=lambda x: valid_axes(x.ndim, permit_none=False, permit_int=False),\n    ),\n    assumptions=_valid_moveaxis_args,\n    vary_each_element=True,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=4, max_dims=5)},\n)\ndef test_moveaxis_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=swapaxes,\n    true_func=np.swapaxes,\n    num_arrays=1,\n    kwargs=dict(axis1=_swap_axes_axis, axis2=_swap_axes_axis),\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=3, min_dims=1, max_dims=3)},\n)\ndef test_swapaxes_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=swapaxes,\n    true_func=np.swapaxes,\n    num_arrays=1,\n    kwargs=dict(axis1=_swap_axes_axis, axis2=_swap_axes_axis),\n    vary_each_element=True,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=3, min_dims=1, max_dims=3)},\n)\ndef test_swapaxes_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=Tensor.flatten,\n    true_func=np.ndarray.flatten,\n    num_arrays=1,\n    permit_0d_array_as_float=False,\n)\ndef test_flatten_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=Tensor.flatten,\n    true_func=np.ndarray.flatten,\n    num_arrays=1,\n    vary_each_element=True,\n)\ndef test_flatten_bkwd():\n    pass\n\n\n@fwdprop_test_factory(mygrad_func=ravel, true_func=np.ravel, num_arrays=1)\ndef test_ravel_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=ravel, true_func=np.ravel, num_arrays=1, vary_each_element=True\n)\ndef test_ravel_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=broadcast_to,\n    true_func=np.broadcast_to,\n    num_arrays=1,\n    index_to_arr_shapes={0: hnp.array_shapes(min_dims=0)},\n    kwargs=dict(\n        shape=lambda arr: hnp.array_shapes(min_dims=0).map(lambda x: x + arr.shape)\n    ),\n)\ndef test_broadcast_to_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=broadcast_to,\n    true_func=np.broadcast_to,\n    num_arrays=1,\n    vary_each_element=True,\n    index_to_arr_shapes={0: hnp.array_shapes(min_dims=0)},\n    kwargs=dict(\n        shape=lambda arr: hnp.array_shapes(min_dims=0).map(lambda x: x + arr.shape)\n    ),\n)\ndef test_broadcast_to_bkwd():\n    pass\n\n\n@st.composite\ndef gen_roll_args(draw, arr):\n    shift = draw(st.integers() | st.tuples(*(st.integers() for i in arr.shape)))\n\n    if arr.ndim:\n        ax_strat = hnp.valid_tuple_axes(\n            arr.ndim,\n            **(\n                dict(min_size=len(shift), max_size=len(shift))\n                if isinstance(shift, tuple)\n                else {}\n            )\n        )\n        axis = draw(st.none() | st.integers(-arr.ndim, arr.ndim - 1) | ax_strat)\n    else:\n        axis = None\n    return dict(shift=shift, axis=axis)\n\n\n@fwdprop_test_factory(\n    mygrad_func=roll, true_func=np.roll, num_arrays=1, kwargs=gen_roll_args\n)\ndef test_roll_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=roll,\n    true_func=np.roll,\n    num_arrays=1,\n    kwargs=gen_roll_args,\n    vary_each_element=True,\n)\ndef test_roll_bkwd():\n    pass\n\n\ndef gen_int_repeat_args(arr: Tensor) -> st.SearchStrategy[dict]:\n    valid_axis = st.none()\n    valid_axis |= st.integers(-arr.ndim, arr.ndim - 1) if arr.ndim else st.just(0)\n    return st.fixed_dictionaries(\n        dict(repeats=st.integers(min_value=0, max_value=5), axis=valid_axis,)\n    )\n\n\n@st.composite\ndef gen_tuple_repeat_args(draw: st.DataObject.draw, arr: Tensor):\n\n    valid_axis = draw(\n        st.none() | (st.integers(-arr.ndim, arr.ndim - 1) if arr.ndim else st.just(0))\n    )\n\n    num_repeats = (\n        arr.shape[valid_axis] if valid_axis is not None and arr.ndim else arr.size\n    )\n    repeats = draw(st.tuples(*[st.integers(0, 5)] * num_repeats))\n    return dict(repeats=repeats, axis=valid_axis,)\n\n\n@fwdprop_test_factory(\n    mygrad_func=repeat, true_func=np.repeat, num_arrays=1, kwargs=gen_int_repeat_args,\n)\ndef test_repeat_int_repeats_only_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=repeat,\n    true_func=np.repeat,\n    num_arrays=1,\n    kwargs=gen_int_repeat_args,\n    vary_each_element=True,\n)\ndef test_repeat_int_repeats_only_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=repeat, true_func=np.repeat, num_arrays=1, kwargs=gen_tuple_repeat_args,\n)\ndef test_repeat_tuple_repeats_only_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=repeat,\n    true_func=np.repeat,\n    num_arrays=1,\n    kwargs=gen_tuple_repeat_args,\n    vary_each_element=True,\n)\ndef test_repeat_tuple_repeats_only_bkwd():\n    pass\n'"
tests/test_utils.py,22,"b'from numbers import Real\nfrom typing import Tuple\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import HealthCheck, given, settings\nfrom numpy.testing import assert_allclose\nfrom pytest import raises\n\nfrom mygrad._utils import is_invalid_gradient, reduce_broadcast\nfrom tests.custom_strategies import broadcastable_shapes, everything_except\n\n\n@pytest.mark.parametrize(\n    (""grad"", ""is_invalid""),\n    [\n        (everything_except((np.ndarray, Real)), True),\n        (None, True),\n        (np.ndarray([1], dtype=""O""), True),\n        (\n            hnp.arrays(\n                shape=hnp.array_shapes(),\n                dtype=hnp.floating_dtypes(),\n                elements=st.floats(width=16),\n            ),\n            False,\n        ),\n        ((st.integers(min_value=int(-1e6), max_value=int(1e6)) | st.floats()), False),\n    ],\n)\n@settings(deadline=None, suppress_health_check=(HealthCheck.too_slow,))\n@given(data=st.data())\ndef test_is_invalid_gradient(grad, is_invalid, data: st.DataObject):\n    if isinstance(grad, st.SearchStrategy):\n        grad = data.draw(grad, label=""grad"")\n\n    assert is_invalid_gradient(grad) is is_invalid, grad\n\n\n@given(shapes=hnp.mutually_broadcastable_shapes(num_shapes=2, max_dims=5))\ndef test_reduce_broadcast_shape_consistency(shapes: hnp.BroadcastableShapes):\n    grad = np.zeros(shapes.result_shape)\n\n    assert (\n        reduce_broadcast(grad, var_shape=shapes.input_shapes[0]).shape\n        == shapes.input_shapes[0]\n    )\n    assert (\n        reduce_broadcast(grad, var_shape=shapes.input_shapes[1]).shape\n        == shapes.input_shapes[1]\n    )\n\n\n@given(\n    shapes=hnp.array_shapes(min_dims=1, max_dims=10).flatmap(\n        lambda shape: st.tuples(\n            st.just(shape), hnp.array_shapes(min_dims=0, max_dims=len(shape) - 1)\n        )\n    )\n)\ndef test_bad_gradient_dimensionality(shapes: Tuple[Tuple[int, ...], Tuple[int, ...]]):\n    """""" test that grad.dim < len(var_shape) raises ValueError""""""\n    var_shape = shapes[0]\n    grad = np.empty(shapes[1])\n    with raises(ValueError):\n        reduce_broadcast(grad=grad, var_shape=var_shape)\n\n\n@given(\n    grad=hnp.arrays(\n        dtype=float, shape=hnp.array_shapes(), elements=st.floats(-100, 100)\n    )\n)\ndef test_broadcast_scalar(grad):\n    """""" test when grad was broadcasted from a scalar""""""\n    assert_allclose(reduce_broadcast(grad, tuple()), grad.sum())\n\n\n@given(\n    grad=hnp.arrays(\n        dtype=float, shape=hnp.array_shapes(), elements=st.floats(-100, 100)\n    )\n)\ndef test_reduce_broadcast_same_shape(grad):\n    """""" test when no broadcasting occurred""""""\n    var_shape = grad.shape\n    reduced_grad = reduce_broadcast(grad=grad, var_shape=var_shape)\n    assert_allclose(actual=reduced_grad, desired=grad)\n\n\n@given(var_shape=hnp.array_shapes(min_side=2), data=st.data())\ndef test_reduce_broadcast_nokeepdim(var_shape, data):\n    """""" example broadcasting: (2, 3) -> (5, 2, 3)""""""\n    grad_shape = data.draw(\n        broadcastable_shapes(\n            shape=var_shape,\n            min_dims=len(var_shape) + 1,\n            max_dims=len(var_shape) + 3,\n            min_side=2,\n        ),\n        label=""grad_shape"",\n    )\n    grad = np.ones(grad_shape, dtype=float)\n\n    reduced_grad = reduce_broadcast(grad=grad, var_shape=var_shape)\n    reduced_grad *= (\n        np.prod(var_shape) / grad.size\n    )  # scale reduced-grad so all elements are 1\n    assert_allclose(actual=reduced_grad, desired=np.ones(var_shape))\n\n\n@given(var_shape=hnp.array_shapes(), data=st.data())\ndef test_reduce_broadcast_keepdim(var_shape, data):\n    """""" example broadcasting: (2, 1, 4) -> (2, 5, 4)""""""\n    grad = data.draw(\n        hnp.arrays(\n            dtype=float,\n            shape=broadcastable_shapes(\n                shape=var_shape, min_dims=len(var_shape), max_dims=len(var_shape)\n            ),\n            elements=st.just(1.0),\n        ),\n        label=""grad"",\n    )\n\n    reduced_grad = reduce_broadcast(grad=grad, var_shape=var_shape)\n    assert reduced_grad.shape == tuple(\n        i if i < j else j for i, j in zip(var_shape, grad.shape)\n    )\n    assert (i == 1 for i, j in zip(var_shape, grad.shape) if i < j)\n    sum_axes = tuple(n for n, (i, j) in enumerate(zip(var_shape, grad.shape)) if i != j)\n    assert_allclose(actual=reduced_grad, desired=grad.sum(axis=sum_axes, keepdims=True))\n\n\n@given(\n    grad=hnp.arrays(dtype=float, shape=(5, 3, 4, 2), elements=st.floats(-0.01, 0.01))\n)\ndef test_hybrid_broadcasting(grad):\n    """""" tests new-dim and keep-dim broadcasting\n         (3, 1, 2) -> (5, 3, 4, 2)""""""\n    var_shape = (3, 1, 2)\n    reduced = reduce_broadcast(grad=grad, var_shape=var_shape)\n    answer = grad.sum(axis=0).sum(axis=-2, keepdims=True)\n    assert_allclose(actual=reduced, desired=answer)\n'"
tests/test_version.py,0,"b'def test_version():\n    import mygrad\n\n    assert isinstance(mygrad.__version__, str)\n    assert mygrad.__version__\n    assert ""unknown"" not in mygrad.__version__\n'"
docs/source/conf.py,0,"b'import mygrad\nfrom mygrad import linalg\n\n# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = ""MyGrad""\ncopyright = ""2018, Ryan Soklaski""\nauthor = ""Ryan Soklaski""\n\n# The short X.Y version\nversion = ""."".join(mygrad.__version__.split(""."")[:2])\n# The full version, including alpha/beta/rc tags\nrelease = mygrad.__version__\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.intersphinx"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.githubpages"",\n    ""sphinx.ext.autosummary"",\n    ""numpydoc"",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n\nsource_suffix = ["".rst""]\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [""_build"", ""Thumbs.db"", "".DS_Store"", ""**.ipynb_checkpoints""]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\n\n\ndef setup(app):\n    app.add_stylesheet(""my_theme.css"")\n    # app.add_javascript(""https://www.googletagmanager.com/gtag/js?id=UA-115029372-1"")\n    # app.add_javascript(""gtag.js"")\n\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""MyGraddoc""\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, ""MyGrad.tex"", ""MyGrad Documentation"", ""Ryan Soklaski"", ""manual"")\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""mygrad"", ""MyGrad Documentation"", [author], 1)]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""MyGrad"",\n        ""MyGrad Documentation"",\n        author,\n        ""MyGrad"",\n        ""A sleek auto-differentiation library that wraps numpy."",\n        ""Miscellaneous"",\n    )\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [""search.html""]\n\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Example configuration for intersphinx: refer to the Python standard library.\nintersphinx_mapping = {""https://docs.python.org/"": None}\n'"
src/mygrad/__init__.py,0,"b'from mygrad.tensor_base import Tensor  # isort:skip  # avoid an import cycle\n\nfrom mygrad.indexing_routines.funcs import *\nfrom mygrad.linalg.funcs import *\nfrom mygrad.math.arithmetic.funcs import *\nfrom mygrad.math.consts import *\nfrom mygrad.math.exp_log.funcs import *\nfrom mygrad.math.hyperbolic_trig.funcs import *\nfrom mygrad.math.misc.funcs import *\nfrom mygrad.math.nondifferentiable import argmax, argmin\nfrom mygrad.math.sequential.funcs import *\nfrom mygrad.math.sequential.funcs import max, min\nfrom mygrad.math.trigonometric.funcs import *\nfrom mygrad.nnet.layers.utils import sliding_window_view\nfrom mygrad.tensor_creation.funcs import *\nfrom mygrad.tensor_manip.array_shape.funcs import *\nfrom mygrad.tensor_manip.tiling.funcs import *\nfrom mygrad.tensor_manip.transpose_like.funcs import *\n\nfrom ._version import get_versions\n\n__version__ = get_versions()[""version""]\ndel get_versions\n\n\nfor attr in (\n    sum,\n    prod,\n    cumprod,\n    cumsum,\n    mean,\n    std,\n    var,\n    max,\n    min,\n    argmax,\n    argmin,\n    swapaxes,\n    transpose,\n    moveaxis,\n    reshape,\n    squeeze,\n    ravel,\n    matmul,\n):\n    setattr(Tensor, attr.__name__, attr)\n'"
src/mygrad/_utils.py,2,"b'from numbers import Real\nfrom typing import Any\n\nimport numpy as np\n\n__all__ = [""is_invalid_gradient"", ""reduce_broadcast"", ""SkipGradient""]\n\n\nclass SkipGradient(Exception):\n    """""" The gradient for the current tensor-label pair has already\n    been computed, scaled, and back-propped, skip gradient calculation.""""""\n\n\ndef reduce_broadcast(grad, var_shape):\n    """""" Sum-reduce axes of `grad` so its shape matches `var_shape.\n\n        This the appropriate mechanism for backpropagating a gradient\n        through an operation in which broadcasting occurred for the\n        given variable.\n\n        Parameters\n        ----------\n        grad : numpy.ndarray\n        var_shape : Tuple[int, ...]\n\n        Returns\n        -------\n        numpy.ndarray\n\n        Raises\n        ------\n        ValueError\n            The dimensionality of the gradient cannot be less than\n            that of its associated variable.""""""\n    if grad.shape == var_shape:\n        return grad\n\n    if grad.ndim != len(var_shape):\n        if grad.ndim < len(var_shape):\n            raise ValueError(\n                f""The dimensionality of the gradient of the broadcasted ""\n                f""operation ({grad.ndim}) is less than that of its associated variable ""\n                f""({len(var_shape)})""\n            )\n        grad = grad.sum(axis=tuple(range(grad.ndim - len(var_shape))))\n\n    keepdims = tuple(n for n, i in enumerate(grad.shape) if i != var_shape[n])\n    if keepdims:\n        grad = grad.sum(axis=keepdims, keepdims=True)\n\n    return grad\n\n\ndef is_invalid_gradient(grad: Any) -> bool:\n    """"""Returns ``True`` if ``grad`` is not array-like.\n\n    Parameters\n    ----------\n    grad : Any\n\n\n    Returns\n    -------\n    ``True`` if ``grad`` is invalid""""""\n    return not isinstance(grad, (np.ndarray, Real)) or not np.issubdtype(\n        np.asarray(grad).dtype, np.number\n    )\n'"
src/mygrad/_version.py,0,"b'# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""$Format:%d$""\n    git_full = ""$Format:%H$""\n    git_date = ""$Format:%ci$""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""pep440""\n    cfg.tag_prefix = ""v""\n    cfg.parentdir_prefix = ""mygrad-""\n    cfg.versionfile_source = ""mygrad/_version.py""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen(\n                [c] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n            )\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried {}"".format(commands))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                ""version"": dirname[len(parentdir_prefix) :],\n                ""full-revisionid"": None,\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": None,\n            }\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            ""Tried directories %s but none started with prefix %s""\n            % (rootdirs, parentdir_prefix)\n        )\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = {r.strip() for r in refnames.strip(""()"").split("","")}\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = {r for r in refs if re.search(r""\\d"", r)}\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            if verbose:\n                print(""picking %s"" % r)\n            return {\n                ""version"": r,\n                ""full-revisionid"": keywords[""full""].strip(),\n                ""dirty"": False,\n                ""error"": None,\n                ""date"": date,\n            }\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": keywords[""full""].strip(),\n        ""dirty"": False,\n        ""error"": ""no suitable tags"",\n        ""date"": None,\n    }\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(\n        GITS,\n        [\n            ""describe"",\n            ""--tags"",\n            ""--dirty"",\n            ""--always"",\n            ""--long"",\n            ""--match"",\n            ""%s*"" % tag_prefix,\n        ],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r""^(.+)-(\\d+)-g([0-9a-f]+)$"", git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = ""unable to parse git-describe output: \'%s\'"" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = ""tag \'%s\' doesn\'t start with prefix \'%s\'"" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""], cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""], cwd=root)[\n        0\n    ].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""], pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {\n            ""version"": ""unknown"",\n            ""full-revisionid"": pieces.get(""long""),\n            ""dirty"": None,\n            ""error"": pieces[""error""],\n            ""date"": None,\n        }\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {\n        ""version"": rendered,\n        ""full-revisionid"": pieces[""long""],\n        ""dirty"": pieces[""dirty""],\n        ""error"": None,\n        ""date"": pieces.get(""date""),\n    }\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(""/""):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            ""version"": ""0+unknown"",\n            ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to find root of source tree"",\n            ""date"": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        ""version"": ""0+unknown"",\n        ""full-revisionid"": None,\n        ""dirty"": None,\n        ""error"": ""unable to compute version"",\n        ""date"": None,\n    }\n'"
src/mygrad/computational_graph.py,4,"b'import numpy as np\nfrom graphviz import Digraph\n\nfrom mygrad.tensor_base import Tensor\n\n\ndef build_graph(\n    fin,\n    names=None,\n    *,\n    render=True,\n    save=False,\n    dims=False,\n    dtypes=False,\n    sum_stats=False,\n):\n    """""" Builds and renders a computational graph.\n\n        Parameters\n        ----------\n        fin : mygrad.Tensor\n            The tensor object that will be the final node in the\n            computational graph.\n\n        names : Optional[Dict[str, Union[mygrad.Tensor, numpy.ndarray]]]\n            A dictionary that maps names of Tensors to Tensor objects. If\n            an argument is passed to names, the key name that maps to a Tensor\n            included in the computational graph will be used as a label for the\n            Tensor\'s node. If no argument is passed, the nodes on the\n            computational graph will display the full Tensor.\n\n            To use the names assigned in the local environment,\n            pass ``names=locals()`` to the build_graph function.\n\n            If different names are used from the local environment,\n            the key must map to the exact Tensor object. A new Tensor or copy\n            of the original Tensor should not be created as the value in the\n            dictionary.\n\n            Only instances of mygrad.Tensor or numpy.ndarray can have labels\n            assigned to Nodes. If a list or tuple is used in an operation\n            with a Tensor, and names is not None, the Node label will be\n            set to *Constant*. If a list or tuple is used in multiple operations,\n            a unique Node will be created for each time it is used.\n\n            A scalar will always be used as the label for a 0-dimensional\n            Tensor\'s Node.\n\n        render : bool, optional (default=True)\n            If True, build_graph will return a graphviz Digraph object that,\n            when called, will render the computational graph in a Jupyter\n            notebook or the Jupyter Qt console. If False, nothing is returned.\n\n        save : bool, optional (default=False)\n            If True, build_graph will save a rendered computational graph to\n            the current working directory as ``computational_graph.pdf``.\n\n        dims : bool, optional (default=False)\n            If True, Tensor dimensions are added to Node labels. Dimensions\n            will not be displayed for scalar values.\n\n        dtypes : bool, optional (default=False)\n            If True, Tensor data types are added to Node labels.\n\n        sum_stats : bool, optional (default=False)\n            If True, Tensor minimums, maximums, medians, and means are\n            added to Node labels. These will not be displayed for scalar values.\n\n        Returns\n        -------\n        Union[graphviz.Digraph, None]\n\n        Notes\n        -----\n        build_graph requires that Graphviz is installed.\n    """"""\n    assert isinstance(fin, Tensor), ""fin must be a Tensor""\n    assert isinstance(names, (dict, type(None)))\n    assert isinstance(render, bool)\n    assert isinstance(save, bool)\n    assert isinstance(dims, bool)\n    assert isinstance(dtypes, bool)\n    assert isinstance(sum_stats, bool)\n\n    graph = Digraph(strict=True)\n    graph.node_attr.update(fontsize=""12"")\n\n    _add_node(fin, graph, names=names, dims=dims, dtypes=dtypes, sum_stats=sum_stats)\n\n    if save:\n        graph.render(filename=""computational_graph"", cleanup=True)\n\n    if render:\n        return graph\n\n\ndef _add_node(node, graph, op_id=None, **kwargs):\n    """""" Recursively traces computational graph and adds nodes to Digraph. """"""\n    node_id = str(id(node))\n    node_lab = repr(node)\n    if kwargs[""names""] is not None:\n        for key in kwargs[""names""]:\n            if id(kwargs[""names""][key]) == id(node):\n                node_lab = key\n                break\n            elif id(kwargs[""names""][key]) == id(node.data):\n                node_lab = key + ""\\n*Constant*""\n                node_id = str(id(node.data))\n                break\n        if node_lab == repr(node):\n            if not node.ndim:\n                node_lab = str(node.data)\n            elif node._constant:\n                node_lab = ""*Constant*""\n            else:\n                node_lab = ""Intermediary Tensor""\n\n    if node.ndim:\n        if kwargs[""dims""]:\n            node_lab += f""\\nDims: {node.shape}""\n        if kwargs[""dtypes""]:\n            node_lab += f""\\nDtype: {node.dtype}""\n        if kwargs[""sum_stats""]:\n            node_lab += (\n                f""\\nMin: {np.amin(node.data)}""\n                f""\\nMedian: {np.median(node.data)}""\n                f""\\nMean: {np.mean(node.data)}""\n                f""\\nMax: {np.amax(node.data)}""\n            )\n    else:\n        if kwargs[""dtypes""]:\n            node_lab += f""\\nDtype: {node.dtype}""\n\n    graph.node(name=node_id, label=node_lab)\n\n    if node._creator is None:\n        if op_id is not None:\n            graph.edge(node_id, op_id)\n        return\n    else:\n        op_lab = repr(node._creator).rpartition(""."")[-1].split("" "")[0]\n        if op_id is not None:\n            graph.edge(node_id, op_id)\n        op_id = str(id(node._creator))\n\n        graph.node(name=op_id, label=op_lab, style=""filled"", fillcolor=""red"")\n        graph.edge(op_id, node_id)\n\n        for var in node._creator.variables:\n            _add_node(var, graph, op_id=op_id, **kwargs)\n'"
src/mygrad/errors.py,0,"b'class MyGradException(Exception):\n    """"""Generic parent class for exceptions thrown by MyGrad.""""""\n\n\nclass InvalidGradient(MyGradException):\n    """"""An invalid gradient (i.e. a non-numeric or non-array-like object)\n    was produced by an operation or was supplied by a user.""""""\n\n\nclass InvalidBackprop(MyGradException):\n    """"""Backpropagation was invoked through a partially-cleared graph\n    or from a non-scalar for a scalar-only graph""""""\n'"
src/mygrad/operation_base.py,3,"b'""""""\nDefines the base class for mathematical operations capable of back-propagating\ngradients to their input tensors.""""""\n\nfrom typing import Optional, Set\n\nimport numpy as np\n\nfrom mygrad._utils import SkipGradient, is_invalid_gradient, reduce_broadcast\nfrom mygrad.errors import InvalidBackprop, InvalidGradient\n\n__all__ = [""Operation"", ""BroadcastableOp""]\n\n\nclass Operation:\n    """""" Base class for all tensor operations that support back-propagation\n        of gradients.\n\n        Consider the Operation-instance ``f``. A forward-pass through ``f`` is defined\n        via ``f.__call__``. Thus, given tensors ``a`` and ``b``, a computational\n        graph is defined ``f.__call__(a, b) -> c``, where the ""creator"" of tensor ``c``\n        is recorded as ``f``::\n\n              (node: a) --+\n                           -> [operation: f(a, b)] --> (node: c)\n              (node: b) --+\n\n        Thus back-propagating through ``c`` will instruct ``f`` to back-propagate\n        the gradient to its inputs, which are recorded as ``a`` and ``b``. Each\n        node then back-propagates to any Operation-instance that is recorded\n        as its creator, and so on.\n\n        If an operation class has `scalar_only=True`, then the terminal node of a\n        computational graph involving that operation can only trigger back-propagation\n        from a 0-dimensional tensor (i.e. a scalar). This is `False` for operations that\n        manifest as trivial element-wise operations over tensors. In such cases, the\n        gradient of the operation can also be treated element-wise, and thus be computed\n        unambiguously.\n        """"""\n\n    # tracks if a given operation-instance performs a\n    # non-vectorized or broadcasted operation , which\n    # requires that backpropagation be invoked from a scalar\n    scalar_only = False  # type: bool\n\n    # stores a set of all the operation-instances that participate in\n    # the computational graph up to and including the present operation\n    graph = None  # type: Optional[Set[Operation]]\n\n    def __call__(self, *input_vars, **kwargs):  # pragma: no cover\n        """""" Performs a forward pass, f, of this Operation::\n\n            f(x1, ...., xn) -> out\n\n        Parameters\n        ----------\n        *input_vars : mygrad.Tensor\n            The input-arguments of f. The tuple (x1, ...., xn)\n            should be bound to the instance-attribute `self.variables`\n\n        **kwargs : Any\n            Additional arguments for the operation\n\n        Returns\n        -------\n        numpy.ndarray\n            The output of the forward pass function.""""""\n\n        self.variables = input_vars\n        raise NotImplementedError\n\n    def backward_var(self, grad, index, **kwargs):  # pragma: no cover\n        """""" Given ``grad = d(out)/d(f)``, computes ``d(out)/d(var)``, and passes this result\n        to ``var.backward()``, where var is the tensor-argument at position ``index``.\n\n        Parameters\n        ----------\n        grad : numpy.ndarray\n            The back-propagated total derivative with respect to the present\n            operation (`f`): d(out)/df\n\n        index : int\n            The index-location of ``var`` in ``self.variables``\n\n        Other Parameters\n        ----------------\n        _broadcastable : bool, optional (default:False)\n            Devs-only: Indicates whether or not the up-stream operation\n            can utilize broadcasting.\n\n        Raises\n        ------\n        SkipGradient""""""\n        raise NotImplementedError\n\n    def backward(self, grad, *, graph, _reduction=None, **kwargs):\n        """""" Back-propagates the gradient through all of the operation\'s inputs.\n        Constant tensors do not propagate a gradient.\n\n        Parameters\n        ----------\n        grad : numpy.ndarray\n            The back-propagated total derivative with respect to the present\n            operation (`f`): d(out)/df\n\n        graph : Set[Operation]\n            The set of all operations relevant to the terminal node of the computational graph,\n            which triggered back-propagation.\n\n        _reduction : Optional[Callable[[ndarray, Tuple[int, ...]], ndarray]]\n            Developer option-only. A callable used to process the gradient\n            prior to accumulation (e.g. broadcast-reduction)\n        """"""\n        for index, var in enumerate(self.variables):\n            if not var.constant:\n                if not var._ops:\n                    raise InvalidBackprop(\n                        ""Part of the computational graph containing ""\n                        ""this tensor was \'cleared\' prior to backprop.""\n                    )\n\n                try:\n                    backed_grad = self.backward_var(grad, index, **kwargs)\n                except SkipGradient:\n                    continue\n\n                if is_invalid_gradient(backed_grad):\n                    raise InvalidGradient(\n                        f""An invalid gradient-value was passed to:""\n                        f""\\n\\t`{type(self).__name__}.backward_var(<gradient>, index={index})`""\n                        f""\\nGradients are expected to be real-valued scalars or ""\n                        f""numpy arrays, got a gradient of type: {type(backed_grad)}""\n                    )\n                if var.grad is None:\n                    tmp_grad = np.asarray(backed_grad)\n\n                    if _reduction is not None:\n                        tmp_grad = _reduction(tmp_grad, var.shape)\n\n                    var.grad = (\n                        np.copy(tmp_grad)\n                        if np.shares_memory(tmp_grad, grad)\n                        else tmp_grad\n                    )\n                else:\n                    if _reduction is None:\n                        var.grad += backed_grad\n                    else:\n                        var.grad += _reduction(backed_grad, var.shape)\n        for var in {\n            i for i in self.variables if not i.constant and i.creator is not None\n        }:\n            var._accum_ops.add(self)\n            var._backward(graph=graph)\n\n\nclass BroadcastableOp(Operation):\n    """""" Signals that an Operation\'s forward pass can broadcast its tensor arguments.""""""\n\n    def backward(self, grad, *, graph, _reduction=None, **kwargs):\n        return super().backward(grad, graph=graph, _reduction=reduce_broadcast)\n'"
src/mygrad/tensor_base.py,13,"b'""""""\nThis module defines the base tensor class along with all of its essential\nattributes and special methods. Public math methods, e.g. ``sum``, ``mean``,\netc., are bound to the Tensor class in ``mygrad.__init__.py``.\n""""""\n\nfrom functools import wraps\nfrom typing import Optional, Set, Type, Union\n\nimport numpy as np\n\nfrom mygrad._utils import is_invalid_gradient\nfrom mygrad.errors import InvalidBackprop, InvalidGradient\nfrom mygrad.linalg.ops import MatMul\nfrom mygrad.math.arithmetic.ops import (\n    Add,\n    Divide,\n    Multiply,\n    Negative,\n    Positive,\n    Power,\n    Subtract,\n)\nfrom mygrad.operation_base import BroadcastableOp, Operation\nfrom mygrad.tensor_core_ops.indexing import GetItem, SetItem\nfrom mygrad.tensor_manip.array_shape.ops import Flatten\nfrom mygrad.tensor_manip.transpose_like.ops import Tensor_Transpose_Property\n\n__all__ = [""Tensor""]\n\n\nclass Tensor:\n    """""" A numpy-array-like object capable of serving as a node in a computational\n    graph that supports back-propagation of derivatives via the chain rule.\n    See the Examples section of the docstring for more details.\n\n    Like the numpy array, mygrad\'s tensor stores data as an N-dimensional array\n    and provides an interface accessing, setting, and performing vectorized\n    operations along the various dimensions of this array. Vectorized operations\n    support numpy-style broadcasting semantics.\n\n    The contents of a tensor can be accessed and written to using all variety\n    of basic and advanced indexing (along with mixtures of the two).\n\n    Creating a Tensor\n    -----------------\n    ``mygrad.Tensor`` can be passed any ""array-like"" object of numerical data.\n    This includes numbers, sequences (e.g. lists), nested sequences, numpy-ndarrays,\n    and other mygrad-tensors. mygrad also provides familiar numpy-style tensor-creation\n    functions (e.g. ``mygrad.arange``, ``mygrad.linspace``, etc.)\n\n    >>> import mygrad as mg\n    >>> mg.Tensor(2.3)  # creating a 0-dimensional tensor\n    Tensor(2.3)\n    >>> mg.Tensor(np.array([1.2, 3.0]))  # casting a numpy-array to a tensor\n    Tensor([1.2, 3.0])\n    >>> mg.Tensor([[1, 2], [3, 4]])  # creating a 2-dimensional tensor\n    Tensor([[1, 2],\n            [3, 4]])\n    >>> mg.arange(4)    # using numpy-style tensor creation functions\n    Tensor([0, 1, 2, 3])\n\n\n    Forward and Back-Propagation\n    ----------------------------\n    Let\'s construct a computational graph consisting of two zero-dimensional\n    tensors, ``x`` and ``y``, which are used to compute an output tensor,\n    ``f``. This is a ""forward pass imperative"" style for creating a computational\n    graph - the graph is constructed as we carry out the forward-pass computation.\n\n    >>> x = Tensor(3.0)\n    >>> y = Tensor(2.0)\n    >>> f = 2 * x + y ** 2\n\n    Invoking ``f.backward()`` signals the computational graph to\n    compute the total-derivative of ``f`` with respect to each one of its dependent\n    variables. I.e. ``x.grad`` will store ``df/dx`` and ``y.grad`` will store\n    ``df/dy``. Thus we have back-propagated a gradient from ``f`` through our graph.\n\n    Each tensor of derivatives is computed elementwise. That is, if `x = Tensor(x0, x1, x2)`,\n    then df/dx represents `[df/d(x0), df/d(x1), df/d(x2)]`\n\n    >>> f.backward()  # computes df/dx and df/dy\n    >>> x.grad  # df/dx\n    array(6.0)\n    >>> y.grad  # df/dy\n    array(4.0)\n    >>> f.grad\n    array(1.0)  # df/df\n\n    Before utilizing ``x`` and ``y`` in a new computational graph, you must\n    \'clear\' their stored derivative values. ``f.null_gradients()`` signals\n    ``f`` and all preceding tensors in its computational graph to clear their\n    derivatives.\n\n    >>> f.null_gradients()\n    >>> x.grad is None and y.grad is None and f.grad is None\n    True\n\n    Accessing the Underlying NumPy Array\n    ------------------------------------\n    ``mygrad.Tensor`` is a thin wrapper on ``numpy.ndarray``. A tensor\'s\n    underlying numpy-array can be accessed via ``.data``:\n\n    >>> x = mg.Tensor([1, 2])\n    >>> x.data\n    array([1, 2])\n\n    **Do not modify this underlying array**. Any in-place modifications made to this\n    array will not be tracked by any computational graph involving that tensor, thus\n    back-propagation through that tensor will likely be incorrect.""""""\n\n    __array_priority__ = 15.0\n\n    def __init__(\n        self, x, *, dtype=None, constant=False, _scalar_only=False, _creator=None\n    ):\n        """"""\n        Parameters\n        ----------\n        x : array_like\n            Input data, in any form that can be converted to an array.  This\n            includes numbers, sequences, nested sequences, numpy-ndarrays,\n            and mygrad-tensors.\n\n        dtype : Optional[type]\n            `int`, `float`, or a real-valued numpy data type. By default the\n            data type is inferred from ``x`` via ``numpy.asarray(x)``.\n\n        constant : bool, optional (default=False)\n            If True, this node is treated as a constant, and thus does not facilitate\n            back propagation; `self.grad` will always return `None`.\n\n        _scalar_only : bool, optional (default=False)\n            Signals that self.backward() can only be invoked if self.ndim == 0.\n            Should not be set manually by users.\n\n        _creator: Optional[mygrad.Operation]\n            The operation-instance whose forward pass produced `self`. Should not\n            be set manually by users.\n        """"""\n        assert isinstance(constant, bool)\n        self._scalar_only = _scalar_only\n        self._creator = _creator  # type: Union[None, Operation]\n\n        if isinstance(x, Tensor):\n            self.data = x.data\n        else:\n            self.data = np.asarray(x, dtype=dtype)\n            self._check_valid_dtype(self.data.dtype)\n\n        self.grad = None  # type: Union[None, np.ndarray]\n        self._constant = constant\n\n        # track all operations that this tensor participates in\n        self._ops = set()  # type: Set[Operation]\n\n        # track the operations that have contributed to this tensor\'s gradient during a back-prop\n        self._accum_ops = set()  # type: Set[Operation]\n\n    def astype(\n        self, dtype: Union[type, str], *, constant: Optional[bool] = None\n    ) -> ""Tensor"":\n        """"""Returns a distinct tensor with its data modified to have the specified\n        data type.\n\n        The resulting tensor does not belong to any pre-existing computation graph; i.e.\n        it is as if this tensor was created \'from scratch\'.\n\n        Parameters\n        ----------\n        dtype : Union[type, str]\n            The real-valued numeric data type. This can be a numpy dtype or\n            a corresponding string identifier.\n\n        constant : Optional[bool]\n            If specified, determines if the returned tensor is a constant.\n            Otherwise this argument is inferred from the original tensor.\n\n        Returns\n        -------\n        Tensor\n            The resulting tensor with the specified data type.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> import numpy as np\n        >>> x = mg.arange(10); x\n        Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n        Using a string to specify the data type:\n\n        >>> x.astype(""float32"")\n        Tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)\n\n        Specifying a numpy data type object, and specifying that the\n        tensor is to be treated as a constant:\n\n        >>> x.astype(np.int8, constant=True)\n        Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int8)\n        """"""\n        constant = constant if constant is not None else self.constant\n        return type(self)(self.data.astype(dtype), constant=constant)\n\n    @staticmethod\n    def _check_valid_dtype(dtype):\n        if not np.issubdtype(dtype, np.number):\n            raise TypeError(f""Tensor data must be a numeric type, received {dtype}"")\n\n    @classmethod\n    def _op(\n        cls,\n        Op: Type[Operation],\n        *input_vars,\n        op_args=None,\n        op_kwargs=None,\n        constant=False,\n    ):\n        """"""Wraps operations performed between tensors: f(a, b, ...).\n\n        For developer use only.\n\n        Parameters\n        ----------\n        Op : Type[Operation]\n            Operation-class, used to perform forward-pass on `input_vars`.\n\n        input_vars : Tuple[array_like, ...]\n            An arbitrary number of input-tensors. These can take any form that\n            can be converted to an array.  This includes numbers, sequences, nested\n            numerical sequences, numpy-ndarrays, and mygrad-tensors.\n\n        op_args : Optional[Tuple[Any, ...]]\n            Arbitrary positional arguments passed to the operation\'s forward pass.\n\n        op_kwargs : Optional[Dict[str, Any]]\n            Arbitrary keyword arguments passed to the operation\'s forward pass.\n\n        constant : bool, optional (default=False)\n            If True, the resulting Tensor is a constant.\n\n        Returns\n        -------\n        mygrad.Tensor\n            The tensor-result of the operation\'s forward-pass.""""""\n\n        if op_args is None:\n            op_args = tuple()\n\n        if op_kwargs is None:\n            op_kwargs = dict()\n\n        tensor_vars = tuple(\n            cls(var, constant=True) if not isinstance(var, cls) else var\n            for var in input_vars\n        )\n\n        is_const = constant or all(var.constant for var in tensor_vars)\n\n        f = Op()\n        f.graph = {f}\n        f.graph.update(\n            *(\n                var._creator.graph\n                for var in tensor_vars\n                if var._creator is not None and not var.constant\n            )\n        )\n        op_out = f(*tensor_vars, *op_args, **op_kwargs)\n\n        if isinstance(f, BroadcastableOp) and not f.scalar_only:\n            # if broadcasting occurred: scalar-only -> True\n            f.scalar_only = any(\n                op_out.shape != i.shape for i in tensor_vars if not i.constant\n            )\n\n        if not is_const:\n            # record that a variable participated in that op\n            for var in tensor_vars:\n                if not var.constant:\n                    var._ops.add(f)\n\n        scalar_only = f.scalar_only and not is_const\n        for var in tensor_vars:\n            scalar_only = scalar_only or (var.scalar_only and not var.constant)\n\n        return cls(op_out, constant=is_const, _creator=f, _scalar_only=scalar_only)\n\n    def backward(self, grad=None):\n        """""" Compute set or accumulate ``self.grad`` with `grad`, and pass ``self.creator.backward(grad)``.\n        In effect, calling ``self.backward()`` will trigger a ""back-propagation"" from ``self`` through\n        the preceding nodes in the computational graph. Thus a node, ``a``, will have the attribute\n        ``self.grad`` return the total derivative `d(self)/da`.\n\n        Parameters\n        ----------\n        grad : Optional[array_like]\n            The value of the incoming derivative. If self.grad is None, it is set to `grad`,\n            otherwise its value is added with `grad`.\n\n        Raises\n        ------\n        Exception\n            The configuration of the computational graph is such that ``self`` must be a 0D tensor\n            (i.e. scalar) to invoke ``self.backward()``.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor(2)\n        >>> y = mg.Tensor(3)\n        >>> w = x * y\n        >>> f = 2 * w\n        >>> f.backward()  # computes df/df, df/dw, df/dy, and df/dx\n\n        >>> f.grad  # df/df == 1 by identity\n        array(1.)\n        >>> w.grad  # df/dw\n        array(2.)\n        >>> y.grad # df/dy = df/dw * dw/dy\n        array(4.)\n        >>> x.grad # df/dx = df/dw * dw/dx\n        array(6.)\n        """"""\n        if self._constant:\n            return\n\n        if grad is not None:\n            self.grad = np.asarray(grad.data if isinstance(grad, Tensor) else grad)\n            if is_invalid_gradient(self.grad):\n                raise InvalidGradient(\n                    f""An invalid gradient-value was passed to ""\n                    f""\\n\\t`{type(self).__name__}.backward(<gradient>)`""\n                    f""\\nGradients are expected to be real-valued scalars or ""\n                    f""numpy arrays, got a gradient of type: {type(grad)}""\n                )\n\n        else:\n            if self.ndim > 0 and self._scalar_only:\n                raise InvalidBackprop(\n                    ""Backpropagation must be invoked from a ""\n                    ""scalar-tensor (a 0D tensor) for this computational ""\n                    ""graph.""\n                )\n            dtype = float if np.issubdtype(self.dtype, np.signedinteger) else self.dtype\n            self.grad = (\n                np.ones(self.shape, dtype=dtype)\n                if self.ndim > 0\n                else np.asarray(1.0, dtype=dtype)\n            )\n\n        if self.creator is not None:\n            self._backward(graph=self.creator.graph)\n\n    def _backward(self, *, graph):\n        """"""\n        **For dev-use only**\n\n        If `self` has accumulated incoming gradients from all operations in the terminal node\'s\n        computational graph, back-propagate the accumulated gradient to the creator of `self`.\n\n        Parameters\n        ----------\n        graph : Set[Operation]\n            The set of all operations relevant to the terminal node of the computational graph,\n            which triggered back-propagation\n\n        Raises\n        ------\n        AssertionError\n            Raises if the tensor and its associated gradient possess different shapes.\n            Raises if `_backward` triggered on a tensor with gradient of `None`.\n        """"""\n        assert self.grad is not None, (\n            ""backprop, post grad-accumulation, was triggered ""\n            ""on a tensor with no gradient""\n        )\n        assert self.grad.shape == self.shape, (\n            f""A tensor and its associated gradient must possess the same shape. Got:""\n            f""\\ntensor-shape: {self.shape}""\n            f""\\ngrad-shape: {self.grad.shape}""\n        )\n        if self._creator is not None and not bool(\n            graph & (self._ops - self._accum_ops)\n        ):\n            self._accum_ops.clear()\n            self._creator.backward(self.grad, graph=graph)\n\n    def null_gradients(self, clear_graph=True):\n        """"""\n        Sets the gradient for this tensor and for all preceding tensors in the computation graph\n        to ``None``.\n\n        Additionally, the computational graph that terminates in this tensor can also be cleared\n        during this process.\n\n        Parameters\n        ----------\n        clear_graph : bool, optional (default=True)\n            If ``True`` clear the computational graph in addition to nulling the gradients.\n\n        Notes\n        -----\n        It is advised to clear the computational graph when nulling gradients, i.e. invoke\n        ``null_gradients(clear_graph=True)`` (or simply ``null_gradients()``). This de-references\n        all intermediate operations and tensors in the computational graph and thus permits\n        garbage collection - freeing the memory that was used by the computational graph.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor(2)\n        >>> y = mg.Tensor(3)\n        >>> w = x * y\n        >>> f = 2 * w\n        >>> f.backward()  # computes df/df, df/dw, df/dy, and df/dx\n        >>> any(tensor.grad is None for tensor in (f, w , x, y))\n        False\n\n        >>> f.null_gradients()  # set tensor.grad to None for all tensors in the graph\n        >>> all(tensor.grad is None for tensor in (f, w , x, y))\n        True\n        """"""\n        self._null_gradients(clear_graph=clear_graph, seen=set())\n\n    def _null_gradients(self, *, clear_graph: bool, seen: Set[""Tensor""]):\n        """"""\n        Nulls gradients using depth-first graph traversal\n\n        Parameters\n        ----------\n        clear_graph : bool, optional (default=True)\n            If ``True`` clear the computational graph in addition to nulling the gradients.\n\n        seen : Set[Tensor]\n            The set of all Tensors already visited during null-gradients traversal""""""\n        self.grad = None\n\n        if self._creator is None:\n            return\n\n        if not clear_graph:\n            assert isinstance(seen, set)\n\n            if self in seen:\n                return\n\n        creator = self._creator\n\n        if clear_graph:\n            # marks tensor as ""visited"" during clear-graph traversal\n            self._creator = None\n        else:\n            # marks tensor as ""visited"" during null-gradients graph traversal\n            seen.add(self)\n\n        for var in creator.variables:  # type: Tensor\n            if clear_graph:\n                var._ops.clear()\n            var._null_gradients(clear_graph=clear_graph, seen=seen)\n\n    def clear_graph(self):\n        """"""\n        Clear the computational graph for all of the nodes preceding this tensor.\n        """"""\n        if self._creator is None:\n            return\n\n        creator = self._creator\n        self._creator = None  # marks tensor as ""visited"" during graph-traversal\n\n        for var in creator.variables:  # type: Tensor\n            var._ops.clear()\n            var.clear_graph()\n\n    @property\n    def scalar_only(self):\n        """""" Indicates whether or not `self.ndim` must be 0 in order to invoke `self.backward()`.\n\n        E.g. a computational graph that involves a broadcast-multiplication of a non-constant\n        tensor can only support back-propagation from a scalar. Otherwise the gradient associated\n        with each element of a given tensor would, itself, have to be represented by a high-dimensional\n        tensor. MyGrad only supports computational graphs in which a tensor\'s gradient has the same\n        shape as the tensor itself.\n\n        Returns\n        -------\n        bool\n\n        Notes\n        -----\n        In the following example, see that, because ``x`` was broadcasted to a\n        shape-(2, 3) tensor, the derivative :math:`df/dx` could not be a shape-(3,) tensor.\n        Each element of ``x`` affects two entries of ``z``, thus :math:`df/dx`\n        would have to be a shape-(2, 3) array. Therefore ``z.scalar_only`` returns ``True``.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor([1., 2., 3.])  # shape-(3,)\n        >>> y = mg.Tensor([[4., 1., 9.], # shape-(2, 3)\n        ...                [0., 2., 8.]])\n        >>> z = x * y  # uses numpy-style broadcasting\n        >>> z.scalar_only\n        True\n        """"""\n        return self._scalar_only\n\n    @property\n    def constant(self):\n        """""" If ``True``, this tensor is a constant; it will not propagate any gradient.\n\n        Additionally, any tensor that is a descendant of constant tensors will also\n        be a constant.\n\n        Python scalars and NumPy arrays are treated as constant tensors when included\n        in MyGrad computational graphs.\n\n        Returns\n        -------\n        bool\n\n        Examples\n        --------\n        Constant-tensors do not back-propagate gradients:\n\n        >>> import mygrad as mg\n        >>> x = mg.Tensor([1., 2.], constant=True)\n        >>> y = mg.Tensor([0., 3.], constant=False)\n        >>> f = x * y\n        >>> f.backward()\n\n        >>> x.grad is None  # x has no gradient\n        True\n        >>> y.grad\n        array([1., 2.])\n\n        A tensor that is derived solely from constant tensors is also\n        a constant:\n\n        >>> import numpy as np\n        >>> x = mg.Tensor([1., 2.], constant=True)\n        >>> y = mg.Tensor([0., 3.], constant=True)\n        >>> z = (x + y) ** 2 - np.array([8., 7.])\n        >>> z.constant\n        True\n        """"""\n        return self._constant\n\n    @property\n    def creator(self) -> Union[Operation, BroadcastableOp]:\n        """""" The ``Operation`` instance that produced ``self``.\n\n        Returns\n        -------\n        Operation\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor(3)\n        >>> y = mg.Tensor(2)\n        >>> z = x * y  # Multiply(x, y) -> z\n        >>> z.creator\n         <mygrad.math.arithmetic.ops.Multiply at 0x2df5a130438>\n        """"""\n        return self._creator\n\n    def __len__(self):\n        return len(self.data)\n\n    def __contains__(self, item):\n        return self.data.__contains__(item)\n\n    def __getitem__(self, item):\n        # In the same way that numpy doesn\'t let you iterate over 0-dimensional\n        # arrays, don\'t allow iteration over 0-dimensional arrays.\n        if self.ndim == 0:\n            raise TypeError(""iteration over a 0-d tensor"")\n        return self._op(GetItem, self, op_args=(item,))\n\n    def __setitem__(self, key, value):\n        if self.constant and (not isinstance(value, Tensor) or value.constant):\n            self.data[key] = value.data if isinstance(value, Tensor) else value\n            return None\n\n        # old_tensor is the tensor pre-setitem\n        old_tensor = Tensor(\n            self,\n            constant=self.constant,\n            _scalar_only=self._scalar_only,\n            _creator=self.creator,\n        )\n        old_tensor._ops = self._ops\n        old_tensor._accum_ops = self._accum_ops\n\n        # point all ops involving `self` to old_tensor instead\n        for op in old_tensor._ops:\n            for i in range(len(op.variables)):\n                if op.variables[i] is self:\n                    op.variables = (\n                        op.variables[:i] + (old_tensor,) + op.variables[i + 1 :]\n                    )\n\n        # self becomes the tensor post-setitem\n        out = self._op(SetItem, old_tensor, value, op_args=(key,),)\n        self._creator = out.creator\n        self._scalar_only = out._scalar_only\n        self._ops = out._ops\n        self._accum_ops = out._accum_ops\n        self.data = out.data\n        self._constant = out.constant\n\n    def __add__(self, other):\n        return self._op(Add, self, other)\n\n    def __radd__(self, other):\n        return self._op(Add, other, self)\n\n    def __sub__(self, other):\n        return self._op(Subtract, self, other)\n\n    def __rsub__(self, other):\n        return self._op(Subtract, other, self)\n\n    def __truediv__(self, other):\n        return self._op(Divide, self, other)\n\n    def __rtruediv__(self, other):\n        return self._op(Divide, other, self)\n\n    def __mul__(self, other):\n        return self._op(Multiply, self, other)\n\n    def __rmul__(self, other):\n        return self._op(Multiply, other, self)\n\n    def __matmul__(self, other):\n        return self._op(MatMul, self, other)\n\n    def __rmatmul__(self, other):\n        return self._op(MatMul, other, self)\n\n    def __pow__(self, other):\n        return self._op(Power, self, other)\n\n    def __rpow__(self, other):\n        return self._op(Power, other, self)\n\n    def __neg__(self):\n        return self._op(Negative, self)\n\n    def __pos__(self):\n        return self._op(Positive, self)\n\n    def __repr__(self):\n        return repr(self.data).replace(""array"", ""Tensor"").replace(""\\n"", ""\\n "")\n\n    def __copy__(self):\n        """""" Produces a copy of ``self`` with ``copy.creator=None``.\n\n        Copies of the underlying numpy data array and gradient array are created.\n\n        Returns\n        -------\n        Tensor\n        """"""\n        copy = Tensor(\n            np.copy(self.data),\n            _creator=None,\n            constant=self.constant,\n            _scalar_only=self._scalar_only,\n        )\n        copy.grad = np.copy(self.grad) if self.grad is not None else None\n        return copy\n\n    def copy(self):\n        """""" Produces a copy of ``self`` with ``copy.creator=None``.\n\n        Copies of the underlying numpy data array and gradient array are created.\n\n        Returns\n        -------\n        Tensor\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor(data, constant=constant)\n        >>> y = x * 2\n        >>> y.backward()\n        >>> y_copy = y.copy()\n        >>> y_copy\n        Tensor(6)\n        >>> y_copy.grad\n        array(1.)\n        >>> y_copy.creator is None\n        True\n        """"""\n        return self.__copy__()\n\n    def item(self):\n        """""" Copy an element of a tensor to a standard Python scalar and return it.\n\n        Note that the returned object does not support back-propagation.\n\n        Returns\n        -------\n        z : Standard Python scalar object\n            A copy of the specified element of the tensor as a suitable\n            Python scalar\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = Tensor([22.2])\n        >>> x.item()\n        22.2\n        >>> type(x.item())\n        float """"""\n        if self.size > 1:\n            raise ValueError(""can only convert a tensor of size 1 to a Python scalar"")\n        return self.data.item()\n\n    def __float__(self):\n        if self.size > 1:\n            raise TypeError(""can only convert a tensor of size 1 to a Python scalar"")\n        return float(self.data)\n\n    def __int__(self):\n        if self.size > 1:\n            raise TypeError(""can only convert a tensor of size 1 to a Python scalar"")\n        return int(self.data)\n\n    def flatten(self, constant=False):\n        """""" Return a copy of the tensor collapsed into one dimension.\n\n        This docstring was adapted from ``numpy.ndarray.flatten``.\n\n        Returns\n        -------\n        mygrad.Tensor\n            A copy of the input tensor, flattened to one dimension.\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Notes\n        -----\n        To return a flattened view of the tensor, use ``x.reshape(-1)``.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor([[1, 2],\n        ...                [3, 4]])\n        >>> x.flatten()\n        Tensor([1, 2, 3, 4])\n        """"""\n        return Tensor._op(Flatten, self, constant=constant)\n\n    @property\n    def size(self):\n        """"""\n        Number of elements in the tensor. i.e., the product of the tensor\'s\n        dimensions.\n\n        Returns\n        -------\n        int\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.zeros((3, 5, 2))  # creates a tensor with 3x5x2 (= 30) elements\n        >>> x.size\n        30\n        """"""\n        return self.data.size\n\n    @property\n    def ndim(self):\n        """""" Number of tensor dimensions. I.e. the number\n        of indices that must be supplied to uniquely specify\n        an element in the tensor.\n\n        Returns\n        -------\n        int\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor([1, 2, 3])\n        >>> x.ndim\n        1\n        >>> x[0]  # a single index identifies an element in `x`\n        Tensor(0)\n\n        >>> y = mg.Tensor([[1, 2, 3],\n        ...                [4, 5, 6]])\n        >>> y.ndim\n        2\n        >>> y[0, 0]  # two indices are required to identify an element in `x`\n        Tensor(0)""""""\n        return self.data.ndim\n\n    @property\n    def dtype(self):\n        """""" Data-type of the tensor\'s elements.\n\n        Returns\n        -------\n        numpy dtype object\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor([[0, 1],\n        ...                [2, 3]])\n        >>> x.dtype\n        dtype(\'int32\')\n        >>> type(x.dtype)\n        <type \'numpy.dtype\'>""""""\n        return self.data.dtype\n\n    @property\n    def shape(self):\n        """""" Tuple of tensor dimension-sizes.\n\n        Sizes are reported in row-major order.\n\n        Returns\n        -------\n        Tuple[int, ...]\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.Tensor([1, 2, 3, 4])  # axis-0 has size 4\n        >>> x.shape\n        (4,)\n        >>> y = mg.Tensor([[1, 2, 3],    # axis-0 has size 2, axis-1 has size 3\n        ...                [4, 5, 6]])\n        >>> y.shape\n        (2, 3)\n\n        See Also\n        --------\n        mygrad.reshape : similar function\n        Tensor.reshape : similar method""""""\n        return self.data.shape\n\n    @property\n    def T(self):\n        """""" Same as self.transpose(), except that self is returned if self.ndim < 2 and\n        a view of the underlying data is utilized whenever possible.\n\n        Returns\n        -------\n        Tensor\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> y = mg.Tensor([[1, 2, 3],\n        ...                [4, 5, 6]])\n        >>> y.T\n        Tensor([[1, 4],\n                [2, 5],\n                [3, 6]])\n        """"""\n        return self._op(Tensor_Transpose_Property, self)\n\n\n# set all comparison operators - mirrors ndarray methods\ndef tensor_to_array_wrapper(func):\n    @wraps(func)\n    def wrapped(x, y):\n        return func(x.data, y.data if isinstance(y, Tensor) else y)\n\n    return wrapped\n\n\nfor op in (""__lt__"", ""__le__"", ""__gt__"", ""__ge__"", ""__eq__"", ""__ne__""):\n    setattr(Tensor, op, tensor_to_array_wrapper(getattr(np.ndarray, op)))\n'"
tests/custom_strategies/__init__.py,5,"b'"""""" Custom hypothesis search strategies """"""\nimport math\nfrom functools import partial\nfrom numbers import Integral\nfrom typing import Any, Iterable, Optional, Tuple, Union\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nfrom hypothesis.extra.numpy import broadcastable_shapes\nfrom numpy import ndarray\n\n__all__ = [\n    ""adv_integer_index"",\n    ""basic_indices"",\n    ""broadcastable_shapes"",\n    ""choices"",\n    ""everything_except"",\n    ""valid_axes"",\n]\n\nShape = Tuple[int, ...]\n\nbasic_indices = partial(hnp.basic_indices, allow_newaxis=True, allow_ellipsis=True)\n\n\ndef everything_except(\n    excluded_types: Union[type, Tuple[type, ...]]\n) -> st.SearchStrategy[Any]:\n    """"""Returns hypothesis strategy that generates values of any type other than\n    those specified in ``excluded_types``.""""""\n    return (\n        st.from_type(type)\n        .flatmap(st.from_type)\n        .filter(lambda x: not isinstance(x, excluded_types))\n    )\n\n\ndef _check_min_max(min_val, min_dim, max_dim, param_name, max_val=None):\n    """"""Ensures that:\n        min_val <= min_dim\n        min_val <= max_dim\n        min_val <= max_val\n    If `max_val` is specified, ensures that `max_dim <= max_val`\n\n    Raises\n    ------\n    ValueError""""""\n    if not isinstance(min_dim, Integral) or min_dim < min_val:\n        raise ValueError(\n            f""`min_{param_name}` must be larger than {min_val}. "" f""Got {min_dim}""\n        )\n\n    if not isinstance(max_dim, Integral) or max_dim < min_dim:\n        raise ValueError(\n            f""`max_{param_name}` must be an integer that is ""\n            f""not smaller than `min_{param_name}`. Got {max_dim}""\n        )\n    if max_val is not None and max_dim > max_val:\n        raise ValueError(\n            f""`min_{param_name}` cannot be larger than {max_val}. "" f""Got {max_dim}""\n        )\n\n    if max_dim < min_dim:\n        raise ValueError(\n            f""`min_{param_name}={min_dim}` cannot be larger than max_{param_name}={max_dim}.""\n        )\n\n\ndef choices(seq, size, replace=True):\n    """"""Randomly choose elements from `seq`, producing a tuple of length `size`.\n\n    Examples from this strategy shrink towards `tuple(seq[:size])` when `replace=False.\n    Examples from this strategy shrink towards `(seq[0], ) * size` when `replace=True.\n\n    Parameters\n    ----------\n    seq : Sequence[Any]\n    size : int\n    replace : bool\n\n    Returns\n    -------\n    hypothesis.strategiesSearchStrategy[Tuple[Any, ...]]\n        A tuple of length `size` containing elements of `seq`""""""\n    if not isinstance(size, Integral) or size < 0:\n        raise ValueError(f""`size` must be a non-negative integer. Got {size}"")\n    if size > len(seq) and not replace:\n        raise ValueError(\n            ""`size` must not exceed the length of `seq` when `replace` is `False`""\n        )\n    if not seq:\n        if size:\n            raise ValueError(""`size` must be 0, given an empty `seq`"")\n        return st.just(())\n    return st.lists(\n        st.sampled_from(range(len(seq))),\n        min_size=size,\n        max_size=size,\n        unique=not replace,\n    ).map(lambda x: tuple(seq[i] for i in x))\n\n\ndef _to_positive(x: Union[int, Iterable], ndim: int) -> Union[int, Tuple[int, ...]]:\n    if hasattr(x, ""__iter__""):\n        return tuple(_to_positive(i, ndim) for i in x)\n    return x if -1 < x else ndim + x\n\n\ndef valid_axes(\n    ndim: int,\n    pos_only: bool = False,\n    single_axis_only: bool = False,\n    permit_none: bool = True,\n    permit_int: bool = True,\n    min_dim: int = 0,\n    max_dim: Optional[int] = None,\n) -> st.SearchStrategy[Union[None, int, Tuple[int, ...]]]:\n    """""" Hypothesis search strategy: Given array dimensionality, generate valid\n    `axis` arguments (including `None`) for numpy\'s sequential functions.\n\n    Examples from this strategy shrink towards an empty tuple of axes.\n    If `single_axis_only=True`, then it shrinks towards 0.\n\n    Parameters\n    ----------\n    ndim : int\n        The dimensionality of the array.\n\n    pos_only : bool, optional (default=False)\n        If True, the returned value(s) will be positive.\n\n    single_axis_only : bool, optional (default=False)\n        If True, a single integer axis or `None` (assuming `permit_none=True`)\n        will be returned.\n\n    permit_none : bool, optional (default=True)\n        If True, `None` may be returned instead of a tuple of all of the\n        available axes.\n\n    permit_int: bool, optional (default=True)\n        If True, the returned value may be an integer\n\n    min_dim : int, optional (default=0)\n        The smallest number of entries permitted in the returned tuple of axes\n\n    max_dim : Optional[int]\n        The largest number of entries permitted in the returned tuple of axes.\n        The defaults is ``ndim``.\n\n    Returns\n    -------\n    st.SearchStrategy[Union[None, int, Tuple[int, ...]]]\n\n    Examples\n    --------\n    >>> valid_axes(4).example()\n    (0, 1)\n    """"""\n    if isinstance(ndim, (tuple, list)):\n        ndim = len(ndim)\n\n    single_axis_strat = st.integers(-ndim, ndim - 1) if ndim else st.just(0)\n\n    strats = []\n\n    if permit_none:\n        strats.append(st.none())\n\n    if permit_int and min_dim <= 1 and (max_dim is None or 1 <= max_dim):\n        strats.append(single_axis_strat)\n\n    if not single_axis_only:\n        strats.append(hnp.valid_tuple_axes(ndim, min_size=min_dim, max_size=max_dim))\n\n    strat = st.one_of(*strats)\n    if pos_only:\n        strat = strat.map(lambda x: x if x is None else _to_positive(x, ndim))\n    return strat\n\n\ndef integer_index(size):\n    """""" Generate a valid integer-index for an axis of a given size,\n    either a positive or negative value: [-size, size).\n\n    Examples from this strategy shrink towards 0.\n\n    Parameters\n    ----------\n    size : int\n        Size of the axis for which the index is drawn\n\n    Returns\n    -------\n    hypothesis.searchstrategy.SearchStrategy[int]\n    """"""\n    return st.integers(-size, size - 1)\n\n\n@st.composite\ndef slice_index(\n    draw,\n    size,\n    min_start=None,\n    max_start=None,\n    min_stop=None,\n    max_stop=None,\n    min_step=1,\n    max_step=2,\n    negative_step=True,\n):\n    """""" Hypothesis search strategy: Generate a valid slice-index\n    for an axis of a given size. Slices are chosen such that\n    most slices will not be empty.\n\n    Examples from this strategy shrink towards `slice(0, 0, 1)`. In the\n    case that a negative step size is drawn, start and stop will be flipped\n    so that it is less likely to have an empty slice\n\n    Parameters\n    ----------\n    size : int\n        Size of the axis for which the index is drawn\n    min_start : int\n    max_start : int\n    min_stop : int\n    max_stop : int\n    min_step : int, optional (default=1)\n    max_step : int\n    negative_step : bool\n\n    Notes\n    -----\n    `draw` is a parameter reserved by hypothesis, and should not be specified\n    by the user.\n\n    Returns\n    -------\n    hypothesis.searchstrategy.SearchStrategy[slice]\n    """"""\n    if not size:\n        return slice(None)\n\n    min_start = -size if min_start is None else min_start\n    max_start = size - 1 if max_start is None else max_start\n    _check_min_max(-math.inf, min_start, max_start, ""start"")\n\n    min_stop = -size if min_stop is None else min_stop\n    max_stop = -size if max_stop is None else max_stop\n    _check_min_max(min_start, min_stop, max_stop, ""stop"")\n\n    _check_min_max(0, min_step, max_step, ""step"")\n\n    start = draw(st.one_of(st.integers(min_start, max_start - 1), st.none()))\n    stop = draw(\n        st.one_of(st.integers(start if start is not None else 0, size), st.none())\n    )\n\n    step = draw(st.integers(min_step, max_step))\n\n    if negative_step:\n        neg_step = draw(st.booleans())\n\n        if neg_step:\n            step *= -1\n    return slice(start, stop, step) if step > 0 else slice(stop, start, step)\n\n\ndef adv_integer_index(\n    shape: Shape,\n    min_dims: int = 1,\n    max_dims: int = 3,\n    min_side: int = 1,\n    max_side: int = 3,\n) -> st.SearchStrategy[Tuple[ndarray, ...]]:\n    """""" Hypothesis search strategy: given an array shape, generate a\n    a valid index for specifying an element/subarray of that array,\n    using advanced indexing with integer-valued arrays.\n\n    Examples from this strategy shrink towards the index\n    `len(shape) * (np.array([0]), )`.\n\n    Parameters\n    ----------\n    shape : Tuple[int, ...]\n        The shape of the array whose indices are being generated\n\n    min_dims : int, optional (default=1)\n        The minimum dimensionality permitted for the index-arrays.\n\n    max_dims : int, optional (default=3)\n        The maximum dimensionality permitted for the index-arrays.\n\n    min_side : int, optional (default=1)\n        The minimum side permitted for the index-arrays.\n\n    max_side : int, optional (default=3)\n        The maximum side permitted for the index-arrays.\n\n    Returns\n    -------\n    hypothesis.searchstrategy.SearchStrategy[Tuple[numpy.ndarray, ...]]\n    """"""\n\n    return hnp.integer_array_indices(\n        shape=shape,\n        result_shape=hnp.array_shapes(\n            min_dims=min_dims, max_dims=max_dims, min_side=min_side, max_side=max_side\n        ),\n    )\n'"
tests/custom_strategies/test_strategies.py,16,"b'from numbers import Real\nfrom typing import List, Tuple\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import given, note\n\nfrom tests.custom_strategies import (\n    adv_integer_index,\n    basic_indices,\n    choices,\n    integer_index,\n    slice_index,\n    valid_axes,\n)\n\n\n@given(seq=st.lists(elements=st.integers()), replace=st.booleans(), data=st.data())\ndef test_choices(seq: List[int], replace: bool, data: st.SearchStrategy):\n    """""" Ensures that the `choices` strategy:\n        - draws from the provided sequence\n        - respects input parameters""""""\n    upper = len(seq) + 10 if replace and seq else len(seq)\n    size = data.draw(st.integers(0, upper), label=""size"")\n    chosen = data.draw(choices(seq, size=size, replace=replace), label=""choices"")\n    assert set(chosen) <= set(seq), (\n        ""choices contains elements that do not "" ""belong to `seq`""\n    )\n    assert len(chosen) == size, ""the number of choices does not match `size`""\n\n    if not replace and len(set(seq)) == len(seq):\n        unique_choices = sorted(set(chosen))\n        assert unique_choices == sorted(chosen), (\n            ""`choices` with `replace=False` draws "" ""elements with replacement""\n        )\n\n\n@given(size=st.integers(1, 10), data=st.data())\ndef test_integer_index(size: int, data: st.SearchStrategy):\n    index = data.draw(integer_index(size), label=""index"")\n    x = np.empty((size,))\n    o = x[index]  # raises if invalid index\n    assert isinstance(\n        o, Real\n    ), ""An integer index should produce a number from a 1D array""\n\n\n@given(size=st.integers(1, 10), data=st.data())\ndef test_slice_index(size: int, data: st.SearchStrategy):\n    index = data.draw(slice_index(size), label=""index"")\n    x = np.empty((size,))\n    o = x[index]  # raises if invalid index\n    assert isinstance(o, np.ndarray) and o.ndim == 1, (\n        ""A slice index should produce "" ""a 1D array from a 1D array""\n    )\n    if o.size:\n        assert np.shares_memory(o, x), ""A slice should produce a view of `x`""\n\n    if index.start is not None:\n        assert -size <= index.start <= size\n\n    if index.stop is not None:\n        assert -size <= index.stop <= size\n\n\n@given(shape=hnp.array_shapes(min_dims=3), data=st.data())\ndef test_basic_index(shape: Tuple[int, ...], data: st.SearchStrategy):\n    min_dim = data.draw(st.integers(0, len(shape) + 2), label=""min_dim"")\n    max_dim = data.draw(st.integers(min_dim, min_dim + len(shape)), label=""max_dim"")\n    index = data.draw(\n        basic_indices(shape=shape, min_dims=min_dim, max_dims=max_dim), label=""index""\n    )\n    x = np.zeros(shape, dtype=int)\n    o = x[index]  # raises if invalid index\n\n    note(f""`x[index]`: {o}"")\n    if o.size and o.ndim > 0:\n        assert np.shares_memory(x, o), (\n            ""The basic index should produce a "" ""view of the original array.""\n        )\n    assert min_dim <= o.ndim <= max_dim, (\n        ""The dimensionality input constraints "" ""were not obeyed""\n    )\n\n\n@given(shape=hnp.array_shapes(min_dims=1), min_dims=st.integers(1, 3), data=st.data())\ndef test_advanced_integer_index(\n    shape: Tuple[int, ...], min_dims: int, data: st.SearchStrategy\n):\n    max_dims = data.draw(st.integers(min_dims, min_dims + 3), label=""max_dims"")\n    index = data.draw(adv_integer_index(shape, min_dims=min_dims, max_dims=max_dims))\n    x = np.zeros(shape)\n    out = x[index]  # raises if the index is invalid\n    note(f""x[index]: {out}"")\n    assert min_dims <= out.ndim <= max_dims, ""The input parameters were not respected""\n    assert not np.shares_memory(\n        x, out\n    ), ""An advanced index should create a copy upon indexing""\n\n\n@given(\n    shape=hnp.array_shapes(min_dims=1, max_dims=5),\n    data=st.data(),\n    permit_none=st.booleans(),\n)\ndef test_valid_single_axis(shape, data, permit_none):\n    axis = data.draw(\n        valid_axes(ndim=len(shape), single_axis_only=True, permit_none=permit_none),\n        label=""axis"",\n    )\n    x = np.empty(shape)\n    np.argmin(x, axis=axis)  # raises if `axis` is invalid\n\n    if not permit_none:\n        assert axis is not None\n\n\n@given(\n    shape=hnp.array_shapes(min_dims=1, max_dims=5),\n    data=st.data(),\n    permit_none=st.booleans(),\n    pos_only=st.booleans(),\n)\ndef test_valid_axes(shape, data, permit_none, pos_only):\n    min_dim = data.draw(st.integers(0, len(shape)), label=""min_dim"")\n    max_dim = data.draw(\n        st.one_of(st.none(), st.integers(min_dim, len(shape))), label=""max_dim""\n    )\n    axis = data.draw(\n        valid_axes(\n            ndim=len(shape),\n            permit_none=permit_none,\n            pos_only=pos_only,\n            min_dim=min_dim,\n            max_dim=max_dim,\n        ),\n        label=""axis"",\n    )\n    x = np.zeros(shape)\n    np.sum(x, axis=axis)\n    if not permit_none:\n        assert axis is not None\n\n    if pos_only and axis is not None:\n        if isinstance(axis, tuple):\n            assert all(i >= 0 for i in axis)\n        else:\n            assert axis >= 0\n\n    if axis is not None:\n        if isinstance(axis, tuple):\n            assert min_dim <= len(axis)\n\n            if max_dim is not None:\n                assert len(axis) <= max_dim\n        else:\n            assert min_dim <= 1\n'"
tests/linalg/__init__.py,0,b''
tests/linalg/test_einsum.py,61,"b'from copy import copy\nfrom itertools import chain\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import assume, given, settings\nfrom numpy.testing import assert_allclose\n\nfrom mygrad import Tensor\nfrom mygrad.linalg.funcs import einsum\n\nfrom ..custom_strategies import broadcastable_shapes\nfrom ..utils.numerical_gradient import numerical_gradient_full\n\n\ndef bool_strat():\n    """""" einsum\'s optimize=True option has bugs prior to version 1.14.5\n        (caught by these very unit tests!), thus we only test `optimize=True`\n        for more recent versions.""""""\n    return st.booleans() if np.__version__ >= ""1.14.5"" else st.just(False)\n\n\ndef compare_einsum(*operands, optimize=False):\n    mygrad_out = einsum(*operands)\n    assert isinstance(mygrad_out, Tensor)\n    operands = tuple(i.data if isinstance(i, Tensor) else i for i in operands)\n    assert_allclose(np.einsum(*operands), einsum(*operands, optimize=optimize).data)\n\n\ndef compare_backprop(*operands, atol=1e-5, rtol=1e-5, optimize=False):\n    """""" Compare back-propagation through mygrad-einsum, and compare\n        against numerical derivative""""""\n    if isinstance(operands[0], str):\n        # operands form: ""ijk, ijk"", x, y\n        script = operands[0]\n        vars = operands[1:]\n        vars = tuple(np.asarray(i).astype(float) for i in vars)\n        tensors = tuple(Tensor(i) for i in vars)\n\n        def f(*args):\n            return np.einsum(script, *args)\n\n        out = einsum(script, *tensors, optimize=optimize)\n    else:\n        # operands form: op0, sublist0, op1, sublist1, ..., [sublistout]\n        end = -1 if len(operands) % 2 else None  # -1 if sublistout is included\n        vars = tuple(np.asarray(i).astype(float) for i in operands[:end:2])\n        tensors = tuple(Tensor(i) for i in vars)\n\n        def f(*args):\n            x = tuple(chain.from_iterable(zip(args, operands[1::2])))\n            if end is not None:\n                x += (operands[-1],)\n            return np.einsum(*x)\n\n        x = tuple(chain.from_iterable(zip(tensors, operands[1::2])))\n        if end is not None:\n            x += (operands[-1],)\n        out = einsum(*x, optimize=optimize)\n\n    grad = np.random.rand(*out.shape)\n    #    grad = np.ones(out.shape)\n    out.backward(grad)\n\n    numerical_derivs = numerical_gradient_full(f, *vars, back_grad=grad)\n\n    for n, (dnum, tensor) in enumerate(zip(numerical_derivs, tensors)):\n        assert dnum.shape == tensor.grad.shape\n        assert_allclose(\n            dnum,\n            tensor.grad,\n            atol=atol,\n            rtol=rtol,\n            err_msg=""The numerical and mygrad derivatives disagree for ""\n            ""variable index {}"".format(n),\n        )\n\n\n@pytest.mark.parametrize(\n    (""full_string"", ""end""),\n    [("""", """"), (""a"", ""a""), (""aaaa"", ""a""), (""aba"", ""ba""), (""abccbac"", ""bac"")],\n)\ndef test_unique_from_end(full_string, end):\n    from mygrad.linalg.ops import _unique_from_end\n\n    assert _unique_from_end(full_string) == end\n\n\ndef test_merge_mappings():\n    from mygrad.linalg.ops import _merge_max_mappings\n\n    a = dict(a=0, b=100, c=3)\n    b = dict(a=10, b=10)\n    c = dict(c=50)\n    d = dict(d=70)\n    e = dict()\n    assert _merge_max_mappings(a, b, c, d, e) == dict(a=10, b=100, c=50, d=70)\n\n\n@given(optimize=bool_strat())\ndef test_einsum_static_fwd(optimize):\n    """""" Check all einsum examples from numpy doc""""""\n    a = Tensor(np.arange(25).reshape(5, 5))\n    b = Tensor(np.arange(5))\n    c = Tensor(np.arange(6).reshape(2, 3))\n\n    compare_einsum(""ii"", a, optimize=optimize)\n    compare_einsum(a, [0, 0], optimize=optimize)\n\n    compare_einsum(""ii->i"", a, optimize=optimize)\n    compare_einsum(a, [0, 0], [0], optimize=optimize)\n\n    compare_einsum(""ij,j"", a, b, optimize=optimize)\n    compare_einsum(a, [0, 1], b, [1], optimize=optimize)\n\n    compare_einsum(""...j,j"", a, b, optimize=optimize)\n    compare_einsum(a, [Ellipsis, 0], b, [Ellipsis, 0], optimize=optimize)\n\n    compare_einsum(""ji"", c, optimize=optimize)\n    compare_einsum(c, [1, 0], optimize=optimize)\n\n    compare_einsum(""..., ..."", 3, c, optimize=optimize)\n    compare_einsum(3, [Ellipsis], c, [Ellipsis], optimize=optimize)\n\n    compare_einsum(""i,i"", b, b, optimize=optimize)\n    compare_einsum(b, [0], b, [0], optimize=optimize)\n\n    compare_einsum(""i,j"", np.arange(2) + 1, b, optimize=optimize)\n    compare_einsum(""i...->..."", a, optimize=optimize)\n\n    a = np.arange(60.0).reshape(3, 4, 5)\n    b = np.arange(24.0).reshape(4, 3, 2)\n    compare_einsum(""ijk,jil->kl"", a, b, optimize=optimize)\n    compare_einsum(a, [0, 1, 2], b, [1, 0, 3], [2, 3], optimize=optimize)\n\n    a = np.arange(6).reshape((3, 2))\n    b = np.arange(12).reshape((4, 3))\n    compare_einsum(""ki,jk->ij"", a, b, optimize=optimize)\n    compare_einsum(a, [0, 1], b, [2, 0], [1, 2], optimize=optimize)\n\n    compare_einsum(""ki,...k->i..."", a, b, optimize=optimize)\n    compare_einsum(a, [0, 1], b, [Ellipsis, 0], [1, Ellipsis], optimize=optimize)\n\n    compare_einsum(""k...,jk"", a, b, optimize=optimize)\n    compare_einsum(a, [0, Ellipsis], b, [2, 0], optimize=optimize)\n\n\n@given(optimize=bool_strat())\ndef test_einsum_static_bkwd(optimize):\n    """""" Check all einsum examples from numpy doc""""""\n    a = np.arange(25).reshape(5, 5)\n    b = np.arange(5)\n    c = np.arange(6).reshape(2, 3)\n\n    compare_backprop(""ii"", a, optimize=optimize)\n    compare_backprop(a, [0, 0], optimize=optimize)\n\n    compare_backprop(""ii->i"", a, optimize=optimize)\n    compare_backprop(a, [0, 0], [0], optimize=optimize)\n\n    compare_backprop(""ij->"", a, optimize=optimize)\n    compare_backprop(a, [0, 1], optimize=optimize)\n\n    compare_backprop(""ij,j"", a, b, optimize=optimize)\n    compare_backprop(a, [0, 1], b, [1], optimize=optimize)\n\n    compare_backprop(""...j,j"", a, b, optimize=optimize)\n    compare_backprop(a, [Ellipsis, 0], b, [Ellipsis, 0], optimize=optimize)\n\n    compare_backprop(""ji"", c, optimize=optimize)\n    compare_backprop(c, [1, 0], optimize=optimize)\n\n    compare_backprop(""..., ..."", 3, c, optimize=optimize)\n    compare_backprop(3, [Ellipsis], c, [Ellipsis], optimize=optimize)\n\n    compare_backprop(""i,i"", b, b, optimize=optimize)\n    compare_backprop(b, [0], b, [0], optimize=optimize)\n\n    compare_backprop(""i,j"", np.arange(2) + 1, b, optimize=optimize)\n\n    a = np.arange(60.0).reshape(3, 4, 5)\n    b = np.arange(24.0).reshape(4, 3, 2)\n    compare_backprop(""ijk,jil->kl"", a, b, atol=1e-3, rtol=1e-3, optimize=optimize)\n    compare_backprop(\n        a, [0, 1, 2], b, [1, 0, 3], [2, 3], atol=1e-3, rtol=1e-3, optimize=optimize\n    )\n\n    a = np.arange(6).reshape((3, 2))\n    b = np.arange(4).reshape((4, 1))\n    compare_backprop(""ki,jk->ij"", a, b, optimize=optimize)\n    compare_backprop(a, [0, 1], b, [2, 0], [1, 2], optimize=optimize)\n\n    compare_backprop(""ki,...k->i..."", a, b, optimize=optimize)\n    compare_backprop(a, [0, 1], b, [Ellipsis, 0], [1, Ellipsis], optimize=optimize)\n\n    compare_backprop(""k...,jk"", a, b, optimize=optimize)\n    compare_backprop(a, [0, Ellipsis], b, [2, 0], optimize=optimize)\n\n\n@settings(deadline=1000)\n@given(optimize=bool_strat())\ndef test_traces_bkwd(optimize):\n    a = np.random.rand(5, 2, 2, 5)\n    b = np.random.rand(3, 2, 1)\n    c = np.random.rand(1, 1)\n    d = np.random.rand(5, 5, 5)\n    compare_backprop(""ijji -> i"", a, optimize=optimize)\n    compare_backprop(a, [0, 1, 1, 0], [0], optimize=optimize)\n\n    compare_backprop(""iii -> i"", d, optimize=optimize)\n    compare_backprop(""ijji -> j"", a, optimize=optimize)\n    compare_backprop(""ijji -> ij"", a, optimize=optimize)\n    compare_backprop(""ijji -> ji"", a, optimize=optimize)\n    compare_backprop(""ijji -> "", a, optimize=optimize)\n    compare_backprop(""ijji,kji -> "", a, b, optimize=optimize)\n    compare_backprop(""ijji,kji -> kj"", a, b, optimize=optimize)\n    compare_backprop(""ijji,kji,jj-> kj"", a, b, c, optimize=optimize)\n    compare_backprop(""ijji,kji,jj-> ijk"", a, b, c, optimize=optimize)\n    compare_backprop(""ijji,kji,jj-> jk"", a, b, c, optimize=optimize)\n\n\ndef test_redundant_args():\n    """"""\n    Test behavior for when einsum receives redundant inputs. An optimization\n    was added such that einsum will only compute the gradient for such an entry\n    once and scale it accordingly.\n    """"""\n    a = Tensor(np.arange(4).reshape(2, 2))\n    a_copy = copy(a)\n\n    # check standard summation\n    o = einsum(""ij,ij"", a, a)\n    assert len(o.creator.cache) == 1\n    o.sum().backward()\n\n    o = einsum(""ij,ij"", a_copy, a_copy * 1)\n    assert len(o.creator.cache) == 2\n    o.sum().backward()\n    assert_allclose(a.grad, a_copy.grad)\n\n    a = Tensor(np.arange(4).reshape(2, 2))\n    a_copy = copy(a)\n\n    # check standard summation using alt signature\n    o = einsum(a, [0, 1], a, [0, 1])\n    assert len(o.creator.cache) == 1\n    o.sum().backward()\n\n    o = einsum(a_copy, [0, 1], a_copy * 1, [0, 1])\n    assert len(o.creator.cache) == 2\n    o.sum().backward()\n    assert_allclose(a.grad, a_copy.grad)\n\n    a = Tensor(np.arange(4).reshape(2, 2))\n    a_copy = copy(a)\n\n    # check matmul (no redundant indices)\n    o = einsum(""ij,jk"", a, a)\n    assert len(o.creator.cache) == 2\n    o.sum().backward()\n\n    o = a_copy @ a_copy\n    o.sum().backward()\n    assert_allclose(a.grad, a_copy.grad)\n\n    a = Tensor(np.arange(4).reshape(2, 2))\n    a_copy = copy(a)\n\n    # check traces\n    o = einsum(""ii,ii"", a, a)\n    assert len(o.creator.cache) == 1\n    o.sum().backward()\n\n    o = einsum(""ii,ii"", a_copy, a_copy * 1)\n    assert len(o.creator.cache) == 2\n    o.sum().backward()\n    assert_allclose(a.grad, a_copy.grad)\n\n    a = Tensor(np.arange(4).reshape(2, 2))\n    a_copy = copy(a)\n\n    b = Tensor(-1 * np.arange(2).reshape(2, 1))\n    b_copy = copy(b)\n\n    # check broadcasting and multiply-redundant input tensors\n    # with distinct einsum labels\n    o = einsum(""ii,ii,i...,i...,...i,...i"", a, a, b, b, a, a)\n    assert len(o.creator.cache) == 3\n    o.sum().backward()\n\n    o = einsum(\n        ""ii,ii,i...,i...,...i,...i"",\n        a_copy,\n        a_copy * 1,\n        b_copy,\n        b_copy * 1,\n        a_copy,\n        1 * a_copy,\n    )\n    assert len(o.creator.cache) == 6\n    o.sum().backward()\n    assert_allclose(a.grad, a_copy.grad)\n    assert_allclose(b.grad, b_copy.grad)\n\n\n@given(num=st.integers(1, 10), optimize=bool_strat(), data=st.data())\ndef test_einsum_bkwd1(num, optimize, data):\n    x = Tensor(np.random.rand(num))\n    y_shape = data.draw(broadcastable_shapes(x.shape, min_dims=1, max_dims=1))\n    y = Tensor(np.random.rand(*y_shape))\n\n    grad = data.draw(st.floats(-100, 100))\n    o = einsum(""i, i"", x, y, optimize=optimize)\n    o.backward(grad)\n\n    def f(x, y):\n        return np.einsum(""i, i"", x, y)\n\n    dx, dy = numerical_gradient_full(f, x.data, y.data, back_grad=grad)\n\n    assert_allclose(x.grad, dx, atol=1e-5, rtol=1e-5)\n    assert_allclose(y.grad, dy, atol=1e-5, rtol=1e-5)\n\n    o.null_gradients()\n    assert x.grad is None\n    assert y.grad is None\n\n    # test broadcasting in reverse direction\n    o = einsum(""i, i"", y, x, optimize=optimize)\n    o.backward(grad)\n\n    dy, dx = numerical_gradient_full(f, y.data, x.data, back_grad=grad)\n\n    assert_allclose(x.grad, dx, atol=1e-5, rtol=1e-5)\n    assert_allclose(y.grad, dy, atol=1e-5, rtol=1e-5)\n\n    o.null_gradients()\n\n\n@given(num=st.integers(1, 10), optimize=bool_strat(), data=st.data())\ndef test_einsum_bkwd2(num, optimize, data):\n    y = Tensor(np.random.rand(num))\n\n    # flip so that leading dim of x is broadcastable with y\n    x_shape = data.draw(broadcastable_shapes(y.shape, min_dims=2, max_dims=2))[::-1]\n    x = Tensor(np.random.rand(*x_shape))\n    grad = np.random.rand(x.shape[-1])\n\n    o = einsum(""ia, i -> a"", x, y, optimize=optimize)\n    o.backward(grad)\n\n    def f(x, y):\n        return np.einsum(""ia, i -> a"", x, y)\n\n    dx, dy = numerical_gradient_full(f, x.data, y.data, back_grad=grad)\n\n    assert_allclose(x.grad, dx, atol=1e-6)\n    assert_allclose(y.grad, dy, atol=1e-6)\n\n\n@given(\n    shape=hnp.array_shapes(min_dims=2, max_dims=2),\n    optimize=bool_strat(),\n    data=st.data(),\n)\ndef test_einsum_bkwd3(shape, optimize, data):\n    script = ""ia, ia, i -> a""\n    x = Tensor(np.random.rand(*shape))\n\n    y_shape = data.draw(\n        broadcastable_shapes(shape, min_dims=2, max_dims=2), label=""y_shape""\n    )\n    y = Tensor(np.random.rand(*y_shape))\n\n    z_shape = data.draw(\n        broadcastable_shapes(x.shape[:1], min_dims=1, max_dims=1), label=""z_shape""\n    )\n    z = Tensor(np.random.rand(*z_shape))\n\n    try:\n        o = einsum(script, x, y, z, optimize=optimize)\n    except ValueError:\n        assume(False)  # skip over invalid einsum shapes\n        return\n\n    grad = np.random.rand(*o.shape)\n    o.backward(grad)\n\n    def f(x, y, z):\n        return np.einsum(script, x, y, z)\n\n    dx, dy, dz = numerical_gradient_full(f, x.data, y.data, z.data, back_grad=grad)\n\n    assert_allclose(x.grad, dx, atol=1e-6)\n    assert_allclose(y.grad, dy, atol=1e-6)\n    assert_allclose(z.grad, dz, atol=1e-6)\n\n\n@given(\n    shape=hnp.array_shapes(min_dims=2, max_dims=2),\n    optimize=bool_strat(),\n    data=st.data(),\n)\ndef test_einsum_bkwd4(shape, optimize, data):\n    script = ""ia, i -> ""\n\n    x = Tensor(np.random.rand(*shape))\n\n    y_shape = data.draw(broadcastable_shapes(x.shape[:1], min_dims=1, max_dims=1))\n    y = Tensor(np.random.rand(*y_shape))\n\n    grad = np.random.rand(1).item()\n\n    o = einsum(script, x, y, optimize=optimize)\n    o.backward(grad)\n\n    def f(x, y):\n        return np.einsum(script, x, y)\n\n    dx, dy = numerical_gradient_full(f, x.data, y.data, back_grad=grad)\n\n    assert_allclose(x.grad, dx, atol=1e-6)\n    assert_allclose(y.grad, dy, atol=1e-6)\n\n\n@settings(deadline=2000)\n@given(optimize=bool_strat())\ndef test_einsum_bkwd5(optimize):\n    x = Tensor(np.random.rand(5, 3, 4, 6))\n    y = Tensor(np.random.rand(1, 5, 6, 2))\n    grad = np.random.rand(1, 3, 4, 2)\n\n    def f(x, y):\n        return np.einsum(""iBCj, aijd -> aBCd"", x, y)\n\n    o = einsum(""iBCj, aijd -> aBCd"", x, y, optimize=optimize)\n    o.backward(grad)\n\n    dx, dy = numerical_gradient_full(f, x.data, y.data, back_grad=grad)\n\n    assert_allclose(x.grad, dx, atol=1e-6)\n    assert_allclose(y.grad, dy, atol=1e-6)\n\n\n@settings(deadline=2000)\n@given(shape=hnp.array_shapes(min_dims=3, max_dims=3), optimize=bool_strat())\ndef test_einsum_bkwd6(shape, optimize):\n    sig = ""ijk, -> j""\n    x = Tensor(np.random.rand(*shape))\n    y = Tensor(np.random.rand(1).item())\n    grad = np.random.rand(x.shape[1])\n\n    o = einsum(sig, x, y, optimize=optimize)\n    o.backward(grad)\n\n    def f(x, y):\n        return np.einsum(sig, x, y)\n\n    dx, dy = numerical_gradient_full(f, x.data, y.data, back_grad=grad)\n\n    assert_allclose(x.grad, dx, atol=1e-6)\n    assert_allclose(y.grad, dy, atol=1e-6)\n'"
tests/linalg/test_matmul.py,4,"b'import hypothesis.extra.numpy as hnp\nimport numpy as np\nfrom hypothesis import settings\n\nfrom mygrad import matmul\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(\n    mygrad_func=matmul,\n    true_func=np.matmul,\n    shapes=hnp.mutually_broadcastable_shapes(\n        signature=""(n?,k),(k,m?)->(n?,m?)"", max_side=4\n    ),\n)\ndef test_matmul_fwd():\n    pass\n\n\n@settings(max_examples=500)\n@backprop_test_factory(\n    mygrad_func=matmul,\n    true_func=np.matmul,\n    shapes=hnp.mutually_broadcastable_shapes(\n        signature=""(n?,k),(k,m?)->(n?,m?)"", max_side=4\n    ),\n    vary_each_element=True,\n)\ndef test_matmul_bkwd():\n    pass\n'"
tests/linalg/test_multi_matmul.py,4,"b'import functools\nfrom typing import List, Tuple\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given\n\nimport mygrad as mg\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef matmul_wrapper(*args, constant=False):\n    return mg.multi_matmul(args, constant)\n\n\ndef multi_matmul_slow(*arrays, **kwargs):\n    return functools.reduce(np.matmul, arrays)\n\n\n@given(st.lists(st.just(mg.Tensor([0.0, 1.0])), min_size=0, max_size=1))\ndef test_input_validation_too_few_tensors(tensors: List[mg.Tensor]):\n    """"""multi_matmul requires at least two input-tensors""""""\n    with pytest.raises(ValueError):\n        mg.multi_matmul(tensors)\n\n\n@given(st.lists(hnp.array_shapes(min_dims=1, min_side=2, max_side=2), min_size=2))\ndef test_input_validation_large_dimensionality(shapes: List[Tuple[int, ...]]):\n    """"""multi_matmul only operates on 1D and 2D tensors""""""\n    if all((1 <= len(x) <= 2) for x in shapes):\n        shapes[0] = (1, 1, *shapes[0])\n    tensors = [mg.ones(shape=shape) for shape in shapes]\n    with pytest.raises(ValueError):\n        mg.multi_matmul(tensors)\n\n\n@pytest.mark.parametrize(\n    ""signature"",\n    (\n        ""(a?,b),(b,e?)->(a?,e?)"",\n        ""(a?,b),(b,c),(c,e?)->(a?,e?)"",\n        ""(a?,b),(b,c),(c,d),(d,e?)->(a?,e?)"",\n    ),\n)\ndef test_matmul_fwd(signature):\n    @fwdprop_test_factory(\n        mygrad_func=matmul_wrapper,\n        true_func=multi_matmul_slow,\n        shapes=hnp.mutually_broadcastable_shapes(signature=signature, max_dims=0),\n        default_bnds=(-10, 10),\n        atol=1e-5,\n        rtol=1e-5,\n    )\n    def test_runner():\n        pass\n\n    test_runner()\n\n\n@pytest.mark.parametrize(\n    ""signature"",\n    (\n        ""(a?,b),(b,e?)->(a?,e?)"",\n        ""(a?,b),(b,c),(c,e?)->(a?,e?)"",\n        ""(a?,b),(b,c),(c,d),(d,e?)->(a?,e?)"",\n    ),\n)\ndef test_matmul_bkwd(signature):\n    @backprop_test_factory(\n        mygrad_func=matmul_wrapper,\n        true_func=multi_matmul_slow,\n        shapes=hnp.mutually_broadcastable_shapes(signature=signature, max_dims=0),\n        default_bnds=(-10, 10),\n        atol=1e-5,\n        rtol=1e-5,\n        vary_each_element=True,\n    )\n    def test_runner():\n        pass\n\n    test_runner()\n'"
tests/math/__init__.py,0,b''
tests/math/test_constants.py,13,"b'from typing import Union\n\nimport numpy as np\nimport pytest\n\nimport mygrad as mg\n\n\n@pytest.mark.parametrize(\n    (""mygrad_constant"", ""numpy_constant""),\n    [\n        (mg.Infinity, np.Infinity),\n        (mg.NAN, np.NAN),\n        (mg.NINF, np.NINF),\n        (mg.NZERO, np.NZERO),\n        (mg.NaN, np.NaN),\n        (mg.PINF, np.PINF),\n        (mg.PZERO, np.PZERO),\n        (mg.e, np.e),\n        (mg.euler_gamma, np.euler_gamma),\n        (mg.inf, np.inf),\n        (mg.infty, np.infty),\n        (mg.nan, np.nan),\n        (mg.newaxis, np.newaxis),\n    ],\n)\ndef test_constants(\n    mygrad_constant: Union[None, float], numpy_constant: Union[None, float]\n):\n    assert mygrad_constant is numpy_constant\n'"
tests/math/test_misc.py,25,"b'from functools import partial\nfrom typing import Callable\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given, settings\nfrom numpy.testing import assert_allclose\n\nfrom mygrad import clip, maximum, minimum\nfrom mygrad.tensor_base import Tensor\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef is_not_close(arr0: Tensor, arr1: Tensor) -> bool:\n    return not np.any(np.isclose(arr0.data, arr1.data))\n\n\n@fwdprop_test_factory(mygrad_func=maximum, true_func=np.maximum, num_arrays=2)\ndef test_maximum_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=maximum, true_func=np.maximum, num_arrays=2, assumptions=is_not_close\n)\ndef test_maximum_bkwd():\n    pass\n\n\ndef test_maximum_bkwd_equal():\n    """""" regression test for documented behavior of maximum/minimum where\n        x == y""""""\n\n    x = Tensor([1.0, 0.0, 2.0])\n    y = Tensor([2.0, 0.0, 1.0])\n\n    o = maximum(x, y)\n    o.backward()\n\n    assert_allclose(x.grad, [0.0, 0.0, 1])\n    assert_allclose(y.grad, [1.0, 0.0, 0])\n    o.null_gradients()\n\n    # ensure branch covered for equal scalars\n    x = Tensor(1.0)\n    y = Tensor(1.0)\n\n    o = maximum(x, y)\n    o.backward()\n\n    assert_allclose(x.grad, 0.0)\n    assert_allclose(y.grad, 0.0)\n    o.null_gradients()\n\n\n@fwdprop_test_factory(mygrad_func=minimum, true_func=np.minimum, num_arrays=2)\ndef test_minimum_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=minimum, true_func=np.minimum, num_arrays=2, assumptions=is_not_close\n)\ndef test_minimum_bkwd():\n    pass\n\n\ndef test_minimum_bkwd_equal():\n    """""" regression test for documented behavior of minimum/minimum where\n        x == y""""""\n\n    x = Tensor([1.0, 0.0, 2.0])\n    y = Tensor([2.0, 0.0, 1.0])\n\n    o = minimum(x, y)\n    o.backward()\n\n    assert_allclose(x.grad, [1.0, 0.0, 0.0])\n    assert_allclose(y.grad, [0.0, 0.0, 1.0])\n    o.null_gradients()\n\n    # ensure branch covered for equal scalars\n    x = Tensor(1.0)\n    y = Tensor(1.0)\n\n    o = minimum(x, y)\n    o.backward()\n\n    assert_allclose(x.grad, 0.0)\n    assert_allclose(y.grad, 0.0)\n    o.null_gradients()\n\n\ndef to_min_max(arr: np.ndarray) -> st.SearchStrategy:\n    bnd_shape = hnp.broadcastable_shapes(\n        shape=arr.shape, max_dims=arr.ndim, max_side=min(arr.shape) if arr.ndim else 1\n    )\n    bnd_strat = hnp.arrays(\n        shape=bnd_shape, elements=st.floats(-1e6, 1e6), dtype=np.float64\n    )\n    return st.fixed_dictionaries(dict(a_min=bnd_strat, a_max=bnd_strat))\n\n\ndef amin_clip_only(clip_func, a, b, constant=False):\n    return (\n        clip_func(a, a_min=b, a_max=None, constant=constant)\n        if constant is not None\n        else clip_func(a, a_min=b, a_max=None)\n    )\n\n\ndef amax_clip_only(clip_func, a, b, constant=False):\n    return (\n        clip_func(a, a_max=b, a_min=None, constant=constant)\n        if constant is not None\n        else clip_func(a, a_max=b, a_min=None)\n    )\n\n\nskip_if_lower_than_numpy_1p17 = pytest.mark.skipif(\n    np.__version__ < ""1.17"",\n    reason=""numpy.clip behavior was made consistent in numpy-1.17; ""\n    ""test must by run on numpy 1.17 or later"",\n)\n\n\n@pytest.mark.parametrize(\n    (""mygrad_clip"", ""numpy_clip"", ""num_arrays""),\n    [\n        (\n            partial(amin_clip_only, clip),\n            partial(amin_clip_only, np.clip, constant=None),\n            2,\n        ),\n        (\n            partial(amax_clip_only, clip),\n            partial(amax_clip_only, np.clip, constant=None),\n            2,\n        ),\n        (clip, np.clip, 3),\n    ],\n)\n@skip_if_lower_than_numpy_1p17\ndef test_clip_fwd(mygrad_clip: Callable, numpy_clip: Callable, num_arrays: int):\n    @fwdprop_test_factory(\n        num_arrays=num_arrays, mygrad_func=mygrad_clip, true_func=numpy_clip\n    )\n    def wrapped_test():\n        pass\n\n    wrapped_test()\n\n\ndef is_not_close_clip(a: Tensor, a_min=None, a_max=None) -> bool:\n    min_close = np.any(np.isclose(a.data, a_min.data)) if a_min is not None else False\n    max_close = np.any(np.isclose(a.data, a_max.data)) if a_max is not None else False\n    return not (min_close or max_close)\n\n\n@pytest.mark.parametrize(\n    (""mygrad_clip"", ""numpy_clip"", ""num_arrays""),\n    [\n        (\n            partial(amin_clip_only, clip),\n            partial(amin_clip_only, np.clip, constant=None),\n            2,\n        ),\n        (\n            partial(amax_clip_only, clip),\n            partial(amax_clip_only, np.clip, constant=None),\n            2,\n        ),\n        (clip, np.clip, 3),\n    ],\n)\n@skip_if_lower_than_numpy_1p17\ndef test_clip_bkwd(mygrad_clip: Callable, numpy_clip: Callable, num_arrays: int):\n    @backprop_test_factory(\n        num_arrays=num_arrays,\n        mygrad_func=mygrad_clip,\n        true_func=numpy_clip,\n        vary_each_element=True,\n        assumptions=is_not_close_clip,  # derivative is not defined where bounds and array are equal\n    )\n    def wrapped_test():\n        pass\n\n    wrapped_test()\n\n\n@settings(max_examples=500)\n@given(\n    a=hnp.arrays(shape=hnp.array_shapes(min_dims=0), elements=st.floats(), dtype=float),\n    a_min=st.none()\n    | hnp.arrays(\n        shape=hnp.array_shapes(min_dims=0),\n        elements=st.floats(allow_nan=False),\n        dtype=float,\n    ),\n    a_max=st.none()\n    | hnp.arrays(\n        shape=hnp.array_shapes(min_dims=0),\n        elements=st.floats(allow_nan=False),\n        dtype=float,\n    ),\n)\n@skip_if_lower_than_numpy_1p17\n@pytest.mark.filterwarnings(""ignore: invalid value"")\ndef test_clip_input_validation(a, a_min, a_max):\n    try:\n        numpy_out = np.clip(a, a_min, a_max)\n    except Exception as e:\n        with pytest.raises(type(e)):\n            clip(a, a_min, a_max)\n        return\n    mygrad_out = clip(a, a_min, a_max)\n\n    np.testing.assert_array_equal(numpy_out, mygrad_out.data)\n'"
tests/nnet/__init__.py,0,b''
tests/nnet/test_sliding_window.py,14,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given, settings\nfrom numpy.testing import assert_allclose\n\nfrom mygrad.nnet.layers.utils import sliding_window_view\n\ndtype_strat_numpy = st.sampled_from(\n    (np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64)\n)\n\n\n@pytest.mark.parametrize(\n    ""args"",\n    [\n        dict(step=None),\n        dict(step=st.integers(max_value=-1)),\n        dict(window_shape=st.none() | st.just(1)),\n        dict(\n            window_shape=st.lists(st.just(1), min_size=3)\n        ),  # more window dims than arr\n        dict(\n            window_shape=(\n                st.just((-1, 1))\n                | st.just((-1, 1))\n                | st.tuples(st.floats(), st.floats())\n            )\n        ),\n        dict(\n            window_shape=st.lists(st.integers(5, 7), min_size=1, max_size=2).filter(\n                lambda x: 7 in x\n            )\n        ),  # window dim too large\n        dict(dilation=st.sampled_from([-1, (1, 0), ""aa"", (1, 1, 1), 1.0])),\n        dict(dilation=st.sampled_from([7, (1, 7), (10, 1)])),\n    ],\n)\n@settings(deadline=None)\n@given(data=st.data())\ndef test_input_validation(args: dict, data: st.DataObject):\n    kwargs = dict(\n        arr=np.arange(36).reshape(6, 6), window_shape=(1, 1), step=1, dilation=None\n    )\n    kwargs.update(\n        (k, (data.draw(v, label=k)) if isinstance(v, st.SearchStrategy) else v)\n        for k, v in args.items()\n    )\n\n    with pytest.raises((ValueError, TypeError)):\n        sliding_window_view(**kwargs)\n\n\n@settings(deadline=None)\n@given(\n    data=st.data(),\n    x=hnp.arrays(\n        dtype=dtype_strat_numpy,\n        shape=hnp.array_shapes(max_dims=5, min_dims=1, max_side=20),\n    ),\n)\ndef test_sliding_window(data, x):\n    """""" Test variations of window-shape, step, and dilation for sliding window\n        view of N-dimensional array.""""""\n\n    win_dim = data.draw(st.integers(1, x.ndim), label=""win_dim"")\n    win_shape = data.draw(\n        st.tuples(*(st.integers(1, s) for s in x.shape[-win_dim:])), label=""win_shape""\n    )\n    step = data.draw(\n        st.tuples(*(st.integers(1, s) for s in x.shape[-win_dim:])), label=""step""\n    )\n\n    max_dilation = np.array(x.shape[-win_dim:]) // win_shape\n    dilation = data.draw(\n        st.one_of(\n            st.none()\n            | st.integers(1, min(max_dilation))\n            | st.tuples(*(st.integers(1, s) for s in max_dilation))\n        ),\n        label=""dilation"",\n    )\n    y = sliding_window_view(x, window_shape=win_shape, step=step, dilation=dilation)\n\n    if dilation is None:\n        dilation = np.ones((len(win_shape),), dtype=int)\n\n    if isinstance(dilation, int):\n        dilation = np.full((len(win_shape),), fill_value=dilation, dtype=int)\n\n    for ind in np.ndindex(*y.shape[:win_dim]):\n        slices = tuple(\n            slice(i * s, i * s + w * d, d)\n            for i, w, s, d in zip(ind, win_shape, step, dilation)\n        )\n        assert_allclose(actual=y[tuple([*ind])], desired=x[(..., *slices)])\n\n\n@given(dtype=dtype_strat_numpy)\ndef test_memory_details(dtype):\n    """""" Ensure that:\n          - function handles non C-contiguous layouts correctly\n          - output is view of input\n          - output is not writeable""""""\n    x = np.arange(20).reshape(2, 10).astype(dtype)\n    x = np.asfortranarray(x)\n    y = sliding_window_view(x, (5,), 5)\n    soln = np.array(\n        [\n            [[0, 1, 2, 3, 4], [10, 11, 12, 13, 14]],\n            [[5, 6, 7, 8, 9], [15, 16, 17, 18, 19]],\n        ]\n    )\n    assert not y.flags[""WRITEABLE""]\n    assert_allclose(actual=y, desired=soln)\n\n    x = np.arange(20).reshape(2, 10)\n    x = np.ascontiguousarray(x)\n    y = sliding_window_view(x, (5,), 5)\n    assert not y.flags[""WRITEABLE""]\n    assert np.shares_memory(x, y)\n    assert_allclose(actual=y, desired=soln)\n'"
tests/state_testing/__init__.py,0,b''
tests/state_testing/simple_graph.py,5,"b'import numpy as np\n\n\nclass SimpleOperation:\n    def __call__(self, *input_vars):\n        self.variables = input_vars\n        raise NotImplementedError\n\n    def backward_var(self, grad, index):\n        raise NotImplementedError\n\n    def backward(self, grad):\n        for index, var in enumerate(self.variables):\n            if not var.constant:\n                self.backward_var(grad, index)\n\n    def null_gradients(self, clear_graph=True):\n        for var in self.variables:\n            var.null_gradients(clear_graph=clear_graph)\n\n    def clear_graph(self, terminal_node):\n        for var in self.variables:\n            var.clear_graph(terminal_node)\n\n\nclass Node:\n    __array_priority__ = 15.0\n\n    def __init__(self, x, *, constant=False, _creator=None):\n        self.creator = _creator\n        self.data = x.data if isinstance(x, Node) else np.asarray(x)\n        self.grad = None\n        self.constant = constant\n        self._ops = []  # Operation instances that utilized self an input tensor\n\n    def __repr__(self):\n        return repr(self.data).replace(""array"", ""Node"").replace(""\\n"", ""\\n "")\n\n    @classmethod\n    def _op(cls, Op, *input_vars, constant=False):\n        tensor_vars = []\n        for var in input_vars:\n            if not isinstance(var, cls):\n                var = cls(var, constant=True)\n            tensor_vars.append(var)\n\n        f = Op()\n        op_out = f(*tensor_vars)\n        is_const = constant or all(var.constant for var in tensor_vars)\n        if not is_const:\n            # record that a variable participated in that op\n            for var in tensor_vars:\n                if not var.constant:\n                    var._ops.append(f)\n        return cls(op_out, constant=is_const, _creator=f)\n\n    def backward(self, grad=None, terminal_node=False):\n        if self.constant:\n            return\n        grad = np.asarray(grad) if grad is not None else np.asarray(1.0, dtype=float)\n        if terminal_node:\n            self.grad = np.asarray(grad)\n        else:\n            if not terminal_node and not self._ops:\n                raise Exception(\n                    ""Invalid Backprop: part of the computational graph containing ""\n                    ""this tensor was cleared prior to backprop""\n                )\n            self.grad = np.asarray(grad if self.grad is None else self.grad + grad)\n\n        if self.creator is not None:\n            self.creator.backward(grad)\n\n    def null_gradients(self, clear_graph=True):\n        self.grad = None\n        if self.creator is not None:\n            self.creator.null_gradients(False)\n        if clear_graph:\n            self.clear_graph()\n\n    def clear_graph(self, terminal_node=True):\n        if not terminal_node:\n            self._ops.clear()\n        if self.creator is not None:\n            self.creator.clear_graph(terminal_node=False)\n            self.creator = None\n\n\nclass Multiply(SimpleOperation):\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        out = a.data * b.data\n        return out\n\n    def backward_var(self, grad, index):\n        a, b = self.variables\n        if index == 0:  # backprop through a\n            a.backward(grad * b.data)\n        elif index == 1:  # backprop through b\n            b.backward(grad * a.data)\n\n\ndef _multiply(a, b, constant=False):\n    return Node._op(Multiply, a, b, constant=constant)\n\n\nclass Add(SimpleOperation):\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        out = a.data + b.data\n        return out\n\n    def backward_var(self, grad, index):\n        self.variables[index].backward(np.copy(grad))\n\n\ndef _add(a, b, constant=False):\n    return Node._op(Add, a, b, constant=constant)\n'"
tests/state_testing/test_state.py,0,"b'""""""\nTests Tensor/Operation implementation that uses topological sorting against the naive implementation.\n\nHere, a rule-based state machine constructs computational graphs via \'rules\' by which nodes (tensors)\nare \'fused\' via addition and multiplication operations. The state machine create and fuse nodes in\nany patterns, invoke `null_gradients` and `clear_graph` arbitrarily as well.\n\nThe values and gradients of the nodes in the mygrad and naive graphs must match as an invariant to\nany permutation of test states (i.e. permutations of the aforementioned rules)""""""\n\nfrom typing import List, Tuple\n\nimport hypothesis.strategies as st\nfrom hypothesis import settings\nfrom hypothesis.stateful import (\n    Bundle,\n    RuleBasedStateMachine,\n    invariant,\n    precondition,\n    rule,\n)\nfrom numpy.testing import assert_allclose, assert_equal\nfrom pytest import raises\n\nfrom mygrad import Tensor, add, multiply\n\nfrom .simple_graph import Node, _add, _multiply\n\n\ndef _node_ID_str(num):\n    return ""v{}"".format(num + 1)\n\n\n@settings(max_examples=200, deadline=None)\nclass GraphCompare(RuleBasedStateMachine):\n    def __init__(self):\n        super().__init__()\n        # stores the corresponding node/tensor v1, v2, ... as they are\n        # created via the unit test (through `create_node` or `fuse_nodes`)\n        # `Node` is the naive implementation of `Tensor` that we are checking\n        # against\n        self.node_list = []  # type: List[Tuple[Node, Tensor]]\n        self.str_to_tensor_op = {""add"": add, ""multiply"": multiply}\n        self.str_to_node_op = {""add"": _add, ""multiply"": _multiply}\n        self.raised = False\n\n    nodes = Bundle(""nodes"")\n\n    @rule(target=nodes, value=st.floats(-10, 10), constant=st.booleans())\n    def create_node(self, value, constant):\n        n = Node(value, constant=constant)\n        t = Tensor(value, constant=constant)\n        self.node_list.append((n, t))\n        return n, t\n\n    @rule(\n        target=nodes,\n        a=nodes,\n        b=nodes,\n        op=st.sampled_from([""add"", ""multiply""]),\n        constant=st.booleans(),\n    )\n    def fuse_nodes(self, a, b, op, constant):\n        """"""\n        Combine any pair of nodes (tensors) using either addition or multiplication, producing\n        a new node (tensor)""""""\n        n_a, t_a = a\n        n_b, t_b = b\n        n_op = self.str_to_node_op[op]\n        t_op = self.str_to_tensor_op[op]\n        out = (n_op(n_a, n_b, constant=constant), t_op(t_a, t_b, constant=constant))\n        self.node_list.append(out)\n        return out\n\n    @rule(items=nodes, clear_graph=st.booleans())\n    def null_gradients(self, items, clear_graph):\n        """"""\n        Invoke `null_gradients` on the computational graph (naive and mygrad), with\n        `clear_graph=True` specified optionally.\n        """"""\n        n, t = items\n        n.null_gradients(clear_graph=clear_graph)\n        t.null_gradients(clear_graph=clear_graph)\n\n    @rule(items=nodes)\n    def clear_graph(self, items):\n        """"""\n        Invoke `clear_graph` on the computational graph (naive and mygrad)\n        """"""\n        n, t = items\n        n.clear_graph()\n        t.clear_graph()\n\n    @rule(items=nodes, grad=st.floats(-10, 10))\n    def backprop(self, items, grad):\n        """"""\n        Invoke `backward(grad)` on the computational graph (naive and mygrad) from a randomly-selected\n        node in the computational graph and using a randomly-generated gradient value.\n\n        An exception should be raised if `clear_graph` is invoked anywhere prior to the invoking node.\n        """"""\n        n, t = items\n        n.null_gradients(clear_graph=False)\n        t.null_gradients(clear_graph=False)\n        try:\n            n.backward(grad, terminal_node=True)\n        except Exception:\n            with raises(Exception):\n                t.backward(grad)\n            self.raised = True\n        else:\n            t.backward(grad)\n\n    @precondition(lambda self: not self.raised)\n    @invariant()\n    def all_agree(self):\n        """"""\n        Ensure that all corresponding nodes/tensors have matching data and gradients\n        across the respective graphs.\n        """"""\n        for num, (n, t) in enumerate(self.node_list):\n            assert bool(n._ops) is bool(t._ops), _node_ID_str(num)\n            assert_equal(n.data, t.data, err_msg=_node_ID_str(num))\n            if n.grad is None or t.grad is None:\n                assert n.grad is t.grad, _node_ID_str(num)\n            else:\n                assert_allclose(\n                    actual=t.grad,\n                    desired=n.grad,\n                    atol=1e-5,\n                    rtol=1e-5,\n                    err_msg=_node_ID_str(num),\n                )\n            assert not t._accum_ops, _node_ID_str(num)\n\n\nTestGraphComparison = GraphCompare.TestCase\n'"
tests/tensor_base/__init__.py,0,b''
tests/tensor_base/test_astype.py,7,"b'from typing import Optional\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given, settings\nfrom numpy.testing import assert_array_equal\n\nfrom mygrad import Tensor\n\nreal_types = (\n    hnp.integer_dtypes() | hnp.unsigned_integer_dtypes() | hnp.floating_dtypes()\n)\n\n\n@given(\n    tensor=st.tuples(\n        hnp.arrays(shape=hnp.array_shapes(), dtype=real_types), st.booleans(),\n    ).map(lambda x: Tensor(x[0], constant=x[1])),\n    dest_type=real_types,\n    constant=st.booleans() | st.none(),\n)\ndef test_astype(tensor: Tensor, dest_type: type, constant: Optional[bool]):\n    tensor = tensor * 1  # give tensor a creator\n    new_tensor = tensor.astype(dest_type, constant=constant)\n\n    assert new_tensor.constant is (tensor.constant if constant is None else constant)\n    assert tensor.creator is not None\n    assert new_tensor.creator is None\n    assert new_tensor.dtype is dest_type\n    assert new_tensor.shape == tensor.shape\n\n    if new_tensor.dtype is tensor.dtype:\n        assert_array_equal(new_tensor.data, tensor.data)\n\n\n@settings(max_examples=30)\n@pytest.mark.parametrize(\n    ""type_strategy"",\n    [hnp.integer_dtypes(), hnp.unsigned_integer_dtypes(), hnp.floating_dtypes()],\n)\n@given(data=st.data())\ndef test_upcast_roundtrip(type_strategy, data: st.DataObject):\n    thin, wide = data.draw(\n        st.tuples(type_strategy, type_strategy).map(\n            lambda x: sorted(x, key=lambda y: np.dtype(y).itemsize)\n        )\n    )\n    orig_tensor = data.draw(\n        hnp.arrays(\n            dtype=thin,\n            shape=hnp.array_shapes(),\n            elements=hnp.from_dtype(thin).filter(np.isfinite),\n        ).map(Tensor)\n    )\n\n    roundtripped_tensor = orig_tensor.astype(wide).astype(thin)\n    assert_array_equal(orig_tensor, roundtripped_tensor)\n'"
tests/tensor_base/test_chainrule.py,4,"b'import hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import given\nfrom numpy.testing import assert_allclose\n\nimport mygrad as mg\nfrom mygrad.tensor_base import Tensor\n\n\n@given(\n    x=st.floats(min_value=-1e-6, max_value=1e6),\n    y=st.floats(min_value=-1e-6, max_value=1e6),\n    z=st.floats(min_value=-1e-6, max_value=1e6),\n    side_effects=st.booleans(),\n)\ndef test_chainrule_scalar(x, y, z, side_effects):\n    x = Tensor(x)\n    y = Tensor(y)\n    z = Tensor(z)\n\n    f = x * y + z\n    g = x + z * f * f\n\n    if side_effects:\n        # check side effects\n        unused = 2 * g - f\n        w = 1 * f\n    else:\n        unused = Tensor(0)\n        w = Tensor(0)\n    assert unused is not None\n\n    g.backward()\n    assert_allclose(f.grad, 2 * z.data * f.data)\n    assert_allclose(x.grad, 1 + 2 * z.data * f.data * y.data)\n    assert_allclose(y.grad, 2 * z.data * f.data * x.data)\n    assert_allclose(z.grad, f.data ** 2 + z.data * 2 * f.data)\n\n    assert w.grad is None\n\n\ndef test_identical_inputs():\n    v1 = Tensor(2.0, constant=False)\n    v2 = v1 + v1\n    v3 = v2 + v2\n    v3.backward(1.0)  # v3 = 4 * v1\n    assert v3.data.item() == 8.0\n    assert v1.grad.item() == 4.0\n\n\n@given(data=st.floats(-10, 10), grad=(st.none() | st.floats(-10, 10)))\ndef test_non_broadcastable(data, grad):\n    v1 = Tensor(data, constant=False)\n    v2 = mg.exp(v1)\n    v3 = mg.cos(v2)\n    v3.backward(grad)\n\n    if grad is None:\n        grad = 1.0\n\n    assert_allclose(actual=v2.data, desired=np.exp(v1.data))\n    assert_allclose(actual=v3.data, desired=np.cos(np.exp(v1.data)))\n\n    assert_allclose(actual=v3.grad, desired=grad)\n    assert_allclose(actual=v2.grad, desired=-np.sin(v2.data) * grad)\n    assert_allclose(actual=v1.grad, desired=np.exp(v1.data) * -np.sin(v2.data) * grad)\n'"
tests/tensor_base/test_graph_tracking.py,0,"b'import mygrad as mg\nfrom mygrad.math.arithmetic.ops import Add, Divide, Power\n\n\ndef test_op_tracks_graph():\n    """"""Ensures that ``Operation.graph`` tracks operations as expected""""""\n    x = mg.Tensor(1)\n    y = mg.Tensor(2)\n\n    z = x * y\n    assert z.creator.graph == {z.creator}\n\n    f = z + 2\n    assert f.creator.graph == {z.creator, f.creator}\n\n    h = z - f\n    assert h.creator.graph == {h.creator} | f.creator.graph\n\n    i = ((h + 3) ** 2) / 5\n    assert h.creator.graph < i.creator.graph\n    assert (\n        len(i.creator.graph - h.creator.graph) == 3\n    ), ""should be {{Add, Subtract, Divide}}, but got {}"".format(\n        i.creator.graph - h.creator.graph\n    )\n    assert all(\n        isinstance(x, (Add, Power, Divide)) for x in i.creator.graph - h.creator.graph\n    )\n'"
tests/tensor_base/test_op_wrapper.py,0,"b'from mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\n\nclass Dummy(Operation):\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        return 1\n\n\ndef dummy(a, b, constant=False):\n    return Tensor._op(Dummy, a, b, constant=constant)\n\n\ndef test_constant_arg():\n    """""" test that the `constant` arg works as intended in Tensor._op""""""\n    a = Tensor(1)\n    b = Tensor(1)\n    o_true = dummy(a, b, constant=True)\n    assert o_true.constant is True\n    assert a._ops == set()\n    assert b._ops == set()\n\n    o_false = dummy(a, b, constant=False)\n    assert o_false.constant is False\n    assert a._ops == {o_false.creator}\n    assert b._ops == {o_false.creator}\n'"
tests/tensor_base/test_scalar_only.py,2,"b'import hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given\nfrom pytest import raises\n\nfrom mygrad.errors import InvalidBackprop\nfrom mygrad.operation_base import BroadcastableOp, Operation\nfrom mygrad.tensor_base import Tensor\n\n\nclass ScalarOnlyOp(BroadcastableOp):\n    def __init__(self):\n        self.scalar_only = True\n\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        return np.array([0])\n\n\nclass NotScalarOnlyOp(Operation):\n    def __init__(self):\n        self.scalar_only = False\n\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        return np.array([0])\n\n\n@given(\n    a_const=st.booleans(),\n    a_scalar_only=st.booleans(),\n    b_const=st.booleans(),\n    b_scalar_only=st.booleans(),\n)\ndef test_scalar_only_op(a_const, a_scalar_only, b_const, b_scalar_only):\n    """""" op produces scalar_only result unless result is scalar. """"""\n    a = Tensor(0, constant=a_const, _scalar_only=a_scalar_only)\n    b = Tensor(0, constant=b_const, _scalar_only=b_scalar_only)\n\n    out = Tensor._op(ScalarOnlyOp, a, b)\n    scalar_only = True and not out.constant\n\n    assert scalar_only is out.scalar_only\n\n    # check out.backward()\n    if scalar_only:\n        with raises(InvalidBackprop):\n            out.backward()\n    else:\n        out.backward()  # a, b, out are const (nothing computed)\n\n\n@given(\n    a_const=st.booleans(),\n    a_scalar_only=st.booleans(),\n    b_const=st.booleans(),\n    b_scalar_only=st.booleans(),\n)\ndef test_standard_op(a_const, a_scalar_only, b_const, b_scalar_only):\n    """""" op produces standard result unless an `a` or `b` is a scalar_only variable. """"""\n    a = Tensor(0, constant=a_const, _scalar_only=a_scalar_only)\n    b = Tensor(0, constant=b_const, _scalar_only=b_scalar_only)\n\n    scalar_only = (a.scalar_only and not a.constant) or (\n        b.scalar_only and not b.constant\n    )\n    out = Tensor._op(NotScalarOnlyOp, a, b)\n\n    assert scalar_only is (out.scalar_only and not out.constant)\n\n    # check out.backward()\n    if scalar_only:\n        with raises(InvalidBackprop):\n            out.backward()\n    else:\n        if a.constant and b.constant:\n            out.backward()  # a, b, out are const (nothing computed)\n        else:\n            with raises(NotImplementedError):\n                out.backward()\n\n\n@pytest.mark.parametrize(""constant"", [True, False])\n@pytest.mark.parametrize(""operation"", [""add"", ""sub"", ""mul"", ""truediv"", ""pow""])\ndef test_practical_scalar_only(constant, operation):\n    a = Tensor([1, 2, 3], constant=constant)\n    b = Tensor(3, constant=constant)\n    out = getattr(a, ""__"" + operation + ""__"")(b)\n\n    if constant:\n        out.backward()\n    else:\n        with raises(InvalidBackprop):\n            out.backward()\n'"
tests/tensor_base/test_tensor.py,39,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import assume, given, settings\nfrom numpy.testing import assert_allclose, assert_array_equal, assert_equal\nfrom pytest import raises\n\nimport mygrad as mg\nfrom mygrad import Tensor\nfrom mygrad.errors import InvalidBackprop, InvalidGradient\nfrom mygrad.linalg.ops import MatMul\nfrom mygrad.math.arithmetic.ops import Add, Divide, Multiply, Negative, Power, Subtract\nfrom mygrad.operation_base import Operation\nfrom tests.utils import does_not_raise\n\n\n@pytest.mark.parametrize(\n    ""data"", [None, np.array(None), np.array([[0], [0, 0]]), np.array(1, dtype=""O"")]\n)\n@given(constant=st.booleans(), creator=st.none() | st.just(MatMul()))\ndef test_input_type_checking(data, constant, creator):\n    with raises(TypeError):\n        Tensor(data, constant=constant, _creator=creator)\n\n\n@given(\n    data=hnp.arrays(shape=hnp.array_shapes(), dtype=hnp.floating_dtypes()),\n    constant=st.booleans(),\n)\ndef test_copy(data, constant):\n    x = Tensor(data, constant=constant)\n    y = +x\n    y.backward()\n    y_copy = y.copy()\n\n    assert y.creator is not None\n    assert y.dtype == y_copy.dtype\n    assert y_copy.constant is constant\n    if y.grad is None:\n        assert y_copy.grad is None\n    else:\n        assert_array_equal(y.grad, y_copy.grad)\n    assert_array_equal(y.data, y_copy.data)\n\n\ndef test_to_scalar():\n    nd_tensor = Tensor([1, 2])\n    with raises(TypeError):\n        float(nd_tensor)\n\n    with raises(TypeError):\n        int(nd_tensor)\n\n    with raises(ValueError):\n        nd_tensor.item()\n\n    for size1_tensor in (Tensor(1), Tensor([[1]])):\n        assert float(size1_tensor) == 1.0\n        assert int(size1_tensor) == 1\n        assert size1_tensor.item() == 1.0\n\n\n@pytest.mark.parametrize(\n    (""tensor"", ""repr_""),\n    [\n        (Tensor(1), ""Tensor(1)""),\n        (Tensor([1]), ""Tensor([1])""),\n        (Tensor([1, 2]), ""Tensor([1, 2])""),\n        (\n            mg.arange(9).reshape((3, 3)),\n            ""Tensor([[0, 1, 2],\\n        [3, 4, 5],\\n        [6, 7, 8]])"",\n        ),\n    ],\n)\ndef test_repr(tensor, repr_):\n    assert repr(tensor) == repr_\n\n\n@given(constant=st.booleans())\ndef test_invalid_gradient_raises(constant: bool):\n    x = Tensor(3, constant=constant) * 2\n    with (pytest.raises(InvalidGradient) if not constant else does_not_raise()):\n        x.backward(""bad"")\n\n\n@pytest.mark.parametrize(""element"", (0, [0, 1, 2]))\ndef test_contains(element):\n    t = Tensor([[0, 1, 2], [3, 4, 5]])\n    assert (element in t) is (element in t.data)\n\n\n@given(\n    a=hnp.arrays(\n        shape=hnp.array_shapes(max_side=3, max_dims=5),\n        dtype=float,\n        elements=st.floats(-100, 100),\n    ),\n    constant=st.booleans(),\n    scalar=st.booleans(),\n    creator=st.booleans(),\n)\ndef test_properties(a, constant, scalar, creator):\n    array = np.asarray(a)\n    if creator:\n        ref = Operation()\n        tensor = Tensor(a, constant=constant, _creator=ref, _scalar_only=scalar)\n    else:\n        ref = None\n        tensor = Tensor(a, constant=constant, _scalar_only=scalar)\n\n    assert tensor.ndim == array.ndim\n    assert tensor.shape == array.shape\n    assert tensor.size == array.size\n    assert len(tensor) == len(array)\n    assert tensor.dtype == array.dtype\n    assert_equal(actual=tensor.data, desired=a)\n    assert (not creator) or tensor.creator is ref\n\n\ndef test_init_data():\n    for data in [0, [], (0, 0), ((0, 0), (0, 0)), np.random.rand(3, 4, 2)]:\n        assert_equal(\n            actual=Tensor(data).data,\n            desired=np.asarray(data),\n            err_msg=""Initialization with non-tensor failed"",\n        )\n        assert_equal(\n            actual=Tensor(Tensor(data)).data,\n            desired=np.asarray(data),\n            err_msg=""Initialization with tensor failed"",\n        )\n\n\n@given(x=hnp.arrays(dtype=float, shape=hnp.array_shapes(min_dims=1, max_dims=4)))\ndef test_init_data_rand(x):\n    assert_equal(actual=Tensor(x).data, desired=x)\n\n\n@given(\n    x=hnp.arrays(\n        dtype=float,\n        shape=hnp.array_shapes(),\n        elements=st.floats(allow_infinity=False, allow_nan=False),\n    )\n    | st.floats(allow_infinity=False, allow_nan=False)\n    | st.integers(-100, 100),\n)\ndef test_items(x):\n    """""" verify that tensor.item() mirrors array.item()""""""\n    tensor = Tensor(x)\n    try:\n        value = np.asarray(x).item()\n        assert_array_equal(value, tensor.item())\n    except ValueError:\n        with raises(ValueError):\n            tensor.item()\n\n\nop = Operation()\ndtype_strat = st.sampled_from(\n    (\n        None,\n        int,\n        float,\n        np.int8,\n        np.int16,\n        np.int32,\n        np.int64,\n        np.float16,\n        np.float32,\n        np.float64,\n    )\n)\ndtype_strat_numpy = st.sampled_from(\n    (np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64)\n)\n\n\n@given(\n    data=st.data(),\n    creator=st.sampled_from((None, op)),\n    constant=st.booleans(),\n    scalar_only=st.booleans(),\n    dtype=dtype_strat,\n    numpy_dtype=dtype_strat_numpy,\n)\ndef test_init_params(data, creator, constant, scalar_only, dtype, numpy_dtype):\n    elements = (\n        (lambda x, y: st.floats(x, y, width=8 * np.dtype(numpy_dtype).itemsize))\n        if np.issubdtype(numpy_dtype, np.floating)\n        else st.integers\n    )\n    a = data.draw(\n        hnp.arrays(\n            shape=hnp.array_shapes(max_side=3, max_dims=5),\n            dtype=numpy_dtype,\n            elements=elements(-100, 100),\n        ),\n        label=""a"",\n    )\n    if dtype is not None:\n        a = a.astype(dtype)\n\n    tensor = Tensor(\n        a, _creator=creator, constant=constant, _scalar_only=scalar_only, dtype=dtype\n    )\n\n    assert tensor.creator is creator\n    assert tensor.constant is constant\n    assert tensor.scalar_only is scalar_only\n    assert tensor.dtype is a.dtype\n    assert_equal(tensor.data, a)\n    assert tensor.grad is None\n\n\n@pytest.mark.parametrize(\n    (""op_name"", ""op""),\n    [\n        (""add"", Add),\n        (""sub"", Subtract),\n        (""mul"", Multiply),\n        (""truediv"", Divide),\n        (""pow"", Power),\n        (""matmul"", MatMul),\n    ],\n)\n@pytest.mark.parametrize(""right_op"", [True, False])\n@given(constant_x=st.booleans(), constant_y=st.booleans())\ndef test_special_methods(\n    op_name: str, op: Operation, constant_x: bool, constant_y: bool, right_op: bool\n):\n    if right_op:\n        op_name = ""r"" + op_name\n    op_name = ""__"" + op_name + ""__""\n    x = Tensor([2.0, 8.0, 5.0], constant=constant_x)\n    y = Tensor([1.0, 3.0, 2.0], constant=constant_y)\n\n    constant = constant_x and constant_y\n    assert hasattr(Tensor, op_name)\n    tensor_out = getattr(Tensor, op_name)(x, y)\n    numpy_out = getattr(np.ndarray, op_name)(x.data, y.data)\n    assert isinstance(tensor_out, Tensor)\n    assert tensor_out.constant is constant\n    assert_equal(tensor_out.data, numpy_out)\n    assert isinstance(tensor_out.creator, op)\n\n    if not right_op:\n        assert tensor_out.creator.variables[0] is x\n        assert tensor_out.creator.variables[1] is y\n    else:\n        assert tensor_out.creator.variables[0] is y\n        assert tensor_out.creator.variables[1] is x\n\n\n@given(\n    x=hnp.arrays(shape=hnp.array_shapes(), dtype=hnp.floating_dtypes()),\n    constant=st.booleans(),\n)\ndef test_pos(x: np.ndarray, constant: bool):\n    assume(np.all(np.isfinite(x)))\n    x = Tensor(x, constant=constant)\n    y = +x\n    assert y.creator.variables[0] is x\n    assert_array_equal(y.data, x.data)\n    assert y.constant is x.constant\n\n\n@given(x=hnp.arrays(shape=hnp.array_shapes(), dtype=hnp.floating_dtypes()))\ndef test_neg(x):\n    assume(np.all(np.isfinite(x)))\n    x = Tensor(x)\n    op_name = ""__neg__""\n    assert hasattr(Tensor, op_name)\n    tensor_out = getattr(Tensor, ""__neg__"")(x)\n    numpy_out = getattr(np.ndarray, ""__neg__"")(x.data)\n    assert isinstance(tensor_out, Tensor)\n    assert_equal(tensor_out.data, numpy_out)\n    assert isinstance(tensor_out.creator, Negative)\n    assert tensor_out.creator.variables[0] is x\n\n\n@pytest.mark.parametrize(\n    ""op"", (""__lt__"", ""__le__"", ""__gt__"", ""__ge__"", ""__eq__"", ""__ne__"")\n)\n@given(\n    x=hnp.arrays(\n        shape=hnp.array_shapes(),\n        dtype=hnp.floating_dtypes(),\n        elements=st.floats(-10, 10, width=16),\n    ),\n    x_constant=st.booleans(),\n    y_constant=st.booleans(),\n    data=st.data(),\n)\ndef test_comparison_ops(\n    op: str, x: np.ndarray, x_constant: bool, y_constant: bool, data: st.SearchStrategy\n):\n    y = data.draw(\n        hnp.arrays(shape=x.shape, dtype=x.dtype, elements=st.floats(-10, 10, width=16))\n    )\n    x = Tensor(x, constant=x_constant)\n    y = Tensor(y, constant=y_constant)\n    assert hasattr(Tensor, op), ""`Tensor` is missing the attribute {}"".format(op)\n    tensor_out = getattr(Tensor, op)(x, y)\n    array_out = getattr(np.ndarray, op)(x.data, y.data)\n    assert_equal(actual=tensor_out, desired=array_out)\n\n\n@pytest.mark.parametrize(\n    ""attr"",\n    (\n        ""sum"",\n        ""prod"",\n        ""cumprod"",\n        ""cumsum"",\n        ""mean"",\n        ""std"",\n        ""var"",\n        ""max"",\n        ""min"",\n        ""transpose"",\n        ""squeeze"",\n        ""ravel"",\n    ),\n)\n@given(constant=st.booleans())\ndef test_math_methods(attr: str, constant: bool):\n    x = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], constant=constant)\n\n    assert hasattr(x, attr)\n    method_out = getattr(x, attr).__call__()\n    function_out = getattr(mg, attr).__call__(x)\n    assert_equal(method_out.data, function_out.data)\n    assert method_out.constant is constant\n    assert type(method_out.creator) is type(function_out.creator)\n\n\n# Test https://github.com/rsokl/MyGrad/issues/210\ndef test_0d_iter():\n    x = Tensor(3)\n    with pytest.raises(TypeError):\n        sum(x)\n\n\n@pytest.mark.parametrize(""op"", (""moveaxis"", ""swapaxes""))\n@given(constant=st.booleans())\ndef test_axis_interchange_methods(op: str, constant: bool):\n    x = Tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], constant=constant)\n    method_out = getattr(x, op)(0, -1)\n    function_out = getattr(mg, op)(x, 0, -1)\n    assert_equal(method_out.data, function_out.data)\n    assert method_out.constant is constant\n    assert type(method_out.creator) is type(function_out.creator)\n\n\n@given(\n    x=st.floats(min_value=-1e6, max_value=1e6),\n    y=st.floats(min_value=-1e6, max_value=1e6),\n    z=st.floats(min_value=-1e6, max_value=1e6),\n    clear_graph=st.booleans(),\n)\ndef test_null_gradients(x, y, z, clear_graph):\n    x = Tensor(x)\n    y = Tensor(y)\n    z = Tensor(z)\n\n    f = x * y + z\n    g = x + z * f * f\n\n    # check side effects\n    unused = 2 * g - f\n    w = 1 * f\n    assert unused is not None\n\n    g.backward()\n    assert x.grad is not None\n    assert y.grad is not None\n    assert z.grad is not None\n    assert f.grad is not None\n    assert g.grad is not None\n    assert len(x._ops) > 0\n    assert len(y._ops) > 0\n    assert len(z._ops) > 0\n    assert len(f._ops) > 0\n    assert len(g._ops) > 0\n    assert w.grad is None\n\n    g.null_gradients(clear_graph=clear_graph)\n    assert x.grad is None\n    assert y.grad is None\n    assert z.grad is None\n    assert f.grad is None\n    assert g.grad is None\n\n    if clear_graph:\n        assert len(x._ops) == 0\n        assert len(y._ops) == 0\n        assert len(z._ops) == 0\n        assert len(f._ops) == 0\n        assert len(g._ops) > 0\n        assert x.creator is None\n        assert y.creator is None\n        assert z.creator is None\n        assert f.creator is None\n        assert g.creator is None\n    else:\n        assert len(x._ops) > 0\n        assert len(y._ops) > 0\n        assert len(z._ops) > 0\n        assert len(f._ops) > 0\n        assert len(g._ops) > 0\n        assert x.creator is None\n        assert y.creator is None\n        assert z.creator is None\n        assert f.creator is not None\n        assert g.creator is not None\n\n\n@settings(deadline=None)\n@given(\n    x=st.floats(min_value=-1e-6, max_value=1e6),\n    y=st.floats(min_value=-1e-6, max_value=1e6),\n    z=st.floats(min_value=-1e-6, max_value=1e6),\n)\ndef test_clear_graph(x, y, z):\n    x_orig = x\n    y_orig = y\n    z_orig = z\n\n    x = Tensor(x)\n    y = Tensor(y)\n    z = Tensor(z)\n\n    f = x * y + z\n    g = x + z * f * f\n\n    # check side effects\n    unused = 2 * g - f\n    w = 1 * f\n    assert unused is not None\n\n    g.backward()\n    assert_allclose(f.grad, 2 * z.data * f.data)\n    assert_allclose(x.grad, 1 + 2 * z.data * f.data * y.data)\n    assert_allclose(y.grad, 2 * z.data * f.data * x.data)\n    assert_allclose(z.grad, f.data ** 2 + z.data * 2 * f.data)\n    assert w.grad is None\n\n    assert_array_equal(x.data, x_orig, err_msg=""x was mutated during the operation"")\n    assert_array_equal(y.data, y_orig, err_msg=""y was mutated during the operation"")\n    assert_array_equal(z.data, z_orig, err_msg=""z was mutated during the operation"")\n\n    # null-gradients without clearing the graph, confirm that backprop still works\n    g.null_gradients(clear_graph=False)\n    g.backward()\n    assert_allclose(f.grad, 2 * z.data * f.data)\n    assert_allclose(x.grad, 1 + 2 * z.data * f.data * y.data)\n    assert_allclose(y.grad, 2 * z.data * f.data * x.data)\n    assert_allclose(z.grad, f.data ** 2 + z.data * 2 * f.data)\n    assert w.grad is None\n\n    assert_array_equal(x.data, x_orig, err_msg=""x was mutated during the operation"")\n    assert_array_equal(y.data, y_orig, err_msg=""y was mutated during the operation"")\n    assert_array_equal(z.data, z_orig, err_msg=""z was mutated during the operation"")\n\n    g.null_gradients(clear_graph=False)\n    w.backward()\n    assert_allclose(x.grad, y.data)\n    assert_allclose(y.grad, x.data)\n    assert_allclose(z.grad, np.array(1.0))\n\n    w.clear_graph()\n    assert_allclose(x.grad, y.data)\n    assert_allclose(y.grad, x.data)\n    assert_allclose(z.grad, np.array(1.0))\n    assert len(g._ops) > 0\n    assert g.creator is not None\n    assert len(x._ops) == 0\n    assert len(y._ops) == 0\n    assert len(z._ops) == 0\n    assert len(f._ops) == 0\n    assert x.creator is None\n    assert y.creator is None\n    assert z.creator is None\n    assert f.creator is None\n\n    with raises(InvalidBackprop):\n        g.backward()\n'"
tests/tensor_ops/__init__.py,0,b''
tests/tensor_ops/test_getitem.py,18,"b'import hypothesis.extra.numpy as hnp\nimport numpy as np\nfrom hypothesis import settings\nfrom numpy.testing import assert_allclose\n\nfrom mygrad.tensor_base import Tensor\n\nfrom ..custom_strategies import adv_integer_index, basic_indices\nfrom ..wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef test_getitem():\n    x = Tensor([1, 2, 3])\n    a, b, c = x\n    f = 2 * a + 3 * b + 4 * c\n    f.backward()\n\n    assert a.data == 1\n    assert b.data == 2\n    assert c.data == 3\n    assert f.data == 20\n\n    assert_allclose(a.grad, np.array(2))\n    assert_allclose(b.grad, np.array(3))\n    assert_allclose(c.grad, np.array(4))\n    assert_allclose(x.grad, np.array([2, 3, 4]))\n\n\ndef get_item(*arrs, index, constant=False):\n    o = arrs[0][index]\n    if isinstance(o, Tensor):\n        o._constant = constant\n    return o\n\n\ndef basic_index_wrap(*arrs):\n    return basic_indices(arrs[0].shape)\n\n\ndef adv_index_int_wrap(*arrs):\n    return adv_integer_index(arrs[0].shape)\n\n\ndef adv_index_bool_wrap(*arrs):\n    return hnp.arrays(shape=arrs[0].shape, dtype=bool)\n\n\n@fwdprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=6, max_dims=4)},\n    kwargs=dict(index=basic_index_wrap),\n)\ndef test_getitem_basicindex_fwdprop():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=6, max_dims=4)},\n    kwargs=dict(index=basic_index_wrap),\n    vary_each_element=True,\n)\ndef test_getitem_basicindex_bkwdprop():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=6, max_dims=4)},\n    kwargs=dict(index=adv_index_int_wrap),\n)\ndef test_getitem_advindex_int_fwdprop():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=6, max_dims=4)},\n    kwargs=dict(index=adv_index_int_wrap),\n    vary_each_element=True,\n)\ndef test_getitem_advindex_int_bkwdprop():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=6, max_dims=4)},\n    kwargs=dict(index=adv_index_bool_wrap),\n)\ndef test_getitem_advindex_bool_fwdprop():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=6, max_dims=4)},\n    kwargs=dict(index=adv_index_bool_wrap),\n    vary_each_element=True,\n)\ndef test_getitem_advindex_bool_bkwdprop():\n    pass\n\n\n# test broadcast-compatible int-arrays\nrows = np.array([0, 3], dtype=np.intp)\ncolumns = np.array([0, 2], dtype=np.intp)\n\n\n@fwdprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: (4, 3)},\n    kwargs=dict(index=np.ix_(rows, columns)),\n)\ndef test_getitem_broadcast_index_fwdprop():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: (4, 3)},\n    kwargs=dict(index=np.ix_(rows, columns)),\n    vary_each_element=True,\n)\ndef test_getitem_broadcast_index_bkprop():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: (3, 2, 4, 3)},\n    kwargs=dict(index=(Ellipsis, 2, 0)),\n)\ndef test_getitem_ellipsis_index_fwdprop():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: (3, 2, 4, 3)},\n    kwargs=dict(index=(Ellipsis, 2, 0)),\n    vary_each_element=True,\n)\ndef test_getitem_ellipsis_index_bkprop():\n    pass\n\n\nrows1 = np.array([False, True, False, True])\ncolumns1 = [0, 2]\n\n\n@fwdprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: (4, 3)},\n    kwargs=dict(index=np.ix_(rows1, columns1)),\n)\ndef test_getitem_bool_int_fwdprop():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: (4, 3)},\n    kwargs=dict(index=np.ix_(rows1, columns1)),\n    vary_each_element=True,\n)\ndef test_getitem_bool_int_bkprop():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: (4, 3)},\n    kwargs=dict(index=(slice(1, 2), [1, 2])),\n)\ndef test_getitem_basic_w_adv_fwdprop():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=get_item,\n    true_func=get_item,\n    num_arrays=1,\n    index_to_arr_shapes={0: (4, 3)},\n    kwargs=dict(index=(slice(1, 2), [1, 2])),\n    vary_each_element=True,\n)\ndef test_getitem_basic_w_adv_bkprop():\n    pass\n'"
tests/tensor_ops/test_reshape.py,14,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given\nfrom numpy.testing import assert_allclose\n\nfrom mygrad import reshape\nfrom mygrad.tensor_base import Tensor\n\n\ndef gen_shape(size):\n    """""" Given an array\'s size, generate a compatible random shape\n\n        Parameters\n        ----------\n        size : Integral\n\n        Returns\n        -------\n        Tuple[int, ...] """"""\n\n    def _factors(n):\n        """""" Returns the divisors of n\n\n            >>> _factors(4)\n            {1, 2, 4}""""""\n        gen = ([i, n // i] for i in range(1, int(n ** 0.5) + 1) if n % i == 0)\n        return set(sum(gen, []))\n\n    assert size > 0\n    if size == 1:\n        return (1,)\n\n    shape = []\n    rem = int(size / np.prod(shape))\n    while rem > 1:\n        if len(shape) > 6:\n            shape.append(rem)\n            break\n\n        shape.append(np.random.choice(list(_factors(rem))))\n        rem = int(size / np.prod(shape))\n\n    return tuple(int(i) for i in shape)\n\n\ndef test_input_validation():\n    x = np.array([1, 2])\n\n    with pytest.raises(TypeError):\n        reshape(x)\n\n    with pytest.raises(TypeError):\n        reshape(x, (2,), 2)\n\n\n@given(\n    a=hnp.arrays(\n        shape=hnp.array_shapes(max_side=3, max_dims=5),\n        dtype=float,\n        elements=st.floats(-100, 100),\n    )\n)\ndef test_reshape_method_fwd(a):\n    new_shape = gen_shape(a.size)\n\n    x = Tensor(a).reshape(new_shape)\n    a = a.reshape(new_shape)\n\n    assert x.shape == a.shape, ""Tensor.reshape failed""\n    assert_allclose(a, x.data), ""Tensor.reshape failed""\n\n\n@given(\n    a=hnp.arrays(\n        shape=hnp.array_shapes(max_side=3, max_dims=5),\n        dtype=float,\n        elements=st.floats(-100, 100),\n    )\n)\ndef test_reshape_method_backward(a):\n    new_shape = gen_shape(a.size)\n    grad = np.arange(a.size).reshape(a.shape)\n\n    x = Tensor(a)\n    o = x.reshape(new_shape)\n    o.backward(grad.reshape(new_shape))\n\n    assert x.grad.shape == grad.shape\n    assert_allclose(x.grad, grad)\n\n\n@given(\n    a=hnp.arrays(\n        shape=hnp.array_shapes(max_side=3, max_dims=5),\n        dtype=float,\n        elements=st.floats(-100, 100),\n    )\n)\ndef test_reshape_fwd(a):\n    new_shape = gen_shape(a.size)\n\n    x = Tensor(a)\n    x = reshape(x, new_shape, constant=True)\n    a = a.reshape(new_shape)\n\n    assert x.shape == a.shape, ""Tensor.reshape failed""\n    assert_allclose(a, x.data), ""Tensor.reshape failed""\n\n\n@given(\n    a=hnp.arrays(\n        shape=hnp.array_shapes(max_side=3, max_dims=5),\n        dtype=float,\n        elements=st.floats(-100, 100),\n    )\n)\ndef test_reshape_backward(a):\n    new_shape = gen_shape(a.size)\n    grad = np.arange(a.size).reshape(a.shape)\n\n    x = Tensor(a)\n    o = reshape(x, new_shape, constant=False)\n    o.backward(grad.reshape(new_shape))\n\n    assert x.grad.shape == grad.shape\n    assert_allclose(x.grad, grad)\n'"
tests/tensor_ops/test_sequence_ops.py,4,"b'from functools import reduce\nfrom typing import Callable, Tuple\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given, settings\nfrom numpy.testing import assert_allclose\n\nfrom mygrad import add_sequence, multiply_sequence\nfrom mygrad.tensor_base import Tensor\n\n\n@given(st.lists(st.just(1.0), max_size=1))\ndef test_input_validation(arrays):\n    with pytest.raises(ValueError):\n        add_sequence(*arrays)\n\n    with pytest.raises(ValueError):\n        multiply_sequence(*arrays)\n\n\ndef prod(seq):\n    return reduce(lambda x, y: x * y, seq)\n\n\n@pytest.mark.parametrize(\n    ""sequential_function"", ((add_sequence, sum), (multiply_sequence, prod))\n)\n@settings(deadline=None)\n@given(\n    shapes=st.integers(2, 4).flatmap(\n        lambda n: hnp.mutually_broadcastable_shapes(num_shapes=n, min_dims=0)\n    ),\n    data=st.data(),\n)\ndef test_sequential_arithmetic(\n    sequential_function: Tuple[Callable, Callable],\n    shapes: hnp.BroadcastableShapes,\n    data: st.DataObject,\n):\n    mygrad_func, true_func = sequential_function\n    tensors = data.draw(\n        st.tuples(\n            *(\n                hnp.arrays(\n                    shape=shape, dtype=np.float32, elements=st.floats(-10, 10, width=32)\n                ).map(Tensor)\n                for shape in shapes.input_shapes\n            )\n        ),\n        label=""tensors"",\n    )\n\n    tensors_copy = [x.copy() for x in tensors]\n\n    f = mygrad_func(*tensors)\n    f1 = true_func(tensors_copy)\n\n    assert_allclose(f.data, f1.data)\n\n    f.sum().backward()\n    f1.sum().backward()\n\n    assert_allclose(f.data, f1.data, rtol=1e-4, atol=1e-4)\n\n    for n, (expected, actual) in enumerate(zip(tensors_copy, tensors)):\n        assert_allclose(\n            expected.grad,\n            actual.grad,\n            rtol=1e-4,\n            atol=1e-4,\n            err_msg=""tensor-{}"".format(n),\n        )\n\n    f.null_gradients()\n    assert all(x.grad is None for x in tensors)\n    assert all(not x._ops for x in tensors)\n'"
tests/tensor_ops/test_setitem.py,112,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import assume, given, note, settings\nfrom numpy.testing import assert_allclose, assert_array_equal\n\nfrom mygrad.tensor_base import Tensor\nfrom mygrad.tensor_core_ops.indexing import (\n    _arr,\n    _is_bool_array_index,\n    _is_int_array_index,\n)\n\nfrom ..custom_strategies import adv_integer_index, basic_indices, broadcastable_shapes\nfrom ..utils.numerical_gradient import numerical_gradient_full\n\n\n# test utilties used by setitem\ndef test_arr_util():\n    assert_array_equal(_arr(2, 2), np.arange(4).reshape(2, 2))\n    assert_array_equal(_arr(4, 3), np.arange(12).reshape(4, 3))\n\n\n@pytest.mark.parametrize(\n    (""arr"", ""truth""),\n    [\n        ((0, 0), False),\n        ((np.array([True]),), False),\n        ((np.array([True]), [1]), True),\n        ((np.array([True]), [1]), True),\n        ((np.array([1]), [1]), True),\n        ((np.array([True]), 1), False),\n        ((np.array([True]), slice(None)), False),\n    ],\n)\ndef test_int_array_test(arr, truth):\n    assert _is_int_array_index(arr) is truth\n\n\n@pytest.mark.parametrize(\n    (""arr"", ""truth""),\n    [\n        ((0, 0), False),\n        ((np.array([True]),), True),\n        ((np.array([True]), np.array([False])), False),\n        ((np.array([1]), [1]), False),\n        ((np.array([True]), 1), False),\n        ((np.array([True]), slice(None)), False),\n    ],\n)\ndef test_bool_array_test(arr, truth):\n    assert _is_bool_array_index(arr) is truth\n\n\ndef setitem(x, y, index):\n    x_copy = np.copy(x)\n    x_copy[index] = y\n    return x_copy\n\n\ndef test_setitem_multiple_input():\n    """"""\n    Ensures proper backprop through computational graph\n    in which variable that is set on serves as multiple\n    inputs to a single operation.\n\n    Ensures that null-gradient and clear-graph works properly.\n    """"""\n    from mygrad import add_sequence\n\n    x = Tensor([1.0])\n    y = x + 0\n\n    assert_array_equal(y.data, np.array([1.0]))\n\n    o = add_sequence(y, y, y)\n    y[0] = 4\n\n    assert_array_equal(y.data, np.array([4.0]))\n\n    f = o * y  # 3 * 4\n    f.backward()\n\n    assert_array_equal(o.data, np.array([3.0]))\n    assert_array_equal(f.data, np.array([12.0]))\n\n    assert_array_equal(x.grad, np.array([12.0]))\n    assert_array_equal(o.grad, np.array([4.0]))\n    assert_array_equal(y.grad, np.array([3.0]))\n\n    f.null_gradients()\n    assert x.grad is None and not x._ops and not x._accum_ops\n    assert y.grad is None and not y._ops and not y._accum_ops\n    assert o.grad is None and not o._ops and not o._accum_ops\n    assert f.grad is None and not f._ops and not f._accum_ops\n\n\n@given(x_constant=st.booleans(), y_constant=st.booleans(), data=st.data())\ndef test_setitem_sanity_check(x_constant, y_constant, data):\n    """""" Ensure proper setitem behavior for all combinations of constant/variable Tensors""""""\n    x = Tensor([1.0, 2.0, 3.0, 4.0], constant=x_constant)\n    w = 4 * x\n\n    as_tensor = data.draw(st.booleans()) if y_constant else True\n    y = Tensor([1.0, 0.0], constant=y_constant) if as_tensor else np.array([1.0, 0.0])\n\n    w[::2] = np.array([-1.0, -2.0]) * y\n    assert_allclose(np.array((-1.0, 8.0, 0.0, 16.0)), w.data)\n    w.sum().backward()\n\n    assert isinstance(w, Tensor)\n    assert_allclose(w.data, np.array([-1.0, 8.0, 0.0, 16.0]))\n    assert w.constant is (x.constant and (not as_tensor or y.constant))\n\n    if x.constant:\n        assert x.grad is None\n    else:\n        assert_allclose(x.grad, np.array([0.0, 4.0, 0.0, 4.0]))\n\n    if as_tensor:\n        if y.constant:\n            assert y.grad is None\n        else:\n            assert_allclose(y.grad, np.array([-1.0, -2.0]))\n\n    w.null_gradients()\n    assert x.grad is None, ""null_gradients failed""\n\n    if as_tensor:\n        assert y.grad is None, ""null_gradients failed""\n\n\ndef test_setitem_sanity_check2():\n    x = Tensor([1.0, 2.0, 3.0, 4.0])\n    y = Tensor([-1.0, -2.0, -3.0, -4.0])\n\n    z = x * y\n    y[:] = 0\n\n    z.backward()\n    assert_allclose(np.array([-1.0, -2.0, -3.0, -4.0]), x.grad)\n    assert_allclose(np.array([0.0, 0.0, 0.0, 0.0]), y.data)\n    assert y.grad is None\n\n\ndef test_no_mutate():\n    """""" Ensure setitem doesn\'t mutate variable non-constant tensor""""""\n    x = Tensor([1.0, 2.0])\n    y = Tensor([3.0, 4.0])\n    x + y\n    y[:] = 0\n    y_old = x._ops.pop().variables[-1]  # version of y that participated in x + y\n    assert_allclose(np.array([3.0, 4.0]), y_old.data)\n    assert_allclose(np.array([0.0, 0.0]), y.data)\n\n\n@settings(deadline=None, max_examples=1000)\n@given(\n    x=hnp.arrays(\n        shape=hnp.array_shapes(max_side=4, max_dims=5),\n        dtype=float,\n        elements=st.floats(-10.0, 10.0),\n    ),\n    data=st.data(),\n)\ndef test_setitem_basic_index(x: np.ndarray, data: st.DataObject):\n    """""" index conforms strictly to basic indexing """"""\n    index = data.draw(basic_indices(x.shape), label=""index"")\n    o = np.asarray(x[index])\n\n    note(""x[index]: {}"".format(o))\n    y = data.draw(\n        (\n            hnp.arrays(\n                # Permit shapes that are broadcast-compatible with x[index]\n                # The only excess dimensions permitted in this shape are\n                # leading singletons\n                shape=broadcastable_shapes(o.shape).map(\n                    lambda _x: tuple(\n                        1 if (len(_x) - n) > o.ndim else s for n, s in enumerate(_x)\n                    )\n                ),\n                dtype=float,\n                elements=st.floats(-10.0, 10.0),\n            )\n            if o.shape and o.size\n            else st.floats(-10.0, 10.0).map(lambda _x: np.array(_x))\n        ),\n        label=""y"",\n    )\n\n    x0 = np.copy(x)\n    y0 = np.copy(y)\n\n    x_arr = Tensor(np.copy(x))\n    y_arr = Tensor(np.copy(y))\n    x1_arr = x_arr[:]\n\n    try:\n        x0[index] = y0  # don\'t permit invalid set-items\n    except ValueError:\n        assume(False)\n        return\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    x1_arr[index] = y_arr\n    (x1_arr * grad).sum().backward()\n\n    assert_allclose(x1_arr.data, x0)\n    assert_allclose(y_arr.data, y0)\n\n    dx, dy = numerical_gradient_full(\n        setitem, x, y, back_grad=grad, kwargs=dict(index=index)\n    )\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(y_arr.grad, dy)\n\n\n@settings(deadline=None)\n@given(\n    x=hnp.arrays(\n        shape=hnp.array_shapes(max_side=4, max_dims=5),\n        dtype=float,\n        elements=st.floats(-10.0, 10.0),\n    ),\n    data=st.data(),\n)\ndef test_setitem_adv_int_index(x, data):\n    """""" index consists of a tuple of integer-valued arrays """"""\n    index = data.draw(adv_integer_index(x.shape), label=""index"")\n    o = np.asarray(x[index])\n    y = data.draw(\n        hnp.arrays(\n            shape=broadcastable_shapes(o.shape, max_dims=o.ndim, max_side=max(o.shape)),\n            dtype=float,\n            elements=st.floats(-10.0, 10.0),\n        )\n        if o.shape and o.size\n        else st.floats(-10.0, 10.0).map(lambda _x: np.array(_x)),\n        label=""y"",\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    x0 = np.copy(x)\n    y0 = np.copy(y)\n\n    x_arr = Tensor(np.copy(x))\n    y_arr = Tensor(np.copy(y))\n\n    try:\n        x0[index] = y0  # don\'t permit invalid set-items\n    except ValueError:\n        assume(False)\n        return\n\n    x1_arr = x_arr[:]\n    x1_arr[index] = y_arr\n    (x1_arr * grad).sum().backward()\n\n    assert_allclose(x1_arr.data, x0)\n    assert_allclose(y_arr.data, y0)\n\n    dx, dy = numerical_gradient_full(\n        setitem, x, y, back_grad=grad, kwargs=dict(index=index)\n    )\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(y_arr.grad, dy)\n\n\n@settings(deadline=None)\n@given(\n    x=hnp.arrays(\n        shape=hnp.array_shapes(max_side=4, max_dims=5),\n        dtype=float,\n        elements=st.floats(-10.0, 10.0),\n    ),\n    data=st.data(),\n)\ndef test_setitem_adv_bool_index(x, data):\n    """""" index consists of a single boolean-valued array """"""\n    index = data.draw(hnp.arrays(shape=x.shape, dtype=bool), label=""index"")\n    o = np.asarray(x[index])\n    y = data.draw(\n        hnp.arrays(\n            shape=broadcastable_shapes(o.shape, max_dims=o.ndim, max_side=max(o.shape)),\n            dtype=float,\n            elements=st.floats(-10.0, 10.0),\n        )\n        if o.shape and o.size\n        else st.floats(-10.0, 10.0).map(lambda _x: np.array(_x)),\n        label=""y"",\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    x0 = np.copy(x)\n    y0 = np.copy(y)\n\n    try:\n        x0[index] = y0  # don\'t permit invalid set-items\n    except ValueError:\n        assume(False)\n        return\n\n    x_arr = Tensor(np.copy(x))\n    y_arr = Tensor(np.copy(y))\n    x1_arr = x_arr[:]\n    x1_arr[index] = y_arr\n    (x1_arr * grad).sum().backward()\n\n    assert_allclose(x1_arr.data, x0)\n    assert_allclose(y_arr.data, y0)\n\n    dx, dy = numerical_gradient_full(\n        setitem, x, y, back_grad=grad, kwargs=dict(index=index)\n    )\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(y_arr.grad, dy)\n\n\n@settings(deadline=None)\n@given(\n    x=hnp.arrays(shape=(4, 3), dtype=float, elements=st.floats(-10.0, 10.0)),\n    data=st.data(),\n)\ndef test_setitem_broadcast_index(x, data):\n    """""" index is two broadcast-compatible integer arrays""""""\n    # test broadcast-compatible int-arrays\n    rows = np.array([0, 3], dtype=np.intp)\n    columns = np.array([0, 2], dtype=np.intp)\n    index = np.ix_(rows, columns)\n    o = np.asarray(x[index])\n    y = data.draw(\n        hnp.arrays(\n            shape=broadcastable_shapes(o.shape, max_dims=o.ndim),\n            dtype=float,\n            elements=st.floats(-10.0, 10.0),\n        ),\n        label=""y"",\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    x0 = np.copy(x)\n    y0 = np.copy(y)\n\n    x_arr = Tensor(np.copy(x))\n    y_arr = Tensor(np.copy(y))\n    x1_arr = x_arr[:]\n    x1_arr[index] = y_arr\n    (x1_arr * grad).sum().backward()\n\n    x0[index] = y0\n    assert_allclose(x1_arr.data, x0)\n    assert_allclose(y_arr.data, y0)\n\n    dx, dy = numerical_gradient_full(\n        setitem, x, y, back_grad=grad, kwargs=dict(index=index)\n    )\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(y_arr.grad, dy)\n\n\n@settings(deadline=None)\n@given(\n    x=hnp.arrays(shape=(4, 3), dtype=float, elements=st.floats(-10.0, 10.0)),\n    data=st.data(),\n)\ndef test_setitem_mixed_index(x, data):\n    """""" index is mixes basic and advanced int-array indexing""""""\n    index = (slice(1, 2), [1, 2])\n    o = np.asarray(x[index])\n    y = data.draw(\n        hnp.arrays(\n            shape=broadcastable_shapes(o.shape, max_dims=o.ndim),\n            dtype=float,\n            elements=st.floats(-10.0, 10.0),\n        ),\n        label=""y"",\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    x0 = np.copy(x)\n    y0 = np.copy(y)\n\n    try:\n        x0[index] = y0  # don\'t permit invalid set-items\n    except ValueError:\n        assume(False)\n        return\n\n    x_arr = Tensor(np.copy(x))\n    y_arr = Tensor(np.copy(y))\n    x1_arr = x_arr[:]\n    x1_arr[index] = y_arr\n    (x1_arr * grad).sum().backward()\n\n    x0[index] = y0\n    assert_allclose(x1_arr.data, x0)\n    assert_allclose(y_arr.data, y0)\n\n    dx, dy = numerical_gradient_full(\n        setitem, x, y, back_grad=grad, kwargs=dict(index=index)\n    )\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(y_arr.grad, dy)\n\n\n@settings(deadline=None)\n@given(\n    x=hnp.arrays(shape=(4, 3), dtype=float, elements=st.floats(-10.0, 10.0)),\n    data=st.data(),\n)\ndef test_setitem_broadcast_bool_index(x, data):\n    """""" index mixes boolean and int-array indexing""""""\n    rows = np.array([False, True, False, True])\n    columns = np.array([0, 2], dtype=np.intp)\n    index = np.ix_(rows, columns)\n    o = np.asarray(x[index])\n    y = data.draw(\n        hnp.arrays(\n            shape=broadcastable_shapes(o.shape, max_dims=o.ndim),\n            dtype=float,\n            elements=st.floats(-10.0, 10.0),\n        ),\n        label=""y"",\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    x0 = np.copy(x)\n    y0 = np.copy(y)\n\n    x_arr = Tensor(np.copy(x))\n    y_arr = Tensor(np.copy(y))\n    x1_arr = x_arr[:]\n    x1_arr[index] = y_arr\n    (x1_arr * grad).sum().backward()\n\n    x0[index] = y0\n    assert_allclose(x1_arr.data, x0)\n    assert_allclose(y_arr.data, y0)\n\n    dx, dy = numerical_gradient_full(\n        setitem, x, y, back_grad=grad, kwargs=dict(index=index)\n    )\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(y_arr.grad, dy)\n\n\n@settings(deadline=None)\n@given(\n    x=hnp.arrays(shape=(4, 3), dtype=float, elements=st.floats(-10.0, 10.0)),\n    data=st.data(),\n)\ndef test_setitem_bool_basic_index(x, data):\n    """""" index mixes boolean and basic indexing""""""\n    index = (np.array([False, True, False, True]), np.newaxis, slice(None))\n    o = np.asarray(x[index])\n    y = data.draw(\n        hnp.arrays(\n            shape=broadcastable_shapes(o.shape, max_dims=o.ndim),\n            dtype=float,\n            elements=st.floats(-10.0, 10.0),\n        ),\n        label=""y"",\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    x0 = np.copy(x)\n    y0 = np.copy(y)\n\n    try:\n        x0[index] = y0  # don\'t permit invalid set-items\n    except ValueError:\n        assume(False)\n        return\n\n    x_arr = Tensor(np.copy(x))\n    y_arr = Tensor(np.copy(y))\n    x1_arr = x_arr[:]\n    x1_arr[index] = y_arr\n    (x1_arr * grad).sum().backward()\n\n    assert_allclose(x1_arr.data, x0)\n    assert_allclose(y_arr.data, y0)\n\n    dx, dy = numerical_gradient_full(\n        setitem, x, y, back_grad=grad, kwargs=dict(index=index)\n    )\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(y_arr.grad, dy)\n\n\n@settings(deadline=None)\n@given(\n    x=hnp.arrays(shape=(3, 3), dtype=float, elements=st.floats(-10.0, 10.0)),\n    data=st.data(),\n)\ndef test_setitem_bool_axes_index(x, data):\n    """""" index consists of boolean arrays specified for each axis """"""\n    index = data.draw(\n        st.tuples(\n            hnp.arrays(shape=(3,), dtype=bool), hnp.arrays(shape=(3,), dtype=bool)\n        )\n    )\n    try:\n        o = np.asarray(x[index])\n    except IndexError:\n        return None\n    y = data.draw(\n        hnp.arrays(\n            shape=broadcastable_shapes(o.shape, max_dims=o.ndim, max_side=max(o.shape)),\n            dtype=float,\n            elements=st.floats(-10.0, 10.0),\n        )\n        if o.shape and o.size\n        else st.floats(-10.0, 10.0).map(lambda _x: np.array(_x)),\n        label=""y"",\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(1, 10), unique=True),\n        label=""grad"",\n    )\n\n    x0 = np.copy(x)\n    y0 = np.copy(y)\n\n    try:\n        x0[index] = y0  # don\'t permit invalid set-items\n    except ValueError:\n        assume(False)\n        return\n\n    x_arr = Tensor(np.copy(x))\n    y_arr = Tensor(np.copy(y))\n    x1_arr = x_arr[:]\n    x1_arr[index] = y_arr\n    (x1_arr * grad).sum().backward()\n\n    assert_allclose(x1_arr.data, x0)\n    assert_allclose(y_arr.data, y0)\n\n    dx, dy = numerical_gradient_full(\n        setitem, x, y, back_grad=grad, kwargs=dict(index=index)\n    )\n\n    assert_allclose(x_arr.grad, dx)\n    assert_allclose(y_arr.grad, dy)\n'"
tests/utils/__init__.py,0,"b'from contextlib import contextmanager\n\n\n@contextmanager\ndef does_not_raise():\n    """"""An \'empty\' constext manager that yields ``None``. This is\n    to be used in conjunction with ``pytest.raises`` in scenarios\n    where the tested function ought not raise any exception.\n\n    Examples\n    --------\n    >>> import pytest\n    >>> x = ""hello""\n    >>> with (pytest.raises(AttributeError) if not isinstance(x, str) else does_not_raise()):\n        ... x.lower()\n    """"""\n    yield\n'"
tests/utils/numerical_gradient.py,12,"b'from decimal import Decimal\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\n\nfrom mygrad._utils import reduce_broadcast\n\n\ndef finite_difference(\n    f,\n    *args,\n    back_grad,\n    vary_ind=None,\n    h=Decimal(1) / Decimal(int(1e8)),\n    as_decimal=False,\n    kwargs=None\n):\n    """""" Computes numerical partial derivatives of f(x0, x1, ...) in each\n        of its variables, using the central difference method.\n        This is a ""fast"" method - it varies entire arrays at once. Thus\n        this is only appropriate for trivial vectorized functions that\n        map accross entries of arrays (like add or multiply). E.g.\n        matrix multiplication is *not* suited for this style of gradient.\n\n        Parameters\n        ----------\n        f : Callable[[numpy.ndarray, ...], numpy.ndarray]\n            f(x, ...) -> numpy.ndarray\n        *args : Tuple[numpy.ndarray, ...]\n            The input arguments to be fed to f.\n\n        back_grad : numpy.ndarray\n            The gradient being back-propagated to x and y, via f\n\n        vary_ind : Optional[Tuple[int, ...]]\n            If `None`, the partials of f with respect to all the inputs are.\n            computed. Otherwise you can specify a sequence of the indices\n            of the variables whose partials are to be computed\n               0 -> w.r.t x only, 1 -> w.r.t y only, etc.\n\n        h : float, optional, (default=Decimal(1E-8))\n            Approximating infinitesimal.\n\n        as_decimal : bool, optional (default=True)\n            If True, f\'s arguments are passed as Decimal-type arrays. This\n            improves numerical precision, but is not permitted by some functions.\n\n        kwargs : Optional[Dict]\n\n        Returns\n        -------\n        Tuple[Union[NoneType, numpy.ndarray], ...]\n            df/dx0, df/dx1, ... - evaluated at (`x0`, `x1`, ... ).\n        """"""\n\n    def to_decimal_array(arr):\n        """""" Convert numpy ND-array to Decimal-type object array of the same shape.\n            Used for facilitating high-precision arithmetic.\n\n            Parameters\n            ----------\n            arr : Union[float, numpy.ndarray]\n\n            Returns\n            -------\n            numpy.ndarray\n                Decimal-type object array""""""\n        arr = np.asarray(arr)\n\n        if arr.dtype.kind == ""O"":\n            return arr\n        return np.array(\n            tuple(Decimal(float(i)) for i in arr.flat), dtype=Decimal\n        ).reshape(arr.shape)\n\n    if kwargs is None:\n        kwargs = {}\n\n    if not args:\n        raise ValueError(""At least one value must be passed to `args`"")\n\n    h = Decimal(h) if as_decimal else float(h)\n    two_h = Decimal(2) * h if as_decimal else 2 * h\n\n    args = tuple(to_decimal_array(i) if as_decimal else i for i in args)\n\n    grads = [None] * len(args)\n\n    def gen_fwd_diff(i):\n        # x1, ..., x_i + h, ..., xn\n        return ((var if j != i else var + h) for j, var in enumerate(args))\n\n    def gen_bkwd_diff(i):\n        # x1, ..., x_i - h, ..., xn\n        return ((var if j != i else var - h) for j, var in enumerate(args))\n\n    for n in range(len(args)):\n        if vary_ind is not None and n not in vary_ind:\n            continue\n        # central difference in variable n\n        dvar = (f(*gen_fwd_diff(n), **kwargs) - f(*gen_bkwd_diff(n), **kwargs)) / (\n            two_h\n        )\n        grads[n] = reduce_broadcast(back_grad * dvar.astype(float), args[n].shape)\n\n    return grads\n\n\ndef numerical_gradient(f, *args, back_grad, vary_ind=None, h=1e-20, kwargs=None):\n    """""" Computes numerical partial derivatives of f(x0, x1, ...) in each\n        of its variables, using the central difference method.\n        This is a ""fast"" method - it varies entire arrays at once. Thus\n        this is only appropriate for trivial vectorized functions that\n        map across entries of arrays (like add or multiply). E.g.\n        matrix multiplication is *not* suited for this style of gradient.\n\n        Parameters\n        ----------\n        f : Callable[[numpy.ndarray, ...], numpy.ndarray]\n            f(x, ...) -> numpy.ndarray\n        *args : Tuple[numpy.ndarray, ...]\n            The input arguments to be fed to f.\n\n        back_grad : numpy.ndarray\n            The gradient being back-propagated to x and y, via f\n\n        vary_ind : Optional[Tuple[int, ...]]\n            If `None`, the partials of f with respect to all the inputs are.\n            computed. Otherwise you can specify a sequence of the indices\n            of the variables whose partials are to be computed\n               0 -> w.r.t x only, 1 -> w.r.t y only, etc.\n\n        h : float, optional, (default=Decimal(1E-8))\n            Approximating infinitesimal.\n\n        kwargs : Optional[Dict]\n\n        Returns\n        -------\n        Tuple[Union[NoneType, numpy.ndarray], ...]\n            df/dx0, df/dx1, ... - evaluated at (`x0`, `x1`, ... ).\n        """"""\n\n    if kwargs is None:\n        kwargs = {}\n\n    if not args:\n        raise ValueError(""At least one value must be passed to `args`"")\n\n    args = tuple(i.astype(np.complex128) for i in args)\n    grads = [None] * len(args)\n\n    def gen_fwd_diff(i):\n        # x1, ..., x_i + h, ..., xn\n        return ((var if j != i else var + h * 1j) for j, var in enumerate(args))\n\n    for n in range(len(args)):\n        if vary_ind is not None and n not in vary_ind:\n            continue\n        # central difference in variable n\n        dvar = f(*gen_fwd_diff(n), **kwargs).imag / h\n        grads[n] = reduce_broadcast(back_grad * dvar, args[n].shape)\n\n    return grads\n\n\ndef numerical_gradient_full(\n    f, *args, back_grad, kwargs=None, vary_ind=None\n) -> Tuple[np.ndarray, ...]:\n    """""" Computes numerical partial derivatives of f(x, y, ..., **kwargs), by\n    varying each entry of x, y, ... independently producing a gradient\n    in each variable.\n\n    This method requires that `f` be able to operate on complex-valued arrays.\n\n    Parameters\n    ----------\n    f : Callable[[numpy.ndarray, ...], numpy.ndarray]\n        f(x, ...) -> numpy.ndarray\n\n    *args : numpy.ndarray\n        The array(s) to be passed to f\n\n    back_grad : numpy.ndarray\n        The gradient being back-propagated to {x}, via f\n\n\n    kwargs : Dict[str, Any], optional (default=dict())\n        The keyword arguments to be passed to f.\n\n    vary_ind : Optional[Tuple[int, ...]]\n        If `None`, the partials of f with respect to all the inputs are.\n        computed. Otherwise you can specify a sequence of the indices\n        of the variables whose partials are to be computed\n           0 -> w.r.t x only, 1 -> w.r.t y only, etc.\n\n    Returns\n    -------\n    Tuple[numpy.ndarray, ...]\n        df/dx, df/dy, ...\n        df/dvar will be None if var was excluded via `vary_ind`\n\n    Notes\n    -----\n    The numerical derivative is computed using the so-called complex-step method [1]_:\n\n    .. math::\n               F\'(x_0) = Im(F(x_0+ih))/h + O(h^2)\n\n    Critically, this permits us to compute the numerical difference without subtracting\n    two similar numbers; thus we avoid incurring the typical loss of floating point\n    precision. Accordingly, we need not concern ourselves with selecting a balanced\n    value for :math:`h` to accommodate the trade off of floating point precision with\n    that of the numerical method. The smaller the value of :math:`h`, the better.\n\n    The relationship stated above can be derived trivially via Taylor-series expansion\n    of :math:`F` along the imaginary axis:\n\n\n    .. math::\n                F(x_0+ih) = F(x_0)+ihF\'(x_0)-h^2F\'\'(x_0)/2!-ih^3F^{(3)}/3!+...\n\n                Im(F(x_0+ih)) = hF\'(x_0) + O(h^2)\n\n    This is basically the coolest thing in the world.\n\n    References\n    ----------\n\n    .. [1] Squire, William, and Trapp, George, ""Using complex variables to estimate\n           derivatives of real functions"", SIAM Review 40, 1998, pp. 110-112.\n           epubs.siam.org/doi/abs/10.1137/S003614459631241X\n    """"""\n    if kwargs is None:\n        kwargs = {}\n\n    args = tuple(i.astype(np.complex128) for i in args)\n    grads = [None] * len(args)  # type: List[Optional[np.ndarray]]\n    if isinstance(vary_ind, int):\n        vary_ind = [vary_ind]\n\n    for n in range(len(args)):\n        if vary_ind is not None and n not in vary_ind:\n            continue\n\n        def tmp_f(var: np.ndarray) -> np.ndarray:\n            return f(*args[:n], var, *args[n + 1 :], **kwargs)\n\n        grads[n] = _numerical_gradient_full(tmp_f, x=args[n], back_grad=back_grad)\n    return tuple(grads)\n\n\ndef _numerical_gradient_full(f, *, x, back_grad, h=1e-20):\n    """""" Computes numerical partial derivatives of f(x), by\n        varying each entry of `x` independently.\n\n        Parameters\n        ----------\n        f : Callable[[numpy.ndarray], numpy.ndarray]\n            f(x) -> numpy.ndarray\n\n        x : numpy.ndarray\n            An array storing the sequence(s) of values in the array. More than once\n            sequence may be designated, according to the `axis` argument of `f`.\n\n        back_grad : numpy.ndarray\n            The gradient being back-propagated to {x}, via f\n\n        h : float, optional, (default=Decimal(1E-8))\n            Approximating infinitesimal.\n\n        Returns\n        -------\n        numpy.ndarray\n            df/dx\n        """"""\n\n    grad = np.empty(x.shape, dtype=np.float64)\n    x_orig = np.copy(x)\n    back_grad = back_grad\n\n    for ind, val in np.ndenumerate(x):\n        x_fwd = x\n        x_fwd[ind] = x_orig[ind] + h * 1j\n        f_fwd = f(x_fwd)\n\n        df_dxi = f_fwd.imag / h\n\n        dl_dxi = df_dxi * back_grad\n        grad[ind] = np.float64(\n            dl_dxi.sum() if isinstance(dl_dxi, np.ndarray) else dl_dxi\n        )\n\n        # reset x\n        x[ind] = x_orig[ind]\n    return grad\n'"
tests/utils/test_utils.py,22,"b'"""""" Test `numerical_gradient`, `numerical_derivative`, and `broadcast_check`""""""\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import given\nfrom numpy.testing import assert_allclose\n\nfrom tests.utils.numerical_gradient import (\n    finite_difference,\n    numerical_gradient,\n    numerical_gradient_full,\n)\n\n\ndef unary_func(x):\n    return x ** 2\n\n\ndef binary_func(x, y):\n    return x * y ** 2\n\n\ndef ternary_func(x, y, z):\n    return z * x * y ** 2\n\n\n@given(\n    data=st.data(),\n    x=hnp.arrays(\n        shape=hnp.array_shapes(max_side=3, max_dims=3),\n        dtype=float,\n        elements=st.floats(-10, 10),\n    ),\n)\ndef test_finite_difference_no_broadcast(data, x):\n    atol, rtol = (1e-2, 1e-2)\n    y = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(-100, 100)), label=""y""\n    )\n\n    z = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(-100, 100)), label=""z""\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(-100, 100)),\n        label=""grad"",\n    )\n\n    # check variable-selection\n    assert finite_difference(unary_func, x, back_grad=grad, vary_ind=[])[0] is None\n\n    # no broadcast\n    (dx,) = finite_difference(unary_func, x, back_grad=grad)\n\n    assert_allclose(dx, grad * 2 * x, atol=atol, rtol=rtol)\n\n    dx, dy = numerical_gradient(binary_func, x, y, back_grad=grad)\n    assert_allclose(dx, grad * y ** 2, atol=atol, rtol=rtol)\n    assert_allclose(dy, grad * 2 * x * y, atol=atol, rtol=rtol)\n\n    dx, dy, dz = numerical_gradient(ternary_func, x, y, z, back_grad=grad)\n    assert_allclose(dx, grad * z * y ** 2, atol=atol, rtol=rtol)\n    assert_allclose(dy, grad * z * 2 * x * y, atol=atol, rtol=rtol)\n    assert_allclose(dz, grad * x * y ** 2, atol=atol, rtol=rtol)\n\n\n@given(\n    data=st.data(),\n    x=hnp.arrays(\n        shape=hnp.array_shapes(max_side=3, max_dims=3),\n        dtype=float,\n        elements=st.floats(-100, 100),\n    ),\n)\ndef test_numerical_gradient_no_broadcast(data, x):\n    atol, rtol = (1e-7, 1e-7)\n    y = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(-100, 100)), label=""y""\n    )\n\n    z = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(-100, 100)), label=""z""\n    )\n\n    grad = data.draw(\n        hnp.arrays(shape=x.shape, dtype=float, elements=st.floats(-100, 100)),\n        label=""grad"",\n    )\n\n    # check variable-selection\n    assert numerical_gradient(unary_func, x, back_grad=grad, vary_ind=[])[0] is None\n\n    # no broadcast\n    (dx,) = numerical_gradient(unary_func, x, back_grad=grad)\n\n    assert_allclose(dx, grad * 2 * x, atol=atol, rtol=rtol)\n\n    dx, dy = numerical_gradient(binary_func, x, y, back_grad=grad)\n    assert_allclose(dx, grad * y ** 2, atol=atol, rtol=rtol)\n    assert_allclose(dy, grad * 2 * x * y, atol=atol, rtol=rtol)\n\n    dx, dy, dz = numerical_gradient(ternary_func, x, y, z, back_grad=grad)\n    assert_allclose(dx, grad * z * y ** 2, atol=atol, rtol=rtol)\n    assert_allclose(dy, grad * z * 2 * x * y, atol=atol, rtol=rtol)\n    assert_allclose(dz, grad * x * y ** 2, atol=atol, rtol=rtol)\n\n\n@given(\n    x=hnp.arrays(shape=(3, 4), dtype=float, elements=st.floats(-100, 100)),\n    y=hnp.arrays(shape=(2, 3, 4), dtype=float, elements=st.floats(-100, 100)),\n    grad=hnp.arrays(shape=(2, 3, 4), dtype=float, elements=st.floats(-100, 100)),\n)\ndef test_numerical_gradient_x_broadcast(x, y, grad):\n    atol, rtol = (1e-7, 1e-7)\n\n    # broadcast x\n    dx, dy = numerical_gradient(binary_func, x, y, back_grad=grad)\n    assert_allclose(dx, (grad * y ** 2).sum(axis=0), atol=atol, rtol=rtol)\n    assert_allclose(dy, grad * 2 * x * y, atol=atol, rtol=rtol)\n\n\n@given(\n    y=hnp.arrays(shape=(3, 4), dtype=float, elements=st.floats(-100, 100)),\n    x=hnp.arrays(shape=(2, 3, 4), dtype=float, elements=st.floats(-100, 100)),\n    grad=hnp.arrays(shape=(2, 3, 4), dtype=float, elements=st.floats(-100, 100)),\n)\ndef test_numerical_gradient_y_broadcast(x, y, grad):\n    atol, rtol = (1e-7, 1e-7)\n\n    # broadcast x\n    dx, dy = numerical_gradient(binary_func, x, y, back_grad=grad)\n    assert_allclose(dx, grad * y ** 2, atol=atol, rtol=rtol)\n    assert_allclose(dy, (grad * 2 * x * y).sum(axis=0), atol=atol, rtol=rtol)\n\n\n@given(\n    x=hnp.arrays(shape=(2, 1, 4), dtype=float, elements=st.floats(-100, 100)),\n    y=hnp.arrays(shape=(1, 3, 4), dtype=float, elements=st.floats(-100, 100)),\n    grad=hnp.arrays(shape=(2, 3, 4), dtype=float, elements=st.floats(-100, 100)),\n)\ndef test_numerical_gradient_xy_broadcast(x, y, grad):\n    atol, rtol = (1e-7, 1e-7)\n\n    # broadcast x\n    dx, dy = numerical_gradient(binary_func, x, y, back_grad=grad)\n    x_grad = (grad * y ** 2).sum(axis=1, keepdims=True)\n    y_grad = (grad * 2 * x * y).sum(axis=0, keepdims=True)\n    assert_allclose(dx, x_grad, atol=atol, rtol=rtol)\n    assert_allclose(dy, y_grad, atol=atol, rtol=rtol)\n\n\n@given(\n    x=hnp.arrays(dtype=float, elements=st.floats(-1, 1), shape=(2,)),\n    grad=hnp.arrays(dtype=float, elements=st.floats(-1, 1), shape=(2,)),\n)\ndef test_numerical_gradient_vary_each(x, grad):\n    atol, rtol = (1e-7, 1e-7)\n    (dx,) = numerical_gradient_full(lambda y: y[::-1], x, back_grad=np.array(grad))\n    x_grad = grad[::-1]\n    assert_allclose(actual=dx, desired=x_grad, atol=atol, rtol=rtol)\n'"
tests/wrappers/__init__.py,0,b''
tests/wrappers/test_uber.py,3,"b'import hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import given, settings\nfrom numpy.testing import assert_allclose\n\nimport mygrad as mg\n\nfrom .uber import backprop_test_factory\n\n\nclass KWARG1:\n    passed = False\n\n\nname_to_pos = dict(a=0, b=1, c=2)\n\n\n@settings(deadline=None, max_examples=20)\n@given(\n    args_as_kwargs=st.fixed_dictionaries(\n        {},\n        optional=dict(\n            a=st.just(mg.Tensor(1.0)),\n            b=st.just(mg.Tensor(2.0)),\n            c=st.just(mg.Tensor(3.0)),\n            kwarg1=st.just(KWARG1()),\n        ),\n    ),\n)\ndef test_arr_from_kwargs(args_as_kwargs):\n    expected_a = 1.0 if ""a"" in args_as_kwargs else -1.0\n    expected_b = 2.0 if ""b"" in args_as_kwargs else -2.0\n    expected_c = 3.0 if ""c"" in args_as_kwargs else -3.0\n    KWARG1.passed = False\n    arrs_from_kwargs = {name_to_pos[k]: k for k in args_as_kwargs if k != ""kwarg1""}\n\n    def sentinel(a, b, c, kwarg1=None):\n        assert_allclose(\n            np.real(a if isinstance(a, np.ndarray) else a.data),\n            expected_a,\n            err_msg=f""Got bad value for `a`: {a}"",\n        )\n\n        assert_allclose(\n            np.real(b if isinstance(b, np.ndarray) else b.data),\n            expected_b,\n            err_msg=f""Got bad value for `b`: {b}"",\n        )\n        assert_allclose(\n            np.real(c if isinstance(c, np.ndarray) else c.data),\n            expected_c,\n            err_msg=f""Got bad value for `c`: {c}"",\n        )\n\n        if kwarg1 is not None:\n            assert isinstance(kwarg1, KWARG1)\n            KWARG1.passed = True\n\n        return a * b * c\n\n    @settings(deadline=None, max_examples=5)\n    @backprop_test_factory(\n        mygrad_func=sentinel,\n        true_func=sentinel,\n        num_arrays=3,\n        index_to_arr_shapes={0: tuple(), 1: tuple(), 2: tuple()},\n        index_to_bnds={0: (-1, -1), 1: (-2, -2), 2: (-3, -3)},\n        arrs_from_kwargs=arrs_from_kwargs,\n        kwargs=args_as_kwargs,\n    )\n    def factory_func():\n        pass\n\n    factory_func()\n\n    if ""kwarg1"" in args_as_kwargs:\n        assert KWARG1.passed\n'"
tests/wrappers/uber.py,33,"b'from copy import copy\nfrom functools import wraps\nfrom itertools import combinations\nfrom numbers import Real\nfrom typing import Any, Callable, Dict, Optional, Sequence, Tuple, Union\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import assume, given\nfrom hypothesis.strategies import SearchStrategy\nfrom hypothesis.strategies._internal.lazy import LazyStrategy\nfrom numpy.testing import assert_allclose, assert_array_equal\n\nfrom mygrad import Tensor\n\nfrom ..utils.numerical_gradient import (\n    finite_difference,\n    numerical_gradient,\n    numerical_gradient_full,\n)\n\n\ndef _to_dict(x):\n    if x is None:\n        return {}\n    return x\n\n\ndef get_hypothesis_db_key(self, *extras: Any) -> bytes:\n    """"""\n    The Hypothesis example database is keyed off a hash of the test function,\n    and so reusing a test function via this kind of factory can lead to\n    unexpected collisions or evictions.  While it\'s never *incorrect*, this\n    can make the DB much less effective.\n\n    Assigning a distinctive bytestring - such as a stable hash or repr of\n    all the arguments to the factory instance - to the `add_digest` attribute\n    will adjust the key, as if it was a @pytest.mark.parametrize-d test.\n\n    (this isn\'t part of the public interface either, so it could stop working\n    or even break things in future; but since MyGrad and Hypothesis share\n    maintainers we can probably cope with that.)\n    """"""\n\n    def stable_maybe_dict_repr(x):\n        return sorted(_to_dict(x).items())\n\n    attributes = (\n        getattr(self.op, ""__qualname__"", repr(self.op)),\n        # function (__qualname__), ufunc (__name__), or functools.partial (repr)\n        getattr(\n            self.true_func,\n            ""__qualname__"",\n            getattr(self.true_func, ""__name__"", repr(self.true_func)),\n        ),\n        stable_maybe_dict_repr(self.tolerances),\n        stable_maybe_dict_repr(self.index_to_bnds),\n        self.default_bnds,\n        stable_maybe_dict_repr(self.index_to_no_go),\n        stable_maybe_dict_repr(self.index_to_arr_shapes),\n        self.num_arrays,\n        self.shapes,\n        *extras,\n    )\n    return repr(attributes).encode()\n\n\nclass fwdprop_test_factory:\n    """""" Decorator\n\n        Randomly draw N arrays x, (...) to verify that a mygrad function,\n        `f(x, ..., **kwargs)` returns a correct result through forward-propagation.\n\n        By default, all arrays will have shapes that are broadcast-compatible with x.\n\n        This decorator is extremely uber: the decorated function\'s body is\n        never executed; this decorator builds the entire unit tests The\n        function definition is effectively there to name the test being\n        constructed.\n\n        The user specifies the number of arrays to be generated (at least one must be\n        generated), along with the bounds on their elements, their shapes, as well\n        as the keyword-args to be passed to the function.\n\n        Examples\n        --------\n        Writing a test that compares mygrad\'s add to numpy\'s add\n\n        >>> from mygrad import add\n        >>> import numpy as np\n        >>> @fwdprop_test_factory(mygrad_func=add, true_func=np.add, num_array_args=2)\n        ... def test_add():\n        ...     pass\n        """"""\n\n    def __init__(\n        self,\n        *,\n        mygrad_func: Callable[[Tensor], Tensor],\n        true_func: Callable[[np.ndarray], np.ndarray],\n        num_arrays: Optional[int] = None,\n        shapes: Optional[hnp.MutuallyBroadcastableShapesStrategy] = None,\n        index_to_bnds: Dict[int, Tuple[int, int]] = None,\n        default_bnds: Tuple[float, float] = (-1e6, 1e6),\n        index_to_no_go: Dict[int, Sequence[int]] = None,\n        kwargs: Union[\n            Callable, Dict[str, Union[Any, Callable[[Any], SearchStrategy]]]\n        ] = None,\n        index_to_arr_shapes: Dict[int, Union[Sequence[int], SearchStrategy]] = None,\n        atol: float = 1e-7,\n        rtol: float = 1e-7,\n        assumptions: Optional[Callable[..., bool]] = None,\n        permit_0d_array_as_float: bool = True,\n    ):\n        """"""\n        Parameters\n        ----------\n        mygrad_func : Callable[[numpy.ndarray, ...], mygrad.Tensor]\n            The mygrad function whose forward pass validity is being checked.\n\n        true_func : Callable[[numpy.ndarray, ...], numpy.ndarray]\n            A known correct version of the function\n\n        num_arrays : Optional[int]\n            The number of arrays to be fed to the function\n\n        shapes : Optional[hnp.MutuallyBroadcastableShapesStrategy]\n            A strategy that generates all of the input shapes to feed to the function.\n\n        index_to_bnds : Dict[int, Tuple[int, int]]\n            Indicate the lower and upper bounds from which the elements\n            for array-i is drawn.\n\n        default_bnds : Tuple[float, float]\n            Default lower and upper bounds from which all array elements are drawn\n\n        index_to_no_go : Dict[int, Sequence[int]]\n            Values that array-i cannot possess. By default, no values are\n            excluded.\n\n        index_to_arr_shapes : Dict[int, Union[Sequence[int], hypothesis.searchstrategy.SearchStrategy]]\n            The shape for array-i. This can be an exact shape or a hypothesis search\n            strategy that draws shapes.\n                Default for array-0: `hnp.array_shapes(max_side=3, max_dims=3)`\n                Default for array-i: `broadcastable_shapes(arr-0.shape)`\n\n        kwargs : Union[Callable, Dict[str, Union[Any, Callable[[Any], SearchStrategy]]]]\n            Keyword arguments and their values to be passed to the functions.\n            The values can be hypothesis search-strategies, in which case\n            a value when be drawn at test time for that argument using the provided\n            strategy.\n\n            Note that any search strategy must be ""wrapped"" in a function, which\n            will be called, passing it the list of arrays as an input argument, such\n            that the strategy can draw based on those particular arrays.\n\n        assumptions : Optional[Callable[[arrs, **kwargs], bool]]\n            A callable that is fed the generated arrays and keyword arguments that will\n            be fed to ``mygrad_func``. If ``assumptions`` returns ``False``, that test\n            case will be marked as skipped by hypothesis.\n\n        permit_0d_array_as_float : bool, optional (default=True)\n            If True, drawn 0D arrays will potentially be cast to numpy-floats.\n        """"""\n        self.tolerances = dict(atol=atol, rtol=rtol)\n        index_to_bnds = _to_dict(index_to_bnds)\n        index_to_no_go = _to_dict(index_to_no_go)\n        kwargs = _to_dict(kwargs)\n        index_to_arr_shapes = _to_dict(index_to_arr_shapes)\n\n        if not ((num_arrays is not None) ^ (shapes is not None)):\n            raise ValueError(\n                f""Either `num_arrays`(={num_arrays}) must be specified ""\n                f""xor `shapes`(={shapes}) must be specified""\n            )\n\n        if shapes is not None:\n            if not isinstance(shapes, st.SearchStrategy):\n                raise TypeError(\n                    f""`shapes` should be ""\n                    f""Optional[hnp.MutuallyBroadcastableShapesStrategy]""\n                    f"", got {shapes}""\n                )\n\n            shapes_type = (\n                shapes.wrapped_strategy if isinstance(shapes, LazyStrategy) else shapes\n            )\n\n            if not isinstance(shapes_type, hnp.MutuallyBroadcastableShapesStrategy):\n                raise TypeError(\n                    f""`shapes` should be ""\n                    f""Optional[hnp.MutuallyBroadcastableShapesStrategy]""\n                    f"", got {shapes}""\n                )\n            num_arrays = shapes_type.num_shapes\n\n        assert num_arrays > 0\n\n        self.op = mygrad_func\n        self.true_func = true_func\n\n        self.index_to_bnds = index_to_bnds\n        self.default_bnds = default_bnds\n        self.index_to_no_go = index_to_no_go\n        self.index_to_arr_shapes = index_to_arr_shapes\n        self.kwargs = kwargs\n        self.num_arrays = num_arrays\n        self.shapes = shapes\n        self.assumptions = assumptions\n        self.permit_0d_array_as_float = permit_0d_array_as_float\n\n        # stores the indices of the unspecified array shapes\n        self.missing_shapes = set(range(self.num_arrays)) - set(\n            self.index_to_arr_shapes\n        )\n\n        if shapes is None:\n            self.shapes = (\n                hnp.mutually_broadcastable_shapes(num_shapes=len(self.missing_shapes))\n                if self.missing_shapes\n                else st.just(hnp.BroadcastableShapes(input_shapes=(), result_shape=()))\n            )\n        else:\n            self.shapes = shapes\n\n    def arrays(self, i: int) -> st.SearchStrategy:\n        """"""\n        Hypothesis search strategy for drawing an array y to be passed to f(x, ..., y_i,...).\n        By default, y is drawn to have a shape that is broadcast-compatible with x.\n\n        Parameters\n        ----------\n        i : int\n            The argument index-location of y in the signature of f.\n\n        Returns\n        -------\n        hypothesis.searchstrategy.SearchStrategy""""""\n        return hnp.arrays(\n            shape=self.index_to_arr_shapes.get(i),\n            dtype=float,\n            elements=st.floats(*self.index_to_bnds.get(i, self.default_bnds)),\n        )\n\n    def __call__(self, f):\n        @given(shapes=self.shapes, constant=st.booleans(), data=st.data())\n        @wraps(f)\n        def wrapper(shapes: hnp.BroadcastableShapes, constant, data: st.DataObject):\n            self.index_to_arr_shapes.update(\n                (k, v) for k, v in zip(sorted(self.missing_shapes), shapes.input_shapes)\n            )\n\n            # list of drawn arrays to feed to functions\n            arrs = data.draw(\n                st.tuples(*(self.arrays(i) for i in range(self.num_arrays))),\n                label=""arrays"",\n            )\n\n            # list of array-copies to check for mutation\n            arr_copies = tuple(copy(arr) for arr in arrs)\n\n            if callable(self.kwargs):\n                kwargs = data.draw(self.kwargs(*arrs))\n                if not isinstance(kwargs, dict):\n                    raise TypeError(\n                        f""`kwargs` was a search strategy. This needs to draw dictionaries,""\n                        f""instead drew: {kwargs}""\n                    )\n            else:\n                # set or draw keyword args to be passed to functions\n                kwargs = {\n                    k: (data.draw(v(*arrs), label=f""kwarg: {f}"") if callable(v) else v)\n                    for k, v in self.kwargs.items()\n                }\n\n            if self.assumptions is not None:\n                assume(self.assumptions(*arrs, **kwargs))\n\n            for i, arr in enumerate(\n                arrs\n            ):  # assure arrays don\'t contain forbidden values\n                for value in self.index_to_no_go.get(i, ()):\n                    assume(np.all(arr != value))\n\n            if self.permit_0d_array_as_float:\n                # potentially cast a 0D array as a float\n                arrs = tuple(\n                    arr.item()\n                    if arr.ndim == 0\n                    and data.draw(st.booleans(), label=f""arr-{n} to float"")\n                    else arr\n                    for n, arr in enumerate(arrs)\n                )\n\n            # execute mygrad and ""true"" functions. Compare outputs and check mygrad behavior\n            o = self.op(*(Tensor(i) for i in arrs), **kwargs, constant=constant)\n            tensor_out = o.data\n            true_out = self.true_func(*arrs, **kwargs)\n\n            assert isinstance(\n                o, Tensor\n            ), ""`mygrad_func` returned type {type(o)}, should return `mygrad.Tensor`""\n            assert (\n                o.constant is constant\n            ), f""`mygrad_func` returned tensor.constant={o.constant}, should be constant={constant}""\n\n            assert_allclose(\n                actual=tensor_out,\n                desired=true_out,\n                err_msg=""`mygrad_func(x)` and `true_func(x)` produce different results"",\n                **self.tolerances,\n            )\n\n            for n, (arr, arr_copy) in enumerate(zip(arrs, arr_copies)):\n                assert_array_equal(\n                    arr, arr_copy, err_msg=f""arr-{n} was mutated during forward prop"",\n                )\n\n        wrapper._hypothesis_internal_add_digest = get_hypothesis_db_key(\n            self, self.permit_0d_array_as_float\n        )\n        return wrapper\n\n\nclass backprop_test_factory:\n    """""" Decorator\n\n        Randomly draw arrays x, ... to verify that a binary mygrad function,\n        `f(x, ..., **kwargs)` performs back-propagation appropriately.\n\n        x.grad, ... are compared against numerical derivatives of f.\n\n        This decorator is extremely uber: the decorated function\'s body is\n        never executed. The function definition is effectively there to name\n        the test being constructed. This constructs the entire test\n\n        Notes\n        -----\n        By default this wrapper dispatches a numerical derivative that utilizes the complex\n        step methodology. This requires that the function being tested be analytic and have\n        a complex value implementation. See `tests.utils.numerical_gradient` for more details.\n\n        Examples\n        --------\n        >>> from mygrad import add\n        >>> import numpy as np\n        >>> @backprop_test_factory(mygrad_func=add, true_func=np.add)\n        ... def test_add():\n        ...     pass""""""\n\n    def __init__(\n        self,\n        *,\n        mygrad_func: Callable[[Tensor], Tensor],\n        true_func: Callable[[np.ndarray], np.ndarray],\n        num_arrays: Optional[int] = None,\n        shapes: Optional[hnp.MutuallyBroadcastableShapesStrategy] = None,\n        index_to_bnds: Optional[Dict[int, Tuple[int, int]]] = None,\n        default_bnds: Tuple[float, float] = (-1e6, 1e6),\n        index_to_no_go: Optional[Dict[int, Sequence[int]]] = None,\n        index_to_arr_shapes: Optional[\n            Dict[int, Union[Sequence[int], SearchStrategy]]\n        ] = None,\n        index_to_unique: Optional[Union[Dict[int, bool], bool]] = None,\n        elements_strategy: Optional[SearchStrategy] = None,\n        kwargs: Optional[\n            Union[Callable, Dict[str, Union[Any, Callable[[Any], SearchStrategy]]]]\n        ] = None,\n        arrs_from_kwargs: Optional[Dict[int, str]] = None,\n        h: float = 1e-20,\n        rtol: float = 1e-8,\n        atol: float = 1e-8,\n        vary_each_element: bool = False,\n        use_finite_difference=False,\n        assumptions: Optional[Callable[..., bool]] = None,\n    ):\n        """"""\n        Parameters\n        ----------\n        mygrad_func : Callable[[numpy.ndarray, ...], mygrad.Tensor]\n            The mygrad function whose backward pass validity is being checked.\n\n        true_func : Callable[[numpy.ndarray, ...], numpy.ndarray]\n            A known correct version of the function, which is used to compute\n            numerical derivatives.\n\n        num_arrays : Optional[int]\n            The number of arrays that must be passed to ``mygrad_func``\n\n        shapes : Optional[hnp.MutuallyBroadcastableShapesStrategy]\n            A strategy that generates all of the input shapes to feed to the function.\n\n        index_to_bnds : Optional[Dict[int, Tuple[int, int]]]\n            Indicate the lower and upper bounds from which the elements\n            for array-i is drawn. By default, [-100, 100].\n\n        default_bnds : Tuple[float, float]\n            Default lower and upper bounds from which all array elements are drawn.\n\n        index_to_no_go : Optional[Dict[int, Sequence[int]]]\n            Values that array-i cannot possess. By default, no values are\n            excluded.\n\n        index_to_arr_shapes : Optional[Dict[int, Union[Sequence[int], SearchStrategy]]]\n            The shape for array-i. This can be an exact shape or a hypothesis search\n            strategy that draws shapes.\n                Default for array-0: `hnp.array_shapes(max_side=3, max_dims=3)`\n                Default for array-i: `broadcastable_shapes(arr-0.shape)`\n\n        index_to_unique : Optional[Union[Dict[int, bool], bool]]\n            Determines whether the elements drawn for each of the input-arrays are\n            required to be unique or not. By default this is `False` for each array.\n            If a single boolean value is supplied, this is applied for every array.\n\n        elements_strategy : Optional[Union[SearchStrategy]\n            The hypothesis-type-strategy used to draw the array elements.\n            The default value is ``hypothesis.strategies.floats``.\n\n        kwargs : Optional[Dict[str, Union[Any, Callable[[Any], SearchStrategy]]]]\n            Keyword arguments and their values to be passed to the functions.\n            The values can be hypothesis search strategies, in which case\n            a value when be drawn at test time for that argument.\n\n            Note that any search strategy must be ""wrapped"" in a function, which\n            will be called, passing it the list of arrays as an input argument, such\n            that the strategy can draw based on those particular arrays.\n\n        arrs_from_kwargs : Optional[Dict[int, str]]\n            The mapping i (int) -> k (str) indicates that array-i should be\n            derived from kwargs[k], which must be a numpy array or MyGrad\n            tensor.\n\n        vary_each_element : bool, optional (default=False)\n            If False, then use a faster numerical derivative that varies entire\n            arrays at once: arr -> arr + h; valid only for functions that map over\n            entries, like \'add\' and \'sum\'. Otherwise, the gradient is constructed\n            by varying each element of each array independently.\n\n        use_finite_difference : bool, optional (default=False)\n            If True, the finite-difference method will be used to compute the numerical\n            derivative instead of the complex step method (default). This is necessary\n            if the function being tested is not analytic or does not have a complex-value\n            implementation.\n\n        assumptions : Optional[Callable[[arrs, **kwargs], bool]]\n            A callable that is fed the generated arrays and keyword arguments that will\n            be fed to ``mygrad_func``. If ``assumptions`` returns ``False``, that test\n            case will be marked as skipped by hypothesis.\n        """"""\n\n        index_to_bnds = _to_dict(index_to_bnds)\n        index_to_no_go = _to_dict(index_to_no_go)\n        index_to_arr_shapes = _to_dict(index_to_arr_shapes)\n        index_to_unique = _to_dict(index_to_unique)\n        self.elements_strategy = (\n            elements_strategy if elements_strategy is not None else st.floats\n        )\n        kwargs = _to_dict(kwargs)\n        arrs_from_kwargs = _to_dict(arrs_from_kwargs)\n\n        if not set(arrs_from_kwargs) <= (\n            set(range(num_arrays)) if num_arrays is not None else set()\n        ):\n\n            raise ValueError(\n                ""`kwargs_to_arr` must map an array-ID to a kwarg-name. ""\n                ""Got invalid key(s): ""\n                + "", "".join(\n                    k\n                    for k in set(arrs_from_kwargs)\n                    - (set(range(num_arrays)) if num_arrays is not None else set())\n                )\n            )\n\n        if any(not isinstance(v, str) for v in arrs_from_kwargs.values()):\n            raise ValueError(\n                ""`kwargs_to_arr` must map an array-ID to a kwarg-name.""\n                ""Got invalid key(s): ""\n                + "", "".join(\n                    v for v in arrs_from_kwargs.values() if not isinstance(v, str)\n                )\n            )\n\n        self.arrs_from_kwargs = arrs_from_kwargs\n\n        if not ((num_arrays is not None) ^ (shapes is not None)):\n            raise ValueError(\n                f""Either `num_arrays`(={num_arrays}) must be specified ""\n                f""xor `shapes`(={shapes}) must be specified""\n            )\n\n        if shapes is not None:\n            if not isinstance(shapes, st.SearchStrategy):\n                raise TypeError(\n                    f""`shapes` should be ""\n                    f""Optional[hnp.MutuallyBroadcastableShapesStrategy]""\n                    f"", got {shapes}""\n                )\n\n            shapes_type = (\n                shapes.wrapped_strategy if isinstance(shapes, LazyStrategy) else shapes\n            )\n\n            if not isinstance(shapes_type, hnp.MutuallyBroadcastableShapesStrategy):\n                raise TypeError(\n                    f""`shapes` should be ""\n                    f""Optional[hnp.MutuallyBroadcastableShapesStrategy]""\n                    f"", got {shapes}""\n                )\n            num_arrays = shapes_type.num_shapes\n\n        assert num_arrays > 0\n\n        self.op = mygrad_func\n        self.true_func = true_func\n\n        self.default_bnds = default_bnds\n        if isinstance(index_to_bnds, (tuple, list, np.ndarray)):\n            index_to_bnds = {k: index_to_bnds for k in range(num_arrays)}\n        self.index_to_bnds = index_to_bnds\n\n        if isinstance(index_to_no_go, (tuple, list, np.ndarray)):\n            index_to_no_go = {k: index_to_no_go for k in range(num_arrays)}\n        self.index_to_no_go = index_to_no_go\n\n        if isinstance(\n            index_to_arr_shapes, (tuple, list, np.ndarray, st.SearchStrategy)\n        ):\n            index_to_arr_shapes = {k: index_to_arr_shapes for k in range(num_arrays)}\n            self.index_to_arr_shapes = index_to_arr_shapes\n        self.index_to_arr_shapes = index_to_arr_shapes\n\n        if isinstance(index_to_unique, bool):\n            index_to_unique = {k: index_to_unique for k in range(num_arrays)}\n        self.index_to_unique = index_to_unique\n        self.kwargs = kwargs\n        self.num_arrays = num_arrays\n\n        assert isinstance(h, Real) and h > 0\n        self.h = h\n\n        self.tolerances = dict(rtol=rtol, atol=atol)\n\n        assert isinstance(vary_each_element, bool)\n        self.vary_each_element = vary_each_element\n\n        assert assumptions is None or callable(assumptions)\n        self.assumptions = assumptions\n\n        assert isinstance(use_finite_difference, bool)\n        self.use_finite_difference = use_finite_difference\n\n        if use_finite_difference and vary_each_element:\n            raise NotImplementedError(\n                ""`finite_difference` does not have an implementation supporting ""\n                ""\\n`vary_each_element=True`""\n            )\n\n        if use_finite_difference and h < 1e-8:\n            from warnings import warn\n\n            warn(\n                f""The `finite_difference` method is being used with an h-value of {h}.""\n                f""\\nThis is likely too small, and was intended for use with the complex-step ""\n                f""\\nmethod. Please update `h` in this call to `backprop_test_factory`""\n            )\n\n        # stores the indices of the unspecified array shapes\n        self.missing_shapes = set(range(self.num_arrays)) - set(\n            self.index_to_arr_shapes\n        )\n\n        if shapes is None:\n            self.shapes = (\n                hnp.mutually_broadcastable_shapes(num_shapes=len(self.missing_shapes))\n                if self.missing_shapes\n                else st.just(hnp.BroadcastableShapes(input_shapes=(), result_shape=()))\n            )\n        else:\n            self.shapes = shapes\n\n    def arrays(self, i: int) -> st.SearchStrategy:\n        """"""\n        Hypothesis search strategy for drawing an array y to be passed to f(x, ..., y_i,...).\n        By default, y is drawn to have a shape that is broadcast-compatible with x.\n\n        Parameters\n        ----------\n        i : int\n            The argument index-location of y in the signature of f.\n\n        Returns\n        -------\n        hypothesis.searchstrategy.SearchStrategy""""""\n        return hnp.arrays(\n            shape=self.index_to_arr_shapes.get(i),\n            dtype=float,\n            elements=self.elements_strategy(\n                *self.index_to_bnds.get(i, self.default_bnds)\n            ),\n            unique=self.index_to_unique.get(i, False),\n        )\n\n    def __call__(self, f):\n        @given(shapes=self.shapes, data=st.data())\n        @wraps(f)\n        def wrapper(shapes: hnp.BroadcastableShapes, data: st.DataObject):\n            self.index_to_arr_shapes.update(\n                (k, v) for k, v in zip(sorted(self.missing_shapes), shapes.input_shapes)\n            )\n\n            # list of drawn arrays to feed to functions\n            arrs = data.draw(\n                st.tuples(\n                    *(\n                        self.arrays(i).map(Tensor)\n                        for i in range(self.num_arrays)\n                        if i not in self.arrs_from_kwargs\n                    )\n                ).map(list),\n                label=""arrays"",\n            )\n\n            if callable(self.kwargs):\n                kwargs = data.draw(self.kwargs(*arrs), label=""kwargs"")\n                if not isinstance(kwargs, dict):\n                    raise TypeError(\n                        f""`kwargs` was a search strategy. This needs to draw dictionaries,""\n                        f""instead drew: {kwargs}""\n                    )\n            else:\n                # The keyword args to be passed to `self.op`. If any provided argument is callable\n                # it is assumed to by a hypothesis search strategy, and all of the drawn arrays will\n                # be passed to the strategy, in order to draw a value for that keyword argument.\n                # Otherwise the provided value is used as-is.\n                kwargs = {\n                    k: (data.draw(v(*arrs), label=f""kwarg: {k}"") if callable(v) else v)\n                    for k, v in self.kwargs.items()\n                }\n\n            if not set(self.arrs_from_kwargs.values()) <= set(kwargs):\n                raise ValueError(\n                    f""`arrs_from_kwargs` specifies kwargs that aren\'t present: ""\n                    f""{\', \'.join(v for v in self.arrs_from_kwargs.values() if v not in kwargs)}""\n                )\n\n            for arr_id, key in sorted(\n                self.arrs_from_kwargs.items(), key=lambda x: x[0]\n            ):\n                v = kwargs.pop(key)\n                if not isinstance(v, (np.ndarray, Tensor)):\n                    raise ValueError(\n                        f""kwarg {key} is to be used as array-{arr_id}, but is neither ""\n                        f""an array nor a tensor, got {v}""\n                    )\n\n                arrs.insert(arr_id, Tensor(v))\n\n            arrs = tuple(arrs)\n\n            arr_copies = tuple(copy(arr) for arr in arrs)\n\n            if self.assumptions is not None:\n                assume(self.assumptions(*arrs, **kwargs))\n\n            for i, arr in enumerate(\n                arrs\n            ):  # assure arrays don\'t contain forbidden values\n                for value in self.index_to_no_go.get(i, ()):\n                    assume(np.all(arr != value))\n\n            # forward pass of the function\n            out = self.op(*arrs, **kwargs)\n\n            # gradient to be backpropped through this operation\n            grad = data.draw(\n                hnp.arrays(\n                    shape=out.shape,\n                    dtype=float,\n                    elements=st.floats(-10, 10),\n                    unique=True,\n                ),\n                label=""grad"",\n            )\n            grad_copy = copy(grad)  # keep a copy to check for later mutations\n\n            # compute analytic derivatives via mygrad-backprop\n            if any(out.shape != i.shape for i in arrs):\n                # Broadcasting occurred\n                # Must reduce `out` to scalar\n                # first multiply by `grad` to simulate non-trivial back-prop\n                (grad * out).sum().backward()\n            else:\n                out.backward(grad)\n\n            if not self.use_finite_difference:\n                # compute derivatives via numerical approximation of derivative\n                # using the complex-step method\n                numerical_grad = (\n                    numerical_gradient_full\n                    if self.vary_each_element\n                    else numerical_gradient\n                )\n\n            else:\n                numerical_grad = finite_difference\n            grads_numerical = numerical_grad(\n                self.true_func, *(i.data for i in arrs), back_grad=grad, kwargs=kwargs\n            )\n\n            # check that the analytic and numeric derivatives match\n            for n, (arr, d_num) in enumerate(zip(arrs, grads_numerical)):\n                assert arr.grad is not None, f""arr-{n} grad is None, expected {d_num}""\n                assert_allclose(\n                    arr.grad,\n                    d_num,\n                    **self.tolerances,\n                    err_msg=f""arr-{n}: mygrad derivative and numerical derivative do not match"",\n                )\n\n                # check that none of the set derivatives is a view of `grad`\n                assert not np.shares_memory(\n                    arr.grad, grad\n                ), f""arr-{n}.grad stores a view of grad""\n\n            # check that none of the set derivatives are views of one another\n            for arr_i, arr_j in combinations(arrs, 2):\n                assert not np.shares_memory(\n                    arr_i.grad, arr_j.grad\n                ), ""two input arrays were propagated views of the same gradient""\n\n            # verify that null_gradients works\n            out.null_gradients()\n            assert all(i.grad is None for i in arrs), ""null_gradients failed""\n\n            # check if any of the input-arrays were mutated\n            for n, (arr, arr_copy) in enumerate(zip(arrs, arr_copies)):\n                assert_array_equal(\n                    arr.data,\n                    arr_copy.data,\n                    err_msg=f""arr-{n} was mutated during backward prop"",\n                )\n\n            # check if `grad` was mutated\n            assert_array_equal(\n                grad, grad_copy, err_msg=""`grad` was mutated during backward prop""\n            )\n\n        wrapper._hypothesis_internal_add_digest = get_hypothesis_db_key(\n            self,\n            self.elements_strategy,\n            self.h,\n            self.vary_each_element,\n            self.use_finite_difference,\n        )\n        return wrapper\n'"
src/mygrad/indexing_routines/__init__.py,0,b''
src/mygrad/indexing_routines/funcs.py,3,"b'import numpy as np\n\nfrom mygrad.tensor_base import Tensor\n\nfrom .ops import Where\n\n__all__ = [""where""]\n\n\nclass _UniqueIdentifier:\n    def __init__(self, identifier):\n        self.identifier = identifier\n\n    def __repr__(self):  # pragma: nocover\n        return self.identifier\n\n\nnot_set = _UniqueIdentifier(""not_set"")\n\n\ndef where(condition, x=not_set, y=not_set, constant=False):\n    """"""\n    where(condition, [x, y])\n\n    Return elements chosen from `x` or `y` depending on `condition`.\n\n    .. note::\n        When only ``condition`` is provided, this function is a shorthand for\n        ``np.asarray(condition).nonzero()``. The rest of this\n        documentation covers only the case where all three arguments are\n        provided.\n\n    This docstring was adapted from that of ``numpy.where``.\n\n    Parameters\n    ----------\n    condition : array_like, bool\n        Where True, yield `x`, otherwise yield ``y``. ``x``, ``y``\n        and `condition` need to be broadcastable to some shape.\n\n    x : array_like\n        Values from which to chosen where ``condition`` is ``True``.\n\n    y : array_like\n       Values from which to chosen where ``condition`` is ``False``.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    out : mygrad.Tensor\n        A tensor with elements from `x` where `condition` is True, and elements\n        from `y` elsewhere.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> a = mg.arange(10)\n    >>> a\n    Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> mg.where(a < 5, a, 10*a)\n    Tensor([ 0,  1,  2,  3,  4, 50, 60, 70, 80, 90])\n\n    This can be used on multidimensional tensors too:\n\n    >>> mg.where([[True, False], [True, True]],\n    ...          [[1, 2], [3, 4]],\n    ...          [[9, 8], [7, 6]])\n    Tensor([[1, 8],\n            [3, 4]])\n\n    The shapes of x, y, and the condition are broadcast together:\n\n    >>> x, y = np.ogrid[:3, :4]\n    >>> mg.where(x < y, x, 10 + y)  # both x and 10+y are broadcast\n    Tensor([[10,  0,  0,  0],\n            [10, 11,  1,  1],\n            [10, 11, 12,  2]])\n\n    >>> a = mg.Tensor([[0, 1, 2],\n    ...                [0, 2, 4],\n    ...                [0, 3, 6]])\n    >>> mg.where(a < 4, a, -1)  # -1 is broadcast\n    Tensor([[ 0,  1,  2],\n            [ 0,  2, -1],\n            [ 0,  3, -1]])\n    """"""\n    if x is not_set and y is not_set:\n        if isinstance(condition, Tensor):\n            condition = condition.data\n        return np.where(condition)\n\n    if x is not_set or y is not_set:\n        raise ValueError(""either both or neither of x and y should be given"")\n\n    return Tensor._op(\n        Where, x, y, op_kwargs=dict(condition=condition), constant=constant\n    )\n'"
src/mygrad/indexing_routines/ops.py,3,"b'import numpy as np\n\nfrom mygrad.operation_base import BroadcastableOp\n\n\nclass Where(BroadcastableOp):\n    def __call__(self, a, b, *, condition):\n        self.variables = (a, b)\n        self.condition = np.asarray(condition, dtype=bool)\n        return np.where(condition, a.data, b.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        condition = self.condition if index == 0 else ~self.condition\n        return np.where(condition, grad, 0)\n'"
src/mygrad/linalg/__init__.py,0,b''
src/mygrad/linalg/funcs.py,23,"b'import numpy as np\nfrom numpy.core.einsumfunc import _parse_einsum_input\n\nimport mygrad as mg\nfrom mygrad import Tensor\n\nfrom .ops import *\n\n__all__ = [""multi_matmul"", ""matmul"", ""einsum""]\n\n\ndef matmul(a, b, constant=False):\n    r""""""\n    Matrix product of two tensors:\n\n    ``matmul(x, y)`` is equivalent to ``x @ y``.\n\n    This documentation was adapted from ``numpy.matmul``\n\n    The behavior depends on the arguments in the following way.\n\n    - If both arguments are 2-D they are multiplied like conventional\n      matrices.\n    - If either argument is N-D, N > 2, it is treated as a stack of\n      matrices residing in the last two indexes and broadcast accordingly.\n    - If the first argument is 1-D, it is promoted to a matrix by\n      prepending a 1 to its dimensions. After matrix multiplication\n      the prepended 1 is removed.\n    - If the second argument is 1-D, it is promoted to a matrix by\n      appending a 1 to its dimensions. After matrix multiplication\n      the appended 1 is removed.\n\n    Multiplication by a scalar is not allowed, use ``*`` instead. Note that\n    multiplying a stack of matrices with a vector will result in a stack of\n    vectors, but matmul will not recognize it as such.\n\n    ``matmul`` differs from ``numpy.dot`` in two important ways.\n\n    - Multiplication by scalars is not allowed.\n    - Stacks of matrices are broadcast together as if the matrices\n      were elements.\n\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    output : mygrad.Tensor\n        Returns the matrix product of `a` and `b`.  If `a` and `b` are both\n        1-D arrays then a scalar is returned; otherwise an array is\n        returned.\n\n\n    Raises\n    ------\n    ValueError\n        If the last dimension of `a` is not the same size as\n        the second-to-last dimension of `b`.\n\n        If scalar value is passed.\n\n    Notes\n    -----\n    The matmul function implements the semantics of the `@` operator introduced\n    in Python 3.5 following PEP465.\n\n    Examples\n    --------\n    For two 2D tensors, ``matmul(a, b)`` is the matrix product :math:`\\sum_{j}{A_{ij} B_{jk}} = F_{ik}`:\n\n    >>> import mygrad as mg\n    >>> a = [[1, 0], [0, 1]]\n    >>> b = [[4, 1], [2, 2]]\n    >>> mg.matmul(a, b)\n    Tensor([[4, 1],\n            [2, 2]])\n\n    For 2-D mixed with 1-D, the result is the matrix-vector product, :math:`\\sum_{j}{A_{ij} B_{j}} = F_{i}`:\n\n    >>> a = [[1, 0], [0, 1]]\n    >>> b = [1, 2]\n    >>> mg.matmul(a, b)\n    Tensor([1, 2])\n\n    Broadcasting is conventional for stacks of arrays. Here ``a`` is treated\n    like a stack of three 5x6 matrices, and the 6x4 matrix ``b`` is broadcast\n    matrix-multiplied against each one. This produces a shape-(3, 5, 4) tensor\n    as a result.\n\n    >>> a = mg.arange(3*5*6).reshape((3,5,6))\n    >>> b = mg.arange(6*4).reshape((6,4))\n    >>> mg.matmul(a,b).shape\n    (3, 5, 4)\n\n    Scalar multiplication raises an error.\n\n    >>> mg.matmul(a, 3)\n    Traceback (most recent call last):\n    ...\n    ValueError: Scalar operands are not allowed, use \'*\' instead""""""\n    return Tensor._op(MatMul, a, b, constant=constant)\n\n\ndef einsum(*operands, optimize=False, constant=False):\n    r""""""\n    einsum(subscripts, *operands)\n\n    Evaluates the Einstein summation convention on the operands. This implementation\n    exactly mirrors that of ``numpy.einsum`` and supports back-propagation through\n    all variety of tensor-products, sums, traces, and views that it can perform.\n\n    The following docstring was adapted from the documentation for ``numpy.einsum``\n\n    Using the Einstein summation convention, many common multi-dimensional\n    array operations can be represented in a simple fashion.  This function\n    provides a way to compute such summations. The best way to understand this\n    function is to try the examples below, which show how many common NumPy/MyGrad\n    functions can be implemented as calls to ``einsum``.\n\n    Back-propagation via ``einsum`` is optimized such that any tensor that occurs\n    redundantly within the summation will only have its gradient computed once.\n    This optimization accommodates all number and combination of redundancies that can\n    be encountered.\n\n    E.g. back-propping through ``einsum(\'...,...->\', x, x)`` will only incur a single\n    computation/accumulation for ``x.grad`` rather than two. This permits users to\n    leverage the efficiency of sum-reduction, where ``(x ** 2).sum()`` is sub-optimal,\n    without being penalized during back-propagation.\n\n    Parameters\n    ----------\n    subscripts : str\n        Specifies the subscripts for summation.\n\n    operands : array_like\n        The tensors used in the summation.\n\n    optimize : {False, True, \'greedy\', \'optimal\'}, optional (default=False)\n        Controls if intermediate optimization should occur; also enables\n        the use of BLAS where possible. This can produce significant speedups\n        for computations like matrix multiplication.\n\n        No optimization will occur if False and True will default to the \'greedy\'\n        algorithm. Also accepts an explicit contraction list from the\n        ``np.einsum_path`` function. See ``np.einsum_path`` for more details.\n\n    constant : bool, optional (default=False)\n        If True, the resulting Tensor is a constant.\n\n    Returns\n    -------\n    output : mygrad.Tensor\n        The calculation based on the Einstein summation convention.\n\n    Notes\n    -----\n    The subscripts string is a comma-separated list of subscript labels,\n    where each label refers to a dimension of the corresponding operand.\n    Repeated subscripts labels in one operand take the diagonal.  For example,\n    ``einsum(\'ii\', a)`` is equivalent to ``np.trace(a)`` (however, the former\n    supports back-propagation).\n\n    Whenever a label is repeated, it is summed, so ``einsum(\'i, i\', a, b)``\n    is equivalent to ``np.inner(a, b)``.  If a label appears only once,\n    it is not summed, so ``einsum(\'i\', a)`` produces a view of ``a``\n    with no changes.\n\n    The order of labels in the output is by default alphabetical.  This\n    means that ``np.einsum(\'ij\', a)`` doesn\'t affect a 2D tensor, while\n    ``einsum(\'ji\', a)`` takes its transpose.\n\n    The output can be controlled by specifying output subscript labels\n    as well.  This specifies the label order, and allows summing to\n    be disallowed or forced when desired.  The call ``einsum(\'i->\', a)``\n    is like ``np.sum(a, axis=-1)``, and ``einsum(\'ii->i\', a)``\n    is like ``np.diag(a)``.  The difference is that `einsum` does not\n    allow broadcasting by default.\n\n    To enable and control broadcasting, use an ellipsis.  Default\n    NumPy-style broadcasting is done by adding an ellipsis\n    to the left of each term, like ``einsum(\'...ii->...i\', a)``.\n    To take the trace along the first and last axes,\n    you can do ``einsum(\'i...i\', a)``, or to do a matrix-matrix\n    product with the left-most indices instead of rightmost, you can do\n    ``einsum(\'ij...,jk...->ik...\', a, b)``.\n\n    When there is only one operand, no axes are summed, and no output\n    parameter is provided, a view into the operand is returned instead\n    of a new tensor.  Thus, taking the diagonal as ``einsum(\'ii->i\', a)``\n    produces a view.\n\n    An alternative way to provide the subscripts and operands is as\n    ``einsum(op0, sublist0, op1, sublist1, ..., [sublistout])``. The examples\n    below have corresponding `einsum` calls with the two parameter methods.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> import numpy as np\n    >>> a = mg.arange(25).reshape(5,5)\n    >>> b = mg.arange(5)\n    >>> c = mg.arange(6).reshape(2,3)\n\n    Compute the trace of ``a``, :math:`\\sum_{i}{A_{ii}} = f`:\n\n    >>> einsum(\'ii\', a)\n    Tensor(60)\n    >>> einsum(a, [0, 0])\n    Tensor(60)\n    >>> np.trace(a.data)\n    array(60)\n\n    Return a view along the diagonal of ``a``, :math:`A_{ii} = F_{i}`:\n\n    >>> einsum(\'ii->i\', a)\n    Tensor([ 0,  6, 12, 18, 24])\n    >>> einsum(a, [0,0], [0])\n    Tensor([ 0,  6, 12, 18, 24])\n    >>> np.diag(a.data)\n    array([ 0,  6, 12, 18, 24])\n\n    Compute the matrix-vector product of ``a`` with ``b``, :math:`\\sum_{j}{A_{ij} B_{j}} = F_{i}`:\n\n    >>> einsum(\'ij,j\', a, b)\n    Tensor([ 30,  80, 130, 180, 230])\n    >>> einsum(a, [0,1], b, [1])\n    Tensor([ 30,  80, 130, 180, 230])\n    >>> mg.matmul(a, b)\n    Tensor([ 30,  80, 130, 180, 230])\n    >>> einsum(\'...j,j\', a, b)\n    Tensor([ 30,  80, 130, 180, 230])\n\n    Take the transpose of ``c``, :math:`C_{ji} = F_{ij}`:\n\n    >>> einsum(\'ji\', c)\n    Tensor([[0, 3],\n            [1, 4],\n            [2, 5]])\n    >>> einsum(c, [1, 0])\n    Tensor([[0, 3],\n            [1, 4],\n            [2, 5]])\n    >>> c.T\n    Tensor([[0, 3],\n            [1, 4],\n            [2, 5]])\n\n    Compute ``3 * c``:\n\n    >>> einsum(\'..., ...\', 3, c)\n    Tensor([[ 0,  3,  6],\n            [ 9, 12, 15]])\n    >>> einsum(\',ij\', 3, c)\n    Tensor([[ 0,  3,  6],\n            [ 9, 12, 15]])\n    >>> einsum(3, [Ellipsis], c, [Ellipsis])\n    Tensor([[ 0,  3,  6],\n            [ 9, 12, 15]])\n    >>> 3 * c\n    Tensor([[ 0,  3,  6],\n            [ 9, 12, 15]])\n\n    Compute the inner product of ``b`` with itself, :math:`\\sum_{i}{B_{i} B_{i}} = f`:\n\n    >>> einsum(\'i,i\', b, b)\n    Tensor(30)\n    >>> einsum(b, [0], b, [0])\n    Tensor(30)\n    >>> np.inner(b.data, b.data)\n    30\n\n    Compute the outer product of ``array([1, 2])`` with ``b``, :math:`A_{i}B_{j} = F_{ij}`:\n\n    >>> einsum(\'i,j\', np.arange(2)+1, b)\n    Tensor([[0, 1, 2, 3, 4],\n           [0, 2, 4, 6, 8]])\n    >>> einsum(np.arange(2)+1, [0], b, [1])\n    Tensor([[0, 1, 2, 3, 4],\n           [0, 2, 4, 6, 8]])\n    >>> np.outer(np.arange(2)+1, b)\n    array([[0, 1, 2, 3, 4],\n           [0, 2, 4, 6, 8]])\n    >>> einsum(\'i...->...\', a)\n    Tensor([50, 55, 60, 65, 70])\n    >>> einsum(a, [0,Ellipsis], [Ellipsis])\n    Tensor([50, 55, 60, 65, 70])\n    >>> np.sum(a, axis=0)\n    array([50, 55, 60, 65, 70])\n\n    Compute the tensor product :math:`\\sum_{ij}{A_{ijk} B_{jil}} = F_{kl}`\n\n    >>> a = mg.arange(60.).reshape(3,4,5)\n    >>> b = mg.arange(24.).reshape(4,3,2)\n    >>> einsum(\'ijk,jil->kl\', a, b)\n    Tensor([[ 4400.,  4730.],\n            [ 4532.,  4874.],\n            [ 4664.,  5018.],\n            [ 4796.,  5162.],\n            [ 4928.,  5306.]])\n    >>> einsum(a, [0,1,2], b, [1,0,3], [2,3])\n    Tensor([[ 4400.,  4730.],\n            [ 4532.,  4874.],\n            [ 4664.,  5018.],\n            [ 4796.,  5162.],\n            [ 4928.,  5306.]])\n    >>> np.tensordot(a,b, axes=([1,0],[0,1]))\n    array([[ 4400.,  4730.],\n            [ 4532.,  4874.],\n            [ 4664.,  5018.],\n            [ 4796.,  5162.],\n            [ 4928.,  5306.]])\n\n    Matrix multiply ``a.T`` with ``b.T``, :math:`\\sum_{k}{A_{ki} B_{jk}} = F_{ij}`\n\n    >>> a = mg.arange(6).reshape((3,2))\n    >>> b = mg.arange(12).reshape((4,3))\n    >>> einsum(\'ki,jk->ij\', a, b)\n    Tensor([[10, 28, 46, 64],\n            [13, 40, 67, 94]])\n    >>> einsum(\'ki,...k->i...\', a, b)\n    Tensor([[10, 28, 46, 64],\n            [13, 40, 67, 94]])\n    >>> einsum(\'k...,jk\', a, b)\n    Tensor([[10, 28, 46, 64],\n            [13, 40, 67, 94]])\n\n    Make an assignment to a view along the diagonal of ``a``:\n\n    >>> a = mg.zeros((3, 3))\n    >>> einsum(\'ii->i\', a).data[:] = 1\n    >>> a\n    Tensor([[ 1.,  0.,  0.],\n            [ 0.,  1.,  0.],\n            [ 0.,  0.,  1.]])\n    """"""\n\n    # TODO: normalize error handling for invalid inputs\n    operands = list(operands)\n    if isinstance(operands[0], str):\n        # operands form: ""ijk, ijk"", x, y\n        variables = operands[1:]\n        if any(isinstance(i, Tensor) for i in operands):\n            operands[1:] = (\n                var.data if isinstance(var, Tensor) else var for var in operands[1:]\n            )\n    else:\n        # operands form: op0, sublist0, op1, sublist1, ..., [sublistout]\n        end = -1 if len(operands) % 2 else None  # -1 if sublistout is included\n        variables = operands[:end:2]\n        if any(isinstance(i, Tensor) for i in operands):\n            operands[:end:2] = (\n                var.data if isinstance(var, Tensor) else var for var in operands[:end:2]\n            )\n\n    in_lbls, out_lbls, _ = _parse_einsum_input(operands)\n    return Tensor._op(\n        EinSum,\n        *variables,\n        op_kwargs=dict(in_lbls=in_lbls, out_lbls=out_lbls, optimize=optimize),\n        constant=constant\n    )\n\n\ndef multi_matmul(tensors, constant=False):\n    """"""\n    Matrix product of two or more tensors calculated in the optimal ordering\n\n    Parameters\n    ----------\n    tensors: Sequence[array_like]\n        The sequence of tensors to be matrix-multiplied.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient).\n\n    Returns\n    -------\n    mygrad.Tensor\n        Returns the matrix product of the tensors provided\n\n\n    Extended Summary\n    ----------------\n    This documentation was adapted from ``numpy.linalg.multi_dot``\n\n    Compute the matrix multiplication of two or more arrays in a single function\n    call, while automatically selecting the fastest evaluation order.\n    ``multi_matmul`` chains ``matmul`` and uses optimal parenthesization  [1]_ [2]_.\n    Depending on the shapes of the matrices, this can speed up the multiplication a lot.\n\n    If the first argument is 1-D it is treated as a row vector.\n\n    If the last argument is 1-D it is treated as a column vector.\n\n    The other arguments must be 2-D or greater.\n\n    Think of `multi_dot` as an optimized version of::\n\n        def multi_dot(tensors): return functools.reduce(mg.matmul, tensors)\n\n    Raises\n    ------\n    ValueError\n        If ``tensors`` contains less than two array_like items.\n\n    ValueError\n        If ``tensor`` other than the first or last is less than two dimensional\n\n    See Also\n    --------\n    matmul : matrix multiplication with two arguments.\n\n    References\n    ----------\n\n    .. [1] Cormen, ""Introduction to Algorithms"", Chapter 15.2, p. 370-378\n    .. [2] http://en.wikipedia.org/wiki/Matrix_chain_multiplication\n\n    Notes\n    -----\n    The cost for a matrix multiplication can be calculated with the\n    following function::\n\n        def cost(A, B):\n            return A.shape[0] * A.shape[1] * B.shape[1]\n\n    Let\'s assume we have three matrices :math:`A_{10x100}, B_{100x5}, C_{5x50}`.\n\n    The costs for the two different parenthesizations are as follows::\n\n        cost((AB)C) = 10*100*5 + 10*5*50   = 5000 + 2500   = 7500\n        cost(A(BC)) = 10*100*50 + 100*5*50 = 50000 + 25000 = 75000\n\n    Examples\n    --------\n    ``multi_matmul`` allows you to write:\n\n    >>> from mygrad import multi_matmul, matmul, Tensor\n    >>> import numpy as np\n    >>> # Prepare some random tensors\n    >>> A = Tensor(np.random.random((10000, 100)))\n    >>> B = Tensor(np.random.random((100, 1000)))\n    >>> C = Tensor(np.random.random((1000, 5)))\n    >>> D = Tensor(np.random.random((5, 333)))\n    >>> # the actual matrix multiplication\n    >>> multi_matmul([A, B, C, D]) # computes (A @ (B @ C)) @ D\n\n    instead of:\n\n    >>> matmul(matmul(matmul(A, B), C), D)\n    >>> # or\n    >>> A @ B @ C @ D\n    """"""\n\n    for a in tensors:\n        if not (1 <= a.ndim <= 2):\n            raise ValueError(\n                ""%d-dimensional tensor given. Tensor must be one or two-dimensional""\n                % (a.ndim,)\n            )\n\n    n = len(tensors)\n    if n < 2:\n        raise ValueError(""Expecting at least two arrays."")\n    elif n == 2:\n        return matmul(tensors[0], tensors[1], constant)\n\n    tensors = [a if isinstance(a, Tensor) else np.asarray(a) for a in tensors]\n\n    # save original ndim to reshape the result array into the proper form later\n    ndim_first, ndim_last = tensors[0].ndim, tensors[-1].ndim\n\n    # Explicitly convert vectors to 2D arrays to keep the logic of this function simpler\n    if tensors[0].ndim == 1:\n        tensors[0] = mg.expand_dims(\n            tensors[0],\n            axis=0,\n            constant=tensors[0].constant if isinstance(tensors[0], Tensor) else True,\n        )\n    if tensors[-1].ndim == 1:\n        tensors[-1] = mg.expand_dims(\n            tensors[-1],\n            axis=1,\n            constant=tensors[-1].constant if isinstance(tensors[-1], Tensor) else True,\n        )\n\n    if n == 3:\n        result = _multi_matmul_three(tensors[0], tensors[1], tensors[2], constant)\n    else:\n        order = _multi_matmul_chain_order(tensors)\n        result = _multi_matmul(tensors, order, 0, n - 1, constant)\n\n    # return proper shape since we possibly added dimensions to the first\n    # and last arrays\n    if ndim_first == 1 and ndim_last == 1:\n        return result[0, 0]\n    elif ndim_first == 1 or ndim_last == 1:\n        return result.reshape(-1)\n    else:\n        return result\n\n\ndef _multi_matmul_three(A, B, C, constant=False) -> Tensor:\n    """"""\n    Find the best order for three arrays and do the multiplication.\n\n    """"""\n    a0, a1b0 = A.shape[-2:]\n    b1c0, c1 = C.shape[-2:]\n    cost1 = a0 * b1c0 * (a1b0 + c1)\n    cost2 = a1b0 * c1 * (a0 + b1c0)\n\n    if cost1 < cost2:\n        return matmul(matmul(A, B, constant), C, constant)\n    else:\n        return matmul(A, matmul(B, C, constant), constant)\n\n\ndef _multi_matmul_chain_order(arrays):\n    """"""\n    Return a np.array that encodes the optimal order of multiplications.\n    The optimal order array is then used by `_multi_matmul()` to do the\n    multiplication.\n\n    The implementation CLOSELY follows Cormen, ""Introduction to Algorithms"",\n    Chapter 15.2, p. 370-378.  Note that Cormen uses 1-based indices.\n\n        cost[i, j] = min([\n            cost[prefix] + cost[suffix] + cost_mult(prefix, suffix)\n            for k in range(i, j)])\n    """"""\n    n = len(arrays)\n    # p stores the dimensions of the matrices\n    # Example for p: A_{10x100}, B_{100x5}, C_{5x50} --> p = [10, 100, 5, 50]\n    # Using -2 to generalize for shapes that are more than 2 dimmensions\n    p = [a.shape[-2] for a in arrays] + [arrays[-1].shape[-1]]\n    # m is a matrix of costs of the subproblems\n    # m[i,j]: min number of scalar multiplications needed to compute A_{i..j}\n    m = np.zeros((n, n), dtype=np.double)\n    # s is the actual ordering\n    # s[i, j] is the value of k at which we split the product A_i..A_j\n    s = np.empty((n, n), dtype=np.intp)\n\n    for l in range(1, n):\n        for i in range(n - l):\n            j = i + l\n            m[i, j] = np.inf\n            for k in range(i, j):\n                q = m[i, k] + m[k + 1, j] + p[i] * p[k + 1] * p[j + 1]\n                if q < m[i, j]:\n                    m[i, j] = q\n                    s[i, j] = k  # Note that Cormen uses 1-based index\n    return s\n\n\ndef _multi_matmul(arrays, order, i, j, constant=False) -> Tensor:\n    """"""Actually do the multiplication with the given order.""""""\n    if i == j:\n        return arrays[i]\n    else:\n        return matmul(\n            _multi_matmul(arrays, order, i, order[i, j], constant),\n            _multi_matmul(arrays, order, order[i, j] + 1, j, constant),\n            constant,\n        )\n'"
src/mygrad/linalg/ops.py,14,"b'from collections import Counter\nfrom copy import copy\nfrom functools import reduce\nfrom itertools import chain\n\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\nfrom mygrad._utils import SkipGradient, reduce_broadcast\nfrom mygrad.operation_base import BroadcastableOp\n\n__all__ = [""MatMul"", ""EinSum""]\n\n\nclass MatMul(BroadcastableOp):\n    scalar_only = True\n\n    def __call__(self, a, b):\n        """""" f(a) -> matmul(a, b)\n\n            Parameters\n            ----------\n            a : mygrad.Tensor\n            b : mygrad.Tensor\n\n            Returns\n            -------\n            numpy.ndarray""""""\n        self.variables = (a, b)\n        return np.matmul(a.data, b.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a, b = (i.data for i in self.variables)\n\n        # handle 1D w/ 1D (dot product of vectors)\n        if a.ndim == 1 and b.ndim == 1:\n            if index == 0:\n                return grad * b\n            elif index == 1:\n                return grad * a\n\n        if index == 0:  # compute grad through a\n            if b.ndim > 1:  # ([...], j) w/ ([...], j, k)\n                if a.ndim == 1:\n                    grad = np.expand_dims(grad, -2)\n                dfdx = np.matmul(grad, b.swapaxes(-1, -2))\n            else:  # ([...], i, j) w/ (j,)\n                dfdx = np.expand_dims(grad, -1) * b\n            return dfdx\n\n        if index == 1:  # compute grad through b\n            if a.ndim > 1:  # ([...], i, j) w/ ([...], j, [k])\n                if b.ndim == 1:\n                    grad = np.expand_dims(grad, -1)\n                dfdx = np.matmul(a.swapaxes(-1, -2), grad)\n                if b.ndim == 1:\n                    dfdx = dfdx.squeeze(-1)\n            else:  # (j,) w/ ([...], j, k)\n                dfdx = a[:, np.newaxis] * np.expand_dims(grad, -2)\n            return dfdx\n\n\n# EinSum #\n\n\ndef _unique_from_end(in_str):\n    """""" Return a string with all redundant characters removed,\n        removing left-most redundant entries\n\n        i.e. ""ijikik"" -> ""jik""\n\n        Parameters\n        ----------\n        in_str: str\n\n        Returns\n        -------\n        str\n\n        Examples\n        --------\n        >>> _unique_from_end(""ijikik"")\n        ""jik""\n    """"""\n\n    return reduce(lambda acc, x: acc + x if x not in acc else acc, in_str[::-1], """")[\n        ::-1\n    ]\n\n\ndef _merge_max_mappings(*mappings):\n    """""" Merge dictionaries based on largest values in key->value.\n\n        Parameters\n        ----------\n        *mappings : Dict[Any, Any]\n\n        Returns\n        -------\n        Dict[Any, Any]\n\n        Examples\n        --------\n        >>> _merge_max_mappings({""a"":1, ""b"":4}, {""a"":2})\n        {""a"":2, ""b"":4}\n    """"""\n\n    def _merge_max(d1, d2):\n        d1.update((k, v) for k, v in d2.items() if d1.get(k, 0) < v)\n        return d1\n\n    return reduce(_merge_max, mappings, {})\n\n\ndef _get_indices(item, seq):\n    """""" Return the indices where `item` occurs in `seq`\n\n        Returns\n        -------\n        Generator[int]""""""\n    return (n for n, x in enumerate(seq) if x == item)\n\n\nclass EinSum(BroadcastableOp):\n    scalar_only = True\n\n    def __call__(self, *variables, in_lbls, out_lbls, optimize=False):\n        """"""\n        einsum(\'{in_lbls}->{out_lbls}\', *variables, optimize=optimize)\n\n        Parameters\n        ----------\n        variables : mygrad.Tensor\n        in_lbls : str\n        out_lbls : str\n        optimize : bool\n\n        Returns\n        -------\n        numpy.ndarray\n        """"""\n        self.in_lbls = in_lbls.split("","")\n        self.out_lbls = out_lbls\n        self.variables = variables\n        self.optimize = optimize\n\n        # cache counts the number of redundant tensor-label pairs\n        # fed to einsum. Only one gradient will be computed for a\n        # unique tensor-label pair\n        self.cache = Counter(zip(variables, self.in_lbls))\n        return np.einsum(\n            ""->"".join((in_lbls, out_lbls)),\n            *(var.data for var in self.variables),\n            optimize=optimize\n        )\n\n    def backward_var(self, grad, index, **kwargs):\n        """"""\n        example\n        -------\n        fwd:          ""ijk, k -> ji"", x, y\n        bkwd (var: 0): ""ji, k -> ijk"", grad, y\n        bkwd (var: 1): ""ji, ijk -> k"", grad, x\n        """"""\n\n        # ijk, k\n        in_lbls = copy(self.in_lbls)\n        original_var_lbl = in_lbls.pop(index)\n        var = self.variables[index]\n\n        factor = self.cache[(var, original_var_lbl)]\n        if factor == 0:\n            # the gradient for the current tensor-label pair\n            # has already been computed, scaled, and back-propped,\n            # skip gradient calculation.\n            raise SkipGradient()\n\n        numpy_arrays = tuple(i.data for i in self.variables)\n        self.cache[(var, original_var_lbl)] = 0\n\n        var_lbl = _unique_from_end(original_var_lbl)\n        repeat_lbls = len(var_lbl) != len(original_var_lbl)\n\n        if repeat_lbls:\n            # example fwd-prop: einsum(""iji -> ij"", x)\n            # ""iji"" becomes ""ji"", later we will write along\n            # the diagonal of an array to reinstate this axis that\n            # we just removed\n            mapping_gen = (\n                {k: v for k, v in zip(lbl, arr.shape)}\n                for lbl, arr in zip(self.in_lbls, numpy_arrays)\n            )\n            lbl_to_size = _merge_max_mappings(*mapping_gen)\n            var_shape = tuple(lbl_to_size[lbl] for lbl in var_lbl)\n        else:\n            var_shape = self.variables[index].shape\n\n        # ji\n        grad_lbl = self.out_lbls\n\n        # Catch indices over which un-contracted sum was performed\n        # for the given variable: e.g for var-0 in ""ijk, jk -> k""\n        # i is summed over without contraction with another tensor\n        #\n        # Backpropping through this is illegal, as it requires the creation\n        # of an axis; e.g. k, jk -> ijk\n        # Broadcast the gradient along all such dimensions; e.g. k -> ik\n        # then proceed as usual; e.g. ik, jk -> ijk\n        unique_in_lbls = set(chain.from_iterable(in_lbls)) | set(grad_lbl)\n        if len(set(var_lbl) - unique_in_lbls) > 0:\n            exp_dims = [slice(None) for i in range(grad.ndim)]\n            grad_shape = list(grad.shape)\n            for n, lbl in enumerate(var_lbl):\n                if lbl not in unique_in_lbls:\n                    grad_lbl = grad_lbl[:n] + lbl + grad_lbl[n:]\n                    exp_dims.insert(n, np.newaxis)\n                    grad_shape.insert(n, var_shape[n])\n\n            grad = np.broadcast_to(\n                grad if not grad.ndim else grad[tuple(exp_dims)], grad_shape\n            )\n\n        # ""ji, k -> ijk""\n        back_prop_lbls = "","".join([grad_lbl] + in_lbls) + ""->"" + var_lbl\n\n        # (grad, y)\n        operands = (grad,) + numpy_arrays[:index] + numpy_arrays[index + 1 :]\n\n        if not repeat_lbls:\n            # dfdx: einsum(""ji, k -> ijk"", grad, y)\n            outshape = self.variables[index].shape\n            dfdx = reduce_broadcast(\n                np.einsum(back_prop_lbls, *operands, optimize=self.optimize), outshape\n            )\n            if var_shape != dfdx.shape:\n                # if y was broadcast over x, the gradient needs to\n                # be broadcast to x\'s shape: dfdx-shape (i,j,1) -> (i,j,k)\n                dfdx = np.broadcast_to(dfdx, var_shape)\n            if factor > 1:\n                # This tensor-label pair appears several times as\n                # input to einsum. Scale the gradient accordingly\n                # such that the full contribution of the tensor-label\n                # pair is accounted for.\n                dfdx *= factor\n            return dfdx\n\n        # Accommodate trace by writing to strided view on array of zeros\n        # For example:\n        #\n        # fwd:  einsum(\'ijkji, k -> jk\', x, y)\n        # dfdx: einsum(\'jk, k -> kji\', grad, y, out=view_of_x)\n        #\n        # writing to `view_of_x`, which is a view along the appropriate\n        # diagonals of x, is equivalent to:\n        #\n        # dfdx: einsum(\'jk, k -> ijkji\', grad, y)\n        #\n        # which is formally correct but not supported by einsum.\n        dfdx = np.zeros(tuple(lbl_to_size[i] for i in original_var_lbl))\n        out_view_shape = tuple(lbl_to_size[i] for i in var_lbl)\n\n        # compute strides required to traverse the appropriate diagonals of\n        # the output tensor.\n        strides = tuple(\n            sum(dfdx.strides[ind] for ind in _get_indices(lbl, original_var_lbl))\n            for lbl in var_lbl\n        )\n        out_view = as_strided(dfdx, shape=out_view_shape, strides=strides)\n        np.einsum(back_prop_lbls, *operands, out=out_view, optimize=self.optimize)\n        if factor > 1:\n            # This tensor-label pair appears several times as\n            # input to einsum. Scale the gradient accordingly\n            # such that the full contribution of the tensor-label\n            # pair is accounted for.\n            dfdx *= factor\n        return dfdx\n'"
src/mygrad/math/__init__.py,0,b''
src/mygrad/math/_special.py,13,"b'import numpy as np\n\n\ndef logsumexp(a, axis=None, keepdims=False):\n    """"""Compute the log of the sum of exponentials of input elements.\n\n    Note that this was copied from scipy.special so as to remove\n    mygrad\'s dependency from scipy. This is *not* a tensor-operation\n    that supports backpropagation. It is meant strictly as an\n    internal mathematical utility for mygrad.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    axis : None or int or tuple of ints, optional\n        Axis or axes over which the sum is taken. By default `axis` is None,\n        and all elements are summed.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left in the\n        result as dimensions with size one. With this option, the result\n        will broadcast correctly against the original array.\n\n    Returns\n    -------\n    res : ndarray\n        The result, ``np.log(np.sum(np.exp(a)))`` calculated in a numerically\n        more stable way. If `b` is given then ``np.log(np.sum(b*np.exp(a)))``\n        is returned.\n\n\n    Notes\n    -----\n    Numpy has a logaddexp function which is very similar to `logsumexp`, but\n    only handles two arguments. `logaddexp.reduce` is similar to this\n    function, but may be less stable.\n\n    Examples\n    --------\n    >>> from scipy.special import logsumexp\n    >>> a = np.arange(10)\n    >>> np.log(np.sum(np.exp(a)))\n    9.4586297444267107\n    >>> logsumexp(a)\n    9.4586297444267107\n    """"""\n    a = np.asarray(a)\n    a_max = np.amax(a, axis=axis, keepdims=True)\n\n    if a_max.ndim > 0:\n        a_max[~np.isfinite(a_max)] = 0\n    elif not np.isfinite(a_max):\n        a_max = 0\n\n    tmp = np.exp(a - a_max)\n\n    # suppress warnings about log of zero\n    with np.errstate(divide=""ignore""):\n        s = np.sum(tmp, axis=axis, keepdims=keepdims)\n        out = np.log(s)\n\n    if not keepdims:\n        a_max = np.squeeze(a_max, axis=axis)\n    out += a_max\n\n    return out\n'"
src/mygrad/math/consts.py,15,"b'import numpy as np\n\n__all__ = [\n    ""Inf"",\n    ""Infinity"",\n    ""NAN"",\n    ""NINF"",\n    ""NZERO"",\n    ""NaN"",\n    ""PINF"",\n    ""PZERO"",\n    ""e"",\n    ""euler_gamma"",\n    ""inf"",\n    ""infty"",\n    ""nan"",\n    ""newaxis"",\n    ""pi"",\n]\n\nInf = np.Inf\nInfinity = np.Infinity\nNAN = np.NAN\nNINF = np.NINF\nNZERO = np.NZERO\nNaN = np.NaN\nPINF = np.PINF\nPZERO = np.PZERO\ne = np.e\neuler_gamma = np.euler_gamma\ninf = np.inf\ninfty = np.infty\nnan = np.nan\nnewaxis = np.newaxis\npi = np.pi\n'"
src/mygrad/math/nondifferentiable.py,2,"b'import numpy as np\n\nfrom mygrad.tensor_base import Tensor\n\n__all__ = [""argmin"", ""argmax""]\n\n\ndef argmax(a, axis=None, out=None):\n    """""" Returns the indices of the maximum values along an axis.\n\n        Parameters\n        ----------\n        a: array_like\n        \n        axis: int, optional\n            By default, the index is into the flattened array, otherwise along the specified axis.\n        \n        out: numpy.array, optional\n            If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.\n        \n        Returns\n        -------\n        numpy.ndarray[int]""""""\n\n    a = a.data if isinstance(a, Tensor) else a\n    return np.argmax(a, axis, out)\n\n\ndef argmin(a, axis=None, out=None):\n    """""" Returns the indices of the minimum values along an axis.\n\n        Parameters\n        ----------\n        a: array_like\n        \n        axis: int, optional\n            By default, the index is into the flattened array, otherwise along the specified axis.\n        \n        out: numpy.array, optional\n            If provided, the result will be inserted into this array. It should be of the appropriate shape and dtype.\n        \n        Returns\n        -------\n        numpy.ndarray[int]""""""\n\n    a = a.data if isinstance(a, Tensor) else a\n    return np.argmin(a, axis, out)\n'"
src/mygrad/nnet/__init__.py,0,b'from .activations import *\nfrom .layers import *\nfrom .losses import *\n'
src/mygrad/tensor_core_ops/__init__.py,0,b''
src/mygrad/tensor_core_ops/indexing.py,14,"b'import numpy as np\n\nfrom mygrad.operation_base import BroadcastableOp, Operation\n\n__all__ = [""GetItem"", ""SetItem""]\n\n\nclass GetItem(Operation):\n    """""" Defines the __getitem__ interface for a Tensor, supporting back-propagation\n\n        Supports back-propagation through all valid numpy-indexing (basic, advanced, mixed, etc.)""""""\n\n    def __call__(self, a, index):\n        """""" ``a[index]``\n\n        Parameters\n        ----------\n        a : mygrad.Tensor\n            The tensor whose entries are being accessed.\n\n        index : valid-array-index\n            An n-dimensional index for specifying entries or subregions of `a`.\n            All means of numpy-array indexing (basic, advanced, mixed, etc) are\n            supported.\n\n        Returns\n        -------\n        numpy.ndarray\n            The array returned by the get-item operation""""""\n        self.variables = (a,)\n        self.index = index\n        return a.data[index]\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        out = np.zeros_like(a.data)\n        np.add.at(out, self.index, grad)\n        return out\n\n\ndef _arr(*shape):\n    """""" Construct an array of a specified consisting of values [0, _arr.size)\n        filled in row-major order.\n\n        Parameters\n        ----------\n        *shape : int\n\n        Returns\n        -------\n        numpy.ndarray""""""\n    return np.arange(np.prod(shape)).reshape(shape)\n\n\ndef _is_int_array_index(index):\n    """""" Returns True if `index` contains any array-like integer-valued sequences\n\n        Parameters\n        ----------\n        index : Tuple[Any]\n\n        Returns\n        -------\n        bool """"""\n    return any(\n        np.issubdtype(np.asarray(ind).dtype, np.int_) and np.asarray(ind).ndim\n        for ind in index\n    )\n\n\ndef _is_bool_array_index(index):\n    """""" Returns True if `index` solely contains a boolean-valued array\n\n        Parameters\n        ----------\n        index : Tuple[Any]\n\n        Returns\n        -------\n        bool """"""\n    return len(index) == 1 and np.issubdtype(np.asarray(index[0]).dtype, np.bool_)\n\n\nclass SetItem(BroadcastableOp):\n    """""" Defines the __setitem__ interface for a Tensor, supporting back-propagation through\n        both the tensor being set and the tensor whose .\n\n        Supports back-propagation through all valid numpy-indexing (basic, advanced, mixed, etc.),\n        as well as """"""\n\n    def __call__(self, a, b, index):\n        """""" a[index] = b\n\n            Parameters\n            ----------\n            a : mygrad.Tensor\n                The tensor whose entries are being set. A copy of the underlying\n                data is made if `a` is a non-constant tensor.\n\n            b : mygrad.Tensor\n                `b` must be broadcast-compatible with `a[index]`\n\n            index : valid-array-index\n                An n-dimensional index for specifying entries or subregions of `a`.\n                All means of numpy-array indexing (basic, advanced, mixed, etc) are\n                supported.\n\n            Notes\n            -----\n            Additional computational overhead is required for back-propagation when\n            `index` contains any integer-valued arrays, to accommodate for the scenario\n            in which a single element is set multiple times.""""""\n\n        out = np.copy(a.data) if not a.constant else a.data\n        self.variables = (a, b)\n        self.index = index if isinstance(index, tuple) else (index,)\n        out[index] = b.data\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        a, b = self.variables\n        if index == 0:\n            grad = np.copy(grad)\n            grad[self.index] = 0\n            return grad\n        elif index == 1:\n            grad_sel = np.asarray(grad[self.index])\n\n            # Basic indexing and indexing with a single boolean-array is trivial. The\n            # gradient into b can just be accessed by indexing into `grad`.\n            # Indexing with integer-valued arrays can be problematic, as the same\n            # item can be specified multiple for ""setting""; here only the last set-item\n            # for that element has an effect. For example:\n            #     x[np.array([0, 0])] = np.array([2, 3])  # `3` gets set to x[0]; 2 has no effect\n            # Thus only that corresponding element in `grad` (that corresponding to `3`)\n            # should be propagated back into b. Thus we must check to see if any items are\n            # being set redundantly, and mask out any elements in `grad` corresponding to\n            # the elements in `b` that weren\'t actually set.\n            if (\n                not np.shares_memory(grad_sel, grad)\n                and grad_sel.size > 0\n                and grad_sel.ndim > 0\n                and not _is_bool_array_index(self.index)\n                and _is_int_array_index(self.index)\n            ):\n                # create an array of unique elements, and see if indexing into it produces\n                # any redundant elements\n                unique = _arr(*grad.shape)\n                sub_sel = unique[self.index].flat\n                elements, first_inds, = np.unique(\n                    np.flip(sub_sel, axis=0), return_index=True\n                )\n                if len(first_inds) < len(sub_sel):\n                    # one or more elements were set redundantly, identify the entries in `b`\n                    # that actually were set to those elements (the last-most set-item calls\n                    # for those elements) and propagate only the corresponding elements from grad\n\n                    first_inds = (len(sub_sel) - 1) - first_inds\n                    mask = np.zeros_like(sub_sel)\n                    mask[first_inds] = 1\n                    mask = mask.reshape(grad_sel.shape)\n                    grad_sel *= mask\n\n            # handle the edge case of ""projecting down"" on setitem. E.g:\n            # x = Tensor([0, 1, 2])\n            # y = Tensor([3])\n            # x[0] = y  # this is legal since x[0] and y have the same size\n            if grad_sel.ndim < b.ndim:\n                if grad_sel.size == b.size:\n                    grad_sel = grad_sel.reshape(b.shape)\n                else:\n                    # Broadcasting occurred during set-item and `b` contains\n                    # excess leading singleton dimensions. Make `grad_sel`\n                    # commensurate with `b` for subsequent `reduce_broadcast`\n                    # to work\n                    grad_sel = grad_sel[(np.newaxis,) * (b.ndim - grad_sel.ndim)]\n\n            return grad_sel\n        else:\n            raise IndexError()  # pragma: no cover\n'"
src/mygrad/tensor_creation/__init__.py,0,b''
src/mygrad/tensor_creation/funcs.py,23,"b'import numpy as np\n\nfrom mygrad.tensor_base import Tensor\n\n__all__ = [\n    ""arange"",\n    ""empty"",\n    ""empty_like"",\n    ""eye"",\n    ""geomspace"",\n    ""identity"",\n    ""linspace"",\n    ""logspace"",\n    ""ones"",\n    ""ones_like"",\n    ""full"",\n    ""full_like"",\n    ""zeros"",\n    ""zeros_like"",\n]\n\n\ndef empty(shape, dtype=np.float32, constant=False):\n    """""" Return a new Tensor of the given shape and type, without initializing entries.\n\n        Parameters\n        ----------\n        shape : Union[int, Tuple[int]]\n            The shape of the empty array.\n\n        dtype : data-type, optional (default=numpy.float32)\n            The data type of the output Tensor.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        Tensor\n            A tensor of uninitialized data of the given shape and dtype.\n\n        See Also\n        --------\n        empty_like : Return an empty tensor with shape and type of input.\n        ones : Return a new tensor setting values to one.\n        zeros : Return a new tensor setting values to zero.\n        full : Return a new tensor of given shape filled with value.\n\n\n        Notes\n        -----\n        `empty`, unlike `zeros`, does not set the array values to zero,\n        and may therefore be marginally faster.  On the other hand, it requires\n        the user to manually set all the values in the array, and should be\n        used with caution.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> mg.empty([2, 2], constant=True)\n        Tensor([[ -9.74499359e+001,   6.69583040e-309],\n                [  2.13182611e-314,   3.06959433e-309]])         #random\n\n        >>> mg.empty([2, 2], dtype=int)\n        Tensor([[-1073741821, -1067949133],\n                [  496041986,    19249760]])                     #random\n    """"""\n    return Tensor(np.empty(shape, dtype), constant=constant)\n\n\ndef empty_like(other, dtype=None, constant=False):\n    """""" Return a new Tensor of the same shape and type as the given array.\n\n        Parameters\n        ----------\n        other : Union[Tensor, ArrayLike]\n            The Tensor or array whose shape and datatype should be mirrored.\n\n        dtype : data-type, optional (default=None)\n            Override the data type of the returned Tensor with this value, or None to not override.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        Tensor\n            A tensor of uninitialized data whose shape and type match `other`.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.arange(4).reshape\n        >>> mg.empty(x, constant=True)\n        Tensor([[ -9.74499359e+001,   6.69583040e-309],\n                [  2.13182611e-314,   3.06959433e-309]])         #random\n\n        >>> mg.empty(x, dtype=int)\n        Tensor([[-1073741821, -1067949133],\n                [  496041986,    19249760]])                     #random\n    """"""\n    if isinstance(other, Tensor):\n        other = other.data\n\n    return Tensor(np.empty_like(other, dtype), constant=constant)\n\n\ndef eye(rows, cols=None, diag_idx=0, dtype=np.float32, constant=False):\n    """""" Return a 2D Tensor with ones on the diagonal and zeros elsewhere.\n\n        Parameters\n        ----------\n        rows : int\n            The number of rows in the output Tensor.\n\n        cols : int, optional (default=None)\n            The number of columns in the output, or None to match `rows`.\n\n        diag_idx : int, optional (default=0)\n            The index of the diagonal. 0 is the main diagonal; a positive value is the upper\n            diagonal, while a negative value refers to the lower diagonal.\n\n        dtype : data-type, optional (default=numpy.float32)\n            The data type of the output Tensor.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        Tensor\n            A tensor whose elements are 0, except for the :math:`k`-th diagonal, whose values are 1.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> mg.eye(2, dtype=int)\n        Tensor([[1, 0],\n                [0, 1]])\n        >>> mg.eye(3, k=1)\n        Tensor([[ 0.,  1.,  0.],\n                [ 0.,  0.,  1.],\n                [ 0.,  0.,  0.]])\n    """"""\n    return Tensor(np.eye(rows, cols, diag_idx, dtype), constant=constant)\n\n\ndef identity(n, dtype=np.float32, constant=False):\n    """""" Return the identity Tensor; a square Tensor with 1s on the main diagonal and 0s elsewhere.\n\n        Parameters\n        ----------\n        n : int\n            The number of rows and columns in the output Tensor.\n\n        dtype : data-type, optional (default=numpy.float32)\n            The data type of the output Tensor.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        Tensor\n            A square Tensor whose main diagonal is 1 and all other elements are 0.\n\n        Examples\n        --------\n        >>> importy mygrad as mg\n        >>> mg.identity(3)\n        Tensor([[ 1.,  0.,  0.],\n                [ 0.,  1.,  0.],\n                [ 0.,  0.,  1.]])\n    """"""\n    return Tensor(np.identity(n, dtype), constant=constant)\n\n\ndef ones(shape, dtype=np.float32, constant=False):\n    """"""\n    Return a Tensor of the given shape and type, filled with ones.\n\n    Parameters\n    ----------\n    shape : Union[int, Tuple[int]]\n        The shape of the output Tensor.\n\n    dtype : data-type, optional (default=numpy.float32)\n        The data type of the output Tensor.\n\n    constant : bool, optional (default=False)\n        If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n    Returns\n    -------\n    Tensor\n        A Tensor of ones with the given shape and data type.\n\n\n    See Also\n    --------\n    ones_like : Return an tensor of ones with shape and type of input.\n    empty : Return a new uninitialized tensor.\n    zeros : Return a new tensor setting values to zero.\n    full : Return a new tensor of given shape filled with value.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> mg.ones(5)\n    Tensor([ 1.,  1.,  1.,  1.,  1.])\n\n    >>> mg.ones((5,), dtype=int)\n    Tensor([1, 1, 1, 1, 1])\n\n    >>> mg.ones((2, 1))\n    Tensor([[ 1.],\n           [ 1.]])\n\n    >>> mg.ones((2, 2))\n    Tensor([[ 1.,  1.],\n            [ 1.,  1.]])\n    """"""\n    return Tensor(np.ones(shape, dtype), constant=constant)\n\n\ndef ones_like(other, dtype=None, constant=False):\n    """"""\n    Return a Tensor of the same shape and type as the given, filled with ones.\n\n    Parameters\n    ----------\n    other : array_like\n        The Tensor or array whose shape and datatype should be mirrored.\n\n    dtype : data-type, optional (default=None)\n        Override the data type of the returned Tensor with this value, or None to not override.\n\n    constant : bool, optional (default=False)\n        If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n    Returns\n    -------\n    Tensor\n        A Tensor of ones whose shape and data type match `other`.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> x = mg.arange(6).reshape((2, 3))\n    >>> x\n    Tensor([[0, 1, 2],\n            [3, 4, 5]])\n\n    >>> mg.ones_like(x)\n    Tensor([[1, 1, 1],\n            [1, 1, 1]])\n\n    >>> y = mg.arange(3, dtype=float)\n    >>> y\n    Tensor([ 0.,  1.,  2.])\n\n    >>> mg.ones_like(y)\n    Tensor([ 1.,  1.,  1.])\n    """"""\n    if isinstance(other, Tensor):\n        other = other.data\n\n    return Tensor(np.ones_like(other, dtype), constant=constant)\n\n\ndef zeros(shape, dtype=np.float32, constant=False):\n    """"""\n    Return a Tensor of the given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    shape : Union[int, Tuple[int]]\n        The shape of the output Tensor.\n\n    dtype : data-type, optional (default=numpy.float32)\n        The data type of the output Tensor.\n\n    constant : bool, optional (default=False)\n        If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n    Returns\n    -------\n    Tensor\n        A Tensor of zeros with the given shape and data type.\n\n    See Also\n    --------\n    ones_like : Return an tensor of ones with shape and type of input.\n    empty : Return a new uninitialized tensor.\n    ones : Return a new tensor setting values to one.\n    full : Return a new tensor of given shape filled with value.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> mg.zeros(5)\n    Tensor([ 0.,  0.,  0.,  0.,  0.])\n\n    >>> mg.zeros((5,), dtype=int, constant=True) # tensor will not back-propagate a gradient\n    Tensor([0, 0, 0, 0, 0])\n\n    >>> mg.zeros((2, 1))\n    Tensor([[ 0.],\n            [ 0.]])\n\n    >>> mg.zeros((2, 2))\n    Tensor([[ 0.,  0.],\n            [ 0.,  0.]])\n    """"""\n    return Tensor(np.zeros(shape, dtype), constant=constant)\n\n\ndef zeros_like(other, dtype=None, constant=False):\n    """"""\n    Return a Tensor of the same shape and type as the given, filled with zeros.\n\n    Parameters\n    ----------\n    other : Union[Tensor, ArrayLike]\n        The Tensor or array whose shape and datatype should be mirrored.\n\n    dtype : data-type, optional (default=None)\n        Override the data type of the returned Tensor with this value, or None to not override.\n\n    constant : bool, optional (default=False)\n        If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n    Returns\n    -------\n    Tensor\n        A Tensor of zeros whose shape and data type match `other`.\n\n    See Also\n    --------\n    empty_like : Return an empty tensor with shape and type of input.\n    ones_like : Return an tensor of ones with shape and type of input.\n    full_like : Return a new tensor with shape of input filled with value.\n    zeros : Return a new tensor setting values to zero.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> x = mg.arange(6).reshape((2, 3))\n    >>> x\n    Tensor([[0, 1, 2],\n            [3, 4, 5]])\n\n    >>> mg.zeros_like(x, constant=True)  # tensor will not back-propagate a gradient\n    Tensor([[0, 0, 0],\n            [0, 0, 0]])\n\n    >>> y = mg.arange(3, dtype=float)\n    >>> y\n    Tensor([ 0.,  1.,  2.])\n\n    >>> mg.zeros_like(y)\n    Tensor([ 0.,  0.,  0.])\n    """"""\n    if isinstance(other, Tensor):\n        other = other.data\n\n    return Tensor(np.zeros_like(other, dtype), constant=constant)\n\n\ndef full(shape, fill_value, dtype=None, constant=False):\n    """"""\n    Return a Tensor of the given shape and type, filled with `fill_value`.\n\n    Parameters\n    ----------\n    shape : Union[int, Tuple[int]]\n        The shape of the output Tensor.\n\n    fill_value : Real\n        The value with which to fill the output Tensor.\n\n    dtype : data-type, optional (default=None)\n        The data type of the output Tensor, or None to match `fill_value`..\n\n    constant : bool, optional (default=False)\n        If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n    Returns\n    -------\n    Tensor\n        A Tensor of `fill_value` with the given shape and dtype.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> mg.full((2, 2), 33)\n    Tensor([[ 33,  33],\n            [ 33,  33]])\n\n    >>> mg.full((2, 2), 10)\n    Tensor([[10, 10],\n            [10, 10]])\n    """"""\n    return Tensor(np.full(shape, fill_value, dtype), constant=constant)\n\n\ndef full_like(other, fill_value, dtype=None, constant=False):\n    """""" Return a Tensor of the same shape and type as the given, filled with `fill_value`.\n\n        Parameters\n        ----------\n        other : Union[Tensor, ArrayLike]\n            The Tensor or array whose shape and datatype should be mirrored.\n\n        dtype : data-type, optional (default=None)\n            Override the data type of the returned Tensor with this value, or None to not override.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        Tensor\n            A Tensor of `fill_value` whose shape and data type match `other`.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> x = mg.arange(6, dtype=int)\n        >>> mg.full_like(x, 1)\n        Tensor([1, 1, 1, 1, 1, 1])\n        >>> mg.full_like(x, 0.1)\n        Tensor([0, 0, 0, 0, 0, 0])\n        >>> mg.full_like(x, 0.1, dtype=np.double)\n        Tensor([ 0.1,  0.1,  0.1,  0.1,  0.1,  0.1])\n        >>> mg.full_like(x, np.nan, dtype=np.double)\n        Tensor([ nan,  nan,  nan,  nan,  nan,  nan])\n\n        >>> y = mg.arange(6, dtype=np.double)\n        >>> mg.full_like(y, 0.1)\n        Tensor([ 0.1,  0.1,  0.1,  0.1,  0.1,  0.1])\n    """"""\n    if isinstance(other, Tensor):\n        other = other.data\n\n    return Tensor(np.full_like(other, fill_value, dtype), constant=constant)\n\n\ndef arange(stop, start=0, step=1, dtype=None, constant=False):\n    """""" Return a Tensor with evenly-spaced values within a given interval.\n\n        Values are generated within [start, stop). Note that for non-integer steps, results may be\n        inconsistent; you are better off using `linspace` instead.\n\n        Parameters\n        ----------\n        start : Real, optional, default=0\n            The start of the interval, inclusive.\n\n        stop : Real\n            The end of the interval, exclusive.\n\n        step : Real, optional (default=1)\n            The spacing between successive values.\n\n        dtype : data-type, optional (default=None)\n            The data type of the output Tensor, or None to infer from the inputs.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        Tensor\n            A Tensor of evenly-spaced values in [start, end).\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> mg.arange(3)\n        Tensor([0, 1, 2])\n        >>> mg.arange(3.0, constant=True)\n        Tensor([ 0.,  1.,  2.])  # resulting tensor will not back-propagate a gradient\n        >>> mg.arange(3,7)\n        Tensor([3, 4, 5, 6])\n        >>> mg.arange(3,7,2)\n        Tensor([3, 5])\n    """"""\n    if start > stop:\n        tmp = start\n        start = stop\n        stop = tmp\n    return Tensor(np.arange(start, stop, step, dtype), constant=constant)\n\n\ndef linspace(start, stop, num=50, include_endpoint=True, dtype=None, constant=False):\n    """""" Return a Tensor with evenly-spaced numbers over a specified interval.\n\n        Values are generated within [start, stop], with the endpoint optionally excluded.\n\n        Parameters\n        ----------\n        start : Real\n            The starting value of the sequence, inclusive.\n\n        stop : Real\n            The ending value of the sequence, inclusive unless `include_endpoint` is False.\n\n        num : int, optional (default=50)\n            The number of values to generate. Must be non-negative.\n\n        include_endpoint : bool, optional (default=True)\n            Whether to include the endpoint in the Tensor. Note that if False, the step size changes\n            to accommodate the sequence excluding the endpoint.\n\n        dtype : data-type, optional (default=None)\n            The data type of the output Tensor, or None to infer from the inputs.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        Tensor\n            A Tensor of `num` evenly-spaced values in [start, stop] or [start, stop), depending on\n            `include_endpoint`.\n\n        See Also\n        --------\n        arange : Similar to `linspace`, but uses a step size (instead of the\n                 number of samples).\n        logspace : Samples uniformly distributed in log space.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> mg.linspace(2.0, 3.0, num=5)\n        Tensor([ 2.  ,  2.25,  2.5 ,  2.75,  3.  ])\n        >>> mg.linspace(2.0, 3.0, num=5, endpoint=False)\n        Tensor([ 2. ,  2.2,  2.4,  2.6,  2.8])\n    """"""\n    return Tensor(\n        np.linspace(start, stop, num, include_endpoint, dtype=dtype), constant=constant\n    )\n\n\ndef logspace(\n    start, stop, num=50, include_endpoint=True, base=10, dtype=None, constant=False\n):\n    """""" Return a Tensor with evenly-spaced numbers over a specified interval on a log scale.\n\n        In linear space, values are generated within [base**start, base**stop], with the endpoint\n        optionally excluded.\n\n        Parameters\n        ----------\n        start : Real\n            The starting value of the sequence, inclusive; start at `base ** start`.\n\n        stop : Real\n            The ending value of the sequence, inclusive unless `include_endpoint` is False; end at\n            `base ** stop`.\n\n        num : int, optional (default=50)\n            The number of values to generate. Must be non-negative.\n\n        include_endpoint : bool, optional (default=True)\n            Whether to include the endpoint in the Tensor. Note that if False, the step size changes\n            to accommodate the sequence excluding the endpoint.\n\n        base : Real, optional (default=10)\n            The base of the log space.\n\n        dtype : data-type, optional (default=None)\n            The data type of the output Tensor, or None to infer from the inputs.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        See Also\n        --------\n        arange : Similar to linspace, with the step size specified instead of the\n                 number of samples. Note that, when used with a float endpoint, the\n                 endpoint may or may not be included.\n        linspace : Similar to logspace, but with the samples uniformly distributed\n                   in linear space, instead of log space.\n        geomspace : Similar to logspace, but with endpoints specified directly.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> mg.logspace(2.0, 3.0, num=4)\n        Tensor([  100.        ,   215.443469  ,   464.15888336,  1000.        ])\n        >>> mg.logspace(2.0, 3.0, num=4, endpoint=False)\n        Tensor([ 100.        ,  177.827941  ,  316.22776602,  562.34132519])\n        >>> mg.logspace(2.0, 3.0, num=4, base=2.0)\n        Tensor([ 4.        ,  5.0396842 ,  6.34960421,  8.        ])\n\n        Returns\n        -------\n        Tensor\n            A Tensor of `num` evenly-spaced values in the log interval [base**start, base**stop].\n    """"""\n    return Tensor(\n        np.logspace(start, stop, num, include_endpoint, base, dtype), constant=constant\n    )\n\n\ndef geomspace(start, stop, num=50, include_endpoint=True, dtype=None, constant=False):\n    """""" Return a Tensor with evenly-spaced values in a geometric progression.\n\n        Each output sample is a constant multiple of the previous output.\n\n        Parameters\n        ----------\n        start : Real\n            The starting value of the output.\n\n        stop : Real\n            The ending value of the sequence, inclusive unless `include_endpoint` is false.\n\n        num : int, optional (default=50)\n            The number of values to generate. Must be non-negative.\n\n        include_endpoint : bool, optional (default=True)\n            Whether to include the endpoint in the Tensor. Note that if False, the step size changes\n            to accommodate the sequence excluding the endpoint.\n\n        dtype : data-type, optional (default=None)\n            The data type of the output Tensor, or None to infer from the inputs.\n\n        constant : bool, optional (default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        Tensor\n            A Tensor of `num` samples, evenly-spaced in a geometric progression.\n\n        See Also\n        --------\n        logspace : Similar to geomspace, but with endpoints specified using log\n                   and base.\n        linspace : Similar to geomspace, but with arithmetic instead of geometric\n                   progression.\n        arange : Similar to linspace, with the step size specified instead of the\n                 number of samples.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> import numpy as np\n        >>> mg.geomspace(1, 1000, num=4)\n        Tensor([    1.,    10.,   100.,  1000.])\n        >>> mg.geomspace(1, 1000, num=3, endpoint=False)\n        Tensor([   1.,   10.,  100.])\n        >>> mg.geomspace(1, 1000, num=4, endpoint=False)\n        Tensor([   1.        ,    5.62341325,   31.6227766 ,  177.827941  ])\n        >>> mg.geomspace(1, 256, num=9)\n        Tensor([   1.,    2.,    4.,    8.,   16.,   32.,   64.,  128.,  256.])\n\n        Note that the above may not produce exact integers:\n\n        >>> mg.geomspace(1, 256, num=9, dtype=int)\n        Tensor([  1,   2,   4,   7,  16,  32,  63, 127, 256])\n        >>> np.around(mg.geomspace(1, 256, num=9).data).astype(int)\n        array([  1,   2,   4,   8,  16,  32,  64, 128, 256])\n\n        Negative, decreasing, and complex inputs are allowed:\n\n        >>> mg.geomspace(1000, 1, num=4)\n        Tensor([ 1000.,   100.,    10.,     1.])\n        >>> mg.geomspace(-1000, -1, num=4)\n        Tensor([-1000.,  -100.,   -10.,    -1.])\n    """"""\n    return Tensor(\n        np.geomspace(start, stop, num, include_endpoint, dtype), constant=constant\n    )\n'"
src/mygrad/tensor_manip/__init__.py,0,b''
tests/math/binary/__init__.py,0,b''
tests/math/binary/test_binary_funcs.py,16,"b'"""""" Test all binary arithmetic operations, checks for appropriate broadcast behavior""""""\nimport numpy as np\nfrom hypothesis import settings\n\nfrom mygrad import (\n    add,\n    arctan2,\n    divide,\n    logaddexp,\n    logaddexp2,\n    multiply,\n    power,\n    subtract,\n)\n\nfrom ...wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(mygrad_func=add, true_func=np.add, num_arrays=2)\ndef test_add_fwd():\n    pass\n\n\n@backprop_test_factory(mygrad_func=add, true_func=np.add, num_arrays=2)\ndef test_add_bkwd():\n    pass\n\n\n@fwdprop_test_factory(mygrad_func=subtract, true_func=np.subtract, num_arrays=2)\ndef test_subtract_fwd():\n    pass\n\n\n@backprop_test_factory(mygrad_func=subtract, true_func=np.subtract, num_arrays=2)\ndef test_subtract_bkwd():\n    pass\n\n\n@fwdprop_test_factory(mygrad_func=multiply, true_func=np.multiply, num_arrays=2)\ndef test_multiply_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=multiply, true_func=np.multiply, atol=1e-4, rtol=1e-4, num_arrays=2\n)\ndef test_multiply_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=divide, true_func=np.divide, index_to_bnds={1: (1, 10)}, num_arrays=2\n)\ndef test_divide_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=divide, true_func=np.divide, index_to_bnds={1: (1, 10)}, num_arrays=2\n)\ndef test_divide_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=power,\n    true_func=np.power,\n    index_to_bnds={0: (1, 10), 1: (-3, 3)},\n    num_arrays=2,\n)\ndef test_power_fwd():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=power,\n    true_func=np.power,\n    index_to_bnds={0: (1, 10), 1: (-3, 3)},\n    num_arrays=2,\n)\ndef test_power_bkwd():\n    pass\n\n\n@fwdprop_test_factory(mygrad_func=logaddexp, true_func=np.logaddexp, num_arrays=2)\ndef test_logaddexp_fwd():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=logaddexp,\n    true_func=np.logaddexp,\n    num_arrays=2,\n    index_to_bnds=(-10, 10),\n    use_finite_difference=True,\n    h=1e-8,\n    atol=1e-4,\n    rtol=1e-4,\n)\ndef test_logaddexp_bkwd():\n    pass\n\n\n@fwdprop_test_factory(mygrad_func=logaddexp2, true_func=np.logaddexp2, num_arrays=2)\ndef test_logaddexp2_fwd():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=logaddexp2,\n    true_func=np.logaddexp2,\n    num_arrays=2,\n    atol=1e-4,\n    rtol=1e-4,\n    use_finite_difference=True,\n    h=1e-8,\n    index_to_bnds=(-100, 100),\n)\ndef test_logaddexp2_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arctan2, true_func=np.arctan2, num_arrays=2, index_to_bnds={1: (1, 10)}\n)\ndef test_arctan2_fwd():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=arctan2,\n    true_func=np.arctan2,\n    num_arrays=2,\n    atol=1e-4,\n    rtol=1e-4,\n    index_to_bnds={1: (1, 10)},\n    use_finite_difference=True,\n    h=1e-8,\n)\ndef test_arctan2_bkwd():\n    pass\n'"
tests/math/sequence/__init__.py,0,b''
tests/math/sequence/test_sequential_funcs.py,46,"b'from functools import partial\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given, settings\nfrom pytest import raises\n\nimport mygrad\nfrom mygrad import amax, amin, cumprod, cumsum, mean, prod, std, sum, var\n\nfrom ...custom_strategies import valid_axes\nfrom ...wrappers.uber import (\n    backprop_test_factory as backprop_test_factory,\n    fwdprop_test_factory as fwdprop_test_factory,\n)\n\n\ndef axis_arg(*arrs, min_dim=0):\n    """""" Wrapper for passing valid-axis search strategy to test factory""""""\n    if arrs[0].ndim:\n        return valid_axes(arrs[0].ndim, min_dim=min_dim)\n    else:\n        return st.just(tuple())\n\n\ndef single_axis_arg(*arrs):\n    """""" Wrapper for passing valid-axis (single-value only)\n    search strategy to test factory""""""\n    if arrs[0].ndim:\n        return valid_axes(arrs[0].ndim, single_axis_only=True)\n    else:\n        return st.none()\n\n\ndef keepdims_arg(*arrs):\n    """""" Wrapper for passing keep-dims strategy to test factory""""""\n    return st.booleans()\n\n\ndef ddof_arg(*arrs):\n    """""" Wrapper for passing ddof strategy to test factory\n    (argument for var and std)""""""\n    min_side = min(arrs[0].shape) if arrs[0].shape else 0\n    return st.integers(0, min_side - 1) if min_side else st.just(0)\n\n\n@fwdprop_test_factory(\n    mygrad_func=amax,\n    true_func=np.amax,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n)\ndef test_max_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=amax,\n    true_func=np.amax,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n    vary_each_element=True,\n    index_to_unique={0: True},\n    elements_strategy=st.integers,\n)\ndef test_max_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=amin,\n    true_func=np.amin,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n)\ndef test_min_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=amin,\n    true_func=np.amin,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n    vary_each_element=True,\n    index_to_unique={0: True},\n    elements_strategy=st.integers,\n)\ndef test_min_bkwd():\n    pass\n\n\ndef test_min_max_aliases():\n    assert mygrad.max == amax\n    assert mygrad.min == amin\n\n\n@fwdprop_test_factory(\n    mygrad_func=sum,\n    true_func=np.sum,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n)\ndef test_sum_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=sum,\n    true_func=np.sum,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n    vary_each_element=True,\n    atol=1e-5,\n)\ndef test_sum_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=mean,\n    true_func=np.mean,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n)\ndef test_mean_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=mean,\n    true_func=np.mean,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n    index_to_bnds={0: (-10, 10)},\n    vary_each_element=True,\n)\ndef test_mean_bkwd():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=var,\n    true_func=np.var,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg, ddof=ddof_arg),\n)\n@pytest.mark.filterwarnings(""ignore: Degrees of freedom"")\n@pytest.mark.filterwarnings(""ignore: invalid value encountered in true_divide"")\ndef test_var_fwd():\n    pass\n\n\ndef _var(x, keepdims=False, axis=None, ddof=0):\n    """"""Defines variance without using abs. Permits use of\n    complex-step numerical derivative.""""""\n    x = np.asarray(x)\n\n    def mean(y, keepdims=False, axis=None, ddof=0):\n        if isinstance(axis, int):\n            axis = (axis,)\n        N = y.size if axis is None else np.prod([y.shape[i] for i in axis])\n        return y.sum(keepdims=keepdims, axis=axis) / (N - ddof)\n\n    return mean(\n        (x - x.mean(axis=axis, keepdims=True)) ** 2,\n        keepdims=keepdims,\n        axis=axis,\n        ddof=ddof,\n    )\n\n\n@fwdprop_test_factory(\n    mygrad_func=var,\n    true_func=_var,\n    num_arrays=1,\n    kwargs=dict(\n        axis=partial(axis_arg, min_dim=1), keepdims=keepdims_arg, ddof=ddof_arg\n    ),\n)\ndef test_custom_var_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=var,\n    true_func=_var,\n    num_arrays=1,\n    kwargs=dict(\n        axis=partial(axis_arg, min_dim=1), keepdims=keepdims_arg, ddof=ddof_arg\n    ),\n    vary_each_element=True,\n    index_to_bnds={0: (-10, 10)},\n)\ndef test_var_bkwd():\n    pass\n\n\n@given(\n    x=hnp.arrays(\n        dtype=np.float,\n        shape=hnp.array_shapes(),\n        elements=st.floats(allow_infinity=False, allow_nan=False),\n    )\n)\ndef test_var_no_axis_fwd(x):\n    import mygrad as mg\n\n    x = mg.Tensor(x, constant=False)\n    o = mg.var(x, axis=())\n    assert np.all(o.data == np.zeros_like(x.data))\n\n\n@given(\n    x=hnp.arrays(\n        dtype=np.float,\n        shape=hnp.array_shapes(),\n        elements=st.floats(allow_infinity=False, allow_nan=False),\n    )\n)\ndef test_var_no_axis_bkwrd(x):\n    import mygrad as mg\n\n    x = mg.Tensor(x, constant=False)\n    mg.var(x, axis=()).backward()\n    assert np.all(x.grad == np.zeros_like(x.data))\n\n\n@fwdprop_test_factory(\n    mygrad_func=std,\n    true_func=np.std,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg, ddof=ddof_arg),\n)\n@pytest.mark.filterwarnings(""ignore: Degrees of freedom"")\n@pytest.mark.filterwarnings(""ignore: invalid value encountered in true_divide"")\ndef test_std_fwd():\n    pass\n\n\ndef _std(x, keepdims=False, axis=None, ddof=0):\n    """"""Defines standard dev without using abs. Permits use of\n    complex-step numerical derivative.""""""\n    x = np.asarray(x)\n\n    def mean(y, keepdims=False, axis=None, ddof=0):\n        if isinstance(axis, int):\n            axis = (axis,)\n        N = y.size if axis is None else np.prod([y.shape[i] for i in axis])\n        return y.sum(keepdims=keepdims, axis=axis) / (N - ddof)\n\n    return np.sqrt(\n        mean(\n            (x - x.mean(axis=axis, keepdims=True)) ** 2,\n            keepdims=keepdims,\n            axis=axis,\n            ddof=ddof,\n        )\n    )\n\n\n@fwdprop_test_factory(\n    mygrad_func=std,\n    true_func=_std,\n    num_arrays=1,\n    kwargs=dict(\n        axis=partial(axis_arg, min_dim=1), keepdims=keepdims_arg, ddof=ddof_arg\n    ),\n)\ndef test_custom_std_fwd():\n    pass\n\n\ndef _assume(*arrs, **kwargs):\n    return all(i > 1 for i in arrs[0].shape)\n\n\n@backprop_test_factory(\n    mygrad_func=std,\n    true_func=_std,\n    num_arrays=1,\n    kwargs=dict(\n        axis=partial(axis_arg, min_dim=1), keepdims=keepdims_arg, ddof=ddof_arg\n    ),\n    vary_each_element=True,\n    index_to_bnds={0: (-10, 10)},\n    elements_strategy=st.integers,\n    index_to_unique={0: True},\n    assumptions=_assume,\n)\ndef test_std_bkwd():\n    pass\n\n\n@given(\n    x=hnp.arrays(\n        dtype=np.float,\n        shape=hnp.array_shapes(),\n        elements=st.floats(allow_infinity=False, allow_nan=False),\n    )\n)\ndef test_std_no_axis_fwd(x):\n    import mygrad as mg\n\n    x = mg.Tensor(x, constant=False)\n    o = mg.std(x, axis=())\n    assert np.all(o.data == np.zeros_like(x.data))\n\n\n@given(\n    x=hnp.arrays(\n        dtype=np.float,\n        shape=hnp.array_shapes(),\n        elements=st.floats(allow_infinity=False, allow_nan=False),\n    )\n)\ndef test_std_no_axis_bkwrd(x):\n    import mygrad as mg\n\n    x = mg.Tensor(x, constant=False)\n    mg.std(x, axis=()).backward()\n    assert np.all(x.grad == np.zeros_like(x.data))\n\n\n@fwdprop_test_factory(\n    mygrad_func=prod,\n    true_func=np.prod,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n)\ndef test_prod_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=prod,\n    true_func=np.prod,\n    num_arrays=1,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n    vary_each_element=True,\n    index_to_bnds={0: (-2, 2)},\n)\ndef test_prod_bkwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=prod,\n    true_func=np.prod,\n    num_arrays=1,\n    elements_strategy=st.integers,\n    kwargs=dict(axis=axis_arg, keepdims=keepdims_arg),\n    vary_each_element=True,\n    index_to_bnds={0: (-2, 2)},\n)\ndef test_multi_zero_prod_bkwd():\n    """"""Drives tests cases with various configurations of zeros""""""\n\n\ndef test_int_axis_cumprod():\n    """"""check if numpy cumprod begins to support tuples for the axis argument""""""\n\n    x = np.array([[1, 1, 0], [1, 1, 0], [1, 1, 0]])\n    ""`np.cumprod` is expected to raise a TypeError ""\n    ""when it is provided a tuple of axes.""\n    with raises(TypeError):\n        np.cumprod(x, axis=(0, 1))\n\n    ""`mygrad.cumprod` is expected to raise a TypeError ""\n    ""when it is provided a tuple of axes.""\n    with raises(TypeError):\n        cumprod(x, axis=(0, 1))\n\n\n@fwdprop_test_factory(\n    mygrad_func=cumprod,\n    true_func=np.cumprod,\n    num_arrays=1,\n    kwargs=dict(axis=single_axis_arg),\n)\ndef test_cumprod_fwd():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=cumprod,\n    true_func=np.cumprod,\n    num_arrays=1,\n    kwargs=dict(axis=single_axis_arg),\n    vary_each_element=True,\n    index_to_bnds={0: (-2, 2)},\n)\ndef test_cumprod_bkwd():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=cumprod,\n    true_func=np.cumprod,\n    num_arrays=1,\n    kwargs=dict(axis=single_axis_arg),\n    vary_each_element=True,\n    index_to_bnds={0: (-0.5, 0.5)},\n    index_to_unique={0: True},\n    index_to_arr_shapes={0: hnp.array_shapes(max_side=5, max_dims=4)},\n)\ndef test_cumprod_bkwd2():\n    pass\n\n\ndef test_int_axis_cumsum():\n    """"""check if numpy cumsum begins to support tuples for the axis argument""""""\n\n    x = np.array([[1, 1, 0], [1, 1, 0], [1, 1, 0]])\n    ""`np.cumsum` is expected to raise a TypeError ""\n    ""when it is provided a tuple of axes.""\n    with raises(TypeError):\n        np.cumsum(x, axis=(0, 1))\n\n    ""`mygrad.cumsum` is expected to raise a TypeError ""\n    ""when it is provided a tuple of axes.""\n    with raises(TypeError):\n        cumsum(x, axis=(0, 1))\n\n\n@fwdprop_test_factory(\n    mygrad_func=cumsum,\n    true_func=np.cumsum,\n    num_arrays=1,\n    kwargs=dict(axis=single_axis_arg),\n)\ndef test_cumsum_fwd():\n    pass\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=cumsum,\n    true_func=np.cumsum,\n    num_arrays=1,\n    kwargs=dict(axis=single_axis_arg),\n    vary_each_element=True,\n    index_to_bnds={0: (-2, 2)},\n    atol=1e-5,\n)\ndef test_cumsum_bkwd():\n    pass\n'"
tests/math/unary/__init__.py,0,b''
tests/math/unary/test_abs.py,5,"b'import numpy as np\n\nfrom mygrad import abs, absolute\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef _is_non_zero(x):\n    return np.all(np.abs(x.data) > 1e-8)\n\n\n@fwdprop_test_factory(mygrad_func=abs, true_func=np.abs, num_arrays=1)\ndef test_abs_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=abs,\n    true_func=np.abs,\n    num_arrays=1,\n    index_to_bnds={0: (-10, 10)},\n    assumptions=_is_non_zero,\n    atol=1e-5,\n    use_finite_difference=True,\n    h=1e-8,\n)\ndef test_abs_backward():\n    pass\n\n\n@fwdprop_test_factory(mygrad_func=absolute, true_func=np.absolute, num_arrays=1)\ndef test_absolute_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=absolute,\n    true_func=np.absolute,\n    num_arrays=1,\n    index_to_bnds={0: (-100, 100)},\n    index_to_no_go={0: (0,)},\n    atol=1e-5,\n    assumptions=_is_non_zero,\n    use_finite_difference=True,\n    h=1e-8,\n)\ndef test_absolute_backward():\n    pass\n'"
tests/math/unary/test_arithmetic.py,9,"b'import numpy as np\n\nfrom mygrad import negative, positive, reciprocal, square\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef _is_non_zero(x):\n    return np.all(np.abs(x.data) > 1e-3)\n\n\n@fwdprop_test_factory(mygrad_func=positive, true_func=np.positive, num_arrays=1)\ndef test_positive_fwd():\n    pass\n\n\n@backprop_test_factory(mygrad_func=positive, true_func=np.positive, num_arrays=1)\ndef test_positive_backward():\n    pass\n\n\n@fwdprop_test_factory(mygrad_func=negative, true_func=np.negative, num_arrays=1)\ndef test_negative_fwd():\n    pass\n\n\n@backprop_test_factory(mygrad_func=negative, true_func=np.negative, num_arrays=1)\ndef test_negative_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=reciprocal,\n    true_func=np.reciprocal,\n    num_arrays=1,\n    assumptions=_is_non_zero,\n)\ndef test_reciprocal_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=reciprocal,\n    true_func=np.reciprocal,\n    num_arrays=1,\n    assumptions=_is_non_zero,\n    atol=1e-5,\n    rtol=1e-5,\n)\ndef test_reciprocal_backward():\n    pass\n\n\n@fwdprop_test_factory(mygrad_func=square, true_func=np.square, num_arrays=1)\ndef test_square_fwd():\n    pass\n\n\n@backprop_test_factory(mygrad_func=square, true_func=np.square, num_arrays=1)\ndef test_square_backward():\n    pass\n'"
tests/math/unary/test_cbrt.py,3,"b'import numpy as np\n\nfrom mygrad import cbrt\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef _is_non_zero(x):\n    return np.all(np.abs(x.data) > 1e-5)\n\n\n@fwdprop_test_factory(mygrad_func=cbrt, true_func=np.cbrt, num_arrays=1)\ndef test_cbrt_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=cbrt,\n    true_func=np.cbrt,\n    index_to_bnds={0: (-100, 100)},\n    num_arrays=1,\n    assumptions=_is_non_zero,\n    atol=1e-5,\n    rtol=1e-5,\n    use_finite_difference=True,\n    h=1e-8,\n)\ndef test_cbrt_backward():\n    pass\n'"
tests/math/unary/test_exp.py,12,"b'import sys\n\nimport numpy as np\n\nfrom mygrad import exp, exp2, expm1\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(\n    mygrad_func=exp,\n    true_func=np.exp,\n    num_arrays=1,\n    index_to_bnds={0: (None, np.log(sys.float_info.max))},\n)\ndef test_exp_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=exp,\n    true_func=np.exp,\n    num_arrays=1,\n    index_to_bnds={0: (None, np.log(sys.float_info.max) / 10)},\n)\ndef test_exp_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=expm1,\n    true_func=np.expm1,\n    num_arrays=1,\n    index_to_bnds={0: (None, np.log(sys.float_info.max))},\n)\ndef test_expm1_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=expm1,\n    true_func=lambda x: np.exp(x) - 1,\n    num_arrays=1,\n    index_to_bnds={0: (None, np.log(sys.float_info.max) / 10)},\n)\ndef test_expm1_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=exp2,\n    true_func=np.exp2,\n    num_arrays=1,\n    index_to_bnds={0: (None, np.log2(sys.float_info.max))},\n)\ndef test_exp2_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=exp2,\n    true_func=np.exp2,\n    num_arrays=1,\n    index_to_bnds={0: (None, np.log2(sys.float_info.max) / 10)},\n)\ndef test_exp2_backward():\n    pass\n'"
tests/math/unary/test_hyperbolictrig.py,13,"b'import numpy as np\n\nfrom mygrad import cosh, coth, csch, sech, sinh, tanh\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef _is_nonzero(x):\n    return np.all(np.abs(x.data) > 1e-8)\n\n\n@fwdprop_test_factory(\n    mygrad_func=sinh, true_func=np.sinh, index_to_bnds={0: (-10, 10)}, num_arrays=1\n)\ndef test_sinh_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=sinh, true_func=np.sinh, index_to_bnds={0: (-10, 10)}, num_arrays=1\n)\ndef test_sinh_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=cosh, true_func=np.cosh, index_to_bnds={0: (-10, 10)}, num_arrays=1\n)\ndef test_cosh_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=cosh,\n    true_func=np.cosh,\n    index_to_bnds={0: (-10, 10)},\n    atol=1e-5,\n    num_arrays=1,\n)\ndef test_cosh_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=tanh, true_func=np.tanh, index_to_bnds={0: (-10, 10)}, num_arrays=1\n)\ndef test_tanh_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=tanh,\n    true_func=np.tanh,\n    index_to_bnds={0: (-10, 10)},\n    atol=1e-5,\n    num_arrays=1,\n)\ndef test_tanh_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=csch,\n    true_func=lambda x: 1 / np.sinh(x),\n    index_to_bnds={0: (0.001, 10)},\n    num_arrays=1,\n)\ndef test_csch_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=csch,\n    true_func=lambda x: 1 / np.sinh(x),\n    index_to_bnds={0: (0.001, 10)},\n    num_arrays=1,\n)\ndef test_csch_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=sech,\n    true_func=lambda x: 1 / np.cosh(x),\n    index_to_bnds={0: (-10, 10)},\n    num_arrays=1,\n)\ndef test_sech_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=sech,\n    true_func=lambda x: 1 / np.cosh(x),\n    index_to_bnds={0: (0.001, 10)},\n    atol=1e-5,\n    num_arrays=1,\n)\ndef test_sech_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=coth,\n    true_func=lambda x: 1 / np.tanh(x),\n    index_to_bnds={0: (-10, 10)},\n    assumptions=_is_nonzero,\n    num_arrays=1,\n)\ndef test_coth_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=coth,\n    true_func=lambda x: 1 / np.tanh(x),\n    index_to_bnds={0: (0.001, 10)},\n    atol=1e-5,\n    num_arrays=1,\n)\ndef test_coth_backward():\n    pass\n'"
tests/math/unary/test_inversehyperbolictrig.py,11,"b'import numpy as np\n\nfrom mygrad import arccosh, arccoth, arccsch, arcsinh, arctanh\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\ndef _is_non_zero(x):\n    return np.all(np.abs(x.data) > 1e-8)\n\n\n@fwdprop_test_factory(mygrad_func=arcsinh, num_arrays=1, true_func=np.arcsinh)\ndef test_arcsinh_fwd():\n    pass\n\n\n@backprop_test_factory(mygrad_func=arcsinh, true_func=np.arcsinh, num_arrays=1)\ndef test_arcsinh_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arccosh,\n    num_arrays=1,\n    true_func=np.arccosh,\n    index_to_bnds={0: (1.001, 10)},\n)\ndef test_arccosh_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arccosh,\n    true_func=np.arccosh,\n    num_arrays=1,\n    index_to_bnds={0: (1.001, 10)},\n)\ndef test_arccosh_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arctanh,\n    true_func=np.arctanh,\n    num_arrays=1,\n    index_to_bnds={0: (-0.5, 0.5)},\n)\ndef test_arctanh_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arctanh,\n    true_func=np.arctanh,\n    num_arrays=1,\n    index_to_bnds={0: (-0.5, 0.5)},\n    assumptions=_is_non_zero,\n)\ndef test_arctanh_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arccsch,\n    num_arrays=1,\n    true_func=lambda x: np.arcsinh(1 / x),\n    index_to_bnds={0: (1, 10)},\n)\ndef test_arccsch_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arccsch,\n    true_func=lambda x: np.arcsinh(1 / x),\n    num_arrays=1,\n    index_to_bnds={0: (1, 10)},\n)\ndef test_arccsch_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arccoth,\n    num_arrays=1,\n    true_func=lambda x: np.arctanh(1 / x),\n    index_to_bnds={0: (5, 10)},\n)\ndef test_arccoth_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arccoth,\n    true_func=lambda x: np.arctanh(1 / x),\n    num_arrays=1,\n    index_to_bnds={0: (5, 10)},\n)\ndef test_arccoth_backward():\n    pass\n'"
tests/math/unary/test_inversetrig.py,12,"b'import numpy as np\n\nfrom mygrad import arccos, arccot, arccsc, arcsec, arcsin, arctan\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(\n    mygrad_func=arcsin,\n    true_func=np.arcsin,\n    index_to_bnds={0: (-0.9, 0.9)},\n    num_arrays=1,\n)\ndef test_arcsin_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arcsin,\n    true_func=np.arcsin,\n    index_to_bnds={0: (-0.9, 0.9)},\n    num_arrays=1,\n)\ndef test_arcsin_backward(data):\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arccos,\n    true_func=np.arccos,\n    index_to_bnds={0: (-0.9, 0.9)},\n    num_arrays=1,\n)\ndef test_arccos_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arccos,\n    true_func=np.arccos,\n    index_to_bnds={0: (-0.9, 0.9)},\n    num_arrays=1,\n)\ndef test_arccos_backward(data):\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arctan,\n    true_func=np.arctan,\n    index_to_bnds={0: (0.1, 10.0)},\n    num_arrays=1,\n)\ndef test_arctan_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arctan,\n    true_func=np.arctan,\n    index_to_bnds={0: (0.1, 10.0)},\n    num_arrays=1,\n)\ndef test_arctan_backward(data):\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arccsc,\n    true_func=lambda x: np.arcsin(1 / x),\n    index_to_bnds={0: (-10.1, -1.1)},\n    num_arrays=1,\n)\ndef test_arccsc_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arccsc,\n    true_func=lambda x: np.arcsin(1 / x),\n    index_to_bnds={0: (1.1, 100.0)},\n    num_arrays=1,\n)\ndef test_arccsc_backward(data):\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arcsec,\n    true_func=lambda x: np.arccos(1 / x),\n    index_to_bnds={0: (-10.1, -1.1)},\n    num_arrays=1,\n)\ndef test_arcsec_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arcsec,\n    true_func=lambda x: np.arccos(1 / x),\n    index_to_bnds={0: (1.1, 100.0)},\n    num_arrays=1,\n)\ndef test_arcsec_backward(data):\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=arccot,\n    true_func=lambda x: np.arctan(1 / x),\n    index_to_bnds={0: (0.1, 10)},\n    num_arrays=1,\n)\ndef test_arccot_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=arccot,\n    true_func=lambda x: np.arctan(1 / x),\n    index_to_bnds={0: (0.1, 10)},\n    num_arrays=1,\n)\ndef test_arccot_backward(data):\n    pass\n'"
tests/math/unary/test_log.py,8,"b'import numpy as np\n\nfrom mygrad import log, log1p, log2, log10\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(\n    mygrad_func=log, true_func=np.log, index_to_bnds={0: (1e-5, 100)}, num_arrays=1\n)\ndef test_log_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=log, true_func=np.log, index_to_bnds={0: (1e-5, 100)}, num_arrays=1\n)\ndef test_log_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=log2, true_func=np.log2, index_to_bnds={0: (1e-5, 100)}, num_arrays=1\n)\ndef test_log2_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=log2, true_func=np.log2, index_to_bnds={0: (1e-5, 100)}, num_arrays=1\n)\ndef test_log2_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=log10, true_func=np.log10, index_to_bnds={0: (1e-5, 100)}, num_arrays=1\n)\ndef test_log10_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=log10, true_func=np.log10, index_to_bnds={0: (1e-5, 100)}, num_arrays=1\n)\ndef test_log10_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    mygrad_func=log1p, true_func=np.log1p, index_to_bnds={0: (1e-5, 100)}, num_arrays=1\n)\ndef test_log1p_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=log1p, true_func=np.log1p, index_to_bnds={0: (1e-5, 100)}, num_arrays=1\n)\ndef test_log1p_backward():\n    pass\n'"
tests/math/unary/test_nondifferentiable.py,9,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import given\nfrom numpy.testing import assert_allclose\n\nimport mygrad as mg\nfrom mygrad.tensor_base import Tensor\n\nfrom ...custom_strategies import valid_axes\n\ndtype_strat_numpy = st.sampled_from(\n    (np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64)\n)\n\n\n@given(\n    a=hnp.arrays(\n        shape=hnp.array_shapes(max_side=4, max_dims=5), dtype=dtype_strat_numpy\n    ),\n    data=st.data(),\n)\ndef test_argmin(a, data):\n    axis = data.draw(valid_axes(ndim=a.ndim, single_axis_only=True), label=""axis"")\n    tensor = Tensor(a)\n    # tensor input\n    assert_allclose(mg.argmin(tensor, axis=axis), np.argmin(a, axis=axis))\n\n    # tensor method\n    assert_allclose(tensor.argmin(axis=axis), a.argmin(axis=axis))\n\n    # array input\n    assert_allclose(mg.argmin(a, axis=axis), np.argmin(a, axis=axis))\n\n\n@given(\n    a=hnp.arrays(\n        shape=hnp.array_shapes(max_side=4, max_dims=5), dtype=dtype_strat_numpy\n    ),\n    data=st.data(),\n)\ndef test_argmax(a, data):\n    axis = data.draw(valid_axes(ndim=a.ndim, single_axis_only=True), label=""axis"")\n    tensor = Tensor(a)\n\n    # tensor input\n    assert_allclose(mg.argmax(tensor, axis=axis), np.argmax(a, axis=axis))\n\n    # tensor method\n    assert_allclose(tensor.argmax(axis=axis), a.argmax(axis=axis))\n\n    # array input\n    assert_allclose(mg.argmax(a, axis=axis), np.argmax(a, axis=axis))\n'"
tests/math/unary/test_scipy_mirror.py,3,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given, settings\nfrom numpy.testing import assert_array_equal\nfrom scipy import special\n\nfrom mygrad.math._special import logsumexp\nfrom tests.custom_strategies import valid_axes\n\n\n@settings(deadline=None, max_examples=500)\n@given(\n    data=st.data(),\n    x=hnp.arrays(\n        shape=hnp.array_shapes(min_dims=0), dtype=np.float, elements=st.floats(),\n    ),\n    keepdims=st.booleans(),\n)\n@pytest.mark.filterwarnings(""ignore: overflow"")\n@pytest.mark.filterwarnings(""ignore: invalid"")\ndef test_logsumexp(data: st.SearchStrategy, x: np.ndarray, keepdims: bool):\n    axes = data.draw(valid_axes(ndim=x.ndim), label=""axes"")\n    mygrad_result = logsumexp(x, axis=axes, keepdims=keepdims)\n    scipy_result = special.logsumexp(x, axis=axes, keepdims=keepdims)\n    assert_array_equal(\n        mygrad_result,\n        scipy_result,\n        err_msg=""mygrad\'s implementation of logsumexp does ""\n        ""not match that of scipy\'s"",\n    )\n'"
tests/math/unary/test_sqrt.py,2,"b'import numpy as np\n\nfrom mygrad import sqrt\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(\n    mygrad_func=sqrt, true_func=np.sqrt, num_arrays=1, index_to_bnds={0: (0, 100)}\n)\ndef test_sqrt_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=sqrt,\n    true_func=np.sqrt,\n    num_arrays=1,\n    index_to_bnds={0: (1e-5, 100)},\n    atol=1e-3,\n)\ndef test_sqrt_backward():\n    pass\n'"
tests/math/unary/test_trigonometric.py,22,"b'import numpy as np\n\nfrom mygrad import cos, cot, csc, sec, sin, sinc, tan\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(num_arrays=1, mygrad_func=sin, true_func=np.sin)\ndef test_sin_fwd():\n    pass\n\n\n@backprop_test_factory(num_arrays=1, mygrad_func=sin, true_func=np.sin)\ndef test_sin_backward():\n    pass\n\n\n@fwdprop_test_factory(num_arrays=1, mygrad_func=sinc, true_func=np.sinc)\ndef test_sinc_fwd():\n    pass\n\n\n@backprop_test_factory(num_arrays=1, mygrad_func=sinc, true_func=np.sinc, atol=1e-5)\ndef test_sinc_backward():\n    pass\n\n\n@fwdprop_test_factory(num_arrays=1, mygrad_func=cos, true_func=np.cos)\ndef test_cos_fwd():\n    pass\n\n\n@backprop_test_factory(num_arrays=1, mygrad_func=cos, true_func=np.cos, atol=1e-5)\ndef test_cos_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    num_arrays=1,\n    mygrad_func=tan,\n    true_func=np.tan,\n    index_to_bnds={0: (-np.pi / 2 + 1e-5, np.pi / 2 - 1e-5)},\n)\ndef test_tan_fwd():\n    pass\n\n\n@backprop_test_factory(\n    num_arrays=1,\n    mygrad_func=tan,\n    true_func=np.tan,\n    index_to_bnds={0: (-np.pi / 2 + 1e-5, np.pi / 2 - 1e-5)},\n)\ndef test_tan_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    num_arrays=1,\n    mygrad_func=csc,\n    true_func=lambda x: 1 / np.sin(x),\n    index_to_bnds={0: (0 + 1e-5, np.pi - 1e-5)},\n)\ndef test_csc_fwd():\n    pass\n\n\n@backprop_test_factory(\n    num_arrays=1,\n    mygrad_func=csc,\n    true_func=lambda x: 1 / np.sin(x),\n    index_to_bnds={0: (0 + 1e-5, np.pi - 1e-5)},\n)\ndef test_csc_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    num_arrays=1,\n    mygrad_func=sec,\n    true_func=lambda x: 1 / np.cos(x),\n    index_to_bnds={0: (-np.pi / 2 + 1e-5, np.pi / 2 - 1e-5)},\n)\ndef test_sec_fwd():\n    pass\n\n\n@backprop_test_factory(\n    num_arrays=1,\n    mygrad_func=sec,\n    true_func=lambda x: 1 / np.cos(x),\n    index_to_bnds={0: (-np.pi / 2 + 1e-5, np.pi / 2 - 1e-5)},\n    atol=1e-4,\n)\ndef test_sec_backward():\n    pass\n\n\n@fwdprop_test_factory(\n    num_arrays=1,\n    mygrad_func=cot,\n    true_func=lambda x: 1 / np.tan(x),\n    index_to_bnds={0: (0 + 1e-5, np.pi - 1e-5)},\n)\ndef test_cot_fwd():\n    pass\n\n\n@backprop_test_factory(\n    num_arrays=1,\n    mygrad_func=cot,\n    true_func=lambda x: 1 / np.tan(x),\n    index_to_bnds={0: (0 + 1e-5, np.pi - 1e-5)},\n)\ndef test_cot_backward():\n    pass\n'"
tests/nnet/activations/__init__.py,0,b''
tests/nnet/activations/test_hard_tanh.py,9,"b'import hypothesis.strategies as st\nimport numpy as np\nimport pytest\n\nfrom mygrad.nnet.activations import hard_tanh\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@pytest.mark.parametrize(""lower_bound, upper_bound"", [(None, 1), (1, None)])\ndef test_input_validation(lower_bound, upper_bound):\n    with pytest.raises(TypeError):\n        hard_tanh(2, lower_bound=lower_bound, upper_bound=upper_bound)\n\n\nfinite_floats = st.floats(allow_infinity=False, allow_nan=False)\n\n\n@fwdprop_test_factory(\n    mygrad_func=hard_tanh,\n    true_func=lambda x, lower_bound, upper_bound: np.maximum(\n        np.minimum(x, upper_bound), lower_bound\n    ),\n    num_arrays=1,\n    kwargs={\n        ""lower_bound"": lambda x: finite_floats | finite_floats.map(np.array),\n        ""upper_bound"": lambda x: finite_floats | finite_floats.map(np.array),\n    },\n)\ndef test_hard_tanh_fwd():\n    pass\n\n\ndef assume_data_not_on_bounds(x, lower_bound, upper_bound):\n    return np.all(np.logical_and(x != lower_bound, x != upper_bound))\n\n\n@backprop_test_factory(\n    mygrad_func=hard_tanh,\n    true_func=lambda x, lower_bound, upper_bound: np.maximum(\n        np.minimum(x, upper_bound), lower_bound\n    ),\n    num_arrays=1,\n    kwargs={\n        ""lower_bound"": lambda x: finite_floats | finite_floats.map(np.array),\n        ""upper_bound"": lambda x: finite_floats | finite_floats.map(np.array),\n    },\n    assumptions=assume_data_not_on_bounds,\n)\ndef test_hard_tanh_bkwd():\n    pass\n'"
tests/nnet/activations/test_relu.py,3,"b'import numpy as np\n\nfrom mygrad.nnet.activations import relu\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(\n    mygrad_func=relu, true_func=lambda x: np.maximum(x, 0), num_arrays=1\n)\ndef test_relu_fwd():\n    pass\n\n\ndef _away_from_zero(*arrs, **kwargs):\n    x = arrs[0]\n    return np.all(np.abs(x.data) > 1e-8)\n\n\n@backprop_test_factory(\n    mygrad_func=relu,\n    true_func=lambda x: np.maximum(x, 0),\n    num_arrays=1,\n    assumptions=_away_from_zero,\n)\ndef test_relu_bkwd():\n    pass\n'"
tests/nnet/activations/test_sigmoid.py,4,"b'import sys\n\nimport numpy as np\n\nfrom mygrad.nnet.activations import sigmoid\nfrom tests.wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@fwdprop_test_factory(\n    mygrad_func=sigmoid,\n    true_func=lambda x: 1 / (1 + np.exp(-x)),\n    num_arrays=1,\n    index_to_bnds={0: (-np.log(sys.float_info.max), None)},\n)\ndef test_sigmoid_fwd():\n    pass\n\n\n@backprop_test_factory(\n    mygrad_func=sigmoid,\n    true_func=lambda x: 1 / (1 + np.exp(-x)),\n    num_arrays=1,\n    index_to_bnds={0: (-np.log(sys.float_info.max), None)},\n)\ndef test_sigmoid_bkwd():\n    pass\n'"
tests/nnet/activations/test_softmax.py,22,"b'import numpy as np\nfrom numpy.testing import assert_allclose\n\nfrom mygrad import Tensor\nfrom mygrad.nnet.activations import logsoftmax, softmax\n\n\ndef test_static_softmax_integer():\n    # reuse the test cases from below with integer arrays\n    skew = np.array([0.87566484, 0.53596079, 0.85693981, 0.09526036])\n    x = Tensor([0, 1, 2, 3])\n\n    f = (softmax(x, constant=False) * skew).sum()\n\n    out = np.array(0.33911235096116465)\n    assert_allclose(actual=f.data, desired=out)\n\n    f.backward()\n    dx = np.array([0.01720112, 0.01715422, 0.12266443, -0.15701977])\n\n    assert_allclose(x.grad, dx, atol=1e-5, rtol=1e-5)\n\n    skew = np.array(\n        [\n            [0.87566484, 0.53596079, 0.85693981, 0.09526036],\n            [0.32024455, 0.81532148, 0.2480434, 0.85119342],\n            [0.57943085, 0.33958252, 0.95864464, 0.22881712],\n        ]\n    )\n    x = Tensor([[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]])\n\n    f = (softmax(x, constant=False) * skew).sum()\n\n    out = np.array(1.449875865467131)\n    assert_allclose(actual=f.data, desired=out)\n\n    f.backward()\n    dx = np.array(\n        [\n            [0.01720112, 0.01715422, 0.12266443, -0.15701977],\n            [-0.01179518, 0.01108053, -0.10425844, 0.10497309],\n            [0.00502799, -0.00723393, 0.12698131, -0.12477536],\n        ]\n    )\n\n    assert_allclose(x.grad, dx, atol=1e-5, rtol=1e-5)\n\n\ndef test_static_softmax1d():\n    # Verified against theano.tensor.softmax\n\n    skew = np.array([0.87566484, 0.53596079, 0.85693981, 0.09526036])\n    x = np.array([0.0, 1.0, 2.0, 3.0])\n\n    x = Tensor(x)\n    f = (softmax(x, constant=False) * skew).sum()\n\n    out = np.array(0.33911235096116465)\n    assert_allclose(actual=f.data, desired=out)\n\n    f.backward()\n    dx = np.array([0.01720112, 0.01715422, 0.12266443, -0.15701977])\n\n    assert_allclose(x.grad, dx, atol=1e-5, rtol=1e-5)\n\n\ndef test_static_softmax2d():\n    # Verified against theano.tensor.softmax\n\n    skew = np.array(\n        [\n            [0.87566484, 0.53596079, 0.85693981, 0.09526036],\n            [0.32024455, 0.81532148, 0.2480434, 0.85119342],\n            [0.57943085, 0.33958252, 0.95864464, 0.22881712],\n        ]\n    )\n\n    x = np.array([[0.0, 1.0, 2.0, 3.0], [4.0, 5.0, 6.0, 7.0], [8.0, 9.0, 10.0, 11.0]])\n\n    x = Tensor(x)\n    f = (softmax(x, constant=False) * skew).sum()\n\n    out = np.array(1.449875865467131)\n    assert_allclose(actual=f.data, desired=out)\n\n    f.backward()\n    dx = np.array(\n        [\n            [0.01720112, 0.01715422, 0.12266443, -0.15701977],\n            [-0.01179518, 0.01108053, -0.10425844, 0.10497309],\n            [0.00502799, -0.00723393, 0.12698131, -0.12477536],\n        ]\n    )\n\n    assert_allclose(x.grad, dx, atol=1e-5, rtol=1e-5)\n\n\ndef test_static_logsoftmax1d():\n    # Verified against theano.tensor.softmax\n\n    skew = np.array([0.87566484, 0.53596079, 0.85693981, 0.09526036])\n    x = np.array([0.0, 1.0, 2.0, 3.0])\n\n    x = Tensor(x)\n    f = (logsoftmax(x, constant=False) * skew).sum()\n\n    out = np.array(-5.596387676353177)\n    assert_allclose(actual=f.data, desired=out)\n\n    f.backward()\n    dx = np.array([0.79988389, 0.3299668, 0.29699009, -1.42684078])\n\n    assert_allclose(x.grad, dx, atol=1e-5, rtol=1e-5)\n\n\ndef test_static_logsoftmax2d():\n    # Verified against theano.tensor.softmax\n    skew = np.array(\n        [\n            [0.87566484, 0.53596079, 0.85693981, 0.09526036],\n            [0.32024455, 0.81532148, 0.2480434, 0.85119342],\n            [0.57943085, 0.33958252, 0.95864464, 0.22881712],\n        ]\n    )\n\n    x = np.array([[0.0, 1.0, 2.0, 3.0], [4.0, 5.0, 6.0, 7.0], [8.0, 9.0, 10.0, 11.0]])\n\n    x = Tensor(x)\n    f = (logsoftmax(x, constant=False) * skew).sum()\n\n    out = np.array(-13.722895761739732)\n    assert_allclose(actual=f.data, desired=out)\n\n    f.backward()\n    dx = np.array(\n        [\n            [0.79988389, 0.3299668, 0.29699009, -1.42684078],\n            [0.24859989, 0.62057111, -0.281343, -0.587828],\n            [0.5119002, 0.15601518, 0.45965687, -1.12757225],\n        ]\n    )\n\n    assert_allclose(x.grad, dx, atol=1e-5, rtol=1e-5)\n'"
tests/nnet/layers/__init__.py,0,b''
tests/nnet/layers/test_batchnorm.py,11,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nfrom hypothesis import given\nfrom numpy.testing import assert_allclose, assert_array_equal\n\nimport mygrad as mg\nfrom mygrad import Tensor\nfrom mygrad.nnet.layers.batchnorm import batchnorm\n\n\ndef simple_batchnorm(x, gamma, beta, eps):\n    axes = [i for i in range(x.ndim)]\n    axes.pop(1)  # every axis except 1\n    axes = tuple(axes)\n    keepdims_shape = tuple(1 if n != 1 else d for n, d in enumerate(x.shape))\n\n    mean = mg.mean(x, axis=axes, keepdims=True)\n    var = mg.var(x, axis=axes, keepdims=True)\n    norm = (x - mean) / mg.sqrt(var + eps)\n\n    if gamma is not None:\n        gamma = gamma.reshape(keepdims_shape)\n        norm *= gamma\n\n    if beta is not None:\n        beta = beta.reshape(keepdims_shape)\n        norm += beta\n    return norm\n\n\n@given(\n    x=hnp.arrays(\n        shape=hnp.array_shapes(min_dims=2, max_dims=4),\n        dtype=float,\n        elements=st.floats(-100, 100),\n    ),\n    data=st.data(),\n)\ndef test_batchnorm(x, data):\n    # optionally draw affine parameters\n    gamma = data.draw(\n        st.none()\n        | hnp.arrays(shape=x.shape[1:2], dtype=float, elements=st.floats(-10, 10)),\n        label=""gamma"",\n    )\n    beta = data.draw(\n        st.none()\n        | hnp.arrays(shape=x.shape[1:2], dtype=float, elements=st.floats(-10, 10)),\n        label=""beta"",\n    )\n    x_orig = np.copy(x)\n\n    gamma_orig = np.copy(gamma) if gamma is not None else None\n    beta_orig = np.copy(beta) if beta is not None else None\n\n    t1 = Tensor(x)\n    t2 = Tensor(x)\n\n    g1 = Tensor(gamma) if gamma is not None else None\n    g2 = Tensor(gamma) if gamma is not None else None\n\n    b1 = Tensor(beta) if beta is not None else None\n    b2 = Tensor(beta) if beta is not None else None\n\n    y1 = simple_batchnorm(t1, g1, b1, eps=1e-6)\n    y2 = batchnorm(t2, gamma=g2, beta=b2, eps=1e-6)\n\n    assert_allclose(actual=y2.data, desired=y1.data, atol=1e-4, rtol=1e-4)\n    grad = data.draw(\n        hnp.arrays(shape=y2.shape, dtype=t2.dtype, elements=st.floats(-10, 10)),\n        label=""grad"",\n    )\n    grad_orig = np.copy(grad)\n\n    y1.backward(grad)\n    y2.backward(grad)\n\n    assert_allclose(actual=t2.grad, desired=t1.grad, atol=1e-4, rtol=1e-4)\n\n    if beta is not None:\n        assert_allclose(actual=b2.grad, desired=b1.grad, atol=1e-4, rtol=1e-4)\n    else:\n        assert b2 is None\n\n    if gamma is not None:\n        assert_allclose(actual=g2.grad, desired=g1.grad, atol=1e-4, rtol=1e-4)\n    else:\n        assert g2 is None\n\n    for n, (o, c) in enumerate(\n        zip((x, gamma, beta, grad), (x_orig, gamma_orig, beta_orig, grad_orig))\n    ):\n        if o is None or c is None:\n            assert o is c, ""(\'{x}\', \'{gamma}\', \'{beta}\', \'{grad}\')[{n}]"".format(\n                x=x, gamma=gamma, beta=beta, grad=grad, n=n\n            )\n        else:\n            assert_array_equal(\n                o,\n                c,\n                err_msg=""(\'{x}\', \'{gamma}\', \'{beta}\', \'{grad}\')[{n}]"".format(\n                    x=x, gamma=gamma, beta=beta, grad=grad, n=n\n                ),\n            )\n\n    if gamma is not None and beta is not None:\n        assert not np.shares_memory(g2.grad, b2.grad)\n    assert not np.shares_memory(grad, t2.grad)\n\n    y2.null_gradients()\n    assert t2.grad is None\n\n    if gamma is not None:\n        assert g2.grad is None\n\n    if beta is not None:\n        assert b2.grad is None\n'"
tests/nnet/layers/test_conv.py,36,"b'"""""" Test conv fwd-prop and back-prop for ND convs""""""\n\nfrom typing import Tuple\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import HealthCheck, assume, given, settings\nfrom numpy.testing import assert_allclose\nfrom pytest import raises\n\nimport mygrad as mg\nfrom mygrad import Tensor\nfrom mygrad.nnet.layers import conv_nd\n\nfrom ...utils.numerical_gradient import numerical_gradient_full\nfrom ...wrappers.uber import backprop_test_factory, fwdprop_test_factory\n\n\n@pytest.mark.parametrize(\n    ""shapes"",\n    [  # x has too few dims\n        st.tuples(\n            hnp.array_shapes(min_dims=0, max_dims=2), hnp.array_shapes(min_dims=3)\n        ),\n        # x.ndim != k.ndim\n        hnp.array_shapes(min_dims=3).flatmap(\n            lambda x: st.tuples(\n                st.just(x),\n                hnp.array_shapes(min_dims=2).filter(lambda s: len(s) != len(x)),\n            )\n        ),\n        # channel sizes don\'t match\n        hnp.array_shapes(min_dims=3).flatmap(\n            lambda x: st.tuples(\n                st.just(x),\n                hnp.array_shapes(min_dims=len(x), max_dims=len(x)).filter(\n                    lambda s: s[1] != x[1]\n                ),\n            )\n        ),\n    ],\n)\n@given(data=st.data())\ndef test_input_validation(\n    shapes: st.SearchStrategy[Tuple[Tuple[int, ...], Tuple[int, ...]]],\n    data: st.DataObject,\n):\n    x_shape, k_shape = data.draw(shapes, label=""x_shape, k_shape"")\n    x = mg.zeros(x_shape, dtype=""float"")\n    k = mg.zeros(k_shape, dtype=""float"")\n\n    with raises(ValueError):\n        conv_nd(x, k, stride=1)\n\n\ndef get_outshape(x_shape, w_shape, stride, dilation):\n    """""" Compute the shape of the output tensor given an input shape, convolutional\n    filter shape, and stride.\n\n    Parameters\n    ----------\n    x_shape : Tuple[int, ...]\n        The shape of the input tensor.\n\n    w_shape : Tuple[int, ...]\n        The shape of the convolutional filter.\n\n    stride : Tuple[int, ...]\n        The stride at which to apply the convolutional filter to the input.\n\n    dilation : Tuple[int, ...]\n        The dilation used to form each window over the data.\n\n    Returns\n    -------\n    numpy.ndarray[int], shape=(num_conv,)\n        The shape of the output tensor resulting from convolving a tensor of shape `x_shape`\n        with a tensor of shape `w_shape`.\n\n        Returns `None` if an invalid combination of shapes are provided.\n    """"""\n    x_shape = np.array(x_shape)\n    w_shape = np.array(w_shape)\n    stride = np.array(stride)\n    dilation = np.array(dilation)\n    out_shape = (x_shape - ((w_shape - 1) * dilation + 1)) / stride + 1\n\n    if not all(i.is_integer() and i > 0 for i in out_shape):\n        msg = ""Stride and kernel dimensions are incompatible: \\n""\n        msg += ""Input dimensions: {}\\n"".format(tuple(x_shape))\n        msg += ""Stride dimensions: {}\\n"".format(tuple(stride))\n        msg += ""Kernel dimensions: {}\\n"".format(tuple(w_shape))\n        msg += ""Dilation dimensions: {}\\n"".format(tuple(dilation))\n        return None\n    return out_shape.astype(np.int32)\n\n\ndef convolve_numpy(input_image, conv_filter, stride, dilation=None):\n    """""" Convolve `input_image` with `conv_filter` at a stride of `stride`.\n\n    Parameters\n    ----------\n    input_image : numpy.ndarray, shape=(C, H, ...)\n        The input over which to perform convolution.\n\n    conv_filter : numpy.ndarray, shape=(C, Hf, ...)\n        The convolutional filter to slide across the image.\n\n    stride : Sequence[int]\n        The stride at which to apply `conv_filter` across `input_image`.\n\n    Returns\n    -------\n    numpy.ndarray, shape=(H\', ...)\n        The result of convolving `input_image` with `conv_filter` at a stride of `stride`,\n        where (H\', W\') is the result of `get_outshape`.\n    """"""\n    conv_shape = conv_filter.shape[1:]\n    in_shape = input_image.shape[1:]\n    if dilation is None:\n        dilation = (1,) * len(stride)\n    out_shape = tuple(get_outshape(in_shape, conv_shape, stride, dilation))\n    out = np.empty(out_shape, np.float32)\n    for ind in np.ndindex(out_shape):\n        slices = (slice(None),) + tuple(\n            slice(i * s, i * s + w * d, d)\n            for i, w, s, d in zip(ind, conv_shape, stride, dilation)\n        )\n        out[ind] = np.sum(conv_filter * input_image[slices])\n    return out\n\n\ndef conv_bank(input_images, conv_filters, stride, dilation=None, padding=tuple()):\n    """""" Convolve a bank of filters over a stack of images.\n\n    Parameters\n    ----------\n    input_images : numpy.ndarray, shape=(N, C, H, ...)\n        The images over which to convolve our filters.\n\n    conv_filters : numpy.ndarray, shape=(K, C, Hf, ...)\n        The convolutional filters to apply to the images.\n\n    stride : Sequence[int]\n        The stride at which to apply each filter to the images.\n\n    dilation : Sequence[int]\n\n    Returns\n    -------\n    numpy.ndarray, shape=(N, K, H\', ...)\n        The result of convolving `input_image` with `conv_filter` at a stride of `stride`,\n        where (H\', ...) is the result of `get_outshape`.\n    """"""\n\n    if isinstance(padding, int):\n        padding = (padding,) * (input_images.ndim - 2)\n    if sum(padding):\n        # symmetric 0-padding for X0, X1, ... dimensions\n        axis_pad = tuple((i, i) for i in (0, 0, *padding))\n        input_images = np.pad(input_images, axis_pad, mode=""constant"")\n\n    img_shape = input_images.shape[2:]\n    conv_shape = conv_filters.shape[2:]\n\n    if dilation is None:\n        dilation = (1,) * len(stride)\n    out_shape = get_outshape(img_shape, conv_shape, stride, dilation)\n\n    out = np.empty((len(input_images), len(conv_filters), *out_shape))\n    for i, image in enumerate(input_images):\n        for j, conv in enumerate(conv_filters):\n            out[i, j] = convolve_numpy(image, conv, stride, dilation)\n    return out\n\n\ndef test_convnd_fwd_trivial():\n\n    # trivial by-hand test: 1-dimensional conv\n    # x:\n    # [ 1,  2,  3,  4]\n\n    # k:\n    # [-1, -2],\n\n    # stride = (2,)\n    x = Tensor(np.arange(1, 5).reshape(1, 1, 4).astype(float))\n    k = Tensor(-1 * np.arange(1, 3).reshape(1, 1, 2).astype(float))\n\n    o = conv_nd(x, k, stride=(2,), constant=True)\n\n    out = np.array([[[-5.0, -11.0]]])\n    assert isinstance(o, Tensor)\n    assert o.constant is True\n    assert o.scalar_only is False\n    assert_allclose(actual=o.data, desired=out, err_msg=""1d trivial test failed"")\n\n    # trivial by-hand test: 2-dimensional conv\n    # x:\n    # [ 1,  2,  3,  4],\n    # [ 5,  6,  7,  8],\n    # [ 9, 10, 11, 12]]\n\n    # k:\n    # [-1, -2],\n    # [-3, -4]\n\n    # stride = (1, 2)\n    x = Tensor(np.arange(1, 13).reshape(1, 1, 3, 4).astype(float))\n    k = Tensor(-1 * np.arange(1, 5).reshape(1, 1, 2, 2).astype(float))\n\n    o = conv_nd(Tensor(x), k, stride=(1, 2), constant=True)\n\n    out = np.array([[[[-44.0, -64.0], [-84.0, -104.0]]]])\n    assert isinstance(o, Tensor)\n    assert o.constant is True\n    assert o.scalar_only is False\n    assert_allclose(actual=o.data, desired=out, err_msg=""2d trivial test failed"")\n\n\ndef test_bad_conv_shapes():\n    x = np.zeros((1, 2, 2, 2))\n    w = np.zeros((1, 3, 2, 2))\n    with raises(ValueError):\n        conv_nd(x, w, stride=1, padding=0)  # mismatched channels\n\n    w = np.zeros((1, 2, 3, 2))\n    with raises(ValueError):\n        conv_nd(x, w, stride=1, padding=0)  # large filter\n\n    w = np.zeros((1, 2, 2, 2))\n    with raises(AssertionError):\n        conv_nd(x, w, stride=0, padding=0)  # bad stride\n\n    with raises(AssertionError):\n        conv_nd(x, w, stride=[1, 2, 3])  # bad stride\n\n    with raises(AssertionError):\n        conv_nd(x, w, stride=1, padding=-1)  # bad pad\n\n    with raises(AssertionError):\n        conv_nd(x, w, stride=1, padding=[1, 2, 3])  # bad pad\n\n    with raises(ValueError):\n        conv_nd(x, w, stride=3, padding=1)  # shape mismatch\n\n\n@settings(deadline=None)\n@given(ndim=st.integers(1, 4), data=st.data())\ndef test_padding(ndim: int, data: st.DataObject):\n    """"""Ensure that convolving a padding-only image with a commensurate kernel yields the single entry: 0""""""\n    padding = data.draw(\n        st.integers(1, 3) | st.tuples(*[st.integers(1, 3)] * ndim), label=""padding""\n    )\n    x = Tensor(\n        data.draw(\n            hnp.arrays(shape=(1, 1) + (0,) * ndim, dtype=float, elements=st.floats()),\n            label=""x"",\n        )\n    )\n    pad_tuple = padding if isinstance(padding, tuple) else (padding,) * ndim\n    kernel = data.draw(\n        hnp.arrays(\n            shape=(1, 1) + tuple(2 * p for p in pad_tuple),\n            dtype=float,\n            elements=st.floats(allow_nan=False, allow_infinity=False),\n        )\n    )\n    out = conv_nd(x, kernel, padding=padding, stride=1)\n    assert out.shape == (1,) * x.ndim\n    assert out.item() == 0.0\n\n    out.sum().backward()\n    assert x.grad.shape == x.shape\n\n\n@fwdprop_test_factory(\n    mygrad_func=conv_nd,\n    true_func=conv_bank,\n    num_arrays=2,\n    index_to_arr_shapes={0: (4, 5, 7), 1: (2, 5, 3)},\n    kwargs=dict(stride=(1,), dilation=(1,)),\n    index_to_bnds={0: (-10, 10), 1: (-10, 10)},\n)\ndef test_conv_1d_fwd():\n    """""" (N=4, C=5, W=7) x (F=2, C=5, Wf=3); stride=1, dilation=1\n\n    Also tests meta properties of conv function - appropriate return type,\n    behavior with `constant` arg, etc.""""""\n\n\ndef _conv_nd(x, w, stride, dilation=1, padding=0):\n    """""" use mygrad-conv_nd forward pass for numerical derivative\n\n        Returns\n        -------\n        numpy.ndarray""""""\n    return conv_nd(\n        x, w, stride=stride, dilation=dilation, padding=padding, constant=True\n    ).data\n\n\n@settings(deadline=None)\n@backprop_test_factory(\n    mygrad_func=conv_nd,\n    true_func=_conv_nd,\n    num_arrays=2,\n    index_to_arr_shapes={0: (2, 1, 7), 1: (2, 1, 3)},\n    kwargs={""stride"": (1,)},\n    index_to_bnds={0: (-10, 10), 1: (-10, 10)},\n    vary_each_element=True,\n)\ndef test_conv_1d_bkwd():\n    """""" (N=2, C=1, W=7) x (F=2, C=1, Wf=3); stride=1, dilation=1\n\n    Also tests meta properties of conv-backprop - appropriate return type,\n    behavior with `constant` arg, good behavior of null_gradients, etc.""""""\n\n\n@settings(deadline=None, suppress_health_check=(HealthCheck.filter_too_much,))\n@given(\n    data=st.data(),\n    shape=hnp.array_shapes(min_dims=1, max_dims=3, max_side=10),\n    num_filters=st.integers(1, 3),\n    num_batch=st.integers(1, 3),\n    num_channel=st.integers(1, 3),\n)\ndef test_conv_ND_fwd(data, shape, num_filters, num_batch, num_channel):\n    img_shape = (num_batch, num_channel) + shape\n\n    padding = data.draw(\n        st.integers(0, 2) | st.tuples(*[st.integers(0, 2)] * len(shape)),\n        label=""padding"",\n    )\n\n    if isinstance(padding, tuple):\n        shape = tuple(s + 2 * p for s, p in zip(shape, padding))\n    else:\n        shape = tuple(s + 2 * padding for s in shape)\n\n    win_dim = len(shape)\n    shape = (num_batch, num_channel) + shape\n    win_shape = data.draw(\n        st.tuples(*(st.integers(1, s) for s in shape[-win_dim:])), label=""win_shape""\n    )\n    kernel_shape = (num_filters, shape[1], *win_shape)\n    stride = data.draw(\n        st.tuples(*(st.integers(1, s) for s in shape[-win_dim:])), label=""stride""\n    )\n    max_dilation = np.array(shape[-win_dim:]) // win_shape\n    dilation = data.draw(\n        st.tuples(*(st.integers(1, s) for s in max_dilation)), label=""dilation""\n    )\n    conf = dict(stride=stride, dilation=dilation, padding=padding)\n\n    # skip invalid data/kernel/stride/dilation combinations\n    assume(get_outshape(shape[2:], kernel_shape[2:], stride, dilation) is not None)\n\n    kernels = data.draw(\n        hnp.arrays(dtype=float, shape=kernel_shape, elements=st.floats(-10, 10)),\n        label=""kernels"",\n    )\n    x = data.draw(\n        hnp.arrays(dtype=float, shape=img_shape, elements=st.floats(-10, 10)), label=""x""\n    )\n\n    mygrad_conv = conv_nd(x, kernels, **conf).data\n    numpy_conv = conv_bank(x, kernels, **conf)\n    assert_allclose(actual=mygrad_conv, desired=numpy_conv, atol=1e-6, rtol=1e-6)\n\n\n@settings(deadline=None, suppress_health_check=(HealthCheck.filter_too_much,))\n@given(\n    data=st.data(),\n    shape=hnp.array_shapes(min_dims=1, max_dims=3, max_side=6),\n    num_filters=st.integers(1, 3),\n    num_batch=st.integers(1, 3),\n    num_channel=st.integers(1, 3),\n)\ndef test_conv_ND_bkwd(data, shape, num_filters, num_batch, num_channel):\n    """""" Test conv-backprop 1D-3D with various strides and dilations.""""""\n    img_shape = (num_batch, num_channel) + shape\n\n    padding = data.draw(\n        st.integers(0, 2) | st.tuples(*[st.integers(0, 2)] * len(shape)),\n        label=""padding"",\n    )\n\n    if isinstance(padding, tuple):\n        shape = tuple(s + 2 * p for s, p in zip(shape, padding))\n    else:\n        shape = tuple(s + 2 * padding for s in shape)\n\n    win_dim = len(shape)\n    shape = (num_batch, num_channel) + shape\n    win_shape = data.draw(\n        st.tuples(*(st.integers(1, s) for s in shape[-win_dim:])), label=""win_shape""\n    )\n    kernel_shape = (num_filters, shape[1], *win_shape)\n\n    stride = data.draw(\n        st.tuples(*(st.integers(1, s) for s in shape[-win_dim:])), label=""stride""\n    )\n\n    max_dilation = np.array(shape[-win_dim:]) // win_shape\n    dilation = data.draw(\n        st.tuples(*(st.integers(1, s) for s in max_dilation)), label=""dilation""\n    )\n    conf = dict(stride=stride, dilation=dilation, padding=padding)\n\n    # skip invalid data/kernel/stride/dilation combinations\n    assume(get_outshape(shape[2:], kernel_shape[2:], stride, dilation) is not None)\n\n    kernels = data.draw(\n        hnp.arrays(dtype=float, shape=kernel_shape, elements=st.floats(-10, 10)),\n        label=""kernels"",\n    )\n    x = data.draw(\n        hnp.arrays(dtype=float, shape=img_shape, elements=st.floats(-10, 10)), label=""x""\n    )\n\n    x = Tensor(x)\n    kernels = Tensor(kernels)\n\n    out = conv_nd(x, kernels, **conf)\n    grad = data.draw(\n        hnp.arrays(\n            shape=out.shape, dtype=float, elements=st.floats(-10, 10), unique=True\n        ),\n        label=""grad"",\n    )\n\n    out.backward(grad)\n    grads_numerical = numerical_gradient_full(\n        _conv_nd, *(i.data for i in (x, kernels)), back_grad=grad, kwargs=conf\n    )\n\n    for n, (arr, d_num) in enumerate(zip((x, kernels), grads_numerical)):\n        assert_allclose(\n            arr.grad,\n            d_num,\n            atol=1e-4,\n            rtol=1e-4,\n            err_msg=""arr-{}: numerical derivative and mygrad derivative do not match"".format(\n                n\n            ),\n        )\n'"
tests/nnet/layers/test_gru.py,31,"b'from typing import Tuple\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import example, given, settings\nfrom numpy.testing import assert_allclose\n\nfrom mygrad import matmul\nfrom mygrad.nnet.activations import sigmoid, tanh\nfrom mygrad.nnet.layers import gru\nfrom mygrad.tensor_base import Tensor\nfrom tests.utils import does_not_raise\n\n\n@settings(deadline=None)\n@example(s0=np.array([[0.0, 0.0]]), dropout=0.0, out_constant=False)\n@given(\n    s0=st.none()\n    | hnp.arrays(shape=(1, 2), dtype=float, elements=st.just(0))\n    | hnp.arrays(shape=(1, 2), dtype=float, elements=st.just(0)).map(\n        lambda x: Tensor(x)\n    )\n    | hnp.arrays(shape=(1, 2), dtype=float, elements=st.just(0)).map(\n        lambda x: Tensor(x, constant=True)\n    ),\n    dropout=st.floats(0, 1),\n    out_constant=st.booleans(),\n)\ndef test_nonconstant_s0_raises(s0, dropout: float, out_constant: bool):\n    T, N, C, D = 5, 1, 3, 2\n    X = Tensor(np.random.rand(T, N, C))\n    Wz, Wr, Wh = Tensor(np.random.rand(3, D, D))\n    Uz, Ur, Uh = Tensor(np.random.rand(3, C, D))\n    bz, br, bh = Tensor(np.random.rand(3, D))\n\n    with does_not_raise() if (\n        out_constant or s0 is None or isinstance(s0, np.ndarray) or s0.constant\n    ) else pytest.raises(ValueError):\n        gru(\n            X,\n            Uz,\n            Wz,\n            bz,\n            Ur,\n            Wr,\n            br,\n            Uh,\n            Wh,\n            bh,\n            s0=s0,\n            dropout=dropout,\n            constant=out_constant,\n        )\n\n\n@settings(deadline=None)\n@given(out_constant=st.booleans())\ndef test_all_constant(out_constant: bool):\n    T, N, C, D = 5, 1, 3, 2\n    X = Tensor(np.random.rand(T, N, C), constant=True)\n    Wz, Wr, Wh = Tensor(np.random.rand(3, D, D), constant=True)\n    Uz, Ur, Uh = Tensor(np.random.rand(3, C, D), constant=True)\n    bz, br, bh = Tensor(np.random.rand(3, D), constant=True)\n\n    gru(X, Uz, Wz, bz, Ur, Wr, br, Uh, Wh, bh, constant=out_constant).backward()\n\n    assert X.grad is None\n\n    assert Wz.grad is None\n    assert Wr.grad is None\n    assert Wh.grad is None\n\n    assert Uz.grad is None\n    assert Ur.grad is None\n    assert Uh.grad is None\n\n    assert bz.grad is None\n    assert br.grad is None\n    assert bh.grad is None\n\n\n@settings(deadline=None)\n@given(\n    X=hnp.arrays(\n        shape=hnp.array_shapes(max_side=5, min_dims=3, max_dims=3),\n        dtype=float,\n        elements=st.floats(-10, 10),\n    ),\n    D=st.sampled_from(list(range(1, 5))),\n    dropout=st.sampled_from([0, 0.45]),\n    data=st.data(),\n)\n@pytest.mark.filterwarnings(""ignore: overflow encountered in exp"")\n@pytest.mark.filterwarnings(""ignore: overflow encountered in sig"")\ndef test_gru_fwd(X, D, dropout, data: st.DataObject):\n    T, N, C = X.shape\n\n    Wz, Wr, Wh = data.draw(\n        hnp.arrays(shape=(3, D, D), dtype=float, elements=st.floats(-10.0, 10.0)),\n        label=""Wz, Wr, Wh"",\n    )\n\n    Uz, Ur, Uh = data.draw(\n        hnp.arrays(shape=(3, C, D), dtype=float, elements=st.floats(-10.0, 10.0)),\n        label=""Uz, Ur, Uh"",\n    )\n\n    bz, br, bh = data.draw(\n        hnp.arrays(shape=(3, D), dtype=float, elements=st.floats(-10.0, 10.0)),\n        label=""bz, br, bh"",\n    )\n\n    V = data.draw(\n        hnp.arrays(shape=(D, C), dtype=float, elements=st.floats(-10.0, 10.0)),\n        label=""V"",\n    )\n\n    s0 = np.zeros((N, D), dtype=float)\n\n    X = Tensor(X)\n    X2 = X.__copy__()\n\n    Wz = Tensor(Wz)\n    Wz2 = Wz.__copy__()\n\n    Uz = Tensor(Uz)\n    Uz2 = Uz.__copy__()\n\n    bz = Tensor(bz)\n    bz2 = bz.__copy__()\n\n    Wr = Tensor(Wr)\n    Wr2 = Wr.__copy__()\n\n    Ur = Tensor(Ur)\n    Ur2 = Ur.__copy__()\n\n    br = Tensor(br)\n    br2 = br.__copy__()\n\n    Wh = Tensor(Wh)\n    Wh2 = Wh.__copy__()\n\n    Uh = Tensor(Uh)\n    Uh2 = Uh.__copy__()\n\n    bh = Tensor(bh)\n    bh2 = bh.__copy__()\n\n    V = Tensor(V)\n    V2 = V.__copy__()\n\n    s0 = Tensor(s0)\n    s2 = s0.__copy__()\n\n    s = gru(X, Uz, Wz, bz, Ur, Wr, br, Uh, Wh, bh, dropout=dropout, constant=True)\n    o = matmul(s[1:], V)\n    ls = o.sum()\n\n    assert s.constant is True\n\n    if dropout:\n        for d in [\n            s.creator._dropUr,\n            s.creator._dropUz,\n            s.creator._dropUh,\n            s.creator._dropWr,\n            s.creator._dropWz,\n            s.creator._dropWh,\n        ]:\n            assert np.all(np.logical_or(d == 1 / (1 - dropout), d == 0))\n\n    stt = s2\n    all_s = [s0.data]\n    ls2 = 0\n    if dropout:\n        Wz2d = s.creator._dropWz * Wz2\n        Wr2d = s.creator._dropWr * Wr2\n        Wh2d = s.creator._dropWh * Wh2\n    else:\n        Wz2d = Wz2\n        Wr2d = Wr2\n        Wh2d = Wh2\n    for n, x in enumerate(X2):\n        if not dropout:\n            z = sigmoid(matmul(x, Uz2) + matmul(stt, Wz2d) + bz2)\n            r = sigmoid(matmul(x, Ur2) + matmul(stt, Wr2d) + br2)\n            h = tanh(matmul(x, Uh2) + matmul((r * stt), Wh2d) + bh2)\n        else:\n            z = sigmoid(\n                (s.creator._dropUz[0] * matmul(x, Uz2)) + matmul(stt, Wz2d) + bz2\n            )\n            r = sigmoid(\n                (s.creator._dropUr[0] * matmul(x, Ur2)) + matmul(stt, Wr2d) + br2\n            )\n            h = tanh(\n                (s.creator._dropUh[0] * matmul(x, Uh2)) + matmul((r * stt), Wh2d) + bh2\n            )\n\n        stt = (1 - z) * h + z * stt\n        all_s.append(stt)\n        o = matmul(stt, V2)\n        ls2 += o.sum()\n\n    tolerances = dict(atol=1e-5, rtol=1e-5)\n    rec_s_dat = np.stack([i.data for i in all_s])\n\n    assert_allclose(ls.data, ls2.data, **tolerances)\n\n    assert_allclose(rec_s_dat, s.data, **tolerances)\n\n    assert_allclose(Wz.data, Wz2.data, **tolerances)\n    assert_allclose(Wr.data, Wr2.data, **tolerances)\n    assert_allclose(Wh.data, Wh2.data, **tolerances)\n\n    assert_allclose(Uz.data, Uz2.data, **tolerances)\n    assert_allclose(Ur.data, Ur2.data, **tolerances)\n    assert_allclose(Uh.data, Uh2.data, **tolerances)\n\n    assert_allclose(bz.data, bz2.data, **tolerances)\n    assert_allclose(br.data, br2.data, **tolerances)\n    assert_allclose(bh.data, bh2.data, **tolerances)\n\n    assert_allclose(V.data, V2.data, **tolerances)\n\n    assert_allclose(X.data, X2.data, **tolerances)\n\n    ls.null_gradients()\n    for x in [s, Wz, Wr, Wh, bz, br, bh, X, Uz, Ur, Uh, V]:\n        assert x.grad is None\n\n\n# There is an occasional overflow in the oracle sigmoid\n# that is acceptable - reducing the input domain to ameliorate this\n# would potentially mask real numerical issues\n@settings(deadline=None)\n@given(\n    data=st.data(),\n    X=hnp.arrays(\n        shape=hnp.array_shapes(max_side=5, min_dims=3, max_dims=3),\n        dtype=float,\n        elements=st.floats(-10, 10),\n    ),\n    D=st.sampled_from(list(range(1, 5))),\n    bp_lim=st.booleans(),\n    dropout=st.sampled_from([0, 0.45]),\n    U_constants=st.tuples(*[st.booleans()] * 3),\n    W_constants=st.tuples(*[st.booleans()] * 3),\n    b_constants=st.tuples(*[st.booleans()] * 3),\n    X_constant=st.booleans(),\n    V_constant=st.booleans(),\n)\n@pytest.mark.filterwarnings(""ignore: overflow encountered in exp"")\n@pytest.mark.filterwarnings(""ignore: overflow encountered in sig"")\ndef test_gru_backward(\n    data: st.DataObject,\n    X: np.ndarray,\n    D: int,\n    bp_lim: bool,\n    dropout: bool,\n    U_constants: Tuple[bool, bool, bool],\n    W_constants: Tuple[bool, bool, bool],\n    b_constants: Tuple[bool, bool, bool],\n    X_constant: bool,\n    V_constant: bool,\n):\n    tolerances = dict(atol=1e-5, rtol=1e-5)\n    T, N, C = X.shape\n\n    Wz, Wr, Wh = data.draw(\n        hnp.arrays(shape=(3, D, D), dtype=float, elements=st.floats(-10.0, 10.0)),\n        label=""Wz, Wr, Wh"",\n    )\n\n    Uz, Ur, Uh = data.draw(\n        hnp.arrays(shape=(3, C, D), dtype=float, elements=st.floats(-10.0, 10.0)),\n        label=""Uz, Ur, Uh"",\n    )\n\n    bz, br, bh = data.draw(\n        hnp.arrays(shape=(3, D), dtype=float, elements=st.floats(-10.0, 10.0)),\n        label=""bz, br, bh"",\n    )\n\n    V = data.draw(\n        hnp.arrays(shape=(D, C), dtype=float, elements=st.floats(-10.0, 10.0)),\n        label=""V"",\n    )\n\n    s0 = np.zeros((N, D), dtype=float)\n\n    X = Tensor(X, constant=X_constant)\n    X2 = X.__copy__()\n\n    Wz = Tensor(Wz, constant=W_constants[0])\n    Wz2 = Wz.__copy__()\n\n    Uz = Tensor(Uz, constant=U_constants[0])\n    Uz2 = Uz.__copy__()\n\n    bz = Tensor(bz, constant=b_constants[0])\n    bz2 = bz.__copy__()\n\n    Wr = Tensor(Wr, constant=W_constants[1])\n    Wr2 = Wr.__copy__()\n\n    Ur = Tensor(Ur, constant=U_constants[1])\n    Ur2 = Ur.__copy__()\n\n    br = Tensor(br, constant=b_constants[1])\n    br2 = br.__copy__()\n\n    Wh = Tensor(Wh, constant=W_constants[2])\n    Wh2 = Wh.__copy__()\n\n    Uh = Tensor(Uh, constant=U_constants[2])\n    Uh2 = Uh.__copy__()\n\n    bh = Tensor(bh, constant=b_constants[2])\n    bh2 = bh.__copy__()\n\n    V = Tensor(V, constant=V_constant)\n    V2 = V.__copy__()\n\n    s0 = Tensor(s0)\n    s2 = s0.__copy__()\n\n    # bp_lim = len(X) - 1 should behave the same as no bp-lim\n    s = gru(\n        X,\n        Uz,\n        Wz,\n        bz,\n        Ur,\n        Wr,\n        br,\n        Uh,\n        Wh,\n        bh,\n        dropout=dropout,\n        constant=False,\n        bp_lim=len(X) - 1 if bp_lim else None,\n    )\n    o = matmul(s[1:], V)\n    ls = o.sum()\n    ls.backward()\n\n    stt = s2\n    all_s = [s0.data]\n    ls2 = 0\n    if dropout:\n        Wz2d = s.creator._dropWz * Wz2\n        Wr2d = s.creator._dropWr * Wr2\n        Wh2d = s.creator._dropWh * Wh2\n    else:\n        Wz2d = Wz2\n        Wr2d = Wr2\n        Wh2d = Wh2\n    for n, x in enumerate(X2):\n        if not dropout:\n            z = sigmoid(matmul(x, Uz2) + matmul(stt, Wz2d) + bz2)\n            r = sigmoid(matmul(x, Ur2) + matmul(stt, Wr2d) + br2)\n            h = tanh(matmul(x, Uh2) + matmul((r * stt), Wh2d) + bh2)\n        else:\n            z = sigmoid(\n                (s.creator._dropUz[0] * matmul(x, Uz2)) + matmul(stt, Wz2d) + bz2\n            )\n            r = sigmoid(\n                (s.creator._dropUr[0] * matmul(x, Ur2)) + matmul(stt, Wr2d) + br2\n            )\n            h = tanh(\n                (s.creator._dropUh[0] * matmul(x, Uh2)) + matmul((r * stt), Wh2d) + bh2\n            )\n        stt = (1 - z) * h + z * stt\n        all_s.append(stt)\n        o = matmul(stt, V2)\n        ls2 += o.sum()\n    ls2.backward()\n\n    rec_s_grad = np.stack([i.grad for i in all_s[1:]])\n\n    if not s.constant:\n        assert_allclose(rec_s_grad, s.grad, **tolerances)\n    else:\n        assert s.grad is None\n\n    if not Wz.constant:\n        assert_allclose(Wz.grad, Wz2.grad, **tolerances)\n    else:\n        assert Wz.grad is None\n\n    if not Wr.constant:\n        assert_allclose(Wr.grad, Wr2.grad, **tolerances)\n    else:\n        assert Wr.grad is None\n\n    if not Wh.constant:\n        assert_allclose(Wh.grad, Wh2.grad, **tolerances)\n    else:\n        assert Wh.grad is None\n\n    if not Uz.constant:\n        assert_allclose(Uz.grad, Uz2.grad, **tolerances)\n    else:\n        assert Uz.grad is None\n\n    if not Ur.constant:\n        assert_allclose(Ur.grad, Ur2.grad, **tolerances)\n    else:\n        assert Ur.grad is None\n\n    if not Uh.constant:\n        assert_allclose(Uh.grad, Uh2.grad, **tolerances)\n    else:\n        assert Uh.grad is None\n\n    if not bz.constant:\n        assert_allclose(bz.grad, bz2.grad, **tolerances)\n    else:\n        assert bz.grad is None\n\n    if not br.constant:\n        assert_allclose(br.grad, br2.grad, **tolerances)\n    else:\n        assert br.grad is None\n\n    if not bh.constant:\n        assert_allclose(bh.grad, bh2.grad, **tolerances)\n    else:\n        assert bh.grad is None\n\n    if not V.constant:\n        assert_allclose(V.grad, V2.grad, **tolerances)\n    else:\n        assert V.grad is None\n\n    if not X.constant:\n        assert_allclose(X.grad, X2.grad, **tolerances)\n    else:\n        assert X.grad is None\n\n    ls.null_gradients()\n    ls2.null_gradients()\n\n    for x in [s, Wz, Wr, Wh, bz, br, bh, X, Uz, Ur, Uh, V]:\n        assert x.grad is None\n'"
tests/nnet/layers/test_maxpool.py,13,"b'import numpy as np\nfrom numpy.testing import assert_allclose\nfrom pytest import raises\n\nfrom mygrad.nnet.layers import max_pool\nfrom mygrad.tensor_base import Tensor\n\n\ndef text_constant():\n    x = np.array(\n        [\n            [\n                [17, 10, 15, 28, 25, 23],\n                [44, 26, 18, 16, 39, 34],\n                [5, 42, 36, 0, 2, 46],\n                [30, 20, 1, 31, 35, 43],\n            ],\n            [\n                [6, 7, 45, 27, 11, 8],\n                [37, 4, 41, 22, 9, 33],\n                [47, 3, 13, 32, 21, 38],\n                [19, 12, 40, 24, 14, 29],\n            ],\n        ]\n    )\n    x = Tensor(x)\n    pool = (3,)\n    stride = (1,)\n    assert max_pool(x, pool, stride, constant=True).constant is True\n    assert max_pool(x, pool, stride, constant=False).constant is False\n\n\ndef test_1d_case():\n    x = np.array(\n        [\n            [\n                [17, 10, 15, 28, 25, 23],\n                [44, 26, 18, 16, 39, 34],\n                [5, 42, 36, 0, 2, 46],\n                [30, 20, 1, 31, 35, 43],\n            ],\n            [\n                [6, 7, 45, 27, 11, 8],\n                [37, 4, 41, 22, 9, 33],\n                [47, 3, 13, 32, 21, 38],\n                [19, 12, 40, 24, 14, 29],\n            ],\n        ]\n    )\n    x = Tensor(x)\n    pool = (3,)\n    stride = (1,)\n    out = max_pool(x, pool, stride)\n    out.backward(np.arange(out.data.size).reshape(out.shape))\n\n    fwd_ans = np.array(\n        [\n            [[17, 28, 28, 28], [44, 26, 39, 39], [42, 42, 36, 46], [30, 31, 35, 43]],\n            [[45, 45, 45, 27], [41, 41, 41, 33], [47, 32, 32, 38], [40, 40, 40, 29]],\n        ]\n    )\n\n    bkc_ans = np.array(\n        [\n            [\n                [0.0, 0.0, 0.0, 6.0, 0.0, 0.0],\n                [4.0, 5.0, 0.0, 0.0, 13.0, 0.0],\n                [0.0, 17.0, 10.0, 0.0, 0.0, 11.0],\n                [12.0, 0.0, 0.0, 13.0, 14.0, 15.0],\n            ],\n            [\n                [0.0, 0.0, 51.0, 19.0, 0.0, 0.0],\n                [0.0, 0.0, 63.0, 0.0, 0.0, 23.0],\n                [24.0, 0.0, 0.0, 51.0, 0.0, 27.0],\n                [0.0, 0.0, 87.0, 0.0, 0.0, 31.0],\n            ],\n        ]\n    )\n    assert isinstance(out, Tensor)\n    assert_allclose(fwd_ans, out.data)\n    assert_allclose(bkc_ans, x.grad)\n    assert max_pool(x, pool, stride, constant=True).constant is True\n    assert max_pool(x, pool, stride, constant=False).constant is False\n\n\ndef test_2d_case():\n    x = np.array(\n        [\n            [\n                [17, 10, 15, 28, 25, 23],\n                [44, 26, 18, 16, 39, 34],\n                [5, 42, 36, 0, 2, 46],\n                [30, 20, 1, 31, 35, 43],\n            ],\n            [\n                [6, 7, 45, 27, 11, 8],\n                [37, 4, 41, 22, 9, 33],\n                [47, 3, 13, 32, 21, 38],\n                [19, 12, 40, 24, 14, 29],\n            ],\n        ]\n    )\n    x = Tensor(x)\n    pool = (2, 3)\n    stride = (2, 1)\n    out = max_pool(x, pool, stride)\n    out.sum().backward()\n\n    fwd_ans = np.array(\n        [[[44, 28, 39, 39], [42, 42, 36, 46]], [[45, 45, 45, 33], [47, 40, 40, 38]]]\n    )\n\n    bkc_ans = np.array(\n        [\n            [\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0, 0.0, 2.0, 0.0],\n                [0.0, 2.0, 1.0, 0.0, 0.0, 1.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            ],\n            [\n                [0.0, 0.0, 3.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n                [1.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n                [0.0, 0.0, 2.0, 0.0, 0.0, 0.0],\n            ],\n        ]\n    )\n    assert isinstance(out, Tensor)\n    assert_allclose(fwd_ans, out.data)\n    assert_allclose(bkc_ans, x.grad)\n\n\ndef test_3d_case():\n    x = np.array(\n        [\n            [\n                [33, 27, 47, 11, 7, 36],\n                [20, 18, 9, 2, 3, 17],\n                [45, 31, 24, 12, 25, 19],\n                [28, 1, 8, 16, 34, 14],\n            ],\n            [\n                [37, 39, 40, 41, 21, 13],\n                [35, 15, 6, 4, 23, 30],\n                [43, 46, 32, 10, 26, 42],\n                [38, 5, 44, 29, 0, 22],\n            ],\n        ]\n    )\n    x = Tensor(x)\n    pool = (2, 2, 2)\n    stride = (1, 1, 2)\n    out = max_pool(x, pool, stride)\n    g = np.arange(out.data.size)\n    out.backward(g.reshape(out.shape))\n\n    fwd_ans = np.array([[[39, 47, 36], [46, 32, 42], [46, 44, 42]]])\n\n    bkc_ans = np.array(\n        [\n            [\n                [0.0, 0.0, 1.0, 0.0, 0.0, 2.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n            ],\n            [\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 9.0, 4.0, 0.0, 0.0, 13.0],\n                [0.0, 0.0, 7.0, 0.0, 0.0, 0.0],\n            ],\n        ]\n    )\n    assert isinstance(out, Tensor)\n    assert_allclose(fwd_ans, out.data)\n    assert_allclose(bkc_ans, x.grad)\n\n\ndef test_bad_max_shapes():\n    x = Tensor(np.zeros((1, 2, 2, 2)))\n    with raises(ValueError):\n        max_pool(x, (3,) * 3, (1,) * 3)  # large filter\n\n    with raises(AssertionError):\n        max_pool(x, (2,) * 3, (0,) * 3)  # bad stride\n\n    with raises(AssertionError):\n        max_pool(x, (2,) * 2, [1, 2, 3])  # bad stride\n\n    with raises(ValueError):\n        max_pool(x, (1,) * 3, (3,) * 3)  # shape mismatch\n'"
tests/nnet/losses/__init__.py,0,b''
tests/nnet/losses/test_hinge.py,11,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given\nfrom numpy.testing import assert_allclose\nfrom pytest import raises\n\nimport mygrad as mg\nfrom mygrad.nnet.losses import multiclass_hinge\nfrom mygrad.tensor_base import Tensor\n\n\n@pytest.mark.parametrize(\n    (""data"", ""labels""),\n    [\n        (np.ones((2,), dtype=float), np.zeros((2,), dtype=int)),  # 1D data\n        (np.ones((2, 1), dtype=float), np.zeros((2,), dtype=float)),  # non-int labels\n        (np.ones((2, 1), dtype=float), np.zeros((2, 1), dtype=int)),  # bad label-ndim\n        (np.ones((2, 1), dtype=float), np.zeros((3,), dtype=int)),  # bad label-shape\n    ],\n)\ndef test_input_validation(data, labels):\n    with raises((ValueError, TypeError)):\n        multiclass_hinge(data, labels)\n\n\n@given(st.data())\ndef test_multiclass_hinge(data):\n    """"""Test the built-in implementation of multiclass hinge\n    against the pure mygrad version""""""\n    s = data.draw(\n        hnp.arrays(\n            shape=hnp.array_shapes(max_side=10, min_dims=2, max_dims=2),\n            dtype=float,\n            elements=st.floats(-100, 100),\n        )\n    )\n    loss = data.draw(\n        hnp.arrays(\n            shape=(s.shape[0],),\n            dtype=hnp.integer_dtypes(),\n            elements=st.integers(min_value=0, max_value=s.shape[1] - 1),\n        )\n    )\n    hinge_scores = Tensor(s)\n    hinge_loss = multiclass_hinge(hinge_scores, loss, constant=False)\n    hinge_loss.backward()\n\n    mygrad_scores = Tensor(s)\n    correct_labels = (range(len(loss)), loss)\n    correct_class_scores = mygrad_scores[correct_labels]  # Nx1\n\n    Lij = mygrad_scores - correct_class_scores[:, np.newaxis] + 1.0  # NxC margins\n    Lij[Lij <= 0] = 0\n    Lij[correct_labels] = 0\n\n    mygrad_loss = Lij.sum() / mygrad_scores.shape[0]\n    mygrad_loss.backward()\n    assert_allclose(hinge_loss.data, mygrad_loss.data)\n    assert_allclose(mygrad_scores.grad, hinge_scores.grad)\n\n\n@given(shape=st.sampled_from([(3, 1), (3, 4), tuple()]))\ndef test_bad_label_shape(shape):\n    """"""\n    Ensures that `multiclass_hinge` checks for shape-(N,) `y_true`\n    """"""\n    scores = mg.arange(12).reshape(3, 4)\n    labels = mg.zeros(shape, dtype=int)\n    with raises(ValueError):\n        multiclass_hinge(scores, labels)\n\n\n@given(type=st.sampled_from([bool, float, np.float32]))\ndef test_bad_label_type(type):\n    """"""\n    Ensures that `multiclass_hinge` checks integer-type `y_true`\n    """"""\n    scores = mg.arange(12).reshape(3, 4)\n    labels = np.zeros((3,), dtype=type)\n    with raises(TypeError):\n        multiclass_hinge(scores, labels)\n'"
tests/nnet/losses/test_margin_ranking.py,19,"b'from typing import Tuple\n\nimport hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given\nfrom numpy.testing import assert_allclose, assert_array_equal\n\nimport mygrad as mg\nfrom mygrad.nnet.losses import margin_ranking_loss\n\n\n@pytest.mark.parametrize(\n    ""args"",\n    [\n        dict(x1=st.sampled_from([np.ones((1,)), np.ones((1, 1, 1))])),\n        dict(x1=np.ones((3, 4), dtype=int)),\n        dict(x2=np.ones((3, 4), dtype=int)),\n        dict(x2=np.ones((3, 5))),\n        dict(margin=None),\n        dict(margin=-1),\n        dict(y=np.ones((3, 4))),\n        dict(y=np.ones((5,))),\n    ],\n)\n@given(data=st.data())\ndef test_input_validation(args, data: st.DataObject):\n    kwargs = dict(x1=np.ones((3, 4)), y=mg.Tensor(1), margin=0.5)\n\n    if ""x2"" not in args:\n        args[""x2""] = np.ones_like(kwargs[""x1""])\n\n    kwargs.update(\n        (k, (data.draw(v, label=k)) if isinstance(v, st.SearchStrategy) else v)\n        for k, v in args.items()\n    )\n\n    with pytest.raises((ValueError, TypeError)):\n        margin_ranking_loss(**kwargs)\n\n\ndef simple_loss(x1, x2, y, margin):\n    """"""\n    x1 : mygrad.Tensor, shape=(N, D)\n    x2 : mygrad.Tensor, shape=(N, D)\n    y : Union[int, numpy.ndarray, mygrad.Tensor], scalar or shape=(N,)\n    margin : float\n\n    Returns\n    -------\n    mygrad.Tensor, shape=()\n    """"""\n    if isinstance(y, mg.Tensor):\n        y = y.data\n    y = np.asarray(y)\n    if y.ndim:\n        assert y.size == 1 or len(y) == len(x1)\n        if x1.ndim == 2:\n            y = y.reshape(-1, 1)\n\n    return mg.mean(mg.maximum(0, margin - y * (x1 - x2)))\n\n\n@given(\n    shape=hnp.array_shapes(min_dims=1, max_dims=2),\n    margin=st.floats(0, 1000),\n    labels_as_tensor=st.booleans(),\n    data=st.data(),\n)\ndef test_ranked_margin(\n    shape: Tuple[int, ...], margin: float, labels_as_tensor: bool, data: st.DataObject\n):\n    x1 = data.draw(\n        hnp.arrays(shape=shape, dtype=float, elements=st.floats(-1000, 1000)),\n        label=""x1"",\n    )\n    x2 = data.draw(\n        hnp.arrays(shape=shape, dtype=float, elements=st.floats(-1000, 1000)),\n        label=""x2"",\n    )\n    y = data.draw(\n        st.sampled_from((-1, 1))\n        | hnp.arrays(\n            shape=shape[:1],\n            dtype=hnp.integer_dtypes(),\n            elements=st.sampled_from((-1, 1)),\n        ).map(lambda x: mg.Tensor(x) if labels_as_tensor else x),\n        label=""y"",\n    )\n\n    x1_copy = np.copy(x1)\n    x2_copy = np.copy(x2)\n    y_copy = y.copy() if isinstance(y, mg.Tensor) else np.copy(y)\n\n    x1_dum = mg.Tensor(x1)\n    x2_dum = mg.Tensor(x2)\n\n    x1_real = mg.Tensor(x1)\n    x2_real = mg.Tensor(x2)\n\n    loss_dum = simple_loss(x1_dum, x2_dum, y, margin)\n\n    loss_real = margin_ranking_loss(x1_real, x2_real, y, margin)\n\n    assert_allclose(\n        actual=loss_real.data, desired=loss_dum.data, err_msg=""losses don\'t match""\n    )\n\n    assert_array_equal(x1, x1_copy, err_msg=""`x1` was mutated by forward"")\n    assert_array_equal(x2, x2_copy, err_msg=""`x2` was mutated by forward"")\n    if isinstance(y, np.ndarray):\n        assert_array_equal(y, y_copy, err_msg=""`y` was mutated by forward"")\n\n    loss_dum.backward()\n    loss_real.backward()\n\n    assert_allclose(\n        actual=x1_real.grad, desired=x1_dum.grad, err_msg=""x1.grad doesn\'t match""\n    )\n    assert_allclose(\n        actual=x2_real.grad, desired=x2_dum.grad, err_msg=""x2.grad doesn\'t match""\n    )\n\n    assert_array_equal(x1, x1_copy, err_msg=""`x1` was mutated by backward"")\n    assert_array_equal(x2, x2_copy, err_msg=""`x2` was mutated by backward"")\n\n    if isinstance(y, mg.Tensor):\n        y = y.data\n\n    if isinstance(y, np.ndarray):\n        assert_array_equal(y, y_copy, err_msg=""`y` was mutated by backward"")\n\n    loss_real.null_gradients()\n    assert x1_real.grad is None\n    assert x2_real.grad is None\n'"
tests/nnet/losses/test_softmaxcrossentropy.py,11,"b'import hypothesis.extra.numpy as hnp\nimport hypothesis.strategies as st\nimport numpy as np\nimport pytest\nfrom hypothesis import given\nfrom numpy.testing import assert_allclose\nfrom pytest import raises\n\nimport mygrad as mg\nfrom mygrad import log\nfrom mygrad.nnet.activations import softmax\nfrom mygrad.nnet.losses import softmax_crossentropy\nfrom mygrad.tensor_base import Tensor\n\n\n@pytest.mark.parametrize(\n    (""data"", ""labels""),\n    [\n        (np.ones((2,), dtype=float), np.zeros((2,), dtype=int)),  # 1D data\n        (np.ones((2, 1), dtype=float), np.zeros((2,), dtype=float)),  # non-int labels\n        (np.ones((2, 1), dtype=float), np.zeros((2, 1), dtype=int)),  # bad label-ndim\n        (np.ones((2, 1), dtype=float), np.zeros((3,), dtype=int)),  # bad label-shape\n    ],\n)\ndef test_input_validation(data, labels):\n    with raises((ValueError, TypeError)):\n        softmax_crossentropy(data, labels)\n\n\n@given(data=st.data(), labels_as_tensor=st.booleans())\ndef test_softmax_crossentropy(data: st.DataObject, labels_as_tensor: bool):\n    s = data.draw(\n        hnp.arrays(\n            shape=hnp.array_shapes(max_side=10, min_dims=2, max_dims=2),\n            dtype=float,\n            elements=st.floats(-100, 100),\n        )\n    )\n    y_true = data.draw(\n        hnp.arrays(\n            shape=(s.shape[0],),\n            dtype=hnp.integer_dtypes(),\n            elements=st.integers(min_value=0, max_value=s.shape[1] - 1),\n        ).map(Tensor if labels_as_tensor else lambda x: x)\n    )\n    scores = Tensor(s)\n    softmax_cross = softmax_crossentropy(scores, y_true, constant=False)\n    softmax_cross.backward()\n\n    mygrad_scores = Tensor(s)\n    probs = softmax(mygrad_scores)\n\n    correct_labels = (range(len(y_true)), y_true.data if labels_as_tensor else y_true)\n    truth = np.zeros(mygrad_scores.shape)\n    truth[correct_labels] = 1\n\n    mygrad_cross = (-1 / s.shape[0]) * (log(probs) * truth).sum()\n    mygrad_cross.backward()\n    assert_allclose(softmax_cross.data, mygrad_cross.data, atol=1e-5, rtol=1e-5)\n    assert_allclose(scores.grad, mygrad_scores.grad, atol=1e-5, rtol=1e-5)\n\n\n@given(shape=st.sampled_from([(3, 1), (3, 4), tuple()]))\ndef test_bad_label_shape(shape):\n    """"""\n    Ensures that softmax_crossentropy checks for shape-(N,) `y_true`\n    """"""\n    scores = mg.arange(12).reshape(3, 4)\n    labels = mg.zeros(shape, dtype=int)\n    with raises(ValueError):\n        softmax_crossentropy(scores, labels)\n\n\n@given(type=st.sampled_from([bool, float, np.float32]))\ndef test_bad_label_type(type):\n    """"""\n    Ensures that softmax_crossentropy checks integer-type `y_true`\n    """"""\n    scores = mg.arange(12).reshape(3, 4)\n    labels = np.zeros((3,), dtype=type)\n    with raises(TypeError):\n        softmax_crossentropy(scores, labels)\n'"
src/mygrad/math/arithmetic/__init__.py,0,b''
src/mygrad/math/arithmetic/funcs.py,0,"b'from mygrad.tensor_base import Tensor\n\nfrom .ops import (\n    Add,\n    AddSequence,\n    Divide,\n    Multiply,\n    MultiplySequence,\n    Negative,\n    Positive,\n    Power,\n    Reciprocal,\n    Square,\n    Subtract,\n)\n\n__all__ = [\n    ""add"",\n    ""add_sequence"",\n    ""divide"",\n    ""multiply"",\n    ""multiply_sequence"",\n    ""negative"",\n    ""positive"",\n    ""power"",\n    ""reciprocal"",\n    ""square"",\n    ""subtract"",\n]\n\n\ndef add(a, b, constant=False):\n    """""" ``f(a, b) -> a + b``\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> mg.add(1.0, 4.0)\n    Tensor(5.0)\n    >>> x1 = mg.arange(9.0).reshape((3, 3))\n    >>> x2 = mg.arange(3.0)\n    >>> mg.add(x1, x2)  # equivalent to `x1 + x2`\n    Tensor([[  0.,   2.,   4.],\n            [  3.,   5.,   7.],\n            [  6.,   8.,  10.]])""""""\n    return Tensor._op(Add, a, b, constant=constant)\n\n\ndef subtract(a, b, constant=False):\n    """""" ``f(a, b) -> a - b``\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> mg.subtract(1.0, 4.0, constant=True)  # resulting tensor will not back-propagate a gradient\n    Tensor(5.0)\n    >>> x1 = mg.arange(9.0).reshape((3, 3))\n    >>> x2 = mg.arange(3.0)\n    >>> mg.subtract(x2, x1)  # equivalent to `x2 - x1`\n    Tensor([[  0.,   0.,   0.],\n            [  3.,   3.,   3.],\n            [  6.,   6.,  6.]])\n    """"""\n    return Tensor._op(Subtract, a, b, constant=constant)\n\n\ndef divide(a, b, constant=False):\n    """""" ``f(a, b) -> a / b``\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Divide, a, b, constant=constant)\n\n\ndef square(a, constant=False):\n    """""" ``f(a) -> a ** 2``\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Square, a, constant=constant)\n\n\ndef reciprocal(a, constant=False):\n    """""" ``f(a) -> 1 / a``\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Reciprocal, a, constant=constant)\n\n\ndef power(a, b, constant=False):\n    """""" ``f(a, b) -> a ** b``\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Power, a, b, constant=constant)\n\n\ndef multiply(a, b, constant=False):\n    """""" ``f(a, b) -> a * b``\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Multiply, a, b, constant=constant)\n\n\ndef multiply_sequence(*variables, constant=False):\n    """"""  ``f(a, b, ...) -> a * b * ...``\n\n    Multiply a sequence of N tensors.\n\n    Parameters\n    ----------\n    variables : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    It is more efficient to back-propagate through this\n    function than it is through a computational graph\n    with N-1 corresponding multiplication operations.""""""\n    if len(variables) < 2:\n        raise ValueError(\n            f""`multiply_sequence` requires at least two inputs, got {len(variables)} inputs""\n        )\n    return Tensor._op(MultiplySequence, *variables, constant=constant)\n\n\ndef add_sequence(*variables, constant=False):\n    """""" ``f(a, b, ...) -> a + b + ...``\n\n    Add a sequence of N tensors.\n\n    Parameters\n    ----------\n    variables : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    It is more efficient to back-propagate through this\n    function than it is through a computational graph\n    with N-1 corresponding addition operations.""""""\n    if len(variables) < 2:\n        raise ValueError(\n            f""`add_sequence` requires at least two inputs, got {len(variables)} inputs""\n        )\n    return Tensor._op(AddSequence, *variables, constant=constant)\n\n\ndef positive(a, where=True, constant=False):\n    """""" ``f(a) -> +a``\n\n    Creates a new tensor.\n\n    Parameters\n    ----------\n    a : array_like\n\n    where : numpy.ndarray\n        Accepts a boolean array which is broadcast together\n        with the operand(s). Values of True indicate to calculate\n        the function at that position, values of False indicate\n        to leave the value in the output alone.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Positive, a, op_kwargs=(dict(where=where)), constant=constant)\n\n\ndef negative(a, where=True, constant=False):\n    """""" ``f(a) -> -a``\n\n    Parameters\n    ----------\n    a : array_like\n\n    where : numpy.ndarray\n        Accepts a boolean array which is broadcast together\n        with the operand(s). Values of True indicate to calculate\n        the function at that position, values of False indicate\n        to leave the value in the output alone.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n""""""\n    return Tensor._op(Negative, a, op_kwargs=(dict(where=where)), constant=constant)\n'"
src/mygrad/math/arithmetic/ops.py,10,"b'from functools import reduce\n\nimport numpy as np\n\nfrom mygrad.operation_base import BroadcastableOp, Operation\n\n__all__ = [\n    ""Add"",\n    ""Subtract"",\n    ""Multiply"",\n    ""Divide"",\n    ""Reciprocal"",\n    ""Power"",\n    ""Square"",\n    ""Positive"",\n    ""Negative"",\n    ""AddSequence"",\n    ""MultiplySequence"",\n]\n\n\nclass Add(BroadcastableOp):\n    def __call__(self, a, b):\n        """""" Performs \'add\' forward-pass: f(a,b) -> a + b\n\n            Parameters\n            ----------\n            a : mygrad.Tensor\n            b : mygrad.Tensor\n\n            Returns\n            -------\n            out : numpy.ndarray """"""\n\n        self.variables = (a, b)\n        out = a.data + b.data\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad\n\n\nclass Subtract(BroadcastableOp):\n    def __call__(self, a, b):\n        """""" f(a,b) -> a - b\n\n            Parameters\n            ----------\n            a : mygrad.Tensor\n            b : mygrad.Tensor\n\n            Returns\n            -------\n            out : numpy.ndarray """"""\n        self.variables = (a, b)\n        out = a.data - b.data\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        if index == 0:\n            return grad\n        else:\n            return -grad\n\n\nclass Multiply(BroadcastableOp):\n    def __call__(self, a, b):\n        """""" Parameters\n            ----------\n            a : mygrad.Tensor\n            b : mygrad.Tensor""""""\n        self.variables = (a, b)\n        out = a.data * b.data\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        a, b = self.variables\n        if index == 0:  # backprop through a\n            return grad * b.data\n        elif index == 1:  # backprop through b\n            return grad * a.data\n\n\nclass Divide(BroadcastableOp):\n    def __call__(self, a, b):\n        """""" f(a, b) -> a / b""""""\n        self.variables = (a, b)\n        out = a.data / b.data\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        a, b = self.variables\n        if index == 0:  # backprop through a\n            return grad / b.data\n        else:  # broadcast through b\n            return -grad * a.data / (b.data ** 2)\n\n\nclass Reciprocal(BroadcastableOp):\n    def __call__(self, a):\n        """""" f(a) -> 1 / a""""""\n        self.variables = (a,)\n        return np.reciprocal(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return -grad * np.reciprocal(a.data ** 2)\n\n\nclass Power(BroadcastableOp):\n    def __call__(self, a, b):\n        """""" f(a, b) -> a ** b\n\n            Parameters\n            ----------\n            a: mygrad.Tensor\n            b: mygrad.Tensor""""""\n        self.variables = (a, b)\n        out = a.data ** b.data\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        a, b = self.variables\n        x, y = a.data, b.data\n        if index == 0:\n            return grad * y * (x ** np.where(y, (y - 1), 1))\n        else:\n            return grad * (x ** y) * np.log(np.where(x, x, 1))\n\n\nclass Square(Operation):\n    def __call__(self, a):\n        """""" f(a) -> a ** 2\n\n            Parameters\n            ----------\n            a : mygrad.Tensor""""""\n        self.variables = (a,)\n        return np.square(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad * 2 * self.variables[index].data\n\n\nclass Positive(Operation):\n    """""" f(a) = +a """"""\n\n    def __call__(self, a, where=True):\n        """"""\n        Parameters\n        ----------\n        a: mygrad.Tensor\n\n        where : array_like, optional\n            Values of True indicate to calculate the ufunc at that position,\n            values of False indicate to leave the value in the output alone.""""""\n        self.variables = (a,)\n        self.conf = dict(where=where)\n        return np.positive(a.data, where=where)\n\n    def backward_var(self, grad, index, **kwargs):\n        return np.positive(grad, **self.conf)\n\n\nclass Negative(Operation):\n    """""" f(a) = -a """"""\n\n    def __call__(self, a, where=True):\n        """"""\n        Parameters\n        ----------\n        a : mygrad.Tensor\n\n        where : array_like, optional\n            Values of True indicate to calculate the ufunc at that position,\n            values of False indicate to leave the value in the output alone.""""""\n        self.variables = (a,)\n        self.conf = dict(where=where)\n        return np.negative(a.data, where=where)\n\n    def backward_var(self, grad, index, **kwargs):\n        return np.negative(grad, **self.conf)\n\n\nclass AddSequence(BroadcastableOp):\n    """"""Performs f(a, b, ..., z) = a + b + ... + z""""""\n\n    def __call__(self, *input_vars):\n        assert len(input_vars) > 1, ""`add_sequence` requires at least two operands""\n        self.variables = input_vars\n        out = sum(var.data for var in input_vars)\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad\n\n\nclass MultiplySequence(BroadcastableOp):\n    """""" Performs f(a, b, ..., z) = a * b * ... * z""""""\n\n    def __call__(self, *input_vars):\n        self.variables = input_vars\n        assert 2 <= len(self.variables)\n\n        out = reduce(lambda x, y: x * y, (var.data for var in input_vars))\n        self._iszero = np.any(out == 0)\n        return out\n\n    def backward(self, grad, *, graph, **kwargs):\n        """""" Back-propagates the gradient through all of the operation\'s inputs. This needs to be updated\n            by an operation if that operation takes more than 2 Tensor arguments.""""""\n        if not self._iszero:\n            self._product = grad * reduce(\n                lambda x, y: x * y, (var.data for n, var in enumerate(self.variables))\n            )\n        else:\n            self._product = None\n        super().backward(grad, graph=graph, **kwargs)\n\n    def backward_var(self, grad, index, **kwargs):\n        var = self.variables[index]\n        if not self._iszero:\n            return self._product / var.data\n        else:\n            return grad * reduce(\n                lambda x, y: x * y,\n                (var.data for n, var in enumerate(self.variables) if n != index),\n            )\n'"
src/mygrad/math/exp_log/__init__.py,0,b''
src/mygrad/math/exp_log/funcs.py,0,"b'from mygrad.tensor_base import Tensor\n\nfrom .ops import Exp, Exp2, Expm1, Log, Log1p, Log2, Log10, Logaddexp, Logaddexp2\n\n__all__ = [\n    ""exp"",\n    ""exp2"",\n    ""expm1"",\n    ""logaddexp"",\n    ""logaddexp2"",\n    ""log"",\n    ""log2"",\n    ""log10"",\n    ""log1p"",\n]\n\n\ndef exp(a, constant=False):\n    """""" ``f(a) -> exp(a)``\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Exp, a, constant=constant)\n\n\ndef exp2(a, constant=False):\n    """"""``f(a) -> 2^a``\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Exp2, a, constant=constant)\n\n\ndef expm1(a, constant=False):\n    """""" ``f(a) -> exp(a) - 1``\n\n    The inverse of ``logp1``.\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=True)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    This function provides greater precision than ``exp(x) - 1`` for\n    small values of ``x``.""""""\n    return Tensor._op(Expm1, a, constant=constant)\n\n\ndef logaddexp(a, b, constant=False):\n    """""" ``f(a, b) -> log(exp(a) + exp(b))``\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    This function is useful\n    in statistics where the calculated probabilities of events may\n    be so small as to exceed the range of normal floating point\n    numbers. In such cases the logarithm of the calculated\n    probability is stored. This function allows adding probabilities\n    stored in such a fashion. """"""\n    return Tensor._op(Logaddexp, a, b, constant=constant)\n\n\ndef logaddexp2(a, b, constant=False):\n    """""" ``f(a, b) -> log_2(2 ** a + 2 ** b)``\n\n    Utilizes base-2 log.\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    This function is useful\n    in statistics where the calculated probabilities of events may\n    be so small as to exceed the range of normal floating point\n    numbers. In such cases the logarithm of the calculated\n    probability is stored. This function allows adding probabilities\n    stored in such a fashion. """"""\n    return Tensor._op(Logaddexp2, a, b, constant=constant)\n\n\ndef log(a, constant=False):\n    """""" ``f(a) -> log(a)``\n\n    Natural logarithm\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=True)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    This function provides greater precision than ``exp(x) - 1`` for\n    small values of ``x``.""""""\n    return Tensor._op(Log, a, constant=constant)\n\n\ndef log2(a, constant=False):\n    """""" ``f(a) -> log2(a)``\n\n    Base-2 logarithm\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=True)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Log2, a, constant=constant)\n\n\ndef log10(a, constant=False):\n    """""" ``f(a) -> log10(a)``\n\n    Base-10 logarithm\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=True)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Log10, a, constant=constant)\n\n\ndef log1p(a, constant=False):\n    """""" f(a) -> log(1 + a)\n\n    The inverse of ``expm1``.\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=True)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    For real-valued input, log1p is accurate also\n    for ``x`` so small that ``1 + x == 1`` in floating-point\n    accuracy.""""""\n    return Tensor._op(Log1p, a, constant=constant)\n'"
src/mygrad/math/exp_log/ops.py,16,"b'import numpy as np\n\nfrom mygrad.operation_base import BroadcastableOp, Operation\n\n__all__ = [\n    ""Exp"",\n    ""Exp2"",\n    ""Expm1"",\n    ""Log"",\n    ""Log2"",\n    ""Log10"",\n    ""Log1p"",\n    ""Logaddexp"",\n    ""Logaddexp2"",\n]\n\n\nclass Exp(Operation):\n    def __call__(self, a):\n        """""" f(a) -> exp(a)\n\n            Parameters\n            ----------\n            a : mygrad.Tensor\n\n            Returns\n            -------\n            numpy.ndarray""""""\n        self.variables = (a,)\n        return np.exp(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad * np.exp(self.variables[index].data)\n\n\nclass Exp2(Operation):\n    def __call__(self, a):\n        """""" f(a) -> 2^a\n\n            Parameters\n            ----------\n            a : mygrad.Tensor\n\n            Returns\n            -------\n            numpy.ndarray""""""\n        self.variables = (a,)\n        return np.exp2(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad * np.exp2(self.variables[index].data) * np.log(2)\n\n\nclass Expm1(Operation):\n    """""" f(a) -> exp(a) - 1\n\n        This function provides greater precision than exp(x) - 1\n        for small values of x.""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.expm1(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad * np.exp(self.variables[index].data)\n\n\nclass Logaddexp(BroadcastableOp):\n    """"""f(a,b) -> log(exp(a) + exp(b))""""""\n\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        out = np.logaddexp(a.data, b.data)\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        a, b = self.variables\n        if index == 0:\n            return grad / (1 + np.exp(b.data - a.data))\n        elif index == 1:\n            return grad / (1 + np.exp(a.data - b.data))\n        else:  # pragma: no cover\n            raise IndexError(f""Back-propagation through tensor-{index}"")\n\n\nclass Logaddexp2(BroadcastableOp):\n    """"""f(a,b) -> log2(exp(a) + exp(b))""""""\n\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        out = np.logaddexp2(a.data, b.data)\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        a, b = self.variables\n        if index == 0:\n            return grad / (1 + 2 ** (b.data - a.data))\n        elif index == 1:\n            return grad / (1 + 2 ** (a.data - b.data))\n        else:  # pragma: no cover\n            raise IndexError(f""Back-propagation through tensor-{index}"")\n\n\nclass Log(Operation):\n    """""" f(a) -> ln(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.log(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad / self.variables[index].data\n\n\nclass Log2(Operation):\n    def __call__(self, a):\n        """""" f(a) -> log2(a)""""""\n        self.variables = (a,)\n        return np.log2(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad / (self.variables[index].data * np.log(2))\n\n\nclass Log10(Operation):\n    """""" f(a) -> log10(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.log10(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad / (self.variables[index].data * np.log(10))\n\n\nclass Log1p(Operation):\n    """""" f(a) -> ln(1 + a)\n\n        log1p is accurate for x so small that 1 + x == 1 in\n        floating-point accuracy.""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.log1p(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad / (1 + self.variables[index].data)\n'"
src/mygrad/math/hyperbolic_trig/__init__.py,0,b''
src/mygrad/math/hyperbolic_trig/funcs.py,0,"b'from mygrad.tensor_base import Tensor\n\nfrom .ops import *\n\n__all__ = [\n    ""arccosh"",\n    ""arccoth"",\n    ""arccsch"",\n    ""arcsinh"",\n    ""arctanh"",\n    ""cosh"",\n    ""coth"",\n    ""csch"",\n    ""sech"",\n    ""sinh"",\n    ""tanh"",\n]\n\n\ndef arccosh(a, constant=False):\n    """""" ``f(a) -> arccosh(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arccosh, a, constant=constant)\n\n\ndef arccoth(a, constant=False):\n    """""" ``f(a) -> arccoth(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arccoth, a, constant=constant)\n\n\ndef arccsch(a, constant=False):\n    """""" ``f(a) -> arccsch(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arccsch, a, constant=constant)\n\n\ndef arcsinh(a, constant=False):\n    """""" ``f(a) -> arcsinh(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arcsinh, a, constant=constant)\n\n\ndef arctanh(a, constant=False):\n    """""" ``f(a) -> arctanh(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arctanh, a, constant=constant)\n\n\ndef cosh(a, constant=False):\n    """""" ``f(a) -> cosh(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Cosh, a, constant=constant)\n\n\ndef coth(a, constant=False):\n    """""" ``f(a) -> coth(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Coth, a, constant=constant)\n\n\ndef csch(a, constant=False):\n    """""" ``f(a) -> csch(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Csch, a, constant=constant)\n\n\ndef sech(a, constant=False):\n    """""" ``f(a) -> sech(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Sech, a, constant=constant)\n\n\ndef sinh(a, constant=False):\n    """""" ``f(a) -> sinh(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not backpropagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Sinh, a, constant=constant)\n\n\ndef tanh(x, constant=False):\n    """""" ``f(x) -> tanh(x)``\n\n    Parameters\n    ----------\n    x : array_like\n        tanh is applied element-wise to ``x``\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not backpropagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> from mygrad.nnet import softmax, sigmoid, tanh\n    >>> x = mg.linspace(-5, 5, 10)\n    >>> tanh(x)\n    Tensor([-0.9999092 , -0.99916247, -0.99229794, -0.93110961, -0.5046724 ,\n             0.5046724 ,  0.93110961,  0.99229794,  0.99916247,  0.9999092 ])\n    """"""\n    return Tensor._op(Tanh, x, constant=constant)\n'"
src/mygrad/math/hyperbolic_trig/ops.py,20,"b'import numpy as np\n\nfrom mygrad.operation_base import Operation\n\n__all__ = [\n    ""Sinh"",\n    ""Cosh"",\n    ""Tanh"",\n    ""Csch"",\n    ""Sech"",\n    ""Coth"",\n    """" ""Arcsinh"",\n    ""Arccosh"",\n    ""Arctanh"",\n    ""Arccsch"",\n    ""Arccoth"",\n]\n\n\nclass Sinh(Operation):\n    """""" f(a) -> sinh(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.sinh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * np.cosh(a.data)\n\n\nclass Cosh(Operation):\n    """""" f(a) -> cosh(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.cosh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * np.sinh(a.data)\n\n\nclass Tanh(Operation):\n    """""" f(a) -> tanh(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.tanh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * (1 - np.tanh(a.data) ** 2)\n\n\nclass Csch(Operation):\n    """""" f(a) -> csch(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return 1 / np.sinh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * -np.cosh(a.data) / np.sinh(a.data) ** 2\n\n\nclass Sech(Operation):\n    """""" f(a) -> sech(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return 1 / np.cosh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * -np.sinh(a.data) / np.cosh(a.data) ** 2\n\n\nclass Coth(Operation):\n    """""" f(a) -> coth(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return 1 / np.tanh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * -1 / np.sinh(a.data) ** 2\n\n\nclass Arcsinh(Operation):\n    """""" f(a) -> arcsinh(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arcsinh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad / np.sqrt(1 + a.data ** 2)\n\n\nclass Arccosh(Operation):\n    """""" f(a) -> arccosh(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arccosh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad / np.sqrt(a.data ** 2 - 1)\n\n\nclass Arctanh(Operation):\n    """""" f(a) -> arctanh(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arctanh(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad / (1 - a.data ** 2)\n\n\nclass Arccsch(Operation):\n    """""" f(a) -> arccsch(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arcsinh(1 / a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return -grad / (np.abs(a.data) * np.sqrt(1 + a.data ** 2))\n\n\nclass Arccoth(Operation):\n    """""" f(a) -> arccoth(a) """"""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arctanh(1 / a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad / (1 - a.data ** 2)\n'"
src/mygrad/math/misc/__init__.py,0,b''
src/mygrad/math/misc/funcs.py,0,"b'from mygrad.tensor_base import Tensor\n\nfrom .ops import Abs, Cbrt, Maximum, Minimum, Sqrt\n\n__all__ = [""abs"", ""absolute"", ""cbrt"", ""clip"", ""sqrt"", ""maximum"", ""minimum""]\n\n\ndef abs(a, constant=False):\n    """""" ``f(a) -> abs(a)``\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    The derivative at a == 0 returns nan""""""\n    return Tensor._op(Abs, a, constant=constant)\n\n\nabsolute = abs\n\n\ndef sqrt(a, constant=False):\n    """""" ``f(a) -> sqrt(a)``\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not backpropagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Sqrt, a, constant=constant)\n\n\ndef cbrt(a, constant=False):\n    """""" ``f(a) -> cbrt(a)``\n\n    Parameters\n    ----------\n    a : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not backpropagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor""""""\n    return Tensor._op(Cbrt, a, constant=constant)\n\n\ndef maximum(a, b, constant=False):\n    """""" Element-wise maximum of array elements.\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not backpropagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    The gradient does not exist where a == b; we use a\n    value of 0 here.""""""\n    return Tensor._op(Maximum, a, b, constant=constant)\n\n\ndef minimum(a, b, constant=False):\n    """""" Element-wise minimum of array elements.\n\n    Parameters\n    ----------\n    a : array_like\n\n    b : array_like\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not backpropagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    The gradient does not exist where a == b; we use a\n    value of 0 here.""""""\n    return Tensor._op(Minimum, a, b, constant=constant)\n\n\ndef clip(a, a_min, a_max, constant=False):\n    """""" Clip (limit) the values in an array.\n\n    Given an interval, values outside the interval are clipped to\n    the interval edges.  For example, if an interval of ``[0, 1]``\n    is specified, values smaller than 0 become 0, and values larger\n    than 1 become 1.\n\n    Equivalent to `mg.minimum(a_max, mg.maximum(a, a_min))``.\n\n    No check is performed to ensure ``a_min < a_max``.\n\n    This docstring was adapted from that of `numpy.clip`\n\n    Parameters\n    ----------\n    a : array_like\n        Array containing elements to clip.\n\n    a_min : Optional[float, array_like]\n        Minimum value. If `None`, clipping is not performed on lower\n        interval edge. Not more than one of `a_min` and `a_max` may be\n        `None`.\n\n    a_max : Optional[float, array_like]\n        Maximum value. If `None`, clipping is not performed on upper\n        interval edge. Not more than one of `a_min` and `a_max` may be\n        `None`. If `a_min` or `a_max` are array_like, then the three\n        arrays will be broadcasted to match their shapes.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not backpropagate a gradient)\n\n    Returns\n    -------\n    Tensor\n        A tensor with the elements of `a`, but where values\n        < `a_min` are replaced with `a_min`, and those > `a_max`\n        with `a_max`.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> a = mg.arange(10)\n    >>> mg.clip(a, 1, 8)\n    Tensor([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])\n    >>> a\n    Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n    >>> mg.clip(a, [3, 4, 1, 1, 1, 4, 4, 4, 4, 4], 8)\n    Tensor([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])""""""\n    if a_min is None and a_max is None:\n        raise ValueError(""`a_min` and `a_max` cannot both be set to `None`"")\n\n    if a_min is not None:\n        a = maximum(a_min, a, constant=constant)\n\n    if a_max is not None:\n        a = minimum(a_max, a, constant=constant)\n\n    return a\n'"
src/mygrad/math/misc/ops.py,15,"b'import numpy as np\n\nfrom mygrad.operation_base import BroadcastableOp, Operation\n\n__all__ = [""Abs"", ""Sqrt"", ""Cbrt"", ""Maximum"", ""Minimum""]\n\n\nclass Abs(Operation):\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.abs(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * np.piecewise(\n            a.data, [a.data < 0, a.data == 0, a.data > 0], [-1, np.nan, 1]\n        )\n\n\nclass Sqrt(Operation):\n    def __call__(self, a):\n        """""" f(a) = sqrt(a)\n\n            Parameters\n            ----------\n            a : mygrad.Tensor""""""\n        self.variables = (a,)\n        return np.sqrt(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad / (2 * np.sqrt(a.data))\n\n\nclass Cbrt(Operation):\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.cbrt(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad / (3 * np.cbrt(a.data ** 2))\n\n\nclass Maximum(BroadcastableOp):\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        self.greater_than_mask = a.data > b.data\n        self.equal_mask = a.data == b.data\n        return np.where(self.greater_than_mask, a.data, b.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        if index == 0:\n            mask = self.greater_than_mask\n        elif index == 1:\n            mask = np.logical_not(self.greater_than_mask)\n            if mask.ndim:\n                np.logical_not(mask, out=mask, where=self.equal_mask)\n            elif self.equal_mask:\n                mask = np.logical_not(mask)\n        else:  # pragma: no cover\n            raise IndexError(f""Back-propagation through tensor-{index}"")\n\n        return mask * grad\n\n\nclass Minimum(BroadcastableOp):\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        self.less_than_mask = a.data < b.data\n        self.equal_mask = a.data == b.data\n        return np.where(self.less_than_mask, a.data, b.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        if index == 0:\n            mask = self.less_than_mask\n        elif index == 1:\n            mask = np.logical_not(self.less_than_mask)\n            if mask.ndim:\n                np.logical_not(mask, out=mask, where=self.equal_mask)\n            elif self.equal_mask:\n                mask = np.logical_not(mask)\n        else:  # pragma: no cover\n            raise IndexError(f""Back-propagation through tensor-{index}"")\n\n        return mask * grad\n'"
src/mygrad/math/sequential/__init__.py,0,b''
src/mygrad/math/sequential/funcs.py,10,"b'from mygrad.tensor_base import Tensor\n\nfrom .ops import *\n\n__all__ = [\n    ""sum"",\n    ""mean"",\n    ""var"",\n    ""std"",\n    ""amax"",\n    ""amin"",\n    ""max"",\n    ""min"",\n    ""prod"",\n    ""cumprod"",\n    ""cumsum"",\n]\n\n\ndef sum(x, axis=None, keepdims=False, constant=False):\n    """"""\n    Sum of tensor elements over a given axis.\n\n    Parameters\n    ----------\n    x : array_like\n\n    axis : Optional[int, Tuple[ints, ...]]\n        Axis or axes along which a sum is performed.  The default,\n        axis=None, will sum all of the elements of the input tensor.  If\n        axis is negative it counts from the last to the first axis.\n        If axis is a tuple of ints, a sum is performed on all of the axes\n        specified in the tuple instead of a single axis or all the axes as\n        before.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input tensor.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    sum_along_axis : mygrad.Tensor\n        A Tensor with the same shape as `self`, with the specified\n        axis/axes removed. If `self` is a 0-d tensor, or if `axis` is None,\n        a 0-dim Tensor is returned.\n\n    See Also\n    --------\n    mygrad.Tensor.sum : Equivalent method.\n\n    cumsum : Cumulative sum of array elements.\n\n    mean, average\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.\n\n    The sum of an empty tensor is the neutral element 0:\n\n    >>> mygrad.sum([])\n    Tensor(0.0)\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> import numpy as np\n    >>> mg.sum([0.5, 1.5])\n    Tensor(2.0)\n    >>> mg.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n    Tensor(1)\n    >>> mg.sum([[0, 1], [0, 5]])\n    Tensor(6)\n    >>> mg.sum([[0, 1], [0, 5]], axis=0)\n    Tensor([0, 6])\n    >>> mg.sum([[0, 1], [0, 5]], axis=1)\n    Tensor([1, 5])\n\n    If the accumulator is too small, overflow occurs:\n\n    >>> mg.ones(128, dtype=mg.int8).sum(dtype=np.int8)\n    Tensor(-128)\n\n    You can also start the sum with a value other than zero:\n\n    >>> mg.sum([10], initial=5)\n    Tensor(15)\n    """"""\n    return Tensor._op(Sum, x, op_args=(axis, keepdims), constant=constant)\n\n\ndef mean(x, axis=None, keepdims=False, constant=False):\n    """"""\n    Mean of tensor elements over a given axis.\n\n    Parameters\n    ----------\n    x : array_like\n\n    axis : Optional[int, Tuple[ints, ...]\n        Axis or axes along which a mean is performed.  The default,\n        axis=None, will mean all of the elements of the input tensor.  If\n        axis is negative it counts from the last to the first axis.\n\n        If axis is a tuple of ints, a mean is performed on all of the axes\n        specified in the tuple instead of a single axis or all the axes as\n        before.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input tensor.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mean_along_axis : Tensor\n        A Tensor with the same shape as `self`, with the specified\n        axis/axes removed. If `self` is a 0-d tensor, or if `axis` is None,\n        a 0-dim Tensor is returned.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> import numpy as np\n    >>> a = mg.Tensor([[1, 2],\n    ...                [3, 4]])\n    >>> mg.mean(a)\n    Tensor(2.5)\n    >>> mg.mean(a, axis=0)\n    Tensor([ 2.,  3.])\n    >>> mg.mean(a, axis=1)\n    Tensor([ 1.5,  3.5])\n\n    In single precision, `mean` can be inaccurate:\n\n    >>> a = mg.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> mg.mean(a)\n    Tensor(0.54999924)\n\n    Computing the mean in float64 is more accurate:\n\n    >>> mg.mean(a, dtype=np.float64)\n    Tensor(0.55000000074505806)\n    """"""\n    return Tensor._op(Mean, x, op_args=(axis, keepdims), constant=constant)\n\n\ndef var(x, axis=None, ddof=0, keepdims=False, constant=False):\n    """"""\n    Compute the variance along the specified axis.\n\n    Returns the variance of the array elements, a measure of the spread of a\n    distribution.  The variance is computed for the flattened array by\n    default, otherwise over the specified axis.\n\n    Parameters\n    ----------\n    x : array_like\n        Array containing numbers whose variance is desired.\n\n    axis : Optional[int, Tuple[int, ...]]\n        Axis or axes along which the variance is computed.  The default is to\n        compute the variance of the flattened array.\n\n    ddof : int, optional (default=0)\n        ""Delta Degrees of Freedom"": the divisor used in the calculation is\n        ``N - ddof``, where ``N`` represents the number of elements. By\n        default `ddof` is zero.\n\n    keepdims : bool, optional (default=False)\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array..\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    variance : mygrad.Tensor\n\n    Notes\n    -----\n    The variance is the average of the squared deviations from the mean,\n    i.e.,  ``var = mean(abs(x - x.mean())**2)``.\n\n    The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.\n    If, however, `ddof` is specified, the divisor ``N - ddof`` is used\n    instead.  In standard statistical practice, ``ddof=1`` provides an\n    unbiased estimator of the variance of a hypothetical infinite population.\n    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n    normally distributed variables.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> import numpy as np\n    >>> a = mg.Tensor([[1, 2],\n    ...                [3, 4]])\n    >>> mg.var(a)\n    Tensor(1.25)\n    >>> mg.var(a, axis=0)\n    Tensor([ 1.,  1.])\n    >>> mg.var(a, axis=1)\n    Tensor([ 0.25,  0.25])\n\n    In single precision, ``var()`` can be inaccurate:\n\n    >>> a = mg.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> mg.var(a)\n    Tensor(0.20250003)\n\n    Computing the variance in float64 is more accurate:\n\n    >>> mg.var(a, dtype=np.float64)\n    Tensor(0.20249999932944759)\n    >>> ((1-0.55)**2 + (0.1-0.55)**2)/2\n    Tensor(0.2025)\n    """"""\n    return Tensor._op(\n        Variance,\n        x,\n        op_kwargs=dict(axis=axis, keepdims=keepdims, ddof=ddof),\n        constant=constant,\n    )\n\n\ndef std(x, axis=None, ddof=0, keepdims=False, constant=False):\n    """"""\n    Compute the standard deviation along the specified axis.\n\n    Returns the variance of the array elements, a measure of the spread of a\n    distribution.  The variance is computed for the flattened array by\n    default, otherwise over the specified axis.\n\n    Parameters\n    ----------\n    x : array_like\n        Array containing numbers whose standard deviation is desired.\n\n    axis : Optional[int, Tuple[int, ...]]\n        Axis or axes along which the variance is computed.  The default is to\n        compute the variance of the flattened array.\n\n    ddof : int, optional (default=0)\n        ""Delta Degrees of Freedom"": the divisor used in the calculation is\n        ``N - ddof``, where ``N`` represents the number of elements. By\n        default `ddof` is zero.\n\n    keepdims : bool, optional (default=False)\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the input array.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    std : mygrad.Tensor\n\n    Notes\n    -----\n    The variance is the average of the squared deviations from the mean,\n    i.e.,  ``var = mean(abs(x - x.mean())**2)``.\n\n    The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``.\n    If, however, `ddof` is specified, the divisor ``N - ddof`` is used\n    instead.  In standard statistical practice, ``ddof=1`` provides an\n    unbiased estimator of the variance of a hypothetical infinite population.\n    ``ddof=0`` provides a maximum likelihood estimate of the variance for\n    normally distributed variables.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> import numpy as np\n    >>> a = mg.Tensor([[1, 2],\n    ...                [3, 4]])\n    >>> mg.std(a)\n    Tensor(1.1180339887498949)\n    >>> mg.std(a, axis=0)\n    Tensor([ 1.,  1.])\n    >>> mg.std(a, axis=1)\n    Tensor([ 0.5,  0.5])\n\n    In single precision, ``var()`` can be inaccurate:\n\n    >>> a = mg.zeros((2, 512*512), dtype=np.float32)\n    >>> a[0, :] = 1.0\n    >>> a[1, :] = 0.1\n    >>> mg.std(a)\n    Tensor(0.45000005)\n\n    Computing the variance in float64 is more accurate:\n\n    >>> mg.std(a, dtype=np.float64)\n    Tensor(0.44999999925494177)\n    """"""\n    return Tensor._op(\n        StdDev,\n        x,\n        op_kwargs=dict(axis=axis, keepdims=keepdims, ddof=ddof),\n        constant=constant,\n    )\n\n\ndef max(x, axis=None, keepdims=False, constant=False):\n    """"""\n    Return the maximum of a tensor or maximum along its axes.\n\n    Parameters\n    ----------\n    x : array_like\n\n    axis : Optional[int, Tuple[int, ...]]\n        Axis or axes along which to operate. By default, flattened input is used.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original `arr`.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    max : mygrad.Tensor\n        Maximum of `a`. If `axis` is None, the result is a 0-D tensor.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> import numpy as np\n    >>> a = mg.arange(4).reshape((2,2))\n    >>> a\n    Tensor([[0, 1],\n            [2, 3]])\n    >>> mg.amax(a)           # Maximum of the flattened array\n    Tensor(3)\n    >>> mg.amax(a, axis=0)   # Maxima along the first axis\n    Tensor([2, 3])\n    >>> mg.amax(a, axis=1)   # Maxima along the second axis\n    Tensor([1, 3])\n    >>> b = mg.arange(5, dtype=float)\n    >>> b[2] = np.NaN\n    >>> mg.amax(b)\n    Tensor(nan)\n    """"""\n    return Tensor._op(\n        MaxMin,\n        x,\n        op_kwargs=dict(axis=axis, keepdims=keepdims, maxmin=""max""),\n        constant=constant,\n    )\n\n\ndef min(x, axis=None, keepdims=False, constant=False):\n    """"""\n    Return the minimum of a tensor or minimum along its axes.\n\n    Parameters\n    ----------\n    axis : Optional[int, Tuple[int, ...]]\n        Axis or axes along which to operate. By default, flattened input is used.\n\n    keepdims : bool, optional\n        If this is set to True, the axes which are reduced are left\n        in the result as dimensions with size one. With this option,\n        the result will broadcast correctly against the original `arr`.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    min : mygrad.Tensor\n        Minimum of `a`. If `axis` is None, the result is a 0-D tensor.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> import numpy as np\n    >>> a = mg.arange(4).reshape((2,2))\n    >>> a\n    Tensor([[0, 1],\n            [2, 3]])\n    >>> mg.amin(a)           # Minimum of the flattened array\n    Tensor(0)\n    >>> mg.amin(a, axis=0)   # Minima along the first axis\n    Tensor([0, 1])\n    >>> mg.amin(a, axis=1)   # Minima along the second axis\n    Tensor([0, 2])\n    >>> b = mg.arange(5, dtype=float)\n    >>> b[2] = np.NaN\n    >>> mg.amin(b)\n    Tensor(nan)\n    """"""\n    return Tensor._op(\n        MaxMin,\n        x,\n        op_kwargs=dict(axis=axis, keepdims=keepdims, maxmin=""min""),\n        constant=constant,\n    )\n\n\n# aliases\namin = min\namax = max\n\n\ndef prod(a, axis=None, keepdims=False, constant=False):\n    """"""\n    Return the product of array elements over given axes.\n\n    Parameters\n    ----------\n    a : array_like\n        Input data.\n\n    axis : Optional[int, Tuple[int, ...]]\n        Axis or axes along which to operate. By default, flattened input is used.\n\n    keepdims : bool, optional (default=False)\n        If this is set to True, the axes which are reduced are left in the\n        result as dimensions with size one. With this option, the result\n        will broadcast correctly against the input array.\n\n    Returns\n    -------\n    product_along_axis : mygrad.Tensor\n        A tensor shaped as `a` but with the specified axis removed.\n\n    Notes\n    -----\n    The product of an empty tensor is the neutral element 1:\n\n    >>> import mygrad\n    >>> mygrad.prod([])\n    Tensor(1.0)\n\n    Examples\n    --------\n    By default, calculate the product of all elements:\n\n    >>> import mygrad as mg\n    >>> mg.prod([1.,2.])\n    Tensor(2.0)\n\n    Even when the input array is two-dimensional:\n\n    >>> mg.prod([[1.,2.],\n    ...          [3.,4.]])\n    Tensor(24.0)\n\n    But we can also specify the axis over which to multiply:\n\n    >>> mg.prod([[1.,2.],\n    ...          [3.,4.]], axis=1)\n    Tensor([  2.,  12.]) """"""\n    return Tensor._op(\n        Prod, a, op_kwargs=dict(axis=axis, keepdims=keepdims), constant=constant\n    )\n\n\ndef cumprod(a, axis=None, constant=False):\n    """"""\n    Return the cumulative product of elements along a given axis.\n\n    This docstring was adapted from the official numpy documentation\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    axis : Optional[int]\n        Axis along which the cumulative product is computed.  By default\n        the input is flattened.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    Arithmetic is modular when using integer types, and no error is\n    raised on overflow.\n\n    Examples\n    --------\n    >>> from mygrad import cumprod, Tensor\n    >>> a = Tensor([[1, 2, 3],\n    ...             [4, 5, 6]])\n\n    >>> cumprod(a)\n    Tensor([  1   2   6  24 120 720])\n\n    The cumulative product for each column (i.e., over the rows) of `a`:\n\n    >>> cumprod(a, axis=0)\n    Tensor([[ 1,  2,  3],\n           [ 4, 10, 18]])\n\n    The cumulative product for each row (i.e. over the columns) of `a`:\n\n    >>> cumprod(a, axis=1)\n    Tensor([[  1,   2,   6],\n            [  4,  20, 120]])""""""\n\n    return Tensor._op(CumProd, a, op_kwargs=dict(axis=axis), constant=constant)\n\n\ndef cumsum(a, axis=None, constant=False):\n    """"""\n    Return the cumulative sum of the elements along a given axis.\n\n    This docstring was adapted from the official numpy documentation\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n\n    axis : int, optional\n        Axis along which the cumulative sum is computed. The default\n        (None) is to compute the cumsum over the flattened array.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Examples\n    --------\n    >>> from mygrad import cumsum, Tensor\n    >>> a = Tensor([[1, 2, 3],\n    ...             [4, 5, 6]])\n    >>> cumsum(a)\n    Tensor([ 1,  3,  6, 10, 15, 21])\n\n    >>> cumsum(a, axis=0)      # sum over rows for each of the 3 columns\n    Tensor([[1, 2, 3],\n            [5, 7, 9]])\n    >>> cumsum(a, axis=1)      # sum over columns for each of the 2 rows\n    Tensor([[ 1,  3,  6],\n            [ 4,  9, 15]])\n    """"""\n\n    return Tensor._op(CumSum, a, op_kwargs=dict(axis=axis), constant=constant)\n'"
src/mygrad/math/sequential/ops.py,46,"b'from collections.abc import Sequence\nfrom functools import reduce\nfrom typing import Any\n\nimport numpy as np\n\nfrom mygrad.operation_base import Operation\n\n__all__ = [""MaxMin"", ""Sum"", ""Mean"", ""Prod"", ""CumProd"", ""CumSum"", ""Variance"", ""StdDev""]\n\n\nclass MaxMin(Operation):\n    def __call__(self, a, axis=None, keepdims=False, maxmin=None):\n        """""" Return the maximum (minimum) of a tensor, or along its axes.\n\n            Parameters\n            ----------\n            a : pygrad.Tensor\n                Input data.\n\n            axis : Optional[int, Tuple[int, ...]]\n                Axis or axes along which to operate. By default, flattened input is used.\n\n            keepdims : bool, optional\n                If this is set to True, the axes which are reduced are left\n                in the result as dimensions with size one. With this option,\n                the result will broadcast correctly against the original `arr`.\n\n            maxmin : str\n                \'max\' or \'min\'. Selects the operation that is performed\n\n            Returns\n            -------\n            amax : ndarray\n                Maximum (minimum) of `a`. If `axis` is None, the result is a 0-D array.""""""\n        assert maxmin in (""max"", ""min""), ""Invalid keyword argument""\n        op = np.argmax if maxmin == ""max"" else np.argmin\n\n        # let numpy handle error checking\n        np.amax(np.empty([1] * a.ndim), axis=axis, keepdims=keepdims)\n\n        self.variables = (a,)\n\n        if a.ndim == 0:\n            return a.data\n\n        if hasattr(axis, ""__iter__""):\n            assert isinstance(axis, tuple)\n            axis = tuple(ax % a.ndim for ax in axis)\n            axis = None if len(axis) == a.ndim else tuple(sorted(axis))\n        elif axis is not None:\n            axis = (axis % a.ndim,)\n\n        self.axis = axis\n        self.keepdims = keepdims\n\n        # max(a) -> use argmax\n        if self.axis is None:\n            self.indices = np.unravel_index(op(a.data), a.shape)\n            dat = a.data[self.indices]\n\n        # max(x, axis=i) -> use argmax with specified axis\n        elif len(self.axis) == 1:  #\n            op_index = op(a.data, axis=self.axis[0])\n            self.indices = list(np.indices(op_index.shape))\n            self.indices.insert(self.axis[0], op_index)\n            self.indices = tuple(self.indices)\n            dat = a.data[self.indices]\n\n        # max(x, axis=(i,j,...) ) -> Reshape data to use argmax along trailing axis\n        else:\n            self.static_ax = tuple(\n                sorted(set(range(a.ndim)) - set(self.axis))\n            )  # non-reduced axes (m, n, ..)\n            self.to_trans = self.static_ax + self.axis  # (m, n, ..., i, j, ...)\n            self.from_trans = tuple(np.argsort(self.to_trans))\n            outshape = tuple(a.shape[i] for i in self.static_ax)\n\n            z = a.data.transpose(*self.to_trans).reshape(\n                *outshape, -1\n            )  # (m, n, ..., i*j*[...])\n\n            k = op(z, axis=-1)\n            self.indices = tuple(i for i in np.indices(k.shape))\n            self.indices += (k,)\n            self.tmp_grad_shape = z.shape\n            z = z[self.indices]\n\n            dat = z.reshape(outshape)  # (m, n, ...)\n\n        if not self.keepdims:\n            return dat\n\n        elif self.axis is None:\n            keep_index = (np.newaxis,) * a.ndim\n        else:\n            keep_index = [slice(None)] * a.ndim\n            for i in self.axis:\n                keep_index[i] = np.newaxis\n            keep_index = tuple(keep_index)\n\n        return np.asarray(dat)[keep_index]\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        if a.ndim == 0:\n            return grad\n\n        # normalize shape of grad to be same as when keepdims=False\n        if self.keepdims:\n            if self.axis is not None:\n                reduce = [slice(None)] * a.ndim\n                for i in self.axis:\n                    reduce[i] = 0\n                reduce = tuple(reduce)\n            else:\n                reduce = (0,) * a.ndim\n            grad = grad[reduce]\n\n        # use argmax indices to broadcast grad to correct elements\n        if self.axis is None or len(self.axis) == 1:\n            out = np.zeros_like(a.data, dtype=float)\n            out[self.indices] = grad\n            return out\n        else:\n            out = np.zeros(self.tmp_grad_shape, dtype=float)\n            out[self.indices] = grad\n            shape = tuple(a.shape[i] for i in self.to_trans)\n            return out.reshape(shape).transpose(*self.from_trans)\n\n\nclass Sum(Operation):\n    def __call__(self, a, axis=None, keepdims=False):\n        """""" Parameters\n            ----------\n            a : mygrad.Tensor""""""\n        self.variables = (a,)\n\n        if axis is not None and not hasattr(axis, ""__iter__""):\n            axis = (axis,)\n        self.axis = axis\n\n        self.keepdims = keepdims\n        out = a.data.sum(axis=axis, keepdims=keepdims)\n        self.outshape = out.shape if isinstance(out, np.ndarray) else None\n        return out\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        if self.outshape is None:\n            return np.full(a.shape, grad, dtype=float)\n\n        if not self.keepdims:\n            index = [slice(None) for i in range(a.ndim)]\n            for i in self.axis:\n                index[i] = np.newaxis\n            grad = grad[tuple(index)]\n        return np.broadcast_to(grad, a.data.shape).astype(float)\n\n\nclass Mean(Sum):\n    def __call__(self, a, axis=None, keepdims=False):\n        out = super().__call__(a, axis, keepdims)\n        self.n = (\n            a.data.size\n            if self.axis is None\n            else np.prod([a.shape[i] for i in self.axis])\n        )\n        return out / self.n\n\n    def backward_var(self, grad, index, **kwargs):\n        return super().backward_var(grad / self.n, index, **kwargs)\n\n\nclass Prod(Operation):\n    def __call__(self, a, axis=None, keepdims=False):\n        """""" Parameters\n            ----------\n            a : mygrad.Tensor""""""\n        self.variables = (a,)\n        if axis is not None and not hasattr(axis, ""__iter__""):\n            axis = (axis,)\n        self.axis = axis\n        self.keepdims = keepdims\n        return a.data.prod(axis=axis, keepdims=keepdims)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        x = a.data\n        grad = np.asarray(grad)\n\n        if a.ndim == 0:\n            return grad\n\n        axes = (\n            set(range(a.ndim))\n            if self.axis is None\n            else {i if i >= 0 else a.ndim + i for i in self.axis}\n        )\n\n        # make grad broadcast-compatible against x\n        grad = grad.reshape(*(1 if n in axes else i for n, i in enumerate(a.shape)))\n\n        # This is a valid method for taking the derivative\n        # of prod(x) only if there are no zeros in `x`.\n        # If there are zeros we need to patch some of the nans\n        # that we just created, with the correct derivative.\n        with np.errstate(divide=""ignore"", invalid=""ignore""):\n            dldx = np.prod(x, axis=self.axis, keepdims=True) / x\n\n        # if two or more zeros occur within a given sequence, then\n        # the nans in the sequences can simply be set to 0\n        if np.any(np.isnan(dldx)):\n            x = x.copy()\n\n            # computes the number of 0s to occur within each sequence\n            has_zero = np.broadcast_to(\n                np.sum(x == 0, axis=self.axis, keepdims=True), x.shape\n            )\n            dldx[has_zero > 1] = np.nan_to_num(dldx[has_zero > 1])\n\n            # if only a single 0 occurs within a given sequence, the\n            # derivative needs to be recomputed at that location by\n            # setting that element 0 -> 1\n            if np.any(np.isnan(dldx)):\n                is_zero = x == 0\n                x[is_zero] = 1\n                with np.errstate(divide=""ignore"", invalid=""ignore""):\n                    loc = np.logical_and(is_zero, has_zero == 1)\n                    dldx[loc] = (np.prod(x, axis=self.axis, keepdims=True) / x)[loc]\n\n        return grad * dldx\n\n\ndef _reverse_cumsum(x, axis=None):  # pragma: no cover\n    """""" (x0, x1, x2) -> (x0, x0 + x1, x0 + x1 + x2)""""""\n    if axis is None:\n        axis = 0\n    return np.flip(np.cumsum(np.flip(x, axis=axis), axis=axis), axis=axis)\n\n\ndef _find_first_zeros_along_axis(x, axis):  # pragma: no cover\n    """""" Return the indices at which 0 first occurs in `x` as viewed\n        along the specified axis\n\n        Parameters\n        ----------\n        x : numpy.ndarray\n        axis : Union[None, int]\n            The axis along which zeros are looked for. If\n            `None`, then `x` must be a flat-array\n\n        Returns\n        -------\n        Tuple[Tuple[int, ...], ...]\n            x.ndim tuple-entries, specifying the corresponding\n            positions where the first 0 is encountered along the\n            given axis.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> x = np.array([[1, 1, 0],\n                          [1, 1, 0],\n                          [1, 1, 0]])\n\n        All of the zeros fall along a single column, thus\n        only one is ""encountered"" when looking across rows\n        within each column for a single zero. It is located\n        at: row-0, col-2\n\n        >>> _find_first_zeros_along_axis(x, axis=0)\n        ((0,), (2,))\n\n        Looking along the columns within each row,\n        each of the three zeros are ""found"", they are\n        located at: row-0, col=2\n                    row-1, col=2\n                    row-2, col=2\n\n        >>> _find_first_zeros_along_axis(x, axis=1)\n        ((0, 1, 2), (2, 2, 2))""""""\n\n    def add_to_seen(seen, inds):\n        if inds[1:] not in (i[1:] for i in seen):\n            seen.append(inds)\n        return seen\n\n    def move(seq, origin, dest):\n        if origin == dest:\n            return seq\n        o = seq.pop(origin)\n        seq.insert(dest, o)\n        return seq\n\n    wer = np.where((np.moveaxis(x, axis, 0) if axis is not None else x) == 0)\n\n    if axis is None:\n        axis = 0\n\n    gen_inds = (\n        move(list(seq), origin=0, dest=axis)\n        for seq in reduce(add_to_seen, zip(*wer), [])\n    )\n    return tuple(zip(*gen_inds))\n\n\nclass CumProd(Operation):\n    def __call__(self, a, axis=None):\n        self.variables = (a,)\n        self.axis = axis\n        return np.cumprod(a.data, axis)\n\n    def backward_var(self, grad, index, **kwargs):\n        x = self.variables[index].data\n        axis = self.axis\n        g = grad\n\n        if axis is None:\n            orig_shape = x.shape\n            x = x.flat\n            g = g.flat\n        else:\n            orig_shape = None\n            if axis < 0:\n                axis += x.ndim\n\n        g_cumprod = g * np.cumprod(x, axis=axis)\n\n        # This is a valid method for taking the derivative\n        # of cumprod(x) only if there are no zeros in `x`.\n        # If there are zeros we need to patch some of the nans\n        # that we just created, with the correct derivative.\n        with np.errstate(divide=""ignore"", invalid=""ignore""):\n            # assuming x0, ..., xn are all non-zero\n            #\n            # dldx = [g0 + g1*x1 + g2*x1*x2 + ...,\n            #              g1*x0 + g2*x0*x2 + ...,\n            #                      g2*x0*x1 + ...,\n            #                               + ...]\n            dldx = _reverse_cumsum(g_cumprod, axis=axis) / x\n\n        # Only the first occurrences of 0 along the specified\n        # axis in x need have its correct derivative computed\n        # instead of being nan. All other nans in `dldx` can be\n        # safely set to zero since they fall ""downstream"" from\n        # a 0 in the cumulative-product and the derivatives of\n        # all such elements are 0.\n        # See `_find_first_zeros_along_axis` for more details\n        if np.any(np.isnan(dldx)):\n            x = x.copy()\n            locs = _find_first_zeros_along_axis(x, axis=axis)\n            x[locs] = 1\n\n            g_cumprod = g * np.cumprod(x, axis=axis)\n            with np.errstate(divide=""ignore"", invalid=""ignore""):\n                dldx[locs] = (_reverse_cumsum(g_cumprod, axis=axis) / x)[locs]\n            np.nan_to_num(dldx, copy=False)\n\n        if axis is None:\n            dldx.shape = orig_shape\n        return dldx\n\n\nclass CumSum(Operation):\n    def __call__(self, a, axis=None):\n        self.variables = (a,)\n        self.axis = axis\n        return np.cumsum(a.data, axis)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        g = _reverse_cumsum(grad, self.axis)\n        if self.axis is None:\n            g.shape = a.shape\n        return g\n\n\nclass Variance(Operation):\n    _method_name = ""var""\n\n    def _grad_preprocess(self, grad: Any) -> np.ndarray:\n        """"""Helper method provided so that `Variance` and `StdDev` can\n        share the same implementation for `backward_var`.""""""\n        return np.asarray(grad)\n\n    def __call__(self, a, axis=None, keepdims=False, ddof=0):\n        self.variables = (a,)\n\n        if axis is not None and not hasattr(axis, ""__iter__""):\n            axis = (axis,)\n\n        self.kwargs = dict(axis=axis, keepdims=keepdims, ddof=ddof)\n        return getattr(a.data, self._method_name)(**self.kwargs)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        if isinstance(self.kwargs[""axis""], Sequence) and len(self.kwargs[""axis""]) == 0:\n            return np.zeros(a.shape, dtype=float)\n\n        N = (\n            a.size\n            if self.kwargs[""axis""] is None\n            else np.prod([a.shape[i] for i in self.kwargs[""axis""]])\n        )\n        N -= self.kwargs[""ddof""]\n\n        grad = self._grad_preprocess(grad)\n        if grad.ndim == 0:\n            grad = np.full(a.shape, grad, dtype=float)\n        else:\n            if not self.kwargs[""keepdims""]:\n                index = [slice(None)] * a.ndim\n                for i in self.kwargs[""axis""]:\n                    index[i] = np.newaxis\n                grad = grad[tuple(index)]\n        back = (2.0 / N) * (\n            a.data - a.data.mean(axis=self.kwargs[""axis""], keepdims=True)\n        )\n        return back * grad\n\n\nclass StdDev(Variance):\n    _method_name = ""std""\n\n    def _grad_preprocess(self, grad: Any) -> np.ndarray:\n        """"""Helper method provided so that `Variance` and `StdDev` can\n        share the same implementation for `backward_var`.\n\n        Includes backpropagation through the sqrt after the variance.""""""\n        a = self.variables[0]\n        return np.asarray(grad) / (2 * np.sqrt(a.data.var(**self.kwargs)))\n'"
src/mygrad/math/trigonometric/__init__.py,0,b''
src/mygrad/math/trigonometric/funcs.py,0,"b'from mygrad.tensor_base import Tensor\n\nfrom .ops import *\n\n__all__ = [\n    ""sin"",\n    ""sinc"",\n    ""cos"",\n    ""tan"",\n    ""cot"",\n    ""csc"",\n    ""sec"",\n    ""arccos"",\n    ""arccsc"",\n    ""arcsin"",\n    ""arctan"",\n    ""arcsec"",\n    ""arccot"",\n    ""arctan2"",\n]\n\n\ndef sin(a, constant=False):\n    """""" ``f(a) -> sin(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Sin, a, constant=constant)\n\n\ndef sinc(a, constant=False):\n    """""" ``f(a) -> sin(a) / a``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Sinc, a, constant=constant)\n\n\ndef cos(a, constant=False):\n    """""" ``f(a) -> cos(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Cos, a, constant=constant)\n\n\ndef tan(a, constant=False):\n    """""" ``f(a) -> tan(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Tan, a, constant=constant)\n\n\ndef cot(a, constant=False):\n    """""" ``f(a) -> cot(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Cot, a, constant=constant)\n\n\ndef csc(a, constant=False):\n    """""" ``f(a) -> csc(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Csc, a, constant=constant)\n\n\ndef sec(a, constant=False):\n    """""" ``f(a) -> sec(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Sec, a, constant=constant)\n\n\ndef arccos(a, constant=False):\n    """""" ``f(a) -> arccos(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arccos, a, constant=constant)\n\n\ndef arccsc(a, constant=False):\n    """""" ``f(a) -> arccsc(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arccsc, a, constant=constant)\n\n\ndef arccot(a, constant=False):\n    """""" ``f(a) -> arccot(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arccot, a, constant=constant)\n\n\ndef arcsin(a, constant=False):\n    """""" ``f(a) -> arctanh(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arcsin, a, constant=constant)\n\n\ndef arctan(a, constant=False):\n    """""" ``f(a) -> arctan(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arctan, a, constant=constant)\n\n\ndef arcsec(a, constant=False):\n    """""" ``f(a) -> arcsec(a)``\n\n        Parameters\n        ----------\n        a : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arcsec, a, constant=constant)\n\n\ndef arctan2(a, b, constant=False):\n    """""" ``f(a, b) -> arctan(a/b)``\n\n        Parameters\n        ----------\n        a : array_like\n        b : array_like\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor""""""\n    return Tensor._op(Arctan2, a, b, constant=constant)\n'"
src/mygrad/math/trigonometric/ops.py,30,"b'import numpy as np\n\nfrom mygrad.operation_base import BroadcastableOp, Operation\n\n__all__ = [\n    ""Sin"",\n    ""Sinc"",\n    ""Cos"",\n    ""Tan"",\n    ""Csc"",\n    ""Sec"",\n    ""Cot"",\n    ""Arcsin"",\n    ""Arccos"",\n    ""Arctan"",\n    ""Arccsc"",\n    ""Arcsec"",\n    ""Arccot"",\n    ""Arctan2"",\n]\n\n\nclass Sin(Operation):\n    """""" f(a) -> sin(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.sin(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * np.cos(a.data)\n\n\ndef _dsinc(x):\n    x = x * np.pi\n    return (x * np.cos(x) - np.sin(x)) / x ** 2\n\n\nclass Sinc(Operation):\n    """""" f(a) -> sin(pi*a)/(pi*a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.sinc(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        x = a.data\n        return np.pi * grad * np.piecewise(x, [x == 0, x != 0], [np.zeros_like, _dsinc])\n\n\nclass Cos(Operation):\n    """""" f(a) -> cos(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.cos(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * -np.sin(a.data)\n\n\nclass Tan(Operation):\n    """""" f(a) -> tan(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.tan(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad / np.cos(a.data) ** 2\n\n\nclass Csc(Operation):\n    """""" f(a) -> csc(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return 1 / np.sin(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * -np.cos(a.data) / np.sin(a.data) ** 2\n\n\nclass Sec(Operation):\n    """""" f(a) -> sec(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return 1 / np.cos(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad * np.sin(a.data) / np.cos(a.data) ** 2\n\n\nclass Cot(Operation):\n    """""" f(a) -> cot(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return 1 / np.tan(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return -grad / np.sin(a.data) ** 2\n\n\nclass Arcsin(Operation):\n    """""" f(a) -> arcsin(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arcsin(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        # d arcsin / dx at x = -1, 1 returns 0, not NaN\n        a = self.variables[index]\n        return np.select([np.abs(a.data) != 1], [grad / np.sqrt(1 - a.data ** 2)])\n\n\nclass Arccos(Operation):\n    """""" f(a) -> arccos(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arccos(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        # d arccos / dx at x = -1, 1 returns 0, not NaN\n        a = self.variables[index]\n        return np.select([np.abs(a.data) != 1], [-grad / np.sqrt(1 - a.data ** 2)])\n\n\nclass Arctan(Operation):\n    """""" f(a) -> arctan(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arctan(a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad / (1 + a.data ** 2)\n\n\nclass Arccsc(Operation):\n    """""" f(a) -> arccsc(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arcsin(1 / a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        # d arccsc / dx at x = -1, 1 returns 0, not NaN\n        a = self.variables[index]\n        return np.select(\n            [np.abs(a.data) != 1], [-grad / (np.abs(a.data) * np.sqrt(a.data ** 2 - 1))]\n        )\n\n\nclass Arcsec(Operation):\n    """""" f(a) -> arcsec(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.arccos(1 / a.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        # d arcsec / dx at x = -1, 1 returns 0, not NaN\n        a = self.variables[index]\n        return np.select(\n            [np.abs(a.data) != 1], [grad / (np.abs(a.data) * np.sqrt(a.data ** 2 - 1))]\n        )\n\n\nclass Arccot(Operation):\n    """""" f(a) -> arccot(a)""""""\n\n    def __call__(self, a):\n        self.variables = (a,)\n        return np.piecewise(\n            a.data, [a.data == 0, a.data != 0], [np.pi / 2, lambda x: np.arctan(1 / x)]\n        )\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return -grad / (1 + a.data ** 2)\n\n\nclass Arctan2(BroadcastableOp):\n    """""" f(a, b) -> arctan(a/b)""""""\n\n    def __call__(self, a, b):\n        self.variables = (a, b)\n        return np.arctan2(a.data, b.data)\n\n    def backward_var(self, grad, index, **kwargs):\n        a, b = self.variables\n        if index == 0:\n            return grad * b.data / (a.data ** 2 + b.data ** 2)\n        else:\n            return -1.0 * grad * a.data / (a.data ** 2 + b.data ** 2)\n'"
src/mygrad/nnet/activations/__init__.py,0,"b'from mygrad import tanh\n\nfrom .hard_tanh import hard_tanh\nfrom .relu import relu\nfrom .sigmoid import sigmoid\nfrom .softmax import logsoftmax, softmax\n\n__all__ = [""hard_tanh"", ""logsoftmax"", ""relu"", ""sigmoid"", ""softmax"", ""tanh""]\n'"
src/mygrad/nnet/activations/hard_tanh.py,0,"b'from numbers import Real\n\nfrom numpy import ndarray\n\nfrom mygrad.math.misc.funcs import maximum, minimum\nfrom mygrad.tensor_base import Tensor\n\n__all__ = [""hard_tanh""]\n\n\ndef hard_tanh(x, *, lower_bound=-1, upper_bound=1, constant=False):\n    """""" Returns the hard hyperbolic tangent function.\n\n    The hard_tanh function is `lower_bound` where `x` <= `lower_bound`, `upper_bound` where\n    `x` >= `upper_bound`, and `x` where `lower_bound` < `x` < `upper_bound`.\n\n    Parameters\n    ----------\n    x : array_like\n        The input, to which to apply the hard tanh function.\n\n    lower_bound : Real, optional (default=-1)\n        The lower bound on the hard tanh.\n\n    upper_bound : Real, optional (default=1)\n        The upper bound on the hard tanh.\n\n    constant : boolean, optional (default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient).\n\n    Returns\n    -------\n    mygrad.Tensor\n        The result of applying the ""hard-tanh"" function elementwise to `x`.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> from mygrad.nnet.activations import hard_tanh\n    >>> x = mg.arange(-5, 6)\n    >>> x\n    Tensor([-5, -4, -3, -2, -1,  0,  1,  2,  3,  4,  5])\n    >>> y = hard_tanh(x, lower_bound=-3, upper_bound=3); y\n    Tensor([-3, -3, -3, -2, -1,  0,  1,  2,  3,  3,  3])\n    >>> y.backward()\n    >>> x.grad\n    array([0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.])\n    """"""\n    if isinstance(lower_bound, (ndarray, Tensor)):\n        lower_bound = lower_bound.item()\n\n    if isinstance(upper_bound, (ndarray, Tensor)):\n        upper_bound = upper_bound.item()\n\n    if not isinstance(lower_bound, Real):\n        raise TypeError(\n            f""`lower_bound` must be a real-valued scalar, got {lower_bound} (type { type(lower_bound)})""\n        )\n\n    if not isinstance(upper_bound, Real):\n        raise TypeError(\n            f""`upper_bound` must be a real-valued scalar, got {upper_bound} (type {type(upper_bound)})""\n        )\n\n    return maximum(\n        lower_bound, minimum(x, upper_bound, constant=constant), constant=constant\n    )\n'"
src/mygrad/nnet/activations/relu.py,1,"b'import numpy as np\n\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\n\nclass ReLu(Operation):\n    def __call__(self, a):\n        self.variables = (a,)\n        self.back = np.asarray(a > 0, dtype=a.dtype)\n        return a.data * self.back\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad * self.back\n\n\ndef relu(x, constant=False):\n    """"""\n    Applies the recitfied linear unit activation function::\n\n        f(x) = {x, x > 0\n                0, x <= 0 }\n\n    Parameters\n    ----------\n    x : array_like\n        relu is applied element-wise on ``x``.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> from mygrad.nnet import relu\n    >>> x = mg.linspace(-5, 5, 5)\n    >>> x\n    Tensor([-5. , -2.5,  0. ,  2.5,  5. ])\n    >>> relu(x)\n    Tensor([-0. , -0. ,  0. ,  2.5,  5. ])\n    >>> relu(x).backward()\n    >>> x.grad  # d(relu(x))/dx\n    array([0., 0., 0., 1., 1.])\n    """"""\n    return Tensor._op(ReLu, x, constant=constant)\n'"
src/mygrad/nnet/activations/sigmoid.py,3,"b'import numpy as np\n\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\n\nclass Sigmoid(Operation):\n    def __call__(self, a):\n        self.variables = (a,)\n        x = np.asarray(-1.0 * a.data)\n        np.exp(x, out=x)\n        x += 1\n        np.reciprocal(x, out=x)\n        self.sigmoid = x\n        return self.sigmoid\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad * self.sigmoid * (1.0 - self.sigmoid)\n\n\ndef sigmoid(x, constant=False):\n    """""" Applies the sigmoid activation function::\n\n      f(x) = 1 / (1 + exp(-x))\n\n    Parameters\n    ----------\n    x : array_like\n        sigmoid is applied element-wise on ``x``.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> from mygrad.nnet import sigmoid\n    >>> x = mg.linspace(-5, 5, 10)\n    >>> sigmoid(x)\n    Tensor([0.00669285, 0.02005754, 0.0585369 , 0.1588691 , 0.36457644,\n        0.63542356, 0.8411309 , 0.9414631 , 0.97994246, 0.99330715])""""""\n    return Tensor._op(Sigmoid, x, constant=constant)\n'"
src/mygrad/nnet/activations/softmax.py,5,"b'import numpy as np\n\nfrom mygrad.math._special import logsumexp as _logsumexp\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\n\ndef _softmax(x, kwargs):\n    x = x - x.max(**kwargs)\n    if np.issubdtype(x.dtype, np.integer):\n        x = x.astype(np.float)\n    np.exp(x, out=x)\n    x /= x.sum(**kwargs)\n    return x\n\n\nclass Softmax(Operation):\n    scalar_only = True\n\n    def __call__(self, a):\n        self.variables = (a,)\n        x = a.data\n        assert 0 < a.ndim < 3\n\n        self.__kw = (\n            dict(axis=1, keepdims=True)\n            if a.ndim == 2\n            else dict(axis=None, keepdims=False)\n        )\n        return _softmax(x, self.__kw)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        soft = _softmax(a.data, self.__kw)\n        sg = soft * grad\n        return sg - soft * np.sum(sg, **self.__kw)\n\n\ndef softmax(x, constant=False):\n    r""""""\n    Applies the softmax activation function::\n\n        f(x) = exp(x) / sum( exp(x) )\n\n    Compute the softmax over a 1D tensor of data, or along the\n    respective rows of a 2D tensor\n\n    Parameters\n    ----------\n    x : array_like, shape=(D,) or shape=(N,D)\n        softmax is computed along the rows of ``x`` if\n        ``x`` is a 2D array. Otherwise softmax is computed\n        on the 1D ``x``.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    - :math:`N` is the number of samples in the batch.\n    - :math:`C` is the number of possible classes for which scores are provided.\n    \n    This implements a numerically-stable version of softmax, however\n    log-softmax is still the more numerically stable activation function.\n\n    Given the shape-:math:`(N, C)` tensor of scores, ``x``, the softmax classification\n    probabilities are computed. That is, the score for class-:math:`k` of a given datum\n    (:math:`s_{k}`) is normalized using the \'softmax\' transformation:\n\n    .. math::\n        p_{k} = \\frac{e^{s_k}}{\\sum_{i=1}^{C}{e^{s_i}}}\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> from mygrad.nnet import softmax\n    >>> x = mg.Tensor([[ 2.,  2.,  2.],\n    ...                [2E50, 2E50,  1E50]])\n    >>> softmax(x)\n    Tensor([[0.33333333, 0.33333333, 0.33333333],\n            [0.5       , 0.5       , 0.        ]])\n    """"""\n    return Tensor._op(Softmax, x, constant=constant)\n\n\nclass LogSoftmax(Operation):\n    scalar_only = True\n\n    def __call__(self, a):\n        self.variables = (a,)\n        x = a.data\n        assert 0 < a.ndim < 3\n\n        self.__kw = (\n            dict(axis=1, keepdims=True)\n            if x.ndim == 2\n            else dict(axis=None, keepdims=False)\n        )\n        return x - _logsumexp(x, **self.__kw)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        x = a.data\n        soft = _softmax(x, self.__kw)\n        return grad - soft * np.sum(grad, **self.__kw)\n\n\ndef logsoftmax(x, constant=False):\n    r""""""\n    Applies the log-softmax activation function::\n\n        f(x) = log ( exp(x) / sum( exp(x) ) )\n\n    Compute the softmax over a 1D tensor of data, or along the\n    respective rows of a 2D tensor\n\n    Parameters\n    ----------\n    x : array_like, shape=(D,) or shape=(N,D)\n        log-softmax is computed along the rows of ``x`` if\n        ``x`` is a 2D array. Otherwise log-softmax is computed\n        on the 1D ``x``.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    - :math:`N` is the number of samples in the batch.\n    - :math:`C` is the number of possible classes for which scores are provided.\n\n    This implements a numerically-stable version of log-softmax, compared\n    to the naive implementation using ``mygrad.log``, ``mygrad.exp``, and\n    ``mygrad.sum``.\n\n    Given the shape-:math:`(N, C)` tensor of scores, ``x``, the softmax classification\n    probabilities are computed. That is, the score for class-:math:`k` of a given datum\n    (:math:`s_{k}`) is normalized using the \'softmax\' transformation:\n\n    .. math::\n        p_{k} = \\log{\\frac{e^{s_k}}{\\sum_{i=1}^{C}{e^{s_i}}}}\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> from mygrad.nnet import logsoftmax\n    >>> x = mg.Tensor([[  2.,   2.,    2.],\n    ...                [2E50, 2E50,  1E50]])\n    >>> logsoftmax(x)\n    Tensor([[-1.09861229e+00, -1.09861229e+00, -1.09861229e+00],\n            [ 0.00000000e+00,  0.00000000e+00, -1.00000000e+50]])\n    """"""\n    return Tensor._op(LogSoftmax, x, constant=constant)\n'"
src/mygrad/nnet/layers/__init__.py,0,"b'from .batchnorm import batchnorm\nfrom .conv import conv_nd\nfrom .pooling import max_pool\n\n__all__ = [""conv_nd"", ""max_pool"", ""batchnorm""]\n\n\ntry:\n    from .gru import gru\n\n    __all__ += [""gru""]\nexcept ImportError:  # pragma: no cover\n    pass\n'"
src/mygrad/nnet/layers/batchnorm.py,9,"b'import numpy as np\n\nfrom mygrad import Tensor\nfrom mygrad.operation_base import Operation\n\n__all__ = [""batchnorm""]\n\n\n# TODO: Remove affine parameters from Operation\nclass BatchNorm(Operation):\n    """"""\n    Attributes\n    ----------\n    mean : numpy.ndarray\n    var : numpy.ndarray\n\n    Notes\n    -----\n    `mean` and `var` are bound as instance-attributes upon\n    calling the batch-norm instance.\n    """"""\n\n    scalar_only = True\n\n    def __call__(self, x, gamma, beta, *, eps):\n        """"""\n        y(x) = (x - E[x]) / sqrt(Var[x} + eps)\n        batchnorm(x) = gamma * y(x) + beta\n\n        Parameters\n        ----------\n        x : mygrad.Tensor\n        gamma : Optional[mygrad.Tensor]\n        beta : Optional[mygrad.Tensor]\n        eps : Real\n           A small non-negative number.\n\n        Returns\n        -------\n        numpy.ndarray\n        """"""\n        normed_dims = tuple(i for i in range(x.ndim) if i != 1)\n        keepdims_shape = tuple(1 if n != 1 else d for n, d in enumerate(x.shape))\n\n        if gamma.size == 0:\n            gamma = None\n        if beta.size == 0:\n            beta = None\n\n        self.variables = tuple(i for i in (x, gamma, beta) if i is not None)\n        self.gamma = gamma\n        self.beta = beta\n\n        x = x.data\n        self.x_norm = None  # required for backprop through gamma\n        self.mean = x.mean(axis=normed_dims)\n        self.var = np.einsum(x, range(x.ndim), x, range(x.ndim), [1])\n        self.var /= x.size / x.shape[1]\n        self.var -= self.mean ** 2\n        if eps:\n            self.var += eps\n\n        y = x - self.mean.reshape(keepdims_shape)\n        y /= np.sqrt(self.var).reshape(keepdims_shape)\n\n        # optional affine transformation\n        if gamma is not None:\n            self.x_norm = y\n            gamma = gamma.data\n            # must copy `y` to prevent mutation of `self.x_norm`\n            y = y * gamma.reshape(keepdims_shape)\n\n        if beta is not None:\n            beta = beta.data\n            y += beta.reshape(keepdims_shape)\n        return y\n\n    def backward_var(self, grad, index, **kwargs):\n        x = self.variables[0].data\n        if index == 0:  # backprop through x\n            normed_dims = tuple(i for i in range(x.ndim) if i != 1)\n            keepdims_shape = tuple(1 if n != 1 else d for n, d in enumerate(x.shape))\n            mean = self.mean.reshape(keepdims_shape)\n            var = self.var.reshape(keepdims_shape)\n\n            grad = grad - np.mean(grad, axis=normed_dims, keepdims=True)\n            x_sub_Ex = x - mean\n            rterm = x_sub_Ex / var\n            rterm /= x.size / x.shape[1]\n            rterm *= np.reshape(\n                np.einsum(grad, range(x.ndim), x_sub_Ex, range(x.ndim), [1]),\n                keepdims_shape,\n            )\n            grad -= rterm\n            grad /= np.sqrt(var)\n            if (\n                self.gamma is not None\n            ):  # backprop through optional affine transformation\n                gamma = self.gamma.data\n                grad *= gamma.reshape(keepdims_shape)\n            return grad\n\n        elif index == 1 and self.gamma is not None:  # backprop through gamma\n            return np.einsum(grad, range(x.ndim), self.x_norm, range(x.ndim), [1])\n\n        elif (index == 1 and self.gamma is None) or index == 2:\n            normed_dims = tuple(i for i in range(x.ndim) if i != 1)\n            return grad.sum(axis=normed_dims)\n        else:  # pragma: no cover\n            raise IndexError\n\n\ndef batchnorm(x, *, gamma=None, beta=None, eps, constant=False):\n    """"""\n    Performs batch normalization on ``x``::\n\n                 y(x) = (x - E[x]) / sqrt(Var[x] + eps)\n                 batchnorm(x) = gamma * y(x) + beta\n\n    Where :math:`E[x]` and :math:`Var[x]` represent the mean and variance, respectively,\n    over axis-1 of ``x``. The subsequent affine transformation on ``y``\n    is optional.\n\n    Parameters\n    ----------\n    x : array_like, shape=(N, C, ...)\n        The batch to be normalized within each entry of C\n\n    gamma : Optional[array_like], shape=(C,)\n        Optional per-channel scaling factors to be applied after the\n        normalization step.\n\n    beta  : Optional[array_like], shape=(C,)\n        Optional per-channel scaling bias factors to be applied after the\n        normalization step.\n\n    eps : Real\n       A small non-negative number.\n\n    constant : bool, optional (default=False)\n        If True, the resulting Tensor is a constant.\n\n    Returns\n    -------\n    mygrad.Tensor\n        The batch-normalized data.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> from mygrad.nnet import batchnorm\n    >>> x = mg.Tensor([1., 4., 1.]).reshape(3, 1)\n    >>> batchnorm(x, eps=0)\n    Tensor([[-0.70710678],\n            [ 1.41421356],\n            [-0.70710678]])\n    """"""\n    # pass gamma and beta as empty arrays if they are not supplied\n    if gamma is None:\n        gamma = np.array([])\n    if beta is None:\n        beta = np.array([])\n    return Tensor._op(\n        BatchNorm, x, gamma, beta, op_kwargs=dict(eps=eps), constant=constant\n    )\n'"
src/mygrad/nnet/layers/conv.py,19,"b'from numbers import Integral\n\nimport numpy as np\n\nfrom mygrad.nnet.layers.utils import sliding_window_view\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\n__all__ = [""conv_nd""]\n\n\nclass ConvND(Operation):\n    scalar_only = True\n\n    def __call__(self, x, w, *, stride, padding=0, dilation=1):\n        self.variables = (x, w)\n        # x ... data:    (N, C, X0, X1, ...)\n        # w ... filters: (F, C, W0, W1, ...)\n\n        x = x.data\n        w = w.data\n\n        assert x.ndim > 2\n        assert x.ndim == w.ndim\n        assert (\n            w.shape[1] == x.shape[1]\n        ), ""The channel-depth of the batch and filters must agree""\n\n        num_conv_channels = w.ndim - 2\n        x_shape = np.array(\n            x.shape[2:]\n        )  # (X0, ...): shape of the channels being convolved over\n        w_shape = np.array(w.shape[2:])  # (W0, ...): shape of each conv filter\n\n        dilation = (\n            np.array((dilation,) * num_conv_channels)\n            if isinstance(dilation, Integral)\n            else np.array(dilation, dtype=int)\n        )\n\n        assert len(dilation) == num_conv_channels and all(\n            d >= 1 and isinstance(d, Integral) for d in dilation\n        )\n\n        padding = (\n            np.array((padding,) * num_conv_channels)\n            if isinstance(padding, Integral)\n            else np.array(padding, dtype=int)\n        )\n        assert len(padding) == num_conv_channels and all(\n            p >= 0 and isinstance(p, Integral) for p in padding\n        )\n\n        stride = (\n            np.array((stride,) * num_conv_channels)\n            if isinstance(stride, Integral)\n            else np.asarray(stride, dtype=int)\n        )\n        assert len(stride) == num_conv_channels and all(\n            s >= 1 and isinstance(s, Integral) for s in stride\n        )\n\n        out_shape = (\n            x_shape + 2 * padding - ((w_shape - 1) * dilation + 1)\n        ) / stride + 1\n\n        if not all(i.is_integer() and i > 0 for i in out_shape):\n            msg = ""Stride and kernel dimensions are incompatible: \\n""\n            msg += f""Input dimensions: {tuple(x_shape)}\\n""\n            msg += f""Stride dimensions: {tuple(stride)}\\n""\n            msg += f""Kernel dimensions: {tuple(w_shape)}\\n""\n            msg += f""Padding dimensions: {tuple(padding)}\\n""\n            msg += f""Dilation dimensions: {tuple(dilation)}\\n""\n            raise ValueError(msg)\n\n        self.padding = padding\n        self.stride = stride\n        self.dilation = dilation\n\n        # symmetric 0-padding for X0, X1, ... dimensions\n        axis_pad = tuple((i, i) for i in (0, 0, *padding))\n        x = np.pad(x, axis_pad, mode=""constant"") if sum(padding) else x\n\n        # (G0, ...) is the tuple of grid-positions for placing each window (not including stride)\n        # (N, C, X0, ...) -> (G0, ..., N, C, W0, ...)\n        windowed_data = sliding_window_view(\n            x, window_shape=w_shape, step=self.stride, dilation=self.dilation\n        )\n\n        w_conv_channels = list(range(1, num_conv_channels + 2))  # C, W0, ...\n        window_conv_channels = [\n            i + 1 + num_conv_channels  # C, W0, ...\n            for i in range(num_conv_channels + 1)\n        ]\n\n        # (F, C, W0, ...) \xe2\x8b\x86 (G0, ..., N, C, W0, ...) -> (F, G0, ..., N)\n        conv_out = np.tensordot(\n            w, windowed_data, axes=[w_conv_channels, window_conv_channels]\n        )\n\n        # (F, G0, ..., N) -> (N, F, G0, ...)\n        out = np.moveaxis(conv_out, source=-1, destination=0)\n        return out if out.flags[""C_CONTIGUOUS""] else np.ascontiguousarray(out)\n\n    def backward_var(self, grad, index, **kwargs):\n        """""" Computes dX, where X is the data batch\n\n            Parameters\n            ----------\n            grad : numpy.ndarray, shape=(N, F, G0, ...)""""""\n        x, w = (i.data for i in self.variables)\n        num_conv_channels = grad.ndim - 2\n\n        if index == 0:  # backprop through x\n            x_shape = x.shape[:2] + tuple(\n                i + 2 * p for i, p in zip(x.shape[-num_conv_channels:], self.padding)\n            )\n            dx = np.zeros(x_shape, dtype=x.dtype)  # (N, C, X0, ...)\n\n            # `gp` stores all of the various broadcast multiplications of each grad\n            # element against the conv filter.\n            # (N, F, G0, ...) -tdot- (F, C, W0, ...) --> (N, G0, ..., C, W0, ...)\n            gp = np.tensordot(grad, w, axes=[[1], [0]])\n            for ind in np.ndindex(grad.shape[-num_conv_channels:]):\n                # ind: (g0, ...) - grid-position of filter placement\n                slices = tuple(\n                    slice(i * s, i * s + w * d, d)\n                    for i, w, s, d in zip(ind, w.shape[2:], self.stride, self.dilation)\n                )\n                # Add (grad-element * filter) to each appropriate window position in `dx`\n                # dx[N, C, g0*s0 : g0*s0 + w0*d0 : d0, (...)] += gp[N, g0, (...), C, W0, (...)]\n                dx[(..., *slices)] += gp[(slice(None), *ind, ...)]\n\n            # remove padding from dx\n            if sum(self.padding):\n                no_pads = tuple(slice(p, -p if p else None) for p in self.padding)\n                dx = dx[(..., *no_pads)]\n            return dx\n\n        else:  # backprop through w\n            # backprop into f\n            # symmetric 0-padding for H, W dimensions\n            axis_pad = tuple((i, i) for i in (0, 0, *self.padding))\n            x = np.pad(x, axis_pad, mode=""constant"") if sum(self.padding) else x\n\n            # (G0, ...) is the tuple of grid-indices for placing each window (not including stride)\n            # (N, C, X0, ...) -> (G0, ..., N, C, W0, ...)\n            windowed_data = sliding_window_view(\n                x, window_shape=w.shape[2:], step=self.stride, dilation=self.dilation\n            )\n\n            # (N, F, G0, ...) -tdot- (G0, ..., N, C, W0, ...) --> (F, C, W0, ...)\n            grad_axes = list(range(2, num_conv_channels + 2)) + [0]  # (G0, ..., N)\n            window_axes = list(range(num_conv_channels + 1))  # (G0, ..., N)\n            return np.tensordot(grad, windowed_data, axes=[grad_axes, window_axes])\n\n\ndef conv_nd(x, filter_bank, *, stride, padding=0, dilation=1, constant=False):\n    """""" Use `filter_bank` to perform strided N-dimensional neural network-style\n    convolutions (see Notes) over `x`.::\n\n            f(x, w) -> x \xe2\x8b\x86 w\n\n            shapes:\n            (N, C, X0, ...) \xe2\x8b\x86 (F, C, W0, ...) -> (N, F, G0, ...)\n\n    ``x`` represents a batch of data over which the filters\n    are convolved. Specifically, it must be a tensor of shape\n    :math:`(N, C, X_0, ...)`, where :math:`N` is the number of samples in the batch,\n    C is the channel-depth of each datum, and :math:`(X_0, ...)` are the\n    dimensions over which the filters are convolved. Accordingly,\n    each filter must have a channel depth of :math:`C`.\n\n    Thus convolving :math:`F` filters, each with a shape :math:`(C, W_0, ...)`,\n    over the data batch will produce a tensor of shape\n    :math:`(N, F, G_0, ...)`, where :math:`(G_0, ...)` is the shape of the grid\n    commensurate with the filter placements\n\n    Parameters\n    ----------\n    x : Union[Tensor, array_like], shape=(N, C, Xo, ...)\n        The data batch to be convolved over.\n\n    filter_bank : Union[Tensor, array_like], shape=(F, C, Wo, ...)\n        The filters used to perform the convolutions.\n\n    stride : Union[int, Tuple[int, ...]]\n        (keyword-only argument) The step-size with which each\n        filter is placed along the H and W axes during the\n        convolution. The tuple indicates (stride-0, ...). If a\n        single integer is provided, this stride is used for all\n        convolved dimensions\n\n    padding : Union[int, Tuple[int, ...]]\n        (keyword-only argument) The number of zeros to be padded\n        to both ends of each convolved dimension, respectively.\n        If a single integer is provided, this padding is used for\n        all of the convolved axes\n\n    dilation : Union[int, Tuple[int, ...]], optional (default=1)\n        (keyword-only argument) The spacing used when placing kernel\n        elements along the data. E.g. for a 1D convolution the ith\n        placement of the kernel multiplied  against the dilated-window:\n        ``x[:, :, i*s:(i*s + w*d):d]``, where ``s`` is\n        the stride, ``w`` is the kernel-size, and ``d`` is the dilation factor.\n\n        If a single integer is provided, that dilation value is used for all\n        of the convolved axes\n\n    constant : bool, optional (default=False)\n        If True, the resulting Tensor is a constant.\n\n    Returns\n    -------\n    Tensor, shape=(N, F, G0, ...)\n        The result of each filter being convolved over each datum in\n        the batch.\n\n    Notes\n    -----\n     - The filters are *not* flipped by this operation, meaning that\n       an auto-correlation is being performed rather than a true convolution.\n\n     - Only \'valid\' filter placements are permitted - where the filters overlap\n       completely with the (padded) data.\n\n     - This is a ""scalar-only"" operation, meaning that back propagation through\n       this layer assumes that a scalar (i.e. a 0-dimensional tensor) will invoke\n       ``tensor.backward()`` for the computational graph. This is standard for a\n       neural network, which terminates in a scalar loss.\n\n    Examples\n    --------\n    Here we perform a 1D convolution of a constant-valued kernel, ``k``, with a\n    \'square-wave\' signal, ``x``, using stride-1. Note that because we are constrained\n    to doing deep learning-style convolutions, that we prepend the dimensions\n    :math:`(N=1, C=1)` to ``x``, and :math:`(F=1, C=1)` and to ``k``. That is,\n    we are performing a convolution on one, single-channeled signal using\n    one kernel.\n\n    See that this convolution produces the expected triangle-shaped\n    response. The shape of the resulting tensor is :math:`(N=1, F=1, G_0=12)`.\n    That is, the length-5 kernel can be placed in 12 valid positions, using a\n    stride of 1.\n\n    >>> import mygrad as mg\n    >>> from mygrad.nnet import conv_nd\n    >>> x = mg.zeros((1, 1, 16))  # a square-wave signal\n    >>> x[..., 5:11] = 1\n    >>> k = mg.ones((1, 1, 5))    # a constant-valued kernel\n    >>> conv_nd(x, k, stride=1)   # performing a stride-1, 1D convolution\n    Tensor([[[0., 1., 2., 3., 4., 5., 5., 4., 3., 2., 1., 0.]]], dtype=float32)\n\n    Back-propagating through the (summed) convolution:\n\n    >>> conv_nd(x, k, stride=1).sum().backward()  # sum to a scalar to perform back-prop\n    >>> x.grad  # d(summed_conv)/dx\n    array([[[1., 2., 3., 4., 5., 5., 5., 5., 5., 5., 5., 5., 4., 3., 2., 1.]]],\n          dtype=float32)\n    >>> k.grad  # d(summed_conv)/dk\n    array([[[6., 6., 6., 6., 6.]]])\n\n    Now, let\'s demonstrate a more typical usage for ``conv_nd`` in the context of\n    neural networks. ``x`` will represent 10, 32x32 RGB images, and we will use\n    5 distinct 2x2 kernels to convolve over each of these images . Note that\n    each kernel must possess 3-channel - one for each RGB channel.\n\n    That is, we will be performing NxF channel-wise 2D convolutions. Supposing\n    that we don\'t want the kernel placements to overlap, we can use a stride of 2. In\n    total, this will produce a shape-:math:`(N=10, F=5, G_0=16, G_1=16)` tensor as a\n    result.\n\n    >>> import numpy as np\n    >>> x = mg.Tensor(np.random.rand(10, 3, 32, 32))  # creating 10 random 32x32 RGB images\n    >>> k = mg.Tensor(np.random.rand(5, 3, 2, 2))     # creating 5 random 3-channel 2x2 kernels\n\n    Given the shapes of ``x`` and ``k``, ``conv_nd`` automatically executes a 2D convolution:\n\n    >>> conv_nd(x, k, stride=2).shape\n    (10, 5, 16, 16)\n\n    Extrapolating further, ``conv_nd`` is capable of performing ND convolutions!\n    """"""\n    if x.ndim < 3:\n        raise ValueError(\n            f""`x` must possess at least three "" f""dimensions, got {x.ndim} dimensions""\n        )\n\n    if x.ndim != filter_bank.ndim:\n        raise ValueError(\n            f""`x` ({x.ndim}-dimensions) must have the same dimensionality as ""\n            f""`filter_bank` ({filter_bank.ndim}-dimensions)""\n        )\n\n    if filter_bank.shape[1] != x.shape[1]:\n        raise ValueError(\n            f""`x.shape[1]` ({x.shape[1]}) must match `filter_bank.shape[1]` ({filter_bank.shape[1]})""\n        )\n\n    return Tensor._op(\n        ConvND,\n        x,\n        filter_bank,\n        op_kwargs=dict(stride=stride, padding=padding, dilation=dilation),\n        constant=constant,\n    )\n'"
src/mygrad/nnet/layers/gru.py,23,"b'from numbers import Integral\n\nimport numpy as np\n\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\ntry:\n    from numba import njit, vectorize\nexcept ImportError:  # pragma: no cover\n    raise ImportError(\n        ""The package `numba` must be installed in order to access the gru.""\n    )\n\n\n@vectorize(\n    [""int32(int32)"", ""int64(int64)"", ""float32(float32)"", ""float64(float64)""],\n    nopython=True,\n)\ndef sig(f):  # pragma: no cover\n    """"""\n    Calculates a sigmoid function\n    """"""\n    return 1 / (1 + np.exp(-f))\n\n\n@vectorize(\n    [""int32(int32)"", ""int64(int64)"", ""float32(float32)"", ""float64(float64)""],\n    nopython=True,\n)\ndef d_sig(f):  # pragma: no cover\n    """"""\n    Calculates the derivative of a sigmoid function\n    """"""\n    return f * (1 - f)\n\n\n@vectorize(\n    [""int32(int32)"", ""int64(int64)"", ""float32(float32)"", ""float64(float64)""],\n    nopython=True,\n)\ndef d_tanh(f):  # pragma: no cover\n    """"""\n    Calculates the derivative of a tanh function\n    """"""\n    return 1 - f ** 2\n\n\n@njit\ndef dot(a, b):\n    """"""\n    Calculates the dot product between 2 arrays\n    of shapes (W,X,Y) and (Y,Z), respectively\n    """"""\n    out = np.zeros((a.shape[0], a.shape[1], b.shape[-1]))\n    for i in range(len(a)):\n        out[i] = np.dot(a[i], b)\n    return out\n\n\n@njit\ndef _gru_layer(s, z, r, h, Wz, Wr, Wh):\n    """""" Given:\n            S(t=0)\n            z = X(t) Uz + bz\n            r = X(t) Ur + br\n            h = X(t) Uh + bh\n\n        Compute Z(t), R(t), H(t), S(t) for all 1 <= t <= T\n\n        Parameters\n        ----------\n        s : numpy.ndarray, shape=(T+1, N, D)\n            Modified in-place\n        z : numpy.ndarray, shape=(T, N, D)\n            Modified in-place\n        r : numpy.ndarray, shape=(T, N, D)\n            Modified in-place\n        h : numpy.ndarray, shape=(T, N, D)\n            Modified in-place\n        Wz : numpy.ndarray, shape=(D, D)\n        Wr : numpy.ndarray, shape=(D, D)\n        Wh : numpy.ndarray, shape=(D, D) """"""\n    for n in range(len(s) - 1):\n        z[n] += np.dot(s[n], Wz)\n        z[n] = sig(z[n])\n\n        r[n] += np.dot(s[n], Wr)\n        r[n] = sig(r[n])\n\n        h[n] += np.dot(r[n] * s[n], Wh)\n        h[n] = np.tanh(h[n])\n\n        s[n + 1] = (1 - z[n]) * h[n] + z[n] * s[n]\n\n\n@njit\ndef _gru_dLds(s, z, r, dLds, Wz, Wh, Wr, dz, dh, dr, s_h, one_z):\n    """"""\n                        Z_{t} = sigmoid(Uz X_{t} + Wz S_{t-1} + bz)\n                        R_{t} = sigmoid(Ur X_{t} + Wr S_{t-1} + br)\n                        H_{t} = tanh(Uh X_{t} + Wh (R{t} * S_{t-1}) + bh)\n                        S_{t} = (1 - Z{t}) * H{t} + Z{t} * S_{t-1}\n\n    Returns\n    --------\n\n        dL / ds(t) =   partial dL / ds(t+1) * ds(t+1) / ds(t)\n                     + partial dL / ds(t+1) * ds(t+1) / dz(t) * dz(t) / ds(t)\n                     + partial dL / ds(t+1) * ds(t+1) / dh(t) * dh(t) / ds(t)\n                     + partial dL / ds(t+1) * ds(t+1) / dh(t) * dh(t) / dr(t) * dr(t) / ds(t)\n    """"""\n    dLdh = dot(dLds * one_z * dh, Wh)\n\n    out = z * dLds\n    out += dot(dLds * s_h * dz, Wz)\n    out += dLdh * r\n    out += dot(dLdh * s * dr, Wr)\n\n    return out\n\n\n@njit\ndef _gru_bptt(\n    X, dLds, s, z, r, Wz, Wh, Wr, dz, dh, dr, s_h, one_z, bp_lim, old_dLds=None\n):\n    Wz, Wh, Wr = Wz.T, Wh.T, Wr.T\n    bptt = bp_lim < len(X) - 1\n    if bptt:  # pragma: no cover\n        old_dLds = np.zeros_like(dLds)\n\n    for i in range(bp_lim):\n        #  dL(t) / ds(t) + dL(t+1) / ds(t)\n        if bptt:  # pragma: no cover\n            source_index = slice(1, len(dLds) - i)\n            target_index = slice(None, len(dLds) - (i + 1))\n            dt = dLds[source_index] - old_dLds[source_index]\n            old_dLds = np.copy(dLds)\n        else:  # no backprop truncation\n            source_index = slice(len(dLds) - (i + 1), len(dLds) - i)\n            target_index = slice(len(dLds) - (i + 2), len(dLds) - (i + 1))\n            dt = dLds[source_index]\n\n        dLds[target_index] += _gru_dLds(\n            s[source_index],\n            z[source_index],\n            r[source_index],\n            dt,\n            Wz,\n            Wh,\n            Wr,\n            dz[source_index],\n            dh[source_index],\n            dr[source_index],\n            s_h[source_index],\n            one_z[source_index],\n        )\n\n\ndef _backprop(var, grad):  # pragma: no cover\n    if not var.constant:\n        if var.grad is None:\n            var.grad = np.asarray(grad)\n        else:\n            var.grad += grad\n\n\nclass GRUnit(Operation):\n    scalar_only = True\n\n    def __call__(\n        self, X, Uz, Wz, bz, Ur, Wr, br, Uh, Wh, bh, s0=None, bp_lim=None, dropout=0.0\n    ):\n        if bp_lim is not None:\n            assert isinstance(bp_lim, Integral) and 0 <= bp_lim < len(X)\n        assert 0.0 <= dropout < 1.0\n        self._dropout = dropout\n        self.bp_lim = bp_lim if bp_lim is not None else len(X) - 1\n\n        self.X = X  # type: Tensor  # shape=(T, N, C)\n\n        self.Uz = Uz  # type: Tensor  # shape=(C, D)\n        self.Wz = Wz  # type: Tensor  # shape=(D, D)\n        self.bz = bz  # type: Tensor  # shape=(D,)\n\n        self.Ur = Ur  # type: Tensor  # shape=(C, D)\n        self.Wr = Wr  # type: Tensor  # shape=(D, D)\n        self.br = br  # type: Tensor  # shape=(D,)\n\n        self.Uh = Uh  # type: Tensor  # shape=(C, D)\n        self.Wh = Wh  # type: Tensor  # shape=(D, D)\n        self.bh = bh  # type: Tensor  # shape=(D,)\n\n        self.variables = (\n            self.X,\n            self.Uz,\n            self.Wz,\n            self.bz,\n            self.Ur,\n            self.Wr,\n            self.br,\n            self.Uh,\n            self.Wh,\n            self.bh,\n        )\n        T, N, C = X.shape\n        (D,) = bz.shape\n\n        seq = self.X.data\n\n        # t starts at 0 for S; all other sequences begin at t = 1\n        out = np.zeros((T + 1, N, D))\n\n        if s0 is not None:\n            out[0] = s0.data if isinstance(s0, Tensor) else s0\n\n        # compute all contributions to Z, R, H from the input sequence\n        # shape: T, N, D\n        z = np.tensordot(seq, self.Uz.data, [[-1], [0]])\n        r = np.tensordot(seq, self.Ur.data, [[-1], [0]])\n        h = np.tensordot(seq, self.Uh.data, [[-1], [0]])\n\n        if dropout:\n            p = 1 - dropout\n            # For Uz/Ur/Uh: a dropout mask is generated for each datum and is applied uniformly across T\n            self._dropUz, self._dropUr, self._dropUh = (\n                np.random.binomial(1, p, size=(3, 1, N, D)) / p\n            )\n            self._dropWz, self._dropWr, self._dropWh = (\n                np.random.binomial(1, p, size=(3, D, D)) / p\n            )\n\n            z *= self._dropUz\n            r *= self._dropUr\n            h *= self._dropUh\n\n            Wz = self._dropWz * self.Wz.data\n            Wr = self._dropWr * self.Wr.data\n            Wh = self._dropWh * self.Wh.data\n\n        else:\n            self._dropUz, self._dropUr, self._dropUh = None, None, None\n            self._dropWz, self._dropWr, self._dropWh = None, None, None\n            Wz = self.Wz.data\n            Wr = self.Wr.data\n            Wh = self.Wh.data\n\n        z += bz.data  # X Uz + bz\n        r += br.data  # X Ur + br\n        h += bh.data  # X Uh + bh\n\n        _gru_layer(out, z, r, h, Wz, Wr, Wh)\n\n        self._hidden_seq = Tensor(out, _creator=self)\n        self._z = Tensor(z, _creator=self)\n        self._r = Tensor(r, _creator=self)\n        self._h = Tensor(h, _creator=self)\n\n        return self._hidden_seq\n\n    def backward(self, grad, *, graph, **kwargs):\n\n        s = self._hidden_seq.data[:-1]\n        z = self._z.data\n        r = self._r.data\n        h = self._h.data\n\n        dLds = grad[1:]\n\n        const = {""1 - h**2"": d_tanh(h), ""z*(1 - z)"": d_sig(z), ""r*(1 - r)"": d_sig(r)}\n\n        if self._dropout:\n            Wz = self._dropWz * self.Wz.data\n            Wr = self._dropWr * self.Wr.data\n            Wh = self._dropWh * self.Wh.data\n        else:\n            Wz = self.Wz.data\n            Wr = self.Wr.data\n            Wh = self.Wh.data\n\n        const[""s - h""] = s - h\n        const[""1 - z""] = 1 - z\n\n        _gru_bptt(\n            self.X.data,\n            dLds,\n            s,\n            z,\n            r,\n            Wz,\n            Wh,\n            Wr,\n            const[""z*(1 - z)""],\n            const[""1 - h**2""],\n            const[""r*(1 - r)""],\n            const[""s - h""],\n            const[""1 - z""],\n            self.bp_lim,\n        )\n\n        zgrad = dLds * const[""s - h""]  # dL / dz\n        hgrad = dLds * const[""1 - z""]  # dL / dh\n        rgrad = dot(const[""1 - h**2""] * hgrad, Wh.T) * s  # dL / dr\n\n        self._hidden_seq.grad = dLds\n\n        if not (self.Uz.constant and self.Wz.constant and self.bz.constant):\n            dz = zgrad * const[""z*(1 - z)""]\n        # backprop through Wz\n        if not self.Wz.constant:\n            dWz = np.tensordot(s, dz, ([0, 1], [0, 1]))\n            if self._dropout:\n                dWz *= self._dropWz\n            _backprop(self.Wz, dWz)  # self.Wz.backward(dWz, **kwargs)\n        # backprop through bz\n        if not self.bz.constant:\n            _backprop(self.bz, dz.sum(axis=(0, 1)))\n        # backprop through bz\n        if not self.Uz.constant:\n            if self._dropout:\n                dz *= (\n                    self._dropUz\n                )  # IMPORTANT augmented update: this must come after Wz and bz backprop\n            _backprop(self.Uz, np.tensordot(self.X.data, dz, ([0, 1], [0, 1])))\n\n        if not (self.Ur.constant and self.Wr.constant and self.br.constant):\n            dr = rgrad * const[""r*(1 - r)""]\n        # backprop through Wr\n        if not self.Wr.constant:\n            dWr = np.tensordot(s, dr, ([0, 1], [0, 1]))\n            if self._dropout:\n                dWr *= self._dropWr\n            _backprop(self.Wr, dWr)\n        # backprop through br\n        if not self.br.constant:\n            _backprop(\n                self.br, dr.sum(axis=(0, 1))\n            )  # self.br.backward(dr.sum(axis=(0, 1)), **kwargs)\n        # backprop through Ur\n        if not self.Ur.constant:\n            if self._dropout:\n                dr *= (\n                    self._dropUr\n                )  # IMPORTANT augmented update: this must come after Wr and br backprop\n            _backprop(self.Ur, np.tensordot(self.X.data, dr, ([0, 1], [0, 1])))\n\n        if not (self.Uh.constant and self.Wh.constant and self.bh.constant):\n            dh = hgrad * const[""1 - h**2""]\n        # backprop through Wh\n        if not self.Wh.constant:\n            dWh = np.tensordot((s * r), dh, ([0, 1], [0, 1]))\n            if self._dropout:\n                dWh *= self._dropWh\n            _backprop(self.Wh, dWh)  # self.Wh.backward(dWh, **kwargs)\n        # backprop through bh\n        if not self.bh.constant:\n            _backprop(\n                self.bh, dh.sum(axis=(0, 1))\n            )  # self.bh.backward(dh.sum(axis=(0, 1)), **kwargs)\n        # backprop through Uh\n        if not self.Uh.constant:\n            if self._dropout:\n                dh *= (\n                    self._dropUh\n                )  # IMPORTANT augmented update: this must come after Wh and bh backprop\n            _backprop(self.Uh, np.tensordot(self.X.data, dh, ([0, 1], [0, 1])))\n\n        # backprop through X\n        if not self.X.constant:\n            tmp = dLds * const[""1 - z""] * const[""1 - h**2""]\n            if not self._dropout:\n                dLdX = dot((dLds * const[""s - h""]) * const[""z*(1 - z)""], self.Uz.data.T)\n                dLdX += dot(tmp, self.Uh.data.T)\n                dLdX += dot(dot(tmp, Wh.T) * s * const[""r*(1 - r)""], self.Ur.data.T)\n            else:\n                dLdX = dot(\n                    (self._dropUz * (dLds * const[""s - h""]) * const[""z*(1 - z)""]),\n                    self.Uz.data.T,\n                )\n                dLdX += dot(self._dropUh * tmp, self.Uh.data.T)\n                dLdX += dot(\n                    self._dropUr * (dot(tmp, Wh.T) * s * const[""r*(1 - r)""]),\n                    self.Ur.data.T,\n                )\n            _backprop(self.X, dLdX)  # self.X.backward(dLdX, **kwargs)\n\n        del self._z\n        del self._r\n        del self._h\n\n        for x in (\n            self.X,\n            self.Uz,\n            self.Wz,\n            self.bz,\n            self.Ur,\n            self.Wr,\n            self.br,\n            self.Uh,\n            self.Wh,\n            self.bh,\n        ):\n            if not x.constant:\n                x._accum_ops.add(self)\n                x._backward(graph=graph)\n\n\ndef gru(\n    X,\n    Uz,\n    Wz,\n    bz,\n    Ur,\n    Wr,\n    br,\n    Uh,\n    Wh,\n    bh,\n    s0=None,\n    bp_lim=None,\n    dropout=0.0,\n    constant=False,\n):\n    r"""""" Performs a forward pass of sequential data through a Gated Recurrent Unit layer, returning\n    the \'hidden-descriptors\' arrived at by utilizing the trainable parameters as follows::\n\n                Z_{t} = sigmoid(X_{t} Uz + S_{t-1} Wz + bz)\n                R_{t} = sigmoid(X_{t} Ur + S_{t-1} Wr + br)\n                H_{t} =    tanh(X_{t} Uh + (R{t} * S_{t-1}) Wh + bh)\n                S_{t} = (1 - Z{t}) * H{t} + Z{t} * S_{t-1}\n\n    Parameters\n    ----------\n    X : array_like, shape=(T, N, C)\n       The sequential data to be passed forward.\n\n    Uz : array_like, shape=(C, D)\n       The weights used to map sequential data to its hidden-descriptor representation\n\n    Wz : array_like, shape=(D, D)\n        The weights used to map a hidden-descriptor to a hidden-descriptor.\n\n    bz : array_like, shape=(D,)\n       The biases used to scale a hidden-descriptor.\n\n    Ur : array_like, shape=(C, D)\n       The weights used to map sequential data to its hidden-descriptor representation\n\n    Wr : array_like, shape=(D, D)\n        The weights used to map a hidden-descriptor to a hidden-descriptor.\n\n    br : array_like, shape=(D,)\n       The biases used to scale a hidden-descriptor.\n\n    Uh : array_like, shape=(C, D)\n       The weights used to map sequential data to its hidden-descriptor representation\n\n    Wh : array_like, shape=(D, D)\n        The weights used to map a hidden-descriptor to a hidden-descriptor.\n\n    bh : array_like, shape=(D,)\n       The biases used to scale a hidden-descriptor.\n\n    s0 : Optional[array_like], shape=(N, D)\n        The \'seed\' hidden descriptors to feed into the RNN. If None, a Tensor\n        of zeros of shape (N, D) is created.\n\n    bp_lim : Optional[int]\n        *This feature is experimental and is currently untested*.\n        The (non-zero) limit of the depth of back propagation through time to be\n        performed. If `None` back propagation is passed back through the entire sequence.\n\n        E.g. `bp_lim=3` will propagate gradients only up to 3 steps backward through the\n        recursive sequence.\n\n    dropout : float (default=0.), 0 <= dropout < 1\n        If non-zero, the dropout scheme described in [1]_ is applied. See Notes\n        for more details.\n\n    constant : bool, optional (default=False)\n        If True, the resulting Tensor is a constant.\n\n    Returns\n    -------\n    mygrad.Tensor, shape=(T+1, N, D)\n        The sequence of \'hidden-descriptors\' produced by the forward pass of the RNN.\n\n    Notes\n    -----\n    - :math:`T` : Sequence length\n    - :math:`N` : Batch size\n    - :math:`C` : Length of single datum\n    - :math:`D` : Length of \'hidden\' descriptor\n\n    The GRU system of equations is given by:\n\n    .. math::\n\n                Z_{t} = \\sigma (X_{t} U_z + S_{t-1} Wz + bz)\n\n                R_{t} = \\sigma (X_{t} U_r + S_{t-1} Wr + br)\n\n                H_{t} =    tanh(X_{t} U_h + (R_{t} * S_{t-1}) W_h + b_h)\n\n                S_{t} = (1 - Z_{t}) * H_{t} + Z_{t} * S_{t-1}\n\n    Following the dropout scheme specified in [1]_, the hidden-hidden weights (Wz/Wr/Wh)\n    randomly have their weights dropped prior to forward/back-prop. The input connections\n    (via Uz/Ur/Uh) have variational dropout ([2]_) applied to them with a common dropout\n    mask across all t. That is three static dropout masks, each with shape-(N,D), are\n    applied to\n\n    .. math::\n                                          X_{t} U_z\n\n                                          X_{t} U_r\n\n                                          X_{t} U_h\n    respectively, for all :math:`t`.\n\n    References\n    ----------\n    .. [1] S. Merity, et. al. ""Regularizing and Optimizing LSTM Language Models"",\n           arXiv:1708.02182v1, 2017.\n\n    .. [2] Y. Gal, Z. Ghahramani ""A Theoretically Grounded Application of Dropout\n           in Recurrent Neural Networks"" arXiv:1512.05287v5, 2016. """"""\n    if s0 is not None:\n        if not isinstance(s0, np.ndarray) and not (\n            isinstance(s0, Tensor) and (constant or s0.constant)\n        ):\n            raise ValueError(\n                ""GRU does not support non-constant tensors for the initial hidden""\n                ""state value, `s0`""\n            )\n    s = Tensor._op(\n        GRUnit,\n        X,\n        Uz,\n        Wz,\n        bz,\n        Ur,\n        Wr,\n        br,\n        Uh,\n        Wh,\n        bh,\n        op_kwargs=dict(s0=s0, bp_lim=bp_lim, dropout=dropout),\n        constant=constant,\n    )\n    s.creator._hidden_seq = s\n    return s\n'"
src/mygrad/nnet/layers/pooling.py,14,"b'from numbers import Integral\n\nimport numpy as np\n\nfrom mygrad.nnet.layers.utils import sliding_window_view\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\n\nclass MaxPoolND(Operation):\n    scalar_only = True\n\n    def __call__(self, x, pool, stride):\n        """""" Perform max-pooling over the last N dimensions of a data batch.\n\n        Extended Summary\n        ----------------\n        The data consists of N trailing axes to be pooled over, denoted by ``C0, ...``. These\n        can be preceded, optionally, by un-pooled axes, denoted by ``(N0, ...)``. The dimensions\n        of the window over which pooling is performed is denoted by ``P0, ...``. The window\n        is placed with stride values ``S0, ...``.\n\n        Ultimately the pooled channels have a shape ``G0, ...``.\n\n        Parameters\n        ----------\n        x : mygrad.Tensor, shape=([...], C0, ...)\n            The data batch; to be pooled along the trailing axes denoted by ``C0, ...``.\n\n        pool : Tuple[Integral, ...], (P0, ...)\n            The extent of the pooling window along the ``(C0, ...)`` axes, respectively. The\n            length of `pool` determines ``N`` - the number of trailing dimensions to pool over.\n\n        stride : Union[Integral, Tuple[Integral, ...]], (S0, ...)\n            The spacing used to place the pooling window, along ``(P0, ...)`` axes, respectively.\n            If a single value is provided, it is used for all N pooling axes.\n\n        Returns\n        -------\n        numpy.ndarray, shape=([...], G0, ...)\n            The pooled data batch.\n\n        Notes\n        -----\n        Only \'valid\' placements of the pooling window are permitted - the pooling\n        window cannot extend passed the ""boundaries"" of the data\n        dimensions.\n        """"""\n        self.variables = (x,)  # data: ((N0, ...), C0, ...)\n        x = x.data\n\n        assert isinstance(pool, (tuple, list, np.ndarray)) and all(\n            i >= 0 and isinstance(i, Integral) for i in pool\n        )\n        pool = np.asarray(pool, dtype=int)\n        assert all(i > 0 for i in pool)\n        assert x.ndim >= len(\n            pool\n        ), ""The number of pooled dimensions cannot exceed the dimensionality of the data.""\n\n        stride = (\n            np.array([stride] * len(pool))\n            if isinstance(stride, Integral)\n            else np.asarray(stride, dtype=int)\n        )\n        assert len(stride) == len(pool) and all(\n            s >= 1 and isinstance(s, Integral) for s in stride\n        )\n\n        self.pool = pool  # (P0, ...)\n        self.stride = stride  # (S0, ...)\n\n        num_pool = len(pool)\n        num_no_pool = x.ndim - num_pool\n\n        x_shape = np.array(x.shape[num_no_pool:])\n        w_shape = pool\n\n        out_shape = (x_shape - w_shape) / stride + 1\n\n        if not all(i.is_integer() and i > 0 for i in out_shape):\n            msg = f""Stride and kernel dimensions are incompatible: \\n""\n            msg += f""Input dimensions: {(tuple(x_shape))}\\n""\n            msg += f""Stride dimensions: {(tuple(stride))}\\n""\n            msg += f""Pooling dimensions: {(tuple(w_shape))}\\n""\n            raise ValueError(msg)\n\n        pool_axes = tuple(-(i + 1) for i in range(num_pool))\n\n        # (G0, ...) is the tuple of grid-positions for placing each window (not including stride)\n        # sliding_window_view(x): ((N0, ...), C0, ...)          -> (G0, ..., (N0, ...), P0, ...)\n        # max-pool:               (G0, ..., (N0, ...), P0, ...) -> (G0, ..., (N0, ...))\n        maxed = sliding_window_view(x, self.pool, self.stride).max(axis=pool_axes)\n        axes = tuple(range(maxed.ndim))\n\n        # (G0, ..., (N0, ...)) -> ((N0, ...), G0, ...)\n        out = maxed.transpose(axes[-num_no_pool:] + axes[:-num_no_pool])\n        return out if out.flags[""C_CONTIGUOUS""] else np.ascontiguousarray(out)\n\n    def backward_var(self, grad, index, **kwargs):\n        """""" Parameters\n            ----------\n            grad : numpy.ndarray, shape=((N0, ...), G0, ...),\n            index : int""""""\n        var = self.variables[index]\n        x = var.data\n        num_pool = len(self.pool)\n\n        sl = sliding_window_view(x, self.pool, self.stride)\n        grid_shape = sl.shape\n        maxed = sl.reshape(*sl.shape[:-num_pool], -1).argmax(-1)\n        axes = tuple(range(maxed.ndim))\n\n        # argmax within a given flat-window\n        maxed = maxed.transpose(\n            axes[num_pool:] + axes[:num_pool]\n        )  # ((N0, ...), G0, ...)\n\n        # flat-index offset associated with reshaped window within `x`\n        row_major_offset = tuple(np.cumprod(x.shape[-num_pool:][:0:-1])[::-1]) + (1,)\n\n        # flat index of argmax, updated based on position within window, according to shape of `x`\n        in_window_offset = sum(\n            ind * off\n            for ind, off in zip(np.unravel_index(maxed, self.pool), row_major_offset)\n        )\n\n        # flat-index of strided window placement, relative to `x`\n        window_offset = sum(\n            ind * s * off\n            for ind, s, off in zip(\n                np.indices(grid_shape[:num_pool]), self.stride, row_major_offset\n            )\n        )\n\n        # indices required to traverse pool-axis-flattened array\n        # ((N0, ...) G0*...)\n        flat_grid_shape = (*maxed.shape[:-num_pool], np.prod(maxed.shape[-num_pool:]))\n        index = np.indices(flat_grid_shape)\n\n        # update trailing indices to traverse location of max entries within pooled axes\n        index[-1] = (in_window_offset + window_offset).reshape(\n            *flat_grid_shape[:-1], -1\n        )\n\n        # accumulate gradient within pool-axis-flattened dx, then reshape to match shape of `x`\n        dx = np.zeros(x.shape[:-num_pool] + (np.prod(x.shape[-num_pool:]),))\n        np.add.at(dx, tuple(index), grad.reshape(*x.shape[:-num_pool], -1))\n        return dx.reshape(x.shape)\n\n\ndef max_pool(x, pool, stride, constant=False):\n    """""" Perform max-pooling over the last N dimensions of a data batch.\n\n    Extended Summary\n    ----------------\n    The data consists of N trailing axes to be pooled over, denoted by ``C0, ...``. These\n    can be preceded, optionally, by un-pooled axes, denoted by ``(N0, ...)``. The dimensions\n    of the window over which pooling is performed is denoted by ``P0, ...``. The window\n    is placed with stride values ``S0, ...``.\n\n    Ultimately the pooled channels have a shape ``G0, ...``.\n\n    Parameters\n    ----------\n    x : mygrad.Tensor, shape=([...], C0, ...)\n        The data batch; to be pooled along the trailing axes denoted by ``C0, ...``.\n\n    pool : Tuple[Integral, ...], (P0, ...)\n        The extent of the pooling window along the ``(C0, ...)`` axes, respectively. The\n        length of `pool` determines ``N`` - the number of trailing dimensions to pool over.\n\n    stride : Union[Integral, Tuple[Integral, ...]], (S0, ...)\n        The spacing used to place the pooling window, along ``(P0, ...)`` axes, respectively.\n        If a single value is provided, it is used for all ``N`` pooling axes.\n\n    Returns\n    -------\n    numpy.ndarray, shape=([...], G0, ...)\n        The pooled data batch.\n\n    Notes\n    -----\n    Only ""valid"" placements of the pooling window are permitted - the pooling\n    window cannot extend passed the ""boundaries"" of the data\n    dimensions.\n\n    Examples\n    --------\n    Simple 2D pooling on a 2D tensor. Tiling a 2x2 max-pool window with\n    stride-1 over a shape-(3, 3) tensor ``x``:\n\n    >>> import  mygrad as mg\n    >>> from mygrad.nnet import max_pool\n    >>> x = mg.Tensor([[0., 10.,  8.],\n    ...                [2.,  7.,  3.],\n    ...                [5.,  7., 20.]])\n    >>> out = max_pool(x, pool=(2, 2), stride=1)\n    >>> out\n    Tensor([[ 10., 10.],\n            [  7., 20.]])\n    >>> out.sum().backward()  # sum to reduce to scalar for back-prop\n    >>> x.grad  # dout/dx\n    array([[0., 2., 0.],\n           [0., 1., 0.],\n           [0., 0., 1.]])\n\n    Let\'s perform 1D pooling on a 2D tensor. Each row of the tensor\n    will be pooled over independently. Let\'s apply a size-2 max-pool\n    window to each row of ``x``, using a stride of 1:\n\n    >>> x = mg.Tensor([[0., 10., 8.],\n    ...                [9., 7.,  3.],\n    ...                [5., 0., 20.]])\n    >>> max_pool(x, pool=(2,), stride=1)\n    Tensor([[10., 10.],\n            [ 9.,  7.],\n            [ 5., 20.]])\n\n    Here we perform pooling over the trailing two dimensions of a\n    4D tensor, ``x``. By specifying ``pool = (2, 2)``, we instruct\n    ``max_pool`` to tile a 2x2 pooling window along these last two\n    axes. Let\'s apply the window every two rows, and for each column;\n    i.e. we specify ``stride = (2, 1)``:\n\n    >>> import numpy as np\n    >>> x = mg.Tensor(np.random.rand(10, 3, 12, 12))\n    >>> pool = (2, 2)   # 2x2 pooling over the last axes\n    >>> stride = (2, 1) # Apply 2x1 stride\n    >>> out = max_pool(x, pool, stride)  # max-pooled Tensor\n    >>> out.shape\n    (10, 3, 6, 11)\n\n    Had we specified, say, ``pool = (3, 2, 2)``, then a 3x2x2\n    pooling window would have been tiled along the last *three* axes\n    of ``x``.\n    """"""\n    return Tensor._op(MaxPoolND, x, op_args=(pool, stride), constant=constant)\n'"
src/mygrad/nnet/layers/utils.py,12,"b'from numbers import Integral\n\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\n\n\ndef sliding_window_view(arr, window_shape, step, dilation=None):\n    """""" Create a sliding window view over the trailing dimensions of an array.\n        No copy is made.\n\n        The window is applied only to valid regions of ``arr``, but is applied geedily.\n\n        See Notes section for details.\n\n        Parameters\n        ----------\n        arr : numpy.ndarray, shape=(..., [x, (...), z])\n            C-contiguous array over which sliding view-window is applied along the trailing\n            dimensions ``[x, ..., z]``, as determined by the length of ``window_shape``.\n\n            If ``arr`` is not C-contiguous, it will be replaced by ``numpy.ascontiguousarray(arr)``\n\n        window_shape : Sequence[int]\n            Specifies the shape of the view-window: ``[Wx, (...), Wz]``.\n            The length of `window_shape` determines the length of ``[x, (...) , z]``.\n\n        step : Union[int, Sequence[int]]\n            The step sized used along the ``[x, (...), z]`` dimensions: ``[Sx, (...), Sz]``.\n            If a single integer is specified, a uniform step size is used.\n\n        dilation : Optional[Union[int, Sequence[int]]]\n            The dilation factor used along the ``[x, (...), z]`` directions: ``[Dx, (...), Dz]``.\n            If no value is specified, a dilation factor of 1 is used along each direction.\n            Dilation specifies the step size used when filling the window\'s elements\n\n        Returns\n        -------\n        numpy.ndarray\n            A contiguous view of ``arr``, of shape ``([X, (...), Z], ..., [Wx, (...), Wz])``, where\n            ``[X, ..., Z]`` is the shape of the grid on which the window was applied. See Notes\n            sections for more details.\n\n        Raises\n        ------\n        ValueError, TypeError\n            Invalid step-size, window shape, or dilation\n\n        Notes\n        -----\n        Window placement:\n            Given a dimension of size x, with a window of size W along this dimension, applied\n            with stride S and dilation D, the window will be applied::\n                                      X = (x - (W - 1) * D + 1) // S + 1\n            number of times along that dimension.\n\n        Interpreting output:\n            In general, given an array ``arr`` of shape (..., x, (...), z), and::\n\n                out = sliding_window_view(arr, window_shape=[Wx, (...), Wz], step=[Sx, (...), Sz])\n\n            then indexing ``out`` with ``[ix, (...), iz]`` produces the following view of ``x``::\n\n                out[ix, (...), iz] ==\n                    x[..., ix*Sx:(ix*Sx + Wx*Dx):Dx, (...), iz*Sz:(iz*Sz + Wz*Dz):Dz]\n\n            For example, suppose ``arr`` is an array of shape-(10, 12, 6). Specifying sliding\n            window of shape ``(3, 3)`` with step size ``(2, 2)``, dilation ``(2, 1)`` will create the view::\n\n                            [[arr[:,  0:6:2, 0:3], arr[:,   0:6:3, 3:6]]\n                             [arr[:, 6:12:2, 0:3], arr[:, 6:12:12, 3:6]]]\n\n            producing a view of shape ``(2, 2, 10, 3, 3)`` in total.\n\n        Examples\n        --------\n        >>> import numpy as np\n        >>> x = np.arange(36).reshape(6, 6)\n        >>> x\n        array([[ 0,  1,  2,  3,  4,  5],\n               [ 6,  7,  8,  9, 10, 11],\n               [12, 13, 14, 15, 16, 17],\n               [18, 19, 20, 21, 22, 23],\n               [24, 25, 26, 27, 28, 29],\n               [30, 31, 32, 33, 34, 35]])\n\n        Apply an 3x2 window with step-sizes of (2, 2). This results in\n        the window being placed twice along axis-0 and three times along axis-1.\n\n        >>> y = sliding_window_view(x, step=(2, 2), window_shape=(3, 2))\n        >>> y.shape\n        (2, 3, 3, 2)\n\n        window applied at (0, 0)\n\n        >>> y[0, 0]\n        array([[ 0,  1],\n               [ 6,  7],\n               [12, 13]])\n\n        window applied at (2, 0)\n\n        >>> y[1, 0]\n        array([[12, 13],\n               [18, 19],\n               [24, 25]])\n\n        window applied at (0, 2)\n\n        >>> y[0, 1]\n        array([[ 2,  3],\n               [ 8,  9],\n               [14, 15]])\n\n        verify that an element in this window-view is correct\n\n        >>> i, j = np.random.randint(0, 2, size=2)\n        >>> wx, wy = (2, 2)\n        >>> sx, sy = (2, 2)\n        >>> np.all(y[i, j] == x[..., i*sx:(i*sx + wx), j*sy:(j*sy + wy)])\n        True\n        """"""\n\n    if not hasattr(window_shape, ""__iter__""):\n        raise TypeError(\n            f""`window_shape` must be a sequence of positive integers, got: {window_shape}""\n        )\n    window_shape = tuple(window_shape)\n    if not all(isinstance(i, Integral) and i > 0 for i in window_shape):\n        raise TypeError(\n            f""`window_shape` must be a sequence of positive integers, ""\n            f""got: {window_shape}""\n        )\n\n    if len(window_shape) > arr.ndim:\n        raise ValueError(\n            f""`window_shape` ({window_shape}) cannot specify more values than ""\n            f""`arr.ndim` ({arr.ndim}).""\n        )\n\n    if not isinstance(step, Integral) and not hasattr(step, ""__iter__""):\n        raise TypeError(\n            f""`step` must be a positive integer or a sequence of positive ""\n            f""integers, got: {step}""\n        )\n\n    step = (\n        (int(step),) * len(window_shape) if isinstance(step, Integral) else tuple(step)\n    )\n\n    if not all(isinstance(i, Integral) and i > 0 for i in step):\n        raise ValueError(\n            f""`step` must be a positive integer or a sequence of positive ""\n            f""integers, got: {step}""\n        )\n\n    if any(i > j for i, j in zip(window_shape[::-1], arr.shape[::-1])):\n        raise ValueError(\n            f""Each size of the window-shape must fit within the trailing ""\n            f""dimensions of `arr`.""\n            f""{window_shape} does not fit in {arr.shape[-len(window_shape) :]}""\n        )\n\n    if (\n        dilation is not None\n        and not isinstance(dilation, Integral)\n        and not hasattr(dilation, ""__iter__"")\n    ):\n        raise TypeError(\n            f""`dilation` must be None, a positive integer, or a sequence of ""\n            f""positive integers, got: {dilation}""\n        )\n    if dilation is None:\n        dilation = np.ones((len(window_shape),), dtype=int)\n    else:\n        if isinstance(dilation, Integral):\n            dilation = np.full((len(window_shape),), fill_value=dilation, dtype=int)\n        else:\n            np.asarray(dilation)\n\n        if not all(isinstance(i, Integral) and i > 0 for i in dilation) or len(\n            dilation\n        ) != len(window_shape):\n            raise ValueError(\n                f""`dilation` must be None, a positive integer, or a sequence of ""\n                f""positive integers with the same length as `window_shape` ""\n                f""({window_shape}), got: {dilation}""\n            )\n        if any(\n            w * d > s\n            for w, d, s in zip(window_shape[::-1], dilation[::-1], arr.shape[::-1])\n        ):\n            raise ValueError(\n                f""The dilated window ({tuple(w * d for w, d in zip(window_shape, dilation))}) ""\n                f""must fit within the trailing ""\n                f""dimensions of `arr` ({arr.shape[-len(window_shape) :]})""\n            )\n\n    if not arr.flags[""C_CONTIGUOUS""]:\n        arr = np.ascontiguousarray(arr)\n\n    step = np.array(step)  # (Sx, ..., Sz)\n    window_shape = np.array(window_shape)  # (Wx, ..., Wz)\n    in_shape = np.array(arr.shape[-len(step) :])  # (x, ... , z)\n    nbyte = arr.strides[-1]  # size, in bytes, of element in `arr`\n\n    # per-byte strides required to fill a window\n    win_stride = tuple(np.cumprod(arr.shape[:0:-1])[::-1]) + (1,)\n\n    # per-byte strides required to advance the window\n    step_stride = tuple(win_stride[-len(step) :] * step)\n\n    # update win_stride to accommodate dilation\n    win_stride = np.array(win_stride)\n    win_stride[-len(step) :] *= dilation\n    win_stride = tuple(win_stride)\n\n    # tuple of bytes to step to traverse corresponding dimensions of view\n    # see: \'internal memory layout of an ndarray\'\n    stride = tuple(int(nbyte * i) for i in step_stride + win_stride)\n\n    # number of window placements along x-dim: X = (x - (Wx - 1)*Dx + 1) // Sx + 1\n    out_shape = tuple((in_shape - ((window_shape - 1) * dilation + 1)) // step + 1)\n\n    # ([X, (...), Z], ..., [Wx, (...), Wz])\n    out_shape = out_shape + arr.shape[: -len(step)] + tuple(window_shape)\n    out_shape = tuple(int(i) for i in out_shape)\n\n    return as_strided(arr, shape=out_shape, strides=stride, writeable=False)\n'"
src/mygrad/nnet/losses/__init__.py,0,"b'from .margin_ranking_loss import margin_ranking_loss\nfrom .multiclass_hinge import multiclass_hinge\nfrom .softmax_crossentropy import softmax_crossentropy\n\n__all__ = [""margin_ranking_loss"", ""multiclass_hinge"", ""softmax_crossentropy""]\n'"
src/mygrad/nnet/losses/_utils.py,2,"b'import numpy as np\n\nfrom mygrad.tensor_base import Tensor\n\n\ndef check_loss_inputs(x, y_true):\n    """""" Ensures that the inputs to scores-truth style loss functions\n    are of the correct shapes and types.\n\n    Parameters\n    ----------\n    x : mygrad.Tensor, shape=(N, C)\n        The C class scores for each of the N pieces of data.\n\n    y_true : Sequence[int]\n        The correct class-indices, in [0, C), for each datum.\n\n    Raises\n    ------\n    TypeError\n        `y_true` must be an integer-type array-like object\n    ValueError\n        `x` must be a 2-dimensional array-like object\n        `y_true` must be a shape-(N,) array-like object\n    """"""\n    if not x.ndim == 2:\n        raise ValueError(\n            ""`x` must be a 2-dimensional array-like object, got {}-dim"".format(x.ndim)\n        )\n\n    if isinstance(y_true, Tensor):\n        y_true = y_true.data\n\n    y_true = np.asarray(y_true)\n    if not np.issubdtype(y_true.dtype, np.integer):\n        raise TypeError(\n            ""`y_true` must be an integer-type ""\n            ""array-like object, got {}"".format(y_true.dtype)\n        )\n\n    if y_true.ndim != 1 or y_true.shape[0] != x.shape[0]:\n        raise ValueError(\n            ""`y_true` must be a shape-(N,) array: \\n""\n            ""\\tExpected shape-{}\\n""\n            ""\\tGot shape-{}"".format((x.shape[0],), y_true.shape)\n        )\n'"
src/mygrad/nnet/losses/margin_ranking_loss.py,6,"b'from numbers import Real\n\nimport numpy as np\n\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\n\nclass MarginRanking(Operation):\n    def __call__(self, x1, x2, y, margin):\n        """""" Computes the margin ranking loss between ``x1``\n        and ``x2``.\n\n        Parameters\n        ----------\n        x1 : mygrad.Tensor, shape=(N,) or (N, D)\n\n        x2 : mygrad.Tensor, shape=(N,) or (N, D)\n\n        y : numpy.ndarray\n\n        margin : float\n\n        Returns\n        -------\n        numpy.ndarray, shape=()\n        """"""\n        self.variables = (x1, x2)\n        x1 = x1.data\n        x2 = x2.data\n\n        self.y = y\n\n        M = margin - self.y * (x1 - x2)\n        not_thresh = M <= 0\n        loss = M\n        loss[not_thresh] = 0.0\n\n        self._grad = np.ones_like(M)\n        self._grad[not_thresh] = 0.0\n        self._grad /= M.size\n        return np.mean(loss)\n\n    def backward_var(self, grad, index, **kwargs):\n        sign = -self.y if index == 0 else self.y\n        return grad * (sign * self._grad)\n\n\ndef margin_ranking_loss(x1, x2, y, margin, constant=False):\n    r""""""Computes the margin average margin ranking loss.\n    Equivalent to::\n\n    >>> import mygrad as mg\n    >>> mg.mean(mg.maximum(0, margin - y * (x1 - x2)))\n\n    Parameters\n    ----------\n    x1 : array_like, shape=(N,) or (N, D)\n        A batch of scores or descriptors to compare against those in `x2`\n\n    x2 : array_like, shape=(N,) or (N, D)\n        A batch of scores or descriptors to compare against those in `x1`\n\n    y  : Union[int, array_like], scalar or shape=(N,)\n        1 or -1. Specifies whether the margin is compared against `(x1 - x2)`\n        or `(x2 - x1)`, for each of the N comparisons.\n\n    margin : float\n        A non-negative value to be used as the margin for the loss.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor, shape=()\n        The mean margin ranking loss.\n    """"""\n    if not 0 < x1.ndim < 3:\n        raise ValueError(""`x1` must have shape (N,) or (N, D)"")\n    if not x1.shape == x2.shape:\n        raise ValueError(""`x1` and `x2` must have the same shape"")\n    if not np.issubdtype(x1.dtype, np.floating):\n        raise TypeError(""`x1` must contain floats"")\n    if not np.issubdtype(x2.dtype, np.floating):\n        raise TypeError(""`x2` must contain floats"")\n    if not isinstance(margin, Real) or margin < 0:\n        raise ValueError(""`margin` must be a non-negative scalar"")\n\n    if isinstance(y, Tensor):\n        y = y.data\n\n    y = np.asarray(y)\n\n    if y.size == 1:\n        y = np.array(y.item())\n\n    if not y.ndim == 0 and not (y.ndim == 1 and len(y) == len(x1)):\n        raise ValueError(""`y` must be a scalar or shape-(N,) array of ones"")\n\n    if y.ndim:\n        if x1.ndim == 2:\n            y = y.reshape(-1, 1)\n    return Tensor._op(MarginRanking, x1, x2, op_args=(y, margin), constant=constant)\n'"
src/mygrad/nnet/losses/multiclass_hinge.py,4,"b'import numpy as np\n\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\nfrom ._utils import check_loss_inputs\n\n\nclass MulticlassHinge(Operation):\n    def __call__(self, a, y, hinge=1.0):\n        """""" Computes the average multiclass hinge loss.\n\n        Parameters\n        ----------\n        a : mygrad.Tensor, shape=(N, C)\n            The C class scores for each of the N pieces of data.\n\n        y : numpy.ndarray, shape=(N,)\n            The correct class-index, in [0, C), for each datum.\n\n        Returns\n        -------\n        The average multiclass hinge loss\n\n        Raises\n        ------\n        TypeError\n            `y_true` must be an integer-type array-like object\n\n        ValueError\n            `x` must be a 2-dimensional array-like object\n            `y_true` must be a shape-(N,) array-like object""""""\n\n        check_loss_inputs(a, y)\n        self.variables = (a,)\n        scores = a.data\n        correct_labels = (range(len(y)), y)\n        correct_class_scores = scores[correct_labels]  # Nx1\n\n        M = scores - correct_class_scores[:, np.newaxis] + hinge  # NxC margins\n        not_thresh = np.where(M <= 0)\n        Lij = M\n        Lij[not_thresh] = 0\n        Lij[correct_labels] = 0\n\n        TMP = np.ones(M.shape, dtype=float)\n        TMP[not_thresh] = 0\n        TMP[correct_labels] = 0  # NxC; 1 where margin > 0\n        TMP[correct_labels] = -1 * TMP.sum(axis=-1)\n        self.back = TMP\n        self.back /= scores.shape[0]\n        return np.sum(Lij) / scores.shape[0]\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad * self.back\n\n\ndef multiclass_hinge(x, y_true, hinge=1.0, constant=False):\n    """""" Computes the average multiclass hinge loss.\n\n    Parameters\n    ----------\n    x : array_like, shape=(N, K)\n        The K class scores for each of the N pieces of data.\n\n    y_true : array_like, shape=(N,)\n        The correct class-indices, in [0, K), for each datum.\n\n    hinge : float\n        The size of the ""hinge"" outside of which a nonzero loss\n        is incurred.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    The average multiclass hinge loss\n\n    Raises\n    ------\n    TypeError\n        `y_true` must be an integer-type array-like object\n\n    ValueError\n        `x` must be a 2-dimensional array-like object\n        `y_true` must be a shape-(N,) array-like object\n    """"""\n    return Tensor._op(MulticlassHinge, x, op_args=(y_true, hinge), constant=constant)\n'"
src/mygrad/nnet/losses/softmax_crossentropy.py,2,"b'import numpy as np\n\nfrom mygrad.math._special import logsumexp\nfrom mygrad.operation_base import Operation\nfrom mygrad.tensor_base import Tensor\n\nfrom ._utils import check_loss_inputs\n\n\nclass SoftmaxCrossEntropy(Operation):\n    """""" Given the classification scores of C classes for N pieces of data,\n        computes the NxC softmax classification probabilities. The\n        cross entropy is then computed by using the true classification labels.\n\n        log-softmax is used for improved numerical stability""""""\n\n    def __call__(self, x, y_true):\n        """""" Parameters\n            ----------\n            x : mygrad.Tensor, shape=(N, C)\n                The C class scores for each of the N pieces of data.\n\n            y_true : Sequence[int]\n                The correct class-indices, in [0, C), for each datum.\n\n            Returns\n            -------\n            The average softmax loss""""""\n        if isinstance(y_true, Tensor):\n            y_true = y_true.data\n\n        check_loss_inputs(x, y_true)\n        self.variables = (x,)\n        scores = x.data\n        log_softmax = scores - logsumexp(scores, axis=-1, keepdims=True)\n        label_locs = (range(len(scores)), y_true)\n        loss = -np.sum(log_softmax[label_locs]) / scores.shape[0]\n\n        self.back = np.exp(log_softmax)\n        self.back[label_locs] -= 1.0\n        self.back /= scores.shape[0]\n        return loss\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad * self.back\n\n\ndef softmax_crossentropy(x, y_true, constant=False):\n    r"""""" Given the classification scores of C classes for N pieces of data,\n\n    computes the NxC softmax classification probabilities. The\n    cross entropy is then computed by using the true classification labels.\n\n    log-softmax is used for improved numerical stability.\n\n    Parameters\n    ----------\n    x : array_like, shape=(N, C)\n        The C class scores for each of the N pieces of data.\n\n    y_true : array_like, shape=(N,)\n        The correct class-indices, in [0, C), for each datum.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    The average softmax loss\n\n    Raises\n    ------\n    ValueError\n        Bad dimensionalities for ``x`` or ``y_true``\n\n    Notes\n    -----\n    - :math:`N` is the number of samples in the batch.\n    - :math:`C` is the number of possible classes for which scores are provided.\n\n    Given the shape-:math:`(N, C)` tensor of scores, ``x``, the softmax classification\n    probabilities are computed. That is, the score for class-:math:`k` of a given datum\n    (:math:`s_{k}`) is normalized using the \'softmax\' transformation:\n\n    .. math::\n\n        p_{k} = \\frac{e^{s_k}}{\\sum_{i=1}^{C}{e^{s_i}}}\n\n    This produces the ""prediction probability distribution"", :math:`p`, for each datum.\n    The cross-entropy loss for that datum is then computed according to the true class-index\n    for that datum, as reported in ``y_true``. That is the ""true probability distribution"",\n    :math:`t`, for the datum is :math:`1` for the correct class-index and :math:`0` elsewhere.\n\n    The cross-entropy loss for that datum is thus:\n\n    .. math::\n       l = - \\sum_{k=1}^{C}{t_{k} \\log{p_{k}}}\n\n    Having computed each per-datum cross entropy loss, this function then returns the loss\n    averaged over all :math:`N` pieces of data:\n\n    .. math::\n\n       L = \\frac{1}{N}\\sum_{i=1}^{N}{l_{i}}\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> from mygrad.nnet import softmax_crossentropy\n\n    Let\'s take a simple case where N=1, and C=3. We\'ll thus make up classification\n    scores for a single datum. Suppose the scores are identical for the three classes\n    and that the true class is class-0:\n\n    >>> x = mg.Tensor([[2., 2., 2.]])  # a shape-(1, 3) tensor of scores\n    >>> y_true = mg.Tensor([0])  # the correct class for this datum is class-0\n\n    Because the scores are identical for all three classes, the softmax normalization\n    will simply produce :math:`p = [\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}]`. Because\n    class-0 is the ""true"" class, :math:`t = [1., 0., 0.]`. Thus our softmax cross-entropy\n    loss should be:\n\n    .. math::\n      -(1 \\times \\log{\\frac{1}{3}} + 0 \\times \\log{\\frac{1}{3}} + 0 \\times \\log{\\frac{1}{3}})\n      = \\log(3) \\approx 1.099\n\n    Let\'s see that this is what ``softmax_crossentropy`` returns:\n\n    >>> softmax_crossentropy(x, y_true)\n    Tensor(1.09861229)\n\n    Similarly, suppose a datum\'s scores are :math:`[0, 0, 10^6]`, then the softmax normalization\n    will return :math:`p \\approx [0., 0., 1.]`. If the true class for this datum is class-2, then\n    the loss should be nearly 0, since :math:`p` and :math:`t` are essentially identical:\n\n    .. math::\n      -(0 \\times \\log{0} + 0 \\times \\log{0} + 1 \\times \\log{1})\n      = -\\log(1) = 0\n\n    Now, let\'s construct ``x`` and ``y_true`` so that they incorporate the scores/labels for\n    both of the data that we have considered:\n\n    >>> x = mg.Tensor([[2., 2.,  2.],  # a shape-(2, 3) tensor of scores\n    ...                [0., 0., 1E6]])\n    >>> y_true = mg.Tensor([0, 2])     # the class IDs for the two data\n\n    ``softmax_crossentropy(x, y_true)`` will return the average loss of these two data,\n    :math:`\\frac{1}{2}(1.099 + 0) \\approx 0.55`:\n\n    >>> softmax_crossentropy(x, y_true)\n    Tensor(0.54930614)\n    """"""\n    return Tensor._op(SoftmaxCrossEntropy, x, op_args=(y_true,), constant=constant)\n'"
src/mygrad/tensor_manip/array_shape/__init__.py,0,b''
src/mygrad/tensor_manip/array_shape/funcs.py,0,"b'from mygrad.tensor_base import Tensor\n\nfrom .ops import *\n\n__all__ = [""reshape"", ""squeeze"", ""ravel"", ""expand_dims"", ""broadcast_to""]\n\n\ndef reshape(a, *newshape, constant=False):\n    """""" Returns a tensor with a new shape, without changing its data.\n\n        This docstring was adapted from ``numpy.reshape``\n\n        Parameters\n        ----------\n        a : array_like\n            The tensor to be reshaped\n\n        *newshape : Union[int, Tuple[int, ...]]\n            The new shape should be compatible with the original shape. If\n            an integer, then the result will be a 1-D tensor of that length.\n            One shape dimension can be -1. In this case, the value is\n            inferred from the length of the tensor and remaining dimensions.\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor\n            ``a`` with its shape changed permuted.  A new tensor is returned.\n\n        Notes\n        -----\n        ``reshape`` utilizes C-ordering, meaning that it reads & writes elements using\n        C-like index ordering; the last axis index changing fastest, and, proceeding\n        in reverse order, the first axis index changing slowest. \n            \n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> a = mg.Tensor([[1,2,3], [4,5,6]])\n        >>> mg.reshape(a, 6)\n        Tensor([1, 2, 3, 4, 5, 6])\n\n        >>> mg.reshape(a, (3,-1))   # the unspecified value is inferred to be 2\n        Tensor([[1, 2],\n                [3, 4],\n                [5, 6]])""""""\n    if not newshape:\n        raise TypeError(""reshape() takes at least 1 argument (0 given)"")\n    if hasattr(newshape[0], ""__iter__""):\n        if len(newshape) > 1:\n            raise TypeError(""an integer is required"")\n        newshape = newshape[0]\n    return Tensor._op(Reshape, a, op_args=(newshape,), constant=constant)\n\n\ndef squeeze(a, axis=None, constant=False):\n    """"""\n    Remove single-dimensional entries from the shape of a tensor.\n\n    This docstring was adapted from ``numpy.squeeze``\n\n    Parameters\n    ----------\n    a : array_like\n        The tensor to be reshaped\n    \n    axis : Optional[int, Tuple[int, ...]]\n        Selects a subset of the single-dimensional entries in the \n        shape. If an axis is selected with shape entry greater than \n        one, an error is raised.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Raises\n    ------\n    ValueError\n        If ``axis`` is not ``None``, and an axis being squeezed is not of length 1\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> x = mg.Tensor([[[0], [1], [2]]])\n    >>> x.shape\n    (1, 3, 1)\n    >>> mg.squeeze(x).shape\n    (3,)\n    >>> mg.squeeze(x, axis=0).shape\n    (3, 1)\n    >>> mg.squeeze(x, axis=1).shape\n    Traceback (most recent call last):\n    ...\n    ValueError: cannot select an axis to squeeze out which has size not equal to one\n    >>> mg.squeeze(x, axis=2).shape\n    (1, 3)""""""\n    return Tensor._op(Squeeze, a, op_args=(axis,), constant=constant)\n\n\ndef ravel(a, constant=False):\n    """"""\n    Flattens contents of a tensor into a contiguous 1-D array.  A copy is made only if needed.\n\n    This docstring was adapted from ``numpy.ravel``.\n\n    Parameters\n    ----------\n    a : array_like\n        The tensor to be flattened\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Notes\n    -----\n    ``ravel`` utilizes C-ordering, meaning that it reads & writes elements using\n    C-like index ordering; the last axis index changing fastest, and, proceeding\n    in reverse order, the first axis index changing slowest.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> x = mg.Tensor([[1, 2],\n    ...                [3, 4]])\n    >>> mg.ravel(x)\n    Tensor([1, 2, 3, 4])\n    """"""\n    return Tensor._op(Ravel, a, constant=constant)\n\n\ndef expand_dims(a, axis, constant=False):\n    """"""\n    Expand the dimensions of a tensor by adding a new axis.\n\n    This docstring was adapted from ``numpy.expand_dims``.\n\n    Parameters\n    ----------\n    a : array_like\n        The tensor to be expanded\n\n    axis : int\n        The position of the new axis in the expanded array shape.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> x = mg.Tensor([1, 2])\n    >>> x.shape\n    (2,)\n    >>> y = mg.expand_dims(x, 1)\n    >>> y.shape\n    (2, 1)\n    >>> z = mg.expand_dims(y, 0)\n    >>> z.shape\n    (1, 2, 1)\n    """"""\n    return Tensor._op(ExpandDims, a, op_args=(axis,), constant=constant)\n\n\ndef broadcast_to(a, shape, constant=False):\n    """"""\n    Broadcast a tensor to a new shape.\n\n    This docstring was adapted from ``numpy.broadcast_to``.\n\n    Parameters\n    ----------\n    a : array_like\n        The tensor to be broadcasted\n\n    shape: Tuple[int, ...]\n        The shape of the broadcasted tensor. This shape\n        should be broadcast-compatible with the original\n        shape.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it\n        does not back-propagate a gradient)\n\n    Returns\n    -------\n    mygrad.Tensor\n\n    Raises\n    ------\n    ValueError\n        If the array is not compatible with the new shape\n        according to Numpy\'s broadcasting rules.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> x = mg.Tensor([1, 2, 3])\n    >>> mg.broadcast_to(x, (3,3))\n    Tensor([[1, 2, 3],\n            [1, 2, 3],\n            [1, 2, 3]])\n    >>> mg.broadcast_to(x, (4,4))\n    Traceback (most recent call last):\n    ...\n    ValueError: operands could not be broadcast together with remapped\n    shapes [original->remapped]: (3,) and requested shape (4,4)\n    """"""\n    return Tensor._op(BroadcastTo, a, op_args=(shape,), constant=constant)\n'"
src/mygrad/tensor_manip/array_shape/ops.py,4,"b'import numpy as np\n\nfrom mygrad.operation_base import BroadcastableOp, Operation\n\n__all__ = [""Reshape"", ""Flatten"", ""Squeeze"", ""Ravel"", ""ExpandDims"", ""BroadcastTo""]\n\n\nclass Reshape(Operation):\n    def __call__(self, a, shape):\n        """""" Parameters\n            ----------\n            a : mygrad.Tensor\n            shape : Tuple[int, ...]""""""\n        self.variables = (a,)\n        return a.data.reshape(shape)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad.reshape(*a.shape)\n\n\nclass Squeeze(Operation):\n    def __call__(self, a, axis):\n        """""" Parameters\n            ----------\n            axis : Optional[int, Tuple[int, ...]] """"""\n        self.variables = (a,)\n        return np.squeeze(a.data, axis=axis)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad.reshape(a.shape)\n\n\nclass Flatten(Operation):\n    def __call__(self, a):\n        """""" Parameters\n            ----------\n            a : mygrad.Tensor""""""\n        self.variables = (a,)\n        return a.data.flatten(order=""C"")\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad.reshape(a.shape)\n\n\nclass Ravel(Operation):\n    def __call__(self, a):\n        """""" Parameters\n            ----------\n            a : mygrad.Tensor""""""\n        self.variables = (a,)\n        return np.ravel(a.data, order=""C"")\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad.reshape(a.shape)\n\n\nclass ExpandDims(Operation):\n    def __call__(self, a, axis):\n        """""" Parameters\n            ----------\n            a : mygrad.Tensor\n            axis : int """"""\n        self.variables = (a,)\n        return np.expand_dims(a.data, axis=axis)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        return grad.reshape(a.shape)\n\n\nclass BroadcastTo(BroadcastableOp):\n    def __call__(self, a, shape):\n        """""" Parameters\n            ----------\n            a : mygrad.Tensor\n            shape : Tuple[int, ...]""""""\n        self.variables = (a,)\n        return np.broadcast_to(a.data, shape=shape)\n\n    def backward_var(self, grad, index, **kwargs):\n        if index != 0:  # pragma: no cover\n            raise IndexError(\n                f""`broadcast_to` is a unary operation. ""\n                f""`backward_var` was called for index {index}""\n            )\n        return grad\n'"
src/mygrad/tensor_manip/tiling/__init__.py,0,b''
src/mygrad/tensor_manip/tiling/funcs.py,0,"b'from typing import Optional, Sequence, Union\n\nfrom mygrad.tensor_base import Tensor\n\nfrom .ops import Repeat\n\n__all__ = [""repeat""]\n\n\ndef repeat(\n    a,\n    repeats: Union[int, Sequence[int]],\n    axis: Optional[int] = None,\n    constant: bool = False,\n) -> Tensor:\n    """"""\n    Repeat elements of a tensor.\n\n    This docstring was adapted from ``numpy.repeat``\n\n    Parameters\n    ----------\n    a : array_like\n        Input tensor.\n\n    repeats : Union[int, Sequence[int]]\n        The number of repetitions for each element. ``repeats``\n        is broadcasted to fit the shape of the given axis.\n\n    axis : Optional[int]\n        The axis along which to repeat values. By default, use the\n        flattened input array, and return a flat output tensor.\n\n    constant : bool, optional(default=False)\n        If ``True``, the returned tensor is a constant (it does not\n        back-propagate a gradient).\n\n    Returns\n    -------\n    repeated_tensor : Tensor\n        Output tensor which has the same shape as `a`, except along\n        the given axis.\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> mg.repeat(3, 4)\n    Tensor([3, 3, 3, 3])\n    >>> x = mg.Tensor([[1, 2], [3, 4]])\n    >>> mg.repeat(x, 2)\n    Tensor([1, 1, 2, 2, 3, 3, 4, 4])\n    >>> mg.repeat(x, 3, axis=1)\n    Tensor([[1, 1, 1, 2, 2, 2],\n            [3, 3, 3, 4, 4, 4]])\n    >>> mg.repeat(x, [1, 2], axis=0)\n    Tensor([[1, 2],\n            [3, 4],\n            [3, 4]])\n    """"""\n    return Tensor._op(Repeat, a, op_args=(repeats, axis), constant=constant)\n'"
src/mygrad/tensor_manip/tiling/ops.py,7,"b'from typing import Optional, Sequence, Union\n\nimport numpy as np\n\nfrom mygrad.nnet.layers.utils import sliding_window_view\nfrom mygrad.operation_base import BroadcastableOp\nfrom mygrad.tensor_base import Tensor\n\n__all__ = [""Repeat""]\n\n\nclass Repeat(BroadcastableOp):\n    # Repeat can broadcast in the case:\n    #    repeat(1, 2) -> [1 1]\n    scalar_only = True  # type: bool\n\n    def __call__(\n        self, a: Tensor, repeats: Union[int, Sequence[int]], axis: Optional[int] = None\n    ):\n        self.variables = (a,)\n        self._axis = axis\n        self._repeats = repeats\n        return np.repeat(a.data, repeats=repeats, axis=axis)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index].data  # type: np.ndarray\n        if isinstance(self._repeats, int) or len(self._repeats) == 1:\n            if not isinstance(self._repeats, int):\n                (self._repeats,) = self._repeats\n\n            if not self._repeats:\n                # skip accumulation if `repeats` is all zeros\n                return np.zeros(a.shape, dtype=grad.dtype)\n\n            if self._axis is None:\n                # input array was treated as if it was flattened\n                grad = grad.ravel()\n                window_shape = (self._repeats,)\n            else:\n                # if `a` is a scalar, we will treat it like a\n                # 1D array, since the `repeat` op did broadcasting\n                window_shape = [1] * max(1, a.ndim)\n                window_shape[self._axis] = self._repeats\n                window_shape = tuple(window_shape)\n\n            # Create windowed view of gradient, where each window\n            # extends/strides along the repeated axis, and with a\n            # window size given by `repeats`. Thus summing over the\n            # trailing window dimensions accumulates the gradient\n            # to the appropriate ""source"" entries of the input tensor\n            grad = sliding_window_view(\n                grad, window_shape=window_shape, step=window_shape\n            )\n            grad = grad.sum(axis=tuple(range(-len(window_shape), 0)))\n\n            if self._axis is None:\n                grad.shape = a.shape\n            return grad\n        else:\n            # We will create a grid of flat-indices commensurate with the\n            # original input array. These will be used accumulate the incoming\n            # gradient into the appropriate tensor elements.\n            #\n            # In order to deal with the flexibility of specifying multiple\n            # distinct repeat values, we will perform the identical repeat\n            # operation on the grid of flat-indices. Thus making them\n            # commensurate with the incoming gradient. `add.at` then makes\n            # short work of accumulating the incoming gradient as appropriate\n            out_grad = np.zeros((a.size,), dtype=grad.dtype)\n            indices = np.arange(a.size).reshape(a.shape)\n            indices = np.repeat(indices, repeats=self._repeats, axis=self._axis)\n            np.add.at(out_grad, indices.ravel(), grad.ravel())\n            return out_grad.reshape(a.shape)\n'"
src/mygrad/tensor_manip/transpose_like/__init__.py,0,b''
src/mygrad/tensor_manip/transpose_like/funcs.py,1,"b'from mygrad.tensor_base import Tensor\n\nfrom .ops import MoveAxis, Roll, SwapAxes, Transpose\n\n__all__ = [""transpose"", ""moveaxis"", ""swapaxes"", ""roll""]\n\n\ndef transpose(a, *axes, constant=False):\n    """""" Permute the dimensions of a tensor.\n\n        Parameters\n        ----------\n        a : array_like\n            The tensor to be transposed\n\n        axes : Optional[Tuple[int]]\n            By default, reverse the dimensions, otherwise permute the axes\n            according to the values given.\n\n        Returns\n        -------\n        mygrad.Tensor\n            `a` with its axes permuted.  A new tensor is returned.\n\n        Examples\n        --------\n        >>> import mygrad as mg\n        >>> a = mg.Tensor([[1, 2], [3, 4]])\n        >>> a\n        Tensor([[1, 2],\n                [3, 4]])\n        >>> a.transpose()\n        Tensor([[1, 3],\n                [2, 4]])\n        >>> a.transpose((1, 0))\n        Tensor([[1, 3],\n                [2, 4]])\n        >>> a.transpose(1, 0)\n        Tensor([[1, 3],\n                [2, 4]]) """"""\n    if not axes:\n        axes = None\n    elif hasattr(axes[0], ""__iter__"") or axes[0] is None:\n        if len(axes) > 1:\n            raise TypeError(\n                f""\'{type(axes[0])}\' object cannot be interpreted as an integer""\n            )\n        axes = axes[0]\n    return Tensor._op(Transpose, a, op_args=(axes,), constant=constant)\n\n\ndef moveaxis(a, source, destination, constant=False):\n    """""" Move axes of a tensor to new positions. Other axes remain in their\n        original order.\n\n\n        Parameters\n        ----------\n        a : array_like\n            The array whose axes should be reordered.\n\n        source : Union[int, Sequence[int]]\n            Original positions of the axes to move. These must be unique.\n\n        destination : Union[int, Sequence[int]]\n            Destination positions for each of the original axes. These must also be\n            unique.\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        result : mygrad.Tensor\n            Array with moved axes. This array is a view of the input array..\n\n        Examples\n        --------\n        >>> from mygrad import Tensor, moveaxis\n        >>> x = Tensor(np.zeros((3, 4, 5)))\n        >>> moveaxis(x, 0, -1).shape\n        (4, 5, 3)\n        >>> moveaxis(x, -1, 0).shape\n        (5, 3, 4)\n        >>> moveaxis(x, [0, 1], [-1, -2]).shape\n        (5, 4, 3) """"""\n    return Tensor._op(MoveAxis, a, op_args=(source, destination), constant=constant)\n\n\ndef swapaxes(a, axis1, axis2, constant=False):\n    """""" Interchange two axes of a tensor.\n\n        Parameters\n        ----------\n        a : array_like\n            Input array.\n\n        axis1 : int\n            First axis.\n\n        axis2 : int\n            Second axis.\n\n        constant : bool, optional(default=False)\n            If ``True``, the returned tensor is a constant (it\n            does not back-propagate a gradient)\n\n        Returns\n        -------\n        mygrad.Tensor\n\n        Examples\n        --------\n        >>> from mygrad import Tensor, swapaxes\n        >>> x = Tensor([[1, 2, 3]])\n        >>> swapaxes(x, 0, 1)\n        Tensor([[1],\n               [2],\n               [3]])\n        >>> x = Tensor([[[0, 1], [2, 3]], [[4, 5], [6, 7]]])\n        >>> x\n        Tensor([[[0, 1],\n                [2, 3]],\n               [[4, 5],\n                [6, 7]]])\n        >>> swapaxes(x, 0, 2)\n        Tensor([[[0, 4],\n                [2, 6]],\n               [[1, 5],\n                [3, 7]]])\n    """"""\n    return Tensor._op(SwapAxes, a, op_args=(axis1, axis2), constant=constant)\n\n\ndef roll(a, shift, axis=None, constant=False):\n    """"""\n    Roll tensor elements along a given axis.\n\n    Elements that roll beyond the end of an axis ""wrap back around"" to the beginning.\n\n    This docstring was adapted from ``numpy.roll``\n\n    Parameters\n    ----------\n    a : array_like\n        Input tensor.\n\n    shift : Union[int, Tuple[int, ...]]\n        The number of places by which elements are shifted.  If a tuple,\n        then `axis` must be a tuple of the same size, and each of the\n        given axes is shifted by the corresponding number.  If an int\n        while `axis` is a tuple of ints, then the same value is used for\n        all given axes.\n\n    axis : Optional[Union[int, Tuple[int, ...]]]\n        Axis or axes along which elements are shifted.  By default, the\n        array is flattened before shifting, after which the original\n        shape is restored.\n\n    Returns\n    -------\n    res : Tensor\n        Output array, with the same shape as `a`.\n\n\n    Examples\n    --------\n    >>> import mygrad as mg\n    >>> x = mg.arange(10)\n    >>> mg.roll(x, 2)\n    Tensor([8, 9, 0, 1, 2, 3, 4, 5, 6, 7])\n    >>> x2 = mg.reshape(x, (2,5))\n    >>> x2\n    Tensor([[0, 1, 2, 3, 4],\n           [5, 6, 7, 8, 9]])\n    >>> mg.roll(x2, 1)\n    Tensor([[9, 0, 1, 2, 3],\n           [4, 5, 6, 7, 8]])\n    >>> mg.roll(x2, 1, axis=0)\n    Tensor([[5, 6, 7, 8, 9],\n           [0, 1, 2, 3, 4]])\n    >>> mg.roll(x2, 1, axis=1)\n    Tensor([[4, 0, 1, 2, 3],\n           [9, 5, 6, 7, 8]])\n    """"""\n    return Tensor._op(\n        Roll, a, op_kwargs=dict(shift=shift, axis=axis), constant=constant\n    )\n'"
src/mygrad/tensor_manip/transpose_like/ops.py,8,"b'import numpy as np\n\nfrom mygrad.operation_base import Operation\n\n__all__ = [""Tensor_Transpose_Property"", ""Transpose"", ""MoveAxis"", ""Roll"", ""SwapAxes""]\n\n\nclass Tensor_Transpose_Property(Operation):\n    def __call__(self, a):\n        """""" Same as a.transpose(), except that a is returned if\n            a.ndim < 2.\n\n            Parameters\n            ----------\n            a : mygrad.Tensor""""""\n        self.variables = (a,)\n        return a.data.T\n\n    def backward_var(self, grad, index, **kwargs):\n        return grad.T\n\n\nclass Transpose(Operation):\n    def __call__(self, a, axes=None):\n        self.variables = (a,)\n        if axes is not None:\n            self.axes = tuple(axis % a.ndim for axis in axes)\n        else:\n            self.axes = tuple(range(a.ndim)[::-1])\n        return np.transpose(a.data, axes)\n\n    def backward_var(self, grad, index, **kwargs):\n        a = self.variables[index]\n        if a.ndim > 1:\n            grad = grad.transpose(np.argsort(self.axes))\n        return grad\n\n\nclass MoveAxis(Operation):\n    def __call__(self, a, source, destination):\n        self.variables = (a,)\n        self.source = source\n        self.destination = destination\n        return np.moveaxis(a, source, destination)\n\n    def backward_var(self, grad, index, **kwargs):\n        if not index == 0:  # pragma: no cover\n            raise IndexError\n        return np.moveaxis(grad, self.destination, self.source)\n\n\nclass SwapAxes(Operation):\n    def __call__(self, a, axis1, axis2):\n        self.variables = (a,)\n        self.axis1 = axis1\n        self.axis2 = axis2\n        return np.swapaxes(a.data, axis1, axis2)\n\n    def backward_var(self, grad, index, **kwargs):\n        if not index == 0:  # pragma: no cover\n            raise IndexError\n        return np.swapaxes(grad, self.axis2, self.axis1)\n\n\nclass Roll(Operation):\n    def __call__(self, a, shift, axis):\n        self.variables = (a,)\n        self.shift = shift\n        self.axis = axis\n        return np.roll(a.data, shift=shift, axis=axis)\n\n    def backward_var(self, grad, index, **kwargs):\n        if not index == 0:  # pragma: no cover\n            raise IndexError\n        rev_shift = (\n            -self.shift\n            if not hasattr(self.shift, ""__iter__"")\n            else tuple(-i for i in self.shift)\n        )\n        return np.roll(grad, axis=self.axis, shift=rev_shift)\n'"
