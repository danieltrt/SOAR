file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\nfrom setuptools import setup, find_packages\n\ninstall_requires = [""numpy >= 1.12"", \'scipy>=0.19\', \'scikit-learn>=0.18\']\n\nsetup(\n    name=\'dnn\',\n    version=\'0.0.1\',\n    description=\'DNN numpy\',\n    long_description="""",\n    author=""Takuya wakisaka"",\n    author_email=\'takuya.wakisaka@moldweorp.com\',\n    url=\'https://github.com/ruimashita/dnn\',\n    license="""",\n    packages=find_packages(exclude=(\'tests\',)),\n    install_requires=install_requires,\n    setup_requires=[\'pytest-runner\'],\n    tests_require=[\'pytest\', ],\n)\n'"
dnn/__init__.py,0,b''
dnn/gradient.py,1,"b'# -*- coding: utf-8 -*-\nfrom collections import OrderedDict\nimport functools\n\nimport numpy as np\n\nfrom dnn.tensor import Variable\n\n\nclass _BackwardCache:\n    """"""Cache backward""""""\n\n    def __init__(self):\n        self.memo = {}\n        self.operations = []\n\n    def __call__(self, func):\n        """"""Wrap backward to memorize""""""\n        closure = getattr(func, ""__closure__"")\n\n        # if func is already wrapped, return func.\n        if closure and type(closure[-1].cell_contents) is type(self):\n            return func\n\n        @functools.wraps(func)\n        def wrapper(grad_outputs):\n            operation = func.__self__\n            self.operations.append(operation)\n            _hash = hash((operation, grad_outputs.tostring()))\n            if _hash not in self.memo:\n                self.memo[_hash] = func(grad_outputs)\n            return self.memo[_hash]\n        return wrapper\n\n    def clean(self):\n        """"""Remove memorize wrapper""""""\n        for operation in self.operations:\n            closure = getattr(operation.backward, ""__closure__"")\n            if closure and type(closure[-1].cell_contents) is type(self):\n                original_backward = closure[-2].cell_contents\n                operation.backward = original_backward\n        self.operations = []\n\n    def __del__(self):\n        self.clean()\n\n\ndef gradients(y, xs):\n    """"""Compute graindents.\n\n    Return the list of partial derivatives y with respect to x in xs.\n    The list of length is `len(xs)`.\n\n    Args:\n        y(Tensor): A tensor to be differentiated.\n        xs(list of Tensor): list of Tensor to be used for differentiated.\n\n    Returns:\n        List of partial derivatives y with respect to x in xs.\n    """"""\n    grad_y = np.ones_like(y.data)\n    grads = []\n\n    cache = _BackwardCache()\n\n    for i, x in enumerate(xs):\n        operation_and_index_path = OrderedDict(_traverse(y, x))\n\n        grad_outputs = grad_y\n        for operation, index in operation_and_index_path.items():\n            operation.backward = cache(operation.backward)\n\n            grad_inputs = operation.backward(grad_outputs)\n            grad_outputs = grad_inputs[index]\n\n        grads.append(grad_outputs)\n\n    del cache\n    return grads\n\n\ndef get_variables(y):\n    """"""Find all variables form y.\n\n    Args:\n        y(Tensor): Graph root tensor.\n    """"""\n    if type(y) is Variable:\n        yield y\n        return\n\n    operation = y.owner\n    # if y is leaf\n    if operation is None:\n        return\n\n    inputs = operation.tensor_inputs\n    for _input in inputs:\n        yield from get_variables(_input)\n\n\ndef _traverse(y, x):\n    """"""Depth first traversal form y to x.\n\n    Generate operation and operation input index tuple.\n\n    Params:\n    y: Tensor\n    x: Tensor\n    """"""\n    # print(""_traverse"", y, x)\n    operation = y.owner\n    # if y is leaf\n    if operation is None:\n        return\n\n    inputs = operation.tensor_inputs\n    inputs_id = [id(_input) for _input in inputs]\n\n    if id(x) in inputs_id:\n        index = inputs_id.index(id(x))\n        yield operation, index\n        return\n\n    for index, tmp_tensor in enumerate(inputs):\n\n        results = list(_traverse(tmp_tensor, x))\n        if len(results) > 0:\n            yield operation, index\n            for result in results:\n                yield result\n            return\n'"
dnn/operation.py,10,"b'# -*- coding: utf-8 -*-\nfrom logging import getLogger\n\nimport numpy as np\n\nfrom dnn.tensor import Tensor\n\nlogger = getLogger(""dnn.operation"")\n\n\nclass OperationMeta(type):\n    """"""Operation Meta Class.\n\n    Check forward() and backward() parameters by wrapping the methods.\n    """"""\n\n    @staticmethod\n    def check_forward(func):\n        """"""Check forward() parameters.""""""\n        def func_wrapper(self, *inputs):\n            for _input in inputs:\n                # TODO(wakisaka): numpy\n                assert isinstance(_input, np.ndarray)\n\n            logger.debug(""forward func wrapper: %s %s %s"", self, func, inputs)\n            outputs = func(self, *inputs)\n\n            if isinstance(outputs, (tuple, list)):\n                for output in outputs:\n                    # TODO(wakisaka): numpy\n                    assert isinstance(output, (np.ndarray,))\n            else:\n                # TODO(wakisaka): numpy\n                assert isinstance(_input, (np.ndarray, ))\n\n            return outputs\n        return func_wrapper\n\n    @staticmethod\n    def check_backward(func):\n        """"""Check backward() parameters""""""\n        def func_wrapper(self, grad_outputs):\n            if isinstance(grad_outputs, np.ndarray):\n                # TODO(wakisaka): numpy\n                assert isinstance(self.outputs, np.ndarray)\n                assert self.outputs.shape == grad_outputs.shape\n\n            elif isinstance(grad_outputs, tuple):\n                assert len(grad_outputs) == len(self.outputs)\n\n                for output, grad_output in zip(self.outputs, grad_outputs):\n                    assert output.shape == grad_output.shape\n\n            else:\n                msg = ""type error grad_outputs: {}"".format(grad_outputs)\n                raise TypeError(msg)\n\n            grad_inputs = func(self, grad_outputs)\n            logger.debug(\n                ""backwward func wrapper: %s %s %s %s %s %s"",\n                self, func, grad_outputs, self.outputs, self.inputs, grad_inputs\n            )\n\n            # TODO(wakisaka): numpy\n            if isinstance(grad_inputs, np.ndarray):\n                assert isinstance(grad_inputs, np.ndarray)\n                assert grad_inputs.shape == self.inputs.shape\n\n            elif isinstance(grad_inputs, tuple):\n                assert len(self.inputs) == len(grad_inputs)\n\n                for _input, grad_input in zip(self.inputs, grad_inputs):\n                    assert _input.shape == grad_input.shape\n\n            else:\n                msg = ""type error grad_inputs: {}"".format(type(grad_inputs))\n                raise TypeError(msg)\n\n            return grad_inputs\n\n        return func_wrapper\n\n    @staticmethod\n    def cast_from_tensor_to_data(func):\n        """"""Cast function parameters from tensor to data.""""""\n        def cast_wrapper(self, args):\n            if isinstance(args, (tuple, list)):\n                data_args = tuple([i.data for i in args])\n\n            else:\n                if isinstance(args, (Tensor)):\n                    data_args = args.data\n\n                else:\n                    data_args = args\n\n            returns = func(self, data_args)\n\n            return returns\n\n        return cast_wrapper\n\n    def __new__(cls, cls_name, cls_bases, cls_dict):\n        # wrap forward()\n        if ""forward"" not in cls_dict:\n            raise NotImplementedError(""{}.forward() not implemented "".format(cls_name))\n        forward_func = cls_dict[""forward""]\n        forward_func = OperationMeta.check_forward(forward_func)\n        cls_dict[""forward""] = forward_func\n\n        # wrap backward()\n        if ""backward"" not in cls_dict:\n            raise NotImplementedError(""{}.backward() not implemented "".format(cls_name))\n        backward_func = cls_dict[""backward""]\n        backward_func = OperationMeta.check_backward(backward_func)\n        backward_func = OperationMeta.cast_from_tensor_to_data(backward_func)\n        cls_dict[""backward""] = backward_func\n\n        result = super().__new__(cls, cls_name, cls_bases, cls_dict)\n        return result\n\n\nclass Operation(metaclass=OperationMeta):\n\n    def __call__(self, *inputs):\n        """"""Do the operation.\n\n        Params:\n        inputs: Tensors or a Tensor.\n\n        Returns:\n        outputs: python list of Tensors or a Tensor.\n        """"""\n\n        self.tensor_inputs = inputs\n        self.inputs = tuple([\n            _input.data if isinstance(_input, Tensor) else _input for _input in inputs\n        ])\n\n        outputs = self.forward(*self.inputs)\n\n        logger.debug(""outputs %s"", outputs)\n\n        if isinstance(outputs, np.ndarray):\n            tensor_outputs = Tensor(outputs)\n        elif isinstance(outputs, (tuple, list)):\n            tensor_outputs = tuple([Tensor(output) for output in outputs])\n        else:\n            # outputs is scaler\n            outputs = np.array([outputs])\n            tensor_outputs = Tensor(outputs)\n\n        if isinstance(tensor_outputs, Tensor):\n            tensor_outputs.owner = self\n        else:\n            for tensor_output in tensor_outputs:\n                tensor_output.owner = self\n\n        # TODO(wakisaka): use weakref\n        self.outputs = outputs\n\n        return tensor_outputs\n\n    def forward(self, *inputs):\n        raise NotImplementedError()\n\n    def backward(self, grad_outputs):\n        """"""Backward.\n\n        Params:\n        grad_outputs: gradients of outputs. list of Tensor or a Tensor.\n        """"""\n        raise NotImplementedError()\n\n\ndef unbroadcast(grad, data):\n    """"""Unbroadcast grad from grad`s shape to data\'s shape.\n\n    https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n    """"""\n    if grad.shape == data.shape:\n        return grad\n\n    dimention_broardcasted_axis = tuple(range(grad.ndim - data.ndim))\n    grad = grad.sum(dimention_broardcasted_axis)\n\n    size_one_broardcasted_axis = tuple(i for i, shape in enumerate(data.shape) if shape == 1)\n    if len(size_one_broardcasted_axis) > 0:\n        return grad.sum(keepdims=True, axis=size_one_broardcasted_axis)\n\n    return grad\n\n\ndef needs_broadcast(*inputs):\n    broadcasted = np.broadcast(*inputs)\n\n    for _input in inputs:\n        if _input.shape is not broadcasted.shape:\n            return True\n\n    return False\n'"
dnn/optimizer.py,3,"b'# -*- coding: utf-8 -*-\nimport math\n\nimport numpy as np\n\nfrom dnn.gradient import gradients, get_variables\n\n\nclass Optimizer:\n\n    def minimize(self, loss, tensors=None):\n        self.loss = loss\n        if tensors is None:\n            tensors = list(get_variables(loss))\n        self.tensors = tensors\n\n        return self\n\n    def update(self, loss_func=None, *args, **kwargs):\n        if loss_func is None:\n            loss = self.loss\n        else:\n            loss = loss_func(*args, **kwargs)\n        grads = gradients(loss, self.tensors)\n\n        for grad, tensor in zip(grads, self.tensors):\n            self.step(tensor, grad)\n\n    def step(self, tensor, grad, state=None):\n        raise NotImplementedError\n\n\nclass SGD(Optimizer):\n\n    def __init__(self, learning_rate):\n        self.learning_rate = learning_rate\n\n    def step(self, tensor, grad, state=None):\n        tensor.data = tensor.data - self.learning_rate * grad\n\n\nclass Adam(Optimizer):\n\n    def __init__(self, learning_rate, beta1=0.9, beta2=0.999, eps=1e-8):\n        self.learning_rate = learning_rate\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eps = eps\n\n        self.global_step = 1\n\n    def _setup(self):\n        self.m_dict = {}\n        self.v_dict = {}\n\n        for tensor in self.tensors:\n            # TODO(wakisaka): numpy\n            self.m_dict[id(tensor)] = np.zeros_like(tensor.data)\n            self.v_dict[id(tensor)] = np.zeros_like(tensor.data)\n\n    def update(self, loss_func=None, *args, **kwargs):\n        if self.global_step == 1:\n            self._setup()\n        super().update(loss_func, *args, **kwargs)\n        self.global_step += 1\n\n    def step(self, tensor, grad, state=None):\n        m = self.m_dict[id(tensor)]\n        v = self.v_dict[id(tensor)]\n\n        # update m and v.\n        m += (1. - self.beta1) * (grad - m)\n        v += (1. - self.beta2) * (grad * grad - v)\n\n        learning_rate = self.learning_rate \\\n            * math.sqrt(1. - self.beta2 ** self.global_step) \\\n            / (1. - self.beta1 ** self.global_step)\n\n        # TODO(wakisaka): numpy\n        tensor.data = tensor.data - learning_rate * m / (np.sqrt(v) + self.eps)\n'"
dnn/tensor.py,3,"b'# -*- coding: utf-8 -*-\nimport numpy as np\n\nfrom logging import getLogger\n\nlogger = getLogger(""dnn.tensor"")\n\n\nclass Tensor(object):\n    def __init__(self, data, name=None):\n        if not isinstance(data, np.ndarray):\n            msg = ""numpy.ndarray are expected. Actual: {0}"".format(type(data))\n            raise TypeError(msg)\n\n        self.data = data\n        self.name = name\n        self._owner = None\n\n    def __str__(self):\n        return ""Tensor(name:{}, data.shape:{}, data:{},)"".format(\n            self.name, self.data.shape, self.data\n        )\n\n    def __repr__(self):\n        return str(self)\n\n    @property\n    def shape(self):\n        return self.data.shape\n\n    @property\n    def owner(self):\n        """"""Creater `Operation` of the tensor""""""\n        return self._owner\n\n    @owner.setter\n    def owner(self, value):\n        self._owner = value\n\n    def __hash__(self):\n        return hash(self.data.tostring())\n\n    def __add__(self, other):\n        from dnn.operations.math import add\n        return add(self, other)\n\n    def __sub__(self, other):\n        from dnn.operations.math import Subtract\n        result = Subtract()(self, other)\n        result.name = ""{} - {}"".format(self.name, other.name)\n        return result\n\n    def __pow__(self, other):\n        from dnn.operations.math import Pow\n        if isinstance(other, int):\n            other = Tensor(np.array([other]))\n        result = Pow()(self, other)\n        result.name = ""{} ** {}"".format(self.name, other.name)\n        return result\n\n    def __truediv__(self, other):\n        from dnn.operations.math import Truediv\n        result = Truediv()(self, other)\n        result.name = ""{} / {}"".format(self.name, other.name)\n        return result\n\n    def __matmul__(self, other):\n        from dnn.operations.math import Matmul\n        result = Matmul()(self, other)\n        result.name = ""{} @ {}"".format(self.name, other.name)\n        return result\n\n    def __mul__(self, other):\n        from dnn.operations.math import multiply\n        return multiply(self, other)\n\n    def __neg__(self):\n        return self * Tensor(np.array([-1]))\n\n\nclass Variable(Tensor):\n    def __init__(self, data, name=None):\n        super().__init__(data, name=name)\n'"
dnn/test.py,9,"b'# -*- coding: utf-8 -*-\nimport copy\n\nimport numpy as np\n\nfrom dnn.tensor import Tensor\n\n\ndef _as_tuple(x):\n    if isinstance(x, tuple):\n        return x\n    elif isinstance(x, list):\n        return tuple(x)\n    else:\n        return x,\n\n\ndef check_gradients(y, xs, epsilon=1e-8):\n    """"""Calculate theorical and numerical Jacobian list for each inputs (xs).\n\n    2-d numpy array representing the Jacobian for dy/dx in xs. \\\n    Each has ``y size`` rows and ``x size`` columns.\n    ``y size`` is the number of elements in operations output\n    and ``x size`` is the number of elements in input::\n\n        [\n            [dy[0]/dx[0], dy[0]/dx[1], ... , dy[0],dx[x_size]],\n            [dy[1]/dx[0], dy[1]/dx[1], ... , dy[1],dx[x_size]],\n            ...,\n            ...,\n            [dy[y_size]/dx[0], dy[y_size]/dx[1], ... , dy[y_size],dx[x_size]],\n        ].\n\n    Args:\n        y(Operation): Operation to be partial derivatived.\n        xs(tuple or list of numpy.ndarray): Inputs of operation.\n\n    Returns:\n        tuple:\n            theorical_jacobians(list): list of theorical jacobians for each inputs(xs).\n            numerical_jacobians(list): list of numerical jacobians for each inputs(xs).\n    """"""\n    xs = _as_tuple(xs)\n    xs_tensor = [Tensor(x) for x in xs]\n\n    outputs = y(*xs_tensor)\n    if isinstance(outputs, tuple):\n        # TODO(wakisaka): Implement tuple outputs pattern. the pattern cannot (yet) gradient with multiple outputs.\n        raise Exception(""cannot check gradient with multiple outputs"")\n    y_shape = outputs.shape\n    theorical_jacobians = _theorical_jacobians(y, xs, y_shape)\n    numerical_jacobians = _numerical_jacobians(y, xs, y_shape, epsilon)\n\n    return theorical_jacobians, numerical_jacobians\n\n\ndef _theorical_jacobians(y, xs, y_shape):\n    """"""Calculate theorical jacobian for each xs.\n\n    Args:\n        y(Operation): Operation to be partial derivatived.\n        input_data(tuple of numpy.ndarray): Inputs of operation.\n        y_shape(tuple): shape of operation(y) output.\n\n    Returns:\n        list of jacobian.\n    """"""\n    output_size = np.prod(y_shape)\n    jacobians = []\n    for input_index, x in enumerate(xs):\n        # allocate jacobian matrix.\n        jacobian = np.empty((output_size, x.size))\n\n        # partial derivatives of y[element_indecies] w.r.t x.\n        for element_indecies in np.ndindex(y_shape):\n            # Gradients of output, we set one element to be 1.0 and everything else to be 0 like 1 of k.\n            grad_outputs = np.zeros(shape=y_shape)\n            grad_outputs[element_indecies] = 1.0\n\n            # backward with 1 of k give us one row of the jacobian.\n            # [dy[element_indecies]/dx]\n            grad_inputs = y.backward(grad_outputs)\n\n            # converts element_indecie into the flattened.\n            index = np.ravel_multi_index(element_indecies, y_shape)\n            jacobian[index, :] = grad_inputs[input_index].ravel()\n\n        jacobians.append(jacobian)\n    return jacobians\n\n\ndef _numerical_jacobians(y, xs, y_shape, epsilon):\n    """"""Calculate numerical jacobian for each xs.\n\n    Args:\n        y(Operation): Operation to be partial derivatived.\n        input_data(tuple of numpy.ndarray): Inputs of operation.\n        y_shape(tuple): shape of operation(y) output.\n\n    Returns:\n        list of jacobian.\n    """"""\n    output_size = np.prod(y_shape)\n    jacobians = []\n    for input_index, x in enumerate(xs):\n\n        # allocate jacobian matrix.\n        jacobian = np.empty((output_size, x.size))\n\n        # calculate partial derivative of y w.r.t traget(x[element_indecies]).\n        for element_indecies in np.ndindex(x.shape):\n            target = x[element_indecies]\n\n            increase_inputs = copy.deepcopy(xs)\n            increase_inputs[input_index][element_indecies] = target + epsilon\n\n            # result of increaseing traget inputs. y(target + epsilon)\n            increase_result = y.forward(*increase_inputs)\n\n            reduce_inputs = copy.deepcopy(xs)\n            reduce_inputs[input_index][element_indecies] = target - epsilon\n\n            # result of reducing traget inputs. y(target - epsilon)\n            reduce_result = y.forward(*reduce_inputs)\n\n            # TODO(wakisaka): make pattern of results is tuple. cant (yet) gradient  with multiple outputs\n            # partial derivatives of y w.r.t traget [dy[i] / dtraget].\n            # It is one colomun of jacobian.\n            result = (increase_result - reduce_result) / (2 * epsilon)\n\n            # converts element_indecie into the flattened.\n            index = np.ravel_multi_index(element_indecies, x.shape)\n            jacobian[:, index] = result.ravel()\n\n        jacobians.append(jacobian)\n    return jacobians\n'"
examples/iris_mlp.py,9,"b'# -*- coding: utf-8 -*-\nimport numpy as np\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\n\nfrom dnn.tensor import Tensor, Variable\nfrom dnn import operations as ops\nfrom dnn.optimizer import Adam\n\n\ndef iris_data():\n    iris = load_iris()\n    data = iris.data\n\n    data_std = np.copy(data)\n    data_std[:, 0] = (data[:, 0] - data[:, 0].mean()) / data[:, 0].std()\n    data_std[:, 1] = (data[:, 1] - data[:, 1].mean()) / data[:, 1].std()\n    data_std[:, 2] = (data[:, 2] - data[:, 2].mean()) / data[:, 2].std()\n    data_std[:, 3] = (data[:, 3] - data[:, 3].mean()) / data[:, 3].std()\n\n    labels = iris.target\n    labels = LabelBinarizer().fit_transform(labels)\n\n    data = data_std\n\n    train_data, test_data, train_labels, test_labels = train_test_split(\n        data,\n        labels,\n        random_state=1,\n        test_size=0.1,\n    )\n\n    return train_data, test_data, train_labels, test_labels\n\n\ndef mlp():\n    num_first = 10\n    num_input = 4\n    num_output = num_first\n\n    first_weights = Variable(np.random.normal(size=(num_input, num_output)))\n    first_bias = Variable(np.zeros(num_output))\n\n    num_input = num_output\n    num_output = 3\n\n    last_weights = Variable(np.random.normal(size=(num_input, num_output)))\n    last_bias = Variable(np.zeros(num_output))\n\n    optimizer = Adam(0.01)\n\n    def call(data, labels, is_training=False):\n        first_output = data @ first_weights + first_bias\n        first_output = ops.relu(first_output)\n\n        last_output = first_output @ last_weights + last_bias\n\n        output = ops.softmax(last_output)\n\n        cross_entropy = - ops.sum(labels * ops.log(output), axis=1)\n        loss = ops.mean(cross_entropy)\n\n        if is_training:\n            optimize = optimizer.minimize(loss)\n            optimize.update()\n\n        correct_prediction = np.equal(np.argmax(output.data, 1), np.argmax(labels.data, 1))\n        accuracy = np.mean(correct_prediction, dtype=np.float32)\n\n        return np.float32(loss.data)[0], accuracy\n\n    return call\n\n\ndef mlp_training(train_data, test_data, train_labels, test_labels):\n    np.random.seed(seed=32)\n    train_data = Tensor(train_data, name=""train_datas"")\n    train_labels = Tensor(train_labels, name=""train_labels"")\n\n    test_data = Tensor(test_data, name=""test_datas"")\n    test_labels = Tensor(test_labels, name=""test_labels"")\n\n    model = mlp()\n    for i in range(100):\n        loss, accuracy = model(train_data, train_labels, is_training=True)\n        yield loss, accuracy\n\n    loss, accuracy = model(test_data, test_labels, is_training=False)\n    yield loss, accuracy\n\n\nif __name__ == \'__main__\':\n    train_data, test_data, train_labels, test_labels = iris_data()\n    results = mlp_training(train_data, test_data, train_labels, test_labels)\n\n    for i, (loss, accuracy) in enumerate(results):\n        print(""step: {}, loss {}, accuracy {}"".format(i, loss, accuracy))\n'"
examples/mnist_cnn.py,12,"b'# -*- coding: utf-8 -*-\nimport os\nimport gzip\nimport struct\nimport urllib.request\n\nimport numpy as np\n\nfrom dnn.tensor import Tensor, Variable\nfrom dnn import operations as ops\nfrom dnn.optimizer import Adam\n\nMNIST_URL = ""http://yann.lecun.com/exdb/mnist/""\n\nTRAIN_IMAGE_FILE = ""train-images-idx3-ubyte.gz""\nTRAIN_LABEL_FILE = ""train-labels-idx1-ubyte.gz""\nTEST_IMAGE_FILE = ""t10k-images-idx3-ubyte.gz""\nTEST_LABEL_FILE = ""t10k-labels-idx1-ubyte.gz""\n\nDATA_DIR = os.path.join(""data"", ""MNIST"")\n\n\ndef _maybe_download_and_parse(filename):\n    target_file = os.path.join(DATA_DIR, filename)\n    download_url = MNIST_URL + filename\n\n    if not os.path.exists(DATA_DIR):\n        os.makedirs(DATA_DIR)\n\n    if not os.path.exists(target_file):\n        urllib.request.urlretrieve(download_url, target_file)\n\n    with gzip.open(target_file) as f:\n        if ""images"" in filename:\n            header, num, rows, cols = struct.unpack("">4i"", f.read(16))\n            size = num * rows * cols\n            data = struct.unpack(""{}B"".format(size), f.read(size))\n            data = np.array(data, dtype=np.uint8).reshape(num, rows, cols)\n\n        if ""labels"" in filename:\n            header, num = struct.unpack("">2i"", f.read(8))\n            data = struct.unpack(""{}B"".format(num), f.read(num))\n            data = np.array(data, dtype=np.uint8)\n\n    return data\n\n\ndef mnist_data(mode=""train""):\n    print(""load {} mnist..."".format(mode))\n    if mode == ""train"":\n        images = _maybe_download_and_parse(TRAIN_IMAGE_FILE)\n        labels = _maybe_download_and_parse(TRAIN_LABEL_FILE)\n    else:\n        images = _maybe_download_and_parse(TEST_IMAGE_FILE)\n        labels = _maybe_download_and_parse(TEST_LABEL_FILE)\n\n    print(""done"")\n\n    assert len(images) == len(labels)\n    num_data = len(images)\n\n    images = images / 255.0\n    images = images.reshape((-1, 1, 28, 28))\n\n    # onehot.\n    labels = np.eye(10)[labels]\n\n    start_index = 0\n\n    def batch(batch_size=num_data):\n        nonlocal start_index\n        while True:\n            end_index = start_index + batch_size\n            data_slice = slice(start_index, end_index)\n            yield images[data_slice], labels[data_slice]\n            if end_index < num_data:\n                start_index = end_index\n            else:\n                start_index = 0\n    return batch\n\n\ndef mnist_softmax():\n    np.random.seed(seed=615)\n    train_mnist = mnist_data(""train"")\n    train_mnist_batch = train_mnist(batch_size=100)\n\n    weights_1 = Variable(np.random.normal(size=(16, 1, 7, 7)))\n    weights_2 = Variable(np.random.normal(size=(32, 16, 3, 3)))\n    weights_3 = Variable(np.random.normal(size=(10, 32, 1, 1)))\n    biases_1 = Variable(np.random.normal(size=(16, 1, 1)))\n    biases_2 = Variable(np.random.normal(size=(32, 1, 1)))\n    biases_3 = Variable(np.random.normal(size=(10, 1, 1)))\n\n    optimizer = Adam(0.01)\n    for step in range(1000):\n        images_data, labels_data = next(train_mnist_batch)\n\n        images = Tensor(images_data, name=""datas"")\n        labels = Tensor(labels_data, name=""labels"")\n\n        x = ops.convolution2d(images, weights_1, pad=3, stride=2) + biases_1\n        x = ops.relu(x)\n        x = ops.convolution2d(x, weights_2, pad=1) + biases_2\n        x = ops.relu(x)\n        x = ops.convolution2d(x, weights_3, pad=1) + biases_3\n\n        # global average pooling\n        x = ops.mean(x, axis=3)\n        x = ops.mean(x, axis=2)\n\n        output = ops.softmax(x, axis=1)\n\n        cross_entropy = - ops.sum(labels * ops.log(output), axis=1)\n        loss = ops.mean(cross_entropy)\n\n        optimizer = optimizer.minimize(loss)\n        optimizer.update()\n\n        correct_prediction = np.equal(np.argmax(output.data, 1), np.argmax(labels.data, 1))\n        accuracy = np.mean(correct_prediction, dtype=np.float32)\n        print(""step: {}, train accuracy: {}"".format(step, accuracy))\n\n    return accuracy\n\n\nif __name__ == \'__main__\':\n    mnist_softmax()\n'"
examples/mnist_softmax.py,8,"b'# -*- coding: utf-8 -*-\nimport os\nimport gzip\nimport struct\nimport urllib.request\n\nimport numpy as np\n\nfrom dnn.tensor import Tensor, Variable\nfrom dnn import operations as ops\nfrom dnn.optimizer import Adam\n\nMNIST_URL = ""http://yann.lecun.com/exdb/mnist/""\n\nTRAIN_IMAGE_FILE = ""train-images-idx3-ubyte.gz""\nTRAIN_LABEL_FILE = ""train-labels-idx1-ubyte.gz""\nTEST_IMAGE_FILE = ""t10k-images-idx3-ubyte.gz""\nTEST_LABEL_FILE = ""t10k-labels-idx1-ubyte.gz""\n\nDATA_DIR = os.path.join(""data"", ""MNIST"")\n\n\ndef _maybe_download_and_parse(filename):\n    target_file = os.path.join(DATA_DIR, filename)\n    download_url = MNIST_URL + filename\n\n    if not os.path.exists(DATA_DIR):\n        os.makedirs(DATA_DIR)\n\n    if not os.path.exists(target_file):\n        urllib.request.urlretrieve(download_url, target_file)\n\n    with gzip.open(target_file) as f:\n        if ""images"" in filename:\n            header, num, rows, cols = struct.unpack("">4i"", f.read(16))\n            size = num * rows * cols\n            data = struct.unpack(""{}B"".format(size), f.read(size))\n            data = np.array(data, dtype=np.uint8).reshape(num, rows, cols)\n\n        if ""labels"" in filename:\n            header, num = struct.unpack("">2i"", f.read(8))\n            data = struct.unpack(""{}B"".format(num), f.read(num))\n            data = np.array(data, dtype=np.uint8)\n\n    return data\n\n\ndef mnist_data(mode=""train""):\n    print(""load {} mnist..."".format(mode))\n    if mode == ""train"":\n        images = _maybe_download_and_parse(TRAIN_IMAGE_FILE)\n        labels = _maybe_download_and_parse(TRAIN_LABEL_FILE)\n    else:\n        images = _maybe_download_and_parse(TEST_IMAGE_FILE)\n        labels = _maybe_download_and_parse(TEST_LABEL_FILE)\n\n    print(""done"")\n\n    assert len(images) == len(labels)\n    num_data = len(images)\n\n    images = images / 255.0\n    images = images.reshape((-1, 784))\n\n    # onehot.\n    labels = np.eye(10)[labels]\n\n    start_index = 0\n\n    def batch(batch_size=num_data):\n        nonlocal start_index\n        while True:\n            end_index = start_index + batch_size\n            data_slice = slice(start_index, end_index)\n            yield images[data_slice], labels[data_slice]\n            if end_index < num_data:\n                start_index = end_index\n            else:\n                start_index = 0\n    return batch\n\n\ndef mnist_softmax():\n    np.random.seed(seed=615)\n    train_mnist = mnist_data(""train"")\n    train_mnist_batch = train_mnist(batch_size=200)\n\n    weights = Variable(np.random.normal(size=(784, 10)))\n    biases = Variable(np.random.normal(size=(10)))\n\n    optimizer = Adam(0.01)\n    for step in range(100):\n        images_data, labels_data = next(train_mnist_batch)\n\n        images = Tensor(images_data, name=""datas"")\n        labels = Tensor(labels_data, name=""labels"")\n\n        logits = images @ weights + biases\n        output = ops.softmax(logits, axis=1)\n\n        cross_entropy = - ops.sum(labels * ops.log(output), axis=1)\n        loss = ops.mean(cross_entropy)\n\n        optimizer = optimizer.minimize(loss)\n        optimizer.update()\n\n        correct_prediction = np.equal(np.argmax(output.data, 1), np.argmax(labels.data, 1))\n        accuracy = np.mean(correct_prediction, dtype=np.float32)\n        print(""step: {}, train accuracy: {}"".format(step, accuracy))\n\n    return accuracy\n\n\nif __name__ == \'__main__\':\n    mnist_softmax()\n'"
tests/test_examples.py,6,"b'# -*- coding: utf-8 -*-\nimport math\nimport unittest\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom examples.mnist_softmax import mnist_data, mnist_softmax\nfrom examples.iris_mlp import iris_data, mlp_training\n\n\nclass TestMnistSoftmax(unittest.TestCase):\n\n    @staticmethod\n    def tensorflow_mnist_softmax():\n        np.random.seed(seed=615)\n\n        train_mnist = mnist_data(""train"")\n        train_mnist_batch = train_mnist(batch_size=200)\n\n        graph = tf.Graph()\n\n        with graph.as_default():\n            images = tf.placeholder(tf.float32, [None, 784])\n            labels = tf.placeholder(tf.float32, [None, 10])\n\n            weights = tf.Variable(initial_value=np.random.normal(size=(784, 10)), dtype=tf.float32)\n            biases = tf.Variable(initial_value=np.random.normal(size=(10)), dtype=tf.float32)\n            output = tf.nn.softmax(tf.matmul(images, weights) + biases)\n            cross_entropy = tf.reduce_mean(\n                -tf.reduce_sum(labels * tf.log(output), reduction_indices=[1])\n            )\n            train_step = tf.train.AdamOptimizer(0.01).minimize(cross_entropy)\n\n            correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n            init = tf.global_variables_initializer()\n\n        sess = tf.Session(graph=graph)\n        sess.run(init)\n        for step in range(100):\n            images_data, labels_data = next(train_mnist_batch)\n            _, accuracy_value = sess.run([train_step, accuracy], feed_dict={images: images_data, labels: labels_data})\n            print(""step: {}, train accuracy: {}"".format(step, accuracy_value))\n        return accuracy_value\n\n    def test_accuracy(self):\n        expect = TestMnistSoftmax.tensorflow_mnist_softmax()\n        result = mnist_softmax()\n\n        assert expect == result\n\n\nclass TestIrisMlp(unittest.TestCase):\n\n    @staticmethod\n    def tesnsorflow(train_data, test_data, train_labels, test_labels):\n        np.random.seed(seed=32)\n        graph = tf.Graph()\n\n        with graph.as_default():\n            data = tf.placeholder(tf.float32, [None, 4])\n            labels = tf.placeholder(tf.float32, [None, 3])\n\n            with tf.variable_scope(""layer1""):\n                num_first = 10\n                num_input = 4\n                num_output = num_first\n                first_weights = tf.Variable(initial_value=np.random.normal(size=(num_input, num_output)),\n                                            dtype=tf.float32)\n                first_bias = tf.Variable(tf.zeros([num_output]), dtype=tf.float32)\n                output = tf.matmul(data, first_weights) + first_bias\n                output = tf.nn.relu(output)\n\n            with tf.variable_scope(""layer2""):\n                num_input = num_first\n                num_output = 3\n                last_weights = tf.Variable(initial_value=np.random.normal(size=(num_input, num_output)),\n                                           dtype=tf.float32)\n                last_bias = tf.Variable(tf.zeros([num_output]), dtype=tf.float32)\n                output = tf.matmul(output, last_weights) + last_bias\n\n            output = tf.nn.softmax(output)\n            loss = tf.reduce_mean(\n                -tf.reduce_sum(labels * tf.log(output), reduction_indices=[1])\n            )\n            optimizer = tf.train.AdamOptimizer(0.01)\n            train_op = optimizer.minimize(loss)\n\n            correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(labels, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n            init_op = tf.global_variables_initializer()\n\n        sess = tf.Session(graph=graph)\n        sess.run(init_op)\n        for i in range(100):\n            _, loss_value, accuracy_value = sess.run(\n                [train_op, loss, accuracy],\n                feed_dict={data: train_data, labels: train_labels}\n            )\n            yield loss_value, accuracy_value\n\n        loss_value, accuracy_value = sess.run([loss, accuracy], feed_dict={data: test_data, labels: test_labels})\n        yield loss_value, accuracy_value\n\n    def test_loss_accuracy(self):\n        train_data, test_data, train_labels, test_labels = iris_data()\n        results = mlp_training(train_data, test_data, train_labels, test_labels)\n\n        tensorflow_results = TestIrisMlp.tesnsorflow(train_data, test_data, train_labels, test_labels)\n\n        for i, (tensorflow_result, result) in enumerate(zip(tensorflow_results, results)):\n            tf_loss, tf_accuracy = tensorflow_result\n            loss, accuracy = result\n            print(tf_accuracy)\n            assert math.isclose(tf_loss, loss, rel_tol=1e-05), ""step {}"".format(i)\n            assert tf_accuracy == accuracy, ""step {}"".format(i)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
dnn/operations/__init__.py,0,b'# -*- coding: utf-8 -*-\nfrom dnn.operations.activation import relu  # NOQA\nfrom dnn.operations.activation import softmax  # NOQA\n\nfrom dnn.operations.math import log  # NOQA\nfrom dnn.operations.math import matmul  # NOQA\nfrom dnn.operations.math import mean  # NOQA\nfrom dnn.operations.math import sum  # NOQA\n\nfrom dnn.operations.loss import mean_squared_error  # NOQA\n\nfrom dnn.operations.convolution import convolution2d  # NOQA\n'
dnn/operations/activation.py,3,"b'# -*- coding: utf-8 -*-\nfrom logging import getLogger\n\nimport numpy as np\n\nfrom dnn.operation import Operation\n\nlogger = getLogger(""dnn.operations.activation"")\n\n\nclass Relu(Operation):\n    def forward(self, x):\n        # TODO(wakisaka): numpy\n        return np.maximum(x, 0.0, dtype=x.dtype)\n\n    def backward(self, grad_outputs):\n        x = self.inputs[0]\n        return (x > 0.0) * grad_outputs,\n\n\ndef relu(x):\n    return Relu()(x)\n\n\nclass Sigmoid(Operation):\n\n    def forward(self, x):\n        # TODO(wakisaka): numpy\n        return 1 / (1 + np.exp(-x))\n\n    def backward(self, grad_outputs):\n        derivative = self.outputs * (1 - self.outputs)\n        return grad_outputs * derivative,\n\n\ndef sigmoid(x):\n    return Sigmoid()(x)\n\n\nclass Softmax(Operation):\n    def __init__(self, axis=1):\n        self.axis = axis\n\n    def forward(self, x):\n        # TODO(wakisaka): numpy\n        shift_x = x - x.max(axis=self.axis, keepdims=True)\n        exps = np.exp(shift_x)\n        return exps / exps.sum(axis=self.axis, keepdims=True)\n\n    def backward(self, grad_outputs):\n        output = self.outputs\n        grad = output * (grad_outputs - (grad_outputs * output).sum(axis=self.axis, keepdims=True))\n        return grad,\n\n\ndef softmax(a, axis=1):\n    return Softmax(axis=1)(a)\n'"
dnn/operations/convolution.py,19,"b'# -*- coding: utf-8 -*-\nfrom logging import getLogger\n\nimport numpy as np\n\nfrom dnn.operation import Operation\n\nlogger = getLogger(""dnn.operations.convolution"")\n\n\ndef _col2im(col, input_h, input_w, stride_y, stride_x, pad_h, pad_w):\n    batch, input_c, kernel_h, kernel_w, out_h, out_w = col.shape\n\n    img = np.zeros((batch,\n                    input_c,\n                    input_h + 2 * pad_h + stride_y - 1,\n                    input_w + 2 * pad_w + stride_x - 1,\n                    ),\n                   dtype=col.dtype)\n\n    for i in range(kernel_h):\n        i_end = i + stride_y * out_h\n        for j in range(kernel_w):\n            j_end = j + stride_x * out_w\n\n            img[:, :, i:i_end:stride_y, j:j_end:stride_x] += col[:, :, i, j, :, :]\n\n    img = img[:, :, pad_h:input_h + pad_h, pad_w:input_w + pad_w]\n    return img\n\n\ndef _im2col(x, kernel_h, kernel_w, stride_y, stride_x, pad_h, pad_w):\n    batch = x.shape[0]\n    input_c = x.shape[1]\n    input_h = x.shape[2]\n    input_w = x.shape[3]\n    out_h = 1 + ((input_h + 2 * pad_h - kernel_h) // stride_y)\n    out_w = 1 + ((input_w + 2 * pad_w - kernel_w) // stride_x)\n\n    # zero padding\n    img = np.pad(\n        x,\n        ((0, 0), (0, 0), (pad_h, pad_h + stride_y - 1), (pad_w, pad_w + stride_x - 1)),\n        mode=\'constant\',\n        constant_values=(0,))\n\n    col = np.ndarray((batch, input_c, kernel_h, kernel_w, out_h, out_w), dtype=img.dtype)\n\n    for i in range(kernel_h):\n        i_end = i + stride_y * out_h\n        for j in range(kernel_w):\n            j_end = j + stride_x * out_w\n\n            col[:, :, i, j, :, :] = img[:, :, i:i_end:stride_y, j:j_end:stride_x]\n\n    return col\n\n\nclass Convolution2DBase():\n\n    def __init__(self, stride=1, pad=0):\n\n        if type(stride) is int:\n            self.stride_y = stride\n            self.stride_x = stride\n        elif len(stride) == 2:\n            self.stride_y, self.stride_x = stride\n        else:\n            raise Exception(""Expected int, tuple/list with 2 entries. Got %s."" % (type(stride)))\n\n        if type(pad) is int:\n            self.pad_h = pad\n            self.pad_w = pad\n        elif len(pad) == 2:\n            self.pad_h, self.pad_w = pad\n        else:\n            raise Exception(""Expected int, tuple/list with 2 entries. Got %s."" % (type(pad)))\n\n\nclass Convolution2D_IM2COL(Convolution2DBase, Operation):\n\n    def forward(self, x, kernel):\n        """"""\n\n        x.shape -> batch, input_channel, height, width.\n        kernel.shape -> output_channel, input_channel, kernel_height, kernel_width.\n        """"""\n        kernel_h = kernel.shape[2]\n        kernel_w = kernel.shape[3]\n\n        # col.shape -> batch, input_c, kernel_h, kernel_w, out_h, out_w\n        col = _im2col(x, kernel_h, kernel_w, self.stride_y, self.stride_x, self.pad_h, self.pad_w)\n\n        # y.shape -> batch, out_h, out_w, out_c\n        y = np.tensordot(\n            col, kernel, ((1, 2, 3), (1, 2, 3)))\n\n        y = np.moveaxis(y, 3, 1)\n        return y\n\n    def backward(self, grad_outputs):\n        x, kernel = self.inputs\n\n        kernel_h = kernel.shape[2]\n        kernel_w = kernel.shape[3]\n        col = _im2col(x, kernel_h, kernel_w, self.stride_y, self.stride_x, self.pad_h, self.pad_w)\n\n        # (b, out_c, out_h, out_w) * (b, in_c, k_h, k_w, out_h, out_w) -> out_c, in_c, k_h, k_w\n        grad_kernel = np.tensordot(\n            grad_outputs, col, ((0, 2, 3), (0, 4, 5)))\n\n        # (out_c, in_c, k_h, k_w) * (b, out_c, h, w) -> (in_c, k_h, k_w, b, h, w)\n        grad_col = np.tensordot(kernel, grad_outputs, (0, 1))\n        grad_col = np.moveaxis(grad_col, 3, 0)  # (b, in_c, k_h, k_w, h, w)\n\n        input_h = x.shape[2]\n        input_w = x.shape[3]\n        grad_x = _col2im(grad_col, input_h, input_w, self.stride_y, self.stride_x, self.pad_h, self.pad_w)\n\n        return grad_x, grad_kernel\n\n\nclass Convolution2D_Naive(Convolution2DBase, Operation):\n\n    def forward(self, x, kernel):\n        """"""\n\n        x.shape -> batch, input_channel, height, width.\n        kernel.shape -> output_channel, input_channel, kernel_height, kernel_width.\n        """"""\n        batch = x.shape[0]\n        input_h = x.shape[2]\n        input_w = x.shape[3]\n        out_c = kernel.shape[0]\n        kernel_h = kernel.shape[2]\n        kernel_w = kernel.shape[3]\n        out_h = 1 + ((input_h + 2 * self.pad_h - kernel_h) // self.stride_y)\n        out_w = 1 + ((input_w + 2 * self.pad_w - kernel_w) // self.stride_x)\n\n        y = np.empty((batch, out_c, out_h, out_w))\n\n        # zero padding\n        x = np.pad(\n            x,\n            ((0, 0),\n             (0, 0),\n             (self.pad_h, self.pad_h + self.stride_y - 1),\n             (self.pad_w, self.pad_w + self.stride_x - 1)),\n            mode=\'constant\',\n            constant_values=(0,))\n\n        for h in range(out_h):\n            for w in range(out_w):\n                new_h = h * self.stride_y\n                new_w = w * self.stride_x\n                # partial_input.shape -> b, in_c, k_h, k_w\n                partial_input = x[:, :, new_h:new_h+kernel_h, new_w:new_w+kernel_w]\n                tmp = np.tensordot(partial_input, kernel, ((1, 2, 3), (1, 2, 3)))\n                y[:, :, h, w] = tmp\n\n        return y\n\n    def backward(self, grad_outputs):\n        x, kernel = self.inputs\n\n        input_h = x.shape[2]\n        input_w = x.shape[3]\n        kernel_h = kernel.shape[2]\n        kernel_w = kernel.shape[3]\n        out_h = 1 + ((input_h + 2 * self.pad_h - kernel_h) // self.stride_y)\n        out_w = 1 + ((input_w + 2 * self.pad_w - kernel_w) // self.stride_x)\n\n        # zero padding\n        x = np.pad(\n            x,\n            ((0, 0),\n             (0, 0),\n             (self.pad_h, self.pad_h + self.stride_y - 1),\n             (self.pad_w, self.pad_w + self.stride_x - 1)),\n            mode=\'constant\',\n            constant_values=(0,))\n\n        grad_x = np.zeros(x.shape)\n        grad_kernel = np.zeros(kernel.shape)\n\n        for h in range(out_h):\n            for w in range(out_w):\n                new_h = h * self.stride_y\n                new_w = w * self.stride_x\n                # partial_input.shape -> b, in_c, k_h, k_w\n                partial_input = x[:, :, new_h:new_h+kernel_h, new_w:new_w+kernel_w]\n                # partial_output.shape -> b, out_c\n                partial_output = grad_outputs[:, :, h, w]\n                tmp = np.tensordot(partial_output, partial_input, (0, 0))\n                grad_kernel[:, :, :, :] += tmp\n\n                tmp = np.tensordot(partial_output, kernel, ((1), (0)))\n                grad_x[:, :, new_h:new_h+kernel_h, new_w:new_w+kernel_w] += tmp\n\n        grad_x = grad_x[:, :, self.pad_h:self.pad_h+input_h, self.pad_w:self.pad_h+input_w]\n\n        return grad_x, grad_kernel\n\n\nclass Convolution2D_KN2ROW(Convolution2DBase, Operation):\n\n    def forward(self, x, kernel):\n        """"""\n\n        x.shape -> batch, input_channel, height, width.\n        kernel.shape -> output_channel, input_channel, kernel_height, kernel_width.\n        """"""\n        batch = x.shape[0]\n        input_h = x.shape[2]\n        input_w = x.shape[3]\n        out_c = kernel.shape[0]\n        kernel_h = kernel.shape[2]\n        kernel_w = kernel.shape[3]\n        out_h = 1 + ((input_h + 2 * self.pad_h - kernel_h) // self.stride_y)\n        out_w = 1 + ((input_w + 2 * self.pad_w - kernel_w) // self.stride_x)\n\n        # zero padding\n        x = np.pad(\n            x,\n            ((0, 0),\n             (0, 0),\n             (self.pad_h, self.pad_h + self.stride_y - 1),\n             (self.pad_w, self.pad_w + self.stride_x - 1)),\n            mode=\'constant\',\n            constant_values=(0,))\n\n        # 1x1 convolution\n        # tmp.shape -> (batch, input_height+pad, input_widht+pad, out_channel, kernel_h, kernel_w)\n        tmp = np.tensordot(\n            x, kernel, ((1, ), (1, )))\n\n        out = np.zeros((batch, out_c, out_h, out_w))\n        for h in range(out_h):\n            for w in range(out_w):\n                stride_h = h * self.stride_y\n                stride_w = w * self.stride_x\n                for k_h in range(kernel_h):\n                    for k_w in range(kernel_w):\n                        out[:, :, h, w] += tmp[:, stride_h+k_h, stride_w+k_w, :, k_h, k_w]\n\n        return out\n\n    def backward(self, grad_outputs):\n        Exception(""Should be implement"")\n\n\ndef convolution2d(x, kernel, stride=1, pad=0):\n    return Convolution2D_IM2COL(stride=stride, pad=pad)(x, kernel)\n'"
dnn/operations/loss.py,0,"b'# -*- coding: utf-8 -*-\nfrom logging import getLogger\n\nfrom dnn.operation import Operation\n\nlogger = getLogger(""dnn.operations.loss"")\n\n\nclass MeanSquaredError(Operation):\n\n    def forward(self, a, b):\n        return (a - b) ** 2\n\n    def backward(self, grad_outputs):\n        a, b = self.inputs\n        grad_a = 2 * (a - b) * grad_outputs\n        grad_b = 2 * (b - a) * grad_outputs\n        return grad_a, grad_b\n\n\ndef mean_squared_error(a, b):\n    return MeanSquaredError()(a, b)\n'"
dnn/operations/math.py,7,"b'# -*- coding: utf-8 -*-\nfrom logging import getLogger\n\nimport numpy as np\n\nfrom dnn.tensor import Tensor\nfrom dnn.operation import Operation, unbroadcast, needs_broadcast\n\nlogger = getLogger(""dnn.operations.math"")\n\n\nclass Add(Operation):\n\n    def forward(self, a, b):\n        output = a + b\n        return output\n\n    def backward(self, grad_outputs):\n        a, b = self.inputs\n\n        grad_a = 1 * grad_outputs\n        grad_b = 1 * grad_outputs\n\n        if needs_broadcast(*self.inputs):\n            grad_a = unbroadcast(grad_a, a)\n            grad_b = unbroadcast(grad_b, b)\n\n        return grad_a, grad_b\n\n\ndef add(a, b):\n    if isinstance(b, (int, float)):\n        b = Tensor(np.array([b]))\n\n    result = Add()(a, b)\n    result.name = ""{} + {}"".format(a.name, b.name)\n    return result\n\n\nclass Log(Operation):\n\n    def forward(self, a):\n        # TODO(wakisaka): numpy\n        output = np.log(a)\n        return output\n\n    def backward(self, grad_outputs):\n        a, = self.inputs\n        grad_a = 1/a * grad_outputs\n        return grad_a,\n\n\ndef log(a):\n    return Log()(a)\n\n\nclass Matmul(Operation):\n\n    def forward(self, a, b):\n        output = a @ b\n        return output\n\n    def backward(self, grad_outputs):\n        a, b = self.inputs\n        grad_a = grad_outputs @ b.T\n        grad_b = a.T @ grad_outputs\n        return grad_a, grad_b\n\n\ndef matmul(a, b):\n    return Matmul()(a, b)\n\n\nclass Multiply(Operation):\n\n    def forward(self, a, b):\n        output = a * b\n        return output\n\n    def backward(self, grad_outputs):\n        a, b = self.inputs\n        grad_a = b * grad_outputs\n        grad_b = a * grad_outputs\n\n        if needs_broadcast(*self.inputs):\n            grad_a = unbroadcast(grad_a, a)\n            grad_b = unbroadcast(grad_b, b)\n        return grad_a, grad_b\n\n\ndef multiply(a, b):\n    if isinstance(b, (int, float)):\n        b = Tensor(np.array([b]))\n    result = Multiply()(a, b)\n    result.name = ""{} * {}"".format(a.name, b.name)\n    return result\n\n\nclass Pow(Operation):\n\n    def forward(self, a, b):\n        output = a ** b\n        return output\n\n    def backward(self, grad_outputs):\n        a, b = self.inputs\n\n        grad_a = (b * (a ** (b - 1.))) * grad_outputs\n        # TODO(wakisaka): numpy\n        grad_b = (np.log(a) * self.outputs) * grad_outputs\n\n        if needs_broadcast(*self.inputs):\n            grad_a = unbroadcast(grad_a, a)\n            grad_b = unbroadcast(grad_b, b)\n\n        return grad_a, grad_b\n\n\nclass Subtract(Operation):\n\n    def forward(self, a, b):\n        output = a - b\n        return output\n\n    def backward(self, grad_outputs):\n        a, b = self.inputs\n\n        grad_a = 1 * grad_outputs\n        grad_b = -1 * grad_outputs\n\n        if needs_broadcast(*self.inputs):\n            grad_a = unbroadcast(grad_a, a)\n            grad_b = unbroadcast(grad_b, b)\n\n        return grad_a, grad_b\n\n\nclass Sum(Operation):\n    def __init__(self, axis=None):\n        self.axis = axis\n\n    def forward(self, a):\n        return a.sum(axis=self.axis)\n\n    def backward(self, grad_outputs):\n        a = self.inputs[0]\n        if self.axis is not None:\n            # TODO(wakisaka): numpy\n            grad_outputs = np.expand_dims(grad_outputs, axis=self.axis)\n        _, grad = np.broadcast_arrays(a, grad_outputs)\n        return grad,\n\n\ndef sum(a, axis=None):\n    return Sum(axis=axis)(a)\n\n\nclass Truediv(Operation):\n\n    def forward(self, a, b):\n        output = a / b\n        return output\n\n    def backward(self, grad_outputs):\n        a, b = self.inputs\n        grad_a = 1/b * grad_outputs\n        grad_b = -(a / (b ** 2)) * grad_outputs\n\n        if needs_broadcast(*self.inputs):\n            grad_a = unbroadcast(grad_a, a)\n            grad_b = unbroadcast(grad_b, b)\n\n        return grad_a, grad_b\n\n\ndef mean(a, axis=None):\n    if axis is None:\n        divider = a.data.size\n    else:\n        divider = a.data.shape[axis]\n\n    divider = Tensor(np.array([divider]))\n    sumed = Sum(axis)(a)\n    return sumed / divider\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# dnn documentation build configuration file, created by\n# sphinx-quickstart on Wed May 24 11:12:42 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, \'../../\')\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.napoleon\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'dnn\'\ncopyright = \'2017, Author\'\nauthor = \'Author\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = \'\'\n# The full version, including alpha/beta/rc tags.\nrelease = \'\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = \'en\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'dnndoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'dnn.tex\', \'dnn Documentation\',\n     \'Author\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'dnn\', \'dnn Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'dnn\', \'dnn Documentation\',\n     author, \'dnn\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n\n# -- Options for Epub output ----------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\nepub_author = author\nepub_publisher = author\nepub_copyright = copyright\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n'"
tests/dnn_tests/test_gradient.py,7,"b'# -*- coding: utf-8 -*-\n\nimport numpy as np\n\nfrom dnn.tensor import Tensor, Variable\nfrom dnn import operations\nfrom dnn.gradient import gradients, get_variables\n\n\ndef test_gradients():\n    x = Tensor(np.array([1]))\n    y = x + 2\n    z = y * 3\n\n    grads = gradients(z, [y, x])\n\n    assert grads[0] == np.array([3])\n    assert grads[1] == np.array([3])\n\n\ndef test_get_variables():\n    inputs = np.random.rand(10, 5)\n    labels = np.random.rand(10, 5)\n    init_weights = np.random.rand(10, 5)\n    init_bias = np.random.rand(5)\n\n    weights = Variable(init_weights, name=""weights"")\n    bias = Variable(init_bias, name=""bias"")\n\n    inputs = Tensor(inputs, name=""inputs"")\n    labels = Tensor(labels, name=""labels"")\n\n    tmp = inputs * weights\n    tmp.name = ""tmp""\n\n    y = tmp + bias\n    y.name = ""y""\n\n    mse = (y - labels) ** 2\n    loss_tmp = operations.sum(mse, axis=1)\n    loss = operations.mean(loss_tmp)\n\n    variables = get_variables(loss)\n\n    assert [id(weights), id(bias)] == [id(v) for v in variables]\n\n\nif __name__ == \'__main__\':\n    test_gradients()\n    test_get_variables()\n'"
tests/dnn_tests/test_operation.py,5,"b""# -*- coding: utf-8 -*-\nimport numpy as np\nimport pytest\n\nfrom dnn.operation import Operation\nfrom dnn.tensor import Tensor\n\n\ndef test_not_implemented():\n\n    with pytest.raises(NotImplementedError):\n        class _NoForwardBackwardOperation(Operation):\n            pass\n\n    with pytest.raises(NotImplementedError):\n        class _NoBackwardOperation(Operation):\n            def forward(*inputs):\n                return 1\n\n\nclass _ForwardReturnNotNdarrayOperation(Operation):\n    def forward(*inputs):\n        return 1, 2\n\n    def backward(grad_outputs):\n        return grad_outputs\n\n\ndef test_forward_return_not_ndarray():\n    op = _ForwardReturnNotNdarrayOperation()\n\n    with pytest.raises(AssertionError):\n        op(1)\n\n    with pytest.raises(AssertionError):\n        op(np.array([1]))\n\n    with pytest.raises(AssertionError):\n        op(Tensor(np.array([1])))\n\n\nclass _BackwardOutputWrongLengthOperation(Operation):\n    def forward(*inputs):\n        return np.array([1]), np.array([2])\n\n    def backward(grad_outputs):\n        return grad_outputs\n\n\ndef test_backward_ouput_wrong_length():\n    op = _BackwardOutputWrongLengthOperation()\n    op(Tensor(np.array([1])))\n\n    with pytest.raises(AssertionError):\n        op.backward(Tensor(np.array([1])))\n\n\nif __name__ == '__main__':\n    test_not_implemented()\n"""
tests/dnn_tests/operations_tests/test_activation.py,9,"b'# -*- coding: utf-8 -*-\nimport unittest\nimport math\n\nimport numpy as np\n\nfrom dnn.operations.activation import Relu, Sigmoid, Softmax\nfrom dnn.test import check_gradients\n\n\ndef test_softmax():\n    input_shape = (2, 3)\n    input_data = np.random.rand(*input_shape)\n\n    op = Softmax(axis=1)\n\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, \\n numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\nclass TestRelu(unittest.TestCase):\n\n    def test_backward(self):\n        input_shape = (2, 3, 4)\n        input_data = np.random.rand(*input_shape)\n\n        op = Relu()\n\n        theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n        for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n            assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n                ""theorical: {}, \\n numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\nclass TestSigmoid(unittest.TestCase):\n\n    def test_forward(self):\n        input_data = np.array([-1e+10, -1, 0, 1, 1e+10])\n        expect = np.array([0, 1/(1+math.exp(1)), 0.5, 1/(1+math.exp(-1)), 1])\n        op = Sigmoid()\n        result = op.forward(input_data)\n\n        assert np.allclose(expect, result)\n\n    def test_backward(self):\n        input_shape = (2, 3, 4)\n        input_data = np.random.rand(*input_shape)\n\n        op = Sigmoid()\n\n        theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n        for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n            assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n                ""theorical: {}, \\n numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\nif __name__ == \'__main__\':\n    test_softmax()\n'"
tests/dnn_tests/operations_tests/test_convolution.py,10,"b'# -*- coding: utf-8 -*-\nimport numpy as np\n\nfrom dnn.operations.convolution import (\n    Convolution2D_IM2COL, Convolution2D_Naive,\n    Convolution2D_KN2ROW\n)\nfrom dnn.test import check_gradients\n\n\ndef test_compare_convolution2d():\n    input_shape = (2, 2, 20, 20)  # b, c, h, w\n    input_data = np.random.rand(*input_shape)\n\n    kernel_shape = (4, 2, 3, 3)  # out_c, in_c, h, w,\n    kernel_data = np.random.rand(*kernel_shape)\n\n    pad = 1\n    stride = 4\n\n    naive = Convolution2D_Naive(pad=pad, stride=stride)\n    naive_output = naive(input_data, kernel_data)\n\n    kn2row = Convolution2D_KN2ROW(pad=pad, stride=stride)\n    kn2row_output = kn2row(input_data, kernel_data)\n\n    assert np.allclose(naive_output.data, kn2row_output.data)\n\n    im2col = Convolution2D_IM2COL(pad=pad, stride=stride)\n    im2col_output = im2col(input_data, kernel_data)\n\n    assert np.allclose(naive_output.data, im2col_output.data)\n\n\ndef test_convolution2d_im2col():\n    input_shape = (2, 5, 7, 7)  # b, c, h, w\n    input_data = np.random.rand(*input_shape)\n\n    kernel_shape = (6, 5, 3, 3)  # out_c, in_c, h, w,\n    kernel_data = np.random.rand(*kernel_shape)\n\n    op = Convolution2D_IM2COL(pad=1, stride=2)\n\n    theorical_jacobians, numerical_jacobians = check_gradients(op, (input_data, kernel_data))\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian, atol=1e-06), \\\n            ""theorical: {}, \\n numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_convolution2d_naive():\n    input_shape = (2, 5, 7, 7)  # b, c, h, w\n    input_data = np.random.rand(*input_shape)\n\n    kernel_shape = (6, 5, 3, 3)  # out_c, in_c, h, w,\n    kernel_data = np.random.rand(*kernel_shape)\n\n    op = Convolution2D_Naive(pad=1, stride=1)\n\n    theorical_jacobians, numerical_jacobians = check_gradients(op, (input_data, kernel_data))\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian, atol=1e-06), \\\n            ""theorical: {}, \\n numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\nif __name__ == \'__main__\':\n    test_compare_convolution2d()\n    test_convolution2d_im2col()\n    test_convolution2d_naive()\n'"
tests/dnn_tests/operations_tests/test_loss.py,2,"b'# -*- coding: utf-8 -*-\nimport numpy as np\n\nfrom dnn.operations.loss import MeanSquaredError\nfrom dnn.test import check_gradients\n\n\ndef test_mean_squared_error():\n    shape = (2, 3)\n    input_data = [np.random.rand(*shape), np.random.rand(*shape)]\n\n    op = MeanSquaredError()\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\nif __name__ == \'__main__\':\n    test_mean_squared_error()\n'"
tests/dnn_tests/operations_tests/test_math.py,39,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport pytest\n\nfrom dnn.tensor import Tensor\nfrom dnn.operations.math import Add, Matmul, Multiply, Sum, Truediv, Pow, Subtract\nfrom dnn.test import check_gradients\n\n\n@pytest.fixture(params=[(4, 2, 3), (4, 2, 3), (2, 3), (2, 3)])\ndef a_shape(request):\n    return request.param\n\n\n@pytest.fixture(params=[(2, 1,), (3,), (2, 3), (1,)])\ndef b_shape(request):\n    return request.param\n\n\ndef test_add():\n    a_shape = (2, 3)\n    b_shape = (3,)\n    a = Tensor(np.random.rand(*a_shape))\n    b = Tensor(np.random.rand(*b_shape))\n\n    op = Add()\n\n    input_data = a.data, b.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_matmul():\n    a_shape = (2, 3)\n    b_shape = (3, 6)\n    a = Tensor(np.random.rand(*a_shape))\n    b = Tensor(np.random.rand(*b_shape))\n\n    op = Matmul()\n\n    input_data = a.data, b.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_multiply():\n    a_shape = (2, 3)\n    b_shape = (2, 3)\n    a = Tensor(np.random.rand(*a_shape))\n    b = Tensor(np.random.rand(*b_shape))\n\n    op = Multiply()\n\n    forward_expects = a.data * b.data\n    forward_results = op(a, b)\n\n    assert np.allclose(forward_expects, forward_results.data)\n\n    input_data = a.data, b.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_multiply_broadcast(a_shape, b_shape):\n    a_data = np.random.rand(*a_shape)\n    b_data = np.random.rand(*b_shape)\n    a = Tensor(a_data)\n    b = Tensor(b_data)\n\n    out_shape = np.broadcast(a_data, b_data).shape\n\n    op = Multiply()\n\n    forward_expects = a_data * b_data\n    forward_results = op(a, b)\n\n    assert out_shape == forward_results.shape\n    assert np.allclose(forward_expects, forward_results.data)\n\n    input_data = a.data, b.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_pow():\n    a_shape = (2, 3)\n    b_shape = (2, 3)\n    a_data = np.random.rand(*a_shape)\n    b_data = np.random.rand(*b_shape)\n    a = Tensor(a_data)\n    b = Tensor(b_data)\n\n    op = Pow()\n\n    forward_expects = a_data ** b_data\n    forward_results = op(a, b)\n\n    assert np.allclose(forward_expects, forward_results.data)\n\n    input_data = a_data, b_data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_pow_broadcast(a_shape, b_shape):\n    a_data = np.random.rand(*a_shape)\n    b_data = np.random.rand(*b_shape)\n    a = Tensor(a_data)\n    b = Tensor(b_data)\n\n    out_shape = np.broadcast(a_data, b_data).shape\n\n    op = Pow()\n\n    forward_expects = a_data ** b_data\n    forward_results = op(a, b)\n\n    assert out_shape == forward_results.shape\n    assert np.allclose(forward_expects, forward_results.data)\n\n    input_data = a_data, b_data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_subtract():\n    a_shape = (2, 3)\n    b_shape = (3,)\n    a = Tensor(np.random.rand(*a_shape))\n    b = Tensor(np.random.rand(*b_shape))\n\n    op = Subtract()\n\n    input_data = a.data, b.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_sum_axis():\n    a_shape = (2, 3)\n    a = Tensor(np.random.rand(*a_shape))\n\n    out_shape = (2,)\n\n    op = Sum(axis=1)\n    forward_results = op(a)\n\n    assert forward_results.data.shape == out_shape\n\n    input_data = a.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_sum():\n    a_shape = (6,)\n    a = Tensor(np.random.rand(*a_shape))\n\n    out_shape = (1,)\n\n    op = Sum(axis=None)\n    forward_results = op(a)\n\n    assert forward_results.data.shape == out_shape\n\n    input_data = a.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_truediv():\n    a_shape = (2, 3)\n    b_shape = (2, 3)\n    a = Tensor(np.random.rand(*a_shape))\n    b = Tensor(np.random.rand(*b_shape))\n\n    op = Truediv()\n\n    input_data = a.data, b.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\ndef test_truediv_broadcast(a_shape, b_shape):\n    a_data = np.random.rand(*a_shape)\n    b_data = np.random.rand(*b_shape)\n    a = Tensor(a_data)\n    b = Tensor(b_data)\n\n    out_shape = np.broadcast(a_data, b_data).shape\n\n    op = Truediv()\n\n    forward_expects = a_data / b_data\n    forward_results = op(a, b)\n\n    assert out_shape == forward_results.shape\n    assert np.allclose(forward_expects, forward_results.data)\n\n    input_data = a.data, b.data\n    theorical_jacobians, numerical_jacobians = check_gradients(op, input_data)\n    for theorical_jacobian, numerical_jacobian in zip(theorical_jacobians, numerical_jacobians):\n        assert np.allclose(theorical_jacobian, numerical_jacobian), \\\n            ""theorical: {}, numerical: {}"".format(theorical_jacobian, numerical_jacobian)\n\n\nif __name__ == \'__main__\':\n    test_add()\n    test_matmul()\n    test_multiply()\n    test_pow()\n    test_subtract()\n    test_sum()\n    test_sum_axis()\n    test_truediv()\n'"
