file_path,api_count,code
setup.py,0,"b""from setuptools import setup\nfrom setuptools import find_packages\nimport pip\n\nsetup(name='nnn',\n      version='0.0.1',\n      description='Numpy Neural Network',\n      author='Ramon Vi\xc3\xb1as',\n      author_email='rvinast@gmail.com',\n      url='https://github.com/rvinas/nnn',\n      license='Apache',\n      packages=find_packages())\n\npip.main(['install', 'numpy'])\npip.main(['install', 'matplotlib'])"""
nnn/__init__.py,0,b''
nnn/core/__init__.py,0,b'from __future__ import absolute_import\n'
nnn/core/activations.py,2,"b'""""""\nactivations.py: Activation functions. They require its derivatives\n                with respect to their input.\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport math\nimport numpy as np\n\n\nclass Activation:\n    def __init__(self):\n        raise NotImplementedError\n\n    def activation(self, x):\n        raise NotImplementedError\n\n    def derivative(self, x):\n        raise NotImplementedError\n\n\nclass Sigmoid(Activation):\n    def __init__(self):\n        pass\n\n    def activation(self, x):\n        return 1 / (1 + math.e ** -x)\n\n    def derivative(self, x):\n        s = self.activation(x)\n        return s * (1 - s)\n\n\nclass Identity(Activation):\n    def __init__(self):\n        pass\n\n    def activation(self, x):\n        return x\n\n    def derivative(self, x):\n        return 1\n\n\nclass Tanh(Activation):\n    def __init__(self):\n        pass\n\n    def activation(self, x):\n        return np.array([math.tanh(i) for i in x])\n\n    def derivative(self, x):\n        return np.array([math.cosh(i) ** -2 for i in x])\n'"
nnn/core/initializers.py,1,"b'""""""\ninitializers.py: Initializers used for weight initialization\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\n\n\nclass Initializer:\n    def __init__(self):\n        raise NotImplementedError\n\n    def get_init(self, shape):\n        raise NotImplementedError\n\n\nclass GaussianInitializer:\n    def __init__(self, gaussian_mean=0.0, gaussian_std=0.1):\n        self.gaussian_mean = gaussian_mean\n        self.gaussian_std = gaussian_std\n\n    def get_init(self, shape):\n        return np.random.normal(self.gaussian_mean, self.gaussian_std, size=shape)\n'"
nnn/core/layers.py,6,"b'""""""\nlayer.py: Supported layers for the neural network. They must implement both\n          the forward and backward pass functions.\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\n\n\nclass Layer:\n    def __init__(self, input_dim, output_dim, activation, initializer):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.activation = activation\n        self.initializer = initializer\n\n\nclass Dense(Layer):\n    def __init__(self, input_dim, output_dim, activation, initializer):\n        super(Dense, self).__init__(input_dim, output_dim, activation, initializer)\n        self.weights = self.initializer.get_init(\n            shape=(output_dim,\n                   1 + input_dim))  # Shape of weights: (output_dim, 1 + input_dim,). Note that weights[:, 0] corresponds to bias term for every output unit\n        self.der_act_wrt_h = None\n        self.input = None\n\n    def forward_pass(self, x):\n        """"""\n        Computes f(x)=g(W*x + b), where g is the activation function, W the layer weights and b the bias term.\n        In addition, stores the derivative of f with respect to g to be used during the backward pass\n        :param x: layer input. Shape: (self.input_dim,)\n        :return: f(x)\n        """"""\n        assert len(x) == self.input_dim\n        self.input = np.concatenate(([1.0], x))  # adds product identity element for bias term\n        h = np.dot(self.weights, self.input)\n        self.der_act_wrt_h = self.activation.derivative(h)  # this derivate will be used in the backward pass\n        return self.activation.activation(h)\n\n    def backward_pass(self, der_loss_wrt_act, lr):\n        """"""\n        Computes the loss with respect to every weight in the layer according to backpropagation algorithm.\n        Updates all the weights in layer according to Stochastic Gradient Descent. Returns the backward loss to be transmitted to the previous layer.\n        :param lr: learning rate\n        :param der_loss_wrt_act: derivative of the loss with respect to the activation function of dense\'s output. Shape: (self.output_dim,)\n        :return: backward loss. Shape: (self.input_dim,)\n        """"""\n        assert len(der_loss_wrt_act) == self.output_dim\n        der_h_wrt_w = np.repeat(np.expand_dims(self.input, axis=0), self.output_dim, axis=0)\n        aux = der_loss_wrt_act * self.der_act_wrt_h\n        der_loss_wrt_w = np.expand_dims(aux, axis=-1) * der_h_wrt_w\n        backward = np.dot(aux, self.weights[:, 1:])  # computes the loss to be transmitted to the previous layer (note that bias term is being excluded)\n        self.weights -= lr * der_loss_wrt_w\n        return np.squeeze(backward)\n\n'"
nnn/core/neural_network.py,2,"b'""""""\nneural_network.py: Simple neural network from scratch\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom nnn.core.activations import *\nfrom nnn.core.layers import *\nfrom nnn.core.initializers import *\n\n\nclass NeuralNetwork:\n    def __init__(self, input_dim, output_dim):\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.last_output_dim = None\n        self.layers = []\n\n    def add_dense(self, output_dim, activation=Sigmoid,\n                  initializer=GaussianInitializer(gaussian_mean=0.0, gaussian_std=0.1)):\n        """"""\n        Adds a dense (fully connected) layer to the network.\n        :param output_dim: number of output units\n        :param activation: activation function. See core/activations\n        :param initializer: weight initializer. See core/initializers\n        """"""\n        input_dim = self.last_output_dim\n        if input_dim is None:\n            input_dim = self.input_dim\n        layer = Dense(input_dim=input_dim, output_dim=output_dim, activation=activation, initializer=initializer)\n        self.last_output_dim = output_dim\n        self.layers += [layer]\n\n    def _forward_pass(self, input):\n        """"""\n        Computes the forward step through all the layers in the network.\n        :param input: network\'s input. Shape: (self.input_dim,)\n        :return: network\'s output. Shape: (self.output_dim,)\n        """"""\n        assert len(input) == self.input_dim\n        h = input\n        for layer in self.layers:\n            h = layer.forward_pass(h)\n        return h\n\n    def _backward_pass(self, der_loss_wrt_act, lr):\n        """"""\n        Computes the backward step through all the layers in the network, updating all weights.\n        :param der_loss_wrt_act: derivative of the loss function with respect to the output activation. Shape: (self.output_dim,)\n        :param lr: learning rate\n        """"""\n        assert len(der_loss_wrt_act) == self.output_dim\n        der_out_wrt_act = np.array(der_loss_wrt_act)\n        for layer in reversed(self.layers):\n            der_out_wrt_act = layer.backward_pass(der_out_wrt_act, lr)\n\n    def train(self, x, y, objective, epochs=5, lr=0.01):\n        """"""\n        Trains the network using batches of size 1. TODO: batch support\n        :param x: Input data. Shape: (?, self.input_dim,)\n        :param y: Input labels. Shape: (?, self.output_dim,)\n        :param objective: Objective function. See core/objectives\n        :param epochs: Number of training epochs\n        :param lr: learning rate\n        """"""\n        assert self.last_output_dim == self.output_dim\n        indices = list(range(len(x)))\n\n        for epoch in range(epochs):\n            np.random.shuffle(indices)\n            x = x[indices]\n            y = y[indices]\n            print(\'\\nEpoch: \', epoch)\n            for i in range(len(x)):\n                output = self._forward_pass(x[i])\n                loss = objective.get_loss(y[i], output)\n                self._backward_pass(objective.derivative(y[i], output), lr)\n                print(\'iter: {0:2d}, loss: {1:0.2f}\'.format(i, loss))\n\n    def predict(self, x):\n        """"""\n        Computes the network forward step.\n        :param x: Input data. Shape: (self.input_dim,)\n        :return: prediction. Shape: (self.output_dim,)\n        """"""\n        return self._forward_pass(x)'"
nnn/core/objectives.py,0,"b'""""""\nobjectives.py: Objective (or loss) functions. They require its derivatives\n               with respect to the network prediction.\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport math\n\n\nclass Objective:\n    def __init__(self):\n        raise NotImplementedError\n\n    def get_loss(self, y_true, y_pred):\n        raise NotImplementedError\n\n    def derivative(self, y_true, y_pred):\n        raise NotImplementedError\n\n\nclass CrossEntropy(Objective):\n    def __init__(self):\n        pass\n\n    def get_loss(self, y_true, y_pred, eps=1e-6):\n        return -y_true * math.log(y_pred + eps) - (1 - y_true) * math.log(1 - y_pred + eps)\n\n    def derivative(self, y_true, y_pred):\n        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n\n\nclass SquaredError(Objective):\n    def __init__(self):\n        pass\n\n    def get_loss(self, y_true, y_pred):\n        return (y_true - y_pred) ** 2\n\n    def derivative(self, y_true, y_pred):\n        return 2 * (y_true - y_pred)\n'"
nnn/examples/__init__.py,0,b''
nnn/examples/a_approx_b.py,5,"b'""""""\na_equal_to_b.py: Example demonstrating that a non linearly separable dataset\n                 requires at least a hidden layer in order to perform a classification.\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom nnn.core.activations import Sigmoid\nfrom nnn.core.initializers import GaussianInitializer\nfrom nnn.core.neural_network import NeuralNetwork\nfrom nnn.core.objectives import CrossEntropy\nfrom nnn.utils.plots import plot_boundaries_2d, plot_data\nimport numpy as np\nnp.random.seed(27) # Note that the neural network is highly sensitive to initial conditions.\n\nn_samples_train = 100\nn_samples_test = 25\nx_train = np.random.rand(n_samples_train, 2)\nx_test = np.random.rand(n_samples_test, 2)\ny_train = np.array([int(abs(x[0]-x[1]) > 0.2) for x in x_train])\ny_test = np.array([int(abs(x[0]-x[1]) > 0.2) for x in x_test])\n\nplot_data(x_train=x_train,\n          y_train=y_train,\n          x_test=x_test,\n          y_test=y_test,\n          init=0,\n          end=1,\n          xlabel=\'A\',\n          ylabel=\'B\',\n          title=\'Data plot. Star points are test samples\',\n          save_path=\'a_approx_b_data.png\')\n\nnn = NeuralNetwork(input_dim=2,\n                   output_dim=1)\n# Try commenting the next instruction: the neural network won\'t be able to classify the samples, because data isn\'t linearly separable. Thus, we need a hidden layer.\nnn.add_dense(output_dim=2,\n             activation=Sigmoid(),\n             initializer=GaussianInitializer(gaussian_mean=0.0, gaussian_std=1))\nnn.add_dense(output_dim=1,\n             activation=Sigmoid(),\n             initializer=GaussianInitializer(gaussian_mean=0.0, gaussian_std=1))\nnn.train(x=x_train,\n         y=y_train,\n         objective=CrossEntropy(),\n         epochs=100,\n         lr=0.3)\n\nprint(\'Train data\')\nfor i in range(n_samples_train):\n    f = nn.predict(x_train[i])\n    print(\'Pred: {0:2f}. Label: {1:2d}\'.format(f[0], y_train[i]))\n\nprint(\'Test data\')\nfor i in range(n_samples_test):\n    f = nn.predict(x_test[i])\n    print(\'Pred: {0:2f}. Label: {1:2d}\'.format(f[0], y_test[i]))\n\nplot_boundaries_2d(nn=nn,\n                   resolution=100,\n                   init=0.0,\n                   end=1.0,\n                   xlabel=\'A\',\n                   ylabel=\'B\',\n                   title=\'Probability that |A-B|>0.2\',\n                   save_path=\'a_approx_b_boundaries.png\')\n'"
nnn/examples/a_greater_than_b.py,5,"b'""""""\na_greater_than_b.py: Example demonstrating that a linearly separable dataset can\n                     be classified using a neural network without any hidden layer.\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nfrom nnn.core.activations import Sigmoid\nfrom nnn.core.initializers import GaussianInitializer\nfrom nnn.core.neural_network import NeuralNetwork\nfrom nnn.core.objectives import CrossEntropy\nfrom nnn.utils.plots import plot_boundaries_2d, plot_data, plot_loss_function_3d\nfrom nnn.utils.loss_grid import loss_grid_nn\nimport numpy as np\n\nnp.random.seed(27)\n\nn_samples_train = 100\nn_samples_test = 25\nx_train = np.random.rand(n_samples_train, 2)\nx_test = np.random.rand(n_samples_test, 2)\ny_train = np.array([int(x[0] > x[1]) for x in x_train])\ny_test = np.array([int(x[0] > x[1]) for x in x_test])\n\nplot_data(x_train=x_train,\n          y_train=y_train,\n          x_test=x_test,\n          y_test=y_test,\n          init=0,\n          end=1,\n          xlabel=\'A\',\n          ylabel=\'B\',\n          title=\'Data plot. Star points are test samples\',\n          save_path=\'a_greater_than_b_data.png\')\n\nnn = NeuralNetwork(input_dim=2,\n                   output_dim=1)\nnn.add_dense(output_dim=1,\n             activation=Sigmoid(),\n             initializer=GaussianInitializer(gaussian_mean=0.0, gaussian_std=0.3))\nnn.train(x=x_train,\n         y=y_train,\n         objective=CrossEntropy(),\n         epochs=100,\n         lr=0.3)\n\nprint(\'Train data\')\nfor i in range(n_samples_train):\n    f = nn.predict(x_train[i])\n    print(\'Pred: {0:2f}. Label: {1:2d}\'.format(f[0], y_train[i]))\n\nprint(\'Test data\')\nfor i in range(n_samples_test):\n    f = nn.predict(x_test[i])\n    print(\'Pred: {0:2f}. Label: {1:2d}\'.format(f[0], y_test[i]))\n\nplot_boundaries_2d(nn,\n                   resolution=100,\n                   init=0.0,\n                   end=1.0,\n                   xlabel=\'A\',\n                   ylabel=\'B\',\n                   title=\'Probability that A>B\',\n                   save_path=\'a_greater_than_b_boundaries.png\')\n\nloss_g, w1, w2 = loss_grid_nn(nn=nn,\n                              objective=CrossEntropy(),\n                              x_train=x_train,\n                              y_train=y_train,\n                              resolution=20,\n                              w1_init=-5,\n                              w1_end=5,\n                              w2_init=-5,\n                              w2_end=5)\nplot_loss_function_3d(loss_grid=loss_g,\n                      w1=w1,\n                      w2=w2,\n                      xlabel = \'w1\',\n                      ylabel = \'w2\',\n                      title=\'Loss function\',\n                      save_path=None)\n'"
nnn/utils/__init__.py,0,b'from __future__ import absolute_import\nfrom . import plots\n'
nnn/utils/loss_grid.py,4,"b'""""""\nloss_grid.py: Computation of the loss grid for a given neural network and objective function\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\n\n\ndef loss_grid_nn(nn, objective, x_train, y_train, resolution=100, w1_init=0.0, w1_end=2.0, w2_init=0.0, w2_end=2.0):\n    w1 = np.linspace(w1_init, w1_end, resolution)\n    w2 = np.linspace(w2_init, w2_end, resolution)\n\n    loss_g = np.zeros(shape=(resolution, resolution))\n    for i in range(resolution):\n        for j in range(resolution):\n            nn.layers[0].weights = np.array([[1, w1[i], w2[j]]])\n            for k in range(len(x_train)):\n                pred = nn.predict(x_train[k])\n                loss_g[i, j] += objective.get_loss(y_train[k], pred)\n\n    return loss_g, w1, w2'"
nnn/utils/plots.py,4,"b'""""""\nplots.py: Module with plotting tools. It contains a function to plot the classification boundaries of a 2d classifier, among others.\nCopyright 2017 Ramon Vinas\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\ndef plot_boundaries_2d(nn, resolution=100, init=0.0, end=1.0, xlabel = \'w1\', ylabel = \'w2\', title=None, save_path=None):\n    print(\'Plotting boundaries ...\')\n    a = np.linspace(init, end, resolution)\n    b = np.linspace(init, end, resolution)\n\n    pred_grid = np.zeros(shape=(resolution, resolution))\n    for i in range(resolution):\n        for j in range(resolution):\n            pred_grid[i, j] = nn.predict([a[i], b[j]])\n\n    cmap = mpl.colors.LinearSegmentedColormap.from_list(\'my_colormap\', [\'orange\', \'black\', \'white\'], 256)\n    img = plt.imshow(pred_grid, origin=\'lower\', interpolation=\'quadric\', extent=[init, end, init, end], cmap=cmap)\n    plt.colorbar(img, cmap=cmap)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    if title is not None:\n        plt.title(title)\n    if save_path is not None:\n        plt.savefig(save_path)\n    plt.show()\n\n\ndef plot_data(x_train, y_train, x_test, y_test, xlabel = \'A\', ylabel = \'B\', init=0.0, end=1.0, title=None, save_path=None):\n    print(\'Plotting data ...\')\n    colors = [\'r\' if y == 1 else \'b\' for y in y_train]\n    plt.scatter(x_train[:, 0], x_train[:, 1], marker=\'o\', c=colors)\n    colors = [\'r\' if y == 1 else \'b\' for y in y_test]\n    plt.scatter(x_test[:, 0], x_test[:, 1], marker=\'o\', c=colors)\n    plt.scatter(x_test[:, 0], x_test[:, 1], marker=\'*\', s=20, c=\'w\')\n    plt.xlim(init, end)\n    plt.ylim(init, end)\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    if title is not None:\n        plt.title(title)\n    if save_path is not None:\n        plt.savefig(save_path)\n    plt.show()\n\n\ndef plot_loss_function_3d(loss_grid, w1, w2, xlabel = \'w1\', ylabel = \'w2\', title=None, save_path=None):\n    print(\'Plotting loss function ...\')\n\n    fig = plt.figure()\n    ax = Axes3D(fig)\n    x, y = np.meshgrid(w1, w2)\n    ax.plot_surface(x, y, loss_grid, cmap=""CMRmap_r"", lw=3, linestyles=""solid"")\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    if title is not None:\n        plt.title(title)\n    if save_path is not None:\n        plt.savefig(save_path)\n    plt.show()\n'"
