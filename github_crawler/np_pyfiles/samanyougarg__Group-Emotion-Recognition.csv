file_path,api_count,code
AlignDlib.py,4,"b'# #### 3.4 Dlib Align Class\n# Copyright 2015-2016 Carnegie Mellon University\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n""""""Module for dlib-based alignment.""""""\n\nimport numpy as np\nimport dlib\nimport cv2\n\nTEMPLATE = np.float32([\n    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n\nTPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\nMINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n\n\nclass AlignDlib:\n    """"""\n    Use `dlib\'s landmark estimation <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_ to align faces.\n\n    The alignment preprocess faces for input into a neural network.\n    Faces are resized to the same size (such as 96x96) and transformed\n    to make landmarks (such as the eyes and nose) appear at the same\n    location on every image.\n\n    Normalized landmarks:\n\n    .. image:: ../images/dlib-landmark-mean.png\n    """"""\n\n    #: Landmark indices.\n    INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n    OUTER_EYES_AND_NOSE = [36, 45, 33]\n\n    def __init__(self, facePredictor):\n        """"""\n        Instantiate an \'AlignDlib\' object.\n\n        :param facePredictor: The path to dlib\'s\n        :type facePredictor: str\n        """"""\n        assert facePredictor is not None\n\n        self.detector = dlib.get_frontal_face_detector()\n        self.predictor = dlib.shape_predictor(facePredictor)\n\n    def getAllFaceBoundingBoxes(self, rgbImg):\n        """"""\n        Find all face bounding boxes in an image.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :return: All face bounding boxes in an image.\n        :rtype: dlib.rectangles\n        """"""\n        assert rgbImg is not None\n\n        try:\n            return self.detector(rgbImg, 1)\n        except Exception as e:\n            print(""Warning: {}"".format(e))\n            # In rare cases, exceptions are thrown.\n            return []\n\n    def getLargestFaceBoundingBox(self, rgbImg, skipMulti=False):\n        """"""\n        Find the largest face bounding box in an image.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param skipMulti: Skip image if more than one face detected.\n        :type skipMulti: bool\n        :return: The largest face bounding box in an image, or None.\n        :rtype: dlib.rectangle\n        """"""\n        assert rgbImg is not None\n\n        faces = self.getAllFaceBoundingBoxes(rgbImg)\n        if (not skipMulti and len(faces) > 0) or len(faces) == 1:\n            return max(faces, key=lambda rect: rect.width() * rect.height())\n        else:\n            return None\n\n    def findLandmarks(self, rgbImg, bb):\n        """"""\n        Find the landmarks of a face.\n\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param bb: Bounding box around the face to find landmarks for.\n        :type bb: dlib.rectangle\n        :return: Detected landmark locations.\n        :rtype: list of (x,y) tuples\n        """"""\n        assert rgbImg is not None\n        assert bb is not None\n\n        points = self.predictor(rgbImg, bb)\n        return list(map(lambda p: (p.x, p.y), points.parts()))\n\n    def align(self, imgDim, rgbImg, bb=None,\n              landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP,\n              skipMulti=False):\n        r""""""align(imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP)\n\n        Transform and align a face in an image.\n\n        :param imgDim: The edge length in pixels of the square the image is resized to.\n        :type imgDim: int\n        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n        :type rgbImg: numpy.ndarray\n        :param bb: Bounding box around the face to align. \\\n                   Defaults to the largest face.\n        :type bb: dlib.rectangle\n        :param landmarks: Detected landmark locations. \\\n                          Landmarks found on `bb` if not provided.\n        :type landmarks: list of (x,y) tuples\n        :param landmarkIndices: The indices to transform to.\n        :type landmarkIndices: list of ints\n        :param skipMulti: Skip image if more than one face detected.\n        :type skipMulti: bool\n        :return: The aligned RGB image. Shape: (imgDim, imgDim, 3)\n        :rtype: numpy.ndarray\n        """"""\n        assert imgDim is not None\n        assert rgbImg is not None\n        assert landmarkIndices is not None\n\n        if bb is None:\n            bb = self.getLargestFaceBoundingBox(rgbImg, skipMulti)\n            if bb is None:\n                return\n\n        if landmarks is None:\n            landmarks = self.findLandmarks(rgbImg, bb)\n\n        npLandmarks = np.float32(landmarks)\n        npLandmarkIndices = np.array(landmarkIndices)\n\n        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices],\n                                   imgDim * MINMAX_TEMPLATE[npLandmarkIndices])\n        thumbnail = cv2.warpAffine(rgbImg, H, (imgDim, imgDim))\n\n        return thumbnail'"
bayesian_network.py,8,"b'""""""Module to predict the emotions for a group image using a Bayesian model.""""""\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport numpy as np\nimport pandas as pd\nfrom pgmpy.models import BayesianModel\nfrom pgmpy.inference import VariableElimination\nfrom pgmpy.factors.discrete import TabularCPD\nfrom sklearn.preprocessing import normalize\nfrom copy import deepcopy\nimport os\n\n# class mapping\nclasses = {0: \'Positive\', 1: \'Negative\', 2: \'Neutral\'}\n# reverse class mapping\nreverse_classes = {\'Positive\': 0, \'Negative\': 1, \'Neutral\': 2}\n\n# function to load the Bayesian model\ndef load_model():\n    # 1. Prepare and read the labels histogram file\n\n    # Assign the file to a variable\n    labels_histogram = \'labels_histogram.xlsx\'\n\n    # Read the file using pandas\' as a dataframe\n    # sheet_name specifies the sheet to read\n    # header = 0 tells pandas to consider the first line as the header\n    df = pd.read_excel(labels_histogram, sheet_name=""labels_histogram"", header=0)\n\n    # print the first 5 rows of the dataframe\n    df.head()\n\n    # -----------------------------------------------------------------\n\n    # 2. Prepare data for the Bayesian Network\n\n    # List of labels\n    # first column of the dataframe\n    # skip last row i.e. ""total""\n    labels_list = df[\'label\'][:-1]\n\n\n    # Count total Positive, Negative and Neutral emotions\n\n    # get the value of the last row in the positive column of the dataframe\n    total_positive_labels = df[\'positive\'].iloc[-1]\n\n    # get the value of the last row in the neutral column of the dataframe\n    total_neutral_labels = df[\'neutral\'].iloc[-1]\n\n    # get the value of the last row in the negative column of the dataframe\n    total_negative_labels = df[\'negative\'].iloc[-1]\n\n\n    # Frequencies of Positive, Negative and Neutral emotions for each label\n\n    # get the column with the name ""positive"" as a numpy array\n    # skip last row\n    positive_ndarray = np.array(df[\'positive\'][:-1])\n\n    # get the column with the name ""neutral"" as a numpy array\n    # skip last row\n    neutral_ndarray = np.array(df[\'neutral\'][:-1])\n\n    # get the column with the name ""negative"" as a numpy array\n    # skip last row\n    negative_ndarray = np.array(df[\'negative\'][:-1])\n\n\n    # -----------------------------------------------------------------\n\n    # 3. Model\n\n    # Define the edges for the model\n\n    # edge from Emotion node to each label node i.e. 809 edges\n    edges_list = [(""Emotion"", label) for label in labels_list]\n    # edge from Emotion node to CNN node\n    edges_list.append((""Emotion"", ""CNN""))\n\n\n    # Define the Model\n    model = BayesianModel()\n\n\n    # Add nodes and edges to the model\n    # Add all the labels from labels_list as nodes\n    model.add_nodes_from(labels_list)\n    # Add all the edges from edges_list\n    model.add_edges_from(edges_list)\n\n\n    # Create the Conditional Probability Distribution Table for the Emotion node\n    # Name of the node is ""Emotion""\n    # Total variables = 3 i.e. 1 for each emotion\n    # Since, each emotion is equally likely so each will have 1/3 probability\n    emotion_cpd = TabularCPD(""Emotion"", 3, values=[[1./3,1./3,1./3]])\n\n\n    # Create the Conditional Probability Distribution Table for the CNN node\n\n    # Calculate the conditional probability values using the confusion matrix obtained from the CNN\n    # Store the confusion matrix obtained from CNN as a numpy array\n\n    # \t\t    Pos\t\tNeg\t\tNeu\n    # Pos\t\t583.0\t117.0\t60.0\n    # Neg\t\t65.0\t396.0\t43.0\n    # Neu\t\t149.0\t305.0\t211.0\n\n    cnn_confusion_matrix = np.array([[2735.0, 210.0, 430.0],\n                                     [330.0, 791.0, 552.0],\n                                     [374.0, 387.0, 1097.0]])\n    # Normalize the confusion matrix\n    cnn_confusion_matrix = normalize(cnn_confusion_matrix, axis=1, norm=""l1"")\n    # CNN CPD values will be the transpose of the confusion matrix\n    cnn_values = cnn_confusion_matrix.T\n\n\n    # Name of the node is ""CNN""\n    # Total variables = 3 i.e. 1 for each emotion\n    # Set the Emotion node as the evidence\n    cnn_cpd = TabularCPD(""CNN"", 3, evidence=[\'Emotion\'], evidence_card=[3], values=cnn_values)\n\n\n    # Create Conditional Probability Distribution Tables for each Label node\n\n    # create a list to store each label cpd\n    label_cpd_list = []\n\n    for i in range(len(labels_list)):\n        # P(label=1|Emotion=Positive)\n        p_label_1_given_emo_positive = float(positive_ndarray[i]/total_positive_labels)\n        # P(label=1|Emotion=Neutral)\n        p_label_1_given_emo_neutral = float(neutral_ndarray[i]/total_neutral_labels)\n        # P(label=1|Emotion=Negative)\n        p_label_1_given_emo_negative = float(negative_ndarray[i]/total_negative_labels)\n\n        # if P(label=1|Emotion=Positive) is 0, set it to 0.0001 to fix the error\n        if p_label_1_given_emo_positive == 0.0:\n            p_label_1_given_emo_positive = 0.0001\n        \n        # if P(label=1|Emotion=Neutral) is 0, set it to 0.0001 to fix the error\n        if p_label_1_given_emo_neutral == 0.0:\n            p_label_1_given_emo_neutral = 0.0001\n\n        # if P(label=1|Emotion=Negative) is 0, set it to 0.0001 to fix the error  \n        if p_label_1_given_emo_negative == 0.0:\n            p_label_1_given_emo_negative = 0.0001\n        \n        # P(label=0|Emotion=Positive)\n        p_label_0_given_emo_positive = 1.0 - p_label_1_given_emo_positive\n        # P(label=0|Emotion=Neutal)\n        p_label_0_given_emo_neutral = 1.0 - p_label_1_given_emo_neutral\n        # P(label=0|Emotion=Negative)\n        p_label_0_given_emo_negative = 1.0 - p_label_1_given_emo_negative\n        \n        # Condition Probability Table for the label\n        label_conditional_probability_table = [[p_label_0_given_emo_positive, p_label_0_given_emo_negative, p_label_0_given_emo_neutral], \n                                                [p_label_1_given_emo_positive, p_label_1_given_emo_negative, p_label_1_given_emo_neutral]]\n        \n        # generate the conditional probability table for that label\n        # Name of the node is the name of the label\n        # Total variables = 2 i.e. either 1 or 0\n        # Set the Emotion node as the evidence\n        label_cpd = TabularCPD(labels_list[i], 2, evidence=[\'Emotion\'], evidence_card=[3], values=label_conditional_probability_table)\n\n    #         print(cpd_label)\n    #         \xe2\x95\x92\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\xa4\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\xa4\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\xa4\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x95\n    #         \xe2\x94\x82 Emotion \xe2\x94\x82 Emotion_0            \xe2\x94\x82 Emotion_1 \xe2\x94\x82 Emotion_2             \xe2\x94\x82\n    #         \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xa4\n    #         \xe2\x94\x82 gown_0  \xe2\x94\x82 0.974076983503535    \xe2\x94\x82 1.0       \xe2\x94\x82 0.9983333333333333    \xe2\x94\x82\n    #         \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xbc\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\x80\xe2\x94\xa4\n    #         \xe2\x94\x82 gown_1  \xe2\x94\x82 0.025923016496465043 \xe2\x94\x82 0.0       \xe2\x94\x82 0.0016666666666666668 \xe2\x94\x82\n    #         \xe2\x95\x98\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\xa7\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\xa7\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\xa7\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x90\xe2\x95\x9b\n\n        # add it to the list\n        label_cpd_list.append(label_cpd)\n\n\n    # Add Conditional Probability Tables to the Model\n\n    # Add the emotion and CNN nodes to the model\n    model.add_cpds(emotion_cpd, cnn_cpd)\n    # Add the cpd for each label\n    for label_cpd in label_cpd_list: model.add_cpds(label_cpd)\n\n\n    # Check if model is valid\n\n    print(model.check_model())  # returns True if the model is correct\n\n    # print the first 10 cpds from the model\n    model.get_cpds()[:10]\n\n    # return\n    # 1. the model\n    # 2. the labels list\n    return model, labels_list\n\n\n# function to perform inference on the Bayesian network\ndef inference(model, labels_list, labels_for_image, cnn_prediction=None):\n    # Set evidences for the nodes using results from Vision API and CNN\n\n    # if detected label is present in labels list then set that label to 1 else 0\n    label_evidences = {label:(1 if label in labels_for_image else 0) for label in labels_list}\n\n    # Initialize Variable Elimination and query\n\n    # Set the inference method\n    emotion_infer = VariableElimination(model)\n\n    # Compute the probability of the emotions given the detected labels list\n    q = emotion_infer.query([\'Emotion\'], evidence=label_evidences)\n    # print(q[\'Emotion\'])\n\n    # set bayes_prediction and bayes + cnn prediction to None\n    bayes_prediction, bayes_cnn_prediction = None, None\n    # a dictionary to store emotion predictions predicted by the Bayesian network\n    emotion_dict = {\'Positive\': 0, \'Negative\': 1, \'Neutral\': 2}\n    # predictions for the emotions\n    emotion_preds = q[\'Emotion\'].values\n    \n    # if all the predictions are nan then set prediction label to None\n    if np.isnan(emotion_preds).all():\n        bayes_prediction = ""None""\n    else:\n        # set the probability for Positive emotion for the image\n        emotion_dict[\'Positive\'] = round(emotion_preds[0], 4)\n        # set the probability for Negative emotion for the image\n        emotion_dict[\'Negative\'] = round(emotion_preds[1], 4)\n        # set the probability for Neutral emotion for the image\n        emotion_dict[\'Neutral\'] = round(emotion_preds[2], 4)\n        # set bayes prediction to be the class with the highest probability\n        bayes_prediction = classes[np.argmax(emotion_preds)]\n\n    # a dictionary to store emotion predictions predicted by the Bayesian network + CNN node\n    emotion_cnn_dict = deepcopy(emotion_dict)\n\n    # if the CNN prediction is not None\n    if cnn_prediction is not None:\n        # get prediction from CNN\n        label_evidences[\'CNN\'] = reverse_classes[cnn_prediction]\n\n        # Compute the probability of the emotions given the detected labels list\n        q = emotion_infer.query([\'Emotion\'], evidence=label_evidences)\n        # print(q[\'Emotion\'])\n        emotion_preds = q[\'Emotion\'].values\n\n        # if all the predictions are nan then set prediction label to None\n        if np.isnan(emotion_preds).all():\n            bayes_cnn_prediction = ""None""\n        else:\n            # set the probability for Positive emotion for the image\n            emotion_cnn_dict[\'Positive\'] = round(emotion_preds[0], 4)\n            # set the probability for Negative emotion for the image\n            emotion_cnn_dict[\'Negative\'] = round(emotion_preds[1], 4)\n            # set the probability for Neutral emotion for the image\n            emotion_cnn_dict[\'Neutral\'] = round(emotion_preds[2], 4)\n            # set bayes + CNN prediction to be the class with the highest probability\n            bayes_cnn_prediction = classes[np.argmax(emotion_preds)]\n    else:\n        bayes_cnn_prediction = bayes_prediction\n\n    # return -\n    # i. label of the predicted emotion for the whole image (using the Bayesian Network)\n    # ii. label of the predicted emotion for the whole image (using the Bayesian Network + CNN as a node)\n    # iii. Bayesian predictions for the whole image\n    # iv. Bayesian + CNN predictions for the whole image\n    return bayes_prediction, bayes_cnn_prediction, emotion_dict, emotion_cnn_dict'"
classify_image.py,0,"b'""""""Module to classify an image as Positive, Negative or Neutral.\n\n    python classify_image.py input/val/Positive/ Positive\n\n""""""\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport sys\nimport glob\nimport numpy as np\nimport image_preprocessing\nimport cnn\nimport bayesian_network\nimport json\n\nimage_label_dict = {\n    ""image_happy_1.jpg"": [\'people\', \'friendship\', \'fun\', \'event\', \'drinking\', \'happy\', \'picnic\', \'recreation\', \'smile\', \'leisure\'],\n    ""image_happy_2.jpg"": [\'hair\', \'facial expression\', \'fun\', \'friendship\', \'hairstyle\', \'smile\', \'yellow\', \'event\', \'human\', \'laugh\'],\n    ""image_happy_neutral_1.jpg"": [\'people\', \'family taking photos together\', \'social group\', \'child\', \'father\', \'event\',\'family\', \'fun\', \'smile\', \'photography\'],\n    ""image_happy_neutral_2.jpg"": [\'people\', \'social group\', \'fun\', \'team\', \'event\', \'crew\', \'tourism\', \'uniform\', \'leisure\', \'smile\'],\n    ""image_happy_neutral_3.jpg"": [\'people\', \'tribe\', \'fun\', \'human\', \'smile\', \'community\', \'child\', \'happy\', \'adaptation\'],\n    ""image_happy_neutral_4.jpg"": [\'people\', \'child\', \'smile\', \'community\', \'youth\', \'friendship\', \'adaptation\', \'fun\', \'happy\', \'event\'],\n    ""image_neutral_sad.jpg"": [\'hair\', \'face\', \'chin\', \'hairstyle\', \'cool\', \'forehead\', \'black hair\', \'fun\', \'neck\', \'smile\'],\n    ""image_neutral.jpg"": [\'face\', \'people\', \'facial expression\', \'child\', \'smile\', \'skin\', \'fun\', \'child model\',\'human\', \'happy\'],\n    ""image_sad_1.jpg"": [\'people\', \'community\', \'tribe\', \'adaptation\', \'tradition\', \'event\', \'child\', \'smile\', \'tourism\', \'turban\']\n}\n\n# function to classify an image\ndef classify_image(image_folder_path, image_name, real_label, cnn_model, bayesian_model, labels_list):\n    # if image is from collection, get the labels from the dictionary\n    if image_name in image_label_dict.keys():\n        print(""RadhaKrishna"")\n        labels = image_label_dict[image_name]\n    # else get the labels from the Google Vision API\n    else:\n        labels = image_preprocessing.detect_labels(""input/"" + image_name)\n\n    # labels = [\'people\', \'friendship\', \'fun\', \'event\', \'drinking\', \'happy\', \'picnic\', \'recreation\', \'smile\', \'leisure\']\n\n    print(""RadhaKrishna"")\n    print(labels)\n\n    # preprocess the image\n    image_preprocessing.preprocess(image_folder_path, image_name)\n\n    # Gets the following using the CNN model -\n    # i. label of the predicted emotion for the whole image\n    # ii. mean cnn predictions for all the faces in the image\n    # iii. a boolean that specifies whether any faces were detected in the image\n    cnn_label, cnn_dict, faces_detected = cnn.predict_image(cnn_model, image_folder_path + ""Aligned/"", image_name)\n\n    # Gets the following using the Bayesian model -\n    # i. label of the predicted emotion for the whole image (using the Bayesian Network)\n    # ii. label of the predicted emotion for the whole image (using the Bayesian Network + CNN as a node)\n    # iii. Bayesian predictions for the whole image\n    # iv. Bayesian + CNN predictions for the whole image\n    bayesian_label, bayesian_cnn_label, emotion_dict, emotion_cnn_dict = bayesian_network.inference(bayesian_model, labels_list, labels, cnn_label)\n\n    print(""Faces detected: "" + str(faces_detected))\n    print(""Real Label: "" + str(real_label))\n    print(""CNN Label: "" + str(cnn_label))\n    print(""Bayesian Label: "" + str(bayesian_label))\n    print(""Bayesian + CNN Label: "" + str(bayesian_cnn_label))\n\n    # return the label of the emotion with the highest probability\n    return bayesian_cnn_label\n\ndef main(image_folder_path, real_label):\n    print(""RadhaKrishna"")\n    # load the cnn model\n    cnn_model = cnn.load_model()\n    # load the bayesian model\n    bayesian_model, labels_list = bayesian_network.load_model()\n    # for each image in the test path\n    for file in sorted(glob.glob(image_folder_path + ""*.jpg"")):\n        # extract the image name from the image path\n        image_name = (file.split(\'/\'))[-1]\n        print(""Image: "" + image_name)\n        # classify the image\n        prediction = classify_image(image_folder_path, image_name, real_label, cnn_model, bayesian_model, labels_list)\n\n\nif __name__==""__main__"":\n    main(sys.argv[1], sys.argv[2])'"
cnn.py,4,"b'""""""Module to predict the emotions for a face image using a pre-trained CNN model.""""""\n\n#!/usr/bin/env python\n# coding: utf-8\n\nfrom keras.models import model_from_json\nfrom keras.preprocessing.image import img_to_array, load_img\nimport numpy as np\nimport glob, os\nimport cv2\nfrom tensorflow.python.framework.ops import Tensor\nfrom typing import Tuple, List\nfrom keras.engine import training\nfrom tensorflow.python.framework.ops import Tensor\nfrom keras.models import Model, Input\nfrom keras.layers import Average\nimport keras.layers as layers\n\n# class mapping\nclasses = {0: \'Negative\', 1: \'Neutral\', 2: \'Positive\'}\n\n# function to create an ensemble of models\ndef ensemble_models(models, model_input):\n    # append the output of each model to a list\n    model_output = [model(model_input) for model in models] \n    # average the outputs of each model\n    avg_output = layers.average(model_output) \n    # build the ensemble model having same input as our models but the average\n    # of the output of our models as output\n    ensemble_model = Model(inputs=model_input, outputs=avg_output, name=\'ensemble\')  \n    # return the ensembled model\n    return ensemble_model\n\n# function to load a pretrained CNN models\ndef load_model():\n    # Generate an ensemble of models and save to disk\n\n    # model_names = [""model_vgg"", ""model_aligned_gen_128"", ""model_vggface"", ""model_aligned_gen_128_2048""]\n    # models = []\n\n    # for model_name in model_names:\n    #     # read the model json file\n    #     json_file = open(\'models/\' + model_name + \'.json\', \'r\')\n    #     loaded_model_json = json_file.read()\n    #     json_file.close()\n\n    #     # load the model from json file\n    #     model = model_from_json(loaded_model_json)\n    #     # load weights into new model\n    #     model.load_weights(\'models/\' + model_name + \'.h5\')\n        \n    #     # append the model to our models list\n    #     models.append(model)\n\n    # # input layer for our ensemble model\n    # model_input = Input(shape = models[0].input_shape[1:])\n    # # create the ensemble model from our models\n    # ensemble_model = ensemble_models(models, model_input)\n\n    # # serialize model to JSON\n    # model_json = ensemble_model.to_json()\n    # with open(""radhakrishna.json"", ""w"") as json_file:\n    #     json_file.write(model_json)\n    # # serialize weights to HDF5\n    # ensemble_model.save_weights(""radhakrishna.h5"")\n    # print(""Saved model to disk"")\n\n    # # -------------------------------------------------------------------\n\n    # Load the saved ensemble model\n\n    # read the model json file\n    json_file = open(\'model.json\', \'r\')\n    loaded_model_json = json_file.read()\n    json_file.close()\n\n    # load the model from json file\n    model = model_from_json(loaded_model_json)\n    # load weights into new model\n    model.load_weights(\'model.h5\')\n\n    print(""Loaded CNN model from disk"")\n\n    return model\n\n# function to predict the emotions for an individual face\ndef predict_face(model, image_path):\n    # Load the image and resize to 64X64\n    image = load_img(image_path, target_size=(64, 64))\n    # Covert to a numpy array\n    image = img_to_array(image)\n    # Normalize it\n    image = image / 255\n    # Expand dimensions\n    image = np.expand_dims(image, axis=0)\n\n    # Get the predicted probabilities for each class\n    pred = model.predict(image)\n    # Get the class with the highest probability\n    pred_digits=np.argmax(pred,axis=1)\n\n    # print(""\\n"")\n    # print((image_path.split(""/"")[-1])[:-4])\n    # print(""Predicted probabilities: "" + str(pred[0])) \n    # print(""Predicted Index: "" + str(pred_digits[0]))\n    # print(""\\n"")\n\n    # return the predicted probabilities for the face\n    return pred\n\n# function to predict the emotions for the whole image\ndef predict_image(model, input_path, image_path):\n    # extract image name from the image path\n    image_name = (image_path.split(""/"")[-1])[:-4]\n\n    # list to store predictions for all faces in the image\n    predictions_list = []\n    # a boolean that specifies whether faces were detected in the image\n    faces_detected = True\n\n    # for each face image in the input directory\n    for face_image in glob.glob(input_path + ""*.jpg""):\n        # extract image name from the image path\n        face_image_name = (face_image.split(""/"")[-1])[:-4]\n        # if face is from the current image\n        if (image_name + ""_face"" in face_image_name):\n            # get the predicted probabilities for current face image\n            predicted_probabilities = predict_face(model, face_image)\n            # append to the probabilities list\n            predictions_list.append(predicted_probabilities)\n\n    # if predictions list is empty\n    if predictions_list == []:\n        # set faces_detected = False\n        faces_detected = False\n        \n    # if faces were detected in the image\n    if faces_detected:\n        # mean of the predicted probabilities for each face in the image\n        mean_probabilities_for_image = np.mean(predictions_list, axis=0)\n        # print(mean_probabilities_for_image)\n        emotion_dict = {\'Positive\': 0, \'Negative\': 1, \'Neutral\': 2}\n        # set the mean probability for Positive emotion for the image\n        emotion_dict[\'Positive\'] = float(round(mean_probabilities_for_image[0][2], 4))\n        # set the mean probability for Negative emotion for the image\n        emotion_dict[\'Negative\'] = float(round(mean_probabilities_for_image[0][0], 4))\n        # set the mean probability for Neutral emotion for the image\n        emotion_dict[\'Neutral\'] = float(round(mean_probabilities_for_image[0][1], 4))\n\n        # Return -\n        # i. label of the predicted emotion for the whole image\n        # ii. mean cnn predictions for all the faces in the image\n        # iii. a boolean that specifies whether any faces were detected in the image\n        return classes[np.argmax(mean_probabilities_for_image)], emotion_dict, faces_detected\n    return None, None, faces_detected\n'"
evaluate.py,0,"b'""""""Module to evaluate full pipeline on the validation set.\n\n    python evaluate.py\n""""""\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport sys\nimport glob\nimport numpy as np\nimport image_preprocessing\nimport cnn\nimport bayesian_network\nimport json\nimport pandas as pd\n\n# class mapping\nclasses = {""Positive"": 0, ""Neutral"": 1, ""Negative"": 2, ""None"": 3}\n\n# function to classify an image\ndef classify_image(image_folder_path, image_name, real_label, cnn_model, bayesian_model, labels_list):\n    with open(\'val_labels.json\', mode=\'r\', encoding=\'utf-8\') as f:\n        image_labels_dict = json.load(f)\n    labels = image_labels_dict[image_name]\n\n    # print(""RadhaKrishna"")\n#     print(labels)\n\n    # preprocess the image\n    image_preprocessing.preprocess(image_folder_path, image_name)\n\n    # get mean cnn predictions for the faces from the image\n    cnn_label, cnn_dict, faces_detected = cnn.predict_image(cnn_model, image_folder_path + ""Aligned/"", image_name)\n\n    # get the bayesian and bayesian + cnn predictions for the image\n    bayesian_label, bayesian_cnn_label, emotion_dict, emotion_cnn_dict = bayesian_network.inference(bayesian_model, labels_list, labels, cnn_label)\n\n    # print(""Faces detected: "" + str(faces_detected))\n    # print(""Real Label: "" + str(real_label))\n    # print(""CNN Label: "" + str(cnn_label))\n    # print(""Bayesian Label: "" + str(bayesian_label))\n    # print(""Bayesian + CNN Label: "" + str(bayesian_cnn_label))\n\n    return classes[real_label], classes[str(cnn_label)], classes[str(bayesian_label)], classes[str(bayesian_cnn_label)], faces_detected\n\n\n\n# load the cnn model\ncnn_model = cnn.load_model()\n# load the bayesian model\nbayesian_model, labels_list = bayesian_network.load_model()\n\n\n# function to evaluate the pipeline on a given directory\ndef evaluate(image_folder_path, real_label):\n    # print(""RadhaKrishna"")\n    # get the count of total number of files in the directory\n    _, _, files = next(os.walk(image_folder_path))\n    file_count = len(files)-1\n    # list to store the predictions\n    predictions = []\n    # set count = 1\n    i = 1\n\n    # for each image in the directory\n    for file in sorted(glob.glob(image_folder_path + ""*.jpg"")):\n        # extract the image name\n        image_name = (file.split(\'/\'))[-1]\n        print(""Image: "" + image_name)\n        print(str(i) + ""/"" + str(file_count))\n        # create a dict to store the image name and predictions\n        prediction = {""Image"": image_name}\n        prediction[""Actual""], prediction[""CNN""], prediction[""Bayesian""], prediction[""Bayesian + CNN""], prediction[""Faces Detected""] = classify_image(image_folder_path, image_name, real_label, cnn_model, bayesian_model, labels_list)\n        # append the dict to the list of predictions\n        predictions.append(prediction)\n        # increase the count\n        i+=1\n    # return the predictions list\n    return predictions\n\n# class list\nclass_list = [\'Positive\', \'Neutral\', \'Negative\']\npredictions_list = []\n\n# for each class in the class list\nfor emotion_class in class_list:\n    # evaluate all the images in that folder\n    predictions = evaluate(\'input/val/\' + emotion_class + \'/\', emotion_class)\n    # add the predictions to the predictions list\n    predictions_list += predictions\n# create a pandas dataframe from the predictions list\ndf = pd.DataFrame(predictions_list)\n# store the dataframe to a file\ndf.to_pickle(\'predictions\')\n\n\n\n\n'"
evaluate_300.py,0,"b'""""""Module to evaluate full pipeline on 100 images of each class of the validation set.""""""\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport sys\nimport glob\nimport numpy as np\nimport image_preprocessing\nimport cnn\nimport bayesian_network\nimport json\nimport pandas as pd\n\n\nclasses = {""Positive"": 0, ""Neutral"": 1, ""Negative"": 2, ""None"": 3}\ndef classify_image(image_folder_path, image_name, real_label, cnn_model, bayesian_model, labels_list):\n    with open(\'val_labels.json\', mode=\'r\', encoding=\'utf-8\') as f:\n        image_labels_dict = json.load(f)\n    labels = image_labels_dict[image_name]\n\n    # print(""RadhaKrishna"")\n#     print(labels)\n\n    # preprocess the image\n    # image_preprocessing.preprocess(image_folder_path, image_name)\n\n    # get mean cnn predictions for the faces from the image\n    cnn_label, cnn_dict, faces_detected = cnn.predict_image(cnn_model, image_folder_path + ""Aligned/"", image_name)\n\n    # get the bayesian and bayesian + cnn predictions for the image\n    bayesian_label, bayesian_cnn_label, emotion_dict, emotion_cnn_dict = bayesian_network.inference(bayesian_model, labels_list, labels, cnn_label)\n\n    # print(""Faces detected: "" + str(faces_detected))\n    # print(""Real Label: "" + str(real_label))\n    # print(""CNN Label: "" + str(cnn_label))\n    print(""Bayesian Label: "" + str(bayesian_label))\n    # print(""Bayesian + CNN Label: "" + str(bayesian_cnn_label))\n\n    return classes[real_label], classes[str(cnn_label)], classes[str(bayesian_label)], classes[str(bayesian_cnn_label)], faces_detected\n\n\n\n# print(""RadhaKrishna"")\ncnn_model = cnn.load_model()\nbayesian_model, labels_list = bayesian_network.load_model()\n\n\ndef evaluate(image_folder_path, real_label):\n    # print(""RadhaKrishna"")\n    _, _, files = next(os.walk(image_folder_path))\n    file_count = len(files)-1\n    predictions = []\n    i = 1\n    for file in sorted(glob.glob(image_folder_path + ""*.jpg"")):\n        image_name = (file.split(\'/\'))[-1]\n        print(""Image: "" + image_name)\n        print(str(i) + ""/"" + str(file_count))\n        prediction = {""Image"": image_name}\n        prediction[""Actual""], prediction[""CNN""], prediction[""Bayesian""], prediction[""Bayesian + CNN""], prediction[""Faces Detected""] = classify_image(image_folder_path, image_name, real_label, cnn_model, bayesian_model, labels_list)\n        predictions.append(prediction)\n        i+=1\n        if (i==100): \n            break\n    return predictions\n\n\nclass_list = [\'Positive\', \'Neutral\', \'Negative\']\npredictions_list = []\nfor emotion_class in class_list:\n    predictions = evaluate(\'input/val/\' + emotion_class + \'/\', emotion_class)\n    predictions_list += predictions\ndf = pd.DataFrame(predictions_list)\ndf.to_pickle(\'predictions_300\')\n\n\n\n\n'"
image_preprocessing.py,1,"b'""""""Module to preprocess images.""""""\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport cv2\nimport os\nfrom PIL import Image\nimport glob\nimport numpy as np\nfrom AlignDlib import AlignDlib\nimport io\nfrom mtcnn.mtcnn import MTCNN\n\n# ## Face and Label Detection\n\n# ### 1. Using Google Vision API\n\n# #### 1.1 Import Google Vision Library\nfrom google.cloud import vision\n\n\n# #### 1.2 Label Detection function\ndef detect_labels(path):\n    """"""Detects labels in the file.""""""\n    client = vision.ImageAnnotatorClient()\n\n    with io.open(path, \'rb\') as image_file:\n        content = image_file.read()\n\n    image = vision.types.Image(content=content)\n\n    response = client.label_detection(image=image)\n    labels = response.label_annotations\n    \n    labels_list = [(label.description).lower() for label in labels]\n    \n    return labels_list\n\n\n# #### 1.3 Face Detection function\ndef detect_face_google(face_file, max_results=4):\n    """"""Uses the Vision API to detect faces in the given file.\n\n    Args:\n        face_file: A file-like object containing an image with faces.\n\n    Returns:\n        An array of Face objects with information about the picture.\n    """"""\n    client = vision.ImageAnnotatorClient()\n\n    content = face_file.read()\n    image = vision.types.Image(content=content)\n\n    return client.face_detection(image=image).face_annotations\n\n\n# #### 1.4 Function to crop the detected faces\ndef crop_faces_google(image_file, cropped_images_path, faces):\n    count = 1\n    # open the image\n    image = Image.open(image_file)\n    \n#     print(len(faces))\n\n    # if faces is null, then it means no face was detected in the image\n    if not faces:\n        print(""No face detected in the image."")\n        return\n    \n    # for each detected face in the faces list\n    for face in faces:\n        # get the coordinates for each vertex\n        coordinates = [(vertex.x, vertex.y)\n            for vertex in face.bounding_poly.vertices]\n        \n        # separate the x and y coordinates\n        x_coordinates, y_coordinates = [], []\n        for vertex in face.bounding_poly.vertices:\n            x_coordinates.append(vertex.x)\n            y_coordinates.append(vertex.y)\n        \n        x0, x1, y0, y1 = x_coordinates[0], x_coordinates[2], y_coordinates[0], y_coordinates[2]\n        \n        # set the coordinates of the box for each face\n        box = (x0, y0, x1, y1)\n        # crop the image using coordinates of the box\n        cropped_image = image.crop(box)\n        \n        # extract image name from filename\n        image_name = (image_file.split(""/"")[-1])[:-4]\n        # save the cropped image\n        cropped_image.save(cropped_images_path + image_name + ""_face_"" + str(count) + "".jpg"")\n        count+=1\n\n\n# #### 1.5 Function to resize the cropped faces\ndef resize_faces_google(cropped_images_path, scaled_images_path, size):\n    count = 1\n    # for each image in the cropped images path\n    for file in glob.glob(cropped_images_path+""*.jpg""):\n        # read the image\n        image = cv2.imread(file)\n        # get the height and width of the image\n        height, width = image.shape[:2]\n        \n        # get the height and weight ratios\n        height_ratio, width_ratio = float(size/height), float(size/width)\n\n        # resize the image making sure that the original ratio is maintained\n        resized = cv2.resize(image, None, fx=width_ratio, fy=height_ratio, interpolation=cv2.INTER_AREA)\n        \n        # extract image name from full file name\n        image_name = (file.split(""/"")[-1])\n        # save the scaled image\n        cv2.imwrite(scaled_images_path + image_name, resized)\n\n\n# #### 1.6 Apply preprocessing to the dataset using the functions above\ndef preprocess_google(image_file):\n    with open(image_file, \'rb\') as image:\n        # detect faces in the image\n        faces = detect_face_google(image)\n        \n        # Reset file pointer, so we can read the file again\n        image.seek(0)\n        # crop detected faces and save in ""Faces"" directory\n        crop_faces_google(image_file, ""test/Faces/"", faces)\n\n    # resize the cropped faces stored in the ""Faces"" directory and save in ""Scaled"" directory\n    resize_faces_google(""test/Faces/"", ""test/Scaled/"", 64)\n\n# ---\n\n# ### 2. Using OpenCV\n\n# #### 2.1 Load the serialized DNN model from disk\nnet = cv2.dnn.readNetFromCaffe(\'deploy.prototxt.txt\',\n                               \'res10_300x300_ssd_iter_140000.caffemodel\'\n                               )\n\n\n# #### 2.2 Function to detect and crop faces using OpenCV DNN\ndef extract_faces_cv_dnn(image_file, cropped_images_path):\n    # load the input image and construct an input blob for the image\n    # by resizing to a fixed 300x300 pixels and then normalizing it\n    image = cv2.imread(image_file)\n    (h, w) = image.shape[:2]\n    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300,\n                                 300), (104.0, 177.0, 123.0))\n\n    # pass the blob through the network and obtain the detections and\n    # predictions\n    net.setInput(blob)\n    faces = net.forward()\n    count = 1\n    image = Image.open(image_file)\n    \n    if faces is None:\n        print(""No face detected in the image."")\n        return\n\n    # loop over the faces\n    for i in range(faces.shape[2]):\n\n        # extract the confidence (i.e., probability) associated with the\n        # prediction\n        confidence = faces[0, 0, i, 2]\n\n        # filter out weak detections by ensuring the `confidence` is\n        # greater than the minimum confidence\n        if confidence > 0.5:\n\n            # compute the (x, y)-coordinates of the bounding box for the\n            # object\n            box = faces[0, 0, i, 3:7] * np.array([w, h, w, h])\n            cropped_image = image.crop(box)\n\n            image_name = (image_file.split(""/"")[-1])[:-4]\n            cropped_image.save(cropped_images_path + image_name + ""_face_"" + str(count) + "".jpg"")\n            count+=1\n\n\n# #### 2.3 Load the Haar Cascade model\nfaceCascade = cv2.CascadeClassifier(\'haarcascade_frontalface_default.xml\')\n\n# #### 2.4 Function to detect and crop faces using Haar Cascade\ndef extract_faces_cv(image_file, cropped_images_path):\n    # Read the image\n    image = cv2.imread(image_file)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Detect faces in the image\n    faces = faceCascade.detectMultiScale(\n        gray,\n        scaleFactor=1.1,\n        minNeighbors=5,\n        minSize=(30, 30)\n    )\n\n    count = 0\n    image = Image.open(image_file)\n    \n    if faces is None:\n        print(""No face detected in the image."")\n        return\n\n    # loop over the faces\n    for (x, y, w, h) in faces:\n        # crop the face in the image\n        cropped_image = image.crop((x, y, x+w, y+h))\n        image_name = (image_file.split(""/"")[-1])[:-4]\n        cropped_image.save(cropped_images_path + image_name + ""_face_"" + str(count) + "".jpg"")\n        count+=1\n    return count\n\n\n# #### 2.5 Function to resize the cropped faces\ndef resize_faces_cv(cropped_images_path, scaled_images_path, size):\n    count = 1\n    # for each image in the cropped images path\n    for file in glob.glob(cropped_images_path+""*.jpg""):\n        # read the image\n        image = cv2.imread(file)\n        # get the height and width of the image\n        height, width = image.shape[:2]\n        \n        # get the height and weight ratios\n        height_ratio, width_ratio = float(size/height), float(size/width)\n\n        # resize the image making sure that the original ratio is maintained\n        resized = cv2.resize(image, None, fx=width_ratio, fy=height_ratio, interpolation=cv2.INTER_AREA)\n        \n        # extract image name from full file name\n        image_name = (file.split(""/"")[-1])\n        # save the scaled image\n        cv2.imwrite(scaled_images_path + image_name, resized)\n\ndef preprocess_cv(image_file):\n    # detect and crop faces in the image\n    extract_faces_cv(image_file, ""test/Faces/"")\n    # resize the cropped faces and save in ""Scaled"" directory\n    resize_faces_cv(""test/Faces/"", ""test/Scaled/"", 64)\n\n# ---\n\n# ### 3. Using MTCNN\n\n# #### 3.1 Initialize the MTCNN detector\nmtcnn = MTCNN()\n\n# #### 3.2 Function to detect and crop faces\ndef extract_faces_mtcnn(image_file, cropped_images_path):\n    # Read the image\n    image = cv2.imread(image_file)\n\n    # detect faces\n    faces = mtcnn.detect_faces(image)\n\n    count = 0\n    image = Image.open(image_file)\n    \n    if faces is None:\n        print(""No face detected in the image."")\n        return\n\n    # loop over the faces\n    for face in faces:\n        bounding_box = face[\'box\']\n        # crop the face in the image\n        cropped_image = image.crop((bounding_box[0], bounding_box[1], bounding_box[0]+bounding_box[2], bounding_box[1]+bounding_box[3]))\n        image_name = (image_file.split(""/"")[-1])[:-4]\n        cropped_image.save(cropped_images_path + image_name + ""_face_"" + str(count) + "".jpg"")\n        count+=1\n    return count\n\n\n # ---\n\n# ### 4. Using Dlib\n\n# #### 4.1 Import Dlib\nimport dlib\n\n# #### 4.2 Function to detect and crop faces\n# dlib hog + svm based face detector\ndetector = dlib.get_frontal_face_detector()\n\ndef extract_faces(image_file, cropped_images_path):\n    # load input image\n    image = cv2.imread(image_file)\n    count = 0\n    # get the image height and width\n    image_height, image_width = image.shape[:2]\n    \n    if image is None:\n        print(""Could not read input image"")\n        exit()\n    \n    # apply face detection\n    faces = detector(image, 1)\n\n    # loop over detected faces\n    for face in faces:\n        # crop the image\n        cropped_image = image[max(0, face.top()): min(face.bottom(), image_height),\n                    max(0, face.left()): min(face.right(), image_width)]\n        # extract image name from filename\n        image_name = (image_file.split(""/"")[-1])[:-4]\n        # save the cropped image\n        cv2.imwrite(cropped_images_path + image_name + ""_face_"" + str(count) + "".jpg"", cropped_image)\n        count+=1\n    return count\n\n\n# #### 4.3 Function to resize the cropped faces\ndef resize_faces(image_file, cropped_images_path, scaled_images_path, size):\n    count = 1\n    # for each image in the cropped images path\n    for file in glob.glob(cropped_images_path+""*.jpg""):\n        # only scale the faces of the current image\n        if((image_file.split(""/"")[-1])[:-4] in file):\n            # read the image\n            image = cv2.imread(file)\n            # get the height and width of the image\n            height, width = image.shape[:2]\n            \n            # get the height and weight ratios\n            height_ratio, width_ratio = float(size/height), float(size/width)\n            \n            # resize the image making sure that the original ratio is maintained\n            resized = cv2.resize(image, None, fx=width_ratio, fy=height_ratio, interpolation=cv2.INTER_AREA)\n            \n            # extract image name from full file name\n            image_name = (file.split(""/"")[-1])\n            # save the scaled image\n            cv2.imwrite(scaled_images_path + image_name, resized)\n\n\n# #### 4.4 Function to align the faces\nalign_dlib = AlignDlib(\'shape_predictor_68_face_landmarks.dat\')\n\ndef align_faces(image_file, scaled_images_path, aligned_images_path):\n    count = 1\n    # for each image in the scaled images directory\n    for file in glob.glob(scaled_images_path+""*.jpg""):\n        # only align the faces of the current image\n        if((image_file.split(""/"")[-1])[:-4] in file):\n        \n            # read the image\n            image = cv2.imread(file)\n            \n            # initialize the bounding box\n            bb = align_dlib.getLargestFaceBoundingBox(image)\n            # align the face\n            aligned = align_dlib.align(64, image, bb, landmarkIndices=AlignDlib.INNER_EYES_AND_BOTTOM_LIP)\n\n            image_name = (file.split(""/"")[-1])\n            \n            # if aligned\n            if aligned is not None:\n                # save the image in the aligned images directory\n                cv2.imwrite(aligned_images_path + image_name, aligned)\n            else:\n                # save the image without alignment in the aligned images directory\n                cv2.imwrite(aligned_images_path + image_name, image)\n\n\n# #### 4.5 Apply preprocessing to the dataset using the functions above\ndef preprocess(data_dir, image_file):\n    # detect and crop faces in the image\n    faces_count = extract_faces(data_dir + image_file, data_dir + ""Faces/"")\n    if faces_count == 0:\n        extract_faces_mtcnn(data_dir + image_file, data_dir + ""Faces/"")\n    # resize the cropped faces and save in ""Scaled"" directory\n    resize_faces(data_dir + image_file, data_dir + ""Faces/"", data_dir + ""Scaled/"", 64)\n    # align the scaled faces and save in ""Aligned"" directory\n    align_faces(data_dir + image_file, data_dir + ""Scaled/"", data_dir + ""Aligned/"")'"
label_json.py,0,"b'""""""Module to evaluate full pipeline on the validation set.\n\n    python label_json.py input/val/Positive/ Positive val_labels_positive.json\n\n""""""\n\n#!/usr/bin/env python\n# coding: utf-8\n\nimport os\nimport sys\nimport glob\nimport image_preprocessing\nimport json\n\n# function to get the labels for an image and store to the json file\ndef label_image(image_path, image_name, real_label, filename):\n    # detect labels for the image\n    labels = image_preprocessing.detect_labels(image_path + image_name)\n\n    # labels = [\'people\', \'friendship\', \'fun\', \'event\', \'drinking\', \'happy\', \'picnic\', \'recreation\', \'smile\', \'leisure\']\n\n    print(""RadhaKrishna"")\n    print(""Image name: "" + image_name)\n    print(labels)\n\n    # open the file\n    with open(filename, mode=\'r+\', encoding=\'utf-8\') as f:\n        # read the first byte\n        first = f.read(1)\n        # if file is empty, dump an empty dict to the file\n        if not first:\n            json.dump({}, f)\n\n    # open the file again\n    with open(filename, mode=\'r\', encoding=\'utf-8\') as f:\n        # load the contents of the file as a dict\n        image_labels_dict = json.load(f)\n    \n    # open the file in writable mode\n    with open(filename, mode=\'w\', encoding=\'utf-8\') as f:\n        # create a dict with image name as key and its labels as value\n        image_labels_dict[image_name] = labels\n        # dump the dict to the file\n        json.dump(image_labels_dict, f)\n\n\ndef main(image_path, real_label, filename):\n    print(""RadhaKrishna"")\n    # for each image in the image path\n    for file in sorted(glob.glob(image_path + ""*.jpg"")):\n        # extract the image name\n        image_name = (file.split(\'/\'))[-1]\n        print(""Image: "" + image_name)\n        # get the labels for the image and store to the json file\n        label_image(image_path, image_name, real_label, filename)\n\nif __name__==""__main__"":\n    main(sys.argv[1], sys.argv[2], sys.argv[3])'"
