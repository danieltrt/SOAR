file_path,api_count,code
setup.py,0,"b'import setuptools\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\n\nsetuptools.setup(\n    name=""sanpy"",\n    version=""0.7.18"",\n    author=""Santiment"",\n    author_email=""admin@santiment.net"",\n    description=""Package for Santiment API access with python"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    license=""MIT"",\n    url=""https://github.com/santiment/sanpy"",\n    test_suite=\'nose.collector\',\n    tests_require=[\'nose\'],\n    packages=setuptools.find_packages(),\n    install_requires=[\n        \'pandas\',\n        \'requests\',\n        \'iso8601\',\n        \'setuptools\'\n    ],\n    extras_require={\n        ""extras"":  [""numpy"", ""matplotlib"", ""scipy"", ""mlfinlab""]\n    }\n)\n'"
san/__init__.py,0,"b""from .get import get\nfrom .metadata import metadata\nfrom .available_metrics import available_metrics\nfrom .batch import Batch\nfrom .api_config import ApiConfig\nimport pkg_resources\nimport requests\nimport json\nfrom warnings import warn\n\nPROJECT = 'sanpy'\n\n\ndef get_latest():\n    url = 'https://pypi.python.org/pypi/%s/json' % (PROJECT)\n    try:\n        response = requests.get(url).text\n        return json.loads(response)['info']['version']\n    except requests.exceptions.RequestException as e:\n        return pkg_resources.get_distribution(PROJECT).version\n\n\ndef warn_if_outdated():\n    current_version = pkg_resources.get_distribution(PROJECT).version\n    latest_version = get_latest()\n\n    if current_version != latest_version:\n        warning = 'The package %s is out of date. Your version is %s, the latest is %s.' % (\n            PROJECT, current_version, latest_version)\n        warn(warning)\n        pass\n\n\nwarn_if_outdated()\n"""
san/api_config.py,0,b'class ApiConfig:\n    api_key = None\n'
san/available_metrics.py,0,"b""import inspect\nimport san.sanbase_graphql\nfrom san.v2_metrics_list import V2_METRIC_QUERIES\n\ndef available_metrics():\n    sanbase_graphql_functions = inspect.getmembers(san.sanbase_graphql, inspect.isfunction)\n    all_functions =  list(map(lambda x: x[0], sanbase_graphql_functions)) + V2_METRIC_QUERIES\n    all_functions.remove('get_metric')\n    return all_functions\n"""
san/batch.py,0,"b'import san.sanbase_graphql\nfrom san.query import get_gql_query\nfrom san.graphql import execute_gql\nfrom san.transform import transform_query_result\nfrom san.v2_metrics_list import V2_METRIC_QUERIES\nfrom san.error import SanError\n\n\nclass Batch:\n    def __init__(self):\n        self.queries = []\n\n    def get(self, dataset, **kwargs):\n        self.queries.append([dataset, kwargs])\n\n    def execute(self):\n        graphql_string = self.__create_batched_query_string()\n        result = execute_gql(graphql_string)\n        return self.__transform_batch_result(result)\n\n    def __create_batched_query_string(self):\n        batched_queries = []\n\n        for idx, query in enumerate(self.queries):\n            [metric, _separator, slug] = query[0].partition(\'/\')\n            if metric in V2_METRIC_QUERIES:\n                if slug != \'\':\n                    batched_queries.append(\n                        san.sanbase_graphql.get_metric(\n                            idx, metric, slug, **query[1]))\n                else:\n                    raise SanError(\'Invalid metric!\')\n            else:\n                batched_queries.append(\n                    get_gql_query(\n                        idx, query[0], **query[1]))\n        self.__batch_gql_queries(batched_queries)\n        return self.__batch_gql_queries(batched_queries)\n\n    def __transform_batch_result(self, graphql_result):\n        result = []\n\n        idxs = sorted([int(k.split(\'_\')[1]) for k in graphql_result.keys()])\n        for idx in idxs:\n            query = self.queries[idx][0].split(""/"")[0]\n            df = transform_query_result(idx, query, graphql_result)\n            result.append(df)\n        return result\n\n    def __batch_gql_queries(self, batched_queries):\n        gql_string = ""{\\n""\n        for q in batched_queries:\n            gql_string = gql_string + q + ""\\n""\n\n        gql_string = gql_string + ""}""\n        return gql_string\n'"
san/env_vars.py,0,"b'import os\n\nSANBASE_GQL_HOST = os.environ.get(\n    \'SANBASE_GQL_HOST\',\n    ""https://api.santiment.net/graphql"")\n'"
san/error.py,0,b'class SanError(ValueError):\n    pass\n'
san/get.py,0,"b'import san.sanbase_graphql\nfrom san.graphql import execute_gql\nfrom san.query import get_gql_query, parse_dataset\nfrom san.transform import transform_query_result\nfrom san.v2_metrics_list import V2_METRIC_QUERIES\nfrom san.error import SanError\n\nCUSTOM_QUERIES = {\n    \'ohlcv\': \'get_ohlcv\'\n}\n\nDEPRECATED_QUERIES = {\n    \'mvrv_ratio\': \'mvrv_usd\',\n    \'nvt_ratio\': \'nvt\',\n    \'realized_value\': \'realized_value_usd\',\n    \'token_circulation\': \'circulation_1d\',\n    \'burn_rate\': \'age_destroyed\',\n    \'token_age_consumed\': \'age_destroyed\',\n    \'token_velocity\': \'velocity\'\n}\n\n\ndef get(dataset, **kwargs):\n    query, slug = parse_dataset(dataset)\n    if query in DEPRECATED_QUERIES:\n        print(\n            \'**NOTICE**\\n{} will be deprecated in version 0.9.0, please use {} instead\'.format(\n                query, DEPRECATED_QUERIES[query]))\n    if query in CUSTOM_QUERIES:\n        return getattr(san.sanbase_graphql, query)(0, slug, **kwargs)\n    if query in V2_METRIC_QUERIES:\n        if slug != \'\':\n            gql_query = ""{"" + \\\n                san.sanbase_graphql.get_metric(0, query, slug, **kwargs) + ""}""\n        else:\n            raise SanError(\'Invalid metric!\')\n    else:\n        gql_query = ""{"" + get_gql_query(0, dataset, **kwargs) + ""}""\n    res = execute_gql(gql_query)\n\n    return transform_query_result(0, query, res)\n'"
san/graphql.py,0,"b'import requests\nfrom san.api_config import ApiConfig\nfrom san.env_vars import SANBASE_GQL_HOST\nfrom san.error import SanError\n\n\ndef execute_gql(gql_query_str):\n    headers = {}\n    if ApiConfig.api_key:\n        headers = {\'authorization\': ""Apikey {}"".format(ApiConfig.api_key)}\n\n    response = requests.post(\n        SANBASE_GQL_HOST,\n        json={\'query\': gql_query_str},\n        headers=headers)\n\n    if response.status_code == 200:\n        return __handle_success_response__(response, gql_query_str)\n    else:\n        raise SanError(\n            ""Error running query. Status code: {}.\\n {}"".format(\n                response.status_code,\n                gql_query_str))\n\n\ndef __handle_success_response__(response, gql_query_str):\n    if __result_has_gql_errors__(response):\n        raise SanError(\n            ""GraphQL error occured running query {} \\n errors: {}"".format(\n                gql_query_str,\n                response.json()[\'errors\']))\n    elif __exist_not_empty_result(response):\n        return response.json()[\'data\']\n    else:\n        raise SanError(\n            ""Error running query. Status code: {}.\\n {}"" .format(\n                response.status_code,\n                gql_query_str))\n\n\ndef __result_has_gql_errors__(response):\n    return \'errors\' in response.json().keys()\n\n\ndef __exist_not_empty_result(response):\n    return \'data\' in response.json().keys() and len(\n        list(\n            filter(\n                lambda x: x is not None,\n                response.json()[\'data\'].values()))) > 0\n'"
san/metadata.py,0,"b'from .graphql import execute_gql\n\ndef metadata(metric, arr):\n    query_str = (""""""{{\n    getMetric (metric: \\""{metric}\\"") {{\n        metadata {{\n            """""" + \' \'.join(arr) + """"""\n        }}\n    }}\n    }}\n    """""").format(\n        metric=metric\n    )\n\n    return execute_gql(query_str)[\'getMetric\'][\'metadata\']\n'"
san/pandas_utils.py,0,"b""import pandas as pd\n\n\ndef convert_to_datetime_idx_df(data):\n    df = pd.DataFrame(data)\n\n    if 'datetime' in df.columns:\n        df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n        df.set_index('datetime', inplace=True)\n\n    return df\n\n\ndef merge(df1, df2):\n    return pd.concat([df1, df2], axis=1)\n"""
san/query.py,0,"b'import san.sanbase_graphql\nfrom san.error import SanError\n\n\ndef get_gql_query(idx, identifier, **kwargs):\n    query, separator, slug = identifier.partition(""/"")\n\n    if slug == \'\' and separator != \'\':\n        raise SanError(\'Invalid metric!\')\n    elif slug == \'\':\n        return getattr(\n            san.sanbase_graphql,\n            query,\n            lambda *args, **kwargs: not_found(query)\n        )(idx, **kwargs)\n    else:\n        return getattr(\n            san.sanbase_graphql,\n            query,\n            lambda *args, **kwargs: not_found(query)\n        )(idx, slug, **kwargs)\n\n\ndef parse_dataset(dataset):\n    left, _separator, right = dataset.partition(""/"")\n    return [left, right]\n\n\ndef not_found(query):\n    raise SanError(query + \' not found\')\n'"
san/sanbase_graphql.py,0,"b'import san.pandas_utils\nimport san.sanbase_graphql_helper as sgh\nfrom san.batch import Batch\nfrom san.error import SanError\n\n\n# to be removed\n\n\ndef burn_rate(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'burn_rate\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef token_age_consumed(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'token_age_consumed\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef average_token_age_consumed_in_days(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\n        \'average_token_age_consumed_in_days\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef github_activity(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'github_activity\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef dev_activity(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'dev_activity\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef network_growth(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'network_growth\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef prices(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'prices\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef token_velocity(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'token_velocity\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef token_circulation(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'token_circulation\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef realized_value(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'realized_value\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef mvrv_ratio(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'mvrv_ratio\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef nvt_ratio(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'nvt_ratio\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef daily_active_deposits(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'daily_active_deposits\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef ohlc(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'ohlc\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef gas_used(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'gas_used\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef miners_balance(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'miners_balance\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef mining_pools_distribution(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\n        \'mining_pools_distribution\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef historical_balance(idx, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'historical_balance\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: historicalBalance (\n        address: \\""{address}\\"",\n        slug: \\""{slug}\\"",\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\"",\n        interval: \\""{interval}\\""\n    ){{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}\').format(\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef social_dominance(idx, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'social_dominance\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: socialDominance (\n        slug: \\""{slug}\\"",\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\"",\n        interval: \\""{interval}\\"",\n        source: {source}\n    ){{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}\').format(\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef top_holders_percent_of_total_supply(idx, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'top_holders_percent_of_total_supply\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: topHoldersPercentOfTotalSupply(\n        slug: \\""{slug}\\"",\n        numberOfHolders: {number_of_holders},\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\""\n    ){{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}\').format(\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef history_twitter_data(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'history_twitter_data\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef price_volume_difference(idx, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'price_volume_difference\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: priceVolumeDiff (\n        slug: \\""{slug}\\"",\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\"",\n        interval: \\""{interval}\\"",\n        currency: \\""{currency}\\""\n    ){{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}\').format(\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef eth_top_transactions(idx, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'eth_top_transactions\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: projectBySlug (slug: \\""{slug}\\""){{\n            ethTopTransactions (\n                from: \\""{from_date}\\"",\n                to: \\""{to_date}\\"",\n                limit: {limit},\n                transactionType: {transaction_type}\n            ){{\n            """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}}}\').format(\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef news(idx, tag, **kwargs):\n    print(\'WARNING! This metric is going to be removed in version 0.8.0\')\n    kwargs = sgh.transform_query_args(\'news\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: news(\n        tag: \\""{tag}\\"",\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\"",\n        size: {size}\n    ){{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}\').format(\n        idx=idx,\n        tag=tag,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef eth_spent_over_time(idx, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'eth_spent_over_time\', **kwargs)\n\n    query_str = """"""\n    query_{idx}: projectBySlug (slug: \\""{slug}\\""){{\n            ethSpentOverTime(\n                from: \\""{from_date}\\"",\n                to: \\""{to_date}\\"",\n                interval: \\""{interval}\\""\n            ){{\n        datetime,\n        ethSpent\n        }}\n    }}\n    """""".format(\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef token_top_transactions(idx, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'token_top_transactions\', **kwargs)\n\n    query_str = """"""\n    query_{idx}: projectBySlug (slug: \\""{slug}\\""){{\n            tokenTopTransactions (\n                from: \\""{from_date}\\"",\n                to: \\""{to_date}\\"",\n                limit: {limit}\n            ){{\n        datetime,\n        fromAddress{{\n            address,\n            isExchange\n        }},\n        toAddress{{\n            address,\n            isExchange\n        }},\n        trxHash,\n        trxValue\n        }}\n    }}\n    """""".format(\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef emerging_trends(idx, **kwargs):\n    kwargs = sgh.transform_query_args(\'emerging_trends\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: getTrendingWords (\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\"",\n        size: {size},\n        interval: \\""{interval}\\""\n    ){{"""""" + \' \'.join(kwargs[\'return_fields\']) + """"""\n    }}\n    """""").format(\n        idx=idx,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef top_social_gainers_losers(idx, **kwargs):\n    kwargs = sgh.transform_query_args(\'top_social_gainers_losers\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: topSocialGainersLosers(\n                    from: \\""{from_date}\\"",\n                    to: \\""{to_date}\\"",\n                    status: {status},\n                    size: {size},\n                    timeWindow: \\""{time_window}\\""\n                ){{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + """"""\n    }}\n    """""").format(\n        idx=idx,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef ohlcv(idx, slug, **kwargs):\n    return_fields = [\n        \'openPriceUsd\',\n        \'closePriceUsd\',\n        \'highPriceUsd\',\n        \'lowPriceUsd\',\n        \'volume\',\n        \'marketcap\']\n\n    batch = Batch()\n    batch.get(\n        ""prices/{slug}"".format(slug=slug),\n        **kwargs\n    )\n    batch.get(\n        ""ohlc/{slug}"".format(slug=slug),\n        **kwargs\n    )\n    [price_df, ohlc_df] = batch.execute()\n    merged = san.pandas_utils.merge(price_df, ohlc_df)\n    return merged[return_fields]\n\n\ndef get_metric(idx, metric, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'get_metric\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: getMetric(metric: \\""{metric}\\""){{\n        timeseriesData(\n            slug: \\""{slug}\\""\n            from: \\""{from_date}\\""\n            to: \\""{to_date}\\""\n            interval: \\""{interval}\\"",\n            aggregation: {aggregation}\n        ){{\n        """""" + \' \'.join(kwargs[\'return_fields\']) + """"""\n        }}\n    }}\n    """""").format(\n        idx=idx,\n        metric=metric,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef projects(idx, slug, **kwargs):\n    if (slug == ""erc20""):\n        return sgh.erc20_projects(idx, **kwargs)\n    elif (slug == ""all""):\n        return sgh.all_projects(idx, **kwargs)\n\n    raise SanError(""Unknown project group: {}"".format(slug))\n\n\ndef exchange_funds_flow(idx, slug, **kwargs):\n    query_str = sgh.create_query_str(\'exchange_funds_flow\', idx, slug, **kwargs)\n\n    return query_str\n\n\ndef social_volume_projects(idx, **kwargs):\n    query_str = """"""\n    query_{idx}: socialVolumeProjects\n    """""".format(idx=idx)\n\n    return query_str\n\n\ndef social_volume(idx, slug, **kwargs):\n    kwargs = sgh.transform_query_args(\'social_volume\', **kwargs)\n\n    query_str = (""""""\n    query_{idx}: socialVolume (\n        slug: \\""{slug}\\"",\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\"",\n        interval: \\""{interval}\\"",\n        socialVolumeType: {social_volume_type}\n    ){{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}\').format(\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef topic_search(idx, **kwargs):\n    kwargs = sgh.transform_query_args(\'topic_search\', **kwargs)\n    query_str = (""""""\n    query_{idx}: topicSearch (\n        source: {source},\n        searchText: \\""{search_text}\\"",\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\"",\n        interval: \\""{interval}\\""\n    ){{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + """"""\n    }}\n    """""").format(\n        idx=idx,\n        **kwargs\n    )\n\n    return query_str\n'"
san/sanbase_graphql_helper.py,0,"b'import iso8601\nimport datetime\n\n_DEFAULT_INTERVAL = \'1d\'\n_DEFAULT_SOCIAL_VOLUME_TYPE = \'PROFESSIONAL_TRADERS_CHAT_OVERVIEW\'\n_DEFAULT_SOURCE = \'TELEGRAM\'\n_DEFAULT_SEARCH_TEXT = \'\'\n\n_QUERY_MAPPING = {\n    \'daily_active_addresses\': {\n        \'query\': \'dailyActiveAddresses\',\n        \'return_fields\': [\'datetime\', \'activeAddresses\']\n    },\n    \'burn_rate\': {  # to be removed\n        \'query\': \'burnRate\',\n        \'return_fields\': [\'datetime\', \'burnRate\']\n    },\n    \'token_age_consumed\': {\n        \'query\': \'tokenAgeConsumed\',\n        \'return_fields\': [\'datetime\', \'tokenAgeConsumed\']\n    },\n    \'average_token_age_consumed_in_days\': {\n        \'query\': \'averageTokenAgeConsumedInDays\',\n        \'return_fields\': [\'datetime\', \'tokenAge\']\n    },\n    \'transaction_volume\': {\n        \'query\': \'transactionVolume\',\n        \'return_fields\': [\'datetime\', \'transactionVolume\']\n    },\n    \'github_activity\': {\n        \'query\': \'githubActivity\',\n        \'return_fields\': [\'datetime\', \'activity\']\n    },\n    \'dev_activity\': {\n        \'query\': \'devActivity\',\n        \'return_fields\': [\'datetime\', \'activity\']\n    },\n    \'network_growth\': {\n        \'query\': \'networkGrowth\',\n        \'return_fields\': [\'datetime\', \'newAddresses\']\n    },\n    \'prices\': {\n        \'query\': \'historyPrice\',\n        \'return_fields\': [\'datetime\', \'priceUsd\', \'priceBtc\', \'marketcap\', \'volume\']\n    },\n    \'ohlc\': {\n        \'query\': \'ohlc\',\n        \'return_fields\': [\'datetime\', \'openPriceUsd\', \'closePriceUsd\', \'highPriceUsd\', \'lowPriceUsd\']\n    },\n    \'exchange_funds_flow\': {\n        \'query\': \'exchangeFundsFlow\',\n        \'return_fields\': [\'datetime\', \'inOutDifference\']\n    },\n    \'token_velocity\': {\n        \'query\': \'tokenVelocity\',\n        \'return_fields\': [\'datetime\', \'tokenVelocity\']\n    },\n    \'token_circulation\': {\n        \'query\': \'tokenCirculation\',\n        \'return_fields\': [\'datetime\', \'tokenCirculation\']\n    },\n    \'realized_value\': {\n        \'query\': \'realizedValue\',\n        \'return_fields\': [\'datetime\', \'realizedValue\']\n    },\n    \'mvrv_ratio\': {\n        \'query\': \'mvrvRatio\',\n        \'return_fields\': [\'datetime\', \'ratio\']\n    },\n    \'nvt_ratio\': {\n        \'query\': \'nvtRatio\',\n        \'return_fields\': [\'datetime\', \'nvtRatioCirculation\', \'nvtRatioTxVolume\']\n    },\n    \'daily_active_deposits\': {\n        \'query\': \'dailyActiveDeposits\',\n        \'return_fields\': [\'datetime\', \'activeDeposits\']\n    },\n    \'gas_used\': {\n        \'query\': \'gasUsed\',\n        \'return_fields\': [\'datetime\', \'gasUsed\']\n    },\n    \'miners_balance\': {\n        \'query\': \'minersBalance\',\n        \'return_fields\': [\'balance\', \'datetime\']\n    },\n    \'mining_pools_distribution\': {\n        \'query\': \'miningPoolsDistribution\',\n        \'return_fields\': [\'datetime\', \'other\', \'top10\', \'top3\']\n    },\n    \'history_twitter_data\': {\n        \'query\': \'historyTwitterData\',\n        \'return_fields\': [\'datetime\', \'followers_count\']\n    },\n    \'historical_balance\': {\n        \'query\': \'historicalBalance\',\n        \'return_fields\': [\'datetime\', \'balance\']\n    },\n    \'social_dominance\': {\n        \'query\': \'socialDominance\',\n        \'return_fields\': [\'datetime\', \'dominance\']\n    },\n    \'top_holders_percent_of_total_supply\': {\n        \'query\': \'topHoldersPercentOfTotalSupply\',\n        \'return_fields\': [\'datetime\', \'inExchanges\', \'outsideExchanges\', \'inTopHoldersTotal\']\n    },\n    \'projects\': {\n        \'query\': \'allProjects\',\n        \'return_fields\': [\'name\', \'slug\', \'ticker\', \'totalSupply\', \'marketSegment\']\n    },\n    \'get_metric\': {\n        \'query\': \'getMetric\',\n        \'return_fields\': [\n            \'datetime\',\n            \'value\'\n        ]\n    },\n    \'topic_search\': {\n        \'query\': \'topicSearch\',\n        \'return_fields\': [\n            (\'chartData\', [\'datetime, \'\'mentionsCount\'])\n        ]\n    },\n    \'eth_top_transactions\': {\n        \'query\': \'ethTopTransactions\',\n        \'return_fields\': [\n            \'datetime\',\n            (\'fromAddress\', [\'address\', \'isExchange\']),\n            (\'toAddress\', [\'address\', \'isExchange\']),\n            \'trxHash\',\n            \'trxValue\'\n        ]\n    },\n    \'token_top_transactions\': {\n        \'query\': \'tokenTopTransactions\',\n        \'return_fields\': [\n            \'datetime\',\n            (\'fromAddress\', [\'address\', \'isExchange\']),\n            (\'toAddress\', [\'address\', \'isExchange\']),\n            \'trxHash\',\n            \'trxValue\'\n        ]\n    },\n    \'eth_spent_over_time\': {\n        \'query\': \'ethSpentOverTime\',\n        \'return_fields\': [\n            \'datetime\',\n            \'ethSpent\'\n        ]\n    },\n    \'news\': {\n        \'query\': \'news\',\n        \'return_fields\': [\n            \'datetime\',\n            \'title\',\n            \'sourceName\',\n            \'url\',\n            \'description\'\n        ]\n    },\n    \'price_volume_difference\': {\n        \'query\': \'priceVolumeDiff\',\n        \'return_fields\': [\n            \'datetime\',\n            \'priceChange\',\n            \'priceVolumeDiff\',\n            \'volumeChange\'\n        ]\n    },\n    \'social_volume\': {\n        \'query\': \'socialVolume\',\n        \'return_fields\': [\n            \'datetime\',\n            \'mentionsCount\'\n        ]\n    },\n    \'top_social_gainers_losers\': {\n        \'query\': \'topSocialGainersLosers\',\n        \'return_fields\': [\n            \'datetime\',\n            (\'projects\', [\'change\', \'slug\', \'status\'])\n        ]\n    },\n    \'emerging_trends\': {\n        \'query\': \'getTrendingWords\',\n        \'return_fields\': [\n            \'datetime\',\n            (\'topWords\', [\'score\', \'word\'])\n        ]\n    }\n}\n\n\ndef all_projects(idx, **kwargs):\n    kwargs = transform_query_args(\'projects\', **kwargs)\n    query_str = (""""""\n    query_{idx}: allProjects\n    {{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}\').format(idx=idx)\n\n    return query_str\n\n\ndef erc20_projects(idx, **kwargs):\n    kwargs = transform_query_args(\'projects\', **kwargs)\n    query_str = (""""""\n    query_{idx}: allErc20Projects\n    {{\n    """""" + \' \'.join(kwargs[\'return_fields\']) + \'}}\').format(idx=idx)\n\n    return query_str\n\n\ndef create_query_str(query, idx, slug, **kwargs):\n    kwargs = transform_query_args(query, **kwargs)\n\n    query_str = (""""""\n    query_{idx}: {query}(\n        slug: \\""{slug}\\"",\n        from: \\""{from_date}\\"",\n        to: \\""{to_date}\\"",\n        interval: \\""{interval}\\""\n    ){{\n    """""" +  \' \'.join(kwargs[\'return_fields\']) + \'}}\'\n    ).format(\n        query=_QUERY_MAPPING[query][\'query\'],\n        idx=idx,\n        slug=slug,\n        **kwargs\n    )\n\n    return query_str\n\n\ndef transform_query_args(query, **kwargs):\n    kwargs[\'from_date\'] = kwargs[\'from_date\'] if \'from_date\' in kwargs else _default_from_date()\n    kwargs[\'to_date\'] = kwargs[\'to_date\'] if \'to_date\' in kwargs else _default_to_date()\n    kwargs[\'interval\'] = kwargs[\'interval\'] if \'interval\' in kwargs else _DEFAULT_INTERVAL\n    kwargs[\'social_volume_type\'] = kwargs[\'social_volume_type\'] if \'social_volume_type\' in kwargs else _DEFAULT_SOCIAL_VOLUME_TYPE\n    kwargs[\'source\'] = kwargs[\'source\'] if \'source\' in kwargs else _DEFAULT_SOURCE\n    kwargs[\'search_text\'] = kwargs[\'search_text\'] if \'search_text\' in kwargs else _DEFAULT_SEARCH_TEXT\n    kwargs[\'aggregation\'] = kwargs[\'aggregation\'] if \'aggregation\' in kwargs else ""null""\n\n    kwargs[\'from_date\'] = _format_from_date(kwargs[\'from_date\'])\n    kwargs[\'to_date\'] = _format_to_date(kwargs[\'to_date\'])\n\n    if \'return_fields\' in kwargs:\n        kwargs[\'return_fields\'] = _format_all_return_fields(kwargs[\'return_fields\'])\n    else:\n        kwargs[\'return_fields\'] = _format_all_return_fields(_QUERY_MAPPING[query][\'return_fields\'])\n\n    return kwargs\n\n\ndef _default_to_date():\n    return datetime.datetime.now()\n\n\ndef _default_from_date():\n    return datetime.datetime.now() - datetime.timedelta(days=365)\n\n\ndef _format_from_date(datetime_obj_or_str):\n    if isinstance(datetime_obj_or_str, datetime.datetime):\n        datetime_obj_or_str = datetime_obj_or_str.isoformat()\n\n    return iso8601.parse_date(datetime_obj_or_str).isoformat()\n\n\ndef _format_to_date(datetime_obj_or_str):\n    if isinstance(datetime_obj_or_str, datetime.datetime):\n      return iso8601.parse_date(datetime_obj_or_str.isoformat())\n    \n    try:\n        # Throw if the string is not date-formated, parse as date otherwise\n        datetime.datetime.strptime(datetime_obj_or_str, \'%Y-%m-%d\')\n        dt = iso8601.parse_date(datetime_obj_or_str) + \\\n            datetime.timedelta(hours=23, minutes=59, seconds=59)\n    except:\n        dt = iso8601.parse_date(datetime_obj_or_str)\n    \n    return dt.isoformat()\n\ndef _format_all_return_fields(fields):\n    while any(isinstance(x, tuple) for x in fields):\n        fields = _format_return_fields(fields)\n    return fields\n\ndef _format_return_fields(fields):\n    return list(map(\n        lambda el: el[0] + \'{{\' + \' \'.join(el[1]) + \'}}\' if isinstance(el, tuple) else el\n    , fields))\n'"
san/transform.py,0,"b'""""""\nIn order to have metrics, which require different order, we need to have transform\n functions, which reorder or make different dictionaries in general.\n""""""\nimport operator\nimport pandas as pd\nfrom san.pandas_utils import convert_to_datetime_idx_df\nfrom functools import reduce\nfrom collections import OrderedDict\nfrom san.graphql import execute_gql\nfrom san.v2_metrics_list import V2_METRIC_QUERIES\n\nQUERY_PATH_MAP = {\n    \'eth_top_transactions\': [\'ethTopTransactions\'],\n    \'eth_spent_over_time\': [\'ethSpentOverTime\'],\n    \'token_top_transactions\': [\'tokenTopTransactions\'],\n    \'get_metric\': [\'timeseriesData\'],\n    \'topic_search\': [\'chartData\']\n}\n\ndef path_to_data(idx, query, data):\n    """"""\n    With this function we jump straight onto the key from the dataframe, that we want and start from there. We use our future starting points from the QUERY_PATH_MAP.\n    """"""\n    return reduce(\n        operator.getitem, [\n            \'query_\' + str(idx), ] + QUERY_PATH_MAP[query], data)\n\n\ndef transform_query_result(idx, query, data):\n    """"""\n    If there is a transforming function for this query, then the result is\n    passed for it for another transformation\n    """"""\n    if query in QUERY_PATH_MAP:\n        result = path_to_data(idx, query, data)\n    elif query in V2_METRIC_QUERIES:\n        result = path_to_data(idx, \'get_metric\', data)\n    else:\n        result = data[\'query_\' + str(idx)]\n\n    if query + \'_transform\' in globals():\n        result = globals()[query + \'_transform\'](result)\n\n    return convert_to_datetime_idx_df(result)\n\n\ndef eth_top_transactions_transform(data):\n    return list(map(lambda column: {\n        \'datetime\': column[\'datetime\'],\n        \'fromAddress\': column[\'fromAddress\'][\'address\'],\n        \'fromAddressIsExchange\': column[\'fromAddress\'][\'isExchange\'],\n        \'toAddress\': column[\'toAddress\'][\'address\'],\n        \'toAddressIsExchange\': column[\'toAddress\'][\'isExchange\'],\n        \'trxHash\': column[\'trxHash\'],\n        \'trxValue\': column[\'trxValue\']\n    }, data))\n\n\ndef news_transform(data):\n    result = list(map(lambda column: OrderedDict({\n        \'datetime\': column[\'datetime\'],\n        \'title\': column[\'title\'],\n        \'description\': column[\'description\'],\n        \'sourceName\': column[\'sourceName\'],\n        \'url\': column[\'url\']\n    }), data))\n\n    return result\n\n\ndef token_top_transactions_transform(data):\n    return list(map(lambda column: {\n        \'datetime\': column[\'datetime\'],\n        \'fromAddress\': column[\'fromAddress\'][\'address\'],\n        \'fromAddressIsExchange\': column[\'fromAddress\'][\'isExchange\'],\n        \'toAddress\': column[\'toAddress\'][\'address\'],\n        \'toAddressIsExchange\': column[\'toAddress\'][\'isExchange\'],\n        \'trxHash\': column[\'trxHash\'],\n        \'trxValue\': column[\'trxValue\']\n    }, data))\n\n\ndef emerging_trends_transform(data):\n    result = []\n    for column in data:\n        for i in range(0, len(column[\'topWords\'])):\n            result.append({\n                \'datetime\': column[\'datetime\'],\n                \'score\': column[\'topWords\'][i][\'score\'],\n                \'word\': column[\'topWords\'][i][\'word\']\n            })\n    result.sort(key=lambda elem: elem[\'datetime\'])\n\n    return result\n\n\ndef top_social_gainers_losers_transform(data):\n    result = []\n    for column in data:\n        for i in range(0, len(column[\'projects\'])):\n            result.append({\n                \'datetime\': column[\'datetime\'],\n                \'slug\': column[\'projects\'][i][\'slug\'],\n                \'change\': column[\'projects\'][i][\'change\'],\n                \'status\': column[\'projects\'][i][\'status\'],\n            })\n    \n    result = list(map(lambda column: OrderedDict({\n        \'datetime\': column[\'datetime\'],\n        \'slug\': column[\'slug\'],\n        \'change\': column[\'change\'],\n        \'status\': column[\'status\']\n    }), result))\n\n    return result\n'"
san/v2_metrics_list.py,0,"b'V2_METRIC_QUERIES = [\n      ""daily_avg_marketcap_usd"",\n      ""daily_avg_price_usd"",\n      ""daily_closing_marketcap_usd"",\n      ""daily_closing_price_usd"",\n      ""daily_high_price_usd"",\n      ""daily_low_price_usd"",\n      ""daily_opening_price_usd"",\n      ""daily_trading_volume_usd"",\n      ""daily_active_addresses"",\n      ""mean_realized_price_usd"",\n      ""mean_realized_price_usd_10y"",\n      ""mean_realized_price_usd_5y"",\n      ""mean_realized_price_usd_3y"",\n      ""mean_realized_price_usd_2y"",\n      ""mean_realized_price_usd_365d"",\n      ""mean_realized_price_usd_180d"",\n      ""mean_realized_price_usd_90d"",\n      ""mean_realized_price_usd_60d"",\n      ""mean_realized_price_usd_30d"",\n      ""mean_realized_price_usd_7d"",\n      ""mean_realized_price_usd_1d"",\n      ""mvrv_usd_long_short_diff"",\n      ""mvrv_usd"",\n      ""mvrv_usd_10y"",\n      ""mvrv_usd_5y"",\n      ""mvrv_usd_3y"",\n      ""mvrv_usd_2y"",\n      ""mvrv_usd_365d"",\n      ""mvrv_usd_180d"",\n      ""mvrv_usd_90d"",\n      ""mvrv_usd_60d"",\n      ""mvrv_usd_30d"",\n      ""mvrv_usd_7d"",\n      ""mvrv_usd_1d"",\n      ""circulation"",\n      ""circulation_10y"",\n      ""circulation_5y"",\n      ""circulation_3y"",\n      ""circulation_2y"",\n      ""circulation_365d"",\n      ""circulation_180d"",\n      ""circulation_90d"",\n      ""circulation_60d"",\n      ""circulation_30d"",\n      ""circulation_7d"",\n      ""circulation_1d"",\n      ""mean_age"",\n      ""realized_value_usd"",\n      ""realized_value_usd_10y"",\n      ""realized_value_usd_5y"",\n      ""realized_value_usd_3y"",\n      ""realized_value_usd_2y"",\n      ""realized_value_usd_365d"",\n      ""realized_value_usd_180d"",\n      ""realized_value_usd_90d"",\n      ""realized_value_usd_60d"",\n      ""realized_value_usd_30d"",\n      ""realized_value_usd_7d"",\n      ""realized_value_usd_1d"",\n      ""velocity"",\n      ""transaction_volume"",\n      ""exchange_inflow"",\n      ""exchange_outflow"",\n      ""exchange_balance"",\n      ""age_destroyed"",\n      ""nvt"",\n      ""nvt_transaction_volume"",\n      ""network_growth"",\n      ""mean_dollar_invested_age"",\n      ""active_deposits"",\n      ""active_withdrawals"",\n      ""withdrawal_transactions"",\n      ""supply_on_exchanges"",\n      ""percent_of_total_supply_on_exchanges"",\n      ""amount_in_top_holders""\n]\n'"
san/extras/__init__.py,0,b''
san/extras/backtest.py,7,"b'import datetime\nimport pandas as pd\nimport numpy as np\nimport san\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\n\nclass Backtest:\n\n    def __init__(self, returns: pd.Series, trades: pd.Series, lagged=True, transaction_cost=0, percent_invested_per_trade=1):\n        """""" Initializing Backtesting function\n            Init function generates performance of the test and several risk metrics. The object lets\n            you specify wether you want to lag the trades to avoid overfitting, the transaction costs\n            and the percentage of the portfolio to be invested per trade (50% as 0.5).\n            Trade example:\n                With given prices [P1, P2, P3] and given trades [False, True, False]\n                you buy asset when the price is P2 and sell when the price is P3.\n        """"""\n\n        if lagged:\n            trades = trades.shift(1)\n            trades.iloc[0] = False\n        self.strategy_returns = ((returns * percent_invested_per_trade) * trades)\n        self.trades = trades\n\n        self.nr_trades = {\'buy\': [], \'sell\': []}\n        for i in range(1, len(trades)):\n            if trades[i] != trades[i - 1]:\n                self.strategy_returns.iloc[i] -= transaction_cost\n                if trades[i]:\n                    self.nr_trades[\'buy\'].append(self.trades.index[i])\n                else:\n                    self.nr_trades[\'sell\'].append(self.trades.index[i])\n        if trades[-1]:  # include last day sell to make benchmark possible\n            self.nr_trades[\'sell\'].append(self.trades.index[i])\n\n        self.performance = (self.strategy_returns + 1).cumprod() - 1\n        self.benchmark = (returns + 1).cumprod() - 1\n\n    def get_sharpe_ratio(self, decimals=2):\n        sharpe_ratio = (self.strategy_returns.mean() * 365) / (self.strategy_returns.std() * np.sqrt(365))\n        return round(sharpe_ratio, decimals)\n\n    def get_value_at_risk(self, percentile=5):\n        sorted_rets = sorted(self.strategy_returns)\n        var = np.percentile(sorted_rets, percentile)\n        return round(var * 100, 2)\n\n    def get_nr_trades(self):\n        return len(self.nr_trades[\'sell\']) + len(self.nr_trades[\'buy\'])\n\n    def get_maximum_drawdown(self, decimals=2):\n        running_value = np.array(self.performance + 1)\n        running_value[0] = 0\n        end = np.argmax(np.maximum.accumulate(running_value) - running_value)  # end of the dropdown period\n        start = np.argmax(running_value[:end])  # start of the dropdown period\n        maximum_drawdown = (running_value[end] - running_value[start]) / running_value[start]\n        return round(maximum_drawdown * 100, decimals)\n\n    def get_return(self, decimals=2):\n        return round(((self.performance.iloc[-1] + 1) / (self.performance.iloc[1] + 1) - 1) * 100, decimals)\n\n    def get_return_benchmark(self, decimals=2):\n        return round(((self.benchmark.iloc[-1] + 1) / (self.benchmark.iloc[0] + 1) - 1) * 100, decimals)\n\n    def get_annualized_return(self, decimals=2):\n        return round((((self.performance.iloc[-1] + 1) ** (1 / len(self.performance))) - 1) * 365 * 100, decimals)\n\n    def summary(self):\n        print(""Returns in Percent: "", self.get_return())\n        print(""Returns Benchmark in Percent: "", self.get_return_benchmark())\n        print(""Annualized Returns in Percent: "", self.get_annualized_return())\n        print(""Annualized Sharpe Raito: "", self.get_sharpe_ratio())\n        print(""Number of Trades: "", self.get_nr_trades())\n\n    def plot_backtest(self, viz=None):\n        \'\'\' param viz: None OR ""trades"" OR ""hodl"".\n        \'\'\'\n        plt.figure(figsize=(15, 8))\n        plt.plot(self.performance, label=""performance"")\n        plt.plot(self.benchmark, label=""holding"")\n\n        if viz == \'trades\':\n            min_y = min(self.performance.min(), self.benchmark.min())\n            max_y = max(self.performance.max(), self.benchmark.max())\n            plt.vlines(self.nr_trades[\'sell\'], min_y, max_y, color=\'red\')\n            plt.vlines(self.nr_trades[\'buy\'], min_y, max_y, color=\'green\')\n        elif viz == \'hodl\':\n            hodl_periods = []\n            for i in range(len(self.trades)):\n                state = self.trades[i - 1] if i > 0 else self.trades[i]\n                if self.trades[i] and not state:\n                    start = self.strategy_returns.index[i]\n                elif not self.trades[i] and state:\n                    hodl_periods.append([start, self.strategy_returns.index[i]])\n            if self.trades[-1]:\n                hodl_periods.append([start, self.strategy_returns.index[i]])\n            for hodl_period in hodl_periods:\n                plt.axvspan(hodl_period[0], hodl_period[1], color=\'#aeffa8\')\n\n        plt.legend()\n        plt.show()\n\n\nclass Portfolio:\n\n    def __init__(self, start_date=""2017-01-01"", end_date=datetime.datetime.now().strftime(""%Y-%m-%d""), asset_list=[]):\n        """""" Takes in list of project slugs""""""\n\n        self.start_date = start_date\n        self.end_date = end_date\n        self.asset_list = asset_list\n        self.portfolio = pd.DataFrame()\n        self.benchmark = san.get(""ohlcv/bitcoin"", from_date=start_date,\n                                 to_date=end_date).closePriceUsd.pct_change()\n\n        for portfolio_asset in asset_list:\n            self.portfolio[portfolio_asset] = san.get(""ohlcv/"" + portfolio_asset,\n                                                      from_date=start_date,\n                                                      to_date=end_date).closePriceUsd.pct_change()\n            self.portfolio = self.portfolio.replace([np.inf, -np.inf], 0)\n            self.metrics = dict()\n\n    def add_project(self, project):\n        self.asset_list.append(project)\n        self.portfolio[project] = san.get(""ohlcv/"" + project, from_date=self.start_date,\n                                          to_date=self.end_date).closePriceUsd.pct_change()\n        self.portfolio = self.portfolio.replace([np.inf, -np.inf], 0)\n\n    def remove_project(self, project):\n        self.portfolio = self.portfolio.drop([project], axis=1)\n        self.asset_list.remove(project)\n\n    def all_assets(self):\n        print(self.asset_list)\n        return self.asset_list\n\n    def metrics(self, metric):\n        metric_data = pd.DataFrame()\n        for asset in self.asset_list:\n            metric_data[asset] = san.get(metric + ""/"" + asset,\n                                         from_date=self.start_date, to_date=self.end_date).iloc[:, 0]\n\n        self.metrics[metric] = metric_data\n        return metric_data\n\n    def show_portfolio(self):\n        return self.portfolio\n'"
san/extras/event_study.py,11,"b'import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as pyplot\nfrom datetime import timedelta\nfrom scipy import stats\n\n\n""""""\nEvent study to  evaluate events or signals.\nThe main parameters the event study function accepts are a pandas dataframe containing the price data\nof the observed projects and the benchmark (data) and dataframe containing the events(ev_data)\nthat contains the data of occurance in the index and the name of the project for every date.\n""""""\n\nFIGURE_WIDTH = 20\nFIGURE_HEIGHT = 7\nFIGURE_SIZE = [FIGURE_WIDTH, FIGURE_HEIGHT]\n\ndef get_close_price(data, sid, current_date, day_number):\n    # If we\'re looking at day 0 just return the indexed date\n    if day_number == 0:\n        return data.loc[current_date][sid]\n    # Find the close price day_number away from the current_date\n    else:\n        # If the close price is too far ahead, just get the last available\n        total_date_index_length = len(data.index)\n        # Find the closest date to the target date\n        date_index = data.index.searchsorted(current_date + timedelta(day_number))\n        # If the closest date is too far ahead, reset to the latest date possible\n        date_index = total_date_index_length - 1 if date_index >= total_date_index_length else date_index\n        # Use the index to return a close price that matches\n        return data.iloc[date_index][sid]\n\ndef get_first_price(data, starting_point, sid, date):\n    starting_day = date - timedelta(starting_point)\n    date_index = data.index.searchsorted(starting_day)\n\n    return data.iloc[date_index][sid]\n\ndef remove_outliers(returns, num_std_devs):\n    return returns[~((returns-returns.mean()).abs()>num_std_devs*returns.std())]\n\ndef get_returns(data, starting_point, sid, date, day_num):\n    first_price = get_first_price(data, starting_point, sid, date)\n    close_price = get_close_price(data, sid, date, day_num)\n    if first_price==0:\n        return 0\n    return (close_price - first_price) / (first_price + 0.0)\n\ndef calc_beta(stock, benchmark, price_history):\n    """"""\n    Calculate beta amounts for each security\n    """"""\n    stock_prices = price_history[stock].pct_change().dropna()\n    bench_prices = price_history[benchmark].pct_change().dropna()\n    aligned_prices = bench_prices.align(stock_prices, join=\'inner\')\n    bench_prices = aligned_prices[0]\n    stock_prices = aligned_prices[1]\n    bench_prices = np.array(bench_prices.values)\n    stock_prices = np.array(stock_prices.values)\n    bench_prices = np.reshape(bench_prices,len(bench_prices))\n    stock_prices = np.reshape(stock_prices,len(stock_prices))\n    if len(stock_prices) == 0:\n        return None\n#    market_beta, benchmark_beta = np.polyfit(bench_prices, stock_prices, 1)\n    slope,intercept,r_value,p_value,stderr = stats.linregress(bench_prices,stock_prices)\n    return slope\n\n\ndef build_x_ticks(day_numbers):\n    return [d for d in day_numbers if d % 2 == 0]\n\ndef plot_cumulative_returns(returns, x_ticks,events):\n    pyplot.figure(figsize=FIGURE_SIZE)\n\n    returns.plot(xticks=x_ticks, label=""events=%s"" % events)\n\n    pyplot.title(""Cumulative Return from Events"")\n    pyplot.xlabel(""Window Length (t)"")\n    pyplot.ylabel(""Cumulative Return (r)"")\n    pyplot.grid(b=None, which=u\'major\', axis=u\'y\')\n    pyplot.legend()\n\ndef plot_average_returns(returns, benchmark_returns, x_ticks):\n    pyplot.figure(figsize=FIGURE_SIZE)\n\n    returns.plot(xticks=x_ticks, label=""Cumulative Return from Events"")\n    benchmark_returns.plot(xticks=x_ticks, label=\'Benchmark\')\n\n    pyplot.title(""Benchmark\'s average returns around that time to Signals_Events"")\n    pyplot.ylabel(""% Cumulative Return"")\n    pyplot.xlabel(""Time Window"")\n    pyplot.grid(b=None, which=u\'major\', axis=u\'y\')\n    pyplot.legend()\n\ndef plot_cumulative_abnormal_returns(returns, abnormal_returns, x_ticks):\n    pyplot.figure(figsize=FIGURE_SIZE)\n\n    returns.plot(xticks=x_ticks, label=""Average Cumulative Returns"")\n    abnormal_returns.plot(xticks=x_ticks, label=""Abnormal Average Cumulative Returns"")\n\n    pyplot.axhline(\n        y=abnormal_returns.loc[0],\n        linestyle=\'--\',\n        color=\'black\',\n        alpha=.3,\n        label=\'Drift\'\n    )\n\n    pyplot.axhline(\n        y=abnormal_returns.max(),\n        linestyle=\'--\',\n        color=\'black\',\n        alpha=.3\n    )\n\n    pyplot.title(""Cumulative Abnormal Returns versus Cumulative Returns"")\n    pyplot.ylabel(""% Cumulative Return"")\n    pyplot.xlabel(""Time Window"")\n    pyplot.grid(b=None, which=u\'major\', axis=u\'y\')\n    pyplot.legend()\n\n\ndef plot_cumulative_return_with_errors(returns, std_devs,events):\n    """"""\n    Plotting the same graph but with error bars\n    """"""\n    pyplot.figure(figsize=FIGURE_SIZE)\n\n    pyplot.errorbar(returns.index, returns, xerr=0, yerr=std_devs, label=""events=%s"" % events)\n    pyplot.grid(b=None, which=u\'major\', axis=u\'y\')\n    pyplot.title(""Cumulative Return from Events with error"")\n    pyplot.xlabel(""Window Length (t)"")\n    pyplot.ylabel(""Cumulative Return (r)"")\n    pyplot.legend()\n    pyplot.show()\n\ndef plot_abnormal_cumulative_return_with_errors(abnormal_volatility, abnormal_returns,events):\n    """"""\n    Capturing volatility of abnormal returns\n    """"""\n    pyplot.figure(figsize=FIGURE_SIZE)\n\n    pyplot.errorbar(\n        abnormal_returns.index,\n        abnormal_returns,\n        xerr=0,\n        yerr=abnormal_volatility,\n        label=""events=%s"" % events\n    )\n\n    pyplot.grid(b=None, which=u\'major\', axis=u\'y\')\n    pyplot.title(""Abnormal Cumulative Return from Events with error"")\n    pyplot.xlabel(""Window Length (t)"")\n    pyplot.ylabel(""Cumulative Return (r)"")\n    pyplot.legend()\n    pyplot.show()\n\ndef build_day_numbers(starting_point):\n    """"""\n    Create our range of day_numbers that will be used to calculate returns\n    Looking from -starting_point to +starting_point to create timeframe band\n    """"""\n    return [i for i in range(-starting_point, starting_point)]\n\ndef get_price_history(data,date,beta_window,sid,benchmark):\n    """"""\n    Create a DataFrame containing the data for the necessary sids within that time frame\n    """"""\n    if beta_window==None:\n        history_index = data.index.searchsorted(date)\n        history_index_start = data.index.searchsorted(data[data[sid]!=0].index[0])\n        histotical_prices=data.iloc[history_index_start:history_index][[sid, benchmark]]\n    else:\n        history_index = data.index.searchsorted(date)\n        history_index_start = max([history_index - beta_window, 0])\n        histotical_prices=data.iloc[history_index_start:history_index][[sid, benchmark]]\n        histotical_prices=histotical_prices[histotical_prices[sid]!=0]\n    return histotical_prices[histotical_prices!=0].dropna()\n\n\ndef compute_return_matrix(ev_data,data,sample_size,starting_point,\n                          day_num,benchmark,returns,benchmark_returns,abnormal_returns,beta_window):\n    """"""\n    Computes the returns for the project, benchmark and abnormal\n    """"""\n    for date, row in ev_data.iterrows():\n        sid = row.symbol\n        if date not in data.index or sid not in data.columns:\n                continue\n\t\t\n        if sid==\'ethereum\' and benchmark==\'ethereum\':\n            benchmark=\'bitcoin\'\n        elif sid==\'bitcoin\' and benchmark==\'bitcoin\':\n            benchmark=\'ethereum\'\n\t\t\n        project_return = get_returns(data, starting_point, sid, date, day_num)\n        benchmark_return = get_returns(data, starting_point, benchmark, date, day_num)\n\n        returns.append(project_return)\n        benchmark_returns.append(benchmark_return)\n        sample_size += 1\n\n        beta = calc_beta(sid, benchmark, get_price_history(data,date,beta_window,sid,benchmark))\n        if beta is None:\n            continue\n        abnormal_return = project_return - (beta * benchmark_return)\n        abnormal_returns.append(abnormal_return)\n    return sample_size     \n        \n\ndef compute_averages(ev_data,data,starting_point,day_numbers,\n                     benchmark,all_returns,all_std_devs,\n                     total_sample_size,all_benchmark_returns,\n                     abnormal_volatility,all_abnormal_returns,beta_window): \n    \n    """"""\n    Computes the avegare returns and standards deviation of the events\n    """"""\n        \n    for day_num in day_numbers:\n        returns = []\n        benchmark_returns = []\n        abnormal_returns = []\n        sample_size = 0\n\n        sample_size=compute_return_matrix(ev_data,data,sample_size,starting_point,\n                                          day_num,benchmark,returns,benchmark_returns,abnormal_returns,beta_window)\n        returns = pd.Series(returns).dropna()\n        returns = remove_outliers(returns, 2)\n\n        abnormal_returns = pd.Series(abnormal_returns).dropna()\n        abnormal_returns = remove_outliers(abnormal_returns, 2)\n\n        all_returns[day_num] = np.average(returns)\n        all_std_devs[day_num] = np.std(returns)\n        total_sample_size[day_num] = sample_size\n        all_benchmark_returns[day_num] = np.average(pd.Series(benchmark_returns).dropna())\n\n        abnormal_volatility[day_num] = np.std(abnormal_returns)\n        all_abnormal_returns[day_num] = np.average(abnormal_returns)\n        \n        \ndef clean_data(data, events, starting_point):\n    """"""\n    Cleans signals that does not have enough pricing data\n    """"""\n    events_df=events.copy(deep=True)\n    events_df[\'in_pricesdf\']=0\n    id=0\n    \n    for date, row in events_df.iterrows():\n        sid = row.symbol\n        if date not in data.index or sid not in data.columns:\n            events_df.iloc[id,-1]=1\n            id=id+1\n            continue\n        event_day= data.index.searchsorted(date)\n        hist_index_start = event_day - starting_point\n        hist_index_end = event_day + starting_point\n        event_window=data.iloc[hist_index_start:hist_index_end][[sid]]\n        if event_window.min()[0]==0 or len(event_window)==0:\n            events_df.iloc[id,-1]=1\n        id=id+1\n    return events_df[events_df[\'in_pricesdf\']==0]    \n        \n\n\ndef event_study(data, events, starting_point=30, benchmark=\'bitcoin\', origin_zero=True,beta_window=None):\n    \n    ev_data=clean_data(data, events, starting_point)\n    \n    all_returns = {}\n    all_std_devs = {}\n    all_benchmark_returns = {}\n    all_abnormal_returns = {}\n    abnormal_volatility = {}\n    total_sample_size = {}\n\n    day_numbers = build_day_numbers(starting_point)\n    compute_averages(ev_data,data,starting_point,day_numbers,\n                     benchmark,all_returns,all_std_devs,\n                     total_sample_size,all_benchmark_returns,\n                     abnormal_volatility,all_abnormal_returns,beta_window)\n   \n    plotting_events(day_numbers,all_returns,all_benchmark_returns,all_abnormal_returns,\n                    all_std_devs,abnormal_volatility,\n                    total_sample_size,origin_zero)\n\ndef signals_format(signals,project):\n    """"""\n    Returns signals in the needed format.\n    Accepts a column with the signals as boolean values and the projects name as a string\n    """"""\n    sign = pd.DataFrame(signals).tz_convert(None)\n    sign.columns = [\'symbol\']\n    sign = sign.replace(True, project)\n    events_ = sign[sign[""symbol""] == project]\n\n    return events_\n\ndef plotting_events(day_numbers,all_returns,all_benchmark_returns,all_abnormal_returns,all_std_devs,\n                    abnormal_volatility,total_sample_size,origin_zero):\n\n    all_returns = pd.Series(all_returns)\n    all_std_devs = pd.Series(all_std_devs)\n    all_benchmark_returns = pd.Series(all_benchmark_returns)\n    all_abnormal_returns = pd.Series(all_abnormal_returns)\n    abnormal_volatility = pd.Series(abnormal_volatility)\n    events = np.average(pd.Series(total_sample_size))\n    \n    if origin_zero==True:\n        all_returns = all_returns - all_returns.loc[0]\n        all_benchmark_returns = all_benchmark_returns - all_benchmark_returns.loc[0]\n        all_abnormal_returns = all_abnormal_returns - all_abnormal_returns.loc[0]\n        all_std_devs = all_std_devs - all_std_devs.loc[0]\n        abnormal_volatility = abnormal_volatility - abnormal_volatility.loc[0]\n\n    all_std_devs.loc[:-1] = 0\n    abnormal_volatility.loc[:-1] = 0\n    \n    x_ticks = build_x_ticks(day_numbers)\n\n    plot_cumulative_returns(\n        returns=all_returns,\n        x_ticks=x_ticks,\n        events=events\n    )\n\n    plot_average_returns(\n        returns=all_returns,\n        benchmark_returns=all_benchmark_returns,\n        x_ticks=x_ticks\n    )\n\n    plot_cumulative_abnormal_returns(\n        returns=all_returns,\n        abnormal_returns=all_abnormal_returns,\n        x_ticks=x_ticks\n    )\n    plot_cumulative_return_with_errors(\n        returns=all_returns,\n        std_devs=all_std_devs,\n        events=events\n    )\n\n    plot_abnormal_cumulative_return_with_errors(\n        abnormal_volatility=abnormal_volatility,\n        abnormal_returns=all_abnormal_returns,\n        events=events)\n'"
san/extras/triple_barrier.py,0,"b'import pandas as pd\nimport mlfinlab as ml\nimport matplotlib.pyplot as plt\n\n\ndef get_labels_df(prices, signals, days, pt_sl, min_ret, lookback):\n    daily_vol = ml.util.get_daily_vol(close=prices, lookback=lookback)\n\n    vertical_barriers = ml.labeling.add_vertical_barrier(t_events=signals, close=prices, num_days=days)\n    triple_barrier_events = ml.labeling.get_events(close=prices,\n                                                   t_events=signals,\n                                                   pt_sl=pt_sl,\n                                                   target=daily_vol,\n                                                   min_ret=min_ret,\n                                                   num_threads=1,\n                                                   vertical_barrier_times=vertical_barriers)\n\n    labels = ml.labeling.get_bins(triple_barrier_events, prices)\n    return labels\n\n\ndef evaluate(prices, signals, pt_sl = [1, 2],min_ret = 0.005, num_days = 5, lookback=50, interval=\'1d\'):\n\n    labels = pd.DataFrame()\n    slugs = signals.slug.unique()\n    for slug in slugs:\n        try:\n            project_signals=signals[signals[\'slug\']==slug].index.tz_localize(\'UTC\')\n            project_prices = prices[slug]\n            label_project= get_labels_df(project_prices, project_signals, pt_sl=pt_sl, min_ret=min_ret, days=num_days, lookback=lookback)\n            label_project[\'slug\']=slug\n            labels=pd.concat([labels, label_project])\n        except Exception as e:\n            print(e)\n    return labels\n\n\ndef plot_rectangle(ax1, project_prices, labels, color, side, num_days, pt_sl, alpha=0.4, ls=\'--\'):\n    for point in labels[labels[\'bin\']==side].index:\n\n        xdata=[point, (point + pd.DateOffset(num_days))]\n        ypoint_price_upper=project_prices[point]+(project_prices[point]*labels[\'trgt\'][point]*pt_sl[0])\n        ypoint_price_lower=project_prices[point]-(project_prices[point]*labels[\'trgt\'][point]*pt_sl[1])\n        ydata=[ypoint_price_upper, ypoint_price_upper]\n        ax1.plot(xdata, ydata,color=color, alpha=alpha, ls=ls)\n        ax1.plot(xdata, [ypoint_price_lower,ypoint_price_lower], color=color,alpha=alpha,ls=ls)\n\n\n        xdata=[point, point]\n        ydata=[ypoint_price_upper,ypoint_price_lower]\n        ax1.plot(xdata, ydata,color=color, alpha=alpha, ls=ls)\n\n        xdata=[point+ pd.DateOffset(num_days), point+ pd.DateOffset(num_days)]\n        ydata=[ypoint_price_upper,ypoint_price_lower]\n        ax1.plot(xdata, ydata,color=color, alpha=alpha, ls=ls)\n    \n\n\ndef plot(prices, labels,pt_sl = [1, 2], num_days=5, lookback=50):\n    \n    slug=labels[\'slug\'][0]\n    project_prices = prices[slug]\n    plt.rcParams[""figure.figsize""] = (20,3)\n    fig = plt.figure()\n    ax = plt.subplot()\n    ax.plot(prices[slug].index, project_prices, color=\'g\', label=\'price\')\n    ax1 = plt.subplot()     \n    plot_rectangle(ax1, project_prices, labels, color=\'red\', side=1, pt_sl = [1, 2], num_days=num_days)\n    plot_rectangle(ax1, project_prices, labels, color=\'blue\', side=-1, pt_sl = [1, 2], num_days=num_days)\n    plot_rectangle(ax1, project_prices, labels, color=\'black\', side=0, pt_sl = [1, 2], num_days=num_days)\n    ax.legend()\n    plt.show()\n'"
san/tests/__init__.py,0,b''
san/tests/test_batch.py,0,"b'from san import Batch\nfrom unittest.mock import Mock, patch\nfrom san.pandas_utils import convert_to_datetime_idx_df\nimport pandas.testing as pdt\nfrom san.tests.utils import TestResponse\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_batch(mock):\n    expected = {\'query_0\': [{\'balance\': 212664.33000000002,\n                             \'datetime\': \'2019-05-18T00:00:00Z\'},\n                            {\'balance\': 212664.33000000002,\n                             \'datetime\': \'2019-05-19T00:00:00Z\'},\n                            {\'balance\': 212664.33000000002,\n                             \'datetime\': \'2019-05-20T00:00:00Z\'},\n                            {\'balance\': 212664.33000000002,\n                             \'datetime\': \'2019-05-21T00:00:00Z\'},\n                            {\'balance\': 212664.33000000002,\n                             \'datetime\': \'2019-05-22T00:00:00Z\'}],\n                \'query_1\': [{\'balance\': 212664.33000000002,\n                             \'datetime\': \'2019-05-23T00:00:00Z\'},\n                            {\'balance\': 212664.33000000002,\n                             \'datetime\': \'2019-05-24T00:00:00Z\'},\n                            {\'balance\': 212664.33000000002,\n                             \'datetime\': \'2019-05-25T00:00:00Z\'}]}\n    mock.return_value = TestResponse(status_code=200, data=expected)\n\n    batch = Batch()\n    batch.get(\n        ""historical_balance/santiment"",\n        address=""0x1f3df0b8390bb8e9e322972c5e75583e87608ec2"",\n        from_date=""2019-05-18"",\n        to_date=""2019-05-23"",\n        interval=""1d""\n    )\n    batch.get(\n        ""historical_balance/santiment"",\n        address=""0x1f3df0b8390bb8e9e322972c5e75583e87608ec2"",\n        from_date=""2019-05-23"",\n        to_date=""2019-05-26"",\n        interval=""1d""\n    )\n    [res1, res2] = batch.execute()\n\n    df1 = convert_to_datetime_idx_df(expected[\'query_0\'])\n    df2 = convert_to_datetime_idx_df(expected[\'query_1\'])\n\n    pdt.assert_frame_equal(res1, df1, check_dtype=False)\n    pdt.assert_frame_equal(res2, df2, check_dtype=False)\n'"
san/tests/test_get.py,0,"b'import san\nimport pandas as pd\nimport pandas.testing as pdt\nfrom san.error import SanError\nfrom nose.tools import assert_raises\nfrom unittest.mock import Mock, patch\nfrom san.pandas_utils import convert_to_datetime_idx_df\nfrom san.tests.utils import TestResponse\nfrom copy import deepcopy\nfrom san.batch import Batch\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_get(mock):\n    api_call_result = {\'query_0\': [{\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-23T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-24T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-25T00:00:00Z\'}]}\n    mock.return_value = TestResponse(\n        status_code=200, data=deepcopy(api_call_result))\n\n    res = san.get(\n        ""historical_balance/santiment"",\n        address=""0x1f3df0b8390bb8e9e322972c5e75583e87608ec2"",\n        from_date=""2019-05-23"",\n        to_date=""2019-05-26"",\n        interval=""1d""\n    )\n    expected_df = convert_to_datetime_idx_df(api_call_result[\'query_0\'])\n    pdt.assert_frame_equal(res, expected_df, check_dtype=False)\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_prices(mock):\n    api_call_result = {\n        \'query_0\':\n            [\n                {\n                    \'priceUsd\': \'1.234634930555555\',\n                    \'priceBtc\': \'0.0001649780416666666\',\n                    \'datetime\': \'2018-06-01T00:00:00Z\'\n                },\n                {\n                    \'priceUsd\': \'1.2551352777777771\',\n                    \'priceBtc\': \'0.00016521851041666669\',\n                    \'datetime\': \'2018-06-02T00:00:00Z\'\n                },\n                {\n                    \'priceUsd\': \'1.251881943462897\',\n                    \'priceBtc\': \'0.000162902558303887\',\n                    \'datetime\': \'2018-06-03T00:00:00Z\'\n                },\n                {\n                    \'priceUsd\': \'1.2135782638888888\',\n                    \'priceBtc\': \'0.0001600935277777778\',\n                    \'datetime\': \'2018-06-04T00:00:00Z\'\n                }\n            ]\n    }\n    mock.return_value = TestResponse(\n        status_code=200, data=deepcopy(api_call_result))\n\n    res = san.get(\n        ""prices/santiment_usd"",\n        from_date=""2018-06-01"",\n        to_date=""2018-06-05"",\n        interval=""1d""\n    )\n\n    df = convert_to_datetime_idx_df(api_call_result[\'query_0\'])\n    pdt.assert_frame_equal(res, df, check_dtype=False)\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_error_response(mock):\n    mock.return_value = TestResponse(\n        status_code=500, data={\n            \'errors\': {\n                \'detail\': \'Internal server error\'}})\n    with assert_raises(SanError):\n        san.get(\n            ""prices/santiment_usd"",\n            from_date=""2018-06-01"",\n            to_date=""2018-06-05"",\n            interval=""1d""\n        )\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_get_without_transform(mock):\n    api_call_result = {\'query_0\': [{\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-23T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-24T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-25T00:00:00Z\'}]}\n\n    # The value is passed by refference, that\'s why deepcopy is used for\n    # expected\n    mock.return_value = TestResponse(\n        status_code=200, data=deepcopy(api_call_result))\n\n    res = san.get(\n        ""historical_balance/santiment"",\n        address=""0x1f3df0b8390bb8e9e322972c5e75583e87608ec2"",\n        from_date=""2019-05-23"",\n        to_date=""2019-05-26"",\n        interval=""1d""\n    )\n    expected_df = convert_to_datetime_idx_df(api_call_result[\'query_0\'])\n    pdt.assert_frame_equal(res, expected_df, check_dtype=False)\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_get_with_transform(mock):\n    api_call_result = {\n        \'query_0\': {\n            \'ethTopTransactions\': [\n                {\n                    \'datetime\': \'2019-04-19T14:14:52.000000Z\',\n                    \'fromAddress\': {\n                        \'address\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\',\n                        \'isExchange\': False},\n                    \'toAddress\': {\n                        \'address\': \'0xd69bc0585e05ea381ce3ae69626ce4e8a0629e16\',\n                        \'isExchange\': False},\n                    \'trxHash\': \'0x590512e1f1fbcfd48a13d1997842777269f7c1915b7baef3c11ba83ca62495be\',\n                    \'trxValue\': 19.48},\n                {\n                    \'datetime\': \'2019-04-19T14:09:58.000000Z\',\n                    \'fromAddress\': {\n                                \'address\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\',\n                                \'isExchange\': False},\n                    \'toAddress\': {\n                        \'address\': \'0x723fb5c14eaff826b386052aace3b9b21999bf50\',\n                        \'isExchange\': False},\n                    \'trxHash\': \'0x78e0720b9e72d1d4d44efeff8393758d7b09c16f5156d63e021655e70be29db5\',\n                    \'trxValue\': 15.15}]}}\n\n    # The value is passed by refference, that\'s why deepcopy is used for\n    # expected\n    mock.return_value = TestResponse(\n        status_code=200, data=deepcopy(api_call_result))\n\n    result = san.get(\n        ""eth_top_transactions/santiment"",\n        from_date=""2019-04-18"",\n        to_date=""2019-04-22"",\n        limit=5,\n        transaction_type=""ALL""\n    )\n    expected = [{\'datetime\': \'2019-04-19T14:14:52.000000Z\',\n                 \'fromAddress\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\',\n                 \'fromAddressIsExchange\': False,\n                 \'toAddress\': \'0xd69bc0585e05ea381ce3ae69626ce4e8a0629e16\',\n                 \'toAddressIsExchange\': False,\n                 \'trxHash\': \'0x590512e1f1fbcfd48a13d1997842777269f7c1915b7baef3c11ba83ca62495be\',\n                 \'trxValue\': 19.48},\n                {\'datetime\': \'2019-04-19T14:09:58.000000Z\',\n                 \'fromAddress\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\',\n                 \'fromAddressIsExchange\': False,\n                 \'toAddress\': \'0x723fb5c14eaff826b386052aace3b9b21999bf50\',\n                 \'toAddressIsExchange\': False,\n                 \'trxHash\': \'0x78e0720b9e72d1d4d44efeff8393758d7b09c16f5156d63e021655e70be29db5\',\n                 \'trxValue\': 15.15}]\n\n    expected_df = pd.DataFrame(expected, columns=expected[0].keys())\n    if \'datetime\' in expected_df.columns:\n        expected_df[\'datetime\'] = pd.to_datetime(\n            expected_df[\'datetime\'], utc=True)\n        expected_df.set_index(\'datetime\', inplace=True)\n    pdt.assert_frame_equal(result, expected_df, check_dtype=False)\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_batch_only_with_nontransform_queries(mock):\n    api_call_result = {\'query_0\': [{\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-18T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-19T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-20T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-21T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-22T00:00:00Z\'}],\n                       \'query_1\': [{\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-23T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-24T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-25T00:00:00Z\'}]}\n\n    # The value is passed by refference, that\'s why deepcopy is used for\n    # expected\n    mock.return_value = TestResponse(\n        status_code=200, data=deepcopy(api_call_result))\n\n    expected = api_call_result\n\n    batch = Batch()\n\n    batch.get(\n        ""historical_balance/santiment"",\n        address=""0x1f3df0b8390bb8e9e322972c5e75583e87608ec2"",\n        from_date=""2019-05-18"",\n        to_date=""2019-05-23"",\n        interval=""1d""\n    )\n\n    batch.get(\n        ""historical_balance/santiment"",\n        address=""0x1f3df0b8390bb8e9e322972c5e75583e87608ec2"",\n        from_date=""2019-05-23"",\n        to_date=""2019-05-26"",\n        interval=""1d""\n    )\n\n    result = batch.execute()\n\n    expected_result = []\n\n    queries = [\'historical_balance\', \'historical_balance\']\n    for idx, query in enumerate(queries):\n        df = pd.DataFrame(expected[\'query_\' + str(idx)])\n        if \'datetime\' in df.columns:\n            df[\'datetime\'] = pd.to_datetime(df[\'datetime\'], utc=True)\n            df.set_index(\'datetime\', inplace=True)\n        expected_result.append(df)\n\n    assert len(result) == len(expected_result)\n\n    for i in range(0, len(result)):\n        pdt.assert_frame_equal(\n            result[i],\n            expected_result[i],\n            check_dtype=False)\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_batch_only_with_transform_queries(mock):\n    api_call_result = {\n        \'query_0\': {\n            \'ethTopTransactions\': [\n                {\n                    \'datetime\': \'2019-04-19T14:14:52.000000Z\',\n                    \'fromAddress\': {\n                        \'address\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\',\n                        \'isExchange\': False},\n                    \'toAddress\': {\n                        \'address\': \'0xd69bc0585e05ea381ce3ae69626ce4e8a0629e16\',\n                        \'isExchange\': False},\n                    \'trxHash\': \'0x590512e1f1fbcfd48a13d1997842777269f7c1915b7baef3c11ba83ca62495be\',\n                    \'trxValue\': 19.48},\n                {\n                    \'datetime\': \'2019-04-19T14:09:58.000000Z\',\n                    \'fromAddress\': {\n                                \'address\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\',\n                                \'isExchange\': False},\n                    \'toAddress\': {\n                        \'address\': \'0x723fb5c14eaff826b386052aace3b9b21999bf50\',\n                        \'isExchange\': False},\n                    \'trxHash\': \'0x78e0720b9e72d1d4d44efeff8393758d7b09c16f5156d63e021655e70be29db5\',\n                    \'trxValue\': 15.15}]},\n        \'query_1\': {\n            \'ethTopTransactions\': [\n                {\n                    \'datetime\': \'2019-04-29T21:33:31.000000Z\',\n                    \'fromAddress\': {\n                        \'address\': \'0xe76fe52a251c8f3a5dcd657e47a6c8d16fdf4bfa\',\n                        \'isExchange\': False},\n                    \'toAddress\': {\n                        \'address\': \'0x45d6275d9496bc61b5b072a75089be4c5ab54068\',\n                        \'isExchange\': False},\n                    \'trxHash\': \'0x776cd57382456adb2f9c22bc2e06b4ddc9937b073e7f2206737933b46806f658\',\n                    \'trxValue\': 100.0},\n                {\n                    \'datetime\': \'2019-04-29T21:21:18.000000Z\',\n                    \'fromAddress\': {\n                        \'address\': \'0xe76fe52a251c8f3a5dcd657e47a6c8d16fdf4bfa\',\n                        \'isExchange\': False},\n                    \'toAddress\': {\n                        \'address\': \'0x468bdccdc334f646f1acfb4f69e3e855d23440e2\',\n                        \'isExchange\': False},\n                    \'trxHash\': \'0x848414fb5c382f2a9fb1856425437b60a5471ae383b884100794f8adf12227a6\',\n                    \'trxValue\': 40.95}]}}\n\n    # The value is passed by refference, that\'s why deepcopy is used for\n    # expected\n    mock.return_value = TestResponse(\n        status_code=200, data=deepcopy(api_call_result))\n\n    batch = Batch()\n\n    batch.get(\n        ""eth_top_transactions/santiment"",\n        from_date=""2019-04-18"",\n        to_date=""2019-04-22"",\n        limit=5,\n        transaction_type=""ALL""\n    )\n\n    batch.get(\n        ""eth_top_transactions/santiment"",\n        from_date=""2019-04-23"",\n        to_date=""2019-04-29"",\n        limit=5,\n        transaction_type=""ALL""\n    )\n\n    result = batch.execute()\n\n    expected = [[{\'datetime\': \'2019-04-19T14:14:52.000000Z\',\n                  \'fromAddress\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\',\n                  \'fromAddressIsExchange\': False,\n                  \'toAddress\': \'0xd69bc0585e05ea381ce3ae69626ce4e8a0629e16\',\n                  \'toAddressIsExchange\': False,\n                  \'trxHash\': \'0x590512e1f1fbcfd48a13d1997842777269f7c1915b7baef3c11ba83ca62495be\',\n                  \'trxValue\': 19.48},\n                 {\'datetime\': \'2019-04-19T14:09:58.000000Z\',\n                  \'fromAddress\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\',\n                  \'fromAddressIsExchange\': False,\n                  \'toAddress\': \'0x723fb5c14eaff826b386052aace3b9b21999bf50\',\n                  \'toAddressIsExchange\': False,\n                  \'trxHash\': \'0x78e0720b9e72d1d4d44efeff8393758d7b09c16f5156d63e021655e70be29db5\',\n                  \'trxValue\': 15.15}],\n                [{\'datetime\': \'2019-04-29T21:33:31.000000Z\',\n                  \'fromAddress\': \'0xe76fe52a251c8f3a5dcd657e47a6c8d16fdf4bfa\',\n                  \'fromAddressIsExchange\': False,\n                  \'toAddress\': \'0x45d6275d9496bc61b5b072a75089be4c5ab54068\',\n                  \'toAddressIsExchange\': False,\n                  \'trxHash\': \'0x776cd57382456adb2f9c22bc2e06b4ddc9937b073e7f2206737933b46806f658\',\n                  \'trxValue\': 100.0},\n                 {\'datetime\': \'2019-04-29T21:21:18.000000Z\',\n                    \'fromAddress\': \'0xe76fe52a251c8f3a5dcd657e47a6c8d16fdf4bfa\',\n                    \'fromAddressIsExchange\': False,\n                    \'toAddress\': \'0x468bdccdc334f646f1acfb4f69e3e855d23440e2\',\n                    \'toAddressIsExchange\': False,\n                    \'trxHash\': \'0x848414fb5c382f2a9fb1856425437b60a5471ae383b884100794f8adf12227a6\',\n                    \'trxValue\': 40.95}]]\n\n    expected_result = []\n\n    queries = [\'eth_top_transactions\', \'eth_top_transactions\']\n    for idx, query in enumerate(queries):\n        df = pd.DataFrame(expected[idx], columns=expected[idx][0].keys())\n\n        if \'datetime\' in df.columns:\n            df[\'datetime\'] = pd.to_datetime(df[\'datetime\'], utc=True)\n            df.set_index(\'datetime\', inplace=True)\n        expected_result.append(df)\n\n    assert len(result) == len(expected_result)\n\n    for i in range(0, len(result)):\n        pdt.assert_frame_equal(\n            result[i],\n            expected_result[i],\n            check_dtype=False)\n\n\n@patch(\'san.graphql.requests.post\')\ndef test_batch_with_mixed_queries(mock):\n    api_call_result = {\'query_0\': [{\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-18T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-19T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-20T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-21T00:00:00Z\'},\n                                   {\'balance\': 212664.33000000002,\n                                    \'datetime\': \'2019-05-22T00:00:00Z\'}],\n                       \'query_1\': {\'ethTopTransactions\': [{\'datetime\': \'2019-04-29T21:33:31.000000Z\',\n                                                           \'fromAddress\': {\'address\': \'0xe76fe52a251c8f3a5dcd657e47a6c8d16fdf4bfa\',\n                                                                           \'isExchange\': False},\n                                                           \'toAddress\': {\'address\': \'0x45d6275d9496bc61b5b072a75089be4c5ab54068\',\n                                                                         \'isExchange\': False},\n                                                           \'trxHash\': \'0x776cd57382456adb2f9c22bc2e06b4ddc9937b073e7f2206737933b46806f658\',\n                                                           \'trxValue\': 100.0},\n                                                          {\'datetime\': \'2019-04-29T21:21:18.000000Z\',\n                                                           \'fromAddress\': {\'address\': \'0xe76fe52a251c8f3a5dcd657e47a6c8d16fdf4bfa\',\n                                                                           \'isExchange\': False},\n                                                           \'toAddress\': {\'address\': \'0x468bdccdc334f646f1acfb4f69e3e855d23440e2\',\n                                                                         \'isExchange\': False},\n                                                           \'trxHash\': \'0x848414fb5c382f2a9fb1856425437b60a5471ae383b884100794f8adf12227a6\',\n                                                           \'trxValue\': 40.95}]}}\n\n    # The value is passed by refference, that\'s why deepcopy is used for\n    # expected\n    mock.return_value = TestResponse(\n        status_code=200, data=deepcopy(api_call_result))\n\n    expected = [[{\'balance\': 212664.33000000002,\n                  \'datetime\': \'2019-05-18T00:00:00Z\'},\n                 {\'balance\': 212664.33000000002,\n                  \'datetime\': \'2019-05-19T00:00:00Z\'},\n                 {\'balance\': 212664.33000000002,\n                  \'datetime\': \'2019-05-20T00:00:00Z\'},\n                 {\'balance\': 212664.33000000002,\n                  \'datetime\': \'2019-05-21T00:00:00Z\'},\n                 {\'balance\': 212664.33000000002,\n                  \'datetime\': \'2019-05-22T00:00:00Z\'}],\n                [{\'datetime\': \'2019-04-29T21:33:31.000000Z\',\n                  \'fromAddress\': \'0xe76fe52a251c8f3a5dcd657e47a6c8d16fdf4bfa\',\n                  \'fromAddressIsExchange\': False,\n                  \'toAddress\': \'0x45d6275d9496bc61b5b072a75089be4c5ab54068\',\n                  \'toAddressIsExchange\': False,\n                  \'trxHash\': \'0x776cd57382456adb2f9c22bc2e06b4ddc9937b073e7f2206737933b46806f658\',\n                  \'trxValue\': 100.0},\n                 {\'datetime\': \'2019-04-29T21:21:18.000000Z\',\n                    \'fromAddress\': \'0xe76fe52a251c8f3a5dcd657e47a6c8d16fdf4bfa\',\n                    \'fromAddressIsExchange\': False,\n                    \'toAddress\': \'0x468bdccdc334f646f1acfb4f69e3e855d23440e2\',\n                    \'toAddressIsExchange\': False,\n                    \'trxHash\': \'0x848414fb5c382f2a9fb1856425437b60a5471ae383b884100794f8adf12227a6\',\n                    \'trxValue\': 40.95}]]\n\n    batch = Batch()\n\n    batch.get(\n        ""historical_balance/santiment"",\n        address=""0x1f3df0b8390bb8e9e322972c5e75583e87608ec2"",\n        from_date=""2019-05-18"",\n        to_date=""2019-05-23"",\n        interval=""1d""\n    )\n\n    batch.get(\n        ""eth_top_transactions/santiment"",\n        from_date=""2019-04-23"",\n        to_date=""2019-04-29"",\n        limit=5,\n        transaction_type=""ALL""\n    )\n\n    result = batch.execute()\n\n    expected_result = []\n\n    queries = [\'historical_balance\', \'eth_top_transactions\']\n    for idx, query in enumerate(queries):\n        df = pd.DataFrame(expected[idx], columns=expected[idx][0].keys())\n        if \'datetime\' in df.columns:\n            df[\'datetime\'] = pd.to_datetime(df[\'datetime\'], utc=True)\n            df.set_index(\'datetime\', inplace=True)\n\n        expected_result.append(df)\n\n    assert len(result) == len(expected_result)\n\n    for i in range(0, len(result)):\n        pdt.assert_frame_equal(\n            result[i],\n            expected_result[i],\n            check_dtype=False)\n\n\ndef test_invalid_request():\n    with assert_raises(ValueError):\n        san.get(""invalid_request"")\n\n\ndef test_invalid_method():\n    with assert_raises(SanError):\n        san.get(""invalid_method/slug"")\n'"
san/tests/test_sanbase_integration.py,0,"b'import san\nfrom san import Batch, sanbase_graphql, sanbase_graphql_helper, available_metrics\nfrom san.tests.utils import two_days_ago, four_days_ago, month_ago\nfrom nose.plugins.attrib import attr\n\n\nparams = {\n    \'project_slug\': \'santiment\',\n    \'from_date\': month_ago(),\n    \'to_date\': two_days_ago(),\n    \'interval\': \'1d\',\n    \'address\': \'0x1f3df0b8390bb8e9e322972c5e75583e87608ec2\'\n}\n\n# Metrics, which are made with _create_query_string, excluding the ones\n# using ETHEREUM\nMETRICS_EQUAL_FORMAT = [\n    \'daily_active_addresses\',\n    \'burn_rate\',\n    \'transaction_volume\',\n    \'token_age_consumed\',\n    \'average_token_age_consumed_in_days\',\n    \'github_activity\',\n    \'dev_activity\',\n    \'network_growth\',\n    \'prices\',\n    \'token_velocity\',\n    \'token_circulation\',\n    \'realized_value\',\n    \'mvrv_ratio\',\n    \'nvt_ratio\',\n    \'daily_active_deposits\',\n    \'ohlc\',\n    \'history_twitter_data\',\n    \'exchange_funds_flow\',\n]\n# Metrics, which are made with _create_query_string, using ETHEREUM\nMETRICS_USING_ETHEREUM = [\n    \'gas_used\',\n    \'miners_balance\',\n    \'mining_pools_distribution\'\n]\n\n\ndef _test_ordinary_function(\n        query,\n        graphiql_query,\n        slug=params[\'project_slug\']):\n    result = san.get(query + \'/\' + slug,\n                     from_date=params[\'from_date\'],\n                     to_date=params[\'to_date\'],\n                     interval=params[\'interval\']\n                     )\n    assert result.empty == False\n    assert len(result.index) >= 1\n    assert \'DatetimeIndex\' in str(type(result.index))\n\n\n@attr(\'integration\')\ndef test_batched_queries_equal_format():\n    batch = Batch()\n    for query in METRICS_EQUAL_FORMAT:\n        batch.get(\n            ""{}/{}"".format(query, params[\'project_slug\']),\n            from_date=params[\'from_date\'],\n            to_date=params[\'to_date\'],\n            interval=params[\'interval\']\n        )\n    for query in METRICS_USING_ETHEREUM:\n        batch.get(\n            ""{}/ethereum"".format(query),\n            from_date=params[\'from_date\'],\n            to_date=params[\'to_date\'],\n            interval=params[\'interval\']\n        )\n\n    result = batch.execute()\n\n    for df in result:\n        assert len(df.index) >= 1\n\n\n@attr(\'integration\')\ndef test_batched_queries_different_format():\n    batch = Batch()\n\n    batch.get(\'projects/all\')\n    batch.get(\'social_volume_projects\')\n    batch.get(\n        ""{}/{}"".format(\'social_volume\', \'ethereum\'),\n        from_date=month_ago(),\n        to_date=params[\'to_date\'],\n        interval=params[\'interval\'],\n        social_volume_type=\'PROFESSIONAL_TRADERS_CHAT_OVERVIEW\'\n    )\n    batch.get(\n        \'topic_search\',\n        source=\'TELEGRAM\',\n        search_text=\'btc\',\n        from_date=month_ago(),\n        to_date=params[\'to_date\'],\n        interval=params[\'interval\']\n    )\n\n    result = batch.execute()\n    for df in result:\n        assert len(df.index) >= 1\n\n\n@attr(\'integration\')\ndef test_erc20_projects():\n    result = san.get(\'projects/erc20\')\n\n    assert result.empty == False\n    assert len(result[result.slug == \'bitcoin\']) == 0\n\n\n@attr(\'integration\')\ndef test_daily_active_addresses():\n    _test_ordinary_function(\'daily_active_addresses\', \'dailyActiveAddresses\')\n\n\n@attr(\'integration\')\ndef test_burn_rate():\n    _test_ordinary_function(\'burn_rate\', \'burnRate\')\n\n\n@attr(\'integration\')\ndef test_gas_used():\n    _test_ordinary_function(\'gas_used\', \'gasUsed\', \'ethereum\')\n\n\n@attr(\'integration\')\ndef test_token_age_consumed():\n    _test_ordinary_function(\'token_age_consumed\', \'tokenAgeConsumed\')\n\n\n@attr(\'integration\')\ndef test_average_token_age_consumed_in_days():\n    _test_ordinary_function(\n        \'average_token_age_consumed_in_days\',\n        \'averageTokenAgeConsumedInDays\')\n\n\n@attr(\'integration\')\ndef test_transaction_volume():\n    _test_ordinary_function(\'transaction_volume\', \'transactionVolume\')\n\n\n@attr(\'integration\')\ndef test_github_activity():\n    _test_ordinary_function(\'github_activity\', \'githubActivity\')\n\n\n@attr(\'integration\')\ndef test_dev_activity():\n    _test_ordinary_function(\'dev_activity\', \'devActivity\')\n\n\n@attr(\'integration\')\ndef test_network_growth():\n    _test_ordinary_function(\'network_growth\', \'networkGrowth\')\n\n\n@attr(\'integration\')\ndef test_prices():\n    _test_ordinary_function(\'prices\', \'prices\')\n\n\n@attr(\'integration\')\ndef test_ohlc():\n    _test_ordinary_function(\'ohlc\', \'ohlc\')\n\n\n@attr(\'integration\')\ndef test_exchange_funds_flow():\n    _test_ordinary_function(\'exchange_funds_flow\', \'exchangeFundsFlow\')\n\n\n@attr(\'integration\')\ndef test_token_velocity():\n    _test_ordinary_function(\'token_velocity\', \'tokenVelocity\')\n\n\n@attr(\'integration\')\ndef test_token_circulation():\n    _test_ordinary_function(\'token_circulation\', \'tokenCirculation\')\n\n\n@attr(\'integration\')\ndef test_realized_value():\n    _test_ordinary_function(\'realized_value\', \'realizedValue\')\n\n\n@attr(\'integration\')\ndef test_mvrv_ratio():\n    _test_ordinary_function(\'mvrv_ratio\', \'mvrvRatio\')\n\n\n@attr(\'integration\')\ndef test_nvt_ratio():\n    _test_ordinary_function(\'nvt_ratio\', \'nvtRatio\')\n\n\n@attr(\'integration\')\ndef test_daily_active_deposits():\n    _test_ordinary_function(\'daily_active_deposits\', \'dailyActiveDeposits\')\n\n\n@attr(\'integration\')\ndef test_miners_balance():\n    _test_ordinary_function(\'miners_balance\', \'minersBalance\', \'ethereum\')\n\n\n@attr(\'integration\')\ndef test_mining_pools_distribution():\n    _test_ordinary_function(\n        \'mining_pools_distribution\',\n        \'miningPoolsDistribution\',\n        \'ethereum\')\n\n\n@attr(\'integration\')\ndef test_history_twitter_data():\n    _test_ordinary_function(\'history_twitter_data\', \'historyTwitterData\')\n\n\n@attr(\'integration\')\ndef test_historical_balance():\n    result = san.get(\'historical_balance/\' + params[\'project_slug\'],\n                     address=params[\'address\'],\n                     from_date=params[\'from_date\'],\n                     to_date=params[\'to_date\'],\n                     interval=params[\'interval\']\n                     )\n    assert len(result.index) >= 1\n    assert \'DatetimeIndex\' in str(type(result.index))\n    assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_social_dominance():\n    sources = [\n        \'ALL\',\n        \'REDDIT\',\n        \'TELEGRAM\',\n        \'DISCORD\',\n        \'PROFESSIONAL_TRADERS_CHAT\']\n    for item in sources:\n        result = san.get(\'social_dominance/\' + params[\'project_slug\'],\n                         from_date=params[\'from_date\'],\n                         to_date=params[\'to_date\'],\n                         interval=params[\'interval\'],\n                         source=item\n                         )\n        assert len(result.index) >= 1\n        assert \'DatetimeIndex\' in str(type(result.index))\n        assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_top_holders_percent_of_total_supply():\n    result = san.get(\n        \'top_holders_percent_of_total_supply/\' +\n        params[\'project_slug\'],\n        number_of_holders=10,\n        from_date=params[\'from_date\'],\n        to_date=params[\'to_date\'])\n    assert len(result.index) >= 1\n    assert \'DatetimeIndex\' in str(type(result.index))\n    assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_price_volume_difference():\n    currencies = [\'USD\', \'BTC\']\n    for item in currencies:\n        result = san.get(\'price_volume_difference/\' + params[\'project_slug\'],\n                         from_date=params[\'from_date\'],\n                         to_date=params[\'to_date\'],\n                         interval=params[\'interval\'],\n                         currency=item\n                         )\n        assert len(result.index) >= 1\n        assert \'DatetimeIndex\' in str(type(result.index))\n        assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_eth_top_transactions():\n    transaction_types = [\'ALL\', \'IN\', \'OUT\']\n    for item in transaction_types:\n        result = san.get(\'eth_top_transactions/\' + params[\'project_slug\'],\n                         from_date=\'2019-06-11\',\n                         to_date=\'2019-07-11\',\n                         limit=5,\n                         transaction_type=item\n                         )\n        assert len(result.index) >= 1\n        assert \'DatetimeIndex\' in str(type(result.index))\n        assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_news():\n    result = san.get(\'news/\' + params[\'project_slug\'],\n                     from_date=\'2019-04-18\',\n                     to_date=\'2019-07-11\',\n                     size=5\n                     )\n    assert len(result.index) >= 1\n    assert \'DatetimeIndex\' in str(type(result.index))\n    assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_eth_spent_over_time():\n    result = san.get(\'eth_spent_over_time/\' + params[\'project_slug\'],\n                     from_date=params[\'from_date\'],\n                     to_date=params[\'to_date\'],\n                     interval=params[\'interval\']\n                     )\n    assert len(result.index) >= 1\n    assert \'DatetimeIndex\' in str(type(result.index))\n    assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_token_top_transactions():\n    result = san.get(\'token_top_transactions/\' + params[\'project_slug\'],\n                     from_date=\'2019-06-18\',\n                     to_date=\'2019-07-11\',\n                     limit=5\n                     )\n    assert len(result.index) >= 1\n    assert \'DatetimeIndex\' in str(type(result.index))\n    assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_all_projects():\n    result = san.get(\'projects/all\')\n\n    assert len(result.index) >= 1\n    assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_social_volume_projects():\n    result = san.get(\'social_volume_projects\')\n\n    assert len(result.index) >= 1\n    assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_social_volume():\n    social_volume_types = [\n        \'PROFESSIONAL_TRADERS_CHAT_OVERVIEW\',\n        \'TELEGRAM_CHATS_OVERVIEW\',\n        \'TELEGRAM_DISCUSSION_OVERVIEW\',\n        \'DISCORD_DISCUSSION_OVERVIEW\']\n    for item in social_volume_types:\n        result = san.get(\'social_volume/\' + params[\'project_slug\'],\n                         from_date=params[\'from_date\'],\n                         to_date=params[\'to_date\'],\n                         interval=params[\'interval\'],\n                         social_volume_type=item\n                         )\n        assert len(result.index) >= 1\n        assert \'DatetimeIndex\' in str(type(result.index))\n        assert result.empty == False\n\n\n@attr(\'integration\')\ndef test_ohlcv():\n    ohlcv_df = san.get(\n        ""{}/{}"".format(\'ohlcv\', params[\'project_slug\']),\n        from_date=params[\'from_date\'],\n        to_date=params[\'to_date\'],\n        interval=params[\'interval\']\n    )\n\n    assert len(ohlcv_df.index) >= 1\n    assert \'DatetimeIndex\' in str(type(ohlcv_df.index))\n    assert ohlcv_df.empty == False\n\n\n@attr(\'integration\')\ndef test_top_social_gainers_losers():\n    tsgl_df = san.get(\n        \'top_social_gainers_losers\',\n        from_date=params[\'from_date\'],\n        to_date=params[\'to_date\'],\n        size=5,\n        time_window=\'2d\',\n        status=\'ALL\'\n    )\n\n    assert len(tsgl_df) >= 1\n    assert \'DatetimeIndex\' in str(type(tsgl_df.index))\n    assert tsgl_df.empty == False\n\n\n@attr(\'integration\')\ndef test_get_metric():\n    get_metric_df = san.get(\n        \'daily_active_addresses/\' + params[\'project_slug\'],\n        from_date=params[\'from_date\'],\n        to_date=params[\'to_date\'],\n        interval=\'2d\',\n        aggregation=\'AVG\'\n    )\n\n    assert len(get_metric_df) >= 1\n    assert \'DatetimeIndex\' in str(type(get_metric_df.index))\n    assert get_metric_df.empty == False\n\n\n@attr(\'integration\')\ndef test_emerging_trends():\n    get_metric_df = san.get(\n        \'emerging_trends\',\n        from_date=params[\'from_date\'],\n        to_date=params[\'to_date\'],\n        interval=\'1d\',\n        size=5\n    )\n\n    assert len(get_metric_df) >= 1\n    assert \'DatetimeIndex\' in str(type(get_metric_df.index))\n    assert get_metric_df.empty == False\n\n\n@attr(\'integration\')\ndef test_available_metrics():\n    functions_list = [\n        \'average_token_age_consumed_in_days\',\n        \'burn_rate\',\n        \'daily_active_deposits\',\n        \'dev_activity\',\n        \'emerging_trends\',\n        \'eth_spent_over_time\',\n        \'eth_top_transactions\',\n        \'exchange_funds_flow\',\n        \'gas_used\',\n        \'github_activity\',\n        \'historical_balance\',\n        \'history_twitter_data\',\n        \'miners_balance\',\n        \'mining_pools_distribution\',\n        \'mvrv_ratio\',\n        \'network_growth\',\n        \'news\',\n        \'nvt_ratio\',\n        \'ohlc\',\n        \'ohlcv\',\n        \'price_volume_difference\',\n        \'prices\',\n        \'projects\',\n        \'realized_value\',\n        \'social_dominance\',\n        \'social_volume\',\n        \'social_volume_projects\',\n        \'token_age_consumed\',\n        \'token_circulation\',\n        \'token_top_transactions\',\n        \'token_velocity\',\n        \'top_holders_percent_of_total_supply\',\n        \'top_social_gainers_losers\',\n        \'topic_search\',\n        \'daily_avg_marketcap_usd\',\n        \'daily_avg_price_usd\',\n        \'daily_closing_marketcap_usd\',\n        \'daily_closing_price_usd\',\n        \'daily_high_price_usd\',\n        \'daily_low_price_usd\',\n        \'daily_opening_price_usd\',\n        \'daily_trading_volume_usd\',\n        \'daily_active_addresses\',\n        \'mean_realized_price_usd\',\n        \'mean_realized_price_usd_10y\',\n        \'mean_realized_price_usd_5y\',\n        \'mean_realized_price_usd_3y\',\n        \'mean_realized_price_usd_2y\',\n        \'mean_realized_price_usd_365d\',\n        \'mean_realized_price_usd_180d\',\n        \'mean_realized_price_usd_90d\',\n        \'mean_realized_price_usd_60d\',\n        \'mean_realized_price_usd_30d\',\n        \'mean_realized_price_usd_7d\',\n        \'mean_realized_price_usd_1d\',\n        \'mvrv_usd_long_short_diff\',\n        \'mvrv_usd\',\n        \'mvrv_usd_10y\',\n        \'mvrv_usd_5y\',\n        \'mvrv_usd_3y\',\n        \'mvrv_usd_2y\',\n        \'mvrv_usd_365d\',\n        \'mvrv_usd_180d\',\n        \'mvrv_usd_90d\',\n        \'mvrv_usd_60d\',\n        \'mvrv_usd_30d\',\n        \'mvrv_usd_7d\',\n        \'mvrv_usd_1d\',\n        \'circulation\',\n        \'circulation_10y\',\n        \'circulation_5y\',\n        \'circulation_3y\',\n        \'circulation_2y\',\n        \'circulation_365d\',\n        \'circulation_180d\',\n        \'circulation_90d\',\n        \'circulation_60d\',\n        \'circulation_30d\',\n        \'circulation_7d\',\n        \'circulation_1d\',\n        \'mean_age\',\n        \'realized_value_usd\',\n        \'realized_value_usd_10y\',\n        \'realized_value_usd_5y\',\n        \'realized_value_usd_3y\',\n        \'realized_value_usd_2y\',\n        \'realized_value_usd_365d\',\n        \'realized_value_usd_180d\',\n        \'realized_value_usd_90d\',\n        \'realized_value_usd_60d\',\n        \'realized_value_usd_30d\',\n        \'realized_value_usd_7d\',\n        \'realized_value_usd_1d\',\n        \'velocity\',\n        \'transaction_volume\',\n        \'exchange_inflow\',\n        \'exchange_outflow\',\n        \'exchange_balance\',\n        \'age_destroyed\',\n        \'nvt\',\n        \'nvt_transaction_volume\',\n        \'network_growth\',\n        \'mean_dollar_invested_age\',\n        \'active_deposits\',\n        \'active_withdrawals\',\n        \'withdrawal_transactions\',\n        \'supply_on_exchanges\',\n        \'percent_of_total_supply_on_exchanges\',\n        \'amount_in_top_holders\'\n        ]\n\n    assert functions_list == san.available_metrics()\n    assert len(san.available_metrics()) >= 1\n\n\n@attr(\'integration\')\ndef test_metadata():\n    result = san.metadata(\n        \'nvt\',\n        arr=[\'defaultAggregation\', \'metric\']\n    )\n\n    expecter_result = {\'defaultAggregation\': \'AVG\', \'metric\': \'nvt\'}\n\n    assert result == expecter_result\n'"
san/tests/utils.py,0,"b""import datetime\n\n\ndef today():\n    return datetime.date.today()\n\n\ndef two_days_ago():\n    return (today() - datetime.timedelta(days=2)).isoformat()\n\n\ndef four_days_ago():\n    return (today() - datetime.timedelta(days=4)).isoformat()\n\n\ndef month_ago():\n    return (today() - datetime.timedelta(days=30)).isoformat()\n\n\nclass TestResponse:\n    def __init__(self, **kwargs):\n        self.status_code = kwargs['status_code']\n        if 'errors' in kwargs['data']:\n            self.data = kwargs['data']\n        else:\n            self.data = {'data': kwargs['data']}\n\n    def json(self):\n        return self.data\n\n    def status_code(self):\n        return self.status_code\n"""
