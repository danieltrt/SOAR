file_path,api_count,code
generate.py,0,"b'from rnn import RNN\nfrom tokens import tokenize, detokenize\n\nmodel = RNN(61,200)\nmodel.load(\'uwv.pkl\')\n\n# print(model.U.shape)\n# generate(start_token, num_of_chars, top_k random predictions)\ntext = model.generate(tokenize(\'F\'), 300, 3)\n\nfor i in text:\n    print(detokenize(i), end="""")\n'"
rnn.py,31,"b'import numpy as np\nimport sys\nfrom datetime import datetime\nimport pickle\nimport tqdm\nimport random\nfrom pathlib import Path\n\nclass Sigmoid:\n    def forward(self, x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    def backward(self, x):\n        return (1.0 - x) * x\n\nclass Softmax:\n    def forward(self, x):\n        exp_scores = np.exp(x)\n        return exp_scores / np.sum(exp_scores)\n\n    def loss(self, x, y):\n        return -np.log(x[y])\n\n    def backward(self, x, y):\n        x[y] -= 1.0\n        return x\n\n\nclass Layer:\n    \n\n    def forward(self, x, prev_s, u, w, v):\n        """"""\n        x : input array\n        prev_s : array\n        u,v,w : weight matrices\n        """"""\n        activation = Sigmoid()\n        output = Softmax()\n        self.mulu = np.matmul(x, u)\n        self.mulw = np.matmul(prev_s, w)\n        self.sin = np.add(self.mulu, self.mulw)\n        self.sout = activation.forward(self.sin)\n        self.oin = np.matmul(self.sout, v)\n        self.oout = output.forward(self.oin)\n\n    def backward(self, x, prev_s, y, u, w, v):\n        """"""\n        y : integer\n        """"""\n        activation = Sigmoid()\n        output = Softmax()\n        self.loss = output.loss(self.oout, y)\n        self.dldoi = output.backward(self.oout, y)\n        self.doidso = v\n        self.doidv = self.sout\n        self.dsodsi = activation.backward(self.sout)\n        self.dsidu = x\n        self.dsidpso = w\n        self.dsidw = prev_s\n\n\n\nclass RNN:\n    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n        self.word_dim = word_dim\n        self.hidden_dim = hidden_dim\n        self.bptt_truncate = bptt_truncate\n        self.U = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (word_dim, hidden_dim))\n        self.W = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, hidden_dim))\n        self.V = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, word_dim))\n        self.layers = []\n\n    def forward(self, x):\n        """"""\n        x : array of integers (denoting one training example i.e. a sentence)\n        """"""\n        T = len(x)\n        self.layers = []\n        prev_s = np.zeros(self.hidden_dim)\n        for t in range(T):\n            layer = Layer()\n            input = np.zeros(self.word_dim)\n            input[x[t]]=1\n            layer.forward(input, prev_s, self.U, self.W, self.V)\n            prev_s = layer.sout\n            self.layers.append(layer)\n    \n    def generate(self, seed, num=100, k=10):\n        """"""\n        seed - one character (tokenized) - integer\n        num - number of characters to generate\n        """"""\n        # print(detokenize(seed))\n        text = []\n        text.append(seed)\n        prev_s = np.zeros(self.hidden_dim)\n        for i in range(num):\n            layer = Layer()\n            input = np.zeros(self.word_dim)\n            input[seed]=1\n            layer.forward(input, prev_s, self.U, self.W, self.V)\n            prev_s = layer.sout\n            # seed = np.argmax(layer.oout)\n            # print(detokenize(seed))\n            # select randomly from 75% and above\n            temp = sorted(layer.oout, reverse=True)\n            threshold = temp[k-1]\n            top = [index for index, val in enumerate(layer.oout) if val>=threshold]\n            seed = random.choice(top)\n            # print(layer.oout[seed])\n            text.append(seed)\n        return text\n\n    def load(self, filename):\n        myfile = Path(filename)\n        if(myfile.exists):\n            print(\'Loading weights...\')\n            with open(filename,\'rb\') as f:\n                self.U, self.W, self.V = pickle.load(f)\n\n    # def total_loss(self):\n    #     loss  = 0\n    #     for layer in self.layers:\n    #         loss += layer.loss\n    #     return loss\n\n    def calculate_total_loss(self, x, y):\n        L = 0\n        # For each sentence...\n        for i in np.arange(len(y)):\n            self.forward(x[i])\n            # We only care about our prediction of the ""correct"" words\n            correct_word_predictions = [j.oout[y[i][k]] for k,j in enumerate(self.layers)]\n            # Add to the loss based on how off we were\n            L += -1 * np.sum(np.log(correct_word_predictions))\n        return L\n \n    def calculate_loss(self, x, y):\n        # Divide the total loss by the number of training examples\n        N = np.sum((len(y_i) for y_i in y))\n        return self.calculate_total_loss(x,y)/N\n\n    def calculate_grads(self, x, y):\n        for t, layer in enumerate(self.layers):\n            input = np.zeros(self.word_dim)\n            input[x[t]]=1\n            prev_s = np.zeros(self.hidden_dim)\n            layer.backward(input, prev_s, y[t], self.U, self.W, self.V)\n            prev_s = layer.sout\n\n    def backward(self, x, y):\n        """"""\n        x,y: array of integers\n        """"""\n        self.forward(x)\n        self.calculate_grads(x, y)\n        dldu = np.zeros(self.U.shape)\n        dldw = np.zeros(self.W.shape)\n        dldv = np.zeros(self.V.shape)\n        T = len(self.layers)\n        for t in np.arange(T)[::-1]:\n            dldv += np.outer(self.layers[t].doidv,self.layers[t].dldoi) # dim: [hidden_dim * word_dim]\n            delta_t = np.matmul(self.layers[t].doidso, self.layers[t].dldoi) # dEt/dSt(out) dim: [hidden_dim] \n            for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n                delta_t = delta_t * self.layers[bptt_step].dsodsi # dim: [hidden_dim]\n                dldw += np.outer(self.layers[bptt_step].dsidw, delta_t)\n                dldu += np.outer(self.layers[bptt_step].dsidu, delta_t)\n                delta_t = np.matmul(self.layers[bptt_step].dsidpso, delta_t) # dim: [hidden_dim]\n        return (dldu, dldw, dldv)\n\n    def sgd_step(self, x, y, learning_rate):\n        dU, dW, dV = self.backward(x, y)\n        self.U -= learning_rate * dU\n        self.V -= learning_rate * dV\n        self.W -= learning_rate * dW\n\n    def train(self, X, Y, learning_rate, nepoch, evaluate_loss_after):\n        num_examples_seen = 0\n        losses = []\n        print(""\\n\\n TRAINING STARTED \\n\\n"")\n        for epoch in range(nepoch):\n            if (epoch % evaluate_loss_after == 0):\n                    loss = self.calculate_loss(X,Y)\n                    losses.append((num_examples_seen, loss))\n                    time = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\')\n                    print(""%s: Loss after num_examples_seen=%d epoch=%d: %f"" % (time, num_examples_seen, epoch, loss))\n                    # Adjust the learning rate if loss increases\n                    if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n                        learning_rate = learning_rate * 0.5\n                        print(""Setting learning rate to %f"" % learning_rate)\n                    sys.stdout.flush()\n            # For each training example...\n            for i in tqdm.tqdm(range(len(Y))):\n                # print(i, )\n                # X[i] and Y[i] are one training example\n                self.sgd_step(X[i], Y[i], learning_rate)\n                num_examples_seen += 1\n            with open(\'uwv.pkl\', \'wb\') as f:  \n                pickle.dump([self.U, self.W, self.V], f)\n            f.close()\n                \n        \n        return losses\n                        \n\n\n\n        \n            \n'"
test.py,0,"b'from rnn import RNN\nfrom tokens import tokenize\n\nf = open(\'input.txt\', \'r\')\nsentences = list(filter(None, f.read().split(\'\\n\\n\')))\nf.close()\n# print((sentences))\ntokens = []\nX = []\nY = []\n# word_dim = 58\nfor i in range(len(sentences)):\n    tokens.append([])\n    for j in range(len(sentences[i])):\n        if(tokenize(sentences[i][j])!=""UNKNOWN""):\n            tokens[i].append(tokenize(sentences[i][j]))\n    X.append(tokens[i][:-1])\n    Y.append(tokens[i][1:])\n\nmodel = RNN(61, 200, 10)\nmodel.load(\'uwv.pkl\')\nmodel.train(X, Y, 0.03, 10, 1)\n\n\n\n'"
tokens.py,0,"b'def tokenize(x):\n    if(ord(x)>=ord(\'a\') and ord(x)<=ord(\'z\')):\n        return ord(x)-97\n    elif(ord(x)>=ord(\'A\') and ord(x)<=ord(\'Z\')):\n        return ord(x)-39\n    elif(x == \' \'):\n        return 52\n    elif(x == \'.\'):\n        return 53\n    elif(x == \',\'):\n        return 54\n    elif(x == ""\'""):\n        return 55\n    elif(x == \'?\'):\n        return 56\n    elif(x == \'!\'):\n        return 57\n    elif(x == \';\'):\n        return 58\n    elif(x == "":""):\n        return 59\n    elif(x == \'\\n\'):\n        return 60\n    else:\n        return ""UNKNOWN""\n\ndef detokenize(x):\n    if(x>=0 and x<=25):\n        return chr(x+97)\n    elif(x>=26 and x<=51):\n        return chr(x+39)\n    elif(x==52):\n        return \' \'\n    elif(x==53):\n        return \'.\'\n    elif(x==54):\n        return \',\'\n    elif(x==55):\n        return ""\'""\n    elif(x==56):\n        return \'?\'\n    elif(x==57):\n        return \'!\'\n    elif(x==58):\n        return \';\'\n    elif(x==59):\n        return \':\'\n    elif(x==60):\n        return \'\\n\'\n    else:\n        return ""UNKNOWN""\n\n'"
