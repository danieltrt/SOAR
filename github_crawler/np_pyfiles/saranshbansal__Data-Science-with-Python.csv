file_path,api_count,code
src/grains_data_from_dataset.py,1,"b""import csv\n\nimport numpy as np\n\nfrom helper import path\n\nwith open('../' + path + 'seeds-width-vs-length.csv', 'r') as f:\n    grains = list(csv.reader(f, delimiter=','))\n    grains = np.array(grains).astype(np.float)"""
src/helper.py,1,"b'import numpy as np\nimport pandas as pd\n\npath = \'../../_datasets/\'\n\n\ndef smart_print(data):\n    print(data)\n    print(\'-------\')\n    print(""data type: "", type(data))\n    if type(data) in [\'list\', \'tuple\', np.ndarray]:\n        print(""data ndim: "", data.ndim)\n        print(""data shape:"", data.shape)\n        print(""data size: "", data.size)\n    elif type(data) == \'dict\':\n        print(""data length: "", len(data))\n    elif type(data) == pd.DataFrame:\n        print(""data head: "", data.head())\n        print(""data shape: "", data.shape)\n        print(""data info: "", data.info(verbose=True))\n\n\npoints = [[0.06544649, -0.76866376],\n          [-1.52901547, -0.42953079],\n          [1.70993371, 0.69885253],\n          [1.16779145, 1.01262638],\n          [-1.80110088, -0.31861296],\n          [-1.63567888, -0.02859535],\n          [1.21990375, 0.74643463],\n          [-0.26175155, -0.62492939],\n          [-1.61925804, -0.47983949],\n          [-1.84329582, -0.16694431],\n          [1.35999602, 0.94995827],\n          [0.42291856, -0.7349534],\n          [-1.68576139, 0.10686728],\n          [0.90629995, 1.09105162],\n          [-1.56478322, -0.84675394],\n          [-0.0257849, -1.18672539],\n          [0.83027324, 1.14504612],\n          [1.22450432, 1.35066759],\n          [-0.15394596, -0.71704301],\n          [0.86358809, 1.06824613],\n          [-1.43386366, -0.2381297],\n          [0.03844769, -0.74635022],\n          [-1.58567922, 0.08499354],\n          [0.6359888, -0.58477698],\n          [0.24417242, -0.53172465],\n          [-2.19680359, 0.49473677],\n          [1.0323503, -0.55688],\n          [-0.28858067, -0.39972528],\n          [0.20597008, -0.80171536],\n          [-1.2107308, -0.34924109],\n          [1.33423684, 0.7721489],\n          [1.19480152, 1.04788556],\n          [0.9917477, 0.89202008],\n          [-1.8356219, -0.04839732],\n          [0.08415721, -0.71564326],\n          [-1.48970175, -0.19299604],\n          [0.38782418, -0.82060119],\n          [-0.01448044, -0.9779841],\n          [-2.0521341, -0.02129125],\n          [0.10331194, -0.82162781],\n          [-0.44189315, -0.65710974],\n          [1.10390926, 1.02481182],\n          [-1.59227759, -0.17374038],\n          [-1.47344152, -0.02202853],\n          [-1.35514704, 0.22971067],\n          [0.0412337, -1.23776622],\n          [0.4761517, -1.13672124],\n          [1.04335676, 0.82345905],\n          [-0.07961882, -0.85677394],\n          [0.87065059, 1.08052841],\n          [1.40267313, 1.07525119],\n          [0.80111157, 1.28342825],\n          [-0.16527516, -1.23583804],\n          [-0.33779221, -0.59194323],\n          [0.80610749, -0.73752159],\n          [-1.43590032, -0.56384446],\n          [0.54868895, -0.95143829],\n          [0.46803131, -0.74973907],\n          [-1.5137129, -0.83914323],\n          [0.9138436, 1.51126532],\n          [-1.97233903, -0.41155375],\n          [0.5213406, -0.88654894],\n          [0.62759494, -1.18590477],\n          [0.94163014, 1.35399335],\n          [0.56994768, 1.07036606],\n          [-1.87663382, 0.14745773],\n          [0.90612186, 0.91084011],\n          [-1.37481454, 0.28428395],\n          [-1.80564029, -0.96710574],\n          [0.34307757, -0.79999275],\n          [0.70380566, 1.00025804],\n          [-1.68489862, -0.30564595],\n          [1.31473221, 0.98614978],\n          [0.26151216, -0.26069251],\n          [0.9193121, 0.82371485],\n          [-1.21795929, -0.20219674],\n          [-0.17722723, -1.02665245],\n          [0.64824862, -0.66822881],\n          [0.41206786, -0.28783784],\n          [1.01568202, 1.13481667],\n          [0.67900254, -0.91489502],\n          [-1.05182747, -0.01062376],\n          [0.61306599, 1.78210384],\n          [-1.50219748, -0.52308922],\n          [-1.72717293, -0.46173916],\n          [-1.60995631, -0.1821007],\n          [-1.09111021, -0.0781398],\n          [-0.01046978, -0.80913034],\n          [0.32782303, -0.80734754],\n          [1.22038503, 1.1959793],\n          [-1.33328681, -0.30001937],\n          [0.87959517, 1.11566491],\n          [-1.14829098, -0.30400762],\n          [-0.58019755, -1.19996018],\n          [-0.01161159, -0.78468854],\n          [0.17359724, -0.63398145],\n          [1.32738556, 0.67759969],\n          [-1.93467327, 0.30572472],\n          [-1.57761893, -0.27726365],\n          [0.47639, 1.21422648],\n          [-1.65237509, -0.6803981],\n          [-0.12609976, -1.04327457],\n          [-1.89607082, -0.70085502],\n          [0.57466899, 0.74878369],\n          [-0.16660312, -0.83110295],\n          [0.8013355, 1.22244435],\n          [1.18455426, 1.4346467],\n          [1.08864428, 0.64667112],\n          [-1.61158505, 0.22805725],\n          [-1.57512205, -0.09612576],\n          [0.0721357, -0.69640328],\n          [-1.40054298, 0.16390598],\n          [1.09607713, 1.16804691],\n          [-2.54346204, -0.23089822],\n          [-1.34544875, 0.25151126],\n          [-1.35478629, -0.19103317],\n          [0.18368113, -1.15827725],\n          [-1.31368677, -0.376357],\n          [0.09990129, 1.22500491],\n          [1.17225574, 1.30835143],\n          [0.0865397, -0.79714371],\n          [-0.21053923, -1.13421511],\n          [0.26496024, -0.94760742],\n          [-0.2557591, -1.06266022],\n          [-0.26039757, -0.74774225],\n          [-1.91787359, 0.16434571],\n          [0.93021139, 0.49436331],\n          [0.44770467, -0.72877918],\n          [-1.63802869, -0.58925528],\n          [-1.95712763, -0.10125137],\n          [0.9270337, 0.88251423],\n          [1.25660093, 0.60828073],\n          [-1.72818632, 0.08416887],\n          [0.3499788, -0.30490298],\n          [-1.51696082, -0.50913109],\n          [0.18763605, -0.55424924],\n          [0.89609809, 0.83551508],\n          [-1.54968857, -0.17114782],\n          [1.2157457, 1.23317728],\n          [0.20307745, -1.03784906],\n          [0.84589086, 1.03615273],\n          [0.53237919, 1.47362884],\n          [-0.05319044, -1.36150553],\n          [1.38819743, 1.11729915],\n          [1.00696304, 1.0367721],\n          [0.56681869, -1.09637176],\n          [0.86888296, 1.05248874],\n          [-1.16286609, -0.55875245],\n          [0.27717768, -0.83844015],\n          [0.16563267, -0.80306607],\n          [0.38263303, -0.42683241],\n          [1.14519807, 0.89659026],\n          [0.81455857, 0.67533667],\n          [-1.8603152, -0.09537561],\n          [0.965641, 0.90295579],\n          [-1.49897451, -0.33254044],\n          [-0.1335489, -0.80727582],\n          [0.12541527, -1.13354906],\n          [1.06062436, 1.28816358],\n          [-1.49154578, -0.2024641],\n          [1.16189032, 1.28819877],\n          [0.54282033, 0.75203524],\n          [0.89221065, 0.99211624],\n          [-1.49932011, -0.32430667],\n          [0.3166647, -1.34482915],\n          [0.13972469, -1.22097448],\n          [-1.5499724, -0.10782584],\n          [1.23846858, 1.37668804],\n          [1.25558954, 0.72026098],\n          [0.25558689, -1.28529763],\n          [0.45168933, -0.55952093],\n          [1.06202057, 1.03404604],\n          [0.67451908, -0.54970299],\n          [0.22759676, -1.02729468],\n          [-1.45835281, -0.04951074],\n          [0.23273501, -0.70849262],\n          [1.59679589, 1.11395076],\n          [0.80476105, 0.544627],\n          [1.15492521, 1.04352191],\n          [0.59632776, -1.19142897],\n          [0.02839068, -0.43829366],\n          [1.13451584, 0.5632633],\n          [0.21576204, -1.04445753],\n          [1.41048987, 1.02830719],\n          [1.12289302, 0.58029441],\n          [0.25200688, -0.82588436],\n          [-1.28566081, -0.07390909],\n          [1.52849815, 1.11822469],\n          [-0.23907858, -0.70541972],\n          [-0.25792784, -0.81825035],\n          [0.59367818, -0.45239915],\n          [0.07931909, -0.29233213],\n          [-1.27256815, 0.11630577],\n          [0.66930129, 1.00731481],\n          [0.34791546, -1.20822877],\n          [-2.11283993, -0.66897935],\n          [-1.6293824, -0.32718222],\n          [-1.53819139, -0.01501972],\n          [-0.11988545, -0.6036339],\n          [-1.54418956, -0.30389844],\n          [0.30026614, -0.77723173],\n          [0.00935449, -0.53888192],\n          [-1.33424393, -0.11560431],\n          [0.47504489, 0.78421384],\n          [0.59313264, 1.232239],\n          [0.41370369, -1.35205857],\n          [0.55840948, 0.78831053],\n          [0.49855018, -0.789949],\n          [0.35675809, -0.81038693],\n          [-1.86197825, -0.59071305],\n          [-1.61977671, -0.16076687],\n          [0.80779295, -0.73311294],\n          [1.62745775, 0.62787163],\n          [-1.56993593, -0.08467567],\n          [1.02558561, 0.89383302],\n          [0.24293461, -0.6088253],\n          [1.23130242, 1.00262186],\n          [-1.9651013, -0.15886289],\n          [0.42795032, -0.70384432],\n          [-1.58306818, -0.19431923],\n          [-1.57195922, 0.01413469],\n          [-0.98145373, 0.06132285],\n          [-1.48637844, -0.5746531],\n          [0.98745828, 0.69188053],\n          [1.28619721, 1.28128821],\n          [0.85850596, 0.95541481],\n          [0.19028286, -0.82112942],\n          [0.26561046, -0.04255239],\n          [-1.61897897, 0.00862372],\n          [0.24070183, -0.52664209],\n          [1.15220993, 0.43916694],\n          [-1.21967812, -0.2580313],\n          [0.33412533, -0.86117761],\n          [0.17131003, -0.75638965],\n          [-1.19828397, -0.73744665],\n          [-0.12245932, -0.45648879],\n          [1.51200698, 0.88825741],\n          [1.10338866, 0.92347479],\n          [1.30972095, 0.59066989],\n          [0.19964876, 1.14855889],\n          [0.81460515, 0.84538972],\n          [-1.6422739, -0.42296206],\n          [0.01224351, -0.21247816],\n          [0.33709102, -0.74618065],\n          [0.47301054, 0.72712075],\n          [0.34706626, 1.23033757],\n          [-0.00393279, -0.97209694],\n          [-1.64303119, 0.05276337],\n          [1.44649625, 1.14217033],\n          [-1.93030087, -0.40026146],\n          [-2.37296135, -0.72633645],\n          [0.45860122, -1.06048953],\n          [0.4896361, -1.18928313],\n          [-1.02335902, -0.17520578],\n          [-1.32761107, -0.93963549],\n          [-1.50987909, -0.09473658],\n          [0.02723057, -0.79870549],\n          [1.0169412, 1.26461701],\n          [0.47733527, -0.9898471],\n          [-1.27784224, -0.547416],\n          [0.49898802, -0.6237259],\n          [1.06004731, 0.86870008],\n          [1.00207501, 1.38293512],\n          [1.31161394, 0.62833956],\n          [1.13428443, 1.18346542],\n          [1.27671346, 0.96632878],\n          [-0.63342885, -0.97768251],\n          [0.12698779, -0.93142317],\n          [-1.34510812, -0.23754226],\n          [-0.53162278, -1.25153594],\n          [0.21959934, -0.90269938],\n          [-1.78997479, -0.12115748],\n          [1.23197473, -0.07453764],\n          [1.4163536, 1.21551752],\n          [-1.90280976, -0.1638976],\n          [-0.22440081, -0.75454248],\n          [0.59559412, 0.92414553],\n          [1.21930773, 1.08175284],\n          [-1.99427535, -0.37587799],\n          [-1.27818474, -0.52454551],\n          [0.62352689, -1.01430108],\n          [0.14024251, -0.428266],\n          [-0.16145713, -1.16359731],\n          [-1.74795865, -0.06033101],\n          [-1.16659791, 0.0902393],\n          [0.41110408, -0.8084249],\n          [1.14757168, 0.77804528],\n          [-1.65590748, -0.40105446],\n          [-1.15306865, 0.00858699],\n          [0.60892121, 0.68974833],\n          [-0.08434138, -0.97615256],\n          [0.19170053, -0.42331438],\n          [0.29663162, -1.13357399],\n          [-1.36893628, -0.25052124],\n          [-0.08037807, -0.56784155],\n          [0.35695011, -1.15064408],\n          [0.02482179, -0.63594828],\n          [-1.49075558, -0.2482507],\n          [-1.408588, 0.25635431],\n          [-1.98274626, -0.54584475]]\n\nnew_points = [[4.00233332e-01, -1.26544471e+00],\n              [8.03230370e-01, 1.28260167e+00],\n              [-1.39507552e+00, 5.57292921e-02],\n              [-3.41192677e-01, -1.07661994e+00],\n              [1.54781747e+00, 1.40250049e+00],\n              [2.45032018e-01, -4.83442328e-01],\n              [1.20706886e+00, 8.88752605e-01],\n              [1.25132628e+00, 1.15555395e+00],\n              [1.81004415e+00, 9.65530731e-01],\n              [-1.66963401e+00, -3.08103509e-01],\n              [-7.17482105e-02, -9.37939700e-01],\n              [6.82631927e-01, 1.10258160e+00],\n              [1.09039598e+00, 1.43899529e+00],\n              [-1.67645414e+00, -5.04557049e-01],\n              [-1.84447804e+00, 4.52539544e-02],\n              [1.24234851e+00, 1.02088661e+00],\n              [-1.86147041e+00, 6.38645811e-03],\n              [-1.46044943e+00, 1.53252383e-01],\n              [4.98981817e-01, 8.98006058e-01],\n              [9.83962244e-01, 1.04369375e+00],\n              [-1.83136742e+00, -1.63632835e-01],\n              [1.30622617e+00, 1.07658717e+00],\n              [3.53420328e-01, -7.51320218e-01],\n              [1.13957970e+00, 1.54503860e+00],\n              [2.93995694e-01, -1.26135005e+00],\n              [-1.14558225e+00, -3.78709636e-02],\n              [1.18716105e+00, 6.00240663e-01],\n              [-2.23211946e+00, 2.30475094e-01],\n              [-1.28320430e+00, -3.93314568e-01],\n              [4.94296696e-01, -8.83972009e-01],\n              [6.31834930e-02, -9.11952228e-01],\n              [9.35759539e-01, 8.66820685e-01],\n              [1.58014721e+00, 1.03788392e+00],\n              [1.06304960e+00, 1.02706082e+00],\n              [-1.39732536e+00, -5.05162249e-01],\n              [-1.09935240e-01, -9.08113619e-01],\n              [1.17346758e+00, 9.47501092e-01],\n              [9.20084511e-01, 1.45767672e+00],\n              [5.82658956e-01, -9.00086832e-01],\n              [9.52772328e-01, 8.99042386e-01],\n              [-1.37266956e+00, -3.17878215e-02],\n              [2.12706760e-02, -7.07614194e-01],\n              [3.27049052e-01, -5.55998107e-01],\n              [-1.71590267e+00, 2.15222266e-01],\n              [5.12516209e-01, -7.60128245e-01],\n              [1.13023469e+00, 7.22451122e-01],\n              [-1.43074310e+00, -3.42787511e-01],\n              [-1.82724625e+00, 1.17657775e-01],\n              [1.41801350e+00, 1.11455080e+00],\n              [1.26897304e+00, 1.41925971e+00],\n              [8.04076494e-01, 1.63988557e+00],\n              [8.34567752e-01, 1.09956689e+00],\n              [-1.24714732e+00, -2.23522320e-01],\n              [-1.29422537e+00, 8.18770024e-02],\n              [-2.27378316e-01, -4.13331387e-01],\n              [2.18830387e-01, -4.68183120e-01],\n              [-1.22593414e+00, 2.55599147e-01],\n              [-1.31294033e+00, -4.28892070e-01],\n              [-1.33532382e+00, 6.52053776e-01],\n              [-3.01100233e-01, -1.25156451e+00],\n              [2.02778356e-01, -9.05277445e-01],\n              [1.01357784e+00, 1.12378981e+00],\n              [8.18324394e-01, 8.60841257e-01],\n              [1.26181556e+00, 1.46613744e+00],\n              [4.64867724e-01, -7.97212459e-01],\n              [3.60908898e-01, 8.44106720e-01],\n              [-2.15098310e+00, -3.69583937e-01],\n              [1.05005281e+00, 8.74181364e-01],\n              [1.06580074e-01, -7.49268153e-01],\n              [-1.73945723e+00, 2.52183577e-01],\n              [-1.12017687e-01, -6.52469788e-01],\n              [5.16618951e-01, -6.41267582e-01],\n              [3.26621787e-01, -8.80608015e-01],\n              [1.09017759e+00, 1.10952558e+00],\n              [3.64459576e-01, -6.94215622e-01],\n              [-1.90779318e+00, 1.87383674e-01],\n              [-1.95601829e+00, 1.39959126e-01],\n              [3.18541701e-01, -4.05271704e-01],\n              [7.36512699e-01, 1.76416255e+00],\n              [-1.44175162e+00, -5.72320429e-02],\n              [3.21757168e-01, -5.34283821e-01],\n              [-1.37317305e+00, 4.64484644e-02],\n              [6.87225910e-02, -1.10522944e+00],\n              [9.59314218e-01, 6.52316210e-01],\n              [-1.62641919e+00, -5.62423280e-01],\n              [1.06788305e+00, 7.29260482e-01],\n              [-1.79643547e+00, -9.88307418e-01],\n              [-9.88628377e-02, -6.81198092e-02],\n              [-1.05135700e-01, 1.17022143e+00],\n              [8.79964699e-01, 1.25340317e+00],\n              [9.80753407e-01, 1.15486539e+00],\n              [-8.33224966e-02, -9.24844368e-01],\n              [8.48759673e-01, 1.09397425e+00],\n              [1.32941649e+00, 1.13734563e+00],\n              [3.23788068e-01, -7.49732451e-01],\n              [-1.52610970e+00, -2.49016929e-01],\n              [-1.48598116e+00, -2.68828608e-01],\n              [-1.80479553e+00, 1.87052700e-01],\n              [-2.01907347e+00, -4.49511651e-01],\n              [2.87202402e-01, -6.55487415e-01],\n              [8.22295102e-01, 1.38443234e+00],\n              [-3.56997036e-02, -8.01825807e-01],\n              [-1.66955440e+00, -1.38258505e-01],\n              [-1.78226821e+00, 2.93353033e-01],\n              [7.25837138e-01, -6.23374024e-01],\n              [3.88432593e-01, -7.61283497e-01],\n              [1.49002783e+00, 7.95678671e-01],\n              [6.55423228e-04, -7.40580702e-01],\n              [-1.34533116e+00, -4.75629937e-01],\n              [-8.03845106e-01, -3.09943013e-01],\n              [-2.49041295e-01, -1.00662418e+00],\n              [-1.41095118e+00, -7.06744127e-02],\n              [-1.75119594e+00, -3.00491336e-01],\n              [-1.27942724e+00, 1.73774600e-01],\n              [3.35028183e-01, 6.24761151e-01],\n              [1.16819649e+00, 1.18902251e+00],\n              [7.15210457e-01, 9.26077419e-01],\n              [1.30057278e+00, 9.16349565e-01],\n              [-1.21697008e+00, 1.10039477e-01],\n              [-1.70707935e+00, -5.99659536e-02],\n              [1.20730655e+00, 1.05480463e+00],\n              [1.86896009e-01, -9.58047234e-01],\n              [8.03463471e-01, 3.86133140e-01],\n              [-1.73486790e+00, -1.49831913e-01],\n              [1.31261499e+00, 1.11802982e+00],\n              [4.04993148e-01, -5.10900347e-01],\n              [-1.93267968e+00, 2.20764694e-01],\n              [6.56004799e-01, 9.61887161e-01],\n              [-1.40588215e+00, 1.17134403e-01],\n              [-1.74306264e+00, -7.47473959e-02],\n              [5.43745412e-01, 1.47209224e+00],\n              [-1.97331669e+00, -2.27124493e-01],\n              [1.53901171e+00, 1.36049081e+00],\n              [-1.48323452e+00, -4.90302063e-01],\n              [3.86748484e-01, -1.26173400e+00],\n              [1.17015716e+00, 1.18549415e+00],\n              [-8.05381721e-02, -3.21923627e-01],\n              [-6.82273156e-02, -8.52825887e-01],\n              [7.13500028e-01, 1.27868520e+00],\n              [-1.85014378e+00, -5.03490558e-01],\n              [6.36085266e-02, -1.41257040e+00],\n              [1.52966062e+00, 9.66056572e-01],\n              [1.62165714e-01, -1.37374843e+00],\n              [-3.23474497e-01, -7.06620269e-01],\n              [-1.51768993e+00, 1.87658302e-01],\n              [8.88895911e-01, 7.62237161e-01],\n              [4.83164032e-01, 8.81931869e-01],\n              [-5.52997766e-02, -7.11305016e-01],\n              [-1.57966441e+00, -6.29220313e-01],\n              [5.51308645e-02, -8.47206763e-01],\n              [-2.06001582e+00, 5.87697787e-02],\n              [1.11810855e+00, 1.30254175e+00],\n              [4.87016164e-01, -9.90143937e-01],\n              [-1.65518042e+00, -1.69386383e-01],\n              [-1.44349738e+00, 1.90299243e-01],\n              [-1.70074547e-01, -8.26736022e-01],\n              [-1.82433979e+00, -3.07814626e-01],\n              [1.03093485e+00, 1.26457691e+00],\n              [1.64431169e+00, 1.27773115e+00],\n              [-1.47617693e+00, 2.60783872e-02],\n              [1.00953067e+00, 1.14270181e+00],\n              [-1.45285636e+00, -2.55216207e-01],\n              [-1.74092917e+00, -8.34443177e-02],\n              [1.22038299e+00, 1.28699961e+00],\n              [9.16925397e-01, 7.32070275e-01],\n              [-1.60754185e-03, -7.26375571e-01],\n              [8.93841238e-01, 8.41146643e-01],\n              [6.33791961e-01, 1.00915134e+00],\n              [-1.47927075e+00, -6.99781936e-01],\n              [5.44799374e-02, -1.06441970e+00],\n              [-1.51935568e+00, -4.89276929e-01],\n              [2.89939026e-01, -7.73145523e-01],\n              [-9.68154061e-03, -1.13302207e+00],\n              [1.13474639e+00, 9.71541744e-01],\n              [5.36421406e-01, -8.47906388e-01],\n              [1.14759864e+00, 6.89915205e-01],\n              [5.73291902e-01, 7.90802710e-01],\n              [2.12377397e-01, -6.07569808e-01],\n              [5.26579548e-01, -8.15930264e-01],\n              [-2.01831641e+00, 6.78650740e-02],\n              [-2.35512624e-01, -1.08205132e+00],\n              [1.59274780e-01, -6.00717261e-01],\n              [2.28120356e-01, -1.16003549e+00],\n              [-1.53658378e+00, 8.40798808e-02],\n              [1.13954609e+00, 6.31782001e-01],\n              [1.01119255e+00, 1.04360805e+00],\n              [-1.42039867e-01, -4.81230337e-01],\n              [-2.23120182e+00, 8.49162905e-02],\n              [1.25554811e-01, -1.01794793e+00],\n              [-1.72493509e+00, -6.94426177e-01],\n              [-1.60434630e+00, 4.45550868e-01],\n              [7.37153979e-01, 9.26560744e-01],\n              [6.72905271e-01, 1.13366030e+00],\n              [1.20066456e+00, 7.26273093e-01],\n              [7.58747209e-02, -9.83378326e-01],\n              [1.28783262e+00, 1.18088601e+00],\n              [1.06521930e+00, 1.00714746e+00],\n              [1.05871698e+00, 1.12956519e+00],\n              [-1.12643410e+00, 1.66787744e-01],\n              [-1.10157218e+00, -3.64137806e-01],\n              [2.35118217e-01, -1.39769949e-01],\n              [1.13853795e+00, 1.01018519e+00],\n              [5.31205654e-01, -8.81990792e-01],\n              [4.33085936e-01, -7.64059042e-01],\n              [-4.48926156e-03, -1.30548411e+00],\n              [-1.76348589e+00, -4.97430739e-01],\n              [1.36485681e+00, 5.83404699e-01],\n              [5.66923900e-01, 1.51391963e+00],\n              [1.35736826e+00, 6.70915318e-01],\n              [1.07173397e+00, 6.11990884e-01],\n              [1.00106915e+00, 8.93815326e-01],\n              [1.33091007e+00, 8.79773879e-01],\n              [-1.79603740e+00, -3.53883973e-02],\n              [-1.27222979e+00, 4.00156642e-01],\n              [8.47480603e-01, 1.17032364e+00],\n              [-1.50989129e+00, -7.12318330e-01],\n              [-1.24953576e+00, -5.57859730e-01],\n              [-1.27717973e+00, -5.99350550e-01],\n              [-1.81946743e+00, 7.37057673e-01],\n              [1.19949867e+00, 1.56969386e+00],\n              [-1.25543847e+00, -2.33892826e-01],\n              [-1.63052058e+00, 1.61455865e-01],\n              [1.10611305e+00, 7.39698224e-01],\n              [6.70193192e-01, 8.70567001e-01],\n              [3.69670156e-01, -6.94645306e-01],\n              [-1.26362293e+00, -6.99249285e-01],\n              [-3.66687507e-01, -1.35310260e+00],\n              [2.44032147e-01, -6.59470793e-01],\n              [-1.27679142e+00, -4.85453412e-01],\n              [3.77473612e-02, -6.99251605e-01],\n              [-2.19148539e+00, -4.91199500e-01],\n              [-2.93277777e-01, -5.89488212e-01],\n              [-1.65737397e+00, -2.98337786e-01],\n              [7.36638861e-01, 5.78037057e-01],\n              [1.13709081e+00, 1.30119754e+00],\n              [-1.44146601e+00, 3.13934680e-02],\n              [5.92360708e-01, 1.22545114e+00],\n              [6.51719414e-01, 4.92674894e-01],\n              [5.94559139e-01, 8.25637315e-01],\n              [-1.87900722e+00, -5.21899626e-01],\n              [2.15225041e-01, -1.28269851e+00],\n              [4.99145965e-01, -6.70268634e-01],\n              [-1.82954176e+00, -3.39269731e-01],\n              [7.92721403e-01, 1.33785606e+00],\n              [9.54363372e-01, 9.80396626e-01],\n              [-1.35359846e+00, 1.03976340e-01],\n              [1.05595062e+00, 8.07031927e-01],\n              [-1.94311010e+00, -1.18976964e-01],\n              [-1.39604137e+00, -3.10095976e-01],\n              [1.28977624e+00, 1.01753365e+00],\n              [-1.59503139e+00, -5.40574609e-01],\n              [-1.41994046e+00, -3.81032569e-01],\n              [-2.35569801e-02, -1.10133702e+00],\n              [-1.26038568e+00, -6.93273886e-01],\n              [9.60215981e-01, -8.11553694e-01],\n              [5.51803308e-01, -1.01793176e+00],\n              [3.70185085e-01, -1.06885468e+00],\n              [8.25529207e-01, 8.77007060e-01],\n              [-1.87032595e+00, 2.87507199e-01],\n              [-1.56260769e+00, -1.89196712e-01],\n              [-1.26346548e+00, -7.74725237e-01],\n              [-6.33800421e-02, -7.59400611e-01],\n              [8.85298280e-01, 8.85620519e-01],\n              [-1.43324686e-01, -1.16083678e+00],\n              [-1.83908725e+00, -3.26655515e-01],\n              [2.74709229e-01, -1.04546829e+00],\n              [-1.45703573e+00, -2.91842036e-01],\n              [-1.59048842e+00, 1.66063031e-01],\n              [9.25549284e-01, 7.41406406e-01],\n              [1.97245469e-01, -7.80703225e-01],\n              [2.88401697e-01, -8.32425551e-01],\n              [7.24141618e-01, -7.99149200e-01],\n              [-1.62658639e+00, -1.80005543e-01],\n              [5.84481588e-01, 1.13195640e+00],\n              [1.02146732e+00, 4.59657799e-01],\n              [8.65050554e-01, 9.57714887e-01],\n              [3.98717766e-01, -1.24273147e+00],\n              [8.62234892e-01, 1.10955561e+00],\n              [-1.35999430e+00, 2.49942654e-02],\n              [-1.19178505e+00, -3.82946323e-02],\n              [1.29392424e+00, 1.10320509e+00],\n              [1.25679630e+00, -7.79857582e-01],\n              [9.38040302e-02, -5.53247258e-01],\n              [-1.73512175e+00, -9.76271667e-02],\n              [2.23153587e-01, -9.43474351e-01],\n              [4.01989100e-01, -1.10963051e+00],\n              [-1.42244158e+00, 1.81914703e-01],\n              [3.92476267e-01, -8.78426277e-01],\n              [1.25181875e+00, 6.93614996e-01],\n              [1.77481317e-02, -7.20304235e-01],\n              [-1.87752521e+00, -2.63870424e-01],\n              [-1.58063602e+00, -5.50456344e-01],\n              [-1.59589493e+00, -1.53932892e-01],\n              [-1.01829770e+00, 3.88542370e-02],\n              [1.24819659e+00, 6.60041803e-01],\n              [-1.25551377e+00, -2.96172009e-02],\n              [-1.41864559e+00, -3.58230179e-01],\n              [5.25758326e-01, 8.70500543e-01],\n              [5.55599988e-01, 1.18765072e+00],\n              [2.81344439e-02, -6.99111314e-01]]\n\nscaled_samples = [[-0.50109735, -0.36878558, -0.34323399, -0.23781518, 1.0032125,\n                   0.25373964],\n                  [-0.37434344, -0.29750241, -0.26893461, -0.14634781, 1.15869615,\n                   0.44376493],\n                  [-0.24230812, -0.30641281, -0.25242364, -0.15397009, 1.13926069,\n                   1.0613471],\n                  [-0.18157187, -0.09256329, -0.04603648, 0.02896467, 0.96434159,\n                   0.20623332],\n                  [-0.00464454, -0.0747425, -0.04603648, 0.06707608, 0.8282934,\n                   1.0613471],\n                  [0.04816959, -0.04801131, 0.01175193, 0.12043205, 1.08095432,\n                   0.63379021],\n                  [0.18020491, -0.04801131, 0.01175193, 0.10518748, 1.26559115,\n                   1.15635974],\n                  [-0.11027279, 0.02327186, 0.03651839, 0.14329889, 0.78942248,\n                   0.25373964],\n                  [0.04816959, 0.02327186, 0.03651839, 0.15092117, 1.14897842,\n                   0.44376493],\n                  [0.18020491, 0.10346543, 0.09430679, 0.23476627, 1.09067205,\n                   0.39625861],\n                  [0.11418725, 0.09455503, 0.11907325, 0.23476627, 1.10038978,\n                   0.58628389],\n                  [0.18020491, 0.12128622, 0.11907325, 0.23476627, 1.12954296,\n                   0.20623332],\n                  [0.18020491, 0.1569278, 0.16035069, 0.25001083, 0.94490613,\n                   -0.41134885],\n                  [0.44427556, 0.18365899, 0.20162812, 0.31098909, 1.1781316,\n                   0.49127125],\n                  [0.44427556, 0.18365899, 0.20162812, 0.31098909, 1.30446206,\n                   1.01384078],\n                  [0.7083462, 0.27276296, 0.28418298, 0.39483418, 1.04208341,\n                   0.44376493],\n                  [0.7083462, 0.27276296, 0.28418298, 0.41007875, 1.04208341,\n                   0.30124596],\n                  [0.47068262, 0.31731494, 0.32546042, 0.41770103, 1.20728478,\n                   0.20623332],\n                  [0.57631088, 0.32622533, 0.32546042, 0.42532331, 0.90603522,\n                   0.91882813],\n                  [0.3782579, 0.35295652, 0.36673785, 0.48630156, 0.99349477,\n                   0.58628389],\n                  [0.66873561, 0.36186692, 0.36673785, 0.46343472, 1.23643797,\n                   0.39625861],\n                  [0.49708969, 0.37077732, 0.40801528, 0.50154612, 1.07123659,\n                   0.20623332],\n                  [0.65553208, 0.3975085, 0.44929271, 0.57014666, 0.97405931,\n                   1.0613471],\n                  [0.7083462, 0.4064189, 0.44929271, 0.56252438, 1.16841387,\n                   0.44376493],\n                  [0.77436387, 0.3975085, 0.44929271, 0.5930135, 1.15869615,\n                   0.91882813],\n                  [0.76116033, 0.4153293, 0.44929271, 0.57014666, 1.18784933,\n                   1.01384078],\n                  [0.74531609, 0.47770207, 0.53184758, 0.63874719, 1.13926069,\n                   0.58628389],\n                  [1.10445217, 0.48661247, 0.53184758, 0.64636947, 1.21700251,\n                   0.96633446],\n                  [1.50055814, 0.54898524, 0.61440245, 0.72259229, 1.5959939,\n                   1.25137238],\n                  [1.28930162, 0.68264119, 0.73823474, 0.83692651, 1.2461557,\n                   0.68129653],\n                  [1.38172635, 0.68264119, 0.73823474, 0.82930423, 1.26559115,\n                   0.68129653],\n                  [1.30250516, 0.78956594, 0.82078961, 0.92839389, 1.29474434,\n                   0.96633446],\n                  [1.43454048, 0.8964907, 0.94462191, 0.97412758, 1.21700251,\n                   0.87132181],\n                  [1.36852282, 0.94995308, 0.94462191, 1.01986127, 0.95462386,\n                   0.39625861],\n                  [-1.03452005, -1.2865564, -1.27610397, -1.28969003, -0.24065667,\n                   0.53877757],\n                  [-0.95793956, -0.96578213, -0.93762902, -0.97717649, -0.19206803,\n                   0.49127125],\n                  [-0.93417321, -0.87667817, -0.8880961, -0.90857596, -0.17263258,\n                   0.39625861],\n                  [-0.91040685, -0.8143054, -0.80554124, -0.83235314, -0.26980986,\n                   0.68129653],\n                  [-0.82326354, -0.77866381, -0.78903027, -0.83235314, -0.0074312,\n                   1.5364103],\n                  [-1.14014831, -0.74302223, -0.74775283, -0.78661945, 0.03143971,\n                   0.87132181],\n                  [-0.8496706, -0.73411183, -0.72298637, -0.76375261, -0.13376167,\n                   0.87132181],\n                  [-0.82326354, -0.70738064, -0.7064754, -0.71801892, -0.22122122,\n                   0.49127125],\n                  [-0.74404234, -0.61827668, -0.62392054, -0.6417961, -0.44472896,\n                   1.10885342],\n                  [-0.75724587, -0.60936628, -0.62392054, -0.67228523, -0.0754553,\n                   0.82381549],\n                  [-0.71763528, -0.60936628, -0.5826431, -0.59606241, -0.02686666,\n                   1.0613471],\n                  [-0.77044941, -0.5648143, -0.5826431, -0.61892926, -0.18235031,\n                   0.20623332],\n                  [-0.71763528, -0.5559039, -0.5826431, -0.61892926, -0.24065667,\n                   1.10885342],\n                  [-0.69386892, -0.47571034, -0.4588108, -0.45123907, -0.03658439,\n                   0.58628389],\n                  [-0.71499457, -0.47571034, -0.50834372, -0.48935047, -0.21150349,\n                   0.34875228],\n                  [-0.61200702, -0.46679994, -0.50008824, -0.48172819, -0.04630212,\n                   1.20386606],\n                  [-0.66482115, -0.33314399, -0.35974497, -0.39788309, -0.26009213,\n                   0.53877757],\n                  [-0.37434344, -0.29750241, -0.29370107, -0.29879344, 0.22579427,\n                   1.20386606],\n                  [-0.42187616, -0.20839845, -0.21114621, -0.19208149, -0.0074312,\n                   1.2988787],\n                  [-0.11027279, 0.19256939, 0.17686166, 0.14329889, -0.09489075,\n                   1.15635974],\n                  [-1.12245558, -1.60733067, -1.63108989, -1.70129323, -1.16384082,\n                   -1.50399423],\n                  [-1.12034301, -1.5449579, -1.57330149, -1.64031498, -1.07638127,\n                   -1.36147526],\n                  [-1.12166336, -1.5360475, -1.565046, -1.64031498, -1.28045356,\n                   -1.40898159],\n                  [-1.11453346, -1.50931631, -1.53202405, -1.60982586, -0.95005081,\n                   -0.64888045],\n                  [-1.11426939, -1.48258512, -1.51551308, -1.57933673, -1.09581673,\n                   -1.2189563],\n                  [-1.11717416, -1.47367473, -1.5072576, -1.56409217, -1.20271174,\n                   -1.26646262],\n                  [-1.11374125, -1.42912274, -1.46598016, -1.52598076, -1.086099,\n                   -1.45648791],\n                  [-1.11400532, -1.42912274, -1.46598016, -1.52598076, -1.086099,\n                   -1.88404479],\n                  [-1.11426939, -1.42021235, -1.44946919, -1.51835848, -1.10553446,\n                   -1.97905744],\n                  [-1.10793169, -1.41130195, -1.43295822, -1.50311391, -1.21242946,\n                   -1.17144998],\n                  [-1.10476284, -1.39348116, -1.41644724, -1.49549163, -0.97920399,\n                   -1.64651319],\n                  [-1.10793169, -1.35783957, -1.36691432, -1.47262479, -1.12496991,\n                   -1.78903215],\n                  [-1.08812639, -1.25982521, -1.259593, -1.36591285, -0.89174444,\n                   0.34875228],\n                  [-1.08759825, -1.20636284, -1.20180459, -1.28969003, -0.96948627,\n                   -0.60137413],\n                  [-0.61200702, 0.23712137, 0.22639458, 0.12805433, -1.17355855,\n                   -1.50399423],\n                  [-0.34793638, 0.38859811, 0.36673785, 0.35672277, -1.2610181,\n                   -0.88641206],\n                  [-0.34793638, 0.47770207, 0.44929271, 0.43294559, -1.24158265,\n                   -0.74389309],\n                  [-0.34793638, 0.6648204, 0.6391689, 0.5091684, -1.19299401,\n                   -1.31396894],\n                  [-0.00464454, 0.72719317, 0.69695731, 0.56252438, -0.97920399,\n                   -0.74389309],\n                  [-0.22910458, 0.77174515, 0.73823474, 0.60063578, -1.21242946,\n                   -1.50399423],\n                  [0.06401383, 1.128161, 1.0684542, 0.94363845, -1.17355855,\n                   -1.59900687],\n                  [0.20661198, 1.128161, 1.0684542, 0.94363845, -1.27073583,\n                   -1.45648791],\n                  [0.28583317, 1.1370714, 1.10973164, 0.9665053, -1.07638127,\n                   -0.79139942],\n                  [0.18020491, 1.30636893, 1.27484137, 1.13419549, -1.31932447,\n                   -1.26646262],\n                  [0.35713225, 1.41329369, 1.35739623, 1.18755146, -1.17355855,\n                   -1.36147526],\n                  [0.89319566, 1.55586003, 1.52250596, 1.3781085, -1.27073583,\n                   -1.12394366],\n                  [1.36852282, 1.8677239, 1.82795897, 1.67537748, -1.1541231,\n                   -0.79139942],\n                  [2.16073475, 2.19740857, 2.18294489, 2.02600243, -0.98892172,\n                   -0.55386781],\n                  [3.08498201, 2.55382442, 2.51316435, 2.35376053, -1.27073583,\n                   -1.55150055],\n                  [2.95294669, 2.55382442, 2.51316435, 2.35376053, -1.27073583,\n                   -1.55150055],\n                  [3.21701733, 2.82113631, 2.79385089, 2.65865179, -1.18327628,\n                   -0.88641206]]\n\ntitles = [\'HTTP 404\',\n          \'Alexa Internet\',\n          \'Internet Explorer\',\n          \'HTTP cookie\',\n          \'Google Search\',\n          \'Tumblr\',\n          \'Hypertext Transfer Protocol\',\n          \'Social search\',\n          \'Firefox\',\n          \'LinkedIn\',\n          \'Global warming\',\n          \'Nationally Appropriate Mitigation Action\',\n          \'Nigel Lawson\',\n          \'Connie Hedegaard\',\n          \'Climate change\',\n          \'Kyoto Protocol\',\n          \'350.org\',\n          \'Greenhouse gas emissions by the United States\',\n          \'2010 United Nations Climate Change Conference\',\n          \'2007 United Nations Climate Change Conference\',\n          \'Angelina Jolie\',\n          \'Michael Fassbender\',\n          \'Denzel Washington\',\n          \'Catherine Zeta-Jones\',\n          \'Jessica Biel\',\n          \'Russell Crowe\',\n          \'Mila Kunis\',\n          \'Dakota Fanning\',\n          \'Anne Hathaway\',\n          \'Jennifer Aniston\',\n          \'France national football team\',\n          \'Cristiano Ronaldo\',\n          \'Arsenal F.C.\',\n          \'Radamel Falcao\',\n          \'Zlatan Ibrahimovi\xc4\x87\',\n          \'Colombia national football team\',\n          \'2014 FIFA World Cup qualification\',\n          \'Football\',\n          \'Neymar\',\n          \'Franck Rib\xc3\xa9ry\',\n          \'Tonsillitis\',\n          \'Hepatitis B\',\n          \'Doxycycline\',\n          \'Leukemia\',\n          \'Gout\',\n          \'Hepatitis C\',\n          \'Prednisone\',\n          \'Fever\',\n          \'Gabapentin\',\n          \'Lymphoma\',\n          \'Chad Kroeger\',\n          \'Nate Ruess\',\n          \'The Wanted\',\n          \'Stevie Nicks\',\n          \'Arctic Monkeys\',\n          \'Black Sabbath\',\n          \'Skrillex\',\n          \'Red Hot Chili Peppers\',\n          \'Sepsis\',\n          \'Adam Levine\']\n'"
src/case_studies/case_study_1.1.py,0,"b'import pandas as pd\n\n# If you need to use real values\ndf = pd.DataFrame(pd.read_csv(\'../_datasets/WDIData_min.csv\'))\ncols = df.iloc[0, 0:5]\nrows = df.iloc[1:10, 0:5].values\n\n# Dummy subset\nfeature_names = [\'CountryName\',\n                 \'CountryCode\',\n                 \'IndicatorName\',\n                 \'IndicatorCode\',\n                 \'Year\',\n                 \'Value\']\n\nrow_val = [\'Arab World\',\n           \'ARB\',\n           \'Adolescent fertility rate (births per 1,000 women ages 15-19)\',\n           \'SP.ADO.TFRT\',\n           \'1960\',\n           \'133.56090740552298\']\n\nrow_vals = [[\'Arab World\',\n             \'ARB\',\n             \'Adolescent fertility rate (births per 1,000 women ages 15-19)\',\n             \'SP.ADO.TFRT\',\n             \'1960\',\n             \'133.56090740552298\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Age dependency ratio (% of working-age population)\',\n             \'SP.POP.DPND\',\n             \'1960\',\n             \'87.7976011532547\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Age dependency ratio, old (% of working-age population)\',\n             \'SP.POP.DPND.OL\',\n             \'1960\',\n             \'6.634579191565161\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Age dependency ratio, young (% of working-age population)\',\n             \'SP.POP.DPND.YG\',\n             \'1960\',\n             \'81.02332950839141\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Arms exports (SIPRI trend indicator values)\',\n             \'MS.MIL.XPRT.KD\',\n             \'1960\',\n             \'3000000.0\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Arms imports (SIPRI trend indicator values)\',\n             \'MS.MIL.MPRT.KD\',\n             \'1960\',\n             \'538000000.0\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Birth rate, crude (per 1,000 people)\',\n             \'SP.DYN.CBRT.IN\',\n             \'1960\',\n             \'47.697888095096395\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'CO2 emissions (kt)\',\n             \'EN.ATM.CO2E.KT\',\n             \'1960\',\n             \'59563.9892169935\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'CO2 emissions (metric tons per capita)\',\n             \'EN.ATM.CO2E.PC\',\n             \'1960\',\n             \'0.6439635478877049\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'CO2 emissions from gaseous fuel consumption (% of total)\',\n             \'EN.ATM.CO2E.GF.ZS\',\n             \'1960\',\n             \'5.041291753975099\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'CO2 emissions from liquid fuel consumption (% of total)\',\n             \'EN.ATM.CO2E.LF.ZS\',\n             \'1960\',\n             \'84.8514729446567\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'CO2 emissions from liquid fuel consumption (kt)\',\n             \'EN.ATM.CO2E.LF.KT\',\n             \'1960\',\n             \'49541.707291032304\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'CO2 emissions from solid fuel consumption (% of total)\',\n             \'EN.ATM.CO2E.SF.ZS\',\n             \'1960\',\n             \'4.72698138789597\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Death rate, crude (per 1,000 people)\',\n             \'SP.DYN.CDRT.IN\',\n             \'1960\',\n             \'19.7544519237187\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Fertility rate, total (births per woman)\',\n             \'SP.DYN.TFRT.IN\',\n             \'1960\',\n             \'6.92402738655897\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Fixed telephone subscriptions\',\n             \'IT.MLT.MAIN\',\n             \'1960\',\n             \'406833.0\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Fixed telephone subscriptions (per 100 people)\',\n             \'IT.MLT.MAIN.P2\',\n             \'1960\',\n             \'0.6167005703199\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'Hospital beds (per 1,000 people)\',\n             \'SH.MED.BEDS.ZS\',\n             \'1960\',\n             \'1.9296220724398703\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'International migrant stock (% of population)\',\n             \'SM.POP.TOTL.ZS\',\n             \'1960\',\n             \'2.9906371279862403\'],\n            [\'Arab World\',\n             \'ARB\',\n             \'International migrant stock, total\',\n             \'SM.POP.TOTL\',\n             \'1960\',\n             \'3324685.0\']]\n\n# Zip lists: zipped_lists\nzipped_lists = zip(feature_names, row_val)\n\n# Create a dictionary: rs_dict\nrs_dict = dict(zipped_lists)\n\n# Print the dictionary\nprint(\'--------- 1 --------\')\nprint(rs_dict)\n\n\n# Suppose you needed to repeat the same process done in the previous exercise to many,\n# many rows of data. Rewriting your code again and again could become very tedious, repetitive,\n# and unmaintainable.\n#\n# In this exercise, you will create a function to house the code you wrote earlier to make things\n# easier and much more concise. Why? This way, you only need to call the function and supply the appropriate\n# lists to create your dictionaries! Again, the lists feature_names and row_vals are preloaded and\n# these contain the header names of the dataset and actual values of a row from the dataset, respectively,\n\n# Define lists2dict()\ndef list2dict(list1, list2):\n    """"""Return a dictionary where list1 provides\n    the keys and list2 provides the values.""""""\n\n    # Zip lists: zipped_lists\n    zipped_lists = zip(list1, list2)\n\n    # Create a dictionary: rs_dict\n    rs_dict = dict(zipped_lists)\n\n    # Return the dictionary\n    return rs_dict\n\n\n# Call lists2dict: rs_fxn\nrs_fxn = list2dict(feature_names, row_vals)\n\n# Print rs_fxn\nprint(\'--------- 2 --------\')\nprint(rs_fxn)\n\n# Using a list comprehension\n#\n# This time, you\'re going to use the lists2dict() function you defined in the last exercise to turn a bunch of\n# lists into a list of dictionaries with the help of a list comprehension.\n#\n# The lists2dict() function has already been preloaded, together with a couple of lists, feature_names and row_lists.\n# feature_names contains the header names of the World Bank dataset and row_lists is a list of lists, where each\n# sublist is a list of actual values of a row from the dataset.\n#\n# Your goal is to use a list comprehension to generate a list of dicts, where the keys are the header names and the\n# values are the row entries.\n\n# Print the first two lists in row_vals\nprint(\'--------- 3 --------\')\nprint(row_vals[0])\nprint(row_vals[1])\n\n# Turn list of lists into list of dicts: list_of_dicts\nlist_of_dicts = [list2dict(feature_names, sublist) for sublist in row_vals]\n\n# Print the first two dictionaries in list_of_dicts\nprint(\'--------- 4 --------\')\nprint(list_of_dicts[0])\n\n# Turning this all into a DataFrame\n#\n# You\'ve zipped lists together, created a function to house your code, and even used the function in a list\n# comprehension to generate a list of dictionaries. That was a lot of work and you did a great job!\n#\n# You will now use of all these to convert the list of dictionaries into a pandas DataFrame. You will see how\n# convenient it is to generate a DataFrame from dictionaries with the DataFrame() function from the pandas package.\n#\n# The lists2dict() function, feature_names list, and row_lists list have been preloaded for this exercise.\n#\n# Go for it!\n\n# Turn list of dicts into a DataFrame: df\ndf = pd.DataFrame(list_of_dicts)\n\n# Print the head of the DataFrame\nprint(\'--------- 5 --------\')\nprint(df.head())\n'"
src/case_studies/case_study_1.2.py,0,"b'# Processing data in chunks (1)\n#\n# Sometimes, data sources can be so large in size that storing the entire dataset in memory becomes too\n# resource-intensive. In this exercise, you will process the first 1000 rows of a file line by line, to create a\n# dictionary of the counts of how many times each country appears in a column in the dataset.\n#\n# The csv file \'world_dev_ind.csv\' is in your current directory for your use. To begin, you need to open a\n# connection to this file using what is known as a context manager. For example, the command with open(\'datacamp.csv\')\n# as datacamp binds the csv file \'datacamp.csv\' as datacamp in the context manager. Here, the with statement is\n# the context manager, and its purpose is to ensure that _datasets are efficiently allocated when opening a\n# connection to a file.\n#\n# If you\'d like to learn more about context managers, refer to the DataCamp course on Importing Data in Python\n# (https://www.datacamp.com/courses/importing-data-in-python-part-1).\n\n# Open a connection to the file\nwith open(\'../_datasets/WDIData_min.csv\') as file:\n    # Skip the column names\n    file.readline()\n\n    # Initialize an empty dictionary: counts_dict\n    counts_dict = {}\n\n    # Process only the first 1000 rows\n    for j in range(0, 1000):\n\n        # Split the current line into a list: line\n        line = file.readline().split(\',\')\n\n        # Get the value for the first column: first_col\n        first_col = line[0]\n\n        # If the column value is in the dict, increment its value\n        if first_col in counts_dict.keys():\n            counts_dict[first_col] += 1\n\n        # Else, add to the dict and set value to 1\n        else:\n            counts_dict[first_col] = 1\n\n# Print the resulting dictionary\nprint(counts_dict)\n\n\n#\n# Writing a generator to load data in chunks (2)\n#\n# In the previous exercise, you processed a file line by line for a given number of lines. What if, however, you want\n# to do this for the entire file?\n#\n# In this case, it would be useful to use generators. Generators allow users to lazily evaluate data. This concept of\n# lazy evaluation is useful when you have to deal with very large datasets because it lets you generate values in an\n# efficient manner by yielding only chunks of data at a time instead of the whole thing at once.\n#\n# In this exercise, you will define a generator function read_large_file() that produces a generator object which\n# yields a single line from a file each time next() is called on it. The csv file \'world_dev_ind.csv\' is in your\n# current directory for your use.\n#\n# Note that when you open a connection to a file, the resulting file object is already a generator! So out in the\n# wild, you won\'t have to explicitly create generator objects in cases such as this. However, for pedagogical reasons,\n# we are having you practice how to do this here with the read_large_file() function. Go for it!\n# Define read_large_file()\n\ndef read_large_file(file_object):\n    """"""A generator function to read a large file lazily.""""""\n\n    # DO NOT Loop indefinitely until the end of the file else it will run out of memory\n    for i in range(0, 100):\n\n        # Read a line from the file: data\n        data = file_object.readline()\n\n        # Break if this is the end of the file\n        if not data:\n            break\n\n        # Yield the line of data\n        yield data\n\n    # Create a generator object for the file: gen_file\n    gen_file = read_large_file(file)\n\n    # Print the first three lines of the file\n    print(next(gen_file))\n    print(next(gen_file))\n    print(next(gen_file))\n\n\n# Writing a generator to load data in chunks (3)\n#\n# Great! You\'ve just created a generator function that you can use to help you process large file_operations.\n#\n# Now let\'s use your generator function to process the World Bank dataset like you did previously.\n# You will process the file line by line, to create a dictionary of the counts of how many times each country\n# appears in a column in the dataset. For this exercise, however, you won\'t process just 1000 rows of data, you\'ll\n# process the entire dataset!\n#\n# The generator function read_large_file() and the csv file \'world_dev_ind.csv\' are preloaded and ready for your use.\n#  Go for it!\n# Initialize an empty dictionary: counts_dict\ncounts_dict = {}\nwith open(\'../_datasets/WDIData_min.csv\') as file:\n    # Iterate over the generator from read_large_file()\n    for line in read_large_file(file):\n\n        row = line.split(\',\')\n        first_col = row[0]\n\n        if first_col in counts_dict.keys():\n            counts_dict[first_col] += 1\n        else:\n            counts_dict[first_col] = 1\n\n# Print\nprint(counts_dict)\n'"
src/case_studies/case_study_1.3.py,0,"b'import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Writing an iterator to load data in chunks (1)\n#\n# Another way to read data too large to store in memory in chunks is to read the file in as DataFrames of a certain\n# length, say, 100. For example, with the pandas package (imported as pd), you can do\n# pd.read_csv(filename, chunksize=100). This creates an iterable reader object, which means that you can use next()\n# on it.\n#\n# In this exercise, you will read a file in small DataFrame chunks with read_csv(). You\'re going to use the World\n# Bank Indicators data \'ind_pop_data.csv\', available in your current directory, to look at the urban population indicator\n# for numerous countries and years.\n# Import the pandas package\n\n# Initialize reader object: df_reader\ndf_reader = pd.read_csv(\'../_datasets/WDIData_min.csv\', chunksize=10)\n\n# Print two chunks\nprint(next(df_reader))\nprint(next(df_reader))\n\n# Writing an iterator to load data in chunks (2)\n#\n# In the previous exercise, you used read_csv() to read in DataFrame chunks from a large dataset. In this exercise,\n# you will read in a file using a bigger DataFrame chunk size and then process the data from the first chunk.\n#\n# To process the data, you will create another DataFrame composed of only the rows from a specific country. You will\n# then zip together two of the columns from the new DataFrame, \'Total Population\' and \'Urban population (% of\n# total)\'. Finally, you will create a list of tuples from the zip object, where each tuple is composed of a value\n# from each of the two columns mentioned.\n#\n# You\'re going to use the data from \'ind_pop_data.csv\', available in your current directory. Pandas has been imported\n#  as pd.\n\n# Initialize reader object: urb_pop_reader\nurb_pop_reader = pd.read_csv(\'../_datasets/ind_pop_data.csv\', chunksize=1000)\n\n# Get the first DataFrame chunk: df_urb_pop\ndf_urb_pop = next(urb_pop_reader)\n\n# Check out the head of the DataFrame\nprint(df_urb_pop.head())\n\n# Check out specific country: df_pop_ceb\ndf_pop_ceb = df_urb_pop[df_urb_pop[\'CountryCode\'] == \'CEB\']\n\n# Zip DataFrame columns of interest: pops\npops = zip(df_pop_ceb[\'Total Population\'], df_pop_ceb[\'Urban population (% of total)\'])\n\n# Turn zip object into list: pops_list\npops_list = list(pops)\n\n# Print pops_list\nprint(pops_list)\n\n# Writing an iterator to load data in chunks (3)\n# You\'re getting used to reading and processing data in chunks by now.\n# Let\'s push your skills a little further by adding a column to a DataFrame.\n#\n# In this exercise, you will be using a list comprehension to create the values for a new column \'Total Urban\n# Population\' from the list of tuples that you generated earlier. Recall from the previous exercise that the first\n# and second elements of each tuple consist of, respectively, values from the columns \'Total Population\' and \'Urban\n# population (% of total)\'. The values in this new column \'Total Urban Population\', therefore, are the product of the\n#  first and second element in each tuple. Furthermore, because the 2nd element is a percentage, you need to divide\n# the entire result by 100, or alternatively, multiply it by 0.01.\n#\n# You will also plot the data from this new column to create a visualization of the urban population data.\n#\n# You\'re going to use the data from \'ind_pop_data.csv\', available in your current directory. The packages pandas and\n# matplotlib.pyplot have been imported as pd and plt respectively for your use.\n\n# Initialize reader object: urb_pop_reader(see above)\n\n# Get the first DataFrame chunk: df_urb_pop(see above)\n\n# Check out specific country: df_pop_ceb(see above)\n\n# Zip DataFrame columns of interest: pops(see above)\n\n# Turn zip object into list: pops_list(see above)\n\n# Use list comprehension to create new DataFrame column \'Total Urban Population\'\ndf_pop_ceb[\'Total Urban Population\'] = [int(tup[0] * tup[1] * 0.01) for tup in pops_list]\nprint(df_pop_ceb[\'Total Urban Population\'])\n\n# Plot urban population data\ndf_pop_ceb.plot(kind=""scatter"", x=\'Year\', y=\'Total Urban Population\')\nplt.show()\n\n# Writing an iterator to load data in chunks (4)\n#\n# In the previous exercises, you\'ve only processed the data from the\n# first DataFrame chunk. This time, you will aggregate the results over all the DataFrame chunks in the dataset. This\n#  basically means you will be processing the entire dataset now. This is neat because you\'re going to be able to\n# process the entire large dataset by just working on smaller pieces of it!\n#\n# You\'re going to use the data from \'ind_pop_data.csv\', available in your current directory. The packages pandas and\n# matplotlib.pyplot have been imported as pd and plt respectively for your use.\n\n# Initialize reader object: urb_pop_reader(see above)\n\n# Initialize empty DataFrame: data\ndata = pd.DataFrame()\n\n# Iterate over each DataFrame chunk\nfor df_urb_pop in urb_pop_reader:\n    # Check out specific country: df_pop_ceb(see above)\n\n    # Zip DataFrame columns of interest: pops(see above)\n\n    # Turn zip object into list: pops_list(see above)\n\n    # Use list comprehension to create new DataFrame column \'Total Urban Population1\'(similar to above)\n    df_pop_ceb[\'Total Urban Population1\'] = [int(tup[0] * tup[1]) for tup in pops_list]\n\n    # Append DataFrame chunk to data: data\n    data = data.append(df_pop_ceb)\n\n# Plot urban population data\ndata.plot(kind=\'scatter\', x=\'Year\', y=\'Total Urban Population1\')\nplt.show()'"
src/case_studies/case_study_pipelining_and_scaling.py,1,"b'# Bringing it all together II:\n#\n# Pipeline for regression For this final exercise, you will return to the Gapminder\n# dataset. Guess what? Even this dataset has missing values that we dealt with for you in earlier chapters! Now,\n# you have all the tools to take care of them yourself!\n#\n# Your job is to build a pipeline that imputes the missing data, scales the features, and fits an ElasticNet to the\n# Gapminder data. You will then tune the l1_ratio of your ElasticNet using GridSearchCV.\n#\n# All the necessary modules have been imported, and the feature and target variable arrays have been pre-loaded as X\n# and y.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet\n\nfrom helper import path\n\n# Read \'gm_2008_region.csv\' into a DataFrame: df\ndf = pd.read_csv(path + \'gm_2008_region.csv\')\n\nX = df.drop(\'life\', axis=1)\ny = df[\'life\']\n\n# Setup the pipeline steps: steps\nsteps = [(\'imputation\', Imputer(missing_values=\'NaN\', strategy=\'mean\', axis=0)),\n         (\'scaler\', StandardScaler()),\n         (\'elasticnet\', ElasticNet())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {\'elasticnet__l1_ratio\': np.linspace(0, 1, 30)}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(pipeline, param_grid=parameters)\n\n# Fit to the training set\ngm_cv.fit(X_train, y_train)\n\n# Compute and print the metrics\nr2 = gm_cv.score(X_test, y_test)\nprint(""Tuned ElasticNet Alpha: {}"".format(gm_cv.best_params_))\nprint(""Tuned ElasticNet R squared: {}"".format(r2))\n'"
src/case_studies/case_study_trumps_twitter_RTs.py,0,"b'# Import pandas as pd\nimport pandas as pd\n\n# Import twitter data\ntweets_df = pd.DataFrame(pd.read_excel(""../_datasets/Trump Tweets(2017).xlsx""))\n\n\n# Define count_entries()\ndef count_entries(df, *args):\n    """"""Return a dictionary with counts of\n    occurrences as value for each key.""""""\n\n    # Initialize an empty dictionary: cols_count\n    cols_count = {}\n\n    # Iterate over column names in args\n    for col_name in args:\n\n        # Extract column from DataFrame: col\n        col = df[col_name]\n\n        # Iterate over the column in DataFrame\n        for entry in col:\n\n            # If entry is in cols_count, add 1\n            if entry in cols_count.keys():\n                cols_count[entry] += 1\n\n            # Else add the entry to cols_count, set the value to 1\n            else:\n                cols_count[entry] = 1\n\n    # Return the cols_count dictionary\n    return cols_count\n\n\n# Call count_entries(): result2\nresult = count_entries(tweets_df, \'Tweet\')\n\n# Filter our Retweets\nretweets = (lambda x: x[0:2] == \'RT\', tweets_df[\'Tweet\'])\n\n# Print result\n\n# print(list(result))\nfor tweet in retweets:\n    print(tweet)\n'"
src/case_studies/case_study_urban_population_trends.py,0,"b""# Case Study: Plot Urban population trends in various countries over the years based on publically available data set.\n#\n# In this case study, I have to define the function plot_pop() which takes two arguments: the filename of the file to\n# be processed, and the country code of the rows we want to process in the dataset.\n#\n# calling the function already does the following:\n#\n# Loading of the file chunk by chunk,\n# Creating the new column of urban population values,\n# and Plotting the urban population data.\n#\n# The function makes it convenient to repeat the same process for whatever file and country code we want to process\n# and visualize!\n#\n# We are using the data from 'ind_pop_data.csv', available in /_datasets/ directory.\n# The packages pandas and matplotlib.pyplot has been imported as pd and plt respectively.\n#\n# If you have enjoyed working with this data, you can continue exploring it using the pre-processed version available\n# on Kaggle.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef_file_path = '../../_datasets/'\n\n# Define plot_pop()\ndef plot_pop(filename, country_code):\n    # Initialize reader object: urb_pop_reader\n    urb_pop_reader = pd.read_csv(filename, chunksize=1000)\n\n    # Initialize empty DataFrame: data\n    data = pd.DataFrame()\n\n    # Iterate over each DataFrame chunk\n    for df_urb_pop in urb_pop_reader:\n        # Check out specific country: df_pop_ceb\n        df_pop_ceb = df_urb_pop[df_urb_pop['CountryCode'] == country_code]\n\n        # Zip DataFrame columns of interest: pops\n        pops = zip(df_pop_ceb['Total Population'],\n                   df_pop_ceb['Urban population (% of total)'])\n\n        # Turn zip object into list: pops_list\n        pops_list = list(pops)\n\n        # Use list comprehension to create new DataFrame column 'Total Urban Population'\n        df_pop_ceb['Total Urban Population'] = [int(tup[0] * tup[1]) for tup in pops_list]\n\n        # Append DataFrame chunk to data: data\n        data = data.append(df_pop_ceb)\n\n    # Plot urban population data\n    data.plot(kind='scatter', x='Year', y='Total Urban Population')\n    plt.show()\n\n\n# Set the filename: fn\nfn = 'ind_pop_data.csv'\n\n# Call plot_pop for country code 'CEB'\nplot_pop(def_file_path + fn, 'CEB')\n\n# Call plot_pop for country code 'ARB'\nplot_pop(def_file_path + fn, 'ARB')\n"""
src/case_studies/case_study_webscraping_imdb.py,2,"b'""""""\n@author: Saransh Bansal\nPurpose: Scrape top 250 movies in IMDB and visualize the frequency of these top films released in specific years\n""""""\nimport os\nimport re\nimport sys\n\nimport numpy as np\nimport requests\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nos.getcwd()  # current working directory\n\n# get the current encoding\ntype = sys.getfilesystemencoding()\n\n# request the webpage\nreq = requests.get(""https://www.imdb.com/chart/top"")\npage = req.text\n\nsoup = BeautifulSoup(page, \'html.parser\')\nprint(soup.prettify())\n\n# get top 250 movie names and years, may take ~30 seconds\nmovie_names = []\nmovie_year = [0] * 250\n\nj = 0\nfor i in range(250):\n    title = str(soup.findAll(\'td\', {\'class\': \'titleColumn\'})[i])\n    movie_names.append(re.findall(\'>(.*?)</a>\', title)[0])\n\n    year = str(soup.findAll(\'span\', {\'class\': \'secondaryInfo\'})[i])\n    movie_year[i] = int(re.findall(r""\\(([0-9_]+)\\)"", year)[0])\n\n    # keep track of the progress\n    print(\'Extracted movie :: \' + movie_names[i] + \' (\' + str(movie_year[i]) + \') \')\n    j = j + 1\n\nprint(movie_names)\nprint(movie_year)\n\n\ndef encode_title(item):\n    return str(item.encode(\'utf-8\'))\n\n\n# export to the text file\nopen(""top250names.txt"", ""w"").write(""\\n"".join(encode_title(item) for item in movie_names))\n\n# compute the frequency table\ny = np.bincount(movie_year)\nii = np.nonzero(y)[0]\nout = list(zip(ii, y[ii]))\n# create a dataframe\ndf = pd.DataFrame(out, columns=[\'Year\', \'Freq\'], index=ii)\n# drop the first Year column since I already assign valid index\ndf.drop(df.columns[0], axis=1)\n# plot\nplt.plot(ii, df[\'Freq\'])\nplt.show()\n'"
src/core/py_comprehensions.py,0,"b""# Create a list of strings: fellowship\nfellowship = ['frodo', 'samwise', 'merry', 'aragorn', 'legolas', 'boromir', 'gimli']\n\n# Create list comprehension: new_fellowship with condition in predicate expression\nnew_fellowship = [member for member in fellowship if (len(member) >= 7)]\n\n# Create list comprehension: new_fellowship with condition in predicate expression\nnew_fellowship_1 = [member if (len(member) >= 7) else '' for member in fellowship]\n\n# Print the new list\nprint(new_fellowship)\n\n# Print the new list\nprint(new_fellowship_1)\n\n# -----------------------------#\n\n# Create dict comprehension: new_fellowship_dict\nnew_fellowship_dict = {member: len(member) for member in fellowship}\n\n# Print the new list\nprint(new_fellowship_dict)\n"""
src/core/py_enumeration_example.py,0,"b""# Create a list of strings: mutants\nmutants = ['charles xavier',\n           'bobby drake',\n           'kurt wagner',\n           'max eisenhardt',\n           'kitty pride']\n\n# Create a list of tuples: mutant_list\nmutant_list = enumerate(mutants)\n\n# Print the list of tuples\nprint(list(mutant_list))\n\nprint('----------\\n')\n\n# Unpack and print the tuple pairs\nfor index1, value1 in enumerate(mutants):\n    print(index1, value1)\n\nprint('----------\\n')\n\n# Change the start index\nfor index2, value2 in enumerate(mutants, start=1):\n    print(index2, value2)\n"""
src/core/py_filter_example.py,0,"b""# Import reduce from functools\nfrom functools import reduce\n\n# Create a list of strings: stark\nstark = ['robb', 'sansa', 'arya', 'eddard', 'jon']\n\n# Use reduce() to apply a lambda function over stark: result\nresult = reduce(lambda item1, item2: item1 + item2, stark)\n\n# Print the result\nprint(result)\n"""
src/core/py_generators.py,0,"b'# Create a list of strings\nlannister = [\'cersei\', \'jaime\', \'tywin\', \'tyrion\', \'joffrey\']\n\n\n# Define generator function get_lengths\ndef get_lengths(input_list):\n    """"""Generator function that yields the\n    length of the strings in input_list.""""""\n\n    # Yield the length of a string\n    for person in input_list:\n        yield len(person)\n\n\n# Print the values generated by get_lengths()\nfor value in get_lengths(lannister):\n    print(value)\n'"
src/core/py_iterable_and_iterator.py,0,"b'# An ITERABLE is:\n#\n# anything that can be looped over (i.e. you can loop over a string or file) or\n# anything that can appear on the right-side of a for-loop:  for x in iterable: ... or\n# anything you can call with iter() that will return an ITERATOR:  iter(obj) or\n# an object that defines __iter__ that returns a fresh ITERATOR,\n# or it may have a __getitem__ method suitable for indexed lookup.\n#\n# An ITERATOR is an object:\n#\n# with state that remembers where it is during iteration,\n# with a __next__ method that:\n# returns the next value in the iteration\n# updates the state to point at the next value\n# signals when it is done by raising StopIteration\n# and that is self-iterable (meaning that it has an __iter__ method that returns self).\n#\n# Notes:\n#\n# The __next__ method in Python 3 is spelt next in Python 2, and\n# The builtin function next() calls that method on the object passed to it.\n#\n# EXAMPLES ::\n\n# s is a str object that is immutable\n# s has no state\n# s has a __getitem__() method\ns = \'cat\'      # s is an ITERABLE\nprint(next(s))  # TypeError: \'str\' object is not an iterator\n\n# t has state (it starts by pointing at the ""c""\n# t has a next() method and an __iter__() method\nt = iter(s)    # t is an ITERATOR\n\nnext(t)        # the next() function returns the next value and advances the state\nnext(t)        # the next() function returns the next value and advances\nnext(t)        # the next() function returns the next value and advances\nnext(t)        # next() raises StopIteration to signal that iteration is complete\n\n# >>> iter(t) is t   # the iterator is self-iterable\n'"
src/core/py_regex.py,0,"b'import re\n\nmytext = str([\n    \'20080620033027/http://www.mrvc.indianrail.gov.in/overview.htm). _Official webpage of Mumbai Railway Vikas Corporation_. Archived from [the original](http://www.mrvc.indianrail.gov.in/overview.htm) on 2008-06-20. Retrieved 2008-12-11.\'])\n\nmyregex = r\'(?:[a-zA-Z0-9](?:[a-zA-Z0-9\\-]{,61}[a-zA-Z0-9])?\\.)+[a-zA-Z]{2,6}\'\nx = re.findall(myregex, mytext)\n\nresult = []\nfor res in x:\n    result.append(res.replace(""www."", """").split(\'//\')[-1].split(\'/\')[0])\n\nprint(\';\'.join(result))\n'"
src/db/__init__.py,0,b''
src/db/py_mongo_integration.py,0,"b'import pprint\n\nimport pymongo\nfrom bson import ObjectId\nfrom pymongo import MongoClient\n\nMONGODB_HOST = \'localhost\'\nMONGODB_PORT = 27017\nDB_NAME = \'testdb\'\nCOLLECTION_TEST = \'collection_1\'\nCOLLECTION_PROFILES = \'profiles\'\n\ntest_data = {\n    \'title\': \'My First MongoDB document\',\n    \'author\': \'Saransh Bansal\',\n    \'likes\': 100,\n}\n\nuser_profiles = [{\'user_id\': 211, \'name\': \'Luke\'}, {\'user_id\': 212, \'name\': \'Ziltoid\'}]\n\n\nclass MongoUtil():\n    db = None\n\n    def __init__(self):\n        client = MongoClient(MONGODB_HOST, MONGODB_PORT)\n        self.db = client.DB_NAME\n\n    def connect_to_mongo(self):\n        return self.db\n\n    def print_collection(self, coll_name):\n        mycol = self.db[coll_name]\n        print(mycol)\n\n    def insert_document(self, coll_name, document=None):\n        mycol = self.db[coll_name]\n        mycol.insert_one(document)\n\n    def update_document(self, coll_name, obj_id):\n        mycol = self.db[coll_name]\n        mycol.update_one({\'_id\': ObjectId(obj_id)}, {""$set"": {""title"": ""abc""}})\n\n    def print_all(self, coll_name):\n        results = self.db[coll_name].find()\n        for node in results:\n            pprint.pprint(node)\n\n    def count_documents(self, coll_name):\n        mycol = self.db[coll_name]\n        count = mycol.count_documents({})\n        print(count)\n\n    def create_profiles(self, coll_name):\n        self.db[coll_name].insert_many(user_profiles)\n\n    def create_index(self, coll_name, index_col):\n        self.db[coll_name].create_index([(index_col, pymongo.ASCENDING)],\n                                        unique=True)\n        print(sorted(list(self.db[coll_name].index_information())))\n\n\ninstance = MongoUtil()\n\ninstance.print_collection(COLLECTION_TEST)\n\n# instance.insert_document(COLLECTION_TEST, test_data)\n\n# instance.create_profiles(COLLECTION_PROFILES)\n\ninstance.print_all(COLLECTION_TEST)\n\nprint(\'\\n\')\n\ninstance.print_all(COLLECTION_PROFILES)\n\nprint(\'\\n\')\n\ninstance.count_documents(COLLECTION_TEST)\n\n# instance.update_document(COLLECTION_TEST, \'5d2c87e03c30f6680050c521\')\n\ninstance.create_index(COLLECTION_PROFILES, \'user_id\')\n'"
src/db/py_sql.py,0,"b""# Import necessary module\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nfrom helper import path\n\n# Create engine: engine\nengine = create_engine('sqlite:///' + path + 'Chinook.sqlite')\n\n# Save the table names to a list: table_names\ntable_names = engine.table_names()\n\n# Print the table names to the shell\nprint(table_names)\n\n# Open engine connection: con\ncon = engine.connect()\n\n# Perform query: rs\nrs = con.execute('select * from Album')\n\n# Save results of the query to DataFrame: df\ndf1 = pd.DataFrame(rs.fetchall())\n\n# Close connection\ncon.close()\n\n# Print head of DataFrame df\nprint(df1.head())\n\n# ---------------------------------------\n\n# Perform query and save results to DataFrame: df\nwith engine.connect() as con:\n    rs = con.execute('select LastName, Title from Employee')\n    df2 = pd.DataFrame(rs.fetchmany(size=3))\n    df2.columns = rs.keys()\n\n# Print the length of the DataFrame df\nprint(len(df2))\n\n# Print the head of the DataFrame df\nprint(df2.head())\n\n# ---------------------------------------\n# Open engine in context manager\n# Perform query and save results to DataFrame: df\nwith engine.connect() as con:\n    rs = con.execute('select * from Employee where EmployeeId >= 6')\n    df3 = pd.DataFrame(rs.fetchall())\n    df3.columns = rs.keys()\n\n# Print the head of the DataFrame df\nprint(df3.head())\n"""
src/db/py_sql_with_pandas.py,0,"b""# Import necessary module\nimport random\nfrom datetime import datetime\n\nimport pandas as pd\nfrom sqlalchemy import create_engine\n\nfrom helper import path\n\n# Create engine: engine\nengine = create_engine('sqlite:///' + path + 'Chinook.sqlite');\n\n# Execute query and store records in DataFrame: df\ndf1 = pd.read_sql_query('select * from Album', engine)\n\n# Execute query and store records in DataFrame: df\ndf2 = pd.read_sql_query('select * from Employee where EmployeeId >= 6 order by BirthDate', engine)\n\ndf3 = pd.read_sql_query('select Title, Name from Album al inner join Artist ar on al.ArtistID=ar.ArtistID', engine)\n\ndf4 = pd.read_sql_query(\n    'select * from PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId = Track.TrackId where Milliseconds < 250000',\n    engine)\n\nrand_dates = [datetime(random.randrange(2000, 2001), random.randrange(1, 6), random.randrange(1, 3)) for d in\n              range(0, len(df4))]\ndf4['dates'] = rand_dates\n\ndf4 = df4.loc[:, ~df4.columns.duplicated()]\n# Print head of DataFrame\n# print(df1.head())\n# print(df2.head())\n# print(df3.head())\n# print(df4.head())\ndf4 = df4.groupby([df4.dates.dt.year.rename('year'), df4.dates.dt.month.rename('month')]).size()\nprint(df4)\n# df4.groupby(df4.dates).agg({'count'})\n# print(df4.columns)\n# print(df4['dates'].value_counts())\n\n"""
src/file_operations/py_corrupt_file_read.py,0,"b'# Import matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom helper import path\n\n# Assign filename: file\nfile = \'titanic_corrupt.txt\'\n\n# Import file: data\ndata = pd.read_csv(path + file, sep=\'\\t\', comment=\'#\', na_values=[\'NA\', \'NaN\', ""Nothing""])\n\n# Print the head of the DataFrame\nprint(data.head())\n\n# Plot \'Age\' variable in a histogram\npd.DataFrame.hist(data[[\'Age\']])\nplt.xlabel(\'Age (years)\')\nplt.ylabel(\'count\')\nplt.show()\n'"
src/file_operations/py_default_file_read_1.py,0,"b""from helper import path\n\n# Open a file: file\nfile = open(path + 'moby_dick.txt', mode='r')\n\n# Print it\nprint(file.read())\n\n# Check whether file is closed\nprint(file.closed)\n\n# Close file\nfile.close()\n\n# Check whether file is closed\nprint(file.closed)\n\n# Importing text file_operations line by line\n\n# Read & print the first 3 lines\nwith open(path + 'moby_dick.txt') as file:\n    print(file.readline())\n    print(file.readline())\n    print(file.readline())\n\n\n"""
src/file_operations/py_numpy_file_read_1.py,2,"b""# Import package\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom helper import path\n\n# Assign filename to variable: file\nfile = 'digits.csv'\n\n# Load file as array: digits\ndigits = np.loadtxt(path + file, delimiter=',', dtype=str)\n\n# Print datatype of digits\nprint(type(digits))\n\n# Select and reshape a row\nim = digits[21, 1:]\nim_sq = np.reshape(im, (8, 98))\n\n# Plot reshaped data (matplotlib.pyplot already loaded as plt)\nplt.imshow(im_sq, cmap='Greys', interpolation='nearest')\nplt.show()\n"""
src/file_operations/py_numpy_file_read_2.py,2,"b""# Import package\nimport numpy as np\nimport pandas as pd\n\nfrom helper import path\n\n# Assign the filename: file\nfile = 'amis.csv'\n\n# Load the data: data\ndata = np.loadtxt(path + file, delimiter=',', skiprows=1, usecols=[1, 3])\n\n# Print data\nprint(data)\n\n# Read flat file to data frame\n\n# Read the first 5 rows of the file into a DataFrame: data\ndata = pd.read_csv(path + file, nrows=5, header=None)\n\n# Build a numpy array from the DataFrame: data_array\ndata_array = np.array(data)\n\n# Print the datatype of data_array to the shell\nprint(type(data_array))\n"""
src/file_operations/py_pandas_excel_read.py,0,"b""# Listing sheets in Excel file_operations\n#\n# Whether you like it or not, any working data scientist will need to deal with Excel spreadsheets at some point in\n# time. You won't always want to do so in Excel, however!\n#\n# Here, you'll learn how to use pandas to import Excel spreadsheets and how to list the names of the sheets in any\n# loaded .xlsx file.\n#\n# Recall from the video that, given an Excel file imported into a variable spreadsheet, you can retrieve a list of\n# the sheet names using the attribute spreadsheet.sheet_names.\n#\n# Specifically, you'll be loading and checking out the spreadsheet 'battledeath.xlsx', modified from the Peace\n# Research Institute Oslo's (PRIO) dataset. This data contains age-adjusted mortality rates due to war in various\n# countries over several years.\n\n# Import pandas\nimport pandas as pd\n\nfrom helper import path\n\n# Assign spreadsheet filename: file\nfile = 'battledeath.xlsx'\n\n# Load spreadsheet: xl\nxl = pd.ExcelFile(path + file)\n\n# Print sheet names\nprint(xl.sheet_names)\n\n# -------------------------------------------\n# Load a sheet into a DataFrame by name: df1\ndf1 = xl.parse('2004')\n\n# Print the head of the DataFrame df1\nprint(df1.head())\n\n# Load a sheet into a DataFrame by index: df2\n# parse_args :: sheet index/name | skiprows | custom column names | parse_cols columns to show\ndf2 = xl.parse(0, skiprows=[0])\n\n# Print the head of the DataFrame df2\nprint(df2.head())\n\n# PS: both are ~ same!\n"""
src/file_operations/py_pandas_file_read_1.py,0,"b""# Import pandas as pd\nimport pandas as pd\nfrom helper import path\n# Assign the filename: file\nfile = 'digits.csv'\n\n# Read the file into a DataFrame: df\ndf = pd.read_csv(file)\n\n# View the head of the DataFrame\nprint(df.head())\n"""
src/file_operations/py_pandas_read_csv.py,0,"b'# Import pandas as pd\nimport pandas as pd\n\n# Import the cars.csv data: cars\ncars = pd.DataFrame(pd.read_csv(""../_datasets/cars.csv""))\ntweets = pd.DataFrame(pd.read_csv(""../_datasets/tweets.csv""))\njobs = pd.DataFrame(pd.read_csv(""../_datasets/Information_gain_job_advertisements.csv""))\nindustries = pd.DataFrame(pd.read_json(""../_datasets/industries.json""))\n\n# Print out cars\nprint(cars.describe())\n\n# Print out tweets\nprint(tweets.keys())\n\n# Print all columns of industries\nprint(list(industries.resultList))\n'"
src/file_operations/py_pickle_read_test.py,0,"b'# Save a dictionary into a pickle file.\nimport pickle\n\nfrom helper import path\n\nd = {\'Aug\': \'85\', \'Airline\': \'8\', \'June\': \'69.4\', \'Mar\': \'84.4\'}\npickle.dump(d, open(path + ""data.pk1"", ""wb""))\n\n# Load the dictionary back from the pickle file.\nd = pickle.load(open(path + \'data.pk1\', ""rb""))\n\nprint(d)\n\nprint(type(d))\n'"
src/file_operations/py_read_hdf5_file.py,0,"b""import h5py\n\nfrom helper import path\n\n# Assign filename: file\nfile = 'NEONDS.hdf5'\n\n# Load file: data\ndata = h5py.File(path + file, 'r')\n\n# Print the datatype of the loaded file\nprint(type(data))\n\n# Print the keys of the file\nfor key in data.keys():\n    print(data[key])\n"""
src/file_operations/py_read_matlab_file.py,1,"b""# Import package\nimport scipy.io\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom helper import path\n\n# Load MATLAB file: mat\nmat = scipy.io.loadmat(path + 'albeck_gene_expression.mat')\n\n# Print the datatype type of mat\nprint(type(mat))\n\n# Print the keys of the MATLAB dictionary\nprint(mat.keys())\n\n# Print the type of the value corresponding to the key 'CYratioCyt'\nprint(type(mat['fret']))\n\n# Print the shape of the value corresponding to the key 'CYratioCyt'\nprint(np.shape(mat['fret']))\n\n# Subset the array and plot it\ndata = mat['fret'][25, 5:]\nfig = plt.figure()\nplt.plot(data)\nplt.xlabel('time (min.)')\nplt.ylabel('normalized fluorescence (measure of expression)')\nplt.show()\n\n"""
src/file_operations/py_read_sas_file.py,0,"b""import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Import sas7bdat package\nfrom sas7bdat import SAS7BDAT\n\nfrom helper import path\n\n# Save file to a DataFrame: df_sas\nwith SAS7BDAT(path+'sales.sas7bdat') as file:\n    df_sas = file.to_data_frame()\n\n# Print head of DataFrame\nprint((df_sas.head()))\n\n# Plot histogram of DataFrame features (pandas and pyplot already imported)\npd.DataFrame.hist(df_sas[['P']])\nplt.ylabel('count')\nplt.show()\n"""
src/file_operations/py_read_stata_file.py,0,"b""import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Import sas7bdat package\n\nfrom helper import path\n\ndf = pd.read_stata(path + 'disarea.dta', 'rb')\n\n# Print head of DataFrame\nprint(df.head())\n\n\n# Plot histogram of DataFrame features (pandas and pyplot already imported)\ndef plot(key):\n    if key not in ['wbcode', 'country']:\n        pd.DataFrame.hist(df[[key]])\n        plt.ylabel('count')\n        plt.show()\n\nfor key in df.keys():\n    plot(key)\n\n\n"""
src/file_operations/py_test_loops_algo.py,0,"b""# Here are some comparisons of the performances for in, set and bisect. Note the time (in second) is in log scale.\n\nimport bisect\nimport math\nimport random\nimport time\n\nimport matplotlib.pyplot as plt\n\n\ndef method_in(a, b, c):\n    start_time = time.time()\n    for i, x in enumerate(a):\n        if x in b:\n            c[i] = 1\n    return time.time() - start_time\n\n\ndef method_set_in(a, b, c):\n    start_time = time.time()\n    s = set(b)\n    for i, x in enumerate(a):\n        if x in s:\n            c[i] = 1\n    return time.time() - start_time\n\n\ndef method_bisect(a, b, c):\n    start_time = time.time()\n    b.sort()\n    for i, x in enumerate(a):\n        index = bisect.bisect_left(b, x)\n        if index < len(a):\n            if x == b[index]:\n                c[i] = 1\n    return time.time() - start_time\n\n\ndef profile():\n    time_method_in = []\n    time_method_set_in = []\n    time_method_bisect = []\n\n    Nls = [x for x in range(1000, 20000, 1000)]\n    for N in Nls:\n        a = [x for x in range(0, N)]\n        random.shuffle(a)\n        b = [x for x in range(0, N)]\n        random.shuffle(b)\n        c = [0 for x in range(0, N)]\n\n        time_method_in.append(math.log(method_in(a, b, c)))\n        time_method_set_in.append(math.log(method_set_in(a, b, c)))\n        time_method_bisect.append(math.log(method_bisect(a, b, c)))\n\n    plt.plot(Nls, time_method_in, marker='o', color='r', linestyle='-', label='in')\n    plt.plot(Nls, time_method_set_in, marker='o', color='b', linestyle='-', label='set')\n    plt.plot(Nls, time_method_bisect, marker='o', color='g', linestyle='-', label='bisect')\n    plt.xlabel('list size', fontsize=18)\n    plt.ylabel('log(time)', fontsize=18)\n    plt.legend(loc='upper left')\n    plt.show()\n\n\nprofile()\n"""
src/file_operations/read_in_chunks.py,0,"b'import pandas as pd\nfrom helper import path\n\n\n# Define count_entries()\ndef count_entries(csv_file, c_size, colname):\n    """"""Return a dictionary with counts of\n    occurrences as value for each key.""""""\n\n    # Initialize an empty dictionary: counts_dict\n    counts_dict = {}\n\n    # Iterate over the file chunk by chunk\n    for chunk in pd.read_csv(csv_file, chunksize=c_size):\n\n        # Iterate over the column in DataFrame\n        for entry in chunk[colname]:\n            if entry in counts_dict.keys():\n                counts_dict[entry] += 1\n            else:\n                counts_dict[entry] = 1\n\n    # Return counts_dict\n    return counts_dict\n\n\n# Call count_entries(): result_counts\nresult_counts = count_entries(path + \'Information_gain_job_advertisements.csv\', 10, \'Term\')\n\n# Print result_counts\nprint(result_counts)\n'"
src/file_operations/read_tweets.py,0,"b'#!/usr/bin/env python\n# encoding: utf-8\n\nimport csv\n\nimport tweepy  # https://github.com/tweepy/tweepy\n\n# Twitter API credentials\nconsumer_key = """"\nconsumer_secret = """"\naccess_key = """"\naccess_secret = """"\n\n\ndef get_all_tweets(screen_name):\n    # Twitter only allows access to a users most recent 3240 tweets with this method\n\n    # authorize twitter, initialize tweepy\n    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n    auth.set_access_token(access_key, access_secret)\n    api = tweepy.API(auth)\n\n    # initialize a list to hold all the tweepy Tweets\n    alltweets = []\n\n    # make initial request for most recent tweets (200 is the maximum allowed count)\n    new_tweets = api.user_timeline(screen_name=screen_name, count=200)\n\n    # save most recent tweets\n    alltweets.extend(new_tweets)\n\n    # save the id of the oldest tweet less one\n    oldest = alltweets[-1].id - 1\n\n    # keep grabbing tweets until there are no tweets left to grab\n    while len(new_tweets) > 0:\n        print\n        ""getting tweets before %s"" % oldest\n\n        # all subsiquent requests use the max_id param to prevent duplicates\n        new_tweets = api.user_timeline(screen_name=screen_name, count=200, max_id=oldest)\n\n        # save most recent tweets\n        alltweets.extend(new_tweets)\n\n        # update the id of the oldest tweet less one\n        oldest = alltweets[-1].id - 1\n\n        print\n        ""...%s tweets downloaded so far"" % (len(alltweets))\n\n    # transform the tweepy tweets into a 2D array that will populate the csv\n    outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(""utf-8"")] for tweet in alltweets]\n\n    # write the csv\n    with open(\'%s_tweets.csv\' % screen_name, \'wb\') as f:\n        writer = csv.writer(f)\n        writer.writerow([""id"", ""created_at"", ""text""])\n        writer.writerows(outtweets)\n\n    pass\n\n\nif __name__ == \'__main__\':\n    # pass in the username of the account you want to download\n    get_all_tweets(""J_tsar"")\n'"
src/misc/__init__.py,0,b''
src/misc/py_test_loops_algo.py,0,"b""# Here are some comparisons of the performances for in, set and bisect. Note the time (in second) is in log scale.\n\nimport bisect\nimport math\nimport random\nimport time\n\nimport matplotlib.pyplot as plt\n\n\ndef method_in(a, b, c):\n    start_time = time.time()\n    for i, x in enumerate(a):\n        if x in b:\n            c[i] = 1\n    return time.time() - start_time\n\n\ndef method_set_in(a, b, c):\n    start_time = time.time()\n    s = set(b)\n    for i, x in enumerate(a):\n        if x in s:\n            c[i] = 1\n    return time.time() - start_time\n\n\ndef method_bisect(a, b, c):\n    start_time = time.time()\n    b.sort()\n    for i, x in enumerate(a):\n        index = bisect.bisect_left(b, x)\n        if index < len(a):\n            if x == b[index]:\n                c[i] = 1\n    return time.time() - start_time\n\n\ndef profile():\n    time_method_in = []\n    time_method_set_in = []\n    time_method_bisect = []\n\n    Nls = [x for x in range(1000, 20000, 1000)]\n    for N in Nls:\n        a = [x for x in range(0, N)]\n        random.shuffle(a)\n        b = [x for x in range(0, N)]\n        random.shuffle(b)\n        c = [0 for x in range(0, N)]\n\n        time_method_in.append(math.log(method_in(a, b, c)))\n        time_method_set_in.append(math.log(method_set_in(a, b, c)))\n        time_method_bisect.append(math.log(method_bisect(a, b, c)))\n\n    plt.plot(Nls, time_method_in, marker='o', color='r', linestyle='-', label='in')\n    plt.plot(Nls, time_method_set_in, marker='o', color='b', linestyle='-', label='set')\n    plt.plot(Nls, time_method_bisect, marker='o', color='g', linestyle='-', label='bisect')\n    plt.xlabel('list size', fontsize=18)\n    plt.ylabel('log(time)', fontsize=18)\n    plt.legend(loc='upper left')\n    plt.show()\n\n\nprofile()\n"""
src/misc/py_zip_example.py,0,"b""# Using zip\n#\n# Another interesting function that you've learned is zip(), which takes any number of iterables and\n# returns a zip object that is an iterator of tuples. If you wanted to print the values of a zip object,\n# you can convert it into a list and then print it. Printing just a zip object will not return the values unless you\n# unpack it first. In this exercise, you will explore this for yourself.\n#\n# Three lists of strings are pre-loaded: mutants, aliases, and powers. First, you will use list() and zip() on these\n# lists to generate a list of tuples. Then, you will create a zip object using zip(). Finally, you will unpack this\n# zip object in a for loop to print the values in each tuple. Observe the different output generated by printing the\n# list of tuples, then the zip object, and finally, the tuple values in the for loop.\n\nmutants = ['charles xavier',\n           'bobby drake',\n           'kurt wagner',\n           'max eisenhardt',\n           'kitty pride']\n\naliases = ['prof x', 'iceman', 'nightcrawler', 'magneto', 'shadowcat']\n\npowers = ['telepathy',\n          'thermokinesis',\n          'teleportation',\n          'magnetokinesis',\n          'intangibility']\n\n# Create a list of tuples: mutant_data\nmutant_data = list(zip(mutants, aliases, powers))\n\n# Print the list of tuples\nprint(mutant_data)\n\n# Create a zip object using the three lists: mutant_zip\nmutant_zip = zip(mutants, aliases, powers)\n\n# Print the zip object\nprint(mutant_zip)\n\n# Unpack the zip object and print the tuple values\nfor value1, value2, value3 in mutant_zip:\n    print(value1, value2, value3)\n"""
src/misc/random.py,0,"b'from datetime import datetime\n\na = [x for x in range(3, 13)]\nprint(a)\nvalue = []\nprint(value[0] if value else 0)\n\nval = \'2018,1\'\ndate_object = datetime.strptime(val, \'%Y,%m\')\n\nprint(date_object.strftime(""%b %y""))\n\nprint(\'sdada\' + \'123\')\n\n\nclass B(object):\n    def __init__(self):\n        body = ""aaa""\n        self.context = {\n            \'body\': body,\n        }\n\n\nclass A(B):\n    def __init__(self):\n        super().__init__()\n        self.context[\'body\'] = self.context[\'body\'] + ""BBB""\n        self.context = {\n            **self.context,\n        }\n        print(self.context[\'body\'])\n\n\nclass C(B):\n    def __init__(self):\n        super().__init__()\n\n        print(self.context[\'body\'])\n\n\nb = B()\na = A()\nc = C()\n\ndata = {\n    \'key\': 100\n}\n\nprint(\'{}\\\\xE2\\\\x80\\\\xAD\\\\xE2\\\\x80\\\\xAD\'.format(data))\n\nprint(\'+381652522560\')\n\nname = \'Larry Lam\'\nif name:\n    names = name.split(\' \')\n    given = names[0]\n    family = names[len(names) - 1]\n    print(\'{}, {}\'.format(given, family))\n\n\nprint(\'   \'.strip() or \'NA\')\n\n'"
src/misc/tensorflow_starter.py,3,"b'import math\nfrom builtins import print\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn import metrics\nfrom tensorflow.python.data import Dataset\n\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = \'{:.1f}\'.format\n\ncalifornia_housing_dataframe = pd.read_csv(\'https://storage.googleapis.com/mledu-datasets/california_housing_train.csv\',\n                                           sep="","")\ncalifornia_housing_dataframe = california_housing_dataframe.reindex(\n    np.random.permutation(california_housing_dataframe.index))\ncalifornia_housing_dataframe[""median_house_value""] /= 1000.0\ncalifornia_housing_dataframe\n\n# Define the input feature: total_rooms.\nmy_feature = california_housing_dataframe[[""total_rooms""]]\n\n# Configure a numeric feature column for total_rooms.\nfeature_columns = [tf.feature_column.numeric_column(""total_rooms"")]\n\n# Define the label.\ntargets = california_housing_dataframe[""median_house_value""]\n\n# Use gradient descent as the optimizer for training the model.\nmy_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n\n# Configure the linear regression model with our feature columns and optimizer.\n# Set a learning rate of 0.0000001 for Gradient Descent.\nlinear_regressor = tf.estimator.LinearRegressor(\n    feature_columns=feature_columns,\n    optimizer=my_optimizer\n)\n\n\ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    """"""Trains a linear regression model of one feature.\n\n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    """"""\n\n    # Convert pandas data into a dict of np arrays.\n    features = {key: np.array(value) for key, value in dict(features).items()}\n\n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features, targets))  # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n\n    # Shuffle the data, if specified.\n    if shuffle:\n        ds = ds.shuffle(buffer_size=10000)\n\n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\n\n\n_ = linear_regressor.train(\n    input_fn=lambda: my_input_fn(my_feature, targets),\n    steps=100\n)\n\n# Create an input function for predictions.\n# Note: Since we\'re making just one prediction for each example, we don\'t\n# need to repeat or shuffle the data here.\nprediction_input_fn =lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\n\n# Call predict() on the linear_regressor to make predictions.\npredictions = linear_regressor.predict(input_fn=prediction_input_fn)\n\n# Format predictions as a NumPy array, so we can calculate error metrics.\npredictions = np.array([item[\'predictions\'][0] for item in predictions])\n\n# Print Mean Squared Error and Root Mean Squared Error.\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nroot_mean_squared_error = math.sqrt(mean_squared_error)\nprint(""Mean Squared Error (on training data): %0.3f"" % mean_squared_error)\nprint(""Root Mean Squared Error (on training data): %0.3f"" % root_mean_squared_error)'"
src/ml-supervised/__init__.py,0,b''
src/ml-supervised/k-fold_cross_validation.py,3,"b'# 5-fold cross-validation\n#\n# Cross-validation is a vital step in evaluating a model. It maximizes the amount of data\n# that is used to train the model, as during the course of training, the model is not only trained, but also tested\n# on all of the available data.\n#\n# In this exercise, you will practice 5-fold cross validation on the Gapminder data. By default, scikit-learn\'s\n# cross_val_score() function uses R2 as the metric of choice for regression. Since you are performing 5-fold\n# cross-validation, the function will return 5 scores. Your job is to compute these 5 scores and then take their\n# average.\n#\n# The DataFrame has been loaded as df and split into the feature/target variable arrays X and y. The modules pandas\n# and numpy have been imported as pd and np, respectively.\n\n# Import the necessary modules\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + \'gm_2008_region.csv\')\n\n# Create arrays for features and target variable\nX = df[\'fertility\'].values\ny = df[\'life\'].values\n\n# Reshape X and y\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n# Create a linear regression object: reg\nreg = LinearRegression()\n\n# Compute 5-fold cross-validation scores: cv_scores\ncv_scores = cross_val_score(reg, X, y, cv=5)\n\n# Print the 5-fold cross-validation scores\nprint(cv_scores)\n\nprint(""Average 5-Fold CV Score: {}"".format(np.mean(cv_scores)))\n\n# ---------------------------------\n\n# Test time for 3-fold & 10 fold operations :: %timeit cross_val_score(reg, X, y, cv = ____)\n# Perform 3-fold CV\ncvscores_3 = cross_val_score(reg, X, y, cv=3)\nprint(""Average 3-Fold CV Score: {}"".format(np.mean(cvscores_3)))\n\n# Perform 10-fold CV\ncvscores_10 = cross_val_score(reg, X, y, cv=10)\nprint(""Average 10-Fold CV Score: {}"".format(np.mean(cvscores_10)))\n'"
src/ml-supervised/ml_centering_and_scaling.py,4,"b'# Centering and scaling your data\n#\n# In the video, Hugo demonstrated how significantly the performance of a model can\n# improve if the features are scaled. Note that this is not always the case: In the Congressional voting records\n# dataset, for example, all of the features are binary. In such a situation, scaling will have minimal impact.\n#\n# You will now explore scaling for yourself on a new dataset - White Wine Quality! Hugo used the Red Wine Quality\n# dataset in the video. We have used the \'quality\' feature of the wine to create a binary target variable: If\n# \'quality\' is less than 5, the target variable is 1, and otherwise, it is 0.\n#\n# The DataFrame has been pre-loaded as df, along with the feature and target variable arrays X and y. Explore it in\n# the IPython Shell. Notice how some features seem to have different units of measurement. \'density\', for instance,\n# only takes values between 0 and 1, while \'total sulfur dioxide\' has a maximum value of 289. As a result,\n# it may be worth scaling the features here. Your job in this exercise is to scale the features and compute the mean\n# and standard deviation of the unscaled features compared to the scaled features.\n\n\n# Import scale\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import scale\n\nfrom helper import path\n\n# Read \'white-wine.csv\' into a DataFrame: df\ndf = pd.read_csv(path + \'white-wine.csv\')\n\nX = df.drop(\'quality\', axis=1)\ny = df[\'quality\']\n\n# Scale the features: X_scaled\nX_scaled = scale(X)\n\n# Print the mean and standard deviation of the unscaled features\nprint(""Mean of Unscaled Features: {}"".format(np.mean(X)))\nprint(""Standard Deviation of Unscaled Features: {}"".format(np.std(X)))\n\n# Print the mean and standard deviation of the scaled features\nprint(""Mean of Scaled Features: {}"".format(np.mean(X_scaled)))\nprint(""Standard Deviation of Scaled Features: {}"".format(np.std(X_scaled)))\n\n# -----------\n\n\n# Import the necessary modules\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Setup the pipeline steps: steps\nsteps = [(\'scaler\', StandardScaler()),\n         (\'knn\', KNeighborsClassifier())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the training set: knn_scaled\nknn_scaled = pipeline.fit(X_train, y_train)\n\n# Instantiate and fit a k-NN classifier to the unscaled data\nknn_unscaled = KNeighborsClassifier().fit(X_train, y_train)\n\n# Compute and print metrics\nprint(\'Accuracy with Scaling: {}\'.format(knn_scaled.score(X_test, y_test)))\nprint(\'Accuracy without Scaling: {}\'.format(knn_unscaled.score(X_test, y_test)))\n'"
src/ml-supervised/ml_manually_remove_missing_data.py,1,"b'# Regression with categorical features\n#\n# Having created the dummy variables from the \'Region\' feature, you can build\n# regression models as you did before. Here, you\'ll use ridge regression to perform 5-fold cross-validation.\n# The feature array X and target variable array y have been pre-loaded.\n#\n# Dropping missing data\n#\n# The voting dataset from Chapter 1 contained a bunch of missing values that we dealt with for\n# you behind the scenes. Now, it\'s time for you to take care of these yourself!\n#\n# The unprocessed dataset has been loaded into a DataFrame df. Explore it in the IPython Shell with the .head()\n# method. You will see that there are certain data points labeled with a \'?\'. These denote missing values. As you saw\n#  in the video, different datasets encode missing values in different ways. Sometimes it may be a \'9999\',\n# other times a 0 - real-world data can be very messy! If you\'re lucky, the missing values will already be encoded as\n#  NaN. We use NaN because it is an efficient and simplified way of internally representing missing data, and it lets\n#  us take advantage of pandas methods such as .dropna() and .fillna(), as well as scikit-learn\'s Imputation\n# transformer Imputer().\n# In this exercise, your job is to convert the \'?\'s to NaNs, and then drop the rows that contain them from the\n# DataFrame.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom helper import path\n\n# Read \'gapminder.csv\' into a DataFrame: df\ndf = pd.read_csv(path + \'gm_2008_region.csv\')\n\n# Create a boxplot of life expectancy per region\ndf.boxplot(\'life\', \'Region\', rot=60)\n\n# Show the plot\nplt.show()\n\n# ----------------------\n\n# Create arrays for features and target variable\nX = df[\'population\'].values\ny = df[\'life\'].values\n\n# Reshape X and y\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n# Import necessary modules\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate a ridge regressor: ridge\nridge = Ridge(alpha=0.5, normalize=True)\n\n# Perform 5-fold cross-validation: ridge_cv\nridge_cv = cross_val_score(ridge, X, y, cv=5)\n\n# Print the cross-validated scores\nprint(ridge_cv)\n\n# -------------------------\n\n# Convert \'?\' to NaN\ndf[df == \'?\'] = np.nan\n\n# Print the number of NaNs\nprint(df.isnull().sum())\n\n# Print shape of original DataFrame\nprint(""Shape of Original DataFrame: {}"".format(df.shape))\n\n# Drop missing values and print shape of new DataFrame\ndf = df.dropna()\n\n# Print shape of new DataFrame\nprint(""Shape of DataFrame After Dropping All Rows with Missing Values: {}"".format(df.shape))\n'"
src/ml-supervised/ml_pipeline_with_hyperparameters.py,0,"b'# Bringing it all together I:\n#\n# Pipeline for classification It is time now to piece together everything you have\n# learned so far into a pipeline for classification! Your job in this exercise is to build a pipeline that includes\n# scaling and hyperparameter tuning to classify wine quality.\n#\n# You\'ll return to using the SVM classifier you were briefly introduced to earlier in this chapter. The\n# hyperparameters you will tune are C and gamma. C controls the regularization strength. It is analogous to the C you\n#  tuned for logistic regression in Chapter 3, while gamma controls the kernel coefficient: Do not worry about this\n# now as it is beyond the scope of this course.\n#\n# The following modules have been pre-loaded: Pipeline, svm, train_test_split, GridSearchCV, classification_report,\n# accuracy_score. The feature and target variable arrays X and y have also been pre-loaded.e.\n\nimport pandas as pd\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nfrom helper import path\n\n# Read \'white-wine.csv\' into a DataFrame: df\ndf = pd.read_csv(path + \'white-wine.csv\')\n\nX = df.drop(\'quality\', axis=1)\ny = df[\'quality\']\n\n# Setup the pipeline\nsteps = [(\'scaler\', StandardScaler()),\n         (\'SVM\', SVC())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {\'SVM__C\': [1, 10, 100],\n              \'SVM__gamma\': [0.1, 0.01]}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=21)\n\n# Instantiate the GridSearchCV object: cv\ncv = GridSearchCV(pipeline, param_grid=parameters)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\nprint(""Accuracy: {}"".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(""Tuned Model Parameters: {}"".format(cv.best_params_))\n'"
src/ml-supervised/ml_pipelines.py,0,"b""# Imputing missing data in a ML Pipeline I\n#\n# As you've come to appreciate, there are many steps to building a model,\n# from creating training and test sets, to fitting a classifier or regressor, to tuning its parameters, to evaluating\n#  its performance on new data. Imputation can be seen as the first step of this machine learning process,\n# the entirety of which can be viewed within the context of a pipeline. Scikit-learn provides a pipeline constructor\n# that allows you to piece together these steps into one process and thereby simplify your workflow.\n#\n# You'll now practice setting up a pipeline with two steps: the imputation step, followed by the instantiation of a\n# classifier. You've seen three classifiers in this course so far: k-NN, logistic regression, and the decision tree.\n# You will now be introduced to a fourth one - the Support Vector Machine, or SVM. For now, do not worry about how it\n#  works under the hood. It works exactly as you would expect of the scikit-learn estimators that you have worked\n# with previously, in that it has the same .fit() and .predict() methods as before.\n\n\n# Import the Imputer module\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.svm import SVC\n\n# Setup the Imputation transformer: imp\nimp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n\n# Instantiate the SVC classifier: clf\nclf = SVC()\n\n# Setup the pipeline with the required steps: steps\nsteps = [('imputation', imp),\n         ('SVM', clf)]\n\n# --------------\n# Import necessary modules\nimport pandas as pd\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom helper import path\n\n# Read 'white-wine.csv' into a DataFrame: df\ndf = pd.read_csv(path + 'white-wine.csv')\n\nX = df.drop('quality', axis=1)\ny = df['quality']\n# Setup the pipeline steps: steps\nsteps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n         ('SVM', SVC())]\n\n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Fit the pipeline to the train set\npipeline.fit(X_train, y_train)\n\n# Predict the labels of the test set\ny_pred = pipeline.predict(X_test)\n\n# Compute metrics\nprint(classification_report(y_test, y_pred))\n"""
src/ml-supervised/py_hyperparamter_tuning_hold-out_set_with_GridSearchCV-1.py,1,"b'# Hold-out set in practice I: Classification\n#\n# You will now practice evaluating a model with tuned hyperparameters on a\n#  hold-out set. The feature array and target variable array from the diabetes dataset have been pre-loaded as X and y.\n#\n# In addition to C, logistic regression has a \'penalty\' hyperparameter which specifies whether to use \'l1\' or \'l2\'\n# regularization. Your job in this exercise is to create a hold-out set, tune the \'C\' and \'penalty\' hyperparameters\n# of a logistic regression classifier using GridSearchCV on the training set, and then evaluate its performance\n# against the hold-out set.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + \'diabetes.csv\')\n\n# Create arrays for features and target variable\nX = df.drop(\'diabetes\', axis=1)\ny = df[\'diabetes\']\n\n# Create the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {\'C\': c_space, \'penalty\': [\'l1\', \'l2\']}\n\n# Instantiate the logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train, y_train)\n\n# Print the optimal parameters and best score\nprint(""Tuned Logistic Regression Parameter: {}"".format(logreg_cv.best_params_))\nprint(""Tuned Logistic Regression Accuracy: {}"".format(logreg_cv.best_score_))\n'"
src/ml-supervised/py_hyperparamter_tuning_hold-out_set_with_GridSearchCV-2.py,1,"b'# Hold-out set in practice II: Regression\n#\n# Remember lasso and ridge regression from the previous chapter? Lasso used\n# the L1 penalty to regularize, while ridge used the L2 penalty. There is another type of regularized regression\n# known as the elastic net. In elastic net regularization, the penalty term is a linear combination of the L1 and L2\n# penalties:\n#\n# a\xe2\x88\x97L1+b\xe2\x88\x97L2 In scikit-learn, this term is represented by the \'l1_ratio\' parameter: An \'l1_ratio\' of 1 corresponds to\n# an L1 penalty, and anything lower is a combination of L1 and L2.\n#\n# In this exercise, you will GridSearchCV to tune the \'l1_ratio\' of an elastic net model trained on the Gapminder\n# data. As in the previous exercise, use a hold-out set to evaluate your model\'s performance.\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + \'diabetes.csv\')\n\n# Create arrays for features and target variable\nX = df.drop(\'diabetes\', axis=1)\ny = df[\'diabetes\']\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the hyperparameter grid\nl1_space = np.linspace(0, 1, 30)\nparam_grid = {\'l1_ratio\': l1_space}\n\n# Instantiate the ElasticNet regressor: elastic_net\nelastic_net = ElasticNet()\n\n# Setup the GridSearchCV object: gm_cv\ngm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n\n# Fit it to the training data\ngm_cv.fit(X_train, y_train)\n\n# Predict on the test set and compute metrics\ny_pred = gm_cv.predict(X_test)\nr2 = gm_cv.score(X_test, y_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(""Tuned ElasticNet l1 ratio: {}"".format(gm_cv.best_params_))\nprint(""Tuned ElasticNet R squared: {}"".format(r2))\nprint(""Tuned ElasticNet MSE: {}"".format(mse))\n'"
src/ml-supervised/py_hyperparamter_tuning_with_GridSearchCV.py,1,"b'# Hyperparameter tuning with GridSearchCV\n#\n# Hugo demonstrated how to use to tune the n_neighbors parameter of the\n# KNeighborsClassifier() using GridSearchCV on the voting dataset. You will now practice this yourself, but by using\n# logistic regression on the diabetes dataset instead!\n#\n# Like the alpha parameter of lasso and ridge regularization that you saw earlier, logistic regression also has a\n# regularization parameter: C. C controls the inverse of the regularization strength, and this is what you will tune\n# in this exercise. A large C can lead to an overfit model, while a small C can lead to an underfit model.\n#\n# The hyperparameter space for C has been setup for you. Your job is to use GridSearchCV and logistic regression to\n# find the optimal C in this hyperparameter space. The feature array is available as X and target variable array is\n# available as y.\n#\n# You may be wondering why you aren\'t asked to split the data into training and test sets. Good observation! Here,\n# we want you to focus on the process of setting up the hyperparameter grid and performing grid-search\n# cross-validation. In practice, you will indeed want to hold out a portion of your data for evaluation purposes,\n# and you will learn all about this in the next video!\n\nimport numpy as np\nimport pandas as pd\n# Import necessary modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + \'diabetes.csv\')\n\n# Create arrays for features and target variable\nX = df.drop(\'diabetes\', axis=1)\ny = df[\'diabetes\']\n\n# Setup the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {\'C\': c_space}\n\n# Instantiate a logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(""Tuned Logistic Regression Parameters: {}"".format(logreg_cv.best_params_))\nprint(""Best score is {}"".format(logreg_cv.best_score_))\n'"
src/ml-supervised/py_hyperparamter_tuning_with_RandomizedSearchCV.py,0,"b'# Hyperparameter tuning with RandomizedSearchCV\n#\n# GridSearchCV can be computationally expensive, especially if you are\n# searching over a large hyperparameter space and dealing with multiple hyperparameters. A solution to this is to use\n#  RandomizedSearchCV, in which not all hyperparameter values are tried out. Instead, a fixed number of\n# hyperparameter settings is sampled from specified probability distributions. You\'ll practice using\n# RandomizedSearchCV in this exercise and see how this works.\n#\n# Here, you\'ll also be introduced to a new model: the Decision Tree. Don\'t worry about the specifics of how this\n# model works. Just like k-NN, linear regression, and logistic regression, decision trees in scikit-learn have .fit()\n#  and .predict() methods that you can use in exactly the same way as before. Decision trees have many parameters\n# that can be tuned, such as max_features, max_depth, and min_samples_leaf: This makes it an ideal use case for\n# RandomizedSearchCV.\n#\n# As before, the feature array X and target variable array y of the diabetes dataset have been pre-loaded. The\n# hyperparameter settings have been specified for you. Your goal is to use RandomizedSearchCV to find the optimal\n# hyperparameters. Go for it!\n\n# Import the necessary modules\nimport pandas as pd\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + \'diabetes.csv\')\n\n# Create arrays for features and target variable\nX = df.drop(\'diabetes\', axis=1)\ny = df[\'diabetes\']\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {""max_depth"": [3, None],\n              ""max_features"": randint(1, 9),\n              ""min_samples_leaf"": randint(1, 9),\n              ""criterion"": [""gini"", ""entropy""]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(""Tuned Decision Tree Parameters: {}"".format(tree_cv.best_params_))\nprint(""Best score is {}"".format(tree_cv.best_score_))\n'"
src/ml-supervised/py_knn_classifier_modal.py,0,"b""#\n# k-Nearest Neighbors: Fit Having explored the Congressional voting records dataset, it is time now to build your\n# first classifier. In this exercise, you will fit a k-Nearest Neighbors classifier to the voting dataset,\n# which has once again been pre-loaded for you into a DataFrame df.\n#\n# In the video, Hugo discussed the importance of ensuring your data adheres to the format required by the\n# scikit-learn API. The features need to be in an array where each column is a feature and each row a different\n# observation or data point - in this case, a Congressman's voting record. The target needs to be a single column\n# with the same number of observations as the feature data. We have done this for you in this exercise. Notice we\n# named the feature array X and response variable y: This is in accordance with the common scikit-learn practice.\n#\n# Your job is to create an instance of a k-NN classifier with 6 neighbors (by specifying the n_neighbors parameter)\n# and then fit it to the data. The data has been pre-loaded into a DataFrame called df. #\n\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom helper import path\n\n# this dataset won't work. Can't run this program.\nfile = 'house-votes-84.csv'\n\ndf = pd.read_csv(path + file)\n\n# Explore Data\nprint(df.describe())\n\n# Create arrays for the features(X) and the response variable/target(y)\nX = df.drop('party', axis=1).values\ny = df['party'].values\n\n# Create a k-NN classifier with 6 neighbors\nknn = KNeighborsClassifier(n_neighbors=60)\n\n# Fit the classifier to the data\nknn.fit(X, y)\n\n# Predict the labels for the training data X\ny_pred = knn.predict(X)\n\n# This is our prediction based of knn-classifier - Prediction: ['democrat']\nprint(y_pred)\n"""
src/ml-supervised/py_knn_classifier_modal_train_test.py,3,"b""# Import necessary modules\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Load the digits dataset: digits\ndigits = datasets.load_digits()\n\n# Create feature and target arrays\nX = digits.data\ny = digits.target\n\n# Split into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Create a k-NN classifier with 7 neighbors: knn\nknn = KNeighborsClassifier(n_neighbors=7)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Print the accuracy\nprint(knn.score(X_test, y_test))\n\n# -------------------------#\n\n# Test model accuracy on Training data and Test data and plot a graph\n\n# Setup arrays to store train and test accuracies\nneighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n\n    # Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    # Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()\n"""
src/ml-supervised/py_knn_classifiers_performance_metrics.py,0,"b""# Metrics for classification\n#\n# In Chapter 1, you evaluated the performance of your k-NN classifier based on its\n# accuracy. However, as Andy discussed, accuracy is not always an informative metric. In this exercise, you will dive\n#  more deeply into evaluating the performance of binary classifiers by computing a confusion matrix and generating a\n#  classification report.\n#\n# You may have noticed in the video that the classification report consisted of three rows, and an additional support\n#  column. The support gives the number of samples of the true response that lie in that class - so in the video\n# example, the support was the number of Republicans or Democrats in the test set on which the classification report\n# was computed. The precision, recall, and f1-score columns, then, gave the respective metrics for that particular\n# class.\n#\n# Here, you'll work with the PIMA Indians dataset obtained from the UCI Machine Learning Repository. The goal is to\n# predict whether or not a given female patient will contract diabetes based on features such as BMI, age,\n# and number of pregnancies. Therefore, it is a binary classification problem. A target value of 0 indicates that the\n#  patient does not have diabetes, while a value of 1 indicates that the patient does have diabetes. As in Chapters 1\n#  and 2, the dataset has been preprocessed to deal with missing values.\n#\n# The dataset has been loaded into a DataFrame df and the feature and target variable arrays X and y have been\n# created for you. In addition, sklearn.model_selection.train_test_split and sklearn.neighbors.KNeighborsClassifier\n# have already been imported.\n#\n# Your job is to train a k-NN classifier to the data and evaluate its performance by generating a confusion matrix\n# and classification report.\n\n\n# Import numpy and pandas\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n# Import necessary modules\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + 'diabetes.csv')\n\n# Create arrays for features and target variable\nX = df['age'].values\ny = df['diabetes'].values\n\n# Reshape X and y\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n# Import necessary modules\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Instantiate a k-NN classifier: knn\nknn = KNeighborsClassifier(n_neighbors=6)\n\n# Fit the classifier to the training data\nknn.fit(X_train, y_train)\n\n# Predict the labels of the test data: y_pred\ny_pred = knn.predict(X_test)\n\n# Generate the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n"""
src/ml-supervised/py_lasso_regularized_linear_regression.py,0,"b""# What is Lasso Regression?\n#\n# http://www.statisticshowto.com/lasso-regression/\n#\n# Lasso regression is a type of linear\n# regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean.\n# The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of\n# regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain\n# parts of model selection, like variable selection/parameter elimination.\n#\n# The acronym \xe2\x80\x9cLASSO\xe2\x80\x9d stands for Least Absolute Shrinkage and Selection Operator. Lasso regression is what is called\n# the Penalized regression method, often used in machine learning to select the subset of variables. It is a\n# supervised machine learning method. Specifically, LASSO is a Shrinkage and Variable Selection method for linear\n# regression models. LASSO, is actually an acronym for Least Absolute Selection and Shrinkage Operator.\n#\n# 0:34\n#\n# The LASSO imposes a constraint on the sum of the absolute values of the model parameters, where the sum has a\n# specified constant as an upper bound. This constraint causes regression coefficients for some variables to shrink\n# towards zero. This is the shrinkage process. The shrinkage process allows for better interpretation of the model\n# and identifies the variables most strongly associated with the target corresponds variable. That is the variable\n# selection process. It goes to obtain the subset of predictors that minimizes prediction error.\n#\n# So why use Lasso instead of just using ordinary least squares multiple regression?\n#\n# Well, first, it can provide greater prediction accuracy. If the true relationship between the response variable and\n#  the predictors is approximately linear and you have a large number of observations, then OLS regression parameter\n# estimates will have low bias and low variance. However, if you have a relatively small number of observations and a\n#  large number of predictors, then the variance of the OLS perimeter estimates will be higher. In this case,\n# Lasso Regression is useful because shrinking the regression coefficient can reduce variance without a substantial\n# increase in bias. 1:43 Second, Lasso Regression can increase model interpretability. Often times, at least some of\n# the explanatory variables in an OLS multiple regression analysis are not really associated with the response\n# variable. As a result, we often end up with a model that's over fitted and more difficult to interpret. With Lasso\n# Regression, the regression coefficients for unimportant variables are reduced to zero which effectively removes\n# them from the model and produces a simpler model that selects only the most important predictors. In Lasso\n# Regression, a tuning parameter called lambda is applied to the regression model to control the strength of the\n# penalty. As lambda increases, more coefficients are reduced to zero that is fewer predictors are selected and there\n#  is more shrinkage of the non-zero coefficient. With Lasso Regression where lambda is equal to zero then we have an\n#  OLS regression analysis. Bias increases and variance decreases as lambda increases. To demonstrate how lasso\n# regression works, let's use and example from the ad help data set in which our goal is to identify a set of\n# variables that best predicts the extent to which students feel connected to their school. We will use the same\n# ad-health data set that we used for the decision tree in random forced machine learning applications. The response\n# or target variable is a quantitative variable that measures school connectedness. The response values range from 6\n# to 38, where higher values indicate a greater connection with the school. There are a total of 23 Categorical and\n# Quantitative predictor variables. This is a pretty large number of predictor variables, so using OLS multiple\n# regression analysis would not be ideal, particularly if the goal is to identify a smaller subset of these\n# predictors that most accurately predicts school connectedness. Categorical predictors include gender and race and\n# ethnicity. Although Lasso Regression models can handle categorical variables with more than two levels In\n# conducting my data management, I created a series of five binary categorical variables for race and ethnicity,\n# Hispanic, White, Black, Native American, and Asian. I did this to improve interpratability of the selected model.\n# Binary substitutes variables for measure with individual questions of about whether the adolescent had ever used\n# alcohol, marijuana, cocaine, or inhalants. Additional categorical variables include the availability of cigarettes\n# in the home, whether or not either parent was on public assistance, and any experience with being expelled from\n# school. Finally, quantitative predictive variables include age, alcohol problems, and a measure of deviance. That\n# includes such behaviors as vandalism, other property damage, lying, stealing, running away,\n\nimport matplotlib.pyplot as plt\n# Import the necessary modules\nimport pandas as pd\n# Import Lasso\nfrom sklearn.linear_model import Lasso\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + 'gm_2008_region.csv')\nprint(df.info())\nprint(df.describe())\nprint(df.head())\n\n# Create arrays for features and target variable\nX = df['population'].values\ny = df['life'].values\n\n# Reshape X and y\nX = X.reshape(-1, 1)\n# y = y.reshape(-1, 1)\n\n# Instantiate a lasso regressor: lasso\nlasso = Lasso(alpha=0.4, normalize=True)\n\n# Fit the regressor to the data\nlasso.fit(X, y)\n\n# Compute and print the coefficients\nlasso_coef = lasso.coef_\nprint(lasso_coef)\n\ndf_columns = df.keys()\nprint(df_columns)\n\n# Plot the coefficients\nplt.plot(range(len(df_columns)), lasso_coef)\nplt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\nplt.margins(0.02)\nplt.show()\n"""
src/ml-supervised/py_linear_regression_modal.py,1,"b'# Fit & predict for regression\n#\n# Now, you will fit a linear regression and predict life expectancy using just one\n# feature. You saw Andy do this earlier using the \'RM\' feature of the Boston housing dataset. In this exercise,\n# you will use the \'fertility\' feature of the Gapminder dataset. Since the goal is to predict life expectancy,\n# the target variable here is \'life\'. The array for the target variable has been pre-loaded as y and the array for\n# \'fertility\' has been pre-loaded as X_fertility.\n#\n# A scatter plot with \'fertility\' on the x-axis and \'life\' on the y-axis has been generated. As you can see,\n# there is a strongly negative correlation, so a linear regression should be able to capture this trend. Your job is\n# to fit a linear regression and then predict the life expectancy, overlaying these predicted values on the plot to\n# generate a regression line. You will also compute and print the R2 score using sckit-learn\'s .score() method.\n\n# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n# Import LinearRegression\nfrom sklearn.linear_model import LinearRegression\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + \'gm_2008_region.csv\')\n\n# Create arrays for features and target variable\nX_fertility = df[\'fertility\'].values\ny = df[\'life\'].values\n\n# Print the dimensions of X and y before reshaping\nprint(""Dimensions of y before reshaping: {}"".format(y.shape))\nprint(""Dimensions of X before reshaping: {}"".format(X_fertility.shape))\n\n# Reshape X and y\ny = y.reshape(-1, 1)\nX_fertility = X_fertility.reshape(-1, 1)\n\n# Print the dimensions of X and y after reshaping\nprint(""Dimensions of y after reshaping: {}"".format(y.shape))\nprint(""Dimensions of X after reshaping: {}"".format(X_fertility.shape))\n\n# Create the regressor: reg\nreg = LinearRegression()\n\n# Create the prediction space\nprediction_space = np.linspace(min(X_fertility), max(X_fertility)).reshape(-1, 1)\n\n# Fit the model to the data\nreg.fit(X_fertility, y)\n\n# Compute predictions over the prediction space: y_pred\ny_pred = reg.predict(prediction_space)\n\n# Print R^2\nprint(reg.score(X_fertility, y))\n\n# Plot regression line over original scatter plot\nplt.scatter(X_fertility, y, color=\'blue\')\nplt.plot(prediction_space, y_pred, color=\'black\', linewidth=3)\nplt.show()\n'"
src/ml-supervised/py_linear_regression_modal_train_test.py,1,"b'# Fit & predict for regression\n# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\n# Import necessary modules\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + \'gm_2008_region.csv\')\n\n# Create arrays for features and target variable\nX_fertility = df[\'fertility\'].values\ny = df[\'life\'].values\n\n# Reshape X and y\nX_fertility = X_fertility.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_fertility, y, test_size=0.3, random_state=42)\n\n# Create the regressor: reg_all\nreg_all = LinearRegression()\n\n# Fit the regressor to the training data\nreg_all.fit(X_train, y_train)\n\n# Predict on the test data: y_pred\ny_pred = reg_all.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(""R^2: {}"".format(reg_all.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(""Root Mean Squared Error: {}"".format(rmse))\n'"
src/ml-supervised/py_logistic_regression_modal.py,0,"b""# Building a logistic regression model\n#\n# Time to build your first logistic regression model! As Hugo showed in the\n# video, scikit-learn makes it very easy to try different models, since the Train-Test-Split/Instantiate/Fit/Predict\n# paradigm applies to all classifiers and regressors - which are known in scikit-learn as 'estimators'. You'll see\n# this now for yourself as you train a logistic regression model on exactly the same data as in the previous\n# exercise. Will it outperform k-NN? There's only one way to find out!\n#\n# The feature and target variable arrays X and y have been pre-loaded, and train_test_split has been imported for you\n#  from sklearn.model_selection.\n\n# Import the necessary modules\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + 'diabetes.csv')\n\n# Create arrays for features and target variable\nX = df['age'].values\ny = df['diabetes'].values\n\n# Reshape X and y\nX = X.reshape(-1, 1)\ny = y.reshape(-1, 1)\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Create the classifier: logreg\nlogreg = LogisticRegression()\n\n# Fit the classifier to the training data\nlogreg.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\n\n# -------------------------\n\n# Plotting an ROC curve\n#\n# Classification reports and confusion matrices are great methods to quantitatively evaluate model performance,\n# while ROC curves provide a way to visually evaluate models. As Hugo demonstrated in the video, most classifiers in\n# scikit-learn have a .predict_proba() method which returns the probability of a given sample being in a particular\n# class. Having built a logistic regression model, you'll now evaluate its performance by plotting an ROC curve. In\n# doing so, you'll make use of the .predict_proba() method and become familiar with its functionality.\n#\n# Here, you'll continue working with the PIMA Indians diabetes dataset. The classifier has already been fit to the\n# training data and is available as logreg\n\n# Import necessary modules\nfrom sklearn.metrics import roc_curve\nfrom matplotlib import pyplot as plt\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:, 1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\n\n# -------------------------\n\n# Calculating ROC AUC score\n# Larger area under ROC curve = better model\n\nfrom sklearn.metrics import roc_auc_score\n\nprint(roc_auc_score(y_test, y_pred_prob))\n"""
src/ml-supervised/py_ridge_regularized_linear_regression.py,5,"b""# Regularization II: Ridge\n#\n# Lasso is great for feature selection, but when building regression models,\n# Ridge regression should be your first choice.\n#\n# Recall that lasso performs regularization by adding to the loss function a penalty term of the absolute value of\n# each coefficient multiplied by some alpha. This is also known as L1 regularization because the regularization term\n# is the L1 norm of the coefficients. This is not the only way to regularize, however.\n#\n# If instead you took the sum of the squared values of the coefficients multiplied by some alpha - like in Ridge\n# regression - you would be computing the L2 norm. In this exercise, you will practice fitting ridge regression\n# models over a range of different alphas, and plot cross-validated R2 scores for each, using this function that we\n# have defined for you, which plots the R2 score as well as standard error for each alpha:\n\n\ndef display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1, 1, 1)\n    ax.plot(alpha_space, cv_scores)\n\n    std_error = cv_scores_std / np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + 'gm_2008_region.csv')\nprint(df.info())\nprint(df.describe())\nprint(df.head())\n\n# Create arrays for features and target variable\nX = df['population'].values\ny = df['life'].values\n\n# Reshape X and y\nX = X.reshape(-1, 1)\n# y = y.reshape(-1, 1)\n\n# Setup the array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n\n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge, X, y, cv=10)\n\n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n\n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)\n"""
src/ml-supervised/py_sklearn_digits_dataset.py,0,"b""# The digits recognition dataset Up until now, you have been performing binary classification, since the target\n# variable had two possible outcomes. Hugo, however, got to perform multi-class classification in the videos,\n# where the target variable could take on three possible outcomes. Why does he get to have all the fun?! In the\n# following exercises, you'll be working with the MNIST digits recognition dataset, which has 10 classes, the digits\n# 0 through 9! A reduced version of the MNIST dataset is one of scikit-learn's included datasets, and that is the one\n#  we will use in this exercise.\n#\n# Each sample in this scikit-learn dataset is an 8x8 image representing a handwritten digit. Each pixel is\n# represented by an integer in the range 0 to 16, indicating varying levels of black. Recall that scikit-learn's\n# built-in datasets are of type Bunch, which are dictionary-like objects. Helpfully for the MNIST dataset,\n# scikit-learn provides an 'images' key in addition to the 'data' and 'target' keys that you have seen with the Iris\n# data. Because it is a 2D array of the images corresponding to each sample, this 'images' key is useful for\n# visualizing the images, as you'll see in this exercise (for more on plotting 2D arrays, see Chapter 2 of DataCamp's\n#  course on Data Visualization with Python). On the other hand, the 'data' key contains the feature array - that is,\n#  the images as a flattened array of 64 pixels.\n#\n# Notice that you can access the keys of these Bunch objects in two different ways: By using the . notation,\n# as in digits.images, or the [] notation, as in digits['images'].\n#\n# For more on the MNIST data, check out this exercise in Part 1 of DataCamp's Importing Data in Python course. There,\n#  the full version of the MNIST dataset is used, in which the images are 28x28. It is a famous dataset in machine\n# learning and computer vision, and frequently used as a benchmark to evaluate the performance of a new model.\n\nimport matplotlib.pyplot as plt\n# Import necessary modules\nfrom sklearn import datasets\n\n# Load the digits dataset: digits\ndigits = datasets.load_digits()\n\n# Print the keys and DESCR of the dataset\nprint(digits.keys())\nprint(digits.DESCR)\n\n# Print the shape of the images and data keys\nprint(digits.images.shape)\nprint(digits.data.shape)\n\n# Display digit 1010\nplt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()"""
src/ml-unsupervised/__init__.py,0,b''
src/ml-unsupervised/k-means_clustering.py,1,"b""import numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\nfrom helper import path\n\n# Read the CSV file into a DataFrame: df\ndf = pd.read_csv(path + 'data_1024.csv', sep='\\t')\n\nf1 = df['Distance_Feature'].values\nf2 = df['Speeding_Feature'].values\n\nX = np.matrix(zip(f1, f2))\nkmeans = KMeans(n_clusters=2).fit(X)\n"""
src/python_core/__init__.py,0,b''
src/ml-unsupervised/01-clustering-for-dataset-exploration/01-how-many-clusters.py,1,"b'""""""\nHow many clusters?\n\nYou are given an array points of size 300x2, where each row gives the (x, y) co-ordinates of a point on a map. Make a\nscatter plot of these points, and use the scatter plot to guess how many clusters there are.\n\nmatplotlib.pyplot has already been imported as plt. In the IPython Shell:\n\nCreate an array called xs that contains the values of points[:,0] - that is, column 0 of points.\nCreate an array called ys that contains the values of points[:,1] - that is, column 1 of points.\nMake a scatter plot by passing xs and ys to the plt.scatter() function.\nCall the plt.show() function to show your plot.\nHow many clusters do you see?\n""""""\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom helper import points\n\nprint(type(points))\npoints = np.array(points)\n\nxs = points[:, 0]\n\nys = points[:, 1]\n\nplt.scatter(xs, ys, alpha=0.5)\n\nplt.show()\n'"
src/ml-unsupervised/01-clustering-for-dataset-exploration/02-clustering-2d-points.py,2,"b""'''\nClustering 2D points\n\nFrom the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now\ncreate a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model\nhas been fit, you'll obtain the cluster labels for some new points using the .predict() method.\n\nYou are given the array points from the previous exercise, and also an array new_points.\n\nINSTRUCTIONS\n100XP\nINSTRUCTIONS\n100XP\nImport KMeans from sklearn.cluster.\nUsing KMeans(), create a KMeans instance called model to find 3 clusters. To specify the number of clusters, use the n_clusters keyword argument.\nUse the .fit() method of model to fit the model to the array of points points.\nUse the .predict() method of model to predict the cluster labels of new_points, assigning the result to labels.\nHit 'Submit Answer' to see the cluster labels of new_points.\n'''\nimport numpy as np\n# Import KMeans\nfrom sklearn.cluster import KMeans\n\nfrom helper import points, new_points, smart_print\n\n# Convert to np-arrays\npoints = np.array(points)\nnew_points = np.array(new_points)\n\n# Create a KMeans instance with 3 clusters: model\nmodel = KMeans(n_clusters=3)\n\n# Fit model to points\nmodel.fit(points)\n\n# Determine the cluster labels of new_points: labels\nlabels = model.predict(new_points)\n\n# Print cluster labels of new_points\nsmart_print(labels)\n"""
src/ml-unsupervised/01-clustering-for-dataset-exploration/03-inspect-your-clustering.py,0,"b""'''\nInspect your clustering\n\nLet's now inspect the clustering you performed in the previous exercise!\n\nA solution to the previous exercise has already run, so new_points is an array of points and labels is the array of their cluster labels.\n\nINSTRUCTIONS\n100XP\nImport matplotlib.pyplot as plt.\nAssign column 0 of new_points to xs, and column 1 of new_points to ys.\nMake a scatter plot of xs and ys, specifying the c=labels keyword arguments to color the points by their cluster label. Also specify alpha=0.5.\nCompute the coordinates of the centroids using the .cluster_centers_ attribute of model.\nAssign column 0 of centroids to centroids_x, and column 1 of centroids to centroids_y.\nMake a scatter plot of centroids_x and centroids_y, using 'D' (a diamond) as a marker by specifying the marker parameter. Set the size of the markers to be 50 using s=50.\n'''\n# Import pyplot\nimport matplotlib.pyplot as plt\nfrom numpy import array\nfrom sklearn.cluster import KMeans\n\nfrom helper import points, new_points\n\n# Convert to np-arrays\npoints = array(points)\nnew_points = array(new_points)\n\n# Create a KMeans instance with 3 clusters: model\nmodel = KMeans(n_clusters=3)\n\n# Fit model to points\nmodel.fit(points)\n\n# Determine the cluster labels of new_points: labels\nlabels = model.predict(new_points)\n# Assign the columns of new_points: xs and ys\nxs = new_points[:, 0]\nys = new_points[:, 1]\n\n# Make a scatter plot of xs and ys, using labels to define the colors\nplt.scatter(xs, ys, alpha=0.5, c=labels)\n\n# Assign the cluster centers: centroids\ncentroids = model.cluster_centers_\n\n# Assign the columns of centroids: centroids_x, centroids_y\ncentroids_x = centroids[:, 0]\ncentroids_y = centroids[:, 1]\n\n# Make a scatter plot of centroids_x and centroids_y\nplt.scatter(centroids_x, centroids_y, marker='D', s=50)\nplt.show()\n"""
src/ml-unsupervised/01-clustering-for-dataset-exploration/04-how-many-clusters-of-grain.py,1,"b""'''\nHow many clusters of grain?\n\nIn the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You\nare given an array samples containing the measurements (such as area, perimeter, length, and several others) of\nsamples of grain. What's a good number of clusters in this case?\n\nKMeans and PyPlot (plt) have already been imported for you.\n\nThis dataset was sourced from the UCI Machine Learning Repository.\n\nINSTRUCTIONS\n100XP\nFor each of the given values of k, perform the following steps:\nCreate a KMeans instance called model with k clusters.\nFit the model to the grain data samples.\nAppend the value of the inertia_ attribute of model to the list inertias.\nThe code to plot ks vs inertias has been written for you, so hit 'Submit Answer' to see the plot!\n'''\n# Import pyplot\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nfrom helper import points\n\nsamples = np.array(points)\n\nks = range(1, 6)\ninertias = []\n\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k)\n\n    # Fit model to samples\n    model.fit(samples)\n\n    # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n\n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()\n"""
src/ml-unsupervised/01-clustering-for-dataset-exploration/05-evaluating-the-grain-clustering.py,1,"b'\'\'\'\nEvaluating the grain clustering\n\nIn the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data.\nIn fact, the grain samples come from a mix of 3 different grain varieties: ""Kama"", ""Rosa"" and ""Canadian"". In this\nexercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a\ncross-tabulation.\n\nYou have the array samples of grain samples, and a list varieties giving the grain variety for each sample. Pandas (\npd) and KMeans have already been imported for you.\n\nINSTRUCTIONS 100XP Create a KMeans model called model with 3 clusters. Use the .fit_predict() method of model to fit\nit to samples and derive the cluster labels. Using .fit_predict() is the same as using .fit() followed by .predict().\nCreate a DataFrame df with two columns named \'labels\' and \'varieties\', using labels and varieties, respectively,\nfor the column values. This has been done for you. Use the pd.crosstab() function on df[\'labels\'] and df[\'varieties\']\nto count the number of times each grain variety coincides with each cluster label. Assign the result to ct. Hit\n\'Submit Answer\' to see the cross-tabulation! \'\'\'\n\n# Import pyplot\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\n\nfrom helper import points\n\nsamples = np.array(points)\n\n# Create a KMeans model with 3 clusters: model\nmodel = KMeans(n_clusters=3)\n\n# Use fit_predict to fit model and obtain cluster labels: labels\nlabels = model.fit_predict(samples)\n\n# Create a DataFrame with labels and varieties as columns: df\ndf = pd.DataFrame({\'labels\': labels, \'varieties\': varieties})\n\n# Create crosstab: ct\nct = pd.crosstab(df[\'labels\'], df[\'varieties\'])\n\n# Display ct\nprint(ct)\n'"
src/ml-unsupervised/01-clustering-for-dataset-exploration/06-07-scaling-&-clustering-the-fish-data.py,1,"b""'''\nScaling fish data for clustering\n\nYou are given an array samples giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you'll need to standardize these features first. In this exercise, you'll build a pipeline to standardize and cluster the data.\n\nThese fish measurement data were sourced from the Journal of Statistics Education.\n\nINSTRUCTIONS\n100XP\nINSTRUCTIONS\n100XP\nImport:\nmake_pipeline from sklearn.pipeline.\nStandardScaler from sklearn.preprocessing.\nKMeans from sklearn.cluster.\nCreate an instance of StandardScaler called scaler.\nCreate an instance of KMeans with 4 clusters called kmeans.\nCreate a pipeline called pipeline that chains scaler and kmeans. To do this, you just need to pass them in as arguments to make_pipeline().\n'''\nfrom sklearn.cluster import KMeans\n# Perform the necessary imports\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create KMeans instance: kmeans\nkmeans = KMeans(n_clusters=4)\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, kmeans)\n\n'''\nClustering the fish data\n\nYou'll now use your standardization and clustering pipeline from the previous exercise to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.\n\nAs before, samples is the 2D array of fish measurements. Your pipeline is available as pipeline, and the species of every fish sample is given by the list species.\n\nINSTRUCTIONS\n100XP\nImport pandas as pd.\nFit the pipeline to the fish measurements samples.\nObtain the cluster labels for samples by using the .predict() method of pipeline.\nUsing pd.DataFrame(), create a DataFrame df with two columns named 'labels' and 'species', using labels and species, respectively, for the column values.\nUsing pd.crosstab(), create a cross-tabulation ct of df['labels'] and df['species'].\n'''\n# Import pandas\nimport pandas as pd\nimport numpy as np\n\nfrom helper import points\n\nsamples = np.array(points)\n\n# Fit the pipeline to samples\npipeline.fit(samples)\n\n# Calculate the cluster labels: labels\nlabels = pipeline.predict(samples)\n\n# Create a DataFrame with labels and species as columns: df\ndf = pd.DataFrame({'labels': labels, 'species': species})\n\n# Create crosstab: ct\nct = pd.crosstab(df['labels'], df['species'])\n\n# Display ct\nprint(ct)\n"""
src/ml-unsupervised/01-clustering-for-dataset-exploration/08-09-scaling-&-clustering-which-stocks-move-together.py,0,"b""'''\nClustering stocks using KMeans\n\nIn this exercise, you'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.\n\nSome stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.\n\nNote that Normalizer() is different to StandardScaler(), which you used in the previous exercise. While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, Normalizer() rescales each sample - here, each company's stock price - independently of the other.\n\nKMeans and make_pipeline have already been imported for you.\n\nINSTRUCTIONS\n100XP\nINSTRUCTIONS\n100XP\nImport Normalizer from sklearn.preprocessing.\nCreate an instance of Normalizer called normalizer.\nCreate an instance of KMeans called kmeans with 10 clusters.\nUsing make_pipeline(), create a pipeline called pipeline that chains normalizer and kmeans.\nFit the pipeline to the movements array.\n'''\nfrom sklearn.cluster import KMeans\n# Import Normalizer\nfrom sklearn.preprocessing import Normalizer\n\n# Create a normalizer: normalizer\nnormalizer = Normalizer()\n\n# Create a KMeans model with 10 clusters: kmeans\nkmeans = KMeans(n_clusters=10)\n\n# Make a pipeline chaining normalizer and kmeans: pipeline\npipeline = make_pipeline(normalizer, kmeans)\n\n# Fit pipeline to the daily price movements\npipeline.fit(movements)\n\n'''\nWhich stocks move together?\n\nIn the previous exercise, you clustered companies by their daily stock price movements. So which company have stock \nprices that tend to change in the same way? You'll now inspect the cluster labels from your clustering to find out. \n\nYour solution to the previous exercise has already been run. Recall that you constructed a Pipeline pipeline\ncontaining a KMeans model and fit it to the NumPy array movements of daily stock movements. In addition,\na list companies of the company names is available.\n\nINSTRUCTIONS 100XP INSTRUCTIONS 100XP Import pandas as pd. Use the .predict() method of the pipeline to predict the\nlabels for movements. Align the cluster labels with the list of company names companies by creating a DataFrame df\nwith labels and companies as columns. This has been done for you. Use the .sort_values() method of df to sort the\nDataFrame by the 'labels' column, and print the result. Hit 'Submit Answer' and take a moment to see which companies\nare together in each cluster! '''\n# Import pandas\nimport pandas as pd\n\n# Predict the cluster labels: labels\nlabels = pipeline.predict(movements)\n\n# Create a DataFrame aligning labels and companies: df\ndf = pd.DataFrame({'labels': labels, 'companies': companies})\n\n# Display df sorted by cluster label\nprint(df.sort_values('labels'))\n"""
src/ml-unsupervised/02-visualization-with-hierarchical-clustering-and-t-sne/01-hierarchical-clustering-of-the-grain-data.py,0,"b""'''\nHierarchical clustering of the grain data\n\nIn the video, you learned that the SciPy linkage() function performs hierarchical clustering on an array of samples.\nUse the linkage() function to obtain a hierarchical clustering of the grain samples, and use dendrogram() to\nvisualize the result. A sample of the grain measurements is provided in the array samples, while the variety of each\ngrain sample is given by the list varieties.\n\nINSTRUCTIONS 100XP Import: linkage and dendrogram from scipy.cluster.hierarchy. matplotlib.pyplot as plt. Perform\nhierarchical clustering on samples using the linkage() function with the method='complete' keyword argument. Assign\nthe result to mergings. Plot a dendrogram using the dendrogram() function on mergings. Specify the keyword arguments\nlabels=varieties, leaf_rotation=90, and leaf_font_size=6. '''\n\nimport matplotlib.pyplot as plt\n# Perform the necessary imports\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n# Calculate the linkage: mergings\nmergings = linkage(samples, method='complete')\n\n# Plot the dendrogram, using varieties as labels\ndendrogram(mergings,\n           labels=varieties,\n           leaf_rotation=90,\n           leaf_font_size=6,\n           )\nplt.show()\n"""
src/ml-unsupervised/02-visualization-with-hierarchical-clustering-and-t-sne/02-hierarchies-of-stocks.py,0,"b""'''\nHierarchies of stocks\n\nIn chapter 1, you used k-means clustering to cluster companies according to their stock price movements. Now,\nyou'll perform hierarchical clustering of the companies. You are given a NumPy array of price movements movements,\nwhere the rows correspond to companies, and a list of the company names companies. SciPy hierarchical clustering\ndoesn't fit into a sklearn pipeline, so you'll need to use the normalize() function from sklearn.preprocessing\ninstead of Normalizer.\n\nlinkage and dendrogram have already been imported from sklearn.cluster.hierarchy, and PyPlot has been imported as plt.\n\nINSTRUCTIONS\n100XP\nINSTRUCTIONS\n100XP\nImport normalize from sklearn.preprocessing.\nRescale the price movements for each stock by using the normalize() function on movements.\nApply the linkage() function to normalized_movements, using 'complete' linkage, to calculate the hierarchical clustering. Assign the result to mergings.\nPlot a dendrogram of the hierarchical clustering, using the list companies of company names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you did in the previous exercise.\n'''\n# Import normalize\nfrom sklearn.preprocessing import normalize\n\n# Normalize the movements: normalized_movements\nnormalized_movements = normalize(movements)\n\n# Calculate the linkage: mergings\nmergings = linkage(normalized_movements, method='complete')\n\n# Plot the dendrogram\ndendrogram(mergings, labels=companies, leaf_rotation=90, leaf_font_size=6)\nplt.show()\n"""
src/ml-unsupervised/02-visualization-with-hierarchical-clustering-and-t-sne/03-different-linkage-different-hierarchical-clustering.py,0,"b""'''\nDifferent linkage, different hierarchical clustering!\n\nIn the video, you saw a hierarchical clustering of the voting countries at the Eurovision song contest using\n'complete' linkage. Now, perform a hierarchical clustering of the voting countries with 'single' linkage, and compare\nthe resulting dendrogram with the one in the video. Different linkage, different hierarchical clustering!\n\nYou are given an array samples. Each row corresponds to a voting country, and each column corresponds to a\nperformance that was voted for. The list country_names gives the name of each voting country. This dataset was\nobtained from Eurovision.\n\nINSTRUCTIONS\n100XP\nINSTRUCTIONS\n100XP\nImport:\nlinkage and dendrogram from scipy.cluster.hierarchy.\nmatplotlib.pyplot as plt.\nPerform hierarchical clustering on samples using the linkage() function with the method='single' keyword argument. Assign the result to mergings.\nPlot a dendrogram of the hierarchical clustering, using the list country_names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you have done earlier.\n'''\n# Perform the necessary imports\nimport matplotlib.pyplot as plt\nfrom pytz import country_names\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n# Calculate the linkage: mergings\nmergings = linkage(samples, method='single')\n\n# Plot the dendrogram\ndendrogram(mergings, labels=country_names, leaf_rotation=90, leaf_font_size=6)\nplt.show()\n"""
src/ml-unsupervised/02-visualization-with-hierarchical-clustering-and-t-sne/04-extracting-the-cluster-labels.py,0,"b""'''\nExtracting the cluster labels\n\nIn the previous exercise, you saw that the intermediate clustering of the grain samples at height 6 has 3 clusters.\nNow, use the fcluster() function to extract the cluster labels for this intermediate clustering, and compare the\nlabels with the grain varieties using a cross-tabulation.\n\nThe hierarchical clustering has already been performed and mergings is the result of the linkage() function. The list\nvarieties gives the variety of each grain sample.\n\nINSTRUCTIONS\n100XP\nImport:\npandas as pd.\nfcluster from scipy.cluster.hierarchy.\nPerform a flat hierarchical clustering by using the fcluster() function on mergings. Specify a maximum height of 6 and the keyword argument criterion='distance'.\nCreate a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.\nCreate a cross-tabulation ct between df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label.\n'''\n# Perform the necessary imports\nimport pandas as pd\nfrom scipy.cluster.hierarchy import fcluster\n\n# Use fcluster to extract labels: labels\nlabels = fcluster(mergings, 6, criterion='distance')\n\n# Create a DataFrame with labels and varieties as columns: df\ndf = pd.DataFrame({'labels': labels, 'varieties': varieties})\n\n# Create crosstab: ct\nct = pd.crosstab(df['labels'], df['varieties'])\n\n# Display ct\nprint(ct)\n"""
src/ml-unsupervised/02-visualization-with-hierarchical-clustering-and-t-sne/05-tsne-visualization-of-grain-dataset.py,0,"b""'''\nt-SNE visualization of grain dataset\n\nIn the video, you saw t-SNE applied to the iris dataset. In this exercise, you'll apply t-SNE to the grain samples\ndata and inspect the resulting t-SNE features using a scatter plot. You are given an array samples of grain samples\nand a list variety_numbers giving the variety number of each grain sample.\n\nINSTRUCTIONS 100XP Import TSNE from sklearn.manifold. Create a TSNE instance called model with learning_rate=200.\nApply the .fit_transform() method of model to samples. Assign the result to tsne_features. Select the column 0 of\ntsne_features. Assign the result to xs. Select the column 1 of tsne_features. Assign the result to ys. Make a scatter\nplot of the t-SNE features xs and ys. To color the points by the grain variety, specify the additional keyword\nargument c=variety_numbers. '''\n# Import TSNE\nfrom sklearn.manifold import TSNE\n\n# Create a TSNE instance: model\nmodel = TSNE(learning_rate=200)\n\n# Apply fit_transform to samples: tsne_features\ntsne_features = model.fit_transform(samples)\n\n# Select the 0th feature: xs\nxs = tsne_features[:, 0]\n\n# Select the 1st feature: ys\nys = tsne_features[:, 1]\n\n# Scatter plot, coloring by variety_numbers\nplt.scatter(xs, ys, c=variety_numbers)\nplt.show()\n"""
src/ml-unsupervised/02-visualization-with-hierarchical-clustering-and-t-sne/06-a-tsne-map-of-the-stock-market.py,0,"b""'''\nA t-SNE map of the stock market\n\nt-SNE provides great visualizations when the individual samples can be labeled. In this exercise, you'll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives you a map of the stock market! The stock price movements for each company are available as the array normalized_movements (these have already been normalized for you). The list companies gives the name of each company. PyPlot (plt) has been imported for you.\n\nINSTRUCTIONS\n100XP\nINSTRUCTIONS\n100XP\nImport TSNE from sklearn.manifold.\nCreate a TSNE instance called model with learning_rate=50.\nApply the .fit_transform() method of model to normalized_movements. Assign the result to tsne_features.\nSelect column 0 and column 1 of tsne_features.\nMake a scatter plot of the t-SNE features xs and ys. Specify the additional keyword argument alpha=0.5.\nCode to label each point with its company name has been written for you using plt.annotate(), so just hit 'Submit Answer' to see the visualization!\n'''\n# Import TSNE\nfrom sklearn.manifold import TSNE\n\n# Create a TSNE instance: model\nmodel = TSNE(learning_rate=50)\n\n# Apply fit_transform to normalized_movements: tsne_features\ntsne_features = model.fit_transform(normalized_movements)\n\n# Select the 0th feature: xs\nxs = tsne_features[:,0]\n\n# Select the 1th feature: ys\nys = tsne_features[:,1]\n\n# Scatter plot\nplt.scatter(xs, ys, alpha=0.5)\n\n# Annotate the points\nfor x, y, company in zip(xs, ys, companies):\n    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)\nplt.show()\n"""
src/ml-unsupervised/03-decorrelating-your-data-and-dimension-reduction/01-correlated-data-in-nature.py,0,"b""'''\nCorrelated data in nature\n\nYou are given an array grains giving the width and length of samples of grain. You suspect that width and length will be correlated. To confirm this, make a scatter plot of width vs length and measure their Pearson correlation.\n\nINSTRUCTIONS\n100XP\nImport:\nmatplotlib.pyplot as plt.\npearsonr from scipy.stats.\nAssign column 0 of grains to width and column 1 of grains to length.\nMake a scatter plot with width on the x-axis and length on the y-axis.\nUse the pearsonr() function to calculate the Pearson correlation of width and length.\n'''\n\n# Perform the necessary imports\nimport matplotlib.pyplot as plt\nfrom grains_data_from_dataset import grains\nfrom scipy.stats import pearsonr\n\n# Assign the 0th column of grains: width\nwidth = grains[:, 0]\n\n# Assign the 1st column of grains: length\nlength = grains[:, 1]\n# Scatter plot width vs length\nplt.scatter(width, length)\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation\ncorrelation, pvalue = pearsonr(width, length)\n\n# Display the correlation\nprint(correlation)\n"""
src/ml-unsupervised/03-decorrelating-your-data-and-dimension-reduction/02-decorrelating-the-grain-measurements-with-pca.py,0,"b""'''\nDecorrelating the grain measurements with PCA\n\nYou observed in the previous exercise that the width and length measurements of the grain are correlated. Now, you'll use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation.\n\nINSTRUCTIONS\n100XP\nImport PCA from sklearn.decomposition.\nCreate an instance of PCA called model.\nUse the .fit_transform() method of model to apply the PCA transformation to grains. Assign the result to pca_features.\nThe subsequent code to extract, plot, and compute the Pearson correlation of the first two columns pca_features has been written for you, so hit 'Submit Answer' to see the result!\n'''\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n# Import PCA\nfrom sklearn.decomposition import PCA\n\nfrom grains_data_from_dataset import grains\n\n# Create PCA instance: model\nmodel = PCA()\n\n# Apply the fit_transform method of model to grains: pca_features\npca_features = model.fit_transform(grains)\n\n# Assign 0th column of pca_features: xs\nxs = pca_features[:, 0]\n\n# Assign 1st column of pca_features: ys\nys = pca_features[:, 1]\n\n# Scatter plot xs vs ys\nplt.scatter(xs, ys)\nplt.axis('equal')\nplt.show()\n\n# Calculate the Pearson correlation of xs and ys\ncorrelation, pvalue = pearsonr(xs, ys)\n\n# Display the correlation\nprint(correlation)\n"""
src/ml-unsupervised/03-decorrelating-your-data-and-dimension-reduction/03-the-first-principal-component.py,0,"b""'''\nThe first principal component\n\nThe first principal component of the data is the direction in which the data varies the most. In this exercise,\nyour job is to use PCA to find the first principal component of the length and width measurements of the grain\nsamples, and represent it as an arrow on the scatter plot.\n\nThe array grains gives the length and width of the grain samples. PyPlot (plt) and PCA have already been imported for\nyou.\n\nINSTRUCTIONS\n100XP\nINSTRUCTIONS\n100XP\nMake a scatter plot of the grain measurements. This has been done for you.\nCreate a PCA instance called model.\nFit the model to the grains data.\nExtract the coordinates of the mean of the data using the .mean_ attribute of model.\nGet the first principal component of model using the .components_[0,:] attribute.\nPlot the first principal component as an arrow on the scatter plot, using the plt.arrow() function. You have to specify the first two arguments - mean[0] and mean[1].\n'''\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\nfrom grains_data_from_dataset import grains\n\n# Make a scatter plot of the untransformed points\nplt.scatter(grains[:, 0], grains[:, 1])\n\n# Create a PCA instance: model\nmodel = PCA()\n\n# Fit model to points\nmodel.fit(grains)\n\n# Get the mean of the grain samples: mean\nmean = model.mean_\n\n# Get the first principal component: first_pc\nfirst_pc = model.components_[0, :]\n\n# Plot first_pc as an arrow, starting at mean\nplt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)\n\n# Keep axes on same scale\nplt.axis('equal')\nplt.show()\n"""
src/ml-unsupervised/03-decorrelating-your-data-and-dimension-reduction/04-variance-of-the-pca-features.py,1,"b""'''\nVariance of the PCA features\n\nThe fish dataset is 6-dimensional. But what is its intrinsic dimension? Make a plot of the variances of the PCA\nfeatures to find out. As before, samples is a 2D array, where each row represents a fish. You'll need to standardize\nthe features first.\n\nINSTRUCTIONS 100XP Create an instance of StandardScaler called scaler. Create a PCA instance called pca. Use the\nmake_pipeline() function to create a pipeline chaining scaler and pca. Use the .fit() method of pipeline to fit it to\nthe fish samples samples. Extract the number of components used using the .n_components_ attribute of pca. Place this\ninside a range() function and store the result as features. Use the plt.bar() function to plot the explained\nvariances, with features on the x-axis and pca.explained_variance_ on the y-axis. '''\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Perform the necessary imports\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom helper import points\n\nsamples = np.array(points)\n\n# Create scaler: scaler\nscaler = StandardScaler()\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create pipeline: pipeline\npipeline = make_pipeline(scaler, pca)\n\n# Fit the pipeline to 'samples'\npipeline.fit(samples)\n\n# Plot the explained variances\nfeatures = range(pca.n_components_)\nplt.bar(features, pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.xticks(features)\nplt.show()\n"""
src/ml-unsupervised/03-decorrelating-your-data-and-dimension-reduction/05-dimension-reduction-of-the-fish-measuremenys.py,0,"b'\'\'\'\nDimension reduction of the fish measurements\n\nIn a previous exercise, you saw that 2 was a reasonable choice for the ""intrinsic dimension"" of the fish\nmeasurements. Now use PCA for dimensionality reduction of the fish measurements, retaining only the 2 most important\ncomponents.\n\nThe fish measurements have already been scaled for you, and are available as scaled_samples.\n\nINSTRUCTIONS\n100XP\nImport PCA from sklearn.decomposition.\nCreate a PCA instance called pca with n_components=2.\nUse the .fit() method of pca to fit it to the scaled fish measurements scaled_samples.\nUse the .transform() method of pca to transform the scaled_samples. Assign the result to pca_features.\n\'\'\'\n# Import PCA\nfrom sklearn.decomposition import PCA\n\nfrom helper import scaled_samples\n\n# Create a PCA instance: pca\npca = PCA()\n\n# Create a PCA model with 2 components: pca\npca = PCA(n_components=2)\n\n# Fit the PCA instance to the scaled samples\npca.fit(scaled_samples)\n\n# Transform the scaled samples: pca_features\npca_features = pca.transform(scaled_samples)\n\n# Print the shape of pca_features\nprint(pca_features.shape)\n'"
src/ml-unsupervised/03-decorrelating-your-data-and-dimension-reduction/06-a-tfidf-word-frequency-array.py,0,"b""'''\nA tf-idf word-frequency array\n\nIn this exercise, you'll create a tf-idf word frequency array for a toy collection of documents. For this,\nuse the TfidfVectorizer from sklearn. It transforms a list of documents into a word frequency array, which it outputs\nas a csr_matrix. It has fit() and transform() methods like other sklearn objects.\n\nYou are given a list documents of toy documents about pets. Its contents have been printed in the IPython Shell.\n\nINSTRUCTIONS\n100XP\nImport TfidfVectorizer from sklearn.feature_extraction.text.\nCreate a TfidfVectorizer instance called tfidf.\nApply .fit_transform() method of tfidf to documents and assign the result to csr_mat. This is a word-frequency array in csr_matrix format.\nInspect csr_mat by calling its .toarray() method and printing the result. This has been done for you.\nThe columns of the array correspond to words. Get the list of words by calling the .get_feature_names() method of tfidf, and assign the result to words.\n'''\n# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndocuments = ['cats say meow', 'dogs say woof', 'dogs chase cats']\n\n# Create a TfidfVectorizer: tfidf\ntfidf = TfidfVectorizer()\n\n# Apply fit_transform to document: csr_mat\ncsr_mat = tfidf.fit_transform(documents)\n\n# Print result of toarray() method\nprint(csr_mat.toarray())\n\n# Get the words: words\nwords = tfidf.get_feature_names()\n\n# Print words\nprint(words)\n"""
src/ml-unsupervised/03-decorrelating-your-data-and-dimension-reduction/07-clustering-wikipedia-part-1.py,0,"b""'''\nClustering Wikipedia part I\n\nYou saw in the video that TruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays. Combine your knowledge of TruncatedSVD and k-means to cluster some popular pages from Wikipedia. In this exercise, build the pipeline. In the next exercise, you'll apply it to the word-frequency array of some Wikipedia articles.\n\nCreate a Pipeline object consisting of a TruncatedSVD followed by KMeans. (This time, we've precomputed the word-frequency matrix for you, so there's no need for a TfidfVectorizer).\n\nThe Wikipedia dataset you will be working with was obtained from here.\n\nINSTRUCTIONS\n100XP\nImport:\nTruncatedSVD from sklearn.decomposition.\nKMeans from sklearn.cluster.\nmake_pipeline from sklearn.pipeline.\nCreate a TruncatedSVD instance called svd with n_components=50.\nCreate a KMeans instance called kmeans with n_clusters=6.\nCreate a pipeline called pipeline consisting of svd and kmeans.\n'''\nfrom sklearn.cluster import KMeans\n# Perform the necessary imports\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import make_pipeline\n\n# Create a TruncatedSVD instance: svd\nsvd = TruncatedSVD(n_components=50)\n\n# Create a KMeans instance: kmeans\nkmeans = KMeans(n_clusters=6)\n\n# Create a pipeline: pipeline\npipeline = make_pipeline(svd, kmeans)\n\n--------------\n\n'''\nClustering Wikipedia part II\n\nIt is now time to put your pipeline from the previous exercise to work! You are given an array articles of tf-idf\nword-frequencies of some popular Wikipedia articles, and a list titles of their titles. Use your pipeline to cluster\nthe Wikipedia articles.\n\nA solution to the previous exercise has been pre-loaded for you, so a Pipeline pipeline chaining TruncatedSVD with\nKMeans is available.\n\nINSTRUCTIONS\n100XP\nImport pandas as pd.\nFit the pipeline to the word-frequency array articles.\nPredict the cluster labels.\nAlign the cluster labels with the list titles of article titles by creating a DataFrame df with labels and titles as columns. This has been done for you.\nUse the .sort_values() method of df to sort the DataFrame by the 'label' column, and print the result.\nHit 'Submit Answer' and take a moment to investigate your amazing clustering of Wikipedia pages!\n'''\n# Import pandas\nimport pandas as pd\nfrom helper import titles\n\n# Fit the pipeline to articles\npipeline.fit(articles)\n\n# Calculate the cluster labels: labels\nlabels = pipeline.predict(articles)\n\n# Create a DataFrame aligning labels and titles: df\ndf = pd.DataFrame({'label': labels, 'article': titles})\n\n# Display df sorted by cluster label\nprint(df.sort_values('label'))\n"""
src/ml-unsupervised/03-decorrelating-your-data-and-dimension-reduction/08-clustering-wikipedia-part-2.py,0,"b""'''\nClustering Wikipedia part II\n\nIt is now time to put your pipeline from the previous exercise to work! You are given an array articles of tf-idf\nword-frequencies of some popular Wikipedia articles, and a list titles of their titles. Use your pipeline to cluster\nthe Wikipedia articles.\n\nA solution to the previous exercise has been pre-loaded for you, so a Pipeline pipeline chaining TruncatedSVD with\nKMeans is available.\n\nINSTRUCTIONS\n100XP\nImport pandas as pd.\nFit the pipeline to the word-frequency array articles.\nPredict the cluster labels.\nAlign the cluster labels with the list titles of article titles by creating a DataFrame df with labels and titles as columns. This has been done for you.\nUse the .sort_values() method of df to sort the DataFrame by the 'label' column, and print the result.\nHit 'Submit Answer' and take a moment to investigate your amazing clustering of Wikipedia pages!\n'''\n# Import pandas\nimport pandas as pd\n# Fit the pipeline to articles\npipeline.fit(articles)\n\n# Calculate the cluster labels: labels\nlabels = pipeline.predict(articles)\n\n# Create a DataFrame aligning labels and titles: df\ndf = pd.DataFrame({'label': labels, 'article': titles})\n\n# Display df sorted by cluster label\nprint(df.sort_values('label'))\n"""
src/ml-unsupervised/04-discovering-interpretable-features/01-nmf-applied-to-wikipedia-articles.py,0,"b""'''\nNMF applied to Wikipedia articles\n\nIn the video, you saw NMF applied to transform a toy word-frequency array. Now it's your turn to apply NMF, this time using the tf-idf word-frequency array of Wikipedia articles, given as a csr matrix articles. Here, fit the model and transform the articles. In the next exercise, you'll explore the result.\n\nINSTRUCTIONS\n100XP\nImport NMF from sklearn.decomposition.\nCreate an NMF instance called model with 6 components.\nFit the model to the word count data articles.\nUse the .transform() method of model to transform articles, and assign the result to nmf_features.\nPrint nmf_features to get a first idea what it looks like.\n'''\n# Import NMF\nfrom sklearn.decomposition import NMF\n\n# Create an NMF instance: model\nmodel = NMF(n_components=6)\n\n# Fit the model to articles\nmodel.fit(articles)\n\n# Transform the articles: nmf_features\nnmf_features = model.transform(articles)\n\n# Print the NMF features\nprint(nmf_features)\n"""
src/ml-unsupervised/04-discovering-interpretable-features/02-nmf-features-of-the-wikipedia-articles.py,0,"b""'''\nNMF features of the Wikipedia articles\n\nNow you will explore the NMF features you created in the previous exercise. A solution to the previous exercise has been pre-loaded, so the array nmf_features is available. Also available is a list titles giving the title of each Wikipedia article.\n\nWhen investigating the features, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. In the next video, you'll see why: NMF components represent topics (for instance, acting!).\n\nINSTRUCTIONS\n100XP\nImport pandas as pd.\nCreate a DataFrame df from nmf_features using pd.DataFrame(). Set the index to titles using index=titles.\nUse the .loc[] accessor of df to select the row with title 'Anne Hathaway', and print the result. These are the NMF features for the article about the actress Anne Hathaway.\nRepeat the last step for 'Denzel Washington' (another actor).\n'''\n# Import pandas\nimport pandas as pd\n\n# Create a pandas DataFrame: df\ndf = pd.DataFrame(nmf_features, index=titles)\n\n# Print the row for 'Anne Hathaway'\nprint(df.loc['Anne Hathaway'])\n\n# Print the row for 'Denzel Washington'\nprint(df.loc['Denzel Washington'])\n"""
src/ml-unsupervised/04-discovering-interpretable-features/03-nmf-learns-topics-of-documents.py,0,"b""'''\nNMF learns topics of documents\n\nIn the video, you learned when NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. Verify this for yourself for the NMF model that you built earlier using the Wikipedia articles. Previously, you saw that the 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. In this exercise, identify the topic of the corresponding NMF component.\n\nThe NMF model you built earlier is available as model, while words is a list of the words that label the columns of the word-frequency array.\n\nAfter you are done, take a moment to recognise the topic that the articles about Anne Hathaway and Denzel Washington have in common!\n\nINSTRUCTIONS\n100XP\nImport pandas as pd.\nCreate a DataFrame components_df from model.components_, setting columns=words so that columns are labeled by the words.\nPrint components_df.shape to check the dimensions of the DataFrame.\nUse the .iloc[] accessor on the DataFrame components_df to select row 3. Assign the result to component.\nCall the .nlargest() method of component, and print the result. This gives the five words with the highest values for that component.\n'''\n# Import pandas\nimport pandas as pd\n\n# Create a DataFrame: components_df\ncomponents_df = pd.DataFrame(model.components_, columns=words)\n\n# Print the shape of the DataFrame\nprint(components_df.shape)\n\n# Select row 3: component\ncomponent = components_df.iloc[3]\n\n# Print result of nlargest\nprint(component.nlargest())"""
src/ml-unsupervised/04-discovering-interpretable-features/04-explore-the-led-digits-dataset.py,1,"b""'''\nExplore the LED digits dataset\n\nIn the following exercises, you'll use NMF to decompose grayscale images into their commonly occurring patterns. Firstly, explore the image dataset and see how it is encoded as an array. You are given 100 images as a 2D array samples, where each row represents a single 13x8 image. The images in your dataset are pictures of a LED digital display.\n\nINSTRUCTIONS\n100XP\nImport matplotlib.pyplot as plt.\nSelect row 0 of samples and assign the result to digit. For example, to select column 2 of an array a, you could use a[:,2]. Remember that since samples is a NumPy array, you can't use the .loc[] or iloc[] accessors to select specific rows or columns.\nPrint digit. This has been done for you. Notice that it is a 1D array of 0s and 1s.\nUse the .reshape() method of digit to get a 2D array with shape (13, 8). Assign the result to bitmap.\nPrint bitmap, and notice that the 1s show the digit 7!\nUse the plt.imshow() function to display bitmap as an image.\n'''\nimport csv\n\nimport numpy as np\n# Import pyplot\nfrom matplotlib import pyplot as plt\n\nfrom helper import path\n\nwith open('../' + path + 'lcd-digits.csv', 'r') as f:\n    samples = list(csv.reader(f, delimiter=','))\n    samples = np.array(samples).astype(np.float)\n\n# Select the 0th row: digit\ndigit = samples[0, :]\n\n# Print digit\nprint(digit)\n\n# Reshape digit to a 13x8 array: bitmap\nbitmap = digit.reshape(13, 8)\n\n# Print bitmap\nprint(bitmap)\n\n# Use plt.imshow to display bitmap\nplt.imshow(bitmap, cmap='gray', interpolation='nearest')\nplt.colorbar()\nplt.show()\n"""
src/ml-unsupervised/04-discovering-interpretable-features/05-nmf-learns-the-parts-of-images.py,1,"b""'''\nNMF learns the parts of images\n\nNow use what you've learned about NMF to decompose the digits dataset. You are again given the digit images as a 2D array samples. This time, you are also provided with a function show_as_image() that displays the image encoded by any 1D array:\n\ndef show_as_image(sample):\n    bitmap = sample.reshape((13, 8))\n    plt.figure()\n    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n    plt.colorbar()\n    plt.show()\nAfter you are done, take a moment to look through the plots and notice how NMF has expressed the digit as a sum of the components!\n\nINSTRUCTIONS\n100XP\nImport NMF from sklearn.decomposition.\nCreate an NMF instance called model with 7 components. (7 is the number of cells in an LED display).\nApply the .fit_transform() method of model to samples. Assign the result to features.\nTo each component of the model (accessed via model.components_), apply the show_as_image() function to that component inside the loop.\nAssign the row 0 of features to digit_features.\nPrint digit_features.\n'''\nimport csv\n\nimport numpy as np\n# Import pyplot\nfrom matplotlib import pyplot as plt\n# Import NMF\nfrom sklearn.decomposition import NMF\n\nfrom helper import path\n\nwith open('../' + path + 'lcd-digits.csv', 'r') as f:\n    samples = list(csv.reader(f, delimiter=','))\n    samples = np.array(samples).astype(np.float)\n\n\ndef show_as_image(sample):\n    bitmap = sample.reshape((13, 8))\n    plt.figure()\n    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n    plt.colorbar()\n    plt.show()\n\n\n# Create an NMF model: model\nmodel = NMF(n_components=7)\n\n# Apply fit_transform to samples: features\nfeatures = model.fit_transform(samples)\n\n# Call show_as_image on each component\nfor component in model.components_:\n    show_as_image(component)\n\n# Assign the 0th row of features: digit_features\ndigit_features = features[0, :]\n\n# Print digit_features\nprint(digit_features)\n"""
src/ml-unsupervised/04-discovering-interpretable-features/06-pca-doesnt-learn-parts.py,1,"b""'''\nPCA doesn't learn parts\n\nUnlike NMF, PCA doesn't learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Verify this for yourself by inspecting the components of a PCA model fit to the dataset of LED digit images from the previous exercise. The images are available as a 2D array samples. Also available is a modified version of the show_as_image() function which colors a pixel red if the value is negative.\n\nAfter submitting the answer, notice that the components of PCA do not represent meaningful parts of images of LED digits!\n\nINSTRUCTIONS\n100XP\nImport PCA from sklearn.decomposition.\nCreate a PCA instance called model with 7 components.\nApply the .fit_transform() method of model to samples. Assign the result to features.\nTo each component of the model (accessed via model.components_), apply the show_as_image() function to that component inside the loop.\n'''\nimport csv\n\nimport numpy as np\n# Import pyplot\nfrom matplotlib import pyplot as plt\n# Import PCA\nfrom sklearn.decomposition import PCA\n\nfrom helper import path\n\nwith open('../' + path + 'lcd-digits.csv', 'r') as f:\n    samples = list(csv.reader(f, delimiter=','))\n    samples = np.array(samples).astype(np.float)\n\n\ndef show_as_image(sample):\n    bitmap = sample.reshape((13, 8))\n    bitmap[bitmap >= 0] = 1\n    bitmap[bitmap < 0] = 0\n    plt.figure()\n    plt.imshow(bitmap, cmap='gist_yarg', interpolation='nearest', vmin=-.1, vmax=1.1)\n    plt.colorbar()\n    plt.show()\n\n\n# Create a PCA instance: model\nmodel = PCA(n_components=7)\n\n# Apply fit_transform to samples: features\nfeatures = model.fit_transform(samples)\n\n# Call show_as_image on each component\nfor component in model.components_:\n    show_as_image(component)\n"""
src/ml-unsupervised/04-discovering-interpretable-features/07-which-articles-are-similar-to-cristiano-ronaldo.py,0,"b""'''\nWhich articles are similar to 'Cristiano Ronaldo'?\n\nIn the video, you learned how to use NMF features and the cosine similarity to find similar articles. Apply this to your NMF model for popular Wikipedia articles, by finding the articles most similar to the article about the footballer Cristiano Ronaldo. The NMF features you obtained earlier are available as nmf_features, while titles is a list of the article titles.\n\nINSTRUCTIONS\n100XP\nImport normalize from sklearn.preprocessing.\nApply the normalize() function to nmf_features. Store the result as norm_features.\nCreate a DataFrame df from norm_features, using titles as an index.\nUse the .loc[] accessor of df to select the row of 'Cristiano Ronaldo'. Assign the result to article.\nApply the .dot() method of df to article to calculate the cosine similarity of every row with article.\nPrint the result of the .nlargest() method of similarities to display the most similiar articles. This has been done for you, so hit 'Submit Answer' to see the result!\n'''\n# Perform the necessary imports\nimport pandas as pd\nfrom sklearn.preprocessing import normalize\n\n# Normalize the NMF features: norm_features\nnorm_features = normalize(nmf_features)\n\n# Create a DataFrame: df\ndf = pd.DataFrame(norm_features, index=titles)\n\n# Select the row corresponding to 'Cristiano Ronaldo': article\narticle = df.loc['Cristiano Ronaldo']\n\n# Compute the dot products: similarities\nsimilarities = df.dot(article)\n\n# Display those with the largest cosine similarity\nprint(similarities.nlargest())"""
src/ml-unsupervised/04-discovering-interpretable-features/08-recommend-musical-artists-part-1.py,0,"b""'''\nRecommend musical artists part I\n\nIn this exercise and the next, you'll use what you've learned about NMF to recommend popular music artists! You are given a sparse array artists whose rows correspond to artists and whose column correspond to users. The entries give the number of times each artist was listened to by each user.\n\nIn this exercise, build a pipeline and transform the array into normalized NMF features. The first step in the pipeline, MaxAbsScaler, transforms the data so that all users have the same influence on the model, regardless of how many different artists they've listened to. In the next exercise, you'll use the resulting normalized NMF features for recommendation!\n\nThis data is part of a larger dataset available here.\n\nINSTRUCTIONS\n100XP\nImport:\nNMF from sklearn.decomposition.\nNormalizer and MaxAbsScaler from sklearn.preprocessing.\nmake_pipeline from sklearn.pipeline.\nCreate an instance of MaxAbsScaler called scaler.\nCreate an NMF instance with 20 components called nmf.\nCreate an instance of Normalizer called normalizer.\nCreate a pipeline called pipeline that chains together scaler, nmf, and normalizer.\nApply the .fit_transform() method of pipeline to artists. Assign the result to norm_features.\n'''\n# Perform the necessary imports\nfrom sklearn.decomposition import NMF\nfrom sklearn.preprocessing import Normalizer, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Create a MaxAbsScaler: scaler\nscaler = MaxAbsScaler()\n\n# Create an NMF model: nmf\nnmf = NMF(n_components=20)\n\n# Create a Normalizer: normalizer\nnormalizer = Normalizer()\n\n# Create a pipeline: pipeline\npipeline = make_pipeline(scaler, nmf, normalizer)\n\n# Apply fit_transform to artists: norm_features\nnorm_features = pipeline.fit_transform(artists)\n"""
src/ml-unsupervised/04-discovering-interpretable-features/09-recommend-musical-artists-part-2.py,0,"b""'''\nRecommend musical artists part II\n\nSuppose you were a big fan of Bruce Springsteen - which other musicial artists might you like? Use your NMF features from the previous exercise and the cosine similarity to find similar musical artists. A solution to the previous exercise has been run, so norm_features is an array containing the normalized NMF features as rows. The names of the musical artists are available as the list artist_names.\n\nINSTRUCTIONS\n100XP\nImport pandas as pd.\nCreate a DataFrame df from norm_features, using artist_names as an index.\nUse the .loc[] accessor of df to select the row of 'Bruce Springsteen'. Assign the result to artist.\nApply the .dot() method of df to artist to calculate the dot product of every row with artist. Save the result as similarities.\nPrint the result of the .nlargest() method of similarities to display the artists most similar to 'Bruce Springsteen'.\n'''\n# Import pandas\nimport pandas as pd\n\n# Create a DataFrame: df\ndf = pd.DataFrame(norm_features, index=artist_names)\n\n# Select row of 'Bruce Springsteen': artist\nartist = df.loc['Bruce Springsteen']\n\n# Compute cosine similarities: similarities\nsimilarities = df.dot(artist)\n\n# Display those with highest cosine similarity\nprint(similarities.nlargest())\n"""
src/python_core/output_questions/1.py,0,"b""# what will be the output?\n\nn = [1, 2, 3, 4, 5, 10, 3, 100, 9, 24]\n\nn1 = [i for i in n if i > 5]\n\nfor e in n:\n    print('inter-> ')\n    if e < 5:\n        print('removing: {}'.format(e))\n        n.remove(e)\n        print('list after removal: {}'.format(n))\n\nprint(n)\nprint(n1)\n"""
src/python_core/output_questions/__init__.py,0,b''
