file_path,api_count,code
src/Digit_Recognition_ANN.py,5,"b'import numpy as np\nimport mnist_loader as ml\nimport Network as net\nimport test as tt\nimport bar as br\n\nnp.random.seed(1)\n\n""""""randomly intitalising weights mapping input layer to hidden layer and\n    hidden layer to output layer""""""\n\nweights = 2*np.random.random((784,50)) - 1\nweights1 = 2*np.random.random((50,10)) - 1\n\n""""""Loading the MNIST data set into three lists two containg the training data\n    and the third one containing the test data""""""\n\ntr_data, val_data, test_data = ml.load_data()\n\n""""""Fitting the 28*28 input image into a numpy array of 784*1 dimension""""""\n\ntr_inputs = [np.reshape(x, (784, 1)) for x in tr_data[0]]\n\n""""""Converting the single output into a numpy array of 10 dimensions with 1 at\n    the index of the output an 0 elsewhere""""""\ntr_outputs = [ml.vectorized_result(x) for x in tr_data[1]]\n\n""""""Loop to train the data taking an input of 10,000 images""""""\nfor i in range(50000):\n\n    weights , weights1 = net.train(tr_inputs[i],tr_outputs[i],weights,weights1)\n    if(i % 500) == 0 :\n        br.progress(i, 50000)\nbr.progress(50000, 50000, cond = True)\n\nprint (""\\n"")\n\nprint (""Network Trained and ready to be operated"")\n\nte_inputs = [np.reshape(x, (784,1)) for x in test_data[0]]\nte_outputs = test_data[1]\n\n""""""Function to check the accuracy of our trained network by testing it on\n    unchecked data of 10,000 images""""""\ntt.check(te_inputs,te_outputs,weights,weights1)\n'"
src/Digit_Recognition_CNN.py,2,"b'import tensorflow as tf\nimport numpy as np\nimport mnist_loader as ml\nimport bar as br\nimport random\nimport gzip\nimport pickle\n\nf = gzip.open(\'mnist.pkl.gz\', \'rb\')\n\ntraining_data, validation_data, test_data = pickle.load(f, encoding = \'latin1\')\n\nf.close()\n\ntr_inputs = [np.reshape(x, (1,784)) for x in training_data[0]]\ntr_outputs = [ml.vectorized_result(x) for x in training_data[1]]\n\n\nte_inputs = [np.reshape(x, (1,784)) for x in test_data[0]]\nte_outputs = [ml.vectorized_result(x) for x in test_data[1]]\n\nn_classes = 10\nbatch_size = 256\n\nx = tf.placeholder(\'float\', [None, 784])\ny = tf.placeholder(\'float\')\n\ndef conv2d(x ,W):\n\treturn tf.nn.conv2d(x, W, strides = [1,1,1,1], padding = \'SAME\')\n\ndef maxpool2d(x):\n\treturn tf.nn.max_pool(x, ksize = [1,2,2,1], strides = [1,2,2,1], padding = \'SAME\')\n\ndef convulation_net(x):\n\tweights = {\'W_conv1\' : tf.Variable(tf.random_normal([5,5,1,32])),\n\t\t\t   \'W_conv2\' : tf.Variable(tf.random_normal([5,5,32,64])),\n\t\t\t   \'W_fc\' : tf.Variable(tf.random_normal([7*7*64,1024])),\n\t\t\t   \'out\' : tf.Variable(tf.random_normal([1024,n_classes])),\n\t\t\t   }\n\n\tbiases = {\'b_conv1\' : tf.Variable(tf.random_normal([32])),\n\t\t\t  \'b_conv2\' : tf.Variable(tf.random_normal([64])),\n\t\t\t  \'b_fc\' : tf.Variable(tf.random_normal([1024])),\n\t\t\t  \'out\' : tf.Variable(tf.random_normal([n_classes])),\n\t\t\t  }\t\t  \n\t\n\tx = tf.reshape(x, shape = [-1, 28, 28, 1])\n\n\tconv1 = tf.nn.relu(conv2d(x, weights[\'W_conv1\']) + biases[\'b_conv1\'])\n\tconv1 = maxpool2d(conv1)\n\n\tconv2 = tf.nn.relu(conv2d(conv1, weights[\'W_conv2\']) + biases[\'b_conv2\'])\n\tconv2 = maxpool2d(conv2)\n\n\n\tfc = tf.reshape(conv2, [-1,7*7*64])\n\tfc = tf.nn.relu(tf.matmul(fc,weights[\'W_fc\']) + biases[\'b_fc\'])\n\n\touptut = tf.matmul(fc, weights[\'out\']) + biases[\'out\']\n\n\treturn ouptut\n\ndef train_net(x):\n\n\tsess = tf.InteractiveSession\n\tprediction = convulation_net(x)\n\tprint(prediction)\n\tcost = tf.nn.softmax_cross_entropy_with_logits(logits = prediction, labels=y)\n\toptimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)\n\tcorrect_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n\taccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\twith tf.Session() as sess :\n\t\tsess.run(tf.global_variables_initializer())\n\n\t\tfor i in range(20000):\n\n\t\t\tif(i % 200) == 0 :\n\t\t\t\tbr.progress(i, 20000)\n\n\t\t\tfor j in range(10):\n\t\t\t\tk = random.randint(0,20000)\n\t\t\t\toptimizer.run(feed_dict = {x: tr_inputs[k], y: tr_outputs[k]})\n\n\t\tbr.progress(20000, 20000, cond = True)\n\n\t\tprint(""\\n"")\n\n\t\tacc = 0\n\t\tfor i in range(10000):\n\t\t\tacc += accuracy.eval(feed_dict={\n\t\t\t\tx: te_inputs[i], y: te_outputs[i]})\n\n\t\tprint(\'test accuracy %g\' % acc)\n\n\n\n\ntrain_net(x)\n'"
src/Network.py,5,"b'import numpy as np\n\n""""""Return the sigmoid value or activation value of our neuron i:e it maps any value to a value between 0 to 1""""""\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n""""""Return the derivative of the signoid function""""""\ndef deriv_sigmoid(x):\n    return x*(1-x)\n\ndef train(inputs,output,weights,weights1):\n    \n    x = inputs.T\n    y = output.T\n    """"""Giving the input to our network and calculating the outptut and\n        storing it in l2""""""\n    \n    l1 = sigmoid(np.dot(x,weights))\n    l2 = sigmoid(np.dot(l1,weights1))\n\n    """"""Calculating the error by subtracting our Networks output from expected output""""""\n    error = y - l2 \n\n    """"""This gives how much did our output layer contributed in our missed output""""""\n    l2_del = error * deriv_sigmoid(l2)\n\n    \n    """"""Calculating the error of out hidden layer""""""\n    error0 = l2_del.dot(weights1.T)\n\n    """"""This gives how much did our hidden layer contributed in our missed output""""""\n    l1_del = error0 * deriv_sigmoid(l1)\n\n    """"""updating the values of our weights by how much we missed""""""\n    weights1 += np.dot(l1.T,l2_del)\n    weights += np.dot(x.T,l1_del)\n\n    return weights,weights1\n\n\n'"
src/bar.py,0,"b""import sys\n\ndef progress(count, total, cond=False):\n    bar_len = 60\n    filled_len = int(round(bar_len * count / float(total)))\n\n    percents = round(100.0 * count / float(total), 1)\n    bar = '|' * filled_len + '-' * (bar_len - filled_len)\n\n    if cond == False:\n    \tsys.stdout.write('[%s] %s%s\\r' % (bar, percents, '%'))\n    \tsys.stdout.flush()\n\n    else:\n    \tsys.stdout.write('[%s] %s%s' % (bar, percents, '%'))\n"""
src/mnist_loader.py,1,"b""import pickle\nimport gzip\n\nimport numpy as np\n\ndef load_data():\n    f = gzip.open('mnist.pkl.gz', 'rb')\n\n    training_data, validation_data, test_data = pickle.load(f, encoding = 'latin1')\n\n    f.close()\n\n    return (training_data, validation_data, test_data)\n\ndef vectorized_result(j):\n    e = np.zeros((10, 1))\n    e[j] = 1.0\n    return e\n"""
src/test.py,3,"b'import Network as net\nimport numpy as np\n\n\ndef feedforward(x,weights,weights1):\n\n    l = x.T\n    l1 = net.sigmoid(np.dot(l,weights))\n    l2 = net.sigmoid(np.dot(l1,weights1))\n    return l2;\n\ndef check(te_inputs,te_outputs,weights,weights1):\n\n    correct = 0\n    \n    for i in range(len(te_inputs)):\n        \n        out = feedforward(te_inputs[i],weights,weights1)\n        f_out = np.argmax(out)\n        if(f_out == te_outputs[i]):\n            correct += 1\n\n    print (""Accuracy Of the Network is "" , ((correct/10000)*100))\n'"
