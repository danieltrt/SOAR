file_path,api_count,code
classification.py,0,"b""from metric_learn import LMNN, ITML_Supervised, LSML_Supervised, SDML_Supervised\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\nfrom mlm import MinimalLearningMachine as MLM\nfrom mlm import MinimalLearningMachineClassifier as MLMC\nfrom mlm import NearestNeighborMinimalLearningMachineClassifier as NNMLMC\nfrom mlm.selectors import KSSelection, NLSelection\n\ndb = datasets.load_breast_cancer()\nX = db.data\ny = db.target\n\nscaler = StandardScaler().fit(X)\nX = scaler.transform(X)\n\n\nmlm1 = MLMC()\nmlm1.fit(X, y)\nmlm_r1 = (mlm1.score(X, y))\n\n\nmlm2 = MLMC(selector=KSSelection())\nmlm2.fit(X, y)\nmlm_r2 = mlm2.score(X, y)\n\n\nmlm3 = MLMC(selector=NLSelection())\nmlm3.fit(X, y)\nmlm_r3 = mlm3.score(X, y)\n\nprint(f'RN: R2 of {round(mlm_r1, 2)} with sparsity of {mlm1.sparsity()}')\nprint(f'KS: R2 of {round(mlm_r2, 2)} with sparsity of {mlm1.sparsity()}')\nprint(f'NL: R2 of {round(mlm_r3, 2)} with sparsity of {mlm1.sparsity()}')\n\n"""
mcycle.py,2,"b'import numpy as np\nfrom builtins import len\n\nfrom sklearn.preprocessing import StandardScaler\nfrom mlm import MinimalLearningMachine as MLM\nfrom mlm.selectors import KSSelection, NLSelection, RandomSelection, MutualInformationSelection\nfrom sklearn.utils.validation import check_X_y\n\nimport matplotlib.pyplot as plt\n\n\nmydata = np.genfromtxt(\'/Users/sauloafoliveira/Dropbox/thesis_code/mcycle.csv\', delimiter="","")\n\nX = mydata[:, :-1].reshape(-1, 1)\ny = mydata[:, -1].reshape(-1, 1)\n\nX, y = check_X_y(X, y, multi_output=True)\n\n\nscaler = StandardScaler().fit(X)\nX = scaler.transform(X)\n\nmlm1 = MLM(selector=KSSelection())\nmlm1.fit(X, y)\nr = mlm1.score(X, y)\n\n\n\nmlm2 = MLM(selector=NLSelection())\nmlm2.fit(X, y)\ns = mlm2.score(X, y)\n\nprint(r, s)\nprint(mlm1.sparsity(), mlm2.sparsity())\n\n\nf, ax = plt.subplots(2, 2, sharey=True)\n\n\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\nsvrr = SVR(C=128).fit(X, y)\nytrue = svrr.predict(X)\n\n\nknnr1 = MLM(selector=RandomSelection(k=1-mlm2.sparsity()[0]))\nknnr1.fit(X, y)\n#knnr2 = KNeighborsRegressor().fit(mlm2.M, mlm2.t)#\nknnr2 = MLM(selector=MutualInformationSelection()).fit(X, y)\n\nax = ax.ravel()\n\ncl = [mlm1, mlm2,  knnr1, knnr2 ]\n\nnewX = np.asmatrix(np.linspace(np.min(X), np.max(X))).T\n\nfor i in range(len(ax)):\n\n    ax[i].plot(X, y, \'.r\', alpha=0.5)\n    ax[i].plot(X, ytrue, \'-k\', alpha=0.8)\n    ax[i].plot(newX, cl[i].predict(newX), \'-b\')\n    #ax[i].plot(mlm1.M, mlm1.t, \'.k\')\n    if i != -1:\n        ax[i].set_title(\'Model: {} {}\'.format(round(cl[i].score(X, y), 2), cl[i].sparsity()))\n    else:\n        ax[i].set_title(\'Model: {} \'.format(round(cl[i].score(X, y), 2)))\n#\n# ax2.plot(X, y, \'.r\', alpha=0.5)\n# ax2.plot(X, mlm2.predict(X), \'-b\')\n# ax2.plot(mlm2.M, mlm2.t, \'.k\')\n# ax2.set_title(\'NLSelection {} -> {}\'.format(round(s, 2), mlm2.sparsity()))\n#\n#\n# ax3.plot(X, y, \'.r\', alpha=0.5)\n# ax3.plot(X, knnr.predict(X), \'-b\')\n# ax3.plot(mlm2.M, mlm2.t, \'.k\')\n# ax3.set_title(\'KNN for R {} -> {}\'.format(round(knnr.score(X, y), 2), mlm2.sparsity()))\n#\n#\n\n\n\nplt.show()\n'"
regression.py,0,"b""from sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\nfrom mlm import MinimalLearningMachine as MLM\nfrom mlm.selectors import KSSelection, NLSelection\n\nboston = datasets.load_boston()\n\nX = boston.data\ny = boston.target\n\nscaler = StandardScaler().fit(X)\nX = scaler.transform(X)\n\nmlm1 = MLM()\nmlm1.fit(X, y)\nmlm_r1 = (mlm1.score(X, y))\n\nmlm2 = MLM(selector=KSSelection())\nmlm2.fit(X, y)\nmlm_r2 = mlm2.score(X, y)\n\nmlm3 = MLM(selector=NLSelection())\nmlm3.fit(X, y)\nmlm_r3 = mlm3.score(X, y)\n\nprint(f'RN: R2 of {round(mlm_r1, 2)} with sparsity of {mlm1.sparsity()}')\nprint(f'KS: R2 of {round(mlm_r2, 2)} with sparsity of {mlm2.sparsity()}')\nprint(f'NL: R2 of {round(mlm_r3, 2)} with sparsity of {mlm3.sparsity()}')\n\n\nprint('*' * 30)\n\nboston = datasets.load_boston()\n\nX = scaler.transform(boston.data)\ny = boston.target\n\n\nmlm1 = MLM()\nmlm1.fit(X, y)\nmlm_r1 = (mlm1.score(X, y))\n\nmlm2 = MLM(selector=KSSelection())\nmlm2.fit(X, y)\nmlm_r2 = mlm2.score(X, y)\n\nmlm3 = MLM(selector=NLSelection())\nmlm3.fit(X, y)\nmlm_r3 = mlm3.score(X, y)\n\nprint(f'RN: R2 of {round(mlm_r1, 2)} with sparsity of {mlm1.sparsity()}')\nprint(f'KS: R2 of {round(mlm_r2, 2)} with sparsity of {mlm2.sparsity()}')\nprint(f'NL: R2 of {round(mlm_r3, 2)} with sparsity of {mlm3.sparsity()}')\n\nprint('*' * 30)\n"""
mlm/__init__.py,27,"b'from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\nfrom scipy.optimize import root\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.utils.validation import check_X_y\nfrom mlm.selectors import RandomSelection\n\n\n__all__ = [\'MinimalLearningMachine\',\n           \'MinimalLearningMachineClassifier\',\n           \'NearestNeighborMinimalLearningMachineClassifier\',\n           \'CubicMinimalLearningMachine\']\n\n__author__  = ""Saulo Oliveira <saulo.freitas.oliveira@gmail.com>""\n__status__  = ""production""\n__version__ = ""1.0.0""\n__date__    = ""07 September 2018""\n\n\n\nclass MinimalLearningMachine(BaseEstimator, RegressorMixin):\n\n    def __init__(self, selector=None, estimator_type=\'regressor\', bias=False, l=0):\n        self.selector = RandomSelection(k=np.inf) if selector is None else selector\n        self.M = []\n        self.t = []\n        self._sparsity_scores = (0, np.inf) # sparsity and norm frobenius\n        self._estimator_type = estimator_type\n        self.bias = bias\n        self.l = l\n\n    def __validade_params(self):\n        if not(hasattr(self, \'bias\')):\n            self.bias\n\n\n    def fit(self, X, y):\n        from numpy.linalg import norm\n\n        X, y = check_X_y(X, y, multi_output=True, y_numeric=True)\n\n        if y.ndim == 1:\n            y = np.array([y.ravel()]).T\n\n        idx, _, _ = self.selector.select(X, y)\n\n        self.M = X[idx]\n        self.t = y[idx]\n\n        assert (len(self.M) != 0), ""No reference point was yielded by the selector""\n\n        dx = cdist(np.asmatrix(X), np.asmatrix(self.M))\n        dy = cdist(np.asmatrix(y), np.asmatrix(self.t))\n\n        # if self.bias:\n        #     dx = np.concatenate(np.ones(len(X)), dx, axis=1)\n        #\n        # if self.l > 0 :\n        #     dx2 = dx.T @ dx\n        #     np.fill_diagonal(dx2, self.l + np.diagonal(dx2))\n        #     self.B_ = np.linalg.pinv(dx2 @ dx.T) @ dy\n        # else:\n\n        self.B_ = np.linalg.pinv(dx) @ dy\n\n        self._sparsity_scores = (1 - len(self.M) / len(X), norm(self.B_, ord=\'fro\'))\n\n        return self\n\n    def mulat_(self, y, dyh):\n        if y.ndim == 1:\n            y = np.array([y.ravel()]).T\n\n        dy2t = cdist(np.asmatrix(y), np.asmatrix(self.t))\n        return np.sum(np.power(np.power(dy2t, 2) - np.power(dyh, 2), 2))\n\n    def active_(self, dyhat):\n        y0h = np.mean(self.t)\n\n        result = [root(method=\'lm\', fun=lambda y: self.mulat_(y, dyh), x0=y0h) for dyh in dyhat]\n        yhat = list(map(lambda y: y.x, result))\n        return np.asarray(yhat)\n\n    def predict(self, X, y=None):\n        try:\n            getattr(self, ""B_"")\n        except AttributeError:\n            raise RuntimeError(""You must train classifier before predicting data!"")\n\n        X = np.asmatrix(X)\n\n        dyhat = cdist(X, self.M) @ self.B_\n\n        return self.active_(dyhat)\n\n    def sparsity(self):\n        try:\n            getattr(self, ""B_"")\n        except AttributeError:\n            raise RuntimeError(""You must train classifier before predicting data!"")\n\n        s = np.round(self._sparsity_scores, 2)\n\n        return s[0], s[1]\n\n\nclass MinimalLearningMachineClassifier(MinimalLearningMachine, ClassifierMixin):\n\n    def __init__(self, selector=None):\n        MinimalLearningMachine.__init__(self, selector, estimator_type=\'classifier\')\n        self.lb = LabelBinarizer()\n\n    def fit(self, X, y=None):\n        self.lb.fit(y)\n        return MinimalLearningMachine.fit(self, X, self.lb.transform(y))\n\n    def active_(self, dyhat):\n        classes = self.lb.transform(self.lb.classes_)\n\n        result = [np.argmin(list(map(lambda y_class: self.mulat_(y_class, dyh), classes))) for dyh in dyhat]\n\n        return self.lb.inverse_transform(self.lb.classes_[result])\n\n    def score(self, X, y, sample_weight=None):\n        return ClassifierMixin.score(self, X, y, sample_weight)\n\nclass NearestNeighborMinimalLearningMachineClassifier(MinimalLearningMachineClassifier):\n\n    def __init__(self, selector=None):\n        MinimalLearningMachineClassifier.__init__(self, selector)\n\n    def active_(self, dyhat):\n        m = np.argmin(dyhat, 1)\n        return self.t[m]\n\nclass CubicMinimalLearningMachine(MinimalLearningMachine):\n\n    def active_(self, dyhat):\n        a = len(self.t)\n        b = -3 * np.sum(self.t)\n        c = 3 * np.sum(np.power(self.t, 2)) - np.sum(np.power(dyhat, 2), 1)\n        d = np.power(dyhat, 2) @ self.t - np.sum(np.power(self.t, 3))\n\n        return [self.cases_(np.roots([a, b, c[i], d[i]]), dyhat[i]) for i in range(len(dyhat))]\n\n    def cases_(self, roots, dyhat):\n        r = list(map(np.isreal, roots))\n        if np.sum(r) == 3:\n            # Rescue the root with the lowest cost associated\n            j = [self.mulat_(y, dyhat) for y in np.real(roots)]\n            return np.real(roots[np.argmin(j)])\n        else:\n            # As True > False, then rescue the first real value\n            return np.real(roots[np.argmax(r)])\n\n\n'"
mlm/selectors.py,62,"b'import numpy as np\n\n__author__ = ""Saulo Oliveira <saulo.freitas.oliveira@gmail.com>""\n__status__ = ""production""\n__version__ = ""1.0.0""\n__date__ = ""07 September 2018""\n\n\nclass SelectionAlgorithm:\n    def __init__(self):\n        pass\n\n    def select(self, X, y):\n        return np.ones((len(X), 1), bool), X, y\n\n\nclass RandomSelection(SelectionAlgorithm):\n    def __init__(self, k=None):\n        self.k = k\n\n    def select(self, X, y):\n        n = len(X)\n        if self.k is None:\n            k = round(np.log10(n) * 5).astype(\'int\')\n        elif np.isinf(self.k):\n            k = len(X)\n        elif np.isscalar(self.k):\n            if self.k <= 1:\n                k = np.round(self.k * len(X)).astype(\'int\')\n            else:\n                k = np.round(max(min(self.k, len(X)), 2)).astype(\'int\')\n\n        perm = np.random.permutation(n)\n        perm = perm[:k]\n\n        idx = np.zeros(n, dtype=bool)\n\n        idx[perm] = True\n\n        idx = np.array(idx.ravel())\n\n        return idx, X[idx], y[idx]\n\n\nclass CondensedSelection(SelectionAlgorithm):\n    def __init__(self, k=None):\n        self.k = k\n\n    def select(self, X, y):\n        from sklearn.neighbors import KNeighborsClassifier\n\n        if self.k is None:\n            self.k = round(5 * np.log10(len(X)))\n\n        knn = KNeighborsClassifier(n_neighbors=self.k + 1)\n        knn.fit(X, y)\n\n        out = knn.predict(X)\n        idx = (out != y)\n        return idx, X[idx], y[idx]\n\n\nclass EditedSelection(SelectionAlgorithm):\n    def __init__(self, k=None):\n        self.k = k\n\n    def select(self, X, y):\n        from sklearn.neighbors import KNeighborsClassifier\n\n        if self.k is None:\n            self.k = round(5 * np.log10(len(X)))\n\n        knn = KNeighborsClassifier(n_neighbors=self.k + 1)\n        knn.fit(X, y)\n\n        out = knn.predict(X)\n        idx = (out == y)\n        return idx, X[idx], y[idx]\n\n\nclass FASTSelection(SelectionAlgorithm):\n    def __init__(self, k=16, p=0.75, distance_threshold=None):\n        self.k = k\n        self.p = p\n        self.distance_threshold = distance_threshold\n\n    def _candidates(self, X, y):\n        self.nn.fit(X, y)\n\n        dist, yhat = self.nn.radius_neighbors(X)\n\n        result = [self._iscorner(X[i], y[i], y[yhat[i]]) for i in range(len(X))]\n\n        corner, score = zip(*result)\n        corner = np.array(corner)\n        score = np.array(score)\n\n        return X[np.array(corner)], y[corner], score[corner]\n\n    def _nonmax_supress(self, Xcand, ycand, corner_response):\n\n        perm = np.random.permutation(len(Xcand))\n        Xcand = Xcand[perm]\n        ycand = ycand[perm]\n        corner_response = corner_response[perm]\n\n        self.nn.fit(Xcand, ycand)\n        dist, ind = self.nn.radius_neighbors()\n\n        idx = list(filter(lambda i: np.all(corner_response[i] >= corner_response[ind[i]]), range(len(Xcand))))\n        idx = np.array(idx)\n        return Xcand[idx], ycand[idx]\n\n    def _iscorner(self, x, y, y_neighbors):\n        from scipy.spatial.distance import cdist\n\n        if np.isscalar(y):\n            y = np.array([y])\n            y = y[:, None]\n            y_neighbors = y_neighbors[:, None]\n\n        if (cdist(y, y_neighbors[0]) > 0) and self._allsame(y_neighbors):\n            return True, self.k\n        else:\n            d = cdist(y, y_neighbors)\n            score = self.k - np.sum(d[d > 0])\n            return (score / self.k) > self.p, score\n\n    @staticmethod\n    def _allsame(y):\n        if len(y) == 0:\n            return True\n        return np.all(y == y[0])\n\n    def select(self, X, y):\n        if self.distance_threshold is None:\n            from sklearn.neighbors import KNeighborsRegressor\n\n            nn = KNeighborsRegressor(n_neighbors=self.k + 1)\n            nn.fit(X, y)\n            dist, ind = nn.kneighbors(X)\n\n            self.distance_threshold = np.max(np.min(dist[:, 1:], 1))\n\n        from sklearn.neighbors import RadiusNeighborsRegressor\n        self.nn = RadiusNeighborsRegressor(radius=self.distance_threshold)\n\n        Xcand, ycand, corner_response = self._candidates(X, y)\n\n        Xcorner, ycorner = self._nonmax_supress(Xcand, ycand, corner_response)\n\n        idx = np.where(np.isin(X, Xcorner))\n        return idx, Xcorner, ycorner\n\n\nclass AcitveSelection(SelectionAlgorithm):\n    \'\'\'\n    Optimized Fixed-Size Kernel Models for Large Data Sets\n    \'\'\'\n\n    def __init__(self, M=None, l=1, trials=None):\n        self.l = l\n        self.M = M\n        self.trials = trials\n\n    def select(self, X, y):\n        idx, Xr, yr = RandomSelection(k=self.M).select(X, y)\n\n        old_crit = np.Inf\n\n        for trial in range(1, self.trials + 1):\n            old_Xr = Xr\n            old_yr = yr\n\n            i = np.random.randint(0, self.M)\n            j = np.random.randint(0, len(X))\n\n            Xr[i] = X[j]\n            yr[i] = y[j]\n\n            crit = self.__quad_renyi_entropy(Xr, self.l)\n\n            if old_crit <= crit:\n                # undo permutation\n                Xr = old_Xr\n                yr = old_yr\n\n        idx = np.where(np.isin(X, Xr))\n        return idx, Xr, yr\n\n    @staticmethod\n    def __quad_renyi_entropy(X, l):\n\n        from scipy.spatial.distance import cdist\n\n        Dx = cdist(X, X)\n\n        np.fill_diagonal(Dx, l)\n\n        U, lam = np.linalg.eig(Dx)\n\n        if lam.shape[0] == lam.shape[1]:\n            lam = np.diagonal(lam)\n\n        return np.log(np.power(np.mean(U), 2) * lam)\n\n\nclass CriticalSelection(SelectionAlgorithm):\n    def __init__(self, kb=None, l=0.3, ke=None, gamma=0.1):\n        self.kb = kb\n        self.ke = ke\n        self.l = l\n        self.gamma = gamma\n\n    def select(self, X, y):\n        from scipy.stats import mode\n\n        from sklearn.neighbors import KNeighborsClassifier\n\n        if self.kb is None:\n            self.kb = round(5 * np.log10(len(X)))\n\n        knn = KNeighborsClassifier(n_neighbors=self.kb + 1)\n        knn.fit(X, y)\n\n        kneighbors = knn.kneighbors(X)\n\n        w, freq = mode(kneighbors[0])\n\n        # 1st test\n\n        r = (y == w)\n\n        test1 = (freq[r] > 1) and (np.power(freq[r], -1) <= w[r]) and (w[r] <= np.power(freq[r], -1) + self.l)\n\n        idx = np.zeros((1, len(X)), bool)\n        idx[test1] = True\n\n        # 2nd test\n        test2 = w[r] > (np.power(len(np.unique(y)), -1) + self.l)\n\n        possible_edges = zip(X[not test1 and test2], y[not test1 and test2])\n\n        actual_edges = [self.edge_pattern_selection(xy, X) for xy in possible_edges]\n\n        idx[not test1 and test2] = actual_edges\n\n        return idx, X[idx], y[idx]\n\n    def edge_pattern_selection(self, xy, X):\n        from scipy.stats import zscore\n\n        V = X - xy[0]\n\n        Z = zscore(V)\n\n        Vin = np.sum(Z)\n\n        theta = Vin @ V\n\n        l = np.sum(theta >= 0) / len(X)\n\n        return l >= (1 - xy[1])\n\n\nclass NLSelection(SelectionAlgorithm):\n\n    def __init__(self, cutoff=(.2, .32), k=None):\n        self.cutoff = cutoff\n        self.k = k\n\n    @staticmethod\n    def __moving_average(a, periods):\n        weights = np.ones(1, periods) / periods\n        return np.convolve(a, weights, mode=\'valid\')\n\n    @staticmethod\n    def __order_of(X):\n        from scipy.spatial.distance import cdist\n\n        x_origin = np.min(X, axis=0)\n        keys = cdist(np.asmatrix(x_origin), X)\n\n        return np.argsort(keys)\n\n\n    def select(self, X, y):\n        from scipy.signal import find_peaks\n\n        n = len(X)\n\n        # trick to sort rows\n        order = self.__class__.__order_of(X)\n\n        yl = y[order].ravel()\n\n        s = np.round(np.sqrt(np.std(yl)))\n\n        h_peaks, l = find_peaks(yl, distance=s)\n        l_peaks, l = find_peaks(-yl, distance=s)\n\n        idx = np.zeros(n, dtype=bool)\n\n        idx[h_peaks] = True\n        idx[l_peaks] = True\n\n        return idx, X[idx], y[idx]\n\n\nclass KSSelection(SelectionAlgorithm):\n\n    def __init__(self, cutoff=(.2, .32), k=None):\n        self.cutoff = cutoff\n        self.k = k\n\n    @staticmethod\n    def __pval_ks_2smap(entry):\n        from scipy.stats import ks_2samp, zscore\n\n        a = zscore(entry[0]) if np.std(entry[0]) > 0 else entry[0]\n\n        b = zscore(entry[1]) if np.std(entry[1]) > 0 else entry[1]\n\n        _, pval = ks_2samp(a, b)\n\n        return pval\n\n\n    def select(self, X, y):\n        from sklearn.neighbors import NearestNeighbors\n\n        n = len(X)\n\n        if self.k is None:\n            self.k = round(5 * np.log10(n))\n\n        knn = NearestNeighbors(n_neighbors=int(self.k + 1), algorithm=\'ball_tree\').fit(X)\n\n        distx, ind = knn.kneighbors(X)\n\n        knn = NearestNeighbors(n_neighbors=int(self.k + 1), algorithm=\'ball_tree\').fit(y)\n\n        disty, ind = knn.kneighbors(y, return_distance=True)\n\n        zipped = list(zip(distx[:, :1], disty[:, :1]))\n\n        p = [self.__pval_ks_2smap(entry) for entry in zipped]\n\n        order = np.argsort(p)\n\n        h_cutoff = round(self.cutoff[0] * n)\n        l_cutoff = round(self.cutoff[1] * n)\n\n        idx = np.zeros((n, 1), dtype=bool)\n        idx[order[:l_cutoff]] = True\n        idx[order[-h_cutoff:]] = True\n\n        idx = idx.ravel()\n\n        return idx, X[idx], y[idx]\n\n\nclass DROP2_RE(SelectionAlgorithm):\n\n    def __init__(self, k=None, a=0.1):\n        from sklearn.neighbors import KNeighborsRegressor\n        super(DROP2_RE, self).__init__()\n        self.k = k\n        self.alpha = a\n        self.model = KNeighborsRegressor()\n\n    def __err(self, Xy, associates, i):\n        X, y = Xy\n\n        self.model.fit(X[associates], y[associates])\n\n        error_with = np.abs(self.model.predict(np.asmatrix(X[i])) - y[i])\n\n        associates_without_x = list(set(associates) - set([i]))\n\n        self.model.fit(X[associates_without_x], y[associates_without_x])\n\n        error_without = np.abs(self.model.predict(np.asmatrix(X[i])) - y[i])\n\n        return np.asarray([error_with, error_without])\n\n    def __theta(self, y, A):\n\n        associates = A[:self.k] if len(A) >= self.k else A\n\n        return self.alpha * np.std( y[ associates ] )\n\n\n    def select(self, X, y):\n        from sklearn.neighbors import NearestNeighbors\n        _, associates = NearestNeighbors(n_neighbors=len(X)).fit(X).kneighbors(X)\n\n        Xy = (X, y)\n\n        selected = np.ones(len(X), dtype=bool)\n\n        for i in range(len(X)):\n            A = associates[i]\n\n            errors = np.zeros(2)\n\n            for a in A:\n                err = np.array(self.__err(Xy, A, a)).ravel()\n                t = self.__theta(y, associates[a])\n                errors += np.asarray(err < t)\n\n            # error with <= error without\n            if errors[0] <= errors[1]:\n                selected[i] = False\n\n                for a in A:\n                    associates[a] = np.delete(associates[a], np.where(associates[a] == i))\n\n        return selected, X[selected], y[selected]\n\n\nclass MutualInformationSelection(SelectionAlgorithm):\n\n    def __init__(self, k=6, alpha=0.05):\n        self.k = k\n        self.alpha = alpha\n\n    def select(self, X, y):\n        from sklearn.feature_selection import mutual_info_regression\n        from sklearn.preprocessing import MinMaxScaler\n        from sklearn.neighbors import NearestNeighbors\n\n        n = len(X)\n        mask = np.arange(n)\n\n        mi = [mutual_info_regression(X[mask != i], y[mask != i]) for i in range(n)]\n\n        mi = MinMaxScaler().fit_transform(mi)\n\n        _, neighbors = NearestNeighbors(n_neighbors=self.k + 1).fit(X).kneighbors(X)\n\n        # dropout themselves\n        neighbors = neighbors[:, 1:]\n\n        mask_as_set = set(mask)\n\n        not_neighbors = [list(mask_as_set - set(neighbors[i])) for i in range(n)]\n\n        # mutual information without neighbors\n        nn_mi = [mutual_info_regression(X[not_neighbors[i]], y[not_neighbors[i]]) for i in range(n)]\n\n        selected = np.zeros(len(X), dtype=bool)\n\n        for i in range(n):\n\n            cdiff = np.asarray([(mi[i] - mi[k]) for k in neighbors[i]])\n\n            selected[i] = np.sum(cdiff > self.alpha) < self.k\n\n\n        return selected, X[selected], y[selected]\n\n\n'"
