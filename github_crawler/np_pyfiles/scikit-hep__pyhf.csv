file_path,api_count,code
setup.py,0,"b""from setuptools import setup\n\nextras_require = {\n    'tensorflow': ['tensorflow~=2.0', 'tensorflow-probability~=0.8'],\n    'torch': ['torch~=1.2'],\n    'jax': ['jax~=0.1,>0.1.51', 'jaxlib~=0.1,>0.1.33'],\n    'xmlio': ['uproot'],\n    'minuit': ['iminuit'],\n}\nextras_require['backends'] = sorted(\n    set(\n        extras_require['tensorflow']\n        + extras_require['torch']\n        + extras_require['jax']\n        + extras_require['minuit']\n    )\n)\nextras_require['contrib'] = sorted(set(['matplotlib']))\n\nextras_require['test'] = sorted(\n    set(\n        extras_require['backends']\n        + extras_require['xmlio']\n        + extras_require['contrib']\n        + [\n            'pyflakes',\n            'pytest~=3.5',\n            'pytest-cov>=2.5.1',\n            'pytest-mock',\n            'pytest-benchmark[histogram]',\n            'pytest-console-scripts',\n            'pytest-mpl',\n            'pydocstyle',\n            'coverage>=4.0',  # coveralls\n            'papermill~=2.0',\n            'nteract-scrapbook~=0.2',\n            'check-manifest',\n            'jupyter',\n            'uproot~=3.3',\n            'graphviz',\n            'jsonpatch',\n            'black',\n        ]\n    )\n)\nextras_require['docs'] = sorted(\n    set(\n        [\n            'sphinx!=3.1.0',\n            'sphinxcontrib-bibtex',\n            'sphinx-click',\n            'sphinx_rtd_theme',\n            'nbsphinx',\n            'ipywidgets',\n            'sphinx-issues',\n            'sphinx-copybutton>0.2.9',\n        ]\n    )\n)\nextras_require['develop'] = sorted(\n    set(\n        extras_require['docs']\n        + extras_require['test']\n        + ['nbdime', 'bumpversion', 'ipython', 'pre-commit', 'twine']\n    )\n)\nextras_require['complete'] = sorted(set(sum(extras_require.values(), [])))\n\n\nsetup(\n    extras_require=extras_require,\n    use_scm_version=lambda: {'local_scheme': lambda version: ''},\n)\n"""
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# pyhf documentation build configuration file, created by\n# sphinx-quickstart on Fri Feb  9 11:58:49 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use Path(\'../relative_path_to_dir\').resolve() to make it absolute, like shown here.\n\nfrom pathlib import Path\nimport sys\nfrom pkg_resources import get_distribution\n\nsys.path.insert(0, str(Path(\'../src\').resolve()))\nsys.path.insert(1, str(Path(\'./exts\').resolve()))\n\n\ndef setup(app):\n    app.add_css_file(\n        \'https://cdnjs.cloudflare.com/ajax/libs/github-fork-ribbon-css/0.2.2/gh-fork-ribbon.min.css\'\n    )\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx.ext.ifconfig\',\n    \'sphinx.ext.viewcode\',\n    \'sphinx.ext.githubpages\',\n    \'sphinxcontrib.bibtex\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx_click.ext\',\n    \'nbsphinx\',\n    \'sphinx_issues\',\n    \'sphinx_copybutton\',\n    \'xref\',\n]\n\n# external links\nxref_links = {""arXiv:1007.1727"": (""[1007.1727]"", ""https://arxiv.org/abs/1007.1727"")}\n\n# Github repo\nissues_github_path = \'scikit-hep/pyhf\'\n\n# Generate the API documentation when building\nautosummary_generate = True\nnumpydoc_show_class_members = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_suffix = [\'.rst\', \'.md\']\n# source_suffix = \'.rst\'\n\n# The encoding of source files.\n#\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'pyhf\'\ncopyright = u\'2018, Lukas Heinrich, Matthew Feickert, Giordon Stark\'\nauthor = u\'Lukas Heinrich, Matthew Feickert, Giordon Stark\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n# The full version, including alpha/beta/rc tags.\nrelease = get_distribution(\'pyhf\').version\n# for example take major/minor/patch\nversion = \'.\'.join(release.split(\'.\')[:3])\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#\n# today = \'\'\n#\n# Else, today_fmt is used as the format for a strftime call.\n#\n# today_fmt = \'%B %d, %Y\'\n\nautodoc_mock_imports = [\n    \'tensorflow\',\n    \'torch\',\n    \'jax\',\n    \'iminuit\',\n    \'tensorflow_probability\',\n]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\n    \'_build\',\n    \'**.ipynb_checkpoints\',\n    \'examples/experiments/edwardpyhf.ipynb\',\n    \'examples/notebooks/ImpactPlot.ipynb\',\n    \'examples/notebooks/Recast.ipynb\',\n    \'examples/notebooks/StatError.ipynb\',\n    \'examples/notebooks/example-tensorflow.ipynb\',\n    \'examples/notebooks/histogrammar.ipynb\',\n    \'examples/notebooks/histosys.ipynb\',\n    \'examples/notebooks/histosys-pytorch.ipynb\',\n    \'examples/notebooks/importxml.ipynb\',\n    \'examples/notebooks/multichannel-coupled-normsys.ipynb\',\n    \'examples/notebooks/multichannel-normsys.ipynb\',\n    \'examples/notebooks/normsys.ipynb\',\n    \'examples/notebooks/pullplot.ipynb\',\n    \'examples/notebooks/pytorch_tests_onoff.ipynb\',\n    \'examples/notebooks/tensorflow-limit.ipynb\',\n]\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#\n# default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#\n# add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#\n# add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#\n# show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n# modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n# keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\nhtml_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n#\n# html_title = u\'pyhf v0.3.0\'\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#\n# html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\n#\n# html_logo = None\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#\n# html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#\nhtml_extra_path = [\'_extras\']\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#\n# html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#\n# html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#\n# html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#\n# html_additional_pages = {}\n\n# If false, no module index is generated.\n#\n# html_domain_indices = True\n\n# If false, no index is generated.\n#\n# html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#\n# html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#\n# html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#\n# html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#\n# html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#\n# html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n# html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n#\n# html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#\n# html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#\n# html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'pyhfdoc\'\n\n# sphinx-copybutton configuration\ncopybutton_prompt_text = "">>> ""\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        master_doc,\n        \'pyhf.tex\',\n        u\'pyhf Documentation\',\n        u\'Lukas Heinrich, Matthew Feickert, Giordon Stark\',\n        \'manual\',\n    )\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#\n# latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#\n# latex_use_parts = False\n\n# If true, show page references after internal links.\n#\n# latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#\n# latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#\n# latex_appendices = []\n\n# It false, will not define \\strong, \\code, \titleref, \\crossref ... but only\n# \\sphinxstrong, ..., \\sphinxtitleref, ... To help avoid clash with user added\n# packages.\n#\n# latex_keep_old_macro_names = True\n\n# If false, no module index is generated.\n#\n# latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \'pyhf\', u\'pyhf Documentation\', [author], 1)]\n\n# If true, show URL addresses after external links.\n#\n# man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        \'pyhf\',\n        u\'pyhf Documentation\',\n        author,\n        \'pyhf\',\n        \'One line description of project.\',\n        \'Miscellaneous\',\n    )\n]\n\n# Documents to append as an appendix to all manuals.\n#\n# texinfo_appendices = []\n\n# If false, no module index is generated.\n#\n# texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#\n# texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#\n# texinfo_no_detailmenu = False\n\nmathjax_config = {\n    \'tex2jax\': {\'inlineMath\': [[\'$\', \'$\'], [\'\\\\(\', \'\\\\)\']]},\n    \'TeX\': {\n        \'Macros\': {\n            \'bm\': [""\\\\boldsymbol{#1}"", 1],  # \\usepackage{bm}, see mathjax/MathJax#1219\n            \'HiFa\': r\'\\texttt{HistFactory}\',\n            \'Root\': r\'\\texttt{ROOT}\',\n            \'RooStats\': r\'\\texttt{RooStats}\',\n            \'RooFit\': r\'\\texttt{RooFit}\',\n            \'pyhf\': r\'\\texttt{pyhf}\',\n            \'CLs\': r\'\\mathrm{CL}_{s}\',\n            \'freeset\': r\'\\bm{\\eta}\',\n            \'constrset\': r\'\\bm{\\chi}\',\n            \'singleconstr\': r\'\\chi\',\n            \'channelcounts\': r\'\\bm{n}\',\n            \'auxdata\': r\'\\bm{a}\',\n            \'poiset\': r\'\\bm{\\psi}\',\n            \'nuisset\': r\'\\bm{\\theta}\',\n            \'fullset\': r\'\\bm{\\phi}\',\n            \'singlefull\': r\'\\phi\',\n            \'TeV\': r\'\\textrm{TeV}\',\n        }\n    },\n}\n'"
tests/__init__.py,0,b''
tests/conftest.py,0,"b'import pytest\nimport pyhf\nimport sys\nimport requests\nimport hashlib\nimport tarfile\nimport json\nimport os\nimport pathlib\nimport distutils.dir_util\n\n\n@pytest.fixture(scope=\'module\')\ndef sbottom_likelihoods_download():\n    """"""Download the sbottom likelihoods tarball from HEPData""""""\n    sbottom_HEPData_URL = ""https://doi.org/10.17182/hepdata.89408.v1/r2""\n    targz_filename = ""sbottom_workspaces.tar.gz""\n    response = requests.get(sbottom_HEPData_URL, stream=True)\n    assert response.status_code == 200\n    with open(targz_filename, ""wb"") as file:\n        file.write(response.content)\n    assert (\n        hashlib.sha256(open(targz_filename, ""rb"").read()).hexdigest()\n        == ""9089b0e5fabba335bea4c94545ccca8ddd21289feeab2f85e5bcc8bada37be70""\n    )\n    # Open as a tarfile\n    yield tarfile.open(targz_filename, ""r:gz"")\n    os.remove(targz_filename)\n\n\n# Factory as fixture pattern\n@pytest.fixture\ndef get_json_from_tarfile():\n    def _get_json_from_tarfile(tarfile, json_name):\n        json_file = (\n            tarfile.extractfile(tarfile.getmember(json_name)).read().decode(""utf8"")\n        )\n        return json.loads(json_file)\n\n    return _get_json_from_tarfile\n\n\n@pytest.fixture(scope=\'function\')\ndef isolate_modules():\n    """"""\n    This fixture isolates the sys.modules imported in case you need to mess around with them and do not want to break other tests.\n\n    This is not done automatically.\n    """"""\n    CACHE_MODULES = sys.modules.copy()\n    yield isolate_modules\n    sys.modules.update(CACHE_MODULES)\n\n\n@pytest.fixture(scope=\'function\', autouse=True)\ndef reset_events():\n    """"""\n    This fixture is automatically run to clear out the events registered before and after a test function runs.\n    """"""\n    pyhf.events.__events.clear()\n    pyhf.events.__disabled_events.clear()\n    yield reset_events\n    pyhf.events.__events.clear()\n    pyhf.events.__disabled_events.clear()\n\n\n@pytest.fixture(scope=\'function\', autouse=True)\ndef reset_backend():\n    """"""\n    This fixture is automatically run to reset the backend before and after a test function runs.\n    """"""\n    pyhf.set_backend(pyhf.default_backend)\n    yield reset_backend\n    pyhf.set_backend(pyhf.default_backend)\n\n\n@pytest.fixture(\n    scope=\'function\',\n    params=[\n        (pyhf.tensor.numpy_backend(), None),\n        (pyhf.tensor.pytorch_backend(), None),\n        (pyhf.tensor.pytorch_backend(float=\'float64\', int=\'int64\'), None),\n        (pyhf.tensor.tensorflow_backend(), None),\n        (pyhf.tensor.jax_backend(), None),\n        (\n            pyhf.tensor.numpy_backend(poisson_from_normal=True),\n            pyhf.optimize.minuit_optimizer(),\n        ),\n    ],\n    ids=[\'numpy\', \'pytorch\', \'pytorch64\', \'tensorflow\', \'jax\', \'numpy_minuit\'],\n)\ndef backend(request):\n    # a better way to get the id? all the backends we have so far for testing\n    param_ids = request._fixturedef.ids\n    # the backend we\'re using: numpy, tensorflow, etc...\n    param_id = param_ids[request.param_index]\n    # name of function being called (with params), the original name is .originalname\n    func_name = request._pyfuncitem.name\n\n    # skip backends if specified\n    skip_backend = request.node.get_marker(\'skip_{param}\'.format(param=param_id))\n    # allow the specific backend to fail if specified\n    fail_backend = request.node.get_marker(\'fail_{param}\'.format(param=param_id))\n    # only look at the specific backends\n    only_backends = [\n        pid\n        for pid in param_ids\n        if request.node.get_marker(\'only_{param}\'.format(param=pid))\n    ]\n\n    if skip_backend and (param_id in only_backends):\n        raise ValueError(\n            ""Must specify skip_{param} or only_{param} but not both!"".format(\n                param=param_id\n            )\n        )\n\n    if skip_backend:\n        pytest.skip(""skipping {func} as specified"".format(func=func_name))\n    elif only_backends and param_id not in only_backends:\n        pytest.skip(\n            ""skipping {func} as specified to only look at: {backends}"".format(\n                func=func_name, backends=\', \'.join(only_backends)\n            )\n        )\n\n    if fail_backend:\n        pytest.xfail(""expect {func} to fail as specified"".format(func=func_name))\n\n    # actual execution here, after all checks is done\n    pyhf.set_backend(*request.param)\n\n    yield request.param\n\n\n@pytest.fixture(\n    scope=\'function\',\n    params=[0, 1, 2, 4],\n    ids=[\'interpcode0\', \'interpcode1\', \'interpcode2\', \'interpcode4\'],\n)\ndef interpcode(request):\n    yield request.param\n\n\n@pytest.fixture(scope=\'function\')\ndef datadir(tmpdir, request):\n    \'\'\'\n    Fixture responsible for searching a folder with the same name of test\n    module and, if available, moving all contents to a temporary directory so\n    tests can use them freely.\n    \'\'\'\n    # this gets the module name (e.g. /path/to/pyhf/tests/test_schema.py)\n    # and then gets the directory by removing the suffix (e.g. /path/to/pyhf/tests/test_schema)\n    test_dir = pathlib.Path(request.module.__file__).with_suffix(\'\')\n\n    if test_dir.is_dir():\n        distutils.dir_util.copy_tree(test_dir, tmpdir.strpath)\n        # shutil is nicer, but doesn\'t work: https://bugs.python.org/issue20849\n        # shutil.copytree(test_dir, tmpdir)\n\n    return tmpdir\n'"
tests/test_backend_consistency.py,10,"b'import pyhf\nimport numpy as np\nimport pytest\n\n\ndef generate_source_static(n_bins):\n    """"""\n    Create the source structure for the given number of bins.\n\n    Args:\n        n_bins: `list` of number of bins\n\n    Returns:\n        source\n    """"""\n    binning = [n_bins, -0.5, n_bins + 0.5]\n    data = [120.0] * n_bins\n    bkg = [100.0] * n_bins\n    bkgerr = [10.0] * n_bins\n    sig = [30.0] * n_bins\n\n    source = {\n        \'binning\': binning,\n        \'bindata\': {\'data\': data, \'bkg\': bkg, \'bkgerr\': bkgerr, \'sig\': sig},\n    }\n    return source\n\n\ndef generate_source_poisson(n_bins):\n    """"""\n    Create the source structure for the given number of bins.\n    Sample from a Poisson distribution\n\n    Args:\n        n_bins: `list` of number of bins\n\n    Returns:\n        source\n    """"""\n    np.random.seed(0)  # Fix seed for reproducibility\n    binning = [n_bins, -0.5, n_bins + 0.5]\n    data = np.random.poisson(120.0, n_bins).tolist()\n    bkg = np.random.poisson(100.0, n_bins).tolist()\n    bkgerr = np.random.poisson(10.0, n_bins).tolist()\n    sig = np.random.poisson(30.0, n_bins).tolist()\n\n    source = {\n        \'binning\': binning,\n        \'bindata\': {\'data\': data, \'bkg\': bkg, \'bkgerr\': bkgerr, \'sig\': sig},\n    }\n    return source\n\n\n# bins = [1, 10, 50, 100, 200, 500, 800, 1000]\nbins = [50, 500]\nbin_ids = [\'{}_bins\'.format(n_bins) for n_bins in bins]\n\n\n@pytest.mark.parametrize(\'n_bins\', bins, ids=bin_ids)\n@pytest.mark.parametrize(\'invert_order\', [False, True], ids=[\'normal\', \'inverted\'])\ndef test_hypotest_q_mu(\n    n_bins, invert_order, tolerance={\'numpy\': 1e-02, \'tensors\': 5e-03}\n):\n    """"""\n    Check that the different backends all compute a test statistic\n    that is within a specific tolerance of each other.\n\n    Args:\n        n_bins: `list` of number of bins given by pytest parameterization\n        tolerance: `dict` of the maximum differences the test statistics\n                    can differ relative to each other\n\n    Returns:\n        None\n    """"""\n\n    source = generate_source_static(n_bins)\n\n    signal_sample = {\n        \'name\': \'signal\',\n        \'data\': source[\'bindata\'][\'sig\'],\n        \'modifiers\': [{\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}],\n    }\n\n    background_sample = {\n        \'name\': \'background\',\n        \'data\': source[\'bindata\'][\'bkg\'],\n        \'modifiers\': [\n            {\n                \'name\': \'uncorr_bkguncrt\',\n                \'type\': \'shapesys\',\n                \'data\': source[\'bindata\'][\'bkgerr\'],\n            }\n        ],\n    }\n    samples = (\n        [background_sample, signal_sample]\n        if invert_order\n        else [signal_sample, background_sample]\n    )\n    spec = {\'channels\': [{\'name\': \'singlechannel\', \'samples\': samples}]}\n    pdf = pyhf.Model(spec)\n\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    backends = [\n        pyhf.tensor.numpy_backend(),\n        pyhf.tensor.tensorflow_backend(),\n        pyhf.tensor.pytorch_backend(),\n        pyhf.tensor.jax_backend(),\n    ]\n\n    test_statistic = []\n    for backend in backends:\n        pyhf.set_backend(backend)\n\n        q_mu = pyhf.infer.test_statistics.qmu(\n            1.0, data, pdf, pdf.config.suggested_init(), pdf.config.suggested_bounds(),\n        )\n        test_statistic.append(pyhf.tensorlib.tolist(q_mu))\n\n    # compare to NumPy/SciPy\n    test_statistic = np.array(test_statistic)\n    numpy_ratio = np.divide(test_statistic, test_statistic[0])\n    numpy_ratio_delta_unity = np.absolute(np.subtract(numpy_ratio, 1))\n\n    # compare tensor libraries to each other\n    tensors_ratio = np.divide(test_statistic[1], test_statistic[2])\n    tensors_ratio_delta_unity = np.absolute(np.subtract(tensors_ratio, 1))\n\n    try:\n        assert (numpy_ratio_delta_unity < tolerance[\'numpy\']).all()\n    except AssertionError:\n        print(\n            \'Ratio to NumPy+SciPy exceeded tolerance of {}: {}\'.format(\n                tolerance[\'numpy\'], numpy_ratio_delta_unity.tolist()\n            )\n        )\n        assert False\n    try:\n        assert (tensors_ratio_delta_unity < tolerance[\'tensors\']).all()\n    except AssertionError:\n        print(\n            \'Ratio between tensor backends exceeded tolerance of {}: {}\'.format(\n                tolerance[\'tensors\'], tensors_ratio_delta_unity.tolist()\n            )\n        )\n        assert False\n'"
tests/test_combined_modifiers.py,56,"b""from pyhf.modifiers.histosys import histosys_combined\nfrom pyhf.modifiers.normsys import normsys_combined\nfrom pyhf.modifiers.lumi import lumi_combined\nfrom pyhf.modifiers.staterror import staterror_combined\nfrom pyhf.modifiers.shapesys import shapesys_combined\nfrom pyhf.modifiers.normfactor import normfactor_combined\nfrom pyhf.modifiers.shapefactor import shapefactor_combined\nfrom pyhf.parameters import paramset\nimport numpy as np\nimport pyhf\n\n\nclass MockConfig(object):\n    def __init__(self, par_map, par_order, samples, channels=None, channel_nbins=None):\n        self.par_order = par_order\n        self.par_map = par_map\n        self.samples = samples\n        self.channels = channels\n        self.channel_nbins = channel_nbins\n        self.npars = len(self.suggested_init())\n\n    def suggested_init(self):\n        init = []\n        for name in self.par_order:\n            init = init + self.par_map[name]['paramset'].suggested_init\n        return init\n\n    def suggested_bounds(self):\n        bounds = []\n        for name in self.par_order:\n            bounds = bounds + self.par_map[name]['paramset'].suggested_bounds\n        return bounds\n\n    def par_slice(self, name):\n        return self.par_map[name]['slice']\n\n    def param_set(self, name):\n        return self.par_map[name]['paramset']\n\n\ndef test_histosys(backend):\n    mc = MockConfig(\n        par_map={\n            'hello': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[-5, 5]]),\n                'slice': slice(0, 1),\n            },\n            'world': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[-5, 5]]),\n                'slice': slice(1, 2),\n            },\n        },\n        par_order=['hello', 'world'],\n        samples=['signal', 'background'],\n    )\n\n    mega_mods = {\n        'histosys/hello': {\n            'signal': {\n                'type': 'histosys',\n                'name': 'hello',\n                'data': {\n                    'hi_data': [11, 12, 13],\n                    'lo_data': [9, 8, 7],\n                    'nom_data': [10, 10, 10],\n                    'mask': [True, True, True],\n                },\n            },\n            'background': {\n                'type': 'histosys',\n                'name': 'hello',\n                'data': {\n                    'hi_data': [11, 12, 13],\n                    'lo_data': [9, 8, 7],\n                    'nom_data': [10, 10, 10],\n                    'mask': [True, True, True],\n                },\n            },\n        },\n        'histosys/world': {\n            'signal': {\n                'type': 'histosys',\n                'name': 'world',\n                'data': {\n                    'hi_data': [10, 10, 10],\n                    'lo_data': [5, 6, 7],\n                    'nom_data': [10, 10, 10],\n                    'mask': [True, True, True],\n                },\n            },\n            'background': {\n                'type': 'histosys',\n                'name': 'world',\n                'data': {\n                    'hi_data': [10, 10, 10],\n                    'lo_data': [5, 6, 7],\n                    'nom_data': [10, 10, 10],\n                    'mask': [True, True, True],\n                },\n            },\n        },\n    }\n\n    hsc = histosys_combined(\n        [('hello', 'histosys'), ('world', 'histosys')], mc, mega_mods\n    )\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([0.5, -1.0]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 1, 3)\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [0.5, 1.0, 1.5])\n\n    hsc = histosys_combined(\n        [('hello', 'histosys'), ('world', 'histosys')], mc, mega_mods, batch_size=4\n    )\n\n    mod = hsc.apply(\n        pyhf.tensorlib.astensor([[-1.0, -1.0], [1.0, 1.0], [-1.0, 1.0], [1.0, 1.0]])\n    )\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 4, 3)\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [-1.0, -2.0, -3.0])\n    assert np.allclose(mod[0, 0, 1], [1.0, 2.0, 3.0])\n    assert np.allclose(mod[0, 0, 2], [-1.0, -2.0, -3.0])\n    assert np.allclose(mod[0, 0, 3], [1.0, 2.0, 3.0])\n\n\ndef test_normsys(backend):\n    mc = MockConfig(\n        par_map={\n            'hello': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[-5, 5]]),\n                'slice': slice(0, 1),\n            },\n            'world': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[-5, 5]]),\n                'slice': slice(1, 2),\n            },\n        },\n        par_order=['hello', 'world'],\n        samples=['signal', 'background'],\n    )\n\n    mega_mods = {\n        'normsys/hello': {\n            'signal': {\n                'type': 'normsys',\n                'name': 'hello',\n                'data': {\n                    'hi': [1.1] * 3,\n                    'lo': [0.9] * 3,\n                    'nom_data': [1, 1, 1],\n                    'mask': [True, True, True],\n                },\n            },\n            'background': {\n                'type': 'normsys',\n                'name': 'hello',\n                'data': {\n                    'hi': [1.2] * 3,\n                    'lo': [0.8] * 3,\n                    'nom_data': [1, 1, 1],\n                    'mask': [True, True, True],\n                },\n            },\n        },\n        'normsys/world': {\n            'signal': {\n                'type': 'v',\n                'name': 'world',\n                'data': {\n                    'hi': [1.3] * 3,\n                    'lo': [0.7] * 3,\n                    'nom_data': [1, 1, 1],\n                    'mask': [True, True, True],\n                },\n            },\n            'background': {\n                'type': 'normsys',\n                'name': 'world',\n                'data': {\n                    'hi': [1.4] * 3,\n                    'lo': [0.6] * 3,\n                    'nom_data': [1, 1, 1],\n                    'mask': [True, True, True],\n                },\n            },\n        },\n    }\n\n    hsc = normsys_combined([('hello', 'normsys'), ('world', 'normsys')], mc, mega_mods)\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([1.0, -1.0]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 1, 3)\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [1.1, 1.1, 1.1])\n    assert np.allclose(mod[0, 1, 0], [1.2, 1.2, 1.2])\n    assert np.allclose(mod[1, 0, 0], [0.7, 0.7, 0.7])\n    assert np.allclose(mod[1, 1, 0], [0.6, 0.6, 0.6])\n\n    hsc = normsys_combined(\n        [('hello', 'normsys'), ('world', 'normsys')], mc, mega_mods, batch_size=4\n    )\n\n    mod = hsc.apply(\n        pyhf.tensorlib.astensor([[-1.0, -1.0], [1.0, 1.0], [-1.0, -1.0], [1.0, 1.0]])\n    )\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 4, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [0.9, 0.9, 0.9])\n    assert np.allclose(mod[0, 0, 1], [1.1, 1.1, 1.1])\n    assert np.allclose(mod[0, 0, 2], [0.9, 0.9, 0.9])\n    assert np.allclose(mod[0, 0, 3], [1.1, 1.1, 1.1])\n\n\ndef test_lumi(backend):\n    mc = MockConfig(\n        par_map={\n            'lumi': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[-5, 5]]),\n                'slice': slice(0, 1),\n            }\n        },\n        par_order=['lumi'],\n        samples=['signal', 'background'],\n    )\n\n    mega_mods = {\n        'lumi/lumi': {\n            'signal': {\n                'type': 'lumi',\n                'name': 'lumi',\n                'data': {'mask': [True, True, True]},\n            },\n            'background': {\n                'type': 'lumi',\n                'name': 'lumi',\n                'data': {'mask': [True, True, True]},\n            },\n        },\n    }\n\n    hsc = lumi_combined([('lumi', 'lumi')], mc, mega_mods)\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([0.5]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (1, 2, 1, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [0.5, 0.5, 0.5])\n    assert np.allclose(mod[0, 1, 0], [0.5, 0.5, 0.5])\n\n    hsc = lumi_combined([('lumi', 'lumi')], mc, mega_mods, batch_size=4)\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([[1.0], [2.0], [3.0], [4.0]]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (1, 2, 4, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [1.0, 1.0, 1.0])\n    assert np.allclose(mod[0, 0, 1], [2.0, 2.0, 2.0])\n    assert np.allclose(mod[0, 0, 2], [3.0, 3.0, 3.0])\n    assert np.allclose(mod[0, 0, 3], [4.0, 4.0, 4.0])\n\n\ndef test_stat(backend):\n    mc = MockConfig(\n        par_map={\n            'staterror_chan1': {\n                'paramset': paramset(n_parameters=1, inits=[1], bounds=[[0, 10]]),\n                'slice': slice(0, 1),\n            },\n            'staterror_chan2': {\n                'paramset': paramset(\n                    n_parameters=2, inits=[1, 1], bounds=[[0, 10], [0, 10]]\n                ),\n                'slice': slice(1, 3),\n            },\n        },\n        channels=['chan1', 'chan2'],\n        channel_nbins={'chan1': 1, 'chan2': 2},\n        par_order=['staterror_chan1', 'staterror_chan2'],\n        samples=['signal', 'background'],\n    )\n\n    mega_mods = {\n        'staterror/staterror_chan1': {\n            'signal': {\n                'type': 'staterror',\n                'name': 'staterror_chan1',\n                'data': {\n                    'mask': [True, False, False],\n                    'nom_data': [10, 10, 10],\n                    'uncrt': [1, 0, 0],\n                },\n            },\n            'background': {\n                'type': 'staterror',\n                'name': 'staterror_chan1',\n                'data': {\n                    'mask': [True, False, False],\n                    'nom_data': [10, 10, 10],\n                    'uncrt': [1, 0, 0],\n                },\n            },\n        },\n        'staterror/staterror_chan2': {\n            'signal': {\n                'type': 'staterror',\n                'name': 'staterror_chan2',\n                'data': {\n                    'mask': [False, True, True],\n                    'nom_data': [10, 10, 10],\n                    'uncrt': [0, 1, 1],\n                },\n            },\n            'background': {\n                'type': 'staterror',\n                'name': 'staterror_chan2',\n                'data': {\n                    'mask': [False, True, True],\n                    'nom_data': [10, 10, 10],\n                    'uncrt': [0, 1, 1],\n                },\n            },\n        },\n    }\n    hsc = staterror_combined(\n        [('staterror_chan1', 'staterror'), ('staterror_chan2', 'staterror')],\n        mc,\n        mega_mods,\n    )\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([1.1, 1.2, 1.3]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 1, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [1.1, 1.0, 1.0])\n    assert np.allclose(mod[1, 0, 0], [1, 1.2, 1.3])\n\n\ndef test_shapesys(backend):\n    mc = MockConfig(\n        par_map={\n            'dummy1': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[0, 10]]),\n                'slice': slice(0, 1),\n            },\n            'shapesys1': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[0, 10]]),\n                'slice': slice(1, 2),\n            },\n            'shapesys2': {\n                'paramset': paramset(\n                    n_parameters=2, inits=[0, 0], bounds=[[0, 10], [0, 10]]\n                ),\n                'slice': slice(2, 4),\n            },\n            'dummy2': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[0, 10]]),\n                'slice': slice(4, 5),\n            },\n        },\n        channels=['chan1', 'chan2'],\n        channel_nbins={'chan1': 1, 'chan2': 2},\n        par_order=['dummy1', 'shapesys1', 'shapesys2', 'dummy2'],\n        samples=['signal', 'background'],\n    )\n\n    mega_mods = {\n        'shapesys/shapesys1': {\n            'signal': {\n                'type': 'shapesys',\n                'name': 'shapesys1',\n                'data': {\n                    'mask': [True, False, False],\n                    'nom_data': [10, 10, 10],\n                    'uncrt': [1, 0, 0],\n                },\n            },\n            'background': {\n                'type': 'shapesys',\n                'name': 'shapesys1',\n                'data': {\n                    'mask': [True, False, False],\n                    'nom_data': [10, 10, 10],\n                    'uncrt': [1, 0, 0],\n                },\n            },\n        },\n        'shapesys/shapesys2': {\n            'signal': {\n                'type': 'shapesys',\n                'name': 'shapesys1',\n                'data': {\n                    'mask': [False, True, True],\n                    'nom_data': [10, 10, 10],\n                    'uncrt': [0, 1, 1],\n                },\n            },\n            'background': {\n                'type': 'shapesys',\n                'name': 'shapesys1',\n                'data': {\n                    'mask': [False, True, True],\n                    'nom_data': [10, 10, 10],\n                    'uncrt': [0, 1, 1],\n                },\n            },\n        },\n    }\n    hsc = shapesys_combined(\n        [('shapesys1', 'shapesys'), ('shapesys2', 'shapesys')], mc, mega_mods\n    )\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([-10, 1.1, 1.2, 1.3, -20]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 1, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [1.1, 1.0, 1.0])\n    assert np.allclose(mod[1, 0, 0], [1, 1.2, 1.3])\n\n\ndef test_normfactor(backend):\n    mc = MockConfig(\n        par_map={\n            'mu1': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[0, 10]]),\n                'slice': slice(0, 1),\n            },\n            'mu2': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[0, 10]]),\n                'slice': slice(1, 2),\n            },\n        },\n        par_order=['mu1', 'mu2'],\n        samples=['signal', 'background'],\n    )\n\n    mega_mods = {\n        'normfactor/mu1': {\n            'signal': {\n                'type': 'normfactor',\n                'name': 'mu1',\n                'data': {'mask': [True, False, False]},\n            },\n            'background': {\n                'type': 'normfactor',\n                'name': 'mu1',\n                'data': {'mask': [True, False, False]},\n            },\n        },\n        'normfactor/mu2': {\n            'signal': {\n                'type': 'normfactor',\n                'name': 'mu2',\n                'data': {'mask': [False, True, True]},\n            },\n            'background': {\n                'type': 'normfactor',\n                'name': 'mu2',\n                'data': {'mask': [False, True, True]},\n            },\n        },\n    }\n    hsc = normfactor_combined(\n        [('mu1', 'normfactor'), ('mu2', 'normfactor')], mc, mega_mods\n    )\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([2.0, 3.0]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 1, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [2.0, 1.0, 1.0])\n    assert np.allclose(mod[1, 0, 0], [1.0, 3.0, 3.0])\n\n    hsc = normfactor_combined(\n        [('mu1', 'normfactor'), ('mu2', 'normfactor')], mc, mega_mods, batch_size=4\n    )\n\n    mod = hsc.apply(\n        pyhf.tensorlib.astensor([[1.0, 5.0], [2.0, 6.0], [3.0, 7.0], [4.0, 8.0]])\n    )\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 4, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [1.0, 1.0, 1.0])\n    assert np.allclose(mod[0, 0, 1], [2.0, 1.0, 1.0])\n    assert np.allclose(mod[0, 0, 2], [3.0, 1.0, 1.0])\n    assert np.allclose(mod[0, 0, 3], [4.0, 1.0, 1.0])\n\n    assert np.allclose(mod[1, 0, 0], [1.0, 5.0, 5.0])\n    assert np.allclose(mod[1, 0, 1], [1.0, 6.0, 6.0])\n    assert np.allclose(mod[1, 0, 2], [1.0, 7.0, 7.0])\n    assert np.allclose(mod[1, 0, 3], [1.0, 8.0, 8.0])\n\n\ndef test_shapesys_zero(backend):\n    mc = MockConfig(\n        par_map={\n            'SigXsecOverSM': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[0, 10]]),\n                'slice': slice(0, 1),\n            },\n            'syst': {\n                'paramset': paramset(\n                    n_parameters=5, inits=[0] * 5, bounds=[[0, 10]] * 5\n                ),\n                'slice': slice(1, 6),\n            },\n            'syst_lowstats': {\n                'paramset': paramset(\n                    n_parameters=0, inits=[0] * 0, bounds=[[0, 10]] * 0\n                ),\n                'slice': slice(6, 6),\n            },\n        },\n        channels=['channel1'],\n        channel_nbins={'channel1': 6},\n        par_order=['SigXsecOverSM', 'syst', 'syst_lowstats'],\n        samples=['signal', 'background'],\n    )\n\n    mega_mods = {\n        'shapesys/syst': {\n            'background': {\n                'type': 'shapesys',\n                'name': 'syst',\n                'data': {\n                    'mask': [True, True, False, True, True, True],\n                    'nom_data': [100.0, 90.0, 0.0, 70, 0.1, 50],\n                    'uncrt': [10, 9, 1, 0.0, 0.1, 5],\n                },\n            },\n            'signal': {\n                'type': 'shapesys',\n                'name': 'syst',\n                'data': {\n                    'mask': [False, False, False, False, False, False],\n                    'nom_data': [20.0, 10.0, 5.0, 3.0, 2.0, 1.0],\n                    'uncrt': [10, 9, 1, 0.0, 0.1, 5],\n                },\n            },\n        },\n        'shapesys/syst_lowstats': {\n            'background': {\n                'type': 'shapesys',\n                'name': 'syst_lowstats',\n                'data': {\n                    'mask': [False, False, False, False, False, False],\n                    'nom_data': [100.0, 90.0, 0.0, 70, 0.1, 50],\n                    'uncrt': [0, 0, 0, 0, 0, 0],\n                },\n            },\n            'signal': {\n                'type': 'shapesys',\n                'name': 'syst',\n                'data': {\n                    'mask': [False, False, False, False, False, False],\n                    'nom_data': [20.0, 10.0, 5.0, 3.0, 2.0, 1.0],\n                    'uncrt': [10, 9, 1, 0.0, 0.1, 5],\n                },\n            },\n        },\n    }\n    hsc = shapesys_combined(\n        [('syst', 'shapesys'), ('syst_lowstats', 'shapesys')], mc, mega_mods\n    )\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([-10, 1.1, 1.2, 1.3, -20, -30]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 1, 6)\n\n    # expect the 'background' sample to have a single masked bin for 'syst'\n    assert mod[0, 1, 0, 2] == 1.0\n    # expect the 'background' sample to have all bins masked for 'syst_lowstats'\n    assert np.all(kappa == 1 for kappa in mod[1, 1, 0])\n\n\ndef test_shapefactor(backend):\n    mc = MockConfig(\n        par_map={\n            'shapefac1': {\n                'paramset': paramset(n_parameters=1, inits=[0], bounds=[[0, 10]]),\n                'slice': slice(0, 1),\n            },\n            'shapefac2': {\n                'paramset': paramset(\n                    n_parameters=2, inits=[0, 0], bounds=[[0, 10], [0, 10]]\n                ),\n                'slice': slice(1, 3),\n            },\n        },\n        par_order=['shapefac1', 'shapefac2'],\n        samples=['signal', 'background'],\n        channels=['chan_one', 'chan_two'],\n        channel_nbins={'chan_one': 1, 'chan_two': 2},\n    )\n\n    mega_mods = {\n        'shapefactor/shapefac1': {\n            'signal': {\n                'type': 'shapefactor',\n                'name': 'shapefac1',\n                'data': {'mask': [True, False, False]},\n            },\n            'background': {\n                'type': 'shapefactor',\n                'name': 'shapefac1',\n                'data': {'mask': [True, False, False]},\n            },\n        },\n        'shapefactor/shapefac2': {\n            'signal': {\n                'type': 'shapefactor',\n                'name': 'shapefac2',\n                'data': {'mask': [False, True, True]},\n            },\n            'background': {\n                'type': 'normfactor',\n                'name': 'shapefac2',\n                'data': {'mask': [False, True, True]},\n            },\n        },\n    }\n\n    hsc = shapefactor_combined(\n        [('shapefac1', 'shapefactor'), ('shapefac2', 'shapefactor')], mc, mega_mods\n    )\n\n    mod = hsc.apply(pyhf.tensorlib.astensor([2.0, 3.0, 4.0]))\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 1, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [2.0, 1.0, 1.0])\n    assert np.allclose(mod[1, 0, 0], [1.0, 3.0, 4.0])\n\n    hsc = shapefactor_combined(\n        [('shapefac1', 'shapefactor'), ('shapefac2', 'shapefactor')],\n        mc,\n        mega_mods,\n        batch_size=4,\n    )\n    mod = hsc.apply(\n        pyhf.tensorlib.astensor(\n            [[2.0, 3.0, 4.0], [5.0, 6.0, 7.0], [8.0, 9.0, 10.0], [11.0, 12.0, 13.0]]\n        )\n    )\n    shape = pyhf.tensorlib.shape(mod)\n    assert shape == (2, 2, 4, 3)\n\n    mod = np.asarray(pyhf.tensorlib.tolist(mod))\n    assert np.allclose(mod[0, 0, 0], [2.0, 1.0, 1.0])\n    assert np.allclose(mod[0, 0, 1], [5.0, 1.0, 1.0])\n    assert np.allclose(mod[0, 0, 2], [8.0, 1.0, 1.0])\n    assert np.allclose(mod[0, 0, 3], [11.0, 1.0, 1.0])\n\n    assert np.allclose(mod[1, 0, 0], [1.0, 3.0, 4.0])\n    assert np.allclose(mod[1, 0, 1], [1.0, 6.0, 7.0])\n    assert np.allclose(mod[1, 0, 2], [1.0, 9.0, 10.0])\n    assert np.allclose(mod[1, 0, 3], [1.0, 12.0, 13.0])\n"""
tests/test_constraints.py,10,"b""import pytest\nimport pyhf\nfrom pyhf.parameters import constrained_by_poisson, constrained_by_normal\nfrom pyhf.constraints import gaussian_constraint_combined, poisson_constraint_combined\nfrom pyhf import default_backend\nimport numpy as np\n\n\nclass MockConfig(object):\n    def __init__(self, par_map, par_order):\n        self.par_order = par_order\n        self.par_map = par_map\n\n        self.auxdata = []\n        self.auxdata_order = []\n        for name in self.par_order:\n            self.auxdata = self.auxdata + self.par_map[name]['paramset'].auxdata\n            self.auxdata_order.append(name)\n        self.npars = len(self.suggested_init())\n\n    def suggested_init(self):\n        init = []\n        for name in self.par_order:\n            init = init + self.par_map[name]['paramset'].suggested_init\n        return init\n\n    def par_slice(self, name):\n        return self.par_map[name]['slice']\n\n    def param_set(self, name):\n        return self.par_map[name]['paramset']\n\n\ndef test_numpy_pdf_inputs(backend):\n    spec = {\n        'channels': [\n            {\n                'name': 'firstchannel',\n                'samples': [\n                    {\n                        'name': 'mu',\n                        'data': [10.0, 10.0],\n                        'modifiers': [\n                            {'name': 'mu', 'type': 'normfactor', 'data': None}\n                        ],\n                    },\n                    {\n                        'name': 'bkg1',\n                        'data': [50.0, 70.0],\n                        'modifiers': [\n                            {\n                                'name': 'stat_firstchannel',\n                                'type': 'staterror',\n                                'data': [12.0, 12.0],\n                            }\n                        ],\n                    },\n                    {\n                        'name': 'bkg2',\n                        'data': [30.0, 20.0],\n                        'modifiers': [\n                            {\n                                'name': 'stat_firstchannel',\n                                'type': 'staterror',\n                                'data': [5.0, 5.0],\n                            }\n                        ],\n                    },\n                    {\n                        'name': 'bkg3',\n                        'data': [20.0, 15.0],\n                        'modifiers': [\n                            {'name': 'bkg_norm', 'type': 'shapesys', 'data': [10, 10]}\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n\n    m = pyhf.Model(spec)\n\n    def slow(self, auxdata, pars):\n        tensorlib, _ = pyhf.get_backend()\n        # iterate over all constraints order doesn't matter....\n        start_index = 0\n        summands = None\n        for cname in self.config.auxdata_order:\n            parset, parslice = (\n                self.config.param_set(cname),\n                self.config.par_slice(cname),\n            )\n            end_index = start_index + parset.n_parameters\n            thisauxdata = auxdata[start_index:end_index]\n            start_index = end_index\n            if parset.pdf_type == 'normal':\n                paralphas = pars[parslice]\n                sigmas = (\n                    parset.sigmas\n                    if hasattr(parset, 'sigmas')\n                    else tensorlib.ones(paralphas.shape)\n                )\n                sigmas = tensorlib.astensor(sigmas)\n                constraint_term = tensorlib.normal_logpdf(\n                    thisauxdata, paralphas, sigmas\n                )\n            elif parset.pdf_type == 'poisson':\n                paralphas = tensorlib.product(\n                    tensorlib.stack(\n                        [pars[parslice], tensorlib.astensor(parset.factors)]\n                    ),\n                    axis=0,\n                )\n\n                constraint_term = tensorlib.poisson_logpdf(thisauxdata, paralphas)\n            summands = (\n                constraint_term\n                if summands is None\n                else tensorlib.concatenate([summands, constraint_term])\n            )\n        return tensorlib.sum(summands) if summands is not None else 0\n\n    def fast(self, auxdata, pars):\n        return self.constraint_logpdf(auxdata, pars)\n\n    auxd = pyhf.tensorlib.astensor(m.config.auxdata)\n    pars = pyhf.tensorlib.astensor(m.config.suggested_init())\n    slow_result = pyhf.tensorlib.tolist(slow(m, auxd, pars))\n    fast_result = pyhf.tensorlib.tolist(fast(m, auxd, pars))\n    assert pytest.approx(slow_result) == fast_result\n\n\ndef test_batched_constraints(backend):\n    config = MockConfig(\n        par_order=['pois1', 'pois2', 'norm1', 'norm2'],\n        par_map={\n            'pois1': {\n                'paramset': constrained_by_poisson(\n                    n_parameters=1,\n                    inits=[1.0],\n                    bounds=[[0, 10]],\n                    auxdata=[12],\n                    factors=[12],\n                ),\n                'slice': slice(0, 1),\n                'auxdata': [1],\n            },\n            'pois2': {\n                'paramset': constrained_by_poisson(\n                    n_parameters=2,\n                    inits=[1.0] * 2,\n                    bounds=[[0, 10]] * 2,\n                    auxdata=[13, 14],\n                    factors=[13, 14],\n                ),\n                'slice': slice(1, 3),\n            },\n            'norm1': {\n                'paramset': constrained_by_normal(\n                    n_parameters=2,\n                    inits=[0] * 2,\n                    bounds=[[0, 10]] * 2,\n                    auxdata=[0, 0],\n                    sigmas=[1.5, 2.0],\n                ),\n                'slice': slice(3, 5),\n            },\n            'norm2': {\n                'paramset': constrained_by_normal(\n                    n_parameters=3,\n                    inits=[0] * 3,\n                    bounds=[[0, 10]] * 3,\n                    auxdata=[0, 0, 0],\n                ),\n                'slice': slice(5, 8),\n            },\n        },\n    )\n    suggested_pars = [1.0] * 3 + [0.0] * 5  # 2 pois 5 norm\n    constraint = poisson_constraint_combined(config)\n    result = default_backend.astensor(\n        pyhf.tensorlib.tolist(\n            constraint.logpdf(\n                pyhf.tensorlib.astensor(config.auxdata),\n                pyhf.tensorlib.astensor(suggested_pars),\n            )\n        )\n    )\n    assert np.isclose(\n        result[0],\n        sum(\n            [\n                default_backend.poisson_logpdf(data, rate)\n                for data, rate in zip([12, 13, 14], [12, 13, 14])\n            ]\n        ),\n    )\n    assert result.shape == (1,)\n\n    suggested_pars = [1.1] * 3 + [0.0] * 5  # 2 pois 5 norm\n    constraint = poisson_constraint_combined(config)\n    result = default_backend.astensor(\n        pyhf.tensorlib.tolist(\n            constraint.logpdf(\n                pyhf.tensorlib.astensor(config.auxdata),\n                pyhf.tensorlib.astensor(suggested_pars),\n            )\n        )\n    )\n    assert np.isclose(\n        result[0],\n        sum(\n            [\n                default_backend.poisson_logpdf(data, rate)\n                for data, rate in zip([12, 13, 14], [12 * 1.1, 13 * 1.1, 14 * 1.1])\n            ]\n        ),\n    )\n    assert result.shape == (1,)\n\n    constraint = poisson_constraint_combined(config, batch_size=10)\n    result = constraint.logpdf(\n        pyhf.tensorlib.astensor(config.auxdata),\n        pyhf.tensorlib.astensor([suggested_pars] * 10),\n    )\n    assert result.shape == (10,)\n\n    suggested_pars = [\n        [1.1, 1.2, 1.3] + [0.0] * 5,  # 2 pois 5 norm\n        [0.7, 0.8, 0.9] + [0.0] * 5,  # 2 pois 5 norm\n        [0.4, 0.5, 0.6] + [0.0] * 5,  # 2 pois 5 norm\n    ]\n    constraint = poisson_constraint_combined(config, batch_size=3)\n    result = default_backend.astensor(\n        pyhf.tensorlib.tolist(\n            constraint.logpdf(\n                pyhf.tensorlib.astensor(config.auxdata),\n                pyhf.tensorlib.astensor(suggested_pars),\n            )\n        )\n    )\n    assert np.all(\n        np.isclose(\n            result,\n            np.sum(\n                [\n                    [\n                        default_backend.poisson_logpdf(data, rate)\n                        for data, rate in zip(\n                            [12, 13, 14], [12 * 1.1, 13 * 1.2, 14 * 1.3]\n                        )\n                    ],\n                    [\n                        default_backend.poisson_logpdf(data, rate)\n                        for data, rate in zip(\n                            [12, 13, 14], [12 * 0.7, 13 * 0.8, 14 * 0.9]\n                        )\n                    ],\n                    [\n                        default_backend.poisson_logpdf(data, rate)\n                        for data, rate in zip(\n                            [12, 13, 14], [12 * 0.4, 13 * 0.5, 14 * 0.6]\n                        )\n                    ],\n                ],\n                axis=1,\n            ),\n        )\n    )\n    assert result.shape == (3,)\n\n    suggested_pars = [1.0] * 3 + [0.0] * 5  # 2 pois 5 norm\n    constraint = gaussian_constraint_combined(config, batch_size=1)\n    result = default_backend.astensor(\n        pyhf.tensorlib.tolist(\n            constraint.logpdf(\n                pyhf.tensorlib.astensor(config.auxdata),\n                pyhf.tensorlib.astensor(suggested_pars),\n            )\n        )\n    )\n    assert np.isclose(\n        result[0],\n        sum(\n            [\n                default_backend.normal_logpdf(data, mu, sigma)\n                for data, mu, sigma in zip(\n                    [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [1.5, 2.0, 1.0, 1.0, 1.0]\n                )\n            ]\n        ),\n    )\n    assert result.shape == (1,)\n\n    suggested_pars = [1.0] * 3 + [1, 2, 3, 4, 5]  # 2 pois 5 norm\n    constraint = gaussian_constraint_combined(config, batch_size=1)\n    result = default_backend.astensor(\n        pyhf.tensorlib.tolist(\n            constraint.logpdf(\n                pyhf.tensorlib.astensor(config.auxdata),\n                pyhf.tensorlib.astensor(suggested_pars),\n            )\n        )\n    )\n    assert np.isclose(\n        result[0],\n        sum(\n            [\n                default_backend.normal_logpdf(data, mu, sigma)\n                for data, mu, sigma in zip(\n                    [0, 0, 0, 0, 0], [1, 2, 3, 4, 5], [1.5, 2.0, 1.0, 1.0, 1.0]\n                )\n            ]\n        ),\n    )\n    assert result.shape == (1,)\n\n    suggested_pars = [\n        [1.0] * 3 + [1, 2, 3, 4, 5],  # 2 pois 5 norm\n        [1.0] * 3 + [-1, -2, -3, -4, -5],  # 2 pois 5 norm\n        [1.0] * 3 + [-1, -2, 0, 1, 2],  # 2 pois 5 norm\n    ]\n    constraint = gaussian_constraint_combined(config, batch_size=3)\n    result = default_backend.astensor(\n        pyhf.tensorlib.tolist(\n            constraint.logpdf(\n                pyhf.tensorlib.astensor(config.auxdata),\n                pyhf.tensorlib.astensor(suggested_pars),\n            )\n        )\n    )\n    assert np.all(\n        np.isclose(\n            result,\n            np.sum(\n                [\n                    [\n                        default_backend.normal_logpdf(data, mu, sigma)\n                        for data, mu, sigma in zip(\n                            [0, 0, 0, 0, 0], [1, 2, 3, 4, 5], [1.5, 2.0, 1.0, 1.0, 1.0]\n                        )\n                    ],\n                    [\n                        default_backend.normal_logpdf(data, mu, sigma)\n                        for data, mu, sigma in zip(\n                            [0, 0, 0, 0, 0],\n                            [-1, -2, -3, -4, -5],\n                            [1.5, 2.0, 1.0, 1.0, 1.0],\n                        )\n                    ],\n                    [\n                        default_backend.normal_logpdf(data, mu, sigma)\n                        for data, mu, sigma in zip(\n                            [0, 0, 0, 0, 0],\n                            [-1, -2, 0, 1, 2],\n                            [1.5, 2.0, 1.0, 1.0, 1.0],\n                        )\n                    ],\n                ],\n                axis=1,\n            ),\n        )\n    )\n    assert result.shape == (3,)\n\n    constraint = gaussian_constraint_combined(config, batch_size=10)\n    result = constraint.logpdf(\n        pyhf.tensorlib.astensor(config.auxdata),\n        pyhf.tensorlib.astensor([suggested_pars] * 10),\n    )\n    assert result.shape == (10,)\n"""
tests/test_events.py,0,"b""import pyhf.events as events\nimport mock\n\n\ndef test_subscribe_event():\n    ename = 'test'\n\n    m = mock.Mock()\n    events.subscribe(ename)(m)\n\n    assert ename in events.__events\n    assert m in events.__events.get(ename)\n    del events.__events[ename]\n\n\ndef test_event():\n    ename = 'test'\n\n    m = mock.Mock()\n    events.subscribe(ename)(m)\n\n    events.trigger(ename)()\n    m.assert_called_once()\n    del events.__events[ename]\n\n\ndef test_disable_event():\n    ename = 'test'\n\n    m = mock.Mock()\n    noop, noop_m = events.noop, mock.Mock()\n    events.noop = noop_m\n    events.subscribe(ename)(m)\n\n    events.disable(ename)\n    assert m.called is False\n    assert ename in events.__disabled_events\n    assert events.trigger(ename) == events.noop\n    assert events.trigger(ename)() == events.noop()\n    assert m.called is False\n    assert noop_m.is_called_once()\n    events.enable(ename)\n    assert ename not in events.__disabled_events\n    del events.__events[ename]\n    events.noop = noop\n\n\ndef test_trigger_noevent():\n    noop, noop_m = events.noop, mock.Mock()\n\n    assert 'fake' not in events.__events\n    assert events.trigger('fake') == events.noop\n    assert events.trigger('fake')() == events.noop()\n    assert noop_m.is_called_once()\n\n    events.noop = noop\n"""
tests/test_examples.py,0,"b""import shlex\n\n\ndef test_2bin_1channel(tmpdir, script_runner):\n    command = 'pyhf inspect {0:s}'.format('docs/examples/json/2-bin_1-channel.json')\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n"""
tests/test_export.py,0,"b'import pyhf\nimport pyhf.writexml\nimport pytest\nimport json\nimport xml.etree.cElementTree as ET\n\n\ndef spec_staterror():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'firstchannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'mu\',\n                        \'data\': [10.0, 10.0],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg1\',\n                        \'data\': [50.0, 70.0],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'stat_firstchannel\',\n                                \'type\': \'staterror\',\n                                \'data\': [12.0, 12.0],\n                            }\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg2\',\n                        \'data\': [30.0, 20.0],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'stat_firstchannel\',\n                                \'type\': \'staterror\',\n                                \'data\': [5.0, 5.0],\n                            }\n                        ],\n                    },\n                    {\'name\': \'bkg3\', \'data\': [20.0, 15.0], \'modifiers\': []},\n                ],\n            }\n        ]\n    }\n    return spec\n\n\ndef spec_histosys():\n    source = json.load(open(\'validation/data/2bin_histosys_example2.json\'))\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'bkg_norm\',\n                                \'type\': \'histosys\',\n                                \'data\': {\n                                    \'lo_data\': source[\'bindata\'][\'bkgsys_dn\'],\n                                    \'hi_data\': source[\'bindata\'][\'bkgsys_up\'],\n                                },\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    return spec\n\n\ndef spec_normsys():\n    source = json.load(open(\'validation/data/2bin_histosys_example2.json\'))\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'bkg_norm\',\n                                \'type\': \'normsys\',\n                                \'data\': {\'lo\': 0.9, \'hi\': 1.1},\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    return spec\n\n\ndef spec_shapesys():\n    source = json.load(open(\'validation/data/2bin_histosys_example2.json\'))\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\'name\': \'bkg_norm\', \'type\': \'shapesys\', \'data\': [10, 10]}\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    return spec\n\n\ndef test_export_measurement():\n    measurementspec = {\n        ""config"": {\n            ""parameters"": [\n                {\n                    ""auxdata"": [1.0],\n                    ""bounds"": [[0.855, 1.145]],\n                    ""inits"": [1.0],\n                    ""name"": ""lumi"",\n                    ""sigmas"": [0.029],\n                }\n            ],\n            ""poi"": ""mu"",\n        },\n        ""name"": ""NormalMeasurement"",\n    }\n    m = pyhf.writexml.build_measurement(measurementspec)\n    assert m is not None\n    assert m.attrib[\'Name\'] == measurementspec[\'name\']\n    assert m.attrib[\'Lumi\'] == str(\n        measurementspec[\'config\'][\'parameters\'][0][\'auxdata\'][0]\n    )\n    assert m.attrib[\'LumiRelErr\'] == str(\n        measurementspec[\'config\'][\'parameters\'][0][\'sigmas\'][0]\n    )\n    poi = m.find(\'POI\')\n    assert poi is not None\n    assert poi.text == measurementspec[\'config\'][\'poi\']\n    paramsetting = m.find(\'ParamSetting\')\n    assert paramsetting is None\n\n\n@pytest.mark.parametrize(\n    ""spec, has_root_data, attrs"",\n    [\n        (spec_staterror(), True, [\'Activate\', \'HistoName\']),\n        (spec_histosys(), True, [\'HistoNameHigh\', \'HistoNameLow\']),\n        (spec_normsys(), False, [\'High\', \'Low\']),\n        (spec_shapesys(), True, [\'ConstraintType\', \'HistoName\']),\n    ],\n    ids=[\'staterror\', \'histosys\', \'normsys\', \'shapesys\'],\n)\ndef test_export_modifier(mocker, spec, has_root_data, attrs):\n    channelspec = spec[\'channels\'][0]\n    channelname = channelspec[\'name\']\n    samplespec = channelspec[\'samples\'][1]\n    samplename = samplespec[\'name\']\n    sampledata = samplespec[\'data\']\n    modifierspec = samplespec[\'modifiers\'][0]\n\n    mocker.patch(\'pyhf.writexml._ROOT_DATA_FILE\')\n    modifier = pyhf.writexml.build_modifier(\n        {\'measurements\': [{\'config\': {\'parameters\': []}}]},\n        modifierspec,\n        channelname,\n        samplename,\n        sampledata,\n    )\n    # if the modifier is a staterror, it has no Name\n    if \'Name\' in modifier.attrib:\n        assert modifier.attrib[\'Name\'] == modifierspec[\'name\']\n    assert all(attr in modifier.attrib for attr in attrs)\n    assert pyhf.writexml._ROOT_DATA_FILE.__setitem__.called == has_root_data\n\n\n@pytest.mark.parametrize(\n    ""spec, normfactor_config"",\n    [\n        (spec_staterror(), dict(name=\'mu\', inits=[1.0], bounds=[[0.0, 8.0]])),\n        (spec_histosys(), dict()),\n        (spec_normsys(), dict(name=\'mu\', inits=[2.0], bounds=[[0.0, 10.0]])),\n        (spec_shapesys(), dict(name=\'mu\', inits=[1.0], bounds=[[5.0, 10.0]])),\n    ],\n    ids=[\'upper-bound\', \'empty-config\', \'init\', \'lower-bound\'],\n)\ndef test_export_modifier_normfactor(mocker, spec, normfactor_config):\n    channelspec = spec[\'channels\'][0]\n    channelname = channelspec[\'name\']\n    samplespec = channelspec[\'samples\'][0]\n    samplename = samplespec[\'name\']\n    sampledata = samplespec[\'data\']\n    modifierspec = samplespec[\'modifiers\'][0]\n\n    mocker.patch(\'pyhf.writexml._ROOT_DATA_FILE\')\n    modifier = pyhf.writexml.build_modifier(\n        {\n            \'measurements\': [\n                {\n                    \'config\': {\n                        \'parameters\': [normfactor_config] if normfactor_config else []\n                    }\n                }\n            ]\n        },\n        modifierspec,\n        channelname,\n        samplename,\n        sampledata,\n    )\n\n    assert all(attr in modifier.attrib for attr in [\'Name\', \'Val\', \'High\', \'Low\'])\n    assert float(modifier.attrib[\'Val\']) == normfactor_config.get(\'inits\', [1.0])[0]\n    assert (\n        float(modifier.attrib[\'Low\'])\n        == normfactor_config.get(\'bounds\', [[0.0, 10.0]])[0][0]\n    )\n    assert (\n        float(modifier.attrib[\'High\'])\n        == normfactor_config.get(\'bounds\', [[0.0, 10.0]])[0][1]\n    )\n\n\n@pytest.mark.parametrize(\n    ""spec"",\n    [spec_staterror(), spec_histosys(), spec_normsys(), spec_shapesys()],\n    ids=[\'staterror\', \'histosys\', \'normsys\', \'shapesys\'],\n)\ndef test_export_sample(mocker, spec):\n    channelspec = spec[\'channels\'][0]\n    channelname = channelspec[\'name\']\n    samplespec = channelspec[\'samples\'][1]\n\n    mocker.patch(\'pyhf.writexml.build_modifier\', return_value=ET.Element(""Modifier""))\n    mocker.patch(\'pyhf.writexml._ROOT_DATA_FILE\')\n    sample = pyhf.writexml.build_sample({}, samplespec, channelname)\n    assert sample.attrib[\'Name\'] == samplespec[\'name\']\n    assert sample.attrib[\'HistoName\']\n    assert sample.attrib[\'InputFile\']\n    assert sample.attrib[\'NormalizeByTheory\'] == str(False)\n    assert pyhf.writexml.build_modifier.called\n    assert pyhf.writexml._ROOT_DATA_FILE.__setitem__.called\n\n\n@pytest.mark.parametrize(\n    ""spec"", [spec_staterror(), spec_shapesys()], ids=[\'staterror\', \'shapesys\']\n)\ndef test_export_sample_zerodata(mocker, spec):\n    channelspec = spec[\'channels\'][0]\n    channelname = channelspec[\'name\']\n    samplespec = channelspec[\'samples\'][1]\n    samplename = samplespec[\'name\']\n    sampledata = [0.0] * len(samplespec[\'data\'])\n\n    mocker.patch(\'pyhf.writexml._ROOT_DATA_FILE\')\n    # make sure no RuntimeWarning, https://stackoverflow.com/a/45671804\n    with pytest.warns(None) as record:\n        for modifierspec in samplespec[\'modifiers\']:\n            pyhf.writexml.build_modifier(\n                {\'measurements\': [{\'config\': {\'parameters\': []}}]},\n                modifierspec,\n                channelname,\n                samplename,\n                sampledata,\n            )\n    assert not record.list\n\n\n@pytest.mark.parametrize(\n    ""spec"",\n    [spec_staterror(), spec_histosys(), spec_normsys(), spec_shapesys()],\n    ids=[\'staterror\', \'histosys\', \'normsys\', \'shapesys\'],\n)\ndef test_export_channel(mocker, spec):\n    channelspec = spec[\'channels\'][0]\n\n    mocker.patch(\'pyhf.writexml.build_data\', return_value=ET.Element(""Data""))\n    mocker.patch(\'pyhf.writexml.build_sample\', return_value=ET.Element(""Sample""))\n    mocker.patch(\'pyhf.writexml._ROOT_DATA_FILE\')\n    channel = pyhf.writexml.build_channel({}, channelspec, {})\n    assert channel.attrib[\'Name\'] == channelspec[\'name\']\n    assert channel.attrib[\'InputFile\']\n    assert pyhf.writexml.build_data.called is False\n    assert pyhf.writexml.build_sample.called\n    assert pyhf.writexml._ROOT_DATA_FILE.__setitem__.called is False\n\n\ndef test_export_data(mocker):\n    channelname = \'channel\'\n    dataspec = [{\'name\': channelname, \'data\': [0, 1, 2, 3]}]\n\n    mocker.patch(\'pyhf.writexml._ROOT_DATA_FILE\')\n    data = pyhf.writexml.build_data(dataspec, channelname)\n    assert data.attrib[\'HistoName\']\n    assert data.attrib[\'InputFile\']\n    assert pyhf.writexml._ROOT_DATA_FILE.__setitem__.called\n'"
tests/test_import.py,1,"b'import pyhf\nimport pyhf.readxml\nimport numpy as np\nimport uproot\nfrom pathlib import Path\nimport pytest\nimport xml.etree.cElementTree as ET\n\n\ndef assert_equal_dictionary(d1, d2):\n    ""recursively compare 2 dictionaries""\n    for k in d1.keys():\n        assert k in d2\n        if isinstance(d1[k], dict):\n            assert_equal_dictionary(d1[k], d2[k])\n        else:\n            assert d1[k] == d2[k]\n\n\ndef test_dedupe_parameters():\n    parameters = [\n        {\'name\': \'SigXsecOverSM\', \'bounds\': [[0.0, 10.0]]},\n        {\'name\': \'SigXsecOverSM\', \'bounds\': [[0.0, 10.0]]},\n    ]\n    assert len(pyhf.readxml.dedupe_parameters(parameters)) == 1\n    parameters[1][\'bounds\'] = [[0.0, 2.0]]\n    with pytest.raises(RuntimeError) as excinfo:\n        pyhf.readxml.dedupe_parameters(parameters)\n        assert \'SigXsecOverSM\' in str(excinfo.value)\n\n\ndef test_process_normfactor_configs():\n    # Check to see if mu_ttbar NormFactor is overridden correctly\n    # - ParamSetting has a config for it\n    # - other_parameter_configs has a config for it\n    # Make sure that when two measurements exist, we\'re copying things across correctly\n    toplvl = ET.Element(""Combination"")\n    meas = ET.Element(\n        ""Measurement"",\n        Name=\'NormalMeasurement\',\n        Lumi=str(1.0),\n        LumiRelErr=str(0.017),\n        ExportOnly=str(True),\n    )\n    poiel = ET.Element(\'POI\')\n    poiel.text = \'mu_SIG\'\n    meas.append(poiel)\n\n    setting = ET.Element(\'ParamSetting\', Const=\'True\')\n    setting.text = \' \'.join([\'Lumi\', \'mu_both\', \'mu_paramSettingOnly\'])\n    meas.append(setting)\n\n    setting = ET.Element(\'ParamSetting\', Val=\'2.0\')\n    setting.text = \' \'.join([\'mu_both\'])\n    meas.append(setting)\n\n    toplvl.append(meas)\n\n    meas = ET.Element(\n        ""Measurement"",\n        Name=\'ParallelMeasurement\',\n        Lumi=str(1.0),\n        LumiRelErr=str(0.017),\n        ExportOnly=str(True),\n    )\n    poiel = ET.Element(\'POI\')\n    poiel.text = \'mu_BKG\'\n    meas.append(poiel)\n\n    setting = ET.Element(\'ParamSetting\', Val=\'3.0\')\n    setting.text = \' \'.join([\'mu_both\'])\n    meas.append(setting)\n\n    toplvl.append(meas)\n\n    other_parameter_configs = [\n        dict(name=\'mu_both\', inits=[1.0], bounds=[[1.0, 5.0]], fixed=False),\n        dict(name=\'mu_otherConfigOnly\', inits=[1.0], bounds=[[0.0, 10.0]], fixed=False),\n    ]\n\n    result = pyhf.readxml.process_measurements(\n        toplvl, other_parameter_configs=other_parameter_configs\n    )\n    result = {\n        m[\'name\']: {k[\'name\']: k for k in m[\'config\'][\'parameters\']} for m in result\n    }\n    assert result\n\n    # make sure ParamSetting configs override NormFactor configs\n    assert result[\'NormalMeasurement\'][\'mu_both\'][\'fixed\']\n    assert result[\'NormalMeasurement\'][\'mu_both\'][\'inits\'] == [2.0]\n    assert result[\'NormalMeasurement\'][\'mu_both\'][\'bounds\'] == [[1.0, 5.0]]\n\n    # make sure ParamSetting is doing the right thing\n    assert result[\'NormalMeasurement\'][\'mu_paramSettingOnly\'][\'fixed\']\n    assert \'inits\' not in result[\'NormalMeasurement\'][\'mu_paramSettingOnly\']\n    assert \'bounds\' not in result[\'NormalMeasurement\'][\'mu_paramSettingOnly\']\n\n    # make sure our code doesn\'t accidentally override other parameter configs\n    assert not result[\'NormalMeasurement\'][\'mu_otherConfigOnly\'][\'fixed\']\n    assert result[\'NormalMeasurement\'][\'mu_otherConfigOnly\'][\'inits\'] == [1.0]\n    assert result[\'NormalMeasurement\'][\'mu_otherConfigOnly\'][\'bounds\'] == [[0.0, 10.0]]\n\n    # make sure settings from one measurement don\'t leak to other\n    assert not result[\'ParallelMeasurement\'][\'mu_both\'][\'fixed\']\n    assert result[\'ParallelMeasurement\'][\'mu_both\'][\'inits\'] == [3.0]\n    assert result[\'ParallelMeasurement\'][\'mu_both\'][\'bounds\'] == [[1.0, 5.0]]\n\n\ndef test_import_histogram():\n    data, uncert = pyhf.readxml.import_root_histogram(\n        ""validation/xmlimport_input/data"", ""example.root"", """", ""data""\n    )\n    assert data == [122.0, 112.0]\n    assert uncert == [11.045361017187261, 10.583005244258363]\n\n\ndef test_import_histogram_KeyError():\n    with pytest.raises(KeyError):\n        pyhf.readxml.import_root_histogram(\n            ""validation/xmlimport_input/data"", ""example.root"", """", ""invalid_key""\n        )\n\n\ndef test_import_measurements():\n    parsed_xml = pyhf.readxml.parse(\n        \'validation/xmlimport_input/config/example.xml\', \'validation/xmlimport_input/\'\n    )\n    measurements = parsed_xml[\'measurements\']\n    assert len(measurements) == 4\n\n    measurement_configs = measurements[0][\'config\']\n\n    assert \'parameters\' in measurement_configs\n    assert len(measurement_configs[\'parameters\']) == 3\n    parnames = [p[\'name\'] for p in measurement_configs[\'parameters\']]\n    assert sorted(parnames) == sorted([\'lumi\', \'SigXsecOverSM\', \'alpha_syst1\'])\n\n    lumi_param_config = measurement_configs[\'parameters\'][0]\n    assert \'auxdata\' in lumi_param_config\n    assert lumi_param_config[\'auxdata\'] == [1.0]\n    assert \'bounds\' in lumi_param_config\n    assert lumi_param_config[\'bounds\'] == [[0.5, 1.5]]\n    assert \'inits\' in lumi_param_config\n    assert lumi_param_config[\'inits\'] == [1.0]\n    assert \'sigmas\' in lumi_param_config\n    assert lumi_param_config[\'sigmas\'] == [0.1]\n\n\ndef test_import_prepHistFactory():\n    parsed_xml = pyhf.readxml.parse(\n        \'validation/xmlimport_input/config/example.xml\', \'validation/xmlimport_input/\'\n    )\n\n    # build the spec, strictly checks properties included\n    spec = {\n        \'channels\': parsed_xml[\'channels\'],\n        \'parameters\': parsed_xml[\'measurements\'][0][\'config\'][\'parameters\'],\n    }\n    pdf = pyhf.Model(spec, poi_name=\'SigXsecOverSM\')\n\n    data = [\n        binvalue\n        for k in pdf.spec[\'channels\']\n        for binvalue in next(\n            obs for obs in parsed_xml[\'observations\'] if obs[\'name\'] == k[\'name\']\n        )[\'data\']\n    ] + pdf.config.auxdata\n\n    channels = {channel[\'name\'] for channel in pdf.spec[\'channels\']}\n    samples = {\n        channel[\'name\']: [sample[\'name\'] for sample in channel[\'samples\']]\n        for channel in pdf.spec[\'channels\']\n    }\n\n    ###\n    # signal overallsys\n    # bkg1 overallsys (stat ignored)\n    # bkg2 stateror (2 bins)\n    # bkg2 overallsys\n\n    assert \'channel1\' in channels\n    assert \'signal\' in samples[\'channel1\']\n    assert \'background1\' in samples[\'channel1\']\n    assert \'background2\' in samples[\'channel1\']\n\n    assert pdf.spec[\'channels\'][0][\'samples\'][1][\'modifiers\'][0][\'type\'] == \'lumi\'\n    assert pdf.spec[\'channels\'][0][\'samples\'][2][\'modifiers\'][0][\'type\'] == \'lumi\'\n\n    assert pdf.spec[\'channels\'][0][\'samples\'][2][\'modifiers\'][1][\'type\'] == \'staterror\'\n    assert pdf.spec[\'channels\'][0][\'samples\'][2][\'modifiers\'][1][\'data\'] == [0, 10.0]\n\n    assert pdf.spec[\'channels\'][0][\'samples\'][1][\'modifiers\'][1][\'type\'] == \'staterror\'\n    assert all(\n        np.isclose(\n            pdf.spec[\'channels\'][0][\'samples\'][1][\'modifiers\'][1][\'data\'], [5.0, 0.0]\n        )\n    )\n\n    assert pdf.expected_actualdata(\n        pyhf.tensorlib.astensor(pdf.config.suggested_init())\n    ).tolist() == [120.0, 110.0]\n\n    assert pdf.config.auxdata_order == sorted(\n        [\'lumi\', \'syst1\', \'staterror_channel1\', \'syst2\', \'syst3\']\n    )\n\n    assert data == [122.0, 112.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n\n    pars = pdf.config.suggested_init()\n    pars[pdf.config.par_slice(\'SigXsecOverSM\')] = [2.0]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [140, 120]\n\n\ndef test_import_histosys():\n    parsed_xml = pyhf.readxml.parse(\n        \'validation/xmlimport_input2/config/example.xml\', \'validation/xmlimport_input2\'\n    )\n\n    # build the spec, strictly checks properties included\n    spec = {\n        \'channels\': parsed_xml[\'channels\'],\n        \'parameters\': parsed_xml[\'measurements\'][0][\'config\'][\'parameters\'],\n    }\n    pdf = pyhf.Model(spec, poi_name=\'SigXsecOverSM\')\n\n    channels = {channel[\'name\']: channel for channel in pdf.spec[\'channels\']}\n\n    assert channels[\'channel2\'][\'samples\'][0][\'modifiers\'][0][\'type\'] == \'lumi\'\n    assert channels[\'channel2\'][\'samples\'][0][\'modifiers\'][1][\'type\'] == \'histosys\'\n\n\ndef test_import_filecache(mocker):\n\n    mocker.patch(""pyhf.readxml.uproot.open"", wraps=uproot.open)\n\n    pyhf.readxml.clear_filecache()\n\n    parsed_xml = pyhf.readxml.parse(\n        \'validation/xmlimport_input/config/example.xml\', \'validation/xmlimport_input/\'\n    )\n\n    # call a second time (file should be cached now)\n    parsed_xml2 = pyhf.readxml.parse(\n        \'validation/xmlimport_input/config/example.xml\', \'validation/xmlimport_input/\'\n    )\n\n    # check if uproot.open was only called once with the expected root file\n    pyhf.readxml.uproot.open.assert_called_once_with(\n        str(Path(""validation/xmlimport_input"").joinpath(""./data/example.root""))\n    )\n\n    assert_equal_dictionary(parsed_xml, parsed_xml2)\n\n\ndef test_import_shapesys():\n    parsed_xml = pyhf.readxml.parse(\n        \'validation/xmlimport_input3/config/examples/example_ShapeSys.xml\',\n        \'validation/xmlimport_input3\',\n    )\n\n    # build the spec, strictly checks properties included\n    spec = {\n        \'channels\': parsed_xml[\'channels\'],\n        \'parameters\': parsed_xml[\'measurements\'][0][\'config\'][\'parameters\'],\n    }\n    pdf = pyhf.Model(spec, poi_name=\'SigXsecOverSM\')\n\n    channels = {channel[\'name\']: channel for channel in pdf.spec[\'channels\']}\n\n    assert channels[\'channel1\'][\'samples\'][1][\'modifiers\'][0][\'type\'] == \'lumi\'\n    assert channels[\'channel1\'][\'samples\'][1][\'modifiers\'][1][\'type\'] == \'shapesys\'\n    # NB: assert that relative uncertainty is converted to absolute uncertainty for shapesys\n    assert channels[\'channel1\'][\'samples\'][1][\'data\'] == pytest.approx([100.0, 1.0e-4])\n    assert channels[\'channel1\'][\'samples\'][1][\'modifiers\'][1][\'data\'] == pytest.approx(\n        [10.0, 1.5e-5]\n    )\n\n\ndef test_import_normfactor_bounds():\n    parsed_xml = pyhf.readxml.parse(\n        \'validation/xmlimport_input2/config/example.xml\', \'validation/xmlimport_input2\'\n    )\n\n    ws = pyhf.Workspace(parsed_xml)\n    assert (\'SigXsecOverSM\', \'normfactor\') in ws.modifiers\n    parameters = [\n        p\n        for p in ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\n            \'parameters\'\n        ]\n        if p[\'name\'] == \'SigXsecOverSM\'\n    ]\n    assert len(parameters) == 1\n    parameter = parameters[0]\n    assert parameter[\'bounds\'] == [[0, 10]]\n'"
tests/test_infer.py,1,"b'import pytest\nimport pyhf\nimport numpy as np\n\n\n@pytest.fixture(scope=\'module\')\ndef hypotest_args():\n    pdf = pyhf.simplemodels.hepdata_like(\n        signal_data=[12.0, 11.0], bkg_data=[50.0, 52.0], bkg_uncerts=[3.0, 7.0]\n    )\n    mu_test = 1.0\n    data = [51, 48] + pdf.config.auxdata\n    return mu_test, data, pdf\n\n\ndef check_uniform_type(in_list):\n    return all(\n        [isinstance(item, type(pyhf.tensorlib.astensor(item))) for item in in_list]\n    )\n\n\ndef test_hypotest_default(tmpdir, hypotest_args):\n    """"""\n    Check that the default return structure of pyhf.infer.hypotest is as expected\n    """"""\n    tb = pyhf.tensorlib\n\n    kwargs = {}\n    result = pyhf.infer.hypotest(*hypotest_args, **kwargs)\n    # CLs_obs\n    assert len(list(result)) == 1\n    assert isinstance(result, type(tb.astensor(result)))\n\n\ndef test_hypotest_return_tail_probs(tmpdir, hypotest_args):\n    """"""\n    Check that the return structure of pyhf.infer.hypotest with the\n    return_tail_probs keyword arg is as expected\n    """"""\n    tb = pyhf.tensorlib\n\n    kwargs = {\'return_tail_probs\': True}\n    result = pyhf.infer.hypotest(*hypotest_args, **kwargs)\n    # CLs_obs, [CL_sb, CL_b]\n    assert len(list(result)) == 2\n    assert isinstance(result[0], type(tb.astensor(result[0])))\n    assert len(result[1]) == 2\n    assert check_uniform_type(result[1])\n\n\ndef test_hypotest_return_expected(tmpdir, hypotest_args):\n    """"""\n    Check that the return structure of pyhf.infer.hypotest with the\n    additon of the return_expected keyword arg is as expected\n    """"""\n    tb = pyhf.tensorlib\n\n    kwargs = {\'return_tail_probs\': True, \'return_expected\': True}\n    result = pyhf.infer.hypotest(*hypotest_args, **kwargs)\n    # CLs_obs, [CLsb, CLb], CLs_exp\n    assert len(list(result)) == 3\n    assert isinstance(result[0], type(tb.astensor(result[0])))\n    assert len(result[1]) == 2\n    assert check_uniform_type(result[1])\n    assert isinstance(result[2], type(tb.astensor(result[2])))\n\n\ndef test_hypotest_return_expected_set(tmpdir, hypotest_args):\n    """"""\n    Check that the return structure of pyhf.infer.hypotest with the\n    additon of the return_expected_set keyword arg is as expected\n    """"""\n    tb = pyhf.tensorlib\n\n    kwargs = {\n        \'return_tail_probs\': True,\n        \'return_expected\': True,\n        \'return_expected_set\': True,\n    }\n    result = pyhf.infer.hypotest(*hypotest_args, **kwargs)\n    # CLs_obs, [CLsb, CLb], CLs_exp, CLs_exp @[-2, -1, 0, +1, +2]sigma\n    assert len(list(result)) == 4\n    assert isinstance(result[0], type(tb.astensor(result[0])))\n    assert len(result[1]) == 2\n    assert check_uniform_type(result[1])\n    assert isinstance(result[2], type(tb.astensor(result[2])))\n    assert len(result[3]) == 5\n    assert check_uniform_type(result[3])\n\n\ndef test_inferapi_pyhf_independence():\n    \'\'\'\n    pyhf.infer should eventually be factored out so it should be\n    infependent from pyhf internals. This is testing that\n    a much simpler model still can run through pyhf.infer.hypotest\n    \'\'\'\n    from pyhf import get_backend\n\n    class _NonPyhfConfig(object):\n        def __init__(self):\n            self.poi_index = 0\n            self.npars = 2\n\n        def suggested_init(self):\n            return [1.0, 1.0]\n\n        def suggested_bounds(self):\n            return [[0.0, 10.0], [0.0, 10.0]]\n\n    class NonPyhfModel(object):\n        def __init__(self, spec):\n            self.sig, self.nominal, self.uncert = spec\n            self.factor = (self.nominal / self.uncert) ** 2\n            self.aux = 1.0 * self.factor\n            self.config = _NonPyhfConfig()\n\n        def _make_main_pdf(self, pars):\n            mu, gamma = pars\n            expected_main = gamma * self.nominal + mu * self.sig\n            return pyhf.probability.Poisson(expected_main)\n\n        def _make_constraint_pdf(self, pars):\n            mu, gamma = pars\n            return pyhf.probability.Poisson(gamma * self.factor)\n\n        def expected_data(self, pars, include_auxdata=True):\n            tensorlib, _ = get_backend()\n            expected_main = tensorlib.astensor(\n                [self._make_main_pdf(pars).expected_data()]\n            )\n            aux_data = tensorlib.astensor(\n                [self._make_constraint_pdf(pars).expected_data()]\n            )\n            if not include_auxdata:\n                return expected_main\n            return tensorlib.concatenate([expected_main, aux_data])\n\n        def logpdf(self, pars, data):\n            tensorlib, _ = get_backend()\n            maindata, auxdata = data\n            main = self._make_main_pdf(pars).log_prob(maindata)\n            constraint = self._make_constraint_pdf(pars).log_prob(auxdata)\n            return tensorlib.astensor([main + constraint])\n\n    model = NonPyhfModel([5, 50, 7])\n    cls = pyhf.infer.hypotest(\n        1.0, model.expected_data(model.config.suggested_init()), model\n    )\n\n    assert np.isclose(cls[0], 0.7267836451638846)\n\n\n@pytest.mark.parametrize(""qtilde"", [True, False])\ndef test_calculator_distributions_without_teststatistic(qtilde):\n    calc = pyhf.infer.AsymptoticCalculator(\n        [0.0], {}, [1.0], [(0.0, 10.0)], qtilde=qtilde\n    )\n    with pytest.raises(RuntimeError):\n        calc.distributions(1.0)\n'"
tests/test_init.py,0,"b'import pytest\nimport sys\nimport pyhf\n\n\n@pytest.mark.parametrize(\n    ""param"",\n    [\n        [""numpy"", ""numpy_backend"", ""numpy_backend"", pytest.raises(ImportError)],\n        [\n            ""torch"",\n            ""pytorch_backend"",\n            ""pytorch_backend"",\n            pytest.raises(pyhf.exceptions.ImportBackendError),\n        ],\n        [\n            ""tensorflow"",\n            ""tensorflow_backend"",\n            ""tensorflow_backend"",\n            pytest.raises(pyhf.exceptions.ImportBackendError),\n        ],\n    ],\n    ids=[""numpy"", ""pytorch"", ""tensorflow""],\n)\ndef test_missing_backends(isolate_modules, param):\n    backend_name, module_name, import_name, expectation = param\n\n    # hide\n    CACHE_BACKEND, sys.modules[backend_name] = sys.modules[backend_name], None\n    sys.modules.setdefault(\'pyhf.tensor.{}\'.format(import_name), None)\n    CACHE_MODULE, sys.modules[\'pyhf.tensor.{}\'.format(module_name)] = (\n        sys.modules[\'pyhf.tensor.{}\'.format(module_name)],\n        None,\n    )\n    try:\n        delattr(pyhf.tensor, module_name)\n    except:\n        pass\n\n    with expectation:\n        getattr(pyhf.tensor, module_name)\n\n    # put back\n    CACHE_BACKEND, sys.modules[backend_name] = None, CACHE_BACKEND\n    CACHE_MODULE, sys.modules[\'pyhf.tensor.{}\'.format(module_name)] = (\n        None,\n        CACHE_MODULE,\n    )\n\n\n@pytest.mark.parametrize(\n    ""param"",\n    [\n        [""scipy"", ""scipy_optimizer"", ""opt_scipy"", pytest.raises(ImportError)],\n        [\n            ""torch"",\n            ""pytorch_optimizer"",\n            ""opt_pytorch"",\n            pytest.raises(pyhf.exceptions.ImportBackendError),\n        ],\n        [\n            ""tensorflow"",\n            ""tflow_optimizer"",\n            ""opt_tflow"",\n            pytest.raises(pyhf.exceptions.ImportBackendError),\n        ],\n        [\n            ""iminuit"",\n            ""minuit_optimizer"",\n            ""opt_minuit"",\n            pytest.raises(pyhf.exceptions.ImportBackendError),\n        ],\n    ],\n    ids=[""scipy"", ""pytorch"", ""tensorflow"", ""minuit""],\n)\ndef test_missing_optimizer(isolate_modules, param):\n    backend_name, module_name, import_name, expectation = param\n\n    # hide\n    CACHE_BACKEND, sys.modules[backend_name] = sys.modules[backend_name], None\n    sys.modules.setdefault(\'pyhf.optimize.{}\'.format(import_name), None)\n    CACHE_MODULE, sys.modules[\'pyhf.optimize.{}\'.format(import_name)] = (\n        sys.modules[\'pyhf.optimize.{}\'.format(import_name)],\n        None,\n    )\n    try:\n        delattr(pyhf.optimize, module_name)\n    except:\n        pass\n\n    with expectation:\n        getattr(pyhf.optimize, module_name)\n\n    # put back\n    CACHE_BACKEND, sys.modules[backend_name] = None, CACHE_BACKEND\n    CACHE_MODULE, sys.modules[\'pyhf.optimize.{}\'.format(import_name)] = (\n        None,\n        CACHE_MODULE,\n    )\n'"
tests/test_interpolate.py,15,"b'import pyhf\nimport numpy as np\nimport pytest\nimport mock\n\n\n@pytest.fixture\ndef random_histosets_alphasets_pair():\n    def generate_shapes(histogramssets, alphasets):\n        h_shape = [len(histogramssets), 0, 0, 0]\n        a_shape = (len(alphasets), max(map(len, alphasets)))\n        for hs in histogramssets:\n            h_shape[1] = max(h_shape[1], len(hs))\n            for h in hs:\n                h_shape[2] = max(h_shape[2], len(h))\n                for sh in h:\n                    h_shape[3] = max(h_shape[3], len(sh))\n        return tuple(h_shape), a_shape\n\n    def filled_shapes(histogramssets, alphasets):\n        # pad our shapes with NaNs\n        histos, alphas = generate_shapes(histogramssets, alphasets)\n        histos, alphas = np.ones(histos) * np.nan, np.ones(alphas) * np.nan\n        for i, syst in enumerate(histogramssets):\n            for j, sample in enumerate(syst):\n                for k, variation in enumerate(sample):\n                    histos[i, j, k, : len(variation)] = variation\n        for i, alphaset in enumerate(alphasets):\n            alphas[i, : len(alphaset)] = alphaset\n        return histos, alphas\n\n    nsysts = 150\n    nhistos_per_syst_upto = 300\n    nalphas = 1\n    nbins_upto = 1\n\n    nsyst_histos = np.random.randint(1, 1 + nhistos_per_syst_upto, size=nsysts)\n    nhistograms = [np.random.randint(1, nbins_upto + 1, size=n) for n in nsyst_histos]\n    random_alphas = [np.random.uniform(-1, 1, size=nalphas) for n in nsyst_histos]\n\n    random_histogramssets = [\n        [  # all histos affected by systematic $nh\n            [  # sample $i, systematic $nh\n                np.random.uniform(10 * i + j, 10 * i + j + 1, size=nbin).tolist()\n                for j in range(3)\n            ]\n            for i, nbin in enumerate(nh)\n        ]\n        for nh in nhistograms\n    ]\n    h, a = filled_shapes(random_histogramssets, random_alphas)\n    return h, a\n\n\ndef test_interpolator_structure(interpcode, random_histosets_alphasets_pair):\n    histogramssets, alphasets = random_histosets_alphasets_pair\n\n    interpolator = pyhf.interpolators.get(interpcode)(\n        histogramssets.tolist(), subscribe=False\n    )\n    assert callable(interpolator)\n    assert hasattr(interpolator, \'alphasets_shape\')\n    assert hasattr(interpolator, \'_precompute\') and callable(interpolator._precompute)\n    assert hasattr(interpolator, \'_precompute_alphasets\') and callable(\n        interpolator._precompute_alphasets\n    )\n\n\ndef test_interpolator_subscription(interpcode, random_histosets_alphasets_pair):\n    histogramssets, alphasets = random_histosets_alphasets_pair\n    ename = \'tensorlib_changed\'\n\n    # inject into our interpolator class\n    interpolator_cls = pyhf.interpolators.get(interpcode)\n    with mock.patch(\'{0:s}._precompute\'.format(interpolator_cls.__module__)) as m:\n        interpolator_cls(histogramssets.tolist(), subscribe=False)\n        assert m.call_count == 1\n        assert m not in pyhf.events.__events.get(ename, [])\n        pyhf.events.trigger(ename)()\n        assert m.call_count == 1\n\n    with mock.patch(\'{0:s}._precompute\'.format(interpolator_cls.__module__)) as m:\n        interpolator_cls(histogramssets.tolist(), subscribe=True)\n        assert m.call_count == 1\n        assert m in pyhf.events.__events.get(ename, [])\n        pyhf.events.trigger(ename)()\n        assert m.call_count == 2\n\n\ndef test_interpolator_alphaset_change(\n    backend, interpcode, random_histosets_alphasets_pair\n):\n    histogramssets, alphasets = random_histosets_alphasets_pair\n    interpolator = pyhf.interpolators.get(interpcode)(\n        histogramssets.tolist(), subscribe=False\n    )\n    # set to None to force recomputation\n    interpolator.alphasets_shape = None\n    # expect recomputation to not fail\n    interpolator._precompute_alphasets(alphasets.shape)\n    # make sure it sets the right shape\n    assert interpolator.alphasets_shape == alphasets.shape\n\n\ndef test_interpolator(backend, interpcode, random_histosets_alphasets_pair):\n    histogramssets, alphasets = random_histosets_alphasets_pair\n\n    interpolator = pyhf.interpolators.get(interpcode)(\n        histogramssets.tolist(), subscribe=False\n    )\n    assert interpolator.alphasets_shape == (histogramssets.shape[0], 1)\n    interpolator.alphasets_shape = None\n    interpolator(pyhf.tensorlib.astensor(alphasets.tolist()))\n    assert interpolator.alphasets_shape == alphasets.shape\n\n\ndef test_validate_implementation(backend, interpcode, random_histosets_alphasets_pair):\n    histogramssets, alphasets = random_histosets_alphasets_pair\n\n    # single-float precision backends, calculate using single-floats\n    if pyhf.tensorlib.name in [\'tensorflow\', \'pytorch\']:\n        abs_tolerance = 1e-6\n        histogramssets = np.asarray(histogramssets, dtype=np.float32)\n        alphasets = np.asarray(alphasets, dtype=np.float32)\n    else:\n        abs_tolerance = 1e-12\n\n    histogramssets = histogramssets.tolist()\n    alphasets = pyhf.tensorlib.astensor(alphasets.tolist())\n\n    slow_interpolator = pyhf.interpolators.get(interpcode, do_tensorized_calc=False)(\n        histogramssets, subscribe=False\n    )\n    fast_interpolator = pyhf.interpolators.get(interpcode, do_tensorized_calc=True)(\n        histogramssets, subscribe=False\n    )\n    slow_result = np.asarray(pyhf.tensorlib.tolist(slow_interpolator(alphasets)))\n    fast_result = np.asarray(pyhf.tensorlib.tolist(fast_interpolator(alphasets)))\n\n    assert slow_result.shape == fast_result.shape\n\n    assert (\n        pytest.approx(\n            slow_result[~np.isnan(slow_result)].ravel().tolist(), abs=abs_tolerance\n        )\n        == fast_result[~np.isnan(fast_result)].ravel().tolist()\n    )\n\n\n@pytest.mark.parametrize(""do_tensorized_calc"", [False, True], ids=[\'slow\', \'fast\'])\ndef test_code0_validation(backend, do_tensorized_calc):\n    histogramssets = [[[[0.5], [1.0], [2.0]]]]\n    alphasets = pyhf.tensorlib.astensor([[-2, -1, 0, 1, 2]])\n    expected = pyhf.tensorlib.astensor([[[[0], [0.5], [1.0], [2.0], [3.0]]]])\n\n    interpolator = pyhf.interpolators.get(0, do_tensorized_calc=do_tensorized_calc)(\n        histogramssets, subscribe=False\n    )\n    result_deltas = pyhf.tensorlib.astensor(interpolator(alphasets))\n\n    # calculate the actual change\n    histogramssets = pyhf.tensorlib.astensor(histogramssets)\n    allsets_allhistos_noms_repeated = pyhf.tensorlib.einsum(\n        \'sa,shb->shab\',\n        pyhf.tensorlib.ones(pyhf.tensorlib.shape(alphasets)),\n        histogramssets[:, :, 1],\n    )\n    results = allsets_allhistos_noms_repeated + result_deltas\n\n    assert (\n        pytest.approx(np.asarray(pyhf.tensorlib.tolist(results)).ravel().tolist())\n        == np.asarray(pyhf.tensorlib.tolist(expected)).ravel().tolist()\n    )\n\n\n@pytest.mark.parametrize(""do_tensorized_calc"", [False, True], ids=[\'slow\', \'fast\'])\ndef test_code1_validation(backend, do_tensorized_calc):\n    histogramssets = [[[[0.9], [1.0], [1.1]]]]\n    alphasets = pyhf.tensorlib.astensor([[-2, -1, 0, 1, 2]])\n    expected = pyhf.tensorlib.astensor(\n        [[[[0.9 ** 2], [0.9], [1.0], [1.1], [1.1 ** 2]]]]\n    )\n\n    interpolator = pyhf.interpolators.get(1, do_tensorized_calc=do_tensorized_calc)(\n        histogramssets, subscribe=False\n    )\n    result_deltas = interpolator(alphasets)\n\n    # calculate the actual change\n    histogramssets = pyhf.tensorlib.astensor(histogramssets)\n    allsets_allhistos_noms_repeated = pyhf.tensorlib.einsum(\n        \'sa,shb->shab\',\n        pyhf.tensorlib.ones(pyhf.tensorlib.shape(alphasets)),\n        histogramssets[:, :, 1],\n    )\n    results = allsets_allhistos_noms_repeated * result_deltas\n\n    assert (\n        pytest.approx(np.asarray(pyhf.tensorlib.tolist(results)).ravel().tolist())\n        == np.asarray(pyhf.tensorlib.tolist(expected)).ravel().tolist()\n    )\n\n\ndef test_invalid_interpcode():\n    with pytest.raises(pyhf.exceptions.InvalidInterpCode):\n        pyhf.interpolators.get(\'fake\')\n\n    with pytest.raises(pyhf.exceptions.InvalidInterpCode):\n        pyhf.interpolators.get(1.2)\n\n    with pytest.raises(pyhf.exceptions.InvalidInterpCode):\n        pyhf.interpolators.get(-1)\n'"
tests/test_mixins.py,0,"b""import pyhf\nimport pyhf.readxml\nimport pytest\n\n\n@pytest.fixture(\n    scope='session',\n    params=[\n        ('validation/xmlimport_input/config/example.xml', 'validation/xmlimport_input/')\n    ],\n    ids=['example-one'],\n)\ndef spec(request):\n    return pyhf.readxml.parse(*request.param)\n\n\ndef test_channel_summary_mixin(spec):\n    assert 'channels' in spec\n    mixin = pyhf.mixins._ChannelSummaryMixin(channels=spec['channels'])\n    assert mixin.channel_nbins == {'channel1': 2}\n    assert mixin.channels == ['channel1']\n    assert mixin.modifiers == [\n        ('SigXsecOverSM', 'normfactor'),\n        ('lumi', 'lumi'),\n        ('staterror_channel1', 'staterror'),\n        ('syst1', 'normsys'),\n        ('syst2', 'normsys'),\n        ('syst3', 'normsys'),\n    ]\n    assert mixin.parameters == [\n        'SigXsecOverSM',\n        'lumi',\n        'staterror_channel1',\n        'syst1',\n        'syst2',\n        'syst3',\n    ]\n    assert mixin.samples == ['background1', 'background2', 'signal']\n\n\ndef test_channel_summary_mixin_empty():\n    mixin = pyhf.mixins._ChannelSummaryMixin(channels=[])\n    assert mixin.channel_nbins == {}\n    assert mixin.channels == []\n    assert mixin.modifiers == []\n    assert mixin.parameters == []\n    assert mixin.samples == []\n"""
tests/test_modifiers.py,0,"b'import pytest\nimport inspect\n\nimport pyhf\n\nmodifiers_to_test = [\n    ""histosys"",\n    ""normfactor"",\n    ""normsys"",\n    ""shapefactor"",\n    ""shapesys"",\n    ""staterror"",\n]\nmodifier_pdf_types = [""normal"", None, ""normal"", None, ""poisson"", ""normal""]\n\n# we make sure we can import all of our pre-defined modifiers correctly\n@pytest.mark.parametrize(\n    ""test_modifierPair"", zip(modifiers_to_test, modifier_pdf_types)\n)\ndef test_import_default_modifiers(test_modifierPair):\n    test_modifier, test_mod_type = test_modifierPair\n    modifier = pyhf.modifiers.registry.get(test_modifier, None)\n    assert test_modifier in pyhf.modifiers.registry\n    assert modifier is not None\n    assert callable(modifier)\n    assert hasattr(modifier, \'is_constrained\')\n    assert hasattr(modifier, \'pdf_type\')\n    assert hasattr(modifier, \'op_code\')\n    assert modifier.op_code in [\'addition\', \'multiplication\']\n\n\n# we make sure modifiers have right structure\ndef test_modifiers_structure():\n    from pyhf.modifiers import modifier\n\n    @modifier(name=\'myUnconstrainedModifier\')\n    class myCustomModifier(object):\n        @classmethod\n        def required_parset(cls, sample_data, modifier_data):\n            pass\n\n    assert inspect.isclass(myCustomModifier)\n    assert \'myUnconstrainedModifier\' in pyhf.modifiers.registry\n    assert pyhf.modifiers.registry[\'myUnconstrainedModifier\'] == myCustomModifier\n    assert pyhf.modifiers.registry[\'myUnconstrainedModifier\'].is_constrained is False\n    del pyhf.modifiers.registry[\'myUnconstrainedModifier\']\n\n    @modifier(name=\'myConstrainedModifier\', constrained=True)\n    class myCustomModifier(object):\n        @classmethod\n        def required_parset(cls, sample_data, modifier_data):\n            pass\n\n    assert inspect.isclass(myCustomModifier)\n    assert \'myConstrainedModifier\' in pyhf.modifiers.registry\n    assert pyhf.modifiers.registry[\'myConstrainedModifier\'] == myCustomModifier\n    assert pyhf.modifiers.registry[\'myConstrainedModifier\'].is_constrained is True\n    del pyhf.modifiers.registry[\'myConstrainedModifier\']\n\n\n# we make sure decorate can use auto-naming\ndef test_modifier_name_auto():\n    from pyhf.modifiers import modifier\n\n    @modifier\n    class myCustomModifier(object):\n        @classmethod\n        def required_parset(cls, sample_data, modifier_data):\n            pass\n\n    assert inspect.isclass(myCustomModifier)\n    assert \'myCustomModifier\' in pyhf.modifiers.registry\n    assert pyhf.modifiers.registry[\'myCustomModifier\'] == myCustomModifier\n    del pyhf.modifiers.registry[\'myCustomModifier\']\n\n\n# we make sure decorate can use auto-naming with keyword arguments\ndef test_modifier_name_auto_withkwargs():\n    from pyhf.modifiers import modifier\n\n    @modifier(name=None, constrained=False)\n    class myCustomModifier(object):\n        @classmethod\n        def required_parset(cls, sample_data, modifier_data):\n            pass\n\n    assert inspect.isclass(myCustomModifier)\n    assert \'myCustomModifier\' in pyhf.modifiers.registry\n    assert pyhf.modifiers.registry[\'myCustomModifier\'] == myCustomModifier\n    del pyhf.modifiers.registry[\'myCustomModifier\']\n\n\n# we make sure decorate allows for custom naming\ndef test_modifier_name_custom():\n    from pyhf.modifiers import modifier\n\n    @modifier(name=\'myCustomName\')\n    class myCustomModifier(object):\n        @classmethod\n        def required_parset(cls, sample_data, modifier_data):\n            pass\n\n    assert inspect.isclass(myCustomModifier)\n    assert \'myCustomModifier\' not in pyhf.modifiers.registry\n    assert \'myCustomName\' in pyhf.modifiers.registry\n    assert pyhf.modifiers.registry[\'myCustomName\'] == myCustomModifier\n    del pyhf.modifiers.registry[\'myCustomName\']\n\n\n# we make sure decorate raises errors if passed more than one argument, or not a string\ndef test_decorate_with_wrong_values():\n    from pyhf.modifiers import modifier\n\n    with pytest.raises(ValueError):\n\n        @modifier(\'too\', \'many\', \'args\')\n        class myCustomModifier(object):\n            pass\n\n    with pytest.raises(TypeError):\n\n        @modifier(name=1.5)\n        class myCustomModifierTypeError(object):\n            pass\n\n    with pytest.raises(ValueError):\n\n        @modifier(unused=\'arg\')\n        class myCustomModifierValueError(object):\n            pass\n\n\n# we catch name clashes when adding duplicate names for modifiers\ndef test_registry_name_clash():\n    from pyhf.modifiers import modifier\n\n    with pytest.raises(KeyError):\n\n        @modifier(name=\'histosys\')\n        class myCustomModifierKeyError(object):\n            pass\n\n    with pytest.raises(KeyError):\n\n        class myCustomModifier(object):\n            @classmethod\n            def required_parset(cls, sample_data, modifier_data):\n                pass\n\n        pyhf.modifiers.add_to_registry(myCustomModifier, \'histosys\')\n'"
tests/test_notebooks.py,0,"b""import sys\nimport os\nimport papermill as pm\nimport scrapbook as sb\nimport pytest\n\n\n@pytest.fixture()\ndef common_kwargs(tmpdir):\n    outputnb = tmpdir.join('output.ipynb')\n    return {\n        'output_path': str(outputnb),\n        'kernel_name': 'python{}'.format(sys.version_info.major),\n    }\n\n\ndef test_hello_world(common_kwargs):\n    pm.execute_notebook('docs/examples/notebooks/hello-world.ipynb', **common_kwargs)\n\n\ndef test_xml_importexport(common_kwargs):\n    pm.execute_notebook(\n        'docs/examples/notebooks/XML_ImportExport.ipynb', **common_kwargs\n    )\n\n\ndef test_statisticalanalysis(common_kwargs):\n    # The Binder example uses specific relative paths\n    cwd = os.getcwd()\n    os.chdir(os.path.join(cwd, 'docs/examples/notebooks/binderexample'))\n    pm.execute_notebook('StatisticalAnalysis.ipynb', **common_kwargs)\n    os.chdir(cwd)\n\n\ndef test_shapefactor(common_kwargs):\n    pm.execute_notebook('docs/examples/notebooks/ShapeFactor.ipynb', **common_kwargs)\n\n\ndef test_multichannel_coupled_histos(common_kwargs):\n    pm.execute_notebook(\n        'docs/examples/notebooks/multichannel-coupled-histo.ipynb',\n        parameters={'validation_datadir': 'validation/data'},\n        **common_kwargs,\n    )\n\n\ndef test_multibinpois(common_kwargs):\n    pm.execute_notebook(\n        'docs/examples/notebooks/multiBinPois.ipynb',\n        parameters={'validation_datadir': 'validation/data'},\n        **common_kwargs,\n    )\n    nb = sb.read_notebook(common_kwargs['output_path'])\n    assert nb.scraps['number_2d_successpoints'].data > 200\n\n\ndef test_pullplot(common_kwargs):\n    pm.execute_notebook('docs/examples/notebooks/pullplot.ipynb', **common_kwargs)\n\n\ndef test_impactplot(common_kwargs):\n    pm.execute_notebook('docs/examples/notebooks/ImpactPlot.ipynb', **common_kwargs)\n\n\ndef test_learn_interpolationcodes(common_kwargs):\n    pm.execute_notebook(\n        'docs/examples/notebooks/learn/InterpolationCodes.ipynb', **common_kwargs\n    )\n\n\ndef test_learn_tensorizinginterpolations(common_kwargs):\n    pm.execute_notebook(\n        'docs/examples/notebooks/learn/TensorizingInterpolations.ipynb', **common_kwargs\n    )\n"""
tests/test_optim.py,0,"b'import pyhf\nimport pytest\nfrom scipy.optimize import minimize\n\n\ndef test_get_invalid_optimizer():\n    with pytest.raises(pyhf.exceptions.InvalidOptimizer):\n        assert pyhf.optimize.scipy\n\n\n# from https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#nelder-mead-simplex-algorithm-method-nelder-mead\n@pytest.mark.skip_pytorch\n@pytest.mark.skip_pytorch64\n@pytest.mark.skip_tensorflow\n@pytest.mark.skip_numpy_minuit\ndef test_scipy_minimize(backend, capsys):\n    tensorlib, _ = backend\n\n    def rosen(x):\n        """"""The Rosenbrock function""""""\n        return sum(100.0 * (x[1:] - x[:-1] ** 2.0) ** 2.0 + (1 - x[:-1]) ** 2.0)\n\n    x0 = tensorlib.astensor([1.3, 0.7, 0.8, 1.9, 1.2])\n    res = minimize(rosen, x0, method=\'SLSQP\', options=dict(disp=True))\n    captured = capsys.readouterr()\n    assert ""Optimization terminated successfully"" in captured.out\n    assert pytest.approx([1.0, 1.0, 1.0, 1.0, 1.0], rel=5e-5) == tensorlib.tolist(res.x)\n\n\n@pytest.fixture(scope=\'module\')\ndef source():\n    source = {\n        \'binning\': [2, -0.5, 1.5],\n        \'bindata\': {\n            \'data\': [120.0, 180.0],\n            \'bkg\': [100.0, 150.0],\n            \'bkgsys_up\': [102, 190],\n            \'bkgsys_dn\': [98, 100],\n            \'sig\': [30.0, 95.0],\n        },\n    }\n    return source\n\n\n@pytest.fixture(scope=\'module\')\ndef spec(source):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'bkg_norm\',\n                                \'type\': \'histosys\',\n                                \'data\': {\n                                    \'lo_data\': source[\'bindata\'][\'bkgsys_dn\'],\n                                    \'hi_data\': source[\'bindata\'][\'bkgsys_up\'],\n                                },\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    return spec\n\n\n@pytest.mark.parametrize(\'mu\', [1.0], ids=[\'mu=1\'])\ndef test_optim(backend, source, spec, mu):\n    pdf = pyhf.Model(spec)\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    init_pars = pdf.config.suggested_init()\n    par_bounds = pdf.config.suggested_bounds()\n\n    optim = pyhf.optimizer\n\n    result = optim.minimize(pyhf.infer.mle.twice_nll, data, pdf, init_pars, par_bounds)\n    assert pyhf.tensorlib.tolist(result)\n\n    result = optim.minimize(\n        pyhf.infer.mle.twice_nll,\n        data,\n        pdf,\n        init_pars,\n        par_bounds,\n        [(pdf.config.poi_index, mu)],\n    )\n    assert pyhf.tensorlib.tolist(result)\n\n\n@pytest.mark.parametrize(\'mu\', [1.0], ids=[\'mu=1\'])\ndef test_optim_with_value(backend, source, spec, mu):\n    pdf = pyhf.Model(spec)\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    init_pars = pdf.config.suggested_init()\n    par_bounds = pdf.config.suggested_bounds()\n\n    optim = pyhf.optimizer\n\n    result = optim.minimize(pyhf.infer.mle.twice_nll, data, pdf, init_pars, par_bounds)\n    assert pyhf.tensorlib.tolist(result)\n\n    result, fitted_val = optim.minimize(\n        pyhf.infer.mle.twice_nll,\n        data,\n        pdf,\n        init_pars,\n        par_bounds,\n        [(pdf.config.poi_index, mu)],\n        return_fitted_val=True,\n    )\n    assert pyhf.tensorlib.tolist(result)\n\n\n@pytest.mark.parametrize(\'mu\', [1.0], ids=[\'mu=1\'])\n@pytest.mark.only_numpy_minuit\ndef test_optim_uncerts(backend, source, spec, mu):\n    pdf = pyhf.Model(spec)\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    init_pars = pdf.config.suggested_init()\n    par_bounds = pdf.config.suggested_bounds()\n\n    optim = pyhf.optimizer\n\n    result = optim.minimize(pyhf.infer.mle.twice_nll, data, pdf, init_pars, par_bounds)\n    assert pyhf.tensorlib.tolist(result)\n\n    result = optim.minimize(\n        pyhf.infer.mle.twice_nll,\n        data,\n        pdf,\n        init_pars,\n        par_bounds,\n        [(pdf.config.poi_index, mu)],\n        return_uncertainties=True,\n    )\n    assert result.shape[1] == 2\n    assert pyhf.tensorlib.tolist(result)\n'"
tests/test_paramsets.py,0,"b'from pyhf.parameters import paramsets\nimport pytest\n\n\ndef test_paramset_unconstrained():\n    pset = paramsets.unconstrained(\n        n_parameters=5,\n        inits=[0, 1, 2, 3, 4],\n        bounds=[(-1, 1), (-2, 2), (-3, 3), (-4, 4)],\n    )\n    assert pset.suggested_init == [0, 1, 2, 3, 4]\n    assert pset.suggested_bounds == [(-1, 1), (-2, 2), (-3, 3), (-4, 4)]\n    assert not pset.constrained\n\n\ndef test_paramset_constrained_custom_sigmas():\n    pset = paramsets.constrained_by_normal(\n        n_parameters=5,\n        inits=[0, 1, 2, 3, 4],\n        bounds=[(-1, 1), (-2, 2), (-3, 3), (-4, 4)],\n        auxdata=[0, 0, 0, 0, 0],\n        sigmas=[1, 2, 3, 4, 5],\n    )\n    assert pset.suggested_init == [0, 1, 2, 3, 4]\n    assert pset.suggested_bounds == [(-1, 1), (-2, 2), (-3, 3), (-4, 4)]\n    assert pset.constrained\n    assert pset.width() == [1, 2, 3, 4, 5]\n\n\ndef test_paramset_constrained_default_sigmas():\n    pset = paramsets.constrained_by_normal(\n        n_parameters=5,\n        inits=[0, 1, 2, 3, 4],\n        bounds=[(-1, 1), (-2, 2), (-3, 3), (-4, 4)],\n        auxdata=[0, 0, 0, 0, 0],\n    )\n    assert pset.suggested_init == [0, 1, 2, 3, 4]\n    assert pset.suggested_bounds == [(-1, 1), (-2, 2), (-3, 3), (-4, 4)]\n    assert pset.constrained\n    assert pset.width() == [1, 1, 1, 1, 1]\n\n\ndef test_paramset_constrained_custom_factors():\n    pset = paramsets.constrained_by_poisson(\n        n_parameters=5,\n        inits=[0, 1, 2, 3, 4],\n        bounds=[(-1, 1), (-2, 2), (-3, 3), (-4, 4)],\n        auxdata=[0, 0, 0, 0, 0],\n        factors=[100, 400, 900, 1600, 2500],\n    )\n    assert pset.suggested_init == [0, 1, 2, 3, 4]\n    assert pset.suggested_bounds == [(-1, 1), (-2, 2), (-3, 3), (-4, 4)]\n    assert pset.constrained\n    assert pset.width() == [1 / 10.0, 1 / 20.0, 1 / 30.0, 1 / 40.0, 1 / 50.0]\n\n\ndef test_paramset_constrained_missiing_factors():\n    pset = paramsets.constrained_by_poisson(\n        n_parameters=5,\n        inits=[0, 1, 2, 3, 4],\n        bounds=[(-1, 1), (-2, 2), (-3, 3), (-4, 4)],\n        auxdata=[0, 0, 0, 0, 0],\n        factors=None,\n    )\n    with pytest.raises(RuntimeError):\n        pset.width()\n'"
tests/test_paramviewer.py,0,"b'import pyhf\nfrom pyhf.parameters import ParamViewer\n\n\ndef test_paramviewer_simple_nonbatched(backend):\n    pars = pyhf.tensorlib.astensor([1, 2, 3, 4, 5, 6, 7])\n\n    parshape = pyhf.tensorlib.shape(pars)\n\n    view = ParamViewer(\n        parshape,\n        {\'hello\': {\'slice\': slice(0, 2)}, \'world\': {\'slice\': slice(5, 7)}},\n        [\'world\', \'hello\'],\n    )\n    par_slice = view.get(pars)\n    assert pyhf.tensorlib.tolist(par_slice[slice(2, 4)]) == [1, 2]\n\n    assert pyhf.tensorlib.tolist(par_slice[slice(0, 2)]) == [6, 7]\n\n    assert pyhf.tensorlib.tolist(par_slice) == [6, 7, 1, 2]\n\n\ndef test_paramviewer_order(sbottom_likelihoods_download, get_json_from_tarfile):\n    lhood = get_json_from_tarfile(sbottom_likelihoods_download, ""RegionA/BkgOnly.json"")\n    patch = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionA/patch.sbottom_1300_205_60.json""\n    )\n    workspace = pyhf.workspace.Workspace(lhood)\n    model = workspace.model(patches=[patch])\n\n    pv = ParamViewer((model.config.npars,), model.config.par_map, [])\n    assert list(pv.allpar_viewer.names) == model.config.par_order\n\n\ndef test_paramviewer_simple_batched(backend):\n    pars = pyhf.tensorlib.astensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n\n    parshape = pyhf.tensorlib.shape(pars)\n\n    view = ParamViewer(\n        parshape,\n        {\'hello\': {\'slice\': slice(0, 2)}, \'world\': {\'slice\': slice(3, 4)}},\n        [\'world\', \'hello\'],\n    )\n    par_slice = view.get(pars)\n\n    assert isinstance(view.index_selection, list)\n    assert all(\n        [len(x) == 3 for x in view.index_selection]\n    )  # first dimension is batch dim\n\n    assert pyhf.tensorlib.shape(par_slice) == (3, 3)\n    assert pyhf.tensorlib.tolist(par_slice[slice(1, 3)]) == [[1, 5, 9], [2, 6, 10]]\n    assert pyhf.tensorlib.tolist(par_slice[slice(0, 1)]) == [[4, 8, 12]]\n\n    assert pyhf.tensorlib.tolist(par_slice) == [[4, 8, 12], [1, 5, 9], [2, 6, 10]]\n'"
tests/test_patchset.py,0,"b'import pyhf\nimport pytest\nimport pyhf.exceptions\nimport pyhf.patchset\nimport json\nimport mock\n\n\n@pytest.fixture(\n    scope=\'function\',\n    params=[\'patchset_good.json\', \'patchset_good_2_patches.json\'],\n    ids=[\'patchset_good.json\', \'patchset_good_2_patches.json\'],\n)\ndef patchset(datadir, request):\n    spec = json.load(open(datadir.join(request.param)))\n    return pyhf.PatchSet(spec)\n\n\n@pytest.fixture(scope=\'function\')\ndef patch():\n    return pyhf.patchset.Patch(\n        {\'metadata\': {\'name\': \'test\', \'values\': [1.0, 2.0, 3.0]}, \'patch\': {}}\n    )\n\n\n@pytest.mark.parametrize(\n    \'patchset_file\',\n    [\'patchset_bad_empty_patches.json\', \'patchset_bad_no_version.json\',],\n)\ndef test_patchset_invalid_spec(datadir, patchset_file):\n    patchsetspec = json.load(open(datadir.join(patchset_file)))\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.PatchSet(patchsetspec)\n\n\n@pytest.mark.parametrize(\n    \'patchset_file\',\n    [\n        \'patchset_bad_duplicate_patch_name.json\',\n        \'patchset_bad_duplicate_patch_values.json\',\n        \'patchset_bad_wrong_values_multiplicity.json\',\n    ],\n)\ndef test_patchset_bad(datadir, patchset_file):\n    patchsetspec = json.load(open(datadir.join(patchset_file)))\n    with pytest.raises(pyhf.exceptions.InvalidPatchSet):\n        pyhf.PatchSet(patchsetspec)\n\n\ndef test_patchset_attributes(patchset):\n    assert \'hepdata\' in patchset.references\n    assert patchset.description == ""signal patchset for the SUSY Multi-b-jet analysis""\n    assert len(patchset.digests) == 1\n    assert patchset.digests[\'md5\'] == ""098f6bcd4621d373cade4e832627b4f6""\n    assert patchset.labels == [""mass_stop"", ""mass_neutralino""]\n    assert patchset.version == ""1.0.0""\n\n\ndef test_patchset_get_patch_by_name(patchset):\n    assert patchset[\'Gtt_2100_5000_800\']\n\n\ndef test_patchset_get_patch_by_values(patchset):\n    assert patchset[2100, 800]\n    assert patchset[(2100, 800)]\n    assert patchset[[2100, 800]]\n\n\ndef test_patchset_get_nonexisting_patch(patchset):\n    with pytest.raises(pyhf.exceptions.InvalidPatchLookup) as excinfo:\n        patchset.__getitem__(\'nonexisting_patch\')\n    assert \'No patch associated with\' in str(excinfo.value)\n    assert \'nonexisting_patch\' in str(excinfo.value)\n\n\ndef test_patchset_iterable(patchset):\n    assert iter(patchset)\n    assert list(iter(patchset))\n    assert len(list(iter(patchset))) >= 1\n\n\ndef test_patchset_len(patchset):\n    assert len(patchset) == len(list(iter(patchset)))\n    assert len(patchset) == len(patchset.patches)\n\n\ndef test_patchset_repr(patchset):\n    assert repr(patchset)\n    if len(patchset) == 1:\n        assert \'PatchSet object with 1 patch at\' in repr(patchset)\n    else:\n        assert f\'PatchSet object with {len(patchset)} patches at\' in repr(patchset)\n\n\ndef test_patchset_verify(datadir):\n    patchset = pyhf.PatchSet(json.load(open(datadir.join(\'example_patchset.json\'))))\n    ws = pyhf.Workspace(json.load(open(datadir.join(\'example_bkgonly.json\'))))\n    assert patchset.verify(ws) is None\n\n\ndef test_patchset_verify_failure(datadir):\n    patchset = pyhf.PatchSet(json.load(open(datadir.join(\'example_patchset.json\'))))\n    with pytest.raises(pyhf.exceptions.PatchSetVerificationError):\n        assert patchset.verify({})\n\n\ndef test_patchset_apply(datadir):\n    patchset = pyhf.PatchSet(json.load(open(datadir.join(\'example_patchset.json\'))))\n    ws = pyhf.Workspace(json.load(open(datadir.join(\'example_bkgonly.json\'))))\n    with mock.patch(\'pyhf.patchset.PatchSet.verify\') as m:\n        assert m.call_count == 0\n        assert patchset.apply(ws, \'patch_channel1_signal_syst1\')\n        assert m.call_count == 1\n\n\ndef test_patch_hashable(patch):\n    assert patch.name == \'test\'\n    assert isinstance(patch.values, tuple)\n    assert patch.values == (1.0, 2.0, 3.0)\n\n\ndef test_patch_repr(patch):\n    assert repr(patch)\n    assert ""Patch object \'test(1.0, 2.0, 3.0)\' at"" in repr(patch)\n\n\ndef test_patch_equality(patch):\n    assert patch == patch\n    assert patch != object()\n'"
tests/test_pdf.py,13,"b'import pyhf\nimport pytest\nimport pyhf.exceptions\nimport numpy as np\nimport json\n\n\ndef test_pdf_inputs(backend):\n    source = {\n        ""binning"": [2, -0.5, 1.5],\n        ""bindata"": {""data"": [55.0], ""bkg"": [50.0], ""bkgerr"": [7.0], ""sig"": [10.0]},\n    }\n    pdf = pyhf.simplemodels.hepdata_like(\n        source[\'bindata\'][\'sig\'], source[\'bindata\'][\'bkg\'], source[\'bindata\'][\'bkgerr\']\n    )\n\n    pars = pdf.config.suggested_init()\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    tensorlib, _ = backend\n    assert tensorlib.shape(tensorlib.astensor(data)) == (2,)\n    assert tensorlib.shape(tensorlib.astensor(pars)) == (2,)\n    assert tensorlib.tolist(pdf.pdf(pars, data)) == pytest.approx(\n        [0.002417160663753748], abs=1e-4\n    )\n    assert tensorlib.tolist(pdf.logpdf(pars, data)) == pytest.approx(\n        [-6.025179228209936], abs=1e-4\n    )\n\n\ndef test_invalid_pdf_pars():\n    source = {\n        ""binning"": [2, -0.5, 1.5],\n        ""bindata"": {""data"": [55.0], ""bkg"": [50.0], ""bkgerr"": [7.0], ""sig"": [10.0]},\n    }\n    pdf = pyhf.simplemodels.hepdata_like(\n        source[\'bindata\'][\'sig\'], source[\'bindata\'][\'bkg\'], source[\'bindata\'][\'bkgerr\']\n    )\n\n    pars = pdf.config.suggested_init() + [1.0]\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    with pytest.raises(pyhf.exceptions.InvalidPdfParameters):\n        pdf.logpdf(pars, data)\n\n\ndef test_invalid_pdf_data():\n    source = {\n        ""binning"": [2, -0.5, 1.5],\n        ""bindata"": {""data"": [55.0], ""bkg"": [50.0], ""bkgerr"": [7.0], ""sig"": [10.0]},\n    }\n    pdf = pyhf.simplemodels.hepdata_like(\n        source[\'bindata\'][\'sig\'], source[\'bindata\'][\'bkg\'], source[\'bindata\'][\'bkgerr\']\n    )\n\n    pars = pdf.config.suggested_init()\n    data = source[\'bindata\'][\'data\'] + [10.0] + pdf.config.auxdata\n\n    with pytest.raises(pyhf.exceptions.InvalidPdfData):\n        pdf.logpdf(pars, data)\n\n\ndef test_pdf_basicapi_tests(backend):\n    source = {\n        ""binning"": [2, -0.5, 1.5],\n        ""bindata"": {""data"": [55.0], ""bkg"": [50.0], ""bkgerr"": [7.0], ""sig"": [10.0]},\n    }\n    pdf = pyhf.simplemodels.hepdata_like(\n        source[\'bindata\'][\'sig\'], source[\'bindata\'][\'bkg\'], source[\'bindata\'][\'bkgerr\']\n    )\n\n    pars = pdf.config.suggested_init()\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    tensorlib, _ = backend\n    assert tensorlib.tolist(pdf.pdf(pars, data)) == pytest.approx(\n        [0.002417118312751542], 2.5e-05\n    )\n    assert tensorlib.tolist(pdf.expected_data(pars)) == pytest.approx(\n        [60.0, 51.020408630], 1e-08\n    )\n\n    pdf = pyhf.simplemodels.hepdata_like(\n        source[\'bindata\'][\'sig\'],\n        source[\'bindata\'][\'bkg\'],\n        source[\'bindata\'][\'bkgerr\'],\n        batch_size=2,\n    )\n\n    pars = [pdf.config.suggested_init()] * 2\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    tensorlib, _ = backend\n    assert tensorlib.tolist(pdf.pdf(pars, data)) == pytest.approx(\n        [0.002417118312751542] * 2, 2.5e-05\n    )\n    assert tensorlib.tolist(pdf.expected_data(pars))\n    assert tensorlib.tolist(pdf.expected_data(pars)[0]) == pytest.approx(\n        [60.0, 51.020408630], 1e-08\n    )\n    assert tensorlib.tolist(pdf.expected_data(pars)[1]) == pytest.approx(\n        [60.0, 51.020408630], 1e-08\n    )\n\n\n@pytest.mark.only_numpy\ndef test_core_pdf_broadcasting(backend):\n    data = [10, 11, 12, 13, 14, 15]\n    lambdas = [15, 14, 13, 12, 11, 10]\n    naive_python = [pyhf.tensorlib.poisson(d, lam) for d, lam in zip(data, lambdas)]\n\n    broadcasted = pyhf.tensorlib.poisson(data, lambdas)\n\n    assert np.array(data).shape == np.array(lambdas).shape\n    assert broadcasted.shape == np.array(data).shape\n    assert np.all(naive_python == broadcasted)\n\n    data = [10, 11, 12, 13, 14, 15]\n    mus = [15, 14, 13, 12, 11, 10]\n    sigmas = [1, 2, 3, 4, 5, 6]\n    naive_python = [\n        pyhf.tensorlib.normal(d, mu, sig) for d, mu, sig in zip(data, mus, sigmas)\n    ]\n\n    broadcasted = pyhf.tensorlib.normal(data, mus, sigmas)\n\n    assert np.array(data).shape == np.array(mus).shape\n    assert np.array(data).shape == np.array(sigmas).shape\n    assert broadcasted.shape == np.array(data).shape\n    assert np.all(naive_python == broadcasted)\n\n\ndef test_pdf_integration_staterror(backend):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'firstchannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'mu\',\n                        \'data\': [10.0, 10.0],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg1\',\n                        \'data\': [50.0, 70.0],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'stat_firstchannel\',\n                                \'type\': \'staterror\',\n                                \'data\': [12.0, 12.0],\n                            }\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg2\',\n                        \'data\': [30.0, 20.0],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'stat_firstchannel\',\n                                \'type\': \'staterror\',\n                                \'data\': [5.0, 5.0],\n                            }\n                        ],\n                    },\n                    {\'name\': \'bkg3\', \'data\': [20.0, 15.0], \'modifiers\': []},\n                ],\n            }\n        ]\n    }\n    pdf = pyhf.Model(spec)\n    par_set = pdf.config.param_set(\'stat_firstchannel\')\n    tensorlib, _ = backend\n    uncerts = tensorlib.astensor([[12.0, 12.0], [5.0, 5.0]])\n    nominal = tensorlib.astensor([[50.0, 70.0], [30.0, 20.0]])\n    quad = tensorlib.sqrt(tensorlib.sum(tensorlib.power(uncerts, 2), axis=0))\n    totals = tensorlib.sum(nominal, axis=0)\n    assert pytest.approx(tensorlib.tolist(par_set.sigmas)) == tensorlib.tolist(\n        tensorlib.divide(quad, totals)\n    )\n\n\ndef test_pdf_integration_shapesys_zeros(backend):\n    spec = {\n        ""channels"": [\n            {\n                ""name"": ""channel1"",\n                ""samples"": [\n                    {\n                        ""data"": [20.0, 10.0, 5.0, 3.0, 2.0, 1.0],\n                        ""modifiers"": [\n                            {""data"": None, ""name"": ""mu"", ""type"": ""normfactor""}\n                        ],\n                        ""name"": ""signal"",\n                    },\n                    {\n                        ""data"": [100.0, 90, 0.0, 70, 0.1, 50],\n                        ""modifiers"": [\n                            {\n                                ""data"": [10, 9, 1, 0.0, 0.1, 5],\n                                ""name"": ""syst"",\n                                ""type"": ""shapesys"",\n                            },\n                            {\n                                ""data"": [0, 0, 0, 0, 0, 0],\n                                ""name"": ""syst_lowstats"",\n                                ""type"": ""shapesys"",\n                            },\n                        ],\n                        ""name"": ""background1"",\n                    },\n                ],\n            }\n        ]\n    }\n    pdf = pyhf.Model(spec)\n    par_set_syst = pdf.config.param_set(\'syst\')\n    par_set_syst_lowstats = pdf.config.param_set(\'syst_lowstats\')\n\n    assert par_set_syst.n_parameters == 4\n    assert par_set_syst_lowstats.n_parameters == 0\n    tensorlib, _ = backend\n    nominal_sq = tensorlib.power(tensorlib.astensor([100.0, 90, 0.0, 70, 0.1, 50]), 2)\n    uncerts_sq = tensorlib.power(tensorlib.astensor([10, 9, 1, 0.0, 0.1, 5]), 2)\n    factors = tensorlib.divide(nominal_sq, uncerts_sq)\n    indices = tensorlib.astensor([0, 1, 4, 5], dtype=\'int\')\n    assert pytest.approx(tensorlib.tolist(par_set_syst.factors)) == tensorlib.tolist(\n        tensorlib.gather(factors, indices)\n    )\n    assert getattr(par_set_syst_lowstats, \'factors\', None) is None\n\n\n@pytest.mark.only_numpy\ndef test_pdf_integration_histosys(backend):\n    source = json.load(open(\'validation/data/2bin_histosys_example2.json\'))\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'bkg_norm\',\n                                \'type\': \'histosys\',\n                                \'data\': {\n                                    \'lo_data\': source[\'bindata\'][\'bkgsys_dn\'],\n                                    \'hi_data\': source[\'bindata\'][\'bkgsys_up\'],\n                                },\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    pdf = pyhf.Model(spec)\n\n    pars = [None, None]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [1.0],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [102, 190]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [2.0],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [104, 230]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [-1.0],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [98, 100]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [-2.0],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [96, 50]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [1.0],\n        [1.0],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [\n        102 + 30,\n        190 + 95,\n    ]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [1.0],\n        [-1.0],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [\n        98 + 30,\n        100 + 95,\n    ]\n\n\ndef test_pdf_integration_normsys(backend):\n    source = json.load(open(\'validation/data/2bin_histosys_example2.json\'))\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'bkg_norm\',\n                                \'type\': \'normsys\',\n                                \'data\': {\'lo\': 0.9, \'hi\': 1.1},\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    pdf = pyhf.Model(spec)\n\n    pars = [None, None]\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [0.0],\n    ]\n    assert np.allclose(\n        pyhf.tensorlib.tolist(pdf.expected_data(pars, include_auxdata=False)),\n        [100, 150],\n    )\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [1.0],\n    ]\n    assert np.allclose(\n        pyhf.tensorlib.tolist(pdf.expected_data(pars, include_auxdata=False)),\n        [100 * 1.1, 150 * 1.1],\n    )\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [-1.0],\n    ]\n    assert np.allclose(\n        pyhf.tensorlib.tolist(pdf.expected_data(pars, include_auxdata=False)),\n        [100 * 0.9, 150 * 0.9],\n    )\n\n\n@pytest.mark.only_numpy\ndef test_pdf_integration_shapesys(backend):\n    source = json.load(open(\'validation/data/2bin_histosys_example2.json\'))\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\'name\': \'bkg_norm\', \'type\': \'shapesys\', \'data\': [10, 10]}\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    pdf = pyhf.Model(spec)\n\n    pars = [None, None]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [1.0, 1.0],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [100, 150]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [1.1, 1.0],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [100 * 1.1, 150]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [1.0, 1.1],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [100, 150 * 1.1]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [1.1, 0.9],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [\n        100 * 1.1,\n        150 * 0.9,\n    ]\n\n    pars[pdf.config.par_slice(\'mu\')], pars[pdf.config.par_slice(\'bkg_norm\')] = [\n        [0.0],\n        [0.9, 1.1],\n    ]\n    assert pdf.expected_data(pars, include_auxdata=False).tolist() == [\n        100 * 0.9,\n        150 * 1.1,\n    ]\n\n\ndef test_invalid_modifier():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\n                        \'name\': \'ttbar\',\n                        \'data\': [1],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'a_name\',\n                                \'type\': \'this_should_not_exist\',\n                                \'data\': [1],\n                            }\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidModifier):\n        pyhf.pdf._ModelConfig(spec)\n\n\ndef test_invalid_modifier_name_resuse():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'reused_name\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': [50.0],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'reused_name\',\n                                \'type\': \'normsys\',\n                                \'data\': {\'lo\': 0.9, \'hi\': 1.1},\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidNameReuse):\n        pyhf.Model(spec, poi_name=\'reused_name\')\n\n\ndef test_override_paramset_defaults():\n    source = json.load(open(\'validation/data/2bin_histosys_example2.json\'))\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\'name\': \'bkg_norm\', \'type\': \'shapesys\', \'data\': [10, 10]}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'parameters\': [\n            {\'name\': \'bkg_norm\', \'inits\': [99, 99], \'bounds\': [[95, 95], [95, 95]]}\n        ],\n    }\n    pdf = pyhf.Model(spec)\n    assert pdf.config.param_set(\'bkg_norm\').suggested_bounds == [[95, 95], [95, 95]]\n    assert pdf.config.param_set(\'bkg_norm\').suggested_init == [99, 99]\n\n\ndef test_override_paramsets_incorrect_num_parameters():\n    source = json.load(open(\'validation/data/2bin_histosys_example2.json\'))\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\'name\': \'bkg_norm\', \'type\': \'shapesys\', \'data\': [10, 10]}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'parameters\': [{\'name\': \'bkg_norm\', \'inits\': [99, 99], \'bounds\': [[95, 95]]}],\n    }\n    with pytest.raises(pyhf.exceptions.InvalidModel):\n        pyhf.Model(spec)\n\n\ndef test_lumi_np_scaling():\n    spec = {\n        ""channels"": [\n            {\n                ""name"": ""channel1"",\n                ""samples"": [\n                    {\n                        ""data"": [20.0, 10.0],\n                        ""modifiers"": [\n                            {\n                                ""data"": None,\n                                ""name"": ""SigXsecOverSM"",\n                                ""type"": ""normfactor"",\n                            },\n                            {""data"": None, ""name"": ""lumi"", ""type"": ""lumi""},\n                        ],\n                        ""name"": ""signal"",\n                    },\n                    {""data"": [100.0, 0.0], ""name"": ""background1"", ""modifiers"": []},\n                    {\n                        ""data"": [0.0, 100.0],\n                        ""modifiers"": [{""data"": None, ""name"": ""lumi"", ""type"": ""lumi""}],\n                        ""name"": ""background2"",\n                    },\n                ],\n            }\n        ],\n        ""parameters"": [\n            {\n                ""auxdata"": [1.0],\n                ""bounds"": [[0.0, 10.0]],\n                ""inits"": [1.0],\n                ""name"": ""lumi"",\n                ""sigmas"": [0.1],\n            }\n        ],\n    }\n    pdf = pyhf.pdf.Model(spec, poi_name=""SigXsecOverSM"")\n\n    poi_slice = pdf.config.par_slice(\'SigXsecOverSM\')\n    lumi_slice = pdf.config.par_slice(\'lumi\')\n\n    index_bkg1 = pdf.config.samples.index(\'background1\')\n    index_bkg2 = pdf.config.samples.index(\'background2\')\n    index_sig = pdf.config.samples.index(\'signal\')\n    bkg1_slice = slice(index_bkg1, index_bkg1 + 1)\n    bkg2_slice = slice(index_bkg2, index_bkg2 + 1)\n    sig_slice = slice(index_sig, index_sig + 1)\n\n    alpha_lumi = np.random.uniform(0.0, 10.0, 1)[0]\n\n    mods = [None, None, None]\n    pars = [None, None]\n\n    pars[poi_slice], pars[lumi_slice] = [[1.0], [1.0]]\n    mods[sig_slice], mods[bkg1_slice], mods[bkg2_slice] = [\n        [[[1.0, 1.0]]],\n        [[[1.0, 1.0]]],\n        [[[1.0, 1.0]]],\n    ]\n    assert pdf._modifications(np.array(pars))[1][0].tolist() == [mods]\n    assert pdf.expected_data(pars).tolist() == [120.0, 110.0, 1.0]\n\n    pars[poi_slice], pars[lumi_slice] = [[1.0], [alpha_lumi]]\n    mods[sig_slice], mods[bkg1_slice], mods[bkg2_slice] = [\n        [[[alpha_lumi, alpha_lumi]]],\n        [[[1.0, 1.0]]],\n        [[[alpha_lumi, alpha_lumi]]],\n    ]\n    assert pdf._modifications(np.array(pars))[1][0].tolist() == [mods]\n    assert pytest.approx(pdf.expected_data(pars).tolist()) == [\n        100 + 20.0 * alpha_lumi,\n        110.0 * alpha_lumi,\n        1.0 * alpha_lumi,\n    ]\n\n\ndef test_sample_wrong_bins():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'goodsample\', \'data\': [1.0, 2.0], \'modifiers\': []},\n                    {\'name\': \'badsample\', \'data\': [3.0, 4.0, 5.0], \'modifiers\': []},\n                ],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidModel):\n        pyhf.Model(spec)\n\n\n@pytest.mark.parametrize(\n    \'measurements, msettings\',\n    [\n        (\n            None,\n            {\'normsys\': {\'interpcode\': \'code4\'}, \'histosys\': {\'interpcode\': \'code4p\'},},\n        )\n    ],\n)\ndef test_unexpected_keyword_argument(measurements, msettings):\n    spec = {\n        ""channels"": [\n            {\n                ""name"": ""singlechannel"",\n                ""samples"": [\n                    {\n                        ""name"": ""signal"",\n                        ""data"": [5.0, 10.0],\n                        ""modifiers"": [\n                            {""name"": ""mu"", ""type"": ""normfactor"", ""data"": None}\n                        ],\n                    },\n                    {\n                        ""name"": ""background"",\n                        ""data"": [50.0, 60.0],\n                        ""modifiers"": [\n                            {\n                                ""name"": ""uncorr_bkguncrt"",\n                                ""type"": ""shapesys"",\n                                ""data"": [5.0, 12.0],\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    with pytest.raises(KeyError):\n        pyhf.pdf._ModelConfig(\n            spec, measurement_name=measurements, modifiers_settings=msettings\n        )\n'"
tests/test_probability.py,1,"b'from pyhf import probability\nimport numpy as np\n\n\ndef test_poisson(backend):\n    tb, _ = backend\n    result = probability.Poisson(tb.astensor([10.0])).log_prob(tb.astensor(2.0))\n    assert result.shape == (1,)\n\n    result = probability.Poisson(tb.astensor([10.0, 10.0])).log_prob(tb.astensor(2.0))\n    assert result.shape == (2,)\n\n    result = probability.Poisson(tb.astensor([10.0, 10.0])).log_prob(\n        tb.astensor([2.0, 3.0])\n    )\n    assert result.shape == (2,)\n\n    result = probability.Poisson(tb.astensor([10.0, 10.0])).log_prob(\n        tb.astensor([[2.0, 3.0]])\n    )\n    assert result.shape == (1, 2)\n\n\ndef test_normal(backend):\n    tb, _ = backend\n    result = probability.Normal(tb.astensor([10.0]), tb.astensor([1])).log_prob(\n        tb.astensor(2.0)\n    )\n    assert result.shape == (1,)\n\n    result = probability.Normal(\n        tb.astensor([10.0, 10.0]), tb.astensor([1, 1])\n    ).log_prob(tb.astensor(2.0))\n    assert result.shape == (2,)\n\n    result = probability.Normal(\n        tb.astensor([10.0, 10.0]), tb.astensor([10.0, 10.0])\n    ).log_prob(tb.astensor([2.0, 3.0]))\n    assert result.shape == (2,)\n\n    result = probability.Normal(\n        tb.astensor([10.0, 10.0]), tb.astensor([10.0, 10.0])\n    ).log_prob(tb.astensor([[2.0, 3.0]]))\n    assert result.shape == (1, 2)\n\n\ndef test_joint(backend):\n    tb, _ = backend\n    p1 = probability.Poisson(tb.astensor([10.0])).log_prob(tb.astensor(2.0))\n    p2 = probability.Poisson(tb.astensor([10.0])).log_prob(tb.astensor(3.0))\n    assert tb.tolist(probability.Simultaneous._joint_logpdf([p1, p2])) == tb.tolist(\n        p1 + p2\n    )\n\n\ndef test_independent(backend):\n    tb, _ = backend\n    result = probability.Independent(\n        probability.Poisson(tb.astensor([10.0, 10.0]))\n    ).log_prob(tb.astensor([2.0, 3.0]))\n\n    p1 = probability.Poisson(tb.astensor([10.0])).log_prob(tb.astensor(2.0))\n    p2 = probability.Poisson(tb.astensor([10.0])).log_prob(tb.astensor(3.0))\n    assert tb.tolist(probability.Simultaneous._joint_logpdf([p1, p2]))[0] == tb.tolist(\n        result\n    )\n    assert tb.tolist(probability.Simultaneous._joint_logpdf([p1, p2]))[0] == tb.tolist(\n        result\n    )\n\n\ndef test_simultaneous_list_ducktype():\n    myobjs = np.random.randint(100, size=10).tolist()\n    sim = probability.Simultaneous(myobjs, None)\n    assert sim[3] == myobjs[3]\n    for simobj, myobj in zip(sim, myobjs):\n        assert simobj == myobj\n'"
tests/test_public_api.py,2,"b'import pytest\nimport pyhf\nimport numpy as np\n\n\n@pytest.fixture(scope=\'function\')\ndef model_setup(backend):\n    np.random.seed(0)\n    n_bins = 100\n    model = pyhf.simplemodels.hepdata_like([10] * n_bins, [50] * n_bins, [1] * n_bins)\n    init_pars = model.config.suggested_init()\n    observations = np.random.randint(50, 60, size=n_bins).tolist()\n    data = observations + model.config.auxdata\n    return model, data, init_pars\n\n\n@pytest.mark.parametrize(""backend_name"", [""numpy"", ""tensorflow"", ""pytorch"", ""PyTorch""])\ndef test_set_backend_by_string(backend_name):\n    pyhf.set_backend(backend_name)\n    assert isinstance(\n        pyhf.tensorlib,\n        getattr(pyhf.tensor, ""{0:s}_backend"".format(backend_name.lower())),\n    )\n\n\n@pytest.mark.parametrize(""backend_name"", [b""numpy"", b""tensorflow"", b""pytorch""])\ndef test_set_backend_by_bytestring(backend_name):\n    pyhf.set_backend(backend_name)\n    assert isinstance(\n        pyhf.tensorlib,\n        getattr(pyhf.tensor, ""{0:s}_backend"".format(backend_name.decode(""utf-8""))),\n    )\n\n\n@pytest.mark.parametrize(""backend_name"", [""fail"", b""fail""])\ndef test_supported_backends(backend_name):\n    with pytest.raises(pyhf.exceptions.InvalidBackend):\n        pyhf.set_backend(backend_name)\n\n\ndef test_custom_backend_name_supported():\n    class custom_backend(object):\n        def __init__(self, **kwargs):\n            self.name = ""pytorch""\n\n    with pytest.raises(AttributeError):\n        pyhf.set_backend(custom_backend())\n\n\ndef test_custom_backend_name_notsupported():\n    class custom_backend(object):\n        def __init__(self, **kwargs):\n            self.name = ""notsupported""\n\n    backend = custom_backend()\n    assert pyhf.tensorlib.name != backend.name\n    pyhf.set_backend(backend)\n    assert pyhf.tensorlib.name == backend.name\n\n\ndef test_logpprob(backend, model_setup):\n    model, data, init_pars = model_setup\n    model.logpdf(init_pars, data)\n\n\ndef test_hypotest(backend, model_setup):\n    model, data, init_pars = model_setup\n    mu = 1.0\n    pyhf.infer.hypotest(\n        mu,\n        data,\n        model,\n        init_pars,\n        model.config.suggested_bounds(),\n        return_expected_set=True,\n    )\n\n\ndef test_prob_models(backend):\n    tb, _ = backend\n    pyhf.probability.Poisson(tb.astensor([10.0])).log_prob(tb.astensor(2.0))\n    pyhf.probability.Normal(tb.astensor([10.0]), tb.astensor([1])).log_prob(\n        tb.astensor(2.0)\n    )\n\n\ndef test_pdf_batched(backend):\n    tb, _ = backend\n    source = {\n        ""binning"": [2, -0.5, 1.5],\n        ""bindata"": {""data"": [55.0], ""bkg"": [50.0], ""bkgerr"": [7.0], ""sig"": [10.0]},\n    }\n    model = pyhf.simplemodels.hepdata_like(\n        source[\'bindata\'][\'sig\'],\n        source[\'bindata\'][\'bkg\'],\n        source[\'bindata\'][\'bkgerr\'],\n        batch_size=2,\n    )\n\n    pars = [model.config.suggested_init()] * 2\n    data = source[\'bindata\'][\'data\'] + model.config.auxdata\n\n    model.pdf(pars, data)\n    model.expected_data(pars)\n'"
tests/test_regression.py,20,"b'import pytest\nimport pyhf\nimport numpy as np\n\n\ndef calculate_CLs(bkgonly_json, signal_patch_json):\n    """"""\n    Calculate the observed CLs and the expected CLs band from a background only\n    and signal patch.\n\n    Args:\n        bkgonly_json: The JSON for the background only model\n        signal_patch_json: The JSON Patch for the signal model\n\n    Returns:\n        CLs_obs: The observed CLs value\n        CLs_exp: List of the expected CLs value band\n    """"""\n    workspace = pyhf.workspace.Workspace(bkgonly_json)\n    model = workspace.model(\n        measurement_name=None,\n        patches=[signal_patch_json],\n        modifier_settings={\n            \'normsys\': {\'interpcode\': \'code4\'},\n            \'histosys\': {\'interpcode\': \'code4p\'},\n        },\n    )\n    result = pyhf.infer.hypotest(\n        1.0, workspace.data(model), model, qtilde=True, return_expected_set=True\n    )\n    return result[0].tolist()[0], result[-1].ravel().tolist()\n\n\ndef test_sbottom_regionA_1300_205_60(\n    sbottom_likelihoods_download, get_json_from_tarfile\n):\n    sbottom_regionA_bkgonly_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionA/BkgOnly.json""\n    )\n    sbottom_regionA_1300_205_60_patch_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionA/patch.sbottom_1300_205_60.json""\n    )\n    CLs_obs, CLs_exp = calculate_CLs(\n        sbottom_regionA_bkgonly_json, sbottom_regionA_1300_205_60_patch_json\n    )\n    assert CLs_obs == pytest.approx(0.24443627759085326, rel=1e-5)\n    assert np.all(\n        np.isclose(\n            np.array(CLs_exp),\n            np.array(\n                [\n                    0.09022509053507759,\n                    0.1937839194960632,\n                    0.38432344933992,\n                    0.6557757334303531,\n                    0.8910420971601081,\n                ]\n            ),\n            rtol=1e-5,\n        )\n    )\n\n\ndef test_sbottom_regionA_1400_950_60(\n    sbottom_likelihoods_download, get_json_from_tarfile\n):\n    sbottom_regionA_bkgonly_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionA/BkgOnly.json""\n    )\n    sbottom_regionA_1400_950_60_patch_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionA/patch.sbottom_1400_950_60.json""\n    )\n    CLs_obs, CLs_exp = calculate_CLs(\n        sbottom_regionA_bkgonly_json, sbottom_regionA_1400_950_60_patch_json\n    )\n    assert CLs_obs == pytest.approx(0.021373283911064852, rel=1e-5)\n    assert np.all(\n        np.isclose(\n            np.array(CLs_exp),\n            np.array(\n                [\n                    0.002644707461012826,\n                    0.013976754489151644,\n                    0.06497313811425813,\n                    0.23644505123524753,\n                    0.5744843501873754,\n                ]\n            ),\n            rtol=1e-5,\n        )\n    )\n\n\ndef test_sbottom_regionA_1500_850_60(\n    sbottom_likelihoods_download, get_json_from_tarfile\n):\n    sbottom_regionA_bkgonly_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionA/BkgOnly.json""\n    )\n    sbottom_regionA_1500_850_60_patch_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionA/patch.sbottom_1500_850_60.json""\n    )\n    CLs_obs, CLs_exp = calculate_CLs(\n        sbottom_regionA_bkgonly_json, sbottom_regionA_1500_850_60_patch_json\n    )\n    assert CLs_obs == pytest.approx(0.04536774062150508, rel=1e-5)\n    assert np.all(\n        np.isclose(\n            np.array(CLs_exp),\n            np.array(\n                [\n                    0.0059847029077065295,\n                    0.026103516126601122,\n                    0.10093985752614597,\n                    0.3101988586187604,\n                    0.6553686728646031,\n                ]\n            ),\n            rtol=1e-5,\n        )\n    )\n\n\ndef test_sbottom_regionB_1400_550_60(\n    sbottom_likelihoods_download, get_json_from_tarfile\n):\n    sbottom_regionB_bkgonly_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionB/BkgOnly.json""\n    )\n    sbottom_regionB_1400_550_60_patch_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionB/patch.sbottom_1400_550_60.json""\n    )\n    CLs_obs, CLs_exp = calculate_CLs(\n        sbottom_regionB_bkgonly_json, sbottom_regionB_1400_550_60_patch_json\n    )\n    assert CLs_obs == pytest.approx(0.9744675266677597, rel=1e-5)\n    assert np.all(\n        np.isclose(\n            np.array(CLs_exp),\n            np.array(\n                [\n                    0.9338879894557114,\n                    0.9569045303300702,\n                    0.9771296335437559,\n                    0.9916370124133669,\n                    0.9983701133999316,\n                ]\n            ),\n            rtol=1e-5,\n        )\n    )\n\n\ndef test_sbottom_regionC_1600_850_60(\n    sbottom_likelihoods_download, get_json_from_tarfile\n):\n    sbottom_regionC_bkgonly_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionC/BkgOnly.json""\n    )\n    sbottom_regionC_1600_850_60_patch_json = get_json_from_tarfile(\n        sbottom_likelihoods_download, ""RegionC/patch.sbottom_1600_850_60.json""\n    )\n    CLs_obs, CLs_exp = calculate_CLs(\n        sbottom_regionC_bkgonly_json, sbottom_regionC_1600_850_60_patch_json\n    )\n    assert CLs_obs == pytest.approx(0.711023707425625, rel=1e-5)\n    assert np.all(\n        np.isclose(\n            np.array(CLs_exp),\n            np.array(\n                [\n                    0.2955492909588046,\n                    0.4446885457298284,\n                    0.6371473864200973,\n                    0.8336149623750603,\n                    0.9585901381554178,\n                ]\n            ),\n            rtol=1e-5,\n        )\n    )\n'"
tests/test_schema.py,0,"b'import pyhf\nimport pytest\nimport json\n\n\ndef test_no_channels():\n    spec = {\'channels\': []}\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_no_samples():\n    spec = {\'channels\': [{\'name\': \'channel\', \'samples\': []}]}\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_sample_missing_data():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [{\'name\': \'sample\', \'data\': [], \'modifiers\': []}],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_sample_missing_name():\n    spec = {\n        \'channels\': [{\'name\': \'channel\', \'samples\': [{\'data\': [1], \'modifiers\': []}]}]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_sample_missing_all_modifiers():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [{\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []}],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidModel):\n        pyhf.Model(spec)\n\n\ndef test_one_sample_missing_modifiers():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []},\n                    {\n                        \'name\': \'another_sample\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'mypoi\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    pyhf.Model(spec, poi_name=\'mypoi\')\n\n\ndef test_add_unknown_modifier():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\n                        \'name\': \'ttbar\',\n                        \'data\': [1],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'a_name\',\n                                \'type\': \'this_should_not_exist\',\n                                \'data\': [1],\n                            }\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_empty_staterror():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\n                        \'name\': \'sample\',\n                        \'data\': [10.0],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'staterror_channel\',\n                                \'type\': \'staterror\',\n                                \'data\': [],\n                            }\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_empty_shapesys():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\n                        \'name\': \'sample\',\n                        \'data\': [10.0],\n                        \'modifiers\': [\n                            {\'name\': \'sample_norm\', \'type\': \'shapesys\', \'data\': []}\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_empty_histosys():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\n                        \'name\': \'sample\',\n                        \'data\': [10.0],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'modifier\',\n                                \'type\': \'histosys\',\n                                \'data\': {\'lo_data\': [], \'hi_data\': []},\n                            }\n                        ],\n                    }\n                ],\n            }\n        ]\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_additional_properties():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []},\n                    {\n                        \'name\': \'another_sample\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'mypoi\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'fake_additional_property\': 2,\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec)\n\n\ndef test_parameters_definition():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []},\n                    {\n                        \'name\': \'another_sample\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'mypoi\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'parameters\': [{\'name\': \'mypoi\'}],\n    }\n    pyhf.Model(spec, poi_name=\'mypoi\')\n\n\ndef test_parameters_incorrect_format():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []},\n                    {\n                        \'name\': \'another_sample\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'mypoi\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'parameters\': {\'a\': \'fake\', \'object\': 2},\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec, poi_name=\'mypoi\')\n\n\ndef test_parameters_duplicated():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []},\n                    {\n                        \'name\': \'another_sample\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'mypoi\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'parameters\': [{\'name\': \'mypoi\'}, {\'name\': \'mypoi\'}],\n    }\n    with pytest.raises(pyhf.exceptions.InvalidModel):\n        pyhf.Model(spec, poi_name=\'mypoi\')\n\n\ndef test_parameters_all_props():\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []},\n                    {\n                        \'name\': \'another_sample\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'mypoi\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'parameters\': [{\'name\': \'mypoi\', \'inits\': [1], \'bounds\': [[0, 1]]}],\n    }\n    pyhf.Model(spec, poi_name=\'mypoi\')\n\n\n@pytest.mark.parametrize(\n    \'bad_parameter\',\n    [\n        {\'name\': \'mypoi\', \'inits\': [\'a\']},\n        {\'name\': \'mypoi\', \'bounds\': [0, 1]},\n        {\'name\': \'mypoi\', \'auxdata\': [\'a\']},\n        {\'name\': \'mypoi\', \'factors\': [\'a\']},\n        {\'name\': \'mypoi\', \'paramset_type\': \'fake_paramset_type\'},\n        {\'name\': \'mypoi\', \'n_parameters\': 5},\n        {\'name\': \'mypoi\', \'op_code\': \'fake_op_code\'},\n    ],\n    ids=[\n        \'inits\',\n        \'bounds\',\n        \'auxdata\',\n        \'factors\',\n        \'paramset_type\',\n        \'n_parameters\',\n        \'op_code\',\n    ],\n)\ndef test_parameters_bad_parameter(bad_parameter):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []},\n                    {\n                        \'name\': \'another_sample\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'mypoi\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'parameters\': [bad_parameter],\n    }\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.Model(spec, poi_name=\'mypoi\')\n\n\n@pytest.mark.parametrize(\n    \'bad_parameter\', [{\'name\': \'mypoi\', \'factors\': [0.0]}], ids=[\'factors\']\n)\ndef test_parameters_normfactor_bad_attribute(bad_parameter):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'channel\',\n                \'samples\': [\n                    {\'name\': \'sample\', \'data\': [10.0], \'modifiers\': []},\n                    {\n                        \'name\': \'another_sample\',\n                        \'data\': [5.0],\n                        \'modifiers\': [\n                            {\'name\': \'mypoi\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                ],\n            }\n        ],\n        \'parameters\': [bad_parameter],\n    }\n    with pytest.raises(pyhf.exceptions.InvalidModel):\n        pyhf.Model(spec, poi_name=\'mypoi\')\n\n\n@pytest.mark.parametrize(\n    \'patch\',\n    [\n        {""op"": ""add"", ""path"": ""/foo/0/bar"", ""value"": {""foo"": [1.0]}},\n        {""op"": ""replace"", ""path"": ""/foo/0/bar"", ""value"": {""foo"": [1.0]}},\n        {""op"": ""test"", ""path"": ""/foo/0/bar"", ""value"": {""foo"": [1.0]}},\n        {""op"": ""remove"", ""path"": ""/foo/0/bar""},\n        {""op"": ""move"", ""path"": ""/foo/0/bar"", ""from"": ""/foo/0/baz""},\n        {""op"": ""copy"", ""path"": ""/foo/0/bar"", ""from"": ""/foo/0/baz""},\n    ],\n    ids=[\'add\', \'replace\', \'test\', \'remove\', \'move\', \'copy\'],\n)\ndef test_jsonpatch(patch):\n    pyhf.utils.validate([patch], \'jsonpatch.json\')\n\n\n@pytest.mark.parametrize(\n    \'patch\',\n    [\n        {""path"": ""/foo/0/bar""},\n        {""op"": ""add"", ""path"": ""/foo/0/bar"", ""from"": {""foo"": [1.0]}},\n        {""op"": ""add"", ""path"": ""/foo/0/bar""},\n        {""op"": ""add"", ""value"": {""foo"": [1.0]}},\n        {""op"": ""remove""},\n        {""op"": ""move"", ""path"": ""/foo/0/bar""},\n        {""op"": ""move"", ""from"": ""/foo/0/baz""},\n    ],\n    ids=[\n        \'noop\',\n        \'add_from_novalue\',\n        \'add_novalue\',\n        \'add_nopath\',\n        \'remove_nopath\',\n        \'move_nofrom\',\n        \'move_nopath\',\n    ],\n)\ndef test_jsonpatch_fail(patch):\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.utils.validate([patch], \'jsonpatch.json\')\n\n\n@pytest.mark.parametrize(\n    \'patchset_file\', [\'patchset_good.json\'],\n)\ndef test_patchset(datadir, patchset_file):\n    patchset = json.load(open(datadir.join(patchset_file)))\n    pyhf.utils.validate(patchset, \'patchset.json\')\n\n\n@pytest.mark.parametrize(\n    \'patchset_file\',\n    [\n        \'patchset_bad_label_pattern.json\',\n        \'patchset_bad_no_patch_name.json\',\n        \'patchset_bad_empty_patches.json\',\n        \'patchset_bad_no_patch_values.json\',\n        \'patchset_bad_no_digests.json\',\n        \'patchset_bad_no_description.json\',\n        \'patchset_bad_no_labels.json\',\n        \'patchset_bad_invalid_digests.json\',\n        \'patchset_bad_hepdata_reference.json\',\n        \'patchset_bad_no_version.json\',\n    ],\n)\ndef test_patchset_fail(datadir, patchset_file):\n    patchset = json.load(open(datadir.join(patchset_file)))\n    with pytest.raises(pyhf.exceptions.InvalidSpecification):\n        pyhf.utils.validate(patchset, \'patchset.json\')\n'"
tests/test_scripts.py,1,"b'import json\nimport shlex\nimport pyhf\nimport time\nimport pytest\n\n\ndef test_version(script_runner):\n    command = \'pyhf --version\'\n    start = time.time()\n    ret = script_runner.run(*shlex.split(command))\n    end = time.time()\n    elapsed = end - start\n    assert ret.success\n    assert pyhf.__version__ in ret.stdout\n    assert ret.stderr == \'\'\n    # make sure it took less than a second\n    assert elapsed < 1.0\n\n\n# see test_import.py for the same (detailed) test\ndef test_import_prepHistFactory(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n    assert ret.stdout == \'\'\n    assert ret.stderr == \'\'\n\n    parsed_xml = json.loads(temp.read())\n    spec = {\'channels\': parsed_xml[\'channels\']}\n    pyhf.utils.validate(spec, \'model.json\')\n\n\ndef test_import_prepHistFactory_withProgress(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n    assert ret.stdout == \'\'\n    assert ret.stderr != \'\'\n\n\ndef test_import_prepHistFactory_stdout(tmpdir, script_runner):\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/\'\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n    assert ret.stdout != \'\'\n    assert ret.stderr != \'\'\n    d = json.loads(ret.stdout)\n    assert d\n\n\ndef test_import_prepHistFactory_and_cls(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf cls {0:s}\'.format(temp.strpath)\n    ret = script_runner.run(*shlex.split(command))\n\n    assert ret.success\n    d = json.loads(ret.stdout)\n    assert d\n    assert \'CLs_obs\' in d\n    assert \'CLs_exp\' in d\n\n    for measurement in [\n        \'GaussExample\',\n        \'GammaExample\',\n        \'LogNormExample\',\n        \'ConstExample\',\n    ]:\n        command = \'pyhf cls {0:s} --measurement {1:s}\'.format(temp.strpath, measurement)\n        ret = script_runner.run(*shlex.split(command))\n\n        assert ret.success\n        d = json.loads(ret.stdout)\n        assert d\n        assert \'CLs_obs\' in d\n        assert \'CLs_exp\' in d\n\n        tmp_out = tmpdir.join(\'{0:s}_output.json\'.format(measurement))\n        # make sure output file works too\n        command += \' --output-file {0:s}\'.format(tmp_out.strpath)\n        ret = script_runner.run(*shlex.split(command))\n        assert ret.success\n        d = json.load(tmp_out)\n        assert \'CLs_obs\' in d\n        assert \'CLs_exp\' in d\n\n\n@pytest.mark.parametrize(\n    ""backend"", [""numpy"", ""tensorflow"", ""pytorch"", ""jax""],\n)\ndef test_cls_backend_option(tmpdir, script_runner, backend):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf cls --backend {0:s} {1:s}\'.format(backend, temp.strpath)\n    ret = script_runner.run(*shlex.split(command))\n\n    assert ret.success\n    d = json.loads(ret.stdout)\n    assert d\n    assert \'CLs_obs\' in d\n    assert \'CLs_exp\' in d\n\n\ndef test_import_and_export(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf json2xml {0:s} --output-dir {1:s}\'.format(\n        temp.strpath, tmpdir.mkdir(\'output\').strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n\ndef test_patch(tmpdir, script_runner):\n    patch = tmpdir.join(\'patch.json\')\n\n    patch.write(\n        u\'\'\'\n[{""op"": ""replace"", ""path"": ""/channels/0/samples/0/data"", ""value"": [5,6]}]\n    \'\'\'\n    )\n\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf cls {0:s} --patch {1:s}\'.format(temp.strpath, patch.strpath)\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n    command = \'pyhf json2xml {0:s} --output-dir {1:s} --patch {2:s}\'.format(\n        temp.strpath, tmpdir.mkdir(\'output_1\').strpath, patch.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n    command = f\'pyhf cls {temp.strpath:s} --patch -\'\n\n    ret = script_runner.run(*shlex.split(command), stdin=patch)\n    assert ret.success\n\n    command = f""pyhf json2xml {temp.strpath:s} --output-dir {tmpdir.mkdir(\'output_2\').strpath:s} --patch -""\n    ret = script_runner.run(*shlex.split(command), stdin=patch)\n    assert ret.success\n\n\ndef test_patch_fail(tmpdir, script_runner):\n    patch = tmpdir.join(\'patch.json\')\n\n    patch.write(\'\'\'not,json\'\'\')\n\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf cls {0:s} --patch {1:s}\'.format(temp.strpath, patch.strpath)\n    ret = script_runner.run(*shlex.split(command))\n    assert not ret.success\n\n    command = \'pyhf json2xml {0:s} --output-dir {1:s} --patch {2:s}\'.format(\n        temp.strpath, tmpdir.mkdir(\'output\').strpath, patch.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert not ret.success\n\n\ndef test_bad_measurement_name(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf cls {0:s} --measurement ""a-fake-measurement-name""\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert not ret.success\n    # assert \'no measurement by name\' in ret.stderr  # numpy swallows the log.error() here, dunno why\n\n\ndef test_testpoi(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    pois = [1.0, 0.5, 0.0]\n    results_exp = []\n    results_obs = []\n    for testpoi in pois:\n        command = \'pyhf cls {0:s} --testpoi {testpoi:f}\'.format(\n            temp.strpath, testpoi=testpoi\n        )\n        ret = script_runner.run(*shlex.split(command))\n\n        assert ret.success\n        d = json.loads(ret.stdout)\n        assert d\n        assert \'CLs_obs\' in d\n        assert \'CLs_exp\' in d\n\n        results_exp.append(d[\'CLs_exp\'])\n        results_obs.append(d[\'CLs_obs\'])\n\n    import numpy as np\n    import itertools\n\n    for pair in itertools.combinations(results_exp, r=2):\n        assert not np.array_equal(*pair)\n\n    assert len(list(set(results_obs))) == len(pois)\n\n\n@pytest.mark.parametrize(\n    \'opts,success\',\n    [([\'maxiter=1000\'], True), ([\'maxiter=100\'], True), ([\'maxiter=10\'], False)],\n)\ndef test_cls_optimizer(tmpdir, script_runner, opts, success):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf cls {0:s} --optimizer scipy_optimizer {1:s}\'.format(\n        temp.strpath, \' \'.join(\'--optconf {0:s}\'.format(opt) for opt in opts)\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    assert ret.success == success\n\n\ndef test_inspect(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf inspect {0:s}\'.format(temp.strpath)\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n\ndef test_inspect_outfile(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    tempout = tmpdir.join(""inspect_output.json"")\n    command = \'pyhf inspect {0:s} --output-file {1:s}\'.format(\n        temp.strpath, tempout.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n    summary = json.loads(tempout.read())\n    assert [\n        \'channels\',\n        \'measurements\',\n        \'modifiers\',\n        \'parameters\',\n        \'samples\',\n        \'systematics\',\n    ] == sorted(summary.keys())\n    assert len(summary[\'channels\']) == 1\n    assert len(summary[\'measurements\']) == 4\n    assert len(summary[\'modifiers\']) == 6\n    assert len(summary[\'parameters\']) == 6\n    assert len(summary[\'samples\']) == 3\n    assert len(summary[\'systematics\']) == 6\n\n\ndef test_prune(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf prune -m staterror_channel1 --measurement GammaExample {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n\ndef test_prune_outfile(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    tempout = tmpdir.join(""prune_output.json"")\n    command = \'pyhf prune -m staterror_channel1 --measurement GammaExample {0:s} --output-file {1:s}\'.format(\n        temp.strpath, tempout.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n    spec = json.loads(temp.read())\n    ws = pyhf.Workspace(spec)\n    assert \'GammaExample\' in ws.measurement_names\n    assert \'staterror_channel1\' in ws.parameters\n    pruned_spec = json.loads(tempout.read())\n    pruned_ws = pyhf.Workspace(pruned_spec)\n    assert \'GammaExample\' not in pruned_ws.measurement_names\n    assert \'staterror_channel1\' not in pruned_ws.parameters\n\n\ndef test_rename(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf rename -m staterror_channel1 staterror_channelone --measurement GammaExample GamEx {0:s}\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n\ndef test_rename_outfile(tmpdir, script_runner):\n    temp = tmpdir.join(""parsed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    tempout = tmpdir.join(""rename_output.json"")\n    command = \'pyhf rename -m staterror_channel1 staterror_channelone --measurement GammaExample GamEx {0:s} --output-file {1:s}\'.format(\n        temp.strpath, tempout.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n    spec = json.loads(temp.read())\n    ws = pyhf.Workspace(spec)\n    assert \'GammaExample\' in ws.measurement_names\n    assert \'GamEx\' not in ws.measurement_names\n    assert \'staterror_channel1\' in ws.parameters\n    assert \'staterror_channelone\' not in ws.parameters\n    renamed_spec = json.loads(tempout.read())\n    renamed_ws = pyhf.Workspace(renamed_spec)\n    assert \'GammaExample\' not in renamed_ws.measurement_names\n    assert \'GamEx\' in renamed_ws.measurement_names\n    assert \'staterror_channel1\' not in renamed_ws.parameters\n    assert \'staterror_channelone\' in renamed_ws.parameters\n\n\ndef test_combine(tmpdir, script_runner):\n    temp_1 = tmpdir.join(""parsed_output.json"")\n    temp_2 = tmpdir.join(""renamed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp_1.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    rename_channels = {\'channel1\': \'channel2\'}\n    rename_measurements = {\n        \'ConstExample\': \'OtherConstExample\',\n        \'LogNormExample\': \'OtherLogNormExample\',\n        \'GaussExample\': \'OtherGaussExample\',\n        \'GammaExample\': \'OtherGammaExample\',\n    }\n\n    command = \'pyhf rename {0:s} {1:s} {2:s} --output-file {3:s}\'.format(\n        temp_1.strpath,\n        \'\'.join(\' -c \' + \' \'.join(item) for item in rename_channels.items()),\n        \'\'.join(\n            \' --measurement \' + \' \'.join(item) for item in rename_measurements.items()\n        ),\n        temp_2.strpath,\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    command = \'pyhf combine {0:s} {1:s}\'.format(temp_1.strpath, temp_2.strpath)\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n\ndef test_combine_outfile(tmpdir, script_runner):\n    temp_1 = tmpdir.join(""parsed_output.json"")\n    temp_2 = tmpdir.join(""renamed_output.json"")\n    command = \'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {0:s} --hide-progress\'.format(\n        temp_1.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    rename_channels = {\'channel1\': \'channel2\'}\n    rename_measurements = {\n        \'ConstExample\': \'OtherConstExample\',\n        \'LogNormExample\': \'OtherLogNormExample\',\n        \'GaussExample\': \'OtherGaussExample\',\n        \'GammaExample\': \'OtherGammaExample\',\n    }\n\n    command = \'pyhf rename {0:s} {1:s} {2:s} --output-file {3:s}\'.format(\n        temp_1.strpath,\n        \'\'.join(\' -c \' + \' \'.join(item) for item in rename_channels.items()),\n        \'\'.join(\n            \' --measurement \' + \' \'.join(item) for item in rename_measurements.items()\n        ),\n        temp_2.strpath,\n    )\n    ret = script_runner.run(*shlex.split(command))\n\n    tempout = tmpdir.join(""combined_output.json"")\n    command = \'pyhf combine {0:s} {1:s} --output-file {2:s}\'.format(\n        temp_1.strpath, temp_2.strpath, tempout.strpath\n    )\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n\n    combined_spec = json.loads(tempout.read())\n    combined_ws = pyhf.Workspace(combined_spec)\n    assert combined_ws.channels == [\'channel1\', \'channel2\']\n    assert len(combined_ws.measurement_names) == 8\n\n\n@pytest.mark.parametrize(\'do_json\', [False, True])\n@pytest.mark.parametrize(\n    \'algorithms\', [[\'md5\'], [\'sha256\'], [\'sha256\', \'md5\'], [\'sha256\', \'md5\']]\n)\ndef test_workspace_digest(tmpdir, script_runner, algorithms, do_json):\n    results = {\n        \'md5\': \'202eb7615102c35ba86be47eb6fa5e78\',\n        \'sha256\': \'7c32ca3b8db75cbafcf5cd7ed4672fa2b1fa69e391c9b89068dd947a521866ec\',\n    }\n\n    temp = tmpdir.join(""parsed_output.json"")\n    command = f\'pyhf xml2json validation/xmlimport_input/config/example.xml --basedir validation/xmlimport_input/ --output-file {temp.strpath} --hide-progress\'\n    ret = script_runner.run(*shlex.split(command))\n\n    command = f""pyhf digest {temp.strpath} -a {\' -a \'.join(algorithms)}{\' -j\' if do_json else \'\'}""\n    ret = script_runner.run(*shlex.split(command))\n    assert ret.success\n    assert all(algorithm in ret.stdout for algorithm in algorithms)\n    if do_json:\n        expected_output = json.dumps(\n            {algorithm: results[algorithm] for algorithm in algorithms},\n            sort_keys=True,\n            indent=4,\n        )\n    else:\n        expected_output = \'\\n\'.join(\n            f""{algorithm}:{results[algorithm]}"" for algorithm in algorithms\n        )\n\n    assert ret.stdout == expected_output + \'\\n\'\n    assert ret.stderr == \'\'\n\n    if do_json:\n        assert json.loads(ret.stdout) == {\n            algorithm: results[algorithm] for algorithm in algorithms\n        }\n\n\n@pytest.mark.parametrize(\'output_file\', [False, True])\n@pytest.mark.parametrize(\'with_metadata\', [False, True])\ndef test_patchset_extract(datadir, tmpdir, script_runner, output_file, with_metadata):\n    temp = tmpdir.join(""extracted_output.json"")\n    command = f\'pyhf patchset extract {datadir.join(""example_patchset.json"").strpath} --name patch_channel1_signal_syst1\'\n    if output_file:\n        command += f"" --output-file {temp.strpath}""\n    if with_metadata:\n        command += "" --with-metadata""\n\n    ret = script_runner.run(*shlex.split(command))\n\n    assert ret.success\n    if output_file:\n        extracted_output = json.loads(temp.read())\n    else:\n        extracted_output = json.loads(ret.stdout)\n    if with_metadata:\n        assert \'metadata\' in extracted_output\n    else:\n        assert (\n            extracted_output\n            == json.load(datadir.join(""example_patchset.json""))[\'patches\'][0][\'patch\']\n        )\n\n\ndef test_patchset_verify(datadir, script_runner):\n    command = f\'pyhf patchset verify {datadir.join(""example_bkgonly.json"").strpath} {datadir.join(""example_patchset.json"").strpath}\'\n    ret = script_runner.run(*shlex.split(command))\n\n    assert ret.success\n    assert \'All good\' in ret.stdout\n\n\n@pytest.mark.parametrize(\'output_file\', [False, True])\ndef test_patchset_apply(datadir, tmpdir, script_runner, output_file):\n    temp = tmpdir.join(""patched_output.json"")\n    command = f\'pyhf patchset apply {datadir.join(""example_bkgonly.json"").strpath} {datadir.join(""example_patchset.json"").strpath} --name patch_channel1_signal_syst1\'\n    if output_file:\n        command += f"" --output-file {temp.strpath}""\n\n    ret = script_runner.run(*shlex.split(command))\n\n    assert ret.success\n    if output_file:\n        extracted_output = json.loads(temp.read())\n    else:\n        extracted_output = json.loads(ret.stdout)\n    assert extracted_output[\'channels\'][0][\'samples\'][0][\'modifiers\'][0][\'data\'] == {\n        ""hi"": 1.2,\n        ""lo"": 0.8,\n    }\n'"
tests/test_tensor.py,9,"b'import pytest\nimport logging\nimport numpy as np\nimport tensorflow as tf\nimport pyhf\nfrom pyhf.simplemodels import hepdata_like\n\n\ndef test_astensor_dtype(backend, caplog):\n    tb = pyhf.tensorlib\n    with caplog.at_level(logging.INFO, \'pyhf.tensor\'):\n        with pytest.raises(KeyError):\n            assert tb.astensor([1, 2, 3], dtype=\'long\')\n            assert \'Invalid dtype\' in caplog.text\n\n\ndef test_simple_tensor_ops(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.astensor([1, 2, 3]) + tb.astensor([4, 5, 6])) == [5, 7, 9]\n    assert tb.tolist(tb.astensor([1]) + tb.astensor([4, 5, 6])) == [5, 6, 7]\n    assert tb.tolist(tb.astensor([1, 2, 3]) - tb.astensor([4, 5, 6])) == [-3, -3, -3]\n    assert tb.tolist(tb.astensor([4, 5, 6]) - tb.astensor([1])) == [3, 4, 5]\n    assert tb.tolist(tb.sum(tb.astensor([[1, 2, 3], [4, 5, 6]]), axis=0)) == [5, 7, 9]\n    assert tb.tolist(tb.product(tb.astensor([[1, 2, 3], [4, 5, 6]]), axis=0)) == [\n        4,\n        10,\n        18,\n    ]\n    assert tb.tolist(tb.power(tb.astensor([1, 2, 3]), tb.astensor([1, 2, 3]))) == [\n        1,\n        4,\n        27,\n    ]\n    assert tb.tolist(tb.divide(tb.astensor([4, 9, 16]), tb.astensor([2, 3, 4]))) == [\n        2,\n        3,\n        4,\n    ]\n    assert tb.tolist(tb.sqrt(tb.astensor([4, 9, 16]))) == [2, 3, 4]\n    assert tb.tolist(tb.log(tb.exp(tb.astensor([2, 3, 4])))) == [2, 3, 4]\n    assert tb.tolist(tb.abs(tb.astensor([-1, -2]))) == [1, 2]\n    a = tb.astensor(1)\n    b = tb.astensor(2)\n    assert tb.tolist(a < b)[0] is True\n    assert tb.tolist(b < a)[0] is False\n    assert tb.tolist(a < a)[0] is False\n    assert tb.tolist(a > b)[0] is False\n    assert tb.tolist(b > a)[0] is True\n    assert tb.tolist(a > a)[0] is False\n    a = tb.astensor(4)\n    b = tb.astensor(5)\n    assert tb.tolist(tb.conditional((a < b)[0], lambda: a + b, lambda: a - b)) == [9]\n    assert tb.tolist(tb.conditional((a > b)[0], lambda: a + b, lambda: a - b)) == [-1]\n\n\ndef test_complex_tensor_ops(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.outer(tb.astensor([1, 2, 3]), tb.astensor([4, 5, 6]))) == [\n        [4, 5, 6],\n        [8, 10, 12],\n        [12, 15, 18],\n    ]\n    assert tb.tolist(tb.stack([tb.astensor([1, 2, 3]), tb.astensor([4, 5, 6])])) == [\n        [1, 2, 3],\n        [4, 5, 6],\n    ]\n    assert tb.tolist(\n        tb.concatenate([tb.astensor([1, 2, 3]), tb.astensor([4, 5, 6])])\n    ) == [1, 2, 3, 4, 5, 6]\n    assert tb.tolist(tb.clip(tb.astensor([-2, -1, 0, 1, 2]), -1, 1)) == [\n        -1,\n        -1,\n        0,\n        1,\n        1,\n    ]\n    assert tb.tolist(\n        tb.where(\n            tb.astensor([1, 0, 1], dtype=""bool""),\n            tb.astensor([1, 1, 1]),\n            tb.astensor([2, 2, 2]),\n        )\n    ) == [1, 2, 1]\n\n\ndef test_ones(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.ones((2, 3))) == [[1, 1, 1], [1, 1, 1]]\n    assert tb.tolist(tb.ones((4, 5))) == [[1.0] * 5] * 4\n\n\ndef test_normal(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(\n        tb.normal_logpdf(tb.astensor([0]), tb.astensor([0]), tb.astensor([1]))\n    ) == pytest.approx([-0.9189385332046727], 1e-07)\n\n\ndef test_zeros(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.zeros((4, 5))) == [[0.0] * 5] * 4\n\n\ndef test_broadcasting(backend):\n    tb = pyhf.tensorlib\n    assert list(\n        map(\n            tb.tolist,\n            tb.simple_broadcast(\n                tb.astensor([1, 1, 1]), tb.astensor([2]), tb.astensor([3, 3, 3])\n            ),\n        )\n    ) == [[1, 1, 1], [2, 2, 2], [3, 3, 3]]\n    assert list(\n        map(\n            tb.tolist,\n            tb.simple_broadcast(\n                tb.astensor(1), tb.astensor([2, 3, 4]), tb.astensor([5, 6, 7])\n            ),\n        )\n    ) == [[1, 1, 1], [2, 3, 4], [5, 6, 7]]\n    assert list(\n        map(\n            tb.tolist,\n            tb.simple_broadcast(\n                tb.astensor([1]), tb.astensor([2, 3, 4]), tb.astensor([5, 6, 7])\n            ),\n        )\n    ) == [[1, 1, 1], [2, 3, 4], [5, 6, 7]]\n    with pytest.raises(Exception):\n        tb.simple_broadcast(\n            tb.astensor([1]), tb.astensor([2, 3]), tb.astensor([5, 6, 7])\n        )\n\n\ndef test_reshape(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.reshape(tb.ones((1, 2, 3)), (-1,))) == [1, 1, 1, 1, 1, 1]\n\n\ndef test_shape(backend):\n    tb = pyhf.tensorlib\n    assert tb.shape(tb.ones((1, 2, 3, 4, 5))) == (1, 2, 3, 4, 5)\n    assert tb.shape(tb.ones((0, 0))) == (0, 0)\n    assert tb.shape(tb.astensor([])) == (0,)\n    assert tb.shape(tb.astensor([1.0])) == (1,)\n    assert tb.shape(tb.astensor(1.0)) == tb.shape(tb.astensor([1.0]))\n    assert tb.shape(tb.astensor(0.0)) == tb.shape(tb.astensor([0.0]))\n    assert tb.shape(tb.astensor((1.0, 1.0))) == tb.shape(tb.astensor([1.0, 1.0]))\n    assert tb.shape(tb.astensor((0.0, 0.0))) == tb.shape(tb.astensor([0.0, 0.0]))\n    with pytest.raises(\n        (ValueError, RuntimeError, tf.errors.InvalidArgumentError, TypeError)\n    ):\n        _ = tb.astensor([1, 2]) + tb.astensor([3, 4, 5])\n    with pytest.raises(\n        (ValueError, RuntimeError, tf.errors.InvalidArgumentError, TypeError)\n    ):\n        _ = tb.astensor([1, 2]) - tb.astensor([3, 4, 5])\n    with pytest.raises(\n        (ValueError, RuntimeError, tf.errors.InvalidArgumentError, TypeError)\n    ):\n        _ = tb.astensor([1, 2]) < tb.astensor([3, 4, 5])\n    with pytest.raises(\n        (ValueError, RuntimeError, tf.errors.InvalidArgumentError, TypeError)\n    ):\n        _ = tb.astensor([1, 2]) > tb.astensor([3, 4, 5])\n    with pytest.raises((ValueError, RuntimeError, TypeError)):\n        tb.conditional(\n            (tb.astensor([1, 2]) < tb.astensor([3, 4])),\n            lambda: tb.astensor(4) + tb.astensor(5),\n            lambda: tb.astensor(4) - tb.astensor(5),\n        )\n\n\ndef test_pdf_calculations(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.normal_cdf(tb.astensor([0.8]))) == pytest.approx(\n        [0.7881446014166034], 1e-07\n    )\n    assert tb.tolist(\n        tb.normal_logpdf(\n            tb.astensor([0, 0, 1, 1, 0, 0, 1, 1]),\n            tb.astensor([0, 1, 0, 1, 0, 1, 0, 1]),\n            tb.astensor([0, 0, 0, 0, 1, 1, 1, 1]),\n        )\n    ) == pytest.approx(\n        [\n            np.nan,\n            np.nan,\n            np.nan,\n            np.nan,\n            -0.91893853,\n            -1.41893853,\n            -1.41893853,\n            -0.91893853,\n        ],\n        nan_ok=True,\n    )\n    # poisson(lambda=0) is not defined, should return NaN\n    assert tb.tolist(\n        tb.poisson(tb.astensor([0, 0, 1, 1]), tb.astensor([0, 1, 0, 1]))\n    ) == pytest.approx(\n        [np.nan, 0.3678794503211975, 0.0, 0.3678794503211975], nan_ok=True\n    )\n    assert tb.tolist(\n        tb.poisson_logpdf(tb.astensor([0, 0, 1, 1]), tb.astensor([0, 1, 0, 1]))\n    ) == pytest.approx(\n        np.log([np.nan, 0.3678794503211975, 0.0, 0.3678794503211975]).tolist(),\n        nan_ok=True,\n    )\n\n\ndef test_boolean_mask(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(\n        tb.boolean_mask(\n            tb.astensor([1, 2, 3, 4, 5, 6]),\n            tb.astensor([True, True, False, True, False, False], dtype=\'bool\'),\n        )\n    ) == [1, 2, 4]\n    assert tb.tolist(\n        tb.boolean_mask(\n            tb.astensor([[1, 2], [3, 4], [5, 6]]),\n            tb.astensor([[True, True], [False, True], [False, False]], dtype=\'bool\'),\n        )\n    ) == [1, 2, 4]\n\n\ndef test_tensor_tile(backend):\n    a = [[1], [2], [3]]\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.tile(tb.astensor(a), (1, 2))) == [[1, 1], [2, 2], [3, 3]]\n\n    a = [1, 2, 3]\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.tile(tb.astensor(a), (2,))) == [1, 2, 3, 1, 2, 3]\n\n\ndef test_1D_gather(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(\n        tb.gather(\n            tb.astensor([1, 2, 3, 4, 5, 6]), tb.astensor([4, 0, 3, 2], dtype=\'int\')\n        )\n    ) == [5, 1, 4, 3]\n    assert tb.tolist(\n        tb.gather(\n            tb.astensor([1, 2, 3, 4, 5, 6]), tb.astensor([[4, 0], [3, 2]], dtype=\'int\')\n        )\n    ) == [[5, 1], [4, 3]]\n\n\n@pytest.mark.fail_pytorch\ndef test_ND_gather(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(\n        tb.gather(\n            tb.astensor([[1, 2], [3, 4], [5, 6]]), tb.astensor([1, 0], dtype=\'int\')\n        )\n    ) == [[3, 4], [1, 2]]\n\n\ndef test_isfinite(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.isfinite(tb.astensor([1.0, float(""nan""), float(""inf"")]))) == [\n        True,\n        False,\n        False,\n    ]\n\n\ndef test_einsum(backend):\n    tb = pyhf.tensorlib\n    x = np.arange(20).reshape(5, 4).tolist()\n\n    assert np.all(\n        tb.tolist(tb.einsum(\'ij->ji\', tb.astensor(x))) == np.asarray(x).T.tolist()\n    )\n    assert (\n        tb.tolist(tb.einsum(\'i,j->ij\', tb.astensor([1, 1, 1]), tb.astensor([1, 2, 3])))\n        == [[1, 2, 3]] * 3\n    )\n\n\ndef test_list_to_list(backend):\n    tb = pyhf.tensorlib\n    # test when no other tensor operations are done\n    assert tb.tolist([1, 2, 3, 4]) == [1, 2, 3, 4]\n    assert tb.tolist([[1], [2], [3], [4]]) == [[1], [2], [3], [4]]\n    assert tb.tolist([[1, 2], 3, [4]]) == [[1, 2], 3, [4]]\n\n\ndef test_tensor_to_list(backend):\n    tb = pyhf.tensorlib\n    assert tb.tolist(tb.astensor([1, 2, 3, 4])) == [1, 2, 3, 4]\n    assert tb.tolist(tb.astensor([[1], [2], [3], [4]])) == [[1], [2], [3], [4]]\n\n\n@pytest.mark.only_tensorflow\ndef test_tensor_list_conversion(backend):\n    tb = pyhf.tensorlib\n    # test when a tensor operation is done, but then need to check if this\n    # doesn\'t break in session.run\n    assert tb.tolist(tb.astensor([1, 2, 3, 4])) == [1, 2, 3, 4]\n    assert tb.tolist([1, 2, 3, 4]) == [1, 2, 3, 4]\n\n\ndef test_pdf_eval(backend):\n    source = {\n        ""binning"": [2, -0.5, 1.5],\n        ""bindata"": {\n            ""data"": [120.0, 180.0],\n            ""bkg"": [100.0, 150.0],\n            ""bkgsys_up"": [102, 190],\n            ""bkgsys_dn"": [98, 100],\n            ""sig"": [30.0, 95.0],\n        },\n    }\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'bkg_norm\',\n                                \'type\': \'histosys\',\n                                \'data\': {\n                                    \'lo_data\': source[\'bindata\'][\'bkgsys_dn\'],\n                                    \'hi_data\': source[\'bindata\'][\'bkgsys_up\'],\n                                },\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    pdf = pyhf.Model(spec)\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n    assert pytest.approx([-17.648827643136507], rel=5e-5) == pyhf.tensorlib.tolist(\n        pdf.logpdf(pdf.config.suggested_init(), data)\n    )\n\n\ndef test_pdf_eval_2(backend):\n    source = {\n        ""binning"": [2, -0.5, 1.5],\n        ""bindata"": {\n            ""data"": [120.0, 180.0],\n            ""bkg"": [100.0, 150.0],\n            ""bkgerr"": [10.0, 10.0],\n            ""sig"": [30.0, 95.0],\n        },\n    }\n\n    pdf = hepdata_like(\n        source[\'bindata\'][\'sig\'], source[\'bindata\'][\'bkg\'], source[\'bindata\'][\'bkgerr\']\n    )\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    assert pytest.approx([-23.579605171119738], rel=5e-5) == pyhf.tensorlib.tolist(\n        pdf.logpdf(pdf.config.suggested_init(), data)\n    )\n'"
tests/test_tensorviewer.py,0,"b""from pyhf.tensor.common import _TensorViewer\n\n\ndef test_tensorviewer(backend):\n    tb, _ = backend\n    tv = _TensorViewer(\n        [tb.astensor([0, 4, 5]), tb.astensor([1, 2, 3]), tb.astensor([6]),],\n        names=['zzz', 'aaa', 'x'],\n    )\n\n    data = tb.astensor(tb.astensor(list(range(7))) * 10, dtype='int')\n\n    a = [tb.tolist(x) for x in tv.split(data, selection=['aaa'])]\n    assert a == [[10, 20, 30]]\n\n    a = [tb.tolist(x) for x in tv.split(data, selection=['aaa', 'zzz'])]\n    assert a == [[10, 20, 30], [0, 40, 50]]\n\n    a = [tb.tolist(x) for x in tv.split(data, selection=['zzz', 'aaa'])]\n    assert a == [[0, 40, 50], [10, 20, 30]]\n\n    a = [tb.tolist(x) for x in tv.split(data, selection=['x', 'aaa'])]\n    assert a == [[60], [10, 20, 30]]\n\n    a = [tb.tolist(x) for x in tv.split(data, selection=[])]\n    assert a == []\n\n    a = [tb.tolist(x) for x in tv.split(data)]\n    assert a == [[0, 40, 50], [10, 20, 30], [60]]\n\n    subviewer = _TensorViewer(\n        [tb.astensor([0]), tb.astensor([1, 2, 3]),], names=['x', 'aaa']\n    )\n    assert tb.tolist(subviewer.stitch(tv.split(data, ['x', 'aaa']))) == [60, 10, 20, 30]\n\n    subviewer = _TensorViewer(\n        [tb.astensor([0, 1, 2]), tb.astensor([3]),], names=['aaa', 'x']\n    )\n    assert tb.tolist(subviewer.stitch(tv.split(data, ['aaa', 'x']))) == [10, 20, 30, 60]\n"""
tests/test_toys.py,2,"b'import pyhf\nimport numpy as np\n\n\ndef test_smoketest_toys(backend):\n    tb, _ = backend\n    m = pyhf.simplemodels.hepdata_like([6], [9], [3])\n    s = m.make_pdf(pyhf.tensorlib.astensor(m.config.suggested_init()))\n    assert np.asarray(tb.tolist(s.log_prob(s.sample((1000,))))).shape == (1000,)\n\n    tb, _ = backend\n    m = pyhf.simplemodels.hepdata_like([6, 6], [9, 9], [3, 3], batch_size=13)\n    s = m.make_pdf(pyhf.tensorlib.astensor(m.batch_size * [m.config.suggested_init()]))\n    assert np.asarray(tb.tolist(s.sample((10,)))).shape == (10, 13, 4)\n'"
tests/test_utils.py,0,"b""import pytest\nimport pyhf\n\n\n@pytest.mark.parametrize(\n    'schema', ['defs.json', 'measurement.json', 'model.json', 'workspace.json']\n)\ndef test_get_schema(schema):\n    assert pyhf.utils.load_schema(schema)\n\n\ndef test_load_missing_schema():\n    with pytest.raises(IOError):\n        pyhf.utils.load_schema('fake_schema.json')\n\n\n@pytest.mark.parametrize(\n    'opts,obj',\n    [\n        (['a=10'], {'a': 10}),\n        (['b=test'], {'b': 'test'}),\n        (['c=1.0e-8'], {'c': 1.0e-8}),\n        (['d=3.14'], {'d': 3.14}),\n        (['e=True'], {'e': True}),\n        (['f=false'], {'f': False}),\n        (['a=b', 'c=d'], {'a': 'b', 'c': 'd'}),\n        (['g=h=i'], {'g': 'h=i'}),\n    ],\n)\ndef test_options_from_eqdelimstring(opts, obj):\n    assert pyhf.utils.options_from_eqdelimstring(opts) == obj\n\n\n@pytest.mark.parametrize(\n    'obj',\n    [\n        {'a': 2.0, 'b': 1.0, 'c': 'a'},\n        {'b': 1.0, 'c': 'a', 'a': 2.0},\n        {'c': 'a', 'a': 2.0, 'b': 1.0},\n    ],\n)\n@pytest.mark.parametrize('algorithm', ['md5', 'sha256'])\ndef test_digest(obj, algorithm):\n    results = {\n        'md5': '155e52b05179a1106d71e5e053452517',\n        'sha256': '03dfbceade79855fc9b4e4d6fbd4f437109de68330dab37c3091a15f4bffe593',\n    }\n    assert pyhf.utils.digest(obj, algorithm=algorithm) == results[algorithm]\n\n\ndef test_digest_bad_obj():\n    with pytest.raises(ValueError) as excinfo:\n        pyhf.utils.digest(object())\n    assert 'not JSON-serializable' in str(excinfo.value)\n\n\ndef test_digest_bad_alg():\n    with pytest.raises(ValueError) as excinfo:\n        pyhf.utils.digest({}, algorithm='nonexistent_algorithm')\n    assert 'nonexistent_algorithm' in str(excinfo.value)\n"""
tests/test_validation.py,2,"b'import pyhf\nimport pyhf.writexml\nimport pyhf.readxml\nimport json\nimport pytest\nfrom pathlib import Path\nimport numpy as np\n\n\n@pytest.fixture(scope=\'module\')\ndef source_1bin_example1():\n    with open(\'validation/data/1bin_example1.json\') as read_json:\n        return json.load(read_json)\n\n\n@pytest.fixture(scope=\'module\')\ndef spec_1bin_shapesys(source=source_1bin_example1()):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'uncorr_bkguncrt\',\n                                \'type\': \'shapesys\',\n                                \'data\': source[\'bindata\'][\'bkgerr\'],\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    return spec\n\n\n@pytest.fixture(scope=\'module\')\ndef expected_result_1bin_shapesys(mu=1.0):\n    if mu == 1:\n        expected_result = {\n            ""exp"": [\n                0.06371799398864626,\n                0.15096503398048894,\n                0.3279606950533305,\n                0.6046087303039118,\n                0.8662627605298466,\n            ],\n            ""obs"": 0.4541865416107029,\n        }\n    return expected_result\n\n\n@pytest.fixture(scope=\'module\')\ndef setup_1bin_shapesys(\n    source=source_1bin_example1(),\n    spec=spec_1bin_shapesys(source_1bin_example1()),\n    mu=1,\n    expected_result=expected_result_1bin_shapesys(1.0),\n    config={\'init_pars\': 2, \'par_bounds\': 2},\n):\n    return {\n        \'source\': source,\n        \'spec\': spec,\n        \'mu\': mu,\n        \'expected\': {\'result\': expected_result, \'config\': config},\n    }\n\n\n@pytest.fixture(scope=\'module\')\ndef spec_1bin_lumi():\n    spec = {\n        ""channels"": [\n            {\n                ""name"": ""channel1"",\n                ""samples"": [\n                    {\n                        ""data"": [20.0],\n                        ""modifiers"": [\n                            {""data"": None, ""name"": ""mu"", ""type"": ""normfactor""}\n                        ],\n                        ""name"": ""signal"",\n                    },\n                    {\n                        ""data"": [100.0],\n                        ""modifiers"": [{""data"": None, ""name"": ""lumi"", ""type"": ""lumi""}],\n                        ""name"": ""background1"",\n                    },\n                    {\n                        ""data"": [0.0],\n                        ""modifiers"": [{""data"": None, ""name"": ""lumi"", ""type"": ""lumi""}],\n                        ""name"": ""background2"",\n                    },\n                ],\n            }\n        ],\n        ""parameters"": [\n            {\n                ""auxdata"": [1.0],\n                ""bounds"": [[0.0, 10.0]],\n                ""inits"": [1.0],\n                ""name"": ""lumi"",\n                ""sigmas"": [0.1],\n            }\n        ],\n    }\n    return spec\n\n\n@pytest.fixture(scope=\'module\')\ndef expected_result_1bin_lumi(mu=1.0):\n    if mu == 1:\n        expected_result = {\n            ""exp"": [0.01060338, 0.04022273, 0.13614217, 0.37078321, 0.71104119],\n            ""obs"": 0.01047275,\n        }\n    return expected_result\n\n\n@pytest.fixture(scope=\'module\')\ndef setup_1bin_lumi(\n    source=source_1bin_example1(),\n    spec=spec_1bin_lumi(),\n    mu=1,\n    expected_result=expected_result_1bin_lumi(1.0),\n    config={\'init_pars\': 2, \'par_bounds\': 2},\n):\n    return {\n        \'source\': source,\n        \'spec\': spec,\n        \'mu\': mu,\n        \'expected\': {\'result\': expected_result, \'config\': config},\n    }\n\n\n@pytest.fixture(scope=\'module\')\ndef source_1bin_normsys():\n    source = {\n        \'binning\': [2, -0.5, 1.5],\n        \'bindata\': {\'data\': [120.0, 180.0], \'bkg\': [100.0, 150.0], \'sig\': [30.0, 95.0]},\n    }\n    return source\n\n\n@pytest.fixture(scope=\'module\')\ndef spec_1bin_normsys(source=source_1bin_normsys()):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'bkg_norm\',\n                                \'type\': \'normsys\',\n                                \'data\': {\'lo\': 0.90, \'hi\': 1.10},\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    return spec\n\n\n@pytest.fixture(scope=\'module\')\ndef expected_result_1bin_normsys(mu=1.0):\n    if mu == 1:\n        expected_result = {\n            ""exp"": [\n                7.471684419037561e-10,\n                5.7411551509088054e-08,\n                3.6898088062731205e-06,\n                0.00016965731538267896,\n                0.004392708998555453,\n            ],\n            ""obs"": 0.0006735317023683173,\n        }\n    return expected_result\n\n\n@pytest.fixture(scope=\'module\')\ndef setup_1bin_normsys(\n    source=source_1bin_normsys(),\n    spec=spec_1bin_normsys(source_1bin_normsys()),\n    mu=1,\n    expected_result=expected_result_1bin_normsys(1.0),\n    config={\'init_pars\': 2, \'par_bounds\': 2},\n):\n    return {\n        \'source\': source,\n        \'spec\': spec,\n        \'mu\': mu,\n        \'expected\': {\'result\': expected_result, \'config\': config},\n    }\n\n\n@pytest.fixture(scope=\'module\')\ndef source_2bin_histosys_example2():\n    with open(\'validation/data/2bin_histosys_example2.json\') as read_json:\n        return json.load(read_json)\n\n\n@pytest.fixture(scope=\'module\')\ndef spec_2bin_histosys(source=source_2bin_histosys_example2()):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'singlechannel\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'bkg_norm\',\n                                \'type\': \'histosys\',\n                                \'data\': {\n                                    \'lo_data\': source[\'bindata\'][\'bkgsys_dn\'],\n                                    \'hi_data\': source[\'bindata\'][\'bkgsys_up\'],\n                                },\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    return spec\n\n\n@pytest.fixture(scope=\'module\')\ndef expected_result_2bin_histosys(mu=1):\n    if mu == 1:\n        expected_result = {\n            ""exp"": [\n                7.134513306138892e-06,\n                0.00012547100627138575,\n                0.001880010666437615,\n                0.02078964907605385,\n                0.13692494523572218,\n            ],\n            ""obs"": 0.1001463460725534,\n        }\n    return expected_result\n\n\n@pytest.fixture(scope=\'module\')\ndef setup_2bin_histosys(\n    source=source_2bin_histosys_example2(),\n    spec=spec_2bin_histosys(source_2bin_histosys_example2()),\n    mu=1,\n    expected_result=expected_result_2bin_histosys(1.0),\n    config={\'init_pars\': 2, \'par_bounds\': 2},\n):\n    return {\n        \'source\': source,\n        \'spec\': spec,\n        \'mu\': mu,\n        \'expected\': {\'result\': expected_result, \'config\': config},\n    }\n\n\n@pytest.fixture(scope=\'module\')\ndef source_2bin_2channel_example1():\n    with open(\'validation/data/2bin_2channel_example1.json\') as read_json:\n        return json.load(read_json)\n\n\n@pytest.fixture(scope=\'module\')\ndef spec_2bin_2channel(source=source_2bin_2channel_example1()):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'signal\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'uncorr_bkguncrt_signal\',\n                                \'type\': \'shapesys\',\n                                \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\n                                    \'bkgerr\'\n                                ],\n                            }\n                        ],\n                    },\n                ],\n            },\n            {\n                \'name\': \'control\',\n                \'samples\': [\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'channels\'][\'control\'][\'bindata\'][\'bkg\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'uncorr_bkguncrt_control\',\n                                \'type\': \'shapesys\',\n                                \'data\': source[\'channels\'][\'control\'][\'bindata\'][\n                                    \'bkgerr\'\n                                ],\n                            }\n                        ],\n                    }\n                ],\n            },\n        ]\n    }\n    return spec\n\n\n@pytest.fixture(scope=\'module\')\ndef expected_result_2bin_2channel(mu=1.0):\n    if mu == 1:\n        expected_result = {\n            ""exp"": [\n                0.00043491354821983556,\n                0.0034223000502860606,\n                0.02337423265831151,\n                0.1218654225510158,\n                0.40382074249477845,\n            ],\n            ""obs"": 0.056332621064982304,\n        }\n    return expected_result\n\n\n@pytest.fixture(scope=\'module\')\ndef setup_2bin_2channel(\n    source=source_2bin_2channel_example1(),\n    spec=spec_2bin_2channel(source_2bin_2channel_example1()),\n    mu=1,\n    expected_result=expected_result_2bin_2channel(1.0),\n    config={\'init_pars\': 5, \'par_bounds\': 5},\n):\n    # 1 mu + 2 gammas for 2 channels each\n    return {\n        \'source\': source,\n        \'spec\': spec,\n        \'mu\': mu,\n        \'expected\': {\'result\': expected_result, \'config\': config},\n    }\n\n\n@pytest.fixture(scope=\'module\')\ndef source_2bin_2channel_couplednorm():\n    with open(\'validation/data/2bin_2channel_couplednorm.json\') as read_json:\n        return json.load(read_json)\n\n\n@pytest.fixture(scope=\'module\')\ndef spec_2bin_2channel_couplednorm(source=source_2bin_2channel_couplednorm()):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'signal\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg1\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'bkg1\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'coupled_normsys\',\n                                \'type\': \'normsys\',\n                                \'data\': {\'lo\': 0.9, \'hi\': 1.1},\n                            }\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg2\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'bkg2\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'coupled_normsys\',\n                                \'type\': \'normsys\',\n                                \'data\': {\'lo\': 0.5, \'hi\': 1.5},\n                            }\n                        ],\n                    },\n                ],\n            },\n            {\n                \'name\': \'control\',\n                \'samples\': [\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'channels\'][\'control\'][\'bindata\'][\'bkg1\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'coupled_normsys\',\n                                \'type\': \'normsys\',\n                                \'data\': {\'lo\': 0.9, \'hi\': 1.1},\n                            }\n                        ],\n                    }\n                ],\n            },\n        ]\n    }\n    return spec\n\n\n@pytest.fixture(scope=\'module\')\ndef expected_result_2bin_2channel_couplednorm(mu=1.0):\n    if mu == 1:\n        expected_result = {\n            ""exp"": [\n                0.055223914655538435,\n                0.13613239925395315,\n                0.3068720101493323,\n                0.5839470093910164,\n                0.8554725461337025,\n            ],\n            ""obs"": 0.5906228034705155,\n        }\n    return expected_result\n\n\n@pytest.fixture(scope=\'module\')\ndef setup_2bin_2channel_couplednorm(\n    source=source_2bin_2channel_couplednorm(),\n    spec=spec_2bin_2channel_couplednorm(source_2bin_2channel_couplednorm()),\n    mu=1,\n    expected_result=expected_result_2bin_2channel_couplednorm(1.0),\n    config={\'init_pars\': 2, \'par_bounds\': 2},\n):\n    # 1 mu + 1 alpha\n    return {\n        \'source\': source,\n        \'spec\': spec,\n        \'mu\': mu,\n        \'expected\': {\'result\': expected_result, \'config\': config},\n    }\n\n\n@pytest.fixture(scope=\'module\')\ndef source_2bin_2channel_coupledhisto():\n    with open(\'validation/data/2bin_2channel_coupledhisto.json\') as read_json:\n        return json.load(read_json)\n\n\n@pytest.fixture(scope=\'module\')\ndef spec_2bin_2channel_coupledhistosys(source=source_2bin_2channel_coupledhisto()):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'signal\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg1\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'bkg1\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'coupled_histosys\',\n                                \'type\': \'histosys\',\n                                \'data\': {\n                                    \'lo_data\': source[\'channels\'][\'signal\'][\'bindata\'][\n                                        \'bkg1_dn\'\n                                    ],\n                                    \'hi_data\': source[\'channels\'][\'signal\'][\'bindata\'][\n                                        \'bkg1_up\'\n                                    ],\n                                },\n                            }\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg2\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'bkg2\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'coupled_histosys\',\n                                \'type\': \'histosys\',\n                                \'data\': {\n                                    \'lo_data\': source[\'channels\'][\'signal\'][\'bindata\'][\n                                        \'bkg2_dn\'\n                                    ],\n                                    \'hi_data\': source[\'channels\'][\'signal\'][\'bindata\'][\n                                        \'bkg2_up\'\n                                    ],\n                                },\n                            }\n                        ],\n                    },\n                ],\n            },\n            {\n                \'name\': \'control\',\n                \'samples\': [\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'channels\'][\'control\'][\'bindata\'][\'bkg1\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'coupled_histosys\',\n                                \'type\': \'histosys\',\n                                \'data\': {\n                                    \'lo_data\': source[\'channels\'][\'control\'][\'bindata\'][\n                                        \'bkg1_dn\'\n                                    ],\n                                    \'hi_data\': source[\'channels\'][\'control\'][\'bindata\'][\n                                        \'bkg1_up\'\n                                    ],\n                                },\n                            }\n                        ],\n                    }\n                ],\n            },\n        ]\n    }\n    return spec\n\n\n@pytest.fixture(scope=\'module\')\ndef expected_result_2bin_2channel_coupledhistosys(mu=1.0):\n    if mu == 1:\n        expected_result = {\n            ""exp"": [\n                1.7653746536962154e-05,\n                0.00026265644807799805,\n                0.00334003612780065,\n                0.031522353024659715,\n                0.17907742915143962,\n            ],\n            ""obs"": 0.07967400132261188,\n        }\n    return expected_result\n\n\n@pytest.fixture(scope=\'module\')\ndef setup_2bin_2channel_coupledhistosys(\n    source=source_2bin_2channel_coupledhisto(),\n    spec=spec_2bin_2channel_coupledhistosys(source_2bin_2channel_coupledhisto()),\n    mu=1,\n    expected_result=expected_result_2bin_2channel_coupledhistosys(1.0),\n    config={\'auxdata\': 1, \'init_pars\': 2, \'par_bounds\': 2},\n):\n    # 1 mu 1 shared histosys\n    return {\n        \'source\': source,\n        \'spec\': spec,\n        \'mu\': mu,\n        \'expected\': {\'result\': expected_result, \'config\': config},\n    }\n\n\n@pytest.fixture(scope=\'module\')\ndef source_2bin_2channel_coupledshapefactor():\n    with open(\'validation/data/2bin_2channel_coupledshapefactor.json\') as read_json:\n        return json.load(read_json)\n\n\n@pytest.fixture(scope=\'module\')\ndef spec_2bin_2channel_coupledshapefactor(\n    source=source_2bin_2channel_coupledshapefactor(),\n):\n    spec = {\n        \'channels\': [\n            {\n                \'name\': \'signal\',\n                \'samples\': [\n                    {\n                        \'name\': \'signal\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'sig\'],\n                        \'modifiers\': [\n                            {\'name\': \'mu\', \'type\': \'normfactor\', \'data\': None}\n                        ],\n                    },\n                    {\n                        \'name\': \'bkg1\',\n                        \'data\': source[\'channels\'][\'signal\'][\'bindata\'][\'bkg1\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'coupled_shapefactor\',\n                                \'type\': \'shapefactor\',\n                                \'data\': None,\n                            }\n                        ],\n                    },\n                ],\n            },\n            {\n                \'name\': \'control\',\n                \'samples\': [\n                    {\n                        \'name\': \'background\',\n                        \'data\': source[\'channels\'][\'control\'][\'bindata\'][\'bkg1\'],\n                        \'modifiers\': [\n                            {\n                                \'name\': \'coupled_shapefactor\',\n                                \'type\': \'shapefactor\',\n                                \'data\': None,\n                            }\n                        ],\n                    }\n                ],\n            },\n        ]\n    }\n    return spec\n\n\n@pytest.fixture(scope=\'module\')\ndef expected_result_2bin_2channel_coupledshapefactor(mu=1.0):\n    if mu == 1:\n        expected_result = {\n            \'obs\': 0.5421679124909312,\n            \'exp\': [\n                0.013753299929451691,\n                0.048887400056355966,\n                0.15555296253957684,\n                0.4007561343326305,\n                0.7357169630955912,\n            ],\n        }\n    return expected_result\n\n\n@pytest.fixture(scope=\'module\')\ndef setup_2bin_2channel_coupledshapefactor(\n    source=source_2bin_2channel_coupledshapefactor(),\n    spec=spec_2bin_2channel_coupledshapefactor(\n        source_2bin_2channel_coupledshapefactor()\n    ),\n    mu=1,\n    expected_result=expected_result_2bin_2channel_coupledshapefactor(1.0),\n    config={\'auxdata\': 0, \'init_pars\': 3, \'par_bounds\': 3},\n):\n    # 1 mu 2 shared shapefactors\n    return {\n        \'source\': source,\n        \'spec\': spec,\n        \'mu\': mu,\n        \'expected\': {\'result\': expected_result, \'config\': config},\n    }\n\n\ndef validate_hypotest(pdf, data, mu_test, expected_result, tolerance=1e-6):\n    init_pars = pdf.config.suggested_init()\n    par_bounds = pdf.config.suggested_bounds()\n\n    CLs_obs, CLs_exp_set = pyhf.infer.hypotest(\n        mu_test,\n        data,\n        pdf,\n        init_pars,\n        par_bounds,\n        return_expected_set=True,\n        qtilde=False,\n    )\n    assert abs(CLs_obs - expected_result[\'obs\']) / expected_result[\'obs\'] < tolerance\n    for result, expected in zip(CLs_exp_set, expected_result[\'exp\']):\n        assert abs(result - expected) / expected < tolerance\n\n\n@pytest.mark.parametrize(\n    \'setup_and_tolerance\',\n    [\n        (setup_1bin_shapesys(), 1e-6),\n        (setup_1bin_lumi(), 4e-6),\n        (setup_1bin_normsys(), 1e-6),\n        (setup_2bin_histosys(), 8e-5),\n        (setup_2bin_2channel(), 1e-6),\n        (setup_2bin_2channel_couplednorm(), 1e-6),\n        (setup_2bin_2channel_coupledhistosys(), 1e-6),\n        (setup_2bin_2channel_coupledshapefactor(), 2.5e-6),\n    ],\n    ids=[\n        \'1bin_shapesys_mu1\',\n        \'1bin_lumi_mu1\',\n        \'1bin_normsys_mu1\',\n        \'2bin_histosys_mu1\',\n        \'2bin_2channel_mu1\',\n        \'2bin_2channel_couplednorm_mu1\',\n        \'2bin_2channel_coupledhistosys_mu1\',\n        \'2bin_2channel_coupledshapefactor_mu1\',\n    ],\n)\ndef test_validation(setup_and_tolerance):\n    setup, tolerance = setup_and_tolerance\n    source = setup[\'source\']\n\n    pdf = pyhf.Model(setup[\'spec\'])\n\n    if \'channels\' in source:\n        data = []\n        for c in pdf.config.channels:\n            data += source[\'channels\'][c][\'bindata\'][\'data\']\n        data = data + pdf.config.auxdata\n    else:\n        data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n\n    if \'auxdata\' in setup[\'expected\'][\'config\']:\n        assert len(pdf.config.auxdata) == setup[\'expected\'][\'config\'][\'auxdata\']\n    assert len(pdf.config.suggested_init()) == setup[\'expected\'][\'config\'][\'init_pars\']\n    assert (\n        len(pdf.config.suggested_bounds()) == setup[\'expected\'][\'config\'][\'par_bounds\']\n    )\n\n    validate_hypotest(\n        pdf, data, setup[\'mu\'], setup[\'expected\'][\'result\'], tolerance=tolerance\n    )\n\n\n@pytest.mark.parametrize(\n    \'toplvl, basedir\',\n    [\n        (\n            \'validation/xmlimport_input/config/example.xml\',\n            \'validation/xmlimport_input/\',\n        ),\n        (\n            \'validation/xmlimport_input2/config/example.xml\',\n            \'validation/xmlimport_input2\',\n        ),\n        (\n            \'validation/xmlimport_input3/config/examples/example_ShapeSys.xml\',\n            \'validation/xmlimport_input3\',\n        ),\n    ],\n    ids=[\'example-one\', \'example-two\', \'example-three\'],\n)\ndef test_import_roundtrip(tmpdir, toplvl, basedir):\n    parsed_xml_before = pyhf.readxml.parse(toplvl, basedir)\n    spec = {\n        \'channels\': parsed_xml_before[\'channels\'],\n        \'parameters\': parsed_xml_before[\'measurements\'][0][\'config\'][\'parameters\'],\n    }\n    pdf_before = pyhf.Model(spec, poi_name=\'SigXsecOverSM\')\n\n    tmpconfig = tmpdir.mkdir(\'config\')\n    tmpdata = tmpdir.mkdir(\'data\')\n    tmpxml = tmpdir.join(\'FitConfig.xml\')\n    tmpxml.write(\n        pyhf.writexml.writexml(\n            parsed_xml_before,\n            tmpconfig.strpath,\n            tmpdata.strpath,\n            Path(tmpdir.strpath).joinpath(\'FitConfig\'),\n        ).decode(\'utf-8\')\n    )\n    parsed_xml_after = pyhf.readxml.parse(tmpxml.strpath, tmpdir.strpath)\n    spec = {\n        \'channels\': parsed_xml_after[\'channels\'],\n        \'parameters\': parsed_xml_after[\'measurements\'][0][\'config\'][\'parameters\'],\n    }\n    pdf_after = pyhf.Model(spec, poi_name=\'SigXsecOverSM\')\n\n    data_before = [\n        binvalue\n        for k in pdf_before.config.channels\n        for binvalue in next(\n            obs for obs in parsed_xml_before[\'observations\'] if obs[\'name\'] == k\n        )[\'data\']\n    ] + pdf_before.config.auxdata\n\n    data_after = [\n        binvalue\n        for k in pdf_after.config.channels\n        for binvalue in next(\n            obs for obs in parsed_xml_after[\'observations\'] if obs[\'name\'] == k\n        )[\'data\']\n    ] + pdf_after.config.auxdata\n\n    assert data_before == data_after\n\n    init_pars_before = pdf_before.config.suggested_init()\n    init_pars_after = pdf_after.config.suggested_init()\n    assert init_pars_before == init_pars_after\n\n    par_bounds_before = pdf_before.config.suggested_bounds()\n    par_bounds_after = pdf_after.config.suggested_bounds()\n    assert par_bounds_before == par_bounds_after\n\n    CLs_obs_before, CLs_exp_set_before = pyhf.infer.hypotest(\n        1,\n        data_before,\n        pdf_before,\n        init_pars_before,\n        par_bounds_before,\n        return_expected_set=True,\n    )\n    CLs_obs_after, CLs_exp_set_after = pyhf.infer.hypotest(\n        1,\n        data_after,\n        pdf_after,\n        init_pars_after,\n        par_bounds_after,\n        return_expected_set=True,\n    )\n\n    tolerance = 1e-6\n    assert abs(CLs_obs_after - CLs_obs_before) / CLs_obs_before < tolerance\n    for result, expected_result in zip(CLs_exp_set_after, CLs_exp_set_before):\n        assert abs(result - expected_result) / expected_result < tolerance\n\n\ndef test_shapesys_nuisparfilter_validation():\n    reference_root_results = {\n        ""CLs_exp"": [\n            2.702197937866914e-05,\n            0.00037099917612576155,\n            0.004360634386335687,\n            0.03815031509701916,\n            0.20203027564155074,\n        ],\n        ""CLs_obs"": 0.004360634405484502,\n    }\n    null = None\n    spec = {\n        ""channels"": [\n            {\n                ""name"": ""channel1"",\n                ""samples"": [\n                    {\n                        ""data"": [20, 10],\n                        ""modifiers"": [\n                            {\n                                ""data"": null,\n                                ""name"": ""SigXsecOverSM"",\n                                ""type"": ""normfactor"",\n                            }\n                        ],\n                        ""name"": ""signal"",\n                    },\n                    {\n                        ""data"": [100, 10],\n                        ""modifiers"": [\n                            {""data"": [10, 0], ""name"": ""syst"", ""type"": ""shapesys""}\n                        ],\n                        ""name"": ""background1"",\n                    },\n                ],\n            }\n        ],\n        ""measurements"": [\n            {\n                ""config"": {\n                    ""parameters"": [\n                        {\n                            ""auxdata"": [1],\n                            ""bounds"": [[0.5, 1.5]],\n                            ""inits"": [1],\n                            ""name"": ""lumi"",\n                            ""sigmas"": [0.1],\n                        }\n                    ],\n                    ""poi"": ""SigXsecOverSM"",\n                },\n                ""name"": ""GaussExample"",\n            }\n        ],\n        ""observations"": [{""data"": [100, 10], ""name"": ""channel1""}],\n        ""version"": ""1.0.0"",\n    }\n    w = pyhf.Workspace(spec)\n    m = w.model(\n        modifier_settings={\n            \'normsys\': {\'interpcode\': \'code4\'},\n            \'histosys\': {\'interpcode\': \'code4p\'},\n        },\n    )\n    d = w.data(m)\n    obs, exp = pyhf.infer.hypotest(1.0, d, m, return_expected_set=True)\n    pyhf_results = {\'CLs_obs\': obs[0], \'CLs_exp\': [e[0] for e in exp]}\n\n    assert np.allclose(\n        reference_root_results[\'CLs_obs\'], pyhf_results[\'CLs_obs\'], atol=1e-4, rtol=1e-5\n    )\n    assert np.allclose(\n        reference_root_results[\'CLs_exp\'], pyhf_results[\'CLs_exp\'], atol=1e-4, rtol=1e-5\n    )\n'"
tests/test_workspace.py,0,"b'import pyhf\nimport pyhf.readxml\nimport pytest\nimport pyhf.exceptions\nimport json\nimport logging\n\n\n@pytest.fixture(\n    scope=\'session\',\n    params=[\n        (\n            \'validation/xmlimport_input/config/example.xml\',\n            \'validation/xmlimport_input/\',\n        ),\n        (\n            \'validation/xmlimport_input2/config/example.xml\',\n            \'validation/xmlimport_input2\',\n        ),\n        (\n            \'validation/xmlimport_input3/config/examples/example_ShapeSys.xml\',\n            \'validation/xmlimport_input3\',\n        ),\n    ],\n    ids=[\'example-one\', \'example-two\', \'example-three\'],\n)\ndef workspace_xml(request):\n    return pyhf.readxml.parse(*request.param)\n\n\n@pytest.fixture(scope=\'function\')\ndef workspace_factory(workspace_xml):\n    return lambda: pyhf.Workspace(workspace_xml)\n\n\ndef test_build_workspace(workspace_factory):\n    w = workspace_factory()\n    assert w\n\n\ndef test_version_workspace(workspace_factory):\n    ws = workspace_factory()\n    assert ws.version is not None\n\n\ndef test_build_model(workspace_factory):\n    w = workspace_factory()\n    assert w.model()\n\n\ndef test_get_measurement_default(workspace_factory):\n    w = workspace_factory()\n    m = w.get_measurement()\n    assert m\n\n\ndef test_get_measurement(workspace_factory):\n    w = workspace_factory()\n    for measurement in w.measurement_names:\n        m = w.get_measurement(measurement_name=measurement)\n        assert m[\'name\'] == measurement\n    for measurement_idx in range(len(w.measurement_names)):\n        m = w.get_measurement(measurement_index=measurement_idx)\n        assert m[\'name\'] == w.measurement_names[measurement_idx]\n\n\ndef test_get_measurement_fake(workspace_factory):\n    w = workspace_factory()\n    m = w.get_measurement(poi_name=\'fake_poi\')\n    assert m\n\n\ndef test_get_measurement_nonexist(workspace_factory):\n    w = workspace_factory()\n    with pytest.raises(pyhf.exceptions.InvalidMeasurement) as excinfo:\n        w.get_measurement(measurement_name=\'nonexistent_measurement\')\n    assert \'nonexistent_measurement\' in str(excinfo.value)\n\n\ndef test_get_measurement_index_outofbounds(workspace_factory):\n    ws = workspace_factory()\n    with pytest.raises(pyhf.exceptions.InvalidMeasurement) as excinfo:\n        ws.get_measurement(measurement_index=9999)\n    assert \'out of bounds\' in str(excinfo.value)\n\n\ndef test_get_measurement_no_measurements_defined(workspace_factory):\n    ws = workspace_factory()\n    ws.measurement_names = []\n    with pytest.raises(pyhf.exceptions.InvalidMeasurement) as excinfo:\n        ws.get_measurement()\n    assert \'No measurements have been defined\' in str(excinfo.value)\n\n\ndef test_get_workspace_measurement_priority(workspace_factory):\n    w = workspace_factory()\n\n    # does poi_name override all others?\n    m = w.get_measurement(\n        poi_name=\'fake_poi\', measurement_name=\'FakeMeasurement\', measurement_index=999\n    )\n    assert m[\'config\'][\'poi\'] == \'fake_poi\'\n\n    # does measurement_name override measurement_index?\n    m = w.get_measurement(\n        measurement_name=w.measurement_names[0], measurement_index=999\n    )\n    assert m[\'name\'] == w.measurement_names[0]\n    # only in cases where we have more than one measurement to pick from\n    if len(w.measurement_names) > 1:\n        assert m[\'name\'] != w.measurement_names[-1]\n\n\ndef test_get_measurement_schema_validation(mocker, workspace_factory):\n    mocker.patch(\'pyhf.utils.validate\', return_value=None)\n    assert pyhf.utils.validate.called is False\n    w = workspace_factory()\n    assert pyhf.utils.validate.call_count == 1\n    assert pyhf.utils.validate.call_args[0][1] == \'workspace.json\'\n    w.get_measurement()\n    assert pyhf.utils.validate.call_count == 2\n    assert pyhf.utils.validate.call_args[0][1] == \'measurement.json\'\n\n\ndef test_get_workspace_repr(workspace_factory):\n    w = workspace_factory()\n    assert \'pyhf.workspace.Workspace\' in str(w)\n\n\ndef test_get_workspace_model_default(workspace_factory):\n    w = workspace_factory()\n    m = w.model()\n    assert m\n\n\ndef test_workspace_observations(workspace_factory):\n    w = workspace_factory()\n    assert w.observations\n\n\ndef test_get_workspace_data(workspace_factory):\n    w = workspace_factory()\n    m = w.model()\n    assert w.data(m)\n\n\ndef test_get_workspace_data_bad_model(workspace_factory, caplog):\n    w = workspace_factory()\n    m = w.model()\n    # the iconic fragrance of an expected failure\n    m.config.channels = [c.replace(\'channel\', \'chanel\') for c in m.config.channels]\n    with caplog.at_level(logging.INFO, \'pyhf.pdf\'):\n        with pytest.raises(KeyError):\n            assert w.data(m)\n            assert \'Invalid channel\' in caplog.text\n\n\ndef test_json_serializable(workspace_factory):\n    assert json.dumps(workspace_factory())\n\n\ndef test_prune_nothing(workspace_factory):\n    ws = workspace_factory()\n    new_ws = ws.prune(\n        channels=[\'fake-name\'],\n        samples=[\'fake-sample\'],\n        modifiers=[\'fake-modifier\'],\n        modifier_types=[\'fake-type\'],\n    )\n    assert new_ws\n\n\ndef test_prune_channel(workspace_factory):\n    ws = workspace_factory()\n    channel = ws.channels[0]\n    if len(ws.channels) == 1:\n        with pytest.raises(pyhf.exceptions.InvalidSpecification):\n            new_ws = ws.prune(channels=channel)\n        with pytest.raises(pyhf.exceptions.InvalidSpecification):\n            new_ws = ws.prune(channels=[channel])\n    else:\n        new_ws = ws.prune(channels=channel)\n        assert channel not in new_ws.channels\n        assert channel not in [obs[\'name\'] for obs in new_ws[\'observations\']]\n\n        new_ws_list = ws.prune(channels=[channel])\n        assert new_ws_list == new_ws\n\n\ndef test_prune_sample(workspace_factory):\n    ws = workspace_factory()\n    sample = ws.samples[1]\n    new_ws = ws.prune(samples=sample)\n    assert new_ws\n    assert sample not in new_ws.samples\n\n    new_ws_list = ws.prune(samples=[sample])\n    assert new_ws_list == new_ws\n\n\ndef test_prune_modifier(workspace_factory):\n    ws = workspace_factory()\n    modifier = \'lumi\'\n    new_ws = ws.prune(modifiers=modifier)\n    assert new_ws\n    assert modifier not in new_ws.parameters\n    assert modifier not in [\n        p[\'name\']\n        for measurement in new_ws[\'measurements\']\n        for p in measurement[\'config\'][\'parameters\']\n    ]\n\n    new_ws_list = ws.prune(modifiers=[modifier])\n    assert new_ws_list == new_ws\n\n\ndef test_prune_modifier_type(workspace_factory):\n    ws = workspace_factory()\n    modifier_type = \'lumi\'\n    new_ws = ws.prune(modifier_types=modifier_type)\n    assert new_ws\n    assert modifier_type not in [item[1] for item in new_ws.modifiers]\n\n    new_ws_list = ws.prune(modifier_types=[modifier_type])\n    assert new_ws_list == new_ws\n\n\ndef test_prune_measurements(workspace_factory):\n    ws = workspace_factory()\n    measurement = ws.measurement_names[0]\n\n    if len(ws.measurement_names) == 1:\n        with pytest.raises(pyhf.exceptions.InvalidSpecification):\n            new_ws = ws.prune(measurements=measurement)\n        with pytest.raises(pyhf.exceptions.InvalidSpecification):\n            new_ws = ws.prune(measurements=[measurement])\n    else:\n        new_ws = ws.prune(measurements=[measurement])\n        assert new_ws\n        assert measurement not in new_ws.measurement_names\n\n        new_ws_list = ws.prune(measurements=[measurement])\n        assert new_ws_list == new_ws\n\n\ndef test_rename_channel(workspace_factory):\n    ws = workspace_factory()\n    channel = ws.channels[0]\n    renamed = \'renamedChannel\'\n    assert renamed not in ws.channels\n    new_ws = ws.rename(channels={channel: renamed})\n    assert channel not in new_ws.channels\n    assert renamed in new_ws.channels\n    assert channel not in [obs[\'name\'] for obs in new_ws[\'observations\']]\n    assert renamed in [obs[\'name\'] for obs in new_ws[\'observations\']]\n\n\ndef test_rename_sample(workspace_factory):\n    ws = workspace_factory()\n    sample = ws.samples[1]\n    renamed = \'renamedSample\'\n    assert renamed not in ws.samples\n    new_ws = ws.rename(samples={sample: renamed})\n    assert sample not in new_ws.samples\n    assert renamed in new_ws.samples\n\n\ndef test_rename_modifier(workspace_factory):\n    ws = workspace_factory()\n    modifier = ws.parameters[0]\n    renamed = \'renamedModifier\'\n    assert renamed not in ws.parameters\n    new_ws = ws.rename(modifiers={modifier: renamed})\n    assert modifier not in new_ws.parameters\n    assert renamed in new_ws.parameters\n\n\ndef test_rename_poi(workspace_factory):\n    ws = workspace_factory()\n    poi = ws.get_measurement()[\'config\'][\'poi\']\n    renamed = \'renamedPoi\'\n    assert renamed not in ws.parameters\n    new_ws = ws.rename(modifiers={poi: renamed})\n    assert poi not in new_ws.parameters\n    assert renamed in new_ws.parameters\n    assert new_ws.get_measurement()[\'config\'][\'poi\'] == renamed\n\n\ndef test_rename_measurement(workspace_factory):\n    ws = workspace_factory()\n    measurement = ws.measurement_names[0]\n    renamed = \'renamedMeasurement\'\n    assert renamed not in ws.measurement_names\n    new_ws = ws.rename(measurements={measurement: renamed})\n    assert measurement not in new_ws.measurement_names\n    assert renamed in new_ws.measurement_names\n\n\n@pytest.fixture(scope=\'session\')\ndef join_items():\n    left = [{\'name\': \'left\', \'key\': \'value\'}, {\'name\': \'common\', \'key\': \'left\'}]\n    right = [{\'name\': \'right\', \'key\': \'value\'}, {\'name\': \'common\', \'key\': \'right\'}]\n    return (left, right)\n\n\ndef test_join_items_none(join_items):\n    left_items, right_items = join_items\n    joined = pyhf.workspace._join_items(\'none\', left_items, right_items, key=\'name\')\n    assert all(left in joined for left in left_items)\n    assert all(right in joined for right in right_items)\n\n\ndef test_join_items_outer(join_items):\n    left_items, right_items = join_items\n    joined = pyhf.workspace._join_items(\'outer\', left_items, right_items, key=\'name\')\n    assert all(left in joined for left in left_items)\n    assert all(right in joined for right in right_items)\n\n\ndef test_join_items_left_outer(join_items):\n    left_items, right_items = join_items\n    joined = pyhf.workspace._join_items(\n        \'left outer\', left_items, right_items, key=\'name\'\n    )\n    assert all(left in joined for left in left_items)\n    assert not all(right in joined for right in right_items)\n\n\ndef test_join_items_right_outer(join_items):\n    left_items, right_items = join_items\n    joined = pyhf.workspace._join_items(\n        \'right outer\', left_items, right_items, key=\'name\'\n    )\n    assert not all(left in joined for left in left_items)\n    assert all(right in joined for right in right_items)\n\n\n@pytest.mark.parametrize(""join"", [\'none\', \'outer\'])\ndef test_combine_workspace_same_channels_incompatible_structure(\n    workspace_factory, join\n):\n    ws = workspace_factory()\n    new_ws = ws.rename(\n        channels={\'channel2\': \'channel3\'},\n        samples={\'signal\': \'signal_other\'},\n        measurements={\'GaussExample\': \'GaussExample2\'},\n    ).prune(measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\'])\n    with pytest.raises(pyhf.exceptions.InvalidWorkspaceOperation) as excinfo:\n        pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert \'channel1\' in str(excinfo.value)\n    assert \'channel2\' not in str(excinfo.value)\n\n\n@pytest.mark.parametrize(""join"", [\'outer\', \'left outer\', \'right outer\'])\ndef test_combine_workspace_same_channels_outer_join(workspace_factory, join):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel2\': \'channel3\'})\n    combined = pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert \'channel1\' in combined.channels\n\n\n@pytest.mark.parametrize(""join"", [\'left outer\', \'right outer\'])\ndef test_combine_workspace_same_channels_outer_join_unsafe(\n    workspace_factory, join, caplog\n):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel2\': \'channel3\'})\n    pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert \'using an unsafe join operation\' in caplog.text\n\n\n@pytest.mark.parametrize(""join"", [\'none\', \'outer\'])\ndef test_combine_workspace_incompatible_poi(workspace_factory, join):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    new_ws = new_ws.rename(\n        modifiers={new_ws.get_measurement()[\'config\'][\'poi\']: \'renamedPOI\'}\n    )\n    with pytest.raises(pyhf.exceptions.InvalidWorkspaceOperation) as excinfo:\n        pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert \'GaussExample\' in str(excinfo.value)\n\n\n@pytest.mark.parametrize(""join"", [\'none\', \'outer\', \'left outer\', \'right outer\'])\ndef test_combine_workspace_diff_version(workspace_factory, join):\n    ws = workspace_factory()\n    ws.version = \'1.0.0\'\n    new_ws = ws.rename(\n        channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'},\n        samples={\n            \'background1\': \'background3\',\n            \'background2\': \'background4\',\n            \'signal\': \'signal2\',\n        },\n        modifiers={\n            \'syst1\': \'syst4\',\n            \'bkg1Shape\': \'bkg3Shape\',\n            \'bkg2Shape\': \'bkg4Shape\',\n        },\n        measurements={\n            \'ConstExample\': \'OtherConstExample\',\n            \'LogNormExample\': \'OtherLogNormExample\',\n            \'GaussExample\': \'OtherGaussExample\',\n            \'GammaExample\': \'OtherGammaExample\',\n        },\n    )\n    new_ws[\'version\'] = \'1.2.0\'\n    with pytest.raises(pyhf.exceptions.InvalidWorkspaceOperation) as excinfo:\n        pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert \'1.0.0\' in str(excinfo.value)\n    assert \'1.2.0\' in str(excinfo.value)\n\n\n@pytest.mark.parametrize(""join"", [\'none\'])\ndef test_combine_workspace_duplicate_parameter_configs(workspace_factory, join):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    with pytest.raises(pyhf.exceptions.InvalidWorkspaceOperation) as excinfo:\n        pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert \'GaussExample\' in str(excinfo.value)\n\n\n@pytest.mark.parametrize(""join"", [\'outer\', \'left outer\', \'right outer\'])\ndef test_combine_workspace_duplicate_parameter_configs_outer_join(\n    workspace_factory, join\n):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    combined = pyhf.Workspace.combine(ws, new_ws, join=join)\n\n    poi = ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'poi\']\n    ws_parameter_configs = [\n        parameter[\'name\']\n        for parameter in ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\n            \'parameters\'\n        ]\n    ]\n    new_ws_parameter_configs = [\n        parameter[\'name\']\n        for parameter in new_ws.get_measurement(measurement_name=\'GaussExample\')[\n            \'config\'\n        ][\'parameters\']\n    ]\n    combined_parameter_configs = [\n        parameter[\'name\']\n        for parameter in combined.get_measurement(measurement_name=\'GaussExample\')[\n            \'config\'\n        ][\'parameters\']\n    ]\n\n    assert poi in ws_parameter_configs\n    assert poi in new_ws_parameter_configs\n    assert poi in combined_parameter_configs\n    assert \'lumi\' in ws_parameter_configs\n    assert \'lumi\' in new_ws_parameter_configs\n    assert \'lumi\' in combined_parameter_configs\n    assert len(combined_parameter_configs) == len(set(combined_parameter_configs))\n\n\ndef test_combine_workspace_parameter_configs_ordering(workspace_factory):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    assert (\n        ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\']\n        == new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\n            \'parameters\'\n        ]\n    )\n\n\ndef test_combine_workspace_observation_ordering(workspace_factory):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    assert ws[\'observations\'][0][\'data\'] == new_ws[\'observations\'][0][\'data\']\n\n\ndef test_combine_workspace_deepcopied(workspace_factory):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\'][0][\n        \'bounds\'\n    ] = [[0.0, 1.0]]\n    new_ws[\'observations\'][0][\'data\'][0] = -10.0\n    assert (\n        ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\'][0][\n            \'bounds\'\n        ]\n        != new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\n            \'parameters\'\n        ][0][\'bounds\']\n    )\n    assert ws[\'observations\'][0][\'data\'] != new_ws[\'observations\'][0][\'data\']\n\n\n@pytest.mark.parametrize(""join"", [\'fake join operation\'])\ndef test_combine_workspace_invalid_join_operation(workspace_factory, join):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    with pytest.raises(ValueError) as excinfo:\n        pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert join in str(excinfo.value)\n\n\n@pytest.mark.parametrize(""join"", [\'none\'])\ndef test_combine_workspace_incompatible_parameter_configs(workspace_factory, join):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\'][0][\n        \'bounds\'\n    ] = [[0.0, 1.0]]\n    with pytest.raises(pyhf.exceptions.InvalidWorkspaceOperation) as excinfo:\n        pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert \'GaussExample\' in str(excinfo.value)\n\n\n@pytest.mark.parametrize(""join"", [\'outer\'])\ndef test_combine_workspace_incompatible_parameter_configs_outer_join(\n    workspace_factory, join\n):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\'][0][\n        \'bounds\'\n    ] = [[0.0, 1.0]]\n    with pytest.raises(pyhf.exceptions.InvalidWorkspaceOperation) as excinfo:\n        pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert \'GaussExample\' in str(excinfo.value)\n    assert ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\'][\n        0\n    ][\'name\'] in str(excinfo.value)\n    assert new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\n        \'parameters\'\n    ][0][\'name\'] in str(excinfo.value)\n\n\ndef test_combine_workspace_incompatible_parameter_configs_left_outer_join(\n    workspace_factory,\n):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\'][0][\n        \'bounds\'\n    ] = [[0.0, 1.0]]\n    combined = pyhf.Workspace.combine(ws, new_ws, join=\'left outer\')\n    assert (\n        combined.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\n            \'parameters\'\n        ][0]\n        == ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\'][\n            0\n        ]\n    )\n\n\ndef test_combine_workspace_incompatible_parameter_configs_right_outer_join(\n    workspace_factory,\n):\n    ws = workspace_factory()\n    new_ws = ws.rename(channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'}).prune(\n        measurements=[\'GammaExample\', \'ConstExample\', \'LogNormExample\']\n    )\n    new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\'parameters\'][0][\n        \'bounds\'\n    ] = [[0.0, 1.0]]\n    combined = pyhf.Workspace.combine(ws, new_ws, join=\'right outer\')\n    assert (\n        combined.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\n            \'parameters\'\n        ][0]\n        == new_ws.get_measurement(measurement_name=\'GaussExample\')[\'config\'][\n            \'parameters\'\n        ][0]\n    )\n\n\n@pytest.mark.parametrize(""join"", [\'none\', \'outer\'])\ndef test_combine_workspace_incompatible_observations(workspace_factory, join):\n    ws = workspace_factory()\n    new_ws = ws.rename(\n        channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'},\n        samples={\n            \'background1\': \'background3\',\n            \'background2\': \'background4\',\n            \'signal\': \'signal2\',\n        },\n        modifiers={\n            \'syst1\': \'syst4\',\n            \'bkg1Shape\': \'bkg3Shape\',\n            \'bkg2Shape\': \'bkg4Shape\',\n        },\n        measurements={\n            \'GaussExample\': \'OtherGaussExample\',\n            \'GammaExample\': \'OtherGammaExample\',\n            \'ConstExample\': \'OtherConstExample\',\n            \'LogNormExample\': \'OtherLogNormExample\',\n        },\n    )\n    new_ws[\'observations\'][0][\'name\'] = ws[\'observations\'][0][\'name\']\n    new_ws[\'observations\'][0][\'data\'][0] = -10.0\n    with pytest.raises(pyhf.exceptions.InvalidWorkspaceOperation) as excinfo:\n        pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert ws[\'observations\'][0][\'name\'] in str(excinfo.value)\n    assert \'observations\' in str(excinfo.value)\n\n\ndef test_combine_workspace_incompatible_observations_left_outer(workspace_factory):\n    ws = workspace_factory()\n    new_ws = ws.rename(\n        channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'},\n        samples={\n            \'background1\': \'background3\',\n            \'background2\': \'background4\',\n            \'signal\': \'signal2\',\n        },\n        modifiers={\n            \'syst1\': \'syst4\',\n            \'bkg1Shape\': \'bkg3Shape\',\n            \'bkg2Shape\': \'bkg4Shape\',\n        },\n        measurements={\n            \'GaussExample\': \'OtherGaussExample\',\n            \'GammaExample\': \'OtherGammaExample\',\n            \'ConstExample\': \'OtherConstExample\',\n            \'LogNormExample\': \'OtherLogNormExample\',\n        },\n    )\n    new_ws[\'observations\'][0][\'name\'] = ws[\'observations\'][0][\'name\']\n    new_ws[\'observations\'][0][\'data\'][0] = -10.0\n    combined = pyhf.Workspace.combine(ws, new_ws, join=\'left outer\')\n    assert (\n        combined.observations[ws[\'observations\'][0][\'name\']]\n        == ws[\'observations\'][0][\'data\']\n    )\n\n\ndef test_combine_workspace_incompatible_observations_right_outer(workspace_factory):\n    ws = workspace_factory()\n    new_ws = ws.rename(\n        channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'},\n        samples={\n            \'background1\': \'background3\',\n            \'background2\': \'background4\',\n            \'signal\': \'signal2\',\n        },\n        modifiers={\n            \'syst1\': \'syst4\',\n            \'bkg1Shape\': \'bkg3Shape\',\n            \'bkg2Shape\': \'bkg4Shape\',\n        },\n        measurements={\n            \'GaussExample\': \'OtherGaussExample\',\n            \'GammaExample\': \'OtherGammaExample\',\n            \'ConstExample\': \'OtherConstExample\',\n            \'LogNormExample\': \'OtherLogNormExample\',\n        },\n    )\n    new_ws[\'observations\'][0][\'name\'] = ws[\'observations\'][0][\'name\']\n    new_ws[\'observations\'][0][\'data\'][0] = -10.0\n    combined = pyhf.Workspace.combine(ws, new_ws, join=\'right outer\')\n    assert (\n        combined.observations[ws[\'observations\'][0][\'name\']]\n        == new_ws[\'observations\'][0][\'data\']\n    )\n\n\n@pytest.mark.parametrize(""join"", pyhf.Workspace.valid_joins)\ndef test_combine_workspace(workspace_factory, join):\n    ws = workspace_factory()\n    new_ws = ws.rename(\n        channels={\'channel1\': \'channel3\', \'channel2\': \'channel4\'},\n        samples={\n            \'background1\': \'background3\',\n            \'background2\': \'background4\',\n            \'signal\': \'signal2\',\n        },\n        modifiers={\n            \'syst1\': \'syst4\',\n            \'bkg1Shape\': \'bkg3Shape\',\n            \'bkg2Shape\': \'bkg4Shape\',\n        },\n        measurements={\n            \'GaussExample\': \'OtherGaussExample\',\n            \'GammaExample\': \'OtherGammaExample\',\n            \'ConstExample\': \'OtherConstExample\',\n            \'LogNormExample\': \'OtherLogNormExample\',\n        },\n    )\n    combined = pyhf.Workspace.combine(ws, new_ws, join=join)\n    assert set(combined.channels) == set(ws.channels + new_ws.channels)\n    assert set(combined.samples) == set(ws.samples + new_ws.samples)\n    assert set(combined.parameters) == set(ws.parameters + new_ws.parameters)\n\n\ndef test_workspace_equality(workspace_factory):\n    ws = workspace_factory()\n    ws_other = workspace_factory()\n    assert ws == ws\n    assert ws == ws_other\n    assert ws != \'not a workspace\'\n'"
validation/makedata.py,0,"b""import ROOT\n\nimport json\nimport sys\n\nsource_data = json.load(open(sys.argv[1]))\nroot_file = sys.argv[2]\n\nbinning = source_data['binning']\nbindata = source_data['bindata']\n\n\nf = ROOT.TFile(root_file, 'RECREATE')\ndata = ROOT.TH1F('data', 'data', *binning)\nfor i, v in enumerate(bindata['data']):\n    data.SetBinContent(i + 1, v)\ndata.Sumw2()\n\nbkg = ROOT.TH1F('bkg', 'bkg', *binning)\nfor i, v in enumerate(bindata['bkg']):\n    bkg.SetBinContent(i + 1, v)\nbkg.Sumw2()\n\n\nif 'bkgerr' in bindata:\n    bkgerr = ROOT.TH1F('bkgerr', 'bkgerr', *binning)\n\n    # shapesys must be as multiplicative factor\n    for i, v in enumerate(bindata['bkgerr']):\n        bkgerr.SetBinContent(i + 1, v / bkg.GetBinContent(i + 1))\n    bkgerr.Sumw2()\n\nsig = ROOT.TH1F('sig', 'sig', *binning)\nfor i, v in enumerate(bindata['sig']):\n    sig.SetBinContent(i + 1, v)\nsig.Sumw2()\nf.Write()\n"""
validation/run_cls.py,0,"b'import ROOT\nimport sys\n\ninfile = sys.argv[1]\n\ninfile = ROOT.TFile.Open(infile)\nworkspace = infile.Get(""combined"")\ndata = workspace.data(""obsData"")\n\n\nsbModel = workspace.obj(""ModelConfig"")\npoi = sbModel.GetParametersOfInterest().first()\n\nsbModel.SetSnapshot(ROOT.RooArgSet(poi))\n\nbModel = sbModel.Clone()\nbModel.SetName(""bonly"")\npoi.setVal(0)\nbModel.SetSnapshot(ROOT.RooArgSet(poi))\n\nac = ROOT.RooStats.AsymptoticCalculator(data, bModel, sbModel)\nac.SetPrintLevel(10)\nac.SetOneSided(True)\nac.SetQTilde(True)\n\ncalc = ROOT.RooStats.HypoTestInverter(ac)\ncalc.RunFixedScan(51, 0, 5)\ncalc.SetConfidenceLevel(0.95)\ncalc.UseCLs(True)\n\n\nresult = calc.GetInterval()\n\nplot = ROOT.RooStats.HypoTestInverterPlot(""plot"", ""plot"", result)\nc = ROOT.TCanvas()\nc.SetLogy(False)\nplot.Draw(""OBS EXP CLb 2CL"")\nc.Draw()\nc.SaveAs(\'scan.pdf\')\n\n\nprint(\'observed: {}\'.format(result.UpperLimit()))\n\nfor i in [-2, -1, 0, 1, 2]:\n    print(\'expected {}: {}\'.format(i, result.GetExpectedUpperLimit(i)))\n'"
validation/run_single.py,0,"b'import ROOT\nimport sys\n\ninfile = sys.argv[1]\n\ninfile = ROOT.TFile.Open(infile)\nworkspace = infile.Get(""combined"")\ndata = workspace.data(""obsData"")\n\nsbModel = workspace.obj(""ModelConfig"")\npoi = sbModel.GetParametersOfInterest().first()\n\nsbModel.SetSnapshot(ROOT.RooArgSet(poi))\n\nbModel = sbModel.Clone()\nbModel.SetName(""bonly"")\npoi.setVal(0)\nbModel.SetSnapshot(ROOT.RooArgSet(poi))\n\nac = ROOT.RooStats.AsymptoticCalculator(data, bModel, sbModel)\nac.SetPrintLevel(10)\nac.SetOneSided(True)\nac.SetQTilde(True)\n\n\ncalc = ROOT.RooStats.HypoTestInverter(ac)\ncalc.SetConfidenceLevel(0.95)\ncalc.UseCLs(True)\ncalc.RunFixedScan(1, 1, 1)\n\nresult = calc.GetInterval()\n\nindex = 0\n\nw = result.GetExpectedPValueDist(index)\nv = w.GetSamplingDistribution()\n\nCLs_obs = result.CLs(index)\nCLs_exp = list(v)[3:-3]\n\nimport json\n\nprint(json.dumps({\'CLs_obs\': CLs_obs, \'CLs_exp\': CLs_exp}, sort_keys=True, indent=4))\n'"
docs/exts/xref.py,0,"b'# -*- coding: utf-8 -*-\n\nfrom docutils import nodes\n\nfrom sphinx.util import caption_ref_re\n\n\ndef xref(typ, rawtext, text, lineno, inliner, options=None, content=None):\n    # avoid mutable defaults\n    options = {} if options is None else options\n    content = [] if content is None else content\n\n    title = target = text\n    # titleistarget = True\n    # look if explicit title and target are given with `foo <bar>` syntax\n    brace = text.find(\'<\')\n    if brace != -1:\n        # titleistarget = False\n        m = caption_ref_re.match(text)\n        if m:\n            target = m.group(2)\n            title = m.group(1)\n        else:\n            # fallback: everything after \'<\' is the target\n            target = text[brace + 1 :]\n            title = text[:brace]\n\n    link = xref.links[target]\n\n    if brace != -1:\n        pnode = nodes.reference(target, title, refuri=link[1])\n    else:\n        pnode = nodes.reference(target, link[0], refuri=link[1])\n\n    return [pnode], []\n\n\ndef get_refs(app):\n\n    xref.links = app.config.xref_links\n\n\ndef setup(app):\n\n    app.add_config_value(\'xref_links\', {}, True)\n    app.add_role(\'xref\', xref)\n    app.connect(""builder-inited"", get_refs)\n'"
src/pyhf/__init__.py,0,"b'from .tensor import BackendRetriever as tensor\nfrom .optimize import OptimizerRetriever as optimize\nfrom .version import __version__\nfrom .exceptions import InvalidBackend\nfrom . import events\n\ntensorlib = tensor.numpy_backend()\ndefault_backend = tensorlib\noptimizer = optimize.scipy_optimizer()\ndefault_optimizer = optimizer\n\n\ndef get_backend():\n    """"""\n    Get the current backend and the associated optimizer\n\n    Example:\n        >>> import pyhf\n        >>> pyhf.get_backend()\n        (<pyhf.tensor.numpy_backend.numpy_backend object at 0x...>, <pyhf.optimize.opt_scipy.scipy_optimizer object at 0x...>)\n\n    Returns:\n        backend, optimizer\n    """"""\n    global tensorlib\n    global optimizer\n    return tensorlib, optimizer\n\n\n@events.register(\'change_backend\')\ndef set_backend(backend, custom_optimizer=None):\n    """"""\n    Set the backend and the associated optimizer\n\n    Example:\n        >>> import pyhf\n        >>> pyhf.set_backend(""tensorflow"")\n        >>> pyhf.tensorlib.name\n        \'tensorflow\'\n        >>> pyhf.set_backend(b""pytorch"")\n        >>> pyhf.tensorlib.name\n        \'pytorch\'\n        >>> pyhf.set_backend(pyhf.tensor.numpy_backend())\n        >>> pyhf.tensorlib.name\n        \'numpy\'\n\n    Args:\n        backend (`str` or `pyhf.tensor` backend): One of the supported pyhf backends: NumPy, TensorFlow, PyTorch, and JAX\n        custom_optimizer (`pyhf.optimize` optimizer): Optional custom optimizer defined by the user\n\n    Returns:\n        None\n    """"""\n    global tensorlib\n    global optimizer\n\n    if isinstance(backend, (str, bytes)):\n        if isinstance(backend, bytes):\n            backend = backend.decode(""utf-8"")\n        backend = backend.lower()\n        try:\n            backend = getattr(tensor, ""{0:s}_backend"".format(backend))()\n        except TypeError:\n            raise InvalidBackend(\n                ""The backend provided is not supported: {0:s}. Select from one of the supported backends: numpy, tensorflow, pytorch"".format(\n                    backend\n                )\n            )\n\n    _name_supported = getattr(tensor, ""{0:s}_backend"".format(backend.name))\n    if _name_supported:\n        if not isinstance(backend, _name_supported):\n            raise AttributeError(\n                ""\'{0:s}\' is not a valid name attribute for backend type {1}\\n                 Custom backends must have names unique from supported backends"".format(\n                    backend.name, type(backend)\n                )\n            )\n\n    # need to determine if the tensorlib changed or the optimizer changed for events\n    tensorlib_changed = bool(backend.name != tensorlib.name)\n    optimizer_changed = False\n\n    if backend.name == \'tensorflow\':\n        new_optimizer = (\n            custom_optimizer if custom_optimizer else optimize.tflow_optimizer(backend)\n        )\n    elif backend.name == \'pytorch\':\n        new_optimizer = (\n            custom_optimizer\n            if custom_optimizer\n            else optimize.pytorch_optimizer(tensorlib=backend)\n        )\n    elif backend.name == \'jax\':\n        new_optimizer = (\n            custom_optimizer if custom_optimizer else optimize.jax_optimizer()\n        )\n    else:\n        new_optimizer = (\n            custom_optimizer if custom_optimizer else optimize.scipy_optimizer()\n        )\n\n    optimizer_changed = bool(optimizer != new_optimizer)\n    # set new backend\n    tensorlib = backend\n    optimizer = new_optimizer\n    # trigger events\n    if tensorlib_changed:\n        events.trigger(""tensorlib_changed"")()\n    if optimizer_changed:\n        events.trigger(""optimizer_changed"")()\n\n\nfrom .pdf import Model\nfrom .workspace import Workspace\nfrom . import simplemodels\nfrom . import infer\nfrom .patchset import PatchSet\n\n__all__ = [\n    \'Model\',\n    \'Workspace\',\n    \'PatchSet\',\n    \'infer\',\n    \'utils\',\n    \'modifiers\',\n    \'simplemodels\',\n    \'__version__\',\n]\n'"
src/pyhf/constraints.py,0,"b'from . import get_backend, default_backend\nfrom . import events\nfrom . import probability as prob\nfrom .parameters import ParamViewer\n\n\nclass gaussian_constraint_combined(object):\n    def __init__(self, pdfconfig, batch_size=None):\n        self.batch_size = batch_size\n        # iterate over all constraints order doesn\'t matter....\n\n        self.data_indices = list(range(len(pdfconfig.auxdata)))\n        self.parsets = [pdfconfig.param_set(cname) for cname in pdfconfig.auxdata_order]\n\n        pars_constrained_by_normal = [\n            constrained_parameter\n            for constrained_parameter in pdfconfig.auxdata_order\n            if pdfconfig.param_set(constrained_parameter).pdf_type == \'normal\'\n        ]\n\n        parfield_shape = (self.batch_size or 1, pdfconfig.npars)\n        self.param_viewer = ParamViewer(\n            parfield_shape, pdfconfig.par_map, pars_constrained_by_normal\n        )\n\n        start_index = 0\n        normal_constraint_data = []\n        normal_constraint_sigmas = []\n        # loop over parameters (in auxdata order) and collect\n        # means / sigmas of constraint term as well as data\n        # skip parsets that are not constrained by onrmal\n        for parset in self.parsets:\n            end_index = start_index + parset.n_parameters\n            thisauxdata = self.data_indices[start_index:end_index]\n            start_index = end_index\n            if not parset.pdf_type == \'normal\':\n                continue\n\n            normal_constraint_data.append(thisauxdata)\n\n            # many constraints are defined on a unit gaussian\n            # but we reserved the possibility that a paramset\n            # can define non-standard uncertainties. This is used\n            # by the paramset associated to staterror modifiers.\n            # Such parsets define a \'sigmas\' attribute\n            try:\n                normal_constraint_sigmas.append(parset.sigmas)\n            except AttributeError:\n                normal_constraint_sigmas.append([1.0] * len(thisauxdata))\n\n        self._normal_data = None\n        self._sigmas = None\n        self._access_field = None\n        # if this constraint terms is at all used (non-zrto idx selection\n        # start preparing constant tensors\n        if self.param_viewer.index_selection:\n            self._normal_data = default_backend.astensor(\n                default_backend.concatenate(normal_constraint_data), dtype=\'int\'\n            )\n\n            _normal_sigmas = default_backend.concatenate(normal_constraint_sigmas)\n            if self.batch_size:\n                sigmas = default_backend.reshape(_normal_sigmas, (1, -1))\n                self._sigmas = default_backend.tile(sigmas, (self.batch_size, 1))\n            else:\n                self._sigmas = _normal_sigmas\n\n            access_field = default_backend.concatenate(\n                self.param_viewer.index_selection, axis=1\n            )\n            self._access_field = access_field\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        self.sigmas = tensorlib.astensor(self._sigmas)\n        self.normal_data = tensorlib.astensor(self._normal_data, dtype=\'int\')\n        self.access_field = tensorlib.astensor(self._access_field, dtype=\'int\')\n\n    def has_pdf(self):\n        \'\'\'\n        Returns:\n            flag (`bool`): Whether the model has a Gaussian Constraint\n        \'\'\'\n        return bool(self.param_viewer.index_selection)\n\n    def make_pdf(self, pars):\n        """"""\n        Args:\n            pars (`tensor`): The model parameters\n\n        Returns:\n            pdf: The pdf object for the Normal Constraint\n        """"""\n        tensorlib, _ = get_backend()\n        if not self.param_viewer.index_selection:\n            return None\n        if self.batch_size is None:\n            flat_pars = pars\n        else:\n            flat_pars = tensorlib.reshape(pars, (-1,))\n\n        normal_means = tensorlib.gather(flat_pars, self.access_field)\n\n        # pdf pars are done, now get data and compute\n        if self.batch_size is None:\n            normal_means = normal_means[0]\n\n        result = prob.Independent(\n            prob.Normal(normal_means, self.sigmas), batch_size=self.batch_size\n        )\n        return result\n\n    def logpdf(self, auxdata, pars):\n        """"""\n        Args:\n            auxdata (`tensor`): The auxiliary data (a subset of the full data in a HistFactory model)\n            pars (`tensor`): The model parameters\n\n        Returns:\n            log pdf value: The log of the pdf value of the Normal constraints\n        """"""\n        tensorlib, _ = get_backend()\n        pdf = self.make_pdf(pars)\n        if pdf is None:\n            return (\n                tensorlib.zeros(self.batch_size)\n                if self.batch_size is not None\n                else tensorlib.astensor(0.0)[0]\n            )\n        normal_data = tensorlib.gather(auxdata, self.normal_data)\n        return pdf.log_prob(normal_data)\n\n\nclass poisson_constraint_combined(object):\n    def __init__(self, pdfconfig, batch_size=None):\n        self.batch_size = batch_size\n        # iterate over all constraints order doesn\'t matter....\n\n        self.par_indices = list(range(pdfconfig.npars))\n        self.data_indices = list(range(len(pdfconfig.auxdata)))\n        self.parsets = [pdfconfig.param_set(cname) for cname in pdfconfig.auxdata_order]\n\n        pars_constrained_by_poisson = [\n            constrained_parameter\n            for constrained_parameter in pdfconfig.auxdata_order\n            if pdfconfig.param_set(constrained_parameter).pdf_type == \'poisson\'\n        ]\n\n        parfield_shape = (self.batch_size or 1, pdfconfig.npars)\n        self.param_viewer = ParamViewer(\n            parfield_shape, pdfconfig.par_map, pars_constrained_by_poisson\n        )\n\n        start_index = 0\n        poisson_constraint_data = []\n        poisson_constraint_rate_factors = []\n        for parset in self.parsets:\n            end_index = start_index + parset.n_parameters\n            thisauxdata = self.data_indices[start_index:end_index]\n            start_index = end_index\n            if not parset.pdf_type == \'poisson\':\n                continue\n\n            poisson_constraint_data.append(thisauxdata)\n\n            # poisson constraints can specify a scaling factor for the\n            # backgrounds rates (see: on-off problem with a aux measurement\n            # with tau*b). If such a scale factor is not defined we just\n            # take a factor of one\n            try:\n                poisson_constraint_rate_factors.append(parset.factors)\n            except AttributeError:\n                # this seems to be dead code\n                # TODO: add coverage (issue #540)\n                poisson_constraint_rate_factors.append([1.0] * len(thisauxdata))\n\n        self._poisson_data = None\n        self._access_field = None\n        self._batched_factors = None\n        if self.param_viewer.index_selection:\n            self._poisson_data = default_backend.astensor(\n                default_backend.concatenate(poisson_constraint_data), dtype=\'int\'\n            )\n\n            _poisson_rate_fac = default_backend.astensor(\n                default_backend.concatenate(poisson_constraint_rate_factors),\n                dtype=\'float\',\n            )\n            factors = default_backend.reshape(_poisson_rate_fac, (1, -1))\n            self._batched_factors = default_backend.tile(\n                factors, (self.batch_size or 1, 1)\n            )\n\n            access_field = default_backend.concatenate(\n                self.param_viewer.index_selection, axis=1\n            )\n            self._access_field = access_field\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        self.poisson_data = tensorlib.astensor(self._poisson_data, dtype=\'int\')\n        self.access_field = tensorlib.astensor(self._access_field, dtype=\'int\')\n        self.batched_factors = tensorlib.astensor(self._batched_factors)\n\n    def has_pdf(self):\n        \'\'\'\n        Returns:\n            flag (`bool`): Whether the model has a Gaussian Constraint\n        \'\'\'\n        return bool(self.param_viewer.index_selection)\n\n    def make_pdf(self, pars):\n        """"""\n        Args:\n            pars (`tensor`): The model parameters\n\n        Returns:\n            pdf: the pdf object for the Poisson Constraint\n        """"""\n        if not self.param_viewer.index_selection:\n            return None\n        tensorlib, _ = get_backend()\n        if self.batch_size is None:\n            flat_pars = pars\n        else:\n            flat_pars = tensorlib.reshape(pars, (-1,))\n        nuispars = tensorlib.gather(flat_pars, self.access_field)\n\n        # similar to expected_data() in constrained_by_poisson\n        # we multiply by the appropriate factor to achieve\n        # the desired variance for poisson-type cosntraints\n        pois_rates = tensorlib.product(\n            tensorlib.stack([nuispars, self.batched_factors]), axis=0\n        )\n        if self.batch_size is None:\n            pois_rates = pois_rates[0]\n        # pdf pars are done, now get data and compute\n        return prob.Independent(prob.Poisson(pois_rates), batch_size=self.batch_size)\n\n    def logpdf(self, auxdata, pars):\n        """"""\n        Args:\n            auxdata (`tensor`): The auxiliary data (a subset of the full data in a HistFactory model)\n            pars (`tensor`): The model parameters\n\n        Returns:\n            log pdf value: The log of the pdf value of the Poisson constraints\n        """"""\n        tensorlib, _ = get_backend()\n        pdf = self.make_pdf(pars)\n        if pdf is None:\n            return (\n                tensorlib.zeros(self.batch_size)\n                if self.batch_size is not None\n                else tensorlib.astensor(0.0)[0]\n            )\n        poisson_data = tensorlib.gather(auxdata, self.poisson_data)\n        return pdf.log_prob(poisson_data)\n'"
src/pyhf/events.py,0,"b'__events = {}\n__disabled_events = set([])\n\n\ndef noop(*args, **kwargs):\n    pass\n\n\nclass Callables(list):\n    def __call__(self, *args, **kwargs):\n        for f in self:\n            f(*args, **kwargs)\n\n    def __repr__(self):\n        return ""Callables(%s)"" % list.__repr__(self)\n\n\ndef subscribe(event):\n    """"""\n    This is meant to be used as a decorator.\n    """"""\n    # Example:\n    #\n    # >>> @pyhf.events.subscribe(\'myevent\')\n    # ... def test(a,b):\n    # ...   print a+b\n    # ...\n    # >>> pyhf.events.trigger_myevent(1,2)\n    # 3\n    global __events\n\n    def __decorator(func):\n        __events.setdefault(event, Callables()).append(func)\n        return func\n\n    return __decorator\n\n\ndef register(event):\n    """"""\n    This is meant to be used as a decorator to register a function for triggering events.\n\n    This creates two events: ""<event_name>::before"" and ""<event_name>::after""\n    """"""\n    # Examples:\n    #\n    # >>> @pyhf.events.register(\'test_func\')\n    # ... def test(a,b):\n    # ...   print a+b\n    # ...\n    # >>> @pyhf.events.subscribe(\'test_func::before\')\n    # ... def precall():\n    # ...   print \'before call\'\n    # ...\n    # >>> @pyhf.events.subscribe(\'test_func::after\')\n    # ... def postcall():\n    # ...   print \'after call\'\n    # ...\n    # >>> test(1,2)\n    # ""before call""\n    # 3\n    # ""after call""\n    # >>>\n\n    def _register(func):\n        def register_wrapper(*args, **kwargs):\n            trigger(""{0:s}::before"".format(event))()\n            result = func(*args, **kwargs)\n            trigger(""{0:s}::after"".format(event))()\n            return result\n\n        return register_wrapper\n\n    return _register\n\n\ndef trigger(event):\n    """"""\n    Trigger an event if not disabled.\n    """"""\n    global __events, __disabled_events, noop\n    is_noop = bool(event in __disabled_events or event not in __events)\n    return noop if is_noop else __events.get(event)\n\n\ndef disable(event):\n    """"""\n    Disable an event from firing.\n    """"""\n    global __disabled_events\n    __disabled_events.add(event)\n\n\ndef enable(event):\n    """"""\n    Enable an event to be fired if disabled.\n    """"""\n    global __disabled_events\n    __disabled_events.remove(event)\n'"
src/pyhf/mixins.py,0,"b'import logging\n\nlog = logging.getLogger(__name__)\n\n\nclass _ChannelSummaryMixin(object):\n    """"""\n    A mixin that provides summary data of the provided channels.\n\n    This mixin will forward all other information to other classes defined in the Child class.\n\n    Args:\n      **channels: A list of channels to provide summary information about. Follows the `defs.json#/definitions/channel` schema.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        channels = kwargs.pop(\'channels\')\n        super(_ChannelSummaryMixin, self).__init__(*args, **kwargs)\n        self.channels = []\n        self.samples = []\n        self.parameters = []\n        self.modifiers = []\n        # keep track of the width of each channel (how many bins)\n        self.channel_nbins = {}\n        # need to keep track in which order we added the constraints\n        # so that we can generate correctly-ordered data\n        for channel in channels:\n            self.channels.append(channel[\'name\'])\n            self.channel_nbins[channel[\'name\']] = len(channel[\'samples\'][0][\'data\'])\n            for sample in channel[\'samples\']:\n                self.samples.append(sample[\'name\'])\n                for modifier_def in sample[\'modifiers\']:\n                    self.parameters.append(modifier_def[\'name\'])\n                    self.modifiers.append(\n                        (\n                            modifier_def[\'name\'],  # mod name\n                            modifier_def[\'type\'],  # mod type\n                        )\n                    )\n\n        self.channels = sorted(list(set(self.channels)))\n        self.samples = sorted(list(set(self.samples)))\n        self.parameters = sorted(list(set(self.parameters)))\n        self.modifiers = sorted(list(set(self.modifiers)))\n'"
src/pyhf/patchset.py,0,"b'""""""\npyhf patchset provides a user-friendly interface for interacting with patchsets.\n""""""\nimport logging\nimport jsonpatch\nfrom . import exceptions\nfrom . import utils\nfrom .workspace import Workspace\n\nlog = logging.getLogger(__name__)\n\n\nclass Patch(jsonpatch.JsonPatch):\n    """"""\n    A way to store a patch definition as part of a patchset (:class:`~pyhf.patchset.PatchSet`).\n\n    It contains :attr:`~pyhf.patchset.Patch.metadata` about the Patch itself:\n\n      * a descriptive :attr:`~pyhf.patchset.Patch.name`\n      * a list of the :attr:`~pyhf.patchset.Patch.values` for each dimension in the phase-space the associated :class:`~pyhf.patchset.PatchSet` is defined for, see :attr:`~pyhf.patchset.PatchSet.labels`\n\n    In addition to the above metadata, the Patch object behaves like the underlying :class:`jsonpatch.JsonPatch`.\n    """"""\n\n    def __init__(self, spec):\n        """"""\n        Construct a Patch.\n\n        Args:\n            spec (`jsonable`): The patch JSON specification\n\n        Returns:\n            patch (:class:`~pyhf.patchset.Patch`): The Patch instance.\n\n        """"""\n        super(Patch, self).__init__(spec[\'patch\'])\n        self._metadata = spec[\'metadata\']\n\n    @property\n    def metadata(self):\n        """""" The metadata of the patch """"""\n        return self._metadata\n\n    @property\n    def name(self):\n        """""" The name of the patch """"""\n        return self.metadata[\'name\']\n\n    @property\n    def values(self):\n        """""" The values of the associated labels for the patch """"""\n        return tuple(self.metadata[\'values\'])\n\n    def __repr__(self):\n        """""" Representation of the object """"""\n        module = type(self).__module__\n        qualname = type(self).__qualname__\n        return f""<{module}.{qualname} object \'{self.name}{self.values}\' at {hex(id(self))}>""\n\n    def __eq__(self, other):\n        """""" Equality for subclass with new attributes """"""\n        if not isinstance(other, Patch):\n            return False\n        return (\n            jsonpatch.JsonPatch.__eq__(self, other) and self.metadata == other.metadata\n        )\n\n\nclass PatchSet(object):\n    """"""\n    A way to store a collection of patches (:class:`~pyhf.patchset.Patch`).\n\n    It contains :attr:`~PatchSet.metadata` about the PatchSet itself:\n\n      * a high-level :attr:`~pyhf.patchset.PatchSet.description` of what the patches represent or the analysis it is for\n      * a list of :attr:`~pyhf.patchset.PatchSet.references` where the patchset is sourced from (e.g. hepdata)\n      * a list of :attr:`~pyhf.patchset.PatchSet.digests` corresponding to the background-only workspace the patchset was made for\n      * the :attr:`~pyhf.patchset.PatchSet.labels` of the dimensions of the phase-space for what the patches cover\n\n    In addition to the above metadata, the PatchSet object behaves like a:\n\n      * smart list allowing you to iterate over all the patches defined\n      * smart dictionary allowing you to access a patch by the patch name or the patch values\n\n    The below example shows various ways one can interact with a :class:`PatchSet` object.\n\n    Example:\n        >>> import pyhf\n        >>> patchset = pyhf.PatchSet({\n        ...     ""metadata"": {\n        ...         ""references"": { ""hepdata"": ""ins1234567"" },\n        ...         ""description"": ""example patchset"",\n        ...         ""digests"": { ""md5"": ""098f6bcd4621d373cade4e832627b4f6"" },\n        ...         ""labels"": [""x"", ""y""]\n        ...     },\n        ...     ""patches"": [\n        ...         {\n        ...             ""metadata"": {\n        ...                 ""name"": ""patch_name_for_2100x_800y"",\n        ...                 ""values"": [2100, 800]\n        ...             },\n        ...             ""patch"": [\n        ...                 {\n        ...                     ""op"": ""add"",\n        ...                     ""path"": ""/foo/0/bar"",\n        ...                     ""value"": {\n        ...                         ""foo"": [1.0]\n        ...                     }\n        ...                 }\n        ...             ]\n        ...         }\n        ...     ],\n        ...     ""version"": ""1.0.0""\n        ... })\n        ...\n        >>> patchset.version\n        \'1.0.0\'\n        >>> patchset.references\n        {\'hepdata\': \'ins1234567\'}\n        >>> patchset.description\n        \'example patchset\'\n        >>> patchset.digests\n        {\'md5\': \'098f6bcd4621d373cade4e832627b4f6\'}\n        >>> patchset.labels\n        [\'x\', \'y\']\n        >>> patchset.patches\n        [<pyhf.patchset.Patch object \'patch_name_for_2100x_800y(2100, 800)\' at 0x...>]\n        >>> patchset[\'patch_name_for_2100x_800y\']\n        <pyhf.patchset.Patch object \'patch_name_for_2100x_800y(2100, 800)\' at 0x...>\n        >>> patchset[(2100,800)]\n        <pyhf.patchset.Patch object \'patch_name_for_2100x_800y(2100, 800)\' at 0x...>\n        >>> patchset[[2100,800]]\n        <pyhf.patchset.Patch object \'patch_name_for_2100x_800y(2100, 800)\' at 0x...>\n        >>> patchset[2100,800]\n        <pyhf.patchset.Patch object \'patch_name_for_2100x_800y(2100, 800)\' at 0x...>\n        >>> for patch in patchset:\n        ...     print(patch.name)\n        ...\n        patch_name_for_2100x_800y\n        >>> len(patchset)\n        1\n    """"""\n\n    def __init__(self, spec, **config_kwargs):\n        """"""\n        Construct a PatchSet.\n\n        Args:\n            spec (`jsonable`): The patchset JSON specification\n            config_kwargs: Possible keyword arguments for the patchset validation\n\n        Returns:\n            patchset (:class:`~pyhf.patchset.PatchSet`): The PatchSet instance.\n\n        """"""\n        self.schema = config_kwargs.pop(\'schema\', \'patchset.json\')\n        self._version = config_kwargs.pop(\'version\', spec.get(\'version\', None))\n\n        # run jsonschema validation of input specification against the (provided) schema\n        log.info(f""Validating spec against schema: {self.schema}"")\n        utils.validate(spec, self.schema, version=self._version)\n\n        # set properties based on metadata\n        self._metadata = spec[\'metadata\']\n\n        # list of all patch objects\n        self._patches = []\n        # look-up table for retrieving patch by name or values\n        self._patches_by_key = {\'name\': {}, \'values\': {}}\n\n        # inflate all patches\n        for patchspec in spec[\'patches\']:\n            patch = Patch(patchspec)\n\n            if patch.name in self._patches_by_key:\n                raise exceptions.InvalidPatchSet(\n                    f\'Multiple patches were defined by name for {patch}.\'\n                )\n\n            if patch.values in self._patches_by_key:\n                raise exceptions.InvalidPatchSet(\n                    f\'Multiple patches were defined by values for {patch}.\'\n                )\n\n            if len(patch.values) != len(self.labels):\n                raise exceptions.InvalidPatchSet(\n                    f\'Incompatible number of values ({len(patch.values)} for {patch} in patchset. Expected {len(self.labels)}.\'\n                )\n\n            # all good, register patch\n            self._patches.append(patch)\n            # register lookup keys for the patch\n            self._patches_by_key[patch.name] = patch\n            self._patches_by_key[patch.values] = patch\n\n    @property\n    def version(self):\n        """""" The version of the PatchSet """"""\n        return self._version\n\n    @property\n    def metadata(self):\n        """""" The metadata of the PatchSet """"""\n        return self._metadata\n\n    @property\n    def references(self):\n        """""" The references in the PatchSet metadata """"""\n        return self.metadata[\'references\']\n\n    @property\n    def description(self):\n        """""" The description in the PatchSet metadata """"""\n        return self.metadata[\'description\']\n\n    @property\n    def digests(self):\n        """""" The digests in the PatchSet metadata """"""\n        return self.metadata[\'digests\']\n\n    @property\n    def labels(self):\n        """""" The labels in the PatchSet metadata """"""\n        return self.metadata[\'labels\']\n\n    @property\n    def patches(self):\n        """""" The patches in the PatchSet """"""\n        return self._patches\n\n    def __repr__(self):\n        """""" Representation of the object """"""\n        module = type(self).__module__\n        qualname = type(self).__qualname__\n        return f""<{module}.{qualname} object with {len(self.patches)} patch{\'es\' if len(self.patches) != 1 else \'\'} at {hex(id(self))}>""\n\n    def __getitem__(self, key):\n        """"""\n        Access the patch in the patchset by the specified key, either by name or by values.\n\n        Raises:\n            ~pyhf.exceptions.InvalidPatchLookup: if the provided patch name is not in the patchset\n\n        Returns:\n            patch (:class:`~pyhf.patchset.Patch`): The patch associated with the specified key\n        """"""\n        # might be specified as a list, convert to hashable tuple instead for lookup\n        if isinstance(key, list):\n            key = tuple(key)\n        try:\n            return self._patches_by_key[key]\n        except KeyError:\n            raise exceptions.InvalidPatchLookup(\n                f\'No patch associated with ""{key}"" is defined in patchset.\'\n            )\n\n    def __iter__(self):\n        """"""\n        Iterate over the defined patches in the patchset.\n\n        Returns:\n            iterable (:obj:`iter`): An iterable over the list of patches in the patchset.\n        """"""\n        return iter(self.patches)\n\n    def __len__(self):\n        """"""\n        The number of patches in the patchset.\n\n        Returns:\n            quantity (:obj:`int`): The number of patches in the patchset.\n        """"""\n        return len(self.patches)\n\n    def verify(self, spec):\n        """"""\n        Verify the patchset digests against a background-only workspace specification. Verified if no exception was raised.\n\n        Args:\n            spec (:class:`~pyhf.workspace.Workspace`): The workspace specification to verify the patchset against.\n\n        Raises:\n            ~pyhf.exceptions.PatchSetVerificationError: if the patchset cannot be verified against the workspace specification\n\n        Returns:\n            None\n        """"""\n        for hash_alg, digest in self.digests.items():\n            digest_calc = utils.digest(spec, algorithm=hash_alg)\n            if not digest_calc == digest:\n                raise exceptions.PatchSetVerificationError(\n                    f""The digest verification failed for hash algorithm \'{hash_alg}\'. Expected: {digest}. Got: {digest_calc}""\n                )\n\n    def apply(self, spec, key):\n        """"""\n        Apply the patch associated with the key to the background-only workspace specificatiom.\n\n        Args:\n            spec (:class:`~pyhf.workspace.Workspace`): The workspace specification to verify the patchset against.\n            key (:obj:`str` or :obj:`tuple` of :obj:`int`/:obj:`float`): The key to look up the associated patch - either a name or a set of values.\n\n        Raises:\n            ~pyhf.exceptions.InvalidPatchLookup: if the provided patch name is not in the patchset\n            ~pyhf.exceptions.PatchSetVerificationError: if the patchset cannot be verified against the workspace specification\n\n        Returns:\n            workspace (:class:`~pyhf.workspace.Workspace`): The background-only workspace with the patch applied.\n        """"""\n        self.verify(spec)\n        return Workspace(self[key].apply(spec))\n'"
src/pyhf/pdf.py,0,"b'""""""The main module of pyhf.""""""\n\nimport copy\nimport logging\n\nfrom . import get_backend, default_backend\nfrom . import exceptions\nfrom . import modifiers\nfrom . import utils\nfrom . import events\nfrom . import probability as prob\nfrom .constraints import gaussian_constraint_combined, poisson_constraint_combined\nfrom .parameters import reduce_paramsets_requirements, ParamViewer\nfrom .tensor.common import _TensorViewer, _tensorviewer_from_sizes\nfrom .mixins import _ChannelSummaryMixin\n\nlog = logging.getLogger(__name__)\n\n\ndef _paramset_requirements_from_channelspec(spec, channel_nbins):\n    # bookkeep all requirements for paramsets we need to build\n    _paramsets_requirements = {}\n    # need to keep track in which order we added the constraints\n    # so that we can generate correctly-ordered data\n    for channel in spec[\'channels\']:\n        for sample in channel[\'samples\']:\n            if len(sample[\'data\']) != channel_nbins[channel[\'name\']]:\n                raise exceptions.InvalidModel(\n                    \'The sample {0:s} has {1:d} bins, but the channel it belongs to ({2:s}) has {3:d} bins.\'.format(\n                        sample[\'name\'],\n                        len(sample[\'data\']),\n                        channel[\'name\'],\n                        channel_nbins[channel[\'name\']],\n                    )\n                )\n            for modifier_def in sample[\'modifiers\']:\n                # get the paramset requirements for the given modifier. If\n                # modifier does not exist, we\'ll have a KeyError\n                try:\n                    paramset_requirements = modifiers.registry[\n                        modifier_def[\'type\']\n                    ].required_parset(sample[\'data\'], modifier_def[\'data\'])\n                except KeyError:\n                    log.exception(\n                        \'Modifier not implemented yet (processing {0:s}). Available modifiers: {1}\'.format(\n                            modifier_def[\'type\'], modifiers.registry.keys()\n                        )\n                    )\n                    raise exceptions.InvalidModifier()\n\n                # check the shareability (e.g. for shapesys for example)\n                is_shared = paramset_requirements[\'is_shared\']\n                if not (is_shared) and modifier_def[\'name\'] in _paramsets_requirements:\n                    raise ValueError(\n                        ""Trying to add unshared-paramset but other paramsets exist with the same name.""\n                    )\n                if is_shared and not (\n                    _paramsets_requirements.get(\n                        modifier_def[\'name\'], [{\'is_shared\': True}]\n                    )[0][\'is_shared\']\n                ):\n                    raise ValueError(\n                        ""Trying to add shared-paramset but other paramset of same name is indicated to be unshared.""\n                    )\n                _paramsets_requirements.setdefault(modifier_def[\'name\'], []).append(\n                    paramset_requirements\n                )\n    return _paramsets_requirements\n\n\ndef _paramset_requirements_from_modelspec(spec, channel_nbins):\n    _paramsets_requirements = _paramset_requirements_from_channelspec(\n        spec, channel_nbins\n    )\n\n    # build up a dictionary of the parameter configurations provided by the user\n    _paramsets_user_configs = {}\n    for parameter in spec.get(\'parameters\', []):\n        if parameter[\'name\'] in _paramsets_user_configs:\n            raise exceptions.InvalidModel(\n                \'Multiple parameter configurations for {} were found.\'.format(\n                    parameter[\'name\']\n                )\n            )\n        _paramsets_user_configs[parameter.pop(\'name\')] = parameter\n\n    _reqs = reduce_paramsets_requirements(\n        _paramsets_requirements, _paramsets_user_configs\n    )\n\n    _sets = {}\n    for param_name, paramset_requirements in _reqs.items():\n        paramset_type = paramset_requirements.get(\'paramset_type\')\n        paramset = paramset_type(**paramset_requirements)\n        _sets[param_name] = paramset\n    return _sets\n\n\ndef _nominal_and_modifiers_from_spec(config, spec):\n    default_data_makers = {\n        \'histosys\': lambda: {\'hi_data\': [], \'lo_data\': [], \'nom_data\': [], \'mask\': [],},\n        \'lumi\': lambda: {\'mask\': []},\n        \'normsys\': lambda: {\'hi\': [], \'lo\': [], \'nom_data\': [], \'mask\': []},\n        \'normfactor\': lambda: {\'mask\': []},\n        \'shapefactor\': lambda: {\'mask\': []},\n        \'shapesys\': lambda: {\'mask\': [], \'uncrt\': [], \'nom_data\': []},\n        \'staterror\': lambda: {\'mask\': [], \'uncrt\': [], \'nom_data\': []},\n    }\n\n    # the mega-channel will consist of mega-samples that subscribe to\n    # mega-modifiers. i.e. while in normal histfactory, each sample might\n    # be affected by some modifiers and some not, here we change it so that\n    # samples are affected by all modifiers, but we set up the modifier\n    # data such that the application of the modifier does not actually\n    # change the bin value for bins that are not originally affected by\n    # that modifier\n    #\n    # We don\'t actually set up the modifier data here for no-ops, but we do\n    # set up the entire structure\n    mega_mods = {}\n    for m, mtype in config.modifiers:\n        for s in config.samples:\n            key = \'{}/{}\'.format(mtype, m)\n            mega_mods.setdefault(key, {})[s] = {\n                \'type\': mtype,\n                \'name\': m,\n                \'data\': default_data_makers[mtype](),\n            }\n\n    # helper maps channel-name/sample-name to pairs of channel-sample structs\n    helper = {}\n    for c in spec[\'channels\']:\n        for s in c[\'samples\']:\n            helper.setdefault(c[\'name\'], {})[s[\'name\']] = (c, s)\n\n    mega_samples = {}\n    for s in config.samples:\n        mega_nom = []\n        for c in config.channels:\n            defined_samp = helper.get(c, {}).get(s)\n            defined_samp = None if not defined_samp else defined_samp[1]\n            # set nominal to 0 for channel/sample if the pair doesn\'t exist\n            nom = (\n                defined_samp[\'data\']\n                if defined_samp\n                else [0.0] * config.channel_nbins[c]\n            )\n            mega_nom += nom\n            defined_mods = (\n                {\n                    \'{}/{}\'.format(x[\'type\'], x[\'name\']): x\n                    for x in defined_samp[\'modifiers\']\n                }\n                if defined_samp\n                else {}\n            )\n            for m, mtype in config.modifiers:\n                key = \'{}/{}\'.format(mtype, m)\n                # this is None if modifier doesn\'t affect channel/sample.\n                thismod = defined_mods.get(key)\n                # print(\'key\',key,thismod[\'data\'] if thismod else None)\n                if mtype == \'histosys\':\n                    lo_data = thismod[\'data\'][\'lo_data\'] if thismod else nom\n                    hi_data = thismod[\'data\'][\'hi_data\'] if thismod else nom\n                    maskval = True if thismod else False\n                    mega_mods[key][s][\'data\'][\'lo_data\'] += lo_data\n                    mega_mods[key][s][\'data\'][\'hi_data\'] += hi_data\n                    mega_mods[key][s][\'data\'][\'nom_data\'] += nom\n                    mega_mods[key][s][\'data\'][\'mask\'] += [maskval] * len(\n                        nom\n                    )  # broadcasting\n                elif mtype == \'normsys\':\n                    maskval = True if thismod else False\n                    lo_factor = thismod[\'data\'][\'lo\'] if thismod else 1.0\n                    hi_factor = thismod[\'data\'][\'hi\'] if thismod else 1.0\n                    mega_mods[key][s][\'data\'][\'nom_data\'] += [1.0] * len(nom)\n                    mega_mods[key][s][\'data\'][\'lo\'] += [lo_factor] * len(\n                        nom\n                    )  # broadcasting\n                    mega_mods[key][s][\'data\'][\'hi\'] += [hi_factor] * len(nom)\n                    mega_mods[key][s][\'data\'][\'mask\'] += [maskval] * len(\n                        nom\n                    )  # broadcasting\n                elif mtype in [\'normfactor\', \'shapefactor\', \'lumi\']:\n                    maskval = True if thismod else False\n                    mega_mods[key][s][\'data\'][\'mask\'] += [maskval] * len(\n                        nom\n                    )  # broadcasting\n                elif mtype in [\'shapesys\', \'staterror\']:\n                    uncrt = thismod[\'data\'] if thismod else [0.0] * len(nom)\n                    if mtype == \'shapesys\':\n                        maskval = [(x > 0 and y > 0) for x, y in zip(uncrt, nom)]\n                    else:\n                        maskval = [True if thismod else False] * len(nom)\n                    mega_mods[key][s][\'data\'][\'mask\'] += maskval\n                    mega_mods[key][s][\'data\'][\'uncrt\'] += uncrt\n                    mega_mods[key][s][\'data\'][\'nom_data\'] += nom\n\n        sample_dict = {\'name\': \'mega_{}\'.format(s), \'nom\': mega_nom}\n        mega_samples[s] = sample_dict\n\n    nominal_rates = default_backend.astensor(\n        [mega_samples[s][\'nom\'] for s in config.samples]\n    )\n    _nominal_rates = default_backend.reshape(\n        nominal_rates,\n        (\n            1,  # modifier dimension.. nominal_rates is the base\n            len(config.samples),\n            1,  # alphaset dimension\n            sum(list(config.channel_nbins.values())),\n        ),\n    )\n\n    return mega_mods, _nominal_rates\n\n\nclass _ModelConfig(_ChannelSummaryMixin):\n    def __init__(self, spec, **config_kwargs):\n        super(_ModelConfig, self).__init__(channels=spec[\'channels\'])\n        _required_paramsets = _paramset_requirements_from_modelspec(\n            spec, self.channel_nbins\n        )\n\n        poi_name = config_kwargs.pop(\'poi_name\', \'mu\')\n\n        default_modifier_settings = {\'normsys\': {\'interpcode\': \'code1\'}}\n        self.modifier_settings = config_kwargs.pop(\n            \'modifier_settings\', default_modifier_settings\n        )\n\n        if config_kwargs:\n            raise KeyError(\n                f""""""Unexpected keyword argument(s): \'{""\', \'"".join(config_kwargs.keys())}\'""""""\n            )\n\n        self.par_map = {}\n        self.par_order = []\n        self.poi_name = None\n        self.poi_index = None\n        self.auxdata = []\n        self.auxdata_order = []\n\n        self._create_and_register_paramsets(_required_paramsets)\n        self.set_poi(poi_name)\n        self.npars = len(self.suggested_init())\n        self.nmaindata = sum(self.channel_nbins.values())\n\n    def suggested_init(self):\n        init = []\n        for name in self.par_order:\n            init = init + self.par_map[name][\'paramset\'].suggested_init\n        return init\n\n    def suggested_bounds(self):\n        bounds = []\n        for name in self.par_order:\n            bounds = bounds + self.par_map[name][\'paramset\'].suggested_bounds\n        return bounds\n\n    def par_slice(self, name):\n        return self.par_map[name][\'slice\']\n\n    def param_set(self, name):\n        return self.par_map[name][\'paramset\']\n\n    def set_poi(self, name):\n        if name not in [x for x, _ in self.modifiers]:\n            raise exceptions.InvalidModel(\n                ""The parameter of interest \'{0:s}\' cannot be fit as it is not declared in the model specification."".format(\n                    name\n                )\n            )\n        s = self.par_slice(name)\n        assert s.stop - s.start == 1\n        self.poi_name = name\n        self.poi_index = s.start\n\n    def _create_and_register_paramsets(self, required_paramsets):\n        next_index = 0\n        for param_name, paramset in required_paramsets.items():\n            log.info(\n                \'adding modifier %s (%s new nuisance parameters)\',\n                param_name,\n                paramset.n_parameters,\n            )\n\n            sl = slice(next_index, next_index + paramset.n_parameters)\n            next_index = next_index + paramset.n_parameters\n\n            self.par_order.append(param_name)\n            self.par_map[param_name] = {\'slice\': sl, \'paramset\': paramset}\n\n\nclass _ConstraintModel(object):\n    """"""Factory class to create pdfs for the constraint terms.""""""\n\n    def __init__(self, config, batch_size):\n        self.batch_size = batch_size\n        self.config = config\n\n        self.constraints_gaussian = gaussian_constraint_combined(\n            config, batch_size=self.batch_size\n        )\n        self.constraints_poisson = poisson_constraint_combined(\n            config, batch_size=self.batch_size\n        )\n\n        self.viewer_aux = ParamViewer(\n            (self.batch_size or 1, self.config.npars),\n            self.config.par_map,\n            self.config.auxdata_order,\n        )\n\n        assert self.constraints_gaussian.batch_size == self.batch_size\n        assert self.constraints_poisson.batch_size == self.batch_size\n\n        indices = []\n        if self.constraints_gaussian.has_pdf():\n            indices.append(self.constraints_gaussian._normal_data)\n        if self.constraints_poisson.has_pdf():\n            indices.append(self.constraints_poisson._poisson_data)\n        if self.has_pdf():\n            self.constraints_tv = _TensorViewer(indices, self.batch_size)\n\n    def has_pdf(self):\n        """"""\n        Indicate whether this model has a constraint.\n\n        Returns:\n            Bool: Whether the model has a constraint term\n\n        """"""\n        return self.constraints_gaussian.has_pdf() or self.constraints_poisson.has_pdf()\n\n    def make_pdf(self, pars):\n        """"""\n        Construct a pdf object for a given set of parameter values.\n\n        Args:\n            pars (`tensor`): The model parameters\n\n        Returns:\n            pdf: A distribution object implementing the constraint pdf of HistFactory.\n                 Either a Poissonn, a Gaussian or a joint pdf of both depending on the\n                 constraints used in the specification.\n\n        """"""\n        pdfobjs = []\n\n        gaussian_pdf = self.constraints_gaussian.make_pdf(pars)\n        if gaussian_pdf:\n            pdfobjs.append(gaussian_pdf)\n\n        poisson_pdf = self.constraints_poisson.make_pdf(pars)\n        if poisson_pdf:\n            pdfobjs.append(poisson_pdf)\n\n        if pdfobjs:\n            simpdf = prob.Simultaneous(pdfobjs, self.constraints_tv, self.batch_size)\n            return simpdf\n\n    def logpdf(self, auxdata, pars):\n        """"""\n        Compute the logarithm of the value of the probability density.\n\n        Args:\n            auxdata (`tensor`): The auxiliary data (a subset of the full data in a HistFactory model)\n            pars (`tensor`): The model parameters\n\n        Returns:\n            Tensor: The log of the pdf value\n\n        """"""\n        simpdf = self.make_pdf(pars)\n        return simpdf.log_prob(auxdata)\n\n\nclass _MainModel(object):\n    """"""Factory class to create pdfs for the main measurement.""""""\n\n    def __init__(self, config, mega_mods, nominal_rates, batch_size):\n        self.config = config\n        self._factor_mods = [\n            modtype\n            for modtype, mod in modifiers.uncombined.items()\n            if mod.op_code == \'multiplication\'\n        ]\n        self._delta_mods = [\n            modtype\n            for modtype, mod in modifiers.uncombined.items()\n            if mod.op_code == \'addition\'\n        ]\n        self.batch_size = batch_size\n\n        self._nominal_rates = default_backend.tile(\n            nominal_rates, (1, 1, self.batch_size or 1, 1)\n        )\n\n        self.modifiers_appliers = {\n            k: c(\n                [x for x in config.modifiers if x[1] == k],  # x[1] is mtype\n                config,\n                mega_mods,\n                batch_size=self.batch_size,\n                **config.modifier_settings.get(k, {}),\n            )\n            for k, c in modifiers.combined.items()\n        }\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        self.nominal_rates = tensorlib.astensor(self._nominal_rates)\n\n    def has_pdf(self):\n        """"""\n        Indicate whether the main model exists.\n\n        Returns:\n            Bool: Whether the model has a Main Model component (yes it does)\n\n        """"""\n        return True\n\n    def make_pdf(self, pars):\n        lambdas_data = self._expected_data(pars)\n        return prob.Independent(prob.Poisson(lambdas_data))\n\n    def logpdf(self, maindata, pars):\n        """"""\n        Compute the logarithm of the value of the probability density.\n\n        Args:\n            maindata (`tensor`): The main channnel data (a subset of the full data in a HistFactory model)\n            pars (`tensor`): The model parameters\n\n        Returns:\n            Tensor: The log of the pdf value\n\n        """"""\n        return self.make_pdf(pars).log_prob(maindata)\n\n    def _modifications(self, pars):\n        deltas = list(\n            filter(\n                lambda x: x is not None,\n                [self.modifiers_appliers[k].apply(pars) for k in self._delta_mods],\n            )\n        )\n        factors = list(\n            filter(\n                lambda x: x is not None,\n                [self.modifiers_appliers[k].apply(pars) for k in self._factor_mods],\n            )\n        )\n\n        return deltas, factors\n\n    def _expected_data(self, pars):\n        """"""\n        Compute the expected rates for given values of parameters.\n\n        For a single channel single sample, we compute:\n\n            Pois(d | fac(pars) * (delta(pars) + nom) ) * Gaus(a | pars[is_gaus], sigmas) * Pois(a * cfac | pars[is_poi] * cfac)\n\n        where:\n            - delta(pars) is the result of an apply(pars) of combined modifiers\n              with \'addition\' op_code\n            - factor(pars) is the result of apply(pars) of combined modifiers\n              with \'multiplication\' op_code\n            - pars[is_gaus] are the subset of parameters that are constrained by\n              gauss (with sigmas accordingly, some of which are computed by\n              modifiers)\n            - pars[is_pois] are the poissons and their rates (they come with\n              their own additional factors unrelated to factor(pars) which are\n              also computed by the finalize() of the modifier)\n\n        So in the end we only make 3 calls to pdfs\n\n            1. The pdf of data and modified rates\n            2. All Gaussian constraint as one call\n            3. All Poisson constraints as one call\n\n        """"""\n        tensorlib, _ = get_backend()\n        deltas, factors = self._modifications(pars)\n\n        allsum = tensorlib.concatenate(deltas + [self.nominal_rates])\n\n        nom_plus_delta = tensorlib.sum(allsum, axis=0)\n        nom_plus_delta = tensorlib.reshape(\n            nom_plus_delta, (1,) + tensorlib.shape(nom_plus_delta)\n        )\n\n        allfac = tensorlib.concatenate(factors + [nom_plus_delta])\n\n        newbysample = tensorlib.product(allfac, axis=0)\n        newresults = tensorlib.sum(newbysample, axis=0)\n        if self.batch_size is None:\n            return newresults[0]\n        return newresults\n\n\nclass Model(object):\n    """"""The main pyhf model class.""""""\n\n    def __init__(self, spec, batch_size=None, **config_kwargs):\n        """"""\n        Construct a HistFactory Model.\n\n        Args:\n            spec (`jsonable`): The HistFactory JSON specification\n            batch_size (`None` or `int`): Number of simultaneous (batched) Models to compute.\n            config_kwargs: Possible keyword arguments for the model configuration\n\n        Returns:\n            model (`Model`): The Model instance.\n\n        """"""\n        self.batch_size = batch_size\n        self.spec = copy.deepcopy(spec)  # may get modified by config\n        self.schema = config_kwargs.pop(\'schema\', \'model.json\')\n        self.version = config_kwargs.pop(\'version\', None)\n        # run jsonschema validation of input specification against the (provided) schema\n        log.info(""Validating spec against schema: {0:s}"".format(self.schema))\n        utils.validate(self.spec, self.schema, version=self.version)\n        # build up our representation of the specification\n        self.config = _ModelConfig(self.spec, **config_kwargs)\n\n        mega_mods, _nominal_rates = _nominal_and_modifiers_from_spec(\n            self.config, self.spec\n        )\n        self.main_model = _MainModel(\n            self.config,\n            mega_mods=mega_mods,\n            nominal_rates=_nominal_rates,\n            batch_size=self.batch_size,\n        )\n\n        # this is tricky, must happen before constraint\n        # terms try to access auxdata but after\n        # combined mods have been created that\n        # set the aux data\n        for k in sorted(self.config.par_map.keys()):\n            parset = self.config.param_set(k)\n            if hasattr(parset, \'pdf_type\'):  # is constrained\n                self.config.auxdata += parset.auxdata\n                self.config.auxdata_order.append(k)\n        self.config.nauxdata = len(self.config.auxdata)\n\n        self.constraint_model = _ConstraintModel(\n            config=self.config, batch_size=self.batch_size\n        )\n\n        sizes = []\n        if self.main_model.has_pdf():\n            sizes.append(self.config.nmaindata)\n        if self.constraint_model.has_pdf():\n            sizes.append(self.config.nauxdata)\n        self.fullpdf_tv = _tensorviewer_from_sizes(\n            sizes, [\'main\', \'aux\'], self.batch_size\n        )\n\n    def expected_auxdata(self, pars):\n        """"""\n        Compute the expected value of the auxiliary measurements.\n\n        Args:\n            pars (`tensor`): The parameter values\n\n        Returns:\n            Tensor: The expected data of the auxiliary pdf\n\n        """"""\n        return self.make_pdf(pars)[1].expected_data()\n\n    def _modifications(self, pars):\n        return self.main_model._modifications(pars)\n\n    @property\n    def nominal_rates(self):\n        """"""Nominal value of bin rates of the main model.""""""\n        return self.main_model.nominal_rates\n\n    def expected_actualdata(self, pars):\n        """"""\n        Compute the expected value of the main model.\n\n        Args:\n            pars (`tensor`): The parameter values\n\n        Returns:\n            Tensor: The expected data of the main model (no auxiliary data)\n\n        """"""\n        return self.make_pdf(pars)[0].expected_data()\n\n    def expected_data(self, pars, include_auxdata=True):\n        """"""\n        Compute the expected value of the main model\n\n        Args:\n            pars (`tensor`): The parameter values\n\n        Returns:\n            Tensor: The expected data of the main and auxiliary model\n\n        """"""\n        tensorlib, _ = get_backend()\n        pars = tensorlib.astensor(pars)\n        if not include_auxdata:\n            return self.make_pdf(pars)[0].expected_data()\n        return self.make_pdf(pars).expected_data()\n\n    def constraint_logpdf(self, auxdata, pars):\n        """"""\n        Compute the log value of the constraint pdf.\n\n        Args:\n            auxdata (`tensor`): The auxiliary measurement data\n            pars (`tensor`): The parameter values\n\n        Returns:\n            Tensor: The log density value\n\n        """"""\n        return self.make_pdf(pars)[1].log_prob(auxdata)\n\n    def mainlogpdf(self, maindata, pars):\n        """"""\n        Compute the log value of the main term.\n\n        Args:\n            maindata (`tensor`): The main measurement data\n            pars (`tensor`): The parameter values\n\n        Returns:\n            Tensor: The log density value\n\n        """"""\n        return self.make_pdf(pars)[0].log_prob(maindata)\n\n    def make_pdf(self, pars):\n        """"""\n        Construct a pdf object for a given set of parameter values.\n\n        Args:\n            pars (`tensor`): The model parameters\n\n        Returns:\n            pdf: A distribution object implementing the main measurement pdf of HistFactory\n\n        """"""\n        tensorlib, _ = get_backend()\n\n        pdfobjs = []\n        mainpdf = self.main_model.make_pdf(pars)\n        if mainpdf:\n            pdfobjs.append(mainpdf)\n        constraintpdf = self.constraint_model.make_pdf(pars)\n        if constraintpdf:\n            pdfobjs.append(constraintpdf)\n\n        simpdf = prob.Simultaneous(pdfobjs, self.fullpdf_tv, self.batch_size)\n        return simpdf\n\n    def logpdf(self, pars, data):\n        """"""\n        Compute the log value of the full density.\n\n        Args:\n            pars (`tensor`): The parameter values\n            data (`tensor`): The measurement data\n\n        Returns:\n            Tensor: The log density value\n\n        """"""\n        try:\n            tensorlib, _ = get_backend()\n            pars, data = tensorlib.astensor(pars), tensorlib.astensor(data)\n            # Verify parameter and data shapes\n            if pars.shape[-1] != self.config.npars:\n                raise exceptions.InvalidPdfParameters(\n                    \'eval failed as pars has len {} but {} was expected\'.format(\n                        pars.shape[-1], self.config.npars\n                    )\n                )\n\n            if data.shape[-1] != self.nominal_rates.shape[-1] + len(\n                self.config.auxdata\n            ):\n                raise exceptions.InvalidPdfData(\n                    \'eval failed as data has len {} but {} was expected\'.format(\n                        data.shape[-1], self.config.nmaindata + self.config.nauxdata\n                    )\n                )\n\n            result = self.make_pdf(pars).log_prob(data)\n\n            if (\n                not self.batch_size\n            ):  # force to be not scalar, should we changed with #522\n                return tensorlib.reshape(result, (1,))\n            return result\n        except:\n            log.error(\n                \'eval failed for data {} pars: {}\'.format(\n                    tensorlib.tolist(data), tensorlib.tolist(pars)\n                )\n            )\n            raise\n\n    def pdf(self, pars, data):\n        """"""\n        Compute the density at a given observed point in data space of the full model.\n\n        Args:\n            pars (`tensor`): The parameter values\n            data (`tensor`): The measurement data\n\n        Returns:\n            Tensor: The density value\n\n        """"""\n        tensorlib, _ = get_backend()\n        return tensorlib.exp(self.logpdf(pars, data))\n'"
src/pyhf/probability.py,0,"b'""""""The probability density function module.""""""\nfrom . import get_backend\n\n\nclass _SimpleDistributionMixin(object):\n    """"""The mixin class for distributions.""""""\n\n    def log_prob(self, value):\n        r""""""\n        The log of the probability density function at the given value.\n\n        Args:\n            value (`tensor` or `float`): The value at which to evaluate the distribution\n\n        Returns:\n            Tensor: The value of :math:`\\log(f\\left(x\\middle|\\theta\\right))` for :math:`x=`:code:`value`\n\n        """"""\n        return self._pdf.log_prob(value)\n\n    def expected_data(self):\n        r""""""\n        The expectation value of the probability density function.\n\n        Returns:\n            Tensor: The expectation value of the distribution :math:`\\mathrm{E}\\left[f(\\theta)\\right]`\n\n        """"""\n        return self._pdf.expected_data()\n\n    def sample(self, sample_shape=()):\n        r""""""\n        The collection of values sampled from the probability density function.\n\n        Args:\n            sample_shape (`tuple`): The shape of the sample to be returned\n\n        Returns:\n            Tensor: The values :math:`x \\sim f(\\theta)` where :math:`x` has shape :code:`sample_shape`\n\n        """"""\n        return self._pdf.sample(sample_shape)\n\n\nclass Poisson(_SimpleDistributionMixin):\n    r""""""\n    The Poisson distribution with rate parameter :code:`rate`.\n\n    Example:\n        >>> import pyhf\n        >>> rates = pyhf.tensorlib.astensor([5, 8])\n        >>> pyhf.probability.Poisson(rates)\n        <pyhf.probability.Poisson object at 0x...>\n\n    """"""\n\n    def __init__(self, rate):\n        """"""\n        Args:\n            rate (`tensor` or `float`): The mean of the Poisson distribution (the expected number of events)\n        """"""\n        tensorlib, _ = get_backend()\n        self.rate = rate\n        self._pdf = tensorlib.poisson_dist(rate)\n\n    def expected_data(self):\n        r""""""\n        The expectation value of the Poisson distribution.\n\n        Example:\n            >>> import pyhf\n            >>> rates = pyhf.tensorlib.astensor([5, 8])\n            >>> poissons = pyhf.probability.Poisson(rates)\n            >>> poissons.expected_data()\n            array([5., 8.])\n\n        Returns:\n            Tensor: The mean of the Poisson distribution (which is the :code:`rate`)\n\n        """"""\n        return self.rate\n\n\nclass Normal(_SimpleDistributionMixin):\n    r""""""\n    The Normal distribution with mean :code:`loc` and standard deviation :code:`scale`.\n\n    Example:\n        >>> import pyhf\n        >>> means = pyhf.tensorlib.astensor([5, 8])\n        >>> stds = pyhf.tensorlib.astensor([1, 0.5])\n        >>> pyhf.probability.Normal(means, stds)\n        <pyhf.probability.Normal object at 0x...>\n    """"""\n\n    def __init__(self, loc, scale):\n        """"""\n        Args:\n            loc (`tensor` or `float`): The mean of the Normal distribution\n            scale (`tensor` or `float`): The standard deviation of the Normal distribution\n        """"""\n\n        tensorlib, _ = get_backend()\n        self.loc = loc\n        self.scale = scale\n        self._pdf = tensorlib.normal_dist(loc, scale)\n\n    def expected_data(self):\n        r""""""\n        The expectation value of the Normal distribution.\n\n        Example:\n            >>> import pyhf\n            >>> means = pyhf.tensorlib.astensor([5, 8])\n            >>> stds = pyhf.tensorlib.astensor([1, 0.5])\n            >>> normals = pyhf.probability.Normal(means, stds)\n            >>> normals.expected_data()\n            array([5., 8.])\n\n        Returns:\n            Tensor: The mean of the Normal distribution (which is the :code:`loc`)\n\n        """"""\n        return self.loc\n\n\nclass Independent(_SimpleDistributionMixin):\n    """"""\n    A probability density corresponding to the joint distribution of a batch of\n    identically distributed random variables.\n\n    Example:\n        >>> import pyhf\n        >>> import numpy.random as random\n        >>> random.seed(0)\n        >>> rates = pyhf.tensorlib.astensor([10.0, 10.0])\n        >>> poissons = pyhf.probability.Poisson(rates)\n        >>> independent = pyhf.probability.Independent(poissons)\n        >>> independent.sample()\n        array([10, 11])\n    """"""\n\n    def __init__(self, batched_pdf, batch_size=None):\n        """"""\n        Args:\n            batched_pdf (`pyhf.probability` distribution): The batch of pdfs of the same type (e.g. Poisson)\n            batch_size (`int`): The size of the batch\n        """"""\n        self.batch_size = batch_size\n        self._pdf = batched_pdf\n\n    def log_prob(self, value):\n        r""""""\n        The log of the probability density function at the given value.\n        As the distribution is a joint distribution of the same type, this is the\n        sum of the log probabilities of each of the distributions the compose the joint.\n\n        Example:\n            >>> import pyhf\n            >>> import numpy.random as random\n            >>> random.seed(0)\n            >>> rates = pyhf.tensorlib.astensor([10.0, 10.0])\n            >>> poissons = pyhf.probability.Poisson(rates)\n            >>> independent = pyhf.probability.Independent(poissons)\n            >>> values = pyhf.tensorlib.astensor([8.0, 9.0])\n            >>> independent.log_prob(values)\n            -4.262483801927939\n            >>> broadcast_value = pyhf.tensorlib.astensor([11.0])\n            >>> independent.log_prob(broadcast_value)\n            -4.347743645878765\n\n        Args:\n            value (`tensor` or `float`): The value at which to evaluate the distribution\n\n        Returns:\n            Tensor: The value of :math:`\\log(f\\left(x\\middle|\\theta\\right))` for :math:`x=`:code:`value`\n\n        """"""\n        tensorlib, _ = get_backend()\n        result = super(Independent, self).log_prob(value)\n        result = tensorlib.sum(result, axis=-1)\n        return result\n\n\nclass Simultaneous(object):\n    """"""\n    A probability density corresponding to the joint\n    distribution of multiple non-identical component distributions\n\n    Example:\n        >>> import pyhf\n        >>> import numpy.random as random\n        >>> from pyhf.tensor.common import _TensorViewer\n        >>> random.seed(0)\n        >>> poissons = pyhf.probability.Poisson(pyhf.tensorlib.astensor([1.,100.]))\n        >>> normals = pyhf.probability.Normal(pyhf.tensorlib.astensor([1.,100.]), pyhf.tensorlib.astensor([1.,2.]))\n        >>> tv = _TensorViewer([[0,2],[1,3]])\n        >>> sim = pyhf.probability.Simultaneous([poissons,normals], tv)\n        >>> sim.sample((4,))\n        array([[  2.        ,   1.3130677 , 101.        ,  98.29180852],\n               [  1.        ,  -1.55298982,  97.        , 101.30723719],\n               [  1.        ,   1.8644362 , 118.        ,  98.51566996],\n               [  0.        ,   3.26975462,  99.        ,  97.09126865]])\n\n    """"""\n\n    def __init__(self, pdfobjs, tensorview, batch_size=None):\n        """"""\n        Construct a simultaneous pdf.\n\n        Args:\n\n            pdfobjs (`Distribution`): The constituent pdf objects\n            tensorview (`_TensorViewer`): The :code:`_TensorViewer` defining the data composition\n            batch_size (`int`): The size of the batch\n\n        """"""\n        self.tv = tensorview\n        self._pdfobjs = pdfobjs\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        """"""\n        Iterate over the constituent pdf objects\n\n        Returns:\n            pdfobj (`Distribution`): A constituent pdf object\n\n        """"""\n        for pdfobj in self._pdfobjs:\n            yield pdfobj\n\n    def __getitem__(self, index):\n        """"""\n        Access the constituent pdf object at the specified index\n\n        Args:\n\n            index (`int`): The index to access the constituent pdf object\n\n        Returns:\n            pdfobj (`Distribution`): A constituent pdf object\n\n        """"""\n        return self._pdfobjs[index]\n\n    def expected_data(self):\n        r""""""\n        The expectation value of the probability density function.\n\n        Returns:\n            Tensor: The expectation value of the distribution :math:`\\mathrm{E}\\left[f(\\theta)\\right]`\n\n        """"""\n        tostitch = [p.expected_data() for p in self]\n        return self.tv.stitch(tostitch)\n\n    def sample(self, sample_shape=()):\n        r""""""\n        The collection of values sampled from the probability density function.\n\n        Args:\n            sample_shape (`tuple`): The shape of the sample to be returned\n\n        Returns:\n            Tensor: The values :math:`x \\sim f(\\theta)` where :math:`x` has shape :code:`sample_shape`\n\n        """"""\n        return self.tv.stitch([p.sample(sample_shape) for p in self])\n\n    def log_prob(self, value):\n        r""""""\n        The log of the probability density function at the given value.\n\n        Args:\n            value (`tensor`): The observed value\n\n        Returns:\n            Tensor: The value of :math:`\\log(f\\left(x\\middle|\\theta\\right))` for :math:`x=`:code:`value`\n\n        """"""\n        constituent_data = self.tv.split(value)\n        pdfvals = [p.log_prob(d) for p, d in zip(self, constituent_data)]\n        return Simultaneous._joint_logpdf(pdfvals, batch_size=self.batch_size)\n\n    @staticmethod\n    def _joint_logpdf(terms, batch_size=None):\n        tensorlib, _ = get_backend()\n        if len(terms) == 1:\n            return terms[0]\n        if len(terms) == 2 and batch_size is None:\n            return terms[0] + terms[1]\n        terms = tensorlib.stack(terms)\n        return tensorlib.sum(terms, axis=0)\n'"
src/pyhf/readxml.py,2,"b'from . import utils\n\nimport logging\n\nfrom pathlib import Path\nimport xml.etree.ElementTree as ET\nimport numpy as np\nimport tqdm\nimport uproot\n\nlog = logging.getLogger(__name__)\n\n__FILECACHE__ = {}\n\n\ndef extract_error(h):\n    """"""\n    Determine the bin uncertainties for a histogram.\n\n    If `fSumw2` is not filled, then the histogram must have been\n    filled with no weights and `.Sumw2()` was never called. The\n    bin uncertainties are then Poisson, and so the `sqrt(entries)`.\n\n    Args:\n        h (uproot.rootio.TH1 object): The histogram\n\n    Returns:\n        list: The uncertainty for each bin in the histogram\n    """"""\n    err = h.variances if h.variances.any() else h.numpy()[0]\n    return np.sqrt(err).tolist()\n\n\ndef import_root_histogram(rootdir, filename, path, name, filecache=None):\n    global __FILECACHE__\n    filecache = filecache or __FILECACHE__\n\n    # strip leading slashes as uproot doesn\'t use ""/"" for top-level\n    path = path or \'\'\n    path = path.strip(\'/\')\n    fullpath = str(Path(rootdir).joinpath(filename))\n    if not fullpath in filecache:\n        f = uproot.open(fullpath)\n        filecache[fullpath] = f\n    else:\n        f = filecache[fullpath]\n    try:\n        h = f[name]\n    except KeyError:\n        try:\n            h = f[str(Path(path).joinpath(name))]\n        except KeyError:\n            raise KeyError(\n                f\'Both {name} and {Path(path).joinpath(name)} were tried and not found in {Path(rootdir).joinpath(filename)}\'\n            )\n    return h.numpy()[0].tolist(), extract_error(h)\n\n\ndef process_sample(\n    sample, rootdir, inputfile, histopath, channelname, track_progress=False\n):\n    if \'InputFile\' in sample.attrib:\n        inputfile = sample.attrib.get(\'InputFile\')\n    if \'HistoPath\' in sample.attrib:\n        histopath = sample.attrib.get(\'HistoPath\')\n    histoname = sample.attrib[\'HistoName\']\n\n    data, err = import_root_histogram(rootdir, inputfile, histopath, histoname)\n\n    parameter_configs = []\n    modifiers = []\n    # first check if we need to add lumi modifier for this sample\n    if sample.attrib.get(""NormalizeByTheory"", ""False"") == \'True\':\n        modifiers.append({\'name\': \'lumi\', \'type\': \'lumi\', \'data\': None})\n\n    modtags = tqdm.tqdm(\n        sample.iter(), unit=\'modifier\', disable=not (track_progress), total=len(sample)\n    )\n\n    for modtag in modtags:\n        modtags.set_description(\n            \'  - modifier {0:s}({1:s})\'.format(\n                modtag.attrib.get(\'Name\', \'n/a\'), modtag.tag\n            )\n        )\n        if modtag == sample:\n            continue\n        if modtag.tag == \'OverallSys\':\n            modifiers.append(\n                {\n                    \'name\': modtag.attrib[\'Name\'],\n                    \'type\': \'normsys\',\n                    \'data\': {\n                        \'lo\': float(modtag.attrib[\'Low\']),\n                        \'hi\': float(modtag.attrib[\'High\']),\n                    },\n                }\n            )\n        elif modtag.tag == \'NormFactor\':\n            modifiers.append(\n                {\'name\': modtag.attrib[\'Name\'], \'type\': \'normfactor\', \'data\': None}\n            )\n            parameter_config = {\n                \'name\': modtag.attrib[\'Name\'],\n                \'bounds\': [[float(modtag.attrib[\'Low\']), float(modtag.attrib[\'High\'])]],\n                \'inits\': [float(modtag.attrib[\'Val\'])],\n            }\n            if modtag.attrib.get(\'Const\'):\n                parameter_config[\'fixed\'] = modtag.attrib[\'Const\'] == \'True\'\n\n            parameter_configs.append(parameter_config)\n        elif modtag.tag == \'HistoSys\':\n            lo, _ = import_root_histogram(\n                rootdir,\n                modtag.attrib.get(\'HistoFileLow\', inputfile),\n                modtag.attrib.get(\'HistoPathLow\', \'\'),\n                modtag.attrib[\'HistoNameLow\'],\n            )\n            hi, _ = import_root_histogram(\n                rootdir,\n                modtag.attrib.get(\'HistoFileHigh\', inputfile),\n                modtag.attrib.get(\'HistoPathHigh\', \'\'),\n                modtag.attrib[\'HistoNameHigh\'],\n            )\n            modifiers.append(\n                {\n                    \'name\': modtag.attrib[\'Name\'],\n                    \'type\': \'histosys\',\n                    \'data\': {\'lo_data\': lo, \'hi_data\': hi},\n                }\n            )\n        elif modtag.tag == \'StatError\' and modtag.attrib[\'Activate\'] == \'True\':\n            if modtag.attrib.get(\'HistoName\', \'\') == \'\':\n                staterr = err\n            else:\n                extstat, _ = import_root_histogram(\n                    rootdir,\n                    modtag.attrib.get(\'HistoFile\', inputfile),\n                    modtag.attrib.get(\'HistoPath\', \'\'),\n                    modtag.attrib[\'HistoName\'],\n                )\n                staterr = np.multiply(extstat, data).tolist()\n            if not staterr:\n                raise RuntimeError(\'cannot determine stat error.\')\n            modifiers.append(\n                {\n                    \'name\': \'staterror_{}\'.format(channelname),\n                    \'type\': \'staterror\',\n                    \'data\': staterr,\n                }\n            )\n        elif modtag.tag == \'ShapeSys\':\n            # NB: ConstraintType is ignored\n            if modtag.attrib.get(\'ConstraintType\', \'Poisson\') != \'Poisson\':\n                log.warning(\n                    \'shapesys modifier %s has a non-poisson constraint\',\n                    modtag.attrib[\'Name\'],\n                )\n            shapesys_data, _ = import_root_histogram(\n                rootdir,\n                modtag.attrib.get(\'InputFile\', inputfile),\n                modtag.attrib.get(\'HistoPath\', \'\'),\n                modtag.attrib[\'HistoName\'],\n            )\n            # NB: we convert relative uncertainty to absolute uncertainty\n            modifiers.append(\n                {\n                    \'name\': modtag.attrib[\'Name\'],\n                    \'type\': \'shapesys\',\n                    \'data\': [a * b for a, b in zip(data, shapesys_data)],\n                }\n            )\n        else:\n            log.warning(\'not considering modifier tag %s\', modtag)\n\n    return {\n        \'name\': sample.attrib[\'Name\'],\n        \'data\': data,\n        \'modifiers\': modifiers,\n        \'parameter_configs\': parameter_configs,\n    }\n\n\ndef process_data(sample, rootdir, inputfile, histopath):\n    if \'InputFile\' in sample.attrib:\n        inputfile = sample.attrib.get(\'InputFile\')\n    if \'HistoPath\' in sample.attrib:\n        histopath = sample.attrib.get(\'HistoPath\')\n    histoname = sample.attrib[\'HistoName\']\n\n    data, _ = import_root_histogram(rootdir, inputfile, histopath, histoname)\n    return data\n\n\ndef process_channel(channelxml, rootdir, track_progress=False):\n    channel = channelxml.getroot()\n\n    inputfile = channel.attrib.get(\'InputFile\')\n    histopath = channel.attrib.get(\'HistoPath\')\n\n    samples = tqdm.tqdm(\n        channel.findall(\'Sample\'), unit=\'sample\', disable=not (track_progress)\n    )\n\n    data = channel.findall(\'Data\')\n    if data:\n        parsed_data = process_data(data[0], rootdir, inputfile, histopath)\n    else:\n        parsed_data = None\n    channelname = channel.attrib[\'Name\']\n\n    results = []\n    channel_parameter_configs = []\n    for sample in samples:\n        samples.set_description(\'  - sample {}\'.format(sample.attrib.get(\'Name\')))\n        result = process_sample(\n            sample, rootdir, inputfile, histopath, channelname, track_progress\n        )\n        channel_parameter_configs.extend(result.pop(\'parameter_configs\'))\n        results.append(result)\n\n    return channelname, parsed_data, results, channel_parameter_configs\n\n\ndef process_measurements(toplvl, other_parameter_configs=None):\n    results = []\n    other_parameter_configs = other_parameter_configs if other_parameter_configs else []\n\n    for x in toplvl.findall(\'Measurement\'):\n        parameter_configs_map = {k[\'name\']: dict(**k) for k in other_parameter_configs}\n        lumi = float(x.attrib[\'Lumi\'])\n        lumierr = lumi * float(x.attrib[\'LumiRelErr\'])\n\n        result = {\n            \'name\': x.attrib[\'Name\'],\n            \'config\': {\n                \'poi\': x.findall(\'POI\')[0].text,\n                \'parameters\': [\n                    {\n                        \'name\': \'lumi\',\n                        \'auxdata\': [lumi],\n                        \'bounds\': [[lumi - 5.0 * lumierr, lumi + 5.0 * lumierr]],\n                        \'inits\': [lumi],\n                        \'sigmas\': [lumierr],\n                    }\n                ],\n            },\n        }\n\n        for param in x.findall(\'ParamSetting\'):\n            # determine what all parameters in the paramsetting have in common\n            overall_param_obj = {}\n            if param.attrib.get(\'Const\'):\n                overall_param_obj[\'fixed\'] = param.attrib[\'Const\'] == \'True\'\n            if param.attrib.get(\'Val\'):\n                overall_param_obj[\'inits\'] = [float(param.attrib[\'Val\'])]\n\n            # might be specifying multiple parameters in the same ParamSetting\n            if param.text:\n                for param_name in param.text.split(\' \'):\n                    # lumi will always be the first parameter\n                    if param_name == \'Lumi\':\n                        result[\'config\'][\'parameters\'][0].update(overall_param_obj)\n                    else:\n                        # pop from parameter_configs_map because we don\'t want to duplicate\n                        param_obj = parameter_configs_map.pop(\n                            param_name, {\'name\': param_name}\n                        )\n                        # ParamSetting will always take precedence\n                        param_obj.update(overall_param_obj)\n                        # add it back in to the parameter_configs_map\n                        parameter_configs_map[param_name] = param_obj\n        result[\'config\'][\'parameters\'].extend(parameter_configs_map.values())\n        results.append(result)\n\n    return results\n\n\ndef dedupe_parameters(parameters):\n    duplicates = {}\n    for p in parameters:\n        duplicates.setdefault(p[\'name\'], []).append(p)\n    for parname in duplicates.keys():\n        parameter_list = duplicates[parname]\n        if len(parameter_list) == 1:\n            continue\n        elif any(p != parameter_list[0] for p in parameter_list[1:]):\n            for p in parameter_list:\n                log.warning(p)\n            raise RuntimeError(\n                \'cannot import workspace due to incompatible parameter configurations for {0:s}.\'.format(\n                    parname\n                )\n            )\n    # no errors raised, de-dupe and return\n    return list({v[\'name\']: v for v in parameters}.values())\n\n\ndef parse(configfile, rootdir, track_progress=False):\n    toplvl = ET.parse(configfile)\n    inputs = tqdm.tqdm(\n        [x.text for x in toplvl.findall(\'Input\')],\n        unit=\'channel\',\n        disable=not (track_progress),\n    )\n\n    channels = {}\n    parameter_configs = []\n    for inp in inputs:\n        inputs.set_description(\'Processing {}\'.format(inp))\n        channel, data, samples, channel_parameter_configs = process_channel(\n            ET.parse(str(Path(rootdir).joinpath(inp))), rootdir, track_progress\n        )\n        channels[channel] = {\'data\': data, \'samples\': samples}\n        parameter_configs.extend(channel_parameter_configs)\n\n    parameter_configs = dedupe_parameters(parameter_configs)\n    result = {\n        \'measurements\': process_measurements(\n            toplvl, other_parameter_configs=parameter_configs\n        ),\n        \'channels\': [{\'name\': k, \'samples\': v[\'samples\']} for k, v in channels.items()],\n        \'observations\': [{\'name\': k, \'data\': v[\'data\']} for k, v in channels.items()],\n        \'version\': utils.SCHEMA_VERSION,\n    }\n    utils.validate(result, \'workspace.json\')\n\n    return result\n\n\ndef clear_filecache():\n    global __FILECACHE__\n    __FILECACHE__ = {}\n'"
src/pyhf/simplemodels.py,0,"b""from . import Model\n\n\ndef hepdata_like(signal_data, bkg_data, bkg_uncerts, batch_size=None):\n    spec = {\n        'channels': [\n            {\n                'name': 'singlechannel',\n                'samples': [\n                    {\n                        'name': 'signal',\n                        'data': signal_data,\n                        'modifiers': [\n                            {'name': 'mu', 'type': 'normfactor', 'data': None}\n                        ],\n                    },\n                    {\n                        'name': 'background',\n                        'data': bkg_data,\n                        'modifiers': [\n                            {\n                                'name': 'uncorr_bkguncrt',\n                                'type': 'shapesys',\n                                'data': bkg_uncerts,\n                            }\n                        ],\n                    },\n                ],\n            }\n        ]\n    }\n    return Model(spec, batch_size=batch_size)\n"""
src/pyhf/utils.py,0,"b'import json\nimport jsonschema\nimport pkg_resources\nfrom pathlib import Path\nimport yaml\nimport click\nimport hashlib\n\nfrom .exceptions import InvalidSpecification\n\nSCHEMA_CACHE = {}\nSCHEMA_BASE = ""https://scikit-hep.org/pyhf/schemas/""\nSCHEMA_VERSION = \'1.0.0\'\n\n\ndef load_schema(schema_id, version=None):\n    global SCHEMA_CACHE\n    if not version:\n        version = SCHEMA_VERSION\n    try:\n        return SCHEMA_CACHE[f\'{SCHEMA_BASE}{Path(version).joinpath(schema_id)}\']\n    except KeyError:\n        pass\n\n    path = pkg_resources.resource_filename(\n        __name__, str(Path(\'schemas\').joinpath(version, schema_id))\n    )\n    with open(path) as json_schema:\n        schema = json.load(json_schema)\n        SCHEMA_CACHE[schema[\'$id\']] = schema\n    return SCHEMA_CACHE[schema[\'$id\']]\n\n\n# load the defs.json as it is included by $ref\nload_schema(\'defs.json\')\n\n\ndef validate(spec, schema_name, version=None):\n    schema = load_schema(schema_name, version=version)\n    try:\n        resolver = jsonschema.RefResolver(\n            base_uri=\'file://{0:s}\'.format(\n                pkg_resources.resource_filename(__name__, \'schemas/\')\n            ),\n            referrer=schema_name,\n            store=SCHEMA_CACHE,\n        )\n        validator = jsonschema.Draft6Validator(\n            schema, resolver=resolver, format_checker=None\n        )\n        return validator.validate(spec)\n    except jsonschema.ValidationError as err:\n        raise InvalidSpecification(err)\n\n\ndef options_from_eqdelimstring(opts):\n    document = \'\\n\'.join(\'{0}: {1}\'.format(*opt.split(\'=\', 1)) for opt in opts)\n    return yaml.full_load(document)\n\n\nclass EqDelimStringParamType(click.ParamType):\n    name = \'equal-delimited option\'\n\n    def convert(self, value, param, ctx):\n        try:\n            return options_from_eqdelimstring([value])\n        except IndexError:\n            self.fail(\n                \'{0:s} is not a valid equal-delimited string\'.format(value), param, ctx\n            )\n\n\ndef digest(obj, algorithm=\'sha256\'):\n    """"""\n    Get the digest for the provided object. Note: object must be JSON-serializable.\n\n    The hashing algorithms supported are in :mod:`hashlib`, part of Python\'s Standard Libraries.\n\n    Example:\n\n        >>> import pyhf\n        >>> obj = {\'a\': 2.0, \'b\': 3.0, \'c\': 1.0}\n        >>> pyhf.utils.digest(obj)\n        \'a38f6093800189b79bc22ef677baf90c75705af2cfc7ff594159eca54eaa7928\'\n        >>> pyhf.utils.digest(obj, algorithm=\'md5\')\n        \'2c0633f242928eb55c3672fed5ba8612\'\n        >>> pyhf.utils.digest(obj, algorithm=\'sha1\')\n        \'49a27f499e763766c9545b294880df277be6f545\'\n\n    Raises:\n        ValueError: If the object is not JSON-serializable or if the algorithm is not supported.\n\n    Args:\n        obj (`obj`): A JSON-serializable object to compute the digest of. Usually a :class:`~pyhf.workspace.Workspace` object.\n        algorithm (`str`): The hashing algorithm to use.\n\n    Returns:\n        digest (`str`): The digest for the JSON-serialized object provided and hash algorithm specified.\n    """"""\n\n    try:\n        stringified = json.dumps(obj, sort_keys=True, ensure_ascii=False).encode(\'utf8\')\n    except TypeError:\n        raise ValueError(\n            ""The supplied object is not JSON-serializable for calculating a hash.""\n        )\n    try:\n        hash_alg = getattr(hashlib, algorithm)\n    except AttributeError:\n        raise ValueError(\n            f""{algorithm} is not an algorithm provided by Python\'s hashlib library.""\n        )\n    return hash_alg(stringified).hexdigest()\n'"
src/pyhf/version.py,0,"b'""""""Define pyhf version information.""""""\n\n# Use semantic versioning (https://semver.org/)\n# The version number is controlled through bumpversion.cfg\n__version__ = \'0.4.3\'\n'"
src/pyhf/workspace.py,0,"b'""""""\npyhf workspaces hold the three data items:\n\n* the statistical model p(data|parameters)\n* the observed data (optional)\n* fit configurations (""measurements"")\n""""""\nimport logging\nimport jsonpatch\nimport copy\nimport collections\nfrom . import exceptions\nfrom . import utils\nfrom .pdf import Model\nfrom .mixins import _ChannelSummaryMixin\n\nlog = logging.getLogger(__name__)\n\n\ndef _join_items(join, left_items, right_items, key=\'name\'):\n    """"""\n    Join two lists of dictionaries along the given key.\n\n    This is meant to be as generic as possible for any pairs of lists of dictionaries for many join operations.\n\n    Args:\n        join (`str`): The join operation to apply. See ~pyhf.workspace.Workspace for valid join operations.\n        left_items (`list`): A list of dictionaries to join on the left\n        right_items (`list`): A list of dictionaries to join on the right\n\n    Returns:\n        :obj:`list`: A joined list of dictionaries.\n\n    """"""\n    if join == \'right outer\':\n        primary_items, secondary_items = right_items, left_items\n    else:\n        primary_items, secondary_items = left_items, right_items\n    joined_items = copy.deepcopy(primary_items)\n    for secondary_item in secondary_items:\n        # outer join: merge primary and secondary, matching where possible\n        if join == \'outer\' and secondary_item in primary_items:\n            continue\n        # left/right outer join: only add secondary if existing item (by key value) is not in primary\n        # NB: this will be slow for large numbers of items\n        elif join in [\'left outer\', \'right outer\'] and secondary_item[key] in [\n            item[key] for item in joined_items\n        ]:\n            continue\n        joined_items.append(copy.deepcopy(secondary_item))\n    return joined_items\n\n\ndef _join_versions(join, left_version, right_version):\n    """"""\n    Join two workspace versions.\n\n    Raises:\n      ~pyhf.exceptions.InvalidWorkspaceOperation: Versions are incompatible.\n\n    Args:\n        join (`str`): The join operation to apply. See ~pyhf.workspace.Workspace for valid join operations.\n        left_version (`str`): The left workspace version.\n        right_version (`str`): The right workspace version.\n\n    Returns:\n        :obj:`str`: The workspace version.\n\n    """"""\n    if left_version != right_version:\n        raise exceptions.InvalidWorkspaceOperation(\n            f""Workspaces of different versions cannot be combined: {left_version} != {right_version}""\n        )\n    return left_version\n\n\ndef _join_channels(join, left_channels, right_channels):\n    """"""\n    Join two workspace channel specifications.\n\n    Raises:\n      ~pyhf.exceptions.InvalidWorkspaceOperation: Channel specifications are incompatible.\n\n    Args:\n        join (`str`): The join operation to apply. See ~pyhf.workspace.Workspace for valid join operations.\n        left_channels (`list`): The left channel specification.\n        right_channels (`list`): The right channel specification.\n\n    Returns:\n        :obj:`list`: A joined list of channels. Each channel follows the :obj:`defs.json#/definitions/channel` `schema <https://scikit-hep.org/pyhf/likelihood.html#channel>`__\n\n    """"""\n\n    joined_channels = _join_items(join, left_channels, right_channels)\n    if join == \'none\':\n        common_channels = set(c[\'name\'] for c in left_channels).intersection(\n            c[\'name\'] for c in right_channels\n        )\n        if common_channels:\n            raise exceptions.InvalidWorkspaceOperation(\n                f""Workspaces cannot have any channels in common with the same name: {common_channels}. You can also try a different join operation: {Workspace.valid_joins}.""\n            )\n\n    elif join == \'outer\':\n        counted_channels = collections.Counter(\n            channel[\'name\'] for channel in joined_channels\n        )\n        incompatible_channels = [\n            channel for channel, count in counted_channels.items() if count > 1\n        ]\n        if incompatible_channels:\n            raise exceptions.InvalidWorkspaceOperation(\n                f""Workspaces cannot have channels in common with incompatible structure: {incompatible_channels}. You can also try a different join operation: {Workspace.valid_joins}.""\n            )\n    return joined_channels\n\n\ndef _join_observations(join, left_observations, right_observations):\n    """"""\n    Join two workspace observation specifications.\n\n    Raises:\n      ~pyhf.exceptions.InvalidWorkspaceOperation: Observation specifications are incompatible.\n\n    Args:\n        join (`str`): The join operation to apply. See ~pyhf.workspace.Workspace for valid join operations.\n        left_observations (`list`): The left observation specification.\n        right_observations (`list`): The right observation specification.\n\n    Returns:\n        :obj:`list`: A joined list of observations. Each observation follows the :obj:`defs.json#/definitions/observation` `schema <https://scikit-hep.org/pyhf/likelihood.html#observations>`__\n\n    """"""\n    joined_observations = _join_items(join, left_observations, right_observations)\n    if join == \'none\':\n        common_observations = set(\n            obs[\'name\'] for obs in left_observations\n        ).intersection(obs[\'name\'] for obs in right_observations)\n        if common_observations:\n            raise exceptions.InvalidWorkspaceOperation(\n                f""Workspaces cannot have any observations in common with the same name: {common_observations}. You can also try a different join operation: {Workspace.valid_joins}.""\n            )\n\n    elif join == \'outer\':\n        counted_observations = collections.Counter(\n            observation[\'name\'] for observation in joined_observations\n        )\n        incompatible_observations = [\n            observation\n            for observation, count in counted_observations.items()\n            if count > 1\n        ]\n        if incompatible_observations:\n            raise exceptions.InvalidWorkspaceOperation(\n                f""Workspaces cannot have observations in common with incompatible structure: {incompatible_observations}. You can also try a different join operation: {Workspace.valid_joins}.""\n            )\n    return joined_observations\n\n\ndef _join_parameter_configs(measurement_name, left_parameters, right_parameters):\n    """"""\n    Join two measurement parameter config specifications.\n\n    Only uses by :method:`_join_measurements` when join=\'outer\'.\n\n    Raises:\n      ~pyhf.exceptions.InvalidWorkspaceOperation: Parameter configuration specifications are incompatible.\n\n    Args:\n        measurement_name (`str`): The name of the measurement being joined (a detail for raising exceptions correctly)\n        left_parameters (`list`): The left parameter configuration specification.\n        right_parameters (`list`): The right parameter configuration specification.\n\n    Returns:\n        :obj:`list`: A joined list of parameter configurations. Each parameter configuration follows the :obj:`defs.json#/definitions/config` schema\n\n    """"""\n    joined_parameter_configs = _join_items(\'outer\', left_parameters, right_parameters)\n    counted_parameter_configs = collections.Counter(\n        parameter[\'name\'] for parameter in joined_parameter_configs\n    )\n    incompatible_parameter_configs = [\n        parameter for parameter, count in counted_parameter_configs.items() if count > 1\n    ]\n    if incompatible_parameter_configs:\n        raise exceptions.InvalidWorkspaceOperation(\n            f""Workspaces cannot have a measurement ({measurement_name}) with incompatible parameter configs: {incompatible_parameter_configs}. You can also try a different join operation: {Workspace.valid_joins}.""\n        )\n    return joined_parameter_configs\n\n\ndef _join_measurements(join, left_measurements, right_measurements):\n    """"""\n    Join two workspace measurement specifications.\n\n    Raises:\n      ~pyhf.exceptions.InvalidWorkspaceOperation: Measurement specifications are incompatible.\n\n    Args:\n        join (`str`): The join operation to apply. See ~pyhf.workspace.Workspace for valid join operations.\n        left_measurements (`list`): The left measurement specification.\n        right_measurements (`list`): The right measurement specification.\n\n    Returns:\n        :obj:`list`: A joined list of measurements. Each measurement follows the :obj:`defs.json#/definitions/measurement` `schema <https://scikit-hep.org/pyhf/likelihood.html#measurements>`__\n\n    """"""\n    joined_measurements = _join_items(join, left_measurements, right_measurements)\n    if join == \'none\':\n        common_measurements = set(\n            meas[\'name\'] for meas in left_measurements\n        ).intersection(meas[\'name\'] for meas in right_measurements)\n        if common_measurements:\n            raise exceptions.InvalidWorkspaceOperation(\n                f""Workspaces cannot have any measurements in common with the same name: {common_measurements}. You can also try a different join operation: {Workspace.valid_joins}.""\n            )\n\n    elif join == \'outer\':\n        # need to store a mapping of measurement name to all measurement objects with that name\n        _measurement_mapping = {}\n        for measurement in joined_measurements:\n            _measurement_mapping.setdefault(measurement[\'name\'], []).append(measurement)\n        # first check for incompatible POI\n        # then merge parameter configs\n        incompatible_poi = [\n            measurement_name\n            for measurement_name, measurements in _measurement_mapping.items()\n            if len(set(measurement[\'config\'][\'poi\'] for measurement in measurements))\n            > 1\n        ]\n        if incompatible_poi:\n            raise exceptions.InvalidWorkspaceOperation(\n                f""Workspaces cannot have the same measurements with incompatible POI: {incompatible_poi}.""\n            )\n\n        joined_measurements = []\n        for measurement_name, measurements in _measurement_mapping.items():\n            if len(measurements) != 1:\n                new_measurement = {\n                    \'name\': measurement_name,\n                    \'config\': {\n                        \'poi\': measurements[0][\'config\'][\'poi\'],\n                        \'parameters\': _join_parameter_configs(\n                            measurement_name,\n                            *[\n                                measurement[\'config\'][\'parameters\']\n                                for measurement in measurements\n                            ],\n                        ),\n                    },\n                }\n            else:\n                new_measurement = measurements[0]\n            joined_measurements.append(new_measurement)\n    return joined_measurements\n\n\nclass Workspace(_ChannelSummaryMixin, dict):\n    """"""\n    A JSON-serializable object that is built from an object that follows the :obj:`workspace.json` `schema <https://scikit-hep.org/pyhf/likelihood.html#workspace>`__.\n    """"""\n\n    valid_joins = [\'none\', \'outer\', \'left outer\', \'right outer\']\n\n    def __init__(self, spec, **config_kwargs):\n        """"""Workspaces hold the model, data and measurements.""""""\n        super(Workspace, self).__init__(spec, channels=spec[\'channels\'])\n        self.schema = config_kwargs.pop(\'schema\', \'workspace.json\')\n        self.version = config_kwargs.pop(\'version\', spec.get(\'version\', None))\n\n        # run jsonschema validation of input specification against the (provided) schema\n        log.info(f""Validating spec against schema: {self.schema}"")\n        utils.validate(self, self.schema, version=self.version)\n\n        self.measurement_names = []\n        for measurement in self.get(\'measurements\', []):\n            self.measurement_names.append(measurement[\'name\'])\n\n        self.observations = {}\n        for obs in self[\'observations\']:\n            self.observations[obs[\'name\']] = obs[\'data\']\n\n    def __eq__(self, other):\n        """"""Equality is defined as equal dict representations.""""""\n        if not isinstance(other, Workspace):\n            return False\n        return dict(self) == dict(other)\n\n    def __ne__(self, other):\n        """"""Negation of equality.""""""\n        return not self == other\n\n    def __repr__(self):\n        """"""Representation of the Workspace.""""""\n        return object.__repr__(self)\n\n    def get_measurement(\n        self, poi_name=None, measurement_name=None, measurement_index=None\n    ):\n        """"""\n        Get (or create) a measurement object.\n\n        The following logic is used:\n\n          1. if the poi name is given, create a measurement object for that poi\n          2. if the measurement name is given, find the measurement for the given name\n          3. if the measurement index is given, return the measurement at that index\n          4. if there are measurements but none of the above have been specified, return the 0th measurement\n\n        Raises:\n          ~pyhf.exceptions.InvalidMeasurement: If the measurement was not found\n\n        Args:\n            poi_name (`str`): The name of the parameter of interest to create a new measurement from\n            measurement_name (`str`): The name of the measurement to use\n            measurement_index (`int`): The index of the measurement to use\n\n        Returns:\n            :obj:`dict`: A measurement object adhering to the schema defs.json#/definitions/measurement\n\n        """"""\n        measurement = None\n        if poi_name is not None:\n            measurement = {\n                \'name\': \'NormalMeasurement\',\n                \'config\': {\'poi\': poi_name, \'parameters\': []},\n            }\n        elif self.measurement_names:\n            if measurement_name is not None:\n                if measurement_name not in self.measurement_names:\n                    log.debug(\n                        \'measurements defined:\\n\\t{0:s}\'.format(\n                            \'\\n\\t\'.join(self.measurement_names)\n                        )\n                    )\n                    raise exceptions.InvalidMeasurement(\n                        \'no measurement by name \\\'{0:s}\\\' was found in the workspace, pick from one of the valid ones above\'.format(\n                            measurement_name\n                        )\n                    )\n                measurement = self[\'measurements\'][\n                    self.measurement_names.index(measurement_name)\n                ]\n            else:\n                if measurement_index is None and len(self.measurement_names) > 1:\n                    log.warning(\n                        \'multiple measurements defined. Taking the first measurement.\'\n                    )\n\n                measurement_index = (\n                    measurement_index if measurement_index is not None else 0\n                )\n                try:\n                    measurement = self[\'measurements\'][measurement_index]\n                except IndexError:\n                    raise exceptions.InvalidMeasurement(\n                        f""The measurement index {measurement_index} is out of bounds as only {len(self.measurement_names)} measurement(s) have been defined.""\n                    )\n        else:\n            raise exceptions.InvalidMeasurement(""No measurements have been defined."")\n\n        utils.validate(measurement, \'measurement.json\', self.version)\n        return measurement\n\n    def model(self, **config_kwargs):\n        """"""\n        Create a model object with/without patches applied.\n\n        See :func:`pyhf.workspace.Workspace.get_measurement` and :class:`pyhf.pdf.Model` for possible keyword arguments.\n\n        Args:\n            patches: A list of JSON patches to apply to the model specification\n            config_kwargs: Possible keyword arguments for the measurement and model configuration\n\n        Returns:\n            ~pyhf.pdf.Model: A model object adhering to the schema model.json\n\n        """"""\n\n        poi_name = config_kwargs.pop(\'poi_name\', None)\n        measurement_name = config_kwargs.pop(\'measurement_name\', None)\n        measurement_index = config_kwargs.pop(\'measurement_index\', None)\n        measurement = self.get_measurement(\n            poi_name=poi_name,\n            measurement_name=measurement_name,\n            measurement_index=measurement_index,\n        )\n        log.debug(\n            \'model being created for measurement {0:s}\'.format(measurement[\'name\'])\n        )\n\n        patches = config_kwargs.pop(\'patches\', [])\n\n        modelspec = {\n            \'channels\': self[\'channels\'],\n            \'parameters\': measurement[\'config\'][\'parameters\'],\n        }\n        for patch in patches:\n            modelspec = jsonpatch.JsonPatch(patch).apply(modelspec)\n\n        return Model(modelspec, poi_name=measurement[\'config\'][\'poi\'], **config_kwargs)\n\n    def data(self, model, with_aux=True):\n        """"""\n        Return the data for the supplied model with or without auxiliary data from the model.\n\n        The model is needed as the order of the data depends on the order of the channels in the model.\n\n        Raises:\n          KeyError: Invalid or missing channel\n\n        Args:\n            model (~pyhf.pdf.Model): A model object adhering to the schema model.json\n            with_aux (bool): Whether to include auxiliary data from the model or not\n\n        Returns:\n            :obj:`list`: data\n\n        """"""\n        try:\n            observed_data = sum(\n                (self.observations[c] for c in model.config.channels), []\n            )\n        except KeyError:\n            log.error(\n                ""Invalid channel: the workspace does not have observation data for one of the channels in the model.""\n            )\n            raise\n        if with_aux:\n            observed_data += model.config.auxdata\n        return observed_data\n\n    def _prune_and_rename(\n        self,\n        prune_modifiers=None,\n        prune_modifier_types=None,\n        prune_samples=None,\n        prune_channels=None,\n        prune_measurements=None,\n        rename_modifiers=None,\n        rename_samples=None,\n        rename_channels=None,\n        rename_measurements=None,\n    ):\n        """"""\n        Return a new, pruned, renamed workspace specification. This will not modify the original workspace.\n\n        Pruning removes pieces of the workspace whose name or type matches the\n        user-provided lists. The pruned, renamed workspace must also be a valid\n        workspace.\n\n        A workspace is composed of many named components, such as channels and\n        samples, as well as types of systematics (e.g. `histosys`). Components\n        can be removed (pruned away) filtering on name or be renamed according\n        to the provided :obj:`dict` mapping. Additionally, modifiers of\n        specific types can be removed (pruned away).\n\n        This function also handles specific peculiarities, such as\n        renaming/removing a channel which needs to rename/remove the\n        corresponding `observation`.\n\n        Args:\n            prune_modifiers: A :obj:`str` or a :obj:`list` of modifiers to prune.\n            prune_modifier_types: A :obj:`str` or a :obj:`list` of modifier types to prune.\n            prune_samples: A :obj:`str` or a :obj:`list` of samples to prune.\n            prune_channels: A :obj:`str` or a :obj:`list` of channels to prune.\n            prune_measurements: A :obj:`str` or a :obj:`list` of measurements to prune.\n            rename_modifiers: A :obj:`dict` mapping old modifier name to new modifier name.\n            rename_samples: A :obj:`dict` mapping old sample name to new sample name.\n            rename_channels: A :obj:`dict` mapping old channel name to new channel name.\n            rename_measurements: A :obj:`dict` mapping old measurement name to new measurement name.\n\n        Returns:\n            ~pyhf.workspace.Workspace: A new workspace object with the specified components removed or renamed\n\n        """"""\n        # avoid mutable defaults\n        prune_modifiers = [] if prune_modifiers is None else prune_modifiers\n        prune_modifier_types = (\n            [] if prune_modifier_types is None else prune_modifier_types\n        )\n        prune_samples = [] if prune_samples is None else prune_samples\n        prune_channels = [] if prune_channels is None else prune_channels\n        prune_measurements = [] if prune_measurements is None else prune_measurements\n        rename_modifiers = {} if rename_modifiers is None else rename_modifiers\n        rename_samples = {} if rename_samples is None else rename_samples\n        rename_channels = {} if rename_channels is None else rename_channels\n        rename_measurements = {} if rename_measurements is None else rename_measurements\n\n        newspec = {\n            \'channels\': [\n                {\n                    \'name\': rename_channels.get(channel[\'name\'], channel[\'name\']),\n                    \'samples\': [\n                        {\n                            \'name\': rename_samples.get(sample[\'name\'], sample[\'name\']),\n                            \'data\': sample[\'data\'],\n                            \'modifiers\': [\n                                dict(\n                                    modifier,\n                                    name=rename_modifiers.get(\n                                        modifier[\'name\'], modifier[\'name\']\n                                    ),\n                                )\n                                for modifier in sample[\'modifiers\']\n                                if modifier[\'name\'] not in prune_modifiers\n                                and modifier[\'type\'] not in prune_modifier_types\n                            ],\n                        }\n                        for sample in channel[\'samples\']\n                        if sample[\'name\'] not in prune_samples\n                    ],\n                }\n                for channel in self[\'channels\']\n                if channel[\'name\'] not in prune_channels\n            ],\n            \'measurements\': [\n                {\n                    \'name\': rename_measurements.get(\n                        measurement[\'name\'], measurement[\'name\']\n                    ),\n                    \'config\': {\n                        \'parameters\': [\n                            dict(\n                                parameter,\n                                name=rename_modifiers.get(\n                                    parameter[\'name\'], parameter[\'name\']\n                                ),\n                            )\n                            for parameter in measurement[\'config\'][\'parameters\']\n                            if parameter[\'name\'] not in prune_modifiers\n                        ],\n                        \'poi\': rename_modifiers.get(\n                            measurement[\'config\'][\'poi\'], measurement[\'config\'][\'poi\']\n                        ),\n                    },\n                }\n                for measurement in self[\'measurements\']\n                if measurement[\'name\'] not in prune_measurements\n            ],\n            \'observations\': [\n                dict(\n                    copy.deepcopy(observation),\n                    name=rename_channels.get(observation[\'name\'], observation[\'name\']),\n                )\n                for observation in self[\'observations\']\n                if observation[\'name\'] not in prune_channels\n            ],\n            \'version\': self[\'version\'],\n        }\n        return Workspace(newspec)\n\n    def prune(\n        self,\n        modifiers=None,\n        modifier_types=None,\n        samples=None,\n        channels=None,\n        measurements=None,\n    ):\n        """"""\n        Return a new, pruned workspace specification. This will not modify the original workspace.\n\n        The pruned workspace must also be a valid workspace.\n\n        Args:\n            modifiers: A :obj:`str` or a :obj:`list` of modifiers to prune.\n            modifier_types: A :obj:`str` or a :obj:`list` of modifier types to prune.\n            samples: A :obj:`str` or a :obj:`list` of samples to prune.\n            channels: A :obj:`str` or a :obj:`list` of channels to prune.\n            measurements: A :obj:`str` or a :obj:`list` of measurements to prune.\n\n        Returns:\n            ~pyhf.workspace.Workspace: A new workspace object with the specified components removed\n\n        """"""\n        # avoid mutable defaults\n        modifiers = [] if modifiers is None else modifiers\n        modifier_types = [] if modifier_types is None else modifier_types\n        samples = [] if samples is None else samples\n        channels = [] if channels is None else channels\n        measurements = [] if measurements is None else measurements\n\n        return self._prune_and_rename(\n            prune_modifiers=modifiers,\n            prune_modifier_types=modifier_types,\n            prune_samples=samples,\n            prune_channels=channels,\n            prune_measurements=measurements,\n        )\n\n    def rename(self, modifiers=None, samples=None, channels=None, measurements=None):\n        """"""\n        Return a new workspace specification with certain elements renamed.\n\n        This will not modify the original workspace.\n        The renamed workspace must also be a valid workspace.\n\n        Args:\n            modifiers: A :obj:`dict` mapping old modifier name to new modifier name.\n            samples: A :obj:`dict` mapping old sample name to new sample name.\n            channels: A :obj:`dict` mapping old channel name to new channel name.\n            measurements: A :obj:`dict` mapping old measurement name to new measurement name.\n\n        Returns:\n            ~pyhf.workspace.Workspace: A new workspace object with the specified components renamed\n\n        """"""\n        # avoid mutable defaults\n        modifiers = {} if modifiers is None else modifiers\n        samples = {} if samples is None else samples\n        channels = {} if channels is None else channels\n        measurements = {} if measurements is None else measurements\n\n        return self._prune_and_rename(\n            rename_modifiers=modifiers,\n            rename_samples=samples,\n            rename_channels=channels,\n            rename_measurements=measurements,\n        )\n\n    @classmethod\n    def combine(cls, left, right, join=\'none\'):\n        """"""\n        Return a new workspace specification that is the combination of the two workspaces.\n\n        The new workspace must also be a valid workspace. A combination of\n        workspaces is done by combining the set of:\n\n          - channels,\n          - observations, and\n          - measurements\n\n        between the two workspaces. If the two workspaces have modifiers that\n        follow the same naming convention, then correlations across the two\n        workspaces may be possible. In particular, the `lumi` modifier will be\n        fully-correlated.\n\n        If the two workspaces have the same measurement (with the same POI),\n        those measurements will get merged.\n\n        Raises:\n          ~pyhf.exceptions.InvalidWorkspaceOperation: The workspaces have common channel names, incompatible measurements, or incompatible schema versions.\n\n        Args:\n            left (~pyhf.workspace.Workspace): A workspace\n            right (~pyhf.workspace.Workspace): Another workspace\n            join (:obj:`str`): How to join the two workspaces. Pick from ""none"", ""outer"", ""left outer"", or ""right outer"".\n\n        Returns:\n            ~pyhf.workspace.Workspace: A new combined workspace object\n\n        """"""\n        if join not in Workspace.valid_joins:\n            raise ValueError(\n                f""Workspaces must be joined using one of the valid join operations ({Workspace.valid_joins}); not {join}""\n            )\n        if join in [\'left outer\', \'right outer\']:\n            log.warning(\n                ""You are using an unsafe join operation. This will silence exceptions that might be raised during a normal \'outer\' operation.""\n            )\n\n        new_version = _join_versions(join, left[\'version\'], right[\'version\'])\n        new_channels = _join_channels(join, left[\'channels\'], right[\'channels\'])\n        new_observations = _join_observations(\n            join, left[\'observations\'], right[\'observations\']\n        )\n        new_measurements = _join_measurements(\n            join, left[\'measurements\'], right[\'measurements\']\n        )\n\n        newspec = {\n            \'channels\': new_channels,\n            \'measurements\': new_measurements,\n            \'observations\': new_observations,\n            \'version\': new_version,\n        }\n        return Workspace(newspec)\n'"
src/pyhf/writexml.py,8,"b'import logging\n\nfrom pathlib import Path\nimport shutil\nimport pkg_resources\nimport xml.etree.cElementTree as ET\nimport numpy as np\nimport uproot\nfrom uproot_methods.classes import TH1\n\n_ROOT_DATA_FILE = None\n\nlog = logging.getLogger(__name__)\n\n# \'spec\' gets passed through all functions as NormFactor is a unique case of having\n# parameter configurations stored at the modifier-definition-spec level. This means\n# that build_modifier() needs access to the measurements. The call stack is:\n#\n#      writexml\n#          ->build_channel\n#              ->build_sample\n#                  ->build_modifier\n#\n#  Therefore, \'spec\' needs to be threaded through all these calls.\n\n\ndef _make_hist_name(channel, sample, modifier=\'\', prefix=\'hist\', suffix=\'\'):\n    return ""{prefix}{middle}{suffix}"".format(\n        prefix=prefix,\n        suffix=suffix,\n        middle=\'_\'.join(filter(lambda x: x, [channel, sample, modifier])),\n    )\n\n\ndef _export_root_histogram(histname, data):\n    h = TH1.from_numpy((np.asarray(data), np.arange(len(data) + 1)))\n    h._fName = histname\n    # NB: uproot crashes for some reason, figure out why later\n    # if histname in _ROOT_DATA_FILE:\n    #    raise KeyError(\'Duplicate key {0} being written.\'.format(histname))\n    _ROOT_DATA_FILE[histname] = h\n\n\n# https://stackoverflow.com/a/4590052\ndef indent(elem, level=0):\n    i = ""\\n"" + level * ""  ""\n    if elem:\n        if not elem.text or not elem.text.strip():\n            elem.text = i + ""  ""\n        if not elem.tail or not elem.tail.strip():\n            elem.tail = i\n        for subelem in elem:\n            indent(subelem, level + 1)\n        if not elem.tail or not elem.tail.strip():\n            elem.tail = i\n    else:\n        if level and (not elem.tail or not elem.tail.strip()):\n            elem.tail = i\n\n\ndef build_measurement(measurementspec):\n    config = measurementspec[\'config\']\n    name = measurementspec[\'name\']\n    poi = config[\'poi\']\n\n    # we want to know which parameters are fixed (constant)\n    # and to additionally extract the luminosity information\n    fixed_params = []\n    lumi = 1.0\n    lumierr = 0.0\n    for parameter in config[\'parameters\']:\n        if parameter.get(\'fixed\', False):\n            pname = parameter[\'name\']\n            if pname == \'lumi\':\n                fixed_params.append(\'Lumi\')\n            else:\n                fixed_params.append(pname)\n        # we found luminosity, so handle it\n        if parameter[\'name\'] == \'lumi\':\n            lumi = parameter[\'auxdata\'][0]\n            lumierr = parameter[\'sigmas\'][0]\n\n    # define measurement\n    meas = ET.Element(\n        ""Measurement"",\n        Name=name,\n        Lumi=str(lumi),\n        LumiRelErr=str(lumierr),\n        ExportOnly=str(True),\n    )\n    poiel = ET.Element(\'POI\')\n    poiel.text = poi\n    meas.append(poiel)\n\n    # add fixed parameters (constant)\n    if fixed_params:\n        se = ET.Element(\'ParamSetting\', Const=\'True\')\n        se.text = \' \'.join(fixed_params)\n        meas.append(se)\n    return meas\n\n\ndef build_modifier(spec, modifierspec, channelname, samplename, sampledata):\n    if modifierspec[\'name\'] == \'lumi\':\n        return None\n    mod_map = {\n        \'histosys\': \'HistoSys\',\n        \'staterror\': \'StatError\',\n        \'normsys\': \'OverallSys\',\n        \'shapesys\': \'ShapeSys\',\n        \'normfactor\': \'NormFactor\',\n        \'shapefactor\': \'ShapeFactor\',\n    }\n\n    attrs = {\'Name\': modifierspec[\'name\']}\n    if modifierspec[\'type\'] == \'histosys\':\n        attrs[\'HistoNameLow\'] = _make_hist_name(\n            channelname, samplename, modifierspec[\'name\'], suffix=\'Low\'\n        )\n        attrs[\'HistoNameHigh\'] = _make_hist_name(\n            channelname, samplename, modifierspec[\'name\'], suffix=\'High\'\n        )\n        _export_root_histogram(attrs[\'HistoNameLow\'], modifierspec[\'data\'][\'lo_data\'])\n        _export_root_histogram(attrs[\'HistoNameHigh\'], modifierspec[\'data\'][\'hi_data\'])\n    elif modifierspec[\'type\'] == \'normsys\':\n        attrs[\'High\'] = str(modifierspec[\'data\'][\'hi\'])\n        attrs[\'Low\'] = str(modifierspec[\'data\'][\'lo\'])\n    elif modifierspec[\'type\'] == \'normfactor\':\n        # NB: only look at first measurement for normfactor configs. In order\n        # to dump as HistFactory XML, this has to be the same for all\n        # measurements or it will not work correctly. Why?\n        #\n        # Unlike other modifiers, NormFactor has the unique circumstance of\n        # defining its parameter configurations at the modifier level inside\n        # the channel specification, instead of at the measurement level, like\n        # all of the other modifiers.\n        #\n        # However, since I strive for perfection, the ""Const"" attribute will\n        # never be set here, but at the per-measurement configuration instead\n        # like all other parameters. This is an acceptable compromise.\n        #\n        # Lastly, if a normfactor parameter configuration doesn\'t exist in the\n        # first measurement parameter configuration, then set defaults.\n        val = 1\n        low = 0\n        high = 10\n        for p in spec[\'measurements\'][0][\'config\'][\'parameters\']:\n            if p[\'name\'] == modifierspec[\'name\']:\n                val = p[\'inits\'][0]\n                low, high = p[\'bounds\'][0]\n        attrs[\'Val\'] = str(val)\n        attrs[\'Low\'] = str(low)\n        attrs[\'High\'] = str(high)\n    elif modifierspec[\'type\'] == \'staterror\':\n        attrs[\'Activate\'] = \'True\'\n        attrs[\'HistoName\'] = _make_hist_name(\n            channelname, samplename, modifierspec[\'name\']\n        )\n        del attrs[\'Name\']\n        # need to make this a relative uncertainty stored in ROOT file\n        _export_root_histogram(\n            attrs[\'HistoName\'],\n            np.divide(\n                modifierspec[\'data\'],\n                sampledata,\n                out=np.zeros_like(sampledata),\n                where=np.asarray(sampledata) != 0,\n                dtype=\'float\',\n            ).tolist(),\n        )\n    elif modifierspec[\'type\'] == \'shapesys\':\n        attrs[\'ConstraintType\'] = \'Poisson\'\n        attrs[\'HistoName\'] = _make_hist_name(\n            channelname, samplename, modifierspec[\'name\']\n        )\n        # need to make this a relative uncertainty stored in ROOT file\n        _export_root_histogram(\n            attrs[\'HistoName\'],\n            [\n                np.divide(\n                    a, b, out=np.zeros_like(a), where=np.asarray(b) != 0, dtype=\'float\'\n                )\n                for a, b in np.array((modifierspec[\'data\'], sampledata)).T\n            ],\n        )\n    else:\n        log.warning(\n            \'Skipping {0}({1}) for now\'.format(\n                modifierspec[\'name\'], modifierspec[\'type\']\n            )\n        )\n\n    modifier = ET.Element(mod_map[modifierspec[\'type\']], **attrs)\n    return modifier\n\n\ndef build_sample(spec, samplespec, channelname):\n    histname = _make_hist_name(channelname, samplespec[\'name\'])\n    attrs = {\n        \'Name\': samplespec[\'name\'],\n        \'HistoName\': histname,\n        \'InputFile\': _ROOT_DATA_FILE._path,\n        \'NormalizeByTheory\': \'False\',\n    }\n    sample = ET.Element(\'Sample\', **attrs)\n    for modspec in samplespec[\'modifiers\']:\n        # if lumi modifier added for this sample, need to set NormalizeByTheory\n        if modspec[\'type\'] == \'lumi\':\n            sample.attrib.update({\'NormalizeByTheory\': \'True\'})\n        modifier = build_modifier(\n            spec, modspec, channelname, samplespec[\'name\'], samplespec[\'data\']\n        )\n        if modifier is not None:\n            sample.append(modifier)\n    _export_root_histogram(histname, samplespec[\'data\'])\n    return sample\n\n\ndef build_data(obsspec, channelname):\n    histname = _make_hist_name(channelname, \'data\')\n    data = ET.Element(\'Data\', HistoName=histname, InputFile=_ROOT_DATA_FILE._path)\n\n    observation = next((obs for obs in obsspec if obs[\'name\'] == channelname), None)\n    _export_root_histogram(histname, observation[\'data\'])\n    return data\n\n\ndef build_channel(spec, channelspec, obsspec):\n    channel = ET.Element(\n        \'Channel\', Name=channelspec[\'name\'], InputFile=_ROOT_DATA_FILE._path\n    )\n    if obsspec:\n        data = build_data(obsspec, channelspec[\'name\'])\n        channel.append(data)\n    for samplespec in channelspec[\'samples\']:\n        channel.append(build_sample(spec, samplespec, channelspec[\'name\']))\n    return channel\n\n\ndef writexml(spec, specdir, data_rootdir, resultprefix):\n    global _ROOT_DATA_FILE\n\n    shutil.copyfile(\n        pkg_resources.resource_filename(__name__, \'schemas/HistFactorySchema.dtd\'),\n        Path(specdir).parent.joinpath(\'HistFactorySchema.dtd\'),\n    )\n    combination = ET.Element(\n        ""Combination"", OutputFilePrefix=str(Path(specdir).joinpath(resultprefix)),\n    )\n\n    with uproot.recreate(\n        str(Path(data_rootdir).joinpath(\'data.root\'))\n    ) as _ROOT_DATA_FILE:\n        for channelspec in spec[\'channels\']:\n            channelfilename = str(\n                Path(specdir).joinpath(f\'{resultprefix}_{channelspec[""name""]}.xml\')\n            )\n            with open(channelfilename, \'w\') as channelfile:\n                channel = build_channel(spec, channelspec, spec.get(\'observations\'))\n                indent(channel)\n                channelfile.write(\n                    ""<!DOCTYPE Channel SYSTEM \'../HistFactorySchema.dtd\'>\\n\\n""\n                )\n                channelfile.write(\n                    ET.tostring(channel, encoding=\'utf-8\').decode(\'utf-8\')\n                )\n\n            inp = ET.Element(""Input"")\n            inp.text = channelfilename\n            combination.append(inp)\n\n    for measurement in spec[\'measurements\']:\n        combination.append(build_measurement(measurement))\n    indent(combination)\n    return ""<!DOCTYPE Combination  SYSTEM \'HistFactorySchema.dtd\'>\\n\\n"".encode(\n        ""utf-8""\n    ) + ET.tostring(combination, encoding=\'utf-8\')\n'"
tests/benchmarks/test_benchmark.py,5,"b'import pyhf\nfrom pyhf.simplemodels import hepdata_like\nimport numpy as np\nimport pytest\n\n\ndef generate_source_static(n_bins):\n    """"""\n    Create the source structure for the given number of bins.\n\n    Args:\n        n_bins: `list` of number of bins\n\n    Returns:\n        source\n    """"""\n    binning = [n_bins, -0.5, n_bins + 0.5]\n    data = [120.0] * n_bins\n    bkg = [100.0] * n_bins\n    bkgerr = [10.0] * n_bins\n    sig = [30.0] * n_bins\n\n    source = {\n        \'binning\': binning,\n        \'bindata\': {\'data\': data, \'bkg\': bkg, \'bkgerr\': bkgerr, \'sig\': sig},\n    }\n    return source\n\n\ndef generate_source_poisson(n_bins):\n    """"""\n    Create the source structure for the given number of bins.\n    Sample from a Poisson distribution\n\n    Args:\n        n_bins: `list` of number of bins\n\n    Returns:\n        source\n    """"""\n    np.random.seed(0)  # Fix seed for reproducibility\n    binning = [n_bins, -0.5, n_bins + 0.5]\n    data = np.random.poisson(120.0, n_bins).tolist()\n    bkg = np.random.poisson(100.0, n_bins).tolist()\n    bkgerr = np.random.poisson(10.0, n_bins).tolist()\n    sig = np.random.poisson(30.0, n_bins).tolist()\n\n    source = {\n        \'binning\': binning,\n        \'bindata\': {\'data\': data, \'bkg\': bkg, \'bkgerr\': bkgerr, \'sig\': sig},\n    }\n    return source\n\n\ndef hypotest(pdf, data):\n    return pyhf.infer.hypotest(\n        1.0,\n        data,\n        pdf,\n        pdf.config.suggested_init(),\n        pdf.config.suggested_bounds(),\n        return_tail_probs=True,\n        return_expected=True,\n        return_expected_set=True,\n    )\n\n\nbins = [1, 10, 50, 100, 200]\nbin_ids = [\'{}_bins\'.format(n_bins) for n_bins in bins]\n\n\n@pytest.mark.parametrize(\'n_bins\', bins, ids=bin_ids)\ndef test_hypotest(benchmark, backend, n_bins):\n    """"""\n    Benchmark the performance of pyhf.utils.hypotest()\n    for various numbers of bins and different backends\n\n    Args:\n        benchmark: pytest benchmark\n        backend: `pyhf` tensorlib given by pytest parameterization\n        n_bins: `list` of number of bins given by pytest parameterization\n\n    Returns:\n        None\n    """"""\n    source = generate_source_static(n_bins)\n    pdf = hepdata_like(\n        source[\'bindata\'][\'sig\'], source[\'bindata\'][\'bkg\'], source[\'bindata\'][\'bkgerr\']\n    )\n    data = source[\'bindata\'][\'data\'] + pdf.config.auxdata\n    assert benchmark(hypotest, pdf, data)\n'"
tests/contrib/test_viz.py,0,"b""import pytest\nimport json\nimport pyhf.contrib.viz.brazil as brazil\nimport matplotlib.pyplot as plt\n\n\n@pytest.mark.mpl_image_compare\ndef test_brazil():\n    data = json.load(open('tests/contrib/hypotestresults.json'))\n    fig, ax = plt.subplots(1, 1)\n    brazil.plot_results(ax, data['testmus'], data['results'], test_size=0.05)\n    return fig\n"""
validation/manualonoff_roofit/onoff.py,0,"b'import json\nimport ROOT\n\nd = json.load(open(\'data/source.json\'))\nnobs = d[\'bindata\'][\'data\'][0]\nb = d[\'bindata\'][\'bkg\'][0]\ndeltab = d[\'bindata\'][\'bkgerr\'][0]\ns = d[\'bindata\'][\'sig\'][0]\n\n# derived data\ntau = b / deltab / deltab\nmobs = round(tau * b)\n\nprint(\'tau: {}, m: {}\'.format(tau, mobs))\n\nw = ROOT.RooWorkspace(""w"", True)\n\n# -----------------\n\nw.factory(""prod:nsig(mu[1,0,10],s[1])"")\nw.factory(""sum:nexp_sr(nsig,b[1,40,300])"")\nw.factory(""Poisson:on_model(nobs_sr[0,1000],nexp_sr)"")\n\n# -----------------\n\nw.var(\'s\').setVal(s)\nw.var(\'b\').setVal(b)\n\nw.var(\'s\').setConstant(True)\nw.var(\'nobs_sr\').setVal(nobs)\n\n\nw.factory(""prod:nexp_cr(tau[1],b)"")\nw.factory(""Poisson:off_model(nobs_cr[0,1000],nexp_cr)"")\nw.var(\'nobs_cr\').setVal(mobs)\nw.var(\'nobs_cr\').setConstant(True)\nw.var(\'tau\').setVal(tau)\nw.var(\'tau\').setConstant(True)\n\nw.factory(""PROD:onoff(on_model,off_model)"")\n\n\ndata = ROOT.RooDataSet(\n    \'data\', \'data\', ROOT.RooArgSet(w.var(\'nobs_sr\'), w.var(\'nobs_cr\'))\n)\ndata.add(ROOT.RooArgSet(w.var(\'nobs_sr\'), w.var(\'nobs_cr\')))\n\ngetattr(w, \'import\')(data)\n\nmodelConfig = ROOT.RooStats.ModelConfig(w)\nmodelConfig.SetPdf(w.pdf(\'onoff\'))\nmodelConfig.SetParametersOfInterest(ROOT.RooArgSet(w.var(\'mu\')))\nmodelConfig.SetNuisanceParameters(ROOT.RooArgSet(w.var(\'b\')))\nmodelConfig.SetObservables(ROOT.RooArgSet(w.var(\'nobs_sr\'), w.var(\'nobs_cr\')))\nmodelConfig.SetGlobalObservables(ROOT.RooArgSet())\nmodelConfig.SetName(""ModelConfig"")\ngetattr(w, \'import\')(modelConfig)\n\nw.Print()\n\n\n##### model building complete\n\n\nsbModel = w.obj(\'ModelConfig\')\npoi = sbModel.GetParametersOfInterest().first()\n\nsbModel.SetSnapshot(ROOT.RooArgSet(poi))\n\nbModel = sbModel.Clone()\nbModel.SetName(""bonly"")\npoi.setVal(0)\nbModel.SetSnapshot(ROOT.RooArgSet(poi))\n\n\nac = ROOT.RooStats.AsymptoticCalculator(data, bModel, sbModel)\nac.SetOneSided(True)\n# ac.SetQTilde(False)\nac.SetPrintLevel(10)\nROOT.RooStats.AsymptoticCalculator.SetPrintLevel(10)\n\ncalc = ROOT.RooStats.HypoTestInverter(ac)\ncalc.RunFixedScan(51, 0, 5)\ncalc.SetConfidenceLevel(0.95)\ncalc.UseCLs(True)\n\nresult = calc.GetInterval()\n\nplot = ROOT.RooStats.HypoTestInverterPlot(""plot"", ""plot"", result)\nc = ROOT.TCanvas()\nc.SetLogy(False)\nplot.Draw(""OBS EXP CLb 2CL"")\nc.Draw()\nc.SaveAs(\'scan.pdf\')\n\nprint(\'observed: {}\'.format(result.UpperLimit()))\nfor i in [-2, -1, 0, 1, 2]:\n    print(\'expected {}: {}\'.format(i, result.GetExpectedUpperLimit(i)))\n'"
validation/multichan_coupledhistosys_histfactory/makedata.py,0,"b""import ROOT\n\nimport json\nimport sys\n\nsource_data = json.load(open(sys.argv[1]))\nroot_file = sys.argv[2]\n\nf = ROOT.TFile(root_file, 'RECREATE')\n\n\nhists = []\nfor cname, channel_def in source_data['channels'].iteritems():\n    print('CH', cname)\n    binning = channel_def['binning']\n    bindata = channel_def['bindata']\n\n    for hist, data in bindata.iteritems():\n        print('{}_{}'.format(cname, hist))\n        h = ROOT.TH1F(\n            '{}_{}'.format(cname, hist), '{}_{}'.format(cname, hist), *binning\n        )\n        hists += [h]\n        for i, v in enumerate(data):\n            h.SetBinContent(i + 1, v)\n        h.Sumw2()\n\nf.Write()\n"""
validation/multichan_coupledoverall_histfactory/makedata.py,0,"b""import ROOT\n\nimport json\nimport sys\n\nsource_data = json.load(open(sys.argv[1]))\nroot_file = sys.argv[2]\n\nf = ROOT.TFile(root_file, 'RECREATE')\n\nfor cname, channel_def in source_data['channels'].iteritems():\n    print('CH', cname)\n    binning = channel_def['binning']\n    bindata = channel_def['bindata']\n\n    data = ROOT.TH1F('{}_data'.format(cname), '{}_data'.format(cname), *binning)\n    for i, v in enumerate(bindata['data']):\n        data.SetBinContent(i + 1, v)\n    data.Sumw2()\n\n    bkg1 = ROOT.TH1F('{}_bkg1'.format(cname), '{}_bkg1'.format(cname), *binning)\n    for i, v in enumerate(bindata['bkg1']):\n        bkg1.SetBinContent(i + 1, v)\n    bkg1.Sumw2()\n\n    if 'bkg2' in bindata:\n        bkg2 = ROOT.TH1F('{}_bkg2'.format(cname), '{}_bkg2'.format(cname), *binning)\n        for i, v in enumerate(bindata['bkg2']):\n            bkg2.SetBinContent(i + 1, v)\n        bkg2.Sumw2()\n\n    if 'sig' in bindata:\n        sig = ROOT.TH1F('{}_signal'.format(cname), '{}_signal'.format(cname), *binning)\n        for i, v in enumerate(bindata['sig']):\n            sig.SetBinContent(i + 1, v)\n        sig.Sumw2()\n    f.Write()\n"""
validation/multichannel_histfactory/makedata.py,0,"b""import ROOT\n\nimport json\nimport sys\n\nsource_data = json.load(open(sys.argv[1]))\nroot_file = sys.argv[2]\n\nf = ROOT.TFile(root_file, 'RECREATE')\n\nfor cname, channel_def in source_data['channels'].iteritems():\n    print('CH', cname)\n    binning = channel_def['binning']\n    bindata = channel_def['bindata']\n\n    data = ROOT.TH1F('{}_data'.format(cname), '{}_data'.format(cname), *binning)\n    for i, v in enumerate(bindata['data']):\n        data.SetBinContent(i + 1, v)\n    data.Sumw2()\n\n    print(data.GetName())\n\n    bkg = ROOT.TH1F('{}_bkg'.format(cname), '{}_bkg'.format(cname), *binning)\n    for i, v in enumerate(bindata['bkg']):\n        bkg.SetBinContent(i + 1, v)\n    bkg.Sumw2()\n\n    if 'bkgerr' in bindata:\n        bkgerr = ROOT.TH1F(\n            '{}_bkgerr'.format(cname), '{}_bkgerr'.format(cname), *binning\n        )\n\n        # shapesys must be as multiplicative factor\n        for i, v in enumerate(bindata['bkgerr']):\n            bkgerr.SetBinContent(i + 1, v / bkg.GetBinContent(i + 1))\n        bkgerr.Sumw2()\n\n    if 'sig' in bindata:\n        sig = ROOT.TH1F('{}_signal'.format(cname), '{}_signal'.format(cname), *binning)\n        for i, v in enumerate(bindata['sig']):\n            sig.SetBinContent(i + 1, v)\n        sig.Sumw2()\n    f.Write()\n"""
validation/shared_nuispar_across_types/make_data.py,0,"b""import ROOT\n\nsig = 'sig', [3, 1]\nnom = 'nom', [12, 13]\n\nhisto_up = 'hup', [14, 15]\nhisto_dn = 'hdn', [10, 11]\n\ndata = 'data', [15, 16]\n\nf = ROOT.TFile.Open('data.root', 'recreate')\n\n\nfor n, h in [sig, nom, histo_up, histo_dn, data]:\n    rh = ROOT.TH1F(n, n, 2, -0.5, 1.5)\n    for i, c in enumerate(h):\n        rh.SetBinContent(1 + i, c)\n    rh.Sumw2()\n    rh.Write()\n\nf.Close()\n"""
validation/xmlimport_input2/makedata.py,0,"b""import ROOT\n\nimport json\nimport sys\n\nsource_data = json.load(open(sys.argv[1]))\nroot_file = sys.argv[2]\n\nf = ROOT.TFile(root_file, 'RECREATE')\n\n\nhists = []\nfor cname, channel_def in source_data['channels'].iteritems():\n    print('CH', cname)\n    binning = channel_def['binning']\n    bindata = channel_def['bindata']\n\n    for hist, data in bindata.iteritems():\n        print('{}_{}'.format(cname, hist))\n        h = ROOT.TH1F(\n            '{}_{}'.format(cname, hist), '{}_{}'.format(cname, hist), *binning\n        )\n        hists += [h]\n        for i, v in enumerate(data):\n            h.SetBinContent(i + 1, v)\n        h.Sumw2()\n\nf.Write()\n"""
src/pyhf/cli/__init__.py,0,"b'""""""The pyhf command line interface.""""""\nfrom .cli import pyhf as cli\nfrom .rootio import cli as rootio\nfrom .spec import cli as spec\nfrom .infer import cli as infer\n\n__all__ = [\'cli\', \'rootio\', \'spec\', \'infer\']\n'"
src/pyhf/cli/cli.py,0,"b'""""""The pyhf Command Line Interface.""""""\nimport logging\n\nimport click\n\nfrom ..version import __version__\nfrom . import rootio, spec, infer, patchset\n\nlogging.basicConfig()\nlog = logging.getLogger(__name__)\n\n\n@click.group(context_settings=dict(help_option_names=[\'-h\', \'--help\']))\n@click.version_option(version=__version__)\ndef pyhf():\n    """"""Top-level CLI entrypoint.""""""\n\n\n# pyhf.add_command(rootio.cli)\npyhf.add_command(rootio.json2xml)\npyhf.add_command(rootio.xml2json)\n\n# pyhf.add_command(spec.cli)\npyhf.add_command(spec.inspect)\npyhf.add_command(spec.prune)\npyhf.add_command(spec.rename)\npyhf.add_command(spec.combine)\npyhf.add_command(spec.digest)\n\n# pyhf.add_command(infer.cli)\npyhf.add_command(infer.cls)\n\npyhf.add_command(patchset.cli)\n'"
src/pyhf/cli/infer.py,0,"b'""""""The inference CLI group.""""""\nimport logging\n\nimport click\nimport json\n\nfrom ..utils import EqDelimStringParamType\nfrom ..infer import hypotest\nfrom ..workspace import Workspace\nfrom .. import tensor, get_backend, set_backend, optimize\n\nlog = logging.getLogger(__name__)\n\n\n@click.group(name=\'infer\')\ndef cli():\n    """"""Infererence CLI group.""""""\n\n\n@cli.command()\n@click.argument(\'workspace\', default=\'-\')\n@click.option(\n    \'--output-file\',\n    help=\'The location of the output json file. If not specified, prints to screen.\',\n    default=None,\n)\n@click.option(\'--measurement\', default=None)\n@click.option(\'-p\', \'--patch\', multiple=True)\n@click.option(\'--testpoi\', default=1.0)\n@click.option(\'--teststat\', type=click.Choice([\'q\', \'qtilde\']), default=\'qtilde\')\n@click.option(\n    \'--backend\',\n    type=click.Choice([\'numpy\', \'pytorch\', \'tensorflow\', \'jax\', \'np\', \'torch\', \'tf\']),\n    help=\'The tensor backend used for the calculation.\',\n    default=\'numpy\',\n)\n@click.option(\'--optimizer\')\n@click.option(\'--optconf\', type=EqDelimStringParamType(), multiple=True)\ndef cls(\n    workspace,\n    output_file,\n    measurement,\n    patch,\n    testpoi,\n    teststat,\n    backend,\n    optimizer,\n    optconf,\n):\n    """"""\n    Compute CLs value(s) for a given pyhf workspace.\n\n    Example:\n\n    .. code-block:: shell\n\n        $ curl -sL https://raw.githubusercontent.com/scikit-hep/pyhf/master/docs/examples/json/2-bin_1-channel.json | pyhf cls\n        {\n            ""CLs_exp"": [\n                0.07807427911686156,\n                0.17472571775474618,\n                0.35998495263681285,\n                0.6343568235898907,\n                0.8809947004472013\n            ],\n            ""CLs_obs"": 0.3599845631401915\n        }\n    """"""\n    with click.open_file(workspace, \'r\') as specstream:\n        spec = json.load(specstream)\n\n    ws = Workspace(spec)\n\n    is_qtilde = teststat == \'qtilde\'\n\n    patches = [json.loads(click.open_file(pfile, \'r\').read()) for pfile in patch]\n    model = ws.model(\n        measurement_name=measurement,\n        patches=patches,\n        modifier_settings={\n            \'normsys\': {\'interpcode\': \'code4\'},\n            \'histosys\': {\'interpcode\': \'code4p\'},\n        },\n    )\n\n    # set the backend if not NumPy\n    if backend in [\'pytorch\', \'torch\']:\n        set_backend(tensor.pytorch_backend(float=\'float64\'))\n    elif backend in [\'tensorflow\', \'tf\']:\n        set_backend(tensor.tensorflow_backend(float=\'float64\'))\n    elif backend in [\'jax\']:\n        set_backend(tensor.jax_backend())\n    tensorlib, _ = get_backend()\n\n    optconf = {k: v for item in optconf for k, v in item.items()}\n\n    # set the new optimizer\n    if optimizer:\n        new_optimizer = getattr(optimize, optimizer)\n        set_backend(tensorlib, new_optimizer(**optconf))\n\n    result = hypotest(\n        testpoi, ws.data(model), model, qtilde=is_qtilde, return_expected_set=True\n    )\n    result = {\n        \'CLs_obs\': tensorlib.tolist(result[0])[0],\n        \'CLs_exp\': tensorlib.tolist(tensorlib.reshape(result[-1], [-1])),\n    }\n\n    if output_file is None:\n        click.echo(json.dumps(result, indent=4, sort_keys=True))\n    else:\n        with open(output_file, \'w+\') as out_file:\n            json.dump(result, out_file, indent=4, sort_keys=True)\n        log.debug(""Written to {0:s}"".format(output_file))\n'"
src/pyhf/cli/patchset.py,0,"b'""""""The pyhf spec CLI subcommand.""""""\nimport logging\n\nimport click\nimport json\n\nfrom ..patchset import PatchSet\nfrom ..workspace import Workspace\n\nlogging.basicConfig()\nlog = logging.getLogger(__name__)\n\n\n@click.group(name=\'patchset\')\ndef cli():\n    """"""Operations involving patchsets.""""""\n\n\n@cli.command()\n@click.argument(\'patchset\', default=\'-\')\n@click.option(\n    \'--name\', help=\'The name of the patch to extract.\', default=None,\n)\n@click.option(\n    \'--output-file\',\n    help=\'The location of the output json file. If not specified, prints to screen.\',\n    default=None,\n)\n@click.option(\n    \'--with-metadata/--without-metadata\',\n    default=False,\n    help=""Include patchset metadata in output."",\n)\ndef extract(patchset, name, output_file, with_metadata):\n    """"""\n    Extract a patch from a patchset.\n\n    Raises:\n        :class:`~pyhf.exceptions.InvalidPatchLookup`: if the provided patch name is not in the patchset\n\n    Returns:\n        jsonpatch (:obj:`list`): A list of jsonpatch operations to apply to a workspace.\n    """"""\n    with click.open_file(patchset, \'r\') as fstream:\n        patchset_spec = json.load(fstream)\n\n    patchset = PatchSet(patchset_spec)\n    patch = patchset[name]\n\n    if with_metadata:\n        result = {\'metadata\': patch.metadata, \'patch\': patch.patch}\n        result[\'metadata\'].update(patchset.metadata)\n    else:\n        result = patch.patch\n\n    if output_file:\n        with open(output_file, \'w+\') as out_file:\n            json.dump(result, out_file, indent=4, sort_keys=True)\n        log.debug(""Written to {0:s}"".format(output_file))\n    else:\n        click.echo(json.dumps(result, indent=4, sort_keys=True))\n\n\n@cli.command()\n@click.argument(\'background-only\', default=\'-\')\n@click.argument(\'patchset\', default=\'-\')\n@click.option(\n    \'--name\', help=\'The name of the patch to extract.\', default=None,\n)\n@click.option(\n    \'--output-file\',\n    help=\'The location of the output json file. If not specified, prints to screen.\',\n    default=None,\n)\ndef apply(background_only, patchset, name, output_file):\n    """"""\n    Apply a patch from patchset to the background-only workspace specification.\n\n    Raises:\n        :class:`~pyhf.exceptions.InvalidPatchLookup`: if the provided patch name is not in the patchset\n        :class:`~pyhf.exceptions.PatchSetVerificationError`: if the patchset cannot be verified against the workspace specification\n\n    Returns:\n        workspace (:class:`~pyhf.workspace.Workspace`): The patched background-only workspace.\n    """"""\n    with click.open_file(background_only, \'r\') as specstream:\n        spec = json.load(specstream)\n\n    ws = Workspace(spec)\n\n    with click.open_file(patchset, \'r\') as fstream:\n        patchset_spec = json.load(fstream)\n\n    patchset = PatchSet(patchset_spec)\n    patched_ws = patchset.apply(ws, name)\n\n    if output_file:\n        with open(output_file, \'w+\') as out_file:\n            json.dump(patched_ws, out_file, indent=4, sort_keys=True)\n        log.debug(""Written to {0:s}"".format(output_file))\n    else:\n        click.echo(json.dumps(patched_ws, indent=4, sort_keys=True))\n\n\n@cli.command()\n@click.argument(\'background-only\', default=\'-\')\n@click.argument(\'patchset\', default=\'-\')\ndef verify(background_only, patchset):\n    """"""\n    Verify the patchset digests against a background-only workspace specification. Verified if no exception was raised.\n\n    Raises:\n        :class:`~pyhf.exceptions.PatchSetVerificationError`: if the patchset cannot be verified against the workspace specification\n\n    Returns:\n        None\n    """"""\n    with click.open_file(background_only, \'r\') as specstream:\n        spec = json.load(specstream)\n\n    ws = Workspace(spec)\n\n    with click.open_file(patchset, \'r\') as fstream:\n        patchset_spec = json.load(fstream)\n\n    patchset = PatchSet(patchset_spec)\n    patchset.verify(ws)\n\n    click.echo(""All good."")\n'"
src/pyhf/cli/rootio.py,0,"b'""""""CLI subapps to handle conversion from ROOT.""""""\nimport logging\n\nimport click\nimport json\nimport os\nfrom pathlib import Path\nimport jsonpatch\n\nlog = logging.getLogger(__name__)\n\n\n@click.group(name=\'rootio\')\ndef cli():\n    """"""ROOT I/O CLI group.""""""\n\n\n@cli.command()\n@click.argument(\'entrypoint-xml\', type=click.Path(exists=True))\n@click.option(\n    \'--basedir\',\n    help=\'The base directory for the XML files to point relative to.\',\n    type=click.Path(exists=True),\n    default=Path.cwd(),\n)\n@click.option(\n    \'--output-file\',\n    help=\'The location of the output json file. If not specified, prints to screen.\',\n    default=None,\n)\n@click.option(\'--track-progress/--hide-progress\', default=True)\ndef xml2json(entrypoint_xml, basedir, output_file, track_progress):\n    """"""Entrypoint XML: The top-level XML file for the PDF definition.""""""\n    try:\n        import uproot\n\n        assert uproot\n    except ImportError:\n        log.error(\n            ""xml2json requires uproot, please install pyhf using the ""\n            ""xmlio extra: python -m pip install pyhf[xmlio]""\n        )\n    from .. import readxml\n\n    spec = readxml.parse(entrypoint_xml, basedir, track_progress=track_progress)\n    if output_file is None:\n        click.echo(json.dumps(spec, indent=4, sort_keys=True))\n    else:\n        with open(output_file, \'w+\') as out_file:\n            json.dump(spec, out_file, indent=4, sort_keys=True)\n        log.debug(""Written to {0:s}"".format(output_file))\n\n\n@cli.command()\n@click.argument(\'workspace\', default=\'-\')\n@click.option(\'--output-dir\', type=click.Path(exists=True), default=\'.\')\n@click.option(\'--specroot\', default=\'config\')\n@click.option(\'--dataroot\', default=\'data\')\n@click.option(\'--resultprefix\', default=\'FitConfig\')\n@click.option(\'-p\', \'--patch\', multiple=True)\ndef json2xml(workspace, output_dir, specroot, dataroot, resultprefix, patch):\n    """"""Convert pyhf JSON back to XML + ROOT files.""""""\n    try:\n        import uproot\n\n        assert uproot\n    except ImportError:\n        log.error(\n            ""json2xml requires uproot, please install pyhf using the ""\n            ""xmlio extra: python -m pip install pyhf[xmlio]""\n        )\n    from .. import writexml\n\n    os.makedirs(output_dir, exist_ok=True)\n    with click.open_file(workspace, \'r\') as specstream:\n        spec = json.load(specstream)\n        for pfile in patch:\n            patch = json.loads(click.open_file(pfile, \'r\').read())\n            spec = jsonpatch.JsonPatch(patch).apply(spec)\n        os.makedirs(Path(output_dir).joinpath(specroot), exist_ok=True)\n        os.makedirs(Path(output_dir).joinpath(dataroot), exist_ok=True)\n        with click.open_file(\n            Path(output_dir).joinpath(f\'{resultprefix}.xml\'), \'w\'\n        ) as outstream:\n            outstream.write(\n                writexml.writexml(\n                    spec,\n                    Path(output_dir).joinpath(specroot),\n                    Path(output_dir).joinpath(dataroot),\n                    resultprefix,\n                ).decode(\'utf-8\')\n            )\n'"
src/pyhf/cli/spec.py,0,"b'""""""The pyhf spec CLI subcommand.""""""\nimport logging\n\nimport click\nimport json\n\nfrom ..workspace import Workspace\nfrom .. import modifiers\nfrom .. import utils\n\nlog = logging.getLogger(__name__)\n\n\n@click.group(name=\'spec\')\ndef cli():\n    """"""Spec CLI group.""""""\n\n\n@cli.command()\n@click.argument(\'workspace\', default=\'-\')\n@click.option(\n    \'--output-file\',\n    help=\'The location of the output json file. If not specified, prints to screen.\',\n    default=None,\n)\n@click.option(\'--measurement\', default=None)\ndef inspect(workspace, output_file, measurement):\n    """"""\n    Inspect a pyhf JSON document.\n\n    Example:\n\n    .. code-block:: shell\n\n        $ curl -sL https://raw.githubusercontent.com/scikit-hep/pyhf/master/docs/examples/json/2-bin_1-channel.json | pyhf inspect\n                  Summary\n            ------------------\n               channels  1\n                samples  2\n             parameters  2\n              modifiers  2\n\n               channels  nbins\n             ----------  -----\n          singlechannel    2\n\n                samples\n             ----------\n             background\n                 signal\n\n             parameters  constraint              modifiers\n             ----------  ----------              ----------\n                     mu  unconstrained           normfactor\n        uncorr_bkguncrt  constrained_by_poisson  shapesys\n\n            measurement           poi            parameters\n             ----------        ----------        ----------\n        (*) Measurement            mu            (none)\n\n    """"""\n    with click.open_file(workspace, \'r\') as specstream:\n        spec = json.load(specstream)\n\n    ws = Workspace(spec)\n    default_measurement = ws.get_measurement()\n\n    result = {}\n    result[\'samples\'] = ws.samples\n    result[\'channels\'] = [\n        (channel, ws.channel_nbins[channel]) for channel in ws.channels\n    ]\n    result[\'modifiers\'] = dict(ws.modifiers)\n\n    result[\'parameters\'] = sorted(\n        (\n            parname,\n            modifiers.registry[result[\'modifiers\'][parname]]\n            .required_parset([], [])[\'paramset_type\']\n            .__name__,\n        )\n        for parname in ws.parameters\n    )\n    result[\'systematics\'] = [\n        (\n            parameter[0],\n            parameter[1],\n            [modifier[1] for modifier in ws.modifiers if modifier[0] == parameter[0]],\n        )\n        for parameter in result[\'parameters\']\n    ]\n\n    result[\'measurements\'] = [\n        (m[\'name\'], m[\'config\'][\'poi\'], [p[\'name\'] for p in m[\'config\'][\'parameters\']])\n        for m in ws.get(\'measurements\')\n    ]\n\n    maxlen_channels = max(map(len, ws.channels))\n    maxlen_samples = max(map(len, ws.samples))\n    maxlen_parameters = max(map(len, ws.parameters))\n    maxlen_measurements = max(map(lambda x: len(x[0]), result[\'measurements\']))\n    maxlen = max(\n        [maxlen_channels, maxlen_samples, maxlen_parameters, maxlen_measurements]\n    )\n\n    # summary\n    fmtStr = \'{{0: >{0:d}s}}  {{1:s}}\'.format(maxlen + len(\'Summary\'))\n    click.echo(fmtStr.format(\'     Summary     \', \'\'))\n    click.echo(fmtStr.format(\'-\' * 18, \'\'))\n    fmtStr = \'{{0: >{0:d}s}}  {{1:s}}\'.format(maxlen)\n    for key in [\'channels\', \'samples\', \'parameters\', \'modifiers\']:\n        click.echo(fmtStr.format(key, str(len(result[key]))))\n    click.echo()\n\n    fmtStr = \'{{0: >{0:d}s}}  {{1: ^5s}}\'.format(maxlen)\n    click.echo(fmtStr.format(\'channels\', \'nbins\'))\n    click.echo(fmtStr.format(\'-\' * 10, \'-\' * 5))\n    for channel, nbins in result[\'channels\']:\n        click.echo(fmtStr.format(channel, str(nbins)))\n    click.echo()\n\n    fmtStr = \'{{0: >{0:d}s}}\'.format(maxlen)\n    click.echo(fmtStr.format(\'samples\'))\n    click.echo(fmtStr.format(\'-\' * 10))\n    for sample in result[\'samples\']:\n        click.echo(fmtStr.format(sample))\n    click.echo()\n\n    # print parameters, constraints, modifiers\n    fmtStr = \'{{0: >{0:d}s}}  {{1: <22s}}  {{2:s}}\'.format(maxlen)\n    click.echo(fmtStr.format(\'parameters\', \'constraint\', \'modifiers\'))\n    click.echo(fmtStr.format(\'-\' * 10, \'-\' * 10, \'-\' * 10))\n    for parname, constraint, modtypes in result[\'systematics\']:\n        click.echo(fmtStr.format(parname, constraint, \',\'.join(sorted(set(modtypes)))))\n    click.echo()\n\n    fmtStr = \'{{0: >{0:d}s}}  {{1: ^22s}}  {{2:s}}\'.format(maxlen)\n    click.echo(fmtStr.format(\'measurement\', \'poi\', \'parameters\'))\n    click.echo(fmtStr.format(\'-\' * 10, \'-\' * 10, \'-\' * 10))\n    for measurement_name, measurement_poi, measurement_parameters in result[\n        \'measurements\'\n    ]:\n        click.echo(\n            fmtStr.format(\n                (\'(*) \' if measurement_name == default_measurement[\'name\'] else \'\')\n                + measurement_name,\n                measurement_poi,\n                \',\'.join(measurement_parameters)\n                if measurement_parameters\n                else \'(none)\',\n            )\n        )\n\n    click.echo()\n\n    if output_file:\n        with open(output_file, \'w+\') as out_file:\n            json.dump(result, out_file, indent=4, sort_keys=True)\n        log.debug(""Written to {0:s}"".format(output_file))\n\n\n@cli.command()\n@click.argument(\'workspace\', default=\'-\')\n@click.option(\n    \'--output-file\',\n    help=\'The location of the output json file. If not specified, prints to screen.\',\n    default=None,\n)\n@click.option(\'-c\', \'--channel\', default=[], multiple=True, metavar=\'<CHANNEL>...\')\n@click.option(\'-s\', \'--sample\', default=[], multiple=True, metavar=\'<SAMPLE>...\')\n@click.option(\'-m\', \'--modifier\', default=[], multiple=True, metavar=\'<MODIFIER>...\')\n@click.option(\n    \'-t\',\n    \'--modifier-type\',\n    default=[],\n    multiple=True,\n    type=click.Choice(modifiers.uncombined.keys()),\n)\n@click.option(\'--measurement\', default=[], multiple=True, metavar=\'<MEASUREMENT>...\')\ndef prune(\n    workspace, output_file, channel, sample, modifier, modifier_type, measurement\n):\n    """"""\n    Prune components from the workspace.\n\n    See :func:`pyhf.workspace.Workspace.prune` for more information.\n    """"""\n    with click.open_file(workspace, \'r\') as specstream:\n        spec = json.load(specstream)\n\n    ws = Workspace(spec)\n    pruned_ws = ws.prune(\n        channels=channel,\n        samples=sample,\n        modifiers=modifier,\n        modifier_types=modifier_type,\n        measurements=measurement,\n    )\n\n    if output_file is None:\n        click.echo(json.dumps(pruned_ws, indent=4, sort_keys=True))\n    else:\n        with open(output_file, \'w+\') as out_file:\n            json.dump(pruned_ws, out_file, indent=4, sort_keys=True)\n        log.debug(""Written to {0:s}"".format(output_file))\n\n\n@cli.command()\n@click.argument(\'workspace\', default=\'-\')\n@click.option(\n    \'--output-file\',\n    help=\'The location of the output json file. If not specified, prints to screen.\',\n    default=None,\n)\n@click.option(\n    \'-c\',\n    \'--channel\',\n    default=[],\n    multiple=True,\n    type=click.Tuple([str, str]),\n    metavar=\'<PATTERN> <REPLACE>...\',\n)\n@click.option(\n    \'-s\',\n    \'--sample\',\n    default=[],\n    multiple=True,\n    type=click.Tuple([str, str]),\n    metavar=\'<PATTERN> <REPLACE>...\',\n)\n@click.option(\n    \'-m\',\n    \'--modifier\',\n    default=[],\n    multiple=True,\n    type=click.Tuple([str, str]),\n    metavar=\'<PATTERN> <REPLACE>...\',\n)\n@click.option(\n    \'--measurement\',\n    default=[],\n    multiple=True,\n    type=click.Tuple([str, str]),\n    metavar=\'<PATTERN> <REPLACE>...\',\n)\ndef rename(workspace, output_file, channel, sample, modifier, measurement):\n    """"""\n    Rename components of the workspace.\n\n    See :func:`pyhf.workspace.Workspace.rename` for more information.\n    """"""\n    with click.open_file(workspace, \'r\') as specstream:\n        spec = json.load(specstream)\n\n    ws = Workspace(spec)\n    renamed_ws = ws.rename(\n        channels=dict(channel),\n        samples=dict(sample),\n        modifiers=dict(modifier),\n        measurements=dict(measurement),\n    )\n\n    if output_file is None:\n        click.echo(json.dumps(renamed_ws, indent=4, sort_keys=True))\n    else:\n        with open(output_file, \'w+\') as out_file:\n            json.dump(renamed_ws, out_file, indent=4, sort_keys=True)\n        log.debug(""Written to {0:s}"".format(output_file))\n\n\n@cli.command()\n@click.argument(\'workspace-one\', default=\'-\')\n@click.argument(\'workspace-two\', default=\'-\')\n@click.option(\n    \'-j\',\n    \'--join\',\n    default=\'none\',\n    type=click.Choice(Workspace.valid_joins),\n    help=\'The join operation to apply when combining the two workspaces.\',\n)\n@click.option(\n    \'--output-file\',\n    help=\'The location of the output json file. If not specified, prints to screen.\',\n    default=None,\n)\ndef combine(workspace_one, workspace_two, join, output_file):\n    """"""\n    Combine two workspaces into a single workspace.\n\n    See :func:`pyhf.workspace.Workspace.combine` for more information.\n    """"""\n    with click.open_file(workspace_one, \'r\') as specstream:\n        spec_one = json.load(specstream)\n\n    with click.open_file(workspace_two, \'r\') as specstream:\n        spec_two = json.load(specstream)\n\n    ws_one = Workspace(spec_one)\n    ws_two = Workspace(spec_two)\n    combined_ws = Workspace.combine(ws_one, ws_two, join=join)\n\n    if output_file is None:\n        click.echo(json.dumps(combined_ws, indent=4, sort_keys=True))\n    else:\n        with open(output_file, \'w+\') as out_file:\n            json.dump(combined_ws, out_file, indent=4, sort_keys=True)\n        log.debug(""Written to {0:s}"".format(output_file))\n\n\n@cli.command()\n@click.argument(\'workspace\', default=\'-\')\n@click.option(\n    \'-a\',\n    \'--algorithm\',\n    default=[\'sha256\'],\n    multiple=True,\n    help=\'The hashing algorithm used to compute the workspace digest.\',\n)\n@click.option(\n    \'-j/-p\',\n    \'--json/--plaintext\',\n    \'output_json\',\n    help=\'Output the hash values as a JSON dictionary or plaintext strings\',\n)\ndef digest(workspace, algorithm, output_json):\n    """"""\n    Use hashing algorithm to calculate the workspace digest.\n\n    Returns:\n        digests (:obj:`dict`): A mapping of the hashing algorithms used to the computed digest for the workspace.\n\n    Example:\n\n    .. code-block:: shell\n\n        $ curl -sL https://raw.githubusercontent.com/scikit-hep/pyhf/master/docs/examples/json/2-bin_1-channel.json | pyhf digest\n        sha256:dad8822af55205d60152cbe4303929042dbd9d4839012e055e7c6b6459d68d73\n    """"""\n    with click.open_file(workspace, \'r\') as specstream:\n        spec = json.load(specstream)\n\n    workspace = Workspace(spec)\n\n    digests = {\n        hash_alg: utils.digest(workspace, algorithm=hash_alg) for hash_alg in algorithm\n    }\n\n    if output_json:\n        output = json.dumps(digests, indent=4, sort_keys=True,)\n    else:\n        output = \'\\n\'.join(\n            f""{hash_alg}:{digest}"" for hash_alg, digest in digests.items()\n        )\n\n    click.echo(output)\n'"
src/pyhf/contrib/__init__.py,0,"b'""""""\nContributions to pyhf.\n\nModules in contrib should never be dependencies of pyhf.\n""""""\n'"
src/pyhf/exceptions/__init__.py,0,"b'import sys\n\n\nclass InvalidMeasurement(Exception):\n    """"""\n    InvalidMeasurement is raised when a specified measurement is invalid given the specification.\n    """"""\n\n\nclass InvalidNameReuse(Exception):\n    pass\n\n\nclass InvalidSpecification(Exception):\n    """"""\n    InvalidSpecification is raised when a specification does not validate against the given schema.\n    """"""\n\n    def __init__(self, ValidationError):\n        self.exc_info = sys.exc_info()\n        self.parent = ValidationError\n        self.path = \'\'\n        for item in ValidationError.path:\n            if isinstance(item, int):\n                self.path += \'[{}]\'.format(item)\n            else:\n                self.path += \'.{}\'.format(item)\n        self.path = self.path.lstrip(\'.\')\n        self.instance = ValidationError.instance\n        message = \'{0}.\\n\\tPath: {1}\\n\\tInstance: {2}\'.format(\n            ValidationError.message, self.path, self.instance\n        )\n        # Call the base class constructor with the parameters it needs\n        super(InvalidSpecification, self).__init__(message)\n\n\nclass InvalidPatchSet(Exception):\n    """"""InvalidPatchSet is raised when a given patchset object does not have the right configuration, even though it validates correctly against the schema.""""""\n\n\nclass InvalidPatchLookup(Exception):\n    """"""InvalidPatchLookup is raised when the patch lookup from a patchset object has failed""""""\n\n\nclass PatchSetVerificationError(Exception):\n    """"""PatchSetVerificationError is raised when the workspace digest does not match the patchset digests as part of the verification procedure""""""\n\n\nclass InvalidWorkspaceOperation(Exception):\n    """"""InvalidWorkspaceOperation is raised when an operation on a workspace fails.""""""\n\n\nclass InvalidModel(Exception):\n    """"""\n    InvalidModel is raised when a given model does not have the right configuration, even though it validates correctly against the schema.\n\n    This can occur, for example, when the provided parameter of interest to fit against does not get declared in the specification provided.\n    """"""\n\n\nclass InvalidModifier(Exception):\n    """"""\n    InvalidModifier is raised when an invalid modifier is requested. This includes:\n\n        - creating a custom modifier with the wrong structure\n        - initializing a modifier that does not exist, or has not been loaded\n\n    """"""\n\n\nclass InvalidInterpCode(Exception):\n    """"""\n    InvalidInterpCode is raised when an invalid/unimplemented interpolation code is requested.\n    """"""\n\n\nclass ImportBackendError(Exception):\n    """"""\n    MissingLibraries is raised when something is imported by sustained an import error due to missing additional, non-default libraries.\n    """"""\n\n\nclass InvalidBackend(Exception):\n    """"""\n    InvalidBackend is raised when trying to set a backend that does not exist.\n    """"""\n\n\nclass InvalidOptimizer(Exception):\n    """"""\n    InvalidOptimizer is raised when trying to set an optimizer that does not exist.\n    """"""\n\n\nclass InvalidPdfParameters(Exception):\n    """"""\n    InvalidPdfParameters is raised when trying to evaluate a pdf with invalid parameters.\n    """"""\n\n\nclass InvalidPdfData(Exception):\n    """"""\n    InvalidPdfData is raised when trying to evaluate a pdf with invalid data.\n    """"""\n'"
src/pyhf/infer/__init__.py,0,"b'""""""Inference for Statistical Models.""""""\n\nfrom .test_statistics import qmu\nfrom .. import get_backend\nfrom .calculators import AsymptoticCalculator\n\n\ndef hypotest(\n    poi_test, data, pdf, init_pars=None, par_bounds=None, qtilde=False, **kwargs\n):\n    r""""""\n    Compute :math:`p`-values and test statistics for a single value of the parameter of interest.\n\n    Example:\n        >>> import pyhf\n        >>> pyhf.set_backend(""numpy"")\n        >>> model = pyhf.simplemodels.hepdata_like(\n        ...     signal_data=[12.0, 11.0], bkg_data=[50.0, 52.0], bkg_uncerts=[3.0, 7.0]\n        ... )\n        >>> observations = [51, 48]\n        >>> data = pyhf.tensorlib.astensor(observations + model.config.auxdata)\n        >>> test_poi = 1.0\n        >>> CLs_obs, CLs_exp_band = pyhf.infer.hypotest(\n        ...     test_poi, data, model, qtilde=True, return_expected_set=True\n        ... )\n        >>> print(CLs_obs)\n        [0.05251554]\n        >>> print(CLs_exp_band)\n        [[0.00260641]\n         [0.01382066]\n         [0.06445521]\n         [0.23526104]\n         [0.57304182]]\n\n    Args:\n        poi_test (Number or Tensor): The value of the parameter of interest (POI)\n        data (Number or Tensor): The root of the calculated test statistic given the Asimov data, :math:`\\sqrt{q_{\\mu,A}}`\n        pdf (~pyhf.pdf.Model): The HistFactory statistical model\n        init_pars (Array or Tensor): The initial parameter values to be used for minimization\n        par_bounds (Array or Tensor): The parameter value bounds to be used for minimization\n        qtilde (Bool): When ``True`` perform the calculation using the alternative test statistic, :math:`\\tilde{q}`, as defined in Equation (62) of :xref:`arXiv:1007.1727`\n\n    Keyword Args:\n        return_tail_probs (bool): Bool for returning :math:`\\textrm{CL}_{s+b}` and :math:`\\textrm{CL}_{b}`\n        return_expected (bool): Bool for returning :math:`\\textrm{CL}_{\\textrm{exp}}`\n        return_expected_set (bool): Bool for returning the :math:`(-2,-1,0,1,2)\\sigma` :math:`\\textrm{CL}_{\\textrm{exp}}` --- the ""Brazil band""\n\n    Returns:\n        Tuple of Floats and lists of Floats:\n\n            - :math:`\\textrm{CL}_{s}`: The :math:`p`-value compared to the given threshold :math:`\\alpha`, typically taken to be :math:`0.05`, defined in :xref:`arXiv:1007.1727` as\n\n            .. math::\n\n                \\textrm{CL}_{s} = \\frac{\\textrm{CL}_{s+b}}{\\textrm{CL}_{b}} = \\frac{p_{s+b}}{1-p_{b}}\n\n            to protect against excluding signal models in which there is little sensitivity. In the case that :math:`\\textrm{CL}_{s} \\leq \\alpha` the given signal model is excluded.\n\n            - :math:`\\left[\\textrm{CL}_{s+b}, \\textrm{CL}_{b}\\right]`: The signal + background :math:`p`-value and 1 minus the background only :math:`p`-value as defined in Equations (75) and (76) of :xref:`arXiv:1007.1727`\n\n            .. math::\n\n                \\textrm{CL}_{s+b} = p_{s+b} = \\int\\limits_{q_{\\textrm{obs}}}^{\\infty} f\\left(q\\,\\middle|s+b\\right)\\,dq = 1 - \\Phi\\left(\\frac{q_{\\textrm{obs}} + 1/\\sigma_{s+b}^{2}}{2/\\sigma_{s+b}}\\right)\n\n            .. math::\n\n                \\textrm{CL}_{b} = 1- p_{b} = 1 - \\int\\limits_{-\\infty}^{q_{\\textrm{obs}}} f\\left(q\\,\\middle|b\\right)\\,dq = 1 - \\Phi\\left(\\frac{q_{\\textrm{obs}} - 1/\\sigma_{b}^{2}}{2/\\sigma_{b}}\\right)\n\n            with Equations (73) and (74) for the mean\n\n            .. math::\n\n                E\\left[q\\right] = \\frac{1 - 2\\mu}{\\sigma^{2}}\n\n            and variance\n\n            .. math::\n\n                V\\left[q\\right] = \\frac{4}{\\sigma^{2}}\n\n            of the test statistic :math:`q` under the background only and and signal + background hypotheses. Only returned when ``return_tail_probs`` is ``True``.\n\n            - :math:`\\textrm{CL}_{s,\\textrm{exp}}`: The expected :math:`\\textrm{CL}_{s}` value corresponding to the test statistic under the background only hypothesis :math:`\\left(\\mu=0\\right)`. Only returned when ``return_expected`` is ``True``.\n\n            - :math:`\\textrm{CL}_{s,\\textrm{exp}}` band: The set of expected :math:`\\textrm{CL}_{s}` values corresponding to the median significance of variations of the signal strength from the background only hypothesis :math:`\\left(\\mu=0\\right)` at :math:`(-2,-1,0,1,2)\\sigma`. That is, the :math:`p`-values that satisfy Equation (89) of :xref:`arXiv:1007.1727`\n\n            .. math::\n\n                \\textrm{band}_{N\\sigma} = \\mu\' + \\sigma\\,\\Phi^{-1}\\left(1-\\alpha\\right) \\pm N\\sigma\n\n            for :math:`\\mu\'=0` and :math:`N \\in \\left\\{-2, -1, 0, 1, 2\\right\\}`. These values define the boundaries of an uncertainty band sometimes referred to as the ""Brazil band"". Only returned when ``return_expected_set`` is ``True``.\n\n    """"""\n    init_pars = init_pars or pdf.config.suggested_init()\n    par_bounds = par_bounds or pdf.config.suggested_bounds()\n    tensorlib, _ = get_backend()\n\n    calc = AsymptoticCalculator(data, pdf, init_pars, par_bounds, qtilde=qtilde)\n    teststat = calc.teststatistic(poi_test)\n    sig_plus_bkg_distribution, b_only_distribution = calc.distributions(poi_test)\n\n    CLsb = sig_plus_bkg_distribution.pvalue(teststat)\n    CLb = b_only_distribution.pvalue(teststat)\n    CLs = CLsb / CLb\n    CLsb, CLb, CLs = (\n        tensorlib.reshape(CLsb, (1,)),\n        tensorlib.reshape(CLb, (1,)),\n        tensorlib.reshape(CLs, (1,)),\n    )\n\n    _returns = [CLs]\n    if kwargs.get(\'return_tail_probs\'):\n        _returns.append([CLsb, CLb])\n    if kwargs.get(\'return_expected_set\'):\n        CLs_exp = []\n        for n_sigma in [2, 1, 0, -1, -2]:\n            CLs = sig_plus_bkg_distribution.pvalue(\n                n_sigma\n            ) / b_only_distribution.pvalue(n_sigma)\n            CLs_exp.append(tensorlib.reshape(CLs, (1,)))\n        CLs_exp = tensorlib.astensor(CLs_exp)\n        if kwargs.get(\'return_expected\'):\n            _returns.append(CLs_exp[2])\n        _returns.append(CLs_exp)\n    elif kwargs.get(\'return_expected\'):\n        n_sigma = 0\n        CLs = sig_plus_bkg_distribution.pvalue(n_sigma) / b_only_distribution.pvalue(\n            n_sigma\n        )\n        _returns.append(tensorlib.reshape(CLs, (1,)))\n    # Enforce a consistent return type of the observed CLs\n    return tuple(_returns) if len(_returns) > 1 else _returns[0]\n\n\n__all__ = [\'qmu\', \'hypotest\']\n'"
src/pyhf/infer/calculators.py,0,"b'""""""\nCalculators for Hypothesis Testing.\n\nThe role of the calculators is to compute test statistic and\nprovide distributions of said test statistic under various\nhypotheses.\n\nUsing the calculators hypothesis tests can then be performed.\n""""""\nfrom .mle import fixed_poi_fit\nfrom .. import get_backend\nfrom .test_statistics import qmu\n\n\ndef generate_asimov_data(asimov_mu, data, pdf, init_pars, par_bounds):\n    """"""\n    Compute Asimov Dataset (expected yields at best-fit values) for a given POI value.\n\n    Args:\n        asimov_mu (`float`): The value for the parameter of interest to be used.\n        data (`tensor`): The observed data.\n        pdf (~pyhf.pdf.Model): The statistical model adhering to the schema ``model.json``.\n        init_pars (`tensor`): The initial parameter values to be used for fitting.\n        par_bounds (`tensor`): The parameter value bounds to be used for fitting.\n\n    Returns:\n        Tensor: The Asimov dataset.\n\n    """"""\n    bestfit_nuisance_asimov = fixed_poi_fit(asimov_mu, data, pdf, init_pars, par_bounds)\n    return pdf.expected_data(bestfit_nuisance_asimov)\n\n\nclass AsymptoticTestStatDistribution(object):\n    """"""\n    The distribution the test statistic in the asymptotic case.\n\n    Note: These distributions are in :math:`-\\hat{\\mu}/\\sigma` space.\n    In the ROOT implementation the same sigma is assumed for both hypotheses\n    and :math:`p`-values etc are computed in that space.\n    This assumption is necessarily valid, but we keep this for compatibility reasons.\n\n    In the :math:`-\\hat{\\mu}/\\sigma` space, the test statistic (i.e. :math:`\\hat{\\mu}/\\sigma`) is\n    normally distributed with unit variance and its mean at\n    the :math:`-\\mu\'`, where :math:`\\mu\'` is the true poi value of the hypothesis.\n    """"""\n\n    def __init__(self, shift):\n        """"""\n        Asymptotic test statistic distribution.\n\n        Args:\n            shift (`float`): The displacement of the test statistic distribution.\n\n        Returns:\n            ~pyhf.infer.calculators.AsymptoticTestStatDistribution: The asymptotic distribution of test statistic.\n\n        """"""\n        self.shift = shift\n        self.sqrtqmuA_v = None\n\n    def pvalue(self, value):\n        """"""\n        Compute the :math:`p`-value for a given value of the test statistic.\n\n        Args:\n            value (`float`): The test statistic value.\n\n        Returns:\n            Float: The integrated probability to observe a value at least as large as the observed one.\n\n        """"""\n        tensorlib, _ = get_backend()\n        return 1 - tensorlib.normal_cdf(value - self.shift)\n\n    def expected_value(self, nsigma):\n        """"""\n        Return the expected value of the test statistic.\n\n        Args:\n            nsigma (`int` or `tensor`): The number of standard deviations.\n\n        Returns:\n            Float: The expected value of the test statistic.\n        """"""\n        return nsigma\n\n\nclass AsymptoticCalculator(object):\n    """"""The Asymptotic Calculator.""""""\n\n    def __init__(self, data, pdf, init_pars=None, par_bounds=None, qtilde=False):\n        """"""\n        Asymptotic Calculator.\n\n        Args:\n            data (`tensor`): The observed data.\n            pdf (~pyhf.pdf.Model): The statistical model adhering to the schema ``model.json``.\n            init_pars (`tensor`): The initial parameter values to be used for fitting.\n            par_bounds (`tensor`): The parameter value bounds to be used for fitting.\n            qtilde (`bool`): Whether to use qtilde as the test statistic.\n\n        Returns:\n            ~pyhf.infer.calculators.AsymptoticCalculator: The calculator for asymptotic quantities.\n\n        """"""\n        self.data = data\n        self.pdf = pdf\n        self.init_pars = init_pars or pdf.config.suggested_init()\n        self.par_bounds = par_bounds or pdf.config.suggested_bounds()\n        self.qtilde = qtilde\n        self.sqrtqmuA_v = None\n\n    def distributions(self, poi_test):\n        """"""\n        Probability Distributions of the test statistic value under the signal + background and and background-only hypothesis.\n\n        Args:\n            poi_test: The value for the parameter of interest.\n\n        Returns:\n            Tuple (~pyhf.infer.calculators.AsymptoticTestStatDistribution): The distributions under the hypotheses.\n\n        """"""\n        if self.sqrtqmuA_v is None:\n            raise RuntimeError(\'need to call .teststatistic(poi_test) first\')\n        sb_dist = AsymptoticTestStatDistribution(-self.sqrtqmuA_v)\n        b_dist = AsymptoticTestStatDistribution(0.0)\n        return sb_dist, b_dist\n\n    def teststatistic(self, poi_test):\n        """"""\n        Compute the test statistic for the observed data under the studied model.\n\n        Args:\n            poi_test: The value for the parameter of interest.\n\n        Returns:\n            Float: the value of the test statistic.\n\n        """"""\n        tensorlib, _ = get_backend()\n        qmu_v = qmu(poi_test, self.data, self.pdf, self.init_pars, self.par_bounds)\n        sqrtqmu_v = tensorlib.sqrt(qmu_v)\n\n        asimov_mu = 0.0\n        asimov_data = generate_asimov_data(\n            asimov_mu, self.data, self.pdf, self.init_pars, self.par_bounds\n        )\n        qmuA_v = qmu(poi_test, asimov_data, self.pdf, self.init_pars, self.par_bounds)\n        self.sqrtqmuA_v = tensorlib.sqrt(qmuA_v)\n\n        if not self.qtilde:  # qmu\n            teststat = sqrtqmu_v - self.sqrtqmuA_v\n        else:  # qtilde\n\n            def _true_case():\n                teststat = sqrtqmu_v - self.sqrtqmuA_v\n                return teststat\n\n            def _false_case():\n                qmu = tensorlib.power(sqrtqmu_v, 2)\n                qmu_A = tensorlib.power(self.sqrtqmuA_v, 2)\n                teststat = (qmu - qmu_A) / (2 * self.sqrtqmuA_v)\n                return teststat\n\n            teststat = tensorlib.conditional(\n                (sqrtqmu_v < self.sqrtqmuA_v), _true_case, _false_case\n            )\n        return teststat\n'"
src/pyhf/infer/mle.py,0,"b'""""""Module for Maximum Likelihood Estimation.""""""\nfrom .. import get_backend\n\n\ndef twice_nll(pars, data, pdf):\n    """"""\n    Twice the negative Log-Likelihood.\n\n    Args:\n        data (`tensor`): The data\n        pdf (~pyhf.pdf.Model): The statistical model adhering to the schema model.json\n\n    Returns:\n        Twice the negative log likelihood.\n\n    """"""\n    return -2 * pdf.logpdf(pars, data)\n\n\ndef fit(data, pdf, init_pars=None, par_bounds=None, **kwargs):\n    """"""\n    Run a unconstrained maximum likelihood fit.\n\n    Example:\n        >>> import pyhf\n        >>> pyhf.set_backend(""numpy"")\n        >>> model = pyhf.simplemodels.hepdata_like(\n        ...     signal_data=[12.0, 11.0], bkg_data=[50.0, 52.0], bkg_uncerts=[3.0, 7.0]\n        ... )\n        >>> observations = [51, 48]\n        >>> data = pyhf.tensorlib.astensor(observations + model.config.auxdata)\n        >>> pyhf.infer.mle.fit(data, model, return_fitted_val=True)\n        (array([0.        , 1.0030512 , 0.96266961]), 24.98393521454011)\n\n    Args:\n        data (`tensor`): The data\n        pdf (~pyhf.pdf.Model): The statistical model adhering to the schema model.json\n        init_pars (`list`): Values to initialize the model parameters at for the fit\n        par_bounds (`list` of `list`\\s or `tuple`\\s): The extrema of values the model parameters are allowed to reach in the fit\n        kwargs: Keyword arguments passed through to the optimizer API\n\n    Returns:\n        See optimizer API\n\n    """"""\n    _, opt = get_backend()\n    init_pars = init_pars or pdf.config.suggested_init()\n    par_bounds = par_bounds or pdf.config.suggested_bounds()\n    return opt.minimize(twice_nll, data, pdf, init_pars, par_bounds, **kwargs)\n\n\ndef fixed_poi_fit(poi_val, data, pdf, init_pars=None, par_bounds=None, **kwargs):\n    """"""\n    Run a maximum likelihood fit with the POI value fixed.\n\n    Example:\n        >>> import pyhf\n        >>> pyhf.set_backend(""numpy"")\n        >>> model = pyhf.simplemodels.hepdata_like(\n        ...     signal_data=[12.0, 11.0], bkg_data=[50.0, 52.0], bkg_uncerts=[3.0, 7.0]\n        ... )\n        >>> observations = [51, 48]\n        >>> data = pyhf.tensorlib.astensor(observations + model.config.auxdata)\n        >>> test_poi = 1.0\n        >>> pyhf.infer.mle.fixed_poi_fit(test_poi, data, model, return_fitted_val=True)\n        (array([1.        , 0.97224597, 0.87553894]), 28.92218013492061)\n\n    Args:\n        data: The data\n        pdf (~pyhf.pdf.Model): The statistical model adhering to the schema model.json\n        init_pars (`list`): Values to initialize the model parameters at for the fit\n        par_bounds (`list` of `list`\\s or `tuple`\\s): The extrema of values the model parameters are allowed to reach in the fit\n        kwargs: Keyword arguments passed through to the optimizer API\n\n    Returns:\n        See optimizer API\n\n    """"""\n    _, opt = get_backend()\n    init_pars = init_pars or pdf.config.suggested_init()\n    par_bounds = par_bounds or pdf.config.suggested_bounds()\n    return opt.minimize(\n        twice_nll,\n        data,\n        pdf,\n        init_pars,\n        par_bounds,\n        [(pdf.config.poi_index, poi_val)],\n        **kwargs,\n    )\n'"
src/pyhf/infer/test_statistics.py,0,"b'from .. import get_backend\nfrom .mle import fixed_poi_fit, fit\n\n\ndef qmu(mu, data, pdf, init_pars, par_bounds):\n    r""""""\n    The test statistic, :math:`q_{\\mu}`, for establishing an upper\n    limit on the strength parameter, :math:`\\mu`, as defiend in\n    Equation (14) in :xref:`arXiv:1007.1727`.\n\n    .. math::\n       :nowrap:\n\n       \\begin{equation}\n          q_{\\mu} = \\left\\{\\begin{array}{ll}\n          -2\\ln\\lambda\\left(\\mu\\right), &\\hat{\\mu} < \\mu,\\\\\n          0, & \\hat{\\mu} > \\mu\n          \\end{array}\\right.\n        \\end{equation}\n\n    Example:\n        >>> import pyhf\n        >>> pyhf.set_backend(""numpy"")\n        >>> model = pyhf.simplemodels.hepdata_like(\n        ...     signal_data=[12.0, 11.0], bkg_data=[50.0, 52.0], bkg_uncerts=[3.0, 7.0]\n        ... )\n        >>> observations = [51, 48]\n        >>> data = pyhf.tensorlib.astensor(observations + model.config.auxdata)\n        >>> test_mu = 1.0\n        >>> init_pars = model.config.suggested_init()\n        >>> par_bounds = model.config.suggested_bounds()\n        >>> pyhf.infer.test_statistics.qmu(test_mu, data, model, init_pars, par_bounds)\n        3.938244920380498\n\n    Args:\n        mu (Number or Tensor): The signal strength parameter\n        data (Tensor): The data to be considered\n        pdf (~pyhf.pdf.Model): The HistFactory statistical model used in the likelihood ratio calculation\n        init_pars (`list`): Values to initialize the model parameters at for the fit\n        par_bounds (`list` of `list`\\s or `tuple`\\s): The extrema of values the model parameters are allowed to reach in the fit\n\n    Returns:\n        Float: The calculated test statistic, :math:`q_{\\mu}`\n    """"""\n    tensorlib, optimizer = get_backend()\n    mubhathat, fixed_poi_fit_lhood_val = fixed_poi_fit(\n        mu, data, pdf, init_pars, par_bounds, return_fitted_val=True\n    )\n    muhatbhat, unconstrained_fit_lhood_val = fit(\n        data, pdf, init_pars, par_bounds, return_fitted_val=True\n    )\n    qmu = fixed_poi_fit_lhood_val - unconstrained_fit_lhood_val\n    qmu = tensorlib.where(\n        muhatbhat[pdf.config.poi_index] > mu, tensorlib.astensor(0.0), qmu\n    )[0]\n    return tensorlib.clip(qmu, 0, max_value=None)\n'"
src/pyhf/interpolators/__init__.py,0,"b'""""""Histogram Interpolation.""""""\n\n\ndef _slow_interpolator_looper(histogramssets, alphasets, func):\n    all_results = []\n    for histoset, alphaset in zip(histogramssets, alphasets):\n        all_results.append([])\n        set_result = all_results[-1]\n        for histo in histoset:\n            set_result.append([])\n            histo_result = set_result[-1]\n            for alpha in alphaset:\n                alpha_result = []\n                for down, nom, up in zip(histo[0], histo[1], histo[2]):\n                    v = func(down, nom, up, alpha)\n                    alpha_result.append(v)\n                histo_result.append(alpha_result)\n    return all_results\n\n\n# interpolation codes come from https://cds.cern.ch/record/1456844/files/CERN-OPEN-2012-016.pdf\nfrom .code0 import code0, _slow_code0\nfrom .code1 import code1, _slow_code1\nfrom .code2 import code2, _slow_code2\nfrom .code4 import code4, _slow_code4\nfrom .code4p import code4p, _slow_code4p\nfrom .. import exceptions\n\n\ndef get(interpcode, do_tensorized_calc=True):\n    interpcodes = {\n        0: code0 if do_tensorized_calc else _slow_code0,\n        1: code1 if do_tensorized_calc else _slow_code1,\n        2: code2 if do_tensorized_calc else _slow_code2,\n        4: code4 if do_tensorized_calc else _slow_code4,\n        \'4p\': code4p if do_tensorized_calc else _slow_code4p,\n    }\n\n    try:\n        return interpcodes[interpcode]\n    except KeyError:\n        raise exceptions.InvalidInterpCode\n\n\n__all__ = [\'code0\', \'code1\', \'code2\', \'code4\', \'code4p\']\n'"
src/pyhf/interpolators/code0.py,0,"b'""""""Piecewise-linear Interpolation. (Code 0).""""""\nimport logging\nfrom .. import get_backend, default_backend\nfrom .. import events\nfrom . import _slow_interpolator_looper\n\nlog = logging.getLogger(__name__)\n\n\nclass code0(object):\n    r""""""\n    The piecewise-linear interpolation strategy.\n\n    .. math::\n        \\sigma_{sb} (\\vec{\\alpha}) = \\sigma_{sb}^0(\\vec{\\alpha}) + \\underbrace{\\sum_{p \\in \\text{Syst}} I_\\text{lin.} (\\alpha_p; \\sigma_{sb}^0, \\sigma_{psb}^+, \\sigma_{psb}^-)}_\\text{deltas to calculate}\n\n\n    with\n\n    .. math::\n        I_\\text{lin.}(\\alpha; I^0, I^+, I^-) = \\begin{cases} \\alpha(I^+ - I^0) \\qquad \\alpha \\geq 0\\\\ \\alpha(I^0 - I^-) \\qquad \\alpha < 0 \\end{cases}\n\n\n    """"""\n\n    def __init__(self, histogramssets, subscribe=True):\n        """"""Piecewise-linear Interpolation.""""""\n        # nb: this should never be a tensor, store in default backend (e.g. numpy)\n        self._histogramssets = default_backend.astensor(histogramssets)\n        # initial shape will be (nsysts, 1)\n        self.alphasets_shape = (self._histogramssets.shape[0], 1)\n        # precompute terms that only depend on the histogramssets\n        self._deltas_up = self._histogramssets[:, :, 2] - self._histogramssets[:, :, 1]\n        self._deltas_dn = self._histogramssets[:, :, 1] - self._histogramssets[:, :, 0]\n        self._broadcast_helper = default_backend.ones(\n            default_backend.shape(self._deltas_up)\n        )\n        self._precompute()\n        if subscribe:\n            events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        self.deltas_up = tensorlib.astensor(self._deltas_up)\n        self.deltas_dn = tensorlib.astensor(self._deltas_dn)\n        self.broadcast_helper = tensorlib.astensor(self._broadcast_helper)\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n\n    def _precompute_alphasets(self, alphasets_shape):\n        if alphasets_shape == self.alphasets_shape:\n            return\n        tensorlib, _ = get_backend()\n        self.alphasets_shape = alphasets_shape\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n\n    def __call__(self, alphasets):\n        """"""Compute Interpolated Values.""""""\n        tensorlib, _ = get_backend()\n        self._precompute_alphasets(tensorlib.shape(alphasets))\n        where_alphasets_positive = tensorlib.where(\n            alphasets > 0, self.mask_on, self.mask_off\n        )\n\n        # s: set under consideration (i.e. the modifier)\n        # a: alpha variation\n        # h: histogram affected by modifier\n        # b: bin of histogram\n        alphas_times_deltas_up = tensorlib.einsum(\n            \'sa,shb->shab\', alphasets, self.deltas_up\n        )\n        alphas_times_deltas_dn = tensorlib.einsum(\n            \'sa,shb->shab\', alphasets, self.deltas_dn\n        )\n\n        masks = tensorlib.astensor(\n            tensorlib.einsum(\n                \'sa,shb->shab\', where_alphasets_positive, self.broadcast_helper\n            ),\n            dtype=""bool"",\n        )\n\n        return tensorlib.where(masks, alphas_times_deltas_up, alphas_times_deltas_dn)\n\n\nclass _slow_code0(object):\n    def summand(self, down, nom, up, alpha):\n        delta_up = up - nom\n        delta_down = nom - down\n        if alpha > 0:\n            delta = delta_up * alpha\n        else:\n            delta = delta_down * alpha\n        return delta\n\n    def __init__(self, histogramssets, subscribe=True):\n        self._histogramssets = histogramssets\n\n    def __call__(self, alphasets):\n        tensorlib, _ = get_backend()\n        return tensorlib.astensor(\n            _slow_interpolator_looper(\n                self._histogramssets, tensorlib.tolist(alphasets), self.summand\n            )\n        )\n'"
src/pyhf/interpolators/code1.py,0,"b'""""""Piecewise-Exponential Interpolation (Code 1).""""""\nimport logging\nimport math\nfrom .. import get_backend, default_backend\nfrom .. import events\nfrom . import _slow_interpolator_looper\n\nlog = logging.getLogger(__name__)\n\n\nclass code1(object):\n    r""""""\n    The piecewise-exponential interpolation strategy.\n\n    .. math::\n        \\sigma_{sb} (\\vec{\\alpha}) = \\sigma_{sb}^0(\\vec{\\alpha}) \\underbrace{\\prod_{p \\in \\text{Syst}} I_\\text{exp.} (\\alpha_p; \\sigma_{sb}^0, \\sigma_{psb}^+, \\sigma_{psb}^-)}_\\text{factors to calculate}\n\n\n    with\n\n    .. math::\n        I_\\text{exp.}(\\alpha; I^0, I^+, I^-) = \\begin{cases} \\left(\\frac{I^+}{I^0}\\right)^{\\alpha} \\qquad \\alpha \\geq 0\\\\ \\left(\\frac{I^-}{I^0}\\right)^{-\\alpha} \\qquad \\alpha < 0 \\end{cases}\n\n\n    """"""\n\n    def __init__(self, histogramssets, subscribe=True):\n        """"""Piecewise-Exponential Interpolation.""""""\n        # nb: this should never be a tensor, store in default backend (e.g. numpy)\n        self._histogramssets = default_backend.astensor(histogramssets)\n        # initial shape will be (nsysts, 1)\n        self.alphasets_shape = (self._histogramssets.shape[0], 1)\n        # precompute terms that only depend on the histogramssets\n        self._deltas_up = default_backend.divide(\n            self._histogramssets[:, :, 2], self._histogramssets[:, :, 1]\n        )\n        self._deltas_dn = default_backend.divide(\n            self._histogramssets[:, :, 0], self._histogramssets[:, :, 1]\n        )\n        self._broadcast_helper = default_backend.ones(\n            default_backend.shape(self._deltas_up)\n        )\n\n        self._precompute()\n        if subscribe:\n            events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        self.deltas_up = tensorlib.astensor(self._deltas_up)\n        self.deltas_dn = tensorlib.astensor(self._deltas_dn)\n        self.broadcast_helper = tensorlib.astensor(self._broadcast_helper)\n        self.bases_up = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.ones(self.alphasets_shape), self.deltas_up\n        )\n        self.bases_dn = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.ones(self.alphasets_shape), self.deltas_dn\n        )\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n\n    def _precompute_alphasets(self, alphasets_shape):\n        if alphasets_shape == self.alphasets_shape:\n            return\n        tensorlib, _ = get_backend()\n        self.alphasets_shape = alphasets_shape\n        self.bases_up = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.ones(self.alphasets_shape), self.deltas_up\n        )\n        self.bases_dn = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.ones(self.alphasets_shape), self.deltas_dn\n        )\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n        return\n\n    def __call__(self, alphasets):\n        """"""Compute Interpolated Values.""""""\n        tensorlib, _ = get_backend()\n        self._precompute_alphasets(tensorlib.shape(alphasets))\n        where_alphasets_positive = tensorlib.where(\n            alphasets > 0, self.mask_on, self.mask_off\n        )\n\n        # s: set under consideration (i.e. the modifier)\n        # a: alpha variation\n        # h: histogram affected by modifier\n        # b: bin of histogram\n        exponents = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.abs(alphasets), self.broadcast_helper\n        )\n        masks = tensorlib.astensor(\n            tensorlib.einsum(\n                \'sa,shb->shab\', where_alphasets_positive, self.broadcast_helper\n            ),\n            dtype=""bool"",\n        )\n\n        bases = tensorlib.where(masks, self.bases_up, self.bases_dn)\n        return tensorlib.power(bases, exponents)\n\n\nclass _slow_code1(object):\n    def product(self, down, nom, up, alpha):\n        delta_up = up / nom\n        delta_down = down / nom\n        if alpha > 0:\n            delta = math.pow(delta_up, alpha)\n        else:\n            delta = math.pow(delta_down, (-alpha))\n        return delta\n\n    def __init__(self, histogramssets, subscribe=True):\n        self._histogramssets = histogramssets\n\n    def __call__(self, alphasets):\n        tensorlib, _ = get_backend()\n        return tensorlib.astensor(\n            _slow_interpolator_looper(\n                self._histogramssets, tensorlib.tolist(alphasets), self.product\n            )\n        )\n'"
src/pyhf/interpolators/code2.py,0,"b'""""""Quadratic Interpolation (Code 2).""""""\nimport logging\nfrom .. import get_backend, default_backend\nfrom .. import events\nfrom . import _slow_interpolator_looper\n\nlog = logging.getLogger(__name__)\n\n\nclass code2(object):\n    r""""""\n    The quadratic interpolation and linear extrapolation strategy.\n\n    .. math::\n        \\sigma_{sb} (\\vec{\\alpha}) = \\sigma_{sb}^0(\\vec{\\alpha}) + \\underbrace{\\sum_{p \\in \\text{Syst}} I_\\text{quad.|lin.} (\\alpha_p; \\sigma_{sb}^0, \\sigma_{psb}^+, \\sigma_{psb}^-)}_\\text{deltas to calculate}\n\n\n    with\n\n    .. math::\n        I_\\text{quad.|lin.}(\\alpha; I^0, I^+, I^-) = \\begin{cases} (b + 2a)(\\alpha - 1) \\qquad \\alpha \\geq 1\\\\  a\\alpha^2 + b\\alpha \\qquad |\\alpha| < 1 \\\\ (b - 2a)(\\alpha + 1) \\qquad \\alpha < -1 \\end{cases}\n\n    and\n\n    .. math::\n        a = \\frac{1}{2} (I^+ + I^-) - I^0 \\qquad \\mathrm{and} \\qquad b = \\frac{1}{2}(I^+ - I^-)\n\n    """"""\n\n    def __init__(self, histogramssets, subscribe=True):\n        """"""Quadratic Interpolation.""""""\n        # nb: this should never be a tensor, store in default backend (e.g. numpy)\n        self._histogramssets = default_backend.astensor(histogramssets)\n        # initial shape will be (nsysts, 1)\n        self.alphasets_shape = (self._histogramssets.shape[0], 1)\n        # precompute terms that only depend on the histogramssets\n        self._a = (\n            0.5 * (self._histogramssets[:, :, 2] + self._histogramssets[:, :, 0])\n            - self._histogramssets[:, :, 1]\n        )\n        self._b = 0.5 * (self._histogramssets[:, :, 2] - self._histogramssets[:, :, 0])\n        self._b_plus_2a = self._b + 2 * self._a\n        self._b_minus_2a = self._b - 2 * self._a\n        self._broadcast_helper = default_backend.ones(default_backend.shape(self._a))\n        self._precompute()\n        if subscribe:\n            events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        self.a = tensorlib.astensor(self._a)\n        self.b = tensorlib.astensor(self._b)\n        self.b_plus_2a = tensorlib.astensor(self._b_plus_2a)\n        self.b_minus_2a = tensorlib.astensor(self._b_minus_2a)\n        # make up the masks correctly\n        self.broadcast_helper = tensorlib.astensor(self._broadcast_helper)\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n\n    def _precompute_alphasets(self, alphasets_shape):\n        if alphasets_shape == self.alphasets_shape:\n            return\n        tensorlib, _ = get_backend()\n        self.alphasets_shape = alphasets_shape\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n\n    def __call__(self, alphasets):\n        """"""Compute Interpolated Values.""""""\n        tensorlib, _ = get_backend()\n        self._precompute_alphasets(tensorlib.shape(alphasets))\n\n        # select where alpha > 1\n        where_alphasets_gt1 = tensorlib.where(\n            alphasets > 1, self.mask_on, self.mask_off\n        )\n\n        # select where alpha >= -1\n        where_alphasets_not_lt1 = tensorlib.where(\n            alphasets >= -1, self.mask_on, self.mask_off\n        )\n\n        # s: set under consideration (i.e. the modifier)\n        # a: alpha variation\n        # h: histogram affected by modifier\n        # b: bin of histogram\n        value_gt1 = tensorlib.einsum(\n            \'sa,shb->shab\', alphasets - self.mask_on, self.b_plus_2a\n        )\n        value_btwn = tensorlib.einsum(\n            \'sa,sa,shb->shab\', alphasets, alphasets, self.a\n        ) + tensorlib.einsum(\'sa,shb->shab\', alphasets, self.b)\n        value_lt1 = tensorlib.einsum(\n            \'sa,shb->shab\', alphasets + self.mask_off, self.b_minus_2a\n        )\n\n        masks_gt1 = tensorlib.astensor(\n            tensorlib.einsum(\n                \'sa,shb->shab\', where_alphasets_gt1, self.broadcast_helper\n            ),\n            dtype=""bool"",\n        )\n        masks_not_lt1 = tensorlib.astensor(\n            tensorlib.einsum(\n                \'sa,shb->shab\', where_alphasets_not_lt1, self.broadcast_helper\n            ),\n            dtype=""bool"",\n        )\n\n        # first, build a result where:\n        #       alpha > 1   : fill with (b+2a)(alpha - 1)\n        #   not(alpha > 1)  : fill with (a * alpha^2 + b * alpha)\n        results_gt1_btwn = tensorlib.where(masks_gt1, value_gt1, value_btwn)\n        # then, build a result where:\n        #      alpha >= -1  : do nothing (fill with previous result)\n        #   not(alpha >= -1): fill with (b-2a)(alpha + 1)\n        return tensorlib.where(masks_not_lt1, results_gt1_btwn, value_lt1)\n\n\nclass _slow_code2(object):\n    def summand(self, down, nom, up, alpha):\n        a = 0.5 * (up + down) - nom\n        b = 0.5 * (up - down)\n        if alpha > 1:\n            delta = (b + 2 * a) * (alpha - 1)\n        elif -1 <= alpha <= 1:\n            delta = a * alpha * alpha + b * alpha\n        else:\n            delta = (b - 2 * a) * (alpha + 1)\n        return delta\n\n    def __init__(self, histogramssets, subscribe=True):\n        self._histogramssets = histogramssets\n\n    def __call__(self, alphasets):\n        tensorlib, _ = get_backend()\n        return tensorlib.astensor(\n            _slow_interpolator_looper(\n                self._histogramssets, tensorlib.tolist(alphasets), self.summand\n            )\n        )\n'"
src/pyhf/interpolators/code4.py,0,"b'""""""Polynomial Interpolation (Code 4).""""""\nimport logging\nimport math\nfrom .. import get_backend, default_backend\nfrom .. import events\nfrom . import _slow_interpolator_looper\n\nlog = logging.getLogger(__name__)\n\n\nclass code4(object):\n    r""""""\n    The polynomial interpolation and exponential extrapolation strategy.\n\n    .. math::\n        \\sigma_{sb} (\\vec{\\alpha}) = \\sigma_{sb}^0(\\vec{\\alpha}) \\underbrace{\\prod_{p \\in \\text{Syst}} I_\\text{poly|exp.} (\\alpha_p; \\sigma_{sb}^0, \\sigma_{psb}^+, \\sigma_{psb}^-, \\alpha_0)}_\\text{factors to calculate}\n\n\n    with\n\n    .. math::\n        I_\\text{poly|exp.}(\\alpha; I^0, I^+, I^-, \\alpha_0) = \\begin{cases} \\left(\\frac{I^+}{I^0}\\right)^{\\alpha} \\qquad \\alpha \\geq \\alpha_0\\\\ 1 + \\sum_{i=1}^6 a_i \\alpha^i \\qquad |\\alpha| < \\alpha_0 \\\\ \\left(\\frac{I^-}{I^0}\\right)^{-\\alpha} \\qquad \\alpha < -\\alpha_0 \\end{cases}\n\n    and the :math:`a_i` are fixed by the boundary conditions\n\n    .. math::\n        \\sigma_{sb}(\\alpha=\\pm\\alpha_0), \\left.\\frac{\\mathrm{d}\\sigma_{sb}}{\\mathrm{d}\\alpha}\\right|_{\\alpha=\\pm\\alpha_0}, \\mathrm{ and } \\left.\\frac{\\mathrm{d}^2\\sigma_{sb}}{\\mathrm{d}\\alpha^2}\\right|_{\\alpha=\\pm\\alpha_0}.\n\n    Namely that :math:`\\sigma_{sb}(\\vec{\\alpha})` is continuous, and its first- and second-order derivatives are continuous as well.\n\n    """"""\n\n    def __init__(self, histogramssets, subscribe=True, alpha0=1):\n        """"""Polynomial Interpolation.""""""\n        # alpha0 is assumed to be positive and non-zero. If alpha0 == 0, then\n        # we cannot calculate the coefficients (e.g. determinant == 0)\n        assert alpha0 > 0\n        self.__alpha0 = alpha0\n        # nb: this should never be a tensor, store in default backend (e.g. numpy)\n        self._histogramssets = default_backend.astensor(histogramssets)\n        # initial shape will be (nsysts, 1)\n        self.alphasets_shape = (self._histogramssets.shape[0], 1)\n        # precompute terms that only depend on the histogramssets\n        self._deltas_up = default_backend.divide(\n            self._histogramssets[:, :, 2], self._histogramssets[:, :, 1]\n        )\n        self._deltas_dn = default_backend.divide(\n            self._histogramssets[:, :, 0], self._histogramssets[:, :, 1]\n        )\n        self._broadcast_helper = default_backend.ones(\n            default_backend.shape(self._deltas_up)\n        )\n        self._alpha0 = self._broadcast_helper * self.__alpha0\n\n        deltas_up_alpha0 = default_backend.power(self._deltas_up, self._alpha0)\n        deltas_dn_alpha0 = default_backend.power(self._deltas_dn, self._alpha0)\n        # x = A^{-1} b\n        A_inverse = default_backend.astensor(\n            [\n                [\n                    15.0 / (16 * alpha0),\n                    -15.0 / (16 * alpha0),\n                    -7.0 / 16.0,\n                    -7.0 / 16.0,\n                    1.0 / 16 * alpha0,\n                    -1.0 / 16.0 * alpha0,\n                ],\n                [\n                    3.0 / (2 * math.pow(alpha0, 2)),\n                    3.0 / (2 * math.pow(alpha0, 2)),\n                    -9.0 / (16 * alpha0),\n                    9.0 / (16 * alpha0),\n                    1.0 / 16,\n                    1.0 / 16,\n                ],\n                [\n                    -5.0 / (8 * math.pow(alpha0, 3)),\n                    5.0 / (8 * math.pow(alpha0, 3)),\n                    5.0 / (8 * math.pow(alpha0, 2)),\n                    5.0 / (8 * math.pow(alpha0, 2)),\n                    -1.0 / (8 * alpha0),\n                    1.0 / (8 * alpha0),\n                ],\n                [\n                    3.0 / (-2 * math.pow(alpha0, 4)),\n                    3.0 / (-2 * math.pow(alpha0, 4)),\n                    -7.0 / (-8 * math.pow(alpha0, 3)),\n                    7.0 / (-8 * math.pow(alpha0, 3)),\n                    -1.0 / (8 * math.pow(alpha0, 2)),\n                    -1.0 / (8 * math.pow(alpha0, 2)),\n                ],\n                [\n                    3.0 / (16 * math.pow(alpha0, 5)),\n                    -3.0 / (16 * math.pow(alpha0, 5)),\n                    -3.0 / (16 * math.pow(alpha0, 4)),\n                    -3.0 / (16 * math.pow(alpha0, 4)),\n                    1.0 / (16 * math.pow(alpha0, 3)),\n                    -1.0 / (16 * math.pow(alpha0, 3)),\n                ],\n                [\n                    1.0 / (2 * math.pow(alpha0, 6)),\n                    1.0 / (2 * math.pow(alpha0, 6)),\n                    -5.0 / (16 * math.pow(alpha0, 5)),\n                    5.0 / (16 * math.pow(alpha0, 5)),\n                    1.0 / (16 * math.pow(alpha0, 4)),\n                    1.0 / (16 * math.pow(alpha0, 4)),\n                ],\n            ]\n        )\n        b = default_backend.stack(\n            [\n                deltas_up_alpha0 - self._broadcast_helper,\n                deltas_dn_alpha0 - self._broadcast_helper,\n                default_backend.log(self._deltas_up) * deltas_up_alpha0,\n                -default_backend.log(self._deltas_dn) * deltas_dn_alpha0,\n                default_backend.power(default_backend.log(self._deltas_up), 2)\n                * deltas_up_alpha0,\n                default_backend.power(default_backend.log(self._deltas_dn), 2)\n                * deltas_dn_alpha0,\n            ]\n        )\n        self._coefficients = default_backend.einsum(\n            \'rc,shb,cshb->rshb\', A_inverse, self._broadcast_helper, b\n        )\n\n        self._precompute()\n        if subscribe:\n            events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        self.deltas_up = tensorlib.astensor(self._deltas_up)\n        self.deltas_dn = tensorlib.astensor(self._deltas_dn)\n        self.broadcast_helper = tensorlib.astensor(self._broadcast_helper)\n        self.alpha0 = tensorlib.astensor(self._alpha0)\n        self.coefficients = tensorlib.astensor(self._coefficients)\n        self.bases_up = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.ones(self.alphasets_shape), self.deltas_up\n        )\n        self.bases_dn = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.ones(self.alphasets_shape), self.deltas_dn\n        )\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n        self.ones = tensorlib.einsum(\n            \'sa,shb->shab\', self.mask_on, self.broadcast_helper\n        )\n\n    def _precompute_alphasets(self, alphasets_shape):\n        if alphasets_shape == self.alphasets_shape:\n            return\n        tensorlib, _ = get_backend()\n        self.alphasets_shape = alphasets_shape\n        self.bases_up = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.ones(self.alphasets_shape), self.deltas_up\n        )\n        self.bases_dn = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.ones(self.alphasets_shape), self.deltas_dn\n        )\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n        self.ones = tensorlib.einsum(\n            \'sa,shb->shab\', self.mask_on, self.broadcast_helper\n        )\n        return\n\n    def __call__(self, alphasets):\n        """"""Compute Interpolated Values.""""""\n        tensorlib, _ = get_backend()\n        self._precompute_alphasets(tensorlib.shape(alphasets))\n\n        # select where alpha >= alpha0 and produce the mask\n        where_alphasets_gtalpha0 = tensorlib.where(\n            alphasets >= self.__alpha0, self.mask_on, self.mask_off\n        )\n        masks_gtalpha0 = tensorlib.astensor(\n            tensorlib.einsum(\n                \'sa,shb->shab\', where_alphasets_gtalpha0, self.broadcast_helper\n            ),\n            dtype=""bool"",\n        )\n\n        # select where alpha > -alpha0 [""not(alpha <= -alpha0)""] and produce the mask\n        where_alphasets_not_ltalpha0 = tensorlib.where(\n            alphasets > -self.__alpha0, self.mask_on, self.mask_off\n        )\n        masks_not_ltalpha0 = tensorlib.astensor(\n            tensorlib.einsum(\n                \'sa,shb->shab\', where_alphasets_not_ltalpha0, self.broadcast_helper\n            ),\n            dtype=""bool"",\n        )\n\n        # s: set under consideration (i.e. the modifier)\n        # a: alpha variation\n        # h: histogram affected by modifier\n        # b: bin of histogram\n        exponents = tensorlib.einsum(\n            \'sa,shb->shab\', tensorlib.abs(alphasets), self.broadcast_helper\n        )\n        # for |alpha| >= alpha0, we want to raise the bases to the exponent=alpha\n        # and for |alpha| < alpha0, we want to raise the bases to the exponent=1\n        masked_exponents = tensorlib.where(\n            exponents >= self.__alpha0, exponents, self.ones\n        )\n        # we need to produce the terms of alpha^i for summing up\n        alphasets_powers = tensorlib.stack(\n            [\n                alphasets,\n                tensorlib.power(alphasets, 2),\n                tensorlib.power(alphasets, 3),\n                tensorlib.power(alphasets, 4),\n                tensorlib.power(alphasets, 5),\n                tensorlib.power(alphasets, 6),\n            ]\n        )\n        # this is the 1 + sum_i a_i alpha^i\n        value_btwn = tensorlib.ones(exponents.shape) + tensorlib.einsum(\n            \'rshb,rsa->shab\', self.coefficients, alphasets_powers\n        )\n\n        # first, build a result where:\n        #       alpha > alpha0   : fill with bases_up\n        #   not(alpha > alpha0)  : fill with 1 + sum(a_i alpha^i)\n        results_gtalpha0_btwn = tensorlib.where(\n            masks_gtalpha0, self.bases_up, value_btwn\n        )\n        # then, build a result where:\n        #      alpha >= -alpha0  : do nothing (fill with previous result)\n        #   not(alpha >= -alpha0): fill with bases_dn\n        bases = tensorlib.where(\n            masks_not_ltalpha0, results_gtalpha0_btwn, self.bases_dn\n        )\n        return tensorlib.power(bases, masked_exponents)\n\n\nclass _slow_code4(object):\n    """"""\n    Reference Implementation of Code 4.\n\n    delta_up^alpha0 = 1 + a1 alpha0 + a2 alpha0^2 + a3 alpha0^3 + a4 alpha0^4 + a5 alpha0^5 + a6 alpha0^6\n    delta_down^alpha0 = 1 - a1 alpha0 + a2 alpha0^2 - a3 alpha0^3 + a4 alpha0^4 - a5 alpha0^5 + a6 alpha0^6\n\n    f[alpha_] := 1 + a1 * alpha + a2 * alpha^2 + a3 * alpha^3 + a4 * alpha^4 + a5 * alpha^5 + a6 * alpha^6\n    up[alpha_] := delta_up^alpha\n    down[alpha_] := delta_down^(-alpha)\n\n    We want to find the coefficients a1, a2, a3, a4, a5, a6 by solving:\n      f[alpha0] == up[alpha0]\n      f[-alpha0] == down[-alpha0]\n      f\'[alpha0] == up\'[alpha0]\n      f\'[-alpha0] == down\'[-alpha0]\n      f\'\'[alpha0] == up\'\'[alpha0]\n      f\'\'[-alpha0] == down\'\'[-alpha0]\n\n    Treating this as multiplication with a rank-6 matrix A: A*x = b, where x = [a1, a2, a3, a4, a5, a6]\n\n    [alpha0,  alpha0^2, alpha0^3,   alpha0^4,   alpha0^5,     alpha0^6  ] [a1] = [ delta_up^(alpha0) - 1                ]\n    [-alpha0, alpha0^2, -alpha0^3,  alpha0^4,   -alpha0^5,    alpha0^6  ] [a2] = [ delta_down^(alpha0) - 1              ]\n    [1,       2alpha0,  3alpha0^2,  4alpha0^3,  5alpha0^4,    6alpha0^5 ] [a3] = [ ln(delta_up) delta_up^(alpha0)       ]\n    [1,       -2alpha0, 3alpha0^2,  -4alpha0^3, 5alpha0^4,    -6alpha0^5] [a4] = [ - ln(delta_down) delta_down^(alpha0) ]\n    [0,       2,        6alpha0,    12alpha0^2, 20alpha0^3,   30alpha0^4] [a5] = [ ln(delta_up)^2 delta_up^(alpha0)     ]\n    [0,       2,        -6alpha0,   12alpha0^2, -20alpha0^3,  30alpha0^4] [a6] = [ ln(delta_down)^2 delta_down^(alpha0) ]\n\n    The determinant of this matrix is -2048*alpha0^15. The trace is 30*alpha0^4+16*alpha0^3+4*alpha0^2+alpha0. Therefore, this matrix is invertible if and only if alpha0 != 0.\n\n    The inverse of this matrix is (and verifying with http://wims.unice.fr/~wims/wims.cgi)\n\n    [15/(16*alpha0),  -15/(16*alpha0),  -7/16,            -7/16,            1/16*alpha0,      -1/16*alpha0    ]\n    [3/(2*alpha0^2),  3/(2*alpha0^2),   -9/(16*alpha0),   9/(16*alpha0),    1/16,             1/16            ]\n    [-5/(8*alpha0^3), 5/(8*alpha0^3),   5/(8*alpha0^2),   5/(8*alpha0^2),   -1/(8*alpha0),    1/(8*alpha0)    ]\n    [3/(-2*alpha0^4), 3/(-2*alpha0^4),  -7/(-8*alpha0^3), 7/(-8*alpha0^3),  -1/(8*alpha0^2),  -1/(8*alpha0^2) ]\n    [3/(16*alpha0^5), -3/(16*alpha0^5), -3/(16*alpha0^4), -3/(16*alpha0^4), 1/(16*alpha0^3),  -1/(16*alpha0^3)]\n    [1/(2*alpha0^6),  1/(2*alpha0^6),   -5/(16*alpha0^5), 5/(16*alpha0^5),  1/(16*alpha0^4),  1/(16*alpha0^4) ]\n\n    """"""\n\n    def product(self, down, nom, up, alpha):\n        delta_up = up / nom\n        delta_down = down / nom\n        if alpha >= self.alpha0:\n            delta = math.pow(delta_up, alpha)\n        elif -self.alpha0 < alpha < self.alpha0:\n            delta_up_alpha0 = math.pow(delta_up, self.alpha0)\n            delta_down_alpha0 = math.pow(delta_down, self.alpha0)\n            b = [\n                delta_up_alpha0 - 1,\n                delta_down_alpha0 - 1,\n                math.log(delta_up) * delta_up_alpha0,\n                -math.log(delta_down) * delta_down_alpha0,\n                math.pow(math.log(delta_up), 2) * delta_up_alpha0,\n                math.pow(math.log(delta_down), 2) * delta_down_alpha0,\n            ]\n            A_inverse = [\n                [\n                    15.0 / (16 * self.alpha0),\n                    -15.0 / (16 * self.alpha0),\n                    -7.0 / 16.0,\n                    -7.0 / 16.0,\n                    1.0 / 16 * self.alpha0,\n                    -1.0 / 16.0 * self.alpha0,\n                ],\n                [\n                    3.0 / (2 * math.pow(self.alpha0, 2)),\n                    3.0 / (2 * math.pow(self.alpha0, 2)),\n                    -9.0 / (16 * self.alpha0),\n                    9.0 / (16 * self.alpha0),\n                    1.0 / 16,\n                    1.0 / 16,\n                ],\n                [\n                    -5.0 / (8 * math.pow(self.alpha0, 3)),\n                    5.0 / (8 * math.pow(self.alpha0, 3)),\n                    5.0 / (8 * math.pow(self.alpha0, 2)),\n                    5.0 / (8 * math.pow(self.alpha0, 2)),\n                    -1.0 / (8 * self.alpha0),\n                    1.0 / (8 * self.alpha0),\n                ],\n                [\n                    3.0 / (-2 * math.pow(self.alpha0, 4)),\n                    3.0 / (-2 * math.pow(self.alpha0, 4)),\n                    -7.0 / (-8 * math.pow(self.alpha0, 3)),\n                    7.0 / (-8 * math.pow(self.alpha0, 3)),\n                    -1.0 / (8 * math.pow(self.alpha0, 2)),\n                    -1.0 / (8 * math.pow(self.alpha0, 2)),\n                ],\n                [\n                    3.0 / (16 * math.pow(self.alpha0, 5)),\n                    -3.0 / (16 * math.pow(self.alpha0, 5)),\n                    -3.0 / (16 * math.pow(self.alpha0, 4)),\n                    -3.0 / (16 * math.pow(self.alpha0, 4)),\n                    1.0 / (16 * math.pow(self.alpha0, 3)),\n                    -1.0 / (16 * math.pow(self.alpha0, 3)),\n                ],\n                [\n                    1.0 / (2 * math.pow(self.alpha0, 6)),\n                    1.0 / (2 * math.pow(self.alpha0, 6)),\n                    -5.0 / (16 * math.pow(self.alpha0, 5)),\n                    5.0 / (16 * math.pow(self.alpha0, 5)),\n                    1.0 / (16 * math.pow(self.alpha0, 4)),\n                    1.0 / (16 * math.pow(self.alpha0, 4)),\n                ],\n            ]\n\n            coefficients = [\n                sum([A_i * b_j for A_i, b_j in zip(A_row, b)]) for A_row in A_inverse\n            ]\n            delta = 1\n            for i in range(1, 7):\n                delta += coefficients[i - 1] * math.pow(alpha, i)\n        else:\n            delta = math.pow(delta_down, (-alpha))\n        return delta\n\n    def __init__(self, histogramssets, subscribe=True, alpha0=1):\n        self._histogramssets = histogramssets\n        self.alpha0 = alpha0\n\n    def __call__(self, alphasets):\n        tensorlib, _ = get_backend()\n        return tensorlib.astensor(\n            _slow_interpolator_looper(\n                self._histogramssets, tensorlib.tolist(alphasets), self.product\n            )\n        )\n'"
src/pyhf/interpolators/code4p.py,0,"b'""""""Piecewise-Linear + Polynomial Interpolation (Code 4p).""""""\nimport logging\nfrom .. import get_backend, default_backend\nfrom .. import events\nfrom . import _slow_interpolator_looper\n\nlog = logging.getLogger(__name__)\n\n\nclass code4p(object):\n    r""""""\n    The piecewise-linear interpolation strategy, with polynomial at :math:`\\left|a\\right| < 1`.\n\n    .. math::\n        \\sigma_{sb} (\\vec{\\alpha}) = \\sigma_{sb}^0(\\vec{\\alpha}) + \\underbrace{\\sum_{p \\in \\text{Syst}} I_\\text{lin.} (\\alpha_p; \\sigma_{sb}^0, \\sigma_{psb}^+, \\sigma_{psb}^-)}_\\text{deltas to calculate}\n\n    """"""\n\n    def __init__(self, histogramssets, subscribe=True):\n        """"""Piecewise-Linear  + Polynomial Interpolation.""""""\n        # nb: this should never be a tensor, store in default backend (e.g. numpy)\n        self._histogramssets = default_backend.astensor(histogramssets)\n        # initial shape will be (nsysts, 1)\n        self.alphasets_shape = (self._histogramssets.shape[0], 1)\n        # precompute terms that only depend on the histogramssets\n        self._deltas_up = self._histogramssets[:, :, 2] - self._histogramssets[:, :, 1]\n        self._deltas_dn = self._histogramssets[:, :, 1] - self._histogramssets[:, :, 0]\n        self._broadcast_helper = default_backend.ones(\n            default_backend.shape(self._deltas_up)\n        )\n        self._precompute()\n        if subscribe:\n            events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        self.deltas_up = tensorlib.astensor(self._deltas_up)\n        self.deltas_dn = tensorlib.astensor(self._deltas_dn)\n\n        self.S = 0.5 * (self.deltas_up + self.deltas_dn)\n        self.A = 0.0625 * (self.deltas_up - self.deltas_dn)\n\n        self.broadcast_helper = tensorlib.astensor(self._broadcast_helper)\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n\n    def _precompute_alphasets(self, alphasets_shape):\n        if alphasets_shape == self.alphasets_shape:\n            return\n        tensorlib, _ = get_backend()\n        self.alphasets_shape = alphasets_shape\n        self.mask_on = tensorlib.ones(self.alphasets_shape)\n        self.mask_off = tensorlib.zeros(self.alphasets_shape)\n\n    def __call__(self, alphasets):\n        """"""Compute Interpolated Values.""""""\n        tensorlib, _ = get_backend()\n        self._precompute_alphasets(tensorlib.shape(alphasets))\n        where_alphasets_greater_p1 = tensorlib.where(\n            alphasets > 1, self.mask_on, self.mask_off\n        )\n\n        where_alphasets_smaller_m1 = tensorlib.where(\n            alphasets < -1, self.mask_on, self.mask_off\n        )\n\n        # s: set under consideration (i.e. the modifier)\n        # a: alpha variation\n        # h: histogram affected by modifier\n        # b: bin of histogram\n\n        # for a > 1\n        alphas_times_deltas_up = tensorlib.einsum(\n            \'sa,shb->shab\', alphasets, self.deltas_up\n        )\n\n        # for a < -1\n        alphas_times_deltas_dn = tensorlib.einsum(\n            \'sa,shb->shab\', alphasets, self.deltas_dn\n        )\n\n        # for |a| < 1\n        asquare = tensorlib.power(alphasets, 2)\n        tmp1 = asquare * 3.0 - 10.0\n        tmp2 = asquare * tmp1 + 15.0\n        tmp3 = asquare * tmp2\n\n        tmp3_times_A = tensorlib.einsum(\'sa,shb->shab\', tmp3, self.A)\n\n        alphas_times_S = tensorlib.einsum(\'sa,shb->shab\', alphasets, self.S)\n\n        deltas = tmp3_times_A + alphas_times_S\n        # end |a| < 1\n\n        masks_p1 = tensorlib.astensor(\n            tensorlib.einsum(\n                \'sa,shb->shab\', where_alphasets_greater_p1, self.broadcast_helper\n            ),\n            dtype=\'bool\',\n        )\n\n        masks_m1 = tensorlib.astensor(\n            tensorlib.einsum(\n                \'sa,shb->shab\', where_alphasets_smaller_m1, self.broadcast_helper\n            ),\n            dtype=\'bool\',\n        )\n\n        return tensorlib.where(\n            masks_m1,\n            alphas_times_deltas_dn,\n            tensorlib.where(masks_p1, alphas_times_deltas_up, deltas),\n        )\n\n\nclass _slow_code4p(object):\n    def summand(self, down, nom, up, alpha):\n        delta_up = up - nom\n        delta_down = nom - down\n        S = 0.5 * (delta_up + delta_down)\n        A = 0.0625 * (delta_up - delta_down)\n        if alpha > 1:\n            delta = delta_up * alpha\n        elif alpha < -1:\n            delta = delta_down * alpha\n        else:\n            delta = alpha * (\n                S + alpha * A * (15 + alpha * alpha * (-10 + alpha * alpha * 3))\n            )\n        return delta\n\n    def __init__(self, histogramssets, subscribe=True):\n        self._histogramssets = histogramssets\n\n    def __call__(self, alphasets):\n        tensorlib, _ = get_backend()\n        return tensorlib.astensor(\n            _slow_interpolator_looper(\n                self._histogramssets, tensorlib.tolist(alphasets), self.summand\n            )\n        )\n'"
src/pyhf/modifiers/__init__.py,0,"b'import logging\n\nfrom .. import exceptions\nfrom .. import get_backend\n\nlog = logging.getLogger(__name__)\n\nregistry = {}\n\n\ndef validate_modifier_structure(modifier):\n    """"""\n    Check if given object contains the right structure for modifiers\n    """"""\n    required_methods = [\'required_parset\']\n\n    for method in required_methods:\n        if not hasattr(modifier, method):\n            raise exceptions.InvalidModifier(\n                \'Expected {0:s} method on modifier {1:s}\'.format(\n                    method, modifier.__name__\n                )\n            )\n    return True\n\n\ndef add_to_registry(\n    cls, cls_name=None, constrained=False, pdf_type=\'normal\', op_code=\'addition\'\n):\n    """"""\n    Consistent add_to_registry() function that handles actually adding thing to the registry.\n\n    Raises an error if the name to register for the modifier already exists in the registry,\n    or if the modifier does not have the right structure.\n    """"""\n    global registry\n    cls_name = cls_name or cls.__name__\n    if cls_name in registry:\n        raise KeyError(\'The modifier name ""{0:s}"" is already taken.\'.format(cls_name))\n    # validate the structure\n    validate_modifier_structure(cls)\n    # set is_constrained\n    cls.is_constrained = constrained\n    if constrained:\n        tensorlib, _ = get_backend()\n        if not hasattr(tensorlib, pdf_type):\n            raise exceptions.InvalidModifier(\n                \'The specified pdf_type ""{0:s}"" is not valid for {1:s}({2:s}). See pyhf.tensor documentation for available pdfs.\'.format(\n                    pdf_type, cls_name, cls.__name__\n                )\n            )\n        cls.pdf_type = pdf_type\n    else:\n        cls.pdf_type = None\n\n    if op_code not in [\'addition\', \'multiplication\']:\n        raise exceptions.InvalidModifier(\n            \'The specified op_code ""{0:s}"" is not valid for {1:s}({2:s}). See pyhf.modifier documentation for available operation codes.\'.format(\n                op_code, cls_name, cls.__name__\n            )\n        )\n    cls.op_code = op_code\n\n    registry[cls_name] = cls\n\n\ndef modifier(*args, **kwargs):\n    """"""\n    Decorator for registering modifiers. To flag the modifier as a constrained modifier, add `constrained=True`.\n\n\n    Args:\n        name: the name of the modifier to use. Use the class name by default. (default: None)\n        constrained: whether the modifier is constrained or not. (default: False)\n        pdf_type: the name of the pdf to use from tensorlib if constrained. (default: normal)\n        op_code: the name of the operation the modifier performs on the data (e.g. addition, multiplication)\n\n    Returns:\n        modifier\n\n    Raises:\n        ValueError: too many keyword arguments, or too many arguments, or wrong arguments\n        TypeError: provided name is not a string\n        pyhf.exceptions.InvalidModifier: object does not have necessary modifier structure\n    """"""\n    #\n    # Examples:\n    #\n    #   >>> @modifiers.modifier\n    #   >>> ... class myCustomModifier(object):\n    #   >>> ...   @classmethod\n    #   >>> ...   def required_parset(cls, sample_data, modifier_data): pass\n    #\n    #   >>> @modifiers.modifier(name=\'myCustomNamer\')\n    #   >>> ... class myCustomModifier(object):\n    #   >>> ...   @classmethod\n    #   >>> ...   def required_parset(cls, sample_data, modifier_data): pass\n    #\n    #   >>> @modifiers.modifier(constrained=False)\n    #   >>> ... class myUnconstrainedModifier(object):\n    #   >>> ...   @classmethod\n    #   >>> ...   def required_parset(cls, sample_data, modifier_data): pass\n    #   >>> ...\n    #   >>> myUnconstrainedModifier.pdf_type\n    #   None\n    #\n    #   >>> @modifiers.modifier(constrained=True, pdf_type=\'poisson\')\n    #   >>> ... class myConstrainedCustomPoissonModifier(object):\n    #   >>> ...   @classmethod\n    #   >>> ...   def required_parset(cls, sample_data, modifier_data): pass\n    #   >>> ...\n    #   >>> myConstrainedCustomGaussianModifier.pdf_type\n    #   \'poisson\'\n    #\n    #   >>> @modifiers.modifier(constrained=True)\n    #   >>> ... class myCustomModifier(object):\n    #   >>> ...   @classmethod\n    #   >>> ...   def required_parset(cls, sample_data, modifier_data): pass\n    #\n    #   >>> @modifiers.modifier(op_code=\'multiplication\')\n    #   >>> ... class myMultiplierModifier(object):\n    #   >>> ...   @classmethod\n    #   >>> ...   def required_parset(cls, sample_data, modifier_data): pass\n    #   >>> ...\n    #   >>> myMultiplierModifier.op_code\n    #   \'multiplication\'\n\n    def _modifier(name, constrained, pdf_type, op_code):\n        def wrapper(cls):\n            add_to_registry(\n                cls,\n                cls_name=name,\n                constrained=constrained,\n                pdf_type=pdf_type,\n                op_code=op_code,\n            )\n            return cls\n\n        return wrapper\n\n    name = kwargs.pop(\'name\', None)\n    constrained = bool(kwargs.pop(\'constrained\', False))\n    pdf_type = str(kwargs.pop(\'pdf_type\', \'normal\'))\n    op_code = str(kwargs.pop(\'op_code\', \'addition\'))\n    # check for unparsed keyword arguments\n    if kwargs:\n        raise ValueError(\'Unparsed keyword arguments {}\'.format(kwargs.keys()))\n    # check to make sure the given name is a string, if passed in one\n    if not isinstance(name, str) and name is not None:\n        raise TypeError(\n            \'@modifier must be given a string. You gave it {}\'.format(type(name))\n        )\n\n    if not args:\n        # called like @modifier(name=\'foo\', constrained=False, pdf_type=\'normal\', op_code=\'addition\')\n        return _modifier(name, constrained, pdf_type, op_code)\n    elif len(args) == 1:\n        # called like @modifier\n        if not callable(args[0]):\n            raise ValueError(\'You must decorate a callable python object\')\n        add_to_registry(\n            args[0],\n            cls_name=name,\n            constrained=constrained,\n            pdf_type=pdf_type,\n            op_code=op_code,\n        )\n        return args[0]\n    else:\n        raise ValueError(\n            \'@modifier must be called with only keyword arguments, @modifier(name=\\\'foo\\\'), or no arguments, @modifier; ({0:d} given)\'.format(\n                len(args)\n            )\n        )\n\n\nfrom .histosys import histosys, histosys_combined\nfrom .lumi import lumi, lumi_combined\nfrom .normfactor import normfactor, normfactor_combined\nfrom .normsys import normsys, normsys_combined\nfrom .shapefactor import shapefactor, shapefactor_combined\nfrom .shapesys import shapesys, shapesys_combined\nfrom .staterror import staterror, staterror_combined\n\nuncombined = {\n    \'histosys\': histosys,\n    \'lumi\': lumi,\n    \'normfactor\': normfactor,\n    \'normsys\': normsys,\n    \'shapefactor\': shapefactor,\n    \'shapesys\': shapesys,\n    \'staterror\': staterror,\n}\n\ncombined = {\n    \'histosys\': histosys_combined,\n    \'lumi\': lumi_combined,\n    \'normfactor\': normfactor_combined,\n    \'normsys\': normsys_combined,\n    \'shapefactor\': shapefactor_combined,\n    \'shapesys\': shapesys_combined,\n    \'staterror\': staterror_combined,\n}\n\n__all__ = [\n    \'histosys\',\n    \'histosys_combined\',\n    \'lumi\',\n    \'lumi_combined\',\n    \'normfactor\',\n    \'normfactor_combined\',\n    \'normsys\',\n    \'normsys_combined\',\n    \'shapefactor\',\n    \'shapefactor_combined\',\n    \'shapesys\',\n    \'shapesys_combined\',\n    \'staterror\',\n    \'staterror_combined\',\n    \'combined\',\n]\n'"
src/pyhf/modifiers/histosys.py,0,"b'import logging\n\nfrom . import modifier\nfrom .. import get_backend, events\nfrom .. import interpolators\nfrom ..parameters import constrained_by_normal, ParamViewer\n\nlog = logging.getLogger(__name__)\n\n\n@modifier(name=\'histosys\', constrained=True, op_code=\'addition\')\nclass histosys(object):\n    @classmethod\n    def required_parset(cls, sample_data, modifier_data):\n        return {\n            \'paramset_type\': constrained_by_normal,\n            \'n_parameters\': 1,\n            \'modifier\': cls.__name__,\n            \'is_constrained\': cls.is_constrained,\n            \'is_shared\': True,\n            \'inits\': (0.0,),\n            \'bounds\': ((-5.0, 5.0),),\n            \'auxdata\': (0.0,),\n        }\n\n\nclass histosys_combined(object):\n    def __init__(\n        self, histosys_mods, pdfconfig, mega_mods, interpcode=\'code0\', batch_size=None\n    ):\n        self.batch_size = batch_size\n        self.interpcode = interpcode\n        assert self.interpcode in [\'code0\', \'code2\', \'code4p\']\n\n        keys = [\'{}/{}\'.format(mtype, m) for m, mtype in histosys_mods]\n        histosys_mods = [m for m, _ in histosys_mods]\n\n        parfield_shape = (\n            (self.batch_size, pdfconfig.npars)\n            if self.batch_size\n            else (pdfconfig.npars,)\n        )\n        self.param_viewer = ParamViewer(\n            parfield_shape, pdfconfig.par_map, histosys_mods\n        )\n\n        self._histosys_histoset = [\n            [\n                [\n                    mega_mods[m][s][\'data\'][\'lo_data\'],\n                    mega_mods[m][s][\'data\'][\'nom_data\'],\n                    mega_mods[m][s][\'data\'][\'hi_data\'],\n                ]\n                for s in pdfconfig.samples\n            ]\n            for m in keys\n        ]\n        self._histosys_mask = [\n            [[mega_mods[m][s][\'data\'][\'mask\']] for s in pdfconfig.samples] for m in keys\n        ]\n\n        if histosys_mods:\n            self.interpolator = getattr(interpolators, self.interpcode)(\n                self._histosys_histoset\n            )\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        self.histosys_mask = tensorlib.astensor(self._histosys_mask, dtype=""bool"")\n        self.histosys_default = tensorlib.zeros(self.histosys_mask.shape)\n        if self.batch_size is None:\n            self.indices = tensorlib.reshape(\n                self.param_viewer.indices_concatenated, (-1, 1)\n            )\n\n    def apply(self, pars):\n        \'\'\'\n        Returns:\n            modification tensor: Shape (n_modifiers, n_global_samples, n_alphas, n_global_bin)\n        \'\'\'\n        if not self.param_viewer.index_selection:\n            return\n\n        tensorlib, _ = get_backend()\n        if self.batch_size is None:\n            histosys_alphaset = self.param_viewer.get(pars, self.indices)\n        else:\n            histosys_alphaset = self.param_viewer.get(pars)\n\n        results_histo = self.interpolator(histosys_alphaset)\n        # either rely on numerical no-op or force with line below\n        results_histo = tensorlib.where(\n            self.histosys_mask, results_histo, self.histosys_default\n        )\n        return results_histo\n'"
src/pyhf/modifiers/lumi.py,0,"b'import logging\n\nfrom . import modifier\nfrom .. import get_backend, events\nfrom ..parameters import constrained_by_normal, ParamViewer\n\nlog = logging.getLogger(__name__)\n\n\n@modifier(name=\'lumi\', constrained=True, pdf_type=\'normal\', op_code=\'multiplication\')\nclass lumi(object):\n    @classmethod\n    def required_parset(cls, sample_data, modifier_data):\n        return {\n            \'paramset_type\': constrained_by_normal,\n            \'n_parameters\': 1,\n            \'modifier\': cls.__name__,\n            \'is_constrained\': cls.is_constrained,\n            \'is_shared\': True,\n            \'op_code\': cls.op_code,\n            \'inits\': None,  # lumi\n            \'bounds\': None,  # (0, 10*lumi)\n            \'auxdata\': None,  # lumi\n            \'sigmas\': None,  # lumi * lumirelerror\n        }\n\n\nclass lumi_combined(object):\n    def __init__(self, lumi_mods, pdfconfig, mega_mods, batch_size=None):\n        self.batch_size = batch_size\n\n        keys = [\'{}/{}\'.format(mtype, m) for m, mtype in lumi_mods]\n        lumi_mods = [m for m, _ in lumi_mods]\n\n        parfield_shape = (\n            (self.batch_size, pdfconfig.npars)\n            if self.batch_size\n            else (pdfconfig.npars,)\n        )\n        self.param_viewer = ParamViewer(parfield_shape, pdfconfig.par_map, lumi_mods)\n\n        self._lumi_mask = [\n            [[mega_mods[m][s][\'data\'][\'mask\']] for s in pdfconfig.samples] for m in keys\n        ]\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        self.lumi_mask = tensorlib.tile(\n            tensorlib.astensor(self._lumi_mask), (1, 1, self.batch_size or 1, 1)\n        )\n        self.lumi_mask_bool = tensorlib.astensor(self.lumi_mask, dtype=""bool"")\n        self.lumi_default = tensorlib.ones(self.lumi_mask.shape)\n\n    def apply(self, pars):\n        \'\'\'\n        Returns:\n            modification tensor: Shape (n_modifiers, n_global_samples, n_alphas, n_global_bin)\n        \'\'\'\n        if not self.param_viewer.index_selection:\n            return\n\n        tensorlib, _ = get_backend()\n        lumis = self.param_viewer.get(pars)\n        if self.batch_size is None:\n            results_lumi = tensorlib.einsum(\'msab,x->msab\', self.lumi_mask, lumis)\n        else:\n            results_lumi = tensorlib.einsum(\'msab,xa->msab\', self.lumi_mask, lumis)\n\n        return tensorlib.where(self.lumi_mask_bool, results_lumi, self.lumi_default)\n'"
src/pyhf/modifiers/normfactor.py,0,"b'import logging\n\nfrom . import modifier\nfrom .. import get_backend, events\nfrom ..parameters import unconstrained, ParamViewer\n\nlog = logging.getLogger(__name__)\n\n\n@modifier(name=\'normfactor\', op_code=\'multiplication\')\nclass normfactor(object):\n    @classmethod\n    def required_parset(cls, sample_data, modifier_data):\n        return {\n            \'paramset_type\': unconstrained,\n            \'n_parameters\': 1,\n            \'modifier\': cls.__name__,\n            \'is_constrained\': cls.is_constrained,\n            \'is_shared\': True,\n            \'inits\': (1.0,),\n            \'bounds\': ((0, 10),),\n        }\n\n\nclass normfactor_combined(object):\n    def __init__(self, normfactor_mods, pdfconfig, mega_mods, batch_size=None):\n        self.batch_size = batch_size\n\n        keys = [\'{}/{}\'.format(mtype, m) for m, mtype in normfactor_mods]\n        normfactor_mods = [m for m, _ in normfactor_mods]\n\n        parfield_shape = (\n            (self.batch_size, pdfconfig.npars)\n            if self.batch_size\n            else (pdfconfig.npars,)\n        )\n        self.param_viewer = ParamViewer(\n            parfield_shape, pdfconfig.par_map, normfactor_mods\n        )\n\n        self._normfactor_mask = [\n            [[mega_mods[m][s][\'data\'][\'mask\']] for s in pdfconfig.samples] for m in keys\n        ]\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        if not self.param_viewer.index_selection:\n            return\n        self.normfactor_mask = tensorlib.tile(\n            tensorlib.astensor(self._normfactor_mask), (1, 1, self.batch_size or 1, 1)\n        )\n        self.normfactor_mask_bool = tensorlib.astensor(\n            self.normfactor_mask, dtype=""bool""\n        )\n        self.normfactor_default = tensorlib.ones(self.normfactor_mask.shape)\n\n    def apply(self, pars):\n        \'\'\'\n        Returns:\n            modification tensor: Shape (n_modifiers, n_global_samples, n_alphas, n_global_bin)\n        \'\'\'\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        if self.batch_size is None:\n            normfactors = self.param_viewer.get(pars)\n            results_normfactor = tensorlib.einsum(\n                \'msab,m->msab\', self.normfactor_mask, normfactors\n            )\n        else:\n            normfactors = self.param_viewer.get(pars)\n            results_normfactor = tensorlib.einsum(\n                \'msab,ma->msab\', self.normfactor_mask, normfactors\n            )\n\n        results_normfactor = tensorlib.where(\n            self.normfactor_mask_bool, results_normfactor, self.normfactor_default\n        )\n        return results_normfactor\n'"
src/pyhf/modifiers/normsys.py,0,"b'import logging\n\nfrom . import modifier\nfrom .. import get_backend, events\nfrom .. import interpolators\nfrom ..parameters import constrained_by_normal, ParamViewer\n\nlog = logging.getLogger(__name__)\n\n\n@modifier(name=\'normsys\', constrained=True, op_code=\'multiplication\')\nclass normsys(object):\n    @classmethod\n    def required_parset(cls, sample_data, modifier_data):\n        return {\n            \'paramset_type\': constrained_by_normal,\n            \'n_parameters\': 1,\n            \'modifier\': cls.__name__,\n            \'is_constrained\': cls.is_constrained,\n            \'is_shared\': True,\n            \'inits\': (0.0,),\n            \'bounds\': ((-5.0, 5.0),),\n            \'auxdata\': (0.0,),\n        }\n\n\nclass normsys_combined(object):\n    def __init__(\n        self, normsys_mods, pdfconfig, mega_mods, interpcode=\'code1\', batch_size=None\n    ):\n        self.interpcode = interpcode\n        assert self.interpcode in [\'code1\', \'code4\']\n\n        keys = [\'{}/{}\'.format(mtype, m) for m, mtype in normsys_mods]\n        normsys_mods = [m for m, _ in normsys_mods]\n\n        self.batch_size = batch_size\n\n        parfield_shape = (\n            (self.batch_size, pdfconfig.npars)\n            if self.batch_size\n            else (pdfconfig.npars,)\n        )\n        self.param_viewer = ParamViewer(parfield_shape, pdfconfig.par_map, normsys_mods)\n        self._normsys_histoset = [\n            [\n                [\n                    mega_mods[m][s][\'data\'][\'lo\'],\n                    mega_mods[m][s][\'data\'][\'nom_data\'],\n                    mega_mods[m][s][\'data\'][\'hi\'],\n                ]\n                for s in pdfconfig.samples\n            ]\n            for m in keys\n        ]\n        self._normsys_mask = [\n            [[mega_mods[m][s][\'data\'][\'mask\']] for s in pdfconfig.samples] for m in keys\n        ]\n\n        if normsys_mods:\n            self.interpolator = getattr(interpolators, self.interpcode)(\n                self._normsys_histoset\n            )\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        self.normsys_mask = tensorlib.tile(\n            tensorlib.astensor(self._normsys_mask, dtype=""bool""),\n            (1, 1, self.batch_size or 1, 1),\n        )\n        self.normsys_default = tensorlib.ones(self.normsys_mask.shape)\n        if self.batch_size is None:\n            self.indices = tensorlib.reshape(\n                self.param_viewer.indices_concatenated, (-1, 1)\n            )\n\n    def apply(self, pars):\n        \'\'\'\n        Returns:\n            modification tensor: Shape (n_modifiers, n_global_samples, n_alphas, n_global_bin)\n        \'\'\'\n        if not self.param_viewer.index_selection:\n            return\n\n        tensorlib, _ = get_backend()\n        if self.batch_size is None:\n            normsys_alphaset = self.param_viewer.get(pars, self.indices)\n        else:\n            normsys_alphaset = self.param_viewer.get(pars)\n        results_norm = self.interpolator(normsys_alphaset)\n\n        # either rely on numerical no-op or force with line below\n        results_norm = tensorlib.where(\n            self.normsys_mask, results_norm, self.normsys_default\n        )\n        return results_norm\n'"
src/pyhf/modifiers/shapefactor.py,0,"b'import logging\n\nfrom . import modifier\nfrom .. import get_backend, default_backend, events\nfrom ..parameters import unconstrained, ParamViewer\n\nlog = logging.getLogger(__name__)\n\n\n@modifier(name=\'shapefactor\', op_code=\'multiplication\')\nclass shapefactor(object):\n    @classmethod\n    def required_parset(cls, sample_data, modifier_data):\n        return {\n            \'paramset_type\': unconstrained,\n            \'n_parameters\': len(sample_data),\n            \'modifier\': cls.__name__,\n            \'is_constrained\': cls.is_constrained,\n            \'is_shared\': True,\n            \'inits\': (1.0,) * len(sample_data),\n            \'bounds\': ((0.0, 10.0),) * len(sample_data),\n        }\n\n\nclass shapefactor_combined(object):\n    def __init__(self, shapefactor_mods, pdfconfig, mega_mods, batch_size=None):\n        """"""\n        Imagine a situation where we have 2 channels (SR, CR), 3 samples (sig1,\n        bkg1, bkg2), and 2 shapefactor modifiers (coupled_shapefactor,\n        uncoupled_shapefactor). Let\'s say this is the set-up:\n\n            SR(nbins=2)\n              sig1 -> subscribes to normfactor\n              bkg1 -> subscribes to coupled_shapefactor\n            CR(nbins=3)\n              bkg2 -> subscribes to coupled_shapefactor, uncoupled_shapefactor\n\n        The coupled_shapefactor needs to have 3 nuisance parameters to account\n        for the CR, with 2 of them shared in the SR. The uncoupled_shapefactor\n        just has 3 nuisance parameters.\n\n        self._parindices will look like\n            [0, 1, 2, 3, 4, 5, 6]\n\n        self._shapefactor_indices will look like\n            [[1,2,3],[4,5,6]]\n             ^^^^^^^         = coupled_shapefactor\n                     ^^^^^^^ = uncoupled_shapefactor\n\n        with the 0th par-index corresponding to the normfactor. Because\n        channel1 has 2 bins, and channel2 has 3 bins (with channel1 before\n        channel2), global_concatenated_bin_indices looks like\n            [0, 1, 0, 1, 2]\n            ^^^^^            = channel1\n                  ^^^^^^^^^  = channel2\n\n        So now we need to gather the corresponding shapefactor indices\n        according to global_concatenated_bin_indices. Therefore\n        self._shapefactor_indices now looks like\n            [[1, 2, 1, 2, 3], [4, 5, 4, 5, 6]]\n\n        and at that point can be used to compute the effect of shapefactor.\n        """"""\n\n        self.batch_size = batch_size\n        keys = [\'{}/{}\'.format(mtype, m) for m, mtype in shapefactor_mods]\n        shapefactor_mods = [m for m, _ in shapefactor_mods]\n\n        parfield_shape = (self.batch_size or 1, pdfconfig.npars)\n        self.param_viewer = ParamViewer(\n            parfield_shape, pdfconfig.par_map, shapefactor_mods\n        )\n\n        self._shapefactor_mask = [\n            [[mega_mods[m][s][\'data\'][\'mask\']] for s in pdfconfig.samples] for m in keys\n        ]\n\n        global_concatenated_bin_indices = [\n            [[j for c in pdfconfig.channels for j in range(pdfconfig.channel_nbins[c])]]\n        ]\n\n        self._access_field = default_backend.tile(\n            global_concatenated_bin_indices,\n            (len(shapefactor_mods), self.batch_size or 1, 1),\n        )\n        # acess field is now\n        # e.g. for a 3 channnel (3 bins, 2 bins, 5 bins) model\n        # [\n        #   [0 1 2 0 1 0 1 2 3 4] (number of rows according to batch_size but at least 1)\n        #   [0 1 2 0 1 0 1 2 3 4]\n        #   [0 1 2 0 1 0 1 2 3 4]\n        # ]\n\n        # the index selection of param_viewer is a\n        # list of (batch_size, par_slice) tensors\n        # so self.param_viewer.index_selection[s][t]\n        # points to the indices for a given systematic\n        # at a given position in the batch\n        # we thus populate the access field with these indices\n        # up to the point where we run out of bins (in case)\n        # the paramset slice is larger than the number of bins\n        # in which case we use a dummy index that will be masked\n        # anyways in apply (here: 0)\n\n        # access field is shape (sys, batch, globalbin)\n        for s, syst_access in enumerate(self._access_field):\n            for t, batch_access in enumerate(syst_access):\n                selection = self.param_viewer.index_selection[s][t]\n                for b, bin_access in enumerate(batch_access):\n                    self._access_field[s, t, b] = (\n                        selection[bin_access] if bin_access < len(selection) else 0\n                    )\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        self.shapefactor_mask = tensorlib.tile(\n            tensorlib.astensor(self._shapefactor_mask, dtype=""bool""),\n            (1, 1, self.batch_size or 1, 1),\n        )\n        self.access_field = tensorlib.astensor(self._access_field, dtype=\'int\')\n\n        self.shapefactor_default = tensorlib.ones(\n            tensorlib.shape(self.shapefactor_mask)\n        )\n        self.sample_ones = tensorlib.ones(tensorlib.shape(self.shapefactor_mask)[1])\n\n    def apply(self, pars):\n        \'\'\'\n        Returns:\n            modification tensor: Shape (n_modifiers, n_global_samples, n_alphas, n_global_bin)\n        \'\'\'\n        if not self.param_viewer.index_selection:\n            return\n\n        tensorlib, _ = get_backend()\n        if self.batch_size is None:\n            flat_pars = pars\n        else:\n            flat_pars = tensorlib.reshape(pars, (-1,))\n        shapefactors = tensorlib.gather(flat_pars, self.access_field)\n        results_shapefactor = tensorlib.einsum(\n            \'mab,s->msab\', shapefactors, self.sample_ones\n        )\n        results_shapefactor = tensorlib.where(\n            self.shapefactor_mask, results_shapefactor, self.shapefactor_default\n        )\n        return results_shapefactor\n'"
src/pyhf/modifiers/shapesys.py,0,"b'import logging\n\nfrom . import modifier\nfrom .. import get_backend, default_backend, events\nfrom ..parameters import constrained_by_poisson, ParamViewer\n\nlog = logging.getLogger(__name__)\n\n\n@modifier(\n    name=\'shapesys\', constrained=True, pdf_type=\'poisson\', op_code=\'multiplication\'\n)\nclass shapesys(object):\n    @classmethod\n    def required_parset(cls, sample_data, modifier_data):\n        # count the number of bins with nonzero, positive yields\n        valid_bins = [\n            (sample_bin > 0 and modifier_bin > 0)\n            for sample_bin, modifier_bin in zip(modifier_data, sample_data)\n        ]\n        n_parameters = sum(valid_bins)\n        return {\n            \'paramset_type\': constrained_by_poisson,\n            \'n_parameters\': n_parameters,\n            \'modifier\': cls.__name__,\n            \'is_constrained\': cls.is_constrained,\n            \'is_shared\': False,\n            \'inits\': (1.0,) * n_parameters,\n            \'bounds\': ((1e-10, 10.0),) * n_parameters,\n            # nb: auxdata/factors set by finalize. Set to non-numeric to crash\n            # if we fail to set auxdata/factors correctly\n            \'auxdata\': (None,) * n_parameters,\n            \'factors\': (None,) * n_parameters,\n        }\n\n\nclass shapesys_combined(object):\n    def __init__(self, shapesys_mods, pdfconfig, mega_mods, batch_size=None):\n        self.batch_size = batch_size\n\n        keys = [\'{}/{}\'.format(mtype, m) for m, mtype in shapesys_mods]\n        self._shapesys_mods = [m for m, _ in shapesys_mods]\n\n        parfield_shape = (self.batch_size or 1, pdfconfig.npars)\n        self.param_viewer = ParamViewer(\n            parfield_shape, pdfconfig.par_map, self._shapesys_mods\n        )\n\n        self._shapesys_mask = [\n            [[mega_mods[m][s][\'data\'][\'mask\']] for s in pdfconfig.samples] for m in keys\n        ]\n        self.__shapesys_info = default_backend.astensor(\n            [\n                [\n                    [\n                        mega_mods[m][s][\'data\'][\'mask\'],\n                        mega_mods[m][s][\'data\'][\'nom_data\'],\n                        mega_mods[m][s][\'data\'][\'uncrt\'],\n                    ]\n                    for s in pdfconfig.samples\n                ]\n                for m in keys\n            ]\n        )\n        self.finalize(pdfconfig)\n\n        global_concatenated_bin_indices = [\n            [[j for c in pdfconfig.channels for j in range(pdfconfig.channel_nbins[c])]]\n        ]\n\n        self._access_field = default_backend.tile(\n            global_concatenated_bin_indices,\n            (len(shapesys_mods), self.batch_size or 1, 1),\n        )\n        # access field is shape (sys, batch, globalbin)\n\n        # reindex it based on current masking\n        self._reindex_access_field(pdfconfig)\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _reindex_access_field(self, pdfconfig):\n        for syst_index, syst_access in enumerate(self._access_field):\n            if not pdfconfig.param_set(self._shapesys_mods[syst_index]).n_parameters:\n                self._access_field[syst_index] = 0\n                continue\n\n            singular_sample_index = [\n                idx\n                for idx, syst in enumerate(\n                    default_backend.astensor(self._shapesys_mask)[syst_index, :, 0]\n                )\n                if any(syst)\n            ][-1]\n\n            for batch_index, batch_access in enumerate(syst_access):\n                selection = self.param_viewer.index_selection[syst_index][batch_index]\n                access_field_for_syst_and_batch = default_backend.zeros(\n                    len(batch_access)\n                )\n\n                sample_mask = self._shapesys_mask[syst_index][singular_sample_index][0]\n                access_field_for_syst_and_batch[sample_mask] = selection\n                self._access_field[\n                    syst_index, batch_index\n                ] = access_field_for_syst_and_batch\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        if not self.param_viewer.index_selection:\n            return\n        self.shapesys_mask = tensorlib.astensor(self._shapesys_mask, dtype=""bool"")\n        self.shapesys_mask = tensorlib.tile(\n            self.shapesys_mask, (1, 1, self.batch_size or 1, 1)\n        )\n        self.access_field = tensorlib.astensor(self._access_field, dtype=\'int\')\n        self.sample_ones = tensorlib.ones(tensorlib.shape(self.shapesys_mask)[1])\n        self.shapesys_default = tensorlib.ones(tensorlib.shape(self.shapesys_mask))\n\n    def finalize(self, pdfconfig):\n        # self.__shapesys_info: (parameter, sample, [mask, nominal rate, uncertainty], bin)\n        for mod_uncert_info, pname in zip(self.__shapesys_info, self._shapesys_mods):\n            # skip cases where given shapesys modifier affects zero samples\n            if not pdfconfig.param_set(pname).n_parameters:\n                continue\n\n            # identify the information for the sample that the given parameter\n            # affects. shapesys is not shared, so there should only ever be at\n            # most one sample\n            # sample_uncert_info: ([mask, nominal rate, uncertainty], bin)\n            sample_uncert_info = mod_uncert_info[\n                default_backend.astensor(\n                    default_backend.sum(mod_uncert_info[:, 0] > 0, axis=1), dtype=\'bool\'\n                )\n            ][0]\n\n            # bin_mask: ([mask], bin)\n            bin_mask = default_backend.astensor(sample_uncert_info[0], dtype=\'bool\')\n            # nom_unc: ([nominal, uncertainty], bin)\n            nom_unc = sample_uncert_info[1:]\n\n            # compute gamma**2 and sigma**2\n            nom_unc_sq = default_backend.power(nom_unc, 2)\n            # when the nominal rate = 0 OR uncertainty = 0, set = 1\n            nom_unc_sq[nom_unc_sq == 0] = 1\n            # divide (gamma**2 / sigma**2) and mask to set factors for only the\n            # parameters we have allocated\n            factors = (nom_unc_sq[0] / nom_unc_sq[1])[bin_mask]\n            assert len(factors) == pdfconfig.param_set(pname).n_parameters\n\n            pdfconfig.param_set(pname).factors = default_backend.tolist(factors)\n            pdfconfig.param_set(pname).auxdata = default_backend.tolist(factors)\n\n    def apply(self, pars):\n        \'\'\'\n        Returns:\n            modification tensor: Shape (n_modifiers, n_global_samples, n_alphas, n_global_bin)\n        \'\'\'\n        tensorlib, _ = get_backend()\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        if self.batch_size is None:\n            flat_pars = pars\n        else:\n            flat_pars = tensorlib.reshape(pars, (-1,))\n        shapefactors = tensorlib.gather(flat_pars, self.access_field)\n        results_shapesys = tensorlib.einsum(\n            \'mab,s->msab\', shapefactors, self.sample_ones\n        )\n\n        results_shapesys = tensorlib.where(\n            self.shapesys_mask, results_shapesys, self.shapesys_default\n        )\n        return results_shapesys\n'"
src/pyhf/modifiers/staterror.py,0,"b'import logging\n\nfrom . import modifier\nfrom .. import get_backend, default_backend, events\nfrom ..parameters import constrained_by_normal, ParamViewer\n\nlog = logging.getLogger(__name__)\n\n\n@modifier(name=\'staterror\', constrained=True, op_code=\'multiplication\')\nclass staterror(object):\n    @classmethod\n    def required_parset(cls, sample_data, modifier_data):\n        return {\n            \'paramset_type\': constrained_by_normal,\n            \'n_parameters\': len(sample_data),\n            \'modifier\': cls.__name__,\n            \'is_constrained\': cls.is_constrained,\n            \'is_shared\': True,\n            \'inits\': (1.0,) * len(sample_data),\n            \'bounds\': ((1e-10, 10.0),) * len(sample_data),\n            \'auxdata\': (1.0,) * len(sample_data),\n        }\n\n\nclass staterror_combined(object):\n    def __init__(self, staterr_mods, pdfconfig, mega_mods, batch_size=None):\n        self.batch_size = batch_size\n\n        keys = [\'{}/{}\'.format(mtype, m) for m, mtype in staterr_mods]\n        self._staterr_mods = [m for m, _ in staterr_mods]\n\n        parfield_shape = (self.batch_size or 1, pdfconfig.npars)\n        self.param_viewer = ParamViewer(\n            parfield_shape, pdfconfig.par_map, self._staterr_mods\n        )\n\n        self._staterror_mask = [\n            [[mega_mods[m][s][\'data\'][\'mask\']] for s in pdfconfig.samples] for m in keys\n        ]\n        self.__staterror_uncrt = default_backend.astensor(\n            [\n                [\n                    [\n                        mega_mods[m][s][\'data\'][\'uncrt\'],\n                        mega_mods[m][s][\'data\'][\'nom_data\'],\n                    ]\n                    for s in pdfconfig.samples\n                ]\n                for m in keys\n            ]\n        )\n        self.finalize(pdfconfig)\n\n        global_concatenated_bin_indices = [\n            [[j for c in pdfconfig.channels for j in range(pdfconfig.channel_nbins[c])]]\n        ]\n\n        self._access_field = default_backend.tile(\n            global_concatenated_bin_indices,\n            (len(staterr_mods), self.batch_size or 1, 1),\n        )\n        # access field is shape (sys, batch, globalbin)\n        for s, syst_access in enumerate(self._access_field):\n            for t, batch_access in enumerate(syst_access):\n                selection = self.param_viewer.index_selection[s][t]\n                for b, bin_access in enumerate(batch_access):\n                    self._access_field[s, t, b] = (\n                        selection[bin_access] if bin_access < len(selection) else 0\n                    )\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        if not self.param_viewer.index_selection:\n            return\n        tensorlib, _ = get_backend()\n        self.staterror_mask = tensorlib.astensor(self._staterror_mask, dtype=""bool"")\n        self.staterror_mask = tensorlib.tile(\n            self.staterror_mask, (1, 1, self.batch_size or 1, 1)\n        )\n        self.access_field = tensorlib.astensor(self._access_field, dtype=\'int\')\n        self.sample_ones = tensorlib.ones(tensorlib.shape(self.staterror_mask)[1])\n        self.staterror_default = tensorlib.ones(tensorlib.shape(self.staterror_mask))\n\n    def finalize(self, pdfconfig):\n        staterror_mask = default_backend.astensor(self._staterror_mask)\n        for this_mask, uncert_this_mod, mod in zip(\n            staterror_mask, self.__staterror_uncrt, self._staterr_mods\n        ):\n            active_nominals = default_backend.where(\n                this_mask[:, 0, :],\n                uncert_this_mod[:, 1, :],\n                default_backend.zeros(uncert_this_mod[:, 1, :].shape),\n            )\n            summed_nominals = default_backend.sum(active_nominals, axis=0)\n\n            # the below tries to filter cases in which this modifier is not\n            # used by checking non zeroness.. should probably use mask\n            numerator = default_backend.where(\n                uncert_this_mod[:, 1, :] > 0,\n                uncert_this_mod[:, 0, :],\n                default_backend.zeros(uncert_this_mod[:, 1, :].shape),\n            )\n            denominator = default_backend.where(\n                summed_nominals > 0,\n                summed_nominals,\n                default_backend.ones(uncert_this_mod[:, 1, :].shape),\n            )\n            relerrs = numerator / denominator\n            sigmas = default_backend.sqrt(\n                default_backend.sum(default_backend.power(relerrs, 2), axis=0)\n            )\n            assert len(sigmas[sigmas > 0]) == pdfconfig.param_set(mod).n_parameters\n            pdfconfig.param_set(mod).sigmas = default_backend.tolist(sigmas[sigmas > 0])\n\n    def apply(self, pars):\n        if not self.param_viewer.index_selection:\n            return\n\n        tensorlib, _ = get_backend()\n        if self.batch_size is None:\n            flat_pars = pars\n        else:\n            flat_pars = tensorlib.reshape(pars, (-1,))\n        statfactors = tensorlib.gather(flat_pars, self.access_field)\n        results_staterr = tensorlib.einsum(\'mab,s->msab\', statfactors, self.sample_ones)\n        results_staterr = tensorlib.where(\n            self.staterror_mask, results_staterr, self.staterror_default\n        )\n        return results_staterr\n'"
src/pyhf/optimize/__init__.py,0,"b'""""""Optimizers for Tensor Functions.""""""\n\nfrom .. import exceptions\n\n\nclass _OptimizerRetriever(object):\n    def __getattr__(self, name):\n        if name == \'scipy_optimizer\':\n            from .opt_scipy import scipy_optimizer\n\n            assert scipy_optimizer\n            # for autocomplete and dir() calls\n            self.scipy_optimizer = scipy_optimizer\n            return scipy_optimizer\n        elif name == \'jax_optimizer\':\n            try:\n                from .opt_jax import jax_optimizer\n\n                assert jax_optimizer\n                self.jax_optimizer = jax_optimizer\n                return jax_optimizer\n            except ImportError as e:\n                raise exceptions.ImportBackendError(\n                    ""There was a problem importing jax. The jax optimizer cannot be used."",\n                    e,\n                )\n        elif name == \'pytorch_optimizer\':\n            try:\n                from .opt_pytorch import pytorch_optimizer\n\n                assert pytorch_optimizer\n                # for autocomplete and dir() calls\n                self.pytorch_optimizer = pytorch_optimizer\n                return pytorch_optimizer\n            except ImportError as e:\n                raise exceptions.ImportBackendError(\n                    ""There was a problem importing PyTorch. The pytorch optimizer cannot be used."",\n                    e,\n                )\n        elif name == \'tflow_optimizer\':\n            try:\n                from .opt_tflow import tflow_optimizer\n\n                assert tflow_optimizer\n                # for autocomplete and dir() calls\n                self.tflow_optimizer = tflow_optimizer\n                return tflow_optimizer\n            except ImportError as e:\n                raise exceptions.ImportBackendError(\n                    ""There was a problem importing TensorFlow. The tensorflow optimizer cannot be used."",\n                    e,\n                )\n        elif name == \'minuit_optimizer\':\n            try:\n                from .opt_minuit import minuit_optimizer\n\n                assert minuit_optimizer\n                # for autocomplete and dir() calls\n                self.minuit_optimizer = minuit_optimizer\n                return minuit_optimizer\n            except ImportError as e:\n                raise exceptions.ImportBackendError(\n                    ""There was a problem importing Minuit. The minuit optimizer cannot be used."",\n                    e,\n                )\n        elif name == \'__wrapped__\':  # doctest\n            pass\n        else:\n            raise exceptions.InvalidOptimizer(\n                ""The requested optimizer \\""{0:s}\\"" does not exist."".format(name)\n            )\n\n\nOptimizerRetriever = _OptimizerRetriever()\n__all__ = [\'OptimizerRetriever\']\n'"
src/pyhf/optimize/autodiff.py,0,"b'""""""Helper Classes for use of automatic differentiation.""""""\nimport scipy\nfrom .. import get_backend\n\n\nclass AutoDiffOptimizerMixin(object):\n    """"""Mixin Class to build optimizers that use automatic differentiation.""""""\n\n    def __init__(*args, **kwargs):\n        """"""Create Mixin for autodiff-based optimizers.""""""\n\n    def minimize(\n        self,\n        objective,\n        data,\n        pdf,\n        init_pars,\n        par_bounds,\n        fixed_vals=None,\n        return_fitted_val=False,\n    ):\n        """"""\n        Find Function Parameters that minimize the Objective.\n\n        Returns:\n            bestfit parameters\n        \n        """"""\n        tensorlib, _ = get_backend()\n        tv, fixed_values_tensor, func, init, bounds = self.setup_minimize(\n            objective, data, pdf, init_pars, par_bounds, fixed_vals\n        )\n        fitresult = scipy.optimize.minimize(\n            func, init, method=\'SLSQP\', jac=True, bounds=bounds\n        )\n        nonfixed_vals = fitresult.x\n        fitted_val = fitresult.fun\n        fitted_pars = tv.stitch(\n            [fixed_values_tensor, tensorlib.astensor(nonfixed_vals)]\n        )\n        if return_fitted_val:\n            return fitted_pars, tensorlib.astensor(fitted_val)\n        return fitted_pars\n'"
src/pyhf/optimize/opt_jax.py,0,"b'""""""JAX Optimizer Backend.""""""\n\nfrom .. import get_backend, default_backend\nfrom ..tensor.common import _TensorViewer\nfrom .autodiff import AutoDiffOptimizerMixin\nimport jax\n\n\ndef _final_objective(pars, data, fixed_vals, model, objective, fixed_idx, variable_idx):\n    tensorlib, _ = get_backend()\n    tv = _TensorViewer([fixed_idx, variable_idx])\n    pars = tensorlib.astensor(pars)\n    constrained_pars = tv.stitch([fixed_vals, pars])\n    return objective(constrained_pars, data, model)[0]\n\n\n_jitted_objective_and_grad = jax.jit(\n    jax.value_and_grad(_final_objective), static_argnums=(3, 4, 5, 6)\n)\n\n\nclass jax_optimizer(AutoDiffOptimizerMixin):\n    """"""JAX Optimizer Backend.""""""\n\n    def setup_minimize(\n        self, objective, data, pdf, init_pars, par_bounds, fixed_vals=None\n    ):\n        """"""\n        Prepare Minimization for AutoDiff-Optimizer.\n\n        Args:\n            objective: objective function\n            data: observed data\n            pdf: model\n            init_pars: initial parameters\n            par_bounds: parameter boundaries\n            fixed_vals: fixed parameter values\n\n        """"""\n\n        tensorlib, _ = get_backend()\n        all_idx = default_backend.astensor(range(pdf.config.npars), dtype=\'int\')\n        all_init = default_backend.astensor(init_pars)\n\n        fixed_vals = fixed_vals or []\n        fixed_values = [x[1] for x in fixed_vals]\n        fixed_idx = [x[0] for x in fixed_vals]\n\n        variable_idx = [x for x in all_idx if x not in fixed_idx]\n        variable_init = all_init[variable_idx]\n        variable_bounds = [par_bounds[i] for i in variable_idx]\n\n        tv = _TensorViewer([fixed_idx, variable_idx])\n\n        data = tensorlib.astensor(data)\n        fixed_values_tensor = tensorlib.astensor(fixed_values, dtype=\'float\')\n\n        def func(pars):\n            # need to conver to tuple to make args hashable\n            return _jitted_objective_and_grad(\n                pars,\n                data,\n                fixed_values_tensor,\n                pdf,\n                objective,\n                tuple(fixed_idx),\n                tuple(variable_idx),\n            )\n\n        return tv, fixed_values_tensor, func, variable_init, variable_bounds\n'"
src/pyhf/optimize/opt_minuit.py,2,"b'""""""MINUIT Optimizer Backend.""""""\n\nimport iminuit\nimport logging\nimport numpy as np\n\nlog = logging.getLogger(__name__)\n\n\nclass minuit_optimizer(object):\n    """"""MINUIT Optimizer Backend.""""""\n\n    def __init__(self, verbose=False, ncall=10000, errordef=1, steps=1000):\n        """"""\n        Create MINUIT Optimizer.\n\n        Args:\n            verbose (`bool`): print verbose output during minimization\n        \n        """"""\n        self.verbose = verbose\n        self.ncall = ncall\n        self.errordef = errordef\n        self.steps = steps\n\n    def _make_minuit(\n        self, objective, data, pdf, init_pars, init_bounds, fixed_vals=None\n    ):\n        def f(pars):\n            result = objective(pars, data, pdf)\n            logpdf = result[0]\n            return logpdf\n\n        parnames = [\'p{}\'.format(i) for i in range(len(init_pars))]\n        kw = {\'limit_p{}\'.format(i): b for i, b in enumerate(init_bounds)}\n        initvals = {\'p{}\'.format(i): v for i, v in enumerate(init_pars)}\n        step_sizes = {\n            \'error_p{}\'.format(i): (b[1] - b[0]) / float(self.steps)\n            for i, b in enumerate(init_bounds)\n        }\n        fixed_vals = fixed_vals or []\n        constraints = {}\n        for index, value in fixed_vals:\n            constraints = {\'fix_p{}\'.format(index): True}\n            initvals[\'p{}\'.format(index)] = value\n        kwargs = {}\n        for d in [kw, constraints, initvals, step_sizes]:\n            kwargs.update(**d)\n        mm = iminuit.Minuit(\n            f,\n            print_level=1 if self.verbose else 0,\n            errordef=1,\n            use_array_call=True,\n            forced_parameters=parnames,\n            **kwargs,\n        )\n        return mm\n\n    def minimize(\n        self,\n        objective,\n        data,\n        pdf,\n        init_pars,\n        par_bounds,\n        fixed_vals=None,\n        return_fitted_val=False,\n        return_uncertainties=False,\n    ):\n        """"""\n        Find Function Parameters that minimize the Objective.\n\n        Returns:\n            bestfit parameters\n        \n        """"""\n        mm = self._make_minuit(objective, data, pdf, init_pars, par_bounds, fixed_vals)\n        result = mm.migrad(ncall=self.ncall)\n        assert result\n        if return_uncertainties:\n            bestfit_pars = np.asarray([(v, mm.errors[k]) for k, v in mm.values.items()])\n        else:\n            bestfit_pars = np.asarray([v for k, v in mm.values.items()])\n        bestfit_value = mm.fval\n        if return_fitted_val:\n            return bestfit_pars, bestfit_value\n        return bestfit_pars\n'"
src/pyhf/optimize/opt_pytorch.py,0,"b'""""""PyTorch Optimizer Backend.""""""\n\nfrom .. import get_backend, default_backend\nfrom ..tensor.common import _TensorViewer\nfrom .autodiff import AutoDiffOptimizerMixin\nimport torch\n\n\nclass pytorch_optimizer(AutoDiffOptimizerMixin):\n    """"""PyTorch Optimizer Backend.""""""\n\n    def setup_minimize(\n        self, objective, data, pdf, init_pars, par_bounds, fixed_vals=None\n    ):\n        """"""\n        Prepare Minimization for AutoDiff-Optimizer.\n\n        Args:\n            objective: objective function\n            data: observed data\n            pdf: model\n            init_pars: initial parameters\n            par_bounds: parameter boundaries\n            fixed_vals: fixed parameter values\n\n        """"""\n        tensorlib, _ = get_backend()\n        all_idx = default_backend.astensor(range(pdf.config.npars), dtype=\'int\')\n        all_init = default_backend.astensor(init_pars)\n\n        fixed_vals = fixed_vals or []\n        fixed_values = [x[1] for x in fixed_vals]\n        fixed_idx = [x[0] for x in fixed_vals]\n\n        variable_idx = [x for x in all_idx if x not in fixed_idx]\n        variable_init = all_init[variable_idx]\n        variable_bounds = [par_bounds[i] for i in variable_idx]\n\n        tv = _TensorViewer([fixed_idx, variable_idx])\n\n        data = tensorlib.astensor(data)\n        fixed_values_tensor = tensorlib.astensor(fixed_values, dtype=\'float\')\n\n        def func(pars):\n            pars = tensorlib.astensor(pars)\n            pars.requires_grad = True\n            constrained_pars = tv.stitch([fixed_values_tensor, pars])\n            constr_nll = objective(constrained_pars, data, pdf)\n            grad = torch.autograd.grad(constr_nll, pars)[0]\n            return constr_nll.detach().numpy(), grad\n\n        return tv, fixed_values_tensor, func, variable_init, variable_bounds\n'"
src/pyhf/optimize/opt_scipy.py,0,"b'""""""scipy.optimize-based Optimizer using finite differences.""""""\n\nfrom scipy.optimize import minimize\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\nclass scipy_optimizer(object):\n    """"""scipy.optimize-based Optimizer using finite differences.""""""\n\n    def __init__(self, **kwargs):\n        """"""Create scipy.optimize-based Optimizer.""""""\n        self.maxiter = kwargs.get(\'maxiter\', 100000)\n\n    def minimize(\n        self,\n        objective,\n        data,\n        pdf,\n        init_pars,\n        par_bounds,\n        fixed_vals=None,\n        return_fitted_val=False,\n    ):\n        """"""\n        Find Function Parameters that minimize the Objective.\n\n        Returns:\n            bestfit parameters\n        \n        """"""\n        fixed_vals = fixed_vals or []\n        indices = [i for i, _ in fixed_vals]\n        values = [v for _, v in fixed_vals]\n        constraints = [{\'type\': \'eq\', \'fun\': lambda v: v[indices] - values}]\n        result = minimize(\n            objective,\n            init_pars,\n            constraints=constraints,\n            method=\'SLSQP\',\n            args=(data, pdf),\n            bounds=par_bounds,\n            options=dict(maxiter=self.maxiter),\n        )\n        try:\n            assert result.success\n        except AssertionError:\n            log.error(result)\n            raise\n        if return_fitted_val:\n            return result.x, result.fun\n        return result.x\n'"
src/pyhf/optimize/opt_tflow.py,0,"b'""""""Tensorflow Optimizer Backend.""""""\nfrom .. import get_backend, default_backend\nfrom ..tensor.common import _TensorViewer\nfrom .autodiff import AutoDiffOptimizerMixin\nimport tensorflow as tf\n\n\nclass tflow_optimizer(AutoDiffOptimizerMixin):\n    """"""Tensorflow Optimizer Backend.""""""\n\n    def setup_minimize(\n        self, objective, data, pdf, init_pars, par_bounds, fixed_vals=None\n    ):\n        """"""\n        Prepare Minimization for AutoDiff-Optimizer.\n\n        Args:\n            objective: objective function\n            data: observed data\n            pdf: model\n            init_pars: initial parameters\n            par_bounds: parameter boundaries\n            fixed_vals: fixed parameter values\n\n        """"""\n        tensorlib, _ = get_backend()\n\n        all_idx = default_backend.astensor(range(pdf.config.npars), dtype=\'int\')\n        all_init = default_backend.astensor(init_pars)\n\n        fixed_vals = fixed_vals or []\n        fixed_values = [x[1] for x in fixed_vals]\n        fixed_idx = [x[0] for x in fixed_vals]\n\n        variable_idx = [x for x in all_idx if x not in fixed_idx]\n        variable_init = all_init[variable_idx]\n        variable_bounds = [par_bounds[i] for i in variable_idx]\n\n        tv = _TensorViewer([fixed_idx, variable_idx])\n\n        data = tensorlib.astensor(data)\n        fixed_values_tensor = tensorlib.astensor(fixed_values, dtype=\'float\')\n\n        def func(pars):\n            pars = tensorlib.astensor(pars)\n            with tf.GradientTape() as tape:\n                tape.watch(pars)\n                constrained_pars = tv.stitch([fixed_values_tensor, pars])\n                constr_nll = objective(constrained_pars, data, pdf)\n            grad = tape.gradient(constr_nll, pars).values\n            return constr_nll.numpy(), grad\n\n        return tv, fixed_values_tensor, func, variable_init, variable_bounds\n'"
src/pyhf/parameters/__init__.py,0,"b""from .paramsets import (\n    paramset,\n    unconstrained,\n    constrained_by_normal,\n    constrained_by_poisson,\n)\nfrom .utils import reduce_paramsets_requirements\nfrom .paramview import ParamViewer\n\n__all__ = [\n    'paramset',\n    'unconstrained',\n    'constrained_by_normal',\n    'constrained_by_poisson',\n    'reduce_paramsets_requirements',\n    'ParamViewer',\n]\n"""
src/pyhf/parameters/paramsets.py,0,"b""from .. import default_backend\n\n\nclass paramset(object):\n    def __init__(self, **kwargs):\n        self.n_parameters = kwargs.pop('n_parameters')\n        self.suggested_init = kwargs.pop('inits')\n        self.suggested_bounds = kwargs.pop('bounds')\n\n\nclass unconstrained(paramset):\n    def __init__(self, **kwargs):\n        super(unconstrained, self).__init__(**kwargs)\n        self.constrained = False\n\n\nclass constrained_paramset(paramset):\n    def __init__(self, **kwargs):\n        super(constrained_paramset, self).__init__(**kwargs)\n        self.constrained = True\n\n\nclass constrained_by_normal(constrained_paramset):\n    def __init__(self, **kwargs):\n        super(constrained_by_normal, self).__init__(**kwargs)\n        self.pdf_type = 'normal'\n        self.auxdata = kwargs.pop('auxdata')\n        sigmas = kwargs.pop('sigmas', None)\n        if sigmas:\n            self.sigmas = sigmas\n\n    def width(self):\n        try:\n            return self.sigmas\n        except AttributeError:\n            return [1.0] * self.n_parameters\n\n\nclass constrained_by_poisson(constrained_paramset):\n    def __init__(self, **kwargs):\n        super(constrained_by_poisson, self).__init__(**kwargs)\n        self.pdf_type = 'poisson'\n        self.auxdata = kwargs.pop('auxdata')\n        factors = kwargs.pop('factors')\n        if factors:\n            self.factors = factors\n\n    def width(self):\n        try:\n            return default_backend.sqrt(\n                1.0 / default_backend.astensor(self.factors)\n            ).tolist()\n        except AttributeError:\n            raise RuntimeError('need to know rate factor to compu')\n"""
src/pyhf/parameters/paramview.py,0,"b'from .. import get_backend, default_backend, events\nfrom ..tensor.common import (\n    _tensorviewer_from_slices,\n    _tensorviewer_from_sizes,\n)\n\n\ndef _tensorviewer_from_parmap(par_map, batch_size):\n    names, slices, _ = list(\n        zip(\n            *sorted(\n                [(k, v[\'slice\'], v[\'slice\'].start,) for k, v in par_map.items()],\n                key=lambda x: x[2],\n            )\n        )\n    )\n    return _tensorviewer_from_slices(slices, names, batch_size)\n\n\ndef extract_index_access(\n    baseviewer, subviewer, indices,\n):\n    tensorlib, _ = get_backend()\n\n    index_selection = []\n    stitched = None\n    indices_concatenated = None\n    if subviewer:\n        index_selection = baseviewer.split(indices, selection=subviewer.names)\n        stitched = subviewer.stitch(index_selection)\n\n        # the transpose is here so that modifier code doesn\'t have to do it\n        indices_concatenated = tensorlib.astensor(\n            tensorlib.einsum(\'ij->ji\', stitched)\n            if len(tensorlib.shape(stitched)) > 1\n            else stitched,\n            dtype=\'int\',\n        )\n    return index_selection, stitched, indices_concatenated\n\n\nclass ParamViewer(object):\n    """"""\n    Helper class to extract parameter data from possibly batched input\n    """"""\n\n    def __init__(self, shape, par_map, par_selection):\n\n        batch_size = shape[0] if len(shape) > 1 else None\n\n        fullsize = default_backend.product(default_backend.astensor(shape))\n        flat_indices = default_backend.astensor(range(int(fullsize)), dtype=\'int\')\n        self._all_indices = default_backend.reshape(flat_indices, shape)\n\n        # a tensor viewer that can split and stitch parameters\n        self.allpar_viewer = _tensorviewer_from_parmap(par_map, batch_size)\n\n        # a tensor viewer that can split and stitch the selected parameters\n        self.selected_viewer = _tensorviewer_from_sizes(\n            [\n                par_map[s][\'slice\'].stop - par_map[s][\'slice\'].start\n                for s in par_selection\n            ],\n            par_selection,\n            batch_size,\n        )\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n\n        self.all_indices = tensorlib.astensor(self._all_indices)\n        (\n            self.index_selection,\n            self.stitched,\n            self.indices_concatenated,\n        ) = extract_index_access(\n            self.allpar_viewer, self.selected_viewer, self.all_indices\n        )\n\n    def get(self, data, indices=None):\n        if not self.index_selection:\n            return None\n        tensorlib, _ = get_backend()\n        indices = indices if indices is not None else self.indices_concatenated\n        return tensorlib.gather(tensorlib.reshape(data, (-1,)), indices)\n'"
src/pyhf/parameters/utils.py,0,"b'from .. import exceptions\n\n\ndef reduce_paramsets_requirements(paramsets_requirements, paramsets_user_configs):\n    reduced_paramsets_requirements = {}\n\n    paramset_keys = [\n        \'paramset_type\',\n        \'n_parameters\',\n        \'inits\',\n        \'bounds\',\n        \'auxdata\',\n        \'factors\',\n        \'sigmas\',\n    ]\n\n    # - process all defined paramsets\n    # - determine the unique set of paramsets by param-name\n    # - if the paramset is not unique, complain\n    # - if the paramset is unique, build the paramset using the set() defined on its options\n    #   - if the value is a tuple, this came from default options so convert to a list and use it\n    #   - if the value is a list, this came from user-define options, so use it\n    for paramset_name in list(paramsets_requirements.keys()):\n        paramset_requirements = paramsets_requirements[paramset_name]\n        paramset_user_configs = paramsets_user_configs.get(paramset_name, {})\n\n        combined_paramset = {}\n        for k in paramset_keys:\n            for paramset_requirement in paramset_requirements:\n                v = paramset_requirement.get(k, \'undefined\')\n                combined_paramset.setdefault(k, set([])).add(v)\n\n            if len(combined_paramset[k]) != 1:\n                raise exceptions.InvalidNameReuse(\n                    ""Multiple values for \'{}\' ({}) were found for {}. Use unique modifier names when constructing the pdf."".format(\n                        k, list(combined_paramset[k]), paramset_name\n                    )\n                )\n\n            default_v = combined_paramset[k].pop()\n            # get user-defined-config if it exists or set to default config\n            v = paramset_user_configs.get(k, default_v)\n            # if v is a tuple, it\'s not user-configured, so convert to list\n            if v == \'undefined\':\n                continue\n            if isinstance(v, tuple):\n                v = list(v)\n            # this implies user-configured, so check that it has the right number of elements\n            elif isinstance(v, list) and default_v and len(v) != len(default_v):\n                raise exceptions.InvalidModel(\n                    \'Incorrect number of values ({}) for {} were configured by you, expected {}.\'.format(\n                        len(v), k, len(default_v)\n                    )\n                )\n            elif v and default_v == \'undefined\':\n                raise exceptions.InvalidModel(\n                    \'{} does not use the {} attribute.\'.format(paramset_name, k)\n                )\n\n            combined_paramset[k] = v\n\n        reduced_paramsets_requirements[paramset_name] = combined_paramset\n\n    return reduced_paramsets_requirements\n'"
src/pyhf/tensor/__init__.py,0,"b'from .. import exceptions\n\n\nclass _BackendRetriever(object):\n    def __getattr__(self, name):\n        if name == \'numpy_backend\':\n            from .numpy_backend import numpy_backend\n\n            assert numpy_backend\n            # for autocomplete and dir() calls\n            self.numpy_backend = numpy_backend\n            return numpy_backend\n        elif name == \'jax_backend\':\n            from .jax_backend import jax_backend\n\n            assert jax_backend\n            # for autocomplete and dir() calls\n            self.jax_backend = jax_backend\n            return jax_backend\n        elif name == \'pytorch_backend\':\n            try:\n                from .pytorch_backend import pytorch_backend\n\n                assert pytorch_backend\n                # for autocomplete and dir() calls\n                self.pytorch_backend = pytorch_backend\n                return pytorch_backend\n            except ImportError as e:\n                raise exceptions.ImportBackendError(\n                    ""There was a problem importing PyTorch. The pytorch backend cannot be used."",\n                    e,\n                )\n        elif name == \'tensorflow_backend\':\n            try:\n                from .tensorflow_backend import tensorflow_backend\n\n                assert tensorflow_backend\n                # for autocomplete and dir() calls\n                self.tensorflow_backend = tensorflow_backend\n                return tensorflow_backend\n            except ImportError as e:\n                raise exceptions.ImportBackendError(\n                    ""There was a problem importing TensorFlow. The tensorflow backend cannot be used."",\n                    e,\n                )\n\n\nBackendRetriever = _BackendRetriever()\n__all__ = [\'BackendRetriever\']\n'"
src/pyhf/tensor/common.py,2,"b'from .. import default_backend, get_backend\nfrom .. import events\n\n\nclass _TensorViewer(object):\n    def __init__(self, indices, batch_size=None, names=None):\n        # self.partition_indices has the ""target"" indices\n        # of the stitched vector. In order to  .gather()\n        # an concatennation of source arrays into the\n        # desired form, one needs to gather on the ""sorted""\n        # indices\n        # >>> source = np.asarray([9,8,7,6])\n        # >>> target = np.asarray([2,1,3,0])\n        # >>> source[target.argsort()]\n        # array([6, 8, 9, 7])\n\n        self.batch_size = batch_size\n        self.names = names\n        self._partition_indices = indices\n        _concat_indices = default_backend.astensor(\n            default_backend.concatenate(self._partition_indices), dtype=\'int\'\n        )\n        self._sorted_indices = default_backend.tolist(_concat_indices.argsort())\n\n        self._precompute()\n        events.subscribe(\'tensorlib_changed\')(self._precompute)\n\n    def _precompute(self):\n        tensorlib, _ = get_backend()\n        self.sorted_indices = tensorlib.astensor(self._sorted_indices, dtype=\'int\')\n        self.partition_indices = [\n            tensorlib.astensor(idx, dtype=\'int\') for idx in self._partition_indices\n        ]\n        if self.names:\n            self.name_map = dict(zip(self.names, self.partition_indices))\n\n    def stitch(self, data):\n        tensorlib, _ = get_backend()\n        assert len(self.partition_indices) == len(data)\n\n        data = tensorlib.concatenate(data, axis=-1)\n        if len(tensorlib.shape(data)) == 1:\n            stitched = tensorlib.gather(data, self.sorted_indices)\n        else:\n            data = tensorlib.einsum(\'...j->j...\', data)\n            stitched = tensorlib.gather(data, self.sorted_indices)\n            stitched = tensorlib.einsum(\'j...->...j\', stitched)\n        return stitched\n\n    def split(self, data, selection=None):\n        tensorlib, _ = get_backend()\n        indices = (\n            self.partition_indices\n            if selection is None\n            else [self.name_map[n] for n in selection]\n        )\n        if len(tensorlib.shape(data)) == 1:\n            return [tensorlib.gather(data, idx) for idx in indices]\n        data = tensorlib.einsum(\'...j->j...\', tensorlib.astensor(data))\n        return [\n            tensorlib.einsum(\'j...->...j\', tensorlib.gather(data, idx))\n            for idx in indices\n        ]\n\n\ndef _tensorviewer_from_slices(target_slices, names, batch_size):\n    db = default_backend\n    ranges = []\n    for sl in target_slices:\n        ranges.append(db.astensor(range(sl.start, sl.stop)))\n    if not target_slices:\n        return None\n    return _TensorViewer(ranges, names=names, batch_size=batch_size)\n\n\ndef _tensorviewer_from_sizes(sizes, names, batch_size):\n    \'\'\'\n    Creates a _Tensorviewer based on tensor sizes.\n\n    the TV will be able to stitch together data with\n    matching sizes (e.g. extracted using the slices)\n\n    tv.stitch([foo[slice1],foo[slice2],foo[slice3])\n\n    and split them again accordingly.\n    \'\'\'\n    target_slices = []\n    start = 0\n    for sz in sizes:\n        stop = start + sz\n        target_slices.append(slice(start, stop))\n        start = stop\n\n    return _tensorviewer_from_slices(target_slices, names, batch_size)\n'"
src/pyhf/tensor/jax_backend.py,35,"b'import jax.numpy as np\nfrom jax.config import config\nfrom jax.scipy.special import gammaln\nfrom jax.scipy.stats import norm, poisson\nimport numpy as onp\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\nclass _BasicPoisson(object):\n    def __init__(self, rate):\n        self.rate = rate\n\n    def sample(self, sample_shape):\n        return poisson.osp_stats.poisson(self.rate).rvs(\n            size=sample_shape + self.rate.shape\n        )\n\n    def log_prob(self, value):\n        tensorlib = jax_backend()\n        return tensorlib.poisson_logpdf(value, self.rate)\n\n\nclass _BasicNormal(object):\n    def __init__(self, loc, scale):\n        self.loc = loc\n        self.scale = scale\n\n    def sample(self, sample_shape):\n        return norm.osp_stats.norm(self.loc, self.scale).rvs(\n            size=sample_shape + self.loc.shape\n        )\n\n    def log_prob(self, value):\n        tensorlib = jax_backend()\n        return tensorlib.normal_logpdf(value, self.loc, self.scale)\n\n\nclass jax_backend(object):\n    """"""JAX backend for pyhf""""""\n\n    def __init__(self, **kwargs):\n        self.name = \'jax\'\n        config.update(\'jax_enable_x64\', True)\n\n    def clip(self, tensor_in, min_value, max_value):\n        """"""\n        Clips (limits) the tensor values to be within a specified min and max.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> a = pyhf.tensorlib.astensor([-2, -1, 0, 1, 2])\n            >>> pyhf.tensorlib.clip(a, -1, 1)\n            DeviceArray([-1., -1.,  0.,  1.,  1.], dtype=float64)\n\n        Args:\n            tensor_in (`tensor`): The input tensor object\n            min_value (`scalar` or `tensor` or `None`): The minimum value to be cliped to\n            max_value (`scalar` or `tensor` or `None`): The maximum value to be cliped to\n\n        Returns:\n            JAX ndarray: A clipped `tensor`\n        """"""\n        return np.clip(tensor_in, min_value, max_value)\n\n    def tile(self, tensor_in, repeats):\n        """"""\n        Repeat tensor data along a specific dimension\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> a = pyhf.tensorlib.astensor([[1.0], [2.0]])\n            >>> pyhf.tensorlib.tile(a, (1, 2))\n            DeviceArray([[1., 1.],\n                         [2., 2.]], dtype=float64)\n\n        Args:\n            tensor_in (`Tensor`): The tensor to be repeated\n            repeats (`Tensor`): The tuple of multipliers for each dimension\n\n        Returns:\n            JAX ndarray: The tensor with repeated axes\n        """"""\n        return np.tile(tensor_in, repeats)\n\n    def conditional(self, predicate, true_callable, false_callable):\n        """"""\n        Runs a callable conditional on the boolean value of the evaulation of a predicate\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> tensorlib = pyhf.tensorlib\n            >>> a = tensorlib.astensor([4])\n            >>> b = tensorlib.astensor([5])\n            >>> tensorlib.conditional((a < b)[0], lambda: a + b, lambda: a - b)\n            DeviceArray([9.], dtype=float64)\n\n        Args:\n            predicate (`scalar`): The logical condition that determines which callable to evaluate\n            true_callable (`callable`): The callable that is evaluated when the :code:`predicate` evalutes to :code:`true`\n            false_callable (`callable`): The callable that is evaluated when the :code:`predicate` evalutes to :code:`false`\n\n        Returns:\n            JAX ndarray: The output of the callable that was evaluated\n        """"""\n        return true_callable() if predicate else false_callable()\n\n    def tolist(self, tensor_in):\n        try:\n            return onp.asarray(tensor_in).tolist()\n        except AttributeError:\n            if isinstance(tensor_in, list):\n                return tensor_in\n            raise\n\n    def outer(self, tensor_in_1, tensor_in_2):\n        return np.outer(tensor_in_1, tensor_in_2)\n\n    def gather(self, tensor, indices):\n        return tensor[indices]\n\n    def boolean_mask(self, tensor, mask):\n        return tensor[mask]\n\n    def isfinite(self, tensor):\n        return np.isfinite(tensor)\n\n    def astensor(self, tensor_in, dtype=\'float\'):\n        """"""\n        Convert to a JAX ndarray.\n\n        Args:\n            tensor_in (Number or Tensor): Tensor object\n\n        Returns:\n            `jax.interpreters.xla.DeviceArray`: A multi-dimensional, fixed-size homogenous array.\n        """"""\n        dtypemap = {\'float\': np.float64, \'int\': np.int64, \'bool\': np.bool_}\n        try:\n            dtype = dtypemap[dtype]\n        except KeyError:\n            log.error(\'Invalid dtype: dtype must be float, int, or bool.\')\n            raise\n        tensor = np.asarray(tensor_in, dtype=dtype)\n        # Ensure non-empty tensor shape for consistency\n        try:\n            tensor.shape[0]\n        except IndexError:\n            tensor = np.reshape(tensor, [1])\n        return np.asarray(tensor, dtype=dtype)\n\n    def sum(self, tensor_in, axis=None):\n        return np.sum(tensor_in, axis=axis)\n\n    def product(self, tensor_in, axis=None):\n        return np.prod(tensor_in, axis=axis)\n\n    def abs(self, tensor):\n        return np.abs(tensor)\n\n    def ones(self, shape):\n        return np.ones(shape)\n\n    def zeros(self, shape):\n        return np.zeros(shape)\n\n    def power(self, tensor_in_1, tensor_in_2):\n        return np.power(tensor_in_1, tensor_in_2)\n\n    def sqrt(self, tensor_in):\n        return np.sqrt(tensor_in)\n\n    def divide(self, tensor_in_1, tensor_in_2):\n        return np.divide(tensor_in_1, tensor_in_2)\n\n    def log(self, tensor_in):\n        return np.log(tensor_in)\n\n    def exp(self, tensor_in):\n        return np.exp(tensor_in)\n\n    def stack(self, sequence, axis=0):\n        if axis == 0:\n            return np.stack(sequence)\n        raise RuntimeError(\'stack axis!=0\')\n\n    def where(self, mask, tensor_in_1, tensor_in_2):\n        return np.where(mask, tensor_in_1, tensor_in_2)\n\n    def concatenate(self, sequence, axis=0):\n        """"""\n        Join a sequence of arrays along an existing axis.\n\n        Args:\n            sequence: sequence of tensors\n            axis: dimension along which to concatenate\n\n        Returns:\n            output: the concatenated tensor\n\n        """"""\n        return np.concatenate(sequence, axis=axis)\n\n    def simple_broadcast(self, *args):\n        """"""\n        Broadcast a sequence of 1 dimensional arrays.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> pyhf.tensorlib.simple_broadcast(\n            ...   pyhf.tensorlib.astensor([1]),\n            ...   pyhf.tensorlib.astensor([2, 3, 4]),\n            ...   pyhf.tensorlib.astensor([5, 6, 7]))\n            [DeviceArray([1., 1., 1.], dtype=float64), DeviceArray([2., 3., 4.], dtype=float64), DeviceArray([5., 6., 7.], dtype=float64)]\n\n        Args:\n            args (Array of Tensors): Sequence of arrays\n\n        Returns:\n            list of Tensors: The sequence broadcast together.\n        """"""\n        return np.broadcast_arrays(*args)\n\n    def shape(self, tensor):\n        return tensor.shape\n\n    def reshape(self, tensor, newshape):\n        return np.reshape(tensor, newshape)\n\n    def einsum(self, subscripts, *operands):\n        """"""\n        Evaluates the Einstein summation convention on the operands.\n\n        Using the Einstein summation convention, many common multi-dimensional\n        array operations can be represented in a simple fashion. This function\n        provides a way to compute such summations. The best way to understand\n        this function is to try the examples below, which show how many common\n        NumPy functions can be implemented as calls to einsum.\n\n        Args:\n            subscripts: str, specifies the subscripts for summation\n            operands: list of array_like, these are the tensors for the operation\n\n        Returns:\n            tensor: the calculation based on the Einstein summation convention\n        """"""\n        # return contract(subscripts,*operands)\n        return np.einsum(subscripts, *operands)\n\n    def poisson_logpdf(self, n, lam):\n        n = np.asarray(n)\n        lam = np.asarray(lam)\n        return n * np.log(lam) - lam - gammaln(n + 1.0)\n\n    def poisson(self, n, lam):\n        r""""""\n        The continous approximation, using :math:`n! = \\Gamma\\left(n+1\\right)`,\n        to the probability mass function of the Poisson distribution evaluated\n        at :code:`n` given the parameter :code:`lam`.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> pyhf.tensorlib.poisson(5., 6.)\n            DeviceArray(0.16062314, dtype=float64)\n            >>> values = pyhf.tensorlib.astensor([5., 9.])\n            >>> rates = pyhf.tensorlib.astensor([6., 8.])\n            >>> pyhf.tensorlib.poisson(values, rates)\n            DeviceArray([0.16062314, 0.12407692], dtype=float64)\n\n        Args:\n            n (`tensor` or `float`): The value at which to evaluate the approximation to the Poisson distribution p.m.f.\n                                  (the observed number of events)\n            lam (`tensor` or `float`): The mean of the Poisson distribution p.m.f.\n                                    (the expected number of events)\n\n        Returns:\n            JAX ndarray: Value of the continous approximation to Poisson(n|lam)\n        """"""\n        n = np.asarray(n)\n        lam = np.asarray(lam)\n        return np.exp(n * np.log(lam) - lam - gammaln(n + 1.0))\n\n    def normal_logpdf(self, x, mu, sigma):\n        # this is much faster than\n        # norm.logpdf(x, loc=mu, scale=sigma)\n        # https://codereview.stackexchange.com/questions/69718/fastest-computation-of-n-likelihoods-on-normal-distributions\n        root2 = np.sqrt(2)\n        root2pi = np.sqrt(2 * np.pi)\n        prefactor = -np.log(sigma * root2pi)\n        summand = -np.square(np.divide((x - mu), (root2 * sigma)))\n        return prefactor + summand\n\n    # def normal_logpdf(self, x, mu, sigma):\n    #     return norm.logpdf(x, loc=mu, scale=sigma)\n\n    def normal(self, x, mu, sigma):\n        r""""""\n        The probability density function of the Normal distribution evaluated\n        at :code:`x` given parameters of mean of :code:`mu` and standard deviation\n        of :code:`sigma`.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> pyhf.tensorlib.normal(0.5, 0., 1.)\n            DeviceArray(0.35206533, dtype=float64)\n            >>> values = pyhf.tensorlib.astensor([0.5, 2.0])\n            >>> means = pyhf.tensorlib.astensor([0., 2.3])\n            >>> sigmas = pyhf.tensorlib.astensor([1., 0.8])\n            >>> pyhf.tensorlib.normal(values, means, sigmas)\n            DeviceArray([0.35206533, 0.46481887], dtype=float64)\n\n        Args:\n            x (`tensor` or `float`): The value at which to evaluate the Normal distribution p.d.f.\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            JAX ndarray: Value of Normal(x|mu, sigma)\n        """"""\n        return norm.pdf(x, loc=mu, scale=sigma)\n\n    def normal_cdf(self, x, mu=0, sigma=1):\n        """"""\n        The cumulative distribution function for the Normal distribution\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> pyhf.tensorlib.normal_cdf(0.8)\n            DeviceArray(0.7881446, dtype=float64)\n            >>> values = pyhf.tensorlib.astensor([0.8, 2.0])\n            >>> pyhf.tensorlib.normal_cdf(values)\n            DeviceArray([0.7881446 , 0.97724987], dtype=float64)\n\n        Args:\n            x (`tensor` or `float`): The observed value of the random variable to evaluate the CDF for\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            JAX ndarray: The CDF\n        """"""\n        return norm.cdf(x, loc=mu, scale=sigma)\n\n    def poisson_dist(self, rate):\n        r""""""\n        The Poisson distribution with rate parameter :code:`rate`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> rates = pyhf.tensorlib.astensor([5, 8])\n            >>> values = pyhf.tensorlib.astensor([4, 9])\n            >>> poissons = pyhf.tensorlib.poisson_dist(rates)\n            >>> poissons.log_prob(values)\n            DeviceArray([-1.74030218, -2.0868536 ], dtype=float64)\n\n        Args:\n            rate (`tensor` or `float`): The mean of the Poisson distribution (the expected number of events)\n\n        Returns:\n            Poisson distribution: The Poisson distribution class\n        """"""\n        return _BasicPoisson(rate)\n\n    def normal_dist(self, mu, sigma):\n        r""""""\n        The Normal distribution with mean :code:`mu` and standard deviation :code:`sigma`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""jax"")\n            >>> means = pyhf.tensorlib.astensor([5, 8])\n            >>> stds = pyhf.tensorlib.astensor([1, 0.5])\n            >>> values = pyhf.tensorlib.astensor([4, 9])\n            >>> normals = pyhf.tensorlib.normal_dist(means, stds)\n            >>> normals.log_prob(values)\n            DeviceArray([-1.41893853, -2.22579135], dtype=float64)\n\n        Args:\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            Normal distribution: The Normal distribution class\n\n        """"""\n        return _BasicNormal(mu, sigma)\n'"
src/pyhf/tensor/numpy_backend.py,30,"b'""""""NumPy Tensor Library Module.""""""\nimport numpy as np\nimport logging\nfrom scipy.special import gammaln\nfrom scipy.stats import norm, poisson\n\n\nlog = logging.getLogger(__name__)\n\n\nclass _BasicPoisson(object):\n    def __init__(self, rate):\n        self.rate = rate\n\n    def sample(self, sample_shape):\n        return poisson(self.rate).rvs(size=sample_shape + self.rate.shape)\n\n    def log_prob(self, value):\n        tensorlib = numpy_backend()\n        return tensorlib.poisson_logpdf(value, self.rate)\n\n\nclass _BasicNormal(object):\n    def __init__(self, loc, scale):\n        self.loc = loc\n        self.scale = scale\n\n    def sample(self, sample_shape):\n        return norm(self.loc, self.scale).rvs(size=sample_shape + self.loc.shape)\n\n    def log_prob(self, value):\n        tensorlib = numpy_backend()\n        return tensorlib.normal_logpdf(value, self.loc, self.scale)\n\n\nclass numpy_backend(object):\n    """"""NumPy backend for pyhf""""""\n\n    def __init__(self, **kwargs):\n        self.name = \'numpy\'\n\n    def clip(self, tensor_in, min_value, max_value):\n        """"""\n        Clips (limits) the tensor values to be within a specified min and max.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> a = pyhf.tensorlib.astensor([-2, -1, 0, 1, 2])\n            >>> pyhf.tensorlib.clip(a, -1, 1)\n            array([-1., -1.,  0.,  1.,  1.])\n\n        Args:\n            tensor_in (`tensor`): The input tensor object\n            min_value (`scalar` or `tensor` or `None`): The minimum value to be cliped to\n            max_value (`scalar` or `tensor` or `None`): The maximum value to be cliped to\n\n        Returns:\n            NumPy ndarray: A clipped `tensor`\n        """"""\n        return np.clip(tensor_in, min_value, max_value)\n\n    def tile(self, tensor_in, repeats):\n        """"""\n        Repeat tensor data along a specific dimension\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> a = pyhf.tensorlib.astensor([[1.0], [2.0]])\n            >>> pyhf.tensorlib.tile(a, (1, 2))\n            array([[1., 1.],\n                   [2., 2.]])\n\n        Args:\n            tensor_in (`Tensor`): The tensor to be repeated\n            repeats (`Tensor`): The tuple of multipliers for each dimension\n\n        Returns:\n            NumPy ndarray: The tensor with repeated axes\n        """"""\n        return np.tile(tensor_in, repeats)\n\n    def conditional(self, predicate, true_callable, false_callable):\n        """"""\n        Runs a callable conditional on the boolean value of the evaulation of a predicate\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> tensorlib = pyhf.tensorlib\n            >>> a = tensorlib.astensor([4])\n            >>> b = tensorlib.astensor([5])\n            >>> tensorlib.conditional((a < b)[0], lambda: a + b, lambda: a - b)\n            array([9.])\n\n        Args:\n            predicate (`scalar`): The logical condition that determines which callable to evaluate\n            true_callable (`callable`): The callable that is evaluated when the :code:`predicate` evalutes to :code:`true`\n            false_callable (`callable`): The callable that is evaluated when the :code:`predicate` evalutes to :code:`false`\n\n        Returns:\n            NumPy ndarray: The output of the callable that was evaluated\n        """"""\n        return true_callable() if predicate else false_callable()\n\n    def tolist(self, tensor_in):\n        try:\n            return tensor_in.tolist()\n        except AttributeError:\n            if isinstance(tensor_in, list):\n                return tensor_in\n            raise\n\n    def outer(self, tensor_in_1, tensor_in_2):\n        return np.outer(tensor_in_1, tensor_in_2)\n\n    def gather(self, tensor, indices):\n        return tensor[indices]\n\n    def boolean_mask(self, tensor, mask):\n        return tensor[mask]\n\n    def isfinite(self, tensor):\n        return np.isfinite(tensor)\n\n    def astensor(self, tensor_in, dtype=\'float\'):\n        """"""\n        Convert to a NumPy array.\n\n        Args:\n            tensor_in (Number or Tensor): Tensor object\n\n        Returns:\n            `numpy.ndarray`: A multi-dimensional, fixed-size homogenous array.\n        """"""\n        dtypemap = {\'float\': np.float64, \'int\': np.int64, \'bool\': np.bool_}\n        try:\n            dtype = dtypemap[dtype]\n        except KeyError:\n            log.error(\'Invalid dtype: dtype must be float, int, or bool.\')\n            raise\n\n        tensor = np.asarray(tensor_in, dtype=dtype)\n        # Ensure non-empty tensor shape for consistency\n        try:\n            tensor.shape[0]\n        except IndexError:\n            tensor = tensor.reshape(1)\n        return tensor\n\n    def sum(self, tensor_in, axis=None):\n        return np.sum(tensor_in, axis=axis)\n\n    def product(self, tensor_in, axis=None):\n        return np.product(tensor_in, axis=axis)\n\n    def abs(self, tensor):\n        return np.abs(tensor)\n\n    def ones(self, shape):\n        return np.ones(shape)\n\n    def zeros(self, shape):\n        return np.zeros(shape)\n\n    def power(self, tensor_in_1, tensor_in_2):\n        return np.power(tensor_in_1, tensor_in_2)\n\n    def sqrt(self, tensor_in):\n        return np.sqrt(tensor_in)\n\n    def divide(self, tensor_in_1, tensor_in_2):\n        return np.divide(tensor_in_1, tensor_in_2)\n\n    def log(self, tensor_in):\n        return np.log(tensor_in)\n\n    def exp(self, tensor_in):\n        return np.exp(tensor_in)\n\n    def stack(self, sequence, axis=0):\n        return np.stack(sequence, axis=axis)\n\n    def where(self, mask, tensor_in_1, tensor_in_2):\n        return np.where(mask, tensor_in_1, tensor_in_2)\n\n    def concatenate(self, sequence, axis=0):\n        """"""\n        Join a sequence of arrays along an existing axis.\n\n        Args:\n            sequence: sequence of tensors\n            axis: dimension along which to concatenate\n\n        Returns:\n            output: the concatenated tensor\n\n        """"""\n        return np.concatenate(sequence, axis=axis)\n\n    def simple_broadcast(self, *args):\n        """"""\n        Broadcast a sequence of 1 dimensional arrays.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> pyhf.tensorlib.simple_broadcast(\n            ...   pyhf.tensorlib.astensor([1]),\n            ...   pyhf.tensorlib.astensor([2, 3, 4]),\n            ...   pyhf.tensorlib.astensor([5, 6, 7]))\n            [array([1., 1., 1.]), array([2., 3., 4.]), array([5., 6., 7.])]\n\n        Args:\n            args (Array of Tensors): Sequence of arrays\n\n        Returns:\n            list of Tensors: The sequence broadcast together.\n        """"""\n        return np.broadcast_arrays(*args)\n\n    def shape(self, tensor):\n        return tensor.shape\n\n    def reshape(self, tensor, newshape):\n        return np.reshape(tensor, newshape)\n\n    def einsum(self, subscripts, *operands):\n        """"""\n        Evaluates the Einstein summation convention on the operands.\n\n        Using the Einstein summation convention, many common multi-dimensional\n        array operations can be represented in a simple fashion. This function\n        provides a way to compute such summations. The best way to understand\n        this function is to try the examples below, which show how many common\n        NumPy functions can be implemented as calls to einsum.\n\n        Args:\n            subscripts: str, specifies the subscripts for summation\n            operands: list of array_like, these are the tensors for the operation\n\n        Returns:\n            tensor: the calculation based on the Einstein summation convention\n        """"""\n        return np.einsum(subscripts, *operands)\n\n    def poisson_logpdf(self, n, lam):\n        return n * np.log(lam) - lam - gammaln(n + 1.0)\n\n    def poisson(self, n, lam):\n        r""""""\n        The continous approximation, using :math:`n! = \\Gamma\\left(n+1\\right)`,\n        to the probability mass function of the Poisson distribution evaluated\n        at :code:`n` given the parameter :code:`lam`.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> pyhf.tensorlib.poisson(5., 6.)\n            0.16062314104797995\n            >>> values = pyhf.tensorlib.astensor([5., 9.])\n            >>> rates = pyhf.tensorlib.astensor([6., 8.])\n            >>> pyhf.tensorlib.poisson(values, rates)\n            array([0.16062314, 0.12407692])\n\n        Args:\n            n (`tensor` or `float`): The value at which to evaluate the approximation to the Poisson distribution p.m.f.\n                                  (the observed number of events)\n            lam (`tensor` or `float`): The mean of the Poisson distribution p.m.f.\n                                    (the expected number of events)\n\n        Returns:\n            NumPy float: Value of the continous approximation to Poisson(n|lam)\n        """"""\n        n = np.asarray(n)\n        lam = np.asarray(lam)\n        return np.exp(n * np.log(lam) - lam - gammaln(n + 1.0))\n\n    def normal_logpdf(self, x, mu, sigma):\n        # this is much faster than\n        # norm.logpdf(x, loc=mu, scale=sigma)\n        # https://codereview.stackexchange.com/questions/69718/fastest-computation-of-n-likelihoods-on-normal-distributions\n        root2 = np.sqrt(2)\n        root2pi = np.sqrt(2 * np.pi)\n        prefactor = -np.log(sigma * root2pi)\n        summand = -np.square(np.divide((x - mu), (root2 * sigma)))\n        return prefactor + summand\n\n    # def normal_logpdf(self, x, mu, sigma):\n    #     return norm.logpdf(x, loc=mu, scale=sigma)\n\n    def normal(self, x, mu, sigma):\n        r""""""\n        The probability density function of the Normal distribution evaluated\n        at :code:`x` given parameters of mean of :code:`mu` and standard deviation\n        of :code:`sigma`.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> pyhf.tensorlib.normal(0.5, 0., 1.)\n            0.3520653267642995\n            >>> values = pyhf.tensorlib.astensor([0.5, 2.0])\n            >>> means = pyhf.tensorlib.astensor([0., 2.3])\n            >>> sigmas = pyhf.tensorlib.astensor([1., 0.8])\n            >>> pyhf.tensorlib.normal(values, means, sigmas)\n            array([0.35206533, 0.46481887])\n\n        Args:\n            x (`tensor` or `float`): The value at which to evaluate the Normal distribution p.d.f.\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            NumPy float: Value of Normal(x|mu, sigma)\n        """"""\n        return norm.pdf(x, loc=mu, scale=sigma)\n\n    def normal_cdf(self, x, mu=0, sigma=1):\n        """"""\n        The cumulative distribution function for the Normal distribution\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> pyhf.tensorlib.normal_cdf(0.8)\n            0.7881446014166034\n            >>> values = pyhf.tensorlib.astensor([0.8, 2.0])\n            >>> pyhf.tensorlib.normal_cdf(values)\n            array([0.7881446 , 0.97724987])\n\n        Args:\n            x (`tensor` or `float`): The observed value of the random variable to evaluate the CDF for\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            NumPy float: The CDF\n        """"""\n        return norm.cdf(x, loc=mu, scale=sigma)\n\n    def poisson_dist(self, rate):\n        r""""""\n        The Poisson distribution with rate parameter :code:`rate`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> rates = pyhf.tensorlib.astensor([5, 8])\n            >>> values = pyhf.tensorlib.astensor([4, 9])\n            >>> poissons = pyhf.tensorlib.poisson_dist(rates)\n            >>> poissons.log_prob(values)\n            array([-1.74030218, -2.0868536 ])\n\n        Args:\n            rate (`tensor` or `float`): The mean of the Poisson distribution (the expected number of events)\n\n        Returns:\n            Poisson distribution: The Poisson distribution class\n        """"""\n        return _BasicPoisson(rate)\n\n    def normal_dist(self, mu, sigma):\n        r""""""\n        The Normal distribution with mean :code:`mu` and standard deviation :code:`sigma`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""numpy"")\n            >>> means = pyhf.tensorlib.astensor([5, 8])\n            >>> stds = pyhf.tensorlib.astensor([1, 0.5])\n            >>> values = pyhf.tensorlib.astensor([4, 9])\n            >>> normals = pyhf.tensorlib.normal_dist(means, stds)\n            >>> normals.log_prob(values)\n            array([-1.41893853, -2.22579135])\n\n        Args:\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            Normal distribution: The Normal distribution class\n\n        """"""\n        return _BasicNormal(mu, sigma)\n'"
src/pyhf/tensor/pytorch_backend.py,0,"b'""""""PyTorch Tensor Library Module.""""""\nimport torch\nimport torch.autograd\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\nclass pytorch_backend(object):\n    """"""PyTorch backend for pyhf""""""\n\n    def __init__(self, **kwargs):\n        self.name = \'pytorch\'\n        self.dtypemap = {\n            \'float\': getattr(torch, kwargs.get(\'float\', \'float32\')),\n            \'int\': getattr(torch, kwargs.get(\'float\', \'int32\')),\n            \'bool\': torch.bool,\n        }\n\n    def clip(self, tensor_in, min_value, max_value):\n        """"""\n        Clips (limits) the tensor values to be within a specified min and max.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> a = pyhf.tensorlib.astensor([-2, -1, 0, 1, 2])\n            >>> pyhf.tensorlib.clip(a, -1, 1)\n            tensor([-1., -1.,  0.,  1.,  1.])\n\n        Args:\n            tensor_in (`tensor`): The input tensor object\n            min_value (`scalar` or `tensor` or `None`): The minimum value to be cliped to\n            max_value (`scalar` or `tensor` or `None`): The maximum value to be cliped to\n\n        Returns:\n            PyTorch tensor: A clipped `tensor`\n        """"""\n        return torch.clamp(tensor_in, min_value, max_value)\n\n    def conditional(self, predicate, true_callable, false_callable):\n        """"""\n        Runs a callable conditional on the boolean value of the evaulation of a predicate\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> tensorlib = pyhf.tensorlib\n            >>> a = tensorlib.astensor([4])\n            >>> b = tensorlib.astensor([5])\n            >>> tensorlib.conditional((a < b)[0], lambda: a + b, lambda: a - b)\n            tensor([9.])\n\n        Args:\n            predicate (`scalar`): The logical condition that determines which callable to evaluate\n            true_callable (`callable`): The callable that is evaluated when the :code:`predicate` evalutes to :code:`true`\n            false_callable (`callable`): The callable that is evaluated when the :code:`predicate` evalutes to :code:`false`\n\n        Returns:\n            PyTorch Tensor: The output of the callable that was evaluated\n        """"""\n        return true_callable() if predicate else false_callable()\n\n    def tolist(self, tensor_in):\n        try:\n            return tensor_in.data.numpy().tolist()\n        except AttributeError:\n            if isinstance(tensor_in, list):\n                return tensor_in\n            raise\n\n    def tile(self, tensor_in, repeats):\n        """"""\n        Repeat tensor data along a specific dimension\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> a = pyhf.tensorlib.astensor([[1.0], [2.0]])\n            >>> pyhf.tensorlib.tile(a, (1, 2))\n            tensor([[1., 1.],\n                    [2., 2.]])\n\n        Args:\n            tensor_in (`Tensor`): The tensor to be repeated\n            repeats (`Tensor`): The tuple of multipliers for each dimension\n\n        Returns:\n            PyTorch tensor: The tensor with repeated axes\n        """"""\n        return tensor_in.repeat(repeats)\n\n    def outer(self, tensor_in_1, tensor_in_2):\n        return torch.ger(tensor_in_1, tensor_in_2)\n\n    def astensor(self, tensor_in, dtype=\'float\'):\n        """"""\n        Convert to a PyTorch Tensor.\n\n        Args:\n            tensor_in (Number or Tensor): Tensor object\n\n        Returns:\n            torch.Tensor: A multi-dimensional matrix containing elements of a single data type.\n        """"""\n        try:\n            dtype = self.dtypemap[dtype]\n        except KeyError:\n            log.error(\'Invalid dtype: dtype must be float, int, or bool.\')\n            raise\n\n        tensor = torch.as_tensor(tensor_in, dtype=dtype)\n        # Ensure non-empty tensor shape for consistency\n        try:\n            tensor.shape[0]\n        except IndexError:\n            tensor = tensor.expand(1)\n        return tensor\n\n    def gather(self, tensor, indices):\n        return tensor[indices.type(torch.LongTensor)]\n\n    def boolean_mask(self, tensor, mask):\n        return torch.masked_select(tensor, mask)\n\n    def reshape(self, tensor, newshape):\n        return torch.reshape(tensor, newshape)\n\n    def shape(self, tensor):\n        return tuple(map(int, tensor.shape))\n\n    def sum(self, tensor_in, axis=None):\n        return (\n            torch.sum(tensor_in)\n            if (axis is None or tensor_in.shape == torch.Size([]))\n            else torch.sum(tensor_in, axis)\n        )\n\n    def product(self, tensor_in, axis=None):\n        return torch.prod(tensor_in) if axis is None else torch.prod(tensor_in, axis)\n\n    def abs(self, tensor):\n        return torch.abs(tensor)\n\n    def ones(self, shape):\n        return torch.ones(shape, dtype=self.dtypemap[\'float\'])\n\n    def zeros(self, shape):\n        return torch.zeros(shape, dtype=self.dtypemap[\'float\'])\n\n    def power(self, tensor_in_1, tensor_in_2):\n        return torch.pow(tensor_in_1, tensor_in_2)\n\n    def sqrt(self, tensor_in):\n        return torch.sqrt(tensor_in)\n\n    def divide(self, tensor_in_1, tensor_in_2):\n        return torch.div(tensor_in_1, tensor_in_2)\n\n    def log(self, tensor_in):\n        return torch.log(tensor_in)\n\n    def exp(self, tensor_in):\n        return torch.exp(tensor_in)\n\n    def stack(self, sequence, axis=0):\n        return torch.stack(sequence, dim=axis)\n\n    def where(self, mask, tensor_in_1, tensor_in_2):\n        return torch.where(mask, tensor_in_1, tensor_in_2)\n\n    def concatenate(self, sequence, axis=0):\n        """"""\n        Join a sequence of arrays along an existing axis.\n\n        Args:\n            sequence: sequence of tensors\n            axis: dimension along which to concatenate\n\n        Returns:\n            output: the concatenated tensor\n\n        """"""\n        return torch.cat(sequence, dim=axis)\n\n    def isfinite(self, tensor):\n        return torch.isfinite(tensor)\n\n    def simple_broadcast(self, *args):\n        """"""\n        Broadcast a sequence of 1 dimensional arrays.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> pyhf.tensorlib.simple_broadcast(\n            ...   pyhf.tensorlib.astensor([1]),\n            ...   pyhf.tensorlib.astensor([2, 3, 4]),\n            ...   pyhf.tensorlib.astensor([5, 6, 7]))\n            [tensor([1., 1., 1.]), tensor([2., 3., 4.]), tensor([5., 6., 7.])]\n\n        Args:\n            args (Array of Tensors): Sequence of arrays\n\n        Returns:\n            list of Tensors: The sequence broadcast together.\n        """"""\n\n        max_dim = max(map(len, args))\n        try:\n            assert not [arg for arg in args if 1 < len(arg) < max_dim]\n        except AssertionError as error:\n            log.error(\n                \'ERROR: The arguments must be of compatible size: 1 or %i\', max_dim\n            )\n            raise error\n\n        broadcast = [arg if len(arg) > 1 else arg.expand(max_dim) for arg in args]\n        return broadcast\n\n    def einsum(self, subscripts, *operands):\n        """"""\n        This function provides a way of computing multilinear expressions (i.e.\n        sums of products) using the Einstein summation convention.\n\n        Args:\n            subscripts: str, specifies the subscripts for summation\n            operands: list of array_like, these are the tensors for the operation\n\n        Returns:\n            tensor: the calculation based on the Einstein summation convention\n        """"""\n        return torch.einsum(subscripts, operands)\n\n    def poisson_logpdf(self, n, lam):\n        return torch.distributions.Poisson(lam).log_prob(n)\n\n    def poisson(self, n, lam):\n        r""""""\n        The continous approximation, using :math:`n! = \\Gamma\\left(n+1\\right)`,\n        to the probability mass function of the Poisson distribution evaluated\n        at :code:`n` given the parameter :code:`lam`.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> pyhf.tensorlib.poisson(5., 6.)\n            tensor(0.1606)\n            >>> values = pyhf.tensorlib.astensor([5., 9.])\n            >>> rates = pyhf.tensorlib.astensor([6., 8.])\n            >>> pyhf.tensorlib.poisson(values, rates)\n            tensor([0.1606, 0.1241])\n\n        Args:\n            n (`tensor` or `float`): The value at which to evaluate the approximation to the Poisson distribution p.m.f.\n                                  (the observed number of events)\n            lam (`tensor` or `float`): The mean of the Poisson distribution p.m.f.\n                                    (the expected number of events)\n\n        Returns:\n            PyTorch FloatTensor: Value of the continous approximation to Poisson(n|lam)\n        """"""\n        return torch.exp(torch.distributions.Poisson(lam).log_prob(n))\n\n    def normal_logpdf(self, x, mu, sigma):\n        normal = torch.distributions.Normal(mu, sigma)\n        return normal.log_prob(x)\n\n    def normal(self, x, mu, sigma):\n        r""""""\n        The probability density function of the Normal distribution evaluated\n        at :code:`x` given parameters of mean of :code:`mu` and standard deviation\n        of :code:`sigma`.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> pyhf.tensorlib.normal(0.5, 0., 1.)\n            tensor(0.3521)\n            >>> values = pyhf.tensorlib.astensor([0.5, 2.0])\n            >>> means = pyhf.tensorlib.astensor([0., 2.3])\n            >>> sigmas = pyhf.tensorlib.astensor([1., 0.8])\n            >>> pyhf.tensorlib.normal(values, means, sigmas)\n            tensor([0.3521, 0.4648])\n\n        Args:\n            x (`tensor` or `float`): The value at which to evaluate the Normal distribution p.d.f.\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            PyTorch FloatTensor: Value of Normal(x|mu, sigma)\n        """"""\n        normal = torch.distributions.Normal(mu, sigma)\n        return self.exp(normal.log_prob(x))\n\n    def normal_cdf(self, x, mu=0.0, sigma=1.0):\n        """"""\n        The cumulative distribution function for the Normal distribution\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> pyhf.tensorlib.normal_cdf(0.8)\n            tensor(0.7881)\n            >>> values = pyhf.tensorlib.astensor([0.8, 2.0])\n            >>> pyhf.tensorlib.normal_cdf(values)\n            tensor([0.7881, 0.9772])\n\n        Args:\n            x (`tensor` or `float`): The observed value of the random variable to evaluate the CDF for\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            PyTorch FloatTensor: The CDF\n        """"""\n        normal = torch.distributions.Normal(mu, sigma)\n        return normal.cdf(x)\n\n    def poisson_dist(self, rate):\n        r""""""\n        The Poisson distribution with rate parameter :code:`rate`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> rates = pyhf.tensorlib.astensor([5, 8])\n            >>> values = pyhf.tensorlib.astensor([4, 9])\n            >>> poissons = pyhf.tensorlib.poisson_dist(rates)\n            >>> poissons.log_prob(values)\n            tensor([-1.7403, -2.0869])\n\n        Args:\n            rate (`tensor` or `float`): The mean of the Poisson distribution (the expected number of events)\n\n        Returns:\n            PyTorch Poisson distribution: The Poisson distribution class\n\n        """"""\n        return torch.distributions.Poisson(rate)\n\n    def normal_dist(self, mu, sigma):\n        r""""""\n        The Normal distribution with mean :code:`mu` and standard deviation :code:`sigma`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""pytorch"")\n            >>> means = pyhf.tensorlib.astensor([5, 8])\n            >>> stds = pyhf.tensorlib.astensor([1, 0.5])\n            >>> values = pyhf.tensorlib.astensor([4, 9])\n            >>> normals = pyhf.tensorlib.normal_dist(means, stds)\n            >>> normals.log_prob(values)\n            tensor([-1.4189, -2.2258])\n\n        Args:\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            PyTorch Normal distribution: The Normal distribution class\n\n        """"""\n        return torch.distributions.Normal(mu, sigma)\n'"
src/pyhf/tensor/tensorflow_backend.py,0,"b'""""""Tensorflow Tensor Library Module.""""""\nimport logging\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\nlog = logging.getLogger(__name__)\n\n\nclass tensorflow_backend(object):\n    """"""TensorFlow backend for pyhf""""""\n\n    def __init__(self, **kwargs):\n        self.name = \'tensorflow\'\n        self.dtypemap = {\n            \'float\': getattr(tf, kwargs.get(\'float\', \'float32\')),\n            \'int\': getattr(tf, kwargs.get(\'int\', \'int32\')),\n            \'bool\': tf.bool,\n        }\n\n    def clip(self, tensor_in, min_value, max_value):\n        """"""\n        Clips (limits) the tensor values to be within a specified min and max.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> a = pyhf.tensorlib.astensor([-2, -1, 0, 1, 2])\n            >>> t = pyhf.tensorlib.clip(a, -1, 1)\n            >>> print(t)\n            tf.Tensor([-1. -1.  0.  1.  1.], shape=(5,), dtype=float32)\n\n        Args:\n            tensor_in (`tensor`): The input tensor object\n            min_value (`scalar` or `tensor` or `None`): The minimum value to be cliped to\n            max_value (`scalar` or `tensor` or `None`): The maximum value to be cliped to\n\n        Returns:\n            TensorFlow Tensor: A clipped `tensor`\n\n        """"""\n        if min_value is None:\n            min_value = tf.reduce_min(tensor_in)\n        if max_value is None:\n            max_value = tf.reduce_max(tensor_in)\n        return tf.clip_by_value(tensor_in, min_value, max_value)\n\n    def tile(self, tensor_in, repeats):\n        """"""\n        Repeat tensor data along a specific dimension\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> a = pyhf.tensorlib.astensor([[1.0], [2.0]])\n            >>> t = pyhf.tensorlib.tile(a, (1, 2))\n            >>> print(t)\n            tf.Tensor(\n            [[1. 1.]\n             [2. 2.]], shape=(2, 2), dtype=float32)\n\n        Args:\n            tensor_in (`Tensor`): The tensor to be repeated\n            repeats (`Tensor`): The tuple of multipliers for each dimension\n\n        Returns:\n            TensorFlow Tensor: The tensor with repeated axes\n\n        """"""\n        return tf.tile(tensor_in, repeats)\n\n    def conditional(self, predicate, true_callable, false_callable):\n        """"""\n        Runs a callable conditional on the boolean value of the evaulation of a predicate\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> tensorlib = pyhf.tensorlib\n            >>> a = tensorlib.astensor([4])\n            >>> b = tensorlib.astensor([5])\n            >>> t = tensorlib.conditional((a < b)[0], lambda: a + b, lambda: a - b)\n            >>> print(t)\n            tf.Tensor([9.], shape=(1,), dtype=float32)\n\n        Args:\n            predicate (`scalar`): The logical condition that determines which callable to evaluate\n            true_callable (`callable`): The callable that is evaluated when the :code:`predicate` evalutes to :code:`true`\n            false_callable (`callable`): The callable that is evaluated when the :code:`predicate` evalutes to :code:`false`\n\n        Returns:\n            TensorFlow Tensor: The output of the callable that was evaluated\n\n        """"""\n        return tf.cond(predicate, true_callable, false_callable)\n\n    def tolist(self, tensor_in):\n        try:\n            return tensor_in.numpy().tolist()\n        except AttributeError:\n            if isinstance(tensor_in, list):\n                return tensor_in\n            raise\n\n    def outer(self, tensor_in_1, tensor_in_2):\n        tensor_in_1 = (\n            tensor_in_1\n            if tensor_in_1.dtype != tf.bool\n            else tf.cast(tensor_in_1, tf.float32)\n        )\n        tensor_in_1 = (\n            tensor_in_1\n            if tensor_in_2.dtype != tf.bool\n            else tf.cast(tensor_in_2, tf.float32)\n        )\n        return tf.einsum(\'i,j->ij\', tensor_in_1, tensor_in_2)\n\n    def gather(self, tensor, indices):\n        return tf.compat.v2.gather(tensor, indices)\n\n    def boolean_mask(self, tensor, mask):\n        return tf.boolean_mask(tensor, mask)\n\n    def isfinite(self, tensor):\n        return tf.math.is_finite(tensor)\n\n    def astensor(self, tensor_in, dtype=\'float\'):\n        """"""\n        Convert to a TensorFlow Tensor.\n\n        Args:\n            tensor_in (Number or Tensor): Tensor object\n\n        Returns:\n            `tf.Tensor`: A symbolic handle to one of the outputs of a `tf.Operation`.\n\n        """"""\n        try:\n            dtype = self.dtypemap[dtype]\n        except KeyError:\n            log.error(\'Invalid dtype: dtype must be float, int, or bool.\')\n            raise\n\n        tensor = tensor_in\n        # If already a tensor then done\n        try:\n            # Use a tensor attribute that isn\'t meaningless when eager execution is enabled\n            tensor.device\n        except AttributeError:\n            tensor = tf.convert_to_tensor(tensor_in)\n            # Ensure non-empty tensor shape for consistency\n            try:\n                tensor.shape[0]\n            except IndexError:\n                tensor = tf.reshape(tensor, [1])\n        if tensor.dtype is not dtype:\n            tensor = tf.cast(tensor, dtype)\n        return tensor\n\n    def sum(self, tensor_in, axis=None):\n        return (\n            tf.reduce_sum(tensor_in)\n            if (axis is None or tensor_in.shape == tf.TensorShape([]))\n            else tf.reduce_sum(tensor_in, axis)\n        )\n\n    def product(self, tensor_in, axis=None):\n        return (\n            tf.reduce_prod(tensor_in)\n            if axis is None\n            else tf.reduce_prod(tensor_in, axis)\n        )\n\n    def abs(self, tensor):\n        return tf.abs(tensor)\n\n    def ones(self, shape):\n        return tf.ones(shape, dtype=self.dtypemap[\'float\'])\n\n    def zeros(self, shape):\n        return tf.zeros(shape, dtype=self.dtypemap[\'float\'])\n\n    def power(self, tensor_in_1, tensor_in_2):\n        return tf.pow(tensor_in_1, tensor_in_2)\n\n    def sqrt(self, tensor_in):\n        return tf.sqrt(tensor_in)\n\n    def shape(self, tensor):\n        return tuple(map(int, tensor.shape))\n\n    def reshape(self, tensor, newshape):\n        return tf.reshape(tensor, newshape)\n\n    def divide(self, tensor_in_1, tensor_in_2):\n        return tf.divide(tensor_in_1, tensor_in_2)\n\n    def log(self, tensor_in):\n        return tf.math.log(tensor_in)\n\n    def exp(self, tensor_in):\n        return tf.exp(tensor_in)\n\n    def stack(self, sequence, axis=0):\n        return tf.stack(sequence, axis=axis)\n\n    def where(self, mask, tensor_in_1, tensor_in_2):\n        """"""\n        Apply a boolean selection mask to the elements of the input tensors.\n\n        Example:\n\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> t = pyhf.tensorlib.where(\n            ...     pyhf.tensorlib.astensor([1, 0, 1], dtype=\'bool\'),\n            ...     pyhf.tensorlib.astensor([1, 1, 1]),\n            ...     pyhf.tensorlib.astensor([2, 2, 2]),\n            ... )\n            >>> print(t)\n            tf.Tensor([1. 2. 1.], shape=(3,), dtype=float32)\n\n        Args:\n            mask (bool): Boolean mask (boolean or tensor object of booleans)\n            tensor_in_1 (Tensor): Tensor object\n            tensor_in_2 (Tensor): Tensor object\n\n        Returns:\n            TensorFlow Tensor: The result of the mask being applied to the tensors.\n\n        """"""\n        return tf.where(mask, tensor_in_1, tensor_in_2)\n\n    def concatenate(self, sequence, axis=0):\n        """"""\n        Join a sequence of arrays along an existing axis.\n\n        Args:\n            sequence: sequence of tensors\n            axis: dimension along which to concatenate\n\n        Returns:\n            output: the concatenated tensor\n\n        """"""\n        return tf.concat(sequence, axis=axis)\n\n    def simple_broadcast(self, *args):\n        """"""\n        Broadcast a sequence of 1 dimensional arrays.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> b = pyhf.tensorlib.simple_broadcast(\n            ...   pyhf.tensorlib.astensor([1]),\n            ...   pyhf.tensorlib.astensor([2, 3, 4]),\n            ...   pyhf.tensorlib.astensor([5, 6, 7]))\n            >>> print([str(t) for t in b]) # doctest: +NORMALIZE_WHITESPACE\n            [\'tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\',\n             \'tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\',\n             \'tf.Tensor([5. 6. 7.], shape=(3,), dtype=float32)\']\n\n        Args:\n            args (Array of Tensors): Sequence of arrays\n\n        Returns:\n            list of Tensors: The sequence broadcast together.\n\n        """"""\n        max_dim = max(map(lambda arg: arg.shape[0], args))\n        try:\n            assert not [arg for arg in args if 1 < arg.shape[0] < max_dim]\n        except AssertionError as error:\n            log.error(\n                \'ERROR: The arguments must be of compatible size: 1 or %i\', max_dim\n            )\n            raise error\n\n        broadcast = [\n            arg\n            if arg.shape[0] > 1\n            else tf.tile(tf.slice(arg, [0], [1]), tf.stack([max_dim]))\n            for arg in args\n        ]\n        return broadcast\n\n    def einsum(self, subscripts, *operands):\n        """"""\n        A generalized contraction between tensors of arbitrary dimension.\n\n        This function returns a tensor whose elements are defined by equation,\n        which is written in a shorthand form inspired by the Einstein summation\n        convention.\n\n        Args:\n            subscripts: str, specifies the subscripts for summation\n            operands: list of array_like, these are the tensors for the operation\n\n        Returns:\n            TensorFlow Tensor: the calculation based on the Einstein summation convention\n        """"""\n        return tf.einsum(subscripts, *operands)\n\n    def poisson_logpdf(self, n, lam):\n        r""""""\n        The log of the continous approximation, using :math:`n! = \\Gamma\\left(n+1\\right)`,\n        to the probability mass function of the Poisson distribution evaluated\n        at :code:`n` given the parameter :code:`lam`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> t = pyhf.tensorlib.poisson_logpdf(5., 6.)\n            >>> print(t)\n            tf.Tensor(-1.8286943, shape=(), dtype=float32)\n            >>> values = pyhf.tensorlib.astensor([5., 9.])\n            >>> rates = pyhf.tensorlib.astensor([6., 8.])\n            >>> t = pyhf.tensorlib.poisson_logpdf(values, rates)\n            >>> print(t)\n            tf.Tensor([-1.8286943 -2.086854 ], shape=(2,), dtype=float32)\n\n        Args:\n            n (`tensor` or `float`): The value at which to evaluate the approximation to the Poisson distribution p.m.f.\n                                  (the observed number of events)\n            lam (`tensor` or `float`): The mean of the Poisson distribution p.m.f.\n                                    (the expected number of events)\n\n        Returns:\n            TensorFlow Tensor: Value of the continous approximation to log(Poisson(n|lam))\n        """"""\n        return tfp.distributions.Poisson(lam).log_prob(n)\n\n    def poisson(self, n, lam):\n        r""""""\n        The continous approximation, using :math:`n! = \\Gamma\\left(n+1\\right)`,\n        to the probability mass function of the Poisson distribution evaluated\n        at :code:`n` given the parameter :code:`lam`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> t = pyhf.tensorlib.poisson(5., 6.)\n            >>> print(t)\n            tf.Tensor(0.16062315, shape=(), dtype=float32)\n            >>> values = pyhf.tensorlib.astensor([5., 9.])\n            >>> rates = pyhf.tensorlib.astensor([6., 8.])\n            >>> t = pyhf.tensorlib.poisson(values, rates)\n            >>> print(t)\n            tf.Tensor([0.16062315 0.12407687], shape=(2,), dtype=float32)\n\n        Args:\n            n (`tensor` or `float`): The value at which to evaluate the approximation to the Poisson distribution p.m.f.\n                                  (the observed number of events)\n            lam (`tensor` or `float`): The mean of the Poisson distribution p.m.f.\n                                    (the expected number of events)\n\n        Returns:\n            TensorFlow Tensor: Value of the continous approximation to Poisson(n|lam)\n        """"""\n        return tf.exp(tfp.distributions.Poisson(lam).log_prob(n))\n\n    def normal_logpdf(self, x, mu, sigma):\n        r""""""\n        The log of the probability density function of the Normal distribution evaluated\n        at :code:`x` given parameters of mean of :code:`mu` and standard deviation\n        of :code:`sigma`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> t = pyhf.tensorlib.normal_logpdf(0.5, 0., 1.)\n            >>> print(t)\n            tf.Tensor(-1.0439385, shape=(), dtype=float32)\n            >>> values = pyhf.tensorlib.astensor([0.5, 2.0])\n            >>> means = pyhf.tensorlib.astensor([0., 2.3])\n            >>> sigmas = pyhf.tensorlib.astensor([1., 0.8])\n            >>> t = pyhf.tensorlib.normal_logpdf(values, means, sigmas)\n            >>> print(t)\n            tf.Tensor([-1.0439385 -0.7661075], shape=(2,), dtype=float32)\n\n        Args:\n            x (`tensor` or `float`): The value at which to evaluate the Normal distribution p.d.f.\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            TensorFlow Tensor: Value of log(Normal(x|mu, sigma))\n        """"""\n        normal = tfp.distributions.Normal(mu, sigma)\n        return normal.log_prob(x)\n\n    def normal(self, x, mu, sigma):\n        r""""""\n        The probability density function of the Normal distribution evaluated\n        at :code:`x` given parameters of mean of :code:`mu` and standard deviation\n        of :code:`sigma`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> t = pyhf.tensorlib.normal(0.5, 0., 1.)\n            >>> print(t)\n            tf.Tensor(0.35206532, shape=(), dtype=float32)\n            >>> values = pyhf.tensorlib.astensor([0.5, 2.0])\n            >>> means = pyhf.tensorlib.astensor([0., 2.3])\n            >>> sigmas = pyhf.tensorlib.astensor([1., 0.8])\n            >>> t = pyhf.tensorlib.normal(values, means, sigmas)\n            >>> print(t)\n            tf.Tensor([0.35206532 0.46481887], shape=(2,), dtype=float32)\n\n        Args:\n            x (`tensor` or `float`): The value at which to evaluate the Normal distribution p.d.f.\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            TensorFlow Tensor: Value of Normal(x|mu, sigma)\n        """"""\n        normal = tfp.distributions.Normal(mu, sigma)\n        return normal.prob(x)\n\n    def normal_cdf(self, x, mu=0.0, sigma=1):\n        """"""\n        Compute the value of cumulative distribution function for the Normal distribution at x.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> t = pyhf.tensorlib.normal_cdf(0.8)\n            >>> print(t)\n            tf.Tensor(0.7881446, shape=(), dtype=float32)\n            >>> values = pyhf.tensorlib.astensor([0.8, 2.0])\n            >>> t = pyhf.tensorlib.normal_cdf(values)\n            >>> print(t)\n            tf.Tensor([0.7881446  0.97724986], shape=(2,), dtype=float32)\n\n        Args:\n            x (`tensor` or `float`): The observed value of the random variable to evaluate the CDF for\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            TensorFlow Tensor: The CDF\n        """"""\n        normal = tfp.distributions.Normal(\n            self.astensor(mu, dtype=\'float\')[0], self.astensor(sigma, dtype=\'float\')[0],\n        )\n        return normal.cdf(x)\n\n    def poisson_dist(self, rate):\n        r""""""\n        Construct a Poisson distribution with rate parameter :code:`rate`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> rates = pyhf.tensorlib.astensor([5, 8])\n            >>> values = pyhf.tensorlib.astensor([4, 9])\n            >>> poissons = pyhf.tensorlib.poisson_dist(rates)\n            >>> t = poissons.log_prob(values)\n            >>> print(t)\n            tf.Tensor([-1.7403021 -2.086854 ], shape=(2,), dtype=float32)\n\n        Args:\n            rate (`tensor` or `float`): The mean of the Poisson distribution (the expected number of events)\n\n        Returns:\n            TensorFlow Probability Poisson distribution: The Poisson distribution class\n\n        """"""\n        return tfp.distributions.Poisson(rate)\n\n    def normal_dist(self, mu, sigma):\n        r""""""\n        Construct a Normal distribution with mean :code:`mu` and standard deviation :code:`sigma`.\n\n        Example:\n            >>> import pyhf\n            >>> pyhf.set_backend(""tensorflow"")\n            >>> means = pyhf.tensorlib.astensor([5, 8])\n            >>> stds = pyhf.tensorlib.astensor([1, 0.5])\n            >>> values = pyhf.tensorlib.astensor([4, 9])\n            >>> normals = pyhf.tensorlib.normal_dist(means, stds)\n            >>> t = normals.log_prob(values)\n            >>> print(t)\n            tf.Tensor([-1.4189385 -2.2257915], shape=(2,), dtype=float32)\n\n        Args:\n            mu (`tensor` or `float`): The mean of the Normal distribution\n            sigma (`tensor` or `float`): The standard deviation of the Normal distribution\n\n        Returns:\n            TensorFlow Probability Normal distribution: The Normal distribution class\n\n        """"""\n        return tfp.distributions.Normal(mu, sigma)\n'"
src/pyhf/contrib/viz/__init__.py,0,"b'""""""Visualizations of pyhf models and results.""""""\n'"
src/pyhf/contrib/viz/brazil.py,2,"b'""""""Brazil Band Plots.""""""\nimport numpy as np\n\n\ndef plot_results(ax, mutests, tests, test_size=0.05):\n    """"""Plot a series of hypothesis tests for various POI values.""""""\n    cls_obs = np.array([test[0] for test in tests]).flatten()\n    cls_exp = [np.array([test[1][i] for test in tests]).flatten() for i in range(5)]\n    ax.plot(mutests, cls_obs, c=\'black\')\n    for idx, color in zip(range(5), 5 * [\'black\']):\n        ax.plot(\n            mutests, cls_exp[idx], c=color, linestyle=\'dotted\' if idx != 2 else \'dashed\'\n        )\n    ax.fill_between(mutests, cls_exp[0], cls_exp[-1], facecolor=\'yellow\')\n    ax.fill_between(mutests, cls_exp[1], cls_exp[-2], facecolor=\'green\')\n    ax.plot(mutests, [test_size] * len(mutests), c=\'red\')\n    ax.set_ylim(0, 1)\n'"
