file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport sys\nimport os\nfrom setuptools import setup, Extension\nfrom setuptools import find_packages\n\n#########\n# settings\n#########\n\nproject_var_name = ""pandas_streaming""\nversionPython = ""%s.%s"" % (sys.version_info.major, sys.version_info.minor)\npath = ""Lib/site-packages/"" + project_var_name\nreadme = \'README.rst\'\nhistory = ""HISTORY.rst""\nrequirements = None\n\nKEYWORDS = project_var_name + \', Xavier Dupr\xc3\xa9\'\nDESCRIPTION = """"""Streaming operations with pandas.""""""\nCLASSIFIERS = [\n    \'Programming Language :: Python :: 3\',\n    \'Intended Audience :: Developers\',\n    \'Topic :: Scientific/Engineering\',\n    \'Topic :: Education\',\n    \'License :: OSI Approved :: MIT License\',\n    \'Development Status :: 5 - Production/Stable\'\n]\n\n#######\n# data\n#######\n\nhere = os.path.dirname(__file__)\npackages = find_packages()\npackage_dir = {k: os.path.join(here, k.replace(""."", ""/"")) for k in packages}\npackage_data = {}\n\n############\n# functions\n############\n\n\ndef ask_help():\n    return ""--help"" in sys.argv or ""--help-commands"" in sys.argv\n\n\ndef is_local():\n    file = os.path.abspath(__file__).replace(""\\\\"", ""/"").lower()\n    if ""/temp/"" in file and ""pip-"" in file:\n        return False\n    from pyquickhelper.pycode.setup_helper import available_commands_list\n    return available_commands_list(sys.argv)\n\n\ndef verbose():\n    print(""---------------------------------"")\n    print(""package_dir ="", package_dir)\n    print(""packages    ="", packages)\n    print(""package_data="", package_data)\n    print(""current     ="", os.path.abspath(os.getcwd()))\n    print(""---------------------------------"")\n\n##########\n# version\n##########\n\n\nif is_local() and not ask_help():\n    def write_version():\n        from pyquickhelper.pycode import write_version_for_setup\n        return write_version_for_setup(__file__)\n\n    write_version()\n\n    versiontxt = os.path.join(os.path.dirname(__file__), ""version.txt"")\n    if os.path.exists(versiontxt):\n        with open(versiontxt, ""r"") as f:\n            lines = f.readlines()\n        subversion = ""."" + lines[0].strip(""\\r\\n "")\n        if subversion == "".0"":\n            raise Exception(""Git version is wrong: \'{0}\'."".format(subversion))\n    else:\n        raise FileNotFoundError(versiontxt)\nelse:\n    # when the module is installed, no commit number is displayed\n    subversion = """"\n\nif ""upload"" in sys.argv and not subversion and not ask_help():\n    # avoid uploading with a wrong subversion number\n    raise Exception(\n        ""Git version is empty, cannot upload, is_local()={0}"".format(is_local()))\n\n##############\n# common part\n##############\n\nif os.path.exists(readme):\n    with open(readme, ""r"", encoding=\'utf-8-sig\') as f:\n        long_description = f.read()\nelse:\n    long_description = """"\nif os.path.exists(history):\n    with open(history, ""r"", encoding=\'utf-8-sig\') as f:\n        long_description += f.read()\n\nif ""--verbose"" in sys.argv:\n    verbose()\n\nif is_local():\n    import pyquickhelper\n    logging_function = pyquickhelper.get_fLOG()\n    logging_function(OutputPrint=True)\n    must_build, run_build_ext = pyquickhelper.get_insetup_functions()\n\n    if must_build():\n        out = run_build_ext(__file__)\n        print(out)\n\n    if ""build_sphinx"" in sys.argv and not sys.platform.startswith(""win""):\n        # There is an issue with matplotlib and notebook gallery on linux\n        # _tkinter.TclError: no display name and no $DISPLAY environment variable\n        import matplotlib\n        matplotlib.use(\'agg\')\n\n    from pyquickhelper.pycode import process_standard_options_for_setup\n    r = process_standard_options_for_setup(\n        sys.argv, __file__, project_var_name,\n        unittest_modules=[""pyquickhelper""],\n        additional_notebook_path=[""pyquickhelper"", ""jyquickhelper""],\n        additional_local_path=[""pyquickhelper"", ""jyquickhelper""],\n        requirements=[""pyquickhelper"", ""jyquickhelper""],\n        layout=[""html""], github_owner=\'sdpython\',\n        add_htmlhelp=sys.platform.startswith(""win""),\n        coverage_options=dict(omit=[""*exclude*.py""]),\n        fLOG=logging_function, covtoken=(""14c7930a-a5c0-405d-a22f-3f9c6feaf0bc"", ""\'_UT_37_std\' in outfile""))\n    if not r and not ({""bdist_msi"", ""sdist"",\n                       ""bdist_wheel"", ""publish"", ""publish_doc"", ""register"",\n                       ""upload_docs"", ""bdist_wininst"", ""build_ext""} & set(sys.argv)):\n        raise Exception(""unable to interpret command line: "" + str(sys.argv))\nelse:\n    r = False\n\nif ask_help():\n    from pyquickhelper.pycode import process_standard_options_for_setup_help\n    process_standard_options_for_setup_help(sys.argv)\n\nif not r:\n    if len(sys.argv) in (1, 2) and sys.argv[-1] in (""--help-commands"",):\n        from pyquickhelper.pycode import process_standard_options_for_setup_help\n        process_standard_options_for_setup_help(sys.argv)\n    root = os.path.abspath(os.path.dirname(__file__))\n    if sys.platform.startswith(""win""):\n        extra_compile_args = None\n    else:\n        extra_compile_args = [\'-std=c++11\']\n\n    from pyquickhelper.pycode import clean_readme\n    from pandas_streaming import __version__ as sversion\n    long_description = clean_readme(long_description)\n\n    setup(\n        name=project_var_name,\n        version=sversion,\n        author=\'Xavier Dupr\xc3\xa9\',\n        author_email=\'xavier.dupre@gmail.com\',\n        license=""MIT"",\n        url=""http://www.xavierdupre.fr/app/pandas_streaming/helpsphinx/index.html"",\n        download_url=""https://github.com/sdpython/pandas_streaming/"",\n        description=DESCRIPTION,\n        long_description=long_description,\n        keywords=KEYWORDS,\n        classifiers=CLASSIFIERS,\n        packages=packages,\n        package_dir=package_dir,\n        package_data=package_data,\n        setup_requires=[""pyquickhelper>=1.9""],\n        install_requires=[\'numpy\', \'pandas\', \'ijson\'],\n    )\n'"
pandas_streaming/__init__.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Module *pandas_streaming*.\nProcesses large datasets with :epkg:`pandas` by\nreimplementing streeaming versions of\n:epkg:`pandas` functionalites.\n""""""\n\n__version__ = ""0.2.160""\n__author__ = ""Xavier Dupr\xc3\xa9""\n__github__ = ""https://github.com/sdpython/pandas_streaming""\n__url__ = ""http://www.xavierdupre.fr/app/pandas_streaming/helpsphinx/index.html""\n__license__ = ""MIT License""\n__blog__ = """"""\n<?xml version=""1.0"" encoding=""UTF-8""?>\n<opml version=""1.0"">\n    <head>\n        <title>blog</title>\n    </head>\n    <body>\n        <outline text=""pandas_streaming""\n            title=""pandas_streaming""\n            type=""rss""\n            xmlUrl=""http://www.xavierdupre.fr/app/pandas_streaming/helpsphinx/_downloads/rss.xml""\n            htmlUrl=""http://www.xavierdupre.fr/app/pandas_streaming/helpsphinx/blog/main_0000.html"" />\n    </body>\n</opml>\n""""""\n\n\ndef check(log=False):\n    """"""\n    Checks the library is working.\n    It raises an exception.\n    If you want to disable the logs:\n\n    @param      log     if True, display information, otherwise\n    @return             0 or exception\n    """"""\n    return True\n\n\ndef _setup_hook(use_print=False):\n    """"""\n    if this function is added to the module,\n    the help automation and unit tests call it first before\n    anything goes on as an initialization step.\n    """"""\n    # we can check many things, needed module\n    # any others things before unit tests are started\n    if use_print:\n        print(""Success: _setup_hook"")\n'"
_unittests/ut_df/test_connex_split.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=4s)\n""""""\nimport unittest\nimport pandas\nfrom pyquickhelper.loghelper import fLOG\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pandas_streaming.df import dataframe_shuffle, train_test_split_weights, train_test_connex_split\n\n\nclass TestConnexSplit(ExtTestCase):\n\n    def test_shuffle(self):\n        df = pandas.DataFrame([dict(a=1, b=""e"", c=5.6, ind=""a1""),\n                               dict(a=2, b=""f"", c=5.7, ind=""a2""),\n                               dict(a=4, b=""g"", c=5.8, ind=""a3""),\n                               dict(a=8, b=""h"", c=5.9, ind=""a4""),\n                               dict(a=16, b=""i"", c=6.2, ind=""a5"")])\n        shuffled = dataframe_shuffle(df, random_state=0)\n        sorted_ = shuffled.sort_values(\'a\')\n        self.assertEqualDataFrame(df, sorted_)\n\n        df2 = df.set_index(\'ind\')\n        shuffled = dataframe_shuffle(df2, random_state=0)\n        sorted_ = shuffled.sort_values(\'a\')\n        self.assertEqualDataFrame(df2, sorted_)\n\n        df2 = df.set_index([\'ind\', \'c\'])\n        shuffled = dataframe_shuffle(df2, random_state=0)\n        sorted_ = shuffled.sort_values(\'a\')\n        self.assertEqualDataFrame(df2, sorted_)\n\n    def test_split_weights_errors(self):\n        df = pandas.DataFrame([dict(a=1, b=""e"", c=1),\n                               dict(a=2, b=""f"", c=1),\n                               dict(a=4, b=""g"", c=1),\n                               dict(a=8, b=""h"", c=1),\n                               dict(a=12, b=""h"", c=1),\n                               dict(a=16, b=""i"", c=1)])\n\n        train, test = train_test_split_weights(df, train_size=0.5, weights=\'c\')\n        self.assertTrue(train is not None)\n        self.assertTrue(test is not None)\n        self.assertRaise(lambda: train_test_split_weights(\n            df, test_size=0.5, weights=[0.5, 0.5]), ValueError, \'Dimension\')\n        self.assertRaise(lambda: train_test_split_weights(\n            df, test_size=0), ValueError, \'null\')\n        self.assertRaise(lambda: train_test_split_weights(\n            df, test_size=0, weights=\'c\'), ValueError, \'null\')\n\n    def test_split_weights(self):\n        df = pandas.DataFrame([dict(a=1, b=""e"", c=1),\n                               dict(a=2, b=""f"", c=1),\n                               dict(a=4, b=""g"", c=1),\n                               dict(a=8, b=""h"", c=1),\n                               dict(a=12, b=""h"", c=1),\n                               dict(a=16, b=""i"", c=1)])\n\n        train, test = train_test_split_weights(df, test_size=0.5)\n        self.assertEqual(train.shape[1], test.shape[1])\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n\n        train, test = train_test_split_weights(df, test_size=0.5, weights=\'c\')\n        self.assertEqual(train.shape[1], test.shape[1])\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n\n        train, test = train_test_split_weights(\n            df, test_size=0.5, weights=df[\'c\'])\n        self.assertEqual(train.shape[1], test.shape[1])\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n\n        df = pandas.DataFrame([dict(a=1, b=""e"", c=1),\n                               dict(a=2, b=""f"", c=2),\n                               dict(a=4, b=""g"", c=3),\n                               dict(a=8, b=""h"", c=1),\n                               dict(a=12, b=""h"", c=2),\n                               dict(a=16, b=""i"", c=3)])\n\n        train, test = train_test_split_weights(df, test_size=0.5, weights=\'c\',\n                                               fail_imbalanced=0.4)\n        self.assertEqual(train.shape[1], test.shape[1])\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n        w1, w2 = train[\'c\'].sum(), test[\'c\'].sum()\n        delta = abs(w1 - w2) / (w1 + w2)\n        self.assertGreater(0.4, delta)\n\n    def test_split_connex(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        df = pandas.DataFrame([dict(user=""UA"", prod=""PA"", card=""C1""),\n                               dict(user=""UA"", prod=""PB"", card=""C1""),\n                               dict(user=""UB"", prod=""PC"", card=""C2""),\n                               dict(user=""UB"", prod=""PD"", card=""C2""),\n                               dict(user=""UC"", prod=""PE"", card=""C3""),\n                               dict(user=""UC"", prod=""PF"", card=""C4""),\n                               dict(user=""UD"", prod=""PG"", card=""C5""),\n                               ])\n\n        train, test = train_test_connex_split(  # pylint: disable=W0632\n            df, test_size=0.5, groups=[\'user\', \'prod\', \'card\'],\n            fail_imbalanced=0.4, fLOG=fLOG)\n\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n        for col in [\'user\', \'prod\', \'card\']:\n            s1 = set(train[col])\n            s2 = set(test[col])\n            if s1 & s2:\n                raise Exception(\n                    \'Non empty intersection {0} & {1}\\n{2}\\n{3}\'.format(s1, s2, train, test))\n\n        df[\'connex\'] = \'ole\'\n        train, test = train_test_connex_split(  # pylint: disable=W0632\n            df, test_size=0.5, groups=[\'user\', \'prod\', \'card\'],\n            fail_imbalanced=0.4, fLOG=fLOG)\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n\n    def test_split_connex2(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        df = pandas.DataFrame([dict(user=""UA"", prod=""PAA"", card=""C1""),\n                               dict(user=""UA"", prod=""PB"", card=""C1""),\n                               dict(user=""UB"", prod=""PC"", card=""C2""),\n                               dict(user=""UB"", prod=""PD"", card=""C2""),\n                               dict(user=""UC"", prod=""PAA"", card=""C3""),\n                               dict(user=""UC"", prod=""PF"", card=""C4""),\n                               dict(user=""UD"", prod=""PG"", card=""C5""),\n                               ])\n\n        train_test_connex_split(df, test_size=0.5, groups=[\'user\', \'prod\', \'card\'],\n                                fail_imbalanced=0.5, fLOG=fLOG, return_cnx=True)\n        train, test, stats = train_test_connex_split(df, test_size=0.5,\n                                                     groups=[\n                                                         \'user\', \'prod\', \'card\'],\n                                                     fail_imbalanced=0.5, fLOG=fLOG,\n                                                     return_cnx=True, random_state=0)\n\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n        for col in [\'user\', \'prod\', \'card\']:\n            s1 = set(train[col])\n            s2 = set(test[col])\n            if s1 & s2:\n                rows = []\n                for k, v in sorted(stats[0].items()):\n                    rows.append(""{0}={1}"".format(k, v))\n                raise Exception(\n                    \'Non empty intersection {0} & {1}\\n{2}\\n{3}\\n{4}\'.format(s1, s2, train, test, ""\\n"".join(rows)))\n\n    def test_split_connex_missing(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        df = pandas.DataFrame([dict(user=""UA"", prod=""PAA"", card=""C1""),\n                               dict(user=""UA"", prod=""PB"", card=""C1""),\n                               dict(user=""UB"", prod=""PC"", card=""C2""),\n                               dict(user=""UB"", prod=""PD"", card=""C2""),\n                               dict(user=""UC"", prod=""PAA"", card=""C3""),\n                               dict(user=""UC"", card=""C4""),\n                               dict(user=""UD"", prod=""PG""),\n                               ])\n\n        train, test, stats = train_test_connex_split(df, test_size=0.5,\n                                                     groups=[\n                                                         \'user\', \'prod\', \'card\'],\n                                                     fail_imbalanced=0.4, fLOG=fLOG,\n                                                     return_cnx=True, random_state=0)\n\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n        for col in [\'user\', \'prod\', \'card\']:\n            s1 = set(train[col])\n            s2 = set(test[col])\n            if s1 & s2:\n                rows = []\n                for k, v in sorted(stats[0].items()):\n                    rows.append(""{0}={1}"".format(k, v))\n                raise Exception(\n                    \'Non empty intersection {0} & {1}\\n{2}\\n{3}\\n{4}\'.format(s1, s2, train, test, ""\\n"".join(rows)))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_df/test_connex_split_big.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=30s)\n""""""\nimport os\nimport unittest\nfrom collections import Counter\nimport pandas\nfrom pyquickhelper.loghelper import fLOG\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pandas_streaming.df import train_test_connex_split\n\n\nclass TestConnexSplitBig(ExtTestCase):\n\n    def test_connex_big(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        data = os.path.join(os.path.dirname(__file__), ""data"")\n        name = os.path.join(data, ""buggy_hash.csv"")\n        df = pandas.read_csv(name, sep=""\\t"", encoding=""utf-8"")\n        train, test, stats = train_test_connex_split(df, fLOG=fLOG,\n                                                     groups=[\n                                                         ""cart_id"", ""mail"", ""product_id""],\n                                                     fail_imbalanced=0.9, return_cnx=True)\n        self.assertGreater(train.shape[0], 0)\n        self.assertGreater(test.shape[0], 0)\n        elements = stats[1][\'connex\']\n        counts = Counter(elements)\n        nbc = len(counts)\n        maxi = max(counts.values())\n        self.assertEqual(nbc, 5376)\n        self.assertEqual(maxi, 14181)\n\n    def test_connex_big_approx(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        data = os.path.join(os.path.dirname(__file__), ""data"")\n        name = os.path.join(data, ""buggy_hash.csv"")\n        df = pandas.read_csv(name, sep=""\\t"", encoding=""utf-8"")\n        train, test, stats = train_test_connex_split(df, fLOG=fLOG,\n                                                     groups=[\n                                                         ""cart_id"", ""mail"", ""product_id""],\n                                                     stop_if_bigger=0.05, return_cnx=True,\n                                                     keep_balance=0.8)\n        self.assertGreater(train.shape[0], 0)\n        self.assertGreater(test.shape[0], 0)\n        elements = stats[1][\'connex\']\n        counts = Counter(elements)\n        nbc = len(counts)\n        maxi = max(counts.values())\n        self.assertGreater(nbc, 5376)\n        self.assertLesser(maxi, 14181)\n\n    def test_connex_big_approx_must(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        data = os.path.join(os.path.dirname(__file__), ""data"")\n        name = os.path.join(data, ""buggy_hash.csv"")\n        df = pandas.read_csv(name, sep=""\\t"", encoding=""utf-8"")\n        train, test, stats = train_test_connex_split(df, fLOG=fLOG,\n                                                     groups=[\n                                                         ""cart_id"", ""mail"", ""product_id""],\n                                                     stop_if_bigger=0.05, return_cnx=True,\n                                                     keep_balance=0.8, must_groups=[""product_id""])\n        self.assertGreater(train.shape[0], 0)\n        self.assertGreater(test.shape[0], 0)\n        elements = stats[1][\'connex\']\n        counts = Counter(elements)\n        nbc = len(counts)\n        maxi = max(counts.values())\n        self.assertGreater(nbc, 5376)\n        self.assertLesser(maxi, 14181)\n        train_ids = set(train.product_id)\n        test_ids = set(test.product_id)\n        inter = train_ids & test_ids\n        self.assertEqual(len(inter), 0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_df/test_connex_split_cat.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=4s)\n""""""\nimport unittest\nfrom collections import Counter\nimport pandas\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pandas_streaming.df import train_test_apart_stratify\n\n\nclass TestConnexSplitCat(ExtTestCase):\n\n    def test_cat_strat(self):\n        df = pandas.DataFrame([dict(a=1, b=""e""),\n                               dict(a=2, b=""e""),\n                               dict(a=4, b=""f""),\n                               dict(a=8, b=""f""),\n                               dict(a=32, b=""f""),\n                               dict(a=16, b=""f"")])\n\n        train, test = train_test_apart_stratify(\n            df, group=""a"", stratify=""b"", test_size=0.5)\n        self.assertEqual(train.shape[1], test.shape[1])\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n        c1 = Counter(train[""b""])\n        c2 = Counter(train[""b""])\n        self.assertEqual(c1, c2)\n\n        self.assertRaise(lambda: train_test_apart_stratify(df, group=None, stratify=""b"", test_size=0.5),\n                         ValueError)\n        self.assertRaise(lambda: train_test_apart_stratify(df, group=""b"", test_size=0.5),\n                         ValueError)\n\n    def test_cat_strat_multi(self):\n        df = pandas.DataFrame([dict(a=1, b=""e""),\n                               dict(a=1, b=""f""),\n                               dict(a=2, b=""e""),\n                               dict(a=2, b=""f""),\n                               ])\n\n        train, test = train_test_apart_stratify(\n            df, group=""a"", stratify=""b"", test_size=0.5)\n        self.assertEqual(train.shape[1], test.shape[1])\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n        c1 = Counter(train[""b""])\n        c2 = Counter(train[""b""])\n        self.assertEqual(c1, c2)\n        self.assertEqual(len(set(train[\'a\'])), 1)\n        self.assertEqual(len(set(test[\'a\'])), 1)\n        self.assertTrue(set(train[\'a\']) != set(test[\'a\']))\n\n    def test_cat_strat_multi_force(self):\n        df = pandas.DataFrame([dict(a=1, b=""e""),\n                               dict(a=1, b=""f""),\n                               dict(a=2, b=""e""),\n                               dict(a=2, b=""f""),\n                               ])\n\n        train, test = train_test_apart_stratify(\n            df, group=""a"", stratify=""b"", test_size=0.1, force=True)\n        self.assertEqual(train.shape[1], test.shape[1])\n        self.assertEqual(train.shape[0] + test.shape[0], df.shape[0])\n        c1 = Counter(train[""b""])\n        c2 = Counter(train[""b""])\n        self.assertEqual(c1, c2)\n        self.assertEqual(len(set(train[\'a\'])), 1)\n        self.assertEqual(len(set(test[\'a\'])), 1)\n        self.assertTrue(set(train[\'a\']) != set(test[\'a\']))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_df/test_dataframe_helpers.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=4s)\n""""""\nimport os\nimport unittest\nimport numpy\nimport pandas\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pandas_streaming.df import dataframe_hash_columns\n\n\nclass TestDataFrameHelpers(ExtTestCase):\n\n    def test_hash_columns(self):\n        df = pandas.DataFrame([dict(a=1, b=""e"", c=5.6, ind=""a1"", ai=1),\n                               dict(b=""f"", c=5.7, ind=""a2"", ai=2),\n                               dict(a=4, b=""g"", ind=""a3"", ai=3),\n                               dict(a=8, b=""h"", c=5.9, ai=4),\n                               dict(a=16, b=""i"", c=6.2, ind=""a5"", ai=5)])\n        df2 = dataframe_hash_columns(df)\n        self.assertEqual(df2.shape, df.shape)\n        for j in range(df.shape[1]):\n            self.assertEqual(df.columns[j], df2.columns[j])\n            self.assertEqual(df.dtypes[j], df2.dtypes[j])\n            for i in range(df.shape[0]):\n                v1 = df.iloc[i, j]\n                v2 = df2.iloc[i, j]\n                if isinstance(v1, float):\n                    if numpy.isnan(v1):\n                        self.assertTrue(numpy.isnan(v2))\n                    else:\n                        self.assertEqual(type(v1), type(v2))\n                else:\n                    self.assertEqual(type(v1), type(v2))\n\n    def test_hash_columns_bigger(self):\n        data = os.path.join(os.path.dirname(__file__), ""data"")\n        name = os.path.join(data, ""buggy_hash.csv"")\n        df = pandas.read_csv(name, sep=""\\t"", encoding=""utf-8"")\n        df2 = dataframe_hash_columns(df)\n        self.assertEqual(df.shape, df2.shape)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_df/test_dataframe_helpers_simple.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=4s)\n""""""\nimport unittest\nimport pandas\nimport numpy\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pandas_streaming.df import dataframe_unfold\nfrom pandas_streaming.df.dataframe_helpers import hash_int, hash_str, hash_float\n\n\nclass TestDataFrameHelpersSimple(ExtTestCase):\n\n    def test_unfold(self):\n        df = pandas.DataFrame([dict(a=1, b=""e,f""),\n                               dict(a=2, b=""g""),\n                               dict(a=3)])\n        df2 = dataframe_unfold(df, ""b"")\n\n        exp = pandas.DataFrame([dict(a=1, b=""e,f"", b_unfold=""e""),\n                                dict(a=1, b=""e,f"", b_unfold=""f""),\n                                dict(a=2, b=""g"", b_unfold=""g""),\n                                dict(a=3)])\n        self.assertEqualDataFrame(df2, exp)\n\n        # fold\n        folded = df2.groupby(\'a\').apply(lambda row: \',\'.join(\n            row[\'b_unfold\'].dropna()) if len(row[\'b_unfold\'].dropna()) > 0 else numpy.nan)\n        bf = folded.reset_index(drop=False)\n        bf.columns = [\'a\', \'b\']\n        self.assertEqualDataFrame(df, bf)\n\n    def test_hash_except(self):\n        self.assertRaise(lambda: hash_int(0.1, 3),\n                         ValueError, ""numpy.nan expected"")\n        r = hash_int(numpy.nan, 3)\n        self.assertTrue(numpy.isnan(r))\n\n        self.assertRaise(lambda: hash_str(0.1, 3),\n                         ValueError, ""numpy.nan expected"")\n        r = hash_str(numpy.nan, 3)\n        self.assertTrue(numpy.isnan(r))\n\n        self.assertRaise(lambda: hash_float(""0.1"", 3), TypeError, ""isnan"")\n        r = hash_float(numpy.nan, 3)\n        self.assertTrue(numpy.isnan(r))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_df/test_dataframe_io.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=4s)\n""""""\nimport os\nimport unittest\nimport io\nimport zipfile\nimport numpy\nimport pandas\nfrom pyquickhelper.pycode import ExtTestCase, get_temp_folder\nfrom pandas_streaming.df import to_zip, read_zip\n\n\nclass TestDataFrameIO(ExtTestCase):\n\n    def test_zip_dataframe(self):\n        df = pandas.DataFrame([dict(a=1, b=""e\xc3\xa9"", c=5.6, ind=""a1"", ai=1),\n                               dict(b=""f"", c=5.7, ind=""a2"", ai=2),\n                               dict(a=4, b=""g"", ind=""a3"", ai=3),\n                               dict(a=8, b=""h"", c=5.9, ai=4),\n                               dict(a=16, b=""i"", c=6.2, ind=""a5"", ai=5)])\n\n        temp = get_temp_folder(__file__, ""temp_zip"")\n        name = os.path.join(temp, ""df.zip"")\n        to_zip(df, name, encoding=""utf-8"", index=False)\n        df2 = read_zip(name, encoding=""utf-8"")\n        self.assertEqualDataFrame(df, df2)\n\n        st = io.BytesIO()\n        zp = zipfile.ZipFile(st, \'w\')\n        to_zip(df, zp, encoding=""utf-8"", index=False)\n        zp.close()\n\n        st = io.BytesIO(st.getvalue())\n        zp = zipfile.ZipFile(st, \'r\')\n        df3 = read_zip(zp, encoding=\'utf-8\')\n        zp.close()\n        self.assertEqualDataFrame(df, df3)\n\n    def test_zip_numpy(self):\n        df = numpy.zeros((3, 4))\n        df[2, 3] = 1\n\n        temp = get_temp_folder(__file__, ""temp_zip"")\n        name = os.path.join(temp, ""df.zip"")\n        to_zip(df, name, ""arr.npy"")\n        df2 = read_zip(name, ""arr.npy"")\n        self.assertEqualArray(df, df2)\n\n        st = io.BytesIO()\n        zp = zipfile.ZipFile(st, \'w\')\n        to_zip(df, zp, ""arr.npy"")\n        zp.close()\n\n        st = io.BytesIO(st.getvalue())\n        zp = zipfile.ZipFile(st, \'r\')\n        df3 = read_zip(zp, ""arr.npy"")\n        zp.close()\n        self.assertEqualArray(df, df3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_df/test_dataframe_io_helpers.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=4s)\n""""""\nimport unittest\nfrom io import StringIO, BytesIO\nfrom json import loads\nimport pandas\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pandas_streaming.df.dataframe_io_helpers import enumerate_json_items, JsonPerRowsStream\nfrom pandas_streaming.df import StreamingDataFrame\n\n\nclass TestDataFrameIOHelpers(ExtTestCase):\n\n    text_json = b\'\'\'\n        [\n        {\n            ""glossary"": {\n                ""title"": ""example glossary"",\n                ""GlossDiv"": {\n                    ""title"": ""S"",\n                    ""GlossList"": [{\n                        ""GlossEntry"": {\n                            ""ID"": ""SGML"",\n                            ""SortAs"": ""SGML"",\n                            ""GlossTerm"": ""Standard Generalized Markup Language"",\n                            ""Acronym"": ""SGML"",\n                            ""Abbrev"": ""ISO 8879:1986"",\n                            ""GlossDef"": {\n                                ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",\n                                ""GlossSeeAlso"": [""GML"", ""XML""]\n                            },\n                            ""GlossSee"": ""markup""\n                        }\n                    }]\n                }\n            }\n        },\n        {\n            ""glossary"": {\n                ""title"": ""example glossary"",\n                ""GlossDiv"": {\n                    ""title"": ""X"",\n                    ""GlossList"": {\n                        ""GlossEntry"": [{\n                            ""ID"": ""SGML"",\n                            ""SortAs"": ""SGML"",\n                            ""GlossTerm"": ""Standard Generalized Markup Language"",\n                            ""Acronym"": ""SGML"",\n                            ""Abbrev"": ""ISO 8879:1986"",\n                            ""GlossDef"": {\n                                ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",\n                                ""GlossSeeAlso"": [""GML"", ""XML""]\n                            },\n                            ""GlossSee"": ""markup""\n                        }]\n                    }\n                }\n            }\n        }\n        ]\n    \'\'\'\n    text_json_exp = [\n        {\n            ""glossary"": {\n                ""title"": ""example glossary"",\n                ""GlossDiv"": {\n                    ""title"": ""S"",\n                    ""GlossList"": [{\n                        ""GlossEntry"": {\n                            ""ID"": ""SGML"",\n                            ""SortAs"": ""SGML"",\n                            ""GlossTerm"": ""Standard Generalized Markup Language"",\n                            ""Acronym"": ""SGML"",\n                            ""Abbrev"": ""ISO 8879:1986"",\n                            ""GlossDef"": {\n                                ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",\n                                ""GlossSeeAlso"": [""GML"", ""XML""]\n                            },\n                            ""GlossSee"": ""markup""\n                        }\n                    }]\n                }\n            }\n        },\n        {\n            ""glossary"": {\n                ""title"": ""example glossary"",\n                ""GlossDiv"": {\n                    ""title"": ""X"",\n                    ""GlossList"": {\n                        ""GlossEntry"": [{\n                            ""ID"": ""SGML"",\n                            ""SortAs"": ""SGML"",\n                            ""GlossTerm"": ""Standard Generalized Markup Language"",\n                            ""Acronym"": ""SGML"",\n                            ""Abbrev"": ""ISO 8879:1986"",\n                            ""GlossDef"": {\n                                ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",\n                                ""GlossSeeAlso"": [""GML"", ""XML""]\n                            },\n                            ""GlossSee"": ""markup""\n                        }]\n                    }\n                }\n            }\n        }\n    ]\n\n    def test_enumerate_json_items(self):\n        items = list(enumerate_json_items(TestDataFrameIOHelpers.text_json))\n        self.assertEqual(TestDataFrameIOHelpers.text_json_exp, items)\n        items = list(enumerate_json_items(\n            BytesIO(TestDataFrameIOHelpers.text_json)))\n        self.assertEqual(TestDataFrameIOHelpers.text_json_exp, items)\n\n    def test_read_json_raw(self):\n        data = [{\'id\': 1, \'name\': {\'first\': \'Coleen\', \'last\': \'Volk\'}},\n                {\'name\': {\'given\': \'Mose\', \'family\': \'Regner\'}},\n                {\'id\': 2, \'name\': \'FayeRaker\'}]\n        exp = """"""[{""id"":1.0,""name"":null,""name.family"":null,""name.first"":""Coleen"",""name.given"":null,""name.last"":""Volk""},\n                {""id"":null,""name"":null,""name.family"":""Regner"",""name.first"":null,""name.given"":""Mose"",""name.last"":null},\n                {""id"":2.0,""name"":""FayeRaker"",""name.family"":null,""name.first"":null,\n                ""name.given"":null,""name.last"":null}]"""""".replace("" "", """").replace(""\\n"", """")\n        self.assertRaise(lambda: StreamingDataFrame.read_json(\n            data), NotImplementedError)\n        it = StreamingDataFrame.read_json(data, flatten=True)\n        dfs = list(it)\n        self.assertEqual(len(dfs), 1)\n        js = dfs[0].to_json(orient=\'records\')\n        js_read = loads(js)\n        js_exp = loads(exp)\n        self.assertEqual(js_exp, js_read)\n\n    def test_pandas_json_chunksize(self):\n        jsonl = \'\'\'{""a"": 1, ""b"": 2}\n                   {""a"": 3, ""b"": 4}\'\'\'\n        df = pandas.read_json(jsonl, lines=True)\n        idf = pandas.read_json(jsonl, lines=True, chunksize=2)\n        ldf = list(idf)\n        self.assertEqualDataFrame(df, ldf[0])\n\n    def test_read_json_rows(self):\n        data = \'\'\'{""a"": 1, ""b"": 2}\n                  {""a"": 3, ""b"": 4}\'\'\'\n        it = StreamingDataFrame.read_json(StringIO(data), lines=True)\n        dfs = list(it)\n        self.assertEqual(len(dfs), 1)\n        js = dfs[0].to_json(orient=\'records\')\n        self.assertEqual(js, \'[{""a"":1,""b"":2},{""a"":3,""b"":4}]\')\n\n    def test_read_json_rows2(self):\n        data = b\'\'\'{""a"": 1, ""b"": 2}\n                  {""a"": 3, ""b"": 4}\'\'\'\n        it = StreamingDataFrame.read_json(BytesIO(data), lines=""stream"")\n        dfs = list(it)\n        self.assertEqual(len(dfs), 1)\n        js = dfs[0].to_json(orient=\'records\')\n        self.assertEqual(js, \'[{""a"":1,""b"":2},{""a"":3,""b"":4}]\')\n\n    def test_read_json_ijson(self):\n        it = StreamingDataFrame.read_json(\n            BytesIO(TestDataFrameIOHelpers.text_json))\n        dfs = list(it)\n        self.assertEqual(len(dfs), 1)\n        js = dfs[0].to_json(orient=\'records\', lines=True)\n        jsjson = loads(\'[\' + js.replace(""\\n"", "","") + \']\')\n        self.assertEqual(jsjson, TestDataFrameIOHelpers.text_json_exp)\n\n    def test_read_json_stream(self):\n        text = """"""{\'a\': 1}\n        {\'b\': 1, \'a\', \'r\'}""""""\n        st = JsonPerRowsStream(StringIO(text))\n        val = st.getvalue().replace("" "", """").replace(""\\n"", """")\n        exp = ""[{\'a\':1},{\'b\':1,\'a\',\'r\'}]""\n        self.assertEqual(val, exp)\n\n        st = JsonPerRowsStream(StringIO(text))\n        t = st.read(0)\n        t = st.read(1)\n        c = """"\n        while t:\n            c += t\n            t = st.read(1)\n        val = c.replace("" "", """").replace(""\\n"", """")\n        self.assertEqual(val, exp)\n\n    def test_enumerate_json_items_lines(self):\n        data = b\'\'\'{""a"": 1, ""b"": 2}\n                   {""a"": 3, ""b"": 4}\'\'\'\n        items = list(enumerate_json_items(data, lines=True))\n        self.assertEqual(items, [{\'a\': 1, \'b\': 2}, {\'a\': 3, \'b\': 4}])\n\n    def test_read_json_file2(self):\n        data = b\'\'\'{""a"": {""c"": 1}, ""b"": [2, 3]}\n                   {""a"": {""a"": 3}, ""b"": [4, 5, ""r""]}\'\'\'\n\n        obj1 = list(enumerate_json_items(\n            BytesIO(data), flatten=False, lines=True))\n        obj2 = list(enumerate_json_items(\n            BytesIO(data), flatten=True, lines=True))\n        self.assertNotEqual(obj1, obj2)\n        self.assertEqual(obj2, [{\'a_c\': 1, \'b_0\': 2, \'b_1\': 3},\n                                {\'a_a\': 3, \'b_0\': 4, \'b_1\': 5, \'b_2\': \'r\'}])\n\n        it = StreamingDataFrame.read_json(\n            BytesIO(data), lines=""stream"", flatten=True)\n        dfs = list(it)\n        self.assertEqual(list(sorted(dfs[0].columns)), [\n                         \'a_a\', \'a_c\', \'b_0\', \'b_1\', \'b_2\'])\n        self.assertEqual(len(dfs), 1)\n        js = dfs[0].to_json(orient=\'records\', lines=True)\n        jsjson = loads(\'[\' + js.replace(""\\n"", "","") + \']\')\n        exp = [{\'a_a\': None, \'a_c\': 1.0, \'b_0\': 2, \'b_1\': 3, \'b_2\': None},\n               {\'a_a\': 3.0, \'a_c\': None, \'b_0\': 4, \'b_1\': 5, \'b_2\': \'r\'}]\n        self.assertEqual(jsjson, exp)\n\n    def test_read_json_item(self):\n        text = TestDataFrameIOHelpers.text_json\n        st = JsonPerRowsStream(BytesIO(text))\n        res = []\n        while True:\n            n = st.read()\n            if not n:\n                break\n            res.append(n)\n        self.assertGreater(len(res), 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_df/test_pandas_groupbynan.py,0,"b'""""""\n@brief      test log(time=1s)\n""""""\nimport unittest\nimport pandas\nimport numpy\nfrom scipy.sparse.linalg import lsqr as sparse_lsqr\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pandas_streaming.df import pandas_groupby_nan, numpy_types\n\n\nclass TestPandasHelper(ExtTestCase):\n\n    def test_pandas_groupbynan(self):\n        self.assertTrue(sparse_lsqr is not None)\n        types = [(int, -10), (float, -20.2), (str, ""e""),\n                 (bytes, bytes(""a"", ""ascii""))]\n        skip = (numpy.bool_, numpy.complex64, numpy.complex128)\n        types += [(_, _(5)) for _ in numpy_types() if _ not in skip]\n\n        for ty in types:\n            data = [{""this"": ""cst"", ""type"": ""tt1="" + str(ty[0]), ""value"": ty[1]},\n                    {""this"": ""cst"", ""type"": ""tt2="" +\n                        str(ty[0]), ""value"": ty[1]},\n                    {""this"": ""cst"", ""type"": ""row_for_nan""}]\n            df = pandas.DataFrame(data)\n            gr = pandas_groupby_nan(df, ""value"")\n            co = gr.sum()\n            li = list(co[""value""])\n            try:\n                self.assertIsInstance(li[-1], float)\n            except AssertionError as e:\n                raise AssertionError(""Issue with {0}"".format(ty)) from e\n            try:\n                self.assertTrue(numpy.isnan(li[-1]))\n            except AssertionError as e:\n                raise AssertionError(\n                    ""Issue with value {0}\\n--df--\\n{1}\\n--co--\\n{2}"".format(li, df, co)) from e\n\n        for ty in types:\n            data = [{""this"": ""cst"", ""type"": ""tt1="" + str(ty[0]), ""value"": ty[1]},\n                    {""this"": ""cst"", ""type"": ""tt2="" +\n                        str(ty[0]), ""value"": ty[1]},\n                    {""this"": ""cst"", ""type"": ""row_for_nan""}]\n            df = pandas.DataFrame(data)\n            try:\n                gr = pandas_groupby_nan(df, (""value"", ""this""))\n                t = True\n                raise Exception(""---"")\n            except TypeError:\n                t = False\n            if t:\n                co = gr.sum()\n                li = list(co[""value""])\n                self.assertIsInstance(li[-1], float)\n                self.assertTrue(numpy.isnan(li[-1]))\n            try:\n                gr = pandas_groupby_nan(df, [""value"", ""this""])\n                t = True\n            except (TypeError, NotImplementedError):\n                t = False\n\n            if t:\n                co = gr.sum()\n                li = list(co[""value""])\n                self.assertEqual(len(li), 2)\n\n    def test_pandas_groupbynan_tuple(self):\n        data = [dict(a=""a"", b=""b"", c=""c"", n=1), dict(\n            b=""b"", n=2), dict(a=""a"", n=3), dict(c=""c"", n=4)]\n        df = pandas.DataFrame(data)\n        gr = df.groupby([""a"", ""b"", ""c""]).sum()\n        self.assertEqual(gr.shape, (1, 1))\n\n        for nanback in [True, False]:\n            try:\n                gr2_ = pandas_groupby_nan(\n                    df, [""a"", ""b"", ""c""], nanback=nanback, suffix=""NAN"")\n            except NotImplementedError:\n                continue\n            gr2 = gr2_.sum().sort_values(""n"")\n            self.assertEqual(gr2.shape, (4, 4))\n            d = gr2.to_dict(""records"")\n            self.assertEqual(d[0][""a""], ""a"")\n            self.assertEqual(d[0][""b""], ""b"")\n            self.assertEqual(d[0][""c""], ""c"")\n            self.assertEqual(d[0][""n""], 1)\n            self.assertEqual(d[1][""a""], ""NAN"")\n\n    def test_pandas_groupbynan_regular(self):\n        df = pandas.DataFrame([dict(a=""a"", b=1), dict(a=""a"", b=2)])\n        gr = df.groupby([""a""]).sum()\n        gr2_ = pandas_groupby_nan(df, [""a""]).sum()\n        self.assertEqualDataFrame(gr, gr2_)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_df/test_streaming_dataframe.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=4s)\n""""""\nimport os\nimport unittest\nfrom io import StringIO\nimport pandas\nimport numpy\nfrom pyquickhelper.pycode import ExtTestCase, get_temp_folder\nfrom pandas_streaming.data import dummy_streaming_dataframe\nfrom pandas_streaming.exc import StreamingInefficientException\nfrom pandas_streaming.df import StreamingDataFrame\nfrom pandas_streaming.df.dataframe import StreamingDataFrameSchemaError\n\n\nclass TestStreamingDataFrame(ExtTestCase):\n\n    def test_shape(self):\n        sdf = dummy_streaming_dataframe(100)\n        dfs = list(sdf)\n        self.assertEqual(len(dfs), 10)\n        self.assertEqual(len(dfs), 10)\n        shape = sdf.shape\n        self.assertEqual(shape, (100, 2))\n        self.assertRaise(lambda: sdf.sort_values(\n            ""r""), StreamingInefficientException)\n\n    def test_init(self):\n        sdf = dummy_streaming_dataframe(100)\n        df1 = sdf.to_df()\n        sdf2 = StreamingDataFrame(sdf)\n        df2 = sdf2.to_df()\n        self.assertEqualDataFrame(df1, df2)\n\n    def test_to_csv(self):\n        sdf = dummy_streaming_dataframe(100)\n        st = sdf.to_csv()\n        self.assertStartsWith("",cint,cstr\\n0,0,s0"",\n                              st.replace(\'\\r\', \'\'))\n        st = sdf.to_csv()\n        self.assertStartsWith("",cint,cstr\\n0,0,s0"",\n                              st.replace(\'\\r\', \'\'))\n\n    def test_iterrows(self):\n        sdf = dummy_streaming_dataframe(100)\n        rows = list(sdf.iterrows())\n        self.assertEqual(sdf.shape[0], len(rows))\n        rows = list(sdf.iterrows())\n        self.assertEqual(sdf.shape[0], len(rows))\n\n    def test_head(self):\n        sdf = dummy_streaming_dataframe(100)\n        st = sdf.head()\n        self.assertEqual(st.shape, (5, 2))\n        st = sdf.head(n=20)\n        self.assertEqual(st.shape, (20, 2))\n        st = sdf.head(n=20)\n        self.assertEqual(st.shape, (20, 2))\n\n    def test_tail(self):\n        sdf = dummy_streaming_dataframe(100)\n        st = sdf.tail()\n        self.assertEqual(st.shape, (5, 2))\n        st = sdf.tail(n=20)\n        self.assertEqual(st.shape, (10, 2))\n\n    def test_read_csv(self):\n        temp = get_temp_folder(__file__, ""temp_read_csv"")\n        df = pandas.DataFrame(data=dict(a=[5, 6], b=[""er"", ""r""]))\n        name = os.path.join(temp, ""df.csv"")\n        name2 = os.path.join(temp, ""df2.csv"")\n        name3 = os.path.join(temp, ""df3.csv"")\n        df.to_csv(name, index=False)\n        df.to_csv(name2, index=True)\n        sdf = StreamingDataFrame.read_csv(name)\n        text = sdf.to_csv(index=False)\n        self.assertRaise(\n            lambda: StreamingDataFrame.read_csv(\n                name2, index_col=0, chunksize=None),\n            ValueError)\n        self.assertRaise(\n            lambda: StreamingDataFrame.read_csv(\n                name2, index_col=0, iterator=False),\n            ValueError)\n        sdf2 = StreamingDataFrame.read_csv(name2, index_col=0)\n        text2 = sdf2.to_csv(index=True)\n        sdf2.to_csv(name3, index=True)\n        with open(name, ""r"") as f:\n            exp = f.read()\n        with open(name2, ""r"") as f:\n            exp2 = f.read()\n        with open(name3, ""r"") as f:\n            text3 = f.read()\n        self.assertEqual(text.replace(\'\\r\', \'\'), exp)\n        sdf2 = StreamingDataFrame.read_df(df)\n        self.assertEqualDataFrame(sdf.to_dataframe(), sdf2.to_dataframe())\n        self.assertEqual(text2.replace(\'\\r\', \'\'), exp2)\n        self.assertEqual(text3.replace(\'\\r\', \'\').replace(\'\\n\\n\', \'\\n\'),\n                         exp2.replace(\'\\r\', \'\'))\n\n    def test_where(self):\n        sdf = dummy_streaming_dataframe(100)\n        cols = sdf.columns\n        self.assertEqual(list(cols), [\'cint\', \'cstr\'])\n        dts = sdf.dtypes\n        self.assertEqual(len(dts), 2)\n        res = sdf.where(lambda row: row[""cint""] == 1)\n        st = res.to_csv()\n        self.assertStartsWith("",cint,cstr\\n0,,\\n1,1.0,s1"",\n                              st.replace(\'\\r\', \'\'))\n        res = sdf.where(lambda row: row[""cint""] == 1)\n        st = res.to_csv()\n        self.assertStartsWith("",cint,cstr\\n0,,\\n1,1.0,s1"",\n                              st.replace(\'\\r\', \'\'))\n\n    def test_dataframe(self):\n        sdf = dummy_streaming_dataframe(100)\n        df = sdf.to_dataframe()\n        self.assertEqual(df.shape, (100, 2))\n\n    def test_sample(self):\n        sdf = dummy_streaming_dataframe(100)\n        res = sdf.sample(frac=0.1)\n        self.assertLesser(res.shape[0], 30)\n        self.assertRaise(lambda: sdf.sample(n=5), ValueError)\n        res = sdf.sample(frac=0.1)\n        self.assertLesser(res.shape[0], 30)\n        self.assertRaise(lambda: sdf.sample(n=5), ValueError)\n\n    def test_sample_cache(self):\n        sdf = dummy_streaming_dataframe(100)\n        res = sdf.sample(frac=0.1, cache=True)\n        df1 = res.to_df()\n        df2 = res.to_df()\n        self.assertEqualDataFrame(df1, df2)\n        self.assertTrue(res.is_stable(n=df1.shape[0], do_check=True))\n        self.assertTrue(res.is_stable(n=df1.shape[0], do_check=False))\n        res = sdf.sample(frac=0.1, cache=False)\n        self.assertFalse(res.is_stable(n=df1.shape[0], do_check=False))\n\n    def test_sample_reservoir_cache(self):\n        sdf = dummy_streaming_dataframe(100)\n        res = sdf.sample(n=10, cache=True, reservoir=True)\n        df1 = res.to_df()\n        df2 = res.to_df()\n        self.assertEqualDataFrame(df1, df2)\n        self.assertEqual(df1.shape, (10, res.shape[1]))\n        self.assertRaise(lambda: sdf.sample(n=10, cache=False, reservoir=True),\n                         ValueError)\n        self.assertRaise(lambda: sdf.sample(frac=0.1, cache=True, reservoir=True),\n                         ValueError)\n\n    def test_apply(self):\n        sdf = dummy_streaming_dataframe(100)\n        self.assertNotEmpty(list(sdf))\n        sdf = sdf.applymap(str)\n        self.assertNotEmpty(list(sdf))\n        sdf = sdf.apply(lambda row: row[[""cint""]] + ""r"", axis=1)\n        self.assertNotEmpty(list(sdf))\n        text = sdf.to_csv(header=False)\n        self.assertStartsWith(""0,0r\\n1,1r\\n2,2r\\n3,3r"",\n                              text.replace(\'\\r\', \'\'))\n\n    def test_train_test_split(self):\n        sdf = dummy_streaming_dataframe(100)\n        tr, te = sdf.train_test_split(index=False, streaming=False)\n        self.assertRaise(\n            lambda: StreamingDataFrame.read_str(tr, chunksize=None),\n            ValueError)\n        self.assertRaise(\n            lambda: StreamingDataFrame.read_str(tr, iterator=False),\n            ValueError)\n        StreamingDataFrame.read_str(tr.encode(\'utf-8\'))\n        trsdf = StreamingDataFrame.read_str(tr)\n        tesdf = StreamingDataFrame.read_str(te)\n        trdf = trsdf.to_dataframe()\n        tedf = tesdf.to_dataframe()\n        df_exp = sdf.to_dataframe()\n        df_val = pandas.concat([trdf, tedf])\n        self.assertEqual(df_exp.shape, df_val.shape)\n        df_val = df_val.sort_values(""cint"").reset_index(drop=True)\n        self.assertEqualDataFrame(df_val, df_exp)\n\n    def test_train_test_split_streaming(self):\n        sdf = dummy_streaming_dataframe(100, asfloat=True)\n        trsdf, tesdf = sdf.train_test_split(\n            streaming=True, unique_rows=True, partitions=[0.7, 0.3])\n        trdf = trsdf.to_dataframe()\n        tedf = tesdf.to_dataframe()\n        df_exp = sdf.to_dataframe()\n        df_val = pandas.concat([trdf, tedf])\n        self.assertEqual(df_exp.shape, df_val.shape)\n        df_val = df_val.sort_values(""cfloat"").reset_index(drop=True)\n        self.assertEqualDataFrame(df_val, df_exp)\n        trdf2 = trsdf.to_dataframe()\n        tedf2 = tesdf.to_dataframe()\n        df_val = pandas.concat([trdf2, tedf2])\n        self.assertEqual(df_exp.shape, df_val.shape)\n        df_val = df_val.sort_values(""cfloat"").reset_index(drop=True)\n        self.assertEqualDataFrame(df_val, df_exp)\n        self.assertEqual(trdf.shape, trdf2.shape)\n        self.assertEqual(tedf.shape, tedf2.shape)\n        self.assertGreater(trdf.shape[0], tedf.shape[0])\n        self.assertGreater(trdf2.shape[0], tedf2.shape[0])\n\n    def test_train_test_split_streaming_tiny(self):\n        df = pandas.DataFrame(data=dict(X=[4.5, 6, 7], Y=[""a"", ""b"", ""c""]))\n\n        sdf2 = StreamingDataFrame.read_df(pandas.concat([df, df]))\n        sdftr, sdfte = sdf2.train_test_split(test_size=0.5)\n        df1 = sdfte.head()\n        df2 = sdfte.head()\n        if df1 is not None or df2 is not None:\n            self.assertEqualDataFrame(df1, df2)\n        df1 = sdftr.head()\n        df2 = sdftr.head()\n        if df1 is not None or df2 is not None:\n            self.assertEqualDataFrame(df1, df2)\n        sdf = StreamingDataFrame.read_df(df)\n        sdf2 = sdf.concat(sdf, axis=0)\n        sdftr, sdfte = sdf2.train_test_split(test_size=0.5)\n        df1 = sdfte.head()\n        df2 = sdfte.head()\n        if df1 is not None or df2 is not None:\n            self.assertEqualDataFrame(df1, df2)\n        df1 = sdftr.head()\n        df2 = sdftr.head()\n        if df1 is not None or df2 is not None:\n            self.assertEqualDataFrame(df1, df2)\n\n    def test_train_test_split_streaming_strat(self):\n        sdf = dummy_streaming_dataframe(100, asfloat=True,\n                                        tify=[""t1"" if i % 3 else ""t0"" for i in range(0, 100)])\n        trsdf, tesdf = sdf.train_test_split(\n            streaming=True, unique_rows=True, stratify=""tify"")\n        trdf = trsdf.to_dataframe()\n        tedf = tesdf.to_dataframe()\n        df_exp = sdf.to_dataframe()\n        df_val = pandas.concat([trdf, tedf])\n        self.assertEqual(df_exp.shape, df_val.shape)\n        df_val = df_val.sort_values(""cfloat"").reset_index(drop=True)\n        self.assertEqualDataFrame(df_val, df_exp)\n        trdf = trsdf.to_dataframe()\n        tedf = tesdf.to_dataframe()\n        df_val = pandas.concat([trdf, tedf])\n        self.assertEqual(df_exp.shape, df_val.shape)\n        df_val = df_val.sort_values(""cfloat"").reset_index(drop=True)\n        self.assertEqualDataFrame(df_val, df_exp)\n        trgr = trdf.groupby(""tify"").count()\n        trgr[""part""] = 0\n        tegr = tedf.groupby(""tify"").count()\n        tegr[""part""] = 1\n        gr = pandas.concat([trgr, tegr])\n        self.assertGreater(gr[\'cfloat\'].min(), 4)\n\n    def test_train_test_split_file(self):\n        temp = get_temp_folder(__file__, ""temp_train_test_split_file"")\n        names = [os.path.join(temp, ""train.txt""),\n                 os.path.join(temp, ""test.txt"")]\n        sdf = dummy_streaming_dataframe(100)\n        sdf.train_test_split(names, index=False, streaming=False)\n        trsdf = StreamingDataFrame.read_csv(names[0])\n        tesdf = StreamingDataFrame.read_csv(names[1])\n        self.assertGreater(trsdf.shape[0], 20)\n        self.assertGreater(tesdf.shape[0], 20)\n        trdf = trsdf.to_dataframe()\n        tedf = tesdf.to_dataframe()\n        self.assertGreater(trdf.shape[0], 20)\n        self.assertGreater(tedf.shape[0], 20)\n        df_exp = sdf.to_dataframe()\n        df_val = pandas.concat([trdf, tedf])\n        self.assertEqual(df_exp.shape, df_val.shape)\n        df_val = df_val.sort_values(""cint"").reset_index(drop=True)\n        self.assertEqualDataFrame(df_val, df_exp)\n\n    def test_train_test_split_file_pattern(self):\n        temp = get_temp_folder(__file__, ""temp_train_test_split_file_pattern"")\n        sdf = dummy_streaming_dataframe(100)\n        names = os.path.join(temp, ""spl_{0}.txt"")\n        self.assertRaise(lambda: sdf.train_test_split(\n            names, index=False, streaming=False), ValueError)\n        names = os.path.join(temp, ""spl_{}.txt"")\n        tr, te = sdf.train_test_split(names, index=False, streaming=False)\n        trsdf = StreamingDataFrame.read_csv(tr)\n        tesdf = StreamingDataFrame.read_csv(te)\n        trdf = trsdf.to_dataframe()\n        tedf = tesdf.to_dataframe()\n        df_exp = sdf.to_dataframe()\n        df_val = pandas.concat([trdf, tedf])\n        self.assertEqual(df_exp.shape, df_val.shape)\n        df_val = df_val.sort_values(""cint"").reset_index(drop=True)\n        self.assertEqualDataFrame(df_val, df_exp)\n\n    def test_merge(self):\n        def compares(a, b, how):\n            m = a.merge(b, on=""cint"", indicator=True)\n            dm = m.to_dataframe()\n            da = a.to_dataframe()\n            db = b.to_dataframe()\n            exp = da.merge(db, on=""cint"", indicator=True)\n            self.assertEqualDataFrame(dm.reset_index(drop=True),\n                                      exp.reset_index(drop=True))\n\n        sdf20 = dummy_streaming_dataframe(20)\n        sdf30 = dummy_streaming_dataframe(30)\n        # itself\n        hows = ""inner left right outer"".split()\n        for how in hows:\n            compares(sdf20, sdf20, how)\n            compares(sdf20, sdf20, how)\n        for how in hows:\n            compares(sdf20, sdf30, how)\n            compares(sdf20, sdf30, how)\n        for how in hows:\n            compares(sdf30, sdf20, how)\n            compares(sdf30, sdf20, how)\n        sdf20.merge(sdf20.to_dataframe(), on=""cint"", indicator=True)\n\n    def test_concatv(self):\n        sdf20 = dummy_streaming_dataframe(20)\n        sdf30 = dummy_streaming_dataframe(30)\n        df20 = sdf20.to_dataframe()\n        df30 = sdf30.to_dataframe()\n        df = pandas.concat([df20, df30], axis=0)\n\n        m1 = sdf20.concat(sdf30, axis=0)\n        self.assertEqualDataFrame(m1.to_dataframe(), df)\n        m1 = sdf20.concat(df30, axis=0)\n        self.assertEqualDataFrame(m1.to_dataframe(), df)\n        m1 = sdf20.concat(map(lambda x: x, [df30]), axis=0)\n        self.assertEqualDataFrame(m1.to_dataframe(), df)\n        m1 = sdf20.concat(map(lambda x: x, [df30]), axis=0)\n        self.assertEqualDataFrame(m1.to_dataframe(), df)\n\n        df30[""g""] = 4\n        self.assertRaise(lambda: sdf20.concat(df30).to_dataframe(),\n                         ValueError, ""Frame others[0] do not have the same column names"")\n        df20[""cint""] = df20[""cint""].astype(float)\n        self.assertRaise(lambda: sdf20.concat(df20).to_dataframe(),\n                         ValueError, ""Frame others[0] do not have the same column types"")\n\n    def test_concath(self):\n        sdf20 = dummy_streaming_dataframe(20)\n        sdf30 = dummy_streaming_dataframe(20)\n        df20 = sdf20.to_dataframe()\n        df30 = sdf30.to_dataframe()\n        df = pandas.concat([df20, df30], axis=1)\n\n        m1 = sdf20.concat(sdf30, axis=1)\n        self.assertEqualDataFrame(m1.to_dataframe(), df)\n        sdf22 = dummy_streaming_dataframe(22)\n        sdf25 = dummy_streaming_dataframe(25)\n        self.assertRaise(lambda: sdf22.concat(sdf25, axis=1).to_dataframe(),\n                         RuntimeError)\n\n    def test_groupby(self):\n        df20 = dummy_streaming_dataframe(20).to_dataframe()\n        df20[""key""] = df20[""cint""].apply(lambda i: i % 3 == 0)\n        sdf20 = StreamingDataFrame.read_df(df20, chunksize=5)\n        gr = sdf20.groupby(""key"", lambda gr: gr.sum())\n        gr2 = df20.groupby(""key"").sum()\n        self.assertEqualDataFrame(gr, gr2)\n        self.assertRaise(lambda: sdf20.groupby(\n            ""key"", in_memory=False), NotImplementedError)\n\n        # Do not replace lambda c:sum(c) by sum or...\n        # pandas.core.base.SpecificationError: Function names must be unique, found multiple named sum\n        gr2 = df20.groupby(""key"").agg([numpy.sum, lambda c:sum(c)])\n        gr = sdf20.groupby(""key"", lambda gr: gr.agg(\n            [numpy.sum, lambda c:sum(c)]))\n        self.assertEqualDataFrame(gr, gr2)\n\n        gr = sdf20.groupby(""key"", lambda gr: gr.count())\n        gr2 = df20.groupby(""key"").count()\n        self.assertEqualDataFrame(gr, gr2)\n\n        df = pandas.DataFrame(dict(A=[3, 4, 3], B=[5, 6, 7]))\n        sdf = StreamingDataFrame.read_df(df)\n        gr = sdf.groupby(""A"")\n        gr2 = df.groupby(""A"").sum()\n        self.assertEqualDataFrame(gr, gr2)\n\n    def test_groupby_cum(self):\n        df20 = dummy_streaming_dataframe(20).to_dataframe()\n        df20[""key""] = df20[""cint""].apply(lambda i: i % 3 == 0)\n        sdf20 = StreamingDataFrame.read_df(df20, chunksize=5)\n        sgr = sdf20.groupby_streaming(\n            ""key"", lambda gr: gr.sum(), strategy=\'cum\', as_index=False)\n        gr2 = df20.groupby(""key"", as_index=False).sum()\n        lastgr = None\n        for gr in sgr:\n            self.assertEqual(list(gr.columns), list(gr2.columns))\n            lastgr = gr\n        self.assertEqualDataFrame(lastgr, gr2)\n\n    def test_groupby_streaming(self):\n        df20 = dummy_streaming_dataframe(20).to_dataframe()\n        df20[""key""] = df20[""cint""].apply(lambda i: i % 3 == 0)\n        sdf20 = StreamingDataFrame.read_df(df20, chunksize=5)\n        sgr = sdf20.groupby_streaming(\n            ""key"", lambda gr: gr.sum(), strategy=\'streaming\', as_index=False)\n        gr2 = df20.groupby(""key"", as_index=False).sum()\n        grs = list(sgr)\n        gr = pandas.concat(grs).groupby(""key"", as_index=False).sum()\n        self.assertEqualDataFrame(gr, gr2)\n\n    def test_groupby_cum_asindex(self):\n        df20 = dummy_streaming_dataframe(20).to_dataframe()\n        df20[""key""] = df20[""cint""].apply(lambda i: i % 3 == 0)\n        sdf20 = StreamingDataFrame.read_df(df20, chunksize=5)\n        sgr = sdf20.groupby_streaming(\n            ""key"", lambda gr: gr.sum(), strategy=\'cum\', as_index=True)\n        gr2 = df20.groupby(""key"", as_index=True).sum()\n        lastgr = None\n        for gr in sgr:\n            self.assertEqual(list(gr.columns), list(gr2.columns))\n            lastgr = gr\n        self.assertEqualDataFrame(lastgr, gr2)\n\n    def test_merge_2(self):\n        df = pandas.DataFrame(data=dict(X=[4.5, 6, 7], Y=[""a"", ""b"", ""c""]))\n        df2 = pandas.concat([df, df])\n        sdf = StreamingDataFrame.read_df(df)\n        sdf2 = sdf.concat(sdf, axis=0)\n        self.assertEqualDataFrame(df2, sdf2.to_dataframe())\n        self.assertEqualDataFrame(df2, sdf2.to_dataframe())\n        m = pandas.DataFrame(dict(Y=[""a"", ""b""], Z=[10, 20]))\n        jm = df2.merge(m, left_on=""Y"", right_on=""Y"", how=""outer"")\n        sjm = sdf2.merge(m, left_on=""Y"", right_on=""Y"", how=""outer"")\n        self.assertEqualDataFrame(jm.sort_values([""X"", ""Y""]).reset_index(drop=True),\n                                  sjm.to_dataframe().sort_values([""X"", ""Y""]).reset_index(drop=True))\n\n    def test_schema_consistant(self):\n        df = pandas.DataFrame([dict(cf=0, cint=0, cstr=""0""), dict(cf=1, cint=1, cstr=""1""),\n                               dict(cf=2, cint=""s2"", cstr=""2""), dict(cf=3, cint=3, cstr=""3"")])\n        temp = get_temp_folder(__file__, ""temp_schema_consistant"")\n        name = os.path.join(temp, ""df.csv"")\n        stio = StringIO()\n        df.to_csv(stio, index=False)\n        self.assertNotEmpty(stio.getvalue())\n        df.to_csv(name, index=False)\n        self.assertEqual(df.shape, (4, 3))\n        sdf = StreamingDataFrame.read_csv(name, chunksize=2)\n        self.assertRaise(lambda: list(sdf), StreamingDataFrameSchemaError)\n        sdf = StreamingDataFrame.read_csv(\n            name, chunksize=2, check_schema=False)\n        pieces = list(sdf)\n        self.assertEqual(len(pieces), 2)\n\n    def test_getitem(self):\n        sdf = dummy_streaming_dataframe(100)\n        sdf2 = sdf[[""cint""]]\n        self.assertEqual(sdf2.shape, (100, 1))\n        df1 = sdf.to_df()\n        df2 = sdf2.to_df()\n        self.assertEqualDataFrame(df1[[""cint""]], df2)\n        self.assertRaise(lambda: sdf[""cint""], NotImplementedError)\n        self.assertRaise(lambda: sdf[:, ""cint""], NotImplementedError)\n\n    def test_read_csv_names(self):\n        this = os.path.abspath(os.path.dirname(__file__))\n        data = os.path.join(this, ""data"", ""buggy_hash2.csv"")\n        df = pandas.read_csv(data, sep=""\\t"", names=[\n                             ""A"", ""B"", ""C""], header=None)\n        sdf = StreamingDataFrame.read_csv(\n            data, sep=""\\t"", names=[""A"", ""B"", ""C""], chunksize=2, header=None)\n        head = sdf.head(n=1)\n        self.assertEqualDataFrame(df.head(n=1), head)\n\n    def test_add_column(self):\n        df = pandas.DataFrame(data=dict(X=[4.5, 6, 7], Y=[""a"", ""b"", ""c""]))\n        sdf = StreamingDataFrame.read_df(df)\n        sdf2 = sdf.add_column(""d"", lambda row: int(1))\n        df2 = sdf2.to_dataframe()\n        df[""d""] = 1\n        self.assertEqualDataFrame(df, df2)\n\n        sdf3 = StreamingDataFrame.read_df(df)\n        sdf4 = sdf3.add_column(""dd"", 2)\n        df4 = sdf4.to_dataframe()\n        df[""dd""] = 2\n        self.assertEqualDataFrame(df, df4)\n\n        sdfA = StreamingDataFrame.read_df(df)\n        sdfB = sdfA.add_column(""dd12"", lambda row: row[""dd""] + 10)\n        dfB = sdfB.to_dataframe()\n        df[""dd12""] = 12\n        self.assertEqualDataFrame(df, dfB)\n\n    def test_fillna(self):\n        df = pandas.DataFrame(\n            data=dict(X=[4.5, numpy.nan, 7], Y=[""a"", ""b"", numpy.nan]))\n        sdf = StreamingDataFrame.read_df(df)\n\n        df2 = pandas.DataFrame(\n            data=dict(X=[4.5, 10.0, 7], Y=[""a"", ""b"", ""NAN""]))\n        na = sdf.fillna(value=dict(X=10.0, Y=""NAN""))\n        ndf = na.to_df()\n        self.assertEqual(ndf, df2)\n\n        df3 = pandas.DataFrame(\n            data=dict(X=[4.5, 10.0, 7], Y=[""a"", ""b"", numpy.nan]))\n        na = sdf.fillna(value=dict(X=10.0))\n        ndf = na.to_df()\n        self.assertEqual(ndf, df3)\n\n\nif __name__ == ""__main__"":\n    TestStreamingDataFrame().test_apply()\n    unittest.main()\n'"
_unittests/ut_documentation/test_run_notebooks.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=33s)\n""""""\nimport os\nimport unittest\nfrom pyquickhelper.loghelper import fLOG\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pyquickhelper.ipythonhelper import test_notebook_execution_coverage\nimport pandas_streaming\n\n\nclass TestRunNotebooksPython(ExtTestCase):\n\n    def setUp(self):\n        import jyquickhelper  # pylint: disable=C0415\n        self.assertTrue(jyquickhelper is not None)\n\n    def test_notebook_artificiel(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        self.assertTrue(pandas_streaming is not None)\n        folder = os.path.join(os.path.dirname(__file__),\n                              "".."", "".."", ""_doc"", ""notebooks"")\n        test_notebook_execution_coverage(\n            __file__, ""first_steps"", folder, \'pandas_streaming\', copy_files=[], fLOG=fLOG)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_module/test_code_style.py,0,"b'""""""\n@brief      test log(time=0s)\n""""""\nimport os\nimport unittest\nfrom pyquickhelper.loghelper import fLOG\nfrom pyquickhelper.pycode import check_pep8, ExtTestCase\n\n\nclass TestCodeStyle(ExtTestCase):\n    """"""Test style.""""""\n\n    def test_style_src(self):\n        thi = os.path.abspath(os.path.dirname(__file__))\n        src_ = os.path.normpath(os.path.join(\n            thi, "".."", "".."", ""pandas_streaming""))\n        check_pep8(src_, fLOG=fLOG,\n                   pylint_ignore=(\'C0103\', \'C1801\', \'R0201\', \'R1705\', \'W0108\', \'W0613\',\n                                  \'W0212\', \'W0703\', \'W0107\'),\n                   skip=[""Too many nested blocks"",\n                         ""Module \'numpy.random\' has no \'RandomState\' member"",\n                         ""dataframe_split.py:60: [E731]"",\n                         ])\n\n    def test_style_test(self):\n        thi = os.path.abspath(os.path.dirname(__file__))\n        test = os.path.normpath(os.path.join(thi, "".."", ))\n        check_pep8(test, fLOG=fLOG, neg_pattern=""temp_.*"",\n                   pylint_ignore=(\'C0103\', \'C1801\', \'R0201\', \'R1705\', \'W0108\', \'W0613\',\n                                  \'C0111\', \'W0107\'),\n                   skip=[])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_module/test_convert_notebooks.py,0,"b'""""""\n@brief      test log(time=0s)\n""""""\nimport os\nimport unittest\nfrom pyquickhelper.loghelper import fLOG\nfrom pyquickhelper.filehelper import explore_folder_iterfile\nfrom pyquickhelper.pycode import ExtTestCase\nfrom pyquickhelper.ipythonhelper import upgrade_notebook, remove_execution_number\n\n\nclass TestConvertNotebooks(ExtTestCase):\n\n    def test_convert_notebooks(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        fold = os.path.abspath(os.path.dirname(__file__))\n        fold2 = os.path.normpath(\n            os.path.join(fold, "".."", "".."", ""_doc"", ""notebooks""))\n        for nbf in explore_folder_iterfile(fold2, pattern="".*[.]ipynb""):\n            t = upgrade_notebook(nbf)\n            if t:\n                fLOG(""modified"", nbf)\n            # remove numbers\n            remove_execution_number(nbf, nbf)\n\n        fold2 = os.path.normpath(os.path.join(fold, "".."", "".."", ""_unittests""))\n        for nbf in explore_folder_iterfile(fold2, pattern="".*[.]ipynb""):\n            t = upgrade_notebook(nbf)\n            if t:\n                fLOG(""modified"", nbf)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_module/test_readme.py,0,"b'""""""\n@brief      test tree node (time=50s)\n""""""\nimport os\nimport unittest\nfrom pyquickhelper.loghelper import fLOG\nfrom pyquickhelper.pycode import get_temp_folder, ExtTestCase, check_readme_syntax\n\n\nclass TestReadme(ExtTestCase):\n\n    def test_venv_docutils08_readme(self):\n        fLOG(\n            __file__,\n            self._testMethodName,\n            OutputPrint=__name__ == ""__main__"")\n\n        fold = os.path.dirname(os.path.abspath(__file__))\n        readme = os.path.join(fold, "".."", "".."", ""README.rst"")\n        self.assertTrue(os.path.exists(readme))\n        with open(readme, ""r"", encoding=""utf8"") as f:\n            content = f.read()\n\n        self.assertTrue(len(content) > 0)\n        temp = get_temp_folder(__file__, ""temp_readme"")\n\n        if __name__ != ""__main__"":\n            # does not work from a virtual environment\n            return\n\n        check_readme_syntax(readme, folder=temp, fLOG=fLOG)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
_unittests/ut_module/test_sklearn.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@brief      test log(time=2s)\n""""""\nimport unittest\nimport numpy\nimport pandas\nfrom sklearn.linear_model import LogisticRegression\nfrom pyquickhelper.pycode import ExtTestCase\n\n\nclass TestScikitLearn(ExtTestCase):\n\n    def test_logistic_regression_check(self):\n        X = pandas.DataFrame(numpy.array([[0.1, 0.2], [-0.2, 0.3]]))\n        Y = numpy.array([0, 1])\n        clq = LogisticRegression(fit_intercept=False, solver=""liblinear"",\n                                 random_state=42)\n        clq.fit(X, Y)\n        pred2 = clq.predict(X)\n        self.assertEqual(numpy.array([0, 1]), pred2)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
pandas_streaming/data/__init__.py,0,"b'""""""\n@file\n@brief Shortcuts to *df*.\n""""""\n\nfrom .dummy import dummy_streaming_dataframe\n'"
pandas_streaming/data/dummy.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Dummy datasets.\n""""""\nfrom pandas import DataFrame\nfrom ..df import StreamingDataFrame\n\n\ndef dummy_streaming_dataframe(n, chunksize=10, asfloat=False, **cols):\n    """"""\n    Returns a dummy streaming dataframe\n    mostly for unit test purposes.\n\n    @param      n           number of rows\n    @param      chunksize   chunk size\n    @param      asfloat     use random float and not random int\n    @param      cols        additional columns\n    @return                 a @see cl StreamingDataFrame\n    """"""\n    if asfloat:\n        df = DataFrame(dict(cfloat=[_ + 0.1 for _ in range(0, n)], cstr=[\n                       ""s{0}"".format(i) for i in range(0, n)]))\n    else:\n        df = DataFrame(dict(cint=list(range(0, n)), cstr=[\n                       ""s{0}"".format(i) for i in range(0, n)]))\n    for k, v in cols.items():\n        df[k] = v\n    return StreamingDataFrame.read_df(df, chunksize=chunksize)\n'"
pandas_streaming/df/__init__.py,0,"b'""""""\n@file\n@brief Shortcuts to *df*.\n""""""\n\nfrom .connex_split import train_test_split_weights, train_test_connex_split, train_test_apart_stratify\nfrom .dataframe import StreamingDataFrame\nfrom .dataframe_helpers import dataframe_hash_columns, dataframe_unfold, dataframe_shuffle\nfrom .dataframe_helpers import pandas_groupby_nan, numpy_types\nfrom .dataframe_io import to_zip, read_zip\n'"
pandas_streaming/df/connex_split.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Implements a connex split between train and test.\n""""""\nfrom collections import Counter\nimport pandas\nimport numpy\nfrom sklearn.model_selection import train_test_split\nfrom .dataframe_helpers import dataframe_shuffle\n\n\nclass ImbalancedSplitException(Exception):\n    """"""\n    Raised when an imbalanced split is detected.\n    """"""\n    pass\n\n\ndef train_test_split_weights(df, weights=None, test_size=0.25, train_size=None,\n                             shuffle=True, fail_imbalanced=0.05, random_state=None):\n    """"""\n    Splits a database in train/test given, every row\n    can have a different weight.\n\n    @param  df              :epkg:`pandas:DataFrame` or @see cl StreamingDataFrame\n    @param  weights         None or weights or weights column name\n    @param  test_size       ratio for the test partition (if *train_size* is not specified)\n    @param  train_size      ratio for the train partition\n    @param  shuffle         shuffles before the split\n    @param  fail_imbalanced raises an exception if relative weights difference is higher than this value\n    @param  random_state    seed for random generators\n    @return                 train and test :epkg:`pandas:DataFrame`\n\n    If the dataframe is not shuffled first, the function\n    will produce two datasets which are unlikely to be randomized\n    as the function tries to keep equal weights among both paths\n    without using randomness.\n    """"""\n    if hasattr(df, \'iter_creation\'):\n        raise NotImplementedError(\n            \'Not implemented yet for StreamingDataFrame.\')\n    if isinstance(df, numpy.ndarray):\n        raise NotImplementedError(""Not implemented on numpy arrays."")\n    if shuffle:\n        df = dataframe_shuffle(df, random_state=random_state)\n    if weights is None:\n        if test_size == 0 or train_size == 0:\n            raise ValueError(\n                ""test_size={0} or train_size={1} cannot be null (1)"".format(test_size, train_size))\n        return train_test_split(df, test_size=test_size, train_size=train_size, random_state=random_state)\n\n    if isinstance(weights, pandas.Series):\n        weights = list(weights)\n    elif isinstance(weights, str):\n        weights = list(df[weights])\n    if len(weights) != df.shape[0]:\n        raise ValueError(""Dimension mismatch between weights and dataframe {0} != {1}"".format(\n            df.shape[0], len(weights)))\n\n    p = (1 - test_size) if test_size else None\n    if train_size is not None:\n        p = train_size\n        test_size = 1 - p\n    if p is None or min(test_size, p) <= 0:\n        raise ValueError(\n            ""test_size={0} or train_size={1} cannot be null (2)"".format(test_size, train_size))\n    ratio = test_size / p\n\n    if random_state is None:\n        randint = numpy.random.randint\n    else:\n        state = numpy.random.RandomState(random_state)\n        randint = state.randint\n\n    balance = 0\n    train_ids = []\n    test_ids = []\n    test_weights = 0\n    train_weights = 0\n    for i in range(0, df.shape[0]):\n        w = weights[i]\n        if balance == 0:\n            h = randint(0, 1)\n            totest = h == 0\n        else:\n            totest = balance < 0\n        if totest:\n            test_ids.append(i)\n            balance += w\n            test_weights += w\n        else:\n            train_ids.append(i)\n            balance -= w * ratio\n            train_weights += w * ratio\n\n    r = abs(train_weights - test_weights) / \\\n        (1.0 * (train_weights + test_weights))\n    if r >= fail_imbalanced:\n        raise ImbalancedSplitException(\n            ""Split is imbalanced: train_weights={0} test_weights={1} r={2}"".format(train_weights, test_weights, r))\n\n    return df.iloc[train_ids, :], df.iloc[test_ids, :]\n\n\ndef train_test_connex_split(df, groups, test_size=0.25, train_size=None,\n                            stratify=None, hash_size=9, unique_rows=False,\n                            shuffle=True, fail_imbalanced=0.05, keep_balance=None,\n                            stop_if_bigger=None, return_cnx=False,\n                            must_groups=None, random_state=None, fLOG=None):\n    """"""\n    This split is for a specific case where data is linked\n    in many ways. Let\'s assume we have three ids as we have\n    for online sales: *(product id, user id, card id)*.\n    As we may need to compute aggregated features,\n    we need every id not to be present in both train and\n    test set. The function computes the connected components\n    and breaks each of them in two parts for train and test.\n\n    @param  df              :epkg:`pandas:DataFrame`\n    @param  groups          columns name for the ids\n    @param  test_size       ratio for the test partition (if *train_size* is not specified)\n    @param  train_size      ratio for the train partition\n    @param  stratify        column holding the stratification\n    @param  hash_size       size of the hash to cache information about partition\n    @param  unique_rows     ensures that rows are unique\n    @param  shuffle         shuffles before the split\n    @param  fail_imbalanced raises an exception if relative weights difference is higher than this value\n    @param  stop_if_bigger  (float) stops a connected components from being\n                            bigger than this ratio of elements, this should not be used\n                            unless a big components emerges, the algorithm stops merging\n                            but does not guarantee it returns the best cut,\n                            the value should be close to 0\n    @param  keep_balance    (float), if not None, does not merge connected components\n                            if their relative sizes are too different, the value should be\n                            close to 1\n    @param  return_cnx      returns connected components as a third results\n    @param  must_groups     column name for ids which must not be shared by train/test partitions\n    @param  random_state    seed for random generator\n    @param  fLOG            logging function\n    @return                 Two @see cl StreamingDataFrame, one\n                            for train, one for test.\n\n    The list of ids must hold in memory.\n    There is no streaming implementation for the ids.\n\n    .. exref::\n        :title: Splits a dataframe, keep ids in separate partitions\n        :tag: dataframe\n\n        In some data science problems, rows are not independant\n        and share common value, most of the time ids. In some\n        specific case, multiple ids from different columns are\n        connected and must appear in the same partition.\n        Testing that each id column is evenly split and do not\n        appear in both sets in not enough. Connected components\n        are needed.\n\n        .. runpython::\n            :showcode:\n\n            from pandas import DataFrame\n            from pandas_streaming.df import train_test_connex_split\n\n            df = DataFrame([dict(user=""UA"", prod=""PAA"", card=""C1""),\n                            dict(user=""UA"", prod=""PB"", card=""C1""),\n                            dict(user=""UB"", prod=""PC"", card=""C2""),\n                            dict(user=""UB"", prod=""PD"", card=""C2""),\n                            dict(user=""UC"", prod=""PAA"", card=""C3""),\n                            dict(user=""UC"", prod=""PF"", card=""C4""),\n                            dict(user=""UD"", prod=""PG"", card=""C5""),\n                            ])\n\n            train, test = train_test_connex_split(df, test_size=0.5,\n                                                  groups=[\'user\', \'prod\', \'card\'],\n                                                  fail_imbalanced=0.6)\n            print(train)\n            print(test)\n\n    If *return_cnx* is True, the third results contains:\n\n    * connected components for each id\n    * the dataframe with connected components as a new column\n\n    .. runpython::\n        :showcode:\n\n        from pandas import DataFrame\n        from pandas_streaming.df import train_test_connex_split\n\n        df = DataFrame([dict(user=""UA"", prod=""PAA"", card=""C1""),\n                        dict(user=""UA"", prod=""PB"", card=""C1""),\n                        dict(user=""UB"", prod=""PC"", card=""C2""),\n                        dict(user=""UB"", prod=""PD"", card=""C2""),\n                        dict(user=""UC"", prod=""PAA"", card=""C3""),\n                        dict(user=""UC"", prod=""PF"", card=""C4""),\n                        dict(user=""UD"", prod=""PG"", card=""C5""),\n                        ])\n\n        train, test, cnx = train_test_connex_split(df, test_size=0.5,\n                                              groups=[\'user\', \'prod\', \'card\'],\n                                              fail_imbalanced=0.6, return_cnx=True)\n\n        print(cnx[0])\n        print(cnx[1])\n    """"""\n    if stratify is not None:\n        raise NotImplementedError(""Option stratify is not implemented."")\n    if groups is None or len(groups) == 0:\n        raise ValueError(  # pragma: no cover\n            ""groups is empty. Use regular train_test_split."")\n    if hasattr(df, \'iter_creation\'):\n        raise NotImplementedError(\n            \'Not implemented yet for StreamingDataFrame.\')\n    if isinstance(df, numpy.ndarray):\n        raise NotImplementedError(""Not implemented on numpy arrays."")\n    if shuffle:\n        df = dataframe_shuffle(df, random_state=random_state)\n\n    dfids = df[groups].copy()\n    if must_groups is not None:\n        dfids_must = df[must_groups].copy()\n\n    name = ""connex""\n    while name in dfids.columns:\n        name += ""_""\n    one = ""weight""\n    while one in dfids.columns:\n        one += ""_""\n\n    # Connected components.\n    elements = list(range(dfids.shape[0]))\n    counts_cnx = {i: {i} for i in elements}\n    connex = {}\n    avoids_merge = {}\n\n    def do_connex_components(dfrows, local_groups, kb, sib):\n        ""run connected components algorithms""\n        itern = 0\n        modif = 1\n\n        while modif > 0 and itern < len(elements):\n            if fLOG and df.shape[0] > 10000:\n                fLOG(""[train_test_connex_split] iteration={0}-#nb connect={1} - modif={2}"".format(\n                    iter, len(set(elements)), modif))\n            modif = 0\n            itern += 1\n            for i, row in enumerate(dfrows.itertuples(index=False, name=None)):\n                vals = [val for val in zip(local_groups, row) if not isinstance(\n                    val[1], float) or not numpy.isnan(val[1])]\n\n                c = elements[i]\n\n                for val in vals:\n                    if val not in connex:\n                        connex[val] = c\n                        modif += 1\n\n                set_c = set(connex[val] for val in vals)\n                set_c.add(c)\n                new_c = min(set_c)\n\n                add_pair_c = []\n                for c in set_c:\n                    if c == new_c or (new_c, c) in avoids_merge:\n                        continue\n                    if kb is not None:\n                        maxi = min(len(counts_cnx[new_c]), len(counts_cnx[c]))\n                        if maxi > 5:\n                            diff = len(counts_cnx[new_c]) + \\\n                                len(counts_cnx[c]) - maxi\n                            r = diff / float(maxi)\n                            if r > kb:\n                                if fLOG:  # pragma: no cover\n                                    fLOG(\'[train_test_connex_split]    balance r={0:0.00000}>{1:0.00}, #[{2}]={3}, #[{4}]={5}\'.format(\n                                        r, kb, new_c, len(counts_cnx[new_c]), c, len(counts_cnx[c])))\n                                continue\n\n                    if sib is not None:\n                        r = (len(counts_cnx[new_c]) +\n                             len(counts_cnx[c])) / float(len(elements))\n                        if r > sib:\n                            if fLOG:\n                                fLOG(\'[train_test_connex_split]    no merge r={0:0.00000}>{1:0.00}, #[{2}]={3}, #[{4}]={5}\'.format(\n                                    r, sib, new_c, len(counts_cnx[new_c]), c, len(counts_cnx[c])))\n                            avoids_merge[new_c, c] = i\n                            continue\n\n                    add_pair_c.append(c)\n\n                if len(add_pair_c) > 0:\n                    for c in add_pair_c:\n                        modif += len(counts_cnx[c])\n                        for ii in counts_cnx[c]:\n                            elements[ii] = new_c\n                        counts_cnx[new_c] = counts_cnx[new_c].union(\n                            counts_cnx[c])\n                        counts_cnx[c] = set()\n\n                        keys = list(vals)\n                        for val in keys:\n                            if connex[val] == c:\n                                connex[val] = new_c\n                                modif += 1\n\n    if must_groups:\n        do_connex_components(dfids_must, must_groups, None, None)\n    do_connex_components(dfids, groups, keep_balance, stop_if_bigger)\n\n    # final\n    dfids[name] = elements\n    dfids[one] = 1\n    grsum = dfids[[name, one]].groupby(name, as_index=False).sum()\n    if fLOG:\n        for g in groups:\n            fLOG(""[train_test_connex_split]     #nb in \'{0}\': {1}"".format(\n                g, len(set(dfids[g]))))\n        fLOG(\n            ""[train_test_connex_split] #connex {0}/{1}"".format(grsum.shape[0], dfids.shape[0]))\n    if grsum.shape[0] <= 1:\n        raise ValueError(  # pragma: no cover\n            ""Every element is in the same connected components."")\n\n    # Statistics: top connected components\n    if fLOG:\n        # Global statistics\n        counts = Counter(elements)\n        cl = [(v, k) for k, v in counts.items()]\n        cum = 0\n        maxc = None\n        fLOG(""[train_test_connex_split] number of connected components: {0}"".format(\n            len(set(elements))))\n        for i, (v, k) in enumerate(sorted(cl, reverse=True)):\n            if i == 0:\n                maxc = k, v\n            if i >= 10:\n                break\n            cum += v\n            fLOG(""[train_test_connex_split]     c={0} #elements={1} cumulated={2}/{3}"".format(\n                k, v, cum, len(elements)))\n\n        # Most important component\n        fLOG(\n            \'[train_test_connex_split] first row of the biggest component {0}\'.format(maxc))\n        tdf = dfids[dfids[name] == maxc[0]]\n        fLOG(\'[train_test_connex_split] \\n{0}\'.format(tdf.head(n=10)))\n\n    # Splits.\n    train, test = train_test_split_weights(grsum, weights=one, test_size=test_size,\n                                           train_size=train_size, shuffle=shuffle,\n                                           fail_imbalanced=fail_imbalanced,\n                                           random_state=random_state)\n    train.drop(one, inplace=True, axis=1)\n    test.drop(one, inplace=True, axis=1)\n\n    # We compute the final dataframe.\n    def double_merge(d):\n        ""merge twice""\n        merge1 = dfids.merge(d, left_on=name, right_on=name)\n        merge2 = df.merge(merge1, left_on=groups, right_on=groups)\n        return merge2\n\n    train_f = double_merge(train)\n    test_f = double_merge(test)\n    if return_cnx:\n        return train_f, test_f, (connex, dfids)\n    else:\n        return train_f, test_f\n\n\ndef train_test_apart_stratify(df, group, test_size=0.25, train_size=None,\n                              stratify=None, force=False, random_state=None, fLOG=None):\n    """"""\n    This split is for a specific case where data is linked\n    in one way. Let\'s assume we have two ids as we have\n    for online sales: *(product id, category id)*.\n    A product can have multiple categories. We need to have\n    distinct products on train and test but common categories\n    on both sides.\n\n    @param  df              :epkg:`pandas:DataFrame`\n    @param  group           columns name for the ids\n    @param  test_size       ratio for the test partition (if *train_size* is not specified)\n    @param  train_size      ratio for the train partition\n    @param  stratify        column holding the stratification\n    @param  force           if True, tries to get at least one example on the test side\n                            for each value of the column *stratify*\n    @param  random_state    seed for random generators\n    @param  fLOG            logging function\n    @return                 Two @see cl StreamingDataFrame, one\n                            for train, one for test.\n\n    .. index:: multi-label\n\n    The list of ids must hold in memory.\n    There is no streaming implementation for the ids.\n    This split was implemented for a case of a multi-label\n    classification. A category (*stratify*) is not exclusive\n    and an observation can be assigned to multiple\n    categories. In that particular case, the method\n    `train_test_split <http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html>`_\n    can not directly be used.\n\n    .. runpython::\n        :showcode:\n\n        import pandas\n        df = pandas.DataFrame([dict(a=1, b=""e""),\n                               dict(a=1, b=""f""),\n                               dict(a=2, b=""e""),\n                               dict(a=2, b=""f""),\n                               ])\n\n        from pandas_streaming.df import train_test_apart_stratify\n        train, test = train_test_apart_stratify(df, group=""a"", stratify=""b"", test_size=0.5)\n        print(train)\n        print(\'-----------\')\n        print(test)\n    """"""\n    if stratify is None:\n        raise ValueError(""stratify must be specified."")\n    if group is None:\n        raise ValueError(""group must be specified."")\n    if hasattr(df, \'iter_creation\'):\n        raise NotImplementedError(\n            \'Not implemented yet for StreamingDataFrame.\')\n    if isinstance(df, numpy.ndarray):\n        raise NotImplementedError(""Not implemented on numpy arrays."")\n\n    p = (1 - test_size) if test_size else None\n    if train_size is not None:\n        p = train_size\n    test_size = 1 - p\n    if p is None or min(test_size, p) <= 0:\n        raise ValueError(  # pragma: no cover\n            ""test_size={0} or train_size={1} cannot be null"".format(test_size, train_size))\n\n    couples = df[[group, stratify]].itertuples(name=None, index=False)\n    hist = Counter(df[stratify])\n    sorted_hist = [(v, k) for k, v in hist.items()]\n    sorted_hist.sort()\n    ids = {c: set() for c in hist}\n\n    for g, s in couples:\n        ids[s].add(g)\n\n    if random_state is None:\n        permutation = numpy.random.permutation\n    else:\n        state = numpy.random.RandomState(random_state)\n        permutation = state.permutation\n\n    split = {}\n    for _, k in sorted_hist:\n        not_assigned = [c for c in ids[k] if c not in split]\n        if len(not_assigned) == 0:\n            continue\n        assigned = [c for c in ids[k] if c in split]\n        nb_test = sum(split[c] for c in assigned)\n        expected = min(len(ids[k]), int(\n            test_size * len(ids[k]) + 0.5)) - nb_test\n        if force and expected == 0 and nb_test == 0:\n            nb_train = len(assigned) - nb_test\n            if nb_train > 0 or len(not_assigned) > 1:\n                expected = min(1, len(not_assigned))\n        if expected > 0:\n            permutation(not_assigned)\n            for e in not_assigned[:expected]:\n                split[e] = 1\n            for e in not_assigned[expected:]:\n                split[e] = 0\n        else:\n            for c in not_assigned:\n                split[c] = 0\n\n    train_set = set(k for k, v in split.items() if v == 0)\n    test_set = set(k for k, v in split.items() if v == 1)\n    train_df = df[df[group].isin(train_set)]\n    test_df = df[df[group].isin(test_set)]\n    return train_df, test_df\n'"
pandas_streaming/df/dataframe.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Defines a streaming dataframe.\n""""""\nfrom io import StringIO, BytesIO\nfrom inspect import isfunction\nimport numpy.random as nrandom\nimport pandas\nfrom pandas.testing import assert_frame_equal\nfrom pandas.io.json import json_normalize\nfrom ..exc import StreamingInefficientException\nfrom .dataframe_split import sklearn_train_test_split, sklearn_train_test_split_streaming\nfrom .dataframe_io_helpers import enumerate_json_items, JsonIterator2Stream\n\n\nclass StreamingDataFrameSchemaError(Exception):\n    """"""\n    Reveals an issue with inconsistant schemas.\n    """"""\n    pass\n\n\nclass StreamingDataFrame:\n    """"""\n    Defines a streaming dataframe.\n    The goal is to reduce the memory footprint.\n    The class takes a function which creates an iterator\n    on :epkg:`dataframe`. We assume this function can\n    be called multiple time. As a matter of fact, the\n    function is called every time the class needs to walk\n    through the stream with the following loop:\n\n    ::\n\n        for df in self:  # self is a StreamingDataFrame\n            # ...\n\n    The constructor cannot receive an iterator otherwise\n    this class would be able to walk through the data\n    only once. The main reason is it is impossible to\n    :epkg:`*py:pickle` (or :epkg:`dill`)\n    an iterator: it cannot be replicated.\n    Instead, the class takes a function which generates\n    an iterator on :epkg:`DataFrame`.\n    Most of the methods returns either a :epkg:`DataFrame`\n    either a @see cl StreamingDataFrame. In the second case,\n    methods can be chained.\n\n    By default, the object checks that the schema remains\n    the same between two chunks. This can be disabled\n    by setting *check_schema=False* in the constructor.\n\n    The user should expect the data to remain stable.\n    Every loop should produce the same data. However,\n    in some situations, it is more efficient not to keep\n    that constraints. Draw a random @see me sample\n    is one of these cases.\n    """"""\n\n    def __init__(self, iter_creation, check_schema=True, stable=True):\n        """"""\n        @param      iter_creation   function which creates an iterator or an instance of\n                                    @see cl StreamingDataFrame\n        @param      check_schema    checks that the schema is the same for every :epkg:`dataframe`\n        @param      stable          indicates if the :epkg:`dataframe` remains the same whenever\n                                    it is walked through\n        """"""\n        if isinstance(iter_creation, StreamingDataFrame):\n            self.iter_creation = iter_creation.iter_creation\n            self.stable = iter_creation.stable\n        else:\n            self.iter_creation = iter_creation\n            self.stable = stable\n        self.check_schema = check_schema\n\n    def is_stable(self, do_check=False, n=10):\n        """"""\n        Tells if the :epkg:`dataframe` is supposed to be stable.\n\n        @param      do_check    do not trust the value sent to the constructor\n        @param      n           number of rows used to check the stability,\n                                None for all rows\n        @return                 boolean\n\n        *do_check=True* means the methods checks the first\n        *n* rows remains the same for two iterations.\n        """"""\n        if do_check:\n            for i, (a, b) in enumerate(zip(self, self)):\n                if n is not None and i >= n:\n                    break\n                try:\n                    assert_frame_equal(a, b)\n                except AssertionError:  # pragma: no cover\n                    return False\n            return True\n        else:\n            return self.stable\n\n    def get_kwargs(self):\n        """"""\n        Returns the parameters used to call the constructor.\n        """"""\n        return dict(check_schema=self.check_schema)\n\n    def train_test_split(self, path_or_buf=None, export_method=""to_csv"",\n                         names=None, streaming=True, partitions=None,\n                         **kwargs):\n        """"""\n        Randomly splits a :epkg:`dataframe` into smaller pieces.\n        The function returns streams of file names.\n        It chooses one of the options from module\n        :mod:`dataframe_split <pandas_streaming.df.dataframe_split>`.\n\n        @param  path_or_buf     a string, a list of strings or buffers, if it is a\n                                string, it must contain ``{}`` like ``partition{}.txt``,\n                                if None, the function returns strings.\n        @param  export_method   method used to store the partitions, by default\n                                :epkg:`pandas:DataFrame:to_csv`, additional parameters\n                                will be given to that function\n        @param  names           partitions names, by default ``(\'train\', \'test\')``\n        @param  kwargs          parameters for the export function and\n                                :epkg:`sklearn:model_selection:train_test_split`.\n        @param  streaming       the function switches to a\n                                streaming version of the algorithm.\n        @param  partitions      splitting partitions\n        @return                 outputs of the exports functions or two\n                                @see cl StreamingDataFrame if path_or_buf is None.\n\n        The streaming version of this algorithm is implemented by function\n        @see fn sklearn_train_test_split_streaming. Its documentation\n        indicates the limitation of the streaming version and gives some\n        insights about the additional parameters.\n        """"""\n        if streaming:\n            if partitions is not None:\n                if len(partitions) != 2:\n                    raise NotImplementedError(\n                        ""Only train and test split is allowed, *partitions* must be of length 2."")\n                kwargs = kwargs.copy()\n                kwargs[\'train_size\'] = partitions[0]\n                kwargs[\'test_size\'] = partitions[1]\n            return sklearn_train_test_split_streaming(self, **kwargs)\n        return sklearn_train_test_split(self, path_or_buf=path_or_buf,\n                                        export_method=export_method,\n                                        names=names, **kwargs)\n\n    @staticmethod\n    def _process_kwargs(kwargs):\n        """"""\n        Filters out parameters for the constructor of this class.\n        """"""\n        kw = {}\n        for k in {\'check_schema\'}:\n            if k in kwargs:\n                kw[k] = kwargs[k]\n                del kwargs[k]\n        return kw\n\n    @staticmethod\n    def read_json(*args, chunksize=100000, flatten=False, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        Reads a :epkg:`json` file or buffer as an iterator\n        on :epkg:`DataFrame`. The signature is the same as\n        :epkg:`pandas:read_json`. The important parameter is\n        *chunksize* which defines the number\n        of rows to parse in a single bloc\n        and it must be defined to return an iterator.\n        If *lines* is True, the function falls back into\n        :epkg:`pandas:read_json`, otherwise it used\n        @see fn enumerate_json_items. If *lines is ``\'stream\'``,\n        @see fn enumerate_json_items is called with parameter\n        ``lines=True``.\n        Parameter *flatten* uses the trick described at\n        ` <https://towardsdatascience.com/flattening-json-objects-in-python-f5343c794b10>`_.\n        Examples::\n\n        .. runpython::\n            :showcode:\n\n            from io import BytesIO\n            from pandas_streaming.df import StreamingDataFrame\n\n            data = b\'\'\'{""a"": 1, ""b"": 2}\n                       {""a"": 3, ""b"": 4}\'\'\'\n            it = StreamingDataFrame.read_json(BytesIO(data), lines=True)\n            dfs = list(it)\n            print(dfs)\n\n        .. runpython::\n            :showcode:\n\n            from io import BytesIO\n            from pandas_streaming.df import StreamingDataFrame\n\n            data = b\'\'\'[{""a"": 1,\n                         ""b"": 2},\n                        {""a"": 3,\n                         ""b"": 4}]\'\'\'\n\n            it = StreamingDataFrame.read_json(BytesIO(data))\n            dfs = list(it)\n            print(dfs)\n        """"""\n        if not isinstance(chunksize, int) or chunksize <= 0:\n            raise ValueError(\n                \'chunksize must be a positive integer\')  # pragma: no cover\n        kwargs_create = StreamingDataFrame._process_kwargs(kwargs)\n        if isinstance(args[0], (list, dict)):\n            if flatten:\n                return StreamingDataFrame.read_df(json_normalize(args[0]), **kwargs_create)\n            return StreamingDataFrame.read_df(args[0], **kwargs_create)\n        if kwargs.get(\'lines\', None) == \'stream\':\n            del kwargs[\'lines\']\n            st = JsonIterator2Stream(enumerate_json_items(\n                args[0], encoding=kwargs.get(\'encoding\', None), lines=True, flatten=flatten))\n            args = args[1:]\n            return StreamingDataFrame(lambda: pandas.read_json(st, *args, chunksize=chunksize, lines=True, **kwargs), **kwargs_create)\n        if kwargs.get(\'lines\', False):\n            if flatten:\n                raise NotImplementedError(\n                    ""flatten==True is implemented with option lines=\'stream\'"")\n            return StreamingDataFrame(lambda: pandas.read_json(*args, chunksize=chunksize, **kwargs), **kwargs_create)\n        st = JsonIterator2Stream(enumerate_json_items(\n            args[0], encoding=kwargs.get(\'encoding\', None), flatten=flatten))\n        args = args[1:]\n        if \'lines\' in kwargs:\n            del kwargs[\'lines\']\n        return StreamingDataFrame(lambda: pandas.read_json(st, *args, chunksize=chunksize, lines=True, **kwargs), **kwargs_create)\n\n    @staticmethod\n    def read_csv(*args, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        Reads a :epkg:`csv` file or buffer\n        as an iterator on :epkg:`DataFrame`.\n        The signature is the same as :epkg:`pandas:read_csv`.\n        The important parameter is *chunksize* which defines the number\n        of rows to parse in a single bloc. If not specified,\n        it will be equal to 100000.\n        """"""\n        if not kwargs.get(\'iterator\', True):\n            raise ValueError(""If specified, iterator must be True."")\n        if not kwargs.get(\'chunksize\', 100000):\n            raise ValueError(""If specified, chunksize must not be None."")\n        kwargs_create = StreamingDataFrame._process_kwargs(kwargs)\n        kwargs[\'iterator\'] = True\n        if \'chunksize\' not in kwargs:\n            kwargs[\'chunksize\'] = 100000\n        return StreamingDataFrame(lambda: pandas.read_csv(*args, **kwargs), **kwargs_create)\n\n    @staticmethod\n    def read_str(text, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        Reads a :epkg:`DataFrame` as an iterator on :epkg:`DataFrame`.\n        The signature is the same as :epkg:`pandas:read_csv`.\n        The important parameter is *chunksize* which defines the number\n        of rows to parse in a single bloc.\n        """"""\n        if not kwargs.get(\'iterator\', True):\n            raise ValueError(""If specified, iterator must be True."")\n        if not kwargs.get(\'chunksize\', 100000):\n            raise ValueError(""If specified, chunksize must not be None."")\n        kwargs_create = StreamingDataFrame._process_kwargs(kwargs)\n        kwargs[\'iterator\'] = True\n        if \'chunksize\' not in kwargs:\n            kwargs[\'chunksize\'] = 100000\n        if isinstance(text, str):\n            buffer = StringIO(text)\n        else:\n            buffer = BytesIO(text)\n        return StreamingDataFrame(\n            lambda: pandas.read_csv(buffer, **kwargs), **kwargs_create)\n\n    @staticmethod\n    def read_df(df, chunksize=None, check_schema=True) -> \'StreamingDataFrame\':\n        """"""\n        Splits a :epkg:`DataFrame` into small chunks mostly for\n        unit testing purposes.\n\n        @param      df              :epkg:`DataFrame`\n        @param      chunksize       number rows per chunks (// 10 by default)\n        @param      check_schema    check schema between two iterations\n        @return                     iterator on @see cl StreamingDataFrame\n        """"""\n        if chunksize is None:\n            if hasattr(df, \'shape\'):\n                chunksize = df.shape[0]\n            else:\n                raise NotImplementedError(\n                    ""Cannot retrieve size to infer chunksize for type={0}"".format(type(df)))\n\n        if hasattr(df, \'shape\'):\n            size = df.shape[0]\n        else:\n            raise NotImplementedError(\n                ""Cannot retrieve size for type={0}"".format(type(df)))\n\n        def local_iterator():\n            ""local iterator""\n            for i in range(0, size, chunksize):\n                end = min(size, i + chunksize)\n                yield df[i:end].copy()\n        return StreamingDataFrame(local_iterator, check_schema=check_schema)\n\n    def __iter__(self):\n        """"""\n        Iterator on a large file with a sliding window.\n        Each windows is a :epkg:`DataFrame`.\n        The method stores a copy of the initial iterator\n        and restores it after the end of the iterations.\n        If *check_schema* was enabled when calling the constructor,\n        the method checks that every :epkg:`DataFrame`\n        follows the same schema as the first chunck.\n\n        Even with a big chunk size, it might happen\n        that consecutive chunks might detect different type\n        for one particular column. An error message shows up\n        saying ``Column types are different after row``\n        with more information about the column which failed.\n        In that case, :epkg:`pandas:DataFrame.read_csv` can overwrite\n        the type on one column by specifying\n        ``dtype={column_name: new_type}``. It frequently happens\n        when a string column has many missing values.\n        """"""\n        iters = self.iter_creation()\n        sch = None\n        rows = 0\n        for it in iters:\n            if sch is None:\n                sch = (list(it.columns), list(it.dtypes))\n            elif self.check_schema:\n                if list(it.columns) != sch[0]:  # pylint: disable=E1136\n                    msg = \'Column names are different after row {0}\\nFirst   chunk: {1}\\nCurrent chunk: {2}\'\n                    raise StreamingDataFrameSchemaError(\n                        msg.format(rows, sch[0], list(it.columns)))  # pylint: disable=E1136\n                if list(it.dtypes) != sch[1]:  # pylint: disable=E1136\n                    errdf = pandas.DataFrame(\n                        dict(names=sch[0], schema1=sch[1], schema2=list(it.dtypes)))  # pylint: disable=E1136\n                    tdf = StringIO()\n                    errdf[\'diff\'] = errdf[\'schema2\'] != errdf[\'schema1\']\n                    errdf = errdf[errdf[\'diff\']]\n                    errdf.to_csv(tdf, sep="","")\n                    msg = \'Column types are different after row {0}\\n{1}\'\n                    raise StreamingDataFrameSchemaError(\n                        msg.format(rows, tdf.getvalue()))\n            rows += it.shape[0]\n            yield it\n\n    def sort_values(self, *args, **kwargs):\n        """"""\n        Not implemented.\n        """"""\n        raise StreamingInefficientException(StreamingDataFrame.sort_values)\n\n    @property\n    def shape(self):\n        """"""\n        This is the kind of operations you do not want to do\n        when a file is large because it goes through the whole\n        stream just to get the number of rows.\n        """"""\n        nl, nc = 0, 0\n        for it in self:\n            nc = max(it.shape[1], nc)\n            nl += it.shape[0]\n        return nl, nc\n\n    @property\n    def columns(self):\n        """"""\n        See :epkg:`pandas:DataFrame:columns`.\n        """"""\n        for it in self:\n            return it.columns\n\n    @property\n    def dtypes(self):\n        """"""\n        See :epkg:`pandas:DataFrame:dtypes`.\n        """"""\n        for it in self:\n            return it.dtypes\n\n    def to_csv(self, path_or_buf=None, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        Saves the :epkg:`DataFrame` into string.\n        See :epkg:`pandas:DataFrame.to_csv`.\n        """"""\n        if path_or_buf is None:\n            st = StringIO()\n            close = False\n        elif isinstance(path_or_buf, str):\n            st = open(path_or_buf, ""w"", encoding=kwargs.get(\'encoding\'))\n            close = True\n        else:\n            st = path_or_buf\n            close = False\n\n        for df in self:\n            df.to_csv(st, **kwargs)\n            kwargs[\'header\'] = False\n\n        if close:\n            st.close()\n        if isinstance(st, StringIO):\n            return st.getvalue()\n        return path_or_buf\n\n    def to_dataframe(self) -> pandas.DataFrame:\n        """"""\n        Converts everything into a single :epkg:`DataFrame`.\n        """"""\n        return pandas.concat(self, axis=0)\n\n    def to_df(self) -> pandas.DataFrame:\n        """"""\n        Converts everything into a single :epkg:`DataFrame`.\n        """"""\n        return self.to_dataframe()\n\n    def iterrows(self):\n        """"""\n        See :epkg:`pandas:DataFrame:iterrows`.\n        """"""\n        for df in self:\n            for it in df.iterrows():\n                yield it\n\n    def head(self, n=5) -> pandas.DataFrame:\n        """"""\n        Returns the first rows as a :epkg:`DataFrame`.\n        """"""\n        st = []\n        total = 0\n        for df in self:\n            h = df.head(n=n)\n            total += h.shape[0]\n            st.append(h)\n            if total >= n:\n                break\n            n -= h.shape[0]\n        if len(st) == 1:\n            return st[0]\n        if len(st) == 0:\n            return None\n        return pandas.concat(st, axis=0)\n\n    def tail(self, n=5) -> pandas.DataFrame:\n        """"""\n        Returns the last rows as a :epkg:`DataFrame`.\n        The size of chunks must be greater than ``n`` to\n        get ``n`` lines. This method is not efficient\n        because the whole dataset must be walked through.\n        """"""\n        for df in self:\n            h = df.tail(n=n)\n        return h\n\n    def where(self, *args, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        Applies :epkg:`pandas:DataFrame:where`.\n        *inplace* must be False.\n        This function returns a @see cl StreamingDataFrame.\n        """"""\n        kwargs[\'inplace\'] = False\n        return StreamingDataFrame(\n            lambda: map(lambda df: df.where(*args, **kwargs), self),\n            **self.get_kwargs())\n\n    def sample(self, reservoir=False, cache=False, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        See :epkg:`pandas:DataFrame:sample`.\n        Only *frac* is available, otherwise choose\n        @see me reservoir_sampling.\n        This function returns a @see cl StreamingDataFrame.\n\n        @param      reservoir   use `reservoir sampling <https://en.wikipedia.org/wiki/Reservoir_sampling>`_\n        @param      cache       cache the sample\n        @param      kwargs      additional parameters for :epkg:`pandas:DataFrame:sample`\n\n        If *cache* is True, the sample is cached (assuming it holds in memory).\n        The second time an iterator walks through the\n        """"""\n        if reservoir or \'n\' in kwargs:\n            if \'frac\' in kwargs:\n                raise ValueError(\n                    \'frac cannot be specified for reservoir sampling.\')\n            return self._reservoir_sampling(cache=cache, n=kwargs[\'n\'], random_state=kwargs.get(\'random_state\'))\n        if cache:\n            sdf = self.sample(cache=False, **kwargs)\n            df = sdf.to_df()\n            return StreamingDataFrame.read_df(df, chunksize=df.shape[0])\n        return StreamingDataFrame(lambda: map(lambda df: df.sample(**kwargs), self), **self.get_kwargs(), stable=False)\n\n    def _reservoir_sampling(self, cache=True, n=1000, random_state=None) -> \'StreamingDataFrame\':\n        """"""\n        Uses the `reservoir sampling <https://en.wikipedia.org/wiki/Reservoir_sampling>`_\n        algorithm to draw a random sample with exactly *n* samples.\n\n        @param      cache           cache the sample\n        @param      n               number of observations to keep\n        @param      random_state    sets the random_state\n        @return                     @see cl StreamingDataFrame\n\n        .. warning::\n            The sample is split by chunks of size 1000.\n            This parameter is not yet exposed.\n        """"""\n        if not cache:\n            raise ValueError(\n                ""cache=False is not available for reservoir sampling."")\n        indices = []\n        seen = 0\n        for i, df in enumerate(self):\n            for ir, _ in enumerate(df.iterrows()):\n                seen += 1\n                if len(indices) < n:\n                    indices.append((i, ir))\n                else:\n                    x = nrandom.random()  # pylint: disable=E1101\n                    if x * n < (seen - n):\n                        k = nrandom.randint(0, len(indices) - 1)\n                        indices[k] = (i, ir)  # pylint: disable=E1126\n        indices = set(indices)\n\n        def reservoir_iterate(sdf, indices, chunksize):\n            ""iterator""\n            buffer = []\n            for i, df in enumerate(self):\n                for ir, row in enumerate(df.iterrows()):\n                    if (i, ir) in indices:\n                        buffer.append(row)\n                        if len(buffer) >= chunksize:\n                            yield pandas.DataFrame(buffer)\n                            buffer.clear()\n            if len(buffer) > 0:\n                yield pandas.DataFrame(buffer)\n\n        return StreamingDataFrame(\n            lambda: reservoir_iterate(sdf=self, indices=indices, chunksize=1000))\n\n    def apply(self, *args, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        Applies :epkg:`pandas:DataFrame:apply`.\n        This function returns a @see cl StreamingDataFrame.\n        """"""\n        return StreamingDataFrame(\n            lambda: map(lambda df: df.apply(*args, **kwargs), self),\n            **self.get_kwargs())\n\n    def applymap(self, *args, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        Applies :epkg:`pandas:DataFrame:applymap`.\n        This function returns a @see cl StreamingDataFrame.\n        """"""\n        return StreamingDataFrame(\n            lambda: map(lambda df: df.applymap(*args, **kwargs), self),\n            **self.get_kwargs())\n\n    def merge(self, right, **kwargs) -> \'StreamingDataFrame\':\n        """"""\n        Merges two @see cl StreamingDataFrame and returns @see cl StreamingDataFrame.\n        *right* can be either a @see cl StreamingDataFrame or simply\n        a :epkg:`pandas:DataFrame`. It calls :epkg:`pandas:DataFrame:merge` in\n        a double loop, loop on *self*, loop on *right*.\n        """"""\n        if isinstance(right, pandas.DataFrame):\n            return self.merge(StreamingDataFrame.read_df(right, chunksize=right.shape[0]), **kwargs)\n\n        def iterator_merge(sdf1, sdf2, **kw):\n            ""iterate on dataframes""\n            for df1 in sdf1:\n                for df2 in sdf2:\n                    df = df1.merge(df2, **kw)\n                    yield df\n\n        return StreamingDataFrame(\n            lambda: iterator_merge(self, right, **kwargs), **self.get_kwargs())\n\n    def concat(self, others, axis=0) -> \'StreamingDataFrame\':\n        """"""\n        Concatenates :epkg:`dataframes`. The function ensures all :epkg:`pandas:DataFrame`\n        or @see cl StreamingDataFrame share the same columns (name and type).\n        Otherwise, the function fails as it cannot guess the schema without\n        walking through all :epkg:`dataframes`.\n\n        @param  others      list, enumeration, :epkg:`pandas:DataFrame`\n        @return             @see cl StreamingDataFrame\n        """"""\n        if axis == 1:\n            return self._concath(others)\n        if axis == 0:\n            return self._concatv(others)\n        raise ValueError(""axis must be 0 or 1"")  # pragma: no cover\n\n    def _concath(self, others):\n        if not isinstance(others, list):\n            others = [others]\n\n        def iterateh(self, others):\n            cols = tuple([self] + others)\n            for dfs in zip(*cols):\n                nrows = [_.shape[0] for _ in dfs]\n                if min(nrows) != max(nrows):\n                    raise RuntimeError(\n                        ""StreamingDataFram cannot merge DataFrame with different size or chunksize"")\n                yield pandas.concat(list(dfs), axis=1)\n\n        return StreamingDataFrame(lambda: iterateh(self, others), **self.get_kwargs())\n\n    def _concatv(self, others):\n\n        def iterator_concat(this, lothers):\n            ""iterator on dataframes""\n            columns = None\n            dtypes = None\n            for df in this:\n                if columns is None:\n                    columns = df.columns\n                    dtypes = df.dtypes\n                yield df\n            for obj in lothers:\n                check = True\n                for i, df in enumerate(obj):\n                    if check:\n                        if list(columns) != list(df.columns):\n                            raise ValueError(\n                                ""Frame others[{0}] do not have the same column names or the same order."".format(i))\n                        if list(dtypes) != list(df.dtypes):\n                            raise ValueError(\n                                ""Frame others[{0}] do not have the same column types."".format(i))\n                        check = False\n                    yield df\n\n        if isinstance(others, pandas.DataFrame):\n            others = [others]\n        elif isinstance(others, StreamingDataFrame):\n            others = [others]\n\n        def change_type(obj):\n            ""change column type""\n            if isinstance(obj, pandas.DataFrame):\n                return StreamingDataFrame.read_df(obj, obj.shape[0])\n            else:\n                return obj\n\n        others = list(map(change_type, others))\n        return StreamingDataFrame(\n            lambda: iterator_concat(self, others), **self.get_kwargs())\n\n    def groupby(self, by=None, lambda_agg=None, lambda_agg_agg=None,\n                in_memory=True, **kwargs) -> pandas.DataFrame:\n        """"""\n        Implements the streaming :epkg:`pandas:DataFrame:groupby`.\n        We assume the result holds in memory. The out-of-memory is\n        not implemented yet.\n\n        @param      by              see :epkg:`pandas:DataFrame:groupby`\n        @param      in_memory       in-memory algorithm\n        @param      lambda_agg      aggregation function, *sum* by default\n        @param      lambda_agg_agg  to aggregate the aggregations, *sum* by default\n        @param      kwargs          additional parameters for :epkg:`pandas:DataFrame:groupby`\n        @return                     :epkg:`pandas:DataFrame`\n\n        As the input @see cl StreamingDataFrame does not necessarily hold\n        in memory, the aggregation must be done at every iteration.\n        There are two levels of aggregation: one to reduce every iterated\n        :epkg:`dataframe`, another one to combine all the reduced :epkg:`dataframes`.\n        This second one is always a **sum**.\n        As a consequence, this function should not compute any *mean* or *count*,\n        only *sum* because we do not know the size of each iterated\n        :epkg:`dataframe`. To compute an average, sum and weights must be\n        aggregated.\n\n        Parameter *lambda_agg* is ``lambda gr: gr.sum()`` by default.\n        It could also be ``lambda gr: gr.max()`` or\n        ``lambda gr: gr.min()`` but not ``lambda gr: gr.mean()``\n        as it would lead to incoherent results.\n\n        .. exref::\n            :title: StreamingDataFrame and groupby\n            :tag: streaming\n\n            Here is an example which shows how to write a simple *groupby*\n            with :epkg:`pandas` and @see cl StreamingDataFrame.\n\n            .. runpython::\n                :showcode:\n\n                from pandas import DataFrame\n                from pandas_streaming.df import StreamingDataFrame\n\n                df = DataFrame(dict(A=[3, 4, 3], B=[5,6, 7]))\n                sdf = StreamingDataFrame.read_df(df)\n\n                # The following:\n                print(sdf.groupby(""A"", lambda gr: gr.sum()))\n\n                # Is equivalent to:\n                print(df.groupby(""A"").sum())\n        """"""\n        if not in_memory:\n            raise NotImplementedError(\n                ""Out-of-memory group by is not implemented."")\n        if lambda_agg is None:\n            def lambda_agg_(gr):\n                ""sum""\n                return gr.sum()\n            lambda_agg = lambda_agg_\n        if lambda_agg_agg is None:\n            def lambda_agg_agg_(gr):\n                ""sum""\n                return gr.sum()\n            lambda_agg_agg = lambda_agg_agg_\n        ckw = kwargs.copy()\n        ckw[""as_index""] = False\n\n        agg = []\n        for df in self:\n            gr = df.groupby(by=by, **ckw)\n            agg.append(lambda_agg(gr))\n        conc = pandas.concat(agg, sort=False)\n        return lambda_agg_agg(conc.groupby(by=by, **kwargs))\n\n    def groupby_streaming(self, by=None, lambda_agg=None, lambda_agg_agg=None, in_memory=True,\n                          strategy=\'cum\', **kwargs) -> pandas.DataFrame:\n        """"""\n        Implements the streaming :epkg:`pandas:DataFrame:groupby`.\n        We assume the result holds in memory. The out-of-memory is\n        not implemented yet.\n\n        @param      by              see :epkg:`pandas:DataFrame:groupby`\n        @param      in_memory       in-memory algorithm\n        @param      lambda_agg      aggregation function, *sum* by default\n        @param      lambda_agg_agg  to aggregate the aggregations, *sum* by default\n        @param      kwargs          additional parameters for :epkg:`pandas:DataFrame:groupby`\n        @param      strategy        ``\'cum\'``, or ``\'streaming\'``,\n                                    see below\n        @return                     :epkg:`pandas:DataFrame`\n\n        As the input @see cl StreamingDataFrame does not necessarily hold\n        in memory, the aggregation must be done at every iteration.\n        There are two levels of aggregation: one to reduce every iterated\n        :epkg:`dataframe`, another one to combine all the reduced :epkg:`dataframes`.\n        This second one is always a **sum**.\n        As a consequence, this function should not compute any *mean* or *count*,\n        only *sum* because we do not know the size of each iterated\n        :epkg:`dataframe`. To compute an average, sum and weights must be\n        aggregated.\n\n        Parameter *lambda_agg* is ``lambda gr: gr.sum()`` by default.\n        It could also be ``lambda gr: gr.max()`` or\n        ``lambda gr: gr.min()`` but not ``lambda gr: gr.mean()``\n        as it would lead to incoherent results.\n\n        Parameter *strategy* allows three scenarios.\n        First one if ``strategy is None`` goes through\n        the whole datasets to produce a final :epkg:`DataFrame`.\n        Second if ``strategy==\'cum\'`` returns a\n        @see cl StreamingDataFrame, each iteration produces\n        the current status of the *group by*. Last case,\n        ``strategy==\'streaming\'`` produces :epkg:`DataFrame`\n        which must be concatenated into a single :epkg:`DataFrame`\n        and grouped again to get the results.\n\n        .. exref::\n            :title: StreamingDataFrame and groupby\n            :tag: streaming\n\n            Here is an example which shows how to write a simple *groupby*\n            with :epkg:`pandas` and @see cl StreamingDataFrame.\n\n            .. runpython::\n                :showcode:\n\n                from pandas import DataFrame\n                from pandas_streaming.df import StreamingDataFrame\n                from pandas_streaming.data import dummy_streaming_dataframe\n\n                df20 = dummy_streaming_dataframe(20).to_dataframe()\n                df20[""key""] = df20[""cint""].apply(lambda i: i % 3 == 0)\n                sdf20 = StreamingDataFrame.read_df(df20, chunksize=5)\n                sgr = sdf20.groupby_streaming(""key"", lambda gr: gr.sum(), strategy=\'cum\', as_index=False)\n                for gr in sgr:\n                    print()\n                    print(gr)\n        """"""\n        if not in_memory:\n            raise NotImplementedError(\n                ""Out-of-memory group by is not implemented."")\n        if lambda_agg is None:\n            def lambda_agg_(gr):\n                ""sum""\n                return gr.sum()\n            lambda_agg = lambda_agg_\n        if lambda_agg_agg is None:\n            def lambda_agg_agg_(gr):\n                ""sum""\n                return gr.sum()\n            lambda_agg_agg = lambda_agg_agg_\n        ckw = kwargs.copy()\n        ckw[""as_index""] = False\n\n        if strategy == \'cum\':\n            def iterate_cum():\n                agg = None\n                for df in self:\n                    gr = df.groupby(by=by, **ckw)\n                    gragg = lambda_agg(gr)\n                    if agg is None:\n                        yield lambda_agg_agg(gragg.groupby(by=by, **kwargs))\n                        agg = gragg\n                    else:\n                        lagg = pandas.concat([agg, gragg], sort=False)\n                        yield lambda_agg_agg(lagg.groupby(by=by, **kwargs))\n                        agg = lagg\n            return StreamingDataFrame(lambda: iterate_cum(), **self.get_kwargs())\n\n        if strategy == \'streaming\':\n            def iterate_streaming():\n                for df in self:\n                    gr = df.groupby(by=by, **ckw)\n                    gragg = lambda_agg(gr)\n                    yield lambda_agg(gragg.groupby(by=by, **kwargs))\n            return StreamingDataFrame(lambda: iterate_streaming(), **self.get_kwargs())\n\n        raise ValueError(  # pragma: no cover\n            ""Unknown strategy \'{0}\'"".format(strategy))\n\n    def ensure_dtype(self, df, dtypes):\n        """"""\n        Ensures the :epkg:`dataframe` *df* has types indicated in dtypes.\n        Changes it if not.\n\n        @param      df      dataframe\n        @param      dtypes  list of types\n        @return             updated?\n        """"""\n        ch = False\n        cols = df.columns\n        for i, (has, exp) in enumerate(zip(df.dtypes, dtypes)):\n            if has != exp:\n                name = cols[i]\n                df[name] = df[name].astype(exp)\n                ch = True\n        return ch\n\n    def __getitem__(self, *args):\n        """"""\n        Implements some of the functionalities :epkg:`pandas`\n        offers for the operator ``[]``.\n        """"""\n        if len(args) != 1:\n            raise NotImplementedError(""Only a list of columns is supported."")\n        cols = args[0]\n        if not isinstance(cols, list):\n            raise NotImplementedError(""Only a list of columns is supported."")\n\n        def iterate_cols(sdf):\n            ""iterate on columns""\n            for df in sdf:\n                yield df[cols]\n\n        return StreamingDataFrame(lambda: iterate_cols(self), **self.get_kwargs())\n\n    def add_column(self, col, value):\n        """"""\n        Implements some of the functionalities :epkg:`pandas`\n        offers for the operator ``[]``.\n\n        @param      col             new column\n        @param      value           @see cl StreamingDataFrame or a lambda function\n        @return                     @see cl StreamingDataFrame\n\n        ..note::\n\n            If value is a @see cl StreamingDataFrame,\n            *chunksize* must be the same for both.\n\n        .. exref::\n            :title: Add a new column to a StreamingDataFrame\n            :tag: streaming\n\n            .. runpython::\n                :showcode:\n\n                from pandas import DataFrame\n                from pandas_streaming.df import StreamingDataFrame\n\n                df = DataFrame(data=dict(X=[4.5, 6, 7], Y=[""a"", ""b"", ""c""]))\n                sdf = StreamingDataFrame.read_df(df)\n                sdf2 = sdf.add_column(""d"", lambda row: int(1))\n                print(sdf2.to_dataframe())\n\n                sdf2 = sdf.add_column(""d"", lambda row: int(1))\n                print(sdf2.to_dataframe())\n\n        """"""\n        if not isinstance(col, str):\n            raise NotImplementedError(\n                ""Only a column as a string is supported."")\n\n        if isfunction(value):\n            def iterate_fct(self, value, col):\n                ""iterate on rows""\n                for df in self:\n                    dfc = df.copy()\n                    dfc.insert(dfc.shape[1], col, dfc.apply(value, axis=1))\n                    yield dfc\n\n            return StreamingDataFrame(lambda: iterate_fct(self, value, col), **self.get_kwargs())\n\n        if isinstance(value, (pandas.Series, pandas.DataFrame, StreamingDataFrame)):\n            raise NotImplementedError(\n                ""Unable set a new column based on a datadframe."")\n\n        def iterate_cst(self, value, col):\n            ""iterate on rows""\n            for df in self:\n                dfc = df.copy()\n                dfc[col] = value\n                yield dfc\n\n        return StreamingDataFrame(\n            lambda: iterate_cst(self, value, col), **self.get_kwargs())\n\n    def fillna(self, **kwargs):\n        """"""\n        Replaces the missing values, calls\n        :epkg:`pandas:DataFrame:fillna`.\n\n        @param      kwargs      see :epkg:`pandas:DataFrame:fillna`\n        @return                 @see cl StreamingDataFrame\n\n        .. warning::\n            The function does not check what happens at the\n            limit of every chunk of data. Anything but a constant value\n            will probably have an inconsistent behaviour.\n        """"""\n\n        def iterate_na(self, **kwargs):\n            ""iterate on rows""\n            if kwargs.get(\'inplace\', True):\n                kwargs[\'inplace\'] = True\n                for df in self:\n                    df.fillna(**kwargs)\n                    yield df\n            else:\n                for df in self:\n                    yield df.fillna(**kwargs)\n\n        return StreamingDataFrame(\n            lambda: iterate_na(self, **kwargs), **self.get_kwargs())\n'"
pandas_streaming/df/dataframe_helpers.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Helpers for dataframes.\n""""""\nimport hashlib\nimport struct\nimport warnings\nimport numpy\nfrom pandas import DataFrame, Index\n\n\ndef hash_str(c, hash_length):\n    """"""\n    Hashes a string.\n\n    @param      c               value to hash\n    @param      hash_length     hash_length\n    @return                     string\n    """"""\n    if isinstance(c, float):\n        if numpy.isnan(c):\n            return c\n        else:\n            raise ValueError(""numpy.nan expected, not {0}"".format(c))\n    else:\n        m = hashlib.sha256()\n        m.update(c.encode(""utf-8""))\n        r = m.hexdigest()\n        if len(r) >= hash_length:\n            return r[:hash_length]\n        return r\n\n\ndef hash_int(c, hash_length):\n    """"""\n    Hashes an integer into an integer.\n\n    @param      c               value to hash\n    @param      hash_length     hash_length\n    @return                     int\n    """"""\n    if isinstance(c, float):\n        if numpy.isnan(c):\n            return c\n        else:\n            raise ValueError(""numpy.nan expected, not {0}"".format(c))\n    else:\n        b = struct.pack(""i"", c)\n        m = hashlib.sha256()\n        m.update(b)\n        r = m.hexdigest()\n        if len(r) >= hash_length:\n            r = r[:hash_length]\n        return int(r, 16) % (10 ** 8)\n\n\ndef hash_float(c, hash_length):\n    """"""\n    Hashes a float into a float.\n\n    @param      c               value to hash\n    @param      hash_length     hash_length\n    @return                     int\n    """"""\n    if numpy.isnan(c):\n        return c\n    else:\n        b = struct.pack(""d"", c)\n        m = hashlib.sha256()\n        m.update(b)\n        r = m.hexdigest()\n        if len(r) >= hash_length:\n            r = r[:hash_length]\n        i = int(r, 16) % (2 ** 53)\n        return float(i)\n\n\ndef dataframe_hash_columns(df, cols=None, hash_length=10, inplace=False):\n    """"""\n    Hashes a set of columns in a dataframe.\n    Keeps the same type. Skips missing values.\n\n    @param      df          dataframe\n    @param      cols        columns to hash or None for alls.\n    @param      hash_length for strings only, length of the hash\n    @param      inplace     modifies inplace\n    @return                 new dataframe\n\n    This might be useful to anonimized data before\n    making it public.\n\n    .. exref::\n        :title: Hashes a set of columns in a dataframe\n        :tag: dataframe\n\n        .. runpython::\n            :showcode:\n\n            import pandas\n            from pandas_streaming.df import dataframe_hash_columns\n            df = pandas.DataFrame([dict(a=1, b=""e"", c=5.6, ind=""a1"", ai=1),\n                                   dict(b=""f"", c=5.7, ind=""a2"", ai=2),\n                                   dict(a=4, b=""g"", ind=""a3"", ai=3),\n                                   dict(a=8, b=""h"", c=5.9, ai=4),\n                                   dict(a=16, b=""i"", c=6.2, ind=""a5"", ai=5)])\n            print(df)\n            print(\'--------------\')\n            df2 = dataframe_hash_columns(df)\n            print(df2)\n    """"""\n    if cols is None:\n        cols = list(df.columns)\n\n    if not inplace:\n        df = df.copy()\n\n    def hash_intl(c):\n        ""hash int""\n        return hash_int(c, hash_length)\n\n    def hash_strl(c):\n        ""hash string""\n        return hash_str(c, hash_length)\n\n    def hash_floatl(c):\n        ""hash float""\n        return hash_float(c, hash_length)\n\n    coltype = {n: t for n, t in zip(  # pylint: disable=R1721\n        df.columns, df.dtypes)}  # pylint: disable=R1721\n    for c in cols:\n        t = coltype[c]\n        if t == int:\n            df[c] = df[c].apply(hash_intl)\n        elif t == numpy.int64:\n            df[c] = df[c].apply(lambda x: numpy.int64(hash_intl(x)))\n        elif t == float:\n            df[c] = df[c].apply(hash_floatl)\n        elif t == object:\n            df[c] = df[c].apply(hash_strl)\n        else:\n            raise NotImplementedError(\n                ""Conversion of type {0} in column \'{1}\' is not implemented"".format(t, c))\n\n    return df\n\n\ndef dataframe_unfold(df, col, new_col=None, sep="",""):\n    """"""\n    One column may contain concatenated values.\n    This function splits these values and multiplies the\n    rows for each split value.\n\n    @param      df      dataframe\n    @param      col     column with the concatenated values (strings)\n    @param      new_col new column name, if None, use default value.\n    @param      sep     separator\n    @return             a new dataframe\n\n    .. exref::\n        :title: Unfolds a column of a dataframe.\n        :tag: dataframe\n\n        .. runpython::\n            :showcode:\n\n            import pandas\n            import numpy\n            from pandas_streaming.df import dataframe_unfold\n\n            df = pandas.DataFrame([dict(a=1, b=""e,f""),\n                                   dict(a=2, b=""g""),\n                                   dict(a=3)])\n            print(df)\n            df2 = dataframe_unfold(df, ""b"")\n            print(\'----------\')\n            print(df2)\n\n            # To fold:\n            folded = df2.groupby(\'a\').apply(lambda row: \',\'.join(row[\'b_unfold\'].dropna()) \\\\\n                                            if len(row[\'b_unfold\'].dropna()) > 0 else numpy.nan)\n            print(\'----------\')\n            print(folded)\n    """"""\n    if new_col is None:\n        col_name = col + ""_unfold""\n    else:\n        col_name = new_col\n    temp_col = \'__index__\'\n    while temp_col in df.columns:\n        temp_col += ""_""\n    rows = []\n    for i, v in enumerate(df[col]):\n        if isinstance(v, str):\n            spl = v.split(sep)\n            for vs in spl:\n                rows.append({col: v, col_name: vs, temp_col: i})\n        else:\n            rows.append({col: v, col_name: v, temp_col: i})\n    df = df.copy()\n    df[temp_col] = list(range(df.shape[0]))\n    dfj = DataFrame(rows)\n    res = df.merge(dfj, on=[col, temp_col])\n    return res.drop(temp_col, axis=1).copy()\n\n\ndef dataframe_shuffle(df, random_state=None):\n    """"""\n    Shuffles a dataframe.\n\n    @param      df              :epkg:`pandas:DataFrame`\n    @param      random_state    seed\n    @return                     new :epkg:`pandas:DataFrame`\n\n    .. exref::\n        :title: Shuffles the rows of a dataframe\n        :tag: dataframe\n\n        .. runpython::\n            :showcode:\n\n            import pandas\n            from pandas_streaming.df import dataframe_shuffle\n\n            df = pandas.DataFrame([dict(a=1, b=""e"", c=5.6, ind=""a1""),\n                                   dict(a=2, b=""f"", c=5.7, ind=""a2""),\n                                   dict(a=4, b=""g"", c=5.8, ind=""a3""),\n                                   dict(a=8, b=""h"", c=5.9, ind=""a4""),\n                                   dict(a=16, b=""i"", c=6.2, ind=""a5"")])\n            print(df)\n            print(\'----------\')\n\n            shuffled = dataframe_shuffle(df, random_state=0)\n            print(shuffled)\n    """"""\n    if random_state is not None:\n        state = numpy.random.RandomState(random_state)\n        permutation = state.permutation\n    else:\n        permutation = numpy.random.permutation\n    ori_cols = list(df.columns)\n    scols = set(ori_cols)\n\n    no_index = df.reset_index(drop=False)\n    keep_cols = [_ for _ in no_index.columns if _ not in scols]\n    index = no_index.index\n    index = permutation(index)\n    shuffled = no_index.iloc[index, :]\n    res = shuffled.set_index(keep_cols)[ori_cols]\n    res.index.names = df.index.names\n    return res\n\n\ndef pandas_fillna(df, by, hasna=None, suffix=None):\n    """"""\n    Replaces the :epkg:`nan` values for something not :epkg:`nan`.\n    Mostly used by @see fn pandas_groupby_nan.\n\n    @param      df      dataframe\n    @param      by      list of columns for which we need to replace nan\n    @param      hasna   None or list of columns for which we need to replace NaN\n    @param      suffix  use a prefix for the NaN value\n    @return             list of values chosen for each column, new dataframe (new copy)\n    """"""\n    suffix = suffix if suffix else ""\xc2\xb2""\n    df = df.copy()\n    rep = {}\n    for c in by:\n        if hasna is not None and c not in hasna:\n            continue\n        if df[c].dtype in (str, bytes, object):\n            se = set(df[c].dropna())\n            val = se.pop()\n            if isinstance(val, str):\n                cst = suffix\n                val = """"\n            elif isinstance(val, bytes):\n                cst = b""_""\n            else:\n                raise TypeError(  # pragma: no cover\n                    ""Unable to determine a constant for type=\'{0}\' dtype=\'{1}\'"".format(\n                        val, df[c].dtype))\n            val += cst\n            while val in se:\n                val += suffix\n            df[c].fillna(val, inplace=True)\n            rep[c] = val\n        else:\n            dr = df[c].dropna()\n            mi = abs(dr.min())\n            ma = abs(dr.max())\n            val = ma + mi\n            if val <= ma:\n                raise ValueError(  # pragma: no cover\n                    ""Unable to find a different value for column \'{0}\': min={1} max={2}""\n                    """".format(val, mi, ma))\n            df[c].fillna(val, inplace=True)\n            rep[c] = val\n    return rep, df\n\n\ndef pandas_groupby_nan(df, by, axis=0, as_index=False, suffix=None, nanback=True, **kwargs):\n    """"""\n    Does a *groupby* including keeping missing values (:epkg:`nan`).\n\n    @param      df          dataframe\n    @param      by          column or list of columns\n    @param      axis        only 0 is allowed\n    @param      as_index    should be False\n    @param      suffix      None or a string\n    @param      nanback     put :epkg:`nan` back in the index,\n                            otherwise it leaves a replacement for :epkg:`nan`.\n                            (does not work when grouping by multiple columns)\n    @param      kwargs      other parameters sent to\n                            `groupby <http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html>`_\n    @return                 groupby results\n\n    See `groupby and missing values <http://pandas-docs.github.io/pandas-docs-travis/groupby.html#na-and-nat-group-handling>`_.\n    If no :epkg:`nan` is detected, the function falls back in regular\n    :epkg:`pandas:DataFrame:groupby` which has the following\n    behavior.\n\n    .. exref::\n        :title: Group a dataframe by one column including nan values\n        :tag: dataframe\n\n        The regular :epkg:`pandas:dataframe:GroupBy` of a\n        :epkg:`pandas:DataFrame` removes every :epkg:`nan`\n        values from the index.\n\n        .. runpython::\n            :showcode:\n\n            from pandas import DataFrame\n\n            data = [dict(a=2, ind=""a"", n=1), dict(a=2, ind=""a""), dict(a=3, ind=""b""), dict(a=30)]\n            df = DataFrame(data)\n            print(df)\n            gr = df.groupby([""ind""]).sum()\n            print(gr)\n\n        Function @see fn pandas_groupby_nan modifies keeps them.\n\n        .. runpython::\n            :showcode:\n\n            from pandas import DataFrame\n            from pandas_streaming.df import pandas_groupby_nan\n\n            data = [dict(a=2, ind=""a"", n=1), dict(a=2, ind=""a""), dict(a=3, ind=""b""), dict(a=30)]\n            df = DataFrame(data)\n            gr2 = pandas_groupby_nan(df, [""ind""]).sum()\n            print(gr2)\n    """"""\n    if axis != 0:\n        raise NotImplementedError(""axis should be 0"")\n    if as_index:\n        raise NotImplementedError(""as_index must be False"")\n    if isinstance(by, tuple):\n        raise TypeError(""by should be of list not tuple"")\n    if not isinstance(by, list):\n        by = [by]\n    hasna = {}\n    for b in by:\n        h = df[b].isnull().values.any()\n        if h:\n            hasna[b] = True\n    if len(hasna) > 0:\n        rep, df_copy = pandas_fillna(df, by, hasna, suffix=suffix)\n        res = df_copy.groupby(by, axis=axis, as_index=as_index, **kwargs)\n        if len(by) == 1:\n            if not nanback:\n                dummy = DataFrame([{""a"": ""a""}])\n                do = dummy.dtypes[0]\n                typ = {c: t for c, t in zip(  # pylint: disable=R1721\n                    df.columns, df.dtypes)}  # pylint: disable=R1721\n                if typ[by[0]] != do:\n                    warnings.warn(\n                        ""[pandas_groupby_nan] NaN value: {0}"".format(rep))\n                return res\n            for b in by:\n                fnan = rep[b]\n                if fnan in res.grouper.groups:\n                    res.grouper.groups[numpy.nan] = res.grouper.groups[fnan]\n                    del res.grouper.groups[fnan]\n                new_val = list((numpy.nan if b == fnan else b)\n                               for b in res.grouper.result_index)\n                res.grouper.groupings[0]._group_index = Index(new_val)\n                res.grouper.groupings[0].obj[b].replace(\n                    fnan, numpy.nan, inplace=True)\n                if isinstance(res.grouper.groupings[0].grouper, numpy.ndarray):\n                    arr = numpy.array(new_val)\n                    res.grouper.groupings[0].grouper = arr\n                    if hasattr(res.grouper.groupings[0], \'_cache\') and \'result_index\' in res.grouper.groupings[0]._cache:\n                        del res.grouper.groupings[0]._cache[\'result_index\']\n                else:\n                    raise NotImplementedError(""Not implemented for type: {0}"".format(\n                        type(res.grouper.groupings[0].grouper)))\n                res.grouper._cache[\'result_index\'] = res.grouper.groupings[0]._group_index\n        else:\n            if not nanback:\n                dummy = DataFrame([{""a"": ""a""}])\n                do = dummy.dtypes[0]\n                typ = {c: t for c, t in zip(  # pylint: disable=R1721\n                    df.columns, df.dtypes)}  # pylint: disable=R1721\n                for b in by:\n                    if typ[b] != do:\n                        warnings.warn(  # pragma: no cover\n                            ""[pandas_groupby_nan] NaN values: {0}"".format(rep))\n                        break\n                return res\n            raise NotImplementedError(\n                ""Not yet implemented. Replacing pseudo nan values by real nan values is not as easy as it looks. Use nanback=False"")\n\n            # keys = list(res.grouper.groups.keys())\n            # didit = False\n            # mapping = {}\n            # for key in keys:\n            #     new_key = list(key)\n            #     mod = False\n            #     for k, b in enumerate(by):\n            #         if b not in rep:\n            #             continue\n            #         fnan = rep[b]\n            #         if key[k] == fnan:\n            #             new_key[k] = numpy.nan\n            #             mod = True\n            #             didit = True\n            #             mapping[fnan] = numpy.nan\n            #     if mod:\n            #         new_key = tuple(new_key)\n            #         mapping[key] = new_key\n            #         res.grouper.groups[new_key] = res.grouper.groups[key]\n            #         del res.grouper.groups[key]\n            # if didit:\n            #     # this code deos not work\n            #     vnan = numpy.nan\n            #     new_index = list(mapping.get(v, v)\n            #                      for v in res.grouper.result_index)\n            #     names = res.grouper.result_index.names\n            #     # index = MultiIndex.from_tuples(tuples=new_index, names=names)\n            #     # res.grouper.result_index = index  # does not work cannot set\n            #     # values for [result_index]\n            #     for k in range(len(res.grouper.groupings)):\n            #         grou = res.grouper.groupings[k]\n            #         new_val = list(mapping.get(v, v) for v in grou)\n            #         grou._group_index = Index(new_val)\n            #         b = names[k]\n            #         if b in rep:\n            #             vv = rep[b]\n            #             grou.obj[b].replace(vv, vnan, inplace=True)\n            #         if isinstance(grou.grouper, numpy.ndarray):\n            #             grou.grouper = numpy.array(new_val)\n            #         else:\n            #             raise NotImplementedError(\n            #                 ""Not implemented for type: {0}"".format(type(grou.grouper)))\n            #     del res.grouper._cache\n        return res\n    else:\n        return df.groupby(by, axis=axis, **kwargs)\n\n\ndef numpy_types():\n    """"""\n    Returns the list of :epkg:`numpy` available types.\n\n    @return     list of types\n    """"""\n\n    return [numpy.bool_,\n            numpy.int_,\n            numpy.intc,\n            numpy.intp,\n            numpy.int8,\n            numpy.int16,\n            numpy.int32,\n            numpy.int64,\n            numpy.uint8,\n            numpy.uint16,\n            numpy.uint32,\n            numpy.uint64,\n            numpy.float_,\n            numpy.float16,\n            numpy.float32,\n            numpy.float64,\n            numpy.complex_,\n            numpy.complex64,\n            numpy.complex128]\n'"
pandas_streaming/df/dataframe_io.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Saves and reads a :epkg:`dataframe` into a :epkg:`zip` file.\n""""""\nimport io\nimport os\nimport zipfile\nimport pandas\nimport numpy\n\n\ndef to_zip(df, zipfilename, zname=""df.csv"", **kwargs):\n    """"""\n    Saves a :epkg:`Dataframe` into a :epkg:`zip` file.\n    It can be read by @see fn to_zip.\n\n    @param      df          :epkg:`dataframe` or :epkg:`numpy:array`\n    @param      zipfilename a :epkg:`*py:zipfile:ZipFile` or a filename\n    @param      zname       a filename in th zipfile\n    @param      kwargs      parameters for :epkg:`pandas:to_csv` or\n                            :epkg:`numpy:save`\n    @return                 zipfilename\n\n    .. exref::\n        :title: Saves and reads a dataframe in a zip file\n        :tag: dataframe\n\n        This shows an example on how to save and read a\n        :epkg:`pandas:dataframe` directly into a zip file.\n\n        .. runpython::\n            :showcode:\n\n            import pandas\n            from pandas_streaming.df import to_zip, read_zip\n\n            df = pandas.DataFrame([dict(a=1, b=""e""),\n                                   dict(b=""f"", a=5.7)])\n\n            name = ""dfs.zip""\n            to_zip(df, name, encoding=""utf-8"", index=False)\n            df2 = read_zip(name, encoding=""utf-8"")\n            print(df2)\n\n    .. exref::\n        :title: Saves and reads a numpy array in a zip file\n        :tag: array\n\n        This shows an example on how to save and read a\n        :epkg:`numpy:ndarray` directly into a zip file.\n\n        .. runpython::\n            :showcode:\n\n            import numpy\n            from pandas_streaming.df import to_zip, read_zip\n\n            arr = numpy.array([[0.5, 1.5], [0.4, 1.6]])\n\n            name = ""dfsa.zip""\n            to_zip(arr, name, \'arr.npy\')\n            arr2 = read_zip(name, \'arr.npy\')\n            print(arr2)\n    """"""\n    if isinstance(df, pandas.DataFrame):\n        stb = io.StringIO()\n        ext = os.path.splitext(zname)[-1]\n        if ext == \'.npy\':\n            raise ValueError(\n                ""Extension \'.npy\' cannot be used to save a dataframe."")\n        df.to_csv(stb, **kwargs)\n    elif isinstance(df, numpy.ndarray):\n        stb = io.BytesIO()\n        ext = os.path.splitext(zname)[-1]\n        if ext != \'.npy\':\n            raise ValueError(\n                ""Extension \'.npy\' is required when saving a numpy array."")\n        numpy.save(stb, df, **kwargs)\n    else:\n        raise TypeError(""Type not handled {0}"".format(type(df)))\n    text = stb.getvalue()\n\n    if isinstance(zipfilename, str):\n        ext = os.path.splitext(zipfilename)[-1]\n        if ext != \'.zip\':\n            raise NotImplementedError(\n                ""Only zip file are implemented not \'{0}\'."".format(ext))\n        zf = zipfile.ZipFile(zipfilename, \'w\')\n        close = True\n    elif isinstance(zipfilename, zipfile.ZipFile):\n        zf = zipfilename\n        close = False\n    else:\n        raise TypeError(\n            ""No implementation for type \'{0}\'"".format(type(zipfilename)))\n\n    zf.writestr(zname, text)\n    if close:\n        zf.close()\n\n\ndef read_zip(zipfilename, zname=""df.csv"", **kwargs):\n    """"""\n    Reads a :epkg:`dataframe` from a :epkg:`zip` file.\n    It can be saved by @see fn read_zip.\n\n    @param      zipfilename a :epkg:`*py:zipfile:ZipFile` or a filename\n    @param      zname       a filename in th zipfile\n    @param      kwargs      parameters for :epkg:`pandas:read_csv`\n    @return                 :epkg:`pandas:dataframe` or :epkg:`numpy:array`\n    """"""\n    if isinstance(zipfilename, str):\n        ext = os.path.splitext(zipfilename)[-1]\n        if ext != \'.zip\':\n            raise NotImplementedError(\n                ""Only zip file are implemented not \'{0}\'."".format(ext))\n        zf = zipfile.ZipFile(zipfilename, \'r\')\n        close = True\n    elif isinstance(zipfilename, zipfile.ZipFile):\n        zf = zipfilename\n        close = False\n    else:\n        raise TypeError(\n            ""No implementation for type \'{0}\'"".format(type(zipfilename)))\n\n    content = zf.read(zname)\n    stb = io.BytesIO(content)\n    ext = os.path.splitext(zname)[-1]\n    if ext == \'.npy\':\n        df = numpy.load(stb, **kwargs)\n    else:\n        df = pandas.read_csv(stb, **kwargs)\n\n    if close:\n        zf.close()\n\n    return df\n'"
pandas_streaming/df/dataframe_io_helpers.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Saves and reads a :epkg:`dataframe` into a :epkg:`zip` file.\n""""""\nimport os\nfrom io import StringIO, BytesIO\ntry:\n    from ujson import dumps\nexcept ImportError:  # pragma: no cover\n    from json import dumps\nimport ijson\n\n\nclass JsonPerRowsStream:\n    """"""\n    Reads a :epkg:`json` streams and adds\n    ``,``, ``[``, ``]`` to convert a stream containing\n    one :pekg:`json` object per row into one single :epkg:`json` object.\n    It only implements method *readline*.\n    """"""\n\n    def __init__(self, st):\n        """"""\n        @param      st      stream\n        """"""\n        self.st = st\n        self.begin = True\n        self.newline = False\n        self.end = True\n\n    def readline(self, size=-1):\n        """"""\n        Reads a line, adds ``,``, ``[``, ``]`` if needed.\n        So the number of read characters is not recessarily\n        the requested one but could be greater.\n        """"""\n        text = self.st.readline(size)\n        if size == 0:\n            return text\n        if self.newline:\n            text = \',\' + text\n            self.newline = False\n        elif self.begin:\n            text = \'[\' + text\n            self.begin = False\n\n        if text.endswith(""\\n""):\n            self.newline = True\n            return text\n        elif len(text) == 0 or len(text) < size:\n            if self.end:\n                self.end = False\n                return text + \']\'\n            else:\n                return text\n        else:\n            return text\n\n    def read(self, size=-1):\n        """"""\n        Reads characters, adds ``,``, ``[``, ``]`` if needed.\n        So the number of read characters is not recessarily\n        the requested one but could be greater.\n        """"""\n        text = self.st.read(size)\n        if isinstance(text, bytes):\n            cst = b""\\n"", b""\\n,"", b"","", b""["", b""]""\n        else:\n            cst = ""\\n"", ""\\n,"", "","", ""["", ""]""\n        if size == 0:\n            return text\n        if len(text) > 1:\n            t1, t2 = text[:len(text) - 1], text[len(text) - 1:]\n            t1 = t1.replace(cst[0], cst[1])\n            text = t1 + t2\n\n        if self.newline:\n            text = cst[2] + text\n            self.newline = False\n        elif self.begin:\n            text = cst[3] + text\n            self.begin = False\n\n        if text.endswith(cst[0]):\n            self.newline = True\n            return text\n        elif len(text) == 0 or len(text) < size:\n            if self.end:\n                self.end = False\n                return text + cst[4]\n            else:\n                return text\n        else:\n            return text\n\n    def getvalue(self):\n        """"""\n        Returns the whole stream content.\n        """"""\n        def byline():\n            line = self.readline()\n            while line:\n                yield line\n                line = self.readline()\n        return """".join(byline())\n\n\ndef flatten_dictionary(dico, sep=""_""):\n    """"""\n    Flattens a dictionary with nested structure to a dictionary with no\n    hierarchy.\n    :param dico: dictionary to flatten\n    :param sep: string to separate dictionary keys by\n    :return: flattened dictionary\n\n    Inspired from `flatten_json <https://github.com/amirziai/flatten/blob/master/flatten_json.py>`_.\n    """"""\n    flattened_dict = dict()\n\n    def _flatten(obj, key):\n        if obj is None:\n            flattened_dict[key] = obj\n        elif isinstance(obj, dict):\n            for k, v in obj.items():\n                if not isinstance(k, str):\n                    raise TypeError(""All keys must a string."")  # pragma: no cover\n                k2 = k if key is None else ""{0}{1}{2}"".format(key, sep, k)\n                _flatten(v, k2)\n        elif isinstance(obj, (list, set)):\n            for index, item in enumerate(obj):\n                k2 = k if key is None else ""{0}{1}{2}"".format(key, sep, index)\n                _flatten(item, k2)\n        else:\n            flattened_dict[key] = obj\n\n    _flatten(dico, None)\n    return flattened_dict\n\n\ndef enumerate_json_items(filename, encoding=None, lines=False, flatten=False, fLOG=None):\n    """"""\n    Enumerates items from a :epkg:`JSON` file or string.\n\n    @param      filename        filename or string or stream to parse\n    @param      encoding        encoding\n    @param      lines           one record per row\n    @param      flatten         call @see fn flatten_dictionary\n    @param      fLOG            logging function\n    @return                     iterator on records at first level.\n\n    It assumes the syntax follows the format: ``[ {""id"":1, ...}, {""id"": 2, ...}, ...]``.\n    However, if option *lines* if true, the function considers that the\n    stream or file does have one record per row as follows:\n\n        {""id"":1, ...}\n        {""id"": 2, ...}\n\n    .. exref::\n        :title: Processes a json file by streaming.\n\n        The module :epkg:`ijson` can read a :epkg:`JSON` file by streaming.\n        This module is needed because a record can be written on multiple lines.\n        This function leverages it produces the following results.\n\n        .. runpython::\n            :showcode:\n\n            from pandas_streaming.df.dataframe_io_helpers import enumerate_json_items\n\n            text_json = b\'\'\'\n                [\n                {\n                    ""glossary"": {\n                        ""title"": ""example glossary"",\n                        ""GlossDiv"": {\n                            ""title"": ""S"",\n                            ""GlossList"": [{\n                                ""GlossEntry"": {\n                                    ""ID"": ""SGML"",\n                                    ""SortAs"": ""SGML"",\n                                    ""GlossTerm"": ""Standard Generalized Markup Language"",\n                                    ""Acronym"": ""SGML"",\n                                    ""Abbrev"": ""ISO 8879:1986"",\n                                    ""GlossDef"": {\n                                        ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",\n                                        ""GlossSeeAlso"": [""GML"", ""XML""]\n                                    },\n                                    ""GlossSee"": ""markup""\n                                }\n                            }]\n                        }\n                    }\n                },\n                {\n                    ""glossary"": {\n                        ""title"": ""example glossary"",\n                        ""GlossDiv"": {\n                            ""title"": ""S"",\n                            ""GlossList"": {\n                                ""GlossEntry"": [{\n                                    ""ID"": ""SGML"",\n                                    ""SortAs"": ""SGML"",\n                                    ""GlossTerm"": ""Standard Generalized Markup Language"",\n                                    ""Acronym"": ""SGML"",\n                                    ""Abbrev"": ""ISO 8879:1986"",\n                                    ""GlossDef"": {\n                                        ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",\n                                        ""GlossSeeAlso"": [""GML"", ""XML""]\n                                    },\n                                    ""GlossSee"": ""markup""\n                                }]\n                            }\n                        }\n                    }\n                }\n                ]\n            \'\'\'\n\n            for item in enumerate_json_items(text_json):\n                print(item)\n    """"""\n    if isinstance(filename, str):\n        if ""{"" not in filename and os.path.exists(filename):\n            with open(filename, ""r"", encoding=encoding) as f:\n                for el in enumerate_json_items(f, encoding=encoding, lines=lines, flatten=flatten, fLOG=fLOG):\n                    yield el\n        else:\n            st = StringIO(filename)\n            for el in enumerate_json_items(st, encoding=encoding, lines=lines, flatten=flatten, fLOG=fLOG):\n                yield el\n    elif isinstance(filename, bytes):\n        st = BytesIO(filename)\n        for el in enumerate_json_items(st, encoding=encoding, lines=lines, flatten=flatten, fLOG=fLOG):\n            yield el\n    elif lines:\n        for el in enumerate_json_items(JsonPerRowsStream(filename),\n                                       encoding=encoding, lines=False, flatten=flatten, fLOG=fLOG):\n            yield el\n    else:\n        parser = ijson.parse(filename)\n        current = None\n        curkey = None\n        stack = []\n        nbyield = 0\n        for i, (_, event, value) in enumerate(parser):\n            if i % 1000000 == 0 and fLOG is not None:\n                fLOG(  # pragma: no cover\n                    ""[enumerate_json_items] i={0} yielded={1}"".format(i, nbyield))\n            if event == ""start_array"":\n                if curkey is None:\n                    current = []\n                else:\n                    if not isinstance(current, dict):\n                        raise RuntimeError(  # pragma: no cover\n                            ""Type issue {0}"".format(type(current)))\n                    c = []\n                    current[curkey] = c\n                    current = c\n                curkey = None\n                stack.append(current)\n            elif event == ""end_array"":\n                stack.pop()\n                if len(stack) == 0:\n                    # We should be done.\n                    current = None\n                else:\n                    current = stack[-1]\n            elif event == ""start_map"":\n                c = {}\n                if curkey is None:\n                    if current is None:\n                        current = []\n                    current.append(c)\n                else:\n                    current[curkey] = c  # pylint: disable=E1137\n                stack.append(c)\n                current = c\n                curkey = None\n            elif event == ""end_map"":\n                stack.pop()\n                current = stack[-1]\n                if len(stack) == 1:\n                    nbyield += 1\n                    if flatten:\n                        yield flatten_dictionary(current[-1])\n                    else:\n                        yield current[-1]\n                    # We clear the memory.\n                    current.clear()\n            elif event == ""map_key"":\n                curkey = value\n            elif event in {""string"", ""number"", ""boolean""}:\n                if curkey is None:\n                    current.append(value)\n                else:\n                    current[curkey] = value  # pylint: disable=E1137\n                    curkey = None\n            elif event == ""null"":\n                if curkey is None:\n                    current.append(None)\n                else:\n                    current[curkey] = None  # pylint: disable=E1137\n                    curkey = None\n            else:\n                raise ValueError(""Unknown event \'{0}\'"".format(event))  # pragma: no cover\n\n\nclass JsonIterator2Stream:\n    """"""\n    Transforms an iterator on :epkg:`JSON` items\n    into a stream which returns an items as a string every time\n    method *read* is called.\n    The iterator could be one returned by @see fn enumerate_json_items.\n\n    .. exref::\n        :title: Reshape a json file\n\n        The function @see fn enumerate_json_items reads any\n        :epkg:`json` even if every record is split over\n        multiple lines. Class @see cl JsonIterator2Stream\n        mocks this iterator as a stream. Each row is a single item.\n\n        .. runpython::\n            :showcode:\n\n            from pandas_streaming.df.dataframe_io_helpers import enumerate_json_items, JsonIterator2Stream\n\n            text_json = b\'\'\'\n                [\n                {\n                    ""glossary"": {\n                        ""title"": ""example glossary"",\n                        ""GlossDiv"": {\n                            ""title"": ""S"",\n                            ""GlossList"": [{\n                                ""GlossEntry"": {\n                                    ""ID"": ""SGML"",\n                                    ""SortAs"": ""SGML"",\n                                    ""GlossTerm"": ""Standard Generalized Markup Language"",\n                                    ""Acronym"": ""SGML"",\n                                    ""Abbrev"": ""ISO 8879:1986"",\n                                    ""GlossDef"": {\n                                        ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",\n                                        ""GlossSeeAlso"": [""GML"", ""XML""]\n                                    },\n                                    ""GlossSee"": ""markup""\n                                }\n                            }]\n                        }\n                    }\n                },\n                {\n                    ""glossary"": {\n                        ""title"": ""example glossary"",\n                        ""GlossDiv"": {\n                            ""title"": ""S"",\n                            ""GlossList"": {\n                                ""GlossEntry"": [{\n                                    ""ID"": ""SGML"",\n                                    ""SortAs"": ""SGML"",\n                                    ""GlossTerm"": ""Standard Generalized Markup Language"",\n                                    ""Acronym"": ""SGML"",\n                                    ""Abbrev"": ""ISO 8879:1986"",\n                                    ""GlossDef"": {\n                                        ""para"": ""A meta-markup language, used to create markup languages such as DocBook."",\n                                        ""GlossSeeAlso"": [""GML"", ""XML""]\n                                    },\n                                    ""GlossSee"": ""markup""\n                                }]\n                            }\n                        }\n                    }\n                }\n                ]\n            \'\'\'\n\n            for item in JsonIterator2Stream(enumerate_json_items(text_json)):\n                print(item)\n    """"""\n\n    def __init__(self, it, **kwargs):\n        """"""\n        @param      it      iterator\n        @param      kwargs  arguments to :epkg:`*py:json:dumps`\n        """"""\n        self.it = it\n        self.kwargs = kwargs\n\n    def write(self):\n        """"""\n        The class does not write.\n        """"""\n        raise NotImplementedError()\n\n    def read(self):\n        """"""\n        Reads the next item and returns it as a string.\n        """"""\n        try:\n            value = next(self.it)\n            return dumps(value, **self.kwargs)\n        except StopIteration:\n            return None\n\n    def __iter__(self):\n        """"""\n        Iterate on each row.\n        """"""\n        for value in self.it:\n            yield dumps(value, **self.kwargs)\n'"
pandas_streaming/df/dataframe_split.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Implements different methods to split a dataframe.\n""""""\nimport hashlib\nimport pickle\nimport random\nimport warnings\nfrom io import StringIO\nimport pandas\n\n\ndef sklearn_train_test_split(self, path_or_buf=None, export_method=""to_csv"",\n                             names=None, **kwargs):\n    """"""\n    Randomly splits a dataframe into smaller pieces.\n    The function returns streams of file names.\n    The function relies on :epkg:`sklearn:model_selection:train_test_split`.\n    It does not handle stratified version of it.\n\n    @param  self            @see cl StreamingDataFrame\n    @param  path_or_buf     a string, a list of strings or buffers, if it is a\n                            string, it must contain ``{}`` like ``partition{}.txt``\n    @param  export_method   method used to store the partitions, by default\n                            :epkg:`pandas:DataFrame:to_csv`\n    @param  names           partitions names, by default ``(\'train\', \'test\')``\n    @param  kwargs          parameters for the export function and\n                            :epkg:`sklearn:model_selection:train_test_split`.\n    @return                 outputs of the exports functions\n\n    The function cannot return two iterators or two\n    @see cl StreamingDataFrame because running through one\n    means running through the other. We can assume both\n    splits do not hold in memory and we cannot run through\n    the same iterator again as random draws would be different.\n    We need to store the results into files or buffers.\n\n    .. warning::\n        The method *export_method* must write the data in\n        mode *append* and allows stream.\n    """"""\n    if kwargs.get(""stratify"") is not None:\n        raise NotImplementedError(\n            ""No implementation yet for the stratified version."")\n    with warnings.catch_warnings():\n        warnings.filterwarnings(""ignore"", category=ImportWarning)\n        from sklearn.model_selection import train_test_split  # pylint: disable=C0415\n\n    opts = [\'test_size\', \'train_size\',\n            \'random_state\', \'shuffle\', \'stratify\']\n    split_ops = {}\n    for o in opts:\n        if o in kwargs:\n            split_ops[o] = kwargs[o]\n            del kwargs[o]\n\n    exportf_ = getattr(pandas.DataFrame, export_method)\n    if export_method == \'to_csv\' and \'mode\' not in kwargs:\n        exportf = lambda *a, **kw: exportf_(*a, mode=\'a\', **kw)\n    else:\n        exportf = exportf_\n\n    if isinstance(path_or_buf, str):\n        if ""{}"" not in path_or_buf:\n            raise ValueError(\n                ""path_or_buf must contain {} to insert the partition name"")\n        if names is None:\n            names = [\'train\', \'test\']\n        elif len(names) != len(path_or_buf):\n            raise ValueError(\n                \'names and path_or_buf must have the same length\')\n        path_or_buf = [path_or_buf.format(n) for n in names]\n    elif path_or_buf is None:\n        path_or_buf = [None, None]\n    else:\n        if not isinstance(path_or_buf, list):\n            raise TypeError(\'path_or_buf must be a list or a string\')\n\n    bufs = []\n    close = []\n    for p in path_or_buf:\n        if p is None:\n            st = StringIO()\n            cl = False\n        elif isinstance(p, str):\n            st = open(p, ""w"", encoding=kwargs.get(\'encoding\'))\n            cl = True\n        else:\n            st = p\n            cl = False\n        bufs.append(st)\n        close.append(cl)\n\n    for df in self:\n        train, test = train_test_split(df, **split_ops)\n        exportf(train, bufs[0], **kwargs)\n        exportf(test, bufs[1], **kwargs)\n        kwargs[\'header\'] = False\n\n    for b, c in zip(bufs, close):\n        if c:\n            b.close()\n    return [st.getvalue() if isinstance(st, StringIO) else p for st, p in zip(bufs, path_or_buf)]\n\n\ndef sklearn_train_test_split_streaming(self, test_size=0.25, train_size=None,\n                                       stratify=None, hash_size=9, unique_rows=False):\n    """"""\n    Randomly splits a dataframe into smaller pieces.\n    The function returns streams of file names.\n    The function relies on :epkg:`sklearn:model_selection:train_test_split`.\n    It handles the stratified version of it.\n\n    @param  self            @see cl StreamingDataFrame\n    @param  test_size       ratio for the test partition (if *train_size* is not specified)\n    @param  train_size      ratio for the train partition\n    @param  stratify        column holding the stratification\n    @param  hash_size       size of the hash to cache information about partition\n    @param  unique_rows     ensures that rows are unique\n    @return                 Two @see cl StreamingDataFrame, one\n                            for train, one for test.\n\n    The function returns two iterators or two\n    @see cl StreamingDataFrame. It\n    tries to do everything without writing anything on disk\n    but it requires to store the repartition somehow.\n    This function hashes every row and maps the hash with a part\n    (train or test). This cache must hold in memory otherwise the\n    function fails. The two returned iterators must not be used\n    for the first time in the same time. The first time is used to\n    build the cache. The function changes the order of rows if\n    the parameter *stratify* is not null. The cache has a side effect:\n    every exact same row will be put in the same partition.\n    If that is not what you want, you should add an index column\n    or a random one.\n    """"""\n    p = (1 - test_size) if test_size else None\n    if train_size is not None:\n        p = train_size\n    n = 2 * max(1 / p, 1 / (1 - p))  # changement\n\n    static_schema = []\n\n    def iterator_rows():\n        ""iterates on rows""\n        counts = {}\n        memory = {}\n        pos_col = None\n        for df in self:\n            if pos_col is None:\n                static_schema.append(list(df.columns))\n                static_schema.append(list(df.dtypes))\n                static_schema.append(df.shape[0])\n                if stratify is not None:\n                    pos_col = list(df.columns).index(stratify)\n                else:\n                    pos_col = -1\n\n            for obs in df.itertuples(index=False, name=None):\n                strat = 0 if stratify is None else obs[pos_col]\n                if strat not in memory:\n                    memory[strat] = []\n                memory[strat].append(obs)\n\n                for k in memory:\n                    v = memory[k]\n                    if len(v) >= n + random.randint(0, 10):  # changement\n                        vr = list(range(len(v)))\n                        # on permute al\xc3\xa9atoirement\n                        random.shuffle(vr)\n                        if (0, k) in counts:\n                            tt = counts[1, k] + counts[0, k]\n                            delta = - int(counts[0, k] - tt * p + 0.5)\n                        else:\n                            delta = 0\n                        i = int(len(v) * p + 0.5)\n                        i += delta\n                        i = max(0, min(len(v), i))\n                        one = set(vr[:i])\n                        for d, obs_ in enumerate(v):\n                            yield obs_, 0 if d in one else 1\n                        if (0, k) not in counts:\n                            counts[0, k] = i\n                            counts[1, k] = len(v) - i\n                        else:\n                            counts[0, k] += i\n                            counts[1, k] += len(v) - i\n                        # on efface de la m\xc3\xa9moire les informations produites\n                        memory[k].clear()\n\n        # Lorsqu\'on a fini, il faut tout de m\xc3\xaame r\xc3\xa9partir les\n        # observations stock\xc3\xa9es.\n        for k in memory:\n            v = memory[k]\n            vr = list(range(len(v)))\n            # on permute al\xc3\xa9atoirement\n            random.shuffle(vr)\n            if (0, k) in counts:\n                tt = counts[1, k] + counts[0, k]\n                delta = - int(counts[0, k] - tt * p + 0.5)\n            else:\n                delta = 0\n            i = int(len(v) * p + 0.5)\n            i += delta\n            i = max(0, min(len(v), i))\n            one = set(vr[:i])\n            for d, obs in enumerate(v):\n                yield obs, 0 if d in one else 1\n            if (0, k) not in counts:\n                counts[0, k] = i\n                counts[1, k] = len(v) - i\n            else:\n                counts[0, k] += i\n                counts[1, k] += len(v) - i\n\n    def h11(w):\n        ""pickle and hash""\n        b = pickle.dumps(w)\n        return hashlib.md5(b).hexdigest()[:hash_size]\n\n    # We store the repartition in a cache.\n    cache = {}\n\n    def iterator_internal(part_requested):\n        ""internal iterator on dataframes""\n        iy = 0\n        accumul = []\n        if len(cache) == 0:\n            for obs, part in iterator_rows():\n                h = h11(obs)\n                if unique_rows and h in cache:\n                    raise ValueError(  # pragma: no cover\n                        ""A row or at least its hash is already cached. ""\n                        ""Increase hash_size or check for duplicates ""\n                        ""(\'{0}\')\\n{1}."".format(h, obs))\n                if h not in cache:\n                    cache[h] = part\n                else:\n                    part = cache[h]\n                if part == part_requested:\n                    accumul.append(obs)\n                    if len(accumul) >= static_schema[2]:\n                        dfo = pandas.DataFrame(\n                            accumul, columns=static_schema[0])\n                        self.ensure_dtype(dfo, static_schema[1])\n                        iy += dfo.shape[0]\n                        accumul.clear()\n                        yield dfo\n        else:\n            for df in self:\n                for obs in df.itertuples(index=False, name=None):\n                    h = h11(obs)\n                    part = cache.get(h)\n                    if part is None:\n                        raise ValueError(  # pragma: no cover\n                            ""Second iteration. A row was never met in the first one\\n{0}"".format(obs))\n                    if part == part_requested:\n                        accumul.append(obs)\n                        if len(accumul) >= static_schema[2]:\n                            dfo = pandas.DataFrame(\n                                accumul, columns=static_schema[0])\n                            self.ensure_dtype(dfo, static_schema[1])\n                            iy += dfo.shape[0]\n                            accumul.clear()\n                            yield dfo\n        if len(accumul) > 0:\n            dfo = pandas.DataFrame(accumul, columns=static_schema[0])\n            self.ensure_dtype(dfo, static_schema[1])\n            iy += dfo.shape[0]\n            yield dfo\n\n    return (self.__class__(lambda: iterator_internal(0)),\n            self.__class__(lambda: iterator_internal(1)))\n'"
pandas_streaming/exc/__init__.py,0,"b'""""""\n@file\n@brief Shortcuts to *exc*.\n""""""\n\nfrom .exc_streaming import StreamingInefficientException\n'"
pandas_streaming/exc/exc_streaming.py,0,"b'# -*- coding: utf-8 -*-\n""""""\n@file\n@brief Defines a streming dataframe.\n""""""\n\n\nclass StreamingInefficientException(Exception):\n    """"""\n    Kind of operations doable with a :epkg:`pandas:DataFrame`\n    but which should not be done in streaming mode.\n    """"""\n\n    def __init__(self, meth):\n        """"""\n        This method is inefficient in streaming mode\n        and not implemented.\n\n        @param      meth    method\n        """"""\n        Exception.__init__(\n            self, ""{0} should not be done in streaming mode."".format(meth))\n'"
_doc/sphinxdoc/source/conf.py,0,"b'# -*- coding: utf-8 -*-\nimport sys\nimport os\nimport sphinx_readable_theme\nfrom pyquickhelper.helpgen.default_conf import set_sphinx_variables, get_default_stylesheet\n\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.split(__file__)[0])))\n\nlocal_template = os.path.join(os.path.abspath(\n    os.path.dirname(__file__)), ""phdoc_templates"")\n\nset_sphinx_variables(__file__, ""pandas_streaming"", ""Xavier Dupr\xc3\xa9"", 2020,\n                     ""readable"", sphinx_readable_theme.get_html_theme_path(),\n                     locals(), extlinks=dict(\n                         issue=(\'https://github.com/sdpython/pandas_streaming/issues/%s\', \'issue\')),\n                     title=""Streaming functionalities for pandas"", book=True)\n\nblog_root = ""http://www.xavierdupre.fr/app/pandas_streaming/helpsphinx/""\n\nhtml_context = {\n    \'css_files\': get_default_stylesheet() + [\'_static/my-styles.css\'],\n}\n\nhtml_logo = ""phdoc_static/project_ico.png""\n\nhtml_sidebars = {}\n\nlanguage = ""en""\ncustom_preamble = """"""\\n\n\\\\newcommand{\\\\vecteur}[2]{\\\\pa{#1,\\\\dots,#2}}\n\\\\newcommand{\\\\N}[0]{\\\\mathbb{N}}\n\\\\newcommand{\\\\indicatrice}[1]{\\\\mathbf{1\\\\!\\\\!1}_{\\\\acc{#1}}}\n\\\\usepackage[all]{xy}\n\\\\newcommand{\\\\infegal}[0]{\\\\leqslant}\n\\\\newcommand{\\\\supegal}[0]{\\\\geqslant}\n\\\\newcommand{\\\\ensemble}[2]{\\\\acc{#1,\\\\dots,#2}}\n\\\\newcommand{\\\\fleche}[1]{\\\\overrightarrow{ #1 }}\n\\\\newcommand{\\\\intervalle}[2]{\\\\left\\\\{#1,\\\\cdots,#2\\\\right\\\\}}\n\\\\newcommand{\\\\loinormale}[2]{{\\\\cal N}\\\\pa{#1,#2}}\n\\\\newcommand{\\\\independant}[0]{\\\\;\\\\makebox[3ex]{\\\\makebox[0ex]{\\\\rule[-0.2ex]{3ex}{.1ex}}\\\\!\\\\!\\\\!\\\\!\\\\makebox[.5ex][l]{\\\\rule[-.2ex]{.1ex}{2ex}}\\\\makebox[.5ex][l]{\\\\rule[-.2ex]{.1ex}{2ex}}} \\\\,\\\\,}\n\\\\newcommand{\\\\esp}{\\\\mathbb{E}}\n\\\\newcommand{\\\\var}{\\\\mathbb{V}}\n\\\\newcommand{\\\\pr}[1]{\\\\mathbb{P}\\\\pa{#1}}\n\\\\newcommand{\\\\loi}[0]{{\\\\cal L}}\n\\\\newcommand{\\\\vecteurno}[2]{#1,\\\\dots,#2}\n\\\\newcommand{\\\\norm}[1]{\\\\left\\\\Vert#1\\\\right\\\\Vert}\n\\\\newcommand{\\\\dans}[0]{\\\\rightarrow}\n\\\\newcommand{\\\\partialfrac}[2]{\\\\frac{\\\\partial #1}{\\\\partial #2}}\n\\\\newcommand{\\\\partialdfrac}[2]{\\\\dfrac{\\\\partial #1}{\\\\partial #2}}\n\\\\newcommand{\\\\loimultinomiale}[1]{{\\\\cal M}\\\\pa{#1}}\n\\\\newcommand{\\\\trace}[1]{tr\\\\pa{#1}}\n\\\\newcommand{\\\\abs}[1]{\\\\left|#1\\\\right|}\n""""""\n# \\\\usepackage{eepic}\n\nimgmath_latex_preamble += custom_preamble\nlatex_elements[\'preamble\'] += custom_preamble\nmathdef_link_only = True\n\nepkg_dictionary.update({\n    \'csv\': \'https://en.wikipedia.org/wiki/Comma-separated_values\',\n    \'dask\': \'https://dask.pydata.org/en/latest/\',\n    \'dataframe\': \'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\',\n    \'Dataframe\': \'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\',\n    \'DataFrame\': \'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\',\n    \'dataframes\': \'https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\',\n    \'pandas\': (\'http://pandas.pydata.org/pandas-docs/stable/\',\n               (\'http://pandas.pydata.org/pandas-docs/stable/generated/pandas.{0}.html\', 1),\n               (\'http://pandas.pydata.org/pandas-docs/stable/generated/pandas.{0}.{1}.html\', 2)),\n    \'sklearn\': (\'http://scikit-learn.org/stable/\',\n                (\'http://scikit-learn.org/stable/modules/generated/{0}.html\', 1),\n                (\'http://scikit-learn.org/stable/modules/generated/{0}.{1}.html\', 2)),\n    \'Hadoop\': \'http://hadoop.apache.org/\',\n    \'pyarrow\': \'https://arrow.apache.org/docs/python/\',\n    \'pyspark\': \'http://spark.apache.org/docs/2.1.1/api/python/index.html\',\n    \'scikit-multiflow\': \'https://scikit-multiflow.github.io/\',\n    \'streamz\': \'https://streamz.readthedocs.io/en/latest/index.html\',\n    \'tornado\': \'https://www.tornadoweb.org/en/stable/\',\n})\n'"
