file_path,api_count,code
part-1/deepl-stud.py,0,"b'##############@@@@@@@@####################\n## STEP 1: DATA IMPORT & PRE-PROCESSING\n##############@@@@@@@@####################\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n############################\n# 1.2: Import the dataset\n############################\n# Importing the dataset\ndt = pd.read_csv(\'stud-por-preformat.csv\')\n\n###########################\n# 1.3: Data-Type Correction\n###########################\n# Mapping the dataset to their correct datatypes.\ndt = dt.astype({\'school\':\'category\', \'sex\':\'category\', \'age\':\'float64\', \'address\':\'category\', \'famsize\':\'category\', \'Pstatus\':\'category\', \'Medu\':\'float64\', \'Fedu\':\'float64\', \'Mjob\':\'category\', \'Fjob\':\'category\', \'reason\':\'category\', \'guardian\':\'category\', \'traveltime\':\'float64\', \'studytime\':\'float64\', \'failures\':\'float64\', \'schoolsup\':\'bool\', \'famsup\':\'bool\', \'paid\':\'bool\', \'activities\':\'bool\', \'nursery\':\'bool\', \'higher\':\'bool\', \'internet\':\'bool\', \'romantic\':\'bool\', \'famrel\':\'float64\', \'freetime\':\'float64\', \'goout\':\'float64\', \'Dalc\':\'float64\', \'Walc\':\'float64\', \'health\':\'float64\', \'absences\':\'float64\', \'G1\':\'float64\', \'G2\':\'float64\', \'G3\':\'float64\'}) \n\n# Shuffle the order of the records to clear out any biases.\nfrom sklearn.utils import shuffle\ndt = shuffle(dt)\n\n# Lets do some integrity datatype checks & prints to the console\nprint(dt[\'age\'].dtypes)\nprint(dt[\'school\'].dtypes)\nprint(dt.head(5)) \n\n###########################\n# 1.4: Extracting independant/dependant variables\n###########################\n# Insert a Final-Grade decision binary field/column. (Gfinal Range 0-20: True if x >= 10, otherwise false) \ndt.insert(33, \'Gfinal\', dt.iloc[:, 32].values >= 10)      \nprint(dt[\'Gfinal\'].dtypes)\n\n# Independant Variables\nX = dt.iloc[:, 0:32]\n\n# Dependant Variable\ny = dt.iloc[:, 33]\n\n###########################\n# 1.5: Encoding Categorical Fields\n###########################\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\ncat_fields_no = [0, 1, 3, 4, 5, 8, 9, 10, 11] # Column/Field-No.s with categoriacal data.\n\nlex1 = LabelEncoder()\nX[\'school\'] = lex1.fit_transform(X[\'school\'])\nlex2 = LabelEncoder()\nX[\'sex\'] = lex2.fit_transform(X[\'sex\'])\nlex3 = LabelEncoder()\nX[\'address\'] = lex3.fit_transform(X[\'address\'])\nlex4 = LabelEncoder()\nX[\'famsize\'] = lex4.fit_transform(X[\'famsize\'])\nlex5 = LabelEncoder()\nX[\'Pstatus\'] = lex5.fit_transform(X[\'Pstatus\'])\nlex6 = LabelEncoder()\nX[\'Mjob\'] = lex6.fit_transform(X[\'Mjob\'])\nlex7 = LabelEncoder()\nX[\'Fjob\'] = lex7.fit_transform(X[\'Fjob\'])\nlex8 = LabelEncoder()\nX[\'reason\'] = lex8.fit_transform(X[\'reason\'])\nlex9 = LabelEncoder()\nX[\'guardian\'] = lex9.fit_transform(X[\'guardian\'])\n\nlex10 = LabelEncoder()\nX[\'schoolsup\'] = lex10.fit_transform(X[\'schoolsup\'])\nlex11 = LabelEncoder()\nX[\'famsup\'] = lex11.fit_transform(X[\'famsup\'])\nlex12 = LabelEncoder()\nX[\'paid\'] = lex12.fit_transform(X[\'paid\'])\nlex13 = LabelEncoder()\nX[\'activities\'] = lex13.fit_transform(X[\'activities\'])\nlex14 = LabelEncoder()\nX[\'nursery\'] = lex14.fit_transform(X[\'nursery\'])\nlex15 = LabelEncoder()\nX[\'higher\'] = lex15.fit_transform(X[\'higher\'])\nlex16 = LabelEncoder()\nX[\'internet\'] = lex16.fit_transform(X[\'guardian\'])\nlex17 = LabelEncoder()\nX[\'romantic\'] = lex17.fit_transform(X[\'romantic\'])\n\nohe = OneHotEncoder(categorical_features = cat_fields_no)\nX = ohe.fit_transform(X).toarray()\n\n# Already Done\n# ley = LabelEncoder()\n# y = ley.fit_transform(y)\n  \n###########################\n# 1.6: Split Training & Test Sets\n###########################\n# Splitting the dataset into Training & Test Sets.\nfrom sklearn.model_selection import train_test_split as tts\nX_train, X_test, Y_train, Y_test = tts(X, y, test_size = 0.2, random_state = 0)\n\n###########################\n# 1.7: Feature Scaling\n###########################\n# Applying Feature Scaling >>> Around the Zero-Mean-Variance\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\n\n##############@@@@@@@@####################\n## STEP 2: CREATE & FIT THE DATA TO THE ANN\n##############@@@@@@@@####################\n\n# Importing the Keras Labraries & Packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n###########################\n# 2.1: Initialise the ANN\n###########################\nclassifier = Sequential()\n\n# Input & 1st Hidden Layer\nclassifier.add(Dense(activation=""relu"", input_dim=50, units=26, kernel_initializer=""uniform""))\n\n# 2nd Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=26, kernel_initializer=""uniform""))\n\n# 3rd Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=26, kernel_initializer=""uniform""))\n\n# 4th Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=26, kernel_initializer=""uniform""))\n\n# 5th Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=26, kernel_initializer=""uniform""))\n\n# 6th Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=26, kernel_initializer=""uniform""))\n\n# 7th Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=20, kernel_initializer=""uniform""))\n\n# 8th Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=18, kernel_initializer=""uniform""))\n\n# 9th Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=16, kernel_initializer=""uniform""))\n\n# 10th Hidden Layer\nclassifier.add(Dense(activation=""relu"", units=12, kernel_initializer=""uniform""))\n\n# Output Layer\nclassifier.add(Dense(activation=""sigmoid"", units=1, kernel_initializer=""uniform""))\n\n###########################\n# 2.2: Compile the ANN\n###########################\nclassifier.compile(optimizer = \'adam\', loss = \'binary_crossentropy\', metrics = [\'accuracy\'])\n\n###########################\n# 2.3: Train the ANN\n###########################\nh_cb = classifier.fit(X_train, Y_train, batch_size = 10, nb_epoch = 1000)\n\n##############@@@@@@@@####################\n## STEP 3: ANN MODEL EVALUATION & PREDICTIONS\n##############@@@@@@@@####################\n\n###########################\n# 3.1: Making predictions\n###########################\n# Use the classfisier to make predictions on the test-set.\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred >= 0.5)\n\n###########################\n# 3.2: Model Evaluation\n###########################\n# Evaluate the predictions using the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\ncm = confusion_matrix(Y_test, y_pred)\n\nacc = accuracy_score(Y_test, y_pred) * 100.00\n\nprint (f\'Accuracy : {acc}\')\n\n\n##############@@@@@@@@####################\n# 4: Visualise the Training Loss & Accuracy\n##############@@@@@@@@####################\n# Keys to plot\nprint(h_cb.history.keys()) \n\n###########################\n# 4.1: Plot Training Accuracy\n###########################\nplt.plot(h_cb.history[\'acc\'], color=\'blue\')\nplt.title(\'Training Accuracy vs Epoch\')\nplt.ylabel(\'Training Accuracy\')\nplt.xlabel(\'Epoch\')\nplt.grid(True)\nplt.legend([\'Training-accuracy\'], loc=\'best\')\nplt.show()\n\n###########################\n# 4.2: Plot Training Loss\n###########################\nplt.plot(h_cb.history[\'loss\'], color=\'g\')\nplt.title(\'Training Loss vs Epoch\')\nplt.ylabel(\'Training Loss\')\nplt.xlabel(\'Epoch\')\nplt.legend([\'Training-loss\'], loc=\'best\')\nplt.grid(True)\nplt.show()\n\n'"
