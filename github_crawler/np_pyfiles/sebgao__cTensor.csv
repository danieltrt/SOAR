file_path,api_count,code
categorical_testcase.py,9,"b""import os\nimport sys\nsys.path.append(os.path.abspath('..'))\n\nimport numpy as np\nfrom ctensor import Tensor\nfrom ctensor.functional import leaky_relu, sigmoid, binary_cross_entropy\nfrom ctensor.optim import Adam\n\n# Data Generation\nx = np.random.rand(1000, 5)*10 - 5\nx_scale = np.array([1.2, 2, 0.4, 0.7, 0.2]).reshape(1, 5)\nx = x*x_scale\ny = x[:, 0]**2-2*x[:, 1]+10*x[:, 2]+0.04*x[:, 3]+np.abs(2*x[:, 4])\ny = ((y+np.random.rand(1000)) > 17).astype(np.float)\n\n# Tensor Converting\nX = Tensor(x, requires_grad=False)\nY = Tensor(y.reshape(1000, 1), requires_grad=False)\n\n# Parameters\nM1 = Tensor(np.random.randn(5, 100)/500)\nB1 = Tensor(np.zeros((1, 100)))\n\nM2 = Tensor(np.random.randn(100, 1)/100)\nB2 = Tensor(np.zeros((1, 1)))\n\n# Optimizer\nadam = Adam([M1, B1, M2, B2])\n\n# Compute Graph Definition, note that the graph is actually dynamic\n\ndef compute(X):\n    X1 = leaky_relu((X @ M1) + B1, 0.01)\n    X2 = sigmoid((X1 @ M2) + B2)\n    return X2\n\nfor _ in range(10000):\n    indices = np.random.randint(0, 1000, size=128)\n    X_ = X[indices]\n    Y_ = Y[indices]\n    pred_y = compute(X_)\n    loss = binary_cross_entropy(pred_y, Y_)\n    adam.zero_grad()\n    loss.backward()\n    adam.step(1e-3)\n    if _ % 1000 == 0:\n        print(loss)\n\n# If succeeded, the score would be relatively high\npred_y = compute(X)\nprint(((pred_y.data > 0.5) == Y.data).mean())\n"""
nn_testcase.py,5,"b'import numpy as np\nfrom ctensor import Tensor\nimport ctensor.functional as F\nimport ctensor.nn as nn\nfrom ctensor.optim import Adam\n\n# Data Generation\nx = np.random.rand(1000, 5)*10 - 5\nx_scale = np.array([1.2, 2, 0.4, 0.7, 0.2]).reshape(1, 5)\nx = x*x_scale\ny = x[:, 0]**2-2*x[:, 1]+10*x[:, 2]+0.04*x[:, 3]+np.abs(2*x[:, 4])\ny = ((y+np.random.rand(1000)) > 17).astype(np.float)\n\n# Tensor Converting\nX = Tensor(x)\nY = Tensor(y.reshape(1000, 1))\n\n# class Model(nn.Module):\n#     def __init__(self):\n#         self.linear_1 = nn.Linear(5, 100)\n#         self.linear_2 = nn.Linear(100, 2)\n\n#     def forward(self, x):\n#         return F.sigmoid(self.linear_2(F.leaky_relu(self.linear_1(x))))\n\nclass Model(nn.Module):\n    def __init__(self):\n        self.main = nn.Sequential(\n            nn.Linear(5, 1000),\n            nn.ReLU(),\n            nn.Linear(1000, 2),\n            nn.Sigmoid(),\n        )\n    \n    def forward(self, x):\n        return self.main(x)\n\nmodel = Model()\nadam = Adam(model.parameters())\n\nfor _ in range(3000):\n    indices = np.random.randint(0, 1000, size=128)\n    X_ = X[indices]\n    Y_ = Y[indices]\n    pred_y = model(X_)\n    loss = F.binary_cross_entropy(pred_y, Y_)\n    adam.zero_grad()\n    loss.backward()\n    adam.step(1e-3)\n    if _ % 1000 == 0:\n        print(loss)\n\npred_y = model(X)\nprint(((pred_y.data > 0.5) == Y.data).mean())\n'"
testcase.py,0,"b'import numpy as np\nfrom ctensor import Tensor\nfrom ctensor.functional import conv2d, relu\n\n\nD = Tensor.ones((4, 1, 199, 199))\nW1 = Tensor.ones((1, 1, 3, 3))\n\nI = conv2d(D, W1, padding=(1, 1))\nI.mean().backward()\nprint(W1.grad)\n\nD = Tensor.ones((1, 1, 19, 19))\nW1 = Tensor.ones((1, 1, 3, 3))\n\nI = conv2d(D, W1, padding=(1, 1))\nI.mean().backward()\nprint(W1.grad)\n\nD = Tensor.ones((1, 100))\nW1 = Tensor.ones((100, 50))\n(D @ W1).mean().backward()\nprint(W1.grad)\n\nD = Tensor.ones((100, 100))\nW1 = Tensor.ones((100, 50))\n(D @ W1).mean().backward()\nprint(W1.grad)\n'"
ctensor/__init__.py,0,"b""from .tensor import Tensor\n__all__ = ['Tensor']\n"""
ctensor/functional.py,0,"b'from .operator import Sigmoid, ReLU, LeakyReLU, Conv2d\n\ndef sigmoid(x):\n    return Sigmoid()(x)\n    #return  1.0/(1.0+(-x).exp()) # slower implementation\n\n\ndef relu(x):\n    return ReLU()(x)\n\n\ndef leaky_relu(x, leaky_rate=0.01):\n    return LeakyReLU(leaky_rate=leaky_rate)(x)\n\n\ndef mean_squared_error(pred, label):\n    return ((pred-label)**2).mean()\n\n\ndef binary_cross_entropy(pred, label):\n    return -((1. - label)*((1. + 1e-7 - pred).log())+(label)*(pred.log())).mean()\n\n\ndef conv2d(x, weight, padding=(0, 0), stride=(1, 1)):\n    return Conv2d(padding, stride)(x, weight)\n'"
ctensor/nn.py,2,"b'import numpy as np\nfrom .tensor import Tensor\nfrom . import functional as F\n\nclass Parameter(Tensor):\n    def __init__(self, data):\n        if isinstance(data, np.ndarray):\n            Tensor.__init__(self, data)\n        else:\n            Tensor.__init__(self, data.data)\n\nclass Module:\n    def parameters(self):\n        params = [getattr(self, i) for i in dir(self) if isinstance(\n            getattr(self, i), Parameter)]\n        submodules = [getattr(self, i) for i in dir(\n            self) if isinstance(getattr(self, i), Module)]\n        for p in params:\n            yield p\n        for sm in submodules:\n            yield from sm.parameters()\n\n    def __call__(self, *args):\n        return self.forward(*args)\n\nclass Sequential(Module):\n    def __init__(self, *args):\n        self.seqential = args\n    \n    def forward(self, x):\n        for s in self.seqential:\n            x = s(x)\n        return x\n    \n    def parameters(self):\n        for sm in self.seqential:\n            yield from sm.parameters()\n\nclass Linear(Module):\n    def __init__(self, in_channels, out_channels, bias=True):\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.w = Parameter(np.random.randn(\n            in_channels, out_channels)/((in_channels*out_channels)))\n        if bias:\n            self.bias = Parameter(Tensor.zeros(((1, out_channels))))\n        else:\n            self.bias = None\n    \n    def forward(self, x):\n        if self.bias:\n            return x @ self.w + self.bias\n        else:\n            return x @ self.w\n\nclass Conv2d(Module):\n    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1), stride=(1, 1), bias=True):\n        param = (in_channels, ) + (out_channels, ) + kernel_size\n        self.w = Parameter(\n            Tensor.randn(*param)/(in_channels*(kernel_size[0]*kernel_size[1])**0.5)\n            )\n        if bias:\n            self.bias = Parameter(Tensor.zeros((1, out_channels, 1, 1)))\n        else:\n            self.bias = None\n        self.padding = padding\n        self.stride = stride\n    \n    def forward(self, x):\n        if self.bias:\n            return F.conv2d(x, self.w, self.padding, self.stride) + self.bias\n        else:\n            return F.conv2d(x, self.w, self.padding, self.stride)\n\nclass ReLU(Module):\n    def forward(self, x):\n        return F.relu(x)\n\n\nclass LeakyReLU(Module):\n    def forward(self, x):\n        return F.leaky_relu(x)\n\nclass Sigmoid(Module):\n    def forward(self, x):\n        return F.sigmoid(x)\n'"
ctensor/operator.py,8,"b""import numpy as np\nfrom .tensor import *\n\nclass ReLU(Operator):\n    def forward(self, x):\n        data = x.data\n        self.loc = data >= 0\n        return Tensor(data*self.loc)\n\n    def backward(self, x, precedents):\n        u, = precedents\n        u.grad += x.grad*self.loc\n\n\nclass LeakyReLU(Operator):\n    def __init__(self, leaky_rate=0.01):\n        self.leaky_rate = leaky_rate\n\n    def forward(self, x):\n        data = x.data\n        loc = data >= 0\n        self.effc = loc + self.leaky_rate*(1-loc)\n        return Tensor(data*self.effc)\n\n    def backward(self, x, precedents):\n        u, = precedents\n        u.grad += x.grad*self.effc\n\n\nclass Sigmoid(Operator):\n    def forward(self, x):\n        data = x.data\n        self.result = 1.0/(1.0+np.exp(-data))\n        return Tensor(self.result)\n\n    def backward(self, x, precedents):\n        u, = precedents\n        u.grad += x.grad*(self.result)*(1-self.result)\n\n\nclass Conv2d(Operator):\n    def __init__(self, padding=(0, 0), stride=(1, 1)):\n        self.padding = padding\n        self.stride = stride\n\n    def forward(self, t, weight):\n        t = t.data\n        w = weight.data\n        t = make_padding(t, self.padding)\n        B, C, iH, iW = t.shape\n        iC, oC, kH, kW = w.shape\n        assert C == iC, 'Conv2d channels in not equal.'\n        return Tensor(batch_conv2d_f(t, w, self.stride))\n\n    def backward(self, x, precedents):\n        t, weight = precedents\n\n        t.grad += unwrap_padding(\n            batch_conv2d_im_backward_f(x.grad, weight.data, self.stride),\n            self.padding\n        )\n        weight.grad += batch_conv2d_weight_backward_f(\n            x.grad,\n            make_padding(t.data, self.padding),\n            self.stride\n        )\n\ndef batch_conv2d_f(x, kernel, stride=(1, 1)):\n    x = im2bchwkl(x, kernel.shape[-2:], stride)\n    return np.tensordot(x, kernel, [(1, 4, 5), (0, 2, 3)]).transpose(0, 3, 1, 2)\n\n\ndef batch_conv2d_weight_backward_f(kernel, input, stride=(1, 1)):\n    '''kernel is result tensor grad, input is original tensor'''\n    B, C, H, W = kernel.shape\n    x = im2bchwkl(input, kernel.shape[-2:], dilation=stride)\n    return np.tensordot(x, kernel, [(0, 4, 5), (0, 2, 3)]).transpose(0, 3, 1, 2)\n\n\ndef batch_conv2d_im_backward_f(x, kernel, stride=(1, 1)):\n    '''input is result tensor grad, kernel is weight tensor'''\n    ksize = kernel.shape\n    x = dilate_input(x, stride)\n    x = make_padding(x, ((ksize[2]-1), (ksize[3]-1)))\n    return batch_transposed_conv2d_f(x, kernel, invert=True)\n\n\ndef batch_transposed_conv2d_f(x, kernel, invert=False):\n    ksize = kernel.shape\n    x = transpose_kernel(\n        im2bchwkl(x, ksize[-2:])\n    )\n    i = 1 if invert else 0\n    return np.tensordot(x, kernel, [(1, 4, 5), (i, 2, 3)]).transpose(0, 3, 1, 2)\n\n\ndef im2bchwkl(input, ksize, stride=(1, 1), padding=(0, 0), dilation=(1, 1), writeable=False):\n    if padding != (0, 0):\n        assert not writeable, 'No writable in padding mode.'\n        input = make_padding(input, (padding[0], padding[1]))\n\n    isize = input.shape\n    istrides = input.strides\n\n    H = (isize[2]-(dilation[0]*(ksize[0]-1)+1))/(stride[0])+1\n    W = (isize[3]-(dilation[1]*(ksize[1]-1)+1))/(stride[1])+1\n    assert int(H) == H and int(W) == W, 'conv2d not aligned'\n    H = int(H)\n    W = int(W)\n    istrides = list(istrides+istrides[-2:])\n    istrides[2] *= stride[0]\n    istrides[3] *= stride[1]\n    istrides[4] *= dilation[0]\n    istrides[5] *= dilation[1]\n    return np.lib.stride_tricks.as_strided(input,\n                                           (isize[0], isize[1], H,\n                                            W, ksize[0], ksize[1]),\n                                           istrides,\n                                           writeable=writeable,\n                                           )\n\n\ndef make_padding(input, padding):\n    if padding == (0, 0):\n        return input\n    b, c, h, w = input.shape\n    p, q = padding\n    result = np.zeros((b, c, h+2*p, w+2*q), dtype=np.float32)\n    result[:, :, p:-p, q:-q] = input\n    return result\n\n\ndef unwrap_padding(input, padding):\n    if padding == (0, 0):\n        return input\n    p, q = padding\n    return input[..., p:-p, q:-q]\n\n\ndef transpose_kernel(kernel):\n    return kernel[..., ::-1, ::-1]\n\n\ndef dilate_input(input, stride=(1, 1)):\n    if stride == (1, 1):\n        return input\n    isize = input.shape\n    x = np.zeros((isize[0], isize[1], (isize[2]-1) *\n                  stride[0]+1, (isize[3]-1)*stride[1]+1), dtype=np.float32)\n    x[..., ::stride[0], ::stride[1]] = input\n    return x\n"""
ctensor/optim.py,4,"b""import numpy as np\nfrom .tensor import *\n\nclass Optimizer:\n    def __init__(self, parameters):\n        assert parameters, 'Your parameters?'\n        self.parameters = list(parameters)\n\n    def zero_grad(self):\n        for p in self.parameters:\n            #p.grad = np.zeros_like(p.data)\n            p.grad *= 0.0\n\n    def step(self, lr=0.01):\n        assert False, 'Optimizer class is virtual'\n\n\nclass SGD(Optimizer):\n    def step(self, lr=0.01):\n        for p in self.parameters:\n            p.data -= lr*p.grad\n\n\nclass Adam(Optimizer):\n    def __init__(self, parameters, beta1=0.9, beta2=0.99, eta=1e-6):\n        super(Adam, self).__init__(parameters)\n        self.m = []\n        self.v = []\n        for p in self.parameters:\n            self.m.append(np.zeros_like(p.grad))\n            self.v.append(np.zeros_like(p.grad))\n\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.eta = eta\n\n    def step(self, lr=0.01):\n        '''\n        Adam optimizer stepping.\n        We ignore the unbiasing process as it hurts efficiency.\n        '''\n\n        beta1 = self.beta1\n        beta2 = self.beta2\n        for idx, p in enumerate(self.parameters):\n            m = self.m[idx]\n            v = self.v[idx]\n            grad = p.grad\n            m[...] = beta1*m + (1-beta1)*grad\n            v[...] = beta2*v + (1-beta2)*(grad**2)\n            # ignoring unbiasing\n            p.data -= lr*m/(np.sqrt(v)+self.eta)\n\n"""
ctensor/tensor.py,15,"b'import numpy as np\n\ndef T(x):\n    \'\'\'Transpose a 3-d matrix, [batch x N x M].\'\'\'\n\n    return x.transpose(0, 2, 1)\n\n\ndef make_tensor_like(another, sel):\n    \'\'\'Make the operand a tensor.\'\'\'\n\n    assert isinstance(another, int) or isinstance(another, float) or isinstance(\n        another, Tensor), \'Cannot convert to tensor\'\n\n    if isinstance(another, int) or isinstance(another, float):\n        s = (1,)*len(sel.data.shape)\n        return Tensor(np.zeros(s, dtype=np.float32)+another, requires_grad=False)\n    return another\n\n\ndef attach_grad(u, grad):\n    \'\'\'\n    During backpropagation, attach computed grad into precedents.\n    If forward process includes broadcasting, unbroadcast grad.\n    \'\'\'\n    if not u.requires_grad:\n        return\n\n    if u.grad.shape == grad.shape:\n        u.grad += grad\n        return\n\n    # unbroadcasting\n    for dim, chn in enumerate(u.grad.shape):\n        if chn != grad.shape[dim]:\n            assert chn == 1, ""Backward unbroadcasting errors""\n            grad = grad.mean(axis=dim, keepdims=True)\n\n    u.grad += grad\n\n\nclass Tensor:\n    def __init__(self, ndarray, precedents=None, operator=None, requires_grad=True):\n        \n        if ndarray.dtype == np.float32:\n            self.data = ndarray\n        else:\n            self.data = ndarray.astype(np.float32)\n        \n        if requires_grad:\n            self.grad = np.zeros_like(self.data)\n        else:\n            self.grad = np.empty_like(self.data)\n            \n        self.precedents = precedents\n        self.operator = operator\n        self.requires_grad = requires_grad\n        if precedents:\n            self.leaf = False\n        else:\n            self.leaf = True\n\n    def backward(self, internal=False):\n        if not internal:\n            self.grad = np.ones_like(self.grad)\n\n        if self.leaf:\n            return\n\n        if isinstance(self.operator, Operator):\n            self.operator.backward(self, self.precedents)\n\n        elif self.operator == \'neg\':\n            u, = self.precedents\n            u.grad += -self.grad\n\n        elif self.operator == \'abs\':\n            u, = self.precedents\n            u.grad += self.grad * np.sign(u.data)\n\n        elif self.operator == \'exp\':\n            u, = self.precedents\n            u.grad += self.grad * self.data\n\n        elif self.operator == \'log\':\n            u, = self.precedents\n            u.grad += self.grad * (1.0/u.data)\n\n        elif self.operator == \'sum\':\n            u, = self.precedents\n            u.grad += self.grad\n\n        elif self.operator == \'mean\':\n            u, = self.precedents\n            elements = 1\n            for s in u.grad.shape:\n                elements *= s\n            u.grad += self.grad / elements\n\n        elif self.operator == \'slice\':\n            u, slic = self.precedents\n            u.grad[slic] += self.grad\n\n        elif self.operator == \'+\':\n            u, v = self.precedents\n            attach_grad(u, self.grad)\n            attach_grad(v, self.grad)\n\n        elif self.operator == \'-\':\n            u, v = self.precedents\n            attach_grad(u, self.grad)\n            attach_grad(v, -self.grad)\n\n        elif self.operator == \'/\':\n            u, v = self.precedents\n            attach_grad(u, -self.grad * v.data / (u.data**2))\n\n        elif self.operator == \'*\':\n            u, v = self.precedents\n            attach_grad(u, self.grad * v.data)\n            attach_grad(v, self.grad * u.data)\n\n        elif self.operator == \'**\':\n            u, v = self.precedents\n            attach_grad(u, self.grad * u.data**(v.data-1) * v.data)\n\n        elif self.operator == \'greater\':\n            u, v = self.precedents\n            attach_grad(u, self.grad * (u.data > v.data))\n\n        elif self.operator == \'@\':\n            u, v = self.precedents\n            if len(self.data.shape) == 3:\n                attach_grad(u, self.grad @ T(v.data))\n                attach_grad(v, T(u.data) @ self.grad)\n            else:\n                attach_grad(u, self.grad @ v.data.T)\n                attach_grad(v, u.data.T @ self.grad)\n\n        for p in self.precedents:\n            if isinstance(p, Tensor) and p.requires_grad:\n                p.backward(internal=True)\n\n    def __neg__(self):\n        return Tensor(-self.data, precedents=[self], operator=\'neg\')\n\n    def __pos__(self):\n        return self\n\n    def abs(self):\n        return Tensor(np.abs(self.data), precedents=[self], operator=\'abs\')\n\n    def sum(self, dim=None):\n        return Tensor(np.sum(self.data, dim), precedents=[self], operator=\'sum\')\n\n    def mean(self, dim=None):\n        return Tensor(np.mean(self.data, dim), precedents=[self], operator=\'mean\')\n\n    def exp(self):\n        return Tensor(np.exp(self.data), precedents=[self], operator=\'exp\')\n\n    def log(self):\n        return Tensor(np.log(self.data), precedents=[self], operator=\'log\')\n\n    def __add__(self, another):\n        another = make_tensor_like(another, self)\n        return Tensor(self.data + another.data, precedents=[self, another], operator=\'+\')\n\n    def __radd__(self, another):\n        return Tensor.__add__(self, another)\n\n    def __sub__(self, another):\n        another = make_tensor_like(another, self)\n        return Tensor(self.data - another.data, precedents=[self, another], operator=\'-\')\n\n    def greater(self, another):\n        another = make_tensor_like(another, self)\n        return Tensor(self.data > another.data, precedents=[self, another], operator=\'greater\', requires_grad=False)\n\n    def __rsub__(self, another):\n        another = make_tensor_like(another, self)\n        return Tensor.__sub__(another, self)\n\n    def __pow__(self, another):\n        another = make_tensor_like(another, self)\n        another.requires_grad = False\n        return Tensor(self.data ** another.data, precedents=[self, another], operator=\'**\')\n\n    def __truediv__(self, another):\n        assert isinstance(another, int) or isinstance(\n            another, float),         \'Right divide only supports int or float. Please use *\'\n        another = make_tensor_like(another, self)\n        another.data = 1.0/another.data\n        return Tensor.__mul__(self, another)\n\n    def __rtruediv__(self, another):\n        another = make_tensor_like(another, self)\n        return Tensor(another.data / self.data, precedents=[self, another], operator=\'/\')\n\n    def __mul__(self, another):\n        another = make_tensor_like(another, self)\n        return Tensor(self.data * another.data, precedents=[self, another], operator=\'*\')\n\n    def __rmul__(self, another):\n        return Tensor.__mul__(self, another)\n\n    def __matmul__(self, another):\n        return Tensor(self.data @ another.data, precedents=[self, another], operator=\'@\')\n\n    def __getitem__(self, slic):\n        return Tensor(self.data[slic], precedents=[self, slic], operator=\'slice\')\n\n    def view(self, *shape):\n        return View(shape)(self)\n\n    def __repr__(self):\n        return ""Tensor({})"".format(self.data)\n\n    @staticmethod\n    def zeros(args):\n        return Tensor(np.zeros(args, dtype=np.float32))\n\n    @staticmethod\n    def randn(args):\n        return Tensor(np.random.randn(*args, dtype=np.float32))\n\n    @staticmethod\n    def ones(args):\n        return Tensor(np.ones(args, dtype=np.float32))\n\nclass Operator:\n    def forward(self, *args):\n        return args[0]\n\n    def __call__(self, *args):\n        fwd = self.forward(*args)\n        if fwd.precedents:\n            # Operator in Tensor\n            return Tensor(fwd.data, precedents=[fwd], operator=self)\n        else:\n            # Operation in NumPy\n            return Tensor(fwd.data, precedents=args, operator=self)\n\n    def backward(self, x, precedents):\n        p, = precedents\n        p.grad = x.grad\n\n\nclass View(Operator):\n    def __init__(self, shape):\n        self.shape = shape\n\n    def forward(self, x):\n        self.origin_shape = x.data.shape\n        return Tensor(x.data.reshape(*self.shape))\n\n    def backward(self, x, precedents):\n        u, = precedents\n        u.grad += x.grad.reshape(self.origin_shape)\n\n\n# class Add(Operator):\n#     def forward(self, x, y):\n#         return Tensor(x.data + y.data)\n    \n#     def backward(self, u, precedents):\n#         x, y = precedents\n#         x.grad += u.grad\n#         y.grad += u.grad\n\n\n# class Mul(Operator):\n#     def forward(self, x, y):\n#         return Tensor(x.data @ y.data)\n\n#     def backward(self, u, precedents):\n#         x, y = precedents\n#         if len(u.data.shape) == 3:\n#             attach_grad(x, u.grad @ T(y.data))\n#             attach_grad(y, T(x.data) @ u.grad)\n#         else:\n#             attach_grad(x, u.grad @ y.data.T)\n#             attach_grad(y, x.data.T @ u.grad)\n'"
