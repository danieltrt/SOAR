file_path,api_count,code
setup.py,0,"b'import os\nimport setuptools\nimport sys\n\ndef read(fname):\n  with open(os.path.join(os.path.dirname(__file__), fname), \'rt\') as f:\n    return f.read()\n\ndef requirements():\n  with open(os.path.join(os.path.dirname(__file__), \'requirements.txt\'), \'rt\') as f:\n    return f.readlines()\n\nsetuptools.setup(\n  name=""cloud-volume"",\n  version=""1.18.0"",\n  setup_requires=[\n    \'numpy<1.17; python_version<""3.5""\',\n    \'numpy; python_version>=""3.5""\',\n  ],\n  install_requires=requirements(),\n  # Environment Marker Examples:\n  # https://www.python.org/dev/peps/pep-0496/\n  extras_require={\n    ""boss"": [\n      ""intern>=0.9.11"",\n      ""blosc==1.8.3"",\n    ],\n    \':sys_platform!=""win32""\': [\n      ""posix_ipc==1.0.4"",\n      ""psutil==5.4.3"",\n    ],\n    \':sys_platform!=""win32"" and python_version>=""3.0""\': [\n      ""DracoPy"",\n    ],\n    ""mesh_viewer"": [ \'vtk\' ],\n    ""skeleton_viewer"": [ \'matplotlib\' ],\n    ""all_viewers"": [ \'vtk\', \'matplotlib\' ],\n    ""dask"": [ \'dask[array]\' ],\n    ""test"": [ ""pytest"", ""pytest-cov"", ""codecov"", ""requests_mock"", ""scipy""]\n  },\n  author=""William Silversmith, Nico Kemnitz, Ignacio Tartavull, and others"",\n  author_email=""ws9@princeton.edu"",\n  packages=setuptools.find_packages(),\n  package_data={\n    \'cloudvolume\': [\n      \'./microviewer/*\',\n      \'LICENSE\',\n    ],\n  },\n  description=""A serverless client for reading and writing Neuroglancer Precomputed volumes both locally and on cloud services."",\n  long_description=read(\'README.md\'),\n  long_description_content_type=""text/markdown"",\n  license = ""BSD 3-Clause"",\n  keywords = ""neuroglancer volumetric-data numpy connectomics microscopy image-processing biomedical-image-processing s3 gcs mesh meshes skeleton skeletons"",\n  url = ""https://github.com/seung-lab/cloud-volume/"",\n  classifiers=[\n    ""Intended Audience :: Developers"",\n    ""Development Status :: 5 - Production/Stable"",\n    ""License :: OSI Approved :: BSD License"",\n    ""Programming Language :: Python"",\n    ""Programming Language :: Python :: 2"",\n    ""Programming Language :: Python :: 2.7"",\n    ""Programming Language :: Python :: 3"",\n    ""Programming Language :: Python :: 3.5"",\n    ""Programming Language :: Python :: 3.6"",\n    ""Programming Language :: Python :: 3.7"",\n    ""Topic :: Scientific/Engineering"",\n    ""Intended Audience :: Science/Research"",\n    ""Operating System :: POSIX"",\n    ""Operating System :: MacOS"",\n    ""Operating System :: Microsoft :: Windows :: Windows 10"",\n    ""Topic :: Utilities"",\n  ],\n)\n'"
setversion.py,0,"b'import re\nimport subprocess\nimport sys\n\ndef get_latest_version():\n\ttags = str(subprocess.check_output(\'git tag\', shell=True))\n\ttags = re.sub(""^b\'"", \'\', tags)\n\ttags = re.sub(""\'$"", \'\', tags)\n\ttags = re.split(r\'\\\\n\', tags)\n\ttags = [ tuple(map(int, re.split(\'\\.\', tag))) for tag in tags if tag ]\n\ttags = sorted(tags, reverse=True)\n\tversion = [ str(_) for _ in tags[0] ]\n\treturn \'.\'.join(version)\n\ndef materialize_version(version):\n\t# version = get_version()\n\twith open(\'cloudvolume/__init__.py\', \'rt\') as f:\n\t\tcode = f.read()\n\n\tcode = re.sub(""__version__ = \'.*?\'"", ""__version__ = \'{}\'"".format(version), code)\n\twith open(\'cloudvolume/__init__.py\', \'wt\') as f:\n\t\tf.write(code)\n\n\twith open(\'setup.py\', \'rt\') as f:\n\t\tcode = f.read()\n\n\tcode = re.sub(r\'version\\s*=\\s*"".*?"",\', \'version=""{}"",\'.format(version), code)\n\twith open(\'setup.py\', \'wt\') as f:\n\t\tf.write(code)\n\nversion = sys.argv[1]\nmessage = sys.argv[2]\n\nmaterialize_version(version)\nprint(""Updated __init__.py with version "" + version)\n\nassert len(version) < 9\nsubprocess.check_output(""git reset && git add cloudvolume/__init__.py setup.py && git commit -m \'Version {}\'"".format(version), shell=True)\n\ntry:\n\tsubprocess.check_output([\'git\', \'tag\', \'-a\',  version, \'-m\', message])\n\tprint(\'Created tag \' + version)\nexcept:\n\tpass\n\n'"
benchmarks/benchmark.py,2,"b'from collections import defaultdict\nfrom copy import deepcopy\nimport time\nimport re\nimport json\nimport socket\nfrom operator import mul\nfrom functools import reduce\n\nimport numpy as np\nfrom cloudvolume import CloudVolume\n\nN = 3\n\nCHUNK_SIZES = ( \n\t(128,128,1),\n\t(128,128,2),\n\t(128,128,4),\n\t(128,128,8),\n\t(128,128,16),\n\t(128,128,32),\n\t(128,128,64),\n\t(128,128,128),\n\t(1024,1024,1),\n\t(1024,1024,2),\n\t(1024,1024,4),\n\t(1024,1024,8),\n\t(1024,1024,16),\n\t(1024,1024,32),\n\t(1024,1024,64),\n\t(1024,1024,100), # max size of SNEMI3D\n)\n\nlogfile = open(\'./benchmark.tsv\', \'wt\')\nlogfile.write(""hostname\\tdirection\\tcompression\\timage_type\\tchunk_size\\tdtype\\tMB\\tMean MB/sec\\tN\\tmean (sec)\\tfastest (sec)\\tslowest (sec)\\tcloudpath\\n"")\nlogfile.flush()\n\ndef stopwatch(fn):\n\tstart = time.time()\n\tfn()\n\treturn time.time() - start\n\ndef benchmark(fn, N):\n\tlst = [ stopwatch(fn) for _ in range(N) ]\n\treturn {\n\t\t\'mean\': (sum(lst) / N), \n\t\t\'fastest\': min(lst), \n\t\t\'slowest\': max(lst), \n\t\t\'N\': N,\n\t}\n\ndef MBs(vol, sec):\n\tdtype_bytes = np.dtype(vol.dtype).itemsize\n\ttotal_bytes = vol.bounds.volume() * dtype_bytes\n\treturn total_bytes / 1e6 / sec\n\ndef disp(desc, vol, stats):\n\tmbs = lambda sec: MBs(vol, sec)\n\treturn ""%s -- mean=%.2f MB/sec, range=%.2f to %.2f MB/sec; N=%d"" % (\n\t\tdesc, mbs(stats[\'mean\']), mbs(stats[\'slowest\']), mbs(stats[\'fastest\']), stats[\'N\']\n\t)\n\ndef log(row):\n\tglobal logfile\n\tdtype_bytes = np.dtype(row[\'dtype\']).itemsize\n\trow[\'MB\'] = reduce(mul, row[\'chunk_size\']) * dtype_bytes / 1024**2\n\trow[\'MBs\'] = row[\'MB\'] / row[\'mean\']\n\n\tfor k,v in row.items():\n\t\tif type(v) is float:\n\t\t\trow[k] = \'%.3f\' % v\n\n\tentry = ""{hostname}\\t{direction}\\t{compression}\\t{image_type}\\t{chunk_size}\\t{dtype}\\t{MB}\\t{MBs}\\t{N}\\t{mean}\\t{fastest}\\t{slowest}\\t{cloudpath}\\n"".format(**row)\n\tlogfile.write(entry)\n\tlogfile.flush()\n\ndef benchmark_upload(voltype):\n\tglobal CHUNK_SIZES\n\tglobal N\n\n\toriginvol = CloudVolume(\'gs://seunglab-test/test_v0/{}\'.format(voltype))\n\toriginvol.reset_scales()\n\tinfo = deepcopy(originvol.info)\n\n\timg = originvol[:]\n\n\tfor chunk_size in CHUNK_SIZES[::-1]:\n\t\tcloudpath = \'gs://seunglab-test/test_v0/{}_upload_{}_{}_{}\'.format(voltype, *chunk_size)\n\t\tinfo[\'scales\'][0][\'chunk_sizes\'] = [ list(chunk_size) ]\n\t\tvol = CloudVolume(cloudpath, progress=True, info=info, compress=\'gzip\')\n\n\t\tdef upload():\n\t\t\tvol[:] = img\n\n\t\tstats = benchmark(upload, N)\n\n\t\tlog({\n\t\t\t""direction"": ""upload"",\n\t\t\t""compression"": ""gzip"",\n\t\t\t""image_type"": voltype,\n\t\t\t""N"": N,\n\t\t\t""mean"": stats[\'mean\'],\n\t\t\t""fastest"": stats[\'fastest\'],\n\t\t\t""slowest"": stats[\'slowest\'],\n\t\t\t""cloudpath"": cloudpath,\n\t\t\t""chunk_size"": chunk_size,\n\t\t\t""dtype"": vol.dtype,\n\t\t\t""hostname"": socket.gethostname(),\n\t\t})\n\n\t\tvol.delete(vol.bounds)\n\n\tfor chunk_size in CHUNK_SIZES[::-1]:\n\t\tcloudpath = \'gs://seunglab-test/test_v0/{}_upload_{}_{}_{}\'.format(voltype, *chunk_size)\n\t\tinfo[\'scales\'][0][\'chunk_sizes\'] = [ list(chunk_size) ]\n\t\tvol = CloudVolume(cloudpath, progress=True, info=info, compress=\'\')\n\n\t\tdef upload():\n\t\t\tvol[:] = img\n\t\t\n\t\tstats = benchmark(upload, N)\n\n\t\tlog({\n\t\t\t""direction"": ""upload"",\n\t\t\t""compression"": ""none"",\n\t\t\t""image_type"": voltype,\n\t\t\t""N"": N,\n\t\t\t""mean"": stats[\'mean\'],\n\t\t\t""fastest"": stats[\'fastest\'],\n\t\t\t""slowest"": stats[\'slowest\'],\n\t\t\t""cloudpath"": cloudpath,\n\t\t\t""chunk_size"": chunk_size,\n\t\t\t""dtype"": vol.dtype,\n\t\t\t""hostname"": socket.gethostname(),\n\t\t})\n\n\t\tvol.delete(vol.bounds)\n\n\ndef benchmark_download(voltype):\n\tglobal CHUNK_SIZES\n\tglobal N\n\n\tfor chunk_size in CHUNK_SIZES[::-1]:\n\t\tcloudpath = \'gs://seunglab-test/test_v0/{}_{}_{}_{}\'.format(voltype, *chunk_size)\n\t\tvol = CloudVolume(cloudpath, progress=True)\n\t\tstats = benchmark(lambda: vol[:], N)\n\n\t\tlog({\n\t\t\t""direction"": ""download"",\n\t\t\t""compression"": ""gzip"",\n\t\t\t""image_type"": voltype,\n\t\t\t""N"": N,\n\t\t\t""mean"": stats[\'mean\'],\n\t\t\t""fastest"": stats[\'fastest\'],\n\t\t\t""slowest"": stats[\'slowest\'],\n\t\t\t""cloudpath"": cloudpath,\n\t\t\t""chunk_size"": chunk_size,\n\t\t\t""dtype"": vol.dtype,\n\t\t\t""hostname"": socket.gethostname(),\n\t\t})\n\n\tfor chunk_size in CHUNK_SIZES[::-1]:\n\t\tcloudpath = \'gs://seunglab-test/test_v0/{}_uncompressed_{}_{}_{}\'.format(voltype, *chunk_size)\n\t\tvol = CloudVolume(cloudpath, progress=True)\n\t\tstats = benchmark(lambda: vol[:], N)\n\n\t\tlog({\n\t\t\t""direction"": ""download"",\n\t\t\t""compression"": ""none"",\n\t\t\t""image_type"": voltype,\n\t\t\t""N"": N,\n\t\t\t""mean"": stats[\'mean\'],\n\t\t\t""fastest"": stats[\'fastest\'],\n\t\t\t""slowest"": stats[\'slowest\'],\n\t\t\t""cloudpath"": cloudpath,\n\t\t\t""chunk_size"": chunk_size,\n\t\t\t""dtype"": vol.dtype,\n\t\t\t""hostname"": socket.gethostname(),\n\t\t})\n\nbenchmark_download(\'black\')\nbenchmark_download(\'image\')\nbenchmark_download(\'segmentation\')\n\nbenchmark_upload(\'black\')\nbenchmark_upload(\'image\')\nbenchmark_upload(\'segmentation\')\n\n\nlogfile.close()\n'"
benchmarks/create_test_volumes.py,1,"b""from cloudvolume import CloudVolume\nimport numpy as np\nfrom copy import deepcopy\n\nblack = CloudVolume('gs://seunglab-test/test_v0/black')\ninfo = deepcopy(black.info)\n\nsizes = ( \n\t# [128,128,1],\n\t# [128,128,2],\n\t# [128,128,4],\n\t# [128,128,4],\n\t# [128,128,8],\n\t# [128,128,16],\n\t# [128,128,32],\n\t# [128,128,64],\n\t[128,128,128],\n\t[1024,1024,1],\n\t[1024,1024,2],\n\t[1024,1024,4],\n\t[1024,1024,8],\n\t[1024,1024,16],\n\t[1024,1024,32],\n\t[1024,1024,64],\n\t[1024,1024,100], # max size of SNEMI3D\n)\n\nimgvol = CloudVolume('gs://seunglab-test/test_v0/black', progress=True)\nimg = imgvol[:]\n\nfor chunksize in sizes:\n\tinfo['scales'][0]['chunk_sizes'] = [ chunksize ]\n\n\tlayer = 'black_{}_{}_{}'.format(*chunksize)\n\n\tvol = CloudVolume('gs://seunglab-test/test_v0/' + layer, info=info, compress='gzip', progress=True)\n\tvol.reset_scales()\n\tvol.commit_info()\n\n\t# vol[:] = np.zeros(shape=vol.shape, dtype=vol.dtype)\n\tvol[:] = img\n\n\tlayer = 'black_uncompressed_{}_{}_{}'.format(*chunksize)\n\n\tvol = CloudVolume('gs://seunglab-test/test_v0/' + layer, info=info, compress='', progress=True)\n\tvol.reset_scales()\n\tvol.commit_info()\n\n\tvol[:] = img\n\n"""
benchmarks/plot.py,0,"b""import matplotlib.pyplot as plt\nimport numpy as np\nimport csv\n\nimport sys\nfrom collections import defaultdict\n\nrows = []\n\nfilename = sys.argv[1] \n\nwith open(filename, 'r') as csvfile:\n  votereader = csv.reader(csvfile, delimiter='\\t', quotechar=None)\n  for row in votereader:\n        rows.append(row)\n\nheaders = rows.pop(0)\n\nstats = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n\nfor row in rows:\n\tdirection, imgtype, compression, MB, meanMBs = row[1], row[3], row[2], row[6], row[7]\n\tstats[direction][compression][imgtype][float(MB)] = float(meanMBs)\n\n\nstyles = {\n\t'black': 'k',\n\t'image': (.7, .7, .7, 1),\n\t'segmentation': 'r',\n}\n\n\nindex = 0\nfor direction in sorted(stats.keys()):\n\tdirexpr = stats[direction]\n\n\tfor compress in sorted(direxpr.keys()):\n\t\timageexper = direxpr[compress]\n\t\tindex += 1\n\n\t\tplt.subplot(2, 2, index)\n\t\tplt.title(filename + ': CloudVolume ' + direction + ' Speed w/ ' + compress + ' Compression')\n\t\tplt.xlabel('Chunk Size (MB)')\n\t\tplt.ylabel('MB/sec')\n\n\t\tlines = []\n\t\tlegends = []\n\n\t\tfor imgtype in imageexper.keys():\n\t\t\tstyle = styles[imgtype]\n\t\t\texperiment = imageexper[imgtype]\n\t\t\txdata = sorted([ float(mb) for mb in experiment.keys() ])\n\t\t\tydata = [ float(experiment[mb]) for mb in xdata ]\n\t\t\tline = plt.plot(xdata, ydata, color=style, linestyle='-', linewidth=2)\n\t\t\tlines.append(line)\n\t\t\tlegends.append(imgtype)\n\n\t\tplt.legend(legends)\nplt.show()\n\n\n\n\n\n\n"""
cloudvolume/__init__.py,0,"b'""""""\nA ""serverless"" Python client for reading and writing arbitrarily large \nNeuroglancer Precomputed volumes both locally and on cloud services. \nPrecomputed volumes consist of chunked numpy arrays, meshes, and \nskeletons and can be visualized using Neuroglancer. Typically these \nvolumes represent one or three channel 3D image stacks of microscopy \ndata or labels annotating them, but they can also represent large \ntensors with compatible dimensions. \n\n  https://github.com/seung-lab/cloud-volume\n\nPrecomputed volumes can be stored on any service that provides a \nkey-value mapping between a file path and file data. Typically, \nPrecomputed volumes are located on cloud storage providers such \nas Amazon S3 or Google Cloud Storage. However, these volumes can \nbe stored on any service, including the local file system or an \nordinary webserver that can process these key value mappings.\n\nNeuroglancer is a browser based WebGL 3D image viewer principally \nauthored by Jeremy Maitin-Shepard at Google. CloudVolume is a \nthird-party client for reading and writing Neuroglancer compatible \nformats: \n\n  https://github.com/google/neuroglancer\n\nCloudVolume is often paired with Igneous, an image processing engine \nfor visualizing and managing Precomputed volumes. Igneous can be\nrun locally or in the cloud using Kubernetes.\n\n  https://github.com/seung-lab/igneous\n\nThe combination of Neuroglancer, Igneous, and CloudVolume comprises \na system for visualizing, processing, and sharing (via browser viewable \nURLs) petascale datasets within and between laboratories.\n\nCloudVolume Example: \n\n  from cloudvolume import CloudVolume\n\n  vol = CloudVolume(\'gs://mylab/mouse/image\', progress=True)\n  image = vol[:,:,:] # Download an image stack as a numpy array\n  vol[:,:,:] = image # Upload an image stack from a numpy array\n\n  label = 1\n  mesh = vol.mesh.get(label) \n  skel = vol.skeletons.get(label)\n""""""\n\nfrom .cloudvolume import CloudVolume, register_plugin\n\nfrom .connectionpools import ConnectionPool\nfrom .lib import Bbox, Vec\nfrom .mesh import Mesh\nfrom .provenance import DataLayerProvenance\nfrom .storage import Storage\nfrom .threaded_queue import ThreadedQueue\nfrom .exceptions import (\n  EmptyVolumeException, EmptyRequestException, AlignmentError,\n  SkeletonEncodeError, SkeletonDecodeError\n)\nfrom .volumecutout import VolumeCutout\n\nfrom .skeleton import Skeleton, PrecomputedSkeleton\n\nfrom . import exceptions\nfrom . import secrets\n\nfrom . import microviewer\nfrom .microviewer import view, hyperview\n\n__version__ = \'1.18.0\'\n\n# Register plugins\nfrom .datasource.precomputed import register as register_precomputed\nfrom .datasource.graphene import register as register_graphene\n\nregister_precomputed()\nregister_graphene()\n\ntry:\n  from .datasource.boss import register as register_boss\n  register_boss()\nexcept ImportError:\n  pass\n\n\n\n\n\n\n'"
cloudvolume/cacheservice.py,0,"b'from functools import partial\nimport json\nimport os\nimport posixpath\nimport shutil\n\nfrom . import scheduler\n\nfrom .provenance import DataLayerProvenance\nfrom .storage import SimpleStorage, Storage, GreenStorage\nfrom .storage.storage_interfaces import COMPRESSION_EXTENSIONS\n\nfrom .lib import (\n  Bbox, colorize, jsonify, mkdir, \n  toabs, Vec\n)\nfrom .paths import extract\nfrom .secrets import CLOUD_VOLUME_DIR\n\ndef warn(text):\n  print(colorize(\'yellow\', text))\n\nclass CacheService(object):\n  def __init__(\n    self, cloudpath,\n    enabled, config,\n    meta=None, compress=None\n  ):\n    """"""\n    enabled: bool or path string\n    config: SharedConfiguration \n    meta: PrecomputedMetadata\n    compress: None = linked to dataset setting, bool = Force\n    """"""\n    self._path = extract(cloudpath)\n    self.path = self.default_path()\n\n    self.config = config\n    self._enabled = enabled \n    self.compress = compress \n\n    # b/c there\'s a semi-circular dependency\n    # meta is usually set afterwards\n    self.meta = meta \n\n    self.initialize()\n\n  def initialize(self):\n    if not self.enabled:\n      return \n\n    if not os.path.exists(self.path):\n      mkdir(self.path)\n\n    if not os.access(self.path, os.R_OK|os.W_OK):\n      raise IOError(\'Cache directory needs read/write permission: \' + self.path)\n\n  @property\n  def enabled(self):\n    return self._enabled\n\n  @enabled.setter\n  def enabled(self, val):\n    self._enabled = val \n    self.initialize()\n\n  def default_path(self):\n    basepath = self._path.basepath\n    if basepath[0] == os.path.sep:\n      basepath = basepath[1:]\n\n    return toabs(os.path.join(CLOUD_VOLUME_DIR, \'cache\', \n      self._path.protocol, basepath, self._path.layer\n    ))\n  \n  def num_files(self, all_mips=False):\n    def size(mip):\n      path_at_mip = os.path.join(self.path, self.meta.key(mip))\n      if not os.path.exists(path_at_mip):\n        return 0\n      return len(os.listdir(path_at_mip))\n\n    if all_mips:\n      sizes = [ 0 ] * (max(list(self.meta.available_mips)) + 1)\n      for i in self.meta.available_mips:\n        sizes[i] = size(i)\n      return sizes\n    else:\n      return size(self.config.mip)\n\n  def num_bytes(self, all_mips=False):\n    def mip_size(mip):\n      path_at_mip = os.path.join(self.path, self.meta.key(mip))\n      if not os.path.exists(path_at_mip):\n        return 0\n\n      return sum( \n        ( os.path.getsize(os.path.join(path_at_mip, filename)) for filename in os.listdir(path_at_mip) ) \n      )\n\n    if all_mips:\n      sizes = [ 0 ] * (max(list(self.meta.available_mips)) + 1)\n      for i in self.meta.available_mips:\n        sizes[i] = mip_size(i)\n      return sizes\n    else:\n      return mip_size(self.config.mip)\n\n  def has(self, filename):\n    with SimpleStorage(\'file://\' + self.path) as stor:\n      return stor.exists(filename)\n\n  def list(self, mip=None):\n    mip = self.config.mip if mip is None else mip\n\n    path = os.path.join(self.path, self.meta.key(mip))\n\n    if not os.path.exists(path):\n      return []\n\n    return os.listdir(path)\n\n  def list_skeletons(self):\n    if self.meta.skeletons is None:\n      return []\n\n    path = os.path.join(self.path, self.meta.skeletons)\n    if not os.path.exists(path):\n      return []\n\n    return os.listdir(path)\n\n  def list_meshes(self):\n    if self.meta.mesh is None:\n      return []\n\n    path = os.path.join(self.path, self.meta.mesh)\n    if not os.path.exists(path):\n      return []\n\n    return os.listdir(path)\n\n  def flush_info(self):\n    path = os.path.join(self.path , \'info\')\n    if not os.path.exists(path):\n      return\n    os.remove(path)\n\n  def flush_provenance(self):\n    path = os.path.join(self.path , \'provenance\')\n    if not os.path.exists(path):\n      return\n    os.remove(path)\n\n  def flush(self, preserve=None):\n    """"""\n    Delete the cache for this dataset. Optionally preserve\n    a region. Helpful when working with overlaping volumes.\n\n    Warning: the preserve option is not multi-process safe.\n    You\'re liable to end up deleting the entire cache.\n\n    Optional:\n      preserve (Bbox: None): Preserve chunks located partially\n        or entirely within this bounding box. \n    \n    Return: void\n    """"""\n    if not os.path.exists(self.path):\n      return\n\n    if preserve is None:\n      shutil.rmtree(self.path)\n      return\n\n    for mip in self.meta.available_mips:\n      preserve_mip = self.meta.bbox_to_mip(preserve, 0, mip)\n      mip_path = os.path.join(self.path, self.meta.key(mip))\n\n      if not os.path.exists(mip_path):\n        continue\n\n      for filename in os.listdir(mip_path):\n        bbox = Bbox.from_filename(filename)\n        if not Bbox.intersects(preserve_mip, bbox):\n          os.remove(os.path.join(mip_path, filename))\n\n  # flush_cache_region seems like it could be tacked on\n  # as a flag to delete, but there are reasons not\n  # to do that. \n  # 1) reduces the risks of disasterous programming errors. \n  # 2) doesn\'t require chunk alignment\n  # 3) processes potentially multiple mips at once\n\n  def flush_region(self, region, mips=None):\n    """"""\n    Delete a cache region at one or more mip levels \n    bounded by a Bbox for this dataset. Bbox coordinates\n    should be specified in mip 0 coordinates.\n\n    Required:\n      region (Bbox): Delete cached chunks located partially\n        or entirely within this bounding box. \n    Optional:\n      mip (int: None): Flush the cache from this mip. Region\n        is in global coordinates.\n    \n    Return: void\n    """"""\n    if not os.path.exists(self.path):\n      return\n  \n    cur_mip = self.config.mip\n\n    region = Bbox.create(region, self.meta.bounds(cur_mip))\n    mips = ( cur_mip, ) if mips == None else mips\n\n    for mip in mips:\n      mip_path = os.path.join(self.path, self.meta.key(mip))\n      if not os.path.exists(mip_path):\n        continue\n\n      region_mip = self.meta.bbox_to_mip(region, mip=0, to_mip=mip)\n      for filename in os.listdir(mip_path):\n        bbox = Bbox.from_filename(filename)\n        if not Bbox.intersects(region, bbox):\n          os.remove(os.path.join(mip_path, filename))\n\n  def check_info_validity(self):\n    """"""\n    ValueError if cache differs at all from source data layer with\n    an excepton for volume_size which prints a warning.\n    """"""\n    cache_info = self.get_json(\'info\')\n    if not cache_info:\n      return\n\n    fresh_info = self.meta.fetch_info()\n\n    mismatch_error = ValueError(""""""\n      Data layer info file differs from cache. Please check whether this\n      change invalidates your cache. \n\n      If VALID do one of:\n      1) Manually delete the cache (see location below)\n      2) Refresh your on-disk cache as follows:\n        vol = CloudVolume(..., cache=False) # refreshes from source\n        vol.cache = True\n        vol.commit_info() # writes to disk\n      If INVALID do one of: \n      1) Delete the cache manually (see cache location below) \n      2) Instantiate as follows: \n        vol = CloudVolume(..., cache=False) # refreshes info from source\n        vol.flush_cache() # deletes cache\n        vol.cache = True\n        vol.commit_info() # writes info to disk\n\n      CACHED: {cache}\n      SOURCE: {source}\n      CACHE LOCATION: {path}\n      """""".format(\n      cache=cache_info, \n      source=fresh_info, \n      path=self.path\n    ))\n\n    try:\n      fresh_sizes = [ scale[\'size\'] for scale in fresh_info[\'scales\'] ]\n      cache_sizes = [ scale[\'size\'] for scale in cache_info[\'scales\'] ]\n    except KeyError:\n      raise mismatch_error\n\n    for scale in fresh_info[\'scales\']:\n      del scale[\'size\']\n\n    for scale in cache_info[\'scales\']:\n      del scale[\'size\']\n\n    if fresh_info != cache_info:\n      raise mismatch_error\n\n    if fresh_sizes != cache_sizes:\n      warn(""WARNING: Data layer bounding box differs in cache.\\nCACHED: {}\\nSOURCE: {}\\nCACHE LOCATION:{}"".format(\n      cache_sizes, fresh_sizes, self.path\n      ))\n\n  def check_provenance_validity(self):\n    try:\n      cached_prov = self.get_json(\'provenance\')\n    except json.decoder.JSONDecodeError:\n      warn(""Cached provenance file is not valid JSON."")\n      return\n\n    if not cached_prov:\n      return\n\n    cached_prov = self.meta._cast_provenance(cached_prov)\n    fresh_prov = self.meta.fetch_provenance()\n    if cached_prov != fresh_prov:\n      warn(""""""\n      WARNING: Cached provenance file does not match source.\n\n      CACHED: {}\n      SOURCE: {}\n      """""".format(cached_prov.serialize(), fresh_prov.serialize()))\n\n  def get_json(self, filename):\n    with SimpleStorage(\'file://\' + self.path) as storage:\n      return storage.get_json(filename)\n\n  def maybe_cache_info(self):\n    if self.enabled:\n      with SimpleStorage(\'file://\' + self.path) as storage:\n        storage.put_file(\'info\', jsonify(self.meta.info), \'application/json\')\n\n  def maybe_cache_provenance(self):\n    if self.enabled and self.meta.provenance:\n      with SimpleStorage(\'file://\' + self.path) as storage:\n        storage.put_file(\'provenance\', self.meta.provenance.serialize(), \'application/json\')\n\n  def upload_single(self, filename, content, *args, **kwargs):\n    kwargs[\'progress\'] = False\n    return self.upload( [(filename, content)], *args, **kwargs )\n\n  def upload(self, files, compress, cache_control, compress_level=None, content_type=None, progress=None):\n    files = list(files)\n\n    progress = progress if progress is not None else self.config.progress\n\n    StorageClass = self.pick_storage_class(files)\n    with StorageClass(self.meta.cloudpath, progress=progress) as stor:\n      remote_fragments = stor.put_files(\n        files=files,\n        compress=compress,\n        compress_level=compress_level,\n        cache_control=cache_control,\n        content_type=content_type,\n      )\n\n    if self.enabled:\n      self.put(files, compress=compress)\n\n  def download_json(self, path, compress=None):\n    """"""\n    Download a single path, but grab from \n    cache first if present and cache is enabled.\n\n    Returns: content or None\n    """"""\n    res = self.download( [ path ], compress=compress, progress=False )\n    res = res[path]\n    if res is None:\n      return None    \n    return json.loads(res.decode(\'utf8\'))\n\n  def download_single(self, path, compress=None):\n    files = self.download([ path ], compress=compress, progress=False)\n    return files[path]\n\n  def download_single_as(\n    self, path, local_alias, \n    compress=None, start=None, end=None\n  ):\n    """"""\n    Download a file or a byte range from a file \n    and save it locally as `local_alias`.\n    """"""\n    if self.enabled:\n      locs = self.compute_data_locations([local_alias])\n      if locs[\'local\']:\n        return self.get_single(local_alias)\n\n    with SimpleStorage(self.meta.cloudpath) as stor:\n      filedata = stor.get_file(path, start=start, end=end)\n\n    if self.enabled:\n      self.put([ (local_alias, filedata) ], compress=compress)\n\n    return filedata\n\n  def download_as(self, requests, compress=None, progress=None):\n    """"""\n    Works with byte ranges.\n\n    requests: [{\n      \'path\': ...,\n      \'local_alias\': ...,\n      \'start\': ...,\n      \'end\': ...,\n    }]\n    """"""\n    # import pdb; pdb.set_trace()\n    if len(requests) == 0:\n      return {}\n\n    progress = progress if progress is not None else self.config.progress\n\n    aliases = [ req[\'local_alias\'] for req in requests ]\n    alias_tuples = { req[\'local_alias\']: (req[\'path\'], req[\'start\'], req[\'end\']) for req in requests }\n    alias_to_path = { req[\'local_alias\']: req[\'path\'] for req in requests }\n    path_to_alias = { v:k for k,v in alias_tuples.items() }\n\n    if None in alias_to_path:\n      del alias_to_path[None]\n      del alias_tuples[None]\n\n    locs = self.compute_data_locations(aliases)\n\n    fragments = {}\n    if self.enabled:\n      fragments = self.get(locs[\'local\'], progress=progress)\n      keys = list(fragments.keys())\n      for key in keys:\n        return_key = (alias_to_path[key], alias_tuples[key][1], alias_tuples[key][2])\n        fragments[return_key] = fragments[key]\n        del fragments[key]\n      for alias in locs[\'local\']:\n        del alias_tuples[alias]\n\n    remote_path_tuples = list(alias_tuples.values())\n\n    remote_paths = ( path for path, start, end in remote_path_tuples )\n    starts = ( start for path, start, end in remote_path_tuples )\n    ends = ( end for path, start, end in remote_path_tuples )\n\n    StorageClass = self.pick_storage_class(remote_path_tuples)\n    with StorageClass(self.meta.cloudpath, progress=progress) as stor:\n      remote_fragments = stor.get_files(remote_paths, starts=starts, ends=ends)\n\n    for frag in remote_fragments:\n      if frag[\'error\'] is not None:\n        raise frag[\'error\']\n\n    remote_fragments = { \n      (res[\'filename\'], res[\'byte_range\'][0], res[\'byte_range\'][1]): res[\'content\'] \\\n      for res in remote_fragments \n    }\n\n    if self.enabled:\n      self.put(\n        [ \n          (path_to_alias[file_bytes_tuple], content) \\\n          for file_bytes_tuple, content in remote_fragments.items() \\\n          if content is not None \n        ],\n        compress=compress,\n        progress=progress\n      )\n\n    fragments.update(remote_fragments)\n    return fragments\n\n  def download(self, paths, compress=None, progress=None):\n    """"""\n    Download the provided paths, but grab them from cache first\n    if they are present and the cache is enabled. \n\n    Returns: { filename: content, ... }\n    """"""\n    if len(paths) == 0:\n      return {}\n\n    progress = progress if progress is not None else self.config.progress\n\n    locs = self.compute_data_locations(paths)\n    locs[\'remote\'] = [ str(x) for x in locs[\'remote\'] ]\n\n    fragments = {}\n    if self.enabled:\n      fragments = self.get(locs[\'local\'], progress=progress)\n\n    StorageClass = self.pick_storage_class(locs[\'remote\'])\n    with StorageClass(self.meta.cloudpath, progress=progress) as stor:\n      remote_fragments = stor.get_files(locs[\'remote\'])\n\n    for frag in remote_fragments:\n      if frag[\'error\'] is not None:\n        raise frag[\'error\']\n\n    remote_fragments = { \n      res[\'filename\']: res[\'content\'] \\\n      for res in remote_fragments \n    }\n\n    if self.enabled:\n      self.put(\n        [ \n          (filename, content) for filename, content in remote_fragments.items() \\\n          if content is not None \n        ],\n        compress=compress,\n        progress=progress\n      )\n\n    fragments.update(remote_fragments)\n    return fragments\n\n  def get_single(self, cloudpath, progress=None):\n    res = self.get([ cloudpath ], progress=progress)\n    return res[cloudpath]\n\n  def get(self, cloudpaths, progress=None):\n    progress = self.config.progress if progress is None else progress\n    \n    StorageClass = self.pick_storage_class(cloudpaths)\n    with StorageClass(\'file://\' + self.path, progress=progress) as stor:\n      results = stor.get_files(list(cloudpaths))\n\n    return { res[\'filename\']: res[\'content\'] for res in results }\n\n  def put_single(self, path, content, *args, **kwargs):\n    kwargs[\'progress\'] = False\n    return self.put([ (path, content) ], *args, **kwargs)\n\n  def put(self, files, progress=None, compress=None, compress_level=None):\n    """"""files: [ (filename, content) ]""""""\n    if progress is None:\n      progress = self.config.progress\n\n    if compress is None:\n      compress = self.compress\n\n    if compress is None:\n      compress = self.config.compress\n    \n    StorageClass = self.pick_storage_class(files)\n\n    save_location = \'file://\' + self.path\n    progress = \'to Cache\' if progress else None\n    with StorageClass(save_location, progress=progress) as stor:\n      stor.put_files(\n        [ (name, content) for name, content in files ],\n        compress=compress,\n        compress_level=compress_level,\n      )\n\n  def compute_data_locations(self, cloudpaths):\n    if not self.enabled:\n      return { \'local\': [], \'remote\': cloudpaths }\n\n    pathmodule = posixpath if self.meta.path.protocol != \'file\' else os.path\n\n    def no_compression_ext(fnames):\n      results = []\n      for fname in fnames:\n        (name, ext) = pathmodule.splitext(fname)\n        if ext in COMPRESSION_EXTENSIONS:\n          results.append(name)\n        else:\n          results.append(fname)\n      return results\n\n    list_dirs = set([ pathmodule.dirname(pth) for pth in cloudpaths ])\n    filenames = []\n\n    for list_dir in list_dirs:\n      list_dir = os.path.join(self.path, list_dir)\n      filenames += no_compression_ext(os.listdir(mkdir(list_dir)))\n\n    basepathmap = { pathmodule.basename(path): pathmodule.dirname(path) for path in cloudpaths }\n\n    # check which files are already cached, we only want to download ones not in cache\n    requested = set([ pathmodule.basename(path) for path in cloudpaths ])\n    already_have = requested.intersection(set(filenames))\n    to_download = requested.difference(already_have)\n\n    download_paths = [ pathmodule.join(basepathmap[fname], fname) for fname in to_download ]    \n    already_have = [ os.path.join(basepathmap[fname], fname) for fname in already_have ]\n\n    return { \'local\': already_have, \'remote\': download_paths }\n\n  def pick_storage_class(self, cloudpaths):\n    if len(cloudpaths) <= 1:\n      return SimpleStorage\n    elif self.config.green:\n      return GreenStorage\n    else:\n      return Storage\n    \n  def __repr__(self):\n    return ""CacheService(enabled={}, compress={}, path=\'{}\')"".format(\n      self.enabled, self.compress, self.path\n    )\n'"
cloudvolume/chunks.py,19,"b'# @license\n# Copyright 2017 The Neuroglancer Authors\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport zlib\nimport io\nimport numpy as np\nfrom PIL import Image\n\nfrom .lib import yellow\n\ntry:\n  import compressed_segmentation as cseg\n  ACCELERATED_CSEG = True # C extension version\nexcept ImportError:\n  ACCELERATED_CSEG = False # Pure Python implementation\n\nfrom . import py_compressed_segmentation as csegpy\n\ntry:\n  import fpzip \nexcept ImportError:\n  fpziperrormsg = yellow(""CloudVolume: fpzip codec is not available. Was it installed? pip install fpzip"")\n  class fpzip():\n    @classmethod\n    def compress(cls, content):\n      raise NotImplementedError(fpziperrormsg)\n    @classmethod\n    def decompress(cls, content):\n      raise NotImplementedError(fpziperrormsg)\n\ndef encode(img_chunk, encoding, block_size=None):\n  if encoding == ""raw"":\n    return encode_raw(img_chunk)\n  elif encoding == ""kempressed"":\n    return encode_kempressed(img_chunk)\n  elif encoding == ""fpzip"":\n    img_chunk = np.asfortranarray(img_chunk)\n    return fpzip.compress(img_chunk, order=\'F\')\n  elif encoding == ""compressed_segmentation"":\n    return encode_compressed_segmentation(img_chunk, block_size=block_size)\n  elif encoding == ""jpeg"":\n    return encode_jpeg(img_chunk)\n  elif encoding == ""npz"":\n    return encode_npz(img_chunk)\n  elif encoding == ""npz_uint8"":\n    chunk = img_chunk * 255\n    chunk = chunk.astype(np.uint8)\n    return encode_npz(chunk)\n  else:\n    raise NotImplementedError(encoding)\n\ndef decode(filedata, encoding, shape=None, dtype=None, block_size=None):\n  if (shape is None or dtype is None) and encoding not in (\'npz\', \'fpzip\', \'kempressed\'):\n    raise ValueError(""Only npz encoding can omit shape and dtype arguments. {}"".format(encoding))\n\n  if filedata is None or len(filedata) == 0:\n    return np.zeros(shape=shape, dtype=dtype)\n  elif encoding == ""raw"":\n    return decode_raw(filedata, shape=shape, dtype=dtype)\n  elif encoding == ""kempressed"":\n    return decode_kempressed(filedata)\n  elif encoding == ""fpzip"":\n    return fpzip.decompress(filedata, order=\'F\')\n  elif encoding == ""compressed_segmentation"":\n    return decode_compressed_segmentation(filedata, shape=shape, dtype=dtype, block_size=block_size)\n  elif encoding == ""jpeg"":\n    return decode_jpeg(filedata, shape=shape, dtype=dtype)\n  elif encoding == ""npz"":\n    return decode_npz(filedata)\n  else:\n    raise NotImplementedError(encoding)\n\ndef encode_jpeg(arr):\n  assert arr.dtype == np.uint8\n\n  # simulate multi-channel array for single channel arrays\n  while arr.ndim < 4:\n    arr = arr[..., np.newaxis] # add channels to end of x,y,z\n\n  reshaped = arr.T \n  reshaped = np.moveaxis(reshaped, 0, -1)\n  reshaped = reshaped.reshape(reshaped.shape[0], reshaped.shape[1] * reshaped.shape[2], reshaped.shape[3])\n  if reshaped.shape[2] == 1:\n    img = Image.fromarray(reshaped[:,:,0], mode=\'L\')\n  elif reshaped.shape[2] == 3:\n    img = Image.fromarray(reshaped, mode=\'RGB\')\n  else:\n    raise ValueError(""Number of image channels should be 1 or 3. Got: {}"".format(arr.shape[3]))\n\n  f = io.BytesIO()\n  img.save(f, ""JPEG"")\n  return f.getvalue()\n\ndef encode_npz(subvol):\n  """"""\n  This file format is unrelated to np.savez\n  We are just saving as .npy and the compressing\n  using zlib. \n  The .npy format contains metadata indicating\n  shape and dtype, instead of np.tobytes which doesn\'t\n  contain any metadata.\n  """"""\n  fileobj = io.BytesIO()\n  if len(subvol.shape) == 3:\n    subvol = np.expand_dims(subvol, 0)\n  np.save(fileobj, subvol)\n  cdz = zlib.compress(fileobj.getvalue())\n  return cdz\n\ndef encode_compressed_segmentation(subvol, block_size, accelerated=ACCELERATED_CSEG):\n  assert np.dtype(subvol.dtype) in (np.uint32, np.uint64)\n\n  if accelerated:\n    return encode_compressed_segmentation_c_ext(subvol, block_size)  \n  return encode_compressed_segmentation_pure_python(subvol, block_size)\n\ndef encode_compressed_segmentation_c_ext(subvol, block_size):\n  subvol = np.squeeze(subvol, axis=3)\n  subvol = np.copy(subvol, order=\'C\')\n  return cseg.compress(subvol, block_size=block_size, order=\'F\')\n\ndef encode_compressed_segmentation_pure_python(subvol, block_size):\n  return csegpy.encode_chunk(subvol.T, block_size=block_size)\n\ndef encode_raw(subvol):\n  return subvol.tostring(\'F\')\n\ndef encode_kempressed(subvol):\n  data = 2.0 + np.swapaxes(subvol, 2,3)\n  return fpzip.compress(data, order=\'F\')\n\ndef decode_kempressed(bytestring):\n  """"""subvol not bytestring since numpy conversion is done inside fpzip extension.""""""\n  subvol = fpzip.decompress(bytestring, order=\'F\')\n  return np.swapaxes(subvol, 3,2) - 2.0\n\ndef decode_npz(string):\n  fileobj = io.BytesIO(zlib.decompress(string))\n  return np.load(fileobj)\n\ndef decode_jpeg(bytestring, shape, dtype):\n  img = Image.open(io.BytesIO(bytestring))\n  data = np.array(img.getdata(), dtype=dtype)\n\n  return data.reshape(shape, order=\'F\')\n\ndef decode_raw(bytestring, shape, dtype):\n  return np.frombuffer(bytearray(bytestring), dtype=dtype).reshape(shape, order=\'F\')\n\ndef decode_compressed_segmentation(bytestring, shape, dtype, block_size, accelerated=ACCELERATED_CSEG):\n  assert block_size is not None\n\n  if accelerated:\n    return decode_compressed_segmentation_c_ext(bytestring, shape, dtype, block_size)\n\n  return decode_compressed_segmentation_pure_python(bytestring, shape, dtype, block_size)\n\ndef decode_compressed_segmentation_c_ext(bytestring, shape, dtype, block_size):\n  return cseg.decompress(bytes(bytestring), shape, dtype, block_size, order=\'F\')\n\ndef decode_compressed_segmentation_pure_python(bytestring, shape, dtype, block_size):\n  chunk = np.empty(shape=shape[::-1], dtype=dtype)\n  csegpy.decode_chunk_into(chunk, bytestring, block_size=block_size)\n  return chunk.T\n\n'"
cloudvolume/cloudvolume.py,3,"b'import sys\nimport time\n\nimport multiprocessing as mp\nimport numpy as np\n\nfrom .exceptions import UnsupportedFormatError\nfrom .lib import generate_random_string\nfrom .paths import strict_extract, to_https_protocol\n\n# NOTE: Plugins are registered in __init__.py\n\n# Set the interpreter bool\ntry:\n  INTERACTIVE = bool(sys.ps1)\nexcept AttributeError:\n  INTERACTIVE = bool(sys.flags.interactive)\n\nREGISTERED_PLUGINS = {}\ndef register_plugin(key, creation_function):\n  REGISTERED_PLUGINS[key.lower()] = creation_function\n\nclass SharedConfiguration(object):\n  """"""\n  Hack around python\'s inability to\n  pass primatives by reference.\n  We would like updates to e.g. mip or parallel\n  to be passively absorbed by all listening\n  data sources rather than work hard to actively\n  synchronize them.\n  """"""\n  def __init__(\n    self, cdn_cache, compress, compress_level, green,\n    mip, parallel, progress,\n    *args, **kwargs\n  ):\n    if type(parallel) == bool:\n      parallel = mp.cpu_count() if parallel == True else 1\n    elif parallel <= 0:\n      raise ValueError(\'Number of processes must be >= 1. Got: \' + str(parallel))\n    else:\n      parallel = int(parallel)\n\n    self.cdn_cache = cdn_cache\n    self.compress = compress\n    self.compress_level = compress_level\n    self.green = bool(green)\n    self.mip = mip\n    self.parallel = parallel \n    self.progress = bool(progress)\n    self.args = args\n    self.kwargs = kwargs\n\nclass CloudVolume(object):\n  def __new__(cls,\n    cloudpath, mip=0, bounded=True, autocrop=False,\n    fill_missing=False, cache=False, compress_cache=None,\n    cdn_cache=True, progress=INTERACTIVE, info=None, provenance=None,\n    compress=None, compress_level=None, non_aligned_writes=False, parallel=1,\n    delete_black_uploads=False, background_color=0,\n    green_threads=False, use_https=False,\n    max_redirects=10, mesh_dir=None, skel_dir=None\n  ):\n    """"""\n    A ""serverless"" Python client for reading and writing arbitrarily large \n    Neuroglancer Precomputed volumes both locally and on cloud services using.  \n    A CloudVolume instance represents a dataset interface at a given mip level \n    (it doesn\'t load the entire dataset into memory).  \n\n    Neuroglancer datasets specify metadata in an `info` file located at the root \n    of a data layer. It contains, among other things, the bounds of the \n    volume described as a 3D ""voxel_offset"" and 3D ""size"" in voxels, and the\n    resolution of the dataset.\n\n    Example:\n\n      from cloudvolume import CloudVolume\n\n      vol = CloudVolume(\'gs://mylab/mouse/image\', progress=True)\n      image = vol[:,:,:] # Download an image stack as a numpy array\n      vol[:,:,:] = image # Upload an image stack from a numpy array\n      \n      label = 1\n      mesh = vol.mesh.get(label) \n      skel = vol.skeletons.get(label)\n\n    Required:\n      cloudpath: Path to the dataset layer. This should match storage\'s supported\n        providers.\n\n        e.g. Google: gs://$BUCKET/$DATASET/$LAYER/\n             S3    : s3://$BUCKET/$DATASET/$LAYER/\n             Lcl FS: file:///tmp/$DATASET/$LAYER/\n             Boss  : boss://$COLLECTION/$EXPERIMENT/$CHANNEL\n             HTTP/S: http(s)://.../$CHANNEL\n             matrix: matrix://$BUCKET/$DATASET/$LAYER/\n    Optional:\n      autocrop: (bool) If the specified retrieval bounding box exceeds the\n          volume bounds, process only the area contained inside the volume. \n          This can be useful way to ensure that you are staying inside the \n          bounds when `bounded=False`.\n      background_color: (number) Specifies what the ""background value"" of the\n        volume is (traditionally 0). This is mainly for changing the behavior\n        of delete_black_uploads.\n      bounded: (bool) If a region outside of volume bounds is accessed:\n          True: Throw an error\n          False: Allow accessing the region. If no files are present, an error \n              will still be thrown. Consider combining this option with \n              `fill_missing=True`. However, this can be dangrous as it allows\n              missing files and potentially network errors to be intepreted as \n              zeros.\n      cache: (bool or str) Store downs and uploads in a cache on disk\n            and preferentially read from it before redownloading.\n          - falsey value: no caching will occur.\n          - True: cache will be located in a standard location.\n          - non-empty string: cache is located at this file path\n\n          After initialization, you can adjust this setting via:\n          `cv.cache.enabled = ...` which accepts the same values.\n\n      cdn_cache: (int, bool, or str) Sets Cache-Control HTTP header on uploaded \n        image files. Most cloud providers perform some kind of caching. As of \n        this writing, Google defaults to 3600 seconds. Most of the time you\'ll \n        want to go with the default. \n        - int: number of seconds for cache to be considered fresh (max-age)\n        - bool: True: max-age=3600, False: no-cache\n        - str: set the header manually\n      compress: (bool, str, None) pick which compression method to use.\n          None: (default) gzip for raw arrays and no additional compression\n            for compressed_segmentation and fpzip.\n          bool: \n            True=gzip, \n            False=no compression, Overrides defaults\n          str: \n            \'gzip\': Extension so that we can add additional methods in the future \n                    like lz4 or zstd. \n            \'br\': Brotli compression, better compression rate than gzip\n            \'\': no compression (same as False).\n      compress_level: (int, None) level for compression. Higher number results\n          in better compression but takes longer.\n        Defaults to 9 for gzip (ranges from 0 to 9).\n        Defaults to 5 for brotli (ranges from 0 to 11).\n      compress_cache: (None or bool) If not None, override default compression \n          behavior for the cache.\n      delete_black_uploads: (bool) If True, on uploading an entirely black chunk,\n          issue a DELETE request instead of a PUT. This can be useful for avoiding storing\n          tiny files in the region around an ROI. Some storage systems using erasure coding \n          don\'t do well with tiny file sizes.\n      fill_missing: (bool) If a chunk file is unable to be fetched:\n          True: Use a block of zeros\n          False: Throw an error\n      green_threads: (bool) Use green threads instead of preemptive threads. This\n        can result in higher download performance for some compression types. Preemptive\n        threads seem to reduce performance on multi-core machines that aren\'t densely\n        loaded as the CPython threads are assigned to multiple cores and the thrashing\n        + GIL reduces performance. You\'ll need to add the following code to the top\n        of your program to use green threads:\n\n            import gevent.monkey\n            gevent.monkey.patch_all(threads=False)\n\n      info: (dict) In lieu of fetching a neuroglancer info file, use this one.\n          This is useful when creating new datasets and for repeatedly initializing\n          a new cloudvolume instance.\n      max_redirects: (int) if > 0, allow up to this many redirects via info file \'redirect\'\n          data fields. If <= 0, allow no redirections and access the current info file directly\n          without raising an error.\n      mesh_dir: (str) if not None, override the info[\'mesh\'] key before pulling the\n        mesh info file.\n      mip: (int or iterable) Which level of downsampling to read and write from.\n          0 is the highest resolution. You can also specify the voxel resolution\n          like mip=[6,6,30] which will search for the appropriate mip level.\n      non_aligned_writes: (bool) Enable non-aligned writes. Not multiprocessing \n          safe without careful design. When not enabled, a \n          cloudvolume.exceptions.AlignmentError is thrown for non-aligned writes. \n          \n          https://github.com/seung-lab/cloud-volume/wiki/Advanced-Topic:-Non-Aligned-Writes\n\n      parallel (int: 1, bool): Number of extra processes to launch, 1 means only \n          use the main process. If parallel is True use the number of CPUs \n          returned by multiprocessing.cpu_count(). When parallel > 1, shared\n          memory (Linux) or emulated shared memory via files (other platforms) \n          is used by the underlying download.\n      progress: (bool) Show progress bars. \n          Defaults to True in interactive python, False in script execution mode.\n      provenance: (string, dict) In lieu of fetching a provenance \n          file, use this one. \n      skel_dir: (str) if not None, override the info[\'skeletons\'] key before \n        pulling the skeleton info file.\n      use_https: (bool) maps gs:// and s3:// to their respective https paths. The \n        https paths hit a cached, read-only version of the data and may be faster.\n    """"""\n    if use_https:\n      cloudpath = to_https_protocol(cloudpath)\n\n    kwargs = dict(locals())\n    del kwargs[\'cls\']\n\n    path = strict_extract(cloudpath)\n    if path.format in REGISTERED_PLUGINS:\n      return REGISTERED_PLUGINS[path.format](**kwargs)\n    else:\n      raise UnsupportedFormatError(\n        ""Unknown format {}"".format(path.format)\n      )\n\n  @classmethod\n  def create_new_info(cls, *args, **kwargs):\n    from .frontends import CloudVolumePrecomputed\n    # For backwards compatibility, but this only \n    # makes sense for Precomputed anyway\n    return CloudVolumePrecomputed.create_new_info(*args, **kwargs)\n\n  @classmethod\n  def from_numpy(cls, \n    arr, \n    vol_path=\'file:///tmp/image/\' + generate_random_string(),\n    resolution=(4,4,40), voxel_offset=(0,0,0), \n    chunk_size=(128,128,64), layer_type=None, max_mip=0,\n    encoding=\'raw\', compress=None\n  ):\n    """"""\n    Create a new dataset from a numpy array.\n\n    max_mip: (int) the maximum mip level id in the info file. \n    Note that currently the numpy array can only sit in mip 0,\n    the max_mip was only created in info file.\n    the numpy array itself was not downsampled. \n    """"""\n    if not layer_type:\n      if arr.dtype in (np.bool, np.uint32, np.uint64, np.uint16):\n        layer_type = \'segmentation\'\n      elif np.issubdtype(arr.dtype, np.integer) \\\n                        or np.issubdtype(arr.dtype, np.floating):\n        layer_type = \'image\'\n      else:\n        raise NotImplementedError\n\n    if arr.ndim == 3:\n      num_channels = 1\n    elif arr.ndim == 4:\n      num_channels = arr.shape[-1]\n    else:\n      raise NotImplementedError\n\n    info = cls.create_new_info(\n      num_channels, layer_type, arr.dtype.name,\n      encoding, resolution,\n      voxel_offset, arr.shape[:3],\n      chunk_size=chunk_size, max_mip=max_mip\n    )\n    vol = CloudVolume(vol_path, info=info, bounded=True, compress=compress)\n    # save the info file\n    vol.commit_info()\n    vol.provenance.processing.append({\n      \'method\': \'from_numpy\',\n      \'date\': time.strftime(\'%Y-%m-%d %H:%M %Z\')\n    })\n    vol.commit_provenance()\n    # save the numpy array\n    vol[:,:,:] = arr\n    return vol\n'"
cloudvolume/compression.py,0,"b'from six import StringIO, BytesIO\n\nimport gzip\nimport sys\n\nimport brotli\n\nfrom .exceptions import DecompressionError, CompressionError\nfrom .lib import yellow\n\nCOMPRESSION_TYPES = [ None, False, True, \'\', \'gzip\', \'br\' ]\n\ndef decompress(content, encoding, filename=\'N/A\'):\n  """"""\n  Decompress file content. \n\n  Required: \n    content (bytes): a file to be compressed\n    encoding: None (no compression) or \'gzip\' or \'br\'\n  Optional:   \n    filename (str:default:\'N/A\'): Used for debugging messages\n  Raises: \n    NotImplementedError if an unsupported codec is specified. \n    compression.EncodeError if the encoder has an issue\n\n  Return: decompressed content\n  """"""\n  try:\n    encoding = (encoding or \'\').lower()\n    if encoding == \'\':\n      return content\n    elif len(content) == 0:\n      raise DecompressionError(\'File contains zero bytes: \' + str(filename))\n    elif encoding == \'gzip\':\n      return gunzip(content)\n    elif encoding == \'br\':\n      return brotli_decompress(content)\n  except DecompressionError as err:\n    print(""Filename: "" + str(filename))\n    raise\n  \n  raise NotImplementedError(str(encoding) + \' is not currently supported. Supported Options: None, gzip\')\n\ndef compress(content, method=\'gzip\', compress_level=None):\n  """"""\n  Compresses file content.\n\n  Required:\n    content (bytes): The information to be compressed\n    method (str, default: \'gzip\'): None or gzip\n  Raises: \n    NotImplementedError if an unsupported codec is specified. \n    compression.DecodeError if the encoder has an issue\n\n  Return: compressed content\n  """"""\n  if method == True:\n    method = \'gzip\' # backwards compatibility\n\n  method = (method or \'\').lower()\n\n  if method == \'\':\n    return content\n  elif method == \'gzip\': \n    return gzip_compress(content, compresslevel=compress_level)\n  elif method == \'br\':\n    return brotli_compress(content, quality=compress_level)\n  raise NotImplementedError(str(method) + \' is not currently supported. Supported Options: None, gzip\')\n\ndef gzip_compress(content, compresslevel=None):\n  if compresslevel is None:\n    compresslevel = 9\n  \n  stringio = BytesIO()\n  gzip_obj = gzip.GzipFile(mode=\'wb\', fileobj=stringio, compresslevel=compresslevel)\n\n  if sys.version_info < (3,):\n    content = str(content)\n\n  gzip_obj.write(content)\n  gzip_obj.close()\n  return stringio.getvalue()\n\ndef gunzip(content):\n  """""" \n  Decompression is applied if the first to bytes matches with\n  the gzip magic numbers. \n  There is once chance in 65536 that a file that is not gzipped will\n  be ungzipped.\n  """"""\n  if len(content) == 0:\n    raise DecompressionError(\'File contains zero bytes.\')\n\n  gzip_magic_numbers = [ 0x1f, 0x8b ]\n  first_two_bytes = [ byte for byte in bytearray(content)[:2] ]\n  if first_two_bytes != gzip_magic_numbers:\n    raise DecompressionError(\'File is not in gzip format. Magic numbers {}, {} did not match {}, {}.\'.format(\n      hex(first_two_bytes[0]), hex(first_two_bytes[1])), hex(gzip_magic_numbers[0]), hex(gzip_magic_numbers[1]))\n\n  stringio = BytesIO(content)\n  with gzip.GzipFile(mode=\'rb\', fileobj=stringio) as gfile:\n    return gfile.read()\n\ndef brotli_compress(content, quality=None):\n  if quality is None:\n    # 5/6 are good balance between compression speed and compression rate\n    quality = 5\n  return brotli.compress(content, quality=quality)\n\ndef brotli_decompress(content):\n  if len(content) == 0:\n    raise DecompressionError(\'File contains zero bytes.\')\n\n  return brotli.decompress(content)\n'"
cloudvolume/connectionpools.py,0,"b'from six.moves import queue as Queue\nimport threading\nimport time\nfrom functools import partial\n\nimport boto3 \nfrom google.cloud.storage import Client\nimport tenacity\n\nfrom .secrets import google_credentials, aws_credentials\nfrom .exceptions import UnsupportedProtocolError\n\nretry = tenacity.retry(\n  reraise=True, \n  stop=tenacity.stop_after_attempt(7), \n  wait=tenacity.wait_random_exponential(0.5, 60.0),\n)\n\nclass ConnectionPool(object):\n  """"""\n  This class is intended to be subclassed. See below.\n  \n  Creating fresh client or connection objects\n  for Google or Amazon eventually starts causing\n  breakdowns when too many connections open.\n  \n  To promote efficient resource use and prevent\n  containers from dying, we create a ConnectionPool\n  that allows for the reuse of connections.\n  \n  Storage interfaces may acquire and release connections \n  when they need or finish using them. \n  \n  If the limit is reached, additional requests for\n  acquiring connections will block until they can\n  be serviced.\n  """"""\n  def __init__(self):\n    self.pool = Queue.Queue(maxsize=0)\n    self.outstanding = 0\n    self._lock = threading.Lock()\n\n  def total_connections(self):\n    return self.pool.qsize() + self.outstanding\n\n  def _create_connection(self):\n    raise NotImplementedError\n\n  def get_connection(self):    \n    with self._lock:\n      try:        \n        conn = self.pool.get(block=False)\n        self.pool.task_done()\n      except Queue.Empty:\n        conn = self._create_connection()\n      finally:\n        self.outstanding += 1\n\n    return conn\n\n  def release_connection(self, conn):\n    if conn is None:\n      return\n\n    self.pool.put(conn)\n    with self._lock:\n      self.outstanding -= 1\n\n  def close(self, conn):\n    return \n\n  def reset_pool(self):\n    while True:\n      if not self.pool.qsize():\n        break\n      try:\n        conn = self.pool.get()\n        self.close(conn)\n        self.pool.task_done()\n      except Queue.Empty:\n        break\n\n    with self._lock:\n      self.outstanding = 0\n\n  def __del__(self):\n    self.reset_pool()\n\nclass S3ConnectionPool(ConnectionPool):\n  def __init__(self, service, bucket):\n    self.service = service\n    self.bucket = bucket\n    self.credentials = aws_credentials(bucket, service)\n    super(S3ConnectionPool, self).__init__()\n\n  @retry\n  def _create_connection(self):\n    if self.service in (\'aws\', \'s3\'):\n      return boto3.client(\n        \'s3\',\n        aws_access_key_id=self.credentials[\'AWS_ACCESS_KEY_ID\'],\n        aws_secret_access_key=self.credentials[\'AWS_SECRET_ACCESS_KEY\'],\n        region_name=\'us-east-1\',\n      )\n    elif self.service == \'matrix\':\n      return boto3.client(\n        \'s3\',\n        aws_access_key_id=self.credentials[\'AWS_ACCESS_KEY_ID\'],\n        aws_secret_access_key=self.credentials[\'AWS_SECRET_ACCESS_KEY\'],\n        endpoint_url=\'https://s3-hpcrc.rc.princeton.edu\',\n      )\n    else:\n      raise UnsupportedProtocolError(""{} unknown. Choose from \'s3\' or \'matrix\'."", self.service)\n      \n  def close(self, conn):\n    try:\n      return conn.close()\n    except AttributeError:\n      pass # AttributeError: \'S3\' object has no attribute \'close\' on shutdown\n\n\nclass GCloudBucketPool(ConnectionPool):\n  def __init__(self, bucket):\n    self.bucket = bucket\n    self.project, self.credentials = google_credentials(bucket)\n    super(GCloudBucketPool, self).__init__()\n\n  @retry\n  def _create_connection(self):\n    client = Client(\n      credentials=self.credentials,\n      project=self.project,\n    )\n\n    return client.bucket(self.bucket)\n'"
cloudvolume/dask.py,2,"b'""""""Utilties for translating to/from dask arrays.\n\nNOTE: Using the thread-based dask scheduler is very inefficient with\n      CloudVolume because CV is pure python and will cause substantial\n      GIL contention between the dask worker threads.  It is HIGHLY\n      ADVISABLE to use a distributed process scheduler with one thread\n      per process.\n\n""""""\nimport numpy as np\nfrom .cloudvolume import CloudVolume\n\n\ndef to_cloudvolume(arr,\n                   cloudpath,\n                   resolution=(1, 1, 1),\n                   voxel_offset=(0, 0, 0),\n                   layer_type=None,\n                   encoding=\'raw\',\n                   max_mip=0,\n                   compute=True,\n                   return_stored=False,\n                   **kwargs):\n  """"""Save 3d or 4d dask array to the precomputed CloudVolume storage format.\n\n  NOTE: DO NOT USE thread-based dask scheduler. See comment at top of module.\n\n  See https://docs.dask.org/en/latest/array.html for details about the format.\n\n  Parameters\n  ----------\n  arr: dask.array\n    Data to store\n  cloudpath: str\n    Path to the dataset layer. This should match storage\'s supported\n    providers.\n    e.g. Google: gs://$BUCKET/$DATASET/$LAYER/\n         S3    : s3://$BUCKET/$DATASET/$LAYER/\n         Lcl FS: file:///tmp/$DATASET/$LAYER/\n         Boss  : boss://$COLLECTION/$EXPERIMENT/$CHANNEL\n         HTTP/S: http(s)://.../$CHANNEL\n         matrix: matrix://$BUCKET/$DATASET/$LAYER/\n  resolution: Iterable of ints of length 3\n    The x, y, z voxel dimensions in nanometers\n  voxel_offset: Iterable of ints of length 3\n    The x, y, z beginning of dataset in positive cartesian space.\n  layer_type: str\n    ""image"" or ""segmentation""\n  max_mip: int\n    Maximum mip level id.\n  compute: boolean, optional\n    If true compute immediately, return ``dask.delayed.Delayed`` otherwise.\n  return_stored: boolean, optional\n    Optionally return stored results.\n  kwargs: passed to the ``cloudvolume.CloudVolume()`` function, e.g., compression options\n\n  Raises\n  ------\n  ValueError\n    If ``arr`` has ndim different that 3 or 4, or ``layer_type`` is unsupported.\n\n  Returns\n  -------\n  See notes on `compute` and `return_stored` parameters.\n  """"""\n  import dask\n  import dask.array as da\n  if not da.core._check_regular_chunks(arr.chunks):\n    raise ValueError(\'Attempt to save array to cloudvolume with irregular \'\n                     \'chunking, please call `arr.rechunk(...)` first.\')\n\n  if not layer_type:\n    if arr.dtype in (np.bool, np.uint32, np.uint64, np.uint16):\n      layer_type = \'segmentation\'\n    elif np.issubdtype(arr.dtype, np.integer) or np.issubdtype(arr.dtype, np.floating):\n      layer_type = \'image\'\n    else:\n      raise ValueError(\'Unsupported layer_type for CloudVolume: %s\' % layer_type)\n\n  if arr.ndim == 3:\n    num_channels = 1\n    chunk_size = arr.chunksize\n  elif arr.ndim == 4:\n    num_channels = arr.shape[-1]\n    chunk_size = arr.chunksize[:3]\n  else:\n    raise ValueError(\'CloudVolume only supports 3 or 4 dimensions.  Array has %d.\' % arr.ndim)\n\n  info = CloudVolume.create_new_info(num_channels,\n                                     layer_type,\n                                     arr.dtype.name,\n                                     encoding,\n                                     resolution,\n                                     voxel_offset,\n                                     arr.shape[:3],\n                                     chunk_size=chunk_size,\n                                     max_mip=max_mip)\n\n  # Delay writing any metadata until computation time.\n  #   - the caller may never do the full computation\n  #   - the filesystem may be slow, and there is a desire to open files\n  #     in parallel on worker machines.\n  vol = dask.delayed(_create_cloudvolume)(cloudpath, info, **kwargs)\n  return arr.store(vol, lock=False, compute=compute, return_stored=return_stored)\n\n\ndef _create_cloudvolume(cloudpath, info, **kwargs):\n  """"""Create cloudvolume and commit metadata.""""""\n  vol = CloudVolume(cloudpath, info=info, progress=False, **kwargs)\n  vol.commit_info()\n  vol.provenance.processing = [{\'method\': \'cloudvolume.dask.to_cloudvolume\'}]\n  vol.commit_provenance()\n  return vol\n\n\ndef from_cloudvolume(cloudpath, chunks=None, name=None, **kwargs):\n  """"""Load dask array from a cloudvolume compatible dataset.\n\n  NOTE: DO NOT USE thread-based dask scheduler. See comment at top of module.\n\n  Volumes with a single channel will be returned as 4d arrays with a\n  length-1 channel dimension, even if they were stored from 3d data.\n\n  The channel dimension is returned as a single-chunk by default, as that\n  is how CloudVolumes are stored.\n\n  See https://docs.dask.org/en/latest/array.html for details about the format.\n\n  Parameters\n  ----------\n  cloudpath: str\n    Path to the dataset layer. This should match storage\'s supported\n    providers.\n    e.g. Google: gs://$BUCKET/$DATASET/$LAYER/\n         S3    : s3://$BUCKET/$DATASET/$LAYER/\n         Lcl FS: file:///tmp/$DATASET/$LAYER/\n         Boss  : boss://$COLLECTION/$EXPERIMENT/$CHANNEL\n         HTTP/S: http(s)://.../$CHANNEL\n         matrix: matrix://$BUCKET/$DATASET/$LAYER/\n  chunks: tuple of ints or tuples of ints\n    Passed to ``da.from_array``, allows setting the chunks on\n    initialisation, if the chunking scheme in the stored dataset is not\n    optimal for the calculations to follow.  Note that the chunking should\n    be compatible with an underlying 4d array.\n  name: str, optional\n     An optional keyname for the array.  Defaults to hashing the input\n  kwargs: passed to the ``cloudvolume.CloudVolume()`` function, e.g., compression options\n\n  Returns\n  -------\n  Dask array\n  """"""\n  import dask.array as da\n  from dask.base import tokenize\n\n  vol = CloudVolume(cloudpath, progress=False, **kwargs)\n  if chunks is None:\n      chunks = tuple(vol.chunk_size) + (vol.num_channels, )\n  if name is None:\n      name = \'from-cloudvolume-\' + tokenize(vol, chunks, **kwargs)\n  return da.from_array(vol, chunks, name=name)\n'"
cloudvolume/exceptions.py,0,"b'\nclass InfoError(Exception):\n  pass\n\nclass InfoUnavailableError(InfoError):\n  """"""CloudVolume was unable to access this layer\'s info file.""""""\n  pass\n\nclass RedirectError(InfoError):\n  pass\n\nclass TooManyRedirects(RedirectError):\n  """"""The chain of redirects became unconvincingly long.""""""\n  pass\n\nclass CyclicRedirect(RedirectError):\n  """"""Unable to resolve redirects due to cycle.""""""\n  pass\n\nclass ScaleUnavailableError(IndexError):\n  """"""The info file is not configured to support this scale / mip level.""""""\n  pass\n\nclass ReadOnlyException(Exception):\n  """"""Attempted to write to a readonly data source.""""""\n  pass\n\nclass AlignmentError(ValueError):\n  """"""Signals that an operation requiring chunk alignment was not aligned.""""""\n  pass \n\nclass EmptyVolumeException(Exception):\n  """"""Raised upon finding a missing chunk.""""""\n  pass\n\nclass EmptyFileException(Exception):\n  """"""File was zero bytes.""""""\n  pass\n\nclass EmptyRequestException(ValueError):\n  """"""\n  Requesting uploading or downloading \n  a bounding box of less than one cubic voxel\n  is impossible.\n  """"""\n  pass\n\nclass DecodingError(Exception):\n  """"""Generic decoding error. Applies to content aware and unaware codecs.""""""\n  pass\n\nclass EncodingError(Exception):\n  """"""Generic decoding error. Applies to content aware and unaware codecs.""""""\n  pass\n\nclass OutOfBoundsError(ValueError):\n  """"""\n  Raised upon trying to obtain or assign to a bbox of a volume outside\n  of the volume\'s bounds\n  """"""\n\nclass UnsupportedCompressionType(ValueError):\n  """"""\n  Raised when attempting to use a compression type which is unsupported\n  by the storage interface.\n  """"""\n\n# Inheritance below done for backwards compatibility reasons.\n\nclass DecompressionError(DecodingError):\n  """"""\n  Decompression failed. This exception is used for codecs \n  that are naieve to data contents like gzip, lzma, etc. as opposed\n  to codecs that are aware of array shape like fpzip or compressed_segmentation.\n  """"""\n  pass\n\nclass CompressionError(EncodingError):\n  """"""\n  Compression failed. This exception is used for codecs \n  that are naieve to data contents like gzip, lzma, etc. as opposed\n  to codecs that are aware of array shape like fpzip or compressed_segmentation.\n  """"""\n  pass\n\nclass MeshDecodeError(ValueError):\n  """"""Unable to decode a mesh object.""""""\n  pass\n\nclass SkeletonUnassignedEdgeError(Exception):\n  """"""This skeleton has an edge to a vertex that doesn\'t exist.""""""\n  pass\n\nclass SkeletonDecodeError(Exception):\n  """"""Unable to decode a binary skeleton into a Python object.""""""\n  pass\n\nclass SkeletonEncodeError(Exception):\n  """"""Unable to encode a PrecomputedSkeleton into a binary object.""""""\n  pass\n\nclass SkeletonTransformError(Exception):\n  """"""Unable to apply a spatial transfrom to the current coordinate system.""""""\n  pass\n\nclass SkeletonAttributeMixingError(Exception):\n  """"""\n  These skeletons have different vertex attributes \n  and cannot be recombined without manual intervention.\n  """"""\n  pass\n\nclass UnsupportedFormatError(Exception):\n  """"""Unable to interpret the format of this URI. e.g. precomputed://""""""\n  pass\n\nclass UnsupportedProtocolError(ValueError):\n  """"""Unknown protocol extension.""""""\n  pass\n\nclass UnsupportedGrapheneAPIVersionError(Exception):\n  """"""This dataset does not support the specified api version.""""""\n  pass\n\nclass SpecViolation(Exception):\n  """"""The values held by this object violate its written specification.""""""\n  pass\n\nclass SpatialIndexGapError(Exception):\n  """"""Part of the spatial index was not found. A complete result set cannot be fetched.""""""\n  pass\n\nclass WriteLockAcquisitionError(Exception):\n  """"""Unable to obtain a lock on this data layer element.""""""\n  pass\n\nclass WriteLockReleaseError(Exception):\n  """"""Unable to release a lock on this data layer element.""""""\n  pass'"
cloudvolume/lib.py,57,"b'from __future__ import print_function\nfrom six.moves import range, reduce\n\nimport json\nimport os\nimport re \nimport sys\nimport math\nimport operator\nimport time\nimport random\nimport string \nfrom itertools import product\n\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\n\nfrom .exceptions import OutOfBoundsError\n\nif sys.version_info < (3,):\n  integer_types = (int, long, np.integer)\n  string_types = (str, basestring, unicode)\nelse:\n  integer_types = (int, np.integer)\n  string_types = (str,)\n\nfloating_types = (float, np.floating)\n\nCOLORS = {\n  \'RESET\': ""\\033[m"",\n  \'YELLOW\': ""\\033[1;93m"",\n  \'RED\': \'\\033[1;91m\',\n  \'GREEN\': \'\\033[1;92m\',\n}\n\n# Formula produces machine epsilon regardless of platform architecture\nMACHINE_EPSILON = (7. / 3) - (4. / 3) - 1\n\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        if isinstance(obj, np.integer):\n            return int(obj)\n        if isinstance(obj, np.floating):\n            return float(obj)\n        return json.JSONEncoder.default(self, obj)\n\ndef toiter(obj, is_iter=False):\n  try:\n    iter(obj)\n    if is_iter:\n      return obj, True\n    return obj \n  except TypeError:\n    if is_iter:\n      return [ obj ], False\n    return [ obj ]\n\ndef duplicates(lst):\n  dupes = []\n  seen = set()\n  for elem in lst:\n    if elem in seen:\n      dupes.append(elem)\n    seen.add(elem)\n  return set(dupes)\n\ndef jsonify(obj, **kwargs):\n  return json.dumps(obj, cls=NumpyEncoder, **kwargs)\n\ndef green(text):\n  return colorize(\'green\', text)\n\ndef yellow(text):\n  return colorize(\'yellow\', text)\n\ndef red(text):\n  return colorize(\'red\', text)\n\ndef colorize(color, text):\n  color = color.upper()\n  return COLORS[color] + text + COLORS[\'RESET\']\n\ndef generate_random_string(size=6):\n  return \'\'.join(random.SystemRandom().choice(\n    string.ascii_lowercase + string.digits) for _ in range(size)\n  )\n\ndef toabs(path):\n  path = os.path.expanduser(path)\n  return os.path.abspath(path)\n\ndef mkdir(path):\n  path = toabs(path)\n\n  try:\n    if path != \'\' and not os.path.exists(path):\n      os.makedirs(path)\n  except OSError as e:\n    if e.errno == 17: # File Exists\n      time.sleep(0.1)\n      return mkdir(path)\n    else:\n      raise\n\n  return path\n\ndef touch(path):\n  mkdir(os.path.dirname(path))\n  open(path, \'a\').close()\n\ndef find_closest_divisor(to_divide, closest_to):\n  """"""\n  This is used to find the right chunk size for\n  importing a neuroglancer dataset that has a\n  chunk import size that\'s not evenly divisible by\n  64,64,64. \n\n  e.g. \n    neuroglancer_chunk_size = find_closest_divisor(build_chunk_size, closest_to=[64,64,64])\n\n  Required:\n    to_divide: (tuple) x,y,z chunk size to rechunk\n    closest_to: (tuple) x,y,z ideal chunk size\n\n  Return: [x,y,z] chunk size that works for ingestion\n  """"""\n  def find_closest(td, ct):\n    min_distance = td\n    best = td\n    \n    for divisor in divisors(td):\n      if abs(divisor - ct) < min_distance:\n        min_distance = abs(divisor - ct)\n        best = divisor\n    return best\n  \n  return [ find_closest(td, ct) for td, ct in zip(to_divide, closest_to) ]\n\ndef divisors(n):\n  """"""Generate the divisors of n""""""\n  for i in range(1, int(math.sqrt(n) + 1)):\n    if n % i == 0:\n      yield i\n      if i*i != n:\n        yield n / i\n\ndef scatter(sequence, n):\n  """"""Scatters elements of ``sequence`` into ``n`` blocks. Returns generator.""""""\n  for i in range(n):\n    yield sequence[i::n]\n\ndef xyzrange(start_vec, end_vec=None, stride_vec=(1,1,1)):\n  if end_vec is None:\n    end_vec = start_vec\n    start_vec = (0,0,0)\n\n  start_vec = np.array(start_vec, dtype=int)\n  end_vec = np.array(end_vec, dtype=int)\n\n  rangeargs = ( (start, end, stride) for start, end, stride in zip(start_vec, end_vec, stride_vec) )\n  xyzranges = [ range(*arg) for arg in rangeargs ]\n  \n  # iterate then x first, then y, then z\n  # this way you process in the xy plane slice by slice\n  # but you don\'t create process lots of prefix-adjacent keys\n  # since all the keys start with X\n  zyxranges = xyzranges[::-1]\n\n  def vectorize():\n    pt = Vec(0,0,0)\n    for z,y,x in product(*zyxranges):\n      pt.x, pt.y, pt.z = min(x, end_vec[0]), min(y, end_vec[1]), min(z, end_vec[2])\n      yield pt\n\n  return vectorize()\n\ndef map2(fn, a, b):\n  assert len(a) == len(b), ""Vector lengths do not match: {} (len {}), {} (len {})"".format(a[:3], len(a), b[:3], len(b))\n\n  result = np.empty(len(a))\n\n  for i in range(len(result)):\n    result[i] = fn(a[i], b[i])\n\n  if isinstance(a, Vec) or isinstance(b, Vec):\n    return Vec(*result)\n\n  return result\n\ndef max2(a, b):\n  return map2(max, a, b).astype(a.dtype)\n\ndef min2(a, b):\n  return map2(min, a, b).astype(a.dtype)\n\ndef clamp(val, low, high):\n  return min(max(val, low), high)\n\ndef check_bounds(val, low, high):\n  if val > high or val < low:\n    raise OutOfBoundsError(\'Value {} cannot be outside of inclusive range {} to {}\'.format(val,low,high))\n  return val\n\nclass Vec(np.ndarray):\n    def __new__(cls, *args, **kwargs):\n      dtype = kwargs[\'dtype\'] if \'dtype\' in kwargs else int\n      return super(Vec, cls).__new__(cls, shape=(len(args),), buffer=np.array(args).astype(dtype), dtype=dtype)\n\n    @classmethod\n    def clamp(cls, val, minvec, maxvec):\n      return Vec(*min2(max2(val, minvec), maxvec))\n\n    def clone(self):\n      return Vec(*self[:], dtype=self.dtype)\n\n    def null(self):\n        return self.length() <= 10 * np.finfo(np.float32).eps\n\n    def dot(self, vec):\n      return sum(self * vec)\n\n    def length2(self):\n        return self.dot(self)\n\n    def length(self):\n        return math.sqrt(self.dot(self))\n\n    def rectVolume(self):\n        return reduce(operator.mul, self)\n\n    def __hash__(self):\n      return int(\'\'.join(map(str, self)))\n\n    def __repr__(self):\n      values = u"","".join(list(self.astype(str)))\n      return u""Vec({}, dtype={})"".format(values, self.dtype)\n\ndef __assign(self, val, index):\n  self[index] = val\n\nVec.x = property(lambda self: self[0], lambda self,val: __assign(self,val,0))\nVec.y = property(lambda self: self[1], lambda self,val: __assign(self,val,1))\nVec.z = property(lambda self: self[2], lambda self,val: __assign(self,val,2))\nVec.w = property(lambda self: self[3], lambda self,val: __assign(self,val,3))\n\nVec.r = Vec.x\nVec.g = Vec.y\nVec.b = Vec.z\nVec.a = Vec.w\n\n\ndef floating(lst):\n  return any(( isinstance(x, float) for x in lst ))\n\nFILENAME_RE = re.compile(r\'(-?\\d+)-(-?\\d+)_(-?\\d+)-(-?\\d+)_(-?\\d+)-(-?\\d+)(?:\\.gz|\\.br)?$\')\n\nclass Bbox(object):\n  __slots__ = [ \'minpt\', \'maxpt\', \'_dtype\' ]\n\n  """"""Represents a three dimensional cuboid in space.""""""\n  def __init__(self, a, b, dtype=None):\n    if dtype is None:\n      if floating(a) or floating(b):\n        dtype = np.float32\n      else:\n        dtype = np.int32\n\n    self.minpt = Vec(*[ min(ai,bi) for ai,bi in zip(a,b) ], dtype=dtype)\n    self.maxpt = Vec(*[ max(ai,bi) for ai,bi in zip(a,b) ], dtype=dtype)\n\n    self._dtype = np.dtype(dtype)\n\n  @classmethod\n  def deserialize(cls, bbx_data):\n    bbx_data = json.loads(bbx_data)\n    return Bbox.from_dict(bbx_data)\n\n  def serialize(self):\n    return json.dumps(self.to_dict())\n\n  @property\n  def ndim(self):\n    return len(self.minpt)\n\n  @property \n  def dtype(self):\n    return self._dtype\n\n  @classmethod\n  def intersection(cls, bbx1, bbx2):\n    result = Bbox( [ 0 ] * bbx1.ndim, [ 0 ] * bbx2.ndim )\n\n    if not Bbox.intersects(bbx1, bbx2):\n      return result\n    \n    for i in range(result.ndim):\n      result.minpt[i] = max(bbx1.minpt[i], bbx2.minpt[i])\n      result.maxpt[i] = min(bbx1.maxpt[i], bbx2.maxpt[i])\n\n    return result\n\n  @classmethod\n  def intersects(cls, bbx1, bbx2):\n    return np.all(bbx1.minpt < bbx2.maxpt) and np.all(bbx1.maxpt > bbx2.minpt)\n\n  @classmethod\n  def near_edge(cls, bbx1, bbx2, distance=0):\n    return (\n         np.any( np.abs(bbx1.minpt - bbx2.minpt) <= distance )\n      or np.any( np.abs(bbx1.maxpt - bbx2.maxpt) <= distance )\n    )\n\n  @classmethod\n  def create(cls, obj, context=None, bounded=False, autocrop=False):\n    typ = type(obj)\n    if typ is Bbox:\n      obj = obj\n    elif typ in (list, tuple):\n      obj = Bbox.from_slices(obj, context, bounded, autocrop)\n    elif typ is Vec:\n      obj = Bbox.from_vec(obj)\n    elif typ in string_types:\n      obj = Bbox.from_filename(obj)\n    elif typ is dict:\n      obj = Bbox.from_dict(obj)\n    else:\n      raise NotImplementedError(""{} is not a Bbox convertible type."".format(typ))\n\n    if context and autocrop:\n      obj = Bbox.intersection(obj, context)\n\n    if context and bounded:\n      if not context.contains_bbox(obj):\n        raise OutOfBoundsError(\n          ""{} did not fully contain the specified bounding box {}."".format(\n            context, obj\n        ))\n\n    return obj\n\n  @classmethod\n  def from_delta(cls, minpt, plus):\n    return Bbox( minpt, Vec(*minpt) + plus )\n\n  @classmethod\n  def from_dict(cls, data):\n    dtype = data[\'dtype\'] if \'dtype\' in data else np.float32\n    return Bbox( data[\'minpt\'], data[\'maxpt\'], dtype=dtype)\n\n  @classmethod\n  def from_vec(cls, vec, dtype=int):\n    return Bbox( (0,0,0), vec, dtype=dtype)\n\n  @classmethod\n  def from_filename(cls, filename, dtype=int):\n    match = FILENAME_RE.search(os.path.basename(filename))\n\n    if match is None:\n      raise ValueError(""Unable to decode bounding box from: "" + str(filename))\n\n    (xmin, xmax,\n     ymin, ymax,\n     zmin, zmax) = map(int, match.groups())\n\n    return Bbox( (xmin, ymin, zmin), (xmax, ymax, zmax), dtype=dtype)\n\n  @classmethod\n  def from_slices(cls, slices, context=None, bounded=False, autocrop=False):\n    if context:\n      slices = context.reify_slices(\n        slices, bounded=bounded, autocrop=autocrop\n      )\n\n    return Bbox(\n      [ slc.start for slc in slices ],\n      [ slc.stop for slc in slices ]\n    )\n\n  @classmethod\n  def from_list(cls, lst):\n    """"""\n    from_list(cls, lst)\n    \n    the first half of the values are the minpt, \n    the last half are the maxpt\n    """"""\n    half = len(lst) // 2 \n    return Bbox( lst[:half], lst[half:] )\n\n  @classmethod\n  def from_points(cls, arr):\n    """"""Create a Bbox from a point cloud arranged as:\n      [\n        [x,y,z],\n        [x,y,z],\n        ...\n      ]\n    """"""\n    arr = np.array(arr, dtype=np.float32)\n\n    mins = []\n    maxes = []\n\n    for i in range(arr.shape[1]):\n      mins.append( np.min(arr[:,i]) )\n      maxes.append( np.max(arr[:,i]) )\n\n    return Bbox( mins, maxes, dtype=np.int64)\n\n  def to_filename(self):\n    return \'_\'.join(\n      ( str(self.minpt[i]) + \'-\' + str(self.maxpt[i]) for i in range(self.ndim) )\n    )\n\n  def to_slices(self):\n    return tuple([\n      slice(int(self.minpt[i]), int(self.maxpt[i])) for i in range(self.ndim)\n    ])\n\n  def to_list(self):\n    return list(self.minpt) + list(self.maxpt)\n\n  def to_dict(self):\n    return {\n      \'minpt\': self.minpt.tolist(),\n      \'maxpt\': self.maxpt.tolist(),\n      \'dtype\': np.dtype(self.dtype).name,\n    }\n\n  def reify_slices(self, slices, bounded=True, autocrop=False):\n    """"""\n    Convert free attributes of a slice object \n    (e.g. None (arr[:]) or Ellipsis (arr[..., 0]))\n    into bound variables in the context of this\n    bounding box.\n\n    That is, for a \':\' slice, slice.start will be set\n    to the value of the respective minpt index of \n    this bounding box while slice.stop will be set \n    to the value of the respective maxpt index.\n\n    Example:\n      bbx = Bbox( (-1,-2,-3), (1,2,3) )\n      bbx.reify_slices( (np._s[:],) )\n      \n      >>> [ slice(-1,1,1), slice(-2,2,1), slice(-3,3,1) ]\n\n    Returns: [ slice, ... ]\n    """"""\n    if isinstance(slices, integer_types) or isinstance(slices, floating_types):\n      slices = [ slice(int(slices), int(slices)+1, 1) ]\n    elif type(slices) == slice:\n      slices = [ slices ]\n    elif type(slices) == Bbox:\n      slices = slices.to_slices()\n    elif slices == Ellipsis:\n      slices = []\n\n    slices = list(slices)\n\n    for index, slc in enumerate(slices):\n      if slc == Ellipsis:\n        fill = self.ndim - len(slices) + 1\n        slices = slices[:index] +  (fill * [ slice(None, None, None) ]) + slices[index+1:]\n        break\n\n    while len(slices) < self.ndim:\n      slices.append( slice(None, None, None) )\n\n    # First three slices are x,y,z, last is channel. \n    # Handle only x,y,z here, channel seperately\n    for index, slc in enumerate(slices):\n      if isinstance(slc, integer_types) or isinstance(slc, floating_types):\n        slices[index] = slice(int(slc), int(slc)+1, 1)\n      elif slc == Ellipsis:\n        raise ValueError(""More than one Ellipsis operator used at once."")\n      else:\n        start = self.minpt[index] if slc.start is None else slc.start\n        end = self.maxpt[index] if slc.stop is None else slc.stop \n        step = 1 if slc.step is None else slc.step\n\n        if step < 0:\n          raise ValueError(\'Negative step sizes are not supported. Got: {}\'.format(step))\n\n        if autocrop:\n          start = clamp(start, self.minpt[index], self.maxpt[index])\n          end = clamp(end, self.minpt[index], self.maxpt[index])\n        # note: when unbounded, negative indicies do not refer to\n        # the end of the volume as they can describe, e.g. a 1px\n        # border on the edge of the beginning of the dataset as in\n        # marching cubes.\n        elif bounded:\n          # if start < 0: # this is support for negative indicies\n            # start = self.maxpt[index] + start         \n          check_bounds(start, self.minpt[index], self.maxpt[index])\n          # if end < 0: # this is support for negative indicies\n          #   end = self.maxpt[index] + end\n          check_bounds(end, self.minpt[index], self.maxpt[index])\n\n        slices[index] = slice(start, end, step)\n\n    return slices\n\n  @classmethod\n  def expand(cls, *args):\n    result = args[0].clone()\n    for bbx in args:\n      result.minpt = min2(result.minpt, bbx.minpt)\n      result.maxpt = max2(result.maxpt, bbx.maxpt)\n    return result\n\n  @classmethod\n  def clamp(cls, bbx0, bbx1):\n    result = bbx0.clone()\n    result.minpt = Vec.clamp(bbx0.minpt, bbx1.minpt, bbx1.maxpt)\n    result.maxpt = Vec.clamp(bbx0.maxpt, bbx1.minpt, bbx1.maxpt)\n    return result\n\n  def size(self):\n    return Vec(*(self.maxpt - self.minpt), dtype=self.dtype)\n\n  def size3(self):\n    return Vec(*(self.maxpt[:3] - self.minpt[:3]), dtype=self.dtype)\n\n  def subvoxel(self):\n    """"""\n    Previously, we used bbox.volume() < 1 for testing\n    if a bounding box was larger than one voxel. However, \n    if two out of three size dimensions are negative, the \n    product will be positive. Therefore, we first test that \n    the maxpt is to the right of the minpt before computing \n    whether conjunctioned with volume() < 1.\n\n    Returns: boolean\n    """"""\n    return (not self.valid()) or self.volume() < 1\n\n  def empty(self):\n    """"""\n    Previously, we used bbox.volume() <= 0 for testing\n    if a bounding box was empty. However, if two out of \n    three size dimensions are negative, the product will \n    be positive. Therefore, we first test that the maxpt \n    is to the right of the minpt before computing whether \n    the bbox is empty and account for 20x machine epsilon \n    of floating point error.\n\n    Returns: boolean\n    """"""\n    return (not self.valid()) or (self.volume() < (20 * MACHINE_EPSILON))\n\n  def valid(self):\n    return np.all(self.minpt <= self.maxpt)\n\n  def volume(self):\n    if np.issubdtype(self.dtype, np.integer):\n      return self.size3().astype(np.int64).rectVolume()\n    else:\n      return self.size3().astype(np.float64).rectVolume()\n\n  def center(self):\n    return (self.minpt + self.maxpt) / 2.0\n\n  def grow(self, amt):\n    assert amt >= 0\n    self.minpt -= amt\n    self.maxpt += amt\n    return self\n\n  def shrink(self, amt):\n    assert amt >= 0\n    self.minpt += amt\n    self.maxpt -= amt\n\n    if not self.valid():\n      raise ValueError(""Cannot shrink bbox below zero volume."")\n\n    return self\n\n  def expand_to_chunk_size(self, chunk_size, offset=Vec(0,0,0, dtype=int)):\n    """"""\n    Align a potentially non-axis aligned bbox to the grid by growing it\n    to the nearest grid lines.\n\n    Required:\n      chunk_size: arraylike (x,y,z), the size of chunks in the \n                    dataset e.g. (64,64,64)\n      offset: arraylike (x,y,z) the origin of the coordinate system\n        so that this offset can be accounted for in the grid line \n        calculation.\n    Optional:\n      offset: arraylike (x,y,z), the starting coordinate of the dataset\n    """"""\n    chunk_size = np.array(chunk_size, dtype=np.float32)\n    result = self.clone()\n    result = result - offset\n    result.minpt = np.floor(result.minpt / chunk_size) * chunk_size\n    result.maxpt = np.ceil(result.maxpt / chunk_size) * chunk_size \n    return (result + offset).astype(self.dtype)\n\n  def shrink_to_chunk_size(self, chunk_size, offset=Vec(0,0,0, dtype=int)):\n    """"""\n    Align a potentially non-axis aligned bbox to the grid by shrinking it\n    to the nearest grid lines.\n\n    Required:\n      chunk_size: arraylike (x,y,z), the size of chunks in the \n                    dataset e.g. (64,64,64)\n      offset: arraylike (x,y,z) the origin of the coordinate system\n        so that this offset can be accounted for in the grid line \n        calculation.\n    Optional:\n      offset: arraylike (x,y,z), the starting coordinate of the dataset\n    """"""\n    chunk_size = np.array(chunk_size, dtype=np.float32)\n    result = self.clone()\n    result = result - offset\n    result.minpt = np.ceil(result.minpt / chunk_size) * chunk_size\n    result.maxpt = np.floor(result.maxpt / chunk_size) * chunk_size \n\n    # If we are inside a single chunk, the ends\n    # can invert, which tells us we should collapse\n    # to a single point.\n    if np.any(result.minpt > result.maxpt):\n      result.maxpt = result.minpt.clone()\n\n    return (result + offset).astype(self.dtype)\n\n  def round_to_chunk_size(self, chunk_size, offset=Vec(0,0,0, dtype=int)):\n    """"""\n    Align a potentially non-axis aligned bbox to the grid by rounding it\n    to the nearest grid lines.\n\n    Required:\n      chunk_size: arraylike (x,y,z), the size of chunks in the \n                    dataset e.g. (64,64,64)\n      offset: arraylike (x,y,z) the origin of the coordinate system\n        so that this offset can be accounted for in the grid line \n        calculation.\n    Optional:\n      offset: arraylike (x,y,z), the starting coordinate of the dataset\n    """"""\n    chunk_size = np.array(chunk_size, dtype=np.float32)\n    result = self.clone()\n    result = result - offset\n    result.minpt = np.round(result.minpt / chunk_size) * chunk_size\n    result.maxpt = np.round(result.maxpt / chunk_size) * chunk_size\n    return (result + offset).astype(self.dtype)\n\n  def contains(self, point):\n    """"""\n    Tests if a point on or within a bounding box.\n\n    Returns: boolean\n    """"""\n    return np.all(point >= self.minpt) and np.all(point <= self.maxpt)\n\n  def contains_bbox(self, bbox):\n    return self.contains(bbox.minpt) and self.contains(bbox.maxpt)\n\n  def clone(self):\n    return Bbox(self.minpt, self.maxpt, dtype=self.dtype)\n\n  def astype(self, typ):\n    tmp = self.clone()\n    tmp.minpt = tmp.minpt.astype(typ)\n    tmp.maxpt = tmp.maxpt.astype(typ)\n    tmp._dtype = tmp.minpt.dtype \n    return tmp\n\n  def transpose(self):\n    return Bbox(self.minpt[::-1], self.maxpt[::-1])\n\n  # note that operand can be a vector \n  # or a scalar thanks to numpy\n  def __sub__(self, operand): \n    tmp = self.clone()\n    \n    if isinstance(operand, Bbox):\n      tmp.minpt -= operand.minpt\n      tmp.maxpt -= operand.maxpt\n    else:\n      tmp.minpt -= operand\n      tmp.maxpt -= operand\n\n    return tmp\n\n  def __iadd__(self, operand):\n    if isinstance(operand, Bbox):\n      self.minpt += operand.minpt\n      self.maxpt += operand.maxpt\n    else:\n      self.minpt += operand\n      self.maxpt += operand\n\n    return self\n\n  def __add__(self, operand):\n    tmp = self.clone()\n    return tmp.__iadd__(operand)\n\n  def __imul__(self, operand):\n    self.minpt *= operand\n    self.maxpt *= operand\n    return self\n\n  def __mul__(self, operand):\n    tmp = self.clone()\n    tmp.minpt *= operand\n    tmp.maxpt *= operand\n    return tmp.astype(tmp.minpt.dtype)\n\n  def __idiv__(self, operand):\n    if (\n      isinstance(operand, float) \\\n      or self.dtype in (float, np.float32, np.float64) \\\n      or (hasattr(operand, \'dtype\') and operand.dtype in (float, np.float32, np.float64))\n    ):\n      return self.__itruediv__(operand)\n    else:\n      return self.__ifloordiv__(operand)\n\n  def __div__(self, operand):\n    if (\n      isinstance(operand, float) \\\n      or self.dtype in (float, np.float32, np.float64) \\\n      or (hasattr(operand, \'dtype\') and operand.dtype in (float, np.float32, np.float64))\n    ):\n\n      return self.__truediv__(operand)\n    else:\n      return self.__floordiv__(operand)\n\n  def __ifloordiv__(self, operand):\n    self.minpt //= operand\n    self.maxpt //= operand\n    return self\n\n  def __floordiv__(self, operand):\n    tmp = self.astype(float)\n    tmp.minpt //= operand\n    tmp.maxpt //= operand\n    return tmp.astype(int)\n\n  def __itruediv__(self, operand):\n    res = self.__truediv__(operand)\n    self.minpt[:] = res.minpt[:]\n    self.maxpt[:] = res.maxpt[:]\n    return self\n\n  def __truediv__(self, operand):\n    tmp = self.clone()\n\n    if isinstance(operand, int):\n      operand = float(operand)\n\n    tmp.minpt = Vec(*( tmp.minpt.astype(float) / operand ), dtype=float)\n    tmp.maxpt = Vec(*( tmp.maxpt.astype(float) / operand ), dtype=float)\n    return tmp.astype(tmp.minpt.dtype)\n\n  def __ne__(self, other):\n    return not (self == other)\n\n  def __eq__(self, other):\n    return np.array_equal(self.minpt, other.minpt) and np.array_equal(self.maxpt, other.maxpt)\n\n  def __hash__(self):\n    return int(\'\'.join(map(str, map(int, self.to_list()))))\n\n  def __repr__(self):\n    return ""Bbox({},{}, dtype={})"".format(list(self.minpt), list(self.maxpt), self.dtype)\n\ndef save_images(\n  image, directory=None, axis=\'z\', \n  channel=None, global_norm=True, \n  image_format=\'PNG\', progress=True):\n  """"""\n  Serialize a 3D or 4D array into a series of PNGs for visualization.\n\n  image: A 3D or 4D numpy array. Supported dtypes: integer, float, boolean\n  axis: \'x\', \'y\', \'z\'\n  channel: None, 0, 1, 2, etc, which channel to serialize. Does all by default.\n  directory: override the default output directory\n  global_norm: Normalize floating point volumes globally or per slice?\n  image_format: \'PNG\', \'JPEG\', etc\n  progress: display progress bars and messages\n\n  Returns: the directory path written to\n  """"""\n  if directory is None:\n    directory = os.path.join(\'./saved_images\', \'default\', \'default\', \'0\', Bbox( (0,0,0), image.shape[:3] ).to_filename())\n  \n  mkdir(directory)\n\n  if progress:\n    print(""Saving to {}"".format(directory))\n\n  indexmap = {\n    \'x\': 0,\n    \'y\': 1,\n    \'z\': 2,\n  }\n\n  index = indexmap[axis]\n\n  channel = slice(None) if channel is None else channel\n\n  while image.ndim < 4:\n    image = image[..., np.newaxis ]\n\n  def normalize_float(img):\n    img = np.copy(img)\n    img[ img ==  np.inf ] = 0\n    img[ img == -np.inf ] = 0\n    lower, upper = img.min(), img.max()\n    img = (img - lower) / (upper - lower) * 255.0\n    return img.astype(np.uint8)\n\n  if global_norm and np.issubdtype(image.dtype, np.floating):\n    image = normalize_float(image)      \n\n  for level in tqdm(range(image.shape[index]), desc=""Saving Images"", disable=(not progress)):\n    if index == 0:\n      img = image[level, :, :, channel ]\n    elif index == 1:\n      img = image[:, level, :, channel ]\n    elif index == 2:\n      img = image[:, :, level, channel ]\n    else:\n      raise IndexError(""Index {} is not valid. Expected 0, 1, or 2."".format(index))\n\n    while img.ndim < 3:\n      img = img[..., np.newaxis ]\n\n    num_channels = img.shape[2]\n\n    for channel_index in range(num_channels):\n      img2d = img[:, :, channel_index]\n\n      if not global_norm and img2d.dtype in (np.float32, np.float64):\n        img2d = normalize_float(img2d)\n\n      # discovered that downloaded cube is in a weird rotated state.\n      # it requires a 90deg counterclockwise rotation on xy plane (leaving z alone)\n      # followed by a flip on Y\n      if axis == \'z\':\n        img2d = np.flipud(np.rot90(img2d, 1)) \n\n      if img2d.dtype == np.uint8:\n        img2d = Image.fromarray(img2d, \'L\')\n      elif img2d.dtype == np.bool:\n        img2d = img2d.astype(np.uint8) * 255\n        img2d = Image.fromarray(img2d, \'L\')\n      else:\n        img2d = img2d.astype(\'uint32\')\n        img2d[:,:] |= 0xff000000 # for little endian abgr\n        img2d = Image.fromarray(img2d, \'RGBA\')\n\n      file_index = str(level).zfill(3)\n      filename = \'{}.{}\'.format(file_index, image_format.lower())\n      if num_channels > 1:\n        filename = \'{}-{}\'.format(channel_index, filename)\n\n      path = os.path.join(directory, filename)\n      img2d.save(path, image_format)\n\n  return toabs(directory)'"
cloudvolume/lru.py,0,"b'class DoublyLinkedListIterator(object):\n  def __init__(self, node, reverse=False):\n    self.node = ListNode(None, node, node)\n    self.reverse = reverse\n  def next(self):\n    return self.__next__()\n  def __next__(self):\n    if self.reverse:\n      if self.node.prev is not None:\n        self.node = self.node.prev\n        return self.node\n    else:\n      if self.node.next is not None:\n        self.node = self.node.next\n        return self.node\n    raise StopIteration()\n  def __reversed__(self):\n    return DoublyLinkedListIterator(self.node, not self.reverse)\n\nclass ListNode(object):\n  def __init__(self, val, next, prev):\n    self.val = val\n    self.next = next\n    self.prev = prev \n\n  def __iter__(self):\n    return DoublyLinkedListIterator(self)\n\n  def __reversed__(self):\n    return DoublyLinkedListIterator(self, reverse=True)\n\n  def __str__(self):\n    return ""ListNode({},{},{})"".format(\n      self.val, \n      self.next.val if self.next is not None else None,\n      self.prev.val if self.prev is not None else None\n    )\n\n  def clone(self):\n    return ListNode(self.val, self.next, self.prev)\n\nclass DoublyLinkedList(object):\n  def __init__(self):\n    self.head = None\n    self.tail = None\n    self.size = 0\n\n  @classmethod\n  def create(cls, lst):\n    lst = iter(lst)\n\n    dll = DoublyLinkedList()\n    for val in lst:\n      dll.append(val)\n\n    return dll\n\n  def __len__(self):\n    return self.size\n\n  def __iter__(self):\n    return DoublyLinkedListIterator(self.head)\n\n  def __reversed__(self):\n    return DoublyLinkedListIterator(self.tail, reverse=True)\n\n  def tolist(self):\n    return [ x.val for x in self ]\n\n  def is_empty(self):\n    return self.head is None and self.tail is None\n\n  def peek_head(self):\n    if self.head is None:\n      return None\n    return self.head.val\n\n  def peek_tail(self):\n    if self.tail is None:\n      return None\n    return self.tail.val\n\n  def promote_to_head(self, node):\n    self.delete(node)\n\n    if self.head is None:\n      self.head = node\n      node.next = None \n      node.prev = None\n      self.tail = node\n    else:\n      head = self.head\n      head.prev = node\n      self.head = node\n      node.prev = None \n      node.next = head\n\n    self.size += 1\n\n  def delete(self, node):\n    nxt, prev = node.next, node.prev\n\n    if self.head == node:\n      self.head = nxt\n    if self.tail == node:\n      self.tail = prev\n\n    if prev is not None:\n      prev.next = nxt\n\n    if nxt is not None:\n      nxt.prev = prev\n\n    self.size -= 1\n\n    return node\n\n  def delete_tail(self):\n    if self.tail is None:\n      return None\n    elif self.tail == self.head:\n      val = self.tail.val\n      self.tail = None\n      self.head = None\n      self.size = 0\n      return val\n\n    node = self.tail\n    self.tail = node.prev\n    self.tail.next = None\n\n    self.size -= 1\n\n    return node.val\n\n  def prepend(self, val):\n    if self.head is None and self.tail is None:\n      self.head = ListNode(val, None, None)\n      self.tail = self.head\n    elif self.head is None:\n      self.head = ListNode(val, next=self.tail, prev=None)\n      self.tail.prev = self.head\n    else:\n      prev_head = self.head\n      self.head = ListNode(val, next=prev_head, prev=None)\n      prev_head.prev = self.head\n\n    self.size += 1\n\n    return self\n\n  def append(self, val):\n    if self.head is None and self.tail is None:\n      self.head = ListNode(val, None, None)\n      self.tail = self.head\n    elif self.tail is None:\n      self.tail = ListNode(val, None, self.head)\n      self.head.next = self.tail\n    else:\n      self.tail = ListNode(val, None, self.tail)\n      self.tail.prev.next = self.tail\n\n    self.size += 1\n\n    return self\n\n  def __str__(self):\n    return str([ n.val for n in self ])\n\nclass LRU(object):\n  def __init__(self, size=100):\n    self.size = size\n    self.queue = DoublyLinkedList()\n    self.hash = {}\n\n  def __len__(self):\n    return self.queue.size\n\n  def keys(self):\n    return self.hash.keys()\n\n  def values(self):\n    return ( node.val for val in self.queue )\n\n  def items(self):\n    return ( (key, node.val) for key, node in self.hash.items() )\n\n  def clear(self):\n    self.queue = DoublyLinkedList()\n    self.hash = {}\n\n  def resize(self, new_size):\n    if new_size < 0:\n      raise ValueError(""The LRU limit must be a positive number. Got: "" + str(new_size))\n\n    if new_size == 0:\n      self.clear()\n      return\n\n    if new_size >= self.size:\n      self.size = new_size\n      return \n\n    while len(self.queue) > new_size:\n      (key,val) = self.queue.delete_tail()\n      del self.hash[key]\n\n  def delete(self, key):\n    if key not in self.hash:\n      raise KeyError(""{} not in cache."".format(key))\n\n    node = self.hash[key]\n    self.queue.delete(node)\n    del self.hash[key]\n\n  def get(self, key, default=None):\n    if key not in self.hash:\n      if default is None:\n        raise KeyError(""{} not in cache."".format(key))\n      return default\n\n    node = self.hash[key]\n    self.queue.promote_to_head(node)\n\n    return node.val[1]\n\n  def set(self, key, val):\n    if self.size == 0:\n      return\n\n    pair = (key,val)\n    if key in self.hash:\n      node = self.hash[key]\n      node.val = pair\n      self.queue.promote_to_head(node)\n      return\n\n    self.queue.prepend(pair)\n    self.hash[key] = self.queue.head\n\n    while len(self.queue) > self.size:\n      (tkey,tval) = self.queue.delete_tail()\n      del self.hash[tkey]     \n\n  def __contains__(self, key):\n    return key in self.hash\n\n  def __getitem__(self, key):\n    return self.get(key)\n\n  def __setitem__(self, key, val):\n    return self.set(key, val)\n\n  def __delitem__(self, key):\n    return self.delete(key)\n\n  def __str__(self):\n    return str(self.queue)\n\n\n\n'"
cloudvolume/mesh.py,39,"b'import copy\nimport struct\n\nimport numpy as np\n\nfrom .exceptions import MeshDecodeError\nfrom .lib import yellow, Vec\n\nNOTICE = {\n  \'vertices\': 0,\n  \'num_vertices\': 0,\n  \'faces\': 0,\n}\n\ndef deprecation_notice(key):\n  if NOTICE[key] < 1:\n    print(yellow(""""""\n  Deprecation Notice: Meshes, formerly dicts, are now PrecomputedMesh objects\n  as of CloudVolume 0.51.0, renamed to Mesh objects as of 0.53.0\n\n  Please change mesh[\'{}\'] to mesh.{}\n  """""".format(key, key)))\n    NOTICE[key] += 1\n\n\ndef is_draco_chunk_aligned(verts, chunk_size, draco_grid_size):\n  """"""\n  Return a mask that for each vertex is true iff it is within\n  half a draco_grid_size from a chunk border.\n  """"""\n  dist_to_chunk_behind = np.mod(verts, chunk_size)\n  dist_to_chunk_ahead = chunk_size - dist_to_chunk_behind\n  # Draco rounds up\n  is_on_chunk_behind = np.any(\n    dist_to_chunk_behind < (draco_grid_size / 2),\n    axis=1,\n  )\n  is_on_chunk_ahead = np.any(\n    dist_to_chunk_ahead <= (draco_grid_size / 2),\n    axis=1,\n  )\n  return np.logical_or(is_on_chunk_behind, is_on_chunk_ahead)\n    \n\nclass Mesh(object):\n  """"""\n  Represents the vertices, faces, and normals of a mesh\n  as numpy arrays.\n\n  class Mesh:\n    ndarray[float32, ndim=2] self.vertices: [ [x,y,z], ... ]\n    ndarray[uint32,  ndim=2] self.faces:    [ [v1,v2,v3], ... ]\n    ndarray[float32, ndim=2] self.normals:  [ [nx,ny,nz], ... ]\n  """"""\n  def __init__(\n    self, vertices, faces, normals=None, \n    segid=None, encoding_type=None, encoding_options=None\n  ):\n    self.vertices = np.array(vertices, dtype=np.float32)\n    self.faces = np.array(faces, dtype=np.uint32)\n\n    if normals is None:\n      self.normals = np.array([], dtype=np.float32).reshape((0,3))\n    else:\n      self.normals = np.array(normals, dtype=np.float32)\n\n    self.segid = segid\n    self.encoding_type = encoding_type\n    self.encoding_options = encoding_options \n\n  def __len__(self):\n    return self.vertices.shape[0]\n\n  def __eq__(self, other):\n    """"""Tests strict equality between two meshes.""""""\n\n    no_self_normals = self.normals is None or self.normals.size == 0\n    no_other_normals = other.normals is None or other.normals.size == 0\n\n    if no_self_normals != no_other_normals:\n      return False\n       \n    equality = np.all(self.vertices == other.vertices) \\\n      and np.all(self.faces == other.faces)\n\n    if no_self_normals:\n      return equality\n\n    return (equality and np.all(self.normals == other.normals))\n\n  def __repr__(self):\n    return ""Mesh(vertices<{}>, faces<{}>, normals<{}>, segid={}, encoding_type=<{}>)"".format(\n      self.vertices.shape[0], self.faces.shape[0], self.normals.shape[0],\n      self.segid, self.encoding_type\n    )\n\n  def __getitem__(self, key):\n    val = None \n    if key == \'vertices\':\n      val = self.vertices\n    elif key == \'num_vertices\':\n      val = len(self)\n    elif key == \'faces\':\n      val = self.faces\n    else:\n      raise KeyError(""{} not found."".format(key))\n\n    deprecation_notice(key)\n    return val\n\n  def empty(self):\n    return self.vertices.size == 0 or self.faces.size == 0\n\n  def clone(self):\n    return Mesh(\n      np.copy(self.vertices), np.copy(self.faces), np.copy(self.normals),\n      self.segid, \n      encoding_type=copy.deepcopy(self.encoding_type),\n      encoding_options=copy.deepcopy(self.encoding_options),\n    )\n\n  def triangles(self):\n    Nf = self.faces.shape[0]\n    tris = np.zeros( (Nf, 3, 3), dtype=np.float32, order=\'C\' ) # triangle, vertices, (x,y,z)\n\n    for i in range(Nf):\n      for j in range(3):\n        tris[i,j,:] = self.vertices[ self.faces[i,j] ]\n\n    return tris\n\n  @classmethod\n  def concatenate(cls, *meshes):\n    vertex_ct = np.zeros(len(meshes) + 1, np.uint32)\n    vertex_ct[1:] = np.cumsum([ len(mesh) for mesh in meshes ])\n\n    vertices = np.concatenate([ mesh.vertices for mesh in meshes ])\n    \n    faces = np.concatenate([ \n      mesh.faces + vertex_ct[i] for i, mesh in enumerate(meshes) \n    ])\n\n    normals = np.concatenate([ mesh.normals for mesh in meshes ])\n\n    encoding_type = list(set([ mesh.encoding_type for mesh in meshes ]))\n    if len(encoding_type) == 1:\n      encoding_type = encoding_type[0]\n\n    return Mesh(vertices, faces, normals, encoding_type=encoding_type)\n\n  def consolidate(self):\n    """"""Remove duplicate vertices and faces. Returns a new mesh object.""""""\n    if self.empty():\n      return Mesh([], [], normals=None)\n\n    vertices = self.vertices\n    faces = self.faces\n    normals = self.normals\n\n    eff_verts, uniq_idx, idx_representative = np.unique(\n      vertices, axis=0, return_index=True, return_inverse=True\n    )\n\n    face_vector_map = np.vectorize(lambda x: idx_representative[x])\n    eff_faces = face_vector_map(faces)\n    eff_faces = np.unique(eff_faces, axis=0)\n\n    # normal_vector_map = np.vectorize(lambda idx: normals[idx])\n    # eff_normals = normal_vector_map(uniq_idx)\n\n    return Mesh(eff_verts, eff_faces, None, \n      segid=self.segid,\n      encoding_type=copy.deepcopy(self.encoding_type),\n      encoding_options=copy.deepcopy(self.encoding_options),\n    )\n\n  @classmethod\n  def from_precomputed(self, binary):\n    """"""\n    Mesh from_precomputed(self, binary)\n\n    Decode a Precomputed format mesh from a byte string.\n    \n    Format:\n      uint32        Nv * float32 * 3   uint32 * 3 until end\n      Nv            (x,y,z)            (v1,v2,v2)\n      N Vertices    Vertices           Faces\n    """"""\n    num_vertices = struct.unpack(""=I"", binary[0:4])[0]\n    try:\n      # count=-1 means all data in buffer\n      vertices = np.frombuffer(binary, dtype=np.float32, count=3*num_vertices, offset=4)\n      faces = np.frombuffer(binary, dtype=np.uint32, count=-1, offset=(4 + 12 * num_vertices)) \n    except ValueError:\n      raise MeshDecodeError(""""""\n        The input buffer is too small for the Precomputed format.\n        Minimum Bytes: {} \n        Actual Bytes: {}\n      """""".format(4 + 4 * num_vertices, len(binary)))\n\n    vertices = vertices.reshape(num_vertices, 3)\n    faces = faces.reshape(faces.size // 3, 3)\n\n    return Mesh(\n      vertices, faces, normals=None, \n      encoding_type=\'precomputed\'\n    )\n\n  def to_precomputed(self):\n    """"""\n    bytes to_precomputed(self)\n\n    Convert mesh into binary format compatible with Neuroglancer.\n    Does not preserve normals.\n    """"""\n    vertex_index_format = [\n      np.uint32(self.vertices.shape[0]), # Number of vertices (3 coordinates)\n      self.vertices,\n      self.faces\n    ]\n    return b\'\'.join([ array.tobytes(\'C\') for array in vertex_index_format ])\n\n  @classmethod\n  def from_obj(self, text):\n    """"""Given a string representing a Wavefront OBJ file, decode to a Mesh.""""""\n\n    vertices = []\n    faces = []\n    normals = []\n\n    if type(text) is bytes:\n      text = text.decode(\'utf8\')\n\n    for line in text.split(\'\\n\'):\n      line = line.strip()\n      if len(line) == 0:\n        continue\n      elif line[0] == \'#\':\n        continue\n      elif line[0] == \'f\':\n        if line.find(\'/\') != -1:\n          # e.g. f 6092/2095/6079 6087/2092/6075 6088/2097/6081\n          (v1, vt1, vn1, v2, vt2, vn2, v3, vt3, vn3) = re.match(r\'f\\s+(\\d+)/(\\d*)/(\\d+)\\s+(\\d+)/(\\d*)/(\\d+)\\s+(\\d+)/(\\d*)/(\\d+)\', line).groups()\n        else:\n          (v1, v2, v3) = re.match(r\'f\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\', line).groups()\n        faces.append( (int(v1), int(v2), int(v3)) )\n      elif line[0] == \'v\':\n        if line[1] == \'t\': # vertex textures not supported\n          # e.g. vt 0.351192 0.337058\n          continue \n        elif line[1] == \'n\': # vertex normals\n          # e.g. vn 0.992266 -0.033290 -0.119585\n          (n1, n2, n3) = re.match(r\'vn\\s+([-\\d\\.]+)\\s+([-\\d\\.]+)\\s+([-\\d\\.]+)\', line).groups()\n          normals.append( (float(n1), float(n2), float(n3)) )\n        else:\n          # e.g. v -0.317868 -0.000526 -0.251834\n          (v1, v2, v3) = re.match(r\'v\\s+([-\\d\\.]+)\\s+([-\\d\\.]+)\\s+([-\\d\\.]+)\', line).groups()\n          vertices.append( (float(v1), float(v2), float(v3)) )\n\n    vertices = np.array(vertices, dtype=np.float32)\n    faces = np.array(faces, dtype=np.uint32)\n    normals = np.array(normals, dtype=np.float32)\n\n    return Mesh(vertices, faces - 1, normals)\n\n  def to_obj(self):\n    """"""Return a string representing a .obj file.""""""\n    objdata = []\n    objdata += [ \'v {:.5f} {:.5f} {:.5f}\'.format(*vertex) for vertex in self.vertices ]\n    objdata += [ \'f {} {} {}\'.format(*face) for face in (self.faces+1) ] # obj is 1 indexed\n    objdata = \'\\n\'.join(objdata) + \'\\n\'\n    return objdata.encode(\'utf8\')\n\n  def to_ply(self):\n    """"""\n    Return a bytearray in .ply format, \n    a more compact format than .obj that\'s still widely readable.\n    """"""\n    vertexct = self.vertices.shape[0]\n    trianglect = self.faces.shape[0]\n\n    # Header\n    plydata = bytearray(""""""ply\nformat binary_little_endian 1.0\nelement vertex {}\nproperty float x\nproperty float y\nproperty float z\nelement face {}\nproperty list int int vertex_indices\nend_header\n"""""".format(vertexct, trianglect).encode(\'utf8\'))\n\n    # Vertex data (x y z): ""fff"" \n    plydata.extend(self.vertices.tobytes(\'C\'))\n\n    # Faces (3 f1 f2 f3): ""3iii"" \n    plydata.extend(\n      np.insert(self.faces, 0, 3, axis=1).tobytes(\'C\')\n    )\n\n    return plydata\n\n  @classmethod\n  def from_draco(cls, binary):\n    import DracoPy\n\n    try:\n      mesh_object = DracoPy.decode_buffer_to_mesh(binary)\n      vertices = np.array(mesh_object.points).astype(np.float32)\n      faces = np.array(mesh_object.faces).astype(np.uint32)\n    except ValueError:\n      raise MeshDecodeError(""Not a valid draco mesh."")\n\n    Nv = len(vertices)\n    Nf = len(faces)\n\n    if Nv % 3 != 0: \n      raise MeshDecodeError(""Mesh vertices not 3D. Cannot decode. Num. vertices: {}"".format(Nv))\n\n    if Nf % 3 != 0:\n      raise MeshDecodeError(""Faces are not sets of triples. Cannot decode. Num. faces: {}"".format(Nf))\n\n    vertices = vertices.reshape(Nv // 3, 3)\n    faces = faces.reshape(Nf // 3, 3)\n\n    return Mesh(\n      vertices, faces, normals=None,\n      encoding_type=\'draco\', \n      encoding_options=mesh_object.encoding_options\n    )\n\n  def deduplicate_chunk_boundaries(self, chunk_size, is_draco=False, draco_grid_size=None, offset=(0,0,0)):\n    offset = Vec(*offset)\n    verts = self.vertices - offset\n    # find all vertices that are exactly on chunk_size boundaries\n    if is_draco:\n      if draco_grid_size is None:\n        raise ValueError(\'Must specify draco grid size to dedup draco meshes\')\n      is_chunk_aligned = is_draco_chunk_aligned(verts, chunk_size, draco_grid_size=draco_grid_size)\n    else:\n      is_chunk_aligned = np.any(np.mod(verts, chunk_size) == 0, axis=1)\n\n    faces = self.faces\n\n    # find all vertices that have exactly 2 duplicates\n    unique_vertices, unique_inverse, counts = np.unique(\n      verts, return_inverse=True, return_counts=True, axis=0\n    )\n    \n    only_double = np.where(counts == 2)[0]\n    is_doubled = np.isin(unique_inverse, only_double)\n    # this stores whether each vertex should be merged or not\n    do_merge = np.array(is_doubled & is_chunk_aligned)\n\n    # setup an artificial 4th coordinate for vertex positions\n    # which will be unique in general, \n    # but then repeated for those that are merged\n    new_vertices = np.hstack((verts, np.arange(verts.shape[0])[:, np.newaxis]))\n    new_vertices[do_merge, 3] = -1\n  \n    faces = faces.flatten()\n\n    # use unique to make the artificial vertex list unique and reindex faces\n    vertices, newfaces = np.unique(new_vertices[faces], return_inverse=True, axis=0)\n    newfaces = newfaces.astype(np.uint32).reshape( (len(newfaces) // 3, 3) )\n\n    return Mesh(vertices[:,0:3] + offset, newfaces, None, segid=self.segid, \n      encoding_type=self.encoding_type, encoding_options=self.encoding_options\n    )\n\n  def viewer(self):\n    try:\n      import vtk\n    except ImportError:\n      print(""The mesh viewer requires the OpenGL based vtk. Try: pip install vtk --upgrade"")\n      return\n\n    colors = vtk.vtkNamedColors()\n    # Set the background color.\n    bkg = map(lambda x: x / 255.0, [40, 40, 40, 255])\n    colors.SetColor(""BkgColor"", *bkg)\n\n    # This creates a point cloud model\n    points = vtk.vtkPoints()\n    verts = vtk.vtkCellArray()\n    polydata = vtk.vtkPolyData()\n    polydata.SetPoints(points)\n    polydata.SetVerts(verts)\n    for vertex in self.vertices:\n      pid = points.InsertNextPoint(vertex)\n      verts.InsertNextCell(1)\n      verts.InsertCellPoint(pid)\n\n    # The mapper is responsible for pushing the geometry into the graphics\n    # library. It may also do color mapping, if scalars or other\n    # attributes are defined.\n    mapper = vtk.vtkPolyDataMapper()\n    mapper.SetInputData(polydata)\n\n    # The actor is a grouping mechanism: besides the geometry (mapper), it\n    # also has a property, transformation matrix, and/or texture map.\n    # Here we set its color and rotate it -22.5 degrees.\n    cylinderActor = vtk.vtkActor()\n    cylinderActor.SetMapper(mapper)\n    cylinderActor.GetProperty().SetColor(colors.GetColor3d(""Mint""))\n    cylinderActor.RotateX(30.0)\n    cylinderActor.RotateY(-45.0)\n\n    # Create the graphics structure. The renderer renders into the render\n    # window. The render window interactor captures mouse events and will\n    # perform appropriate camera or actor manipulation depending on the\n    # nature of the events.\n    ren = vtk.vtkRenderer()\n    renWin = vtk.vtkRenderWindow()\n    renWin.AddRenderer(ren)\n    iren = vtk.vtkRenderWindowInteractor()\n    iren.SetRenderWindow(renWin)\n\n    # Add the actors to the renderer, set the background and size\n    ren.AddActor(cylinderActor)\n    ren.SetBackground(colors.GetColor3d(""BkgColor""))\n    renWin.SetSize(800, 800)\n\n    text = ""Point Cloud Rendering of Mesh Object""\n    if self.segid is not None:\n      renWin.SetWindowName(text + "" (Label {})"".format(self.segid))\n    else:\n      renWin.SetWindowName(text)\n\n    # This allows the interactor to initalize itself. It has to be\n    # called before an event loop.\n    iren.Initialize()\n\n    # We\'ll zoom in a little by accessing the camera and invoking a ""Zoom""\n    # method on it.\n    ren.ResetCamera()\n    ren.GetActiveCamera().Zoom(1.5)\n    renWin.Render()\n\n    # Start the event loop.\n    iren.Start()    \n'"
cloudvolume/paths.py,0,"b'from collections import namedtuple\nimport os\nimport posixpath\nimport re\nimport sys\n\nfrom .exceptions import UnsupportedProtocolError\nfrom .lib import yellow, toabs\n\nExtractedPath = namedtuple(\'ExtractedPath\', \n  (\'format\', \'protocol\', \'bucket\', \'basepath\', \'no_bucket_basepath\', \'dataset\', \'layer\')\n)\n\nALLOWED_PROTOCOLS = [ \'gs\', \'file\', \'s3\', \'matrix\', \'http\', \'https\' ]\nALLOWED_FORMATS = [ \'graphene\', \'precomputed\', \'boss\' ] \n\nCLOUDPATH_ERROR = yellow(""""""\nCloud Path must conform to FORMAT://PROTOCOL://BUCKET/PATH\nExamples: \n  precomputed://gs://test_bucket/em\n  gs://test_bucket/em\n  graphene://https://example.com/image/em\n\nSupported Formats: None (precomputed), {}\nSupported Protocols: {}\n\nCloud Path Recieved: {}\n"""""").format(\n  "", "".join(ALLOWED_FORMATS), "", "".join(ALLOWED_PROTOCOLS), \'{}\' # curry first two\n)\n\ndef ascloudpath(epath):\n  return ""{}://{}://{}"".format(\n    epath.format, epath.protocol, \n    posixpath.join(epath.basepath, epath.dataset, epath.layer)\n  )\n\ndef pop_protocol(cloudpath):\n  protocol_re = re.compile(r\'(\\w+)://\')\n\n  match = re.match(protocol_re, cloudpath)\n\n  if not match:\n    return (None, cloudpath)\n\n  (protocol,) = match.groups()\n  cloudpath = re.sub(protocol_re, \'\', cloudpath, count=1)\n\n  return (protocol, cloudpath)\n\ndef extract_format_protocol(cloudpath):\n  error = UnsupportedProtocolError(CLOUDPATH_ERROR.format(cloudpath))\n  \n  (proto, cloudpath) = pop_protocol(cloudpath)\n  \n  if proto is None:\n    raise error # e.g. ://test_bucket, test_bucket, wow//test_bucket\n\n  fmt, protocol = None, None\n\n  if proto in ALLOWED_PROTOCOLS:\n    fmt = \'precomputed\'\n    protocol = proto \n  elif proto in ALLOWED_FORMATS:\n    fmt = proto\n\n  (proto, cloudpath) = pop_protocol(cloudpath)\n\n  if proto in ALLOWED_FORMATS:\n    raise error # e.g. gs://graphene://\n  elif proto in ALLOWED_PROTOCOLS:\n    if protocol is None:\n      protocol = proto\n    else:\n      raise error # e.g. gs://gs:// \n\n  (proto, cloudpath) = pop_protocol(cloudpath)\n  if proto is not None:\n    raise error # e.g. gs://gs://gs://\n\n  return (fmt, protocol, cloudpath)\n\ndef strict_extract(cloudpath, windows=None, disable_toabs=False):\n  """"""\n  Same as cloudvolume.paths.extract, but raise an additional \n  cloudvolume.exceptions.UnsupportedProtocolError\n  if either dataset or layer is not set.\n\n  Returns: ExtractedPath\n  """"""\n  path = extract(cloudpath, windows, disable_toabs)\n\n  if path.dataset == \'\' or path.layer == \'\':\n    raise UnsupportedProtocolError(CLOUDPATH_ERROR.format(cloudpath))\n\n  return path\n\ndef extract(cloudpath, windows=None, disable_toabs=False):\n  """"""\n  Given a valid cloudpath of the form \n  format://protocol://bucket/.../dataset/layer\n\n  Where format in: None, precomputed, boss, graphene\n  Where protocol in: None, file, gs, s3, http(s), matrix\n\n  Return an ExtractedPath which breaks out the components\n  format, protocol, bucket, path, intermediate_path, dataset, layer\n\n  Raise a cloudvolume.exceptions.UnsupportedProtocolError if the\n  path does not conform to a valid path.\n\n  Windows OS may handle file protocol paths slightly differently\n  than other OSes.\n\n  Returns: ExtractedPath\n  """"""\n  if len(cloudpath) == 0:\n    return ExtractedPath(\'\',\'\',\'\',\'\',\'\',\'\',\'\')\n\n  windows_file_re = re.compile(r\'((?:\\w:\\\\)[\\d\\w_\\.\\-]+(?:\\\\)?)\') # for C:\\what\\a\\great\\path\n  bucket_re = re.compile(r\'^(/?[~\\d\\w_\\.\\-]+(?::\\d+)?)/\') # posix /what/a/great/path\n  \n  error = UnsupportedProtocolError(CLOUDPATH_ERROR.format(cloudpath))\n\n  if windows is None:\n    windows = sys.platform == \'win32\'\n\n  if disable_toabs:\n    abspath = lambda x: x # can\'t prepend linux paths when force testing windows\n  else:\n    abspath = toabs    \n\n  fmt, protocol, cloudpath = extract_format_protocol(cloudpath)\n  \n  split_char = \'/\'\n  if protocol == \'file\':\n    cloudpath = abspath(cloudpath)\n    if windows:\n      bucket_re = windows_file_re\n    split_char = os.path.sep\n\n  match = re.match(bucket_re, cloudpath)\n  if not match:\n    raise error\n\n  (bucket,) = match.groups()\n\n  splitcloudpath = cloudpath \n  if splitcloudpath[0] == split_char:\n    splitcloudpath = splitcloudpath[1:]\n  if splitcloudpath[-1] == split_char:\n    splitcloudpath = splitcloudpath[:-1]\n\n  splitties = splitcloudpath.split(split_char)\n  if len(splitties) == 0:\n    return ExtractedPath(fmt, protocol, bucket, cloudpath, \'\', bucket, \'\')\n  elif len(splitties) == 1:\n    dataset = bucket\n    layer = splitties[0]\n    basepath = split_char.join(splitties[:-1])\n    no_bucket_basepath = split_char.join(splitties[1:-1])\n  elif len(splitties) >= 2:\n    dataset, layer = splitties[-2:]\n    no_bucket_basepath = split_char.join(splitties[1:-1])\n    basepath = split_char.join([bucket] + splitties[1:-1])\n\n  return ExtractedPath(\n    fmt, protocol, bucket, \n    basepath, no_bucket_basepath, \n    dataset, layer\n  )\n\ndef to_https_protocol(cloudpath):\n  cloudpath = cloudpath.replace(""gs://"", ""https://storage.googleapis.com/"", 1)\n  cloudpath = cloudpath.replace(""s3://"", ""https://s3.amazonaws.com/"", 1)\n  cloudpath = cloudpath.replace(""matrix://"", ""https://s3-hpcrc.rc.princeton.edu/"", 1)\n  return cloudpath\n'"
cloudvolume/provenance.py,0,"b'import python_jsonschema_objects as pjs\nimport json\nimport json5\n\n__all__ = [ \'DatasetProvenance\', \'DataLayerProvenance\' ]\n\ndataset_provenance_schema = {\n  ""$schema"": ""http://json-schema.org/draft-04/schema#"",\n  ""title"": ""Dataset Provenance"",\n  ""description"": ""Represents a dataset and its derived data layers."",\n  ""required"": [\n    ""dataset_name"", ""dataset_description"", \n    ""organism"", ""imaged_date"", ""imaged_by"", \n    ""owners""\n  ],\n  ""properties"": {\n    \'dataset_name\': { \'type\': \'string\' },\n    \'dataset_description\': { \'type\': \'string\' },\n    \'organism\': {\n      \'type\': \'string\',\n      \'description\': \'Species, sex, strain identifier\',\n    },\n    \'imaged_date\': { \'type\': \'string\' },\n    \'imaged_by\': { \'type\': \'string\' },\n    \'references\': { # e.g. dois, urls, titles\n      ""type"": ""array"",\n      ""items"": {\n        ""type"": ""string""\n      },\n      ""minItems"": 0,\n      ""uniqueItems"": True, # e.g. email addresses  \n    }, \n    \'owners\': {\n      ""type"": ""array"",\n      ""items"": {\n        ""type"": ""string""\n      },\n      ""minItems"": 1,\n      ""uniqueItems"": True, # e.g. email addresses  \n    }\n  }\n}\n\nbuilder = pjs.ObjectBuilder(dataset_provenance_schema)\nclasses = builder.build_classes()\nDatasetProvenance = classes.DatasetProvenance\n\nlayer_provenance_schema = {\n  ""$schema"": ""http://json-schema.org/draft-04/schema#"",\n  ""title"": ""Data Layer Provenance"",\n  ""description"": ""Represents a data layer within a dataset. e.g. image, segmentation, etc"",\n  ""required"": [\n    \'description\', \'sources\', \n    \'processing\', \'owners\'\n  ],\n  ""properties"": {\n    \'description\': { \'type\': \'string\' },\n    \'sources\': { # e.g. [ \'gs://neuroglancer/pinky40_v11/image\' \n      ""type"": ""array"",\n      ""items"": {\n        ""type"": ""string""\n      },\n      ""minItems"": 0,\n      ""uniqueItems"": True,\n    }, \n    \'processing\': {\n      ""type"": ""array"",\n      ""items"": {\n        ""type"": ""object""\n      },\n      ""minItems"": 0,\n    },\n    # e.g. processing = [ \n    #    { \'method\': \'inceptionnet\', \'by\': \'example@princeton.edu\' }, \n    #    { \'method\': \'downsample\', \'by\': \'example2@princeton.edu\', \'description\': \'demo of countless downsampling\' } \n    #    { \'method\': \'meshing\', \'by\': \'example2@princeton.edu\', \'description\': \'512x512x512 mip 3 simplification factor 30\' }\n    # ]\n    \'owners\': {\n      ""type"": ""array"",\n      ""items"": {\n        ""type"": ""string""\n      },\n      ""minItems"": 0,\n      ""uniqueItems"": True,\n    },\n  }\n}\n\nbuilder = pjs.ObjectBuilder(layer_provenance_schema)\nclasses = builder.build_classes()\nDataLayerProvenanceValidation = classes.DataLayerProvenance\n\nclass DataLayerProvenance(dict):\n  def __init__(self, *args, **kwargs):\n    dict.__init__(self, *args, **kwargs)\n    if \'description\' not in self:\n      self[\'description\'] = \'\'\n    if \'owners\' not in self:\n      self[\'owners\'] = []\n    if \'processing\' not in self:\n      self[\'processing\'] = []\n    if \'sources\' not in self:\n      self[\'sources\'] = \'\'\n\n  def validate(self):\n    DataLayerProvenanceValidation(**self).validate()\n\n  def serialize(self):\n    return json.dumps(self)\n\n  def from_json(self, data):\n    data = json5.loads(data)\n    self.update(data)\n    return self\n\n  @classmethod\n  def create(cls, mydict):\n    return DatasetProvenance(**mydict)\n\n  @property \n  def description(self):\n    return self[\'description\']\n  @description.setter \n  def description(self, val):\n    self[\'description\'] = val\n  \n  @property \n  def owners(self):\n    return self[\'owners\']\n  @owners.setter \n  def owners(self, val):\n    self[\'owners\'] = val\n\n  @property \n  def processing(self):\n    return self[\'processing\']\n  @processing.setter \n  def processing(self, val):\n    self[\'processing\'] = val\n\n  @property \n  def sources(self):\n    return self[\'sources\']\n  @sources.setter \n  def sources(self, val):\n    self[\'sources\'] = val\n    \n\n\n\n\n\n\n\n\n\n\n'"
cloudvolume/py_compressed_segmentation.py,17,"b'# Copyright (c) 2016, 2017, 2018 Forschungszentrum Juelich GmbH\n# Author: Yann Leprince <y.leprince@fz-juelich.de>\n# \n# Minor later modifications (c) 2018 by William Silversmith <ws9@princeton.edu>\n#\n# This software is made available under the MIT license, see bottom of file.\n\nfrom six.moves import zip_longest\nimport functools\nimport itertools\nimport struct\nimport sys\n\nimport numpy as np\n\ndef ceil_div(a, b):\n    """"""Ceil integer division (``ceil(a / b)`` using integer arithmetic).""""""\n    return (a - 1) // b + 1\n\n\nclass InvalidFormatError(Exception):\n    """"""Raised when chunk data cannot be decoded properly.""""""\n    pass\n\ndef pad_block(block, block_size):\n    """"""Pad a block to block_size with its most frequent value""""""\n    unique_vals, unique_counts = np.unique(block, return_counts=True)\n    most_frequent_value = unique_vals[np.argmax(unique_counts)]\n    return np.pad(block,\n                  tuple((0, desired_size - actual_size)\n                        for desired_size, actual_size\n                        in zip(block_size, block.shape)),\n                  mode=""constant"", constant_values=most_frequent_value)\n\n\ndef number_of_encoding_bits(elements):\n    for bits in (0, 1, 2, 4, 8, 16, 32):\n        if 2 ** bits >= elements:\n            return bits\n    raise ValueError(""Too many elements!"")\n\n\nCOMPRESSED_SEGMENTATION_DATA_TYPES = (\n    np.dtype(np.uint32).newbyteorder(""<""),\n    np.dtype(np.uint64).newbyteorder(""<""),\n)\n\n\ndef encode_chunk(chunk, block_size):\n    # Construct file in memory step by step\n    num_channels = chunk.shape[0]\n    buf = bytearray(4 * num_channels)\n\n    assert chunk.dtype in COMPRESSED_SEGMENTATION_DATA_TYPES\n\n    for channel in range(num_channels):\n        # Write offset of the current channel into the header\n        assert len(buf) % 4 == 0\n        struct.pack_into(""<I"", buf, channel * 4, len(buf) // 4)\n\n        buf += _encode_channel(\n            chunk[channel, :, :, :], block_size)\n    return buf\n\n\ndef _encode_channel(chunk_channel, block_size):\n    # Grid size (number of blocks in the chunk)\n    gx = ceil_div(chunk_channel.shape[2], block_size[0])\n    gy = ceil_div(chunk_channel.shape[1], block_size[1])\n    gz = ceil_div(chunk_channel.shape[0], block_size[2])\n    stored_lut_offsets = {}\n    buf = bytearray(gx * gy * gz * 8)\n    for z, y, x in np.ndindex((gz, gy, gx)):\n        block = chunk_channel[\n            z*block_size[2] : (z+1)*block_size[2],\n            y*block_size[1] : (y+1)*block_size[1],\n            x*block_size[0] : (x+1)*block_size[0]\n        ]\n        if block.shape != block_size:\n            block = pad_block(block, block_size)\n\n        # TODO optimization: to improve additional compression (gzip), sort the\n        # list of unique symbols by decreasing frequency using\n        # return_counts=True so that low-value symbols are used more often.\n        # Alternatively, sort by label value to improve sharing of lookup\n        # tables...\n        (lookup_table, encoded_values) = np.unique(\n            block, return_inverse=True, return_counts=False)\n        bits = number_of_encoding_bits(len(lookup_table))\n\n        # Write look-up table to the buffer (or re-use another one)\n        lut_bytes = lookup_table.astype(block.dtype).tobytes()\n        if lut_bytes in stored_lut_offsets:\n            lookup_table_offset = stored_lut_offsets[lut_bytes]\n        else:\n            assert len(buf) % 4 == 0\n            lookup_table_offset = len(buf) // 4\n            buf += lut_bytes\n            stored_lut_offsets[lut_bytes] = lookup_table_offset\n\n        assert len(buf) % 4 == 0\n        encoded_values_offset = len(buf) // 4\n        buf += _pack_encoded_values(encoded_values, bits)\n\n        assert lookup_table_offset == (lookup_table_offset & 0xFFFFFF)\n        struct.pack_into(""<II"", buf, 8 * (x + gx * (y + gy * z)),\n                         lookup_table_offset | (bits << 24),\n                         encoded_values_offset)\n    return buf\n\n\ndef _pack_encoded_values(encoded_values, bits):\n    # TODO optimize with np.packbits for bits == 1\n    if bits == 0:\n        return bytes()\n    else:\n        values_per_32bit = 32 // bits\n        assert np.all(encoded_values == encoded_values & ((1 << bits) - 1))\n        padded_values = np.empty(\n            values_per_32bit * ceil_div(len(encoded_values), values_per_32bit),\n            dtype=""<I"")\n        padded_values[:len(encoded_values)] = encoded_values\n        padded_values[len(encoded_values):] = 0\n        packed_values = functools.reduce(\n            np.bitwise_or,\n            (padded_values[shift::values_per_32bit] << (shift * bits)\n             for shift in range(values_per_32bit)))\n        return packed_values.tobytes()\n\n\ndef decode_chunk_into(chunk, buf, block_size):\n    num_channels = chunk.shape[0]\n    # Grid size (number of blocks in the chunk)\n    gx = ceil_div(chunk.shape[3], block_size[0])\n    gy = ceil_div(chunk.shape[2], block_size[1])\n    gz = ceil_div(chunk.shape[1], block_size[2])\n\n    if len(buf) < num_channels * (4 + 8 * gx * gy * gz):\n        raise InvalidFormatError(""compressed_segmentation file too short"")\n\n    if sys.version_info < (3,):\n        channel_offsets = struct.unpack(""<I"", buf[:4*num_channels])\n        channel_offsets = [ 4 * ret for ret in channel_offsets ]\n    else:\n        channel_offsets = [\n            4 * ret[0] for ret in struct.iter_unpack(""<I"", buf[:4*num_channels])\n        ]\n\n    for channel, (offset, next_offset) in \\\n        enumerate(zip_longest(channel_offsets, channel_offsets[1:])):\n\n        # next_offset will be None for the last channel\n        if offset + 8 * gx * gy * gz > len(buf):\n            raise InvalidFormatError(""compressed_segmentation channel offset ""\n                                     ""is too large (truncated file?)"")\n        _decode_channel_into(\n            chunk, channel, buf[offset:next_offset], block_size\n        )\n\n    return chunk\n\n\ndef _decode_channel_into(chunk, channel, buf, block_size):\n    # Grid size (number of blocks in the chunk)\n    gx = ceil_div(chunk.shape[3], block_size[0])\n    gy = ceil_div(chunk.shape[2], block_size[1])\n    gz = ceil_div(chunk.shape[1], block_size[2])\n    block_num_elem = block_size[0] * block_size[1] * block_size[2]\n    for z, y, x in np.ndindex((gz, gy, gx)):\n        # Read the block header\n        res = struct.unpack_from(""<II"", buf, 8 * (x + gx * (y + gy * z)))\n        lookup_table_offset = 4 * (res[0] & 0x00FFFFFF)\n        bits = res[0] >> 24\n        if bits not in (0, 1, 2, 4, 8, 16, 32):\n            raise InvalidFormatError(""Invalid number of encoding bits for ""\n                                     ""compressed_segmentation block ({0})""\n                                     .format(bits))\n        encoded_values_offset = 4 * res[1]\n        if bits != 0 and encoded_values_offset > len(buf):\n            raise InvalidFormatError(""Invalid offset for encoded values in ""\n                                     ""compressed_segmentation block ""\n                                     ""(truncated file?)"")\n        lookup_table_past_end = lookup_table_offset + chunk.itemsize * min(\n            (2 ** bits),\n            ((len(buf) - lookup_table_offset) // chunk.itemsize)\n        )\n        lookup_table = np.frombuffer(\n            buf[lookup_table_offset:lookup_table_past_end], dtype=chunk.dtype)\n        if bits == 0:\n            block = np.empty(block_size, dtype=chunk.dtype)\n            try:\n                block[...] = lookup_table[0]\n            except IndexError as exc:\n                raise InvalidFormatError(\n                    ""Invalid compressed_segmentation data: indexing out of ""\n                    ""the lookup table"")\n        else:\n            values_per_32bit = 32 // bits\n            encoded_values_end = encoded_values_offset + 4 * (\n                ceil_div(block_num_elem, values_per_32bit)\n            )\n            packed_values = np.frombuffer(buf[encoded_values_offset:\n                                              encoded_values_end], dtype=""<I"")\n            encoded_values = _unpack_encoded_values(packed_values, bits,\n                                                    block_num_elem)\n            # Apply the lookup table\n            try:\n                decoded_values = lookup_table[encoded_values]\n            except IndexError as exc:\n                raise InvalidFormatError(\n                    ""Invalid compressed_segmentation data: indexing out of ""\n                    ""the lookup table"")\n            block = decoded_values.reshape((block_size[2],\n                                            block_size[1],\n                                            block_size[0]))\n\n        # Remove padding\n        zmax = min(block_size[2], chunk.shape[1] - z * block_size[2])\n        ymax = min(block_size[1], chunk.shape[2] - y * block_size[1])\n        xmax = min(block_size[0], chunk.shape[3] - x * block_size[0])\n        chunk[\n            channel,\n            z*block_size[2] : (z+1)*block_size[2],\n            y*block_size[1] : (y+1)*block_size[1],\n            x*block_size[0] : (x+1)*block_size[0]\n        ] = block[:zmax, :ymax, :xmax]\n\n\ndef _unpack_encoded_values(packed_values, bits, num_values):\n    if bits == 0:\n        return np.zeros(num_values, dtype=""<I"")\n    else:\n        bitmask = (1 << bits) - 1\n        values_per_32bit = 32 // bits\n        padded_values = np.empty(\n            values_per_32bit * ceil_div(num_values, values_per_32bit),\n            dtype=""<I"")\n        for shift in range(values_per_32bit):\n            padded_values[shift::values_per_32bit] = (\n                (packed_values >> (shift * bits)) & bitmask)\n        return padded_values[:num_values]\n\n\n# MIT License\n\n# Copyright (c) 2016 Forschungszentrum Juelich GmbH\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n'"
cloudvolume/scheduler.py,0,"b'import sys\n\nfrom tqdm import tqdm\n\nfrom .threaded_queue import ThreadedQueue, DEFAULT_THREADS\n\ndef schedule_threaded_jobs(\n    fns, concurrency=DEFAULT_THREADS, \n    progress=None, total=None\n  ):\n\n  if total is None:\n    try:\n      total = len(fns)\n    except TypeError: # generators don\'t have len\n      pass\n\n  pbar = tqdm(total=total, desc=progress, disable=(not progress))\n  results = []\n  \n  def updatefn(fn):\n    def realupdatefn(iface):\n      res = fn()\n      pbar.update(1)\n      results.append(res)\n    return realupdatefn\n\n  with ThreadedQueue(n_threads=concurrency) as tq:\n    for fn in fns:\n      tq.put(updatefn(fn))\n\n  return results\n\ndef schedule_green_jobs(\n    fns, concurrency=DEFAULT_THREADS, \n    progress=None, total=None\n  ):\n  import gevent.pool\n\n  if total is None:\n    try:\n      total = len(fns)\n    except TypeError: # generators don\'t have len\n      pass\n\n  pbar = tqdm(total=total, desc=progress, disable=(not progress))\n  results = []\n  \n  def updatefn(fn):\n    def realupdatefn():\n      res = fn()\n      pbar.update(1)\n      results.append(res)\n    return realupdatefn\n\n  pool = gevent.pool.Pool(concurrency)\n  for fn in fns:\n    pool.spawn( updatefn(fn) )\n\n  pool.join()\n  pool.kill()\n  pbar.close()\n\n  return results\n\ndef schedule_jobs(\n    fns, concurrency=DEFAULT_THREADS, \n    progress=None, total=None, green=False\n  ):\n  """"""\n  Given a list of functions, execute them concurrently until\n  all complete. \n\n  fns: iterable of functions\n  concurrency: number of threads\n  progress: Falsey (no progress), String: Progress + description\n  total: If fns is a generator, this is the number of items to be generated.\n  green: If True, use green threads.\n\n  Return: list of results\n  """"""\n  if green:\n    return schedule_green_jobs(fns, concurrency, progress, total)\n\n  return schedule_threaded_jobs(fns, concurrency, progress, total)\n\n\n'"
cloudvolume/secrets.py,0,"b""from __future__ import print_function\n\nfrom collections import defaultdict\nimport os\nimport json\n\nfrom google.oauth2 import service_account\n\nfrom .lib import mkdir, colorize\n\nHOME = os.path.expanduser('~')\nCLOUD_VOLUME_DIR = mkdir(os.path.join(HOME, '.cloudvolume'))\n\ndef secretpath(filepath):\n  preferred = os.path.join(CLOUD_VOLUME_DIR, filepath)\n  \n  if os.path.exists(preferred):\n    return preferred\n\n  backcompat = [\n    os.path.join(HOME, '.neuroglancer'), # older\n    '/' # original\n  ]\n\n  backcompat = [ os.path.join(path, filepath) for path in backcompat ] \n\n  for path in backcompat:\n    if os.path.exists(path):\n      print(colorize('yellow', 'Deprecation Warning: {} is now preferred to {}.'.format(preferred, path)))  \n      return path\n\n  return preferred\n\nproject_name_paths = [ \n  secretpath('secrets/project_name'),\n  secretpath('project_name')\n]\n\ndef default_google_project_name():\n  if 'GOOGLE_PROJECT_NAME' in os.environ: \n    return os.environ['GOOGLE_PROJECT_NAME']\n  else: \n    for path in project_name_paths:\n      if os.path.exists(path):\n        with open(path, 'r') as f:\n          return f.read().strip()\n\n  default_credentials_path = secretpath('secrets/google-secret.json')\n  if os.path.exists(default_credentials_path):\n    with open(default_credentials_path, 'rt') as f:\n      return json.loads(f.read())['project_id']\n\n  return None\n\nPROJECT_NAME = default_google_project_name()\nGOOGLE_CREDENTIALS_CACHE = {}\ngoogle_credentials_path = secretpath('secrets/google-secret.json')\n\ndef google_credentials(bucket = ''):\n  global PROJECT_NAME\n  global GOOGLE_CREDENTIALS_CACHE\n\n  if bucket in GOOGLE_CREDENTIALS_CACHE.keys():\n    return GOOGLE_CREDENTIALS_CACHE[bucket]\n\n  paths = [\n    secretpath('secrets/google-secret.json')\n  ]\n\n  if bucket:\n    paths = [ secretpath('secrets/{}-google-secret.json'.format(bucket)) ] + paths\n\n  google_credentials = None\n  project_name = PROJECT_NAME\n  for google_credentials_path in paths:\n    if os.path.exists(google_credentials_path):\n      google_credentials = service_account.Credentials \\\n        .from_service_account_file(google_credentials_path)\n      \n      with open(google_credentials_path, 'rt') as f:\n        project_name = json.loads(f.read())['project_id']\n      break\n\n  if google_credentials == None:\n    print(colorize('yellow', 'Using default Google credentials. There is no ~/.cloudvolume/secrets/google-secret.json set.'))  \n  else:\n    GOOGLE_CREDENTIALS_CACHE[bucket] = (project_name, google_credentials)\n\n  return project_name, google_credentials\n\nAWS_CREDENTIALS_CACHE = defaultdict(dict)\naws_credentials_path = secretpath('secrets/aws-secret.json')\ndef aws_credentials(bucket = '', service = 'aws'):\n  global AWS_CREDENTIALS_CACHE\n\n  if service == 's3':\n    service = 'aws'\n\n  if bucket in AWS_CREDENTIALS_CACHE.keys():\n    return AWS_CREDENTIALS_CACHE[bucket]\n\n  default_file_path = 'secrets/{}-secret.json'.format(service)\n\n  paths = [\n    secretpath(default_file_path)\n  ]\n\n  if bucket:\n    paths = [ secretpath('secrets/{}-{}-secret.json'.format(bucket, service)) ] + paths\n\n  aws_credentials = {}\n  aws_credentials_path = secretpath(default_file_path)\n  for aws_credentials_path in paths:\n    if os.path.exists(aws_credentials_path):\n      with open(aws_credentials_path, 'r') as f:\n        aws_credentials = json.loads(f.read())\n      break\n  \n  if not aws_credentials:\n    # did not find any secret json file, will try to find it in environment variables\n    if 'AWS_ACCESS_KEY_ID' in os.environ and 'AWS_SECRET_ACCESS_KEY' in os.environ:\n      aws_credentials = {\n        'AWS_ACCESS_KEY_ID': os.environ['AWS_ACCESS_KEY_ID'],\n        'AWS_SECRET_ACCESS_KEY': os.environ['AWS_SECRET_ACCESS_KEY'],\n      }\n    if 'AWS_DEFAULT_REGION' in os.environ:\n      aws_credentials['AWS_DEFAULT_REGION'] = os.environ['AWS_DEFAULT_REGION']\n\n  AWS_CREDENTIALS_CACHE[service][bucket] = aws_credentials\n  return aws_credentials\n    \n\nboss_credentials_path = secretpath('secrets/boss-secret.json')\nif os.path.exists(boss_credentials_path):\n  with open(boss_credentials_path, 'r') as f:\n    boss_credentials = json.loads(f.read())\nelse:\n  boss_credentials = ''\n\n\nchunkedgraph_credentials_path = secretpath('secrets/chunkedgraph-secret.json')\nif os.path.exists(chunkedgraph_credentials_path):\n  with open(chunkedgraph_credentials_path, 'r') as f:\n    chunkedgraph_credentials = json.loads(f.read())\nelse:\n  chunkedgraph_credentials = None"""
cloudvolume/server.py,0,"b'import os\n\ntry: \n  from http.server import BaseHTTPRequestHandler, HTTPServer\nexcept ImportError:\n  from SocketServer import TCPServer as HTTPServer\n  from BaseHTTPServer import BaseHTTPRequestHandler\n\nimport json\nfrom six.moves import range\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom cloudvolume.storage import Storage\nfrom cloudvolume.lib import Vec, Bbox, mkdir, save_images, yellow\nfrom cloudvolume.paths import ExtractedPath\n\nDEFAULT_PORT = 8080\n\ndef view(cloudpath, hostname=""localhost"", port=DEFAULT_PORT):\n  """"""Start a local web app on the given port that lets you explore this cutout.""""""\n  def handler(*args):\n    return ViewerServerHandler(cloudpath, *args)\n\n  myServer = HTTPServer((hostname, port), handler)\n  print(""Neuroglancer server listening to http://{}:{}"".format(hostname, port))\n  try:\n    myServer.serve_forever()\n  except KeyboardInterrupt:\n    # extra \\n to prevent display of ""^CContinuing""\n    print(""\\nContinuing program execution..."")\n  finally:\n    myServer.server_close()\n\nclass ViewerServerHandler(BaseHTTPRequestHandler):\n  def __init__(self, cloudpath, *args):\n    self.storage = Storage(cloudpath)\n    BaseHTTPRequestHandler.__init__(self, *args)\n\n  def __del__(self):\n    self.storage.kill_threads()\n\n  def do_GET(self):  \n    if self.path.find(\'..\') != -1:\n      self.send_error(403, ""Relative paths are not allowed."")\n      raise ValueError(""Relative paths are not allowed."")\n\n    path = self.path[1:]\n    data = self.storage.get_file(path)\n\n    if data is None:\n      self.send_error(404, \'/\' + path + "": Not Found"")\n      return \n\n    self.send_response(200)\n    self.serve_data(data)\n\n  def serve_data(self, data):\n    self.send_header(\'Content-type\', \'application/octet-stream\')\n    self.send_header(\'Access-Control-Allow-Origin\', \'*\')\n    self.send_header(\'Content-length\', str(len(data)))\n    self.end_headers()\n    self.wfile.write(data)\n'"
cloudvolume/sharedmemory.py,4,"b'from collections import defaultdict\nimport errno\nimport math\nimport mmap\nimport os\nimport sys\nimport time\n\nimport multiprocessing as mp\n\nfrom six.moves import range\nimport numpy as np\n\nfrom .lib import Bbox, Vec, mkdir\n\nSHM_DIRECTORY = \'/dev/shm/\'\nEMULATED_SHM_DIRECTORY = \'/tmp/cloudvolume-shm\'\n\nEMULATE_SHM = not os.path.isdir(SHM_DIRECTORY)\nPLATFORM_SHM_DIRECTORY = SHM_DIRECTORY if not EMULATE_SHM else EMULATED_SHM_DIRECTORY\n\nclass SharedMemoryReadError(Exception):\n  pass\n\nclass SharedMemoryAllocationError(Exception):\n  pass\n\ndef ndarray(shape, dtype, location, order=\'F\', readonly=False, lock=None, **kwargs):\n  """"""\n  Create a shared memory numpy array. \n  Lock is only necessary while doing multiprocessing on \n  platforms without /dev/shm type  shared memory as \n  filesystem emulation will be used instead.\n\n  Allocating the shared array requires cleanup on your part.\n  A shared memory file will be located at sharedmemory.PLATFORM_SHM_DIRECTORY + location\n  and must be unlinked when you\'re done. It will outlive the program.\n\n  You should also call .close() on the mmap file handle when done. However,\n  this is less of a problem because the operating system will close the\n  file handle on process termination.\n\n  Parameters:\n  shape: same as numpy.ndarray\n  dtype: same as numpy.ndarray\n  location: the shared memory filename \n  lock: (optional) multiprocessing.Lock\n\n  Returns: (mmap filehandle, shared ndarray)\n  """"""\n  if EMULATE_SHM:\n    return ndarray_fs(\n      shape, dtype, location, lock, \n      readonly, order, emulate_shm=True, **kwargs\n    )\n  return ndarray_shm(shape, dtype, location, readonly, order, **kwargs)\n\ndef ndarray_fs(\n    shape, dtype, location, lock, \n    readonly=False, order=\'F\', emulate_shm=False,\n    **kwargs\n  ):\n  """"""Emulate shared memory using the filesystem.""""""\n  dbytes = np.dtype(dtype).itemsize\n  nbytes = Vec(*shape).rectVolume() * dbytes\n\n  if emulate_shm:\n    directory = mkdir(EMULATED_SHM_DIRECTORY)\n    filename = os.path.join(directory, location)\n  else:\n    filename = location\n\n  if lock:\n    lock.acquire()\n\n  try:\n    allocate_shm_file(filename, nbytes, dbytes, readonly)\n  finally:\n    if lock:\n      lock.release()\n\n  with open(filename, \'r+b\') as f:\n    array_like = mmap.mmap(f.fileno(), 0) # map entire file\n  \n  renderbuffer = np.ndarray(buffer=array_like, dtype=dtype, shape=shape, order=order, **kwargs)\n  renderbuffer.setflags(write=(not readonly))\n  return array_like, renderbuffer\n\ndef allocate_shm_file(filename, nbytes, dbytes, readonly):\n  exists = os.path.exists(filename)\n  size = 0 if not exists else os.path.getsize(filename)\n\n  if readonly and not exists:\n    raise SharedMemoryReadError(filename + "" has not been allocated. Requested "" + str(nbytes) + "" bytes."")\n  elif readonly and size != nbytes:\n    raise SharedMemoryReadError(""{} exists, but the allocation size ({} bytes) does not match the request ({} bytes)."".format(\n      filename, size, nbytes\n    ))\n\n  if exists: \n    if size > nbytes:\n      with open(filename, \'wb\') as f:\n        os.ftruncate(f.fileno(), nbytes)\n    elif size < nbytes:\n      # too small? just remake it below\n      os.unlink(filename) \n\n  exists = os.path.exists(filename)\n\n  if not exists:\n    # Previously we were writing out real files full of zeros, \n    # but a) that takes forever and b) modern OSes support sparse\n    # files (i.e. gigabytes of zeros that take up only a few real bytes).\n    #\n    # The following should take advantage of this functionality and be faster.\n    # It should work on Python 2.7 Unix, and Python 3.5+ on Unix and Windows.\n    #\n    # References:\n    #   https://stackoverflow.com/questions/8816059/create-file-of-particular-size-in-python\n    #   https://docs.python.org/3/library/os.html#os.ftruncate\n    #   https://docs.python.org/2/library/os.html#os.ftruncate\n    #\n    with open(filename, \'wb\') as f:\n      os.ftruncate(f.fileno(), nbytes)\n\ndef ndarray_shm(shape, dtype, location, readonly=False, order=\'F\', **kwargs):\n  """"""Create a shared memory numpy array. Requires /dev/shm to exist.""""""\n  import posix_ipc\n  from posix_ipc import O_CREAT\n  import psutil\n\n  nbytes = Vec(*shape).rectVolume() * np.dtype(dtype).itemsize\n  available = psutil.virtual_memory().available\n\n  preexisting = 0\n  # This might only work on Ubuntu\n  shmloc = os.path.join(SHM_DIRECTORY, location)\n  if os.path.exists(shmloc):\n    preexisting = os.path.getsize(shmloc)\n  elif readonly:\n    raise SharedMemoryReadError(shmloc + "" has not been allocated. Requested "" + str(nbytes) + "" bytes."")\n\n  if readonly and preexisting != nbytes:\n    raise SharedMemoryReadError(""{} exists, but the allocation size ({} bytes) does not match the request ({} bytes)."".format(\n      shmloc, preexisting, nbytes\n    ))\n\n  if (nbytes - preexisting) > available:\n    overallocated = nbytes - preexisting - available\n    overpercent = (100 * overallocated / (preexisting + available))\n    raise SharedMemoryAllocationError(""""""\n      Requested more memory than is available. \n\n      Shared Memory Location:  {}\n\n      Shape:                   {}\n      Requested Bytes:         {} \n      \n      Available Bytes:         {} \n      Preexisting Bytes*:      {} \n\n      Overallocated Bytes*:    {} (+{:.2f}%)\n\n      * Preexisting is only correct on linux systems that support /dev/shm/"""""" \\\n        .format(location, shape, nbytes, available, preexisting, overallocated, overpercent))\n\n  # This might seem like we\'re being ""extra safe"" but consider\n  # a threading condition where the condition of the shared memory\n  # was adjusted between the check above and now. Better to make sure\n  # that we don\'t accidently change anything if readonly is set.\n  flags = 0 if readonly else O_CREAT \n  size = 0 if readonly else int(nbytes) \n\n  try:\n    shared = posix_ipc.SharedMemory(location, flags=flags, size=size)\n    array_like = mmap.mmap(shared.fd, shared.size)\n    os.close(shared.fd)\n    renderbuffer = np.ndarray(buffer=array_like, dtype=dtype, shape=shape, order=order, **kwargs)\n  except OSError as err:\n    if err.errno == errno.ENOMEM: # Out of Memory\n      posix_ipc.unlink_shared_memory(location)      \n    raise\n\n  renderbuffer.setflags(write=(not readonly))\n  return array_like, renderbuffer\n\ndef unlink(location):\n  if EMULATE_SHM:\n    return unlink_fs(location)\n  return unlink_shm(location)\n\ndef unlink_shm(location):\n  import posix_ipc\n  try:\n    posix_ipc.unlink_shared_memory(location)\n  except posix_ipc.ExistentialError:\n    return False\n  return True\n\ndef unlink_fs(location):\n  directory = mkdir(EMULATED_SHM_DIRECTORY)\n  try:\n    filename = os.path.join(directory, location)\n    os.unlink(filename)\n    return True\n  except OSError:\n    return False\n'"
cloudvolume/skeleton.py,85,"b'from collections import defaultdict\nimport copy\nimport datetime\nimport re\nimport os\n\ntry:\n  from StringIO import cStringIO as BytesIO\nexcept ImportError:\n  from io import BytesIO\n\nimport numpy as np\nimport struct\n\nfrom . import lib\nfrom .exceptions import (\n  SkeletonDecodeError, SkeletonEncodeError, \n  SkeletonUnassignedEdgeError, SkeletonTransformError,\n  SkeletonAttributeMixingError\n)\nfrom .lib import red, Bbox\nfrom .storage import Storage, SimpleStorage\n\nIDENTITY = np.array([\n  [1, 0, 0, 0],\n  [0, 1, 0, 0],\n  [0, 0, 1, 0],\n], dtype=np.float32)\n\nclass Skeleton(object):\n  """"""\n  A stick figure representation of a 3D object. \n\n  vertices: [[x,y,z], ...] float32\n  edges: [[v1,v2], ...] uint32\n  radii: [r1,r2,...] float32 distance from vertex to nearest boudary\n  vertex_types: [t1,t2,...] uint8 SWC vertex types\n  segid: numerical ID\n  transform: 3x4 scaling and translation matrix (ie homogenous coordinates) \n    that represents the transformaton from voxel to physical coordinates.\n    \n    Example Identity Matrix:\n    [\n      [1, 0, 0, 0],\n      [0, 1, 0, 0],\n      [0, 0, 1, 0]\n    ]\n\n  space: \'voxel\', \'physical\', or user choice (but other choices \n    make .physical_space() and .voxel_space() stop working as they\n    become meaningless.)\n  \n  extra_attributes: You can specify additional per vertex\n    data attributes (the most common are radii and vertex_type) \n    that are present in reading Precomputed binary skeletons using\n    the following format:\n    [\n        {\n          ""id"": ""radius"",\n          ""data_type"": ""uint8"",\n          ""num_components"": 1,\n        }\n    ]\n\n    These attributes will become object properties. i.e. skel.radius\n\n    Note that for backwards compatibility, skel.radius is treated \n    specially and is synonymous with skel.radii.\n  """"""\n  def __init__(self, \n    vertices=None, edges=None, \n    radii=None, vertex_types=None, \n    segid=None, transform=None,\n    space=\'voxel\', extra_attributes=None\n  ):\n    self.id = segid\n    self.space = space\n\n    if vertices is None:\n      self.vertices = np.array([[]], dtype=np.float32)\n    elif type(vertices) is list:\n      self.vertices = np.array(vertices, dtype=np.float32)\n    else:\n      self.vertices = vertices.astype(np.float32)\n\n    if edges is None:\n      self.edges = np.array([[]], dtype=np.uint32)\n    elif type(edges) is list:\n      self.edges = np.array(edges, dtype=np.uint32)\n    else:\n      self.edges = edges.astype(np.uint32)\n\n    if radii is None:\n      self.radius = -1 * np.ones(shape=self.vertices.shape[0], dtype=np.float32)\n    elif type(radii) is list:\n      self.radius = np.array(radii, dtype=np.float32)\n    else:\n      self.radius = radii\n\n    if vertex_types is None:\n      # 0 = undefined in SWC (http://research.mssm.edu/cnic/swc.html)\n      self.vertex_types = np.zeros(shape=self.vertices.shape[0], dtype=np.uint8)\n    elif type(vertex_types) is list:\n      self.vertex_types = np.array(vertex_types, dtype=np.uint8)\n    else:\n      self.vertex_types = vertex_types.astype(np.uint8)\n\n    if extra_attributes is None:\n      self.extra_attributes = self._default_attributes()\n    else:\n      self.extra_attributes = extra_attributes\n\n    if transform is None:\n      self.transform = np.copy(IDENTITY)\n    else:\n      self.transform = np.array(transform).reshape( (3, 4) )\n\n  @classmethod\n  def _default_attributes(self):\n    return [\n      {\n        ""id"": ""radius"",\n        ""data_type"": ""float32"",\n        ""num_components"": 1,\n      }, \n      {\n        ""id"": ""vertex_types"",\n        ""data_type"": ""uint8"",\n        ""num_components"": 1,\n      }\n    ]\n\n  def _check_space(self):\n    if self.space not in (\'physical\', \'voxel\'):\n      raise SkeletonTransformError(\n        """"""\n        Loss of coordinate frame information. If the space is not \'physical\' or \'voxel\',\n        the meaning of applying this transform matrix is unknown.\n\n        space: {}\n        """""".format(self.space)\n      )\n\n  def physical_space(self, copy=True):\n    """"""\n    Convert skeleton vertices into a physical space \n    representation if it\'s not already there.\n\n    copy: if False, don\'t copy if already in the correct\n      coordinate frame.\n\n    Returns: skeleton in physical coordinates\n    """"""\n    self._check_space()\n\n    if self.space == \'physical\':\n      if copy:\n        return self.clone()\n      else:\n        return self\n\n    skel = self.clone()\n    skel.apply_transform()\n    skel.space = \'physical\'\n    return skel\n\n  def voxel_space(self, copy=True):\n    """"""\n    Convert skeleton vertices into a voxel space \n    representation if it\'s not already there.\n\n    copy: if False, don\'t copy if already in the correct\n      coordinate frame.\n\n    Returns: skeleton in voxel coordinates\n    """"""\n    self._check_space()\n\n    if self.space == \'voxel\':\n      if copy:\n        return self.clone()\n      else:\n        return self\n\n    skel = self.clone()\n    skel.apply_inverse_transform()\n    skel.space = \'voxel\'\n    return skel\n\n  @property\n  def transform(self):\n    return self._transform\n\n  @transform.setter \n  def transform(self, val):\n    self._transform = np.array(val, dtype=np.float32).reshape( (3,4) )\n\n  def transform_vertices(self, vertices, transform):\n    verts = np.append(\n      vertices,\n      np.ones( (vertices.shape[0], 1), dtype=vertices.dtype), \n      axis=1\n    )\n    verts = transform.dot(verts.T).T\n    return verts[:,0:3]    \n\n  def apply_transform(self):\n    self.vertices = self.transform_vertices(self.vertices, self.transform)\n\n  def apply_inverse_transform(self, transform=None):\n    if transform is None:\n      transform = self.transform\n\n    verts = np.append(\n      self.vertices, \n      np.ones( (self.vertices.shape[0], 1), dtype=self.vertices.dtype), \n      axis=1\n    )\n    \n    transform = np.zeros( (3,4), dtype=np.float32 )\n    transform[:3,:3] = np.linalg.inv(self.transform[:3,:3])\n    transform[:,3] = -self.transform[:,3]\n\n    verts = transform.dot(verts.T).T\n    self.vertices = verts[:,0:3]    \n\n  @property \n  def radii(self):\n    return self.radius\n\n  @radii.setter \n  def radii(self, val):\n    self.radius = val\n\n  @classmethod\n  def from_path(kls, vertices):\n    """"""\n    Given an Nx3 array of vertices that constitute a single path, \n    generate a skeleton with appropriate edges.\n    """"""\n    if vertices.shape[0] == 0:\n      return Skeleton()\n\n    skel = Skeleton(vertices)\n    edges = np.zeros(shape=(skel.vertices.shape[0] - 1, 2), dtype=np.uint32)\n    edges[:,0] = np.arange(skel.vertices.shape[0] - 1)\n    edges[:,1] = np.arange(1, skel.vertices.shape[0])\n    skel.edges = edges\n    return skel\n\n  @classmethod\n  def simple_merge(kls, skeletons):\n    """"""\n    Simple concatenation of skeletons into one object \n    without adding edges between them.\n    """"""\n    if len(skeletons) == 0:\n      return Skeleton()\n\n    if type(skeletons[0]) is np.ndarray:\n      skeletons = [ skeletons ]\n\n    ct = 0\n    edges = []\n    for skel in skeletons:\n      edge = skel.edges + ct\n      edges.append(edge)\n      ct += skel.vertices.shape[0]\n\n    skel = Skeleton(\n      vertices=np.concatenate([ skel.vertices for skel in skeletons ], axis=0),\n      edges=np.concatenate(edges, axis=0),\n      segid=skeletons[0].id,\n    )\n\n    if len(skeletons) == 0:\n      return skel\n\n    first_extra_attr = skeletons[0].extra_attributes\n    for skl in skeletons[1:]:\n      if first_extra_attr != skl.extra_attributes:\n        raise SkeletonAttributeMixingError(""""""\n          The skeletons were unable to be merged because\n          the extended vertex attributes were not uniformly\n          defined.\n\n          Template being matched against:\n          {}\n\n          Non-matching skeleton:\n          {}\n        """""".format(first_extra_attr, skl.extra_attributes))\n\n    for attr in skeletons[0].extra_attributes:\n      setattr(skel, attr[\'id\'], np.concatenate([\n        getattr(skl, attr[\'id\']) for skl in skeletons\n      ], axis=0))\n\n    return skel\n\n  def terminals(self):\n    """"""\n    Returns vertex indices that correspond to terminal \n    nodes of the skeleton defined as having only one edge.\n    """"""\n    unique_nodes, unique_counts = np.unique(self.edges, return_counts=True)\n    return unique_nodes[ unique_counts == 1 ]\n\n  def branches(self):\n    """"""\n    Returns vertex indices that correspond to branch points\n    in the skeleton defined as having three or more edges.\n    """"""\n    unique_nodes, unique_counts = np.unique(self.edges, return_counts=True)\n    return unique_nodes[ unique_counts >= 3 ]\n\n  def merge(self, skel):\n    """"""Combine with an additional skeleton and consolidate.""""""\n    return Skeleton.simple_merge((self, skel)).consolidate()\n\n  def empty(self):\n    return self.vertices.size == 0 or self.edges.size == 0\n\n  def encode(self):\n    print(lib.yellow(\n      ""WARNING: Skeleton.encode() is deprecated in favor of Skeleton.to_precomputed() and will be removed in a future release.""\n    ))\n    return self.to_precomputed()\n\n  def decode(self, binary):\n    print(lib.yellow(\n      ""WARNING: Skeleton.decode(bytes) is deprecated in favor of Skeleton.from_precomputed(bytes) and will be removed in a future release.""\n    ))\n    return self.from_precomputed(binary)\n\n  def to_precomputed(self):\n    edges = self.edges.astype(np.uint32)\n    vertices = self.vertices.astype(np.float32)\n    \n    result = BytesIO()\n\n    # Write number of positions and edges as first two uint32s\n    result.write(struct.pack(\'<II\', vertices.size // 3, edges.size // 2))\n    result.write(vertices.tobytes(\'C\'))\n    result.write(edges.tobytes(\'C\'))\n\n    def writeattr(attr, dtype, text):\n      if attr is None:\n        return\n\n      attr = attr.astype(dtype)\n\n      if attr.shape[0] != vertices.shape[0]:\n        raise SkeletonEncodeError(""Number of {} {} ({}) must match the number of vertices ({})."".format(\n          dtype, text, attr.shape[0], vertices.shape[0]\n        ))\n      \n      result.write(attr.tobytes(\'C\'))\n\n    for attr in self.extra_attributes:\n      arr = getattr(self, attr[\'id\'])\n      writeattr(arr, np.dtype(attr[\'data_type\']), attr[\'id\'])\n\n    return result.getvalue()\n\n  @classmethod\n  def from_precomputed(kls, skelbuf, segid=None, vertex_attributes=None):\n    """"""\n    Convert a buffer into a Skeleton object.\n\n    Format:\n    num vertices (Nv) (uint32)\n    num edges (Ne) (uint32)\n    XYZ x Nv (float32)\n    edge x Ne (2x uint32)\n\n    Default Vertex Attributes:\n\n      radii x Nv (optional, float32)\n      vertex_type x Nv (optional, req radii, uint8) (SWC definition)\n\n    Specify your own:\n\n    vertex_attributes = [\n      {\n        \'id\': name of attribute,\n        \'num_components\': int,\n        \'data_type\': dtype,\n      },\n    ]\n\n    More documentation: \n    https://github.com/seung-lab/cloud-volume/wiki/Advanced-Topic:-Skeletons-and-Point-Clouds\n    """"""\n    if len(skelbuf) < 8:\n      raise SkeletonDecodeError(""{} bytes is fewer than needed to specify the number of verices and edges."".format(len(skelbuf)))\n\n    num_vertices, num_edges = struct.unpack(\'<II\', skelbuf[:8])\n    min_format_length = 8 + 12 * num_vertices + 8 * num_edges\n\n    if len(skelbuf) < min_format_length:\n      raise SkeletonDecodeError(""The input skeleton was {} bytes but the format requires {} bytes."".format(\n        len(skelbuf), min_format_length\n      ))\n\n    vstart = 2 * 4 # two uint32s in\n    vend = vstart + num_vertices * 3 * 4 # float32s\n    vertbuf = skelbuf[ vstart : vend ]\n\n    estart = vend\n    eend = estart + num_edges * 4 * 2 # 2x uint32s\n\n    edgebuf = skelbuf[ estart : eend ]\n\n    vertices = np.frombuffer(vertbuf, dtype=\'<f4\').reshape( (num_vertices, 3) )\n    edges = np.frombuffer(edgebuf, dtype=\'<u4\').reshape( (num_edges, 2) )\n\n    skeleton = Skeleton(vertices, edges, segid=segid)\n\n    if len(skelbuf) == min_format_length:\n      return skeleton\n\n    if vertex_attributes is None:\n      vertex_attributes = kls._default_attributes()\n\n    start = eend\n    end = -1\n    for attr in vertex_attributes:\n      num_components = int(attr[\'num_components\'])\n      data_type = np.dtype(attr[\'data_type\'])\n      end = start + num_vertices * num_components * data_type.itemsize\n      attrbuf = np.frombuffer(skelbuf[start : end], dtype=data_type)\n\n      if num_components > 1:\n        attrbuf = attrbuf.reshape( (num_vertices, num_components) )\n\n      setattr(skeleton, attr[\'id\'], attrbuf)\n      start = end\n\n    skeleton.extra_attributes = vertex_attributes\n\n    return skeleton\n\n  @classmethod\n  def equivalent(kls, first, second):\n    """"""\n    Tests that two skeletons are the same in form not merely that\n    their array contents are exactly the same. This test can be\n    made more sophisticated. \n    """"""\n    if first.empty() and second.empty():\n      return True\n    elif first.vertices.shape[0] != second.vertices.shape[0]:\n      return False\n    elif first.edges.shape[0] != second.edges.shape[0]:\n      return False\n\n    EPSILON = 1e-7\n\n    vertex1, inv1 = np.unique(first.vertices, axis=0, return_inverse=True)\n    vertex2, inv2 = np.unique(second.vertices, axis=0, return_inverse=True)\n\n    vertex_match = np.all(np.abs(vertex1 - vertex2) < EPSILON)\n    if not vertex_match:\n      return False\n\n    remapping = {}\n    for i in range(len(inv1)):\n      remapping[inv1[i]] = inv2[i]\n    remap = np.vectorize(lambda idx: remapping[idx])\n\n    edges1 = np.sort(np.unique(first.edges, axis=0), axis=1)\n    edges1 = edges1[np.lexsort(edges1[:,::-1].T)]\n\n    edges2 = remap(second.edges)\n    edges2 = np.sort(np.unique(edges2, axis=0), axis=1)\n    edges2 = edges2[np.lexsort(edges2[:,::-1].T)]\n    edges_match = np.all(edges1 == edges2)\n\n    if not edges_match:\n      return False\n\n    second_verts = {}\n    for i, vert in enumerate(second.vertices):\n      second_verts[tuple(vert)] = i\n    \n    attrs = [ attr[\'id\'] for attr in first.extra_attributes ]\n    for attr in attrs:\n      buf1 = getattr(first, attr)\n      buf2 = getattr(second, attr)\n      if len(buf1) != len(buf2):\n        return False\n\n      for i in range(len(buf1)):\n        i2 = second_verts[tuple(first.vertices[i])]\n        if buf1[i] != buf2[i2]:\n          return False\n\n    return True\n\n  def crop(self, bbox):\n    """"""\n    Crop away all vertices and edges that lie outside of the given bbox.\n    The edge counts as inside.\n\n    Returns: new Skeleton\n    """"""\n    skeleton = self.clone()\n    bbox = Bbox.create(bbox)\n\n    if skeleton.empty():\n      return skeleton\n\n    nodes_valid_mask = np.array(\n      [ bbox.contains(vtx) for vtx in skeleton.vertices ], dtype=np.bool\n    )\n    nodes_valid_idx = np.where(nodes_valid_mask)[0]\n\n    # Set invalid vertices to be duplicates\n    # so they\'ll be removed during consolidation\n    if nodes_valid_idx.shape[0] == 0:\n      return Skeleton()\n\n    first_node = nodes_valid_idx[0]\n    skeleton.vertices[~nodes_valid_mask] = skeleton.vertices[first_node]\n  \n    edges_valid_mask = np.isin(skeleton.edges, nodes_valid_idx)\n    edges_valid_idx = edges_valid_mask[:,0] * edges_valid_mask[:,1] \n    skeleton.edges = skeleton.edges[edges_valid_idx,:]\n    return skeleton.consolidate()\n\n  def consolidate(self, remove_disconnected_vertices=True):\n    """"""\n    Remove duplicate vertices and edges from this skeleton without\n    side effects.\n\n    Optional:\n      remove_disconnected_vertices: delete vertices that have no edges\n        associated with them. This does not preserve index order.\n\n    Returns: new consolidated Skeleton \n    """"""\n    nodes = self.vertices\n    edges = self.edges \n\n    if self.empty():\n      return Skeleton(segid=self.id)\n    \n    eff_vertices, uniq_idx, idx_representative = np.unique(\n      nodes, axis=0, return_index=True, return_inverse=True\n    )\n\n    eff_edges = idx_representative[ edges ]\n    eff_edges = np.sort(eff_edges, axis=1) # sort each edge [2,1] => [1,2]\n    eff_edges = eff_edges[np.lexsort(eff_edges[:,::-1].T)] # Sort rows \n    eff_edges = np.unique(eff_edges, axis=0)\n    eff_edges = eff_edges[ eff_edges[:,0] != eff_edges[:,1] ] # remove trivial loops\n\n    skel = Skeleton(eff_vertices, eff_edges, segid=self.id)\n\n    for attr in self.extra_attributes:\n      name = attr[\'id\']\n      buf = getattr(self, name)\n      eff_name = buf[ uniq_idx ]\n      setattr(skel, name, eff_name)\n\n    if remove_disconnected_vertices:\n      return skel.remove_disconnected_vertices()\n\n    return skel\n\n  def remove_disconnected_vertices(self):\n    """"""\n    Delete vertices that have no edges associated with them. \n    This does not preserve index order.\n\n    Returns: new Skeleton\n    """"""\n    if self.empty():\n      return Skeleton(segid=self.id)\n\n    idx_map = {}\n    for i, vert in enumerate(self.vertices):\n      idx_map[tuple(vert)] = i\n\n    connected_verts = np.unique(self.vertices[ self.edges.flatten() ], axis=0)\n\n    edge_map = np.zeros( (len(self.vertices),), dtype=self.edges.dtype)\n    vertex_remap = np.zeros( (len(self.vertices),), dtype=np.int32) - 1\n    for i, vert in enumerate(connected_verts):\n      reverse_idx = idx_map[tuple(vert)]\n      edge_map[reverse_idx] = i\n      vertex_remap[i] = reverse_idx\n\n    edges = np.sort(edge_map[self.edges], axis=1)\n    vertex_remap = vertex_remap[ vertex_remap > -1 ]\n\n    skel = Skeleton(connected_verts, edges, segid=self.id)\n\n    if len(self.extra_attributes) == 0:\n      return skel\n\n    for attr in self.extra_attributes:\n      name = attr[\'id\']\n      self_buf = getattr(self, name)\n      skel_buf = self_buf[vertex_remap]\n      setattr(skel, name, skel_buf)\n        \n    return skel\n\n  def clone(self):\n    vertices = np.copy(self.vertices)\n    edges = np.copy(self.edges)\n    radii = np.copy(self.radii)\n    vertex_types = np.copy(self.vertex_types)\n\n    skel = Skeleton(\n      vertices, edges, radii, vertex_types, \n      segid=self.id, \n      space=self.space, \n      extra_attributes=self.extra_attributes,\n      transform=np.copy(self.transform)\n    )\n    for attr in skel.extra_attributes:\n      setattr(skel, attr[\'id\'], np.copy(getattr(self, attr[\'id\'])))\n\n    return skel\n\n  def cable_length(self):\n    """"""\n    Returns cable length of connected skeleton vertices in the same\n    metric that this volume uses (typically nanometers).\n    """"""\n    skel = self.physical_space(copy=False)\n\n    v1 = skel.vertices[skel.edges[:,0]]\n    v2 = skel.vertices[skel.edges[:,1]]\n\n    delta = (v2 - v1)\n    delta *= delta\n    dist = np.sum(delta, axis=1)\n    dist = np.sqrt(dist)\n\n    return np.sum(dist)\n\n  def downsample(self, factor):\n    """"""\n    Compute a downsampled version of the skeleton by striding while \n    preserving endpoints.\n\n    factor: stride length for downsampling the saved skeleton paths.\n\n    Returns: downsampled Skeleton\n    """"""\n    if int(factor) != factor or factor < 1:\n      raise ValueError(""Argument `factor` must be a positive integer greater than or equal to 1. Got: <{}>({})"", type(factor), factor)\n\n    if factor == 1:\n      return self.clone()\n\n    paths = self.interjoint_paths()\n\n    for i, path in enumerate(paths):\n      paths[i] = np.concatenate(\n        (path[0::factor, :], path[-1:, :]) # preserve endpoints\n      )\n\n    ds_skel = Skeleton.simple_merge(\n      [ Skeleton.from_path(path) for path in paths ]\n    ).consolidate()\n    ds_skel.id = self.id\n\n    # TODO: I\'m sure this could be sped up if need be.\n    index = {}\n    for i, vert in enumerate(self.vertices):\n      vert = tuple(vert)\n      index[vert] = i\n\n    bufs = [ getattr(ds_skel, attr[\'id\']) for attr in ds_skel.extra_attributes ]\n    orig_bufs = [ getattr(self, attr[\'id\']) for attr in ds_skel.extra_attributes ]\n\n    for i, vert in enumerate(ds_skel.vertices):\n      reverse_i = index[tuple(vert)]\n      for buf, buf_rev in zip(bufs, orig_bufs):\n        buf[i] = buf_rev[reverse_i]\n    \n    return ds_skel\n\n  def _single_tree_paths(self, tree, return_indices):\n    """"""Get all traversal paths from a single tree.""""""\n    skel = tree.consolidate()\n\n    tree = defaultdict(list)\n\n    for edge in skel.edges:\n      svert = edge[0]\n      evert = edge[1]\n      tree[svert].append(evert)\n      tree[evert].append(svert)\n\n    def dfs(path, visited):\n      paths = []\n      stack = [ (path, visited) ]\n      \n      while stack:\n        path, visited = stack.pop(0)\n\n        vertex = path[-1]\n        children = tree[vertex]\n        \n        visited[vertex] = True\n\n        children = [ child for child in children if not visited[child] ]\n\n        if len(children) == 0:\n          paths.append(path)\n\n        for child in children:\n          stack.append( \n            (path + [child], copy.deepcopy(visited))\n          )\n\n      return paths\n      \n    root = skel.edges[0,0]\n    paths = dfs([root], defaultdict(bool))\n\n    root = np.argmax([ len(_) for _ in paths ])\n    root = paths[root][-1]\n  \n    paths = dfs([ root ], defaultdict(bool))\n\n    if return_indices:\n      return [ np.flip(path) for path in paths ]\n\n    return [ np.flip(skel.vertices[path], axis=0) for path in paths ]\n\n  def paths(self, return_indices=False):\n    """"""\n    Assuming the skeleton is structured as a single tree, return a \n    list of all traversal paths across all components. For each component, \n    start from the first vertex, find the most distant vertex by \n    hops and set that as the root. Then use depth first traversal \n    to produce paths.\n\n    Returns: [ [(x,y,z), (x,y,z), ...], path_2, path_3, ... ]\n    """"""\n    paths = []\n    for tree in self.components():\n      paths += self._single_tree_paths(tree, return_indices=return_indices)\n    return paths\n\n  def _single_tree_interjoint_paths(self, skeleton, return_indices):\n    vertices = skeleton.vertices\n    edges = skeleton.edges\n\n    unique_nodes, unique_counts = np.unique(edges, return_counts=True)\n    terminal_nodes = unique_nodes[ unique_counts == 1 ]\n    branch_nodes = set(unique_nodes[ unique_counts >= 3 ])\n    \n    critical_points = set(terminal_nodes)\n    critical_points.update(branch_nodes)\n\n    tree = defaultdict(set)\n\n    for e1, e2 in edges:\n      tree[e1].add(e2)\n      tree[e2].add(e1)\n\n    # The below depth first search would be\n    # more elegantly implemented as recursion,\n    # but it quickly blows the stack, mandating\n    # an iterative implementation.\n\n    paths = []\n\n    stack = [ terminal_nodes[0] ]\n    criticals = [ terminal_nodes[0] ]\n    # Saving the path stack is memory intensive\n    # There might be a way to do it more linearly\n    # via a DFS rather than BFS strategy.\n    path_stack = [ [] ] \n    \n    visited = defaultdict(bool)\n\n    while stack:\n      node = stack.pop()\n      root = criticals.pop() # ""root"" is used v. loosely here\n      path = path_stack.pop()\n\n      path.append(node)\n      visited[node] = True\n\n      if node != root and node in critical_points:\n        paths.append(path)\n        path = [ node ]\n        root = node\n\n      for child in tree[node]:\n        if not visited[child]:\n          stack.append(child)\n          criticals.append(root)\n          path_stack.append(list(path))\n\n    if return_indices:\n      return paths\n\n    return [ vertices[path] for path in paths ]\n\n  def interjoint_paths(self, return_indices=False):\n    """"""\n    Returns paths between the adjacent critical points\n    in the skeleton, where a critical point is the set of\n    terminal and branch points.\n    """"""\n    paths = []\n    for tree in self.components():\n      subpaths = self._single_tree_interjoint_paths(\n        tree, return_indices=return_indices\n      )\n      paths.extend(subpaths)\n\n    return paths\n\n  def _compute_components(self):\n    skel = self.consolidate(remove_disconnected_vertices=False)\n    if skel.edges.size == 0:\n      return skel, []\n\n    index = defaultdict(set)\n    visited = defaultdict(bool)\n    for e1, e2 in skel.edges:\n      index[e1].add(e2)\n      index[e2].add(e1)\n\n    def extract_component(start):\n      edge_list = []\n      stack = [ start ]\n      parents = [ -1 ]\n\n      while stack:\n        node = stack.pop()\n        parent = parents.pop()\n\n        if node < parent:\n          edge_list.append( (node, parent) )\n        else:\n          edge_list.append( (parent, node) )\n\n        if visited[node]:\n          continue\n\n        visited[node] = True\n        \n        for child in index[node]:\n          stack.append(child)\n          parents.append(node)\n\n      return edge_list[1:]\n\n    forest = []\n    for edge in np.unique(skel.edges.flatten()):\n      if visited[edge]:\n        continue\n\n      forest.append(\n        extract_component(edge)\n      )\n\n    return skel, forest\n  \n  def components(self):\n    """"""\n    Extract connected components from graph. \n    Useful for ensuring that you\'re working with a single tree.\n\n    Returns: [ Skeleton, Skeleton, ... ]\n    """"""\n    skel, forest = self._compute_components()\n\n    if len(forest) == 0:\n      return []\n    elif len(forest) == 1:\n      return [ skel ]\n\n    orig_verts = { tuple(coord): i for i, coord in enumerate(skel.vertices) }      \n\n    skeletons = []\n    for edge_list in forest:\n      edge_list = np.array(edge_list, dtype=np.uint32)\n      edge_list = np.unique(edge_list, axis=0)\n      vert_idx = np.unique(edge_list.flatten())\n      vert_list = skel.vertices[vert_idx]\n      radii = skel.radii[vert_idx]\n      vtypes = skel.vertex_types[vert_idx]\n\n      new_verts = { orig_verts[tuple(coord)]: i for i, coord in enumerate(vert_list) }\n\n      edge_vector_map = np.vectorize(lambda x: new_verts[x])\n      edge_list = edge_vector_map(edge_list)\n\n      skeletons.append(\n        Skeleton(vert_list, edge_list, radii, vtypes, skel.id)\n      )\n\n    return skeletons\n\n  @classmethod\n  def from_swc(self, swcstr):\n    """"""\n    The SWC format was first defined in \n    \n    R.C Cannona, D.A Turner, G.K Pyapali, H.V Wheal. \n    ""An on-line archive of reconstructed hippocampal neurons"".\n    Journal of Neuroscience Methods\n    Volume 84, Issues 1-2, 1 October 1998, Pages 49-54\n    doi: 10.1016/S0165-0270(98)00091-0\n\n    This website is also helpful for understanding the format:\n\n    https://web.archive.org/web/20180423163403/http://research.mssm.edu/cnic/swc.html\n\n    Returns: Skeleton\n    """"""\n    lines = swcstr.split(""\\n"")\n    while len(lines) and (lines[0] == \'\' or re.match(r\'[#\\s]\', lines[0][0])):\n      l = lines.pop(0)\n\n    if len(lines) == 0:\n      return Skeleton()\n\n    vertices = []\n    edges = []\n    radii = []\n    vertex_types = []\n\n    vertex_index = {}\n    label_index = {}\n    parents = {}\n    N = 0\n\n    for line in lines:\n      if line.replace(r""\\s"", \'\') == \'\':\n        continue\n      (vid, vtype, x, y, z, radius, parent_id) = line.split("" "")\n      \n      coord = tuple([ float(_) for _ in (x,y,z) ])\n      vid = int(vid)\n      parent_id = int(parent_id)\n\n      vertex_index[coord] = N\n      label_index[vid] = coord\n      parents[N] = parent_id\n\n      vertices.append(coord)\n      vertex_types.append(int(vtype))\n\n      try:\n        radius = float(radius)\n      except ValueError:\n        radius = -1 # e.g. radius = NA or N/A\n\n      radii.append(radius)\n\n      N += 1\n\n    for i, parent_id in parents.items():\n      if parent_id < 0:\n        continue\n      \n      edges.append( (i, vertex_index[label_index[parent_id]]) )\n\n    return Skeleton(vertices, edges, radii, vertex_types)\n\n  def to_swc(self, contributors=""""):\n    """"""\n    Prototype SWC file generator. \n\n    The SWC format was first defined in \n    \n    R.C Cannona, D.A Turner, G.K Pyapali, H.V Wheal. \n    ""An on-line archive of reconstructed hippocampal neurons"".\n    Journal of Neuroscience Methods\n    Volume 84, Issues 1-2, 1 October 1998, Pages 49-54\n    doi: 10.1016/S0165-0270(98)00091-0\n\n    This website is also helpful for understanding the format:\n\n    https://web.archive.org/web/20180423163403/http://research.mssm.edu/cnic/swc.html\n\n    Returns: swc as a string\n    """"""\n    from . import __version__\n    swc_header = """"""# ORIGINAL_SOURCE CloudVolume {}\n# CREATURE \n# REGION\n# FIELD/LAYER\n# TYPE\n# CONTRIBUTOR {}\n# REFERENCE\n# RAW \n# EXTRAS \n# SOMA_AREA\n# SHINKAGE_CORRECTION \n# VERSION_NUMBER \n# VERSION_DATE {}\n# SCALE 1.0 1.0 1.0\n\n"""""".format(\n      __version__, \n      contributors,\n      datetime.datetime.utcnow().isoformat()\n    )\n\n    def generate_swc(skel, offset):\n      if skel.edges.size == 0:\n        return """"\n\n      index = defaultdict(set)\n      visited = defaultdict(bool)\n      for e1, e2 in skel.edges:\n        index[e1].add(e2)\n        index[e2].add(e1)\n\n      stack = [ skel.edges[0,0] ]\n      parents = [ -1 ]\n\n      swc = """"\n\n      while stack:\n        node = stack.pop()\n        parent = parents.pop()\n\n        if visited[node]:\n          continue\n\n        swc += ""{n} {T} {x:0.6f} {y:0.6f} {z:0.6f} {R:0.6f} {P}\\n"".format(\n          n=(node + 1 + offset),\n          T=skel.vertex_types[node],\n          x=skel.vertices[node][0],\n          y=skel.vertices[node][1],\n          z=skel.vertices[node][2],\n          R=skel.radii[node],\n          P=parent if parent == -1 else (parent + 1 + offset),\n        )\n\n        visited[node] = True\n        \n        for child in index[node]:\n          stack.append(child)\n          parents.append(node)\n\n      return swc\n\n    skels = self.remove_disconnected_vertices().components()\n\n    swc = swc_header + ""\\n""\n    offset = 0\n    for skel in skels:\n      swc += generate_swc(skel, offset) + ""\\n""\n      offset += skel.vertices.shape[0]\n\n    return swc\n\n  def viewer(\n    self, units=\'nm\', \n    draw_edges=True, draw_vertices=True,\n    color_by=\'radius\'\n  ):\n    """"""\n    View the skeleton with a radius heatmap. \n\n    Requires the matplotlib library which is \n    not installed by default.\n\n    units: label axes with these units\n    draw_edges: draw lines between vertices (more useful when skeleton is sparse)\n    draw_vertices: draw each vertex colored by its radius.\n    color_by: \n      \'radius\': color each vertex according to its radius attribute\n        aliases: \'r\', \'radius\', \'radii\'\n      \'component\': color connected components seperately\n        aliases: \'c\', \'component\', \'components\'\n      anything else: draw everything black\n    """"""\n    try:\n      import matplotlib.pyplot as plt\n      from mpl_toolkits.mplot3d import Axes3D \n      from matplotlib import cm\n    except ImportError:\n      print(""Skeleton.viewer requires matplotlib. Try: pip install matplotlib --upgrade"")\n      return\n\n    RADII_KEYWORDS = (\'radius\', \'radii\', \'r\')\n    COMPONENT_KEYWORDS = (\'component\', \'components\', \'c\')\n\n    fig = plt.figure(figsize=(10,10))\n    ax = Axes3D(fig)\n    ax.set_xlabel(units)\n    ax.set_ylabel(units)\n    ax.set_zlabel(units)\n\n    # Set plot axes equal. Matplotlib doesn\'t have an easier way to\n    # do this for 3d plots.\n    X = self.vertices[:,0]\n    Y = self.vertices[:,1]\n    Z = self.vertices[:,2]\n\n    max_range = np.array([X.max()-X.min(), Y.max()-Y.min(), Z.max()-Z.min()]).max() / 2.0\n\n    mid_x = (X.max()+X.min()) * 0.5\n    mid_y = (Y.max()+Y.min()) * 0.5\n    mid_z = (Z.max()+Z.min()) * 0.5\n    ax.set_xlim(mid_x - max_range, mid_x + max_range)\n    ax.set_ylim(mid_y - max_range, mid_y + max_range)\n    ax.set_zlim(mid_z - max_range, mid_z + max_range)\n    ### END EQUALIZATION CODE ###\n\n    component_colors = [\'k\', \'deeppink\', \'dodgerblue\', \'mediumaquamarine\', \'gold\' ]\n\n    def draw_component(i, skel):\n      component_color = component_colors[ i % len(component_colors) ]\n\n      if draw_vertices:\n        xs = skel.vertices[:,0]\n        ys = skel.vertices[:,1]\n        zs = skel.vertices[:,2]\n\n        if color_by in RADII_KEYWORDS:\n          colmap = cm.ScalarMappable(cmap=cm.get_cmap(\'rainbow\'))\n          colmap.set_array(skel.radii)\n\n          normed_radii = skel.radii / np.max(skel.radii)\n          yg = ax.scatter(xs, ys, zs, c=cm.rainbow(normed_radii), marker=\'o\')\n          cbar = fig.colorbar(colmap)\n          cbar.set_label(\'radius (\' + units + \')\', rotation=270)\n        elif color_by in COMPONENT_KEYWORDS:\n          yg = ax.scatter(xs, ys, zs, color=component_color, marker=\'.\')\n        else:\n          yg = ax.scatter(xs, ys, zs, color=\'k\', marker=\'.\')\n\n      if draw_edges:\n        for e1, e2 in skel.edges:\n          pt1, pt2 = skel.vertices[e1], skel.vertices[e2]\n          ax.plot(  \n            [ pt1[0], pt2[0] ],\n            [ pt1[1], pt2[1] ],\n            zs=[ pt1[2], pt2[2] ],\n            color=(component_color if not draw_vertices else \'silver\'),\n            linewidth=1,\n          )\n\n    if color_by in COMPONENT_KEYWORDS:\n      for i, skel in enumerate(self.components()):\n        draw_component(i, skel)\n    else:\n      draw_component(0, self)\n\n    plt.show()\n\n  def __eq__(self, other):\n    if self.id != other.id:\n      return False\n    elif self.vertices.shape[0] != other.vertices.shape[0]:\n      return False\n    elif self.edges.shape[0] != other.edges.shape[0]:\n      return False\n    elif self.extra_attributes != other.extra_attributes:\n      return False\n\n    attrs = [ attr[\'id\'] for attr in self.extra_attributes ]\n    for attr in attrs:\n      buf1 = getattr(self, attr)\n      buf2 = getattr(other, attr)\n      if np.all(buf1 != buf2):\n        return False\n\n    return (np.all(self.vertices == other.vertices)\n      and np.all(self.edges == other.edges) \\\n      and np.all(self.radii == other.radii) \\\n      and np.all(self.vertex_types == other.vertex_types))\n\n  def __str__(self):\n    template = ""{}=({}, {})""\n    attr_strings = []\n    for attr in self.extra_attributes:\n      attr = attr[\'id\']\n      buf = getattr(self, attr)\n      attr_strings.append(\n        template.format(attr, buf.shape[0], buf.dtype)\n      )\n\n    return ""Skeleton(segid={}, vertices=(shape={}, {}), edges=(shape={}, {}), {}, space=\'{}\' transform={})"".format(\n      self.id,\n      self.vertices.shape[0], self.vertices.dtype,\n      self.edges.shape[0], self.edges.dtype,\n      \', \'.join(attr_strings),\n      self.space, self.transform.tolist()\n    )\n\n  def __repr__(self):\n    return str(self)\n\n\nPrecomputedSkeleton = Skeleton # backwards compatibility\n'"
cloudvolume/threaded_queue.py,0,"b'from __future__ import print_function\n\nfrom six.moves import queue as Queue\nfrom six.moves import range\nfrom functools import partial\nimport threading\nimport time\n\nfrom tqdm import tqdm\n\nDEFAULT_THREADS = 20\n\nclass ThreadedQueue(object):\n  """"""Grant threaded task processing to any derived class.""""""\n  def __init__(self, n_threads, queue_size=0, progress=None):\n    self._n_threads = n_threads\n\n    self._queue = Queue.Queue(maxsize=queue_size) # 0 = infinite size\n    self._error_queue = Queue.Queue(maxsize=queue_size)\n    self._threads = ()\n    self._terminate = threading.Event()\n\n    self._processed_lock = threading.Lock()\n    self.processed = 0\n    self._inserted = 0\n\n    self.with_progress = progress\n\n    self.start_threads(n_threads)\n\n  @property\n  def pending(self):\n      return self._queue.qsize()\n\n  def put(self, fn):\n    """"""\n    Enqueue a task function for processing.\n\n    Requires:\n      fn: a function object that takes one argument\n        that is the interface associated with each\n        thread.\n\n        e.g. def download(api):\n               results.append(api.download())\n\n             self.put(download)\n\n    Returns: self\n    """"""\n    self._inserted += 1\n    self._queue.put(fn, block=True)\n    return self\n\n  def start_threads(self, n_threads):\n    """"""\n    Terminate existing threads and create a \n    new set if the thread number doesn\'t match\n    the desired number.\n\n    Required: \n      n_threads: (int) number of threads to spawn\n\n    Returns: self\n    """"""\n    if n_threads == len(self._threads):\n      return self\n    \n    # Terminate all previous tasks with the existing\n    # event object, then create a new one for the next\n    # generation of threads. The old object will hang\n    # around in memory until the threads actually terminate\n    # after another iteration.\n    self._terminate.set()\n    self._terminate = threading.Event()\n\n    threads = []\n\n    for _ in range(n_threads):\n      worker = threading.Thread(\n        target=self._consume_queue, \n        args=(self._terminate,)\n      )\n      worker.daemon = True\n      worker.start()\n      threads.append(worker)\n\n    self._threads = tuple(threads)\n    return self\n\n  def are_threads_alive(self):\n    """"""Returns: boolean indicating if any threads are alive""""""\n    return any(map(lambda t: t.is_alive(), self._threads))\n\n  def kill_threads(self):\n    """"""Kill all threads.""""""\n    self._terminate.set()\n    while self.are_threads_alive():\n      time.sleep(0.001)\n    self._threads = ()\n    return self\n\n  def _initialize_interface(self):\n    """"""\n    This is used to initialize the interfaces used in each thread.\n    You should reimplement it in subclasses. For example, return\n    an API object, file handle, or network connection. The functions\n    you pass into the self._queue will get it as the first parameter.\n\n    e.g. an implementation in a subclass.\n \n        def _initialize_interface(self):\n          return HTTPConnection()   \n\n        def other_function(self):\n          def threaded_file_read(connection):\n              # do stuff\n\n          self._queue.put(threaded_file_handle)\n\n    Returns: Interface object used in threads\n    """"""\n    return None\n\n  def _close_interface(self, interface):\n    """"""Allows derived classes to clean up after a thread finishes.""""""\n    pass\n\n  def _consume_queue(self, terminate_evt):\n    """"""\n    This is the main thread function that consumes functions that are\n    inside the _queue object. To use, execute self._queue(fn), where fn\n    is a function that performs some kind of network IO or otherwise\n    benefits from threading and is independent.\n\n    terminate_evt is automatically passed in on thread creation and \n    is a common event for this generation of threads. The threads\n    will terminate when the event is set and the queue burns down.\n\n    Returns: void\n    """"""\n    interface = self._initialize_interface()\n\n    while not terminate_evt.is_set():\n      try:\n        fn = self._queue.get(block=True, timeout=0.01)\n      except Queue.Empty:\n        continue # periodically check if the thread is supposed to die\n\n      fn = partial(fn, interface)\n\n      try:\n        self._consume_queue_execution(fn)\n      except Exception as err:\n        self._error_queue.put(err)\n\n    self._close_interface(interface)\n\n  def _consume_queue_execution(self, fn):\n    """"""\n    The actual task execution in each thread. This\n    is broken out so that exceptions can be caught\n    in derived classes and allow them to manipulate \n    the errant task, e.g. putting it back in the queue\n    for a retry.\n\n    Every task processed will automatically be marked complete.\n\n    Required:\n      [0] fn: A curried function that includes the interface\n              as its first argument.\n    Returns: void\n    """"""\n\n    # `finally` fires after all success or exceptions\n    # exceptions are handled in derived classes\n    # and uncaught ones are caught as a last resort\n    # in _consume_queue to be raised on the main thread.\n    try:\n      fn()\n    finally:\n      with self._processed_lock:\n        self.processed += 1\n        self._queue.task_done()\n\n  def _check_errors(self):\n    try:\n      err = self._error_queue.get(block=False) \n      self._error_queue.task_done()\n      self.kill_threads()\n      raise err\n    except Queue.Empty:\n      pass\n\n  def wait(self, progress=None):\n    """"""\n    Allow background threads to process until the\n    task queue is empty. If there are no threads,\n    in theory the queue should always be empty\n    as processing happens immediately on the main thread.\n\n    Optional:\n      progress: (bool or str) show a tqdm progress bar optionally\n        with a description if a string is provided\n    \n    Returns: self (for chaining)\n\n    Raises: The first exception recieved from threads\n    """"""\n    if not len(self._threads):\n      return self\n\n    desc = None\n    if type(progress) is str:\n      desc = progress\n\n    last = self._inserted\n    with tqdm(total=self._inserted, disable=(not progress), desc=desc) as pbar:\n      # Allow queue to consume, but check up on\n      # progress and errors every tenth of a second\n      while not self._queue.empty():\n        size = self._queue.qsize()\n        delta = last - size\n        if delta != 0: # We should crash on negative numbers\n          pbar.update(delta)\n        last = size\n        self._check_errors()\n        time.sleep(0.1)\n\n      # Wait until all tasks in the queue are \n      # fully processed. queue.task_done must be\n      # called for each task.\n      self._queue.join() \n      self._check_errors()\n\n      final = self._inserted - last\n      if final:\n        pbar.update(final)\n\n    if self._queue.empty():\n      self._inserted = 0\n\n    return self\n\n  def __del__(self):\n    self.wait() # if no threads were set the queue is always empty\n    self.kill_threads()\n\n  def __enter__(self):\n    if self.__class__ is ThreadedQueue and self._n_threads == 0:\n      raise ValueError(""Using 0 threads in base class ThreadedQueue with statement will never exit."")\n\n    self.start_threads(self._n_threads)\n    return self\n\n  def __exit__(self, exception_type, exception_value, traceback):\n    self.wait(progress=self.with_progress)\n    self.kill_threads()\n'"
cloudvolume/volumecutout.py,2,"b'import os\n\nimport json\nfrom six.moves import range\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom .lib import mkdir, save_images\n\nfrom . import microviewer\n\nclass VolumeCutout(np.ndarray):\n\n  def __new__(cls, buf, path, cloudpath, resolution, mip, layer_type, bounds, handle, *args, **kwargs):\n    return super(VolumeCutout, cls).__new__(cls, shape=buf.shape, buffer=np.asfortranarray(buf), dtype=buf.dtype, order=\'F\')\n\n  def __init__(self, buf, path, cloudpath, resolution, mip, layer_type, bounds, handle, *args, **kwargs):\n    super(VolumeCutout, self).__init__()\n\n    self.dataset_name = path.dataset\n    self.layer = path.layer\n    self.path = path\n    self.resolution = resolution\n    self.cloudpath = cloudpath\n    self.mip = mip\n    self.layer_type = layer_type\n    self.bounds = bounds\n    self.handle = handle\n\n  def close(self):\n    # This bizzare construction is because of this error:\n    # Traceback (most recent call last):\n    #   File ""cloud-volume/cloudvolume/volumecutout.py"", line 30, in __del__\n    #     self.close()\n    #   File ""cloud-volume/cloudvolume/volumecutout.py"", line 26, in close\n    #     if self.handle and not self.handle.closed:\n    # ValueError: mmap closed or invalid\n\n    # However testing if it is closed does not throw an error. So we test\n    # for closure and capture the exception if self.handle is None.\n\n    try:\n      if not self.handle.closed:\n        self.handle.close()\n    except AttributeError:\n      pass\n\n  # How to add a new attribute to an ndarray:\n  # https://docs.scipy.org/doc/numpy-1.13.0/uer/basics.subclassing.html#simple-example-adding-an-extra-attribute-to-ndarray\n\n  # Overriding __getitem__ and __setitem__ are \n  # not easy to do. \n  def __array_finalize__(self, obj):\n    if obj is None: \n      return\n\n    self.dataset_name = getattr(obj, \'dataset\', None)\n    self.layer = getattr(obj, \'layer\', None)\n    self.path = getattr(obj, \'path\', None)\n    self.resolution = getattr(obj, \'resolution\', None)\n    self.cloudpath = getattr(obj, \'cloudpath\', None)\n    self.mip = getattr(obj, \'mip\', None)\n    self.layer_type = getattr(obj, \'layer_type\', None)\n    self.bounds = getattr(obj, \'bounds\', None)\n    self.handle = None #getattr(obj, \'handle\', None)\n\n  def __del__(self):\n    sup = super(VolumeCutout, self)\n    if hasattr(sup, \'__del__\'):\n      sup.__del__()\n    self.close()\n\n  @classmethod\n  def from_volume(cls, meta, mip, buf, bounds, handle=None):\n    return VolumeCutout(\n      buf=buf,\n      path=meta.path,\n      cloudpath=meta.cloudpath,\n      resolution=meta.resolution(mip),\n      mip=mip,\n      layer_type=meta.layer_type,\n      bounds=bounds,\n      handle=handle,\n    )\n\n  @property\n  def num_channels(self):\n    return self.shape[3]\n\n  def save_images(\n    self, directory=None, axis=\'z\', \n    channel=None, global_norm=True, \n    image_format=\'PNG\', progress=True\n  ):\n    """"""See cloudvolume.lib.save_images for more information.""""""\n    if directory is None:\n      directory = os.path.join(\'./saved_images\', self.path.dataset, self.path.layer, str(self.mip), self.bounds.to_filename())\n\n    return save_images(\n      self, directory, axis, \n      channel, global_norm, image_format, \n      progress\n    )\n\n  def viewer(self, port=8080):\n    """"""Start a local web app on the given port that lets you explore this cutout.""""""\n    img = microviewer.emulate_eight_uint64(self)\n    microviewer.run([ img ], port=port)\n'"
test/layer_harness.py,6,"b'import pytest\n\nimport shutil\nimport numpy as np\n\nimport os\n\nfrom cloudvolume import Storage, CloudVolume\nfrom cloudvolume.lib import Bbox, Vec, min2, max2, find_closest_divisor\n\nTEST_NUMBER = np.random.randint(0, 999999)\nCLOUD_BUCKET = \'seunglab-test\'\n\nlayer_path = \'/tmp/removeme/\'\n\ndef create_storage(layer_name=\'layer\'):\n    stor_path = os.path.join(layer_path, layer_name)\n    return Storage(\'file://\' + stor_path, n_threads=0)\n\ndef create_image(size, layer_type=""image"", dtype=None):\n    default = lambda dt: dtype or dt\n\n    if layer_type == ""image"":\n        return np.random.randint(255, size=size, dtype=default(np.uint8))\n    elif layer_type == \'affinities\':\n        return np.random.uniform(low=0, high=1, size=size).astype(default(np.float32))\n    elif layer_type == ""segmentation"":\n        return np.random.randint(0xFFFFFF, size=size, dtype=np.uint32)\n    else:\n        high = np.array([0], dtype=default(np.uint32)) - 1\n        return np.random.randint(high[0], size=size, dtype=default(np.uint32))\n\ndef create_layer(size, offset, layer_type=""image"", layer_name=\'layer\', dtype=None):\n    random_data = create_image(size, layer_type, dtype)\n    vol = upload_image(random_data, offset, layer_type, layer_name)\n    return vol, random_data\n\ndef upload_image(image, offset, layer_type, layer_name):\n    lpath = \'file://{}\'.format(os.path.join(layer_path, layer_name))\n    \n    neuroglancer_chunk_size = find_closest_divisor(image.shape[:3], closest_to=[64,64,64])\n\n    # Jpeg encoding is lossy so it won\'t work\n    vol = CloudVolume.from_numpy(\n      image, \n      vol_path=lpath,\n      resolution=(1,1,1), \n      voxel_offset=offset, \n      chunk_size=neuroglancer_chunk_size, \n      layer_type=layer_type, \n      encoding=\'raw\', \n    )\n    \n    return vol\n\ndef delete_layer(path=layer_path):\n    if os.path.exists(path):\n        shutil.rmtree(path)  \n'"
test/test_chunks.py,23,"b""import pytest\n\nimport sys\n\nimport numpy as np\n\nfrom cloudvolume.chunks import encode, decode\nfrom cloudvolume import chunks\n\ndef encode_decode(data, format, num_chan=1):\n  encoded = encode(data, format)\n  result = decode(encoded, format, shape=(64,64,64,num_chan), dtype=np.uint8)\n\n  assert np.all(result.shape == data.shape)\n  assert np.all(data == result)\n\ndef test_kempression():\n  data = np.random.random_sample(size=1024 * 3).reshape( (64, 4, 4, 3) ).astype(np.float32)\n  encoded = encode(data, 'kempressed')\n  result = decode(encoded, 'kempressed', shape=(64, 4, 4, 3), dtype=np.float32)\n  assert np.all(result.shape == data.shape)\n  assert np.all(np.abs(data - result) <= np.finfo(np.float32).eps)\n\ndef test_compressed_segmentation():\n  def run_test(shape, block_size, accelerated):\n    data = np.random.randint(255, size=shape, dtype=np.uint32)\n    encoded = chunks.encode_compressed_segmentation(data, block_size, accelerated)\n\n    compressed = np.frombuffer(encoded, dtype=np.uint32)\n\n    assert compressed[0] == 1 # one channel\n\n    # at least check headers for integrity\n    # 64 bit block header \n    # encoded bits (8 bit), lookup table offset (24 bit), encodedValuesOffset (32)\n    grid = np.ceil(np.array(shape[3:], dtype=np.float32) / np.array(block_size, dtype=np.float32))\n    grid = grid.astype(np.uint32)\n    for i in range(np.prod(grid)):\n      encodedbits = (compressed[2*i + 1] & 0xff000000) >> 24\n      table_offset = compressed[2*i + 1] & 0x00ffffff\n      encoded_offset = compressed[2*i + 2]\n\n      assert encodedbits in (0, 1, 2, 4, 8, 16, 32)\n      assert table_offset < len(compressed)\n      assert encoded_offset < len(compressed)\n\n    result = chunks.decode_compressed_segmentation(encoded, \n      shape=shape,\n      dtype=np.uint32,\n      block_size=block_size,\n      accelerated=accelerated,\n    ) \n\n    assert np.all(data == result)\n\n  try:\n    import _compressed_segmentation\n    test_options = (True, False)\n  except:\n    test_options = (False,)\n\n  for accelerated in test_options:\n    run_test( ( 2, 2, 2, 1), (2,2,2), accelerated )\n    run_test( ( 1, 2, 2, 1), (2,2,2), accelerated )\n    run_test( ( 2, 1, 2, 1), (2,2,2), accelerated )\n    run_test( ( 2, 2, 1, 1), (2,2,2), accelerated )\n    run_test( (64,64,64,1), (8,8,8), accelerated )\n    run_test( (16,16,16,1), (8,8,8), accelerated )\n    run_test( (8,8,8,1), (8,8,8), accelerated )\n    run_test( (4,4,4,1), (8,8,8), accelerated )\n    run_test( (4,4,4,1), (2,2,2), accelerated )\n    run_test( (2,4,4,1), (2,2,2), accelerated )\n  \n  if True in test_options:\n    run_test( (10,8,8,1), (10,8,8), True ) # known bug in pure python verison\n\ndef test_fpzip():\n  for N in range(0,100):\n    flts = np.array(range(N), dtype=np.float32).reshape( (N,1,1,1) )\n    compressed = encode(flts, 'fpzip')\n    assert compressed != flts\n    decompressed = decode(compressed, 'fpzip')\n    assert np.all(decompressed == flts)\n\n  for N in range(0, 200, 2):\n    flts = np.array(range(N), dtype=np.float32).reshape( (N // 2, 2, 1, 1) )\n    compressed = encode(flts, 'fpzip')\n    assert compressed != flts\n    decompressed = decode(compressed, 'fpzip')\n    assert np.all(decompressed == flts)\n\ndef test_raw():\n  random_data = np.random.randint(255, size=(64,64,64,1), dtype=np.uint8)\n  encode_decode(random_data, 'raw')\n\ndef test_npz():\n  random_data = np.random.randint(255, size=(64,64,64,1), dtype=np.uint8)\n  encode_decode(random_data, 'npz')\n\ndef test_jpeg():\n  for num_chan in (1,3):\n    data = np.zeros(shape=(64,64,64,num_chan), dtype=np.uint8)\n    encode_decode(data, 'jpeg', num_chan)\n    encode_decode(data + 255, 'jpeg', num_chan)\n\n    # Random jpeg won't decompress to exactly the same image\n    # but it should have nearly the same average power\n    random_data = np.random.randint(255, size=(64,64,64,num_chan), dtype=np.uint8)\n    pre_avg = random_data.copy().flatten().mean()\n    encoded = encode(random_data, 'jpeg')\n    decoded = decode(encoded, 'jpeg', shape=(64,64,64,num_chan), dtype=np.uint8)\n    post_avg = decoded.copy().flatten().mean()\n\n    assert abs(pre_avg - post_avg) < 1\n\n"""
test/test_cloudvolume.py,122,"b'import pytest\n\nimport copy\nimport gzip\nimport json\nimport numpy as np\nimport os\nimport shutil\nimport sys\n\nfrom cloudvolume import exceptions\nfrom cloudvolume.exceptions import AlignmentError, ReadOnlyException\nfrom cloudvolume import CloudVolume, chunks\nfrom cloudvolume.lib import Bbox, Vec, yellow, mkdir\nimport cloudvolume.sharedmemory as shm\nfrom layer_harness import (\n  TEST_NUMBER,  \n  delete_layer, create_layer\n)\nfrom cloudvolume.datasource.precomputed.common import cdn_cache_control\nfrom cloudvolume.datasource.precomputed.image.tx import generate_chunks\n\ndef test_from_numpy():\n  arr = np.random.randint(0, high=256, size=(128,128, 128))\n  arr = np.asarray(arr, dtype=np.uint8)\n  vol = CloudVolume.from_numpy(arr, max_mip=1)\n  arr2 = vol[:,:,:]\n  np.alltrue(arr == arr2)\n  \n  arr = np.random.randn(128,128, 128, 3)\n  arr = np.asarray(arr, dtype=np.float32)\n  vol = CloudVolume.from_numpy(arr, max_mip=1)\n  arr2 = vol[:,:,:]\n  np.alltrue(arr == arr2)\n  shutil.rmtree(\'/tmp/image\')\n\ndef test_cloud_access():\n  CloudVolume(\'gs://seunglab-test/test_v0/image\')\n  CloudVolume(\'s3://seunglab-test/test_dataset/image\')\n\ndef test_fill_missing():\n  info = CloudVolume.create_new_info(\n  num_channels=1, # Increase this number when we add more tests for RGB\n  layer_type=\'image\', \n  data_type=\'uint8\', \n  encoding=\'raw\',\n  resolution=[ 1,1,1 ], \n  voxel_offset=[0,0,0], \n  volume_size=[128,128,64],\n  mesh=\'mesh\', \n  chunk_size=[ 64,64,64 ],\n  )\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/empty_volume\', mip=0, info=info)\n  vol.commit_info()\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/empty_volume\', mip=0, fill_missing=True)\n  assert np.count_nonzero(vol[:]) == 0\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/empty_volume\', mip=0, fill_missing=True, cache=True)\n  assert np.count_nonzero(vol[:]) == 0\n  assert np.count_nonzero(vol[:]) == 0\n\n  vol.cache.flush()\n  delete_layer(\'/tmp/cloudvolume/empty_volume\')\n\ndef test_aligned_read():\n  for green in (False, True):\n    print(""green"", green)\n    delete_layer()\n    cv, data = create_layer(size=(50,50,50,1), offset=(0,0,0))\n    cv.green_threads = green\n    # the last dimension is the number of channels\n    assert cv[0:50,0:50,0:50].shape == (50,50,50,1)\n    assert np.all(cv[0:50,0:50,0:50] == data)\n    \n    delete_layer()\n    cv, data = create_layer(size=(128,64,64,1), offset=(0,0,0))\n    cv.green_threads = green\n    # the last dimension is the number of channels\n    assert cv[0:64,0:64,0:64].shape == (64,64,64,1) \n    assert np.all(cv[0:64,0:64,0:64] ==  data[:64,:64,:64,:])\n\n    delete_layer()\n    cv, data = create_layer(size=(128,64,64,1), offset=(10,20,0))\n    cv.green_threads = green\n    cutout = cv[10:74,20:84,0:64]\n    # the last dimension is the number of channels\n    assert cutout.shape == (64,64,64,1) \n    assert np.all(cutout == data[:64,:64,:64,:])\n    # get the second chunk\n    cutout2 = cv[74:138,20:84,0:64]\n    assert cutout2.shape == (64,64,64,1) \n    assert np.all(cutout2 == data[64:128,:64,:64,:])\n\n    assert cv[25, 25, 25].shape == (1,1,1,1)\n\ndef test_save_images():\n  delete_layer()\n  cv, data = create_layer(size=(50,50,50,1), offset=(0,0,0))  \n\n  img = cv[:]\n  directory = img.save_images()\n\n  for z, fname in enumerate(sorted(os.listdir(directory))):\n    assert fname == str(z).zfill(3) + \'.png\'\n\n  shutil.rmtree(directory)\n\ndef test_bbox_read():\n  delete_layer()\n  cv, data = create_layer(size=(50,50,50,1), offset=(0,0,0))\n\n  x = Bbox( (0,1,2), (48,49,50) )\n  # the last dimension is the number of channels\n  assert cv[x].shape == (48,48,48,1)\n  assert np.all(cv[x] == data[0:48, 1:49, 2:50])  \n\ndef test_number_type_read():\n  delete_layer()\n  cv, data = create_layer(size=(50,50,50,1), offset=(0,0,0))\n\n  for datatype in (\n    np.uint8, np.uint16, np.uint32, np.uint64,\n    np.int8, np.int16, np.int32, np.int64,\n    np.float16, np.float32, np.float64,\n    int, float\n  ):\n    x = datatype(5)\n\n    # the last dimension is the number of channels\n    assert cv[x,x,x].shape == (1,1,1,1)\n    assert np.all(cv[x,x,x] == data[5,5,5])  \n\ndef test_ellipsis_read():\n  delete_layer()\n  cv, data = create_layer(size=(50,50,50,1), offset=(0,0,0))\n\n  img = cv[...]\n  assert np.all(img == data)\n\n  img = cv[5:10, ...]\n  assert np.all(img == data[5:10, :,:,:])\n\n  img = cv[5:10, 7, 8, ...]\n  assert np.all(np.squeeze(img) == data[5:10, 7, 8, 0])\n\n  img = cv[5:10, ..., 8, 0]\n  assert np.all(np.squeeze(img) == data[5:10, :, 8, 0])\n\n  try:\n    img = cv[..., 5, ..., 0]\n  except ValueError:\n    pass\n\n\ndef test_parallel_read():\n  paths = [\n    \'gs://seunglab-test/test_v0/image\',\n    \'s3://seunglab-test/test_v0/image\',\n  ]\n\n  for parallel in (2, True):\n    for cloudpath in paths:\n      vol1 = CloudVolume(cloudpath, parallel=1)\n      vol2 = CloudVolume(cloudpath, parallel=parallel)\n\n      data1 = vol1[:512,:512,:50]\n      img = vol2[:512,:512,:50]\n      assert np.all(data1 == vol2[:512,:512,:50])\n\n\ndef test_parallel_read_shm():\n  paths = [\n    \'gs://seunglab-test/test_v0/image\',\n    \'s3://seunglab-test/test_v0/image\',\n  ]\n\n  for cloudpath in paths:\n    vol1 = CloudVolume(cloudpath, parallel=1)\n    vol2 = CloudVolume(cloudpath, parallel=2)\n\n    data1 = vol1[:512,:512,:50]\n    data2 = vol2.download_to_shared_memory(np.s_[:512,:512,:50])\n    assert np.all(data1 == data2)\n    data2.close()\n    vol2.unlink_shared_memory()\n\ndef test_parallel_write():\n  delete_layer()\n  cv, data = create_layer(size=(512,512,128,1), offset=(0,0,0))\n  \n  # aligned write\n  cv.parallel = 2\n  cv[:] = np.zeros(shape=(512,512,128,1), dtype=cv.dtype) + 5\n  data = cv[:]\n  assert np.all(data == 5)\n\n  # non-aligned-write\n  cv.parallel = 2\n  cv.non_aligned_writes = True\n  cv[1:,1:,1:] = np.zeros(shape=(511,511,127,1), dtype=cv.dtype) + 7\n  data = cv[1:,1:,1:]\n  assert np.all(data == 7)\n\n  # thin non-aligned-write so that there\'s no aligned core\n  cv.parallel = 2\n  cv.non_aligned_writes = True\n  cv[25:75,25:75,25:75] = np.zeros(shape=(50,50,50,1), dtype=cv.dtype) + 8\n  data = cv[25:75,25:75,25:75]\n  assert np.all(data == 8)\n\n\ndef test_parallel_shared_memory_write():\n  delete_layer()\n  cv, _ = create_layer(size=(256,256,128,1), offset=(0,0,0))\n\n  shm_location = \'cloudvolume-test-shm-parallel-write\'\n  mmapfh, shareddata = shm.ndarray(shape=(256,256,128), dtype=np.uint8, location=shm_location)\n  shareddata[:] = 1\n\n  cv.parallel = 1\n  cv.upload_from_shared_memory(shm_location, Bbox((0,0,0), (256,256,128)))\n  assert np.all(cv[:] == 1)\n\n  shareddata[:] = 2\n  cv.parallel = 2\n  cv.upload_from_shared_memory(shm_location, Bbox((0,0,0), (256,256,128)))\n  assert np.all(cv[:] == 2)\n\n  shareddata[:,:,:64] = 3\n  cv.upload_from_shared_memory(shm_location, bbox=Bbox((0,0,0), (256,256,128)), \n    cutout_bbox=Bbox((0,0,0), (256,256,64)))\n  assert np.all(cv[:,:,:64] == 3)    \n  assert np.all(cv[:,:,64:128] == 2)    \n\n  shareddata[:,:,:69] = 4\n  cv.autocrop = True\n  cv.upload_from_shared_memory(shm_location, bbox=Bbox((-5,-5,-5), (251,251,123)), \n    cutout_bbox=Bbox((-5,-5,-5), (128,128,64)))\n  assert np.all(cv[:128,:128,:63] == 4)    \n  assert np.all(cv[128:,128:,:64] == 3)    \n  assert np.all(cv[:,:,64:128] == 2)    \n\n  shareddata[:] = 0\n  shareddata[:,0,0] = 1\n  cv.upload_from_shared_memory(shm_location, bbox=Bbox((0,0,0), (256,256,128)), order=\'C\')\n  assert np.all(cv[0,0,:] == 1)\n  assert np.all(cv[1,0,:] == 0)\n\n  mmapfh.close()\n  shm.unlink(shm_location)\n\ndef test_non_aligned_read():\n  delete_layer()\n  cv, data = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  # the last dimension is the number of channels\n  assert cv[31:65,0:64,0:64].shape == (34,64,64,1) \n  assert np.all(cv[31:65,0:64,0:64] == data[31:65,:64,:64,:])\n\n  # read a single pixel\n  delete_layer()\n  cv, data = create_layer(size=(64,64,64,1), offset=(0,0,0))\n  # the last dimension is the number of channels\n  assert cv[22:23,22:23,22:23].shape == (1,1,1,1) \n  assert np.all(cv[22:23,22:23,22:23] == data[22:23,22:23,22:23,:])\n\n  # Test steps (negative steps are not supported)\n  img1 = cv[::2, ::2, ::2, :]\n  img2 = cv[:, :, :, :][::2, ::2, ::2, :]\n  assert np.array_equal(img1, img2)\n\n  # read a single pixel\n  delete_layer()\n  cv, data = create_layer(size=(256,256,64,1), offset=(3,7,11))\n  # the last dimension is the number of channels\n  assert cv[22:77:2, 22:197:3, 22:32].shape == (28,59,10,1) \n  assert data[19:74:2, 15:190:3, 11:21,:].shape == (28,59,10,1) \n  assert np.all(cv[22:77:2, 22:197:3, 22:32] == data[19:74:2, 15:190:3, 11:21,:])\n\ndef test_autocropped_read():\n  delete_layer()\n  cv, data = create_layer(size=(50,50,50,1), offset=(0,0,0))\n\n  cv.autocrop = True\n  cv.bounded = False\n\n  # left overlap\n  img = cv[-25:25,-25:25,-25:25]\n  assert img.shape == (25,25,25,1)\n  assert np.all(img == data[:25, :25, :25])\n\n  # right overlap\n  img = cv[40:60, 40:60, 40:60]\n  assert img.shape == (10,10,10,1)\n  assert np.all(img == data[40:, 40:, 40:])\n\n  # containing\n  img = cv[-100:100, -100:100, -100:100]\n  assert img.shape == (50,50,50,1)\n  assert np.all(img == data)\n\n  # contained\n  img = cv[10:20, 10:20, 10:20]\n  assert img.shape == (10,10,10,1)\n  assert np.all(img == data[10:20, 10:20, 10:20])\n\n  # non-intersecting\n  img = cv[100:120, 100:120, 100:120]\n  assert img.shape == (0,0,0,1)\n  assert np.all(img == data[0:0, 0:0, 0:0])    \n\ndef test_download_upload_file():\n  for green in (False, True):\n    print(""green:"", green)\n    delete_layer()\n    cv, _ = create_layer(size=(50,50,50,1), offset=(0,0,0))\n    cv.green_threads = green\n\n    mkdir(\'/tmp/file/\')\n\n    cv.download_to_file(\'/tmp/file/test\', cv.bounds)\n    cv2 = CloudVolume(\'file:///tmp/file/test2/\', info=cv.info)\n    cv2.upload_from_file(\'/tmp/file/test\', cv.bounds)\n\n    assert np.all(cv2[:] == cv[:])\n    shutil.rmtree(\'/tmp/file/\')\n\ndef test_write():\n  for green in (False, True):\n    print(""green:"", green)\n    delete_layer()\n    cv, _ = create_layer(size=(50,50,50,1), offset=(0,0,0))\n    cv.green_threads = green\n\n    replacement_data = np.zeros(shape=(50,50,50,1), dtype=np.uint8)\n    cv[0:50,0:50,0:50] = replacement_data\n    assert np.all(cv[0:50,0:50,0:50] == replacement_data)\n\n    replacement_data = np.random.randint(255, size=(50,50,50,1), dtype=np.uint8)\n    cv[0:50,0:50,0:50] = replacement_data\n    assert np.all(cv[0:50,0:50,0:50] == replacement_data)\n\n    replacement_data = np.random.randint(255, size=(50,50,50,1), dtype=np.uint8)\n    bbx = Bbox((0,0,0), (50,50,50))\n    cv[bbx] = replacement_data\n    assert np.all(cv[bbx] == replacement_data)\n\n    # out of bounds\n    delete_layer()\n    cv, _ = create_layer(size=(128,64,64,1), offset=(10,20,0))\n    cv.green_threads = green\n    with pytest.raises(ValueError):\n      cv[74:150,20:84,0:64] = np.ones(shape=(64,64,64,1), dtype=np.uint8)\n    \n    # non-aligned writes\n    delete_layer()\n    cv, _ = create_layer(size=(128,64,64,1), offset=(10,20,0))\n    cv.green_threads = green\n    with pytest.raises(ValueError):\n      cv[21:85,0:64,0:64] = np.ones(shape=(64,64,64,1), dtype=np.uint8)\n\n    # test bounds check for short boundary chunk\n    delete_layer()\n    cv, _ = create_layer(size=(25,25,25,1), offset=(1,3,5))\n    cv.green_threads = green\n    cv.info[\'scales\'][0][\'chunk_sizes\'] = [[ 11,11,11 ]]\n    cv[:] = np.ones(shape=(25,25,25,1), dtype=np.uint8)\n\ndef test_non_aligned_write():\n  delete_layer()\n  offset = Vec(5,7,13)\n  cv, _ = create_layer(size=(1024, 1024, 5, 1), offset=offset)\n\n  cv[:] = np.zeros(shape=cv.shape, dtype=cv.dtype)\n\n  # Write inside a single chunk\n\n  onepx = Bbox( (10,200,15), (11,201,16) )\n  try:\n    cv[ onepx.to_slices() ] = np.ones(shape=onepx.size3(), dtype=cv.dtype)\n    assert False\n  except AlignmentError:\n    pass\n  \n  cv.non_aligned_writes = True\n  cv[ onepx.to_slices() ] = np.ones(shape=onepx.size3(), dtype=cv.dtype)\n  answer = np.zeros(shape=cv.shape, dtype=cv.dtype)\n  answer[ 5, 193, 2 ] = 1\n  assert np.all(cv[:] == answer)\n\n  # Write across multiple chunks\n  cv[:] = np.zeros(shape=cv.shape, dtype=cv.dtype)\n  cv.non_aligned_writes = True\n  middle = Bbox( (512 - 10, 512 - 11, 0), (512 + 10, 512 + 11, 5) ) + offset\n  cv[ middle.to_slices() ] = np.ones(shape=middle.size3(), dtype=cv.dtype)\n  answer = np.zeros(shape=cv.shape, dtype=cv.dtype)\n  answer[ 502:522, 501:523, : ] = 1\n  assert np.all(cv[:] == answer)    \n\n  cv.non_aligned_writes = False\n  try:\n    cv[ middle.to_slices() ] = np.ones(shape=middle.size3(), dtype=cv.dtype)\n    assert False\n  except AlignmentError:\n    pass\n\n  # Big inner shell\n  delete_layer()\n  cv, _ = create_layer(size=(1024, 1024, 5, 1), offset=offset)\n  cv[:] = np.zeros(shape=cv.shape, dtype=cv.dtype)\n  middle = Bbox( (512 - 150, 512 - 150, 0), (512 + 150, 512 + 150, 5) ) + offset\n\n  try:\n    cv[ middle.to_slices() ] = np.ones(shape=middle.size3(), dtype=cv.dtype)\n    assert False\n  except AlignmentError:\n    pass\n\n  cv.non_aligned_writes = True\n  cv[ middle.to_slices() ] = np.ones(shape=middle.size3(), dtype=cv.dtype)\n  answer = np.zeros(shape=cv.shape, dtype=cv.dtype)\n  answer[ 362:662, 362:662, : ] = 1\n  assert np.all(cv[:] == answer)    \n\nfrom cloudvolume import view\n\ndef test_autocropped_write():\n  delete_layer()\n  cv, _ = create_layer(size=(100,100,100,1), offset=(0,0,0))\n\n  cv.autocrop = True\n  cv.bounded = False\n\n  replacement_data = np.ones(shape=(300,300,300,1), dtype=np.uint8)\n  cv[-100:200, -100:200, -100:200] = replacement_data\n  assert np.all(cv[:,:,:] == replacement_data[0:100,0:100,0:100])\n  \n  replacement_data = np.random.randint(255, size=(100,100,100,1), dtype=np.uint8)\n  \n  cv[-50:50, -50:50, -50:50] = replacement_data\n  assert np.all(cv[0:50,0:50,0:50] == replacement_data[50:, 50:, 50:])\n\n  cv[50:150, 50:150, 50:150] = replacement_data\n  assert np.all(cv[50:,50:,50:] == replacement_data[:50, :50, :50])\n\n  cv[0:50, 0:50, 0:50] = replacement_data[:50,:50,:50]\n  assert np.all(cv[0:50, 0:50, 0:50] == replacement_data[:50,:50,:50])    \n\n  replacement_data = np.ones(shape=(100,100,100,1), dtype=np.uint8)\n  cv[:] = replacement_data + 1\n  cv[100:200, 100:200, 100:200] = replacement_data\n  assert np.all(cv[:,:,:] != 1)\n\ndef test_writer_last_chunk_smaller():\n  delete_layer()\n  cv, data = create_layer(size=(100,64,64,1), offset=(0,0,0))\n  cv.info[\'scales\'][0][\'chunk_sizes\'] = [[ 64,64,64 ]]\n  \n  chunks = [ chunk for chunk in generate_chunks(cv.meta, data[:,:,:,:], (0,0,0), cv.mip) ]\n\n  assert len(chunks) == 2\n\n  startpt, endpt, spt, ept = chunks[0]\n  assert np.array_equal(spt, (0,0,0))\n  assert np.array_equal(ept, (64,64,64))\n  assert np.all((endpt - startpt) == Vec(64,64,64))\n\n  startpt, endpt, spt, ept = chunks[1]\n  assert np.array_equal(spt, (64,0,0))\n  assert np.array_equal(ept, (100,64,64))\n  assert np.all((endpt - startpt) == Vec(36,64,64))\n\ndef test_write_compressed_segmentation():\n  delete_layer()\n  cv, data = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  cv.info[\'num_channels\'] = 1\n  cv.info[\'data_type\'] = \'uint32\'\n  cv.scale[\'encoding\'] = \'compressed_segmentation\'\n  cv.scale[\'compressed_segmentation_block_size\'] = (8,8,8)\n  cv.commit_info()\n\n  cv[:] = data.astype(np.uint32)\n  data2 = cv[:]\n\n  assert np.all(data == data2)\n\n  cv.info[\'data_type\'] = \'uint64\'\n  cv.commit_info()\n\n  cv[:] = data.astype(np.uint64)\n  data2 = cv[:]\n  \n  assert np.all(data == data2)\n\n# def test_reader_negative_indexing():\n#     """"""negative indexing is supported""""""\n#     delete_layer()\n#     cv, data = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n#     # Test negative beginnings\n#     img1 = cv[-1:, -1:, -1:, :]\n#     img2 = cv[:, :, :, :][-1:, -1:, -1:, :]\n\n#     assert np.array_equal(img1, img2)    \n\n#     # Test negative ends\n#     with pytest.raises(ValueError):\n#         img1 = cv[::-1, ::-1, ::-1, :]\n\ndef test_negative_coords_upload_download():\n  cv, data = create_layer(size=(128,64,64,1), offset=(-64,-64,-64))\n\n  downloaded = cv[-64:64, -64:0, -64:0]\n  assert np.all(data == downloaded)\n\ndef test_setitem_mismatch():\n  delete_layer()\n  cv, _ = create_layer(size=(64,64,64,1), offset=(0,0,0))\n\n  with pytest.raises(ValueError):\n    cv[0:64,0:64,0:64] = np.zeros(shape=(5,5,5,1), dtype=np.uint8)\n\ndef test_bounds():\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(100,100,100))\n  cv.bounded = True\n\n  try:\n    cutout = cv[0:,0:,0:,:]\n    cutout = cv[100:229,100:165,100:165,0]\n    cutout = cv[99:228,100:164,100:164,0]\n  except ValueError:\n    pass\n  else:\n    assert False\n\n  # don\'t die\n  cutout = cv[100:228,100:164,100:164,0]\n\n  cv.bounded = False\n  cutout = cv[0:,0:,0:,:]\n  assert cutout.shape == (228, 164, 164, 1)\n\n  assert np.count_nonzero(cutout) != 0\n\n  cutout[100:,100:,100:,:] = 0\n\n  assert np.count_nonzero(cutout) == 0\n\ndef test_provenance():\n  delete_layer()\n  cv, _ = create_layer(size=(64,64,64,1), offset=(0,0,0))\n\n  provobj = json.loads(cv.provenance.serialize())\n  provobj[\'processing\'] = [] # from_numpy\n  assert provobj == {\n    ""sources"": [], \n    ""owners"": [], \n    ""processing"": [], \n    ""description"": """"\n  }\n\n  cv.provenance.sources.append(\'cooldude24@princeton.edu\')\n  cv.commit_provenance()\n  cv.refresh_provenance()\n\n  assert cv.provenance.sources == [ \'cooldude24@princeton.edu\' ]\n\n  # should not die\n  cv = CloudVolume(cv.cloudpath, provenance={})\n  cv = CloudVolume(cv.cloudpath, provenance={ \'sources\': [] })\n  cv = CloudVolume(cv.cloudpath, provenance={ \'owners\': [] })\n  cv = CloudVolume(cv.cloudpath, provenance={ \'processing\': [] })\n  cv = CloudVolume(cv.cloudpath, provenance={ \'description\': \'\' })\n\n  # should die\n  try:\n    cv = CloudVolume(cv.cloudpath, provenance={ \'sources\': 3 })\n    assert False\n  except:\n    pass\n\n  cv = CloudVolume(cv.cloudpath, provenance=""""""{\n    ""sources"": [ ""wow"" ]\n  }"""""")\n\n  assert cv.provenance.sources[0] == \'wow\'\n\ndef test_info_provenance_cache():\n  image = np.zeros(shape=(128,128,128,1), dtype=np.uint8)\n  vol = CloudVolume.from_numpy(\n    image, \n    voxel_offset=(0,0,0), \n    vol_path=\'gs://seunglab-test/cloudvolume/caching\', \n    layer_type=\'image\', \n    resolution=(1,1,1), \n    encoding=\'raw\'\n  )\n\n  # Test Info\n  vol.cache.enabled = True\n  vol.cache.flush()\n  info = vol.refresh_info()\n  assert info is not None\n\n  with open(os.path.join(vol.cache.path, \'info\'), \'r\') as infof:\n    info = infof.read()\n    info = json.loads(info)\n\n  with open(os.path.join(vol.cache.path, \'info\'), \'w\') as infof:\n    infof.write(json.dumps({ \'wow\': \'amaze\' }))\n\n  info = vol.refresh_info()\n  assert info == { \'wow\': \'amaze\' }\n  vol.cache.enabled = False\n  info = vol.refresh_info()\n  assert info != { \'wow\': \'amaze\' }\n\n  infopath = os.path.join(vol.cache.path, \'info\')\n  assert os.path.exists(infopath)\n\n  vol.cache.flush_info()\n  assert not os.path.exists(infopath)\n  vol.cache.flush_info() # assert no error by double delete\n\n\n  # Test Provenance\n  vol.cache.enabled = True\n  vol.cache.flush()\n  prov = vol.refresh_provenance()\n  assert prov is not None\n\n  with open(os.path.join(vol.cache.path, \'provenance\'), \'r\') as provf:\n    prov = provf.read()\n    prov = json.loads(prov)\n\n  with open(os.path.join(vol.cache.path, \'provenance\'), \'w\') as provf:\n    prov[\'description\'] = \'wow\'\n    provf.write(json.dumps(prov))\n\n  prov = vol.refresh_provenance()\n  assert prov[\'description\'] == \'wow\'\n  vol.cache.enabled = False\n  prov = vol.refresh_provenance()\n  assert prov[\'description\'] == \'\'\n\n  provpath = os.path.join(vol.cache.path, \'provenance\')\n  vol.cache.flush_provenance()\n  assert not os.path.exists(provpath)\n  vol.cache.flush_provenance() # assert no error by double delete\n\ndef test_caching():\n  image = np.zeros(shape=(128,128,128,1), dtype=np.uint8)\n  image[0:64,0:64,0:64] = 1\n  image[64:128,0:64,0:64] = 2\n  image[0:64,64:128,0:64] = 3\n  image[0:64,0:64,64:128] = 4\n  image[64:128,64:128,0:64] = 5\n  image[64:128,0:64,64:128] = 6\n  image[0:64,64:128,64:128] = 7\n  image[64:128,64:128,64:128] = 8\n\n  dirpath = \'/tmp/cloudvolume/caching-volume-\' + str(TEST_NUMBER)\n  layer_path = \'file://\' + dirpath\n\n  vol = CloudVolume.from_numpy(\n    image, \n    voxel_offset=(0,0,0), \n    vol_path=layer_path, \n    layer_type=\'image\', \n    resolution=(1,1,1), \n    encoding=\'raw\',\n    chunk_size=(64,64,64),\n  )\n\n  vol.cache.enabled = True\n  vol.cache.flush()\n\n  # Test that reading populates the cache\n  read1 = vol[:,:,:]\n  assert np.all(read1 == image)\n\n  read2 = vol[:,:,:]\n  assert np.all(read2 == image)\n\n  assert len(vol.cache.list()) > 0\n\n  files = vol.cache.list()\n  validation_set = [\n    \'0-64_0-64_0-64\',\n    \'64-128_0-64_0-64\',\n    \'0-64_64-128_0-64\',\n    \'0-64_0-64_64-128\',\n    \'64-128_64-128_0-64\',\n    \'64-128_0-64_64-128\',\n    \'0-64_64-128_64-128\',\n    \'64-128_64-128_64-128\'\n  ]\n  assert set([ os.path.splitext(fname)[0] for fname in files ]) == set(validation_set)\n\n  for i in range(8):\n    fname = os.path.join(vol.cache.path, vol.key, validation_set[i]) + \'.gz\'\n    with gzip.GzipFile(fname, mode=\'rb\') as gfile:\n      chunk = gfile.read()\n    img3d = chunks.decode(\n      chunk, \'raw\', (64,64,64,1), np.uint8\n    )\n    assert np.all(img3d == (i+1))\n\n  vol.cache.flush()\n  assert not os.path.exists(vol.cache.path)\n\n  # Test that writing populates the cache\n  vol[:,:,:] = image\n\n  assert os.path.exists(vol.cache.path)\n  assert np.all(vol[:,:,:] == image)\n\n  vol.cache.flush()\n\n  # Test that partial reads work too\n  result = vol[0:64,0:64,:]\n  assert np.all(result == image[0:64,0:64,:])\n  files = vol.cache.list()\n  assert len(files) == 2\n  result = vol[:,:,:]\n  assert np.all(result == image)\n  files = vol.cache.list()\n  assert len(files) == 8\n\n  vol.cache.flush()\n\n  # Test Non-standard Cache Destination\n  dirpath = \'/tmp/cloudvolume/caching-cache-\' + str(TEST_NUMBER)\n  vol.cache.enabled = True\n  vol.cache.path = dirpath\n  vol[:,:,:] = image\n\n  assert len(os.listdir(os.path.join(dirpath, vol.key))) == 8\n\n  vol.cache.flush()\n\n  # Test that caching doesn\'t occur when cache is not set\n  vol.cache.enabled = False\n  result = vol[:,:,:]\n  if os.path.exists(vol.cache.path):\n    files = vol.cache.list()\n    assert len(files) == 0\n\n  vol[:,:,:] = image\n  if os.path.exists(vol.cache.path):\n    files = vol.cache.list()\n    assert len(files) == 0\n\n  vol.cache.flush()\n\n  # Test that deletion works too\n  vol.cache.enabled = True\n  vol[:,:,:] = image\n  files = vol.cache.list()\n  assert len(files) == 8\n  vol.delete( np.s_[:,:,:] )\n  files = vol.cache.list()\n  assert len(files) == 0\n\n  vol.cache.flush()    \n\n  vol[:,:,:] = image\n  files = vol.cache.list()\n  assert len(files) == 8\n  vol.cache.flush(preserve=np.s_[:,:,:])\n  files = vol.cache.list()\n  assert len(files) == 8\n  vol.cache.flush(preserve=np.s_[:64,:64,:])\n  files = vol.cache.list()\n  assert len(files) == 2\n\n  vol.cache.flush()\n\n  vol[:,:,:] = image\n  files = vol.cache.list()\n  assert len(files) == 8\n  vol.cache.flush_region(Bbox( (50, 50, 0), (100, 100, 10) ))\n  files = vol.cache.list()\n  assert len(files) == 4\n\n  vol.cache.flush()\n\n  vol[:,:,:] = image\n  files = vol.cache.list()\n  assert len(files) == 8\n  vol.cache.flush_region(np.s_[50:100, 50:100, 0:10])\n  files = vol.cache.list()\n  assert len(files) == 4\n\n  vol.cache.flush()\n\ndef test_cache_compression_setting():\n  image = np.zeros(shape=(128,128,128,1), dtype=np.uint8)\n  dirpath = \'/tmp/cloudvolume/caching-validity-\' + str(TEST_NUMBER)\n  layer_path = \'file://\' + dirpath\n\n  vol = CloudVolume.from_numpy(\n    image, \n    voxel_offset=(1,1,1), \n    vol_path=layer_path, \n    layer_type=\'image\', \n    resolution=(1,1,1), \n    encoding=\'raw\'\n  )\n  vol.cache.enabled = True\n  vol.cache.flush()\n  vol.commit_info()\n\n  vol.cache.compress = None\n  vol[:] = image\n  assert all([ os.path.splitext(x)[1] == \'.gz\' for x in vol.cache.list() ])\n  vol.cache.flush()\n\n  vol.cache.compress = True\n  vol[:] = image\n  assert all([ os.path.splitext(x)[1] == \'.gz\' for x in vol.cache.list() ])\n  vol.cache.flush()\n\n  vol.cache.compress = False\n  vol[:] = image\n  assert all([ os.path.splitext(x)[1] == \'\' for x in vol.cache.list() ])\n  vol.cache.flush()\n\n  delete_layer(dirpath)\n\ndef test_cache_validity():\n  image = np.zeros(shape=(128,128,128,1), dtype=np.uint8)\n  dirpath = \'/tmp/cloudvolume/caching-validity-\' + str(TEST_NUMBER)\n  layer_path = \'file://\' + dirpath\n\n  vol = CloudVolume.from_numpy(\n    image, \n    voxel_offset=(1,1,1), \n    vol_path=layer_path, \n    layer_type=\'image\', \n    resolution=(1,1,1), \n    encoding=\'raw\'\n  )\n  vol.cache.enabled = True\n  vol.cache.flush()\n  vol.commit_info()\n\n  def test_with_mock_cache_info(info, shoulderror):\n    finfo = os.path.join(vol.cache.path, \'info\')\n    with open(finfo, \'w\') as f:\n      f.write(json.dumps(info))\n\n    if shoulderror:\n      try:\n        CloudVolume(vol.cloudpath, cache=True)\n      except ValueError:\n        pass\n      else:\n        assert False\n    else:\n      CloudVolume(vol.cloudpath, cache=True)\n\n  test_with_mock_cache_info(vol.info, shoulderror=False)\n\n  info = vol.info.copy()\n  info[\'scales\'][0][\'size\'][0] = 666\n  test_with_mock_cache_info(info, shoulderror=False)\n\n  test_with_mock_cache_info({ \'zomg\': \'wow\' }, shoulderror=True)\n\n  def tiny_change(key, val):\n    info = vol.info.copy()\n    info[key] = val\n    test_with_mock_cache_info(info, shoulderror=True)\n\n  tiny_change(\'type\', \'zoolander\')\n  tiny_change(\'data_type\', \'uint32\')\n  tiny_change(\'num_channels\', 2)\n  tiny_change(\'mesh\', \'mesh\')\n\n  def scale_change(key, val, mip=0):\n    info = vol.info.copy()\n    info[\'scales\'][mip][key] = val\n    test_with_mock_cache_info(info, shoulderror=True)\n\n  scale_change(\'voxel_offset\', [ 1, 2, 3 ])\n  scale_change(\'resolution\', [ 1, 2, 3 ])\n  scale_change(\'encoding\', \'npz\')\n\n  vol.cache.flush()\n\n  # Test no info file at all    \n  CloudVolume(vol.cloudpath, cache=True)\n\n  vol.cache.flush()\n\ndef test_pickling():\n  import pickle\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  pckl = pickle.dumps(cv)\n  cv2 = pickle.loads(pckl)\n\n  assert cv2.cloudpath == cv.cloudpath\n  assert cv2.mip == cv.mip\n\ndef test_multiprocess():\n  from concurrent.futures import ProcessPoolExecutor, as_completed\n\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(0,0,0))\n  cv.commit_info()\n\n  # ""The ProcessPoolExecutor class has known (unfixable) \n  # problems on Python 2 and should not be relied on \n  # for mission critical work.""\n  # https://pypi.org/project/futures/\n\n  if sys.version_info[0] < 3:\n    print(yellow(""External multiprocessing not supported in Python 2.""))\n    return\n  \n  futures = []\n  with ProcessPoolExecutor(max_workers=4) as ppe:\n    for _ in range(0, 5):\n      futures.append(ppe.submit(cv.refresh_info))\n\n    for future in as_completed(futures):\n      # an error should be re-raised in one of the futures\n      future.result()\n\n  delete_layer()\n\ndef test_exists():\n  # Bbox version\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  defexists = Bbox( (0,0,0), (128,64,64) )\n  results = cv.exists(defexists)\n  assert len(results) == 2\n  assert results[\'1_1_1/0-64_0-64_0-64\'] == True\n  assert results[\'1_1_1/64-128_0-64_0-64\'] == True\n\n  fpath = os.path.join(cv.cloudpath, cv.key, \'64-128_0-64_0-64\')\n  fpath = fpath.replace(\'file://\', \'\') + \'.gz\'\n  os.remove(fpath)\n\n  results = cv.exists(defexists)\n  assert len(results) == 2\n  assert results[\'1_1_1/0-64_0-64_0-64\'] == True\n  assert results[\'1_1_1/64-128_0-64_0-64\'] == False\n\n  # Slice version\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  defexists = np.s_[ 0:128, :, : ]\n\n  results = cv.exists(defexists)\n  assert len(results) == 2\n  assert results[\'1_1_1/0-64_0-64_0-64\'] == True\n  assert results[\'1_1_1/64-128_0-64_0-64\'] == True\n\n  fpath = os.path.join(cv.cloudpath, cv.key, \'64-128_0-64_0-64\')\n  fpath = fpath.replace(\'file://\', \'\') + \'.gz\'\n  os.remove(fpath)\n\n  results = cv.exists(defexists)\n  assert len(results) == 2\n  assert results[\'1_1_1/0-64_0-64_0-64\'] == True\n  assert results[\'1_1_1/64-128_0-64_0-64\'] == False\n\ndef test_delete():\n\n  # Bbox version\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  defexists = Bbox( (0,0,0), (128,64,64) )\n  results = cv.exists(defexists)\n  assert len(results) == 2\n  assert results[\'1_1_1/0-64_0-64_0-64\'] == True\n  assert results[\'1_1_1/64-128_0-64_0-64\'] == True\n\n\n  cv.delete(defexists)\n  results = cv.exists(defexists)\n  assert len(results) == 2\n  assert results[\'1_1_1/0-64_0-64_0-64\'] == False\n  assert results[\'1_1_1/64-128_0-64_0-64\'] == False\n\n  # Slice version\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  defexists = np.s_[ 0:128, :, : ]\n\n  results = cv.exists(defexists)\n  assert len(results) == 2\n  assert results[\'1_1_1/0-64_0-64_0-64\'] == True\n  assert results[\'1_1_1/64-128_0-64_0-64\'] == True\n\n  cv.delete(defexists)\n  results = cv.exists(defexists)\n  assert len(results) == 2\n  assert results[\'1_1_1/0-64_0-64_0-64\'] == False\n  assert results[\'1_1_1/64-128_0-64_0-64\'] == False\n\n  # Check errors\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  try:\n    results = cv.exists( np.s_[1:129, :, :] )\n    print(results)\n  except exceptions.OutOfBoundsError:\n    pass\n  else:\n    assert False\n\ndef test_delete_black_uploads():\n  for parallel in (1,2):\n    delete_layer()\n    cv, _ = create_layer(size=(256,256,256,1), offset=(0,0,0))\n\n    ls = os.listdir(\'/tmp/removeme/layer/1_1_1/\')\n    assert len(ls) == 64\n\n    cv.parallel = parallel\n    cv.delete_black_uploads = True\n    cv[64:64+128,64:64+128,64:64+128] = 0\n\n    ls = os.listdir(\'/tmp/removeme/layer/1_1_1/\')\n    assert len(ls) == (64 - 8) \n\n    cv[64:64+128,64:64+128,64:64+128] = 0\n\n    ls = os.listdir(\'/tmp/removeme/layer/1_1_1/\')\n    assert len(ls) == (64 - 8) \n\n    cv.image.background_color = 1\n    cv[:] = 1 \n    ls = os.listdir(\'/tmp/removeme/layer/1_1_1/\')\n    assert len(ls) == 0\n\n    cv[:] = 0\n    ls = os.listdir(\'/tmp/removeme/layer/1_1_1/\')\n    assert len(ls) == 64\n\n\ndef test_transfer():\n  # Bbox version\n  delete_layer()\n  cv, _ = create_layer(size=(128,64,64,1), offset=(0,0,0))\n\n  cv.transfer_to(\'file:///tmp/removeme/transfer/\', cv.bounds)\n\n  ls = os.listdir(\'/tmp/removeme/transfer/1_1_1/\')\n\n  assert \'0-64_0-64_0-64.gz\' in ls\n  assert len(ls) == 2\n\n  assert os.path.exists(\'/tmp/removeme/transfer/info\')\n  assert os.path.exists(\'/tmp/removeme/transfer/provenance\')\n\n  dcv = CloudVolume(""file:///tmp/removeme/transfer"")\n  dcv.info[""dont_touch_me_bro""] = True\n  dcv.commit_info()\n\n  cv.transfer_to(\'file:///tmp/removeme/transfer/\', cv.bounds)\n  dcv.refresh_info()\n\n  assert \'dont_touch_me_bro\' in dcv.info\n\ndef test_cdn_cache_control():\n  delete_layer()\n  create_layer(size=(128,10,10,1), offset=(0,0,0))\n\n  assert cdn_cache_control(None) == \'max-age=3600, s-max-age=3600\'\n  assert cdn_cache_control(0) == \'no-cache\'\n  assert cdn_cache_control(False) == \'no-cache\'\n  assert cdn_cache_control(True) == \'max-age=3600, s-max-age=3600\'\n\n  assert cdn_cache_control(1337) == \'max-age=1337, s-max-age=1337\'\n  assert cdn_cache_control(\'private, must-revalidate\') == \'private, must-revalidate\'\n\n  try:\n    cdn_cache_control(-1)\n  except ValueError:\n    pass\n  else:\n    assert False\n\ndef test_bbox_to_mip():\n  info = {\n    \'data_type\': \'uint8\',\n    \'mesh\': \'\',\n    \'num_channels\': 1,\n    \'scales\': [\n      { \n        \'chunk_sizes\': [[64, 64, 1]],\n        \'encoding\': \'raw\',\n        \'key\': \'4_4_40\',\n        \'resolution\': [4, 4, 40],\n        \'size\': [1024, 1024, 32],\n        \'voxel_offset\': [35, 0, 1],\n      },\n      {\n        \'chunk_sizes\': [[64, 64, 1]],\n        \'encoding\': \'raw\',\n        \'key\': \'8_8_40\',\n        \'resolution\': [8, 8, 40],\n        \'size\': [512, 512, 32],\n        \'voxel_offset\': [17, 0, 1],\n      },\n      {\n        \'chunk_sizes\': [[64, 64, 1]],\n        \'encoding\': \'raw\',\n        \'key\': \'16_16_40\',\n        \'resolution\': [16, 16, 40],\n        \'size\': [256, 256, 32],\n        \'voxel_offset\': [8, 0, 1],\n      },\n      {\n        \'chunk_sizes\': [[64, 64, 1]],\n        \'encoding\': \'raw\',\n        \'key\': \'32_32_40\',\n        \'resolution\': [32, 32, 40],\n        \'size\': [128, 128, 32],\n        \'voxel_offset\': [4, 0, 1],\n      },\n    ],\n    \'type\': \'image\'\n  }\n  \n  cv = CloudVolume(\'file:///tmp/removeme/bbox_to_mip\', info=info)\n\n  bbox = Bbox( (35,0,1), (1024, 1024, 32))\n  res = cv.bbox_to_mip(bbox, 0, 3)\n  assert res.minpt.x == 4\n  assert res.minpt.y == 0\n  assert res.minpt.z == 1\n\n  bbox = Bbox( (4, 0, 1), (128, 128, 32) )\n  res = cv.bbox_to_mip(bbox, 3, 0)\n  assert res.minpt.x == 32\n  assert res.minpt.y == 0\n  assert res.minpt.z == 1  \n\n  res = cv.bbox_to_mip(bbox, 0, 0)\n  assert res == bbox\n\ndef test_slices_from_global_coords():\n  delete_layer()\n  cv, _ = create_layer(size=(1024, 1024, 5, 1), offset=(7,0,0))\n\n  scale = cv.info[\'scales\'][0]\n  scale = copy.deepcopy(scale)\n  scale[\'voxel_offset\'] = [ 3, 0, 0 ]\n  scale[\'volume_size\'] = [ 512, 512, 5 ]\n  scale[\'resolution\'] = [ 2, 2, 1 ]\n  scale[\'key\'] = \'2_2_1\'\n  cv.info[\'scales\'].append(scale)\n  cv.commit_info()\n\n  assert len(cv.available_mips) == 2\n\n  cv.mip = 1\n  slices = cv.slices_from_global_coords( Bbox( (100, 100, 1), (500, 512, 2) ) )\n  result = Bbox.from_slices(slices)\n  assert result == Bbox( (50, 50, 1), (250, 256, 2) )\n\n  cv.mip = 0\n  slices = cv.slices_from_global_coords( Bbox( (100, 100, 1), (500, 512, 2) ) )\n  result = Bbox.from_slices(slices)\n  assert result == Bbox( (100, 100, 1), (500, 512, 2) )\n\n  slices = cv.slices_from_global_coords( np.s_[:,:,:] )\n  result = Bbox.from_slices(slices)\n  assert result == Bbox( (7, 0, 0), ( 1031, 1024, 5) )\n\n\ndef test_slices_to_global_coords():\n  delete_layer()\n  cv, _ = create_layer(size=(1024, 1024, 5, 1), offset=(7,0,0))\n\n  scale = cv.info[\'scales\'][0]\n  scale = copy.deepcopy(scale)\n  scale[\'voxel_offset\'] = [ 3, 0, 0 ]\n  scale[\'volume_size\'] = [ 512, 512, 5 ]\n  scale[\'resolution\'] = [ 2, 2, 1 ]\n  scale[\'key\'] = \'2_2_1\'\n  cv.info[\'scales\'].append(scale)\n  cv.commit_info()\n\n  assert len(cv.available_mips) == 2\n\n  cv.mip = 1\n  slices = cv.slices_to_global_coords( Bbox( (100, 100, 1), (500, 512, 2) ) )\n\n  result = Bbox.from_slices(slices)\n  assert result == Bbox( (200, 200, 1), (1000, 1024, 2) )\n\n  cv.mip = 0\n  slices = cv.slices_to_global_coords( Bbox( (100, 100, 1), (500, 512, 2) ) )\n  result = Bbox.from_slices(slices)\n  assert result == Bbox( (100, 100, 1), (500, 512, 2) )\n\n# def test_boss_download():\n#   vol = CloudVolume(\'gs://seunglab-test/test_v0/image\')\n#   bossvol = CloudVolume(\'boss://automated_testing/test_v0/image\')\n\n#   vimg = vol[:,:,:5]\n#   bimg = bossvol[:,:,:5]\n\n#   assert np.all(bimg == vimg)\n#   assert bimg.dtype == vimg.dtype\n\n#   vol.bounded = False\n#   vol.fill_missing = True\n#   bossvol.bounded = False\n#   bossvol.fill_missing = True\n\n#   assert np.all(vol[-100:100,-100:100,-10:10] == bossvol[-100:100,-100:100,-10:10])\n\n#   # BOSS using a different algorithm for creating downsamples\n#   # so hard to compare 1:1 w/ pixels.\n#   bossvol.bounded = True\n#   bossvol.fill_missing = False\n#   bossvol.mip = 1\n#   bimg = bossvol[:,:,5:6]\n#   assert np.any(bimg > 0)\n\ndef test_redirects():\n  info = CloudVolume.create_new_info(\n    num_channels=1, # Increase this number when we add more tests for RGB\n    layer_type=\'image\', \n    data_type=\'uint8\', \n    encoding=\'raw\',\n    resolution=[ 1,1,1 ], \n    voxel_offset=[0,0,0], \n    volume_size=[128,128,64],\n    mesh=\'mesh\', \n    chunk_size=[ 64,64,64 ],\n  )\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/redirects_0\', mip=0, info=info)\n  vol.commit_info()\n  vol.refresh_info()\n\n  vol.info[\'redirect\'] = \'file:///tmp/cloudvolume/redirects_0\'\n  vol.commit_info()\n  vol.refresh_info()\n\n  del vol.info[\'redirect\']\n\n  for i in range(0, 10):\n    info[\'redirect\'] = \'file:///tmp/cloudvolume/redirects_\' + str(i + 1)  \n    vol = CloudVolume(\'file:///tmp/cloudvolume/redirects_\' + str(i), mip=0, info=info)\n    vol.commit_info()\n  else:\n    del vol.info[\'redirect\']\n    vol.commit_info()\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/redirects_0\', mip=0)\n\n  assert vol.cloudpath == \'file:///tmp/cloudvolume/redirects_9\'\n\n  info[\'redirect\'] = \'file:///tmp/cloudvolume/redirects_10\'\n  vol = CloudVolume(\'file:///tmp/cloudvolume/redirects_9\', mip=0, info=info)\n  vol.commit_info()\n\n  try:\n    CloudVolume(\'file:///tmp/cloudvolume/redirects_0\', mip=0)  \n    assert False \n  except exceptions.TooManyRedirects:\n    pass\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/redirects_9\', max_redirects=0)\n  del vol.info[\'redirect\']\n  vol.commit_info()\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/redirects_5\', max_redirects=0)  \n  vol.info[\'redirect\'] = \'file:///tmp/cloudvolume/redirects_1\'\n  vol.commit_info()\n\n  try:\n    vol = CloudVolume(\'file:///tmp/cloudvolume/redirects_5\')\n    assert False\n  except exceptions.CyclicRedirect:\n    pass\n\n  vol.info[\'redirect\'] = \'file:///tmp/cloudvolume/redirects_6\'\n  vol.commit_info()\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/redirects_1\')\n\n  try:\n    vol[:,:,:] = 1\n    assert False \n  except exceptions.ReadOnlyException:\n    pass\n\n  for i in range(0, 10):\n    delete_layer(\'/tmp/cloudvolume/redirects_\' + str(i))  \n  \n@pytest.mark.parametrize(""compress_method"", (\'\', None, True, False, \'gzip\', \'br\'))\ndef test_compression_methods(compress_method):\n  path = ""file:///tmp/cloudvolume/test"" + \'-\' + str(TEST_NUMBER)\n\n\n  # create a NG volume\n  info = CloudVolume.create_new_info(\n      num_channels=1,\n      layer_type=""image"",\n      data_type=""uint16"",  # Channel images might be \'uint8\'\n      encoding=""raw"",  # raw, jpeg, compressed_segmentation, fpzip, kempressed\n      resolution=[4, 4, 40],  # Voxel scaling, units are in nanometers\n      voxel_offset=[0, 0, 0],  # x,y,z offset in voxels from the origin\n      chunk_size=[512, 512, 16],  # units are voxels\n      volume_size=[512 * 4, 512 * 4, 16 * 40],  \n  )\n  vol = CloudVolume(path, info=info, compress=compress_method)\n  vol.commit_info()\n\n  image = np.random.randint(low=0, high=2 ** 16 - 1, size=(512, 512, 16, 1), dtype=""uint16"")\n\n  vol[0:512, 0:512, 0:16] = image\n\n  image_test = vol[0:512, 0:512, 0:16]\n  \n  delete_layer(\'/tmp/cloudvolume/test\' + \'-\' + str(TEST_NUMBER))\n\n  assert vol.compress == compress_method\n  assert np.array_equal(image_test, image)\n\ndef test_mip_locking():\n  delete_layer()\n  cv, _ = create_layer(size=(1024, 1024, 2, 1), offset=(0,0,0))\n\n  cv.meta.lock_mips(0)\n  cv.meta.lock_mips([0])\n\n  try:\n    cv[:,:,:] = 0\n    assert False \n  except ReadOnlyException:\n    pass \n\n  cv.meta.unlock_mips(0)\n  cv.meta.unlock_mips([0])\n\n  cv[:,:,:] = 0\n\n  try:\n    cv.meta.lock_mips(1)\n    assert False \n  except ValueError:\n    pass \n\n  try:\n    cv.meta.unlock_mips(1)\n    assert False \n  except ValueError:\n    pass \n\n  cv.add_scale((2,2,1))\n  cv.commit_info()\n  cv.mip = 1 \n  cv[:] = 1\n\n  cv.meta.lock_mips([0,1])\n\n  try:\n    cv[:,:,:] = 1\n    assert False \n  except ReadOnlyException:\n    pass \n\n  cv.mip = 0\n\n  try:\n    cv[:,:,:] = 1\n    assert False \n  except ReadOnlyException:\n    pass \n'"
test/test_compression.py,4,"b'import pytest\n\nimport numpy as np\n\nfrom cloudvolume.exceptions import DecompressionError\nfrom cloudvolume.compression import compress, decompress\n\n@pytest.mark.parametrize(""compression_method"", (""gzip"", ""br""))\ndef test_compression(compression_method):\n  for N in range(100):\n    flts = np.array(range(N), dtype=np.float32).reshape( (N,1,1,1) ).tostring()\n    compressed = compress(flts, compression_method)\n    assert compressed != flts\n    decompressed = decompress(compressed, compression_method)\n    assert decompressed == flts\n\n  flts = np.array([], dtype=np.float32).tobytes()\n  try:\n    decompress(flts, compression_method)\n    assert False\n  except DecompressionError:\n    pass\n\n@pytest.mark.parametrize(""compression_method"", (""gzip"", ""br""))\ndef test_compress_level(compression_method):\n  N = 10000\n  x = np.array(range(N), dtype=np.float32).reshape( (N, 1, 1, 1) )\n  content = np.ascontiguousarray(x, dtype=np.float32).tostring()\n\n  compr_rate = []\n  compress_levels = (1, 8)\n  for compress_level in compress_levels:\n    print(compress_level)\n    compressed = compress(content, compression_method, compress_level=compress_level)\n\n    assert compressed != content\n    decompressed = decompress(compressed, compression_method)\n    assert decompressed == content\n\n    compr_rate.append(float(len(compressed)) / float(len(content)))\n\n  # make sure we get better compression at highest level than lowest level\n  assert compr_rate[-1] < compr_rate[0]'"
test/test_connectionpools.py,0,"b""from __future__ import print_function\nfrom six.moves import range\n\nimport tenacity\nfrom tqdm import tqdm\n\nfrom cloudvolume.connectionpools import S3ConnectionPool, GCloudBucketPool\nfrom cloudvolume.threaded_queue import ThreadedQueue\nfrom cloudvolume.storage import Storage\n\nS3_POOL = S3ConnectionPool('s3', 'seunglab-test')\nGC_POOL = GCloudBucketPool('seunglab-test')\n\nretry = tenacity.retry(\n    reraise=True, \n    stop=tenacity.stop_after_attempt(7), \n    wait=tenacity.wait_full_jitter(0.5, 60.0),\n)\n\ndef test_gc_stresstest():\n  with Storage('gs://seunglab-test/cloudvolume/connection_pool/', n_threads=0) as stor:\n    stor.put_file('test', 'some string')\n\n  n_trials = 500\n  pbar = tqdm(total=n_trials)\n\n  @retry\n  def create_conn(interface):\n    # assert GC_POOL.total_connections() <= GC_POOL.max_connections * 5\n    bucket = GC_POOL.get_connection()\n    blob = bucket.get_blob('cloudvolume/connection_pool/test')\n    blob.download_as_string()\n    GC_POOL.release_connection(bucket)\n    pbar.update()\n\n  with ThreadedQueue(n_threads=20) as tq:\n    for _ in range(n_trials):\n      tq.put(create_conn)\n\n  pbar.close()\n\ndef test_s3_stresstest():\n  with Storage('s3://seunglab-test/cloudvolume/connection_pool/', n_threads=0) as stor:\n    stor.put_file('test', 'some string')\n\n  n_trials = 500\n  pbar = tqdm(total=n_trials)\n\n  @retry\n  def create_conn(interface):\n    conn = S3_POOL.get_connection()  \n    # assert S3_POOL.total_connections() <= S3_POOL.max_connections * 5\n    bucket = conn.get_object(\n      Bucket='seunglab-test',\n      Key='cloudvolume/connection_pool/test',\n    )\n    S3_POOL.release_connection(conn)\n    pbar.update()\n\n  with ThreadedQueue(n_threads=20) as tq:\n    for _ in range(n_trials):\n      tq.put(create_conn)\n\n  pbar.close()\n"""
test/test_dask.py,1,"b'import os\nimport pytest\nimport sys\n\nimport numpy as np\n\nfrom cloudvolume import dask as dasklib\n\n@pytest.mark.skipif(sys.version_info[0] < 3, reason=""Python 2 not supported."")\ndef test_roundtrip_3d():\n  da = pytest.importorskip(\'dask.array\')\n  du = pytest.importorskip(\'dask.utils\')\n  a = da.zeros((3, 3, 3), chunks=1)\n  with du.tmpdir() as d:\n    d = \'file://\' + d\n    dasklib.to_cloudvolume(a, d)\n    a2 = dasklib.from_cloudvolume(d)\n    a2 = da.squeeze(a2, axis=3)\n    da.utils.assert_eq(a, a2, check_meta=False)  # TODO: add meta check\n    assert a.chunks == a2.chunks\n\n@pytest.mark.skipif(sys.version_info[0] < 3, reason=""Python 2 not supported."")\ndef test_roundtrip_4d():\n  da = pytest.importorskip(\'dask.array\')\n  du = pytest.importorskip(\'dask.utils\')\n  a = da.zeros((3, 3, 3, 3))\n  with du.tmpdir() as d:\n    d = \'file://\' + d\n    dasklib.to_cloudvolume(a, d)\n    a2 = dasklib.from_cloudvolume(d)\n    da.utils.assert_eq(a, a2, check_meta=False)  # TODO: add meta check\n    assert a.chunks == a2.chunks\n\n@pytest.mark.skipif(sys.version_info[0] < 3, reason=""Python 2 not supported."")\ndef test_roundtrip_4d_channel_rechunked():\n  da = pytest.importorskip(\'dask.array\')\n  du = pytest.importorskip(\'dask.utils\')\n  a = da.zeros((3, 3, 3, 3), chunks=(2, 2, 2, 3))\n  with du.tmpdir() as d:\n    d = \'file://\' + d\n    dasklib.to_cloudvolume(a, d)\n    a2 = dasklib.from_cloudvolume(d)\n    np.testing.assert_array_equal(a.compute(), a2.compute())\n    # Channel has single chunk\n    assert a2.chunks == ((2, 1), (2, 1), (2, 1), (3, ))\n\n@pytest.mark.skipif(sys.version_info[0] < 3, reason=""Python 2 not supported."")\ndef test_roundtrip_4d_1channel():\n  da = pytest.importorskip(\'dask.array\')\n  du = pytest.importorskip(\'dask.utils\')\n  a = da.zeros((3, 3, 3, 1))\n  with du.tmpdir() as d:\n    d = \'file://\' + d\n    dasklib.to_cloudvolume(a, d)\n    a2 = dasklib.from_cloudvolume(d)\n    da.utils.assert_eq(a, a2, check_meta=False)  # TODO: add meta check\n    assert a.chunks == a2.chunks\n\n@pytest.mark.skipif(sys.version_info[0] < 3, reason=""Python 2 not supported."")\ndef test_roundtrip_rechunk_3d():\n  da = pytest.importorskip(\'dask.array\')\n  du = pytest.importorskip(\'dask.utils\')\n  a = da.zeros((9, 9, 9), chunks=3)\n  with du.tmpdir() as d:\n    d = \'file://\' + d\n    dasklib.to_cloudvolume(a, d)\n    a2 = dasklib.from_cloudvolume(d, chunks=5)\n    assert a2.chunks == ((5, 4), (5, 4), (5, 4), (1, ))\n\n@pytest.mark.skipif(sys.version_info[0] < 3, reason=""Python 2 not supported."")\ndef test_roundtrip_rechunk_4d():\n  da = pytest.importorskip(\'dask.array\')\n  du = pytest.importorskip(\'dask.utils\')\n  a = da.zeros((9, 9, 9, 3), chunks=3)\n  with du.tmpdir() as d:\n    d = \'file://\' + d\n    dasklib.to_cloudvolume(a, d)\n    a2 = dasklib.from_cloudvolume(d, chunks=5)\n    assert a2.chunks == ((5, 4), (5, 4), (5, 4), (3, ))\n\n@pytest.mark.skipif(sys.version_info[0] < 3, reason=""Python 2 not supported."")\ndef test_delayed_compute():\n  dask = pytest.importorskip(\'dask\')\n  da = pytest.importorskip(\'dask.array\')\n  dd = pytest.importorskip(\'dask.delayed\')\n  du = pytest.importorskip(\'dask.utils\')\n  a = da.zeros((3, 3, 3, 1), chunks=1)\n  with du.tmpdir() as d:\n    d = \'file://\' + d\n    out = dasklib.to_cloudvolume(a, d, compute=False)\n    assert isinstance(out, dd.Delayed)\n    dask.compute(out)\n    a2 = dasklib.from_cloudvolume(d)\n    da.utils.assert_eq(a, a2, check_meta=False)  # TODO: add meta check\n    assert a.chunks == a2.chunks\n\n@pytest.mark.skipif(sys.version_info[0] < 3, reason=""Python 2 not supported."")\ndef test_roundtrip_delayed():\n  da = pytest.importorskip(\'dask.array\')\n  du = pytest.importorskip(\'dask.utils\')\n  a = da.zeros((3, 3, 3, 3))\n  with du.tmpdir() as d:\n    cloudpath = \'file://\' + d\n    task = dasklib.to_cloudvolume(a, cloudpath, compute=False)\n    assert not os.listdir(d)\n    task.compute()\n    a2 = dasklib.from_cloudvolume(cloudpath)\n    da.utils.assert_eq(a, a2, check_meta=False)  # TODO: add meta check\n    assert a.chunks == a2.chunks\n'"
test/test_graphene.py,17,"b'from functools import partial\n\nimport tempfile\nimport cloudvolume\nimport numpy as np\nimport shutil\nimport posixpath\nimport pytest\nimport os\nfrom scipy import sparse \nimport sys\n\ntempdir = tempfile.mkdtemp()\nTEST_PATH = ""file://{}"".format(tempdir)\nTEST_DATASET_NAME = ""testvol""\nPRECOMPUTED_MESH_TEST_DATASET_NAME = ""meshvol_precompute""\nDRACO_MESH_TEST_DATASET_NAME = ""meshvol_draco""\nPCG_LOCATION = ""https://www.dynamicannotationframework.com/""\nTEST_SEG_ID = 144115188084020434\n\n@pytest.fixture()\ndef cv_graphene_mesh_precomputed(requests_mock):\n  test_dir = os.path.dirname(os.path.abspath(__file__))\n  graphene_test_cv_dir = os.path.join(test_dir,\'test_cv\')\n  graphene_test_cv_path = ""file://{}"".format(graphene_test_cv_dir)\n\n  info_d = {\n    ""data_dir"": graphene_test_cv_path,\n    ""data_type"": ""uint64"",\n    ""graph"": {\n      ""chunk_size"": [\n        512,\n        512,\n        128\n      ],\n      ""n_layers"": 9,\n      ""n_bits_for_layer_id"": 8,\n      ""spatial_bit_masks"": {\n        x: 10 for x in range(255)\n      },\n    },\n    ""chunks_start_at_voxel_offset"": False,\n    ""mesh"": ""mesh_mip_2_err_40_sv16"",\n    ""num_channels"": 1,\n    ""scales"": [{\n      ""chunk_sizes"": [[ 512, 512, 16 ]],\n      ""compressed_segmentation_block_size"": [\n        8,\n        8,\n        8\n      ],\n      ""encoding"": ""compressed_segmentation"",\n      ""key"": ""8_8_40"",\n      ""resolution"": [\n        8,\n        8,\n        40\n      ],\n      ""size"": [\n        43520,\n        26112,\n        2176\n      ],\n      ""voxel_offset"": [\n        17920,\n        14848,\n        0\n      ]\n    }],\n    ""type"": ""segmentation""\n  }\n  requests_mock.get(PCG_LOCATION+PRECOMPUTED_MESH_TEST_DATASET_NAME+""/info"", json=info_d)\n  \n  frag_files = os.listdir(os.path.join(graphene_test_cv_dir, info_d[\'mesh\']))\n  # the file are saved as .gz but we want to list the non gz version\n  # as cloudvolume will take care of finding the compressed files\n  frag_files = [f[:-3] for f in frag_files if f[0]==\'9\']\n  frag_d = {\'fragments\':frag_files}\n  mock_url = PCG_MESH_LOCATION + PRECOMPUTED_MESH_TEST_DATASET_NAME+""/manifest/{}:0?verify=True"".format(TEST_SEG_ID)\n  requests_mock.get(mock_url, json=frag_d)\n\n  cloudpath = ""graphene://{}{}"".format(PCG_LOCATION, PRECOMPUTED_MESH_TEST_DATASET_NAME)\n  yield cloudvolume.CloudVolume(cloudpath)\n\n@pytest.fixture()\ndef cv_graphene_mesh_draco(requests_mock):\n  test_dir = os.path.dirname(os.path.abspath(__file__))\n  graphene_test_cv_dir = os.path.join(test_dir,\'test_cv\')\n  graphene_test_cv_path = ""file://{}"".format(graphene_test_cv_dir)\n\n  info_d = {\n    ""data_dir"": graphene_test_cv_path,\n    ""data_type"": ""uint64"",\n    ""graph"": {\n      ""chunk_size"": [\n        512,\n        512,\n        128\n      ],\n      ""n_layers"": 9,\n      ""n_bits_for_layer_id"": 8,\n      ""spatial_bit_masks"": {\n        x: 10 for x in range(255)\n      },\n    },\n    ""chunks_start_at_voxel_offset"": False,\n    ""mesh"": ""mesh_mip_2_draco_sv16"",\n    ""mesh_metadata"": {\n      ""max_meshed_layer"": 6,\n      ""uniform_draco_grid_size"": 21\n    },\n    ""num_channels"": 1,\n    ""scales"": [{\n      ""chunk_sizes"": [[ 512, 512, 16 ]],\n      ""compressed_segmentation_block_size"": [\n        8,\n        8,\n        8\n      ],\n      ""encoding"": ""compressed_segmentation"",\n      ""key"": ""8_8_40"",\n      ""resolution"": [\n        8,\n        8,\n        40\n      ],\n      ""size"": [\n        43520,\n        26112,\n        2176\n      ],\n      ""voxel_offset"": [\n        17920,\n        14848,\n        0\n      ]\n    }],\n    ""type"": ""segmentation""\n  }\n  infourl = posixpath.join(PCG_LOCATION, \'meshing/table\', DRACO_MESH_TEST_DATASET_NAME, ""info"")\n  requests_mock.get(infourl, json=info_d)\n  \n  frag_files = os.listdir(os.path.join(graphene_test_cv_dir, info_d[\'mesh\']))\n  # we want to filter out the manifest file\n  frag_files = [ f for f in frag_files if f[0] == \'1\' ]\n  frag_d = { \'fragments\': frag_files }\n  mock_url = posixpath.join(\n    PCG_LOCATION, \'meshing/api/v1/table\', \n    DRACO_MESH_TEST_DATASET_NAME, \n    ""manifest/{}:0?verify=True"".format(TEST_SEG_ID)\n  )\n  requests_mock.get(mock_url, json=frag_d)\n  \n  cloudpath = posixpath.join(PCG_LOCATION, \'meshing/api/v1\', DRACO_MESH_TEST_DATASET_NAME)\n  yield cloudvolume.CloudVolume(\'graphene://\' + cloudpath)\n\n@pytest.fixture(scope=\'session\')\ndef cv_supervoxels(N=64, blockN=16):\n\n  block_per_row = int(N / blockN)\n\n  chunk_size = [32, 32, 32]\n  info = cloudvolume.CloudVolume.create_new_info(\n    num_channels=1,\n    layer_type=\'segmentation\',\n    data_type=\'uint64\',\n    encoding=\'raw\',\n    resolution=[4, 4, 40],  # Voxel scaling, units are in nanometers\n    voxel_offset=[0, 0, 0],  # x,y,z offset in voxels from the origin\n    # Pick a convenient size for your underlying chunk representation\n    # Powers of two are recommended, doesn\'t need to cover image exactly\n    chunk_size=chunk_size,  # units are voxels\n    volume_size=[N, N, N],\n  )\n\n  vol = cloudvolume.CloudVolume(TEST_PATH, info=info)\n  vol.commit_info()\n  xx, yy, zz = np.meshgrid(*[np.arange(0, N) for cs in chunk_size])\n  id_ind = (\n    np.uint64(xx / blockN),\n    np.uint64(yy / blockN),\n    np.uint64(zz / blockN)\n  )\n  id_shape = (block_per_row, block_per_row, block_per_row)\n\n  seg = np.ravel_multi_index(id_ind, id_shape)\n  vol[:] = np.uint64(seg)\n\n  yield TEST_PATH\n\n  shutil.rmtree(tempdir)\n\n\n@pytest.fixture()\ndef graphene_vol(cv_supervoxels,  requests_mock, monkeypatch, N=64):\n\n  chunk_size = [32, 32, 32]\n\n  info_d = {\n    ""data_dir"": cv_supervoxels,\n    ""data_type"": ""uint64"",\n    ""graph"": {\n      ""chunk_size"": [64, 64, 64]\n    },\n    ""mesh"": ""mesh_mip_2_err_40_sv16"",\n    ""num_channels"": 1,\n    ""scales"": [\n      {\n        ""chunk_sizes"": [\n          [32, 32, 32]\n        ],\n        ""compressed_segmentation_block_size"": [8, 8, 8],\n        ""encoding"": ""compressed_segmentation"",\n        ""key"": ""4_4_40"",\n        ""resolution"": [4, 4, 40],\n        ""size"": [N, N, N],\n        ""voxel_offset"": [0, 0, 0]\n      }\n    ],\n    ""type"": ""segmentation""\n  }\n\n  infourl = posixpath.join(PCG_LOCATION, \'segmentation/table\', TEST_DATASET_NAME, ""info"")\n  requests_mock.get(infourl, json=info_d)\n  \n  def mock_get_leaves(self, root_id, bbox, mip):\n    return np.array([0,1,2,3], dtype=np.uint64)\n\n  cloudpath = ""graphene://"" + posixpath.join(PCG_LOCATION, \'segmentation\', \'api/v1/\', TEST_DATASET_NAME)\n\n  gcv = cloudvolume.CloudVolume(cloudpath)\n  gcv.get_leaves = partial(mock_get_leaves, gcv)\n  yield gcv\n\ndef test_gcv(graphene_vol):\n  cutout = graphene_vol.download(np.s_[0:5,0:5,0:5], segids=[999])\n  assert (np.all(cutout==999))\n  cutout_sv = graphene_vol[0:5,0:5,0:5]\n  assert cutout_sv.shape == (5,5,5,1)\n  assert graphene_vol[0,0,0].shape == (1,1,1,1)\n\n\ndef faces_to_edges(faces, return_index=False):\n  """"""\n  Given a list of faces (n,3), return a list of edges (n*3,2)\n  Parameters\n  -----------\n  faces : (n, 3) int\n    Vertex indices representing faces\n  Returns\n  -----------\n  edges : (n*3, 2) int\n    Vertex indices representing edges\n  """"""\n  faces = np.asanyarray(faces)\n\n  # each face has three edges\n  edges = faces[:, [0, 1, 1, 2, 2, 0]].reshape((-1, 2))\n\n  if return_index:\n    # edges are in order of faces due to reshape\n    face_index = np.tile(np.arange(len(faces)),\n               (3, 1)).T.reshape(-1)\n    return edges, face_index\n  return edges\n\n  \ndef create_csgraph(vertices, edges, euclidean_weight=True, directed=False):\n  \'\'\'\n  Builds a csr graph from vertices and edges, with optional control\n  over weights as boolean or based on Euclidean distance.\n  \'\'\'\n  if euclidean_weight:\n    xs = vertices[edges[:,0]]\n    ys = vertices[edges[:,1]]\n    weights = np.linalg.norm(xs-ys, axis=1)\n    use_dtype = np.float32\n  else:   \n    weights = np.ones((len(edges),)).astype(np.int8)\n    use_dtype = np.int8 \n\n  if directed:\n    edges = edges.T\n  else:\n    edges = np.concatenate([edges.T, edges.T[[1, 0]]], axis=1)\n    weights = np.concatenate([weights, weights]).astype(dtype=use_dtype)\n\n  csgraph = sparse.csr_matrix((weights, edges),\n                shape=[len(vertices), ] * 2,\n                dtype=use_dtype)\n\n  return csgraph\n\n@pytest.mark.skipif(sys.version_info < (3, 0), reason=""requires python3 or higher"")\ndef test_decode_segid(cv_graphene_mesh_draco):\n  decoded = cv_graphene_mesh_draco.meta.decode_label(648518346349515986)\n  assert decoded.level == 9\n  assert decoded.x == 0\n  assert decoded.y == 0\n  assert decoded.z == 0\n  assert decoded.segid == 8164562\n\n  level = \'00000101\' # 5\n  x = \'0000000001\' # 1 \n  y = \'0000000010\' # 2 \n  z = \'0000000011\' # 3\n  segid = \'00000000000000000000001010\' # 10\n\n  label = int(level + x + y + z + segid, 2)\n  decoded = cv_graphene_mesh_draco.meta.decode_label(label)\n  \n  assert decoded.level == 5\n  assert decoded.x == 1\n  assert decoded.y == 2\n  assert decoded.z == 3\n  assert decoded.segid == 10\n\n  encoded = cv_graphene_mesh_draco.meta.encode_label(*decoded)\n  assert decoded == cv_graphene_mesh_draco.meta.decode_label(encoded)\n\n\n@pytest.mark.skipif(sys.version_info < (3, 0), reason=""requires python3 or higher"")\ndef test_graphene_mesh_get(cv_graphene_mesh_precomputed):\n\n  mesh = cv_graphene_mesh_precomputed.mesh.get(TEST_SEG_ID)\n  edges = faces_to_edges(mesh[TEST_SEG_ID].faces)\n  graph = create_csgraph(mesh[TEST_SEG_ID].vertices,\n               edges,\n               directed=False)\n  ccs, labels =  sparse.csgraph.connected_components(graph,\n                             directed=False)\n  assert(ccs==3)\n\n@pytest.mark.skipif(sys.version_info < (3, 0), reason=""requires python3 or higher"")\ndef test_graphene_mesh_get(cv_graphene_mesh_draco):\n\n  mesh = cv_graphene_mesh_draco.mesh.get(TEST_SEG_ID)\n  edges = faces_to_edges(mesh[TEST_SEG_ID].faces)\n  graph = create_csgraph(mesh[TEST_SEG_ID].vertices,\n               edges,\n               directed=False)\n  ccs, labels =  sparse.csgraph.connected_components(graph,\n                             directed=False)\n  assert(ccs==3)\n\n'"
test/test_lib.py,15,"b'import pytest\n\nimport numpy as np\n\nimport cloudvolume.lib as lib\nfrom cloudvolume.lib import Bbox, Vec\n\ndef test_divisors():\n\n  divisors = (\n    (1, (1,)), \n    (2, (1,2)),\n    (3, (1,3)),\n    (4, (1,2,4)),\n    (6, (1,2,3,6)),\n    (35, (1,5,7,35)),\n    (128, (1,2,4,8,16,32,64,128)),\n    (258, (1,2,3,6,43,86,129,258)),\n  )\n\n  for num, ans in divisors:\n    result = [ _ for _ in lib.divisors(num) ]\n    result.sort()\n    assert tuple(result) == ans\n\ndef test_find_closest_divisor():\n  size = lib.find_closest_divisor( (128,128,128), (64,64,64) )\n  assert tuple(size) == (64,64,64)\n\n  size = lib.find_closest_divisor( (240,240,240), (64,64,64) )\n  assert tuple(size) == (60,60,60)\n\n  size = lib.find_closest_divisor( (224,224,224), (64,64,64) )\n  assert tuple(size) == (56,56,56)\n\n  size = lib.find_closest_divisor( (73,73,73), (64,64,64) )\n  assert tuple(size) == (73,73,73)\n\ndef test_bbox_subvoxel():\n  bbox = Bbox( (0,0,0), (1,1,1), dtype=np.float32)\n  \n  assert not bbox.subvoxel()\n  assert not bbox.empty()\n\n  bbox.maxpt[:] *= -1\n  bbox.maxpt.z = 20\n\n  # pathological case\n  assert not (bbox.volume() < 1)\n  assert bbox.subvoxel()\n  assert bbox.empty()\n\n  bbox = Bbox( (1,1,1), (1,1,1) )\n  assert bbox.empty()\n\n  bbox = Bbox( (0,0,0), (0.9, 1.0, 1.0) )\n  assert bbox.subvoxel()\n  assert not bbox.empty()\n\ndef test_vec_division():\n  vec = Vec(2,4,8)\n  assert np.all( (vec/2) == Vec(1,2,4) )\n\n\ndef test_bbox_division():\n  box = Bbox( (0,2,4), (4,8,16) )\n  assert (box//2) == Bbox( (0,1,2), (2,4,8) )\n\n  box = Bbox( (0,3,4), (4,8,16), dtype=np.float32 )\n  print((box/2.))\n  print(Bbox( (0., 1.5, 2.), (2., 4., 8.) ))\n  assert (box/2.) == Bbox( (0., 1.5, 2.), (2., 4., 8.) )\n\ndef test_bbox_intersect():\n  box = Bbox( (0,0,0), (10, 10, 10) )\n  \n  assert Bbox.intersects(box, box)\n  assert Bbox.intersects(box, Bbox((1,1,1), (11,11,11)) )\n  assert Bbox.intersects(box, Bbox((-1,-1,-1), (9,9,9)) ) \n  assert Bbox.intersects(box, Bbox((5, -5, 0), (15, 5, 10)))\n  assert not Bbox.intersects(box, Bbox( (30,30,30), (40,40,40) ))\n  assert not Bbox.intersects(box, Bbox( (-30,-30,-30), (-40,-40,-40) ))\n  assert not Bbox.intersects(box, Bbox( (10, 0, 0), (20, 10, 10) ))\n\ndef test_bbox_division():\n  bbox = Bbox( (1,1,1), (10, 10, 10), dtype=np.float32 )\n  bbox2 = bbox.clone()\n\n  bbox /= 3.0\n  bbx3 = bbox2 / 3.0\n\n  point333 = np.float32(1) / np.float32(3)\n\n  assert np.all( np.abs(bbx3.minpt - point333) < 1e-6 )\n  assert np.all( bbx3.maxpt == np.float32(3) + point333 )\n  assert bbox == bbx3\n\n  bbox = Bbox( (1,1,1), (10, 10, 10), dtype=np.float32 )\n\n  x = bbox.minpt \n  bbox /= 3.0 \n  assert np.all(x == point333)\n\n\n\ndef test_bbox_intersection():\n  bbx1 = Bbox( (0,0,0), (10,10,10) )\n  bbx2 = Bbox( (5,5,5), (15,15,15) )\n\n  assert Bbox.intersection(bbx1, bbx2) == Bbox((5,5,5),(10,10,10))\n  assert Bbox.intersection(bbx2, bbx1) == Bbox((5,5,5),(10,10,10))\n  bbx2.minpt = Vec(11,11,11)\n  assert Bbox.intersection(bbx1, bbx2) == Bbox((0,0,0),(0,0,0))\n\ndef test_bbox_hashing():\n  bbx = Bbox.from_list([ 1,2,3,4,5,6 ])\n  d = {}\n  d[bbx] = 1\n\n  assert len(d) == 1\n  for _,v in d.items():\n    assert v == 1\n\n  bbx = Bbox( (1., 1.3, 2.), (3., 4., 4.) )\n  d = {}\n  d[bbx] = 1\n\n  assert len(d) == 1\n  for _,v in d.items():\n    assert v == 1\n\ndef test_bbox_serialize():\n  bbx = Bbox( (24.125, 2512.2, 2112.3), (33.,532., 124.12412), dtype=np.float32)\n\n  reconstituted = Bbox.deserialize(bbx.serialize())\n  assert bbx == reconstituted\n\ndef test_bbox_volume():\n  bbx = Bbox( (0,0,0), (2000, 2000, 2000) )\n  # important thing is 8B is > int32 size\n  assert bbx.volume() == 8000000000\n\n  bbx = bbx.astype(np.float32)\n  assert bbx.volume() == 8000000000\n\ndef test_jsonify():\n  obj = {\n    \'x\': [ np.array([1,2,3,4,5], dtype=np.uint64) ],\n    \'y\': [ {}, {} ],\n    \'z\': np.int32(5),\n    \'w\': \'1 2 34 5\'\n  }\n\n  assert lib.jsonify(obj, sort_keys=True) == r""""""{""w"": ""1 2 34 5"", ""x"": [[1, 2, 3, 4, 5]], ""y"": [{}, {}], ""z"": 5}""""""\n\n\ndef test_bbox_from_filename():\n  filenames = [\n    ""0-512_0-512_0-16"",\n    ""0-512_0-512_0-16.gz"",\n    ""0-512_0-512_0-16.br"",\n  ]\n\n  for fn in filenames:\n    bbox = Bbox.from_filename(fn)\n    assert np.array_equal(bbox.minpt, [0, 0, 0])\n    assert np.array_equal(bbox.maxpt, [512, 512, 16])\n\n  filenames = [\n    ""gibberish"",\n    ""0-512_0-512_0-16.lol"",\n    ""0-512_0-512_0-16.gzip"",\n    ""0-512_0-512_0-16.brotli"",\n    ""0-512_0-512_0-16.na"",\n    ""0-512_0-512_0-abc"",\n    ""0-512_0-abc_0-16"",\n    ""0-abc_0-512_0-16"",\n  ]\n  for fn in filenames:\n    with pytest.raises(ValueError):\n      bbox = Bbox.from_filename(fn)\n'"
test/test_lru.py,0,"b'import pytest\n\nfrom cloudvolume.lru import LRU\n\nimport time\nimport random\n\ndef test_lru():\n  lru = LRU(5)\n\n  assert len(lru) == 0\n  for i in range(5):\n    lru[i] = i\n  assert len(lru) == 5\n\n  for i in range(5):\n    lru[i] = i\n\n  for i in range(100):\n    lru[i] = i\n\n  assert len(lru) == 5\n\n  lru.resize(10)\n  for i in range(5):\n    lru[i] = i\n\n  assert lru.queue.tolist() == [ \n    (4,4), (3,3), (2,2), (1,1), (0,0),\n    (99,99), (98,98), (97,97), (96,96), (95,95)\n  ]\n\n  lru.resize(5)\n  assert lru.queue.tolist() == [ \n    (4,4), (3,3), (2,2), (1,1), (0,0)\n  ]\n\n  assert lru[0] == 0\n  assert lru.queue.tolist() == [ \n    (0,0), (4,4), (3,3), (2,2), (1,1)\n  ]\n\ndef test_lru_chaos():\n  lru = LRU(10)\n\n  seed = time.time()\n  print(""seed"", seed)\n\n  random.seed(seed)\n  for i in range(150):\n    rand = random.randint(-5000, 5000)\n    lru[rand] = random.randint(-5000, 5000)\n\n    if rand % 17 == 0:\n      keys = lru.keys()\n      key = random.choice(list(keys))\n      lru[key]\n\n\n\n\n\n\n'"
test/test_meshing.py,18,"b'import numpy as np\nimport pytest \n\nfrom cloudvolume import Vec, Bbox, CloudVolume, Storage, Mesh\n\n@pytest.mark.parametrize((""use_https""),[True, False])\ndef test_mesh_fragment_download(use_https):\n  vol = CloudVolume(\'gs://seunglab-test/test_v0/segmentation\', use_https=use_https)\n  paths = vol.mesh._get_manifests(18)\n  assert len(paths) == 1\n  assert paths[18] == [ \'18:0:0-512_0-512_0-100\' ]\n\n  paths = vol.mesh._get_manifests(147)\n  assert len(paths) == 1\n  assert paths[147] == [ \'147:0:0-512_0-512_0-100\' ]\n\ndef test_get_mesh():\n  vol = CloudVolume(\'gs://seunglab-test/test_v0/segmentation\')\n  mesh = vol.mesh.get(18)\n  assert len(mesh) == 6123\n  assert mesh.vertices.shape[0] == 6123\n  assert len(mesh.faces) == 12242\n  assert isinstance(mesh.vertices, np.ndarray)\n  assert mesh.vertices.dtype == np.float32\n  assert mesh.faces.dtype == np.uint32\n\n  meshes = vol.mesh.get([148, 18], fuse=False)\n  assert len(meshes) == 2\n  mesh = meshes[18]\n  assert len(mesh.vertices) == 6123\n  assert len(mesh.vertices) == 6123\n  assert len(mesh.faces) == 12242\n  \n  try:\n    vol.mesh.get(666666666)\n    assert False\n  except ValueError:\n    pass\n\n  # just don\'t crash\n  mesh = vol.mesh.get(18, chunk_size=(512, 512, 100), fuse=True)\n\ndef test_duplicate_vertices():\n  verts = np.array([\n    [0,0,0], [0,1,0],\n    [1,0,0], [1,1,0],\n    [2,0,0], [2,1,0],\n    [3,0,0], [3,1,0], \n    [3,0,0],\n    [4,0,0], [4,1,0],\n    [4,0,0],          # duplicate in x direction\n    [5,0,0], [5,1,0],\n    [5,0,0],\n    [6,0,0], [6,1,0], [6,1,2],\n    [7,0,0], [7,1,0],\n    [4,0,0]\n  ], dtype=np.float32)\n\n  faces = np.array([ \n    [0,1,2], [2,3,4], [4,5,6], [7,8,9],\n    [9,10,11], [10,11,12],\n    [12,13,14], [14,15,16], [15,16,17],\n    [15,18,19], [18,19,20]\n  ], dtype=np.uint32)\n\n  mesh = Mesh(verts, faces, segid=666)\n\n  def deduplicate(mesh, x, offset_x=0):\n    return mesh.deduplicate_chunk_boundaries(\n      (x, 100, 100), is_draco=False, \n      offset=(offset_x,-1,-1) # so y=0,z=0 isn\'t a chunk boundary\n    )\n\n  # test that triple 4 isn\'t affected\n  mesh2 = deduplicate(mesh, x=4)\n  assert not np.all(mesh.vertices == mesh2.vertices)\n  assert mesh2.vertices.shape[0] == mesh.vertices.shape[0] \n\n  # pop off the last 4\n  mesh.vertices = mesh.vertices[:-1]\n  mesh.faces = mesh.faces[:-1]\n\n  # test that 4 is now affected\n  mesh2 = deduplicate(mesh, x=4)\n  assert not np.all(mesh.vertices == mesh2.vertices)\n  assert mesh2.vertices.shape[0] == mesh.vertices.shape[0] - 1\n\n  mesh2 = deduplicate(mesh, x=3)\n  assert not np.all(mesh.vertices == mesh2.vertices)\n  assert mesh2.vertices.shape[0] == mesh.vertices.shape[0] - 1\n\n  mesh2 = deduplicate(mesh, x=4, offset_x=-1)\n  assert not np.all(mesh.vertices == mesh2.vertices)\n  assert mesh2.vertices.shape[0] == mesh.vertices.shape[0] - 1\n\n  mesh2 = deduplicate(mesh, x=5)\n  assert not np.all(mesh.vertices == mesh2.vertices)\n  assert mesh2.vertices.shape[0] == mesh.vertices.shape[0] - 1\n\n  mesh2 = deduplicate(mesh, x=1)\n  assert not np.all(mesh.vertices == mesh2.vertices)\n  assert mesh2.vertices.shape[0] == mesh.vertices.shape[0] - 3\n\ndef test_get_mesh_caching():\n  vol = CloudVolume(\'gs://seunglab-test/test_v0/segmentation\', cache=True)\n  vol.cache.flush()\n\n  mesh = vol.mesh.get(18)\n  \n  assert set(vol.cache.list_meshes()) == set([ \'18:0:0-512_0-512_0-100.gz\', \'18:0\' ])\n\n  assert len(mesh) == 6123\n  assert mesh.vertices.shape[0] == 6123\n  assert len(mesh.faces) == 12242\n  assert isinstance(mesh.vertices, np.ndarray)\n  assert mesh.vertices.dtype == np.float32\n  assert mesh.faces.dtype == np.uint32\n\n  meshes = vol.mesh.get([148, 18], fuse=False)\n  assert len(meshes) == 2\n  mesh = meshes[18]\n  assert len(mesh.vertices) == 6123\n  assert len(mesh.vertices) == 6123\n  assert len(mesh.faces) == 12242\n  \n  try:\n    vol.mesh.get(666666666)\n    assert False\n  except ValueError:\n    pass\n\n  vol.cache.flush()\n\ndef test_get_mesh_order_stability():\n  vol = CloudVolume(\'gs://seunglab-test/test_v0/segmentation\')\n  first_mesh = vol.mesh.get([148, 18], fuse=True)\n  \n  for _ in range(5):\n    next_mesh = vol.mesh.get([148, 18], fuse=True)\n    assert len(first_mesh.vertices) == len(next_mesh.vertices)\n    assert np.all(first_mesh.vertices == next_mesh.vertices)\n    assert np.all(first_mesh.faces == next_mesh.faces)\n'"
test/test_paths.py,0,"b'import os\nimport re\n\nfrom cloudvolume import paths\nfrom cloudvolume.paths import strict_extract, extract, ExtractedPath\n\nfrom cloudvolume.exceptions import UnsupportedProtocolError\nfrom cloudvolume import lib\n\ndef test_path_extraction():\n  extract = paths.extract(r\'file://C:\\wow\\this\\is\\a\\cool\\path\', windows=True, disable_toabs=True)\n  print(extract)\n  assert extract.protocol == \'file\'\n  assert extract.bucket == \'C:\\\\wow\\\\\'\n\n  # on linux the toabs prepends the current path because it\n  # doesn\'t understand C:\\... so we can\'t test that here.\n  # assert extract.path == \'this\\\\is\\\\a\\\\cool\\\\path\' \n\n  try:\n    extract = paths.strict_extract(r\'file://C:\\wow\\this\\is\\a\\cool\\path\', windows=False, disable_toabs=True)\n    assert False \n  except UnsupportedProtocolError:\n    pass\n\n  def shoulderror(url):\n    try:\n        paths.strict_extract(url)\n        assert False, url\n    except:\n        pass\n\n  def okgoogle(url):\n    path = paths.extract(url)\n    assert path.protocol == \'gs\', url\n    assert path.bucket == \'bucket\', url\n    assert path.basepath == \'bucket/dataset\', url\n    assert path.no_bucket_basepath == \'dataset\', url\n    assert path.dataset == \'dataset\', url\n    assert path.layer == \'layer\', url\n\n  okgoogle(\'gs://bucket/dataset/layer\') \n  shoulderror(\'s4://dataset/layer\')\n  shoulderror(\'dataset/layer\')\n  shoulderror(\'s3://dataset\')\n\n  # don\'t error\n  assert (strict_extract(\'graphene://http://localhost:8080/segmentation/1.0/testvol\')\n    == ExtractedPath(\n      \'graphene\', \'http\', \'localhost:8080\', \n      \'localhost:8080/segmentation/1.0\', \'segmentation/1.0\', \'1.0\', \'testvol\'))\n\n  assert (strict_extract(\'precomputed://gs://fafb-ffn1-1234567/segmentation\')\n    == ExtractedPath(\n      \'precomputed\', \'gs\', \'fafb-ffn1-1234567\', \n      \'fafb-ffn1-1234567\', \'\', \'fafb-ffn1-1234567\', \'segmentation\'))\n\n  firstdir = lambda x: \'/\' + x.split(\'/\')[1]\n\n  homepath = lib.toabs(\'~\')\n  homerintermediate = homepath.replace(firstdir(homepath), \'\')[1:]\n\n  curpath = lib.toabs(\'.\')\n  curintermediate = curpath.replace(firstdir(curpath), \'\')[1:]\n  \n  match = re.match(r\'((?:(?:\\w:\\\\\\\\)|/).+?)\\b\', lib.toabs(\'.\'))\n  bucket, = match.groups()\n  \n  print(bucket, curintermediate)\n\n  assert (paths.extract(\'s3://seunglab-test/intermediate/path/dataset/layer\') \n      == ExtractedPath(\n        \'precomputed\', \'s3\', \'seunglab-test\', \n        \'seunglab-test/intermediate/path/dataset\', \'intermediate/path/dataset\', \n        \'dataset\', \'layer\'\n      ))\n\n  assert (paths.extract(\'file:///tmp/dataset/layer\') \n      == ExtractedPath(\n        \'precomputed\', \'file\', ""/tmp"", \'/tmp/dataset\', \'dataset\', \'dataset\', \'layer\'\n      ))\n\n  assert (paths.extract(\'file://seunglab-test/intermediate/path/dataset/layer\') \n      == ExtractedPath(\n        \'precomputed\', \'file\', firstdir(curpath), \n        os.path.join(bucket, curintermediate, \'seunglab-test/intermediate/path/dataset\'),   \n        os.path.join(curintermediate, \'seunglab-test\', \'intermediate/path/dataset\'), \n       \'dataset\', \'layer\'))\n\n  assert (paths.extract(\'gs://seunglab-test/intermediate/path/dataset/layer\') \n      == ExtractedPath(\n        \'precomputed\', \'gs\', \'seunglab-test\',\n        \'seunglab-test/intermediate/path/dataset\', \'intermediate/path/dataset\', \n        \'dataset\', \'layer\'\n      ))\n\n  assert (paths.extract(\'file://~/seunglab-test/intermediate/path/dataset/layer\') \n      == ExtractedPath(\n        \'precomputed\', \'file\', firstdir(homepath), \n        os.path.join(bucket, homerintermediate, \'seunglab-test\', \'intermediate/path/dataset\'),\n        os.path.join(homerintermediate, \'seunglab-test\', \'intermediate/path/dataset\'),  \n        \'dataset\', \n        \'layer\'\n      )\n  )\n\n  assert (paths.extract(\'file:///User/me/.cloudvolume/cache/gs/bucket/dataset/layer\') \n      == ExtractedPath(\n        \'precomputed\', \'file\', \'/User\', \n        \'/User/me/.cloudvolume/cache/gs/bucket/dataset\', \n        \'me/.cloudvolume/cache/gs/bucket/dataset\', \'dataset\', \'layer\'\n      ))\n\n  shoulderror(\'ou3bouqjsa fkj aojsf oaojf ojsaf\')\n\n  okgoogle(\'gs://bucket/dataset/layer/\')\n  shoulderror(\'gs://bucket/dataset/layer/info\')\n\n  path = paths.extract(\'s3://bucketxxxxxx/datasetzzzzz91h8__3/layer1br9bobasjf/\')\n  assert path.format == \'precomputed\'\n  assert path.protocol == \'s3\'\n  assert path.bucket == \'bucketxxxxxx\'\n  assert path.dataset == \'datasetzzzzz91h8__3\'\n  assert path.layer == \'layer1br9bobasjf\'\n\n  path = paths.extract(\'file:///bucket/dataset/layer/\')\n  assert path.format == \'precomputed\'\n  assert path.protocol == \'file\'\n  assert path.bucket == \'/bucket\'\n  assert path.dataset == \'dataset\'\n  assert path.layer == \'layer\'\n\n  shoulderror(\'lucifer://bucket/dataset/layer/\')\n  shoulderror(\'gs://///\')\n  shoulderror(\'gs://seunglab-test//segmentation\')\n\n  path = paths.extract(\'file:///tmp/removeme/layer/\')\n  assert path.format == \'precomputed\'\n  assert path.protocol == \'file\'\n  assert path.bucket == \'/tmp\'\n  assert path.dataset == \'removeme\'\n  assert path.layer == \'layer\'\n'"
test/test_precomputed_multilod.py,0,"b'import pytest \nimport sys\n\nimport cloudvolume\n\n# Basic test of sharded meshes\n# The following test files are used from https://storage.googleapis.com:\n#   /neuroglancer-janelia-flyem-hemibrain/v1.0/segmentation/info\n#   /fafb-ffn1-20190805/segmentation/mesh/info\n#   /neuroglancer-janelia-flyem-hemibrain/v1.0/segmentation/mesh/171.shard\n#   /neuroglancer-janelia-flyem-hemibrain/v1.0/segmentation/mesh/193.shard\n#   /neuroglancer-janelia-flyem-hemibrain/v1.0/segmentation/mesh/185.shard \n\n\n@pytest.mark.skipif(sys.version_info < (3, 0), reason=""requires python3"")\ndef test_get_sharded_mesh():\n  vol = cloudvolume.CloudVolume(\'gs://neuroglancer-janelia-flyem-hemibrain/v1.0/segmentation\', mip=0, cache=False, use_https=True)\n\n  exists = vol.mesh.exists([511271574, 360284300])\n  assert(all(exists))\n\n  exists = vol.mesh.exists([666666666, 666666667, 666666668])\n  assert(all([a == None for a in exists]))\n\n  meshes = vol.mesh.get([511271574, 360284300], lod=2)\n  assert len(meshes[511271574].faces) == 258647\n\n  meshes = vol.mesh.get([511271574, 360284300], lod=3)\n  assert len(meshes[511271574].faces) == 50501\n'"
test/test_provenance.py,0,"b""import pytest\n\nimport os\nimport json \n\nfrom layer_harness import delete_layer\nfrom cloudvolume.provenance import DataLayerProvenance, DatasetProvenance\nfrom cloudvolume.storage import Storage\n\ndef test_dataset_provenance():\n  fs = '/tmp/removeme/provenance/'\n  delete_layer(fs)\n\n  prov = DatasetProvenance()\n\n  prov.dataset_name = 'ur-mom-2039'\n  prov.dataset_description = 'EM serial section of your mom\\'s brain'\n  prov.organism = 'Male wild-type (C57BL/6) mouse'\n  prov.imaged_date = 'March-Feb 2010'\n  prov.imaged_by = 'gradstudent@princeton.edu'\n  prov.references = [ 'doi:presigiousjournalofyourmom-12142' ]\n  prov.owners = [ 'scientist@princeton.edu', 'techstaff@princeton.edu' ]\n\n  with Storage('file://' + fs) as stor:\n    stor.put_file('provenance', prov.serialize(), content_type='application/json')\n\n  path = os.path.join(fs, 'provenance')\n\n  with open(path, 'r') as f:\n    data = json.loads(f.read())\n\n  assert data == { \n    'dataset_name': 'ur-mom-2039',\n    'dataset_description': 'EM serial section of your mom\\'s brain',\n    'organism': 'Male wild-type (C57BL/6) mouse',\n    'imaged_date': 'March-Feb 2010',\n    'imaged_by': 'gradstudent@princeton.edu',\n    'references': [ 'doi:presigiousjournalofyourmom-12142' ],\n    'owners': [ 'scientist@princeton.edu', 'techstaff@princeton.edu' ],\n  }\n\n  with Storage('file://' + fs) as stor:\n    provjson = stor.get_file('provenance')\n    provjson = provjson.decode('utf-8')\n    prov = DatasetProvenance().from_json(provjson)\n\n  assert prov.dataset_name == 'ur-mom-2039'\n  assert prov.dataset_description == 'EM serial section of your mom\\'s brain'\n  assert prov.organism == 'Male wild-type (C57BL/6) mouse'\n  assert prov.imaged_date == 'March-Feb 2010'\n  assert prov.imaged_by == 'gradstudent@princeton.edu'\n  assert prov.references == [ 'doi:presigiousjournalofyourmom-12142' ]\n  assert prov.owners == [ 'scientist@princeton.edu', 'techstaff@princeton.edu' ]\n\ndef test_data_layer_provenance():\n  fs = '/tmp/removeme/provenance/layer/'\n  delete_layer(fs)\n\n  prov = DataLayerProvenance()\n\n  prov.description = 'example dataset'\n  prov.sources = [ 'gs://seunglab-test/example/image' ]\n  prov.processing = [ \n    { 'method': 'convnet', 'by': 'gradstudent@princeton.edu' },\n  ]\n  prov.owners = [ 'gradstudent@princeton.edu' ]\n\n  with Storage('file://' + fs) as stor:\n    stor.put_file('provenance', prov.serialize(), content_type='application/json')\n\n  path = os.path.join(fs, 'provenance')\n\n  with open(path, 'r') as f:\n    data = json.loads(f.read())\n\n  assert data == { \n    'description': 'example dataset', \n    'sources': [ 'gs://seunglab-test/example/image' ],\n    'processing': [\n      { 'method': 'convnet', 'by': 'gradstudent@princeton.edu' },\n    ],\n    'owners': [ 'gradstudent@princeton.edu' ]\n  }\n\n  with Storage('file://' + fs) as stor:\n    provjson = stor.get_file('provenance')\n    provjson = provjson.decode('utf-8')\n    prov = DataLayerProvenance().from_json(provjson)\n\n  assert prov.description == 'example dataset'\n  assert prov.sources == [ 'gs://seunglab-test/example/image' ]\n  assert prov.processing == [ { 'method': 'convnet', 'by': 'gradstudent@princeton.edu' } ]\n  assert prov.owners == [ 'gradstudent@princeton.edu' ]\n\n"""
test/test_sharding.py,3,"b'from cloudvolume import CloudVolume, Skeleton\nfrom cloudvolume.storage import SimpleStorage\nfrom cloudvolume.datasource.precomputed.image.common import compressed_morton_code\nfrom cloudvolume.datasource.precomputed.sharding import ShardingSpecification\nfrom cloudvolume.exceptions import SpecViolation\n\nfrom cloudvolume import Vec\n\nimport numpy as np\n\ndef test_actual_example_hash():\n  spec = ShardingSpecification.from_dict({\n    ""@type"" : ""neuroglancer_uint64_sharded_v1"",\n    ""data_encoding"" : ""gzip"",\n    ""hash"" : ""murmurhash3_x86_128"",\n    ""minishard_bits"" : 11,\n    ""minishard_index_encoding"" : ""gzip"",\n    ""preshift_bits"" : 6,\n    ""shard_bits"" : 7\n  })\n\n  spec.validate()\n\n  shard_loc = spec.compute_shard_location(1822975381)\n  assert shard_loc.shard_number == \'42\'\n  assert shard_loc.minishard_number == 18\n\n  spec = ShardingSpecification.from_dict({\n    ""@type"" : ""neuroglancer_uint64_sharded_v1"",\n    ""data_encoding"" : ""gzip"",\n    ""hash"" : ""murmurhash3_x86_128"",\n    ""minishard_index_encoding"" : ""gzip"",\n    ""preshift_bits"" : 2,\n    ""minishard_bits"" : 3,\n    ""shard_bits"" : 3\n  })\n\n  spec.hash = \'identity\'\n  label = 0b10101010\n  shard_loc = spec.compute_shard_location(label)\n  minishard_no = 0b010\n  shard_no = 0b101\n\n  assert shard_loc.minishard_number == minishard_no\n  assert shard_loc.shard_number == str(shard_no)\n\ndef test_compressed_morton_code():\n  cmc = lambda coord: compressed_morton_code(coord, grid_size=(3,3,3))\n\n  assert cmc((0,0,0)) == 0b000000\n  assert cmc((1,0,0)) == 0b000001\n  assert cmc((2,0,0)) == 0b001000\n  assert cmc((3,0,0)) == 0b001001\n  assert cmc((2,2,0)) == 0b011000\n  assert cmc((2,2,1)) == 0b011100\n\n  cmc = lambda coord: compressed_morton_code(coord, grid_size=(2,3,1))\n\n  assert cmc((0,0,0)) == 0b000000\n  assert cmc((1,0,0)) == 0b000001\n  assert cmc((0,0,7)) == 0b000100\n  assert cmc((2,3,1)) == 0b011110\n\ndef test_image_sharding_hash():\n  spec = ShardingSpecification(\n    type=""neuroglancer_uint64_sharded_v1"",\n    data_encoding=""gzip"",\n    hash=""identity"",\n    minishard_bits=6,\n    minishard_index_encoding=""gzip"",\n    preshift_bits=9,\n    shard_bits=16,\n  ) \n\n  point = Vec(144689, 52487, 2829)\n  volume_size = Vec(*[248832, 134144, 7063])\n  chunk_size = Vec(*[128, 128, 16])\n\n  grid_size = np.ceil(volume_size / chunk_size).astype(np.uint32)\n  gridpt = np.ceil(point / chunk_size).astype(np.int32)\n  code = compressed_morton_code(gridpt, grid_size)\n  loc = spec.compute_shard_location(code)\n\n  assert loc.shard_number == \'458d\'\n\n\n\ndef test_sharding_spec_validation():\n  spec = ShardingSpecification(\n    type=""neuroglancer_uint64_sharded_v1"",\n    data_encoding=""gzip"",\n    hash=""murmurhash3_x86_128"",\n    minishard_bits=11,\n    minishard_index_encoding=""gzip"",\n    preshift_bits=6,\n    shard_bits=7,\n  ) \n\n  spec.validate()\n  \n  spec.minishard_bits = 0\n  spec.shard_bits = 0\n  spec.validate()\n  \n  spec.minishard_bits = 64\n  spec.shard_bits = 0\n  spec.validate()\n\n  spec.minishard_bits = 0\n  spec.shard_bits = 64\n  spec.validate()\n\n  spec.minishard_bits = 1\n  spec.shard_bits = 64\n  try:\n    spec.validate()\n    assert False\n  except SpecViolation:\n    pass\n\n  spec.minishard_bits = 64\n  spec.shard_bits = 1\n  try:\n    spec.validate()\n    assert False\n  except SpecViolation:\n    pass\n\n  spec.minishard_bits = 11\n  spec.shard_bits = 7\n  \n  spec.hash = \'identity\'\n  spec.hash = \'murmurhash3_x86_128\'\n  try:\n    spec.hash = \'murmurhash3_X86_128\'\n    assert False \n  except SpecViolation:\n    pass\n\n  try:\n    spec.hash = \'something else\'\n    assert False\n  except SpecViolation:\n    pass\n\n  try:\n    spec.hash = \'\'\n  except SpecViolation:\n    pass\n\n  spec = ShardingSpecification(\n    type=""neuroglancer_uint64_sharded_v1"",\n    data_encoding=""gzip"",\n    hash=""murmurhash3_x86_128"",\n    minishard_bits=11,\n    minishard_index_encoding=""gzip"",\n    preshift_bits=6,\n    shard_bits=7,\n  ) \n\n  spec.preshift_bits = 0\n  spec.validate()\n\n  spec.preshift_bits = 63\n  spec.validate()\n\n  spec.preshift_bits = 32\n  spec.validate()\n\n  try:\n    spec.preshift_bits = 64\n    spec.validate()\n    assert False\n  except SpecViolation:\n    pass\n\n  try:\n    spec.preshift_bits = -1\n    spec.validate()\n    assert False\n  except SpecViolation:\n    pass\n\n  spec.preshift_bits = 5\n\n  spec.minishard_index_encoding = \'raw\'\n  spec.validate()\n\n  spec.minishard_index_encoding = \'gzip\'\n  spec.validate()\n\n  spec.data_encoding = \'raw\'\n  spec.validate()\n\n  spec.data_encoding = \'gzip\'\n  spec.validate()\n\n  try:\n    spec.type = \'lol_my_type\'\n    spec.validate()\n    assert False\n  except SpecViolation:\n    pass\n\ndef test_skeleton_fidelity():\n  segid = 1822975381\n  cv = CloudVolume(\'gs://seunglab-test/sharded\')\n  sharded_skel = cv.skeleton.get(segid)\n\n  with SimpleStorage(\'gs://seunglab-test/sharded\') as stor:\n    binary = stor.get_file(\'skeletons/\' + str(segid))\n\n  unsharded_skel = Skeleton.from_precomputed(binary, \n    segid=1822975381, vertex_attributes=cv.skeleton.meta.info[\'vertex_attributes\']\n  )\n\n  assert sharded_skel == unsharded_skel\n\ndef test_image_fidelity():\n  point = (142195, 64376, 3130)\n  cv = CloudVolume(\'gs://seunglab-test/sharded\')\n  img = cv.download_point(point, mip=0, size=128)\n\n  N_labels = np.unique(img).shape[0]\n\n  assert N_labels == 144\n\n\n\n\n\n\n'"
test/test_sharedmemory.py,16,"b""import os\nimport sys\n\nimport numpy as np\n\nimport cloudvolume.sharedmemory as shm\n\ndef test_ndarray_fs():\n\tlocation = 'cloudvolume-shm-test-ndarray'\n\tarray_like, array = shm.ndarray_fs(\n\t\tshape=(2,2,2), dtype=np.uint8, location=location, \n\t\tlock=None, emulate_shm=True\n\t)\n\tassert np.all(array == np.zeros(shape=(2,2,2), dtype=np.uint8))\n\tarray[:] = 100\n\tarray_like.close()\n\n\tarray_like, array = shm.ndarray_fs(\n\t\tshape=(2,2,2), dtype=np.uint8, location=location, \n\t\tlock=None, emulate_shm=True\n\t)\n\tassert np.all(array[:] == 100)\n\tarray_like.close()\n\n\tfull_loc = os.path.join(shm.EMULATED_SHM_DIRECTORY, location)\n\n\tassert os.path.exists(full_loc)\n\tassert os.path.getsize(full_loc) == 8\n\n\tassert shm.unlink_fs(location) == True\n\tassert shm.unlink_fs(location) == False\n\n\ttry:\n\t\tarray_like, array = shm.ndarray_fs(\n\t\t\tshape=(2,2,2), dtype=np.uint8, location=location, \n\t\t\tlock=None, readonly=True, emulate_shm=True\n\t\t)\n\t\tassert False\n\texcept shm.SharedMemoryReadError:\n\t\tpass\n\n\tarray_like, array = shm.ndarray_fs(\n\t\tshape=(2,2,2), dtype=np.uint8, location=location, \n\t\tlock=None, emulate_shm=True\n\t)\n\ttry:\n\t\tarray_like, array = shm.ndarray_fs(\n\t\t\tshape=(200,200,200), dtype=np.uint8, location=location, \n\t\t\tlock=None, readonly=True, emulate_shm=True\n\t\t)\n\t\tassert False\n\texcept shm.SharedMemoryReadError:\n\t\tpass\n\n\tassert shm.unlink_fs(location) == True\n\tassert shm.unlink_fs(location) == False\n\n\tassert not os.path.exists(full_loc)\n\ndef test_ndarray_sh():\n\t# Don't bother testing on unsupported platforms.\n\tif shm.EMULATE_SHM:\n\t\treturn\n\n\timport psutil\n\n\tlocation = 'cloudvolume-shm-test-ndarray'\n\tarray_like, array = shm.ndarray_shm(shape=(2,2,2), dtype=np.uint8, location=location)\n\tassert np.all(array == np.zeros(shape=(2,2,2), dtype=np.uint8))\n\tarray[:] = 100\n\tarray_like.close()\n\n\tarray_like, array = shm.ndarray_shm(shape=(2,2,2), dtype=np.uint8, location=location)\n\tassert np.all(array[:] == 100)\n\tarray_like.close()\n\n\tfilename = os.path.join(shm.SHM_DIRECTORY, location)\n\n\tassert os.path.exists(filename)\n\tassert os.path.getsize(filename) == 8\n\n\tassert shm.unlink_shm(location) == True\n\tassert shm.unlink_shm(location) == False\n\n\tassert not os.path.exists(filename)\n\n\tavailable = psutil.virtual_memory().available\n\tarray_like, array = shm.ndarray_shm(shape=(available // 10,2,2), dtype=np.uint8, location=location)\n\tarray_like.close()\n\ttry:\n\t\tarray_like, array = shm.ndarray_shm(shape=(available,2,2), dtype=np.uint8, location=location)\n\t\tassert False\n\texcept shm.SharedMemoryAllocationError:\n\t\tpass\n\n\tassert shm.unlink_shm(location) == True\n\tassert shm.unlink_shm(location) == False\n\n\ttry:\n\t\tarray_like, array = shm.ndarray_shm(shape=(2,2,2), dtype=np.uint8, location=location, readonly=True)\n\t\tassert False\n\texcept shm.SharedMemoryReadError:\n\t\tpass\n\n\tarray_like, array = shm.ndarray_shm(shape=(2,2,2), dtype=np.uint8, location=location)\n\ttry:\n\t\tarray_like, array = shm.ndarray_shm(shape=(200,200,200), dtype=np.uint8, location=location, readonly=True)\n\t\tassert False\n\texcept shm.SharedMemoryReadError:\n\t\tpass\n\n\tassert shm.unlink_shm(location) == True\n\tassert shm.unlink_shm(location) == False\n"""
test/test_skeletons.py,51,"b'import pytest\n\nimport copy\nimport gzip\nimport json\nimport math\nimport numpy as np\nimport os\nimport shutil\n\nfrom cloudvolume import CloudVolume, chunks, Storage, Skeleton\nfrom cloudvolume.storage import SimpleStorage\nfrom cloudvolume.lib import mkdir, Bbox, Vec, jsonify\n\nfrom cloudvolume.datasource.precomputed.sharding import ShardingSpecification\nfrom cloudvolume.exceptions import SkeletonDecodeError, SkeletonAttributeMixingError\n\ninfo = CloudVolume.create_new_info(\n  num_channels=1, # Increase this number when we add more tests for RGB\n  layer_type=\'segmentation\', \n  data_type=\'uint16\', \n  encoding=\'raw\',\n  resolution=[1,1,1], \n  voxel_offset=(0,0,0), \n  skeletons=True,\n  volume_size=(100, 100, 100),\n  chunk_size=(64, 64, 64),\n)\n\nskel_info = {\n  ""@type"": ""neuroglancer_skeletons"", \n  ""transform"": [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0], \n  ""vertex_attributes"": [\n    {""id"": ""radius"", ""data_type"": ""float32"", ""num_components"": 1}\n  ], \n  ""mip"": 3,\n}\n\ndef test_skeletons():\n  \n  # Skeleton of my initials\n  # z=0: W ; z=1 S\n  vertices = np.array([\n    [ 0, 1, 0 ],\n    [ 1, 0, 0 ],\n    [ 1, 1, 0 ],\n\n    [ 2, 0, 0 ],\n    [ 2, 1, 0 ],\n    [ 0, 0, 1 ],\n\n    [ 1, 0, 1 ],\n    [ 1, 1, 1 ],\n    [ 0, 1, 1 ],\n\n    [ 0, 2, 1 ],\n    [ 1, 2, 1 ],\n  ], np.float32)\n\n  edges = np.array([\n    [0, 1],\n    [1, 2],\n    [2, 3],\n    [3, 4],\n    [4, 5],\n    [5, 6],\n    [6, 7],\n    [7, 8],\n    [8, 9],\n    [9, 10],\n    [10, 11],\n  ], dtype=np.uint32)\n\n  radii = np.array([\n    1.0,\n    2.5,\n    3.0,\n    4.1,\n    1.2,\n    5.6,\n    2000.123123,\n    15.33332221,\n    8128.124,\n    -1,\n    1824.03\n  ], dtype=np.float32)\n\n  vertex_types = np.array([\n   1,\n   2,\n   3,\n   5,\n   8,\n   2,\n   0,\n   5,\n   9,\n   11,\n   22,\n  ], dtype=np.uint8)\n\n  vol = CloudVolume(\'file:///tmp/cloudvolume/test-skeletons\', info=info)\n  vol.skeleton.upload_raw(\n    segid=1, vertices=vertices, edges=edges, \n    radii=radii, vertex_types=vertex_types\n  )\n  skel = vol.skeleton.get(1)\n\n  assert skel.id == 1\n  assert np.all(skel.vertices == vertices)\n  assert np.all(skel.edges == edges)\n  assert np.all(skel.radii == radii)\n  assert np.all(skel.vertex_types == vertex_types)\n  assert vol.skeleton.meta.skeleton_path == \'skeletons\'\n  assert not skel.empty()\n\n  with SimpleStorage(\'file:///tmp/cloudvolume/test-skeletons/\') as stor:\n    rawskel = stor.get_file(\'skeletons/1\')\n    assert len(rawskel) == 8 + 11 * (12 + 8 + 4 + 1) \n    stor.delete_file(\'skeletons/1\')\n  \n  try:\n    vol.skeleton.get(5)\n    assert False\n  except SkeletonDecodeError:\n    pass\n\ndef test_no_edges():\n  vertices = np.array([\n    [ 0, 1, 0 ],\n    [ 1, 0, 0 ],\n  ], np.float32)\n\n  edges = None\n  vol = CloudVolume(\'file:///tmp/cloudvolume/test-skeletons\', info=info)\n  vol.skeleton.upload_raw(2, vertices, edges)\n  skel = vol.skeleton.get(2)\n\n  assert skel.id == 2\n  assert np.all(skel.vertices == vertices)\n  assert np.all(skel.edges.shape == (0, 2))\n  assert vol.skeleton.path == \'skeletons\'\n  assert skel.empty()\n\n  with SimpleStorage(\'file:///tmp/cloudvolume/test-skeletons/\') as stor:\n    rawskel = stor.get_file(\'skeletons/2\')\n    assert len(rawskel) == 8 + 2 * (12 + 0 + 4 + 1) \n    stor.delete_file(\'skeletons/2\')\n\ndef test_no_vertices():\n  vertices = np.array([], np.float32).reshape(0,3)\n\n  edges = None\n  vol = CloudVolume(\'file:///tmp/cloudvolume/test-skeletons\', info=info)\n  vol.skeleton.upload_raw(3, vertices, edges)\n  skel = vol.skeleton.get(3)\n\n  assert skel.id == 3\n  assert np.all(skel.vertices == vertices)\n  assert np.all(skel.edges.shape == (0, 2))\n  assert skel.empty()\n  assert vol.skeleton.path == \'skeletons\'\n\n  with SimpleStorage(\'file:///tmp/cloudvolume/test-skeletons/\') as stor:\n    rawskel = stor.get_file(\'skeletons/3\')\n    assert len(rawskel) == 8 + 0 * (12 + 8 + 4 + 1)\n    stor.delete_file(\'skeletons/3\')\n\ndef test_consolidate():\n  skel = Skeleton(\n    vertices=np.array([\n      (0, 0, 0),\n      (1, 0, 0),\n      (2, 0, 0),\n      (0, 0, 0),\n      (2, 1, 0),\n      (2, 2, 0),\n      (2, 2, 1),\n      (2, 2, 2),\n    ], dtype=np.float32),\n\n    edges=np.array([\n      [0, 1],\n      [1, 2],\n      [2, 3],\n      [3, 4],\n      [4, 5],\n      [5, 6],\n      [6, 7],\n    ], dtype=np.uint32),\n\n    radii=np.array([\n      0, 1, 2, 3, 4, 5, 6, 7\n    ], dtype=np.float32),\n\n    vertex_types=np.array([\n      0, 1, 2, 3, 4, 5, 6, 7\n    ], dtype=np.uint8),\n  )\n\n  correct_skel = Skeleton(\n    vertices=np.array([\n      (0, 0, 0),\n      (1, 0, 0),\n      (2, 0, 0),\n      (2, 1, 0),\n      (2, 2, 0),\n      (2, 2, 1),\n      (2, 2, 2),\n    ], dtype=np.float32),\n\n    edges=np.array([\n      [0, 1],\n      [0, 2],\n      [0, 3],\n      [1, 2],\n      [3, 4],\n      [4, 5],\n      [5, 6],\n    ], dtype=np.uint32),\n\n    radii=np.array([\n      0, 1, 2, 4, 5, 6, 7\n    ], dtype=np.float32),\n\n    vertex_types=np.array([\n      0, 1, 2, 4, 5, 6, 7\n    ], dtype=np.uint8),\n  )\n\n  consolidated = skel.consolidate()\n\n  assert np.all(consolidated.vertices == correct_skel.vertices)\n  assert np.all(consolidated.edges == correct_skel.edges)\n  assert np.all(consolidated.radii == correct_skel.radii)\n  assert np.all(consolidated.vertex_types == correct_skel.vertex_types)\n\ndef test_remove_disconnected_vertices():\n  skel = Skeleton(\n    [ \n      (0,0,0), (1,0,0), (2,0,0),\n      (0,1,0), (0,2,0), (0,3,0),\n      (-1, -1, -1)\n    ], \n    edges=[ \n      (0,1), (1,2), \n      (3,4), (4,5), (3,5)\n    ],\n    segid=666,\n  )\n\n  res = skel.remove_disconnected_vertices()\n  assert res.vertices.shape[0] == 6\n  assert res.edges.shape[0] == 5 \n  assert res.radii.shape[0] == 6\n  assert res.vertex_types.shape[0] == 6\n  assert res.id == 666\n\n\ndef test_equivalent():\n  assert Skeleton.equivalent(Skeleton(), Skeleton())\n\n  identity = Skeleton([ (0,0,0), (1,0,0) ], [(0,1)] )\n  assert Skeleton.equivalent(identity, identity)\n\n  diffvertex = Skeleton([ (0,0,0), (0,1,0) ], [(0,1)])\n  assert not Skeleton.equivalent(identity, diffvertex)\n\n  single1 = Skeleton([ (0,0,0), (1,0,0) ], edges=[ (1,0) ])\n  single2 = Skeleton([ (0,0,0), (1,0,0) ], edges=[ (0,1) ])\n  assert Skeleton.equivalent(single1, single2)\n\n  double1 = Skeleton([ (0,0,0), (1,0,0) ], edges=[ (1,0) ])\n  double2 = Skeleton([ (0,0,0), (1,0,0) ], edges=[ (0,1) ])\n  assert Skeleton.equivalent(double1, double2)\n\n  double1 = Skeleton([ (0,0,0), (1,0,0), (1,1,0) ], edges=[ (1,0), (1,2) ])\n  double2 = Skeleton([ (0,0,0), (1,0,0), (1,1,0) ], edges=[ (2,1), (0,1) ])\n  assert Skeleton.equivalent(double1, double2)\n\n  double1 = Skeleton([ (0,0,0), (1,0,0), (1,1,0), (1,1,3) ], edges=[ (1,0), (1,2), (1,3) ])\n  double2 = Skeleton([ (0,0,0), (1,0,0), (1,1,0), (1,1,3) ], edges=[ (3,1), (2,1), (0,1) ])\n  assert Skeleton.equivalent(double1, double2)\n\ndef test_cable_length():\n  skel = Skeleton([ \n      (0,0,0), (1,0,0), (2,0,0), (3,0,0), (4,0,0), (5,0,0)\n    ], \n    edges=[ (1,0), (1,2), (2,3), (3,4), (5,4) ],\n    radii=[ 1, 2, 3, 4, 5, 6 ],\n    vertex_types=[1, 2, 3, 4, 5, 6]\n  )\n\n  assert skel.cable_length() == (skel.vertices.shape[0] - 1)\n\n  skel = Skeleton([ \n      (2,0,0), (1,0,0), (0,0,0), (0,5,0), (0,6,0), (0,7,0)\n    ], \n    edges=[ (1,0), (1,2), (2,3), (3,4), (5,4) ],\n    radii=[ 1, 2, 3, 4, 5, 6 ],\n    vertex_types=[1, 2, 3, 4, 5, 6]\n  )\n  assert skel.cable_length() == 9\n\n  skel = Skeleton([ \n      (1,1,1), (0,0,0), (1,0,0)\n    ], \n    edges=[ (1,0), (1,2) ],\n    radii=[ 1, 2, 3],\n    vertex_types=[1, 2, 3]\n  )\n  assert abs(skel.cable_length() - (math.sqrt(3) + 1)) < 1e-6\n\ndef test_transform():\n  skelv = Skeleton([ \n      (0,0,0), (1,0,0), (1,1,0), (1,1,3), (2,1,3), (2,2,3)\n    ], \n    edges=[ (1,0), (1,2), (2,3), (3,4), (5,4) ],\n    radii=[ 1, 2, 3, 4, 5, 6 ],\n    vertex_types=[1, 2, 3, 4, 5, 6],\n    segid=1337,\n    transform=np.array([\n      [2, 0, 0, 0],\n      [0, 2, 0, 0],\n      [0, 0, 2, 0],\n    ])\n  )\n\n  skelp = skelv.physical_space()\n  assert np.all(skelp.vertices == skelv.vertices * 2)\n  assert np.all(skelv.vertices == skelp.voxel_space().vertices)\n\n  skelv.transform = [\n    [1, 0, 0, 1],\n    [0, 1, 0, 2],\n    [0, 0, 1, 3],\n  ]\n\n  skelp = skelv.physical_space()\n  tmpskel = skelv.clone() \n  tmpskel.vertices[:,0] += 1\n  tmpskel.vertices[:,1] += 2\n  tmpskel.vertices[:,2] += 3\n  assert np.all(skelp.vertices == tmpskel.vertices)\n  assert np.all(skelp.voxel_space().vertices == skelv.vertices)\n\n\ndef test_downsample():\n  skel = Skeleton([ \n      (0,0,0), (1,0,0), (1,1,0), (1,1,3), (2,1,3), (2,2,3)\n    ], \n    edges=[ (1,0), (1,2), (2,3), (3,4), (5,4) ],\n    radii=[ 1, 2, 3, 4, 5, 6 ],\n    vertex_types=[1, 2, 3, 4, 5, 6],\n    segid=1337,\n  )\n\n  def should_error(x):\n    try:\n      skel.downsample(x)\n      assert False\n    except ValueError:\n      pass\n\n  should_error(-1)\n  should_error(0)\n  should_error(.5)\n  should_error(2.00000000000001)\n\n  dskel = skel.downsample(1)\n  assert Skeleton.equivalent(dskel, skel)\n  assert dskel.id == skel.id\n  assert dskel.id == 1337\n\n  dskel = skel.downsample(2)\n  dskel_gt = Skeleton(\n    [ (0,0,0), (1,1,0), (2,1,3), (2,2,3) ], \n    edges=[ (1,0), (1,2), (2,3) ],\n    radii=[1,3,5,6], vertex_types=[1,3,5,6] \n  )\n  assert Skeleton.equivalent(dskel, dskel_gt)\n\n  dskel = skel.downsample(3)\n  dskel_gt = Skeleton(\n    [ (0,0,0), (1,1,3), (2,2,3) ], edges=[ (1,0), (1,2) ],\n    radii=[1,4,6], vertex_types=[1,4,6],\n  )\n  assert Skeleton.equivalent(dskel, dskel_gt)\n\n  skel = Skeleton([ \n      (0,0,0), (1,0,0), (1,1,0), (1,1,3), (2,1,3), (2,2,3)\n    ], \n    edges=[ (1,0), (1,2), (3,4), (5,4) ],\n    radii=[ 1, 2, 3, 4, 5, 6 ],\n    vertex_types=[1, 2, 3, 4, 5, 6]\n  )\n  dskel = skel.downsample(2)\n  dskel_gt = Skeleton(\n    [ (0,0,0), (1,1,0), (1,1,3), (2,2,3) ], \n    edges=[ (1,0), (2,3) ],\n    radii=[1,3,4,6], vertex_types=[1,3,4,6] \n  )\n  assert Skeleton.equivalent(dskel, dskel_gt)\n\n\ndef test_downsample_joints():\n  skel = Skeleton([ \n      \n                        (2, 3,0), # 0\n                        (2, 2,0), # 1\n                        (2, 1,0), # 2\n      (0,0,0), (1,0,0), (2, 0,0), (3,0,0), (4,0,0), # 3, 4, 5, 6, 7\n                        (2,-1,0), # 8\n                        (2,-2,0), # 9\n                        (2,-3,0), # 10\n\n    ], \n    edges=[ \n                  (0, 1),\n                  (1, 2),\n                  (2, 5),\n        (3,4), (4,5), (5, 6), (6,7),\n                  (5, 8),\n                  (8, 9),\n                  (9,10)\n    ],\n    radii=[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ],\n    vertex_types=[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 ],\n    segid=1337,\n  )\n\n  ds_skel = skel.downsample(2)\n  ds_skel_gt = Skeleton([ \n\n                        (2, 3,0), # 0\n                        \n                        (2, 2,0), # 1\n      (0,0,0),          (2, 0,0),     (4,0,0), # 2, 3, 4\n\n                        (2,-2,0), # 5                        \n                        (2,-3,0), # 6\n\n    ], \n    edges=[ \n                  (0,1),\n                  (1,3),\n              (2,3),  (3,4), \n                  (3,5),\n                  (5,6)\n    ],\n    radii=[ 0, 1, 3, 5, 7, 9, 10 ],\n    vertex_types=[ 0, 1, 3, 5, 7, 9, 10 ],\n    segid=1337,\n  )\n\n  assert Skeleton.equivalent(ds_skel, ds_skel_gt)\n\n\ndef test_read_swc():\n\n  # From http://research.mssm.edu/cnic/swc.html\n  test_file = """"""# ORIGINAL_SOURCE NeuronStudio 0.8.80\n# CREATURE\n# REGION\n# FIELD/LAYER\n# TYPE\n# CONTRIBUTOR\n# REFERENCE\n# RAW\n# EXTRAS\n# SOMA_AREA\n# SHINKAGE_CORRECTION 1.0 1.0 1.0\n# VERSION_NUMBER 1.0\n# VERSION_DATE 2007-07-24\n# SCALE 1.0 1.0 1.0\n1 1 14.566132 34.873772 7.857000 0.717830 -1\n2 0 16.022520 33.760513 7.047000 0.463378 1\n3 5 17.542000 32.604973 6.885001 0.638007 2\n4 0 19.163984 32.022469 5.913000 0.602284 3\n5 0 20.448090 30.822802 4.860000 0.436025 4\n6 6 21.897903 28.881084 3.402000 0.471886 5\n7 0 18.461960 30.289471 8.586000 0.447463 3\n8 6 19.420759 28.730757 9.558000 0.496217 7""""""\n\n  skel = Skeleton.from_swc(test_file)\n  assert skel.vertices.shape[0] == 8\n  assert skel.edges.shape[0] == 7\n\n  skel_gt = Skeleton(\n    vertices=[\n      [14.566132, 34.873772, 7.857000],\n      [16.022520, 33.760513, 7.047000],\n      [17.542000, 32.604973, 6.885001],\n      [19.163984, 32.022469, 5.913000],\n      [20.448090, 30.822802, 4.860000],\n      [21.897903, 28.881084, 3.402000],\n      [18.461960, 30.289471, 8.586000],\n      [19.420759, 28.730757, 9.558000]\n    ],\n    edges=[ (0,1), (1,2), (2,3), (3,4), (4,5), (2,6), (7,6) ],\n    radii=[ \n      0.717830, 0.463378, 0.638007, 0.602284, \n      0.436025, 0.471886, 0.447463, 0.496217\n    ],\n    vertex_types=[\n      1, 0, 5, 0, 0, 6, 0, 6\n    ],\n  )\n\n  assert Skeleton.equivalent(skel, skel_gt)\n\n  skel = Skeleton.from_swc(skel.to_swc())\n  assert np.all(np.abs(skel.vertices - skel_gt.vertices) < 0.00001)\n  # sorts edges\n  skel = skel.consolidate()\n  skel_gt = skel_gt.consolidate()\n  assert np.all(skel.edges == skel_gt.edges)\n  assert np.all(np.abs(skel.radii - skel_gt.radii) < 0.00001)\n\n  Nv = skel.vertices.shape[0]\n  Ne = skel.edges.shape[0]\n\n  for _ in range(10):\n    skel = Skeleton.from_swc(skel.to_swc())\n    assert skel.vertices.shape[0] == Nv \n    assert skel.edges.shape[0] == Ne\n\n\ndef test_components():\n  skel = Skeleton(\n    [ \n      (0,0,0), (1,0,0), (2,0,0),\n      (0,1,0), (0,2,0), (0,3,0),\n    ], \n    edges=[ \n      (0,1), (1,2), \n      (3,4), (4,5), (3,5)\n    ],\n    segid=666,\n  )\n\n  components = skel.components()\n  assert len(components) == 2\n  assert components[0].vertices.shape[0] == 3\n  assert components[1].vertices.shape[0] == 3\n  assert components[0].edges.shape[0] == 2\n  assert components[1].edges.shape[0] == 3\n\n  skel1_gt = Skeleton([(0,0,0), (1,0,0), (2,0,0)], [(0,1), (1,2)])\n  skel2_gt = Skeleton([(0,1,0), (0,2,0), (0,3,0)], [(0,1), (0,2), (1,2)])\n\n  assert Skeleton.equivalent(components[0], skel1_gt)\n  assert Skeleton.equivalent(components[1], skel2_gt)\n\ndef test_caching():\n  vol = CloudVolume(\'file:///tmp/cloudvolume/test-skeletons\', \n    info=info, cache=True)\n\n  vol.cache.flush()\n\n  skel = Skeleton(\n    [ \n      (0,0,0), (1,0,0), (2,0,0),\n      (0,1,0), (0,2,0), (0,3,0),\n    ], \n    edges=[ \n      (0,1), (1,2), \n      (3,4), (4,5), (3,5)\n    ],\n    segid=666,\n  )\n\n  vol.skeleton.upload(skel)\n\n  assert vol.cache.list_skeletons() == [ \'666.gz\' ]\n\n  skel.id = 1\n  with open(os.path.join(vol.cache.path, \'skeletons/1\'), \'wb\') as f:\n    f.write(skel.to_precomputed())\n\n  cached_skel = vol.skeleton.get(1)\n\n  assert cached_skel == skel\n\n  vol.cache.flush()\n\n\ndef test_simple_merge():\n  skel1 = Skeleton(\n    [ (0,0,0), (1,0,0), (2,0,0),  ], \n    edges=[ (0,1), (1,2), ],\n    segid=1,\n  )\n\n  skel2 = Skeleton(\n    [ (0,0,1), (1,0,2), (2,0,3),  ], \n    edges=[ (0,1), (1,2), ],\n    segid=1,\n  )\n\n  result = Skeleton.simple_merge([ skel1, skel2 ])\n\n  expected = Skeleton(\n    [ (0,0,0), (1,0,0), (2,0,0), (0,0,1), (1,0,2), (2,0,3), ], \n    edges=[ (0,1), (1,2), (3,4), (4,5) ],\n    segid=1,\n  )\n\n  assert result == expected\n\n  skel1.extra_attributes = [{\n    ""id"": ""wow"",\n    ""data_type"": ""uint8"",\n    ""components"": 1,\n  }]\n  skel1.wow = np.array([1,2,3], dtype=np.uint8)\n\n  skel2.extra_attributes = [{\n    ""id"": ""wow"",\n    ""data_type"": ""uint8"",\n    ""components"": 1,\n  }]\n  skel2.wow = np.array([4,5,6], dtype=np.uint8)\n\n  result = Skeleton.simple_merge([ skel1, skel2 ])\n  expected.wow = np.array([1,2,3,4,5,6], dtype=np.uint8)\n\n  assert result == expected\n\n  skel2.extra_attributes[0][\'data_type\'] = np.uint8\n\n  try:\n    Skeleton.simple_merge([ skel1, skel2 ])\n    assert False\n  except SkeletonAttributeMixingError:\n    pass\n\n  skel2.extra_attributes[0][\'data_type\'] = \'uint8\'\n  skel2.extra_attributes.append({\n    ""id"": ""amaze"",\n    ""data_type"": ""float32"",\n    ""components"": 2,\n  })\n\n  try:\n    Skeleton.simple_merge([ skel1, skel2 ])\n    assert False\n  except SkeletonAttributeMixingError:\n    pass\n\ndef test_sharded():\n  skel = Skeleton(\n    [ \n      (0,0,0), (1,0,0), (2,0,0),\n      (0,1,0), (0,2,0), (0,3,0),\n    ], \n    edges=[ \n      (0,1), (1,2), \n      (3,4), (4,5), (3,5)\n    ],\n    segid=1,\n    extra_attributes=[\n      {\n          ""id"": ""radius"",\n          ""data_type"": ""float32"",\n          ""num_components"": 1,\n      }\n    ]\n  ).physical_space()\n\n  skels = {}\n  for i in range(10):\n    sk = skel.clone()\n    sk.id = i\n    skels[i] = sk.to_precomputed()\n\n  mkdir(\'/tmp/removeme/skeletons/sharded/skeletons\')\n  with open(\'/tmp/removeme/skeletons/sharded/info\', \'wt\') as f:\n    f.write(jsonify(info))\n\n  for idxenc in (\'raw\', \'gzip\'):\n    for dataenc in (\'raw\', \'gzip\'):\n\n      spec = ShardingSpecification(\n        \'neuroglancer_uint64_sharded_v1\', \n        preshift_bits=1,\n        hash=\'murmurhash3_x86_128\', \n        minishard_bits=2, \n        shard_bits=1, \n        minishard_index_encoding=idxenc, \n        data_encoding=dataenc,\n      )\n      skel_info[\'sharding\'] = spec.to_dict()\n\n      with open(\'/tmp/removeme/skeletons/sharded/skeletons/info\', \'wt\') as f:\n        f.write(jsonify(skel_info))\n\n      files = spec.synthesize_shards(skels)\n      for fname in files.keys():\n        with open(\'/tmp/removeme/skeletons/sharded/skeletons/\' + fname, \'wb\') as f:\n          f.write(files[fname])      \n\n      cv = CloudVolume(\'file:///tmp/removeme/skeletons/sharded/\')\n      assert cv.skeleton.meta.mip == 3\n\n      for i in range(10):\n        sk = cv.skeleton.get(i).physical_space()\n        sk.id = 1\n        assert sk == skel\n\n      labels = []\n      for fname in files.keys():\n        lbls = cv.skeleton.reader.list_labels(fname, path=\'skeletons\')\n        labels += list(lbls)\n      \n      labels.sort()\n      assert labels == list(range(10))\n\n      for filename, shard in files.items():\n        decoded_skels = cv.skeleton.reader.disassemble_shard(shard)\n        for label, binary in decoded_skels.items():\n          Skeleton.from_precomputed(binary)\n\n      exists = cv.skeleton.reader.exists(list(range(11)), path=\'skeletons\')\n      assert exists == {\n        0: \'skeletons/0.shard\', \n        1: \'skeletons/0.shard\', \n        2: \'skeletons/0.shard\', \n        3: \'skeletons/0.shard\', \n        4: \'skeletons/0.shard\', \n        5: \'skeletons/0.shard\', \n        6: \'skeletons/0.shard\', \n        7: \'skeletons/0.shard\', \n        8: \'skeletons/1.shard\', \n        9: \'skeletons/1.shard\',\n        10: None,\n      }\n\n  shutil.rmtree(\'/tmp/removeme/skeletons\')\n\n\n\n'"
test/test_storage.py,0,"b'from __future__ import print_function\nfrom six.moves import range\n\nimport pytest\nimport re\nimport time\n\nfrom cloudvolume.storage import Storage\nfrom cloudvolume import exceptions, Bbox, chunks\nfrom layer_harness import delete_layer, TEST_NUMBER\n\n\n#TODO delete files created by tests\ndef test_read_write():\n  urls = [\n    ""file:///tmp/removeme/read_write"",\n    ""gs://seunglab-test/cloudvolume/read_write"",\n    ""s3://seunglab-test/cloudvolume/read_write""\n  ]\n\n  for num_threads in range(0,11,5):\n    for url in urls:\n      url = url + \'-\' + str(TEST_NUMBER)\n      with Storage(url, n_threads=num_threads) as s:\n        content = b\'some_string\'\n        s.put_file(\'info\', content, compress=None, cache_control=\'no-cache\')\n        s.wait()\n        assert s.get_file(\'info\') == content\n        assert s.get_file(\'nonexistentfile\') is None\n\n        num_infos = max(num_threads, 1)\n\n        results = s.get_files([ \'info\' for i in range(num_infos) ])\n\n        assert len(results) == num_infos\n        assert results[0][\'filename\'] == \'info\'\n        assert results[0][\'content\'] == content\n        assert all(map(lambda x: x[\'error\'] is None, results))\n        assert s.get_files([ \'nonexistentfile\' ])[0][\'content\'] is None\n\n        s.delete_file(\'info\')\n        s.wait()\n\n        s.put_json(\'info\', { \'omg\': \'wow\' }, cache_control=\'no-cache\')\n        s.wait()\n        results = s.get_json(\'info\')\n        assert results == { \'omg\': \'wow\' }\n\n  delete_layer(""/tmp/removeme/read_write"")\n\ndef test_http_read():\n  with Storage(""https://storage.googleapis.com/seunglab-test/test_v0/black/"") as stor:\n    info = stor.get_json(\'info\')\n\n  assert info == {\n    ""data_type"": ""uint8"",\n    ""num_channels"": 1,\n    ""scales"": [\n      {\n        ""chunk_sizes"": [\n          [\n            64,\n            64,\n            50\n          ]\n        ],\n        ""encoding"": ""raw"",\n        ""key"": ""6_6_30"",\n        ""resolution"": [\n          6,\n          6,\n          30\n        ],\n        ""size"": [\n          1024,\n          1024,\n          100\n        ],\n        ""voxel_offset"": [\n          0,\n          0,\n          0\n        ]\n      }\n    ],\n    ""type"": ""image""\n  }\n\n\ndef test_http_read_brotli_image():\n  fn = ""2_2_50/4096-4608_4096-4608_112-128""\n  bbox = Bbox.from_filename(fn) # possible off by one error w/ exclusive bounds\n\n  with Storage(""https://open-neurodata.s3.amazonaws.com/kharris15/apical/em"") as stor:\n    img_bytes = stor.get_file(fn)\n\n  img = chunks.decode(img_bytes, \'raw\', shape=bbox.size3(), dtype=""uint8"")\n\n  assert img.shape == (512, 512, 16)\n\n\ndef test_delete():\n  urls = [\n    ""file:///tmp/removeme/delete"",\n    ""gs://seunglab-test/cloudvolume/delete"",\n    ""s3://seunglab-test/cloudvolume/delete""\n  ]\n\n  for url in urls:\n    url = url + \'-\' + str(TEST_NUMBER)\n    with Storage(url, n_threads=1) as s:\n      content = b\'some_string\'\n      s.put_file(\'delete-test\', content, compress=None, cache_control=\'no-cache\').wait()\n      s.put_file(\'delete-test-compressed\', content, compress=\'gzip\', cache_control=\'no-cache\').wait()\n      assert s.get_file(\'delete-test\') == content\n      s.delete_file(\'delete-test\').wait()\n      assert s.get_file(\'delete-test\') is None\n\n      assert s.get_file(\'delete-test-compressed\') == content\n      s.delete_file(\'delete-test-compressed\').wait()\n      assert s.get_file(\'delete-test-compressed\') is None\n\n      # Reset for batch delete\n      s.put_file(\'delete-test\', content, compress=None, cache_control=\'no-cache\').wait()\n      s.put_file(\'delete-test-compressed\', content, compress=\'gzip\', cache_control=\'no-cache\').wait()\n      assert s.get_file(\'delete-test\') == content\n      assert s.get_file(\'delete-test-compressed\') == content\n\n      s.delete_files([\'delete-test\', \'delete-nonexistent\',\n                      \'delete-test-compressed\']).wait()\n      assert s.get_file(\'delete-test\') is None\n      assert s.get_file(\'delete-test-compressed\') is None\n\ndef test_compression():\n  urls = [\n    ""file:///tmp/removeme/compress"",\n    ""gs://seunglab-test/cloudvolume/compress"",\n    ""s3://seunglab-test/cloudvolume/compress""\n  ]\n\n  compression_tests = [\n    \'\',\n    None,\n    True,\n    False,\n    \'gzip\',\n    \'br\',\n  ]\n\n  for url in urls:\n    url = url + \'-\' + str(TEST_NUMBER)\n    for method in compression_tests:\n      with Storage(url, n_threads=5) as s:\n        content = b\'some_string\'\n\n        # remove when GCS enables ""br""\n        if method == ""br"" and ""gs://"" in url:\n          with pytest.raises(\n              exceptions.UnsupportedCompressionType, \n              match=""Brotli unsupported on google cloud storage""\n          ):\n            s.put_file(\'info\', content, compress=method)\n            s.wait()\n            retrieved = s.get_file(\'info\')\n        else:\n          s.put_file(\'info\', content, compress=method)\n          s.wait()\n          retrieved = s.get_file(\'info\')\n          assert content == retrieved\n\n        assert s.get_file(\'nonexistentfile\') is None\n\n    with Storage(url, n_threads=5) as s:\n      content = b\'some_string\'\n      try:\n        s.put_file(\'info\', content, compress=\'nonexistent\').wait()\n        assert False\n      except NotImplementedError:\n        pass\n\n    if ""file://"" in url:\n      delete_layer(""/tmp/removeme/compress-"" + str(TEST_NUMBER))\n\n@pytest.mark.parametrize(""compression_method"", (""gzip"", ""br""))\ndef test_compress_level(compression_method):\n  filepath = ""/tmp/removeme/compress_level-"" + str(TEST_NUMBER)\n  url = ""file://"" + filepath\n\n  content = b\'some_string\' * 1000\n\n  compress_levels = range(1, 9, 2)\n  for compress_level in compress_levels:\n    with Storage(url, n_threads=5) as s:\n      s.put_file(\'info\', content, compress=compression_method, compress_level=compress_level)\n      s.wait()\n\n      retrieved = s.get_file(\'info\')\n      assert content == retrieved\n\n      _, e = s._interface.get_file(""info"")\n      assert e == compression_method\n\n      assert s.get_file(\'nonexistentfile\') is None\n\n    delete_layer(filepath)\n\n\ndef test_list():  \n  urls = [\n    ""file:///tmp/removeme/list"",\n    ""gs://seunglab-test/cloudvolume/list"",\n    ""s3://seunglab-test/cloudvolume/list""\n  ]\n\n  for url in urls:\n    url = url + \'-\' + str(TEST_NUMBER)\n    with Storage(url, n_threads=5) as s:\n      print(\'testing service:\', url)\n      content = b\'some_string\'\n      s.put_file(\'info1\', content, compress=None)\n      s.put_file(\'info2\', content, compress=None)\n      s.put_file(\'build/info3\', content, compress=None)\n      s.put_file(\'level1/level2/info4\', content, compress=None)\n      s.put_file(\'info5\', content, compress=\'gzip\')\n      s.put_file(\'info.txt\', content, compress=None)\n      s.wait()\n      time.sleep(1) # sometimes it takes a moment for google to update the list\n      assert set(s.list_files(prefix=\'\')) == set([\'build/info3\',\'info1\', \'info2\', \'level1/level2/info4\', \'info5\', \'info.txt\'])\n      \n      assert set(s.list_files(prefix=\'inf\')) == set([\'info1\',\'info2\',\'info5\',\'info.txt\'])\n      assert set(s.list_files(prefix=\'info1\')) == set([\'info1\'])\n      assert set(s.list_files(prefix=\'build\')) == set([\'build/info3\'])\n      assert set(s.list_files(prefix=\'build/\')) == set([\'build/info3\'])\n      assert set(s.list_files(prefix=\'level1/\')) == set([\'level1/level2/info4\'])\n      assert set(s.list_files(prefix=\'nofolder/\')) == set([])\n\n      # Tests (1)\n      assert set(s.list_files(prefix=\'\', flat=True)) == set([\'info1\',\'info2\',\'info5\',\'info.txt\'])\n      assert set(s.list_files(prefix=\'inf\', flat=True)) == set([\'info1\',\'info2\',\'info5\',\'info.txt\'])\n      # Tests (2)\n      assert set(s.list_files(prefix=\'build\', flat=True)) == set([])\n      # Tests (3)\n      assert set(s.list_files(prefix=\'level1/\', flat=True)) == set([])\n      assert set(s.list_files(prefix=\'build/\', flat=True)) == set([\'build/info3\'])\n      # Tests (4)\n      assert set(s.list_files(prefix=\'build/inf\', flat=True)) == set([\'build/info3\'])\n\n      for file_path in (\'info1\', \'info2\', \'build/info3\', \'level1/level2/info4\', \'info5\', \'info.txt\'):\n        s.delete_file(file_path)\n  \n  delete_layer(""/tmp/removeme/list"")\n\n\ndef test_exists():\n  urls = [\n    ""file:///tmp/removeme/exists"",\n    ""gs://seunglab-test/cloudvolume/exists"",\n    ""s3://seunglab-test/cloudvolume/exists""\n  ]\n\n  for url in urls:\n    url = url + \'-\' + str(TEST_NUMBER)\n    with Storage(url, n_threads=5) as s:\n      content = b\'some_string\'\n      s.put_file(\'info\', content, compress=None)\n      s.wait()\n      time.sleep(1) # sometimes it takes a moment for google to update the list\n      \n      assert s.exists(\'info\')\n      assert not s.exists(\'doesntexist\')\n      s.delete_file(\'info\')\n\ndef test_access_non_cannonical_paths():\n  urls = [\n    ""file:///tmp/noncanon"",\n    ""gs://seunglab-test/noncanon"",\n    ""s3://seunglab-test/noncanon""\n  ]\n\n  for url in urls:\n    url = url + \'-\' + str(TEST_NUMBER)\n    with Storage(url, n_threads=5) as s:\n      content = b\'some_string\'\n      s.put_file(\'info\', content, compress=None)\n      s.wait()\n      time.sleep(0.5) # sometimes it takes a moment for google to update the list\n      \n      assert s.get_file(\'info\') == content\n      assert s.get_file(\'nonexistentfile\') is None\n      s.delete_file(\'info\')\n      s.wait()'"
test/test_threadedqueue.py,0,"b'from six.moves import range\nfrom cloudvolume.threaded_queue import ThreadedQueue\nfrom functools import partial\n\ndef test_threading():\n  execution_count = 1000\n  executions = []\n\n\n  def reset_executions():\n    return [ False for _ in range(execution_count) ]\n\n  def addone(idnum, should_be_none):\n    executions[idnum] = True    \n    assert should_be_none is None\n\n  executions = reset_executions()\n\n  with ThreadedQueue(n_threads=1) as tq:\n    for idnum in range(execution_count):\n      fn = partial(addone, idnum)\n      tq.put(fn)\n  assert all(executions)\n\n  executions = reset_executions()\n  tq = ThreadedQueue(n_threads=40)\n  for idnum in range(execution_count):\n    fn = partial(addone, idnum)\n    tq.put(fn)\n  tq.wait().kill_threads()\n  assert tq.processed == execution_count\n  assert all(executions)\n\n  # Base class with 0 threads on with statement will never terminate\n  try:\n    with ThreadedQueue(n_threads=0) as tq:\n      assert False\n  except ValueError:\n    assert True\n  except Exception:\n    assert False\n\ndef test_derived_class():\n  \n  def what_fun(should_be_fun):\n    assert should_be_fun == \'fun\'\n\n  class DerivedThreadedQueue(ThreadedQueue):\n    def _initialize_interface(self):\n      return \'fun\'\n\n  with DerivedThreadedQueue(n_threads=1) as tq:\n    for _ in range(1000):\n      tq.put(what_fun)\n\n    tq.wait()\n    assert tq.processed == 1000\n\n  # shouldn\'t crash w/ 0 threads because it\'s a derived class\n  with DerivedThreadedQueue(n_threads=0) as tq:\n    pass\n\ndef test_threads_die():\n  tq = ThreadedQueue(n_threads=40)\n  assert tq.are_threads_alive()\n  tq.kill_threads()\n  assert not tq.are_threads_alive()\n\n  tq = ThreadedQueue(n_threads=0)\n  assert not tq.are_threads_alive()\n\n  with ThreadedQueue(n_threads=40) as tq:\n    threads = tq._threads\n  \n  assert not any(map(lambda t: t.is_alive(), threads))\n\ndef test_thread_exceptions():\n\n  def diediedie(interface):  \n    raise NotImplementedError(""Not implemented at all."")\n\n  tq = ThreadedQueue(n_threads=40)\n  for _ in range(1000):\n    tq.put(diediedie)\n\n  try:\n    tq.wait()\n  except NotImplementedError:\n    pass\n\n\n\n'"
cloudvolume/datasource/__init__.py,2,"b'from .. import exceptions\nfrom ..lib import yellow, Bbox, Vec\n\nimport numpy as np\n\nNON_ALIGNED_WRITE = yellow(\n  """"""\n  Non-Aligned writes are disabled by default. There are several good reasons \n  not to use them. \n\n  1) Memory and Network Inefficiency\n    Producing non-aligned writes requires downloading the chunks that overlap\n    with the write area but are not wholly contained. They are then painted\n    in the overlap region and added to the upload queue. This requires\n    the outer shell of the aligned image to be downloaded painted, and uploaded. \n  2) Race Conditions\n    If you are using multiple instances of CloudVolume, the partially overlapped \n    chunks will be downloaded, partially painted, and uploaded. If this procedure\n    occurs close in time between two processes, the final chunk may be partially\n    painted. If you are sure only one CloudVolume instance will be accessing an\n    area at any one time, this is not a problem.\n\n  If after reading this you are still sure you want non-aligned writes, you can\n  set non_aligned_writes=True.\n\n  Alignment Check: \n    Mip:             {mip}\n    Chunk Size:      {chunk_size}\n    Volume Offset:   {offset}\n    Received:        {got} \n    Nearest Aligned: {check}\n"""""")\n\nclass ImageSourceInterface(object):\n  def download(self, bbox, mip):\n    raise NotImplementedError()\n  def upload(self, image, offset, mip):\n    raise NotImplementedError()\n  def exists(self, bbox, mip):\n    raise NotImplementedError()\n  def delete(self, bbox, mip):\n    raise NotImplementedError()\n  def transfer_to(self, cloudpath, bbox, mip):\n    raise NotImplementedError()\n\ndef readonlyguard(fn):\n  def guardfn(self, *args, **kwargs):\n    if self.readonly:\n      raise exceptions.ReadOnlyException(self.meta.cloudpath)\n    return fn(self, *args, **kwargs)\n  return guardfn\n\ndef check_grid_aligned(meta, img, bounds, mip, throw_error=False):\n  """"""Raise a cloudvolume.exceptions.AlignmentError if the provided image is not grid aligned.""""""\n  shape = Vec(*img.shape)[:3]\n  alignment_check = bounds.expand_to_chunk_size(meta.chunk_size(mip), meta.voxel_offset(mip))\n  alignment_check = Bbox.clamp(alignment_check, meta.bounds(mip))\n  is_aligned = np.all(alignment_check.minpt == bounds.minpt) and np.all(alignment_check.maxpt == bounds.maxpt)\n  \n  if throw_error and is_aligned == False:\n    msg = NON_ALIGNED_WRITE.format(\n      mip=mip, chunk_size=meta.chunk_size(mip), \n      offset=meta.voxel_offset(mip), \n      got=bounds, check=alignment_check\n    )\n    raise exceptions.AlignmentError(msg)\n\n  return is_aligned\n\ndef autocropfn(meta, image, bbox, mip):\n  cropped_bbox = Bbox.intersection(bbox, meta.bounds(mip))\n  dmin = np.abs(bbox.minpt - cropped_bbox.minpt)\n  img_bbox = Bbox(dmin, dmin + cropped_bbox.size())\n  image = image[ img_bbox.to_slices() ]\n\n  return image, cropped_bbox\n\n\n'"
cloudvolume/frontends/__init__.py,0,b'from .precomputed import CloudVolumePrecomputed\nfrom .graphene import CloudVolumeGraphene'
cloudvolume/frontends/graphene.py,13,"b'from collections import defaultdict\nfrom datetime import datetime\nimport json\nimport os\nimport pickle\nimport posixpath\nimport re\nimport requests\nimport sys\n\nimport dateutil.parser\nimport fastremap\nimport numpy as np\n\nfrom .. import compression\nfrom .. import exceptions\nfrom ..cacheservice import CacheService\nfrom ..lib import Bbox, Vec, toiter\nfrom ..storage import SimpleStorage, Storage, reset_connection_pools\nfrom ..volumecutout import VolumeCutout\nfrom ..datasource.graphene.metadata import GrapheneApiVersion\n\nfrom .precomputed import CloudVolumePrecomputed\n\ndef warn(text):\n  print(colorize(\'yellow\', text))\n\ndef to_unix_time(timestamp):\n  """"""\n  Accepts integer UNIX timestamps, ISO 8601 datetime strings,\n  and Python datetime objects and returns them as the equivalent\n  UNIX timestamp or None if timestamp is None.\n  """"""\n  if isinstance(timestamp, str):\n    timestamp = dateutil.parser.parse(timestamp) # returns datetime\n  if isinstance(timestamp, datetime): # NB. do not change to elif\n    timestamp = datetime.timestamp(timestamp)\n\n  if not isinstance(timestamp, int) and timestamp is not None:\n    raise ValueError(""Not able to convert {} to UNIX time."".format(timestamp))\n\n  return timestamp\n\nclass CloudVolumeGraphene(CloudVolumePrecomputed):\n\n  @property\n  def manifest_endpoint(self):\n    return self.meta.manifest_endpoint\n\n  @property\n  def graph_chunk_size(self):\n    return self.meta.graph_chunk_size \n\n  @property\n  def mesh_chunk_size(self):\n    # TODO: add this as new parameter to the info as it can be different from the chunkedgraph chunksize\n    return self.meta.mesh_chunk_size\n  \n  def download_point(\n    self, pt, size=256, \n    mip=None, parallel=None, \n    coord_resolution=None,\n    **kwargs\n  ):\n    """"""\n    Download to the right of point given in mip 0 coords.\n    Useful for quickly visualizing a neuroglancer coordinate\n    at an arbitary mip level.\n\n    pt: (x,y,z)\n    size: int or (sx,sy,sz)\n    mip: int representing resolution level\n    parallel: number of processes to launch (0 means all cores)\n    coord_resolution: (rx,ry,rz) the coordinate resolution of the input point.\n      Sometimes Neuroglancer is working in the resolution of another\n      higher res layer and this can help correct that.\n\n    Also accepts the arguments for download such as segids and preserve_zeros.\n\n    Return: image as VolumeCutout(ndarray)\n    """"""\n    if isinstance(size, int) or isinstance(size, float):\n      size = Vec(size, size, size)\n    else:\n      size = Vec(*size)\n\n    if mip is None:\n      mip = self.mip\n\n    mip = self.meta.to_mip(mip)\n    size2 = size // 2\n\n    if coord_resolution is not None:\n      factor = self.meta.resolution(0) / Vec(*coord_resolution)\n      pt = Vec(*pt) / factor\n\n    pt = self.point_to_mip(pt, mip=0, to_mip=mip)\n\n    if all(size == 1):\n      bbox = Bbox(pt, pt + 1).astype(np.int64)\n    else:\n      bbox = Bbox(pt - size2, pt + size2).astype(np.int64)\n\n    if parallel is None:\n      parallel = self.parallel\n\n    return self.download(bbox, mip, parallel=parallel, **kwargs)\n\n  def download(\n    self, bbox, mip=None, \n    parallel=None, segids=None,\n    preserve_zeros=False,\n    agglomerate=False, timestamp=None,\n    stop_layer=None\n  ):\n    """"""\n    Downloads base segmentation and optionally agglomerates\n    labels based on information in the graph server.\n\n    bbox: specifies cutout to fetch\n    mip: which resolution level to get (default self.mip)\n    parallel: what parallel level to use (default self.parallel)\n\n    agglomerate: if true, remap all watershed ids in the volume\n      and return a flat segmentation.\n\n    if agglomerate is true these options are available:\n\n    timestamp: (agglomerate only) get the roots from this date and time\n      formats accepted:\n        int: unix timestamp\n        datetime: self explainatory\n        string: ISO 8601 date\n    stop_layer: (agglomerate only) (int) if specified, return the lowest \n      parent at or above that layer. If not specified, go all the way \n      to the root id. \n        Layer 1: Watershed\n        Layer 2: Within-Chunk Agglomeration\n        Layer 2+: Between chunk interconnections (skip connections possible)\n\n    if agglomerate is false, these other options come into play:\n\n    segids: agglomerate the leaves of these segids from the graph \n      server and label them with the given segid.\n    preserve_zeros: If segids is not None:\n      False: mask other segids with zero\n      True: mask other segids with the largest integer value\n        contained by the image data type and leave zero as is.\n\n    Returns: img as a VolumeCutout\n    """"""\n    if type(bbox) is Vec:\n      bbox = Bbox(bbox, bbox+1)\n    \n    bbox = Bbox.create(\n      bbox, context=self.bounds, \n      bounded=self.bounded, \n      autocrop=self.autocrop\n    )\n  \n    if bbox.subvoxel():\n      raise exceptions.EmptyRequestException(""Requested {} is smaller than a voxel."".format(bbox))\n\n    if (agglomerate and stop_layer is not None) and (stop_layer <= 0 or stop_layer > self.meta.n_layers):\n      raise ValueError(""Stop layer {} must be 1 <= stop_layer <= {} or None."".format(stop_layer, self.meta.n_layers))\n\n    if mip is None:\n      mip = self.mip\n\n    mip0_bbox = self.bbox_to_mip(bbox, mip=mip, to_mip=0)\n    # Only ever necessary to make requests within the bounding box\n    # to the server. We can fill black in other situations.\n    mip0_bbox = bbox.intersection(self.meta.bounds(0), mip0_bbox)\n\n    img = super(CloudVolumeGraphene, self).download(bbox, mip=mip, parallel=parallel)\n\n    if agglomerate:\n      img = self.agglomerate_cutout(img, timestamp=timestamp, stop_layer=stop_layer)\n      return VolumeCutout.from_volume(self.meta, mip, img, bbox)\n\n    if segids is None:\n      return img\n\n    segids = list(toiter(segids))\n\n    remapping = {}\n    for segid in segids:\n      leaves = self.get_leaves(segid, mip0_bbox, 0)\n      remapping.update({ leaf: segid for leaf in leaves })\n    \n    img = fastremap.remap(img, remapping, preserve_missing_labels=True, in_place=True)\n\n    mask_value = 0\n    if preserve_zeros:\n      mask_value = np.inf\n      if np.issubdtype(self.dtype, np.integer):\n        mask_value = np.iinfo(self.dtype).max\n\n      segids.append(0)\n\n    img = fastremap.mask_except(img, segids, in_place=True, value=mask_value)\n\n    return VolumeCutout.from_volume(\n      self.meta, mip, img, bbox \n    )\n  \n  def agglomerate_cutout(self, img, timestamp=None, stop_layer=None):\n    """"""Remap a graphene volume to its latest root ids. This creates a flat segmentation.""""""\n    if np.all(img == self.image.background_color) or stop_layer == 1:\n      return img\n\n    labels = fastremap.unique(img)\n    if labels.size and labels[0] == 0:\n      labels = labels[1:]\n\n    roots = self.get_roots(labels, timestamp=timestamp, binary=True, stop_layer=stop_layer)\n    mapping = { segid: root for segid, root in zip(labels, roots) }\n    return fastremap.remap(img, mapping, preserve_missing_labels=True, in_place=True)\n\n  def __getitem__(self, slices):\n    return self.download(\n      slices, mip=self.mip,\n      preserve_zeros=True,\n      parallel=self.parallel, \n    )\n\n  def get_chunk_layer(self, node_or_chunk_id):\n    """"""\n    Extract Layer from Node ID or Chunk ID\n    \n    Returns: (int) layer number\n    """"""\n    return int(self.meta.decode_layer_id(node_or_chunk_id))\n\n  def get_root(self, segid, *args, **kwargs):\n    """"""Deprecated. Get a single root id for a single segid.""""""\n    return get_roots(segid, *args, **kwargs)[0]\n\n  def get_roots(self, segids, timestamp=None, binary=True, stop_layer=None):\n    """"""\n    Get the root ids for these labels.\n\n    segids: (int or iterable) one or more segids to remap\n    timestamp: get the roots from this date and time\n      formats accepted:\n        int: unix timestamp\n        datetime: self explainatory\n        string: ISO 8601 date\n    binary: if true, send and receive segids as a \n      binary stream else, use JSON. The difference can\n      be a 2x difference in bandwidth used.\n    stop_layer: (int) if specified, return the lowest parent at or above \n      that layer. If not specified, go all the way to the root id. \n        Layer 1: Watershed\n        Layer 2: Within-Chunk Agglomeration\n        Layer 2+: Between chunk interconnections (skip connections possible)\n    """"""\n    segids = toiter(segids)\n    if isinstance(segids, np.ndarray):\n      segids = segids.tolist()\n\n    try:\n      segids.remove(0) # background segid\n    except ValueError:\n      pass\n\n    timestamp = to_unix_time(timestamp)\n\n    if stop_layer is not None:\n      stop_layer = int(stop_layer)\n      if stop_layer < 1 or stop_layer > self.meta.n_layers:\n        raise ValueError(""stop_layer ({}) must be between 1 and {} inclusive."".format(\n          stop_layer, self.meta.n_layers\n        ))\n\n    if self.meta.supports_api(\'v1\'):\n      roots = self._get_roots_v1(segids, timestamp, binary, stop_layer)\n    elif self.meta.supports_api(\'1.0\'):\n      roots = self._get_roots_legacy(segids, timestamp)\n    else:\n      raise exceptions.UnsupportedGrapheneAPIVersionError(\n        ""{} is not a supported API version. Supported versions: "".format(self.meta.api_version) \\\n        + "", "".join([ str(_) for _ in self.meta.supported_api_versions ])\n      )\n\n    return np.array(roots, dtype=self.meta.dtype)\n\n  def get_chunk_mappings(self, chunk_id, timestamp=None):\n    """"""\n    Get the mapping of segments in a chunk at a given chunk graph layer \n    to their L1 watershed components.\n\n    NOTE: Only L2 chunks are supported at this time.\n\n    Required:\n      chunk_id: uint64 chunk id (ie. an graphene label with a zeroed segid component)\n        NOTE: This function actually accepts any graphene label and automatically converts\n        it to a chunk ID before querying the graph server by zeroing out its segid component.\n    Optional:\n      timestamp: query the state of the graph server at the time point specified\n        by a UNIX timestamp, ISO 8601 datetime string, or a python datetime object.\n\n    Returns: {  \n      chunk_label: [ watershed labels ],\n      ... e.g. ...\n      173729460028178433: [79450023430979610, 79450023431072298, ... ]\n    }\n    """"""\n    timestamp = to_unix_time(timestamp)\n\n    if not self.meta.supports_api(\'v1\'):\n      raise exceptions.UnsupportedGrapheneAPIVersionError(\n        ""{} is not a supported API version for range read requests. Currently, only version 1.0 is supported: "".format(self.meta.api_version) \\\n      )\n\n    layer_id = self.meta.decode_layer_id(chunk_id)\n    if layer_id != 2:\n      raise ValueError(""This function currently only accepts Layer 2 chunk IDs. Got {}"".format(self.meta.decode_label(chunk_id)))\n\n    chunk_id = self.meta.decode_chunk_id(chunk_id)\n    \n    version = GrapheneApiVersion(\'v1\')\n    path = version.path(self.meta.server_path)\n    url = posixpath.join(self.meta.base_path, path, ""l2_chunk_children_binary"", str(chunk_id))\n\n    params = {\'as_array\': True}\n    if timestamp is not None:\n      params[\'timestamp\'] = timestamp\n\n    response = requests.get(url, params=params, headers=self.meta.auth_header)\n    response.raise_for_status()\n\n    chunk_array = np.frombuffer(response.content, dtype=np.uint64)\n    chunk_mappings = defaultdict(list)\n\n    for i in range(0, len(chunk_array), 2):\n      chunk_mappings[chunk_array[i]].append(chunk_array[i+1])\n\n    return chunk_mappings\n\n  def _get_roots_v1(self, segids, timestamp, binary=False, stop_layer=None):\n    args = {}\n    if timestamp is not None:\n      args[\'timestamp\'] = timestamp\n\n    headers = {}\n    headers.update(self.meta.auth_header)\n\n    gzip_condition = len(segids) * 8 > 1e6\n\n    if gzip_condition:\n      headers[\'Content-Encoding\'] = \'gzip\'\n      headers[\'Accept-Encoding\'] = \'gzip;q=1, identity;q=0.1\'\n    else:\n      headers[\'Accept-Encoding\'] = \'identity\'\n\n    version = GrapheneApiVersion(\'v1\')\n    path = version.path(self.meta.server_path)\n\n    params = {}\n    if stop_layer:\n      params[\'stop_layer\'] = int(stop_layer)\n\n    if binary:\n      url = posixpath.join(self.meta.base_path, path, ""roots_binary"")\n      data = np.array(segids, dtype=np.uint64).tobytes()\n      params[\'timestamp\'] = timestamp\n    else:\n      url = posixpath.join(self.meta.base_path, path, ""roots"")\n      args[\'node_ids\'] = segids\n      data = json.dumps(args).encode(\'utf8\')\n\n    if gzip_condition:\n      data = compression.compress(data, method=\'gzip\')\n    \n    response = requests.post(url, data=data, headers=headers, params=params)\n    response.raise_for_status()\n\n    if binary:\n      return np.frombuffer(response.content, dtype=np.uint64)\n    else:\n      return json.loads(response.content)[\'root_ids\']\n\n  def _get_roots_legacy(self, segids, timestamp):\n    args = {}\n    if timestamp is not None:\n      args[\'timestamp\'] = timestamp\n\n    version = GrapheneApiVersion(\'1.0\')\n    path = version.path(self.meta.server_path)\n    roots = []\n    for segid in segids:\n      url = posixpath.join(self.meta.base_path, path, ""graph/{:d}/root"".format(int(segid)))\n      response = requests.get(url, json=args, headers=self.meta.auth_header)\n      response.raise_for_status()\n      root = np.frombuffer(response.content, dtype=np.uint64)[0]\n      roots.append(root)\n    return roots\n\n  def get_leaves(self, root_id, bbox, mip):\n    """"""\n    get the supervoxels for this root_id\n\n    params\n    ------\n    root_id: uint64 root id to find supervoxels for\n    bbox: cloudvolume.lib.Bbox 3d bounding box for segmentation\n    """"""\n    root_id = int(root_id)\n    url = posixpath.join(self.meta.server_url, ""segment"", str(root_id), ""leaves"")\n    bbox = Bbox.create(bbox, context=self.meta.bounds(mip), bounded=self.bounded)\n    response = requests.post(url, json=[ root_id ], params={\n      \'bounds\': bbox.to_filename(),\n    }, headers=self.meta.auth_header)\n    response.raise_for_status()\n\n    return np.frombuffer(response.content, dtype=np.uint64)\n'"
cloudvolume/frontends/precomputed.py,12,"b'from __future__ import print_function\n\nimport itertools\nimport gevent.socket\nimport json\nimport os\nimport sys\nimport uuid\nimport socket\n\nimport fastremap\nfrom six.moves import range\nimport numpy as np\nfrom tqdm import tqdm\nfrom six import string_types\nimport multiprocessing as mp\n\nfrom .. import lib\nfrom ..cacheservice import CacheService\nfrom .. import exceptions \nfrom ..lib import ( \n  colorize, red, mkdir, \n  Vec, Bbox, jsonify\n)\n\nfrom ..datasource import autocropfn\nfrom ..datasource.precomputed import PrecomputedMetadata\n\nfrom ..paths import strict_extract\nfrom ..provenance import DataLayerProvenance\nfrom ..storage import SimpleStorage, Storage, reset_connection_pools\nfrom ..volumecutout import VolumeCutout\nfrom .. import sharedmemory\n\ndef warn(text):\n  print(colorize(\'yellow\', text))\n\nclass CloudVolumePrecomputed(object):\n  def __init__(self, \n    meta, cache, config,\n    image=None, mesh=None, skeleton=None,\n    mip=0\n  ):\n    self.config = config \n    self.cache = cache \n    self.meta = meta\n\n    self.image = image\n    self.mesh = mesh \n    self.skeleton = skeleton\n\n    self.green_threads = self.config.green # display warning message\n\n    # needs to be set after info is defined since\n    # its setter is based off of scales\n    self.mip = mip\n    self.pid = os.getpid()\n\n  @property \n  def autocrop(self):\n    return self.image.autocrop\n\n  @autocrop.setter\n  def autocrop(self, val):\n    self.image.autocrop = val\n\n  @property\n  def background_color(self):\n    return self.image.background_color \n\n  @background_color.setter\n  def background_color(self, val):\n    self.image.background_color = val  \n\n  @property \n  def bounded(self):\n    return self.image.bounded\n\n  @bounded.setter \n  def bounded(self, val):\n    self.image.bounded = val\n\n  @property\n  def fill_missing(self):\n    return self.image.fill_missing\n  \n  @fill_missing.setter\n  def fill_missing(self, val):\n    self.image.fill_missing = val\n  \n  @property\n  def green_threads(self):\n    return self.config.green\n  \n  @green_threads.setter \n  def green_threads(self, val):\n    if val and socket.socket is not gevent.socket.socket:\n      warn(""""""\n      WARNING: green_threads is set but this process is\n      not monkey patched. This will cause severely degraded\n      performance.\n      \n      CloudVolume uses gevent for cooperative (green)\n      threading but it requires patching the Python standard\n      library to perform asynchronous IO. Add this code to\n      the top of your program (before any other imports):\n\n        import gevent.monkey\n        gevent.monkey.patch_all(threads=False)\n\n      More Information:\n\n      http://www.gevent.org/intro.html#monkey-patching\n      """""")\n\n    self.config.green = bool(val)\n\n  @property\n  def non_aligned_writes(self):\n    return self.image.non_aligned_writes\n\n  @non_aligned_writes.setter\n  def non_aligned_writes(self, val):\n    self.image.non_aligned_writes = val\n\n  @property\n  def delete_black_uploads(self):\n    return self.image.delete_black_uploads\n\n  @delete_black_uploads.setter\n  def delete_black_uploads(self, val):\n    self.image.delete_black_uploads = val\n\n  @property\n  def parallel(self):\n    return self.config.parallel\n\n  @parallel.setter\n  def parallel(self, num_processes):\n    if type(num_processes) == bool:\n      num_processes = mp.cpu_count() if num_processes == True else 1\n    elif num_processes <= 0:\n      raise ValueError(\'Number of processes must be >= 1. Got: \' + str(num_processes))\n    else:\n      num_processes = int(num_processes)\n\n    self.config.parallel = num_processes\n\n  @property\n  def cdn_cache(self):\n    return self.config.cdn_cache\n\n  @cdn_cache.setter \n  def cdn_cache(self, val):\n    self.config.cdn_cache = val\n\n  @property \n  def compress(self):\n    return self.config.compress\n\n  @compress.setter \n  def compress(self, val):\n    self.config.compress = val \n\n  @property \n  def progress(self):\n    return self.config.progress \n\n  @progress.setter \n  def progress(self, val):\n    self.config.progress = bool(val)\n\n  @property \n  def info(self):\n    return self.meta.info\n\n  @info.setter\n  def info(self, val):\n    self.meta.info = val\n  \n  @property\n  def provenance(self):\n    return self.meta.provenance\n\n  @provenance.setter\n  def provenance(self, val):\n    self.meta.provenance = val\n\n  def __setstate__(self, d):\n    """"""Called when unpickling which is integral to multiprocessing.""""""\n    self.__dict__ = d \n    \n    pid = os.getpid()\n    if \'pid\' in d and d[\'pid\'] != pid:\n      # otherwise the pickle might have references to old connections\n      reset_connection_pools() \n      self.pid = pid\n  \n  @classmethod\n  def create_new_info(cls, \n    num_channels, layer_type, data_type, encoding, \n    resolution, voxel_offset, volume_size, \n    mesh=None, skeletons=None, chunk_size=(64,64,64),\n    compressed_segmentation_block_size=(8,8,8),\n    max_mip=0, factor=Vec(2,2,1), redirect=None, \n    *args, **kwargs\n  ):\n    """"""\n    Create a new neuroglancer Precomputed info file.\n\n    Required:\n      num_channels: (int) 1 for grayscale, 3 for RGB \n      layer_type: (str) typically ""image"" or ""segmentation""\n      data_type: (str) e.g. ""uint8"", ""uint16"", ""uint32"", ""float32""\n      encoding: (str) ""raw"" for binaries like numpy arrays, ""jpeg""\n      resolution: int (x,y,z), x,y,z voxel dimensions in nanometers\n      voxel_offset: int (x,y,z), beginning of dataset in positive cartesian space\n      volume_size: int (x,y,z), extent of dataset in cartesian space from voxel_offset\n    \n    Optional:\n      mesh: (str) name of mesh directory, typically ""mesh""\n      skeletons: (str) name of skeletons directory, typically ""skeletons""\n      chunk_size: int (x,y,z), dimensions of each downloadable 3D image chunk in voxels\n      compressed_segmentation_block_size: (x,y,z) dimensions of each compressed sub-block\n        (only used when encoding is \'compressed_segmentation\')\n      max_mip: (int), the maximum mip level id.\n      factor: (Vec), the downsampling factor for each mip level\n      redirect: If this volume has moved, you can set an automatic redirect\n        by specifying a cloudpath here.\n\n    Returns: dict representing a single mip level that\'s JSON encodable\n    """"""\n    return PrecomputedMetadata.create_info(\n      num_channels, layer_type, data_type, encoding, \n      resolution, voxel_offset, volume_size, \n      mesh, skeletons, chunk_size,\n      compressed_segmentation_block_size,\n      max_mip, factor,\n      *args, **kwargs\n    )\n\n  def refresh_info(self):\n    """"""Restore the current info from cache or storage.""""""\n    return self.meta.refresh_info()\n\n  def commit_info(self):\n    return self.meta.commit_info()\n\n  def refresh_provenance(self):\n    return self.meta.refresh_provenance()\n\n  def commit_provenance(self):\n    return self.meta.commit_provenance()\n\n  @property\n  def dataset_name(self):\n    return self.meta.dataset\n  \n  @property\n  def layer(self):\n    return self.meta.layer\n\n  @property\n  def mip(self):\n    return self.config.mip\n\n  @mip.setter\n  def mip(self, mip):\n    self.config.mip = self.meta.to_mip(mip)\n\n  @property\n  def scales(self):\n    return self.meta.scales\n\n  @scales.setter\n  def scales(self, val):\n    self.meta.scales = val\n\n  @property\n  def scale(self):\n    return self.meta.scale(self.mip)\n\n  @scale.setter\n  def scale(self, val):\n    self.info[\'scales\'][self.mip] = val\n\n  def mip_scale(self, mip):\n    return self.meta.scale(mip)\n\n  @property\n  def basepath(self):\n    return self.meta.basepath\n\n  @property \n  def layerpath(self):\n    return self.meta.layerpath\n\n  @property\n  def base_cloudpath(self):\n    return self.meta.base_cloudpath\n\n  @property \n  def cloudpath(self):\n    return self.layer_cloudpath\n\n  @property\n  def layer_cloudpath(self):\n    return self.meta.cloudpath\n\n  @property\n  def info_cloudpath(self):\n    return self.meta.infopath\n\n  @property\n  def cache_path(self):\n    return self.cache.path\n\n  @property\n  def ndim(self):\n    return len(self.shape)\n\n  def mip_ndim(self, mip):\n    return len(self.meta.shape(mip))\n\n  @property\n  def shape(self):\n    """"""Returns Vec(x,y,z,channels) shape of the volume similar to numpy."""""" \n    return tuple(self.meta.shape(self.mip))\n\n  def mip_shape(self, mip):\n    return tuple(self.meta.shape(mip))\n\n  @property\n  def volume_size(self):\n    """"""Returns Vec(x,y,z) shape of the volume (i.e. shape - channels)."""""" \n    return self.meta.volume_size(self.mip)\n\n  def mip_volume_size(self, mip):\n    return self.meta.volume_size(mip)\n\n  @property\n  def available_mips(self):\n    """"""Returns a list of mip levels that are defined.""""""\n    return self.meta.available_mips\n\n  @property\n  def available_resolutions(self):\n    """"""Returns a list of defined resolutions.""""""\n    return (s[""resolution""] for s in self.scales)\n\n  @property\n  def layer_type(self):\n    """"""e.g. \'image\' or \'segmentation\'""""""\n    return self.meta.layer_type\n\n  @property\n  def dtype(self):\n    """"""e.g. \'uint8\'""""""\n    return self.meta.dtype\n\n  @property\n  def data_type(self):\n    return self.meta.data_type\n\n  @property\n  def encoding(self):\n    """"""e.g. \'raw\' or \'jpeg\'""""""\n    return self.meta.encoding(self.mip)\n\n  def mip_encoding(self, mip):\n    return self.meta.encoding(mip)\n\n  @property\n  def compressed_segmentation_block_size(self):\n    return self.mip_compressed_segmentation_block_size(self.mip)\n\n  def mip_compressed_segmentation_block_size(self, mip):\n    if \'compressed_segmentation_block_size\' in self.info[\'scales\'][mip]:\n      return self.info[\'scales\'][mip][\'compressed_segmentation_block_size\']\n    return None\n\n  @property\n  def num_channels(self):\n    return self.meta.num_channels\n\n  @property\n  def voxel_offset(self):\n    """"""Vec(x,y,z) start of the dataset in voxels""""""\n    return self.meta.voxel_offset(self.mip)\n\n  def mip_voxel_offset(self, mip):\n    return self.meta.voxel_offset(mip)\n\n  @property \n  def resolution(self):\n    """"""Vec(x,y,z) dimensions of each voxel in nanometers""""""\n    return self.meta.resolution(self.mip)\n\n  def mip_resolution(self, mip):\n    return self.meta.resolution(mip)\n\n  @property\n  def downsample_ratio(self):\n    """"""Describes how downsampled the current mip level is as an (x,y,z) factor triple.""""""\n    return self.meta.downsample_ratio(self.mip)\n\n  @property\n  def chunk_size(self):\n    """"""Underlying chunk size dimensions in voxels. Synonym for underlying.""""""\n    return self.meta.chunk_size(self.mip)\n\n  def mip_chunk_size(self, mip):\n    return self.meta.chunk_size(mip)\n\n  @property\n  def underlying(self):\n    """"""Underlying chunk size dimensions in voxels. Synonym for chunk_size.""""""\n    return self.meta.chunk_size(self.mip)\n\n  def mip_underlying(self, mip):\n    return self.meta.chunk_size(mip)\n\n  @property\n  def key(self):\n    """"""The subdirectory within the data layer containing the chunks for this mip level""""""\n    return self.meta.key(self.mip)\n\n  def mip_key(self, mip):\n    return self.meta.key(mip)\n\n  @property\n  def bounds(self):\n    """"""Returns a bounding box for the dataset with dimensions in voxels""""""\n    return self.meta.bounds(self.mip)\n\n  def mip_bounds(self, mip):\n    offset = self.meta.voxel_offset(mip)\n    shape = self.meta.volume_size(mip)\n    return Bbox( offset, offset + shape )\n\n  def point_to_mip(self, pt, mip, to_mip):\n    return self.meta.point_to_mip(pt, mip, to_mip)\n\n  def bbox_to_mip(self, bbox, mip, to_mip):\n    """"""Convert bbox or slices from one mip level to another.""""""\n    return self.meta.bbox_to_mip(bbox, mip, to_mip)\n\n  def slices_to_global_coords(self, slices):\n    """"""\n    Used to convert from a higher mip level into mip 0 resolution.\n    """"""\n    bbox = self.meta.bbox_to_mip(slices, self.mip, 0)\n    return bbox.to_slices()\n\n  def slices_from_global_coords(self, slices):\n    """"""\n    Used for converting from mip 0 coordinates to upper mip level\n    coordinates. This is mainly useful for debugging since the neuroglancer\n    client displays the mip 0 coordinates for your cursor.\n    """"""\n    bbox = self.meta.bbox_to_mip(slices, 0, self.mip)\n    return bbox.to_slices()\n\n  def reset_scales(self):\n    """"""Used for manually resetting downsamples if something messed up.""""""\n    self.meta.reset_scales()\n    return self.commit_info()\n\n  def add_scale(self, factor, encoding=None, chunk_size=None, info=None):\n    """"""\n    Generate a new downsample scale to for the info file and return an updated dictionary.\n    You\'ll still need to call self.commit_info() to make it permenant.\n\n    Required:\n      factor: int (x,y,z), e.g. (2,2,1) would represent a reduction of 2x in x and y\n\n    Optional:\n      encoding: force new layer to e.g. jpeg or compressed_segmentation\n      chunk_size: force new layer to new chunk size\n\n    Returns: info dict\n    """"""\n    return self.meta.add_scale(factor, encoding, chunk_size, info)\n\n  def exists(self, bbox_or_slices):\n    """"""\n    Produce a summary of whether all the requested chunks exist.\n\n    bbox_or_slices: accepts either a Bbox or a tuple of slices representing\n      the requested volume.\n    Returns: { chunk_file_name: boolean, ... }\n    """"""\n    return self.image.exists(bbox_or_slices)\n\n  def delete(self, bbox_or_slices):\n    """"""\n    Delete the files within the bounding box.\n\n    bbox_or_slices: accepts either a Bbox or a tuple of slices representing\n      the requested volume. \n    """"""\n    return self.image.delete(bbox_or_slices)\n\n  def transfer_to(self, cloudpath, bbox, block_size=None, compress=True):\n    """"""\n    Transfer files from one storage location to another, bypassing\n    volume painting. This enables using a single CloudVolume instance\n    to transfer big volumes. In some cases, gsutil or aws s3 cli tools\n    may be more appropriate. This method is provided for convenience. It\n    may be optimized for better performance over time as demand requires.\n\n    cloudpath (str): path to storage layer\n    bbox (Bbox object): ROI to transfer\n    block_size (int): number of file chunks to transfer per I/O batch.\n    compress (bool): Set to False to upload as uncompressed\n    """"""\n    return self.image.transfer_to(cloudpath, bbox, self.mip, block_size, compress)\n\n  def __getitem__(self, slices):\n    if type(slices) == Bbox:\n      slices = slices.to_slices()\n\n    slices = self.meta.bbox(self.mip).reify_slices(slices, bounded=self.bounded)\n    steps = Vec(*[ slc.step for slc in slices ])\n    channel_slice = slices.pop()\n    requested_bbox = Bbox.from_slices(slices)\n\n    img = self.download(requested_bbox, self.mip)\n    return img[::steps.x, ::steps.y, ::steps.z, channel_slice]\n\n  def download(\n      self, bbox, mip=None, parallel=None,\n      segids=None, preserve_zeros=False,\n      \n      # Absorbing polymorphic Graphene calls\n      agglomerate=None, timestamp=None, stop_layer=None\n    ):\n    """"""\n    Downloads segmentation from the indicated cutout\n    region.\n\n    bbox: specifies cutout to fetch\n    mip: which resolution level to get (default self.mip)\n    parallel: what parallel level to use (default self.parallel)\n\n    segids: agglomerate the leaves of these segids from the graph \n      server and label them with the given segid.\n    preserve_zeros: If segids is not None:\n      False: mask other segids with zero\n      True: mask other segids with the largest integer value\n        contained by the image data type and leave zero as is.\n\n    agglomerate, timestamp, and stop_layer are just there to \n    absorb arguments to what could be a graphene frontend.\n\n    Returns: img\n    """"""  \n    bbox = Bbox.create(\n      bbox, context=self.bounds, \n      bounded=self.bounded, \n      autocrop=self.autocrop\n    )\n\n    if mip is None:\n      mip = self.mip\n\n    if parallel is None:\n      parallel = self.parallel\n\n    img = self.image.download(bbox, mip, parallel=parallel)\n\n    if segids is None:\n      return img\n\n    mask_value = 0\n    if preserve_zeros:\n      mask_value = np.inf\n      if np.issubdtype(self.dtype, np.integer):\n        mask_value = np.iinfo(self.dtype).max\n\n      segids.append(0)\n\n    img = fastremap.mask_except(img, segids, in_place=True, value=mask_value)\n\n    return VolumeCutout.from_volume(\n      self.meta, mip, img, bbox\n    )\n\n  def download_point(\n    self, pt, size=256, \n    mip=None, parallel=None, \n    coord_resolution=None,\n    **kwargs\n  ):\n    """"""\n    Download to the right of point given in mip 0 coords.\n    Useful for quickly visualizing a neuroglancer coordinate\n    at an arbitary mip level.\n\n    pt: (x,y,z)\n    size: int or (sx,sy,sz)\n    mip: int representing resolution level\n    parallel: number of processes to launch (0 means all cores)\n    coord_resolution: (rx,ry,rz) the coordinate resolution of the input point.\n      Sometimes Neuroglancer is working in the resolution of another\n      higher res layer and this can help correct that.\n\n    Return: image\n    """"""\n    if isinstance(size, int):\n      size = Vec(size, size, size)\n    else:\n      size = Vec(*size)\n\n    if mip is None:\n      mip = self.mip\n\n    mip = self.meta.to_mip(mip)\n    size2 = size // 2\n\n    if coord_resolution is not None:\n      factor = self.meta.resolution(0) / Vec(*coord_resolution)\n      pt = Vec(*pt) / factor\n\n    pt = self.point_to_mip(pt, mip=0, to_mip=mip)\n\n    if all(size == 1):\n      bbox = Bbox(pt, pt + 1).astype(np.int64)\n    else:\n      bbox = Bbox(pt - size2, pt + size2).astype(np.int64)\n\n    if self.autocrop:\n      bbox = Bbox.intersection(bbox, self.meta.bounds(mip))\n\n    bbox = bbox.astype(np.int32)\n    if parallel is None:\n      parallel = self.parallel\n\n    return self.image.download(bbox, mip, parallel=parallel, **kwargs)\n\n  def unlink_shared_memory(self):\n    """"""Unlink the current shared memory location from the filesystem.""""""\n    return self.image.unlink_shared_memory()\n\n  def download_to_shared_memory(self, slices, location=None, mip=None):\n    """"""\n    Download images to a shared memory array. \n\n    https://github.com/seung-lab/cloud-volume/wiki/Advanced-Topic:-Shared-Memory\n\n    tip: If you want to use slice notation, np.s_[...] will help in a pinch.\n\n    MEMORY LIFECYCLE WARNING: You are responsible for managing the lifecycle of the \n      shared memory. CloudVolume will merely write to it, it will not unlink the \n      memory automatically. To fully clear the shared memory you must unlink the \n      location and close any mmap file handles. You can use `cloudvolume.sharedmemory.unlink(...)`\n      to help you unlink the shared memory file or `vol.unlink_shared_memory()` if you do \n      not specify location (meaning the default instance location is used).\n\n    EXPERT MODE WARNING: If you aren\'t sure you need this function (e.g. to relieve \n      memory pressure or improve performance in some way) you should use the ordinary \n      download method of img = vol[:]. A typical use case is transferring arrays between \n      different processes without making copies. For reference, this  feature was created \n      for downloading a 62 GB array and working with it in Julia.\n\n    Required:\n      slices: (Bbox or list of slices) the bounding box the shared array represents. For instance\n        if you have a 1024x1024x128 volume and you\'re uploading only a 512x512x64 corner\n        touching the origin, your Bbox would be `Bbox( (0,0,0), (512,512,64) )`.\n    Optional:\n      location: (str) Defaults to self.shared_memory_id. Shared memory location \n        e.g. \'cloudvolume-shm-RANDOM-STRING\' This typically corresponds to a file \n        in `/dev/shm` or `/run/shm/`. It can also be a file if you\'re using that for mmap. \n    \n    Returns: ndarray backed by shared memory\n    """"""\n    if mip is None:\n      mip = self.mip\n\n    slices = self.meta.bbox(mip).reify_slices(slices, bounded=self.bounded)\n    steps = Vec(*[ slc.step for slc in slices ])\n    channel_slice = slices.pop()\n    requested_bbox = Bbox.from_slices(slices)\n\n    if self.autocrop:\n      requested_bbox = Bbox.intersection(requested_bbox, self.bounds)\n\n    img = self.image.download(\n      requested_bbox, mip, parallel=self.parallel,\n      location=location, retain=True, use_shared_memory=True\n    )\n    return img[::steps.x, ::steps.y, ::steps.z, channel_slice]\n\n  def download_to_file(self, path, bbox, mip=None):\n    """"""\n    Download images directly to a file.\n\n    Required:\n      slices: (Bbox) the bounding box the shared array represents. For instance\n        if you have a 1024x1024x128 volume and you\'re uploading only a 512x512x64 corner\n        touching the origin, your Bbox would be `Bbox( (0,0,0), (512,512,64) )`.\n      path: (str)\n    Optional:\n      mip: (int; default: self.mip) The current resolution level.\n\n    Returns: ndarray backed by an mmapped file\n    """"""\n    if mip is None:\n      mip = self.mip\n\n    slices = self.meta.bbox(mip).reify_slices(bbox, bounded=self.bounded)\n    steps = Vec(*[ slc.step for slc in slices ])\n    channel_slice = slices.pop()\n    requested_bbox = Bbox.from_slices(slices)\n\n    if self.autocrop:\n      requested_bbox = Bbox.intersection(requested_bbox, self.bounds)\n\n    img = self.image.download(\n      requested_bbox, mip, parallel=self.parallel,\n      location=lib.toabs(path), retain=True, use_file=True\n    )\n    return img[::steps.x, ::steps.y, ::steps.z, channel_slice]\n\n  def __setitem__(self, slices, img):\n    if type(slices) == Bbox:\n      slices = slices.to_slices()\n\n    slices = self.meta.bbox(self.mip).reify_slices(slices, bounded=self.bounded)\n    bbox = Bbox.from_slices(slices)\n    slice_shape = list(bbox.size())\n    bbox = Bbox.from_slices(slices[:3])\n\n    if np.isscalar(img):\n      img = np.zeros(slice_shape, dtype=self.dtype) + img\n\n    imgshape = list(img.shape)\n    if len(imgshape) == 3:\n      imgshape = imgshape + [ self.num_channels ]\n\n    if not np.array_equal(imgshape, slice_shape):\n      raise exceptions.AlignmentError(""""""\n        Input image shape does not match slice shape.\n\n        Image Shape: {}  \n        Slice Shape: {}\n      """""".format(imgshape, slice_shape))\n\n    if self.autocrop:\n      if not self.bounds.contains_bbox(bbox):\n        img, bbox = autocropfn(self.meta, img, bbox, self.mip)\n\n    if bbox.subvoxel():\n      return\n\n    self.image.upload(img, bbox.minpt, self.mip, parallel=self.parallel)\n\n  def upload_from_shared_memory(self, location, bbox, order=\'F\', cutout_bbox=None):\n    """"""\n    Upload from a shared memory array. \n\n    https://github.com/seung-lab/cloud-volume/wiki/Advanced-Topic:-Shared-Memory\n\n    tip: If you want to use slice notation, np.s_[...] will help in a pinch.\n\n    MEMORY LIFECYCLE WARNING: You are responsible for managing the lifecycle of the \n      shared memory. CloudVolume will merely read from it, it will not unlink the \n      memory automatically. To fully clear the shared memory you must unlink the \n      location and close any mmap file handles. You can use `cloudvolume.sharedmemory.unlink(...)`\n      to help you unlink the shared memory file.\n\n    EXPERT MODE WARNING: If you aren\'t sure you need this function (e.g. to relieve \n      memory pressure or improve performance in some way) you should use the ordinary \n      upload method of vol[:] = img. A typical use case is transferring arrays between \n      different processes without making copies. For reference, this feature was created\n      for uploading a 62 GB array that originated in Julia.\n\n    Required:\n      location: (str) Shared memory location e.g. \'cloudvolume-shm-RANDOM-STRING\'\n        This typically corresponds to a file in `/dev/shm` or `/run/shm/`. It can \n        also be a file if you\'re using that for mmap.\n      bbox: (Bbox or list of slices) the bounding box the shared array represents. For instance\n        if you have a 1024x1024x128 volume and you\'re uploading only a 512x512x64 corner\n        touching the origin, your Bbox would be `Bbox( (0,0,0), (512,512,64) )`.\n    Optional:\n      cutout_bbox: (bbox or list of slices) If you only want to upload a section of the\n        array, give the bbox in volume coordinates (not image coordinates) that should \n        be cut out. For example, if you only want to upload 256x256x32 of the upper \n        rightmost corner of the above example but the entire 512x512x64 array is stored \n        in memory, you would provide: `Bbox( (256, 256, 32), (512, 512, 64) )`\n\n        By default, just upload the entire image.\n\n    Returns: void\n    """"""\n    bbox = Bbox.create(bbox)\n    cutout_bbox = Bbox.create(cutout_bbox) if cutout_bbox else bbox.clone()\n\n    if not bbox.contains_bbox(cutout_bbox):\n      raise exceptions.AlignmentError(""""""\n        The provided cutout is not wholly contained in the given array. \n        Bbox:        {}\n        Cutout:      {}\n      """""".format(bbox, cutout_bbox))\n\n    if self.autocrop:\n      cutout_bbox = Bbox.intersection(cutout_bbox, self.bounds)\n\n    if cutout_bbox.subvoxel():\n      return\n\n    shape = list(bbox.size3()) + [ self.num_channels ]\n    mmap_handle, shared_image = sharedmemory.ndarray(\n      location=location, shape=shape, \n      dtype=self.dtype, order=order, \n      readonly=True\n    )\n\n    delta_box = cutout_bbox.clone() - bbox.minpt\n    cutout_image = shared_image[ delta_box.to_slices() ]\n    \n    self.image.upload(\n      cutout_image, cutout_bbox.minpt, self.mip,\n      parallel=self.parallel, \n      location=location, \n      location_bbox=bbox,\n      order=order,\n      use_shared_memory=True,\n    )\n    mmap_handle.close()\n\n  def upload_from_file(self, location, bbox, order=\'F\', cutout_bbox=None):\n    """"""\n    Upload from an mmapped file.\n\n    tip: If you want to use slice notation, np.s_[...] will help in a pinch.\n\n    Required:\n      location: (str) Shared memory location e.g. \'cloudvolume-shm-RANDOM-STRING\'\n        This typically corresponds to a file in `/dev/shm` or `/run/shm/`. It can \n        also be a file if you\'re using that for mmap.\n      bbox: (Bbox or list of slices) the bounding box the shared array represents. For instance\n        if you have a 1024x1024x128 volume and you\'re uploading only a 512x512x64 corner\n        touching the origin, your Bbox would be `Bbox( (0,0,0), (512,512,64) )`.\n    Optional:\n      cutout_bbox: (bbox or list of slices) If you only want to upload a section of the\n        array, give the bbox in volume coordinates (not image coordinates) that should \n        be cut out. For example, if you only want to upload 256x256x32 of the upper \n        rightmost corner of the above example but the entire 512x512x64 array is stored \n        in memory, you would provide: `Bbox( (256, 256, 32), (512, 512, 64) )`\n\n        By default, just upload the entire image.\n\n    Returns: void\n    """"""        \n    bbox = Bbox.create(bbox)\n    cutout_bbox = Bbox.create(cutout_bbox) if cutout_bbox else bbox.clone()\n\n    if not bbox.contains_bbox(cutout_bbox):\n      raise exceptions.AlignmentError(""""""\n        The provided cutout is not wholly contained in the given array. \n        Bbox:        {}\n        Cutout:      {}\n      """""".format(bbox, cutout_bbox))\n\n    if self.autocrop:\n      cutout_bbox = Bbox.intersection(cutout_bbox, self.bounds)\n\n    if cutout_bbox.subvoxel():\n      return\n\n    shape = list(bbox.size3()) + [ self.num_channels ]\n    mmap_handle, shared_image = sharedmemory.ndarray_fs(\n      location=lib.toabs(location), shape=shape, \n      dtype=self.dtype, order=order, \n      readonly=True, lock=None\n    )\n\n    delta_box = cutout_bbox.clone() - bbox.minpt\n    cutout_image = shared_image[ delta_box.to_slices() ]\n    \n    self.image.upload(\n      cutout_image, cutout_bbox.minpt, self.mip,\n      parallel=self.parallel, \n      location=lib.toabs(location), \n      location_bbox=bbox,\n      order=order,\n      use_file=True,\n    )\n    mmap_handle.close()\n\n  def viewer(self, port=1337):\n    import cloudvolume.server\n\n    cloudvolume.server.view(self.cloudpath, port=port)\n\n  def to_dask(self, chunks=None, name=None):\n    """"""Return a dask array for this volume.\n\n    Parameters\n    ----------\n    chunks: tuple of ints or tuples of ints\n      Passed to ``da.from_array``, allows setting the chunks on\n      initialisation, if the chunking scheme in the stored dataset is not\n      optimal for the calculations to follow. Note that the chunking should\n      be compatible with an underlying 4d array.\n    name: str, optional\n      An optional keyname for the array. Defaults to hashing the input\n\n    Returns\n    -------\n    Dask array\n    """"""\n    import dask.array as da\n    from dask.base import tokenize\n\n    if chunks is None:\n      chunks = tuple(self.chunk_size) + (self.num_channels, )\n    if name is None:\n      name = \'to-dask-\' + tokenize(self, chunks)\n    return da.from_array(self, chunks, name=name)\n'"
cloudvolume/microviewer/__init__.py,7,"b'import os\n\ntry: \n  from http.server import BaseHTTPRequestHandler, HTTPServer\nexcept ImportError:\n  from SocketServer import TCPServer as HTTPServer\n  from BaseHTTPServer import BaseHTTPRequestHandler\n\nimport json\nfrom six.moves import range\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom cloudvolume.lib import Vec, Bbox, mkdir, save_images, yellow\nfrom cloudvolume.paths import ExtractedPath\n\nDEFAULT_PORT = 8080\n\ndef getresolution(img, resolution):\n  if resolution is not None:\n    return Vec(*resolution)\n  else:\n    try:\n      return img.resolution\n    except AttributeError:\n      return Vec(0,0,0) \n\ndef getoffset(img, offset):\n  if offset is not None:\n    return Vec(*offset)\n  else:\n    try:\n      return img.bounds.minpt\n    except AttributeError:\n      return Vec(0,0,0)\n\ndef to_volumecutout(img, image_type, resolution=None, offset=None, hostname=\'localhost\'):\n  from cloudvolume.volumecutout import VolumeCutout\n  if type(img) == VolumeCutout:\n    try:\n      img.dataset_name # check if it\'s an intact VolumeCutout\n      return img\n    except AttributeError:\n      pass\n  \n  resolution = getresolution(img, resolution)\n  offset = getoffset(img, offset)\n\n  return VolumeCutout(\n    buf=img,\n    path=ExtractedPath(\'mem\', hostname, \'/\', \'\', \'\', \'\', \'\'),\n    cloudpath=\'IN MEMORY\',\n    resolution=resolution,\n    mip=-1,\n    layer_type=image_type,\n    bounds=Bbox( offset, offset + Vec(*(img.shape[:3])) ),\n    handle=None,\n  )\n\ndef to3d(img):\n  while len(img.shape) > 3:\n    img = img[..., 0]\n  while len(img.shape) < 3:\n    img = img[..., np.newaxis]\n  return img  \n\ndef hyperview(\n    img, segmentation, resolution=None, offset=None,\n    hostname=\'localhost\', port=DEFAULT_PORT\n  ):\n\n  img = to3d(img)\n  segmentation = emulate_eight_uint64(to3d(segmentation))\n\n  assert np.all(img.shape[:3] == segmentation.shape[:3])\n\n  img = to_volumecutout(img, \'image\', resolution, offset, hostname)\n  segmentation = to_volumecutout(\n    segmentation, \'segmentation\', resolution, offset, hostname\n  )\n\n  return run([ img, segmentation ], hostname=hostname, port=port)\n\ndef emulate_eight_uint64(img):\n  # Makes sense for viewing not segmentation \n  # which requires uints currently. (Jan. 2019)\n  if np.dtype(img.dtype).itemsize == 8 and not np.issubdtype(img.dtype, np.float64):\n    print(yellow(\n      """"""\nConverting {} to float64 for display. \nJavascript does not support native 64-bit integer arrays.\n      """""".format(img.dtype)\n    ))\n    return img.astype(np.float64)\n\n  return img\n\ndef view(\n    img, segmentation=False, resolution=None, offset=None,\n    hostname=""localhost"", port=DEFAULT_PORT\n  ):\n  from cloudvolume.volumecutout import VolumeCutout\n\n  img = to3d(img)\n  resolution = getresolution(img, resolution)\n  offset = getoffset(img, offset)\n  img = emulate_eight_uint64(img)\n\n  cutout = VolumeCutout(\n    buf=img,\n    path=ExtractedPath(\'mem\', hostname, \'/\', \'\', \'\', \'\', \'\'),\n    cloudpath=\'IN MEMORY\',\n    resolution=resolution,\n    mip=-1,\n    layer_type=(\'segmentation\' if segmentation else \'image\'),\n    bounds=Bbox( offset, offset + Vec(*(img.shape[:3])) ),\n    handle=None,\n  )\n  return run([ cutout ], hostname=hostname, port=port)\n\ndef run(cutouts, hostname=""localhost"", port=DEFAULT_PORT):\n  """"""Start a local web app on the given port that lets you explore this cutout.""""""\n  def handler(*args):\n    return ViewerServerHandler(cutouts, *args)\n\n  myServer = HTTPServer((hostname, port), handler)\n  print(""Viewer server listening to http://{}:{}"".format(hostname, port))\n  try:\n    myServer.serve_forever()\n  except KeyboardInterrupt:\n    # extra \\n to prevent display of ""^CContinuing""\n    print(""\\nContinuing program execution..."")\n  finally:\n    myServer.server_close()\n\nclass ViewerServerHandler(BaseHTTPRequestHandler):\n  def __init__(self, cutouts, *args):\n    self.cutouts = cutouts\n    BaseHTTPRequestHandler.__init__(self, *args)\n\n  def do_GET(self):\n    self.send_response(200)\n  \n    allowed_files = (\'/\', \'/datacube.js\', \'/jquery-3.3.1.js\', \'/favicon.ico\')\n\n    if self.path in allowed_files:\n      self.serve_file()\n    elif self.path == \'/parameters\':\n      self.serve_parameters()\n    elif self.path == \'/channel\':\n      self.serve_data(self.cutouts[0])\n    elif self.path == \'/segmentation\':\n      self.serve_data(self.cutouts[1])\n\n  def serve_data(self, data):\n    self.send_header(\'Content-type\', \'application/octet-stream\')\n    self.send_header(\'Content-length\', str(data.nbytes))\n    self.end_headers()\n    self.wfile.write(data.tobytes(\'F\'))\n\n  def serve_parameters(self):\n    self.send_header(\'Content-type\', \'application/json\')\n    self.end_headers()\n\n    if len(self.cutouts) == 1:\n      cutout = self.cutouts[0]\n      msg = json.dumps({\n        \'viewtype\': \'single\',\n        \'dataset\': cutout.dataset_name,\n        \'layer\': cutout.layer,\n        \'layer_type\': cutout.layer_type,\n        \'protocol\': cutout.path.protocol,\n        \'cloudpath\': [ cutout.cloudpath ],\n        \'mip\': cutout.mip,\n        \'bounds\': [ int(_) for _ in cutout.bounds.to_list() ],\n        \'resolution\': cutout.resolution.tolist(),\n        \'data_types\': [ str(cutout.dtype) ],\n        \'data_bytes\': np.dtype(cutout.dtype).itemsize,\n      })\n    else:\n      img, seg = self.cutouts\n      msg = json.dumps({\n        \'viewtype\': \'hyper\',\n        \'dataset\': img.dataset_name,\n        \'layers\': [ img.layer, seg.layer ],\n        \'protocol\': img.path.protocol,\n        \'cloudpath\': [ img.cloudpath, seg.cloudpath ],\n        \'mip\': img.mip,\n        \'bounds\': [ int(_) for _ in img.bounds.to_list() ],\n        \'resolution\': img.resolution.tolist(),\n        \'data_types\': [ str(img.dtype), str(seg.dtype) ],\n        \'data_bytes\': [ \n          np.dtype(img.dtype).itemsize,\n          np.dtype(seg.dtype).itemsize\n        ],\n      })\n    self.wfile.write(msg.encode(\'utf-8\'))\n\n  def serve_file(self):\n    self.send_header(\'Content-type\', \'text/html\')\n    self.end_headers()\n\n    path = self.path.replace(\'/\', \'\')\n\n    if path == \'\':\n      path = \'index.html\'\n\n    dirname = os.path.dirname(__file__)\n    filepath = os.path.join(dirname, \'./\' + path)\n    with open(filepath, \'rb\') as f:\n      self.wfile.write(f.read())  \n'"
cloudvolume/storage/__init__.py,0,"b'""""""\nStorage is a multithreaded key-value object\nmanagement client that supports GET, PUT, DELETE,\nand LIST operations.\n\nIt can support any key-value storage system and \ncurrently supports local filesystem, Google Cloud Storage,\nand Amazon S3 interfaces.\n\nSingle threaded, Python (preemptive) threads and \ngreen (cooperative) threads are available as\n\nSimpleStorage, ThreadedStorage, and GreenStorage respectively.\n\nStorage is an alias for ThreadedStorage\n""""""\n\nfrom .storage import (\n  SimpleStorage, ThreadedStorage, GreenStorage,  \n  DEFAULT_THREADS\n)\nfrom .storage_interfaces import reset_connection_pools\n\n# For backwards compatibility\n\nclass Storage(object):\n  def __new__(kls, *args, **kwargs):\n    green = False\n    if \'green\' in kwargs:\n      green = bool(kwargs[\'green\'])\n      del kwargs[\'green\']\n\n    if green:\n      return GreenStorage(*args, **kwargs)\n    return ThreadedStorage(*args, **kwargs)\n'"
cloudvolume/storage/storage.py,0,"b'import six\nfrom six.moves import queue as Queue\nfrom collections import defaultdict\nimport itertools\nimport json\nimport os.path\nimport posixpath\nimport re\nfrom functools import partial\nimport types\n\nfrom tqdm import tqdm\n\nfrom cloudvolume import compression\nfrom cloudvolume.exceptions import UnsupportedProtocolError\nfrom cloudvolume.lib import mkdir, scatter, jsonify, duplicates\nfrom cloudvolume.threaded_queue import ThreadedQueue, DEFAULT_THREADS\nfrom cloudvolume.scheduler import schedule_green_jobs\n\nimport cloudvolume.paths\n\nfrom .storage_interfaces import (\n  FileInterface, HttpInterface, \n  S3Interface, GoogleCloudStorageInterface\n)\n\ndef get_interface_class(protocol):\n  if protocol == \'file\':\n    return FileInterface\n  elif protocol == \'gs\':\n    return GoogleCloudStorageInterface\n  elif protocol in (\'s3\', \'matrix\'):\n    return S3Interface\n  elif protocol in (\'http\', \'https\'):\n    return HttpInterface\n  else:\n    raise UnsupportedProtocolError(str(self._path))\n\ndef default_byte_iterator(starts, ends):\n  if starts is None:\n    starts = itertools.repeat(None)\n  if ends is None:\n    ends = itertools.repeat(None)\n  return iter(starts), iter(ends)\n\nclass StorageBase(object):\n  """"""Abastract base class of Storage with some implementation details.""""""\n  def __init__(self, layer_path, progress=False):\n    self.progress = progress\n\n    self._layer_path = layer_path\n    self._path = cloudvolume.paths.extract(layer_path)\n    self._interface_cls = get_interface_class(self._path.protocol)\n  \n  @property\n  def layer_path(self):\n    return self._layer_path\n\n  def progress_description(self, prefix):\n    if isinstance(self.progress, str):\n      return prefix + \' \' + self.progress\n    else:\n      return prefix if self.progress else None\n\n  def get_connection(self):\n    return self._interface_cls(self._path)\n\n  def get_path_to_file(self, file_path):\n    return posixpath.join(self._layer_path, file_path)\n\n  def put_json(self, file_path, content, content_type=\'application/json\', *args, **kwargs):\n    if type(content) != str:\n      content = jsonify(content)\n    return self.put_file(file_path, content, content_type=content_type, *args, **kwargs)\n    \n  def get_json(self, file_path):\n    content = self.get_file(file_path)\n    if content is None:\n      return None\n    return json.loads(content.decode(\'utf8\'))\n\n  def put_file(self, file_path, content, content_type=None, compress=None, compress_level=None, cache_control=None):\n    """""" \n    Args:\n      filename (string): it can contains folders\n      content (string): binary data to save\n    """"""\n    return self.put_files([ (file_path, content) ], \n      content_type=content_type, \n      compress=compress, \n      compress_level=compress_level,\n      cache_control=cache_control, \n      block=False\n    )\n\n  def exists(self, file_path):\n    raise NotImplementedError()\n\n  def files_exist(self, file_paths):\n    raise NotImplementedError()\n\n  def get_file(self, file_path):\n    raise NotImplementedError()\n\n  def get_files(self, file_paths, starts=None, ends=None):\n    raise NotImplementedError()\n\n  def delete_file(self, file_path):\n    raise NotImplementedError()\n\n  def delete_files(self, file_paths):\n    raise NotImplementedError()\n\n  def list_files(self, prefix="""", flat=False):\n    raise NotImplementedError()\n\n  def __del__(self):\n    pass\n\n  def __enter__(self):\n    return self\n\n  def __exit__(self, exception_type, exception_value, traceback):\n    pass\n\nclass SimpleStorage(StorageBase):\n  """"""\n  Access files stored in Google Storage (gs), Amazon S3 (s3), \n  or the local Filesystem (file).\n\n  e.g. with Storage(\'gs://bucket/dataset/layer\') as stor:\n      files = stor.get_file(\'filename\')\n\n  Required:\n    layer_path (str): A protocol prefixed path of the above format.\n      Accepts s3:// gs:// and file://. File paths are absolute.\n\n  Optional:\n    progress (bool:false): Show a tqdm progress bar for multiple \n      uploads and downloads.\n  """"""\n  def __init__(self, layer_path, progress=False):\n    super(SimpleStorage, self).__init__(layer_path, progress)\n    self._interface = self.get_connection()\n\n  def put_files(self, files, content_type=None, compress=None, compress_level=None, cache_control=None, block=True):\n    """"""\n    Put lots of files at once and get a nice progress bar. It\'ll also wait\n    for the upload to complete, just like get_files.\n\n    Required:\n      files: [ (filepath, content), .... ]\n    """"""\n    desc = self.progress_description(\'Uploading\')\n    for path, content in tqdm(files, disable=(not self.progress), desc=desc):\n      content = compression.compress(content, method=compress, compress_level=compress_level)\n      self._interface.put_file(path, content, content_type, compress, cache_control=cache_control)\n    return self\n\n  def exists(self, file_path):\n    """"""Test if a single file exists. Returns boolean.""""""\n    return self._interface.exists(file_path)\n\n  def files_exist(self, file_paths):\n    """"""\n    Threaded exists for all file paths. \n\n    file_paths: (list) file paths to test for existence\n\n    Returns: { filepath: bool }\n    """"""\n    return self._interface.files_exist(file_paths)\n\n  def get_file(self, file_path, start=None, end=None):\n    """"""\n    Get the binary contents of a file. Optionally, specify\n    the inclusive byte range to request.\n    """"""\n    content, encoding = self._interface.get_file(file_path, start=start, end=end)\n    content = compression.decompress(content, encoding, filename=file_path)\n    return content\n\n  def get_files(self, file_paths, starts=None, ends=None):\n    starts, ends = default_byte_iterator(starts, ends)\n\n    iterator = tqdm(\n      zip(file_paths, starts, ends),\n      disable=(not self.progress), \n      desc=self.progress_description(""Downloading"")\n    )\n\n    results = []\n    for path, start, end in iterator:\n      error = None \n\n      try:\n        content = self.get_file(path, start, end)\n      except Exception as err:\n        error = err \n        content = None \n\n      results.append({\n        \'filename\': path,\n        \'byte_range\': (start, end),\n        \'content\': content,\n        \'error\': error,\n      })\n\n    return results \n\n  def delete_file(self, file_path):\n    self._interface.delete_file(file_path)\n\n  def delete_files(self, file_paths):\n    for path in file_paths:\n      self._interface.delete_file(path)\n    return self\n\n  def list_files(self, prefix="""", flat=False):\n    """"""\n    List the files in the layer with the given prefix. \n\n    flat means only generate one level of a directory,\n    while non-flat means generate all file paths with that \n    prefix.\n\n    Here\'s how flat=True handles different senarios:\n      1. partial directory name prefix = \'bigarr\'\n        - lists the \'\' directory and filters on key \'bigarr\'\n      2. full directory name prefix = \'bigarray\'\n        - Same as (1), but using key \'bigarray\'\n      3. full directory name + ""/"" prefix = \'bigarray/\'\n        - Lists the \'bigarray\' directory\n      4. partial file name prefix = \'bigarray/chunk_\'\n        - Lists the \'bigarray/\' directory and filters on \'chunk_\'\n    \n    Return: generated sequence of file paths relative to layer_path\n    """"""\n\n    for f in self._interface.list_files(prefix, flat):\n      yield f\n\n  def __del__(self):\n    self._interface.release_connection()\n\n  def __enter__(self):\n    return self\n\n  def __exit__(self, exception_type, exception_value, traceback):\n    self._interface.release_connection()\n\n  def __getitem__(self, key):\n    return self.get_file(key)\n  \n  def __setitem__(self, key, value):\n    if type(key) == tuple:\n      key, kwargs = key\n    else:\n      kwargs = {}\n\n    self.put_item(key, value, **kwargs)\n\nclass GreenStorage(StorageBase):\n  def __init__(\n    self, layer_path, progress=False, \n    n_threads=DEFAULT_THREADS, \n  ):\n    import gevent.pool\n    super(GreenStorage, self).__init__(layer_path, progress)\n    self.concurrency = n_threads if n_threads > 0 else 1\n    self.pool = gevent.pool.Pool(self.concurrency)\n\n  def exists(self, file_path):\n    """"""Test if a single file exists. Returns boolean.""""""\n    with self.get_connection() as conn:\n      return conn.exists(file_path)\n\n  def files_exist(self, file_paths):\n    """"""\n    Threaded exists for all file paths.\n\n    file_paths: (list) file paths to test for existence\n\n    Returns: { filepath: bool }\n    """"""\n    results = {}\n\n    def exist_thunk(paths):\n      with self.get_connection() as conn:\n        results.update(conn.files_exist(paths))\n    \n    desc = self.progress_description(\'Existence Testing\')\n    schedule_green_jobs(  \n      fns=( partial(exist_thunk, paths) for paths in scatter(file_paths, self.concurrency) ),\n      progress=(desc if self.progress else None),\n      concurrency=self.concurrency,\n      total=len(file_paths),\n    )\n\n    return results\n\n  def get_file(self, file_path, start=None, end=None):\n    """"""\n    Get the binary contents of a file. Optionally, specify\n    the inclusive byte range to request.\n    """"""\n    with self.get_connection() as conn:\n      content, encoding = conn.get_file(file_path, start=start, end=end)\n    return compression.decompress(content, encoding, filename=file_path)\n\n  def get_files(self, file_paths, starts=None, ends=None):\n    """"""\n    Returns: [ \n      { ""filename"": ..., ""content"": bytes, ""error"": exception or None }, \n      ... \n    ]\n    """"""\n    starts, ends = default_byte_iterator(starts, ends)\n\n    def getfn(path, start, end):\n      result = error = None \n\n      conn = self.get_connection()\n      try:\n        result = conn.get_file(path, start=start, end=end)\n      except Exception as err:\n        error = err\n        # important to print immediately because \n        # errors are collected at the end\n        print(err)\n        del conn\n      else:\n        conn.release_connection()\n      \n      content, encoding = result\n      content = compression.decompress(content, encoding)\n\n      return {\n        ""filename"": path,\n        ""byte_range"": (start, end),\n        ""content"": content,\n        ""error"": error,\n      }\n\n    desc = self.progress_description(\'Downloading\')\n\n    return schedule_green_jobs(  \n      fns=( \n        partial(getfn, path, start, end) \n        for path, start, end in zip(file_paths, starts, ends) \n      ),\n      progress=(desc if self.progress else None),\n      concurrency=self.concurrency,\n      total=len(file_paths),\n    )\n\n  def put_files(\n    self, files, \n    content_type=None, compress=None, compress_level=None,\n    cache_control=None, block=True\n  ):\n    """"""\n    Put lots of files at once and get a nice progress bar. It\'ll also wait\n    for the upload to complete, just like get_files.\n\n    Required:\n      files: [ (filepath, content), .... ]\n    """""" \n    if compress not in compression.COMPRESSION_TYPES:\n      raise NotImplementedError()\n\n    def uploadfn(path, content):\n      with self.get_connection() as conn:\n        content = compression.compress(content, method=compress, compress_level=compress_level)\n        conn.put_file(\n          file_path=path, \n          content=content, \n          content_type=content_type, \n          compress=compress, \n          cache_control=cache_control,\n        )\n\n    if not isinstance(gen, types.GeneratorType):\n      dupes = duplicates([ path for path, content in files ])\n      if dupes:\n        raise ValueError(""Cannot write the same file multiple times in one pass. This causes a race condition. Files: "" + "", "".join(dupes))\n\n    fns = ( partial(uploadfn, path, content) for path, content in files )\n\n    if block:\n      desc = desc = self.progress_description(\'Uploading\')\n      schedule_green_jobs(\n        fns=fns,\n        progress=(desc if self.progress else None),\n        concurrency=self.concurrency,\n        total=len(files),\n      )\n    else:\n      for fn in fns:\n        self.pool.spawn(fn)\n\n    return self\n\n  def wait(self, desc=None):\n    self.pool.join()\n\n  def start_threads(self):\n    import gevent.pool\n    self.pool.kill()\n    self.pool = gevent.pool.Pool(self.concurrency)\n\n  def kill_threads(self):\n    self.pool.kill()\n\n  def delete_file(self, file_path):\n    with self.get_connection() as conn:\n      conn.delete_file(file_path)\n    return self\n\n  def delete_files(self, file_paths):\n    def thunk_delete(path):\n      with self.get_connection() as conn:\n        conn.delete_file(path)\n\n    desc = self.progress_description(\'Deleting\')\n\n    schedule_green_jobs(\n      fns=( partial(thunk_delete, path) for path in file_paths ),\n      progress=(desc if self.progress else None),\n      concurrency=self.concurrency,\n      total=len(file_paths),\n    )\n    \n    return self\n\n  def list_files(self, prefix="""", flat=False):\n    """"""\n    List the files in the layer with the given prefix. \n\n    flat means only generate one level of a directory,\n    while non-flat means generate all file paths with that \n    prefix.\n\n    Here\'s how flat=True handles different senarios:\n      1. partial directory name prefix = \'bigarr\'\n        - lists the \'\' directory and filters on key \'bigarr\'\n      2. full directory name prefix = \'bigarray\'\n        - Same as (1), but using key \'bigarray\'\n      3. full directory name + ""/"" prefix = \'bigarray/\'\n        - Lists the \'bigarray\' directory\n      4. partial file name prefix = \'bigarray/chunk_\'\n        - Lists the \'bigarray/\' directory and filters on \'chunk_\'\n    \n    Return: generated sequence of file paths relative to layer_path\n    """"""\n    with self.get_connection() as conn:\n      for f in conn.list_files(prefix, flat):\n        yield f\n\n  def __enter__(self):\n    StorageBase.__enter__(self)\n    self.start_threads()\n    return self\n\n  def __exit__(self, exception_type, exception_value, traceback):\n    StorageBase.__exit__(self, exception_type, exception_value, traceback)\n    self.pool.join()\n    self.kill_threads()\n\nclass ThreadedStorage(StorageBase, ThreadedQueue):\n  """"""\n  Access files stored in Google Storage (gs), Amazon S3 (s3), \n  or the local Filesystem (file).\n\n  e.g. with Storage(\'gs://bucket/dataset/layer\') as stor:\n      files = stor.get_file(\'filename\')\n\n  Required:\n    layer_path (str): A protocol prefixed path of the above format.\n      Accepts s3:// gs:// and file://. File paths are absolute.\n\n  Optional:\n    n_threads (int:20): number of threads to use downloading and uplaoding.\n      If 0, execution will be on the main python thread.\n    progress (bool:false): Show a tqdm progress bar for multiple \n      uploads and downloads.\n  """"""\n  def __init__(self, layer_path, n_threads=20, progress=False):\n    StorageBase.__init__(self, layer_path, progress)\n    ThreadedQueue.__init__(self, n_threads)\n    self._interface = self.get_connection()\n\n  def _initialize_interface(self):\n    return self._interface_cls(self._path)\n\n  def _close_interface(self, interface):\n    interface.release_connection()\n\n  def _consume_queue(self, terminate_evt):\n    ThreadedQueue._consume_queue(self, terminate_evt)\n    self._interface.release_connection()\n\n  @property\n  def layer_path(self):\n    return self._layer_path\n\n  def get_path_to_file(self, file_path):\n    return posixpath.join(self._layer_path, file_path)\n\n  def put_json(self, file_path, content, content_type=\'application/json\', *args, **kwargs):\n    if type(content) != str:\n      content = jsonify(content)\n    return self.put_file(file_path, content, content_type=content_type, *args, **kwargs)\n  \n  def put_file(self, file_path, content, content_type=None, compress=None, compress_level=None, cache_control=None):\n    """""" \n    Args:\n      filename (string): it can contains folders\n      content (string): binary data to save\n    """"""\n    return self.put_files([ (file_path, content) ], \n      content_type=content_type, \n      compress=compress, \n      compress_level=compress_level,\n      cache_control=cache_control, \n      block=False\n    )\n\n  def put_files(self, files, content_type=None, compress=None, compress_level=None, cache_control=None, block=True):\n    """"""\n    Put lots of files at once and get a nice progress bar. It\'ll also wait\n    for the upload to complete, just like get_files.\n\n    Required:\n      files: [ (filepath, content), .... ]\n    """"""\n    def base_uploadfn(path, content, interface):\n      interface.put_file(path, content, content_type, compress, cache_control=cache_control)\n\n    seen = set() \n    for path, content in files:\n      if path in seen:\n        raise ValueError(""Cannot write the same file multiple times in one pass. This causes a race condition. File: "" + path)\n      seen.add(path)\n\n      content = compression.compress(content, method=compress, compress_level=compress_level)\n      uploadfn = partial(base_uploadfn, path, content)\n\n      if len(self._threads):\n        self.put(uploadfn)\n      else:\n        uploadfn(self._interface)\n\n    if block:\n      desc = self.progress_description(\'Uploading\')\n      self.wait(desc)\n\n    return self\n\n  def exists(self, file_path):\n    """"""Test if a single file exists. Returns boolean.""""""\n    return self._interface.exists(file_path)\n\n  def files_exist(self, file_paths):\n    """"""\n    Threaded exists for all file paths.\n\n    file_paths: (list) file paths to test for existence\n\n    Returns: { filepath: bool }\n    """"""\n    results = {}\n\n    def exist_thunk(paths, interface):\n      results.update(interface.files_exist(paths))\n\n    if len(self._threads):\n      for block in scatter(file_paths, len(self._threads)):\n        self.put(partial(exist_thunk, block))\n    else:\n      exist_thunk(file_paths, self._interface)\n\n    desc = self.progress_description(\'Existence Testing\')\n    self.wait(desc)\n\n    return results\n\n  def get_file(self, file_path, start=None, end=None):\n    """"""\n    Get the binary contents of a file. Optionally, specify\n    the inclusive byte range to request.\n    """"""\n    content, encoding = self._interface.get_file(file_path, start=start, end=end)\n    content = compression.decompress(content, encoding, filename=file_path)\n    return content\n\n  def get_files(self, file_paths, starts=None, ends=None):\n    """"""\n    returns a list of files faster by using threads\n    """"""\n    results = []\n    starts, ends = default_byte_iterator(starts, ends)\n\n    def get_file_thunk(path, start, end, interface):\n      result = error = None \n\n      try:\n        result = interface.get_file(path, start=start, end=end)\n      except Exception as err:\n        error = err\n        # important to print immediately because \n        # errors are collected at the end\n        print(err) \n      \n      content, encoding = result\n      content = compression.decompress(content, encoding)\n\n      results.append({\n        ""filename"": path,\n        ""byte_range"": (start, end),\n        ""content"": content,\n        ""error"": error,\n      })\n\n    for path, start, end in zip(file_paths, starts, ends):\n      if len(self._threads):\n        self.put(partial(get_file_thunk, path, start, end))\n      else:\n        get_file_thunk(path, start, end, self._interface)\n\n    desc = self.progress_description(\'Downloading\')\n    self.wait(desc)\n\n    return results\n\n  def delete_file(self, file_path):\n\n    def thunk_delete(interface):\n      interface.delete_file(file_path)\n\n    if len(self._threads):\n      self.put(thunk_delete)\n    else:\n      thunk_delete(self._interface)\n\n    return self\n\n  def delete_files(self, file_paths):\n\n    def thunk_delete(path, interface):\n      interface.delete_file(path)\n\n    for path in file_paths:\n      if len(self._threads):\n        self.put(partial(thunk_delete, path))\n      else:\n        thunk_delete(path, self._interface)\n\n    desc = self.progress_description(\'Deleting\')\n    self.wait(desc)\n\n    return self\n\n  def list_files(self, prefix="""", flat=False):\n    """"""\n    List the files in the layer with the given prefix. \n\n    flat means only generate one level of a directory,\n    while non-flat means generate all file paths with that \n    prefix.\n\n    Here\'s how flat=True handles different senarios:\n      1. partial directory name prefix = \'bigarr\'\n        - lists the \'\' directory and filters on key \'bigarr\'\n      2. full directory name prefix = \'bigarray\'\n        - Same as (1), but using key \'bigarray\'\n      3. full directory name + ""/"" prefix = \'bigarray/\'\n        - Lists the \'bigarray\' directory\n      4. partial file name prefix = \'bigarray/chunk_\'\n        - Lists the \'bigarray/\' directory and filters on \'chunk_\'\n    \n    Return: generated sequence of file paths relative to layer_path\n    """"""\n\n    for f in self._interface.list_files(prefix, flat):\n      yield f\n\n  def __del__(self):\n    ThreadedQueue.__del__(self)\n    self._interface.release_connection()\n\n  def __exit__(self, exception_type, exception_value, traceback):\n    ThreadedQueue.__exit__(self, exception_type, exception_value, traceback)\n    self._interface.release_connection()\n\n\n'"
cloudvolume/storage/storage_interfaces.py,0,"b'import six\nfrom collections import defaultdict\nimport json\nimport os.path\nimport posixpath\nimport re\n\nimport boto3\nimport botocore\nfrom glob import glob\nimport google.cloud.exceptions\nfrom google.cloud.storage import Batch, Client\nimport requests\nimport tenacity\n\nfrom cloudvolume.connectionpools import S3ConnectionPool, GCloudBucketPool\nfrom cloudvolume.lib import mkdir\nfrom cloudvolume.exceptions import UnsupportedCompressionType\n\nCOMPRESSION_EXTENSIONS = (\'.gz\', \'.br\')\n\n# This is just to support pooling by bucket\nclass keydefaultdict(defaultdict):\n  def __missing__(self, key):\n    if self.default_factory is None:\n      raise KeyError( key )\n    else:\n      ret = self[key] = self.default_factory(key)\n      return ret\n\nS3_POOL = None\nGC_POOL = None\ndef reset_connection_pools():\n  global S3_POOL\n  global GC_POOL\n  S3_POOL = keydefaultdict(lambda service: keydefaultdict(lambda bucket_name: S3ConnectionPool(service, bucket_name)))\n  GC_POOL = keydefaultdict(lambda bucket_name: GCloudBucketPool(bucket_name))\n\nreset_connection_pools()\n\nretry = tenacity.retry(\n  reraise=True, \n  stop=tenacity.stop_after_attempt(7), \n  wait=tenacity.wait_random_exponential(0.5, 60.0),\n)\n\nclass StorageInterface(object):\n  def release_connection(self):\n    pass\n  def __enter__(self):\n    return self\n  def __exit__(self, exception_type, exception_value, traceback):\n    self.release_connection()\n\nclass FileInterface(StorageInterface):\n  def __init__(self, path):\n    super(StorageInterface, self).__init__()\n    self._path = path\n\n  def get_path_to_file(self, file_path):\n    return os.path.join(\n      self._path.basepath, self._path.layer, file_path\n    )\n\n  def put_file(\n    self, file_path, content, \n    content_type, compress, \n    cache_control=None\n  ):\n    path = self.get_path_to_file(file_path)\n    mkdir(os.path.dirname(path))\n\n    # keep default as gzip\n    if compress == ""br"":\n      path += "".br""\n    elif compress:\n      path += \'.gz\'\n\n    if content \\\n      and content_type \\\n      and re.search(\'json|te?xt\', content_type) \\\n      and type(content) is str:\n\n      content = content.encode(\'utf-8\')\n\n    try:\n      with open(path, \'wb\') as f:\n        f.write(content)\n    except IOError as err:\n      with open(path, \'wb\') as f:\n        f.write(content)\n\n  def get_file(self, file_path, start=None, end=None):\n    path = self.get_path_to_file(file_path)\n\n    if os.path.exists(path + \'.gz\'):\n      encoding = ""gzip""\n      path += \'.gz\'\n    elif os.path.exists(path + \'.br\'):\n      encoding = ""br""\n      path += "".br""\n    else:\n      encoding = None\n\n    try:\n      with open(path, \'rb\') as f:\n        if start is not None:\n          f.seek(start)\n        if end is not None:\n          start = start if start is not None else 0\n          num_bytes = end - start\n          data = f.read(num_bytes)\n        else:\n          data = f.read()\n      return data, encoding\n    except IOError:\n      return None, encoding\n\n  def exists(self, file_path):\n    path = self.get_path_to_file(file_path)\n    return os.path.exists(path) or any(( os.path.exists(path + ext) for ext in COMPRESSION_EXTENSIONS ))\n\n  def files_exist(self, file_paths):\n    return { path: self.exists(path) for path in file_paths }\n\n  def delete_file(self, file_path):\n    path = self.get_path_to_file(file_path)\n    if os.path.exists(path):\n      os.remove(path)\n    elif os.path.exists(path + \'.gz\'):\n      os.remove(path + \'.gz\')\n    elif os.path.exists(path + "".br""):\n      os.remove(path + "".br"")\n\n  def delete_files(self, file_paths):\n    for path in file_paths:\n      self.delete_file(path)\n\n  def list_files(self, prefix, flat):\n    """"""\n    List the files in the layer with the given prefix. \n\n    flat means only generate one level of a directory,\n    while non-flat means generate all file paths with that \n    prefix.\n    """"""\n\n    layer_path = self.get_path_to_file("""")        \n    path = os.path.join(layer_path, prefix) + \'*\'\n\n    filenames = []\n\n    remove = layer_path\n    if len(remove) and remove[-1] != \'/\':\n      remove += \'/\'\n\n    if flat:\n      for file_path in glob(path):\n        if not os.path.isfile(file_path):\n          continue\n        filename = file_path.replace(remove, \'\')\n        filenames.append(filename)\n    else:\n      subdir = os.path.join(layer_path, os.path.dirname(prefix))\n      for root, dirs, files in os.walk(subdir):\n        files = [ os.path.join(root, f) for f in files ]\n        files = [ f.replace(remove, \'\') for f in files ]\n        files = [ f for f in files if f[:len(prefix)] == prefix ]\n        \n        for filename in files:\n          filenames.append(filename)\n    \n    def stripext(fname):\n      (base, ext) = os.path.splitext(fname)\n      if ext in COMPRESSION_EXTENSIONS:\n        return base\n      else:\n        return fname\n\n    filenames = list(map(stripext, filenames))\n    return _radix_sort(filenames).__iter__()\n\nclass GoogleCloudStorageInterface(StorageInterface):\n  def __init__(self, path):\n    super(StorageInterface, self).__init__()\n    global GC_POOL\n    self._path = path\n    self._bucket = GC_POOL[path.bucket].get_connection()\n\n  def get_path_to_file(self, file_path):\n    return posixpath.join(self._path.no_bucket_basepath, self._path.layer, file_path)\n\n  @retry\n  def put_file(self, file_path, content, content_type, compress, cache_control=None):\n    key = self.get_path_to_file(file_path)\n    blob = self._bucket.blob( key )\n\n    # gcloud disable brotli until content-encoding works\n    if compress == ""br"":\n      raise UnsupportedCompressionType(""Brotli unsupported on google cloud storage"")\n    elif compress:\n      blob.content_encoding = ""gzip""\n    if cache_control:\n      blob.cache_control = cache_control\n    blob.upload_from_string(content, content_type)\n\n  @retry\n  def get_file(self, file_path, start=None, end=None):\n    key = self.get_path_to_file(file_path)\n    blob = self._bucket.blob( key )\n\n    if start is not None:\n      start = int(start)\n    if end is not None:\n      end = int(end - 1)      \n\n    try:\n      # blob handles the decompression so the encoding is None\n      return blob.download_as_string(start=start, end=end), None # content, encoding\n    except google.cloud.exceptions.NotFound as err:\n      return None, None\n\n  @retry\n  def exists(self, file_path):\n    key = self.get_path_to_file(file_path)\n    blob = self._bucket.blob(key)\n    return blob.exists()\n\n  def files_exist(self, file_paths):\n    result = {path: None for path in file_paths}\n    MAX_BATCH_SIZE = Batch._MAX_BATCH_SIZE\n\n    for i in range(0, len(file_paths), MAX_BATCH_SIZE):\n      # Retrieve current batch of blobs. On Batch __exit__ it will populate all\n      # future responses before raising errors about the (likely) missing keys.\n      try:\n        with self._bucket.client.batch():\n          for file_path in file_paths[i:i+MAX_BATCH_SIZE]:\n            key = self.get_path_to_file(file_path)\n            result[file_path] = self._bucket.get_blob(key)\n      except google.cloud.exceptions.NotFound as err:\n        pass  # Missing keys are expected\n\n    for file_path, blob in result.items():\n      # Blob exists if ``dict``, missing if ``_FutureDict``\n      result[file_path] = isinstance(blob._properties, dict)\n\n    return result\n\n  @retry\n  def delete_file(self, file_path):\n    key = self.get_path_to_file(file_path)\n    \n    try:\n      self._bucket.delete_blob( key )\n    except google.cloud.exceptions.NotFound:\n      pass\n\n  def delete_files(self, file_paths):\n    MAX_BATCH_SIZE = Batch._MAX_BATCH_SIZE\n\n    for i in range(0, len(file_paths), MAX_BATCH_SIZE):\n      try:\n        with self._bucket.client.batch():\n          for file_path in file_paths[i : i + MAX_BATCH_SIZE]:\n            key = self.get_path_to_file(file_path)\n            self._bucket.delete_blob(key)\n      except google.cloud.exceptions.NotFound:\n        pass\n\n  @retry\n  def list_files(self, prefix, flat=False):\n    """"""\n    List the files in the layer with the given prefix. \n\n    flat means only generate one level of a directory,\n    while non-flat means generate all file paths with that \n    prefix.\n    """"""\n    layer_path = self.get_path_to_file("""")        \n    path = posixpath.join(layer_path, prefix)\n    for blob in self._bucket.list_blobs(prefix=path):\n      filename = blob.name.replace(layer_path, \'\')\n      if not filename:\n        continue\n      elif not flat and filename[-1] != \'/\':\n        yield filename\n      elif flat and \'/\' not in blob.name.replace(path, \'\'):\n        yield filename\n\n  def release_connection(self):\n    global GC_POOL\n    GC_POOL[self._path.bucket].release_connection(self._bucket)\n\nclass HttpInterface(StorageInterface):\n  def __init__(self, path):\n    super(StorageInterface, self).__init__()\n    self._path = path\n\n  def get_path_to_file(self, file_path):\n    path = posixpath.join(\n      self._path.basepath, self._path.layer, file_path\n    )\n    return self._path.protocol + \'://\' + path\n\n  # @retry\n  def delete_file(self, file_path):\n    raise NotImplementedError()\n\n  def delete_files(self, file_paths):\n    raise NotImplementedError()\n\n  # @retry\n  def put_file(self, file_path, content, content_type, compress, cache_control=None):\n    raise NotImplementedError()\n\n  @retry\n  def get_file(self, file_path, start=None, end=None):\n    key = self.get_path_to_file(file_path)\n\n    if start is not None or end is not None:\n      start = int(start) if start is not None else \'\'\n      end = int(end - 1) if end is not None else \'\'\n      headers = { ""Range"": ""bytes={}-{}"".format(start, end) }\n      resp = requests.get(key, headers=headers)\n    else:\n      resp = requests.get(key)\n    if resp.status_code in (404, 403):\n      return None, None\n    resp.raise_for_status()\n\n    if \'Content-Encoding\' not in resp.headers:\n      return resp.content, None\n    # requests automatically decodes these\n    elif resp.headers[\'Content-Encoding\'] in (\'\', \'gzip\', \'deflate\', \'br\'):\n      return resp.content, None\n    else:\n      return resp.content, resp.headers[\'Content-Encoding\']\n\n  @retry\n  def exists(self, file_path):\n    key = self.get_path_to_file(file_path)\n    resp = requests.get(key, stream=True)\n    resp.close()\n    return resp.ok\n\n  def files_exist(self, file_paths):\n    return {path: self.exists(path) for path in file_paths}\n\n  def list_files(self, prefix, flat=False):\n    raise NotImplementedError()\n\nclass S3Interface(StorageInterface):\n  def __init__(self, path):\n    super(StorageInterface, self).__init__()\n    global S3_POOL\n    self._path = path\n    self._conn = S3_POOL[path.protocol][path.bucket].get_connection()\n\n  def get_path_to_file(self, file_path):\n    return posixpath.join(self._path.no_bucket_basepath, self._path.layer, file_path)\n\n  @retry\n  def put_file(self, file_path, content, content_type, compress, cache_control=None, ACL=""bucket-owner-full-control""):\n    key = self.get_path_to_file(file_path)\n\n    attrs = {\n      \'Bucket\': self._path.bucket,\n      \'Body\': content,\n      \'Key\': key,\n      \'ContentType\': (content_type or \'application/octet-stream\'),\n      \'ACL\': ACL,\n    }\n\n    # keep gzip as default\n    if compress == ""br"":\n      attrs[\'ContentEncoding\'] = \'br\'\n    elif compress:\n      attrs[\'ContentEncoding\'] = \'gzip\'\n    if cache_control:\n      attrs[\'CacheControl\'] = cache_control\n\n    self._conn.put_object(**attrs)\n\n  @retry\n  def get_file(self, file_path, start=None, end=None):\n    """"""\n    There are many types of execptions which can get raised\n    from this method. We want to make sure we only return\n    None when the file doesn\'t exist.\n    """"""\n\n    kwargs = {}\n    if start is not None or end is not None:\n      start = int(start) if start is not None else \'\'\n      end = int(end - 1) if end is not None else \'\'\n      kwargs[\'Range\'] = ""bytes={}-{}"".format(start, end)\n\n    try:\n      resp = self._conn.get_object(\n        Bucket=self._path.bucket,\n        Key=self.get_path_to_file(file_path),\n        **kwargs\n      )\n\n      encoding = \'\'\n      if \'ContentEncoding\' in resp:\n        encoding = resp[\'ContentEncoding\']\n\n      return resp[\'Body\'].read(), encoding\n    except botocore.exceptions.ClientError as err: \n      if err.response[\'Error\'][\'Code\'] == \'NoSuchKey\':\n        return None, None\n      else:\n        raise\n\n  def exists(self, file_path):\n    exists = True\n    try:\n      self._conn.head_object(\n        Bucket=self._path.bucket,\n        Key=self.get_path_to_file(file_path),\n      )\n    except botocore.exceptions.ClientError as e:\n      if e.response[\'Error\'][\'Code\'] == ""404"":\n        exists = False\n      else:\n        raise\n    \n    return exists\n\n  def files_exist(self, file_paths):\n    return {path: self.exists(path) for path in file_paths}\n\n  @retry\n  def delete_file(self, file_path):\n\n    # Not necessary to handle 404s here.\n    # From the boto3 documentation:\n\n    # delete_object(**kwargs)\n    # Removes the null version (if there is one) of an object and inserts a delete marker, \n    # which becomes the latest version of the object. If there isn\'t a null version, \n    # Amazon S3 does not remove any objects.\n\n    # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.delete_object\n\n    self._conn.delete_object(\n      Bucket=self._path.bucket,\n      Key=self.get_path_to_file(file_path),\n    )\n\n  def delete_files(self, file_paths):\n    for path in file_paths:\n      self.delete_file(path)\n\n  def list_files(self, prefix, flat=False):\n    """"""\n    List the files in the layer with the given prefix. \n\n    flat means only generate one level of a directory,\n    while non-flat means generate all file paths with that \n    prefix.\n    """"""\n\n    layer_path = self.get_path_to_file("""")        \n    path = posixpath.join(layer_path, prefix)\n\n    @retry\n    def s3lst(continuation_token=None):\n      kwargs = {\n        \'Bucket\': self._path.bucket,\n        \'Prefix\': path,\n      }\n\n      if continuation_token:\n        kwargs[\'ContinuationToken\'] = continuation_token\n\n      return self._conn.list_objects_v2(**kwargs)\n\n    resp = s3lst()\n\n    def iterate(resp):\n      if \'Contents\' not in resp.keys():\n        resp[\'Contents\'] = []\n\n      for item in resp[\'Contents\']:\n        key = item[\'Key\']\n        filename = key.replace(layer_path, \'\')\n        if not flat and filename[-1] != \'/\':\n          yield filename\n        elif flat and \'/\' not in key.replace(path, \'\'):\n          yield filename\n\n\n    for filename in iterate(resp):\n      yield filename\n\n    while resp[\'IsTruncated\'] and resp[\'NextContinuationToken\']:\n      resp = s3lst(resp[\'NextContinuationToken\'])\n\n      for filename in iterate(resp):\n        yield filename\n\n  def release_connection(self):\n    global S3_POOL\n    S3_POOL[self._path.protocol][self._path.bucket].release_connection(self._conn)\n\n\ndef _radix_sort(L, i=0):\n  """"""\n  Most significant char radix sort\n  """"""\n  if len(L) <= 1: \n    return L\n  done_bucket = []\n  buckets = [ [] for x in range(255) ]\n  for s in L:\n    if i >= len(s):\n      done_bucket.append(s)\n    else:\n      buckets[ ord(s[i]) ].append(s)\n  buckets = [ _radix_sort(b, i + 1) for b in buckets ]\n  return done_bucket + [ b for blist in buckets for b in blist ]'"
cloudvolume/datasource/boss/__init__.py,0,"b""from .image import BossImageSource\nfrom .metadata import BossMetadata\n\nfrom ...frontends.precomputed import CloudVolumePrecomputed\n\nfrom ...cacheservice import CacheService\nfrom ...cloudvolume import SharedConfiguration, register_plugin\nfrom ...paths import strict_extract\n\ndef create_boss(\n    cloudpath, mip=0, bounded=True, autocrop=False,\n    fill_missing=False, cache=False, compress_cache=None,\n    cdn_cache=True, progress=False, info=None, provenance=None,\n    compress=None, non_aligned_writes=False, parallel=1,\n    delete_black_uploads=False, green_threads=False\n  ):\n    path = strict_extract(cloudpath)\n    config = SharedConfiguration(\n      cdn_cache=cdn_cache,\n      compress=compress,\n      compress_level=None,\n      green=green_threads,\n      mip=mip,\n      parallel=parallel,\n      progress=progress,\n    )\n    cache = CacheService(\n      cloudpath=(cache if type(cache) == str else cloudpath),\n      enabled=bool(cache),\n      config=config,\n      compress=compress_cache,\n    )\n\n    meta = BossMetadata(cloudpath, cache=cache, info=info)\n    image = BossImageSource(\n      config, meta, cache, \n      autocrop=bool(autocrop),\n      bounded=bool(bounded),\n      non_aligned_writes=bool(non_aligned_writes), \n    )\n\n    return CloudVolumePrecomputed(\n      meta, cache, config, \n      imagesrc, mesh=None, skeleton=None,\n      mip=mip\n    )\n\ndef register():\n  register_plugin('boss', create_boss)"""
cloudvolume/datasource/boss/image.py,2,"b""from __future__ import print_function\n\nfrom intern.remote.boss import BossRemote\nfrom intern.resource.boss.resource import (\n  ChannelResource, ExperimentResource, CoordinateFrameResource\n)\n\nfrom .. import autocropfn, readonlyguard, ImageSourceInterface\n\nfrom ... import exceptions \nfrom ...lib import ( \n  colorize, red, mkdir, Vec, Bbox,  \n  jsonify, generate_random_string\n)\nfrom ...secrets import boss_credentials\nfrom ...volumecutout import VolumeCutout\n\nclass BossImageSource(ImageSourceInterface):\n  def __init__(\n    self, config, meta, cache,\n    autocrop=False, bounded=True,\n    non_aligned_writes=False,\n    delete_black_uploads=False,\n    readonly=False,\n  ):\n    self.config = config\n    self.meta = meta \n    self.cache = cache \n\n    self.autocrop = bool(autocrop)\n    self.bounded = bool(bounded)\n    self.non_aligned_writes = bool(non_aligned_writes)\n    self.readonly = bool(readonly)\n\n  def download(self, bbox, mip):\n    bounds = Bbox.clamp(bbox, self.meta.bounds(mip))\n\n    if self.autocrop:\n      image, bounds = autocropfn(self.meta, image, bounds, mip)\n    \n    if bounds.subvoxel():\n      raise exceptions.EmptyRequestException('Requested less than one pixel of volume. {}'.format(bounds))\n\n    x_rng = [ bounds.minpt.x, bounds.maxpt.x ]\n    y_rng = [ bounds.minpt.y, bounds.maxpt.y ]\n    z_rng = [ bounds.minpt.z, bounds.maxpt.z ]\n\n    layer_type = 'image' if self.meta.layer_type == 'unknown' else self.meta.layer_type\n\n    chan = ChannelResource(\n      collection_name=self.meta.path.bucket, \n      experiment_name=self.meta.path.dataset, \n      name=self.meta.path.layer, # Channel\n      type=layer_type, \n      datatype=self.meta.data_type,\n    )\n\n    rmt = BossRemote(boss_credentials)\n    cutout = rmt.get_cutout(chan, mip, x_rng, y_rng, z_rng, no_cache=True)\n    cutout = cutout.T\n    cutout = cutout.astype(self.meta.dtype)\n    cutout = cutout[::steps.x, ::steps.y, ::steps.z]\n\n    if len(cutout.shape) == 3:\n      cutout = cutout.reshape(tuple(list(cutout.shape) + [ 1 ]))\n\n    if self.bounded or self.autocrop or bounds == bbox:\n      return VolumeCutout.from_volume(self.meta, mip, cutout, bounds)\n\n    # This section below covers the case where the requested volume is bigger\n    # than the dataset volume and the bounds guards have been switched \n    # off. This is useful for Marching Cubes where a 1px excess boundary\n    # is needed.\n    shape = list(bbox.size3()) + [ cutout.shape[3] ]\n    renderbuffer = np.zeros(shape=shape, dtype=self.meta.dtype, order='F')\n    txrx.shade(renderbuffer, bbox, cutout, bounds)\n    return VolumeCutout.from_volume(self.meta, mip, renderbuffer, bbox)\n\n  @readonlyguard\n  def upload(self, image, offset, mip):\n    shape = Vec(*image.shape[:3])\n    offset = Vec(*offset)\n\n    bounds = Bbox(offset, shape + offset)\n\n    if bounds.subvoxel():\n      raise exceptions.EmptyRequestException('Requested less than one pixel of volume. {}'.format(bounds))\n\n    if self.autocrop:\n      image, bounds = autocropfn(self.meta, image, bounds, mip)\n      offset = bounds.minpt\n\n    check_grid_aligned(\n      self.meta, image, bounds, mip, \n      throw_error=(self.non_aligned_writes == False)\n    )\n\n    x_rng = [ bounds.minpt.x, bounds.maxpt.x ]\n    y_rng = [ bounds.minpt.y, bounds.maxpt.y ]\n    z_rng = [ bounds.minpt.z, bounds.maxpt.z ]\n\n    layer_type = 'image' if self.layer_type == 'unknown' else self.meta.layer_type\n\n    chan = ChannelResource(\n      collection_name=self.meta.path.bucket, \n      experiment_name=self.meta.path.dataset, \n      name=self.meta.path.layer, # Channel\n      type=layer_type, \n      datatype=self.meta.data_type,\n    )\n\n    if image.shape[3] == 1:\n      image = image.reshape( image.shape[:3] )\n\n    rmt = BossRemote(boss_credentials)\n    image = image.T\n    image = np.asfortranarray(image.astype(self.meta.dtype))\n\n    rmt.create_cutout(chan, mip, x_rng, y_rng, z_rng, image)\n\n  def exists(self, bbox, mip=None):\n    raise NotImplementedError()\n\n  @readonlyguard\n  def delete(self, bbox, mip=None):\n    raise NotImplementedError()\n\n  def transfer_to(self, cloudpath, bbox, mip, block_size=None, compress=True):\n    raise NotImplementedError()"""
cloudvolume/datasource/boss/metadata.py,0,"b'from intern.remote.boss import BossRemote\nfrom intern.resource.boss.resource import ChannelResource, ExperimentResource, CoordinateFrameResource\nfrom ...secrets import boss_credentials\n\nfrom cloudvolume.datasource.precomputed.metadata import PrecomputedMetadata\n\nclass BossMetadata(PrecomputedMetadata):\n  def __init__(self, cloudpath, cache, info=None):\n    super(BossMetadata, self).__init__(\n      cloudpath, cache, info=info, provenance=None\n    )\n\n  def commit_info(self):\n    """"""BOSS doesn\'t support editing metadata after creation.""""""\n    pass \n\n  def fetch_info(self):\n    experiment = ExperimentResource(\n      name=self.path.dataset, \n      collection_name=self.path.bucket\n    )\n    rmt = BossRemote(boss_credentials)\n    experiment = rmt.get_project(experiment)\n\n    coord_frame = CoordinateFrameResource(name=experiment.coord_frame)\n    coord_frame = rmt.get_project(coord_frame)\n\n    channel = ChannelResource(self.path.layer, self.path.bucket, self.path.dataset)\n    channel = rmt.get_project(channel)    \n\n    unit_factors = {\n      \'nanometers\': 1,\n      \'micrometers\': 1e3,\n      \'millimeters\': 1e6,\n      \'centimeters\': 1e7,\n    }\n\n    unit_factor = unit_factors[coord_frame.voxel_unit]\n\n    cf = coord_frame\n    resolution = [ cf.x_voxel_size, cf.y_voxel_size, cf.z_voxel_size ]\n    resolution = [ int(round(_)) * unit_factor for _ in resolution ]\n\n    bbox = Bbox(\n      (cf.x_start, cf.y_start, cf.z_start),\n      (cf.x_stop, cf.y_stop, cf.z_stop)\n    )\n    bbox.maxpt = bbox.maxpt \n\n    layer_type = \'unknown\'\n    if \'type\' in channel.raw:\n      layer_type = channel.raw[\'type\']\n\n    info = CloudVolume.create_new_info(\n      num_channels=1,\n      layer_type=layer_type,\n      data_type=channel.datatype,\n      encoding=\'raw\',\n      resolution=resolution,\n      voxel_offset=bbox.minpt,\n      volume_size=bbox.size3(),\n      chunk_size=(512, 512, 16), # fixed in s3 implementation\n    )\n\n    each_factor = Vec(2,2,1)\n    if experiment.hierarchy_method == \'isotropic\':\n      each_factor = Vec(2,2,2)\n    \n    factor = each_factor.clone()\n    for _ in range(1, experiment.num_hierarchy_levels):\n      self.add_scale(factor, info=info)\n      factor *= each_factor\n\n    return info\n\n  def commit_provenance(self):\n    """"""BOSS doesn\'t support provenance files.""""""\n    pass\n\n  def fetch_provenance(self):\n    """"""BOSS doesn\'t support provenance files.""""""\n    pass'"
cloudvolume/datasource/graphene/__init__.py,0,"b""from .mesh import GrapheneMeshSource\nfrom .metadata import GrapheneMetadata\nfrom ..precomputed.image import PrecomputedImageSource\nfrom ..precomputed.skeleton import PrecomputedSkeletonSource\n\nfrom ...cacheservice import CacheService\nfrom ...cloudvolume import SharedConfiguration, register_plugin\nfrom ...paths import strict_extract\n\nfrom requests import HTTPError\n\ndef create_graphene(\n    cloudpath, mip=0, bounded=True, autocrop=False,\n    fill_missing=False, cache=False, compress_cache=None,\n    cdn_cache=True, progress=False, info=None, provenance=None,\n    compress=None, parallel=1,\n    delete_black_uploads=False, background_color=0,\n    green_threads=False, use_https=False,\n    mesh_dir=None, skel_dir=None, **kwargs\n  ):\n    from ...frontends import CloudVolumeGraphene\n    \n    path = strict_extract(cloudpath)\n    config = SharedConfiguration(\n      cdn_cache=cdn_cache,\n      compress=compress,\n      compress_level=None,\n      green=green_threads,\n      mip=mip,\n      parallel=parallel,\n      progress=progress,\n    )\n\n    def mkcache(cloudpath):\n      return CacheService(\n        cloudpath=(cache if type(cache) == str else cloudpath),\n        enabled=bool(cache),\n        config=config,\n        compress=compress_cache,\n      )\n    meta = GrapheneMetadata(\n      cloudpath, cache=mkcache(cloudpath),\n      info=info, provenance=provenance, use_https=use_https\n    )\n    # Resetting the cache is necessary because\n    # graphene retrieves a data_dir from the info file\n    # that reflects the real cache location.\n    cache = mkcache(meta.cloudpath) \n    meta.cache = cache\n    cache.meta = meta\n\n    if mesh_dir:\n      meta.info['mesh'] = str(mesh_dir)\n    if skel_dir:\n      meta.info['skeletons'] = str(skel_dir)\n\n    image = PrecomputedImageSource(\n      config, meta, cache,\n      autocrop=bool(autocrop),\n      bounded=bool(bounded),\n      non_aligned_writes=False,\n      fill_missing=bool(fill_missing),\n      delete_black_uploads=bool(delete_black_uploads),\n      background_color=background_color,\n    )\n\n    mesh = GrapheneMeshSource(meta, cache, config)\n    skeleton = PrecomputedSkeletonSource(meta, cache, config)\n\n    return CloudVolumeGraphene(\n      meta, cache, config, \n      image, mesh, skeleton,\n      mip=mip\n    )\n\ndef register():\n  register_plugin('graphene', create_graphene)"""
cloudvolume/datasource/graphene/metadata.py,3,"b'import posixpath\nfrom collections import namedtuple\nimport json\nimport re\nimport requests\nfrom six.moves import urllib\n\nimport numpy as np\n\nfrom ...lib import Vec\nfrom ... import exceptions\nfrom ... import paths\nfrom ...secrets import chunkedgraph_credentials\nfrom ..precomputed import PrecomputedMetadata\n\nVERSION_ORDERING = [  \n  \'1.0\', \'v1\'\n]\nVERSION_MAP = {\n  version: i for i, version in enumerate(VERSION_ORDERING)\n}\n\nuint64 = np.uint64\nGrapheneLabel = namedtuple(\'GrapheneLabel\', (\'level\', \'x\', \'y\', \'z\', \'segid\'))\n\n\nclass GrapheneApiVersion():\n  def __init__(self, version):\n    self.version = version.lower()\n    if self.version == \'table\':\n      self.version = VERSION_ORDERING[-1]\n    elif self.version not in VERSION_MAP:\n      raise ValueError(""Unknown Graphene API version {}"".format(self.version))\n\n  def __eq__(self, rhs):\n    return self.version == rhs.version\n  def __ne__(self, rhs):\n    return self.version != rhs.version\n  def __lt__(self, rhs):\n    return self.sequence_number() < rhs.sequence_number()\n  def __gt__(self, rhs):\n    return self.sequence_number() > rhs.sequence_number()\n  def __le__(self, rhs):\n    return self.sequence_number() <= rhs.sequence_number()\n  def __ge__(self, rhs):\n    return self.sequence_number() >= rhs.sequence_number()\n  def __str__(self):\n    return self.version\n  def __repr__(self):\n    return ""GrapheneApiVersion(\'{}\')"".format(self.version)\n\n  def sequence_number(self):\n    return VERSION_MAP[self.version]\n\n  def path(self, graphene_path):\n    if self.version == \'1.0\':\n      return self.legacy_path(graphene_path)\n    return self.api_vx_path(graphene_path)\n\n  def table_path(self, graphene_path):\n    return posixpath.join(graphene_path.modality, \'table\', graphene_path.dataset)\n\n  def legacy_path(self, graphene_path):\n    """"""All /segmentation/1.0/$DATASET paths""""""\n    return posixpath.join(graphene_path.modality, \'1.0\', graphene_path.dataset)\n\n  def api_vx_path(self, graphene_path):\n    """"""\n    All /segmentation/api/v1/$DATASET paths.\n\n    As of Feb. 2020, these were the latest paths.\n    """"""\n    return posixpath.join( \n      graphene_path.modality, \'api\', self.version, \'table\', graphene_path.dataset\n    )\n\nclass GrapheneMetadata(PrecomputedMetadata):\n  def __init__(self, cloudpath, use_https=False, use_auth=True, auth_token=None, *args, **kwargs):\n    self.server_url = cloudpath.replace(\'graphene://\', \'\')\n    self.server_path = extract_graphene_path(self.server_url)\n    self.use_https = use_https\n    self.auth_header = None\n    self.spatial_index = None\n    if use_auth:\n      token = None\n      if chunkedgraph_credentials:\n        token = chunkedgraph_credentials[""token""]\n      if auth_token:\n        token = auth_token\n      self.auth_header = {\n        ""Authorization"": ""Bearer %s"" % token\n      }\n    super(GrapheneMetadata, self).__init__(cloudpath, *args, **kwargs)\n\n    version = self.server_path.version\n    if version == \'table\':\n      version = self.supported_api_versions[-1].version\n\n    self.api_version = GrapheneApiVersion(version)\n\n  def supports_api(self, version):\n    return GrapheneApiVersion(version) in self.supported_api_versions\n\n  @property  \n  def supported_api_versions(self):\n    versions = [ \n      GrapheneApiVersion(VERSION_ORDERING[i]) \\\n      for i in self.info[\'app\'][\'supported_api_versions\'] \n    ]\n    versions.sort(key=lambda ver: ver.sequence_number())\n    return versions\n\n  @property\n  def base_path(self):\n    path = self.server_path\n    if path.subdomain is None:\n      return path.scheme + \'://\' + path.domain + \'/\'   \n    return path.scheme + \'://\' + path.subdomain + \'.\' + path.domain + \'/\' \n\n  @property\n  def table_path(self):\n    return posixpath.join(self.base_path, self.server_path.modality, \'table\', self.server_path.dataset)\n\n  @property\n  def info_path(self):\n    """"""e.g. https://SUBDOMAIN.dynamicannotationframework.com/segmentation/table/DATASET/info""""""\n    return posixpath.join(self.table_path, \'info\')\n\n  def fetch_info(self):\n    """"""\n    Reads info from chunkedgraph endpoint and extracts relevant information\n    """"""\n    r = requests.get(self.info_path, headers=self.auth_header)\n    r.raise_for_status()\n    return r.json()\n\n  @property\n  def mesh_path(self):\n    if \'mesh\' in self.info:\n      return self.info[\'mesh\']\n    return \'mesh\'\n\n  @property\n  def cloudpath(self):\n    data_dir = self.info[\'data_dir\']\n    if self.use_https:\n      data_dir = paths.to_https_protocol(data_dir)\n    return data_dir\n\n  @property\n  def fan_out(self):\n    """"""Number of chunks agglomerated into a new chunk per a level increase in the graph.""""""\n    graph_object = self.info[\'graph\']\n    return int(graph_object.get(\'fan_out\', 2))\n\n  def decode_label(self, label):\n    level = self.decode_layer_id(label)\n    x,y,z = self.decode_chunk_position(label)\n    segid = self.decode_segid(label)\n    return GrapheneLabel(level, x, y, z, segid)\n\n  def decode_segid(self, label):\n    label = uint64(label)\n    level = self.decode_layer_id(label)\n    segid_bits = self.segid_bits(level)\n\n    mask = uint64(2 ** segid_bits) - uint64(1)\n    \n    return label & mask\n\n  def decode_chunk_id(self, label):\n    """"""The chunk id is a graphene label with the segid portion zeroed out.""""""\n    label = uint64(label)\n    level = self.decode_layer_id(label)\n    bits = self.segid_bits(level)\n    mask = uint64(2 ** bits) - uint64(1)\n    return label & ~mask\n\n  def decode_chunk_position_number(self, label):\n    """"""Returns the chunk position X,Y,Z as a packed uint64.""""""\n    label = uint64(label)\n    level = self.decode_layer_id(label)\n    ct = self.spatial_bit_count(level)\n    label = label & uint64(0x00ffffffffffffff)\n    return label >> uint64(self.segid_bits(level))\n\n  def decode_chunk_position(self, label):\n    """"""Returns the chunk position as a tuple (X,Y,Z)""""""\n    label = uint64(label)\n    level = self.decode_layer_id(label)\n    ct = self.spatial_bit_count(level)\n    label = label & uint64(0x00ffffffffffffff)\n    masks = self.spatial_bit_masks(level)\n    segid_bits = self.segid_bits(level)\n\n    x = (label & masks[0]) >> uint64(segid_bits + 2 * ct)\n    y = (label & masks[1]) >> uint64(segid_bits + 1 * ct)\n    z = (label & masks[2]) >> uint64(segid_bits + 0 * ct)\n\n    return Vec(x,y,z)\n\n  def point_to_chunk_position(self, pt, mip=None):\n    """"""\n    Convert a point into the chunk position.\n\n    pt: x,y,z triple\n    mip: \n      if None, pt is in physical coordinates\n      else pt is in the coordinates of the indicated mip level\n\n    Returns: Vec(chunk_x,chunk_y,chunk_z)\n    """"""\n    pt = Vec(*pt, dtype=np.float)\n\n    if mip is not None:\n      pt *= self.resolution(mip)\n\n    pt /= self.resolution(self.watershed_mip)\n\n    if self.chunks_start_at_voxel_offset:\n      pt -= self.voxel_offset(self.watershed_mip)\n\n    return (pt // self.graph_chunk_size).astype(np.int32)\n\n  def segid_bits(self, level):\n    ct = self.spatial_bit_count(level)\n    return uint64(64 - self.n_bits_for_layer_id - 3 * ct)\n\n  def decode_layer_id(self, label):\n    return uint64(label) >> uint64(64 - self.n_bits_for_layer_id)\n\n  def encode_label(self, layer, x, y, z, segid):\n    """"""\n    Create a graphene label from the specified values.\n\n    Another way to use this:\n\n    glabel = GrapheneLabel(2,1,1,1,777)\n    meta.encode_label(*glabel)\n    """"""\n    if layer > self.n_layers:\n      raise ValueError(""Provided layer %d is greater than the number of layers in the dataset: %d"" % layer, self.n_layers)\n\n    layer_offset = uint64(64 - self.n_bits_for_layer_id)\n    bits_per_dim = uint64(self.spatial_bit_count(layer))\n    x_offset = uint64(layer_offset - bits_per_dim)\n    y_offset = uint64(x_offset - bits_per_dim)\n    z_offset = uint64(y_offset - bits_per_dim)\n\n    if not (\n      x < 2 ** bits_per_dim and y < 2 ** bits_per_dim and z < 2 ** bits_per_dim\n    ):\n      raise ValueError(\n        ""Chunk coordinate is out of range for ""\n        ""this graph on layer %d with %d bits/dim. ""\n        ""[%d, %d, %d]; max = %d.""\n        % (layer, bits_per_dim, x, y, z, 2 ** bits_per_dim)\n      )\n\n    if segid >= 2 ** self.segid_bits(layer):\n      raise ValueError(\n        ""segid {} provided is out of range. It must be less than {}"".format(\n          segid, 2 ** self.segid_bits(layer)\n      ))\n\n    layer = uint64(layer)\n    x, y, z = uint64(x), uint64(y), uint64(z)\n    segid = uint64(segid)\n\n    return uint64(\n      layer << layer_offset | x << x_offset | y << y_offset | z << z_offset | segid\n    )\n\n  def spatial_bit_masks(self, level):\n    ct = self.spatial_bit_count(level)\n\n    mask = uint64(2 ** ct) - uint64(1)\n    segid_bits = 64 - self.n_bits_for_layer_id - 3 * ct\n\n    return [\n      mask << uint64(segid_bits + 2 * ct),\n      mask << uint64(segid_bits + 1 * ct),\n      mask << uint64(segid_bits + 0 * ct)\n    ]\n\n  def spatial_bit_count(self, level):\n    """"""\n    64-bit labels\n\n    8-bit  chunk coord\n    layer | X | Y | Z | segid\n\n    This method returns how many bits in X,Y,Z\n    """"""\n    return int(self.info[\'graph\'][\'spatial_bit_masks\'][str(level)])\n\n  @property\n  def n_bits_for_layer_id(self):\n    return int(self.info[\'graph\'].get(\'n_bits_for_layer_id\', 8))\n\n  @property\n  def n_layers(self):\n    return int(self.info[\'graph\'][\'n_layers\'])\n\n  @property\n  def graph_chunk_size(self):\n    return self.info[\'graph\'][\'chunk_size\']\n\n  @property\n  def uses_new_draco_bin_size(self):\n    graph_object = self.info[\'graph\']\n    return int(graph_object.get(\'uses_new_draco_bin_size\', False))\n  \n  @property\n  def mesh_chunk_size(self):\n    # TODO: add this as new parameter to the info as it can be different from the chunkedgraph chunksize\n    return self.graph_chunk_size\n\n  @property\n  def manifest_endpoint(self):\n    pth = self.server_path\n    pth = GraphenePath(\n      pth.scheme, pth.subdomain, pth.domain, \n      \'meshing\', pth.version, pth.dataset\n    )\n\n    url = self.api_version.path(pth)\n    return posixpath.join(self.base_path, url, \'manifest\')\n\n  @property\n  def chunks_start_at_voxel_offset(self):\n    """"""\n    Boolean property specifying whether ChunkedGraph chunks begin\n    at voxel offset or at origin.\n    """"""\n    if \'chunks_start_at_voxel_offset\' in self.info:\n      return self.info[""chunks_start_at_voxel_offset""]\n    return False\n\n  @property\n  def mesh_metadata(self):\n    if \'mesh_metadata\' in self.info:\n      return self.info[""mesh_metadata""]\n    return None\n\n  @property\n  def uniform_draco_grid_size(self):\n    """"""\n    If not None, a number that specifies the draco_grid_size at every ChunkedGraph level.\n    """"""\n    if self.mesh_metadata and \'uniform_draco_grid_size\' in self.mesh_metadata:\n      return self.mesh_metadata[""uniform_draco_grid_size""]\n    return None\n\n  @property\n  def max_meshed_layer(self):\n    """"""\n    The highest level in the ChunkedGraph that we create meshes for in this dataset.\n    """"""\n    if self.mesh_metadata and \'max_meshed_layer\' in self.mesh_metadata:\n      return self.mesh_metadata[""max_meshed_layer""]\n    return None\n\n  @property\n  def watershed_mip(self):\n    """"""mip level of the base segmentation that all chunk graph operations remap.""""""\n    return self.info[""graph""][""cv_mip""]\n\n  def get_draco_grid_size(self, level):\n    """"""\n    Returns the draco_grid_size for specified ChunkedGraph level.\n    """"""\n    if self.mesh_metadata is None:\n      raise ValueError(\'This layer is not draco meshed\')\n    if self.uniform_draco_grid_size is not None:\n      return self.uniform_draco_grid_size\n    if self.mesh_metadata[""max_meshed_layer""] < level:\n      raise ValueError(\n        ""Request level"",\n        level,\n        "". But the maximum meshed level is "",\n        self.mesh_metadata[""max_meshed_layer""],\n      )\n    return self.mesh_metadata[""draco_grid_sizes""][str(level)]\n\nGraphenePath = namedtuple(\'GraphenePath\', (\'scheme\', \'subdomain\', \'domain\', \'modality\', \'version\', \'dataset\'))\nLEGACY_EXTRACTION_RE = re.compile(r\'/?(\\w+)/([\\d\\.]+)/([\\w\\d\\.\\_\\-]+)/?\')\nAPI_VX_EXTRACTION_RE = re.compile(r\'/?(\\w+)/api/(v[\\d\\.]+)/([\\w\\d\\.\\_\\-]+)/?\')\nLATEST_API_EXTRACTION_RE = re.compile(r\'/?(\\w+)/(table)/([\\w\\d\\.\\_\\-]+)/?\')\n\ndef extract_graphene_path(url):\n  """"""\n  Examples:\n  Legacy endpoint:\n    graphene://https://SUBDOMAIN.dynamicannotationframework.com/segmentation/1.0/DATASET\n  Newer endpoint:\n    graphene://https://SUBDOMAIN.dynamicannotationframework.com/segmentation/api/v1/DATASET\n  Latest endpoint:\n    graphene://https://SUBDOMAIN.DOMAIN_DOT_COM/segmentation/table/DATASET\n  """"""\n  parse = urllib.parse.urlparse(url)\n  subdomain = parse.netloc.split(\'.\')[0]\n  domain = \'.\'.join(parse.netloc.split(\'.\')[1:])\n\n  schemes = [ \n    LATEST_API_EXTRACTION_RE, API_VX_EXTRACTION_RE, LEGACY_EXTRACTION_RE \n  ]\n\n  for scheme in schemes:\n    match = re.match(scheme, parse.path)\n    if match:\n      break\n  else:\n    raise exceptions.UnsupportedFormatError(""Unable to parse Graphene URL: "" + url)\n\n  modality, version, dataset = match.groups()\n  return GraphenePath(parse.scheme, subdomain, domain, modality, version, dataset)\n\n'"
cloudvolume/datasource/graphene/sharding.py,0,"b""from ..precomputed.sharding import ShardingSpecification, ShardReader\n\nclass GrapheneShardReader(ShardReader):\n  def compute_shard_location(self, label):\n    shard_loc = self.spec.compute_shard_location(label)\n    chunk_pos_num = self.meta.meta.decode_chunk_position_number(label)\n    filename = str(chunk_pos_num) + '-' + str(shard_loc.shard_number) + '.shard'\n    return (filename, shard_loc.minishard_number)\n\n\n """
cloudvolume/datasource/precomputed/__init__.py,0,"b'from .image import PrecomputedImageSource\nfrom .metadata import PrecomputedMetadata\nfrom .mesh import PrecomputedMeshSource\nfrom .skeleton import PrecomputedSkeletonSource\n\nfrom ...cloudvolume import register_plugin, SharedConfiguration\nfrom ...cacheservice import CacheService\nfrom ...frontends import CloudVolumePrecomputed\nfrom ...lib import yellow\nfrom ...paths import strict_extract\n\ndef create_precomputed(\n    cloudpath, mip=0, bounded=True, autocrop=False,\n    fill_missing=False, cache=False, compress_cache=None,\n    cdn_cache=True, progress=False, info=None, provenance=None,\n    compress=None, compress_level=None, non_aligned_writes=False, parallel=1,\n    delete_black_uploads=False, background_color=0, \n    green_threads=False, use_https=False,\n    max_redirects=10, mesh_dir=None, skel_dir=None\n  ):\n    path = strict_extract(cloudpath)\n    config = SharedConfiguration(\n      cdn_cache=cdn_cache,\n      compress=compress,\n      compress_level=compress_level,\n      green=green_threads,\n      mip=mip,\n      parallel=parallel,\n      progress=progress,\n    )\n\n    cache = CacheService(\n      cloudpath=(cache if type(cache) == str else cloudpath),\n      enabled=bool(cache),\n      config=config,\n      compress=compress_cache,\n    )\n\n    meta = PrecomputedMetadata(\n      cloudpath, cache=cache,\n      info=info, provenance=provenance,\n      max_redirects=max_redirects\n    )\n\n    if mesh_dir:\n      meta.info[\'mesh\'] = str(mesh_dir)\n    if skel_dir:\n      meta.info[\'skeletons\'] = str(skel_dir)\n\n    readonly = bool(meta.redirected_from)\n\n    if readonly:\n      print(yellow(""""""\n        Redirected \n\n        From: {} \n        To:   {}\n        Hops: {}\n\n        Volume set to readonly to mitigate accidental\n        redirection resulting in writing data to the wrong \n        location.\n\n        To set the data to writable, access the destination\n        location directly or set:\n\n        vol.image.readonly = False\n        vol.mesh.readonly = False \n        vol.skeleton.readonly = False\n      """""".format(\n        cloudpath, meta.cloudpath, len(meta.redirected_from)\n      )))\n\n    image = PrecomputedImageSource(\n      config, meta, cache,\n      autocrop=bool(autocrop),\n      bounded=bool(bounded),\n      non_aligned_writes=bool(non_aligned_writes),\n      fill_missing=bool(fill_missing),\n      delete_black_uploads=bool(delete_black_uploads),\n      background_color=background_color,\n      readonly=readonly,\n    )\n\n    mesh = PrecomputedMeshSource(meta, cache, config, readonly)\n    skeleton = PrecomputedSkeletonSource(meta, cache, config, readonly)\n\n    return CloudVolumePrecomputed(\n      meta, cache, config, \n      image, mesh, skeleton,\n      mip\n    )\n\ndef register():\n  register_plugin(\'precomputed\', create_precomputed)'"
cloudvolume/datasource/precomputed/common.py,0,"b'def content_type(encoding):\n  if encoding == \'jpeg\':\n    return \'image/jpeg\'\n  elif encoding in (\'compressed_segmentation\', \'fpzip\', \'kempressed\'):\n    return \'image/x.\' + encoding \n  return \'application/octet-stream\'\n\ndef should_compress(encoding, compress, cache, iscache=False):\n  if iscache and cache.compress != None:\n    return cache.compress\n\n  if compress is None:\n    return \'gzip\' if encoding in (\'raw\', \'compressed_segmentation\') else None\n  elif compress == True:\n    return \'gzip\'\n  elif compress == False:\n    return None\n  else:\n    return compress\n\ndef cdn_cache_control(val):\n  """"""Translate cdn_cache into a Cache-Control HTTP header.""""""\n  if val is None:\n    return \'max-age=3600, s-max-age=3600\'\n  elif type(val) is str:\n    return val\n  elif type(val) is bool:\n    if val:\n      return \'max-age=3600, s-max-age=3600\'\n    else:\n      return \'no-cache\'\n  elif type(val) is int:\n    if val < 0:\n      raise ValueError(\n        \'cdn_cache must be a positive integer, boolean, or string. Got: \' + str(val)\n      )\n\n    if val == 0:\n      return \'no-cache\'\n    else:\n      return \'max-age={}, s-max-age={}\'.format(val, val)\n  else:\n    raise NotImplementedError(type(val) + \' is not a supported cache_control setting.\')\n'"
cloudvolume/datasource/precomputed/metadata.py,18,"b'import collections\nimport json\nimport os\nimport posixpath\n\nimport json5\nimport multiprocessing as mp\nimport numpy as np\nfrom six import string_types\nfrom six.moves import range\nfrom tqdm import tqdm\n\nfrom ... import exceptions\nfrom ...provenance import DatasetProvenance, DataLayerProvenance\nfrom ...storage import SimpleStorage\n\nfrom ... import lib\nfrom ...lib import ( \n  colorize, red, mkdir, \n  Vec, Bbox, jsonify, \n)\nfrom ...paths import strict_extract, ascloudpath\n\ndef downscale(size, factor_in_mip, roundingfn):\n  smaller = Vec(*size, dtype=np.float32) / Vec(*factor_in_mip)\n  return list(map(int, roundingfn(smaller)))\n\nclass PrecomputedMetadata(object):\n  """"""\n  The PrecomputedMetadataService provides methods for fetching,\n  saving, and accessing information about the data type & compression, \n  bounding box, resolution, and provenance of a given dataset \n  stored in Precomputed format.  \n\n  This class is a building block for building a class that can\n  read and write Precomputed data types.\n  """"""\n  def __init__(\n    self, cloudpath, cache=None, \n    info=None, provenance=None, \n    max_redirects=10\n  ):\n    self.path = strict_extract(cloudpath)\n    self.cache = cache\n    if self.cache:\n      self.cache.meta = self\n    self.info = None\n\n    self.redirected_from = []\n\n    if info is None:\n      self.refresh_info(max_redirects=max_redirects)\n      if self.cache and self.cache.enabled:\n        self.cache.check_info_validity()\n    else:\n      self.info = info\n\n    if provenance is None:\n      self.provenance = None\n      self.refresh_provenance()\n      if self.cache.enabled:\n        self.cache.check_provenance_validity()\n    else:\n      self.provenance = self._cast_provenance(provenance)\n\n  @classmethod\n  def create_info(cls, \n    num_channels, layer_type, data_type, encoding, \n    resolution, voxel_offset, volume_size, \n    mesh=None, skeletons=None, chunk_size=(128,128,64),\n    compressed_segmentation_block_size=(8,8,8),\n    max_mip=0, factor=Vec(2,2,1), redirect=None\n  ):\n    """"""\n    Create a new neuroglancer Precomputed info file.\n\n    Required:\n      num_channels: (int) 1 for grayscale, 3 for RGB \n      layer_type: (str) typically ""image"" or ""segmentation""\n      data_type: (str) e.g. ""uint8"", ""uint16"", ""uint32"", ""float32""\n      encoding: (str) ""raw"" for binaries like numpy arrays, ""jpeg""\n      resolution: int (x,y,z), x,y,z voxel dimensions in nanometers\n      voxel_offset: int (x,y,z), beginning of dataset in positive cartesian space\n      volume_size: int (x,y,z), extent of dataset in cartesian space from voxel_offset\n    \n    Optional:\n      mesh: (str) name of mesh directory, typically ""mesh""\n      skeletons: (str) name of skeletons directory, typically ""skeletons""\n      chunk_size: int (x,y,z), dimensions of each downloadable 3D image chunk in voxels\n      compressed_segmentation_block_size: (x,y,z) dimensions of each compressed sub-block\n        (only used when encoding is \'compressed_segmentation\')\n      max_mip: (int), the maximum mip level id.\n      factor: (Vec), the downsampling factor for each mip level\n      redirect: (str), cloudpath to redirect to\n\n    Returns: dict representing a single mip level that\'s JSON encodable\n    """"""\n    if not isinstance(factor, Vec):\n      factor = Vec(*factor)\n\n    if not isinstance(data_type, str):\n      data_type = np.dtype(data_type).name\n\n    info = {\n      ""num_channels"": int(num_channels),\n      ""type"": layer_type,\n      ""data_type"": data_type,\n      ""scales"": [{\n        ""encoding"": encoding,\n        ""chunk_sizes"": [chunk_size],\n        ""key"": ""_"".join(map(str, resolution)),\n        ""resolution"": list(map(int, resolution)),\n        ""voxel_offset"": list(map(int, voxel_offset)),\n        ""size"": list(map(int, volume_size)),\n      }],\n    }\n\n    if redirect is not None:\n      info[\'redirect\'] = str(redirect)\n    \n    fullres = info[\'scales\'][0]\n    factor_in_mip = factor.clone()\n \n    # add mip levels\n    for _ in range(max_mip):\n      new_resolution = list(map(int, Vec(*fullres[\'resolution\']) * factor_in_mip ))\n      newscale = {\n        u""encoding"": encoding,\n        u""chunk_sizes"": [ list(map(int, chunk_size)) ],\n        u""key"": ""_"".join(map(str, new_resolution)),\n        u""resolution"": new_resolution,\n        u""voxel_offset"": downscale(fullres[\'voxel_offset\'], factor_in_mip, np.floor),\n        u""size"": downscale(fullres[\'size\'], factor_in_mip, np.ceil),\n      }\n      info[\'scales\'].append(newscale)\n      factor_in_mip *= factor\n\n    if encoding == \'compressed_segmentation\':\n      info[\'scales\'][0][\'compressed_segmentation_block_size\'] = list(map(int, compressed_segmentation_block_size))\n\n    if mesh:\n      info[\'mesh\'] = \'mesh\' if not isinstance(mesh, string_types) else mesh\n\n    if skeletons:\n      info[\'skeletons\'] = \'skeletons\' if not isinstance(skeletons, string_types) else skeletons      \n    \n    return info\n\n  def refresh_info(self, max_redirects=10, force_fetch=False):\n    """"""\n    Refresh the current info file from the cache (if enabled) \n    or primary storage (e.g. the cloud) if not cached.\n\n    max_redirects: number of times to allow redirection. set to 0 to\n      force getting the origin info file loaded.\n    force_fetch: bypass the cache for reading, but allow writing\n\n    Raises:\n      cloudvolume.exceptions.InfoUnavailableError when the info file \n        is unable to be retrieved.\n      cloudvolume.exceptions.TooManyRedirects if more than max_redirects\n        are followed.\n      cloudvolume.exceptions.CyclicRedirect if a previously visited \n        location is revisited.\n\n    See also: fetch_info\n\n    Returns: dict\n    """"""\n    if self.cache and self.cache.enabled and not force_fetch:\n      info = self.cache.get_json(\'info\')\n      if info:\n        self.info = info\n        return self.info\n\n    self.info = self.redirectable_fetch_info(max_redirects)\n\n    if self.cache:\n      self.cache.maybe_cache_info()\n    return self.info\n\n  def fetch_info(self):\n    """"""\n    Refresh the current info file from primary storage (e.g. the cloud) without\n    refrence to the cache. The cache will not be updated.\n  \n    Raises cloudvolume.exceptions.InfoUnavailableError when the info file \n    is unable to be retrieved.\n\n    See also: refresh_info\n\n    Returns: dict\n    """"""\n    with SimpleStorage(self.cloudpath) as stor:\n      info = stor.get_json(\'info\')\n\n    if info is None:\n      raise exceptions.InfoUnavailableError(\n        red(\'No info file was found: {}\'.format(self.infopath))\n      )\n\n    return info\n\n  def redirectable_fetch_info(self, max_redirects=10):\n    """"""\n    Refresh the current info file from primary storage (e.g. the cloud) without\n    refrence to the cache. The cache will not be updated. \'redirect\' links\n    in the info file will be followed up to `max_redirects` times after which\n    an exception will be raised.\n\n    Raises:\n      cloudvolume.exceptions.InfoUnavailableError when the info file \n        is unable to be retrieved.\n      cloudvolume.exceptions.TooManyRedirects if more than max_redirects\n        are followed.\n      cloudvolume.exceptions.CyclicRedirect if a previously visited \n        location is revisited.\n\n    See also: refresh_info, fetch_info\n\n    Optional:\n      max_redirects: if \'redirect\' is specified in an info file, \n        follow the link up to this many times to the pointed locations.\n\n    Returns: dict\n    """"""\n    visited = []\n\n    if max_redirects <= 0:\n      return self.fetch_info()\n\n    if self.path.format == \'graphene\':\n      start = self.server_url\n    else:\n      start = self.cloudpath\n\n    for _ in range(max_redirects):\n      info = self.fetch_info()\n\n      if \'redirect\' not in info or not info[\'redirect\']:\n        break\n\n      path = strict_extract(info[\'redirect\'])\n      if path == self.path:\n        break \n      elif path in visited:\n        raise exceptions.CyclicRedirect(\n          """"""\nTried to redirect through a cycle.\n\nStart: {}\nHops: \n\\t{}\n\\n"""""".format(\n          start, \n          ""\\n\\t"".join([ \n            str(i+1) + "". "" + ascloudpath(v) for i, v in enumerate(visited) \n          ]))\n        )\n\n      visited.append(path)\n      self.path = path\n    else:\n      raise exceptions.TooManyRedirects(\n        ""Tried to redirect more than {} hops."".format(max_redirects)\n      )\n\n    self.redirected_from = visited[:-1]\n\n    return info\n\n  def commit_info(self):\n    """"""\n    Save the current info dict as JSON into cache and primary storage.\n\n    Raises KeyError if an encoding of \'compressed_segmentation\' is specified\n    without \'compressed_segmentation_block_size\'.\n\n    Raises ValueError if \'compressed_segmentation\' is specified and the \n    data type is not uint32 or uint64.\n    """"""\n    for scale in self.scales:\n      if scale[\'encoding\'] == \'compressed_segmentation\':\n        if \'compressed_segmentation_block_size\' not in scale.keys():\n          raise KeyError(""""""\n            \'compressed_segmentation_block_size\' must be set if \n            compressed_segmentation is set as the encoding.\n\n            A typical value for compressed_segmentation_block_size is (8,8,8)\n\n            Info file specification:\n            https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/README.md#info-json-file-specification\n          """""")\n        elif self.data_type not in (\'uint32\', \'uint64\'):\n          raise ValueError(""compressed_segmentation can only be used with uint32 and uint64 data types."")\n\n    infojson = jsonify(self.info, \n      sort_keys=True,\n      indent=2, \n      separators=(\',\', \': \')\n    )\n\n    with SimpleStorage(self.cloudpath) as stor:\n      stor.put_file(\'info\', infojson, \n        content_type=\'application/json\', \n        cache_control=\'no-cache\'\n      )\n\n    if self.cache:\n      self.cache.maybe_cache_info()\n\n  def refresh_provenance(self):\n    """"""\n    Refresh the current irovenance file from the cache (if enabled) \n    or primary storage (e.g. the cloud) if not cached. If the provenance\n    file does not exist, no error is raised and None is returned.\n\n    Raises ValueError if the provenance file is present but can not\n    be json5 decoded.\n\n    See also: fetch_provenance\n\n    Returns: dict or None\n    """"""\n    if self.cache and self.cache.enabled:\n      prov = self.cache.get_json(\'provenance\')\n      if prov:\n        self.provenance = DataLayerProvenance(**prov)\n        return self.provenance\n\n    self.provenance = self.fetch_provenance()\n    if self.cache:\n      self.cache.maybe_cache_provenance()\n    return self.provenance\n\n  def _cast_provenance(self, prov):\n    if isinstance(prov, DataLayerProvenance):\n      return prov\n    elif isinstance(prov, string_types):\n      prov = json.loads(prov)\n\n    provobj = DataLayerProvenance(**prov)\n    provobj.sources = provobj.sources or []  \n    provobj.owners = provobj.owners or []\n    provobj.processing = provobj.processing or []\n    provobj.description = provobj.description or """"\n    provobj.validate()\n    return provobj\n\n  def fetch_provenance(self):\n    """"""\n    Refresh the current provenance file from primary storage (e.g. the cloud)\n    without reference to cache. The cache will not be updated.\n  \n    Raises cloudvolume.exceptions.provenanceUnavailableError when the info file \n    is unable to be retrieved.\n\n    See also: refresh_provenance\n\n    Returns: dict\n    """"""\n    with SimpleStorage(self.cloudpath) as stor:\n      provfile = stor.get_file(\'provenance\')\n      if provfile:\n        provfile = provfile.decode(\'utf-8\')\n\n        # The json5 decoder is *very* slow\n        # so use the stricter but much faster json \n        # decoder first, and try it only if it fails.\n        try:\n          provfile = json.loads(provfile)\n        except json.decoder.JSONDecodeError:\n          try:\n            provfile = json5.loads(provfile)\n          except ValueError:\n            raise ValueError(red(""""""The provenance file could not be JSON decoded. \n              Please reformat the provenance file before continuing. \n              Contents: {}"""""".format(provfile)))\n      else:\n        provfile = {\n          ""sources"": [],\n          ""owners"": [],\n          ""processing"": [],\n          ""description"": """",\n        }\n\n    return self._cast_provenance(provfile)\n\n  def commit_provenance(self):\n    """"""\n    Save the current provenance object as JSON into cache (if enabled) \n    and primary storage.\n    """"""\n    prov = self.provenance.serialize()\n\n    # hack to pretty print provenance files\n    prov = json.loads(prov)\n    prov = jsonify(prov, \n      sort_keys=True,\n      indent=2, \n      separators=(\',\', \': \')\n    )\n\n    with SimpleStorage(self.cloudpath) as stor:\n      stor.put_file(\'provenance\', prov, \n        content_type=\'application/json\',\n        cache_control=\'no-cache\',\n      )\n    if self.cache:\n      self.cache.maybe_cache_provenance()\n\n  @property\n  def dataset(self):\n    return self.path.dataset\n  \n  @property\n  def layer(self):\n    return self.path.layer\n\n  @property\n  def scales(self):\n    return self.info[\'scales\']\n\n  @scales.setter\n  def scales(self, val):\n    self.info[\'scales\'] = val\n\n  def scale(self, mip):\n    return self.info[\'scales\'][mip]\n\n  def join(self, *paths):\n    if self.path.protocol == \'file\':\n      return os.path.join(*paths)\n    else:\n      return posixpath.join(*paths)\n\n  @property\n  def basepath(self):\n    return self.path.basepath\n    \n  @property \n  def layerpath(self):\n    return self.join(self.basepath, self.layer)\n\n  @property\n  def base_cloudpath(self):\n    return self.path.protocol + ""://"" + self.basepath\n\n  @property \n  def cloudpath(self):\n    return self.join(self.base_cloudpath, self.layer)\n  \n  @property\n  def infopath(self):\n    return self.join(self.cloudpath, \'info\')\n\n  @property\n  def skeletons(self):\n    return self.info[\'skeletons\'] if \'skeletons\' in self.info else None\n\n  @property\n  def mesh(self):\n    return self.info[\'mesh\'] if \'mesh\' in self.info else None\n\n  def shape(self, mip):\n    """"""Returns Vec(x,y,z,channels) shape of the volume similar to numpy."""""" \n    size = self.volume_size(mip)\n    return Vec(size.x, size.y, size.z, self.num_channels)\n\n  def volume_size(self, mip):\n    """"""Returns Vec(x,y,z) shape of the volume (i.e. shape - channels)."""""" \n    return Vec(*self.info[\'scales\'][mip][\'size\'])\n\n  @property\n  def available_mips(self):\n    """"""Returns a list of mip levels that are defined.""""""\n    return range(len(self.info[\'scales\']))\n\n  @property\n  def available_resolutions(self):\n    """"""Returns a list of defined resolutions.""""""\n    return (s[""resolution""] for s in self.scales)\n\n  @property\n  def layer_type(self):\n    """"""e.g. \'image\' or \'segmentation\'""""""\n    return self.info[\'type\']\n\n  @property\n  def dtype(self):\n    """"""e.g. np.uint8""""""\n    return np.dtype(self.data_type)\n\n  @property\n  def data_type(self):\n    """"""e.g. \'uint8\'""""""\n    return self.info[\'data_type\']\n\n  def encoding(self, mip, encoding=None):\n    """"""\n    If encoding is provided, set the encoding for this\n    mip level. If the encoding is not provided, this is\n    a getter.\n\n    Typical values: \'raw\', \'jpeg\', \'compressed_segmentation\'\n\n    Returns encoding for the mip level either way.\n    """"""\n    if encoding is None:\n      return self.info[\'scales\'][mip][\'encoding\']\n\n    encoding = encoding.lower()\n    scale = self.scale(mip)\n    scale[\'encoding\'] = encoding\n    if (encoding == \'compressed_segmentation\' \\\n      and \'compressed_segmentation_block_size\' not in scale):\n\n      scale[\'compressed_segmentation_block_size\'] = (8,8,8)\n\n    return encoding\n\n  def compressed_segmentation_block_size(self, mip):\n    if \'compressed_segmentation_block_size\' in self.info[\'scales\'][mip]:\n      return self.info[\'scales\'][mip][\'compressed_segmentation_block_size\']\n    return None\n\n  @property\n  def num_channels(self):\n    return int(self.info[\'num_channels\'])\n\n  def voxel_offset(self, mip):\n    """"""Vec(x,y,z) start of the dataset in voxels""""""\n    scale = self.scale(mip)\n    if \'voxel_offset\' in scale:\n      return Vec(*scale[\'voxel_offset\'])\n    else:\n      return Vec(0,0,0)\n\n  def resolution(self, mip):\n    """"""Vec(x,y,z) dimensions of each voxel in nanometers""""""\n    return Vec(*self.info[\'scales\'][mip][\'resolution\'])\n\n  def to_mip(self, mip):\n    mip = list(mip) if isinstance(mip, collections.Iterable) else int(mip)\n    try:\n      if isinstance(mip, list):  # mip specified by voxel resolution\n        return next(\n          (i for (i,s) in enumerate(self.scales) if s[""resolution""] == mip)\n        )\n      else:  # mip specified by index into downsampling hierarchy\n        return self.available_mips[mip]\n    except Exception:\n      if isinstance(mip, list):\n        opening_text = ""Scale <{}>"".format("", "".join(map(str, mip)))\n      else:\n        opening_text = ""MIP {}"".format(str(mip))\n  \n      scales = [ "","".join(map(str, scale)) for scale in self.available_resolutions ]\n      scales = [ ""<{}>"".format(scale) for scale in scales ]\n      scales = "", "".join(scales)\n      msg = ""{} not found. {} available: {}"".format(\n        opening_text, len(self.available_mips), scales\n      )\n      raise exceptions.ScaleUnavailableError(msg)\n\n  def downsample_ratio(self, mip):\n    """"""Describes how downsampled the current mip level is as an (x,y,z) factor triple.""""""\n    return self.resolution(mip) / self.resolution(0)\n\n  def chunk_size(self, mip):\n    """"""Underlying chunk size dimensions in voxels. Synonym for underlying.""""""\n    return Vec(*self.scale(mip)[\'chunk_sizes\'][0])\n\n  def key(self, mip):\n    """"""The subdirectory within the data layer containing the chunks for this mip level""""""\n    return self.scale(mip)[\'key\']\n\n  def bounds(self, mip):\n    """"""Returns a 3D spatial bounding box for the dataset with dimensions in voxels.""""""\n    offset = self.voxel_offset(mip)\n    shape = self.volume_size(mip)\n    return Bbox( offset, offset + shape )\n\n  def bbox(self, mip):\n    bounds = self.bounds(mip)\n    minpt = list(bounds.minpt) + [ 0 ]\n    maxpt = list(bounds.maxpt) + [ self.num_channels ]\n    return Bbox(minpt, maxpt)\n\n  def point_to_mip(self, pt, mip, to_mip):\n    pt = Vec(*pt)\n    downsample_ratio = self.resolution(mip).astype(np.float32) / self.resolution(to_mip).astype(np.float32)\n    return np.floor(pt * downsample_ratio)\n\n  def bbox_to_mip(self, bbox, mip, to_mip):\n    """"""Convert bbox or slices from one mip level to another.""""""\n    bbox = Bbox.create(bbox, self.bounds(mip))\n\n    def one_level(bbox, mip, to_mip):\n      original_dtype = bbox.dtype\n      # setting type required for Python2\n      downsample_ratio = self.resolution(mip).astype(np.float32) / self.resolution(to_mip).astype(np.float32)\n      bbox = bbox.astype(np.float64)\n      bbox *= downsample_ratio\n      bbox.minpt = np.floor(bbox.minpt)\n      bbox.maxpt = np.ceil(bbox.maxpt)\n      bbox = bbox.astype(original_dtype)\n      return bbox\n\n    delta = 1 if to_mip >= mip else -1\n    while (mip != to_mip):\n      bbox = one_level(bbox, mip, mip + delta)\n      mip += delta\n\n    return bbox\n\n  def reset_scales(self):\n    """"""Used for manually resetting downsamples if something messed up.""""""\n    self.info[\'scales\'] = self.info[\'scales\'][0:1]\n\n  def add_resolution(self, res, encodig=None, chunk_size=None, info=None):\n    factor = Vec(res, dtype=np.float32) // self.meta.resolution(0)\n    return self.add_scale(factor, encoding, chunk_size, info)\n\n  def add_scale(self, factor, encoding=None, chunk_size=None, info=None):\n    """"""\n    Generate a new downsample scale to for the info file and return an updated dictionary.\n    You\'ll still need to call self.commit_info() to make it permenant.\n\n    Required:\n      factor: int (x,y,z), e.g. (2,2,1) would represent a reduction of 2x in x and y\n\n    Optional:\n      encoding: force new layer to e.g. jpeg or compressed_segmentation\n      chunk_size: force new layer to new chunk size\n\n    Returns: info dict\n    """"""\n    if not info:\n      info = self.info\n\n    # e.g. {""encoding"": ""raw"", ""chunk_sizes"": [[64, 64, 64]], ""key"": ""4_4_40"", \n    # ""resolution"": [4, 4, 40], ""voxel_offset"": [0, 0, 0], \n    # ""size"": [2048, 2048, 256]}\n    fullres = info[\'scales\'][0]\n\n    # If the voxel_offset is not divisible by the ratio,\n    # zooming out will slightly shift the data.\n    # Imagine the offset is 10\n    #    the mip 1 will have an offset of 5\n    #    the mip 2 will have an offset of 2 instead of 2.5 \n    #        meaning that it will be half a pixel to the left\n    if chunk_size is None:\n      chunk_size = lib.find_closest_divisor(fullres[\'chunk_sizes\'][0], closest_to=[64,64,64])\n\n    if encoding is None:\n      encoding = fullres[\'encoding\']\n\n    newscale = {\n      u""encoding"": encoding,\n      u""chunk_sizes"": [ list(map(int, chunk_size)) ],\n      u""resolution"": list(map(int, Vec(*fullres[\'resolution\']) * factor )),\n      u""voxel_offset"": downscale(fullres[\'voxel_offset\'], factor, np.floor),\n      u""size"": downscale(fullres[\'size\'], factor, np.ceil),\n    }\n\n    if newscale[\'encoding\'] == \'compressed_segmentation\':\n      if \'compressed_segmentation_block_size\' in fullres:\n        newscale[\'compressed_segmentation_block_size\'] = fullres[\'compressed_segmentation_block_size\']  \n      else: \n        newscale[\'compressed_segmentation_block_size\'] = (8,8,8)\n\n    newscale[u\'key\'] = str(""_"".join([ str(res) for res in newscale[\'resolution\']]))\n\n    new_res = np.array(newscale[\'resolution\'], dtype=int)\n\n    preexisting = False\n    for index, scale in enumerate(info[\'scales\']):\n      res = np.array(scale[\'resolution\'], dtype=int)\n      if np.array_equal(new_res, res):\n        preexisting = True\n        info[\'scales\'][index] = newscale\n        break\n\n    if not preexisting:    \n      info[\'scales\'].append(newscale)\n\n    return newscale\n\n  def lock_mips(self, mips):\n    """"""\n    Establishes a write lock on the specified mip levels.\n    The lock is written to the cloud info file.\n    """"""\n    mips = lib.toiter(mips)\n    if max(mips) > max(self.available_mips):\n      raise ValueError(""Cannot lock a mip level that doesn\'t exist. Highest mip: {} Got: {}"".format(\n        max(self.available_mips), mips\n      ))\n\n    try:\n      self.refresh_info(force_fetch=True)\n\n      for mip in mips:\n        self.info[\'scales\'][mip][\'locked\'] = True\n\n      self.commit_info()\n    except Exception as err:\n      msg = lib.red(""Unable to acquire write lock on mips {}!"".format(list(mips)))\n      raise exceptions.WriteLockAcquisitionError(msg)\n\n  def unlock_mips(self, mips):\n    """"""\n    Releases a write lock on the specified mip levels.\n    The lock is written to the cloud info file.\n    """"""\n    mips = lib.toiter(mips)\n    if max(mips) > max(self.available_mips):\n      raise ValueError(""Cannot unlock a mip level that doesn\'t exist. Highest mip: {} Got: {}"".format(\n        max(self.available_mips), mips\n      ))\n\n    try:\n      self.refresh_info(force_fetch=True)\n\n      for mip in mips:\n        self.info[\'scales\'][mip][\'locked\'] = False\n\n      self.commit_info()\n    except Exception as err:\n      msg = lib.yellow(""Unable to release lock on mips {}"".format(list(mips)))\n      raise exceptions.WriteLockReleaseError(msg)\n\n  def locked_mips(self):\n    return set([ i for i, scale in enumerate(self.info[\'scales\']) if scale.get(\'locked\', False) ])'"
cloudvolume/datasource/precomputed/mmh3.py,0,"b'\'\'\'\npymmh3 was written by Fredrik Kihlander and enhanced by Swapnil Gusani, and is placed in the public\ndomain. The authors hereby disclaim copyright to this source code.\n\npure python implementation of the murmur3 hash algorithm\n\nhttps://code.google.com/p/smhasher/wiki/MurmurHash3\n\nThis was written for the times when you do not want to compile c-code and install modules,\nand you only want a drop-in murmur3 implementation.\n\nAs this is purely python it is FAR from performant and if performance is anything that is needed\na proper c-module is suggested!\n\nThis module is written to have the same format as mmh3 python package found here for simple conversions:\n\nhttps://pypi.python.org/pypi/mmh3/2.3.1\n\'\'\'\n\nimport sys as _sys\nif (_sys.version_info > (3, 0)):\n    def xrange( a, b, c ):\n        return range( a, b, c )\n    def xencode(x):\n        if isinstance(x, bytes) or isinstance(x, bytearray):\n            return x\n        else:\n            return x.encode()\nelse:\n    def xencode(x):\n        return x\ndel _sys\n\ndef hash( key, seed = 0x0 ):\n    \'\'\' Implements 32bit murmur3 hash. \'\'\'\n\n    key = bytearray( xencode(key) )\n\n    def fmix( h ):\n        h ^= h >> 16\n        h  = ( h * 0x85ebca6b ) & 0xFFFFFFFF\n        h ^= h >> 13\n        h  = ( h * 0xc2b2ae35 ) & 0xFFFFFFFF\n        h ^= h >> 16\n        return h\n\n    length = len( key )\n    nblocks = int( length / 4 )\n\n    h1 = seed\n\n    c1 = 0xcc9e2d51\n    c2 = 0x1b873593\n\n    # body\n    for block_start in xrange( 0, nblocks * 4, 4 ):\n        # ??? big endian?\n        k1 = key[ block_start + 3 ] << 24 | \\\n             key[ block_start + 2 ] << 16 | \\\n             key[ block_start + 1 ] <<  8 | \\\n             key[ block_start + 0 ]\n             \n        k1 = ( c1 * k1 ) & 0xFFFFFFFF\n        k1 = ( k1 << 15 | k1 >> 17 ) & 0xFFFFFFFF # inlined ROTL32\n        k1 = ( c2 * k1 ) & 0xFFFFFFFF\n        \n        h1 ^= k1\n        h1  = ( h1 << 13 | h1 >> 19 ) & 0xFFFFFFFF # inlined ROTL32\n        h1  = ( h1 * 5 + 0xe6546b64 ) & 0xFFFFFFFF\n\n    # tail\n    tail_index = nblocks * 4\n    k1 = 0\n    tail_size = length & 3\n\n    if tail_size >= 3:\n        k1 ^= key[ tail_index + 2 ] << 16\n    if tail_size >= 2:\n        k1 ^= key[ tail_index + 1 ] << 8\n    if tail_size >= 1:\n        k1 ^= key[ tail_index + 0 ]\n    \n    if tail_size > 0:\n        k1  = ( k1 * c1 ) & 0xFFFFFFFF\n        k1  = ( k1 << 15 | k1 >> 17 ) & 0xFFFFFFFF # inlined ROTL32\n        k1  = ( k1 * c2 ) & 0xFFFFFFFF\n        h1 ^= k1\n\n    #finalization\n    unsigned_val = fmix( h1 ^ length )\n    if unsigned_val & 0x80000000 == 0:\n        return unsigned_val\n    else:\n        return -( (unsigned_val ^ 0xFFFFFFFF) + 1 )\n\n\ndef hash128( key, seed = 0x0, x64arch = True ):\n    \'\'\' Implements 128bit murmur3 hash. \'\'\'\n    def hash128_x64( key, seed ):\n        \'\'\' Implements 128bit murmur3 hash for x64. \'\'\'\n\n        def fmix( k ):\n            k ^= k >> 33\n            k  = ( k * 0xff51afd7ed558ccd ) & 0xFFFFFFFFFFFFFFFF\n            k ^= k >> 33\n            k  = ( k * 0xc4ceb9fe1a85ec53 ) & 0xFFFFFFFFFFFFFFFF\n            k ^= k >> 33\n            return k\n\n        length = len( key )\n        nblocks = int( length / 16 )\n\n        h1 = seed\n        h2 = seed\n\n        c1 = 0x87c37b91114253d5\n        c2 = 0x4cf5ad432745937f\n\n        #body\n        for block_start in xrange( 0, nblocks * 8, 8 ):\n            # ??? big endian?\n            k1 = key[ 2 * block_start + 7 ] << 56 | \\\n                 key[ 2 * block_start + 6 ] << 48 | \\\n                 key[ 2 * block_start + 5 ] << 40 | \\\n                 key[ 2 * block_start + 4 ] << 32 | \\\n                 key[ 2 * block_start + 3 ] << 24 | \\\n                 key[ 2 * block_start + 2 ] << 16 | \\\n                 key[ 2 * block_start + 1 ] <<  8 | \\\n                 key[ 2 * block_start + 0 ]\n\n            k2 = key[ 2 * block_start + 15 ] << 56 | \\\n                 key[ 2 * block_start + 14 ] << 48 | \\\n                 key[ 2 * block_start + 13 ] << 40 | \\\n                 key[ 2 * block_start + 12 ] << 32 | \\\n                 key[ 2 * block_start + 11 ] << 24 | \\\n                 key[ 2 * block_start + 10 ] << 16 | \\\n                 key[ 2 * block_start + 9 ] <<  8 | \\\n                 key[ 2 * block_start + 8 ]\n\n            k1  = ( c1 * k1 ) & 0xFFFFFFFFFFFFFFFF\n            k1  = ( k1 << 31 | k1 >> 33 ) & 0xFFFFFFFFFFFFFFFF # inlined ROTL64\n            k1  = ( c2 * k1 ) & 0xFFFFFFFFFFFFFFFF\n            h1 ^= k1\n\n            h1 = ( h1 << 27 | h1 >> 37 ) & 0xFFFFFFFFFFFFFFFF # inlined ROTL64\n            h1 = ( h1 + h2 ) & 0xFFFFFFFFFFFFFFFF\n            h1 = ( h1 * 5 + 0x52dce729 ) & 0xFFFFFFFFFFFFFFFF\n\n            k2  = ( c2 * k2 ) & 0xFFFFFFFFFFFFFFFF\n            k2  = ( k2 << 33 | k2 >> 31 ) & 0xFFFFFFFFFFFFFFFF # inlined ROTL64\n            k2  = ( c1 * k2 ) & 0xFFFFFFFFFFFFFFFF\n            h2 ^= k2\n\n            h2 = ( h2 << 31 | h2 >> 33 ) & 0xFFFFFFFFFFFFFFFF # inlined ROTL64\n            h2 = ( h1 + h2 ) & 0xFFFFFFFFFFFFFFFF\n            h2 = ( h2 * 5 + 0x38495ab5 ) & 0xFFFFFFFFFFFFFFFF\n\n        #tail\n        tail_index = nblocks * 16\n        k1 = 0\n        k2 = 0\n        tail_size = length & 15\n\n        if tail_size >= 15:\n            k2 ^= key[ tail_index + 14 ] << 48\n        if tail_size >= 14:\n            k2 ^= key[ tail_index + 13 ] << 40\n        if tail_size >= 13:\n            k2 ^= key[ tail_index + 12 ] << 32\n        if tail_size >= 12:\n            k2 ^= key[ tail_index + 11 ] << 24\n        if tail_size >= 11:\n            k2 ^= key[ tail_index + 10 ] << 16\n        if tail_size >= 10:\n            k2 ^= key[ tail_index +  9 ] << 8\n        if tail_size >=  9:\n            k2 ^= key[ tail_index +  8 ]\n\n        if tail_size > 8:\n            k2  = ( k2 * c2 ) & 0xFFFFFFFFFFFFFFFF\n            k2  = ( k2 << 33 | k2 >> 31 ) & 0xFFFFFFFFFFFFFFFF # inlined ROTL64\n            k2  = ( k2 * c1 ) & 0xFFFFFFFFFFFFFFFF\n            h2 ^= k2\n\n        if tail_size >=  8:\n            k1 ^= key[ tail_index +  7 ] << 56\n        if tail_size >=  7:\n            k1 ^= key[ tail_index +  6 ] << 48\n        if tail_size >=  6:\n            k1 ^= key[ tail_index +  5 ] << 40\n        if tail_size >=  5:\n            k1 ^= key[ tail_index +  4 ] << 32\n        if tail_size >=  4:\n            k1 ^= key[ tail_index +  3 ] << 24\n        if tail_size >=  3:\n            k1 ^= key[ tail_index +  2 ] << 16\n        if tail_size >=  2:\n            k1 ^= key[ tail_index +  1 ] << 8\n        if tail_size >=  1:\n            k1 ^= key[ tail_index +  0 ]\n\n        if tail_size > 0:\n            k1  = ( k1 * c1 ) & 0xFFFFFFFFFFFFFFFF\n            k1  = ( k1 << 31 | k1 >> 33 ) & 0xFFFFFFFFFFFFFFFF # inlined ROTL64\n            k1  = ( k1 * c2 ) & 0xFFFFFFFFFFFFFFFF\n            h1 ^= k1\n\n        #finalization\n        h1 ^= length\n        h2 ^= length\n\n        h1  = ( h1 + h2 ) & 0xFFFFFFFFFFFFFFFF\n        h2  = ( h1 + h2 ) & 0xFFFFFFFFFFFFFFFF\n\n        h1  = fmix( h1 )\n        h2  = fmix( h2 )\n\n        h1  = ( h1 + h2 ) & 0xFFFFFFFFFFFFFFFF\n        h2  = ( h1 + h2 ) & 0xFFFFFFFFFFFFFFFF\n\n        return ( h2 << 64 | h1 )\n\n    def hash128_x86( key, seed ):\n        \'\'\' Implements 128bit murmur3 hash for x86. \'\'\'\n\n        def fmix( h ):\n            h ^= h >> 16\n            h  = ( h * 0x85ebca6b ) & 0xFFFFFFFF\n            h ^= h >> 13\n            h  = ( h * 0xc2b2ae35 ) & 0xFFFFFFFF\n            h ^= h >> 16\n            return h\n\n        length = len( key )\n        nblocks = int( length / 16 )\n\n        h1 = seed\n        h2 = seed\n        h3 = seed\n        h4 = seed\n\n        c1 = 0x239b961b\n        c2 = 0xab0e9789\n        c3 = 0x38b34ae5\n        c4 = 0xa1e38b93\n\n        #body\n        for block_start in xrange( 0, nblocks * 16, 16 ):\n            k1 = key[ block_start +  3 ] << 24 | \\\n                 key[ block_start +  2 ] << 16 | \\\n                 key[ block_start +  1 ] <<  8 | \\\n                 key[ block_start +  0 ]\n\n            k2 = key[ block_start +  7 ] << 24 | \\\n                 key[ block_start +  6 ] << 16 | \\\n                 key[ block_start +  5 ] <<  8 | \\\n                 key[ block_start +  4 ]\n\n            k3 = key[ block_start + 11 ] << 24 | \\\n                 key[ block_start + 10 ] << 16 | \\\n                 key[ block_start +  9 ] <<  8 | \\\n                 key[ block_start +  8 ]\n\n            k4 = key[ block_start + 15 ] << 24 | \\\n                 key[ block_start + 14 ] << 16 | \\\n                 key[ block_start + 13 ] <<  8 | \\\n                 key[ block_start + 12 ]\n\n            k1  = ( c1 * k1 ) & 0xFFFFFFFF\n            k1  = ( k1 << 15 | k1 >> 17 ) & 0xFFFFFFFF # inlined ROTL32\n            k1  = ( c2 * k1 ) & 0xFFFFFFFF\n            h1 ^= k1\n\n            h1 = ( h1 << 19 | h1 >> 13 ) & 0xFFFFFFFF # inlined ROTL32\n            h1 = ( h1 + h2 ) & 0xFFFFFFFF\n            h1 = ( h1 * 5 + 0x561ccd1b ) & 0xFFFFFFFF\n\n            k2  = ( c2 * k2 ) & 0xFFFFFFFF\n            k2  = ( k2 << 16 | k2 >> 16 ) & 0xFFFFFFFF # inlined ROTL32\n            k2  = ( c3 * k2 ) & 0xFFFFFFFF\n            h2 ^= k2\n\n            h2 = ( h2 << 17 | h2 >> 15 ) & 0xFFFFFFFF # inlined ROTL32\n            h2 = ( h2 + h3 ) & 0xFFFFFFFF\n            h2 = ( h2 * 5 + 0x0bcaa747 ) & 0xFFFFFFFF\n\n            k3  = ( c3 * k3 ) & 0xFFFFFFFF\n            k3  = ( k3 << 17 | k3 >> 15 ) & 0xFFFFFFFF # inlined ROTL32\n            k3  = ( c4 * k3 ) & 0xFFFFFFFF\n            h3 ^= k3\n\n            h3 = ( h3 << 15 | h3 >> 17 ) & 0xFFFFFFFF # inlined ROTL32\n            h3 = ( h3 + h4 ) & 0xFFFFFFFF\n            h3 = ( h3 * 5 + 0x96cd1c35 ) & 0xFFFFFFFF\n\n            k4  = ( c4 * k4 ) & 0xFFFFFFFF\n            k4  = ( k4 << 18 | k4 >> 14 ) & 0xFFFFFFFF # inlined ROTL32\n            k4  = ( c1 * k4 ) & 0xFFFFFFFF\n            h4 ^= k4\n\n            h4 = ( h4 << 13 | h4 >> 19 ) & 0xFFFFFFFF # inlined ROTL32\n            h4 = ( h1 + h4 ) & 0xFFFFFFFF\n            h4 = ( h4 * 5 + 0x32ac3b17 ) & 0xFFFFFFFF\n\n        #tail\n        tail_index = nblocks * 16\n        k1 = 0\n        k2 = 0\n        k3 = 0\n        k4 = 0\n        tail_size = length & 15\n\n        if tail_size >= 15:\n            k4 ^= key[ tail_index + 14 ] << 16\n        if tail_size >= 14:\n            k4 ^= key[ tail_index + 13 ] << 8\n        if tail_size >= 13:\n            k4 ^= key[ tail_index + 12 ]\n\n        if tail_size > 12:\n            k4  = ( k4 * c4 ) & 0xFFFFFFFF\n            k4  = ( k4 << 18 | k4 >> 14 ) & 0xFFFFFFFF # inlined ROTL32\n            k4  = ( k4 * c1 ) & 0xFFFFFFFF\n            h4 ^= k4\n\n        if tail_size >= 12:\n            k3 ^= key[ tail_index + 11 ] << 24\n        if tail_size >= 11:\n            k3 ^= key[ tail_index + 10 ] << 16\n        if tail_size >= 10:\n            k3 ^= key[ tail_index +  9 ] << 8\n        if tail_size >=  9:\n            k3 ^= key[ tail_index +  8 ]\n\n        if tail_size > 8:\n            k3  = ( k3 * c3 ) & 0xFFFFFFFF\n            k3  = ( k3 << 17 | k3 >> 15 ) & 0xFFFFFFFF # inlined ROTL32\n            k3  = ( k3 * c4 ) & 0xFFFFFFFF\n            h3 ^= k3\n\n        if tail_size >= 8:\n            k2 ^= key[ tail_index + 7 ] << 24\n        if tail_size >= 7:\n            k2 ^= key[ tail_index + 6 ] << 16\n        if tail_size >= 6:\n            k2 ^= key[ tail_index + 5 ] << 8\n        if tail_size >= 5:\n            k2 ^= key[ tail_index + 4 ]\n\n        if tail_size > 4:\n            k2  = ( k2 * c2 ) & 0xFFFFFFFF\n            k2  = ( k2 << 16 | k2 >> 16 ) & 0xFFFFFFFF # inlined ROTL32\n            k2  = ( k2 * c3 ) & 0xFFFFFFFF\n            h2 ^= k2\n\n        if tail_size >= 4:\n            k1 ^= key[ tail_index + 3 ] << 24\n        if tail_size >= 3:\n            k1 ^= key[ tail_index + 2 ] << 16\n        if tail_size >= 2:\n            k1 ^= key[ tail_index + 1 ] << 8\n        if tail_size >= 1:\n            k1 ^= key[ tail_index + 0 ]\n\n        if tail_size > 0:\n            k1  = ( k1 * c1 ) & 0xFFFFFFFF\n            k1  = ( k1 << 15 | k1 >> 17 ) & 0xFFFFFFFF # inlined ROTL32\n            k1  = ( k1 * c2 ) & 0xFFFFFFFF\n            h1 ^= k1\n\n        #finalization\n        h1 ^= length\n        h2 ^= length\n        h3 ^= length\n        h4 ^= length\n\n        h1 = ( h1 + h2 ) & 0xFFFFFFFF\n        h1 = ( h1 + h3 ) & 0xFFFFFFFF\n        h1 = ( h1 + h4 ) & 0xFFFFFFFF\n        h2 = ( h1 + h2 ) & 0xFFFFFFFF\n        h3 = ( h1 + h3 ) & 0xFFFFFFFF\n        h4 = ( h1 + h4 ) & 0xFFFFFFFF\n\n        h1 = fmix( h1 )\n        h2 = fmix( h2 )\n        h3 = fmix( h3 )\n        h4 = fmix( h4 )\n\n        h1 = ( h1 + h2 ) & 0xFFFFFFFF\n        h1 = ( h1 + h3 ) & 0xFFFFFFFF\n        h1 = ( h1 + h4 ) & 0xFFFFFFFF\n        h2 = ( h1 + h2 ) & 0xFFFFFFFF\n        h3 = ( h1 + h3 ) & 0xFFFFFFFF\n        h4 = ( h1 + h4 ) & 0xFFFFFFFF\n\n        return ( h4 << 96 | h3 << 64 | h2 << 32 | h1 )\n\n    key = bytearray( xencode(key) )\n\n    if x64arch:\n        return hash128_x64( key, seed )\n    else:\n        return hash128_x86( key, seed )\n\ndef hash64( key, seed = 0x0, x64arch = True ):\n    \'\'\' Implements 64bit murmur3 hash. Returns a tuple. \'\'\'\n\n    hash_128 = hash128( key, seed, x64arch )\n\n    unsigned_val1 = hash_128 & 0xFFFFFFFFFFFFFFFF\n    if unsigned_val1 & 0x8000000000000000 == 0:\n        signed_val1 = unsigned_val1\n    else:\n        signed_val1 = -( (unsigned_val1 ^ 0xFFFFFFFFFFFFFFFF) + 1 )\n\n    unsigned_val2 = ( hash_128 >> 64 ) & 0xFFFFFFFFFFFFFFFF\n    if unsigned_val2 & 0x8000000000000000 == 0:\n        signed_val2 = unsigned_val2\n    else:\n        signed_val2 = -( (unsigned_val2 ^ 0xFFFFFFFFFFFFFFFF) + 1 )\n\n    return ( int( signed_val1 ), int( signed_val2 ) )\n\n\ndef hash_bytes( key, seed = 0x0, x64arch = True ):\n    \'\'\' Implements 128bit murmur3 hash. Returns a byte string. \'\'\'\n\n    hash_128 = hash128( key, seed, x64arch )\n\n    bytestring = \'\'\n\n    for i in xrange(0, 16, 1):\n        lsbyte = hash_128 & 0xFF\n        bytestring = bytestring + str( chr( lsbyte ) )\n        hash_128 = hash_128 >> 8\n\n    return bytestring\n\n\nif __name__ == ""__main__"":\n    import argparse\n    \n    parser = argparse.ArgumentParser( \'pymurmur3\', \'pymurmur [options] ""string to hash""\' )\n    parser.add_argument( \'--seed\', type = int, default = 0 )\n    parser.add_argument( \'strings\', default = [], nargs=\'+\')\n    \n    opts = parser.parse_args()\n    \n    for str_to_hash in opts.strings:\n        sys.stdout.write( \'""%s"" = 0x%08X\\n\' % ( str_to_hash, hash( str_to_hash ) ) )'"
cloudvolume/datasource/precomputed/sharding.py,12,"b'from __future__ import print_function\n\nfrom collections import namedtuple, defaultdict\nimport copy\nimport json\nfrom os.path import basename\nimport struct\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom . import mmh3\nfrom ... import compression\nfrom ...lib import jsonify, toiter\nfrom ...lru import LRU\nfrom ...exceptions import SpecViolation, EmptyFileException\nfrom ...storage import SimpleStorage, GreenStorage, Storage\n\nShardLocation = namedtuple(\'ShardLocation\', \n  (\'shard_number\', \'minishard_number\', \'remainder\')\n)\n\nuint64 = np.uint64\n\nclass ShardingSpecification(object):\n  def __init__(\n    self, type, preshift_bits, \n    hash, minishard_bits, \n    shard_bits, minishard_index_encoding, \n    data_encoding\n  ):\n\n    self.type = type \n    self.preshift_bits = uint64(preshift_bits)\n    self.hash = hash \n    self.minishard_bits = uint64(minishard_bits)\n    self.shard_bits = uint64(shard_bits)\n    self.minishard_index_encoding = minishard_index_encoding\n    self.data_encoding = data_encoding\n\n    self.minishard_mask = self.compute_minishard_mask(self.minishard_bits)\n    self.shard_mask = self.compute_shard_mask(self.shard_bits, self.minishard_bits)              \n\n    self.validate()\n\n  def clone(self):\n    return ShardingSpecification.from_dict(self.to_dict())\n\n  def index_length(self):\n    return int((2 ** self.minishard_bits) * 16)\n\n  @property\n  def hash(self):\n    return self._hash\n\n  @hash.setter\n  def hash(self, val):\n    if val == \'identity\':\n      self.hashfn = lambda x: uint64(x)\n    elif val == \'murmurhash3_x86_128\':\n      self.hashfn = lambda x: uint64(mmh3.hash64(uint64(x).tobytes(), x64arch=False)[0]) \n    else:\n      raise SpecViolation(""hash {} must be either \'identity\' or \'murmurhash3_x86_128\'"".format(val))\n\n    self._hash = val\n\n  @property\n  def preshift_bits(self):\n    return self._preshift_bits\n  \n  @preshift_bits.setter\n  def preshift_bits(self, val):\n    self._preshift_bits = uint64(val) \n\n  @property\n  def shard_bits(self):\n    return self._shard_bits\n  \n  @shard_bits.setter\n  def shard_bits(self, val):\n    self._shard_bits = uint64(val) \n\n  @property\n  def minishard_bits(self):\n    return self._minishard_bits\n  \n  @minishard_bits.setter\n  def minishard_bits(self, val):\n    val = uint64(val)\n    self.minishard_mask = self.compute_minishard_mask(val)\n    self._minishard_bits = uint64(val)\n\n  def compute_minishard_mask(self, val):\n    if val < 0:\n      raise ValueError(str(val) + "" must be greater or equal to than zero."")\n    elif val == 0:\n      return uint64(0)\n\n    minishard_mask = uint64(1)\n    for i in range(val - uint64(1)):\n      minishard_mask <<= uint64(1)\n      minishard_mask |= uint64(1)\n    return uint64(minishard_mask)\n\n  def compute_shard_mask(self, shard_bits, minishard_bits):\n    ones64 = uint64(0xffffffffffffffff)\n    movement = uint64(minishard_bits + shard_bits)\n    shard_mask = ~((ones64 >> movement) << movement)\n    minishard_mask = self.compute_minishard_mask(minishard_bits)\n    return shard_mask & (~minishard_mask)\n\n  @classmethod\n  def from_json(cls, vals):\n    dct = json.loads(vals.decode(\'utf8\'))\n    return cls.from_dict(dct)\n\n  def to_json(self):\n    return jsonify(self.to_dict())\n\n  @classmethod\n  def from_dict(cls, vals):\n    vals = copy.deepcopy(vals)\n    vals[\'type\'] = vals[\'@type\']\n    del vals[\'@type\']\n    return cls(**vals)\n\n  def to_dict(self):\n    return {\n      \'@type\': self.type,\n      \'preshift_bits\': self.preshift_bits,\n      \'hash\': self.hash,\n      \'minishard_bits\': self.minishard_bits,\n      \'shard_bits\': self.shard_bits,\n      \'minishard_index_encoding\': self.minishard_index_encoding,\n      \'data_encoding\': self.data_encoding,\n    }\n\n  def compute_shard_location(self, key):\n    chunkid = uint64(key) >> uint64(self.preshift_bits)\n    chunkid = self.hashfn(chunkid)\n    minishard_number = uint64(chunkid & self.minishard_mask)\n    shard_number = uint64((chunkid & self.shard_mask) >> uint64(self.minishard_bits))\n    shard_number = format(shard_number, \'x\').zfill(int(np.ceil(self.shard_bits / 4.0)))\n    remainder = chunkid >> uint64(self.minishard_bits + self.shard_bits)\n\n    return ShardLocation(shard_number, minishard_number, remainder)\n\n  def synthesize_shards(self, data, progress=False):\n    """"""\n    Given this specification and a comprehensive listing of\n    all the items that could be combined into a given shard,\n    synthesize the shard files for this set of labels.\n\n    data: { label: binary, ... }\n\n    e.g. { 5: b\'...\', 7: b\'...\' }\n\n    Returns: {\n      $filename: binary data,\n    }\n    """"""\n    return synthesize_shard_files(self, data, progress)\n\n  def synthesize_shard(self, labels, progress=False, presorted=False):\n    """"""\n    Assemble a shard file from a group of labels that all belong in the same shard.\n\n    Assembles the .shard file like:\n    [ shard index; minishards; all minishard indices ]\n\n    label_group: \n      If presorted is True:\n        { minishardno: { label: binary, ... }, ... }\n      If presorted is False:\n        { label: binary }\n    progress: show progress bars\n\n    Returns: binary representing a shard file \n    """"""\n    return synthesize_shard_file(self, labels, progress, presorted)\n\n  def validate(self):\n    if self.type not in (\'neuroglancer_uint64_sharded_v1\',):\n      raise SpecViolation(\n        ""@type ({}) must be \'neuroglancer_uint64_sharded_v1\'."" \\\n        .format(self.type)\n      )\n\n    if not (64 > self.preshift_bits >= 0):\n      raise SpecViolation(""preshift_bits must be a whole number less than 64: {}"".format(self.preshift_bits))\n\n    if not (64 >= self.minishard_bits >= 0):\n      raise SpecViolation(""minishard_bits must be between 0 and 64 inclusive: {}"".format(self.minishard_bits))\n\n    if not (64 >= self.shard_bits >= 0):\n      raise SpecViolation(""shard_bits must be between 0 and 64 inclusive: {}"".format(self.shard_bits))\n\n    if self.minishard_bits + self.shard_bits > 64:\n      raise SpecViolation(\n        ""minishard_bits and shard_bits must sum to less than or equal to 64: minishard_bits<{}> + shard_bits<{}> = {}"".format(\n        self.minishard_bits, self.shard_bits, self.minishard_bits + self.shard_bits\n      ))\n\n    if self.hash not in (\'identity\', \'murmurhash3_x86_128\'):\n      raise SpecViolation(""hash {} must be either \'identity\' or \'murmurhash3_x86_128\'"".format(self.hash))\n\n    if self.minishard_index_encoding not in (\'raw\', \'gzip\'):\n      raise SpecViolation(""minishard_index_encoding only supports values \'raw\' or \'gzip\'."")\n\n    if self.data_encoding not in (\'raw\', \'gzip\'):\n      raise SpecViolation(""data_encoding only supports values \'raw\' or \'gzip\'."")\n    \n  def __str__(self):\n    return ""ShardingSpecification::"" + str(self.to_dict())\n\nclass ShardReader(object):\n  def __init__(\n    self, meta, cache, spec,\n    shard_index_cache_size=512,\n    minishard_index_cache_size=128,\n    green=False\n  ):\n    """"""\n    Reads standard Precomputed shard files. \n\n    meta: a PrecomputedMetadata class\n    cache: a CacheService instance\n    spec: a ShardingSpecification instance\n\n    shard_index_cache_size: size of LRU cache for fixed indices \n    minishard_index_cache_size: size of LRU cache for minishard indices\n    """"""\n    self.meta = meta\n    self.cache = cache\n    self.spec = spec\n    self.green = green\n\n    self.shard_index_cache = LRU(shard_index_cache_size)\n    self.minishard_index_cache = LRU(minishard_index_cache_size)\n\n  def get_filename(self, label):\n    return self.compute_shard_location(label)[0]\n\n  def compute_shard_location(self, label):\n    """"""\n    Returns (filename, shard_number) for meshes and skeletons. \n    Images require a different scheme.\n    """"""\n    shard_loc = self.spec.compute_shard_location(label)\n    filename = str(shard_loc.shard_number) + \'.shard\'\n    return (filename, shard_loc.minishard_number)\n\n  def get_index(self, filename, path=""""):\n    """"""\n    Retrieves the shard index which is used for \n    locating the appropriate minishard index.\n\n    Returns: 2^minishard_bits entries of a uint64 \n      array of [[ byte start, byte end ], ... ] \n    """"""\n    indices = self.get_indices([ filename ], path, progress=False)\n    index = list(indices.values())[0]\n    if index is None:\n      raise EmptyFileException(filename + "" was zero bytes."")\n    return index\n\n  def get_indices(self, filenames, path="""", progress=None):\n    """"""\n    For all given files, retrieves the shard index which \n    is used for locating the appropriate minishard indices.\n\n    Returns: { \n      path_to_/filename.shard: 2^minishard_bits entries of a uint64 \n            array of [[ byte start, byte end ], ... ],\n      ...\n    } \n    """"""\n    filenames = toiter(filenames)\n    filenames = [ self.meta.join(path, fname) for fname in filenames ]\n    fufilled = { \n      fname: self.shard_index_cache[fname] \\\n      for fname in filenames \\\n      if fname in self.shard_index_cache  \n    }\n\n    requests = []\n    for fname in filenames:\n      if fname in fufilled:\n        continue\n      requests.append({\n        \'path\': fname, \n        \'local_alias\': fname + \'.index\', \n        \'start\': 0, \n        \'end\': self.spec.index_length(),\n      })\n\n    progress = \'Shard Indices\' if progress else False\n    binaries = self.cache.download_as(requests, progress=progress)\n    for (fname, start, end), content in binaries.items():\n      try:\n        index = self.decode_index(content, fname)\n        self.shard_index_cache[fname] = index\n        fufilled[fname] = index\n      except EmptyFileException:\n        self.shard_index_cache[fname] = None\n        fufilled[fname] = None\n\n    return fufilled\n\n  def decode_index(self, binary, filename=\'Shard\'):\n    if binary is None or len(binary) == 0:\n      raise EmptyFileException(filename + "" was zero bytes."")\n    elif len(binary) != self.spec.index_length():\n      raise SpecViolation(\n        filename + "": shard index was an incorrect length ({}) for this specification ({})."".format(\n          len(binary), self.spec.index_length()\n        ))\n    \n    index = np.frombuffer(binary, dtype=np.uint64)\n    index = index.reshape( (index.size // 2, 2), order=\'C\' )\n    return index + self.spec.index_length()\n\n  def decode_minishard_index(self, minishard_index, filename=\'\'):\n    """"""Returns [[label, offset, size], ... ] where offset and size are in bytes.""""""\n\n    if self.spec.minishard_index_encoding != \'raw\':\n      minishard_index = compression.decompress(\n        minishard_index, encoding=self.spec.minishard_index_encoding, filename=filename\n      )\n\n    minishard_index = np.copy(np.frombuffer(minishard_index, dtype=np.uint64))\n    minishard_index = minishard_index.reshape( (3, len(minishard_index) // 3), order=\'C\' ).T\n\n    for i in range(1, minishard_index.shape[0]):\n      minishard_index[i, 0] += minishard_index[i-1, 0]\n      minishard_index[i, 1] += minishard_index[i-1, 1] + minishard_index[i-1, 2]\n\n    minishard_index[:,1] += self.spec.index_length()\n\n    return minishard_index \n\n  def get_minishard_index(self, filename, index, minishard_no, path=""""):\n    """"""\n    Retrieves the minishard index for a given minishard number.\n\n    Returns: uint64 Nx3 array with multiple rows of [segid, byte start, byte end]\n    """"""\n    res = self.get_minishard_indices(filename, index, minishard_no, path)\n    return res[minishard_no]\n\n  def get_minishard_indices(self, filename, index, minishard_nos, path=""""):\n    """"""\n    Retrieves the minishard indices for a set of minishard numbers.\n\n    Returns: { minishard_no: uint64 Nx3 array of [segid, byte start, byte end], ... }\n    """"""\n    res = self.get_minishard_indices_for_files(( (filename, index, minishard_nos), ), path)\n    return res[filename]\n\n  def get_minishard_indices_for_files(self, requests, path="""", progress=None):\n    """"""\n    Fetches the specified minishard indices for all the specified files\n    at once. This is required to get high performance as opposed to fetching\n    the all minishard indices for a single file.\n\n    requests: iterable of tuples\n      [  (filename, index, minishard_numbers), ... ]\n\n    Returns: map of filename -> minishard numbers -> minishard indices\n\n    e.g. \n    {\n      filename_1: {\n          0: uint64 Nx3 array of [segid, byte start, byte end],\n          1: ...,\n      }\n      filename_2: ...\n    }\n    """"""\n    fufilled_by_filename = defaultdict(dict)\n    msn_map = {}\n\n    download_requests = []\n    for filename, index, minishard_nos in requests:\n      fufilled_requests, pending_requests = self.compute_minishard_index_requests(\n        filename, index, minishard_nos, path\n      ) \n      fufilled_by_filename[filename] = fufilled_requests\n      for msn, start, end in pending_requests:\n        msn_map[(filename, start, end)] = msn\n\n        filepath = self.meta.join(path, filename)\n\n        download_requests.append({\n          \'path\': filepath,\n          \'local_alias\': \'{}-{}.msi\'.format(filepath, msn),\n          \'start\': start,\n          \'end\': end,\n        })\n\n    progress = \'Minishard Indices\' if progress else False\n    results = self.cache.download_as(download_requests, progress=progress)\n  \n    for (filename, start, end), content in results.items():\n      filename = basename(filename)\n      cache_key = (filename, start, end)\n      msn = msn_map[cache_key]\n      minishard_index = self.decode_minishard_index(content, filename)\n      self.minishard_index_cache[cache_key] = minishard_index\n      fufilled_by_filename[filename][msn] = minishard_index\n\n    return fufilled_by_filename\n\n  def compute_minishard_index_requests(self, filename, index, minishard_nos, path=""""):\n    """"""\n    Helper method for get_minishard_indices_for_files. \n    Computes which requests must be made over the network vs can be fufilled from LRU cache.\n    """"""\n    minishard_nos = toiter(minishard_nos)\n\n    if index is None:\n      return ({ msn: None for msn in minishard_nos }, [])\n\n    fufilled_requests = {}\n\n    byte_ranges = {}\n    for msn in minishard_nos:\n      bytes_start, bytes_end = index[msn]\n\n      # most typically: [0,0] for an incomplete shard\n      if bytes_start == bytes_end:\n        fufilled_requests[msn] = None\n        continue\n\n      bytes_start, bytes_end = int(bytes_start), int(bytes_end)\n      byte_ranges[msn] = (bytes_start, bytes_end)\n\n    full_path = self.meta.join(self.meta.cloudpath, path)\n\n    pending_requests = []\n    for msn, (bytes_start, bytes_end) in byte_ranges.items():\n      cache_key = (filename, bytes_start, bytes_end)\n      if cache_key in self.minishard_index_cache:\n        fufilled_requests[msn] = self.minishard_index_cache[cache_key]\n      else:\n        pending_requests.append((msn, bytes_start, bytes_end))\n\n    return (fufilled_requests, pending_requests)\n\n  def exists(self, labels, path="""", return_byte_range=False, progress=None):\n    """"""\n    Checks a shard\'s minishard index for whether a file exists.\n\n    If return_byte_range = False:\n      OUTPUT = SHARD_FILEPATH or None if not exists\n    Else:\n      OUTPUT = [ SHARD_FILEPATH or None, byte_start, num_bytes ]\n\n    Returns:\n      If labels is not an iterable:\n        return OUTPUT\n      Else:\n        return { label_1: OUTPUT, label_2: OUTPUT, ... }\n    """"""\n    return_one = False\n\n    try:\n      iter(labels)\n    except TypeError:\n      return_one = True\n\n    to_labels = defaultdict(list)\n    to_all_labels = defaultdict(list)\n    filename_to_minishard_num = defaultdict(list)\n\n    for label in set(toiter(labels)):\n      filename, minishard_number = self.compute_shard_location(label)\n      to_labels[(filename, minishard_number)].append(label)\n      to_all_labels[filename].append(label)\n      filename_to_minishard_num[filename].append(minishard_number)\n\n    indices = self.get_indices(to_all_labels.keys(), path, progress=progress)\n\n    all_minishards = self.get_minishard_indices_for_files([ \n      (basename(filepath), index, filename_to_minishard_num[basename(filepath)]) \\\n      for filepath, index in indices.items()\n    ], path, progress=progress)\n\n    results = {}\n    for filename, file_minishards in all_minishards.items():\n      filepath = self.meta.join(path, filename)\n      for mini_no, msi in file_minishards.items():\n        labels = to_labels[(filename, mini_no)]\n\n        for label in labels:\n          if msi is None:\n            results[label] = None\n            continue\n\n          idx = np.where(msi[:,0] == label)[0]\n          if len(idx) == 0:\n            results[label] = None\n          else:\n            if return_byte_range:\n              _, offset, size = msi[idx,:][0]\n              results[label] = [ filepath, int(offset), int(size) ]\n            else:\n              results[label] = filepath\n\n    if return_one:\n      return(list(results.values())[0])\n    return results\n\n  def disassemble_shard(self, shard):\n    """"""\n    Given an entire shard as a bytestring, convert \n    it into a dict of { label: byte content }.\n    """"""\n    index = self.decode_index(shard[:self.spec.index_length()])\n    shattered = {}\n    for start, end in index:\n      start, end = int(start), int(end)\n      if start == end:\n        continue\n\n      msi = self.decode_minishard_index(shard[start:end])\n      for label, offset, size in msi:\n        offset, size = int(offset), int(size)\n        binary = shard[offset:offset+size]\n        \n        if self.spec.data_encoding != \'raw\':\n          binary = compression.decompress(binary, encoding=self.spec.data_encoding)\n        \n        shattered[label] = binary\n\n    return shattered\n\n  def get_data(self, label, path="""", progress=None):\n    """"""Fetches data from shards.\n\n    label: one or more segment ids\n    path: subdirectory path\n    progress: display progress bars\n\n    Return: \n      if label is a scalar:\n        a byte string\n      else: (label is an iterable)\n        {\n          label_1: byte string,\n          ....\n        }\n    """"""\n    label, return_multiple = toiter(label, is_iter=True)\n    label = set(( int(l) for l in label))\n\n    results = {}\n    if self.cache.enabled:\n      results = self.cache.get([ \n        self.meta.join(path, str(lbl)) for lbl in label\n      ], progress=progress)\n\n    for cloudpath, content in results.items():\n      if content is None:\n        label.remove(int(basename(cloudpath)))\n\n\n    # { label: [ filename, byte start, num_bytes ] }\n    exists = self.exists(label, path, return_byte_range=True, progress=progress)\n    for k in list(exists.keys()):\n      if exists[k] is None:\n        results[k] = None\n        del exists[k]\n\n    key_label = { (basename(v[0]), v[1], v[2]): k for k,v in exists.items() }\n\n    filenames = ( basename(ext[0]) for ext in exists.values() )\n    starts = ( int(ext[1]) for ext in exists.values() )\n    ends = ( int(ext[1]) + int(ext[2]) for ext in exists.values() )\n\n    full_path = self.meta.join(self.meta.cloudpath, path)\n\n    StorageClass = Storage\n    if self.green:\n      StorageClass = GreenStorage\n    elif len(exists) == 1:\n      StorageClass = SimpleStorage\n\n    with StorageClass(full_path, progress=progress) as stor:\n      remote_files = stor.get_files(filenames, starts=starts, ends=ends)\n\n    binaries = {}\n    for res in remote_files:\n      if res[\'error\']:\n        raise res[\'error\']\n      start, end = res[\'byte_range\']\n      key = (res[\'filename\'], start, end - start)\n      lbl = key_label[key]\n      binaries[lbl] = res[\'content\']\n    del remote_files\n\n    if self.spec.data_encoding != \'raw\':\n      for filepath, binary in tqdm(binaries.items(), desc=""Decompressing"", disable=(not progress)):\n        if binary is None:\n          continue\n        binaries[filepath] = compression.decompress(\n          binary, encoding=self.spec.data_encoding, filename=filepath\n        )\n      \n    if self.cache.enabled:\n      self.cache.put([ \n        (filepath, binary) for filepath, binary in binaries.items()\n      ], progress=progress)\n\n    results.update(binaries)\n\n    if return_multiple:\n      return results\n    return next(iter(results.values()))\n\n  def list_labels(self, filename, path="""", size=False):\n    """"""\n    List all the labels in the index of a given shard file.\n\n    size: (bool) if True, list the size in bytes of each label\n\n    Returns: \n      if not size:\n        np.uint64 array of labels \n      else:\n        [ (label, size in bytes), ... ] in descending order of size\n    """"""\n    index = self.get_index(filename, path)\n    all_minishard_nos = list(range(len(index)))\n    minishard_indicies = self.get_minishard_indices(filename, index, all_minishard_nos, path)\n    minishard_indicies = [  \n      msi for msi in minishard_indicies.values() if msi is not None\n    ]\n    if not size:\n      labels = np.concatenate([  \n        msi[:,0] for msi in minishard_indicies\n      ])\n      return np.sort(labels)\n    else:\n      labels = np.concatenate([  \n        msi[:,:]\n        for msi in minishard_indicies\n      ])\n      labels = [ (row[0], row[2]) for row in labels[:] ]\n      return sorted(labels, key=lambda x: x[1], reverse=True)\n\ndef synthesize_shard_files(spec, data, progress=False):\n  """"""\n  From a set of data guaranteed to constitute one or more\n  complete and comprehensive shards (no partial shards) \n  return a set of files ready for upload.\n\n  WARNING: This function is only appropriate for Precomputed\n  meshes and skeletons. Use the synthesize_shard_file (singular)\n  function to create arbitrarily named and assigned shard files.\n\n  spec: a ShardingSpecification\n  data: { label: binary, ... }\n\n  Returns: { filename: binary, ... }\n  """"""\n  shard_groupings = defaultdict(lambda: defaultdict(dict))\n  pbar = tqdm(\n    data.items(), \n    desc=\'Creating Shard Groupings\', \n    disable=(not progress)\n  )\n\n  for label, binary in pbar:\n    loc = spec.compute_shard_location(label)\n    shard_groupings[loc.shard_number][loc.minishard_number][label] = binary\n\n  shard_files = {}\n\n  pbar = tqdm(\n    shard_groupings.items(), \n    desc=""Synthesizing Shard Files"", \n    disable=(not progress)\n  )\n\n  for shardno, shardgrp in pbar:\n    filename = str(shardno) + \'.shard\'\n    shard_files[filename] = synthesize_shard_file(spec, shardgrp, progress=(progress > 1), presorted=True)\n\n  return shard_files\n\n# NB: This is going to be memory hungry and can be optimized\ndef synthesize_shard_file(spec, label_group, progress=False, presorted=False):\n  """"""\n  Assemble a shard file from a group of labels that all belong in the same shard.\n\n  Assembles the .shard file like:\n  [ shard index; minishards; all minishard indices ]\n\n  spec: ShardingSpecification\n  label_group: \n    If presorted is True:\n      { minishardno: { label: binary, ... }, ... }\n    If presorted is False:\n      { label: binary }\n  progress: show progress bars\n\n  Returns: binary representing a shard file\n  """"""\n  minishardnos = []\n  minishard_indicies = []\n  minishards = []\n\n  if presorted:\n    minishard_mapping = label_group\n  else:\n    minishard_mapping = defaultdict(dict)\n    pbar = tqdm(label_group.items(), disable=(not progress), desc=""Assigning Minishards"")\n    for label, binary in pbar:\n      loc = spec.compute_shard_location(label)\n      minishard_mapping[loc.minishard_number][label] = binary\n\n  del label_group\n\n  for minishardno, minishardgrp in tqdm(minishard_mapping.items(), desc=""Minishard Indices"", disable=(not progress)):\n    labels = sorted([ int(label) for label in minishardgrp.keys() ])\n    if len(labels) == 0:\n      continue\n\n    minishard_index = np.zeros( (3, len(labels)), dtype=np.uint64, order=\'C\')\n    minishard = b\'\'\n    \n    # label and offset are delta encoded\n    last_label = 0\n    for i, label in enumerate(labels):\n      binary = minishardgrp[label]\n      if spec.data_encoding != \'raw\':\n        binary = compression.compress(binary, method=spec.data_encoding)\n\n      minishard_index[0, i] = label - last_label\n      minishard_index[1, i] = 0 # minishard_index[2, i - 1]\n      minishard_index[2, i] = len(binary)\n      minishard += binary\n      last_label = label\n      del minishardgrp[label]\n    \n    minishardnos.append(minishardno)\n    minishard_indicies.append(minishard_index) \n    minishards.append(minishard)\n\n  del minishard_mapping\n\n  cum_minishard_size = 0\n  for idx, minishard in zip(minishard_indicies, minishards):\n    idx[1, 0] = cum_minishard_size\n    cum_minishard_size += len(minishard)\n\n  if progress:\n    print(""Partial assembly of minishard indicies and data... "", end="""", flush=True)\n\n  variable_index_part = [ idx.tobytes(\'C\') for idx in minishard_indicies ]\n  if spec.minishard_index_encoding != \'raw\':\n    variable_index_part = [ \n      compression.compress(idx, method=spec.minishard_index_encoding) \\\n      for idx in variable_index_part \n    ]\n\n  data_part = b\'\'.join(minishards)\n  del minishards\n\n  if progress:\n    print(""Assembled."")\n\n  fixed_index = np.zeros( \n    (int(2 ** spec.minishard_bits), 2), \n    dtype=np.uint64, order=\'C\'\n  )\n\n  start = len(data_part)\n  end = len(data_part)\n  for i, idx in zip(minishardnos, variable_index_part):\n    start = end\n    end += len(idx)\n    fixed_index[i, 0] = start\n    fixed_index[i, 1] = end\n\n  if progress:\n    print(""Final assembly... "", end="""", flush=True)\n\n  # The order here is important. The fixed index must go first because the locations\n  # of the other parts are calculated with it implicitly in front. The variable\n  # index must go last because otherwise compressing it will affect offset of the\n  # data it is attempting to index.\n\n  result = fixed_index.tobytes(\'C\') + data_part + b\'\'.join(variable_index_part) \n\n  if progress:\n    print(""Done."")\n\n  return result\n'"
cloudvolume/datasource/precomputed/spatial_index.py,0,"b'from collections import defaultdict\nimport json\nimport os \n\nimport numpy as np\n\nfrom ...exceptions import SpatialIndexGapError\nfrom ...storage import Storage, SimpleStorage\nfrom ... import paths\nfrom ...lib import Bbox, Vec, xyzrange, min2, toiter\n\nclass SpatialIndex(object):\n  """"""\n  Implements the client side reader of the \n  spatial index. During data generation, the\n  labels in a given task are enumerated and \n  assigned their bounding box as JSON:\n\n  {\n    SEGID: [ x,y,z, x,y,z ],\n    ...\n  }\n\n  The filename is the physical bounding box of the\n  task dot spatial.\n\n  e.g. ""0-1024_0-1024_0-500.spatial"" where the bbox \n  units are nanometers.\n\n  The info file of the data type can then be augmented\n  with:\n\n  {\n    ""spatial_index"": { ""chunk_size"": [ sx, sy, sz ] }\n  }\n\n  Where sx, sy, and sz are given in physical dimensions.\n  """"""\n  def __init__(self, cloudpath, bounds, chunk_size, progress=False):\n    self.cloudpath = cloudpath\n    self.path = paths.extract(cloudpath)\n    self.bounds = Bbox.create(bounds)\n    self.chunk_size = Vec(*chunk_size)\n    self.progress = progress\n\n  def join(self, *paths):\n    if self.path.protocol == \'file\':\n      return os.path.join(*paths)\n    else:\n      return posixpath.join(*paths)    \n\n  def fetch_index_files(self, index_files):\n    with Storage(self.cloudpath, progress=self.progress) as stor:\n      results = stor.get_files(index_files)\n\n    for res in results:\n      if res[\'error\'] is not None:\n        raise SpatialIndexGapError(res[\'error\'])\n\n    return { res[\'filename\']: res[\'content\'] for res in results }\n\n  def index_file_paths_for_bbox(self, bbox):\n    bbox = bbox.expand_to_chunk_size(self.chunk_size, offset=self.bounds.minpt)\n\n    if bbox.subvoxel():\n      return []\n\n    index_files = []\n    for pt in xyzrange(bbox.minpt, bbox.maxpt, self.chunk_size):\n      search = Bbox( pt, min2(pt + self.chunk_size, self.bounds.maxpt) )\n      index_files.append(search.to_filename() + \'.spatial\')\n    \n    return index_files\n\n  def get_bbox(self, label):\n    """"""\n    Given a label, compute an enclosing bounding box for it.\n\n    Returns: Bbox in physical coordinates\n    """"""\n    index_files = self.index_file_paths_for_bbox(self.bounds)\n    index_files = self.fetch_index_files(index_files)\n    locations = defaultdict(list)\n    \n    label = str(label)\n    bbox = None\n    for filename, content in index_files.items():\n      if content is None:\n        if allow_missing:\n          continue\n        else:\n          raise SpatialIndexGapError(filename + "" was not found."")\n\n      segid_bbox_dict = json.loads(content)\n      filename = os.path.basename(filename)\n\n      if label not in segid_bbox_dict: \n        continue \n\n      current_bbox = Bbox.from_list(segid_bbox_dict[label])\n\n      if bbox is None:\n        bbox = current_bbox\n      else:\n        bbox = Bbox.expand(bbox, current_bbox)\n\n    return bbox\n\n  def file_locations_per_label(self, labels=None, allow_missing=False):\n    """"""\n    Queries entire dataset to find which spatial index files the \n    given labels are located in. Can be expensive. If labels is not \n    specified, all labels are fetched.\n\n    Returns: { filename: [ labels... ], ... }\n    """"""\n    if labels is not None:\n      labels = set(toiter(labels))\n      \n    index_files = self.index_file_paths_for_bbox(self.bounds)\n    index_files = self.fetch_index_files(index_files)\n    locations = defaultdict(list)\n    for filename, content in index_files.items():\n      if content is None:\n        if allow_missing:\n          continue\n        else:\n          raise SpatialIndexGapError(filename + "" was not found."")\n\n      segid_bbox_dict = json.loads(content)\n      filename = os.path.basename(filename)\n\n      if labels is None:\n        for label in segid_bbox_dict.keys():\n          locations[int(label)].append(filename)\n      elif len(labels) > len(segid_bbox_dict):\n        for label in segid_bbox_dict.keys():\n          if int(label) in labels:\n            locations[int(label)].append(filename)\n      else:\n        for label in labels:\n          if str(label) in segid_bbox_dict:\n            locations[int(label)].append(filename)\n\n    return locations\n\n  def query(self, bbox, allow_missing=False):\n    """"""\n    For the specified bounding box (or equivalent representation),\n    list all segment ids enclosed within it.\n\n    If allow_missing is set, then don\'t raise an error if an index\n    file is missing.\n\n    Returns: set(labels)\n    """"""\n    bbox = Bbox.create(bbox, context=self.bounds, autocrop=True)\n    original_bbox = bbox.clone()\n    bbox = bbox.expand_to_chunk_size(self.chunk_size, offset=self.bounds.minpt)\n\n    if bbox.subvoxel():\n      return []\n\n    index_files = self.index_file_paths_for_bbox(bbox)\n    results = self.fetch_index_files(index_files)\n\n    labels = set()\n    for filename, content in results.items():\n      if content is None:\n        if allow_missing:\n          continue\n        else:\n          raise SpatialIndexGapError(filename + "" was not found."")\n\n      res = json.loads(content)\n\n      # The bbox test saps performance a lot\n      # but we can skip it if we know 100% that\n      # the labels are going to be inside. This\n      # optimization is important for querying \n      # entire datasets, which is contemplated\n      # for shard generation.\n      if bbox.contains_bbox(self.bounds):\n        for label, label_bbx in res.items():\n          labels.add(int(label))\n      else:\n        for label, label_bbx in res.items():\n          label = int(label)\n          label_bbx = Bbox.from_list(label_bbx)\n\n          if Bbox.intersects(label_bbx, original_bbox):\n            labels.add(label)\n\n    return labels\n\nclass CachedSpatialIndex(SpatialIndex):\n  def __init__(self, cache, cloudpath, bounds, chunk_size, progress=None):\n    self.cache = cache\n    self.subdir = os.path.relpath(cloudpath, cache.meta.cloudpath)\n\n    super(CachedSpatialIndex, self).__init__(\n      cloudpath, bounds, chunk_size, progress\n    )\n\n  def fetch_index_files(self, index_files):\n    index_files = [ self.cache.meta.join(self.subdir, fname) for fname in index_files ]\n    return self.cache.download(index_files, progress=self.progress)\n'"
cloudvolume/datasource/graphene/mesh/__init__.py,0,"b""from .sharded import GrapheneShardedMeshSource\nfrom .unsharded import GrapheneUnshardedMeshSource\nfrom .metadata import GrapheneMeshMetadata\n\nfrom ..metadata import GrapheneMetadata\nfrom ....cacheservice import CacheService\nfrom ....paths import strict_extract\nfrom ....cloudvolume import SharedConfiguration\n\nclass GrapheneMeshSource(object):\n  def __new__(cls, meta, cache, config, readonly=False):\n    mesh_meta = GrapheneMeshMetadata(meta, cache)\n\n    if mesh_meta.is_sharded():\n      return GrapheneShardedMeshSource(mesh_meta, cache, config, readonly) \n\n    return GrapheneUnshardedMeshSource(mesh_meta, cache, config, readonly)\n\n  @classmethod\n  def from_cloudpath(cls, cloudpath, cache=False, progress=False):\n    config = SharedConfiguration(\n      cdn_cache=False,\n      compress=True,\n      compress_level=None,\n      green=False,\n      mip=0,\n      parallel=1,\n      progress=progress,\n    )\n\n    cache = CacheService(\n      cloudpath=(cache if type(cache) == str else cloudpath),\n      enabled=bool(cache),\n      config=config,\n      compress=True,\n    )\n\n    cloudpath, mesh_dir = os.path.split(cloudpath)\n    meta = GrapheneMetadata(cloudpath, cache, info={ 'mesh': mesh_dir })\n\n    return GrapheneMeshSource(meta, cache, config)"""
cloudvolume/datasource/graphene/mesh/metadata.py,0,"b""from ...precomputed.mesh import PrecomputedMeshMetadata\n\nclass GrapheneMeshMetadata(PrecomputedMeshMetadata):\n  def sharding(self, layer_id):\n    return self.info['sharding'][str(layer_id)]"""
cloudvolume/datasource/graphene/mesh/sharded.py,0,"b'import six\n\nfrom collections import defaultdict\nimport itertools\nimport json\nimport os\nimport posixpath\nimport re\nimport requests\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom ....lib import red, toiter, Bbox, Vec\nfrom ....mesh import Mesh\nfrom .... import paths\nfrom ....storage import SimpleStorage, Storage, GreenStorage\n\nfrom ..sharding import GrapheneShardReader\nfrom ...precomputed.sharding import ShardingSpecification\n\nfrom .unsharded import GrapheneUnshardedMeshSource\n\nclass GrapheneShardedMeshSource(GrapheneUnshardedMeshSource):\n  def __init__(self, mesh_meta, cache, config, readonly):\n    super(GrapheneShardedMeshSource, self).__init__(mesh_meta, cache, config, readonly)\n\n    self.readers = {}\n    for level, sharding in self.meta.info[\'sharding\'].items(): # { level: std sharding, ... }\n      spec = ShardingSpecification.from_dict(sharding)\n      self.readers[int(level)] = GrapheneShardReader(self.meta, self.cache, spec)\n\n  def initial_path(self, level):\n    return self.meta.join(self.meta.mesh_path, \'initial\', str(level))\n\n  def dynamic_path(self):\n    return self.meta.join(self.meta.mesh_path, \'dynamic\')\n\n  # 1. determine if the segid is before or after the shard time point\n  # 2. assuming it is sharded, fetch the draco encoded file from the\n  #    correct level\n\n  def dynamic_exists(self, labels, progress=None):\n    """"""\n    Checks for dynamic mesh existence.\n  \n    Returns: { label: path or None, ... }\n    """"""\n    labels = toiter(labels)\n\n    checks = [ self.compute_filename(label) for label in labels ]\n    \n    cloudpath = self.meta.join(self.meta.meta.cloudpath, self.meta.mesh_path, \'dynamic\') \n    StorageClass = GreenStorage if self.config.green else Storage\n    progress = progress if progress is not None else self.config.progress\n\n    with StorageClass(cloudpath, progress=progress) as stor:\n      results = stor.files_exist(checks)\n\n    output = {}\n    for filepath, exists in results.items():\n      label = int(os.path.basename(filepath)[:-2]) # strip :0\n      output[label] = filepath if exists else None\n\n    return output\n\n  def initial_exists(self, labels, return_byte_range=False, progress=None):\n    """"""\n    Checks for initial mesh existence.\n  \n    Returns: \n      If return_byte_range:\n        { label: [ path, byte offset, byte size ] or None, ... }\n      Else:\n        { label: path or None, ... }\n    """"""\n    labels = toiter(labels)\n    progress = progress if progress is not None else self.config.progress\n\n    layers = defaultdict(list)\n    for label in labels:\n      if label == 0:\n        continue\n      layer = self.meta.meta.decode_layer_id(label)\n      layers[layer].append(label)\n\n    all_results = {}\n    for layer, layer_labels in layers.items():\n      path = self.initial_path(layer)\n      results = self.readers[int(layer)].exists(\n        layer_labels, \n        path=path, \n        return_byte_range=return_byte_range, \n        progress=progress\n      )\n      all_results.update(results)\n\n    return all_results\n\n  def exists(self, labels, progress=None):\n    """"""\n    Checks dynamic then initial meshes for existence.\n\n    Returns: { label: path or None, ... }\n    """"""\n    labels = toiter(labels)\n    labels = set(labels)\n\n    dynamic_labels = self.dynamic_exists(labels, progress)\n    remainder_labels = set([ label for label, path in dynamic_labels.items() if path ])\n\n    initial_labels = self.initial_exists(remainder_labels, progress=progress, return_byte_range=False)\n\n    dynamic_labels.update(initial_labels)\n    return dynamic_labels\n\n  def parse_manifest_filenames(self, filenames):\n    lists = defaultdict(list)\n    initial_regexp = re.compile(r\'~(\\d+)/([\\d\\-]+\\.shard):(\\d+):(\\d+)\')\n\n    for filename in filenames:\n      if not filename:\n        continue\n\n      # eg. ~2/344239114-0.shard:224659:442 \n      # tilde means initial, missing tilde means dynamic\n      initial = filename[0] == \'~\'\n\n      if initial:\n        (layer_id, parsed_filename, byte_start, size) = re.search(\n          initial_regexp, filename\n        ).groups()\n        lists[\'initial\'].append((layer_id, parsed_filename, int(byte_start), int(size)))\n      else:\n        lists[\'dynamic\'].append(filename)\n        \n    return lists\n\n  def get_meshes_via_manifest_byte_offsets(self, seg_id, bounding_box):\n    """"""    \n    The manifest for sharded is a bit strange in that exists(..., return_byte_offset=True)\n    is being called on the server side. To avoid duplicative delay by recomputing the offset\n    locations, the manifest breaks encapsulation by returning the shard filename and byte\n    offsets. This breaks enapsulation of the shard fetching logic rather severely but \n    it is probably worth it.\n    """"""\n    level = self.meta.meta.decode_layer_id(seg_id)\n    dynamic_cloudpath = self.meta.join(self.meta.meta.cloudpath, self.dynamic_path())\n    StorageClass = GreenStorage if self.config.green else Storage\n\n    filenames = self.get_fragment_filenames(seg_id, level=level, bbox=bounding_box)\n    lists = self.parse_manifest_filenames(filenames)\n\n    files = []\n    if lists[\'dynamic\']:\n      with StorageClass(dynamic_cloudpath) as stor:\n        files = stor.get_files(lists[\'dynamic\'])\n\n    meshes = [ \n      f[\'content\'] for f in files \n    ]\n\n    filenames = []\n    starts = []\n    ends = []\n    for layer_id, filename, byte_start, size in lists[\'initial\']:\n      filenames.append(self.meta.join(layer_id, filename))\n      starts.append(byte_start)\n      ends.append(byte_start + size)\n\n    cloudpath = self.meta.join(self.meta.meta.cloudpath, self.meta.mesh_path, \'initial\')\n\n    raw_binaries = []\n    \n    with StorageClass(cloudpath) as stor:\n      initial_meshes = stor.get_files(filenames, starts, ends)\n\n    meshes += initial_meshes\n\n    return [ Mesh.from_draco(mesh[\'content\']) for mesh in meshes ]\n\n  def get_meshes_via_manifest_labels(self, seg_id, bounding_box):\n    level = self.meta.meta.decode_layer_id(seg_id)\n    labels = self.get_fragment_labels(seg_id, level=level, bbox=bounding_box)\n    meshes = self.get_meshes_on_bypass(labels)\n    return list(meshes.values())\n\n  def get_meshes_via_manifest(self, seg_id, bounding_box, use_byte_offsets):\n    if use_byte_offsets:\n      return self.get_meshes_via_manifest_byte_offsets(seg_id, bounding_box)\n    return self.get_meshes_via_manifest_labels(seg_id, bounding_box)\n\n  def get_meshes_on_bypass(self, segids):\n    """"""\n    Attempt to fetch a mesh directly from storage without going through\n    the chunk graph server. This capability should only be used in special\n    circumstances.\n    """"""\n    segids = toiter(segids)\n    StorageClass = GreenStorage if self.config.green else Storage\n    dynamic_cloudpath = self.meta.join(self.meta.meta.cloudpath, self.dynamic_path())\n    filenames = [ self.compute_filename(segid) for segid in segids ]\n    with StorageClass(dynamic_cloudpath, progress=self.config.progress) as stor:\n      raw_binaries = stor.get_files(filenames)\n\n    # extract the label ID from the mesh manifest.\n    # e.g. 387463568301300850:0:24576-25088_17920-18432_2048-3072\n    label_regexp = re.compile(r\'(\\d+):\\d:[\\d_-]+$\')\n\n    output = {}\n    remaining = []\n    for res in raw_binaries:\n      if res[\'error\']:\n        raise res[\'error\']\n\n      (label,) = re.search(label_regexp, res[\'filename\']).groups()\n      label = int(label)\n\n      if res[\'content\'] is None:\n        remaining.append(label)\n      else:\n        output[label] = res[\'content\']\n\n    layers = defaultdict(list)\n    for segid in remaining:\n      layer_id = self.meta.meta.decode_layer_id(segid)\n      layers[layer_id].append(segid)\n\n    for layer_id, labels in layers.items():\n      subdirectory = self.meta.join(self.meta.mesh_path, \'initial\', str(layer_id))\n      initial_output = self.readers[layer_id].get_data(labels, path=subdirectory, progress=self.config.progress)\n      for label, raw_binary in initial_output.items():\n        if raw_binary is None:\n          raise IndexError(\'No mesh found for segment {}\'.format(label))\n      output.update(initial_output)\n\n    return { label: Mesh.from_draco(raw_binary) for label, raw_binary in output.items() }\n\n  def download_segid(self, seg_id, bounding_box, bypass=False, use_byte_offsets=True):    \n    """"""See GrapheneUnshardedMeshSource.get for the user facing function.""""""\n    level = self.meta.meta.decode_layer_id(seg_id)\n    if level not in self.readers:\n      raise KeyError(""There is no shard configuration in the mesh info file for level {}."".format(level))\n\n    if bypass:\n      mesh = self.get_meshes_on_bypass(seg_id)[seg_id]\n    else:\n      meshes = self.get_meshes_via_manifest(seg_id, bounding_box, use_byte_offsets=use_byte_offsets)\n      mesh = Mesh.concatenate(*meshes)\n\n    if mesh is None:\n      raise IndexError(\'No mesh found for segment {}\'.format(seg_id))\n\n    mesh.segid = seg_id\n    return mesh, True\n'"
cloudvolume/datasource/graphene/mesh/unsharded.py,0,"b'import six\n\nfrom collections import defaultdict\nimport itertools\nimport json\nimport os\nimport posixpath\nimport re\nimport requests\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom ....lib import red, toiter, Bbox, Vec, jsonify\nfrom ....mesh import Mesh\nfrom .... import paths\nfrom ....storage import Storage, GreenStorage\nfrom ....scheduler import schedule_jobs\n\nfrom ...precomputed.mesh import UnshardedLegacyPrecomputedMeshSource, PrecomputedMeshMetadata\n\n\nclass GrapheneUnshardedMeshSource(UnshardedLegacyPrecomputedMeshSource):\n\n  def compute_filename(self, label):\n    layer_id = self.meta.meta.decode_layer_id(label)\n    chunk_block_shape = 2 * Vec(*self.meta.meta.mesh_chunk_size)\n    start = self.meta.meta.decode_chunk_position(label)\n    start *= chunk_block_shape\n    bbx = Bbox(start, start + chunk_block_shape)\n    return ""{}:0:{}"".format(label, bbx.to_filename())\n\n  def exists(self, labels, progress=None):\n    """"""\n    Checks for dynamic mesh existence.\n  \n    Returns: { label: boolean, ... }\n    """"""\n    labels = toiter(labels)\n    filenames = [\n      self.compute_filename(label) for label in labels\n    ]\n\n    cloudpath = self.meta.join(self.meta.cloudpath, self.meta.mesh_path)\n    with Storage(cloudpath) as stor:\n      return stor.files_exist(filenames)\n\n  def get_fragment_labels(self, segid, lod=0, level=2, bbox=None, bypass=False):\n    if bypass:\n      return [ segid ]\n\n    manifest = self.fetch_manifest(segid, lod, level, bbox, return_segids=True)\n    return manifest[""seg_ids""]\n\n  def get_fragment_filenames(self, segid, lod=0, level=2, bbox=None, bypass=False):\n    if bypass:\n      return [ self.compute_filename(segid) ]\n\n    manifest = self.fetch_manifest(segid, lod, level, bbox)\n    return manifest[""fragments""]\n\n  def fetch_manifest(self, segid, lod=0, level=2, bbox=None, return_segids=False):\n    # TODO: add lod to endpoint\n    query_d = {\n      \'verify\': True,\n    }\n    if return_segids:\n      query_d[\'return_seg_ids\'] = 1\n\n    if bbox is not None:\n      bbox = Bbox.create(bbox)\n      query_d[\'bounds\'] = bbox.to_filename()\n\n    url = ""%s/%s:%s"" % (self.meta.meta.manifest_endpoint, segid, lod)\n    if level is not None:\n      res = requests.get(\n        url,\n        data=jsonify({ ""start_layer"": level }),\n        params=query_d,\n        headers=self.meta.meta.auth_header\n      )\n    else:\n      res = requests.get(url, params=query_d, headers=self.meta.meta.auth_header)\n\n    res.raise_for_status()\n\n    return json.loads(res.content.decode(\'utf8\'))\n\n  def download_segid(self, seg_id, bounding_box, bypass, use_byte_offsets=True):\n    """"""\n    Download a mesh for a single segment ID.\n\n    seg_id: Download the mesh for this segid.\n    bounding_box: Limit the query for child meshes to this bounding box.\n    bypass: Don\'t fetch the manifest, precompute the filename instead. Use this\n      only when you know the actual mesh labels in advance.\n    use_byte_offsets: Applicable only for the sharded format. Reuse the byte_offsets\n      into the sharded format that the server precalculated to accelerate download.\n      A time when you might want to switch this off is when you\'re working on a new\n      meshing job with different sharding parameters but are keeping the existing \n      meshes for visualization while it runs.\n    allow_missing: If set to True, return None if segid missing. If set to False, throw\n      an error.\n    """"""\n    import DracoPy\n\n    level = self.meta.meta.decode_layer_id(seg_id)\n    fragment_filenames = self.get_fragment_filenames(\n      seg_id, level=level, bbox=bounding_box, bypass=bypass\n    )\n    fragments = self._get_mesh_fragments(fragment_filenames)\n    fragments = sorted(fragments, key=lambda frag: frag[0])  # make decoding deterministic\n\n    fragiter = tqdm(fragments, disable=(not self.config.progress), desc=""Decoding Mesh Buffer"")\n    is_draco = False\n    for i, (filename, frag) in enumerate(fragiter):\n      mesh = None\n      \n      if frag is not None:\n        try:\n          # Easier to ask forgiveness than permission\n          mesh = Mesh.from_draco(frag)\n          is_draco = True\n        except DracoPy.FileTypeException:\n          mesh = Mesh.from_precomputed(frag)\n          \n      fragments[i] = mesh\n    \n    fragments = [ f for f in fragments if f is not None ] \n    if len(fragments) == 0:\n      raise IndexError(\'No mesh fragments found for segment {}\'.format(seg_id))\n\n    mesh = Mesh.concatenate(*fragments)\n    mesh.segid = seg_id\n    return mesh, is_draco\n\n  def get(\n      self, segids, \n      remove_duplicate_vertices=False, \n      fuse=False, bounding_box=None,\n      bypass=False, use_byte_offsets=True,\n      deduplicate_chunk_boundaries=True,\n      allow_missing=False,\n    ):\n    """"""\n    Merge fragments derived from these segids into a single vertex and face list.\n\n    Why merge multiple segids into one mesh? For example, if you have a set of\n    segids that belong to the same neuron.\n\n    segid: (iterable or int) segids to render into a single mesh\n\n    Optional:\n      remove_duplicate_vertices: bool, fuse exactly matching vertices within a chunk\n      fuse: bool, merge all downloaded meshes into a single mesh\n      bounding_box: Bbox, bounding box to restrict mesh download to\n      bypass: bypass requesting the manifest and attempt to get the \n        segids from storage directly by testing the dynamic and then the initial mesh. \n        This is an exceptional usage of this tool and should be applied only with \n        an understanding of what that entails.\n      use_byte_offsets: For sharded volumes, we can use the output of \n        exists(..., return_byte_offsets) that the server already did in order\n        to skip having to query the sharded format again.\n      deduplicate_chunk_boundaries: Our meshing is done in chunks and creates duplicate vertices\n        at the boundaries of chunks. This parameter will automatically deduplicate these if set\n        to True. Superceded by remove_duplicate_vertices.\n      allow_missing: If set to True, missing segids will be ignored. If set to False, an error\n        is thrown.\n    \n    Returns: Mesh object if fused, else { segid: Mesh, ... }\n    """"""\n    segids = list(set([ int(segid) for segid in toiter(segids) ]))\n\n    meta = self.meta.meta\n\n    meshes = []\n    for seg_id in tqdm(segids, disable=(not self.config.progress), desc=""Downloading Meshes""):\n      level = meta.decode_layer_id(seg_id)\n      if allow_missing:\n        try:\n          mesh, is_draco = self.download_segid(\n            seg_id, bounding_box, bypass, use_byte_offsets\n          )\n        except IndexError:\n          continue\n      else:\n        mesh, is_draco = self.download_segid(\n          seg_id, bounding_box, bypass, use_byte_offsets\n        )\n      resolution = meta.resolution(self.config.mip)\n      if meta.chunks_start_at_voxel_offset:\n        offset = meta.voxel_offset(self.config.mip)\n      else:\n        offset = Vec(0,0,0)\n\n      if remove_duplicate_vertices:\n        mesh = mesh.consolidate()\n      elif is_draco:\n        if not deduplicate_chunk_boundaries:\n          pass\n        elif level == 2:\n          # Deduplicate at quantized lvl2 chunk borders\n          draco_grid_size = meta.get_draco_grid_size(level)\n          mesh = mesh.deduplicate_chunk_boundaries(\n            meta.mesh_chunk_size * resolution,\n            offset=offset * resolution,\n            is_draco=True,\n            draco_grid_size=draco_grid_size,\n          )\n        else:\n          # TODO: cyclic draco quantization to properly\n          # stitch and deduplicate draco meshes at variable\n          # levels (see github issue #299)\n          print(\'Warning: deduplication not currently supported for this layer\\\'s variable layered draco meshes\')\n      elif deduplicate_chunk_boundaries:\n        mesh = mesh.deduplicate_chunk_boundaries(\n            meta.mesh_chunk_size * resolution,\n            offset=offset * resolution,\n            is_draco=False,\n          )\n      \n      meshes.append(mesh)\n\n    if not fuse:\n      return { m.segid: m for m in meshes }\n\n    return Mesh.concatenate(*meshes).consolidate()\n\n'"
cloudvolume/datasource/precomputed/image/__init__.py,3,"b'""""""\nThe Precomputed format is a neuroscience imaging format \ndesigned for cloud storage. The specification is located\nhere:\n\nhttps://github.com/google/neuroglancer/tree/master/src/neuroglancer/datasource/precomputed\n\nThis datasource contains the code for manipulating images.\n""""""\nimport itertools\nimport uuid\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom cloudvolume import lib, exceptions\nfrom ....lib import Bbox, Vec\nfrom .... import sharedmemory\nfrom ....storage import Storage\n\nfrom ... import autocropfn, readonlyguard, ImageSourceInterface\nfrom .. import sharding\nfrom .common import chunknames\nfrom . import tx, rx\n\nclass PrecomputedImageSource(ImageSourceInterface):\n  def __init__(\n    self, config, meta, cache,\n    autocrop=False, bounded=True,\n    non_aligned_writes=False,\n    fill_missing=False, \n    delete_black_uploads=False,\n    background_color=0,\n    readonly=False,\n  ):\n    self.config = config\n    self.meta = meta \n    self.cache = cache \n\n    self.autocrop = bool(autocrop)\n    self.bounded = bool(bounded)\n    self.fill_missing = bool(fill_missing)\n    self.non_aligned_writes = bool(non_aligned_writes)\n    self.readonly = bool(readonly)\n    \n    self.delete_black_uploads = bool(delete_black_uploads)\n    self.background_color = background_color\n\n    self.shared_memory_id = self.generate_shared_memory_location()\n\n  def generate_shared_memory_location(self):\n    return \'precomputed-shm-\' + str(uuid.uuid4())\n\n  def unlink_shared_memory(self):\n    """"""Unlink the current shared memory location from the filesystem.""""""\n    return sharedmemory.unlink(self.shared_memory_id)\n\n  def check_bounded(self, bbox, mip):\n    if self.bounded and not self.meta.bounds(mip).contains_bbox(bbox):\n      raise exceptions.OutOfBoundsError(""""""\n        Requested cutout not contained within dataset bounds.\n\n        Cloudpath: {}\n        Requested: {}\n        Bounds: {}\n        Mip: {}\n        Resolution: {}\n\n        Set bounded=False to disable this warning.\n      """""".format(\n          self.meta.cloudpath, \n          bbox, self.meta.bounds(mip), \n          mip, self.meta.resolution(mip)\n        )\n      )\n\n  def download(\n      self, bbox, mip, parallel=1, \n      location=None, retain=False,\n      use_shared_memory=False, use_file=False,\n      order=\'F\'\n    ):\n    """"""\n    Download a cutout image from the dataset.\n\n    bbox: a Bbox object describing what region to download\n    mip: which resolution to fetch, 0 is the highest resolution\n    parallel: how many processes to use for downloading \n    location: if using shared memory or downloading to a file,\n      which file location should be used?\n    retain: don\'t delete the shared memory file after download\n      completes\n    use_shared_memory: download to a shared memory location. \n      This enables efficient inter-process communication and\n      efficient parallel operation. mutually exclusive with\n      use_file.\n    use_file: download image directly to a file named by location. \n      mutually exclusive with use_shared_memory. \n    order: The underlying shared memory or file buffer can use either\n      C or Fortran order for storing a multidimensional array.\n\n    Returns: 4d ndarray\n    """"""\n\n    if self.autocrop:\n      bbox = Bbox.intersection(bbox, self.meta.bounds(mip))\n\n    self.check_bounded(bbox, mip)\n\n    if location is None:\n      location = self.shared_memory_id\n\n    scale = self.meta.scale(mip)\n    if \'sharding\' in scale:\n      spec = sharding.ShardingSpecification.from_dict(scale[\'sharding\'])\n      return rx.download_sharded(\n        bbox, mip, \n        self.meta, self.cache, spec,\n        compress=self.config.compress,\n        progress=self.config.progress,\n        fill_missing=self.fill_missing,\n        order=order,\n      )\n    else:\n      return rx.download(\n        bbox, mip, \n        meta=self.meta,\n        cache=self.cache,\n        parallel=parallel,\n        location=location,\n        retain=retain,\n        use_shared_memory=use_shared_memory,\n        use_file=use_file,\n        fill_missing=self.fill_missing,\n        progress=self.config.progress,\n        compress=self.config.compress,\n        order=order,\n        green=self.config.green,\n      )\n\n  @readonlyguard\n  def upload(\n      self, \n      image, offset, mip, \n      parallel=1,\n      location=None, location_bbox=None, order=\'F\',\n      use_shared_memory=False, use_file=False      \n    ):\n\n    if mip in self.meta.locked_mips():\n      raise exceptions.ReadOnlyException(\n        ""MIP {} is currently write locked. If this should not be the case, run vol.meta.unlock_mip({})."".format(\n          mip, mip\n        )\n      )\n\n    offset = Vec(*offset)\n    bbox = Bbox( offset, offset + Vec(*image.shape[:3]) )\n\n    self.check_bounded(bbox, mip)\n\n    if self.autocrop:\n      image, bbox = autocropfn(self.meta, image, bbox, mip)\n      offset = bbox.minpt\n\n    if location is None:\n      location = self.shared_memory_id\n\n    return tx.upload(\n      self.meta, self.cache,\n      image, offset, mip,\n      compress=self.config.compress,\n      compress_level=self.config.compress_level,\n      cdn_cache=self.config.cdn_cache,\n      parallel=parallel, \n      progress=self.config.progress,\n      location=location, \n      location_bbox=location_bbox,\n      location_order=order,\n      use_shared_memory=use_shared_memory,\n      use_file=use_file,\n      delete_black_uploads=self.delete_black_uploads,\n      background_color=self.background_color,\n      non_aligned_writes=self.non_aligned_writes,\n      green=self.config.green,\n      fill_missing=self.fill_missing, # applies only to unaligned writes\n    )\n\n  def exists(self, bbox, mip=None):\n    if mip is None:\n      mip = self.config.mip\n\n    bbox = Bbox.create(bbox, self.meta.bounds(mip), bounded=True)\n    realized_bbox = bbox.expand_to_chunk_size(\n      self.meta.chunk_size(mip), offset=self.meta.voxel_offset(mip)\n    )\n    realized_bbox = Bbox.clamp(realized_bbox, self.meta.bounds(mip))\n\n    cloudpaths = list(chunknames(\n      realized_bbox, self.meta.bounds(mip), \n      self.meta.key(mip), self.meta.chunk_size(mip),\n      protocol=self.meta.path.protocol\n    ))\n\n    with Storage(self.meta.cloudpath, progress=self.config.progress) as storage:\n      existence_report = storage.files_exist(cloudpaths)\n    return existence_report\n\n  @readonlyguard\n  def delete(self, bbox, mip=None):\n    if mip is None:\n      mip = self.config.mip\n\n    if mip in self.meta.locked_mips():\n      raise exceptions.ReadOnlyException(\n        ""MIP {} is currently write locked. If this should not be the case, run vol.meta.unlock_mip({})."".format(\n          mip, mip\n        )\n      )\n\n    bbox = Bbox.create(bbox, self.meta.bounds(mip), bounded=True)\n    realized_bbox = bbox.expand_to_chunk_size(\n      self.meta.chunk_size(mip), offset=self.meta.voxel_offset(mip)\n    )\n    realized_bbox = Bbox.clamp(realized_bbox, self.meta.bounds(mip))\n\n    if bbox != realized_bbox:\n      raise exceptions.AlignmentError(\n        ""Unable to delete non-chunk aligned bounding boxes. Requested: {}, Realized: {}"".format(\n        bbox, realized_bbox\n      ))\n\n    cloudpaths = list(chunknames(\n      realized_bbox, self.meta.bounds(mip),\n      self.meta.key(mip), self.meta.chunk_size(mip),\n      protocol=self.meta.path.protocol\n    ))\n\n    with Storage(self.meta.cloudpath, progress=self.config.progress) as storage:\n      storage.delete_files(cloudpaths)\n\n    if self.cache.enabled:\n      with Storage(\'file://\' + self.cache.path, progress=self.config.progress) as storage:\n        storage.delete_files(cloudpaths)\n\n\n  def transfer_to(self, cloudpath, bbox, mip, block_size=None, compress=True, compress_level=None):\n    """"""\n    Transfer files from one storage location to another, bypassing\n    volume painting. This enables using a single CloudVolume instance\n    to transfer big volumes. In some cases, gsutil or aws s3 cli tools\n    may be more appropriate. This method is provided for convenience. It\n    may be optimized for better performance over time as demand requires.\n\n    cloudpath (str): path to storage layer\n    bbox (Bbox object): ROI to transfer\n    mip (int): resolution level\n    block_size (int): number of file chunks to transfer per I/O batch.\n    compress (bool): Set to False to upload as uncompressed\n    """"""\n    from cloudvolume import CloudVolume\n\n    if mip is None:\n      mip = self.config.mip\n\n    bbox = Bbox.create(bbox, self.meta.bounds(mip))\n    realized_bbox = bbox.expand_to_chunk_size(\n      self.meta.chunk_size(mip), offset=self.meta.voxel_offset(mip)\n    )\n    realized_bbox = Bbox.clamp(realized_bbox, self.meta.bounds(mip))\n\n    if bbox != realized_bbox:\n      raise exceptions.AlignmentError(\n        ""Unable to transfer non-chunk aligned bounding boxes. Requested: {}, Realized: {}"".format(\n          bbox, realized_bbox\n        ))\n\n    default_block_size_MB = 50 # MB\n    chunk_MB = self.meta.chunk_size(mip).rectVolume() * np.dtype(self.meta.dtype).itemsize * self.meta.num_channels\n    if self.meta.layer_type == \'image\':\n      # kind of an average guess for some EM datasets, have seen up to 1.9x and as low as 1.1\n      # affinites are also images, but have very different compression ratios. e.g. 3x for kempressed\n      chunk_MB /= 1.3 \n    else: # segmentation\n      chunk_MB /= 100.0 # compression ratios between 80 and 800....\n    chunk_MB /= 1024.0 * 1024.0\n\n    if block_size:\n      step = block_size\n    else:\n      step = int(default_block_size_MB // chunk_MB) + 1\n\n    try:\n      destvol = CloudVolume(cloudpath, mip=mip)\n    except exceptions.InfoUnavailableError: \n      destvol = CloudVolume(cloudpath, mip=mip, info=self.meta.info, provenance=self.meta.provenance.serialize())\n      destvol.commit_info()\n      destvol.commit_provenance()\n    except exceptions.ScaleUnavailableError:\n      destvol = CloudVolume(cloudpath)\n      for i in range(len(destvol.scales) + 1, len(self.meta.scales)):\n        destvol.scales.append(\n          self.meta.scales[i]\n        )\n      destvol.commit_info()\n      destvol.commit_provenance()\n\n    num_blocks = np.ceil(self.meta.bounds(mip).volume() / self.meta.chunk_size(mip).rectVolume()) / step\n    num_blocks = int(np.ceil(num_blocks))\n\n    cloudpaths = chunknames(\n      bbox, self.meta.bounds(mip), \n      self.meta.key(mip), self.meta.chunk_size(mip),\n      protocol=self.meta.path.protocol\n    )\n\n    pbar = tqdm(\n      desc=\'Transferring Blocks of {} Chunks\'.format(step), \n      unit=\'blocks\', \n      disable=(not self.config.progress),\n      total=num_blocks,\n    )\n\n    with pbar:\n      with Storage(self.meta.cloudpath) as src_stor:\n        with Storage(cloudpath) as dest_stor:\n          for _ in range(num_blocks, 0, -1):\n            srcpaths = list(itertools.islice(cloudpaths, step))\n            files = src_stor.get_files(srcpaths)\n            files = [ (f[\'filename\'], f[\'content\']) for f in files ]\n            dest_stor.put_files(\n              files=files, \n              compress=compress, \n              compress_level=compress_level,\n              content_type=tx.content_type(destvol),\n            )\n            pbar.update()\n'"
cloudvolume/datasource/precomputed/image/common.py,9,"b'import concurrent.futures\nimport copy\nfrom functools import partial\nimport itertools\nimport json\nimport math\nimport multiprocessing as mp\nimport os\nimport posixpath\nimport signal\n\nimport numpy as np\n\nfrom ....lib import xyzrange, min2, max2, Vec, Bbox\nfrom .... import sharedmemory as shm\n\n# Used in sharedmemory to emulate shared memory on \n# OS X using a file, which has that facility but is \n# more limited than on Linux.\nfs_lock = mp.Lock()\n\ndef parallel_execution(fn, items, parallel, cleanup_shm=None):\n  def cleanup(signum, frame):\n    if cleanup_shm:\n      shm.unlink(cleanup_shm)\n\n  prevsigint = signal.getsignal(signal.SIGINT)\n  prevsigterm = signal.getsignal(signal.SIGTERM)\n\n  signal.signal(signal.SIGINT, cleanup)\n  signal.signal(signal.SIGTERM, cleanup)\n\n  with concurrent.futures.ProcessPoolExecutor(max_workers=parallel) as executor:\n    executor.map(fn, items)\n\n  signal.signal(signal.SIGINT, prevsigint)\n  signal.signal(signal.SIGTERM, prevsigterm)\n\ndef chunknames(bbox, volume_bbox, key, chunk_size, protocol=None):\n  path = posixpath if protocol != \'file\' else os.path\n\n  for x,y,z in xyzrange( bbox.minpt, bbox.maxpt, chunk_size ):\n    highpt = min2(Vec(x,y,z) + chunk_size, volume_bbox.maxpt)\n    filename = ""{}-{}_{}-{}_{}-{}"".format(\n      x, highpt.x,\n      y, highpt.y, \n      z, highpt.z\n    )\n    yield path.join(key, filename)\n\ndef gridpoints(bbox, volume_bbox, chunk_size):\n  """"""\n  Consider a volume as divided into a grid with the \n  first chunk labeled 1, the second 2, etc. \n\n  Return the grid x,y,z coordinates of a cutout as a\n  sequence.\n  """"""\n  chunk_size = Vec(*chunk_size)\n\n  grid_size = np.ceil(volume_bbox.size3() / chunk_size).astype(np.int32)\n  cutout_grid_size = np.ceil(bbox.size3() / chunk_size).astype(np.int32)\n  cutout_grid_offset = np.ceil((bbox.minpt - volume_bbox.minpt) / chunk_size).astype(np.int32)\n\n  grid_cutout = Bbox( cutout_grid_offset, cutout_grid_offset + cutout_grid_size )\n\n  for x,y,z in xyzrange( grid_cutout.minpt, grid_cutout.maxpt, (1,1,1) ):\n    yield Vec(x,y,z)\n\ndef compressed_morton_code(gridpt, grid_size):\n  code = np.uint64(0)\n  num_bits = int(max([ np.ceil(np.log2(size)) for size in grid_size ]))\n  j = np.uint64(0)\n  one = np.uint64(1)\n\n  for i in range(num_bits):\n    for dim in range(3):\n      if 2 ** i <= grid_size[dim]:\n        bit = (((np.uint64(gridpt[dim]) >> np.uint64(i)) & one) << j)\n        code |= bit\n        j += one\n  \n  return code\n\ndef shade(dest_img, dest_bbox, src_img, src_bbox):\n  """"""\n  Shade dest_img at coordinates dest_bbox using the\n  image contained in src_img at coordinates src_bbox.\n\n  The buffer will only be painted in the overlapping\n  region of the content.\n\n  Returns: void\n  """"""\n  if not Bbox.intersects(dest_bbox, src_bbox):\n    return\n\n  spt = max2(src_bbox.minpt, dest_bbox.minpt)\n  ept = min2(src_bbox.maxpt, dest_bbox.maxpt)\n  dbox = Bbox(spt, ept) - dest_bbox.minpt\n\n  ZERO3 = Vec(0,0,0)\n  istart = max2(spt - src_bbox.minpt, ZERO3)\n  iend = min2(ept - src_bbox.maxpt, ZERO3) + src_img.shape[:3]\n  sbox = Bbox(istart, iend)\n\n  while src_img.ndim < 4:\n    src_img = src_img[..., np.newaxis]\n  \n  dest_img[ dbox.to_slices() ] = src_img[ sbox.to_slices() ]\n\n'"
cloudvolume/datasource/precomputed/image/rx.py,3,"b'from functools import partial\nimport itertools\nimport math\nimport os\n\nimport numpy as np\nfrom six.moves import range\nfrom tqdm import tqdm\n\nfrom ....exceptions import EmptyVolumeException, EmptyFileException\nfrom ....lib import (  \n  mkdir, clamp, xyzrange, Vec, \n  Bbox, min2, max2, check_bounds, \n  jsonify, red\n)\nfrom .... import chunks\n\nfrom cloudvolume.scheduler import schedule_jobs\nfrom cloudvolume.storage import SimpleStorage, reset_connection_pools\nfrom cloudvolume.threaded_queue import DEFAULT_THREADS\nfrom cloudvolume.volumecutout import VolumeCutout\n\nimport cloudvolume.sharedmemory as shm\n\nfrom ..common import should_compress, content_type\nfrom .common import (\n  fs_lock, parallel_execution, \n  chunknames, shade, gridpoints,\n  compressed_morton_code\n)\n\nfrom .. import sharding\n\ndef download_sharded(\n    requested_bbox, mip,\n    meta, cache, spec,\n    compress, progress,\n    fill_missing, \n    order\n  ):\n\n  full_bbox = requested_bbox.expand_to_chunk_size(\n    meta.chunk_size(mip), offset=meta.voxel_offset(mip)\n  )\n  full_bbox = Bbox.clamp(full_bbox, meta.bounds(mip))\n  shape = list(requested_bbox.size3()) + [ meta.num_channels ]\n  compress_cache = should_compress(meta.encoding(mip), compress, cache, iscache=True)\n\n  chunk_size = meta.chunk_size(mip)\n  grid_size = np.ceil(meta.bounds(mip).size3() / chunk_size).astype(np.uint32)\n\n  reader = sharding.ShardReader(meta, cache, spec)\n  bounds = meta.bounds(mip)\n\n  renderbuffer = np.zeros(shape=shape, dtype=meta.dtype, order=order)\n\n  gpts = list(gridpoints(full_bbox, bounds, chunk_size))\n\n  code_map = {}\n  for gridpoint in gpts:\n    zcurve_code = compressed_morton_code(gridpoint, grid_size)\n    cutout_bbox = Bbox(\n      bounds.minpt + gridpoint * chunk_size,\n      min2(bounds.minpt + (gridpoint + 1) * chunk_size, bounds.maxpt)\n    )\n    code_map[zcurve_code] = cutout_bbox\n\n  all_chunkdata = reader.get_data(list(code_map.keys()), meta.key(mip), progress=progress)\n  for zcode, chunkdata in all_chunkdata.items():\n    cutout_bbox = code_map[zcode]\n    if chunkdata is None:\n      if fill_missing:\n        chunkdata = None\n      else:\n        raise EmptyVolumeException(cutout_bbox)\n\n    img3d = decode(\n      meta, cutout_bbox, \n      chunkdata, fill_missing, mip\n    )\n\n    shade(renderbuffer, requested_bbox, img3d, cutout_bbox)\n\n  return VolumeCutout.from_volume(\n    meta, mip, renderbuffer, \n    requested_bbox\n  )\n\ndef download(\n    requested_bbox, mip, \n    meta, cache,\n    fill_missing, progress,\n    parallel, location, \n    retain, use_shared_memory, \n    use_file, compress, order=\'F\',\n    green=False\n  ):\n  """"""Cutout a requested bounding box from storage and return it as a numpy array.""""""\n  \n  full_bbox = requested_bbox.expand_to_chunk_size(\n    meta.chunk_size(mip), offset=meta.voxel_offset(mip)\n  )\n  full_bbox = Bbox.clamp(full_bbox, meta.bounds(mip))\n  cloudpaths = list(chunknames(\n    full_bbox, meta.bounds(mip), \n    meta.key(mip), meta.chunk_size(mip), \n    protocol=meta.path.protocol\n  ))\n  shape = list(requested_bbox.size3()) + [ meta.num_channels ]\n\n  compress_cache = should_compress(meta.encoding(mip), compress, cache, iscache=True)\n\n  handle = None\n\n  if use_shared_memory and use_file:\n    raise ValueError(""use_shared_memory and use_file are mutually exclusive arguments."")\n\n  if parallel == 1:\n    if use_shared_memory: # write to shared memory\n      handle, renderbuffer = shm.ndarray(\n        shape, dtype=meta.dtype, order=order,\n        location=location, lock=fs_lock\n      )\n      if not retain:\n        shm.unlink(location)\n    elif use_file: # write to ordinary file\n      handle, renderbuffer = shm.ndarray_fs(\n        shape, dtype=meta.dtype, order=order,\n        location=location, lock=fs_lock,\n        emulate_shm=False\n      )\n      if not retain:\n        os.unlink(location)\n    else:\n      renderbuffer = np.zeros(shape=shape, dtype=meta.dtype, order=order)\n\n    def process(img3d, bbox):\n      shade(renderbuffer, requested_bbox, img3d, bbox)\n\n    download_chunks_threaded(\n      meta, cache, mip, cloudpaths, \n      fn=process, fill_missing=fill_missing,\n      progress=progress, compress_cache=compress_cache, \n      green=green \n    )\n  else:\n    handle, renderbuffer = multiprocess_download(\n      requested_bbox, mip, cloudpaths,\n      meta, cache, compress_cache,\n      fill_missing, progress,\n      parallel, location, retain, \n      use_shared_memory=(use_file == False),\n      order=order,\n      green=green,\n    )\n  \n  return VolumeCutout.from_volume(\n    meta, mip, renderbuffer, \n    requested_bbox, handle=handle\n  )\n\ndef multiprocess_download(\n    requested_bbox, mip, cloudpaths,\n    meta, cache, compress_cache,\n    fill_missing, progress,\n    parallel, location, \n    retain, use_shared_memory, order,\n    green\n  ):\n\n  cloudpaths_by_process = []\n  length = int(math.ceil(len(cloudpaths) / float(parallel)) or 1)\n  for i in range(0, len(cloudpaths), length):\n    cloudpaths_by_process.append(\n      cloudpaths[i:i+length]\n    )\n\n  cpd = partial(child_process_download, \n    meta, cache, mip, compress_cache, \n    requested_bbox, \n    fill_missing, progress,\n    location, use_shared_memory,\n    green\n  )\n  parallel_execution(cpd, cloudpaths_by_process, parallel, cleanup_shm=location)\n\n  shape = list(requested_bbox.size3()) + [ meta.num_channels ]\n\n  if use_shared_memory:\n    mmap_handle, renderbuffer = shm.ndarray(\n      shape, dtype=meta.dtype, order=order, \n      location=location, lock=fs_lock\n    )\n  else:\n    handle, renderbuffer = shm.ndarray_fs(\n      shape, dtype=meta.dtype, order=order,\n      location=location, lock=fs_lock,\n      emulate_shm=False\n    )    \n\n  if not retain:\n    if use_shared_memory:\n      shm.unlink(location)\n    else:\n      os.unlink(location)\n\n  return mmap_handle, renderbuffer\n\ndef child_process_download(\n    meta, cache, mip, compress_cache, \n    dest_bbox, \n    fill_missing, progress,\n    location, use_shared_memory, green,\n    cloudpaths\n  ):\n  reset_connection_pools() # otherwise multi-process hangs\n\n  shape = list(dest_bbox.size3()) + [ meta.num_channels ]\n\n  if use_shared_memory:\n    array_like, dest_img = shm.ndarray(\n      shape, dtype=meta.dtype, \n      location=location, lock=fs_lock\n    )\n  else:\n    array_like, dest_img = shm.ndarray_fs(\n      shape, dtype=meta.dtype, \n      location=location, emulate_shm=False, \n      lock=fs_lock\n    )\n\n  def process(src_img, src_bbox):\n    shade(dest_img, dest_bbox, src_img, src_bbox)\n\n  download_chunks_threaded(\n    meta, cache, mip, cloudpaths,\n    fn=process, fill_missing=fill_missing,\n    progress=progress, compress_cache=compress_cache,\n    green=green\n  )\n\n  array_like.close()\n\ndef download_chunk(\n    meta, cache, \n    cloudpath, mip,\n    filename, fill_missing,\n    enable_cache, compress_cache\n  ):\n  with SimpleStorage(cloudpath) as stor:\n    content = stor.get_file(filename)\n\n  if enable_cache:\n    with SimpleStorage(\'file://\' + cache.path) as stor:\n      stor.put_file(\n        file_path=filename, \n        content=(content or b\'\'), \n        content_type=content_type(meta.encoding(mip)), \n        compress=compress_cache,\n      )\n\n  bbox = Bbox.from_filename(filename) # possible off by one error w/ exclusive bounds\n  img3d = decode(meta, filename, content, fill_missing, mip)\n  return img3d, bbox\n\ndef download_chunks_threaded(\n    meta, cache, mip, cloudpaths, fn, \n    fill_missing, progress, compress_cache,\n    green=False\n  ):\n  locations = cache.compute_data_locations(cloudpaths)\n  cachedir = \'file://\' + os.path.join(cache.path, meta.key(mip))\n\n  def process(cloudpath, filename, enable_cache):\n    img3d, bbox = download_chunk(\n      meta, cache, cloudpath, mip,\n      filename, fill_missing,\n      enable_cache, compress_cache\n    )\n    fn(img3d, bbox)\n\n  local_downloads = ( \n    partial(process, cachedir, os.path.basename(filename), False) for filename in locations[\'local\'] \n  )\n  remote_downloads = ( \n    partial(process, meta.cloudpath, filename, cache.enabled) for filename in locations[\'remote\'] \n  )\n\n  downloads = itertools.chain( local_downloads, remote_downloads )\n\n  schedule_jobs(\n    fns=downloads, \n    concurrency=DEFAULT_THREADS, \n    progress=(\'Downloading\' if progress else None),\n    total=len(cloudpaths),\n    green=green,\n  )\n\ndef decode(meta, input_bbox, content, fill_missing, mip):\n  """"""\n  Decode content from bytes into a numpy array using the \n  dataset metadata.\n\n  If fill_missing is True, return a zeroed array if \n  content is empty. Otherwise, raise an EmptyVolumeException\n  in that case.\n\n  Returns: ndarray\n  """"""\n  bbox = Bbox.create(input_bbox)\n  content_len = len(content) if content is not None else 0\n\n  if not content:\n    if fill_missing:\n      content = \'\'\n    else:\n      raise EmptyVolumeException(input_bbox)\n\n  shape = list(bbox.size3()) + [ meta.num_channels ]\n\n  try:\n    return chunks.decode(\n      content, \n      encoding=meta.encoding(mip), \n      shape=shape, \n      dtype=meta.dtype, \n      block_size=meta.compressed_segmentation_block_size(mip),\n    )\n  except Exception as error:\n    print(red(\'File Read Error: {} bytes, {}, {}, errors: {}\'.format(\n        content_len, bbox, input_bbox, error)))\n    raise\n\n'"
cloudvolume/datasource/precomputed/image/tx.py,5,"b'from functools import partial\nimport os\n\nimport numpy as np\nfrom six.moves import range\nfrom tqdm import tqdm\n\nfrom cloudvolume import lib, chunks\nfrom cloudvolume.exceptions import AlignmentError\nfrom cloudvolume.lib import ( \n  mkdir, clamp, xyzrange, Vec, \n  Bbox, min2, max2\n)\nfrom cloudvolume.scheduler import schedule_jobs\nfrom cloudvolume.storage import Storage, SimpleStorage, reset_connection_pools\nfrom cloudvolume.threaded_queue import ThreadedQueue, DEFAULT_THREADS\nfrom cloudvolume.volumecutout import VolumeCutout\n\nimport cloudvolume.sharedmemory as shm\n\nfrom ... import check_grid_aligned\nfrom .common import (\n  fs_lock, parallel_execution, chunknames, \n  shade\n) \nfrom ..common import (\n  content_type, cdn_cache_control,\n  should_compress\n)\nfrom .rx import download_chunks_threaded\n\ndef upload(\n    meta, cache,\n    image, offset, mip,\n    compress=None,\n    compress_level=None,\n    cdn_cache=None,\n    parallel=1,\n    progress=False,\n    delete_black_uploads=False, \n    background_color=0,\n    non_aligned_writes=False,\n    location=None, location_bbox=None, location_order=\'F\',\n    use_shared_memory=False, use_file=False,\n    green=False, fill_missing=False\n  ):\n  """"""Upload img to vol with offset. This is the primary entry point for uploads.""""""\n\n  if not np.issubdtype(image.dtype, np.dtype(meta.dtype).type):\n    raise ValueError(""""""\n      The uploaded image data type must match the volume data type. \n\n      Volume: {}\n      Image: {}\n      """""".format(meta.dtype, image.dtype)\n    )\n\n  shape = Vec(*image.shape)[:3]\n  offset = Vec(*offset)[:3]\n  bounds = Bbox( offset, shape + offset)\n\n  is_aligned = check_grid_aligned(\n    meta, image, bounds, mip, \n    throw_error=(non_aligned_writes == False)\n  )\n\n  options = {\n    ""compress"": compress,\n    ""compress_level"": compress_level,\n    ""cdn_cache"": cdn_cache,\n    ""parallel"": parallel, \n    ""progress"": progress,\n    ""location"": location, \n    ""location_bbox"": location_bbox,\n    ""location_order"": location_order,\n    ""use_shared_memory"": use_shared_memory,\n    ""use_file"": use_file,\n    ""delete_black_uploads"": delete_black_uploads,\n    ""background_color"": background_color,\n    ""green"": green,  \n  }\n\n  if is_aligned:\n    upload_aligned(\n      meta, cache, \n      image, offset, mip,\n      **options\n    )\n    return\n\n  # Upload the aligned core\n  expanded = bounds.expand_to_chunk_size(meta.chunk_size(mip), meta.voxel_offset(mip))\n  retracted = bounds.shrink_to_chunk_size(meta.chunk_size(mip), meta.voxel_offset(mip))\n  core_bbox = retracted.clone() - bounds.minpt\n\n  if not core_bbox.subvoxel():\n    core_img = image[ core_bbox.to_slices() ] \n    upload_aligned(\n      meta, cache, \n      core_img, retracted.minpt, mip,\n      **options\n    )\n\n  # Download the shell, paint, and upload\n  all_chunks = set(chunknames(expanded, meta.bounds(mip), meta.key(mip), meta.chunk_size(mip)))\n  core_chunks = set(chunknames(retracted, meta.bounds(mip), meta.key(mip), meta.chunk_size(mip)))\n  shell_chunks = all_chunks.difference(core_chunks)\n\n  def shade_and_upload(img3d, bbox):\n    # decode is returning non-writable chunk\n    # we\'re throwing them away so safe to write\n    img3d.setflags(write=1) \n    shade(img3d, bbox, image, bounds)\n    threaded_upload_chunks(\n      meta, cache, \n      img3d, mip,\n      (( Vec(0,0,0), Vec(*img3d.shape[:3]), bbox.minpt, bbox.maxpt),), \n      compress=compress, cdn_cache=cdn_cache,\n      progress=progress, n_threads=0, \n      delete_black_uploads=delete_black_uploads,\n      green=green,\n    )\n\n  compress_cache = should_compress(meta.encoding(mip), compress, cache, iscache=True)\n\n  download_chunks_threaded(\n    meta, cache, mip, shell_chunks, fn=shade_and_upload,\n    fill_missing=fill_missing, progress=progress, \n    compress_cache=compress_cache,\n    green=green\n  )\n\ndef upload_aligned(\n    meta, cache,\n    img, offset, mip,\n    compress=None,\n    compress_level=None,\n    cdn_cache=None,\n    progress=False,\n    parallel=1, \n    location=None, \n    location_bbox=None, \n    location_order=\'F\', \n    use_shared_memory=False,\n    use_file=False,\n    delete_black_uploads=False,\n    background_color=0,\n    green=False,\n  ):\n  global fs_lock\n\n  chunk_ranges = list(generate_chunks(meta, img, offset, mip))\n\n  if parallel == 1:\n    threaded_upload_chunks(\n      meta, cache, \n      img, mip, chunk_ranges, \n      progress=progress,\n      compress=compress, cdn_cache=cdn_cache,\n      delete_black_uploads=delete_black_uploads,\n      background_color=background_color,\n      green=green, compress_level=compress_level,\n    )\n    return\n\n  length = (len(chunk_ranges) // parallel) or 1\n  chunk_ranges_by_process = []\n  for i in range(0, len(chunk_ranges), length):\n    chunk_ranges_by_process.append(\n      chunk_ranges[i:i+length]\n    )\n\n  # use_shared_memory means use a predetermined\n  # shared memory location, not no shared memory \n  # at all.\n  if not use_shared_memory:\n    array_like, renderbuffer = shm.ndarray(\n      shape=img.shape, dtype=img.dtype, \n      location=location, order=location_order, \n      lock=fs_lock\n    )\n    renderbuffer[:] = img\n\n  cup = partial(child_upload_process, \n    meta, cache, \n    img.shape, offset, mip,\n    compress, cdn_cache, progress,\n    location, location_bbox, location_order, \n    delete_black_uploads, background_color, \n    green, compress_level=compress_level\n  )\n\n  parallel_execution(cup, chunk_ranges_by_process, parallel, cleanup_shm=location)\n\n  # If manual mode is enabled, it\'s the \n  # responsibilty of the user to clean up\n  if not use_shared_memory:\n    array_like.close()\n    shm.unlink(location)\n\ndef child_upload_process(\n    meta, cache, \n    img_shape, offset, mip,\n    compress, cdn_cache, progress,\n    location, location_bbox, location_order, \n    delete_black_uploads, background_color,\n    green, chunk_ranges, compress_level=None,\n  ):\n  global fs_lock\n  reset_connection_pools()\n\n  shared_shape = img_shape\n  if location_bbox:\n    shared_shape = list(location_bbox.size3()) + [ meta.num_channels ]\n\n  array_like, renderbuffer = shm.ndarray(\n    shape=shared_shape, \n    dtype=meta.dtype, \n    location=location, \n    order=location_order, \n    lock=fs_lock, \n    readonly=True\n  )\n\n  if location_bbox:\n    cutout_bbox = Bbox( offset, offset + img_shape[:3] )\n    delta_box = cutout_bbox.clone() - location_bbox.minpt\n    renderbuffer = renderbuffer[ delta_box.to_slices() ]\n\n  threaded_upload_chunks(\n    meta, cache, \n    renderbuffer, mip, chunk_ranges, \n    compress=compress, cdn_cache=cdn_cache, progress=progress,\n    delete_black_uploads=delete_black_uploads, \n    background_color=background_color,\n    green=green, compress_level=compress_level,\n  )\n  array_like.close()\n\ndef threaded_upload_chunks(\n    meta, cache, \n    img, mip, chunk_ranges, \n    compress, cdn_cache, progress,\n    n_threads=DEFAULT_THREADS,\n    delete_black_uploads=False,\n    background_color=0,\n    green=False,\n    compress_level=None,\n  ):\n  \n  if cache.enabled:\n    mkdir(cache.path)\n\n  while img.ndim < 4:\n    img = img[ ..., np.newaxis ]\n\n  remotestor = lambda: SimpleStorage(meta.cloudpath, progress=progress)\n  localstor = lambda: SimpleStorage(\'file://\' + cache.path, progress=progress)\n\n  def do_upload(imgchunk, cloudpath):\n    encoded = chunks.encode(imgchunk, meta.encoding(mip), meta.compressed_segmentation_block_size(mip))\n\n    with remotestor() as cloudstorage:\n      cloudstorage.put_file(\n        file_path=cloudpath, \n        content=encoded,\n        content_type=content_type(meta.encoding(mip)), \n        compress=should_compress(meta.encoding(mip), compress, cache),\n        compress_level=compress_level,\n        cache_control=cdn_cache_control(cdn_cache),\n      )\n\n    if cache.enabled:\n      with localstor() as cachestorage:\n        cachestorage.put_file(\n          file_path=cloudpath,\n          content=encoded, \n          content_type=content_type(meta.encoding(mip)), \n          compress=should_compress(meta.encoding(mip), compress, cache, iscache=True)\n        )\n\n  def do_delete(cloudpath):\n    with remotestor() as cloudstorage:\n      cloudstorage.delete_file(cloudpath)\n    \n    if cache.enabled:\n      with localstor() as cachestorage:\n        cachestorage.delete_file(cloudpath)\n\n  def process(startpt, endpt, spt, ept):\n    if np.array_equal(spt, ept):\n      return\n\n    imgchunk = img[ startpt.x:endpt.x, startpt.y:endpt.y, startpt.z:endpt.z, : ]\n\n    # handle the edge of the dataset\n    clamp_ept = min2(ept, meta.bounds(mip).maxpt)\n    newept = clamp_ept - spt\n    imgchunk = imgchunk[ :newept.x, :newept.y, :newept.z, : ]\n\n    filename = ""{}-{}_{}-{}_{}-{}"".format(\n      spt.x, clamp_ept.x,\n      spt.y, clamp_ept.y, \n      spt.z, clamp_ept.z\n    )\n\n    cloudpath = meta.join(meta.key(mip), filename)\n\n    if delete_black_uploads:\n      if np.any(imgchunk != background_color):\n        do_upload(imgchunk, cloudpath)\n      else:\n        do_delete(cloudpath)\n    else:\n      do_upload(imgchunk, cloudpath)\n\n  schedule_jobs(\n    fns=( partial(process, *vals) for vals in chunk_ranges ), \n    concurrency=DEFAULT_THREADS, \n    progress=(\'Uploading\' if progress else None),\n    total=len(chunk_ranges),\n    green=green,\n  )\n\ndef generate_chunks(meta, img, offset, mip):\n  shape = Vec(*img.shape)[:3]\n  offset = Vec(*offset)[:3]\n\n  bounds = Bbox( offset, shape + offset)\n\n  alignment_check = bounds.round_to_chunk_size(meta.chunk_size(mip), meta.voxel_offset(mip))\n\n  if not np.all(alignment_check.minpt == bounds.minpt):\n    raise AlignmentError(""""""\n      Only chunk aligned writes are supported by this function. \n\n      Got:             {}\n      Volume Offset:   {} \n      Nearest Aligned: {}\n    """""".format(\n      bounds, meta.voxel_offset(mip), alignment_check)\n    )\n\n  bounds = Bbox.clamp(bounds, meta.bounds(mip))\n\n  img_offset = bounds.minpt - offset\n  img_end = Vec.clamp(bounds.size3() + img_offset, Vec(0,0,0), shape)\n\n  for startpt in xyzrange( img_offset, img_end, meta.chunk_size(mip) ):\n    startpt = startpt.clone()\n    endpt = min2(startpt + meta.chunk_size(mip), shape)\n    spt = (startpt + bounds.minpt).astype(int)\n    ept = (endpt + bounds.minpt).astype(int)\n    yield (startpt, endpt, spt, ept)'"
cloudvolume/datasource/precomputed/mesh/__init__.py,0,"b""import os\n\nfrom .metadata import PrecomputedMeshMetadata\nfrom .unsharded import UnshardedLegacyPrecomputedMeshSource\nfrom .multilod import ShardedMultiLevelPrecomputedMeshSource, UnshardedMultiLevelPrecomputedMeshSource\n\n\nfrom ..metadata import PrecomputedMetadata\nfrom ....cacheservice import CacheService\nfrom ....exceptions import UnsupportedFormatError\nfrom ....lib import red\nfrom ....paths import strict_extract\nfrom ....cloudvolume import SharedConfiguration\n\nclass PrecomputedMeshSource(object):\n  def __new__(cls, meta, cache, config, readonly=False):\n    mesh_meta = PrecomputedMeshMetadata(meta, cache)\n    if mesh_meta.info.get('@type', None) == 'neuroglancer_multilod_draco':\n      if mesh_meta.info['sharding']['@type'] == 'neuroglancer_uint64_sharded_v1':\n        # Sharded storage of multi-resolution mesh fragment data\n        return ShardedMultiLevelPrecomputedMeshSource(mesh_meta, cache, config, readonly) \n      else:\n        # Unsharded storage of multi-resolution mesh fragment data\n        return UnshardedMultiLevelPrecomputedMeshSource()\n  \n    # Legacy single-resolution mesh format\n    return UnshardedLegacyPrecomputedMeshSource(mesh_meta, cache, config, readonly)\n\n  @classmethod\n  def from_cloudpath(cls, cloudpath, cache=False, progress=False):\n    config = SharedConfiguration(\n      cdn_cache=False,\n      compress=True,\n      compress_level=None,\n      green=False,\n      mip=0,\n      parallel=1,\n      progress=progress,\n    )\n\n    cache = CacheService(\n      cloudpath=(cache if type(cache) == str else cloudpath),\n      enabled=bool(cache),\n      config=config,\n      compress=True,\n    )\n\n    cloudpath, mesh_dir = os.path.split(cloudpath)\n    meta = PrecomputedMetadata(cloudpath, cache, info={ 'mesh': mesh_dir })\n\n    return PrecomputedMeshSource(meta, cache, config)\n\n\n"""
cloudvolume/datasource/precomputed/mesh/metadata.py,0,"b""import re\n\nfrom ....lib import jsonify\n\nimport numpy as np\n\nMESH_MIP_REGEXP = re.compile(r'mesh_mip_(\\d+)')\n\nclass PrecomputedMeshMetadata(object):\n  def __init__(self, meta, cache=None, info=None):\n    self.meta = meta\n    self.cache = cache\n\n    if info:\n      self.info = info\n    else:\n      self.info = self.fetch_info()\n\n  @property\n  def chunk_size(self):\n    if 'chunk_size' in self.info:\n      return self.info['chunk_size']\n    return None\n\n  @property\n  def mip(self):\n    if 'mip' in self.info:\n      return int(self.info['mip'])\n    \n    # Igneous has long used mesh_mip_N_err_M to store\n    # some information about the meshing job. Let's \n    # exploit that for now.\n    matches = re.search(MESH_MIP_REGEXP, self.mesh_path)\n    if matches is None:\n      return None \n\n    mip, = matches.groups()\n    return int(mip)\n  \n  @property\n  def spatial_index(self):\n    if 'spatial_index' in self.info:\n      return self.info['spatial_index']\n    return None\n\n  @property\n  def mesh_path(self):\n    if 'mesh' in self.meta.info:\n      return self.meta.info['mesh']\n    return 'mesh'\n\n  def join(self, *paths):\n    return self.meta.join(*paths)\n\n  @property\n  def basepath(self):\n    return self.meta.basepath\n\n  @property\n  def cloudpath(self):\n    return self.meta.cloudpath\n\n  @property\n  def layerpath(self):\n    return self.meta.join(self.meta.cloudpath, self.mesh_path)\n\n  def fetch_info(self):\n    if 'mesh' not in self.meta.info or not self.meta.info['mesh']:\n      return self.default_info()\n\n    info = self.cache.download_json(self.meta.join(self.mesh_path, 'info'))\n    if not info:\n      return self.default_info()\n    return info\n\n  def refresh_info(self):\n    self.info = self.fetch_info()\n    return self.info\n\n  def commit_info(self):\n    if self.info:\n      self.cache.upload_single(\n        self.meta.join(self.mesh_path, 'info'),\n        jsonify(self.info), \n        content_type='application/json',\n        compress=False,\n        cache_control='no-cache',\n      )\n\n  def default_info(self):\n    return {\n      '@type': 'neuroglancer_legacy_mesh',\n      'spatial_index': None, # { 'chunk_size': physical units }\n    }\n\n  def is_sharded(self):\n    if 'sharding' not in self.info:\n      return False\n    elif self.info['sharding'] is None:\n      return False\n    else:\n      return True\n\n  def is_multires(self):\n    return self.info['@type'] == 'neuroglancer_multilod_draco'\n\n  def is_legacy(self):\n    return not self.is_multires()"""
cloudvolume/datasource/precomputed/mesh/multilod.py,19,"b'from collections import defaultdict\n\nimport numpy as np\n\nfrom ..sharding import ShardingSpecification, ShardReader\nfrom ....storage import SimpleStorage\nfrom ....mesh import Mesh\nfrom ....lib import yellow, red, toiter\nfrom .... import exceptions\n\nclass UnshardedMultiLevelPrecomputedMeshSource(object):\n  def __init__(self):\n    pass\n\n  # Error on any use\n  def __getattr__(self, name):\n    def method(*args, **kwargs):\n      raise exceptions.UnsupportedFormatError(\n      red(""Meshes of type %s are not yet supported."" % self.__class__.__name__))\n    return method\n\n\nclass ShardedMultiLevelPrecomputedMeshSource:\n  def __init__(self, meta, cache, config, readonly=False):\n    self.meta = meta\n    self.cache = cache\n    self.config = config\n    self.readonly = bool(readonly)\n\n    spec = ShardingSpecification.from_dict(self.meta.info[\'sharding\'])\n    self.reader = ShardReader(meta, cache, spec)\n\n    self.vertex_quantization_bits = self.meta.info[\'vertex_quantization_bits\']\n    self.lod_scale_multiplier = self.meta.info[\'lod_scale_multiplier\']\n    self.transform = np.array(self.meta.info[\'transform\'] + [0,0,0,1]).reshape(4,4)\n  \n    if np.any(self.transform * np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]])):\n      raise exceptions.MeshDecodeError(red(""Non-scale homogeneous transforms are not implemented""))\n\n\n  @property\n  def path(self):\n    return self.meta.mesh_path\n\n\n  def exists(self, segids, progress=None):\n    """"""\n    Checks if the mesh exists\n\n    Returns: { MultiLevelPrecomputedMeshManifest or None, ... }\n    """"""\n    return [ self.get_manifest(segid) for segid in segids ]\n\n\n  def get_manifest(self, segid, progress=None):\n    """"""Retrieve the manifest for a single segment.\n\n    Returns:\n      { MultiLevelPrecomputedMeshManifest or None }\n    """"""\n    manifest_info = self.reader.exists(segid, self.meta.mesh_path, return_byte_range=True)\n    if manifest_info is None:\n      # Manifest not found\n      return None\n    shard_filepath, byte_start, num_bytes  = tuple(manifest_info)\n    binary = self.reader.get_data(segid, self.meta.mesh_path)\n    if binary == None:\n      return None\n    return MultiLevelPrecomputedMeshManifest(binary, segment_id=segid, offset=byte_start, shard_filepath=shard_filepath)\n  \n\n  def get(self, segids, lod=0, concat=True, progress=None):\n    """"""Fetch meshes at a given level of detail (lod).\n\n    Parameters:\n    segids: (iterable or int) segids to render\n\n    lod: int, default 0\n      Level of detail to retrieve.  0 is highest level of detail.\n\n    Optional:\n      concat: bool, concatenate fragments (per segment per lod)\n\n    Returns:\n    { segid: { Mesh } }\n    ... or if concatenate=False: { segid: { Mesh, ... } }\n\n    Reference:\n      https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/meshes.md\n    """"""\n\n    segids = toiter(segids)\n\n    # decode all the fragments\n    meshdata = defaultdict(list)\n    for segid in segids:\n      # Read the manifest (with a tweak to sharding.py to get the offset)\n      manifest = self.get_manifest(segid)\n      if manifest == None:\n        raise exceptions.MeshDecodeError(red(\n          \'Manifest not found for segment {}.\'.format(segid)\n        ))\n\n      if lod >= manifest.num_lods:\n        raise exceptions.MeshDecodeError(red(\n          \'LOD value out of range ({} > {}) for segment {}.\'.format(max(lods), manifest.num_lods, segid)\n        ))\n\n      # Read the data for all LODs\n      fragment_sizes = [ np.sum(lod_fragment_sizes) for lod_fragment_sizes in manifest.fragment_offsets ]\n      total_fragment_size = np.sum(fragment_sizes)\n\n      full_path = self.reader.meta.join(self.reader.meta.cloudpath)\n      stor = SimpleStorage(full_path)\n\n      lod_binary = stor.get_file(manifest.shard_filepath,\n          start=(manifest.offset - total_fragment_size) + np.sum(fragment_sizes[0:lod]),\n          end=(manifest.offset - total_fragment_size) + np.sum(fragment_sizes[0:lod+1]))\n\n      for frag in range(manifest.fragment_offsets[lod].shape[0]):\n        frag_binary = lod_binary[\n                    int(np.sum(manifest.fragment_offsets[lod][0:frag])) :\n                    int(np.sum(manifest.fragment_offsets[lod][0:frag+1]))\n                    ]\n        if len(frag_binary) == 0:\n          # According to @JBMS, empty fragments are used in cases where a child fragment exists,\n          # but its parent does not have a corresponding fragment, a possible byproduct of running\n          # marching cubes and mesh simplification independently for each level of detail.\n          continue\n        mesh = Mesh.from_draco(frag_binary)\n\n        # Conversion references:\n        # https://github.com/google/neuroglancer/blob/master/src/neuroglancer/mesh/draco/neuroglancer_draco.cc\n        # Treat the draco result as integers in the range [0, 2**vertex_quantization_bits)\n        mesh.vertices = mesh.vertices.view(dtype=np.int32)\n        \n        # Convert from ""stored model"" space to ""model"" space\n        mesh.vertices = manifest.grid_origin + manifest.vertex_offsets[lod] + \\\n                manifest.chunk_shape * (2 ** lod) * \\\n                (manifest.fragment_positions[lod][:,frag] + \\\n                (mesh.vertices / (2.0 ** self.vertex_quantization_bits - 1)))\n\n        # Scale to native (nm) space\n        mesh.vertices =  mesh.vertices * (self.transform[0,0], self.transform[1,1], self.transform[2,2])\n        \n        meshdata[segid].append(mesh)\n\n    if concat:\n      for segid in meshdata:\n        meshdata[segid] = Mesh.concatenate(*meshdata[segid])\n\n    return meshdata\n\nclass MultiLevelPrecomputedMeshManifest:\n  # Parse the multi-resolution mesh manifest file format:\n  # https://github.com/google/neuroglancer/blob/master/src/neuroglancer/datasource/precomputed/meshes.md\n  # https://github.com/google/neuroglancer/blob/master/src/neuroglancer/mesh/multiscale.ts\n\n  def __init__(self, binary, segment_id, offset, shard_filepath=None):\n    self._segment = segment_id\n    self._binary = binary\n    self._offset = offset\n    self._shard_filepath = shard_filepath\n\n    # num_loads is the 7th word\n    num_lods = int(np.frombuffer(self._binary[6*4:7*4], dtype=np.uint32)[0])\n\n    header_dt = np.dtype([(\'chunk_shape\', np.float32, (3,)),\n            (\'grid_origin\', np.float32, (3,)),\n            (\'num_lods\', np.uint32),\n            (\'lod_scales\', np.float32, (num_lods,)),\n            (\'vertex_offsets\', np.float32, (num_lods,3)),\n            (\'num_fragments_per_lod\', np.uint32, (num_lods,))\n            ])\n    self._header = np.frombuffer(self._binary[0:header_dt.itemsize], dtype=header_dt)\n    offset = header_dt.itemsize\n\n    self._fragment_positions = []\n    self._fragment_offsets = []\n    for lod in range(num_lods):\n      # Read fragment positions\n      pos_size =  3 * 4 * self.num_fragments_per_lod[lod]\n      self._fragment_positions.append(\n        np.frombuffer(self._binary[offset:offset + pos_size], dtype=np.uint32).reshape((3,self.num_fragments_per_lod[lod]))\n      )\n      offset += pos_size\n\n      # Read fragment sizes\n      off_size = 4 * self.num_fragments_per_lod[lod]\n      self._fragment_offsets.append(\n        np.frombuffer(self._binary[offset:offset + off_size], dtype=np.uint32)\n      )\n      offset += off_size\n\n    # Make sure we read the entire manifest\n    if offset != len(binary):\n      raise exceptions.MeshDecodeError(red(\n        \'Error decoding mesh manifest for segment {}\'.format(segment_id)\n      ))\n\n  @property\n  def chunk_shape(self):\n    return self._header[\'chunk_shape\'][0]\n\n  @property\n  def grid_origin(self):\n    return self._header[\'grid_origin\'][0]\n\n  @property\n  def num_lods(self):\n    return self._header[\'num_lods\'][0]\n\n  @property\n  def lod_scales(self):\n    return self._header[\'lod_scales\'][0]\n\n  @property\n  def vertex_offsets(self):\n    return self._header[\'vertex_offsets\'][0]\n\n  @property\n  def num_fragments_per_lod(self):\n    return self._header[\'num_fragments_per_lod\'][0]\n\n  @property\n  def fragment_positions(self):\n    return self._fragment_positions\n\n  @property\n  def fragment_offsets(self):\n    return self._fragment_offsets\n\n  @property\n  def length(self):\n    return len(self._binary)\n\n  @property\n  def offset(self):\n    """"""Manifest offset within the shard file. Used as a base when calculating fragment offsets.""""""\n    return self._offset\n\n  @property\n  def shard_filepath(self):\n    return self._shard_filepath\n'"
cloudvolume/datasource/precomputed/mesh/unsharded.py,0,"b'import six\n\nfrom collections import defaultdict\nimport itertools\nimport json\nimport re\nimport os\n\nimport struct\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom .... import exceptions\nfrom ....lib import yellow, red, toiter\nfrom ....mesh import Mesh\nfrom ....storage import Storage\nfrom ..spatial_index import CachedSpatialIndex\n\nSEGIDRE = re.compile(r\'\\b(\\d+):0.*?$\')\n\ndef filename_to_segid(filename):\n  matches = SEGIDRE.search(filename)\n  if matches is None:\n    raise ValueError(""There was an issue with the fragment filename: "" + filename)\n\n  segid, = matches.groups()\n  return int(segid)\n\nclass UnshardedLegacyPrecomputedMeshSource(object):\n  def __init__(self, meta, cache, config, readonly=False):\n    self.meta = meta\n    self.cache = cache\n    self.config = config\n\n    self.readonly = bool(readonly)\n\n    self.spatial_index = None\n    if self.meta.spatial_index:\n      self.spatial_index = CachedSpatialIndex(\n        self.cache,\n        cloudpath=self.meta.layerpath, \n        bounds=self.meta.meta.bounds(0) * self.meta.meta.resolution(0),\n        chunk_size=self.meta.info[\'spatial_index\'][\'chunk_size\'],\n      )\n\n  @property\n  def path(self):\n    return self.meta.mesh_path\n\n  def manifest_path(self, segid):\n    mesh_json_file_name = str(segid) + \':0\'\n    return self.meta.join(self.path, mesh_json_file_name)\n\n  def _get_manifests(self, segids):\n    segids = toiter(segids)    \n    paths = [ self.manifest_path(segid) for segid in segids ]\n    fragments = self.cache.download(paths)\n\n    contents = {}\n    for filename, content in fragments.items():\n      content = content.decode(\'utf8\')\n      content = json.loads(content)\n      segid = filename_to_segid(filename)\n      contents[segid] = content[\'fragments\']\n\n    return contents\n\n  def _get_mesh_fragments(self, paths):\n    paths = [ self.meta.join(self.path, path) for path in paths ]\n\n    compress = self.config.compress\n    if compress is None:\n      compress = True\n\n    fragments = self.cache.download(paths, compress=compress)\n    fragments = [ (filename, content) for filename, content in fragments.items() ]\n    fragments = sorted(fragments, key=lambda frag: frag[0]) # make decoding deterministic\n    return fragments\n\n  def exists(self, segids, progress=None):\n    """"""\n    Checks if the mesh exists.\n\n    Returns: { label: path or None, ... }\n    """"""\n    manifest_paths = [ self.manifest_path(segid) for segid in segids ]\n    StorageClass = GreenStorage if self.config.green else Storage\n\n    progress = progress if progress is not None else self.config.progress\n\n    with StorageClass(self.meta.cloudpath, progress=progress) as stor:\n      exists = stor.files_exist(manifest_paths)\n\n    segid_regexp = re.compile(r\'(\\d+):0$\')\n\n    output = {}\n    for path, there in exists.items():\n      (segid,) = re.search(segid_regexp, path).groups()\n      output[segid] = path if there else None\n  \n    return output\n  \n  def get(\n      self, segids, \n      remove_duplicate_vertices=True, \n      fuse=True,\n      chunk_size=None\n    ):\n    """"""\n    Merge fragments derived from these segids into a single vertex and face list.\n\n    Why merge multiple segids into one mesh? For example, if you have a set of\n    segids that belong to the same neuron.\n\n    segids: (iterable or int) segids to render into a single mesh\n\n    Optional:\n      remove_duplicate_vertices: bool, fuse exactly matching vertices\n      fuse: bool, merge all downloaded meshes into a single mesh\n      chunk_size: [chunk_x, chunk_y, chunk_z] if passed only merge at chunk boundaries\n    \n    Returns: Mesh object if fused, else { segid: Mesh, ... }\n    """"""\n    segids = toiter(segids)\n    dne = self.exists(segids)\n    dne = [ label for label, path in dne.items() if path is None ]\n\n    if dne:\n      missing = \', \'.join([ str(segid) for segid in dne ])\n      raise ValueError(red(\n        \'Segment ID(s) {} are missing corresponding mesh manifests.\\nAborted.\' \\\n        .format(missing)\n      ))\n\n    fragments = self._get_manifests(segids)\n    fragments = fragments.values()\n    fragments = list(itertools.chain.from_iterable(fragments)) # flatten\n    fragments = self._get_mesh_fragments(fragments)\n\n    # decode all the fragments\n    meshdata = defaultdict(list)\n    for frag in tqdm(fragments, disable=(not self.config.progress), desc=""Decoding Mesh Buffer""):\n      segid = filename_to_segid(frag[0])\n      try:\n        mesh = Mesh.from_precomputed(frag[1])\n      except Exception:\n        print(frag[0], \'had a problem.\')\n        raise\n      meshdata[segid].append(mesh)\n\n    if not fuse:\n      return { segid: Mesh.concatenate(*meshes) for segid, meshes in six.iteritems(meshdata) }\n\n    meshdata = [ (segid, mesh) for segid, mesh in six.iteritems(meshdata) ]\n    meshdata = sorted(meshdata, key=lambda sm: sm[0])\n    meshdata = [ mesh for segid, mesh in meshdata ]\n    meshdata = list(itertools.chain.from_iterable(meshdata)) # flatten\n    mesh = Mesh.concatenate(*meshdata)\n\n    if not remove_duplicate_vertices:\n      return mesh \n\n    if not chunk_size:\n      return mesh.consolidate()\n\n    if self.meta.mip is not None:\n      mip = self.meta.mip\n    else:\n      # This will usually be wrong, but it\'s backwards compatible.\n      # Throwing an exception instead would probably break too many\n      # things.\n      mip = self.config.mip\n\n    if mip not in self.meta.meta.available_mips:\n      raise exceptions.ScaleUnavailableError(""mip {} is not available."".format(mip))\n\n    resolution = self.meta.meta.resolution(mip)\n    chunk_offset = self.meta.meta.voxel_offset(mip)\n\n    return mesh.deduplicate_chunk_boundaries(\n      chunk_size * resolution, is_draco=False,\n      offset=(chunk_offset * resolution)\n    )\n\n  def save(self, segids, filepath=None, file_format=\'ply\'):\n    """"""\n    Save one or more segids into a common mesh format as a single file.\n\n    segids: int, string, or list thereof\n    filepath: string, file-like, or None (optional)\n    file_format: string (optional)\n    \n    Supported Formats: \'obj\', \'ply\', \'precomputed\'\n    """"""\n    if type(segids) != list:\n      segids = [segids]\n\n    mesh = self.get(segids, fuse=True, remove_duplicate_vertices=True)\n\n    if file_format == \'obj\':\n      data = mesh.to_obj()\n    elif file_format == \'ply\':\n      data = mesh.to_ply()\n    elif file_format == \'precomputed\':\n      data = mesh.to_precomputed()\n    else:\n      raise NotImplementedError(\'Only .obj, .ply, and precomputed are currently supported.\')\n\n    if not filepath:\n      filepath = str(segids[0]) + ""."" + file_format\n      if len(segids) > 1:\n        filepath = ""{}_{}.{}"".format(segids[0], segids[-1], file_format)\n\n    try:\n      filepath.write(data)\n    except AttributeError:\n      with open(filepath, \'wb\') as f:\n        f.write(data)\n\n  def get_bbox(self, bbox):\n    if self.spatial_index is None:\n      raise IndexError(""A spatial index has not been created."")\n\n    segids = self.spatial_index.query(bbox)\n    return self.get(segids, fuse=False)\n'"
cloudvolume/datasource/precomputed/skeleton/__init__.py,0,"b""import os\n\nfrom .metadata import PrecomputedSkeletonMetadata\nfrom .sharded import ShardedPrecomputedSkeletonSource\nfrom .unsharded import UnshardedPrecomputedSkeletonSource\n\nfrom ..metadata import PrecomputedMetadata\nfrom ....cacheservice import CacheService\nfrom ....paths import strict_extract\nfrom ....cloudvolume import SharedConfiguration\n\nclass PrecomputedSkeletonSource(object):\n  def __new__(cls, meta, cache, config, readonly=False):\n    skel_meta = PrecomputedSkeletonMetadata(meta, cache)\n\n    if skel_meta.is_sharded():\n      return ShardedPrecomputedSkeletonSource(skel_meta, cache, config, readonly) \n\n    return UnshardedPrecomputedSkeletonSource(skel_meta, cache, config, readonly)\n\n  @classmethod\n  def from_cloudpath(cls, cloudpath, cache=False, progress=False):\n    config = SharedConfiguration(\n      cdn_cache=False,\n      compress=True,\n      compress_level=None,\n      green=False,\n      mip=0,\n      parallel=1,\n      progress=progress,\n    )\n\n    cache = CacheService(\n      cloudpath=(cache if type(cache) == str else cloudpath),\n      enabled=bool(cache),\n      config=config,\n      compress=True,\n    )\n\n    cloudpath, skel_dir = os.path.split(cloudpath)\n    meta = PrecomputedMetadata(cloudpath, cache, info={ 'skeletons': skel_dir })\n\n    return PrecomputedSkeletonSource(meta, cache, config)"""
cloudvolume/datasource/precomputed/skeleton/metadata.py,1,"b'import re\n\nfrom ....lib import jsonify\n\nimport numpy as np\n\nSKEL_MIP_REGEXP = re.compile(r\'skeletons_mip_(\\d+)\')\n\nclass PrecomputedSkeletonMetadata(object):\n  def __init__(self, meta, cache=None, info=None):\n    self.meta = meta\n    self.cache = cache\n\n    if info:\n      self.info = info\n    elif \'skeletons\' in self.meta.info and self.meta.info[\'skeletons\']:\n      self.info = self.fetch_info()\n    else:\n      self.info = self.default_info()\n\n  @property\n  def spatial_index(self):\n    if \'spatial_index\' in self.info:\n      return self.info[\'spatial_index\']\n    return None  \n\n  @property\n  def skeleton_path(self):\n    if \'skeletons\' in self.meta.info:\n      return self.meta.info[\'skeletons\']\n    return \'skeletons\'\n\n  @property\n  def mip(self):\n    if \'mip\' in self.info:\n      return int(self.info[\'mip\'])\n    \n    # Igneous has long used skeletons_mip_N to store\n    # some information about the skeletonizing job. Let\'s \n    # exploit that for now.\n    matches = re.search(SKEL_MIP_REGEXP, self.skeleton_path)\n    if matches is None:\n      return None \n\n    mip, = matches.groups()\n    return int(mip)\n\n  def join(self, *paths):\n    return self.meta.join(*paths)\n\n  @property\n  def transform(self):\n    return np.array(self.info[\'transform\'], dtype=np.float32).reshape( (3,4) )\n\n  @transform.setter\n  def transform(self, val):\n    self.info[\'transform\'] = val\n\n  @property\n  def basepath(self):\n    return self.meta.basepath\n\n  @property\n  def cloudpath(self):\n    return self.meta.cloudpath\n\n  @property\n  def layerpath(self):\n    return self.meta.join(self.meta.cloudpath, self.skeleton_path)\n\n  def fetch_info(self):\n    info = self.cache.download_json(self.meta.join(self.skeleton_path, \'info\'))\n    if not info:\n      return self.default_info()\n    return info\n\n  def refresh_info(self):\n    self.info = self.fetch_info()\n    return self.info\n\n  def commit_info(self):\n    if self.info:\n      self.cache.upload_single(\n        self.meta.join(self.skeleton_path, \'info\'),\n        jsonify(self.info), \n        content_type=\'application/json\',\n        compress=False,\n        cache_control=\'no-cache\',\n      )\n\n  def default_info(self):\n    return {\n      \'@type\': \'neuroglancer_skeletons\',\n      \'transform\': [  \n        1, 0, 0, 0, # identity\n        0, 1, 0, 0,\n        0, 0, 1, 0\n      ],\n      \'vertex_attributes\': [\n        {\n          ""id"": ""radius"",\n          ""data_type"": ""float32"",\n          ""num_components"": 1,\n        }, \n        {\n          ""id"": ""vertex_types"",\n          ""data_type"": ""uint8"",\n          ""num_components"": 1,\n        }\n      ],\n      \'sharding\': None,\n      \'spatial_index\': None, # { \'chunk_size\': physical units }\n    }\n\n  def is_sharded(self):\n    if \'sharding\' not in self.info:\n      return False\n    elif self.info[\'sharding\'] is None:\n      return False\n    else:\n      return True\n'"
cloudvolume/datasource/precomputed/skeleton/sharded.py,0,"b'from ..sharding import ShardingSpecification, ShardReader\nfrom ....skeleton import Skeleton\nfrom ..spatial_index import CachedSpatialIndex\nfrom ....exceptions import EmptyFileException\n\nclass ShardedPrecomputedSkeletonSource(object):\n  def __init__(self, meta, cache, config, readonly=False):\n    self.meta = meta\n    self.cache = cache\n    self.config = config\n    self.readonly = bool(readonly)\n\n    spec = ShardingSpecification.from_dict(self.meta.info[\'sharding\'])\n    self.reader = ShardReader(meta, cache, spec)\n\n    self.spatial_index = None\n    if self.meta.spatial_index:\n      self.spatial_index = CachedSpatialIndex(\n        self.cache,\n        cloudpath=self.meta.layerpath, \n        bounds=self.meta.meta.bounds(0) * self.meta.meta.resolution(0),\n        chunk_size=self.meta.info[\'spatial_index\'][\'chunk_size\'],\n      )\n\n  @property\n  def path(self):\n    return self.meta.skeleton_path\n\n  def get(self, segids):\n    list_return = True\n    if type(segids) in (int, float):\n      list_return = False\n      segids = [ int(segids) ]\n\n    # compress = self.config.compress \n    # if compress is None:\n    #   compress = True\n\n    results = []\n    binaries = self.reader.get_data(\n      segids, self.meta.skeleton_path, \n      progress=self.config.progress\n    )\n\n    for segid in segids:\n      binary = binaries[segid]\n      del binaries[segid]\n\n      if binary is None:\n        raise EmptyFileException(""segid {} is missing."".format(segid))\n\n      skeleton = Skeleton.from_precomputed(\n        binary, segid=segid, \n        vertex_attributes=self.meta.info[\'vertex_attributes\']\n      )\n      skeleton.transform = self.meta.transform\n      results.append(skeleton.physical_space())\n\n    if list_return:\n      return results\n    else:\n      return results[0]\n\n  def upload(self, *args, **kwargs):\n    raise NotImplementedError()\n\n  def raw_upload(self, *args, **kwargs):\n    raise NotImplementedError()\n\n  def get_bbox(self, bbox):\n    if self.spatial_index is None:\n      raise IndexError(""A spatial index has not been created."")\n\n    segids = self.spatial_index.query(bbox)\n    return self.get(segids)'"
cloudvolume/datasource/precomputed/skeleton/unsharded.py,0,"b'import datetime\nimport os\n\nimport posixpath\n\nimport numpy as np\n\nfrom cloudvolume import lib\nfrom cloudvolume.exceptions import (\n  SkeletonDecodeError, SkeletonEncodeError, \n  SkeletonUnassignedEdgeError\n)\nfrom cloudvolume.lib import red, Bbox\nfrom cloudvolume.storage import Storage, SimpleStorage\n\nfrom ..common import cdn_cache_control\nfrom ..spatial_index import CachedSpatialIndex\nfrom ... import readonlyguard\n\nfrom ....skeleton import Skeleton\n\nclass UnshardedPrecomputedSkeletonSource(object):\n  def __init__(self, meta, cache, config, readonly=False):\n    self.meta = meta\n    self.cache = cache\n    self.config = config\n\n    self.readonly = bool(readonly)\n\n    self.spatial_index = None\n    if self.meta.spatial_index:\n      self.spatial_index = CachedSpatialIndex(\n        self.cache,\n        cloudpath=self.meta.layerpath, \n        bounds=self.meta.meta.bounds(0) * self.meta.meta.resolution(0),\n        chunk_size=self.meta.info[\'spatial_index\'][\'chunk_size\'],\n      )\n\n  @property\n  def path(self):\n    return self.meta.skeleton_path \n\n  def get(self, segids, allow_missing=False):\n    """"""\n    Retrieve one or more skeletons from the data layer.\n\n    Example: \n      skel = vol.skeleton.get(5)\n      skels = vol.skeleton.get([1, 2, 3])\n\n    Raises SkeletonDecodeError on missing files or decoding errors.\n\n    Required:\n      segids: list of integers or integer\n    Optional:\n      allow_missing: skip over non-existent files otherwise\n        raise cloudvolume.exceptions.SkeletonDecodeError\n\n    Returns: \n      if segids is a list, returns list of Skeletons\n      else returns a single Skeleton\n    """"""\n    list_return = True\n    if type(segids) in (int, float):\n      list_return = False\n      segids = [ int(segids) ]\n\n    compress = self.config.compress \n    if compress is None:\n      compress = True\n\n    results = self.cache.download(\n      [ self.meta.join(self.meta.skeleton_path, str(segid)) for segid in segids ],\n      compress=compress\n    )\n    missing = [ filename for filename, content in results.items() if content is None ]\n    results = { filename: content for filename, content in results.items() if content is not None }\n\n    if not allow_missing and len(missing):\n      raise SkeletonDecodeError(""File(s) do not exist: {}"".format("", "".join(missing)))\n\n    skeletons = []\n    for filename, content in results.items():\n      segid = int(os.path.basename(filename))\n      try:\n        skel = Skeleton.from_precomputed(content, segid=segid)\n      except Exception as err:\n        raise SkeletonDecodeError(""segid "" + str(segid) + "": "" + str(err))\n      skel.transform = self.meta.transform\n      skeletons.append(skel.physical_space())\n\n    if list_return:\n      return skeletons\n\n    if len(skeletons):\n      return skeletons[0]\n\n    return None\n\n  @readonlyguard\n  def upload_raw(self, segid, vertices, edges, radii=None, vertex_types=None):\n    skel = Skeleton(\n      vertices, edges, radii, \n      vertex_types, segid=segid\n    )\n    return self.upload(skel)\n    \n  @readonlyguard\n  def upload(self, skeletons):\n    if type(skeletons) == Skeleton:\n      skeletons = [ skeletons ]\n\n    files = [ (self.meta.join(self.meta.skeleton_path, str(skel.id)), skel.to_precomputed()) for skel in skeletons ]\n    self.cache.upload(\n      files=files, \n      compress=\'gzip\', \n      cache_control=cdn_cache_control(self.config.cdn_cache)\n    )\n\n  def get_bbox(self, bbox):\n    if self.spatial_index is None:\n      raise IndexError(""A spatial index has not been created."")\n\n    segids = self.spatial_index.query(bbox)\n    return self.get(segids)\n'"
