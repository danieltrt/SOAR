file_path,api_count,code
all data.py,3,"b'# -*- coding: utf-8 -*-\n""""""deep.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/10u3uhh9MmTw86fpR068xExkyWB9z0rlM\n""""""\n\nimport pandas as pd\ndata = pd.read_csv(\'dataset.csv\')\n\nimport tensorflow as tf\ntf.test.gpu_device_name()\n\ndef getVectors(corpus,vectors,size):\n    wordset = set(vectors.wv.index2word) #Checks if the word is in the Word2vec corpus \n    vec = []\n    counter = 0\n    for words in corpus:    \n        featureVec = np.zeros(size,dtype=""object"")\n        for word in words:\n            if word in wordset:\n                featureVec = np.add(featureVec,vectors[word])\n        vec.append(featureVec.T)\n        counter = counter + 1\n        #print(counter)\n    return vec\n\ncorpus = []\nfor words in data.comment:\n    words = words.split()\n    corpus.append(words)\n\nX = corpus\n\nfrom keras.utils import to_categorical\nY = to_categorical(data.rating)\n\nimport gensim.models.word2vec as wv\nvocab_size = 300\nmin_counts = 10\ncontext = 5\nn_workers = 15\ndown_sample = 1e-2\nvectors = wv.Word2Vec(X,\n                     size = vocab_size,\n                     workers =n_workers,\n                     window = context,\n                     min_count = min_counts,\n                     sample = down_sample)\n\nimport numpy as np\n\nX_train = getVectors(X,vectors,vocab_size)\n\nX_train = np.array(X_train)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import layers\nfrom keras import regularizers\n\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim =1024, init = \'uniform\', activation = \'relu\', input_dim = vocab_size,kernel_regularizer= regularizers.l2(0.0001)))\nclassifier.add(layers.Dropout(0.2))\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim =1024, init = \'uniform\', activation = \'relu\'))\n#classifier.add(layers.Dropout(0.25))\n# Adding the third hidden layer\nclassifier.add(Dense(output_dim =1024, init = \'uniform\', activation = \'relu\',kernel_regularizer= regularizers.l2(0.0001)))\n#classifier.add(layers.Dropout(0.25))\n# Adding the output layer\nclassifier.add(Dense(output_dim = 2, init = \'uniform\', activation = \'softmax\'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = \'adam\', loss = \'categorical_crossentropy\', metrics = [\'accuracy\'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, Y, batch_size = 50, epochs = 60)\n\nclassifier.save(""deep.h5"")\n\nimport pickle as p\nf = open(""wv.pickle"",""wb"")\np.dump(vectors,f)\nf.close()'"
deep.py,6,"b'# -*- coding: utf-8 -*-\n""""""deep.ipynb\n\nAutomatically generated by Colaboratory.\n\nOriginal file is located at\n    https://colab.research.google.com/drive/10u3uhh9MmTw86fpR068xExkyWB9z0rlM\n""""""\n\nimport pandas as pd\ndata = pd.read_csv(\'final.csv\')\n\nimport tensorflow as tf\ntf.test.gpu_device_name()\n\ndef getVectors(corpus,vectors,size):\n    wordset = set(vectors.wv.index2word) #Checks if the word is in the Word2vec corpus \n    vec = []\n    counter = 0\n    for words in corpus:    \n        featureVec = np.zeros(size,dtype=""object"")\n        for word in words:\n            if word in wordset:\n                featureVec = np.add(featureVec,vectors[word])\n        vec.append(featureVec.T)\n        counter = counter + 1\n        #print(counter)\n    return vec\n\ncorpus = []\nfor words in data.comment:\n    words = words.split()\n    corpus.append(words)\n\nfrom keras.utils import to_categorical\nY = to_categorical(data.rating)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, y_test = train_test_split(corpus,Y, test_size = 0.20, random_state = 123)\n\nimport gensim.models.word2vec as wv\nvocab_size = 300\nmin_counts = 10\ncontext = 5\nn_workers = 15\ndown_sample = 1e-2\nvectors = wv.Word2Vec(X_train,\n                     size = vocab_size,\n                     workers =n_workers,\n                     window = context,\n                     min_count = min_counts,\n                     sample = down_sample)\n\nimport numpy as np\n\nX_train = getVectors(X_train,vectors,vocab_size)\nX_test = getVectors(X_test,vectors,vocab_size)\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import layers\nfrom keras import regularizers\n\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim =1024, init = \'uniform\', activation = \'relu\', input_dim = vocab_size,kernel_regularizer= regularizers.l2(0.0001)))\nclassifier.add(layers.Dropout(0.2))\n# Adding the second hidden layer\nclassifier.add(Dense(output_dim =1024, init = \'uniform\', activation = \'relu\'))\n#classifier.add(layers.Dropout(0.25))\n# Adding the third hidden layer\nclassifier.add(Dense(output_dim =1024, init = \'uniform\', activation = \'relu\',kernel_regularizer= regularizers.l2(0.0001)))\n#classifier.add(layers.Dropout(0.25))\n# Adding the output layer\nclassifier.add(Dense(output_dim = 2, init = \'uniform\', activation = \'softmax\'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = \'adam\', loss = \'categorical_crossentropy\', metrics = [\'accuracy\'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, Y_train, batch_size = 50, epochs = 60)\n\ny_pred = classifier.predict(X_test)\n\ny_pred = np.argmax(y_pred,axis = 1)\ny_test = np.argmax(y_test,axis = 1)\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n\ncm = confusion_matrix(y_test,y_pred)\nprint(accuracy_score(y_test, y_pred))\n\nclassifier.save(""deep.h5"")\n\n'"
final data frame.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Jan  3 20:12:20 2019\n\n@author: Shaheer Akram\n""""""\n\nimport pandas as pd\n\ndata = pd.read_csv(\'final.csv\')\ndata = data[data.comment.str.len() > 15]\nlen(data[data.rating == True])\ndata = data[\'comment\'].drop_duplicates()\ndata = pd.DataFrame(data)\ndata_false4 = data[data.rating == False]\ndata_true4 = data[data.rating == True]\nmerge_true = pd.concat([data_true1,data_true2,data_true3,data_true4])\nmerge_true.to_csv(\'galiyan.csv\')\ndata = pd.read_csv(\'labeled.csv\')\ndel data[\'Unnamed: 0\']\nmerge_true = pd.read_csv(\'galiyan.csv\')\ndel merge_true[\'Unnamed: 0\']\nmerge_false = pd.concat([data_false1,data_false2,data_false3,data_false4])\nmerge_false.to_csv(\'no galiyan.csv\')\nmerge_false = pd.read_csv(\'no galiyan.csv\')\ndel merge_false[\'Unnamed: 0\']\nfrom sklearn.model_selection import train_test_split\nX_train,data_false4_splice,y_train,y_test = train_test_split(data_false4.comment,data_false4.rating,test_size = 0.05)\ndata_false4_splice = data_false4_splice[:28952]\ndata_false4_splice = pd.DataFrame(data_false4_splice)\ndata_false4_splice[\'rating\'] = y_test\nfinal_false = pd.concat([data_false4_splice,data_false3_splice,data_false1,data_false2])\ndataset = pd.concat([final_false,merge_true])\ndataset.to_csv(\'dataset.csv\')\n\n'"
label bad comments.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Dec 26 20:27:21 2018\n\n@author: Shaheer Akram\n""""""\n\nimport pandas as pd\ndata = pd.read_csv(\'final.csv\',index_col=0)\ndata.head()\ndata[\'comment\'].str.contains(lexicon).astype(int).sum()\nyasir = \'bhosda|chut|chod|chinaal|gaandu|randi|gaandfat|takke|bhenchod|bharw|khotey|lauda|kutta|taata|bdsk|bharwa|gaand|lassan|choot|maderchod|laude|ullu|chhed|bhosri|lora|kutte|bhadwe|lore|gand|randi|cuntmama|bsdk|gaandu|betichod|bhosadike|lund|rundi|bhen|kutte|hijra|chodu|chunni|jhant|dalle|tatti|mader|paad|kamina|tatay|bhsdk|gadha|bhen|kuttiya|lori|jhaat|chutiya|gandu|choot|lund|gaand|muth|gaand|moot|chut|taatay|marani|bhadhava|bhonsri|jhaant|chuda|kutti\'\nlexicon = \'chut|chod|chood|bhosda|bhosadike|rundi|jhand|jhaant|madr|bhadhava|madar|mader|bhonsri|randi|bharw|bsdk|gand|bhsdk|bhosri|lora|lori|choot|muth|lore|laude|lund|takke|gaand|jhant|dalle|taata\'\ndata[\'rating\'] = data[\'comment\'].str.contains(lexicon)\ndata.to_csv(\'labeled.csv\')\ndata.head()\nabuse = data[data.rating == True]'"
merge.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Dec 22 15:25:17 2018\n\n@author: Shaheer Akram\n""""""\n\nimport pandas as pd\nimport csv\nimport re\nimport glob\n\nframes = []\nfiles = glob.glob(\'*.csv\')\nfor names in files:\n    df = pd.read_csv(names)\n    frames.append(df)\nmerge = pd.concat(frames)\ndel merge[\'hasReplies\']\ndel merge[\'numberOfReplies\']\nmerge[\'commentText\'] = merge[\'commentText\'].map(str) + merge[\'replies.commentText\'].map(str)\ndel merge[\'replies.commentText\']\nmerge.head()\nmerge.to_csv(""all.csv"")\ndata = pd.read_csv(\'all.csv\',index_col = 0)         #for first ittration\ncorpus = []             #for every ittration only\nX = data[\'comment\']     #for first ittration only\nfor i in range(len(X)):\n    #review = re.sub(\'nan\',\' \',str(X[i]))           #remove nan\n    #review = re.sub(\'http\\S+\',\' \',str(X[i]))       #remove links\n    review = re.sub(\'[^a-zA-Z]\',\' \',str(X[i]))      #remove emojis and punctuations\n    review = review.lower()         #run this for last ittration only\n    review = review.split()\n    review = \' \'.join(review)\n    corpus.append(review)\n    print(i)\nclean = pd.DataFrame(corpus)\nclean.columns = [\'comment\']\n#clean[\'comment\'].str.contains(""https"").astype(int).sum()\nX = clean[\'comment\']        #from second ittration\nclean.to_csv(""clean.csv"")\ndata = pd.read_csv(\'clean.csv\',index_col = 0)\ndata = data.dropna()\ndata.to_csv(\'final.csv\')\ndata[\'comment\'].str.contains(""bc"").astype(int).sum()\nclean = pd.read_csv(\'clean.csv\')\nclean1 = pd.read_csv(\'clean1.csv\')\nclean2 = pd.read_csv(\'clean2.csv\')\nclean3 = pd.read_csv(\'clean3.csv\')\nclean4 = pd.read_csv(\'clean4.csv\')\nclean5 = pd.read_csv(\'clean5.csv\')\nclean6 = pd.read_csv(\'clean6.csv\')\ndata = pd.concat([clean,clean1,clean2,clean3,clean4,clean5,clean6])\ndata.to_csv(\'all_clean.csv\')'"
ml_clf.py,2,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sat Jan  5 17:40:10 2019\n\n@author: Shaheer Akram\n""""""\n\nimport pandas as pd\nfrom gensim.models import word2vec as wv\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\ndef getVectors(corpus,vectors,size):\n    wordset  = set(vectors.wv.index2word)\n    vec = []\n    count = 0\n    \n    for sentence in corpus:\n        feature_vec = np.zeros(vocab_size,dtype = \'object\')\n        for word in sentence:\n            if word in wordset:\n                feature_vec = np.add(feature_vec,vectors[word])\n        vec.append(feature_vec.T)\n        count = count+1\n        print(count)\n    return vec\n\n\ndata = pd.read_csv(\'final.csv\')\n\ncorpus = [word_tokenize(str(sent)) for sent in data[0]]\n\nvocab_size = 300\nmin_counts = 7\ncontext = 5\nn_workers = 15\ndown_sample = 1e-2\nvectors = wv.Word2Vec(corpus,\n            size = vocab_size,\n            window = context,\n            min_count = min_counts,\n            workers = n_workers,\n            sample = down_sample)\n\nprint(vectors.most_similar(\'\'))\n\nvec = getVectors(corpus,vectors,vocab_size)\n\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nX_train,X_test,y_train,y_test = train_test_split(vec,data.rating,test_size = 0.5,random_state = 20)\n\nclf = svm.SVC(kernel = \'rbf\',cache_size = 1000,C = 0.1)\nclf.fit(X_train,y_train)\ny_pred = clf.predict(X_test)\ncm = confusion_matrix(y_test,y_pred)\nprint(accuracy_score(y_test,y_pred_knn))\n\npoly_svm = svm.SVC(kernel = \'poly\')\npoly_svm.fit(X_train,y_train)\ny_pred_poly_svm = poly_svm.predict(X_test)\n\nlin_svm = svm.SVC(kernel = \'linear\')\nlin_svm.fit(X_train,y_train)\ny_pred_lin_svm = lin_svm.predict(X_test)\n\nfrom sklearn import linear_model\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nlreg = linear_model.LogisticRegression()\nlreg.fit(X_train,y_train)\ny_pred_lreg = lreg.predict(X_test)\n\nsgd = linear_model.SGDClassifier()\nsgd.fit(X_train,y_train)\ny_pred_sgd = sgd.predict(X_test)\n\nmnb = MultinomialNB()\nmnb.fit(X_train,y_train)\ny_pred_mnb = mnb.predict(X_test)\n\nk_range=range(1,25)\nscore=[]\nfor i in k_range:\n    knn=KNeighborsClassifier(n_neighbors=11)\n    knn.fit(X_train,y_train)\n    y_pred_knn=knn.predict(X_test)\n    score.append(accuracy_score(y_test,y_pred_knn))\n\nrnc = RandomForestClassifier(n_estimators=100,max_depth=2,random_state=0)\nrnc.fit(X_train,y_train)\ny_pred_rnc = rnc.predict(X_test)\n\ntest = [""mujhe gali nahi do"",""tumhari amma achi ha"",""chutia ha kia "", ""kutte ka bacha""]\nn = [word_tokenize(str(sent)) for sent in test]\ntest = n\ntest = getVectors(test,vectors,vocab_size)\nlreg.predict(test)\n\n\n\n'"
Web-App/app.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Jun 30 14:24:37 2019\n\n@author: shaheer\n""""""\n\n\nfrom flask import Flask,render_template,request,redirect,jsonify\nimport numpy as np\nimport pandas as pd\nimport pickle as p\nfrom keras.models import load_model\nimport gensim.models.word2vec as wv\nfrom keras import backend as k\n\n\ndef getVectors(corpus,vectors,size):\n    wordset = set(vectors.wv.index2word) #Checks if the word is in the Word2vec corpus \n    vec = []\n    counter = 0\n    for words in corpus:    \n        featureVec = np.zeros(size,dtype=""object"")\n        for word in words:\n            if word in wordset:\n                featureVec = np.add(featureVec,vectors[word])\n        vec.append(featureVec.T)\n        counter = counter + 1\n        #print(counter)\n    return vec\n\ndef readModel():    \n    f = open(\'wv.pickle\',\'rb\')\n    vectors = p.load(f)\n    f.close()\n    vocab_size = vectors.wv.vectors.shape[1]\n    model = load_model(\'deep.h5\')\n    return  model,vectors,vocab_size\n\n\napp = Flask(__name__)\n\n\'\'\'\nif request method is post api will make pridiction and show result on index.html\nif request method is get api will simply render index.html\n\'\'\'\n@app.route(\'/\',methods = [\'GET\',\'POST\'])\ndef home():\n    if request.method == \'POST\':\n        sentence = [\'\']\n        sentence = [request.form.get(\'sent\')]\n        model,vectors,vocab_size = readModel()\n        sentence = [sent.split() for sent in sentence] \n        X_sent = getVectors(sentence,vectors,vocab_size)\n        X_sent = np.array(X_sent)\n        pred = model.predict(X_sent)\n        k.clear_session()\n        pred = np.argmax(pred,axis = 1)\n        del model\n        del vectors\n        del sentence\n        del vocab_size\n        del X_sent\n        return render_template(\'index.html\',result = pred)\n    else:\n        return render_template(\'index.html\')\n\n        \n\n\nif __name__ == \'__main__\':\n    app.run(debug= True, port= 5000)\n\n\n\n\n\n'"
