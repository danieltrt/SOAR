file_path,api_count,code
1-classification/1-KNN.py,0,"b'from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection.tests.test_split import train_test_split\n\ndataset = load_iris()\n\ntrain_feature, test_feature, train_label, test_label = train_test_split(dataset.data, dataset.target, test_size=.5)\n\nclf = KNeighborsClassifier(n_neighbors=2)\nclf.fit(train_feature, train_label)\n\n# Features from https://ru.wikipedia.org/wiki/%D0%98%D1%80%D0%B8%D1%81%D1%8B_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0\nprint clf.predict([\n    [4.4, 3.0, 1.3, 0.2]\n])\n\n# Now let\'s check accuracy\nprint ""Accuracy: {}"".format(accuracy_score(test_label, clf.predict(test_feature)))\n\n'"
1-classification/2-KNN-implementation.py,1,"b'from sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.model_selection.tests.test_split import train_test_split\nfrom collections import Counter\nimport numpy as np\n\niris = load_iris()\n\ntrain_feature, test_feature, train_label, test_label = train_test_split(iris.data, iris.target, test_size=.5)\n\nclass KNearestNeighboard(object):\n    def __init__(self, k):\n        self.k = k\n    def fit(self, features, labels):\n        self.features = features\n        self.labels = labels\n\n    def predict(self, test_features):\n        predictions = []\n\n        for row in test_features:\n            label = self._predictLabel(row)\n            predictions.append(label)\n        return predictions\n    def _predictLabel(self, row):\n        distances = []\n\n        for value in self.features:\n            distance = np.linalg.norm(row - value)\n            distances.append(distance)\n        \n        k_nearest_distances = sorted(distances)[:self.k]\n\n        labels = []\n        for distance in k_nearest_distances:\n            labels.append(self.labels[distances.index(distance)])\n        \n        return Counter(labels).most_common(1)[0][0]\n\n\nc1 = KNN(n_neighbors=5)\nc1.fit(train_feature, train_label)\n\nclassifier = KNearestNeighboard(k=5)\nclassifier.fit(train_feature, train_label)\n\nprint ""Accuracy of sklearn impl: {}"".format(accuracy_score(test_label, c1.predict(test_feature)))\nprint ""Accuracy of custom impl: {}"".format(accuracy_score(test_label, classifier.predict(test_feature)))'"
2-clustering/1-KMeans.py,1,"b""import numpy as np\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n\nstyle.use('ggplot')\n\ndataset = np.array([\n    [1, 2],\n    [1.5, 2],\n    [3, 1],\n    [12, 10],\n    [9, 12],\n    [7, 11]\n])\nclf = KMeans(n_clusters=2)\nclf.fit(dataset)\ncolors = ['g', 'r']\nprint clf.labels_\nfor centroid in clf.cluster_centers_:\n    plt.scatter(centroid[0], centroid[1], marker='x')\n\nfor idx, data in enumerate(dataset):\n    plt.scatter(data[0], data[1], color=colors[clf.labels_[idx]])\n\n\nplt.show()"""
2-clustering/2-KMeans-implementation.py,4,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n\nstyle.use('ggplot')\n\ndataset = np.array([\n    [1, 2],\n    [1.5, 2],\n    [3, 1],\n    [12, 10],\n    [9, 12],\n    [7, 11]\n])\n\nclass KMeans(object):\n    def __init__(self, n_cluster=1, tolerance=.001, max_iteration=300):\n        self.n_cluster = n_cluster\n        self.tolerance = tolerance\n        self.max_iteration = max_iteration\n        self.classification = {}\n        self.centroids = {}\n        self._iterated = 0\n\n    def fit(self, data):\n        # Random\n        for i in range(self.n_cluster):\n            self.centroids[i] = data[i]\n        \n        # Classify data\n        for i in range(self.max_iteration):\n            self._iterated += 1\n            self.classification = {}\n\n            for ii in range(self.n_cluster):\n                self.classification[ii] = []\n            \n            # Iterate over dataset and classify it for current centroids\n            for featureset in data:\n\n                distances = []\n                for centroid in self.centroids:\n                    distances.append(np.linalg.norm(featureset - self.centroids[centroid]))\n                \n                # Get the distance from closest centroid\n                closest_dist = min(distances)\n\n                # Get the index (centroid index)\n                classification = distances.index(closest_dist)\n\n                # Add current featureset to an centroid array\n                self.classification[classification].append(featureset)\n\n            prev_centroid = dict(self.centroids)\n            optimized = True\n\n            for classification in self.classification:\n                '''\n                So we should get average of X and Y coordinates\n                Xavg = (X1 + X2 + X3 + ...Xn) / n\n                Yavg = (Y1 + Y2 + Y3 + ...Yn) / n\n\n                ^\n                | x1\n                |---*\n                |x2\n                |- *\n                |x3\n                |--*\n                0--------------------->\n                |\n                '''\n                self.centroids[classification] = np.average(self.classification[classification], axis=0)\n\n            for centroid in self.centroids:\n                orig_cent = prev_centroid[centroid]\n                curr_cent = self.centroids[centroid]\n\n                if np.sum((curr_cent - orig_cent) / orig_cent * 100.0) > self.tolerance:\n                    optimized = False\n\n            if optimized:\n                break\n\n    \n\nclf = KMeans(n_cluster=2, max_iteration=10000)\nclf.fit(dataset)\ncolors = ['g', 'r']\n\nfor centroid in clf.centroids:\n    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1])\n\nfor classification in clf.classification:\n    plt.scatter(clf.classification[classification][0], clf.classification[classification][1], color=colors[classification])\n\nplt.show()"""
2-clustering/3-MeanShift.py,1,"b""from sklearn.cluster import MeanShift\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = np.array([\n    [1, 2],\n    [1.5, 1.9],\n    [2, 5],\n    [14, 3],\n    [11, 2],\n    [7, 9],\n    [7, 11],\n    [11, 4]\n])\n\nclf = MeanShift(bandwidth=4)\nclf.fit(dataset)\nfor centroid in clf.cluster_centers_:\n    plt.scatter(centroid[0], centroid[1], marker='x')\n\nplt.scatter(dataset[:, 0], dataset[:, 1])\n\nplt.show()"""
2-clustering/4-MeanShift-implementation.py,5,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = np.array([\n    [1, 2],\n    [1.5, 1.9],\n    [2, 5],\n    [14, 3],\n    [11, 2],\n    [7, 9],\n    [7, 11],\n    [11, 4]\n])\n\n\nclass MeanShift(object):\n    def __init__(self, bandwidth):\n        self.bandwidth = bandwidth\n        self.centroids = {}\n\n    def fit(self, data):\n        self.centroids = {}\n\n        # Every item is a centroid\n        for i in range(len(data)):\n            self.centroids[i] = data[i]\n\n        while True:\n            new_centroids = []\n\n            for i in self.centroids:\n                in_bandwidth = []\n                curr_centroid = self.centroids[i]\n\n\n                for features in data:\n                    # is in radius or not\n                    if np.linalg.norm(features - curr_centroid) < self.bandwidth:\n                        in_bandwidth.append(features)\n                \n                new_centroid = np.average(in_bandwidth, axis=0)\n                new_centroids.append(tuple(new_centroid))\n\n            uniques = sorted(list(set(new_centroids)))\n\n            prev_centroids = dict(self.centroids)\n\n            self.centroids = {}\n\n            for i in range(len(uniques)):\n                self.centroids[i] = np.array(uniques[i])\n\n            optimized = True\n\n            for i in self.centroids:\n                if not np.array_equal(self.centroids[i], prev_centroids[i]):\n                    optimized = False\n                if not optimized:\n                    break\n\n            if optimized:\n                break\n\nclst = MeanShift(bandwidth=4)\n\nclst.fit(dataset)\n\nplt.scatter(dataset[:, 0], dataset[:, 1])\n\nfor i in clst.centroids:\n    plt.scatter(clst.centroids[i][0], clst.centroids[i][1], marker='x')\n\nplt.show()\n"""
3-mnist/1-mnist.py,0,"b'import os\nfrom scipy.misc import imread\n\nx = imread(os.path.abspath(""[img_name].jpg""), flatten=True) / pallette'"
intro/1-numpy.py,2,"b'import numpy as np\nfrom math import sqrt\n\nX = [\n    [0, 2]\n]\n\nY = [\n    [6, 10]\n]\n\nnp_x = np.arange(9) - 2\nnp_y = np_x.reshape((3, 3))\n\nprint ""np_x: {}"".format(np_x)\nprint ""np_y: {}"".format(np_y)\n\n\n\ndef euclidean_distance(dx, dy):\n    sum_ = 0\n    for idx, value in enumerate(dx):\n        sum_ += (abs(value - dy[idx]))**2\n    return sqrt(sum_)\n\ndef euclidean_distance_np(dx):\n    return np.linalg.norm(dx)\n\n\nif __name__ == \'__main__\':\n    print ""Custom fn euclidean distance: {}"".format(euclidean_distance(X[0], Y[0]))\n    print ""numpy linalg fn: {}"".format(euclidean_distance_np(np_x))\n    '"
intro/2-matplotlib.py,1,"b'import matplotlib.pyplot as plt\nfrom matplotlib import style\nimport numpy as np\n\nstyle.use(\'ggplot\')\n\nx = np.arange(16) - 2\ny = x.reshape((4, 4))\n\n\nprint ""y matrix: \\n{}"".format(y)\nprint ""\\nfirst column of each row ""\nprint y[:, 0]\nprint ""second column of each row ""\nprint y[:, 1]\n\nplt.scatter(y[:, 0], y[:, 1], color=""g"", marker=\'o\', linewidths=40)\n\nplt.show()\n\n\n'"
intro/3-sklearn.py,0,"b'from sklearn.datasets import load_iris\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection.tests.test_split import train_test_split\nimport numpy\n\n# Load huge dataset into dataset variable\ndataset = load_iris()\n\nprint dataset.data # Features\nprint dataset.target # label for each feature\nprint dataset.target_names # Label names\n\n# Separate data 2 parts, 1. For train 2. For testing\n\ntrain_features, test_features, train_labels, test_labels = train_test_split(dataset.data, dataset.target, test_size=.5)\n\n# Create classifier instance\nclassifier = tree.DecisionTreeClassifier()\n\n# Fit data and train\nclassifier.fit(train_features, train_labels)\n\n# Now let\'s predict,\n# Features from https://ru.wikipedia.org/wiki/%D0%98%D1%80%D0%B8%D1%81%D1%8B_%D0%A4%D0%B8%D1%88%D0%B5%D1%80%D0%B0\nprint classifier.predict([4.4, 3.0, 1.3, 0.2])\n\n# Check accuracy\nprint ""Accuracy: {}"".format(accuracy_score(test_labels, classifier.predict(test_features)))\n'"
