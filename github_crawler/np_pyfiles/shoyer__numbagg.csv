file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nCLASSIFIERS = [\n    ""Development Status :: 3 - Alpha"",\n    ""License :: OSI Approved :: BSD License"",\n    ""Operating System :: OS Independent"",\n    ""Intended Audience :: Science/Research"",\n    ""Programming Language :: Python"",\n    ""Programming Language :: Python :: 3"",\n    ""Programming Language :: Python :: 3.5"",\n    ""Programming Language :: Python :: 3.6"",\n    ""Programming Language :: Python :: 3.7"",\n    ""Programming Language :: Python :: 3 :: Only"",\n    ""Topic :: Scientific/Engineering"",\n]\n\nDESCRIPTION = ""Fast N-dimensional aggregation functions with Numba""\n\nsetup(\n    name=""numbagg"",\n    version=""0.1"",\n    license=""BSD"",\n    author=""Stephan Hoyer"",\n    author_email=""shoyer@gmail.com"",\n    classifiers=CLASSIFIERS,\n    description=DESCRIPTION,\n    install_requires=[""numpy"", ""numba""],\n    tests_require=[""pytest"", ""bottleneck"", ""pandas""],\n    python_requires="">=3.5"",\n    url=""https://github.com/shoyer/numbagg"",\n    test_suite=""pytest"",\n    packages=find_packages(),\n)\n'"
numbagg/__init__.py,0,"b'from .funcs import (\n    allnan,\n    anynan,\n    count,\n    nanargmax,\n    nanargmin,\n    nanmax,\n    nanmean,\n    nanstd,\n    nanvar,\n    nanmin,\n    nansum,\n)\nfrom .moving import move_exp_nanmean, move_mean\n'"
numbagg/cache.py,0,"b'class FunctionCache(dict):\n    """"""A simple dict-subclass for caching the return values of a function.\n    """"""\n\n    def __init__(self, func):\n        self.func = func\n\n    def __missing__(self, key):\n        value = self[key] = self.func(key)\n        return value\n\n\nclass cached_property(object):\n    """"""A property that is only computed once per instance and then replaces\n    itself with an ordinary attribute. Deleting the attribute resets the\n    property.\n\n    Source:\n    https://github.com/pydanny/cached-property\n    https://github.com/bottlepy/bottle/commit/fa7733e075da0d790d809aa3d2f53071897e6f76\n    """"""\n\n    def __init__(self, func):\n        self.__doc__ = getattr(func, ""__doc__"")\n        self.func = func\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            return self\n        value = obj.__dict__[self.func.__name__] = self.func(obj)\n        return value\n'"
numbagg/decorators.py,13,"b'import numbers\nimport numba\nimport numpy as np\n\nfrom .cache import cached_property, FunctionCache\nfrom .transform import rewrite_ndreduce\n\n\ndef _nd_func_maker(cls, arg, **kwargs):\n    if callable(arg) and not kwargs:\n        return cls(arg)\n    else:\n        return lambda func: cls(func, signature=arg, **kwargs)\n\n\ndef ndreduce(*args, **kwargs):\n    """"""Create an N-dimensional aggregation function.\n\n    Functions should have signatures of the form output_type(input_type), where\n    input_type and output_type are numba dtypes. This decorator rewrites them\n    to accept input arrays of arbitrary dimensionality, with an additional\n    optional `axis`, which accepts integers or tuples of integers (defaulting\n    to `axis=None` for all axes).\n\n    For example, to write a simplified version of `np.sum(arr, axis=None)`::\n\n        from numba import float64\n\n        @ndreduce([\n            float64(float64)\n        ])\n        def sum(a):\n            asum = 0.0\n            for ai in a.flat:\n                asum += ai\n            return asum\n    """"""\n    return _nd_func_maker(NumbaNDReduce, *args, **kwargs)\n\n\ndef ndmoving(*args, **kwargs):\n    """"""Create an N-dimensional moving window function along one dimension.\n\n    Functions should accept arguments for the input array, a window\n    size and the output array.\n\n    For example, to write a simplified (and naively implemented) moving window\n    sum::\n\n        @ndmoving([\n            (float64[:], int64, int64, float64[:]),\n        ])\n        def move_sum(a, window, min_count, out):\n            for i in range(a.size):\n                for j in range(window):\n                    if i - j > min_count:\n                        out[i] += a[i - j]\n    """"""\n    return _nd_func_maker(NumbaNDMoving, *args, **kwargs)\n\n\ndef ndmovingexp(*args, **kwargs):\n    """"""N-dimensional exponential moving window function.""""""\n    return _nd_func_maker(NumbaNDMovingExp, *args, **kwargs)\n\n\ndef groupndreduce(*args, **kwargs):\n    """"""Create an N-dimensional grouped aggregation function.""""""\n    return _nd_func_maker(NumbaGroupNDReduce, *args, **kwargs)\n\n\ndef _validate_axis(axis, ndim):\n    """"""Helper function to convert axis into a non-negative integer, or raise if\n    it\'s invalid.\n    """"""\n    if axis < 0:\n        axis += ndim\n    if axis < 0 or axis >= ndim:\n        raise ValueError(""invalid axis %s"" % axis)\n    return axis\n\n\ndef ndim(arg):\n    return getattr(arg, ""ndim"", 0)\n\n\n_ALPHABET = ""abcdefghijkmnopqrstuvwxyz""\n\n\ndef _gufunc_arg_str(arg):\n    return ""(%s)"" % "","".join(_ALPHABET[: ndim(arg)])\n\n\ndef gufunc_string_signature(numba_args):\n    """"""Convert a tuple of numba types into a numpy gufunc signature.\n\n    The last type is used as output argument.\n\n    Example:\n\n    >>> gufunc_string_signature((float64[:], float64))\n    \'(a)->()\'\n    """"""\n    return (\n        "","".join(map(_gufunc_arg_str, numba_args[:-1]))\n        + ""->""\n        + _gufunc_arg_str(numba_args[-1])\n    )\n\n\nclass NumbaNDReduce(object):\n    def __init__(self, func, signature):\n        self.func = func\n\n        for sig in signature:\n            if not hasattr(sig, ""return_type""):\n                raise ValueError(\n                    ""signatures for ndreduce must be functions: {}"".format(signature)\n                )\n            if any(ndim(arg) != 0 for arg in sig.args):\n                raise ValueError(\n                    ""all arguments in signature for ndreduce must be scalars: ""\n                    "" {}"".format(signature)\n                )\n            if ndim(sig.return_type) != 0:\n                raise ValueError(\n                    ""return type for ndreduce must be a scalar: {}"".format(signature)\n                )\n        self.signature = signature\n\n        self._gufunc_cache = FunctionCache(self._create_gufunc)\n\n    @property\n    def __name__(self):\n        return self.func.__name__\n\n    def __repr__(self):\n        return ""<numbagg.decorators.NumbaNDReduce %s>"" % self.__name__\n\n    @cached_property\n    def transformed_func(self):\n        return rewrite_ndreduce(self.func)\n\n    @cached_property\n    def _jit_func(self):\n        vectorize = numba.jit(self.signature, nopython=True)\n        return vectorize(self.func)\n\n    def _create_gufunc(self, core_ndim):\n        # creating compiling gufunc has some significant overhead (~130ms per\n        # function and number of dimensions to aggregate), so do this in a\n        # lazy fashion\n        numba_sig = []\n        for input_sig in self.signature:\n            new_sig = (\n                (input_sig.args[0][(slice(None),) * max(core_ndim, 1)],)\n                + input_sig.args[1:]\n                + (input_sig.return_type[:],)\n            )\n            numba_sig.append(new_sig)\n\n        first_sig = self.signature[0]\n        gufunc_sig = gufunc_string_signature(\n            (\n                first_sig.args[0][(slice(None),) * core_ndim]\n                if core_ndim\n                else first_sig.args[0],\n            )\n            + first_sig.args[1:]\n            + (first_sig.return_type,)\n        )\n\n        vectorize = numba.guvectorize(numba_sig, gufunc_sig, nopython=True)\n        return vectorize(self.transformed_func)\n\n    def __call__(self, arr, axis=None):\n        if axis is None:\n            # TODO: switch to using jit_func (it\'s faster), once numba reliably\n            # returns the right dtype\n            # see: https://github.com/numba/numba/issues/1087\n            # f = self._jit_func\n            f = self._gufunc_cache[arr.ndim]\n        elif isinstance(axis, numbers.Number):\n            arr = np.moveaxis(arr, axis, -1)\n            f = self._gufunc_cache[1]\n        else:\n            arr = np.moveaxis(arr, axis, range(-len(axis), 0, 1))\n            f = self._gufunc_cache[len(axis)]\n        return f(arr)\n\n\nMOVE_WINDOW_ERR_MSG = ""invalid window (not between 1 and %d, inclusive): %r""\n\n\ndef rolling_validator(arr, window):\n    if (window < 1) or (window > arr.shape[-1]):\n        raise ValueError(MOVE_WINDOW_ERR_MSG % (arr.shape[-1], window))\n\n\nDEFAULT_MOVING_SIGNATURE = ((numba.float64[:], numba.int64, numba.float64[:]),)\n\n\nclass NumbaNDMoving(object):\n    def __init__(\n        self,\n        func,\n        signature=DEFAULT_MOVING_SIGNATURE,\n        window_validator=rolling_validator,\n    ):\n        self.func = func\n        self.window_validator = window_validator\n\n        for sig in signature:\n            if not isinstance(sig, tuple):\n                raise TypeError(\n                    ""signatures for ndmoving must be tuples: {}"".format(signature)\n                )\n        self.signature = signature\n\n    @property\n    def __name__(self):\n        return self.func.__name__\n\n    def __repr__(self):\n        return ""<numbagg.decorators.%s %s>"" % (type(self).__name__, self.__name__)\n\n    @cached_property\n    def gufunc(self):\n        gufunc_sig = gufunc_string_signature(self.signature[0])\n        vectorize = numba.guvectorize(self.signature, gufunc_sig, nopython=True)\n        return vectorize(self.func)\n\n    def __call__(self, arr, window, min_count=None, axis=-1):\n        if min_count is None:\n            min_count = window\n        if not 0 < window < arr.shape[axis]:\n            raise ValueError(""window not in valid range: {}"".format(window))\n        if min_count < 0:\n            raise ValueError(""min_count must be positive: {}"".format(min_count))\n        axis = _validate_axis(axis, arr.ndim)\n        arr = np.moveaxis(arr, axis, -1)\n        result = self.gufunc(arr, window, min_count)\n        return np.moveaxis(result, -1, axis)\n\n\nclass NumbaNDMovingExp(NumbaNDMoving):\n    def __call__(self, arr, alpha, axis=-1):\n        if alpha < 0:\n            raise ValueError(""alpha must be positive: {}"".format(alpha))\n        axis = _validate_axis(axis, arr.ndim)\n        arr = np.moveaxis(arr, axis, -1)\n        result = self.gufunc(arr, alpha)\n        return np.moveaxis(result, -1, axis)\n\n\nclass NumbaGroupNDReduce(object):\n    def __init__(self, func, signature=DEFAULT_MOVING_SIGNATURE):\n        self.func = func\n\n        for sig in signature:\n            if not isinstance(sig, tuple):\n                raise TypeError(\n                    ""signatures for ndmoving must be tuples: {}"".format(signature)\n                )\n            if len(sig) != 3:\n                raise TypeError(\n                    ""signature has wrong number of argument != 3: ""\n                    ""{}"".format(signature)\n                )\n            if any(ndim(arg) != 0 for arg in sig):\n                raise ValueError(\n                    ""all arguments in signature for ndreduce must be scalars: ""\n                    "" {}"".format(signature)\n                )\n        self.signature = signature\n        self._gufunc_cache = FunctionCache(self._create_gufunc)\n\n    @property\n    def __name__(self):\n        return self.func.__name__\n\n    def __repr__(self):\n        return ""<numbagg.decorators.NumbaGroupNDReduce %s>"" % self.__name__\n\n    def _create_gufunc(self, core_ndim):\n        # compiling gufuncs has some significant overhead (~130ms per function\n        # and number of dimensions to aggregate), so do this in a lazy fashion\n        numba_sig = []\n        slices = (slice(None),) * core_ndim\n        for input_sig in self.signature:\n            values, labels, out = input_sig\n            new_sig = (values[slices], labels[slices], out[:])\n            numba_sig.append(new_sig)\n\n        first_sig = numba_sig[0]\n        gufunc_sig = "","".join(2 * [_gufunc_arg_str(first_sig[0])]) + "",(z)""\n        vectorize = numba.guvectorize(numba_sig, gufunc_sig, nopython=True)\n        return vectorize(self.func)\n\n    def __call__(self, values, labels, axis=None, num_labels=None):\n        values = np.asarray(values)\n        labels = np.asarray(labels)\n\n        if num_labels is None:\n            num_labels = np.max(labels) + 1\n\n        if axis is None:\n            if values.shape != labels.shape:\n                raise ValueError(\n                    ""axis required if values and labels have different ""\n                    ""shapes: {} vs {}"".format(values.shape, labels.shape)\n                )\n            gufunc = self._gufunc_cache[values.ndim]\n        elif isinstance(axis, numbers.Number):\n            if labels.shape != (values.shape[axis],):\n                raise ValueError(\n                    ""values must have same shape along axis as labels: ""\n                    ""{} vs {}"".format((values.shape[axis],), labels.shape)\n                )\n            values = np.moveaxis(values, axis, -1)\n            gufunc = self._gufunc_cache[1]\n        else:\n            values_shape = tuple(values.shape[ax] for ax in axis)\n            if labels.shape != values_shape:\n                raise ValueError(\n                    ""values must have same shape along axis as labels: ""\n                    ""{} vs {}"".format(values_shape, labels.shape)\n                )\n            values = np.moveaxis(values, axis, range(-len(axis), 0, 1))\n            gufunc = self._gufunc_cache[len(axis)]\n\n        broadcast_ndim = values.ndim - labels.ndim\n        broadcast_shape = values.shape[:broadcast_ndim]\n        result = np.zeros(broadcast_shape + (num_labels,), values.dtype)\n        gufunc(values, labels, result)\n        return result\n'"
numbagg/funcs.py,21,"b'import numpy as np\nfrom numba import bool_, float32, float64, int32, int64\n\nfrom .decorators import ndreduce\n\n\n@ndreduce([bool_(int32), bool_(int64), bool_(float32), bool_(float64)])\ndef allnan(a):\n    f = True\n    for ai in a.flat:\n        if not np.isnan(ai):\n            f = False\n            break\n    return f\n\n\n@ndreduce([bool_(int32), bool_(int64), bool_(float32), bool_(float64)])\ndef anynan(a):\n    f = False\n    for ai in a.flat:\n        if np.isnan(ai):\n            f = True\n            break\n    return f\n\n\n@ndreduce([int64(int32), int64(int64), int64(float32), int64(float64)])\ndef count(a):\n    non_missing = 0\n    for ai in a.flat:\n        if not np.isnan(ai):\n            non_missing += 1\n    return non_missing\n\n\n@ndreduce([int32(int32), int64(int64), float32(float32), float64(float64)])\ndef nansum(a):\n    asum = 0\n    for ai in a.flat:\n        if not np.isnan(ai):\n            asum += ai\n    return asum\n\n\n@ndreduce([float32(float32), float64(float64)])\ndef nanmean(a):\n    asum = 0.0\n    count = 0\n    for ai in a.flat:\n        if not np.isnan(ai):\n            asum += ai\n            count += 1\n    if count > 0:\n        return asum / count\n    else:\n        return np.nan\n\n\n@ndreduce([float32(float32), float64(float64)])\ndef nanstd(a):\n    # for now, fix ddof=0\n    ddof = 0\n    asum = 0\n    count = 0\n    for ai in a.flat:\n        if not np.isnan(ai):\n            asum += ai\n            count += 1\n    if count > ddof:\n        amean = asum / count\n        asum = 0\n        for ai in a.flat:\n            if not np.isnan(ai):\n                ai -= amean\n                asum += ai * ai\n        return np.sqrt(asum / (count - ddof))\n    else:\n        return np.nan\n\n\n@ndreduce([float32(float32), float64(float64)])\ndef nanvar(a):\n    # for now, fix ddof=0\n    ddof = 0\n    asum = 0\n    count = 0\n    for ai in a.flat:\n        if not np.isnan(ai):\n            asum += ai\n            count += 1\n    if count > ddof:\n        amean = asum / count\n        asum = 0\n        for ai in a.flat:\n            if not np.isnan(ai):\n                ai -= amean\n                asum += ai * ai\n        return asum / (count - ddof)\n    else:\n        return np.nan\n\n\n@ndreduce([int64(int32), int64(int64), int64(float32), int64(float64)])\ndef nanargmax(a):\n    if not a.size:\n        raise ValueError(""attempt to get argmax of an empty sequence"")\n    amax = -np.infty\n    idx = -1\n    for i, ai in enumerate(a.flat):\n        if ai > amax or (idx == -1 and not np.isnan(ai)):\n            amax = ai\n            idx = i\n    if idx == -1:\n        raise ValueError(""All-NaN slice encountered"")\n    return idx\n\n\n@ndreduce([int64(int32), int64(int64), int64(float32), int64(float64)])\ndef nanargmin(a):\n    if not a.size:\n        raise ValueError(""attempt to get argmin of an empty sequence"")\n    amin = np.infty\n    idx = -1\n    for i, ai in enumerate(a.flat):\n        if ai < amin or (idx == -1 and not np.isnan(ai)):\n            amin = ai\n            idx = i\n    if idx == -1:\n        raise ValueError(""All-NaN slice encountered"")\n    return idx\n\n\n@ndreduce([int64(int32), int64(int64), float32(float32), float64(float64)])\ndef nanmax(a):\n    if not a.size:\n        raise ValueError(\n            ""zero-size array to reduction operation fmax which has no identity""\n        )\n    amax = -np.infty\n    all_missing = 1\n    for ai in a.flat:\n        if ai >= amax:\n            amax = ai\n            all_missing = 0\n    if all_missing:\n        amax = np.nan\n    return amax\n\n\n@ndreduce([int64(int32), int64(int64), float32(float32), float64(float64)])\ndef nanmin(a):\n    if not a.size:\n        raise ValueError(\n            ""zero-size array to reduction operation fmin which has no identity""\n        )\n    amin = np.infty\n    all_missing = 1\n    for ai in a.flat:\n        if ai <= amin:\n            amin = ai\n            all_missing = 0\n    if all_missing:\n        amin = np.nan\n    return amin\n'"
numbagg/grouped.py,4,"b'from numba import float64, float32, int64, int32\nimport numpy as np\n\n\nfrom .decorators import groupndreduce\n\n\n@groupndreduce(\n    [\n        (float32, int32, float32),\n        (float32, int64, float32),\n        (float64, int32, float64),\n        (float64, int64, float64),\n    ]\n)\ndef group_nanmean(values, labels, out):\n    counts = np.zeros(out.shape, dtype=labels.dtype)\n\n    for indices in np.ndindex(values.shape):\n        label = labels[indices]\n        if label < 0:\n            continue\n\n        value = values[indices]\n        if not np.isnan(value):\n            counts[label] += 1\n            out[label] += value\n\n    for label in range(len(out)):\n        count = counts[label]\n        if count == 0:\n            out[label] = np.nan\n        else:\n            out[label] /= count\n'"
numbagg/moving.py,12,"b'import numpy as np\nfrom numba import float32, float64, int64, int32\n\nfrom .decorators import ndmoving, ndmovingexp\n\n\n@ndmovingexp([(float32[:], float32, float32[:]), (float64[:], float64, float64[:])])\ndef move_exp_nanmean(a, alpha, out):\n\n    N = len(a)\n    if N == 0:\n        return\n\n    old_wt_factor = 1.0 - alpha\n    new_wt = 1.0\n    ignore_na = False  # could add as option in the future\n\n    weighted_avg = a[0]\n    is_observation = not np.isnan(weighted_avg)\n    nobs = int(is_observation)\n    out[0] = weighted_avg\n    old_wt = 1.0\n\n    for i in range(1, N):\n        cur = a[i]\n        is_observation = not np.isnan(cur)\n        nobs += int(is_observation)\n        if not np.isnan(weighted_avg):\n            if is_observation or (not ignore_na):\n                old_wt *= old_wt_factor\n\n                if is_observation:\n                    # avoid numerical errors on constant series\n                    if weighted_avg != cur:\n                        weighted_avg = ((old_wt * weighted_avg) + (new_wt * cur)) / (\n                            old_wt + new_wt\n                        )\n                    old_wt += new_wt\n        elif is_observation:\n            weighted_avg = cur\n\n        out[i] = weighted_avg\n\n\n@ndmoving(\n    [(float32[:], int64, int64, float32[:]), (float64[:], int64, int64, float64[:])]\n)\ndef move_mean(a, window, min_count, out):\n\n    asum = 0.0\n    count = 0\n\n    for i in range(min_count - 1):\n        ai = a[i]\n        if not np.isnan(ai):\n            asum += ai\n            count += 1\n        out[i] = np.nan\n\n    for i in range(min_count - 1, window):\n        ai = a[i]\n        if not np.isnan(ai):\n            asum += ai\n            count += 1\n        out[i] = asum / count if count >= min_count else np.nan\n\n    count_inv = 1 / count if count >= min_count else np.nan\n    for i in range(window, len(a)):\n        ai = a[i]\n        aold = a[i - window]\n\n        ai_valid = not np.isnan(ai)\n        aold_valid = not np.isnan(aold)\n\n        if ai_valid and aold_valid:\n            asum += ai - aold\n        elif ai_valid:\n            asum += ai\n            count += 1\n            count_inv = 1 / count if count >= min_count else np.nan\n        elif aold_valid:\n            asum -= aold\n            count -= 1\n            count_inv = 1 / count if count >= min_count else np.nan\n\n        out[i] = asum * count_inv\n'"
numbagg/transform.py,0,"b'import ast\nimport inspect\nimport sys\n\n\ndef rewrite_ndreduce(func):\n    """"""Transforms aggregation functions into something numba can handle.\n\n    To be more precise, it converts functions with source that looks like\n\n        @ndreduce\n        def my_func(x)\n            ...\n            return foo\n\n    into\n\n        def __sub__gufunc(x, __out):\n            ...\n            __out[0] = foo\n\n    which is the form numba needs for writing a gufunc that returns a scalar\n    value.\n    """"""\n    return _apply_ast_rewrite(func, _NDReduceTransformer())\n\n\n_OUT_NAME = ""__numbagg_out""\n_TRANFORMED_FUNC_NAME = ""__numbagg_transformed_func""\n\n\ndef _apply_ast_rewrite(func, node_transformer):\n    """"""A hack to make the syntax for writing aggregators more Pythonic.\n\n    This should go away once numba is more fully featured.\n    """"""\n    orig_source = inspect.getsource(func)\n\n    tree = ast.parse(orig_source)\n    tree = node_transformer.visit(tree)\n    ast.fix_missing_locations(tree)\n    source = compile(tree, filename=""<ast>"", mode=""exec"")\n\n    scope = {}\n    exec(source, func.__globals__, scope)\n    try:\n        return scope[_TRANFORMED_FUNC_NAME]\n    except KeyError:\n        raise TypeError(""failed to rewrite function definition:\\n%s"" % orig_source)\n\n\nclass _NDReduceTransformer(ast.NodeTransformer):\n    def visit_FunctionDef(self, node):\n        args = node.args.args + [ast.arg(arg=_OUT_NAME, annotation=None)]\n        arguments = ast.arguments(\n            args=args,\n            vararg=None,\n            kwonlyargs=[],\n            kw_defaults=[],\n            kwarg=None,\n            defaults=[],\n        )\n        function_def = ast.FunctionDef(\n            name=_TRANFORMED_FUNC_NAME,\n            args=arguments,\n            body=node.body,\n            decorator_list=[],\n        )\n        return self.generic_visit(function_def)\n\n    def visit_Return(self, node):\n        subscript = ast.Subscript(\n            value=ast.Name(id=_OUT_NAME, ctx=ast.Load()),\n            slice=ast.Index(value=ast.Num(n=0)),\n            ctx=ast.Store(),\n        )\n        assign = ast.Assign(targets=[subscript], value=node.value)\n        return assign\n'"
numbagg/test/__init__.py,0,b''
numbagg/test/test_funcs.py,18,"b'import pytest\nimport numbagg\n\nimport bottleneck as bn\nimport numpy as np\n\nfrom numpy.testing import assert_array_almost_equal, assert_array_equal, assert_equal\nfrom .util import arrays\n\n\ndef functions():\n    # TODO: test tuple axes\n    yield numbagg.nansum, np.nansum, np.inf\n    yield numbagg.nanmax, np.nanmax, np.inf\n    yield numbagg.nanargmin, np.nanargmin, np.inf\n    yield numbagg.nanargmax, np.nanargmax, np.inf\n    yield numbagg.nanmin, np.nanmin, np.inf\n    yield numbagg.nanmean, np.nanmean, 5\n    yield numbagg.nanstd, np.nanstd, 5\n    yield numbagg.nanvar, np.nanvar, 5\n    yield numbagg.anynan, bn.anynan, np.inf\n    yield numbagg.allnan, bn.allnan, np.inf\n    yield numbagg.count, slow_count, np.inf\n\n\n@pytest.mark.filterwarnings(""ignore:Degrees of freedom <= 0 for slice"")\n@pytest.mark.filterwarnings(""ignore:All-NaN slice encountered"")\n@pytest.mark.filterwarnings(""ignore:Mean of empty slice"")\n@pytest.mark.parametrize(""func,func0,decimal"", functions())\ndef test_numerical_results_identical(func, func0, decimal):\n    ""Test that bn.xxx gives the same output as bn.slow.xxx.""\n    msg = ""\\nfunc %s | input %s (%s) | shape %s | axis %s\\n""\n    msg += ""\\nInput array:\\n%s\\n""\n    for i, arr in enumerate(arrays(func.__name__)):\n        for axis in list(range(-arr.ndim, arr.ndim)) + [None]:\n            with np.errstate(invalid=""ignore""):\n                desiredraised = False\n                desired_arr = arr.copy()\n                if desired_arr.dtype == np.float16:\n                    # don\'t use float16 for computation\n                    desired_arr = desired_arr.astype(np.float32)\n                try:\n                    desired = func0(desired_arr, axis=axis)\n                except Exception as err:\n                    desired = str(err)\n                    desiredraised = True\n                actualraised = False\n                try:\n                    actual = func(arr.copy(), axis=axis)\n                except Exception as err:\n                    if not desiredraised:\n                        raise\n                    actual = str(err)\n                    actualraised = True\n            if actualraised and desiredraised:\n                assert desired == actual\n            elif desiredraised and actual.size == 0:\n                # there are no array values, so don\'t worry about not raising\n                pass\n            else:\n                assert actualraised == desiredraised\n\n                actual = np.asarray(actual)\n                desired = np.asarray(desired)\n\n                tup = (\n                    func.__name__,\n                    ""a"" + str(i),\n                    str(arr.dtype),\n                    str(arr.shape),\n                    str(axis),\n                    arr,\n                )\n                err_msg = msg % tup\n                if (decimal < np.inf) and (np.isfinite(arr).sum() > 0):\n                    assert_array_almost_equal(actual, desired, decimal, err_msg)\n                else:\n                    assert_array_equal(actual, desired, err_msg)\n\n                err_msg += ""\\n dtype mismatch %s %s""\n                da = actual.dtype\n                dd = desired.dtype\n                assert_equal(da, dd, err_msg % (da, dd))\n\n\ndef slow_count(x, axis=None):\n    return np.sum(~np.isnan(x), axis=axis)\n'"
numbagg/test/test_grouped.py,10,"b'import pandas as pd\nimport numpy as np\nfrom numpy.testing import assert_almost_equal\nimport pytest\n\nfrom numbagg.grouped import group_nanmean\n\n\ndef groupby_mean_pandas(values, group):\n    # like pd.Series(values).groupby(group).mean()\n    labels, uniques = pd.factorize(group, sort=True)\n    result = group_nanmean(values, labels, num_labels=len(uniques))\n    return pd.Series(result, index=uniques)\n\n\ndef test_groupby_mean_pandas():\n    rs = np.random.RandomState(0)\n    values = rs.rand(2000)\n    group = rs.choice([np.nan, 1, 2, 3, 4, 5], size=values.shape)\n    expected = pd.Series(values).groupby(group).mean()\n    result = groupby_mean_pandas(values, group)\n    assert_almost_equal(expected.values, result.values)\n\n\ndef test_group_nanmean_axis_1d_labels():\n    values = np.arange(5.0)\n    labels = np.arange(5)\n    result = group_nanmean(values, labels)\n    assert_almost_equal(values, result)\n\n    values = np.arange(25.0).reshape(5, 5)\n    labels = np.arange(5)\n\n    with pytest.raises(ValueError) as excinfo:\n        result = group_nanmean(values, labels)\n    assert ""axis required"" in str(excinfo.value)\n\n    result = group_nanmean(values, labels, axis=1)\n    assert_almost_equal(values, result)\n\n    result = group_nanmean(values, labels, axis=(1,))\n    assert_almost_equal(values, result)\n\n    result = group_nanmean(values, labels, axis=0)\n    assert_almost_equal(values.T, result)\n\n    with pytest.raises(ValueError) as excinfo:\n        group_nanmean(values, labels[:4], axis=0)\n    assert ""must have same shape"" in str(excinfo.value)\n\n    with pytest.raises(ValueError) as excinfo:\n        group_nanmean(values, labels[:4], axis=(0,))\n    assert ""must have same shape"" in str(excinfo.value)\n\n    result = group_nanmean(values, [0, 0, -1, 1, 1], axis=1)\n    expected = np.stack(\n        [values[:, :2].mean(axis=1), values[:, 3:].mean(axis=1)], axis=-1\n    )\n    assert_almost_equal(expected, result)\n\n\ndef test_group_nanmean_axis_2d_labels():\n    values = np.arange(25.0).reshape(5, 5)\n    labels = np.arange(25).reshape(5, 5)\n    result = group_nanmean(values, labels)\n    assert_almost_equal(values.ravel(), result)\n\n    values = np.arange(125.0).reshape(5, 5, 5)\n    result = group_nanmean(values, labels, axis=(1, 2))\n    assert_almost_equal(values.reshape(5, -1), result)\n'"
numbagg/test/test_moving.py,47,"b'import warnings\n\nimport numpy as np\nimport pandas as pd\nimport pytest\nfrom numpy.testing import assert_almost_equal\n\nfrom numbagg import move_mean, move_exp_nanmean\nfrom .util import arrays, array_order\n\n\n@pytest.fixture\ndef rand_array():\n    arr = np.random.RandomState(0).rand(2000).reshape(10, -1)\n    return np.where(arr > 0.1, arr, np.nan)\n\n\n@pytest.mark.parametrize(""alpha"", [0.5, 0.1])\ndef test_move_exp_nanmean(rand_array, alpha):\n\n    array = rand_array[0]\n    expected = pd.Series(array).ewm(alpha=alpha).mean()\n    result = move_exp_nanmean(array, alpha)\n\n    assert_almost_equal(expected, result)\n\n\ndef test_move_exp_nanmean_2d(rand_array):\n\n    expected = pd.DataFrame(rand_array).T.ewm(alpha=0.1).mean().T\n    result = move_exp_nanmean(rand_array, 0.1)\n\n    assert_almost_equal(expected, result)\n\n\ndef test_move_mean():\n    array = np.arange(100.0)\n    array[::7] = np.nan\n\n    expected = pd.Series(array).rolling(window=5, min_periods=1).mean().values\n    result = move_mean(array, 5, min_count=1)\n    assert_almost_equal(expected, result)\n\n\ndef test_move_mean_random(rand_array):\n    array = rand_array[0]\n\n    expected = pd.Series(array).rolling(window=10, min_periods=1).mean().values\n    result = move_mean(array, 10, min_count=1)\n    assert_almost_equal(expected, result)\n\n    expected = pd.Series(array).rolling(window=3, min_periods=3).mean().values\n    result = move_mean(array, 3, min_count=3)\n    assert_almost_equal(expected, result)\n\n\ndef test_move_mean_window(rand_array):\n\n    with pytest.raises(TypeError):\n        move_mean(rand_array, window=0.5)\n    with pytest.raises(ValueError):\n        move_mean(rand_array, window=-1)\n    with pytest.raises(ValueError):\n        move_mean(rand_array, window=1, min_count=-1)\n\n\ndef functions():\n    yield move_mean, slow_move_mean\n\n\n@pytest.mark.parametrize(""func,func0"", functions())\ndef test_numerical_results_identical(func, func0):\n    ""Test that bn.xxx gives the same output as a reference function.""\n    fmt = (\n        ""\\nfunc %s | window %d | min_count %s | input %s (%s) | shape %s | ""\n        ""axis %s | order %s\\n""\n    )\n    fmt += ""\\nInput array:\\n%s\\n""\n    func_name = func.__name__\n    if func_name == ""move_var"":\n        decimal = 3\n    else:\n        decimal = 5\n    for i, a in enumerate(arrays(func_name)):\n        axes = range(-1, a.ndim)\n        for axis in axes:\n            windows = range(1, a.shape[axis])\n            for window in windows:\n                min_counts = list(range(1, window + 1)) + [None]\n                for min_count in min_counts:\n                    actual = func(a, window, min_count, axis=axis)\n                    desired_a = a.astype(np.float32) if a.dtype == np.float16 else a\n                    desired = func0(desired_a, window, min_count, axis=axis)\n                    tup = (\n                        func_name,\n                        window,\n                        str(min_count),\n                        ""a"" + str(i),\n                        str(a.dtype),\n                        str(a.shape),\n                        str(axis),\n                        array_order(a),\n                        a,\n                    )\n                    err_msg = fmt % tup\n                    np.testing.assert_array_almost_equal(\n                        actual, desired, decimal, err_msg\n                    )\n                    err_msg += ""\\n dtype mismatch %s %s""\n                    da = actual.dtype\n                    dd = desired.dtype\n                    # don\'t require an exact dtype match, since we don\'t care\n                    # about endianness of the result\n                    assert da.kind == dd.kind, err_msg % (da, dd)\n                    assert da.itemsize == dd.itemsize, err_msg % (da, dd)\n\n\ndef slow_move_sum(a, window, min_count=None, axis=-1):\n    ""Slow move_sum for unaccelerated dtype""\n    return move_func(np.nansum, a, window, min_count, axis=axis)\n\n\ndef slow_move_mean(a, window, min_count=None, axis=-1):\n    ""Slow move_mean for unaccelerated dtype""\n    return move_func(np.nanmean, a, window, min_count, axis=axis)\n\n\ndef slow_move_std(a, window, min_count=None, axis=-1, ddof=0):\n    ""Slow move_std for unaccelerated dtype""\n    return move_func(np.nanstd, a, window, min_count, axis=axis, ddof=ddof)\n\n\ndef slow_move_var(a, window, min_count=None, axis=-1, ddof=0):\n    ""Slow move_var for unaccelerated dtype""\n    return move_func(np.nanvar, a, window, min_count, axis=axis, ddof=ddof)\n\n\ndef slow_move_min(a, window, min_count=None, axis=-1):\n    ""Slow move_min for unaccelerated dtype""\n    return move_func(np.nanmin, a, window, min_count, axis=axis)\n\n\ndef slow_move_max(a, window, min_count=None, axis=-1):\n    ""Slow move_max for unaccelerated dtype""\n    return move_func(np.nanmax, a, window, min_count, axis=axis)\n\n\ndef slow_move_argmin(a, window, min_count=None, axis=-1):\n    ""Slow move_argmin for unaccelerated dtype""\n\n    def argmin(a, axis):\n        a = np.array(a, copy=False)\n        flip = [slice(None)] * a.ndim\n        flip[axis] = slice(None, None, -1)\n        a = a[flip]  # if tie, pick index of rightmost tie\n        try:\n            idx = np.nanargmin(a, axis=axis)\n        except ValueError:\n            # an all nan slice encountered\n            a = a.copy()\n            mask = np.isnan(a)\n            np.copyto(a, np.inf, where=mask)\n            idx = np.argmin(a, axis=axis).astype(np.float64)\n            if idx.ndim == 0:\n                idx = np.nan\n            else:\n                mask = np.all(mask, axis=axis)\n                idx[mask] = np.nan\n        return idx\n\n    return move_func(argmin, a, window, min_count, axis=axis)\n\n\ndef slow_move_argmax(a, window, min_count=None, axis=-1):\n    ""Slow move_argmax for unaccelerated dtype""\n\n    def argmax(a, axis):\n        a = np.array(a, copy=False)\n        flip = [slice(None)] * a.ndim\n        flip[axis] = slice(None, None, -1)\n        a = a[flip]  # if tie, pick index of rightmost tie\n        try:\n            idx = np.nanargmax(a, axis=axis)\n        except ValueError:\n            # an all nan slice encountered\n            a = a.copy()\n            mask = np.isnan(a)\n            np.copyto(a, -np.inf, where=mask)\n            idx = np.argmax(a, axis=axis).astype(np.float64)\n            if idx.ndim == 0:\n                idx = np.nan\n            else:\n                mask = np.all(mask, axis=axis)\n                idx[mask] = np.nan\n        return idx\n\n    return move_func(argmax, a, window, min_count, axis=axis)\n\n\ndef slow_move_median(a, window, min_count=None, axis=-1):\n    ""Slow move_median for unaccelerated dtype""\n    return move_func(np.nanmedian, a, window, min_count, axis=axis)\n\n\ndef slow_move_rank(a, window, min_count=None, axis=-1):\n    ""Slow move_rank for unaccelerated dtype""\n    return move_func(lastrank, a, window, min_count, axis=axis)\n\n\n# magic utility functions ---------------------------------------------------\n\n\ndef move_func(func, a, window, min_count=None, axis=-1, **kwargs):\n    ""Generic moving window function implemented with a python loop.""\n    a = np.array(a, copy=False)\n    if min_count is None:\n        mc = window\n    else:\n        mc = min_count\n        if mc > window:\n            msg = ""min_count (%d) cannot be greater than window (%d)""\n            raise ValueError(msg % (mc, window))\n        elif mc <= 0:\n            raise ValueError(""`min_count` must be greater than zero."")\n    if a.ndim == 0:\n        raise ValueError(""moving window functions require ndim > 0"")\n    if axis is None:\n        raise ValueError(""An `axis` value of None is not supported."")\n    if window < 1:\n        raise ValueError(""`window` must be at least 1."")\n    if window > a.shape[axis]:\n        raise ValueError(""`window` is too long."")\n    if issubclass(a.dtype.type, np.inexact):\n        y = np.empty_like(a)\n    else:\n        y = np.empty(a.shape)\n    idx1 = [slice(None)] * a.ndim\n    idx2 = list(idx1)\n    with warnings.catch_warnings():\n        warnings.simplefilter(""ignore"")\n        for i in range(a.shape[axis]):\n            win = min(window, i + 1)\n            idx1[axis] = slice(i + 1 - win, i + 1)\n            idx2[axis] = i\n            y[tuple(idx2)] = func(a[tuple(idx1)], axis=axis, **kwargs)\n    idx = _mask(a, window, mc, axis)\n    y[idx] = np.nan\n    return y\n\n\ndef _mask(a, window, min_count, axis):\n    n = (a == a).cumsum(axis)\n    idx1 = [slice(None)] * a.ndim\n    idx2 = [slice(None)] * a.ndim\n    idx3 = [slice(None)] * a.ndim\n    idx1[axis] = slice(window, None)\n    idx2[axis] = slice(None, -window)\n    idx3[axis] = slice(None, window)\n    idx1 = tuple(idx1)\n    idx2 = tuple(idx2)\n    idx3 = tuple(idx3)\n    nidx1 = n[idx1]\n    nidx1 = nidx1 - n[idx2]\n    idx = np.empty(a.shape, dtype=np.bool)\n    idx[idx1] = nidx1 < min_count\n    idx[idx3] = n[idx3] < min_count\n    return idx\n\n\ndef lastrank(a, axis=-1):\n    """"""\n    The ranking of the last element along the axis, ignoring NaNs.\n\n    The ranking is normalized to be between -1 and 1 instead of the more\n    common 1 and N. The results are adjusted for ties.\n\n    Parameters\n    ----------\n    a : ndarray\n        Input array. If `a` is not an array, a conversion is attempted.\n    axis : int, optional\n        The axis over which to rank. By default (axis=-1) the ranking\n        (and reducing) is performed over the last axis.\n\n    Returns\n    -------\n    d : array\n        In the case of, for example, a 2d array of shape (n, m) and\n        axis=1, the output will contain the rank (normalized to be between\n        -1 and 1 and adjusted for ties) of the the last element of each row.\n        The output in this example will have shape (n,).\n\n    Examples\n    --------\n    Create an array:\n\n    >>> y1 = larry([1, 2, 3])\n\n    What is the rank of the last element (the value 3 in this example)?\n    It is the largest element so the rank is 1.0:\n\n    >>> import numpy as np\n    >>> from la.afunc import lastrank\n    >>> x1 = np.array([1, 2, 3])\n    >>> lastrank(x1)\n    1.0\n\n    Now let\'s try an example where the last element has the smallest\n    value:\n\n    >>> x2 = np.array([3, 2, 1])\n    >>> lastrank(x2)\n    -1.0\n\n    Here\'s an example where the last element is not the minimum or maximum\n    value:\n\n    >>> x3 = np.array([1, 3, 4, 5, 2])\n    >>> lastrank(x3)\n    -0.5\n\n    """"""\n    a = np.array(a, copy=False)\n    ndim = a.ndim\n    if a.size == 0:\n        # At least one dimension has length 0\n        shape = list(a.shape)\n        shape.pop(axis)\n        r = np.empty(shape, dtype=a.dtype)\n        r.fill(np.nan)\n        if (r.ndim == 0) and (r.size == 1):\n            r = np.nan\n        return r\n    indlast = [slice(None)] * ndim\n    indlast[axis] = slice(-1, None)\n    indlast = tuple(indlast)\n    indlast2 = [slice(None)] * ndim\n    indlast2[axis] = -1\n    indlast2 = tuple(indlast2)\n    n = (~np.isnan(a)).sum(axis)\n    a_indlast = a[indlast]\n    g = (a_indlast > a).sum(axis)\n    e = (a_indlast == a).sum(axis)\n    r = (g + g + e - 1.0) / 2.0\n    r = r / (n - 1.0)\n    r = 2.0 * (r - 0.5)\n    if ndim == 1:\n        if n == 1:\n            r = 0\n        if np.isnan(a[indlast2]):  # elif?\n            r = np.nan\n    else:\n        np.putmask(r, n == 1, 0)\n        np.putmask(r, np.isnan(a[indlast2]), np.nan)\n    return r\n'"
numbagg/test/util.py,37,"b'import numpy as np\n\n\nDTYPES = [np.float32, np.float64]\n\n\ndef arrays(func_name, dtypes=DTYPES):\n    return array_iter(array_generator, func_name, dtypes)\n\n\ndef array_iter(arrays_func, *args):\n    for a in arrays_func(*args):\n        if a.ndim < 2:\n            yield a\n        #  this is good for an extra check but in everyday development it\n        #  is a pain because it doubles the unit test run time\n        #  elif a.ndim == 3:\n        #      for axes in permutations(range(a.ndim)):\n        #          yield np.transpose(a, axes)\n        else:\n            yield a\n            yield a.T\n\n\ndef array_generator(func_name, dtypes):\n    ""Iterator that yields arrays to use for unit testing.""\n\n    # define nan and inf\n    if func_name in (""partition"", ""argpartition""):\n        nan = 0\n    else:\n        nan = np.nan\n    if func_name in (""move_sum"", ""move_mean"", ""move_std"", ""move_var""):\n        # these functions can\'t handle inf\n        inf = 8\n    else:\n        inf = np.inf\n\n    # nan and inf\n    yield np.array([inf, nan])\n    yield np.array([inf, -inf])\n    yield np.array([nan, 2, 3])\n    yield np.array([-inf, 2, 3])\n    if func_name != ""nanargmin"":\n        yield np.array([nan, inf])\n\n    # byte swapped\n    yield np.array([1, 2, 3], dtype="">f4"")\n    yield np.array([1, 2, 3], dtype=""<f4"")\n\n    # make sure slow is callable\n    yield np.array([1, 2, 3], dtype=np.float16)\n\n    # regression tests\n    yield np.array([1, 2, 3]) + 1e9  # check that move_std is robust\n    yield np.array([0, 0, 0])  # nanargmax/nanargmin\n    yield np.array([1, nan, nan, 2])  # nanmedian\n    yield np.array([2 ** 31], dtype=np.int64)  # overflows on windows\n    yield np.array([[1.0, 2], [3, 4]])[..., np.newaxis]  # issue #183\n\n    # ties\n    yield np.array([0, 0, 0])\n    yield np.array([0, 0, 0], dtype=np.float64)\n    yield np.array([1, 1, 1], dtype=np.float64)\n\n    # 0d input\n    if not func_name.startswith(""move""):\n        yield np.array(-9)\n        yield np.array(0)\n        yield np.array(9)\n        yield np.array(-9.0)\n        yield np.array(0.0)\n        yield np.array(9.0)\n        yield np.array(-inf)\n        yield np.array(inf)\n        yield np.array(nan)\n\n    # automate a bunch of arrays to test\n    ss = {}\n    ss[0] = {""size"": 0, ""shapes"": [(0,), (0, 0), (2, 0), (2, 0, 1)]}\n    ss[1] = {""size"": 8, ""shapes"": [(8,)]}\n    ss[2] = {""size"": 12, ""shapes"": [(2, 6), (3, 4)]}\n    ss[3] = {""size"": 16, ""shapes"": [(2, 2, 4)]}\n    ss[4] = {""size"": 24, ""shapes"": [(1, 2, 3, 4)]}\n    for seed in (1, 2):\n        rs = np.random.RandomState(seed)\n        for ndim in ss:\n            size = ss[ndim][""size""]\n            shapes = ss[ndim][""shapes""]\n            for dtype in dtypes:\n                a = np.arange(size, dtype=dtype)\n                if issubclass(a.dtype.type, np.inexact):\n                    if func_name not in (""nanargmin"", ""nanargmax""):\n                        # numpy can\'t handle eg np.nanargmin([np.nan, np.inf])\n                        idx = rs.rand(*a.shape) < 0.2\n                        a[idx] = inf\n                    idx = rs.rand(*a.shape) < 0.2\n                    a[idx] = nan\n                    idx = rs.rand(*a.shape) < 0.2\n                    a[idx] *= -1\n                rs.shuffle(a)\n                for shape in shapes:\n                    yield a.reshape(shape)\n\n    # non-contiguous arrays\n    yield np.array([[1, 2], [3, 4]])[:, [1]]  # gh 161\n    for dtype in dtypes:\n        # 1d\n        a = np.arange(12).astype(dtype)\n        for start in range(3):\n            for step in range(1, 3):\n                yield a[start::step]  # don\'t use astype here; copy created\n    for dtype in dtypes:\n        # 2d\n        a = np.arange(12).reshape(4, 3).astype(dtype)\n        yield a[::2]\n        yield a[:, ::2]\n        yield a[::2][:, ::2]\n    for dtype in dtypes:\n        # 3d\n        a = np.arange(24).reshape(2, 3, 4).astype(dtype)\n        for start in range(2):\n            for step in range(1, 2):\n                yield a[start::step]\n                yield a[:, start::step]\n                yield a[:, :, start::step]\n                yield a[start::step][::2]\n                yield a[start::step][::2][:, ::2]\n\n\ndef array_order(a):\n    f = a.flags\n    string = []\n    if f.c_contiguous:\n        string.append(""C"")\n    if f.f_contiguous:\n        string.append(""F"")\n    if len(string) == 0:\n        string.append(""N"")\n    return "","".join(string)\n'"
