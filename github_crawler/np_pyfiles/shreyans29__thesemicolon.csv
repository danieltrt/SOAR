file_path,api_count,code
chat.py,2,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Tue Mar 07 09:11:32 2017\r\n\r\n@author: Shreyans\r\n""""""\r\n\r\nimport os\r\nfrom scipy import spatial\r\nimport numpy as np\r\nimport gensim\r\nimport nltk\r\nfrom keras.models import load_model\r\n\r\n\r\nimport theano\r\ntheano.config.optimizer=""None""\r\n\r\n\r\nmodel=load_model(\'LSTM5000.h5\')\r\nmod = gensim.models.Word2Vec.load(\'word2vec.bin\');\r\nwhile(True):\r\n    x=raw_input(""Enter the message:"");\r\n    sentend=np.ones((300L,),dtype=np.float32) \r\n\r\n    sent=nltk.word_tokenize(x.lower())\r\n    sentvec = [mod[w] for w in sent if w in mod.vocab]\r\n\r\n    sentvec[14:]=[]\r\n    sentvec.append(sentend)\r\n    if len(sentvec)<15:\r\n        for i in range(15-len(sentvec)):\r\n            sentvec.append(sentend) \r\n    sentvec=np.array([sentvec])\r\n    \r\n    predictions = model.predict(sentvec)\r\n    outputlist=[mod.most_similar([predictions[0][i]])[0][0] for i in range(15)]\r\n    output=\' \'.join(outputlist)\r\n    print output\r\n            '"
chatbotPreprocessing.py,1,"b'# -*- coding: utf-8 -*-\r\n\r\n\r\nimport os\r\nimport json\r\nimport nltk\r\nimport gensim\r\nimport numpy as np\r\nfrom gensim import corpora, models, similarities\r\nimport pickle\r\n\r\nos.chdir(""D:\\semicolon\\Deep Learning\\chatbot"");\r\nmodel = gensim.models.Word2Vec.load(\'word2vec.bin\');\r\npath2=""corpus"";\r\nfile=open(path2+\'/conversation.json\');\r\ndata = json.load(file)\r\ncor=data[""conversations""];\r\n\r\nx=[]\r\ny=[]\r\n\r\npath2=""corpus"";\r\n\r\nfor i in range(len(cor)):\r\n    for j in range(len(cor[i])):\r\n        if j<len(cor[i])-1:\r\n            x.append(cor[i][j]);\r\n            y.append(cor[i][j+1]);\r\n\r\ntok_x=[]\r\ntok_y=[]\r\nfor i in range(len(x)):\r\n    tok_x.append(nltk.word_tokenize(x[i].lower()))\r\n    tok_y.append(nltk.word_tokenize(y[i].lower()))\r\n    \r\n    \r\n\r\nsentend=np.ones((300L,),dtype=np.float32) \r\n\r\nvec_x=[]\r\nfor sent in tok_x:\r\n    sentvec = [model[w] for w in sent if w in model.vocab]\r\n    vec_x.append(sentvec)\r\n    \r\nvec_y=[]\r\nfor sent in tok_y:\r\n    sentvec = [model[w] for w in sent if w in model.vocab]\r\n    vec_y.append(sentvec)           \r\n    \r\n    \r\nfor tok_sent in vec_x:\r\n    tok_sent[14:]=[]\r\n    tok_sent.append(sentend)\r\n    \r\n\r\nfor tok_sent in vec_x:\r\n    if len(tok_sent)<15:\r\n        for i in range(15-len(tok_sent)):\r\n            tok_sent.append(sentend)    \r\n            \r\nfor tok_sent in vec_y:\r\n    tok_sent[14:]=[]\r\n    tok_sent.append(sentend)\r\n    \r\n\r\nfor tok_sent in vec_y:\r\n    if len(tok_sent)<15:\r\n        for i in range(15-len(tok_sent)):\r\n            tok_sent.append(sentend)             \r\n            \r\n            \r\nwith open(\'conversation.pickle\',\'w\') as f:\r\n    pickle.dump([vec_x,vec_y],f)                \r\n'"
chatbotlstmtrain.py,2,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Fri Mar 03 10:07:42 2017\r\n\r\n@author: Shreyans\r\n""""""\r\n\r\nimport os\r\nimport pickle\r\nimport numpy as np\r\nfrom keras.models import Sequential\r\nimport gensim\r\nfrom keras.layers.recurrent import LSTM,SimpleRNN\r\nfrom sklearn.model_selection import train_test_split\r\nimport theano\r\ntheano.config.optimizer=""None""\r\n\r\nwith open(\'conversation.pickle\') as f:\r\n    vec_x,vec_y=pickle.load(f)    \r\n    \r\nvec_x=np.array(vec_x,dtype=np.float64)\r\nvec_y=np.array(vec_y,dtype=np.float64)    \r\n\r\nx_train,x_test, y_train,y_test = train_test_split(vec_x, vec_y, test_size=0.2, random_state=1)\r\n    \r\nmodel=Sequential()\r\nmodel.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init=\'glorot_normal\', inner_init=\'glorot_normal\', activation=\'sigmoid\'))\r\nmodel.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init=\'glorot_normal\', inner_init=\'glorot_normal\', activation=\'sigmoid\'))\r\nmodel.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init=\'glorot_normal\', inner_init=\'glorot_normal\', activation=\'sigmoid\'))\r\nmodel.add(LSTM(output_dim=300,input_shape=x_train.shape[1:],return_sequences=True, init=\'glorot_normal\', inner_init=\'glorot_normal\', activation=\'sigmoid\'))\r\nmodel.compile(loss=\'cosine_proximity\', optimizer=\'adam\', metrics=[\'accuracy\'])\r\n\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM500.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM1000.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM1500.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM2000.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM2500.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM3000.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM3500.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM4000.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM4500.h5\');\r\nmodel.fit(x_train, y_train, nb_epoch=500,validation_data=(x_test, y_test))\r\nmodel.save(\'LSTM5000.h5\');          \r\npredictions=model.predict(x_test) \r\nmod = gensim.models.Word2Vec.load(\'word2vec.bin\');   \r\n[mod.most_similar([predictions[10][i]])[0] for i in range(15)]\r\n'"
cnn.py,5,"b'# -*- coding: utf-8 -*-\r\n\r\n\r\n#importing Keras, Library for deep learning \r\nfrom keras.models import Sequential\r\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\r\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\r\nfrom keras.utils import np_utils\r\nfrom keras.preprocessing.image import  img_to_array\r\nfrom keras import backend as K\r\n# Fix for Issue - #3 https://github.com/shreyans29/thesemicolon/issues/3\r\nK.set_image_dim_ordering(\'th\')\r\n\r\nimport numpy as np\r\n\r\n# Image manipulations and arranging data\r\nimport os\r\nfrom PIL import Image\r\nimport theano\r\ntheano.config.optimizer=""None""\r\n#Sklearn to modify the data\r\n\r\nfrom sklearn.cross_validation import train_test_split\r\nos.chdir(""D:\\semicolon\\Deep Learning"");\r\n\r\n# input image dimensions\r\nm,n = 50,50\r\n\r\npath1=""input"";\r\npath2=""data"";\r\n\r\nclasses=os.listdir(path2)\r\nx=[]\r\ny=[]\r\nfor fol in classes:\r\n    print fol\r\n    imgfiles=os.listdir(path2+\'\\\\\'+fol);\r\n    for img in imgfiles:\r\n        im=Image.open(path2+\'\\\\\'+fol+\'\\\\\'+img);\r\n        im=im.convert(mode=\'RGB\')\r\n        imrs=im.resize((m,n))\r\n        imrs=img_to_array(imrs)/255;\r\n        imrs=imrs.transpose(2,0,1);\r\n        imrs=imrs.reshape(3,m,n);\r\n        x.append(imrs)\r\n        y.append(fol)\r\n        \r\nx=np.array(x);\r\ny=np.array(y);\r\n\r\nbatch_size=32\r\nnb_classes=len(classes)\r\nnb_epoch=20\r\nnb_filters=32\r\nnb_pool=2\r\nnb_conv=3\r\n\r\nx_train, x_test, y_train, y_test= train_test_split(x,y,test_size=0.2,random_state=4)\r\n\r\nuniques, id_train=np.unique(y_train,return_inverse=True)\r\nY_train=np_utils.to_categorical(id_train,nb_classes)\r\nuniques, id_test=np.unique(y_test,return_inverse=True)\r\nY_test=np_utils.to_categorical(id_test,nb_classes)\r\n\r\nmodel= Sequential()\r\nmodel.add(Convolution2D(nb_filters,nb_conv,nb_conv,border_mode=\'same\',input_shape=x_train.shape[1:]))\r\nmodel.add(Activation(\'relu\'));\r\nmodel.add(Convolution2D(nb_filters,nb_conv,nb_conv));\r\nmodel.add(Activation(\'relu\'));\r\nmodel.add(MaxPooling2D(pool_size=(nb_pool,nb_pool)));\r\nmodel.add(Dropout(0.5));\r\nmodel.add(Flatten());\r\nmodel.add(Dense(128));\r\nmodel.add(Dropout(0.5));\r\nmodel.add(Dense(nb_classes));\r\nmodel.add(Activation(\'softmax\'));\r\nmodel.compile(loss=\'categorical_crossentropy\',optimizer=\'adadelta\',metrics=[\'accuracy\'])\r\n\r\nnb_epoch=5;\r\nbatch_size=5;\r\nmodel.fit(x_train,Y_train,batch_size=batch_size,nb_epoch=nb_epoch,verbose=1,validation_data=(x_test, Y_test))\r\n\r\n\r\nfiles=os.listdir(path1);\r\nimg=files[0] \r\nim = Image.open(path1 + \'\\\\\'+img);\r\nimrs = im.resize((m,n))\r\nimrs=img_to_array(imrs)/255;\r\nimrs=imrs.transpose(2,0,1);\r\nimrs=imrs.reshape(3,m,n);\r\n\r\nx=[]\r\nx.append(imrs)\r\nx=np.array(x);\r\npredictions = model.predict(x)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n        \r\n'"
livesenti.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on Fri Jul 22 19:04:47 2016\r\n\r\n@author: Shreyans Shrimal\r\n""""""\r\nimport time\r\nfrom tweepy import Stream\r\nfrom tweepy import OAuthHandler\r\nfrom tweepy.streaming import StreamListener\r\nimport json\r\nfrom textblob import TextBlob\r\nimport matplotlib.pyplot as plt\r\nimport re\r\n\r\n""# -- coding: utf-8 --""\r\n\r\ndef calctime(a):\r\n    return time.time()-a\r\n\r\npositive=0\r\nnegative=0\r\ncompound=0\r\n\r\ncount=0\r\ninitime=time.time()\r\nplt.ion()\r\n\r\nimport test\r\n\r\nckey=test.ckey\r\ncsecret=test.csecret\r\natoken=test.atoken\r\nasecret=test.asecret\r\n\r\nclass listener(StreamListener):\r\n    \r\n    def on_data(self,data):\r\n        global initime\r\n        t=int(calctime(initime))\r\n        all_data=json.loads(data)\r\n        tweet=all_data[""text""].encode(""utf-8"")\r\n        #username=all_data[""user""][""screen_name""]\r\n        tweet="" "".join(re.findall(""[a-zA-Z]+"", tweet))\r\n        blob=TextBlob(tweet.strip())\r\n\r\n        global positive\r\n        global negative     \r\n        global compound  \r\n        global count\r\n        \r\n        count=count+1\r\n        senti=0\r\n        for sen in blob.sentences:\r\n            senti=senti+sen.sentiment.polarity\r\n            if sen.sentiment.polarity >= 0:\r\n                positive=positive+sen.sentiment.polarity   \r\n            else:\r\n                negative=negative+sen.sentiment.polarity  \r\n        compound=compound+senti        \r\n        print count\r\n        print tweet.strip()\r\n        print senti\r\n        print t\r\n        print str(positive) + \' \' + str(negative) + \' \' + str(compound) \r\n        \r\n    \r\n        plt.axis([ 0, 70, -20,20])\r\n        plt.xlabel(\'Time\')\r\n        plt.ylabel(\'Sentiment\')\r\n        plt.plot([t],[positive],\'go\',[t] ,[negative],\'ro\',[t],[compound],\'bo\')\r\n        plt.show()\r\n        plt.pause(0.0001)\r\n        if count==200:\r\n            return False\r\n        else:\r\n            return True\r\n        \r\n    def on_error(self,status):\r\n        print status\r\n\r\n\r\nauth=OAuthHandler(ckey,csecret)\r\nauth.set_access_token(atoken,asecret)\r\n\r\ntwitterStream=  Stream(auth, listener(count))\r\ntwitterStream.filter(track=[""Donald Trump""])\r\n      \r\n \r\n\r\n'"
lstm - RNN.py,4,"b""# -*- coding: utf-8 -*-\r\n\r\n\r\nimport numpy as np\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\n\r\n\r\ndata = [[i for i in range(100)]]\r\ndata = np.array(data, dtype=float)\r\ntarget = [[i for i in range(1,101)]]\r\ntarget = np.array(target, dtype=float)\r\n\r\ndata = data.reshape((1, 1, 100)) \r\ntarget = target.reshape((1, 1, 100)) \r\nx_test=[i for i in range(100,200)]\r\nx_test=np.array(x_test).reshape((1,1,100));\r\ny_test=[i for i in range(101,201)]\r\ny_test=np.array(y_test).reshape(1,1,100)\r\n\r\n\r\nmodel = Sequential()  \r\nmodel.add(LSTM(100, input_shape=(1, 100),return_sequences=True))\r\nmodel.add(Dense(100))\r\nmodel.compile(loss='mean_absolute_error', optimizer='adam',metrics=['accuracy'])\r\nmodel.fit(data, target, nb_epoch=10000, batch_size=1, verbose=2,validation_data=(x_test, y_test))\r\n\r\n\r\n\r\npredict = model.predict(data)\r\n\r\n"""
lstmaccuracy.py,2,"b""# -*- coding: utf-8 -*-\r\n\r\nimport numpy as np\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import LSTM\r\n\r\nleng=3\r\n\r\ndata = [[i+j for j in range(leng)] for i in range(100)]\r\ndata = np.array(data, dtype=np.float32)\r\ntarget = [[i+j+1 for j in range(leng)] for i in range(1,101)]\r\ntarget = np.array(target, dtype=np.float32)\r\n\r\ndata = data.reshape(100, 1, leng)/200\r\ntarget = target.reshape(100,1,leng)/200\r\n\r\n# Build Model\r\nmodel = Sequential()  \r\nmodel.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\r\nmodel.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\r\nmodel.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\r\nmodel.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\r\nmodel.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\r\nmodel.add(LSTM(leng, input_shape=(1, leng),return_sequences=True,activation='sigmoid'))\r\nmodel.compile(loss='mse', optimizer='adam',metrics=['accuracy'])\r\nmodel.fit(data, target, nb_epoch=10000, batch_size=50,validation_data=(data,target))\r\n\r\n\r\npredict = model.predict(data)\r\n\r\n"""
word2vec.py,0,"b'import os\r\nimport pandas as pd\r\nimport nltk\r\nimport gensim\r\nfrom gensim import corpora, models, similarities\r\n\r\nos.chdir(""D:\\semicolon\\Deep Learning"");\r\ndf=pd.read_csv(\'jokes.csv\');\r\n\r\n\r\n\r\nx=df[\'Question\'].values.tolist()\r\ny=df[\'Answer\'].values.tolist()\r\n\r\ncorpus= x+y\r\n  \r\ntok_corp= [nltk.word_tokenize(sent.decode(\'utf-8\')) for sent in corpus]\r\n       \r\n           \r\nmodel = gensim.models.Word2Vec(tok_corp, min_count=1, size = 32)\r\n\r\n#model.save(\'testmodel\')\r\n#model = gensim.models.Word2Vec.load(\'test_model\')\r\n#model.most_similar(\'word\')\r\n#model.most_similar([vector])'"
