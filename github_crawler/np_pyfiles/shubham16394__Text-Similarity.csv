file_path,api_count,code
bm25.py,5,"b'import scipy.sparse as sp \nimport numpy as np\n\ndef compute_corpus_term_idfs(corpus_features, norm_corpus):\n    \n    dfs = np.diff(sp.csc_matrix(corpus_features, copy=True).indptr)\n    dfs = 1 + dfs # to smoothen idf later\n    total_docs = 1 + len(norm_corpus)\n    idfs = 1.0 + np.log(float(total_docs) / dfs)\n    return idfs\n\n\ndef compute_bm25_similarity(doc_features, corpus_features,\n                            corpus_doc_lengths, avg_doc_length,\n                            term_idfs, k1=1.5, b=0.75):\n    # get corpus bag of words features\n    corpus_features = corpus_features.toarray()\n    # convert query document features to binary features\n    # this is to keep a note of which terms exist per document\n    doc_features = doc_features.toarray()[0]\n    doc_features[doc_features >= 1] = 1\n    \n    # compute the document idf scores for present terms\n    doc_idfs = doc_features * term_idfs\n    # compute numerator expression in BM25 equation\n    numerator_coeff = corpus_features * (k1 + 1)\n    numerator = np.multiply(doc_idfs, numerator_coeff)\n    # compute denominator expression in BM25 equation\n    denominator_coeff =  k1 * (1 - b + \n                                (b * (corpus_doc_lengths / \n                                        avg_doc_length)))\n    denominator_coeff = np.vstack(denominator_coeff)\n    denominator = corpus_features + denominator_coeff\n    # compute the BM25 score combining the above equations\n    bm25_scores = np.sum(np.divide(numerator,\n                                   denominator),\n                         axis=1) \n    \n    return bm25_scores'"
execute.py,1,"b'\nfrom normalization import normalize_corpus\nfrom utils import build_feature_matrix\nfrom bm25 import compute_corpus_term_idfs\nfrom bm25 import compute_bm25_similarity\nfrom semantic_similarity import sentence_similarity\nimport numpy as np\n\n\ndef run():\n\tanswers=[\'Functions are used as one-time processing snippet for inling and jumbling the code.\', \n\t\t\'Functions are used for reusing, inlining and jumbling the code.\', \n\t\t\'Functions are used as one-time processing snippet for inlining and organizing the code.\', \n\t\t\'Functions are used as one-time processing snippet for modularizing and jumbling the code.\', \n\t\t\'Functions are used for reusing, inling and organizing the code.\', \n\t\t\'Functions are used as one-time processing snippet for modularizing and organizing the code.\', \n\t\t\'Functions are used for reusing, modularizing and jumbling the code.\', \n\t\t\'Functions are used for reusing, modularizing and organizing the code.\']\n\n\tmodel_answer = [""Functions are used for reusing, modularizing and organizing the code.""]\n\n\n\t# normalize answers\n\tnorm_corpus = normalize_corpus(answers, lemmatize=True)\n\t                                                        \n\t# normalize model_answer\n\tnorm_model_answer =  normalize_corpus(model_answer, lemmatize=True)            \n\n\tvectorizer, corpus_features = build_feature_matrix(norm_corpus,feature_type=\'frequency\')\n\n\t# extract features from model_answer\n\tmodel_answer_features = vectorizer.transform(norm_model_answer)\n\n\tdoc_lengths = [len(doc.split()) for doc in norm_corpus]   \n\tavg_dl = np.average(doc_lengths) \n\tcorpus_term_idfs = compute_corpus_term_idfs(corpus_features, norm_corpus)\n\t                 \n\tfor index, doc in enumerate(model_answer):\n\n\t    doc_features = model_answer_features[index]\n\t    bm25_scores = compute_bm25_similarity(doc_features,corpus_features,doc_lengths,avg_dl,corpus_term_idfs,k1=1.5, b=0.75)\n\t    semantic_similarity_scores=[]\n\t    for sentence in answers:\n\t    \tscore=(sentence_similarity(sentence,model_answer[0])+sentence_similarity(model_answer[0],sentence))/2\n\t    \tsemantic_similarity_scores.append(score)\n\t    print \'Model Answer\',\':\', doc\n\t    print \'-\'*40 \n\t    doc_index=0\n\t    for score_tuple in zip(semantic_similarity_scores,bm25_scores):\n\t    \tsim_score=((score_tuple[0]*10)+score_tuple[1])/2\n\t    \tif(sim_score<1):\n\t    \t\tsim_score=0\n\t    \telif(1<=sim_score<=2):\t\n\t    \t\tsim_score=1\n\t    \telif(2<sim_score<=4):\n\t    \t\tsim_score=2\n\t    \telif(4<sim_score<=6):\n\t    \t\tsim_score=3\n\t    \telif(6<sim_score<=8):\n\t    \t\tsim_score=4\n\t    \telif(8<sim_score<=10):\t\n\t    \t\tsim_score=5\t\t\t\t\t\n\t        print \'Ans num: {} Score: {}\\nAnswer: {}\'.format(doc_index+1, sim_score, answers[doc_index])  \n\t        print \'-\'*40       \n\t        doc_index=doc_index+1\n\t    print\n\nrun()\t    '"
normalization.py,0,"b""import re\nimport nltk\nimport string\nfrom nltk.stem import WordNetLemmatizer\nfrom pattern.en import tag\nfrom nltk.corpus import wordnet as wn\n\nstopword_list = nltk.corpus.stopwords.words('english')\n\nwnl = WordNetLemmatizer()\n\ndef tokenize_text(text):\n    tokens = nltk.word_tokenize(text) \n    tokens = [token.strip() for token in tokens]\n    return tokens\n   \ndef penn_to_wn_tags(pos_tag):\n        if pos_tag.startswith('J'):\n            return wn.ADJ\n        elif pos_tag.startswith('V'):\n            return wn.VERB\n        elif pos_tag.startswith('N'):\n            return wn.NOUN\n        elif pos_tag.startswith('R'):\n            return wn.ADV\n        else:\n            return None\n\n# Annotate text tokens with POS tags\ndef pos_tag_text(text):\n    \n    tagged_text = tag(text)\n    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n                         for word, pos_tag in\n                         tagged_text]\n    return tagged_lower_text\n    \n# lemmatize text based on POS tags    \ndef lemmatize_text(text):\n    \n    pos_tagged_text = pos_tag_text(text)\n    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n                         else word                     \n                         for word, pos_tag in pos_tagged_text]\n    lemmatized_text = ' '.join(lemmatized_tokens)\n    return lemmatized_text\n    \n\ndef remove_special_characters(text):\n    tokens = tokenize_text(text)\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_tokens = filter(None, [pattern.sub(' ', token) for token in tokens])\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n    \n    \ndef remove_stopwords(text):\n    tokens = tokenize_text(text)\n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\n\ndef normalize_corpus(corpus, lemmatize=True):\n    \n    normalized_corpus = []    \n    for text in corpus:\n        if lemmatize:\n            text = lemmatize_text(text)\n        else:\n            text = text.lower()\n        text = remove_special_characters(text)\n        text = remove_stopwords(text)\n        normalized_corpus.append(text)\n    return normalized_corpus\n\n"""
semantic_similarity.py,0,"b'from normalization import penn_to_wn_tags\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet as wn\n\ndef tagged_to_synset(word, tag):\n    wn_tag = penn_to_wn_tags(tag)\n    if wn_tag is None:\n        return None\n \n    try:\n        return wn.synsets(word, wn_tag)[0]\n    except:\n        return None\n \ndef sentence_similarity(sentence1, sentence2):\n    \n    sentence1 = pos_tag(word_tokenize(sentence1))\n    sentence2 = pos_tag(word_tokenize(sentence2))\n\n    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n \n    synsets1 = [ss for ss in synsets1 if ss]\n    synsets2 = [ss for ss in synsets2 if ss]\n \n    score, count = 0.0, 0\n\n    for synset in synsets1:\n        best_score = max([synset.path_similarity(ss) for ss in synsets2])\n \n        if best_score is not None:\n            score += best_score\n            count += 1\n \n    score /= count\n    return score'"
utils.py,0,"b'from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\ndef build_feature_matrix(documents, feature_type=\'frequency\',\n                         ngram_range=(1, 1), min_df=0.0, max_df=1.0):\n\n    feature_type = feature_type.lower().strip()  \n    \n    if feature_type == \'binary\':\n        vectorizer = CountVectorizer(binary=True, min_df=min_df,\n                                     max_df=max_df, ngram_range=ngram_range)\n    elif feature_type == \'frequency\':\n        vectorizer = CountVectorizer(binary=False, min_df=min_df,\n                                     max_df=max_df, ngram_range=ngram_range)\n    elif feature_type == \'tfidf\':\n        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, \n                                     ngram_range=ngram_range)\n    else:\n        raise Exception(""Wrong feature type entered. Possible values: \'binary\', \'frequency\', \'tfidf\'"")\n\n    feature_matrix = vectorizer.fit_transform(documents).astype(float)\n    \n    return vectorizer, feature_matrix'"
