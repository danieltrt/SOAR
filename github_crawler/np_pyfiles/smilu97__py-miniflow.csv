file_path,api_count,code
test.py,0,"b""#!/usr/bin/env python3\n\nimport sys\nfrom test.sin import test as sin_test\nfrom test.xor import test as xor_test\nfrom test.logistic import test as logistic_test\nfrom test.mnist_cnn import test as mnist_test\nfrom test.grad import test as grad_test\nfrom test.tmp import test as tmp_test\n\ntests = [\n    ['tmp', tmp_test],\n    ['sin', sin_test],\n    ['xor', xor_test],\n    ['logistic', logistic_test],\n    ['mnist', mnist_test],\n    ['grad', grad_test],\n]\n\nif len(sys.argv) > 1:\n    name = sys.argv[1]\n    found = False\n    for test in tests:\n        if test[0] == name:\n            test[1]()\n            found = True\n            break\n    if found:\n        exit(0)\n\nfor idx, test in enumerate(tests):\n    print('{}) {} test'.format(idx + 1, test[0]))\ntry:\n    sel = int(input()) - 1\n    if type(sel) is type(1) and 0 <= sel < len(tests):\n        tests[sel][1]()\nexcept Exception as e:\n    print('Invalid input', e)\n"""
flow/__init__.py,0,b'from flow.node import *\nfrom flow.node.funcs import *\nfrom flow.node.util import *\nfrom flow.session import *\nfrom flow.optimizer import *\nfrom flow.initializer import *\nfrom flow.network import *\nfrom flow.save import *\nfrom flow.animation import *'
flow/animation.py,10,"b""import numpy as np\n\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\ndef make_animation1d(x, y, y_, E, optimizer, xlim, ylim, answer, print_error=True, epoch_per_frame=1, **kwargs):\n\n    train_x = np.squeeze(x.get_result())\n    train_y = np.squeeze(y.get_result())\n    \n\n    test_x = np.expand_dims(np.arange(xlim[0], xlim[1], (xlim[1] - xlim[0]) / 300), 1)\n    test_y = answer(test_x)\n\n    fig = plt.figure()\n    ax = plt.axes(xlim=xlim, ylim=ylim)\n    line, = ax.plot([], [], lw=2)\n    ans, = ax.plot(test_x, test_y)\n\n    def animate(i):\n        for _ in range(epoch_per_frame):\n            optimizer.minimize()\n        if print_error:\n            print('E:', E.get_result())\n        line.set_data(train_x, np.squeeze(y_.get_result()))\n        return line,\n\n    return animation.FuncAnimation(fig, animate, **kwargs)\n\ndef make_animation2d(x, y, y_, E, optimizer, xlim, ylim, print_error=True, epoch_per_frame=1, **kwargs):\n\n    train_x = x.get_result()\n    train_y = y.get_result()\n\n    dx = (xlim[1] - xlim[0]) / 100\n    dy = (ylim[1] - ylim[0]) / 100\n\n    test_x = []\n    for i in np.arange(xlim[0], xlim[1], dx):\n        for j in np.arange(ylim[0], ylim[1], dy):\n            test_x.append([i, j])\n    test_x = np.array(test_x)\n\n    fig = plt.figure()\n    ax = plt.axes(xlim=xlim, ylim=ylim)\n    red_scatter = ax.scatter([], [])\n    blue_scatter = ax.scatter([], [])\n    train_scatter = ax.scatter(train_x[:,0], train_x[:,1], c=np.squeeze(train_y))\n    \n    def anim_update(i):\n\n        x.set_result(test_x)\n        test_y = y_.get_result().T[0]\n\n        x.set_result(train_x)\n        for _ in range(epoch_per_frame):\n            optimizer.minimize()\n        if print_error:\n            print('E:', E.get_result())\n            \n        red  = []\n        blue = []\n        for idx, rex in enumerate(test_x):\n            if test_y[idx] > 0.5:\n                red.append(rex)\n            else:\n                blue.append(rex)\n\n        if len(red) > 0:\n            red_scatter.set_offsets(np.array(red))\n        if len(blue) > 0:\n            blue_scatter.set_offsets(np.array(blue))\n\n        return blue_scatter, red_scatter, train_scatter\n    \n    return animation.FuncAnimation(fig, anim_update, **kwargs)\n"""
flow/initializer.py,3,"b'import numpy as np\nfrom flow.node.util import *\n\ndef xavier_initializer(uniform=False):\n    def initializer(sess, shape):\n        return xavier(shape, sess.fan_in, sess.fan_out, uniform)\n    \n    return initializer\n\ndef rand_initializer(min_value=0.0, max_value=1.0):\n    diff = max_value - min_value\n    def initializer(sess, shape):\n        return np.random.rand(*shape) * diff - min_value\n        \n    return initializer\n\ndef randn_initializer(min_value=-1.0, max_value=1.0):\n    diff = max_value - min_value\n    def initializer(sess, shape):\n        return np.random.randn(*shape) * diff - min_value\n    \n    return initializer\n\ndef zero_initializer():\n    def initializer(sess, shape):\n        return np.zeros(shape)\n    \n    return initializer\n'"
flow/network.py,0,"b'from flow.node import *\nfrom flow.node.funcs import *\nfrom flow.initializer import *\n\ndef fully_conntected(x, output_size, activation=sigmoid, initializer=zero_initializer(), bias_initializer=zero_initializer()):\n    sess = x.sess\n\n    W_init = initializer(sess, (x.shape[-1], output_size))\n    W = Variable(x.sess, W_init)\n    \n    b_init = bias_initializer(sess, (output_size,))\n    b = Variable(x.sess, b_init)\n\n    R = matmul(x, W) + b\n    if activation is not None:\n        R = activation(R)\n\n    return R, W, b'"
flow/optimizer.py,2,"b""import flow as fl\nimport numpy as np\n\nclass SampleOptimizer:\n    '''\n    Testing for optimizer symbol\n    '''\n\n    def __init__(self, sess, lr=0.001):\n        self.sess = sess\n        self.lr = fl.Variable(sess, [lr])\n    \n    def minimize(self, target):\n        xs = self.sess.trainable_nodes\n        grads = fl.gradients([target], xs)\n        return fl.group(*[self._apply_gradient(x, grad) for x, grad in zip(xs, grads)])\n    \n    def _apply_gradient(self, target, grad):\n        return fl.assign(target, target - grad * self.lr)\n\nclass GradientDescentOptimizer:\n\n    def __init__(self, sess, ys, lr=0.001):\n        self.sess = sess\n        self.lr = lr\n        xs = self.sess.trainable_nodes\n        self.grads = fl.gradients(ys, xs)\n    \n    def minimize(self):\n        xs = self.sess.trainable_nodes\n        grads = self.sess.run(self.grads)\n        for x, grad in zip(xs, grads):\n            self.apply_gradient(x, grad)\n        \n    def apply_gradient(self, target, grad):\n        target.result -= grad * self.lr\n\nclass AdamOptimizer(GradientDescentOptimizer):\n\n    def __init__(self, sess, ys, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-08):\n        super().__init__(sess, ys, lr=lr)\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.lr = lr * np.sqrt(1 - beta2) / (1 - beta1)\n\n    def apply_gradient(self, target, grad):\n\n        # Get previous props\n        if target.initializer_props is None:\n            target.initializer_props = { 'm': 0, 'v': 0 }\n        props = target.initializer_props\n\n        m = props['m']\n        v = props['v']\n    \n        # Calculate props\n        m_t = self.beta1 * m + (1 - self.beta1) * grad\n        v_t = self.beta2 * v + (1 - self.beta2) * grad * grad\n\n        # Apply to result, and update props\n        target.result -= self.lr * m_t / (np.sqrt(v_t) + self.epsilon)\n        props['m'] = m_t\n        props['v'] = v_t\n"""
flow/save.py,2,"b""import numpy as np\n\ndef save(pathname, weight_vectors):\n    fd = open(pathname, 'wb')\n    np.savez(fd, **weight_vectors)\n    fd.close()\n\ndef load(pathname, weight_vectors):\n    try:\n        fd = open('save.sav', 'rb')\n        data = np.load(fd)\n        for key in weight_vectors:\n            value = weight_vectors[key]\n            value.result = data[key]\n        fd.close()\n    except Exception as e:\n        print('Failed to load')"""
flow/session.py,0,"b'import numpy as np\n\nclass Session:\n\n    def __init__(self):\n        self.nodes = []\n        self.placeholders = {}\n        self.trainable_nodes = []\n        self.name_dict = {}\n    \n    def register_node(self, node):\n        self.nodes.append(node)\n        self.name_dict[node.name] = node\n        if node.trainable:\n            self.trainable_nodes.append(node)\n    \n    def register_placeholder(self, node):\n        self.placeholders[node.name] = node\n    \n    def set_placeholder(self, name, value):\n        self.placeholders[name].result = value\n        \n    def clean_gradients(self):\n        for node in self.nodes:\n            node.gradient = 0\n            node.numGradient = 0\n        \n    def run(self, nodes):\n        for node in self.nodes:\n            if node.trainable or node.placeholder:\n                continue\n            node.result = None\n        return [node.get_result() for node in nodes]\n\n            '"
test/__init__.py,0,b''
test/grad.py,1,"b""import flow as fl\nimport numpy as np\n\ndef test():\n    sess = fl.Session()\n    a = fl.ones(sess, (1,))\n    b = fl.ones(sess, (1,))\n    c = a + b\n    grads = fl.gradients([c], [a, b])\n    print(grads[0].get_result())\n\n    b = a * 2\n    grads = fl.gradients([b], [a])\n    print(grads[0].get_result())\n\n    b = a + a\n    grads = fl.gradients([b], [a])\n    print(grads[0].get_result())\n\n    # 2a^2 + 3a + 4\n    # 4a + 3\n    b = 2 * fl.square(a) + 3 * a + 4\n    grads = fl.gradients([b], [a])\n    print(grads[0].get_result())\n    print('b:', b.get_name())\n    print('g:', grads[0].get_name())\n\n    x = fl.ones(sess, (10, 10), 'x')\n    w = fl.ones(sess, (10, 20), 'w')\n    mul = fl.matmul(x, w, 'mul')\n    y_ = fl.sigmoid(mul, 'sigmoid')\n    y = fl.ones(sess, (10, 20), 'y_')\n    e = fl.l2loss(y_, y)\n    grads = fl.gradients([e], [w])\n    print('g:', grads[0].get_name())\n    print('g:', np.sum(grads[0].get_result()))"""
test/logistic.py,3,"b""import flow as fl\nimport numpy as np\nimport progressbar as pb\n\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\ndef answer(a0):\n    res = []\n    for a in a0:\n        d1 = np.sqrt(np.sum(np.square(a - np.array([3, 3])), axis=None))\n        if d1 < 2.0:\n            res.append(0.0)\n        else:\n            res.append(1.0)\n    return np.expand_dims(np.array(res), 1)\n\ntrain_size = 300\ntrain_x = np.random.rand(train_size, 2) * 7\ntrain_y = answer(train_x)\n\ndef test():\n\n    sess = fl.Session()\n    sess.fan_in = 3\n    sess.fan_out = 1\n\n    x = fl.Placeholder(sess, train_x, 'x')\n    y = fl.Placeholder(sess, train_y, 'y')\n\n    # We can choose to apply kernel to x_input or not\n    x2 = fl.concat(x, fl.square(x), 1)\n    # x2 = x\n\n    S0, W0, b0 = fl.fully_conntected(x2, 30, activation=fl.sigmoid, initializer=fl.xavier_initializer())\n    S1, W1, b1 = fl.fully_conntected(S0, 1, activation=fl.sigmoid, initializer=fl.xavier_initializer())\n\n    y_ = S1\n    E = fl.l2loss(y, y_)\n    optimizer = fl.AdamOptimizer(sess, [E], lr=0.01)\n\n    anim = fl.make_animation2d(x, y, y_, E, optimizer, (0, 7), (0, 7), interval=1, blit=True)\n    plt.show()\n"""
test/mnist_cnn.py,12,"b""import numpy as np\nimport flow as fl\n\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\ndef read_x(pathname):\n    with open(pathname, 'rb') as fd:\n        raw = fd.read()\n    info = np.frombuffer(raw[:16], dtype='>u4')\n    images = np.frombuffer(raw[16:], dtype=np.byte)\n    magic_number, num_images, num_rows, num_cols = info\n    images = np.reshape(images, (num_images, num_rows, num_cols))\n    return num_images, np.float32(images / 255.0), num_rows, num_cols\n\ndef read_y(pathname):\n    with open(pathname, 'rb') as fd:\n        raw = fd.read()\n    info = np.frombuffer(raw[:8], dtype='>u4')\n    labels = np.frombuffer(raw[8:], dtype=np.byte)\n    magic_number, num_items = info\n\n    y = np.zeros((num_items, 10))\n    y[np.arange(num_items),labels] = 1\n\n    return num_items, np.array(y, dtype=np.float32)\n\ndef mini_batch(x, y, sz):\n    n = len(x)\n    for i in range(0, n, sz):\n        yield x[i:i+sz], y[i:i+sz]\n\ndef test():\n\n    num_images, images, num_rows, num_cols = read_x('data/mnist/train_x')\n    _, labels = read_y('data/mnist/train_y')\n    vec_size = 28 * 28\n    batch_size = 6000\n    class_num = 10\n    hidden_sizes = 256, 128\n    lr = 0.001\n    epoch = 15\n\n\n    num_test, test_images, _, _ = read_x('data/mnist/test_x')\n    _, test_labels = read_y('data/mnist/test_y')\n\n    print(num_images, num_test, 'Images Read')\n\n    images = np.reshape(images, (num_images, vec_size))\n    test_images = np.reshape(test_images, (num_test, vec_size))\n\n    sess = fl.Session()\n    sess.fan_in = vec_size\n    sess.fan_out = class_num\n\n    input_x = fl.Placeholder(sess, (None, vec_size), 'x')\n    output_y = fl.Placeholder(sess, (None, class_num), 'y')\n    H = input_x\n    for hs in hidden_sizes:\n        H, _, _ = fl.fully_conntected(H, hs, activation=fl.relu, initializer=fl.xavier_initializer())\n    y_, _, _ = fl.fully_conntected(H, class_num, activation=fl.relu, initializer=fl.xavier_initializer())\n\n    E = fl.softmax_cross_entropy_loss(output_y, y_, 1)\n    # E = fl.l2loss(output_y, y_)\n    optimizer = fl.AdamOptimizer(sess, [E], lr=lr)\n    \n    for _ in range(epoch):\n        for batch_x, batch_y in mini_batch(images, labels, batch_size):\n            input_x.set_result(batch_x)\n            output_y.set_result(batch_y)\n            optimizer.minimize()\n        print('E:', E.get_result() / batch_size)\n    input_x.set_result(test_images)\n    output_y.set_result(test_labels)\n    print('E:', E.get_result() / num_test)\n    print('acc:', np.sum(np.argmax(y_.get_result(), axis=1) == np.argmax(test_labels, 1)) / num_test)\n"""
test/sin.py,2,"b""import numpy as np\nimport flow as fl\nimport progressbar as pb\nimport pickle\n\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\n\ndef answer(x, error=0.0):\n    return np.sin(2 * np.pi * x) + np.random.rand(*(x.shape)) * error\n    \ntrain_x  = np.expand_dims(np.arange(-5, 5, 10 / 300), 1)\ntrain_y = answer(train_x)\n\ndef test(train=True):\n\n    sess =  fl.Session()\n    sess.fan_in = 1\n    sess.fan_out = 1\n\n    x = fl.Placeholder(sess, train_x, 'x')\n    y = fl.Placeholder(sess, train_y, 'y')\n\n    S = x\n    for _ in range(7):\n        S, _, _ = fl.fully_conntected(S, 100, activation=fl.tanh, initializer=fl.xavier_initializer())\n    S, _, _ = fl.fully_conntected(S, 1, activation=None, initializer=fl.xavier_initializer())\n\n    y_ = S\n    # E = fl.avg(fl.avg(fl.square(y - y_), 0), 0)\n    E = fl.l2loss(y, y_)\n\n    optimizer = fl.AdamOptimizer(sess, [E], lr=0.001)\n    \n    if False:  # Pre-training before animation\n        for _ in pb.progressbar(range(epoch)):\n            train()\n\n    anim = fl.make_animation1d(x, y, y_, E, optimizer, (-4, 4), (-2, 2), answer, interval=1, blit=True)\n                            \n    plt.show()\n"""
test/tmp.py,0,"b'import numpy as np\nimport flow as fl\n\ndef test():\n    sess = fl.Session()\n    a = fl.ones(sess, (10,))\n    opt = fl.SampleOptimizer(sess)\n    mini = opt.minimize(a)\n    for _ in range(10):\n        sess.run([mini])\n        print(a.result)'"
test/xor.py,4,"b""import flow as fl\nimport numpy as np\nimport progressbar as pb\n\nimport matplotlib\nmatplotlib.use('TkAgg')\nimport matplotlib.pyplot as plt\n\ndef test():\n\n    sess = fl.Session()\n\n    # Vx + b = y\n\n    train_x = np.array([[0, 0],\n        [0, 1],\n        [1, 0],\n        [1, 1]])\n    train_y = np.array([[0],[1],[1],[0]])\n\n    x = fl.Placeholder(sess, train_x, 'x')\n    y = fl.Placeholder(sess, train_y, 'y')\n    \n    def initializer(*shape):\n        return fl.xavier(shape, 2, 2)\n\n    V0 = fl.Variable(sess, initializer(2,2))\n    b0 = fl.Variable(sess, np.zeros(2))\n    S0 = fl.sigmoid(fl.matmul(x, V0) + b0)\n\n    V1 = fl.Variable(sess, initializer(2,1))\n    b1 = fl.Variable(sess, np.zeros(1))\n    S1 = fl.sigmoid(fl.matmul(S0, V1) + b1)\n\n    y_ = S1\n    E = fl.sum(fl.square(y_ - y), axis=0)\n\n    optimizer = fl.AdamOptimizer(sess, [E], lr=0.1)\n\n    if True:  # Pre-calculate before animation\n        print('start error:', E.get_result())\n\n        epoch = 1000\n        with pb.ProgressBar(max_value=epoch) as bar:\n            for i in range(epoch):\n                optimizer.minimize()\n                bar.update(i)\n        \n        print('last error:', E.get_result())\n\n    anim = fl.make_animation2d(x, y, y_, E, optimizer, (-1, 2), (-1, 2), epoch_per_frame=50, frames=50, interval=80, blit=True)\n\n    if True:\n        plt.show()\n    else:\n        anim.save('static/xor.gif', writer='imagemagick')\n        """
flow/node/__init__.py,37,"b""import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nfrom flow.node.util import *\nfrom flow.node.funcs import *\n\nclass Node:\n\n    def __init__(self, sess, children, trainable=False, name=None):\n        self.sess = sess\n        self.children = children\n        self.parents = []\n        self.parentNum = 0\n        if not hasattr(self, 'result'):\n            self.result = None\n        self.placeholder = False\n        self.gradient = None\n        self.numGradient = 0\n        self.trainable = trainable\n        self.initializer_props = None\n        self.name = self.get_name() if name is None else name\n\n        for child in children:\n            child.parentNum += 1\n            child.parents.append(self)\n        \n        sess.register_node(self)\n\n        self.shape = self.calc_shape(*[child.shape for child in children])\n    \n    def get_name(self):\n        if (not hasattr(self, 'name')) or self.name is None:\n            self.name = self.calc_name(*[child.get_name() for child in self.children])\n        return self.name\n\n    def get_result(self):\n        try:\n            if self.result is None:\n                self.result = self.calc_result(*self.get_children_result())\n            return self.result\n        except Exception as e:\n            print(self.name, 'error')\n            raise e\n    \n    def set_result(self, value):\n        self.result = value\n        for parent in self.parents:\n            parent.set_result(None)\n    \n    def get_children_result(self):\n        return (child.get_result() for child in self.children)\n    \n    def calc_result(self):\n        return self.result\n    \n    def calc_shape(self):\n        return None\n    \n    def check_transform_constant(self, x):\n        if type(x) == type(0) or type(x) == type(0.0):\n            x = Placeholder(self.sess, np.array([x]), str(x))\n        return x\n    \n    def __add__(self, a):\n        a = self.check_transform_constant(a)\n        return AddNode(self.sess, [self, a])\n    \n    def __radd__(self, a):\n        return self + a\n\n    def __sub__(self, a):\n        a = self.check_transform_constant(a)\n        return SubNode(self.sess, [self, a])\n    \n    def __rsub__(self, a):\n        a = self.check_transform_constant(a)\n        return SubNode(self.sess, [a, self])\n    \n    def __mul__(self, a):\n        a = self.check_transform_constant(a)\n        return MulNode(self.sess, [self, a])\n    \n    def __rmul__(self, a):\n        return self * a\n    \n    def __matmul__(self, a):\n        return MatmulNode(self.sess, [self, a])\n    \n    def __truediv__(self, a):\n        a = self.check_transform_constant(a)\n        return DivNode(self.sess, [self, a])\n    \n    def __neg__(self):\n        return NegNode(self.sess, [self])\n    \n    def __getitem__(self, key):\n        return SelectNode(self.sess, [self], key)\n    \n    def __hash__(self):\n        return self.name.__hash__()\n\nclass Variable(Node):\n\n    def __init__(self, sess, value, **kwargs):\n        self.result = np.float32(value)\n        super().__init__(sess, [], trainable=True, **kwargs)\n    \n    def calc_shape(self):\n        return self.result.shape\n    \n    def calc_name(self):\n        return 'Var({})'.format(self.result.shape)\n\nclass Placeholder(Node):\n\n    def __init__(self, sess, value, name):\n        if type(value) is type((1,)) or type(value) is type([1]):\n            value = np.zeros([2 if a is None else a for a in value])\n        self.result = np.float32(value)\n        self.name = name\n        sess.register_placeholder(self)\n        super().__init__(sess, [])\n        self.placeholder = True\n    \n    def calc_shape(self):\n        return self.result.shape\n\nclass AssignNode(Node):\n\n    def calc_result(self, a, b):\n        self.children[0].result = b\n    \n    def calc_shape(self, a, b):\n        return b\n    \n    def calc_name(self, a, b):\n        return 'Assign({},{})'.format(a, b)\n    \nclass GroupNode(Node):\n\n    def calc_result(self, *args):\n        pass\n    \n    def calc_shape(self, *args):\n        pass\n    \n    def calc_name(self, *args):\n        n = ','.join([a.name for a in args])\n        return 'Group({})'.format(n)\n        \nclass MatmulNode(Node):\n\n    def calc_result(self, a, b):\n        return np.matmul(a, b)\n\n    def calc_gradients(op, grad):\n        v0, v1 = op.children\n        v0 = fl.transpose(v0)\n        v1 = fl.transpose(v1)\n        return [grad @ v1, v0 @ grad]\n    \n    def calc_shape(self, a, b):\n        if len(a) != 2 or len(b) != 2:\n            raise Exception('Child of matmul should be 2-dimensional array')\n        return (a[0], b[1])\n    \n    def calc_name(self, a, b):\n        return '({} @ {})'.format(a, b)\n\nclass NegNode(Node):\n    \n    def calc_result(self, a):\n        return -a\n    \n    def calc_gradients(op, grad):\n        return [-grad]\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return '(-{})'.format(a)\n\nclass AddNode(Node):\n\n    def calc_result(self, a, b):\n        return a + b\n\n    def calc_gradients(op, grad):\n        return [\n            fl.reduce_shape(grad, op.children[0].shape),\n            fl.reduce_shape(grad, op.children[1].shape)\n        ]\n    \n    def calc_shape(self, a, b):\n        return shape_broadcast(a, b)\n    \n    def calc_name(self, a, b):\n        return '({} + {})'.format(a, b)\n\nclass SubNode(Node):\n\n    def calc_result(self, a, b):\n        return a - b\n\n    def calc_gradients(op, grad):\n        return [\n            fl.reduce_shape(grad, op.children[0].shape),\n            -fl.reduce_shape(grad, op.children[1].shape)\n        ]\n    \n    def calc_shape(self, a, b):\n        return shape_broadcast(a, b)\n    \n    def calc_name(self, a, b):\n        return '({} - {})'.format(a, b)\n\nclass MulNode(Node):\n\n    def calc_result(self, a, b):\n        return a * b\n    \n    def calc_gradients(op, grad):\n        v0, v1 = op.children\n        return [\n            fl.reduce_shape(grad * v1, v0.shape),\n            fl.reduce_shape(grad * v0, v1.shape)\n        ]\n\n    def calc_shape(self, a, b):\n        return shape_broadcast(a, b)\n            \n    def calc_name(self, a, b):\n        return '({} * {})'.format(a, b)\n\nclass DivNode(Node):\n\n    def calc_result(self, a, b):\n        return a / b\n    \n    def calc_gradients(op, grad):\n        v0, v1 = op.children\n        return [\n            fl.reduce_shape(grad / v1, v0.shape),\n            fl.reduce_shape(-v0 / fl.square(v1) * grad, v0.shape)\n        ]\n    \n    def calc_shape(self, a, b):\n        return shape_broadcast(a, b)\n    \n    def calc_name(self, a, b):\n        return '({} / {})'.format(a, b)\n\nclass SigmoidNode(Node):\n\n    def calc_result(self, a):\n        return 1.0 / (1 + np.exp(-a))\n\n    def calc_gradients(op, grad):\n        return [op * (1 - op) * grad]\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'Sigmoid({})'.format(a)\n\nclass TanhNode(Node):\n\n    def calc_result(self, a):\n        return np.tanh(a)\n    \n    def calc_gradients(op, grad):\n        r = fl.tanh(op.children[0])\n        return [(1 - r) * (1 + r) * grad]\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'Tanh({})'.format(a)\n    \nclass ReluNode(Node):\n\n    def calc_result(self, a):\n        return np.maximum(a, 0)\n    \n    def calc_gradients(op, grad):\n        return [fl.relu_grad(op.children[0]) * grad]\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'Relu({})'.format(a)\n    \nclass ReluGradNode(Node):\n\n    def calc_result(self, a):\n        return np.heaviside(a, 0)\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'ReluGrad({})'.format(a)\n\nclass LeakyReluNode(Node):\n\n    def __init__(self, sess, children, alpha):\n        self.alpha = alpha\n        super().__init__(sess, children)\n\n    def calc_result(self, a):\n        return np.where(a>0, a, a*self.alpha)\n    \n    def calc_gradients(op, grad):\n        return [fl.leaky_relu_grad(op.children[0], grad, op.alpha)]\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'LRelu({})({})'.format(self.alpha, a)\n    \nclass LeakyReluGradNode(Node):\n\n    def __init__(self, sess, children, alpha, **kwargs):\n        self.alpha = alpha\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a, grad):\n        return np.where(a > 0, 1, self.alpha) * grad\n    \n    def calc_shape(self, a, grad):\n        return a\n    \n    def calc_name(self, a, grad):\n        return 'LReluGrad({})({},{})'.format(self.alpha, a, grad)\n\nclass EluNode(Node):\n\n    def __init__(self, sess, children, alpha):\n        self.alpha = alpha\n        super().__init__(sess, children)\n\n    def calc_result(self, a):\n        return np.where(a>0, a, (np.exp(a)-1)*self.alpha)\n    \n    def calc_gradients(op, grad):\n        v0, = op.children\n        return [fl.elu_grad(v0, grad)]\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'LRelu({})({})'.format(self.alpha, a)\n    \nclass EluGradNode(Node):\n\n    def __init__(self, sess, children, alpha, **kwargs):\n        self.alpha = alpha\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a, grad):\n        return [np.where(a > 0, 1, np.exp(a)*self.alpha) * grad]\n    \n    def calc_shape(self, a, grad):\n        return a\n    \n    def calc_name(self, a, grad):\n        return 'LReluGrad({})({},{})'.format(self.alpha, a, grad)\n\nclass SoftmaxNode(Node):\n\n    def __init__(self, sess, children, axis, **kwargs):\n        self.axis = axis\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a):\n        ax = self.axis\n        e = np.exp(a - np.max(a, axis=ax, keepdims=True))\n        return e / np.sum(e, axis=ax, keepdims=True)\n    \n    def calc_gradients(op, grad):\n        return [fl.softmax_grad(op.children[0], op.axis) * grad]\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'Softmax({})'.format(a)\n\nclass SoftmaxGradNode(Node):\n\n    def __init__(self, sess, children, axis, **kwargs):\n        self.axis = axis\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a):\n        ax = self.axis\n        e = np.exp(a - np.max(a, axis=ax, keepdims=True))\n        es = np.sum(e, axis=ax, keepdims=True)\n        return e * (1 - es)\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'SoftmaxGradNode({})'.format(a)\n    \nclass LogNode(Node):\n\n    def calc_result(self, a):\n        return np.log(a)\n    \n    def calc_gradients(op, grad):\n        return [fl.log_grad(op.children[0], grad)]\n    \n    def calc_shape(self, a):\n        return a\n\n    def calc_name(self, a):\n        return 'Log({})'.format(a)\n\nclass LogGradNode(Node):\n\n    def calc_result(self, a, grad):\n        return grad / a\n    \n    def calc_shape(self, a, grad):\n        return a\n    \n    def calc_name(self, a, grad):\n        return 'LogGrad({},{})'.format(a, grad)\n\nclass ExpNode(Node):\n\n    def calc_result(self, a):\n        return np.exp(a)\n    \n    def calc_gradients(op, grad):\n        return [fl.exp(op.children[0]) * grad]\n    \n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'Exp({})'.format(a)\n    \nclass SquareNode(Node):\n\n    def calc_result(self, a):\n        return a * a\n\n    def calc_gradients(op, grad):\n        return [op.children[0] * grad * 2]\n\n    def calc_shape(self, a):\n        return a\n    \n    def calc_name(self, a):\n        return 'Square({})'.format(a)\n\nclass L2LossNode(Node):\n\n    def calc_result(self, a, b):\n        self.diff = a - b\n        return np.sum(np.square(a - b) / 2)\n    \n    def calc_gradients(op, grad):\n        v0, v1 = op.children\n        g = (v0 - v1) * grad\n        return [\n            fl.reduce_shape(g, v0.shape),\n            fl.reduce_shape(-g, v1.shape)\n        ]\n\n    def calc_shape(self, a, b):\n        return (1,)    \n\n    def calc_name(self, a, b):\n        return 'L2Loss({},{})'.format(a, b)\n\nclass ReduceShapeNode(Node):\n\n    def __init__(self, sess, children, shape, **kwargs):\n        self.shape = shape\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a):\n        return array_fit_to_shape(a, self.shape)\n\n    def calc_shape(self, a):\n        return self.shape\n    \n    def calc_name(self, a):\n        return a\n\nclass SumNode(Node):\n\n    def __init__(self, sess, children, axis, **kwargs):\n        self.axis = axis\n        self.num = children[0].shape[axis]\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a):\n        return np.sum(a, axis=self.axis)\n\n    def calc_gradients(op, grad):\n        return [fl.repeat(fl.expand_dims(grad, op.axis), op.axis, op.num)]\n    \n    def calc_shape(self, a):\n        res = list(a)\n        x = self.axis\n        res = res[:x] + res[x+1:]\n        return tuple(res)\n    \n    def calc_name(self, a):\n        return 'Sum({})'.format(a)\n\nclass RepeatNode(Node):\n\n    def __init__(self, sess, children, axis, count, **kwargs):\n        self.axis = axis\n        self.count = count\n        super().__init__(sess, children, **kwargs)\n    \n    def calc_result(self, a):\n        return np.repeat(a, self.count, axis=self.axis)\n    \n    def calc_gradients(op, grad):\n        return fl.fold(grad, op.axis, grad.shape[op.axis] / op.count)\n    \n    def calc_shape(self, a):\n        res = list(a)\n        x = self.axis\n        res[x] *= self.count\n        return tuple(res)\n    \n    def calc_name(self, a):\n        return 'Repeat({},{},{})'.format(a, self.count, self.axis)\n    \nclass FoldNode(Node):\n\n    def __init__(self, sess, children, axis, num, **kwargs):\n        if children[0].shape[axis] % num != 0:\n            raise Exception('Not foldable')\n        \n        self.axis = axis\n        self.num = num\n        self.fold_cnt = children[0].shape[axis] / num\n    \n    def calc_result(self, a):\n        axis = self.axis\n        num = self.num\n        fold_cnt = self.fold_cnt\n        \n        m_shape = list(a.shape)\n        m_shape[axis] = num\n\n        m = as_strided(a, tuple(m_shape) + (fold_cnt,), a.strides + (a.strides[axis],))\n        return np.sum(m, axis=-1)\n    \n    def calc_gradients(op, grad):\n        return [fl.repeat(grad, op.axis, op.fold_cnt)]\n    \n    def calc_shape(self, a):\n        res = list(a)\n        res[self.axis] = self.num\n        return tuple(res)\n    \n    def calc_name(self, a):\n        return 'Fold({})'.format(a)\n\n\nclass ExpandDimsNode(Node):\n\n    def __init__(self, sess, children, axis, **kwargs):\n        self.axis = axis\n        super().__init__(sess, children, **kwargs)\n    \n    def calc_result(self, a):\n        return np.expand_dims(a, axis=self.axis)\n    \n    def calc_gradients(op, grad):\n        return [fl.squeeze(grad, op.axis)]\n    \n    def calc_shape(self, a):\n        res = list(a)\n        a = self.axis\n        return tuple(res[:a] + [1] + res[a:])\n    \n    def calc_name(self, a):\n        return 'ExpDims({},{})'.format(a, self.axis)\n\nclass SqueezeNode(Node):\n\n    def __init__(self, sess, children, axis, **kwargs):\n        self.axis = axis\n        super().__init__(sess, children, **kwargs)\n    \n    def calc_result(self, a):\n        return np.squeeze(a, axis=self.axis)\n    \n    def calc_gradients(op, grad):\n        return [fl.expand_dims(grad, op.axis)]\n    \n    def calc_shape(self, a):\n        res = list(a)\n        del res[self.axis]\n        return tuple(res)\n    \n    def calc_name(self, a):\n        return 'Squeeze({})'.format(a)\n\nclass ReshapeNode(Node):\n\n    def __init__(self, sess, children, target_shape, **kwargs):\n        self.target_shape = target_shape\n        super().__init__(sess, children, **kwargs)\n    \n    def calc_result(self, a):\n        self.orig_shape = a.shape\n        return np.reshape(a, self.target_shape)\n    \n    def calc_gradients(op, grad):\n        return [fl.reshape(grad, op.orig_shape)]\n    \n    def calc_shape(self, a):\n        return self.target_shape\n    \n    def calc_name(self, a):\n        return 'Reshape({},{})'.format(a, self.target_shape)\n\nclass AvgNode(Node):\n\n    def __init__(self, sess, children, axis, **kwargs):\n        self.axis = axis\n        self.num = children[0].shape[axis]\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a):\n        return np.average(a, axis=self.axis)\n\n    def calc_gradients(op, grad):\n        return [fl.repeat(fl.expand_dims(grad, op.axis), op.axis, op.num) / op.num]\n    \n    def calc_shape(self, a):\n        res = list(a)\n        x = self.axis\n        res = res[:x] + res[x+1:]\n        return tuple(res)\n    \n    def calc_name(self, a):\n        return 'Avg({})'.format(a)\n\nclass ConcatenateNode(Node):\n\n    def __init__(self, sess, children, axis=0, **kwargs):\n        self.axis = axis\n        self.alength = None\n\n        ashape = children[0].shape\n        sa = [slice(None,None,None) for _ in ashape]\n        sb = list(sa)\n        sa[axis] = slice(None, ashape[axis], None)\n        sb[axis] = slice(ashape[axis], None, None)\n        self.selector_a = sa\n        self.selector_b = sb\n\n        super().__init__(sess, children, **kwargs)\n     \n    def calc_result(self, a, b):\n        x = self.axis\n        self.alength = a.shape[x]\n        return np.concatenate((a, b), axis=x)\n    \n    def calc_gradients(op, grad):\n        return [grad[op.selector_a], grad[op.selector_b]]\n    \n    def calc_shape(self, a, b):\n        if len(a) != len(b):\n            raise Exception('Children of concat should have same length of shape')\n        for i in range(len(a)):\n            if i != self.axis and a[i] != b[i]:\n                raise Exception('Children of concat should have same shape except of concat-axis')\n        res = list(a)\n        res[self.axis] += b[self.axis]\n        return tuple(res)\n    \n    def calc_name(self, a, b):\n        return 'Concat({},{})'.format(a, b)\n\nclass SelectNode(Node):\n\n    def __init__(self, sess, children, slices, **kwargs):\n        self.slices = slices\n        self.ashape = children[0].shape\n        super().__init__(sess, children, **kwargs)\n    \n    def calc_result(self, a):\n        return a[self.slices]\n    \n    def calc_gradients(op, grad):\n        g = np.zeros(op.ashape)\n        g[op.slices] = grad\n        return [g]\n    \n    def calc_shape(self, a):\n        # TODO: advance it\n        return np.empty(a)[self.slices].shape\n    \n    def calc_name(self, a):\n        return 'Select({})'.format(a)\n        \nclass TransposeNode(Node):\n\n    def calc_result(self, a):\n        return a.T\n    \n    def calc_gradients(op, grad):\n        return [grad.T]\n\n    def calc_shape(self, a):\n        return a[::-1]\n    \n    def calc_name(self, a):\n        return 'Transpose({})'.format(a)\n\nclass CrossEntropyNode(Node):\n\n    def calc_result(self, a, b):\n        return -np.sum(a*np.log(b))\n    \n    def calc_gradients(op, grad):\n        return [None, fl.cross_entropy_grad(op.children[0], op.children[1]) * grad]\n    \n    def calc_shape(self, a, b):\n        return shape_broadcast(a, b)\n    \n    def calc_name(self, a, b):\n        return 'CrossEntropy({},{})'.format(a, b)\n\nclass CrossEntropyGradNode(Node):\n\n    def calc_result(self, a, b):\n        return a / b\n    \n    def calc_shape(self, a, b):\n        return shape_broadcast(a, b)\n    \n    def calc_name(self, a, b):\n        return 'CrossEntropyGrad({},{})'.format(a, b)\n\nclass SoftmaxCrossEntropyLossNode(Node):\n\n    def __init__(self, sess, children, axis, **kwargs):\n        self.axis = axis\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a, b):\n        ax = self.axis\n        e = np.exp(b - np.max(b, axis=ax, keepdims=True))\n        e = e / np.sum(e, axis=ax, keepdims=True)\n        return -np.sum(a*np.log(e))\n    \n    def calc_gradients(op, grad):\n        v0, v1 = op.children\n        return [None, fl.softmax_cross_entropy_loss_grad(v0, v1, op.axis) * grad]\n    \n    def calc_shape(self, a, b):\n        return shape_broadcast(a, b)\n    \n    def calc_name(self, a, b):\n        return 'SCEL({},{})'.format(a, b)\n\nclass SoftmaxCrossEntropyLossGradNode(Node):\n\n    def __init__(self, sess, children, axis, **kwargs):\n        self.axis = axis\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, a, b):\n        ax = self.axis\n        e = np.exp(b - np.max(b, axis=ax, keepdims=True))\n        es = np.sum(e, axis=ax, keepdims=True)\n        e = e / es\n        return e * np.sum(a, axis=ax, keepdims=True) - a\n    \n    def calc_shape(self, a, b):\n        return shape_broadcast(a, b)\n    \n    def calc_name(self, a, b):\n        return 'SCELGrad({},{})'.format(a, b)\n\n    \nclass Conv2DNode(Node):\n\n    def calc_result(self, a, b):\n        self.filter_wh = b.shape[2:]\n        return mult_conv2d(a, b)\n    \n    def calc_gradients(op, grad):\n        g = fl.conv2d_gradient(grad, op.children[0], self.filter_wh)\n        return [None, g]\n    \n    def calc_shape(self, a, b):\n        if a[1] != b[1]:\n            raise Exception('Conv2D: not proper filter in_channel size (2nd dim)')\n        return (a[0], b[0], a[2] - b[2] + 1, a[3] - b[3] + 1)\n    \n    def calc_name(self, a, b):\n        return 'Conv2D({},{})'.format(a, b)\n\nclass Conv2DGradientNode(Node):\n\n    def __init__(self, sess, children, filter_wh, **kwargs):\n        self.filter_wh = filter_wh\n        super().__init__(sess, children, **kwargs)\n\n    def calc_result(self, grad, a):\n        return mult_conv2d_gradient(grad, a, self.filter_wh)\n    \n    def calc_shape(self, a, b):\n        return (grad[1], a[1], self.filter_wh[0], self.filter_wh[1])\n    \n    def calc_name(self, a, b):\n        return 'Conv2DGrad({},{})'.format(a, b)\n"""
flow/node/funcs.py,6,"b""import numpy as np\nimport flow as fl\n\ndef zeros(sess, shape, name='zero'):\n    return fl.Placeholder(sess, np.zeros(shape), name)\n\ndef ones(sess, shape, name='ones'):\n    return fl.Placeholder(sess, np.ones(shape), name)\n\ndef empty(sess, shape, name='empty'):\n    return fl.Placeholder(sess, np.empty(shape), name)\n\ndef zeros_like(sess, a, name='zero'):\n    return fl.Placeholder(sess, np.zeros_like(a.shape), name)\n\ndef ones_like(sess, a, name='ones'):\n    return fl.Placeholder(sess, np.ones_like(a.shape ), name)\n\ndef empty_like(sess, a, name='empty'):\n    return fl.Placeholder(sess, np.empty_like(a.shape), name)\n\ndef assign(a, b, name=None):\n    return fl.AssignNode(a.sess, [a, b], name=name)\n\ndef group(*args, name=None):\n    return fl.GroupNode(a.sess, args, name=name)\n\ndef matmul(a, b, name=None):\n    return fl.MatmulNode(a.sess, [a, b], name=name)\n\ndef neg(a, name=None):\n    return fl.NegNode(a.sess, [a], name=name)\n\ndef add(a, b, name=None):\n    return fl.AddNode(a.sess, [a, b], name=name)\n\ndef sub(a, b, name=None):\n    return fl.SubNode(a.sess, [a, b], name=name)\n\ndef mul(a, b, name=None):\n    return fl.MulNode(a.sess, [a, b], name=name)\n\ndef div(a, b, name=None):\n    return fl.DivNode(a.sess, [a, b], name=name)\n\ndef sigmoid(a, name=None):\n    return fl.SigmoidNode(a.sess, [a], name=name)\n\ndef relu(a, name=None):\n    return fl.ReluNode(a.sess, [a], name=name)\n\ndef relu_grad(a, name=None):\n    return fl.ReluGradNode(a.sess, [a], name=name)\n\ndef elu(a, name=None):\n    return fl.EluNode(a.sess, [a], name=name)\n\ndef elu_grad(a, grad, name=None):\n    return fl.EluGradNode(a.sess, [a, grad], name=name)\n\ndef gelu(a, name=None):\n    return fl.sigmoid(1.702 * a) * a\n\ndef leaky_relu(a, alpha=0.2, name=None):\n    return fl.LeakyReluNode(a.sess, [a], alpha, name=name)\n\ndef leaky_relu_grad(a, grad, alpha=0.2, name=None):\n    return fl.LeakyReluGradNode(a.sess, [a, grad], alpha, name=name)\n\ndef tanh(a, name=None):\n    return fl.TanhNode(a.sess, [a], name=name)\n\ndef softmax(a, axis, name=None):\n    return fl.SoftmaxNode(a.sess, [a], axis, name=name)\n\ndef softmax_grad(a, axis, name=None):\n    return fl.SoftmaxGradNode(a.sess, [a], axis, name=name)\n\ndef log(a, name=None):\n    return fl.LogNode(a.sess, [a], name=name)\n\ndef log_grad(a, grad, name=None):\n    return fl.LogGradNode(a.sess, [a, grad], name=name)\n\ndef exp(a, name=None):\n    return fl.ExpNode(a.sess, [a], name=name)\n\ndef square(a, name=None):\n    return fl.SquareNode(a.sess, [a], name=name)\n\ndef l2loss(a, b, name=None):\n    return fl.L2LossNode(a.sess, [a, b], name=name)\n\ndef transpose(a, name=None):\n    return fl.TransposeNode(a.sess, [a], name=name)\n\ndef concat(a, b, axis=0, name=None):\n    return fl.ConcatenateNode(a.sess, [a, b], axis, name=name)\n\ndef fold(a, axis, num, name=None):\n    return fl.FoldNode(a.sess, [a], axis, num, name=name)\n\ndef repeat(a, axis, count, name=None):\n    return fl.RepeatNode(a.sess, [a], axis, count, name=name)\n\ndef select(a, key, name=None):\n    return fl.SelectNode(a.sess, [a], key, name=name)\n\ndef reduce_shape(a, shape, name=None):\n    return fl.ReduceShapeNode(a.sess, [a], shape, name=name)\n\ndef sum(a, axis, name=None):\n    return fl.SumNode(a.sess, [a], axis, name=name)\n\ndef expand_dims(a, axis, name=None):\n    return fl.ExpandDimsNode(a.sess, [a], axis, name=name)\n\ndef squeeze(a, axis, name=None):\n    return fl.SqueezeNode(a.sess, [a], axis, name=name)\n\ndef reshape(a, shape, name=None):\n    return fl.ReshapeNode(a.sess, [a], shape, name=name)\n\ndef avg(a, axis, name=None):\n    return fl.AvgNode(a.sess, [a], axis, name=name)\n\ndef cross_entropy(a, b, name=None):\n    return fl.CrossEntropyNode(a.sess, [a, b], name=name)\n\ndef cross_entropy_grad(a, b, name=None):\n    return fl.CrossEntropyGradNode(a.sess, [a, b], name=name)\n\ndef softmax_cross_entropy_loss(a, b, axis, name=None):\n    return fl.SoftmaxCrossEntropyLossNode(a.sess, [a, b], axis, name=name)\n\ndef softmax_cross_entropy_loss_grad(a, b, axis, name=None):\n    return fl.SoftmaxCrossEntropyLossGradNode(a.sess, [a, b], axis, name=name)\n\ndef conv2d(a, b, name=None):\n    return fl.Conv2DNode(a.sess, [a, b], name=name)\n\ndef conv2d_grad(grad, b, filter_wh, name=None):\n    return fl.Conv2DGradientNode(a.sess, [a, b], filter_wh, name=name)\n    \ndef gradients(ys, xs):\n    \n    sess = ys[0].sess\n    for node in sess.nodes:\n        node.grad_parent_num = 0\n\n    def clean_back_grad(x):\n        x.grad_cache = None\n        x.grad_recv_num = 0\n        for y in x.children:\n            if y.grad_parent_num == 0:\n                clean_back_grad(y)\n            y.grad_parent_num += 1\n        \n    for y in ys: clean_back_grad(y)\n    for y in ys: y.grad_cache = ones(y.sess, (1,))\n\n    def calc_back_grad(x):\n        if len(x.children) == 0:\n            return\n        if not hasattr(x, 'calc_gradients'):\n            raise Exception('Cannot calc grad from {}'.format(x.name))\n        grads = x.calc_gradients(x.grad_cache)\n        for idx, child in enumerate(x.children):\n            if child.grad_cache is None:\n                child.grad_cache = grads[idx]\n            else:\n                child.grad_cache += grads[idx]\n            child.grad_recv_num += 1\n            if child.grad_recv_num >= child.grad_parent_num:\n                calc_back_grad(child)\n\n    for y in ys: calc_back_grad(y)\n    res = [x.grad_cache if hasattr(x, 'grad_cache') else None for x in xs]\n    for y in ys: clean_back_grad(y)\n\n    return res\n            """
flow/node/util.py,14,"b""import numpy as np\n\ndef xavier(shape, fan_in, fan_out, uniform=False):\n    x = np.sqrt((6 if uniform else 2) / (fan_in + fan_out))\n    res = (np.random.rand if uniform else np.random.randn)(*shape) * x\n    return res\n\ndef shape_broadcast(s0, s1):\n    res = []\n    if len(s0) < len(s1):\n        s0, s1 = s1, s0\n    l0 = len(s0)\n    l1 = len(s1)\n    dl = l0 - l1\n    for i in range(dl):\n        res.append(s0[i])\n    for i in range(l1):\n        e0 = s0[dl + i]\n        e1 = s1[i]\n        if e0 == e1 or e0 == 1 or e1 == 1:\n            res.append(max(e0, e1))\n        else:\n            raise Exception('Shape broadcasting error: {}, {}'.format(s0, s1))\n    return tuple(res)\n\ndef array_fit_to_shape(a, shape):\n    d = len(a.shape) - len(shape)\n    if d < 0:\n        raise Exception('Fitting array to shape error: {}, {}'.format(a.shape, shape))\n    if d > 0:\n        a = np.sum(a, axis=tuple(range(d)))\n    for i in range(len(shape)):\n        if a.shape[i] != shape[i]:\n            if shape[i] == 1:\n                a = np.sum(a, axis=i, keepdims=True)\n            # else:\n            #     raise Exception('Fitting array to shape error: {}, {}'.format(a.shape, shape))\n    return a\n\ndef conv2d(a, f):\n    '''\n    Pure 2d convolution, one channel\n    '''\n    s = f.shape + tuple(np.subtract(a.shape, f.shape) + 1)\n    subM = np.lib.stride_tricks.as_strided(a, shape = s, strides = a.strides * 2)\n    return np.einsum('ij,ijkl->kl', f, subM)\n\ndef conv3d(a, f):\n    '''\n    Pure 3d convolution\n    '''\n    s = f.shape + tuple(np.subtract(a.shape, f.shape) + 1)\n    subM = np.lib.stride_tricks.as_strided(a, shape = s, strides = a.strides * 2)\n    return np.einsum('ijk,ijklmn->lmn', f, subM)\n\ndef mult_conv2d(data, filters):\n    d_shape = data.shape # [batch_size, in_channel, width, height]\n    d_strides = data.strides\n    f_shape = filters.shape # [out_channel, in_channel, filter_width, filter_height]\n    # [batch_size, width - f_width + 1, height - f_height + 1, in_channel, filter_width, filter_height]\n    m_shape = (d_shape[0], d_shape[2] - f_shape[2] + 1, d_shape[3] - f_shape[3] + 1, d_shape[1], f_shape[2], f_shape[3])\n    # m_shape: (batch_size, shrink_width, shrink_height, in_channel, filter_width, filter_height)\n    m_strides = (d_strides[0], d_strides[2], d_strides[3], d_strides[1], d_strides[2], d_strides[3])\n    m = np.lib.stride_tricks.as_strided(data, shape = m_shape, strides = m_strides)\n    # [batch_size, out_channel, width - filter_width + 1, height - filter_height + 1]\n    return np.einsum('jabc,iklabc->ijkl', filters, m)\n\ndef mult_conv2d_gradient(gradient, a, filter_wh):\n    '''\n    gradient: [batch_size, out_channel, width - fwidth + 1, height - fheight + 1]\n    '''\n    ashape = a.shape\n    astrides = a.strides\n    # [batch_size, width - f_width + 1, height - f_height + 1, in_channel, filter_width, filter_height]\n    m_shape = (ashape[0], ashape[2] - filter_wh[0] + 1, ashape[3] - filter_wh[1] + 1, ashape[1], filter_wh[0], filter_wh[1])\n    m_strides = (astrides[0], astrides[2], astrides[3], astrides[1], astrides[2], astrides[3])\n    m = np.lib.stride_tricks.as_strided(a, shape=m_shape, strides=m_strides)\n    # [out_channel, in_channel, filter_width, filter_height]\n    return np.einsum('aibc,abcjkl->ijkl', gradient, m)\n"""
