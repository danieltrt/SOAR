file_path,api_count,code
01-colorspace.py,0,"b'import numpy\nimport cv2\n\n# this exercise references ""Color Transfer between Images"" by Reinhard et al.\n\nnumpyInput = cv2.imread(filename=\'./samples/fruits.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\n\n# convert numpyInput to the LMS color space and store it in numpyOutput according to equation 4\n\n# the two ways i see for doing this (there are others as well though) are as follows\n# either iterate over each pixel, performing the matrix-vector multiplication one by one and storing the result in a pre-allocated numpyOutput\n# or split numpyInput into its three channels, linearly combining them to obtain the three converted color channels, before using numpy.stack to merge them\n\n# keep in mind that that opencv arranges the color channels typically in the order of blue, green, red\n\n\n\n\n\n\n\n\n\ncv2.imwrite(filename=\'./01-colorspace.png\', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))'"
02-colortransfer.py,0,"b'import numpy\nimport cv2\n\n# this exercise references ""Color Transfer between Images"" by Reinhard et al.\n\nnumpyFrom = cv2.imread(filename=\'./samples/transfer-from.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\nnumpyTo = cv2.imread(filename=\'./samples/transfer-to.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\n\n# match the color statistics of numpyTo to those of numpyFrom\n\n# in order for make matching the statistics more meaningful, the images are first converted to the LAB color space\n\nnumpyFrom = cv2.cvtColor(src=numpyFrom, code=cv2.COLOR_BGR2Lab)\nnumpyTo = cv2.cvtColor(src=numpyTo, code=cv2.COLOR_BGR2Lab)\n\n# not that the paper uses the notation target / source, here the notation is from / to\n# calculate the per-channel mean of the data points / intensities of numpyTo, and subtract these from numpyTo according to equation 10\n# calculate the per-channel std of the data points / intensities of numpyTo and numpyFrom, and scale numpyTo according to equation 11\n# calculate the per-channel mean of the data points / intensities of numpyFrom, and add these to numpyTo according to the description after equation 11\n\n\n\n\n\n\n\n\n\n# after matching the statistics, some of the intensity values might be out of the valid range and are hence clipped / clamped\n\nnumpyTo[:, :, 0] = numpyTo[:, :, 0].clip(0.0, 100.0)\nnumpyTo[:, :, 1] = numpyTo[:, :, 1].clip(-127.0, 127.0)\nnumpyTo[:, :, 2] = numpyTo[:, :, 2].clip(-127.0, 127.0)\n\n# finaly, the matched image is being converted back to the RGB color space\n\nnumpyOutput = cv2.cvtColor(src=numpyTo, code=cv2.COLOR_Lab2BGR)\n\ncv2.imwrite(filename=\'./02-colortransfer.png\', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))'"
03-demosaicing.py,0,"b'import numpy\nimport cv2\n\n# this exercise references ""Interactions Between Color Plane Interpolation and Other Image Processing Functions in Electronic Photography"" by Adams\n\nnumpyInput = cv2.imread(filename=\'./samples/demosaicing.png\', flags=cv2.IMREAD_GRAYSCALE).astype(numpy.float32) / 255.0\n\n# demosaic numpyInput by using bilinear interpolation as shown in the slides and described in section 3.3\n\n# the input has the following beyer pattern, id est that the top left corner is red\n\n# RGRGRG ....\n# GBGBGB ....\n# RGRGRG ....\n# GBGBGB ....\n# ...........\n# ...........\n\n# the straightforward way that i see for doing this (there are others as well though) is to iterate over each pixel and resolving each of the four possible cases\n# to simplify this, you can iterate from (1 to numpyInput.shape[0] - 1) and (1 to numpyInput.shape[1] - 1) to avoid corner cases, numpyOutput is accordingly one pixel smaller on each side\n\nnumpyOutput = numpy.zeros([ numpyInput.shape[0] - 2, numpyInput.shape[1] - 2, 3 ], numpy.float32)\n\n# notice that to fill in the missing greens, you will always be able to take the average of four neighboring values\n# however, depending on the case, you either get four or only two neighboring values for red and blue\n# this is perfectly fine, in this case you can simply use the average of two values if only two neighbors are available\n\n\n\n\n\n\n\n\n\ncv2.imwrite(filename=\'./03-demosaicing.png\', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))'"
04-convolution.py,0,"b'import numpy\nimport cv2\n\n# this exercise references ""Interactions Between Color Plane Interpolation and Other Image Processing Functions in Electronic Photography"" by Adams\n\nnumpyInput = cv2.imread(filename=\'./samples/demosaicing.png\', flags=cv2.IMREAD_GRAYSCALE).astype(numpy.float32) / 255.0\n\n# demosaic numpyInput by using convolutions to mimic bilinear interpolation as shown in the slides and described in section 3.3\n\n# the input has the following beyer pattern, id est that the top left corner is red\n\n# RGRGRG ....\n# GBGBGB ....\n# RGRGRG ....\n# GBGBGB ....\n# ...........\n# ...........\n\n# to do this using convolutions, the first step is to separate the colors from numpyInput into four channels such that numpyOutput[:, :, 1] for example becomes\n\n# 0G0G0G ....\n# G0G0G0 ....\n# 0G0G0G ....\n# G0G0G0 ....\n# ...........\n# ...........\n\n# since this can be tricky and you might not be perfectly familiar with indexing and splicing matrices yet, this is already done for you below\n\nnumpyOutput = numpy.zeros([ numpyInput.shape[0], numpyInput.shape[1], 3 ], numpy.float32)\n\nnumpyOutput[1::2, 1::2, 0] = numpyInput[1::2, 1::2]\nnumpyOutput[0::2, 1::2, 1] = numpyInput[0::2, 1::2]\nnumpyOutput[1::2, 0::2, 1] = numpyInput[1::2, 0::2]\nnumpyOutput[0::2, 0::2, 2] = numpyInput[0::2, 0::2]\n\n# for each channel in numpyOutput, use a suitable convolution to perform the demosaicing and store the output back in its respective channel in numpyOutput\n# we already discussed in class what the appropriate kernel for the green channel is, determining the kernel for the other two channels is up to you\n# to be able to convolve a channel from numpyOutput and storing the result back in the same channel, the convolution must not affect the resolution\n# we need padding for the convolution to not affect the resolution, this is already built into OpenCV but make sure to use cv2.BORDER_DEFAULT as the border type\n\n\n\n\n\n\n\n\n\ncv2.imwrite(filename=\'./04-convolution.png\', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))'"
05-median.py,0,"b""import numpy\r\nimport cv2\r\n\r\nnumpyInput = cv2.imread(filename='./samples/noise.png', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\r\n\r\n# use a gaussian kernel of size 3x3 to blur numpyInput and store the result in numpyFirst\r\n# let OpenCV determine the appropriate sigma / deviation of the gaussian kernel for you\r\n\r\n# use a median filter of size 3x3 to filter numpyInput and store the result in numpySecond\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ncv2.imwrite(filename='./05-median-1.png', img=(numpyFirst * 255.0).clip(0.0, 255.0).astype(numpy.uint8))\r\ncv2.imwrite(filename='./05-median-2.png', img=(numpySecond * 255.0).clip(0.0, 255.0).astype(numpy.uint8))"""
06-spectrum.py,0,"b""import numpy\nimport cv2\n\nnumpyImage = cv2.imread(filename='./samples/lenna.png', flags=cv2.IMREAD_GRAYSCALE).astype(numpy.float32) / 255.0\n\n# creating a relief kernel that you are subsequently asked to apply in the frequency space\n\nnumpyKernel = numpy.array([ [ -2, -1, 0 ], [ -1, 1, 1 ], [ 0, 1, 2 ] ], numpy.float32)\n\n# pad numpyKernel such that its resolution is equal to the resolution of numpyImage\n# roll it one pixel to the left and one pixel to the top in order to adjust the phase\n# convert numpyKernel into the frequency space by using the fourier transform\n# similarly, convert numpyImage into the frequency space, no need to pad or roll it though\n# perform the convolution in the frequency space by using the hadamard product\n# convert the result back to the spacial domain by using the inverse fourier transform\n# the result / numpyImage should look as if the convolution was done in the spatial domain\n\n\n\n\n\n\n\n\n\n# plotting the frequency spectrum of the gaussian kernel while making sure that we only plot the real part\n# the logarithmic scaling improves the visualization, the constant bias avoids log(0-1) which is undefined / negative\n# the value range for the spectrum is outside of [ 0, 1 ] and a color mapping is applied before saving it\n\nnumpySpectrum = numpy.log(numpy.fft.fftshift(numpyKernel).__abs__() + 1.0)\nnumpySpectrum = numpySpectrum / numpySpectrum.max()\n\nnumpyImage = numpyImage.real\n\ncv2.imwrite(filename='./06-spectrum-1.png', img=cv2.applyColorMap(src=(numpySpectrum * 255.0).clip(0.0, 255.0).astype(numpy.uint8), colormap=cv2.COLORMAP_WINTER))\ncv2.imwrite(filename='./06-spectrum-2.png', img=(numpyImage * 255.0).clip(0.0, 255.0).astype(numpy.uint8))"""
07-pyramid.py,0,"b'import numpy\nimport cv2\n\n# this exercise references ""The Laplacian Pyramid as a Compact Image Code"" by Burt and Adelson\n\nnumpyInput = cv2.imread(filename=\'./samples/lenna.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\n\n# create a laplacian pyramid with four levels as described in the slides as well as in the referenced paper\n\nnumpyPyramid = []\n\n\n\n\n\n\n\n\n\n# the following iterates over the levels in numpyPyramid and saves them as an image accordingly\n# level four is just a small-scale representation of the original input image and can be saved as usual\n# the value range for the other levels are outside of [0, 1] and a color mapping is applied before saving them\n\nfor intLevel in range(len(numpyPyramid)):\n\tif intLevel == len(numpyPyramid) - 1:\n\t\tcv2.imwrite(filename=\'./07-pyramid-\' + str(intLevel + 1) + \'.png\', img=(numpyPyramid[intLevel] * 255.0).clip(0.0, 255.0).astype(numpy.uint8))\n\n\telif intLevel != len(numpyPyramid) - 1:\n\t\tcv2.imwrite(filename=\'./07-pyramid-\' + str(intLevel + 1) + \'.png\', img=cv2.applyColorMap(src=((numpyPyramid[intLevel] + 0.5) * 255.0).clip(0.0, 255.0).astype(numpy.uint8), colormap=cv2.COLORMAP_COOL))\n\n\t# end\n# end'"
08-homography.py,0,"b""import numpy\nimport cv2\nimport math\n\nnumpyInput = cv2.imread(filename='./samples/homography-2.png', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\n\n# estimate the homography matrix between matching points and warp the image using bilinear interpolation\n\n# creating the mapping between the four corresponding points\n\nintSrc = [ [266, 343], [646, 229], [388, 544], [777, 538] ]\nintDst = [ [302, 222], [746, 231], [296, 490], [754, 485] ]\n\n# construct the linear homogeneous system of equations\n# use a singular value decomposition to solve the system\n# in practice, cv2.findHomography can be used for this\n# however, do not use this function for this exercise\n\n\n\n\n\n\n\n\n\n# use a backward warping algorithm to warp the source\n# to do so, we first create the inverse transform\n# use bilinear interpolation for resampling\n# in practice, cv2.warpPerspective can be used for this\n# however, do not use this function for this exercise\n\nnumpyHomography = numpy.linalg.inv(numpyHomography)\n\nnumpyOutput = numpy.zeros(numpyInput.shape, numpy.float32)\n\n\n\n\n\n\n\n\n\ncv2.imwrite(filename='./08-homography.png', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))"""
09-colorize.py,0,"b""import numpy\nimport cv2\n\nnumpyInput = cv2.imread(filename='./samples/prokudin.png', flags=cv2.IMREAD_GRAYSCALE).astype(numpy.float32) / 255.0\n\n# align the individual images such that their combination results in a proper color image\n\n# splitting the input into the three individual channels while slightly cropping the boundary\n\nintFirst, intSecond = int(1.0 * numpyInput.shape[0] / 3.0), int(2.0 * numpyInput.shape[0] / 3.0)\n\nnumpyB = numpyInput[:intFirst, :][50:-50, 50:-50]\nnumpyG = numpyInput[intFirst:intSecond, :][50:-50, 50:-50]\nnumpyR = numpyInput[intSecond:, :][50:-50, 50:-50]\n\n# find the homography matrices between the images using cv2.findTransformECC\n# based on these matrices, warp numpyG and numpyR towards numpyB using cv2.warpPerspective\n# make sure to use inverse warping as stated in the documentation of cv2.findTransformECC\n# once the channels are aligned, they can simply be stacked to create the color image\n\n\n\n\n\n\n\n\n\nnumpyOutput = numpy.stack([ numpyB, numpyG, numpyR ], 2)\n\ncv2.imwrite(filename='./09-colorize.png', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))"""
10-multiband.py,0,"b'import numpy\nimport cv2\n\n# this exercise references ""Pyramid Methods in Image Processing"" by Adelson et al.\n\nnumpyFirst = cv2.imread(filename=\'./samples/multiband-apple.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\nnumpySecond = cv2.imread(filename=\'./samples/multiband-orange.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\n\n# blend the apple and the orange using multiband blending with laplacian pyramids\n\n# creating a laplacian pyramid with seven levels for each of the two images\n\nnumpyFirst = [ numpyFirst ]\nnumpySecond = [ numpySecond ]\n\nfor intLevel in range(6):\n\tnumpyFirst.append(cv2.pyrDown(numpyFirst[-1]))\n\tnumpySecond.append(cv2.pyrDown(numpySecond[-1]))\n\n\tnumpyFirst[-2] -= cv2.pyrUp(numpyFirst[-1])\n\tnumpySecond[-2] -= cv2.pyrUp(numpySecond[-1])\n# end\n\n# combine the two laplacian pyramids and create a new laplacian pyramid to blend the two images\n# specifically, take the left half from numpyFirst and the right half from numpySecond at each level\n# afterwards, collapse numpyPyramid to obtain the blended result and store it in numpyOutput\n\nnumpyPyramid = []\n\n\n\n\n\n\n\n\n\ncv2.imwrite(filename=\'./10-multiband.png\', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))'"
11-seam.py,0,"b'import numpy\nimport cv2\n\n# this exercise references ""Seam Carving for Content-Aware Image Resizing"" by Avidan and Shamir\n\nnumpyInput = cv2.imread(filename=\'./samples/seam.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\n\n# implement content-aware image resizing to reduce the width of the image by one-hundred pixels\n\n# using a heuristic energy function to extract an energy map\n\nnumpyEnergy = cv2.Sobel(src=cv2.cvtColor(src=numpyInput, code=cv2.COLOR_BGR2GRAY), ddepth=-1, dx=1, dy=0, ksize=3, scale=1, delta=0.0, borderType=cv2.BORDER_DEFAULT).__abs__() \\\n\t\t\t+ cv2.Sobel(src=cv2.cvtColor(src=numpyInput, code=cv2.COLOR_BGR2GRAY), ddepth=-1, dx=0, dy=1, ksize=3, scale=1, delta=0.0, borderType=cv2.BORDER_DEFAULT).__abs__()\n\n# find and remove one-hundred vertical seams, can potentially be slow\n\nfor intRemove in range(100):\n\tintSeam = []\n\n\t# construct the cumulative energy map using the dynamic programming approach\n\t# initialize the cumulative energy map by making a copy of the energy map\n\t# when iterating over the rows, ignore M(y-1, ...) that are out of bounds\n\n\t# several seams can have the same energy, use the following for consistency\n\t# start at the leftmost M(height-1, x) with the lowest cumulative energy\n\t# should M(y-1, x) be equal to M(y-1, x-1) or M(y-1, x+1) then use (y-1, x)\n\t# similarly should M(y-1, x-1) be equal to M(y-1, x+1) then use (y-1, x-1)\n\n\t# the intSeam array should be a list of integers representing the seam\n\t# a seam from the top left to the bottom right: intSeam = [0, 1, 2, 3, 4, ...]\n\t# a seam that is just the first column: intSeam = [0, 0, 0, 0, 0, 0 , ...]\n\n\n\n\n\n\n\n\n\n\t# some sanity checks, such that the length of the seam is equal to the height of the image\n\t# furthermore iterating over the seam and making sure that it is a connected sequence\n\n\tassert(len(intSeam) == numpyInput.shape[0])\n\n\tfor intY in range(1, len(intSeam)):\n\t\tassert(intSeam[intY] - intSeam[intY - 1] in [ -1, 0, 1 ])\n\t# end\n\n\t# change the following condition to true if you want to visualize the seams that are being removed\n\t# note that this will not work if you are connected to the linux lab via ssh but no x forwarding\n\n\tif False:\n\t\tnumpyM /= numpyM.max()\n\n\t\tfor intY in range(len(intSeam)):\n\t\t\tnumpyInput[intY, intSeam[intY], :] = numpy.array([ 0.0, 0.0, 1.0 ], numpy.float32)\n\t\t\tnumpyM[intY, intSeam[intY]] = numpy.array([ 1.0 ], numpy.float32)\n\t\t# end\n\n\t\tcv2.imshow(winname=\'numpyInput\', mat=numpyInput)\n\t\tcv2.imshow(winname=\'numpyM\', mat=numpyM)\n\t\tcv2.waitKey(delay=10)\n\t# end\n\n\t# removing the identified seam by iterating over each row and shifting them accordingly\n\t# after the shifting in each row, the image and the energy map are cropped by one pixel on the right\n\n\tfor intY in range(len(intSeam)):\n\t\tnumpyInput[intY, intSeam[intY]:-1, :] = numpyInput[intY, (intSeam[intY] + 1 ):, :]\n\t\tnumpyEnergy[intY, intSeam[intY]:-1] = numpyEnergy[intY, (intSeam[intY] + 1):]\n\t# end\n\n\tnumpyInput = numpyInput[:, :-1, :]\n\tnumpyEnergy = numpyEnergy[:, :-1]\n# end\n\ncv2.imwrite(filename=\'./11-seam.png\', img=(numpyInput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))'"
12-tonemapping.py,0,"b'import numpy\nimport cv2\nimport os\nimport zipfile\nimport matplotlib.pyplot\n\n# this exercise references ""Photographic Tone Reproduction for Digital Images"" by Reinhard et al.\n\nnumpyRadiance = cv2.imread(filename=\'./samples/ahwahnee.hdr\', flags=-1)\n\n# perform tone mapping according to the photographic luminance mapping\n\n# first extracting the intensity from the color channels\n# note that the eps / delta is to avoid divisions by zero and log of zero\n\nnumpyIntensity = cv2.cvtColor(src=numpyRadiance, code=cv2.COLOR_BGR2GRAY) + 0.0000001\n\n# start off by approximating the key of numpyIntensity according to equation 1\n# then normalize numpyIntensity using a = 0.18 according to equation 2\n# afterwards, apply the non-linear tone mapping prescribed by equation 3\n# finally obtain numpyOutput using the ad-hoc formula with s = 0.6 from the slides\n\n\n\n\n\n\n\n\n\ncv2.imwrite(filename=\'./12-tonemapping.png\', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))'"
13-fusion.py,0,"b'import numpy\nimport cv2\n\n# this exercise references ""Exposure Fusion"" by Mertens et al.\n\nnumpyInputs = [\n\tcv2.imread(filename=\'./samples/fusion-1.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0,\n\tcv2.imread(filename=\'./samples/fusion-2.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0,\n\tcv2.imread(filename=\'./samples/fusion-3.png\', flags=cv2.IMREAD_COLOR).astype(numpy.float32) / 255.0\n]\n\n# use the quality measures to extract a weight map for each image according to section 3.1\n# set the weighting exponents to one, thus equaling the contrition of contrast, saturation, and exposedness\n# make sure to add 0.0000001 to the weight map of each image to avoid divisions by zero in the subsequent step\n# normalize the weight maps such that they sum up to one at each pixel as described in section 3.2\n# store the three weight maps in the numpyWeights array which will be used below to perform the blending\n\nnumpyWeights = []\n\n\n\n\n\n\n\n\n\n# creating the laplacian and gaussian pyramids to perform multiband blending\n# defining separate functions for this steps makes the code easier to read\n\ndef gaussian_pyramid(numpyInput, intLevels):\n\tnumpyPyramid = [ numpyInput ]\n\n\tfor intLevel in range(intLevels):\n\t\tnumpyPyramid.append(cv2.pyrDown(numpyPyramid[-1]))\n\t# end\n\n\treturn numpyPyramid\n# end\n\ndef laplacian_pyramid(numpyInput, intLevels):\n\tnumpyPyramid = [ numpyInput ]\n\n\tfor intLevel in range(intLevels):\n\t\tnumpyPyramid.append(cv2.pyrDown(numpyPyramid[-1]))\n\n\t\tnumpyPyramid[-2] -= cv2.pyrUp(numpyPyramid[-1])\n\t# end\n\n\treturn numpyPyramid\n# end\n\nnumpyInputs = [ laplacian_pyramid(numpyInput, 6) for numpyInput in numpyInputs ]\nnumpyWeights = [ gaussian_pyramid(numpyWeight, 6) for numpyWeight in numpyWeights ]\n\n# constructing a laplacian pyramid by using the weights from the gaussian pyramid\n# eventually obtaining the fused result by recovering the output from the merged pyramid\n\nnumpyPyramid = []\n\nfor intLevel in range(len(numpyInputs[0])):\n\tnumpyPyramid.append(sum([ numpyInputs[intInput][intLevel] * numpyWeights[intInput][intLevel][:, :, None] for intInput in range(len(numpyInputs)) ]))\n# end\n\nnumpyOutput = numpyPyramid.pop(-1)\n\nwhile len(numpyPyramid) > 0:\n\tnumpyOutput = cv2.pyrUp(numpyOutput) + numpyPyramid.pop(-1)\n# end\n\ncv2.imwrite(filename=\'./13-fusion-1.png\', img=(numpyWeights[0][0] * 255.0).clip(0.0, 255.0).astype(numpy.uint8))\ncv2.imwrite(filename=\'./13-fusion-2.png\', img=(numpyWeights[1][0] * 255.0).clip(0.0, 255.0).astype(numpy.uint8))\ncv2.imwrite(filename=\'./13-fusion-3.png\', img=(numpyWeights[2][0] * 255.0).clip(0.0, 255.0).astype(numpy.uint8))\ncv2.imwrite(filename=\'./13-fusion-4.png\', img=(numpyOutput * 255.0).clip(0.0, 255.0).astype(numpy.uint8))'"
14-mnist.py,0,"b""import torch\r\nprint(torch.__version__)\r\n\r\nimport torchvision\r\nprint(torchvision.__version__)\r\n\r\nimport cv2\r\nimport numpy\r\n\r\n# find samples from the dataset that are misclassified by the provided model\r\n\r\n# creating a data loader for the training samples of the mnist dataset\r\n# specifying the batch size and making sure it runs in a background thread\r\n\r\nobjectDataset = torch.utils.data.DataLoader(\r\n\tbatch_size=64,\r\n\tshuffle=False,\r\n\tnum_workers=1,\r\n\tpin_memory=False,\r\n\tdataset=torchvision.datasets.MNIST(\r\n\t\troot='./mnist/',\r\n\t\ttrain=False,\r\n\t\tdownload=True,\r\n\t\ttransform=torchvision.transforms.Compose([\r\n\t\t\ttorchvision.transforms.ToTensor(),\r\n\t\t\ttorchvision.transforms.Normalize(tuple([ 0.1307 ]), tuple([ 0.3081 ]))\r\n\t\t])\r\n\t)\r\n)\r\n\r\n# reducing the size of the dataset to limit the number of misclassified samples\r\n\r\nobjectDataset.dataset.test_data = objectDataset.dataset.test_data[:700]\r\n\r\n# defining the network, just a basic convolutional neural network from the slides\r\n\r\nclass Network(torch.nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper(Network, self).__init__()\r\n\r\n\t\tself.conv1 = torch.nn.Conv2d(1, 32, kernel_size=5)\r\n\t\tself.conv2 = torch.nn.Conv2d(32, 64, kernel_size=5)\r\n\t\tself.fc1 = torch.nn.Linear(256, 200)\r\n\t\tself.fc2 = torch.nn.Linear(200, 10)\r\n\t# end\r\n\r\n\tdef forward(self, x):\r\n\t\tx = self.conv1(x)\r\n\t\tx = torch.nn.functional.relu(x)\r\n\t\tx = torch.nn.functional.max_pool2d(x, kernel_size=3)\r\n\t\tx = self.conv2(x)\r\n\t\tx = torch.nn.functional.relu(x)\r\n\t\tx = torch.nn.functional.max_pool2d(x, kernel_size=2)\r\n\t\tx = x.view(-1, 256)\r\n\t\tx = self.fc1(x)\r\n\t\tx = torch.nn.functional.relu(x)\r\n\t\tx = self.fc2(x)\r\n\r\n\t\treturn torch.nn.functional.log_softmax(x, dim=1)\r\n\t# end\r\n# end\r\n\r\nmoduleNetwork = Network()\r\n\r\n# loading the provided weights, this exercise is not about training the network\r\n\r\nmoduleNetwork.load_state_dict(torch.load('./14-mnist.pytorch'))\r\n\r\n# setting the network to the evaluation mode, this makes no difference here though\r\n\r\nmoduleNetwork.eval()\r\n\r\n# iterate over all examples in objectDataset and classify them using moduleNetwork\r\n# append each misclassified sample to objectOutputs like in the example below\r\n# note that each entry should also have the true / target as well as the estimated label\r\n\r\nobjectOutputs = []\r\n\r\n# objectOutputs.append({\r\n# \t'tensorInput': torch.rand(28, 28),\r\n# \t'intTarget': 1,\r\n# \t'intEstimate': 2\r\n# })\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# making sure that objectOutputs has the correct size and content using asserts\r\n# afterwards combining all the samples into a single image and saving it to disk\r\n\r\nassert(len(objectOutputs) == 6)\r\n\r\nnumpyOutputs = []\r\n\r\nfor objectOutput in objectOutputs:\r\n\tassert(type(objectOutput['tensorInput']) == torch.FloatTensor)\r\n\tassert(type(objectOutput['intTarget']) == int)\r\n\tassert(type(objectOutput['intEstimate']) == int)\r\n\tassert(objectOutput['tensorInput'].size(0) == 28)\r\n\tassert(objectOutput['tensorInput'].size(1) == 28)\r\n\r\n\tnumpyOutput = (numpy.repeat(objectOutput['tensorInput'].numpy()[:, :, None], 3, 2).clip(0.0, 1.0) * 255.0).astype(numpy.uint8)\r\n\tnumpyOutput = cv2.resize(src=numpyOutput, dsize=None, fx=5.0, fy=5.0, interpolation=cv2.INTER_NEAREST)\r\n\tnumpyOutput = numpy.pad(numpyOutput, [ (0, 40), (0, 0), (0, 0) ], 'constant')\r\n\r\n\tcv2.putText(img=numpyOutput, text='truth: ' + str(objectOutput['intTarget']), org=(10, 148), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(255, 255, 255), thickness=1, lineType=cv2.LINE_AA)\r\n\tcv2.putText(img=numpyOutput, text='estimate: ' + str(objectOutput['intEstimate']), org=(10, 168), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.5, color=(255, 255, 255), thickness=1, lineType=cv2.LINE_AA)\r\n\r\n\tnumpyOutputs.append(numpyOutput)\r\n# end\r\n\r\nnumpyOutput = numpy.concatenate(numpyOutputs, 1)\r\n\r\ncv2.imwrite(filename='./14-mnist.png', img=numpyOutput)"""
15-fashion.py,0,"b""import torch\r\nprint(torch.__version__)\r\n\r\nimport torchvision\r\nprint(torchvision.__version__)\r\n\r\nimport numpy\r\nimport cv2\r\nimport matplotlib.pyplot\r\nimport tqdm\r\n\r\n# implement a neural network according to the provided specification\r\n\r\ndblTrain = []\r\ndblValidation = []\r\n\r\n# creating a data loader for the training samples of the fashion dataset\r\n# specifying the batch size and making sure it runs in a background thread\r\n\r\nobjectTrain = torch.utils.data.DataLoader(\r\n\tbatch_size=64,\r\n\tshuffle=True,\r\n\tnum_workers=1,\r\n\tpin_memory=False,\r\n\tdataset=torchvision.datasets.FashionMNIST(\r\n\t\troot='./fashion/',\r\n\t\ttrain=True,\r\n\t\tdownload=True,\r\n\t\ttransform=torchvision.transforms.Compose([\r\n\t\t\ttorchvision.transforms.ToTensor()\r\n\t\t])\r\n\t)\r\n)\r\n\r\n# creating a data loader for the validation samples of the fashion dataset\r\n\r\nobjectValidation = torch.utils.data.DataLoader(\r\n\tbatch_size=64,\r\n\tshuffle=True,\r\n\tnum_workers=1,\r\n\tpin_memory=False,\r\n\tdataset=torchvision.datasets.FashionMNIST(\r\n\t\troot='./fashion/',\r\n\t\ttrain=False,\r\n\t\tdownload=True,\r\n\t\ttransform=torchvision.transforms.Compose([\r\n\t\t\ttorchvision.transforms.ToTensor()\r\n\t\t])\r\n\t)\r\n)\r\n\r\n# visualizing some samples and their labels from the validation set\r\n# note that this will not work if you are connected to the linux lab\r\n\r\nif False:\r\n\tobjectFigure, objectAxis = matplotlib.pyplot.subplots(2, 4)\r\n\r\n\tfor objectRow in objectAxis:\r\n\t\tfor objectCol in objectRow:\r\n\t\t\ttensorInput, tensorTarget = next(iter(objectValidation))\r\n\r\n\t\t\tobjectCol.grid(False)\r\n\t\t\tobjectCol.set_title([ 't-shirt', 'trousers', 'pullover', 'dress', 'coat', 'sandals', 'shirt', 'sneaker', 'bag', 'ankle boot' ][ tensorTarget[0] ])\r\n\t\t\tobjectCol.imshow(tensorInput[0, 0, :, :].cpu().numpy().transpose(1, 2, 0), cmap='gray')\r\n\t\t# end\r\n\t# end\r\n\r\n\tmatplotlib.pyplot.show()\r\n# end\r\n\r\n# defining the network, this is what you are asked to complete\r\n# the network and its layers are summarized in the table below\r\n\r\nclass Network(torch.nn.Module):\r\n\t#######################################################################\r\n\t# OPERATION                 # INPUT              # OUTPUT             #\r\n\t#######################################################################\r\n\t# BatchNorm2d               # (-1,    1, 28, 28) # (-1,    1, 28, 28) #\r\n\t# Conv2d, kernel_size=5     # (-1,    1, 28, 28) # (-1,   64, 24, 24) #\r\n\t# relu                      # (-1,   64, 24, 24) # (-1,   64, 24, 24) #\r\n\t# max_pool2d, kernel_size=3 # (-1,   64, 24, 24) # (-1,   64,  8,  8) #\r\n\t# Conv2d, kernel_size=5     # (-1,   64,  8,  8) # (-1,  512,  4,  4) #\r\n\t# relu                      # (-1,  512,  4,  4) # (-1,  512,  4,  4) #\r\n\t# max_pool2d, kernel_size=2 # (-1,  512,  4,  4) # (-1,  512,  2,  2) #\r\n\t# view(-1, 2048)            # (-1,  512,  2,  2) # (-1, 2048        ) #\r\n\t# Linear                    # (-1, 2048        ) # (-1,  256        ) #\r\n\t# dropout, p=0.35           # (-1,  256        ) # (-1,  256        ) #\r\n\t# relu                      # (-1,  256        ) # (-1,  256        ) #\r\n\t# Linear                    # (-1,  256        ) # (-1,  128        ) #\r\n\t# dropout, p=0.35           # (-1,  128        ) # (-1,  128        ) #\r\n\t# relu                      # (-1,  128        ) # (-1,  128        ) #\r\n\t# Linear                    # (-1,  128        ) # (-1,   10        ) #\r\n\t#######################################################################\r\n\r\n\tdef __init__(self):\r\n\t\tsuper(Network, self).__init__()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\t# end\r\n\r\n\tdef forward(self, x):\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\t\treturn torch.nn.functional.log_softmax(x, dim=1)\r\n\t# end\r\n# end\r\n\r\nmoduleNetwork = Network()\r\n\r\n# specifying the optimizer based on adaptive moment estimation, adam\r\n# it will be responsible for updating the parameters of the network\r\n\r\nobjectOptimizer = torch.optim.Adam(params=moduleNetwork.parameters(), lr=0.001)\r\n\r\ndef train():\r\n\t# setting the network to the training mode, some modules behave differently during training\r\n\r\n\tmoduleNetwork.train()\r\n\ttorch.set_grad_enabled(True)\r\n\r\n\t# obtain samples and their ground truth from the training dataset, one minibatch at a time\r\n\r\n\tfor tensorInput, tensorTarget in tqdm.tqdm(objectTrain):\r\n\t\t# setting all previously computed gradients to zero, we will compute new ones\r\n\r\n\t\tobjectOptimizer.zero_grad()\r\n\r\n\t\t# performing a forward pass through the network while retaining a computational graph\r\n\r\n\t\tvariableEstimate = moduleNetwork(variableInput)\r\n\r\n\t\t# computing the loss according to the cross-entropy / negative log likelihood\r\n\t\t# the backprop is done in the subsequent step such that multiple losses can be combined\r\n\r\n\t\tvariableLoss = torch.nn.functional.nll_loss(input=variableEstimate, target=variableTarget)\r\n\r\n\t\tvariableLoss.backward()\r\n\r\n\t\t# calling the optimizer, allowing it to update the weights according to the gradients\r\n\r\n\t\tobjectOptimizer.step()\r\n\t# end\r\n# end\r\n\r\ndef evaluate():\r\n\t# setting the network to the evaluation mode, some modules behave differently during evaluation\r\n\r\n\tmoduleNetwork.eval()\r\n\ttorch.set_grad_enabled(False)\r\n\r\n\t# defining two variables that will count the number of correct classifications\r\n\r\n\tintTrain = 0\r\n\tintValidation = 0\r\n\r\n\t# iterating over the training and the validation dataset to determine the accuracy\r\n\t# this is typically done one a subset of the samples in each set, unlike here\r\n\t# otherwise the time to evaluate the model would unnecessarily take too much time\r\n\r\n\tfor tensorInput, tensorTarget in objectTrain:\r\n\t\ttensorEstimate = moduleNetwork(tensorInput)\r\n\r\n\t\tintTrain += tensorEstimate.max(dim=1, keepdim=False)[1].eq(tensorTarget).sum()\r\n\t# end\r\n\r\n\tfor tensorInput, tensorTarget in objectValidation:\r\n\t\ttensorEstimate = moduleNetwork(tensorInput)\r\n\r\n\t\tintValidation += tensorEstimate.max(dim=1, keepdim=False)[1].eq(tensorTarget).sum()\r\n\t# end\r\n\r\n\t# determining the accuracy based on the number of correct predictions and the size of the dataset\r\n\r\n\tdblTrain.append(100.0 * intTrain / len(objectTrain.dataset))\r\n\tdblValidation.append(100.0 * intValidation / len(objectValidation.dataset))\r\n\r\n\tprint('')\r\n\tprint('train: ' + str(intTrain) + '/' + str(len(objectTrain.dataset)) + ' (' + str(dblTrain[-1]) + '%)')\r\n\tprint('validation: ' + str(intValidation) + '/' + str(len(objectValidation.dataset)) + ' (' + str(dblValidation[-1]) + '%)')\r\n\tprint('')\r\n# end\r\n\r\n# training the model for 100 epochs, one would typically save / checkpoint the model after each one\r\n\r\nfor intEpoch in range(100):\r\n\ttrain()\r\n\tevaluate()\r\n# end\r\n\r\n# plotting the learning curve according to the accuracies determined in the evaluation function\r\n# note that this will not work if you are connected to the linux lab via ssh but no x forwarding\r\n\r\nif False:\r\n\tmatplotlib.pyplot.figure(figsize=(8.0, 5.0), dpi=150.0)\r\n\tmatplotlib.pyplot.ylim(79.5, 100.5)\r\n\tmatplotlib.pyplot.plot(dblTrain)\r\n\tmatplotlib.pyplot.plot(dblValidation)\r\n\tmatplotlib.pyplot.show()\r\n# end"""
16-pca.py,0,"b'import torch\r\nprint(torch.__version__)\r\n\r\nimport torchvision\r\nprint(torchvision.__version__)\r\n\r\nimport numpy\r\nimport scipy.linalg\r\nimport scipy.spatial\r\n\r\n# use principal component analysis for classification as discussed in class\r\n\r\n# creating a data loader for the training samples of the mnist dataset\r\n# specifying the batch size as well as the normalization transform\r\n# notice that the batch size is huge, we misuse torchvision a little bit\r\n\r\nobjectTrain = torch.utils.data.DataLoader(\r\n\tbatch_size=60000,\r\n\tshuffle=False,\r\n\tnum_workers=1,\r\n\tpin_memory=False,\r\n\tdataset=torchvision.datasets.MNIST(\r\n\t\troot=\'./mnist/\',\r\n\t\ttrain=True,\r\n\t\tdownload=True,\r\n\t\ttransform=torchvision.transforms.Compose([\r\n\t\t\ttorchvision.transforms.ToTensor(),\r\n\t\t\ttorchvision.transforms.Normalize(tuple([ 0.1307 ]), tuple([ 0.3081 ]))\r\n\t\t])\r\n\t)\r\n)\r\n\r\n# creating a data loader for the validation samples of the mnist dataset\r\n\r\nobjectValidation = torch.utils.data.DataLoader(\r\n\tbatch_size=100,\r\n\tshuffle=False,\r\n\tnum_workers=1,\r\n\tpin_memory=False,\r\n\tdataset=torchvision.datasets.MNIST(\r\n\t\troot=\'./mnist/\',\r\n\t\ttrain=False,\r\n\t\tdownload=True,\r\n\t\ttransform=torchvision.transforms.Compose([\r\n\t\t\ttorchvision.transforms.ToTensor(),\r\n\t\t\ttorchvision.transforms.Normalize(tuple([ 0.1307 ]), tuple([ 0.3081 ]))\r\n\t\t])\r\n\t)\r\n)\r\n\r\n# obtaining the mnist data using the data loader and reshaping it\r\n# principal component analysis expects one-dimensional features\r\n\r\ntensorInputTrain, tensorTargetTrain = next(iter(objectTrain))\r\ntensorInputValidation, tensorTargetValidation = next(iter(objectValidation))\r\n\r\nnumpyInputTrain = tensorInputTrain.view(-1, 784).numpy()\r\nnumpyInputValidation = tensorInputValidation.view(-1, 784).numpy()\r\n\r\n# calculating the covariance matrix of the zero-mean data points\r\n# calculating the eigenvectors and the corresponding eigenvalues\r\n# sorting the eigenvectors according to their eigenvalues in descending order\r\n\r\nnumpyCov = numpy.cov(numpyInputTrain - numpyInputTrain.mean(0), None, False, False)\r\nnumpyEvals, numpyEvecs = scipy.linalg.eigh(numpyCov)\r\nnumpyDescending = numpy.argsort(numpyEvals)[::-1]\r\nnumpyEvecs = numpyEvecs[:, numpyDescending]\r\n\r\n# evaluating the effect of the number of utilized principal components\r\n\r\nfor intK in [ 7, 14, 28 ]:\r\n\t# transforming the training data points into intK dimensions\r\n\r\n\tnumpyOutputTrain = numpy.dot(numpyInputTrain, numpyEvecs[:, :intK])\r\n\r\n\t# storing the transformed data points in a k-d tree for efficiency\r\n\r\n\tobjectDatabase = scipy.spatial.KDTree(numpyOutputTrain)\r\n\r\n\t# transform the validation data points into intK dimensions\r\n\t# find the nearest neighbor for each sample in the validation set\r\n\t# use the k-d tree for this search, it will speed it up a bit\r\n\t# classify the sample based on the class of the nearest neighbor\r\n\t# count the correct classifications to determine the accuracy\r\n\r\n\tdblAccuracy = 0.0\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\t# printing the number of used features as well as the accuracy\r\n\t# the solution does print ""7: 0.84"", ""14: 0.96"", and ""28: 0.99""\r\n\r\n\tprint(str(intK) + \': \' + str(dblAccuracy))\r\n# end'"
17-autoencoder.py,0,"b""import torch\r\nprint(torch.__version__)\r\n\r\nimport torchvision\r\nprint(torchvision.__version__)\r\n\r\nimport numpy\r\nimport cv2\r\n\r\n# generate new images by interpolating between two latent representations\r\n\r\n# defining the network, a autoencoder that is similar to the one from the slides\r\n\r\nclass Network(torch.nn.Module):\r\n\tdef __init__(self):\r\n\t\tsuper(Network, self).__init__()\r\n\r\n\t\tself.moduleEncoder = torch.nn.Sequential(\r\n\t\t\ttorch.nn.Conv2d(1, 32, kernel_size=5),\r\n\t\t\ttorch.nn.ReLU(),\r\n\t\t\ttorch.nn.Conv2d(32, 64, kernel_size=5),\r\n\t\t\ttorch.nn.ReLU(),\r\n\t\t\ttorch.nn.Conv2d(64, 128, kernel_size=4, stride=2),\r\n\t\t\ttorch.nn.ReLU(),\r\n\t\t\ttorch.nn.Conv2d(128, 256, kernel_size=3, stride=2),\r\n\t\t\ttorch.nn.ReLU(),\r\n\t\t\ttorch.nn.Conv2d(256, 64, kernel_size=4)\r\n\t\t)\r\n\r\n\t\tself.moduleDecoder = torch.nn.Sequential(\r\n\t\t\ttorch.nn.ConvTranspose2d(64, 256, kernel_size=4),\r\n\t\t\ttorch.nn.ReLU(),\r\n\t\t\ttorch.nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2),\r\n\t\t\ttorch.nn.ReLU(),\r\n\t\t\ttorch.nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2),\r\n\t\t\ttorch.nn.ReLU(),\r\n\t\t\ttorch.nn.ConvTranspose2d(64, 32, kernel_size=5),\r\n\t\t\ttorch.nn.ReLU(),\r\n\t\t\ttorch.nn.ConvTranspose2d(32, 1, kernel_size=5)\r\n\t\t)\r\n\t# end\r\n\r\n\tdef forward(self, x):\r\n\t\tx = self.moduleEncoder(x)\r\n\t\tx = self.moduleDecoder(x)\r\n\r\n\t\treturn x\r\n\t# end\r\n# end\r\n\r\nmoduleNetwork = Network()\r\n\r\n# loading the provided weights, this exercise is not about training the network\r\n\r\nmoduleNetwork.load_state_dict(torch.load('./17-autoencoder.pytorch'))\r\n\r\n# setting the network to the evaluation mode, this makes no difference here though\r\n\r\nmoduleNetwork.eval()\r\n\r\n# loading two samples and converting them to tensors, each of size 1x1x28x28\r\n\r\ntensorFirst = torch.FloatTensor(cv2.imread(filename='./samples/fashion-1.png', flags=cv2.IMREAD_GRAYSCALE).astype(numpy.float32) / 255.0).unsqueeze(0).unsqueeze(0)\r\ntensorSecond = torch.FloatTensor(cv2.imread(filename='./samples/fashion-2.png', flags=cv2.IMREAD_GRAYSCALE).astype(numpy.float32) / 255.0).unsqueeze(0).unsqueeze(0)\r\n\r\n# encode the two samples to retrieve their representation in the latent space\r\n# generate new samples by interpolating between the two latent representations\r\n# use the formula from the slides with alpha = [ x * 0.1 for x in range(11) ]\r\n# append each interpolated result as a tensor of size 28x28 to tensorOutputs\r\n\r\ntensorOutputs = []\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# making sure that tensorOutputs has the correct size and content using asserts\r\n# afterwards combining all the samples into a single image and saving it to disk\r\n\r\nassert(len(tensorOutputs) == 11)\r\n\r\nfor tensorOutput in tensorOutputs:\r\n\tassert(type(tensorOutput) == torch.FloatTensor)\r\n\tassert(tensorOutput.size(0) == 28)\r\n\tassert(tensorOutput.size(1) == 28)\r\n# end\r\n\r\ntensorOutputs = [ tensorFirst[0, 0] ] + tensorOutputs + [ tensorSecond[0, 0] ]\r\n\r\nnumpyOutput = (numpy.concatenate([ tensorOutput.numpy() for tensorOutput in tensorOutputs ], 1).clip(0.0, 1.0) * 255.0).astype(numpy.uint8)\r\nnumpyOutput = cv2.resize(src=numpyOutput, dsize=None, fx=2.0, fy=2.0, interpolation=cv2.INTER_NEAREST)\r\n\r\ncv2.imwrite(filename='./17-autoencoder.png', img=numpyOutput)"""
