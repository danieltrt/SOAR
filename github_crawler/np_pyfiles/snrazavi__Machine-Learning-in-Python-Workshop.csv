file_path,api_count,code
MLiP-week06-07/data_utils.py,4,"b'from six.moves import cPickle as pickle\nimport numpy as np\nimport os\nfrom scipy.misc import imread\nimport platform\n\ndef load_pickle(f):\n    version = platform.python_version_tuple()\n    if version[0] == \'2\':\n        return  pickle.load(f)\n    elif version[0] == \'3\':\n        return  pickle.load(f, encoding=\'latin1\')\n    raise ValueError(""invalid python version: {}"".format(version))\n\ndef load_CIFAR_batch(filename):\n  """""" load single batch of cifar """"""\n  with open(filename, \'rb\') as f:\n    datadict = load_pickle(f)\n    X = datadict[\'data\']\n    Y = datadict[\'labels\']\n    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(""float"")\n    Y = np.array(Y)\n    return X, Y\n\ndef load_CIFAR10(ROOT):\n  """""" load all of cifar """"""\n  xs = []\n  ys = []\n  for b in range(1,6):\n    f = os.path.join(ROOT, \'data_batch_%d\' % (b, ))\n    X, Y = load_CIFAR_batch(f)\n    xs.append(X)\n    ys.append(Y)    \n  Xtr = np.concatenate(xs)\n  Ytr = np.concatenate(ys)\n  del X, Y\n  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \'test_batch\'))\n  return Xtr, Ytr, Xte, Yte\n\n\ndef get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n                     subtract_mean=True):\n    """"""\n    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n    it for classifiers. These are the same steps as we used for the SVM, but\n    condensed to a single function.\n    """"""\n    # Load the raw CIFAR-10 data\n    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n        \n    # Subsample the data\n    mask = list(range(num_training, num_training + num_validation))\n    X_val = X_train[mask]\n    y_val = y_train[mask]\n    mask = list(range(num_training))\n    X_train = X_train[mask]\n    y_train = y_train[mask]\n    mask = list(range(num_test))\n    X_test = X_test[mask]\n    y_test = y_test[mask]\n\n    # Normalize the data: subtract the mean image\n    if subtract_mean:\n      mean_image = np.mean(X_train, axis=0)\n      X_train -= mean_image\n      X_val -= mean_image\n      X_test -= mean_image\n    \n    # Transpose so that channels come first\n    X_train = X_train.transpose(0, 3, 1, 2).copy()\n    X_val = X_val.transpose(0, 3, 1, 2).copy()\n    X_test = X_test.transpose(0, 3, 1, 2).copy()\n\n    # Package data into a dictionary\n    return {\n      \'X_train\': X_train, \'y_train\': y_train,\n      \'X_val\': X_val, \'y_val\': y_val,\n      \'X_test\': X_test, \'y_test\': y_test,\n    }\n'"
MLiP-week06-07/layers.py,23,"b'import numpy as np\r\n\r\n\r\ndef affine_forward(x, W, b):\r\n    """"""\r\n    A linear mapping from inputs to scores.\r\n    \r\n    Inputs:\r\n        - x: input matrix (N, d_1, ..., d_k)\r\n        - W: weigh matrix (D, C)\r\n        - b: bias vector  (C, )\r\n    \r\n    Outputs:\r\n        - out: output of linear layer (N, C)\r\n    """"""\r\n    x2d = np.reshape(x, (x.shape[0], -1))  # convert 4D input matrix to 2D    \r\n    out = np.dot(x2d, W) + b               # linear transformation\r\n    cache = (x, W, b)                      # keep for backward step (stay with us)\r\n    return out, cache\r\n\r\n\r\ndef affine_backward(dout, cache):\r\n    """"""\r\n    Computes the backward pass for an affine layer.\r\n\r\n    Inputs:\r\n        - dout: Upstream derivative, of shape (N, C)\r\n        - cache: Tuple of:\r\n            - x: Input data, of shape (N, d_1, ... d_k)\r\n            - w: Weights, of shape (D, C)\r\n            - b: biases, of shape (C,)\r\n\r\n    Outputs:\r\n        - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\r\n        - dw: Gradient with respect to w, of shape (D, C)\r\n        - db: Gradient with respect to b, of shape (C,)\r\n    """"""\r\n    x, w, b = cache\r\n    x2d = np.reshape(x, (x.shape[0], -1))\r\n\r\n    # compute gradients\r\n    db = np.sum(dout, axis=0)\r\n    dw = np.dot(x2d.T, dout)\r\n    dx = np.dot(dout, w.T)\r\n\r\n    # reshape dx to match the size of x\r\n    dx = dx.reshape(x.shape)\r\n    \r\n    return dx, dw, db\r\n\r\ndef svm_loss_naive(scores, y, W, reg=1e-3):\r\n    """"""\r\n    Naive implementation of SVM loss function.\r\n\r\n    Inputs:\r\n        - scores: scores for all training data (N, C)\r\n        - y: correct labels for the training data\r\n        - reg: regularization strength (lambd)\r\n\r\n    Outputs:\r\n       - loss: data loss plus L2 regularization loss\r\n       - grads: graidents of loss wrt scores\r\n    """"""\r\n\r\n    N, C = scores.shape\r\n\r\n    # Compute svm data loss\r\n    loss = 0.0\r\n    for i in range(N):\r\n        s = scores[i]  # scores for the ith data\r\n        correct_class = y[i]  # correct class score\r\n\r\n        for j in range(C):\r\n            if j == y[i]:\r\n                continue\r\n            else:\r\n                # loss += max(0, s[j] - s[correct_class] + 1.0)\r\n                margin = s[j] - s[correct_class] + 1.0\r\n                if margin > 0:\r\n                    loss += margin\r\n    loss /= N\r\n\r\n    # Adding L2-regularization loss\r\n    loss += 0.5 * reg * np.sum(W * W)\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    # We will write this part later\r\n    grads = {} \r\n\r\n    return loss, grads\r\n\r\ndef svm_loss_half_vectorized(scores, y, W, reg=1e-3):\r\n    """"""\r\n    Half-vectorized implementation of SVM loss function.\r\n\r\n    Inputs:\r\n        - scores: scores for all training data (N, C)\r\n        - y: correct labels for the training data\r\n        - reg: regularization strength (lambd)\r\n\r\n    Outputs:\r\n       - loss: data loss plus L2 regularization loss\r\n       - grads: graidents of loss wrt scores\r\n    """"""\r\n\r\n    N, C = scores.shape\r\n\r\n    # Compute svm data loss\r\n    loss = 0.0\r\n    for i in range(N):\r\n        s = scores[i]  # scores for the ith data\r\n        correct_class = y[i]  # correct class score\r\n\r\n        margins = np.maximum(0.0, s - s[correct_class] + 1.0)\r\n        margins[correct_class] = 0.0\r\n        loss += np.sum(margins)\r\n\r\n    loss /= N\r\n\r\n    # Adding L2-regularization loss\r\n    loss += 0.5 * reg * np.sum(W * W)\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    # We will write this part later\r\n    grads = {} \r\n\r\n    return loss, grads\r\n\r\n\r\ndef svm_loss(scores, y, W, reg=1e-3):\r\n    """"""\r\n    Fully-vectorized implementation of SVM loss function.\r\n\r\n    Inputs:\r\n        - scores: scores for all training data (N, C)\r\n        - y: correct labels for the training data\r\n        - reg: regularization strength (lambd)\r\n\r\n    Outputs:\r\n       - loss: data loss plus L2 regularization loss\r\n       - grads: graidents of loss wrt scores\r\n    """"""\r\n\r\n    N = scores.shape[0]\r\n\r\n    # Compute svm data loss\r\n    correct_class_scores = scores[range(N), y]\r\n    margins = np.maximum(0.0, scores - correct_class_scores[:, None] + 1.0)\r\n    margins[range(N), y] = 0.0\r\n    loss = np.sum(margins) / N\r\n\r\n    # Adding L2-regularization loss\r\n    loss += 0.5 * reg * np.sum(W * W)\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    # We will write this part later\r\n    grads = {} \r\n\r\n    return loss, grads\r\n\r\n\r\ndef softmax_loss_naive(scores, y, W, reg=1e-3):\r\n    """"""\r\n    Softmax loss function, naive implementation (with loops)\r\n\r\n    Inputs have dimension D, there are C classes, and we operate on minibatches\r\n    of N examples.\r\n\r\n    Inputs:\r\n        - scores: A numpy array of shape (N, C).\r\n        - y: A numpy array of shape (N,) containing training labels;\r\n        - W: A numpy array of shape (D, C) containing weights.\r\n        - reg: (float) regularization strength\r\n\r\n    Outputs:\r\n        - loss as single float\r\n        - gradient with respect to weights W; an array of same shape as W\r\n    """"""\r\n    N, C = scores.shape\r\n\r\n    # compute data loss\r\n    loss = 0.0\r\n    for i in range(N):\r\n        correct_class = y[i]\r\n        score = scores[i]\r\n        score -= np.max(scores)\r\n        exp_score = np.exp(score)\r\n        probs = exp_score / np.sum(exp_score)\r\n        loss += -np.log(probs[correct_class])\r\n\r\n    loss /= N\r\n\r\n    # compute regularization loss\r\n    loss += 0.5 * reg * np.sum(W * W)\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    # We will write this part later\r\n    grads = {}  \r\n\r\n    return loss, grads\r\n\r\n\r\ndef softmax_loss(scores, y, W, reg=1e-3):\r\n    """"""\r\n    Softmax loss function, naive implementation (with loops)\r\n\r\n    Inputs have dimension D, there are C classes, and we operate on minibatches\r\n    of N examples.\r\n\r\n    Inputs:\r\n        - scores: A numpy array of shape (N, C).\r\n        - y: A numpy array of shape (N,) containing training labels;\r\n        - W: A numpy array of shape (D, C) containing weights.\r\n        - reg: (float) regularization strength\r\n\r\n    Outputs:\r\n        - loss as single float\r\n        - gradient with respect to weights W; an array of same shape as W\r\n    """"""\r\n    N = scores.shape[0]  # number of input data\r\n\r\n    # compute data loss\r\n    scores -= np.max(scores, axis=1, keepdims=True)\r\n    exp_scores = np.exp(scores)\r\n    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\r\n    loss = -np.sum(np.log(probs[range(N), y])) / N\r\n\r\n    # compute regularization loss\r\n    loss += 0.5 * reg * np.sum(W * W)\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    # We will write this part later\r\n    grads = {}  \r\n\r\n    return loss, grads        \r\n'"
MLiP-week08/data_utils.py,4,"b'from six.moves import cPickle as pickle\nimport numpy as np\nimport os\nfrom scipy.misc import imread\nimport platform\n\ndef load_pickle(f):\n    version = platform.python_version_tuple()\n    if version[0] == \'2\':\n        return  pickle.load(f)\n    elif version[0] == \'3\':\n        return  pickle.load(f, encoding=\'latin1\')\n    raise ValueError(""invalid python version: {}"".format(version))\n\ndef load_CIFAR_batch(filename):\n  """""" load single batch of cifar """"""\n  with open(filename, \'rb\') as f:\n    datadict = load_pickle(f)\n    X = datadict[\'data\']\n    Y = datadict[\'labels\']\n    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(""float"")\n    Y = np.array(Y)\n    return X, Y\n\ndef load_CIFAR10(ROOT):\n  """""" load all of cifar """"""\n  xs = []\n  ys = []\n  for b in range(1,6):\n    f = os.path.join(ROOT, \'data_batch_%d\' % (b, ))\n    X, Y = load_CIFAR_batch(f)\n    xs.append(X)\n    ys.append(Y)    \n  Xtr = np.concatenate(xs)\n  Ytr = np.concatenate(ys)\n  del X, Y\n  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \'test_batch\'))\n  return Xtr, Ytr, Xte, Yte\n\n\ndef get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n                     subtract_mean=True):\n    """"""\n    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n    it for classifiers. These are the same steps as we used for the SVM, but\n    condensed to a single function.\n    """"""\n    # Load the raw CIFAR-10 data\n    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n        \n    # Subsample the data\n    mask = list(range(num_training, num_training + num_validation))\n    X_val = X_train[mask]\n    y_val = y_train[mask]\n    mask = list(range(num_training))\n    X_train = X_train[mask]\n    y_train = y_train[mask]\n    mask = list(range(num_test))\n    X_test = X_test[mask]\n    y_test = y_test[mask]\n\n    # Normalize the data: subtract the mean image\n    if subtract_mean:\n      mean_image = np.mean(X_train, axis=0)\n      X_train -= mean_image\n      X_val -= mean_image\n      X_test -= mean_image\n    \n    # Transpose so that channels come first\n    X_train = X_train.transpose(0, 3, 1, 2).copy()\n    X_val = X_val.transpose(0, 3, 1, 2).copy()\n    X_test = X_test.transpose(0, 3, 1, 2).copy()\n\n    # Package data into a dictionary\n    return {\n      \'X_train\': X_train, \'y_train\': y_train,\n      \'X_val\': X_val, \'y_val\': y_val,\n      \'X_test\': X_test, \'y_test\': y_test,\n    }\n'"
MLiP-week08/layers.py,16,"b'import numpy as np\r\n\r\n\r\ndef affine_forward(x, W, b):\r\n    """"""\r\n    A linear mapping from inputs to scores.\r\n    \r\n    Inputs:\r\n        - x: input matrix (N, d_1, ..., d_k)\r\n        - W: weigh matrix (D, C)\r\n        - b: bias vector  (C, )\r\n    \r\n    Outputs:\r\n        - out: output of linear layer (N, C)\r\n    """"""\r\n    x2d = np.reshape(x, (x.shape[0], -1))  # convert 4D input matrix to 2D    \r\n    out = np.dot(x2d, W) + b               # linear transformation\r\n    cache = (x, W, b)                      # keep for backward step (stay with us)\r\n    return out, cache\r\n\r\n\r\ndef affine_backward(dout, cache):\r\n    """"""\r\n    Computes the backward pass for an affine layer.\r\n\r\n    Inputs:\r\n        - dout: Upstream derivative, of shape (N, C)\r\n        - cache: Tuple of:\r\n            - x: Input data, of shape (N, d_1, ... d_k)\r\n            - w: Weights, of shape (D, C)\r\n            - b: biases, of shape (C,)\r\n\r\n    Outputs:\r\n        - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\r\n        - dw: Gradient with respect to w, of shape (D, C)\r\n        - db: Gradient with respect to b, of shape (C,)\r\n    """"""\r\n    x, w, b = cache\r\n    x2d = np.reshape(x, (x.shape[0], -1))\r\n\r\n    # compute gradients\r\n    db = np.sum(dout, axis=0)\r\n    dw = np.dot(x2d.T, dout)\r\n    dx = np.dot(dout, w.T)\r\n\r\n    # reshape dx to match the size of x\r\n    dx = dx.reshape(x.shape)\r\n    \r\n    return dx, dw, db\r\n\r\ndef relu_forward(x):\r\n    """"""Forward pass for a layer of rectified linear units.\r\n\r\n    Inputs:\r\n        - x: a numpy array of any shape\r\n\r\n    Outputs:\r\n        - out: output of relu, same shape as x\r\n        - cache: x\r\n    """"""\r\n    cache = x\r\n    out = np.maximum(0, x)\r\n    return out, cache\r\n\r\ndef relu_backward(dout, cache):\r\n    """"""Backward pass for a layer of rectified linear units.\r\n\r\n    Inputs:\r\n        - dout: upstream derevatives, of any shape\r\n        - cache: x, same shape as dout\r\n\r\n    Outputs:\r\n        - dx: gradient of loss w.r.t x\r\n    """"""\r\n    x = cache\r\n    dx = dout * (x > 0)\r\n    return dx\r\n\r\ndef svm_loss(scores, y):\r\n    """"""\r\n    Fully-vectorized implementation of SVM loss function.\r\n\r\n    Inputs:\r\n        - scores: scores for all training data (N, C)\r\n        - y: correct labels for the training data of shape (N,)\r\n\r\n    Outputs:\r\n       - loss: data loss plus L2 regularization loss\r\n       - grads: graidents of loss w.r.t scores\r\n    """"""\r\n\r\n    N = scores.shape[0]\r\n\r\n    # Compute svm data loss\r\n    correct_class_scores = scores[range(N), y]\r\n    margins = np.maximum(0.0, scores - correct_class_scores[:, None] + 1.0)\r\n    margins[range(N), y] = 0.0\r\n    loss = np.sum(margins) / N\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    num_pos = np.sum(margins > 0, axis=1)\r\n    dscores = np.zeros(scores.shape)\r\n    dscores[margins > 0] = 1\r\n    dscores[range(N), y] -= num_pos\r\n    dscores /= N\r\n\r\n    return loss, dscores\r\n\r\n\r\ndef softmax_loss(scores, y):\r\n    """"""\r\n    Softmax loss function, fully vectorized implementation.\r\n\r\n    Inputs have dimension D, there are C classes, and we operate on minibatches\r\n    of N examples.\r\n\r\n    Inputs:\r\n        - scores: A numpy array of shape (N, C).\r\n        - y: A numpy array of shape (N,) containing training labels;\r\n\r\n    Outputs:\r\n        - loss as single float\r\n        - gradient with respect to scores\r\n    """"""\r\n    N = scores.shape[0]  # number of input data\r\n\r\n    # compute data loss\r\n    shifted_logits = scores - np.max(scores, axis=1, keepdims=True)\r\n    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\r\n    log_probs = shifted_logits - np.log(Z)\r\n    probs = np.exp(log_probs)\r\n    loss = -np.sum(log_probs[range(N), y]) / N\r\n\r\n    # Compute gradient of loss function w.r.t. scores\r\n    dscores = probs.copy()\r\n    dscores[range(N), y] -= 1\r\n    dscores /= N\r\n    \r\n    return loss, dscores        \r\n'"
MLiP-week09/data_utils.py,4,"b'from six.moves import cPickle as pickle\nimport numpy as np\nimport os\nfrom scipy.misc import imread\nimport platform\n\ndef load_pickle(f):\n    version = platform.python_version_tuple()\n    if version[0] == \'2\':\n        return  pickle.load(f)\n    elif version[0] == \'3\':\n        return  pickle.load(f, encoding=\'latin1\')\n    raise ValueError(""invalid python version: {}"".format(version))\n\ndef load_CIFAR_batch(filename):\n  """""" load single batch of cifar """"""\n  with open(filename, \'rb\') as f:\n    datadict = load_pickle(f)\n    X = datadict[\'data\']\n    Y = datadict[\'labels\']\n    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(""float"")\n    Y = np.array(Y)\n    return X, Y\n\ndef load_CIFAR10(ROOT):\n  """""" load all of cifar """"""\n  xs = []\n  ys = []\n  for b in range(1,6):\n    f = os.path.join(ROOT, \'data_batch_%d\' % (b, ))\n    X, Y = load_CIFAR_batch(f)\n    xs.append(X)\n    ys.append(Y)    \n  Xtr = np.concatenate(xs)\n  Ytr = np.concatenate(ys)\n  del X, Y\n  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \'test_batch\'))\n  return Xtr, Ytr, Xte, Yte\n\n\ndef get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n                     subtract_mean=True):\n    """"""\n    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n    it for classifiers. These are the same steps as we used for the SVM, but\n    condensed to a single function.\n    """"""\n    # Load the raw CIFAR-10 data\n    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n        \n    # Subsample the data\n    mask = list(range(num_training, num_training + num_validation))\n    X_val = X_train[mask]\n    y_val = y_train[mask]\n    mask = list(range(num_training))\n    X_train = X_train[mask]\n    y_train = y_train[mask]\n    mask = list(range(num_test))\n    X_test = X_test[mask]\n    y_test = y_test[mask]\n\n    # Normalize the data: subtract the mean image\n    if subtract_mean:\n      mean_image = np.mean(X_train, axis=0)\n      X_train -= mean_image\n      X_val -= mean_image\n      X_test -= mean_image\n    \n    # Transpose so that channels come first\n    X_train = X_train.transpose(0, 3, 1, 2).copy()\n    X_val = X_val.transpose(0, 3, 1, 2).copy()\n    X_test = X_test.transpose(0, 3, 1, 2).copy()\n\n    # Package data into a dictionary\n    return {\n      \'X_train\': X_train, \'y_train\': y_train,\n      \'X_val\': X_val,   \'y_val\': y_val,\n      \'X_test\': X_test,  \'y_test\': y_test,\n    }\n'"
MLiP-week09/fc_net.py,6,"b'import numpy as np\r\nfrom layers import *\r\n\r\n\r\nclass FullyConnectedNet(object):\r\n    """"""\r\n    A fully-connected neural network with an arbitrary number of hidden layers,\r\n    ReLU nonlinearities, and a softmax loss function. This will also implement\r\n    dropout and batch normalization as options. For a network with L layers,\r\n    the architecture will be\r\n\r\n    {affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax\r\n\r\n    where batch normalization and dropout are optional, and the {...} block is\r\n    repeated L - 1 times.\r\n\r\n    Similar to the TwoLayerNet above, learn-able parameters are stored in the\r\n    self.params dictionary and will be learned using the Solver class.\r\n    """"""\r\n\r\n    def __init__(self, hidden_dims, input_dim=3*32*32, num_classes=10,\r\n                 dropout=0, use_batchnorm=False, reg=0.0,\r\n                 weight_scale=1e-2, dtype=np.float32, seed=None):\r\n        """"""\r\n        Initialize a new FullyConnectedNet.\r\n\r\n        Inputs:\r\n        - hidden_dims: A list of integers giving the size of each hidden layer.\r\n        - input_dim: An integer giving the size of the input.\r\n        - num_classes: An integer giving the number of classes to classify.\r\n        - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then\r\n          the network should not use dropout at all.\r\n        - use_batchnorm: Whether or not the network should use batch normalization.\r\n        - reg: Scalar giving L2 regularization strength.\r\n        - weight_scale: Scalar giving the standard deviation for random\r\n          initialization of the weights.\r\n        - dtype: A numpy data-type object; all computations will be performed using\r\n          this data-type. float32 is faster but less accurate, so you should use\r\n          float64 for numeric gradient checking.\r\n        - seed: If not None, then pass this random seed to the dropout layers. This\r\n          will make the dropout layers deterministic so we can gradient check the\r\n          model.\r\n        """"""\r\n        self.use_batchnorm = use_batchnorm\r\n        self.use_dropout = dropout > 0\r\n        self.reg = reg\r\n        self.num_layers = 1 + len(hidden_dims)\r\n        self.dtype = dtype\r\n        self.params = {}\r\n\r\n        dims = [input_dim] + hidden_dims + [num_classes]\r\n        for i in range(1, self.num_layers + 1):\r\n            self.params[\'W%d\' %i] = weight_scale * np.random.randn(dims[i - 1], dims[i])\r\n            self.params[\'b%d\' %i] = np.zeros(dims[i])\r\n            if i < self.num_layers and self.use_batchnorm:\r\n                self.params[\'gamma%d\' %i] = np.ones(dims[i])\r\n                self.params[\'beta%d\' %i] = np.zeros(dims[i])\r\n\r\n        # When using dropout we need to pass a dropout_param dictionary to each\r\n        # dropout layer so that the layer knows the dropout probability and the mode\r\n        # (train / test). You can pass the same dropout_param to each dropout layer.\r\n        self.dropout_param = {}\r\n        if self.use_dropout:\r\n            self.dropout_param = {\'mode\': \'train\', \'p\': dropout}\r\n            if seed is not None:\r\n                self.dropout_param[\'seed\'] = seed\r\n\r\n        # With batch normalization we need to keep track of running means and\r\n        # variances, so we need to pass a special bn_param object to each batch\r\n        # normalization layer. You should pass self.bn_params[0] to the forward pass\r\n        # of the first batch normalization layer, self.bn_params[1] to the forward\r\n        # pass of the second batch normalization layer, etc.\r\n        self.bn_params = []\r\n        if self.use_batchnorm:\r\n            self.bn_params = [{\'mode\': \'train\'} for i in range(self.num_layers - 1)]\r\n\r\n        # Cast all parameters to the correct data-type\r\n        for k, v in self.params.items():\r\n            self.params[k] = v.astype(dtype)\r\n\r\n    def loss(self, X, y=None):\r\n        """"""\r\n        Compute loss and gradient for the fully-connected net.\r\n\r\n        Input / output: Same as TwoLayerNet above.\r\n        """"""\r\n        X = X.astype(self.dtype)\r\n        mode = \'test\' if y is None else \'train\'\r\n\r\n        # Set train/test mode for batchnorm params and dropout param since they\r\n        # behave differently during training and testing.\r\n        if self.use_dropout:\r\n            self.dropout_param[\'mode\'] = mode\r\n        if self.use_batchnorm:\r\n            for bn_param in self.bn_params:\r\n                bn_param[\'mode\'] = mode\r\n\r\n        scores = None\r\n\r\n        cache = {}\r\n        a_cache, relu_cache, bn_cache, d_cache = {}, {}, {}, {}\r\n        h = X\r\n        for i in range(1, self.num_layers + 1):\r\n            W, b = self.params[\'W%d\' % i], self.params[\'b%d\' % i]\r\n            if i < self.num_layers:\r\n                if self.use_batchnorm:\r\n                    gamma, beta = self.params[\'gamma%d\' % i], self.params[\'beta%d\' % i]\r\n                    h, a_cache[i] = affine_forward(h, W, b)\r\n                    h, bn_cache[i] = batchnorm_forward(h, gamma, beta, self.bn_params[i - 1])\r\n                    h, relu_cache[i] = relu_forward(h)\r\n                else:\r\n                    h, cache[i] = affine_relu_forward(h, W, b)\r\n                if self.use_dropout:\r\n                    h, d_cache[i] = dropout_forward(h, self.dropout_param)\r\n            else:\r\n                scores, cache[i] = affine_forward(h, W, b)\r\n\r\n        # If test mode return early\r\n        if mode == \'test\':\r\n            return scores\r\n\r\n        loss, grads = 0.0, {}\r\n\r\n        loss, dscores = softmax_loss(scores, y)\r\n\r\n        # backward pass\r\n        dout = dscores\r\n        for i in reversed(range(1, self.num_layers + 1)):\r\n            if i < self.num_layers:\r\n                if self.use_dropout:\r\n                    dout = dropout_backward(dout, d_cache[i])\r\n                if self.use_batchnorm:\r\n                    dout = relu_backward(dout, relu_cache[i])\r\n                    dout, grads[\'gamma%d\' % i], grads[\'beta%d\' % i] = batchnorm_backward(dout, bn_cache[i])\r\n                    dout, grads[\'W%d\' % i], grads[\'b%d\' % i] = affine_backward(dout, a_cache[i])\r\n                else:\r\n                    dout, grads[\'W%d\' % i], grads[\'b%d\' % i] = affine_relu_backward(dout, cache[i])\r\n            else:\r\n                dout, grads[\'W%d\' % i], grads[\'b%d\' %i] = affine_backward(dout, cache[i])\r\n\r\n        for i in range(1, self.num_layers):\r\n            W = self.params[\'W%d\' % i]\r\n            loss += 0.5 * self.reg * np.sum(W * W)\r\n            grads[\'W%d\' % i] += self.reg * W\r\n\r\n        return loss, grads\r\n'"
MLiP-week09/layers.py,27,"b'import numpy as np\r\n\r\n\r\ndef affine_forward(x, W, b):\r\n    """"""\r\n    A linear mapping from inputs to scores.\r\n    \r\n    Inputs:\r\n        - x: input matrix (N, d_1, ..., d_k)\r\n        - W: weigh matrix (D, C)\r\n        - b: bias vector  (C, )\r\n    \r\n    Outputs:\r\n        - out: output of linear layer (N, C)\r\n    """"""\r\n    x2d = np.reshape(x, (x.shape[0], -1))  # convert 4D input matrix to 2D    \r\n    out = np.dot(x2d, W) + b               # linear transformation\r\n    cache = (x, W, b)                      # keep for backward step (stay with us)\r\n    return out, cache\r\n\r\n\r\ndef affine_backward(dout, cache):\r\n    """"""\r\n    Computes the backward pass for an affine layer.\r\n\r\n    Inputs:\r\n        - dout: Upstream derivative, of shape (N, C)\r\n        - cache: Tuple of:\r\n            - x: Input data, of shape (N, d_1, ... d_k)\r\n            - w: Weights, of shape (D, C)\r\n            - b: biases, of shape (C,)\r\n\r\n    Outputs:\r\n        - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\r\n        - dw: Gradient with respect to w, of shape (D, C)\r\n        - db: Gradient with respect to b, of shape (C,)\r\n    """"""\r\n    x, w, b = cache\r\n    x2d = np.reshape(x, (x.shape[0], -1))\r\n\r\n    # compute gradients\r\n    db = np.sum(dout, axis=0)\r\n    dw = np.dot(x2d.T, dout)\r\n    dx = np.dot(dout, w.T)\r\n\r\n    # reshape dx to match the size of x\r\n    dx = dx.reshape(x.shape)\r\n    \r\n    return dx, dw, db\r\n\r\ndef relu_forward(x):\r\n    """"""Forward pass for a layer of rectified linear units.\r\n\r\n    Inputs:\r\n        - x: a numpy array of any shape\r\n\r\n    Outputs:\r\n        - out: output of relu, same shape as x\r\n        - cache: x\r\n    """"""\r\n    cache = x\r\n    out = np.maximum(0, x)\r\n    return out, cache\r\n\r\ndef relu_backward(dout, cache):\r\n    """"""Backward pass for a layer of rectified linear units.\r\n\r\n    Inputs:\r\n        - dout: upstream derevatives, of any shape\r\n        - cache: x, same shape as dout\r\n\r\n    Outputs:\r\n        - dx: gradient of loss w.r.t x\r\n    """"""\r\n    x = cache\r\n    dx = dout * (x > 0)\r\n    return dx\r\n\r\n\r\ndef affine_relu_forward(x, w, b):\r\n    out, cache_a = affine_forward(x, w, b)\r\n    out, cache_r = relu_forward(out)\r\n    return out, (cache_a, cache_r)\r\n\r\n\r\ndef affine_relu_backward(dout, cache):\r\n    cache_a, cache_r = cache\r\n    dout = relu_backward(dout, cache_r)\r\n    dx, dw, db = affine_backward(dout, cache_a)\r\n    return dx, dw, db\r\n\r\n\r\ndef dropout_forward(x, dropout_param):\r\n    """"""\r\n    Performs the forward pass for (inverted) dropout.\r\n\r\n    Inputs:\r\n    - x: Input data, of any shape\r\n    - dropout_param: A dictionary with the following keys:\r\n      - p: Dropout parameter. We drop each neuron output with probability p.\r\n      - mode: \'test\' or \'train\'. If the mode is train, then perform dropout;\r\n        if the mode is test, then just return the input.\r\n      - seed: Seed for the random number generator. Passing seed makes this\r\n        function deterministic, which is needed for gradient checking but not\r\n        in real networks.\r\n\r\n    Outputs:\r\n    - out: Array of the same shape as x.\r\n    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout\r\n      mask that was used to multiply the input; in test mode, mask is None.\r\n    """"""\r\n    p, mode = dropout_param[\'p\'], dropout_param[\'mode\']\r\n    if \'seed\' in dropout_param:\r\n        np.random.seed(dropout_param[\'seed\'])\r\n\r\n    mask = None\r\n\r\n    if mode == \'train\':\r\n        mask = (np.random.rand(*x.shape) < (1 - p)) / (1 - p)\r\n        out = x * mask\r\n    elif mode == \'test\':\r\n        out = x\r\n\r\n    cache = (dropout_param, mask)\r\n    out = out.astype(x.dtype, copy=False)\r\n\r\n    return out, cache\r\n\r\n\r\ndef dropout_backward(dout, cache):\r\n    """"""\r\n    Perform the backward pass for (inverted) dropout.\r\n\r\n    Inputs:\r\n    - dout: Upstream derivatives, of any shape\r\n    - cache: (dropout_param, mask) from dropout_forward.\r\n    """"""\r\n    dropout_param, mask = cache\r\n    mode = dropout_param[\'mode\']\r\n\r\n    if mode == \'train\':\r\n        dx = dout * mask\r\n    elif mode == \'test\':\r\n        dx = dout\r\n    return dx\r\n\r\n\r\ndef batchnorm_forward(x, gamma, beta, bn_param):\r\n    N, D = x.shape\r\n\r\n    # get parameters\r\n    mode = bn_param[\'mode\']                   # mode is train or test\r\n    eps = bn_param.get(\'eps\', 1e-5)\r\n    momentum = bn_param.get(\'momentum\', 0.9)\r\n    running_mean = bn_param.get(\'running_mean\', np.zeros(D, dtype=x.dtype))\r\n    running_var  = bn_param.get(\'running_var\',  np.zeros(D, dtype=x.dtype))\r\n    cache = None\r\n\r\n    if mode == \'train\':\r\n        \r\n        # Normalize\r\n        mu = np.mean(x, axis=0)\r\n        xc = x - mu\r\n        var = np.mean(xc ** 2, axis=0)\r\n        std = (var + eps) ** 0.5\r\n        xn = xc / std\r\n        \r\n        # Scale and Shift\r\n        out = gamma * xn + beta\r\n\r\n        cache = (x, xc, var, std, xn, gamma, eps)\r\n\r\n        # update running mean and running average\r\n        running_mean = momentum * running_mean + (1 - momentum) * mu\r\n        running_var  = momentum * running_var  + (1 - momentum) * var\r\n        \r\n        bn_param[\'running_mean\'] = running_mean\r\n        bn_param[\'running_var\' ] = running_var\r\n    \r\n    else:\r\n        xn = (x - running_mean) / (np.sqrt(running_var + eps))\r\n        out = gamma * xn + beta\r\n        \r\n    return out, cache\r\n\r\n\r\ndef batchnorm_backward(dout, cache):\r\n    """"""\r\n    Backward pass for batch normalization.\r\n\r\n    For this implementation, you should write out a computation graph for\r\n    batch normalization on paper and propagate gradients backward through\r\n    intermediate nodes.\r\n\r\n    Inputs:\r\n    - dout: Upstream derivatives, of shape (N, D)\r\n    - cache: Variable of intermediates from batchnorm_forward.\r\n\r\n    Returns a tuple of:\r\n    - dx: Gradient with respect to inputs x, of shape (N, D)\r\n    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\r\n    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\r\n    """"""\r\n    dx, dgamma, dbeta = None, None, None\r\n\r\n    x, xc, var, std, xn, gamma, eps = cache\r\n    N = x.shape[0]\r\n\r\n    dbeta = np.sum(dout, axis=0)\r\n    dgamma = np.sum(dout * xn, axis=0)\r\n    dxn = dout * gamma\r\n\r\n    dxc = dxn / std\r\n    dstd = np.sum(-(xc * dxn) / (std * std), axis=0)\r\n    dvar = 0.5 * dstd / std\r\n\r\n    dxc += (2.0 / N) * xc * dvar\r\n    dmu = -np.sum(dxc, axis=0)\r\n    dx = dxc + dmu / N\r\n\r\n    return dx, dgamma, dbeta\r\n\r\n\r\ndef svm_loss(scores, y):\r\n    """"""\r\n    Fully-vectorized implementation of SVM loss function.\r\n\r\n    Inputs:\r\n        - scores: scores for all training data (N, C)\r\n        - y: correct labels for the training data of shape (N,)\r\n\r\n    Outputs:\r\n       - loss: data loss plus L2 regularization loss\r\n       - grads: graidents of loss w.r.t scores\r\n    """"""\r\n\r\n    N = scores.shape[0]\r\n\r\n    # Compute svm data loss\r\n    correct_class_scores = scores[range(N), y]\r\n    margins = np.maximum(0.0, scores - correct_class_scores[:, None] + 1.0)\r\n    margins[range(N), y] = 0.0\r\n    loss = np.sum(margins) / N\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    num_pos = np.sum(margins > 0, axis=1)\r\n    dscores = np.zeros(scores.shape)\r\n    dscores[margins > 0] = 1\r\n    dscores[range(N), y] -= num_pos\r\n    dscores /= N\r\n\r\n    return loss, dscores\r\n\r\n\r\ndef softmax_loss(scores, y):\r\n    """"""\r\n    Softmax loss function, fully vectorized implementation.\r\n\r\n    Inputs have dimension D, there are C classes, and we operate on minibatches\r\n    of N examples.\r\n\r\n    Inputs:\r\n        - scores: A numpy array of shape (N, C).\r\n        - y: A numpy array of shape (N,) containing training labels;\r\n\r\n    Outputs:\r\n        - loss as single float\r\n        - gradient with respect to scores\r\n    """"""\r\n    N = scores.shape[0]  # number of input data\r\n\r\n    # compute data loss\r\n    shifted_logits = scores - np.max(scores, axis=1, keepdims=True)\r\n    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\r\n    log_probs = shifted_logits - np.log(Z)\r\n    probs = np.exp(log_probs)\r\n    loss = -np.sum(log_probs[range(N), y]) / N\r\n\r\n    # Compute gradient of loss function w.r.t. scores\r\n    dscores = probs.copy()\r\n    dscores[range(N), y] -= 1\r\n    dscores /= N\r\n    \r\n    return loss, dscores        \r\n'"
MLiP-week09/optim.py,6,"b'import numpy as np\r\n\r\n\r\ndef sgd(w, dw, config=None):\r\n    """"""\r\n    Performs vanilla stochastic gradient descent.\r\n\r\n    config format:\r\n    - learning_rate: Scalar learning rate.\r\n    """"""\r\n    if config is None: config = {}\r\n    config.setdefault(\'learning_rate\', 1e-2)\r\n\r\n    w -= config[\'learning_rate\'] * dw\r\n    return w, config\r\n\r\n\r\ndef sgd_momentum(w, dw, config=None):\r\n    """"""\r\n    Performs stochastic gradient descent with momentum.\r\n\r\n    config format:\r\n    - learning_rate: Scalar learning rate.\r\n    - momentum: Scalar between 0 and 1 giving the momentum value.\r\n      Setting momentum = 0 reduces to sgd.\r\n    - velocity: A numpy array of the same shape as w and dw used to store a\r\n      moving average of the gradients.\r\n    """"""\r\n    if config is None: config = {}\r\n    config.setdefault(\'learning_rate\', 1e-2)\r\n    config.setdefault(\'momentum\', 0.9)\r\n    v = config.get(\'velocity\', np.zeros_like(w))\r\n\r\n    v = config[\'momentum\'] * v - config[\'learning_rate\'] * dw\r\n    next_w = w + v\r\n    config[\'velocity\'] = v\r\n\r\n    return next_w, config\r\n\r\n\r\ndef rmsprop(x, dx, config=None):\r\n    """"""\r\n    Uses the RMSProp update rule, which uses a moving average of squared\r\n    gradient values to set adaptive per-parameter learning rates.\r\n\r\n    config format:\r\n    - learning_rate: Scalar learning rate.\r\n    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\r\n      gradient cache.\r\n    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\r\n    - cache: Moving average of second moments of gradients.\r\n    """"""\r\n    if config is None: config = {}\r\n    config.setdefault(\'learning_rate\', 1e-2)\r\n    config.setdefault(\'decay_rate\', 0.99)\r\n    config.setdefault(\'epsilon\', 1e-8)\r\n    config.setdefault(\'cache\', np.zeros_like(x))\r\n\r\n    learning_rate = config[\'learning_rate\']\r\n    decay_rate = config[\'decay_rate\']\r\n    cache = config[\'cache\']\r\n    eps = config[\'epsilon\']\r\n\r\n    cache = decay_rate * cache + (1 - decay_rate) * dx ** 2\r\n    next_x = x - learning_rate * dx / (np.sqrt(cache) + eps)\r\n\r\n    config[\'cache\'] = cache\r\n    return next_x, config\r\n\r\n\r\ndef adam(x, dx, config=None):\r\n    """"""\r\n    Uses the Adam update rule, which incorporates moving averages of both the\r\n    gradient and its square and a bias correction term.\r\n\r\n    config format:\r\n    - learning_rate: Scalar learning rate.\r\n    - beta1: Decay rate for moving average of first moment of gradient.\r\n    - beta2: Decay rate for moving average of second moment of gradient.\r\n    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\r\n    - m: Moving average of gradient.\r\n    - v: Moving average of squared gradient.\r\n    - t: Iteration number.\r\n    """"""\r\n    if config is None: config = {}\r\n    config.setdefault(\'learning_rate\', 1e-3)\r\n    config.setdefault(\'beta1\', 0.9)\r\n    config.setdefault(\'beta2\', 0.999)\r\n    config.setdefault(\'epsilon\', 1e-8)\r\n    config.setdefault(\'m\', np.zeros_like(x))\r\n    config.setdefault(\'v\', np.zeros_like(x))\r\n    config.setdefault(\'t\', 1)\r\n\r\n    # read params from dictionary\r\n    learning_rate = config[\'learning_rate\']\r\n    beta1, beta2, eps = config[\'beta1\'], config[\'beta2\'], config[\'epsilon\']\r\n    m, v, t = config[\'m\'], config[\'v\'], config[\'t\']\r\n\r\n    # apply adam update rule\r\n    t = t + 1\r\n    m = beta1 * m + (1 - beta1) * dx\r\n    v = beta2 * v + (1 - beta2) * dx ** 2\r\n    mb = m / (1 - beta1 ** t)\r\n    vb = v / (1 - beta2 ** t)\r\n    next_x = x - learning_rate * mb / (np.sqrt(vb) + eps)\r\n\r\n    # store new params in the dictionary\r\n    config[\'m\'], config[\'v\'], config[\'t\'] = m, v, t\r\n\r\n    return next_x, config\r\n'"
MLiP-week09/solver.py,5,"b'import os\r\nimport pickle as pickle\r\nimport numpy as np\r\nimport optim\r\n\r\nclass Solver(object):\r\n    """"""\r\n    A Solver encapsulates all the logic necessary for training classification\r\n    models. The Solver performs stochastic gradient descent using different\r\n    update rules defined in optim.py.\r\n\r\n    The solver accepts both training and validation data and labels so it can\r\n    periodically check classification accuracy on both training and validation\r\n    data to watch out for over-fitting.\r\n\r\n    To train a model, you will first construct a Solver instance, passing the\r\n    model, dataset, and various options (learning rate, batch size, etc) to the\r\n    constructor. You will then call the train() method to run the optimization\r\n    procedure and train the model.\r\n\r\n    After the train() method returns, model.params will contain the parameters\r\n    that performed best on the validation set over the course of training.\r\n    In addition, the instance variable solver.loss_history will contain a list\r\n    of all losses encountered during training and the instance variables\r\n    solver.train_acc_history and solver.val_acc_history will be lists of the\r\n    accuracies of the model on the training and validation set at each epoch.\r\n\r\n    Example usage might look something like this:\r\n\r\n    data = {\r\n      \'X_train\': # training data\r\n      \'y_train\': # training labels\r\n      \'X_val\': # validation data\r\n      \'y_val\': # validation labels\r\n    }\r\n    model = MyAwesomeModel(hidden_size=100, reg=10)\r\n    solver = Solver(model, data,\r\n                    update_rule=\'sgd\',\r\n                    optim_config={\r\n                      \'learning_rate\': 1e-3,\r\n                    },\r\n                    lr_decay=0.95,\r\n                    num_epochs=10, batch_size=100,\r\n                    print_every=100)\r\n    solver.train()\r\n\r\n\r\n    A Solver works on a model object that must conform to the following API:\r\n\r\n    - model.params must be a dictionary mapping string parameter names to numpy\r\n      arrays containing parameter values.\r\n\r\n    - model.loss(X, y) must be a function that computes training-time loss and\r\n      gradients, and test-time classification scores, with the following inputs\r\n      and outputs:\r\n\r\n      Inputs:\r\n      - X: Array giving a mini-batch of input data of shape (N, d_1, ..., d_k)\r\n      - y: Array of labels, of shape (N,) giving labels for X where y[i] is the\r\n        label for X[i].\r\n\r\n      Returns:\r\n      If y is None, run a test-time forward pass and return:\r\n      - scores: Array of shape (N, C) giving classification scores for X where\r\n        scores[i, c] gives the score of class c for X[i].\r\n\r\n      If y is not None, run a training time forward and backward pass and\r\n      return a tuple of:\r\n      - loss: Scalar giving the loss\r\n      - grads: Dictionary with the same keys as self.params mapping parameter\r\n        names to gradients of the loss with respect to those parameters.\r\n    """"""\r\n\r\n    def __init__(self, model, data, **kwargs):\r\n        """"""\r\n        Construct a new Solver instance.\r\n\r\n        Required arguments:\r\n        - model: A model object conforming to the API described above\r\n        - data: A dictionary of training and validation data containing:\r\n          \'X_train\': Array, shape (N_train, d_1, ..., d_k) of training images\r\n          \'X_val\': Array, shape (N_val, d_1, ..., d_k) of validation images\r\n          \'y_train\': Array, shape (N_train,) of labels for training images\r\n          \'y_val\': Array, shape (N_val,) of labels for validation images\r\n\r\n        Optional arguments:\r\n        - update_rule: A string giving the name of an update rule in optim.py.\r\n          Default is \'sgd\'.\r\n        - optim_config: A dictionary containing hyper-parameters that will be\r\n          passed to the chosen update rule. Each update rule requires different\r\n          hyper-parameters (see optim.py) but all update rules require a\r\n          \'learning_rate\' parameter so that should always be present.\r\n        - lr_decay: A scalar for learning rate decay; after each epoch the\r\n          learning rate is multiplied by this value.\r\n        - batch_size: Size of mini-batches used to compute loss and gradient\r\n          during training.\r\n        - num_epochs: The number of epochs to run for during training.\r\n        - print_every: Integer; training losses will be printed every\r\n          print_every iterations.\r\n        - verbose: Boolean; if set to false then no output will be printed\r\n          during training.\r\n        - num_train_samples: Number of training samples used to check training\r\n          accuracy; default is 1000; set to None to use entire training set.\r\n        - num_val_samples: Number of validation samples to use to check val\r\n          accuracy; default is None, which uses the entire validation set.\r\n        - checkpoint_name: If not None, then save model checkpoints here every\r\n          epoch.\r\n        """"""\r\n        self.model = model\r\n        self.X_train = data[\'X_train\']\r\n        self.y_train = data[\'y_train\']\r\n        self.X_val = data[\'X_val\']\r\n        self.y_val = data[\'y_val\']\r\n\r\n        # Unpack keyword arguments\r\n        self.update_rule = kwargs.pop(\'update_rule\', \'sgd\')\r\n        self.optim_config = kwargs.pop(\'optim_config\', {})\r\n        self.lr_decay = kwargs.pop(\'lr_decay\', 1.0)\r\n        self.batch_size = kwargs.pop(\'batch_size\', 100)\r\n        self.num_epochs = kwargs.pop(\'num_epochs\', 10)\r\n        self.num_train_samples = kwargs.pop(\'num_train_samples\', 1000)\r\n        self.num_val_samples = kwargs.pop(\'num_val_samples\', None)\r\n\r\n        self.checkpoint_name = kwargs.pop(\'checkpoint_name\', None)\r\n        self.print_every = kwargs.pop(\'print_every\', 10)\r\n        self.verbose = kwargs.pop(\'verbose\', True)\r\n\r\n        # Throw an error if there are extra keyword arguments\r\n        if len(kwargs) > 0:\r\n            extra = \', \'.join(\'""%s""\' % k for k in list(kwargs.keys()))\r\n            raise ValueError(\'Unrecognized arguments %s\' % extra)\r\n\r\n        # Make sure the update rule exists, then replace the string\r\n        # name with the actual function\r\n        if not hasattr(optim, self.update_rule):\r\n            raise ValueError(\'Invalid update_rule ""%s""\' % self.update_rule)\r\n        self.update_rule = getattr(optim, self.update_rule)\r\n\r\n        self._reset()\r\n\r\n    def _reset(self):\r\n        """"""\r\n        Set up some book-keeping variables for optimization. Don\'t call this\r\n        manually.\r\n        """"""\r\n        # Set up some variables for book-keeping\r\n        self.epoch = 0\r\n        self.best_val_acc = 0\r\n        self.best_params = {}\r\n        self.loss_history = []\r\n        self.train_acc_history = []\r\n        self.val_acc_history = []\r\n\r\n        # Make a deep copy of the optim_config for each parameter\r\n        self.optim_configs = {}\r\n        for p in self.model.params:\r\n            d = {k: v for k, v in self.optim_config.items()}\r\n            self.optim_configs[p] = d\r\n\r\n    def _step(self):\r\n        """"""\r\n        Make a single gradient update. This is called by train() and should not\r\n        be called manually.\r\n        """"""\r\n        # Make a minibatch of training data\r\n        num_train = self.X_train.shape[0]\r\n        batch_mask = np.random.choice(num_train, self.batch_size)\r\n        X_batch = self.X_train[batch_mask]\r\n        y_batch = self.y_train[batch_mask]\r\n\r\n        # Compute loss and gradient\r\n        loss, grads = self.model.loss(X_batch, y_batch)\r\n        self.loss_history.append(loss)\r\n\r\n        # Perform a parameter update\r\n        for p, w in self.model.params.items():\r\n            dw = grads[p]\r\n            config = self.optim_configs[p]\r\n            next_w, next_config = self.update_rule(w, dw, config)\r\n            self.model.params[p] = next_w\r\n            self.optim_configs[p] = next_config\r\n\r\n    def _save_checkpoint(self):\r\n        if self.checkpoint_name is None: return\r\n        checkpoint = {\r\n            \'model\': self.model,\r\n            \'update_rule\': self.update_rule,\r\n            \'lr_decay\': self.lr_decay,\r\n            \'optim_config\': self.optim_config,\r\n            \'batch_size\': self.batch_size,\r\n            \'num_train_samples\': self.num_train_samples,\r\n            \'num_val_samples\': self.num_val_samples,\r\n            \'epoch\': self.epoch,\r\n            \'loss_history\': self.loss_history,\r\n            \'train_acc_history\': self.train_acc_history,\r\n            \'val_acc_history\': self.val_acc_history,\r\n        }\r\n        filename = \'%s_epoch_%d.pkl\' % (self.checkpoint_name, self.epoch)\r\n        if self.verbose:\r\n            print(\'Saving checkpoint to ""%s""\' % filename)\r\n        with open(filename, \'wb\') as f:\r\n            pickle.dump(checkpoint, f)\r\n\r\n    def check_accuracy(self, X, y, num_samples=None, batch_size=100):\r\n        """"""\r\n        Check accuracy of the model on the provided data.\r\n\r\n        Inputs:\r\n        - X: Array of data, of shape (N, d_1, ..., d_k)\r\n        - y: Array of labels, of shape (N,)\r\n        - num_samples: If not None, subsample the data and only test the model\r\n          on num_samples data points.\r\n        - batch_size: Split X and y into batches of this size to avoid using\r\n          too much memory.\r\n\r\n        Returns:\r\n        - acc: Scalar giving the fraction of instances that were correctly\r\n          classified by the model.\r\n        """"""\r\n\r\n        # Maybe subsample the data\r\n        N = X.shape[0]\r\n        if num_samples is not None and N > num_samples:\r\n            mask = np.random.choice(N, num_samples)\r\n            N = num_samples\r\n            X = X[mask]\r\n            y = y[mask]\r\n\r\n        # Compute predictions in batches\r\n        num_batches = N // batch_size\r\n        if N % batch_size != 0:\r\n            num_batches += 1\r\n        y_pred = []\r\n        for i in range(num_batches):\r\n            start = i * batch_size\r\n            end = (i + 1) * batch_size\r\n            scores = self.model.loss(X[start:end])\r\n            y_pred.append(np.argmax(scores, axis=1))\r\n        y_pred = np.hstack(y_pred)\r\n        acc = np.mean(y_pred == y)\r\n\r\n        return acc\r\n\r\n    def train(self):\r\n        """"""\r\n        Run optimization to train the model.\r\n        """"""\r\n        num_train = self.X_train.shape[0]\r\n        iterations_per_epoch = max(num_train // self.batch_size, 1)\r\n        num_iterations = self.num_epochs * iterations_per_epoch\r\n\r\n        for t in range(num_iterations):\r\n            self._step()\r\n\r\n            # Maybe print training loss\r\n            if self.verbose and t % self.print_every == 0:\r\n                print(\'(Iteration %4d / %4d) loss: %f\' % (\r\n                    t + 1, num_iterations, self.loss_history[-1]))\r\n\r\n            # At the end of every epoch, increment the epoch counter and decay\r\n            # the learning rate.\r\n            epoch_end = (t + 1) % iterations_per_epoch == 0\r\n            if epoch_end:\r\n                self.epoch += 1\r\n                for k in self.optim_configs:\r\n                    self.optim_configs[k][\'learning_rate\'] *= self.lr_decay\r\n\r\n            # Check train and val accuracy on the first iteration, the last\r\n            # iteration, and at the end of each epoch.\r\n            first_it = (t == 0)\r\n            last_it = (t == num_iterations - 1)\r\n            if first_it or last_it or epoch_end:\r\n                train_acc = self.check_accuracy(self.X_train, self.y_train,\r\n                                                num_samples=self.num_train_samples)\r\n                val_acc = self.check_accuracy(self.X_val, self.y_val,\r\n                                              num_samples=self.num_val_samples)\r\n                self.train_acc_history.append(train_acc)\r\n                self.val_acc_history.append(val_acc)\r\n                self._save_checkpoint()\r\n\r\n                if self.verbose:\r\n                    print(\'(Epoch %2d / %2d) train acc: %f; val_acc: %f\' % (\r\n                        self.epoch, self.num_epochs, train_acc, val_acc))\r\n\r\n                # Keep track of the best model\r\n                if val_acc > self.best_val_acc:\r\n                    self.best_val_acc = val_acc\r\n                    self.best_params = {}\r\n                    for k, v in self.model.params.items():\r\n                        self.best_params[k] = v.copy()\r\n\r\n        # At the end of training swap the best params into the model\r\n        self.model.params = self.best_params\r\n'"
MLiP-week01and02/utils/data_utils.py,3,"b'import cPickle as pickle\nimport numpy as np\nimport os\nfrom scipy.misc import imread\n\ndef load_CIFAR_batch(filename):\n  """""" load single batch of cifar """"""\n  with open(filename, \'rb\') as f:\n    datadict = pickle.load(f)\n    X = datadict[\'data\']\n    Y = datadict[\'labels\']\n    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(""float"")\n    Y = np.array(Y)\n    return X, Y\n\ndef load_CIFAR10(ROOT):\n  """""" load all of cifar """"""\n  xs = []\n  ys = []\n  for b in range(1,6):\n    f = os.path.join(ROOT, \'data_batch_%d\' % (b, ))\n    X, Y = load_CIFAR_batch(f)\n    xs.append(X)\n    ys.append(Y)    \n  Xtr = np.concatenate(xs)\n  Ytr = np.concatenate(ys)\n  del X, Y\n  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \'test_batch\'))\n  return Xtr, Ytr, Xte, Yte\n'"
MLiP-week03and4/fig_codes/plot_2d_separator.py,4,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_2d_separator(classifier, X, fill=False, ax=None, eps=None):\n    if eps is None:\n        eps = X.std() / 2.\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 100)\n    yy = np.linspace(y_min, y_max, 100)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    try:\n        decision_values = classifier.decision_function(X_grid)\n        levels = [0]\n        fill_levels = [decision_values.min(), 0, decision_values.max()]\n    except AttributeError:\n        # no decision_function\n        decision_values = classifier.predict_proba(X_grid)[:, 1]\n        levels = [.5]\n        fill_levels = [0, .5, 1]\n\n    if ax is None:\n        ax = plt.gca()\n    if fill:\n        ax.contourf(X1, X2, decision_values.reshape(X1.shape),\n                    levels=fill_levels, colors=[\'blue\', \'red\'])\n    else:\n        ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels,\n                   colors=""black"")\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\nif __name__ == \'__main__\':\n    from sklearn.datasets import make_blobs\n    from sklearn.linear_model import LogisticRegression\n    X, y = make_blobs(centers=2, random_state=42)\n    clf = LogisticRegression().fit(X, y)\n    plot_2d_separator(clf, X, fill=True)\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.show()\n'"
MLiP-week03and4/solutions/03A_faces_plot.py,0,"b""faces = fetch_olivetti_faces()\n\n# set up the figure\nfig = plt.figure(figsize=(6, 6))  # figure size in inches\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\n# plot the faces:\nfor i in range(64):\n    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n    ax.imshow(faces.images[i], cmap=plt.cm.bone, interpolation='nearest')\n"""
MLiP-week03and4/solutions/04_wrong-predictions.py,1,"b'plt.figure(figsize=(10, 6))\n\n\nfor i in incorrect_idx:\n    print(\'%d: Predicted %d | True label %d\' % (i, y_pred[i], y_test[i]))\n\n# Plot two dimensions\n\ncolors = [""darkblue"", ""darkgreen"", ""gray""]\n\nfor n, color in enumerate(colors):\n    idx = np.where(y_test == n)[0]\n    plt.scatter(X_test[idx, 1], X_test[idx, 2], color=color, label=""Class %s"" % str(n))\n\nfor i, marker in zip(incorrect_idx, [\'x\', \'s\', \'v\']):\n    plt.scatter(X_test[i, 1], X_test[i, 2],\n                color=""darkred"",\n                marker=marker,\n                s=60,\n                label=i)\n\nplt.xlabel(\'sepal width [cm]\')\nplt.ylabel(\'petal length [cm]\')\nplt.legend(loc=1, scatterpoints=1)\nplt.title(""Iris Classification results"")\nplt.show()\n'"
MLiP-week03and4/solutions/05A_knn_with_diff_k.py,0,"b""from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.25,\n                                                    random_state=1234,\n                                                    stratify=y)\n\nX_trainsub, X_valid, y_trainsub, y_valid = train_test_split(X_train, y_train,\n                                                            test_size=0.5,\n                                                            random_state=1234,\n                                                            stratify=y_train)\n\nfor k in range(1, 20):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    train_score = knn.fit(X_trainsub, y_trainsub).\\\n        score(X_trainsub, y_trainsub)\n    valid_score = knn.score(X_valid, y_valid)\n    print('k: %d, Train/Valid Acc: %.3f/%.3f' %\n          (k, train_score, valid_score))\n\n\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train, y_train)\nprint('k=9 Test Acc: %.3f' % knn.score(X_test, y_test))\n"""
MLiP-week03and4/solutions/06A_knn_vs_linreg.py,0,"b""from sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\nboston = load_boston()\nX = boston.data\ny = boston.target\n\nprint('X.shape:', X.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.25,\n                                                    random_state=42)\n\nlinreg = LinearRegression()\nknnreg = KNeighborsRegressor(n_neighbors=1)\n\nlinreg.fit(X_train, y_train)\nprint('Linear Regression Train/Test: %.3f/%.3f' %\n      (linreg.score(X_train, y_train),\n       linreg.score(X_test, y_test)))\n\nknnreg.fit(X_train, y_train)\nprint('KNeighborsRegressor Train/Test: %.3f/%.3f' %\n      (knnreg.score(X_train, y_train),\n       knnreg.score(X_test, y_test)))\n"""
MLiP-week03and4/solutions/06B_lin_with_sine.py,2,"b'XX_train = np.concatenate((X_train, np.sin(4 * X_train)), axis=1)\nXX_test = np.concatenate((X_test, np.sin(4 * X_test)), axis=1)\nregressor.fit(XX_train, y_train)\ny_pred_test_sine = regressor.predict(XX_test)\n\nplt.plot(X_test, y_test, \'o\', label=""data"")\nplt.plot(X_test, y_pred_test_sine, \'o\', label=""prediction with sine"")\nplt.plot(X_test, y_pred_test, label=\'prediction without sine\')\nplt.legend(loc=\'best\');\n'"
MLiP-week05/figs/figures.py,11,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\n\ndef plot_kmeans_interactive(min_clusters=1, max_clusters=6):\n    #from IPython.html.widgets import interact\n    from ipywidgets import interact\n    from sklearn.metrics.pairwise import euclidean_distances\n    from sklearn.datasets.samples_generator import make_blobs\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\'ignore\')\n\n        X, y = make_blobs(n_samples=300, centers=4,\n                          random_state=0, cluster_std=0.60)\n\n        def _kmeans_step(frame=0, n_clusters=4):\n            rng = np.random.RandomState(2)\n            labels = np.zeros(X.shape[0])\n            centers = rng.randn(n_clusters, 2)\n\n            nsteps = frame // 3\n\n            for i in range(nsteps + 1):\n                old_centers = centers\n                if i < nsteps or frame % 3 > 0:\n                    dist = euclidean_distances(X, centers)\n                    labels = dist.argmin(1)\n\n                if i < nsteps or frame % 3 > 1:\n                    centers = np.array([X[labels == j].mean(0)\n                                        for j in range(n_clusters)])\n                    nans = np.isnan(centers)\n                    centers[nans] = old_centers[nans]\n\n\n            # plot the data and cluster centers\n            plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap=\'rainbow\',\n                        vmin=0, vmax=n_clusters - 1);\n            plt.scatter(old_centers[:, 0], old_centers[:, 1], marker=\'o\',\n                        c=np.arange(n_clusters),\n                        s=200, cmap=\'rainbow\')\n            plt.scatter(old_centers[:, 0], old_centers[:, 1], marker=\'o\',\n                        c=\'black\', s=50)\n\n            # plot new centers if third frame\n            if frame % 3 == 2:\n                for i in range(n_clusters):\n                    plt.annotate(\'\', centers[i], old_centers[i], \n                                 arrowprops=dict(arrowstyle=\'->\', linewidth=1))\n                plt.scatter(centers[:, 0], centers[:, 1], marker=\'o\',\n                            c=np.arange(n_clusters),\n                            s=200, cmap=\'rainbow\')\n                plt.scatter(centers[:, 0], centers[:, 1], marker=\'o\',\n                            c=\'black\', s=50)\n\n            plt.xlim(-4, 4)\n            plt.ylim(-2, 10)\n\n            if frame % 3 == 1:\n                plt.text(3.8, 9.5, ""1. Reassign points to nearest centroid"",\n                         ha=\'right\', va=\'top\', size=14)\n            elif frame % 3 == 2:\n                plt.text(3.8, 9.5, ""2. Update centroids to cluster means"",\n                         ha=\'right\', va=\'top\', size=14)\n\n    \n    return interact(_kmeans_step, frame=np.arange(0, 50),\n                    n_clusters=np.arange(min_clusters, max_clusters))\n\n\ndef plot_image_components(x, coefficients=None, mean=0, components=None,\n                          imshape=(8, 8), n_components=6, fontsize=12):\n    if coefficients is None:\n        coefficients = x\n        \n    if components is None:\n        components = np.eye(len(coefficients), len(x))\n        \n    mean = np.zeros_like(x) + mean\n        \n\n    fig = plt.figure(figsize=(1.2 * (5 + n_components), 1.2 * 2))\n    g = plt.GridSpec(2, 5 + n_components, hspace=0.3)\n\n    def show(i, j, x, title=None):\n        ax = fig.add_subplot(g[i, j], xticks=[], yticks=[])\n        ax.imshow(x.reshape(imshape), interpolation=\'nearest\')\n        if title:\n            ax.set_title(title, fontsize=fontsize)\n\n    show(slice(2), slice(2), x, ""True"")\n\n    approx = mean.copy()\n    show(0, 2, np.zeros_like(x) + mean, r\'$\\mu$\')\n    show(1, 2, approx, r\'$1 \\cdot \\mu$\')\n\n    for i in range(0, n_components):\n        approx = approx + coefficients[i] * components[i]\n        show(0, i + 3, components[i], r\'$c_{0}$\'.format(i + 1))\n        show(1, i + 3, approx,\n             r""${0:.2f} \\cdot c_{1}$"".format(coefficients[i], i + 1))\n        plt.gca().text(0, 1.05, \'$+$\', ha=\'right\', va=\'bottom\',\n                       transform=plt.gca().transAxes, fontsize=fontsize)\n\n    show(slice(2), slice(-2, None), approx, ""Approx"")\n\n\ndef plot_pca_interactive(data, n_components=6):\n    from sklearn.decomposition import PCA\n    #from IPython.html.widgets import interact\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        plot_image_components(data[i], Xproj[i],\n                              pca.mean_, pca.components_)\n    \n    interact(show_decomp, i=(0, data.shape[0] - 1));\n'"
MLiP-week05/figs/plot_digits_dataset.py,6,"b'# Taken from example in scikit-learn examples\n# Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2011\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nfrom sklearn import datasets, decomposition\n\n\ndef digits_plot():\n    digits = datasets.load_digits(n_class=6)\n    n_digits = 500\n    X = digits.data[:n_digits]\n    y = digits.target[:n_digits]\n    n_samples, n_features = X.shape\n\n    def plot_embedding(X, title=None):\n        x_min, x_max = np.min(X, 0), np.max(X, 0)\n        X = (X - x_min) / (x_max - x_min)\n\n        plt.figure()\n        ax = plt.subplot(111)\n        for i in range(X.shape[0]):\n            plt.text(X[i, 0], X[i, 1], str(digits.target[i]),\n                     color=plt.cm.Set1(y[i] / 10.),\n                     fontdict={\'weight\': \'bold\', \'size\': 9})\n\n        if hasattr(offsetbox, \'AnnotationBbox\'):\n            # only print thumbnails with matplotlib > 1.0\n            shown_images = np.array([[1., 1.]])  # just something big\n            for i in range(X.shape[0]):\n                dist = np.sum((X[i] - shown_images) ** 2, 1)\n                if np.min(dist) < 1e5:\n                    # don\'t show points that are too close\n                    # set a high threshold to basically turn this off\n                    continue\n                shown_images = np.r_[shown_images, [X[i]]]\n                imagebox = offsetbox.AnnotationBbox(\n                    offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n                    X[i])\n                ax.add_artist(imagebox)\n        plt.xticks([]), plt.yticks([])\n        if title is not None:\n            plt.title(title)\n\n    n_img_per_row = 10\n    img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n    for i in range(n_img_per_row):\n        ix = 10 * i + 1\n        for j in range(n_img_per_row):\n            iy = 10 * j + 1\n            img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n\n    plt.imshow(img, cmap=plt.cm.binary)\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(\'A selection from the 64-dimensional digits dataset\')\n    print(""Computing PCA projection"")\n    pca = decomposition.PCA(n_components=2).fit(X)\n    X_pca = pca.transform(X)\n    plot_embedding(X_pca, ""Principal Components projection of the digits"")\n    plt.figure()\n    plt.title(""First Principal Component"")\n    plt.matshow(pca.components_[0, :].reshape(8, 8), cmap=""gray"")\n    plt.axis(\'off\')\n    plt.figure()\n    plt.title(""Second Principal Component"")\n    plt.matshow(pca.components_[1, :].reshape(8, 8), cmap=""gray"")\n    plt.axis(\'off\')\n    plt.show()\n'"
MLiP-week05/figs/plot_helpers.py,0,"b""from matplotlib.colors import ListedColormap\n\ncm3 = ListedColormap(['#0000aa', '#ff2020', '#50ff50'])\ncm2 = ListedColormap(['#0000aa', '#ff2020'])\n"""
MLiP-week05/figs/plot_pca.py,5,"b'from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_pca_illustration():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n\n    pca = PCA()\n    pca.fit(X_blob)\n    X_pca = pca.transform(X_blob)\n\n    S = X_pca.std(axis=0)\n\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n    axes = axes.ravel()\n\n    axes[0].set_title(""Original data"")\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_pca[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].arrow(pca.mean_[0], pca.mean_[1], S[0] * pca.components_[0, 0],\n                  S[0] * pca.components_[0, 1], width=.1, head_width=.3,\n                  color=\'k\')\n    axes[0].arrow(pca.mean_[0], pca.mean_[1], S[1] * pca.components_[1, 0],\n                  S[1] * pca.components_[1, 1], width=.1, head_width=.3,\n                  color=\'k\')\n    axes[0].text(-1.5, -.5, ""Component 2"", size=14)\n    axes[0].text(-4, -4, ""Component 1"", size=14)\n    axes[0].set_aspect(\'equal\')\n\n    axes[1].set_title(""Transformed data"")\n    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""First principal component"")\n    axes[1].set_ylabel(""Second principal component"")\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_ylim(-8, 8)\n\n    pca = PCA(n_components=1)\n    pca.fit(X_blob)\n    X_inverse = pca.inverse_transform(pca.transform(X_blob))\n\n    axes[2].set_title(""Transformed data w/ second component dropped"")\n    axes[2].scatter(X_pca[:, 0], np.zeros(X_pca.shape[0]), c=X_pca[:, 0],\n                    linewidths=0, s=60, cmap=\'viridis\')\n    axes[2].set_xlabel(""First principal component"")\n    axes[2].set_aspect(\'equal\')\n    axes[2].set_ylim(-8, 8)\n\n    axes[3].set_title(""Back-rotation using only first component"")\n    axes[3].scatter(X_inverse[:, 0], X_inverse[:, 1], c=X_pca[:, 0],\n                    linewidths=0, s=60, cmap=\'viridis\')\n    axes[3].set_xlabel(""feature 1"")\n    axes[3].set_ylabel(""feature 2"")\n    axes[3].set_aspect(\'equal\')\n    axes[3].set_xlim(-8, 4)\n    axes[3].set_ylim(-8, 4)\n\n\ndef plot_pca_whitening():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n\n    pca = PCA(whiten=True)\n    pca.fit(X_blob)\n    X_pca = pca.transform(X_blob)\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n    axes = axes.ravel()\n\n    axes[0].set_title(""Original data"")\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_pca[:, 0], linewidths=0, s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].set_aspect(\'equal\')\n\n    axes[1].set_title(""Whitened data"")\n    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], linewidths=0, s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""First principal component"")\n    axes[1].set_ylabel(""Second principal component"")\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_xlim(-3, 4)\n'"
MLiP-week05/figs/plot_scaling.py,2,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom .plot_helpers import cm2\n\n\ndef plot_scaling():\n    X, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\n    X += 3\n\n    plt.figure(figsize=(15, 8))\n    main_ax = plt.subplot2grid((2, 4), (0, 0), rowspan=2, colspan=2)\n\n    main_ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm2, s=60)\n    maxx = np.abs(X[:, 0]).max()\n    maxy = np.abs(X[:, 1]).max()\n\n    main_ax.set_xlim(-maxx + 1, maxx + 1)\n    main_ax.set_ylim(-maxy + 1, maxy + 1)\n    main_ax.set_title(""Original Data"")\n    other_axes = [plt.subplot2grid((2, 4), (i, j)) for j in range(2, 4) for i in range(2)]\n\n    for ax, scaler in zip(other_axes, [StandardScaler(), RobustScaler(),\n                                       MinMaxScaler(), Normalizer(norm=\'l2\')]):\n        X_ = scaler.fit_transform(X)\n        ax.scatter(X_[:, 0], X_[:, 1], c=y, cmap=cm2, s=60)\n        ax.set_xlim(-2, 2)\n        ax.set_ylim(-2, 2)\n        ax.set_title(type(scaler).__name__)\n\n    other_axes.append(main_ax)\n\n    for ax in other_axes:\n        ax.spines[\'left\'].set_position(\'center\')\n        ax.spines[\'right\'].set_color(\'none\')\n        ax.spines[\'bottom\'].set_position(\'center\')\n        ax.spines[\'top\'].set_color(\'none\')\n        ax.xaxis.set_ticks_position(\'bottom\')\n        ax.yaxis.set_ticks_position(\'left\')\n\n\ndef plot_relative_scaling():\n    # make synthetic data\n    X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n    # split it into training and test set\n    X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n    # plot the training and test set\n    fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n    axes[0].scatter(X_train[:, 0], X_train[:, 1],\n                    c=\'b\', label=""training set"", s=60, alpha=0.25)\n    axes[0].scatter(X_test[:, 0], X_test[:, 1], marker=\'x\',\n                    c=\'r\', label=""test set"", s=60)\n    axes[0].legend(loc=\'upper left\')\n    axes[0].set_title(""original data"")\n\n    # scale the data using MinMaxScaler\n    scaler = MinMaxScaler()\n    scaler.fit(X_train)\n    X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # visualize the properly scaled data\n    axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                    c=\'b\', label=""training set"", s=60, alpha=0.25)\n    axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker=\'x\',\n                    c=\'r\', label=""test set"", s=60)\n    axes[1].set_title(""scaled data"")\n\n    # rescale the test set separately, so that test set min is 0 and test set max is 1\n    # DO NOT DO THIS! For illustration purposes only\n    test_scaler = MinMaxScaler()\n    test_scaler.fit(X_test)\n    X_test_scaled_badly = test_scaler.transform(X_test)\n\n    # visualize wrongly scaled data\n    axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                    c=\'b\', label=""training set"", s=60, alpha=0.25)\n    axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1], marker=\'x\',\n                    c=\'r\', label=""test set"", s=60)\n    axes[2].set_title(""improperly scaled data"")\n'"
MLiP-week05/solutions/07A_iris-pca.py,0,"b""from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\niris = load_iris()\n\nX_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                    iris.target,\n                                                    random_state=0,\n                                                    stratify=iris.target)\n\nsc = StandardScaler()\nsc.fit(X_train)\npca = PCA(n_components=2)\n\nX_train_pca = pca.fit_transform(sc.transform(X_train))\nX_test_pca = pca.transform(sc.transform(X_test))\n\nfor X, y in zip((X_train_pca, X_test_pca), (y_train, y_test)):\n\n    for i, annot in enumerate(zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                                  ('blue', 'red', 'green'))):\n        plt.scatter(X[y==i, 0],\n                    X[y==i, 1],\n                    label=annot[0],\n                    c=annot[1])\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n"""
MLiP-week05/solutions/08B_digits_clustering.py,0,"b'from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=10)\nclusters = kmeans.fit_predict(digits.data)\n\nprint(kmeans.cluster_centers_.shape)\n\n#------------------------------------------------------------\n# visualize the cluster centers\nfig = plt.figure(figsize=(8, 3))\nfor i in range(10):\n    ax = fig.add_subplot(2, 5, 1 + i)\n    ax.imshow(kmeans.cluster_centers_[i].reshape((8, 8)),\n              cmap=plt.cm.binary)\nfrom sklearn.manifold import Isomap\nX_iso = Isomap(n_neighbors=10).fit_transform(digits.data)\n\n#------------------------------------------------------------\n# visualize the projected data\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\n\nax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=clusters)\nax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=digits.target)\n'"
nbs/fig_codes/figures.py,11,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\n\ndef plot_kmeans_interactive(min_clusters=1, max_clusters=6):\n    #from IPython.html.widgets import interact\n    from ipywidgets import interact\n    from sklearn.metrics.pairwise import euclidean_distances\n    from sklearn.datasets.samples_generator import make_blobs\n\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\'ignore\')\n\n        X, y = make_blobs(n_samples=300, centers=4,\n                          random_state=0, cluster_std=0.60)\n\n        def _kmeans_step(frame=0, n_clusters=4):\n            rng = np.random.RandomState(2)\n            labels = np.zeros(X.shape[0])\n            centers = rng.randn(n_clusters, 2)\n\n            nsteps = frame // 3\n\n            for i in range(nsteps + 1):\n                old_centers = centers\n                if i < nsteps or frame % 3 > 0:\n                    dist = euclidean_distances(X, centers)\n                    labels = dist.argmin(1)\n\n                if i < nsteps or frame % 3 > 1:\n                    centers = np.array([X[labels == j].mean(0)\n                                        for j in range(n_clusters)])\n                    nans = np.isnan(centers)\n                    centers[nans] = old_centers[nans]\n\n\n            # plot the data and cluster centers\n            plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap=\'rainbow\',\n                        vmin=0, vmax=n_clusters - 1);\n            plt.scatter(old_centers[:, 0], old_centers[:, 1], marker=\'o\',\n                        c=np.arange(n_clusters),\n                        s=200, cmap=\'rainbow\')\n            plt.scatter(old_centers[:, 0], old_centers[:, 1], marker=\'o\',\n                        c=\'black\', s=50)\n\n            # plot new centers if third frame\n            if frame % 3 == 2:\n                for i in range(n_clusters):\n                    plt.annotate(\'\', centers[i], old_centers[i], \n                                 arrowprops=dict(arrowstyle=\'->\', linewidth=1))\n                plt.scatter(centers[:, 0], centers[:, 1], marker=\'o\',\n                            c=np.arange(n_clusters),\n                            s=200, cmap=\'rainbow\')\n                plt.scatter(centers[:, 0], centers[:, 1], marker=\'o\',\n                            c=\'black\', s=50)\n\n            plt.xlim(-4, 4)\n            plt.ylim(-2, 10)\n\n            if frame % 3 == 1:\n                plt.text(3.8, 9.5, ""1. Reassign points to nearest centroid"",\n                         ha=\'right\', va=\'top\', size=14)\n            elif frame % 3 == 2:\n                plt.text(3.8, 9.5, ""2. Update centroids to cluster means"",\n                         ha=\'right\', va=\'top\', size=14)\n\n    \n    return interact(_kmeans_step, frame=np.arange(0, 50),\n                    n_clusters=np.arange(min_clusters, max_clusters))\n\n\ndef plot_image_components(x, coefficients=None, mean=0, components=None,\n                          imshape=(8, 8), n_components=6, fontsize=12):\n    if coefficients is None:\n        coefficients = x\n        \n    if components is None:\n        components = np.eye(len(coefficients), len(x))\n        \n    mean = np.zeros_like(x) + mean\n        \n\n    fig = plt.figure(figsize=(1.2 * (5 + n_components), 1.2 * 2))\n    g = plt.GridSpec(2, 5 + n_components, hspace=0.3)\n\n    def show(i, j, x, title=None):\n        ax = fig.add_subplot(g[i, j], xticks=[], yticks=[])\n        ax.imshow(x.reshape(imshape), interpolation=\'nearest\')\n        if title:\n            ax.set_title(title, fontsize=fontsize)\n\n    show(slice(2), slice(2), x, ""True"")\n\n    approx = mean.copy()\n    show(0, 2, np.zeros_like(x) + mean, r\'$\\mu$\')\n    show(1, 2, approx, r\'$1 \\cdot \\mu$\')\n\n    for i in range(0, n_components):\n        approx = approx + coefficients[i] * components[i]\n        show(0, i + 3, components[i], r\'$c_{0}$\'.format(i + 1))\n        show(1, i + 3, approx,\n             r""${0:.2f} \\cdot c_{1}$"".format(coefficients[i], i + 1))\n        plt.gca().text(0, 1.05, \'$+$\', ha=\'right\', va=\'bottom\',\n                       transform=plt.gca().transAxes, fontsize=fontsize)\n\n    show(slice(2), slice(-2, None), approx, ""Approx"")\n\n\ndef plot_pca_interactive(data, n_components=6):\n    from sklearn.decomposition import PCA\n    #from IPython.html.widgets import interact\n    from ipywidgets import interact\n\n    pca = PCA(n_components=n_components)\n    Xproj = pca.fit_transform(data)\n\n    def show_decomp(i=0):\n        plot_image_components(data[i], Xproj[i],\n                              pca.mean_, pca.components_)\n    \n    interact(show_decomp, i=(0, data.shape[0] - 1));\n'"
nbs/fig_codes/plot_2d_separator.py,4,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_2d_separator(classifier, X, fill=False, ax=None, eps=None):\n    if eps is None:\n        eps = X.std() / 2.\n    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps\n    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps\n    xx = np.linspace(x_min, x_max, 100)\n    yy = np.linspace(y_min, y_max, 100)\n\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    try:\n        decision_values = classifier.decision_function(X_grid)\n        levels = [0]\n        fill_levels = [decision_values.min(), 0, decision_values.max()]\n    except AttributeError:\n        # no decision_function\n        decision_values = classifier.predict_proba(X_grid)[:, 1]\n        levels = [.5]\n        fill_levels = [0, .5, 1]\n\n    if ax is None:\n        ax = plt.gca()\n    if fill:\n        ax.contourf(X1, X2, decision_values.reshape(X1.shape),\n                    levels=fill_levels, colors=[\'blue\', \'red\'])\n    else:\n        ax.contour(X1, X2, decision_values.reshape(X1.shape), levels=levels,\n                   colors=""black"")\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n\nif __name__ == \'__main__\':\n    from sklearn.datasets import make_blobs\n    from sklearn.linear_model import LogisticRegression\n    X, y = make_blobs(centers=2, random_state=42)\n    clf = LogisticRegression().fit(X, y)\n    plot_2d_separator(clf, X, fill=True)\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.show()\n'"
nbs/fig_codes/plot_digits_dataset.py,6,"b'# Taken from example in scikit-learn examples\n# Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2011\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nfrom sklearn import datasets, decomposition\n\n\ndef digits_plot():\n    digits = datasets.load_digits(n_class=6)\n    n_digits = 500\n    X = digits.data[:n_digits]\n    y = digits.target[:n_digits]\n    n_samples, n_features = X.shape\n\n    def plot_embedding(X, title=None):\n        x_min, x_max = np.min(X, 0), np.max(X, 0)\n        X = (X - x_min) / (x_max - x_min)\n\n        plt.figure()\n        ax = plt.subplot(111)\n        for i in range(X.shape[0]):\n            plt.text(X[i, 0], X[i, 1], str(digits.target[i]),\n                     color=plt.cm.Set1(y[i] / 10.),\n                     fontdict={\'weight\': \'bold\', \'size\': 9})\n\n        if hasattr(offsetbox, \'AnnotationBbox\'):\n            # only print thumbnails with matplotlib > 1.0\n            shown_images = np.array([[1., 1.]])  # just something big\n            for i in range(X.shape[0]):\n                dist = np.sum((X[i] - shown_images) ** 2, 1)\n                if np.min(dist) < 1e5:\n                    # don\'t show points that are too close\n                    # set a high threshold to basically turn this off\n                    continue\n                shown_images = np.r_[shown_images, [X[i]]]\n                imagebox = offsetbox.AnnotationBbox(\n                    offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n                    X[i])\n                ax.add_artist(imagebox)\n        plt.xticks([]), plt.yticks([])\n        if title is not None:\n            plt.title(title)\n\n    n_img_per_row = 10\n    img = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\n    for i in range(n_img_per_row):\n        ix = 10 * i + 1\n        for j in range(n_img_per_row):\n            iy = 10 * j + 1\n            img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n\n    plt.imshow(img, cmap=plt.cm.binary)\n    plt.xticks([])\n    plt.yticks([])\n    plt.title(\'A selection from the 64-dimensional digits dataset\')\n    print(""Computing PCA projection"")\n    pca = decomposition.PCA(n_components=2).fit(X)\n    X_pca = pca.transform(X)\n    plot_embedding(X_pca, ""Principal Components projection of the digits"")\n    plt.figure()\n    plt.title(""First Principal Component"")\n    plt.matshow(pca.components_[0, :].reshape(8, 8), cmap=""gray"")\n    plt.axis(\'off\')\n    plt.figure()\n    plt.title(""Second Principal Component"")\n    plt.matshow(pca.components_[1, :].reshape(8, 8), cmap=""gray"")\n    plt.axis(\'off\')\n    plt.show()\n'"
nbs/fig_codes/plot_helpers.py,0,"b""from matplotlib.colors import ListedColormap\n\ncm3 = ListedColormap(['#0000aa', '#ff2020', '#50ff50'])\ncm2 = ListedColormap(['#0000aa', '#ff2020'])\n"""
nbs/fig_codes/plot_pca.py,5,"b'from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_pca_illustration():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n\n    pca = PCA()\n    pca.fit(X_blob)\n    X_pca = pca.transform(X_blob)\n\n    S = X_pca.std(axis=0)\n\n    fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n    axes = axes.ravel()\n\n    axes[0].set_title(""Original data"")\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_pca[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].arrow(pca.mean_[0], pca.mean_[1], S[0] * pca.components_[0, 0],\n                  S[0] * pca.components_[0, 1], width=.1, head_width=.3,\n                  color=\'k\')\n    axes[0].arrow(pca.mean_[0], pca.mean_[1], S[1] * pca.components_[1, 0],\n                  S[1] * pca.components_[1, 1], width=.1, head_width=.3,\n                  color=\'k\')\n    axes[0].text(-1.5, -.5, ""Component 2"", size=14)\n    axes[0].text(-4, -4, ""Component 1"", size=14)\n    axes[0].set_aspect(\'equal\')\n\n    axes[1].set_title(""Transformed data"")\n    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], linewidths=0,\n                    s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""First principal component"")\n    axes[1].set_ylabel(""Second principal component"")\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_ylim(-8, 8)\n\n    pca = PCA(n_components=1)\n    pca.fit(X_blob)\n    X_inverse = pca.inverse_transform(pca.transform(X_blob))\n\n    axes[2].set_title(""Transformed data w/ second component dropped"")\n    axes[2].scatter(X_pca[:, 0], np.zeros(X_pca.shape[0]), c=X_pca[:, 0],\n                    linewidths=0, s=60, cmap=\'viridis\')\n    axes[2].set_xlabel(""First principal component"")\n    axes[2].set_aspect(\'equal\')\n    axes[2].set_ylim(-8, 8)\n\n    axes[3].set_title(""Back-rotation using only first component"")\n    axes[3].scatter(X_inverse[:, 0], X_inverse[:, 1], c=X_pca[:, 0],\n                    linewidths=0, s=60, cmap=\'viridis\')\n    axes[3].set_xlabel(""feature 1"")\n    axes[3].set_ylabel(""feature 2"")\n    axes[3].set_aspect(\'equal\')\n    axes[3].set_xlim(-8, 4)\n    axes[3].set_ylim(-8, 4)\n\n\ndef plot_pca_whitening():\n    rnd = np.random.RandomState(5)\n    X_ = rnd.normal(size=(300, 2))\n    X_blob = np.dot(X_, rnd.normal(size=(2, 2))) + rnd.normal(size=2)\n\n    pca = PCA(whiten=True)\n    pca.fit(X_blob)\n    X_pca = pca.transform(X_blob)\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n    axes = axes.ravel()\n\n    axes[0].set_title(""Original data"")\n    axes[0].scatter(X_blob[:, 0], X_blob[:, 1], c=X_pca[:, 0], linewidths=0, s=60, cmap=\'viridis\')\n    axes[0].set_xlabel(""feature 1"")\n    axes[0].set_ylabel(""feature 2"")\n    axes[0].set_aspect(\'equal\')\n\n    axes[1].set_title(""Whitened data"")\n    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=X_pca[:, 0], linewidths=0, s=60, cmap=\'viridis\')\n    axes[1].set_xlabel(""First principal component"")\n    axes[1].set_ylabel(""Second principal component"")\n    axes[1].set_aspect(\'equal\')\n    axes[1].set_xlim(-3, 4)\n'"
nbs/fig_codes/plot_scaling.py,2,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom .plot_helpers import cm2\n\n\ndef plot_scaling():\n    X, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\n    X += 3\n\n    plt.figure(figsize=(15, 8))\n    main_ax = plt.subplot2grid((2, 4), (0, 0), rowspan=2, colspan=2)\n\n    main_ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm2, s=60)\n    maxx = np.abs(X[:, 0]).max()\n    maxy = np.abs(X[:, 1]).max()\n\n    main_ax.set_xlim(-maxx + 1, maxx + 1)\n    main_ax.set_ylim(-maxy + 1, maxy + 1)\n    main_ax.set_title(""Original Data"")\n    other_axes = [plt.subplot2grid((2, 4), (i, j)) for j in range(2, 4) for i in range(2)]\n\n    for ax, scaler in zip(other_axes, [StandardScaler(), RobustScaler(),\n                                       MinMaxScaler(), Normalizer(norm=\'l2\')]):\n        X_ = scaler.fit_transform(X)\n        ax.scatter(X_[:, 0], X_[:, 1], c=y, cmap=cm2, s=60)\n        ax.set_xlim(-2, 2)\n        ax.set_ylim(-2, 2)\n        ax.set_title(type(scaler).__name__)\n\n    other_axes.append(main_ax)\n\n    for ax in other_axes:\n        ax.spines[\'left\'].set_position(\'center\')\n        ax.spines[\'right\'].set_color(\'none\')\n        ax.spines[\'bottom\'].set_position(\'center\')\n        ax.spines[\'top\'].set_color(\'none\')\n        ax.xaxis.set_ticks_position(\'bottom\')\n        ax.yaxis.set_ticks_position(\'left\')\n\n\ndef plot_relative_scaling():\n    # make synthetic data\n    X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n    # split it into training and test set\n    X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n    # plot the training and test set\n    fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n    axes[0].scatter(X_train[:, 0], X_train[:, 1],\n                    c=\'b\', label=""training set"", s=60, alpha=0.25)\n    axes[0].scatter(X_test[:, 0], X_test[:, 1], marker=\'x\',\n                    c=\'r\', label=""test set"", s=60)\n    axes[0].legend(loc=\'upper left\')\n    axes[0].set_title(""original data"")\n\n    # scale the data using MinMaxScaler\n    scaler = MinMaxScaler()\n    scaler.fit(X_train)\n    X_train_scaled = scaler.transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # visualize the properly scaled data\n    axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                    c=\'b\', label=""training set"", s=60, alpha=0.25)\n    axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker=\'x\',\n                    c=\'r\', label=""test set"", s=60)\n    axes[1].set_title(""scaled data"")\n\n    # rescale the test set separately, so that test set min is 0 and test set max is 1\n    # DO NOT DO THIS! For illustration purposes only\n    test_scaler = MinMaxScaler()\n    test_scaler.fit(X_test)\n    X_test_scaled_badly = test_scaler.transform(X_test)\n\n    # visualize wrongly scaled data\n    axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                    c=\'b\', label=""training set"", s=60, alpha=0.25)\n    axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1], marker=\'x\',\n                    c=\'r\', label=""test set"", s=60)\n    axes[2].set_title(""improperly scaled data"")\n'"
nbs/solutions/03A_faces_plot.py,0,"b""faces = fetch_olivetti_faces()\n\n# set up the figure\nfig = plt.figure(figsize=(6, 6))  # figure size in inches\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\n# plot the faces:\nfor i in range(64):\n    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n    ax.imshow(faces.images[i], cmap=plt.cm.bone, interpolation='nearest')\n"""
nbs/solutions/04_wrong-predictions.py,1,"b'plt.figure(figsize=(10, 6))\n\n\nfor i in incorrect_idx:\n    print(\'%d: Predicted %d | True label %d\' % (i, y_pred[i], y_test[i]))\n\n# Plot two dimensions\n\ncolors = [""darkblue"", ""darkgreen"", ""gray""]\n\nfor n, color in enumerate(colors):\n    idx = np.where(y_test == n)[0]\n    plt.scatter(X_test[idx, 1], X_test[idx, 2], color=color, label=""Class %s"" % str(n))\n\nfor i, marker in zip(incorrect_idx, [\'x\', \'s\', \'v\']):\n    plt.scatter(X_test[i, 1], X_test[i, 2],\n                color=""darkred"",\n                marker=marker,\n                s=60,\n                label=i)\n\nplt.xlabel(\'sepal width [cm]\')\nplt.ylabel(\'petal length [cm]\')\nplt.legend(loc=1, scatterpoints=1)\nplt.title(""Iris Classification results"")\nplt.show()\n'"
nbs/solutions/05A_knn_with_diff_k.py,0,"b""from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\n\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.25,\n                                                    random_state=1234,\n                                                    stratify=y)\n\nX_trainsub, X_valid, y_trainsub, y_valid = train_test_split(X_train, y_train,\n                                                            test_size=0.5,\n                                                            random_state=1234,\n                                                            stratify=y_train)\n\nfor k in range(1, 20):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    train_score = knn.fit(X_trainsub, y_trainsub).\\\n        score(X_trainsub, y_trainsub)\n    valid_score = knn.score(X_valid, y_valid)\n    print('k: %d, Train/Valid Acc: %.3f/%.3f' %\n          (k, train_score, valid_score))\n\n\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train, y_train)\nprint('k=9 Test Acc: %.3f' % knn.score(X_test, y_test))\n"""
nbs/solutions/06A_knn_vs_linreg.py,0,"b""from sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\nboston = load_boston()\nX = boston.data\ny = boston.target\n\nprint('X.shape:', X.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.25,\n                                                    random_state=42)\n\nlinreg = LinearRegression()\nknnreg = KNeighborsRegressor(n_neighbors=1)\n\nlinreg.fit(X_train, y_train)\nprint('Linear Regression Train/Test: %.3f/%.3f' %\n      (linreg.score(X_train, y_train),\n       linreg.score(X_test, y_test)))\n\nknnreg.fit(X_train, y_train)\nprint('KNeighborsRegressor Train/Test: %.3f/%.3f' %\n      (knnreg.score(X_train, y_train),\n       knnreg.score(X_test, y_test)))\n"""
nbs/solutions/06B_lin_with_sine.py,2,"b'XX_train = np.concatenate((X_train, np.sin(4 * X_train)), axis=1)\nXX_test = np.concatenate((X_test, np.sin(4 * X_test)), axis=1)\nregressor.fit(XX_train, y_train)\ny_pred_test_sine = regressor.predict(XX_test)\n\nplt.plot(X_test, y_test, \'o\', label=""data"")\nplt.plot(X_test, y_pred_test_sine, \'o\', label=""prediction with sine"")\nplt.plot(X_test, y_pred_test, label=\'prediction without sine\')\nplt.legend(loc=\'best\');\n'"
nbs/solutions/07A_iris-pca.py,0,"b""from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\niris = load_iris()\n\nX_train, X_test, y_train, y_test = train_test_split(iris.data,\n                                                    iris.target,\n                                                    random_state=0,\n                                                    stratify=iris.target)\n\nsc = StandardScaler()\nsc.fit(X_train)\npca = PCA(n_components=2)\n\nX_train_pca = pca.fit_transform(sc.transform(X_train))\nX_test_pca = pca.transform(sc.transform(X_test))\n\nfor X, y in zip((X_train_pca, X_test_pca), (y_train, y_test)):\n\n    for i, annot in enumerate(zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                                  ('blue', 'red', 'green'))):\n        plt.scatter(X[y==i, 0],\n                    X[y==i, 1],\n                    label=annot[0],\n                    c=annot[1])\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n"""
nbs/solutions/08B_digits_clustering.py,0,"b'from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=10)\nclusters = kmeans.fit_predict(digits.data)\n\nprint(kmeans.cluster_centers_.shape)\n\n#------------------------------------------------------------\n# visualize the cluster centers\nfig = plt.figure(figsize=(8, 3))\nfor i in range(10):\n    ax = fig.add_subplot(2, 5, 1 + i)\n    ax.imshow(kmeans.cluster_centers_[i].reshape((8, 8)),\n              cmap=plt.cm.binary)\nfrom sklearn.manifold import Isomap\nX_iso = Isomap(n_neighbors=10).fit_transform(digits.data)\n\n#------------------------------------------------------------\n# visualize the projected data\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\n\nax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=clusters)\nax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=digits.target)\n'"
nbs/utils/data_utils.py,4,"b'from six.moves import cPickle as pickle\nimport numpy as np\nimport os\nfrom scipy.misc import imread\nimport platform\n\ndef load_pickle(f):\n    version = platform.python_version_tuple()\n    if version[0] == \'2\':\n        return  pickle.load(f)\n    elif version[0] == \'3\':\n        return  pickle.load(f, encoding=\'latin1\')\n    raise ValueError(""invalid python version: {}"".format(version))\n\ndef load_CIFAR_batch(filename):\n  """""" load single batch of cifar """"""\n  with open(filename, \'rb\') as f:\n    datadict = load_pickle(f)\n    X = datadict[\'data\']\n    Y = datadict[\'labels\']\n    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(""float"")\n    Y = np.array(Y)\n    return X, Y\n\ndef load_CIFAR10(ROOT):\n  """""" load all of cifar """"""\n  xs = []\n  ys = []\n  for b in range(1,6):\n    f = os.path.join(ROOT, \'data_batch_%d\' % (b, ))\n    X, Y = load_CIFAR_batch(f)\n    xs.append(X)\n    ys.append(Y)    \n  Xtr = np.concatenate(xs)\n  Ytr = np.concatenate(ys)\n  del X, Y\n  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \'test_batch\'))\n  return Xtr, Ytr, Xte, Yte\n\n\ndef get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n                     subtract_mean=True):\n    """"""\n    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n    it for classifiers. These are the same steps as we used for the SVM, but\n    condensed to a single function.\n    """"""\n    # Load the raw CIFAR-10 data\n    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n        \n    # Subsample the data\n    mask = list(range(num_training, num_training + num_validation))\n    X_val = X_train[mask]\n    y_val = y_train[mask]\n    mask = list(range(num_training))\n    X_train = X_train[mask]\n    y_train = y_train[mask]\n    mask = list(range(num_test))\n    X_test = X_test[mask]\n    y_test = y_test[mask]\n\n    # Normalize the data: subtract the mean image\n    if subtract_mean:\n      mean_image = np.mean(X_train, axis=0)\n      X_train -= mean_image\n      X_val -= mean_image\n      X_test -= mean_image\n    \n    # Transpose so that channels come first\n    X_train = X_train.transpose(0, 3, 1, 2).copy()\n    X_val = X_val.transpose(0, 3, 1, 2).copy()\n    X_test = X_test.transpose(0, 3, 1, 2).copy()\n\n    # Package data into a dictionary\n    return {\n      \'X_train\': X_train, \'y_train\': y_train,\n      \'X_val\': X_val, \'y_val\': y_val,\n      \'X_test\': X_test, \'y_test\': y_test,\n    }\n'"
nbs/utils/layers.py,22,"b'import numpy as np\r\n\r\n\r\ndef affine_forward(x, W, b):\r\n    """"""\r\n    A linear mapping from inputs to scores.\r\n    \r\n    Inputs:\r\n        - x: input matrix (N, d_1, ..., d_k)\r\n        - W: weigh matrix (D, C)\r\n        - b: bias vector  (C, )\r\n    \r\n    Outputs:\r\n        - out: output of linear layer (N, C)\r\n    """"""\r\n    x2d = np.reshape(x, (x.shape[0], -1))  # convert 4D input matrix to 2D    \r\n    out = np.dot(x2d, W) + b               # linear transformation\r\n    cache = (x, W, b)                      # keep for backward step (stay with us)\r\n    return out, cache\r\n\r\n\r\ndef affine_backward(dout, cache):\r\n    """"""\r\n    Computes the backward pass for an affine layer.\r\n\r\n    Inputs:\r\n        - dout: Upstream derivative, of shape (N, C)\r\n        - cache: Tuple of:\r\n            - x: Input data, of shape (N, d_1, ... d_k)\r\n            - w: Weights, of shape (D, C)\r\n            - b: biases, of shape (C,)\r\n\r\n    Outputs:\r\n        - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\r\n        - dw: Gradient with respect to w, of shape (D, C)\r\n        - db: Gradient with respect to b, of shape (C,)\r\n    """"""\r\n    x, w, b = cache\r\n    x2d = np.reshape(x, (x.shape[0], -1))\r\n\r\n    # compute gradients\r\n    db = np.sum(dout, axis=0)\r\n    dw = np.dot(x2d.T, dout)\r\n    dx = np.dot(dout, w.T)\r\n\r\n    # reshape dx to match the size of x\r\n    dx = dx.reshape(x.shape)\r\n    \r\n    return dx, dw, db\r\n\r\ndef relu_forward(x):\r\n    """"""Forward pass for a layer of rectified linear units.\r\n\r\n    Inputs:\r\n        - x: a numpy array of any shape\r\n\r\n    Outputs:\r\n        - out: output of relu, same shape as x\r\n        - cache: x\r\n    """"""\r\n    cache = x\r\n    out = np.maximum(0, x)\r\n    return out, cache\r\n\r\ndef relu_backward(dout, cache):\r\n    """"""Backward pass for a layer of rectified linear units.\r\n\r\n    Inputs:\r\n        - dout: upstream derevatives, of any shape\r\n        - cache: x, same shape as dout\r\n\r\n    Outputs:\r\n        - dx: gradient of loss w.r.t x\r\n    """"""\r\n    x = cache\r\n    dx = dout * (x > 0)\r\n    return dx\r\n\r\ndef svm_loss_naive(scores, y):\r\n    """"""\r\n    Naive implementation of SVM loss function.\r\n\r\n    Inputs:\r\n        - scores: scores for all training data (N, C)\r\n        - y: correct labels for the training data\r\n\r\n    Outputs:\r\n       - loss: data loss plus L2 regularization loss\r\n       - grads: graidents of loss w.r.t. scores\r\n    """"""\r\n\r\n    N, C = scores.shape\r\n\r\n    # Compute svm data loss\r\n    loss = 0.0\r\n    for i in range(N):\r\n        s = scores[i]  # scores for the ith data\r\n        correct_class = y[i]  # correct class score\r\n\r\n        for j in range(C):\r\n            if j == y[i]:\r\n                continue\r\n            else:\r\n                # loss += max(0, s[j] - s[correct_class] + 1.0)\r\n                margin = s[j] - s[correct_class] + 1.0\r\n                if margin > 0:\r\n                    loss += margin\r\n    loss /= N\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    # We will write this part later\r\n    grads = {} \r\n\r\n    return loss, grads\r\n\r\ndef svm_loss_half_vectorized(scores, y):\r\n    """"""\r\n    Half-vectorized implementation of SVM loss function.\r\n\r\n    Inputs:\r\n        - scores: scores for all training data (N, C)\r\n        - y: correct labels for the training data\r\n\r\n    Outputs:\r\n       - loss: data loss plus L2 regularization loss\r\n       - grads: graidents of loss wrt scores\r\n    """"""\r\n\r\n    N, C = scores.shape\r\n\r\n    # Compute svm data loss\r\n    loss = 0.0\r\n    for i in range(N):\r\n        s = scores[i]  # scores for the ith data\r\n        correct_class = y[i]  # correct class score\r\n\r\n        margins = np.maximum(0.0, s - s[correct_class] + 1.0)\r\n        margins[correct_class] = 0.0\r\n        loss += np.sum(margins)\r\n\r\n    loss /= N\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    # We will write this part later\r\n    grads = {} \r\n\r\n    return loss, grads\r\n\r\ndef svm_loss(scores, y):\r\n    """"""\r\n    Fully-vectorized implementation of SVM loss function.\r\n\r\n    Inputs:\r\n        - scores: scores for all training data (N, C)\r\n        - y: correct labels for the training data of shape (N,)\r\n\r\n    Outputs:\r\n       - loss: data loss plus L2 regularization loss\r\n       - grads: graidents of loss w.r.t scores\r\n    """"""\r\n\r\n    N = scores.shape[0]\r\n\r\n    # Compute svm data loss\r\n    correct_class_scores = scores[range(N), y]\r\n    margins = np.maximum(0.0, scores - correct_class_scores[:, None] + 1.0)\r\n    margins[range(N), y] = 0.0\r\n    loss = np.sum(margins) / N\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    num_pos = np.sum(margins > 0, axis=1)\r\n    dscores = np.zeros(scores.shape)\r\n    dscores[margins > 0] = 1\r\n    dscores[range(N), y] -= num_pos\r\n    dscores /= N\r\n\r\n    return loss, dscores\r\n\r\n\r\ndef softmax_loss_naive(scores, y):\r\n    """"""\r\n    Softmax loss function, naive implementation (with loops)\r\n\r\n    Inputs have dimension D, there are C classes, and we operate on minibatches\r\n    of N examples.\r\n\r\n    Inputs:\r\n        - scores: A numpy array of shape (N, C).\r\n        - y: A numpy array of shape (N,) containing training labels;\r\n\r\n    Outputs:\r\n        - loss: as single float\r\n        - grads: gradient with respect to weights W; an array of same shape as W\r\n    """"""\r\n    N, C = scores.shape\r\n\r\n    # compute data loss\r\n    loss = 0.0\r\n    for i in range(N):\r\n        correct_class = y[i]\r\n        score = scores[i]\r\n        score -= np.max(scores)\r\n        exp_score = np.exp(score)\r\n        probs = exp_score / np.sum(exp_score)\r\n        loss += -np.log(probs[correct_class])\r\n\r\n    loss /= N\r\n\r\n    # Compute gradient off loss function w.r.t. scores\r\n    # We will write this part later\r\n    grads = {}  \r\n\r\n    return loss, grads\r\n\r\n\r\ndef softmax_loss(scores, y):\r\n    """"""\r\n    Softmax loss function, fully vectorized implementation.\r\n\r\n    Inputs have dimension D, there are C classes, and we operate on minibatches\r\n    of N examples.\r\n\r\n    Inputs:\r\n        - scores: A numpy array of shape (N, C).\r\n        - y: A numpy array of shape (N,) containing training labels;\r\n\r\n    Outputs:\r\n        - loss as single float\r\n        - gradient with respect to scores\r\n    """"""\r\n    N = scores.shape[0]  # number of input data\r\n\r\n    # compute data loss\r\n    shifted_logits = scores - np.max(scores, axis=1, keepdims=True)\r\n    Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\r\n    log_probs = shifted_logits - np.log(Z)\r\n    probs = np.exp(log_probs)\r\n    loss = -np.sum(log_probs[range(N), y]) / N\r\n\r\n    # Compute gradient of loss function w.r.t. scores\r\n    dscores = probs.copy()\r\n    dscores[range(N), y] -= 1\r\n    dscores /= N\r\n    \r\n    return loss, dscores        \r\n'"
