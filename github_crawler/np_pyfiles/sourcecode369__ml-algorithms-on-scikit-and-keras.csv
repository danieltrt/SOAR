file_path,api_count,code
Part 1 - Data Preprocessing/data_preprocessing.py,0,"b'#Data Preprocessing\r\n\r\n#Import the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv(\'Data.csv\')\r\nX = dataset.iloc[:,:-1].values\r\nY = dataset.iloc[:,3].values\r\n\r\n\r\n#Handling Missing Data\r\nfrom sklearn.preprocessing import Imputer\r\nimputer = Imputer(missing_values=""NaN"",strategy = \'mean\',axis=0)\r\nimputer = imputer.fit(X[:,1:3])\r\nX[:,1:3] = imputer.transform(X[:,1:3])\r\n\r\n#Encoding Categorical Data\r\nfrom sklearn.preprocessing import LabelEncoder\r\nfrom sklearn.preprocessing import OneHotEncoder\r\nlabelencoder_X = LabelEncoder()\r\nX[:,0]=labelencoder_X.fit_transform(X[:,0])\r\nonehotencoder = OneHotEncoder(categorical_features = [0])\r\nX = onehotencoder.fit_transform(X).toarray()\r\nlabelencoder_Y = LabelEncoder()\r\nY=labelencoder_Y.fit_transform(Y)\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)'"
Part 10 - Model Selection & Boosting/1. Model Selection/grid_search.py,8,"b""# Grid Search\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Social_Network_Ads.csv')\nX = dataset.iloc[:, [2, 3]].values\ny = dataset.iloc[:, 4].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting Kernel SVM to the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\naccuracies.std()\n\n# Applying Grid Search to find the best model and the best parameters\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Kernel SVM (Training set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('Kernel SVM (Test set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()"""
Part 10 - Model Selection & Boosting/1. Model Selection/k_fold_cross_validations.py,8,"b""#Model Selection - 1. K-Fold Cross Validation (Evaluating the model performance)\r\n\r\n#Data Preprocessing\r\n\r\n#Importing the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:,[2,3]].values\r\nY = dataset.iloc[:,4].values\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)\r\n\r\n#Fitting Kernel_SVM Classification Model\r\nfrom sklearn.svm import SVC\r\nclassifier = SVC(kernel = 'rbf',random_state = 0)\r\nclassifier.fit(X_train,Y_train)\r\n\r\n#Applying K-Fold Cross Validation\r\nfrom sklearn.model_selection import cross_val_score\r\naccuracy = cross_val_score(estimator=classifier, X = X_train, y = Y_train, cv=10)\r\naccuracy.mean()\r\naccuracy.std()\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(Y_test,y_pred)\r\n\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, Y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Kernel_SVM Classification (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, Y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Kernel_SVM Classification (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n"""
Part 10 - Model Selection & Boosting/2. XGBoost/xgboost.py,0,"b""# XGBoost\n\n# Install xgboost following the instructions on this link: http://xgboost.readthedocs.io/en/latest/build.html#\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Churn_Modelling.csv')\nX = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Fitting XGBoost to the Training set\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()\naccuracies.std()"""
Part 2 - Regression/1. Simple Linear Regression/simple_lnear_regression.py,0,"b""#SimpleLinearRegressions\r\n\r\n#Import the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Salary_Data.csv')\r\nX = dataset.iloc[:,:-1].values\r\nY = dataset.iloc[:,1].values\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=1/3,random_state=0)\r\n\r\n'''#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)'''\r\n\r\n#Fitting Simple Linear Regression Model\r\nfrom sklearn.linear_model import LinearRegression\r\nregressor = LinearRegression()\r\nregressor.fit(X_train,Y_train)\r\n\r\n#Predicting the Test Set Results\r\nY_pred = regressor.predict(X_test)\r\n\r\n#Visualizing the Training Set Results\r\nplt.scatter(X_train,Y_train,color ='red')\r\nplt.plot(X_train,regressor.predict(X_train),color='blue')\r\nplt.title('SALARY VS EXPERIENCE(Traing Set)')\r\nplt.xlabel('Years of Experience')\r\nplt.xlabel('Salary')\r\nplt.show()\r\nplt.scatter(X_test,Y_test,color ='red')\r\nplt.plot(X_train,regressor.predict(X_train),color='blue')\r\nplt.title('SALARY VS EXPERIENCE(Test Set)')\r\nplt.xlabel('Years of Experience')\r\nplt.xlabel('Salary')\r\nplt.show()"""
Part 2 - Regression/2. Multiple Linear Regression/multi_linear_regressions.py,1,"b""#Multiple Linear Regressions\r\n\r\n#Data Preprocessing\r\n\r\n#importing the libraries\r\nimport numpy as np \r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n#importing the dataset\r\ndataset = pd.read_csv('50_Startups.csv')\r\nX = dataset.iloc[:,:-1].values\r\nY = dataset.iloc[:,4].values\r\n\r\n#encoding categorical variable\r\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\r\nlabelencoder_X = LabelEncoder()\r\nX[:,3] = labelencoder_X.fit_transform(X[:,3])\r\nonehotencoder = OneHotEncoder(categorical_features = [3])\r\nX = onehotencoder.fit_transform(X).toarray()\r\n\r\n#avoiding dummy variable trap\r\nX = X[:,1:]\r\n\r\n#splitting the dataset into traing set and test set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=111)\r\n\r\n#fitting the multiple linear regressions\r\nfrom sklearn.linear_model import LinearRegression\r\nregressor = LinearRegression()\r\nregressor.fit(X,Y)\r\n\r\n#predicting the values\r\ny_pred = regressor.predict(X_test)\r\n\r\n#Building optimal model using Backward Elimination\r\nimport statsmodels.formula.api as sm\r\nX = np.append(arr=np.ones((50,1)).astype(int),values = X,axis = 1)\r\nX_opt = X[:,[0,1,2,3,4,5]]\r\nregressor_OLS = sm.OLS(endog=Y,exog=X_opt).fit()\r\nregressor_OLS.summary()\r\nX_opt = X[:,[0,1,3,4,5]]\r\nregressor_OLS = sm.OLS(endog=Y,exog=X_opt).fit()\r\nregressor_OLS.summary()\r\nX_opt = X[:,[0,3,4,5]]\r\nregressor_OLS = sm.OLS(endog=Y,exog=X_opt).fit()\r\nregressor_OLS.summary()\r\nX_opt = X[:,[0,3,5]]\r\nregressor_OLS = sm.OLS(endog=Y,exog=X_opt).fit()\r\nregressor_OLS.summary()\r\nX_opt = X[:,[0,3]]\r\nregressor_OLS = sm.OLS(endog=Y,exog=X_opt).fit()\r\nregressor_OLS.summary()\r\n\r\n"""
Part 2 - Regression/3. Polynomial Regression/polynomial _regressions.py,1,"b""#Polynomial regressions\r\n\r\n#Data Preprocessing\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Position_Salaries.csv')\r\nX = dataset.iloc[:,1:2].values\r\nY = dataset.iloc[:,2].values\r\n\r\n'''#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)'''\r\n\r\n'''#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)'''\r\n\r\n#Fitting Linear Regression to dataset\r\nfrom sklearn.linear_model import LinearRegression\r\nlin_reg = LinearRegression()\r\nlin_reg.fit(X,Y)\r\n\r\n#Fitting Polynomial Regression to dataset\r\nfrom sklearn.preprocessing import PolynomialFeatures\r\npoly_reg = PolynomialFeatures(degree=4)\r\nX_poly = poly_reg.fit_transform(X)\r\nlin_reg2 = LinearRegression()\r\nlin_reg2.fit(X_poly,Y)\r\n\r\n#Visualizing the Linear Regression Results\r\nplt.scatter(X,Y,color='red')\r\nplt.plot(X,lin_reg.predict(X),color='blue')\r\nplt.title('Linear Regression Prediction(Truth or Bluff)')\r\nplt.xlabel('Position level')\r\nplt.ylabel('Salary')\r\n\r\n#Visualizing the Polynomial Regression Results\r\nX_grid = np.arange(min(X),max(X),0.1)\r\nX_grid = X_grid.reshape(len(X_grid),1)\r\nplt.scatter(X,Y,color='red')\r\nplt.plot(X_grid,lin_reg2.predict(poly_reg.fit_transform(X_grid)),color='blue')\r\nplt.title('Polynomial Regression Prediction(Truth or Bluff)')\r\nplt.xlabel('Position level')\r\nplt.ylabel('Salary')\r\n\r\n#Visualizing the Polynomial Regression Results\r\nplt.scatter(X,Y,color='red')\r\nplt.plot(X,lin_reg2.predict(poly_reg.fit_transform(X)),color='blue')\r\nplt.title('Polynomial Regression Prediction(Truth or Bluff)')\r\nplt.xlabel('Position level')\r\nplt.ylabel('Salary')\r\n\r\n#Predicting a new result using Linear Regression\r\nlin_pred = lin_reg.predict(6.5)\r\n#Predicting a new result using Polynomial Regression\r\npoly_pred = lin_reg2.predict(poly_reg.fit_transform(6.5))"""
Part 2 - Regression/4. Support Vector Regression (SVR)/svrs.py,2,"b""#Support vector regression\r\n#Regression Template\r\n\r\n#SVR regressions\r\n\r\n#Data Preprocessing\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Position_Salaries.csv')\r\nX = dataset.iloc[:,1:2].values\r\nY = dataset.iloc[:,2:3].values\r\n\r\n'''#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)'''\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nsc_Y = StandardScaler()\r\nX = sc_X.fit_transform(X)\r\nY = sc_Y.fit_transform(Y)\r\n\r\n#Fitting the SVR Regression to dataset\r\nfrom sklearn.svm import SVR\r\nregressor = SVR(kernel = 'rbf')\r\nregressor.fit(X,Y)\r\n\r\n#Predicting a new result using SVR Regression\r\nY_pred = sc_Y.inverse_transform(regressor.predict(sc_X.transform(np.array([[6.5]]))))\r\n#Visualizing the SVR Regression Results\r\nplt.scatter(X,Y,color='red')\r\nplt.plot(X,regressor.predict(X),color='blue')\r\nplt.title('SVR Regression Prediction')\r\nplt.xlabel('Position')\r\nplt.ylabel('Salary')\r\nplt.show()\r\n#Visualzing XYZ regression at a higher resolution\r\nX_grid = np.arange(min(X),max(X),0.1)\r\nX_grid = X_grid.reshape(len(X_grid),1)\r\nplt.scatter(X,Y,color='red')\r\nplt.plot(X_grid,regressor.predict(X_grid),color='blue')\r\nplt.title('SVR Regression Prediction')\r\nplt.xlabel('Position')\r\nplt.ylabel('Salary')\r\nplt.show()"""
Part 2 - Regression/5. Decision Tree Regression/Decision_Tree_Regressions.py,1,"b""#Decision_Tree Regression Template\r\n\r\n#Decision_Tree regressions\r\n\r\n#Data Preprocessing\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n#Importing DataSets\r\ndataset = pd.read_csv('Position_Salaries.csv')\r\nX = dataset.iloc[:,1:2].values\r\nY = dataset.iloc[:,2].values\r\n\r\n'''#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)'''\r\n\r\n'''#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)'''\r\n\r\n#Fitting the Decision_Tree Regression to dataset\r\nfrom sklearn.tree import DecisionTreeRegressor\r\nregressor = DecisionTreeRegressor(random_state=0)\r\nregressor.fit(X,Y)\r\n\r\n#Predicting a new result using Decision_Tree Regression\r\nY_pred = regressor.predict(9.5)\r\n\r\n#Visualzing Decision_Tree regression at a higher resolution\r\nX_grid = np.arange(min(X),max(X),0.01)\r\nX_grid = X_grid.reshape(len(X_grid),1)\r\nplt.scatter(X,Y,color='red')\r\nplt.plot(X_grid,regressor.predict(X_grid),color='blue')\r\nplt.title('Decision_Tree Regression Prediction')\r\nplt.xlabel('Position')\r\nplt.ylabel('Salary')\r\nplt.show()\r\n\r\n#Visualizing the Decision_Tree Regression Results\r\nplt.scatter(X,Y,color='red')\r\nplt.plot(X,regressor.predict(X),color='blue')\r\nplt.title('Decision_Tree Regression Prediction')\r\nplt.xlabel('')\r\nplt.ylabel('')\r\nplt.show()\r\n"""
Part 2 - Regression/6. Random Forest Regression/random_forest_regressions.py,1,"b""#Random_Forest regressions\r\n\r\n#Data Preprocessing\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Position_Salaries.csv')\r\nX = dataset.iloc[:,1:2].values\r\nY = dataset.iloc[:,2].values\r\n\r\n'''#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.20,random_state=0)'''\r\n\r\n'''#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)'''\r\n\r\n#Fitting the Random_Forest Regression to dataset\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nregressor = RandomForestRegressor(n_estimators=1000,random_state=0)\r\nregressor.fit(X,Y)\r\n\r\n#Predicting a new result using Random_Forest Regression\r\nY_pred = regressor.predict(6.5)\r\n\r\n\r\n#Visualzing Random_Forest regression at a higher resolution\r\nX_grid = np.arange(min(X),max(X),0.01)\r\nX_grid = X_grid.reshape(len(X_grid),1)\r\nplt.scatter(X,Y,color='red')\r\nplt.plot(X_grid,regressor.predict(X_grid),color='blue')\r\nplt.title('Random_Forest Regression Prediction')\r\nplt.xlabel('Psoition')\r\nplt.ylabel('Salaries')\r\nplt.show()"""
Part 3 - Classification/1.  Logistic Regression/logistic_regressions.py,8,"b""#Logistic Regression\r\n\r\n#Data Preprocessing\r\n\r\n#Importing the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:,[2,3]].values\r\nY = dataset.iloc[:,4].values\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)\r\n\r\n#Fitting Logistic Regression Model\r\nfrom sklearn.linear_model import LogisticRegression\r\nclassifier = LogisticRegression(random_state = 0)\r\nclassifier.fit(X_train,Y_train)\r\n\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(Y_test,y_pred)\r\n\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, Y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.001),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.001))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Logistic Regression (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, Y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Logistic Regression (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()"""
Part 3 - Classification/2. K-Nearest Neighbors (K-NN)/knns.py,8,"b""#K-NN (K-Nearest Neighbour)\r\n#knn Classification \r\n\r\n#Data Preprocessing\r\n\r\n#Importing the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:,[2,3]].values\r\nY = dataset.iloc[:,4].values\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)\r\n\r\n#Fitting knn Classification Model\r\nfrom sklearn.neighbors import KNeighborsClassifier\r\nclassifier = KNeighborsClassifier(n_neighbors=10,p=2,metric='minkowski')\r\nclassifier.fit(X_train,Y_train)\r\n\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(Y_test,y_pred)\r\n\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, Y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('knn Classification (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, Y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('knn Classification (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n"""
Part 3 - Classification/3. Support Vector Machine (SVM)/svms.py,8,"b""#Support vector machines (SVM)\r\n#SVM Classification \r\n\r\n#Data Preprocessing\r\n\r\n#Importing the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:,[2,3]].values\r\nY = dataset.iloc[:,4].values\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)\r\n\r\n#Fitting SVM Classification Model\r\nfrom sklearn.svm import SVC\r\nclassifier = SVC(kernel = 'linear',random_state = 0)\r\nclassifier.fit(X_train,Y_train)\r\n\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(Y_test,y_pred)\r\n\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, Y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('SVM Classification (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, Y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('SVM Classification (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n"""
Part 3 - Classification/4. Kernel SVM/kernel_svms.py,8,"b""#Kernel SVM (Support Vector Machine)\r\n#Kernel_SVM Classification \r\n\r\n#Data Preprocessing\r\n\r\n#Importing the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:,[2,3]].values\r\nY = dataset.iloc[:,4].values\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)\r\n\r\n#Fitting Kernel_SVM Classification Model\r\nfrom sklearn.svm import SVC\r\nclassifier = SVC(kernel = 'rbf',random_state = 0)\r\nclassifier.fit(X_train,Y_train)\r\n\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(Y_test,y_pred)\r\n\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, Y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Kernel_SVM Classification (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, Y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Kernel_SVM Classification (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n"""
Part 3 - Classification/5. Naive Bayes/naives_bayes.py,8,"b""#Naive Bayes\r\n#Naive_Bayes Classification \r\n\r\n#Data Preprocessing\r\n\r\n#Importing the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:,[2,3]].values\r\nY = dataset.iloc[:,4].values\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)\r\n\r\n#Fitting Naive_Bayes Classification Model\r\nfrom sklearn.naive_bayes import GaussianNB\r\nclassifier = GaussianNB()\r\nclassifier.fit(X_train,Y_train)\r\n\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(Y_test,y_pred)\r\n\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, Y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Naive_Bayes Classification (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, Y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'blue')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'blue'))(i), label = j)\r\nplt.title('Naive_Bayes Classification (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\nacc=(65+25)/(100)"""
Part 3 - Classification/6. Decision Tree Classification/decision_tree_classifications.py,8,"b""#Decision Tree Classification \r\n\r\n#Data Preprocessing\r\n\r\n#Importing the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:,[2,3]].values\r\nY = dataset.iloc[:,4].values\r\n\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)\r\n\r\n#Fitting Decision_Tree Classification Model\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nclassifier = DecisionTreeClassifier(criterion = 'entropy',random_state=0)\r\nclassifier.fit(X_train,Y_train)\r\n\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(Y_test,y_pred)\r\n\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, Y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Decision_Tree Classification (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, Y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Decision_Tree Classification (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n"""
Part 3 - Classification/7. Random Forest Classification/Random_Forest_Classifications.py,8,"b""#Random Forest Classification \r\n\r\n#Data Preprocessing\r\n\r\n#Importing the Libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n#Importing DataSets\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:,[2,3]].values\r\nY = dataset.iloc[:,4].values\r\ndataset.sample(10)\r\ndataset.describe()\r\n#Splitting Training and Test Set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25,random_state=0)\r\n\r\n#Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc_X = StandardScaler()\r\nX_train = sc_X.fit_transform(X_train)\r\nX_test = sc_X.transform(X_test)\r\n\r\n#Fitting Random_Forest Classification Model\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nclassifier = RandomForestClassifier(n_estimators = 10,criterion='entropy',random_state=0)\r\nclassifier.fit(X_test,Y_test)\r\n\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(Y_test,y_pred)\r\n\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, Y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Random_Forest Classification (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, Y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Random_Forest Classification (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()"""
Part 4 - Clustering/1. K-Means Clustering/kmean.py,0,"b'#Importing the Libraries\r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n#Importing the dataset\r\ndataset = pd.read_csv(""Mall_Customers.csv"")\r\nX= dataset.iloc[:,[3,4]].values\r\n\r\n#Using Elbow Method to find the optimal number of K(clusters)\r\nfrom sklearn.cluster import KMeans\r\nwcss = []\r\nfor i in range(1,11):\r\n    kmeans = KMeans(n_clusters=i,init=\'k-means++\',max_iter=300,n_init=10,random_state=0)\r\n    kmeans.fit(X)\r\n    wcss.append(kmeans.inertia_)\r\nplt.plot(range(1,11),wcss)\r\nplt.title(\'The elbow method\')\r\nplt.xlabel(\'Number of clusters\')\r\nplt.ylabel(\'WCSS\')\r\nplt.show()\r\n \r\n#Applying K-Means to dataset\r\nkmeans = KMeans(n_clusters=5,init=\'k-means++\',max_iter=300,n_init=10,random_state=0)\r\ny_kmeans = kmeans.fit_predict(X)\r\n\r\n#Visualizing the Clusters\r\nplt.scatter(X[y_kmeans== 0,0],X[y_kmeans==0,1],s=100,c=\'red\',label=\'Cluster 1\')\r\nplt.scatter(X[y_kmeans== 1,0],X[y_kmeans==1,1],s=100,c=\'blue\',label=\'Cluster 2\')\r\nplt.scatter(X[y_kmeans== 2,0],X[y_kmeans==2,1],s=100,c=\'green\',label=\'Cluster 3\')\r\nplt.scatter(X[y_kmeans== 3,0],X[y_kmeans==3,1],s=100,c=\'cyan\',label=\'Cluster 4\')\r\nplt.scatter(X[y_kmeans== 4,0],X[y_kmeans==4,1],s=100,c=\'magenta\',label=\'Cluster 5\')\r\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c=\'yellow\',label=\'centroids\')\r\nplt.title(\'Cluster of clients\')\r\nplt.xlabel(\'Anual Income\')\r\nplt.ylabel(\'Spending score (1-100)\')\r\nplt.legend()\r\nplt.show()'"
Part 5 - Association Rule Learning/1. Apriori/apriori.py,0,"b""# Apriori\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Data Preprocessing\ndataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None)\ntransactions = []\nfor i in range(0, 7501):\n    transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])\n\n# Training Apriori on the dataset\nfrom apyori import apriori\nrules = apriori(transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2)\n\n# Visualising the results\nresults = list(rules)"""
Part 5 - Association Rule Learning/1. Apriori/apyori.py,0,"b'#!/usr/bin/env python\n\n""""""\na simple implementation of Apriori algorithm by Python.\n""""""\n\nimport sys\nimport csv\nimport argparse\nimport json\nimport os\nfrom collections import namedtuple\nfrom itertools import combinations\nfrom itertools import chain\n\n\n# Meta informations.\n__version__ = \'1.1.1\'\n__author__ = \'Yu Mochizuki\'\n__author_email__ = \'ymoch.dev@gmail.com\'\n\n\n################################################################################\n# Data structures.\n################################################################################\nclass TransactionManager(object):\n    """"""\n    Transaction managers.\n    """"""\n\n    def __init__(self, transactions):\n        """"""\n        Initialize.\n\n        Arguments:\n            transactions -- A transaction iterable object\n                            (eg. [[\'A\', \'B\'], [\'B\', \'C\']]).\n        """"""\n        self.__num_transaction = 0\n        self.__items = []\n        self.__transaction_index_map = {}\n\n        for transaction in transactions:\n            self.add_transaction(transaction)\n\n    def add_transaction(self, transaction):\n        """"""\n        Add a transaction.\n\n        Arguments:\n            transaction -- A transaction as an iterable object (eg. [\'A\', \'B\']).\n        """"""\n        for item in transaction:\n            if item not in self.__transaction_index_map:\n                self.__items.append(item)\n                self.__transaction_index_map[item] = set()\n            self.__transaction_index_map[item].add(self.__num_transaction)\n        self.__num_transaction += 1\n\n    def calc_support(self, items):\n        """"""\n        Returns a support for items.\n\n        Arguments:\n            items -- Items as an iterable object (eg. [\'A\', \'B\']).\n        """"""\n        # Empty items is supported by all transactions.\n        if not items:\n            return 1.0\n\n        # Empty transactions supports no items.\n        if not self.num_transaction:\n            return 0.0\n\n        # Create the transaction index intersection.\n        sum_indexes = None\n        for item in items:\n            indexes = self.__transaction_index_map.get(item)\n            if indexes is None:\n                # No support for any set that contains a not existing item.\n                return 0.0\n\n            if sum_indexes is None:\n                # Assign the indexes on the first time.\n                sum_indexes = indexes\n            else:\n                # Calculate the intersection on not the first time.\n                sum_indexes = sum_indexes.intersection(indexes)\n\n        # Calculate and return the support.\n        return float(len(sum_indexes)) / self.__num_transaction\n\n    def initial_candidates(self):\n        """"""\n        Returns the initial candidates.\n        """"""\n        return [frozenset([item]) for item in self.items]\n\n    @property\n    def num_transaction(self):\n        """"""\n        Returns the number of transactions.\n        """"""\n        return self.__num_transaction\n\n    @property\n    def items(self):\n        """"""\n        Returns the item list that the transaction is consisted of.\n        """"""\n        return sorted(self.__items)\n\n    @staticmethod\n    def create(transactions):\n        """"""\n        Create the TransactionManager with a transaction instance.\n        If the given instance is a TransactionManager, this returns itself.\n        """"""\n        if isinstance(transactions, TransactionManager):\n            return transactions\n        return TransactionManager(transactions)\n\n\n# Ignore name errors because these names are namedtuples.\nSupportRecord = namedtuple( # pylint: disable=C0103\n    \'SupportRecord\', (\'items\', \'support\'))\nRelationRecord = namedtuple( # pylint: disable=C0103\n    \'RelationRecord\', SupportRecord._fields + (\'ordered_statistics\',))\nOrderedStatistic = namedtuple( # pylint: disable=C0103\n    \'OrderedStatistic\', (\'items_base\', \'items_add\', \'confidence\', \'lift\',))\n\n\n################################################################################\n# Inner functions.\n################################################################################\ndef create_next_candidates(prev_candidates, length):\n    """"""\n    Returns the apriori candidates as a list.\n\n    Arguments:\n        prev_candidates -- Previous candidates as a list.\n        length -- The lengths of the next candidates.\n    """"""\n    # Solve the items.\n    item_set = set()\n    for candidate in prev_candidates:\n        for item in candidate:\n            item_set.add(item)\n    items = sorted(item_set)\n\n    # Create the temporary candidates. These will be filtered below.\n    tmp_next_candidates = (frozenset(x) for x in combinations(items, length))\n\n    # Return all the candidates if the length of the next candidates is 2\n    # because their subsets are the same as items.\n    if length < 3:\n        return list(tmp_next_candidates)\n\n    # Filter candidates that all of their subsets are\n    # in the previous candidates.\n    next_candidates = [\n        candidate for candidate in tmp_next_candidates\n        if all(\n            True if frozenset(x) in prev_candidates else False\n            for x in combinations(candidate, length - 1))\n    ]\n    return next_candidates\n\n\ndef gen_support_records(transaction_manager, min_support, **kwargs):\n    """"""\n    Returns a generator of support records with given transactions.\n\n    Arguments:\n        transaction_manager -- Transactions as a TransactionManager instance.\n        min_support -- A minimum support (float).\n\n    Keyword arguments:\n        max_length -- The maximum length of relations (integer).\n    """"""\n    # Parse arguments.\n    max_length = kwargs.get(\'max_length\')\n\n    # For testing.\n    _create_next_candidates = kwargs.get(\n        \'_create_next_candidates\', create_next_candidates)\n\n    # Process.\n    candidates = transaction_manager.initial_candidates()\n    length = 1\n    while candidates:\n        relations = set()\n        for relation_candidate in candidates:\n            support = transaction_manager.calc_support(relation_candidate)\n            if support < min_support:\n                continue\n            candidate_set = frozenset(relation_candidate)\n            relations.add(candidate_set)\n            yield SupportRecord(candidate_set, support)\n        length += 1\n        if max_length and length > max_length:\n            break\n        candidates = _create_next_candidates(relations, length)\n\n\ndef gen_ordered_statistics(transaction_manager, record):\n    """"""\n    Returns a generator of ordered statistics as OrderedStatistic instances.\n\n    Arguments:\n        transaction_manager -- Transactions as a TransactionManager instance.\n        record -- A support record as a SupportRecord instance.\n    """"""\n    items = record.items\n    for combination_set in combinations(sorted(items), len(items) - 1):\n        items_base = frozenset(combination_set)\n        items_add = frozenset(items.difference(items_base))\n        confidence = (\n            record.support / transaction_manager.calc_support(items_base))\n        lift = confidence / transaction_manager.calc_support(items_add)\n        yield OrderedStatistic(\n            frozenset(items_base), frozenset(items_add), confidence, lift)\n\n\ndef filter_ordered_statistics(ordered_statistics, **kwargs):\n    """"""\n    Filter OrderedStatistic objects.\n\n    Arguments:\n        ordered_statistics -- A OrderedStatistic iterable object.\n\n    Keyword arguments:\n        min_confidence -- The minimum confidence of relations (float).\n        min_lift -- The minimum lift of relations (float).\n    """"""\n    min_confidence = kwargs.get(\'min_confidence\', 0.0)\n    min_lift = kwargs.get(\'min_lift\', 0.0)\n\n    for ordered_statistic in ordered_statistics:\n        if ordered_statistic.confidence < min_confidence:\n            continue\n        if ordered_statistic.lift < min_lift:\n            continue\n        yield ordered_statistic\n\n\n################################################################################\n# API function.\n################################################################################\ndef apriori(transactions, **kwargs):\n    """"""\n    Executes Apriori algorithm and returns a RelationRecord generator.\n\n    Arguments:\n        transactions -- A transaction iterable object\n                        (eg. [[\'A\', \'B\'], [\'B\', \'C\']]).\n\n    Keyword arguments:\n        min_support -- The minimum support of relations (float).\n        min_confidence -- The minimum confidence of relations (float).\n        min_lift -- The minimum lift of relations (float).\n        max_length -- The maximum length of the relation (integer).\n    """"""\n    # Parse the arguments.\n    min_support = kwargs.get(\'min_support\', 0.1)\n    min_confidence = kwargs.get(\'min_confidence\', 0.0)\n    min_lift = kwargs.get(\'min_lift\', 0.0)\n    max_length = kwargs.get(\'max_length\', None)\n\n    # Check arguments.\n    if min_support <= 0:\n        raise ValueError(\'minimum support must be > 0\')\n\n    # For testing.\n    _gen_support_records = kwargs.get(\n        \'_gen_support_records\', gen_support_records)\n    _gen_ordered_statistics = kwargs.get(\n        \'_gen_ordered_statistics\', gen_ordered_statistics)\n    _filter_ordered_statistics = kwargs.get(\n        \'_filter_ordered_statistics\', filter_ordered_statistics)\n\n    # Calculate supports.\n    transaction_manager = TransactionManager.create(transactions)\n    support_records = _gen_support_records(\n        transaction_manager, min_support, max_length=max_length)\n\n    # Calculate ordered stats.\n    for support_record in support_records:\n        ordered_statistics = list(\n            _filter_ordered_statistics(\n                _gen_ordered_statistics(transaction_manager, support_record),\n                min_confidence=min_confidence,\n                min_lift=min_lift,\n            )\n        )\n        if not ordered_statistics:\n            continue\n        yield RelationRecord(\n            support_record.items, support_record.support, ordered_statistics)\n\n\n################################################################################\n# Application functions.\n################################################################################\ndef parse_args(argv):\n    """"""\n    Parse commandline arguments.\n\n    Arguments:\n        argv -- An argument list without the program name.\n    """"""\n    output_funcs = {\n        \'json\': dump_as_json,\n        \'tsv\': dump_as_two_item_tsv,\n    }\n    default_output_func_key = \'json\'\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'-v\', \'--version\', action=\'version\',\n        version=\'%(prog)s {0}\'.format(__version__))\n    parser.add_argument(\n        \'input\', metavar=\'inpath\', nargs=\'*\',\n        help=\'Input transaction file (default: stdin).\',\n        type=argparse.FileType(\'r\'), default=[sys.stdin])\n    parser.add_argument(\n        \'-o\', \'--output\', metavar=\'outpath\',\n        help=\'Output file (default: stdout).\',\n        type=argparse.FileType(\'w\'), default=sys.stdout)\n    parser.add_argument(\n        \'-l\', \'--max-length\', metavar=\'int\',\n        help=\'Max length of relations (default: infinite).\',\n        type=int, default=None)\n    parser.add_argument(\n        \'-s\', \'--min-support\', metavar=\'float\',\n        help=\'Minimum support ratio (must be > 0, default: 0.1).\',\n        type=float, default=0.1)\n    parser.add_argument(\n        \'-c\', \'--min-confidence\', metavar=\'float\',\n        help=\'Minimum confidence (default: 0.5).\',\n        type=float, default=0.5)\n    parser.add_argument(\n        \'-t\', \'--min-lift\', metavar=\'float\',\n        help=\'Minimum lift (default: 0.0).\',\n        type=float, default=0.0)\n    parser.add_argument(\n        \'-d\', \'--delimiter\', metavar=\'str\',\n        help=\'Delimiter for items of transactions (default: tab).\',\n        type=str, default=\'\\t\')\n    parser.add_argument(\n        \'-f\', \'--out-format\', metavar=\'str\',\n        help=\'Output format ({0}; default: {1}).\'.format(\n            \', \'.join(output_funcs.keys()), default_output_func_key),\n        type=str, choices=output_funcs.keys(), default=default_output_func_key)\n    args = parser.parse_args(argv)\n\n    args.output_func = output_funcs[args.out_format]\n    return args\n\n\ndef load_transactions(input_file, **kwargs):\n    """"""\n    Load transactions and returns a generator for transactions.\n\n    Arguments:\n        input_file -- An input file.\n\n    Keyword arguments:\n        delimiter -- The delimiter of the transaction.\n    """"""\n    delimiter = kwargs.get(\'delimiter\', \'\\t\')\n    for transaction in csv.reader(input_file, delimiter=delimiter):\n        yield transaction if transaction else [\'\']\n\n\ndef dump_as_json(record, output_file):\n    """"""\n    Dump an relation record as a json value.\n\n    Arguments:\n        record -- A RelationRecord instance to dump.\n        output_file -- A file to output.\n    """"""\n    def default_func(value):\n        """"""\n        Default conversion for JSON value.\n        """"""\n        if isinstance(value, frozenset):\n            return sorted(value)\n        raise TypeError(repr(value) + "" is not JSON serializable"")\n\n    converted_record = record._replace(\n        ordered_statistics=[x._asdict() for x in record.ordered_statistics])\n    json.dump(\n        converted_record._asdict(), output_file,\n        default=default_func, ensure_ascii=False)\n    output_file.write(os.linesep)\n\n\ndef dump_as_two_item_tsv(record, output_file):\n    """"""\n    Dump a relation record as TSV only for 2 item relations.\n\n    Arguments:\n        record -- A RelationRecord instance to dump.\n        output_file -- A file to output.\n    """"""\n    for ordered_stats in record.ordered_statistics:\n        if len(ordered_stats.items_base) != 1:\n            continue\n        if len(ordered_stats.items_add) != 1:\n            continue\n        output_file.write(\'{0}\\t{1}\\t{2:.8f}\\t{3:.8f}\\t{4:.8f}{5}\'.format(\n            list(ordered_stats.items_base)[0], list(ordered_stats.items_add)[0],\n            record.support, ordered_stats.confidence, ordered_stats.lift,\n            os.linesep))\n\n\ndef main(**kwargs):\n    """"""\n    Executes Apriori algorithm and print its result.\n    """"""\n    # For tests.\n    _parse_args = kwargs.get(\'_parse_args\', parse_args)\n    _load_transactions = kwargs.get(\'_load_transactions\', load_transactions)\n    _apriori = kwargs.get(\'_apriori\', apriori)\n\n    args = _parse_args(sys.argv[1:])\n    transactions = _load_transactions(\n        chain(*args.input), delimiter=args.delimiter)\n    result = _apriori(\n        transactions,\n        max_length=args.max_length,\n        min_support=args.min_support,\n        min_confidence=args.min_confidence)\n    for record in result:\n        args.output_func(record, args.output)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
Part 6 - Reinforcement Learning/1. Upper Confidence Bound (UCB)/random_selection.py,0,"b""# Random Selection\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Ads_CTR_Optimisation.csv')\n\n# Implementing Random Selection\nimport random\nN = 10000\nd = 10\nads_selected = []\ntotal_reward = 0\nfor n in range(0, N):\n    ad = random.randrange(d)\n    ads_selected.append(ad)\n    reward = dataset.values[n, ad]\n    total_reward = total_reward + reward\n\n# Visualising the results\nplt.hist(ads_selected)\nplt.title('Histogram of ads selections')\nplt.xlabel('Ads')\nplt.ylabel('Number of times each ad was selected')\nplt.show()"""
Part 6 - Reinforcement Learning/1. Upper Confidence Bound (UCB)/upper_confidence_bound.py,0,"b""# Upper Confidence Bound\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Ads_CTR_Optimisation.csv')\n\n# Implementing UCB\nimport math\nN = 10000\nd = 10\nads_selected = []\nnumbers_of_selections = [0] * d\nsums_of_rewards = [0] * d\ntotal_reward = 0\nfor n in range(0, N):\n    ad = 0\n    max_upper_bound = 0\n    for i in range(0, d):\n        if (numbers_of_selections[i] > 0):\n            average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n            delta_i = math.sqrt(3/2 * math.log(n + 1) / numbers_of_selections[i])\n            upper_bound = average_reward + delta_i\n        else:\n            upper_bound = 1e400\n        if upper_bound > max_upper_bound:\n            max_upper_bound = upper_bound\n            ad = i\n    ads_selected.append(ad)\n    numbers_of_selections[ad] = numbers_of_selections[ad] + 1\n    reward = dataset.values[n, ad]\n    sums_of_rewards[ad] = sums_of_rewards[ad] + reward\n    total_reward = total_reward + reward\n\n# Visualising the results\nplt.hist(ads_selected)\nplt.title('Histogram of ads selections')\nplt.xlabel('Ads')\nplt.ylabel('Number of times each ad was selected')\nplt.show()"""
Part 6 - Reinforcement Learning/2. Thompson Sampling/random_selection.py,0,"b""# Random Selection\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Ads_CTR_Optimisation.csv')\n\n# Implementing Random Selection\nimport random\nN = 10000\nd = 10\nads_selected = []\ntotal_reward = 0\nfor n in range(0, N):\n    ad = random.randrange(d)\n    ads_selected.append(ad)\n    reward = dataset.values[n, ad]\n    total_reward = total_reward + reward\n\n# Visualising the results\nplt.hist(ads_selected)\nplt.title('Histogram of ads selections')\nplt.xlabel('Ads')\nplt.ylabel('Number of times each ad was selected')\nplt.show()"""
Part 6 - Reinforcement Learning/2. Thompson Sampling/thompson_sampling.py,0,"b""# Thompson Sampling\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Ads_CTR_Optimisation.csv')\n\n# Implementing Thompson Sampling\nimport random\nN = 10000\nd = 10\nads_selected = []\nnumbers_of_rewards_1 = [0] * d\nnumbers_of_rewards_0 = [0] * d\ntotal_reward = 0\nfor n in range(0, N):\n    ad = 0\n    max_random = 0\n    for i in range(0, d):\n        random_beta = random.betavariate(numbers_of_rewards_1[i] + 1, numbers_of_rewards_0[i] + 1)\n        if random_beta > max_random:\n            max_random = random_beta\n            ad = i\n    ads_selected.append(ad)\n    reward = dataset.values[n, ad]\n    if reward == 1:\n        numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1\n    else:\n        numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1\n    total_reward = total_reward + reward\n\n# Visualising the results - Histogram\nplt.hist(ads_selected)\nplt.title('Histogram of ads selections')\nplt.xlabel('Ads')\nplt.ylabel('Number of times each ad was selected')\nplt.show()"""
Part 7 - Natural Language Processing/Sentiment Analysis/sentiment_analysis.py,0,"b""# Natural Language Processing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n\n# Cleaning the texts\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 1000):\n    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)\n\n# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = dataset.iloc[:, 1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)"""
Part 8 - Deep Learning/1. Artificial Neural Networks (ANN)/anns.py,0,"b""# Artificial Neural Network\r\n\r\n# Installing Theano\r\n# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\r\n\r\n# Installing Tensorflow\r\n# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\r\n\r\n# Installing Keras\r\n# pip install --upgrade keras\r\n\r\n# Part 1 - Data Preprocessing\r\n\r\n# Importing the libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n# Importing the dataset\r\ndataset = pd.read_csv('Churn_Modelling.csv')\r\nX = dataset.iloc[:, 3:13].values\r\ny = dataset.iloc[:, 13].values\r\n\r\n# Encoding categorical data\r\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\r\nlabelencoder_X_1 = LabelEncoder()\r\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\r\nlabelencoder_X_2 = LabelEncoder()\r\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\r\nonehotencoder = OneHotEncoder(categorical_features = [1])\r\nX = onehotencoder.fit_transform(X).toarray()\r\nX = X[:, 1:]\r\n\r\n# Splitting the dataset into the Training set and Test set\r\nfrom sklearn.model_selection import train_test_split\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\r\n\r\n# Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc = StandardScaler()\r\nX_train = sc.fit_transform(X_train)\r\nX_test = sc.transform(X_test)\r\n\r\n# Part 2 - Building the Artificial Neural Network\r\nimport keras\r\nfrom keras.models import Sequential \r\nfrom keras.layers import Dense\r\n\r\n#Initializing the ANN\r\nclassifier = Sequential()\r\n\r\n#Building the Input Layer and First Hidden Layer\r\nclassifier.add(Dense(output_dim=6,init = 'uniform',activation='relu',input_dim=11))\r\n\r\n#Building the Second Hidden Layer\r\nclassifier.add(Dense(output_dim=6,init = 'uniform',activation='relu'))\r\n\r\n#Building the Third Hidden layer\r\nclassifier.add(Dense(output_dim=6,init = 'uniform',activation='relu'))\r\n\r\n#Adding the Output Layer\r\nclassifier.add(Dense(output_dim=1,init = 'uniform',activation='sigmoid'))\r\n\r\n#Compiling the ANN\r\nclassifier.compile(optimizer = 'adam',loss='binary_crossentropy',metrics=['accuracy'])\r\n\r\n\r\n#Part 3 - Predicting the Results\r\nclassifier.fit(X_train,y_train,batch_size=10,epochs=100)\r\n\r\n#Predicting the test set results\r\ny_pred = classifier.predict(X_test)\r\ny_pred = (y_pred>0.5)\r\n\r\n# Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(y_test, y_pred)\r\n\r\n#Accuracy of the Artificial Neural Network Model\r\nacc = (1528+196)/2000"""
Part 8 - Deep Learning/2. Convolutional Neural Networks (CNN)/convolution_neural_network.py,1,"b""#Convolution Neural Network\r\n\r\n#Part - 1 : Building the CNN\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense\r\nfrom keras.layers import Convolution2D\r\nfrom keras.layers import MaxPooling2D\r\nfrom keras.layers import Flatten\r\nfrom keras.preprocessing.image import ImageDataGenerator\r\n\r\n #Initializing the CNN\r\nclassifier = Sequential()\r\n\r\n#Step - 1 Convolution Operation\r\nclassifier.add(Convolution2D(32, (3, 3), input_shape = (64,64,3),activation = 'relu'))\r\n#Step - 2 Pooling\r\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\r\n\r\n'''#Adding a second convolution layer for better performance\r\nclassifier.add(Convolution2D(32, (3, 3), activation = 'relu'))\r\nclassifier.add(MaxPooling2D(pool_size=(2,2)))'''\r\n\r\n#Step - 3 Flattening\r\nclassifier.add(Flatten()) \r\n\r\n#Step - 4 Full Connection\r\nclassifier.add(Dense(units = 128, activation = 'relu'))\r\nclassifier.add(Dense(units = 1, activation = 'sigmoid'))\r\n\r\n#Compiling the ANN\r\nclassifier.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\r\n\r\n#Part - 2 : Fitting the CNN to the images (Image Preprocessing to prevent Overfitting)\r\n'''Keras Documentation -> Preprocessing -> Image Preprocessing -> flow_fromdirectory'''\r\n\r\ntrain_datagen = ImageDataGenerator(\r\n        rescale=1./255,\r\n        shear_range=0.2,\r\n        zoom_range=0.2,\r\n        horizontal_flip=True)\r\ntest_datagen = ImageDataGenerator(rescale=1./255)\r\ntraining_set = train_datagen.flow_from_directory('dataset/training_set',\r\n                                                  target_size=(64, 64),\r\n                                                  batch_size=32,\r\n                                                  class_mode='binary')\r\ntest_set = test_datagen.flow_from_directory('dataset/test_set',\r\n                                         target_size=(64, 64),\r\n                                         batch_size=32,\r\n                                         class_mode='binary')\r\nclassifier.fit_generator(training_set,\r\n                         steps_per_epoch=8000,\r\n                         epochs=2,\r\n                         validation_data=test_set,\r\n                         validation_steps=2000)\r\n'''\r\nAfter second epoch:\r\n    Epoch 2/25\r\n8000/8000 [==============================] - 3079s 385ms/step - \r\nloss: 0.2054 - \r\nacc: 0.9163 - Training Set accuracy\r\nval_loss: 0.8122 - \r\nval_acc: 0.7795 - Test Set Accuracy\r\n'''\r\n#Part - 3 - Making a new prediction\r\nprediction = ' '\r\nimport numpy as np\r\nfrom keras.preprocessing import image\r\ntest_image = image.load_img('dataset\\single_prediction\\cat_or_dog_1.jpg',target_size=(64,64))\r\ntest_image = image.img_to_array(test_image)\r\ntest_image = np.expand_dims(test_image,axis=0)\r\nresult = classifier.predict(test_image)\r\ntraining_set.class_indices\r\nif result[0][0]==1:\r\n    prediction = 'Dog'\r\nelse:\r\n    prediction = 'Cat'\r\n"""
Part 9 - Dimensionality Reduction/1. Principal Component Analysis (PCA)/pca.py,8,"b""# PCA\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Wine.csv')\nX = dataset.iloc[:, 0:13].values\ny = dataset.iloc[:, 13].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Applying PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\n\n# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\n# Visualising the Training set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('Logistic Regression (Training set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()\n\n# Visualising the Test set results\nfrom matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('Logistic Regression (Test set)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend()\nplt.show()"""
Part 9 - Dimensionality Reduction/2. Linear Discriminant Analysis (LDA)/ldas.py,8,"b""#LDA - Linear Discriminant Analysis\r\n\r\n#Data Preprocessing\r\n# Importing the libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n# Importing the dataset\r\ndataset = pd.read_csv('Wine.csv')\r\nX = dataset.iloc[:,0:13].values\r\ny = dataset.iloc[:, 13].values\r\n\r\n# Splitting the dataset into the Training set and Test set\r\nfrom sklearn.model_selection import train_test_split\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\r\n\r\n# Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc = StandardScaler()\r\nX_train = sc.fit_transform(X_train)\r\nX_test = sc.transform(X_test)\r\n\r\n#Applying Principle Component Analysis(PCA)\r\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\r\nlda = LinearDiscriminantAnalysis(n_components = 2)\r\nX_train = lda.fit_transform(X_train,y_train)\r\nX_test = lda.transform(X_test)\r\n\r\n# Fitting Logistic Regression to the Training set\r\nfrom sklearn.linear_model import LogisticRegression\r\nclassifier = LogisticRegression(random_state = 0)\r\nclassifier.fit(X_train, y_train)\r\n\r\n# Predicting the Test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n# Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(y_test, y_pred)\r\nacc=35/36\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green','blue')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green','blue'))(i), label = j)\r\nplt.title('Logistic Regression (Training set)')\r\nplt.xlabel('LD1')\r\nplt.ylabel('LD2')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green','blue')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green','blue'))(i), label = j)\r\nplt.title('Logistic Regression (Test set)')\r\nplt.xlabel('LD1')\r\nplt.ylabel('LD2')\r\nplt.legend()\r\nplt.show()"""
Part 9 - Dimensionality Reduction/3. Kernel PCA/kernel_pcas.py,8,"b""#Kernal PCA(Principle Component Analysis)\r\n\r\n# Importing the libraries\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n# Importing the dataset\r\ndataset = pd.read_csv('Social_Network_Ads.csv')\r\nX = dataset.iloc[:, [2, 3]].values\r\ny = dataset.iloc[:, 4].values\r\n\r\n# Splitting the dataset into the Training set and Test set\r\nfrom sklearn.cross_validation import train_test_split\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\r\n\r\n# Feature Scaling\r\nfrom sklearn.preprocessing import StandardScaler\r\nsc = StandardScaler()\r\nX_train = sc.fit_transform(X_train)\r\nX_test = sc.transform(X_test)\r\n\r\n#Applying Kernel PCA\r\nfrom sklearn.decomposition import KernelPCA\r\nkpca = KernelPCA(n_components = 2,kernel ='rbf')\r\nX_train = kpca.fit_transform(X_train)\r\nX_test = kpca.transform(X_test)\r\n# Fitting Logistic Regression to the Training set\r\nfrom sklearn.linear_model import LogisticRegression\r\nclassifier = LogisticRegression(random_state = 0)\r\nclassifier.fit(X_train, y_train)\r\n\r\n# Predicting the Test set results\r\ny_pred = classifier.predict(X_test)\r\n\r\n# Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(y_test, y_pred)\r\nacc = (64+26)/(64+26+4+6)\r\n# Visualising the Training set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_train, y_train\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Logistic Regression (Training set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()\r\n\r\n# Visualising the Test set results\r\nfrom matplotlib.colors import ListedColormap\r\nX_set, y_set = X_test, y_test\r\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\r\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\r\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\r\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\r\nplt.xlim(X1.min(), X1.max())\r\nplt.ylim(X2.min(), X2.max())\r\nfor i, j in enumerate(np.unique(y_set)):\r\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\r\n                c = ListedColormap(('red', 'green'))(i), label = j)\r\nplt.title('Logistic Regression (Test set)')\r\nplt.xlabel('Age')\r\nplt.ylabel('Estimated Salary')\r\nplt.legend()\r\nplt.show()"""
Part 10 - Model Selection & Boosting/2. XGBoost/Titanic/titanic.py,0,"b""#Model Selection and Gradient Boosting - XGBoost \r\n\r\n#Part - 1 : Data Preprocessing\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\n# Importing the dataset\r\ndataset = pd.read_csv('titanic_data.csv')\r\nX = dataset.iloc[:, 2:3].values\r\ny = dataset.iloc[:, 1:2].values\r\nX = X + dataset.loc[:,5:8]\r\n\r\n# Encoding categorical data\r\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\r\nlabelencoder_X_1 = LabelEncoder()\r\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\r\nlabelencoder_X_2 = LabelEncoder()\r\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\r\nonehotencoder = OneHotEncoder(categorical_features = [1])\r\nX = onehotencoder.fit_transform(X).toarray()\r\nX = X[:, 1:]\r\n\r\n# Splitting the dataset into the Training set and Test set\r\nfrom sklearn.model_selection import train_test_split\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\r\n\r\n#Fitting XGBoost on the training set\r\nfrom xgboost import XGBClassifier\r\nclassifier = XGBClassifier()\r\nclassifier.fit(X_train,y_train)\r\ny_pred = classifier.predict(X_test)\r\n\r\n#Making the Confusion Matrix\r\nfrom sklearn.metrics import confusion_matrix\r\ncm = confusion_matrix(y_test,y_pred)\r\n\r\n#Applying K-Fold Cross Validation\r\nfrom sklearn.model_selection import cross_val_score\r\naccuracy = cross_val_score(estimator=classifier, X = X_train, y = y_train, cv=10)\r\naccuracy.mean()\r\naccuracy.std()"""
Part 4 - Clustering/2. Hierarchical Clustering/Hierarchical_Clustering/data_preprocessing_template.py,0,"b'# Data Preprocessing Template\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv(\'Data.csv\')\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, 3].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# Feature Scaling\n""""""from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nsc_y = StandardScaler()\ny_train = sc_y.fit_transform(y_train)""""""'"
Part 4 - Clustering/2. Hierarchical Clustering/Hierarchical_Clustering/hc.py,0,"b'# Hierarchical Clustering\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv(\'Mall_Customers.csv\')\nX = dataset.iloc[:, [3, 4]].values\n# y = dataset.iloc[:, 3].values\n\n# Splitting the dataset into the Training set and Test set\n""""""from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)""""""\n\n# Feature Scaling\n""""""from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nsc_y = StandardScaler()\ny_train = sc_y.fit_transform(y_train)""""""\n\n# Using the dendrogram to find the optimal number of clusters\nimport scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = \'ward\'))\nplt.title(\'Dendrogram\')\nplt.xlabel(\'Customers\')\nplt.ylabel(\'Euclidean distances\')\nplt.show()\n\n# Fitting Hierarchical Clustering to the dataset\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 5, affinity = \'euclidean\', linkage = \'ward\')\ny_hc = hc.fit_predict(X)\n\n# Visualising the clusters\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = \'red\', label = \'Cluster 1\')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = \'blue\', label = \'Cluster 2\')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = \'green\', label = \'Cluster 3\')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = \'cyan\', label = \'Cluster 4\')\nplt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = \'magenta\', label = \'Cluster 5\')\nplt.title(\'Clusters of customers\')\nplt.xlabel(\'Annual Income (k$)\')\nplt.ylabel(\'Spending Score (1-100)\')\nplt.legend()\nplt.show()'"
Part 4 - Clustering/2. Hierarchical Clustering/Hierarchical_Clustering/hcs.py,0,"b""#Heirarchical Clustering\r\n\r\n#Data Preprocessing \r\nimport numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\n\r\n#Importing the dataset\r\ndataset = pd.read_csv('Mall_Customers.csv')\r\nX = dataset.iloc[:,[3,4]].values\r\n\r\n#Using dendograms to find optimal numbeer of clusters\r\nimport scipy.cluster.hierarchy as sch\r\ndendogram = sch.dendrogram(sch.linkage(X,method='ward'))\r\nplt.title('Dendogram')\r\nplt.xlabel('Customers')\r\nplt.ylabel('Euclidean Distance')\r\nplt.show()\r\n\r\n#Fitting Heirarchical clustering to the data\r\nfrom sklearn.cluster import AgglomerativeClustering\r\nhc = AgglomerativeClustering(n_clusters = 5,affinity = 'euclidean',linkage='ward')\r\ny_hc = hc.fit_predict(X)\r\n\r\n#Visualizing the data\r\nplt.scatter(X[y_hc== 0,0],X[y_hc==0,1],s=100,c='red',label='Cluster 1')\r\nplt.scatter(X[y_hc== 1,0],X[y_hc==1,1],s=100,c='blue',label='Cluster 2')\r\nplt.scatter(X[y_hc== 2,0],X[y_hc==2,1],s=100,c='green',label='Cluster 3')\r\nplt.scatter(X[y_hc== 3,0],X[y_hc==3,1],s=100,c='cyan',label='Cluster 4')\r\nplt.scatter(X[y_hc== 4,0],X[y_hc==4,1],s=100,c='magenta',label='Cluster 5')\r\nplt.title('Cluster of clients')\r\nplt.xlabel('Anual Income')\r\nplt.ylabel('Spending score (1-100)')\r\nplt.legend()\r\nplt.show()"""
