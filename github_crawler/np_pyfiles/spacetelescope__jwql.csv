file_path,api_count,code
setup.py,1,"b""import numpy as np\nfrom setuptools import setup\nfrom setuptools import find_packages\n\nVERSION = '0.22.0'\n\nAUTHORS = 'Matthew Bourque, Misty Cracraft, Joe Filippazzo, Bryan Hilbert, '\nAUTHORS += 'Graham Kanarek, Catherine Martlin, Johannes Sahlmann, Ben Sunnquist'\n\nDESCRIPTION = 'The James Webb Space Telescope Quicklook Project'\n\nDEPENDENCY_LINKS = ['git+https://github.com/spacetelescope/jwst#0.13.0']\nREQUIRES = [\n    'asdf>=2.3.3',\n    'astropy>=3.2.1',\n    'astroquery>=0.3.9',\n    'authlib',\n    'bokeh>=1.0,<1.4',\n    'codecov',\n    'django>=2.0',\n    'flake8',\n    'inflection',\n    'ipython',\n    'jinja2',\n    'jsonschema==2.6.0',\n    'jwedb>=0.0.3',\n    'matplotlib',\n    'nodejs',\n    'numpy',\n    'numpydoc',\n    'pandas',\n    'psycopg2',\n    'pysiaf',\n    'pytest',\n    'pytest-cov',\n    'scipy',\n    'sphinx',\n    'sqlalchemy',\n    'stsci_rtd_theme',\n    'twine'\n]\n\nsetup(\n    name='jwql',\n    version=VERSION,\n    description=DESCRIPTION,\n    url='https://github.com/spacetelescope/jwql.git',\n    author=AUTHORS,\n    author_email='jwql@stsci.edu',\n    license='BSD',\n    keywords=['astronomy', 'python'],\n    classifiers=['Programming Language :: Python'],\n    packages=find_packages(),\n    install_requires=REQUIRES,\n    dependency_links=DEPENDENCY_LINKS,\n    include_package_data=True,\n    include_dirs=[np.get_include()],\n)\n"""
jwql/__init__.py,0,"b'import os\nimport pkg_resources\n\nmodule_path = pkg_resources.resource_filename(\'jwql\', \'\')\nsetup_path = os.path.normpath(os.path.join(module_path, \'../setup.py\'))\n\ntry:\n    with open(setup_path) as f:\n        data = f.readlines()\n\n    for line in data:\n        if \'VERSION =\' in line:\n            __version__ = line.split(\' \')[-1].replace(""\'"", """").strip()\n\nexcept FileNotFoundError:\n    print(\'Could not determine jwql version\')\n    __version__ = \'0.0.0\'\n'"
style_guide/example.py,0,"b'#! /usr/bin/env python\n""""""This is the module docstring.\n\nThe module docstring should have a one line description (as above) as\nwell as a more detailed description in a paragraph below the one line\ndescription (i.e. this).  Module dosctring lines should be limited to\n72 characters.  Monospace font can be achived with ``two single\nforward-apostrophes``.  The description should provided a detailed\noverview of the purpose of the module (what does it do) and how it\nachieves this purpose (how does it do it).\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    This module can be executed via the command line as such:\n\n    ::\n        python example.py [path] [-f|--filter filter]\n\n    Required arguments:\n\n    ``path`` - The path to the input file\n\n    Optional arguments:\n\n    ``-f|--filter`` - The filter to process.  if not provided, the\n        defult value is ""F606W"".\n\nDependencies\n------------\n\n    Here is where any external dependencies can be listed or described.\n    For example:\n\n    The user must have a configuration file named ``config.yaml``\n    placed in the current working directory.\n\nReferences\n----------\n\n    Here is where any references to external sources related to the\n    code can be listed or described.  For example:\n\n    Code adopted from IDL routine written by Hilbert et al., 2009.\n\nNotes\n-----\n\n    Here is where any additional notes (that are beyond the scope of the\n    description) can be described.\n""""""\n\nimport argparse\nimport glob\nimport logging\nimport os\nimport sys\nfrom typing import List, Union, Tuple, Optional, Any, Dict\n\nfrom astropy.io import fits\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy\nfrom sqlalchemy import Float, Integer, String\n\nfrom jwql.utils.logging_functions import configure_logging, log_info, log_fail, log_timing\n\n\n# Global variables should be avoided, but if used should be named with\n# all-caps\nA_GLOBAL_VARIABLE = \'foo\' # type: str\n\n\n@log_fail\n@log_info\ndef my_main_function(path: str, filter: str) -> None:\n    """"""The main function of the ``example`` module.\n\n    This function performs the main tasks of the module.  See module\n    docstrings for further details.\n\n    Parameters\n    ----------\n    path : str\n        The path to the input file.\n    filter : str\n        The filter to process (e.g. ""F606W"").\n    """"""\n\n    logging.info(\'Using {} as an input file\'.format(path))\n\n    an_int = 1 # type: int\n    a_float = 3.14 # type: float\n    a_bool = True # type: bool\n    a_list = [\'Dog\', \'Cat\', \'Turtle\', False, 7] # type: List[Union[str, bool, int]]\n    a_tuple = (\'Dog\', \'Cat\', \'Turtle\', False, 7) # type: Tuple[str, str, str, bool, int]\n    a_dict = {\'key1\': \'value1\', \'key2\': \'value2\'} # type: Dict[str, str]\n    an_obj = object() # type: object\n\n    result = some_other_function(an_int, a_float, a_bool, a_list, a_tuple, a_dict, an_obj) # type: Optional[int]\n\n    logging.info(result)\n\n\ndef parse_args() -> argparse.Namespace:\n    """"""Parse command line arguments. Returns ``args`` object.\n\n    Returns\n    -------\n    args : obj\n        An argparse object containing all of the arguments\n    """"""\n\n    # Create help strings\n    path_help = \'The path to the input file.\' # type: str\n    filter_help = \'The filter to process (e.g. ""F606W"").\' # type: str\n\n    # Add arguments\n    parser = argparse.ArgumentParser() # type: argparse.ArgumentParser\n    parser.add_argument(\'path\',\n                        type=str,\n                        default=os.getcwd(),\n                        help=path_help)\n    parser.add_argument(\'-f --filter\',\n                        dest=\'filter\',\n                        type=str,\n                        required=False,\n                        default=\'F606W\',\n                        help=filter_help)\n\n    # Parse args\n    args = parser.parse_args() # type: argparse.Namespace\n\n    return args\n\n\n@log_timing\ndef some_other_function(an_int: int, a_float: float, a_bool: bool, a_list: List[Any],\n                        a_tuple: Tuple[Any], a_dict: Dict[Any, Any], an_obj: object) -> int:\n    """"""This function just does a bunch of nonsense.\n\n    But it serves as a decent example of some things.\n\n    Parameters\n    ----------\n    an_int : int\n        Who knows what we will use this integer for.\n    a_bool : bool\n        Who knows what we will use this boolean for.\n    a_float: float\n        Who knows what we will use this float for.\n    a_list : list\n        Who knows what we will use this list for.\n    a_tuple : tuple\n        Who knows what we will use this tuple for.\n    a_dict : dict\n        Who knows what we will use this dictionary for.\n    an_obj : obj\n        Who knows what we will use this object for.\n\n    Returns\n    -------\n    results : int\n        The result of the function.\n    """"""\n\n    # File I/O should be handeled with \'with open\' when possible\n    with open(\'my_file\', \'w\') as f:\n        f.write(\'My favorite integer is {}\'.format(an_int))\n\n    # Operators should be separated by spaces\n    logging.info(a_float + a_float)\n\n    return an_int\n\n\nif __name__ == \'__main__\':\n\n    # Configure logging\n    module = os.path.basename(__file__).strip(\'.py\')\n    configure_logging(module)\n\n    args = parse_args() # type: argparse.Namespace\n\n    my_main_function(args.path, args.filter)\n'"
docs/source/conf.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# jwql documentation build configuration file, created by\n# sphinx-quickstart on Wed Apr  4 10:30:20 2018.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\nimport stsci_rtd_theme\nimport jwql\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\'sphinx_automodapi.automodapi\',\n              \'sphinx_automodapi.automodsumm\',\n              \'numpydoc\',\n              \'sphinx.ext.autodoc\',\n              \'sphinx.ext.mathjax\',\n              \'sphinx.ext.viewcode\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# Numpy doc setting, right now this supresses some warnings, not exactly sure why?\nnumpydoc_show_class_members = False\n\n# General information about the project.\nproject = \'jwql\'\ncopyright = \'2018, STScI\'\nauthor = \'STScI\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion_parts = jwql.__version__.split(\'.\')\nversion = ""{}.{}"".format(version_parts[0], version_parts[1])\n\n# The full version, including alpha/beta/rc tags.\nrelease = jwql.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# This is a fix for warnings because of sphinx-autodoc interaction for classes, however it removes\n# method table from the docs.\nnumpydoc_show_class_members = False\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'api/*.rst\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'stsci_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\nhtml_theme_path = [stsci_rtd_theme.get_html_theme_path()]\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'jwqldoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'jwql.tex\', \'jwql Documentation\',\n     \'STScI\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'jwql\', \'jwql Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'jwql\', \'jwql Documentation\',\n     author, \'jwql\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n'"
jwql/bokeh_templating/__init__.py,0,b'from .template import BokehTemplate\n'
jwql/bokeh_templating/bokeh_surface.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon May 21 13:45:34 2018\n\n@author: gkanarek\n""""""\n\nfrom bokeh.core.properties import Instance, String, Any, Dict\nfrom bokeh.models import ColumnDataSource, LayoutDOM\n\nDEFAULTS = {\n    \'width\': \'600px\',\n    \'height\': \'600px\',\n    \'style\': \'surface\',\n    \'showPerspective\': True,\n    \'showGrid\': True,\n    \'keepAspectRatio\': True,\n    \'verticalRatio\': 1.0,\n    \'legendLabel\': \'stuff\',\n    \'cameraPosition\': {\n        \'horizontal\': -0.35,\n        \'vertical\': 0.22,\n        \'distance\': 1.8,\n    }\n}\n\nJS_CODE = """"""\n# This file contains the JavaScript (CoffeeScript) implementation\n# for a Bokeh custom extension. The ""surface3d.py"" contains the\n# python counterpart.\n#\n# This custom model wraps one part of the third-party vis.js library:\n#\n#     http://visjs.org/index.html\n#\n# Making it easy to hook up python data analytics tools (NumPy, SciPy,\n# Pandas, etc.) to web presentations using the Bokeh server.\n\n# These ""require"" lines are similar to python ""import"" statements\nimport * as p from ""core/properties""\nimport {LayoutDOM, LayoutDOMView} from ""models/layouts/layout_dom""\n\n# This defines some default options for the Graph3d feature of vis.js\n# See: http://visjs.org/graph3d_examples.html for more details.\nOPTIONS =\n  width:  \'600px\'\n  height: \'600px\'\n  style: \'surface\'\n  showPerspective: true\n  showGrid: true\n  keepAspectRatio: true\n  verticalRatio: 1.0\n  legendLabel: \'stuff\'\n  cameraPosition:\n    horizontal: -0.35\n    vertical: 0.22\n    distance: 1.8\n\n# To create custom model extensions that will render on to the HTML canvas\n# or into the DOM, we must create a View subclass for the model. Currently\n# Bokeh models and views are based on BackBone. More information about\n# using Backbone can be found here:\n#\n#     http://backbonejs.org/\n#\n# In this case we will subclass from the existing BokehJS ``LayoutDOMView``,\n# corresponding to our\nexport class Surface3dView extends LayoutDOMView\n\n  initialize: (options) ->\n    super(options)\n\n    url = ""https://cdnjs.cloudflare.com/ajax/libs/vis/4.16.1/vis.min.js""\n\n    script = document.createElement(\'script\')\n    script.src = url\n    script.async = false\n    script.onreadystatechange = script.onload = () => @_init()\n    document.querySelector(""head"").appendChild(script)\n\n  _init: () ->\n    # Create a new Graph3s using the vis.js API. This assumes the vis.js has\n    # already been loaded (e.g. in a custom app template). In the future Bokeh\n    # models will be able to specify and load external scripts automatically.\n    #\n    # Backbone Views create <div> elements by default, accessible as @el. Many\n    # Bokeh views ignore this default <div>, and instead do things like draw\n    # to the HTML canvas. In this case though, we use the <div> to attach a\n    # Graph3d to the DOM.\n    @_graph = new vis.Graph3d(@el, @get_data(), @model.options)\n\n    # Set Backbone listener so that when the Bokeh data source has a change\n    # event, we can process the new data\n    @listenTo(@model.data_source, \'change\', () =>\n        @_graph.setData(@get_data())\n    )\n\n  # This is the callback executed when the Bokeh data has an change. Its basic\n  # function is to adapt the Bokeh data source to the vis.js DataSet format.\n  get_data: () ->\n    data = new vis.DataSet()\n    source = @model.data_source\n    for i in [0...source.get_length()]\n      data.add({\n        x:     source.get_column(@model.x)[i]\n        y:     source.get_column(@model.y)[i]\n        z:     source.get_column(@model.z)[i]\n      })\n    return data\n\n# We must also create a corresponding JavaScript Backbone model sublcass to\n# correspond to the python Bokeh model subclass. In this case, since we want\n# an element that can position itself in the DOM according to a Bokeh layout,\n# we subclass from ``LayoutDOM``\nexport class Surface3d extends LayoutDOM\n\n  # This is usually boilerplate. In some cases there may not be a view.\n  default_view: Surface3dView\n\n  # The ``type`` class attribute should generally match exactly the name\n  # of the corresponding Python class.\n  type: ""Surface3d""\n\n  # The @define block adds corresponding ""properties"" to the JS model. These\n  # should basically line up 1-1 with the Python model class. Most property\n  # types have counterparts, e.g. ``bokeh.core.properties.String`` will be\n  # ``p.String`` in the JS implementatin. Where the JS type system is not yet\n  # as rich, you can use ``p.Any`` as a ""wildcard"" property type.\n  @define {\n    x:           [ p.String           ]\n    y:           [ p.String           ]\n    z:           [ p.String           ]\n    data_source: [ p.Instance         ]\n    options:     [ p.Any,     OPTIONS ]\n  }\n""""""\n\n\n# This custom extension model will have a DOM view that should layout-able in\n# Bokeh layouts, so use ``LayoutDOM`` as the base class. If you wanted to create\n# a custom tool, you could inherit from ``Tool``, or from ``Glyph`` if you\n# wanted to create a custom glyph, etc.\nclass Surface3d(LayoutDOM):\n\n    # The special class attribute ``__implementation__`` should contain a string\n    # of JavaScript (or CoffeeScript) code that implements the JavaScript side\n    # of the custom extension model.\n    __implementation__ = JS_CODE\n\n    # Below are all the ""properties"" for this model. Bokeh properties are\n    # class attributes that define the fields (and their types) that can be\n    # communicated automatically between Python and the browser. Properties\n    # also support type validation. More information about properties in\n    # can be found here:\n    #\n    #    http://bokeh.pydata.org/en/latest/docs/reference/core.html#bokeh-core-properties\n\n    # This is a Bokeh ColumnDataSource that can be updated in the Bokeh\n    # server by Python code\n    data_source = Instance(ColumnDataSource)\n\n    # The vis.js library that we are wrapping expects data for x, y, z, and\n    # color. The data will actually be stored in the ColumnDataSource, but\n    # these properties let us specify the *name* of the column that should\n    # be used for each field.\n    x = String\n    y = String\n    z = String\n    color = String\n\n    options = Dict(String, Any, default=DEFAULTS)\n'"
jwql/bokeh_templating/factory.py,0,"b'#!/usr/bin/env python3\n""""""\nCreated on Mon Feb 20 14:05:03 2017\n\n@author: gkanarek\n""""""\n\nfrom bokeh.io import curdoc\n\nfrom .keyword_map import bokeh_mappings as mappings, bokeh_sequences as sequences\n\n# Figures get their own constructor\nFigure = mappings.pop(""Figure"")\ndel sequences[""figure""]\n\n\ndef mapping_factory(tool, element_type):\n    def mapping_constructor(loader, node):\n        fmt = tool.formats.get(element_type, {})\n        value = loader.construct_mapping(node, deep=True)\n        ref = value.pop(""ref"", """")\n        callback = value.pop(""on_change"", [])\n        selection_callback = value.pop(""selection_on_change"", [])\n        onclick = value.pop(""on_click"", None)\n        fmt.update(value)\n        # convert the ""range"" YAML keyword of a slider into something Bokeh can read\n        if element_type == ""Slider"":\n            fmt[""start""], fmt[""end""], fmt[""step""] = fmt.pop(""range"", [0, 1, 0.1])\n\n        # Many of these have hybrid signatures, with both positional and\n        # keyword arguments, so we need to convert an ""args"" keyword into\n        # positional arguments\n        arg = fmt.pop(""arg"", None)\n        if arg is not None:\n            obj = mappings[element_type](*arg, **fmt)\n        else:\n            obj = mappings[element_type](**fmt)\n\n        # Store the object in the tool\'s ""refs"" dictionary\n        if ref:\n            tool.refs[ref] = obj\n\n        # Handle callbacks and on_clicks\n        if callback:\n            obj.on_change(*callback)\n        if onclick:\n            obj.on_click(onclick)\n        if selection_callback:\n            obj.selected.on_change(*selection_callback)\n\n        yield obj\n\n    mapping_constructor.__name__ = element_type.lower() + \'_\' + mapping_constructor.__name__\n    return mapping_constructor\n\n\ndef sequence_factory(tool, element_type):\n    def sequence_constructor(loader, node):\n        fmt = tool.formats.get(element_type, {})\n        value = loader.construct_sequence(node, deep=True)\n        obj = sequences[element_type](*value, **fmt)\n        yield obj\n\n    sequence_constructor.__name__ = element_type.lower() + \'_\' + sequence_constructor.__name__\n    return sequence_constructor\n\n\n# These constructors need more specialized treatment\n\ndef document_constructor(tool, loader, node):\n    layout = loader.construct_sequence(node, deep=True)\n    for element in layout:\n        curdoc().add_root(element)\n    tool.document = curdoc()\n    yield tool.document\n\n\ndef figure_constructor(tool, loader, node):\n\n    fig = loader.construct_mapping(node, deep=True)\n    fmt = tool.formats.get(\'Figure\', {})\n\n    elements = fig.pop(\'elements\', [])\n    cmds = []\n    ref = fig.pop(""ref"", """")\n    callback = fig.pop(""on_change"", [])\n    axis = tool.formats.get(""Axis"", {})\n    axis.update(fig.pop(""axis"", {}))\n\n    for key in fig:\n        val = fig[key]\n        if key in [\'text\', \'add_tools\', \'js_on_event\']:\n            cmds.append((key, val))\n        else:\n            fmt[key] = val\n\n    figure = Figure(**fmt)\n\n    for key, cmd in cmds:\n        if key == \'add_tools\':\n            figure.add_tools(*cmd)\n        elif key == \'text\':\n            figure.text(*cmd.pop(\'loc\'), **cmd)\n        elif key == \'js_on_event\':\n            for event in cmd:\n                figure.js_on_event(*event)\n\n    for element in elements:\n        key = element.pop(\'kind\')\n        shape = {\'line\': (\'Line\', figure.line),\n                 \'circle\': (\'Circle\', figure.circle),\n                 \'diamond\': (\'Diamond\', figure.diamond),\n                 \'triangle\': (\'Triangle\', figure.triangle),\n                 \'square\': (\'Square\', figure.square),\n                 \'asterisk\': (\'Asterisk\', figure.asterisk),\n                 \'x\': (\'XGlyph\', figure.x),\n                 \'vbar\': (\'VBar\', figure.vbar)}\n        if key in shape:\n            fmt_key, glyph = shape[key]\n            shape_fmt = tool.formats.get(fmt_key, {})\n            shape_fmt.update(element)\n            x = shape_fmt.pop(\'x\', \'x\')\n            y = shape_fmt.pop(\'y\', \'y\')\n            glyph(x, y, **shape_fmt)\n        elif key == \'rect\':\n            rect_fmt = tool.formats.get(\'Rect\', {})\n            rect_fmt.update(element)\n            figure.rect(\'rx\', \'ry\', \'rw\', \'rh\', **rect_fmt)\n        elif key == \'quad\':\n            quad_fmt = tool.formats.get(\'Quad\', {})\n            quad_fmt.update(element)\n            figure.quad(**quad_fmt)\n        elif key == \'image\':\n            image_fmt = tool.formats.get(\'Image\', {})\n            image_fmt.update(element)\n            arg = image_fmt.pop(""image"", None)\n            figure.image(arg, **image_fmt)\n        elif key == \'image_rgba\':\n            image_fmt = tool.formats.get(\'ImageRGBA\', {})\n            image_fmt.update(element)\n            arg = image_fmt.pop(""image"", None)\n            figure.image_rgba(arg, **image_fmt)\n        elif key == \'multi_line\':\n            multi_fmt = tool.formats.get(\'MultiLine\', {})\n            multi_fmt.update(element)\n            figure.multi_line(**multi_fmt)\n        elif key == \'layout\':\n            obj = element.pop(\'obj\', None)\n            figure.add_layout(obj, **element)\n\n    for attr, val in axis.items():\n        # change axis attributes, hopefully\n        setattr(figure.axis, attr, val)\n\n    if ref:\n        tool.refs[ref] = figure\n    if callback:\n        figure.on_change(*callback)\n\n    yield figure\n'"
jwql/bokeh_templating/keyword_map.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Jul 19 09:54:47 2018\n\n@author: gkanarek\n""""""\n\nfrom bokeh import layouts, models, palettes, plotting, transform\nfrom inspect import getmembers, isclass, isfunction\n\nfrom .bokeh_surface import Surface3d\n\nbokeh_sequences = {}\nbokeh_mappings = {""Surface3d"": Surface3d}  # Note that abstract base classes *are* included\n\n\ndef parse_module(module):\n    test = lambda nm, mem: (not nm.startswith(""_"")) and (module.__name__ in mem.__module__)\n    seqs = {nm: mem for nm, mem in getmembers(module, isfunction) if test(nm, mem)}\n    maps = {nm: mem for nm, mem in getmembers(module, isclass) if test(nm, mem)}\n\n    # these need to be mappings\n    if \'gridplot\' in seqs:\n        maps[\'gridplot\'] = seqs.pop(\'gridplot\')\n    if \'Donut\' in seqs:\n        maps[\'Donut\'] = seqs.pop(\'Donut\')\n    return (seqs, maps)\n\n\nfor module in [models, plotting, layouts, palettes, transform]:\n    seqs, maps = parse_module(module)\n    bokeh_sequences.update(seqs)\n    bokeh_mappings.update(maps)\n'"
jwql/bokeh_templating/template.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Jul 20 09:49:53 2018\n\n@author: gkanarek\n""""""\n\nimport yaml\nimport os\nfrom . import factory\nfrom bokeh.embed import components\n\n\nclass BokehTemplateParserError(Exception):\n    """"""\n    A custom error for problems with parsing the interface files.\n    """"""\n\n\nclass BokehTemplateEmbedError(Exception):\n    """"""\n    A custom error for problems with embedding components.\n    """"""\n\n\nclass BokehTemplate():\n    """"""\n    This is the base class for creating Bokeh web apps using a YAML templating\n    framework.\n    """"""\n\n    _mapping_factory = factory.mapping_factory\n    _sequence_factory = factory.sequence_factory\n    _figure_constructor = factory.figure_constructor\n    _document_constructor = factory.document_constructor\n    _embed = False\n\n    def _self_constructor(self, loader, tag_suffix, node):\n        """"""\n        A multi_constructor for `!self` tag in the interface file.\n        """"""\n\n        yield eval(""self"" + tag_suffix, globals(), locals())\n\n    def _register_default_constructors(self):\n        for m in factory.mappings:\n            yaml.add_constructor(""!"" + m + "":"", self._mapping_factory(m))\n\n        for s in factory.sequences:\n            yaml.add_constructor(""!"" + s + "":"", self._sequence_factory(s))\n\n        yaml.add_constructor(""!Figure:"", self._figure_constructor)\n        yaml.add_constructor(""!Document:"", self._document_constructor)\n        yaml.add_multi_constructor(u""!self"", self._self_constructor)\n\n    def pre_init(self):\n        """"""\n        This should be implemented by the app subclass, to do any pre-\n        initialization steps that it requires (setting defaults, loading\n        data, etc).\n\n        If this is not required, subclass should set `pre_init = None`\n        in the class definition.\n        """"""\n\n        raise NotImplementedError\n\n    def post_init(self):\n        """"""\n        This should be implemented by the app subclass, to do any post-\n        initialization steps that the tool requires.\n\n        If this is not required, subclass should set `post_init = None`\n        in the class definition.\n        """"""\n\n        raise NotImplementedError\n\n    def __init__(self):\n        self._register_default_constructors()\n\n        # Allow for pre-init stuff from the subclass.\n        if self.pre_init is not None:\n            self.pre_init()\n\n        # Initialize attributes for YAML parsing\n        self.formats = {}\n        self.refs = {}\n        self.document = None\n\n        # Parse formatting string, if any, and the interface YAML file\n        self.include_formatting()\n        self.parse_interface()\n\n        # Allow for post-init stuff from the subclass.\n        if self.post_init is not None:\n            self.post_init()\n\n    def include_formatting(self):\n        """"""\n        This should simply be a dictionary of formatting keywords at the end.\n        """"""\n        if not self.format_string:\n            return\n\n        self.formats = yaml.load(self.format_string, Loader=yaml.Loader)\n\n    def parse_interface(self):\n        """"""\n        This is the workhorse YAML parser, which creates the interface based\n        on the layout file.\n\n        `interface_file` is the path to the interface .yaml file to be parsed.\n        """"""\n\n        if not self.interface_file:\n            raise NotImplementedError(""Interface file required."")\n\n        # Read the interface file into a string\n        filepath = os.path.abspath(os.path.expanduser(self.interface_file))\n        if not os.path.exists(filepath):\n            raise BokehTemplateParserError(""Interface file path does not exist."")\n        with open(filepath) as f:\n            interface = f.read()\n\n        # First, let\'s make sure that there\'s a Document in here\n        if not self._embed and \'!Document\' not in interface:\n            raise BokehTemplateParserError(""Interface file must contain a Document tag"")\n\n        # Now, since we\'ve registered all the constructors, we can parse the\n        # entire string with yaml. We don\'t need to assign the result to a\n        # variable, since the constructors store everything in self.refs\n        # (and self.document, for the document)\n\n        self.full_stream = list(yaml.load(interface, Loader=yaml.Loader))\n\n    def parse_string(self, yaml_string):\n        return list(yaml.load(yaml_string, Loader=yaml.Loader))\n\n    def embed(self, ref):\n        element = self.refs.get(ref, None)\n        if element is None:\n            raise BokehTemplateEmbedError(""Undefined component reference"")\n        return components(element)\n\n    def register_sequence_constructor(self, tag, parse_func):\n        if tag.startswith(""!""):\n            tag = tag[1:]\n\n        def user_constructor(loader, node):\n            value = loader.construct_sequence(node, deep=True)\n            yield parse_func(value)\n        user_constructor.__name__ = tag.lower() + ""_constructor""\n        yaml.add_constructor(""!"" + tag, user_constructor)\n\n    def register_mapping_constructor(self, tag, parse_func):\n        if tag.startswith(""!""):\n            tag = tag[1:]\n\n        def user_constructor(loader, node):\n            value = loader.construct_mapping(node, deep=True)\n            yield parse_func(value)\n        user_constructor.__name__ = tag.lower() + ""_constructor""\n        yaml.add_constructor(""!"" + tag, user_constructor)\n'"
jwql/database/__init__.py,0,b''
jwql/database/database_interface.py,0,"b'""""""\nA module to interact with the JWQL postgresql database ``jwqldb``\n\nThe ``load_connection()`` function within this module allows the user\nto connect to the ``jwqldb`` database via the ``session``, ``base``,\nand ``engine`` objects (described below).  The classes within serve as\nORMs (Object-relational mappings) that define the individual tables of\nthe relational database.\n\nThe ``engine`` object serves as the low-level database API and perhaps\nmost importantly contains dialects which allows the ``sqlalchemy``\nmodule to communicate with the database.\n\nThe ``base`` object serves as a base class for class definitions.  It\nproduces ``Table`` objects and constructs ORMs.\n\nThe ``session`` object manages operations on ORM-mapped objects, as\nconstruced by the base.  These operations include querying, for\nexample.\n\nAuthors\n-------\n\n    - Joe Filippazzo\n    - Johannes Sahlmann\n    - Matthew Bourque\n    - Lauren Chambers\n    - Bryan Hilbert\n    - Misty Cracraft\n    - Sara Ogaz\n\nUse\n---\n\n    Executing the module on the command line will build the database\n    tables defined within:\n\n    ::\n\n        python database_interface.py\n\n    Users wishing to interact with the existing database may do so by\n    importing various connection objects and database tables, for\n    example:\n\n    ::\n\n        from jwql.database.database_interface import Anomaly\n        from jwql.database.database_interface import session\n\n        results = session.query(Anomaly).all()\n\nDependencies\n------------\n\n    The user must have a configuration file named ``config.json``\n    placed in the ``utils`` directory.\n""""""\n\nfrom datetime import datetime\nimport os\nimport socket\n\nimport pandas as pd\nfrom sqlalchemy import Boolean, Column, DateTime, Integer, MetaData, String, Table\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import Date\nfrom sqlalchemy import DateTime\nfrom sqlalchemy import Enum\nfrom sqlalchemy import Float\nfrom sqlalchemy import Integer\nfrom sqlalchemy import MetaData\nfrom sqlalchemy import String\nfrom sqlalchemy import Time\nfrom sqlalchemy import UniqueConstraint\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.ext.automap import automap_base\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.orm.query import Query\nfrom sqlalchemy.types import ARRAY\n\nfrom jwql.utils.constants import ANOMALIES_PER_INSTRUMENT, FILE_SUFFIX_TYPES, JWST_INSTRUMENT_NAMES\nfrom jwql.utils.utils import get_config\n\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\n# Monkey patch Query with data_frame method\n@property\ndef data_frame(self):\n    """"""Method to return a ``pandas.DataFrame`` of the results""""""\n\n    return pd.read_sql(self.statement, self.session.bind)\n\n\nQuery.data_frame = data_frame\n\n\ndef load_connection(connection_string):\n    """"""Return ``session``, ``base``, ``engine``, and ``metadata``\n    objects for connecting to the ``jwqldb`` database.\n\n    Create an ``engine`` using an given ``connection_string``. Create\n    a ``base`` class and ``session`` class from the ``engine``. Create\n    an instance of the ``session`` class. Return the ``session``,\n    ``base``, and ``engine`` instances. This was stolen from the\n    `ascql` repository.\n\n    Parameters\n    ----------\n    connection_string : str\n        A postgresql database connection string. The\n        connection string should take the form:\n        ``dialect+driver://username:password@host:port/database``\n\n    Returns\n    -------\n    session : sesson object\n        Provides a holding zone for all objects loaded or associated\n        with the database.\n    base : base object\n        Provides a base class for declarative class definitions.\n    engine : engine object\n        Provides a source of database connectivity and behavior.\n    meta: metadata object\n        The connection metadata\n\n    References\n    ----------\n    ``ascql``:\n        https://github.com/spacetelescope/acsql/blob/master/acsql/database/database_interface.py\n    """"""\n    engine = create_engine(connection_string, echo=False)\n    base = declarative_base(engine)\n    Session = sessionmaker(bind=engine)\n    session = Session()\n    meta = MetaData(engine)\n\n    return session, base, engine, meta\n\n\n# Import a global session.  If running from readthedocs or Jenkins, pass a dummy connection string\nif \'build\' and \'project\' in socket.gethostname() or ON_JENKINS:\n    dummy_connection_string = \'postgresql+psycopg2://account:password@hostname:0000/db_name\'\n    session, base, engine, meta = load_connection(dummy_connection_string)\nelse:\n    SETTINGS = get_config()\n    session, base, engine, meta = load_connection(SETTINGS[\'connection_string\'])\n\n\nclass FilesystemGeneral(base):\n    """"""ORM for the general (non instrument specific) filesystem monitor\n    table""""""\n\n    # Name the table\n    __tablename__ = \'filesystem_general\'\n\n    # Define the columns\n    id = Column(Integer, primary_key=True, nullable=False)\n    date = Column(DateTime, unique=True, nullable=False)\n    total_file_count = Column(Integer, nullable=False)\n    total_file_size = Column(Float, nullable=False)\n    fits_file_count = Column(Integer, nullable=False)\n    fits_file_size = Column(Float, nullable=False)\n    used = Column(Float, nullable=False)\n    available = Column(Float, nullable=False)\n\n\nclass FilesystemInstrument(base):\n    """"""ORM for the instrument specific filesystem monitor table""""""\n\n    # Name the table\n    __tablename__ = \'filesystem_instrument\'\n    __table_args__ = (UniqueConstraint(\'date\', \'instrument\', \'filetype\',\n                                       name=\'filesystem_instrument_uc\'),)\n\n    # Define the columns\n    id = Column(Integer, primary_key=True, nullable=False)\n    date = Column(DateTime, nullable=False)\n    instrument = Column(Enum(*JWST_INSTRUMENT_NAMES, name=\'instrument_enum\'), nullable=False)\n    filetype = Column(Enum(*FILE_SUFFIX_TYPES, name=\'filetype_enum\'), nullable=False)\n    count = Column(Integer, nullable=False)\n    size = Column(Float, nullable=False)\n\n    @property\n    def colnames(self):\n        """"""A list of all the column names in this table EXCEPT the date column""""""\n        # Get the columns\n        a_list = [col for col, val in self.__dict__.items()\n                  if not isinstance(val, datetime)]\n\n        return a_list\n\n\nclass CentralStore(base):\n    """"""ORM for the central storage area filesystem monitor\n    table""""""\n\n    # Name the table\n    __tablename__ = \'central_storage\'\n\n    # Define the columns\n    id = Column(Integer, primary_key=True, nullable=False)\n    date = Column(DateTime, nullable=False)\n    area = Column(String(), nullable=False)\n    size = Column(Float, nullable=False)\n    used = Column(Float, nullable=False)\n    available = Column(Float, nullable=False)\n\n\nclass Monitor(base):\n    """"""ORM for the ``monitor`` table""""""\n\n    # Name the table\n    __tablename__ = \'monitor\'\n\n    id = Column(Integer, primary_key=True)\n    monitor_name = Column(String(), nullable=False)\n    start_time = Column(DateTime, nullable=False)\n    end_time = Column(DateTime, nullable=True)\n    status = Column(Enum(\'SUCCESS\', \'FAILURE\', name=\'monitor_status\'), nullable=True)\n    affected_tables = Column(ARRAY(String, dimensions=1), nullable=True)\n    log_file = Column(String(), nullable=False)\n\n\ndef anomaly_orm_factory(class_name):\n    """"""Create a ``SQLAlchemy`` ORM Class for an anomaly table.\n\n    Parameters\n    ----------\n    class_name : str\n        The name of the class to be created\n\n    Returns\n    -------\n    class : obj\n        The ``SQLAlchemy`` ORM\n    """"""\n\n    # Initialize a dictionary to hold the column metadata\n    data_dict = {}\n    data_dict[\'__tablename__\'] = class_name.lower()\n\n    instrument = data_dict[\'__tablename__\'].split(\'_\')[0]\n    instrument_anomalies = []\n    for anomaly in ANOMALIES_PER_INSTRUMENT:\n        if instrument in ANOMALIES_PER_INSTRUMENT[anomaly]:\n            instrument_anomalies.append(anomaly)\n\n    # Define anomaly table column names\n    data_dict[\'columns\'] = instrument_anomalies\n    data_dict[\'names\'] = [name.replace(\'_\', \' \') for name in data_dict[\'columns\']]\n\n    # Create a table with the appropriate Columns\n    data_dict[\'id\'] = Column(Integer, primary_key=True, nullable=False)\n    data_dict[\'rootname\'] = Column(String(), nullable=False)\n    data_dict[\'flag_date\'] = Column(DateTime, nullable=False)\n    data_dict[\'user\'] = Column(String(), nullable=False)\n\n    for column in data_dict[\'columns\']:\n        data_dict[column] = Column(Boolean, nullable=False, default=False)\n\n    return type(class_name, (base,), data_dict)\n\n\ndef get_monitor_columns(data_dict, table_name):\n    """"""Read in the corresponding table definition text file to\n    generate ``SQLAlchemy`` columns for the table.\n\n    Parameters\n    ----------\n    data_dict : dict\n        A dictionary whose keys are column names and whose values\n        are column definitions.\n    table_name : str\n        The name of the database table\n\n    Returns\n    -------\n    data_dict : dict\n        An updated ``data_dict`` with the approriate columns for\n        the monitor added.\n    """"""\n\n    # Define column types\n    data_type_dict = {\'integer\': Integer(),\n                      \'string\': String(),\n                      \'float\': Float(precision=32),\n                      \'decimal\': Float(precision=\'13,8\'),\n                      \'date\': Date(),\n                      \'time\': Time(),\n                      \'datetime\': DateTime,\n                      \'bool\': Boolean\n                      }\n\n    # Get the data from the table definition file\n    instrument = table_name.split(\'_\')[0]\n    table_definition_file = os.path.join(os.path.split(__file__)[0],\n                                         \'monitor_table_definitions\',\n                                         instrument.lower(),\n                                         \'{}.txt\'.format(table_name))\n    with open(table_definition_file, \'r\') as f:\n        data = f.readlines()\n\n    # Parse out the column names from the data types\n    column_definitions = [item.strip().split(\', \') for item in data]\n    for column_definition in column_definitions:\n        column_name = column_definition[0]\n        data_type = column_definition[1]\n\n        if \'array\' in data_type:\n            dtype, _a, dimension = data_type.split(\'_\')\n            dimension = int(dimension[0])\n            array = True\n        else:\n            dtype = data_type\n            array = False\n\n        # Create a new column\n        if dtype in list(data_type_dict.keys()):\n            if array:\n                data_dict[column_name.lower()] = Column(ARRAY(data_type_dict[dtype],\n                                                              dimensions=dimension))\n            else:\n                data_dict[column_name.lower()] = Column(data_type_dict[dtype])\n        else:\n            raise ValueError(\'Unrecognized column type: {}:{}\'.format(column_name, data_type))\n\n    return data_dict\n\n\ndef get_monitor_table_constraints(data_dict, table_name):\n    """"""Add any necessary table constrains to the given table via the\n    ``data_dict``.\n\n    Parameters\n    ----------\n    data_dict : dict\n        A dictionary whose keys are column names and whose values\n        are column definitions.\n    table_name : str\n        The name of the database table\n\n    Returns\n    -------\n    data_dict : dict\n        An updated ``data_dict`` with the approriate table constraints\n        for the monitor added.\n    """"""\n\n    return data_dict\n\n\ndef monitor_orm_factory(class_name):\n    """"""Create a ``SQLAlchemy`` ORM Class for a ``jwql`` instrument\n    monitor.\n\n    Parameters\n    ----------\n    class_name : str\n        The name of the class to be created\n\n    Returns\n    -------\n    class : obj\n        The ``SQLAlchemy`` ORM\n    """"""\n\n    # Initialize a dictionary to hold the column metadata\n    data_dict = {}\n    data_dict[\'__tablename__\'] = class_name.lower()\n\n    # Columns specific to all monitor ORMs\n    data_dict[\'id\'] = Column(Integer, primary_key=True, nullable=False)\n    data_dict[\'entry_date\'] = Column(DateTime, unique=True, nullable=False, default=datetime.now())\n    data_dict[\'__table_args__\'] = (\n        UniqueConstraint(\'id\', \'entry_date\', name=\'{}_uc\'.format(data_dict[\'__tablename__\'])),\n    )\n\n    # Get monitor-specific columns\n    data_dict = get_monitor_columns(data_dict, data_dict[\'__tablename__\'])\n\n    # Get monitor-specific table constrains\n    data_dict = get_monitor_table_constraints(data_dict, data_dict[\'__tablename__\'])\n\n    return type(class_name, (base,), data_dict)\n\n\n# Create tables from ORM factory\nNIRCamAnomaly = anomaly_orm_factory(\'nircam_anomaly\')\nNIRISSAnomaly = anomaly_orm_factory(\'niriss_anomaly\')\nNIRSpecAnomaly = anomaly_orm_factory(\'nirspec_anomaly\')\nMIRIAnomaly = anomaly_orm_factory(\'miri_anomaly\')\nFGSAnomaly = anomaly_orm_factory(\'fgs_anomaly\')\nNIRCamDarkQueryHistory = monitor_orm_factory(\'nircam_dark_query_history\')\nNIRCamDarkPixelStats = monitor_orm_factory(\'nircam_dark_pixel_stats\')\nNIRCamDarkDarkCurrent = monitor_orm_factory(\'nircam_dark_dark_current\')\nNIRISSDarkQueryHistory = monitor_orm_factory(\'niriss_dark_query_history\')\nNIRISSDarkPixelStats = monitor_orm_factory(\'niriss_dark_pixel_stats\')\nNIRISSDarkDarkCurrent = monitor_orm_factory(\'niriss_dark_dark_current\')\nNIRSpecDarkQueryHistory = monitor_orm_factory(\'nirspec_dark_query_history\')\nNIRSpecDarkPixelStats = monitor_orm_factory(\'nirspec_dark_pixel_stats\')\nNIRSpecDarkDarkCurrent = monitor_orm_factory(\'nirspec_dark_dark_current\')\nMIRIDarkQueryHistory = monitor_orm_factory(\'miri_dark_query_history\')\nMIRIDarkPixelStats = monitor_orm_factory(\'miri_dark_pixel_stats\')\nMIRIDarkDarkCurrent = monitor_orm_factory(\'miri_dark_dark_current\')\nFGSDarkQueryHistory = monitor_orm_factory(\'fgs_dark_query_history\')\nFGSDarkPixelStats = monitor_orm_factory(\'fgs_dark_pixel_stats\')\nFGSDarkDarkCurrent = monitor_orm_factory(\'fgs_dark_dark_current\')\nNIRCamBiasQueryHistory = monitor_orm_factory(\'nircam_bias_query_history\')\nNIRCamBiasStats = monitor_orm_factory(\'nircam_bias_stats\')\n\n\nif __name__ == \'__main__\':\n\n    base.metadata.create_all(engine)\n'"
jwql/database/reset_database.py,0,"b'#! /usr/bin/env python\n\n""""""Reset all tables in the ``jwqldb`` database.\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    This script is intended to be used in the command line:\n    ::\n\n        python reset_database.py\n\nDependencies\n------------\n\n    Users must have a ``config.json`` configuration file with a proper\n    ``connection_string`` key that points to the ``jwqldb`` database.\n    The ``connection_string`` format is\n    ``postgresql+psycopg2://user:password@host:port/database``.\n""""""\n\nfrom jwql.database.database_interface import base\nfrom jwql.utils.utils import get_config\n\n\nif __name__ == \'__main__\':\n\n    connection_string = get_config()[\'connection_string\']\n    server_type = connection_string.split(\'@\')[-1][0]\n\n    assert server_type != \'p\', \'Cannot reset production database!\'\n\n    prompt = (\'About to reset all tables for database instance {}. Do you \'\n              \'wish to proceed? (y/n)\\n\'.format(connection_string))\n    response = input(prompt)\n\n    if response.lower() == \'y\':\n        base.metadata.drop_all()\n        base.metadata.create_all()\n        print(\'\\nDatabase instance {} has been reset\'.format(connection_string))\n'"
jwql/edb/__init__.py,0,b''
jwql/edb/engineering_database.py,3,"b'#! /usr/bin/env python\n""""""Module for dealing with JWST DMS Engineering Database mnemonics.\n\nThis module provides ``jwql`` with convenience classes and functions\nto retrieve and manipulate mnemonics from the JWST DMS EDB. It uses\nthe `edb_interface` module to interface the EDB directly.\n\nAuthors\n-------\n\n    - Johannes Sahlmann\n\nUse\n---\n\n    This module can be imported and used with\n\n    ::\n\n        from jwql.edb.engineering_database import get_mnemonic\n        get_mnemonic(mnemonic_identifier, start_time, end_time)\n\n    Required arguments:\n\n    ``mnemonic_identifier`` - String representation of a mnemonic name.\n    ``start_time`` - astropy.time.Time instance\n    ``end_time`` - astropy.time.Time instance\n\nNotes\n-----\n    There are three possibilities for MAST authentication:\n\n    1. A valid MAST authentication token is present in the local\n    ``jwql`` configuration file (config.json).\n    2. The MAST_API_TOKEN environment variable is set to a valid\n    MAST authentication token.\n    3. The user has logged into the ``jwql`` web app, in which\n    case they are authenticated via auth.mast.\n\n    When querying mnemonic values, the underlying MAST service returns\n    data that include the datapoint preceding the requested start time\n    and the datapoint that follows the requested end time.\n\n""""""\n\nfrom collections import OrderedDict\n\nfrom astropy.time import Time\nfrom bokeh.embed import components\nfrom bokeh.plotting import figure\nimport numpy as np\n\nfrom jwql.utils.credentials import get_mast_token\nfrom jwedb.edb_interface import query_single_mnemonic, query_mnemonic_info\n\n\nclass EdbMnemonic:\n    """"""Class to hold and manipulate results of DMS EngDB queries.""""""\n\n    def __init__(self, mnemonic_identifier, start_time, end_time, data, meta, info):\n        """"""Populate attributes.\n\n        Parameters\n        ----------\n        mnemonic_identifier : str\n            Telemetry mnemonic identifier\n        start_time : astropy.time.Time instance\n            Start time\n        end_time : astropy.time.Time instance\n            End time\n        data : astropy.table.Table\n            Table representation of the returned data.\n        meta : dict\n            Additional information returned by the query\n        info : dict\n            Auxiliary information on the mnemonic (description,\n            category, unit)\n\n        """"""\n        self.mnemonic_identifier = mnemonic_identifier\n        self.requested_start_time = start_time\n        self.requested_end_time = end_time\n        self.data = data\n        self.data_start_time = Time(np.min(np.array(self.data[\'MJD\'])), format=\'mjd\')\n        self.data_end_time = Time(np.max(np.array(self.data[\'MJD\'])), format=\'mjd\')\n        self.meta = meta\n        self.info = info\n\n    def __str__(self):\n        """"""Return string describing the instance.""""""\n        return \'EdbMnemonic {} with {} records between {} and {}\'.format(\n            self.mnemonic_identifier, len(self.data), self.data_start_time.isot,\n            self.data_end_time.isot)\n\n    def interpolate(self, times, **kwargs):\n        """"""Interpolate value at specified times.""""""\n        raise NotImplementedError\n\n    def bokeh_plot(self):\n        """"""Make basic bokeh plot showing value as a function of time.\n\n        Returns\n        -------\n        [div, script] : list\n            List containing the div and js representations of figure.\n\n        """"""\n        abscissa = Time(self.data[\'MJD\'], format=\'mjd\').datetime\n        ordinate = self.data[\'euvalue\']\n\n        p1 = figure(tools=\'pan,box_zoom,reset,wheel_zoom,save\', x_axis_type=\'datetime\',\n                    title=self.mnemonic_identifier, x_axis_label=\'Time\',\n                    y_axis_label=\'Value ({})\'.format(self.info[\'unit\']))\n        p1.line(abscissa, ordinate, line_width=1, line_color=\'blue\', line_dash=\'dashed\')\n        p1.circle(abscissa, ordinate, color=\'blue\')\n\n        script, div = components(p1)\n\n        return [div, script]\n\n\ndef get_mnemonic(mnemonic_identifier, start_time, end_time):\n    """"""Execute query and return a EdbMnemonic instance.\n\n    The underlying MAST service returns data that include the\n    datapoint preceding the requested start time and the datapoint\n    that follows the requested end time.\n\n    Parameters\n    ----------\n    mnemonic_identifier : str\n        Telemetry mnemonic identifiers, e.g. \'SA_ZFGOUTFOV\'\n    start_time : astropy.time.Time instance\n        Start time\n    end_time : astropy.time.Time instance\n        End time\n\n    Returns\n    -------\n    mnemonic : instance of EdbMnemonic\n        EdbMnemonic object containing query results\n\n    """"""\n    mast_token = get_mast_token()\n    data, meta, info = query_single_mnemonic(mnemonic_identifier, start_time, end_time,\n                                             token=mast_token)\n\n    # create and return instance\n    mnemonic = EdbMnemonic(mnemonic_identifier, start_time, end_time, data, meta, info)\n    return mnemonic\n\n\ndef get_mnemonics(mnemonics, start_time, end_time):\n    """"""Query DMS EDB with a list of mnemonics and a time interval.\n\n    Parameters\n    ----------\n    mnemonics : list or numpy.ndarray\n        Telemetry mnemonic identifiers, e.g. [\'SA_ZFGOUTFOV\',\n        \'IMIR_HK_ICE_SEC_VOLT4\']\n    start_time : astropy.time.Time instance\n        Start time\n    end_time : astropy.time.Time instance\n        End time\n\n    Returns\n    -------\n    mnemonic_dict : dict\n        Dictionary. keys are the queried mnemonics, values are\n        instances of EdbMnemonic\n\n    """"""\n    if not isinstance(mnemonics, (list, np.ndarray)):\n        raise RuntimeError(\'Please provide a list/array of mnemonic_identifiers\')\n\n    mnemonic_dict = OrderedDict()\n    for mnemonic_identifier in mnemonics:\n        # fill in dictionary\n        mnemonic_dict[mnemonic_identifier] = get_mnemonic(mnemonic_identifier, start_time, end_time)\n\n    return mnemonic_dict\n\n\ndef get_mnemonic_info(mnemonic_identifier):\n    """"""Return the mnemonic description.\n\n    Parameters\n    ----------\n    mnemonic_identifier : str\n        Telemetry mnemonic identifier, e.g. ``SA_ZFGOUTFOV``\n\n    Returns\n    -------\n    info : dict\n        Object that contains the returned data\n\n    """"""\n    mast_token = get_mast_token()\n    return query_mnemonic_info(mnemonic_identifier, token=mast_token)\n'"
jwql/instrument_monitors/__init__.py,0,b''
jwql/instrument_monitors/pipeline_tools.py,3,"b'""""""Various utility functions related to the JWST calibration pipeline\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\nUse\n---\n\n    This module can be imported as such:\n    ::\n\n        from jwql.instrument_monitors import pipeline_tools\n        pipeline_steps = pipeline_tools.completed_pipeline_steps(filename)\n """"""\n\nfrom collections import OrderedDict\nimport copy\nimport numpy as np\n\nfrom astropy.io import fits\nfrom jwst.dq_init import DQInitStep\nfrom jwst.dark_current import DarkCurrentStep\nfrom jwst.firstframe import FirstFrameStep\nfrom jwst.group_scale import GroupScaleStep\nfrom jwst.ipc import IPCStep\nfrom jwst.jump import JumpStep\nfrom jwst.lastframe import LastFrameStep\nfrom jwst.linearity import LinearityStep\nfrom jwst.persistence import PersistenceStep\nfrom jwst.ramp_fitting import RampFitStep\nfrom jwst.refpix import RefPixStep\nfrom jwst.rscd import RSCD_Step\nfrom jwst.saturation import SaturationStep\nfrom jwst.superbias import SuperBiasStep\n\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES_UPPERCASE\n\n# Define the fits header keyword that accompanies each step\nPIPE_KEYWORDS = {\'S_GRPSCL\': \'group_scale\', \'S_DQINIT\': \'dq_init\', \'S_SATURA\': \'saturation\',\n                 \'S_IPC\': \'ipc\', \'S_REFPIX\': \'refpix\', \'S_SUPERB\': \'superbias\',\n                 \'S_PERSIS\': \'persistence\', \'S_DARK\': \'dark_current\', \'S_LINEAR\': \'linearity\',\n                 \'S_FRSTFR\': \'firstframe\', \'S_LASTFR\': \'lastframe\', \'S_RSCD\': \'rscd\',\n                 \'S_JUMP\': \'jump\', \'S_RAMP\': \'rate\'}\n\nPIPELINE_STEP_MAPPING = {\'dq_init\': DQInitStep, \'dark_current\': DarkCurrentStep,\n                         \'firstframe\': FirstFrameStep, \'group_scale\': GroupScaleStep,\n                         \'ipc\': IPCStep, \'jump\': JumpStep, \'lastframe\': LastFrameStep,\n                         \'linearity\': LinearityStep, \'persistence\': PersistenceStep,\n                         \'rate\': RampFitStep, \'refpix\': RefPixStep, \'rscd\': RSCD_Step,\n                         \'saturation\': SaturationStep, \'superbias\': SuperBiasStep}\n\n# Readout patterns that have nframes != a power of 2. These readout patterns\n# require the group_scale pipeline step to be run.\nGROUPSCALE_READOUT_PATTERNS = [\'NRSIRS2\']\n\n\ndef completed_pipeline_steps(filename):\n    """"""Return a list of the completed pipeline steps for a given file.\n\n    Parameters\n    ----------\n    filename : str\n        File to examine\n\n    Returns\n    -------\n    completed : collections.OrderedDict\n        Dictionary with boolean entry for each pipeline step,\n        indicating which pipeline steps have been run on filename\n    """"""\n\n    # Initialize using PIPE_KEYWORDS so that entries are guaranteed to\n    # be in the correct order\n    completed = OrderedDict({})\n    for key in PIPE_KEYWORDS.values():\n        completed[key] = False\n\n    header = fits.getheader(filename)\n    for key in PIPE_KEYWORDS.keys():\n        try:\n            value = header.get(key)\n        except KeyError:\n            value == \'NOT DONE\'\n        if value == \'COMPLETE\':\n            completed[PIPE_KEYWORDS[key]] = True\n\n    return completed\n\n\ndef get_pipeline_steps(instrument):\n    """"""Get the names and order of the ``calwebb_detector1`` pipeline\n    steps for a given instrument. Use values that match up with the\n    values in the ``PIPE_STEP`` defintion in ``definitions.py``\n\n    Parameters\n    ----------\n    instrument : str\n        Name of JWST instrument\n\n    Returns\n    -------\n    steps : collections.OrderedDict\n        Dictionary of step names\n    """"""\n\n    # Ensure instrument name is valid\n    instrument = instrument.upper()\n    if instrument not in JWST_INSTRUMENT_NAMES_UPPERCASE.values():\n        raise ValueError(""WARNING: {} is not a valid instrument name."".format(instrument))\n\n    # Order is important in \'steps\' lists below!!\n    if instrument == \'MIRI\':\n        steps = [\'group_scale\', \'dq_init\', \'saturation\', \'ipc\', \'firstframe\', \'lastframe\',\n                 \'linearity\', \'rscd\', \'dark_current\', \'refpix\', \'persistence\', \'jump\', \'rate\']\n        # No persistence correction for MIRI\n        steps.remove(\'persistence\')\n        # MIRI is limited to one frame per group\n        steps.remove(\'group_scale\')\n    else:\n        steps = [\'group_scale\', \'dq_init\', \'saturation\', \'ipc\', \'superbias\', \'refpix\', \'linearity\',\n                 \'persistence\', \'dark_current\', \'jump\', \'rate\']\n\n        # No persistence correction for NIRSpec\n        if instrument == \'NIRSPEC\':\n            steps.remove(\'persistence\')\n        else:\n            # NIRCam, NISISS, FGS all do not need group scale as nframes is\n            # always a multiple of 2\n            steps.remove(\'group_scale\')\n\n    # IPC correction currently not done for any instrument\n    steps.remove(\'ipc\')\n\n    # Initialize using PIPE_KEYWORDS so the steps will be in the right order\n    required_steps = OrderedDict({})\n    for key in steps:\n        required_steps[key] = True\n    for key in PIPE_KEYWORDS.values():\n        if key not in required_steps.keys():\n            required_steps[key] = False\n\n    return required_steps\n\n\ndef image_stack(file_list):\n    """"""Given a list of fits files containing 2D images, read in all data\n    and place into a 3D stack\n\n    Parameters\n    ----------\n    file_list : list\n        List of fits file names\n\n    Returns\n    -------\n    cube : numpy.ndarray\n        3D stack of the 2D images\n    """"""\n\n    exptimes = []\n    for i, input_file in enumerate(file_list):\n        with fits.open(input_file) as hdu:\n            image = hdu[1].data\n            exptime = hdu[0].header[\'EFFINTTM\']\n            num_ints = hdu[0].header[\'NINTS\']\n\n        # Stack all inputs together into a single 3D image cube\n        if i == 0:\n            ndim_base = image.shape\n            if len(ndim_base) == 3:\n                cube = copy.deepcopy(image)\n            elif len(ndim_base) == 2:\n                cube = np.expand_dims(image, 0)\n        else:\n            ndim = image.shape\n            if ndim_base[-2:] == ndim[-2:]:\n                if len(ndim) == 2:\n                    image = np.expand_dims(image, 0)\n                elif len(ndim) > 3:\n                    raise ValueError(""4-dimensional input slope images not supported."")\n                cube = np.vstack((cube, image))\n            else:\n                raise ValueError(""Input images are of inconsistent size in x/y dimension."")\n        exptimes.append([exptime] * num_ints)\n\n    return cube, exptimes\n\n\ndef run_calwebb_detector1_steps(input_file, steps):\n    """"""Run the steps of ``calwebb_detector1`` specified in the steps\n    dictionary on the input file\n\n    Parameters\n    ----------\n    input_file : str\n        File on which to run the pipeline steps\n\n    steps : collections.OrderedDict\n        Keys are the individual pipeline steps (as seen in the\n        ``PIPE_KEYWORDS`` values above). Boolean values indicate whether\n        a step should be run or not. Steps are run in the official\n        ``calwebb_detector1`` order.\n    """"""\n\n    first_step_to_be_run = True\n    for step_name in steps:\n        if steps[step_name]:\n            if first_step_to_be_run:\n                model = PIPELINE_STEP_MAPPING[step_name].call(input_file)\n                first_step_to_be_run = False\n            else:\n                model = PIPELINE_STEP_MAPPING[step_name].call(model)\n            suffix = step_name\n    output_filename = input_file.replace(\'.fits\', \'_{}.fits\'.format(suffix))\n    if suffix != \'rate\':\n        model.save(output_filename)\n    else:\n        model[0].save(output_filename)\n\n    return output_filename\n\n\ndef steps_to_run(all_steps, finished_steps):\n    """"""Given a list of pipeline steps that need to be completed as well\n    as a list of steps that have already been completed, return a list\n    of steps remaining to be done.\n\n    Parameters\n    ----------\n    all_steps : collections.OrderedDict\n        A dictionary of all steps that need to be completed\n\n    finished_steps : collections.OrderedDict\n        A dictionary with keys equal to the pipeline steps and boolean\n        values indicating whether a particular step has been completed\n        or not (i.e. output from ``completed_pipeline_steps``)\n\n    Returns\n    -------\n    steps_to_run : collections.OrderedDict\n        A dictionaru with keys equal to the pipeline steps and boolean\n        values indicating whether a particular step has yet to be run.\n    """"""\n\n    torun = copy.deepcopy(finished_steps)\n\n    for key in all_steps:\n        if all_steps[key] == finished_steps[key]:\n            torun[key] = False\n        elif ((all_steps[key] is True) & (finished_steps[key] is False)):\n            torun[key] = True\n        elif ((all_steps[key] is False) & (finished_steps[key] is True)):\n            print((""WARNING: {} step has been run ""\n                   ""but the requirements say that it should not ""\n                   ""be. Need a new input file."".format(key)))\n\n    return torun\n'"
jwql/jwql_monitors/__init__.py,0,b''
jwql/jwql_monitors/generate_preview_images.py,12,"b'#! /usr/bin/env python\n\n""""""Generate preview images for all files in the ``jwql`` filesystem.\n\nExecution of this script will generate preview images and thumbnail\nimages for each file in the ``jwql`` filesystem.  Preview images have\naxes labels, titles, and colorbars, wheras thumbnail images are\nsmaller and contain no labels.  Images are saved into the\n``preview_image_filesystem`` and ``thumbnail_filesystem``, organized\nby subdirectories pertaining to the ``program_id`` in the filenames.\n\nAuthors\n-------\n\n    - Matthew Bourque\n    - Bryan Hilbert\n\n\nUse\n---\n\n    This script is intended to be executed as such:\n\n    ::\n\n        python generate_preview_images.py\n""""""\n\nimport glob\nimport logging\nimport multiprocessing\nimport os\nimport re\n\nimport numpy as np\n\nfrom jwql.utils import permissions\nfrom jwql.utils.constants import NIRCAM_LONGWAVE_DETECTORS, NIRCAM_SHORTWAVE_DETECTORS\nfrom jwql.utils.logging_functions import configure_logging, log_info, log_fail\nfrom jwql.utils.preview_image import PreviewImage\nfrom jwql.utils.utils import get_config, filename_parser\n\n# Size of NIRCam inter- and intra-module chip gaps\nSW_MOD_GAP = 1387  # pixels = int(43 arcsec / 0.031 arcsec/pixel)\nLW_MOD_GAP = 741  # pixels = int(46 arcsec / 0.062 arcsec/pixel)\nSW_DET_GAP = 145  # pixels = int(4.5 arcsec / 0.031 arcsec/pixel)\nFULLX = 2048  # Width of the full detector\nFULLY = 2048  # Height of the full detector\n\n\ndef array_coordinates(channelmod, detector_list, lowerleft_list):\n    """"""Create an appropriately sized ``numpy`` array to contain the\n    mosaic image given the channel and module of the data.\n\n    Parameters\n    ----------\n    channelmod : str\n        Indicator of the NIRCam channel/module of the data.\n        Options are:\n        ``LW`` - for longwave channel data\n        ``SWA`` - for shortwave A module only (4 detectors) data\n        ``SWB`` - for shortwave B module only (4 detectors) data\n        ``SW`` - for shortwave both module data (8 detectors)\n\n    detector_list : list\n        List of detectors used in data to be simulated\n\n    lowerleft_list : list\n        Each element is a tuple giving the (x, y) coordinates\n        corresponding to the lower left corner of the aperture within\n        the full frame detector. These values come from the\n        ``SUBSTRT1`` and 2 values in the file headers.\n\n    Returns\n    -------\n    xdim : int\n        Length of the output array needed to contain all detectors\' data\n\n    ydim : int\n        Height of the output array needed to contain all detectors\' data\n\n    module_lowerlefts : dict\n        Dictionary giving the ``(x, y)`` coordinate in the coordinate\n        system of the full module(s) where the lower left corner of the\n        data from a given detector will be placed. (e.g.\n        ``NRCA1: (1888, 1888)`` means that the data from detector\n        ``NRCA1`` should be placed into\n        ``[1888: 1888+y_dim_of_data, 1888: 1888+x_dim_of_data]`` within\n        the final array (which has total dimensions of ``(xdim, ydim)``\n    """"""\n\n    # Create dictionary of lower left pixel values for each\n    # detector as it sits in the MODULE. B1-B4 values here will\n    # need to have sw_mod_gap added to their x coordinates in\n    # order to translate to the full A and B module coordinate system.\n    # The only case where LW data will be in this function is where\n    # both detectors are used, so set A5/B5 coordinates to be in\n    # the full module A and B coordinate system. Note that these\n    # tuples are (x,y), NOT (y,x)\n    ashort = [""NRCA1"", ""NRCA2"", ""NRCA3"", ""NRCA4""]\n    bshort = [""NRCB1"", ""NRCB2"", ""NRCB3"", ""NRCB4""]\n    module_lowerlefts = {""NRCA1"": (0, 0),\n                         ""NRCA2"": (0, FULLY + SW_DET_GAP),\n                         ""NRCA3"": (FULLX + SW_DET_GAP, 0),\n                         ""NRCA4"": (FULLX + SW_DET_GAP, FULLY + SW_DET_GAP),\n                         ""NRCB1"": (FULLX + SW_DET_GAP, FULLY + SW_DET_GAP),\n                         ""NRCB2"": (FULLX + SW_DET_GAP, 0),\n                         ""NRCB3"": (0, FULLY + SW_DET_GAP),\n                         ""NRCB4"": (0, 0),\n                         ""NRCA5"": (0, 0),\n                         ""NRCB5"": (FULLX + LW_MOD_GAP, 0)}\n\n    # If both module A and B are used, then shift the B module\n    # coordinates to account for the intermodule gap\n    if channelmod == ""SW"":\n        mod_delta = (FULLX * 2 + SW_DET_GAP + SW_MOD_GAP, 0)\n        for b_detector in bshort:\n            module_lowerlefts[b_detector] = tuple([sum(x) for x in\n                                                  zip(module_lowerlefts[b_detector],\n                                                      mod_delta)])\n\n    # The only subarrays we need to worry about are the SW SUBXXX\n    # All other subarrays are on a single detector.\n    # All we need to do is find the lower left values for NRCA1 and/or NRCB4.\n    # From that we can calculate the rest. Also, if observing with both\n    # A and B modules, only full frame is allowed, so no need to worry about\n    # subarrays in both modules simultaneously.\n    subx = 1\n    suby = 1\n    if \'SW\' in channelmod:\n        detector_list = np.array(detector_list)\n        a1 = detector_list == \'NRCA1\'\n        b4 = detector_list == \'NRCB4\'\n\n        lowerleft_list = np.array(lowerleft_list)\n        if np.sum(a1) == 1:\n            subx, suby = lowerleft_list[a1][0]\n        elif np.sum(b4) == 1:\n            subx, suby = lowerleft_list[b4][0]\n        else:\n            missing_a14 = ""SW data provided, but neither NRCA1 nor NRCB4 are present.""\n            logging.error(missing_a14)\n            raise ValueError(missing_a14)\n\n    # Adjust the lower left positions of the apertures within\n    # the module(s) in the case of subarrays\n    if ((subx != 1) | (suby != 1)):\n        subarr_delta = {""NRCA1"": (0, 0),\n                        ""NRCA2"": (0, 0 - suby),\n                        ""NRCA3"": (0 - subx, 0),\n                        ""NRCA4"": (0 - subx, 0 - suby),\n                        ""NRCB1"": (0 - subx, 0 - suby),\n                        ""NRCB2"": (0 - subx, 0),\n                        ""NRCB3"": (0, 0 - suby),\n                        ""NRCB4"": (0, 0)}\n\n        for det in ashort + bshort:\n            module_lowerlefts[det] = tuple([sum(x) for x in zip(module_lowerlefts[det],\n                                                                subarr_delta[det])])\n\n    # Dimensions of array to hold all data\n    # Adjust dimensions of individual detector for subarray if necessary\n    aperturex = FULLX - (subx - 1)\n    aperturey = FULLY - (suby - 1)\n\n    # Full module(s) dimensions\n    if channelmod in [\'SWA\', \'SWB\']:\n        xdim = 2 * aperturex + SW_DET_GAP\n        ydim = 2 * aperturey + SW_DET_GAP\n    elif channelmod == \'SW\':\n        xdim = 4 * aperturex + 2 * SW_DET_GAP + SW_MOD_GAP\n        ydim = 2 * aperturey + SW_DET_GAP\n    elif channelmod == \'LW\':\n        xdim = 2 * aperturex + LW_MOD_GAP\n        ydim = aperturey\n\n    return xdim, ydim, module_lowerlefts\n\n\ndef check_existence(file_list, outdir):\n    """"""Given a list of fits files, determine if a preview image has\n    already been created in ``outdir``.\n\n    Parameters\n    ----------\n    file_list : list\n        List of fits filenames from which preview image will be\n        generated\n\n    outdir : str\n        Directory that will contain the preview image if it exists\n\n    Returns\n    -------\n    exists : bool\n        ``True`` if preview image exists, ``False`` if it does not\n    """"""\n\n    # If file_list contains only a single file, then we need to search\n    # for a preview image name that contains the detector name\n    if len(file_list) == 1:\n        filename = os.path.split(file_list[0])[1]\n        search_string = filename.split(\'.fits\')[0] + \'_*.jpg\'\n    else:\n        # If file_list contains multiple files, then we need to search\n        # for the appropriately named jpg of the mosaic, which depends\n        # on the specific detectors in the file_list\n        file_parts = filename_parser(file_list[0])\n        if file_parts[\'detector\'].upper() in NIRCAM_SHORTWAVE_DETECTORS:\n            mosaic_str = ""NRC_SW*_MOSAIC_""\n        elif file_parts[\'detector\'].upper() in NIRCAM_LONGWAVE_DETECTORS:\n            mosaic_str = ""NRC_LW*_MOSAIC_""\n        search_string = \'jw{}{}{}_{}{}{}_{}_{}{}*.jpg\'.format(\n                        file_parts[\'program_id\'], file_parts[\'observation\'],\n                        file_parts[\'visit\'], file_parts[\'visit_group\'],\n                        file_parts[\'parallel_seq_id\'], file_parts[\'activity\'],\n                        file_parts[\'exposure_id\'], mosaic_str, file_parts[\'suffix\'])\n\n    current_files = glob.glob(os.path.join(outdir, search_string))\n    if len(current_files) > 0:\n        return True\n    else:\n        return False\n\n\ndef create_dummy_filename(filelist):\n    """"""Create a dummy filename indicating the detectors used to create\n    the mosaic. Check the list of detectors used to determine the proper\n    text to substitute into the initial filename.\n\n    Parameters\n    ----------\n    filelist : list\n        List of filenames containing the data used to create the mosaic.\n        It is assumed these filenames follow JWST filenaming\n        conventions.\n\n    Returns\n    -------\n    dummy_name : str\n        The first filename in ``filelist`` is modified, such that the\n        detector name is replaced with text indicating the source of the\n        mosaic data.\n    """"""\n\n    det_string_list = []\n    modules = []\n    for filename in filelist:\n        indir, infile = os.path.split(filename)\n        det_string = filename_parser(infile)[\'detector\']\n        det_string_list.append(det_string)\n        modules.append(det_string[3].upper())\n\n    # Previous sorting means that either all of the\n    # input files are LW, or all are SW. So we can check any\n    # file to determine LW vs SW\n    if \'5\' in det_string_list[0]:\n        suffix = ""NRC_LW_MOSAIC""\n    else:\n        moda = modules.count(\'A\')\n        modb = modules.count(\'B\')\n        if moda > 0:\n            if modb > 0:\n                suffix = ""NRC_SWALL_MOSAIC""\n            else:\n                suffix = ""NRC_SWA_MOSAIC""\n        else:\n            if modb > 0:\n                suffix = ""NRC_SWB_MOSAIC""\n    dummy_name = filelist[0].replace(det_string_list[0], suffix)\n\n    return dummy_name\n\n\ndef create_mosaic(filenames):\n    """"""If an exposure comprises data from multiple detectors read in all\n    the appropriate files and create a mosaic so that the preview image\n    will show all the data together.\n\n    Parameters\n    ----------\n    filenames : list\n        List of filenames to be combined into a mosaic\n\n    Returns\n    -------\n    mosaic_filename : str\n        Name of fits file containing the mosaicked data\n    """"""\n\n    # Use preview_image to load data and create difference image\n    # for each detector. Save in a list\n    data = []\n    detector = []\n    data_lower_left = []\n    for filename in filenames:\n        image = PreviewImage(filename, ""SCI"")  # Now have image.data, image.dq\n        data_dim = len(image.data.shape)\n        if data_dim == 4:\n            diff_im = image.difference_image(image.data)\n        else:\n            diff_im = image.data\n        data.append(diff_im)\n        detector.append(filename_parser(filename)[\'detector\'].upper())\n        data_lower_left.append((image.xstart, image.ystart))\n\n    # Make sure SW and LW data are not being mixed. Create the\n    # appropriately sized numpy array to hold all the data based\n    # on the channel, module, and subarray size\n    mosaic_channel = find_data_channel(detector)\n    full_xdim, full_ydim, full_lower_left = array_coordinates(mosaic_channel, detector,\n                                                              data_lower_left)\n\n    # Create the array to hold all the data\n    datashape = data[0].shape\n    datadim = len(datashape)\n    if datadim == 2:\n        full_array = np.zeros((1, full_ydim, full_xdim)) * np.nan\n    elif datadim == 3:\n        full_array = np.zeros((datashape[0], full_ydim, full_xdim)) * np.nan\n    else:\n        raise ValueError(\'Difference image for {} must be either 2D or 3D.\'.format(filenames[0]))\n\n    # Place the data from the individual detectors in the appropriate\n    # places in the final image\n    for pixdata, detect in zip(data, detector):\n        x0, y0 = full_lower_left[detect]\n        if datadim == 2:\n            yd, xd = pixdata.shape\n            full_array[0, y0: y0 + yd, x0: x0 + xd] = pixdata\n        elif datadim == 3:\n            ints, yd, xd = pixdata.shape\n            full_array[:, y0: y0 + yd, x0: x0 + xd] = pixdata\n\n    # Create associated DQ array and set unpopulated pixels to be skipped\n    # in preview image scaling\n    full_dq = create_dq_array(full_xdim, full_ydim, full_array[0, :, :], mosaic_channel)\n\n    return full_array, full_dq\n\n\ndef create_dq_array(xd, yd, mosaic, module):\n    """"""Create DQ array that goes with the mosaic image. Set unpopulated\n    pixels to be skipped in preview image scaling. Same for the\n    reference pixels for all detectors\n\n    Parameters\n    ----------\n    xd : int\n        X-coordinate dimension of the DQ array\n\n    yd : int\n        Y-coordinate dimension of the DQ array\n\n    mosaic : obj\n        2D ``numpy`` array containing the mosaic image\n\n    module : str\n        Module used for mosaic. Options are ``LW``,`` SW``, ``SWA``,\n        ``SWB``\n\n    Returns\n    -------\n    dq : obj\n        2D ``numpy`` array containing the DQ array. Pixels that are\n        ``True`` are considered science pixels and are used when\n        scaling the preview image. Pixels that are ``False`` are\n        skipped.\n    """"""\n\n    # Create array\n    dq = np.ones((yd, xd), dtype=""bool"")\n\n    # Flag inter-chip and inter-module pixels as False\n    dq[np.isnan(mosaic)] = 0\n\n    # Flag reference pixels as False\n\n    # Present in all cases other than subarrays\n    if xd >= FULLX:\n        dq[0:4, :] = 0\n        dq[:, 0:4] = 0\n        dq[2044:2048, :] = 0\n        dq[:, 2044:2048] = 0\n\n        if module == ""LW"":\n            lwb_lower = FULLX + LW_MOD_GAP\n            dq[:, lwb_lower:lwb_lower + 4] = 0\n            dq[:, lwb_lower + 2044:lwb_lower + 2048] = 0\n\n        if module in [""SWA"", ""SWB"", ""SW""]:\n            # Present for full frame single module or both modules\n            lowerval = FULLX + SW_DET_GAP\n            dq[lowerval: lowerval + 4, :] = 0\n            dq[(lowerval + 2044):, :] = 0\n            dq[:, lowerval: lowerval + 4] = 0\n            dq[:, (lowerval + 2044):(lowerval + 2048)] = 0\n\n        if module == ""SW"":\n            # Present only if both modules are used (full frame)\n            modb_lower = lowerval + FULLX + SW_MOD_GAP\n            dq[:, modb_lower:(modb_lower + 4)] = 0\n            dq[:, (modb_lower + 2044):(modb_lower + 2048)] = 0\n            modb_upper = modb_lower + FULLX + SW_DET_GAP\n            dq[:, modb_upper:(modb_upper + 4)] = 0\n            dq[:, (modb_upper + 2044):(modb_upper + 2048)] = 0\n\n    else:\n        # Subarrays: expand the pixels flagged due to chip gaps\n        # by one row and column\n        nan_indexes = np.where(np.isnan(mosaic))\n        match = nan_indexes[0] == yd - 1\n        vert_xmin = np.min(nan_indexes[1][match])\n        vert_xmax = vert_xmin + SW_DET_GAP - 1\n\n        match2 = nan_indexes[1] == xd - 1\n        horiz_ymin = np.min(nan_indexes[0][match2])\n        horiz_ymax = horiz_ymin + SW_DET_GAP - 1\n\n        dq[:, vert_xmin - 4:vert_xmin] = 0\n        dq[:, vert_xmax + 1:vert_xmax + 5] = 0\n        dq[horiz_ymin - 4:horiz_ymin, :] = 0\n        dq[horiz_ymax + 1:horiz_ymax + 5, :] = 0\n\n    return dq\n\n\ndef detector_check(detector_list, search_string):\n    """"""Search a given list of detector names for the provided regular\n    expression sting.\n\n    Parameters\n    ----------\n    detector_list : list\n        List of detector names (e.g. ``NRCA5``)\n\n    search_string : str\n        Regular expression string to use for search\n\n    Returns\n    -------\n    total : int\n        Number of detectors in ``detector_list`` that match\n        ``search_string``\n    """"""\n\n    pattern = re.compile(search_string, re.IGNORECASE)\n    match = [pattern.match(detector) for detector in detector_list]\n    total = np.sum(np.array([m is not None for m in match]))\n\n    return total\n\n\ndef find_data_channel(detectors):\n    """"""Using a list of detectors, identify the channel(s) that the data\n    are from.\n\n    Parameters\n    ----------\n    detectors : list\n        List of detector names\n\n    Returns\n    -------\n    channel : str\n        Identifier noting which channels the given detectors are in.\n        Can be ``SWA`` for shortwave, module A only, ``SWB`` for\n        shortwave, module B only, ``SW``, for shortwave modules A and B,\n        and ``LW`` for longwave.\n    """"""\n\n    # Check the detector names for all files.\n    nrc_swa_total = detector_check(detectors, ""NRCA[1-4]"")\n    nrc_swb_total = detector_check(detectors, ""NRCB[1-4]"")\n    nrc_lw_total = detector_check(detectors, ""NRC[AB]5"")\n\n    both_channels = ""Can\'t mix NIRCam SW and LW data in same mosaic.""\n    if nrc_swa_total != 0:\n        if nrc_lw_total != 0:\n            raise ValueError(both_channels)\n        else:\n            if nrc_swb_total != 0:\n                channel = ""SW""\n            else:\n                channel = ""SWA""\n    else:\n        if nrc_swb_total != 0:\n            if nrc_lw_total != 0:\n                raise ValueError(both_channels)\n            else:\n                channel = ""SWB""\n        else:\n            if nrc_lw_total != 0:\n                channel = ""LW""\n            else:\n                raise ValueError(""No NIRCam SW nor LW data"")\n    return channel\n\n\ndef get_base_output_name(filename_dict):\n    """"""Returns the base output name used for preview images and\n    thumbnails.\n\n    Parameters\n    ----------\n    filename_dict : dict\n        A dictionary containing parsed filename parts via\n        ``filename_parser``\n\n    Returns\n    -------\n    base_output_name : str\n        The base output name, e.g. ``jw96090001002_03101_00001_nrca2_rate``\n    """"""\n\n    base_output_name = \'jw{}{}{}_{}{}{}_{}_\'.format(\n        filename_dict[\'program_id\'], filename_dict[\'observation\'],\n        filename_dict[\'visit\'], filename_dict[\'visit_group\'],\n        filename_dict[\'parallel_seq_id\'], filename_dict[\'activity\'],\n        filename_dict[\'exposure_id\'])\n\n    return base_output_name\n\n\n@log_fail\n@log_info\ndef generate_preview_images():\n    """"""The main function of the ``generate_preview_image`` module.\n    See module docstring for further details.""""""\n\n    # Begin logging\n    logging.info(""Beginning the script run"")\n\n    # Process programs in parallel\n    program_list = [os.path.basename(item) for item in glob.glob(os.path.join(get_config()[\'filesystem\'], \'*\'))]\n    pool = multiprocessing.Pool(processes=int(get_config()[\'cores\']))\n    pool.map(process_program, program_list)\n    pool.close()\n    pool.join()\n\n    # Complete logging:\n    logging.info(""Completed."")\n\n\ndef group_filenames(filenames):\n    """"""Given a list of JWST filenames, group together files from the\n    same exposure. These files will share the same ``program_id``,\n    ``observation``, ``visit``, ``visit_group``, ``parallel_seq_id``,\n    ``activity``, ``exposure``, and ``suffix``. Only the ``detector``\n    will be different. Currently only NIRCam files for a given exposure\n    will be grouped together. For other instruments multiple files for\n    a given exposure will be kept separate from one another and no\n    mosaic will be made.  Stage 3 files will remain as individual\n    files, and will not be grouped together with any other files.\n\n    Parameters\n    ----------\n    filenames : list\n        list of filenames\n\n    Returns\n    -------\n    grouped : list\n        grouped list of filenames where each element is a list and\n        contains the names of filenames with matching exposure\n        information.\n    """"""\n\n    # Some initializations\n    grouped, matched_names = [], []\n    filenames.sort()\n\n    # Loop over each file in the list of good files\n    for filename in filenames:\n\n        # Holds list of matching files for exposure\n        subgroup = []\n\n        # Generate string to be matched with other filenames\n        filename_dict = filename_parser(os.path.basename(filename))\n\n        # If the filename was already involved in a match, then skip\n        if filename not in matched_names:\n\n            # For stage 3 filenames, treat individually\n            if \'stage_3\' in filename_dict[\'filename_type\']:\n                matched_names.append(filename)\n                subgroup.append(filename)\n\n            # Group together stage 1 and 2 filenames\n            elif filename_dict[\'filename_type\'] == \'stage_1_and_2\':\n\n                # Determine detector naming convention\n                if filename_dict[\'detector\'].upper() in NIRCAM_SHORTWAVE_DETECTORS:\n                    detector_str = \'NRC[AB][1234]\'\n                elif filename_dict[\'detector\'].upper() in NIRCAM_LONGWAVE_DETECTORS:\n                    detector_str = \'NRC[AB]5\'\n                else:  # non-NIRCam detectors\n                    detector_str = filename_dict[\'detector\'].upper()\n\n                # Build pattern to match against\n                base_output_name = get_base_output_name(filename_dict)\n                match_str = \'{}{}_{}.fits\'.format(base_output_name, detector_str, filename_dict[\'suffix\'])\n                match_str = os.path.join(os.path.dirname(filename), match_str)\n                pattern = re.compile(match_str, re.IGNORECASE)\n\n                # Try to match the substring to each good file\n                for file_to_match in filenames:\n                    if pattern.match(file_to_match) is not None:\n                        matched_names.append(file_to_match)\n                        subgroup.append(file_to_match)\n\n        if len(subgroup) > 0:\n            grouped.append(subgroup)\n\n    return grouped\n\n\ndef process_program(program):\n    """"""Generate preview images and thumbnails for the given program.\n\n    Parameters\n    ----------\n    program : str\n        The program identifier (e.g. ``88600``)\n    """"""\n\n    # Group together common exposures\n    filenames = glob.glob(os.path.join(get_config()[\'filesystem\'], program, \'*.fits\'))\n    grouped_filenames = group_filenames(filenames)\n    logging.info(\'Found {} filenames\'.format(len(filenames)))\n\n    for file_list in grouped_filenames:\n        filename = file_list[0]\n\n        # Determine the save location\n        try:\n            identifier = \'jw{}\'.format(filename_parser(filename)[\'program_id\'])\n        except ValueError:\n            identifier = os.path.basename(filename).split(\'.fits\')[0]\n        preview_output_directory = os.path.join(get_config()[\'preview_image_filesystem\'], identifier)\n        thumbnail_output_directory = os.path.join(get_config()[\'thumbnail_filesystem\'], identifier)\n\n        # Check to see if the preview images already exist and skip if they do\n        file_exists = check_existence(file_list, preview_output_directory)\n        if file_exists:\n            logging.info(""JPG already exists for {}, skipping."".format(filename))\n            continue\n\n        # Create the output directories if necessary\n        if not os.path.exists(preview_output_directory):\n            os.makedirs(preview_output_directory)\n            permissions.set_permissions(preview_output_directory)\n            logging.info(\'Created directory {}\'.format(preview_output_directory))\n        if not os.path.exists(thumbnail_output_directory):\n            os.makedirs(thumbnail_output_directory)\n            permissions.set_permissions(thumbnail_output_directory)\n            logging.info(\'Created directory {}\'.format(thumbnail_output_directory))\n\n        # If the exposure contains more than one file (because more\n        # than one detector was used), then create a mosaic\n        max_size = 8\n        numfiles = len(file_list)\n        if numfiles > 1:\n            try:\n                mosaic_image, mosaic_dq = create_mosaic(file_list)\n                logging.info(\'Created mosiac for:\')\n                for item in file_list:\n                    logging.info(\'\\t{}\'.format(item))\n            except (ValueError, FileNotFoundError) as error:\n                logging.error(error)\n            dummy_file = create_dummy_filename(file_list)\n            if numfiles in [2, 4]:\n                max_size = 16\n            elif numfiles in [8]:\n                max_size = 32\n\n        # Create the nominal preview image and thumbnail\n        try:\n            im = PreviewImage(filename, ""SCI"")\n            im.clip_percent = 0.01\n            im.scaling = \'log\'\n            im.cmap = \'viridis\'\n            im.output_format = \'jpg\'\n            im.preview_output_directory = preview_output_directory\n            im.thumbnail_output_directory = thumbnail_output_directory\n\n            # If a mosaic was made from more than one file\n            # insert it and it\'s associated DQ array into the\n            # instance of PreviewImage. Also set the input\n            # filename to indicate that we have mosaicked data\n            if numfiles != 1:\n                im.data = mosaic_image\n                im.dq = mosaic_dq\n                im.file = dummy_file\n\n            im.make_image(max_img_size=max_size)\n            logging.info(\'Created preview image and thumbnail for: {}\'.format(filename))\n        except ValueError as error:\n            logging.warning(error)\n\n\nif __name__ == \'__main__\':\n\n    module = os.path.basename(__file__).strip(\'.py\')\n    configure_logging(module)\n\n    generate_preview_images()\n'"
jwql/jwql_monitors/monitor_cron_jobs.py,3,"b'#! /usr/bin/env python\n\n""""""This module monitors the status of the ``jwql`` monitors via their\nlog files. Basic results (e.g. ``success``, ``failure``) are collected\nand placed in a ``bokeh`` table for display on the web app.\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\nUse\n---\n\n    This module can be executed as such:\n\n    ::\n\n        from jwql.jwql_monitors import monitor_cron_jobs\n        monitor_cron_jobs.status()\n\nDependencies\n------------\n\n    The user must have a configuration file named ``config.json``\n    placed in the ``utils`` directory.\n""""""\n\nfrom datetime import datetime\nimport logging\nimport numpy as np\nimport os\nimport time\n\nfrom bokeh.io import save, output_file\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.models.widgets import DataTable, DateFormatter, HTMLTemplateFormatter, TableColumn\n\nfrom jwql.utils.logging_functions import configure_logging, log_info, log_fail\nfrom jwql.utils.permissions import set_permissions\nfrom jwql.utils.utils import get_config\n\n\ndef create_table(status_dict):\n    """"""Create interactive ``bokeh`` table containing the logfile status\n    results.\n\n    Parameters\n    ----------\n    status_dict : dict\n        Nested dictionary with status results from all logfiles\n    """"""\n    # Rearrange the nested dictionary into a non-nested dict for the table\n    filenames = []\n    dates = []\n    missings = []\n    results = []\n    for key in status_dict:\n        filenames.append(status_dict[key][\'logname\'])\n        dates.append(datetime.fromtimestamp(status_dict[key][\'latest_time\']))\n        missings.append(str(status_dict[key][\'missing_file\']))\n        results.append(status_dict[key][\'status\'])\n\n    # div to color the boxes in the status column\n    success_template = """"""\n    <div style=""background:<%=\n        (function colorfromstr(){\n            if(value == ""success""){\n                return(""green"")}\n            else{return(""red"")}\n            }()) %>;\n        color: white"">\n    <%= value %></div>\n    """"""\n\n    # div to color the boxes in the column for possibly late logfiles\n    missing_template = """"""\n    <div style=""background:<%=\n        (function colorfrombool(){\n            if(value == ""True""){\n                return(""orange"")}\n            else{return(""green"")}\n            }()) %>;\n        color: white"">\n    <%= value %></div>\n    """"""\n    success_formatter = HTMLTemplateFormatter(template=success_template)\n    missing_formatter = HTMLTemplateFormatter(template=missing_template)\n\n    data = dict(name=list(status_dict.keys()), filename=filenames, date=dates, missing=missings,\n                result=results)\n    source = ColumnDataSource(data)\n\n    datefmt = DateFormatter(format=""RFC-2822"")\n    columns = [\n        TableColumn(field=""name"", title=""Monitor Name"", width=200),\n        TableColumn(field=""filename"", title=""Most Recent File"", width=350),\n        TableColumn(field=""date"", title=""Most Recent Time"", width=200, formatter=datefmt),\n        TableColumn(field=""missing"", title=""Possible Missing File"", width=200, formatter=missing_formatter),\n        TableColumn(field=""result"", title=""Status"", width=100, formatter=success_formatter),\n    ]\n    data_table = DataTable(source=source, columns=columns, width=800, height=280, index_position=None)\n\n    # Get output directory for saving the table files\n    output_dir = get_config()[\'outputs\']\n    output_filename = \'cron_status_table\'\n\n    # Save full html\n    html_outfile = os.path.join(output_dir, \'monitor_cron_jobs\', \'{}.html\'.format(output_filename))\n    output_file(html_outfile)\n    save(data_table)\n    try:\n        set_permissions(html_outfile)\n    except PermissionError:\n        logging.warning(\'Unable to set permissions for {}\'.format(html_outfile))\n    logging.info(\'Saved Bokeh full HTML file: {}\'.format(html_outfile))\n\n\ndef find_latest(logfiles):\n    """"""Given a list of log files in a directory, identify the most\n    recent. The way that ``jwql.utils.logging_functions.make_log_file``\n    is set up, log files for all monitors are guaranteed to be the name\n    of the monitor followed by the datetime that they were run, so we\n    should be able to simply sort the filenames and the last will be the\n    most recent.\n\n    Parameters\n    ----------\n    logfiles : list\n        List of logfiles in the directory\n\n    Returns\n    -------\n    latest : str\n        Filename of the most recent file\n\n    latest_time : float\n        Time associated with the most recent log file\n    """"""\n    latest = sorted(logfiles)[-1]\n    latest_time = os.path.getctime(latest)\n    return (latest, latest_time)\n\n\ndef get_cadence(filenames):\n    """"""Calculate the cadence of the log files in a given directory.\n    Use timestamps\n\n    Parameters\n    ----------\n    filenames : list\n        List of log files to examine\n\n    Returns\n    -------\n    mean_delta : float\n        Mean time in seconds between the appearance of consecutive log\n        files\n\n    stdev_delta : float\n        Standard deviation in seconds between the appearance of\n        consecutive log files\n    """"""\n    minimum_log_num = 3  # Set to a low value for now since we don\'t have many logfiles\n    times = [os.path.getctime(filename) for filename in filenames]\n    if len(times) >= minimum_log_num:\n        sorted_times = np.array(sorted(times))\n        delta_times = sorted_times[1:] - sorted_times[0:-1]\n        mean_delta = np.mean(delta_times)\n        stdev_delta = np.std(delta_times)\n    else:\n        # If there are < minimum_log_num logfiles, then let\'s assume we can\'t\n        # get a reliable measure of cadence. Fall back to a value of\n        # 1 year between files, to avoid accidentally flagging this monitor\n        # as running late in the subsequent check\n        mean_delta = 31556736.0  # sec per year\n        stdev_delta = 31556736.0  # sec per year\n    return mean_delta, stdev_delta\n\n\ndef missing_file_check(avg_time_between, uncertainty, latest_file):\n    """"""Given the name of the most recent log file, along with the\n    historical average time between files and the stdev of the time\n    between files, determine whether we expect a more recent log file\n    than the file given. This could hint at a problem with the cron job\n    used to create the log files.\n\n    Parameters\n    ----------\n    avg_time_between : float\n        Average number of seconds between log files\n\n    uncertainty : float\n        Standard deviation of the number of seconds between log files\n\n    latest_file : str\n        Name of the most recent log file\n\n    Returns\n    -------\n    late : bool\n        True = We expect a more recent file than that given\n        False =  It is reasonable that the file given is the most\n        recent\n    """"""\n    latest_time = os.path.getctime(latest_file)\n    now = time.time()\n    time_since_latest = now - latest_time\n    if time_since_latest > (avg_time_between + 3 * uncertainty):\n        late = True\n    else:\n        late = False\n    return late\n\n\n@log_fail\n@log_info\ndef status(production_mode=True):\n    """"""Main function: determine the status of the instrument montiors\n    by examining log files.\n\n    Parameters\n    ----------\n    production_mode : bool\n        If ``True``, look in the main log directory. If ``False``, look\n        in the ``dev`` log file directory.\n\n    Returns\n    -------\n    logfile_status : dict\n        Nested dictionary containing the status for all monitors. Top\n        level keys include all monitors. Within a given monitor, the\n        value is a dictionary containing \'missing_file\' and \'status\'\n        keys. \'missing_file\' is a boolean describing whether or not\n        there is a suspected missing log file based on the timestamps\n        of the existing files. \'status\' is a string that is either\n        \'success\' or \'failure\'.\n    """"""\n    # Begin logging\n    logging.info(""Beginning cron job status monitor"")\n\n    # Get main logfile path\n    log_path = get_config()[\'log_dir\']\n\n    # If we are in development mode, the log files are in a slightly\n    # different location than in production mode\n    if production_mode:\n        log_path = os.path.join(log_path, \'prod\')\n    else:\n        log_path = os.path.join(log_path, \'dev\')\n\n    # Set up a dictionary to keep track of results\n    logfile_status = {}\n\n    # Get a list of the directories under the main logging directory.\n    generator = os.walk(log_path, topdown=True)\n\n    # Loop over monitors\n    for subdir, subsubdir, filenames in generator:\n        # When running in production mode, skip the \'dev\' subdirectory,\n        # as it contains the development version of the monitor logs\n        if production_mode:\n            subsubdir[:] = [dirname for dirname in subsubdir if dirname != \'dev\']\n\n        if len(filenames) > 0:\n            monitor_name = subdir.split(\'/\')[-1]\n\n            # Avoid monitor_cron_jobs itseft\n            if monitor_name != \'monitor_cron_jobs\':\n\n                log_file_list = [os.path.join(subdir, filename) for filename in filenames]\n\n                # Find the cadence of the monitor\n                delta_time, stdev_time = get_cadence(log_file_list)\n\n                # Identify the most recent log file\n                latest_log, latest_log_time = find_latest(log_file_list)\n\n                # Check to see if we expect a file more recent than the latest\n                missing_file = missing_file_check(delta_time, stdev_time, latest_log)\n                if missing_file:\n                    logging.warning(\'Expected a more recent {} logfile than {}\'\n                                    .format(monitor_name, os.path.basename(latest_log)))\n\n                # Check the file for success/failure\n                result = success_check(latest_log)\n                logging.info(\'{}: Latest log file indicates {}\'.format(monitor_name, result))\n\n                # Add results to the dictionary\n                logfile_status[monitor_name] = {\'logname\': os.path.basename(latest_log),\n                                                \'latest_time\': latest_log_time,\n                                                \'missing_file\': missing_file, \'status\': result}\n\n    # Create table of results using Bokeh\n    create_table(logfile_status)\n    logging.info(\'Cron job status monitor completed successfully.\')\n\n\ndef success_check(filename):\n    """"""Parse the given log file and check whether the script execution\n    was successful or not\n\n    Parameters\n    ----------\n    filename : str\n        Name of the log file to parse\n\n    Returns\n    -------\n    execution : str\n        ``success`` or ``failure``\n    """"""\n    with open(filename, \'r\') as file_obj:\n        all_lines = file_obj.readlines()\n    final_line = all_lines[-1]\n    if \'complete\' in final_line.lower():\n        execution = \'success\'\n    else:\n        execution = \'failure\'\n    return execution\n\n\nif __name__ == \'__main__\':\n\n    module = os.path.basename(__file__).strip(\'.py\')\n    configure_logging(module)\n\n    status()\n'"
jwql/jwql_monitors/monitor_filesystem.py,0,"b'#! /usr/bin/env python\n\n""""""This module monitors and gather statistics of the filesystem and\ncentral storage area that hosts data for the ``jwql`` application.\nThis will answer questions such as the total number of files, how much\ndisk space is being used, and then plot these values over time.\n\nAuthors\n-------\n\n    - Misty Cracraft\n    - Sara Ogaz\n    - Matthew Bourque\n\nUse\n---\n\n    This module is intended to be executed from the command line:\n\n    ::\n\n        python monitor_filesystem.py\n\n    The user must have a ``config.json`` file in the ``utils``\n    directory with the following keys:\n      - ``filesystem`` - The path to the filesystem\n      - ``outputs`` - The path to where the output plots will be\n                      written\n\nDependencies\n------------\n\n    The user must have a configuration file named ``config.json``\n    placed in the ``utils`` directory.\n""""""\n\nfrom collections import defaultdict\nimport datetime\nimport itertools\nimport logging\nimport os\nimport subprocess\n\nfrom bokeh.embed import components\nfrom bokeh.layouts import gridplot\nfrom bokeh.palettes import Category20_20 as palette\nfrom bokeh.plotting import figure, output_file, save\n\nfrom jwql.database.database_interface import engine\nfrom jwql.database.database_interface import session\nfrom jwql.database.database_interface import FilesystemGeneral\nfrom jwql.database.database_interface import FilesystemInstrument\nfrom jwql.database.database_interface import CentralStore\nfrom jwql.utils.logging_functions import configure_logging, log_info, log_fail\nfrom jwql.utils.permissions import set_permissions\nfrom jwql.utils.constants import FILE_SUFFIX_TYPES, JWST_INSTRUMENT_NAMES, JWST_INSTRUMENT_NAMES_MIXEDCASE\nfrom jwql.utils.utils import filename_parser\nfrom jwql.utils.utils import get_config\n\nFILESYSTEM = get_config()[\'filesystem\']\nCENTRAL = get_config()[\'jwql_dir\']\n\n\ndef gather_statistics(general_results_dict, instrument_results_dict):\n    """"""Walks the filesytem to gather various statistics to eventually\n    store in the database\n\n    Parameters\n    ----------\n    general_results_dict : dict\n        A dictionary for the ``filesystem_general`` database table\n    instrument_results_dict : dict\n        A dictionary for the ``filesystem_instrument`` database table\n\n    Returns\n    -------\n    general_results_dict : dict\n        A dictionary for the ``filesystem_general`` database table\n    instrument_results_dict : dict\n        A dictionary for the ``filesystem_instrument`` database table\n    """"""\n\n    logging.info(\'Searching filesystem...\')\n\n    for dirpath, _, files in os.walk(FILESYSTEM):\n        general_results_dict[\'total_file_count\'] += len(files)\n        for filename in files:\n\n            file_path = os.path.join(dirpath, filename)\n            general_results_dict[\'total_file_size\'] += os.path.getsize(file_path)\n\n            if filename.endswith("".fits""):\n\n                # Parse out filename information\n                filename_dict = filename_parser(filename)\n                filetype = filename_dict[\'suffix\']\n                instrument = filename_dict[\'instrument\']\n\n                # Populate general stats\n                general_results_dict[\'fits_file_count\'] += 1\n                general_results_dict[\'fits_file_size\'] += os.path.getsize(file_path)\n\n                # Populate instrument specific stats\n                if instrument not in instrument_results_dict:\n                    instrument_results_dict[instrument] = {}\n                if filetype not in instrument_results_dict[instrument]:\n                    instrument_results_dict[instrument][filetype] = {}\n                    instrument_results_dict[instrument][filetype][\'count\'] = 0\n                    instrument_results_dict[instrument][filetype][\'size\'] = 0\n                instrument_results_dict[instrument][filetype][\'count\'] += 1\n                instrument_results_dict[instrument][filetype][\'size\'] += os.path.getsize(file_path) / (2**40)\n\n    # Convert file sizes to terabytes\n    general_results_dict[\'total_file_size\'] = general_results_dict[\'total_file_size\'] / (2**40)\n    general_results_dict[\'fits_file_size\'] = general_results_dict[\'fits_file_size\'] / (2**40)\n\n    logging.info(\'{} fits files found in filesystem\'.format(general_results_dict[\'fits_file_count\']))\n\n    return general_results_dict, instrument_results_dict\n\n\ndef get_global_filesystem_stats(general_results_dict):\n    """"""Gathers ``used`` and ``available`` ``df``-style stats on the\n    entire filesystem. (Not just directory titled filesystem.)\n\n    Parameters\n    ----------\n    general_results_dict : dict\n        A dictionary for the ``filesystem_general`` database table\n\n    Returns\n    -------\n    general_results_dict : dict\n        A dictionary for the ``filesystem_general`` database table\n    """"""\n\n    command = ""df -k {}"".format(FILESYSTEM)\n    command += "" | awk \'{print $3, $4}\' | tail -n 1""\n    stats = subprocess.check_output(command, shell=True).split()\n    general_results_dict[\'used\'] = int(stats[0]) / (1024**3)\n    general_results_dict[\'available\'] = int(stats[1]) / (1024**3)\n\n    return general_results_dict\n\n\ndef get_area_stats(central_storage_dict):\n    """"""Gathers ``used`` and ``available`` ``df``-style stats on the\n    selected area.\n\n    Parameters\n    ----------\n    central_storage_dict : dict\n        A dictionary for the ``central_storage`` database table\n\n    Returns\n    -------\n    central_storage_dict : dict\n        A dictionary for the ``central_storage`` database table\n    """"""\n    logging.info(\'Searching central storage system...\')\n\n    arealist = [\'logs\', \'outputs\', \'test\', \'preview_images\', \'thumbnails\', \'all\']\n    counteddirs = []\n\n    sums = 0  # to be used to count \'all\'\n    for area in arealist:\n\n        used = 0\n        # initialize area in dictionary\n        if area not in central_storage_dict:\n            central_storage_dict[area] = {}\n\n        if area == \'all\':\n            fullpath = CENTRAL\n        else:\n            fullpath = os.path.join(CENTRAL, area)\n\n        logging.info(\'Searching directory {}\'.format(fullpath))\n        counteddirs.append(fullpath)\n\n        # to get df stats, use -k to get 1024 byte blocks\n        command = ""df -k {}"".format(fullpath)\n        command += "" | awk \'{print $2, $3, $4}\' | tail -n 1""\n        stats = subprocess.check_output(command, shell=True).split()\n        # to put in TB, have to multiply values by 1024 to get in bytes, then\n        # divide by 1024 ^ 4 to put in TB\n        total = int(stats[0]) / (1024 ** 3)\n        free = int(stats[2]) / (1024 ** 3)\n        central_storage_dict[area][\'size\'] = total\n        central_storage_dict[area][\'available\'] = free\n\n        # do an os.walk on each directory to count up used space\n        if area == \'all\':\n            # get listing of subdirectories\n            subdirs = [f.path for f in os.scandir(fullpath) if f.is_dir()]\n            for onedir in subdirs:\n                if onedir not in counteddirs:\n                    logging.info(\'Searching directory {}\'.format(onedir))\n                    for dirpath, _, files in os.walk(onedir):\n                        for filename in files:\n                            file_path = os.path.join(dirpath, filename)\n                            # Check if file_path exists, if so, add to used space\n                            exists = os.path.isfile(file_path)\n                            if exists:\n                                filesize = os.path.getsize(file_path)\n                                sums += filesize\n            use = sums / (1024 ** 4)\n        else:\n            for dirpath, _, files in os.walk(fullpath):\n                for filename in files:\n                    file_path = os.path.join(dirpath, filename)\n                    # Check if file_path exists, if so, add to used space\n                    exists = os.path.isfile(file_path)\n                    if exists:\n                        filesize = os.path.getsize(file_path)\n                        used += filesize\n                        sums += filesize\n            use = used / (1024 ** 4)\n        central_storage_dict[area][\'used\'] = use\n\n    logging.info(\'Finished searching central storage system\')\n    return central_storage_dict\n\n\ndef initialize_results_dicts():\n    """"""Initializes dictionaries that will hold filesystem statistics\n\n    Returns\n    -------\n    general_results_dict : dict\n        A dictionary for the ``filesystem_general`` database table\n    instrument_results_dict : dict\n        A dictionary for the ``filesystem_instrument`` database table\n    central_storage_dict : dict\n        A dictionary for the ``central_storage`` database table\n    """"""\n\n    now = datetime.datetime.now()\n\n    general_results_dict = {}\n    general_results_dict[\'date\'] = now\n    general_results_dict[\'total_file_count\'] = 0\n    general_results_dict[\'fits_file_count\'] = 0\n    general_results_dict[\'total_file_size\'] = 0\n    general_results_dict[\'fits_file_size\'] = 0\n\n    instrument_results_dict = {}\n    instrument_results_dict[\'date\'] = now\n\n    central_storage_dict = {}\n    central_storage_dict[\'date\'] = now\n\n    return general_results_dict, instrument_results_dict, central_storage_dict\n\n\n@log_fail\n@log_info\ndef monitor_filesystem():\n    """"""\n    Tabulates the inventory of the JWST filesystem, saving statistics\n    to database tables, and generates plots.\n    """"""\n\n    logging.info(\'Beginning filesystem monitoring.\')\n\n    # Initialize dictionaries for database input\n    general_results_dict, instrument_results_dict, central_storage_dict = initialize_results_dicts()\n\n    # Walk through filesystem recursively to gather statistics\n    general_results_dict, instrument_results_dict = gather_statistics(general_results_dict, instrument_results_dict)\n\n    # Get df style stats on file system\n    general_results_dict = get_global_filesystem_stats(general_results_dict)\n\n    # Get stats on central storage areas\n    central_storage_dict = get_area_stats(central_storage_dict)\n\n    # Add data to database tables\n    update_database(general_results_dict, instrument_results_dict, central_storage_dict)\n\n    # Create the plots\n    plot_filesystem_stats()\n\n    logging.info(""Completed."")\n\n\ndef plot_by_filetype(plot_type, instrument):\n    """"""Plot ``count`` or ``size`` by filetype versus date for the given\n    instrument, or all instruments.\n\n    Parameters\n    ----------\n    plot_type : str\n        Which data to plot.  Either ``count`` or ``size``.\n    instrument : str\n        The instrument to plot for.  Can be a valid JWST instrument or\n        ``all`` to plot across all instruments.\n\n    Returns\n    -------\n    plot : bokeh.plotting.figure.Figure object\n        ``bokeh`` plot of total file counts versus date\n    """"""\n\n    # Determine plot title\n    if instrument == \'all\':\n        title = \'Total File {} by Type\'.format(plot_type.capitalize())\n    else:\n        instrument_title = JWST_INSTRUMENT_NAMES_MIXEDCASE[instrument]\n        title = \'{} Total File {} by Type\'.format(instrument_title, plot_type.capitalize())\n\n    if plot_type == \'count\':\n        ytitle = \'Counts\'\n    else:\n        ytitle = \'Size (TB)\'\n\n    # Initialize plot\n    plot = figure(\n        tools=\'pan,box_zoom,wheel_zoom,reset,save\',\n        x_axis_type=\'datetime\',\n        title=title,\n        x_axis_label=\'Date\',\n        y_axis_label=ytitle)\n    colors = itertools.cycle(palette)\n\n    for filetype, color in zip(FILE_SUFFIX_TYPES, colors):\n\n        # Query for counts\n        results = session.query(FilesystemInstrument.date, getattr(FilesystemInstrument, plot_type))\\\n                                .filter(FilesystemInstrument.filetype == filetype)\n\n        if instrument == \'all\':\n            results = results.all()\n        else:\n            results = results.filter(FilesystemInstrument.instrument == instrument).all()\n\n        # Group by date\n        if results:\n            results_dict = defaultdict(int)\n            for date, value in results:\n                results_dict[date] += value\n\n            # Parse results so they can be easily plotted\n            dates = list(results_dict.keys())\n            values = list(results_dict.values())\n\n            # Plot the results\n            plot.line(dates, values, legend=\'{} files\'.format(filetype), line_color=color)\n            plot.circle(dates, values, color=color)\n\n    return plot\n\n\ndef plot_filesystem_size():\n    """"""Plot filesystem sizes (size, used, available) versus date\n\n    Returns\n    -------\n    plot : bokeh.plotting.figure.Figure object\n        ``bokeh`` plot of total file counts versus date\n    """"""\n\n    # Plot system stats vs. date\n    results = session.query(FilesystemGeneral.date, FilesystemGeneral.total_file_size,\n                            FilesystemGeneral.used, FilesystemGeneral.available).all()\n    dates, total_sizes, useds, availables = zip(*results)\n    plot = figure(\n        tools=\'pan,box_zoom,wheel_zoom,reset,save\',\n        x_axis_type=\'datetime\',\n        title=\'System stats\',\n        x_axis_label=\'Date\',\n        y_axis_label=\'Size TB\')\n    plot.line(dates, total_sizes, legend=\'Total size\', line_color=\'red\')\n    plot.circle(dates, total_sizes, color=\'red\')\n    plot.line(dates, useds, legend=\'Used bytes\', line_color=\'green\')\n    plot.circle(dates, useds, color=\'green\')\n    plot.line(dates, availables, legend=\'Free bytes\', line_color=\'blue\')\n    plot.circle(dates, availables, color=\'blue\')\n\n    return plot\n\n\ndef plot_central_store_dirs():\n    """"""Plot central store sizes (size, used, available) versus date\n\n        Returns\n        -------\n        plot : bokeh.plotting.figure.Figure object\n            ``bokeh`` plot of total directory size versus date\n        """"""\n\n    # Plot system stats vs. date\n    results = session.query(CentralStore.date, CentralStore.size, CentralStore.available).all()\n\n    arealist = [\'logs\', \'outputs\', \'test\', \'preview_images\', \'thumbnails\', \'all\']\n\n    # Initialize plot\n    dates, total_sizes, availables = zip(*results)\n    plot = figure(\n        tools=\'pan,box_zoom,wheel_zoom,reset,save\',\n        x_axis_type=\'datetime\',\n        title=\'Central Store stats\',\n        x_axis_label=\'Date\',\n        y_axis_label=\'Size TB\')\n    colors = itertools.cycle(palette)\n\n    plot.line(dates, total_sizes, legend=\'Total size\', line_color=\'red\')\n    plot.circle(dates, total_sizes, color=\'red\')\n    plot.line(dates, availables, legend=\'Free\', line_color=\'blue\')\n    plot.circle(dates, availables, color=\'blue\')\n\n    # This part of the plot should cycle through areas and plot area used values vs. date\n    for area, color in zip(arealist, colors):\n\n        # Query for used sizes\n        results = session.query(CentralStore.date, CentralStore.used).filter(CentralStore.area == area)\n\n        # Group by date\n        if results:\n            results_dict = defaultdict(int)\n            for date, value in results:\n                results_dict[date] += value\n\n            # Parse results so they can be easily plotted\n            dates = list(results_dict.keys())\n            values = list(results_dict.values())\n\n            # Plot the results\n            plot.line(dates, values, legend=\'{} files\'.format(area), line_color=color)\n            plot.circle(dates, values, color=color)\n\n    return plot\n\n\ndef plot_filesystem_stats():\n    """"""\n    Plot various filesystem statistics using ``bokeh`` and save them to\n    the output directory.\n    """"""\n    logging.info(\'Starting plots.\')\n\n    p1 = plot_total_file_counts()\n    p2 = plot_filesystem_size()\n    p3 = plot_by_filetype(\'count\', \'all\')\n    p4 = plot_by_filetype(\'size\', \'all\')\n    p5 = plot_central_store_dirs()\n    plot_list = [p1, p2, p3, p4, p5]\n\n    for instrument in JWST_INSTRUMENT_NAMES:\n        plot_list.append(plot_by_filetype(\'count\', instrument))\n        plot_list.append(plot_by_filetype(\'size\', instrument))\n\n    # Create a layout with a grid pattern\n    grid_chunks = [plot_list[i:i+2] for i in range(0, len(plot_list), 2)]\n    grid = gridplot(grid_chunks)\n\n    # Save all of the plots in one file\n    outputs_dir = os.path.join(get_config()[\'outputs\'], \'monitor_filesystem\')\n    outfile = os.path.join(outputs_dir, \'filesystem_monitor.html\')\n    output_file(outfile)\n    save(grid)\n    set_permissions(outfile)\n    logging.info(\'Saved plot of all statistics to {}\'.format(outfile))\n\n    # Save each plot\'s components\n    for plot in plot_list:\n        plot_name = plot.title.text.lower().replace(\' \', \'_\')\n        plot.sizing_mode = \'stretch_both\'\n        script, div = components(plot)\n\n        div_outfile = os.path.join(outputs_dir, ""{}_component.html"".format(plot_name))\n        with open(div_outfile, \'w\') as f:\n            f.write(div)\n            f.close()\n        set_permissions(div_outfile)\n\n        script_outfile = os.path.join(outputs_dir, ""{}_component.js"".format(plot_name))\n        with open(script_outfile, \'w\') as f:\n            f.write(script)\n            f.close()\n        set_permissions(script_outfile)\n\n        logging.info(\'Saved components files: {}_component.html and {}_component.js\'.format(plot_name, plot_name))\n\n    logging.info(\'Filesystem statistics plotting complete.\')\n\n\ndef plot_total_file_counts():\n    """"""Plot total file counts versus date\n\n    Returns\n    -------\n    plot : bokeh.plotting.figure.Figure object\n        ``bokeh`` plot of total file counts versus date\n    """"""\n\n    # Total file counts vs. date\n    results = session.query(FilesystemGeneral.date, FilesystemGeneral.total_file_count).all()\n    dates, file_counts = zip(*results)\n    plot = figure(\n        tools=\'pan,box_zoom,reset,wheel_zoom,save\',\n        x_axis_type=\'datetime\',\n        title=""Total File Counts"",\n        x_axis_label=\'Date\',\n        y_axis_label=\'Count\')\n    plot.line(dates, file_counts, line_width=2, line_color=\'blue\')\n    plot.circle(dates, file_counts, color=\'blue\')\n\n    return plot\n\n\ndef update_database(general_results_dict, instrument_results_dict, central_storage_dict):\n    """"""Updates the ``filesystem_general`` and ``filesystem_instrument``\n    database tables.\n\n    Parameters\n    ----------\n    general_results_dict : dict\n        A dictionary for the ``filesystem_general`` database table\n    instrument_results_dict : dict\n        A dictionary for the ``filesystem_instrument`` database table\n    central_storage_dict : dict\n        A dictionary for the ``central_storage`` database table\n\n    """"""\n    logging.info(\'Updating databases.\')\n\n    engine.execute(FilesystemGeneral.__table__.insert(), general_results_dict)\n    session.commit()\n\n    # Add data to filesystem_instrument table\n    for instrument in JWST_INSTRUMENT_NAMES:\n        for filetype in instrument_results_dict[instrument]:\n            new_record = {}\n            new_record[\'date\'] = instrument_results_dict[\'date\']\n            new_record[\'instrument\'] = instrument\n            new_record[\'filetype\'] = filetype\n            new_record[\'count\'] = instrument_results_dict[instrument][filetype][\'count\']\n            new_record[\'size\'] = instrument_results_dict[instrument][filetype][\'size\']\n            engine.execute(FilesystemInstrument.__table__.insert(), new_record)\n            session.commit()\n\n    # Add data to central_storage table\n    arealist = [\'logs\', \'outputs\', \'test\', \'preview_images\', \'thumbnails\', \'all\']\n    for area in arealist:\n        new_record = {}\n        new_record[\'date\'] = central_storage_dict[\'date\']\n        new_record[\'area\'] = area\n        new_record[\'size\'] = central_storage_dict[area][\'size\']\n        new_record[\'used\'] = central_storage_dict[area][\'used\']\n        new_record[\'available\'] = central_storage_dict[area][\'available\']\n        engine.execute(CentralStore.__table__.insert(), new_record)\n        session.commit()\n\n\nif __name__ == \'__main__\':\n\n    # Configure logging\n    module = os.path.basename(__file__).strip(\'.py\')\n    configure_logging(module)\n\n    monitor_filesystem()'"
jwql/jwql_monitors/monitor_mast.py,0,"b'#! /usr/bin/env python\n\n""""""This module is home to a suite of MAST queries that gather bulk\nproperties of available JWST data for JWQL.\n\nAuthors\n-------\n\n    Joe Filippazzo\n\nUse\n---\n\n    To get an inventory of all JWST files do:\n    ::\n\n        from jwql.jwql_monitors import monitor_mast\n        inventory, keywords = monitor_mast.jwst_inventory()\n""""""\n\nimport logging\nimport os\n\nfrom astroquery.mast import Mast\nfrom bokeh.embed import components\nfrom bokeh.io import save, output_file\nimport pandas as pd\n\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES, JWST_DATAPRODUCTS\nfrom jwql.utils.logging_functions import configure_logging, log_info, log_fail\nfrom jwql.utils.permissions import set_permissions\nfrom jwql.utils.utils import get_config\nfrom jwql.utils.plotting import bar_chart\n\n\ndef instrument_inventory(instrument, dataproduct=JWST_DATAPRODUCTS,\n                         add_filters=None, add_requests=None,\n                         caom=False, return_data=False):\n    """"""Get the counts for a given instrument and data product\n\n    Parameters\n    ----------\n    instrument: str\n        The instrument name, i.e. one of [\'niriss\',\'nircam\',\'nirspec\',\n        \'miri\',\'fgs\']\n    dataproduct: sequence, str\n        The type of data product to search\n    add_filters: dict\n        The (\'paramName\':\'values\') pairs to include in the \'filters\'\n        argument of the request e.g. add_filters = {\'filter\':\'GR150R\'}\n    add_requests: dict\n        The (\'request\':\'value\') pairs to include in the request\n        e.g. add_requests = {\'pagesize\':1, \'page\':1}\n    caom: bool\n        Query CAOM service\n    return_data: bool\n        Return the actual data instead of counts only\n\n    Returns\n    -------\n    int, dict\n        The number of database records that satisfy the search criteria\n        or a dictionary of the data if `return_data=True`\n    """"""\n    filters = []\n\n    # Make sure the dataproduct is a list\n    if isinstance(dataproduct, str):\n        dataproduct = [dataproduct]\n\n    # Make sure the instrument is supported\n    if instrument.lower() not in [ins.lower() for ins in JWST_INSTRUMENT_NAMES]:\n        raise TypeError(\'Supported instruments include:\', JWST_INSTRUMENT_NAMES)\n\n    # CAOM service\n    if caom:\n\n        # Declare the service\n        service = \'Mast.Caom.Filtered\'\n\n        # Set the filters\n        filters += [{\'paramName\': \'obs_collection\', \'values\': [\'JWST\']},\n                    {\'paramName\': \'instrument_name\', \'values\': [instrument]},\n                    {\'paramName\': \'dataproduct_type\', \'values\': dataproduct}]\n\n    # Instruent filtered service\n    else:\n\n        # Declare the service\n        service = \'Mast.Jwst.Filtered.{}\'.format(instrument.title())\n\n    # Include additonal filters\n    if isinstance(add_filters, dict):\n        filters += [{""paramName"": name, ""values"": [val]}\n                    for name, val in add_filters.items()]\n\n    # Assemble the request\n    params = {\'columns\': \'COUNT_BIG(*)\',\n              \'filters\': filters,\n              \'removenullcolumns\': True}\n\n    # Just get the counts\n    if return_data:\n        params[\'columns\'] = \'*\'\n\n    # Add requests\n    if isinstance(add_requests, dict):\n        params.update(add_requests)\n\n    response = Mast.service_request_async(service, params)\n    result = response[0].json()\n\n    # Return all the data\n    if return_data:\n        return result\n\n    # Or just the counts\n    else:\n        return result[\'data\'][0][\'Column1\']\n\n\ndef instrument_keywords(instrument, caom=False):\n    """"""Get the keywords for a given instrument service\n\n    Parameters\n    ----------\n    instrument: str\n        The instrument name, i.e. one of [\'niriss\',\'nircam\',\'nirspec\',\n        \'miri\',\'fgs\']\n    caom: bool\n        Query CAOM service\n\n    Returns\n    -------\n    pd.DataFrame\n        A DataFrame of the keywords\n    """"""\n    # Retrieve one dataset to get header keywords\n    sample = instrument_inventory(instrument, return_data=True, caom=caom,\n                                  add_requests={\'pagesize\': 1, \'page\': 1})\n    data = [[i[\'name\'], i[\'type\']] for i in sample[\'fields\']]\n    keywords = pd.DataFrame(data, columns=(\'keyword\', \'dtype\'))\n\n    return keywords\n\n\ndef jwst_inventory(instruments=JWST_INSTRUMENT_NAMES,\n                   dataproducts=[\'image\', \'spectrum\', \'cube\'],\n                   caom=False, plot=False):\n    """"""Gather a full inventory of all JWST data in each instrument\n    service by instrument/dtype\n\n    Parameters\n    ----------\n    instruments: sequence\n        The list of instruments to count\n    dataproducts: sequence\n        The types of dataproducts to count\n    caom: bool\n        Query CAOM service\n    plot: bool\n        Return a pie chart of the data\n\n    Returns\n    -------\n    astropy.table.table.Table\n        The table of record counts for each instrument and mode\n    """"""\n    logging.info(\'Searching database...\')\n    # Iterate through instruments\n    inventory, keywords = [], {}\n    for instrument in instruments:\n        ins = [instrument]\n        for dp in dataproducts:\n            count = instrument_inventory(instrument, dataproduct=dp, caom=caom)\n            ins.append(count)\n\n        # Get the total\n        ins.append(sum(ins[-3:]))\n\n        # Add it to the list\n        inventory.append(ins)\n\n        # Add the keywords to the dict\n        keywords[instrument] = instrument_keywords(instrument, caom=caom)\n\n    logging.info(\'Completed database search for {} instruments and {} data products.\'.\n                 format(instruments, dataproducts))\n\n    # Make the table\n    all_cols = [\'instrument\'] + dataproducts + [\'total\']\n    table = pd.DataFrame(inventory, columns=all_cols)\n\n    # Plot it\n    if plot:\n        # Determine plot location and names\n        output_dir = get_config()[\'outputs\']\n\n        if caom:\n            output_filename = \'database_monitor_caom\'\n        else:\n            output_filename = \'database_monitor_jwst\'\n\n        # Make the plot\n        plt = bar_chart(table, \'instrument\', dataproducts,\n                        title=""JWST Inventory"")\n\n        # Save the plot as full html\n        html_filename = output_filename + \'.html\'\n        outfile = os.path.join(output_dir, \'monitor_mast\', html_filename)\n        output_file(outfile)\n        save(plt)\n        set_permissions(outfile)\n\n        logging.info(\'Saved Bokeh plots as HTML file: {}\'.format(html_filename))\n\n        # Save the plot as components\n        plt.sizing_mode = \'stretch_both\'\n        script, div = components(plt)\n\n        div_outfile = os.path.join(output_dir, \'monitor_mast\', output_filename + ""_component.html"")\n        with open(div_outfile, \'w\') as f:\n            f.write(div)\n            f.close()\n        set_permissions(div_outfile)\n\n        script_outfile = os.path.join(output_dir, \'monitor_mast\', output_filename + ""_component.js"")\n        with open(script_outfile, \'w\') as f:\n            f.write(script)\n            f.close()\n        set_permissions(script_outfile)\n\n        logging.info(\'Saved Bokeh components files: {}_component.html and {}_component.js\'.format(\n            output_filename, output_filename))\n\n    # Melt the table\n    table = pd.melt(table, id_vars=[\'instrument\'],\n                    value_vars=dataproducts,\n                    value_name=\'files\', var_name=\'dataproduct\')\n\n    return table, keywords\n\n\n@log_fail\n@log_info\ndef monitor_mast():\n    """"""Tabulates the inventory of all JWST data products in the MAST\n    archive and generates plots.\n    """"""\n    logging.info(\'Beginning database monitoring.\')\n\n    outputs_dir = os.path.join(get_config()[\'outputs\'], \'monitor_mast\')\n\n    # Perform inventory of the JWST service\n    jwst_inventory(instruments=JWST_INSTRUMENT_NAMES,\n                   dataproducts=[\'image\', \'spectrum\', \'cube\'],\n                   caom=False, plot=True)\n\n    # Perform inventory of the CAOM service\n    jwst_inventory(instruments=JWST_INSTRUMENT_NAMES,\n                   dataproducts=[\'image\', \'spectrum\', \'cube\'],\n                   caom=True, plot=True)\n\n\nif __name__ == \'__main__\':\n\n    # Configure logging\n    module = os.path.basename(__file__).strip(\'.py\')\n    configure_logging(module)\n\n    # Run the monitors\n    monitor_mast()\n'"
jwql/tests/__init__.py,0,b''
jwql/tests/test_api_views.py,0,"b'#!/usr/bin/env python\n\n""""""Tests for the ``api_views`` module in the ``jwql`` web application.\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    These tests can be run via the command line (omit the -s to\n    suppress verbose output to stdout):\n\n    ::\n\n        pytest -s test_api_views.py\n""""""\n\nimport http\nimport json\nimport os\nfrom urllib import request, error\n\nimport pytest\n\nfrom jwql.utils.utils import get_base_url\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n# Determine if the local server is running\ntry:\n    url = request.urlopen(\'http://127.0.0.1:8000\')\n    LOCAL_SERVER = True\nexcept error.URLError:\n    LOCAL_SERVER = False\n\nurls = []\n\n# Generic URLs\nurls.append(\'api/proposals/\')  # all_proposals\n\n# Instrument-specific URLs\nfor instrument in JWST_INSTRUMENT_NAMES:\n    urls.append(\'api/{}/proposals/\'.format(instrument))  # instrument_proposals\n    urls.append(\'api/{}/preview_images/\'.format(instrument))  # preview_images_by_instrument\n    urls.append(\'api/{}/thumbnails/\'.format(instrument))  # thumbnails_by_instrument\n\n# Proposal-specific URLs\nproposals = [\'86700\',  # FGS\n             \'98012\',  # MIRI\n             \'93025\',  # NIRCam\n             \'00308\',  # NIRISS\n             \'308\',  # NIRISS\n             \'96213\']  # NIRSpec\nfor proposal in proposals:\n    urls.append(\'api/{}/filenames/\'.format(proposal))  # filenames_by_proposal\n    urls.append(\'api/{}/preview_images/\'.format(proposal))  # preview_images_by_proposal\n    urls.append(\'api/{}/thumbnails/\'.format(proposal))  # thumbnails_by_proposal\n\n# Filename-specific URLs\nrootnames = [\'jw86600007001_02101_00001_guider2\',  # FGS\n             \'jw98012001001_02102_00001_mirimage\',  # MIRI\n             \'jw93025001001_02102_00001_nrca2\',  # NIRCam\n             \'jw00308001001_02103_00001_nis\',  # NIRISS\n             \'jw96213001001_02101_00001_nrs1\']  # NIRSpec\nfor rootname in rootnames:\n    urls.append(\'api/{}/filenames/\'.format(rootname))  # filenames_by_rootname\n    urls.append(\'api/{}/preview_images/\'.format(rootname))  # preview_images_by_rootname\n    urls.append(\'api/{}/thumbnails/\'.format(rootname))  # thumbnails_by_rootname\n\n\n@pytest.mark.parametrize(\'url\', urls)\ndef test_api_views(url):\n    """"""Test to see if the given ``url`` returns a populated JSON object\n\n    Parameters\n    ----------\n    url : str\n        The url to the api view of interest (e.g.\n        ``http://127.0.0.1:8000/api/86700/filenames/\'``).\n    """"""\n\n    # Build full URL\n    if not ON_JENKINS:\n        base_url = get_base_url()\n    else:\n        base_url = \'https://dljwql.stsci.edu\'\n\n    if base_url == \'http://127.0.0.1:8000\' and not LOCAL_SERVER:\n        pytest.skip(""Local server not running"")\n\n    url = \'{}/{}\'.format(base_url, url)\n\n    # Determine the type of data to check for based on the url\n    data_type = url.split(\'/\')[-2]\n\n    try:\n        url = request.urlopen(url)\n    except error.HTTPError as e:\n        if e.code == 502:\n            pytest.skip(""Dev server problem"")\n        raise(e)\n\n    try:\n        data = json.loads(url.read().decode())\n        assert len(data[data_type]) > 0\n    except (http.client.IncompleteRead) as e:\n        data = e.partial\n        assert len(data) > 0\n'"
jwql/tests/test_calculations.py,13,"b'#! /usr/bin/env python\n\n""""""Tests for the ``calculations`` module.\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to stdout):\n    ::\n\n        pytest -s test_calculations.py\n""""""\n\nimport numpy as np\n\nfrom jwql.utils import calculations\n\n\ndef test_double_gaussian_fit():\n    """"""Test the double Gaussian fitting function""""""\n\n    amplitude1 = 500\n    mean_value1 = 0.5\n    sigma_value1 = 0.05\n    amplitude2 = 300\n    mean_value2 = 0.4\n    sigma_value2 = 0.03\n\n    bin_centers = np.arange(0., 1.1, 0.007)\n    input_params = [amplitude1, mean_value1, sigma_value1, amplitude2, mean_value2, sigma_value2]\n    input_values = calculations.double_gaussian(bin_centers, *input_params)\n\n    initial_params = [np.max(input_values), 0.55, 0.1, np.max(input_values), 0.5, 0.05]\n    params, sigma = calculations.double_gaussian_fit(bin_centers, input_values, initial_params)\n\n    assert np.allclose(np.array(params[0:3]), np.array([amplitude2, mean_value2, sigma_value2]),\n                       atol=0, rtol=0.000001)\n    assert np.allclose(np.array(params[3:]), np.array([amplitude1, mean_value1, sigma_value1]),\n                       atol=0, rtol=0.000001)\n\n\ndef test_gaussian1d_fit():\n    """"""Test histogram fitting function""""""\n\n    mean_value = 0.5\n    sigma_value = 0.05\n    image = np.random.normal(loc=mean_value, scale=sigma_value, size=(100, 100))\n    hist, bin_edges = np.histogram(image, bins=\'auto\')\n    bin_centers = (bin_edges[1:] + bin_edges[0: -1]) / 2.\n    initial_params = [np.max(hist), 0.55, 0.1]\n    amplitude, peak, width = calculations.gaussian1d_fit(bin_centers, hist, initial_params)\n\n    assert np.isclose(peak[0], mean_value, atol=0.0035, rtol=0.)\n    assert np.isclose(width[0], sigma_value, atol=0.0035, rtol=0.)\n    assert ((mean_value <= peak[0]+7*peak[1]) & (mean_value >= peak[0]-7*peak[1]))\n    assert ((sigma_value <= width[0]+7*width[1]) & (sigma_value >= width[0]-7*width[1]))\n\n\ndef test_mean_image():\n    """"""Test the sigma-clipped mean and stdev image calculator""""""\n\n    # Create a stack of 50 5x5 pixel images\n    nstack = 50\n    cube = np.zeros((nstack, 5, 5))\n\n    # Set alternating frames equal to 4 and 5\n    for i in range(nstack):\n        if i % 2 == 0:\n            cube[i, :, :] = 4.\n        else:\n            cube[i, :, :] = 5.\n\n    # Insert a few signal values that will be removed by sigma clipping.\n    # Make sure you ""remove"" and equal number of 4\'s and 5\'s from each\n    # pixel in order to keep the mean at 4.5 and dev at 0.5\n    cube[0, 0, 0] = 55.\n    cube[1, 0, 0] = -78.\n    cube[3, 3, 3] = 150.\n    cube[2, 3, 3] = 32.\n    cube[1, 4, 4] = -96.\n    cube[4, 4, 4] = -25.\n    mean_img, dev_img = calculations.mean_image(cube, sigma_threshold=3)\n\n    assert np.all(mean_img == 4.5)\n    assert np.all(dev_img == 0.5)\n\n\ndef test_mean_stdev():\n    """"""Test calcualtion of the sigma-clipped mean from an image""""""\n\n    image = np.zeros((50, 50)) + 1.\n    badx = [1, 4, 10, 14, 16, 20, 22, 25, 29, 30]\n    bady = [13, 27, 43, 21, 1, 32, 25, 21, 9, 14]\n    for x, y in zip(badx, bady):\n        image[y, x] = 100.\n\n    meanval, stdval = calculations.mean_stdev(image, sigma_threshold=3)\n    assert meanval == 1.\n    assert stdval == 0.\n'"
jwql/tests/test_dark_monitor.py,13,"b'#! /usr/bin/env python\n\n""""""Tests for the ``dark_monitor`` module.\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to stdout):\n    ::\n\n        pytest -s test_dark_monitor.py\n""""""\n\nimport os\nimport pytest\n\nfrom astropy.time import Time\nimport numpy as np\n\nfrom jwql.instrument_monitors.common_monitors import dark_monitor\nfrom jwql.utils.utils import get_config\n\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\ndef test_find_hot_dead_pixels():\n    """"""Test hot and dead pixel searches""""""\n    monitor = dark_monitor.Dark()\n\n    # Create ""baseline"" image\n    comparison_image = np.zeros((10, 10)) + 1.\n\n    # Create mean slope image to compare\n    mean_image = np.zeros((10, 10)) + 1.\n    mean_image[0, 0] = 1.7\n    mean_image[1, 1] = 2.2\n    mean_image[7, 7] = 4.5\n    mean_image[5, 5] = 0.12\n    mean_image[6, 6] = 0.06\n    mean_image[7, 3] = 0.09\n\n    hot, dead = monitor.find_hot_dead_pixels(mean_image, comparison_image, hot_threshold=2., dead_threshold=0.1)\n    assert len(hot) == 2\n    assert np.all(hot[0] == np.array([1, 7]))\n    assert np.all(hot[1] == np.array([1, 7]))\n    assert len(dead) == 2\n    assert np.all(dead[0] == np.array([6, 7]))\n    assert np.all(dead[1] == np.array([6, 3]))\n\n\n@pytest.mark.skipif(ON_JENKINS,\n                    reason=\'Requires access to central storage.\')\ndef test_get_metadata():\n    """"""Test retrieval of metadata from input file""""""\n\n    monitor = dark_monitor.Dark()\n    filename = os.path.join(get_config()[\'test_dir\'], \'dark_monitor\', \'test_image_1.fits\')\n    monitor.get_metadata(filename)\n\n    assert monitor.detector == \'NRCA1\'\n    assert monitor.x0 == 0\n    assert monitor.y0 == 0\n    assert monitor.xsize == 10\n    assert monitor.ysize == 10\n    assert monitor.sample_time == 10\n    assert monitor.frame_time == 10.5\n\n\ndef test_mast_query_darks():\n    """"""Test that the MAST query for darks is functional""""""\n\n    instrument = \'NIRCAM\'\n    aperture = \'NRCA1_FULL\'\n    start_date = Time(""2016-01-01T00:00:00"").mjd\n    end_date = Time(""2018-01-01T00:00:00"").mjd\n    query = dark_monitor.mast_query_darks(instrument, aperture, start_date, end_date)\n    apernames = [entry[\'apername\'] for entry in query]\n    filenames = [entry[\'filename\'] for entry in query]\n\n    truth_filenames = [\'jw96003001001_02201_00001_nrca1_dark.fits\',\n                       \'jw82600013001_02102_00002_nrca1_dark.fits\',\n                       \'jw82600013001_02101_00001_nrca1_dark.fits\',\n                       \'jw82600013001_02103_00003_nrca1_dark.fits\',\n                       \'jw82600013001_02103_00001_nrca1_dark.fits\',\n                       \'jw82600013001_02103_00002_nrca1_dark.fits\',\n                       \'jw82600016001_02101_00002_nrca1_dark.fits\',\n                       \'jw82600016001_02101_00001_nrca1_dark.fits\',\n                       \'jw82600013001_02102_00001_nrca1_dark.fits\',\n                       \'jw82600016001_02103_00002_nrca1_dark.fits\',\n                       \'jw82600016001_02103_00001_nrca1_dark.fits\',\n                       \'jw82600016001_02103_00004_nrca1_dark.fits\',\n                       \'jw82600016001_02103_00003_nrca1_dark.fits\',\n                       \'jw82600016001_02102_00001_nrca1_dark.fits\']\n\n    assert len(query) == 14\n    assert apernames == [aperture]*len(query)\n    assert filenames == truth_filenames\n\n\ndef test_noise_check():\n    """"""Test the search for noisier than average pixels""""""\n\n    noise_image = np.zeros((10, 10)) + 0.5\n    baseline = np.zeros((10, 10)) + 0.5\n\n    noise_image[3, 3] = 0.8\n    noise_image[6, 6] = 0.6\n    noise_image[9, 9] = 1.0\n\n    baseline[5, 5] = 1.0\n    noise_image[5, 5] = 1.25\n\n    monitor = dark_monitor.Dark()\n    noisy = monitor.noise_check(noise_image, baseline, threshold=1.5)\n\n    assert len(noisy[0]) == 2\n    assert np.all(noisy[0] == np.array([3, 9]))\n    assert np.all(noisy[1] == np.array([3, 9]))\n\n\ndef test_shift_to_full_frame():\n    """"""Test pixel coordinate shifting to be in full frame coords""""""\n\n    monitor = dark_monitor.Dark()\n    monitor.x0 = 512\n    monitor.y0 = 512\n\n    coordinates = (np.array([6, 7]), np.array([6, 3]))\n    new_coords = monitor.shift_to_full_frame(coordinates)\n\n    assert np.all(new_coords[0] == np.array([518, 519]))\n    assert np.all(new_coords[1] == np.array([518, 515]))\n'"
jwql/tests/test_data_containers.py,0,"b'#!/usr/bin/env python\n\n""""""Tests for the ``data_containers`` module in the ``jwql`` web\napplication.\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    These tests can be run via the command line (omit the -s to\n    suppress verbose output to stdout):\n\n    ::\n\n        pytest -s test_data_containers.py\n""""""\n\nimport glob\nimport os\n\nimport pytest\n\n# Skip testing this module if on Jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\ntry:\n    from jwql.website.apps.jwql import data_containers\n    from jwql.utils.utils import get_config\nexcept:\n    pass\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_acknowledgements():\n    """"""Tests the ``get_acknowledgements`` function.""""""\n\n    acknowledgements = data_containers.get_acknowledgements()\n    assert isinstance(acknowledgements, list)\n    assert len(acknowledgements) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_all_proposals():\n    """"""Tests the ``get_all_proposals`` function.""""""\n\n    proposals = data_containers.get_all_proposals()\n    assert isinstance(proposals, list)\n    assert len(proposals) > 0\n\n\n@pytest.mark.xfail\ndef test_get_dashboard_components():\n    """"""Tests the ``get_dashboard_components`` function.""""""\n\n    dashboard_components, dashboard_html = data_containers.get_dashboard_components()\n    assert isinstance(dashboard_components, dict)\n    assert isinstance(dashboard_html, dict)\n    assert len(dashboard_components) > 0\n    assert len(dashboard_html) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_expstart():\n    """"""Tests the ``get_expstart`` function.""""""\n\n    expstart = data_containers.get_expstart(\'jw86700006001_02101_00006_guider1\')\n    assert isinstance(expstart, float)\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_filenames_by_instrument():\n    """"""Tests the ``get_filenames_by_instrument`` function.""""""\n\n    filepaths = data_containers.get_filenames_by_instrument(\'FGS\')\n    assert isinstance(filepaths, list)\n    assert len(filepaths) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_filenames_by_proposal():\n    """"""Tests the ``get_filenames_by_proposal`` function.""""""\n\n    filenames = data_containers.get_filenames_by_proposal(\'88600\')\n    assert isinstance(filenames, list)\n    assert len(filenames) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_filenames_by_rootname():\n    """"""Tests the ``get_filenames_by_rootname`` function.""""""\n\n    filenames = data_containers.get_filenames_by_rootname(\'jw86600008001_02101_00007_guider2\')\n    assert isinstance(filenames, list)\n    assert len(filenames) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_header_info():\n    """"""Tests the ``get_header_info`` function.""""""\n\n    header = data_containers.get_header_info(\'jw86600008001_02101_00007_guider2_uncal.fits\')\n    assert isinstance(header, str)\n    assert len(header) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_image_info():\n    """"""Tests the ``get_image_info`` function.""""""\n\n    image_info = data_containers.get_image_info(\'jw86600008001_02101_00007_guider2\', False)\n\n    assert isinstance(image_info, dict)\n\n    keys = [\'all_jpegs\', \'suffixes\', \'num_ints\', \'all_files\']\n    for key in keys:\n        assert key in image_info\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_instrument_proposals():\n    """"""Tests the ``get_instrument_proposals`` function.""""""\n\n    proposals = data_containers.get_instrument_proposals(\'Fgs\')\n    assert isinstance(proposals, list)\n    assert len(proposals) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_preview_images_by_instrument():\n    """"""Tests the ``get_preview_images_by_instrument`` function.""""""\n\n    preview_images = data_containers.get_preview_images_by_instrument(\'fgs\')\n    assert isinstance(preview_images, list)\n    assert len(preview_images) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_preview_images_by_proposal():\n    """"""Tests the ``get_preview_images_by_proposal`` function.""""""\n\n    preview_images = data_containers.get_preview_images_by_proposal(\'88600\')\n    assert isinstance(preview_images, list)\n    assert len(preview_images) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_preview_images_by_rootname():\n    """"""Tests the ``get_preview_images_by_rootname`` function.""""""\n\n    preview_images = data_containers.get_preview_images_by_rootname(\'jw86600008001_02101_00007_guider2\')\n    assert isinstance(preview_images, list)\n    assert len(preview_images) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_proposal_info():\n    """"""Tests the ``get_proposal_info`` function.""""""\n\n    filepaths = glob.glob(os.path.join(get_config()[\'filesystem\'], \'jw88600\', \'*.fits\'))\n    proposal_info = data_containers.get_proposal_info(filepaths)\n\n    assert isinstance(proposal_info, dict)\n\n    keys = [\'num_proposals\', \'proposals\', \'thumbnail_paths\', \'num_files\']\n    for key in keys:\n        assert key in proposal_info\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_thumbnails_by_instrument():\n    """"""Tests the ``get_thumbnails_by_instrument`` function.""""""\n\n    preview_images = data_containers.get_thumbnails_by_instrument(\'fgs\')\n    assert isinstance(preview_images, list)\n    assert len(preview_images) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_thumbnails_by_proposal():\n    """"""Tests the ``get_thumbnails_by_proposal`` function.""""""\n\n    preview_images = data_containers.get_thumbnails_by_proposal(\'88600\')\n    assert isinstance(preview_images, list)\n    assert len(preview_images) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_thumbnails_by_rootname():\n    """"""Tests the ``get_thumbnails_by_rootname`` function.""""""\n\n    preview_images = data_containers.get_thumbnails_by_rootname(\'jw86600008001_02101_00007_guider2\')\n    assert isinstance(preview_images, list)\n    assert len(preview_images) > 0\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_thumbnails_ajax():\n    """"""Tests the ``get_thumbnails_ajax`` function.""""""\n\n    thumbnail_dict = data_containers.thumbnails_ajax(\'FGS\')\n\n    assert isinstance(thumbnail_dict, dict)\n\n    keys = [\'inst\', \'file_data\', \'tools\', \'dropdown_menus\', \'prop\']\n    for key in keys:\n        assert key in thumbnail_dict\n'"
jwql/tests/test_database_interface.py,0,"b'#! /usr/bin/env python\n\n""""""Tests for the ``database_interface.py`` module.\n\nAuthors\n-------\n\n    - Joe Filippazzo\n    - Matthew Bourque\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to stdout):\n    ::\n\n        pytest -s database_interface.py\n""""""\n\nimport datetime\nimport os\nimport pytest\nimport random\nimport string\n\nfrom jwql.database import database_interface as di\nfrom jwql.utils.constants import ANOMALIES_PER_INSTRUMENT\nfrom jwql.utils.utils import get_config\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to development database server.\')\ndef test_all_tables_exist():\n    """"""Test that the table ORMs defined in ``database_interface``\n    actually exist as tables in the database""""""\n\n    # Get list of table ORMs from database_interface\n    table_orms = []\n    database_interface_attributes = di.__dict__.keys()\n    for attribute in database_interface_attributes:\n        table_object = getattr(di, attribute)\n        try:\n            table_orms.append(table_object.__tablename__)\n        except AttributeError:\n            pass  # Not all attributes of database_interface are table ORMs\n\n    # Get list of tables that are actually in the database\n    existing_tables = di.engine.table_names()\n\n    # Ensure that the ORMs defined in database_interface actually exist\n    # as tables in the database\n    for table in table_orms:\n        assert table in existing_tables\n\n\ndef test_anomaly_orm_factory():\n    """"""Test that the ``anomaly_orm_factory`` function successfully\n    creates an ORM and contains the appropriate columns""""""\n\n    test_table_name = \'test_anomaly_table\'\n    TestAnomalyTable = di.anomaly_orm_factory(test_table_name)\n    table_attributes = TestAnomalyTable.__dict__.keys()\n\n    assert str(TestAnomalyTable) == ""<class \'jwql.database.database_interface.{}\'>""\\\n        .format(test_table_name)\n\n    for item in [\'id\', \'rootname\', \'flag_date\', \'user\']:\n        assert item in table_attributes\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to development database server.\')\ndef test_anomaly_records():\n    """"""Test to see that new records can be entered""""""\n\n    # Add some data\n    random_rootname = \'\'.join(random.SystemRandom().choice(string.ascii_lowercase + \\\n                                                           string.ascii_uppercase + \\\n                                                           string.digits) for _ in range(10))\n    di.session.add(di.FGSAnomaly(rootname=random_rootname,\n                              flag_date=datetime.datetime.today(),\n                              user=\'test\', ghost=True))\n    di.session.commit()\n\n    # Test the ghosts column\n    ghosts = di.session.query(di.FGSAnomaly)\\\n        .filter(di.FGSAnomaly.rootname == random_rootname)\\\n        .filter(di.FGSAnomaly.ghost == ""True"")\n    assert ghosts.data_frame.iloc[0][\'ghost\'] == True\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to development database server.\')\ndef test_load_connections():\n    """"""Test to see that a connection to the database can be\n    established""""""\n\n    session, base, engine, meta = di.load_connection(get_config()[\'connection_string\'])\n    assert str(type(session)) == ""<class \'sqlalchemy.orm.session.Session\'>""\n    assert str(type(base)) == ""<class \'sqlalchemy.ext.declarative.api.DeclarativeMeta\'>""\n    assert str(type(engine)) == ""<class \'sqlalchemy.engine.base.Engine\'>""\n    assert str(type(meta)) == ""<class \'sqlalchemy.sql.schema.MetaData\'>""\n\n\ndef test_monitor_orm_factory():\n    """"""Test that the ``monitor_orm_factory`` function successfully\n    creates an ORM and contains the appropriate columns""""""\n\n    test_table_name = \'instrument_test_monitor_table\'\n\n    # Create temporary table definitions file\n    test_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)),\n                            \'database\', \'monitor_table_definitions\', \'instrument\')\n    test_filename = os.path.join(test_dir, \'{}.txt\'.format(test_table_name))\n    if not os.path.isdir(test_dir):\n        os.mkdir(test_dir)\n    with open(test_filename, \'w\') as f:\n        f.write(\'TEST_COLUMN, string\')\n\n    # Create the test table ORM\n    TestMonitorTable = di.monitor_orm_factory(test_table_name)\n    table_attributes = TestMonitorTable.__dict__.keys()\n\n    # Ensure the ORM exists and contains appropriate columns\n    assert str(TestMonitorTable) == ""<class \'jwql.database.database_interface.{}\'>""\\\n        .format(test_table_name)\n    for column in [\'id\', \'entry_date\', \'test_column\']:\n        assert column in table_attributes\n\n    # Remove test files and directories\n    if os.path.isfile(test_filename):\n        os.remove(test_filename)\n    if os.path.isdir(test_dir):\n        os.rmdir(test_dir)\n'"
jwql/tests/test_edb.py,0,"b'#! /usr/bin/env python\n""""""Tests for the ``engineering_database`` module.\n\nAuthors\n-------\n\n    - Johannes Sahlmann\n\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to ``stdout``):\n\n    ::\n\n        pytest -s test_edb.py\n""""""\n\nimport os\n\nfrom astropy.time import Time\nimport pytest\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_mnemonic():\n    """"""Test the query of a single mnemonic.""""""\n    from jwql.edb.engineering_database import get_mnemonic\n\n    mnemonic_identifier = \'IMIR_HK_ICE_SEC_VOLT4\'\n    start_time = Time(\'2019-01-16 00:00:00.000\', format=\'iso\')\n    end_time = Time(\'2019-01-16 00:01:00.000\', format=\'iso\')\n\n    mnemonic = get_mnemonic(mnemonic_identifier, start_time, end_time)\n    assert len(mnemonic.data) == mnemonic.meta[\'paging\'][\'rows\']\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_mnemonic_info():\n    """"""Test retrieval of mnemonic info.""""""\n    from jwql.edb.engineering_database import get_mnemonic_info\n\n    mnemonic_identifier = \'IMIR_HK_ICE_SEC_VOLT4\'\n    info = get_mnemonic_info(mnemonic_identifier)\n    assert \'subsystem\' in info.keys()\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_mnemonics():\n    """"""Test the query of a list of mnemonics.""""""\n    from jwql.edb.engineering_database import get_mnemonics\n\n    mnemonics = [\'SA_ZFGOUTFOV\', \'SA_ZFGBADCNT\']\n    start_time = Time(2018.0, format=\'decimalyear\')\n    end_time = Time(2018.1, format=\'decimalyear\')\n\n    mnemonic_dict = get_mnemonics(mnemonics, start_time, end_time)\n    assert len(mnemonic_dict) == len(mnemonics)\n'"
jwql/tests/test_instrument_properties.py,3,"b'#! /usr/bin/env python\n\n""""""Tests for the ``instrument_properties`` module.\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to stdout):\n    ::\n\n        pytest -s test_instrument_properties.py\n""""""\n\nimport os\nimport pytest\n\nimport numpy as np\n\nfrom jwql.utils import instrument_properties\nfrom jwql.utils.utils import get_config\n\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\n@pytest.mark.skipif(ON_JENKINS,\n                    reason=\'Requires access to central storage.\')\ndef test_amplifier_info():\n    """"""Test that the correct number of amplifiers are found for a given\n    file\n    """"""\n\n    data_dir = os.path.join(get_config()[\'test_dir\'], \'dark_monitor\')\n\n    fullframe = instrument_properties.amplifier_info(os.path.join(data_dir, \'test_image_ff.fits\'))\n    fullframe_truth = (4, {\'1\': [(4, 4), (512, 2044)],\n                           \'2\': [(512, 4), (1024, 2044)],\n                           \'3\': [(1024, 4), (1536, 2044)],\n                           \'4\': [(1536, 4), (2044, 2044)]})\n    assert fullframe == fullframe_truth\n\n    fullframe = instrument_properties.amplifier_info(os.path.join(data_dir, \'test_image_ff.fits\'), omit_reference_pixels=False)\n    fullframe_truth = (4, {\'1\': [(0, 0), (512, 2048)],\n                           \'2\': [(512, 0), (1024, 2048)],\n                           \'3\': [(1024, 0), (1536, 2048)],\n                           \'4\': [(1536, 0), (2048, 2048)]})\n    assert fullframe == fullframe_truth\n\n    subarray = instrument_properties.amplifier_info(os.path.join(data_dir, \'test_image_1.fits\'))\n    subarray_truth = (1, {\'1\': [(0, 0), (10, 10)]})\n    assert subarray == subarray_truth\n\n    subarray_one = instrument_properties.amplifier_info(os.path.join(data_dir, \'test_image_grismstripe_one_amp.fits\'))\n    subarray_one_truth = (1, {\'1\': [(4, 4), (2044, 64)]})\n    assert subarray_one == subarray_one_truth\n\n    subarray_four = instrument_properties.amplifier_info(os.path.join(data_dir, \'test_image_grismstripe_four_amp.fits\'))\n    subarray_four_truth = (4, {\'1\': [(4, 4), (512, 64)],\n                               \'2\': [(512, 4), (1024, 64)],\n                               \'3\': [(1024, 4), (1536, 64)],\n                               \'4\': [(1536, 4), (2044, 64)]})\n    assert subarray_four == subarray_four_truth\n\n\ndef test_calc_frame_time():\n    """"""Test calcuation of frametime for a given instrument/aperture""""""\n\n    nearir_fullframe = 10.73677\n    nircam_160 = 0.27864\n    nrc_fullframe = instrument_properties.calc_frame_time(\'nircam\', \'NRCA1_FULL\', 2048, 2048, 4)\n    nrc_160 = instrument_properties.calc_frame_time(\'nircam\', \'NRCA1_SUB160\', 160, 160, 1)\n    nrs_fullframe = instrument_properties.calc_frame_time(\'niriss\', \'NIS_CEN\', 2048, 2048, 4)\n    #nrs_some_subarra = instrument_properies.calc_frame_time(\'niriss\', \'????\', ??, ??, ?)\n\n    print(\'STILL NEED TO ADD FRAMETIME CALCS FOR MIRI AND NIRSPEC TO THE CALC_FRAME_TIME_FUNCTION\')\n    print(\'CONFIRM NIRCAMSUB160 TIME ON JDOX\')\n\n    assert np.isclose(nrc_fullframe, nearir_fullframe, atol=0.001, rtol=0)\n    assert np.isclose(nrc_160, nircam_160, atol=0.001, rtol=0)\n    assert np.isclose(nrs_fullframe, nearir_fullframe, atol=0.001, rtol=0)\n'"
jwql/tests/test_loading_times.py,0,"b'#!/usr/bin/env python\n\n""""""Tests various webpages of the ``jwql`` web application to make sure\nthat loading times are not too long\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    These tests can be run via the command line (omit the -s to\n    suppress verbose output to stdout):\n\n    ::\n\n        pytest -s test_loading_times.py\n""""""\n\nimport os\nimport pytest\nimport time\nimport urllib.request\n\nfrom jwql.utils.utils import get_base_url\n\nTIME_CONSTRAINT = 30  # seconds\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\nurls = []\n\n# Generic URLs\nurls.append(\'\')\nurls.append(\'about/\')\nurls.append(\'edb/\')\n\n# Specific URLs\ntest_mappings = [(\'fgs\', \'86700\', \'jw86600007001_02101_00001_guider2\'),\n                 (\'miri\', \'98012\', \'jw98012001001_02102_00001_mirimage\'),\n                 (\'nircam\', \'93025\', \'jw93065002001_02101_00001_nrcb2\'),\n                 (\'niriss\', \'00308\', \'jw00308001001_02101_00001_nis\'),\n                 (\'nirspec\', \'96213\', \'jw96213001001_02101_00001_nrs1\')]\nfor mapping in test_mappings:\n    (instrument, proposal, rootname) = mapping\n    urls.append(\'{}/\'.format(instrument))\n    urls.append(\'{}/archive/\'.format(instrument))\n    urls.append(\'{}/archive/{}/\'.format(instrument, proposal))\n    urls.append(\'{}/{}/\'.format(instrument, rootname))\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\n@pytest.mark.parametrize(\'url\', urls)\ndef test_loading_times(url):\n    """"""Test to see if the given ``url`` returns a webpage successfully\n    within a reasonable time.\n\n    Parameters\n    ----------\n    url : str\n        The url to the webpage of interest (e.g.\n        ``http://127.0.0.1:8000/fgs/archive/\'``).\n    """"""\n\n    # Build full URL\n    base_url = get_base_url()\n    url = \'{}/{}\'.format(base_url, url)\n    print(\'Testing {}\'.format(url))\n\n    t1 = time.time()\n    url = urllib.request.urlopen(url)\n    t2 = time.time()\n\n    assert (t2 - t1) <= TIME_CONSTRAINT\n'"
jwql/tests/test_logging_functions.py,0,"b'#!/usr/bin/env python\n\n""""""Tests for the ``logging_functions`` module.\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    These tests can be run via the command line (omit the -s to\n    suppress verbose output to stdout):\n\n    ::\n\n        pytest -s test_logging_functions.py\n""""""\n\nimport logging\nimport os\nimport pytest\nimport shutil\n\nfrom jwql.utils import logging_functions\nfrom jwql.utils.logging_functions import configure_logging, log_fail, log_info, make_log_file\nfrom jwql.utils.utils import get_config\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\n@log_fail\n@log_info\ndef perform_basic_logging():\n    """"""Performs some basic logging to the test log file""""""\n\n    logging.info(\'This is some logging info\')\n    logging.warning(\'This is a normal warning\')\n    logging.critical(\'This is a critical warning\')\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_configure_logging():\n    """"""Assert that the ``configure_logging`` function successfully\n    creates a log file""""""\n\n    log_file = logging_functions.configure_logging(\'test_logging_functions\')\n    assert os.path.exists(log_file)\n\n    # Remove the log file\n    shutil.rmtree(os.path.dirname(log_file), ignore_errors=True)\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_make_log_file():\n    """"""Assert that ``make_log_file`` function returns the appropriate\n    path for a log file""""""\n\n    module = \'test_logging_functions\'\n    log_file = make_log_file(module)\n\n    correct_locations = [\n        os.path.join(get_config()[\'log_dir\'], \'dev\', module, os.path.basename(log_file)),\n        os.path.join(get_config()[\'log_dir\'], \'test\', module, os.path.basename(log_file)),\n        os.path.join(get_config()[\'log_dir\'], \'prod\', module, os.path.basename(log_file))]\n\n    assert log_file in correct_locations\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_logging_functions():\n    """"""A generic end-to-end test that creates a log file, does some\n    basic logging, then asserts that some logging content exists""""""\n\n    log_file = configure_logging(\'test_logging_functions\')\n    perform_basic_logging()\n\n    # Open the log file and make some assertions\n    with open(log_file, \'r\') as f:\n        data = f.readlines()\n    data = str([line.strip() for line in data])\n    testable_content = [\'User:\', \'System:\', \'Python Executable Path:\', \'INFO:\',\n                        \'WARNING:\', \'CRITICAL:\', \'Elapsed Real Time:\',\n                        \'Elapsed CPU Time:\', \'Completed Successfully\']\n    for item in testable_content:\n        assert item in data\n'"
jwql/tests/test_monitor_mast.py,0,"b'#! /usr/bin/env python\n\n""""""Tests for the ``monitor_mast`` module.\n\nAuthors\n-------\n\n    Joe Filippazzo\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to stdout):\n    ::\n\n        pytest -s test_monitor_mast.py\n""""""\n\nfrom astroquery.mast import Mast\nimport pytest\n\nfrom jwql.jwql_monitors import monitor_mast as mm\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES\n\n\ndef test_astroquery_mast():\n    """"""Test if the astroquery.mast service can complete a request""""""\n    service = \'Mast.Caom.Filtered\'\n    params = {\'columns\': \'COUNT_BIG(*)\', \'filters\': [], \'pagesize\': 1,\n              \'page\': 1}\n    response = Mast.service_request_async(service, params)\n    result = response[0].json()\n\n    assert result[\'status\'] == \'COMPLETE\'\n\n\ndef test_caom_instrument_keywords():\n    """"""Test to see that the CAOM keywords are the same for all\n    instruments""""""\n    kw = []\n    for ins in JWST_INSTRUMENT_NAMES:\n        kw.append(mm.instrument_keywords(ins, caom=True)[\'keyword\'].tolist())\n\n    assert kw[0] == kw[1] == kw[2] == kw[3] == kw[4]\n\n\ndef test_filtered_instrument_keywords():\n    """"""Test to see that the instrument specific service keywords are\n    different for all instruments""""""\n    kw = []\n    for ins in JWST_INSTRUMENT_NAMES:\n        kw.append(mm.instrument_keywords(ins, caom=False)[\'keyword\'].tolist())\n\n    assert kw[0] != kw[1] != kw[2] != kw[3] != kw[4]\n\n\ndef test_instrument_inventory_filtering():\n    """"""Test to see that the instrument inventory can be filtered""""""\n    filt = \'GR150R\'\n    data = mm.instrument_inventory(\'niriss\',\n                                   add_filters={\'filter\': filt},\n                                   return_data=True)\n\n    filters = [row[\'filter\'] for row in data[\'data\']]\n\n    assert all([i == filt for i in filters])\n\n\ndef test_instrument_dataproduct_filtering():\n    """"""Test to see that the instrument inventory can be filtered\n    by data product""""""\n    dp = \'spectrum\'\n    data = mm.instrument_inventory(\'nirspec\', dataproduct=dp, caom=True,\n                                   return_data=True)\n\n    dps = [row[\'dataproduct_type\'] for row in data[\'data\']]\n\n    assert all([i == dp for i in dps])\n'"
jwql/tests/test_permissions.py,0,"b'#! /usr/bin/env python\n\n""""""Tests for the ``permissions`` module.\n\nAuthors\n-------\n\n    - Johannes Sahlmann\n\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to ``stdout``):\n\n    ::\n\n        pytest -s test_permissions.py\n""""""\n\nimport grp\nimport os\nimport pytest\n\nfrom jwql.utils.permissions import set_permissions, has_permissions, \\\n    get_owner_string, get_group_string\n\n# directory to be created and populated during tests running\nTEST_DIRECTORY = os.path.join(os.environ[\'HOME\'], \'permission_test\')\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\n@pytest.fixture(scope=""module"")\ndef test_directory(test_dir=TEST_DIRECTORY):\n    """"""Create a test directory for permission management.\n\n    Parameters\n    ----------\n    test_dir : str\n        Path to directory used for testing\n\n    Yields\n    -------\n    test_dir : str\n        Path to directory used for testing\n    """"""\n    os.mkdir(test_dir)  # creates directory with default mode=511\n\n    yield test_dir\n    if os.path.isdir(test_dir):\n        os.remove(test_dir)\n\n\ndef test_directory_permissions(test_directory):\n    """"""Create a directory with the standard permissions\n    ``(\'-rw-r--r--\')``.\n\n    Set the default permissions defined in ``permissions.py``. Assert\n    that these were set correctly.\n\n    Parameters\n    ----------\n    test_directory : str\n        Path of directory used for testing\n    """"""\n    # Get owner and group on the current system.This allows to implement the tests\n    # independently from the user.\n    owner = get_owner_string(test_directory)\n    group = get_group_string(test_directory)\n\n    set_permissions(test_directory, owner=owner, group=group)\n    assert has_permissions(test_directory, owner=owner, group=group)\n\n\n@pytest.fixture()\ndef test_file(test_dir=TEST_DIRECTORY):\n    """"""Create a test file for permission management.\n\n    Parameters\n    ----------\n    test_dir : str\n        Path to directory used for testing\n\n    Yields\n    -------\n    filename : str\n        Path of file used for testing\n    """"""\n    if not os.path.isdir(test_dir):\n        os.mkdir(test_dir)\n\n    filename = os.path.join(test_dir, \'permission_test.txt\')\n    with open(filename, \'w\') as filestream:\n        filestream.write(\'jwql permission test\')\n    yield filename\n    if os.path.isfile(filename):\n        os.remove(filename)\n    if os.path.isdir(test_dir):\n        os.rmdir(test_dir)\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_file_group(test_file):\n    """"""Create a file with the standard permissions ``(\'-rw-r--r--\')``\n    and default group.\n\n    Modify the group and set the default permissions defined in\n    ``permissions.py``.  Assert that both group and permissions were\n    set correctly.\n\n    Parameters\n    ----------\n    test_file : str\n        Path of file used for testing\n    """"""\n    # Get owner and group on the current system.\n    owner = get_owner_string(test_file)\n    group = get_group_string(test_file)\n\n    # attempt to retrieve a group name different from default\n    group_index = 0\n    test_group = grp.getgrgid(os.getgroups()[group_index]).gr_name\n\n    set_permissions(test_file, group=test_group, owner=owner)\n    assert has_permissions(test_file, group=test_group, owner=owner)\n\n    # return to default group\n    set_permissions(test_file, owner=owner, group=group)\n    assert has_permissions(test_file, owner=owner, group=group)\n\n\ndef test_file_permissions(test_file):\n    """"""Create a file with the standard permissions ``(\'-rw-r--r--\')``.\n\n    Set the default permissions defined in ``permissions.py``. Assert\n    that these were set correctly.\n\n    Parameters\n    ----------\n    test_file : str\n        Path of file used for testing\n    """"""\n    # Get owner and group on the current system.\n    owner = get_owner_string(test_file)\n    group = get_group_string(test_file)\n\n    set_permissions(test_file, owner=owner, group=group)\n    assert has_permissions(test_file, owner=owner, group=group)\n'"
jwql/tests/test_pipeline_tools.py,2,"b'#! /usr/bin/env python\n\n""""""Tests for the ``pipeline_tools`` module.\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to stdout):\n    ::\n\n        pytest -s test_pipeline_tools.py\n""""""\n\nfrom collections import OrderedDict\nimport os\nimport pytest\n\nimport numpy as np\n\nfrom jwql.instrument_monitors import pipeline_tools\nfrom jwql.utils.utils import get_config\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_completed_pipeline_steps():\n    """"""Test that the list of completed pipeline steps for a file is\n    correct\n\n    Parameters\n    ----------\n    filename : str\n        File to be checked\n    """"""\n\n    filename = os.path.join(get_config()[\'filesystem\'], \'jw00312\',\n                            \'jw00312002001_02102_00001_nrcb4_rateints.fits\')\n    completed_steps = pipeline_tools.completed_pipeline_steps(filename)\n    true_completed = OrderedDict([(\'group_scale\', False),\n                                  (\'dq_init\', True),\n                                  (\'saturation\', True),\n                                  (\'ipc\', False),\n                                  (\'refpix\', True),\n                                  (\'superbias\', True),\n                                  (\'persistence\', True),\n                                  (\'dark_current\', True),\n                                  (\'linearity\', True),\n                                  (\'firstframe\', False),\n                                  (\'lastframe\', False),\n                                  (\'rscd\', False),\n                                  (\'jump\', True),\n                                  (\'rate\', True)])\n\n    # Only test steps that have a value of True\n    completed_steps = OrderedDict((k, v) for k, v in completed_steps.items() if v is True)\n    true_completed = OrderedDict((k, v) for k, v in true_completed.items() if v is True)\n\n    assert completed_steps == true_completed\n\n\ndef test_get_pipeline_steps():\n    """"""Test that the proper pipeline steps are returned for an\n    instrument\n    """"""\n\n    # FGS, NIRCam, and NIRISS\n    instruments = [\'fgs\', \'nircam\', \'niriss\']\n    for instrument in instruments:\n        req_steps = pipeline_tools.get_pipeline_steps(instrument)\n        steps = [\'dq_init\', \'saturation\', \'superbias\', \'refpix\', \'linearity\',\n                 \'persistence\', \'dark_current\', \'jump\', \'rate\']\n        not_required = [\'group_scale\', \'ipc\', \'firstframe\', \'lastframe\', \'rscd\']\n        steps_dict = OrderedDict({})\n        for step in steps:\n            steps_dict[step] = True\n        for step in not_required:\n            steps_dict[step] = False\n\n        # Only test steps that have a value of True\n        req_steps = OrderedDict((k, v) for k, v in req_steps.items() if v is True)\n        steps_dict = OrderedDict((k, v) for k, v in steps_dict.items() if v is True)\n        assert req_steps == steps_dict\n\n    # NIRSpec\n    nrs_req_steps = pipeline_tools.get_pipeline_steps(\'nirspec\')\n    nrs_steps = [\'group_scale\', \'dq_init\', \'saturation\', \'superbias\', \'refpix\', \'linearity\',\n                 \'dark_current\', \'jump\', \'rate\']\n    not_required = [\'ipc\', \'persistence\', \'firstframe\', \'lastframe\', \'rscd\']\n    nrs_dict = OrderedDict({})\n    for step in nrs_steps:\n        nrs_dict[step] = True\n    for step in not_required:\n        nrs_dict[step] = False\n    # Only test steps that have a value of True\n    nrs_req_steps = OrderedDict((k, v) for k, v in nrs_req_steps.items() if v is True)\n    nrs_dict = OrderedDict((k, v) for k, v in nrs_dict.items() if v is True)\n    assert nrs_req_steps == nrs_dict\n\n    # MIRI\n    miri_req_steps = pipeline_tools.get_pipeline_steps(\'miri\')\n    miri_steps = [\'dq_init\', \'saturation\', \'firstframe\', \'lastframe\',\n                  \'linearity\', \'rscd\', \'dark_current\', \'refpix\', \'jump\', \'rate\']\n    not_required = [\'group_scale\', \'ipc\', \'superbias\', \'persistence\']\n    miri_dict = OrderedDict({})\n    for step in miri_steps:\n        miri_dict[step] = True\n    for step in not_required:\n        miri_dict[step] = False\n    # Only test steps that have a value of True\n    miri_req_steps = OrderedDict((k, v) for k, v in miri_req_steps.items() if v is True)\n    miri_dict = OrderedDict((k, v) for k, v in miri_dict.items() if v is True)\n    assert miri_req_steps == miri_dict\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_image_stack():\n    """"""Test stacking of slope images""""""\n\n    directory = os.path.join(get_config()[\'test_dir\'], \'dark_monitor\')\n    files = [os.path.join(directory, \'test_image_{}.fits\'.format(str(i + 1))) for i in range(3)]\n\n    image_stack, exptimes = pipeline_tools.image_stack(files)\n    truth = np.zeros((3, 10, 10))\n    truth[0, :, :] = 5.\n    truth[1, :, :] = 10.\n    truth[2, :, :] = 15.\n\n    assert np.all(image_stack == truth)\n    assert exptimes == [[10.5], [10.5], [10.5]]\n\n\ndef test_steps_to_run():\n    """"""Test that the dictionaries for steps required and steps completed\n    are correctly combined to create a dictionary of pipeline steps to\n    be done\n\n    Parameters\n    ----------\n    filename : str\n        File to be checked\n\n    required : OrderedDict\n        Dict of all pipeline steps to be run on filename\n\n    already_done : OrderedDict\n        Dict of pipeline steps already run on filename\n    """"""\n\n    required = OrderedDict([(\'group_scale\', True),\n                            (\'dq_init\', False),\n                            (\'saturation\', False),\n                            (\'ipc\', False),\n                            (\'refpix\', False),\n                            (\'superbias\', False),\n                            (\'persistence\', True),\n                            (\'dark_current\', True),\n                            (\'linearity\', False),\n                            (\'firstframe\', False),\n                            (\'lastframe\', False),\n                            (\'rscd\', False),\n                            (\'jump\', True),\n                            (\'rate\', True)])\n    already_done = OrderedDict([(\'group_scale\', True),\n                                (\'dq_init\', False),\n                                (\'saturation\', False),\n                                (\'ipc\', False),\n                                (\'refpix\', False),\n                                (\'superbias\', False),\n                                (\'persistence\', True),\n                                (\'dark_current\', True),\n                                (\'linearity\', False),\n                                (\'firstframe\', False),\n                                (\'lastframe\', False),\n                                (\'rscd\', False),\n                                (\'jump\', False),\n                                (\'rate\', False)])\n\n    steps_to_run = pipeline_tools.steps_to_run(required, already_done)\n    true_steps_to_run = OrderedDict([(\'group_scale\', False),\n                                     (\'dq_init\', False),\n                                     (\'saturation\', False),\n                                     (\'ipc\', False),\n                                     (\'refpix\', False),\n                                     (\'superbias\', False),\n                                     (\'persistence\', False),\n                                     (\'dark_current\', False),\n                                     (\'linearity\', False),\n                                     (\'firstframe\', False),\n                                     (\'lastframe\', False),\n                                     (\'rscd\', False),\n                                     (\'jump\', True),\n                                     (\'rate\', True)])\n\n    assert steps_to_run == true_steps_to_run\n'"
jwql/tests/test_plotting.py,0,"b'#!/usr/bin/env python\n\n""""""Tests for the ``plotting`` module.\n\nAuthors\n-------\n\n    - Joe Filippazzo\n\nUse\n---\n\n    These tests can be run via the command line (omit the -s to\n    suppress verbose output to stdout):\n\n    ::\n\n        pytest -s test_plotting.py\n""""""\n\nimport glob\nimport os\nimport re\nimport sys\n\nimport bokeh\nfrom pandas import DataFrame\nimport pytest\n\nfrom jwql.utils.plotting import bar_chart\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\nJWQL_DIR = __location__.split(\'tests\')[0]\n\n\ndef test_bar_chart():\n    """"""Make sure some dummy data generates a ``bokeh`` plot""""""\n\n    # Make a toy dataframe\n    data = DataFrame({\'meow\': {\'foo\': 12, \'bar\': 23, \'baz\': 2},\n                      \'mix\': {\'foo\': 45, \'bar\': 31, \'baz\': 23},\n                      \'deliver\': {\'foo\': 62, \'bar\': 20, \'baz\': 9}})\n    data = data.reset_index()\n\n    # And generate a figure\n    plt = bar_chart(data, \'index\')\n\n    assert str(type(plt)) == ""<class \'bokeh.plotting.figure.Figure\'>""\n\n\n@pytest.mark.skipif(sys.version_info[:2] != (3, 6),\n                    reason=""Web server run on Python 3.6"")\ndef test_bokeh_version():\n    """"""Make sure that the current version of Bokeh matches the version being\n    used in all the web app HTML templates.\n    """"""\n    env_version = bokeh.__version__\n\n    template_paths = os.path.join(JWQL_DIR, \'website/apps/jwql/templates\', \'*.html\')\n    all_web_html_files = glob.glob(template_paths)\n\n    for file in all_web_html_files:\n        with open(file, \'r+\', encoding=""utf-8"") as f:\n            content = f.read()\n\n        # Find all of the times ""bokeh-#.#.#\' appears in a template\n        html_versions = re.findall(r\'(?<=bokeh-)\\d+\\.\\d+\\.\\d+\', content)\n        html_versions += re.findall(r\'(?<=bokeh-widgets-)\\d+\\.\\d+\\.\\d+\', content)\n\n        # Make sure they all match the environment version\n        for version in html_versions:\n            assert version == env_version, \\\n                \'Bokeh version ({}) in HTML template {} \'.format(version, os.path.basename(file)) + \\\n                \'does not match current environment version ({}).\'.format(env_version)\n'"
jwql/tests/test_preview_image.py,0,"b'#! /usr/bin/env python\n\n""""""Tests for the ``preview_image`` module.\n\nAuthors\n-------\n\n    - Johannes Sahlmann\n    - Lauren Chambers\n\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to ``stdout``):\n\n    ::\n\n        pytest -s test_preview_image.py\n""""""\n\nimport glob\nimport os\nimport pytest\nimport shutil\n\nfrom astropy.io import fits\n\nfrom jwql.utils.preview_image import PreviewImage\nfrom jwql.utils.utils import get_config, ensure_dir_exists\n\n# directory to be created and populated during tests running\nTEST_DIRECTORY = os.path.join(os.environ[\'HOME\'], \'preview_image_test\')\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\n\n@pytest.fixture(scope=""module"")\ndef test_directory(test_dir=TEST_DIRECTORY):\n    """"""Create a test directory for preview image.\n\n    Parameters\n    ----------\n    test_dir : str\n        Path to directory used for testing\n\n    Yields\n    -------\n    test_dir : str\n        Path to directory used for testing\n\n    """"""\n    # Set up local test directory\n    ensure_dir_exists(test_dir)\n    yield test_dir\n\n    # Tear down local test directory and any files within\n    if os.path.isdir(test_dir):\n        shutil.rmtree(test_dir)\n\n    # Empty test directory on central storage\n    jpgs = glob.glob(os.path.join(get_config()[\'test_dir\'], \'*.jpg\'))\n    thumbs = glob.glob(os.path.join(get_config()[\'test_dir\'], \'*.thumbs\'))\n    for file in jpgs + thumbs:\n        os.remove(file)\n\n\ndef get_test_fits_files():\n    """"""Get a list of the FITS files on central storage to make preview images.\n\n    Returns\n    -------\n    filenames : list\n        List of filepaths to FITS files\n    """"""\n    # Get the files from central store\n    if not ON_JENKINS:\n        filenames = glob.glob(os.path.join(get_config()[\'test_dir\'], \'*.fits\'))\n        assert len(filenames) > 0\n        return filenames\n\n    # Or return an empty list\n    else:\n        return []\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\n@pytest.mark.parametrize(\'filename\', get_test_fits_files())\ndef test_make_image(test_directory, filename):\n    """"""Use PreviewImage.make_image to create preview images of a sample\n    JWST exposure.\n\n    Assert that the number of JPGs created corresponds to the number of\n    integrations.\n\n    Parameters\n    ----------\n    test_directory : str\n        Path of directory used for testing\n    filename : str\n        Path of FITS image to generate preview of\n    """"""\n\n    header = fits.getheader(filename)\n\n    # Create and save the preview image or thumbnail\n    for create_thumbnail in [False, True]:\n        try:\n            image = PreviewImage(filename, ""SCI"")\n            image.clip_percent = 0.01\n            image.scaling = \'log\'\n            image.cmap = \'viridis\'\n            image.output_format = \'jpg\'\n            image.thumbnail = create_thumbnail\n\n            if create_thumbnail:\n                image.thumbnail_output_directory = test_directory\n            else:\n                image.preview_output_directory = test_directory\n\n            image.make_image()\n        except ValueError as error:\n            print(error)\n\n        if create_thumbnail:\n            extension = \'thumb\'\n        else:\n            extension = \'jpg\'\n\n        # list of preview images\n        preview_image_filenames = glob.glob(os.path.join(test_directory, \'*.{}\'.format(\n            extension)))\n        assert len(preview_image_filenames) == header[\'NINTS\']\n\n        # clean up: delete preview images\n        for file in preview_image_filenames:\n            os.remove(file)\n'"
jwql/tests/test_setup.py,0,"b'#! /usr/bin/env python\n\n""""""Tests for the ``setup.py`` module.\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to ``stdout``):\n\n    ::\n\n        pytest -s test_setup_info.py\n""""""\n\nimport jwql\n\n\ndef test_version_number():\n    """"""Test that the JWQL version number is retrieved from\n    ``setup.py``\n    """"""\n\n    assert isinstance(jwql.__version__, str)\n    version_parts = jwql.__version__.split(\'.\')\n    assert len(version_parts) == 3\n'"
jwql/tests/test_utils.py,0,"b'#!/usr/bin/env python\n""""""Tests for the ``utils`` module.\n\nAuthors\n-------\n\n    - Lauren Chambers\n    - Matthew Bourque\n\nUse\n---\n\n    These tests can be run via the command line (omit the -s to\n    suppress verbose output to stdout):\n\n    ::\n\n        pytest -s test_utils.py\n""""""\n\nimport os\nfrom pathlib import Path\nimport pytest\n\nfrom jwql.utils.utils import copy_files, get_config, filename_parser, \\\n    filesystem_path, _validate_config\n\n# Determine if tests are being run on jenkins\nON_JENKINS = \'/home/jenkins\' in os.path.expanduser(\'~\')\n\nFILENAME_PARSER_TEST_DATA = [\n\n    # Test full path\n    (\'/test/dir/to/the/file/jw90002/jw90002001001_02102_00001_nis_rateints.fits\',\n     {\'activity\': \'02\',\n      \'detector\': \'nis\',\n      \'exposure_id\': \'00001\',\n      \'filename_type\': \'stage_1_and_2\',\n      \'instrument\': \'niriss\',\n      \'observation\': \'001\',\n      \'parallel_seq_id\': \'1\',\n      \'program_id\': \'90002\',\n      \'suffix\': \'rateints\',\n      \'visit\': \'001\',\n      \'visit_group\': \'02\'}),\n\n    # Test full stage 1 and 2 filename\n    (\'jw00327001001_02101_00002_nrca1_rate.fits\',\n     {\'activity\': \'01\',\n      \'detector\': \'nrca1\',\n      \'exposure_id\': \'00002\',\n      \'filename_type\': \'stage_1_and_2\',\n      \'instrument\': \'nircam\',\n      \'observation\': \'001\',\n      \'parallel_seq_id\': \'1\',\n      \'program_id\': \'00327\',\n      \'suffix\': \'rate\',\n      \'visit\': \'001\',\n      \'visit_group\': \'02\'}),\n\n    # Test root stage 1 and 2 filename\n    (\'jw00327001001_02101_00002_nrca1\',\n     {\'activity\': \'01\',\n      \'detector\': \'nrca1\',\n      \'exposure_id\': \'00002\',\n      \'filename_type\': \'stage_1_and_2\',\n      \'instrument\': \'nircam\',\n      \'observation\': \'001\',\n      \'parallel_seq_id\': \'1\',\n      \'program_id\': \'00327\',\n      \'visit\': \'001\',\n      \'visit_group\': \'02\'}),\n\n    # Test full stage 2c filename\n    (\'jw94015002002_02108_00001_mirimage_o002_crf.fits\',\n     {\'ac_id\': \'o002\',\n      \'activity\': \'08\',\n      \'detector\': \'mirimage\',\n      \'exposure_id\': \'00001\',\n      \'filename_type\': \'stage_2c\',\n      \'instrument\': \'miri\',\n      \'observation\': \'002\',\n      \'parallel_seq_id\': \'1\',\n      \'program_id\': \'94015\',\n      \'suffix\': \'crf\',\n      \'visit\': \'002\',\n      \'visit_group\': \'02\'}),\n\n    # Test root stage 2c filename\n    (\'jw90001001003_02101_00001_nis_o001\',\n     {\'ac_id\': \'o001\',\n      \'activity\': \'01\',\n      \'detector\': \'nis\',\n      \'exposure_id\': \'00001\',\n      \'filename_type\': \'stage_2c\',\n      \'instrument\': \'niriss\',\n      \'observation\': \'001\',\n      \'parallel_seq_id\': \'1\',\n      \'program_id\': \'90001\',\n      \'visit\': \'003\',\n      \'visit_group\': \'02\'}),\n\n    # Test full stage 3 filename with target_id\n    (\'jw80600-o009_t001_miri_f1130w_i2d.fits\',\n     {\'ac_id\': \'o009\',\n      \'filename_type\': \'stage_3_target_id\',\n      \'instrument\': \'miri\',\n      \'optical_elements\': \'f1130w\',\n      \'program_id\': \'80600\',\n      \'suffix\': \'i2d\',\n      \'target_id\': \'t001\'}),\n\n    # Test full stage 3 filename with target_id and different ac_id\n    (\'jw80600-c0001_t001_miri_f1130w_i2d.fits\',\n     {\'ac_id\': \'c0001\',\n      \'filename_type\': \'stage_3_target_id\',\n      \'instrument\': \'miri\',\n      \'optical_elements\': \'f1130w\',\n      \'program_id\': \'80600\',\n      \'suffix\': \'i2d\',\n      \'target_id\': \'t001\'}),\n\n    # Test full stage 3 filename with source_id\n    (\'jw80600-o009_s00001_miri_f1130w_i2d.fits\',\n     {\'ac_id\': \'o009\',\n      \'filename_type\': \'stage_3_source_id\',\n      \'instrument\': \'miri\',\n      \'optical_elements\': \'f1130w\',\n      \'program_id\': \'80600\',\n      \'source_id\': \'s00001\',\n      \'suffix\': \'i2d\'}),\n\n    # Test stage 3 filename with target_id and epoch\n    (\'jw80600-o009_t001-epoch1_miri_f1130w_i2d.fits\',\n     {\'ac_id\': \'o009\',\n      \'filename_type\': \'stage_3_target_id_epoch\',\n      \'instrument\': \'miri\',\n      \'epoch\': \'1\',\n      \'optical_elements\': \'f1130w\',\n      \'program_id\': \'80600\',\n      \'suffix\': \'i2d\',\n      \'target_id\': \'t001\'}),\n\n    # Test stage 3 filename with source_id and epoch\n    (\'jw80600-o009_s00001-epoch1_miri_f1130w_i2d.fits\',\n     {\'ac_id\': \'o009\',\n      \'filename_type\': \'stage_3_source_id_epoch\',\n      \'instrument\': \'miri\',\n      \'epoch\': \'1\',\n      \'optical_elements\': \'f1130w\',\n      \'program_id\': \'80600\',\n      \'source_id\': \'s00001\',\n      \'suffix\': \'i2d\'}),\n\n    # Test root stage 3 filename with target_id\n    (\'jw80600-o009_t001_miri_f1130w\',\n     {\'ac_id\': \'o009\',\n      \'filename_type\': \'stage_3_target_id\',\n      \'instrument\': \'miri\',\n      \'optical_elements\': \'f1130w\',\n      \'program_id\': \'80600\',\n      \'target_id\': \'t001\'}),\n\n    # Test root stage 3 filename with source_id\n    (\'jw80600-o009_s00001_miri_f1130w\',\n     {\'ac_id\': \'o009\',\n      \'filename_type\': \'stage_3_source_id\',\n      \'instrument\': \'miri\',\n      \'optical_elements\': \'f1130w\',\n      \'program_id\': \'80600\',\n      \'source_id\': \'s00001\'}),\n\n    # Test full time series filename\n    (\'jw00733003001_02101_00002-seg001_nrs1_rate.fits\',\n     {\'activity\': \'01\',\n      \'detector\': \'nrs1\',\n      \'exposure_id\': \'00002\',\n      \'filename_type\': \'time_series\',\n      \'instrument\': \'nirspec\',\n      \'observation\': \'003\',\n      \'parallel_seq_id\': \'1\',\n      \'program_id\': \'00733\',\n      \'segment\': \'001\',\n      \'suffix\': \'rate\',\n      \'visit\': \'001\',\n      \'visit_group\': \'02\'}),\n\n    # Test root time series filename\n    (\'jw00733003001_02101_00002-seg001_nrs1\',\n     {\'activity\': \'01\',\n      \'detector\': \'nrs1\',\n      \'exposure_id\': \'00002\',\n      \'filename_type\': \'time_series\',\n      \'instrument\': \'nirspec\',\n      \'observation\': \'003\',\n      \'parallel_seq_id\': \'1\',\n      \'program_id\': \'00733\',\n      \'segment\': \'001\',\n      \'visit\': \'001\',\n      \'visit_group\': \'02\'}),\n\n    # Test full guider ID filename\n    (\'jw00729011001_gs-id_1_image_cal.fits\',\n     {\'date_time\': None,\n      \'filename_type\': \'guider\',\n      \'guide_star_attempt_id\': \'1\',\n      \'guider_mode\': \'id\',\n      \'instrument\': \'fgs\',\n      \'observation\': \'011\',\n      \'program_id\': \'00729\',\n      \'suffix\': \'image_cal\',\n      \'visit\': \'001\'}),\n\n    # Test root guider ID filename\n    (\'jw00327001001_gs-id_2\',\n     {\'date_time\': None,\n      \'filename_type\': \'guider\',\n      \'guide_star_attempt_id\': \'2\',\n      \'guider_mode\': \'id\',\n      \'instrument\': \'fgs\',\n      \'observation\': \'001\',\n      \'program_id\': \'00327\',\n      \'visit\': \'001\'}),\n\n    # Test full guider non-ID filename\n    (\'jw86600048001_gs-fg_2016018175411_stream.fits\',\n     {\'date_time\': \'2016018175411\',\n      \'filename_type\': \'guider\',\n      \'guide_star_attempt_id\': None,\n      \'guider_mode\': \'fg\',\n      \'instrument\': \'fgs\',\n      \'observation\': \'048\',\n      \'program_id\': \'86600\',\n      \'suffix\': \'stream\',\n      \'visit\': \'001\'}),\n\n    # Test root guider non-ID filename\n    (\'jw00729011001_gs-acq2_2019155024808\',\n     {\'date_time\': \'2019155024808\',\n      \'filename_type\': \'guider\',\n      \'guide_star_attempt_id\': None,\n      \'guider_mode\': \'acq2\',\n      \'instrument\': \'fgs\',\n      \'observation\': \'011\',\n      \'program_id\': \'00729\',\n      \'visit\': \'001\'})\n\n]\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_copy_files():\n    """"""Test that files are copied successfully""""""\n\n    # Create an example file to be copied\n    data_dir = os.path.dirname(__file__)\n    file_to_copy = \'file.txt\'\n    original_file = os.path.join(data_dir, file_to_copy)\n    Path(original_file).touch()\n    assert os.path.exists(original_file), \'Failed to create original test file.\'\n\n    # Make a copy one level up\n    new_location = os.path.abspath(os.path.join(data_dir, \'../\'))\n    copied_file = os.path.join(new_location, file_to_copy)\n\n    # Copy the file\n    success, failure = copy_files([original_file], new_location)\n    assert success == [copied_file]\n    assert os.path.isfile(copied_file)\n\n    # Remove the copy\n    os.remove(original_file)\n    os.remove(copied_file)\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_get_config():\n    """"""Assert that the ``get_config`` function successfully creates a\n    dictionary.\n    """"""\n    settings = get_config()\n    assert isinstance(settings, dict)\n\n\n@pytest.mark.parametrize(\'filename, solution\', FILENAME_PARSER_TEST_DATA)\ndef test_filename_parser(filename, solution):\n    """"""Generate a dictionary with parameters from a JWST filename.\n    Assert that the dictionary matches what is expected.\n\n    Parameters\n    ----------\n    filename : str\n        The filename to test (e.g. ``jw00327001001_02101_00002_nrca1_rate.fits``)\n    solution : dict\n        A dictionary of the expected result\n    """"""\n\n    assert filename_parser(filename) == solution\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_filename_parser_whole_filesystem():\n    """"""Test the filename_parser on all files currently in the filesystem.""""""\n    # Get all files\n    filesystem_dir = get_config()[\'filesystem\']\n    all_files = []\n    for dir_name, _, file_list in os.walk(filesystem_dir):\n        for file in file_list:\n            if file.endswith(\'.fits\'):\n                all_files.append(os.path.join(dir_name, file))\n\n    # Run the filename_parser on all files\n    bad_filenames = []\n    for filepath in all_files:\n        try:\n            filename_parser(filepath)\n        except ValueError:\n            bad_filenames.append(os.path.basename(filepath))\n\n    # Determine if the test failed\n    fail = bad_filenames != []\n    failure_msg = \'{} files could not be successfully parsed: \\n - {}\'.\\\n        format(len(bad_filenames), \'\\n - \'.join(bad_filenames))\n\n    # Check which ones failed\n    assert not fail, failure_msg\n\n\ndef test_filename_parser_nonJWST():\n    """"""Attempt to generate a file parameter dictionary from a file\n    that is not formatted in the JWST naming convention. Ensure the\n    appropriate error is raised.\n    """"""\n    with pytest.raises(ValueError):\n        filename = \'not_a_jwst_file.fits\'\n        filename_parser(filename)\n\n\n@pytest.mark.skipif(ON_JENKINS, reason=\'Requires access to central storage.\')\ndef test_filesystem_path():\n    """"""Test that a file\'s location in the filesystem is returned""""""\n\n    filename = \'jw96003001001_02201_00001_nrca1_dark.fits\'\n    check = filesystem_path(filename)\n    location = os.path.join(get_config()[\'filesystem\'], \'jw96003\', filename)\n\n    assert check == location\n\n\ndef test_validate_config():\n    """"""Test that the config validator works.""""""\n    # Make sure a bad config raises an error\n    bad_config_dict = {""just"": ""one_key""}\n\n    with pytest.raises(Exception) as excinfo:\n        _validate_config(bad_config_dict)\n    assert \'Provided config.json does not match the required JSON schema\' \\\n           in str(excinfo.value), \\\n        \'Failed to reject incorrect JSON dict.\'\n\n    # Make sure a good config does not!\n    good_config_dict = {\n        ""connection_string"": """",\n        ""database"": {\n            ""engine"": """",\n            ""name"": """",\n            ""user"": """",\n            ""password"": """",\n            ""host"": """",\n            ""port"": """"\n        },\n        ""filesystem"": """",\n        ""preview_image_filesystem"": """",\n        ""thumbnail_filesystem"": """",\n        ""outputs"": """",\n        ""jwql_dir"": """",\n        ""admin_account"": """",\n        ""log_dir"": """",\n        ""test_dir"": """",\n        ""test_data"": """",\n        ""setup_file"": """",\n        ""auth_mast"": """",\n        ""client_id"": """",\n        ""client_secret"": """",\n        ""mast_token"": """"\n    }\n\n    is_valid = _validate_config(good_config_dict)\n    assert is_valid is None, \'Failed to validate correct JSON dict\'\n'"
jwql/utils/__init__.py,0,b''
jwql/utils/calculations.py,11,"b'""""""Various math-related functions used by the ``jwql`` instrument\nmonitors.\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\nUse\n---\n\n    This module can be imported as such:\n    ::\n\n        from jwql.utils import calculations\n        mean_val, stdev_val = calculations.mean_stdev(image, sigma_threshold=4)\n """"""\n\nimport numpy as np\n\nfrom astropy.modeling import fitting, models\nfrom astropy.stats import sigma_clip\nfrom scipy.optimize import curve_fit\nfrom scipy.stats import sigmaclip\n\n\ndef double_gaussian(x, amp1, peak1, sigma1, amp2, peak2, sigma2):\n    """"""Equate two Gaussians\n\n    Parameters\n    ----------\n    x : numpy.ndarray\n        1D array of x values to be fit\n\n    params : list\n        Gaussian coefficients\n        ``[amplitude1, peak1, stdev1, amplitude2, peak2, stdev2]``\n    """"""\n\n    y_values = amp1 * np.exp(-(x - peak1)**2.0 / (2.0 * sigma1**2.0)) \\\n        + amp2 * np.exp(-(x - peak2)**2.0 / (2.0 * sigma2**2.0))\n\n    return y_values\n\n\ndef double_gaussian_fit(x_values, y_values, input_params):\n    """"""Fit two Gaussians to the given array\n\n    Parameters\n    ----------\n    x_values : numpy.ndarray\n        1D array of x values to be fit\n\n    y_values : numpy.ndarray\n        1D array of y values to be fit\n\n    input_params : list\n        Initial guesses for Gaussian coefficients\n        ``[amplitude1, peak1, stdev1, amplitude2, peak2, stdev2]``\n\n    Returns\n    -------\n    params : list\n        Fitted parameter values\n\n    sigma : numpy.ndarray\n        Uncertainties on the parameters\n    """"""\n\n    params, cov = curve_fit(double_gaussian, x_values, y_values, input_params)\n    sigma = np.sqrt(np.diag(cov))\n\n    return params, sigma\n\n\ndef gaussian1d_fit(x_values, y_values, params):\n    """"""Fit 1D Gaussian to an array. Designed around fitting to histogram\n    of pixel values.\n\n    Parameters\n    ----------\n    x_values : numpy.ndarray\n        1D array of x values to be fit\n\n    y_values : numpy.ndarray\n        1D array of y values to be fit\n\n    Returns\n    -------\n    amplitude : tup\n        Tuple of the best fit Gaussian amplitude and uncertainty\n\n    peak : tup\n        Tuple of the best fit Gaussian peak position and uncertainty\n\n    width : tup\n        Tuple of the best fit Gaussian width and uncertainty\n    """"""\n\n    model_gauss = models.Gaussian1D(amplitude=params[0], mean=params[1], stddev=params[2])\n    fitter_gauss = fitting.LevMarLSQFitter()\n    best_fit = fitter_gauss(model_gauss, x_values, y_values)\n    cov_diag = np.diag(fitter_gauss.fit_info[\'param_cov\'])\n\n    # Arrange each parameter into (best_fit_value, uncertainty) tuple\n    amplitude = (best_fit.amplitude.value, np.sqrt(cov_diag[0]))\n    peak = (best_fit.mean.value, np.sqrt(cov_diag[1]))\n    width = (best_fit.stddev.value, np.sqrt(cov_diag[2]))\n\n    return amplitude, peak, width\n\n\ndef mean_image(cube, sigma_threshold=3):\n    """"""Combine a stack of 2D images into a mean slope image, using\n    sigma-clipping on a pixel-by-pixel basis\n\n    Parameters\n    ----------\n    cube : numpy.ndarray\n        3D array containing a stack of 2D images\n\n    sigma_threshold : int\n        Number of sigma to use when sigma-clipping values in each\n        pixel\n\n    Returns\n    -------\n    mean_image : numpy.ndarray\n        2D sigma-clipped mean image\n\n    stdev_image : numpy.ndarray\n        2D sigma-clipped standard deviation image\n    """"""\n\n    clipped_cube = sigma_clip(cube, sigma=sigma_threshold, axis=0, masked=False)\n    mean_image = np.nanmean(clipped_cube, axis=0)\n    std_image = np.nanstd(clipped_cube, axis=0)\n\n    return mean_image, std_image\n\n\ndef mean_stdev(image, sigma_threshold=3):\n    """"""Calculate the sigma-clipped mean and stdev of an input array\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Array of which to calculate statistics\n\n    sigma_threshold : float\n        Number of sigma to use when sigma-clipping\n\n    Returns\n    -------\n    mean_value : float\n        Sigma-clipped mean of image\n\n    stdev_value : float\n        Sigma-clipped standard deviation of image\n    """"""\n\n    clipped, lower, upper = sigmaclip(image, low=sigma_threshold, high=sigma_threshold)\n    mean_value = np.mean(clipped)\n    stdev_value = np.std(clipped)\n\n    return mean_value, stdev_value\n'"
jwql/utils/constants.py,0,"b'""""""Globally defined and used variables for the ``jwql`` project.\n\nAuthors\n-------\n\n    - Johannes Sahlmann\n\nUse\n---\n    This variables within this module are intended to be directly\n    imported, e.g.:\n    ::\n\n        from jwql.utils.constants import JWST_INSTRUMENT_NAMES\n\nReferences\n----------\n\n    Many variables were transferred from an earlier version of\n    ``utils.py``\n""""""\n\nimport inflection\n\n# Each amplifier is represented by 2 tuples, the first for x coordinates\n# and the second for y coordinates. Within each tuple are value for\n# starting, ending, and step size. Step size is needed for MIRI, where\n# the pixels corresponding to the 4 amplifiers are interleaved.\nAMPLIFIER_BOUNDARIES = {\'nircam\': {\'1\': [(0, 512, 1), (0, 2048, 1)],\n                                   \'2\': [(512, 1024, 1), (0, 2048, 1)],\n                                   \'3\': [(1024, 1536, 1), (0, 2048, 1)],\n                                   \'4\': [(1536, 2048, 1), (0, 2048, 1)]},\n                        \'niriss\': {\'1\': [(0, 2048, 1), (0, 512, 1)],\n                                   \'2\': [(0, 2048, 1), (512, 1024, 1)],\n                                   \'3\': [(0, 2048, 1), (1024, 1536, 1)],\n                                   \'4\': [(0, 2048, 1), (1536, 2048, 1)]},\n                        \'fgs\': {\'1\': [(0, 512, 1), (0, 2048, 1)],\n                                \'2\': [(512, 1024, 1), (0, 2048, 1)],\n                                \'3\': [(1024, 1536, 1), (0, 2048, 1)],\n                                \'4\': [(1536, 2048, 1), (0, 2048, 1)]},\n                        \'nirspec\': {\'1\': [(0, 512, 1), (0, 2048, 1)],\n                                    \'2\': [(512, 1024, 1), (0, 2048, 1)],\n                                    \'3\': [(1024, 1536, 1), (0, 2048, 1)],\n                                    \'4\': [(1536, 2048, 1), (0, 2048, 1)]},\n                        \'miri\': {\'1\': [(0, 1032, 4), (0, 1024, 1)],\n                                 \'2\': [(1, 1032, 4), (0, 1024, 1)],\n                                 \'3\': [(2, 1032, 4), (0, 1024, 1)],\n                                 \'4\': [(3, 1032, 4), (0, 1024, 1)]}}\n\nANOMALIES_PER_INSTRUMENT = {\n    # anomalies affecting all instruments:\n    \'cosmic_ray_shower\': [\'fgs\', \'miri\', \'nircam\', \'niriss\', \'nirspec\'],\n    \'diffraction_spike\': [\'fgs\', \'miri\', \'nircam\', \'niriss\', \'nirspec\'],\n    \'excessive_saturation\': [\'fgs\', \'miri\', \'nircam\', \'niriss\', \'nirspec\'],\n    \'guidestar_failure\': [\'fgs\', \'miri\', \'nircam\', \'niriss\', \'nirspec\'],\n    \'persistence\': [\'fgs\', \'miri\', \'nircam\', \'niriss\', \'nirspec\'],\n    #anomalies affecting multiple instruments:\n    \'crosstalk\': [\'fgs\', \'nircam\', \'niriss\', \'nirspec\'],\n    \'data_transfer_error\': [\'fgs\', \'nircam\', \'niriss\', \'nirspec\'],\n    \'ghost\': [\'fgs\', \'nircam\', \'niriss\', \'nirspec\'],\n    \'snowball\': [\'fgs\', \'nircam\', \'niriss\', \'nirspec\'],\n    # instrument-specific anomalies:\n    \'column_pull_up\': [\'miri\'],\n    \'dominant_msa_leakage\': [\'nirspec\'],\n    \'dragons_breath\': [\'nircam\'],\n    \'glow\': [\'miri\'],\n    \'internal_reflection\': [\'miri\'],\n    \'optical_short\': [\'nirspec\'],  # Only for MOS observations\n    \'row_pull_down\': [\'miri\'],\n    # additional anomalies:\n    \'other\': [\'fgs\', \'miri\', \'nircam\', \'niriss\', \'nirspec\']}\n\n# Defines the possible anomalies (with rendered name) to flag through the web app\nANOMALY_CHOICES = [(anomaly, inflection.titleize(anomaly)) for anomaly in ANOMALIES_PER_INSTRUMENT]\n\nFOUR_AMP_SUBARRAYS = [\'WFSS128R\', \'WFSS64R\', \'WFSS128C\', \'WFSS64C\']\n\n# Names of full-frame apertures for all instruments\nFULL_FRAME_APERTURES = {\'NIRCAM\': [\'NRCA1_FULL\', \'NRCA2_FULL\', \'NRCA3_FULL\', \'NRCA4_FULL\',\n                                   \'NRCA5_FULL\', \'NRCB1_FULL\', \'NRCB2_FULL\', \'NRCB3_FULL\',\n                                   \'NRCB4_FULL\', \'NRCB5_FULL\'],\n                        \'NIRISS\': [\'NIS_CEN\'],\n                        \'NIRSPEC\': [\'NRS1_FULL\', \'NRS2_FULL\'],\n                        \'MIRI\': [\'MIRIM_FULL\']\n                        }\n\n# Possible suffix types for nominal files\nGENERIC_SUFFIX_TYPES = [\'uncal\', \'cal\', \'rateints\', \'rate\', \'trapsfilled\', \'i2d\',\n                        \'x1dints\', \'x1d\', \'s2d\', \'s3d\', \'dark\', \'crfints\',\n                        \'crf\', \'ramp\', \'fitopt\', \'bsubints\', \'bsub\', \'cat\']\n\n# Possible suffix types for guider exposures\nGUIDER_SUFFIX_TYPES = [\'stream\', \'stacked_uncal\', \'image_uncal\', \'stacked_cal\', \'image_cal\']\n\nINSTRUMENT_MONITOR_DATABASE_TABLES = {\n    \'dark_monitor\': [\'nircam_dark_dark_current\', \'nircam_dark_pixel_stats\', \'nircam_dark_query_history\']}\n\n# JWST data products\nJWST_DATAPRODUCTS = [\'IMAGE\', \'SPECTRUM\', \'SED\', \'TIMESERIES\', \'VISIBILITY\',\n                     \'EVENTLIST\', \'CUBE\', \'CATALOG\', \'ENGINEERING\', \'NULL\']\n\n# Lowercase JWST instrument names\nJWST_INSTRUMENT_NAMES = sorted([\'niriss\', \'nircam\', \'nirspec\', \'miri\', \'fgs\'])\n\n# JWST instrument names with shorthand notation\nJWST_INSTRUMENT_NAMES_SHORTHAND = {\'gui\': \'fgs\',\n                                   \'mir\': \'miri\',\n                                   \'nis\': \'niriss\',\n                                   \'nrc\': \'nircam\',\n                                   \'nrs\': \'nirspec\'}\n\n# Mixed case JWST instrument names\nJWST_INSTRUMENT_NAMES_MIXEDCASE = {\'fgs\': \'FGS\',\n                                   \'miri\': \'MIRI\',\n                                   \'nircam\': \'NIRCam\',\n                                   \'niriss\': \'NIRISS\',\n                                   \'nirspec\': \'NIRSpec\'}\n\n# Upper case JWST instrument names\nJWST_INSTRUMENT_NAMES_UPPERCASE = {key: value.upper() for key, value in\n                                   JWST_INSTRUMENT_NAMES_MIXEDCASE.items()}\n\n# Astoquery service string for each JWST instrument\nJWST_MAST_SERVICES = [\'Mast.Jwst.Filtered.{}\'.format(value.title()) for value in\n                      JWST_INSTRUMENT_NAMES]\n\n# Available monitor names and their location for each JWST instrument\nMONITORS = {\n    \'fgs\': [(\'Bad Pixel Monitor\', \'#\')],\n    \'miri\': [(\'Dark Current Monitor\', \'#\'),\n             (\'Data Trending\', \'/miri/miri_data_trending\'),\n             (\'Bad Pixel Monitor\', \'#\'),\n             (\'Cosmic Ray Monitor\', \'#\'),\n             (\'Photometry Monitor\', \'#\'),\n             (\'TA Failure Monitor\', \'#\'),\n             (\'Blind Pointing Accuracy Monitor\', \'#\'),\n             (\'Filter and Calibration Lamp Monitor\', \'#\'),\n             (\'Thermal Emission Monitor\', \'#\')],\n    \'nircam\': [(\'Bias Monitor\', \'#\'),\n               (\'Readnoise Monitor\', \'#\'),\n               (\'Gain Level Monitor\', \'#\'),\n               (\'Mean Dark Current Rate Monitor\', \'/nircam/dark_monitor\'),\n               (\'Photometric Stability Monitor\', \'#\')],\n    \'niriss\': [(\'Bad Pixel Monitor\', \'#\'),\n               (\'Readnoise Monitor\', \'#\'),\n               (\'AMI Calibrator Monitor\', \'#\'),\n               (\'TSO RMS Monitor\', \'#\')],\n    \'nirspec\': [(\'Optical Short Monitor\', \'#\'),\n                (\'Target Acquisition Monitor\', \'#\'),\n                (\'Data Trending\', \'/nirspec/nirspec_data_trending\'),\n                (\'Detector Health Monitor\', \'#\'),\n                (\'Ref Pix Monitor\', \'#\'),\n                (\'Internal Lamp Monitor\', \'#\'),\n                (\'Instrument Model Updates\', \'#\'),\n                (\'Failed-open Shutter Monitor\', \'#\')]}\n\n# Possible suffix types for coronograph exposures\nNIRCAM_CORONAGRAPHY_SUFFIX_TYPES = [\'psfstack\', \'psfalign\', \'psfsub\']\n\n# NIRCam subarrays that use four amps for readout\nNIRCAM_FOUR_AMP_SUBARRAYS = [\'WFSS128R\', \'WFSS64R\']\n\n# NIRCam long wavelength detector names\nNIRCAM_LONGWAVE_DETECTORS = [\'NRCA5\', \'NRCB5\']\n\n# NIRCam short wavelength detector names\nNIRCAM_SHORTWAVE_DETECTORS = [\'NRCA1\', \'NRCA2\', \'NRCA3\', \'NRCA4\',\n                              \'NRCB1\', \'NRCB2\', \'NRCB3\', \'NRCB4\']\n\n# NIRCam subarrays that use either one or four amps\nNIRCAM_SUBARRAYS_ONE_OR_FOUR_AMPS = [\'SUBGRISMSTRIPE64\', \'SUBGRISMSTRIPE128\', \'SUBGRISMSTRIPE256\']\n\n# Possible suffix types for AMI files\nNIRISS_AMI_SUFFIX_TYPES = [\'amiavg\', \'aminorm\', \'ami\']\n\nSUBARRAYS_ONE_OR_FOUR_AMPS = [\'SUBGRISMSTRIPE64\', \'SUBGRISMSTRIPE128\', \'SUBGRISMSTRIPE256\']\n\n# Possible suffix types for time-series exposures\nTIME_SERIES_SUFFIX_TYPES = [\'phot\', \'whtlt\']\n\n# Concatenate all suffix types (ordered to ensure successful matching)\nFILE_SUFFIX_TYPES = GUIDER_SUFFIX_TYPES + GENERIC_SUFFIX_TYPES + \\\n                    TIME_SERIES_SUFFIX_TYPES + NIRCAM_CORONAGRAPHY_SUFFIX_TYPES + \\\n                    NIRISS_AMI_SUFFIX_TYPES\n'"
jwql/utils/credentials.py,0,"b'""""""Utility functions related to accessing remote services and databases.\n\nAuthors\n-------\n\n    - Johannes Sahlmann\n    - Lauren Chambers\n\nUse\n---\n\n    This module can be imported as such:\n    ::\n\n        import credentials\n        token = credentials.get_mast_token()\n\n """"""\nimport os\n\nfrom astroquery.mast import Mast\n\nfrom jwql.utils.utils import get_config, check_config_for_key\n\n\ndef get_mast_token(request=None):\n    """"""Return MAST token from either Astroquery.Mast, webpage cookies, the\n    JWQL configuration file, or an environment variable.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    token : str or None\n        User-specific MAST token string, if available\n    """"""\n    if Mast.authenticated():\n        print(\'Authenticated with Astroquery MAST magic\')\n        return None\n    else:\n        if request is not None:\n            token = str(request.POST.get(\'access_token\'))\n            if token != \'None\':\n                print(\'Authenticated with cached MAST token.\')\n                return token\n        try:\n            # check if token is available via config file\n            check_config_for_key(\'mast_token\')\n            token = get_config()[\'mast_token\']\n            print(\'Authenticated with config.json MAST token.\')\n            return token\n        except (KeyError, ValueError):\n            # check if token is available via environment variable\n            # see https://auth.mast.stsci.edu/info\n            try:\n                token = os.environ[\'MAST_API_TOKEN\']\n                print(\'Authenticated with MAST token environment variable.\')\n                return token\n            except KeyError:\n                return None\n'"
jwql/utils/edb.py,0,"b'#! /usr/bin/env python\n""""""Tests for the ``engineering_database`` module.\n\nAuthors\n-------\n\n    - Johannes Sahlmann\n\n\nUse\n---\n\n    These tests can be run via the command line (omit the ``-s`` to\n    suppress verbose output to ``stdout``):\n\n    ::\n\n        pytest -s test_edb_interface.py\n""""""\n\nfrom astropy.time import Time\nimport jwql.utils.engineering_database as edb\n\ndef test_query_single_mnemonic():\n    """"""Test the query of a mnemonic over a given time range.""""""\n\n    mnemonic_identifier = \'SE_ZIMIRICEA\'\n    start_time = Time(2018.01, format=\'decimalyear\')\n    end_time = Time(2018.02, format=\'decimalyear\')\n\n    mnemonic = edb.query_single_mnemonic(mnemonic_identifier, start_time, end_time)\n    print(mnemonic)\n\ndef main():\n    data, meta = edb.get_all_mnemonic_identifiers()\n    print(data)\n    test_query_single_mnemonic()\n\nif __name__ == ""__main__"":\n    main()\n'"
jwql/utils/instrument_properties.py,10,"b'""""""Collection of functions dealing with retrieving/calculating various\ninstrument properties\n\nAuthors\n-------\n\n    - Bryan Hilbert\n\nUses\n----\n\n    This module can be imported and used as such:\n\n    ::\n\n        from jwql.utils import instrument_properties as inst\n        amps = inst.amplifier_info(\'my_files.fits\')\n""""""\n\nfrom copy import deepcopy\nimport datetime\n\nfrom astropy.io import fits\nfrom jwst.datamodels import dqflags\nimport numpy as np\n\nfrom jwql.utils.constants import AMPLIFIER_BOUNDARIES, FOUR_AMP_SUBARRAYS, NIRCAM_SUBARRAYS_ONE_OR_FOUR_AMPS\n\n\ndef amplifier_info(filename, omit_reference_pixels=True):\n    """"""Calculate the number of amplifiers used to collect the data in a\n    given file using the array size and exposure time of a single frame\n    (This is needed because there is no header keyword specifying\n    how many amps were used.)\n\n    Parameters\n    ----------\n    filename : str\n        Name of fits file to investigate\n\n    omit_reference_pixels : bool\n        If ``True``, return the amp boundary coordinates excluding\n        reference pixels\n\n    Returns\n    -------\n    num_amps : int\n        Number of amplifiers used to read out the data\n\n    amp_bounds : dict\n        Dictionary of amplifier boundary coordinates. Keys are strings\n        of the amp number (1-4). Each value is a list composed of two\n        tuples. The first tuple gives the coordinates of the (minimum\n        x, maximum x, and x step value), and the second tuple gives the\n        (minimum y, maximum y, and y step value). These are set up such\n        that a list of indexes for each amplifier can be generated by using\n        ``np.mgrid[x_min: x_max: x_step, y_min: y_max: y_step]``\n    """"""\n\n    # First get necessary metadata\n    header = fits.getheader(filename)\n    instrument = header[\'INSTRUME\'].lower()\n    detector = header[\'DETECTOR\']\n    x_dim = header[\'SUBSIZE1\']\n    y_dim = header[\'SUBSIZE2\']\n    sample_time = header[\'TSAMPLE\'] * 1.e-6\n    frame_time = header[\'TFRAME\']\n    subarray_name = header[\'SUBARRAY\']\n    aperture = ""{}_{}"".format(detector, subarray_name)\n\n    # Full frame data will be 2048x2048 for all instruments\n    if instrument.lower() == \'miri\' or ((x_dim == 2048) and (y_dim == 2048)) or \\\n       subarray_name in FOUR_AMP_SUBARRAYS:\n        num_amps = 4\n        amp_bounds = deepcopy(AMPLIFIER_BOUNDARIES[instrument])\n\n    else:\n\n        if subarray_name not in NIRCAM_SUBARRAYS_ONE_OR_FOUR_AMPS:\n            num_amps = 1\n            amp_bounds = {\'1\': [(0, x_dim, 1), (0, y_dim, 1)]}\n\n        else:\n\n            # These are the tougher cases. Subarrays that can be\n            # used with multiple amp combinations\n\n            # Compare the given frametime with the calculated frametimes\n            # using 4 amps or 1 amp.\n\n            # Right now this is used only for the NIRCam grism stripe\n            # subarrays, so we don\'t need this to be a general case that\n            # can handle any subarray orientation relative to any amp\n            # orientation\n            amp4_time = calc_frame_time(instrument, aperture, x_dim, y_dim,\n                                        4, sample_time=sample_time)\n            amp1_time = calc_frame_time(instrument, aperture, x_dim, y_dim,\n                                        1, sample_time=sample_time)\n\n            if np.isclose(amp4_time, frame_time, atol=0.001, rtol=0):\n                num_amps = 4\n                # In this case, keep the full frame amp boundaries in\n                # the x direction, and set the boundaries in the y\n                # direction equal to the height of the subarray\n                amp_bounds = deepcopy(AMPLIFIER_BOUNDARIES[instrument])\n                for amp_num in [\'1\', \'2\', \'3\', \'4\']:\n                    newdims = (amp_bounds[amp_num][1][0], y_dim, 1)\n                    amp_bounds[amp_num][1] = newdims\n\n            elif np.isclose(amp1_time, frame_time, atol=0.001, rtol=0):\n                num_amps = 1\n                amp_bounds = {\'1\': [(0, x_dim, 1), (0, y_dim, 1)]}\n\n            else:\n                raise ValueError((\'Unable to determine number of amps used for exposure. 4-amp frametime\'\n                                  \'is {}. 1-amp frametime is {}. Reported frametime is {}.\')\n                                 .format(amp4_time, amp1_time, frame_time))\n\n    if omit_reference_pixels:\n\n        # If requested, ignore reference pixels by adjusting the indexes of\n        # the amp boundaries.\n        with fits.open(filename) as hdu:\n            try:\n                data_quality = hdu[\'DQ\'].data\n            except KeyError:\n                try:\n                    data_quality = hdu[\'PIXELDQ\'].data\n                except KeyError:\n                    raise KeyError(\'DQ extension not found.\')\n\n        # Reference pixels should be flagged in the DQ array with the\n        # REFERENCE_PIXEL flag. Find the science pixels by looping for\n        # pixels that don\'t have that bit set.\n        scipix = np.where(data_quality & dqflags.pixel[\'REFERENCE_PIXEL\'] == 0)\n        ymin = np.min(scipix[0])\n        xmin = np.min(scipix[1])\n        ymax = np.max(scipix[0]) + 1\n        xmax = np.max(scipix[1]) + 1\n\n        # Adjust the minimum and maximum x and y values if they are within\n        # the reference pixels\n        for key in amp_bounds:\n            bounds = amp_bounds[key]\n            prev_xmin, prev_xmax, prev_xstep = bounds[0]\n            prev_ymin, prev_ymax, prev_ystep = bounds[1]\n            if prev_xmin < xmin:\n                new_xmin = xmin\n            else:\n                new_xmin = prev_xmin\n            if prev_ymin < ymin:\n                new_ymin = ymin\n            else:\n                new_ymin = prev_ymin\n            if prev_xmax > xmax:\n                new_xmax = xmax\n            else:\n                new_xmax = prev_xmax\n            if prev_ymax > ymax:\n                new_ymax = ymax\n            else:\n                new_ymax = prev_ymax\n            amp_bounds[key] = [(new_xmin, new_xmax, prev_xstep), (new_ymin, new_ymax, prev_ystep)]\n\n    return num_amps, amp_bounds\n\n\ndef calc_frame_time(instrument, aperture, xdim, ydim, amps, sample_time=1.e-5):\n    """"""Calculate the readout time for a single frame of a given size and\n    number of amplifiers. Note that for NIRISS and FGS, the fast readout\n    direction is opposite to that in NIRCam, so we switch ``xdim`` and\n    ``ydim`` so that we can keep a single equation.\n\n    Parameters:\n    -----------\n    instrument : str\n        Name of the instrument being simulated\n\n    aperture : str\n        Name of aperture being simulated (e.g ``NRCA1_FULL``).\n        Currently this is only used to check for the FGS ``ACQ1``\n        aperture, which uses a unique value of ``colpad`` below.\n\n    xdim : int\n        Number of columns in the frame\n\n    ydim : int\n        Number of rows in the frame\n\n    amps : int\n        Number of amplifiers used to read out the frame\n\n    sample_time : float\n        Time to sample a pixel, in seconds. For NIRCam/NIRISS/FGS\n        this is 10 microseconds = 1e-5 seconds\n\n    Returns:\n    --------\n    frametime : float\n        Readout time in seconds for the frame\n    """"""\n\n    instrument = instrument.lower()\n    if instrument == ""nircam"":\n        colpad = 12\n\n        # Fullframe\n        if amps == 4:\n            rowpad = 1\n            fullpad = 1\n        else:\n            # All subarrays\n            rowpad = 2\n            fullpad = 0\n            if ((xdim <= 8) & (ydim <= 8)):\n                # The smallest subarray\n                rowpad = 3\n\n    elif instrument == ""niriss"":\n        colpad = 12\n\n        # Fullframe\n        if amps == 4:\n            rowpad = 1\n            fullpad = 1\n        else:\n            rowpad = 2\n            fullpad = 0\n\n    elif instrument == \'fgs\':\n        colpad = 6\n        if \'acq1\' in aperture.lower():\n            colpad = 12\n        rowpad = 1\n        if amps == 4:\n            fullpad = 1\n        else:\n            fullpad = 0\n\n    return ((1.0 * xdim / amps + colpad) * (ydim + rowpad) + fullpad) * sample_time\n\n\ndef get_obstime(filename):\n    """"""Extract the observation date and time from a fits file\n\n    Parameters\n    ----------\n    filename : str\n        Name of fits file\n\n    Returns\n    -------\n    obs_time : datetime.datetime\n        Observation date and time\n    """"""\n    with fits.open(filename) as h:\n        date = h[0].header[\'DATE-OBS\']\n        time = h[0].header[\'TIME-OBS\']\n    year, month, day = [int(element) for element in date.split(\'-\')]\n    hour, minute, second = [float(element) for element in time.split(\':\')]\n    return datetime.datetime(year, month, day, int(hour), int(minute), int(second))\n\n\ndef mean_time(times):\n    """"""Given a list of datetime objects, calculate the mean time\n\n    Paramters\n    ---------\n    times : list\n        List of datetime objects\n\n    Returns\n    -------\n    meantime ; datetime.datetime\n        Mean time of the input ``times``\n    """"""\n    seconds_per_day = 24. * 3600.\n    min_time = np.min(times)\n    delta_times = []\n    for time in times:\n        delta_time = time - min_time\n        total_seconds = delta_time.days * seconds_per_day + delta_time.seconds\n        delta_times.append(total_seconds)\n    mean_delta = np.mean(delta_times)\n    mean_time_delta = datetime.timedelta(0, mean_delta, 0)\n    return min_time + mean_time_delta\n'"
jwql/utils/logging_functions.py,0,"b'"""""" Logging functions for the ``jwql`` automation platform.\n\nThis module provides decorators to log the execution of modules.  Log\nfiles are written to the ``logs/`` directory in the ``jwql`` central\nstorage area, named by module name and timestamp, e.g.\n``monitor_filesystem/monitor_filesystem_2018-06-20-15:22:51.log``\n\n\nAuthors\n-------\n\n    - Catherine Martlin\n    - Alex Viana (wfc3ql Version)\n    - Matthew Bourque\n    - Jason Neal\n\nUse\n---\n\n    To log the execution of a module, use:\n    ::\n\n        import os\n        import logging\n\n        from jwql.logging.logging_functions import configure_logging\n        from jwql.logging.logging_functions import log_info\n        from jwql.logging.logging_functions import log_fail\n\n        @log_info\n        @log_fail\n        def my_main_function():\n            pass\n\n        if __name__ == \'__main__\':\n\n            module = os.path.basename(__file__).replace(\'.py\', \'\')\n            configure_logging(module)\n\n            my_main_function()\n\nDependencies\n------------\n\n    The user must have a configuration file named ``config.json``\n    placed in the ``utils`` directory and it must contain keys for\n    ``log_dir`` and ``admin_account``.\n\nReferences\n----------\n    This code is adopted and updated from python routine\n    ``logging_functions.py`` written by Alex Viana, 2013 for the WFC3\n    Quicklook automation platform.\n""""""\n\nimport datetime\nimport getpass\nimport importlib\nimport logging\nimport os\nimport pwd\nimport socket\nimport subprocess\nimport sys\nimport time\nimport traceback\n\nfrom functools import wraps\n\nfrom jwql.utils.permissions import set_permissions\nfrom jwql.utils.utils import get_config, ensure_dir_exists\n\n\ndef configure_logging(module):\n    """"""Configure the log file with a standard logging format.\n\n    Parameters\n    ----------\n    module : str\n        The name of the module being logged.\n    production_mode : bool\n        Whether or not the output should be written to the production\n        environement.\n    path : str\n        Where to write the log if user-supplied path; default to working dir.\n\n    Returns\n    -------\n    log_file : str\n        The path to the file where the log is written to.\n    """"""\n\n    # Determine log file location\n    log_file = make_log_file(module)\n\n    # Make sure no other root lhandlers exist before configuring the logger\n    for handler in logging.root.handlers[:]:\n        logging.root.removeHandler(handler)\n\n    # Create the log file and set the permissions\n    logging.basicConfig(filename=log_file,\n                        format=\'%(asctime)s %(levelname)s: %(message)s\',\n                        datefmt=\'%m/%d/%Y %H:%M:%S %p\',\n                        level=logging.INFO)\n    print(\'Log file initialized to {}\'.format(log_file))\n    set_permissions(log_file)\n\n    return log_file\n\n\ndef get_log_status(log_file):\n    """"""Returns the end status of the given ``log_file`` (i.e.\n    ``SUCCESS`` or ``FAILURE``)\n\n    Parameters\n    ----------\n    log_file : str\n        The path to the file where the log is written to\n\n    Returns\n    -------\n    status : bool\n        The status of the execution of the script described by the log\n        file (i.e. ``SUCCESS`` or ``FAILURE``)\n    """"""\n\n    with open(log_file, \'r\') as f:\n        data = f.readlines()\n    last_line = data[-1].strip()\n\n    if \'Completed Successfully\' in last_line:\n        return \'SUCCESS\'\n    else:\n        return \'FAILURE\'\n\n\ndef make_log_file(module):\n    """"""Create the log file name based on the module name.\n\n    The name of the ``log_file`` is a combination of the name of the\n    module being logged and the current datetime.\n\n    Parameters\n    ----------\n    module : str\n        The name of the module being logged.\n    production_mode : bool\n        Whether or not the output should be written to the production\n        environment.\n    path : str\n        Where to write the log if user-supplied path; default to\n        working dir.\n\n    Returns\n    -------\n    log_file : str\n        The full path to where the log file will be written to.\n    """"""\n\n    # Build filename\n    timestamp = datetime.datetime.now().strftime(\'%Y-%m-%d-%H-%M\')\n    filename = \'{0}_{1}.log\'.format(module, timestamp)\n\n    # Determine save location\n    user = pwd.getpwuid(os.getuid()).pw_name\n    admin_account = get_config()[\'admin_account\']\n    log_path = get_config()[\'log_dir\']\n\n    # For production\n    if user == admin_account and socket.gethostname()[0] == \'p\':\n        log_file = os.path.join(log_path, \'prod\', module, filename)\n\n    # For test\n    elif user == admin_account and socket.gethostname()[0] == \'t\':\n        log_file = os.path.join(log_path, \'test\', module, filename)\n\n    # For dev\n    elif user == admin_account and socket.gethostname()[0] == \'d\':\n        log_file = os.path.join(log_path, \'dev\', module, filename)\n\n    # For local (also write to dev)\n    else:\n        log_file = os.path.join(log_path, \'dev\', module, filename)\n\n    # Make sure parent directory exists\n    ensure_dir_exists(os.path.dirname(log_file))\n\n    return log_file\n\n\ndef log_info(func):\n    """"""Decorator to log useful system information.\n\n    This function can be used as a decorator to log user environment\n    and system information. Future packages we want to track can be\n    added or removed as necessary.\n\n    Parameters\n    ----------\n    func : func\n        The function to decorate.\n\n    Returns\n    -------\n    wrapped : func\n        The wrapped function.\n    """"""\n\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n\n        # Log environment information\n        logging.info(\'User: \' + getpass.getuser())\n        logging.info(\'System: \' + socket.gethostname())\n        logging.info(\'Python Version: \' + sys.version.replace(\'\\n\', \'\'))\n        logging.info(\'Python Executable Path: \' + sys.executable)\n\n        # Read in setup.py file to build list of required modules\n        with open(get_config()[\'setup_file\']) as f:\n            data = f.readlines()\n\n        for i, line in enumerate(data):\n            if \'REQUIRES = [\' in line:\n                begin = i + 1\n            elif \'setup(\' in line:\n                end = i - 2\n        required_modules = data[begin:end]\n\n        # Clean up the module list\n        module_list = [item.strip().replace(""\'"", """").replace("","", """").split(""="")[0].split("">"")[0].split(""<"")[0] for item in required_modules]\n\n        # Log common module version information\n        for module in module_list:\n            try:\n                mod = importlib.import_module(module)\n                logging.info(module + \' Version: \' + mod.__version__)\n                logging.info(module + \' Path: \' + mod.__path__[0])\n            except (ImportError, AttributeError) as err:\n                logging.warning(err)\n\n        environment = subprocess.check_output([\'conda\', \'env\', \'export\'], universal_newlines=True)\n        logging.info(\'Environment:\')\n        for line in environment.split(\'\\n\'):\n            logging.info(line)\n\n        # Call the function and time it\n        t1_cpu = time.clock()\n        t1_time = time.time()\n        func(*args, **kwargs)\n        t2_cpu = time.clock()\n        t2_time = time.time()\n\n        # Log execution time\n        hours_cpu, remainder_cpu = divmod(t2_cpu - t1_cpu, 60 * 60)\n        minutes_cpu, seconds_cpu = divmod(remainder_cpu, 60)\n        hours_time, remainder_time = divmod(t2_time - t1_time, 60 * 60)\n        minutes_time, seconds_time = divmod(remainder_time, 60)\n        logging.info(\'Elapsed Real Time: {}:{}:{}\'.format(int(hours_time), int(minutes_time), int(seconds_time)))\n        logging.info(\'Elapsed CPU Time: {}:{}:{}\'.format(int(hours_cpu), int(minutes_cpu), int(seconds_cpu)))\n\n    return wrapped\n\n\ndef log_fail(func):\n    """"""Decorator to log crashes in the decorated code.\n\n    Parameters\n    ----------\n    func : func\n        The function to decorate.\n\n    Returns\n    -------\n    wrapped : func\n        The wrapped function.\n    """"""\n\n    @wraps(func)\n    def wrapped(*args, **kwargs):\n\n        try:\n\n            # Run the function\n            func(*args, **kwargs)\n            logging.info(\'Completed Successfully\')\n\n        except Exception:\n            logging.critical(traceback.format_exc())\n            logging.critical(\'CRASHED\')\n\n    return wrapped\n\n\ndef log_timing(func):\n    """"""Decorator to time a module or function within a code.\n\n    Parameters\n    ----------\n    func : func\n        The function to time.\n\n    Returns\n    -------\n    wrapped : func\n        The wrapped function. Will log the time.""""""\n\n    def wrapped(*args, **kwargs):\n\n        # Call the function and time it\n        t1_cpu = time.process_time()\n        t1_time = time.time()\n        func(*args, **kwargs)\n        t2_cpu = time.process_time()\n        t2_time = time.time()\n\n        # Log execution time\n        hours_cpu, remainder_cpu = divmod(t2_cpu - t1_cpu, 60 * 60)\n        minutes_cpu, seconds_cpu = divmod(remainder_cpu, 60)\n        hours_time, remainder_time = divmod(t2_time - t1_time, 60 * 60)\n        minutes_time, seconds_time = divmod(remainder_time, 60)\n        logging.info(\'Elapsed Real Time of {}: {}:{}:{}\'.format(func.__name__, int(hours_time), int(minutes_time), int(seconds_time)))\n        logging.info(\'Elapsed CPU Time of {}: {}:{}:{}\'.format(func.__name__, int(hours_cpu), int(minutes_cpu), int(seconds_cpu)))\n\n    return wrapped\n'"
jwql/utils/monitor_template.py,0,"b'#! /usr/bin/env python\n\n""""""\nThis module is intended to be a template to aid in creating new\nmonitoring scripts and to demonstrate how to format them to fully\nutilize the ``jwql`` framework.\n\nEach monitoring script must be executable from the command line (i.e.\nhave a ``if \'__name__\' == \'__main__\' section), as well as have a ""main""\nfunction that calls all other functions, methods, or modules (i.e.\nthe entirety of the code is executed within the scope of the main\nfunction), as shown in this example.\n\nUsers may utilize the ``jwql`` framework functions for logging,\nsetting permissions, parsing filenames, etc. (See related ``import``s).\n\nAuthors\n-------\n\n    - Catherine Martlin\n    - Matthew Bourque\n\nUse\n---\n\n    This module can be executed from the command line:\n    ::\n\n        python monitor_template.py\n\n    Alternatively, it can be called from a python environment via the\n    following import statements:\n    ::\n\n      from monitor_template import main_monitor_function\n      from monitor_template import secondary_function\n\nDependencies\n------------\n\n    The user must have a configuration file named ``config.json``\n    placed in the ``utils`` directory.\n\nNotes\n-----\n\n    Any monitoring script written for ``jwql`` must adhere to the\n    ``jwql`` style guide located at:\n    https://github.com/spacetelescope/jwql/blob/master/style_guide/README.md\n""""""\n\nimport os\nimport logging\n\nfrom astroquery.mast import Mast\nfrom jwst import datamodels\nfrom bokeh.charts import Donut\nfrom bokeh.embed import components\n\n# Functions for logging\nfrom jwql.logging.logging_functions import configure_logging\nfrom jwql.logging.logging_functions import log_info\nfrom jwql.logging.logging_functions import log_fail\n\n# Function for setting permissions of files/directories\nfrom jwql.permissions.permissions import set_permissions\n\n# Function for parsing filenames\nfrom jwql.utils.utils import filename_parser\n\n# Objects for hard-coded information\nfrom jwql.utils.utils import get_config\nfrom jwql.utils.constants import JWST_DATAPRODUCTS, JWST_INSTRUMENT_NAMES\n\n\n@log_fail\n@log_info\ndef monitor_template_main():\n    """""" The main function of the ``monitor_template`` module.""""""\n\n    # Example of logging\n    my_variable = \'foo\'\n    logging.info(\'Some useful information: {}\'.format(my_variable))\n\n    # Example of querying for a dataset via MAST API\n    service = ""Mast.Jwst.Filtered.Niriss""\n    params = {""columns"": ""filename"",\n              ""filters"": [{""paramName"": ""filter"",\n                          ""values"": [\'F430M\']}]}\n    response = Mast.service_request_async(service, params)\n    result = response[0].json()[\'data\']\n    filename_of_interest = result[0][\'filename\']  # jw00304002001_02102_00001_nis_uncal.fits\n\n    # Example of parsing a filename\n    filename_dict = filename_parser(filename_of_interest)\n    # Contents of filename_dict:\n    #     {\'program_id\': \'00304\',\n    #      \'observation\': \'002\',\n    #      \'visit\': \'001\',\n    #      \'visit_group\': \'02\',\n    #      \'parallel_seq_id\': \'1\',\n    #      \'activity\': \'02\',\n    #      \'exposure_id\': \'00001\',\n    #      \'detector\': \'nis\',\n    #      \'suffix\': \'uncal\'}\n\n    # Example of locating a dataset in the filesystem\n    filesystem = get_config()[\'filesystem\']\n    dataset = os.path.join(filesystem, \'jw{}\'.format(filename_dict[\'program_id\']),\n                           filename_of_interest)\n\n    # Example of reading in dataset using jwst.datamodels\n    im = datamodels.open(dataset)\n    # Now have access to:\n    #     im.data  # Data array\n    #     im.err  # ERR array\n    #     im.meta  # Metadata such as header keywords\n\n    # Example of saving a file and setting permissions\n    im.save(\'some_filename.fits\')\n    set_permissions(\'some_filename.fits\')\n\n    # Example of creating and exporting a Bokeh plot\n    plt = Donut(im.data, plot_width=600, plot_height=600)\n    plt.sizing_mode = \'stretch_both\'  # Necessary for responsive sizing on web app\n    script, div = components(plt)\n\n    plot_output_dir = get_config()[\'outputs\']\n    div_outfile = os.path.join(plot_output_dir, \'monitor_name\',\n                               filename_of_interest + ""_component.html"")\n    script_outfile = os.path.join(plot_output_dir, \'monitor_name\',\n                                  filename_of_interest + ""_component.js"")\n\n    for outfile, component in zip([div_outfile, script_outfile], [div, script]):\n        with open(outfile, \'w\') as f:\n            f.write(component)\n            f.close()\n        set_permissions(outfile)\n\n    # Perform any other necessary code\n    well_named_variable = ""Function does something.""\n    result_of_second_function = second_function(well_named_variable)\n\n\ndef second_function(input_value):\n    """""" This is your axiliary function; you may have many of these.\n\n    Parameters\n    ----------\n    input_value : str\n        Some value to modify in the function.\n\n    Returns\n    -------\n    useful_result : str\n        The result of modifying the input value.\n    """"""\n\n    # Begin logging:\n    logging.info("" "")\n    logging.info(""The auxiliary function has started running."")\n\n    # Example function:\n    useful_result = input_value + "" The other function did something, too.""\n\n    logging.info(""The auxiliary function is returning: "")\n    logging.info(useful_result)\n    logging.info("" "")\n\n    return useful_result\n\n\nif __name__ == \'__main__\':\n\n    # Configure logging\n    module = os.path.basename(__file__).strip(\'.py\')\n    configure_logging(module)\n\n    # Call the main function\n    monitor_template_main()\n'"
jwql/utils/monitor_utils.py,0,"b'""""""Various utility functions for instrument monitors\n\nAuthors\n-------\n\n    - Matthew Bourque\n    - Bryan Hilbert\n\nUse\n---\n\n    This module can be imported as such:\n\n    >>> import monitor_utils\n    settings = monitor_utils.update_monitor_table(\'dark_monitor\')\n\n """"""\nimport datetime\nimport os\n\n\nfrom jwql.utils.constants import INSTRUMENT_MONITOR_DATABASE_TABLES\nfrom jwql.database.database_interface import Monitor\nfrom jwql.utils.logging_functions import configure_logging, get_log_status\n\n\ndef initialize_instrument_monitor(module):\n    """"""Configures a log file for the instrument monitor run and\n    captures the start time of the monitor\n\n    Parameters\n    ----------\n    module : str\n        The module name (e.g. ``dark_monitor``)\n\n    Returns\n    -------\n    start_time : datetime object\n        The start time of the monitor\n    log_file : str\n        The path to where the log file is stored\n    """"""\n    start_time = datetime.datetime.now()\n    log_file = configure_logging(module)\n\n    return start_time, log_file\n\n\ndef update_monitor_table(module, start_time, log_file):\n    """"""Update the ``monitor`` database table with information about\n    the instrument monitor run\n\n    Parameters\n    ----------\n    module : str\n        The module name (e.g. ``dark_monitor``)\n    start_time : datetime object\n        The start time of the monitor\n    log_file : str\n        The path to where the log file is stored\n    """"""\n    new_entry = {}\n    new_entry[\'monitor_name\'] = module\n    new_entry[\'start_time\'] = start_time\n    new_entry[\'end_time\'] = datetime.datetime.now()\n    new_entry[\'status\'] = get_log_status(log_file)\n    new_entry[\'affected_tables\'] = INSTRUMENT_MONITOR_DATABASE_TABLES[module]\n    new_entry[\'log_file\'] = os.path.basename(log_file)\n\n    Monitor.__table__.insert().execute(new_entry)\n'"
jwql/utils/permissions.py,0,"b'#! /usr/bin/env python\n""""""Permissions module for managing file permissions for ``jwql``.\n\nThis module provides ``jwql`` with functions to inspect and set file\npermissions.\n\nThe module takes as input a path to a file or directory, checks whether\nthe owner of the file is the ``jwql`` admin account, and if so, (1) set\nthe permissions appropriately, and (2) set the group membership\nappropriately.\n\nAuthors\n-------\n\n    - Johannes Sahlmann\n\nUse\n---\n\n    This module can be imported and used with\n\n    ::\n\n        from jwql.permissions import permissions\n        permissions.set_permissions(pathname)\n\n    Required arguments:\n\n    ``pathname`` - Directory or file for which the default permissions\n    should be set\n\nNotes\n-----\n\n    Permissions are set and read using the stat module, see\n    https://docs.python.org/3/library/stat.html\n\n    Below is a list with the relevant stat attribute names, integer,\n    octal, and string representations.\n\n    ::\n\n        stat_key stat_mode stat_mode_octal stat_mode_string\n        -------- --------- --------------- ----------------\n        S_IFPORT         0             0o0       ?---------\n        S_IFDOOR         0             0o0       ?---------\n         S_IXOTH         1             0o1       ?--------x\n         S_IWOTH         2             0o2       ?-------w-\n         S_IROTH         4             0o4       ?------r--\n         S_IRWXO         7             0o7       ?------rwx\n         S_IXGRP         8            0o10       ?-----x---\n         S_IWGRP        16            0o20       ?----w----\n         S_IRGRP        32            0o40       ?---r-----\n         S_IRWXG        56            0o70       ?---rwx---\n         S_IEXEC        64           0o100       ?--x------\n         S_IXUSR        64           0o100       ?--x------\n         S_IWUSR       128           0o200       ?-w-------\n        S_IWRITE       128           0o200       ?-w-------\n         S_IREAD       256           0o400       ?r--------\n         S_IRUSR       256           0o400       ?r--------\n         S_IRWXU       448           0o700       ?rwx------\n         S_ISVTX       512          0o1000       ?--------T\n         S_ISGID      1024          0o2000       ?-----S---\n         S_ISUID      2048          0o4000       ?--S------\n         S_IFIFO      4096             0o0       p---------\n         S_IFCHR      8192             0o0       c---------\n         S_IFDIR     16384             0o0       d---------\n         S_IFBLK     24576             0o0       b---------\n         S_IFREG     32768             0o0       ----------\n         S_IFLNK     40960             0o0       l---------\n        S_IFSOCK     49152             0o0       s---------\n         S_IFWHT     57344             0o0       w---------\n""""""\n\nimport grp\nimport os\nimport pwd\nimport stat\n\n# owner and group names for JWQL project\nDEFAULT_OWNER = \'jwqladm\'\nDEFAULT_GROUP = \'jwql_dev\'\n\n# set the default mode for DEFAULT_OWNER\nDEFAULT_MODE = stat.S_IRWXU | stat.S_IRGRP | stat.S_IXGRP # equivalent to \'?rwxr-x---\'\n\ndef get_group_string(pathname):\n    """"""Return the group of ``pathname`` in string representation.\n\n    Parameters\n    ----------\n    pathname : str\n        Directory or file to be inspected.\n\n    Returns\n    -------\n    group_name : str\n        String representation of the group.\n    """"""\n    file_statinfo = os.stat(pathname)\n    groupinfo = grp.getgrgid(file_statinfo.st_gid)\n    group_name = groupinfo.gr_name\n    return group_name\n\n\ndef get_owner_string(pathname):\n    """"""Return the owner of ``pathname`` in string representation.\n\n    Parameters\n    ----------\n    pathname : str\n        Directory or file to be inspected\n\n    Returns\n    -------\n    owner_name : str\n        String representation of the owner.\n    """"""\n    file_statinfo = os.stat(pathname)\n    ownerinfo = pwd.getpwuid(file_statinfo.st_uid)\n    owner_name = ownerinfo.pw_name\n    return owner_name\n\n\ndef has_permissions(pathname, owner=DEFAULT_OWNER, mode=DEFAULT_MODE, group=DEFAULT_GROUP):\n    """"""Return boolean indicating whether ``pathname`` has the specified\n    owner, permission, and group scheme.\n\n    Parameters\n    ----------\n    pathname : str\n        Directory or file to be inspected\n    owner : str\n        String representation of the owner\n    mode : int\n        Integer representation of the permission mode, compatible\n        with ``os.stat`` output\n    group : str\n        String representation of the group\n\n    Returns\n    -------\n    boolean : bool\n    """"""\n    verify_path(pathname)\n    file_statinfo = os.stat(pathname)\n    groupinfo = grp.getgrgid(file_statinfo.st_gid)\n\n    # complement mode depending on whether input is file or directory\n    if os.path.isfile(pathname):\n        mode = mode | stat.S_IFREG\n    elif os.path.isdir(pathname):\n        mode = mode | stat.S_IFDIR\n\n    if (get_owner_string(pathname) != owner) or (file_statinfo.st_mode != mode)\\\n            or (groupinfo.gr_name != group):\n        return False\n\n    return True\n\n\ndef set_permissions(pathname, owner=DEFAULT_OWNER, mode=DEFAULT_MODE, group=DEFAULT_GROUP, verbose=False):\n    """"""Set mode and group of the file/directory identfied by\n    ``pathname``, if and only if it is owned by ``owner``.\n\n    Parameters\n    ----------\n    pathname : str\n        Directory or file to be inspected\n    owner : str\n        String representation of the owner\n    mode : int\n        Integer representation of the permission mode, compatible with\n        ``os.stat`` output\n    group : str\n        String representation of the group\n    verbose : bool\n        Boolean indicating whether verbose output is requested\n    """"""\n    if verbose:\n        print(\'\\nBefore:\')\n        show_permissions(pathname)\n\n    if not has_permissions(pathname):\n        if get_owner_string(pathname) == owner:\n            os.chmod(pathname, mode)\n            # change group but not owner\n            os.chown(pathname, -1, grp.getgrnam(group).gr_gid)\n\n    if verbose:\n        print(\'After:\')\n        show_permissions(pathname)\n\n\ndef show_permissions(pathname):\n    """"""Verbose output showing group, user, and permission information\n    for a directory or file.\n\n    Parameters\n    ----------\n    pathname : str\n        Directory or file to be inspected\n    """"""\n    verify_path(pathname)\n    file_statinfo = os.stat(pathname)\n    ownerinfo = pwd.getpwuid(file_statinfo.st_uid)\n    groupinfo = grp.getgrgid(file_statinfo.st_gid)\n\n    if os.path.isdir(pathname):\n        info_string = \'directory\'\n    elif os.path.isfile(pathname):\n        info_string = \'file\'\n\n    print(\'Inspecting {}: {}\'.format(info_string, pathname))\n    print(\'group {} = {}\'.format(file_statinfo.st_gid, groupinfo.gr_name))\n    print(\'owner {} = {}\'.format(file_statinfo.st_uid, ownerinfo.pw_name))\n    print(\'mode {} = {}\'.format(file_statinfo.st_mode, stat.filemode(file_statinfo.st_mode)))\n    print(\'\')\n\n\ndef verify_path(pathname):\n    """"""Verify that pathname is either a directory or a file. If not, an\n    error is raised.\n\n    Parameters\n    ----------\n    pathname : str\n        Directory or file to be inspected\n    """"""\n    if (not os.path.isdir(pathname)) and (not os.path.isfile(pathname)):\n        raise NotImplementedError(\'{} is not a valid path or filename\'.format(pathname))\n'"
jwql/utils/plotting.py,0,"b'#! /usr/bin/env python\n\n""""""\nThis module is a collection of plotting functions that may be used\nacross the ``jwql`` application.\n\nAuthors:\n--------\n\n    - Joe Filippazzo\n\nUse:\n----\n\n    This module can be use as follows:\n\n    ::\n\n        from jwql.utils import plotting\n        from pandas import DataFrame\n        data = DataFrame({\'meow\': {\'foo\': 12, \'bar\': 23, \'baz\': 2},\n                          \'mix\': {\'foo\': 45, \'bar\': 31, \'baz\': 23},\n                          \'deliver\': {\'foo\': 62, \'bar\': 20, \'baz\': 9}})\n        data = data.reset_index()\n        plt = plotting.bar_chart(data, \'index\')\n""""""\n\nfrom bokeh.models import ColumnDataSource, FactorRange, HoverTool\nfrom bokeh.palettes import Category20c\nfrom bokeh.plotting import figure\nfrom bokeh.transform import factor_cmap\n\n\ndef bar_chart(dataframe, groupcol, datacols=None, **kwargs):\n    """"""Create a pie chart from a Pandas DataFrame\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        A dataframe of values\n    groupcol : str\n        The name of the column with the group labels\n    datacol : str, sequence (optional)\n        The name or list of names of the column containing the data.\n        In None, uses all columns except **groupcol**\n\n    Returns\n    -------\n    plt : obj\n        The generated bokeh.figure object\n    """"""\n    \n    # Get the groups\n    groups = list(dataframe[groupcol])\n\n    # Get the datacols\n    if datacols is None:\n        datacols = [col for col in list(dataframe.columns) if col != groupcol]\n\n    # Make a dictionary of the groups and data\n    data = {\'groups\': groups}\n    for col in datacols:\n        data.update({col: list(dataframe[col])})\n\n    # hstack it\n    x = [(group, datacol) for group in groups for datacol in datacols]\n    counts = sum(zip(*[data[col] for col in datacols]), ())\n    colors = max(3, len(datacols))\n    source = ColumnDataSource(data=dict(x=x, counts=counts))\n\n    # Make the figure\n    hover = HoverTool(tooltips=[(\'count\', \'@counts\')])\n    plt = figure(x_range=FactorRange(*x), plot_height=250, tools=[hover],\n                 **kwargs)\n    plt.vbar(x=\'x\', top=\'counts\', width=0.9, source=source, line_color=""white"",\n             fill_color=factor_cmap(\'x\', palette=Category20c[colors],\n                                    factors=datacols, start=1, end=2))\n\n    # Formatting\n    plt.y_range.start = 0\n    plt.x_range.range_padding = 0.1\n    plt.xaxis.major_label_orientation = 1\n    plt.xgrid.grid_line_color = None\n\n    return plt\n'"
jwql/utils/preview_image.py,9,"b'#! /usr/bin/env python\n\n""""""\nCreate a preview image from a fits file containing an observation.\n\nThis module creates and saves a ""preview image"" from a fits file that\ncontains a JWST observation. Data from the user-supplied ``extension``\nof the file are read in, along with the ``PIXELDQ`` extension if\npresent. For each integration in the exposure, the first group is\nsubtracted from the final group in order to create a difference image.\nThe lower and upper limits to be displayed are defined as the\n``clip_percent`` and ``(1. - clip_percent)`` percentile signals.\n``matplotlib`` is then used to display a linear- or log-stretched\nversion of the image, with accompanying colorbar. The image is then\nsaved.\n\nAuthors:\n--------\n\n    - Bryan Hilbert\n\nUse:\n----\n\n    This module can be imported as such:\n\n    ::\n\n        from jwql.preview_image.preview_image import PreviewImage\n        im = PreviewImage(my_file, ""SCI"")\n        im.clip_percent = 0.01\n        im.scaling = \'log\'\n        im.output_format = \'jpg\'\n        im.make_image()\n""""""\n\nimport logging\nimport os\nimport socket\n\nfrom astropy.io import fits\nimport numpy as np\n\nfrom jwql.utils import permissions\n\n# Use the \'Agg\' backend to avoid invoking $DISPLAY\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\n# Only import jwst if not running from readthedocs\nif \'build\' and \'project\' not in socket.gethostname():\n    from jwst.datamodels import dqflags\n\n\nclass PreviewImage():\n    """"""An object for generating and saving preview images, used by\n    ``generate_preview_images``.\n\n    Attributes\n    ----------\n    clip_percent : float\n        The amount to sigma clip the input data by when scaling the\n        preview image.  Default is 0.01.\n    cmap : str\n        The colormap used by ``matplotlib`` in the preview image.\n        Default value is ``viridis``.\n    data : obj\n        The data used to generate the preview image.\n    dq : obj\n        The DQ data used to generate the preview image.\n    file : str\n        The filename to generate the preview image from.\n    output_format : str\n        The format to which the preview image is saved.  Options are\n        ``jpg`` and ``thumb``\n    preview_output_directory : str or None\n        The output directory to which the preview image is saved.\n    scaling : str\n        The scaling used in the preview image.  Default is ``log``.\n    thumbnail_output_directory : str or None\n        The output directory to which the thumbnail is saved.\n\n    Methods\n    -------\n    difference_image(data)\n        Create a difference image from the data\n    find_limits(data, pixmap, clipperc)\n        Find the min and max signal levels after clipping by\n        ``clipperc``\n    get_data(filename, ext)\n        Read in data from the given ``filename`` and ``ext``\n    make_figure(image, integration_number, min_value, max_value, scale, maxsize, thumbnail)\n        Create the ``matplotlib`` figure\n    make_image(max_img_size)\n        Main function\n    save_image(fname, thumbnail)\n        Save the figure\n    """"""\n\n    def __init__(self, filename, extension):\n        """"""Initialize the class.\n\n        Parameters\n        ----------\n        filename : str\n            Name of fits file containing data\n        extension : str\n            Extension name to be read in\n        """"""\n        self.clip_percent = 0.01\n        self.cmap = \'viridis\'\n        self.file = filename\n        self.output_format = \'jpg\'\n        self.preview_output_directory = None\n        self.scaling = \'log\'\n        self.thumbnail_output_directory = None\n\n        # Read in file\n        self.data, self.dq = self.get_data(self.file, extension)\n\n    def difference_image(self, data):\n        """"""\n        Create a difference image from the data. Use last group minus\n        first group in order to maximize signal to noise. With 4D\n        input, make a separate difference image for each integration.\n\n        Parameters\n        ----------\n        data : obj\n            4D ``numpy`` ``ndarray`` array of floats\n\n        Returns\n        -------\n        result : obj\n            3D ``numpy`` ``ndarray`` containing the difference image(s)\n            from the input exposure\n        """"""\n        return data[:, -1, :, :] - data[:, 0, :, :]\n\n    def find_limits(self, data, pixmap, clipperc):\n        """"""\n        Find the minimum and maximum signal levels after clipping the\n        top and bottom ``clipperc`` of the pixels.\n\n        Parameters\n        ----------\n        data : obj\n            2D numpy ndarray of floats\n        pixmap : obj\n            2D numpy ndarray boolean array of science pixel locations\n            (``True`` for science pixels, ``False`` for non-science\n            pixels)\n        clipperc : float\n            Fraction of top and bottom signal levels to clip (e.g. 0.01\n            means to clip brightest and dimmest 1% of pixels)\n\n        Returns\n        -------\n        results : tuple\n            Tuple of floats, minimum and maximum signal levels\n        """"""\n        nelem = np.sum(pixmap)\n        numclip = np.int(clipperc * nelem)\n        sorted = np.sort(data[pixmap], axis=None)\n        minval = sorted[numclip]\n        maxval = sorted[-numclip - 1]\n        return (minval, maxval)\n\n    def get_data(self, filename, ext):\n        """"""\n        Read in the data from the given file and extension.  Also find\n        how many rows/cols of reference pixels are present.\n\n        Parameters\n        ----------\n        filename : str\n            Name of fits file containing data\n        ext : str\n            Extension name to be read in\n\n        Returns\n        -------\n        data : obj\n            Science data from file. A 2-, 3-, or 4D numpy ndarray\n        dq : obj\n            2D ``ndarray`` boolean map of reference pixels. Science\n            pixels flagged as ``True`` and non-science pixels are\n            ``False``\n        """"""\n        if os.path.isfile(filename):\n            extnames = []\n            with fits.open(filename) as hdulist:\n                for exten in hdulist:\n                    try:\n                        extnames.append(exten.header[\'EXTNAME\'])\n                    except:\n                        pass\n                if ext in extnames:\n                    dimensions = len(hdulist[ext].data.shape)\n                    if dimensions == 4:\n                        data = hdulist[ext].data[:, [0, -1], :, :].astype(np.float)\n                    else:\n                        data = hdulist[ext].data.astype(np.float)\n                else:\n                    raise ValueError(\'WARNING: no {} extension in {}!\'.format(ext, filename))\n\n                if \'PIXELDQ\' in extnames:\n                    dq = hdulist[\'PIXELDQ\'].data\n                    dq = (dq & dqflags.pixel[\'NON_SCIENCE\'] == 0)\n                else:\n                    yd, xd = data.shape[-2:]\n                    dq = np.ones((yd, xd), dtype=""bool"")\n\n                # Collect information on aperture location within the\n                # full detector. This is needed for mosaicking NIRCam\n                # detectors later.\n                try:\n                    self.xstart = hdulist[0].header[\'SUBSTRT1\']\n                    self.ystart = hdulist[0].header[\'SUBSTRT2\']\n                    self.xlen = hdulist[0].header[\'SUBSIZE1\']\n                    self.ylen = hdulist[0].header[\'SUBSIZE2\']\n                except KeyError:\n                    logging.warning(\'SUBSTR and SUBSIZE header keywords not found\')\n\n        else:\n            raise FileNotFoundError(\'WARNING: {} does not exist!\'.format(filename))\n\n        return data, dq\n\n    def make_figure(self, image, integration_number, min_value, max_value,\n                    scale, maxsize=8, thumbnail=False):\n        """"""\n        Create the matplotlib figure of the image\n\n        Parameters\n        ----------\n        image : obj\n            2D ``numpy`` ``ndarray`` of floats\n\n        integration_number : int\n            Integration number within exposure\n\n        min_value : float\n            Minimum value for display\n\n        max_value : float\n            Maximum value for display\n\n        scale : str\n            Image scaling (``log``, ``linear``)\n\n        maxsize : int\n            Size of the longest dimension of the output figure (inches)\n\n        thumbnail : bool\n            True to create a thumbnail image, False to create the full\n            preview image\n\n        Returns\n        -------\n        result : obj\n            Matplotlib Figure object\n        """"""\n\n        # Check the input scaling\n        if scale not in [\'linear\', \'log\']:\n            raise ValueError(\'WARNING: scaling option {} not supported.\'.format(scale))\n\n        # Set the figure size\n        yd, xd = image.shape\n        ratio = yd / xd\n        if xd >= yd:\n            xsize = maxsize\n            ysize = maxsize * ratio\n        else:\n            ysize = maxsize\n            xsize = maxsize / ratio\n\n        if scale == \'log\':\n\n            # Shift data so everything is positive\n            shiftdata = image - min_value + 1\n            shiftmin = 1\n            shiftmax = max_value - min_value + 1\n\n            # If making a thumbnail, make a figure with no axes\n            if thumbnail:\n                fig = plt.imshow(shiftdata,\n                                 norm=colors.LogNorm(vmin=shiftmin,\n                                                     vmax=shiftmax),\n                                 cmap=self.cmap)\n                # Invert y axis\n                plt.gca().invert_yaxis()\n\n                plt.axis(\'off\')\n                fig.axes.get_xaxis().set_visible(False)\n                fig.axes.get_yaxis().set_visible(False)\n\n            # If preview image, add axes and colorbars\n            else:\n                fig, ax = plt.subplots(figsize=(xsize, ysize))\n                cax = ax.imshow(shiftdata,\n                                norm=colors.LogNorm(vmin=shiftmin,\n                                                    vmax=shiftmax),\n                                cmap=self.cmap)\n                # Invert y axis\n                plt.gca().invert_yaxis()\n\n                # Add colorbar, with original data values\n                tickvals = np.logspace(np.log10(shiftmin), np.log10(shiftmax), 5)\n                tlabelflt = tickvals + min_value - 1\n\n                # Adjust the number of digits after the decimal point\n                # in the colorbar labels based on the signal range\n                delta = tlabelflt[-1] - tlabelflt[0]\n                if delta >= 100:\n                    dig = 0\n                elif ((delta < 100) & (delta >= 10)):\n                    dig = 1\n                elif ((delta < 10) & (delta >= 1)):\n                    dig = 2\n                elif delta < 1:\n                    dig = 3\n                format_string = ""%.{}f"".format(dig)\n                tlabelstr = [format_string % number for number in tlabelflt]\n                cbar = fig.colorbar(cax, ticks=tickvals)\n                cbar.ax.set_yticklabels(tlabelstr)\n                cbar.ax.tick_params(labelsize=maxsize * 5. / 4)\n                ax.set_xlabel(\'Pixels\', fontsize=maxsize * 5. / 4)\n                ax.set_ylabel(\'Pixels\', fontsize=maxsize * 5. / 4)\n                ax.tick_params(labelsize=maxsize)\n                plt.rcParams.update({\'axes.titlesize\': \'small\'})\n                plt.rcParams.update({\'font.size\': maxsize * 5. / 4})\n                plt.rcParams.update({\'axes.labelsize\': maxsize * 5. / 4})\n                plt.rcParams.update({\'ytick.labelsize\': maxsize * 5. / 4})\n                plt.rcParams.update({\'xtick.labelsize\': maxsize * 5. / 4})\n\n        elif scale == \'linear\':\n            fig, ax = plt.subplots(figsize=(xsize, ysize))\n            cax = ax.imshow(image, clim=(min_value, max_value), cmap=self.cmap)\n\n            if not thumbnail:\n                cbar = fig.colorbar(cax)\n                ax.set_xlabel(\'Pixels\')\n                ax.set_ylabel(\'Pixels\')\n\n        # If preview image, set a title\n        if not thumbnail:\n            filename = os.path.split(self.file)[-1]\n            ax.set_title(filename + \' Int: {}\'.format(np.int(integration_number)))\n\n    def make_image(self, max_img_size=8):\n        """"""The main function of the ``PreviewImage`` class.""""""\n\n        shape = self.data.shape\n\n        if len(shape) == 4:\n            # Create difference image(s)\n            diff_img = self.difference_image(self.data)\n        elif len(shape) < 4:\n            diff_img = self.data\n\n        # If there are multiple integrations in the file,\n        # work on one integration at a time from here onwards\n        ndim = len(diff_img.shape)\n        if ndim == 2:\n            diff_img = np.expand_dims(diff_img, axis=0)\n        nint, ny, nx = diff_img.shape\n\n        for i in range(nint):\n            frame = diff_img[i, :, :]\n\n            # Find signal limits for the display\n            minval, maxval = self.find_limits(frame, self.dq,\n                                              self.clip_percent)\n\n            # Create preview image matplotlib object\n            indir, infile = os.path.split(self.file)\n            suffix = \'_integ{}.{}\'.format(i, self.output_format)\n            if self.preview_output_directory is None:\n                outdir = indir\n            else:\n                outdir = self.preview_output_directory\n            outfile = os.path.join(outdir, infile.split(\'.\')[0] + suffix)\n            self.make_figure(frame, i, minval, maxval, self.scaling.lower(),\n                             maxsize=max_img_size, thumbnail=False)\n            self.save_image(outfile, thumbnail=False)\n            plt.close()\n\n            # Create thumbnail image matplotlib object\n            if self.thumbnail_output_directory is None:\n                outdir = indir\n            else:\n                outdir = self.thumbnail_output_directory\n            outfile = os.path.join(outdir, infile.split(\'.\')[0] + suffix)\n            self.make_figure(frame, i, minval, maxval, self.scaling.lower(),\n                             maxsize=max_img_size, thumbnail=True)\n            self.save_image(outfile, thumbnail=True)\n            plt.close()\n\n    def save_image(self, fname, thumbnail=False):\n        """"""\n        Save an image in the requested output format and sets the\n        appropriate permissions\n\n        Parameters\n        ----------\n        image : obj\n            A ``matplotlib`` figure object\n\n        fname : str\n            Output filename\n\n        thumbnail : bool\n            True if saving a thumbnail image, false for the full\n            preview image.\n        """"""\n\n        plt.savefig(fname, bbox_inches=\'tight\', pad_inches=0)\n        permissions.set_permissions(fname)\n\n        # If the image is a thumbnail, rename to \'.thumb\'\n        if thumbnail:\n            thumb_fname = fname.replace(\'.jpg\', \'.thumb\')\n            os.rename(fname, thumb_fname)\n            logging.info(\'Saved image to {}\'.format(thumb_fname))\n        else:\n            logging.info(\'Saved image to {}\'.format(fname))\n'"
jwql/utils/utils.py,0,"b'""""""Various utility functions for the ``jwql`` project.\n\nAuthors\n-------\n\n    - Matthew Bourque\n    - Lauren Chambers\n\nUse\n---\n\n    This module can be imported as such:\n\n    >>> import utils\n    settings = get_config()\n\nReferences\n----------\n\n    Filename parser modified from Joe Hunkeler:\n    https://gist.github.com/jhunkeler/f08783ca2da7bfd1f8e9ee1d207da5ff\n\n    Various documentation related to JWST filename conventions:\n    - https://jwst-docs.stsci.edu/display/JDAT/File+Naming+Conventions+and+Data+Products\n    - https://innerspace.stsci.edu/pages/viewpage.action?pageId=94092600\n    - https://innerspace.stsci.edu/pages/viewpage.action?spaceKey=SCSB&title=JWST+Science+Data+Products\n    - https://jwst-docs.stsci.edu/display/JDAT/Understanding+Associations?q=association%20candidate\n    - https://jwst-pipeline.readthedocs.io/en/stable/jwst/introduction.html#pipeline-step-suffix-definitions\n    - JWST TR JWST-STScI-004800, SM-12\n """"""\n\nimport datetime\nimport getpass\nimport json\nimport os\nimport re\nimport shutil\n\nimport jsonschema\n\nfrom jwql.utils import permissions\nfrom jwql.utils.constants import FILE_SUFFIX_TYPES, JWST_INSTRUMENT_NAMES_SHORTHAND\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\n\ndef copy_files(files, out_dir):\n    """"""Copy a given file to a given directory. Only try to copy the file\n    if it is not already present in the output directory.\n\n    Parameters\n    ----------\n    files : list\n        List of files to be copied\n\n    out_dir : str\n        Destination directory\n\n    Returns\n    -------\n    success : list\n        Files successfully copied (or that already existed in out_dir)\n\n    failed : list\n        Files that were not copied\n    """"""\n\n    # Copy files if they do not already exist\n    success = []\n    failed = []\n    for input_file in files:\n        input_new_path = os.path.join(out_dir, os.path.basename(input_file))\n        if os.path.isfile(input_new_path):\n            success.append(input_new_path)\n        else:\n            try:\n                shutil.copy2(input_file, out_dir)\n                success.append(input_new_path)\n                permissions.set_permissions(input_new_path)\n            except:\n                failed.append(input_file)\n    return success, failed\n\n\ndef download_mast_data(query_results, output_dir):\n    """"""Example function for downloading MAST query results. From MAST\n    website (``https://mast.stsci.edu/api/v0/pyex.html``)\n\n    Parameters\n    ----------\n    query_results : list\n        List of dictionaries returned by a MAST query.\n\n    output_dir : str\n        Directory into which the files will be downlaoded\n    """"""\n\n    # Set up the https connection\n    server = \'mast.stsci.edu\'\n    conn = httplib.HTTPSConnection(server)\n\n    # Dowload the products\n    print(\'Number of query results: {}\'.format(len(query_results)))\n\n    for i in range(len(query_results)):\n\n        # Make full output file path\n        output_file = os.path.join(output_dir, query_results[i][\'filename\'])\n\n        print(\'Output file is {}\'.format(output_file))\n\n        # Download the data\n        uri = query_results[i][\'dataURI\']\n\n        print(\'uri is {}\'.format(uri))\n\n        conn.request(""GET"", ""/api/v0/download/file?uri="" + uri)\n        resp = conn.getresponse()\n        file_content = resp.read()\n\n        # Save to file\n        with open(output_file, \'wb\') as file_obj:\n            file_obj.write(file_content)\n\n        # Check for file\n        if not os.path.isfile(output_file):\n            print(""ERROR: {} failed to download."".format(output_file))\n        else:\n            statinfo = os.stat(output_file)\n            if statinfo.st_size > 0:\n                print(""DOWNLOAD COMPLETE: "", output_file)\n            else:\n                print(""ERROR: {} file is empty."".format(output_file))\n    conn.close()\n\n\ndef ensure_dir_exists(fullpath):\n    """"""Creates dirs from ``fullpath`` if they do not already exist.""""""\n    if not os.path.exists(fullpath):\n        os.makedirs(fullpath)\n        permissions.set_permissions(fullpath)\n\n\ndef filename_parser(filename):\n    """"""Return a dictionary that contains the properties of a given\n    JWST file (e.g. program ID, visit number, detector, etc.).\n\n    Parameters\n    ----------\n    filename : str\n        Path or name of JWST file to parse\n\n    Returns\n    -------\n    filename_dict : dict\n        Collection of file properties\n\n    Raises\n    ------\n    ValueError\n        When the provided file does not follow naming conventions\n    """"""\n\n    filename = os.path.basename(filename)\n    file_root_name = (len(filename.split(\'.\')) < 2)\n\n    # Stage 1 and 2 filenames\n    # e.g. ""jw80500012009_01101_00012_nrcalong_uncal.fits""\n    stage_1_and_2 = \\\n        r""jw"" \\\n        r""(?P<program_id>\\d{5})""\\\n        r""(?P<observation>\\d{3})""\\\n        r""(?P<visit>\\d{3})""\\\n        r""_(?P<visit_group>\\d{2})""\\\n        r""(?P<parallel_seq_id>\\d{1})""\\\n        r""(?P<activity>\\w{2})""\\\n        r""_(?P<exposure_id>\\d+)""\\\n        r""_(?P<detector>((?!_)[\\w])+)""\n\n    # Stage 2c outlier detection filenames\n    # e.g. ""jw94015002002_02108_00001_mirimage_o002_crf.fits""\n    stage_2c = \\\n        r""jw"" \\\n        r""(?P<program_id>\\d{5})"" \\\n        r""(?P<observation>\\d{3})"" \\\n        r""(?P<visit>\\d{3})"" \\\n        r""_(?P<visit_group>\\d{2})"" \\\n        r""(?P<parallel_seq_id>\\d{1})"" \\\n        r""(?P<activity>\\w{2})"" \\\n        r""_(?P<exposure_id>\\d+)"" \\\n        r""_(?P<detector>((?!_)[\\w])+)""\\\n        r""_(?P<ac_id>(o\\d{3}|(c|a|r)\\d{4}))""\n\n    # Stage 3 filenames with target ID\n    # e.g. ""jw80600-o009_t001_miri_f1130w_i2d.fits""\n    stage_3_target_id = \\\n        r""jw"" \\\n        r""(?P<program_id>\\d{5})""\\\n        r""-(?P<ac_id>(o\\d{3}|(c|a|r)\\d{4}))""\\\n        r""_(?P<target_id>(t)\\d{3})""\\\n        r""_(?P<instrument>(nircam|niriss|nirspec|miri|fgs))""\\\n        r""_(?P<optical_elements>((?!_)[\\w-])+)""\n\n    # Stage 3 filenames with source ID\n    # e.g. ""jw80600-o009_s00001_miri_f1130w_i2d.fits""\n    stage_3_source_id = \\\n        r""jw"" \\\n        r""(?P<program_id>\\d{5})""\\\n        r""-(?P<ac_id>(o\\d{3}|(c|a|r)\\d{4}))""\\\n        r""_(?P<source_id>(s)\\d{5})""\\\n        r""_(?P<instrument>(nircam|niriss|nirspec|miri|fgs))""\\\n        r""_(?P<optical_elements>((?!_)[\\w-])+)""\n\n    # Stage 3 filenames with target ID and epoch\n    # e.g. ""jw80600-o009_t001-epoch1_miri_f1130w_i2d.fits""\n    stage_3_target_id_epoch = \\\n        r""jw"" \\\n        r""(?P<program_id>\\d{5})""\\\n        r""-(?P<ac_id>(o\\d{3}|(c|a|r)\\d{4}))""\\\n        r""_(?P<target_id>(t)\\d{3})""\\\n        r""-epoch(?P<epoch>\\d{1})""\\\n        r""_(?P<instrument>(nircam|niriss|nirspec|miri|fgs))""\\\n        r""_(?P<optical_elements>((?!_)[\\w-])+)""\n\n    # Stage 3 filenames with source ID and epoch\n    # e.g. ""jw80600-o009_s00001-epoch1_miri_f1130w_i2d.fits""\n    stage_3_source_id_epoch = \\\n        r""jw"" \\\n        r""(?P<program_id>\\d{5})""\\\n        r""-(?P<ac_id>(o\\d{3}|(c|a|r)\\d{4}))""\\\n        r""_(?P<source_id>(s)\\d{5})""\\\n        r""-epoch(?P<epoch>\\d{1})""\\\n        r""_(?P<instrument>(nircam|niriss|nirspec|miri|fgs))""\\\n        r""_(?P<optical_elements>((?!_)[\\w-])+)""\n\n    # Time series filenames\n    # e.g. ""jw00733003001_02101_00002-seg001_nrs1_rate.fits""\n    time_series = \\\n        r""jw"" \\\n        r""(?P<program_id>\\d{5})""\\\n        r""(?P<observation>\\d{3})""\\\n        r""(?P<visit>\\d{3})""\\\n        r""_(?P<visit_group>\\d{2})""\\\n        r""(?P<parallel_seq_id>\\d{1})""\\\n        r""(?P<activity>\\w{2})""\\\n        r""_(?P<exposure_id>\\d+)""\\\n        r""-seg(?P<segment>\\d{3})""\\\n        r""_(?P<detector>\\w+)""\n\n    # Guider filenames\n    # e.g. ""jw00729011001_gs-id_1_image_cal.fits"" or\n    # ""jw00799003001_gs-acq1_2019154181705_stream.fits""\n    guider = \\\n        r""jw"" \\\n        r""(?P<program_id>\\d{5})"" \\\n        r""(?P<observation>\\d{3})"" \\\n        r""(?P<visit>\\d{3})"" \\\n        r""_gs-(?P<guider_mode>(id|acq1|acq2|track|fg))"" \\\n        r""_((?P<date_time>\\d{13})|(?P<guide_star_attempt_id>\\d{1}))""\n\n    # Build list of filename types\n    filename_types = [\n        stage_1_and_2,\n        stage_2c,\n        stage_3_target_id,\n        stage_3_source_id,\n        stage_3_target_id_epoch,\n        stage_3_source_id_epoch,\n        time_series,\n        guider]\n\n    filename_type_names = [\n        \'stage_1_and_2\',\n        \'stage_2c\',\n        \'stage_3_target_id\',\n        \'stage_3_source_id\',\n        \'stage_3_target_id_epoch\',\n        \'stage_3_source_id_epoch\',\n        \'time_series\',\n        \'guider\'\n    ]\n\n    # Try to parse the filename\n    for filename_type, filename_type_name in zip(filename_types, filename_type_names):\n\n        # If full filename, try using suffix\n        if not file_root_name:\n            filename_type += r""_(?P<suffix>{}).*"".format(\'|\'.join(FILE_SUFFIX_TYPES))\n        # If not, make sure the provided regex matches the entire filename root\n        else:\n            filename_type += r""$""\n\n        elements = re.compile(filename_type)\n        jwst_file = elements.match(filename)\n\n        # Stop when you find a format that matches\n        if jwst_file is not None:\n            name_match = filename_type_name\n            break\n\n    try:\n        # Convert the regex match to a dictionary\n        filename_dict = jwst_file.groupdict()\n\n        # Add the filename type to that dict\n        filename_dict[\'filename_type\'] = name_match\n\n        # Also, add the instrument if not already there\n        if \'instrument\' not in filename_dict.keys():\n            if name_match == \'guider\':\n                filename_dict[\'instrument\'] = \'fgs\'\n            elif \'detector\' in filename_dict.keys():\n                filename_dict[\'instrument\'] = JWST_INSTRUMENT_NAMES_SHORTHAND[\n                    filename_dict[\'detector\'][:3]\n                ]\n\n    # Raise error if unable to parse the filename\n    except AttributeError:\n        jdox_url = \'https://jwst-docs.stsci.edu/display/JDAT/\' \\\n                   \'File+Naming+Conventions+and+Data+Products\'\n        raise ValueError(\n            \'Provided file {} does not follow JWST naming conventions.  \'\n            \'See {} for further information.\'.format(filename, jdox_url)\n        )\n\n    return filename_dict\n\n\ndef filesystem_path(filename):\n    """"""Return the full path to a given file in the filesystem\n\n    Parameters\n    ----------\n    filename : str\n        File to locate (e.g. ``jw86600006001_02101_00008_guider1_cal.fits``)\n\n    Returns\n    -------\n    full_path : str\n        Full path to the given file, including filename\n    """"""\n\n    filesystem_base = get_config()[""filesystem""]\n\n    # Subdirectory name is based on the proposal ID\n    subdir = \'jw{}\'.format(filename_parser(filename)[\'program_id\'])\n    full_path = os.path.join(filesystem_base, subdir, filename)\n\n    # Check to see if the file exists\n    if os.path.isfile(full_path):\n        return full_path\n    else:\n        raise FileNotFoundError(\n            \'{} is not in the predicted location: {}\'.format(filename, full_path)\n        )\n\n\ndef get_base_url():\n    """"""Return the beginning part of the URL to the ``jwql`` web app\n    based on which user is running the software.\n\n    If the admin account is running the code, the ``base_url`` is\n    assumed to be the production URL.  If not, the ``base_url`` is\n    assumed to be local.\n\n    Returns\n    -------\n    base_url : str\n        The beginning part of the URL to the ``jwql`` web app\n    """"""\n\n    username = getpass.getuser()\n    if username == get_config()[\'admin_account\']:\n        base_url = \'https://dljwql.stsci.edu\'\n    else:\n        base_url = \'http://127.0.0.1:8000\'\n\n    return base_url\n\n\ndef get_config():\n    """"""Return a dictionary that holds the contents of the ``jwql``\n    config file.\n\n    Returns\n    -------\n    settings : dict\n        A dictionary that holds the contents of the config file.\n    """"""\n    config_file_location = os.path.join(__location__, \'config.json\')\n\n    # Make sure the file exists\n    if not os.path.isfile(config_file_location):\n        raise FileNotFoundError(\'The JWQL package requires a configuration file (config.json) \'\n                                \'to be placed within the jwql/utils directory. \'\n                                \'This file is missing. Please read the relevant wiki page \'\n                                \'(https://github.com/spacetelescope/jwql/wiki/\'\n                                \'Config-file) for more information.\')\n\n    with open(config_file_location, \'r\') as config_file_object:\n        try:\n            # Load it with JSON\n            settings = json.load(config_file_object)\n        except json.JSONDecodeError as e:\n            # Raise a more helpful error if there is a formatting problem\n            raise ValueError(\'Incorrectly formatted config.json file. \'\n                             \'Please fix JSON formatting: {}\'.format(e))\n\n    # Ensure the file has all the needed entries with expected data types\n    _validate_config(settings)\n\n    return settings\n\n\ndef check_config_for_key(key):\n    """"""Check that the config.json file contains the specified key\n    and that the entry is not empty\n\n    Parameters\n    ----------\n    key : str\n        The configuration file key to verify\n    """"""\n    try:\n        get_config()[key]\n    except KeyError:\n        raise KeyError(\n            \'The key `{}` is not present in config.json. Please add it.\'.format(key)\n            + \' See the relevant wiki page (https://github.com/spacetelescope/\'\n            \'jwql/wiki/Config-file) for more information.\'\n        )\n\n    if get_config()[key] == """":\n        raise ValueError(\n            \'Please complete the `{}` field in your config.json. \'.format(key)\n            + \' See the relevant wiki page (https://github.com/spacetelescope/\'\n            \'jwql/wiki/Config-file) for more information.\'\n        )\n\n\ndef _validate_config(config_file_dict):\n    """"""Check that the config.json file contains all the needed entries with\n    expected data types\n\n    Parameters\n    ----------\n    config_file_dict : dict\n        The configuration JSON file loaded as a dictionary\n\n    Notes\n    -----\n    See here for more information on JSON schemas:\n        https://json-schema.org/learn/getting-started-step-by-step.html\n    """"""\n    # Define the schema for config.json\n    schema = {\n        ""type"": ""object"",  # Must be a JSON object\n        ""properties"": {  # List all the possible entries and their types\n            ""connection_string"": {""type"": ""string""},\n            ""database"": {\n                ""type"": ""object"",\n                ""properties"": {\n                    ""engine"": {""type"": ""string""},\n                    ""name"": {""type"": ""string""},\n                    ""user"": {""type"": ""string""},\n                    ""password"": {""type"": ""string""},\n                    ""host"": {""type"": ""string""},\n                    ""port"": {""type"": ""string""}\n                    },\n                    ""required"": [\'engine\', \'name\', \'user\', \'password\', \'host\', \'port\']\n                 },\n            ""filesystem"": {""type"": ""string""},\n            ""preview_image_filesystem"": {""type"": ""string""},\n            ""thumbnail_filesystem"": {""type"": ""string""},\n            ""outputs"": {""type"": ""string""},\n            ""jwql_dir"": {""type"": ""string""},\n            ""admin_account"": {""type"": ""string""},\n            ""log_dir"": {""type"": ""string""},\n            ""test_dir"": {""type"": ""string""},\n            ""test_data"": {""type"": ""string""},\n            ""setup_file"": {""type"": ""string""},\n            ""auth_mast"": {""type"": ""string""},\n            ""client_id"": {""type"": ""string""},\n            ""client_secret"": {""type"": ""string""},\n            ""mast_token"": {""type"": ""string""},\n        },\n        # List which entries are needed (all of them)\n        ""required"": [""connection_string"", ""database"", ""filesystem"",\n                     ""preview_image_filesystem"", ""thumbnail_filesystem"",\n                     ""outputs"", ""jwql_dir"", ""admin_account"", ""log_dir"",\n                     ""test_dir"", ""test_data"", ""setup_file"", ""auth_mast"",\n                     ""client_id"", ""client_secret"", ""mast_token""]\n    }\n\n    # Test that the provided config file dict matches the schema\n    try:\n        jsonschema.validate(instance=config_file_dict, schema=schema)\n    except jsonschema.ValidationError as e:\n        raise jsonschema.ValidationError(\n            \'Provided config.json does not match the \' + \\\n            \'required JSON schema: {}\'.format(e.message)\n        )\n\n\ndef initialize_instrument_monitor(module):\n    """"""Configures a log file for the instrument monitor run and\n    captures the start time of the monitor\n\n    Parameters\n    ----------\n    module : str\n        The module name (e.g. ``dark_monitor``)\n\n    Returns\n    -------\n    start_time : datetime object\n        The start time of the monitor\n    log_file : str\n        The path to where the log file is stored\n    """"""\n\n    from jwql.utils.logging_functions import configure_logging\n\n    start_time = datetime.datetime.now()\n    log_file = configure_logging(module)\n\n    return start_time, log_file\n\n\ndef update_monitor_table(module, start_time, log_file):\n    """"""Update the ``monitor`` database table with information about\n    the instrument monitor run\n\n    Parameters\n    ----------\n    module : str\n        The module name (e.g. ``dark_monitor``)\n    start_time : datetime object\n        The start time of the monitor\n    log_file : str\n        The path to where the log file is stored\n    """"""\n\n    from jwql.database.database_interface import Monitor\n\n    new_entry = {}\n    new_entry[\'monitor_name\'] = module\n    new_entry[\'start_time\'] = start_time\n    new_entry[\'end_time\'] = datetime.datetime.now()\n    new_entry[\'log_file\'] = os.path.basename(log_file)\n\n    Monitor.__table__.insert().execute(new_entry)\n'"
jwql/website/__init__.py,0,b''
jwql/website/manage.py,0,"b'#! /usr/bin/env python\n\n""""""Utility module for administrative tasks.\n\nA python script version of Django\'s command-line utility for\nadministrative tasks (``django-admin``). Additionally, puts the project\npackage on ``sys.path`` and defines the ``DJANGO_SETTINGS_MODULE``\nvariable to point to the jwql ``settings.py`` file.\n\nGenerated by ``django-admin startproject`` using Django 2.0.1.\n\nUse\n---\n\n    To run the web app server:\n    ::\n\n        python manage.py runserver\n\n    To start the interactive shellL:\n    ::\n\n        python manage.py shell\n\n    To run tests for all installed apps:\n    ::\n\n        python manage.py test\n\nReferences\n----------\nFor more information please see:\n    ``https://docs.djangoproject.com/en/2.0/ref/django-admin/``\n""""""\n\nimport os\nimport sys\n\nif __name__ == ""__main__"":\n\n    os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""jwql_proj.settings"")\n\n    try:\n        from django.core.management import execute_from_command_line\n    except ImportError as exc:\n        raise ImportError(\n            ""Couldn\'t import Django. Are you sure it\'s installed and ""\n            ""available on your PYTHONPATH environment variable? Did you ""\n            ""forget to activate a virtual environment?""\n        ) from exc\n    execute_from_command_line(sys.argv)\n'"
style_guide/typing_demo/typing_demo_1.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nmypy + typing demo for JWQL dev meeting 2018-1-3\n\nPart 1: Intro\n""""""\n\nimport sys\nfrom typing import (List, Set, Dict, Tuple, Union, Optional, Callable, \n                    Iterable, Any)\n\nassert sys.version_info >= (3, 6) # PEP 526 added variable annotations\n\nan_integer: int = 1\na_float: float = 1.0\na_bool: bool = True\na_string: str = ""jwql""\na_list: List[int] = [1]\na_set: Set[int] = {1, 2, 3}\na_dict: Dict[str, bool] = {\'jwql\':True} # Have to specify both keys and values\n\n#For python versions prior to 3.6, the variable annotation syntax uses comments:\n#    annotated_variable = 1 # type: int\n\n#Tuples are a little different - we can specify a type for each element of a\n#tuple because they\'re immutable\na_heterogeneous_tuple: Tuple[int, bool] = (1, True)\nan_empty_tuple: Tuple[()] = ()\n\n#For heterogeneous non-tuples, use Union.\na_heterogeneous_list: List[Union[int, bool, str]] = [1, True, ""jwql""]\na_heterogeneous_dict: Dict[Union[str, int], Union[bool, int]] = {""jwql"": True, 1:1}\n\n#If a value can be None, use Optional\nmaybe_a_string: Optional[str] = ""jwql"" if not a_bool else None\n\n#For functions, there\'s a similar annotation syntax\ndef a_generic_function(num: int) -> str:\n    return f""You passed {num} to this completely generic function.""\n\ndef two_arg_function(name: str, num: float = 0.0) -> None:\n    print(f""Sorry {name}, this function won\'t return {num}"")\n\n#Function aliases and anonymous functions can also be annotated  with the \n#same variable syntax\n\nfunc_alias: Callable[[str, float], None] = two_arg_function\nanon_func: Callable[[Any], int] = lambda x: 1\n\n#Generators are just functions which return iterables:\ndef a_generator() -> Iterable[int]:\n    i = 0\n    while True:\n        yield i\n        i += 1\n\n#NOT RECOMMENDED\nmy_metavar: ""hey i\'m metadata!"" = ""not metadata""\nprint(__annotations__[""my_metavar""])\n\n#Type annotations are stored in __annotations__, either as a local variable\n#or as an object attribute.\n\ndef print_annotations(arg: Any) -> bool:\n    if not hasattr(arg, ""__annotations__""):\n        print(""Sorry, that argument doesn\'t have its own __annotations__."")\n        return False\n    print(arg.__annotations__)\n    return bool(arg.__annotations__)\n\nfor name in [""an_integer"", ""a_generic_function"", ""two_arg_function"",\n             ""func_alias"", ""anon_func"", ""a_generator""]:\n    var = locals()[name]\n    print(f""Annotations for {name}:"")\n    if not print_annotations(var):\n        print(""Instead, we\'ll check the local instance of __annotations__:"")\n        print(__annotations__[name])\n'"
style_guide/typing_demo/typing_demo_2.py,5,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nmypy + typing demo for JWQL dev meeting 2018-1-3\n\nPart 2: More advanced techniques\n""""""\n\nfrom typing import (Iterable, Sequence, Mapping, MutableMapping, Any, List,\n                    Tuple, IO, ClassVar, NewType, Set, Union)\nfrom astropy.io import fits\nimport numpy as np\n\n#Use the above generic types for standard duck typing of functions, in the\n#same way that you\'d use abstract base classes\n\ndef needs_an_iterable(iterable_arg: Iterable[Any] = []) -> List[str]:\n    return [str(x) for x in iterable_arg]\n\ndef dont_mutate(immut_dict: Mapping[Any, Any]) -> List[Tuple[Any, Any]]:\n    return list(immut_dict.items())\n\ndef do_mutate(mut_dict: MutableMapping[Any, Any]) -> Set[Any]:\n    mut_dict[\'jwql\'] = True\n    return set(mut_dict.keys())\n\n#Variables can be annotated without initializing\nstream: IO[str]\nprint(__annotations__[\'stream\'])\n\n#The IO type doesn\'t distinguish between reading, writing, or appending.\nwith open(\'demo.txt\', \'w\') as stream:\n    for i in range(10):\n        stream.write(f""{i}\\n"")\n\n#Pre-annotation is also useful with conditional branches\nconditional: str\nif ""jwql"":\n    conditional = ""Yay!""\nelse:\n    conditional = ""Boo!""\n    \n#Data types from imported modules can be used just as easily as builtin types\nan_array: np.ndarray = np.arange(10)\na_fits_header: fits.Header = fits.getheader(""nirspec_irs2_nrs1_i_02.01.fits"")\n\n#Class attributes and methods can be annotated as well, and user-defined\n#classes can be used to annotate other variables and functions.\nclass aClass(object):\n    x: int = 0 #this is an instance variable with a default value\n    y: ClassVar[List[int]] #this is a class variable with no default value\n    \n    def __init__(self) -> None: #doesn\'t return anything\n        self.x = 1\n        self.y = [2]\n        #Can also annotate attributes in __init__\n        self.z: np.float64 = np.float64(3.0)\n        print(__annotations__)\n    \n    def result(self) -> np.float64: #self shouldn\'t be annotated\n        return x + np.array(self.y).sum() + self.z\n\nprint(aClass.__annotations__)\nan_instance: aClass = aClass()\nprint(__annotations__[""an_instance""])\nprint(an_instance.__annotations__)\n\n#You can use forward references if you like defining things out of order\ndef preemptive_function(num: ""Numberlike"", user: ""UserID"") -> None: \n    #Note that neither Numberlike or UserID have been defined.\n    print(f""Y\'know, {user}..."")\n    print(f""{num} should probably be some kind of number."")\n    print(""Just saying..."")\n\n#You can also define new types and type aliases\nNumberlike = Union[int, float, np.float64]\nUserID = NewType(\'UserID\', str)\n\n#Note that you can do anything with UserID that you can do with a string,\n#and can pass a UserID to any function that would accept a string. However,\n#operations on UserIDs will always result in strings, not UserIDs.\noutput = UserID(\'Gray\') + UserID(\'Kanarek\')\nprint(output) # is of type string, not UserID.\n'"
style_guide/typing_demo/typing_demo_3.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nmypy + typing demo for JWQL dev meeting 2018-1-3\n\nPart 3: mypy for type checking\n\nMany thanks to Tommy Tutone...\n""""""\n\nfrom typing import NewType\n\n#mypy can check for incorrectly-typed variables\n\nbad_variable: str = 1 #no runtime error\n\n#This can especially be useful when using pre-annotation, since types can be\n#hinted before calculations, I/O, or other complex code determines its value.\njenny: str\n\n#mypy can also check function arguments and return values\ndef ive_got_your_number(num: int) -> bool:\n    if num == 867_5309:\n        return True\n    else:\n        return ""Jenny don\'t change your number""\n\nive_got_your_number(""jenny"") #no runtime error\nive_got_your_number(555_1212) #no runtime error\n\nif ive_got_your_number(8675_309):\n    jenny = 867_5309 #no runtime error\nelse:\n    jenny = ""Don\'t change your number""\n    \n#If for some reason you don\'t want a particular variable\'s type to be checked, \n#then use comment syntax and ""ignore""\ndummy = None # type: ignore # otherwise this will throw a mypy error!\n\n#mypy can handle user-created types\nUserID = NewType(""UserID"", str)\n\ngray: UserID = UserID(""Gray"")\nkanarek: UserID = ""Kanarek"" #no runtime error\n\nuser: UserID = gray + kanarek #no runtime error\n\ndef get_first_char(user: UserID) -> str:\n    return user[0]\n\nget_first_char(gray)\nget_first_char(""Gray"") #no runtime error\n\n    \n#mypy can help you figure out the types of variables, if it\'s complicated to \n#find out beforehand\nreveal_type(1)\n'"
style_guide/typing_demo/typing_demo_4.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nmypy + typing demo for JWQL dev meeting 2018-1-3\n\nPart 4: Subtlety\n""""""\n\n#Why do we care about this? Because errors can be subtle.\n\n#A simple example!\n\ndef get_favorite_number():\n    return input(""What\'s your favorite number? "")\n\nnum = get_favorite_number()\nprint(""Twice your favorite number is"", num*2)\n'"
jwql/bokeh_templating/example/main.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Jul 20 10:03:35 2018\n\n@author: gkanarek\n""""""\nimport os\nimport numpy as np\n\nfrom jwql.bokeh_templating import BokehTemplate\n\nfile_dir = os.path.dirname(os.path.realpath(__file__))\n\n\nclass TestBokehApp(BokehTemplate):\n\n    def pre_init(self):\n        self.a, self.b = 4, 2\n\n        self.format_string = None\n        self.interface_file = os.path.join(file_dir, ""example_interface.yaml"")\n\n    post_init = None\n\n    @property\n    def x(self):\n        return 4. * np.sin(self.a * np.linspace(0, 2 * np.pi, 500))\n\n    @property\n    def y(self):\n        return 3. * np.sin(self.b * np.linspace(0, 2 * np.pi, 500))\n\n    def controller(self, attr, old, new):\n        self.a = self.refs[""a_slider""].value\n        self.b = self.refs[""b_slider""].value\n\n        self.refs[""figure_source""].data = {\'x\': self.x, \'y\': self.y}\n\n\nTestBokehApp()\n'"
jwql/instrument_monitors/common_monitors/__init__.py,0,b''
jwql/instrument_monitors/common_monitors/bias_monitor.py,4,"b'#! /usr/bin/env python\n\n""""""This module contains code for the bias monitor, which monitors\nthe bias levels in dark exposures as well as the performance of\nthe pipeline superbias subtraction over time.\n\nFor each instrument, the 0th group of full-frame dark exposures is\nsaved to a fits file. The median signal levels in these images are\nrecorded in the ``<Instrument>BiasStats`` database table for the\nodd/even columns of each amp.\n\nNext, these images are run through the jwst pipeline up through the\nreference pixel correction step. These calibrated images are saved\nto a fits file as well as a png file for visual inspection of the\nquality of the pipeline calibration. The median-collpsed row and\ncolumn values, as well as the sigma-clipped mean and standard\ndeviation of these images, are recorded in the\n``<Instrument>BiasStats`` database table.\n\nAuthor\n------\n    - Ben Sunnquist\n\nUse\n---\n    This module can be used from the command line as such:\n\n    ::\n\n        python bias_monitor.py\n""""""\n\nimport datetime\nimport logging\nimport os\n\nfrom astropy.io import fits\nfrom astropy.stats import sigma_clipped_stats\nfrom astropy.time import Time\nfrom astropy.visualization import ZScaleInterval\nfrom jwst.dq_init import DQInitStep\nfrom jwst.group_scale import GroupScaleStep\nfrom jwst.refpix import RefPixStep\nfrom jwst.saturation import SaturationStep\nfrom jwst.superbias import SuperBiasStep\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport numpy as np\nfrom pysiaf import Siaf\nfrom sqlalchemy import func\nfrom sqlalchemy.sql.expression import and_\n\nfrom jwql.database.database_interface import session\nfrom jwql.database.database_interface import NIRCamBiasQueryHistory, NIRCamBiasStats\nfrom jwql.instrument_monitors import pipeline_tools\nfrom jwql.instrument_monitors.common_monitors.dark_monitor import mast_query_darks\nfrom jwql.utils import instrument_properties\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES_MIXEDCASE\nfrom jwql.utils.logging_functions import log_info, log_fail\nfrom jwql.utils.permissions import set_permissions\nfrom jwql.utils.utils import ensure_dir_exists, filesystem_path, get_config, initialize_instrument_monitor, update_monitor_table\n\nclass Bias():\n    """"""Class for executing the bias monitor.\n\n    This class will search for new full-frame dark current files in\n    the file system for each instrument and will run the monitor on\n    these files. The monitor will extract the 0th group from the new\n    dark files and output the contents into a new file located in\n    a working directory. It will then perform statistical measurements\n    on these files before and after pipeline calibration in order to\n    monitor the bias levels over time as well as ensure the pipeline\n    superbias is sufficiently calibrating new data. Results are all\n    saved to database tables.\n\n    Attributes\n    ----------\n    output_dir : str\n        Path into which outputs will be placed\n\n    data_dir : str\n        Path into which new dark files will be copied to be worked on\n\n    query_start : float\n        MJD start date to use for querying MAST\n\n    query_end : float\n        MJD end date to use for querying MAST\n\n    instrument : str\n        Name of instrument used to collect the dark current data\n\n    aperture : str\n        Name of the aperture used for the dark current (e.g.\n        ``NRCA1_FULL``)\n    """"""\n\n    def __init__(self):\n        """"""Initialize an instance of the ``Bias`` class.""""""\n\n    def collapse_image(self, image):\n        """"""Median-collapse the rows and columns of an image.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            2D array on which to calculate statistics\n\n        Returns\n        -------\n        collapsed_rows : numpy.ndarray\n            1D array of the collapsed row values\n\n        collapsed_columns : numpy.ndarray\n            1D array of the collapsed column values\n        """"""\n\n        collapsed_rows = np.nanmedian(image, axis=1)\n        collapsed_columns = np.nanmedian(image, axis=0)\n\n        return collapsed_rows, collapsed_columns\n\n    def extract_zeroth_group(self, filename):\n        """"""Extracts the 0th group of a fits image and outputs it into\n        a new fits file.\n\n        Parameters\n        ----------\n        filename : str\n            The fits file from which the 0th group will be extracted.\n\n        Returns\n        -------\n        output_filename : str\n            The full path to the output file\n        """"""\n\n        output_filename = os.path.join(self.data_dir, os.path.basename(filename).replace(\'.fits\', \'_0thgroup.fits\'))\n\n        # Write a new fits file containing the primary and science\n        # headers from the input file, as well as the 0th group\n        # data of the first integration\n        if not os.path.isfile(output_filename):\n            hdu = fits.open(filename)\n            new_hdu = fits.HDUList([hdu[\'PRIMARY\'], hdu[\'SCI\']])\n            new_hdu[\'SCI\'].data = hdu[\'SCI\'].data[0:1, 0:1, :, :]\n            new_hdu.writeto(output_filename)\n            hdu.close()\n            new_hdu.close()\n            set_permissions(output_filename)\n            logging.info(\'\\t{} created\'.format(output_filename))\n        else:\n            logging.info(\'\\t{} already exists\'.format(output_filename))\n\n        return output_filename\n\n    def file_exists_in_database(self, filename):\n        """"""Checks if an entry for filename exists in the bias stats\n        database.\n\n        Parameters\n        ----------\n        filename : str\n            The full path to the uncal filename\n\n        Returns\n        -------\n        file_exists : bool\n            ``True`` if filename exists in the bias stats database\n        """"""\n\n        query = session.query(self.stats_table)\n        results = query.filter(self.stats_table.uncal_filename == filename).all()\n\n        if len(results) != 0:\n            file_exists = True\n        else:\n            file_exists = False\n\n        return file_exists\n\n    def get_amp_medians(self, image, amps):\n        """"""Calculates the median in the input image for each amplifier\n        and for odd and even columns separately.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            2D array on which to calculate statistics\n\n        amps : dict\n            Dictionary containing amp boundary coordinates (output from\n            ``amplifier_info`` function)\n            ``amps[key] = [(xmin, xmax, xstep), (ymin, ymax, ystep)]``\n\n        Returns\n        -------\n        amp_medians : dict\n            Median values for each amp. Keys are ramp numbers as\n            strings with even/odd designation (e.g. ``\'1_even\'``)\n        """"""\n\n        amp_medians = {}\n\n        for key in amps:\n            x_start, x_end, x_step = amps[key][0]\n            y_start, y_end, y_step = amps[key][1]\n\n            # Find median value of both even and odd columns for this amp\n            amp_med_even = np.nanmedian(image[y_start: y_end, x_start: x_end][:, 1::2])\n            amp_medians[\'amp{}_even_med\'.format(key)] = amp_med_even\n            amp_med_odd = np.nanmedian(image[y_start: y_end, x_start: x_end][:, ::2])\n            amp_medians[\'amp{}_odd_med\'.format(key)] = amp_med_odd\n\n        return amp_medians\n\n    def identify_tables(self):\n        """"""Determine which database tables to use for a run of the bias\n        monitor.\n        """"""\n\n        mixed_case_name = JWST_INSTRUMENT_NAMES_MIXEDCASE[self.instrument]\n        self.query_table = eval(\'{}BiasQueryHistory\'.format(mixed_case_name))\n        self.stats_table = eval(\'{}BiasStats\'.format(mixed_case_name))\n\n    def image_to_png(self, image, outname):\n        """"""Ouputs an image array into a png file.\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            2D image array\n\n        outname : str\n            The name given to the output png file\n\n        Returns\n        -------\n        output_filename : str\n            The full path to the output png file\n        """"""\n\n        output_filename = os.path.join(self.data_dir, \'{}.png\'.format(outname))\n\n        if not os.path.isfile(output_filename):\n            # Get image scale limits\n            z = ZScaleInterval()\n            vmin, vmax = z.get_limits(image)\n\n            # Plot the image\n            plt.figure(figsize=(12,12))\n            ax = plt.gca()\n            im = ax.imshow(image, cmap=\'gray\', origin=\'lower\', vmin=vmin, vmax=vmax)\n            ax.set_title(\'{}\'.format(outname))\n\n            # Make the colorbar\n            divider = make_axes_locatable(ax)\n            cax = divider.append_axes(""right"", size=""5%"", pad=0.4)\n            cbar = plt.colorbar(im, cax=cax)\n            cbar.set_label(\'Signal [DN]\')\n\n            plt.savefig(output_filename, bbox_inches=\'tight\', dpi=200)\n            set_permissions(output_filename)\n            logging.info(\'\\t{} created\'.format(output_filename))\n        else:\n            logging.info(\'\\t{} already exists\'.format(output_filename))\n\n        return output_filename\n\n    def most_recent_search(self):\n        """"""Query the query history database and return the information\n        on the most recent query for the given ``aperture_name`` where\n        the bias monitor was executed.\n\n        Returns\n        -------\n        query_result : float\n            Date (in MJD) of the ending range of the previous MAST query\n            where the bias monitor was run.\n        """"""\n\n        sub_query = session.query(\n            self.query_table.aperture,\n            func.max(self.query_table.end_time_mjd).label(\'maxdate\')\n            ).group_by(self.query_table.aperture).subquery(\'t2\')\n\n        # Note that ""self.query_table.run_monitor == True"" below is\n        # intentional. Switching = to ""is"" results in an error in the query.\n        query = session.query(self.query_table).join(\n            sub_query,\n            and_(\n                self.query_table.aperture == self.aperture,\n                self.query_table.end_time_mjd == sub_query.c.maxdate,\n                self.query_table.run_monitor == True\n            )\n        ).all()\n\n        query_count = len(query)\n        if query_count == 0:\n            query_result = 57357.0  # a.k.a. Dec 1, 2015 == CV3\n            logging.info((\'\\tNo query history for {}. Beginning search date will be set to {}.\'.format(self.aperture, query_result)))\n        elif query_count > 1:\n            raise ValueError(\'More than one ""most recent"" query?\')\n        else:\n            query_result = query[0].end_time_mjd\n\n        return query_result\n\n    def process(self, file_list):\n        """"""The main method for processing darks.  See module docstrings\n        for further details.\n\n        Parameters\n        ----------\n        file_list : list\n            List of filenames (including full paths) to the dark current\n            files\n        """"""\n\n        for filename in file_list:\n            logging.info(\'\\tWorking on file: {}\'.format(filename))\n\n            # Skip processing if an entry for this file already exists in\n            # the bias stats database.\n            file_exists = self.file_exists_in_database(filename)\n            if file_exists:\n                logging.info(\'\\t{} already exists in the bias database table.\'.format(filename))\n                continue\n\n            # Get the exposure start time of this file\n            expstart = \'{}T{}\'.format(fits.getheader(filename, 0)[\'DATE-OBS\'], fits.getheader(filename, 0)[\'TIME-OBS\'])\n\n            # Determine if the file needs group_scale in pipeline run\n            read_pattern = fits.getheader(filename, 0)[\'READPATT\']\n            if read_pattern not in pipeline_tools.GROUPSCALE_READOUT_PATTERNS:\n                group_scale = False\n            else:\n                group_scale = True\n\n            # Run the file through the pipeline up through the refpix step\n            logging.info(\'\\tRunning pipeline on {}\'.format(filename))\n            processed_file = self.run_early_pipeline(filename, odd_even_rows=False, odd_even_columns=True, use_side_ref_pixels=True, group_scale=group_scale)\n            logging.info(\'\\tPipeline complete. Output: {}\'.format(processed_file))\n\n            # Find amplifier boundaries so per-amp statistics can be calculated\n            _, amp_bounds = instrument_properties.amplifier_info(processed_file, omit_reference_pixels=True)\n            logging.info(\'\\tAmplifier boundaries: {}\'.format(amp_bounds))\n\n            # Get the uncalibrated 0th group data for this file\n            uncal_data = fits.getdata(filename, \'SCI\')[0, 0, :, :].astype(float)\n\n            # Calculate the uncal median values of each amplifier for odd/even columns\n            amp_medians = self.get_amp_medians(uncal_data, amp_bounds)\n            logging.info(\'\\tCalculated uncalibrated image stats: {}\'.format(amp_medians))\n\n            # Calculate image statistics and the collapsed row/column values\n            # in the calibrated image\n            cal_data = fits.getdata(processed_file, \'SCI\')[0, 0, :, :]\n            dq = fits.getdata(processed_file, \'PIXELDQ\')\n            mean, median, stddev = sigma_clipped_stats(cal_data[dq==0], sigma=3.0, maxiters=5)\n            logging.info(\'\\tCalculated calibrated image stats: {:.3f} +/- {:.3f}\'.format(mean, stddev))\n            collapsed_rows, collapsed_columns = self.collapse_image(cal_data)\n            logging.info(\'\\tCalculated collapsed row/column values of calibrated image.\')\n\n            # Save a png of the calibrated image for visual inspection\n            logging.info(\'\\tCreating png of calibrated image\')\n            output_png = self.image_to_png(cal_data, outname=os.path.basename(processed_file).replace(\'.fits\',\'\'))\n\n            # Construct new entry for this file for the bias database table.\n            # Can\'t insert values with numpy.float32 datatypes into database\n            # so need to change the datatypes of these values.\n            bias_db_entry = {\'aperture\': self.aperture,\n                             \'uncal_filename\': filename,\n                             \'cal_filename\': processed_file,\n                             \'cal_image\': output_png,\n                             \'expstart\': expstart,\n                             \'mean\': float(mean),\n                             \'median\': float(median),\n                             \'stddev\': float(stddev),\n                             \'collapsed_rows\': collapsed_rows.astype(float),\n                             \'collapsed_columns\': collapsed_columns.astype(float),\n                             \'entry_date\': datetime.datetime.now()\n                            }\n            for key in amp_medians.keys():\n                bias_db_entry[key] = float(amp_medians[key])\n\n            # Add this new entry to the bias database table\n            self.stats_table.__table__.insert().execute(bias_db_entry)\n            logging.info(\'\\tNew entry added to bias database table: {}\'.format(bias_db_entry))\n\n    @log_fail\n    @log_info\n    def run(self):\n        """"""The main method.  See module docstrings for further details.""""""\n\n        logging.info(\'Begin logging for bias_monitor\')\n\n        # Get the output directory and setup a directory to store the data\n        self.output_dir = os.path.join(get_config()[\'outputs\'], \'bias_monitor\')\n        ensure_dir_exists(os.path.join(self.output_dir, \'data\'))\n\n        # Use the current time as the end time for MAST query\n        self.query_end = Time.now().mjd\n\n        # Loop over all instruments\n        for instrument in [\'nircam\']:\n            self.instrument = instrument\n\n            # Identify which database tables to use\n            self.identify_tables()\n\n            # Get a list of all possible full-frame apertures for this instrument\n            siaf = Siaf(self.instrument)\n            possible_apertures = [aperture for aperture in siaf.apertures if siaf[aperture].AperType==\'FULLSCA\']\n\n            for aperture in possible_apertures:\n\n                logging.info(\'Working on aperture {} in {}\'.format(aperture, instrument))\n                self.aperture = aperture\n\n                # Locate the record of the most recent MAST search; use this time\n                # (plus a 30 day buffer to catch any missing files from the previous\n                # run) as the start time in the new MAST search.\n                most_recent_search = self.most_recent_search()\n                self.query_start = most_recent_search - 30\n\n                # Query MAST for new dark files for this instrument/aperture\n                logging.info(\'\\tQuery times: {} {}\'.format(self.query_start, self.query_end))\n                new_entries = mast_query_darks(instrument, aperture, self.query_start, self.query_end)\n                logging.info(\'\\tAperture: {}, new entries: {}\'.format(self.aperture, len(new_entries)))\n\n                # Set up a directory to store the data for this aperture\n                self.data_dir = os.path.join(self.output_dir, \'data/{}_{}\'.format(self.instrument.lower(), self.aperture.lower()))\n                if len(new_entries) > 0:\n                    ensure_dir_exists(self.data_dir)\n\n                # Save the 0th group image from each new file in the output directory;\n                # some dont exist in JWQL filesystem.\n                new_files = []\n                for file_entry in new_entries:\n                    try:\n                        filename = filesystem_path(file_entry[\'filename\'])\n                        uncal_filename = filename.replace(\'_dark\', \'_uncal\')\n                        if not os.path.isfile(uncal_filename):\n                            logging.info(\'\\t{} does not exist in JWQL filesystem, even though {} does\'.format(uncal_filename, filename))\n                        else:\n                            new_file = self.extract_zeroth_group(uncal_filename)\n                            new_files.append(new_file)\n                    except FileNotFoundError:\n                        logging.info(\'\\t{} does not exist in JWQL filesystem\'.format(file_entry[\'filename\']))\n\n                # Run the bias monitor on any new files\n                if len(new_files) > 0:\n                    self.process(new_files)\n                    monitor_run = True\n                else:\n                    logging.info(\'\\tBias monitor skipped. {} new dark files for {}, {}.\'.format(len(new_files), instrument, aperture))\n                    monitor_run = False\n\n                # Update the query history\n                new_entry = {\'instrument\': instrument,\n                             \'aperture\': aperture,\n                             \'start_time_mjd\': self.query_start,\n                             \'end_time_mjd\': self.query_end,\n                             \'entries_found\': len(new_entries),\n                             \'files_found\': len(new_files),\n                             \'run_monitor\': monitor_run,\n                             \'entry_date\': datetime.datetime.now()}\n                self.query_table.__table__.insert().execute(new_entry)\n                logging.info(\'\\tUpdated the query history table\')\n\n        logging.info(\'Bias Monitor completed successfully.\')\n\n    def run_early_pipeline(self, filename, odd_even_rows=False, odd_even_columns=True,\n                           use_side_ref_pixels=True, group_scale=False):\n        """"""Runs the early steps of the jwst pipeline (dq_init, saturation,\n        superbias, refpix) on uncalibrated files and outputs the result.\n\n        Parameters\n        ----------\n        filename : str\n            File on which to run the pipeline steps\n\n        odd_even_rows : bool\n            Option to treat odd and even rows separately during refpix step\n\n        odd_even_columns : bools\n            Option to treat odd and even columns separately during refpix step\n\n        use_side_ref_pixels : bool\n            Option to perform the side refpix correction during refpix step\n\n        group_scale : bool\n            Option to rescale pixel values to correct for instances where\n            on-board frame averaging did not result in the proper values\n\n        Returns\n        -------\n        output_filename : str\n            The full path to the calibrated file\n        """"""\n\n        output_filename = filename.replace(\'_uncal\', \'\').replace(\'.fits\', \'_superbias_refpix.fits\')\n\n        if not os.path.isfile(output_filename):\n            # Run the group_scale and dq_init steps on the input file\n            if group_scale:\n                model = GroupScaleStep.call(filename)\n                model = DQInitStep.call(model)\n            else:\n                model = DQInitStep.call(filename)\n\n            # Run the saturation and superbias steps\n            model = SaturationStep.call(model)\n            model = SuperBiasStep.call(model)\n\n            # Run the refpix step and save the output\n            model = RefPixStep.call(model, odd_even_rows=odd_even_rows, odd_even_columns=odd_even_columns, use_side_ref_pixels=use_side_ref_pixels)\n            model.save(output_filename)\n            set_permissions(output_filename)\n        else:\n            logging.info(\'\\t{} already exists\'.format(output_filename))\n\n        return output_filename\n\n\nif __name__ == \'__main__\':\n\n    module = os.path.basename(__file__).strip(\'.py\')\n    start_time, log_file = initialize_instrument_monitor(module)\n\n    monitor = Bias()\n    monitor.run()\n\n    update_monitor_table(module, start_time, log_file)\n'"
jwql/instrument_monitors/common_monitors/dark_monitor.py,15,"b'#! /usr/bin/env python\n\n""""""This module contains code for the dark current monitor, which\nperforms some basic analysis to check whether the dark current behavior\nfor the most recent set of input files is consistent with that from\npast files.\n\nIf enough new files for a given instrument/aperture combination\n(currently the files must be identified as dark current files in the\n``exp_type`` header keyword) are present in the filesystem at the time\nthe ``dark_monitor`` is called, the files are first run through the the\nappropriate pipeline steps to produce slope images.\n\nA mean slope image as well as a standard deviation slope image is\ncreated by sigma-clipping on a pixel by pixel basis. The mean and\nstandard deviation images are saved to a fits file, the name of which\nis entered into the ``<Instrument>DarkCurrent`` database table.\n\nThe mean slope image is then normalized by an existing baseline slope\nimage. New hot pixels are identified as those with normalized signal\nrates above a ``hot_threshold`` value. Similarly, pixels with\nnormalized signal rates below a ``dead_threshold`` are flagged as new\ndead pixels.\n\nThe standard deviation slope image is normalized by a baseline\n(historical) standard deviation image. Pixels with normalized values\nabove a noise threshold are flagged as newly noisy pixels.\n\nNew hot, dead, and noisy pixels are saved to the ``DarkPixelStats``\ndatabase table.\n\nNext, the dark current in the mean slope image is examined. A histogram\nof the slope values is created for the pixels in each amplifier, as\nwell as for all pixels on the detector. In all cases, a Gaussian is fit\nto the histogram. Currently for NIRCam and NIRISS, a double Gaussian is\nalso fit to the histogram from the entire detector.\n\nThe histogram itself as well as the best-fit Gaussian and double\nGaussian parameters are saved to the DarkDarkCurrent database table.\n\n\nAuthor\n------\n\n    - Bryan Hilbert\n\nUse\n---\n\n    This module can be used from the command line as such:\n\n    ::\n\n        python dark_monitor.py\n""""""\n\nfrom copy import copy, deepcopy\nimport datetime\nimport logging\nimport os\n\nfrom astropy.io import ascii, fits\nfrom astropy.modeling import models\nfrom astropy.time import Time\nimport numpy as np\nfrom pysiaf import Siaf\nfrom sqlalchemy import func\nfrom sqlalchemy.sql.expression import and_\n\nfrom jwql.database.database_interface import session\nfrom jwql.database.database_interface import NIRCamDarkQueryHistory, NIRCamDarkPixelStats, NIRCamDarkDarkCurrent\nfrom jwql.database.database_interface import NIRISSDarkQueryHistory, NIRISSDarkPixelStats, NIRISSDarkDarkCurrent\nfrom jwql.database.database_interface import MIRIDarkQueryHistory, MIRIDarkPixelStats, MIRIDarkDarkCurrent\nfrom jwql.database.database_interface import NIRSpecDarkQueryHistory, NIRSpecDarkPixelStats, NIRSpecDarkDarkCurrent\nfrom jwql.database.database_interface import FGSDarkQueryHistory, FGSDarkPixelStats, FGSDarkDarkCurrent\nfrom jwql.instrument_monitors import pipeline_tools\nfrom jwql.jwql_monitors import monitor_mast\nfrom jwql.utils import calculations, instrument_properties\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES, JWST_INSTRUMENT_NAMES_MIXEDCASE, JWST_DATAPRODUCTS\nfrom jwql.utils.logging_functions import log_info, log_fail\nfrom jwql.utils.monitor_utils import initialize_instrument_monitor, update_monitor_table\nfrom jwql.utils.permissions import set_permissions\nfrom jwql.utils.utils import copy_files, ensure_dir_exists, get_config, filesystem_path\n\nTHRESHOLDS_FILE = os.path.join(os.path.split(__file__)[0], \'dark_monitor_file_thresholds.txt\')\n\n\ndef mast_query_darks(instrument, aperture, start_date, end_date):\n    """"""Use ``astroquery`` to search MAST for dark current data\n\n    Parameters\n    ----------\n    instrument : str\n        Instrument name (e.g. ``nircam``)\n\n    aperture : str\n        Detector aperture to search for (e.g. ``NRCA1_FULL``)\n\n    start_date : float\n        Starting date for the search in MJD\n\n    end_date : float\n        Ending date for the search in MJD\n\n    Returns\n    -------\n    query_results : list\n        List of dictionaries containing the query results\n    """"""\n\n    # Make sure instrument is correct case\n    if instrument.lower() == \'nircam\':\n        instrument = \'NIRCam\'\n        dark_template = [\'NRC_DARK\']\n    elif instrument.lower() == \'niriss\':\n        instrument = \'NIRISS\'\n        dark_template = [\'NIS_DARK\']\n    elif instrument.lower() == \'nirspec\':\n        instrument = \'NIRSpec\'\n        dark_template = [\'NRS_DARK\']\n    elif instrument.lower() == \'fgs\':\n        instrument = \'FGS\'\n        dark_template = [\'FGS_DARK\']\n    elif instrument.lower() == \'miri\':\n        instrument = \'MIRI\'\n        dark_template = [\'MIR_DARKALL\', \'MIR_DARKIMG\', \'MIR_DARKMRS\']\n\n    # monitor_mast.instrument_inventory does not allow list inputs to\n    # the added_filters input (or at least if you do provide a list, then\n    # it becomes a nested list when it sends the query to MAST. The\n    # nested list is subsequently ignored by MAST.)\n    # So query once for each dark template, and combine outputs into a\n    # single list.\n    query_results = []\n    for template_name in dark_template:\n\n        # Create dictionary of parameters to add\n        parameters = {""date_obs_mjd"": {""min"": start_date, ""max"": end_date},\n                      ""apername"": aperture, ""exp_type"": template_name}\n\n        query = monitor_mast.instrument_inventory(instrument, dataproduct=JWST_DATAPRODUCTS,\n                                                  add_filters=parameters, return_data=True, caom=False)\n        if len(query[\'data\']) > 0:\n            query_results.extend(query[\'data\'])\n\n    return query_results\n\n\nclass Dark():\n    """"""Class for executing the dark current monitor.\n\n    This class will search for new (since the previous instance of the\n    class) dark current files in the file system. It will loop over\n    instrument/aperture combinations and find the number of new dark\n    current files available. If there are enough, it will copy the files\n    over to a working directory and run the monitor. This will create a\n    mean dark current rate image, create a histogram of the dark current\n    values, and fit several functions to the histogram. It will also\n    compare the dark current image to a historical image in order to\n    search for new hot or dead pixels. Results are all saved to\n    database tables.\n\n    Parameters\n    ----------\n    testing : bool\n        For pytest. If ``True``, an instance of ``Dark`` is created, but\n        no other code is executed.\n\n    Attributes\n    ----------\n    output_dir : str\n        Path into which outputs will be placed\n\n    data_dir : str\n        Path into which new dark files will be copied to be worked on\n\n    query_start : float\n        MJD start date to use for querying MAST\n\n    query_end : float\n        MJD end date to use for querying MAST\n\n    instrument : str\n        Name of instrument used to collect the dark current data\n\n    aperture : str\n        Name of the aperture used for the dark current (e.g.\n        ``NRCA1_FULL``)\n\n    query_table : sqlalchemy table\n        Table containing the history of dark current queries to MAST\n        for each instrument/aperture combination\n\n    pixel_table : sqlalchemy table\n        Table containing lists of hot/dead/noisy pixels found for each\n        instrument/detector\n\n    stats_table : sqlalchemy table\n        Table containing dark current analysis results. Mean/stdev\n        values, histogram information, Gaussian fitting results, etc.\n\n    Raises\n    ------\n    ValueError\n        If encountering an unrecognized bad pixel type\n\n    ValueError\n        If the most recent query search returns more than one entry\n    """"""\n\n    def __init__(self):\n        """"""Initialize an instance of the ``Dark`` class.""""""\n\n    def add_bad_pix(self, coordinates, pixel_type, files, mean_filename, baseline_filename,\n                    observation_start_time, observation_mid_time, observation_end_time):\n        """"""Add a set of bad pixels to the bad pixel database table\n\n        Parameters\n        ----------\n        coordinates : tuple\n            Tuple of two lists, containing x,y coordinates of bad\n            pixels (Output of ``np.where`` call)\n\n        pixel_type : str\n            Type of bad pixel. Options are ``dead``, ``hot``, and\n            ``noisy``\n\n        files : list\n            List of fits files which were used to identify the bad\n            pixels\n\n        mean_filename : str\n            Name of fits file containing the mean dark rate image used\n            to find these bad pixels\n\n        baseline_filename : str\n            Name of fits file containing the baseline dark rate image\n            used to find these bad pixels\n\n        observation_start_time : datetime.datetime\n            Observation time of the earliest file in ``files``\n\n        observation_mid_time : datetime.datetime\n            Average of the observation times in ``files``\n\n        observation_end_time : datetime.datetime\n            Observation time of the latest file in ``files``\n        """"""\n\n        logging.info(\'Adding {} {} pixels to database.\'.format(len(coordinates[0]), pixel_type))\n\n        source_files = [os.path.basename(item) for item in files]\n        entry = {\'detector\': self.detector,\n                 \'x_coord\': coordinates[0],\n                 \'y_coord\': coordinates[1],\n                 \'type\': pixel_type,\n                 \'source_files\': source_files,\n                 \'obs_start_time\': observation_start_time,\n                 \'obs_mid_time\': observation_mid_time,\n                 \'obs_end_time\': observation_end_time,\n                 \'mean_dark_image_file\': os.path.basename(mean_filename),\n                 \'baseline_file\': os.path.basename(baseline_filename),\n                 \'entry_date\': datetime.datetime.now()}\n        self.pixel_table.__table__.insert().execute(entry)\n\n    def get_metadata(self, filename):\n        """"""Collect basic metadata from a fits file\n\n        Parameters\n        ----------\n        filename : str\n            Name of fits file to examine\n        """"""\n\n        header = fits.getheader(filename)\n\n        try:\n            self.detector = header[\'DETECTOR\']\n            self.x0 = header[\'SUBSTRT1\']\n            self.y0 = header[\'SUBSTRT2\']\n            self.xsize = header[\'SUBSIZE1\']\n            self.ysize = header[\'SUBSIZE2\']\n            self.sample_time = header[\'TSAMPLE\']\n            self.frame_time = header[\'TFRAME\']\n            self.read_pattern = header[\'READPATT\']\n\n        except KeyError as e:\n            logging.error(e)\n\n    def exclude_existing_badpix(self, badpix, pixel_type):\n        """"""Given a set of coordinates of bad pixels, determine which of\n        these pixels have been previously identified and remove them\n        from the list\n\n        Parameters\n        ----------\n        badpix : tuple\n            Tuple of lists containing x and y pixel coordinates. (Output\n            of ``numpy.where`` call)\n\n        pixel_type : str\n            Type of bad pixel being examined. Options are ``hot``,\n            ``dead``, and ``noisy``\n\n        Returns\n        -------\n        new_pixels_x : list\n            List of x coordinates of new bad pixels\n\n        new_pixels_y : list\n            List of y coordinates of new bad pixels\n        """"""\n\n        if pixel_type not in [\'hot\', \'dead\', \'noisy\']:\n            raise ValueError(\'Unrecognized bad pixel type: {}\'.format(pixel_type))\n\n        db_entries = session.query(self.pixel_table) \\\n            .filter(self.pixel_table.type == pixel_type) \\\n            .filter(self.pixel_table.detector == self.detector) \\\n            .all()\n\n        already_found = []\n        if len(db_entries) != 0:\n            for _row in db_entries:\n                x_coords = _row.x_coord\n                y_coords = _row.y_coord\n                for x, y in zip(x_coords, y_coords):\n                    already_found.append((x, y))\n\n        # Check to see if each pixel already appears in the database for\n        # the given bad pixel type\n        new_pixels_x = []\n        new_pixels_y = []\n        for x, y in zip(badpix[0], badpix[1]):\n            pixel = (x, y)\n            if pixel not in already_found:\n                new_pixels_x.append(x)\n                new_pixels_y.append(y)\n\n        return (new_pixels_x, new_pixels_y)\n\n    def find_hot_dead_pixels(self, mean_image, comparison_image, hot_threshold=2., dead_threshold=0.1):\n        """"""Create the ratio of the slope image to a baseline slope\n        image. Pixels in the ratio image with values above\n        ``hot_threshold`` will be marked as hot, and those with ratio\n        values less than ``dead_threshold`` will be marked as dead.\n\n        Parameters\n        ----------\n        mean_image : numpy.ndarray\n            2D array containing the slope image from the new data\n\n        comparison_image : numpy.ndarray\n            2D array containing the baseline slope image to compare\n            against the new slope image.\n\n        hot_threshold : float\n            ``(mean_image / comparison_image)`` ratio value above which\n            a pixel is considered hot.\n\n        dead_threshold : float\n            ``(mean_image / comparison_image)`` ratio value below which\n            a pixel is considered dead.\n\n        Returns\n        -------\n        hotpix : tuple\n            Tuple (of lists) containing x,y coordinates of newly hot\n            pixels\n\n        deadpix : tuple\n            Tuple (of lists) containing x,y coordinates of newly dead\n            pixels\n        """"""\n\n        # Avoid divide by zeros\n        zeros = comparison_image == 0.\n        comparison_image[zeros] = 1.\n        mean_image[zeros] += 1.\n\n        ratio = mean_image / comparison_image\n        hotpix = np.where(ratio > hot_threshold)\n        deadpix = np.where(ratio < dead_threshold)\n\n        return hotpix, deadpix\n\n    def get_baseline_filename(self):\n        """"""Query the database and return the filename of the baseline\n        (comparison) mean dark slope image to use when searching for\n        new hot/dead/noisy pixels. For this we assume that the most\n        recent baseline file for the given detector is the one to use.\n\n        Returns\n        -------\n        filename : str\n            Name of fits file containing the baseline image\n        """"""\n\n        subq = session.query(self.pixel_table.detector,\n                             func.max(self.pixel_table.entry_date).label(\'maxdate\')\n                             ).group_by(self.pixel_table.detector).subquery(\'t2\')\n\n        query = session.query(self.pixel_table).join(\n            subq,\n            and_(\n                self.pixel_table.detector == self.detector,\n                self.pixel_table.entry_date == subq.c.maxdate\n            )\n        )\n\n        count = query.count()\n        if not count:\n            filename = None\n        else:\n            filename = query.all()[0].baseline_file\n            # Specify the full path\n            filename = os.path.join(self.output_dir, \'mean_slope_images\', filename)\n            logging.info(\'Baseline filename: {}\'.format(filename))\n\n        return filename\n\n    def identify_tables(self):\n        """"""Determine which database tables to use for a run of the dark\n        monitor\n        """"""\n\n        mixed_case_name = JWST_INSTRUMENT_NAMES_MIXEDCASE[self.instrument]\n        self.query_table = eval(\'{}DarkQueryHistory\'.format(mixed_case_name))\n        self.pixel_table = eval(\'{}DarkPixelStats\'.format(mixed_case_name))\n        self.stats_table = eval(\'{}DarkDarkCurrent\'.format(mixed_case_name))\n\n    def most_recent_search(self):\n        """"""Query the query history database and return the information\n        on the most recent query for the given ``aperture_name`` where\n        the dark monitor was executed.\n\n        Returns\n        -------\n        query_result : float\n            Date (in MJD) of the ending range of the previous MAST query\n            where the dark monitor was run.\n        """"""\n\n        sub_query = session.query(self.query_table.aperture,\n                                  func.max(self.query_table.end_time_mjd).label(\'maxdate\')\n                                  ).group_by(self.query_table.aperture).subquery(\'t2\')\n\n        # Note that ""self.query_table.run_monitor == True"" below is\n        # intentional. Switching = to ""is"" results in an error in the query.\n        query = session.query(self.query_table).join(\n            sub_query,\n            and_(\n                self.query_table.aperture == self.aperture,\n                self.query_table.end_time_mjd == sub_query.c.maxdate,\n                self.query_table.run_monitor == True\n            )\n        ).all()\n\n        query_count = len(query)\n        if query_count == 0:\n            query_result = 57357.0  # a.k.a. Dec 1, 2015 == CV3\n            logging.info((\'\\tNo query history for {}. Beginning search date will be set to {}.\'\n                         .format(self.aperture, query_result)))\n        elif query_count > 1:\n            raise ValueError(\'More than one ""most recent"" query?\')\n        else:\n            query_result = query[0].end_time_mjd\n\n        return query_result\n\n    def noise_check(self, new_noise_image, baseline_noise_image, threshold=1.5):\n        """"""Create the ratio of the stdev (noise) image to a baseline\n        noise image. Pixels in the ratio image with values above\n        ``threshold`` will be marked as newly noisy.\n\n        Parameters\n        ----------\n        new_noise_image : numpy.ndarray\n            2D array containing the noise image from the new data\n\n        baseline_noise_image : numpy.ndarray\n            2D array containing the baseline noise image to compare\n            against the new noise image.\n\n        threshold : float\n            ``(new_noise_image / baseline_noise_image)`` ratio value\n            above which a pixel is considered newly noisey.\n\n        Returns\n        -------\n        noisy : tuple\n            Tuple (of lists) of x,y coordinates of newly noisy pixels\n        """"""\n\n        # Avoid divide by zeros\n        zeros = baseline_noise_image == 0.\n        baseline_noise_image[zeros] = 1.\n        new_noise_image[zeros] += 1.\n\n        ratio = new_noise_image / baseline_noise_image\n        noisy = np.where(ratio > threshold)\n\n        return noisy\n\n    def process(self, file_list):\n        """"""The main method for processing darks.  See module docstrings\n        for further details.\n\n        Parameters\n        ----------\n        file_list : list\n            List of filenames (including full paths) to the dark current\n            files\n        """"""\n\n        # Basic metadata that will be needed later\n        self.get_metadata(file_list[0])\n\n        # Determine which pipeline steps need to be executed\n        required_steps = pipeline_tools.get_pipeline_steps(self.instrument)\n        logging.info(\'\\tRequired calwebb1_detector pipeline steps to have the data in the \'\n                     \'correct format:\')\n        for item in required_steps:\n            logging.info(\'\\t\\t{}: {}\'.format(item, required_steps[item]))\n\n        # Modify the list of pipeline steps to skip those not needed for the\n        # preparation of dark current data\n        required_steps[\'dark_current\'] = False\n        required_steps[\'persistence\'] = False\n\n        # NIRSpec IR^2 readout pattern NRSIRS2 is the only one with\n        # nframes not a power of 2\n        if self.read_pattern not in pipeline_tools.GROUPSCALE_READOUT_PATTERNS:\n            required_steps[\'group_scale\'] = False\n\n        # Run pipeline steps on files, generating slope files\n        slope_files = []\n        for filename in file_list:\n\n            completed_steps = pipeline_tools.completed_pipeline_steps(filename)\n            steps_to_run = pipeline_tools.steps_to_run(required_steps, completed_steps)\n\n            logging.info(\'\\tWorking on file: {}\'.format(filename))\n            logging.info(\'\\tPipeline steps that remain to be run:\')\n            for item in steps_to_run:\n                logging.info(\'\\t\\t{}: {}\'.format(item, steps_to_run[item]))\n\n            # Run any remaining required pipeline steps\n            if any(steps_to_run.values()) is False:\n                slope_files.append(filename)\n            else:\n                processed_file = filename.replace(\'.fits\', \'_{}.fits\'.format(\'rate\'))\n\n                # If the slope file already exists, skip the pipeline call\n                if not os.path.isfile(processed_file):\n                    logging.info(\'\\tRunning pipeline on {}\'.format(filename))\n                    processed_file = pipeline_tools.run_calwebb_detector1_steps(os.path.abspath(filename), steps_to_run)\n                    logging.info(\'\\tPipeline complete. Output: {}\'.format(processed_file))\n\n                else:\n                    logging.info(\'\\tSlope file {} already exists. Skipping call to pipeline.\'\n                                 .format(processed_file))\n                    pass\n\n                slope_files.append(processed_file)\n\n                # Delete the original dark ramp file to save disk space\n                os.remove(filename)\n\n        obs_times = []\n        logging.info(\'\\tSlope images to use in the dark monitor for {}, {}:\'.format(self.instrument, self.aperture))\n        for item in slope_files:\n            logging.info(\'\\t\\t{}\'.format(item))\n            # Get the observation time for each file\n            obstime = instrument_properties.get_obstime(item)\n            obs_times.append(obstime)\n\n        # Find the earliest and latest observation time, and calculate\n        # the mid-time.\n        min_time = np.min(obs_times)\n        max_time = np.max(obs_times)\n        mid_time = instrument_properties.mean_time(obs_times)\n\n        # Read in all slope images and place into a list\n        slope_image_stack, slope_exptimes = pipeline_tools.image_stack(slope_files)\n\n        # Calculate a mean slope image from the inputs\n        slope_image, stdev_image = calculations.mean_image(slope_image_stack, sigma_threshold=3)\n        mean_slope_file = self.save_mean_slope_image(slope_image, stdev_image, slope_files)\n        logging.info(\'\\tSigma-clipped mean of the slope images saved to: {}\'.format(mean_slope_file))\n\n        # ----- Search for new hot/dead/noisy pixels -----\n        # Read in baseline mean slope image and stdev image\n        # The baseline image is used to look for hot/dead/noisy pixels,\n        # but not for comparing mean dark rates. Therefore, updates to\n        # the baseline can be minimal.\n\n        # Limit checks for hot/dead/noisy pixels to full frame data since\n        # subarray data have much shorter exposure times and therefore lower\n        # signal-to-noise\n        aperture_type = Siaf(self.instrument)[self.aperture].AperType\n        if aperture_type == \'FULLSCA\':\n            baseline_file = self.get_baseline_filename()\n            if baseline_file is None:\n                logging.warning((\'\\tNo baseline dark current countrate image for {} {}. Setting the \'\n                                 \'current mean slope image to be the new baseline.\'.format(self.instrument, self.aperture)))\n                baseline_file = mean_slope_file\n                baseline_mean = deepcopy(slope_image)\n                baseline_stdev = deepcopy(stdev_image)\n            else:\n                logging.info(\'\\tBaseline file is {}\'.format(baseline_file))\n                baseline_mean, baseline_stdev = self.read_baseline_slope_image(baseline_file)\n\n            # Check the hot/dead pixel population for changes\n            new_hot_pix, new_dead_pix = self.find_hot_dead_pixels(slope_image, baseline_mean)\n\n            # Shift the coordinates to be in full frame coordinate system\n            new_hot_pix = self.shift_to_full_frame(new_hot_pix)\n            new_dead_pix = self.shift_to_full_frame(new_dead_pix)\n\n            # Exclude hot and dead pixels found previously\n            new_hot_pix = self.exclude_existing_badpix(new_hot_pix, \'hot\')\n            new_dead_pix = self.exclude_existing_badpix(new_dead_pix, \'dead\')\n\n            # Add new hot and dead pixels to the database\n            logging.info(\'\\tFound {} new hot pixels\'.format(len(new_hot_pix[0])))\n            logging.info(\'\\tFound {} new dead pixels\'.format(len(new_dead_pix[0])))\n            self.add_bad_pix(new_hot_pix, \'hot\', file_list, mean_slope_file, baseline_file, min_time, mid_time, max_time)\n            self.add_bad_pix(new_dead_pix, \'dead\', file_list, mean_slope_file, baseline_file, min_time, mid_time, max_time)\n\n            # Check for any pixels that are significantly more noisy than\n            # in the baseline stdev image\n            new_noisy_pixels = self.noise_check(stdev_image, baseline_stdev)\n\n            # Shift coordinates to be in full_frame coordinate system\n            new_noisy_pixels = self.shift_to_full_frame(new_noisy_pixels)\n\n            # Exclude previously found noisy pixels\n            new_noisy_pixels = self.exclude_existing_badpix(new_noisy_pixels, \'noisy\')\n\n            # Add new noisy pixels to the database\n            logging.info(\'\\tFound {} new noisy pixels\'.format(len(new_noisy_pixels[0])))\n            self.add_bad_pix(new_noisy_pixels, \'noisy\', file_list, mean_slope_file, baseline_file, min_time, mid_time, max_time)\n\n        # ----- Calculate image statistics -----\n\n        # Find amplifier boundaries so per-amp statistics can be calculated\n        number_of_amps, amp_bounds = instrument_properties.amplifier_info(slope_files[0])\n        logging.info(\'\\tAmplifier boundaries: {}\'.format(amp_bounds))\n\n        # Calculate mean and stdev values, and fit a Gaussian to the\n        # histogram of the pixels in each amp\n        (amp_mean, amp_stdev, gauss_param, gauss_chisquared, double_gauss_params, double_gauss_chisquared,\n            histogram, bins) = self.stats_by_amp(slope_image, amp_bounds)\n\n        # Construct new entry for dark database table\n        source_files = [os.path.basename(item) for item in file_list]\n        for key in amp_mean.keys():\n            dark_db_entry = {\'aperture\': self.aperture, \'amplifier\': key, \'mean\': amp_mean[key],\n                             \'stdev\': amp_stdev[key],\n                             \'source_files\': source_files,\n                             \'obs_start_time\': min_time,\n                             \'obs_mid_time\': mid_time,\n                             \'obs_end_time\': max_time,\n                             \'gauss_amplitude\': list(gauss_param[key][0]),\n                             \'gauss_peak\': list(gauss_param[key][1]),\n                             \'gauss_width\': list(gauss_param[key][2]),\n                             \'gauss_chisq\': gauss_chisquared[key],\n                             \'double_gauss_amplitude1\': double_gauss_params[key][0],\n                             \'double_gauss_peak1\': double_gauss_params[key][1],\n                             \'double_gauss_width1\': double_gauss_params[key][2],\n                             \'double_gauss_amplitude2\': double_gauss_params[key][3],\n                             \'double_gauss_peak2\': double_gauss_params[key][4],\n                             \'double_gauss_width2\': double_gauss_params[key][5],\n                             \'double_gauss_chisq\': double_gauss_chisquared[key],\n                             \'mean_dark_image_file\': os.path.basename(mean_slope_file),\n                             \'hist_dark_values\': bins,\n                             \'hist_amplitudes\': histogram,\n                             \'entry_date\': datetime.datetime.now()\n                             }\n            self.stats_table.__table__.insert().execute(dark_db_entry)\n\n    def read_baseline_slope_image(self, filename):\n        """"""Read in a baseline mean slope image and associated standard\n        deviation image from the given fits file\n\n        Parameters\n        ----------\n        filename : str\n            Name of fits file to be read in\n\n        Returns\n        -------\n        mean_image : numpy.ndarray\n            2D mean slope image\n\n        stdev_image : numpy.ndarray\n            2D stdev image\n        """"""\n\n        try:\n            with fits.open(filename) as hdu:\n                mean_image = hdu[\'MEAN\'].data\n                stdev_image = hdu[\'STDEV\'].data\n            return mean_image, stdev_image\n        except (FileNotFoundError, KeyError) as e:\n            logging.warning(\'Trying to read {}: {}\'.format(filename, e))\n\n    @log_fail\n    @log_info\n    def run(self):\n        """"""The main method.  See module docstrings for further\n        details.\n        """"""\n\n        logging.info(\'Begin logging for dark_monitor\')\n\n        apertures_to_skip = [\'NRCALL_FULL\', \'NRCAS_FULL\', \'NRCBS_FULL\']\n\n        # Get the output directory\n        self.output_dir = os.path.join(get_config()[\'outputs\'], \'dark_monitor\')\n\n        # Read in config file that defines the thresholds for the number\n        # of dark files that must be present in order for the monitor to run\n        limits = ascii.read(THRESHOLDS_FILE)\n\n        # Use the current time as the end time for MAST query\n        self.query_end = Time.now().mjd\n\n        # Loop over all instruments\n        for instrument in JWST_INSTRUMENT_NAMES:\n            self.instrument = instrument\n\n            # Identify which database tables to use\n            self.identify_tables()\n\n            # Get a list of all possible apertures from pysiaf\n            possible_apertures = list(Siaf(instrument).apernames)\n            possible_apertures = [ap for ap in possible_apertures if ap not in apertures_to_skip]\n\n            for aperture in possible_apertures:\n                logging.info(\'\')\n                logging.info(\'Working on aperture {} in {}\'.format(aperture, instrument))\n\n                # Find the appropriate threshold for the number of new files needed\n                match = aperture == limits[\'Aperture\']\n                file_count_threshold = limits[\'Threshold\'][match]\n\n                # Locate the record of the most recent MAST search\n                self.aperture = aperture\n                self.query_start = self.most_recent_search()\n                logging.info(\'\\tQuery times: {} {}\'.format(self.query_start, self.query_end))\n\n                # Query MAST using the aperture and the time of the\n                # most recent previous search as the starting time\n                new_entries = mast_query_darks(instrument, aperture, self.query_start, self.query_end)\n\n                logging.info(\'\\tAperture: {}, new entries: {}\'.format(self.aperture, len(new_entries)))\n\n                # Check to see if there are enough new files to meet the\n                # monitor\'s signal-to-noise requirements\n                if len(new_entries) >= file_count_threshold:\n                    logging.info(\'\\tSufficient new dark files found for {}, {} to run the dark monitor.\'\n                                 .format(self.instrument, self.aperture))\n\n                    # Get full paths to the files\n                    new_filenames = []\n                    for file_entry in new_entries:\n                        try:\n                            new_filenames.append(filesystem_path(file_entry[\'filename\']))\n                        except FileNotFoundError:\n                            logging.warning(\'\\t\\tUnable to locate {} in filesystem. Not including in processing.\'\n                                            .format(file_entry[\'filename\']))\n\n                    # Set up directories for the copied data\n                    ensure_dir_exists(os.path.join(self.output_dir, \'data\'))\n                    self.data_dir = os.path.join(self.output_dir,\n                                                 \'data/{}_{}\'.format(self.instrument.lower(),\n                                                                     self.aperture.lower()))\n                    ensure_dir_exists(self.data_dir)\n\n                    # Copy files from filesystem\n                    dark_files, not_copied = copy_files(new_filenames, self.data_dir)\n\n                    logging.info(\'\\tNew_filenames: {}\'.format(new_filenames))\n                    logging.info(\'\\tData dir: {}\'.format(self.data_dir))\n                    logging.info(\'\\tCopied to working dir: {}\'.format(dark_files))\n                    logging.info(\'\\tNot copied: {}\'.format(not_copied))\n\n                    # Run the dark monitor\n                    self.process(dark_files)\n                    monitor_run = True\n\n                else:\n                    logging.info((\'\\tDark monitor skipped. {} new dark files for {}, {}. {} new files are \'\n                                  \'required to run dark current monitor.\').format(\n                        len(new_entries), instrument, aperture, file_count_threshold[0]))\n                    monitor_run = False\n\n                # Update the query history\n                new_entry = {\'instrument\': instrument,\n                             \'aperture\': aperture,\n                             \'start_time_mjd\': self.query_start,\n                             \'end_time_mjd\': self.query_end,\n                             \'files_found\': len(new_entries),\n                             \'run_monitor\': monitor_run,\n                             \'entry_date\': datetime.datetime.now()}\n                self.query_table.__table__.insert().execute(new_entry)\n                logging.info(\'\\tUpdated the query history table\')\n\n        logging.info(\'Dark Monitor completed successfully.\')\n\n    def save_mean_slope_image(self, slope_img, stdev_img, files):\n        """"""Save the mean slope image and associated stdev image to a\n        file\n\n        Parameters\n        ----------\n        slope_img : numpy.ndarray\n            2D array containing the mean slope image\n\n        stdev_img : numpy.ndarray\n            2D array containing the stdev image associated with the mean\n            slope image.\n\n        files : list\n            List of input files used to construct the mean slope image\n\n        Returns\n        -------\n        output_filename : str\n            Name of fits file to save mean and stdev images within\n        """"""\n\n        output_filename = \'{}_{}_{}_to_{}_mean_slope_image.fits\'.format(self.instrument.lower(),\n                                                                        self.aperture.lower(),\n                                                                        self.query_start, self.query_end)\n\n        mean_slope_dir = os.path.join(get_config()[\'outputs\'], \'dark_monitor\', \'mean_slope_images\')\n        ensure_dir_exists(mean_slope_dir)\n        output_filename = os.path.join(mean_slope_dir, output_filename)\n        logging.info(""Name of mean slope image: {}"".format(output_filename))\n\n        primary_hdu = fits.PrimaryHDU()\n        primary_hdu.header[\'INSTRUME\'] = (self.instrument, \'JWST instrument\')\n        primary_hdu.header[\'APERTURE\'] = (self.aperture, \'Aperture name\')\n        primary_hdu.header[\'QRY_STRT\'] = (self.query_start, \'MAST Query start time (MJD)\')\n        primary_hdu.header[\'QRY_END\'] = (self.query_end, \'MAST Query end time (MJD)\')\n\n        files_string = \'FILES USED: \'\n        for filename in files:\n            files_string += \'{}, \'.format(filename)\n\n        primary_hdu.header.add_history(files_string)\n        mean_img_hdu = fits.ImageHDU(slope_img, name=\'MEAN\')\n        stdev_img_hdu = fits.ImageHDU(stdev_img, name=\'STDEV\')\n        hdu_list = fits.HDUList([primary_hdu, mean_img_hdu, stdev_img_hdu])\n        hdu_list.writeto(output_filename, overwrite=True)\n        set_permissions(output_filename)\n\n        return output_filename\n\n    def shift_to_full_frame(self, coords):\n        """"""Shift the input list of pixels from the subarray coordinate\n        system to the full frame coordinate system\n\n        Parameters\n        ----------\n        coords : tup\n            (x, y) pixel coordinates in subarray coordinate system\n\n        Returns\n        -------\n        coords : tup\n            (x, y) pixel coordinates in full frame coordinate system\n        """"""\n\n        x = coords[0]\n        x += self.x0\n        y = coords[1]\n        y += self.y0\n\n        return (x, y)\n\n    def stats_by_amp(self, image, amps):\n        """"""Calculate statistics in the input image for each amplifier as\n        well as the full image\n\n        Parameters\n        ----------\n        image : numpy.ndarray\n            2D array on which to calculate statistics\n\n        amps : dict\n            Dictionary containing amp boundary coordinates (output from\n            ``amplifier_info`` function)\n            ``amps[key] = [(xmin, xmax, xstep), (ymin, ymax, ystep)]``\n\n        Returns\n        -------\n        amp_means : dict\n            Sigma-clipped mean value for each amp. Keys are amp numbers\n            as strings (e.g. ``\'1\'``)\n\n        amp_stdevs : dict\n            Sigma-clipped standard deviation for each amp. Keys are amp\n            numbers as strings (e.g. ``\'1\'``)\n\n        gaussian_params : dict\n            Best-fit Gaussian parameters to the dark current histogram.\n            Keys are amp numbers as strings. Values are three-element\n            lists ``[amplitude, peak, width]``. Each element in the list\n            is a tuple of the best-fit value and the associated\n            uncertainty.\n\n        gaussian_chi_squared : dict\n            Reduced chi-squared for the best-fit parameters. Keys are\n            amp numbers as strings\n\n        double_gaussian_params : dict\n            Best-fit double Gaussian parameters to the dark current\n            histogram. Keys are amp numbers as strings. Values are six-\n            element lists. (3-elements * 2 Gaussians).\n            ``[amplitude1, peak1, stdev1, amplitude2, peak2, stdev2]``\n            Each element of the list is a tuple containing the best-fit\n            value and associated uncertainty.\n\n        double_gaussian_chi_squared : dict\n            Reduced chi-squared for the best-fit parameters. Keys are\n            amp numbers as strings\n\n        hist : numpy.ndarray\n            1D array of histogram values\n\n        bin_centers : numpy.ndarray\n            1D array of bin centers that match the ``hist`` values.\n        """"""\n\n        amp_means = {}\n        amp_stdevs = {}\n        gaussian_params = {}\n        gaussian_chi_squared = {}\n        double_gaussian_params = {}\n        double_gaussian_chi_squared = {}\n\n        # Add full image coords to the list of amp_boundaries, so that full\n        # frame stats are also calculated.\n        if \'FULL\' in self.aperture:\n            maxx = 0\n            maxy = 0\n            for amp in amps:\n                mxx = amps[amp][0][1]\n                mxy = amps[amp][1][1]\n                if mxx > maxx:\n                    maxx = copy(mxx)\n                if mxy > maxy:\n                    maxy = copy(mxy)\n            amps[\'5\'] = [(0, maxx, 1), (0, maxy, 1)]\n            logging.info((\'\\tFull frame exposure detected. Adding the full frame to the list \'\n                          \'of amplifiers upon which to calculate statistics.\'))\n\n        for key in amps:\n            x_start, x_end, x_step = amps[key][0]\n            y_start, y_end, y_step = amps[key][1]\n            indexes = np.mgrid[y_start: y_end: y_step, x_start: x_end: x_step]\n\n            # Basic statistics, sigma clipped areal mean and stdev\n            amp_mean, amp_stdev = calculations.mean_stdev(image[indexes[0], indexes[1]])\n            amp_means[key] = amp_mean\n            amp_stdevs[key] = amp_stdev\n\n            # Create a histogram\n            lower_bound = (amp_mean - 7 * amp_stdev)\n            upper_bound = (amp_mean + 7 * amp_stdev)\n\n            hist, bin_edges = np.histogram(image[indexes[0], indexes[1]], bins=\'auto\',\n                                           range=(lower_bound, upper_bound))\n            bin_centers = (bin_edges[1:] + bin_edges[0: -1]) / 2.\n            initial_params = [np.max(hist), amp_mean, amp_stdev]\n\n            # Fit a Gaussian to the histogram. Save best-fit params and\n            # uncertainties, as well as reduced chi squared\n            amplitude, peak, width = calculations.gaussian1d_fit(bin_centers, hist, initial_params)\n            gaussian_params[key] = [amplitude, peak, width]\n\n            gauss_fit_model = models.Gaussian1D(amplitude=amplitude[0], mean=peak[0], stddev=width[0])\n            gauss_fit = gauss_fit_model(bin_centers)\n\n            positive = hist > 0\n            degrees_of_freedom = len(hist) - 3.\n            total_pix = np.sum(hist[positive])\n            p_i = gauss_fit[positive] / total_pix\n            gaussian_chi_squared[key] = (np.sum((hist[positive] - (total_pix * p_i) ** 2) / (total_pix * p_i))\n                                         / degrees_of_freedom)\n\n            # Double Gaussian fit only for full frame data (and only for\n            # NIRISS, NIRCam at the moment.)\n            if key == \'5\':\n                if self.instrument.upper() in [\'NIRISS\', \'NIRCAM\']:\n                    initial_params = (np.max(hist), amp_mean, amp_stdev * 0.8,\n                                      np.max(hist) / 7., amp_mean / 2., amp_stdev * 0.9)\n                    double_gauss_params, double_gauss_sigma = calculations.double_gaussian_fit(bin_centers, hist, initial_params)\n                    double_gaussian_params[key] = [[param, sig] for param, sig in zip(double_gauss_params, double_gauss_sigma)]\n                    double_gauss_fit = calculations.double_gaussian(bin_centers, *double_gauss_params)\n                    degrees_of_freedom = len(bin_centers) - 6.\n                    dp_i = double_gauss_fit[positive] / total_pix\n                    double_gaussian_chi_squared[key] = np.sum((hist[positive] - (total_pix * dp_i) ** 2) / (total_pix * dp_i)) / degrees_of_freedom\n\n                else:\n                    double_gaussian_params[key] = [[0., 0.] for i in range(6)]\n                    double_gaussian_chi_squared[key] = 0.\n            else:\n                double_gaussian_params[key] = [[0., 0.] for i in range(6)]\n                double_gaussian_chi_squared[key] = 0.\n\n        logging.info(\'\\tMean dark rate by amplifier: {}\'.format(amp_means))\n        logging.info(\'\\tStandard deviation of dark rate by amplifier: {}\'.format(amp_means))\n        logging.info(\'\\tBest-fit Gaussian parameters [amplitude, peak, width]\'.format(gaussian_params))\n        logging.info(\'\\tReduced chi-squared associated with Gaussian fit: {}\'.format(gaussian_chi_squared))\n        logging.info(\'\\tBest-fit double Gaussian parameters [amplitude1, peak1, width1, amplitude2, peak2, \'\n                     \'width2]\'.format(double_gaussian_params))\n        logging.info(\'\\tReduced chi-squared associated with double Gaussian fit: {}\'\n                     .format(double_gaussian_chi_squared))\n\n        return (amp_means, amp_stdevs, gaussian_params, gaussian_chi_squared, double_gaussian_params,\n                double_gaussian_chi_squared, hist.astype(np.float), bin_centers)\n\n\nif __name__ == \'__main__\':\n\n    module = os.path.basename(__file__).strip(\'.py\')\n    start_time, log_file = initialize_instrument_monitor(module)\n\n    monitor = Dark()\n    monitor.run()\n\n    update_monitor_table(module, start_time, log_file)\n'"
jwql/instrument_monitors/fgs_monitors/__init__.py,0,b''
jwql/instrument_monitors/miri_monitors/__init__.py,0,b''
jwql/instrument_monitors/nircam_monitors/__init__.py,0,b''
jwql/instrument_monitors/niriss_monitors/__init__.py,0,b''
jwql/instrument_monitors/nirspec_monitors/__init__.py,0,b''
jwql/website/apps/__init__.py,0,b''
jwql/website/jwql_proj/__init__.py,0,b''
jwql/website/jwql_proj/jinja2.py,0,"b'""""""Jinja2 config for ``jwql`` project.\n\nSet up Jinja2 environment and configure it to read Django ``static``\nand ``url`` tags. Define custom Jinja extensions.\n\nReferences\n----------\n    Environment definition copied from blog post about integrating Django\n    with Jinja2, found here:\n        ``https://medium.com/@samuh/using-jinja2-with-django-1-8-onwards-9c58fe1204dc``\n    Re-implementation of the Django ""now"" tag found here:\n        ``https://www.webforefront.com/django/useandcreatejinjaextensions.html``\n""""""\n\nfrom datetime import datetime\n\nfrom django.conf import settings\nfrom django.urls import reverse\nfrom django.utils import timezone\nfrom django.template.defaultfilters import date\nfrom django.contrib.staticfiles.storage import staticfiles_storage\nfrom jinja2 import Environment, lexer, nodes\nfrom jinja2.ext import Extension\n\n\ndef environment(**options):\n    env = Environment(**options)\n    env.globals.update({\n        \'static\': staticfiles_storage.url,\n        \'url\': reverse,\n    })\n\n    return env\n\n\nclass DjangoNow(Extension):\n    tags = set([\'now\'])\n\n    def _now(self, date_format):\n        tzinfo = timezone.get_current_timezone() if settings.USE_TZ else None\n        formatted = date(datetime.now(tz=tzinfo), date_format)\n        return formatted\n\n    def parse(self, parser):\n        lineno = next(parser.stream).lineno\n        token = parser.stream.expect(lexer.TOKEN_STRING)\n        date_format = nodes.Const(token.value)\n        call = self.call_method(\'_now\', [date_format], lineno=lineno)\n        token = parser.stream.current\n        if token.test(\'name:as\'):\n            next(parser.stream)\n            as_var = parser.stream.expect(lexer.TOKEN_NAME)\n            as_var = nodes.Name(as_var.value, \'store\', lineno=as_var.lineno)\n            return nodes.Assign(as_var, call, lineno=lineno)\n        else:\n            return nodes.Output([call], lineno=lineno)\n'"
jwql/website/jwql_proj/settings.py,0,"b'""""""Django settings for ``jwql`` project.\n\nContains essential project settings, including a list of installed\napps, where to find templates, credentials for connection to the\n``db.sqlite3`` database, time zone, & locations where static files are\nlocated. Generated by ``django-admin startproject`` using Django 2.0.1.\n\nAuthors\n-------\n\n    - Lauren Chambers\n\nReferences\n----------\n\n    For more information on this file, see\n        ``https://docs.djangoproject.com/en/2.0/topics/settings/``\n    For the full list of settings and their values, see\n        ``https://docs.djangoproject.com/en/2.0/ref/settings/``\n\nDependencies\n------------\n\n    The user must have a configuration file named ``config.json``\n    placed in the ``jwql/utils/`` directory.\n""""""\n\nimport os\n\nfrom jwql.utils.utils import get_config\n\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = \'nx4xai#69@7pfb@l182z9aa#h8dfoms0$eitcrt5!77en*8(y4\'\n\n# SECURITY WARNING: don\'t run with debug turned on in production!\nDEBUG = True\n\n\nALLOWED_HOSTS = [\'*\']\n\n# Application definition\nINSTALLED_APPS = [\n    \'jwql.website.apps.jwql\',\n    \'django.contrib.admin\',\n    \'django.contrib.auth\',\n    \'django.contrib.contenttypes\',\n    \'django.contrib.sessions\',\n    \'django.contrib.messages\',\n    \'django.contrib.staticfiles\',\n]\n\nMIDDLEWARE = [\n    \'django.middleware.security.SecurityMiddleware\',\n    \'django.contrib.sessions.middleware.SessionMiddleware\',\n    \'django.middleware.common.CommonMiddleware\',\n    \'django.middleware.csrf.CsrfViewMiddleware\',\n    \'django.contrib.auth.middleware.AuthenticationMiddleware\',\n    \'django.contrib.messages.middleware.MessageMiddleware\',\n    \'django.middleware.clickjacking.XFrameOptionsMiddleware\',\n]\n\nROOT_URLCONF = \'jwql.website.jwql_proj.urls\'\n\nTEMPLATES = [\n    {\n        \'BACKEND\': \'django.template.backends.jinja2.Jinja2\',\n        \'DIRS\': [os.path.join(BASE_DIR, \'apps\', \'jwql\', \'templates\')],\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'environment\': \'jwql.website.jwql_proj.jinja2.environment\',\n            \'extensions\': [\'jwql.website.jwql_proj.jinja2.DjangoNow\'],\n            \'context_processors\': [\n                \'jwql.website.apps.jwql.context_processors.base_context\',\n            ],\n        },\n    },\n    {\n        \'BACKEND\': \'django.template.backends.django.DjangoTemplates\',\n        \'DIRS\': [],\n        \'APP_DIRS\': True,\n        \'OPTIONS\': {\n            \'context_processors\': [\n                \'django.contrib.auth.context_processors.auth\',\n                \'django.contrib.messages.context_processors.messages\'\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = \'jwql.website.jwql_proj.wsgi.application\'\n\n\n# Database\n# https://docs.djangoproject.com/en/2.0/ref/settings/#databases\nDATABASES = {\n    \'default\': {\n        \'ENGINE\': \'django.db.backends.sqlite3\',\n        \'NAME\': os.path.join(BASE_DIR, \'db.sqlite3\'),\n    },\n}\n\n# Password validation\n# https://docs.djangoproject.com/en/2.0/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.UserAttributeSimilarityValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.MinimumLengthValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.CommonPasswordValidator\',\n    },\n    {\n        \'NAME\': \'django.contrib.auth.password_validation.NumericPasswordValidator\',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.0/topics/i18n/\n\nLANGUAGE_CODE = \'en-us\'\n\nTIME_ZONE = \'EST\'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.0/howto/static-files/\n\nSTATIC_URL = \'/static/\'\n\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, ""static/""),\n    get_config()[\'jwql_dir\']\n]\n'"
jwql/website/jwql_proj/urls.py,0,"b'""""""Maps URL paths to views in the ``jwql`` project.\n\nThis module connects requested URL paths to the corresponding view in\n``views.py`` for each webpage in the JWQL application. When Django is\nprovided a path, it searches through the urlpatterns list provided\nhere until it finds one that matches. It then calls the assigned view\nto load the appropriate webpage, passing an HttpRequest object.\n\nAuthors\n-------\n\n    - Lauren Chambers\n\nUse\n---\n\n    Function views\n        1. Add an import:\n            from my_app import views\n        2. Add a URL to urlpatterns:\n            path(\'\', views.home, name=\'home\')\n    Class-based views\n        1. Add an import:\n            from other_app.views import Home\n        2. Add a URL to urlpatterns:\n            path(\'\', Home.as_view(), name=\'home\')\n    Including another URLconf\n        1. Import the include() function:\n            from django.urls import include, path\n        2. Add a URL to urlpatterns:\n            path(\'blog/\', include(\'blog.urls\'))\n\nReferences\n----------\n\n    For more information please see:\n        ``https://docs.djangoproject.com/en/2.0/topics/http/urls/``\n\nNotes\n-----\n\n    Be aware that when a url is requested, it will be directed to the\n    first matching path in the ``urlpatterns`` list that it finds. The\n    ``<str:var>`` tag is just a placeholder. To avoid complications,\n    users should order their paths in order from shortest to longest,\n    and after that from most to least specific.\n""""""\n\nfrom django.contrib import admin\nfrom django.urls import include, path\n\nfrom ..apps.jwql import views\n\n# Define custom error page views\nhandler404 = views.not_found  # Page not found\nhandler500 = views.not_found  # Internal error\nhandler403 = views.not_found  # Permission denied\nhandler400 = views.not_found  # Bad request\n\nurlpatterns = [\n    path(\'\', include(\'jwql.website.apps.jwql.urls\')),\n    path(\'admin/\', admin.site.urls),\n]\n'"
jwql/website/jwql_proj/wsgi.py,0,"b'""""""WSGI config for ``jwql`` project.\n\nIt exposes the WSGI callable as a module-level variable named\n``application``.\n\nReferences\n----------\n\n    For more information on this file, see:\n        ``https://docs.djangoproject.com/en/2.0/howto/deployment/wsgi/``\n""""""\n\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\n#os.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""jwql_proj.settings"")\nos.environ.setdefault(""DJANGO_SETTINGS_MODULE"", ""jwql.website.jwql_proj.settings"")\n\napplication = get_wsgi_application()\n'"
jwql/instrument_monitors/miri_monitors/data_trending/15min_to_db.py,0,"b'#! /usr/bin/env python\r\n\'\'\' Auxiliary module to populate database\r\n\r\n    This module was used throughout development to populate the database. Since\r\n    the EDB had no valid data during implementation we had to download data elsewhere.\r\n    The downloaded data is in .CSV format and can easily be read by the program.\r\n    After import and sorting the process_file function extracts the useful part and\r\n    pushes it to the auxiliary database. This function can be implemented in the\r\n    final cron job.\r\n\r\nAuthors\r\n-------\r\n\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n    make sure ""directory"" points to a folder where useable 15min-samples are storedself.\r\n    make sure you already ran .utils/sql_interface.py in order to create a empty database\r\n    with prepared tables.\r\n    Run the module form the command line.\r\n\r\nNotes\r\n-----\r\n    For developement only\r\n\'\'\'\r\n\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.mnemonics as mn\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.csv_to_AstropyTable as apt\r\nfrom jwql.instrument_monitors.miri_monitors.data_trending.utils.process_data import once_a_day_routine\r\nfrom jwql.utils.utils import get_config, filename_parser\r\n\r\nimport statistics\r\nimport os\r\nimport glob\r\n\r\n#set _location_ variable\r\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\r\n\r\n#point to the directory where your files are located!\r\ndirectory = os.path.join(get_config()[\'outputs\'], \'miri_data_trending\', \'trainings_data_15min\', \'*.CSV\')\r\npaths = glob.glob(directory)\r\n\r\ndef process_file(conn, path):\r\n    \'\'\'Parse CSV file, process data within and put to DB\r\n\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to auxiliary database\r\n    path : str\r\n        defines file to read\r\n    \'\'\'\r\n\r\n    #import mnemonic data and append dict to variable below\r\n    m_raw_data = apt.mnemonics(path)\r\n\r\n    #process raw data with once a day routine\r\n    processed_data = once_a_day_routine(m_raw_data)\r\n\r\n    #push extracted and filtered data to temporary database\r\n    for key, value in processed_data.items():\r\n\r\n        #abbreviate data table\r\n        m = m_raw_data.mnemonic(key)\r\n\r\n        if key == ""SE_ZIMIRICEA"":\r\n            length = len(value)\r\n            mean = statistics.mean(value)\r\n            deviation = statistics.stdev(value)\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, ""SE_ZIMIRICEA_IDLE"", dataset)\r\n\r\n        elif key == ""IMIR_HK_ICE_SEC_VOLT4"":\r\n            length = len(value)\r\n            mean = statistics.mean(value)\r\n            deviation = statistics.stdev(value)\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, ""IMIR_HK_ICE_SEC_VOLT4_IDLE"", dataset)\r\n\r\n        else:\r\n            length = len(value)\r\n            mean = statistics.mean(value)\r\n            deviation = statistics.stdev(value)\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, key, dataset)\r\n\r\ndef main():\r\n    #generate paths\r\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'miri_database.db\')\r\n\r\n    #connect to temporary database\r\n    conn = sql.create_connection(DATABASE_FILE)\r\n\r\n    #process every csv file in directory folder\r\n    for path in paths:\r\n        process_file(conn, path)\r\n\r\n    #close connection\r\n    sql.close_connection(conn)\r\n    print(""done"")\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
jwql/instrument_monitors/miri_monitors/data_trending/__init__.py,0,b''
jwql/instrument_monitors/miri_monitors/data_trending/dashboard.py,0,"b'#! /usr/bin/env python\n""""""Combines plots to tabs and prepares dashboard\n\nThe module imports all prepares plot functions from .plots and combines\nprebuilt tabs to a dashboard. Furthermore it defines the timerange for\nthe visualisation. Default time_range should be set to about 4 Month (120days)\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``data_container.py``, e.g.:\n\n    ::\n        import jwql.instrument_monitors.miri_monitors.data_trending.dashboard as dash\n        dashboard, variables = dash.data_trending_dashboard(start_time, end_time)\n\nDependencies\n------------\n    User must provide ""miri_database.db"" in folder jwql/database\n\n""""""\nimport datetime\nimport os\n\nfrom bokeh.embed import components\nfrom bokeh.models.widgets import Tabs\n\n#import plot functions\nfrom .plots.power_tab import power_plots\nfrom .plots.ice_voltage_tab import volt_plots\nfrom .plots.fpe_voltage_tab import fpe_plots\nfrom .plots.temperature_tab import temperature_plots\nfrom .plots.bias_tab import bias_plots\nfrom .plots.wheel_ratio_tab import wheel_ratios\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\nfrom jwql.utils.utils import get_config\n\n\n#configure actual datetime in order to implement range function\nnow = datetime.datetime.now()\n#default_start = now - datetime.timedelta(1000)\ndefault_start = datetime.date(2017, 8, 15).isoformat()\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\nPACKAGE_DIR = __location__.split(\'instrument_monitors\')[0]\n\ndef data_trending_dashboard(start = default_start, end = now):\n    """"""Builds dashboard\n    Parameters\n    ----------\n    start : time\n        configures start time for query and visualisation\n    end : time\n        configures end time for query and visualisation\n    Return\n    ------\n    plot_data : list\n        A list containing the JavaScript and HTML content for the dashboard\n    variables : dict\n        no use\n    """"""\n\n    #connect to database\n    # DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\n    DATABASE_LOCATION = os.path.join(PACKAGE_DIR, \'database\')\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'miri_database.db\')\n\n    conn = sql.create_connection(DATABASE_FILE)\n\n    #some variables can be passed to the template via following\n    variables = dict(init = 1)\n\n    #some variables can be passed to the template via following\n    variables = dict(init = 1)\n\n    #add tabs to dashboard\n    tab1 = power_plots(conn, start, end)\n    tab2 = volt_plots(conn, start, end)\n    tab3 = fpe_plots(conn, start, end)\n    tab4 = temperature_plots(conn, start, end)\n    tab5 = bias_plots(conn, start, end)\n    tab6 = wheel_ratios(conn, start, end)\n\n    #build dashboard\n    tabs = Tabs( tabs=[ tab1, tab2, tab3, tab5, tab4, tab6 ] )\n\n    #return dashboard to web app\n    script, div = components(tabs)\n    plot_data = [div, script]\n\n    #close sql connection\n    sql.close_connection(conn)\n\n    return plot_data, variables\n'"
jwql/instrument_monitors/miri_monitors/data_trending/day_to_db.py,0,"b'#! /usr/bin/env python\r\n\'\'\' Auxiliary module to populate database\r\n\r\n    This module was used throughout development to populate the database. Since\r\n    the EDB had no valid data during implementation we had to download data elsewhere.\r\n    The downloaded data is in .CSV format and can easily be read by the program.\r\n    After import and sorting the process_file function extracts the useful part and\r\n    pushes it to the auxiliary database. This function can be implemented in the\r\n    final cron job.\r\n\r\nAuthors\r\n-------\r\n\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n    make sure ""directory"" points to a folder where useable day-samples are stored.\r\n    make sure you already ran .utils/sql_interface.py in order to create a empty database\r\n    with prepared tables.\r\n    Run the module form the command line.\r\n\r\nNotes\r\n-----\r\n    For developement only\r\n\'\'\'\r\n\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.mnemonics as mn\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.csv_to_AstropyTable as apt\r\nfrom jwql.instrument_monitors.miri_monitors.data_trending.utils.process_data import whole_day_routine, wheelpos_routine\r\nfrom jwql.utils.utils import get_config, filename_parser\r\n\r\nimport os\r\nimport glob\r\nimport statistics\r\nimport sqlite3\r\n\r\n#set _location_ variable\r\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\r\n\r\n#files with data to initially fill the database\r\ndirectory = os.path.join(get_config()[\'outputs\'], \'miri_data_trending\', \'trainings_data_day\', \'*.CSV\')\r\npaths = glob.glob(directory)\r\n\r\n\r\ndef process_file(conn, path):\r\n    \'\'\'Parse CSV file, process data within and put to DB\r\n\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to temporary database\r\n    path : str\r\n        defines file to read\r\n    \'\'\'\r\n\r\n    m_raw_data = apt.mnemonics(path)\r\n\r\n    cond3, FW_volt, GW14_volt, GW23_volt, CCC_volt = whole_day_routine(m_raw_data)\r\n    FW, GW14, GW23, CCC= wheelpos_routine(m_raw_data)\r\n\r\n    #put data from con3 to database\r\n    for key, value in cond3.items():\r\n\r\n        m = m_raw_data.mnemonic(key)\r\n\r\n        if value != None:\r\n            if len(value) > 2:\r\n                if key == ""SE_ZIMIRICEA"":\r\n                    length = len(value)\r\n                    mean = statistics.mean(value)\r\n                    deviation = statistics.stdev(value)\r\n                    dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n                    sql.add_data(conn, ""SE_ZIMIRICEA_HV_ON"", dataset)\r\n\r\n                elif key == ""IMIR_HK_ICE_SEC_VOLT4"":\r\n                    length = len(value)\r\n                    mean = statistics.mean(value)\r\n                    deviation = statistics.stdev(value)\r\n                    dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n                    sql.add_data(conn, ""IMIR_HK_ICE_SEC_VOLT4_HV_ON"", dataset)\r\n\r\n                else:\r\n                    length = len(value)\r\n                    mean = statistics.mean(value)\r\n                    deviation = statistics.stdev(value)\r\n                    dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n                    sql.add_data(conn, key, dataset)\r\n\r\n\r\n    #########################################################################################\r\n    for pos in mn.fw_positions:\r\n        try:\r\n            data = FW[pos]\r\n            for element in data:\r\n                sql.add_wheel_data(conn, \'IMIR_HK_FW_POS_RATIO_{}\'.format(pos), element)\r\n        except KeyError:\r\n            pass\r\n\r\n    for pos in mn.gw_positions:\r\n        try:\r\n            data_GW14 = GW14[pos]\r\n            data_GW23 = GW23[pos]\r\n\r\n            for element in data_GW14:\r\n                sql.add_wheel_data(conn, \'IMIR_HK_GW14_POS_RATIO_{}\'.format(pos), element)\r\n            for element in data_GW23:\r\n                sql.add_wheel_data(conn, \'IMIR_HK_GW23_POS_RATIO_{}\'.format(pos), element)\r\n        except KeyError:\r\n            pass\r\n\r\n    for pos in mn.ccc_positions:\r\n        try:\r\n            data = CCC[pos]\r\n            for element in data:\r\n                sql.add_wheel_data(conn, \'IMIR_HK_CCC_POS_RATIO_{}\'.format(pos), element)\r\n        except KeyError:\r\n            pass\r\n\r\n\r\ndef main():\r\n    #point to database\r\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'miri_database.db\')\r\n\r\n    #connect to temporary database\r\n    conn = sql.create_connection(DATABASE_FILE)\r\n\r\n    #process all files found ind folder ""directory""\r\n    for path in paths:\r\n        process_file(conn, path)\r\n\r\n    sql.close_connection(conn)\r\n    print(""done"")\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
jwql/instrument_monitors/miri_monitors/data_trending/dt_cron_job.py,0,"b'#! /usr/bin/env python\r\n\'\'\' Cron Job for miri datatrending -> populates database\r\n\r\n    This module holds functions to connect with the engineering database in order\r\n    to grab and process data for the specific miri database. The scrips queries\r\n    a daily 15 min chunk and a whole day dataset. These contain several mnemonics\r\n    defined in \'\'mnemonics.py\'\'. The queried data gets processed and stored in\r\n    an auxiliary database.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nDependencies\r\n------------\r\n    For further information please contact Brian O\'Sullivan\r\n\r\nReferences\r\n----------\r\n\r\n\'\'\'\r\nimport utils.mnemonics as mn\r\nimport utils.sql_interface as sql\r\nfrom utils.process_data import whole_day_routine, wheelpos_routine\r\nfrom jwql.utils.engineering_database import query_single_mnemonic\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport statistics\r\nimport sqlite3\r\nimport os\r\n\r\nfrom astropy.time import Time\r\n\r\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\r\nPACKAGE_DIR = __location__.split(\'instrument_monitors\')[0]\r\n\r\n\r\ndef process_day_sample(conn, m_raw_data):\r\n    \'\'\'Parse CSV file, process data within and put to DB\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to temporary database\r\n    path : str\r\n        defines path to the files\r\n    \'\'\'\r\n\r\n    m_raw_data = apt.mnemonics(path)\r\n\r\n    cond3, FW_volt, GW14_volt, GW23_volt, CCC_volt = whole_day_routine(m_raw_data)\r\n    FW, GW14, GW23, CCC= wheelpos_routine(m_raw_data)\r\n\r\n    #put data from con3 to database\r\n    for key, value in cond3.items():\r\n\r\n        m = m_raw_data.mnemonic(key)\r\n\r\n        if value != None:\r\n            if len(value) > 2:\r\n                if key == ""SE_ZIMIRICEA"":\r\n                    length = len(value)\r\n                    mean = statistics.mean(value)\r\n                    deviation = statistics.stdev(value)\r\n                    dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n                    sql.add_data(conn, ""SE_ZIMIRICEA_HV_ON"", dataset)\r\n\r\n                elif key == ""IMIR_HK_ICE_SEC_VOLT4"":\r\n                    length = len(value)\r\n                    mean = statistics.mean(value)\r\n                    deviation = statistics.stdev(value)\r\n                    dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n                    sql.add_data(conn, ""IMIR_HK_ICE_SEC_VOLT4_HV_ON"", dataset)\r\n\r\n                else:\r\n                    length = len(value)\r\n                    mean = statistics.mean(value)\r\n                    deviation = statistics.stdev(value)\r\n                    dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n                    sql.add_data(conn, key, dataset)\r\n\r\n\r\n    #########################################################################################\r\n    for pos in mn.fw_positions:\r\n        try:\r\n            data = FW[pos]\r\n            for element in data:\r\n                sql.add_wheel_data(conn, \'IMIR_HK_FW_POS_RATIO_{}\'.format(pos), element)\r\n        except KeyError:\r\n            pass\r\n\r\n    for pos in mn.gw_positions:\r\n        try:\r\n            data_GW14 = GW14[pos]\r\n            data_GW23 = GW23[pos]\r\n\r\n            for element in data_GW14:\r\n                sql.add_wheel_data(conn, \'IMIR_HK_GW14_POS_RATIO_{}\'.format(pos), element)\r\n            for element in data_GW23:\r\n                sql.add_wheel_data(conn, \'IMIR_HK_GW23_POS_RATIO_{}\'.format(pos), element)\r\n        except KeyError:\r\n            pass\r\n\r\n    for pos in mn.ccc_positions:\r\n        try:\r\n            data = CCC[pos]\r\n            for element in data:\r\n                sql.add_wheel_data(conn, \'IMIR_HK_CCC_POS_RATIO_{}\'.format(pos), element)\r\n        except KeyError:\r\n            pass\r\n\r\n\r\ndef process_15min_sample(conn, m_raw_data):\r\n    \'\'\'Parse CSV file, process data within and put to DB\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to temporary database\r\n    path : str\r\n        defines path to the files\r\n    \'\'\'\r\n\r\n    #import mnemonic data and append dict to variable below\r\n    m_raw_data = apt.mnemonics(path)\r\n\r\n    #process raw data with once a day routine\r\n    processed_data = once_a_day_routine(m_raw_data)\r\n\r\n    #push extracted and filtered data to temporary database\r\n    for key, value in processed_data.items():\r\n\r\n        #abbreviate data table\r\n        m = m_raw_data.mnemonic(key)\r\n\r\n        if key == ""SE_ZIMIRICEA"":\r\n            length = len(value)\r\n            mean = statistics.mean(value)\r\n            deviation = statistics.stdev(value)\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, ""SE_ZIMIRICEA_IDLE"", dataset)\r\n\r\n        elif key == ""IMIR_HK_ICE_SEC_VOLT4"":\r\n            length = len(value)\r\n            mean = statistics.mean(value)\r\n            deviation = statistics.stdev(value)\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, ""IMIR_HK_ICE_SEC_VOLT4_IDLE"", dataset)\r\n\r\n        else:\r\n            length = len(value)\r\n            mean = statistics.mean(value)\r\n            deviation = statistics.stdev(value)\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, key, dataset)\r\n\r\ndef main():\r\n\r\n    from ..utils.engineering_database import query_single_mnemonic\r\n\r\n    mnemonic_identifier = \'SA_ZFGOUTFOV\'\r\n    start_time = Time(2016.0, format=\'decimalyear\')\r\n    end_time = Time(2018.1, format=\'decimalyear\')\r\n\r\n\r\n    mnemonic = query_single_mnemonic(mnemonic_identifier, start_time, end_time)\r\n    assert len(mnemonic.data) == mnemonic.meta[\'paging\'][\'rows\']\r\n\r\n\r\n\r\n    for mnemonic in mn.mnemonic_set_15min:\r\n        whole_day.update(mnemonic = query_single_mnemonic(mnemonic, start, end))\r\n\r\n\r\n    #configure start and end time for query\r\n    #\r\n    #\r\n    #\r\n    #\r\n\r\n    #query table start and end from engineering_database\r\n    #\r\n    #\r\n    #\r\n    #\r\n    #return table_day, table_15min\r\n\r\n    #open temporary database and write data!\r\n    DATABASE_LOCATION = os.path.join(PACKAGE_DIR, \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'miri_database.db\')\r\n\r\n    conn = sql.create_connection(DATABASE_FILE)\r\n\r\n    process_day_sample(conn, table_day)\r\n    process_15process_15min_sample(conn, table_15min)\r\n\r\n    sql.close_connection(conn)\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/15min_to_db.py,0,"b'import statistics\r\nimport os\r\nimport glob\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.mnemonics as mn\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.csv_to_AstropyTable as apt\r\nfrom jwql.utils.utils import get_config, filename_parser\r\n\r\nfrom astropy.table import Table, Column\r\n\r\nfrom jwql.instrument_monitors.nirspec_monitors.data_trending.utils.process_data import once_a_day_routine\r\n\r\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\r\n\r\n#point to the directory where your files are located!\r\ndirectory = os.path.join(get_config()[\'outputs\'], \'nirspec_data_trending\', \'nirspec_new_15min\', \'*.CSV\')\r\n\r\n#here some some files contain the same data but they are all incomplete\r\n#in order to generate a full database we have to import all of them\r\nfilenames = glob.glob(directory)\r\n\r\ndef process_file(conn, path):\r\n    \'\'\'Parse CSV file, process data within and put to DB\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to temporary database\r\n    path : str\r\n        defines path to the files\r\n    \'\'\'\r\n\r\n    #import mnemonic data and append dict to variable below\r\n    m_raw_data = apt.mnemonics(path)\r\n\r\n    #process raw data with once a day routine\r\n    returndata = once_a_day_routine(m_raw_data)\r\n\r\n    #put all data in a database that uses a condition\r\n    for key, value in returndata.items():\r\n        m = m_raw_data.mnemonic(key)\r\n        length = len(value)\r\n        mean = statistics.mean(value)\r\n        deviation = statistics.stdev(value)\r\n        dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n        sql.add_data(conn, key, dataset)\r\n\r\n    #add rest of the data to database\r\n    for identifier in mn.mnemSet_15min:\r\n\r\n        m = m_raw_data.mnemonic(identifier)\r\n\r\n        temp = []\r\n\r\n        #look for all values that fit to the given conditions\r\n        for element in m:\r\n            temp.append(float(element[\'value\']))\r\n\r\n        #return None if no applicable data was found\r\n        if len(temp) > 2:\r\n            length = len(temp)\r\n            mean = statistics.mean(temp)\r\n            deviation = statistics.stdev(temp)\r\n\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, identifier, dataset)\r\n        elif len(temp) == 2:\r\n            dataset = (float(element[\'time\']), float(element[\'time\']), 1, temp[0], 0)\r\n            sql.add_data(conn, identifier, dataset)\r\n        else:\r\n            print(\'No data for {}\'.format(identifier))\r\n            print(temp)\r\n\r\n        del temp\r\n\r\n\r\ndef main():\r\n    #generate paths\r\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'nirspec_database.db\')\r\n\r\n    #connect to temporary database\r\n    conn = sql.create_connection(DATABASE_FILE)\r\n\r\n    #do for every file in list above\r\n    for path in filenames:\r\n        process_file(conn, path)\r\n\r\n    #close connection\r\n    sql.close_connection(conn)\r\n    print(""done"")\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/__init__.py,0,b''
jwql/instrument_monitors/nirspec_monitors/data_trending/dashboard.py,0,"b'#! /usr/bin/env python\n""""""Combines plots to tabs and prepares dashboard\n\nThe module imports all prepares plot functions from .plots and combines\nprebuilt tabs to a dashboard. Furthermore it defines the timerange for\nthe visualisation. Default time_range should be set to about 4 Month (120days)\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``data_container.py``, e.g.:\n\n    ::\n        import jwql.instrument_monitors.miri_monitors.data_trending.dashboard as dash\n        dashboard, variables = dash.data_trending_dashboard(start_time, end_time)\n\nDependencies\n------------\n    User must provide ""nirspec_database.db"" in folder jwql/database\n\n""""""\nimport os\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nfrom jwql.utils.utils import get_config, filename_parser\n\nfrom bokeh.embed import components\nfrom bokeh.models.widgets import Tabs\nfrom bokeh.resources import Resources\nfrom bokeh.io.state import curstate\n\nfrom astropy.time import Time\nimport datetime\nfrom datetime import date\n\n#import plot functions\nfrom .plots.power_tab import power_plots\nfrom .plots.voltage_tab import volt_plots\nfrom .plots.temperature_tab import temperature_plots\nfrom .plots.msa_mce_tab import msa_mce_plots\nfrom .plots.fpe_fpa_tab import fpe_fpa_plots\nfrom .plots.caa_tab import caa_plots\nfrom .plots.wheel_tab import wheel_pos\n\n#configure actual datetime in order to implement range function\nnow = datetime.datetime.now()\n#default_start = now - datetime.timedelta(1000)\ndefault_start = datetime.date(2017, 8, 15).isoformat()\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n\ndef data_trending_dashboard(start = default_start, end = now):\n    """"""Bulilds dashboard\n    Parameters\n    ----------\n    start : time\n        configures start time for query and visualisation\n    end : time\n        configures end time for query and visualisation\n    Return\n    ------\n    plot_data : list\n        A list containing the JavaScript and HTML content for the dashboard\n    variables : dict\n        no use\n    """"""\n\n    #connect to database\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'nirspec_database.db\')\n\n    conn = sql.create_connection(DATABASE_FILE)\n\n    #some variables can be passed to the template via following\n    variables = dict(init = 1)\n\n    #some variables can be passed to the template via following\n    variables = dict(init = 1)\n\n    #add tabs to dashboard\n    tab1 = power_plots(conn, start, end)\n    tab2 = volt_plots(conn, start, end)\n    tab3 = temperature_plots(conn, start, end)\n    tab4 = msa_mce_plots(conn, start, end)\n    tab5 = fpe_fpa_plots(conn, start, end)\n    tab6 = caa_plots(conn, start, end)\n    tab7 = wheel_pos(conn, start, end)\n\n    #build dashboard\n    tabs = Tabs( tabs=[ tab1, tab2, tab3, tab4, tab5, tab6, tab7] )\n    #tabs = Tabs( tabs=[ tab1, tab7] )\n\n    #return dasboard to webapp\n    script, div = components(tabs)\n    plot_data = [div, script]\n\n    #close sql connection\n    sql.close_connection(conn)\n\n    return plot_data, variables\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/day_to_db.py,0,"b'import statistics\r\nimport os\r\nimport glob\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.mnemonics as mn\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.csv_to_AstropyTable as apt\r\nfrom jwql.utils.utils import get_config, filename_parser\r\n\r\nfrom astropy.table import Table, Column\r\n\r\nfrom jwql.instrument_monitors.nirspec_monitors.data_trending.utils.process_data import whole_day_routine, wheelpos_routine\r\n\r\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\r\n\r\n#point to the directory where your files are located!\r\ndirectory = os.path.join(get_config()[\'outputs\'], \'nirspec_data_trending\', \'nirspec_more\', \'*.CSV\')\r\n\r\n#here some some files contain the same data but they are all incomplete\r\n#in order to generate a full database we have to import all of them\r\nfilenames = glob.glob(directory)\r\ntest = ""FOFTLM2019073163845064.CSV""\r\n\r\ndef process_file(conn, path):\r\n    \'\'\'Parse CSV file, process data within and put to DB\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to temporary database\r\n    path : str\r\n        defines path to the files\r\n    \'\'\'\r\n\r\n    #import mnemonic data and append dict to variable below\r\n    m_raw_data = apt.mnemonics(path)\r\n\r\n    #process raw data with once a day routine\r\n    return_data, lamp_data = whole_day_routine(m_raw_data)\r\n    FW, GWX, GWY = wheelpos_routine(m_raw_data)\r\n\r\n    for key, values in FW.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_FWA_POSITION_{}\'.format(key), data)\r\n\r\n    for key, values in GWX.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_GWA_X_POSITION_{}\'.format(key), data)\r\n\r\n    for key, values in GWY.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_GWA_Y_POSITION_{}\'.format(key), data)\r\n\r\n    #put all data to a database that uses a condition\r\n    for key, value in return_data.items():\r\n        m = m_raw_data.mnemonic(key)\r\n        length = len(value)\r\n        if length > 2:\r\n            mean = statistics.mean(value)\r\n            deviation = statistics.stdev(value)\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, key, dataset)\r\n\r\n\r\n    #add rest of the data to database -> no conditions applied\r\n    for identifier in mn.mnemSet_day:\r\n\r\n        m = m_raw_data.mnemonic(identifier)\r\n\r\n        temp = []\r\n\r\n        #look for all values that fit to the given conditions\r\n        for element in m:\r\n            temp.append(float(element[\'value\']))\r\n\r\n        #return None if no applicable data was found\r\n        if len(temp) > 2:\r\n            length = len(temp)\r\n            mean = statistics.mean(temp)\r\n            deviation = statistics.stdev(temp)\r\n\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, identifier, dataset)\r\n        else:\r\n            print(\'No data for {}\'.format(identifier))\r\n            print(temp)\r\n\r\n        del temp\r\n    #add lamp data to database -> distiction over lamps\r\n    for key, values in lamp_data.items():\r\n        for data in values:\r\n            dataset_volt = (data[0], data[1], data[5], data[6], data[7])\r\n            dataset_curr = (data[0], data[1], data[2], data[3], data[4])\r\n            sql.add_data(conn, \'LAMP_{}_VOLT\'.format(key), dataset_volt)\r\n            sql.add_data(conn, \'LAMP_{}_CURR\'.format(key), dataset_curr)\r\n\r\ndef main():\r\n    #generate paths\r\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'nirspec_database.db\')\r\n\r\n    #connect to temporary database\r\n    conn = sql.create_connection(DATABASE_FILE)\r\n\r\n    \'\'\'\r\n    path = directory + test\r\n    process_file(conn, path)\r\n    \'\'\'\r\n    #do for every file in list above\r\n    for path in filenames:\r\n        process_file(conn, path)\r\n\r\n    #close connection\r\n    sql.close_connection(conn)\r\n    print(""done"")\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/dt_cron_job.py,0,"b'#! /usr/bin/env python\r\n\'\'\' Main module for nirspec datatrending -> fills database\r\n\r\n    This module holds functions to connect with the engineering database in order\r\n    to grab and process data for the specific miri database. The scrips queries\r\n    a daily 15 min chunk and a whole day dataset. These contain several mnemonics\r\n    defined in \'\'mnemonics.py\'\'. The queried data gets processed and stored in\r\n    a prepared database.\r\n\r\nAuthors\r\n-------\r\n\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n\r\nDependencies\r\n------------\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\'\'\'\r\nimport utils.mnemonics as mn\r\nimport utils.sql_interface as sql\r\nfrom utils.process_data import whole_day_routine, wheelpos_routine\r\nfrom jwql.utils.engineering_database import query_single_mnemonic\r\n\r\nimport pandas as pd\r\nimport numpy as np\r\nimport statistics\r\nimport sqlite3\r\n\r\nfrom astropy.time import Time\r\n\r\n\r\ndef process_daysample(conn, m_raw_data):\r\n    \'\'\'Parse CSV file, process data within and put to DB\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to temporary database\r\n    path : str\r\n        defines path to the files\r\n    \'\'\'\r\n\r\n    #process raw data with once a day routine\r\n    return_data, lamp_data = whole_day_routine(m_raw_data)\r\n    FW, GWX, GWY = wheelpos_routine(m_raw_data)\r\n\r\n    #put all data to a database that uses a condition\r\n    for key, value in return_data.items():\r\n        m = m_raw_data.mnemonic(key)\r\n        length = len(value)\r\n        mean = statistics.mean(value)\r\n        deviation = statistics.stdev(value)\r\n        dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n        sql.add_data(conn, key, dataset)\r\n\r\n\r\n    #add rest of the data to database -> no conditions applied\r\n    for identifier in mn.mnemSet_day:\r\n        m = m_raw_data.mnemonic(identifier)\r\n        temp = []\r\n        #look for all values that fit to the given conditions\r\n        for element in m:\r\n            temp.append(float(element[\'value\']))\r\n        #return None if no applicable data was found\r\n        if len(temp) > 2:\r\n            length = len(temp)\r\n            mean = statistics.mean(temp)\r\n            deviation = statistics.stdev(temp)\r\n        else:\r\n            print(\'No data for {}\'.format(identifier))\r\n        del temp\r\n\r\n    #add lamp data to database -> distiction over lamps\r\n    for key, values in lamp_data.items():\r\n        for data in values:\r\n            dataset_volt = (data[0], data[1], data[5], data[6], data[7])\r\n            dataset_curr = (data[0], data[1], data[2], data[3], data[4])\r\n            sql.add_data(conn, \'LAMP_{}_VOLT\'.format(key), dataset_volt)\r\n            sql.add_data(conn, \'LAMP_{}_CURR\'.format(key), dataset_curr)\r\n\r\n\r\n    #add wheeldata to database\r\n    for key, values in FW.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_FWA_POSITION_{}\'.format(key), data)\r\n\r\n    for key, values in GWX.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_GWA_X_POSITION_{}\'.format(key), data)\r\n\r\n    for key, values in GWY.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_GWA_Y_POSITION_{}\'.format(key), data)\r\n\r\n\r\ndef process_15minsample(conn, m_raw_data):\r\n    \'\'\'Parse CSV file, process data within and put to DB\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to temporary database\r\n    path : str\r\n        defines path to the files\r\n    \'\'\'\r\n\r\n    #process raw data with once a day routine\r\n    returndata = once_a_day_routine(m_raw_data)\r\n\r\n    #put all data in a database that uses a condition\r\n    for key, value in returndata.items():\r\n        m = m_raw_data.mnemonic(key)\r\n        length = len(value)\r\n        mean = statistics.mean(value)\r\n        deviation = statistics.stdev(value)\r\n        dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n        sql.add_data(conn, key, dataset)\r\n\r\n    #add rest of the data to database\r\n    for identifier in mn.mnemSet_15min:\r\n\r\n        m = m_raw_data.mnemonic(identifier)\r\n\r\n        temp = []\r\n\r\n        #look for all values that fit to the given conditions\r\n        for element in m:\r\n            temp.append(float(element[\'value\']))\r\n\r\n        #return None if no applicable data was found\r\n        if len(temp) > 2:\r\n            length = len(temp)\r\n            mean = statistics.mean(temp)\r\n            deviation = statistics.stdev(temp)\r\n\r\n            dataset = (float(m.meta[\'start\']), float(m.meta[\'end\']), length, mean, deviation)\r\n            sql.add_data(conn, identifier, dataset)\r\n        elif len(temp) == 2:\r\n            dataset = (float(element[\'time\']), float(element[\'time\']), 1, temp[0], 0)\r\n            sql.add_data(conn, identifier, dataset)\r\n        else:\r\n            print(\'No data for {}\'.format(identifier))\r\n            print(temp)\r\n\r\n        del temp\r\n\r\ndef main():\r\n\r\n    \'\'\'\r\n    from ..utils.engineering_database import query_single_mnemonic\r\n\r\n    mnemonic_identifier = \'SA_ZFGOUTFOV\'\r\n    start_time = Time(2016.0, format=\'decimalyear\')\r\n    end_time = Time(2018.1, format=\'decimalyear\')\r\n\r\n\r\n    mnemonic = query_single_mnemonic(mnemonic_identifier, start_time, end_time)\r\n    assert len(mnemonic.data) == mnemonic.meta[\'paging\'][\'rows\']\r\n    \'\'\'\r\n\r\n\r\n    for mnemonic in mn.mnemonic_set_15min:\r\n        whole_day.update(mnemonic = query_single_mnemonic(mnemonic, start, end))\r\n\r\n\r\n    #configure start and end time for query\r\n    #\r\n    #\r\n    #\r\n    #\r\n\r\n    #query table start and end from engineering_database\r\n    #\r\n    #\r\n    #\r\n    #\r\n    #return table_day, table_15min\r\n    #generate paths\r\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'nirspec_database.db\')\r\n\r\n    #connect to temporary database\r\n    conn = sql.create_connection(DATABASE_FILE)\r\n\r\n    process_daysample(conn, table_day)\r\n    process_15minsample(conn, table_15min)\r\n\r\n    #close connection\r\n    sql.close_connection(conn)\r\n    print(""done"")\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/wheel_to_db.py,0,"b'import statistics\r\nimport os\r\nimport glob\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.mnemonics as mn\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.csv_to_AstropyTable as apt\r\nfrom jwql.utils.utils import get_config, filename_parser\r\n\r\nfrom astropy.table import Table, Column\r\n\r\nfrom jwql.instrument_monitors.nirspec_monitors.data_trending.utils.process_data import wheelpos_routine\r\n\r\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\r\n\r\n#point to the directory where your files are located!\r\ndirectory = os.path.join(get_config()[\'outputs\'], \'nirspec_data_trending\', \'nirspec_wheels\', \'*.CSV\')\r\n\r\n#here some some files contain the same data but they are all incomplete\r\n#in order to generate a full database we have to import all of them\r\nfilenames = glob.glob(directory)\r\n\r\ndef process_file(conn, path):\r\n\r\n    #import mnemonic data and append dict to variable below\r\n    m_raw_data = apt.mnemonics(path)\r\n\r\n    #process raw data with once a day routine\r\n    FW, GWX, GWY = wheelpos_routine(m_raw_data)\r\n\r\n    for key, values in FW.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_FWA_POSITION_{}\'.format(key), data)\r\n\r\n    for key, values in GWX.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_GWA_X_POSITION_{}\'.format(key), data)\r\n\r\n    for key, values in GWY.items():\r\n        for data in values:\r\n            sql.add_wheel_data(conn, \'INRSI_C_GWA_Y_POSITION_{}\'.format(key), data)\r\n\r\ndef main():\r\n    #generate paths\r\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'nirspec_database.db\')\r\n\r\n    #connect to temporary database\r\n    conn = sql.create_connection(DATABASE_FILE)\r\n\r\n    \'\'\'\r\n    path = directory + test\r\n    process_file(conn, path)\r\n    \'\'\'\r\n    #do for every file in list above\r\n    for path in filenames:\r\n        process_file(conn, path)\r\n\r\n    #close connection\r\n    sql.close_connection(conn)\r\n    print(""done"")\r\n\r\nif __name__ == ""__main__"":\r\n    main()\r\n'"
jwql/website/apps/jwql/__init__.py,0,b''
jwql/website/apps/jwql/admin.py,0,"b'""""""Customizes the ``jwql`` web app administrative page.\n\n** CURRENTLY NOT IN USE **\n\nUsed to customize django\'s admin interface, and how the data contained\nin specific models is portrayed.\n\nAuthors\n-------\n\n    - Lauren Chambers\n\nReferences\n----------\n\n    For more information please see:\n        ``https://docs.djangoproject.com/en/2.0/ref/contrib/admin/``\n""""""\n\nfrom django.contrib import admin\n\nfrom .models import ImageData\n\n\nclass ImageDataAdmin(admin.ModelAdmin):\n    # fieldsets = [(\'Filepath\', {\'fields\': [\'filepath\']}),\n    # \t\t\t (\'Instrument\', {\'fields\': [\'inst\']}),\n    #              (\'Date information\', {\'fields\': [\'pub_date\']})]\n    list_display = (\'filename\', \'inst\', \'pub_date\')\n    list_filter = [\'pub_date\']\n\nadmin.site.register(ImageData, ImageDataAdmin)\n'"
jwql/website/apps/jwql/api_views.py,0,"b'""""""Defines the views for the ``jwql`` REST API.\n\nThe functions within serve as views for the ``jwql`` REST API.\nCurrently, the following services are available:\n\n    - Return a list of all proposals (``api/proposals/``)\n    - Return a list of proposals for a given instrument (``/api/<instrument>/proposals/``)\n    - Return a list of filenames for a given proposal (``/api/<proposal>/filenames/``)\n    - Return a list of filenames for a given rootname (``/api/<rootname>/filenames/``)\n    - Return a list of preview images for a given instrument (``/api/<instrument>/preview_images/``)\n    - Return a list of preview images for a given proposal (``/api/<proposal>/preview_images/``)\n    - Return a list of preview images for a given rootname (``/api/<rootname>/preview_images/``)\n    - Return a list of thumbnails for a given instrument (``/api/<instrument>/thumbnails/``)\n    - Return a list of thumbnails for a given proposal (``/api/<proposal>/thumbnails/``)\n    - Return a list of thumbnails for a given rootname (``/api/<rootname>/thumbnails/``)\n\nWhere ``<instrument>`` is the instrument of interest (e.g. ``NIRCam``),\n``<proposal>`` is the five-digit proposal number of interest (e.g.\n``86600``), and ``<rootname>`` is the rootname of interest (e.g.\n``jw86600008001_02101_00007_guider2``).  Note that ``<rootname>`` need\nnot be the full rootname, but can be any shortened version, (e.g.\n``jw8660000801_02101``, or ``jw8660``); using an abbreviated version\nwill return all filenames associated with the rootname up to that point.\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    This module is called in ``urls.py`` as such:\n\n    ::\n        from django.urls import path\n        from . import api_views\n        urlpatterns = [path(\'web/path/to/view/\',\n                             api_views.view_name, name=\'view_name\')]\n\nReferences\n----------\n    For more information please see:\n        ``https://docs.djangoproject.com/en/2.0/topics/http/views/``\n""""""\n\nfrom django.http import JsonResponse\n\nfrom .data_containers import get_all_proposals\nfrom .data_containers import get_filenames_by_proposal\nfrom .data_containers import get_filenames_by_rootname\nfrom .data_containers import get_instrument_proposals\nfrom .data_containers import get_preview_images_by_instrument\nfrom .data_containers import get_preview_images_by_proposal\nfrom .data_containers import get_preview_images_by_rootname\nfrom .data_containers import get_thumbnails_by_instrument\nfrom .data_containers import get_thumbnails_by_proposal\nfrom .data_containers import get_thumbnails_by_rootname\nfrom .oauth import auth_required\n\n\ndef all_proposals(request):\n    """"""Return a list of proposals for the mission\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    proposals = get_all_proposals()\n    return JsonResponse({\'proposals\': proposals}, json_dumps_params={\'indent\': 2})\n\n\ndef filenames_by_proposal(request, proposal):\n    """"""Return a list of filenames for the given ``proposal``\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    proposal : str\n        The five-digit proposal number (e.g. ``88600``)\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    filenames = get_filenames_by_proposal(proposal)\n    return JsonResponse({\'filenames\': filenames}, json_dumps_params={\'indent\': 2})\n\n\ndef filenames_by_rootname(request, rootname):\n    """"""Return a list of filenames for the given ``rootname``\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    rootname : str\n        The rootname of interest (e.g. ``jw86600008001_02101_00007_guider2``).\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    filenames = get_filenames_by_rootname(rootname)\n    return JsonResponse({\'filenames\': filenames}, json_dumps_params={\'indent\': 2})\n\n\ndef instrument_proposals(request, inst):\n    """"""Return a list of proposals for the given instrument\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        The instrument of interest.  The name of the instrument must\n        mach one of the following: (``nircam``, ``NIRCam``, ``niriss``,\n        ``NIRISS``, ``nirspec``, ``NIRSpec``, ``miri``, ``MIRI``,\n        ``fgs``, ``FGS``).\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    proposals = get_instrument_proposals(inst)\n    return JsonResponse({\'proposals\': proposals}, json_dumps_params={\'indent\': 2})\n\n\ndef preview_images_by_instrument(request, inst):\n    """"""Return a list of available preview images in the filesystem for\n    the given instrument.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        The instrument of interest.  The name of the instrument must\n        mach one of the following: (``nircam``, ``NIRCam``, ``niriss``,\n        ``NIRISS``, ``nirspec``, ``NIRSpec``, ``miri``, ``MIRI``,\n        ``fgs``, ``FGS``).\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    preview_images = get_preview_images_by_instrument(inst)\n    return JsonResponse({\'preview_images\': preview_images}, json_dumps_params={\'indent\': 2})\n\n\ndef preview_images_by_proposal(request, proposal):\n    """"""Return a list of available preview images in the filesystem for\n    the given ``proposal``.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    proposal : str\n        The five-digit proposal number (e.g. ``88600``)\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    preview_images = get_preview_images_by_proposal(proposal)\n    return JsonResponse({\'preview_images\': preview_images}, json_dumps_params={\'indent\': 2})\n\n\ndef preview_images_by_rootname(request, rootname):\n    """"""Return a list of available preview images in the filesystem for\n    the given ``rootname``.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    rootname : str\n        The rootname of interest (e.g. ``jw86600008001_02101_00007_guider2``).\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    preview_images = get_preview_images_by_rootname(rootname)\n    return JsonResponse({\'preview_images\': preview_images}, json_dumps_params={\'indent\': 2})\n\n\ndef thumbnails_by_instrument(request, inst):\n    """"""Return a list of available thumbnails in the filesystem for the\n    given instrument.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        The instrument of interest.  The name of the instrument must\n        mach one of the following: (``nircam``, ``NIRCam``, ``niriss``,\n        ``NIRISS``, ``nirspec``, ``NIRSpec``, ``miri``, ``MIRI``,\n        ``fgs``, ``FGS``).\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    thumbnails = get_thumbnails_by_instrument(inst)\n    return JsonResponse({\'thumbnails\': thumbnails}, json_dumps_params={\'indent\': 2})\n\n\ndef thumbnails_by_proposal(request, proposal):\n    """"""Return a list of available thumbnails in the filesystem for the\n    given ``proposal``.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    proposal : str\n        The five-digit proposal number (e.g. ``88600``)\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    thumbnails = get_thumbnails_by_proposal(proposal)\n    return JsonResponse({\'thumbnails\': thumbnails}, json_dumps_params={\'indent\': 2})\n\n\ndef thumbnails_by_rootname(request, rootname):\n    """"""Return a list of available thumbnails in the filesystem for the\n    given ``rootname``.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    rootname : str\n        The rootname of interest (e.g. ``jw86600008001_02101_00007_guider2``).\n\n    Returns\n    -------\n    JsonResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    thumbnails = get_thumbnails_by_rootname(rootname)\n    return JsonResponse({\'thumbnails\': thumbnails}, json_dumps_params={\'indent\': 2})\n'"
jwql/website/apps/jwql/apps.py,0,"b'""""""Customizes the ``jwql`` app settings.\n\n** CURRENTLY NOT IN USE **\n\nOptionally defines an ``AppConfig`` class that can be called in\n``INSTALLED_APPS`` in settings.py to configure the web app.\n\nAuthors\n-------\n\n    - Lauren Chambers\n\nUse\n---\n\n    This module is called in ``settings.py`` as such:\n    ::\n        INSTALLED_APPS = [\'apps.jwql.PlotsExampleConfig\',\n        ...\n        ]\n\nReferences\n----------\n    For more information please see:\n        ``https://docs.djangoproject.com/en/2.0/ref/applications/``\n""""""\n\nfrom django.apps import AppConfig\n\n\nclass PlotsExampleConfig(AppConfig):\n    name = \'jwql\'\n'"
jwql/website/apps/jwql/bokeh_containers.py,0,"b'""""""Various functions to generate Bokeh objects to be used by the\n``views`` of the ``jwql`` app.\n\nThis module contains several functions that instantiate\n``BokehTemplate`` objects to be rendered in ``views.py``.\n\nAuthors\n-------\n\n    - Gray Kanarek\n\nUse\n---\n\n    The functions within this module are intended to be imported and\n    used by ``views.py``, e.g.:\n\n    ::\n        from .bokeh_containers import dark_monitor_tabs\n""""""\n\nimport os\n\nfrom bokeh.embed import components\nfrom bokeh.layouts import layout\nfrom bokeh.models.widgets import Tabs, Panel\n\nfrom . import monitor_pages\nfrom jwql.utils.constants import FULL_FRAME_APERTURES\nfrom jwql.utils.utils import get_config\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\nFILESYSTEM_DIR = os.path.join(get_config()[\'jwql_dir\'], \'filesystem\')\nPACKAGE_DIR = os.path.dirname(__location__.split(\'website\')[0])\nREPO_DIR = os.path.split(PACKAGE_DIR)[0]\n\n\ndef dark_monitor_tabs(instrument):\n    """"""Creates the various tabs of the dark monitor results page.\n\n    Parameters\n    ----------\n    instrument : str\n        The JWST instrument of interest (e.g. ``nircam``).\n\n    Returns\n    -------\n    div : str\n        The HTML div to render dark monitor plots\n    script : str\n        The JS script to render dark monitor plots\n    """"""\n\n    full_apertures = FULL_FRAME_APERTURES[instrument.upper()]\n\n    templates_all_apertures = {}\n    for aperture in full_apertures:\n\n        # Start with default values for instrument and aperture because\n        # BokehTemplate\'s __init__ method does not allow input arguments\n        monitor_template = monitor_pages.DarkMonitor()\n\n        # Set instrument and monitor using DarkMonitor\'s setters\n        monitor_template.aperture_info = (instrument, aperture)\n        templates_all_apertures[aperture] = monitor_template\n\n    # Histogram tab\n    histograms_all_apertures = []\n    for aperture_name, template in templates_all_apertures.items():\n        histogram = template.refs[""dark_full_histogram_figure""]\n        histogram.sizing_mode = ""scale_width""  # Make sure the sizing is adjustable\n        histograms_all_apertures.append(histogram)\n\n    if instrument == \'NIRCam\':\n        a1, a2, a3, a4, a5, b1, b2, b3, b4, b5 = histograms_all_apertures\n        histogram_layout = layout(\n            [a2, a4, b3, b1],\n            [a1, a3, b4, b2],\n            [a5, b5]\n        )\n\n    elif instrument in [\'NIRISS\', \'MIRI\']:\n        single_aperture = histograms_all_apertures[0]\n        histogram_layout = layout(\n            [single_aperture]\n        )\n\n    elif instrument == \'NIRSpec\':\n        d1, d2 = histograms_all_apertures\n        histogram_layout = layout(\n            [d1, d2]\n        )\n\n    histogram_layout.sizing_mode = ""scale_width""  # Make sure the sizing is adjustable\n    histogram_tab = Panel(child=histogram_layout, title=""Histogram"")\n\n    # Current v. time tab\n    lines_all_apertures = []\n    for aperture_name, template in templates_all_apertures.items():\n        line = template.refs[""dark_current_time_figure""]\n        line.title.align = ""center""\n        line.title.text_font_size = ""20px""\n        line.sizing_mode = ""scale_width""  # Make sure the sizing is adjustable\n        lines_all_apertures.append(line)\n\n    if instrument == \'NIRCam\':\n        a1, a2, a3, a4, a5, b1, b2, b3, b4, b5 = lines_all_apertures\n        line_layout = layout(\n            [a2, a4, b3, b1],\n            [a1, a3, b4, b2],\n            [a5, b5]\n        )\n\n    elif instrument in [\'NIRISS\', \'MIRI\']:\n        single_aperture = lines_all_apertures[0]\n        line_layout = layout(\n            [single_aperture]\n        )\n\n    elif instrument == \'NIRSpec\':\n        d1, d2 = lines_all_apertures\n        line_layout = layout(\n            [d1, d2]\n        )\n\n    line_layout.sizing_mode = ""scale_width""  # Make sure the sizing is adjustable\n    line_tab = Panel(child=line_layout, title=""Trending"")\n\n    # Mean dark image tab\n\n    # The three lines below work for displaying a single image\n    image = templates_all_apertures[\'NRCA3_FULL\'].refs[""mean_dark_image_figure""]\n    image.sizing_mode = ""scale_width""  # Make sure the sizing is adjustable\n    image_layout = layout(image)\n    image.height = 250  # Not working\n    image_layout.sizing_mode = ""scale_width""\n    image_tab = Panel(child=image_layout, title=""Mean Dark Image"")\n\n    # Build tabs\n    tabs = Tabs(tabs=[histogram_tab, line_tab, image_tab])\n\n    # Return tab HTML and JavaScript to web app\n    script, div = components(tabs)\n\n    return div, script\n'"
jwql/website/apps/jwql/context_processors.py,0,"b'""""""Provides functions that define context inherent to all views.\n\nThe functions within this module define ``context`` that will be\nincluded in requests, in addition to any specific ``context`` provided\nin the view.\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    This module is defined under the ``TEMPLATES.OPTIONS`` setting in\n    ``settings.py``, e.g.:\n    ::\n\n        TEMPLATES = [\n            {\'OPTIONS\': {\'context_processors\': [\'jwql.website.apps.jwql.context_processors.base_context\'],},}\n        ]\n\n    As such, it will automatically be executed upon each request.\n""""""\n\nimport bokeh\n\nimport jwql\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES, MONITORS\n\nfrom .oauth import auth_info\n\n\n@auth_info\ndef base_context(request, user):\n    """"""Provide the context needed for the ``base.html`` template.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    context : dict\n        A dictionary containing data needed to render the ``base.html``\n        template\n    """"""\n\n    context = {}\n    context[\'inst_list\'] = JWST_INSTRUMENT_NAMES\n    context[\'tools\'] = MONITORS\n    context[\'user\'] = user\n    context[\'version\'] = jwql.__version__\n    context[\'bokeh_version\'] = bokeh.__version__\n\n    return context\n'"
jwql/website/apps/jwql/data_containers.py,2,"b'""""""Various functions to collect data to be used by the ``views`` of the\n``jwql`` app.\n\nThis module contains several functions that assist in collecting and\nproducing various data to be rendered in ``views.py`` for use by the\n``jwql`` app.\n\nAuthors\n-------\n\n    - Lauren Chambers\n    - Matthew Bourque\n\nUse\n---\n\n    The functions within this module are intended to be imported and\n    used by ``views.py``, e.g.:\n\n    ::\n        from .data_containers import get_proposal_info\n""""""\n\nimport copy\nimport glob\nimport os\nimport re\nimport tempfile\n\nfrom astropy.io import fits\nfrom astropy.table import Table\nfrom astropy.time import Time\nfrom django.conf import settings\nimport numpy as np\n\n# astroquery.mast import that depends on value of auth_mast\n# this import has to be made before any other import of astroquery.mast\nfrom jwql.utils.utils import get_config, filename_parser, check_config_for_key\ncheck_config_for_key(\'auth_mast\')\nauth_mast = get_config()[\'auth_mast\']\nmast_flavour = \'.\'.join(auth_mast.split(\'.\')[1:])\nfrom astropy import config\nconf = config.get_config(\'astroquery\')\nconf[\'mast\'] = {\'server\': \'https://{}\'.format(mast_flavour)}\nfrom astroquery.mast import Mast\nfrom jwedb.edb_interface import mnemonic_inventory\n\nfrom jwql.database import database_interface as di\nfrom jwql.edb.engineering_database import get_mnemonic, get_mnemonic_info\nfrom jwql.instrument_monitors.miri_monitors.data_trending import dashboard as miri_dash\nfrom jwql.instrument_monitors.nirspec_monitors.data_trending import dashboard as nirspec_dash\nfrom jwql.jwql_monitors import monitor_cron_jobs\nfrom jwql.utils.utils import ensure_dir_exists\nfrom jwql.utils.constants import MONITORS, JWST_INSTRUMENT_NAMES_MIXEDCASE\nfrom jwql.utils.preview_image import PreviewImage\nfrom jwql.utils.credentials import get_mast_token\nfrom .forms import MnemonicSearchForm, MnemonicQueryForm, MnemonicExplorationForm\n\n\n__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\nFILESYSTEM_DIR = os.path.join(get_config()[\'jwql_dir\'], \'filesystem\')\nPREVIEW_IMAGE_FILESYSTEM = os.path.join(get_config()[\'jwql_dir\'], \'preview_images\')\nTHUMBNAIL_FILESYSTEM = os.path.join(get_config()[\'jwql_dir\'], \'thumbnails\')\nPACKAGE_DIR = os.path.dirname(__location__.split(\'website\')[0])\nREPO_DIR = os.path.split(PACKAGE_DIR)[0]\n\n\ndef data_trending():\n    """"""Container for Miri datatrending dashboard and components\n\n    Returns\n    -------\n    variables : int\n        nonsense\n    dashboard : list\n        A list containing the JavaScript and HTML content for the\n        dashboard\n    """"""\n    dashboard, variables = miri_dash.data_trending_dashboard()\n\n    return variables, dashboard\n\n\ndef nirspec_trending():\n    """"""Container for Miri datatrending dashboard and components\n\n    Returns\n    -------\n    variables : int\n        nonsense\n    dashboard : list\n        A list containing the JavaScript and HTML content for the\n        dashboard\n    """"""\n    dashboard, variables = nirspec_dash.data_trending_dashboard()\n\n    return variables, dashboard\n\n\ndef get_acknowledgements():\n    """"""Returns a list of individuals who are acknowledged on the\n    ``about`` page.\n\n    The list is generated by reading in the contents of the ``jwql``\n    ``README`` file.  In this way, the website will automatically\n    update with updates to the ``README`` file.\n\n    Returns\n    -------\n    acknowledgements : list\n        A list of individuals to be acknowledged.\n    """"""\n\n    # Locate README file\n    readme_file = os.path.join(REPO_DIR, \'README.md\')\n\n    # Get contents of the README file\n    with open(readme_file, \'r\') as f:\n        data = f.readlines()\n\n    # Find where the acknowledgements start\n    for i, line in enumerate(data):\n        if \'Acknowledgments\' in line:\n            index = i\n\n    # Parse out the list of individuals\n    acknowledgements = data[index + 1:]\n    acknowledgements = [item.strip().replace(\'- \', \'\').split(\' [@\')[0].strip()\n                        for item in acknowledgements]\n\n    return acknowledgements\n\n\ndef get_all_proposals():\n    """"""Return a list of all proposals that exist in the filesystem.\n\n    Returns\n    -------\n    proposals : list\n        A list of proposal numbers for all proposals that exist in the\n        filesystem\n    """"""\n\n    proposals = glob.glob(os.path.join(FILESYSTEM_DIR, \'*\'))\n    proposals = [proposal.split(\'jw\')[-1] for proposal in proposals]\n    proposals = [proposal for proposal in proposals if len(proposal) == 5]\n\n    return proposals\n\n\ndef get_current_flagged_anomalies(rootname, instrument):\n    """"""Return a list of currently flagged anomalies for the given\n    ``rootname``\n\n    Parameters\n    ----------\n    rootname : str\n        The rootname of interest (e.g.\n        ``jw86600008001_02101_00001_guider2/``)\n\n    Returns\n    -------\n    current_anomalies : list\n        A list of currently flagged anomalies for the given ``rootname``\n        (e.g. ``[\'snowball\', \'crosstalk\']``)\n    """"""\n\n    table_dict = {}\n    for instrument in JWST_INSTRUMENT_NAMES_MIXEDCASE:\n        table_dict[instrument.lower()] = getattr(di, \'{}Anomaly\'.format(JWST_INSTRUMENT_NAMES_MIXEDCASE[instrument]))\n\n    table = table_dict[instrument.lower()]\n    query = di.session.query(table).filter(table.rootname == rootname).order_by(table.flag_date.desc()).limit(1)\n\n    all_records = query.data_frame\n    if not all_records.empty:\n        current_anomalies = [col for col, val in np.sum(all_records, axis=0).items() if val]\n    else:\n        current_anomalies = []\n\n    return current_anomalies\n\n\ndef get_dashboard_components():\n    """"""Build and return dictionaries containing components and html\n    needed for the dashboard.\n\n    Returns\n    -------\n    dashboard_components : dict\n        A dictionary containing components needed for the dashboard.\n    dashboard_html : dict\n        A dictionary containing full HTML needed for the dashboard.\n    """"""\n\n    output_dir = get_config()[\'outputs\']\n    name_dict = {\'\': \'\',\n                 \'monitor_mast\': \'Database Monitor\',\n                 \'monitor_filesystem\': \'Filesystem Monitor\'}\n\n    # Run the cron job monitor to produce an updated table\n    monitor_cron_jobs.status(production_mode=True)\n\n    # Build dictionary of Bokeh components from files in the output directory\n    dashboard_components = {}\n    for dir_name, _, file_list in os.walk(output_dir):\n        monitor_name = os.path.basename(dir_name)\n\n        # Only continue if the dashboard knows how to build that monitor\n        if monitor_name in name_dict.keys():\n            formatted_monitor_name = name_dict[monitor_name]\n            dashboard_components[formatted_monitor_name] = {}\n            for fname in file_list:\n                if \'component\' in fname:\n                    full_fname = \'{}/{}\'.format(monitor_name, fname)\n                    plot_name = fname.split(\'_component\')[0]\n\n                    # Generate formatted plot name\n                    formatted_plot_name = plot_name.title().replace(\'_\', \' \')\n                    for lowercase, mixed_case in JWST_INSTRUMENT_NAMES_MIXEDCASE.items():\n                        formatted_plot_name = formatted_plot_name.replace(lowercase.capitalize(),\n                                                                          mixed_case)\n                    formatted_plot_name = formatted_plot_name.replace(\'Jwst\', \'JWST\')\n                    formatted_plot_name = formatted_plot_name.replace(\'Caom\', \'CAOM\')\n\n                    # Get the div\n                    html_file = full_fname.split(\'.\')[0] + \'.html\'\n                    with open(os.path.join(output_dir, html_file), \'r\') as f:\n                        div = f.read()\n\n                    # Get the script\n                    js_file = full_fname.split(\'.\')[0] + \'.js\'\n                    with open(os.path.join(output_dir, js_file), \'r\') as f:\n                        script = f.read()\n\n                    # Save to dictionary\n                    dashboard_components[formatted_monitor_name][formatted_plot_name] = [div, script]\n\n    # Add HTML that cannot be saved as components to the dictionary\n    with open(os.path.join(output_dir, \'monitor_cron_jobs\', \'cron_status_table.html\'), \'r\') as f:\n        cron_status_table_html = f.read()\n    dashboard_html = {}\n    dashboard_html[\'Cron Job Monitor\'] = cron_status_table_html\n\n    return dashboard_components, dashboard_html\n\n\ndef get_edb_components(request):\n    """"""Return dictionary with content needed for the EDB page.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    edb_components : dict\n        Dictionary with the required components\n\n    """"""\n    mnemonic_name_search_result = {}\n    mnemonic_query_result = {}\n    mnemonic_query_result_plot = None\n    mnemonic_exploration_result = None\n\n    # If this is a POST request, we need to process the form data\n    if request.method == \'POST\':\n\n        if \'mnemonic_name_search\' in request.POST.keys():\n            # authenticate with astroquery.mast if necessary\n            logged_in = log_into_mast(request)\n\n            mnemonic_name_search_form = MnemonicSearchForm(request.POST, logged_in=logged_in,\n                                                           prefix=\'mnemonic_name_search\')\n\n            if mnemonic_name_search_form.is_valid():\n                mnemonic_identifier = mnemonic_name_search_form[\'search\'].value()\n                if mnemonic_identifier is not None:\n                    mnemonic_name_search_result = get_mnemonic_info(mnemonic_identifier)\n\n            # create forms for search fields not clicked\n            mnemonic_query_form = MnemonicQueryForm(prefix=\'mnemonic_query\')\n            mnemonic_exploration_form = MnemonicExplorationForm(prefix=\'mnemonic_exploration\')\n\n        elif \'mnemonic_query\' in request.POST.keys():\n            # authenticate with astroquery.mast if necessary\n            logged_in = log_into_mast(request)\n\n            mnemonic_query_form = MnemonicQueryForm(request.POST, logged_in=logged_in,\n                                                    prefix=\'mnemonic_query\')\n\n            # proceed only if entries make sense\n            if mnemonic_query_form.is_valid():\n                mnemonic_identifier = mnemonic_query_form[\'search\'].value()\n                start_time = Time(mnemonic_query_form[\'start_time\'].value(), format=\'iso\')\n                end_time = Time(mnemonic_query_form[\'end_time\'].value(), format=\'iso\')\n\n                if mnemonic_identifier is not None:\n                    mnemonic_query_result = get_mnemonic(mnemonic_identifier, start_time, end_time)\n                    mnemonic_query_result_plot = mnemonic_query_result.bokeh_plot()\n\n                    # generate table download in web app\n                    result_table = mnemonic_query_result.data\n\n                    # save file locally to be available for download\n                    static_dir = os.path.join(settings.BASE_DIR, \'static\')\n                    ensure_dir_exists(static_dir)\n                    file_name_root = \'mnemonic_query_result_table\'\n                    file_for_download = \'{}.csv\'.format(file_name_root)\n                    path_for_download = os.path.join(static_dir, file_for_download)\n\n                    # add meta data to saved table\n                    comments = []\n                    comments.append(\'DMS EDB query of {}:\'.format(mnemonic_identifier))\n                    for key, value in mnemonic_query_result.info.items():\n                        comments.append(\'{} = {}\'.format(key, str(value)))\n                    result_table.meta[\'comments\'] = comments\n                    comments.append(\' \')\n                    comments.append(\'Start time {}\'.format(start_time.isot))\n                    comments.append(\'End time   {}\'.format(end_time.isot))\n                    comments.append(\'Number of rows {}\'.format(len(result_table)))\n                    comments.append(\' \')\n                    result_table.write(path_for_download, format=\'ascii.fixed_width\',\n                                       overwrite=True, delimiter=\',\', bookend=False)\n                    mnemonic_query_result.file_for_download = file_for_download\n\n            # create forms for search fields not clicked\n            mnemonic_name_search_form = MnemonicSearchForm(prefix=\'mnemonic_name_search\')\n            mnemonic_exploration_form = MnemonicExplorationForm(prefix=\'mnemonic_exploration\')\n\n        elif \'mnemonic_exploration\' in request.POST.keys():\n            mnemonic_exploration_form = MnemonicExplorationForm(request.POST,\n                                                                prefix=\'mnemonic_exploration\')\n            if mnemonic_exploration_form.is_valid():\n                mnemonic_exploration_result, meta = mnemonic_inventory()\n\n                # loop over filled fields and implement simple AND logic\n                for field in mnemonic_exploration_form.fields:\n                    field_value = mnemonic_exploration_form[field].value()\n                    if field_value != \'\':\n                        column_name = mnemonic_exploration_form[field].label\n\n                        # matching indices in table (case-insensitive)\n                        index = [\n                            i for i, item in enumerate(mnemonic_exploration_result[column_name]) if\n                            re.search(field_value, item, re.IGNORECASE)\n                        ]\n                        mnemonic_exploration_result = mnemonic_exploration_result[index]\n\n                mnemonic_exploration_result.n_rows = len(mnemonic_exploration_result)\n\n                # generate tables for display and download in web app\n                display_table = copy.deepcopy(mnemonic_exploration_result)\n\n                # temporary html file,\n                # see http://docs.astropy.org/en/stable/_modules/astropy/table/\n                tmpdir = tempfile.mkdtemp()\n                file_name_root = \'mnemonic_exploration_result_table\'\n                path_for_html = os.path.join(tmpdir, \'{}.html\'.format(file_name_root))\n                with open(path_for_html, \'w\') as tmp:\n                    display_table.write(tmp, format=\'jsviewer\')\n                mnemonic_exploration_result.html_file_content = open(path_for_html, \'r\').read()\n\n                # pass on meta data to have access to total number of mnemonics\n                mnemonic_exploration_result.meta = meta\n\n                # save file locally to be available for download\n                static_dir = os.path.join(settings.BASE_DIR, \'static\')\n                ensure_dir_exists(static_dir)\n                file_for_download = \'{}.csv\'.format(file_name_root)\n                path_for_download = os.path.join(static_dir, file_for_download)\n                display_table.write(path_for_download, format=\'ascii.fixed_width\',\n                                    overwrite=True, delimiter=\',\', bookend=False)\n                mnemonic_exploration_result.file_for_download = file_for_download\n\n                if mnemonic_exploration_result.n_rows == 0:\n                    mnemonic_exploration_result = \'empty\'\n\n            # create forms for search fields not clicked\n            mnemonic_name_search_form = MnemonicSearchForm(prefix=\'mnemonic_name_search\')\n            mnemonic_query_form = MnemonicQueryForm(prefix=\'mnemonic_query\')\n\n    else:\n        mnemonic_name_search_form = MnemonicSearchForm(prefix=\'mnemonic_name_search\')\n        mnemonic_query_form = MnemonicQueryForm(prefix=\'mnemonic_query\')\n        mnemonic_exploration_form = MnemonicExplorationForm(prefix=\'mnemonic_exploration\')\n\n    edb_components = {\'mnemonic_query_form\': mnemonic_query_form,\n                      \'mnemonic_query_result\': mnemonic_query_result,\n                      \'mnemonic_query_result_plot\': mnemonic_query_result_plot,\n                      \'mnemonic_name_search_form\': mnemonic_name_search_form,\n                      \'mnemonic_name_search_result\': mnemonic_name_search_result,\n                      \'mnemonic_exploration_form\': mnemonic_exploration_form,\n                      \'mnemonic_exploration_result\': mnemonic_exploration_result}\n\n    return edb_components\n\n\ndef get_expstart(rootname):\n    """"""Return the exposure start time (``expstart``) for the given\n    group of files.\n\n    The ``expstart`` is gathered from a query to the\n    ``astroquery.mast`` service.\n\n    Parameters\n    ----------\n    rootname : str\n        The rootname of the observation of interest (e.g.\n        ``jw86700006001_02101_00006_guider1``).\n\n    Returns\n    -------\n    expstart : float\n        The exposure start time of the observation (in MJD).\n    """"""\n\n    return 5000.00\n\n\ndef get_filenames_by_instrument(instrument):\n    """"""Returns a list of paths to files that match the given\n    ``instrument``.\n\n    Parameters\n    ----------\n    instrument : str\n        The instrument of interest (e.g. `FGS`).\n\n    Returns\n    -------\n    filepaths : list\n        A list of full paths to the files that match the given\n        instrument.\n    """"""\n\n    # Query files from MAST database\n    # filepaths, filenames = DatabaseConnection(\'MAST\', instrument=instrument).\\\n    #     get_files_for_instrument(instrument)\n\n    # Find all of the matching files in filesytem\n    # (TEMPORARY WHILE THE MAST STUFF IS BEING WORKED OUT)\n    instrument_match = {\'FGS\': \'guider\',\n                        \'MIRI\': \'mir\',\n                        \'NIRCam\': \'nrc\',\n                        \'NIRISS\': \'nis\',\n                        \'NIRSpec\': \'nrs\'}\n    search_filepath = os.path.join(FILESYSTEM_DIR, \'*\', \'*.fits\')\n    filepaths = [f for f in glob.glob(search_filepath) if instrument_match[instrument] in f]\n\n    return filepaths\n\n\ndef get_filenames_by_proposal(proposal):\n    """"""Return a list of filenames that are available in the filesystem\n    for the given ``proposal``.\n\n    Parameters\n    ----------\n    proposal : str\n        The one- to five-digit proposal number (e.g. ``88600``).\n\n    Returns\n    -------\n    filenames : list\n        A list of filenames associated with the given ``proposal``.\n    """"""\n\n    proposal_string = \'{:05d}\'.format(int(proposal))\n    filenames = sorted(glob.glob(os.path.join(\n        FILESYSTEM_DIR, \'jw{}\'.format(proposal_string), \'*\')))\n    filenames = [os.path.basename(filename) for filename in filenames]\n\n    return filenames\n\n\ndef get_filenames_by_rootname(rootname):\n    """"""Return a list of filenames available in the filesystem that\n    are part of the given ``rootname``.\n\n    Parameters\n    ----------\n    rootname : str\n        The rootname of interest (e.g. ``jw86600008001_02101_00007_guider2``).\n\n    Returns\n    -------\n    filenames : list\n        A list of filenames associated with the given ``rootname``.\n    """"""\n\n    proposal = rootname.split(\'_\')[0].split(\'jw\')[-1][0:5]\n    filenames = sorted(glob.glob(os.path.join(\n        FILESYSTEM_DIR,\n        \'jw{}\'.format(proposal),\n        \'{}*\'.format(rootname))))\n    filenames = [os.path.basename(filename) for filename in filenames]\n\n    return filenames\n\n\ndef get_header_info(filename):\n    """"""Return the header information for a given ``filename``.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the file of interest (e.g.\n        ``\'jw86600008001_02101_00007_guider2_uncal.fits\'``).\n\n    Returns\n    -------\n    header : str\n        The primary FITS header for the given ``file``.\n    """"""\n\n    # Initialize dictionary to store header information\n    header_info = {}\n\n    # Open the file\n    fits_filepath = os.path.join(FILESYSTEM_DIR, filename[:7], \'{}.fits\'.format(filename))\n    hdulist = fits.open(fits_filepath)\n\n    # Extract header information from file\n    for ext in range(0, len(hdulist)):\n\n        # Initialize dictionary to store header information for particular extension\n        header_info[ext] = {}\n\n        # Get header\n        header = fits.getheader(fits_filepath, ext=ext)\n\n        # Determine the extension name\n        if ext == 0:\n            header_info[ext][\'EXTNAME\'] = \'PRIMARY\'\n        else:\n            header_info[ext][\'EXTNAME\'] = header[\'EXTNAME\']\n\n        # Get list of keywords and values\n        exclude_list = [\'\', \'COMMENT\']\n        header_info[ext][\'keywords\'] = [item for item in list(header.keys()) if item not in exclude_list]\n        header_info[ext][\'values\'] = []\n        for key in header_info[ext][\'keywords\']:\n            header_info[ext][\'values\'].append(hdulist[ext].header[key])\n\n    # Close the file\n    hdulist.close()\n\n    # Build tables\n    for ext in header_info:\n        table = Table([header_info[ext][\'keywords\'], header_info[ext][\'values\']], names=(\'Key\', \'Value\'))\n        temp_path_for_html = os.path.join(tempfile.mkdtemp(), \'{}_table.html\'.format(header_info[ext][\'EXTNAME\']))\n        with open(temp_path_for_html, \'w\') as f:\n            table.write(f, format=\'jsviewer\', jskwargs={\'display_length\': 20})\n        header_info[ext][\'table\'] = open(temp_path_for_html, \'r\').read()\n\n    return header_info\n\n\ndef get_image_info(file_root, rewrite):\n    """"""Build and return a dictionary containing information for a given\n    ``file_root``.\n\n    Parameters\n    ----------\n    file_root : str\n        The rootname of the file of interest (e.g.\n        ``jw86600008001_02101_00007_guider2``).\n    rewrite : bool\n        ``True`` if the corresponding JPEG needs to be rewritten,\n        ``False`` if not.\n\n    Returns\n    -------\n    image_info : dict\n        A dictionary containing various information for the given\n        ``file_root``.\n    """"""\n\n    # Initialize dictionary to store information\n    image_info = {}\n    image_info[\'all_jpegs\'] = []\n    image_info[\'suffixes\'] = []\n    image_info[\'num_ints\'] = {}\n\n    preview_dir = os.path.join(get_config()[\'jwql_dir\'], \'preview_images\')\n\n    # Find all of the matching files\n    dirname = file_root[:7]\n    search_filepath = os.path.join(FILESYSTEM_DIR, dirname, file_root + \'*.fits\')\n    image_info[\'all_files\'] = glob.glob(search_filepath)\n\n    for file in image_info[\'all_files\']:\n\n        # Get suffix information\n        suffix = os.path.basename(file).split(\'_\')[4].split(\'.\')[0]\n        image_info[\'suffixes\'].append(suffix)\n\n        # Determine JPEG file location\n        jpg_dir = os.path.join(preview_dir, dirname)\n        jpg_filename = os.path.basename(os.path.splitext(file)[0] + \'_integ0.jpg\')\n        jpg_filepath = os.path.join(jpg_dir, jpg_filename)\n\n        # Check that a jpg does not already exist. If it does (and rewrite=False),\n        # just call the existing jpg file\n        if os.path.exists(jpg_filepath) and not rewrite:\n            pass\n\n        # If it doesn\'t, make it using the preview_image module\n        else:\n            if not os.path.exists(jpg_dir):\n                os.makedirs(jpg_dir)\n            im = PreviewImage(file, \'SCI\')\n            im.output_directory = jpg_dir\n            im.make_image()\n\n        # Record how many integrations there are per filetype\n        search_jpgs = os.path.join(preview_dir, dirname,\n                                   file_root + \'_{}_integ*.jpg\'.format(suffix))\n        num_jpgs = len(glob.glob(search_jpgs))\n        image_info[\'num_ints\'][suffix] = num_jpgs\n\n        image_info[\'all_jpegs\'].append(jpg_filepath)\n\n    return image_info\n\n\ndef get_instrument_proposals(instrument):\n    """"""Return a list of proposals for the given instrument\n\n    Parameters\n    ----------\n    instrument : str\n        Name of the JWST instrument, with first letter capitalized\n        (e.g. ``Fgs``)\n\n    Returns\n    -------\n    proposals : list\n        List of proposals for the given instrument\n    """"""\n\n    service = ""Mast.Jwst.Filtered.{}"".format(instrument)\n    params = {""columns"": ""program"",\n              ""filters"": []}\n    response = Mast.service_request_async(service, params)\n    results = response[0].json()[\'data\']\n    proposals = list(set(result[\'program\'] for result in results))\n\n    return proposals\n\n\ndef get_preview_images_by_instrument(inst):\n    """"""Return a list of preview images available in the filesystem for\n    the given instrument.\n\n    Parameters\n    ----------\n    inst : str\n        The instrument of interest (e.g. ``NIRCam``).\n\n    Returns\n    -------\n    preview_images : list\n        A list of preview images available in the filesystem for the\n        given instrument.\n    """"""\n\n    # Make sure the instrument is of the proper format (e.g. ""Nircam"")\n    instrument = inst[0].upper() + inst[1:].lower()\n\n    # Query MAST for all rootnames for the instrument\n    service = ""Mast.Jwst.Filtered.{}"".format(instrument)\n    params = {""columns"": ""filename"",\n              ""filters"": []}\n    response = Mast.service_request_async(service, params)\n    results = response[0].json()[\'data\']\n\n    # Parse the results to get the rootnames\n    filenames = [result[\'filename\'].split(\'.\')[0] for result in results]\n\n    # Get list of all preview_images\n    preview_images = glob.glob(os.path.join(PREVIEW_IMAGE_FILESYSTEM, \'*\', \'*.jpg\'))\n\n    # Get subset of preview images that match the filenames\n    preview_images = [os.path.basename(item) for item in preview_images if\n                      os.path.basename(item).split(\'_integ\')[0] in filenames]\n\n    # Return only\n\n    return preview_images\n\n\ndef get_preview_images_by_proposal(proposal):\n    """"""Return a list of preview images available in the filesystem for\n    the given ``proposal``.\n\n    Parameters\n    ----------\n    proposal : str\n        The one- to five-digit proposal number (e.g. ``88600``).\n\n    Returns\n    -------\n    preview_images : list\n        A list of preview images available in the filesystem for the\n        given ``proposal``.\n    """"""\n\n    proposal_string = \'{:05d}\'.format(int(proposal))\n    preview_images = glob.glob(os.path.join(PREVIEW_IMAGE_FILESYSTEM, \'jw{}\'.format(proposal_string), \'*\'))\n    preview_images = [os.path.basename(preview_image) for preview_image in preview_images]\n\n    return preview_images\n\n\ndef get_preview_images_by_rootname(rootname):\n    """"""Return a list of preview images available in the filesystem for\n    the given ``rootname``.\n\n    Parameters\n    ----------\n    rootname : str\n        The rootname of interest (e.g. ``jw86600008001_02101_00007_guider2``).\n\n    Returns\n    -------\n    preview_images : list\n        A list of preview images available in the filesystem for the\n        given ``rootname``.\n    """"""\n\n    proposal = rootname.split(\'_\')[0].split(\'jw\')[-1][0:5]\n    preview_images = sorted(glob.glob(os.path.join(\n        PREVIEW_IMAGE_FILESYSTEM,\n        \'jw{}\'.format(proposal),\n        \'{}*\'.format(rootname))))\n    preview_images = [os.path.basename(preview_image) for preview_image in preview_images]\n\n    return preview_images\n\n\ndef get_proposal_info(filepaths):\n    """"""Builds and returns a dictionary containing various information\n    about the proposal(s) that correspond to the given ``filepaths``.\n\n    The information returned contains such things as the number of\n    proposals, the paths to the corresponding thumbnails, and the total\n    number of files.\n\n    Parameters\n    ----------\n    filepaths : list\n        A list of full paths to files of interest.\n\n    Returns\n    -------\n    proposal_info : dict\n        A dictionary containing various information about the\n        proposal(s) and files corresponding to the given ``filepaths``.\n    """"""\n\n    proposals = list(set([f.split(\'/\')[-1][2:7] for f in filepaths]))\n    thumbnail_dir = os.path.join(get_config()[\'jwql_dir\'], \'thumbnails\')\n    thumbnail_paths = []\n    num_files = []\n    for proposal in proposals:\n        thumbnail_search_filepath = os.path.join(\n            thumbnail_dir, \'jw{}\'.format(proposal), \'jw{}*rate*.thumb\'.format(proposal)\n        )\n        thumbnail = glob.glob(thumbnail_search_filepath)\n        if len(thumbnail) > 0:\n            thumbnail = thumbnail[0]\n            thumbnail = \'/\'.join(thumbnail.split(\'/\')[-2:])\n        thumbnail_paths.append(thumbnail)\n\n        fits_search_filepath = os.path.join(\n            FILESYSTEM_DIR, \'jw{}\'.format(proposal), \'jw{}*.fits\'.format(proposal)\n        )\n        num_files.append(len(glob.glob(fits_search_filepath)))\n\n    # Put the various information into a dictionary of results\n    proposal_info = {}\n    proposal_info[\'num_proposals\'] = len(proposals)\n    proposal_info[\'proposals\'] = proposals\n    proposal_info[\'thumbnail_paths\'] = thumbnail_paths\n    proposal_info[\'num_files\'] = num_files\n\n    return proposal_info\n\n\ndef get_thumbnails_by_instrument(inst):\n    """"""Return a list of thumbnails available in the filesystem for the\n    given instrument.\n\n    Parameters\n    ----------\n    inst : str\n        The instrument of interest (e.g. ``NIRCam``).\n\n    Returns\n    -------\n    preview_images : list\n        A list of thumbnails available in the filesystem for the\n        given instrument.\n    """"""\n\n    # Make sure the instrument is of the proper format (e.g. ""Nircam"")\n    instrument = inst[0].upper() + inst[1:].lower()\n\n    # Query MAST for all rootnames for the instrument\n    service = ""Mast.Jwst.Filtered.{}"".format(instrument)\n    params = {""columns"": ""filename"",\n              ""filters"": []}\n    response = Mast.service_request_async(service, params)\n    results = response[0].json()[\'data\']\n\n    # Parse the results to get the rootnames\n    filenames = [result[\'filename\'].split(\'.\')[0] for result in results]\n\n    # Get list of all thumbnails\n    thumbnails = glob.glob(os.path.join(THUMBNAIL_FILESYSTEM, \'*\', \'*.thumb\'))\n\n    # Get subset of preview images that match the filenames\n    thumbnails = [os.path.basename(item) for item in thumbnails if\n                  os.path.basename(item).split(\'_integ\')[0] in filenames]\n\n    return thumbnails\n\n\ndef get_thumbnails_by_proposal(proposal):\n    """"""Return a list of thumbnails available in the filesystem for the\n    given ``proposal``.\n\n    Parameters\n    ----------\n    proposal : str\n        The one- to five-digit proposal number (e.g. ``88600``).\n\n    Returns\n    -------\n    thumbnails : list\n        A list of thumbnails available in the filesystem for the given\n        ``proposal``.\n    """"""\n\n    proposal_string = \'{:05d}\'.format(int(proposal))\n    thumbnails = glob.glob(os.path.join(THUMBNAIL_FILESYSTEM, \'jw{}\'.format(proposal_string), \'*\'))\n    thumbnails = [os.path.basename(thumbnail) for thumbnail in thumbnails]\n\n    return thumbnails\n\n\ndef get_thumbnails_by_rootname(rootname):\n    """"""Return a list of preview images available in the filesystem for\n    the given ``rootname``.\n\n    Parameters\n    ----------\n    rootname : str\n        The rootname of interest (e.g. ``jw86600008001_02101_00007_guider2``).\n\n    Returns\n    -------\n    thumbnails : list\n        A list of preview images available in the filesystem for the\n        given ``rootname``.\n    """"""\n\n    proposal = rootname.split(\'_\')[0].split(\'jw\')[-1][0:5]\n    thumbnails = sorted(glob.glob(os.path.join(\n        THUMBNAIL_FILESYSTEM,\n        \'jw{}\'.format(proposal),\n        \'{}*\'.format(rootname))))\n\n    thumbnails = [os.path.basename(thumbnail) for thumbnail in thumbnails]\n\n    return thumbnails\n\n\ndef log_into_mast(request):\n    """"""Login via astroquery.mast if user authenticated in web app.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    """"""\n    if Mast.authenticated():\n        return True\n\n    # get the MAST access token if present\n    access_token = str(get_mast_token(request))\n\n    # authenticate with astroquery.mast if necessary\n    if access_token != \'None\':\n        Mast.login(token=access_token)\n        return Mast.authenticated()\n    else:\n        return False\n\n\ndef random_404_page():\n    """"""Randomly select one of the various 404 templates for JWQL\n\n    Returns\n    -------\n    random_template : str\n        Filename of the selected template\n    """"""\n    templates = [\'404_space.html\', \'404_spacecat.html\']\n    choose_page = np.random.choice(len(templates))\n    random_template = templates[choose_page]\n\n    return random_template\n\n\ndef thumbnails_ajax(inst, proposal=None):\n    """"""Generate a page that provides data necessary to render the\n    ``thumbnails`` template.\n\n    Parameters\n    ----------\n    inst : str\n        Name of JWST instrument\n    proposal : str (optional)\n        Number of APT proposal to filter\n\n    Returns\n    -------\n    data_dict : dict\n        Dictionary of data needed for the ``thumbnails`` template\n    """"""\n\n    # Get the available files for the instrument\n    filepaths = get_filenames_by_instrument(inst)\n\n    # Get set of unique rootnames\n    rootnames = set([\'_\'.join(f.split(\'/\')[-1].split(\'_\')[:-1]) for f in filepaths])\n\n    # If the proposal is specified (i.e. if the page being loaded is\n    # an archive page), only collect data for given proposal\n    if proposal is not None:\n        proposal_string = \'{:05d}\'.format(int(proposal))\n        rootnames = [rootname for rootname in rootnames if rootname[2:7] == proposal_string]\n\n    # Initialize dictionary that will contain all needed data\n    data_dict = {}\n    data_dict[\'inst\'] = inst\n    data_dict[\'file_data\'] = {}\n\n    # Gather data for each rootname\n    for rootname in rootnames:\n\n        # Parse filename\n        try:\n            filename_dict = filename_parser(rootname)\n        except ValueError:\n            # Temporary workaround for noncompliant files in filesystem\n            filename_dict = {\'activity\': rootname[17:19],\n                             \'detector\': rootname[26:],\n                             \'exposure_id\': rootname[20:25],\n                             \'observation\': rootname[7:10],\n                             \'parallel_seq_id\': rootname[16],\n                             \'program_id\': rootname[2:7],\n                             \'visit\': rootname[10:13],\n                             \'visit_group\': rootname[14:16]}\n\n        # Get list of available filenames\n        available_files = get_filenames_by_rootname(rootname)\n\n        # Add data to dictionary\n        data_dict[\'file_data\'][rootname] = {}\n        data_dict[\'file_data\'][rootname][\'filename_dict\'] = filename_dict\n        data_dict[\'file_data\'][rootname][\'available_files\'] = available_files\n        data_dict[\'file_data\'][rootname][\'expstart\'] = get_expstart(rootname)\n        data_dict[\'file_data\'][rootname][\'suffixes\'] = [filename_parser(filename)[\'suffix\'] for\n                                                        filename in available_files]\n\n    # Extract information for sorting with dropdown menus\n    # (Don\'t include the proposal as a sorting parameter if the\n    # proposal has already been specified)\n    detectors = [data_dict[\'file_data\'][rootname][\'filename_dict\'][\'detector\'] for\n                 rootname in list(data_dict[\'file_data\'].keys())]\n    proposals = [data_dict[\'file_data\'][rootname][\'filename_dict\'][\'program_id\'] for\n                 rootname in list(data_dict[\'file_data\'].keys())]\n    if proposal is not None:\n        dropdown_menus = {\'detector\': detectors}\n    else:\n        dropdown_menus = {\'detector\': detectors,\n                          \'proposal\': proposals}\n\n    data_dict[\'tools\'] = MONITORS\n    data_dict[\'dropdown_menus\'] = dropdown_menus\n    data_dict[\'prop\'] = proposal\n\n    return data_dict\n'"
jwql/website/apps/jwql/db.py,0,"b'""""""Connects to the ``jwql`` database.\n\nThis module is the primary interface between the ``jwql`` webapp and\nthe ``jwql`` database. It uses ``SQLAlchemy`` to start a session with\nthe database, and provides class methods that perform useful queries on\nthat database (for example, getting the names of all the files\nassociated with a certain instrument).\n\nAuthors\n-------\n\n    - Lauren Chambers\n\nUse\n---\n    This module can be used as such:\n\n    ::\n        from db import DatabaseConnection\n        db_connect = DatabaseConnection()\n        data = db_connect.get_filenames_for_instrument(\'NIRCam\')\n\nDependencies\n------------\n    The user must have a configuration file named ``config.json``\n    placed in ``jwql/utils/`` directory.\n""""""\n\nimport os\n\nfrom sqlalchemy.ext.automap import automap_base\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import create_engine\nfrom astroquery.mast import Mast\n\nfrom jwql.utils.utils import get_config\n\n\nclass DatabaseConnection:\n    """"""Facilitates connection with the ``jwql`` database.\n\n    Attributes\n    ----------\n    ObservationWebtest : obj\n        Class instance in an ""automap"" schema corresponding to the\n        ``observationwebtest`` database table\n    session : obj\n        Session with the database that enables querying\n    """"""\n\n    def __init__(self, db_type, instrument=None):\n        """"""Determine what kind of database is being queried, and\n        call appropriate initialization method\n        """"""\n\n        self.db_type = db_type\n\n        assert self.db_type in [\'MAST\', \'SQL\'], \\\n            \'Unrecognized database type: {}. Must be SQL or MAST.\'.format(db_type)\n\n        if self.db_type == \'MAST\':\n            self.init_MAST(instrument)\n        elif self.db_type == \'SQL\':\n            self.init_SQL()\n\n    def init_SQL(self):\n        """"""Start SQLAlchemy session with the ``jwql`` database""""""\n\n        # Get database credentials from config file\n        connection_string = get_config()[\'database\'][\'connection_string\']\n\n        # Connect to the database\n        engine = create_engine(connection_string)\n\n        # Allow for automapping of database tables to classes\n        Base = automap_base()\n\n        # Reflect the tables in the database\n        Base.prepare(engine, reflect=True)\n\n        # Find the observations table\n        self.ObservationWebtest = Base.classes.observations_webtest\n\n        # Start a session to enable queries\n        self.session = Session(engine)\n\n    def init_MAST(self, instrument=None):\n        """"""Determine the necessary service string to query the MAST\n        database.\n        """"""\n\n        # Correctly format the instrument string\n        if instrument:\n            instrument = instrument[0].upper() + instrument[1:].lower()\n        else:\n            raise TypeError(\'Must provide instrument to initialize MAST database.\')\n\n        # Define the service name for the given instrument\n        self.service = ""Mast.Jwst.Filtered."" + instrument\n        print(self.service)\n\n\n    def get_files_for_instrument(self, instrument):\n        """"""Given an instrument, query the database for all filenames\n        and paths associated with said instrument\n\n        Parameters\n        ----------\n        instrument : str\n            Name of JWST instrument\n\n        Returns\n        -------\n        filepaths: list\n            List of all filepaths in database for the provided\n            instrument\n        filenames: list\n            List of all filenames in database for the provided\n            instrument\n        """"""\n\n        instrument = instrument.upper()\n\n        if self.db_type == \'SQL\':\n            results = self.session.query(self.ObservationWebtest)\\\n                .filter(self.ObservationWebtest.instrument == instrument)\n        elif self.db_type == \'MAST\':\n            params = {""columns"": ""*"",\n                      ""filters"": []}\n            response = Mast.service_request_async(self.service, params)\n            results = response[0].json()[\'data\']\n\n        filepaths = []\n        filenames = []\n        for i in results:\n            if self.db_type == \'SQL\':\n                filename = i.filename\n            elif self.db_type == \'MAST\':\n                filename = i[\'filename\']\n            prog_id = filename[2:7]\n            file_path = os.path.join(\'jw\' + prog_id, filename)\n            filepaths.append(file_path)\n            filenames.append(filename)\n\n        return filepaths, filenames\n'"
jwql/website/apps/jwql/forms.py,0,"b'""""""Defines the forms for the ``jwql`` web app.\n\nDjango allows for an object-oriented model representation of forms for\nusers to provide input through HTTP POST methods. This module defines\nall of the forms that are used across the various webpages used for the\nJWQL application.\n\nAuthors\n-------\n\n    - Lauren Chambers\n    - Johannes Sahlmann\n    - Matthew Bourque\n\nUse\n---\n\n    This module is used within ``views.py`` as such:\n    ::\n        from .forms import FileSearchForm\n        def view_function(request):\n            form = FileSearchForm(request.POST or None)\n\n            if request.method == \'POST\':\n                if form.is_valid():\n                    # Process form input and redirect\n                    return redirect(new_url)\n\n            template = \'some_template.html\'\n            context = {\'form\': form, ...}\n            return render(request, template, context)\n\nReferences\n----------\n    For more information please see:\n        ``https://docs.djangoproject.com/en/2.1/topics/forms/``\n\nDependencies\n------------\n    The user must have a configuration file named ``config.json``\n    placed in the ``jwql/utils/`` directory.\n\n""""""\n\nimport datetime\nimport glob\nimport os\n\nfrom astropy.time import Time, TimeDelta\nfrom django import forms\nfrom django.shortcuts import redirect\nfrom jwedb.edb_interface import is_valid_mnemonic\n\nfrom jwql.database import database_interface as di\nfrom jwql.utils.constants import ANOMALY_CHOICES, JWST_INSTRUMENT_NAMES_SHORTHAND\nfrom jwql.utils.utils import get_config, filename_parser\n\nFILESYSTEM_DIR = os.path.join(get_config()[\'jwql_dir\'], \'filesystem\')\n\n\nclass AnomalySubmitForm(forms.Form):\n    """"""A multiple choice field for specifying flagged anomalies.""""""\n\n    # Define anomaly choice field\n    anomaly_choices = forms.MultipleChoiceField(choices=ANOMALY_CHOICES, widget=forms.CheckboxSelectMultiple())\n\n    def update_anomaly_table(self, rootname, user, anomaly_choices):\n        """"""Updated the ``anomaly`` table of the database with flagged\n        anomaly information\n\n        Parameters\n        ----------\n        rootname : str\n            The rootname of the image to flag (e.g.\n            ``jw86600008001_02101_00001_guider2``)\n        user : str\n            The ``ezid`` of the authenticated user that is flagging the\n            anomaly\n        anomaly_choices : list\n            A list of anomalies that are to be flagged (e.g.\n            ``[\'snowball\', \'crosstalk\']``)\n        """"""\n\n        data_dict = {}\n        data_dict[\'rootname\'] = rootname\n        data_dict[\'flag_date\'] = datetime.datetime.now()\n        data_dict[\'user\'] = user\n        for choice in anomaly_choices:\n            data_dict[choice] = True\n        di.engine.execute(di.Anomaly.__table__.insert(), data_dict)\n\n\nclass FileSearchForm(forms.Form):\n    """"""Single-field form to search for a proposal or fileroot.""""""\n\n    # Define search field\n    search = forms.CharField(label=\'\', max_length=500, required=True,\n                             empty_value=\'Search\')\n\n    # Initialize attributes\n    fileroot_dict = None\n    search_type = None\n    instrument = None\n\n    def clean_search(self):\n        """"""Validate the ""search"" field.\n\n        Check that the input is either a proposal or fileroot, and one\n        that matches files in the filesystem.\n\n        Returns\n        -------\n        str\n            The cleaned data input into the ""search"" field\n\n        """"""\n        # Get the cleaned search data\n        search = self.cleaned_data[\'search\']\n\n        # Make sure the search is either a proposal or fileroot\n        if search.isnumeric() and 1 < int(search) < 99999:\n            self.search_type = \'proposal\'\n        elif self._search_is_fileroot(search):\n            self.search_type = \'fileroot\'\n        else:\n            raise forms.ValidationError(\'Invalid search term {}. Please provide proposal number \'\n                                        \'or file root.\'.format(search))\n\n        # If they searched for a proposal...\n        if self.search_type == \'proposal\':\n            # See if there are any matching proposals and, if so, what\n            # instrument they are for\n            proposal_string = \'{:05d}\'.format(int(search))\n            search_string = os.path.join(FILESYSTEM_DIR, \'jw{}\'.format(proposal_string),\n                                         \'*{}*.fits\'.format(proposal_string))\n            all_files = glob.glob(search_string)\n            if len(all_files) > 0:\n                all_instruments = []\n                for file in all_files:\n                    instrument = filename_parser(file)[\'instrument\']\n                    all_instruments.append(instrument)\n                if len(set(all_instruments)) > 1:\n                    raise forms.ValidationError(\'Cannot return result for proposal with multiple \'\n                                                \'instruments ({}).\'\n                                                .format(\', \'.join(set(all_instruments))))\n\n                self.instrument = all_instruments[0]\n            else:\n                raise forms.ValidationError(\'Proposal {} not in the filesystem.\'.format(search))\n\n        # If they searched for a fileroot...\n        elif self.search_type == \'fileroot\':\n            # See if there are any matching fileroots and, if so, what\n            # instrument they are for\n            search_string = os.path.join(FILESYSTEM_DIR, search[:7], \'{}*.fits\'.format(search))\n            all_files = glob.glob(search_string)\n\n            if len(all_files) == 0:\n                raise forms.ValidationError(\'Fileroot {} not in the filesystem.\'.format(search))\n\n            instrument = search.split(\'_\')[-1][:3]\n            self.instrument = JWST_INSTRUMENT_NAMES_SHORTHAND[instrument]\n\n        return self.cleaned_data[\'search\']\n\n    def _search_is_fileroot(self, search):\n        """"""Determine if a search value is formatted like a fileroot.\n\n        Parameters\n        ----------\n        search : str\n            The search term input by the user.\n\n        Returns\n        -------\n        bool\n            Is the search term formatted like a fileroot?\n\n        """"""\n        try:\n            self.fileroot_dict = filename_parser(search)\n            return True\n        except ValueError:\n            return False\n\n    def redirect_to_files(self):\n        """"""Determine where to redirect the web app based on user input.\n\n        Returns\n        -------\n        HttpResponseRedirect object\n            Outgoing redirect response sent to the webpage\n\n        """"""\n        # Process the data in form.cleaned_data as required\n        search = self.cleaned_data[\'search\']\n        proposal_string = \'{:05d}\'.format(int(search))\n\n        # If they searched for a proposal\n        if self.search_type == \'proposal\':\n            return redirect(\'/{}/archive/{}\'.format(self.instrument, proposal_string))\n\n        # If they searched for a file root\n        elif self.search_type == \'fileroot\':\n            return redirect(\'/{}/{}\'.format(self.instrument, search))\n\n\nclass MnemonicSearchForm(forms.Form):\n    """"""A single-field form to search for a mnemonic in the DMS EDB.""""""\n\n    # Define search field\n    search = forms.CharField(label=\'\', max_length=500, required=True,\n                             empty_value=\'Search\', initial=\'SA_ZFGOUTFOV\')\n\n    # Initialize attributes\n    search_type = None\n\n    def __init__(self, *args, **kwargs):\n        try:\n            self.logged_in = kwargs.pop(\'logged_in\')\n        except KeyError:\n            self.logged_in = True\n\n        super(MnemonicSearchForm, self).__init__(*args, **kwargs)\n\n    def clean_search(self):\n        """"""Validate the ""search"" field.\n\n        Check that the input is a valid mnemonic identifier.\n\n        Returns\n        -------\n        str\n            The cleaned data input into the ""search"" field\n\n        """"""\n        # Stop now if not logged in\n        if not self.logged_in:\n            raise forms.ValidationError(\'Could not log into MAST. Please login or provide MAST \'\n                                        \'token in environment variable or config.json.\')\n\n        # Get the cleaned search data\n        search = self.cleaned_data[\'search\']\n\n        # Make sure the search is a valid mnemonic identifier\n        if is_valid_mnemonic(search):\n            self.search_type = \'mnemonic\'\n        else:\n            raise forms.ValidationError(\'Invalid search term {}. Please enter a valid DMS EDB \'\n                                        \'mnemonic.\'.format(search))\n\n        return self.cleaned_data[\'search\']\n\n\nclass MnemonicQueryForm(forms.Form):\n    """"""A triple-field form to query mnemonic records in the DMS EDB.""""""\n\n    production_mode = False\n\n    if production_mode:\n        # times for default query (one day one week ago)\n        now = Time.now()\n        delta_day = -7.\n        range_day = 1.\n        default_start_time = now + TimeDelta(delta_day, format=\'jd\')\n        default_end_time = now + TimeDelta(delta_day + range_day, format=\'jd\')\n    else:\n        # example for testing\n        default_start_time = Time(\'2019-01-16 00:00:00.000\', format=\'iso\')\n        default_end_time = Time(\'2019-01-16 00:01:00.000\', format=\'iso\')\n\n    default_mnemonic_identifier = \'IMIR_HK_ICE_SEC_VOLT4\'\n\n    # Define search fields\n    search = forms.CharField(label=\'mnemonic\', max_length=500, required=True,\n                             initial=default_mnemonic_identifier, empty_value=\'Search\',\n                             help_text=""Mnemonic identifier"")\n\n    start_time = forms.CharField(label=\'start\', max_length=500, required=False,\n                                 initial=default_start_time.iso, help_text=""Start time"")\n\n    end_time = forms.CharField(label=\'end\', max_length=500, required=False,\n                               initial=default_end_time.iso, help_text=""End time"")\n\n    # Initialize attributes\n    search_type = None\n\n    def __init__(self, *args, **kwargs):\n        try:\n            self.logged_in = kwargs.pop(\'logged_in\')\n        except KeyError:\n            self.logged_in = True\n\n        super(MnemonicQueryForm, self).__init__(*args, **kwargs)\n\n    def clean_search(self):\n        """"""Validate the ""search"" field.\n\n        Check that the input is a valid mnemonic identifier.\n\n        Returns\n        -------\n        str\n            The cleaned data input into the ""search"" field\n\n        """"""\n        # Stop now if not logged in\n        if not self.logged_in:\n            raise forms.ValidationError(\'Could not log into MAST. Please login or provide MAST \'\n                                        \'token in environment variable or config.json.\')\n\n        # Get the cleaned search data\n        search = self.cleaned_data[\'search\']\n\n        if is_valid_mnemonic(search):\n            self.search_type = \'mnemonic\'\n        else:\n            raise forms.ValidationError(\'Invalid search term {}. Please enter a valid DMS EDB \'\n                                        \'mnemonic.\'.format(search))\n\n        return self.cleaned_data[\'search\']\n\n    def clean_start_time(self):\n        """"""Validate the start time.\n\n        Returns\n        -------\n        str\n           The cleaned data input into the start_time field\n\n        """"""\n        start_time = self.cleaned_data[\'start_time\']\n        try:\n            Time(start_time, format=\'iso\')\n        except ValueError:\n            raise forms.ValidationError(\'Invalid start time {}. Please enter a time in iso format, \'\n                                        \'e.g. {}\'.format(start_time, self.default_start_time))\n        return self.cleaned_data[\'start_time\']\n\n    def clean_end_time(self):\n        """"""Validate the end time.\n\n        Returns\n        -------\n        str\n           The cleaned data input into the end_time field\n\n        """"""\n        end_time = self.cleaned_data[\'end_time\']\n        try:\n            Time(end_time, format=\'iso\')\n        except ValueError:\n            raise forms.ValidationError(\'Invalid end time {}. Please enter a time in iso format, \'\n                                        \'e.g. {}.\'.format(end_time, self.default_end_time))\n\n        if \'start_time\' in self.cleaned_data.keys():\n            # verify that end_time is later than start_time\n            if self.cleaned_data[\'end_time\'] <= self.cleaned_data[\'start_time\']:\n                raise forms.ValidationError(\'Invalid time inputs. End time is required to be after\'\n                                            \' Start time.\')\n\n        return self.cleaned_data[\'end_time\']\n\n\nclass MnemonicExplorationForm(forms.Form):\n    """"""A sextuple-field form to explore the EDB mnemonic inventory.""""""\n\n    default_description = \'centroid data\'\n\n    # Define search fields\n    description = forms.CharField(label=\'description\', max_length=500, required=False,\n                                  initial=default_description, help_text=""Description"")\n    sql_data_type = forms.CharField(label=\'sqlDataType\', max_length=500, required=False,\n                                    help_text=""sqlDataType"")\n    subsystem = forms.CharField(label=\'subsystem\', max_length=500, required=False,\n                                help_text=""subsystem"")\n    tlm_identifier = forms.CharField(label=\'tlmIdentifier\', max_length=500, required=False,\n                                     help_text=""Numerical ID (tlmIdentifier)"")\n    tlm_mnemonic = forms.CharField(label=\'tlmMnemonic\', max_length=500, required=False,\n                                   help_text=""String ID (tlmMnemonic)"")\n    unit = forms.CharField(label=\'unit\', max_length=500, required=False,\n                           help_text=""unit"")\n'"
jwql/website/apps/jwql/models.py,0,"b'""""""Defines the models for the ``jwql`` app.\n\n** CURRENTLY NOT IN USE **\n\nIn Django, ""a model is the single, definitive source of information\nabout your data. It contains the essential fields and behaviors of the\ndata you\xe2\x80\x99re storing. Generally, each model maps to a single database\ntable"" (from Django documentation). Each model contains fields, such\nas character fields or date/time fields, that function like columns in\na data table. This module defines models that might be used to store\ndata related to the JWQL webpage. Interacts with the database located\nat jwql/website/db.sqlite3.\n\nAuthors\n-------\n    - Lauren Chambers\n\nUse\n---\n    This module is used as such:\n\n    ::\n        from models import MyModel\n        data = MyModel.objects.filter(name=""JWQL"")\n\nReferences\n----------\n    For more information please see:\n        ```https://docs.djangoproject.com/en/2.0/topics/db/models/```\n""""""\n\nimport os\n\nfrom django.db import models\n\n\nINSTRUMENT_LIST = ((\'FGS\', \'FGS\'),\n                   (\'MIRI\', \'MIRI\'),\n                   (\'NIRCam\', \'NIRCam\'),\n                   (\'NIRISS\', \'NIRISS\'),\n                   (\'NIRSpec\', \'NIRSpec\'))\n\n\nclass BaseModel(models.Model):\n    """"""A base model that other classes will inherit. Created to avoid\n    an obscure error about a missing ``app_label``.\n    """"""\n\n    class Meta:\n        abstract = True  # specify this model as an Abstract Model\n        app_label = \'jwql\'\n\n\nclass ImageData(BaseModel):\n    """"""A model that collects image filepaths, instrument labels, and\n    publishing date/time. Just an example used for learning django.\n\n    Attributes\n    ----------\n    filepath : FilePathField object\n        The full filepath of the datum\n    inst : CharField object\n        Name of the corresponding JWST instrument\n    pub_date : FilePathField object\n        Date and time when datum was added to the database.\n    """"""\n\n    inst = models.CharField(\'instrument\', max_length=7, choices=INSTRUMENT_LIST, default=None)\n    pub_date = models.DateTimeField(\'date published\')\n    filepath = models.FilePathField(path=\'/user/lchambers/jwql/\')\n\n    def filename(self):\n        return os.path.basename(self.filepath)\n\n    def __str__(self):\n        return self.filename()\n\n    class Meta:\n        verbose_name_plural = ""image data""\n        db_table = \'imagedata\'\n'"
jwql/website/apps/jwql/monitor_views.py,0,"b'""""""Defines the views for the ``jwql`` web app instrument monitors.\n\nAuthors\n-------\n\n    - Lauren Chambers\n\nUse\n---\n\n    This module is called in ``urls.py`` as such:\n    ::\n\n        from django.urls import path\n        from . import monitor_views\n        urlpatterns = [path(\'web/path/to/view/\', monitor_views.view_name,\n        name=\'view_name\')]\n\nReferences\n----------\n    For more information please see:\n        ``https://docs.djangoproject.com/en/2.0/topics/http/views/``\n\nDependencies\n------------\n    The user must have a configuration file named ``config.json``\n    placed in the ``jwql/utils/`` directory.\n""""""\n\nimport os\n\nfrom django.shortcuts import render\n\nfrom . import bokeh_containers\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES_MIXEDCASE\nfrom jwql.utils.utils import get_config\n\nFILESYSTEM_DIR = os.path.join(get_config()[\'jwql_dir\'], \'filesystem\')\n\n\ndef dark_monitor(request, inst):\n    """"""Generate the dark monitor page for a given instrument\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        Name of JWST instrument\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    # Ensure the instrument is correctly capitalized\n    inst = JWST_INSTRUMENT_NAMES_MIXEDCASE[inst.lower()]\n\n    # Deal with the fact that only the NIRCam database is populated\n    if inst == \'NIRCam\':\n        tabs_components = bokeh_containers.dark_monitor_tabs(inst)\n    else:\n        tabs_components = None\n\n    template = ""dark_monitor.html""\n\n    context = {\n        \'inst\': inst,\n        \'tabs_components\': tabs_components,\n    }\n\n    # Return a HTTP response with the template and dictionary of variables\n    return render(request, template, context)\n'"
jwql/website/apps/jwql/oauth.py,0,"b'""""""Provides an OAuth object for authentication of the ``jwql`` web app,\nas well as decorator functions to require user authentication in other\nviews of the web application.\n\n\nAuthors\n-------\n\n    - Matthew Bourque\n    - Christian Mesh\n\nUse\n---\n\n    This module is intended to be imported and used as such:\n    ::\n\n        from .oauth import auth_info\n        from .oauth import auth_required\n        from .oauth import JWQL_OAUTH\n\n        @auth_info\n        def some_view(request):\n            pass\n\n        @auth_required\n        def login(request):\n            pass\n\nReferences\n----------\n    Much of this code was taken from the ``authlib`` documentation,\n    found here: ``http://docs.authlib.org/en/latest/client/django.html``\n\nDependencies\n------------\n    The user must have a configuration file named ``config.json``\n    placed in the ``jwql/utils/`` directory.\n""""""\n\nimport os\nimport requests\n\nfrom authlib.django.client import OAuth\nfrom django.shortcuts import redirect, render\n\nimport jwql\nfrom jwql.utils.constants import MONITORS\nfrom jwql.utils.utils import get_base_url, get_config, check_config_for_key\n\nPREV_PAGE = \'/\'\n\n\ndef register_oauth():\n    """"""Register the ``jwql`` application with the ``auth.mast``\n    authentication service.\n\n    Returns\n    -------\n    oauth : Object\n        An object containing methods to authenticate a user, provided\n        by the ``auth.mast`` service.\n    """"""\n\n    # Get configuration parameters\n    for key in [\'client_id\', \'client_secret\', \'auth_mast\']:\n        check_config_for_key(key)\n    client_id = get_config()[\'client_id\']\n    client_secret = get_config()[\'client_secret\']\n    auth_mast = get_config()[\'auth_mast\']\n\n    # Register with auth.mast\n    oauth = OAuth()\n    client_kwargs = {\'scope\': \'mast:user:info\'}\n    oauth.register(\n        \'mast_auth\',\n        client_id=\'{}\'.format(client_id),\n        client_secret=\'{}\'.format(client_secret),\n        access_token_url=\'https://{}/oauth/access_token?client_secret={}\'.format(\n            auth_mast, client_secret\n        ),\n        access_token_params=None,\n        refresh_token_url=None,\n        authorize_url=\'https://{}/oauth/authorize\'.format(auth_mast),\n        api_base_url=\'https://{}/1.1/\'.format(auth_mast),\n        client_kwargs=client_kwargs)\n\n    return oauth\n\n\nJWQL_OAUTH = register_oauth()\n\n\ndef authorize(request):\n    """"""Spawn the authentication process for the user\n\n    The authentication process involves retreiving an access token\n    from ``auth.mast`` and porting the data to a cookie.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    # Get auth.mast token\n    token = JWQL_OAUTH.mast_auth.authorize_access_token(\n        request, headers={\'Accept\': \'application/json\'}\n    )\n\n    # Determine domain\n    base_url = get_base_url()\n    if \'127\' in base_url:\n        domain = \'127.0.0.1\'\n    else:\n        domain = base_url.split(\'//\')[-1]\n\n    # Set secure cookie parameters\n    cookie_args = {}\n    # cookie_args[\'domain\'] = domain  # Currently broken\n    # cookie_args[\'secure\'] = True  # Currently broken\n    cookie_args[\'httponly\'] = True\n\n    # Set the cookie\n    response = redirect(PREV_PAGE)\n    response.set_cookie(""ASB-AUTH"", token[""access_token""], **cookie_args)\n\n    return response\n\n\ndef auth_info(fn):\n    """"""A decorator function that will return user credentials along\n    with what is returned by the original function.\n\n    Parameters\n    ----------\n    fn : function\n        The function to decorate\n\n    Returns\n    -------\n    user_info : function\n        The decorated function\n    """"""\n\n    def user_info(request, **kwargs):\n        """"""Store authenticated user credentials in a cookie and return\n        it.  If the user is not authenticated, store no credentials in\n        the cookie.\n\n        Parameters\n        ----------\n        request : HttpRequest object\n            Incoming request from the webpage\n\n        Returns\n        -------\n        fn : function\n            The decorated function\n        """"""\n\n        cookie = request.COOKIES.get(""ASB-AUTH"")\n\n        # If user is authenticated, return user credentials\n        if cookie is not None:\n            check_config_for_key(\'auth_mast\')\n            # Note: for now, this must be the development version\n            auth_mast = get_config()[\'auth_mast\']\n\n            response = requests.get(\n                \'https://{}/info\'.format(auth_mast),\n                headers={\'Accept\': \'application/json\',\n                         \'Authorization\': \'token {}\'.format(cookie)})\n            response = response.json()\n            response[\'access_token\'] = cookie\n\n        # If user is not authenticated, return no credentials\n        else:\n            response = {\'ezid\': None, ""anon"": True, \'access_token\': None}\n\n        return fn(request, response, **kwargs)\n\n    return user_info\n\n\ndef auth_required(fn):\n    """"""A decorator function that requires the given function to have\n    authentication through ``auth.mast`` set up.\n\n    Parameters\n    ----------\n    fn : function\n        The function to decorate\n\n    Returns\n    -------\n    check_auth : function\n        The decorated function\n    """"""\n\n    @auth_info\n    def check_auth(request, user, **kwargs):\n        """"""Check if the user is authenticated through ``auth.mast``.\n        If not, perform the authorization.\n\n        Parameters\n        ----------\n        request : HttpRequest object\n            Incoming request from the webpage\n        user : dict\n            A dictionary of user credentials\n\n        Returns\n        -------\n        fn : function\n            The decorated function\n        """"""\n\n        # If user is currently anonymous, require a login\n        if user[\'ezid\']:\n\n            return fn(request, user, **kwargs)\n\n        else:\n            template = \'not_authenticated.html\'\n            context = {\'inst\': \'\'}\n\n            return render(request, template, context)\n\n    return check_auth\n\n\n@auth_info\ndef login(request, user):\n    """"""Spawn a login process for the user\n\n    The ``auth_requred`` decorator is used to require that the user\n    authenticate through ``auth.mast``, then the user is redirected\n    back to the homepage.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    user : dict\n        A dictionary of user credentials.\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    # Redirect to oauth login\n    global PREV_PAGE\n    PREV_PAGE = request.META.get(\'HTTP_REFERER\')\n    redirect_uri = os.path.join(get_base_url(), \'authorize\')\n\n    return JWQL_OAUTH.mast_auth.authorize_redirect(request, redirect_uri)\n\n\ndef logout(request):\n    """"""Spawn a logout process for the user\n\n    Upon logout, the user\'s ``auth.mast`` credientials are removed and\n    the user is redirected back to the homepage.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    user : dict\n        A dictionary of user credentials.\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    global PREV_PAGE\n    PREV_PAGE = request.META.get(\'HTTP_REFERER\')\n    response = redirect(PREV_PAGE)\n    response.delete_cookie(""ASB-AUTH"")\n\n    return response\n'"
jwql/website/apps/jwql/urls.py,0,"b'""""""Maps URL paths to views in the ``jwql`` app.\n\nThis module connects requested URL paths to the corresponding view in\n``views.py`` for each webpage in the ``jwql`` app. When django is\nprovided a path, it searches through the ``urlpatterns`` list provided\nhere until it finds one that matches. It then calls the assigned view\nto load the appropriate webpage, passing an ``HttpRequest`` object.\n\nAuthors\n-------\n\n    - Lauren Chambers\n    - Matthew Bourque\n    - Johannes Sahlmann\n\nUse\n---\n\n    Function views\n        1. Add an import:  from my_app import views\n        2. Add a URL to urlpatterns:  path(\'\', views.home, name=\'home\')\n    Class-based views\n        1. Add an import:  from other_app.views import Home\n        2. Add a URL to urlpatterns:  path(\'\', Home.as_view(), name=\'home\')\n    Including another URLconf\n        1. Import the include() function: from django.urls import include, path\n        2. Add a URL to urlpatterns:  path(\'blog/\', include(\'blog.urls\'))\n\nReferences\n----------\n    For more information please see:\n        ``https://docs.djangoproject.com/en/2.0/topics/http/urls/``\n\nNotes\n-----\n    Be aware that when a url is requested, it will be directed to the\n    first matching path in the ``urlpatterns`` list that it finds. The\n    ``<str:var>`` tag is just a placeholder. To avoid complications,\n    users should order their paths in order from shortest to longest,\n    and after that from most to least specific.\n""""""\n\nfrom django.urls import path\nfrom django.urls import re_path\n\nfrom . import api_views\nfrom . import monitor_views\nfrom . import oauth\nfrom . import views\n\napp_name = \'jwql\'\ninstruments = \'nircam|NIRCam|niriss|NIRISS|nirspec|NIRSpec|miri|MIRI|fgs|FGS\'\n\nurlpatterns = [\n\n    # Home\n    path(\'\', views.home, name=\'home\'),\n\n    # Authentication\n    path(\'login/\', oauth.login, name=\'login\'),\n    path(\'logout/\', oauth.logout, name=\'logout\'),\n    path(\'authorize/\', oauth.authorize, name=\'authorize\'),\n\n    # MIRI-specific views\n    path(\'miri/miri_data_trending/\', views.miri_data_trending, name=\'miri_data_trending\'),\n\n    # NIRSpec-specific views\n    path(\'nirspec/nirspec_data_trending/\', views.nirspec_data_trending, name=\'nirspec_data_trending\'),\n\n    # Common monitor views\n    re_path(r\'^(?P<inst>({}))/.+_monitor/$\'.format(instruments), monitor_views.dark_monitor, name=\'dark_monitor\'),\n\n    # Main site views\n    path(\'about/\', views.about, name=\'about\'),\n    path(\'dashboard/\', views.dashboard, name=\'dashboard\'),\n    path(\'edb/\', views.engineering_database, name=\'edb\'),\n    re_path(r\'^(?P<inst>({}))/$\'.format(instruments), views.instrument, name=\'instrument\'),\n    re_path(r\'^(?P<inst>({}))/archive/$\'.format(instruments), views.archived_proposals, name=\'archive\'),\n    re_path(r\'^(?P<inst>({}))/unlooked/$\'.format(instruments), views.unlooked_images, name=\'unlooked\'),\n    re_path(r\'^(?P<inst>({}))/(?P<file_root>[\\w]+)/$\'.format(instruments), views.view_image, name=\'view_image\'),\n    re_path(r\'^(?P<inst>({}))/(?P<filename>.+)/header/$\'.format(instruments), views.view_header, name=\'view_header\'),\n    re_path(r\'^(?P<inst>({}))/archive/(?P<proposal>[\\d]{{1,5}})/$\'.format(instruments), views.archive_thumbnails, name=\'archive_thumb\'),\n\n    # AJAX views\n    re_path(r\'^ajax/(?P<inst>({}))/archive/$\'.format(instruments), views.archived_proposals_ajax, name=\'archive_ajax\'),\n    re_path(r\'^ajax/(?P<inst>({}))/archive/(?P<proposal>[\\d]{{1,5}})/$\'.format(instruments), views.archive_thumbnails_ajax, name=\'archive_thumb_ajax\'),\n\n    # REST API views\n    path(\'api/proposals/\', api_views.all_proposals, name=\'all_proposals\'),\n    re_path(r\'^api/(?P<inst>({}))/proposals/$\'.format(instruments), api_views.instrument_proposals, name=\'instrument_proposals\'),\n    re_path(r\'^api/(?P<inst>({}))/preview_images/$\'.format(instruments), api_views.preview_images_by_instrument, name=\'preview_images_by_instrument\'),\n    re_path(r\'^api/(?P<inst>({}))/thumbnails/$\'.format(instruments), api_views.thumbnails_by_instrument, name=\'thumbnails_by_instrument\'),\n    re_path(r\'^api/(?P<proposal>[\\d]{1,5})/filenames/$\', api_views.filenames_by_proposal, name=\'filenames_by_proposal\'),\n    re_path(r\'^api/(?P<proposal>[\\d]{1,5})/preview_images/$\', api_views.preview_images_by_proposal, name=\'preview_images_by_proposal\'),\n    re_path(r\'^api/(?P<proposal>[\\d]{1,5})/thumbnails/$\', api_views.thumbnails_by_proposal, name=\'preview_images_by_proposal\'),\n    re_path(r\'^api/(?P<rootname>[\\w]+)/filenames/$\', api_views.filenames_by_rootname, name=\'filenames_by_rootname\'),\n    re_path(r\'^api/(?P<rootname>[\\w]+)/preview_images/$\', api_views.preview_images_by_rootname, name=\'preview_images_by_rootname\'),\n    re_path(r\'^api/(?P<rootname>[\\w]+)/thumbnails/$\', api_views.thumbnails_by_rootname, name=\'thumbnails_by_rootname\'),\n]\n'"
jwql/website/apps/jwql/views.py,0,"b'""""""Defines the views for the ``jwql`` web app.\n\nIn Django, ""a view function, or view for short, is simply a Python\nfunction that takes a Web request and returns a Web response"" (from\nDjango documentation). This module defines all of the views that are\nused to generate the various webpages used for the JWQL application.\nFor example, these views can list the tools available to users, query\nthe ``jwql`` database, and display images and headers.\n\nAuthors\n-------\n\n    - Lauren Chambers\n    - Johannes Sahlmann\n\nUse\n---\n\n    This module is called in ``urls.py`` as such:\n    ::\n\n        from django.urls import path\n        from . import views\n        urlpatterns = [path(\'web/path/to/view/\', views.view_name,\n        name=\'view_name\')]\n\nReferences\n----------\n    For more information please see:\n        ``https://docs.djangoproject.com/en/2.0/topics/http/views/``\n\nDependencies\n------------\n    The user must have a configuration file named ``config.json``\n    placed in the ``jwql/utils/`` directory.\n""""""\n\nimport datetime\nimport os\n\nfrom django.http import JsonResponse\nfrom django.shortcuts import render\n\nfrom .data_containers import get_acknowledgements, get_edb_components\nfrom .data_containers import get_dashboard_components\nfrom .data_containers import get_filenames_by_instrument\nfrom .data_containers import get_header_info\nfrom .data_containers import get_image_info\nfrom .data_containers import get_current_flagged_anomalies\nfrom .data_containers import get_proposal_info\nfrom .data_containers import random_404_page\nfrom .data_containers import thumbnails_ajax\nfrom .data_containers import data_trending\nfrom .data_containers import nirspec_trending\nfrom .forms import AnomalySubmitForm, FileSearchForm\nfrom .oauth import auth_info, auth_required\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES, MONITORS, JWST_INSTRUMENT_NAMES_MIXEDCASE\nfrom jwql.utils.utils import get_base_url, get_config\n\nFILESYSTEM_DIR = os.path.join(get_config()[\'jwql_dir\'], \'filesystem\')\n\n\ndef miri_data_trending(request):\n    """"""Generate the ``MIRI DATA-TRENDING`` page\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    template = ""miri_data_trending.html""\n    variables, dash = data_trending()\n\n    context = {\n        \'dashboard\': dash,\n        \'inst\': \'\',  # Leave as empty string or instrument name; Required for navigation bar\n        \'inst_list\': JWST_INSTRUMENT_NAMES_MIXEDCASE,  # Do not edit; Required for navigation bar\n        \'tools\': MONITORS,  # Do not edit; Required for navigation bar\n        \'user\': None  # Do not edit; Required for authentication\n    }\n\n    # append variables to context\n    context.update(variables)\n\n    # Return a HTTP response with the template and dictionary of variables\n    return render(request, template, context)\n\n\ndef nirspec_data_trending(request):\n    """"""Generate the ``MIRI DATA-TRENDING`` page\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    template = ""nirspec_data_trending.html""\n    variables, dash = nirspec_trending()\n\n    context = {\n        \'dashboard\': dash,\n        \'inst\': \'\',  # Leave as empty string or instrument name; Required for navigation bar\n        \'inst_list\': JWST_INSTRUMENT_NAMES_MIXEDCASE,  # Do not edit; Required for navigation bar\n        \'tools\': MONITORS,  # Do not edit; Required for navigation bar\n        \'user\': None  # Do not edit; Required for authentication\n    }\n\n    # append variables to context\n    context.update(variables)\n\n    # Return a HTTP response with the template and dictionary of variables\n    return render(request, template, context)\n\n\ndef about(request):\n    """"""Generate the ``about`` page\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n    template = \'about.html\'\n    acknowledgements = get_acknowledgements()\n    context = {\'acknowledgements\': acknowledgements,\n               \'inst\': \'\'}\n\n    return render(request, template, context)\n\n\n@auth_required\ndef archived_proposals(request, user, inst):\n    """"""Generate the page listing all archived proposals in the database\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        Name of JWST instrument\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n    # Ensure the instrument is correctly capitalized\n    inst = JWST_INSTRUMENT_NAMES_MIXEDCASE[inst.lower()]\n\n    template = \'archive.html\'\n    context = {\'inst\': inst,\n               \'base_url\': get_base_url()}\n\n    return render(request, template, context)\n\n\n@auth_required\ndef archived_proposals_ajax(request, user, inst):\n    """"""Generate the page listing all archived proposals in the database\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        Name of JWST instrument\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n    # Ensure the instrument is correctly capitalized\n    inst = JWST_INSTRUMENT_NAMES_MIXEDCASE[inst.lower()]\n\n    # For each proposal, get the first available thumbnail and determine\n    # how many files there are\n    filepaths = get_filenames_by_instrument(inst)\n    all_filenames = [os.path.basename(f) for f in filepaths]\n    proposal_info = get_proposal_info(filepaths)\n\n    context = {\'inst\': inst,\n               \'all_filenames\': all_filenames,\n               \'num_proposals\': proposal_info[\'num_proposals\'],\n               \'thumbnails\': {\'proposals\': proposal_info[\'proposals\'],\n                              \'thumbnail_paths\': proposal_info[\'thumbnail_paths\'],\n                              \'num_files\': proposal_info[\'num_files\']}}\n\n    return JsonResponse(context, json_dumps_params={\'indent\': 2})\n\n\n@auth_required\ndef archive_thumbnails(request, user, inst, proposal):\n    """"""Generate the page listing all archived images in the database\n    for a certain proposal\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        Name of JWST instrument\n    proposal : str\n        Number of observing proposal\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    # Ensure the instrument is correctly capitalized\n    inst = JWST_INSTRUMENT_NAMES_MIXEDCASE[inst.lower()]\n\n    template = \'thumbnails.html\'\n    context = {\'inst\': inst,\n               \'prop\': proposal,\n               \'base_url\': get_base_url()}\n\n    return render(request, template, context)\n\n\n@auth_required\ndef archive_thumbnails_ajax(request, user, inst, proposal):\n    """"""Generate the page listing all archived images in the database\n    for a certain proposal\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        Name of JWST instrument\n    proposal : str\n        Number of observing proposal\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    # Ensure the instrument is correctly capitalized\n    inst = JWST_INSTRUMENT_NAMES_MIXEDCASE[inst.lower()]\n\n    data = thumbnails_ajax(inst, proposal)\n\n    return JsonResponse(data, json_dumps_params={\'indent\': 2})\n\n\ndef dashboard(request):\n    """"""Generate the dashbaord page\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n    template = \'dashboard.html\'\n    output_dir = get_config()[\'outputs\']\n    dashboard_components, dashboard_html = get_dashboard_components()\n\n    context = {\'inst\': \'\',\n               \'outputs\': output_dir,\n               \'filesystem_html\': os.path.join(output_dir, \'monitor_filesystem\',\n                                               \'filesystem_monitor.html\'),\n               \'dashboard_components\': dashboard_components,\n               \'dashboard_html\': dashboard_html}\n\n    return render(request, template, context)\n\n\n@auth_info\ndef engineering_database(request, user):\n    """"""Generate the EDB page.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    user : dict\n        A dictionary of user credentials.\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n\n    """"""\n    edb_components = get_edb_components(request)\n\n    template = \'engineering_database.html\'\n    context = {\'inst\': \'\',\n               \'edb_components\': edb_components}\n\n    return render(request, template, context)\n\n\ndef home(request):\n    """"""Generate the home page\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    user : dict\n        A dictionary of user credentials.\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    # Create a form instance and populate it with data from the request\n    form = FileSearchForm(request.POST or None)\n\n    # If this is a POST request, we need to process the form data\n    if request.method == \'POST\':\n        if form.is_valid():\n            return form.redirect_to_files()\n\n    template = \'home.html\'\n    context = {\'inst\': \'\',\n               \'form\': form}\n\n    return render(request, template, context)\n\n\ndef instrument(request, inst):\n    """"""Generate the instrument tool index page.\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        Name of JWST instrument\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n    # Ensure the instrument is correctly capitalized\n    inst = JWST_INSTRUMENT_NAMES_MIXEDCASE[inst.lower()]\n\n    template = \'instrument.html\'\n    url_dict = {\'fgs\': \'http://jwst-docs.stsci.edu/display/JTI/Fine+Guidance+Sensor%2C+FGS?q=fgs\',\n                \'miri\': \'http://jwst-docs.stsci.edu/display/JTI/Mid+Infrared+Instrument\',\n                \'niriss\': \'http://jwst-docs.stsci.edu/display/JTI/Near+Infrared+Imager+and+Slitless+Spectrograph\',\n                \'nirspec\': \'http://jwst-docs.stsci.edu/display/JTI/Near+Infrared+Spectrograph\',\n                \'nircam\': \'http://jwst-docs.stsci.edu/display/JTI/Near+Infrared+Camera\'}\n\n    doc_url = url_dict[inst.lower()]\n\n    context = {\'inst\': inst,\n               \'doc_url\': doc_url}\n\n    return render(request, template, context)\n\n\ndef not_found(request, *kwargs):\n    """"""Generate a ``not_found`` page\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n    template = random_404_page()\n    status_code = 404  # Note that this will show 400, 403, 404, and 500 as 404 status\n    context = {\'inst\': \'\'}\n\n    return render(request, template, context, status=status_code)\n\n\ndef unlooked_images(request, inst):\n    """"""Generate the page listing all unlooked images in the database\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        Name of JWST instrument\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    pass\n\n\ndef view_header(request, inst, filename):\n    """"""Generate the header view page\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    inst : str\n        Name of JWST instrument\n    filename : str\n        FITS filename of selected image in filesystem\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n    # Ensure the instrument is correctly capitalized\n    inst = JWST_INSTRUMENT_NAMES_MIXEDCASE[inst.lower()]\n\n    template = \'view_header.html\'\n    file_root = \'_\'.join(filename.split(\'_\')[:-1])\n\n    context = {\'inst\': inst,\n               \'filename\': filename,\n               \'file_root\': file_root,\n               \'header_info\': get_header_info(filename)}\n\n    return render(request, template, context)\n\n\n@auth_required\ndef view_image(request, user, inst, file_root, rewrite=False):\n    """"""Generate the image view page\n\n    Parameters\n    ----------\n    request : HttpRequest object\n        Incoming request from the webpage\n    user : dict\n        A dictionary of user credentials.\n    inst : str\n        Name of JWST instrument\n    file_root : str\n        FITS filename of selected image in filesystem\n    rewrite : bool, optional\n        Regenerate the jpg preview of `file` if it already exists?\n\n    Returns\n    -------\n    HttpResponse object\n        Outgoing response sent to the webpage\n    """"""\n\n    # Ensure the instrument is correctly capitalized\n    inst = JWST_INSTRUMENT_NAMES_MIXEDCASE[inst.lower()]\n\n    template = \'view_image.html\'\n    image_info = get_image_info(file_root, rewrite)\n\n    # Determine current flagged anomalies\n    current_anomalies = get_current_flagged_anomalies(file_root, inst)\n\n    # Create a form instance\n    form = AnomalySubmitForm(request.POST or None, initial={\'anomaly_choices\': current_anomalies})\n\n    # If this is a POST request, process the form data\n    if request.method == \'POST\':\n        anomaly_choices = dict(request.POST)[\'anomaly_choices\']\n        if form.is_valid():\n            form.update_anomaly_table(file_root, user[\'ezid\'], anomaly_choices)\n\n    # Build the context\n    context = {\'inst\': inst,\n               \'prop_id\': file_root[2:7],\n               \'file_root\': file_root,\n               \'jpg_files\': image_info[\'all_jpegs\'],\n               \'fits_files\': image_info[\'all_files\'],\n               \'suffixes\': image_info[\'suffixes\'],\n               \'num_ints\': image_info[\'num_ints\'],\n               \'form\': form}\n\n    return render(request, template, context)\n'"
jwql/instrument_monitors/miri_monitors/data_trending/plots/__init__.py,0,b''
jwql/instrument_monitors/miri_monitors/data_trending/plots/bias_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for BIAS tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1:\n    IGDP_MIR_IC_V_VDETCOM\n    IGDP_MIR_SW_V_VDETCOM\n    IGDP_MIR_LW_V_VDETCOM\n\n    Plot 2:\n    IGDP_MIR_IC_V_VSSOUT\n    IGDP_MIR_SW_V_VSSOUT\n    IGDP_MIR_LW_V_VSSOUT\n\n    Plot 3:\n    IGDP_MIR_IC_V_VRSTOFF\n    IGDP_MIR_SW_V_VRSTOFF\n    IGDP_MIR_LW_V_VRSTOFF\n\n    Plot 4:\n    IGDP_MIR_IC_V_VP\n    IGDP_MIR_SW_V_VP\n    IGDP_MIR_LW_V_VP\n\n    Plot 5\n    IGDP_MIR_IC_V_VDDUC\n    IGDP_MIR_SW_V_VDDUC\n    IGDP_MIR_LW_V_VDDUC\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``dashborad.py``, e.g.:\n\n    ::\n        from .plots.bias_tab import bias_plots\n        tab = bias_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""miri_database.db""\n\n""""""\n\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.miri_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.layouts import gridplot, Column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef vdetcom(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""VDETCOM""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""VDETCOM IC"", ""IGDP_MIR_IC_V_VDETCOM"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""VDETCOM SW"", ""IGDP_MIR_SW_V_VDETCOM"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""VDETCOM LW"", ""IGDP_MIR_LW_V_VDETCOM"", start, end, conn, color = ""green"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef vssout(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""VSSOUT""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""VSSOUT IC"", ""IGDP_MIR_IC_V_VSSOUT"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""VSSOUT SW"", ""IGDP_MIR_SW_V_VSSOUT"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""VSSOUT LW"", ""IGDP_MIR_LW_V_VSSOUT"", start, end, conn, color = ""green"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef vrstoff(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""VRSTOFF""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""VRSTOFF IC"", ""IGDP_MIR_IC_V_VRSTOFF"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""VRSTOFF SW"", ""IGDP_MIR_SW_V_VRSTOFF"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""VRSTOFF LW"", ""IGDP_MIR_LW_V_VRSTOFF"", start, end, conn, color = ""green"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef vp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""VP""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""VP IC"", ""IGDP_MIR_IC_V_VP"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""VP SW"", ""IGDP_MIR_SW_V_VP"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""VP LW"", ""IGDP_MIR_LW_V_VP"", start, end, conn, color = ""green"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef vdduc(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""VDDUC""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""VDDUC IC"", ""IGDP_MIR_IC_V_VDDUC"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""VDDUC SW"", ""IGDP_MIR_SW_V_VDDUC"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""VDDUC LW"", ""IGDP_MIR_LW_V_VDDUC"", start, end, conn, color = ""green"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef bias_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>VSSOUT</td>\n        <td>IGDP_MIR_IC_V_VSSOUT<br>\n            IGDP_MIR_SW_V_VSSOUT<br>\n            IGDP_MIR_LW_V_VSSOUT<br> </td>\n        <td>Detector Bias VSSOUT (IC,SW, & LW)</td>\n      </tr>\n      <tr>\n        <td>VDETCOM</td>\n        <td>IGDP_MIR_IC_V_VDETCOM<br>\n            IGDP_MIR_SW_V_VDETCOM<br>\n            IGDP_MIR_LW_V_VDETCOM<br> </td>\n        <td>Detector Bias VDETCOM (IC,SW, & LW)</td>\n      </tr>\n      <tr>\n        <td>VRSTOFF</td>\n        <td>IGDP_MIR_IC_V_VRSTOFF<br>\n            IGDP_MIR_SW_V_VRSTOFF<br>\n            IGDP_MIR_LW_V_VRSTOFF<br> </td>\n        <td>Detector Bias VRSTOFF (IC,SW, & LW)</td>\n      </tr>\n      <tr>\n        <td>VP</td>\n        <td>IGDP_MIR_IC_V_VP<br>\n            IGDP_MIR_SW_V_VP<br>\n            IGDP_MIR_LW_V_VP<br> </td>\n        <td>Detector Bias VP (IC,SW, & LW)</td>\n      </tr>\n      <tr>\n        <td>VDDUC</td>\n        <td>IGDP_MIR_IC_V_VDDUC<br>\n            IGDP_MIR_SW_V_VDDUC<br>\n            IGDP_MIR_LW_V_VDDUC<br> </td>\n        <td>Detector Bias VDDUC (IC,SW, & LW)</td>\n      </tr>\n\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = vdetcom(conn, start, end)\n    plot2 = vssout(conn, start, end)\n    plot3 = vrstoff(conn, start, end)\n    plot4 = vp(conn, start, end)\n    plot5 = vdduc(conn, start, end)\n\n    l = gridplot([ [plot2, plot1],              \\\n                        [plot3, plot4],         \\\n                        [plot5, None]], merge_tools=False)\n\n    layout = Column(descr, l)\n\n    tab = Panel(child = layout, title = ""BIAS"")\n\n    return tab\n'"
jwql/instrument_monitors/miri_monitors/data_trending/plots/fpe_voltage_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for FPE VOLTAGE tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1:\n    IMIR_PDU_V_DIG_5V\n    IMIR_PDU_I_DIG_5V\n\n    Plot 2:\n    IMIR_PDU_V_ANA_5V\n    IMIR_PDU_I_ANA_5V\n\n    Plot 3:\n    IMIR_PDU_V_ANA_N5V\n    IMIR_PDU_I_ANA_N5V\n\n    Plot 4:\n    IMIR_PDU_V_ANA_7V\n    IMIR_PDU_I_ANA_7V\n\n    Plot 5:\n    IMIR_PDU_V_ANA_N7V\n    IMIR_PDU_I_ANA_N7V\n\n    Plot 6:\n    IMIR_SPW_V_DIG_2R5V\n    IMIR_PDU_V_REF_2R5V\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``dashborad.py``, e.g.:\n\n    ::\n        from .plots.fpe_voltage_tab import fpe_plots\n        tab = fpe_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""miri_database.db""\n\n""""""\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.miri_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.models import LinearAxis, Range1d\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.layouts import WidgetBox, gridplot, Column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef dig5(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                y_range = [4.9,5.1],\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPE Dig. 5V""\n    pf.add_basic_layout(p)\n\n    p.extra_y_ranges = {""current"": Range1d(start=2100, end=2500)}\n    a = pf.add_to_plot(p, ""FPE Dig. 5V"", ""IMIR_PDU_V_DIG_5V"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""FPE Dig. 5V Current"", ""IMIR_PDU_I_DIG_5V"", start, end, conn, y_axis = ""current"", color = ""blue"")\n    p.add_layout(LinearAxis(y_range_name = ""current"", axis_label = ""Current (mA)"", axis_label_text_color = ""blue""), \'right\')\n\n    pf.add_hover_tool(p,[a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef refdig(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [2.45,2.55],                              \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""2.5V Ref and FPE Dig.""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""FPE Dig. 2.5V"", ""IMIR_SPW_V_DIG_2R5V"", start, end, conn, color = ""orange"")\n    b = pf.add_to_plot(p, ""FPE PDU 2.5V REF"", ""IMIR_PDU_V_REF_2R5V"", start, end, conn, color = ""red"")\n\n    pf.add_hover_tool(p,[a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef ana5(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [4.95,5.05],                              \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPE Ana. 5V""\n    pf.add_basic_layout(p)\n\n    p.extra_y_ranges = {""current"": Range1d(start=100, end=250)}\n    a = pf.add_to_plot(p, ""FPE Ana. 5V"", ""IMIR_PDU_V_ANA_5V"",start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""FPE Ana. 5V Current"", ""IMIR_PDU_I_ANA_5V"",start, end, conn, y_axis = ""current"", color = ""blue"")\n    p.add_layout(LinearAxis(y_range_name = ""current"", axis_label = ""Current (mA)"", axis_label_text_color = ""blue""), \'right\')\n\n    pf.add_hover_tool(p,[a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef ana5n(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [-5.1,-4.85],                             \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPE Ana. N5V""\n    pf.add_basic_layout(p)\n\n    p.extra_y_ranges = {""current"": Range1d(start=100, end=300)}\n    a = pf.add_to_plot(p, ""FPE Ana. N5"", ""IMIR_PDU_V_ANA_N5V"",start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""FPE Ana. N5 Current"", ""IMIR_PDU_I_ANA_N5V"",start, end, conn, y_axis = ""current"", color = ""blue"")\n    p.add_layout(LinearAxis(y_range_name = ""current"", axis_label = ""Current (mA)"", axis_label_text_color = ""blue""), \'right\')\n\n    pf.add_hover_tool(p,[a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef ana7(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [6.85, 7.1],                              \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPE Ana. 7V""\n    pf.add_basic_layout(p)\n\n    p.extra_y_ranges = {""current"": Range1d(start=300, end=450)}\n    a = pf.add_to_plot(p, ""FPE Ana. 7V"", ""IMIR_PDU_V_ANA_7V"",start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""FPE Ana. 7V Current"", ""IMIR_PDU_I_ANA_7V"",start, end, conn, y_axis = ""current"", color = ""blue"")\n    p.add_layout(LinearAxis(y_range_name = ""current"", axis_label = ""Current (mA)"", axis_label_text_color = ""blue""), \'right\')\n\n    pf.add_hover_tool(p,[a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef ana7n(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 560,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [-7.1, -6.9],                             \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPE Ana. N7V""\n    pf.add_basic_layout(p)\n\n    p.extra_y_ranges = {""current"": Range1d(start=350, end=400)}\n    a = pf.add_to_plot(p, ""FPE Dig. N7V"", ""IMIR_PDU_V_ANA_N7V"",start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""FPE Ana. N7V Current"", ""IMIR_PDU_I_ANA_N7V"",start, end, conn, y_axis = ""current"", color = ""blue"")\n    p.add_layout(LinearAxis(y_range_name = ""current"", axis_label = ""Current (mA)"", axis_label_text_color = ""blue""), \'right\')\n\n    pf.add_hover_tool(p,[a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\ndef fpe_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>2.5V Ref and FPE Digg</td>\n        <td>IMIR_SPW_V_DIG_2R5V<br>\n            IMIR_PDU_V_REF_2R5V<br> </td>\n        <td>FPE 2.5V Digital and FPE 2.5V PDU Reference Voltage</td>\n      </tr>\n      <tr>\n        <td>FPE Dig. 5V</td>\n        <td>IMIR_PDU_V_DIG_5V<br>\n            IMIR_PDU_I_DIG_5V</td>\n        <td>FPE 5V Digital Voltage and Current</td>\n      </tr>\n      <tr>\n        <td>FPE Ana. 5V</td>\n        <td>IMIR_PDU_V_ANA_5V<br>\n            IMIR_PDU_I_ANA_5V</td>\n        <td>FPE +5V Analog Voltage and Current</td>\n      </tr>\n      <tr>\n        <td>FPE Ana. N5V</td>\n        <td>IMIR_PDU_V_ANA_N5V<br>\n            IMIR_PDU_I_ANA_N5V</td>\n        <td>FPE -5V Analog Voltage and Current</td>\n      </tr>\n      <tr>\n        <td>FPE Ana. 7V</td>\n        <td>IMIR_PDU_V_ANA_7V<br>\n            IMIR_PDU_I_ANA_7V</td>\n        <td>FPE +7V Analog Voltage and Current</td>\n      </tr>\n       <tr>\n         <td>FPE Ana. N7V</td>\n         <td>IMIR_PDU_V_ANA_N7V<br>\n             IMIR_PDU_I_ANA_N7V</td>\n         <td>FPE -7V Analog Voltage and Current</td>\n       </tr>\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = dig5(conn, start, end)\n    plot2 = refdig(conn, start, end)\n    plot3 = ana5(conn, start, end)\n    plot4 = ana5n(conn, start, end)\n    plot5 = ana7(conn, start, end)\n    plot6 = ana7n(conn, start, end)\n\n    l = gridplot([  [plot2, plot1],        \\\n                    [plot3, plot4],        \\\n                    [plot5, plot6]], merge_tools=False)\n\n    layout = Column(descr, l)\n\n    tab = Panel(child = layout, title = ""FPE VOLTAGE/CURRENT"")\n\n    return tab\n'"
jwql/instrument_monitors/miri_monitors/data_trending/plots/ice_voltage_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for ICE VOLTAGE tab\n\n    Module prepares plots for mnemonics below, combines plots in a grid and\n    returns tab object.\n\n    Plot 1:\n    IMIR_HK_ICE_SEC_VOLT1\n    IMIR_HK_ICE_SEC_VOLT3\n\n    Plot 2:\n    IMIR_HK_ICE_SEC_VOLT2\n\n    Plot 3:\n    IMIR_HK_ICE_SEC_VOLT4 : IDLE and HV_ON\n\n    Plot 4:\n    IMIR_HK_FW_POS_VOLT\n    IMIR_HK_GW14_POS_VOLT\n    IMIR_HK_GW23_POS_VOLT\n    IMIR_HK_CCC_POS_VOLT\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``dashborad.py``, e.g.:\n\n    ::\n        from .plots.ice_voltage_tab import ice_plots\n        tab = ice_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""miri_database.db""\n\n""""""\n\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.miri_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.models import LinearAxis, Range1d\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import gridplot, Column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\ndef volt4(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                y_range = [4.2,5],\n                x_axis_type = \'datetime\',\n                output_backend=""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""ICE_SEC_VOLT4""\n    pf.add_basic_layout(p)\n\n    # add a line renderer with legend and line thickness\n\n    a = pf.add_to_plot(p, ""Volt4 Idle"", ""IMIR_HK_ICE_SEC_VOLT4_IDLE"", start, end, conn, color = ""orange"")\n    b = pf.add_to_plot(p, ""Volt4 Hv on"", ""IMIR_HK_ICE_SEC_VOLT4_HV_ON"", start, end, conn, color = ""red"")\n\n    pf.add_hover_tool(p, [a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef volt1_3(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                y_range = [30,50],\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""ICE_SEC_VOLT1/3""\n    pf.add_basic_layout(p)\n\n    # add a line renderer with legend and line thickness\n    a = pf.add_to_plot(p, ""Volt1"", ""IMIR_HK_ICE_SEC_VOLT1"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""Volt3"", ""IMIR_HK_ICE_SEC_VOLT3"", start, end, conn, color = ""purple"")\n\n    pf.add_hover_tool(p, [a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef volt2(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""ICE_SEC_VOLT2""\n    pf.add_basic_layout(p)\n\n    # add a line renderer with legend and line thickness\n    a = pf.add_to_plot(p, ""Volt2"", ""IMIR_HK_ICE_SEC_VOLT2"", start, end, conn, color = ""red"")\n\n    pf.add_hover_tool(p,[a])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef pos_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                y_range = [280,300],\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (mV)\')\n\n    p.grid.visible = True\n    p.title.text = ""Wheel Sensor Supply""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""FW"", ""IMIR_HK_FW_POS_VOLT"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""GW14"", ""IMIR_HK_GW14_POS_VOLT"", start, end, conn, color = ""purple"")\n    c = pf.add_to_plot(p, ""GW23"", ""IMIR_HK_GW23_POS_VOLT"", start, end, conn, color = ""orange"")\n    d = pf.add_to_plot(p, ""CCC"", ""IMIR_HK_CCC_POS_VOLT"", start, end, conn, color = ""firebrick"")\n\n    pf.add_hover_tool(p, [a,b,c,d])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef volt_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>ICE_SEC_VOLT1/3</td>\n        <td>IMIR_HK_ICE_SEC_VOLT1 <br>\n            IMIR_HK_ICE_SEC_VOLT3 <br> </td>\n        <td>ICE Secondary Voltage (HV) V1 and V3</td>\n      </tr>\n      <tr>\n        <td>ICE_SEC_VOLT2</td>\n        <td>IMIR_HK_SEC_VOLT2</td>\n        <td>ICE secondary voltage (HV) V2</td>\n      </tr>\n      <tr>\n        <td>ICE_SEC_VOLT4</td>\n        <td>IMIR_HK_SEC_VOLT2</td>\n        <td>ICE secondary voltage (HV) V4 - HV on and IDLE</td>\n      </tr>\n      <tr>\n        <td>Wheel Sensor Supply</td>\n        <td>IMIR_HK_FW_POS_VOLT<br>\n            IMIR_HK_GW14_POS_VOLT<br>\n            IMIR_HK_GW23_POS_VOLT<br>\n            IMIR_HK_CCC_POS_VOLT</td>\n        <td>Wheel Sensor supply voltages </td>\n      </tr>\n    </table>\n    </body>\n    """""", width = 1100)\n\n    plot1 = volt1_3(conn, start, end)\n    plot2 = volt2(conn, start, end)\n    plot3 = volt4(conn, start, end)\n    plot4 = pos_volt(conn, start, end)\n\n    l = gridplot([[plot1, plot2], [plot3, plot4]], merge_tools = False)\n    layout = Column(descr, l)\n\n    tab = Panel(child = layout, title = ""ICE VOLTAGE"")\n\n    return tab\n'"
jwql/instrument_monitors/miri_monitors/data_trending/plots/plot_functions.py,2,"b'#! /usr/bin/env python\n""""""Auxilary functions for plots\n\n    Module holds functions that are used for several plots.\n\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n\n\nDependencies\n------------\n\n""""""\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\nfrom bokeh.plotting import figure\nfrom bokeh.models import BoxAnnotation, LinearAxis, Range1d\nfrom bokeh.embed import components\nfrom bokeh.models.widgets import Panel, Tabs\nfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatetimeTicker, SingleIntervalTicker\nfrom bokeh.models.formatters import TickFormatter\nfrom bokeh.models.tools import PanTool, SaveTool\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef pol_regression(x, y, rank):\n    \'\'\' Calculate polynominal regression of certain rank\n    Parameters\n    ----------\n    x : list\n        x parameters for regression\n    y : list\n        y parameters for regression\n    rank : int\n        rank of regression\n    Return\n    ------\n    y_poly : list\n        regression y parameters\n    \'\'\'\n    z = np.polyfit(x, y, rank)\n    f = np.poly1d(z)\n    y_poly = f(x)\n    return y_poly\n\ndef add_hover_tool(p, rend):\n    \'\'\' Append hover tool to plot\n    parameters\n    ----------\n    p : bokeh figure\n        declares where to append hover tool\n    rend : list\n        list of renderer to append hover tool\n    \'\'\'\n\n    from bokeh.models import HoverTool\n\n    #activate HoverTool for scatter plot\n    hover_tool = HoverTool( tooltips =\n    [\n        (\'Name\', \'$name\'),\n        (\'Count\', \'@data_points\'),\n        (\'Mean\', \'@average\'),\n        (\'Deviation\', \'@deviation\'),\n    ], renderers = rend)\n    #append hover tool\n    p.tools.append(hover_tool)\n\ndef add_limit_box(p, lower, upper, alpha = 0.1, color=""green""):\n    \'\'\' Adds box to plot\n    Parameters\n    ----------\n    p : bokeh figure\n        declares where to append hover tool\n    lower : float\n        lower limit of box\n    upper : float\n        upper limit of box\n    alpha : float\n        transperency of box\n    color : str\n        filling color\n    \'\'\'\n    box = BoxAnnotation(bottom = lower, top = upper, fill_alpha = alpha, fill_color = color)\n    p.add_layout(box)\n\ndef add_to_plot(p, legend, mnemonic, start, end, conn, y_axis= ""default"", color=""red"", err=\'n\'):\n    \'\'\'Add scatter and line to certain plot and activates hoover tool\n    Parameters\n    ----------\n    p : bokeh object\n        defines plot where line and scatter should be added\n    legend : str\n        will be showed in legend of plot\n    mnemonic : str\n        defines mnemonic to be plotted\n    start : datetime\n        sets start time for data query\n    end : datetime\n        sets end time for data query\n    conn : DBobject\n        connection object to database\n    y_axis : str (default=\'default\')\n        used if secon y axis is provided\n    color : str (default=\'dred\')\n        defines color for scatter and line plot\n    Return\n    ------\n    scat : plot scatter object\n        used for applying hovertools o plots\n    \'\'\'\n\n    #convert given start and end time to astropy time\n    start_str = str(Time(start).mjd)\n    end_str = str(Time(end).mjd)\n\n    #prepare and execute sql query\n    sql_c = ""SELECT * FROM ""+mnemonic+"" WHERE start_time BETWEEN ""+start_str+"" AND ""+end_str+"" ORDER BY start_time""\n    temp = pd.read_sql_query(sql_c, conn)\n\n    #put data into Dataframe and define ColumnDataSource for each plot\n    reg = pd.DataFrame({\'reg\' : pol_regression(temp[\'start_time\'], temp[\'average\'],3)})\n    temp = pd.concat([temp, reg], axis = 1)\n    temp[\'start_time\'] = pd.to_datetime( Time(temp[\'start_time\'], format = ""mjd"").datetime )\n    plot_data = ColumnDataSource(temp)\n\n    #plot data\n    p.line(x = ""start_time"", y = ""average"", color = color, y_range_name = y_axis, legend = legend, source = plot_data)\n    scat = p.scatter(x = ""start_time"", y = ""average"", color = color, name = mnemonic, y_range_name = y_axis, legend = legend, source = plot_data)\n\n    #generate error lines if wished\n    if err != \'n\':\n        #generate error bars\n        err_xs = []\n        err_ys = []\n\n        for index, item in temp.iterrows():\n            err_xs.append((item[\'start_time\'], item[\'start_time\']))\n            err_ys.append((item[\'average\'] - item[\'deviation\'], item[\'average\'] + item[\'deviation\']))\n\n        # plot them\n        p.multi_line(err_xs, err_ys, color = color, legend = legend)\n\n    return scat\n\ndef add_to_wplot(p, legend, mnemonic, start, end, conn, nominal, color = ""red""):\n    \'\'\'Add line plot to figure (for wheelpositions)\n    Parameters\n    ----------\n    p : bokeh object\n        defines figure where line schould be plotted\n    legend : str\n        will be showed in legend of plot\n    mnemonic : str\n        defines mnemonic to be plotted\n    start : datetime\n        sets start time for data query\n    end : datetime\n        sets end time for data query\n    conn : DBobject\n        connection object to database\n    color : str (default=\'dred\')\n        defines color for scatter and line plot\n    \'\'\'\n\n    start_str = str(Time(start).mjd)\n    end_str = str(Time(end).mjd)\n\n    sql_c = ""SELECT * FROM ""+mnemonic+"" WHERE timestamp BETWEEN ""+start_str+"" AND ""+end_str+"" ORDER BY timestamp""\n    temp = pd.read_sql_query(sql_c, conn)\n\n    #normalize values\n    temp[\'value\'] -= nominal\n    #temp[\'value\'] -= 1\n\n    temp[\'timestamp\'] = pd.to_datetime( Time(temp[\'timestamp\'], format = ""mjd"").datetime )\n    plot_data = ColumnDataSource(temp)\n\n    p.line(x = ""timestamp"", y = ""value"", color = color, legend = legend, source = plot_data)\n    p.scatter(x = ""timestamp"", y = ""value"", color = color, legend = legend, source = plot_data)\n\ndef add_basic_layout(p):\n    \'\'\'Add basic layout to certain plot\n    Parameters\n    ----------\n    p : bokeh object\n        defines plot where line and scatter should be added\n    \'\'\'\n    p.title.align = ""left""\n    p.title.text_color = ""#c85108""\n    p.title.text_font_size = ""25px""\n    p.background_fill_color = ""#efefef""\n\n    p.xaxis.axis_label_text_font_size = ""14pt""\n    p.xaxis.axis_label_text_color =\'#2D353C\'\n    p.yaxis.axis_label_text_font_size = ""14pt""\n    p.yaxis.axis_label_text_color = \'#2D353C\'\n\n    p.xaxis.major_tick_line_color = ""firebrick""\n    p.xaxis.major_tick_line_width = 2\n    p.xaxis.minor_tick_line_color = ""#c85108""\n'"
jwql/instrument_monitors/miri_monitors/data_trending/plots/power_tab.py,0,"b'import jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.miri_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.plotting import figure\nfrom bokeh.models import BoxAnnotation, LinearAxis, Range1d\nfrom bokeh.embed import components\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.layouts import column, row, WidgetBox\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef power_ice(conn, start, end):\n    #query data from database\n    start_str = str(Time(start).mjd)\n    end_str = str(Time(end).mjd)\n\n    sql_c = ""SELECT * FROM SE_ZIMIRICEA_IDLE WHERE start_time BETWEEN ""+start_str+"" AND ""+end_str+"" ORDER BY start_time""\n    _idle = pd.read_sql_query(sql_c, conn)\n    sql_c = ""SELECT * FROM SE_ZIMIRICEA_HV_ON WHERE start_time BETWEEN ""+start_str+"" AND ""+end_str+"" ORDER BY start_time""\n    _hv = pd.read_sql_query(sql_c, conn)\n\n    voltage = 30\n    _idle[\'average\'] *= voltage\n    _hv[\'average\'] *= voltage\n\n    _idle[\'start_time\'] = pd.to_datetime( Time(_idle[\'start_time\'], format = ""mjd"").datetime )\n    _hv[\'start_time\'] = pd.to_datetime( Time(_hv[\'start_time\'], format = ""mjd"").datetime )\n\n    #set column data source\n    idle = ColumnDataSource(_idle)\n    hv = ColumnDataSource(_hv)\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                  \\\n                plot_height = 500,                                  \\\n                y_range = [5,14],                                   \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend = ""webgl"",                           \\\n                x_axis_label = \'Date\', y_axis_label=\'Power (W)\')\n\n    p.grid.visible = True\n    p.title.text = ""POWER ICE""\n    pf.add_basic_layout(p)\n    pf.add_limit_box(p, 6, 8, alpha = 0.1, color = ""green"")\n\n\n    # add a line renderer with legend and line thickness\n    scat1=p.scatter(x = ""start_time"", y = ""average"", color = \'orange\', legend = ""Power idle"", source = idle)\n    scat2=p.scatter(x = ""start_time"", y = ""average"", color = \'red\', legend = ""Power hv on"", source = hv)\n    p.line(x = ""start_time"", y = ""average"", color = \'orange\', legend = ""Power idle"", source = idle)\n    p.line(x = ""start_time"", y = ""average"", color = \'red\', legend = ""Power hv on"", source = hv)\n\n    #generate error bars\n    err_xs_hv = []\n    err_ys_hv = []\n    err_xs_idle = []\n    err_ys_idle = []\n\n    for index, item in _hv.iterrows():\n        err_xs_hv.append((item[\'start_time\'],item[\'start_time\']))\n        err_ys_hv.append((item[\'average\'] - item[\'deviation\'], item[\'average\'] + item[\'deviation\']))\n\n    for index, item in _idle.iterrows():\n        err_xs_idle.append((item[\'start_time\'],item[\'start_time\']))\n        err_ys_idle.append((item[\'average\'] - item[\'deviation\'], item[\'average\'] + item[\'deviation\']))\n    # plot them\n    p.multi_line(err_xs_hv, err_ys_hv, color=\'red\', legend=\'Power hv on\')\n    p.multi_line(err_xs_idle, err_ys_idle, color=\'orange\', legend=\'Power idle\')\n\n    #activate HoverTool for scatter plot\n    hover_tool = HoverTool( tooltips =\n    [\n        (\'count\', \'@data_points\'),\n        (\'mean\', \'@average\'),\n        (\'deviation\', \'@deviation\'),\n\n    ], mode=\'mouse\', renderers=[scat1,scat2])\n\n    p.tools.append(hover_tool)\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef power_fpea(conn, start, end):\n\n    start_str = str(Time(start).mjd)\n    end_str = str(Time(end).mjd)\n\n    sql_c = ""SELECT * FROM SE_ZIMIRFPEA WHERE start_time BETWEEN ""+start_str+"" AND ""+end_str+"" ORDER BY start_time""\n    _fpea = pd.read_sql_query(sql_c, conn)\n\n    voltage = 30\n    _fpea[\'average\'] *= voltage\n\n    _fpea[\'start_time\'] = pd.to_datetime( Time(_fpea[\'start_time\'], format = ""mjd"").datetime )\n\n    #set column data source\n    fpea = ColumnDataSource(_fpea)\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [28.0, 28.5],                               \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend = ""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label=\'Power (W)\')\n\n    p.grid.visible = True\n    p.title.text = ""POWER FPE""\n    pf.add_basic_layout(p)\n\n    # add a line renderer with legend and line thickness\n    scat1 = p.scatter(x = ""start_time"", y = ""average"", color = \'orange\', legend = ""Power FPEA"", source = fpea)\n    p.line(x = ""start_time"", y = ""average"", color = \'orange\', legend = ""Power FPEA"", source = fpea)\n\n    err_xs = []\n    err_ys = []\n\n    for index, item in _fpea.iterrows():\n        err_xs.append((item[\'start_time\'], item[\'start_time\']))\n        err_ys.append((item[\'average\'] - item[\'deviation\'], item[\'average\'] + item[\'deviation\']))\n\n    # plot them\n    p.multi_line(err_xs, err_ys, color=\'orange\', legend=\'Power FPEA\')\n\n    #activate HoverTool for scatter plot\n    hover_tool = HoverTool( tooltips =\n    [\n        (\'count\', \'@data_points\'),\n        (\'mean\', \'@average\'),\n        (\'deviation\', \'@deviation\'),\n\n    ], renderers = [scat1])\n    p.tools.append(hover_tool)\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef currents(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 500,\n                y_range = [0,1.1],\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label = \'Current (A)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPE & ICE Currents""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""ICE Current idle"", ""SE_ZIMIRICEA_IDLE"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""ICE Current HV on"", ""SE_ZIMIRICEA_HV_ON"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""FPE Current"", ""SE_ZIMIRFPEA"", start, end, conn, color = ""brown"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\n\ndef power_plots(conn, start, end):\n\n\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>POWER ICE</td>\n        <td>SE_ZIMIRICEA * 30V (static)</td>\n        <td>Primary power consumption ICE side A - HV on and IDLE</td>\n      </tr>\n      <tr>\n        <td>POWER FPE</td>\n        <td>SE_ZIMIRIFPEA * 30V (static)</td>\n        <td>Primary power consumption FPE side A</td>\n      </tr>\n      <tr>\n        <td>FPE & ICE Voltages/Currents</td>\n        <td>SE_ZIMIRFPEA<br>\n            SE_ZIMIRCEA\n            *INPUT VOLTAGE* (missing)</td>\n        <td>Supply voltage and current ICE/FPE</td>\n      </tr>\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = power_ice(conn, start, end)\n    plot2 = power_fpea(conn, start, end)\n    plot3 = currents(conn, start, end)\n\n    layout = column(descr, plot1, plot2, plot3)\n\n    #layout_volt = row(volt4, volt1_3)\n    tab = Panel(child = layout, title = ""POWER"")\n\n    return tab\n'"
jwql/instrument_monitors/miri_monitors/data_trending/plots/temperature_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for TEMPERATURE tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1:\n    IGDP_MIR_ICE_T1P_CRYO\n    IGDP_MIR_ICE_T2R_CRYO\n    IGDP_MIR_ICE_T3LW_CRYO\n    IGDP_MIR_ICE_T4SW_CRYO\n    IGDP_MIR_ICE_T5IMG_CRYO\n    IGDP_MIR_ICE_T6DECKCRYO\n    IGDP_MIR_ICE_T7IOC_CRYO\n    IGDP_MIR_ICE_FW_CRYO\n    IGDP_MIR_ICE_CCC_CRYO\n    IGDP_MIR_ICE_GW14_CRYO\n    IGDP_MIR_ICE_GW23_CRYO\n    IGDP_MIR_ICE_POMP_CRYO\n    IGDP_MIR_ICE_POMR_CRYO\n    IGDP_MIR_ICE_IFU_CRYO\n    IGDP_MIR_ICE_IMG_CRYO\n\n    Plot 2:\n    ST_ZTC1MIRIA\n    ST_ZTC2MIRIA\n    IMIR_PDU_TEMP\n    IMIR_IC_SCE_ANA_TEMP1\n    IMIR_SW_SCE_ANA_TEMP1\n    IMIR_LW_SCE_ANA_TEMP1\n    IMIR_IC_SCE_DIG_TEMP\n    IMIR_SW_SCE_DIG_TEMP\n    IMIR_LW_SCE_DIG_TEMP\n\n    Plot 3:\n    IGDP_MIR_IC_DET_TEMP\n    IGDP_MIR_LW_DET_TEMP\n    IGDP_MIR_SW_DET_TEMP\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``dashborad.py``, e.g.:\n\n    ::\n        from .plots.temperature_tab import temperature_plots\n        tab = temperature_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""miri_database.db""\n\n""""""\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.miri_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.plotting import figure\nfrom bokeh.models import BoxAnnotation, LinearAxis, Range1d\nfrom bokeh.embed import components\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef cryo(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 700,                                  \\\n                y_range = [5.8,6.4],                                \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label = \'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""Cryo Temperatures""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""T1P"", ""IGDP_MIR_ICE_T1P_CRYO"", start, end, conn, color = ""brown"")\n    b = pf.add_to_plot(p, ""T2R"", ""IGDP_MIR_ICE_T2R_CRYO"", start, end, conn, color = ""burlywood"")\n    c = pf.add_to_plot(p, ""T3LW"", ""IGDP_MIR_ICE_T3LW_CRYO"", start, end, conn, color = ""cadetblue"")\n    d = pf.add_to_plot(p, ""T4SW"", ""IGDP_MIR_ICE_T4SW_CRYO"", start, end, conn, color = ""chartreuse"")\n    e = pf.add_to_plot(p, ""T5IMG"", ""IGDP_MIR_ICE_T5IMG_CRYO"", start, end, conn, color = ""chocolate"")\n    f = pf.add_to_plot(p, ""T6DECK"", ""IGDP_MIR_ICE_T6DECKCRYO"", start, end, conn, color = ""coral"")\n    g = pf.add_to_plot(p, ""T7IOC"", ""IGDP_MIR_ICE_T7IOC_CRYO"", start, end, conn, color = ""darkorange"")\n    h = pf.add_to_plot(p, ""FW"", ""IGDP_MIR_ICE_FW_CRYO"", start, end, conn, color = ""crimson"")\n    i = pf.add_to_plot(p, ""CCC"", ""IGDP_MIR_ICE_CCC_CRYO"", start, end, conn, color = ""cyan"")\n    j = pf.add_to_plot(p, ""GW14"", ""IGDP_MIR_ICE_GW14_CRYO"", start, end, conn, color = ""darkblue"")\n    k = pf.add_to_plot(p, ""GW23"", ""IGDP_MIR_ICE_GW23_CRYO"", start, end, conn, color = ""darkgreen"")\n    l = pf.add_to_plot(p, ""POMP"", ""IGDP_MIR_ICE_POMP_CRYO"", start, end, conn, color = ""darkmagenta"")\n    m = pf.add_to_plot(p, ""POMR"", ""IGDP_MIR_ICE_POMR_CRYO"", start, end, conn, color = ""darkcyan"")\n    n = pf.add_to_plot(p, ""IFU"", ""IGDP_MIR_ICE_IFU_CRYO"", start, end, conn, color = ""cornflowerblue"")\n    o = pf.add_to_plot(p, ""IMG"", ""IGDP_MIR_ICE_IMG_CRYO"", start, end, conn, color = ""orange"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g,h,i,j,k,l,m,n,o])\n\n    p.legend.location = ""bottom_right""\n    p.legend.orientation = ""horizontal""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    start_str = str(Time(start).mjd)\n    end_str = str(Time(end).mjd)\n\n    sql_c = ""SELECT * FROM IGDP_MIR_ICE_INTER_TEMP WHERE start_time BETWEEN ""+start_str+"" AND ""+end_str+"" ORDER BY start_time""\n    temp = pd.read_sql_query(sql_c, conn)\n\n    temp[\'average\']+= 273.15\n    reg = pd.DataFrame({\'reg\' : pf.pol_regression(temp[\'start_time\'], temp[\'average\'],3)})\n    temp = pd.concat([temp, reg], axis=1)\n\n    temp[\'start_time\'] = pd.to_datetime( Time(temp[\'start_time\'], format = ""mjd"").datetime )\n    plot_data = ColumnDataSource(temp)\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 700,                                  \\\n                y_range = [275,295],                             \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label = \'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""IEC Temperatures""\n    pf.add_basic_layout(p)\n\n    p.line(x = ""start_time"", y = ""average"", color = ""brown"", legend = ""ICE Internal"", source = plot_data)\n    p.scatter(x = ""start_time"", y = ""average"", color = ""brown"", legend = ""ICE Internal"", source = plot_data)\n\n    a = pf.add_to_plot(p, ""ICE IEC A"", ""ST_ZTC1MIRIA"", start, end, conn, color = ""burlywood"")\n    b = pf.add_to_plot(p, ""FPE IEC A"", ""ST_ZTC2MIRIA"", start, end, conn, color = ""cadetblue"")\n    j = pf.add_to_plot(p, ""ICE IEC B"", ""ST_ZTC1MIRIB"", start, end, conn, color = ""blue"")\n    k = pf.add_to_plot(p, ""FPE IEC B."", ""ST_ZTC2MIRIB"", start, end, conn, color = ""brown"")\n    c = pf.add_to_plot(p, ""FPE PDU"", ""IMIR_PDU_TEMP"", start, end, conn, color = ""chartreuse"")\n    d = pf.add_to_plot(p, ""ANA IC"", ""IMIR_IC_SCE_ANA_TEMP1"", start, end, conn, color = ""chocolate"")\n    e = pf.add_to_plot(p, ""ANA SW"", ""IMIR_SW_SCE_ANA_TEMP1"", start, end, conn, color = ""coral"")\n    f = pf.add_to_plot(p, ""ANA LW"", ""IMIR_LW_SCE_ANA_TEMP1"", start, end, conn, color = ""darkorange"")\n    g = pf.add_to_plot(p, ""DIG IC"", ""IMIR_IC_SCE_DIG_TEMP"", start, end, conn, color = ""crimson"")\n    h = pf.add_to_plot(p, ""DIG SW"", ""IMIR_SW_SCE_DIG_TEMP"", start, end, conn, color = ""cyan"")\n    i = pf.add_to_plot(p, ""DIG LW"", ""IMIR_LW_SCE_DIG_TEMP"", start, end, conn, color = ""darkblue"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g,h,i,j,k])\n\n    p.legend.location = ""bottom_right""\n    p.legend.orientation = ""horizontal""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef det(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 400,                                  \\\n                y_range = [6.395,6.41],                             \\\n                x_axis_type = \'datetime\',                           \\\n                output_backend=""webgl"",                             \\\n                x_axis_label = \'Date\', y_axis_label = \'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""Detector Temperature""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""Det. Temp. IC"", ""IGDP_MIR_IC_DET_TEMP"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""Det. Temp. LW"", ""IGDP_MIR_LW_DET_TEMP"", start, end, conn, color = ""green"")\n    c = pf.add_to_plot(p, ""Det. Temp. SW"", ""IGDP_MIR_SW_DET_TEMP"", start, end, conn, color = ""blue"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.orientation = ""horizontal""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef temperature_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>CRYO Temperatures</td>\n        <td>IGDP_MIR_ICE_T1P_CRYO<br>\n            IGDP_MIR_ICE_T2R_CRYO<br>\n            IGDP_MIR_ICE_T3LW_CRYO<br>\n            IGDP_MIR_ICE_T4SW_CRYO<br>\n            IGDP_MIR_ICE_T5IMG_CRYO<br>\n            IGDP_MIR_ICE_T6DECKCRYO<br>\n            IGDP_MIR_ICE_T7IOC_CRYO<br>\n            IGDP_MIR_ICE_FW_CRYO<br>\n            IGDP_MIR_ICE_CCC_CRYO<br>\n            IGDP_MIR_ICE_GW14_CRYO<br>\n            IGDP_MIR_ICE_GW23_CRYO<br>\n            IGDP_MIR_ICE_POMP_CRYO<br>\n            IGDP_MIR_ICE_POMR_CRYO<br>\n            IGDP_MIR_ICE_IFU_CRYO<br>\n            IGDP_MIR_ICE_IMG_CRYO<br></td>\n        <td>Deck Nominal Temperature (T1)<br>\n            Deck Redundant Temperature (T2)<br>\n            LW FPM I/F Temperature (T3)<br>\n            SW FPM I/F Temperature (T4)<br>\n            IM FPM I/F Temperature (T5)<br>\n            A-B Strut Apex Temperature (T6)<br>\n            IOC Temperature (T7)<br>\n            FWA Temperature<br>\n            CCC Temperature<br>\n            DGA-A (GW14) Temperature<br>\n            DGA-B (GW23) Temperature<br>\n            POMH Nominal Temperature<br>\n            POMH Redundant Temperature<br>\n            MRS (CF) Cal. Source Temperature<br>\n            Imager (CI) Cal. Source Temperature<br></td>\n      </tr>\n      <tr>\n        <td>IEC Temperatures</td>\n        <td>ST_ZTC1MIRIA<br>\n            ST_ZTC2MIRIA<br>\n            ST_ZTC1MIRIB<br>\n            ST_ZTC2MIRIB<br>\n            IGDP_MIR_ICE_INTER_TEMP<br>\n            IMIR_PDU_TEMP<br>\n            IMIR_IC_SCE_ANA_TEMP1<br>\n            IMIR_SW_SCE_ANA_TEMP1<br>\n            IMIR_LW_SCE_ANA_TEMP1<br>\n            IMIR_IC_SCE_DIG_TEMP<br>\n            IMIR_SW_SCE_DIG_TEMP<br>\n            IMIR_LW_SCE_DIG_TEMP<br></td>\n        <td>ICE IEC Panel Temp A<br>\n            FPE IEC Panel Temp A<br>\n            ICE IEC Panel Temp B<br>\n            FPE IEC Panel Temp B<br>\n            ICE internal Temperature<br>\n            FPE PDU Temperature<br>\n            FPE SCE Analogue board Temperature IC<br>\n            FPE SCE Analogue board Temperature SW<br>\n            FPE SCE Analogue board Temperature LW<br>\n            FPE SCE Digital board Temperature IC<br>\n            FPE SCE Digital board Temperature SW<br>\n            FPE SCE Digital board Temperature LW<br></td>\n      </tr>\n       <tr>\n         <td>Detector Temperatures</td>\n         <td>IGDP_MIR_IC_DET_TEMP<br>\n            IGDP_MIR_lW_DET_TEMP<br>\n            IGDP_MIR_SW_DET_TEMP<br></td>\n         <td>Detector Temperature (IC,SW&LW)<br></td>\n       </tr>\n    </table>\n    </body>\n    """""", width=1100)\n\n\n    plot1 = cryo(conn, start, end)\n    plot2 = temp(conn, start, end)\n    plot3 = det(conn, start, end)\n\n    layout = column(descr, plot1, plot2, plot3)\n    tab = Panel(child = layout, title = ""TEMPERATURE"")\n\n    return tab\n'"
jwql/instrument_monitors/miri_monitors/data_trending/plots/wheel_ratio_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for FPE VOLTAGE tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1:\n    IMIR_HK_FW_POS_RATIO_FND\n    IMIR_HK_FW_POS_RATIO_OPAQUE\n    IMIR_HK_FW_POS_RATIO_F1000W\n    IMIR_HK_FW_POS_RATIO_F1130W\n    IMIR_HK_FW_POS_RATIO_F1280W\n    IMIR_HK_FW_POS_RATIO_P750L\n    IMIR_HK_FW_POS_RATIO_F1500W\n    IMIR_HK_FW_POS_RATIO_F1800W\n    IMIR_HK_FW_POS_RATIO_F2100W\n    IMIR_HK_FW_POS_RATIO_F560W\n    IMIR_HK_FW_POS_RATIO_FLENS\n    IMIR_HK_FW_POS_RATIO_F2300C\n    IMIR_HK_FW_POS_RATIO_F770W\n    IMIR_HK_FW_POS_RATIO_F1550C\n    IMIR_HK_FW_POS_RATIO_F2550W\n    IMIR_HK_FW_POS_RATIO_F1140C\n    IMIR_HK_FW_POS_RATIO_F2550WR\n    IMIR_HK_FW_POS_RATIO_F1065C\n\n    Plot 2:\n    IMIR_HK_GW14_POS_RATIO_SHORT\n    IMIR_HK_GW14_POS_RATIO_MEDIUM\n    IMIR_HK_GW14_POS_RATIO_LONG\n\n    Plot 3:\n    IMIR_HK_GW23_POS_RATIO_SHORT\n    IMIR_HK_GW23_POS_RATIO_MEDIUM\n    IMIR_HK_GW23_POS_RATIO_LONG\n\n    Plot 4:\n    IMIR_HK_CCC_POS_RATIO_LOCKED\n    IMIR_HK_CCC_POS_RATIO_OPEN\n    IMIR_HK_CCC_POS_RATIO_CLOSED\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``dashboard.py``, e.g.:\n\n    ::\n        from .plots.wheel_ratio_tab import wheel_plots\n        tab = wheel_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""miri_database.db""\n\n""""""\n\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.miri_monitors.data_trending.plots.plot_functions as pf\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.mnemonics as mn\nfrom bokeh.plotting import figure\nfrom bokeh.models import BoxAnnotation, LinearAxis, Range1d\nfrom bokeh.embed import components\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import column, row, WidgetBox\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef gw14(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [-2,2],                                   \\\n                x_axis_type = \'datetime\',                           \\\n                x_axis_label = \'Date\', y_axis_label = \'ratio (normalized)\')\n\n    p.grid.visible = True\n    p.title.text = ""DGA-A Ratio""\n    p.title.align = ""left""\n    pf.add_basic_layout(p)\n\n    pf.add_to_wplot(p, ""SHORT"", ""IMIR_HK_GW14_POS_RATIO_SHORT"", start, end, conn, mn.gw14_nominals[\'SHORT\'], color = ""green"")\n    pf.add_to_wplot(p, ""MEDIUM"", ""IMIR_HK_GW14_POS_RATIO_MEDIUM"", start, end, conn, mn.gw14_nominals[\'MEDIUM\'], color = ""red"")\n    pf.add_to_wplot(p, ""LONG"", ""IMIR_HK_GW14_POS_RATIO_LONG"", start, end, conn, mn.gw14_nominals[\'LONG\'], color = ""blue"")\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\ndef gw23(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [-2,2],                                   \\\n                x_axis_type = \'datetime\',                           \\\n                x_axis_label = \'Date\', y_axis_label = \'ratio (normalized)\')\n\n    p.grid.visible = True\n    p.title.text = ""DGA-B Ratio""\n    p.title.align = ""left""\n    pf.add_basic_layout(p)\n\n    pf.add_to_wplot(p, ""SHORT"", ""IMIR_HK_GW23_POS_RATIO_SHORT"", start, end, conn, mn.gw23_nominals[\'SHORT\'], color = ""green"")\n    pf.add_to_wplot(p, ""MEDIUM"", ""IMIR_HK_GW23_POS_RATIO_MEDIUM"", start, end, conn, mn.gw23_nominals[\'MEDIUM\'], color = ""red"")\n    pf.add_to_wplot(p, ""LONG"", ""IMIR_HK_GW23_POS_RATIO_LONG"", start, end, conn, mn.gw23_nominals[\'LONG\'], color = ""blue"")\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\ndef ccc(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [-2,2],                                   \\\n                x_axis_type = \'datetime\',                           \\\n                x_axis_label = \'Date\', y_axis_label = \'ratio (normalized)\')\n\n    p.grid.visible = True\n    p.title.text = ""CCC Ratio""\n    pf.add_basic_layout(p)\n\n    #add_to_wplot(p, ""LOCKED"", ""IMIR_HK_CCC_POS_RATIO_LOCKED"", start, end, conn, mn.ccc_nominals[\'LOCKED\'], color = ""green"")\n    pf.add_to_wplot(p, ""OPEN"", ""IMIR_HK_CCC_POS_RATIO_OPEN"", start, end, conn, mn.ccc_nominals[\'OPEN\'], color = ""red"")\n    pf.add_to_wplot(p, ""CLOSED"", ""IMIR_HK_CCC_POS_RATIO_CLOSED"", start, end, conn, mn.ccc_nominals[\'CLOSED\'], color = ""blue"")\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\ndef fw(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 800,                                  \\\n                y_range = [-6,4],                                   \\\n                x_axis_type = \'datetime\',                           \\\n                x_axis_label = \'Date\', y_axis_label=\'ratio (normalized)\')\n\n    p.grid.visible = True\n    p.title.text = ""Filterwheel Ratio""\n    pf.add_basic_layout(p)\n\n    pf.add_to_wplot(p, ""FND"", ""IMIR_HK_FW_POS_RATIO_FND"", start, end, conn, mn.fw_nominals[\'FND\'], color = ""green"")\n    pf.add_to_wplot(p, ""OPAQUE"", ""IMIR_HK_FW_POS_RATIO_OPAQUE"", start, end, conn, mn.fw_nominals[\'OPAQUE\'], color = ""red"")\n    pf.add_to_wplot(p, ""F1000W"", ""IMIR_HK_FW_POS_RATIO_F1000W"", start, end, conn, mn.fw_nominals[\'F1000W\'], color = ""blue"")\n    pf.add_to_wplot(p, ""F1130W"", ""IMIR_HK_FW_POS_RATIO_F1130W"", start, end, conn, mn.fw_nominals[\'F1130W\'], color = ""orange"")\n    pf.add_to_wplot(p, ""F1280W"", ""IMIR_HK_FW_POS_RATIO_F1280W"", start, end, conn, mn.fw_nominals[\'F1280W\'], color = ""firebrick"")\n    pf.add_to_wplot(p, ""P750L"", ""IMIR_HK_FW_POS_RATIO_P750L"", start, end, conn, mn.fw_nominals[\'P750L\'], color = ""cyan"")\n    pf.add_to_wplot(p, ""F1500W"", ""IMIR_HK_FW_POS_RATIO_F1500W"", start, end, conn, mn.fw_nominals[\'F1500W\'], color = ""magenta"")\n    pf.add_to_wplot(p, ""F1800W"", ""IMIR_HK_FW_POS_RATIO_F1800W"", start, end, conn, mn.fw_nominals[\'F1800W\'], color = ""burlywood"")\n    pf.add_to_wplot(p, ""F2100W"", ""IMIR_HK_FW_POS_RATIO_F2100W"", start, end, conn, mn.fw_nominals[\'F2100W\'], color = ""cadetblue"")\n    pf.add_to_wplot(p, ""F560W"", ""IMIR_HK_FW_POS_RATIO_F560W"", start, end, conn, mn.fw_nominals[\'F560W\'], color = ""chartreuse"")\n    pf.add_to_wplot(p, ""FLENS"", ""IMIR_HK_FW_POS_RATIO_FLENS"", start, end, conn, mn.fw_nominals[\'FLENS\'], color = ""brown"")\n    pf.add_to_wplot(p, ""F2300C"", ""IMIR_HK_FW_POS_RATIO_F2300C"", start, end, conn, mn.fw_nominals[\'F2300C\'], color = ""chocolate"")\n    pf.add_to_wplot(p, ""F770W"", ""IMIR_HK_FW_POS_RATIO_F770W"", start, end, conn, mn.fw_nominals[\'F770W\'], color = ""darkorange"")\n    pf.add_to_wplot(p, ""F1550C"", ""IMIR_HK_FW_POS_RATIO_F1550C"", start, end, conn, mn.fw_nominals[\'F1550C\'], color = ""darkgreen"")\n    pf.add_to_wplot(p, ""F2550W"", ""IMIR_HK_FW_POS_RATIO_F2550W"", start, end, conn, mn.fw_nominals[\'F2550W\'], color = ""darkcyan"")\n    pf.add_to_wplot(p, ""F1140C"", ""IMIR_HK_FW_POS_RATIO_F1140C"", start, end, conn, mn.fw_nominals[\'F1140C\'], color = ""darkmagenta"")\n    pf.add_to_wplot(p, ""F2550WR"", ""IMIR_HK_FW_POS_RATIO_F2550WR"", start, end, conn, mn.fw_nominals[\'F2550WR\'], color = ""crimson"")\n    pf.add_to_wplot(p, ""F1065C"", ""IMIR_HK_FW_POS_RATIO_F1065C"", start, end, conn, mn.fw_nominals[\'F1065C\'], color = ""cornflowerblue"")\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\ndef wheel_ratios(conn, start, end):\n    \'\'\'Combine plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>Filterwheel Ratio</td>\n        <td>IMIR_HK_FW_POS_RATIO<br>\n            IMIR_HK_FW_CUR_POS<br></td>\n        <td>FW position sensor ratio (normalised) and commanded position</td>\n      </tr>\n      <tr>\n        <td>DGA-A Ratio</td>\n        <td>IMIR_HK_GW14_POS_RATIO<br>\n            IMIR_HK_GW14_CUR_POS<br></td>\n        <td>DGA-A position sensor ratio (normalised) and commanded position</td>\n      </tr>\n      <tr>\n        <td>DGA-B Ratio</td>\n        <td>IMIR_HK_GW23_POS_RATIO<br>\n            IMIR_HK_GW23_CUR_POS<br></td>\n        <td>DGA-B position sensor ratio (normalised) and commanded position</td>\n      </tr>\n      <tr>\n        <td>CCC Ratio</td>\n        <td>IMIR_HK_CCC_POS_RATIO<br>\n            IMIR_HK_CCC_CUR_POS<br></td>\n        <td>Contamination Control Cover position sensor ratio (normalised) and commanded position</td>\n      </tr>\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = fw(conn, start, end)\n    plot2 = gw14(conn, start, end)\n    plot3 = gw23(conn,  start, end)\n    plot4 = ccc(conn, start, end)\n\n    layout = column(descr, plot1, plot2, plot3, plot4)\n    tab = Panel(child = layout, title = ""WHEEL RATIO"")\n\n    return tab\n'"
jwql/instrument_monitors/miri_monitors/data_trending/utils/condition.py,0,"b'#! /usr/bin/env python\r\n""""""Module generates conditions over one or more mnemonics\r\n\r\nThe modules purpose is to return True/False for any times by reference of\r\ncertain conditions. If for instance the condition ""x>1"" over a defined period of\r\ntime is needed, the module looks for all elements where the condition applies\r\nand where it does not apply. This generates two lists, which contain the ""start""\r\nand ""end"" times of the condition.\r\nA futher function combines the start- and endtimes to time-tuples between which\r\nthe condition is known as TRUE. A ""state"" function returns True/False for an\r\nexact time attribute, whereby the condition is represented in binary form.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n    This module is not prepared for standalone use.\r\n\r\n    For use in programm set condition up like below:\r\n\r\n    import the module as follow:\r\n    >>>import condition as cond\r\n\r\n    generate list with required conditions:\r\n    >>>con_set = [ cond.equal(m.mnemonic(\'IMIR_HK_POM_LOOP\'),\'OFF\'),\r\n                cond.smaller(m.mnemonic(\'IMIR_HK_ICE_SEC_VOLT1\'),1),\r\n                cond.greater(m.mnemonic(\'SE_ZIMIRICEA\'),0.2)]\r\n\r\n    generate object of condition with the con_set as attribute:\r\n    >>>condition_object=cond.condition(con_set)\r\n\r\n    Now the condition_object can return a True/False statement wheather\r\n    the time given as attribut meets the conditions:\r\n\r\n    >>>if condition.state(float(element[\'Primary Time\'])):\r\n        -> True when condition for the given time applies\r\n        -> False when condition for the given time is not applicable\r\n\r\nDependencies\r\n------------\r\n    no external files needed\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\n\r\n\r\nclass condition:\r\n    """"""Class to hold several subconditions""""""\r\n\r\n    #contains list of representative time pairs for each subcondition\r\n    cond_time_pairs = []\r\n    #state of the condition\r\n    __state = False\r\n\r\n    #initializes condition through condition set\r\n    def __init__(self, cond_set):\r\n        """"""Initialize object with set of conditions\r\n        Parameters\r\n        ----------\r\n        cond_set : list\r\n            list contains subconditions objects\r\n        """"""\r\n        self.cond_set = cond_set\r\n\r\n    #destructor -> take care that all time_pairs are deleted!\r\n    def __del__(self):\r\n        """"""Delete object - destructor method""""""\r\n        del self.cond_time_pairs[:]\r\n\r\n    #prints all stored time pairs (for developement only)\r\n    def print_times(self):\r\n        """"""Print conditions time pairs on command line (developement)""""""\r\n        print(\'Available time pairs:\')\r\n        for times in self.cond_time_pairs:\r\n            print(\'list: \'+str(times))\r\n\r\n    #returns a interval if time is anywhere in between\r\n    def get_interval(self, time):\r\n        """"""Returns time interval if availlable, where ""time"" is in between\r\n        Parameters\r\n        ----------\r\n        time : float\r\n            given time attribute\r\n        Return\r\n        ------\r\n        time_pair : tuple\r\n            pair of start_time and end_time where time is in between\r\n        """"""\r\n        end_time = 10000000\r\n        start_time = 0\r\n\r\n        #do for every condition\r\n        for cond in self.cond_time_pairs:\r\n            #do for every time pair in condition\r\n            for pair in cond:\r\n                if (time > pair[0]) and (time < pair[1]):\r\n                    if (end_time > pair[1]) and (start_time < pair[0]):\r\n                        start_time = pair[0]\r\n                        end_time = pair[1]\r\n                        break\r\n                    else:\r\n                        break\r\n\r\n        if (end_time != 10000000) and (start_time != 0):\r\n            return [start_time, end_time]\r\n        else:\r\n            return None\r\n\r\n\r\n    #generates time pairs out of start and end times\r\n    def generate_time_pairs(start_times, end_times):\r\n        """"""Forms time pairs out of start times and end times\r\n        Parameters\r\n        ----------\r\n        start_times : list\r\n            contains all times where a condition applies\r\n        end_times : list\r\n            contains all times where the condition does not apply\r\n        Return\r\n        ------\r\n        time_pair : list\r\n            list of touples with start and end time\r\n        """"""\r\n        #internal use only\r\n        time_pair: float = []\r\n\r\n        #when the conditons doesn\xc2\xb4t apply anyway\r\n        if not start_times:\r\n            time_pair.append((0,0))\r\n\r\n        #check if the condition indicates an open time range\r\n        elif not end_times:\r\n            time_pair.append((start_times[0], 0))\r\n\r\n        #generate time pairs\r\n        #for each start time a higher or equal end time is searched for\r\n        #these times form am touple which is appended to  time_pair : list\r\n        else:\r\n            time_hook = 0\r\n            last_start_time = 0\r\n\r\n            for start in list(sorted(set(start_times))):\r\n\r\n                if(start > time_hook):\r\n                    for end in list(sorted(set(end_times))):\r\n\r\n                        if end > start:\r\n\r\n                            time_pair.append((start, end))\r\n                            time_hook = end\r\n                            break\r\n\r\n            if list(sorted(set(start_times)))[-1] > list(sorted(set(end_times)))[-1]:\r\n                time_pair.append((list(sorted(set(end_times)))[-1], 0))\r\n\r\n        return(time_pair)\r\n\r\n\r\n    #returns state of the condition at a given time\r\n    #if state(given time)==True -> condition is true\r\n    #if state(given time)==False -> condition is false\r\n    def state(self, time):\r\n        """"""Checks whether condition is true of false at a given time\r\n        Parameters\r\n        ----------\r\n        time : float\r\n            input time for condition query\r\n        Return\r\n        ------\r\n        state : bool\r\n            True/False statement whether the condition applies or not\r\n        """"""\r\n        #checks condition for every sub condition in condition set (subconditions)\r\n\r\n        state = self.__state\r\n\r\n        for cond in self.cond_time_pairs:\r\n\r\n            if self.__check_subcondition(cond, time):\r\n                state = True\r\n            else:\r\n                state = False\r\n                break\r\n\r\n        return state\r\n\r\n\r\n    def __check_subcondition(self, cond, time):\r\n\r\n        #if there are no values availlable\r\n        if cond[0][0] == 0:\r\n            return False\r\n\r\n        for time_pair in cond:\r\n            #if just a positive time is availlable, return true\r\n            if (time_pair[1] == 0) and (time > time_pair[0]):\r\n\r\n                return True\r\n\r\n            #if given time occurs between a time pair, return true\r\n            elif (time_pair[0]) <= time and (time < time_pair[1]):\r\n\r\n                return True\r\n\r\n            else:\r\n                pass\r\n\r\n\r\nclass equal(condition):\r\n    """"""Class to hold single ""is equal"" subcondition""""""\r\n\r\n    #add attributes to function - start function ""cond_time_pairs()""\r\n    def __init__(self, mnemonic, value):\r\n        """"""Initializes subconditon\r\n        Parameters\r\n        ----------\r\n        mnemonic : astropy table\r\n            includes mnemomic engineering data and corresponding primary time\r\n        value : str\r\n            coparison value for equal statement\r\n        """"""\r\n        self.mnemonic = mnemonic\r\n        self.value = value\r\n        condition.cond_time_pairs.append((self.cond_true_time()))\r\n\r\n\r\n    #generates a list of time-touples (start_time, end_time) that mark the beginning and end of\r\n    #wheather the condition is true or not\r\n    def cond_true_time(self):\r\n        """"""Filters all values that are equal to a given comparison value\r\n        if equal: Primary time -> temp_start\r\n        if not equal: Primary time -> temp_end\r\n        Return\r\n        ------\r\n        time_p : list\r\n            list of touples with start and end time\r\n        """"""\r\n        temp_start = []\r\n        temp_end = []\r\n\r\n        for key in self.mnemonic:\r\n\r\n            #find all times whoses Raw values equal the given value\r\n            if key[\'value\'] == self.value:\r\n                temp_start.append(key[""time""])\r\n\r\n            #find all end values\r\n            else:\r\n                temp_end.append(key[""time""])\r\n\r\n        time_p = condition.generate_time_pairs(temp_start, temp_end)\r\n        return time_p\r\n\r\n\r\nclass greater(condition):\r\n    """"""Class to hold single ""greater than"" subcondition""""""\r\n\r\n    #add attributes to function - start function ""cond_time_pairs()""\r\n    def __init__(self, mnemonic, value):\r\n        """"""Initializes subconditon\r\n        Parameters\r\n        ----------\r\n        mnemonic : astropy table\r\n            includes mnemomic engineering data and corresponding primary time\r\n        value : str\r\n            coparison value for equal statement\r\n        """"""\r\n        self.mnemonic= mnemonic\r\n        self.value=value\r\n        condition.cond_time_pairs.append((self.cond_true_time()))\r\n\r\n    def cond_true_time(self):\r\n        """"""Filters all values that are greater than a given comparison value\r\n        if equal: Primary time -> temp_start\r\n        if not equal: Primary time -> temp_end\r\n        Return\r\n        ------\r\n        time_p : list\r\n            list of touples with start and end time\r\n        """"""\r\n        temp_start: float = []\r\n        temp_end: float = []\r\n\r\n        for key in self.mnemonic:\r\n\r\n            #find all times whose Raw values are grater than the given value\r\n            if float(key[\'value\']) > self.value:\r\n                temp_start.append(key[""time""])\r\n\r\n            #find all end values\r\n            else:\r\n                temp_end.append(key[""time""])\r\n\r\n        time_p = condition.generate_time_pairs(temp_start, temp_end)\r\n        return time_p\r\n\r\n\r\nclass smaller(condition):\r\n    """"""Class to hold single ""greater than"" subcondition""""""\r\n\r\n    #add attributes to function - start function ""cond_time_pairs()""\r\n    def __init__(self, mnemonic, value):\r\n        """"""Initializes subconditon\r\n        Parameters\r\n        ----------\r\n        mnemonic : astropy table\r\n            includes mnemomic engineering data and corresponding primary time\r\n        value : str\r\n            coparison value for equal statement\r\n        """"""\r\n        self.mnemonic=mnemonic\r\n        self.value=value\r\n        condition.cond_time_pairs.append((self.cond_true_time()))\r\n\r\n    def cond_true_time(self):\r\n        """"""Filters all values that are greater than a given comparison value\r\n        if equal: Primary time -> temp_start\r\n        if not equal: Primary time -> temp_end\r\n        Return\r\n        ------\r\n        time_p : list\r\n            list of touples with start and end time\r\n        """"""\r\n        temp_start: float = []\r\n        temp_end: float = []\r\n\r\n        for key in self.mnemonic:\r\n\r\n            #find all times whose Raw values are grater than the given value\r\n            if float(key[\'value\']) < self.value:\r\n                temp_start.append(key[""time""])\r\n\r\n            #find all end values\r\n            else:\r\n                temp_end.append(key[""time""])\r\n\r\n        time_p = condition.generate_time_pairs(temp_start, temp_end)\r\n        return time_p\r\n\r\n\r\nif __name__ ==\'__main__\':\r\n    pass\r\n'"
jwql/instrument_monitors/miri_monitors/data_trending/utils/csv_to_AstropyTable.py,0,"b'#! /usr/bin/env python\r\n""""""Module for importing and sorting mnemonics\r\n\r\nThis module imports a whole set of mnemonics from a .CSV sheet and converts it\r\nto an astropy table. In a second step the table is sorted by its mnemoncis\r\nand for each mnemmonic another astropy table with reduced content is created.\r\nThe last step is to append the data (time and engineering value) with its\r\nmnemonic identifier as key to a dictionary.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n\r\n\r\nDependencies\r\n------------\r\n    mnemonics.py -> includes a list of mnemonics to be evaluated\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\nfrom astropy.table import Table\r\nfrom astropy.time import Time\r\nimport warnings\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.mnemonics as mn\r\n\r\n\r\nclass mnemonics:\r\n    """"""class to hold a set of mnemonics""""""\r\n\r\n    __mnemonic_dict = {}\r\n\r\n    def __init__(self, import_path):\r\n        """"""main function of this class\r\n        Parameters\r\n        ----------\r\n        import_path : str\r\n            defines file to import (csv sheet)\r\n        """"""\r\n        imported_data = self.import_CSV(import_path)\r\n        length = len(imported_data)\r\n\r\n        print(\'{} was imported - {} lines\'.format(import_path, length))\r\n\r\n        #look for every mnmonic given in mnemonicy.py\r\n        for mnemonic_name in mn.mnemonic_set_base:\r\n            temp = self.sort_mnemonic(mnemonic_name, imported_data)\r\n            #append temp to dict with related mnemonic\r\n            if temp != None:\r\n                self.__mnemonic_dict.update({mnemonic_name:temp})\r\n            else:\r\n                warnings.warn(""fatal error"")\r\n\r\n\r\n    def import_CSV(self, path):\r\n        """"""imports csv sheet and converts it to AstropyTable\r\n        Parameters\r\n        ----------\r\n        path : str\r\n            defines path to file to import\r\n        Return\r\n        ------\r\n        imported_data : AstropyTable\r\n            container for imported data\r\n        """"""\r\n        #read data from given *CSV file\r\n        imported_data=Table.read(path, format=\'ascii.basic\', delimiter=\',\')\r\n        return imported_data\r\n\r\n\r\n    #returns table of single mnemonic\r\n    def mnemonic(self, name):\r\n        """"""Returns table of one single mnemonic\r\n        Parameters\r\n        ----------\r\n        name : str\r\n            name of mnemonic\r\n        Return\r\n        ------\r\n        __mnemonic_dict[name] : AstropyTable\r\n            corresponding table to mnemonic name\r\n        """"""\r\n        try:\r\n            return self.__mnemonic_dict[name]\r\n        except KeyError:\r\n            print(\'{} not in list\'.format(name))\r\n\r\n\r\n    #looks for given mnemonic in given table\r\n    #returns list containing astropy tables with sorted mnemonics and engineering values\r\n    #adds useful meta data to Table\r\n    def sort_mnemonic(self, mnemonic, table):\r\n        """"""Looks for all values in table with identifier ""mnemonic""\r\n           Converts time string to mjd format\r\n        Parameters\r\n        ----------\r\n        mnemonic : str\r\n            identifies which mnemonic to look for\r\n        table : AstropyTable\r\n            table that stores mnemonics and data\r\n        Return\r\n        ------\r\n        mnemonic_table : AstropyTable\r\n            stores all data associated with identifier ""mnemonic""\r\n        """"""\r\n\r\n        temp1: float = []\r\n        temp2 = []\r\n\r\n        #appends present mnemonic data to temp arrays temp1 and temp2\r\n        for item in table:\r\n            try:\r\n                if item[\'Telemetry Mnemonic\'] == mnemonic:\r\n                    #convert time string to mjd format\r\n                    temp = item[\'Secondary Time\'].replace(\'/\',\'-\').replace(\' \',\'T\')\r\n                    t = Time(temp, format=\'isot\')\r\n\r\n                    temp1.append(t.mjd)\r\n                    temp2.append(item[\'EU Value\'])\r\n            except KeyError:\r\n                warnings.warn(""{} is not in mnemonic table"".format(mnemonic))\r\n\r\n        description = (\'time\',\'value\')\r\n        data = [temp1, temp2]\r\n\r\n        #add some meta data\r\n        if len(temp1) > 0:\r\n            date_start = temp1[0]\r\n            date_end = temp1[len(temp1)-1]\r\n            info = {\'start\':date_start, \'end\':date_end}\r\n        else:\r\n            info = {""n"":""n""}\r\n\r\n        #add name of mnemonic to meta data of list\r\n        info[\'mnemonic\'] = mnemonic\r\n        info[\'len\'] = len(temp1)\r\n\r\n        #table to return\r\n        mnemonic_table = Table(data, names = description, \\\r\n                        dtype = (\'f8\',\'str\'), meta = info)\r\n        return mnemonic_table\r\n\r\nif __name__ ==\'__main__\':\r\n    pass\r\n'"
jwql/instrument_monitors/miri_monitors/data_trending/utils/mnemonics.py,0,"b'""""""Module lists all neccessary mnemonics for MIRI data trending\r\n\r\nThe module includes several lists to import to MIRI data trending monitor program.\r\nThe lists are used for data aquisation and to set up the initial database.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n    import mnemoncis as mn\r\n\r\nDependencies\r\n------------\r\n    further information to included mnemonics: ###############\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\n\r\n#all mnemonic used for condition 1 (see: draft)\r\n#""SE_ZBUSVLT"",\r\nmnemonic_cond_1 = [\r\n""SE_ZIMIRICEA"",\r\n\r\n""IMIR_HK_ICE_SEC_VOLT4"",\r\n""IGDP_MIR_ICE_INTER_TEMP"",\r\n\r\n""ST_ZTC1MIRIA"",\r\n""ST_ZTC1MIRIB"",\r\n\r\n""IGDP_MIR_ICE_T1P_CRYO"",\r\n""IGDP_MIR_ICE_T2R_CRYO"",\r\n""IGDP_MIR_ICE_T3LW_CRYO"",\r\n""IGDP_MIR_ICE_T4SW_CRYO"",\r\n""IGDP_MIR_ICE_T5IMG_CRYO"",\r\n""IGDP_MIR_ICE_T6DECKCRYO"",\r\n""IGDP_MIR_ICE_T7IOC_CRYO"",\r\n""IGDP_MIR_ICE_FW_CRYO"",\r\n""IGDP_MIR_ICE_CCC_CRYO"",\r\n""IGDP_MIR_ICE_GW14_CRYO"",\r\n""IGDP_MIR_ICE_GW23_CRYO"",\r\n""IGDP_MIR_ICE_POMP_CRYO"",\r\n""IGDP_MIR_ICE_POMR_CRYO"",\r\n""IGDP_MIR_ICE_IFU_CRYO"",\r\n""IGDP_MIR_ICE_IMG_CRYO""]\r\n\r\n#all mnemonics used for condition 2 (see: draft)\r\nmnemonic_cond_2=[\r\n""SE_ZIMIRFPEA"",\r\n\r\n""IMIR_PDU_V_DIG_5V"",\r\n""IMIR_PDU_I_DIG_5V"",\r\n""IMIR_PDU_V_ANA_5V"",\r\n""IMIR_PDU_I_ANA_5V"",\r\n\r\n""IMIR_PDU_V_ANA_N5V"",\r\n""IMIR_PDU_I_ANA_N5V"",\r\n\r\n""IMIR_PDU_V_ANA_7V"",\r\n""IMIR_PDU_I_ANA_7V"",\r\n\r\n""IMIR_PDU_V_ANA_N7V"",\r\n""IMIR_PDU_I_ANA_N7V"",\r\n\r\n""IMIR_SPW_V_DIG_2R5V"",\r\n""IMIR_PDU_V_REF_2R5V"",\r\n\r\n""IGDP_MIR_IC_V_VDETCOM"",\r\n""IGDP_MIR_SW_V_VDETCOM"",\r\n""IGDP_MIR_LW_V_VDETCOM"",\r\n\r\n""IGDP_MIR_IC_V_VSSOUT"",\r\n""IGDP_MIR_SW_V_VSSOUT"",\r\n""IGDP_MIR_LW_V_VSSOUT"",\r\n""IGDP_MIR_IC_V_VRSTOFF"",\r\n\r\n""IGDP_MIR_SW_V_VRSTOFF"",\r\n""IGDP_MIR_LW_V_VRSTOFF"",\r\n\r\n""IGDP_MIR_IC_V_VP"",\r\n""IGDP_MIR_SW_V_VP"",\r\n""IGDP_MIR_LW_V_VP"",\r\n\r\n""IGDP_MIR_IC_V_VDDUC"",\r\n""IGDP_MIR_SW_V_VDDUC"",\r\n""IGDP_MIR_LW_V_VDDUC"",\r\n\r\n""IMIR_PDU_TEMP"",\r\n\r\n""ST_ZTC2MIRIA"",\r\n""ST_ZTC2MIRIB"",\r\n\r\n""IMIR_IC_SCE_ANA_TEMP1"",\r\n""IMIR_SW_SCE_ANA_TEMP1"",\r\n""IMIR_LW_SCE_ANA_TEMP1"",\r\n\r\n""IMIR_IC_SCE_DIG_TEMP"",\r\n""IMIR_SW_SCE_DIG_TEMP"",\r\n""IMIR_LW_SCE_DIG_TEMP"",\r\n\r\n""IGDP_MIR_IC_DET_TEMP"",\r\n""IGDP_MIR_LW_DET_TEMP"",\r\n""IGDP_MIR_SW_DET_TEMP""]\r\n\r\n#mnemonics for 15 min evaluation\r\nmnemonic_set_15min = mnemonic_cond_1 + mnemonic_cond_2\r\n\r\n#ICE secondary voltages -> apply to condition3\r\nmnemonic_cond_3 = [\r\n""IMIR_HK_ICE_SEC_VOLT1"",\r\n""IMIR_HK_ICE_SEC_VOLT2"",\r\n""IMIR_HK_ICE_SEC_VOLT3"",\r\n""IMIR_HK_ICE_SEC_VOLT4"",\r\n""SE_ZIMIRICEA""]\r\n\r\n#filter weel positions\r\nfw_positions = [\r\n""FND"",\r\n""OPAQUE"",\r\n""F1000W"",\r\n""F1130W"",\r\n""F1280W"",\r\n""P750L"",\r\n""F1500W"",\r\n""F1800W"",\r\n""F2100W"",\r\n""F560W"",\r\n""FLENS"",\r\n""F2300C"",\r\n""F770W"",\r\n""F1550C"",\r\n""F2550W"",\r\n""F1140C"",\r\n""F2550WR"",\r\n""F1065C""]\r\n\r\n#grating weel positions\r\ngw_positions = [\r\n""SHORT"",\r\n""MEDIUM"",\r\n""LONG""]\r\n\r\n#contamination control clap positions\r\nccc_positions = [\r\n""LOCKED"",\r\n""OPEN"",\r\n""CLOSED""]\r\n\r\nfw_nominals = {\r\n""FND"" : -164.46,\r\n""OPAQUE"" : 380.42,\r\n""F1000W"" : -23.88,\r\n""F1130W"" : 138.04,\r\n""F1280W"" : -298.14,\r\n""P750L"" : 12.79,\r\n""F1500W"" : -377.32,\r\n""F1800W"" : 435.61,\r\n""F2100W"" : -126.04,\r\n""F560W"" : 218.13,\r\n""FLENS"" : -212.44,\r\n""F2300C"" : 306.03,\r\n""F770W"" : -61.90,\r\n""F1550C"" : 188.88,\r\n""F2550W"" : -323.65,\r\n""F1140C"" : 83.08,\r\n""F2550WR"" : -255.18,\r\n""F1065C"" : 261.62 }\r\n\r\ngw23_nominals = {\r\n""SHORT"" : 619.81,\r\n""MEDIUM"" : 373.31,\r\n""LONG"" : 441.4}\r\n\r\ngw14_nominals = {\r\n""SHORT"" : 627.49,\r\n""MEDIUM"" : 342.71,\r\n""LONG"" : 408.75 }\r\n\r\nccc_nominals = {\r\n""LOCKED"" : 577.23,\r\n""OPEN"" : 507.86,\r\n""CLOSED"" : 399.90}\r\n\r\n\r\n#comprises all mnemonics used throughout he programm\r\nmnemonic_set_base = [\r\n""SE_ZIMIRICEA"",\r\n""SE_ZBUSVLT"",\r\n\r\n""IMIR_HK_ICE_SEC_VOLT1"",\r\n""IMIR_HK_ICE_SEC_VOLT2"",\r\n""IMIR_HK_ICE_SEC_VOLT3"",\r\n""IMIR_HK_ICE_SEC_VOLT4"",\r\n\r\n""IGDP_MIR_ICE_INTER_TEMP"",\r\n\r\n""ST_ZTC1MIRIB"",\r\n""ST_ZTC1MIRIA"",\r\n""ST_ZTC2MIRIB"",\r\n""ST_ZTC2MIRIA"",\r\n\r\n""IGDP_MIR_ICE_T1P_CRYO"",\r\n""IGDP_MIR_ICE_T2R_CRYO"",\r\n""IGDP_MIR_ICE_T3LW_CRYO"",\r\n""IGDP_MIR_ICE_T4SW_CRYO"",\r\n""IGDP_MIR_ICE_T5IMG_CRYO"",\r\n""IGDP_MIR_ICE_T6DECKCRYO"",\r\n""IGDP_MIR_ICE_T7IOC_CRYO"",\r\n""IGDP_MIR_ICE_FW_CRYO"",\r\n""IGDP_MIR_ICE_CCC_CRYO"",\r\n""IGDP_MIR_ICE_GW14_CRYO"",\r\n""IGDP_MIR_ICE_GW23_CRYO"",\r\n""IGDP_MIR_ICE_POMP_CRYO"",\r\n""IGDP_MIR_ICE_POMR_CRYO"",\r\n""IGDP_MIR_ICE_IFU_CRYO"",\r\n""IGDP_MIR_ICE_IMG_CRYO"",\r\n\r\n""SE_ZIMIRFPEA"",\r\n\r\n""IMIR_PDU_V_DIG_5V"",\r\n""IMIR_PDU_I_DIG_5V"",\r\n""IMIR_PDU_V_ANA_5V"",\r\n""IMIR_PDU_I_ANA_5V"",\r\n\r\n""IMIR_PDU_V_ANA_N5V"",\r\n""IMIR_PDU_I_ANA_N5V"",\r\n\r\n""IMIR_PDU_V_ANA_7V"",\r\n""IMIR_PDU_I_ANA_7V"",\r\n\r\n""IMIR_PDU_V_ANA_N7V"",\r\n""IMIR_PDU_I_ANA_N7V"",\r\n\r\n""IMIR_SPW_V_DIG_2R5V"",\r\n""IMIR_PDU_V_REF_2R5V"",\r\n\r\n""IGDP_MIR_IC_V_VDETCOM"",\r\n""IGDP_MIR_SW_V_VDETCOM"",\r\n""IGDP_MIR_LW_V_VDETCOM"",\r\n\r\n""IGDP_MIR_IC_V_VSSOUT"",\r\n""IGDP_MIR_SW_V_VSSOUT"",\r\n""IGDP_MIR_LW_V_VSSOUT"",\r\n""IGDP_MIR_IC_V_VRSTOFF"",\r\n\r\n""IGDP_MIR_SW_V_VRSTOFF"",\r\n""IGDP_MIR_LW_V_VRSTOFF"",\r\n\r\n""IGDP_MIR_IC_V_VP"",\r\n""IGDP_MIR_SW_V_VP"",\r\n""IGDP_MIR_LW_V_VP"",\r\n\r\n""IGDP_MIR_IC_V_VDDUC"",\r\n""IGDP_MIR_SW_V_VDDUC"",\r\n""IGDP_MIR_LW_V_VDDUC"",\r\n\r\n""IMIR_PDU_TEMP"",\r\n\r\n""IMIR_IC_SCE_ANA_TEMP1"",\r\n""IMIR_SW_SCE_ANA_TEMP1"",\r\n""IMIR_LW_SCE_ANA_TEMP1"",\r\n\r\n""IMIR_IC_SCE_DIG_TEMP"",\r\n""IMIR_SW_SCE_DIG_TEMP"",\r\n""IMIR_LW_SCE_DIG_TEMP"",\r\n\r\n""IGDP_MIR_IC_DET_TEMP"",\r\n""IGDP_MIR_LW_DET_TEMP"",\r\n""IGDP_MIR_SW_DET_TEMP"",\r\n\r\n""IMIR_HK_IMG_CAL_LOOP"",\r\n""IMIR_HK_IFU_CAL_LOOP"",\r\n""IMIR_HK_POM_LOOP"",\r\n""IGDP_IT_MIR_IC_STATUS"",\r\n""IGDP_IT_MIR_LW_STATUS"",\r\n""IGDP_IT_MIR_SW_STATUS"",\r\n\r\n""IMIR_HK_FW_POS_VOLT"",\r\n""IMIR_HK_FW_POS_RATIO"",\r\n""IMIR_HK_FW_CUR_POS"",\r\n\r\n""IMIR_HK_GW14_POS_VOLT"",\r\n""IMIR_HK_GW14_POS_RATIO"",\r\n""IMIR_HK_GW14_CUR_POS"",\r\n\r\n""IMIR_HK_GW23_POS_VOLT"",\r\n""IMIR_HK_GW23_POS_RATIO"",\r\n""IMIR_HK_GW23_CUR_POS"",\r\n\r\n""IMIR_HK_CCC_POS_RATIO"",\r\n""IMIR_HK_CCC_CUR_POS"",\r\n""IMIR_HK_CCC_POS_VOLT"" ]\r\n\r\n#mnemonic set for setting up database\r\nmnemonic_set_database = [\r\n""SE_ZIMIRICEA_IDLE"",\r\n""SE_ZIMIRICEA_HV_ON"",\r\n\r\n""ICE_POWER_IDLE"",\r\n""ICE_POWER_HV_ON"",\r\n\r\n""FPE_POWER"",\r\n\r\n""SE_ZBUSVLT"",\r\n\r\n""IMIR_HK_ICE_SEC_VOLT1"",\r\n""IMIR_HK_ICE_SEC_VOLT2"",\r\n""IMIR_HK_ICE_SEC_VOLT3"",\r\n""IMIR_HK_ICE_SEC_VOLT4_IDLE"",\r\n""IMIR_HK_ICE_SEC_VOLT4_HV_ON"",\r\n\r\n""IGDP_MIR_ICE_INTER_TEMP"",\r\n\r\n""ST_ZTC1MIRIB"",\r\n""ST_ZTC1MIRIA"",\r\n""ST_ZTC2MIRIB"",\r\n""ST_ZTC2MIRIA"",\r\n\r\n""IGDP_MIR_ICE_T1P_CRYO"",\r\n""IGDP_MIR_ICE_T2R_CRYO"",\r\n""IGDP_MIR_ICE_T3LW_CRYO"",\r\n""IGDP_MIR_ICE_T4SW_CRYO"",\r\n""IGDP_MIR_ICE_T5IMG_CRYO"",\r\n""IGDP_MIR_ICE_T6DECKCRYO"",\r\n""IGDP_MIR_ICE_T7IOC_CRYO"",\r\n""IGDP_MIR_ICE_FW_CRYO"",\r\n""IGDP_MIR_ICE_CCC_CRYO"",\r\n""IGDP_MIR_ICE_GW14_CRYO"",\r\n""IGDP_MIR_ICE_GW23_CRYO"",\r\n""IGDP_MIR_ICE_POMP_CRYO"",\r\n""IGDP_MIR_ICE_POMR_CRYO"",\r\n""IGDP_MIR_ICE_IFU_CRYO"",\r\n""IGDP_MIR_ICE_IMG_CRYO"",\r\n\r\n""SE_ZIMIRFPEA"",\r\n\r\n""IMIR_PDU_V_DIG_5V"",\r\n""IMIR_PDU_I_DIG_5V"",\r\n""IMIR_PDU_V_ANA_5V"",\r\n""IMIR_PDU_I_ANA_5V"",\r\n\r\n""IMIR_PDU_V_ANA_N5V"",\r\n""IMIR_PDU_I_ANA_N5V"",\r\n\r\n""IMIR_PDU_V_ANA_7V"",\r\n""IMIR_PDU_I_ANA_7V"",\r\n\r\n""IMIR_PDU_V_ANA_N7V"",\r\n""IMIR_PDU_I_ANA_N7V"",\r\n\r\n""IMIR_SPW_V_DIG_2R5V"",\r\n""IMIR_PDU_V_REF_2R5V"",\r\n\r\n""IGDP_MIR_IC_V_VDETCOM"",\r\n""IGDP_MIR_SW_V_VDETCOM"",\r\n""IGDP_MIR_LW_V_VDETCOM"",\r\n\r\n""IGDP_MIR_IC_V_VSSOUT"",\r\n""IGDP_MIR_SW_V_VSSOUT"",\r\n""IGDP_MIR_LW_V_VSSOUT"",\r\n""IGDP_MIR_IC_V_VRSTOFF"",\r\n\r\n""IGDP_MIR_SW_V_VRSTOFF"",\r\n""IGDP_MIR_LW_V_VRSTOFF"",\r\n\r\n""IGDP_MIR_IC_V_VP"",\r\n""IGDP_MIR_SW_V_VP"",\r\n""IGDP_MIR_LW_V_VP"",\r\n\r\n""IGDP_MIR_IC_V_VDDUC"",\r\n""IGDP_MIR_SW_V_VDDUC"",\r\n""IGDP_MIR_LW_V_VDDUC"",\r\n\r\n""IMIR_PDU_TEMP"",\r\n\r\n""IMIR_IC_SCE_ANA_TEMP1"",\r\n""IMIR_SW_SCE_ANA_TEMP1"",\r\n""IMIR_LW_SCE_ANA_TEMP1"",\r\n\r\n""IMIR_IC_SCE_DIG_TEMP"",\r\n""IMIR_SW_SCE_DIG_TEMP"",\r\n""IMIR_LW_SCE_DIG_TEMP"",\r\n\r\n""IGDP_MIR_IC_DET_TEMP"",\r\n""IGDP_MIR_LW_DET_TEMP"",\r\n""IGDP_MIR_SW_DET_TEMP"",\r\n\r\n""IMIR_HK_FW_POS_VOLT"",\r\n""IMIR_HK_GW14_POS_VOLT"",\r\n""IMIR_HK_GW23_POS_VOLT"",\r\n""IMIR_HK_CCC_POS_VOLT""]\r\n\r\n#different tables for wheelpostions\r\nmnemonic_wheelpositions = [\r\n""IMIR_HK_FW_POS_RATIO_FND"",\r\n""IMIR_HK_FW_POS_RATIO_OPAQUE"",\r\n""IMIR_HK_FW_POS_RATIO_F1000W"",\r\n""IMIR_HK_FW_POS_RATIO_F1130W"",\r\n""IMIR_HK_FW_POS_RATIO_F1280W"",\r\n""IMIR_HK_FW_POS_RATIO_P750L"",\r\n""IMIR_HK_FW_POS_RATIO_F1500W"",\r\n""IMIR_HK_FW_POS_RATIO_F1800W"",\r\n""IMIR_HK_FW_POS_RATIO_F2100W"",\r\n""IMIR_HK_FW_POS_RATIO_F560W"",\r\n""IMIR_HK_FW_POS_RATIO_FLENS"",\r\n""IMIR_HK_FW_POS_RATIO_F2300C"",\r\n""IMIR_HK_FW_POS_RATIO_F770W"",\r\n""IMIR_HK_FW_POS_RATIO_F1550C"",\r\n""IMIR_HK_FW_POS_RATIO_F2550W"",\r\n""IMIR_HK_FW_POS_RATIO_F1140C"",\r\n""IMIR_HK_FW_POS_RATIO_F2550WR"",\r\n""IMIR_HK_FW_POS_RATIO_F1065C"",\r\n\r\n""IMIR_HK_GW14_POS_RATIO_SHORT"",\r\n""IMIR_HK_GW14_POS_RATIO_MEDIUM"",\r\n""IMIR_HK_GW14_POS_RATIO_LONG"",\r\n\r\n""IMIR_HK_GW23_POS_RATIO_SHORT"",\r\n""IMIR_HK_GW23_POS_RATIO_MEDIUM"",\r\n""IMIR_HK_GW23_POS_RATIO_LONG"",\r\n\r\n""IMIR_HK_CCC_POS_RATIO_LOCKED"",\r\n""IMIR_HK_CCC_POS_RATIO_OPEN"",\r\n""IMIR_HK_CCC_POS_RATIO_CLOSED""]\r\n\r\nfw_pos_mnemonic = [\r\n""IMIR_HK_FW_POS_RATIO_FND"",\r\n""IMIR_HK_FW_POS_RATIO_OPAQUE"",\r\n""IMIR_HK_FW_POS_RATIO_F1000W"",\r\n""IMIR_HK_FW_POS_RATIO_F1130W"",\r\n""IMIR_HK_FW_POS_RATIO_F1280W"",\r\n""IMIR_HK_FW_POS_RATIO_P750L"",\r\n""IMIR_HK_FW_POS_RATIO_F1500W"",\r\n""IMIR_HK_FW_POS_RATIO_F1800W"",\r\n""IMIR_HK_FW_POS_RATIO_F2100W"",\r\n""IMIR_HK_FW_POS_RATIO_F560W"",\r\n""IMIR_HK_FW_POS_RATIO_FLENS"",\r\n""IMIR_HK_FW_POS_RATIO_F2300C"",\r\n""IMIR_HK_FW_POS_RATIO_F770W"",\r\n""IMIR_HK_FW_POS_RATIO_F1550C"",\r\n""IMIR_HK_FW_POS_RATIO_F2550W"",\r\n""IMIR_HK_FW_POS_RATIO_F1140C"",\r\n""IMIR_HK_FW_POS_RATIO_F2550WR"",\r\n""IMIR_HK_FW_POS_RATIO_F1065C""]\r\n'"
jwql/instrument_monitors/miri_monitors/data_trending/utils/process_data.py,0,"b'""""""This module holds functions for miri data trending\r\n\r\nAll functions in this module are tailored for the miri datatrending application.\r\nDetailed descriptions are given for every function individually.\r\n\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n\r\nDependencies\r\n------------\r\nMIRI_trend_requestsDRAFT1900201.docx\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\n\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.mnemonics as mn\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.condition as cond\r\nimport statistics\r\nimport sqlite3\r\nimport warnings\r\nfrom collections import defaultdict\r\n\r\n\r\n\r\ndef extract_data(condition, mnemonic):\r\n    \'\'\'Function extracts data from given mnemmonic at a given condition\r\n    Parameters\r\n    ----------\r\n    condition : object\r\n        conditon object that holds one or more subconditions\r\n    mnemonic : AstropyTable\r\n        holds single table with mnemonic data\r\n    Return\r\n    ------\r\n    temp : list  or None\r\n        holds data that applies to given condition\r\n    \'\'\'\r\n    temp = []\r\n\r\n    #look for all values that fit to the given conditions\r\n    for element in mnemonic:\r\n        if condition.state(float(element[\'time\'])):\r\n            temp.append(float(element[\'value\']))\r\n\r\n    #return temp is one ore more values fit to the condition\r\n    #return None if no applicable data was found\r\n    if len(temp) > 0:\r\n        return temp\r\n    else:\r\n        return None\r\n\r\ndef extract_filterpos1(condition, nominals, ratio_mnem, pos_mnem):\r\n    \'\'\'Extracts ratio values which correspond to given position values and their\r\n       proposed nominals\r\n    Parameters\r\n    ----------\r\n    condition : object\r\n        conditon object that holds one or more subconditions\r\n    nominals : dict\r\n        holds nominal values for all wheel positions\r\n    ratio_mem : AstropyTable\r\n        holds ratio values of one specific mnemonic\r\n    pos_mem : AstropyTable\r\n        holds pos values of one specific mnemonic\r\n    Return\r\n    ------\r\n    pos_values : dict\r\n        holds ratio values and times with corresponding positionlabel as key\r\n    \'\'\'\r\n\r\n    #initilize empty dict\r\n    pos_values = defaultdict(list)\r\n\r\n    #do for every position in mnemonic attribute\r\n    for pos in pos_mnem:\r\n\r\n        #raise warning if position is UNKNOWN\r\n        if pos[\'value\'] != ""UNKNOWN"":\r\n\r\n            #request time interval where the current positon is in between\r\n            interval = condition.get_interval(pos[\'time\'])\r\n\r\n            #get all ratio values in the interval\r\n\r\n\r\n\r\n            #check if condition attribute for current positon is true\r\n            if interval is not None:\r\n                cur_pos_time = pos[\'time\']\r\n                filtername = pos[\'value\']\r\n\r\n                for ratio in ratio_mnem:\r\n\r\n                    #look for ratio values which are in the same time interval\r\n                    #and differ a certain value (here 5mV) from the nominal\r\n                    if (ratio[\'time\'] >= cur_pos_time) and \\\r\n                        (abs(float(ratio[\'value\']) - nominals.get(pos[\'value\'])) < 5):\r\n\r\n                        if (ratio[\'time\'] > interval[0]) and (ratio[\'time\'] < interval[1]):\r\n                            pos_values[pos[\'value\']].append(( ratio[\'time\'], ratio[\'value\']))\r\n\r\n\r\n\r\n        else:\r\n            warnings.warn(""UNKNOWN Position"")\r\n    return pos_values\r\n\r\ndef extract_filterpos(condition, nominals, ratio_mnem, pos_mnem):\r\n    \'\'\'Extracts ratio values which correspond to given position values and their\r\n       proposed nominals\r\n    Parameters\r\n    ----------\r\n    condition : object\r\n        conditon object that holds one or more subconditions\r\n    nominals : dict\r\n        holds nominal values for all wheel positions\r\n    ratio_mem : AstropyTable\r\n        holds ratio values of one specific mnemonic\r\n    pos_mem : AstropyTable\r\n        holds pos values of one specific mnemonic\r\n    Return\r\n    ------\r\n    pos_values : dict\r\n        holds ratio values and times with corresponding positionlabel as key\r\n    \'\'\'\r\n\r\n    #initilize empty dict for assigned ratio values\r\n    pos_values = defaultdict(list)\r\n\r\n    for index, pos in enumerate(pos_mnem):\r\n\r\n        #raise warning if position is UNKNOWN\r\n        if pos[\'value\'] != ""UNKNOWN"":\r\n\r\n            #set up interval beween where the pos value was timed and the supply\r\n            interval = condition.get_interval(pos[\'time\'])\r\n\r\n            if interval is None:\r\n                continue\r\n            else:\r\n                interval[0] = pos[\'time\']\r\n                if pos_mnem[index+1][\'time\'] < interval[1]:\r\n                    interval[1] = pos_mnem[index+1][\'time\']\r\n\r\n            #empty list for pos values\r\n            interval_ratios = []\r\n\r\n            #get all ratio values in the interval\r\n            for ratio in ratio_mnem:\r\n                if (ratio[\'time\'] >= interval[0]) and (ratio[\'time\'] < interval[1]):\r\n                    interval_ratios.append(ratio)\r\n                elif ratio[\'time\']>= interval[1]:\r\n                        break\r\n\r\n            #check wheather pos values are in range of these checkvals\r\n            window = 1\r\n            found_value = False\r\n\r\n            while found_value == False:\r\n                for ratio in interval_ratios:\r\n                    if (abs(float(ratio[\'value\']) - nominals.get(pos[\'value\'])) < window):\r\n                        found_value = True\r\n                        pos_values[pos[\'value\']].append(( ratio[\'time\'], ratio[\'value\']))\r\n                        break\r\n\r\n                window +=2\r\n\r\n                if window > 10:\r\n                    print(\'ratio error\')\r\n                    break\r\n\r\n        else:\r\n            warnings.warn(""UNKNOWN Position"")\r\n    return pos_values\r\n\r\ndef once_a_day_routine(mnemonic_data):\r\n    \'\'\'Proposed routine for processing a 15min data file once a day\r\n    Parameters\r\n    ----------\r\n    mnemonic_data : dict\r\n        dict holds time and value in a astropy table with correspining identifier as key\r\n    Return\r\n    ------\r\n    data_cond_1 : dict\r\n        holds extracted data with condition 1 applied\r\n    data_cond_1 : dict\r\n        holds extracted data with condition 2 applied\r\n    \'\'\'\r\n\r\n    #abbreviate attribute\r\n    m = mnemonic_data\r\n    returndata = dict()\r\n\r\n\r\n    #########################################################################\r\n    con_set_1 = [                                               \\\r\n    cond.equal(m.mnemonic(\'IMIR_HK_IMG_CAL_LOOP\'),\'OFF\'),       \\\r\n    cond.equal(m.mnemonic(\'IMIR_HK_IFU_CAL_LOOP\'),\'OFF\'),       \\\r\n    cond.equal(m.mnemonic(\'IMIR_HK_POM_LOOP\'),\'OFF\'),           \\\r\n    cond.smaller(m.mnemonic(\'IMIR_HK_ICE_SEC_VOLT1\'),1.0),      \\\r\n    cond.greater(m.mnemonic(\'SE_ZIMIRICEA\'),0.2)]\r\n    #setup condition\r\n    condition_1 = cond.condition(con_set_1)\r\n\r\n\r\n    #add filtered engineering values of mnemonics given in list mnemonic_cond_1\r\n    #to dictitonary\r\n    for identifier in mn.mnemonic_cond_1:\r\n        data = extract_data(condition_1, m.mnemonic(identifier))\r\n\r\n        if data != None:\r\n            returndata.update( {identifier:data} )\r\n        else:\r\n            print(""no data for {}"".format(identifier))\r\n\r\n    del condition_1\r\n\r\n    ##########################################################################\r\n    #under normal use following line should be added:\r\n    #cond.equal(m.mnemonic(\'IGDP_IT_MIR_SW_STATUS\'), \'DETECTOR_READY\'),      \\\r\n    #SW was missing in the trainigs data so I could not use it for a condition.\r\n    con_set_2 = [                                                           \\\r\n    cond.greater(m.mnemonic(\'SE_ZIMIRFPEA\'), 0.5),                          \\\r\n    cond.equal(m.mnemonic(\'IGDP_IT_MIR_IC_STATUS\'), \'DETECTOR_READY\'),      \\\r\n    cond.equal(m.mnemonic(\'IGDP_IT_MIR_LW_STATUS\'), \'DETECTOR_READY\')]\r\n    #setup condition\r\n    condition_2 = cond.condition(con_set_2)\r\n\r\n    #add filtered engineering values of mnemonics given in list mnemonic_cond_2\r\n    #to dictitonary\r\n    for identifier in mn.mnemonic_cond_2:\r\n        data = extract_data(condition_2, m.mnemonic(identifier))\r\n\r\n        if data != None:\r\n            returndata.update( {identifier:data} )\r\n        else:\r\n            print(""no data for {}"".format(identifier))\r\n\r\n    del condition_2\r\n\r\n    return returndata\r\n\r\n\r\ndef whole_day_routine(mnemonic_data):\r\n    \'\'\'Proposed routine for processing data representing a whole day\r\n    Parameters\r\n    ----------\r\n    mnemonic_data : dict\r\n        dict holds time and value in a astropy table with correspining identifier as key\r\n    Return\r\n    ------\r\n    data_cond_3 : dict\r\n        holds extracted data with condition 3 applied\r\n    FW_volt : list\r\n        extracted data for IMIR_HK_FW_POS_VOLT\r\n    GW14_volt : list\r\n        extracted data for IMIR_HK_GW14_POS_VOLT\r\n    GW23_volt : list\r\n        extracted data for IMIR_HK_GW23_POS_VOLT\r\n    CCC_volt : list\r\n        extracted data for IMIR_HK_CCC_POS_VOLT\r\n    \'\'\'\r\n\r\n    #abbreviate attribute\r\n    m = mnemonic_data\r\n    returndata = dict()\r\n\r\n    #########################################################################\r\n    con_set_3 = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_ICE_SEC_VOLT1\'), 25.0)]\r\n    #setup condition\r\n    condition_3 = cond.condition(con_set_3)\r\n\r\n    #add filtered engineering values of mnemonics given in list mnemonic_cond_3\r\n    #to dictitonary\r\n    for identifier in mn.mnemonic_cond_3:\r\n        data = extract_data(condition_3, m.mnemonic(identifier))\r\n\r\n        if data != None:\r\n            returndata.update({identifier:data})\r\n        else:\r\n            print(""no data for {}"".format(identifier))\r\n\r\n    del condition_3\r\n\r\n    #########################################################################\r\n    #extract data for IMIR_HK_FW_POS_VOLT under given condition\r\n    con_set_FW = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_FW_POS_VOLT\'),250.0)]\r\n    #setup condition\r\n    condition_FW = cond.condition(con_set_FW)\r\n    FW_volt = extract_data(condition_FW, m.mnemonic(\'IMIR_HK_FW_POS_VOLT\'))\r\n    returndata.update({\'IMIR_HK_FW_POS_VOLT\':FW_volt})\r\n    del condition_FW\r\n\r\n    #extract data for IMIR_HK_GW14_POS_VOLT under given condition\r\n    con_set_GW14 = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_GW14_POS_VOLT\'),250.0)]\r\n    #setup condition\r\n    condition_GW14 = cond.condition(con_set_GW14)\r\n    GW14_volt = extract_data(condition_GW14, m.mnemonic(\'IMIR_HK_GW14_POS_VOLT\'))\r\n    returndata.update({\'IMIR_HK_GW14_POS_VOLT\':GW14_volt})\r\n    del condition_GW14\r\n\r\n    #extract data for IMIR_HK_GW23_POS_VOLT under given condition\r\n    con_set_GW23 = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_GW23_POS_VOLT\'),250.0)]\r\n    #setup condition\r\n    condition_GW23 = cond.condition(con_set_GW23)\r\n    GW23_volt = extract_data(condition_GW23, m.mnemonic(\'IMIR_HK_GW23_POS_VOLT\'))\r\n    returndata.update({\'IMIR_HK_GW23_POS_VOLT\':GW23_volt})\r\n    del condition_GW23\r\n\r\n    #extract data for IMIR_HK_CCC_POS_VOLT under given condition\r\n    con_set_CCC = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_CCC_POS_VOLT\'),250.0)]\r\n    #setup condition\r\n    condition_CCC = cond.condition(con_set_CCC)\r\n    CCC_volt = extract_data(condition_CCC, m.mnemonic(\'IMIR_HK_CCC_POS_VOLT\'))\r\n    returndata.update({\'IMIR_HK_CCC_POS_VOLT\':CCC_volt})\r\n    del condition_CCC\r\n\r\n    return returndata\r\n\r\n\r\ndef wheelpos_routine(mnemonic_data):\r\n    \'\'\'Proposed routine for positionsensors each day\r\n    Parameters\r\n    ----------\r\n    mnemonic_data : dict\r\n        dict holds time and value in a astropy table with correspining identifier as key\r\n    Return\r\n    ------\r\n    FW : dict\r\n        holds FW ratio values and times with corresponding positionlabel as key\r\n    GW14 : dict\r\n        holds GW14 ratio values and times with corresponding positionlabel as key\r\n    GW23 : dict\r\n        holds GW23 ratio values and times with corresponding positionlabel as key\r\n    CCC : dict\r\n        holds CCC ratio values and times with corresponding positionlabel as key\r\n    \'\'\'\r\n\r\n    #abbreviate attribute\r\n    m = mnemonic_data\r\n\r\n    con_set_FW = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_FW_POS_VOLT\'),250.0)]\r\n    #setup condition\r\n    condition_FW = cond.condition(con_set_FW)\r\n    FW = extract_filterpos(condition_FW, mn.fw_nominals, \\\r\n        m.mnemonic(\'IMIR_HK_FW_POS_RATIO\'), m.mnemonic(\'IMIR_HK_FW_CUR_POS\'))\r\n\r\n    del condition_FW\r\n\r\n    con_set_GW14 = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_GW14_POS_VOLT\'),250.0)]\r\n    #setup condition\r\n    condition_GW14 = cond.condition(con_set_GW14)\r\n    GW14 = extract_filterpos(condition_GW14, mn.gw14_nominals, \\\r\n        m.mnemonic(\'IMIR_HK_GW14_POS_RATIO\'), m.mnemonic(\'IMIR_HK_GW14_CUR_POS\'))\r\n\r\n    del condition_GW14\r\n\r\n    con_set_GW23 = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_GW23_POS_VOLT\'),250.0)]\r\n    #setup condition\r\n    condition_GW23 = cond.condition(con_set_GW23)\r\n    GW23 = extract_filterpos(condition_GW23, mn.gw23_nominals, \\\r\n        m.mnemonic(\'IMIR_HK_GW23_POS_RATIO\'), m.mnemonic(\'IMIR_HK_GW23_CUR_POS\'))\r\n\r\n    del condition_GW23\r\n\r\n    con_set_CCC = [                                               \\\r\n    cond.greater(m.mnemonic(\'IMIR_HK_CCC_POS_VOLT\'),250.0)]\r\n    #setup condition\r\n    condition_CCC = cond.condition(con_set_CCC)\r\n    CCC = extract_filterpos(condition_CCC, mn.ccc_nominals, \\\r\n        m.mnemonic(\'IMIR_HK_CCC_POS_RATIO\'), m.mnemonic(\'IMIR_HK_CCC_CUR_POS\'))\r\n\r\n    del condition_CCC\r\n\r\n    return FW, GW14, GW23, CCC\r\n\r\nif __name__ ==\'__main__\':\r\n    pass\r\n'"
jwql/instrument_monitors/miri_monitors/data_trending/utils/sql_interface.py,0,"b'""""""Module holds functions to generate and access sqlite databases\r\n\r\nThe module is tailored for use in miri data trending. It holds functions to\r\ncreate and close connections to a sqlite database. Calling the module itself\r\ncreates a sqlite database with specific tables used at miri data trending.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n\r\nDependencies\r\n------------\r\n    import mnemonics as m\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\nimport os\r\nimport sqlite3\r\nfrom sqlite3 import Error\r\n\r\nimport jwql.instrument_monitors.miri_monitors.data_trending.utils.mnemonics as m\r\nfrom jwql.utils.utils import get_config, filename_parser\r\n\r\ndef create_connection(db_file):\r\n    \'\'\'Sets up a connection or builds database\r\n    Parameters\r\n    ----------\r\n    db_file : string\r\n        represents filename of database\r\n    Return\r\n    ------\r\n    conn : DBobject or None\r\n        Connection object or None\r\n    \'\'\'\r\n    try:\r\n        conn = sqlite3.connect(db_file)\r\n        print(\'Connected to database ""{}""\'.format(db_file))\r\n        return conn\r\n    except Error as e:\r\n        print(e)\r\n    return None\r\n\r\n\r\ndef close_connection(conn):\r\n    \'\'\'Closes connection to database\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to be closed\r\n    \'\'\'\r\n    conn.close()\r\n    print(\'Connection closed\')\r\n\r\n\r\ndef add_data(conn, mnemonic, data):\r\n    \'\'\'Add data of a specific mnemonic to database if it not exists\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        connection object to access database\r\n    mnemonic : string\r\n        identifies the table\r\n    data : list\r\n        specifies the data\r\n    \'\'\'\r\n\r\n    c = conn.cursor()\r\n\r\n    #check if data already exists (start_time as identifier)\r\n    c.execute(\'SELECT id from {} WHERE start_time= {}\'.format(mnemonic, data[0]))\r\n    temp = c.fetchall()\r\n\r\n    if len(temp) == 0:\r\n        c.execute(\'INSERT INTO {} (start_time,end_time,data_points,average,deviation) \\\r\n                VALUES (?,?,?,?,?)\'.format(mnemonic),data)\r\n        conn.commit()\r\n    else:\r\n        print(\'data already exists\')\r\n\r\n\r\ndef add_wheel_data(conn, mnemonic, data):\r\n    \'\'\'Add data of a specific wheel position to database if it not exists\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        connection object to access database\r\n    mnemonic : string\r\n        identifies the table\r\n    data : list\r\n        specifies the data\r\n    \'\'\'\r\n\r\n    c = conn.cursor()\r\n\r\n    #check if data already exists (start_time)\r\n    c.execute(\'SELECT id from {} WHERE timestamp = {}\'.format(mnemonic, data[0]))\r\n    temp = c.fetchall()\r\n\r\n    if len(temp) == 0:\r\n        c.execute(\'INSERT INTO {} (timestamp, value) \\\r\n                VALUES (?,?)\'.format(mnemonic),data)\r\n        conn.commit()\r\n    else:\r\n        print(\'data already exists\')\r\n\r\n\r\ndef main():\r\n    \'\'\' Creates SQLite database with tables proposed in mnemonics.py\'\'\'\r\n\r\n    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\r\n\r\n    #generate paths\r\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'miri_database.db\')\r\n\r\n    conn = create_connection(DATABASE_FILE)\r\n\r\n    c=conn.cursor()\r\n\r\n    for mnemonic in m.mnemonic_set_database:\r\n        try:\r\n            c.execute(\'CREATE TABLE IF NOT EXISTS {} (         \\\r\n                                        id INTEGER,                     \\\r\n                                        start_time REAL,                \\\r\n                                        end_time REAL,                  \\\r\n                                        data_points INTEGER,               \\\r\n                                        average REAL,                   \\\r\n                                        deviation REAL,                 \\\r\n                                        performed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\\r\n                                        PRIMARY KEY (id));\'.format(mnemonic))\r\n        except Error as e:\r\n            print(\'e\')\r\n\r\n    for mnemonic in m.mnemonic_wheelpositions:\r\n        try:\r\n            c.execute(\'CREATE TABLE IF NOT EXISTS {} (         \\\r\n                                        id INTEGER,            \\\r\n                                        timestamp REAL,        \\\r\n                                        value REAL,            \\\r\n                                        performed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\\r\n                                        PRIMARY KEY (id));\'.format(mnemonic))\r\n        except Error as e:\r\n            print(\'e\')\r\n\r\n    print(""Database initial setup complete"")\r\n    conn.commit()\r\n    close_connection(conn)\r\n\r\n#sets up database if called as main\r\nif __name__ == ""__main__"":\r\n    main()\r\n    print(""sql_interface.py done"")\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/__init__.py,0,b''
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/caa_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for CAA tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1 - Lamp Voltages and Currents (Distincted)\n    INRSH_LAMP_SEL\n    INRSI_C_CAA_CURRENT\n    INRSI_C_CAA_VOLTAGE\n\n    Plot 2 - CAA (Voltages and Currents)\n    INRSH_CAA_VREFOFF\n    INRSH_CAA_VREF\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``nirspec_dashboard.py``, e.g.:\n\n    ::\n        from .plots.voltage_tab import voltage_plots\n        tab = voltage_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""nirpsec_database.db""\n\n""""""\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.models import LinearAxis, Range1d\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.layouts import WidgetBox, gridplot, Column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef lamp_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 800,\n                y_range = [1.2,2.3],\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""CAA Lamp Voltages""\n    pf.add_basic_layout(p)\n\n    l = pf.add_to_plot(p, ""FLAT1"", ""LAMP_FLAT1_VOLT"", start, end, conn, color = ""red"")\n    m = pf.add_to_plot(p, ""FLAT2"", ""LAMP_FLAT2_VOLT"", start, end, conn, color = ""green"")\n    n = pf.add_to_plot(p, ""FLAT3"", ""LAMP_FLAT3_VOLT"", start, end, conn, color = ""blue"")\n    o = pf.add_to_plot(p, ""FLAT4"", ""LAMP_FLAT4_VOLT"", start, end, conn, color = ""brown"")\n    x = pf.add_to_plot(p, ""FLAT5"", ""LAMP_FLAT5_VOLT"", start, end, conn, color = ""orange"")\n    q = pf.add_to_plot(p, ""LINE1"", ""LAMP_LINE1_VOLT"", start, end, conn, color = ""cyan"")\n    r = pf.add_to_plot(p, ""LINE2"", ""LAMP_LINE2_VOLT"", start, end, conn, color = ""darkmagenta"")\n    s = pf.add_to_plot(p, ""LINE3"", ""LAMP_LINE3_VOLT"", start, end, conn, color = ""burlywood"")\n    t = pf.add_to_plot(p, ""LINE4"", ""LAMP_LINE4_VOLT"", start, end, conn, color = ""darkkhaki"")\n    u = pf.add_to_plot(p, ""REF"", ""LAMP_REF_VOLT"", start, end, conn, color = ""darkblue"")\n    v = pf.add_to_plot(p, ""TEST"", ""LAMP_TEST_VOLT"", start, end, conn, color = ""goldenrod"")\n\n    pf.add_hover_tool(p,[l,m,n,o,x,q,r,s,t,u,v])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef lamp_current(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 600,\n                y_range = [10.5,14.5],\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Current (mA)\')\n\n    p.grid.visible = True\n    p.title.text = ""CAA Lamp currents""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""FLAT1"", ""LAMP_FLAT1_CURR"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""FLAT2"", ""LAMP_FLAT2_CURR"", start, end, conn, color = ""green"")\n    c = pf.add_to_plot(p, ""FLAT3"", ""LAMP_FLAT3_CURR"", start, end, conn, color = ""blue"")\n    d = pf.add_to_plot(p, ""FLAT4"", ""LAMP_FLAT4_CURR"", start, end, conn, color = ""brown"")\n    e = pf.add_to_plot(p, ""FLAT5"", ""LAMP_FLAT5_CURR"", start, end, conn, color = ""orange"")\n    f = pf.add_to_plot(p, ""LINE1"", ""LAMP_LINE1_CURR"", start, end, conn, color = ""cyan"")\n    g = pf.add_to_plot(p, ""LINE2"", ""LAMP_LINE2_CURR"", start, end, conn, color = ""darkmagenta"")\n    h = pf.add_to_plot(p, ""LINE3"", ""LAMP_LINE3_CURR"", start, end, conn, color = ""burlywood"")\n    i = pf.add_to_plot(p, ""LINE4"", ""LAMP_LINE4_CURR"", start, end, conn, color = ""darkkhaki"")\n    j = pf.add_to_plot(p, ""REF"", ""LAMP_REF_CURR"", start, end, conn, color = ""darkblue"")\n    k = pf.add_to_plot(p, ""TEST"", ""LAMP_TEST_CURR"", start, end, conn, color = ""goldenrod"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g,h,i,j,k])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef caa_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 600,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    a = pf.add_to_plot(p, ""CAA_VREFOFF"", ""INRSH_CAA_VREFOFF"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""CAA_VREF"", ""INRSH_CAA_VREF"", start, end, conn, color = ""green"")\n\n    #pf.add_hover_tool(p,[a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\ndef caa_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>CAA Lamp Voltages</td>\n        <td>INRSH_LAMP_SEL<br>\n            INRSI_C_CAA_VOLTAGE</td>\n        <td>Lamp Voltage for each CAA Lamp</td>\n      </tr>\n\n      <tr>\n        <td>CAA Lamp Currents</td>\n        <td>INRSH_LAMP_SEL<br>\n            INRSI_C_CAA_CURRENT</td>\n        <td>Lamp Currents for each CAA Lamp</td>\n      </tr>\n\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = lamp_volt(conn, start, end)\n    plot2 = lamp_current(conn, start, end)\n    #plot3 = caa_plots(conn, start, end)\n\n    layout = Column(descr, plot1, plot2)\n\n    tab = Panel(child = layout, title = ""CAA/LAMPS"")\n\n    return tab\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/fpe_fpa_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for Temperature tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1 - ASIC 1 Voltages\n    IGDP_NRSD_ALG_A1_VDDA\n    IGDP_NRSD_ALG_A1GND4VDA\n    IGDP_NRSD_ALG_A1GND5VRF\n    INRSD_ALG_A1_VDD3P3\n    INRSD_ALG_A1_VDD\n    INRSD_ALG_A1_REF\n    INRSD_A1_DSUB_V\n    INRSD_A1_VRESET_V\n    INRSD_A1_CELLDRN_V\n    INRSD_A1_DRAIN_V\n    INRSD_A1_VBIASGATE_V\n    INRSD_A1_VBIASPWR_V\n\n    Plot 2 - ASIC 1 Currents\n    IGDP_NRSD_ALG_A1_VDD_C\n    IGDP_NRSD_ALG_A1VDAP12C\n    IGDP_NRSD_ALG_A1VDAN12C\n    INRSD_A1_VDDA_I\n\n    Plot 3 - ASIC 2 Voltages\n    IGDP_NRSD_ALG_A2_VDDA\n    IGDP_NRSD_ALG_A2GND4VDA\n    IGDP_NRSD_ALG_A2GND5VRF\n    INRSD_ALG_A2_VDD3P3\n    INRSD_ALG_A2_VDD\n    INRSD_ALG_A2_REF\n    INRSD_A2_DSUB_V\n    INRSD_A2_VRESET_V\n    INRSD_A2_CELLDRN_V\n    INRSD_A2_DRAIN_V\n    INRSD_A2_VBIASGATE_V\n    INRSD_A2_VBIASPWR_V\n\n    Plot 4 - ASIC 2 Currents\n    IGDP_NRSD_ALG_A2_VDD_C\n    IGDP_NRSD_ALG_A2VDAP12C\n    IGDP_NRSD_ALG_A2VDAN12C\n    INRSD_A2_VDDA_I\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``nirspec_dashboard.py``, e.g.:\n\n    ::\n        from .plots.fpa_fpe_tab import fpa_fpe_plots\n        tab = fpa_fpe_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""nirspec_database.db""\n\n""""""\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.models import LinearAxis, Range1d\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.layouts import WidgetBox, gridplot, Column, Row\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef asic_1_voltages(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 800,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""ASIC 1 Voltages""\n    pf.add_basic_layout(p)\n    a = pf.add_to_plot(p, ""VDDA"", ""IGDP_NRSD_ALG_A1_VDDA"", start, end, conn, color = ""burlywood"")\n    b = pf.add_to_plot(p, ""A1GND4VDA"", ""IGDP_NRSD_ALG_A1GND4VDA"", start, end, conn, color = ""cadetblue"")\n    c = pf.add_to_plot(p, ""A1GND5VRF"", ""IGDP_NRSD_ALG_A1GND5VRF"", start, end, conn, color = ""chartreuse"")\n    d = pf.add_to_plot(p, ""A1VDD3P3"", ""INRSD_ALG_A1_VDD3P3"", start, end, conn, color = ""chocolate"")\n    e = pf.add_to_plot(p, ""VDD"", ""INRSD_ALG_A1_VDD"", start, end, conn, color = ""coral"")\n    f = pf.add_to_plot(p, ""REF"", ""INRSD_ALG_A1_REF"", start, end, conn, color = ""darkorange"")\n    g = pf.add_to_plot(p, ""DSUB_V"", ""INRSD_A1_DSUB_V"", start, end, conn, color = ""crimson"")\n    h = pf.add_to_plot(p, ""VRESET_V"", ""INRSD_A1_VRESET_V"", start, end, conn, color = ""cyan"")\n    i = pf.add_to_plot(p, ""CELLDRN_V"", ""INRSD_A1_CELLDRN_V"", start, end, conn, color = ""darkblue"")\n    j = pf.add_to_plot(p, ""DRAIN_V"", ""INRSD_A1_DRAIN_V"", start, end, conn, color = ""darkgreen"")\n    k = pf.add_to_plot(p, ""VBIASGATE_V"", ""INRSD_A1_VBIASGATE_V"", start, end, conn, color = ""darkmagenta"")\n    l = pf.add_to_plot(p, ""VBIASPWR_V"", ""INRSD_A1_VBIASPWR_V"", start, end, conn, color = ""cornflowerblue"")\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g,h,i,j,k,l])\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef asic_2_voltages(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 800,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""ASIC 2 Voltages""\n    pf.add_basic_layout(p)\n    a = pf.add_to_plot(p, ""VDDA"", ""IGDP_NRSD_ALG_A2_VDDA"", start, end, conn, color = ""burlywood"")\n    b = pf.add_to_plot(p, ""A2GND4VDA"", ""IGDP_NRSD_ALG_A2GND4VDA"", start, end, conn, color = ""cadetblue"")\n    c = pf.add_to_plot(p, ""A2GND5VRF"", ""IGDP_NRSD_ALG_A2GND5VRF"", start, end, conn, color = ""chartreuse"")\n    d = pf.add_to_plot(p, ""A2VDD3P3"", ""INRSD_ALG_A2_VDD3P3"", start, end, conn, color = ""chocolate"")\n    e = pf.add_to_plot(p, ""VDD"", ""INRSD_ALG_A2_VDD"", start, end, conn, color = ""coral"")\n    f = pf.add_to_plot(p, ""REF"", ""INRSD_ALG_A2_REF"", start, end, conn, color = ""darkorange"")\n    g = pf.add_to_plot(p, ""DSUB_V"", ""INRSD_A2_DSUB_V"", start, end, conn, color = ""crimson"")\n    h = pf.add_to_plot(p, ""VRESET_V"", ""INRSD_A2_VRESET_V"", start, end, conn, color = ""cyan"")\n    i = pf.add_to_plot(p, ""CELLDRN_V"", ""INRSD_A2_CELLDRN_V"", start, end, conn, color = ""darkblue"")\n    j = pf.add_to_plot(p, ""DRAIN_V"", ""INRSD_A2_DRAIN_V"", start, end, conn, color = ""darkgreen"")\n    k = pf.add_to_plot(p, ""VBIASGATE_V"", ""INRSD_A2_VBIASGATE_V"", start, end, conn, color = ""darkmagenta"")\n    l = pf.add_to_plot(p, ""VBIASPWR_V"", ""INRSD_A2_VBIASPWR_V"", start, end, conn, color = ""cornflowerblue"")\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g,h,i,j,k,l])\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef asic_1_currents(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Current (mA)\')\n\n    p.grid.visible = True\n    p.title.text = ""ASIC 1 Currents""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""VDD_C"", ""IGDP_NRSD_ALG_A1_VDD_C"", start, end, conn, color = ""burlywood"")\n    b = pf.add_to_plot(p, ""A1VDAP12C"", ""IGDP_NRSD_ALG_A1VDAP12C"", start, end, conn, color = ""cadetblue"")\n    c = pf.add_to_plot(p, ""A1VDAN12C"", ""IGDP_NRSD_ALG_A1VDAN12C"", start, end, conn, color = ""chartreuse"")\n    d = pf.add_to_plot(p, ""VDDA_I"", ""INRSD_A1_VDDA_I"", start, end, conn, color = ""chocolate"")\n\n    pf.add_hover_tool(p,[a,b,c,d])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef asic_2_currents(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Current (mA)\')\n\n    p.grid.visible = True\n    p.title.text = ""ASIC 2 Currents""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""VDD_C"", ""IGDP_NRSD_ALG_A2_VDD_C"", start, end, conn, color = ""burlywood"")\n    b = pf.add_to_plot(p, ""A2VDAP12C"", ""IGDP_NRSD_ALG_A2VDAP12C"", start, end, conn, color = ""cadetblue"")\n    c = pf.add_to_plot(p, ""A2VDAN12C"", ""IGDP_NRSD_ALG_A2VDAN12C"", start, end, conn, color = ""chartreuse"")\n    d = pf.add_to_plot(p, ""VDDA_I"", ""INRSD_A2_VDDA_I"", start, end, conn, color = ""chocolate"")\n\n    pf.add_hover_tool(p,[a,b,c,d])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n    return p\n\n\ndef fpe_fpa_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>ASIC (1,2) Voltages</td>\n        <td>IGDP_NRSD_ALG_A(1,2)_VDDA<br>\n            IGDP_NRSD_ALG_A(1,2)GND4VDA<br>\n            IGDP_NRSD_ALG_A(1,2)GND5VRF<br>\n            INRSD_ALG_A(1,2)_VDD3P3<br>\n            INRSD_ALG_A(1,2)_VDD<br>\n            INRSD_ALG_A(1,2)_REF<br>\n            INRSD_A(1,2)_DSUB_V<br>\n            INRSD_A(1,2)_VRESET_V<br>\n            INRSD_A(1,2)_CELLDRN_V<br>\n            INRSD_A(1,2)_DRAIN_V<br>\n            INRSD_A(1,2)_VBIASGATE_V<br>\n            INRSD_A(1,2)_VBIASPWR_V<br>\n        </td>\n        <td>\n            ASIC (1,2) VDDA Voltage<br>\n            ASIC (1,2) VDDA/Ground Voltage<br>\n            ASIC (1,2) Ref/Ground Voltage<br>\n            ASIC (1,2) VDD 3.3 Supply Voltage<br>\n            ASIC (1,2) VDD Voltage<br>\n            ASIC (1,2) Reference Voltage<br>\n            ASIC (1,2) Dsub Voltage<br>\n            ASIC (1,2) Reset Voltage<br>\n            ASIC (1,2) Cell Drain Voltage<br>\n            ASIC (1,2) Drain Voltage<br>\n            ASIC (1,2) Bias Gate Voltage<br>\n            ASIC (1,2) Bias Power Voltage<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>ASIC (1,2) Currents</td>\n        <td>IGDP_NRSD_ALG_A(1,2)_VDD_C<br>\n            IGDP_NRSD_ALG_A(1,2)VDAP12C<br>\n            IGDP_NRSD_ALG_A(1,2)VDAN12C<br>\n            INRSD_A(1,2)_VDDA_I<br>\n        </td>\n        <td>ASIC (1,2) VDD Current<br>\n            ASIC (1,2) VDDA +12V Current<br>\n            ASIC (1,2) VDDA -12V Current<br>\n            ASIC (1,2) VDDA Current<br>\n        </td>\n      </tr>\n      \n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = asic_1_voltages(conn, start, end)\n    plot2 = asic_2_voltages(conn, start, end)\n    plot3 = asic_1_currents(conn, start, end)\n    plot4 = asic_2_currents(conn, start, end)\n\n    currents = Row(plot3, plot4)\n    layout = Column(descr, plot1, plot2, currents)\n\n    tab = Panel(child = layout, title = ""FPE/FPA"")\n\n    return tab\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/msa_mce_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for Temperature tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1 - MCE Board 1 (AIC) Voltages\n    INRSM_MCE_AIC_1R5_V\n    INRSM_MCE_AIC_3R3_V\n    INRSM_MCE_AIC_5_V\n    INRSM_MCE_AIC_P12_V\n    INRSM_MCE_AIC_N12_V\n\n    Plot 2 - MCE Board 1 (AIC) Currents\n    INRSM_MCE_AIC_3R3_I\n    INRSM_MCE_AIC_5_I\n    INRSM_MCE_AIC_P12_I\n    INRSM_MCE_AIC_N12_I\n\n    Plot 3 - MCE Board 2 (MDAC) Voltages\n    INRSM_MCE_MDAC_1R5_V\n    INRSM_MCE_MDAC_3R3_V\n    INRSM_MCE_MDAC_5_V\n    INRSM_MCE_MDAC_P12_V\n    INRSM_MCE_MDAC_N12_V\n\n    Plot 4 - MCE Board 2 (MDAC) Currents\n    INRSM_MCE_MDAC_3R3_I\n    INRSM_MCE_MDAC_5_I\n    INRSM_MCE_MDAC_P12_I\n    INRSM_MCE_MDAC_N12_I\n\n    Plot (5-8) - QUAD (1-4)\n    INRSM_MSA_Q(1-4)_365VDD\n    INRSM_MSA_Q(1-4)_365VPP\n    INRSM_MSA_Q(1-4)_171VPP\n    IGDPM_MSA_Q(1-4)_365IDD\n    IGDPM_MSA_Q(1-4)_365IPP\n    IGDPM_MSA_Q(1-4)_171RTN\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``nirspec_dashboard.py``, e.g.:\n\n    ::\n        from .plots.msa_mce_tab import msa_mce_plots\n        tab = msa_mce_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""nirspec_database.db""\n\n""""""\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.models import LinearAxis, Range1d\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool, Title\nfrom bokeh.layouts import WidgetBox, gridplot, Column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef aic_voltage(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""MCE Board 1 (AIC)""\n    p.add_layout(Title(text=""Voltages"", text_font_style=""italic"", text_font_size=""12pt""), \'above\')\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""1R5_V"", ""INRSM_MCE_AIC_1R5_V"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""3R3_V"", ""INRSM_MCE_AIC_3R3_V"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""5_V"", ""INRSM_MCE_AIC_5_V"", start, end, conn, color = ""brown"")\n    d = pf.add_to_plot(p, ""P12_V"", ""INRSM_MCE_AIC_P12_V"", start, end, conn, color = ""burlywood"")\n    e = pf.add_to_plot(p, ""N12_V"", ""INRSM_MCE_AIC_N12_V"", start, end, conn, color = ""darkmagenta"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef aic_current(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Current (A)\')\n\n    p.grid.visible = True\n    p.title.text = ""MCE Board 1 (AIC)""\n    p.add_layout(Title(text=""Currents"", text_font_style=""italic"", text_font_size=""12pt""), \'above\')\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""3R3_I"", ""INRSM_MCE_AIC_3R3_I"", start, end, conn, color = ""blue"")\n    b = pf.add_to_plot(p, ""5_I"", ""INRSM_MCE_AIC_5_I"", start, end, conn, color = ""red"")\n    c = pf.add_to_plot(p, ""P12_I"", ""INRSM_MCE_AIC_P12_I"", start, end, conn, color = ""green"")\n    d = pf.add_to_plot(p, ""N12_I"", ""INRSM_MCE_AIC_N12_I"", start, end, conn, color = ""orange"")\n\n    pf.add_hover_tool(p,[a,b,c,d])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef mdac_voltage(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""MCE Board 2 (MDAC)""\n    p.add_layout(Title(text=""Voltages"", text_font_style=""italic"", text_font_size=""12pt""), \'above\')\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""1R5_V"", ""INRSM_MCE_MDAC_1R5_V"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""3R3_V"", ""INRSM_MCE_MDAC_3R3_V"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""5_V"", ""INRSM_MCE_MDAC_5_V"", start, end, conn, color = ""brown"")\n    d = pf.add_to_plot(p, ""P12_V"", ""INRSM_MCE_MDAC_P12_V"", start, end, conn, color = ""burlywood"")\n    e = pf.add_to_plot(p, ""N12_V"", ""INRSM_MCE_MDAC_N12_V"", start, end, conn, color = ""darkmagenta"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef mdac_current(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""MCE Board 2 (MDAC)""\n    p.add_layout(Title(text=""Currents"", text_font_style=""italic"", text_font_size=""12pt""), \'above\')\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""3R3_I"", ""INRSM_MCE_MDAC_3R3_I"", start, end, conn, color = ""blue"")\n    b = pf.add_to_plot(p, ""5_I"", ""INRSM_MCE_MDAC_5_I"", start, end, conn, color = ""red"")\n    c = pf.add_to_plot(p, ""P12_I"", ""INRSM_MCE_MDAC_P12_I"", start, end, conn, color = ""green"")\n    d = pf.add_to_plot(p, ""N12_I"", ""INRSM_MCE_MDAC_N12_I"", start, end, conn, color = ""orange"")\n\n    pf.add_hover_tool(p,[a,b,c,d])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef quad1_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""Quad 1""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""365VDD"", ""INRSM_MSA_Q1_365VDD"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""365VPP"", ""INRSM_MSA_Q1_365VPP"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""171VPP"", ""INRSM_MSA_Q1_171VPP"", start, end, conn, color = ""brown"")\n    d = pf.add_to_plot(p, ""365IDD"", ""IGDPM_MSA_Q1_365IDD"", start, end, conn, color = ""burlywood"")\n    e = pf.add_to_plot(p, ""365IPP"", ""IGDPM_MSA_Q1_365IPP"", start, end, conn, color = ""darkmagenta"")\n    f = pf.add_to_plot(p, ""171RTN"", ""IGDPM_MSA_Q1_171RTN"", start, end, conn, color = ""blue"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef quad2_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""Quad 2""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""365VDD"", ""INRSM_MSA_Q2_365VDD"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""365VPP"", ""INRSM_MSA_Q2_365VPP"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""171VPP"", ""INRSM_MSA_Q2_171VPP"", start, end, conn, color = ""brown"")\n    d = pf.add_to_plot(p, ""365IDD"", ""IGDPM_MSA_Q2_365IDD"", start, end, conn, color = ""burlywood"")\n    e = pf.add_to_plot(p, ""365IPP"", ""IGDPM_MSA_Q2_365IPP"", start, end, conn, color = ""darkmagenta"")\n    f = pf.add_to_plot(p, ""171RTN"", ""IGDPM_MSA_Q2_171RTN"", start, end, conn, color = ""blue"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef quad3_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""Quad 3""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""365VDD"", ""INRSM_MSA_Q3_365VDD"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""365VPP"", ""INRSM_MSA_Q3_365VPP"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""171VPP"", ""INRSM_MSA_Q3_171VPP"", start, end, conn, color = ""brown"")\n    d = pf.add_to_plot(p, ""365IDD"", ""IGDPM_MSA_Q3_365IDD"", start, end, conn, color = ""burlywood"")\n    e = pf.add_to_plot(p, ""365IPP"", ""IGDPM_MSA_Q3_365IPP"", start, end, conn, color = ""darkmagenta"")\n    f = pf.add_to_plot(p, ""171RTN"", ""IGDPM_MSA_Q3_171RTN"", start, end, conn, color = ""blue"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef quad4_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 560,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""Quad 4""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""365VDD"", ""INRSM_MSA_Q4_365VDD"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""365VPP"", ""INRSM_MSA_Q4_365VPP"", start, end, conn, color = ""orange"")\n    c = pf.add_to_plot(p, ""171VPP"", ""INRSM_MSA_Q4_171VPP"", start, end, conn, color = ""brown"")\n    d = pf.add_to_plot(p, ""365IDD"", ""IGDPM_MSA_Q4_365IDD"", start, end, conn, color = ""burlywood"")\n    e = pf.add_to_plot(p, ""365IPP"", ""IGDPM_MSA_Q4_365IPP"", start, end, conn, color = ""darkmagenta"")\n    f = pf.add_to_plot(p, ""171RTN"", ""IGDPM_MSA_Q4_171RTN"", start, end, conn, color = ""blue"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef msa_mce_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>MCE Board 1 (AIC) Voltages</td>\n        <td>INRSM_MCE_AIC_1R5_V<br>\n            INRSM_MCE_AIC_3R3_V<br>\n            INRSM_MCE_AIC_5_V<br>\n            INRSM_MCE_AIC_P12_V<br>\n            INRSM_MCE_AIC_N12_V<br>\n        </td>\n        <td>MCE AIC +1.5V Voltage<br>\n            MCE AIC +3.3V Voltage<br>\n            MCE AIC +5V Voltage<br>\n            MCE AIC +12V Voltage<br>\n            MCE AIC -12V Voltage<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>MCE Board 1 (AIC) Currents</td>\n        <td>INRSM_MCE_AIC_3R3_I<br>\n            INRSM_MCE_AIC_5_I<br>\n            INRSM_MCE_AIC_P12_I<br>\n            INRSM_MCE_AIC_N12_I<br>\n        </td>\n        <td>MCE AIC Board +3.3V Current<br>\n            MCE AIC Board +5V Current<br>\n            MCE AIC Board +12V Current<br>\n            MCE AIC Board -12V Current<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>MCE Board 2 (MDAC) Voltages</td>\n        <td>INRSM_MCE_MDAC_1R5_V<br>\n            INRSM_MCE_MDAC_3R3_V<br>\n            INRSM_MCE_MDAC_5_V<br>\n            INRSM_MCE_MDAC_P12_V<br>\n            INRSM_MCE_MDAC_N12_V<br>\n        </td>\n        <td>MCE MDAC +1.5V Voltage<br>\n            MCE MDAC +3.3V Voltage<br>\n            MCE MDAC +5V Voltage<br>\n            MCE MDAC +12V Voltage<br>\n            MCE MDAC -12V Voltage<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>MCE Board 2 (MDAC) Currents</td>\n        <td>INRSM_MCE_MDAC_3R3_I<br>\n            INRSM_MCE_MDAC_5_I<br>\n            INRSM_MCE_MDAC_P12_I<br>\n            INRSM_MCE_MDAC_N12_I<br>\n        </td>\n        <td>MCE MDAC Board +3.3V Current<br>\n            MCE MDAC Board +5V Current<br>\n            MCE MDAC Board +12V Current<br>\n            MCE MDAC Board -12V Current<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>QUAD (1-4)</td>\n        <td>INRSM_MSA_Q(1-4)_365VDD<br>\n            INRSM_MSA_Q(1-4)_365VPP<br>\n            INRSM_MSA_Q(1-4)_171VPP<br>\n            IGDPM_MSA_Q(1-4)_365IDD<br>\n            IGDPM_MSA_Q(1-4)_365IPP<br>\n            IGDPM_MSA_Q(1-4)_171RTN<br>\n        </td>\n        <td>MSA Quad (1-4) Vdd 365 Voltage<br>\n            MSA Quad (1-4) Vpp 365 Voltage<br>\n            MSA Quad (1-4) Vpp 171 Voltage<br>\n            MSA Quad (1-4) Vdd 365 Current<br>\n            MSA Quad (1-4) Vpp 365 Current<br>\n            MSA Quad (1-4) Return 171 Current<br>\n        </td>\n      </tr>\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = aic_voltage(conn, start, end)\n    plot2 = aic_current(conn, start, end)\n    plot3 = mdac_voltage(conn, start, end)\n    plot4 = mdac_current(conn, start, end)\n    plot5 = quad1_volt(conn, start, end)\n    plot6 = quad2_volt(conn, start, end)\n    plot7 = quad3_volt(conn, start, end)\n    plot8 = quad4_volt(conn, start, end)\n\n    grid = gridplot([[plot1, plot2],\n                    [plot3, plot4],\n                    [plot5, plot6],\n                    [plot7, plot8]],merge_tools=False)\n    layout = Column(descr, grid)\n\n    tab = Panel(child = layout, title = ""MSA/MCE"")\n\n    return tab\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/plot_functions.py,2,"b'#! /usr/bin/env python\n""""""Auxilary functions for plots\n\n    Module holds functions that are used for several plots.\n\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n\n\nDependencies\n------------\n\n""""""\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nfrom bokeh.plotting import figure\nfrom bokeh.models import BoxAnnotation, LinearAxis, Range1d\nfrom bokeh.embed import components\nfrom bokeh.models.widgets import Panel, Tabs\nfrom bokeh.models import ColumnDataSource, HoverTool, DatetimeTickFormatter, DatetimeTicker, SingleIntervalTicker\nfrom bokeh.models.formatters import TickFormatter\nfrom bokeh.models.tools import PanTool, SaveTool\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef pol_regression(x, y, rank):\n    \'\'\' Calculate polynominal regression of certain rank\n    Parameters\n    ----------\n    x : list\n        x parameters for regression\n    y : list\n        y parameters for regression\n    rank : int\n        rank of regression\n    Return\n    ------\n    y_poly : list\n        regression y parameters\n    \'\'\'\n    z = np.polyfit(x, y, rank)\n    f = np.poly1d(z)\n    y_poly = f(x)\n    return y_poly\n\ndef add_hover_tool(p, rend):\n    \'\'\' Append hover tool to plot\n    parameters\n    ----------\n    p : bokeh figure\n        declares where to append hover tool\n    rend : list\n        list of renderer to append hover tool\n    \'\'\'\n\n    from bokeh.models import HoverTool\n\n    #activate HoverTool for scatter plot\n    hover_tool = HoverTool( tooltips =\n    [\n        (\'Name\', \'$name\'),\n        (\'Count\', \'@data_points\'),\n        (\'Mean\', \'@average\'),\n        (\'Deviation\', \'@deviation\'),\n    ], renderers = rend)\n    #append hover tool\n    p.tools.append(hover_tool)\n\ndef add_limit_box(p, lower, upper, alpha = 0.1, color=""green""):\n    \'\'\' Adds box to plot\n    Parameters\n    ----------\n    p : bokeh figure\n        declares where to append hover tool\n    lower : float\n        lower limit of box\n    upper : float\n        upper limit of box\n    alpha : float\n        transperency of box\n    color : str\n        filling color\n    \'\'\'\n    box = BoxAnnotation(bottom = lower, top = upper, fill_alpha = alpha, fill_color = color)\n    p.add_layout(box)\n\ndef add_to_plot(p, legend, mnemonic, start, end, conn, y_axis= ""default"", color=""red"", err=\'y\'):\n    \'\'\'Add scatter and line to certain plot and activates hoover tool\n    Parameters\n    ----------\n    p : bokeh object\n        defines plot where line and scatter should be added\n    legend : str\n        will be showed in legend of plot\n    mnemonic : str\n        defines mnemonic to be plotted\n    start : datetime\n        sets start time for data query\n    end : datetime\n        sets end time for data query\n    conn : DBobject\n        connection object to database\n    y_axis : str (default=\'default\')\n        used if secon y axis is provided\n    color : str (default=\'dred\')\n        defines color for scatter and line plot\n    Return\n    ------\n    scat : plot scatter object\n        used for applying hovertools o plots\n    \'\'\'\n\n    #convert given start and end time to astropy time\n    start_str = str(Time(start).mjd)\n    end_str = str(Time(end).mjd)\n\n    #prepare and execute sql query\n    sql_c = ""SELECT * FROM ""+mnemonic+"" WHERE start_time BETWEEN ""+start_str+"" AND ""+end_str+"" ORDER BY start_time""\n    temp = pd.read_sql_query(sql_c, conn)\n\n    #put data into Dataframe and define ColumnDataSource for each plot\n    #reg = pd.DataFrame({\'reg\' : pol_regression(temp[\'start_time\'], temp[\'average\'],3)})\n    #temp = pd.concat([temp, reg], axis = 1)\n    temp[\'start_time\'] = pd.to_datetime( Time(temp[\'start_time\'], format = ""mjd"").datetime )\n    plot_data = ColumnDataSource(temp)\n\n    #plot data\n    p.line(x = ""start_time"", y = ""average"", color = color, y_range_name=y_axis, legend = legend, source = plot_data)\n    scat = p.scatter(x = ""start_time"", y = ""average"", name = mnemonic, color = color, y_range_name=y_axis, legend = legend, source = plot_data)\n\n    #generate error lines if wished\n    if err != \'n\':\n        #generate error bars\n        err_xs = []\n        err_ys = []\n\n        for index, item in temp.iterrows():\n            err_xs.append((item[\'start_time\'], item[\'start_time\']))\n            err_ys.append((item[\'average\'] - item[\'deviation\'], item[\'average\'] + item[\'deviation\']))\n\n        # plot them\n        p.multi_line(err_xs, err_ys, color = color, legend = legend)\n\n    return scat\n\ndef add_to_plot_normalized(p, legend, mnemonic, start, end, conn, nominal, color = ""red""):\n    \'\'\'Add line plot to figure (for wheelpositions)\n    Parameters\n    ----------\n    p : bokeh object\n        defines figure where line schould be plotted\n    legend : str\n        will be showed in legend of plot\n    mnemonic : str\n        defines mnemonic to be plotted\n    start : datetime\n        sets start time for data query\n    end : datetime\n        sets end time for data query\n    conn : DBobject\n        connection object to database\n    color : str (default=\'dred\')\n        defines color for scatter and line plot\n    \'\'\'\n\n    start_str = str(Time(start).mjd)\n    end_str = str(Time(end).mjd)\n\n    sql_c = ""SELECT * FROM ""+mnemonic+"" WHERE timestamp BETWEEN ""+start_str+"" AND ""+end_str+"" ORDER BY timestamp""\n    temp = pd.read_sql_query(sql_c, conn)\n\n    #normalize values\n    temp[\'value\'] -= nominal\n    #temp[\'value\'] -= 1\n\n    temp[\'timestamp\'] = pd.to_datetime( Time(temp[\'timestamp\'], format = ""mjd"").datetime )\n    plot_data = ColumnDataSource(temp)\n\n    p.line(x = ""timestamp"", y = ""value"", color = color, legend = legend, source = plot_data)\n    p.scatter(x = ""timestamp"", y = ""value"", color = color, legend = legend, source = plot_data)\n\ndef add_basic_layout(p):\n    \'\'\'Add basic layout to certain plot\n    Parameters\n    ----------\n    p : bokeh object\n        defines plot where line and scatter should be added\n    \'\'\'\n    p.title.align = ""left""\n    p.title.text_color = ""#c85108""\n    p.title.text_font_size = ""25px""\n    p.background_fill_color = ""#efefef""\n\n    p.xaxis.axis_label_text_font_size = ""14pt""\n    p.xaxis.axis_label_text_color =\'#2D353C\'\n    p.yaxis.axis_label_text_font_size = ""14pt""\n    p.yaxis.axis_label_text_color = \'#2D353C\'\n\n    p.xaxis.major_tick_line_color = ""firebrick""\n    p.xaxis.major_tick_line_width = 2\n    p.xaxis.minor_tick_line_color = ""#c85108""\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/power_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for POWER tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1 - ICE Power Data\n    GP_ZPSVOLT\n    SE_ZINRSICEA / SE_ZINRSICEB\n    INRSH_HK_P15V\n    INRSH_HK_N15V\n    INRSH_HK_VMOTOR\n    INRSH_HK_P5V\n    INRSH_HK_2P5V\n    INRSH_HK_ADCTGAIN\n    INRSH_HK_ADCTOFFSET\n    INRSH_OA_VREFOFF\n    INRSH_OA_VREF\n\n    Plot 2 - MCE Power Data\n    GP_ZPSVOLT\n    SE_ZINRSMCEA / SE_ZINRSMCEB\n\n    Plot 3 - FPE Power Data\n    GP_ZPSVOLT\n    SE_ZINRSFPEA / SE_ZINRSFPEB\n    INRSD_ALG_ACC_P12C\n    INRSD_ALG_ACC_N12C\n    INRSD_ALG_ACC_3D3_1D5_C\n    INRSD_ALG_CHASSIS\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``nirspec_dashboard.py``, e.g.:\n\n    ::\n        from .plots.power_tab import power_plots\n        tab = power_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""miri_database.db""\n\n""""""\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.models import LinearAxis, Range1d\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.layouts import WidgetBox, gridplot, Column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef ice_power(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                y_range = [-20, 20],\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""ICE Power Parameters""\n    pf.add_basic_layout(p)\n\n    p.extra_y_ranges = {""current"": Range1d(start = 0, end=0.8)}\n    #a = pf.add_to_plot(p, ""In_VOlt"", ""GP_ZPSVOLT"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""ICE A current"", ""SE_ZINRSICEA"", start, end, conn, color = ""blue"", y_axis=""current"")\n    c = pf.add_to_plot(p, ""P15V"", ""INRSH_HK_P15V"", start, end, conn, color = ""red"")\n    d = pf.add_to_plot(p, ""N15V"", ""INRSH_HK_N15V"", start, end, conn, color = ""orange"")\n    e = pf.add_to_plot(p, ""VMOTOR"", ""INRSH_HK_VMOTOR"", start, end, conn, color = ""burlywood"")\n    f = pf.add_to_plot(p, ""P5V"", ""INRSH_HK_P5V"", start, end, conn, color = ""green"")\n    g = pf.add_to_plot(p, ""2P5V"", ""INRSH_HK_2P5V"", start, end, conn, color = ""darkgreen"")\n    h = pf.add_to_plot(p, ""ADCTGAIN"", ""INRSH_HK_ADCTGAIN"", start, end, conn, color = ""brown"")\n    i = pf.add_to_plot(p, ""ADCOFFSET"", ""INRSH_HK_ADCTOFFSET"", start, end, conn, color = ""navy"")\n    p.add_layout(LinearAxis(y_range_name = ""current"", axis_label = ""Current (A)"", axis_label_text_color = ""blue""), \'right\')\n\n    pf.add_hover_tool(p,[b,c,d,e,g,f,h,i])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef mce_power(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 400,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Current (A)\')\n\n    p.grid.visible = True\n    p.title.text = ""MCE Power Parameters""\n    pf.add_basic_layout(p)\n\n    b = pf.add_to_plot(p, ""MCE A current"", ""SE_ZINRSMCEA"", start, end, conn, color = ""blue"")\n\n    pf.add_hover_tool(p,[b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef fpe_power(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                y_range = [-30,280],\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPE Power Parameters""\n    pf.add_basic_layout(p)\n\n\n    p.extra_y_ranges = {""current"": Range1d(start = 0, end=0.8)}\n    #a = pf.add_to_plot(p, ""In_VOlt"", ""GP_ZPSVOLT"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""FPE A current"", ""SE_ZINRSFPEA"", start, end, conn, color = ""blue"", y_axis=""current"")\n    c = pf.add_to_plot(p, ""P12C"", ""INRSD_ALG_ACC_P12C"", start, end, conn, color = ""red"")\n    d = pf.add_to_plot(p, ""N15V"", ""INRSH_HK_N15V"", start, end, conn, color = ""orange"")\n    e = pf.add_to_plot(p, ""N12C"", ""INRSD_ALG_ACC_N12C"", start, end, conn, color = ""burlywood"")\n    f = pf.add_to_plot(p, ""1D5"", ""INRSD_ALG_ACC_3D3_1D5_C"", start, end, conn, color = ""green"")\n    g = pf.add_to_plot(p, ""Chassis"", ""INRSD_ALG_CHASSIS"", start, end, conn, color = ""purple"")\n    p.add_layout(LinearAxis(y_range_name = ""current"", axis_label = ""Current (A)"", axis_label_text_color = ""blue""), \'right\')\n\n    pf.add_hover_tool(p,[b,c,d,e,f,g])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\n\ndef power_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>ICE Power Parameters</td>\n        <td>GP_ZPSVOLT (missing)<br>\n            SE_ZINRSICEA<br>\n            INRSH_HK_P15V<br>\n            INRSH_HK_N15V<br>\n            INRSH_HK_VMOTOR<br>\n            INRSH_HK_P5V<br>\n            INRSH_HK_2P5V<br>\n            INRSH_HK_ADCTGAIN<br>\n            INRSH_HK_ADCTOFFSET<br>\n            INRSH_OA_VREFOFF<br>\n            INRSH_OA_VREF<br>\n        </td>\n        <td>ICE Input Voltage<br>\n            ICE Input Current (A side)<br>\n            ICE +15V Voltage<br>\n            ICE -15V Voltage<br>\n            ICE Motor Voltage<br>\n            ICE +5V FPGA Voltage<br>\n            ICE +2V5 FPGA Voltage<br>\n            ICE ADC TM Chain Gain for Calibration<br>\n            ICE ADC TM Chain Offset for Calibration<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>MCE Power Parameters</td>\n        <td>GP_ZPSVOLT (missing)<br>\n            SE_ZINRSMCEA\n        </td>\n        <td>ICE Input Voltage<br>\n            MCE Input Current (A side)<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>FPE Power Parameters</td>\n        <td>GP_ZPSVOLT (missing)<br>\n            SE_ZINRSFPEA<br>\n            INRSD_ALG_ACC_P12C<br>\n            INRSD_ALG_ACC_N12C<br>\n            INRSD_ALG_ACC_3D3_1D5_C<br>\n            INRSD_ALG_CHASSIS<br>\n        </td>\n        <td>ICE Input Voltage<br>\n            MCE Input Current (A side)<br>\n            ACC +12V Current<br>\n            ACC -12V Current<br>\n            ACC 3.3/1.5 Supply Current<br>\n            Chassis Voltage<br>\n        </td>\n      </tr>\n\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = ice_power(conn, start, end)\n    plot2 = mce_power(conn, start, end)\n    plot3 = fpe_power(conn, start, end)\n\n    layout = Column(descr, plot1, plot2, plot3)\n\n    tab = Panel(child = layout, title = ""POWER"")\n\n    return tab\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/temperature_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for Temperature tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1 - IRSU monitored temps\n    SI_GZCTS75A / SI_GZCTS75B\n    SI_GZCTS68A / SI_GZCTS68B\n    SI_GZCTS81A / SI_GZCTS81B\n    SI_GZCTS80A / SI_GZCTS80B\n    SI_GZCTS76A / SI_GZCTS76B\n    SI_GZCTS79A / SI_GZCTS79B\n    SI_GZCTS77A / SI_GZCTS77B\n    SI_GZCTS78A / SI_GZCTS78B\n    SI_GZCTS69A / SI_GZCTS69B\n\n    Plot 2 - Box Temps\n    IGDP_NRSD_ALG_TEMP\n    INRSH_HK_TEMP1\n    INRSH_HK_TEMP2\n\n    Plot 3 - FPE Power Data\n    IGDP_NRSI_C_CAM_TEMP\n    IGDP_NRSI_C_COL_TEMP\n    IGDP_NRSI_C_COM1_TEMP\n    IGDP_NRSI_C_FOR_TEMP\n    IGDP_NRSI_C_IFU_TEMP\n    IGDP_NRSI_C_BP1_TEMP\n    IGDP_NRSI_C_BP2_TEMP\n    IGDP_NRSI_C_BP3_TEMP\n    IGDP_NRSI_C_BP4_TEMP\n    IGDP_NRSI_C_RMA_TEMP\n    IGDP_NRSI_C_CAAL1_TEMP\n    IGDP_NRSI_C_CAAL2_TEMP\n    IGDP_NRSI_C_CAAL3_TEMP\n    IGDP_NRSI_C_CAAL4_TEMP\n    IGDP_NRSI_C_FWA_TEMP\n    IGDP_NRSI_C_GWA_TEMP\n\n    Plot 4 - MCE internal Temp\n    INRSM_MCE_PCA_TMP1\n    INRSM_MCE_PCA_TMP2\n    INRSM_MCE_AIC_TMP_FPGA\n    INRSM_MCE_AIC_TMP_ADC\n    INRSM_MCE_AIC_TMP_VREG\n    INRSM_MCE_MDAC_TMP_FPGA\n    INRSM_MCE_MDAC_TMP_OSC\n    INRSM_MCE_MDAC_TMP_BRD\n    INRSM_MCE_MDAC_TMP_PHA\n    INRSM_MCE_MDAC_TMP_PHB\n\n    Plot 5 - MSA Temp\n    INRSM_Q1_TMP_A\n    INRSM_Q2_TMP_A\n    INRSM_Q3_TMP_A\n    INRSM_Q4_TMP_A\n    INRSM_MECH_MTR_TMP_A\n    INRSM_LL_MTR_TMP_A\n    INRSM_MSA_TMP_A\n\n    Plot 6 - FPA Temp\n    IGDP_NRSD_ALG_FPA_TEMP\n    IGDP_NRSD_ALG_A1_TEMP\n    IGDP_NRSD_ALG_A2_TEMP\n\n    Plot 7 - Heat Strap Temps (Trim heaters)\n    SI_GZCTS74A / SI_GZCTS74B\n    SI_GZCTS67A / SI_GZCTS67B\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``nirspec_dashboard.py``, e.g.:\n\n    ::\n        from .plots.temperature_tab import temperature_plots\n        tab = temperature_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""nirspec_database.db""\n\n""""""\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.models import LinearAxis, Range1d\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.layouts import WidgetBox, gridplot, Column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef irsu_temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""IRSU monitored Temperatures""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""75A"", ""SI_GZCTS75A"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""68A"", ""SI_GZCTS68A"", start, end, conn, color = ""green"")\n    c = pf.add_to_plot(p, ""81A"", ""SI_GZCTS81A"", start, end, conn, color = ""blue"")\n    d = pf.add_to_plot(p, ""80A"", ""SI_GZCTS80A"", start, end, conn, color = ""orange"")\n    e = pf.add_to_plot(p, ""76A"", ""SI_GZCTS76A"", start, end, conn, color = ""brown"")\n    f = pf.add_to_plot(p, ""79A"", ""SI_GZCTS79A"", start, end, conn, color = ""cyan"")\n    g = pf.add_to_plot(p, ""77A"", ""SI_GZCTS77A"", start, end, conn, color = ""darkmagenta"")\n    h = pf.add_to_plot(p, ""78A"", ""SI_GZCTS78A "", start, end, conn, color = ""burlywood"")\n    i = pf.add_to_plot(p, ""69A"", ""SI_GZCTS69A "", start, end, conn, color = ""chocolate"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g,h,i])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef fpe_temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPE Temperatures""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""CAM"", ""IGDP_NRSI_C_CAM_TEMP"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""COL"", ""IGDP_NRSI_C_COL_TEMP"", start, end, conn, color = ""green"")\n    c = pf.add_to_plot(p, ""COM1"", ""IGDP_NRSI_C_COM1_TEMP"", start, end, conn, color = ""blue"")\n    d = pf.add_to_plot(p, ""FOR"", ""IGDP_NRSI_C_FOR_TEMP"", start, end, conn, color = ""darkorange"")\n    e = pf.add_to_plot(p, ""IFU"", ""IGDP_NRSI_C_IFU_TEMP"", start, end, conn, color = ""cyan"")\n    f = pf.add_to_plot(p, ""BP1"", ""IGDP_NRSI_C_BP1_TEMP"", start, end, conn, color = ""darkmagenta"")\n    g = pf.add_to_plot(p, ""BP2"", ""IGDP_NRSI_C_BP2_TEMP"", start, end, conn, color = ""burlywood"")\n    h = pf.add_to_plot(p, ""BP3"", ""IGDP_NRSI_C_BP3_TEMP"", start, end, conn, color = ""brown"")\n    i = pf.add_to_plot(p, ""BP4"", ""IGDP_NRSI_C_BP4_TEMP"", start, end, conn, color = ""chocolate"")\n    j = pf.add_to_plot(p, ""RMA"", ""IGDP_NRSI_C_RMA_TEMP"", start, end, conn, color = ""darkgreen"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g,h,i,j])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef caal_temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""CAA Lamps / FWA, GWA""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""CAAL1"", ""IGDP_NRSI_C_CAAL1_TEMP"", start, end, conn, color = ""darkblue"")\n    b = pf.add_to_plot(p, ""CAAL2"", ""IGDP_NRSI_C_CAAL2_TEMP"", start, end, conn, color = ""magenta"")\n    c = pf.add_to_plot(p, ""CAAL3"", ""IGDP_NRSI_C_CAAL3_TEMP"", start, end, conn, color = ""mediumaquamarine"")\n    d = pf.add_to_plot(p, ""CAAL4"", ""IGDP_NRSI_C_CAAL4_TEMP"", start, end, conn, color = ""goldenrod"")\n    e = pf.add_to_plot(p, ""FWA"", ""IGDP_NRSI_C_FWA_TEMP"", start, end, conn, color = ""darkseagreen"")\n    f = pf.add_to_plot(p, ""GWA"", ""IGDP_NRSI_C_GWA_TEMP"", start, end, conn, color = ""darkkhaki"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef box_temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""Box Temperatures""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""ALG_TEMP"", ""IGDP_NRSD_ALG_TEMP"", start, end, conn, color = ""red"")\n    b = pf.add_to_plot(p, ""HK_TEMP1"", ""INRSH_HK_TEMP1"", start, end, conn, color = ""green"")\n    c = pf.add_to_plot(p, ""HK_TEMP2"", ""INRSH_HK_TEMP2"", start, end, conn, color = ""blue"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef mce_internal_temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""MCE internal Temperatures""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""PCA_TMP1"", ""INRSM_MCE_PCA_TMP1"", start, end, conn, color = ""green"")\n    b = pf.add_to_plot(p, ""PCA_TMP2"", ""INRSM_MCE_PCA_TMP2"", start, end, conn, color = ""blue"")\n    c = pf.add_to_plot(p, ""FPGA_AIC"", ""INRSM_MCE_AIC_TMP_FPGA"", start, end, conn, color = ""brown"")\n    d = pf.add_to_plot(p, ""ADC_AIC"", ""INRSM_MCE_AIC_TMP_ADC"", start, end, conn, color = ""red"")\n    e = pf.add_to_plot(p, ""VREG_AIC"", ""INRSM_MCE_AIC_TMP_VREG"", start, end, conn, color = ""hotpink"")\n    f = pf.add_to_plot(p, ""FPGA_MDAC"", ""INRSM_MCE_MDAC_TMP_FPGA"", start, end, conn, color = ""cadetblue"")\n    g = pf.add_to_plot(p, ""OSC_MDAC"", ""INRSM_MCE_MDAC_TMP_OSC"", start, end, conn, color = ""navy"")\n    h = pf.add_to_plot(p, ""BRD_MDAC"", ""INRSM_MCE_MDAC_TMP_BRD"", start, end, conn, color = ""darkgreen"")\n    i = pf.add_to_plot(p, ""PHA_MDAC"", ""INRSM_MCE_MDAC_TMP_PHA"", start, end, conn, color = ""magenta"")\n    j = pf.add_to_plot(p, ""PHB_MDAC"", ""INRSM_MCE_MDAC_TMP_PHB"", start, end, conn, color = ""orange"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g,h,i,j])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\ndef msa_temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""MSA Temperatures""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""Q1_TEMP"", ""INRSM_Q1_TMP_A"", start, end, conn, color = ""green"")\n    b = pf.add_to_plot(p, ""Q2_TEMP"", ""INRSM_Q2_TMP_A"", start, end, conn, color = ""red"")\n    c = pf.add_to_plot(p, ""Q3_TEMP"", ""INRSM_Q3_TMP_A"", start, end, conn, color = ""blue"")\n    d = pf.add_to_plot(p, ""Q4_TEMP"", ""INRSM_Q4_TMP_A"", start, end, conn, color = ""brown"")\n    e = pf.add_to_plot(p, ""MECH_MTR"", ""INRSM_MECH_MTR_TMP_A"", start, end, conn, color = ""orange"")\n    f = pf.add_to_plot(p, ""LL_MTR"", ""INRSM_LL_MTR_TMP_A"", start, end, conn, color = ""darkmagenta"")\n    g = pf.add_to_plot(p, ""MSA"", ""INRSM_MSA_TMP_A"", start, end, conn, color = ""indigo"")\n\n    pf.add_hover_tool(p,[a,b,c,d,e,f,g])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\ndef fpa_temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""FPA Temperatures""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""ALG_FPA"", ""IGDP_NRSD_ALG_FPA_TEMP"", start, end, conn, color = ""green"")\n    b = pf.add_to_plot(p, ""ALG_A1"", ""IGDP_NRSD_ALG_A1_TEMP"", start, end, conn, color = ""red"")\n    c = pf.add_to_plot(p, ""ALG_A2"", ""IGDP_NRSD_ALG_A2_TEMP"", start, end, conn, color = ""blue"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\ndef heat_strap_temp(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 700,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Temperature (K)\')\n\n    p.grid.visible = True\n    p.title.text = ""Heat Strap Temperatures (Trim heaters)""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""74A"", ""SI_GZCTS74A"", start, end, conn, color = ""green"")\n    b = pf.add_to_plot(p, ""67A"", ""SI_GZCTS67A"", start, end, conn, color = ""red"")\n\n    pf.add_hover_tool(p,[a,b])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n\n    return p\n\n\ndef temperature_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>IRSU monitored Temperatures</td>\n        <td>SI_GZCTS75A<br>\n            SI_GZCTS68A<br>\n            SI_GZCTS81A<br>\n            SI_GZCTS80A<br>\n            SI_GZCTS76A<br>\n            SI_GZCTS79A<br>\n            SI_GZCTS77A<br>\n            SI_GZCTS78A<br>\n            SI_GZCTS69A</td>\n        <td>CAA IRSU Temperature<br>\n            CAM IRSU Temperature<br>\n            COM1 Nominal IRSU Temperature<br>\n            COM1 Redundant IRSU Temperature<br>\n            FWA IRSU Temperature<br>\n            GWA IRSU Temperature<br>\n            Thermal Strap Nominal IRSU Temperature<br>\n            Thermal Strap Redundant IRSU Temperature<br>\n            MSA Nominal IRSU Temperature<br>\n            MSA Redundant IRSU Temperature</td>\n      </tr>\n\n      <tr>\n        <td>FPE Temperatures/td>\n        <td>IGDP_NRSI_C_CAM_TEMP<br>\n            IGDP_NRSI_C_COL_TEMP<br>\n            IGDP_NRSI_C_COM1_TEMP<br>\n            IGDP_NRSI_C_FOR_TEMP<br>\n            IGDP_NRSI_C_IFU_TEMP<br>\n            IGDP_NRSI_C_BP1_TEMP<br>\n            IGDP_NRSI_C_BP2_TEMP<br>\n            IGDP_NRSI_C_BP3_TEMP<br>\n            IGDP_NRSI_C_BP4_TEMP<br>\n            IGDP_NRSI_C_RMA_TEMP</td>\n        <td>OA CAM Temperature<br>\n            OA COL Temperature<br>\n            OA COM1 Temperature<br>\n            OA FOR Temperature<br>\n            OA IFU Temperature<br>\n            OA BP1 Temperature<br>\n            OA BP2 Temperature<br>\n            OA BP3 Temperature<br>\n            OA BP4 Temperature<br>\n            OA RMA Temperature</td>\n      </tr>\n\n      <tr>\n        <td>Box Temperatures</td>\n        <td>IGDP_NRSD_ALG_TEMP<br>\n            INRSH_HK_TEMP1<br>\n            INRSH_HK_TEMP2</td>\n        <td>ICE Internal Temperature 1<br>\n            ICE Internal Temperature 2</td>\n      </tr>\n\n      <tr>\n        <td>MCE internal Temperatures</td>\n        <td>INRSM_MCE_PCA_TMP1<br>\n            INRSM_MCE_PCA_TMP2<br>\n            INRSM_MCE_AIC_TMP_FPGA<br>\n            INRSM_MCE_AIC_TMP_ADC<br>\n            INRSM_MCE_AIC_TMP_VREG<br>\n            INRSM_MCE_MDAC_TMP_FPGA<br>\n            INRSM_MCE_MDAC_TMP_OSC<br>\n            INRSM_MCE_MDAC_TMP_BRD<br>\n            INRSM_MCE_MDAC_TMP_PHA<br>\n            INRSM_MCE_MDAC_TMP_PHB</td>\n        <td>MCE PCA Board Temperature 1<br>\n            MCE PCA Board Temperature 2<br>\n            MCE AIC Board FPGA Temperature<br>\n            MCE AIC Board Analog/Digital Converter Temperature<br>\n            MCE AIC Board Voltage Regulator Temperature<br>\n            MCE MDAC Board FPGA Temperature<br>\n            MCE MDAC Board Oscillator Temperature<br>\n            MCE MDAC Board Temperature<br>\n            MCE MDAC Board Phase A PA10 Temperature<br>\n            MCE MDAC Board Phase B PA10 Temperature</td>\n      </tr>\n\n      <tr>\n        <td>MSA Temperatures</td>\n        <td>INRSM_Q1_TMP_A<br>\n            INRSM_Q2_TMP_A<br>\n            INRSM_Q3_TMP_A<br>\n            INRSM_Q4_TMP_A<br>\n            INRSM_MECH_MTR_TMP_A<br>\n            INRSM_LL_MTR_TMP_A<br>\n            INRSM_MSA_TMP_A</td>\n        <td>MSA Quad 1 Temperature<br>\n            MSA Quad 2 Temperature<br>\n            MSA Quad 3 Temperature<br>\n            MSA Quad 4 Temperature<br>\n            MSA Magnetic Arm Motor Temperature<br>\n            MSA Launch Lock Motor Temperature<br>\n            MSA Frame Temperature</td>\n      </tr>\n\n      <tr>\n        <td>FPA Temperatures</td>\n        <td>IGDP_NRSD_ALG_FPA_TEMP<br>\n            IGDP_NRSD_ALG_A1_TEMP<br>\n            IGDP_NRSD_ALG_A2_TEMP</td>\n        <td>FPE Temperature<br>\n            FPA Temperature<br>\n            ASIC 1 Temperature<br>\n            ASIC 2 Temperature</td>\n      </tr>\n\n      <tr>\n        <td>Heat Strap Temperatures (Trim Heaters)</td>\n        <td>SI_GZCTS74A<br>\n            SI_GZCTS67A</td>\n        <td>FPA TH-Strap A Temperature from IRSU A<br>\n            FPA TH-Strap B Temperature from IRSU A</td>\n      </tr>\n\n      <tr>\n        <td>CAA Lamps / FWA,GWA</td>\n        <td>IGDP_NRSI_C_CAAL1_TEMP<br>\n            IGDP_NRSI_C_CAAL2_TEMP<br>\n            IGDP_NRSI_C_CAAL3_TEMP<br>\n            IGDP_NRSI_C_CAAL4_TEMP<br>\n            IGDP_NRSI_C_FWA_TEMP<br>\n            IGDP_NRSI_C_GWA_TEMP</td>\n        <td>CAA Temperature LINE1<br>\n            CAA Temperature LINE2<br>\n            CAA Temperature LINE3<br>\n            CAA Temperature LINE4<br>\n            FWA Temperature Sensor Value<br>\n            GWA Temperature Sensor Value</td>\n      </tr>\n\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = irsu_temp(conn, start, end)\n    plot2 = fpe_temp(conn, start, end)\n    plot3 = box_temp(conn, start, end)\n    plot4 = mce_internal_temp(conn, start, end)\n    plot5 = msa_temp(conn, start, end)\n    plot6 = fpa_temp(conn, start, end)\n    plot7 = heat_strap_temp(conn, start, end)\n    plot8 = caal_temp(conn, start, end)\n\n    layout = Column(descr, plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8)\n\n    tab = Panel(child = layout, title = ""TEMPERATURE"")\n\n    return tab\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/voltage_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for Ref. Voltage/Currents tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1 - Ref Voltages\n    INRSH_FWA_MOTOR_VREF\n    INRSH_GWA_MOTOR_VREF\n    INRSH_OA_VREF\n\n    Plot 2 - ADCMGAIN (Voltages)\n    INRSH_FWA_ADCMGAIN\n    INRSH_GWA_ADCMGAIN\n    INRSH_RMA_ADCMGAIN\n\n    Plot 3 - OFFSET (Voltages)\n    INRSH_GWA_ADCMOFFSET\n    INRSH_FWA_ADCMOFFSET\n    INRSH_OA_VREFOFF\n    INRSH_RMA_ADCMOFFSET\n\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``nirspec_dashboard.py``, e.g.:\n\n    ::\n        from .plots.voltage_tab import voltage_plots\n        tab = voltage_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""nirpsec_database.db""\n\n""""""\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.plots.plot_functions as pf\nfrom bokeh.models import LinearAxis, Range1d\nfrom bokeh.plotting import figure\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource, HoverTool\nfrom bokeh.layouts import WidgetBox, gridplot, Column\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef ref_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    \n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""Ref Voltages""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""FWA_MOTOR_VREF"", ""INRSH_FWA_MOTOR_VREF"", start, end, conn, color = ""green"")\n    b = pf.add_to_plot(p, ""GWA_MOTOR_VREF"", ""INRSH_GWA_MOTOR_VREF"", start, end, conn, color = ""blue"")\n    c = pf.add_to_plot(p, ""OA_VREF"", ""INRSH_OA_VREF"", start, end, conn, color = ""red"")\n\n    pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n    return p\n\n\ndef gain_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""ADCMAIN""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""FWA_ADCMGAIN"", ""INRSH_FWA_ADCMGAIN"", start, end, conn, color = ""green"")\n    b = pf.add_to_plot(p, ""GWA_ADCMGAIN"", ""INRSH_GWA_ADCMGAIN"", start, end, conn, color = ""blue"")\n    c = pf.add_to_plot(p, ""RMA_ADCMGAIN"", ""INRSH_RMA_ADCMGAIN"", start, end, conn, color = ""red"")\n\n\n    #pf.add_hover_tool(p,[a,b,c])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n    return p\n\ndef offset_volt(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",\n                toolbar_location = ""above"",\n                plot_width = 1120,\n                plot_height = 500,\n                x_axis_type = \'datetime\',\n                output_backend = ""webgl"",\n                x_axis_label = \'Date\', y_axis_label=\'Voltage (V)\')\n\n    p.grid.visible = True\n    p.title.text = ""OFFSET""\n    pf.add_basic_layout(p)\n\n    a = pf.add_to_plot(p, ""GWA_ADCMOFFSET"", ""INRSH_GWA_ADCMOFFSET"", start, end, conn, color = ""blue"")\n    b = pf.add_to_plot(p, ""FWA_ADCMOFFSET"", ""INRSH_FWA_ADCMOFFSET"", start, end, conn, color = ""green"")\n    c = pf.add_to_plot(p, ""OA_VREFOFF"", ""INRSH_OA_VREFOFF"", start, end, conn, color = ""orange"")\n    d = pf.add_to_plot(p, ""RMA_ADCMOFFSET"", ""INRSH_RMA_ADCMOFFSET"", start, end, conn, color = ""red"")\n\n    pf.add_hover_tool(p,[a,b,c,d])\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = ""horizontal""\n\n    return p\n\n\ndef volt_plots(conn, start, end):\n    \'\'\'Combines plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n      <tr>\n        <td>Ref Voltages</td>\n        <td>INRSH_FWA_MOTOR_VREF<br>\n            INRSH_GWA_MOTOR_VREF<br>\n            INRSH_OA_VREF</td>\n        <td>FWA Motor Reference Voltage for Calibration<br>\n            GWA Motor Reference Voltage for Calibration<br>\n            OA/RMA Reference Voltage for TM Calibration<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>ADCMGAIN</td>\n        <td>INRSH_FWA_ADCMGAIN<br>\n            INRSH_GWA_ADCMGAIN<br>\n            INRSH_RMA_ADCMGAIN</td>\n        <td>FWA ADC Motor Chain Gain for Calibration<br>\n            GWA ADC Motor Chain Gain for Calibration<br>\n            RMA ADC Motor Chain Gain for Calibration<br>\n        </td>\n      </tr>\n\n      <tr>\n        <td>OFFSET</td>\n        <td>INRSH_FWA_ADCMOFFSET<br>\n            INRSH_GWA_ADCMOFFSET<br>\n            INRSH_OA_VREFOFF<br>\n            INRSH_RMA_ADCMOFFSET</td>\n        <td>FWA ADC Motor Chain Offset for Calibration<br>\n            GWA ADC Motor Chain Offset for Calibration<br>\n            CAA Reference Voltage Offset for TM Calibration<br>\n            RMA ADC Motor Chain Offset for Calibration<br>\n        </td>\n      </tr>\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = ref_volt(conn, start, end)\n    plot2 = gain_volt(conn, start, end)\n    plot3 = offset_volt(conn, start, end)\n\n    layout = Column(descr, plot1, plot2, plot3)\n\n    tab = Panel(child = layout, title = ""REF VOLTAGES"")\n\n    return tab\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/plots/wheel_tab.py,0,"b'#! /usr/bin/env python\n""""""Prepares plots for WHEEL tab\n\n    Module prepares plots for mnemonics below. Combines plots in a grid and\n    returns tab object.\n\n    Plot 1 - Filterwheel\n    INRSI_FWA_MECH_POS\n    INRSI_C_FWA_POSITION\n\n    Plot 2 - Gratingwheel X\n    INRSI_GWA_MECH_POS\n    INRSI_C_GWA_X_POSITION\n\n    Plot 3 - Gratingwheel Y\n    INRSI_GWA_MECH_POS\n    INRSI_C_GWA_Y_POSITION\n\nAuthors\n-------\n    - Daniel K\xc3\xbchbacher\n\nUse\n---\n    The functions within this module are intended to be imported and\n    used by ``dashboard.py``, e.g.:\n\n    ::\n        from .plots.wheel_ratio_tab import wheel_plots\n        tab = wheel_plots(conn, start, end)\n\nDependencies\n------------\n    User must provide database ""miri_database.db""\n\n""""""\n\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.sql_interface as sql\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.plots.plot_functions as pf\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.mnemonics as mn\nfrom bokeh.plotting import figure\nfrom bokeh.models import BoxAnnotation, LinearAxis, Range1d\nfrom bokeh.embed import components\nfrom bokeh.models.widgets import Panel, Tabs, Div\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import column, row, WidgetBox\n\nimport pandas as pd\nimport numpy as np\n\nfrom astropy.time import Time\n\n\ndef fw(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [-3,3],                                   \\\n                x_axis_type = \'datetime\',                           \\\n                x_axis_label = \'Date\', y_axis_label = \'mV (normalized)\')\n\n    p.grid.visible = True\n    p.title.text = ""Filterwheel""\n    p.title.align = ""left""\n    pf.add_basic_layout(p)\n\n    pf.add_to_plot_normalized(p, ""F110W"", ""INRSI_C_FWA_POSITION_F110W"", start, end, conn, mn.fw_nominals[\'F110W\'], color = ""green"")\n    pf.add_to_plot_normalized(p, ""F100LP"", ""INRSI_C_FWA_POSITION_F100LP"", start, end, conn, mn.fw_nominals[\'F100LP\'], color = ""red"")\n    pf.add_to_plot_normalized(p, ""F140X"", ""INRSI_C_FWA_POSITION_F140X"", start, end, conn, mn.fw_nominals[\'F140X\'], color = ""blue"")\n    pf.add_to_plot_normalized(p, ""OPAQUE"", ""INRSI_C_FWA_POSITION_OPAQUE"", start, end, conn, mn.fw_nominals[\'OPAQUE\'], color = ""orange"")\n    pf.add_to_plot_normalized(p, ""F290LP"", ""INRSI_C_FWA_POSITION_F290LP"", start, end, conn, mn.fw_nominals[\'F290LP\'], color = ""purple"")\n    pf.add_to_plot_normalized(p, ""F170LP"", ""INRSI_C_FWA_POSITION_F170LP"", start, end, conn, mn.fw_nominals[\'F170LP\'], color = ""brown"")\n    pf.add_to_plot_normalized(p, ""CLEAR"", ""INRSI_C_FWA_POSITION_CLEAR"", start, end, conn, mn.fw_nominals[\'CLEAR\'], color = ""chocolate"")\n    pf.add_to_plot_normalized(p, ""F070LP"", ""INRSI_C_FWA_POSITION_F070LP"", start, end, conn, mn.fw_nominals[\'F070LP\'], color = ""darkmagenta"")\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = \'horizontal\'\n    return p\n\n\ndef gwx(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [-4,4],                                   \\\n                x_axis_type = \'datetime\',                           \\\n                x_axis_label = \'Date\', y_axis_label = \'mV (normalized)\')\n\n    p.grid.visible = True\n    p.title.text = ""Gratingwheel X""\n    p.title.align = ""left""\n    pf.add_basic_layout(p)\n\n    pf.add_to_plot_normalized(p, ""PRISM"", ""INRSI_C_GWA_X_POSITION_PRISM"", start, end, conn, mn.gwx_nominals[\'PRISM\'], color = ""green"")\n    pf.add_to_plot_normalized(p, ""MIRROR"", ""INRSI_C_GWA_X_POSITION_MIRROR"", start, end, conn, mn.gwx_nominals[\'MIRROR\'], color = ""blue"")\n    pf.add_to_plot_normalized(p, ""G140H"", ""INRSI_C_GWA_X_POSITION_G140H"", start, end, conn, mn.gwx_nominals[\'G140H\'], color = ""red"")\n    pf.add_to_plot_normalized(p, ""G235H"", ""INRSI_C_GWA_X_POSITION_G235H"", start, end, conn, mn.gwx_nominals[\'G235H\'], color = ""purple"")\n    pf.add_to_plot_normalized(p, ""G395H"", ""INRSI_C_GWA_X_POSITION_G395H"", start, end, conn, mn.gwx_nominals[\'G395H\'], color = ""orange"")\n    pf.add_to_plot_normalized(p, ""G140M"", ""INRSI_C_GWA_X_POSITION_G140M"", start, end, conn, mn.gwx_nominals[\'G140M\'], color = ""brown"")\n    pf.add_to_plot_normalized(p, ""G235M"", ""INRSI_C_GWA_X_POSITION_G235M"", start, end, conn, mn.gwx_nominals[\'G235M\'], color = ""darkmagenta"")\n    pf.add_to_plot_normalized(p, ""G395M"", ""INRSI_C_GWA_X_POSITION_G395M"", start, end, conn, mn.gwx_nominals[\'G395M\'], color = ""darkcyan"")\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = \'horizontal\'\n\n    return p\n\ndef gwy(conn, start, end):\n    \'\'\'Create specific plot and return plot object\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : Plot object\n        Bokeh plot\n    \'\'\'\n\n    # create a new plot with a title and axis labels\n    p = figure( tools = ""pan,wheel_zoom,box_zoom,reset,save"",       \\\n                toolbar_location = ""above"",                         \\\n                plot_width = 1120,                                   \\\n                plot_height = 500,                                  \\\n                y_range = [-3,3],                                   \\\n                x_axis_type = \'datetime\',                           \\\n                x_axis_label = \'Date\', y_axis_label = \'mV (normalized)\')\n\n    p.grid.visible = True\n    p.title.text = ""Gratingwheel Y""\n    p.title.align = ""left""\n    pf.add_basic_layout(p)\n\n    pf.add_to_plot_normalized(p, ""PRISM"", ""INRSI_C_GWA_Y_POSITION_PRISM"", start, end, conn, mn.gwy_nominals[\'PRISM\'], color = ""green"")\n    pf.add_to_plot_normalized(p, ""MIRROR"", ""INRSI_C_GWA_Y_POSITION_MIRROR"", start, end, conn, mn.gwy_nominals[\'MIRROR\'], color = ""blue"")\n    pf.add_to_plot_normalized(p, ""G140H"", ""INRSI_C_GWA_Y_POSITION_G140H"", start, end, conn, mn.gwy_nominals[\'G140H\'], color = ""red"")\n    pf.add_to_plot_normalized(p, ""G235H"", ""INRSI_C_GWA_Y_POSITION_G235H"", start, end, conn, mn.gwy_nominals[\'G235H\'], color = ""purple"")\n    pf.add_to_plot_normalized(p, ""G395H"", ""INRSI_C_GWA_Y_POSITION_G395H"", start, end, conn, mn.gwy_nominals[\'G395H\'], color = ""orange"")\n    pf.add_to_plot_normalized(p, ""G140M"", ""INRSI_C_GWA_Y_POSITION_G140M"", start, end, conn, mn.gwy_nominals[\'G140M\'], color = ""brown"")\n    pf.add_to_plot_normalized(p, ""G235M"", ""INRSI_C_GWA_Y_POSITION_G235M"", start, end, conn, mn.gwy_nominals[\'G235M\'], color = ""darkmagenta"")\n    pf.add_to_plot_normalized(p, ""G395M"", ""INRSI_C_GWA_Y_POSITION_G395M"", start, end, conn, mn.gwy_nominals[\'G395M\'], color = ""darkcyan"")\n\n    p.legend.location = ""bottom_right""\n    p.legend.click_policy = ""hide""\n    p.legend.orientation = \'horizontal\'\n    return p\n\ndef wheel_pos(conn, start, end):\n    \'\'\'Combine plots to a tab\n    Parameters\n    ----------\n    conn : DBobject\n        Connection object that represents database\n    start : time\n        Startlimit for x-axis and query (typ. datetime.now()- 4Months)\n    end : time\n        Endlimit for x-axis and query (typ. datetime.now())\n    Return\n    ------\n    p : tab object\n        used by dashboard.py to set up dashboard\n    \'\'\'\n    descr = Div(text=\n    """"""\n    <style>\n    table, th, td {\n      border: 1px solid black;\n      background-color: #efefef;\n      border-collapse: collapse;\n      padding: 5px\n    }\n    table {\n      border-spacing: 15px;\n    }\n    </style>\n\n    <body>\n    <table style=""width:100%"">\n      <tr>\n        <th><h6>Plotname</h6></th>\n        <th><h6>Mnemonic</h6></th>\n        <th><h6>Description</h6></th>\n      </tr>\n\n      <tr>\n        <td>Filterwheel</td>\n        <td>INRSI_FWA_MECH_POS<br>\n            INRSI_C_FWA_POSITION</td>\n        <td>Position Sensor Value<br>\n            Current Position</td>\n      </tr>\n\n      <tr>\n        <td>Gratingwheel X</td>\n        <td>INRSI_GWA_MECH_POS<br>\n            INRSI_C_GWA_X_POSITION</td>\n        <td>Position X Sensor Value<br>\n            Current Position</td>\n      </tr>\n\n      <tr>\n        <td>Gratingwheel Y</td>\n        <td>INRSI_GWA_MECH_POS<br>\n            INRSI_C_GWA_Y_POSITION</td>\n        <td>Position Y Sensor Value<br>\n            Current Position</td>\n      </tr>\n\n    </table>\n    </body>\n    """""", width=1100)\n\n    plot1 = fw(conn, start, end)\n    plot2 = gwx(conn, start, end)\n    plot3 = gwy(conn,  start, end)\n\n    layout = column(descr, plot1, plot2, plot3)\n    tab = Panel(child = layout, title = ""FILTER/GRATINGWHEEL"")\n\n    return tab\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/utils/__init__.py,0,b''
jwql/instrument_monitors/nirspec_monitors/data_trending/utils/condition.py,0,"b'#! /usr/bin/env python\r\n""""""Module generates conditions over one or more mnemonics\r\n\r\nThe modules purpose is to return True/False for any times by reference of\r\ncertain conditions. If for instance the condition ""x>1"" over a defined period of\r\ntime is needed, the module looks for all elements where the condition applies\r\nand where it does not apply. This generates two lists, which contain the ""start""\r\nand ""end"" times of the condition.\r\nA futher function combines the start- and endtimes to time-tuples between which\r\nthe condition is known as TRUE. A ""state"" function returns True/False for an\r\nexact time attribute, whereby the condition is represented in binary form.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n    This module is not prepared for standalone use.\r\n\r\n    For use in programm set condition up like below:\r\n\r\n    import the module as follow:\r\n    >>>import condition as cond\r\n\r\n    generate list with required conditions:\r\n    >>>con_set = [ cond.equal(m.mnemonic(\'IMIR_HK_POM_LOOP\'),\'OFF\'),\r\n                cond.smaller(m.mnemonic(\'IMIR_HK_ICE_SEC_VOLT1\'),1),\r\n                cond.greater(m.mnemonic(\'SE_ZIMIRICEA\'),0.2)]\r\n\r\n    generate object of condition with the con_set as attribute:\r\n    >>>condition_object=cond.condition(con_set)\r\n\r\n    Now the condition_object can return a True/False statement wheather\r\n    the time given as attribut meets the conditions:\r\n\r\n    >>>if condition.state(float(element[\'Primary Time\'])):\r\n        -> True when condition for the given time applies\r\n        -> False when condition for the given time is not applicable\r\n\r\nDependencies\r\n------------\r\n    no external files needed\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\n\r\n\r\nclass condition:\r\n    """"""Class to hold several subconditions""""""\r\n\r\n    #contains list of representative time pairs for each subcondition\r\n    cond_time_pairs = []\r\n    #state of the condition\r\n    __state = False\r\n\r\n    #initializes condition through condition set\r\n    def __init__(self, cond_set):\r\n        """"""Initialize object with set of conditions\r\n        Parameters\r\n        ----------\r\n        cond_set : list\r\n            list contains subconditions objects\r\n        """"""\r\n        self.cond_set = cond_set\r\n\r\n    #destructor -> take care that all time_pairs are deleted!\r\n    def __del__(self):\r\n        """"""Delete object - destructor method""""""\r\n        del self.cond_time_pairs[:]\r\n\r\n    #prints all stored time pairs (for developement only)\r\n    def print_times(self):\r\n        """"""Print conditions time pairs on command line (developement)""""""\r\n        print(\'Available time pairs:\')\r\n        for times in self.cond_time_pairs:\r\n            print(\'list: \'+str(times))\r\n\r\n    #returns a interval if time is anywhere in between\r\n    def get_interval(self, time):\r\n        """"""Returns time interval if availlable, where ""time"" is in between\r\n        Parameters\r\n        ----------\r\n        time : float\r\n            given time attribute\r\n        Return\r\n        ------\r\n        time_pair : tuple\r\n            pair of start_time and end_time where time is in between\r\n        """"""\r\n        end_time = 10000000\r\n        start_time = 0\r\n\r\n        #do for every condition\r\n        for cond in self.cond_time_pairs:\r\n            #do for every time pair in condition\r\n            for pair in cond:\r\n                if (time > pair[0]) and (time < pair[1]):\r\n                    if (end_time > pair[1]) and (start_time < pair[0]):\r\n                        start_time = pair[0]\r\n                        end_time = pair[1]\r\n                        break\r\n                    else:\r\n                        break\r\n\r\n        if (end_time != 10000000) and (start_time != 0):\r\n            return [start_time, end_time]\r\n        else:\r\n            return None\r\n\r\n\r\n    #generates time pairs out of start and end times\r\n    def generate_time_pairs(start_times, end_times):\r\n        """"""Forms time pairs out of start times and end times\r\n        Parameters\r\n        ----------\r\n        start_times : list\r\n            contains all times where a condition applies\r\n        end_times : list\r\n            contains all times where the condition does not apply\r\n        Return\r\n        ------\r\n        time_pair : list\r\n            list of touples with start and end time\r\n        """"""\r\n        #internal use only\r\n        time_pair: float = []\r\n\r\n        #when the conditons doesn\xc2\xb4t apply anyway\r\n        if not start_times:\r\n            time_pair.append((0,0))\r\n\r\n        #check if the condition indicates an open time range\r\n        elif not end_times:\r\n            time_pair.append((start_times[0], 0))\r\n\r\n        #generate time pairs\r\n        #for each start time a higher or equal end time is searched for\r\n        #these times form am touple which is appended to  time_pair : list\r\n        else:\r\n            time_hook = 0\r\n            last_start_time = 0\r\n\r\n            for start in list(sorted(set(start_times))):\r\n\r\n                if(start > time_hook):\r\n                    for end in list(sorted(set(end_times))):\r\n\r\n                        if end > start:\r\n\r\n                            time_pair.append((start, end))\r\n                            time_hook = end\r\n                            break\r\n\r\n            if list(sorted(set(start_times)))[-1] > list(sorted(set(end_times)))[-1]:\r\n                time_pair.append((list(sorted(set(end_times)))[-1], 0))\r\n\r\n        return(time_pair)\r\n\r\n\r\n    #returns state of the condition at a given time\r\n    #if state(given time)==True -> condition is true\r\n    #if state(given time)==False -> condition is false\r\n    def state(self, time):\r\n        """"""Checks whether condition is true of false at a given time\r\n        Parameters\r\n        ----------\r\n        time : float\r\n            input time for condition query\r\n        Return\r\n        ------\r\n        state : bool\r\n            True/False statement whether the condition applies or not\r\n        """"""\r\n        #checks condition for every sub condition in condition set (subconditions)\r\n\r\n        state = self.__state\r\n\r\n        for cond in self.cond_time_pairs:\r\n\r\n            if self.__check_subcondition(cond, time):\r\n                state = True\r\n            else:\r\n                state = False\r\n                break\r\n\r\n        return state\r\n\r\n\r\n    def __check_subcondition(self, cond, time):\r\n\r\n        #if there are no values availlable\r\n        if cond[0][0] == 0:\r\n            return False\r\n\r\n        for time_pair in cond:\r\n            #if just a positive time is availlable, return true\r\n            if (time_pair[1] == 0) and (time > time_pair[0]):\r\n\r\n                return True\r\n\r\n            #if given time occurs between a time pair, return true\r\n            elif (time_pair[0]) <= time and (time < time_pair[1]):\r\n\r\n                return True\r\n\r\n            else:\r\n                pass\r\n\r\n\r\nclass equal(condition):\r\n    """"""Class to hold single ""is equal"" subcondition""""""\r\n\r\n    stringval = True\r\n\r\n    #add attributes to function - start function ""cond_time_pairs()""\r\n    def __init__(self, mnemonic, value, stringval=True):\r\n        """"""Initializes subconditon\r\n        Parameters\r\n        ----------\r\n        mnemonic : astropy table\r\n            includes mnemomic engineering data and corresponding primary time\r\n        value : str\r\n            coparison value for equal statement\r\n        """"""\r\n        self.mnemonic = mnemonic\r\n        self.value = value\r\n        self.stringval = stringval\r\n        condition.cond_time_pairs.append((self.cond_true_time()))\r\n\r\n\r\n    #generates a list of time-touples (start_time, end_time) that mark the beginning and end of\r\n    #wheather the condition is true or not\r\n    def cond_true_time(self):\r\n        """"""Filters all values that are equal to a given comparison value\r\n        if equal: Primary time -> temp_start\r\n        if not equal: Primary time -> temp_end\r\n        Return\r\n        ------\r\n        time_p : list\r\n            list of touples with start and end time\r\n        """"""\r\n        temp_start = []\r\n        temp_end = []\r\n\r\n        for key in self.mnemonic:\r\n\r\n            #find all times whoses Raw values equal the given value\r\n            if self.stringval:\r\n                if key[\'value\'] == self.value:\r\n                    temp_start.append(key[""time""])\r\n\r\n                #find all end values\r\n                else:\r\n                    temp_end.append(key[""time""])\r\n            else:\r\n                # just another option to compare float values\r\n                if float(key[\'value\']) == self.value:\r\n                    temp_start.append(key[""time""])\r\n\r\n                #find all end values\r\n                else:\r\n                    temp_end.append(key[""time""])\r\n\r\n        time_p = condition.generate_time_pairs(temp_start, temp_end)\r\n        return time_p\r\n\r\nclass unequal(condition):\r\n    """"""Class to hold single ""is unequal"" subcondition""""""\r\n\r\n    #add attributes to function - start function ""cond_time_pairs()""\r\n    def __init__(self, mnemonic, value):\r\n        """"""Initializes subconditon\r\n        Parameters\r\n        ----------\r\n        mnemonic : astropy table\r\n            includes mnemomic engineering data and corresponding primary time\r\n        value : str\r\n            coparison value for equal statement\r\n        """"""\r\n        self.mnemonic = mnemonic\r\n        self.value = value\r\n        condition.cond_time_pairs.append((self.cond_true_time()))\r\n\r\n\r\n    #generates a list of time-touples (start_time, end_time) that mark the beginning and end of\r\n    #wheather the condition is true or not\r\n    def cond_true_time(self):\r\n        """"""Filters all values that are equal to a given comparison value\r\n        if equal: Primary time -> temp_start\r\n        if not equal: Primary time -> temp_end\r\n        Return\r\n        ------\r\n        time_p : list\r\n            list of touples with start and end time\r\n        """"""\r\n        temp_start = []\r\n        temp_end = []\r\n\r\n        for key in self.mnemonic:\r\n\r\n            #find all times whoses Raw values equal the given value\r\n            if key[\'value\'] != self.value:\r\n                temp_start.append(key[""time""])\r\n\r\n            #find all end values\r\n            else:\r\n                temp_end.append(key[""time""])\r\n\r\n        time_p = condition.generate_time_pairs(temp_start, temp_end)\r\n        return time_p\r\n\r\nclass greater(condition):\r\n    """"""Class to hold single ""greater than"" subcondition""""""\r\n\r\n    #add attributes to function - start function ""cond_time_pairs()""\r\n    def __init__(self, mnemonic, value):\r\n        """"""Initializes subconditon\r\n        Parameters\r\n        ----------\r\n        mnemonic : astropy table\r\n            includes mnemomic engineering data and corresponding primary time\r\n        value : str\r\n            coparison value for equal statement\r\n        """"""\r\n        self.mnemonic= mnemonic\r\n        self.value=value\r\n        condition.cond_time_pairs.append((self.cond_true_time()))\r\n\r\n    def cond_true_time(self):\r\n        """"""Filters all values that are greater than a given comparison value\r\n        if equal: Primary time -> temp_start\r\n        if not equal: Primary time -> temp_end\r\n        Return\r\n        ------\r\n        time_p : list\r\n            list of touples with start and end time\r\n        """"""\r\n        temp_start: float = []\r\n        temp_end: float = []\r\n\r\n        for key in self.mnemonic:\r\n\r\n            #find all times whose Raw values are grater than the given value\r\n            if float(key[\'value\']) > self.value:\r\n                temp_start.append(key[""time""])\r\n\r\n            #find all end values\r\n            else:\r\n                temp_end.append(key[""time""])\r\n\r\n        time_p = condition.generate_time_pairs(temp_start, temp_end)\r\n        return time_p\r\n\r\n\r\nclass smaller(condition):\r\n    """"""Class to hold single ""greater than"" subcondition""""""\r\n\r\n    #add attributes to function - start function ""cond_time_pairs()""\r\n    def __init__(self, mnemonic, value):\r\n        """"""Initializes subconditon\r\n        Parameters\r\n        ----------\r\n        mnemonic : astropy table\r\n            includes mnemomic engineering data and corresponding primary time\r\n        value : str\r\n            coparison value for equal statement\r\n        """"""\r\n        self.mnemonic=mnemonic\r\n        self.value=value\r\n        condition.cond_time_pairs.append((self.cond_true_time()))\r\n\r\n    def cond_true_time(self):\r\n        """"""Filters all values that are greater than a given comparison value\r\n        if equal: Primary time -> temp_start\r\n        if not equal: Primary time -> temp_end\r\n        Return\r\n        ------\r\n        time_p : list\r\n            list of touples with start and end time\r\n        """"""\r\n        temp_start: float = []\r\n        temp_end: float = []\r\n\r\n        for key in self.mnemonic:\r\n\r\n            #find all times whose Raw values are grater than the given value\r\n            if float(key[\'value\']) < self.value:\r\n                temp_start.append(key[""time""])\r\n\r\n            #find all end values\r\n            else:\r\n                temp_end.append(key[""time""])\r\n\r\n        time_p = condition.generate_time_pairs(temp_start, temp_end)\r\n        return time_p\r\n\r\n\r\nif __name__ ==\'__main__\':\r\n    pass\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/utils/csv_to_AstropyTable.py,0,"b'#! /usr/bin/env python\r\n""""""Module for importing and sorting mnemonics\r\n\r\nThis module imports a whole set of mnemonics from a .CSV sheet and converts it\r\nto an astropy table. In a second step the table is sorted by its mnemoncis\r\nand for each mnemmonic another astropy table with reduced content is created.\r\nThe last step is to append the data (time and engineering value) with its\r\nmnemonic identifier as key to a dictionary.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n\r\n\r\nDependencies\r\n------------\r\n    mnemonics.py -> includes a list of mnemonics to be evaluated\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\nfrom astropy.table import Table\r\nfrom astropy.time import Time\r\nimport warnings\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.mnemonics as mn\r\n\r\n\r\nclass mnemonics:\r\n    """"""class to hold a set of mnemonics""""""\r\n\r\n    __mnemonic_dict = {}\r\n\r\n    def __init__(self, import_path):\r\n        """"""main function of this class\r\n        Parameters\r\n        ----------\r\n        import_path : str\r\n            defines file to import (csv sheet)\r\n        """"""\r\n        imported_data = self.import_CSV(import_path)\r\n        length = len(imported_data)\r\n\r\n        print(\'{} was imported - {} lines\'.format(import_path, length))\r\n\r\n        #look for every mnmonic given in mnemonicy.py\r\n        for mnemonic_name in mn.mnemonic_set_query:\r\n            temp = self.sort_mnemonic(mnemonic_name, imported_data)\r\n            #append temp to dict with related mnemonic\r\n            if temp != None:\r\n                self.__mnemonic_dict.update({mnemonic_name:temp})\r\n            else:\r\n                warnings.warn(""fatal error"")\r\n\r\n\r\n    def import_CSV(self, path):\r\n        """"""imports csv sheet and converts it to AstropyTable\r\n        Parameters\r\n        ----------\r\n        path : str\r\n            defines path to file to import\r\n        Return\r\n        ------\r\n        imported_data : AstropyTable\r\n            container for imported data\r\n        """"""\r\n        #read data from given *CSV file\r\n        imported_data=Table.read(path, format=\'ascii.basic\', delimiter=\',\')\r\n        return imported_data\r\n\r\n\r\n    #returns table of single mnemonic\r\n    def mnemonic(self, name):\r\n        """"""Returns table of one single mnemonic\r\n        Parameters\r\n        ----------\r\n        name : str\r\n            name of mnemonic\r\n        Return\r\n        ------\r\n        __mnemonic_dict[name] : AstropyTable\r\n            corresponding table to mnemonic name\r\n        """"""\r\n        try:\r\n            return self.__mnemonic_dict[name]\r\n        except KeyError:\r\n            print(\'{} not in list\'.format(name))\r\n\r\n\r\n    #looks for given mnemonic in given table\r\n    #returns list containing astropy tables with sorted mnemonics and engineering values\r\n    #adds useful meta data to Table\r\n    def sort_mnemonic(self, mnemonic, table):\r\n        """"""Looks for all values in table with identifier ""mnemonic""\r\n           Converts time string to mjd format\r\n        Parameters\r\n        ----------\r\n        mnemonic : str\r\n            identifies which mnemonic to look for\r\n        table : AstropyTable\r\n            table that stores mnemonics and data\r\n        Return\r\n        ------\r\n        mnemonic_table : AstropyTable\r\n            stores all data associated with identifier ""mnemonic""\r\n        """"""\r\n\r\n        temp1: float = []\r\n        temp2 = []\r\n\r\n        #appends present mnemonic data to temp arrays temp1 and temp2\r\n        for item in table:\r\n            try:\r\n                if item[\'Telemetry Mnemonic\'] == mnemonic:\r\n                    #convert time string to mjd format\r\n                    temp = item[\'Secondary Time\'].replace(\'/\',\'-\').replace(\' \',\'T\')\r\n                    t = Time(temp, format=\'isot\')\r\n\r\n                    temp1.append(t.mjd)\r\n                    temp2.append(item[\'EU Value\'])\r\n            except KeyError:\r\n                warnings.warn(""{} is not in mnemonic table"".format(mnemonic))\r\n\r\n        description = (\'time\',\'value\')\r\n        data = [temp1, temp2]\r\n\r\n        #add some meta data\r\n        if len(temp1) > 0:\r\n            date_start = temp1[0]\r\n            date_end = temp1[len(temp1)-1]\r\n            info = {\'start\':date_start, \'end\':date_end}\r\n        else:\r\n            info = {""n"":""n""}\r\n\r\n        #add name of mnemonic to meta data of list\r\n        info[\'mnemonic\'] = mnemonic\r\n        info[\'len\'] = len(temp1)\r\n\r\n        #table to return\r\n        mnemonic_table = Table(data, names = description, \\\r\n                        dtype = (\'f8\',\'str\'), meta = info)\r\n        return mnemonic_table\r\n\r\nif __name__ ==\'__main__\':\r\n    pass\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/utils/mnemonics.py,0,"b'""""""Module lists all neccessary mnemonics for NIRSpec data trending\r\n\r\nThe module includes several lists to import to NIRSpec data trending monitor program.\r\nThe lists are used for data aquisation and to set up the initial database.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n    import mnemoncis as mn\r\n\r\nReferences\r\n----------\r\n    JWQL_NIRSpec_INputs_V4[2414].xlsx\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\n\r\n#mnemonics underlaying certain conditions 15min\r\n# INRSD_EXP_STAT != STARTED\r\nmnemonic_cond_1 = [\r\n""SE_ZINRSFPEA"",\r\n""SE_ZINRSFPEB""]\r\n\r\n#mnemonics underlaying condition 15min\r\n# INRSH_LAMP_SEL = NO_LAMP\r\nmnemonic_cond_2 = [\r\n""SE_ZINRSICEA"",\r\n""SE_ZINRSICEB""]\r\n\r\nmnemonic_cond_3 = [\r\n""SE_ZINRSMCEA"",\r\n""SE_ZINRSMCEB""]\r\n\r\n#menmonics applicable when CAA is powered\r\n#INRSH_CAA_PWRF_ST = ON\r\nmnemonic_caa= [\r\n""IGDP_NRSI_C_CAAL1_TEMP"",\r\n""IGDP_NRSI_C_CAAL2_TEMP"",\r\n""IGDP_NRSI_C_CAAL3_TEMP"",\r\n""IGDP_NRSI_C_CAAL4_TEMP""]\r\n\r\n#only applicable when Filter table 10 is set\r\nmnemonic_ft10 = [\r\n""INRSH_OA_VREFOFF"",\r\n""INRSH_OA_VREF"",\r\n\r\n""INRSH_CAA_VREFOFF"",\r\n""INRSH_CAA_VREF"",\r\n\r\n""INRSH_FWA_ADCMGAIN"",\r\n""INRSH_FWA_ADCMOFFSET"",\r\n""INRSH_FWA_MOTOR_VREF"",\r\n\r\n""INRSH_GWA_ADCMGAIN"",\r\n""INRSH_GWA_ADCMOFFSET"",\r\n""INRSH_GWA_MOTOR_VREF"",\r\n\r\n""INRSH_RMA_ADCMGAIN"",\r\n""INRSH_RMA_ADCMOFFSET""]\r\n\r\n#all mnemonics used for conditions\r\nmnemonic_for_conditions = [\r\n""INRSM_MOVE_STAT"",\r\n""INRSH_WHEEL_MOT_SVREF"",\r\n""INRSI_CAA_ON_FLAG"",\r\n""INRSH_LAMP_SEL"",\r\n""INRSD_EXP_STAT"",\r\n\r\n""INRSH_CAA_PWRF_ST"",\r\n\r\n""INRSI_FWA_MOVE_ST"",\r\n""INRSI_FWA_MECH_POS"",\r\n""INRSI_GWA_MOVE_ST"",\r\n""INRSI_GWA_MECH_POS"",\r\n\r\n""INRSI_C_FWA_POSITION"",\r\n""INRSI_C_GWA_X_POSITION"",\r\n""INRSI_C_GWA_Y_POSITION"",\r\n\r\n""ICTM_RT_FILTER"" ]\r\n\r\n#these mnemonic are used by the day routine\r\nmnemSet_day = [\r\n""INRSM_MSA_Q1_365VDD"",\r\n""INRSM_MSA_Q1_365VPP"",\r\n""INRSM_MSA_Q1_171VPP"",\r\n""IGDPM_MSA_Q1_365IDD"",\r\n""IGDPM_MSA_Q1_365IPP"",\r\n""IGDPM_MSA_Q1_171RTN"",\r\n\r\n""INRSM_MSA_Q2_365VDD"",\r\n""INRSM_MSA_Q2_365VPP"",\r\n""INRSM_MSA_Q2_171VPP"",\r\n""IGDPM_MSA_Q2_365IDD"",\r\n""IGDPM_MSA_Q2_365IPP"",\r\n""IGDPM_MSA_Q2_171RTN"",\r\n\r\n""INRSM_MSA_Q3_365VDD"",\r\n""INRSM_MSA_Q3_365VPP"",\r\n""INRSM_MSA_Q3_171VPP"",\r\n""IGDPM_MSA_Q3_365IDD"",\r\n""IGDPM_MSA_Q3_365IPP"",\r\n""IGDPM_MSA_Q3_171RTN"",\r\n\r\n""INRSM_MSA_Q4_365VDD"",\r\n""INRSM_MSA_Q4_365VPP"",\r\n""INRSM_MSA_Q4_171VPP"",\r\n""IGDPM_MSA_Q4_365IDD"",\r\n""IGDPM_MSA_Q4_365IPP"",\r\n""IGDPM_MSA_Q4_171RTN"",\r\n\r\n""IGDP_NRSD_ALG_FPA_TEMP"",\r\n""IGDP_NRSD_ALG_A1_TEMP"",\r\n""IGDP_NRSD_ALG_A2_TEMP"",\r\n""IGDP_NRSI_C_FWA_TEMP"",\r\n""IGDP_NRSI_C_GWA_TEMP"",\r\n\r\n""SI_GZCTS74A"",\r\n""SI_GZCTS74B"",\r\n""SI_GZCTS67A"",\r\n""SI_GZCTS67B""]\r\n\r\n#these mnemonic are used by the 15min routine\r\nmnemSet_15min = [\r\n""IGDP_NRSD_ALG_TEMP"",\r\n\r\n""INRSD_ALG_ACC_P12C"",\r\n""INRSD_ALG_ACC_N12C"",\r\n""INRSD_ALG_ACC_3D3_1D5_C"",\r\n""INRSD_ALG_CHASSIS"",\r\n\r\n""IGDP_NRSD_ALG_A1_VDD_C"",\r\n""IGDP_NRSD_ALG_A1_VDDA"",\r\n""IGDP_NRSD_ALG_A1VDAP12C"",\r\n""IGDP_NRSD_ALG_A1VDAN12C"",\r\n""IGDP_NRSD_ALG_A1GND4VDA"",\r\n""IGDP_NRSD_ALG_A1GND5VRF"",\r\n""INRSD_ALG_A1_VDD3P3"",\r\n""INRSD_ALG_A1_VDD"",\r\n""INRSD_ALG_A1_REF"",\r\n""INRSD_A1_DSUB_V"",\r\n""INRSD_A1_VRESET_V"",\r\n""INRSD_A1_CELLDRN_V"",\r\n""INRSD_A1_DRAIN_V"",\r\n""INRSD_A1_VBIASGATE_V"",\r\n""INRSD_A1_VBIASPWR_V"",\r\n""INRSD_A1_VDDA_I"",\r\n\r\n""IGDP_NRSD_ALG_A2_VDD_C"",\r\n""IGDP_NRSD_ALG_A2_VDDA"",\r\n""IGDP_NRSD_ALG_A2VDAP12C"",\r\n""IGDP_NRSD_ALG_A2VDAN12C"",\r\n""IGDP_NRSD_ALG_A2GND4VDA"",\r\n""IGDP_NRSD_ALG_A2GND5VRF"",\r\n""INRSD_ALG_A2_VDD3P3"",\r\n""INRSD_ALG_A2_VDD"",\r\n""INRSD_ALG_A2_REF"",\r\n""INRSD_A2_DSUB_V"",\r\n""INRSD_A2_VRESET_V"",\r\n""INRSD_A2_CELLDRN_V"",\r\n""INRSD_A2_DRAIN_V"",\r\n""INRSD_A2_VBIASGATE_V"",\r\n""INRSD_A2_VBIASPWR_V"",\r\n""INRSD_A2_VDDA_I"",\r\n\r\n""INRSH_HK_TEMP1"",\r\n""INRSH_HK_TEMP2"",\r\n\r\n""INRSH_HK_P15V"",\r\n""INRSH_HK_N15V"",\r\n""INRSH_HK_VMOTOR"",\r\n""INRSH_HK_P5V"",\r\n""INRSH_HK_2P5V"",\r\n""INRSH_HK_ADCTGAIN"",\r\n""INRSH_HK_ADCTOFFSET"",\r\n\r\n""IGDP_NRSI_C_CAM_TEMP"",\r\n""IGDP_NRSI_C_COL_TEMP"",\r\n""IGDP_NRSI_C_COM1_TEMP"",\r\n""IGDP_NRSI_C_FOR_TEMP"",\r\n""IGDP_NRSI_C_IFU_TEMP"",\r\n""IGDP_NRSI_C_BP1_TEMP"",\r\n""IGDP_NRSI_C_BP2_TEMP"",\r\n""IGDP_NRSI_C_BP3_TEMP"",\r\n""IGDP_NRSI_C_BP4_TEMP"",\r\n""IGDP_NRSI_C_RMA_TEMP"",\r\n\r\n""SI_GZCTS75A"",\r\n""SI_GZCTS68A"",\r\n""SI_GZCTS81A"",\r\n""SI_GZCTS80A"",\r\n""SI_GZCTS70A"",\r\n""SI_GZCTS76A"",\r\n""SI_GZCTS79A"",\r\n""SI_GZCTS77A"",\r\n""SI_GZCTS78A"",\r\n""SI_GZCTS69A"",\r\n\r\n""INRSM_MCE_AIC_1R5_V"",\r\n""INRSM_MCE_AIC_3R3_V"",\r\n""INRSM_MCE_AIC_5_V"",\r\n""INRSM_MCE_AIC_P12_V"",\r\n""INRSM_MCE_AIC_N12_V"",\r\n""INRSM_MCE_AIC_3R3_I"",\r\n""INRSM_MCE_AIC_5_I"",\r\n""INRSM_MCE_AIC_P12_I"",\r\n""INRSM_MCE_AIC_N12_I"",\r\n\r\n""INRSM_MCE_MDAC_1R5_V"",\r\n""INRSM_MCE_MDAC_3R3_V"",\r\n""INRSM_MCE_MDAC_5_V"",\r\n""INRSM_MCE_MDAC_P12_V"",\r\n""INRSM_MCE_MDAC_N12_V"",\r\n""INRSM_MCE_MDAC_3R3_I"",\r\n""INRSM_MCE_MDAC_5_I"",\r\n""INRSM_MCE_MDAC_P12_I"",\r\n""INRSM_MCE_MDAC_N12_I"",\r\n\r\n""INRSM_MCE_PCA_TMP1"",\r\n""INRSM_MCE_PCA_TMP2"",\r\n""INRSM_MCE_AIC_TMP_FPGA"",\r\n""INRSM_MCE_AIC_TMP_ADC"",\r\n""INRSM_MCE_AIC_TMP_VREG"",\r\n""INRSM_MCE_MDAC_TMP_FPGA"",\r\n""INRSM_MCE_MDAC_TMP_OSC"",\r\n""INRSM_MCE_MDAC_TMP_BRD"",\r\n""INRSM_MCE_MDAC_TMP_PHA"",\r\n""INRSM_MCE_MDAC_TMP_PHB"",\r\n\r\n""INRSM_Q1_TMP_A"",\r\n""INRSM_Q2_TMP_A"",\r\n""INRSM_Q3_TMP_A"",\r\n""INRSM_Q4_TMP_A"",\r\n""INRSM_MECH_MTR_TMP_A"",\r\n""INRSM_LL_MTR_TMP_A"",\r\n""INRSM_MSA_TMP_A""]\r\n\r\n#mnemonic set for setting up database\r\nmnemonic_set_database = [\r\n""GP_ZPSVOLT"",\r\n""SE_ZINRSFPEA"",\r\n""SE_ZINRSFPEB"",\r\n\r\n""IGDP_NRSD_ALG_TEMP"",\r\n\r\n""IGDP_NRSD_ALG_FPA_TEMP"",\r\n""IGDP_NRSD_ALG_A1_TEMP"",\r\n""IGDP_NRSD_ALG_A2_TEMP"",\r\n""SI_GZCTS74A"",\r\n""SI_GZCTS74B"",\r\n""SI_GZCTS67A"",\r\n""SI_GZCTS67B"",\r\n\r\n""INRSD_ALG_ACC_P12C"",\r\n""INRSD_ALG_ACC_N12C"",\r\n""INRSD_ALG_ACC_3D3_1D5_C"",\r\n""INRSD_ALG_CHASSIS"",\r\n\r\n""IGDP_NRSD_ALG_A1_VDD_C"",\r\n""IGDP_NRSD_ALG_A1_VDDA"",\r\n""IGDP_NRSD_ALG_A1VDAP12C"",\r\n""IGDP_NRSD_ALG_A1VDAN12C"",\r\n""IGDP_NRSD_ALG_A1GND4VDA"",\r\n""IGDP_NRSD_ALG_A1GND5VRF"",\r\n""INRSD_ALG_A1_VDD3P3"",\r\n""INRSD_ALG_A1_VDD"",\r\n""INRSD_ALG_A1_REF"",\r\n""INRSD_A1_DSUB_V"",\r\n""INRSD_A1_VRESET_V"",\r\n""INRSD_A1_CELLDRN_V"",\r\n""INRSD_A1_DRAIN_V"",\r\n""INRSD_A1_VBIASGATE_V"",\r\n""INRSD_A1_VBIASPWR_V"",\r\n""INRSD_A1_VDDA_I"",\r\n\r\n""IGDP_NRSD_ALG_A2_VDD_C"",\r\n""IGDP_NRSD_ALG_A2_VDDA"",\r\n""IGDP_NRSD_ALG_A2VDAP12C"",\r\n""IGDP_NRSD_ALG_A2VDAN12C"",\r\n""IGDP_NRSD_ALG_A2GND4VDA"",\r\n""IGDP_NRSD_ALG_A2GND5VRF"",\r\n""INRSD_ALG_A2_VDD3P3"",\r\n""INRSD_ALG_A2_VDD"",\r\n""INRSD_ALG_A2_REF"",\r\n""INRSD_A2_DSUB_V"",\r\n""INRSD_A2_VRESET_V"",\r\n""INRSD_A2_CELLDRN_V"",\r\n""INRSD_A2_DRAIN_V"",\r\n""INRSD_A2_VBIASGATE_V"",\r\n""INRSD_A2_VBIASPWR_V"",\r\n""INRSD_A2_VDDA_I"",\r\n\r\n""SE_ZINRSICEA"",\r\n""SE_ZINRSICEB"",\r\n\r\n""INRSH_HK_TEMP1"",\r\n""INRSH_HK_TEMP2"",\r\n\r\n""INRSH_HK_P15V"",\r\n""INRSH_HK_N15V"",\r\n""INRSH_HK_VMOTOR"",\r\n""INRSH_HK_P5V"",\r\n""INRSH_HK_2P5V"",\r\n""INRSH_HK_ADCTGAIN"",\r\n""INRSH_HK_ADCTOFFSET"",\r\n\r\n""INRSH_OA_VREFOFF"",\r\n""INRSH_OA_VREF"",\r\n\r\n""IGDP_NRSI_C_CAM_TEMP"",\r\n""IGDP_NRSI_C_COL_TEMP"",\r\n""IGDP_NRSI_C_COM1_TEMP"",\r\n""IGDP_NRSI_C_FOR_TEMP"",\r\n""IGDP_NRSI_C_IFU_TEMP"",\r\n""IGDP_NRSI_C_BP1_TEMP"",\r\n""IGDP_NRSI_C_BP2_TEMP"",\r\n""IGDP_NRSI_C_BP3_TEMP"",\r\n""IGDP_NRSI_C_BP4_TEMP"",\r\n""IGDP_NRSI_C_RMA_TEMP"",\r\n\r\n""INRSH_CAA_VREFOFF"",\r\n""INRSH_CAA_VREF"",\r\n\r\n""INRSH_LAMP_SEL"",\r\n""INRSI_C_CAA_CURRENT"",\r\n""INRSI_C_CAA_VOLTAGE"",\r\n\r\n""IGDP_NRSI_C_CAAL1_TEMP"",\r\n""IGDP_NRSI_C_CAAL2_TEMP"",\r\n""IGDP_NRSI_C_CAAL3_TEMP"",\r\n""IGDP_NRSI_C_CAAL4_TEMP"",\r\n\r\n""INRSH_FWA_ADCMGAIN"",\r\n""INRSH_FWA_ADCMOFFSET"",\r\n""INRSH_FWA_MOTOR_VREF"",\r\n\r\n""IGDP_NRSI_C_FWA_TEMP"",\r\n\r\n""INRSH_GWA_ADCMGAIN"",\r\n""INRSH_GWA_ADCMOFFSET"",\r\n""INRSH_GWA_MOTOR_VREF"",\r\n\r\n""IGDP_NRSI_C_GWA_TEMP"",\r\n\r\n""INRSH_RMA_ADCMGAIN"",\r\n""INRSH_RMA_ADCMOFFSET"",\r\n\r\n""SI_GZCTS75A"",\r\n""SI_GZCTS68A"",\r\n""SI_GZCTS81A"",\r\n""SI_GZCTS80A"",\r\n""SI_GZCTS70A"",\r\n""SI_GZCTS76A"",\r\n""SI_GZCTS79A"",\r\n""SI_GZCTS77A"",\r\n""SI_GZCTS78A"",\r\n""SI_GZCTS69A"",\r\n""SI_GZCTS75B"",\r\n""SI_GZCTS68B"",\r\n""SI_GZCTS81B"",\r\n""SI_GZCTS80B"",\r\n""SI_GZCTS70B"",\r\n""SI_GZCTS76B"",\r\n""SI_GZCTS79B"",\r\n""SI_GZCTS77B"",\r\n""SI_GZCTS78B"",\r\n""SI_GZCTS69B"",\r\n\r\n""SE_ZINRSMCEA"",\r\n""SE_ZINRSMCEB"",\r\n\r\n""INRSM_MCE_AIC_1R5_V"",\r\n""INRSM_MCE_AIC_3R3_V"",\r\n""INRSM_MCE_AIC_5_V"",\r\n""INRSM_MCE_AIC_P12_V"",\r\n""INRSM_MCE_AIC_N12_V"",\r\n""INRSM_MCE_AIC_3R3_I"",\r\n""INRSM_MCE_AIC_5_I"",\r\n""INRSM_MCE_AIC_P12_I"",\r\n""INRSM_MCE_AIC_N12_I"",\r\n\r\n""INRSM_MCE_MDAC_1R5_V"",\r\n""INRSM_MCE_MDAC_3R3_V"",\r\n""INRSM_MCE_MDAC_5_V"",\r\n""INRSM_MCE_MDAC_P12_V"",\r\n""INRSM_MCE_MDAC_N12_V"",\r\n""INRSM_MCE_MDAC_3R3_I"",\r\n""INRSM_MCE_MDAC_5_I"",\r\n""INRSM_MCE_MDAC_P12_I"",\r\n""INRSM_MCE_MDAC_N12_I"",\r\n\r\n""INRSM_MCE_PCA_TMP1"",\r\n""INRSM_MCE_PCA_TMP2"",\r\n""INRSM_MCE_AIC_TMP_FPGA"",\r\n""INRSM_MCE_AIC_TMP_ADC"",\r\n""INRSM_MCE_AIC_TMP_VREG"",\r\n""INRSM_MCE_MDAC_TMP_FPGA"",\r\n""INRSM_MCE_MDAC_TMP_OSC"",\r\n""INRSM_MCE_MDAC_TMP_BRD"",\r\n""INRSM_MCE_MDAC_TMP_PHA"",\r\n""INRSM_MCE_MDAC_TMP_PHB"",\r\n\r\n""INRSM_Q1_TMP_A"",\r\n""INRSM_Q2_TMP_A"",\r\n""INRSM_Q3_TMP_A"",\r\n""INRSM_Q4_TMP_A"",\r\n""INRSM_MECH_MTR_TMP_A"",\r\n""INRSM_LL_MTR_TMP_A"",\r\n""INRSM_MSA_TMP_A"",\r\n\r\n""INRSM_Q1_TMP_B"",\r\n""INRSM_Q2_TMP_B"",\r\n""INRSM_Q3_TMP_B"",\r\n""INRSM_Q4_TMP_B"",\r\n""INRSM_MECH_MTR_TMP_B"",\r\n""INRSM_LL_MTR_TMP_B"",\r\n""INRSM_MSA_TMP_B"",\r\n\r\n""INRSM_MSA_Q1_365VDD"",\r\n""INRSM_MSA_Q1_365VPP"",\r\n""INRSM_MSA_Q1_171VPP"",\r\n""IGDPM_MSA_Q1_365IDD"",\r\n""IGDPM_MSA_Q1_365IPP"",\r\n""IGDPM_MSA_Q1_171RTN"",\r\n\r\n""INRSM_MSA_Q2_365VDD"",\r\n""INRSM_MSA_Q2_365VPP"",\r\n""INRSM_MSA_Q2_171VPP"",\r\n""IGDPM_MSA_Q2_365IDD"",\r\n""IGDPM_MSA_Q2_365IPP"",\r\n""IGDPM_MSA_Q2_171RTN"",\r\n\r\n""INRSM_MSA_Q3_365VDD"",\r\n""INRSM_MSA_Q3_365VPP"",\r\n""INRSM_MSA_Q3_171VPP"",\r\n""IGDPM_MSA_Q3_365IDD"",\r\n""IGDPM_MSA_Q3_365IPP"",\r\n""IGDPM_MSA_Q3_171RTN"",\r\n\r\n""INRSM_MSA_Q4_365VDD"",\r\n""INRSM_MSA_Q4_365VPP"",\r\n""INRSM_MSA_Q4_171VPP"",\r\n""IGDPM_MSA_Q4_365IDD"",\r\n""IGDPM_MSA_Q4_365IPP"",\r\n""IGDPM_MSA_Q4_171RTN"",\r\n\r\n""LAMP_FLAT1_CURR"",\r\n""LAMP_FLAT2_CURR"",\r\n""LAMP_FLAT3_CURR"",\r\n""LAMP_FLAT4_CURR"",\r\n""LAMP_FLAT5_CURR"",\r\n""LAMP_LINE1_CURR"",\r\n""LAMP_LINE2_CURR"",\r\n""LAMP_LINE3_CURR"",\r\n""LAMP_LINE4_CURR"",\r\n""LAMP_REF_CURR"",\r\n""LAMP_TEST_CURR"",\r\n\r\n""LAMP_FLAT1_VOLT"",\r\n""LAMP_FLAT2_VOLT"",\r\n""LAMP_FLAT3_VOLT"",\r\n""LAMP_FLAT4_VOLT"",\r\n""LAMP_FLAT5_VOLT"",\r\n""LAMP_LINE1_VOLT"",\r\n""LAMP_LINE2_VOLT"",\r\n""LAMP_LINE3_VOLT"",\r\n""LAMP_LINE4_VOLT"",\r\n""LAMP_REF_VOLT"",\r\n""LAMP_TEST_VOLT""]\r\n\r\nmnemonic_wheelpositions = [\r\n""INRSI_C_FWA_POSITION_F110W"",\r\n""INRSI_C_FWA_POSITION_F100LP"",\r\n""INRSI_C_FWA_POSITION_F140X"",\r\n""INRSI_C_FWA_POSITION_OPAQUE"",\r\n""INRSI_C_FWA_POSITION_F290LP"",\r\n""INRSI_C_FWA_POSITION_F170LP"",\r\n""INRSI_C_FWA_POSITION_CLEAR"",\r\n""INRSI_C_FWA_POSITION_F070LP"",\r\n\r\n""INRSI_C_GWA_X_POSITION_PRISM"",\r\n""INRSI_C_GWA_Y_POSITION_PRISM"",\r\n\r\n""INRSI_C_GWA_X_POSITION_MIRROR"",\r\n""INRSI_C_GWA_Y_POSITION_MIRROR"",\r\n\r\n""INRSI_C_GWA_X_POSITION_G140H"",\r\n""INRSI_C_GWA_Y_POSITION_G140H"",\r\n\r\n""INRSI_C_GWA_X_POSITION_G235H"",\r\n""INRSI_C_GWA_Y_POSITION_G235H"",\r\n\r\n""INRSI_C_GWA_X_POSITION_G395H"",\r\n""INRSI_C_GWA_Y_POSITION_G395H"",\r\n\r\n""INRSI_C_GWA_X_POSITION_G140M"",\r\n""INRSI_C_GWA_Y_POSITION_G140M"",\r\n\r\n""INRSI_C_GWA_X_POSITION_G235M"",\r\n""INRSI_C_GWA_Y_POSITION_G235M"",\r\n\r\n""INRSI_C_GWA_X_POSITION_G395M"",\r\n""INRSI_C_GWA_Y_POSITION_G395M"" ]\r\n\r\nfw_nominals = {\r\n\'F110W\': -123.99,\r\n\'F100LP\' : -10.32,\r\n\'CLEAR\' : -56.44,\r\n\'F070LP\' : 43.45,\r\n\'F140X\' : -78.37,\r\n\'OPAQUE\' : 21.58,\r\n\'F290LP\' : -95.78,\r\n\'F170LP\' : 8.95}\r\n\r\ngwx_nominals = {\r\n\'PRISM\' : 169.01,\r\n\'MIRROR\' : 171.11,\r\n\'G140H\' : 180.25,\r\n\'G235H\' : 176.66,\r\n\'G395H\' : 159.96,\r\n\'G140M\' : 164.31,\r\n\'G235M\' : 159.24,\r\n\'G395M\' : 141.69}\r\n\r\ngwy_nominals = {\r\n\'PRISM\' : 17.08,\r\n\'MIRROR\' : 98.72,\r\n\'G140H\' : 67.47,\r\n\'G235H\' : 70.00,\r\n\'G395H\' : 73.29,\r\n\'G140M\' : 63.18,\r\n\'G235M\' : 69.81,\r\n\'G395M\' : 89.57}\r\n\r\n#use this list for query\r\nmnemonic_set_query = mnemonic_set_database + mnemonic_for_conditions\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/utils/process_data.py,0,"b'""""""This module holds functions for miri data trending\r\n\r\nAll functions in this module are tailored for the miri datatrending application.\r\nDetailed descriptions are given for every function individually.\r\n\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n\r\nDependencies\r\n------------\r\nMIRI_trend_requestsDRAFT1900201.docx\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\n\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.mnemonics as mn\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.condition as cond\r\nimport statistics\r\nimport sqlite3\r\nimport warnings\r\nimport numpy as np\r\nfrom collections import defaultdict\r\n\r\n\r\ndef extract_data(condition, mnemonic):\r\n    \'\'\'Function extracts data from given mnemmonic at a given condition\r\n    Parameters\r\n    ----------\r\n    condition : object\r\n        conditon object that holds one or more subconditions\r\n    mnemonic : AstropyTable\r\n        holds single table with mnemonic data\r\n    Return\r\n    ------\r\n    temp : list  or None\r\n        holds data that applies to given condition\r\n    \'\'\'\r\n    temp = []\r\n\r\n    #look for all values that fit to the given conditions\r\n    for element in mnemonic:\r\n        if condition.state(float(element[\'time\'])):\r\n            temp.append(float(element[\'value\']))\r\n\r\n    #return temp is one ore more values fit to the condition\r\n    #return None if no applicable data was found\r\n    if len(temp) > 0:\r\n        return temp\r\n    else:\r\n        return None\r\n\r\ndef lamp_distinction(caa_flag, lamp_sel, lamp_curr, lamp_volt):\r\n    """"""Distincts over all calibration lamps and returns representative current means\r\n        each\r\n    Parameters\r\n    ----------\r\n    """"""\r\n\r\n    #initilize empty dict\r\n    lamp_values = defaultdict(list)\r\n\r\n    for index, flag in enumerate(caa_flag):\r\n\r\n        if flag[\'value\'] == \'ON\':\r\n\r\n            #initialize lamp value to default\r\n            current_lamp = ""default""\r\n\r\n            #find current lamp value\r\n            for lamp in lamp_sel:\r\n                if lamp[\'time\'] <= flag[\'time\']:\r\n                    current_lamp = lamp[\'value\']\r\n\r\n            #go to next Value if dummy lamps are activated\r\n            if (current_lamp == \'NO_LAMP\') or (current_lamp == \'DUMMY\'):\r\n                continue\r\n\r\n            #define on_time of current lamp\r\n            try:\r\n                start_time = flag[\'time\']\r\n\r\n                i = 1\r\n                if caa_flag[index+i][\'value\'] == \'OFF\':\r\n                    end_time = caa_flag[index+1][\'time\']\r\n                else:\r\n                    i += 1\r\n\r\n            except IndexError:\r\n                break\r\n\r\n            #append and evaluate current and voltage values\r\n            temp_curr = []\r\n            temp_volt = []\r\n\r\n            #append current values to list\r\n            for curr in lamp_curr:\r\n                if curr[\'time\'] >= start_time:\r\n                    if curr[\'time\'] < end_time:\r\n                        temp_curr.append(float(curr[\'value\']))\r\n                    else:\r\n                        break\r\n            #append voltage values to list\r\n            for volt in lamp_volt:\r\n                if volt[\'time\'] >= start_time :\r\n                    if volt[\'time\'] < end_time:\r\n                        temp_volt.append(float(volt[\'value\']))\r\n                    else:\r\n                        break\r\n\r\n            lamp_data = []\r\n            #append current values\r\n            lamp_data.append(start_time)\r\n            lamp_data.append(end_time)\r\n            lamp_data.append(len(temp_curr))\r\n            lamp_data.append(statistics.mean(temp_curr))\r\n            lamp_data.append(statistics.stdev(temp_curr))\r\n            #append voltage values\r\n            lamp_data.append(len(temp_volt))\r\n            lamp_data.append(statistics.mean(temp_volt))\r\n            lamp_data.append(statistics.stdev(temp_volt))\r\n            lamp_values[current_lamp].append(( lamp_data ))\r\n\r\n    return lamp_values\r\n\r\ndef extract_filterpos(move_stat, wheel_pos, wheel_val):\r\n    \'\'\'Extracts ratio values which correspond to given position values and their\r\n       proposed nominals\r\n    Parameters\r\n    ----------\r\n    condition : object\r\n        conditon object that holds one or more subconditions\r\n    nominals : dict\r\n        holds nominal values for all wheel positions\r\n    ratio_mem : AstropyTable\r\n        holds ratio values of one specific mnemonic\r\n    pos_mem : AstropyTable\r\n        holds pos values of one specific mnemonic\r\n    Return\r\n    ------\r\n    pos_values : dict\r\n        holds ratio values and times with corresponding positionlabel as key\r\n    \'\'\'\r\n\r\n    #initilize empty dict for assigned ratio values\r\n    pos_values = defaultdict(list)\r\n\r\n    for index, stat in enumerate(move_stat):\r\n\r\n        #raise warning if position is UNKNOWN\r\n        if stat[\'value\'] == ""SUCCESS"":\r\n\r\n            #initialize lamp value to default\r\n            current_pos = ""default""\r\n            pos_val = 0\r\n            pos_time = 0\r\n\r\n            #Evaluate current position\r\n            for pos in wheel_pos:\r\n                if pos[\'time\'] <= stat[\'time\']:\r\n                    current_pos = pos[\'value\']\r\n                if pos[\'time\'] > stat[\'time\']:\r\n                    break\r\n\r\n            #Evaluate corresponding value\r\n            for val in wheel_val:\r\n                if val[\'time\'] <= stat[\'time\']:\r\n                    pos_val = val[\'value\']\r\n                    pos_time = val[\'time\']\r\n                if val[\'time\'] > stat[\'time\']:\r\n                    break\r\n\r\n            print (current_pos, pos_val, pos_time)\r\n\r\n            if current_pos != \'default\':\r\n                pos_values[current_pos].append((pos_time, pos_val))\r\n        else:\r\n            continue\r\n\r\n    return pos_values\r\n\r\ndef once_a_day_routine(mnemonic_data):\r\n    \'\'\'Routine for processing a 15min data file once a day\r\n    Parameters\r\n    ----------\r\n    mnemonic_data : dict\r\n        dict holds time and value in a astropy table with correspining identifier as key\r\n    Return\r\n    ------\r\n    return_data : dict\r\n        Holds extracted data with applied conditions\r\n    \'\'\'\r\n\r\n    #abbreviate attribute\r\n    m = mnemonic_data\r\n    return_data = dict()\r\n\r\n    ###########################################################################\r\n    con_set_1 = [                                                           \\\r\n    cond.unequal(m.mnemonic(\'INRSD_EXP_STAT\'),\'STARTED\')]\r\n    #setup condition\r\n    condition_1 = cond.condition(con_set_1)\r\n\r\n    for identifier in mn.mnemonic_cond_1:\r\n        data = extract_data(condition_1, m.mnemonic(identifier))\r\n        if data != None:\r\n            return_data.update( {identifier:data} )\r\n        else:\r\n            print(""no data for {}"".format(identifier))\r\n    del condition_1\r\n\r\n    ###########################################################################\r\n    con_set_2 = [                                                           \\\r\n    cond.equal(m.mnemonic(\'INRSH_LAMP_SEL\'), \'NO_LAMP\')]\r\n    #setup condition\r\n    condition_2 = cond.condition(con_set_2)\r\n\r\n    for identifier in mn.mnemonic_cond_2:\r\n        data = extract_data(condition_2, m.mnemonic(identifier))\r\n        if data != None:\r\n            return_data.update( {identifier:data} )\r\n        else:\r\n            print(""no data for {}"".format(identifier))\r\n    del condition_2\r\n\r\n    ###########################################################################\r\n    con_set_3 = [                                                           \\\r\n    cond.unequal(m.mnemonic(\'INRSM_MOVE_STAT\'), \'STARTED\')]\r\n    #setup condition\r\n    condition_3 = cond.condition(con_set_3)\r\n\r\n    for identifier in mn.mnemonic_cond_3:\r\n        data = extract_data(condition_3, m.mnemonic(identifier))\r\n        if data != None:\r\n            return_data.update( {identifier:data} )\r\n        else:\r\n            print(""no data for {}"".format(identifier))\r\n    del condition_3\r\n\r\n    return return_data\r\n\r\n\r\ndef whole_day_routine(mnemonic_data):\r\n    \'\'\'Proposed routine for processing a 15min data file once a day\r\n\r\n    Parameters\r\n    ----------\r\n    mnemonic_data : dict\r\n        dict holds time and value in a astropy table with correspining identifier as key\r\n\r\n    Return\r\n    ------\r\n    data_cond_1 : dict\r\n        holds extracted data with condition 1 applied\r\n    data_cond_1 : dict\r\n        holds extracted data with condition 2 applied\r\n    \'\'\'\r\n\r\n    #abbreviate attribute\r\n    m = mnemonic_data\r\n    return_data = dict()\r\n\r\n    ###########################################################################\r\n    con_set_ft_10 = [\r\n    cond.equal(m.mnemonic(\'ICTM_RT_FILTER\'), 10, stringval = False)]\r\n    #setup condition\r\n    condition_ft_10 = cond.condition(con_set_ft_10)\r\n\r\n    for identifier in mn.mnemonic_ft10:\r\n        data = extract_data(condition_ft_10, m.mnemonic(identifier))\r\n        if data != None:\r\n            return_data.update( {identifier:data} )\r\n        else:\r\n            print(""no data for {}"".format(identifier))\r\n    del condition_ft_10\r\n\r\n    ##########################################################################\r\n    con_set_caa = [                                                           \\\r\n    cond.equal(m.mnemonic(\'INRSH_CAA_PWRF_ST\'), \'ON\')]\r\n    #setup condition\r\n    condition_caa = cond.condition(con_set_caa)\r\n\r\n    for identifier in mn.mnemonic_caa:\r\n        data = extract_data(condition_caa, m.mnemonic(identifier))\r\n\r\n        if data != None:\r\n            return_data.update( {identifier:data} )\r\n        else:\r\n            print(""no data for {}"".format(identifier))\r\n\r\n    del condition_caa\r\n\r\n    ###########################################################################\r\n    data_lamps = lamp_distinction(  m.mnemonic(\'INRSI_CAA_ON_FLAG\'),\r\n                                    m.mnemonic(\'INRSH_LAMP_SEL\'),\r\n                                    m.mnemonic(\'INRSI_C_CAA_CURRENT\'),\r\n                                    m.mnemonic(\'INRSI_C_CAA_VOLTAGE\') )\r\n\r\n    return return_data, data_lamps\r\n\r\ndef wheelpos_routine(mnemonic_data):\r\n    \'\'\'Proposed routine for positionsensors each day\r\n    Parameters\r\n    ----------\r\n    mnemonic_data : dict\r\n        dict holds time and value in a astropy table with correspining identifier as key\r\n    Return\r\n    ------\r\n    FW : dict\r\n        holds FW ratio values and times with corresponding positionlabel as key\r\n    GW14 : dict\r\n        holds GW14 ratio values and times with corresponding positionlabel as key\r\n    GW23 : dict\r\n        holds GW23 ratio values and times with corresponding positionlabel as key\r\n    CCC : dict\r\n        holds CCC ratio values and times with corresponding positionlabel as key\r\n    \'\'\'\r\n\r\n    #abbreviate attribute\r\n    m = mnemonic_data\r\n\r\n    FW = extract_filterpos( m.mnemonic(\'INRSI_FWA_MOVE_ST\'),\r\n                            m.mnemonic(\'INRSI_FWA_MECH_POS\'),\r\n                            m.mnemonic(\'INRSI_C_FWA_POSITION\'))\r\n\r\n    GWX = extract_filterpos(m.mnemonic(\'INRSI_GWA_MOVE_ST\'),\r\n                            m.mnemonic(\'INRSI_GWA_MECH_POS\'),\r\n                            m.mnemonic(\'INRSI_C_GWA_X_POSITION\'))\r\n\r\n    GWY = extract_filterpos(m.mnemonic(\'INRSI_GWA_MOVE_ST\'),\r\n                            m.mnemonic(\'INRSI_GWA_MECH_POS\'),\r\n                            m.mnemonic(\'INRSI_C_GWA_Y_POSITION\'))\r\n\r\n    return FW, GWX, GWY\r\n\r\nif __name__ ==\'__main__\':\r\n    pass\r\n'"
jwql/instrument_monitors/nirspec_monitors/data_trending/utils/sql_interface.py,0,"b'""""""Module holds functions to generate and access sqlite databases\r\n\r\nThe module is tailored for use in miri data trending. It holds functions to\r\ncreate and close connections to a sqlite database. Calling the module itself\r\ncreates a sqlite database with specific tables used at miri data trending.\r\n\r\nAuthors\r\n-------\r\n    - Daniel K\xc3\xbchbacher\r\n\r\nUse\r\n---\r\n\r\nDependencies\r\n------------\r\n    import mnemonics as m\r\n\r\nReferences\r\n----------\r\n\r\nNotes\r\n-----\r\n\r\n""""""\r\nimport os\r\nimport sqlite3\r\nfrom sqlite3 import Error\r\n\r\nimport jwql.instrument_monitors.nirspec_monitors.data_trending.utils.mnemonics as m\r\nfrom jwql.utils.utils import get_config, filename_parser\r\n\r\ndef create_connection(db_file):\r\n    \'\'\'Sets up a connection or builds database\r\n    Parameters\r\n    ----------\r\n    db_file : string\r\n        represents filename of database\r\n    Return\r\n    ------\r\n    conn : DBobject or None\r\n        Connection object or None\r\n    \'\'\'\r\n    try:\r\n        conn = sqlite3.connect(db_file)\r\n        print(\'Connected to database ""{}""\'.format(db_file))\r\n        return conn\r\n    except Error as e:\r\n        print(e)\r\n    return None\r\n\r\n\r\ndef close_connection(conn):\r\n    \'\'\'Closes connection to database\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        Connection object to be closed\r\n    \'\'\'\r\n    conn.close()\r\n    print(\'Connection closed\')\r\n\r\n\r\ndef add_data(conn, mnemonic, data):\r\n    \'\'\'Add data of a specific mnemonic to database if it not exists\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        connection object to access database\r\n    mnemonic : string\r\n        identifies the table\r\n    data : list\r\n        specifies the data\r\n    \'\'\'\r\n\r\n    c = conn.cursor()\r\n\r\n    #check if data already exists (start_time as identifier)\r\n    c.execute(\'SELECT id from {} WHERE start_time= {}\'.format(mnemonic, data[0]))\r\n    temp = c.fetchall()\r\n\r\n    if len(temp) == 0:\r\n        c.execute(\'INSERT INTO {} (start_time,end_time,data_points,average,deviation) \\\r\n                VALUES (?,?,?,?,?)\'.format(mnemonic),data)\r\n        conn.commit()\r\n    else:\r\n        print(\'data for {} already exists\'.format(mnemonic))\r\n\r\n\r\ndef add_wheel_data(conn, mnemonic, data):\r\n    \'\'\'Add data of a specific wheel position to database if it not exists\r\n    Parameters\r\n    ----------\r\n    conn : DBobject\r\n        connection object to access database\r\n    mnemonic : string\r\n        identifies the table\r\n    data : list\r\n        specifies the data\r\n    \'\'\'\r\n\r\n    c = conn.cursor()\r\n\r\n    #check if data already exists (start_time)\r\n    c.execute(\'SELECT id from {} WHERE timestamp = {}\'.format(mnemonic, data[0]))\r\n    temp = c.fetchall()\r\n\r\n    if len(temp) == 0:\r\n        c.execute(\'INSERT INTO {} (timestamp, value) \\\r\n                VALUES (?,?)\'.format(mnemonic),data)\r\n        conn.commit()\r\n    else:\r\n        print(\'data already exists\')\r\n\r\n\r\ndef main():\r\n    \'\'\' Creates SQLite database with tables proposed in mnemonics.py\'\'\'\r\n\r\n    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\r\n\r\n    #generate paths\r\n    DATABASE_LOCATION = os.path.join(get_config()[\'jwql_dir\'], \'database\')\r\n    DATABASE_FILE = os.path.join(DATABASE_LOCATION, \'nirspec_database.db\')\r\n\r\n    conn = create_connection(DATABASE_FILE)\r\n\r\n    c=conn.cursor()\r\n\r\n    for mnemonic in m.mnemonic_set_database:\r\n        try:\r\n            c.execute(\'CREATE TABLE IF NOT EXISTS {} (         \\\r\n                                        id INTEGER,                     \\\r\n                                        start_time REAL,                \\\r\n                                        end_time REAL,                  \\\r\n                                        data_points INTEGER,               \\\r\n                                        average REAL,                   \\\r\n                                        deviation REAL,                 \\\r\n                                        performed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\\r\n                                        PRIMARY KEY (id));\'.format(mnemonic))\r\n        except Error as e:\r\n            print(\'e\')\r\n\r\n    for mnemonic in m.mnemonic_wheelpositions:\r\n        try:\r\n            c.execute(\'CREATE TABLE IF NOT EXISTS {} (         \\\r\n                                        id INTEGER,            \\\r\n                                        timestamp REAL,        \\\r\n                                        value REAL,            \\\r\n                                        performed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\\\r\n                                        PRIMARY KEY (id));\'.format(mnemonic))\r\n        except Error as e:\r\n            print(\'e\')\r\n\r\n    print(""Database initial setup complete"")\r\n    conn.commit()\r\n    close_connection(conn)\r\n\r\n#sets up database if called as main\r\nif __name__ == ""__main__"":\r\n    main()\r\n    print(""sql_interface.py done"")\r\n'"
jwql/website/apps/jwql/monitor_pages/__init__.py,0,b'from .monitor_dark_bokeh import DarkMonitor\nfrom .monitor_filesystem_bokeh import MonitorFilesystem\nfrom .monitor_mast_bokeh import MastMonitor\n'
jwql/website/apps/jwql/monitor_pages/dark_monitor.py,3,"b'import os\n\nimport numpy as np\n\nfrom jwql.bokeh_templating import BokehTemplate\nfrom jwql.utils.utils import get_config\n\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n\nclass DarkMonitor(BokehTemplate):\n\n    def pre_init(self):\n\n        self._embed = True\n\n        #app design\n        self.format_string = None\n        self.interface_file = os.path.join(SCRIPT_DIR, ""dark_monitor_interface.yml"")\n\n        self.settings = get_config()\n        self.output_dir = self.settings[\'outputs\']\n\n        self.load_data()\n        self.timestamps = np.arange(10) / 10.\n        self.dark_current = np.arange(10)\n\n\n    def post_init(self):\n\n        self.refs[\'dark_current_yrange\'].start = min(self.dark_current)\n        self.refs[\'dark_current_yrange\'].end = max(self.dark_current)\n\n\n    def load_data(self):\n        #actually load data:\n        new_data = np.arange(10) / 20\n\n        #update columndatasource\n        self.dark_current = new_data\n\nDarkMonitor()\n'"
jwql/website/apps/jwql/monitor_pages/monitor_dark_bokeh.py,4,"b'""""""This module contains code for the dark current monitor Bokeh plots.\n\nAuthor\n------\n\n    - Bryan Hilbert\n    - Gray Kanarek\n    - Lauren Chambers\n\nUse\n---\n\n    This module can be used from the command line as such:\n\n    ::\n\n        from jwql.website.apps.jwql import monitor_pages\n        monitor_template = monitor_pages.DarkMonitor(\'NIRCam\', \'NRCA3_FULL\')\n        script, div = monitor_template.embed(""dark_current_time_figure"")\n""""""\n\nimport os\n\nfrom astropy.io import fits\nfrom astropy.time import Time\nfrom bokeh.models.tickers import LogTicker\nimport numpy as np\n\nfrom jwql.database.database_interface import session\nfrom jwql.database.database_interface import NIRCamDarkQueryHistory, NIRCamDarkPixelStats, NIRCamDarkDarkCurrent\nfrom jwql.database.database_interface import NIRISSDarkQueryHistory, NIRISSDarkPixelStats, NIRISSDarkDarkCurrent\nfrom jwql.database.database_interface import MIRIDarkQueryHistory, MIRIDarkPixelStats, MIRIDarkDarkCurrent\nfrom jwql.database.database_interface import NIRSpecDarkQueryHistory, NIRSpecDarkPixelStats, NIRSpecDarkDarkCurrent\nfrom jwql.database.database_interface import FGSDarkQueryHistory, FGSDarkPixelStats, FGSDarkDarkCurrent\nfrom jwql.utils.constants import JWST_INSTRUMENT_NAMES_MIXEDCASE\nfrom jwql.utils.utils import get_config\nfrom jwql.bokeh_templating import BokehTemplate\n\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\nclass DarkMonitor(BokehTemplate):\n\n    # Combine instrument and aperture into a single property because we\n    # do not want to invoke the setter unless both are updated\n    @property\n    def aperture_info(self):\n        return (self._instrument, self._aperture)\n\n    @aperture_info.setter\n    def aperture_info(self, info):\n        self._instrument, self._aperture = info\n        self.pre_init()\n        self.post_init()\n\n    def _dark_mean_image(self):\n        """"""Update bokeh objects with mean dark image data.""""""\n\n        # Open the mean dark current file and get the data\n        mean_dark_image_file = self.pixel_table[-1].mean_dark_image_file\n        mean_slope_dir = os.path.join(get_config()[\'outputs\'], \'dark_monitor\', \'mean_slope_images\')\n        mean_dark_image_path = os.path.join(mean_slope_dir, mean_dark_image_file)\n        with fits.open(mean_dark_image_path) as hdulist:\n            data = hdulist[1].data\n\n        # Update the plot with the data and boundaries\n        y_size, x_size = np.shape(data)\n        self.refs[""mean_dark_source""].data[\'image\'] = [data]\n        self.refs[""stamp_xr""].end = x_size\n        self.refs[""stamp_yr""].end = y_size\n        self.refs[""mean_dark_source""].data[\'dw\'] = [x_size]\n        self.refs[""mean_dark_source""].data[\'dh\'] = [x_size]\n\n        # Set the image color scale\n        self.refs[""log_mapper""].high = 0\n        self.refs[""log_mapper""].low = -.2\n\n        # This should add ticks to the colorbar, but it doesn\'t\n        self.refs[""mean_dark_cbar""].ticker = LogTicker()\n\n        # Add a title\n        self.refs[\'mean_dark_image_figure\'].title.text = self._aperture\n        self.refs[\'mean_dark_image_figure\'].title.align = ""center""\n        self.refs[\'mean_dark_image_figure\'].title.text_font_size = ""20px""\n\n    def pre_init(self):\n        # Start with default values for instrument and aperture because\n        # BokehTemplate\'s __init__ method does not allow input arguments\n        try:\n            dummy_instrument = self._instrument\n            dummy_aperture = self._aperture\n        except AttributeError:\n            self._instrument = \'NIRCam\'\n            self._aperture = \'NRCA1_FULL\'\n\n        self._embed = True\n\n        # Fix aperture/detector name discrepency\n        if self._aperture in [\'NRCA5_FULL\', \'NRCB5_FULL\']:\n            self.detector = \'{}LONG\'.format(self._aperture[0:4])\n        else:\n            self.detector = self._aperture.split(\'_\')[0]\n\n        # App design\n        self.format_string = None\n        self.interface_file = os.path.join(SCRIPT_DIR, ""yaml"", ""dark_monitor_interface.yaml"")\n\n        # Load data tables\n        self.load_data()\n\n        # Data for mean dark versus time plot\n        datetime_stamps = [row.obs_mid_time for row in self.dark_table]\n        times = Time(datetime_stamps, format=\'datetime\', scale=\'utc\')  # Convert to MJD\n        self.timestamps = times.mjd\n        self.dark_current = [row.mean for row in self.dark_table]\n\n        # Data for dark current histogram plot (full detector)\n        # Just show the last histogram, which is the one most recently\n        # added to the database\n        last_hist_index = -1\n        self.last_timestamp = datetime_stamps[last_hist_index].isoformat()\n        self.full_dark_bin_center = np.array([row.hist_dark_values for\n                                              row in self.dark_table])[last_hist_index]\n        self.full_dark_amplitude = [row.hist_amplitudes for\n                                    row in self.dark_table][last_hist_index]\n        self.full_dark_bottom = np.zeros(len(self.full_dark_amplitude))\n        deltas = self.full_dark_bin_center[1:] - self.full_dark_bin_center[0: -1]\n        self.full_dark_bin_width = np.append(deltas[0], deltas)\n\n    def post_init(self):\n\n        self._update_dark_v_time()\n        self._update_hist()\n        self._dark_mean_image()\n\n    def identify_tables(self):\n        """"""Determine which dark current database tables as associated with\n        a given instrument""""""\n\n        mixed_case_name = JWST_INSTRUMENT_NAMES_MIXEDCASE[self._instrument.lower()]\n        self.query_table = eval(\'{}DarkQueryHistory\'.format(mixed_case_name))\n        self.pixel_table = eval(\'{}DarkPixelStats\'.format(mixed_case_name))\n        self.stats_table = eval(\'{}DarkDarkCurrent\'.format(mixed_case_name))\n\n    def load_data(self):\n        """"""Query the database tables to get data""""""\n\n        # Determine which database tables are needed based on instrument\n        self.identify_tables()\n\n        # Query database for all data in NIRCamDarkDarkCurrent with a matching aperture\n        self.dark_table = session.query(self.stats_table) \\\n            .filter(self.stats_table.aperture == self._aperture) \\\n            .all()\n\n        self.pixel_table = session.query(self.pixel_table) \\\n            .filter(self.pixel_table.detector == self.detector) \\\n            .all()\n\n    def _update_dark_v_time(self):\n\n        # Define y range of dark current v. time plot\n        buffer_size = 0.05 * (max(self.dark_current) - min(self.dark_current))\n        self.refs[\'dark_current_yrange\'].start = min(self.dark_current) - buffer_size\n        self.refs[\'dark_current_yrange\'].end = max(self.dark_current) + buffer_size\n\n        # Define x range of dark current v. time plot\n        horizontal_half_buffer = (max(self.timestamps) - min(self.timestamps)) * 0.05\n        if horizontal_half_buffer == 0:\n            horizontal_half_buffer = 1.  # day\n        self.refs[\'dark_current_xrange\'].start = min(self.timestamps) - horizontal_half_buffer\n        self.refs[\'dark_current_xrange\'].end = max(self.timestamps) + horizontal_half_buffer\n\n        # Add a title\n        self.refs[\'dark_current_time_figure\'].title.text = self._aperture\n        self.refs[\'dark_current_time_figure\'].title.align = ""center""\n        self.refs[\'dark_current_time_figure\'].title.text_font_size = ""20px""\n\n    def _update_hist(self):\n\n        # Define y range of dark current histogram\n        buffer_size = 0.05 * (max(self.full_dark_amplitude) - min(self.full_dark_bottom))\n        self.refs[\'dark_histogram_yrange\'].start = min(self.full_dark_bottom)\n        self.refs[\'dark_histogram_yrange\'].end = max(self.full_dark_amplitude) + buffer_size\n\n        # Define x range of dark current histogram\n        self.refs[\'dark_histogram_xrange\'].start = min(self.full_dark_bin_center)\n        self.refs[\'dark_histogram_xrange\'].end = max(self.full_dark_bin_center)\n\n        # Add a title\n        self.refs[\'dark_full_histogram_figure\'].title.text = self._aperture\n        self.refs[\'dark_full_histogram_figure\'].title.align = ""center""\n        self.refs[\'dark_full_histogram_figure\'].title.text_font_size = ""20px""\n'"
jwql/website/apps/jwql/monitor_pages/monitor_filesystem_bokeh.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Jan 16 14:09:18 2019\n\n@author: gkanarek\n""""""\n\nimport json\nimport os\n\nfrom astropy.table import Table, vstack\nfrom astropy.time import Time\n\nfrom jwql.bokeh_templating import BokehTemplate\nfrom jwql.utils.utils import get_config\n\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n\nFIG_FORMATS = """"""\nFigure:\n    tools: \'pan,box_zoom,reset,wheel_zoom,save\'\n    x_axis_type: \'datetime\'\n    x_axis_label: \'Date\'\n    sizing_mode: \'stretch_both\'\nLine:\n    line_width: 2\n""""""\n\n\nclass MonitorFilesystem(BokehTemplate):\n\n    def pre_init(self):\n        self._embed = True\n\n        # App design\n        self.format_string = FIG_FORMATS\n        self.interface_file = os.path.join(SCRIPT_DIR, ""yaml"", ""monitor_filesystem_interface.yaml"")\n\n        # Get path, directories and files in system and count files in all directories\n        self.settings = get_config()\n        self.filesystem = self.settings[\'filesystem\']\n        self.outputs_dir = os.path.join(self.settings[\'outputs\'],\n                                        \'monitor_filesystem\')\n\n        self.allowed_types = [\'fits_files\', \'uncal\', \'cal\', \'rate\', \'rateints\',\n                              \'i2d\', \'nrc\', \'nrs\', \'nis\', \'mir\', \'gui\']\n\n        # Load any existing data\n        self.initial_load()\n\n        self.types_k = [\'circle\', \'diamond\', \'square\', \'triangle\',\n                        \'asterisk\'] + [\'x\'] * 6\n        self.types_y = [\'fits\', \'uncal\', \'cal\', \'rate\', \'rateint\',\n                        \'i2d\', \'nrc\', \'nrs\', \'nis\', \'mir\', \'fgs\']\n        self.types_c = [\'black\', \'red\', \'blue\', \'green\', \'orange\', \'purple\',\n                        \'midnightblue\', \'springgreen\', \'darkcyan\',\n                        \'dodgerblue\', \'darkred\']\n        self.types_l = [\'Total FITS files\', \'Uncalibrated FITS files\',\n                        \'Calibrated FITS files\', \'Rate FITS files\',\n                        \'Rateints FITS files\', \'I2D FITS files\',\n                        \'NIRCam FITS files\', \'NIRSpec FITS files\',\n                        \'NIRISS FITS files\', \'MIRI FITS files\',\n                        \'FGS FITS files\']\n\n    def post_init(self):\n        self.update_plots(full=True)\n\n    def initial_load(self):\n        statsfile = os.path.join(self.outputs_dir, \'statsfile.json\')\n        filebytype = os.path.join(self.outputs_dir, \'filesbytype.json\')\n        sizebytype = os.path.join(self.outputs_dir, \'sizebytype.json\')\n\n        self.statistics = Table(names=[\'timestamp\', \'file_count\', \'total\',\n                                       \'available\', \'used\', \'percent_used\'],\n                                dtype=[Time, int, int, int, int, float])\n        self.statistics[\'percent_used\'].format = ""%.1f""\n        if os.path.exists(statsfile):\n            with open(statsfile) as f:\n                stats = json.load(f)\n            times, fc, tot, avail, used, perc = zip(*stats)\n            self.statistics[\'timestamp\'] = Time(times)\n            self.statistics[\'file_count\'] = map(int, fc)\n            self.statistics[\'total\'] = map(int, tot)\n            self.statistics[\'available\'] = map(int, avail)\n            self.statistics[\'used\'] = map(int, used)\n            self.statistics[\'percent_used\'] = map(float, perc)\n\n        self.ftypes = Table(names=[\'timestamp\'] + self.allowed_types,\n                            dtype=[Time] + [int] * 11)\n        if os.path.exists(filebytype):\n            with open(filebytype) as f:\n                fbytype = json.load(f)\n            times, *ftypes = zip(*fbytype)\n            self.ftypes[\'timestamp\'] = Time(times)\n            for c, colname in enumerate(self.allowed_types):\n                self.ftypes[colname] = map(int, ftypes[c])\n\n        self.stypes = Table(names=[\'timestamp\'] + self.allowed_types,\n                            dtype=[Time] + [float] * 11)\n        if os.path.exists(sizebytype):\n            with open(sizebytype) as f:\n                sbytype = json.load(f)\n            times, *stypes = zip(*sbytype)\n            self.stypes[\'timestamp\'] = Time(times)\n            for c, colname in enumerate(self.allowed_types):\n                self.stypes[colname] = map(int, stypes[c])\n\n    def update_plots(self, full=False):\n\n        if full:\n            # Initialize each ColumnDataSource so that we can use stream() later\n            self.refs[\'source_filecount\'].data = {\n                \'dates\': self.statistics[\'timestamp\'].datetime64,\n                \'filecount\': self.statistics[\'file_count\'].data}\n\n            self.refs[\'source_stats\'].data = {\n                \'dates\': self.statistics[\'timestamp\'].datetime64,\n                \'systemsize\': self.statistics[\'total\'].data.astype(float) / (1024.**3),\n                \'freesize\': self.statistics[\'available\'].data.astype(float) / (1024.**3),\n                \'usedsize\': self.statistics[\'used\'].data.astype(float) / (1024.**3)}\n\n            ftype_dict = {\'dates\': self.ftypes[\'timestamp\'].datetime64}\n            ftype_dict.update({x: self.ftypes[y].data for x, y in zip(self.types_y,\n                               self.allowed_types)})\n            self.refs[\'source_files\'].data = ftype_dict\n\n            stype_dict = {\'dates\': self.stypes[\'timestamp\'].datetime64}\n            stype_dict.update({x: self.stypes[y].data for x, y in zip(self.types_y,\n                               self.allowed_types)})\n            self.refs[\'source_sizes\'].data = stype_dict\n        else:\n            new_stats, new_files, new_sizes = self.read_new_data()\n            if new_stats:\n                self.refs[\'source_filecount\'].stream({\n                    \'dates\': new_stats[\'timestamp\'].datetime64,\n                    \'filecount\': new_stats[\'file_count\'].data})\n                self.refs[\'source_stats\'].stream({\n                    \'dates\': new_stats[\'timestamp\'].datetime64,\n                    \'systemsize\': new_stats[\'total\'].data,\n                    \'freesize\': new_stats[\'available\'].data,\n                    \'usedsize\': new_stats[\'used\'].data})\n            if new_files:\n                ftype_dict = {\'dates\': new_files[\'timestamp\'].datetime64}\n                ftype_dict.update({x: new_files[y].data for x, y in zip(self.types_y,\n                                   self.allowed_types)})\n                self.refs[\'source_files\'].stream(ftype_dict)\n\n            if new_sizes:\n                stype_dict = {\'dates\': new_sizes[\'timestamp\'].datetime64}\n                stype_dict.update({x: new_sizes[y].data / (1024.**3)\n                    for x, y in zip(self.types_y, self.allowed_types)})\n                self.refs[\'source_sizes\'].data = stype_dict\n\n        if not self.statistics:\n            self.latest_timestamp = Time(0., format=\'unix\')\n        else:\n            self.latest_timestamp = self.statistics[\'timestamp\'].max()\n\n    def read_new_data(self):\n        """"""\n        Algorithm:\n            1. Read in the json files (this step will be replaced when we move\n               away from json) into tables.\n            2. Create new tables from all rows which have been added since the\n               last timestamp in the current tables.\n            3. Concatenate the new tables with a vertical join.\n            4. Return the new tables so they can be streamed to the plots.\n        """"""\n        statsfile = os.path.join(self.outputs_dir, \'statsfile.json\')\n        filebytype = os.path.join(self.outputs_dir, \'filesbytype.json\')\n        sizebytype = os.path.join(self.outputs_dir, \'sizebytype.json\')\n\n        # Have any of the files been modified since the last timestamp?\n        stats_modtime = Time(os.stat(statsfile).st_mtime, format=\'unix\')\n        files_modtime = Time(os.stat(filebytype).st_mtime, format=\'unix\')\n        sizes_modtime = Time(os.stat(sizebytype).st_mtime, format=\'unix\')\n\n        new_stats = Table(names=self.statistics.colnames,\n                          dtype=self.statistics.dtype)\n        new_files = Table(names=self.ftypes.colnames,\n                          dtype=self.ftypes.dtype)\n        new_sizes = Table(names=self.stypes.colnames,\n                          dtype=self.stypes.dtype)\n\n        if stats_modtime > self.latest_timestamp:\n            with open(statsfile) as f:\n                stats = json.load(f)\n            times, fc, tot, avail, used, perc = zip(*stats)\n            times = Time(times)\n            new_rows = times > self.latest_timestamp\n            new_stats[\'timestamp\'] = times[new_rows]\n            new_stats[\'file_count\'] = map(int, fc[new_rows])\n            new_stats[\'total\'] = map(int, tot[new_rows])\n            new_stats[\'available\'] = map(int, avail[new_rows])\n            new_stats[\'used\'] = map(int, used[new_rows])\n            new_stats[\'percent_used\'] = map(float, perc[new_rows])\n\n            self.statistics = vstack([self.statistics, new_stats])\n\n        if files_modtime > self.latest_timestamp:\n            with open(filebytype) as f:\n                fbytype = json.load(f)\n            times, *ftypes = zip(*fbytype)\n            times = Time(times)\n            new_rows = times > self.latest_timestamp\n            new_files[\'timestamp\'] = times[new_rows]\n            for c, colname in enumerate(self.allowed_types):\n                new_files[colname] = map(int, ftypes[c][new_rows])\n\n            self.ftypes = vstack([self.ftypes, new_files])\n\n        if sizes_modtime > self.latest_timestamp:\n            with open(sizebytype) as f:\n                sbytype = json.load(f)\n            times, *stypes = zip(*sbytype)\n            times = Time(times)\n            new_rows = times > self.latest_timestamp\n            new_sizes[\'timestamp\'] = times[new_rows]\n            for c, colname in enumerate(self.allowed_types):\n                new_sizes[colname] = map(int, stypes[c][new_rows])\n\n            self.stypes = vstack([self.stypes, new_sizes])\n\n        return new_stats, new_files, new_sizes\n'"
jwql/website/apps/jwql/monitor_pages/monitor_mast_bokeh.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Tue Feb  5 15:19:20 2019\n\n@author: gkanarek\n""""""\n\nimport os\n\nfrom astropy.time import Time\nimport pandas as pd\n\nfrom jwql.bokeh_templating import BokehTemplate\nfrom jwql.utils.utils import get_config\n\nSCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\nclass MastMonitor(BokehTemplate):\n\n    def pre_init(self):\n        self._embed = True\n\n        # App design\n        self.format_string = None\n        self.interface_file = os.path.join(SCRIPT_DIR, \'yaml\', ""monitor_mast_interface.yaml"")\n\n        self.settings = get_config()\n        self.output_dir = self.settings[\'outputs\']\n\n        self.read_new_data()\n\n        self.cache_time = Time(0., format=\'unix\')\n\n        self.jwst_bar_colors = self.caom_bar_colors = 3\n        self.jwst_datacols = []\n        self.caom_datacols = []\n\n    def post_init(self):\n        self.update_plots()\n\n    def read_new_data(self):\n        """"""\n        Placeholder to read what are currently Pandas dataframe dumps. Replace\n        this when we have a new database infrastructure.\n        """"""\n\n        jwst_filepath = os.path.join(self.outputs_dir, \'database_monitor_jwst.json\')\n        caom_filepath = os.path.join(self.outputs_dir, \'database_monitor_caom.json\')\n\n        jwst_modtime = Time(os.stat(jwst_filepath).st_mtime, format=\'unix\')\n        caom_modtime = Time(os.stat(caom_filepath).st_mtime, format=\'unix\')\n\n        if jwst_modtime >= self.cache_time:\n            self.jwst_df = pd.read_json(jwst_filepath, orient=\'records\')\n        if caom_modtime >= self.cache_time:\n            self.caom_df = pd.read_json(caom_filepath, orient=\'records\')\n\n        self.cache_time = Time.now()\n\n    def update_plots(self):\n        """"""\n        Update the various sources and variables for the MAST monitor bar charts.\n        """"""\n\n        self.read_new_data()\n\n        jwst_groups = list(self.jwst_df[\'instrument\'])\n        caom_groups = list(self.caom_df[\'instrument\'])\n\n        self.jwst_datacols = [col for col in list(self.jwst_df.columns) if col != \'instrument\']\n        self.caom_datacols = [col for col in list(self.caom_df.columns) if col != \'instrument\']\n\n        jwst_data = {\'groups\': jwst_groups}\n        caom_data = {\'groups\': caom_groups}\n\n        for col in self.jwst_datacols:\n            jwst_data.update({col: list(self.jwst_df[col])})\n        for col in self.caom_datacols:\n            caom_data.update({col: list(self.caom_df[col])})\n\n        self.jwst_bar_colors = max(3, len(self.jwst_datacols))\n        self.caom_bar_colors = max(3, len(self.caom_datacols))\n\n        jwst_x = [(group, datacol) for group in jwst_groups for datacol in self.jwst_datacols]\n        jwst_counts = sum(zip(*[jwst_data[col] for col in self.jwst_datacols]), ())\n        caom_x = [(group, datacol) for group in caom_groups for datacol in self.caom_datacols]\n        caom_counts = sum(zip(*[caom_data[col] for col in self.caom_datacols]), ())\n\n        self.refs[\'jwst_source\'].data = {\'x\': jwst_x, \'counts\': jwst_counts}\n        self.refs[\'caom_source\'].data = {\'x\': caom_x, \'counts\': caom_counts}\n'"
jwql/website/apps/jwql/tests/test_context_processors.py,0,"b'#!/usr/bin/env python\n\n""""""Tests for the ``context_processors`` module in the ``jwql`` web\napplication.\n\nAuthors\n-------\n\n    - Matthew Bourque\n\nUse\n---\n\n    These tests can be run via the command line from the ``website``\n    subpackage as such:\n\n    ::\n\n        python manage.py test apps.jwql.tests\n""""""\n\nfrom django.test import TestCase\nfrom django.test.client import Client\n\nfrom jwql.website.apps.jwql import context_processors\nfrom jwql.utils.utils import get_base_url\n\n\nclass TestBaseContext(TestCase):\n    def test_base_context(self):\n        """"""Tests the ``base_context`` function.""""""\n\n        client = Client()\n        request = client.get(\'{}/about/\'.format(get_base_url()))\n        request.COOKIES = {}\n        context = context_processors.base_context(request)\n\n        assert isinstance(context, dict)\n\n        keys = [\'inst_list\', \'tools\', \'user\', \'version\']\n        for key in keys:\n            assert key in context\n'"
