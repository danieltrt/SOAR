file_path,api_count,code
setup.py,0,"b'#! /usr/bin/env python\n#\n# Copyright 2019 Square Inc.\n\n# Apache License\n# Version 2.0, January 2004\n# http://www.apache.org/licenses/\n\nimport os\nimport codecs\nimport re\nimport glob\nfrom setuptools import setup, Extension, find_packages\n\n# Checking if numpy is installed\ntry:\n  import numpy\nexcept:\n  import subprocess\n  print(""numpy is not installed. So pysurvival will install it now."")\n  subprocess.call(""pip install numpy"", shell=True)\n  import numpy\n\n# Package meta-data.\nNAME = \'pysurvival\'\nDESCRIPTION = \'Open source package for Survival Analysis modeling\'\nURL = \'https://www.pysurvival.io\'\nEMAIL = \'stephane@squareup.com\'\nAUTHOR = \'steph-likes-git\'\nLICENSE = ""Apache Software License (Apache 2.0)""\n\n# Current Directory\ntry:\n    CURRENT_DIR = os.path.dirname(os.path.realpath(__file__)) + \'/\'\nexcept:\n    CURRENT_DIR = os.path.dirname(os.path.realpath(\'__file__\')) + \'/\'\n\n# Utility functions\ndef read_long_description():\n    with open(CURRENT_DIR + \'LONG_DESCRIPTION.txt\', \'r\') as f:\n        return f.read()\n\ndef install_requires():\n\twith open(CURRENT_DIR + \'requirements.txt\', \'r\') as requirements_file:\n\t    requirements = requirements_file.readlines()\n\treturn requirements\n\ndef read_version(*file_paths):\n    with codecs.open(os.path.join(CURRENT_DIR, *file_paths), \'r\') as fp:\n        version_file = fp.read()\n\n    version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"",\n                              version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError(""Unable to find version string."")\n\n# Extensions Compilation arguments #\nextra_compile_args = [\'-std=c++11\', ""-O3""] \n\n# Extensions info #\next_modules = [ \n\n  Extension( \n    name = ""pysurvival.utils._functions"",\n    sources = [""pysurvival/cpp_extensions/_functions.cpp"",\n               ""pysurvival/cpp_extensions/functions.cpp"" ,\n               ],\n    extra_compile_args = extra_compile_args, \n    language=""c++"", \n  ),\n\n  Extension( \n    name = ""pysurvival.utils._metrics"",\n    sources = [""pysurvival/cpp_extensions/_metrics.cpp"",\n               ""pysurvival/cpp_extensions/non_parametric.cpp"",\n               ""pysurvival/cpp_extensions/metrics.cpp"",\n               ""pysurvival/cpp_extensions/functions.cpp"",\n              ],\n    extra_compile_args = extra_compile_args, \n    language=""c++"", \n    ),\n\n  Extension( \n    name = ""pysurvival.models._non_parametric"",\n    sources = [""pysurvival/cpp_extensions/_non_parametric.cpp"",\n               ""pysurvival/cpp_extensions/non_parametric.cpp"",\n               ""pysurvival/cpp_extensions/functions.cpp"" \n               ],\n    extra_compile_args = extra_compile_args, \n    language=""c++"", \n  ),\n\n  Extension( \n    name = ""pysurvival.models._survival_forest"",\n    sources = [ ""pysurvival/cpp_extensions/_survival_forest.cpp"",\n                ""pysurvival/cpp_extensions/survival_forest_data.cpp"",\n                ""pysurvival/cpp_extensions/survival_forest_utility.cpp"",\n                ""pysurvival/cpp_extensions/survival_forest_tree.cpp"",\n                ""pysurvival/cpp_extensions/survival_forest.cpp"", \n                ],\n    extra_compile_args = extra_compile_args, \n    language=""c++"", \n  ),\n\n  Extension( \n    name = ""pysurvival.models._coxph"",\n    sources = [ ""pysurvival/cpp_extensions/_coxph.cpp"",\n                ""pysurvival/cpp_extensions/functions.cpp"" \n              ],\n    extra_compile_args = extra_compile_args, \n    language=""c++"", \n    include_dirs=[numpy.get_include()],\n  ),\n\n  Extension( \n    name = ""pysurvival.models._svm"",\n    sources = [ ""pysurvival/cpp_extensions/_svm.cpp"", \n              ],\n    extra_compile_args = extra_compile_args, \n    language=""c++"", \n    include_dirs=[numpy.get_include()],\n  ),\n  ]\n\n# Setup \nsetup(name=NAME,\n      version=read_version(""pysurvival"", ""__init__.py""),\n      description=DESCRIPTION,\n      long_description=read_long_description(),\n      author=AUTHOR,\n      author_email=EMAIL,\n      url=URL,\n      license=LICENSE,\n      install_requires=install_requires(),\n      include_package_data=True,\n      package_data={ \'\': [\'*.csv\'], },\n      extras_require={ \'tests\': [\'pytest\', \'pytest-pep8\', ] },\n      classifiers=[\n          \'Intended Audience :: Developers\',\n          \'Intended Audience :: Education\',\n          \'Intended Audience :: Science/Research\',\n          \'Operating System :: MacOS\',\n          \'Operating System :: Unix\',\n          \'Programming Language :: Python :: 2\',\n          \'Programming Language :: Python :: 2.7\',\n          \'Programming Language :: Python :: 3\',\n          \'Programming Language :: Python :: 3.7\',\n          \'Topic :: Scientific/Engineering\',\n\t\t      \'Topic :: Scientific/Engineering :: Mathematics\',\n          \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n          \'Topic :: Software Development\',\n          \'Topic :: Software Development :: Libraries :: Python Modules\'\n      ],\n      packages=find_packages(),\n      ext_modules=ext_modules,\n  )'"
pysurvival/__init__.py,0,"b""from __future__ import absolute_import\nimport sys\nimport torch\n\n\n# -------------------------------- VERSION --------------------------------- #\n# PEP0440 compatible formatted version, see:\n# https://www.python.org/dev/peps/pep-0440/\n#\n# Generic release markers:\n#   X.Y\n#   X.Y.Z   # For bugfix releases\n#\n# Admissible pre-release markers:\n#   X.YaN   # Alpha release\n#   X.YbN   # Beta release\n#   X.YrcN  # Release Candidate\n#   X.Y     # Final release\n#\n# Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.\n# 'X.Y.dev0' is the canonical version of 'X.Y.dev'\n#\n__version__ = '0.1.2'\n\n\n# ---------------------------- GLOBAL VARIABLES ---------------------------- #\nPYTHON_VERSION = sys.version_info[0]\nHAS_GPU = torch.cuda.is_available()\n\n\n__all__ = [  'PYTHON_VERSION', 'HAS_GPU', ]\n\n"""
pysurvival/datasets/__init__.py,1,"b'from __future__ import absolute_import\nimport numpy as np\nimport pandas as pd\nimport os \nfrom sklearn.model_selection import train_test_split\n\ntry:\n\tCURRENT_FOLDER = os.path.dirname(os.path.realpath(__file__)) + \'/\'\nexcept:\n\tCURRENT_FOLDER = os.path.dirname(os.path.realpath(\'__file__\')) + \'/\'\n\n\nclass Dataset(object):\n\t"""""" Helper object built to read/load tutorial datasets with ease.""""""\n\n\tdef __init__(self, name):\n\n\t\tif \'maintenance\' in name.lower() :\n\t\t\tself.name = \'maintenance\'\n\t\t\tself.time_column = \'lifetime\'\n\t\t\tself.event_column = \'broken\'\n\t\t\tself.filename = CURRENT_FOLDER +\'maintenance.csv\'\n\n\t\telif \'simple\' in name.lower() or \'example\' in name.lower()  :\n\t\t\tself.name = \'simple_example\'\n\t\t\tself.time_column = \'time\'\n\t\t\tself.event_column = \'event\'\n\t\t\tself.filename = CURRENT_FOLDER +\'simple_example.csv\'\n\n\t\telif \'credit\' in name.lower() :\n\t\t\tself.name = \'credit_risk\'\n\t\t\tself.time_column = \'duration\'\n\t\t\tself.event_column = \'full_repaid\'\n\t\t\tself.filename = CURRENT_FOLDER +\'credit_risk.csv\'\n\n\t\telif \'employee\' in name.lower() :\n\t\t\tself.name = \'employee_attrition\'\n\t\t\tself.time_column = \'time_spend_company\'\n\t\t\tself.event_column = \'left\'\n\t\t\tself.filename = CURRENT_FOLDER +\'employee_attrition.csv\'\n\n\t\telif \'churn\' in name.lower() :\n\t\t\tself.name = \'churn\'\n\t\t\tself.time_column = \'months_active\'\n\t\t\tself.event_column = \'churn\'\n\t\t\tself.filename = CURRENT_FOLDER +\'churn.csv\'\n\n\n\tdef load(self):\n\t\t"""""" Loading the dataset """"""\n\n\t\tif \'maintenance\' in self.filename.lower():\n\t\t\tsep = "";""\n\t\telse:\n\t\t\tsep = "",""\n\n\t\tdata = pd.read_csv(self.filename, sep=sep)\n\t\treturn data\n\n\n\tdef load_train_test(self, test_size = 0.3):\n\t\t"""""" Loading the dataset and returning X, T, E split between \n\t\t\ttraining and testing \n\t\t""""""\n\n\t\t# Reading the dataset\n\t\tdata = self.load()\n\t\tN = data.shape[0]\n\n\t\t# Extracting the features\n\t\tcolumns_to_exclude = [self.time_column, self.event_column]\n\t\tself.features = np.setdiff1d(data.columns, columns_to_exclude).tolist()\n\t\tfeatures = self.features\n\t\ttime = self.time_column\n\t\tevent = self.event_column\n\n\t\t# Building training and testing sets #\n\t\tindex_train, index_test = train_test_split( range(N), \n\t\t\ttest_size = test_size)\n\t\tdata_train = data.loc[index_train].reset_index( drop = True )\n\t\tdata_test  = data.loc[index_test].reset_index( drop = True )\n\n\t\t# Creating the X, T and E input\n\t\tX_train, X_test = data_train[features], data_test[features]\n\t\tT_train, T_test = data_train[time].values, data_test[time].values\n\t\tE_train, E_test = data_train[event].values, data_test[event].values\n\n\t\treturn X_train, T_train, E_train, X_test, T_test, E_test\n\n\n\tdef __repr__(self):\n\t\treturn self.name\n'"
pysurvival/models/__init__.py,2,"b'from __future__ import absolute_import\nimport copy\nimport tempfile\nimport pyarrow as pa\nimport os\nimport torch\nimport zipfile\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom pysurvival import utils\nfrom pysurvival.utils._functions import _get_time_buckets\n\n\nclass BaseModel(object):\n    """""" Base class for all estimators in pysurvival. It should not be used on\n        its own.\n    """"""\n\n    def __init__(self, auto_scaler=True):\n        \n        # Creating a scikit-learner scaler\n        self.auto_scaler = auto_scaler\n        if self.auto_scaler:\n            self.scaler = StandardScaler()\n        else:\n            self.scaler = None\n        \n        # Creating a place holder for the time axis\n        self.times = [0.]\n\n        # Creating the model\'s name\n        self.__repr__()\n\n\n    def __repr__(self):\n        """""" Creates the representation of the Object """"""\n\n        self.name = self.__class__.__name__\n        return self.name\n\n\n    def save(self, path_file):\n        """""" Save the model components: \n                * the paremeters of the model (parameters) \n                * the PyTorch model itself (model) if it exists\n            And Compress them into a zip file\n\n            Parameters\n            ----------\n            * path_file, str\n                address of the file where the model will be saved\n        """""" \n\n        # Ensuring the file has the proper name\n        folder_name = os.path.dirname(path_file) + \'/\'\n        file_name = os.path.basename(path_file)\n        if not file_name.endswith(\'.zip\'):\n            file_name += \'.zip\'\n\n        # Checking if the folder is accessible\n        if not os.access(folder_name, os.W_OK):\n            error_msg = \'{} is not an accessible directory.\'.format(folder_name)\n            raise OSError(error_msg)\n\n        # Saving all the elements to save\n        elements_to_save = []\n\n        # Changing the format of scaler parameters if exist\n        temp_scaler = copy.deepcopy(self.__dict__.get(\'scaler\'))\n        if temp_scaler is not None:\n            self.__dict__[\'scaler\'] = temp_scaler.__dict__\n\n        # Saving the model parameters\n        parameters_to_save = {}\n        for k in self.__dict__ :\n            if k != \'model\' :\n                parameters_to_save[k] = self.__dict__[k]\n\n        # Serializing the parameters\n        elements_to_save.append(\'parameters\')\n        with open(\'parameters\' , \'wb\') as f:\n            serialized_to_save = pa.serialize(parameters_to_save)\n            f.write(serialized_to_save.to_buffer())\n            \n        # Saving the torch model if exists\n        if \'model\' in self.__dict__.keys():\n            elements_to_save.append(\'model\')\n            torch.save(self.model, \'model\') \n            \n        # Compressing the elements to save in zip\n        full_path = folder_name + file_name\n        print(\'Saving the model to disk as {}\'.format(full_path))\n        with zipfile.ZipFile(full_path, \'w\') as myzip:\n            for f in elements_to_save:   \n                myzip.write(f)\n                \n        # Erasing temp files\n        for temp_file in elements_to_save:\n            os.remove(temp_file)\n\n        # Restore the scaler\n        if temp_scaler is not None:\n            self.scaler = StandardScaler()\n            self.__dict__[\'scaler\'] = copy.deepcopy(temp_scaler)\n\n\n    def load(self, path_file):\n        """""" Load the model components from a .zip file: \n                * the parameters of the model (.params) \n                * the PyTorch model itself (.model) is exists\n\n            Parameters\n            ----------\n            * path_file, str\n                address of the file where the model will be loaded from \n        """"""        \n\n        # Ensuring the file has the proper name\n        folder_name = os.path.dirname(path_file) + \'/\'\n        file_name = os.path.basename(path_file)\n        if not file_name.endswith(\'.zip\'):\n            file_name += \'.zip\'\n\n        # Opening the \'.zip\' file \n        full_path = folder_name + file_name\n        print(\'Loading the model from {}\'.format(full_path))\n\n        # Creating temp folder\n        temp_folder = tempfile.mkdtemp() + \'/\'\n            \n        # Unzip files in temp folder\n        with zipfile.ZipFile(path_file, \'r\') as zip_ref:\n            zip_ref.extractall(temp_folder)\n            input_zip=zipfile.ZipFile(path_file)\n\n        # Loading the files\n        elements_to_load = []\n        for file_name in input_zip.namelist():\n\n            # Loading the parameters\n            if \'parameters\' in file_name.lower():\n                content = input_zip.read( \'parameters\' )\n                self.__dict__ = copy.deepcopy(pa.deserialize(content))\n                elements_to_load.append(temp_folder +\'parameters\')\n\n                # If a scaler was available then load it too\n                temp_scaler = copy.deepcopy(self.__dict__.get(\'scaler\'))\n                if temp_scaler is not None:\n                    self.scaler = StandardScaler()\n                    self.scaler.__dict__ = temp_scaler\n\n            # Loading the PyTorch model\n            if \'model\' in file_name.lower():\n                model = torch.load( temp_folder + \'model\' )\n                self.model = model\n                elements_to_load.append(temp_folder +\'model\')\n\n        # Erasing temp files\n        for temp_file in elements_to_load:\n            os.remove(temp_file)\n\n\n    def get_time_buckets(self, extra_timepoint=False):\n        """""" Creating the time buckets based on the times axis such that\n            for the k-th time bin is [ t(k-1), t(k) ] in the time axis.\n        """"""\n        \n        # Checking if the time axis has already been created\n        if self.times is None or len(self.times) <= 1:\n            error = \'The time axis needs to be created before\'\n            error += \' using the method get_time_buckets.\'\n            raise AttributeError(error)\n        \n        # Creating the base time buckets\n        time_buckets = _get_time_buckets(self.times)\n        \n        # Adding an additional element if specified\n        if extra_timepoint:\n            time_buckets += [ (time_buckets[-1][1], time_buckets[-1][1]*1.01) ]\n        self.time_buckets = time_buckets\n\n\n    def predict_hazard(self, x, t = None, **kwargs):\n        """""" Predicts the hazard function h(t, x)\n\n            Parameters\n            ----------\n            * `x` : **array-like** *shape=(n_samples, n_features)* --\n                array-like representing the datapoints. \n                x should not be standardized before, the model\n                will take care of it\n\n            * `t`: **double** *(default=None)* --\n                 time at which the prediction should be performed. \n                 If None, then return the function for all available t.\n\n            Returns\n            -------\n            * `hazard`: **numpy.ndarray** --\n                array-like representing the prediction of the hazard function\n        """"""\n\n        # Checking if the data has the right format\n        x = utils.check_data(x)\n\n        # Calculating hazard, density, survival\n        hazard, density, survival = self.predict( x, t, **kwargs)\n\n        return hazard\n\n\n    def predict_density(self, x, t = None, **kwargs):\n        """""" Predicts the density function d(t, x)\n\n            Parameters\n            ----------\n            * `x` : **array-like** *shape=(n_samples, n_features)* --\n                array-like representing the datapoints. \n                x should not be standardized before, the model\n                will take care of it\n\n            * `t`: **double** *(default=None)* --\n                 time at which the prediction should be performed. \n                 If None, then return the function for all available t.\n\n            Returns\n            -------\n            * `density`: **numpy.ndarray** --\n                array-like representing the prediction of density function\n        """"""        \n\n        # Checking if the data has the right format\n        x = utils.check_data(x)\n\n        # Calculating hazard, density, survival\n        hazard, density, survival = self.predict( x, t, **kwargs )\n        return density\n\n\n    def predict_survival(self, x, t = None, **kwargs):\n        """""" Predicts the survival function S(t, x)\n\n            Parameters\n            ----------\n            * `x` : **array-like** *shape=(n_samples, n_features)* --\n                array-like representing the datapoints. \n                x should not be standardized before, the model\n                will take care of it\n\n            * `t`: **double** *(default=None)* --\n                time at which the prediction should be performed. \n                If None, then return the function for all available t.\n\n            Returns\n            -------\n            * `survival`: **numpy.ndarray** --\n                array-like representing the prediction of the survival function\n        """"""        \n\n        # Checking if the data has the right format\n        x = utils.check_data(x)\n\n        # Calculating hazard, density, survival\n        hazard, density, survival = self.predict( x, t, **kwargs)\n        return survival\n\n\n    def predict_cdf(self, x, t = None, **kwargs):\n        """""" Predicts the cumulative density function F(t, x)\n\n            Parameters\n            ----------\n            * `x` : **array-like** *shape=(n_samples, n_features)* --\n                array-like representing the datapoints. \n                x should not be standardized before, the model\n                will take care of it\n\n            * `t`: **double** *(default=None)* --\n                time at which the prediction should be performed. \n                If None, then return the function for all available t.\n\n            Returns\n            -------\n            * `cdf`: **numpy.ndarray** --\n                array-like representing the prediction of the cumulative \n                density function \n        """"""        \n\n        # Checking if the data has the right format\n        x = utils.check_data(x)\n\n        # Calculating survival and cdf\n        survival = self.predict_survival(x, t, **kwargs)\n        cdf = 1. - survival\n        return cdf\n\n\n    def predict_cumulative_hazard(self, x, t = None, **kwargs):\n        """""" Predicts the cumulative hazard function H(t, x)\n\n            Parameters\n            ----------\n            * `x` : **array-like** *shape=(n_samples, n_features)* --\n                array-like representing the datapoints. \n                x should not be standardized before, the model\n                will take care of it\n\n            * `t`: **double** *(default=None)* --\n                time at which the prediction should be performed. \n                If None, then return the function for all available t.\n\n            Returns\n            -------\n            * `cumulative_hazard`: **numpy.ndarray** --\n                array-like representing the prediction of the cumulative_hazard\n                function\n        """"""        \n\n        # Checking if the data has the right format\n        x = utils.check_data(x)\n\n        # Calculating hazard/cumulative_hazard\n        hazard = self.predict_hazard(x, t, **kwargs)\n        cumulative_hazard = np.cumsum(hazard, 1)\n        return cumulative_hazard\n\n\n    def predict_risk(self, x, **kwargs):\n        """""" Predicts the Risk Score/Mortality function for all t,\n            R(x) = sum( cumsum(hazard(t, x)) )\n            According to Random survival forests from Ishwaran H et al\n            https://arxiv.org/pdf/0811.1645.pdf\n\n            Parameters\n            ----------\n            * `x` : **array-like** *shape=(n_samples, n_features)* --\n                array-like representing the datapoints. \n                x should not be standardized before, the model\n                will take care of it\n\n            Returns\n            -------\n            * `risk_score`: **numpy.ndarray** --\n                array-like representing the prediction of Risk Score function\n        """"""        \n\n        # Checking if the data has the right format\n        x = utils.check_data(x)\n\n        # Calculating cumulative_hazard/risk\n        cumulative_hazard = self.predict_cumulative_hazard(x, None, **kwargs)\n        risk_score = np.sum(cumulative_hazard, 1)\n        return risk_score\n'"
pysurvival/models/multi_task.py,12,"b'from __future__ import absolute_import\nimport torch \nimport numpy as np\nimport copy\nimport multiprocessing\nfrom pysurvival import HAS_GPU\nfrom pysurvival import utils\nfrom pysurvival.utils import neural_networks as nn\nfrom pysurvival.utils import optimization as opt\nfrom pysurvival.models import BaseModel\n# %matplotlib inline\n\nclass BaseMultiTaskModel(BaseModel):\n    """""" Base class for all  Multi-Task estimators:\n        * Multi-Task Logistic Regression model (MTLR)\n        * Neural Multi-Task Logistic Regression model (N-MTLR)\n    BaseMultiTaskModel shouldn\'t be used as is. \n    The underlying model is written in PyTorch.\n\n    The original Multi-Task model, a.k.a the Multi-Task Logistic Regression\n    model (MTLR), was first introduced by  Chun-Nam Yu et al. in \n    *Learning Patient-Specific Cancer Survival Distributions as a Sequence of \n    Dependent Regressors*\n\n    The Neural Multi-Task Logistic Regression model (N-MTLR) was developed \n    by S. Fotso in the paper *Deep Neural Networks for Survival \n    Analysis Based on a Multi-Task Framework*, allowing the use of \n    Neural Networks within the original design.\n\n    Parameters\n    ----------\n    * `structure`:  **list of dictionaries** -- \n        Provides the structure of the MLP built within the N-MTLR.\n        \n        ex: `structure = [ {\'activation\': \'ReLU\', \'num_units\': 128}, ]`.\n\n        Each dictionary corresponds to a fully connected hidden layer:\n\n        * `num_units` is the number of hidden units in this layer\n        * `activation` is the activation function that will be used. \n        The list of all available activation functions can be found :\n            * Atan\n            * BentIdentity\n            * BipolarSigmoid\n            * CosReLU\n            * ELU\n            * Gaussian\n            * Hardtanh\n            * Identity\n            * InverseSqrt\n            * LeakyReLU\n            * LeCunTanh\n            * LogLog\n            * LogSigmoid\n            * ReLU\n            * SELU\n            * Sigmoid\n            * Sinc\n            * SinReLU\n            * Softmax\n            * Softplus\n            * Softsign\n            * Swish\n            * Tanh\n    \n        In case there are more than one dictionary, \n        each hidden layer will be applied in the resulting MLP, \n        using the order it is provided in the structure:\n        ex: structure = [ {\'activation\': \'relu\', \'num_units\': 128}, \n                          {\'activation\': \'tanh\', \'num_units\': 128}, ] \n\n    * `bins`: **int** *(default=100)* -- \n         Number of subdivisions of the time axis \n\n    * `auto_scaler`: **boolean** *(default=True)* -- \n        Determines whether a sklearn scaler should be automatically applied\n    """"""\n\n    def __init__(self, structure, bins = 100, auto_scaler=True):\n\n        # Saving the attributes\n        self.loss_values = []\n        self.bins = bins\n        self.structure = structure\n\n        # Initializing the elements from BaseModel\n        super(BaseMultiTaskModel, self).__init__(auto_scaler)\n        \n        \n    def get_times(self, T, is_min_time_zero = True, extra_pct_time = 0.1):\n        """""" Building the time axis (self.times) as well as the time intervals \n            ( all the [ t(k-1), t(k) ) in the time axis.\n        """"""\n\n        # Setting the min_time and max_time\n        max_time = max(T)\n        if is_min_time_zero :\n            min_time = 0. \n        else:\n            min_time = min(T)\n        \n        # Setting optional extra percentage time\n        if 0. <= extra_pct_time <= 1.:\n            p = extra_pct_time\n        else:\n            raise Exception(""extra_pct_time has to be between [0, 1]."") \n\n        # Building time points and time buckets\n        self.times = np.linspace(min_time, max_time*(1. + p), self.bins)\n        self.get_time_buckets()\n        self.num_times = len(self.time_buckets)\n\n\n    def compute_XY(self, X, T, E, is_min_time_zero, extra_pct_time):\n        """""" Given the survival_times, events and time_points vectors, \n            it returns a ndarray of the encodings for all units \n            such that:\n                Y = [[0, 0, 1, 0, 0],  # unit experienced an event at t = 3\n                     [0, 1, 0, 0, 0],  # unit experienced an event at t = 2\n                     [0, 1, 1, 1, 1],] # unit was censored at t = 2\n        """""" \n\n        # building times axis\n        self.get_times(T, is_min_time_zero, extra_pct_time)\n        n_units  = T.shape[0]\n        \n        # Initializing the output variable\n        Y_cens, Y_uncens = [], []\n        X_cens, X_uncens = [], []\n\n        # Building the output variable\n        for i, (t, e) in enumerate( zip(T, E) ):\n            y = np.zeros(self.num_times+1)\n            min_abs_value = [abs(a_j_1-t) for (a_j_1, a_j) in self.time_buckets]\n            index = np.argmin(min_abs_value)\n\n            if e == 1:\n                y[index] = 1.\n                X_uncens.append( X[i, :].tolist() )\n                Y_uncens.append( y.tolist() )\n\n            else:                \n                y[(index):] = 1.\n                X_cens.append( X[i, :].tolist() )\n                Y_cens.append( y.tolist() )\n\n        # Transform into torch.Tensor\n        X_cens = torch.FloatTensor(X_cens)\n        X_uncens = torch.FloatTensor(X_uncens)\n        Y_cens = torch.FloatTensor(Y_cens)\n        Y_uncens = torch.FloatTensor(Y_uncens)\n\n        return X_cens, X_uncens, Y_cens, Y_uncens \n        \n\n    def loss_function(self, model, X_cens, X_uncens, Y_cens, Y_uncens, \n        Triangle, l2_reg, l2_smooth):\n        """""" Computes the loss function of the any MTLR model. \n            All the operations have been vectorized to ensure optimal speed\n        """"""\n\n        # Likelihood Calculations -- Uncensored\n        score_uncens = model(X_uncens)\n        phi_uncens = torch.exp( torch.mm(score_uncens, Triangle) )\n        reduc_phi_uncens = torch.sum(phi_uncens*Y_uncens, dim = 1)\n\n        # Likelihood Calculations -- Censored\n        score_cens = model(X_cens)\n        phi_cens = torch.exp( torch.mm(score_cens, Triangle) )\n        reduc_phi_cens = torch.sum( phi_cens*Y_cens, dim = 1)\n\n        # Likelihood Calculations -- Normalization\n        z_uncens = torch.exp( torch.mm(score_uncens, Triangle) )\n        reduc_z_uncens = torch.sum( z_uncens, dim = 1)\n\n        z_cens = torch.exp( torch.mm(score_cens, Triangle) )\n        reduc_z_cens = torch.sum( z_cens, dim = 1)\n\n        # MTLR cost function\n        loss = - (\n                    torch.sum( torch.log(reduc_phi_uncens) ) \\\n                  + torch.sum( torch.log(reduc_phi_cens) )  \\\n\n                  - torch.sum( torch.log(reduc_z_uncens) ) \\\n                  - torch.sum( torch.log(reduc_z_cens) ) \n                 )\n\n        # Adding the regularized loss\n        nb_set_parameters = len(list(model.parameters()))\n        for i, w in enumerate(model.parameters()):\n            loss += l2_reg*torch.sum(w*w)/2.\n            \n            if i >= nb_set_parameters - 2:\n                loss += l2_smooth*norm_diff(w)\n                \n        return loss\n\n\n    def fit(self, X, T, E, init_method = \'glorot_uniform\', optimizer =\'adam\', \n            lr = 1e-4, num_epochs = 1000, dropout = 0.2, l2_reg=1e-2, \n            l2_smooth=1e-2, batch_normalization=False, bn_and_dropout=False,\n            verbose=True, extra_pct_time = 0.1, is_min_time_zero=True):\n        """""" Fit the estimator based on the given parameters.\n\n        Parameters:\n        -----------\n        * `X` : **array-like**, *shape=(n_samples, n_features)* --\n            The input samples.\n\n        * `T` : **array-like** -- \n            The target values describing when the event of interest or censoring\n            occurred.\n\n        * `E` : **array-like** --\n            The values that indicate if the event of interest occurred i.e.: \n            E[i]=1 corresponds to an event, and E[i] = 0 means censoring, \n            for all i.\n\n        * `init_method` : **str** *(default = \'glorot_uniform\')* -- \n            Initialization method to use. Here are the possible options:\n\n            * `glorot_uniform`: Glorot/Xavier uniform initializer\n            * `he_uniform`: He uniform variance scaling initializer\n            * `uniform`: Initializing tensors with uniform (-1, 1) distribution\n            * `glorot_normal`: Glorot normal initializer,\n            * `he_normal`: He normal initializer.\n            * `normal`: Initializing tensors with standard normal distribution\n            * `ones`: Initializing tensors to 1\n            * `zeros`: Initializing tensors to 0\n            * `orthogonal`: Initializing tensors with a orthogonal matrix,\n\n        * `optimizer`:  **str** *(default = \'adam\')* -- \n            iterative method for optimizing a differentiable objective function.\n            Here are the possible options:\n\n            - `adadelta`\n            - `adagrad`\n            - `adam`\n            - `adamax`\n            - `rmsprop`\n            - `sparseadam`\n            - `sgd`\n\n        * `lr`: **float** *(default=1e-4)* -- \n            learning rate used in the optimization\n\n        * `num_epochs`: **int** *(default=1000)* -- \n            The number of iterations in the optimization\n\n        * `dropout`: **float** *(default=0.5)* -- \n            Randomly sets a fraction rate of input units to 0 \n            at each update during training time, which helps prevent overfitting.\n\n        * `l2_reg`: **float** *(default=1e-4)* -- \n            L2 regularization parameter for the model coefficients\n\n        * `l2_smooth`: **float** *(default=1e-4)* -- \n            Second L2 regularizer that ensures the parameters vary smoothly \n            across consecutive time points.\n\n        * `batch_normalization`: **bool** *(default=True)* -- \n            Applying Batch Normalization or not\n\n        * `bn_and_dropout`: **bool** *(default=False)* -- \n            Applying Batch Normalization and Dropout at the same time\n\n        * `display_loss`: **bool** *(default=True)* -- \n            Whether or not showing the loss function values at each update\n\n        * `verbose`: **bool** *(default=True)* -- \n            Whether or not producing detailed logging about the modeling\n\n        * `extra_pct_time`: **float** *(default=0.1)* -- \n            Providing an extra fraction of time in the time axis\n\n        * `is_min_time_zero`: **bool** *(default=True)* -- \n            Whether the the time axis starts at 0\n\n        **Returns:**\n\n        * self : object\n\n\n        Example:\n        --------\n            \n        #### 1 - Importing packages\n        import numpy as np\n        import pandas as pd\n        from matplotlib import pyplot as plt\n        from sklearn.model_selection import train_test_split\n        from pysurvival.models.simulations import SimulationModel\n        from pysurvival.models.multi_task import LinearMultiTaskModel\n        from pysurvival.utils.metrics import concordance_index\n        #%matplotlib inline  # To use with Jupyter notebooks\n\n\n        #### 2 - Generating the dataset from a Weibull parametric model\n        # Initializing the simulation model\n        sim = SimulationModel( survival_distribution = \'Weibull\',  \n                               risk_type = \'linear\',\n                               censored_parameter = 10.0, \n                               alpha = .01, beta = 3.0 )\n\n        # Generating N random samples \n        N = 1000\n        dataset = sim.generate_data(num_samples = N, num_features = 3)\n\n        # Showing a few data-points \n        time_column = \'time\'\n        event_column = \'event\'\n        dataset.head(2)\n\n        #### 3 - Creating the modeling dataset\n        # Defining the features\n        features = sim.features\n\n        # Building training and testing sets #\n        index_train, index_test = train_test_split( range(N), test_size = 0.2)\n        data_train = dataset.loc[index_train].reset_index( drop = True )\n        data_test  = dataset.loc[index_test].reset_index( drop = True )\n\n        # Creating the X, T and E input\n        X_train, X_test = data_train[features], data_test[features]\n        T_train, T_test = data_train[\'time\'].values, data_test[\'time\'].values\n        E_train, E_test = data_train[\'event\'].values, data_test[\'event\'].values\n\n        #### 4 - Initializing a MTLR model and fitting the data.\n        # Building a Linear model\n        mtlr = LinearMultiTaskModel(bins=50) \n        mtlr.fit(X_train, T_train, E_train, lr=5e-3, init_method=\'orthogonal\')\n\n        # Building a Neural MTLR\n        # structure = [ {\'activation\': \'Swish\', \'num_units\': 150},  ]\n        # mtlr = NeuralMultiTaskModel(structure=structure, bins=150) \n        # mtlr.fit(X_train, T_train, E_train, lr=5e-3, init_method=\'adam\')\n\n        #### 5 - Cross Validation / Model Performances\n        c_index = concordance_index(mtlr, X_test, T_test, E_test) #0.95\n        print(\'C-index: {:.2f}\'.format(c_index))\n\n        """"""\n\n        # Checking data format (i.e.: transforming into numpy array)\n        X, T, E = utils.check_data(X, T, E)\n\n        # Extracting data parameters\n        nb_units, self.num_vars = X.shape\n        input_shape = self.num_vars\n    \n        # Scaling data \n        if self.auto_scaler:\n            X = self.scaler.fit_transform( X ) \n\n        # Building the time axis, time buckets and output Y\n        X_cens, X_uncens, Y_cens, Y_uncens \\\n            = self.compute_XY(X, T, E, is_min_time_zero, extra_pct_time)\n\n        # Initializing the model\n        model = nn.NeuralNet(input_shape, self.num_times, self.structure, \n                             init_method, dropout, batch_normalization, \n                             bn_and_dropout )\n\n        # Creating the Triangular matrix\n        Triangle = np.tri(self.num_times, self.num_times + 1, dtype=np.float32) \n        Triangle = torch.FloatTensor(Triangle)\n\n        # Performing order 1 optimization\n        model, loss_values = opt.optimize(self.loss_function, model, optimizer, \n            lr, num_epochs, verbose,  X_cens=X_cens, X_uncens=X_uncens, \n            Y_cens=Y_cens, Y_uncens=Y_uncens, Triangle=Triangle, \n            l2_reg=l2_reg, l2_smooth=l2_smooth)\n\n        # Saving attributes\n        self.model = model.eval()\n        self.loss_values = loss_values\n\n        return self\n    \n\n    def predict(self, x, t = None):\n        """""" Predicting the hazard, density and survival functions\n        \n        Parameters:\n        ----------\n        * `x` : **array-like** *shape=(n_samples, n_features)* --\n            array-like representing the datapoints. \n            x should not be standardized before, the model\n            will take care of it\n\n        * `t`: **double** *(default=None)* --\n             time at which the prediction should be performed. \n             If None, then return the function for all available t.\n        """"""\n        \n        # Convert x into the right format\n        x = utils.check_data(x)\n\n        # Scaling the data\n        if self.auto_scaler:\n            if x.ndim == 1:\n                x = self.scaler.transform( x.reshape(1, -1) )\n            elif x.ndim == 2:\n                x = self.scaler.transform( x )\n        else:\n            # Ensuring x has 2 dimensions\n            if x.ndim == 1:\n                x = np.reshape(x, (1, -1))\n\n        # Transforming into pytorch objects\n        x = torch.FloatTensor(x)\n                \n        # Predicting using linear/nonlinear function\n        score_torch = self.model(x)\n        score = score_torch.data.numpy()\n                \n        # Cretaing the time triangles\n        Triangle1 = np.tri(self.num_times , self.num_times + 1 )\n        Triangle2 = np.tri(self.num_times+1 , self.num_times + 1 )\n\n        # Calculating the score, density, hazard and Survival\n        phi = np.exp( np.dot(score, Triangle1) )\n        div = np.repeat(np.sum(phi, 1).reshape(-1, 1), phi.shape[1], axis=1)\n        density = (phi/div)\n        Survival = np.dot(density, Triangle2)\n        hazard = density[:, :-1]/Survival[:, 1:]\n\n        # Returning the full functions of just one time point\n        if t is None:\n            return hazard, density, Survival\n        else:\n            min_abs_value = [abs(a_j_1-t) for (a_j_1, a_j) in self.time_buckets]\n            index = np.argmin(min_abs_value)\n            return hazard[:, index], density[:, index], Survival[:, index]\n\n\n    def predict_risk(self, x, use_log=False):\n        """""" Computing the risk score \n\n        Parameters:\n        -----------\n        * `x` : **array-like** *shape=(n_samples, n_features)* --\n            array-like representing the datapoints. \n            x should not be standardized before, the model\n            will take care of it\n\n        * `use_log`: **bool** *(default=True)* -- \n            Applies the log function to the risk values\n\n        """"""\n\n        risk = super(BaseMultiTaskModel, self).predict_risk(x)\n        if use_log:\n            return np.log(risk)\n        else:\n            return risk\n\n\n\nclass LinearMultiTaskModel(BaseMultiTaskModel):\n    """""" LinearMultiTaskModel is the original Multi-Task model, \n        a.k.a the Multi-Task Logistic Regression model (MTLR).\n        It was first introduced by  Chun-Nam Yu et al. in \n        Learning Patient-Specific Cancer Survival Distributions \n        as a Sequence of Dependent Regressors\n        \n        Reference:\n        ----------\n            * http://www.cs.cornell.edu/~cnyu/papers/nips11_survival.pdf\n        \n        Parameters:\n        ----------\n            * bins: int \n                Number of subdivisions of the time axis \n\n            * auto_scaler: boolean (default=True)\n                Determines whether a sklearn scaler should be automatically \n                applied\n\n    """"""\n    \n    def __init__(self, bins = 100, auto_scaler=True):\n        super(LinearMultiTaskModel, self).__init__(\n            structure = None, bins = bins, auto_scaler=auto_scaler)\n\n\n    def fit(self, X, T, E, init_method = \'glorot_uniform\', optimizer =\'adam\', \n            lr = 1e-4, num_epochs = 1000, l2_reg=1e-2, l2_smooth=1e-2, \n            verbose=True, extra_pct_time = 0.1, is_min_time_zero=True):\n\n        super(LinearMultiTaskModel, self).fit(X=X, T=T, E=E, \n            init_method = init_method, optimizer =optimizer, \n            lr = lr, num_epochs = num_epochs, dropout = None, l2_reg=l2_reg, \n            l2_smooth=l2_smooth, batch_normalization=False, \n            bn_and_dropout=False, verbose=verbose, \n            extra_pct_time = extra_pct_time, is_min_time_zero=is_min_time_zero)\n\n        return self\n    \n\nclass NeuralMultiTaskModel(BaseMultiTaskModel):\n    """""" NeuralMultiTaskModel is the Neural Multi-Task Logistic Regression \n        model (N-MTLR) was developed by Fotso S. in \n        Deep Neural Networks for Survival Analysis Based on a \n        Multi-Task Framework, \n        allowing the use of Neural Networks within the original design.\n        \n    Reference:\n    ----------\n    * https://arxiv.org/pdf/1801.05512\n\n    Parameters:\n    ----------\n    * `structure`:  **list of dictionaries** -- \n        Provides the structure of the MLP built within the N-MTLR.\n        \n        ex: `structure = [ {\'activation\': \'ReLU\', \'num_units\': 128}, ]`.\n\n        Each dictionary corresponds to a fully connected hidden layer:\n\n        * `units` is the number of hidden units in this layer\n        * `activation` is the activation function that will be used. \n        The list of all available activation functions can be found :\n            * Atan\n            * BentIdentity\n            * BipolarSigmoid\n            * CosReLU\n            * ELU\n            * Gaussian\n            * Hardtanh\n            * Identity\n            * InverseSqrt\n            * LeakyReLU\n            * LeCunTanh\n            * LogLog\n            * LogSigmoid\n            * ReLU\n            * SELU\n            * Sigmoid\n            * Sinc\n            * SinReLU\n            * Softmax\n            * Softplus\n            * Softsign\n            * Swish\n            * Tanh\n    \n        In case there are more than one dictionary, \n        each hidden layer will be applied in the resulting MLP, \n        using the order it is provided in the structure:\n        ex: structure = [ {\'activation\': \'relu\', \'num_units\': 128}, \n                          {\'activation\': \'tanh\', \'num_units\': 128}, ] \n\n    * `bins`: **int** *(default=100)* -- \n         Number of subdivisions of the time axis \n\n    * `auto_scaler`: **boolean** *(default=True)* -- \n        Determines whether a sklearn scaler should be automatically applied\n    """"""\n    \n    def __init__(self, structure, bins = 100, auto_scaler = True):\n\n        # Checking the validity of structure\n        structure = nn.check_mlp_structure(structure)\n\n        # Initializing the instance\n        super(NeuralMultiTaskModel, self).__init__(\n            structure = structure, bins = bins, auto_scaler = auto_scaler)\n    \n    \n    def __repr__(self):\n        """""" Representing the class object """"""\n\n        if self.structure is None:\n            super(NeuralMultiTaskModel, self).__repr__()\n            return self.name\n            \n        else:\n            S = len(self.structure)\n            self.name = self.__class__.__name__\n            empty = len(self.name)\n            self.name += \'( \'\n            for i, s in enumerate(self.structure):\n                n = \'Layer({}): \'.format(i+1)\n                activation = nn.activation_function(s[\'activation\'], \n                    return_text=True)\n                n += \'activation = {}, \'.format( s[\'activation\'] )\n                n += \'units = {} \'.format( s[\'num_units\'] )\n                \n                if i != S-1:\n                    self.name += n + \'; \\n\'\n                    self.name += empty*\' \' + \'  \'\n                else:\n                    self.name += n\n            self.name = self.name + \')\'\n            return self.name\n\n\ndef norm_diff(W):\n    """""" Special norm function for the last layer of the MTLR """"""\n    dims=len(W.shape)\n    if dims==1:\n        diff = W[1:]-W[:-1]\n    elif dims==2:\n        diff = W[1:, :]-W[:-1, :]\n    return torch.sum(diff*diff)\n'"
pysurvival/models/non_parametric.py,14,"b'from __future__ import absolute_import\nimport os\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom pysurvival import utils\nfrom pysurvival.models import BaseModel\nfrom pysurvival.models._non_parametric import _KaplanMeierModel\nfrom pysurvival.models._non_parametric import _KernelModel\n\n\nclass NonParametricModel(BaseModel):\n    """""" Non Parametric Model\n        --------------------\n\n        The Non Parametric Model object is tha base model for any Non Parametric \n        models in pysurvival such as :\n        * Kaplan Meier model\n        * Smooth Kaplan Meier model\n\n        This object should not be used on its own.\n    """"""\n\n    def save(self, path_file):\n        """""" Save the model paremeters of the model (.params) and Compress \n            them into a zip file\n        """"""\n\n        # Ensuring the file has the proper name\n        folder_name = os.path.dirname(path_file) + \'/\'\n        file_name = os.path.basename(path_file)\n\n        # Checking if the folder is accessible\n        if not os.access(folder_name, os.W_OK):\n            error_msg = \'{} is not an accessible directory.\'.format(folder_name)\n            raise OSError(error_msg)\n\n        # Delete the CPP object before saving\n        del self.model\n        \n        # Saving the model\n        super(NonParametricModel, self).save(path_file)\n\n        # Re-introduce the C++ object\n        if \'smooth\' in self.name.lower() :\n            self.model = _KernelModel(self.bandwidth, self.kernel_type)\n        else:\n            self.model = _KaplanMeierModel()\n        self.load_properties()\n        \n\n    def load(self, path_file):\n        """""" Load the model paremeters from a zip file into a C++ external\n            model \n        """"""        \n\n        # Loading the model\n        super(NonParametricModel, self).load(path_file)\n\n        # Re-introduce the C++ object\n        if \'smooth\' in self.name.lower() :\n            self.model = _KernelModel(self.bandwidth, self.kernel_type)\n        else:\n            self.model = _KaplanMeierModel()\n        self.load_properties()\n\n\n\n\n    def fit(self, T,  E, weights = None, alpha=0.95):\n        """""" Fitting the model according to the provided data.\n\n        Parameters:\n        -----------\n        * `T` : **array-like** -- \n            The target values describing when the event of interest or censoring\n            occurred.\n\n        * `E` : **array-like** --\n            The values that indicate if the event of interest occurred i.e.: E[i]=1\n            corresponds to an event, and E[i] = 0 symbols censoring, for all i.\n\n        * `weights` : **array-like** *(default = None)* -- \n            Array of weights that are assigned to individual samples.\n            If not provided, then each sample is given a unit weight.\n\n        * `alpha`: **float** *(default = 0.05)* --\n            Significance level\n\n        Returns:\n        --------\n        * self : object\n\n\n        Example:\n        --------\n\n        # Importing modules\n        import numpy as np\n        from matplotlib import pyplot as plt\n        from pysurvival.utils.display import display_non_parametric\n        # %matplotlib inline #Uncomment when using Jupyter \n\n        # Generating random times and event indicators \n        T = np.round(np.abs(np.random.normal(10, 10, 1000)), 1)\n        E = np.random.binomial(1, 0.3, 1000)\n\n        # Initializing the KaplanMeierModel\n        from pysurvival.models.non_parametric import KaplanMeierModel\n        km_model = KaplanMeierModel()\n\n        # Fitting the model \n        km_model.fit(T, E, alpha=0.95)\n\n        # Displaying the survival function and confidence intervals\n        display_non_parametric(km_model)\n\n        # Initializing the SmoothKaplanMeierModel\n        from pysurvival.models.non_parametric import SmoothKaplanMeierModel\n        skm_model = SmoothKaplanMeierModel(bandwith=0.1, kernel=\'normal\')\n\n        # Fitting the model\n        skm_model.fit(T, E)\n\n        # Displaying the survival function and confidence intervals\n        display_non_parametric(skm_model)\n        """"""\n\n        # Checking the format of the data \n        T, E = utils.check_data(T, E)\n\n        # weighting\n        if weights is None:\n            weights = [1.]*T.shape[0]\n\n        # Confidence Intervals\n        z = stats.norm.ppf((1. - alpha) / 2.)\n\n        # Building the Kaplan-Meier model\n        survival = self.model.fit(T, E, weights, z)\n        if sum(survival) <= 0. :\n            mem_error = ""The kernel matrix cannot fit in memory.""\n            mem_error += ""You should use a bigger bandwidth b""\n            raise MemoryError(mem_error)\n\n        # Saving all properties\n        self.save_properties()\n\n        # Generating the Survival table\n        if \'smooth\' not in self.name.lower() :\n            self.get_survival_table()\n\n\n    def predict_cumulative_hazard(self, *args, **kargs):\n        raise NotImplementedError(self.not_implemented_error)\n\n    def predict_risk(self, *args, **kargs):\n        raise NotImplementedError(self.not_implemented_error)\n\n\n    def predict_survival(self, t, is_lagged = False):\n        """""" Extracting the predicted survival probabilities at the individual \n            event times that have been used for fitting the model.\n        """"""\n        return self.model.predict_survival(t, is_lagged)\n        \n\n    def predict_survival_upper(self, t, is_lagged = False):\n        """""" Extracting the predicted survival CI upper probabilities at the \n            individual event times that have been used for fitting the model.\n        """"""\n        return self.model.predict_survival_upper(t, is_lagged)\n        \n\n    def predict_survival_lower(self, t, is_lagged = False):\n        """""" Extracting the predicted survival CI lower probabilities at the \n            individual event times that have been used for fitting the model.\n        """"""\n        return self.model.predict_survival_lower(t, is_lagged)\n        \n\n    def predict_density(self, t, is_lagged = False):\n        """""" Extracting the predicted density probabilities at the individual \n            event times that have been used for fitting the model.\n        """"""\n        return self.model.predict_density( t, is_lagged)\n\n\n    def predict_hazard(self, t, is_lagged = False):\n        """""" Extracting the hazard function values at the individual \n            event times that have been used for fitting the model.\n        """"""\n        return self.model.predict_hazard( t, is_lagged)\n\n\n    def save_properties(self): \n        """""" Saving the C++ attributes in the Python object """"""\n        self.times  = np.array( self.model.times )\n        self.time_buckets  = self.model.time_buckets\n        self.survival  = np.array( self.model.survival )\n        self.hazard = np.array( self.model.hazard )\n        self.cumulative_hazard = np.array( self.model.cumulative_hazard )\n\n        if \'smooth\' in self.name.lower() :\n            self.km_survival = np.array( self.model.km_survival )\n            self.km_times = np.array( self.model.km_times )\n            self.kernel_type = self.model.kernel_type\n            self.bandwidth = self.model.b\n            self.kernel_matrix = np.array( self.model.kernel_matrix )\n\n        else:\n            self.std_error = np.array( self.model.std_error )\n            self.survival_ci_upper = np.array( self.model.survival_ci_upper )\n            self.survival_ci_lower = np.array( self.model.survival_ci_lower )\n            self.at_risk = np.array( self.model.at_risk )\n            self.events = np.array( self.model.events )\n\n\n    def load_properties(self): \n        """""" Loading Python object attributes into the C++  """"""\n        self.model.times  = self.times\n        self.model.time_buckets  = self.time_buckets\n        self.model.survival  = self.survival\n        self.model.hazard = self.hazard\n        self.model.cumulative_hazard = self.cumulative_hazard\n\n        if \'smooth\' in self.name.lower() :\n            self.model.kernel_type = self.kernel_type\n            self.model.b = self.bandwidth\n            self.model.kernel_matrix = self.kernel_matrix\n            self.model.km_survival = self.km_survival\n            self.model.km_times = self.km_times\n\n        else:\n            self.model.std_error = self.std_error\n            self.model.survival_ci_upper = self.survival_ci_upper\n            self.model.survival_ci_lower = self.survival_ci_lower\n            self.model.at_risk = self.at_risk\n            self.model.events = self.events\n            \n\n    def get_survival_table(self):\n        """""" Computing the survival table""""""\n\n        data = {\'Time\': self.times,\n                \'Number at risk\':self.at_risk,\n                \'Number of events\':self.events,\n                \'Survival\': self.survival,\n                \'Survival - CI Lower\': self.survival_ci_lower,\n                \'Survival - CI Upper\': self.survival_ci_upper, \n                }\n        survival_df = pd.DataFrame(data,\n                                   columns = [\'Time\', \'Number at risk\', \n                                              \'Number of events\', \'Survival\',\n                                              \'Survival - CI Lower\',\n                                              \'Survival - CI Upper\'] )\n        self.survival_table = survival_df\n        return survival_df\n\n\n\n\n\n\nclass KaplanMeierModel(NonParametricModel):\n\n    """""" Kaplan-Meier Model\n        ------------------\n        \n    The Kaplan-Meier estimator is a non-parametric statistic \n    used to estimate the survival function from lifetime data. \n    The estimator is named after Edward L. Kaplan and Paul Meier, \n    who each submitted similar manuscripts \n    to the Journal of the American Statistical Association.\n\n    The estimator is given by:\n        S(t) = (1.-d_1/r_1)*...*(1.-d_i/r_i)*...*(1.-d_n/r_n)\n            t_i, a time when at least one event happened, \n            d_i, the number of events that happened at time t_i \n            r_i, the individuals known to survive at t_i.\n\n    References:\n    -----------\n        https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator\n    """"""\n\n    def __init__(self):\n        # Initializing the C++ object\n        self.model = _KaplanMeierModel()\n\n        # Saving the attributes\n        self.__repr__()\n        self.not_implemented_error = ""{} does not have this method.""\\\n            .format(self.name)\n\n\nclass SmoothKaplanMeierModel(NonParametricModel):\n    \n    """""" SmoothKaplanMeierModel\n        ------------------------\n        Because the standard Kaplan-Meier estimator is a step function with jumps \n        located only at the uncensored observations, when many data are censored, \n        it can only have a few jumps with increasing sizes. Thus the accuracy of \n        the estimation might not be acceptable. \n        A Smooth estimator is a good alternative, since it is computed by \n        giving mass to all the data, including the censored observations. (1)\n        \n        The current implementation is based on Nonparametric density estimation \n        from censored data from W.J. Padgett and Diane T. McNichols (2)\n        It was also inspired by CDF and survival function estimation with \n        infinite-order kernels from Berg and Politis(2)\n        \n        References:\n        -----------\n        * survPresmooth: An R Package for PreSmooth Estimation in Survival Analysis\n          https://www.jstatsoft.org/article/view/v054i11 (1)\n        * https://doi.org/10.1080/03610928408828780 (2)\n        * https://projecteuclid.org/download/pdfview_1/euclid.ejs/1261671304 (3)\n\n        Parameters:\n        -----------\n        * bandwidth: double (default=0.1)\n             controls the degree of the smoothing. The smaller it is the closer\n             to the original KM the function will be, but it will increase the \n             computation time. If it is very large, the resulting model will be \n             smoother than the estimator of KM, but it will stop being as accurate.\n             \n        * kernel: str (default=\'normal\')\n            defines the type of kernel the model will be using. \n            Here are the possible options:\n                * uniform: f(x) = 0 if |x|<1 else f(x) = 0.5\n                * epanechnikov: f(x) = 0 if |x|<=1 else f(x) = 0.75*(1. - x^2 )\n                * normal: f(x) = exp( -x*x/2.) / sqrt( 2*pi )\n                * biweight: f(x) = 0 if |x|<=1 else f(x)=(15./16)*(1.-x^2)^2\n                * triweight: f(x) = 0 if |x|<=1 else f(x)=(35./32)*(1.-x^2)^3\n                * Cosine:  f(x) = 0 if |x|<=1 else  f(x)=(pi/4.)*cos( pi*x/2. )  \n    """"""\n\n    def __init__(self,  bandwidth = 0.1, kernel = \'normal\' ):\n\n        # Kernel function\n        if kernel.lower() == \'uniform\' : \n            kernel_type = 0 # Uniform kernel \n            kernel = \'Uniform\'\n            \n        elif kernel.lower().startswith(""epa""):\n            kernel_type = 1 # Epanechnikov kernel \n            kernel = \'Epanechnikov\'\n            \n        elif kernel.lower() == ""normal"" : \n            kernel_type = 2 # Normal kernel \n            kernel = \'Normal\'\n            \n        elif kernel.lower().startswith(""bi""): \n            kernel_type = 3 # Biweight kernel \n            kernel = \'Biweight\'\n            \n        elif kernel.lower().startswith(""tri""): \n            kernel_type = 4 # Triweight kernel \n            kernel = \'Triweight\'\n            \n        elif kernel.lower().startswith(""cos""): \n            kernel_type = 5 # Cosine kernel \n            kernel = \'Cosine\'\n\n        else:\n            raise NotImplementedError(\'{} is not a valid kernel function.\'\n                .format(kernel))\n\n        # bandwidth\n        if bandwidth <= 0.:\n            raise ValueError(\'bandwidth has to be positive.\')\n\n        # Initializing the C++ object\n        self.model = _KernelModel(bandwidth, kernel_type)\n\n        # Saving the attributes\n        self.bandwidth = bandwidth\n        self.kernel = kernel\n        self.kernel_type = kernel_type\n\n        # Creating the representation of the object\n        self.__repr__()\n        self.not_implemented_error = ""{} does not have this method.""\\\n            .format(self.name)\n\n\n    def __repr__(self):\n        """""" Creates the representation of the Object """"""\n        self.name = ""{}(bandwith={:.2f}, kernel=\'{}\')""\n        self.name = self.name.format(self.__class__.__name__, \n            self.bandwidth, self.kernel) \n        return self.name\n        \n\n'"
pysurvival/models/parametric.py,9,"b'from __future__ import absolute_import\nimport numpy as np\nimport scipy \nimport torch\nfrom pysurvival.models import BaseModel\nfrom pysurvival import utils, HAS_GPU\nfrom pysurvival.utils import optimization as opt\nfrom pysurvival.utils import neural_networks as nn\n\n\nclass BaseParametricModel(BaseModel):\n    """""" Base class for all the Parametric estimators:\n        ---------------------------------------------\n\n        Parametric models are special cases of the Cox proportional hazard\n        model where is is assumed that the baseline hazard has a specific \n        parametric form.\n        \n        The BaseParametricModel object provides the necessary framework to use\n        the properties of parametric models. It should not be used on its own.\n\n        Parameters\n        ----------\n        * bins: int (default=100)\n             Number of subdivisions of the time axis \n\n        * auto_scaler: boolean (default=True)\n            Determines whether a sklearn scaler should be automatically applied\n    """"""\n\n    def __init__(self, bins = 100, auto_scaler=True):\n            \n        # Saving the attributes\n        self.loss_values = []\n        self.bins = bins\n\n        # Initializing the elements from BaseModel\n        super(BaseParametricModel, self).__init__(auto_scaler)\n        \n    \n    def get_hazard_survival(self, model, x, t):\n        pass\n    \n\n    def get_times(self, T, is_min_time_zero = True, extra_pct_time = 0.1):\n        """""" Building the time axis (self.times) as well as the time intervals \n            ( all the [ t(k-1), t(k) ) in the time axis.\n        """"""\n\n        # Setting the min_time and max_time\n        max_time = max(T)\n        if is_min_time_zero :\n            min_time = 0. \n        else:\n            min_time = min(T)\n        \n        # Setting optional extra percentage time\n        if 0. <= extra_pct_time <= 1.:\n            p = extra_pct_time\n        else:\n            raise Exception(""extra_pct_time has to be between [0, 1]."") \n\n        # Building time points and time buckets\n        self.times = np.linspace(min_time, max_time*(1. + p), self.bins)\n        self.get_time_buckets()\n        self.nb_times = len(self.time_buckets)\n\n\n\n    def loss_function(self, model, X, T, E, l2_reg):\n        """""" Computes the loss function of any Parametric models. \n            All the operations have been vectorized to ensure optimal speed\n        """"""\n        \n        # Adapting the optimization for the use of GPU\n        # if HAS_GPU:\n        #     X = X.cuda(async=True)\n        #     T = T.cuda(async=True)\n        #     E = E.cuda(async=True)\n        #     model = model.cuda()   \n\n        # Hazard & Survival calculations\n        hazard, Survival = self.get_hazard_survival(model, X, T)\n\n        # Loss function calculations\n        hazard = torch.max(hazard, torch.FloatTensor([1e-6]))\n        Survival = torch.max(Survival, torch.FloatTensor([1e-6]))\n        loss = - torch.sum( E*torch.log(hazard) + torch.log(Survival) ) \n\n        # Adding the regularized loss\n        for w in model.parameters():\n            loss += l2_reg*torch.sum(w*w)/2.\n\n        return loss\n\n    \n    def fit(self, X, T, E, init_method = \'glorot_uniform\', optimizer =\'adam\', \n            lr = 1e-4, num_epochs = 1000, l2_reg=1e-2, verbose=True, \n            is_min_time_zero = True, extra_pct_time = 0.1):\n        """""" \n        Fit the estimator based on the given parameters.\n\n        Parameters:\n        -----------\n        * `X` : **array-like**, *shape=(n_samples, n_features)* --\n            The input samples.\n\n        * `T` : **array-like** -- \n            The target values describing when the event of interest or censoring\n            occurred.\n\n        * `E` : **array-like** --\n            The values that indicate if the event of interest occurred i.e.: \n            E[i]=1 corresponds to an event, and E[i] = 0 means censoring, \n            for all i.\n\n        * `init_method` : **str** *(default = \'glorot_uniform\')* -- \n            Initialization method to use. Here are the possible options:\n\n            * `glorot_uniform`:  Glorot/Xavier uniform initializer \n            * `he_uniform`:  He uniform variance scaling initializer \n            * `uniform`: Initializing tensors with uniform (-1, 1) distribution\n            * `glorot_normal`: Glorot normal initializer,\n            * `he_normal`: He normal initializer.\n            * `normal`: Initializing tensors with standard normal distribution\n            * `ones`: Initializing tensors to 1\n            * `zeros`: Initializing tensors to 0\n            * `orthogonal`: Initializing tensors with a orthogonal matrix,\n\n        * `optimizer`:  **str** *(default = \'adam\')* -- \n            iterative method for optimizing a differentiable objective function.\n            Here are the possible options:\n\n            - `adadelta`\n            - `adagrad`\n            - `adam`\n            - `adamax`\n            - `rmsprop`\n            - `sparseadam`\n            - `sgd`\n\n        * `lr`: **float** *(default=1e-4)* -- \n            learning rate used in the optimization\n\n        * `num_epochs`: **int** *(default=1000)* -- \n            The number of iterations in the optimization\n\n        * `l2_reg`: **float** *(default=1e-4)* -- \n            L2 regularization parameter for the model coefficients\n\n        * `verbose`: **bool** *(default=True)* -- \n            Whether or not producing detailed logging about the modeling\n\n        * `extra_pct_time`: **float** *(default=0.1)* -- \n            Providing an extra fraction of time in the time axis\n\n        * `is_min_time_zero`: **bool** *(default=True)* -- \n            Whether the the time axis starts at 0\n\n        Returns:\n        --------\n        * self : object\n\n\n        Example:\n        --------\n\n        #### 1 - Importing packages\n        import numpy as np\n        import pandas as pd\n        from matplotlib import pyplot as plt\n        from sklearn.model_selection import train_test_split\n        from pysurvival.models.simulations import SimulationModel\n        from pysurvival.models.parametric import GompertzModel\n        from pysurvival.utils.metrics import concordance_index\n        from pysurvival.utils.display import integrated_brier_score\n        #%matplotlib inline  # To use with Jupyter notebooks\n\n        #### 2 - Generating the dataset from a Gompertz parametric model\n        # Initializing the simulation model\n        sim = SimulationModel( survival_distribution = \'Gompertz\',  \n                               risk_type = \'linear\',\n                               censored_parameter = 10.0, \n                               alpha = .01, beta = 3.0 )\n\n        # Generating N random samples \n        N = 1000\n        dataset = sim.generate_data(num_samples = N, num_features = 3)\n\n        # Showing a few data-points \n        time_column = \'time\'\n        event_column = \'event\'\n        dataset.head(2)\n\n        #### 3 - Creating the modeling dataset\n        # Defining the features\n        features = sim.features\n\n        # Building training and testing sets #\n        index_train, index_test = train_test_split( range(N), test_size = 0.2)\n        data_train = dataset.loc[index_train].reset_index( drop = True )\n        data_test  = dataset.loc[index_test].reset_index( drop = True )\n\n        # Creating the X, T and E input\n        X_train, X_test = data_train[features], data_test[features]\n        T_train, T_test = data_train[\'time\'].values, data_test[\'time\'].values\n        E_train, E_test = data_train[\'event\'].values, data_test[\'event\'].values\n\n        #### 4 - Creating an instance of the Gompertz model and fitting the data\n        # Building the model\n        gomp_model = GompertzModel() \n        gomp_model.fit(X_train, T_train, E_train, lr=1e-2, init_method=\'zeros\',\n            optimizer =\'adam\', l2_reg = 1e-3, num_epochs=2000)\n\n        #### 5 - Cross Validation / Model Performances\n        c_index = concordance_index(gomp_model, X_test, T_test, E_test) #0.8\n        print(\'C-index: {:.2f}\'.format(c_index))\n\n        ibs = integrated_brier_score(gomp_model, X_test, T_test, E_test, \n            t_max=30, figure_size=(20, 6.5) )\n\n        """"""\n        \n        # Checking data format (i.e.: transforming into numpy array)\n        X, T, E = utils.check_data(X, T, E)\n        T = np.maximum(T, 1e-6)\n        self.get_times(T, is_min_time_zero, extra_pct_time)\n\n        # Extracting data parameters\n        nb_units, self.num_vars = X.shape\n        input_shape = self.num_vars\n    \n        # Scaling data \n        if self.auto_scaler:\n            X = self.scaler.fit_transform( X ) \n\n        # Does the model need a parameter called Beta\n        is_beta_used = True\n        init_alpha = 1.\n        if self.name == \'ExponentialModel\':\n            is_beta_used = False\n        if self.name == \'GompertzModel\':\n            init_alpha = 1000.\n\n        # Initializing the model\n        model = nn.ParametricNet(input_shape, init_method, init_alpha, \n            is_beta_used)\n\n        # Trasnforming the inputs into tensors\n        X = torch.FloatTensor(X)\n        T = torch.FloatTensor(T.reshape(-1, 1))\n        E = torch.FloatTensor(E.reshape(-1, 1))\n\n        # Performing order 1 optimization\n        model, loss_values = opt.optimize(self.loss_function, model, optimizer, \n            lr, num_epochs, verbose,  X=X, T=T, E=E, l2_reg=l2_reg)\n        \n        # Saving attributes\n        self.model = model.eval()\n        self.loss_values = loss_values\n\n        # Calculating the AIC\n        self.aic = 2*self.loss_values[-1] \n        self.aic -= 2*(self.num_vars + 1 + is_beta_used*1. - 1)\n\n        return self\n    \n    \n    def predict(self, x, t = None):\n        """""" \n        Predicting the hazard, density and survival functions\n        \n        Arguments:\n        ----------\n            * x: pd.Dataframe or np.ndarray or list\n                x is the testing dataset containing the features\n                x should not be standardized before, the model\n                will take care of it\n            * t: float (default=None)\n                Time at which hazard, density and survival functions\n                should be calculated. If None, the method returns \n                the functions for all times t. \n        """"""\n        \n        # Scaling the data\n        if self.auto_scaler:\n            if x.ndim == 1:\n                x = self.scaler.transform( x.reshape(1, -1) )\n            elif x.ndim == 2:\n                x = self.scaler.transform( x )\n        else:\n            # Ensuring x has 2 dimensions\n            if x.ndim == 1:\n                x = np.reshape(x, (1, -1))\n\n        # Transforming into pytorch objects\n        x = torch.FloatTensor(x)\n        times = torch.FloatTensor(self.times.flatten())\n            \n        # Calculating hazard, density, Survival\n        hazard, Survival = self.get_hazard_survival(self.model, x, times)\n        density = hazard*Survival\n\n        # Transforming into numpy objects\n        hazard = hazard.data.numpy()\n        density = density.data.numpy()\n        Survival = Survival.data.numpy()\n            \n        # Returning the full functions of just one time point\n        if t is None:\n            return hazard, density, Survival\n        else:\n            min_abs_value = [abs(a_j_1-t) for (a_j_1, a_j) in self.time_buckets]\n            index = np.argmin(min_abs_value)\n            return hazard[:, index], density[:, index], Survival[:, index]\n\n\n\nclass ExponentialModel(BaseParametricModel):\n    """""" \n    ExponentialModel:\n    -----------------\n        \n    The exponential distribution is the simplest and most\n    important distribution in survival studies. Being independent\n    of prior information, it is known as a ""lack of\n    memory"" distribution requiring that the present age of the\n    living organism does not influence its future survival.\n    (Application of Parametric Models to a Survival Analysis of\n    Hemodialysis Patients)\n    """"""\n    \n    def get_hazard_survival(self, model, x, t):\n        """""" Computing the hazard and Survival functions. """"""\n        \n        # Computing the score\n        score  = model(x).reshape(-1, 1)\n\n        # Computing hazard and Survival\n        hazard   = score\n        Survival = torch.exp(-hazard*t)   \n            \n        return hazard, Survival\n\n\n\nclass WeibullModel(BaseParametricModel):\n    """"""\n    WeibullModel:\n    -------------\n    \n    The Weibull distribution is a generalized form of the exponential\n    distribution and is de facto more flexible than the exponential model. \n    It is a two-parameter model (alpha and beta):\n        * alpha is the location parameter\n        * beta is the shape parameter \n    """"""\n\n    def get_hazard_survival(self, model, x, t):\n        """""" Computing the hazard and Survival functions. """"""\n        \n        # Computing the score\n        score  = model(x).reshape(-1, 1)\n\n        # Extracting beta\n        beta = list(model.parameters())[-1]\n\n        # Computing hazard and Survival\n        hazard   = beta*score*torch.pow(t, beta-1) \n        Survival = torch.exp(- score*torch.pow(t, beta) )  \n        \n        return hazard, Survival\n\n\n\nclass GompertzModel(BaseParametricModel):\n    """"""\n    GompertzModel:\n    --------------\n\n    The Gompertz distribution is a continuous probability distribution,  \n    that has an exponentially increasing failure rate, and is often \n    applied to analyze survival data.\n    \n    """"""\n\n    def get_hazard_survival(self, model, x, t):\n        """""" Computing the hazard and Survival functions. """"""\n        \n        # Computing the score\n        score  = model(x).reshape(-1, 1)\n\n        # Extracting beta\n        beta = list(model.parameters())[-1]\n\n        # Computing hazard and Survival\n        hazard   = score*torch.exp(beta*t)\n        Survival = torch.exp(-score/beta*(torch.exp(beta*t)-1) )\n        \n        return hazard, Survival\n    \n\n\nclass LogLogisticModel(BaseParametricModel):\n    """"""\n    LogLogisticModel:\n    ----------------\n    \n    As the name suggests, the log-logistic distribution is the distribution \n    of a variable whose logarithm has the logistic distribution. \n    The log-logistic distribution is often used to model random lifetimes.\n    (http://www.randomservices.org/random/special/LogLogistic.html)\n    \n    """"""\n\n    def get_hazard_survival(self, model, x, t):\n        """""" Computing the hazard and Survival functions. """"""\n        \n        # Computing the score\n        score  = model(x).reshape(-1, 1)\n\n        # Extracting beta\n        beta = list(model.parameters())[-1]\n\n        # Computing hazard and Survival\n        hazard  = score*beta*torch.pow(t, beta-1)\n        hazard  = hazard/(1 + score*torch.pow(t, beta) )\n        Survival = 1./(1. + torch.pow(score*t, beta) ) \n        \n        return hazard, Survival\n\n\nclass LogNormalModel(BaseParametricModel):\n    """"""\n    LogNormalModel:\n    ---------------\n    \n    The lognormal distribution is used to model continuous random quantities \n    when the distribution is believed to be skewed, such as lifetime variables\n    (http://www.randomservices.org/random/special/LogNormal.html)\n    \n    """"""\n\n    def get_hazard_survival(self, model, x, t):\n        """""" Computing the hazard and Survival functions. """"""\n        \n        # Computing the score\n        score  = model(x).reshape(-1, 1)\n\n        # Extracting beta\n        beta = list(model.parameters())[-1]\n\n        # Initializing the Normal distribution\n        from torch.distributions.normal import Normal\n        m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n\n        # Computing hazard and Survival\n        hazard =  (torch.log(t) - torch.log(score))/(np.sqrt(2)*beta)\n        Survival = 1. - m.cdf((torch.log(t)-torch.log(score))/(np.sqrt(2)*beta))\n        hazard = hazard*(torch.log(t) - torch.log(score))/(np.sqrt(2)*beta)\n        hazard = torch.exp( -hazard/2.)\n        hazard = hazard/(np.sqrt(2*np.pi)*Survival*(t*beta))\n\n        hazard = torch.max(hazard, torch.FloatTensor([1e-6]))\n        Survival = torch.max(Survival, torch.FloatTensor([1e-6]))\n        return hazard, Survival\n'"
pysurvival/models/semi_parametric.py,50,"b'from __future__ import absolute_import\nimport torch \nimport numpy as np\nimport pandas as pd\nimport scipy\nimport copy\nfrom pysurvival import HAS_GPU\nfrom pysurvival import utils\nfrom pysurvival.utils import neural_networks as nn\nfrom pysurvival.utils import optimization as opt\nfrom pysurvival.models import BaseModel\nfrom pysurvival.models._coxph import _CoxPHModel\nfrom pysurvival.models._coxph import _baseline_functions\n\n\nclass CoxPHModel(BaseModel):\n    """""" Cox proportional hazards model:\n        -------------------------------\n        The purpose of the model is to evaluate simultaneously \n        the effect of several factors on survival. \n        In other words, it allows us to examine how specified factors \n        influence the rate of a particular event happening \n        at a particular point in time. \n        \n        The Cox model is expressed by the hazard function h(t)\n        (the risk of dying at time t. )\n        It can be estimated as follow:\n            h(t, x)=h_0(t)*exp(<x, W>)\n\n        Then the Survival function can be calculated as follow:\n            H(t, x) = cumsum( h(t, x) )\n            S(t, x) = exp( -H(t, x) )\n\n        Reference:\n            * http://www.sthda.com/english/wiki/cox-proportional-hazards-model\n    """"""\n\n    def get_summary(self, alpha = 0.95, precision=3):\n        """""" Providing the summary of the regression results:\n                * standard errors\n                * z-score \n                * p-value\n        """"""\n        \n        # Flattening the coef \n        W_flat = self.weights.flatten()\n\n        # calculating standard error \n        self.std_err  = np.sqrt(self.inv_Hessian.diagonal())/self.std_scale   \n\n        # Confidence Intervals \n        alpha = scipy.stats.norm.ppf((1. + alpha) / 2.)\n        lower_ci = np.round( W_flat - alpha * self.std_err, precision)\n        upper_ci = np.round( W_flat + alpha * self.std_err, precision)\n        z = np.round(W_flat / self.std_err , precision)\n        p_values = np.round(scipy.stats.chi2.sf( np.square(z), 1), precision)\n        W = np.round(W_flat, precision)\n        std_err = np.round(self.std_err, precision)\n        \n        # Creating summary\n        df = np.c_[self.variables, W, std_err, \n                    lower_ci, upper_ci, z, p_values]\n        df = pd.DataFrame(data = df,\n                          columns = [\'variables\', \'coef\', \'std. err\', \n                                     \'lower_ci\', \'upper_ci\',\n                                     \'z\', \'p_values\'])\n        self.summary = df\n\n        return df\n\n    \n    def fit(self, X, T, E, init_method=\'glorot_normal\', lr = 1e-2, \n            max_iter = 100, l2_reg = 1e-2, alpha = 0.95,\n            tol = 1e-3, verbose = True ):\n        """"""\n        Fitting a proportional hazards regression model using\n        the Efron\'s approximation method to take into account tied times.\n        \n        As the Hessian matrix of the log-likelihood can be \n        calculated without too much effort, the model parameters are \n        computed using the Newton_Raphson Optimization scheme:\n                W_new = W_old - lr*<Hessian^(-1), gradient>\n        \n        Arguments:\n        ---------\n        * `X` : **array-like**, *shape=(n_samples, n_features)* --\n            The input samples.\n\n        * `T` : **array-like** -- \n            The target values describing when the event of interest or \n            censoring occurred.\n\n        * `E` : **array-like** --\n            The values that indicate if the event of interest occurred \n            i.e.: E[i]=1 corresponds to an event, and E[i] = 0 means censoring, \n            for all i.\n\n        * `init_method` : **str** *(default = \'glorot_uniform\')* -- \n            Initialization method to use. Here are the possible options:\n\n            * `glorot_uniform`: Glorot/Xavier uniform initializer\n            * `he_uniform`: He uniform variance scaling initializer\n            * `uniform`: Initializing tensors with uniform (-1, 1) distribution\n            * `glorot_normal`: Glorot normal initializer,\n            * `he_normal`: He normal initializer.\n            * `normal`: Initializing tensors with standard normal distribution\n            * `ones`: Initializing tensors to 1\n            * `zeros`: Initializing tensors to 0\n            * `orthogonal`: Initializing tensors with a orthogonal matrix,\n            \n        * `lr`: **float** *(default=1e-4)* -- \n            learning rate used in the optimization\n\n        * `max_iter`: **int** *(default=100)* -- \n            The maximum number of iterations in the Newton optimization\n\n        * `l2_reg`: **float** *(default=1e-4)* -- \n            L2 regularization parameter for the model coefficients\n\n        * `alpha`: **float** *(default=0.95)* -- \n            Confidence interval\n\n        * `tol`: **float** *(default=1e-3)* -- \n            Tolerance for stopping criteria\n\n        * `verbose`: **bool** *(default=True)* -- \n            Whether or not producing detailed logging about the modeling\n \n        Example:\n        --------\n\n        #### 1 - Importing packages\n        import numpy as np\n        import pandas as pd\n        from matplotlib import pyplot as plt\n        from sklearn.model_selection import train_test_split\n        from pysurvival.models.simulations import SimulationModel\n        from pysurvival.models.semi_parametric import CoxPHModel\n        from pysurvival.utils.metrics import concordance_index\n        from pysurvival.utils.display import integrated_brier_score\n        #%pylab inline  # To use with Jupyter notebooks\n\n\n        #### 2 - Generating the dataset from a Log-Logistic parametric model\n        # Initializing the simulation model\n        sim = SimulationModel( survival_distribution = \'log-logistic\',  \n                               risk_type = \'linear\',\n                               censored_parameter = 10.1, \n                               alpha = 0.1, beta=1.2 )\n\n        # Generating N random samples \n        N = 1000\n        dataset = sim.generate_data(num_samples = N, num_features = 3)\n\n        #### 3 - Creating the modeling dataset\n        # Defining the features\n        features = sim.features\n\n        # Building training and testing sets #\n        index_train, index_test = train_test_split( range(N), test_size = 0.2)\n        data_train = dataset.loc[index_train].reset_index( drop = True )\n        data_test  = dataset.loc[index_test].reset_index( drop = True )\n\n        # Creating the X, T and E input\n        X_train, X_test = data_train[features], data_test[features]\n        T_train, T_test = data_train[\'time\'].values, data_test[\'time\'].values\n        E_train, E_test = data_train[\'event\'].values, data_test[\'event\'].values\n\n\n        #### 4 - Creating an instance of the Cox PH model and fitting the data.\n        # Building the model\n        coxph = CoxPHModel()\n        coxph.fit(X_train, T_train, E_train, lr=0.5, l2_reg=1e-2, \n            init_method=\'zeros\')\n\n\n        #### 5 - Cross Validation / Model Performances\n        c_index = concordance_index(coxph, X_test, T_test, E_test) #0.92\n        print(\'C-index: {:.2f}\'.format(c_index))\n\n        ibs = integrated_brier_score(coxph, X_test, T_test, E_test, t_max=10, \n                    figure_size=(20, 6.5) )\n\n        References:\n        -----------\n        * https://en.wikipedia.org/wiki/Proportional_hazards_model#Tied_times\n        * Efron, Bradley (1974). ""The Efficiency of Cox\'s Likelihood \n          Function for Censored Data"". Journal of the American Statistical \n          Association. 72 (359): 557-565. \n        """"""\n        \n        # Collecting features names\n        N, self.num_vars = X.shape\n        if isinstance(X, pd.DataFrame):\n            self.variables = X.columns.tolist()\n        else:\n            self.variables = [\'x_{}\'.format(i) for i in range(self.num_vars)]\n\n        # Checking the format of the data \n        X, T, E = utils.check_data(X, T, E)\n        order = np.argsort(-T)\n        T = T[order]\n        E = E[order]\n        X = self.scaler.fit_transform( X[order, :] )\n        self.std_scale = np.sqrt( self.scaler.var_ )\n\n        # Initializing the model\n        self.model = _CoxPHModel()\n\n        # Creating the time axis\n        self.model.get_times(T, E)\n\n        # Initializing the parameters \n        W = np.zeros(self.num_vars)\n        W = opt.initialization(init_method, W, False).flatten()\n        W = W.astype(np.float64)\n        \n        # Optimizing to find best parameters \n        epsilon=1e-9\n        self.model.newton_optimization(X, T, E, W, lr, l2_reg, tol, epsilon, \n            max_iter, verbose)\n        \n        # Saving the Cython attributes in the Python object\n        self.weights = np.array( self.model.W )\n        self.loss = self.model.loss\n        self.times = np.array( self.model.times)\n        self.gradient = np.array( self.model.gradient )\n        self.Hessian = np.array( self.model.Hessian )\n        self.inv_Hessian = np.array( self.model.inv_Hessian )\n        self.loss_values = np.array( self.model.loss_values )\n        self.grad2_values = np.array( self.model.grad2_values )\n\n        # Computing baseline functions\n        score = np.exp( np.dot(X, self.weights) )\n        baselines = _baseline_functions(score, T, E)\n\n        # Saving the Cython attributes in the Python object\n        self.baseline_hazard = np.array( baselines[1] )\n        self.baseline_survival = np.array( baselines[2] )\n        del self.model\n        self.get_time_buckets()\n\n        # Calculating summary \n        self.get_summary(alpha)\n\n        return self\n    \n    \n    \n    def predict(self, x, t = None):\n        """""" \n        Predicting the hazard, density and survival functions\n        \n        Arguments:\n            * x: pd.Dataframe or np.ndarray or list\n                x is the testing dataset containing the features\n                x should not be standardized before, the model\n                will take care of it\n            * t: float (default=None)\n                Time at which hazard, density and survival functions\n                should be calculated. If None, the method returns \n                the functions for all times t. \n        """"""\n        \n        # Convert x into the right format\n        x = utils.check_data(x)\n        \n        # Sacling the dataset\n        if x.ndim == 1:\n            x = self.scaler.transform( x.reshape(1, -1) )\n        elif x.ndim == 2:\n            x = self.scaler.transform( x )\n            \n        # Calculating risk_score, hazard, density and survival \n        phi      = np.exp( np.dot(x, self.weights) )\n        hazard   = self.baseline_hazard*phi.reshape(-1, 1)\n        survival = np.power(self.baseline_survival, phi.reshape(-1, 1))\n        density  = hazard*survival\n        if t is None:\n            return hazard, density, survival\n        else:\n            min_index = [ abs(a_j_1-t) for (a_j_1, a_j) in self.time_buckets ]\n            index = np.argmin(min_index)\n            return hazard[:, index], density[:, index], survival[:, index]\n\n\n    def predict_risk(self, x, use_log = False):\n        """"""\n        Predicting the risk score functions\n        \n        Arguments:\n            * x: pd.Dataframe or np.ndarray or list\n                x is the testing dataset containing the features\n                x should not be standardized before, the model\n                will take care of it\n        """"""        \n\n         # Convert x into the right format\n        x = utils.check_data(x)\n        \n        # Scaling the dataset\n        if x.ndim == 1:\n            x = self.scaler.transform( x.reshape(1, -1) )\n        elif x.ndim == 2:\n            x = self.scaler.transform( x )\n            \n        # Calculating risk_score\n        risk_score  = np.exp( np.dot(x, self.weights) )   \n        if not use_log:\n            risk_score = np.exp(risk_score)\n\n        return risk_score\n\n        \n\n\nclass NonLinearCoxPHModel(BaseModel):\n    """""" NonLinear Cox Proportional Hazard model (NeuralCoxPH)\n        \n        The original Cox Proportional Hazard model, was first introduced \n        by  David R Cox in `Regression models and life-tables`.\n\n        The NonLinear CoxPH model was popularized by Katzman et al.\n        in `DeepSurv: Personalized Treatment Recommender System Using\n        A Cox Proportional Hazards Deep Neural Network` by allowing the use of \n        Neural Networks within the original design. \n        This current adaptation of the model differs from DeepSurv \n        as it uses the Efron\'s method to take ties into account.\n\n        Parameters\n        ----------\n            * structure: None or list of dictionaries\n                Provides an MLP structure within the CoxPH\n                If None, then the model becomes the Linear CoxPH\n                ex: structure = [ {\'activation\': \'relu\', \'num_units\': 128}, \n                                  {\'activation\': \'tanh\', \'num_units\': 128}, ] \n                Here are the possible activation functions:\n                    * Atan\n                    * BentIdentity\n                    * BipolarSigmoid\n                    * CosReLU\n                    * ELU\n                    * Gaussian\n                    * Hardtanh\n                    * Identity\n                    * InverseSqrt\n                    * LeakyReLU\n                    * LeCunTanh\n                    * LogLog\n                    * LogSigmoid\n                    * ReLU\n                    * SELU\n                    * Sigmoid\n                    * Sinc\n                    * SinReLU\n                    * Softmax\n                    * Softplus\n                    * Softsign\n                    * Swish\n                    * Tanh\n\n            * auto_scaler: boolean (default=True)\n                Determines whether a sklearn scaler should be automatically \n                applied             \n    """"""\n\n    def __init__(self, structure=None, auto_scaler = True):\n        \n        # Saving attributes\n        self.structure = structure\n        self.loss_values = []\n        \n        # Initializing the elements from BaseModel\n        super(NonLinearCoxPHModel, self).__init__(auto_scaler)\n\n\n\n    def risk_fail_matrix(self, T, E):\n        """""" Calculating the Risk, Fail matrices to calculate the loss \n            function by vectorizing all the quantities at stake\n        """"""\n\n        N = T.shape[0]\n        Risk = np.zeros( (self.nb_times, N) )\n        Fail = np.zeros( (self.nb_times, N) )\n        \n        for i in range(N):\n            \n            # At risk\n            index_risk = np.argwhere( self.times <= T[i] ).flatten()\n            Risk[ index_risk, i] = 1.\n            \n            # Failed\n            if E[i] == 1 :\n                index_fail = np.argwhere( self.times == T[i] )[0]\n                Fail[index_fail, i] = 1.\n\n        self.nb_fail_per_time = np.sum( Fail, axis = 1 ).astype(int)\n        return torch.FloatTensor(Risk), torch.FloatTensor(Fail)\n\n\n    def efron_matrix(self):\n        """""" Computing the Efron Coefficient matrices to calculate the loss \n            function by vectorizing all the quantities at stake\n        """"""\n        \n        max_nb_fails   = int(max(self.nb_fail_per_time))\n        Efron_coef     = np.zeros( (self.nb_times, max_nb_fails ) )\n        Efron_one      = np.zeros( (self.nb_times, max_nb_fails ) )\n        Efron_anti_one = np.ones(  (self.nb_times, max_nb_fails ) )\n        \n        for i, d in enumerate(self.nb_fail_per_time) :\n            if d > 0:\n                Efron_coef[i, :d] = [ h*1.0/d for h in range( d )]\n                Efron_one [i, :d] = 1.\n                Efron_anti_one[i, :d] = 0.\n                \n        Efron_coef = torch.FloatTensor(Efron_coef)\n        Efron_one = torch.FloatTensor(Efron_one)                \n        Efron_anti_one = torch.FloatTensor(Efron_anti_one)        \n        return Efron_coef, Efron_one, Efron_anti_one\n\n\n    def loss_function(self, model, X, Risk, Fail, \n                      Efron_coef, Efron_one, Efron_anti_one, l2_reg):\n        """""" Efron\'s approximation loss function by vectorizing \n            all the quantities at stake\n        """"""\n\n        # Calculating the score\n        pre_score = model(X)\n        score = torch.reshape( torch.exp(pre_score), (-1, 1) )\n        max_nb_fails = Efron_coef.shape[1]\n\n        # Numerator calculation\n        log_score = torch.log( score )\n        log_fail  = torch.mm(Fail, log_score)\n        numerator = torch.sum(log_fail)  \n\n        # Denominator calculation\n        risk_score = torch.reshape( torch.mm(Risk, score), (-1,1) )\n        risk_score = risk_score.repeat(1, max_nb_fails)        \n        \n        fail_score = torch.reshape( torch.mm(Fail, score), (-1,1) )\n        fail_score = fail_score.repeat(1, max_nb_fails)\n        \n        Efron_Fail  = fail_score*Efron_coef \n        Efron_Risk  = risk_score*Efron_one\n        log_efron   = torch.log( Efron_Risk - Efron_Fail + Efron_anti_one)\n                \n        denominator = torch.sum( torch.sum(log_efron, dim=1) )  \n        \n        # Adding regularization\n        loss = - (numerator - denominator)\n        for w in model.parameters():\n            loss += l2_reg*torch.sum(w*w)/2.\n            \n        return loss\n\n\n    def fit(self, X, T, E, init_method = \'glorot_uniform\',\n            optimizer =\'adam\', lr = 1e-4, num_epochs = 1000,\n            dropout = 0.2, batch_normalization=False, bn_and_dropout=False,\n            l2_reg=1e-5, verbose=True):\n        """""" \n        Fit the estimator based on the given parameters.\n\n        Parameters:\n        -----------\n        * `X` : **array-like**, *shape=(n_samples, n_features)* --\n            The input samples.\n\n        * `T` : **array-like** -- \n            The target values describing when the event of interest or censoring\n            occurred.\n\n        * `E` : **array-like** --\n            The values that indicate if the event of interest occurred i.e.: \n            E[i]=1 corresponds to an event, and E[i] = 0 means censoring, \n            for all i.\n\n        * `init_method` : **str** *(default = \'glorot_uniform\')* -- \n            Initialization method to use. Here are the possible options:\n\n            * `glorot_uniform`: Glorot/Xavier uniform initializer\n            * `he_uniform`: He uniform variance scaling initializer \n            * `uniform`: Initializing tensors with uniform (-1, 1) distribution\n            * `glorot_normal`: Glorot normal initializer,\n            * `he_normal`: He normal initializer.\n            * `normal`: Initializing tensors with standard normal distribution\n            * `ones`: Initializing tensors to 1\n            * `zeros`: Initializing tensors to 0\n            * `orthogonal`: Initializing tensors with a orthogonal matrix,\n\n        * `optimizer`:  **str** *(default = \'adam\')* -- \n            iterative method for optimizing a differentiable objective function.\n            Here are the possible options:\n\n            - `adadelta`\n            - `adagrad`\n            - `adam`\n            - `adamax`\n            - `rmsprop`\n            - `sparseadam`\n            - `sgd`\n\n        * `lr`: **float** *(default=1e-4)* -- \n            learning rate used in the optimization\n\n        * `num_epochs`: **int** *(default=1000)* -- \n            The number of iterations in the optimization\n\n        * `dropout`: **float** *(default=0.5)* -- \n            Randomly sets a fraction rate of input units to 0 \n            at each update during training time, which helps prevent overfitting.\n\n        * `l2_reg`: **float** *(default=1e-4)* -- \n            L2 regularization parameter for the model coefficients\n\n        * `batch_normalization`: **bool** *(default=True)* -- \n            Applying Batch Normalization or not\n\n        * `bn_and_dropout`: **bool** *(default=False)* -- \n            Applying Batch Normalization and Dropout at the same time\n\n        * `verbose`: **bool** *(default=True)* -- \n            Whether or not producing detailed logging about the modeling\n                \n\n        Example:\n        --------\n\n        #### 1 - Importing packages\n        import numpy as np\n        import pandas as pd\n        from matplotlib import pyplot as plt\n        from sklearn.model_selection import train_test_split\n        from pysurvival.models.simulations import SimulationModel\n        from pysurvival.models.semi_parametric import NonLinearCoxPHModel\n        from pysurvival.utils.metrics import concordance_index\n        from pysurvival.utils.display import integrated_brier_score\n        #%matplotlib inline  # To use with Jupyter notebooks\n\n        #### 2 - Generating the dataset from a nonlinear Weibull parametric model\n        # Initializing the simulation model\n        sim = SimulationModel( survival_distribution = \'weibull\',  \n                               risk_type = \'Gaussian\',\n                               censored_parameter = 2.1, \n                               alpha = 0.1, beta=3.2 )\n\n        # Generating N random samples \n        N = 1000\n        dataset = sim.generate_data(num_samples = N, num_features=3)\n\n        # Showing a few data-points \n        dataset.head(2)\n\n        #### 3 - Creating the modeling dataset\n        # Defining the features\n        features = sim.features\n\n        # Building training and testing sets #\n        index_train, index_test = train_test_split( range(N), test_size = 0.2)\n        data_train = dataset.loc[index_train].reset_index( drop = True )\n        data_test  = dataset.loc[index_test].reset_index( drop = True )\n\n        # Creating the X, T and E input\n        X_train, X_test = data_train[features], data_test[features]\n        T_train, T_test = data_train[\'time\'].values, data_test[\'time\'].values\n        E_train, E_test = data_train[\'event\'].values, data_test[\'event\'].values\n\n\n        #### 4 - Creating an instance of the NonLinear CoxPH model and fitting \n        # the data.\n\n        # Defining the MLP structure. Here we will build a 1-hidden layer \n        # with 150 units and `BentIdentity` as its activation function\n        structure = [ {\'activation\': \'BentIdentity\', \'num_units\': 150},  ]\n\n        # Building the model\n        nonlinear_coxph = NonLinearCoxPHModel(structure=structure) \n        nonlinear_coxph.fit(X_train, T_train, E_train, lr=1e-3, \n            init_method=\'xav_uniform\')\n\n\n        #### 5 - Cross Validation / Model Performances\n        c_index = concordance_index(nonlinear_coxph, X_test, T_test, E_test)\n        print(\'C-index: {:.2f}\'.format(c_index))\n\n        ibs = integrated_brier_score(nonlinear_coxph, X_test, T_test, E_test, \n            t_max=10, figure_size=(20, 6.5) )\n\n        """"""\n\n        # Checking data format (i.e.: transforming into numpy array)\n        X, T, E = utils.check_data(X, T, E)\n\n        # Extracting data parameters\n        N, self.num_vars = X.shape\n        input_shape = self.num_vars\n    \n        # Scaling data \n        if self.auto_scaler:\n            X_original = self.scaler.fit_transform( X ) \n            \n        # Sorting X, T, E in descending order according to T\n        order = np.argsort(-T)\n        T = T[order]\n        E = E[order]\n        X_original = X_original[order, :]\n        self.times = np.unique(T[E.astype(bool)])\n        self.nb_times = len(self.times)\n        self.get_time_buckets()\n\n        # Initializing the model\n        model = nn.NeuralNet(input_shape, 1, self.structure, \n                             init_method, dropout, batch_normalization, \n                             bn_and_dropout )\n\n        # Looping through the data to calculate the loss\n        X = torch.FloatTensor(X_original) \n\n        # Computing the Risk and Fail tensors\n        Risk, Fail = self.risk_fail_matrix(T, E)\n        Risk = torch.FloatTensor(Risk) \n        Fail = torch.FloatTensor(Fail) \n\n        # Computing Efron\'s matrices\n        Efron_coef, Efron_one, Efron_anti_one = self.efron_matrix()\n        Efron_coef = torch.FloatTensor(Efron_coef) \n        Efron_one = torch.FloatTensor(Efron_one) \n        Efron_anti_one = torch.FloatTensor(Efron_anti_one) \n\n        # Performing order 1 optimization\n        model, loss_values = opt.optimize(self.loss_function, model, optimizer, \n            lr, num_epochs, verbose, X=X, Risk=Risk, Fail=Fail, \n            Efron_coef=Efron_coef, Efron_one=Efron_one, \n            Efron_anti_one=Efron_anti_one, l2_reg=l2_reg)\n\n        # Saving attributes\n        self.model = model.eval()\n        self.loss_values = loss_values\n\n        # Computing baseline functions\n        x = X_original\n        x = torch.FloatTensor(x)\n\n        # Calculating risk_score\n        score = np.exp(self.model(torch.FloatTensor(x)).data.numpy().flatten())\n        baselines = _baseline_functions(score, T, E)\n\n        # Saving the Cython attributes in the Python object\n        self.times = np.array( baselines[0] )\n        self.baseline_hazard = np.array( baselines[1] )\n        self.baseline_survival = np.array( baselines[2] )\n\n        return self\n\n\n    def predict(self, x, t = None):\n        """""" \n        Predicting the hazard, density and survival functions\n        \n        Arguments:\n            * x: pd.Dataframe or np.ndarray or list\n                x is the testing dataset containing the features\n                x should not be standardized before, the model\n                will take care of it\n            * t: float (default=None)\n                Time at which hazard, density and survival functions\n                should be calculated. If None, the method returns \n                the functions for all times t. \n        """"""\n        \n        # Convert x into the right format\n        x = utils.check_data(x)\n        \n        # Scaling the dataset\n        if x.ndim == 1:\n            x = self.scaler.transform( x.reshape(1, -1) )\n        elif x.ndim == 2:\n            x = self.scaler.transform( x )\n            \n        # Calculating risk_score, hazard, density and survival \n        score    = self.model(torch.FloatTensor(x)).data.numpy().flatten()\n        phi      = np.exp( score )\n        hazard   = self.baseline_hazard*phi.reshape(-1, 1)\n        survival = np.power(self.baseline_survival, phi.reshape(-1, 1))\n        density  = hazard*survival\n        if t is None:\n            return hazard, density, survival\n        else:\n            min_index = [ abs(a_j_1-t) for (a_j_1, a_j) in self.time_buckets ]\n            index = np.argmin(min_index)\n            return hazard[:, index], density[:, index], survival[:, index]\n\n\n    def predict_risk(self, x, use_log = False):\n        """"""\n        Predicting the risk score functions\n        \n        Arguments:\n            * x: pd.Dataframe or np.ndarray or list\n                x is the testing dataset containing the features\n                x should not be standardized before, the model\n                will take care of it\n        """"""        \n\n        # Convert x into the right format\n        x = utils.check_data(x)\n        \n        # Scaling the data\n        if self.auto_scaler:\n            if x.ndim == 1:\n                x = self.scaler.transform( x.reshape(1, -1) )\n            elif x.ndim == 2:\n                x = self.scaler.transform( x )\n        else:\n            # Ensuring x has 2 dimensions\n            if x.ndim == 1:\n                x = np.reshape(x, (1, -1))\n\n        # Transforming into pytorch objects\n        x = torch.FloatTensor(x)\n\n        # Calculating risk_score\n        score = self.model(x).data.numpy().flatten()\n        if not use_log:\n            score = np.exp(score)\n\n        return score\n\n\n    def __repr__(self):\n        """""" Representing the class object """"""\n\n        if self.structure is None:\n            super(NonLinearCoxPHModel, self).__repr__()\n            return self.name\n            \n        else:\n            S = len(self.structure)\n            self.name = self.__class__.__name__\n            empty = len(self.name)\n            self.name += \'( \'\n            for i, s in enumerate(self.structure):\n                n = \'Layer({}): \'.format(i+1)\n                activation = nn.activation_function(s[\'activation\'], \n                    return_text=True)\n                n += \'activation = {}, \'.format( s[\'activation\'] )\n                n += \'num_units = {} \'.format( s[\'num_units\'] )\n                \n                if i != S-1:\n                    self.name += n + \'; \\n\'\n                    self.name += empty*\' \' + \'  \'\n                else:\n                    self.name += n\n            self.name += \')\'\n            return self.name\n'"
pysurvival/models/simulations.py,48,"b'from __future__ import absolute_import\nimport numpy as np\nimport pandas as pd\nimport random\nimport scipy\nimport copy\nfrom pysurvival import utils\nfrom pysurvival.models import BaseModel\n# %matplotlib inline\n\n# List of Survival Distributions\nDISTRIBUTIONS = [\'Exponential\',\n                 \'Weibull\',\n                 \'Gompertz\',\n                 \'Log-Logistic\',\n                 \'Log-Normal\',]    \n\n# List of risk types\nRISK_TYPES = [\'Linear\', \'Square\',  \'Gaussian\']    \n\n\n\nclass SimulationModel(BaseModel):\n    """""" \n    A general framework for simulating right-censored survival data \n    for proportional hazards models by incorporating \n        * a baseline hazard function from a known survival distribution, \n        * a set of covariates. \n    \n    The framework is based on ""Generating Survival Times to Simulate \n    Cox Proportional Hazards Models""\n    https://www.ncbi.nlm.nih.gov/pubmed/22763916\n\n    The formula for the different survival times and functions, and hazard\n    functions can be found at :\n    http://data.princeton.edu/pop509/ParametricSurvival.pdf\n\n    Parameters:\n    -----------\n\n    * survival_distribution: string (default = \'exponential\')\n        Defines a known survival distribution. The available options are:\n            - Exponential\n            - Weibull\n            - Gompertz\n            - Log-Logistic\n            - Log-Normal\n        \n    * risk_type: string (default=\'linear\')\n        Defines the type of risk function. The available options are:\n            - Linear\n            - Square\n            - Gaussian\n        \n    * alpha: double (default = 1.) \n         the scale parameter\n\n    * beta: double (default = 1.)\n         the shape parameter\n         \n    * bins: int (default=100)\n        the number of bins of the time axis\n\n    * censored_parameter: double (default = 1.)\n         coefficient used to calculate the censored distribution. This\n         distribution is a normal such that N(loc=censored_parameter, scale=5)\n         \n    * risk_parameter: double (default = 1.)\n        Scaling coefficient of the risk score such that:\n            - linear: r(x) = exp(<x, W>) \n            - square: r(x) = exp(risk_parameter*(<x, W>)^2)\n            - gaussian: r(x) = exp( exp(-(<x, W>)^2/risk_parameter) )  \n        <.,.> is the dot product                  \n    """"""\n\n    def __init__(self, survival_distribution = \'exponential\',  \n        risk_type = \'linear\', censored_parameter = 1., alpha = 1, beta = 1.,\n        bins = 100, risk_parameter = 1.):\n        \n        # Saving the attributes\n        self.censored_parameter = censored_parameter\n        self.alpha              = alpha \n        self.beta               = beta\n        self.risk_parameter     = risk_parameter\n        self.bins               = bins\n        self.features = []\n        \n        # Checking risk_type\n        if any([risk_type.lower() == r.lower() for r in RISK_TYPES ]):\n            self.risk_type   = risk_type\n        else:\n            error = ""{} isn\'t a valid risk type. ""\n            error += ""Only {} are currently available.""\n            error = error.format(risk_type, "", "".join(RISK_TYPES))\n            raise NotImplementedError(error)\n        \n        # Checking distribution\n        if any([survival_distribution.lower() == d.lower() \\\n                for d in DISTRIBUTIONS ]):\n            self.survival_distribution   = survival_distribution\n        else:\n            error = ""{} isn\'t a valid survival distribution. ""\n            error += ""Only {} are currently available.""\n            error = error.format(survival_distribution,"", "".join(DISTRIBUTIONS))\n            raise NotImplementedError(error)\n        \n        # Initializing the elements from BaseModel\n        super(SimulationModel, self).__init__(auto_scaler = True)\n        \n\n    @staticmethod\n    def random_data(N):\n        """""" \n        Generating a array of size N from a random distribution -- the available \n        distributions are:\n            * binomial,\n            * chisquare,\n            * exponential, \n            * gamma, \n            * normal, \n            * uniform \n            * laplace \n        """"""\n\n        index = np.random.binomial(n = 4, p = 0.5)\n        distributions = {\n            \'binomial_a\': np.random.binomial(n = 20, p = 0.6, size = N ), \n            \'binomial_b\': np.random.binomial(n = 200, p = 0.6, size = N ), \n            \'chisquare\': np.random.chisquare(df = 10, size = N ), \n            \'exponential_a\': np.random.exponential(scale=0.1, size = N ),\n            \'exponential_b\': np.random.exponential(scale=0.01, size = N ),\n            \'gamma\': np.random.gamma(shape=2., scale=2., size = N ),\n            \'normal_a\': np.random.normal(loc=-1.0, scale=5.0, size=N ),\n            \'normal_b\': np.random.normal(loc=10.0, scale=10.0, size=N ),\n            \'uniform_a\': np.random.uniform(low=-2.0, high=10.0, size=N ),\n            \'uniform_b\': np.random.uniform(low=-20.0, high=100.0, size=N ),\n            \'laplace\': np.random.laplace(loc=0.0, scale=1.0, size=N )\n            }\n\n        list_distributions = copy.deepcopy(list(distributions.keys()))\n        random.shuffle(list_distributions)\n        key = list_distributions[ index ]\n        return key, distributions[key]\n\n\n    def time_function(self, BX):\n        """""" \n        Calculating the survival times based on the given distribution\n        T = H^(-1)( -log(U)/risk_score ), where:\n            * H is the cumulative baseline hazard function \n                (H^(-1) is the inverse function)\n            * U is a random variable uniform - Uni[0,1].\n\n        The method is inspired by https://gist.github.com/jcrudy/10481743\n        """"""\n        \n        # Calculating scale coefficient using the features\n        num_samples = BX.shape[0]\n        lambda_exp_BX = np.exp(BX)*self.alpha\n        lambda_exp_BX = lambda_exp_BX.flatten()\n        \n        # Generating random uniform variables\n        U = np.random.uniform(0, 1, num_samples)\n\n        # Exponential \n        if self.survival_distribution.lower().startswith(\'exp\') :\n            self.survival_distribution = \'Exponential\'\n            return - np.log( U )/( lambda_exp_BX )\n\n        # Weibull \n        elif self.survival_distribution.lower().startswith(\'wei\') :\n            self.survival_distribution = \'Weibull\'\n            return np.power( - np.log( U )/( lambda_exp_BX ), 1./self.beta )\n\n        # Gompertz \n        elif self.survival_distribution.lower().startswith(\'gom\') :\n            self.survival_distribution = \'Gompertz\'\n            return  ( 1./self.beta)*\\\n                    np.log( 1 - self.beta*np.log( U )/(lambda_exp_BX) )\n\n        # Log-Logistic \n        elif \'logistic\' in self.survival_distribution.lower() :\n            self.survival_distribution = \'Log-Logistic\'\n            return  np.power( U/(1.-U), 1./self.beta )/(lambda_exp_BX )\n\n        # Log-Normal\n        elif \'normal\' in self.survival_distribution.lower() :\n            self.survival_distribution = \'Log-Normal\'\n            W = np.random.normal(0, 1, num_samples)\n            return  lambda_exp_BX*np.exp(self.beta*W)\n\n    \n    def hazard_function(self, t, BX):\n        """""" Calculating the hazard function based on the given distribution """"""\n        \n        # Calculating scale coefficient using the features\n        _lambda = self.alpha*np.exp( BX )\n\n        # Exponential \n        if self.survival_distribution.lower().startswith( \'exp\' ) : \n            return  np.repeat(_lambda, len(t)) \n\n        # Weibull \n        elif self.survival_distribution.lower().startswith(\'wei\'):\n            return  _lambda*self.beta*np.power( t, self.beta-1 ) \n\n        # Gompertz \n        elif self.survival_distribution.lower().startswith(\'gom\'):\n            return  _lambda*np.exp(self.beta*t )\n\n        # Log-Logistic \n        elif self.survival_distribution.lower().endswith(\'logistic\'):\n            numerator =  _lambda*self.beta*np.power((_lambda*t), self.beta-1 )\n            denominator =  (1 + np.power( (_lambda*t), self.beta) )\n            return numerator/denominator\n    \n        # Log-Normal\n        elif self.survival_distribution.lower().endswith(\'normal\'):\n            arg_normal = (np.log(t) - np.log(_lambda))/self.beta\n            numerator   =  (1./(t*self.beta))*scipy.stats.norm.pdf( arg_normal )\n            denominator =  1. - scipy.stats.norm.cdf(arg_normal)\n            return numerator/denominator\n\n    \n    def survival_function(self, t, BX):\n        """""" \n        Calculating the survival function based on the given \n        distribution \n        """"""\n\n        # Calculating scale coefficient using the features\n        _lambda = self.alpha*np.exp( BX )\n\n        # Exponential \n        if self.survival_distribution.lower().startswith( \'exp\' ) :\n            return np.exp( -t*_lambda )\n\n        # Weibull \n        elif self.survival_distribution.lower().startswith(\'wei\'):\n            return np.exp( -np.power(t, self.beta)*_lambda )\n\n        # Gompertz \n        elif self.survival_distribution.lower().startswith(\'gom\'):\n            return np.exp( -_lambda/self.beta*( np.exp(self.beta*t) - 1) )\n\n        # Log-Logistic \n        elif self.survival_distribution.lower().endswith(\'logistic\'):\n            return 1./(1.+ np.power(_lambda*t, self.beta) )\n\n        # Log-Normal\n        elif self.survival_distribution.lower().endswith(\'normal\'):\n            arg_cdf = (np.log(t) - np.log(_lambda))/self.beta\n            return 1. - scipy.stats.norm.cdf(arg_cdf)\n\n\n    def risk_function(self, x_std):\n        """""" Calculating the risk function based on the given risk type """"""\n        \n        # Dot product\n        risk = np.dot( x_std, self.feature_weights )\n        \n        # Choosing the type of risk\n        if self.risk_type.lower() == \'linear\' :\n            return risk.reshape(-1, 1)\n\n        elif self.risk_type.lower() == \'square\' :\n            risk = np.square(risk*self.risk_parameter)\n\n\n        elif self.risk_type.lower() == \'gaussian\' :\n            risk = np.square(risk)\n            risk = np.exp( - risk*self.risk_parameter)\n\n        return risk.reshape(-1, 1)\n\n        \n    def generate_data(self, num_samples = 100, num_features = 3, \n        feature_weights = None):\n        """""" \n        Generating a dataset of simulated survival times from a given \n        distribution through the hazard function using the Cox model  \n        \n        Parameters:\n        -----------\n        * `num_samples`: **int** *(default=100)* --\n            Number of samples to generate\n\n        * `num_features`: **int** *(default=3)* --\n            Number of features to generate\n\n        * `feature_weights`: **array-like** *(default=None)* -- \n            list of the coefficients of the underlying Cox-Model. \n            The features linked to each coefficient are generated \n            from random distribution from the following list:\n\n            * binomial\n            * chisquare\n            * exponential\n            * gamma\n            * normal\n            * uniform\n            * laplace\n\n            If None then feature_weights = [1.]*num_features\n\n        Returns:\n        --------\n        * dataset: pandas.DataFrame\n            dataset of simulated survival times, event status and features\n\n\n        Example:\n        --------\n        from pysurvival.models.simulations import SimulationModel\n\n        # Initializing the simulation model\n        sim = SimulationModel( survival_distribution = \'gompertz\',  \n                               risk_type = \'linear\',\n                               censored_parameter = 5.0, \n                               alpha = 0.01, \n                               beta = 5., )\n\n        # Generating N Random samples\n        N = 1000\n        dataset = sim.generate_data(num_samples = N, num_features=5)\n\n        # Showing a few data-points\n        dataset.head()\n        """"""\n        \n        # Data parameters\n        self.num_variables = num_features\n        if feature_weights is None :\n            self.feature_weights = [1.]*self.num_variables\n            feature_weights = self.feature_weights\n\n        else:\n            feature_weights = utils.check_data(feature_weights)\n            if num_features != len(feature_weights):\n                error = ""The length of feature_weights ({}) ""\n                error += ""and num_features ({}) are not the same.""\n                error = error.format(len(feature_weights), num_features)\n                raise ValueError(error)\n            self.feature_weights = feature_weights\n\n        # Generating random features\n        # Creating the features\n        X = np.zeros((num_samples, self.num_variables))\n        columns = []\n        for i in range( self.num_variables ) :\n            key, X[:, i] = self.random_data(num_samples)\n            columns.append( \'x_\' + str(i+1) )\n        X_std = self.scaler.fit_transform( X )\n        BX = self.risk_function( X_std )\n        \n        # Building the survival times\n        T = self.time_function(BX) \n        C = np.random.normal( loc = self.censored_parameter,\n            scale = 5, size = num_samples )\n        C = np.maximum(C, 0.)\n        time = np.minimum( T, C )\n        E = 1.*(T == time)\n\n        # Building dataset\n        self.features = columns\n        self.dataset = pd.DataFrame( data = np.c_[X, time, E], \n                                     columns = columns + [\'time\', \'event\'] )\n        \n        # Building the time axis and time buckets\n        self.times = np.linspace(0., max(self.dataset[\'time\']), self.bins)\n        self.get_time_buckets()\n        \n        # Building baseline functions\n        self.baseline_hazard   =  self.hazard_function(self.times, 0)\n        self.baseline_survival =  self.survival_function(self.times, 0) \n\n        # Printing summary message\n        message_to_print = ""Number of data-points: {} - Number of events: {}""\n        print( message_to_print.format(num_samples, sum(E)) )\n\n        return self.dataset\n    \n    \n    def predict(self, x, t = None):\n        """""" \n        Predicting the hazard, density and survival functions\n        \n        Parameters:\n        -----------\n            * x: pd.Dataframe or np.ndarray or list\n                x is the testing dataset containing the features\n                x should not be standardized before, the model\n                will take care of it\n            * t: float (default=None)\n                Time at which hazard, density and survival functions\n                should be calculated. If None, the method returns \n                the functions for all times t. \n        """"""\n\n        # Convert x into the right format\n        x = utils.check_data(x)\n        \n        # Scaling the dataset\n        if x.ndim == 1:\n            x = self.scaler.transform( x.reshape(1, -1) )\n\n        elif x.ndim == 2:\n            x = self.scaler.transform( x )\n\n        else:\n            # Ensuring x has 2 dimensions\n            if x.ndim == 1:\n                x = np.reshape(x, (1, -1))\n\n        # Calculating risk_score, hazard, density and survival \n        BX = self.risk_function(x) \n        hazard   = self.hazard_function(self.times, BX.reshape(-1, 1))\n        survival = self.survival_function(self.times, BX.reshape(-1, 1))\n        density  = (hazard*survival)\n        \n        if t is None:\n            return hazard, density, survival\n        else:\n            min_abs_value = [abs(a_j_1-t) for (a_j_1, a_j) in self.time_buckets]\n            index = np.argmin(min_abs_value)\n            return hazard[:, index], density[:, index], survival[:, index]\n\n\n    def predict_risk(self, x):\n        """"""\n        Predicting the risk score function\n        \n        Parameters:\n        -----------\n            * x: pd.Dataframe or np.ndarray or list\n                x is the testing dataset containing the features\n                x should not be standardized before, the model\n                will take care of it\n        """"""        \n\n        # Convert x into the right format\n        x = utils.check_data(x)\n        \n        # Scaling the dataset\n        if x.ndim == 1:\n            x = self.scaler.transform( x.reshape(1, -1) )\n\n        elif x.ndim == 2:\n            x = self.scaler.transform( x )\n\n        else:\n            # Ensuring x has 2 dimensions\n            if x.ndim == 1:\n                x = np.reshape(x, (1, -1))\n            \n        # Calculating risk_score\n        risk_score  = self.risk_function(x) \n\n        return risk_score\n\n'"
pysurvival/models/survival_forest.py,14,"b'from __future__ import absolute_import\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport os\nimport copy\nfrom sklearn.preprocessing import StandardScaler\nfrom pysurvival import utils\nfrom pysurvival.models import BaseModel\nfrom pysurvival.models._survival_forest import _SurvivalForestModel\nfrom pysurvival import PYTHON_VERSION\n\n\n# Available Splitting\nSPLITTING_RULES = { \'logrank\': 1, \'maxstat\': 4, \'extratrees\':5}\n\nclass BaseSurvivalForest(BaseModel):\n    """"""\n    A random survival forest base class.\n\n    Parameters\n    ----------\n\n    * num_trees : int (default=10)\n        The number of trees in the forest.\n\n    * splitrule: int (default=0)\n        Splitting rule used to build trees:  \n            - 1, ""logrank"" yields the RandomSurvivalForest\n            - 4, ""maxstat"" yields the ConditionalSurvivalForest\n            - 5, ""extratrees"" yields the ExtraSurvivalTrees\n\n    """"""\n\n    def __init__(self, splitrule = ""Logrank"", num_trees = 10):\n\n        # Checking the format of num_trees\n        if not ( isinstance(num_trees, int) or isinstance(num_trees, float) ):\n            error = \'{} is not a valid value for ""num_trees"" \'\n            error += \'as ""num_trees"" is a positive integer\'.format(num_trees)\n            raise ValueError(error)\n        if num_trees <= 0 :\n            error = \'{} is not a valid value for ""num_trees"" \'\n            error += \'as ""num_trees"" is a positive integer\'.format(num_trees)\n            raise ValueError(error)\n        self.num_trees = num_trees\n\n        # Checking the format of splitrule\n        if SPLITTING_RULES.get(splitrule.lower()) is None:\n            error = \'{} is not a valid splitrule method. Choose between \'\n            error += \'""\' + \'"", ""\'.join(SPLITTING_RULES) + \'""\'\n            error = error.format(splitrule)\n            raise ValueError(error)\n        self.splitrule = splitrule\n\n        # Initializing the inner model\n        self.model = _SurvivalForestModel()\n\n        # Initializing the elements from BaseModel\n        super(BaseSurvivalForest, self).__init__(auto_scaler = False)\n\n\n    def save(self, path_file):\n        """""" Save the model parameters of the model and compress them into \n            a zip file\n        """"""\n\n        # Ensuring the file has the proper name\n        folder_name = os.path.dirname(path_file) + \'/\'\n        file_name = os.path.basename(path_file)\n\n        # Checking if the folder is accessible\n        if not os.access(folder_name, os.W_OK):\n            error_msg = \'{} is not an accessible directory.\'.format(folder_name)\n            raise OSError(error_msg)\n\n        # Delete the C++ object before saving\n        del self.model\n        \n        # Saving the model\n        super(BaseSurvivalForest, self).save(path_file)\n\n        # Re-introduce the C++ object\n        self.model = _SurvivalForestModel()\n        self.load_properties()\n        \n\n    def load(self, path_file):\n        """""" Load the model parameters from a zip file into a C++ external\n            model \n        """"""   \n\n        # Loading the model\n        super(BaseSurvivalForest, self).load(path_file)\n\n        # Re-introduce the C++ object\n        self.model = _SurvivalForestModel()\n        self.load_properties()\n\n\n    def fit(self, X, T, E, max_features = \'sqrt\', max_depth = 5, \n            min_node_size = 10, num_threads = -1, weights = None, \n            sample_size_pct = 0.63, alpha = 0.5, minprop=0.1,\n            num_random_splits = 100, importance_mode = \'impurity_corrected\', \n            seed = None, save_memory=False ):\n        """"""\n        Arguments:\n        ---------\n        * X : array-like, shape=(n_samples, n_features)\n            The input samples.\n\n        * T : array-like, shape = [n_samples] \n            The target values describing when the event of interest or censoring\n            occurred\n\n        * E : array-like, shape = [n_samples] \n            The Event indicator array such that E = 1. if the event occurred\n            E = 0. if censoring occurred\n\n        * max_features : int, float or string, optional (default=""all"")\n            The number of features to consider when looking for the best split:\n            - If int, then consider `max_features` features at each split.\n            - If float, then `max_features` is a fraction and\n              `int(max_features * n_features)` features are considered at each\n              split.\n            - If ""sqrt"", then `max_features=sqrt(n_features)` \n            - If ""log2"", then `max_features=log2(n_features)`.\n\n        * min_node_size : int(default=10)\n            The minimum number of samples required to be at a leaf node\n\n        * num_threads: int (Default: -1)\n            The number of jobs to run in parallel for both fit and predict. \n            If -1, then the number of jobs is set to the number of cores.\n\n        * weights: array-like, shape = [n_samples] (default=None)\n            Weights for sampling of training observations. \n            Observations with larger weights will be selected with \n            higher probability in the bootstrap\n\n        * sample_size_pct: double (default = 0.63)\n            Percentage of original samples used in each tree building\n\n        * alpha: float\n            For ""maxstat"" splitrule: Significance threshold to allow splitting.\n\n        * minprop: float\n            For ""maxstat"" splitrule: Lower quantile of covariate \n            distribution to be considered for splitting.\n\n        * num_random_splits: int (default=100)\n            For ""extratrees"" splitrule, it is the Number of random splits \n            to consider for each candidate splitting variable.\n\n        * importance_mode:  (default=impurity_corrected)\n            Variable importance mode. Here are the 2 options:\n            - `impurity` or `impurity_corrected`: \n                it\'s the unbiased heterogeneity reduction developed \n                by Sandri & Zuccolotto (2008)\n            - ""permutation"" it\'s unnormalized as recommended by Nicodemus et al.\n            - ""normalized_permutation"" it\'s normalized version of the \n                permutation importance computations by Breiman et al.\n\n        * `seed`: int (default=None) -- \n            seed used by the random number generator. If None, the current \n            timestamp converted in UNIX is used.\n\n        * save_memory:  bool (default=False) --\n            Use memory saving splitting mode. This will slow down the model \n            training. So, only set to `True` if you encounter memory problems.\n\n\n        Example:\n        --------\n\n        #### 1 - Importing packages\n        import numpy as np\n        import pandas as pd\n        from matplotlib import pyplot as plt\n        from sklearn.model_selection import train_test_split\n        from pysurvival.models.simulations import SimulationModel\n        from pysurvival.models.survival_forest import ConditionalSurvivalForestModel\n        from pysurvival.utils.metrics import concordance_index\n        from pysurvival.utils.display import integrated_brier_score\n        #%matplotlib inline # To use with Jupyter notebooks\n\n        #### 2 - Generating the dataset from a Exponential parametric model\n        # Initializing the simulation model\n        sim = SimulationModel( survival_distribution = \'exponential\',  \n                               risk_type = \'linear\',\n                               censored_parameter = 1, \n                               alpha = 3)\n\n        # Generating N random samples \n        N = 1000\n        dataset = sim.generate_data(num_samples = N, num_features=4)\n\n        # Showing a few data-points \n        dataset.head(2)\n\n        #### 3 - Creating the modeling dataset\n        # Defining the features\n        features = sim.features\n\n        # Building training and testing sets #\n        index_train, index_test = train_test_split( range(N), test_size = 0.2)\n        data_train = dataset.loc[index_train].reset_index( drop = True )\n        data_test  = dataset.loc[index_test].reset_index( drop = True )\n\n        # Creating the X, T and E input\n        X_train, X_test = data_train[features], data_test[features]\n        T_train, T_test = data_train[\'time\'].values, data_test[\'time\'].values\n        E_train, E_test = data_train[\'event\'].values, data_test[\'event\'].values\n\n\n        #### 4 - Creating an instance of the model and fitting the data.\n        # Building the model\n        csf = ConditionalSurvivalForestModel(num_trees=200) \n        csf.fit(X_train, T_train, E_train, \n                max_features=""sqrt"", max_depth=5, min_node_size=20,\n                alpha = 0.05, minprop=0.1)\n\n\n        #### 5 - Cross Validation / Model Performances\n        c_index = concordance_index(l_mtlr, X_test, T_test, E_test) #0.81\n        print(\'C-index: {:.2f}\'.format(c_index))\n\n        ibs = integrated_brier_score(l_mtlr, X_test, T_test, E_test, t_max=30, \n                    figure_size=(20, 6.5) )\n        """"""\n\n        # Collecting features names\n        N, self.num_variables = X.shape\n        if isinstance(X, pd.DataFrame):\n            features = X.columns.tolist()\n        else:\n            features = [\'x_{}\'.format(i) for i in range(self.num_variables)]\n        all_data_features = [""time"", ""event""] + features\n\n        # Transforming the strings into bytes\n        all_data_features = utils.as_bytes(all_data_features, \n            python_version=PYTHON_VERSION)\n\n        # Checking the format of the data \n        X, T, E = utils.check_data(X, T, E)\n        if X.ndim == 1:\n            X = X.reshape(1,-1)\n            T = T.reshape(1,-1)\n            E = E.reshape(1,-1)\n        input_data = np.c_[T, E, X]\n\n        # Number of trees\n        num_trees = self.num_trees\n\n        # Seed\n        if seed is None:\n            seed = 0\n\n        # sample_size_pct\n        if not isinstance(sample_size_pct, float): \n            error = ""Error: Invalid value for sample_size_pct, ""\n            error += ""please provide a value that is > 0 and <= 1.""\n            raise ValueError(error) \n\n        if (sample_size_pct <= 0 or sample_size_pct > 1) :\n            error = ""Error: Invalid value for sample_size_pct, ""\n            error += ""please provide a value that is > 0 and <= 1.""\n            raise ValueError(error)        \n\n        # Split Rule\n        if self.splitrule.lower() == \'logrank\':\n            split_mode = 1\n            alpha = 0\n            minprop = 0\n            num_random_splits = 1\n\n        elif self.splitrule.lower() == ""maxstat"":\n            split_mode = 4\n            num_random_splits = 1\n\n            # Maxstat splitting\n            if not isinstance(alpha, float): \n                error = ""Error: Invalid value for alpha, ""\n                error += ""please provide a value that is > 0 and < 1.""\n                raise ValueError(error) \n\n            if (alpha <= 0 or alpha >= 1) :\n                error = ""Error: Invalid value for alpha, ""\n                error += ""please provide a value between 0 and 1.""\n                raise ValueError(error)\n\n            if not isinstance(minprop, float): \n                error = ""Error: Invalid value for minprop, ""\n                error += ""please provide a value between 0 and 0.5""\n                raise ValueError(error)    \n\n            if (minprop < 0 or minprop > 0.5) :\n                error = ""Error: Invalid value for minprop, ""\n                error += ""please provide a value between 0 and 0.5""\n                raise ValueError(error)             \n\n        elif self.splitrule.lower() == \'extratrees\':\n            split_mode = 5\n            alpha = 0\n            minprop = 0\n\n        # Number of variables to possibly split at in each node\n        self.max_features = max_features\n        if isinstance(self.max_features, str):\n\n            if self.max_features.lower() == \'sqrt\':\n                num_variables_to_use = int( np.sqrt(self.num_variables) )\n\n            elif \'log\' in self.max_features.lower() :\n                num_variables_to_use = int( np.log(self.num_variables) )\n\n            elif self.max_features.lower() == \'all\':\n                num_variables_to_use = self.num_variables\n\n            else:\n                raise ValueError(""Unknown max features option"")\n\n        elif isinstance(self.max_features, float) or \\\n            isinstance(self.max_features, int):\n\n            if 0 < self.max_features < 1:\n                num_variables_to_use = int(self.num_variables*self.max_features)\n\n            elif self.max_features >= 1:\n                num_variables_to_use = min(self.num_variables, self.max_features)\n                if self.max_features > self.num_variables:\n                    msg = ""max features value is greater than the number of ""\n                    msg += ""variables ({num_variables}) of the input X. ""\n                    msg += ""So it was set to {num_variables}.""\n                    msg = msg.format(num_variables=self.num_variables)\n                    warnings.warn(msg, UserWarning)\n\n            elif self.max_features <= 0:\n                raise ValueError(""max features is a positive value"")\n\n        else:\n            raise ValueError(""Unknown max features option"")\n\n        # Defining importance mode\n        if \'permutation\' in importance_mode.lower() :\n\n            if \'scaled\' in importance_mode.lower() or \\\n            \'normalized\' in importance_mode.lower():\n                importance_mode = 2\n            else:\n                importance_mode = 3\n\n        elif \'impurity\' in importance_mode.lower() :\n            importance_mode = 5\n\n        else:\n            error = ""{} is not a valid importance mode"".format(importance_mode)\n            raise ValueError(error)\n\n        # Weights\n        if weights is None:\n            case_weights = [1./N]*N\n        else:\n            case_weights = utils.check_data(weights)\n\n            if abs(sum(case_weights) - 1.) >= 1e-4:\n                raise Exception(""The sum of the weights needs to be equal to 1."")\n\n            if len(case_weights) != N:\n                raise Exception(""weights length needs to be {} "".format(N))\n\n        # Fitting the model using the C++ object\n        verbose = True\n        self.model.fit( input_data, all_data_features, case_weights,\n                num_trees, num_variables_to_use, min_node_size, max_depth,\n                alpha, minprop, num_random_splits, sample_size_pct, \n                importance_mode, split_mode, verbose, seed, num_threads, \n                save_memory)\n\n        # Saving the attributes\n        self.save_properties()\n        self.get_time_buckets()\n\n        # Extracting the Variable Importance\n        self.variable_importance = {}\n        for i, value in enumerate(self.variable_importance_):\n            self.variable_importance[features[i]] = value\n\n        # Saving the importance in a dataframe\n        self.variable_importance_table = pd.DataFrame(\n            data={\'feature\': list(self.variable_importance.keys()),\n                  \'importance\': list(self.variable_importance.values())\n                    },\n            columns=[\'feature\', \'importance\']).\\\n            sort_values(\'importance\', ascending=0).reset_index(drop=True)\n        importance = self.variable_importance_table[\'importance\'].values\n        importance = np.maximum(importance, 0.)\n        sum_imp = sum(importance)*1.\n        self.variable_importance_table[\'pct_importance\']= importance/sum_imp\n\n        return self\n\n\n    def predict(self, X, t = None, num_threads=-1):\n\n        # Checking if the data has the right format\n        X = utils.check_data(X)\n        if X.ndim == 1:\n            X = X.reshape(1,-1)\n        T = np.array([1.]*X.shape[0])\n        E = np.array([1.]*X.shape[0])\n        input_data = np.c_[T, E, X]\n\n        # Loading the attributes of the model\n        self.load_properties()\n\n        # Computing Survival\n        survival = np.array(self.model.predict_survival(input_data,num_threads))     \n\n        # Computing hazard\n        hazard = np.array(self.model.predict_hazard(input_data, num_threads))  \n\n        # Computing density\n        density  = hazard*survival\n\n        if t is None:\n            return hazard, density, survival\n        else:\n            min_index = [ abs(a_j_1-t) for (a_j_1, a_j) in self.time_buckets]\n            index = np.argmin(min_index)\n            return hazard[:, index], density[:, index], survival[:, index]\n\n\n    def predict_risk(self, X, num_threads=-1):\n\n        # Checking if the data has the right format\n        X = utils.check_data(X)\n        if X.ndim == 1:\n            X = X.reshape(1,-1)\n        T = np.array([1.]*X.shape[0])\n        E = np.array([1.]*X.shape[0])\n        input_data = np.c_[T, E, X]\n\n        # Loading the attributes of the model\n        self.load_properties()\n\n        # Computing risk\n        risk = self.model.predict_risk(input_data, num_threads)\n        return np.array(risk)\n\n\n    def save_properties(self):\n        """""" Loading the properties of the model """"""\n\n        self.times                     = self.model.unique_timepoints\n        self.num_trees                 = self.model.num_trees\n        self.chf                       = self.model.chf\n        self.is_ordered                = self.model.is_ordered\n        self.split_varIDs              = self.model.split_varIDs\n        self.split_values              = self.model.split_values\n        self.child_nodeIDs             = self.model.child_nodeIDs\n        self.status_varID              = self.model.status_varID\n        self.overall_prediction_error  = self.model.overall_prediction_error\n        self.dependent_varID           = self.model.dependent_varID\n        self.min_node_size             = self.model.min_node_size\n        self.variable_importance_      = self.model.variable_importance\n        self.mtry                      = self.model.mtry\n        self.num_independent_variables = self.model.num_independent_variables\n        self.variable_names            = self.model.variable_names\n\n\n    def load_properties(self):\n        """""" Loading the properties of the model """"""\n\n        self.model.unique_timepoints         = self.times\n        self.model.num_trees                 = self.num_trees\n        self.model.chf                       = self.chf\n        self.model.is_ordered                = self.is_ordered\n        self.model.split_varIDs              = self.split_varIDs\n        self.model.split_values              = self.split_values\n        self.model.child_nodeIDs             = self.child_nodeIDs\n        self.model.status_varID              = self.status_varID\n        self.model.overall_prediction_error  = self.overall_prediction_error\n        self.model.dependent_varID           = self.dependent_varID\n        self.model.min_node_size             = self.min_node_size\n        self.model.variable_importance       = self.variable_importance_\n        self.model.mtry                      = self.mtry\n        self.model.num_independent_variables = self.num_independent_variables\n        self.model.variable_names            = self.variable_names\n\n\nclass RandomSurvivalForestModel(BaseSurvivalForest):\n\n    def __init__(self, num_trees = 10):\n        super(RandomSurvivalForestModel, self).__init__(""logrank"", num_trees)\n\n    def fit( self, X, T, E, max_features = \'sqrt\', max_depth = 5, \n            min_node_size = 10, num_threads = -1, weights = None, \n            sample_size_pct = 0.63, importance_mode = \'normalized_permutation\', \n            seed = None, save_memory=False ):\n\n        return super(RandomSurvivalForestModel, self).fit(X=X, T=T, E=E, \n            max_features=max_features, max_depth=max_depth, weights = weights, \n            min_node_size=min_node_size, num_threads=num_threads, \n            sample_size_pct = sample_size_pct, seed = seed, \n            save_memory=save_memory, importance_mode = importance_mode)\n\n\nclass ExtraSurvivalTreesModel(BaseSurvivalForest):\n\n    def __init__(self, num_trees = 10):\n        super(ExtraSurvivalTreesModel, self).__init__(""extratrees"", num_trees)\n\n    def fit( self, X, T, E, max_features = \'sqrt\', max_depth = 5, \n            min_node_size = 10, num_threads = -1, weights = None, \n            sample_size_pct = 0.63,  num_random_splits = 100, \n            importance_mode = \'normalized_permutation\', \n            seed = None, save_memory=False ):\n\n        return super(ExtraSurvivalTreesModel, self).fit(X=X, T=T, E=E, \n            max_features=max_features, max_depth=max_depth, weights = weights, \n            min_node_size=min_node_size, num_threads=num_threads, \n            sample_size_pct = sample_size_pct, seed = seed, \n            num_random_splits = num_random_splits, save_memory=save_memory,\n            importance_mode = importance_mode)\n\n\nclass ConditionalSurvivalForestModel(BaseSurvivalForest):\n\n    def __init__(self, num_trees = 10):\n        super(ConditionalSurvivalForestModel, self).__init__(""maxstat"", num_trees)\n\n    def fit( self, X, T, E, max_features = \'sqrt\', max_depth = 5, \n            min_node_size = 10, num_threads = -1, weights = None, \n            sample_size_pct = 0.63, alpha = 0.5, minprop=0.1,\n            importance_mode = \'normalized_permutation\', seed = None, \n            save_memory=False ):\n\n        return super(ConditionalSurvivalForestModel, self).fit(X=X, T=T, E=E, \n            max_features = max_features, max_depth = max_depth, \n            min_node_size = min_node_size, num_threads = num_threads, \n            weights = weights, sample_size_pct = sample_size_pct, \n            alpha = alpha, minprop=minprop, importance_mode = importance_mode, \n            seed = seed, save_memory=save_memory )\n'"
pysurvival/models/svm.py,17,"b'from __future__ import absolute_import\nimport torch \nimport numpy as np\nimport pandas as pd\nimport scipy\nimport os\nimport copy\nfrom pysurvival import utils\nfrom pysurvival.utils import optimization as opt\nfrom pysurvival.models import BaseModel\nfrom pysurvival.models._svm import _SVMModel\n\n\n# Available Kernel functions\nKERNELS = { \'Linear\': 0, \'Polynomial\': 1, \'Gaussian\':2, \'Normal\':2,\n            \'Exponential\':3, \'Tanh\':4, \'Sigmoid\': 5, \'Rational Quadratic\':6,\n            \'Inverse Multiquadratic\': 7,  \'Multiquadratic\': 8}\n\nREVERSE_KERNELS = {value:key for (key, value) in KERNELS.items() }\n\n\n\nclass SurvivalSVMModel(BaseModel):\n    """""" Survival Support Vector Machine model:\n        --------------------------------------\n\n        The purpose of the model is to help us look at Survival Analysis \n        as a Ranking Problem.\n        Indeed, the idea behind formulating the survival problem as a ranking \n        problem is that in some applications, like clinical applications, \n        one is only interested in defining risks groups, and not the prediction \n        of the survival time, but in whether the unit has a high or low risk for \n        the event to occur. \n\n        The current implementation is based on the ""Rank Support Vector Machines \n        (RankSVMs)"" developed by Van Belle et al. This allows us to compute a \n        convex quadratic loss function, so that we can use the Newton \n        optimization to minimize it.\n\n        References:\n        * Fast Training of Support Vector Machines for Survival Analysis\n          from Sebastian Posterl, Nassir Navab, and Amin Katouzian\n          https://link.springer.com/chapter/10.1007/978-3-319-23525-7_15\n        * An Efficient Training Algorithm for Kernel Survival Support Vector \n          Machines from Sebastian Posterl, Nassir Navab, and Amin Katouzian\n          https://arxiv.org/abs/1611.07054\n        * Support vector machines for survival analysis.\n          Van Belle, V., Pelckmans, K., Suykens, J.A., Van Huffel, S.\n          ftp://ftp.esat.kuleuven.be/sista/kpelckma/kp07-70.pdf\n\n\n        Parameters:\n        -----------\n        * kernel: str (default=""linear"")\n            The type of kernel used to fit the model. Here\'s the list\n            of available kernels:\n            \n            * linear\n            * polynomial\n            * gaussian\n            * exponential\n            * tanh\n            * sigmoid\n            * rational_quadratic\n            * inverse_multiquadratic\n            * multiquadratic\n\n        * scale: float (default=1)\n            Scale parameter of the kernel function\n            \n        * offset: float (default=0)\n            Offset parameter of the kernel function\n            \n        * degree: float (default=1)\n            Degree parameter of the polynomial/kernel function\n\n    """"""\n\n\n    def __init__(self, kernel = ""linear"", scale=1., offset=0., degree=1.,\n      auto_scaler = True):\n\n\n        # Ensuring that the provided kernel is available\n        valid_kernel = [key for key in KERNELS.keys() \\\n        if kernel.lower().replace(\'_\', \' \') in key.lower().replace(\'_\', \' \')]\n\n        if len(valid_kernel) == 0:\n            raise NotImplementedError(\'{} is not a valid kernel function.\'\n                .format(kernel))\n        else:\n            kernel_type = KERNELS[valid_kernel[0]]\n            kernel = valid_kernel[0]\n\n        # Checking the kernel parameters\n        if not (degree >= 0. and \\\n          (isinstance(degree, float) or isinstance(degree, int)) ):\n            error = ""degree parameter is not valid. degree is a >= 0 value""\n            raise ValueError(error)\n\n        if not (isinstance(scale, float) or isinstance(scale, int)):\n            error = ""scale parameter is not valid.""\n            raise ValueError(error)\n\n        if not (isinstance(offset, float) or isinstance(offset, int)):\n            error = ""offset parameter is not valid.""\n            raise ValueError(error)\n\n        # Saving the attributes\n        self.kernel = kernel\n        self.kernel_type = kernel_type\n        self.scale = scale\n        self.offset = offset\n        self.degree = degree\n\n        # Initializing the C++ object\n        self.model = _SVMModel( self.kernel_type, self.scale, self.offset, \n          self.degree)\n\n        # Initializing the elements from BaseModel\n        super(SurvivalSVMModel, self).__init__(auto_scaler)\n\n        \n\n    def __repr__(self):\n        """""" Creates the representation of the Object """"""\n\n        self.name = self.__class__.__name__\n        if \'kernel\' in self.name :\n            self.name += ""(kernel: \'{}\'"".format(self.kernel) + \')\'\n        return self.name\n\n\n    def save(self, path_file):\n        """""" Save the model paremeters of the model (.params) and Compress \n            them into a zip file\n        """"""\n\n        # Ensuring the file has the proper name\n        folder_name = os.path.dirname(path_file) + \'/\'\n        file_name = os.path.basename(path_file)\n\n        # Checking if the folder is accessible\n        if not os.access(folder_name, os.W_OK):\n            error_msg = \'{} is not an accessible directory.\'.format(folder_name)\n            raise OSError(error_msg)\n\n        # Delete the C++ object before saving\n        del self.model\n        \n        # Saving the model\n        super(SurvivalSVMModel, self).save(path_file)\n\n        # Re-introduce the C++ object\n        self.model = _SVMModel( self.kernel_type, self.scale, self.offset, \n          self.degree)\n        self.load_properties()\n        \n\n    def load(self, path_file):\n        """""" Load the model parameters from a zip file into a C++ external\n            model \n        """"""        \n        # Loading the model\n        super(SurvivalSVMModel, self).load(path_file)\n\n        # Re-introduce the C++ object\n        self.model = _SVMModel( self.kernel_type, self.scale, self.offset, \n          self.degree)\n        self.load_properties()\n\n\n    def fit(self, X, T, E, with_bias = True, init_method=\'glorot_normal\', \n            lr = 1e-2, max_iter = 100, l2_reg = 1e-4, tol = 1e-3, \n            verbose = True):\n        """"""\n        Fitting a Survival Support Vector Machine model.\n\n        As the Hessian matrix of the log-likelihood can be \n        calculated without too much effort, the model parameters are \n        computed using the Newton_Raphson Optimization scheme:\n                W_new = W_old - lr*<Hessian^(-1), gradient>\n\n        Arguments:\n        ---------\n        \n        * `X` : array-like, shape=(n_samples, n_features)\n            The input samples.\n\n        * `T` : array-like, shape = [n_samples] \n            The target values describing when the event of interest or censoring\n            occurred\n\n        * `E` : array-like, shape = [n_samples] \n            The Event indicator array such that E = 1. if the event occurred\n            E = 0. if censoring occurred\n\n        * `with_bias`: bool (default=True)\n            Whether a bias should be added \n\n        * `init_method` : str (default = \'glorot_uniform\')\n            Initialization method to use. Here are the possible options:\n                * \'glorot_uniform\': Glorot/Xavier uniform initializer, \n                * \'he_uniform\': He uniform variance scaling initializer\n                * \'uniform\': Initializing tensors with uniform (-1, 1) distribution\n                * \'glorot_normal\': Glorot normal initializer,\n                * \'he_normal\': He normal initializer.\n                * \'normal\': Initializing tensors with standard normal distribution\n                * \'ones\': Initializing tensors to 1\n                * \'zeros\': Initializing tensors to 0\n                * \'orthogonal\': Initializing tensors with a orthogonal matrix,\n\n        * `lr`: float (default=1e-4)\n            learning rate used in the optimization\n\n        * `max_iter`: int (default=100)\n            The maximum number of iterations in the Newton optimization\n\n        * `l2_reg`: float (default=1e-4)\n            L2 regularization parameter for the model coefficients\n\n        * `alpha`: float (default=0.95)\n            Confidence interval\n\n        * `tol`: float (default=1e-3)\n            Tolerance for stopping criteria\n\n        * `verbose`: bool (default=True)\n            Whether or not producing detailed logging about the modeling\n\n\n        Example:\n        --------\n\n        #### 1 - Importing packages\n        import numpy as np\n        import pandas as pd\n        from pysurvival.models.svm import LinearSVMModel\n        from pysurvival.models.svm import KernelSVMModel\n        from pysurvival.models.simulations import SimulationModel\n        from pysurvival.utils.metrics import concordance_index\n        from sklearn.model_selection import train_test_split\n        from scipy.stats.stats import pearsonr   \n        # %pylab inline # to use in jupyter notebooks\n\n        #### 2 - Generating the dataset from the parametric model\n        # Initializing the simulation model\n        sim = SimulationModel( survival_distribution = \'Log-Logistic\',  \n                               risk_type = \'linear\',\n                               censored_parameter = 1.1, \n                               alpha = 1.5, beta = 4)\n\n        # Generating N Random samples\n        N = 1000\n        dataset = sim.generate_data(num_samples = N, num_features = 4)\n\n        #### 3 - Splitting the dataset into training and testing sets\n        # Defining the features\n        features = sim.features\n\n        # Building training and testing sets #\n        index_train, index_test = train_test_split( range(N), test_size = 0.2)\n        data_train = dataset.loc[index_train].reset_index( drop = True )\n        data_test  = dataset.loc[index_test].reset_index( drop = True )\n\n        # Creating the X, T and E input\n        X_train, X_test = data_train[features], data_test[features]\n        T_train, T_test = data_train[\'time\'].values, data_test[\'time\'].values\n        E_train, E_test = data_train[\'event\'].values, data_test[\'event\'].values\n\n\n        #### 4 - Creating an instance of the SVM model and fitting the data.\n        svm_model = LinearSVMModel()\n        svm_model = KernelSVMModel(kernel=\'Gaussian\', scale=0.25)\n        svm_model.fit(X_train, T_train, E_train, init_method=\'he_uniform\', \n            with_bias = True, lr = 0.5,  tol = 1e-3,  l2_reg = 1e-3)\n\n        #### 5 - Cross Validation / Model Performances\n        c_index = concordance_index(svm_model, X_test, T_test, E_test) #0.93\n        print(\'C-index: {:.2f}\'.format(c_index))\n\n        #### 6 - Comparing the model predictions to Actual risk score\n        # Comparing risk scores\n        svm_risks = svm_model.predict_risk(X_test)\n        actual_risks = sim.predict_risk(X_test).flatten()\n        print(""corr={:.4f}, p_value={:.5f}"".format(*pearsonr(svm_risks, \n            actual_risks)))# corr=-0.9992, p_value=0.00000\n\n        """"""\n\n        # Collecting features names\n        N, self.num_vars = X.shape\n        if isinstance(X, pd.DataFrame):\n            self.variables = X.columns.tolist()\n        else:\n            self.variables = [\'x_{}\'.format(i) for i in range(self.num_vars)]\n        \n        # Adding a bias or not\n        self.with_bias = with_bias\n        if with_bias:\n            self.variables += [\'intercept\']\n        p = int(self.num_vars + 1.*with_bias)\n\n        # Checking the format of the data \n        X, T, E = utils.check_data(X, T, E)\n\n        if with_bias:\n            # Adding the intercept\n            X = np.c_[X, [1.]*N]\n        X = self.scaler.fit_transform( X )\n\n        # Initializing the parameters \n        if self.kernel_type == 0:\n            W = np.zeros((p, 1))\n        else:\n            W = np.zeros((N, 1))\n        W = opt.initialization(init_method, W, False).flatten()\n        W = W.astype(np.float64)\n        \n        # Optimizing to find best parameters \n        self.model.newton_optimization(X, T, E, W, lr, l2_reg, \n                                 tol, max_iter, verbose)\n        self.save_properties()\n\n        return self\n\n\n    def save_properties(self):\n        """""" Loading the properties of the model """"""\n\n        self.weights        = np.array( self.model.W )\n        self.Kernel_Matrix  = np.array( self.model.Kernel_Matrix )\n        self.kernel_type    =           self.model.kernel_type \n        self.scale          =           self.model.scale \n        self.offset         =           self.model.offset \n        self.degree         =           self.model.degree \n        self.loss           = np.array( self.model.loss )\n        self.inv_Hessian    = np.array( self.model.inv_Hessian )\n        self.loss_values    = np.array( self.model.loss_values )\n        self.grad2_values   = np.array( self.model.grad2_values )\n        self.internal_X     = np.array( self.model.internal_X )\n\n\n    def load_properties(self):\n        """""" Loading the properties of the model """"""\n\n        self.model.W              = self.weights\n        self.model.Kernel_Matrix  = self.Kernel_Matrix\n        self.model.kernel_type    = self.kernel_type\n        self.model.scale          = self.scale\n        self.model.offset         = self.offset\n        self.model.degree         = self.degree\n        self.model.loss           = self.loss\n        self.model.inv_Hessian    = self.inv_Hessian\n        self.model.loss_values    = self.loss_values\n        self.model.grad2_values   = self.grad2_values\n        self.model.internal_X     = self.internal_X \n        self.kernel               = REVERSE_KERNELS[self.kernel_type]\n\n\n    def predict_risk(self, x, use_log = False):\n        """""" Predicts the Risk Score\n        \n            Parameter\n            ----------\n            * `x`, np.ndarray\n                 array-like representing the datapoints\n\n            * `use_log`: bool - (default=False)\n                Applies the log function to the risk values\n\n            Returns\n            -------\n            * `risk_score`, np.ndarray\n                array-like representing the prediction of Risk Score function\n        """"""        \n\n        # Ensuring that the C++ model has the fitted parameters\n        self.load_properties()\n\n        # Convert x into the right format\n        x = utils.check_data(x)\n\n        # Scaling the dataset\n        if x.ndim == 1:\n            if self.with_bias:\n                x = np.r_[x, 1.]\n            x = self.scaler.transform( x.reshape(1, -1) )\n        elif x.ndim == 2:\n            n = x.shape[0]\n            if self.with_bias:\n                x = np.c_[x, [1.]*n]\n            x = self.scaler.transform( x )\n\n        # Calculating prdiction\n        risk = np.exp( self.model.get_score(x) )\n\n        if use_log:\n            return np.log( risk )\n        else:\n            return risk\n\n\n\n    def predict_cumulative_hazard(self, *args, **kargs):\n        raise NotImplementedError(self.not_implemented_error)\n\n\n    def predict_cdf(self, *args, **kargs):\n        raise NotImplementedError(self.not_implemented_error)\n\n\n    def predict_survival(self, *args, **kargs):\n        raise NotImplementedError(self.not_implemented_error)\n\n\n    def predict_density(self, *args, **kargs):\n        raise NotImplementedError(self.not_implemented_error)\n\n\n    def predict_hazard(self, *args, **kargs):\n        raise NotImplementedError(self.not_implemented_error)\n\n\n\nclass LinearSVMModel(SurvivalSVMModel):\n\n    def __init__(self, auto_scaler = True):\n\n        super(LinearSVMModel, self).__init__(kernel = ""linear"", scale=1., \n            offset=0., degree=1., auto_scaler = True)\n\n\n\nclass KernelSVMModel(SurvivalSVMModel):\n\n    def __init__(self, kernel = ""gaussian"", scale=1., offset=0., degree=1.,\n      auto_scaler = True):\n\n        if ""linear"" in kernel.lower():\n            error = ""To use a \'linear\' svm model, create an instance of""\n            error += ""pysurvival.models.svm.LinearSVMModel""\n            raise ValueError(error)\n\n        super(KernelSVMModel, self).__init__(kernel = kernel, scale=scale, \n            offset=offset, degree=degree, auto_scaler = auto_scaler)\n'"
pysurvival/utils/__init__.py,7,"b'import copy\nimport numpy as np\nimport pandas as pd\nfrom pysurvival.utils._functions import _logrankScores\n# %matplotlib inline for Jupyter notebooks \n\n\ndef as_bytes(string_array, python_version=3):\n    """""" Transforming an array of string into an array of bytes in Python 3 \n    """"""\n\n    if python_version >= 3:\n        results = []\n        for s in string_array:\n            # results.append( codecs.latin_1_encode(s)[0] )\n            results.append( s.encode(\'utf-8\') )\n\n    else:\n        results = string_array\n\n    return results\n    \n\ndef check_data(*args):\n    """""" Makes sure that the given inputs are numpy arrays, list or \n        pandas DataFrames.\n\n        Parameters\n        ----------\n        * args : tuple of objects\n                 Input object to check / convert.\n\n        Returns\n        -------\n        * result : tuple of numpy arrays\n                   The converted and validated arg.\n\n        If the input isn\'t numpy arrays, list or pandas DataFrames, it will\n        fail and ask to provide the valid format.\n    """"""\n\n    result = ()\n    for i, arg in enumerate(args):\n\n        if len(arg) == 0 :\n            error = "" The input is empty. ""\n            error += ""Please provide at least 1 element in the array.""\n            raise IndexError(error)         \n\n        else:\n\n            if isinstance(arg, np.ndarray) :\n                x = ( arg.astype(np.double),  )\n            elif isinstance(arg, list):\n                x = ( np.asarray(arg).astype(np.double),  )\n            elif isinstance(arg, pd.Series):\n                x = ( arg.values.astype(np.double),  )\n            elif isinstance(arg, pd.DataFrame):\n                x = ( arg.values.astype(np.double),  )\n            else:\n                \n                error = ""{arg} is not a valid data format. ""\n                error += ""Only use \'list\', \'np.ndarray\', ""\n                error += ""\'pd.Series\' ""\n                error += ""\'pd.DataFrame\' "".format(arg=type(arg))\n                raise TypeError(error)\n            \n            if np.sum( np.isnan(x) ) > 0. :\n                error = ""The #{} argument contains null values""\n                error = error.format(i+1)\n                raise ValueError(error)\n\n            if len(args) > 1:\n                result += x\n            else:\n                result = x[0]\n\n    return result               \n\n\ndef as_bytes(string_array, python_version=3):\n    """""" Transforming an array of string into an array of bytes in Python 3 \n    """"""\n\n    if python_version >= 3:\n        results = []\n        for s in string_array:\n            # results.append( codecs.latin_1_encode(s)[0] )\n            results.append( s.encode(\'utf-8\') )\n    else:\n        results = string_array\n\n    return results\n\n\ndef rank_scores(T, E):\n    """""" \n    Computing the ranks for each survival time T\n\n        Parameters:\n        -----------\n\n        * T : array-like, shape = [n_samples] \n            The target values describing when the event of interest or censoring\n            occured\n\n        * E : array-like, shape = [n_samples] \n            The Event indicator array such that E = 1. if the event occured\n            E = 0. if censoring occured\n\n        Returns:\n        --------\n        * rank_scores : array-like\n            ranks for each survival time T\n    """"""\n    T, E = check_data(T, E)\n    return _logrankScores(T,E)\n\n\ndef save_model(model, path_file):\n    """""" Save the model and its parameters, and compress them into a zip file \n\n    Parameters:\n    -----------\n    * model : Pysurvival object\n        Pysurvival model\n\n    * path_file, str\n        address of the file where the model will be loaded from \n    """"""\n\n    model.save(path_file)\n\n\n\n\ndef load_model(path_file):\n    """""" Load the model and its parameters from a .zip file \n\n    Parameters:\n    -----------\n    * path_file, str\n        address of the file where the model will be loaded from \n\n    Returns:\n    --------\n    * pysurvival_model : Pysurvival object\n        Pysurvival model\n    """"""\n\n    # Initializing a base model\n    from pysurvival.models import BaseModel\n    base_model = BaseModel()\n\n    # Temporary loading the model\n    base_model.load(path_file)\n    model_name = base_model.name\n\n    # Loading the actual Pysurvival model - Kaplan-Meier\n    if \'kaplanmeier\' in model_name.lower():\n\n        if \'smooth\' in model_name.lower():\n            from pysurvival.models.non_parametric import SmoothKaplanMeierModel\n            pysurvival_model = SmoothKaplanMeierModel()\n\n        else:\n            from pysurvival.models.non_parametric import KaplanMeierModel\n            pysurvival_model = KaplanMeierModel()\n\n\n    elif \'linearmultitask\' in model_name.lower():\n\n        from pysurvival.models.multi_task import LinearMultiTaskModel\n        pysurvival_model = LinearMultiTaskModel()\n\n\n    elif \'neuralmultitask\' in model_name.lower():\n\n        from pysurvival.models.multi_task import NeuralMultiTaskModel\n        structure = [ {\'activation\': \'relu\', \'num_units\': 128}, ]         \n        pysurvival_model = NeuralMultiTaskModel(structure=structure)\n\n\n    elif \'exponential\' in model_name.lower():\n\n        from pysurvival.models.parametric import ExponentialModel\n        pysurvival_model = ExponentialModel()\n\n\n    elif \'weibull\' in model_name.lower():\n\n        from pysurvival.models.parametric import WeibullModel\n        pysurvival_model = WeibullModel()\n\n\n    elif \'gompertz\' in model_name.lower():\n\n        from pysurvival.models.parametric import GompertzModel\n        pysurvival_model = GompertzModel()\n\n\n    elif \'loglogistic\' in model_name.lower():\n\n        from pysurvival.models.parametric import LogLogisticModel\n        pysurvival_model = LogLogisticModel()\n\n\n    elif \'lognormal\' in model_name.lower():\n\n        from pysurvival.models.parametric import LogNormalModel\n        pysurvival_model = LogNormalModel()\n\n\n    elif \'simulation\' in model_name.lower():\n\n        from pysurvival.models.simulations import SimulationModel\n        pysurvival_model = SimulationModel()\n\n\n    elif \'coxph\' in model_name.lower():\n\n        if \'nonlinear\' in model_name.lower():\n            from pysurvival.models.semi_parametric import NonLinearCoxPHModel\n            pysurvival_model = NonLinearCoxPHModel()\n\n        else:\n            from pysurvival.models.semi_parametric import CoxPHModel\n            pysurvival_model = CoxPHModel()\n\n\n    elif \'random\' in model_name.lower() and \'survival\' in model_name.lower():\n\n        from pysurvival.models.survival_forest import RandomSurvivalForestModel\n        pysurvival_model = RandomSurvivalForestModel()\n\n\n    elif \'extra\' in model_name.lower() and \'survival\' in model_name.lower():\n\n        from pysurvival.models.survival_forest import ExtraSurvivalTreesModel\n        pysurvival_model = ExtraSurvivalTreesModel()\n\n\n    elif \'condi\' in model_name.lower() and \'survival\' in model_name.lower():\n\n        from pysurvival.models.survival_forest import ConditionalSurvivalForestModel\n        pysurvival_model = ConditionalSurvivalForestModel()\n\n\n    elif \'svm\' in model_name.lower() :\n\n        if \'linear\' in model_name.lower():\n\n            from pysurvival.models.svm import LinearSVMModel\n            pysurvival_model = LinearSVMModel()\n\n        elif \'kernel\' in model_name.lower():\n\n            from pysurvival.models.svm import KernelSVMModel\n            pysurvival_model = KernelSVMModel()\n\n    else:\n        raise NotImplementedError(\'{} is not a valid pysurvival model.\'\n            .format(model_name))\n\n    # Transferring the components\n    pysurvival_model.__dict__.update(copy.deepcopy(base_model.__dict__))\n    del base_model\n\n    return pysurvival_model\n'"
pysurvival/utils/display.py,9,"b'import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm as cm\nfrom matplotlib import colors\nfrom matplotlib.patches import Rectangle\nfrom pysurvival import utils\nfrom pysurvival.utils import metrics\nfrom pysurvival.utils.metrics import brier_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import median_absolute_error\nfrom pysurvival.models.non_parametric import KaplanMeierModel\n\n\ndef display_loss_values(model, figure_size = (18, 5)):\n    """""" Display the loss function values of the model fitting \n\n        Parameters:\n        -----------\n        * model : pysurvival model\n            The model that will be used for prediction\n\n        * figure_size: tuple of double (default= (18, 5))\n            width, height in inches representing the size of the chart \n    """"""\n\n    # Check that the model is not a Non-Parametric model\n    if \'kaplan\' in model.name.lower() :\n        error = ""This function cannot only take as input a Non-Parametric model""\n        raise NotImplementedError(error)\n\n    if \'simulation\' in model.name.lower() :\n        error = ""This function cannot only take as input a simulation model""\n        raise NotImplementedError(error)\n\n    # Extracting the loss values\n    loss_values = model.loss_values\n\n    # Extracting the norm 2 of the gradient, if it exists\n    grad2_values = model.__dict__.get(\'grad2_values\')\n    if grad2_values is None:\n        order = 1\n    else :\n        order = 2\n   \n    # Displaying the loss values bsed on the type of optimization\n    if order == 1:\n        title = ""Loss function values""\n        fig, ax = plt.subplots(figsize=figure_size)\n        ax.plot( loss_values, color = \'blue\',  label = \'Loss values\')\n        ax.set_xlabel( \'Number of epochs\', fontsize=10)\n        plt.legend(fontsize=10)\n        plt.title(title, fontsize=10)\n        plt.show()\n\n    elif order == 2:\n\n        # Initializing Chart \n        fig = plt.figure( figsize=figure_size )\n        fig.suptitle( \'Loss function $l$ and $|| gradient ||_{L_{2}}$\', \n                     fontsize=12, fontweight=\'bold\')\n\n        # Plotting loss function\n        ax1 = fig.add_subplot(111)\n        ax1.set_xlabel(\'epochs\' )\n        ax1.set_ylabel(\'Loss function $l$\')\n        pl1 = ax1.plot( range( len(loss_values)) , loss_values, \n                  label = \'Loss function $l$\', color = \'blue\', linestyle = \'--\')\n\n        # Plotting ||grad|| values\n        ax2 = ax1.twinx()\n        pl2 = ax2.plot( range( len(grad2_values) ) , grad2_values ,\n                  label = \'$|| gradient ||_{L_{2}}$\', color = \'red\')\n        ax2.set_ylabel(\'$|| gradient ||_{L_{2}}$\')\n\n        # added these three lines\n        pl = pl1 + pl2\n        labs = [l.get_label() for l in pl]\n        ax1.legend(pl, labs, loc=1)\n\n        # display chart\n        plt.show() \n        \n\n\ndef display_non_parametric(km_model, figure_size = (18, 5) ):\n    """""" Plotting the survival function and its lower and upper bounds \n\n        Parameters:\n        -----------\n        * km_model : pysurvival Non-Parametric model\n            The model that will be used for prediction\n\n        * figure_size: tuple of double (default= (18, 5))\n            width, height in inches representing the size of the chart \n    """"""\n\n    # Check that the model is a Non-Parametric model\n    if \'kaplan\' not in km_model.name.lower() :\n        error = ""This function can only take as input a Non-Parametric model""\n        raise NotImplementedError(error)\n\n    # Title of the chart\n    if \'smooth\' in km_model.name.lower() :\n        is_smoothed = True\n        title = \'Smooth Kaplan-Meier Survival function\'\n    else:\n        is_smoothed = False\n        title = \'Kaplan-Meier Survival function\'\n\n    # Initializing the chart\n    fig, ax = plt.subplots(figsize=figure_size )\n\n    # Extracting times and survival function\n    times, survival = km_model.times, km_model.survival\n\n    # Plotting Survival\n    plt.plot(times, survival, label = title, \n             color = \'blue\', lw = 3)  \n\n    # Defining the x-axis and y-axis\n    ax.set_xlabel(\'Time\')\n    ax.set_ylabel( \'S(t) Survival function\' )\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlim([0.0, max(times)*1.01])\n    vals = ax.get_yticks()\n    ax.set_yticklabels([\'{:.1f}%\'.format(v*100) for v in vals])\n    plt.title(title, fontsize=25)\n\n    # Extracting times and survival function\n    times, survival = km_model.times, km_model.survival\n\n    if is_smoothed :\n\n        # Display\n        plt.plot(times, survival, label = \'Original Kaplan-Meier\', \n                 color = \'#f44141\', ls = \'-.\', lw = 2.5)        \n        plt.legend(fontsize=15)\n        plt.show()\n\n    else:\n\n        # Extracting CI\n        survival_ci_upper = km_model.survival_ci_upper\n        survival_ci_lower = km_model.survival_ci_lower\n\n        # Plotting the Confidence Intervals\n        plt.plot(times, survival_ci_upper, \n                 color=\'red\', alpha =0.1, ls=\'--\')\n        plt.plot(times, survival_ci_lower, \n                 color=\'red\', alpha =0.1, ls=\'--\')\n\n        # Filling the areas between the Survival and Confidence Intervals curves\n        plt.fill_between(times, survival, survival_ci_lower, \n                label=\'Confidence Interval - lower\', color=\'red\', alpha =0.2)\n        plt.fill_between(times, survival, survival_ci_upper, \n                label=\'Confidence Interval - upper\', color=\'red\', alpha =0.2)\n        \n        # Display\n        plt.legend(fontsize=15)\n        plt.show()\n\n\n\ndef display_baseline_simulations(sim_model, figure_size=(18, 5)):\n    """""" Display Simulation model baseline \n\n        Parameters:\n        -----------\n        * sim_model : pysurvival Simulations model\n            The model that will be used for prediction\n\n        * figure_size: tuple of double (default= (18, 5))\n            width, height in inches representing the size of the chart \n    """"""\n\n    # Check that the model is a Non-Parametric model\n    if \'simulation\' not in sim_model.name.lower() :\n        error = ""This function can only take as input a Non-Parametric model""\n        raise NotImplementedError(error)\n\n    # Extracting parameters\n    name_survival_distribution = sim_model.survival_distribution\n    times = sim_model.times\n    baseline_survival = sim_model.baseline_survival\n    title = \'Base Survival function - \' + name_survival_distribution.title()\n\n    # Display\n    fig, ax = plt.subplots(figsize=figure_size)\n    ax.plot( times, baseline_survival, color = \'blue\', label = \'baseline\')\n    plt.legend()\n    plt.title(title)\n    plt.show()\n\n\n\ndef integrated_brier_score(model, X, T, E, t_max=None, use_mean_point=True,\n    figure_size=(20, 6.5)):\n    """""" The Integrated Brier Score (IBS) provides an overall calculation of \n        the model performance at all available times.\n    """"""\n\n    # Computing the brier scores\n    times, brier_scores = brier_score(model, X, T, E, t_max, use_mean_point)\n\n    # Getting the proper value of t_max\n    if t_max is None:\n        t_max = max(times)\n    else:\n        t_max = min(t_max, max(times))\n\n    # Computing the IBS\n    ibs_value = np.trapz(brier_scores, times)/t_max \n\n    # Displaying the Brier Scores at different t \n    title = \'Prediction error curve with IBS(t = {:.1f}) = {:.2f}\'\n    title = title.format(t_max, ibs_value)\n    fig, ax = plt.subplots(figsize=figure_size)\n    ax.plot( times, brier_scores, color = \'blue\', lw = 3)\n    ax.set_xlim(-0.01, max(times))\n    ax.axhline(y=0.25, ls = \'--\', color = \'red\')\n    ax.text(0.90*max(times), 0.235, \'0.25 limit\', fontsize=20, color=\'brown\', \n        fontweight=\'bold\')\n    plt.title(title, fontsize=20)\n    plt.show()\n\n    return ibs_value\n\n\n\ndef compare_to_actual(model, X, T, E, times = None, is_at_risk = False,  \n    figure_size=(16, 6), metrics = [\'rmse\', \'mean\', \'median\'], **kwargs):\n    """"""\n    Comparing the actual and predicted number of units at risk and units \n    experiencing an event at each time t.\n\n    Parameters:\n    -----------\n    * model : pysurvival model\n        The model that will be used for prediction\n\n    * X : array-like, shape=(n_samples, n_features)\n        The input samples.\n\n    * T : array-like, shape = [n_samples] \n        The target values describing when the event of interest or censoring\n        occured\n\n    * E : array-like, shape = [n_samples] \n        The Event indicator array such that E = 1. if the event occured\n        E = 0. if censoring occured\n\n    * times: array-like, (default=None)\n        A vector of timepoints.\n\n    * is_at_risk: bool (default=True)\n        Whether the function returns Expected number of units at risk\n        or the Expected number of units experiencing the events.\n\n    * figure_size: tuple of double (default= (16, 6))\n        width, height in inches representing the size of the chart \n\n    * metrics: str or list of str (default=\'all\')\n        Indicates the performance metrics to compute:\n            - if None, then no metric is computed\n            - if str, then the metric is computed\n            - if list of str, then the metrics are computed\n\n        The available metrics are:\n            - RMSE: root mean squared error\n            - Mean Abs Error: mean absolute error\n            - Median Abs Error: median absolute error\n\n    Returns:\n    --------\n    * results: float or dict\n        Performance metrics   \n\n    """"""\n\n    # Initializing the Kaplan-Meier model\n    X, T, E = utils.check_data(X, T, E)\n    kmf = KaplanMeierModel()\n    kmf.fit(T, E)\n\n    # Creating actual vs predicted\n    N = T.shape[0]\n\n    # Defining the time axis\n    if times is None:\n        times = kmf.times\n\n    # Number of Expected number of units at risk\n    # or the Expected number of units experiencing the events\n    actual = []\n    actual_upper = []\n    actual_lower = []\n    predicted = []\n    if is_at_risk:\n        model_predicted =  np.sum(model.predict_survival(X, **kwargs), 0)\n\n        for t in times:\n            min_index = [ abs(a_j_1-t) for (a_j_1, a_j) in model.time_buckets]\n            index = np.argmin(min_index)\n            actual.append(N*kmf.predict_survival(t))\n            actual_upper.append(N*kmf.predict_survival_upper(t))\n            actual_lower.append(N*kmf.predict_survival_lower(t))\n            predicted.append( model_predicted[index] )\n\n    else:\n        model_predicted =  np.sum(model.predict_density(X, **kwargs), 0)\n\n        for t in times:\n            min_index = [ abs(a_j_1-t) for (a_j_1, a_j) in model.time_buckets]\n            index = np.argmin(min_index)\n            actual.append(N*kmf.predict_density(t))\n            h = kmf.predict_hazard(t)\n            actual_upper.append(N*kmf.predict_survival_upper(t)*h)\n            actual_lower.append(N*kmf.predict_survival_lower(t)*h)\n            predicted.append( model_predicted[index] )\n\n    # Computing the performance metrics\n    results = None\n    title = \'Actual vs Predicted\'\n    if metrics is not None:\n\n        # RMSE\n        rmse = np.sqrt(mean_squared_error(actual, predicted))\n\n        # Median Abs Error\n        med_ae = median_absolute_error(actual, predicted)\n\n        # Mean Abs Error\n        mae = mean_absolute_error(actual, predicted)\n\n\n        if isinstance(metrics, str) :\n\n            # RMSE\n            if \'rmse\' in metrics.lower() or \'root\' in metrics.lower():\n                results = rmse\n                title += ""\\n""\n                title += ""RMSE = {:.3f}"".format(rmse)\n\n            # Median Abs Error\n            elif \'median\' in metrics.lower() :\n                results = med_ae\n                title += ""\\n""\n                title += ""Median Abs Error = {:.3f}"".format(med_ae)\n\n            # Mean Abs Error\n            elif \'mean\' in metrics.lower() :\n                results = mae\n                title += ""\\n""\n                title += ""Mean Abs Error = {:.3f}"".format(mae)\n\n            else:\n                raise NotImplementedError(\'{} is not a valid metric function.\'\n                .format(metrics))\n\n\n        elif isinstance(metrics, list) or isinstance(metrics, numpy.ndarray) :\n            results = {}\n\n            # RMSE\n            is_rmse = False\n            if any( [ (\'rmse\' in m.lower() or \'root\' in m.lower()) \\\n                for m in metrics ]):\n                is_rmse = True\n                results[\'root_mean_squared_error\'] = rmse\n                title += ""\\n""\n                title += ""RMSE = {:.3f}"".format(rmse)\n\n            # Median Abs Error\n            is_med_ae = False\n            if any( [\'median\' in m.lower() for m in metrics ]):\n                is_med_ae = True\n                results[\'median_absolute_error\'] = med_ae\n                title += ""\\n""\n                title += ""Median Abs Error = {:.3f}"".format(med_ae)\n\n            # Mean Abs Error\n            is_mae = False\n            if any( [\'mean\' in m.lower() for m in metrics ]):\n                is_mae = True\n                results[\'mean_absolute_error\'] = mae\n                title += ""\\n""\n                title += ""Mean Abs Error = {:.3f}"".format(mae)\n\n            if all([not is_mae, not is_rmse, not is_med_ae]):\n                error = \'The provided metrics are not available.\'\n                raise NotImplementedError(error)\n\n    # Plotting\n    fig, ax = plt.subplots(figsize=figure_size)\n    ax.plot(times, actual, color=\'red\', label=\'Actual\', \n        alpha=0.8, lw = 3)\n    ax.plot(times, predicted,color=\'blue\', label=\'Predicted\', \n        alpha=0.8, lw = 3)\n    plt.xlim(0, max(T))\n\n    # Filling the areas between the Survival and Confidence Intervals curves\n    plt.fill_between(times, actual, actual_lower, \n        label=\'Confidence Intervals - Lower\', color=\'red\', alpha =0.2)\n    plt.fill_between(times, actual, actual_upper, \n        label=\'Confidence Intervals - Upper\', color=\'red\', alpha =0.2)\n\n    # Finalizing the chart\n    plt.title(title, fontsize = 15)\n    plt.legend(fontsize = 15)\n    plt.show()\n\n    return results\n\n\ndef create_risk_groups(model, X, use_log = True, num_bins = 50, \n                       figure_size = (20, 8), **kwargs):\n    """"""\n    Computing and displaying the histogram of the risk scores of the given \n    model and test set X. If it is provided args, it will assign a color coding \n    to the scores that are below and above the given thresholds.\n\n    Parameters:\n    -----------\n    \n    * model : Pysurvival object\n        Pysurvival model\n\n    * X : array-like, shape=(n_samples, n_features)\n        The input samples.\n    \n    * use_log: boolean (default=True)\n        Whether applying the log function to the risk score\n        \n    * num_bins: int (default=50)\n        The number of equal-width bins that will constitute the histogram\n        \n    * figure_size: tuple of double (default= (16, 6))\n        width, height in inches representing the size of the chart \n\n    * kwargs: dict (optional)\n        kwargs = low_risk = {\'lower_bound\': 0, \'upper_bound\': 20, \'color\': \'red\'},\n                 high_risk = {\'lower_bound\': 20, \'upper_bound\': 120, \'color\': \'blue\'}\n            that define the risk group\n      \n    """"""\n\n    # Ensuring that the input data has the right format\n    X = utils.check_data(X)\n\n    # Computing the risk scores\n    risk = model.predict_risk(X)\n    if use_log:\n        risk = np.log(risk)\n        \n    # Displaying simple histogram\n    if len(kwargs) == 0:\n        \n        # Initializing the chart\n        fig, ax1 = plt.subplots(figsize=figure_size)\n        risk_groups = None\n    \n    # Applying any color coding\n    else:\n        # Initializing the results\n        risk_groups = {}\n        \n        # Initializing the chart\n        fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=figure_size)\n        \n        # Displaying simple histogram with risk groups\n        nums_per_bins, bins, patches = ax2.hist(risk, bins=num_bins)\n        ax2.set_title(\'Risk groups with colors\', fontsize=15)\n\n        # Number of group definitions\n        num_group_def = len(kwargs.values())\n\n        # Extracting the bounds values\n        bounds = {}\n        colors_ = {}\n        indexes = {}\n        group_names = []\n        handles = []\n\n        # we need to check that the boundaries match the bins\n        is_not_valid = 0\n        for group_name, group_def in kwargs.items():\n\n            # by ensuring that the bounds are not outside\n            # the bins values\n            min_bin, max_bin = min(bins), max(bins)\n            if (group_def[\'lower_bound\'] < min_bin and \\\n                group_def[\'upper_bound\'] < min_bin) or \\\n               (group_def[\'lower_bound\'] > max_bin and \\\n                group_def[\'upper_bound\'] > max_bin) :\n                is_not_valid += 1\n\n            # Extracting the bounds\n            bounds[group_name] = (group_def[\'lower_bound\'], \n                                  group_def[\'upper_bound\'])\n\n            # Extracting the colors\n            colors_[group_name] = group_def[\'color\']\n\n            # Creating index placeholders\n            indexes[group_name] = []\n            group_names.append( group_name )\n            color_indv = group_def[\'color\']\n            handles.append(Rectangle((0,0),1,1, color=color_indv, ec=""k""))\n\n        if is_not_valid >= num_group_def :\n            error_msg =  ""The boundaries definitions {} do not match""\n            error_msg += "", the values of the risk scores.""\n            error_msg = error_msg.format(list(bounds.values()))\n            raise ValueError(error_msg)\n\n        # Assigning each rectangle/bin to its group definition\n        # and color\n        colored_patches = []\n        bin_index = {}\n        for i, bin_, patch_ in zip(range(num_bins), bins, patches):\n\n            # Check if the bin belongs to this bound def\n            for grp_name, bounds_ in bounds.items():\n\n                if bounds_[0] <= bin_ < bounds_[-1] :\n                    bin_index[i] = grp_name\n                    \n                    # Extracting color\n                    color_ = colors_[grp_name]\n                    if color_ not in colors.CSS4_COLORS :\n                        error_msg = \'{} is not a valid color\'\n                        error_msg = error_msg.format(colors_[grp_name])\n                        raise ValueError(error_msg)\n                        \n                    patch_.set_facecolor( color_ )\n\n            # Saving the rectangles\n            colored_patches.append(patch_)\n\n        # Assigning each sample to its group\n        risk_bins = np.minimum(np.digitize(risk, bins, True), num_bins-1) \n        for i, r in enumerate(risk_bins):\n\n            # Extracting the right group_name\n            group_name = bin_index[r]\n            indexes[group_name].append(i)\n            \n\n    # Displaying the original distribution\n    ax1.hist(risk, bins=num_bins, color = \'black\', alpha=0.5)  \n    ax1.set_title(\'Risk Score Distribution\', fontsize=15)\n\n    # Show everything\n    plt.show()\n    \n    # Returning results\n    if risk_groups is not None:\n        for group_name in group_names:\n            result = (colors_[group_name], indexes[group_name])            \n            risk_groups[group_name] = result\n\n    return risk_groups\n\n\n\ndef correlation_matrix(df, figure_size=(12, 8), text_fontsize = 10):\n    """""" Takes dataframe and display the correlations between features """"""\n\n    # Computing correlations\n    corr = df.corr()\n    \n    # Display the correlations using heat map\n    fig, ax1 = plt.subplots(figsize=figure_size)\n    cmap = cm.get_cmap(\'RdYlBu\', 20) #\'rainbow\', 20)\n    cax = ax1.imshow(corr, interpolation=""nearest"", cmap=cmap, \n                     vmax=1,vmin=-1)\n    \n    # Add values in the cells\n    for x in range(corr.shape[0]):\n        for y in range(corr.shape[1]):\n            \n            if x == y:\n                color = \'white\'\n            else:\n                color = \'black\'\n                \n            plt.text(x , y , \'%.2f\' % corr.values[y, x], \n                     horizontalalignment=\'center\',\n                     verticalalignment=\'center\',\n                     color = color,\n                     fontsize = text_fontsize,\n                     )\n    \n    # Reformat the x/y axis\n    labels=df.columns\n    ax1.set_xticks(range(len(labels)))\n    ax1.set_xticklabels(labels,fontsize=16)\n    ax1.set_yticks(range(len(labels)))\n    ax1.set_yticklabels(labels,fontsize=16)\n    plt.xticks(rotation=80)\n    \n    # Add colorbar, specify tick locations to match desired ticklabels\n    fig.colorbar(cax, ticks=np.arange(-1.1, 1.1, 0.1))\n    plt.show()\n'"
pysurvival/utils/metrics.py,8,"b'from __future__ import absolute_import\nimport numpy as np\nimport pandas as pd\nimport scipy\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.metrics import median_absolute_error\nfrom pysurvival.models.non_parametric import KaplanMeierModel\nfrom pysurvival.utils._metrics import _concordance_index\nfrom pysurvival.utils._metrics import _brier_score, _timeROC\nfrom pysurvival import utils\n\n\ndef concordance_index(model, X, T, E, include_ties = True, \n    additional_results=False, **kwargs):\n    """""" \n    Computing the C-index based on *On The C-Statistics For Evaluating Overall\n    Adequacy Of Risk Prediction Procedures With Censored Survival Data* and\n    *Estimating the Concordance Probability in a Survival Analysis\n    with a Discrete Number of Risk Groups* and *Concordance for Survival \n    Time Data: Fixed and Time-Dependent Covariates and Possible Ties in\n    Predictor and Time \n\n    Similarly to the AUC, C-index = 1 corresponds to the best model \n    prediction, and C-index = 0.5 represents a random prediction.\n\n    Parameters:\n    -----------\n    * model : Pysurvival object\n        Pysurvival model\n\n    * X : array-like, shape=(n_samples, n_features)\n        The input samples.\n\n    * E : array-like, shape = [n_samples] \n        The Event indicator array such that E = 1. if the event occured\n        E = 0. if censoring occured\n    \n    * include_ties: bool (default=True)\n        Specifies whether ties in risk score are included in calculations\n\n    * additional_results: bool (default=False)\n        Specifies whether only the c-index should be returned (False)\n        or if a dict of values should returned. the values are:\n            - c-index\n            - nb_pairs\n            - nb_concordant_pairs\n\n    Returns:\n    --------\n        * results: double or dict (if additional_results = True)\n            - results is the c-index (double) if additional_results = False\n            - results is dict if additional_results = True such that\n                results[0] = C-index;\n                results[1] = nb_pairs;\n                results[2] = nb_concordant_pairs;\n                    \n    Example:\n    --------\n\n\n    """"""\n\n    # Checking the format of the data \n    risk = model.predict_risk(X, **kwargs)\n    risk, T, E = utils.check_data(risk, T, E)\n\n    # Ordering risk, T and E in descending order according to T\n    order = np.argsort(-T)\n    risk = risk[order]\n    T = T[order]\n    E = E[order]\n\n    # Calculating th c-index\n    results = _concordance_index(risk, T, E, include_ties)\n\n    if not additional_results:\n        return results[0]\n    else:\n        return results\n\n\ndef c_index(model, X, T, E, include_ties = True, additional_results=False):\n    return concordance_index(model, X, T, E, include_ties, additional_results)\n\n\ndef brier_score(model, X, T, E, t_max=None, use_mean_point=True, **kwargs):\n    """""" \n    Computing the Brier score at all times t such that t <= t_max;\n    it represents the average squared distances between \n    the observed survival status and the predicted\n    survival probability.\n\n    In the case of right censoring, it is necessary to adjust\n    the score by weighting the squared distances to \n    avoid bias. It can be achieved by using \n    the inverse probability of censoring weights method (IPCW),\n    (proposed by Graf et al. 1999; Gerds and Schumacher 2006)\n    by using the estimator of the conditional survival function\n    of the censoring times calculated using the Kaplan-Meier method,\n    such that :\n    BS(t) = 1/N*( W_1(t)*(Y_1(t) - S_1(t))^2 + ... + \n                  W_N(t)*(Y_N(t) - S_N(t))^2)\n\n    In terms of benchmarks, a useful model will have a Brier score below \n    0.25. Indeed, it is easy to see that if for all i in [1,N], \n    if S(t, xi) = 0.5, then BS(t) = 0.25.\n\n    Parameters:\n    -----------\n    * model : Pysurvival object\n        Pysurvival model\n\n    * X : array-like, shape=(n_samples, n_features)\n        The input samples.\n\n    * T : array-like, shape = [n_samples] \n        The target values describing when the event of interest or censoring\n        occured\n\n    * E : array-like, shape = [n_samples] \n        The Event indicator array such that E = 1. if the event occured\n        E = 0. if censoring occured\n    \n    * t_max: float \n        Maximal time for estimating the prediction error curves. \n        If missing the largest value of the response variable is used.\n\n    Returns:\n    --------\n        * (times, brier_scores):tuple of arrays\n            -times represents the time axis at which the brier scores were \n              computed\n            - brier_scores represents the values of the brier scores\n                    \n    Example:\n    --------\n\n\n    """"""\n    # Checking the format of the data \n    T, E = utils.check_data(T, E)\n\n    # computing the Survival function\n    Survival = model.predict_survival(X, None, **kwargs)\n\n    # Extracting the time buckets\n    times = model.times\n    time_buckets = model.time_buckets\n\n    # Ordering Survival, T and E in descending order according to T\n    order = np.argsort(-T)\n    Survival = Survival[order, :]\n    T = T[order]\n    E = E[order]\n\n    if t_max is None or t_max <= 0.:\n        t_max = max(T)\n\n    # Calculating the brier scores at each t <= t_max\n    results = _brier_score(Survival, T, E, t_max, times, time_buckets,\n        use_mean_point)\n    times = results[0] \n    brier_scores = results[1] \n\n    return (times, brier_scores)\n\n\ndef integrated_brier_score(model, X, T, E, t_max=None, use_mean_point=True):\n    """""" The Integrated Brier Score (IBS) provides an overall calculation of \n        the model performance at all available times.\n    """"""\n\n    # Computing the brier scores\n    times, brier_scores = brier_score(model, X, T, E, t_max, use_mean_point)\n\n    # Getting the proper value of t_max\n    if t_max is None:\n        t_max = max(times)\n    else:\n        t_max = min(t_max, max(times))\n\n    # Computing the IBS\n    ibs_value = np.trapz(brier_scores, times)/t_max \n\n    return ibs_value\n\n\ndef ibs(model, X, T, E, t_max=None, use_mean_point = True):\n    return integrated_brier_score(model, X, T, E, t_max, use_mean_point)\n\n\n\n\ndef compare_to_actual(model, X, T, E, times = None, is_at_risk = False,  \n    figsize=(16, 6), metrics = [\'rmse\', \'mean\', \'median\'], **kwargs):\n    """"""\n    Comparing the actual and predicted number of units at risk and units \n    experiencing an event at each time t.\n\n    Parameters:\n    -----------\n    * model : pysurvival model\n        The model that will be used for prediction\n\n    * X : array-like, shape=(n_samples, n_features)\n        The input samples.\n\n    * T : array-like, shape = [n_samples] \n        The target values describing when the event of interest or censoring\n        occured\n\n    * E : array-like, shape = [n_samples] \n        The Event indicator array such that E = 1. if the event occured\n        E = 0. if censoring occured\n\n    * times: array-like, (default=None)\n        A vector of timepoints.\n\n    * is_at_risk: bool (default=True)\n        Whether the function returns Expected number of units at risk\n        or the Expected number of units experiencing the events.\n\n    * figure_size: tuple of double (default= (16, 6))\n        width, height in inches representing the size of the chart \n\n    * metrics: str or list of str (default=\'all\')\n        Indicates the performance metrics to compute:\n            - if None, then no metric is computed\n            - if str, then the metric is computed\n            - if list of str, then the metrics are computed\n\n        The available metrics are:\n            - RMSE: root mean squared error\n            - Mean Abs Error: mean absolute error\n            - Median Abs Error: median absolute error\n\n    Returns:\n    --------\n    * results: float or dict\n        Performance metrics   \n\n    """"""\n\n    # Initializing the Kaplan-Meier model\n    X, T, E = utils.check_data(X, T, E)\n    kmf = KaplanMeierModel()\n    kmf.fit(T, E)\n\n    # Creating actual vs predicted\n    N = T.shape[0]\n\n    # Defining the time axis\n    if times is None:\n        times = kmf.times\n\n    # Number of Expected number of units at risk\n    # or the Expected number of units experiencing the events\n    actual = []\n    actual_upper = []\n    actual_lower = []\n    predicted = []\n    if is_at_risk:\n        model_predicted =  np.sum(model.predict_survival(X, **kwargs), 0)\n\n        for t in times:\n            min_index = [ abs(a_j_1-t) for (a_j_1, a_j) in model.time_buckets]\n            index = np.argmin(min_index)\n            actual.append(N*kmf.predict_survival(t))\n            actual_upper.append(N*kmf.predict_survival_upper(t))\n            actual_lower.append(N*kmf.predict_survival_lower(t))\n            predicted.append( model_predicted[index] )\n\n    else:\n        model_predicted =  np.sum(model.predict_density(X, **kwargs), 0)\n\n        for t in times:\n            min_index = [ abs(a_j_1-t) for (a_j_1, a_j) in model.time_buckets]\n            index = np.argmin(min_index)\n            actual.append(N*kmf.predict_density(t))\n            h = kmf.predict_hazard(t)\n            actual_upper.append(N*kmf.predict_survival_upper(t)*h)\n            actual_lower.append(N*kmf.predict_survival_lower(t)*h)\n            predicted.append( model_predicted[index] )\n\n    # Computing the performance metrics\n    results = None\n    if metrics is not None:\n\n        # RMSE\n        rmse = np.sqrt(mean_squared_error(actual, predicted))\n\n        # Median Abs Error\n        med_ae = median_absolute_error(actual, predicted)\n\n        # Mean Abs Error\n        mae = mean_absolute_error(actual, predicted)\n\n\n        if isinstance(metrics, str) :\n\n            # RMSE\n            if \'rmse\' in metrics.lower() or \'root\' in metrics.lower():\n                results = rmse\n                # title += ""\\n""\n                # title += ""RMSE = {:.3f}"".format(rmse)\n\n            # Median Abs Error\n            elif \'median\' in metrics.lower() :\n                results = med_ae\n                # title += ""\\n""\n                # title += ""Median Abs Error = {:.3f}"".format(med_ae)\n\n            # Mean Abs Error\n            elif \'mean\' in metrics.lower() :\n                results = mae\n                # title += ""\\n""\n                # title += ""Mean Abs Error = {:.3f}"".format(mae)\n\n            else:\n                raise NotImplementedError(\'{} is not a valid metric function.\'\n                .format(metrics))\n\n\n        elif isinstance(metrics, list) or isinstance(metrics, numpy.ndarray) :\n            results = {}\n\n            # RMSE\n            is_rmse = False\n            if any( [ (\'rmse\' in m.lower() or \'root\' in m.lower()) \\\n                for m in metrics ]):\n                is_rmse = True\n                results[\'root_mean_squared_error\'] = rmse\n                # title += ""\\n""\n                # title += ""RMSE = {:.3f}"".format(rmse)\n\n            # Median Abs Error\n            is_med_ae = False\n            if any( [\'median\' in m.lower() for m in metrics ]):\n                is_med_ae = True\n                results[\'median_absolute_error\'] = rmse\n                # title += ""\\n""\n                # title += ""Median Abs Error = {:.3f}"".format(med_ae)\n\n            # Mean Abs Error\n            is_mae = False\n            if any( [\'mean\' in m.lower() for m in metrics ]):\n                is_mae = True\n                results[\'mean_absolute_error\'] = rmse\n                # title += ""\\n""\n                # title += ""Mean Abs Error = {:.3f}"".format(mae)\n\n            if all([not is_mae, not is_rmse, not is_med_ae]):\n                error = \'The provided metrics are not available.\'\n                raise NotImplementedError(error)\n                \n    return results'"
pysurvival/utils/neural_networks.py,2,"b'import torch \nimport torch.nn as nn\nimport numpy as np\nimport pysurvival.utils.optimization as opt\n\n# --------------------------- Activation Functions --------------------------- #\nclass Swish(nn.Module):\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\nclass Gaussian(nn.Module):\n    def forward(self, x):\n        return torch.exp(- x*x/2.)\n    \nclass Atan(nn.Module):\n    def forward(self, x):\n        return torch.atan(x)\n\nclass InverseSqrt(nn.Module):\n    def forward(self, x, alpha=1.):\n        return x/torch.sqrt(1.+alpha*x*x)\n    \nclass Sinc(nn.Module):\n    def forward(self, x, epsilon=1e-9):\n        return torch.sin(x+epsilon)/(x+epsilon)\n    \nclass SinReLU(nn.Module):\n    def forward(self, x):\n        return torch.sin(x)+torch.relu(x)\n\nclass CosReLU(nn.Module):\n    def forward(self, x):\n        return torch.cos(x)+torch.relu(x)\n\nclass LeCunTanh(nn.Module):\n    def forward(self, x):\n        return 1.7159*torch.tanh(2./3*x)\n           \nclass LogLog(nn.Module):\n    def forward(self, x):\n        return 1.-torch.exp(-torch.exp(x))\n    \nclass BipolarSigmoid(nn.Module):\n    def forward(self, x):\n        return (1.-torch.exp(-x))/(1.+torch.exp(-x)) \n    \nclass BentIdentity(nn.Module):\n    def forward(self, x, alpha=1.):\n        return x + (torch.sqrt(1.+ x*x)- 1.)/2.\n\nclass Identity(nn.Module):\n    def forward(self, x):\n        return x \n\nclass Softmax(nn.Module):\n    def forward(self, x):\n        y = torch.exp(x)\n        return y/torch.sum(y, dim=0)\n    \ndef activation_function(activation, alpha=1., return_text=False):\n    """""" Returns the activation function object used by the network """"""\n    \n    if activation.lower() == \'atan\':\n        if return_text :\n            return \'Atan\'\n        else:\n            return Atan()\n    \n    elif activation.lower().startswith(\'bent\'):\n        if return_text :\n            return \'BentIdentity\'\n        else:\n            return BentIdentity()\n\n    elif activation.lower().startswith(\'bipolar\'):\n        if return_text :\n            return \'BipolarSigmoid\'\n        else:\n            return BipolarSigmoid()\n    \n    elif activation.lower().startswith(\'cosrelu\'):\n        if return_text :\n            return \'CosReLU\'\n        else:\n            return CosReLU()    \n    \n    elif activation.lower() == \'elu\':\n        if return_text :\n            return \'ELU\'\n        else:\n            return nn.ELU(alpha=alpha)\n    \n    elif activation.lower() == \'gaussian\':\n        if return_text :\n            return \'Gaussian\'\n        else:\n            return Gaussian()\n        \n    elif activation.lower() == \'hardtanh\':\n        if return_text :\n            return \'Hardtanh\'\n        else:\n            return nn.Hardtanh()\n    \n    elif activation.lower() == \'identity\':\n        if return_text :\n            return \'Identity\'\n        else:\n            return Identity()\n    \n    elif activation.lower().startswith(\'inverse\'):\n        if return_text :\n            return \'InverseSqrt\'\n        else:\n            return InverseSqrt()    \n        \n    elif activation.lower() == \'leakyrelu\':\n        if return_text :\n            return \'LeakyReLU\'\n        else:\n            return nn.LeakyReLU()\n    \n    elif activation.lower().startswith(\'lecun\'):\n        if return_text :\n            return \'LeCunTanh\'\n        else:\n            return LeCunTanh()    \n    \n    elif activation.lower() == \'loglog\':\n        if return_text :\n            return \'LogLog\'\n        else:\n            return LogLog()\n    \n    elif activation.lower() == \'logsigmoid\':\n        if return_text :\n            return \'LogSigmoid\'\n        else:\n            return nn.LogSigmoid()    \n    \n    elif activation.lower() == \'relu\':\n        if return_text :\n            return \'ReLU\'\n        else:\n            return nn.ReLU()\n        \n    elif activation.lower() == \'selu\':\n        if return_text :\n            return \'SELU\'\n        else:\n            return nn.SELU()\n    \n    elif activation.lower() == \'sigmoid\':\n        if return_text :\n            return \'Sigmoid\'\n        else:\n            return nn.Sigmoid()\n    \n    elif activation.lower() == \'sinc\':\n        if return_text :\n            return \'Sinc\'\n        else:\n            return Sinc()\n    \n    elif activation.lower().startswith(\'sinrelu\'):\n        if return_text :\n            return \'SinReLU\'\n        else:\n            return SinReLU()    \n    \n    elif activation.lower() == \'softmax\':\n        if return_text :\n            return \'Softmax\'\n        else:\n            return Softmax()\n        \n    elif activation.lower() == \'softplus\':\n        if return_text :\n            return \'Softplus\'\n        else:\n            return nn.Softplus()\n        \n    elif activation.lower() == \'softsign\':\n        if return_text :\n            return \'Softsign\'\n        else:\n            return nn.Softsign()\n    \n    elif activation.lower() == \'swish\':\n        if return_text :\n            return \'Swish\'\n        else:\n            return Swish()\n        \n    elif activation.lower() == \'tanh\':\n        if return_text :\n            return \'Tanh\'\n        else:\n            return nn.Tanh()\n\n    else:\n        error = ""{} function isn\'t implemented"".format(activation)\n        raise NotImplementedError(error)\n\n\n\ndef check_mlp_structure(structure):\n    """""" Checking that the given MLP structure is valid """"""\n\n\n    # Checking if structure is dict\n    if isinstance(structure, dict):\n        structure = [structure]\n\n    # Checking the keys \n    results = []\n    for inner_structure in structure:\n\n        # Checking the validity of activation\n        activation = inner_structure.get(\'activation\')\n        if activation is None:\n            error = \'An activation function needs to be provided \' \n            error +=\'using the key ""activation""\'\n            raise KeyError(error)\n\n        else:\n            activation = activation_function(activation, return_text=True)\n            inner_structure[\'activation\'] = activation\n\n        # Checking the validity of num_units\n        num_units = inner_structure.get(\'num_units\')\n        if num_units is None:\n            error = \'The number of hidden units needs to be provided \' \n            error +=\'using the key ""num_units""\'\n            raise KeyError(error)\n\n        else:\n            if not isinstance(num_units, int):\n                error = \'num_units in {} needs to be a integer\'\n                error = error.format(inner_structure)\n                raise TypeError(error)\n            else:\n                inner_structure[\'num_units\'] = num_units\n\n        results.append(inner_structure)\n\n    return results\n\n\n# ----------------------------- MLP Object ----------------------------- #\nclass NeuralNet(nn.Module):\n    """""" Defines a Multilayer Perceptron (MLP) that consists in \n        * an input layer,\n        * at least one fully connected neural layer (or hidden layer)\n        * and an output layer\n\n    Parameters:\n    -----------\n    * input_size: int\n        Dimension of the input tensor\n    * output_size: int\n        Size of the output layer\n    * structure: None or list of dictionnaries\n        Provides the structure of the MLP built within the N-MTLR\n        If None, then the model becomes the Linear MTLR\n        ex: structure = [ {\'activation\': \'relu\', \'num_units\': 128}, \n                          {\'activation\': \'tanh\', \'num_units\': 128}, ] \n        Here are the possible activation functions:\n            * Atan\n            * BentIdentity\n            * BipolarSigmoid\n            * CosReLU\n            * ELU\n            * Gaussian\n            * Hardtanh\n            * Identity\n            * InverseSqrt\n            * LeakyReLU\n            * LeCunTanh\n            * LogLog\n            * LogSigmoid\n            * ReLU\n            * SELU\n            * Sigmoid\n            * Sinc\n            * SinReLU\n            * Softmax\n            * Softplus\n            * Softsign\n            * Swish\n            * Tanh\n    * init_method: str\n        Defines the type of initializer to use\n    * dropout: double (default=None)\n        Randomly sets a fraction rate of input units to 0 \n        at each update during training time, which helps prevent overfitting.\n    * batch_normalization: bool (default=True)\n        Applying Batch Normalization or not\n    * bn_and_droupout: bool (default=False)\n        Applying Batch Normalization and Dropout at the same time\n\n\n    Note about Dropout and Batch Normalization:\n    ------------------------------------------\n    As a rule, the dropout Layer and Batch Normalization (BN) shouldn\'t be used \n    together according to : https://arxiv.org/pdf/1801.05134.pdf\n\n    * Dropout is used to Prevent Neural Networks from Overfitting\n      should appears after the activation according to : \n      https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n\n    * Batch Normalization can Accelerate Deep Network Training by Reducing \n      Internal Covariate Shift BN should appear after Fully connected but \n      before activation according to : https://arxiv.org/pdf/1502.03167.pdf\n\n    """"""    \n    def __init__(self, input_size, output_size, structure, init_method\n        , dropout=None, batch_normalization = True, bn_and_droupout = False):\n\n        # Initializing the model\n        super(NeuralNet, self).__init__()\n\n        # Initializing the list of layers\n        self.layers = []\n\n        if structure is not None and structure != []:\n\n            # Checking if structure is dict\n            if isinstance(structure, dict):\n                structure = [structure]\n\n            # Building the hidden layers\n            for hidden in structure:\n\n                # Extracting the hidden layer parameters \n                hidden_size = int(hidden.get(\'num_units\'))\n                activation  = hidden.get(\'activation\')\n                alpha       = hidden.get(\'alpha\')\n\n                # Fully connected layer\n                fully_conn = nn.Linear(input_size, hidden_size) \n                fully_conn.weight = opt.initialization(init_method, \n                    fully_conn.weight)\n                fully_conn.bias = opt.initialization(init_method, \n                    fully_conn.bias)\n                self.layers.append( fully_conn )\n                \n                if not bn_and_droupout:\n                    # Batch Normalization\n                    if batch_normalization:\n                        self.layers.append( torch.nn.BatchNorm1d(hidden_size) )\n\n                    # Activation\n                    self.layers.append( activation_function(activation, \n                        alpha=alpha) )\n                    \n                    # Dropout\n                    if (dropout is not None or 0. < dropout <= 1.) and \\\n                    not batch_normalization :\n                        self.layers.append( torch.nn.Dropout(dropout) )\n\n                else:\n                    # Batch Normalization\n                    if batch_normalization:\n                        self.layers.append( torch.nn.BatchNorm1d(hidden_size) )\n\n                    # Activation\n                    self.layers.append( activation_function(activation, \n                        alpha=alpha) )\n                    \n                    # Dropout\n                    if (dropout is not None or 0. < dropout <= 1.) :\n                        self.layers.append( torch.nn.Dropout(dropout) )\n\n                # Next layer\n                input_size = hidden_size\n\n        # Fully connected last layer\n        fully_conn = nn.Linear(input_size, output_size) \n        fully_conn.weight = opt.initialization(init_method, fully_conn.weight)\n        fully_conn.bias = opt.initialization(init_method, fully_conn.bias)\n        self.layers.append( fully_conn )\n\n        # Putting the model together \n        self.model = nn.Sequential(*self.layers).train()\n\n\n    def forward(self, x):\n\n        out = self.model(x)\n        return out\n\n\n        \nclass ParametricNet(torch.nn.Module):\n    """""" Underlying Pytorch model powering the Parametric models """"""\n\n    def __init__(self, num_features, init_method, init_alpha=1., \n        is_beta_used = True):\n        super(ParametricNet, self).__init__()\n\n        # weights\n        W = torch.randn(num_features, 1) \n        self.W = opt.initialization(init_method, W)\n\n        one =  torch.FloatTensor(np.array([1]))/init_alpha\n        self.alpha = torch.nn.Parameter( one ) \n\n        self.is_beta_used = is_beta_used\n        if self.is_beta_used:\n            one =  torch.FloatTensor(np.array([1.001]))/init_alpha\n            self.beta = torch.nn.Parameter( one ) \n\n    def forward(self, x):\n        score =  self.alpha*torch.exp(torch.matmul(x, self.W))\n        return score\n\n'"
pysurvival/utils/optimization.py,1,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport progressbar\nimport time\nimport copy\n\ndef initialization(init_method, W, is_tensor=True):\n    """""" Initializes the provided tensor. \n    \n    Parameters:\n    -----------\n\n    * init_method : str (default = \'glorot_uniform\')\n        Initialization method to use. Here are the possible options:\n            * \'glorot_uniform\': Glorot/Xavier uniform initializer, \n                Glorot & Bengio, AISTATS 2010 \n                http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\n            * \'he_uniform\': He uniform variance scaling initializer\n               He et al., http://arxiv.org/abs/1502.01852\n            * \'uniform\': Initializing tensors with uniform (-1, 1) distribution\n            * \'glorot_normal\': Glorot normal initializer,\n            * \'he_normal\': He normal initializer.\n            * \'normal\': Initializing tensors with standard normal distribution\n            * \'ones\': Initializing tensors to 1\n            * \'zeros\': Initializing tensors to 0\n            * \'orthogonal\': Initializing tensors with an orthogonal matrix,\n\n    * W: torch.Tensor\n        Corresponds to the Torch tensor\n\n      """"""\n\n    # Checking the dimensions\n    is_one_dim = False\n\n    # Checking if the parameters is a tensor, if not transform it into one\n    if not is_tensor:\n        W = torch.FloatTensor(W)\n\n    # Creating a column vector if one dimensional tensor\n    if len(W.shape)==1:\n        is_one_dim = True\n        W = torch.reshape(W, (1, -1))\n\n    # Initializing the weights\n    if init_method.lower() == \'uniform\':\n        W = nn.init.uniform_(W)\n        \n    elif init_method.lower() == \'normal\':\n        W = nn.init.normal_(W)\n        \n    elif init_method.lower().startswith(\'one\'):\n        W = nn.init.ones_(W)        \n        \n    elif init_method.lower().startswith(\'zero\'):\n        W = nn.init.zeros_(W)   \n\n    elif init_method.lower().startswith(\'ortho\'):\n        W = nn.init.orthogonal_(W) \n        \n    elif init_method.lower().startswith(\'glorot\') or \\\n    init_method.lower().startswith(\'xav\'):\n        \n        if init_method.lower().endswith(\'uniform\'):\n            W = nn.init.xavier_uniform_(W)\n        elif init_method.lower().endswith(\'normal\'):\n            W = nn.init.xavier_normal_(W)\n            \n    elif init_method.lower().startswith(\'he\') or \\\n    init_method.lower().startswith(\'kaiming\'):\n        \n        if init_method.lower().endswith(\'uniform\'):\n            W = nn.init.kaiming_uniform_(W)\n        elif init_method.lower().endswith(\'normal\'):\n            W = nn.init.kaiming_normal_(W)\n\n    else:\n        error = "" {} isn\'t implemented"".format(init_method)\n        raise NotImplementedError(error)\n        \n    # Returning a PyTorch tensor\n    if is_tensor:\n        if is_one_dim:\n            return torch.nn.Parameter(W.flatten())\n        else:\n            return torch.nn.Parameter(W)\n\n    # Returning a Numpy array\n    else:\n        if is_one_dim:\n            return W.data.numpy().flatten()\n        else:\n            return W.data.numpy()      \n        \n        \n\n\ndef optimize(loss_function, model, optimizer_str, lr=1e-4, nb_epochs=1000, \n               verbose = True, num_workers = 0, **kargs):\n    """""" \n    Providing the schema of the iterative method for optimizing a \n    differentiable objective function for models that use gradient centric\n    schemas (a.k.a order 1 optimization)\n\n    Parameters:\n    -----------\n        * loss_function: function\n            Loss function of the model\n\n        * model: torch object\n            Actual model to optimize\n\n        * optimizer_str: str \n            Defines the type of optimizer to use. Here are the possible options:\n                - adadelta\n                - adagrad\n                - adam\n                - adamax\n                - rmsprop\n                - sparseadam\n                - sgd\n\n        * lr: float (default=1e-4)\n            learning reate used in the optimization\n\n        * nb_epochs: int (default=1000)\n            The number of iterations in the optimization\n\n        * verbose: bool (default=True)\n            Whether or not producing detailed logging about the modeling\n    """"""\n\n    # Choosing an optimizer\n    W = model.parameters()\n    if optimizer_str.lower() == \'adadelta\':\n        optimizer = torch.optim.Adadelta(W, lr=lr) \n        \n    elif optimizer_str.lower() == \'adagrad\':\n        optimizer = torch.optim.Adagrad(W, lr=lr) \n    \n    elif optimizer_str.lower() == \'adam\':\n        optimizer = torch.optim.Adam(W, lr=lr) \n    \n    elif optimizer_str.lower() == \'adamax\':\n        optimizer = torch.optim.Adamax(W, lr=lr)     \n    \n    elif optimizer_str.lower() == \'rmsprop\':\n        optimizer = torch.optim.RMSprop(W, lr=lr)  \n    \n    elif optimizer_str.lower() == \'sparseadam\':\n        optimizer = torch.optim.SparseAdam(W, lr=lr)  \n    \n    elif optimizer_str.lower() == \'sgd\':\n        optimizer = torch.optim.SGD(W, lr=lr)  \n\n    elif optimizer_str.lower() == \'lbfgs\':\n        optimizer = torch.optim.LBFGS(W, lr=lr)\n    \n    elif optimizer_str.lower() == \'rprop\':\n        optimizer = torch.optim.Rprop(W, lr=lr)\n\n    else:\n        error = ""{} optimizer isn\'t implemented"".format(optimizer_str)\n        raise NotImplementedError(error)\n    \n    # Initializing the Progress Bar\n    loss_values = []\n    if verbose:\n        widgets = [ \'% Completion: \', progressbar.Percentage(), \n                   progressbar.Bar(\'*\'), \'\'] \n        bar = progressbar.ProgressBar(maxval=nb_epochs, widgets=widgets)\n        bar.start()\n\n    # Updating the weights at each training epoch\n    temp_model = None\n    for epoch in range(nb_epochs):\n\n        # Backward pass and optimization\n        def closure():\n            optimizer.zero_grad()\n            loss = loss_function(model, **kargs)\n            loss.backward()\n            return loss\n\n        if \'lbfgs\' in optimizer_str.lower() :\n            optimizer.step(closure)\n        else:\n            optimizer.step()\n        loss = closure()\n        loss_value = loss.item()\n\n        # Printing error message if the gradient didn\'t explode\n        if np.isnan(loss_value) or np.isinf(loss_value):\n            error = ""The gradient exploded... ""\n            error += ""You should reduce the learning""\n            error += ""rate (lr) of your optimizer""\n            if verbose:\n                widgets[-1] = error\n            else:\n                print(error)\n            break\n            \n        # Otherwise, printing value of loss function\n        else:\n            temp_model = copy.deepcopy(model)\n            loss_values.append( loss_value )\n            if verbose:\n                widgets[-1] = ""Loss: {:6.2f}"".format( loss_value )\n\n        # Updating the progressbar\n        if verbose:\n            bar.update( epoch + 1 )\n    \n    # Terminating the progressbar\n    if verbose:\n        bar.finish()\n        \n    # Finilazing the model\n    if temp_model is not None:\n        temp_model = temp_model.eval()\n        model = copy.deepcopy(temp_model)\n    else:\n        raise ValueError(error)\n\n    return model, loss_values\n\n\n'"
