file_path,api_count,code
setup.py,0,"b'# -*- coding: utf-8 -*-\n""""""Setup file.""""""\n\nfrom setuptools import setup\nfrom setuptools import find_packages\n\n\nsetup(name=\'reach\',\n      version=\'3.3.0\',\n      description=\'A light-weight package for working with pre-trained\'\n                  \' word embeddings\',\n      author=\'St\xc3\xa9phan Tulkens\',\n      author_email=\'stephan.tulkens@uantwerpen.be\',\n      url=\'https://github.com/stephantul/reach\',\n      license=\'MIT\',\n      packages=find_packages(exclude=[\'examples\']),\n      install_requires=[\'numpy>=1.11.0\', \'tqdm\', \'setuptools\'],\n      classifiers=[\n          \'Intended Audience :: Developers\',\n          \'Programming Language :: Python :: 2\',\n          \'Programming Language :: Python :: 3\'],\n      keywords=\'word vectors natural language processing\',\n      zip_safe=True)\n'"
reach/__init__.py,0,"b'""""""A package for reading and manipulating word embeddings.""""""\nfrom __future__ import absolute_import\nfrom reach.reach import Reach\n\n__all__ = [\'Reach\']\n'"
reach/reach.py,37,"b'""""""A class for working with vector representations.""""""\nimport logging\nimport json\nimport numpy as np\nimport os\n\nfrom io import open\nfrom tqdm import tqdm\n\nlogger = logging.getLogger(__name__)\n\n\nclass Reach(object):\n    """"""\n    Work with vector representations of items.\n\n    Supports functions for calculating fast batched similarity\n    between items or composite representations of items.\n\n    Parameters\n    ----------\n    vectors : numpy array\n        The vector space.\n    items : list\n        A list of items. Length must be equal to the number of vectors, and\n        aligned with the vectors.\n    name : string, optional, default \'\'\n        A string giving the name of the current reach. Only useful if you\n        have multiple spaces and want to keep track of them.\n    unk_index : int or None, optional, default None\n        The index of the UNK item. If this is None, any attempts at vectorizing\n        OOV items will throw an error.\n\n    Attributes\n    ----------\n    items : dict\n        A mapping from items to ids.\n    indices : dict\n        A mapping from ids to items.\n    vectors : numpy array\n        The array representing the vector space.\n    unk_index : int\n        The integer index of your unknown glyph. This glyph will be inserted\n        into your BoW space whenever an unknown item is encountered.\n    norm_vectors : numpy array\n        A normalized version of the vector space.\n    size : int\n        The dimensionality of the vector space.\n    name : string\n        The name of the Reach instance.\n\n    """"""\n\n    def __init__(self, vectors, items, name="""", unk_index=None):\n        """"""Initialize a Reach instance with an array and list of items.""""""\n        if len(items) != len(vectors):\n            raise ValueError(""Your vector space and list of items are not ""\n                             ""the same length: ""\n                             ""{} != {}"".format(len(vectors), len(items)))\n        if isinstance(items, (dict, set)):\n            raise ValueError(""Your item list is a set or dict, and might not ""\n                             ""retain order in the conversion to internal look""\n                             ""-ups. Please convert it to list and check the ""\n                             ""order."")\n\n        self.items = {w: idx for idx, w in enumerate(items)}\n        self.indices = {v: k for k, v in self.items.items()}\n\n        self.vectors = np.asarray(vectors)\n        self.unk_index = unk_index\n\n        self.name = name\n\n    @property\n    def size(self):\n        return self.vectors.shape[1]\n\n    @property\n    def vectors(self):\n        return self._vectors\n\n    @vectors.setter\n    def vectors(self, x):\n        x = np.array(x)\n        assert np.ndim(x) == 2 and x.shape[0] == len(self.items)\n        self._vectors = x\n        self.norm_vectors = self.normalize(x)\n\n    @staticmethod\n    def load(pathtovector,\n             wordlist=(),\n             num_to_load=None,\n             truncate_embeddings=None,\n             unk_word=None,\n             sep="" "",\n             recover_from_errors=False,\n             **kwargs):\n        r""""""\n        Read a file in word2vec .txt format.\n\n        The load function will raise a ValueError when trying to load items\n        which do not conform to line lengths.\n\n        Parameters\n        ----------\n        pathtovector : string\n            The path to the vector file.\n        header : bool\n            Whether the vector file has a header of the type\n            (NUMBER OF ITEMS, SIZE OF VECTOR).\n        wordlist : iterable, optional, default ()\n            A list of words you want loaded from the vector file. If this is\n            None (default), all words will be loaded.\n        num_to_load : int, optional, default None\n            The number of items to load from the file. Because loading can take\n            some time, it is sometimes useful to onlyl load the first n items\n            from a vector file for quick inspection.\n        truncate_embeddings : int, optional, default None\n            If this value is not None, the vectors in the vector space will\n            be truncated to the number of dimensions indicated by this value.\n        unk_word : object\n            The object to treat as UNK in your vector space. If this is not\n            in your items dictionary after loading, we add it with a zero\n            vector.\n        recover_from_errors : bool\n            If this flag is True, the model will continue after encountering\n            duplicates or other errors.\n\n        Returns\n        -------\n        r : Reach\n            An initialized Reach instance.\n\n        """"""\n        vectors, items = Reach._load(pathtovector,\n                                     wordlist,\n                                     num_to_load,\n                                     truncate_embeddings,\n                                     sep,\n                                     recover_from_errors)\n        if unk_word is not None:\n            if unk_word not in set(items):\n                unk_vec = np.zeros((1, vectors.shape[1]))\n                vectors = np.concatenate([unk_vec, vectors], 0)\n                items = [unk_word] + items\n                unk_index = 0\n            else:\n                unk_index = items.index(unk_word)\n        else:\n            unk_index = None\n\n        return Reach(vectors,\n                     items,\n                     name=os.path.split(pathtovector)[-1],\n                     unk_index=unk_index)\n\n    @staticmethod\n    def _load(pathtovector,\n              wordlist,\n              num_to_load,\n              truncate_embeddings,\n              sep,\n              recover_from_errors):\n        """"""Load a matrix and wordlist from a .vec file.""""""\n        vectors = []\n        addedwords = set()\n        words = []\n\n        try:\n            wordlist = set(wordlist)\n        except ValueError:\n            wordlist = set()\n\n        logger.info(""Loading {0}"".format(pathtovector))\n\n        firstline = open(pathtovector).readline().strip()\n        try:\n            num, size = firstline.split(sep)\n            num, size = int(num), int(size)\n            logger.info(""Vector space: {} by {}"".format(num, size))\n            header = True\n        except ValueError:\n            size = len(firstline.split(sep)) - 1\n            logger.info(""Vector space: {} dim, # items unknown"".format(size))\n            word, rest = firstline.split(sep, 1)\n            # If the first line is correctly parseable, set header to False.\n            header = False\n\n        if truncate_embeddings is None or truncate_embeddings == 0:\n            truncate_embeddings = size\n\n        for idx, line in enumerate(open(pathtovector, encoding=\'utf-8\')):\n\n            if header and idx == 0:\n                continue\n\n            word, rest = line.rstrip("" \\n"").split(sep, 1)\n\n            if wordlist and word not in wordlist:\n                continue\n\n            if word in addedwords:\n                e = """"""Duplicate: {} on line {} was in the\n                    vector space twice"""""".format(word, idx)\n                if recover_from_errors:\n                    print(e)\n                    continue\n                raise ValueError(e)\n\n            if len(rest.split(sep)) != size:\n                e = """"""Incorrect input at index {}, size\n                       is {}, expected {}"""""".format(idx+1,\n                                                    len(rest.split(sep)),\n                                                    size)\n                if recover_from_errors:\n                    print(e)\n                    continue\n                raise ValueError()\n\n            words.append(word)\n            addedwords.add(word)\n            vectors.append(np.fromstring(rest, sep=sep)[:truncate_embeddings])\n\n            if num_to_load is not None and len(addedwords) >= num_to_load:\n                break\n\n        vectors = np.array(vectors).astype(np.float32)\n\n        logger.info(""Loading finished"")\n        if wordlist:\n            diff = wordlist - addedwords\n            if diff:\n                logger.info(""Not all items from your wordlist were in your ""\n                            ""vector space: {}."".format(diff))\n\n        return vectors, words\n\n    def __getitem__(self, item):\n        """"""Get the vector for a single item.""""""\n        return self.vectors[self.items[item]]\n\n    def vectorize(self, tokens, remove_oov=False, norm=False):\n        """"""\n        Vectorize a sentence by replacing all items with their vectors.\n\n        Parameters\n        ----------\n        tokens : object or list of objects\n            The tokens to vectorize.\n        remove_oov : bool, optional, default False\n            Whether to remove OOV items. If False, OOV items are replaced by\n            the UNK glyph. If this is True, the returned sequence might\n            have a different length than the original sequence.\n        norm : bool, optional, default False\n            Whether to return the unit vectors, or the regular vectors.\n\n        Returns\n        -------\n        s : numpy array\n            An M * N matrix, where every item has been replaced by\n            its vector. OOV items are either removed, or replaced\n            by the value of the UNK glyph.\n\n        """"""\n        if not tokens:\n            raise ValueError(""You supplied an empty list."")\n        index = list(self.bow(tokens, remove_oov=remove_oov))\n        if not index:\n            raise ValueError(""You supplied a list with only OOV tokens: {}, ""\n                             ""which then got removed. Set remove_oov to False,""\n                             "" or filter your sentences to remove any in which""\n                             "" all items are OOV."")\n        if norm:\n            return np.stack([self.norm_vectors[x] for x in index])\n        else:\n            return np.stack([self.vectors[x] for x in index])\n\n    def bow(self, tokens, remove_oov=False):\n        """"""\n        Create a bow representation of a list of tokens.\n\n        Parameters\n        ----------\n        tokens : list.\n            The list of items to change into a bag of words representation.\n        remove_oov : bool.\n            Whether to remove OOV items from the input.\n            If this is True, the length of the returned BOW representation\n            might not be the length of the original representation.\n\n        Returns\n        -------\n        bow : generator\n            A BOW representation of the list of items.\n\n        """"""\n        if remove_oov:\n            tokens = [x for x in tokens if x in self.items]\n\n        for t in tokens:\n            try:\n                yield self.items[t]\n            except KeyError:\n                if self.unk_index is None:\n                    raise ValueError(""You supplied OOV items but didn\'t ""\n                                     ""provide the index of the replacement ""\n                                     ""glyph. Either set remove_oov to True, ""\n                                     ""or set unk_index to the index of the ""\n                                     ""item which replaces any OOV items."")\n                yield self.unk_index\n\n    def transform(self, corpus, remove_oov=False, norm=False):\n        """"""\n        Transform a corpus by repeated calls to vectorize, defined above.\n\n        Parameters\n        ----------\n        corpus : A list of strings, list of list of strings.\n            Represents a corpus as a list of sentences, where sentences\n            can either be strings or lists of tokens.\n        remove_oov : bool, optional, default False\n            If True, removes OOV items from the input before vectorization.\n\n        Returns\n        -------\n        c : list\n            A list of numpy arrays, where each array represents the transformed\n            sentence in the original list. The list is guaranteed to be the\n            same length as the input list, but the arrays in the list may be\n            of different lengths, depending on whether remove_oov is True.\n\n        """"""\n        return [self.vectorize(s, remove_oov=remove_oov, norm=norm)\n                for s in corpus]\n\n    def most_similar(self,\n                     items,\n                     num=10,\n                     batch_size=100,\n                     show_progressbar=False,\n                     return_names=True):\n        """"""\n        Return the num most similar items to a given list of items.\n\n        Parameters\n        ----------\n        items : list of objects or a single object.\n            The items to get the most similar items to.\n        num : int, optional, default 10\n            The number of most similar items to retrieve.\n        batch_size : int, optional, default 100.\n            The batch size to use. 100 is a good default option. Increasing\n            the batch size may increase the speed.\n        show_progressbar : bool, optional, default False\n            Whether to show a progressbar.\n        return_names : bool, optional, default True\n            Whether to return the item names, or just the distances.\n\n        Returns\n        -------\n        sim : array\n            For each items in the input the num most similar items are returned\n            in the form of (NAME, DISTANCE) tuples. If return_names is false,\n            the returned list just contains distances.\n\n        """"""\n        # This line allows users to input single items.\n        # We used to rely on string identities, but we now also allow\n        # anything hashable as keys.\n        # Might fail if a list of passed items is also in the vocabulary.\n        # but I can\'t think of cases when this would happen, and what\n        # user expectations are.\n        try:\n            if items in self.items:\n                items = [items]\n        except TypeError:\n            pass\n        x = np.stack([self.norm_vectors[self.items[x]] for x in items])\n\n        result = self._batch(x,\n                             batch_size,\n                             num+1,\n                             show_progressbar,\n                             return_names)\n\n        # list call consumes the generator.\n        return [x[1:] for x in result]\n\n    def threshold(self,\n                  items,\n                  threshold=.5,\n                  batch_size=100,\n                  show_progressbar=False,\n                  return_names=True):\n        """"""\n        Return all items whose similarity is higher than threshold.\n\n        Parameters\n        ----------\n        items : list of objects or a single object.\n            The items to get the most similar items to.\n        threshold : float, optional, default .5\n            The radius within which to retrieve items.\n        batch_size : int, optional, default 100.\n            The batch size to use. 100 is a good default option. Increasing\n            the batch size may increase the speed.\n        show_progressbar : bool, optional, default False\n            Whether to show a progressbar.\n        return_names : bool, optional, default True\n            Whether to return the item names, or just the distances.\n\n        Returns\n        -------\n        sim : array\n            For each items in the input the num most similar items are returned\n            in the form of (NAME, DISTANCE) tuples. If return_names is false,\n            the returned list just contains distances.\n\n        """"""\n        # This line allows users to input single items.\n        # We used to rely on string identities, but we now also allow\n        # anything hashable as keys.\n        # Might fail if a list of passed items is also in the vocabulary.\n        # but I can\'t think of cases when this would happen, and what\n        # user expectations are.\n        try:\n            if items in self.items:\n                items = [items]\n        except TypeError:\n            pass\n        x = np.stack([self.norm_vectors[self.items[x]] for x in items])\n\n        result = self._threshold_batch(x,\n                                       batch_size,\n                                       threshold,\n                                       show_progressbar,\n                                       return_names)\n\n        # list call consumes the generator.\n        return [x[1:] for x in result]\n\n    def nearest_neighbor(self,\n                         vectors,\n                         num=10,\n                         batch_size=100,\n                         show_progressbar=False,\n                         return_names=True):\n        """"""\n        Find the nearest neighbors to some arbitrary vector.\n\n        This function is meant to be used in composition operations. The\n        most_similar function can only handle items that are in vocab, and\n        looks up their vector through a dictionary. Compositions, e.g.\n        ""King - man + woman"" are necessarily not in the vocabulary.\n\n        Parameters\n        ----------\n        vectors : list of arrays or numpy array\n            The vectors to find the nearest neighbors to.\n        num : int, optional, default 10\n            The number of most similar items to retrieve.\n        batch_size : int, optional, default 100.\n            The batch size to use. 100 is a good default option. Increasing\n            the batch size may increase speed.\n        show_progressbar : bool, optional, default False\n            Whether to show a progressbar.\n        return_names : bool, optional, default True\n            Whether to return the item names, or just the distances.\n\n        Returns\n        -------\n        sim : list of tuples.\n            For each item in the input the num most similar items are returned\n            in the form of (NAME, DISTANCE) tuples. If return_names is set to\n            false, only the distances are returned.\n\n        """"""\n        vectors = np.array(vectors)\n        if np.ndim(vectors) == 1:\n            vectors = vectors[None, :]\n\n        return list(self._batch(vectors,\n                                batch_size,\n                                num,\n                                show_progressbar,\n                                return_names))\n\n    def nearest_neighbor_threshold(self,\n                                   vectors,\n                                   threshold=.5,\n                                   batch_size=100,\n                                   show_progressbar=False,\n                                   return_names=True):\n        """"""\n        Find the nearest neighbors to some arbitrary vector.\n\n        This function is meant to be used in composition operations. The\n        most_similar function can only handle items that are in vocab, and\n        looks up their vector through a dictionary. Compositions, e.g.\n        ""King - man + woman"" are necessarily not in the vocabulary.\n\n        Parameters\n        ----------\n        vectors : list of arrays or numpy array\n            The vectors to find the nearest neighbors to.\n        threshold : float, optional, default .5\n            The threshold within to retrieve items.\n        batch_size : int, optional, default 100.\n            The batch size to use. 100 is a good default option. Increasing\n            the batch size may increase speed.\n        show_progressbar : bool, optional, default False\n            Whether to show a progressbar.\n        return_names : bool, optional, default True\n            Whether to return the item names, or just the distances.\n\n        Returns\n        -------\n        sim : list of tuples.\n            For each item in the input the num most similar items are returned\n            in the form of (NAME, DISTANCE) tuples. If return_names is set to\n            false, only the distances are returned.\n\n        """"""\n        vectors = np.array(vectors)\n        if np.ndim(vectors) == 1:\n            vectors = vectors[None, :]\n\n        return list(self._threshold_batch(vectors,\n                                          batch_size,\n                                          threshold,\n                                          show_progressbar,\n                                          return_names))\n\n    def _threshold_batch(self,\n                         vectors,\n                         batch_size,\n                         threshold,\n                         show_progressbar,\n                         return_names):\n        """"""Batched cosine distance.""""""\n        for i in tqdm(range(0, len(vectors), batch_size),\n                      disable=not show_progressbar):\n\n            batch = vectors[i: i+batch_size]\n            similarities = self._sim(batch, self.norm_vectors)\n            for lidx, sims in enumerate(similarities):\n                indices = np.flatnonzero(sims >= threshold)\n                sorted_indices = indices[np.argsort(-sims[indices])]\n                if return_names:\n                    yield [(self.indices[d], sims[d])\n                           for d in sorted_indices]\n                else:\n                    yield list(sims[sorted_indices])\n\n    def _batch(self,\n               vectors,\n               batch_size,\n               num,\n               show_progressbar,\n               return_names):\n        """"""Batched cosine distance.""""""\n        if num < 1:\n            raise ValueError(""num should be >= 1, is now {}"".format(num))\n\n        for i in tqdm(range(0, len(vectors), batch_size),\n                      disable=not show_progressbar):\n\n            batch = vectors[i: i+batch_size]\n            similarities = self._sim(batch, self.norm_vectors)\n            if num == 1:\n                sorted_indices = np.argmax(similarities, 1)[:, None]\n            else:\n                sorted_indices = np.argpartition(-similarities,\n                                                 kth=num,\n                                                 axis=1)\n                sorted_indices = sorted_indices[:, :num]\n            for lidx, indices in enumerate(sorted_indices):\n                sims = similarities[lidx, indices]\n                if return_names:\n                    dindex = np.argsort(-sims)\n                    yield [(self.indices[indices[d]], sims[d])\n                           for d in dindex]\n                else:\n                    yield list(-1 * np.sort(-sims))\n\n    @staticmethod\n    def normalize(vectors):\n        """"""\n        Normalize a matrix of row vectors to unit length.\n\n        Contains a shortcut if there are no zero vectors in the matrix.\n        If there are zero vectors, we do some indexing tricks to avoid\n        dividing by 0.\n\n        Parameters\n        ----------\n        vectors : np.array\n            The vectors to normalize.\n\n        Returns\n        -------\n        vectors : np.array\n            The input vectors, normalized to unit length.\n\n        """"""\n        vectors = np.copy(vectors)\n        if np.ndim(vectors) == 1:\n            norm = np.linalg.norm(vectors)\n            if norm == 0:\n                return np.zeros_like(vectors)\n            return vectors / norm\n\n        norm = np.linalg.norm(vectors, axis=1)\n\n        if np.any(norm == 0):\n\n            nonzero = norm > 0\n\n            result = np.zeros_like(vectors)\n\n            n = norm[nonzero]\n            p = vectors[nonzero]\n            result[nonzero] = p / n[:, None]\n\n            return result\n        else:\n            return vectors / norm[:, None]\n\n    def vector_similarity(self, vector, items):\n        """"""Compute the similarity between a vector and a set of items.""""""\n        items_vec = np.stack([self.norm_vectors[self.items[x]] for x in items])\n        return self._sim(vector, items_vec)\n\n    def _sim(self, x, y):\n        """"""Distance function.""""""\n        sim = self.normalize(x).dot(y.T)\n        return np.clip(sim, a_min=.0, a_max=1.0)\n\n    def similarity(self, i1, i2):\n        """"""\n        Compute the similarity between two sets of items.\n\n        Parameters\n        ----------\n        i1 : object\n            The first set of items.\n        i2 : object\n            The second set of item.\n\n        Returns\n        -------\n        sim : array of floats\n            An array of similarity scores between 1 and 0.\n\n        """"""\n        try:\n            if i1 in self.items:\n                i1 = [i1]\n        except TypeError:\n            pass\n        try:\n            if i2 in self.items:\n                i2 = [i2]\n        except TypeError:\n            pass\n        i1_vec = np.stack([self.norm_vectors[self.items[x]] for x in i1])\n        i2_vec = np.stack([self.norm_vectors[self.items[x]] for x in i2])\n        return self._sim(i1_vec, i2_vec)\n\n    def intersect(self, wordlist):\n        """"""\n        Intersect a reach instance with a wordlist.\n\n        Parameters\n        ----------\n        wordlist : list of str\n            A list of words to keep. Note that this wordlist need not include\n            all words in the Reach instance. Any words which are in the\n            wordlist, but not in the reach instance are ignored.\n\n        """"""\n        # Remove duplicates and oov words.\n        wordlist = set(self.items) & set(wordlist)\n        # Get indices of intersection.\n        indices = np.sort([self.items[x] for x in wordlist])\n        # Set unk_index to None if it is None or if it is not in indices\n        unk_index = self.unk_index if self.unk_index in indices else None\n        # Index vectors\n        vectors = self.vectors[indices]\n        # Index words\n        wordlist = [self.indices[x] for x in indices]\n        return Reach(vectors, wordlist, unk_index=unk_index)\n\n    def save(self, path, write_header=True):\n        """"""\n        Save the current vector space in word2vec format.\n\n        Parameters\n        ----------\n        path : str\n            The path to save the vector file to.\n        write_header : bool, optional, default True\n            Whether to write a word2vec-style header as the first line of the\n            file\n\n        """"""\n        with open(path, \'w\') as f:\n\n            if write_header:\n                f.write(u""{0} {1}\\n"".format(str(self.vectors.shape[0]),\n                        str(self.vectors.shape[1])))\n\n            for i in range(len(self.items)):\n\n                w = self.indices[i]\n                vec = self.vectors[i]\n\n                f.write(u""{0} {1}\\n"".format(w,\n                                            "" "".join([str(x) for x in vec])))\n\n    def save_fast_format(self, filename):\n        """"""\n        Save a reach instance in a fast format.\n\n        The reach fast format stores the words and vectors of a Reach instance\n        separately in a JSON and numpy format, respectively.\n\n        Parameters\n        ----------\n        filename : str\n            The prefix to add to the saved filename. Note that this is not the\n            real filename under which these items are stored.\n            The words and unk_index are stored under ""{filename}_words.json"",\n            and the numpy matrix is saved under ""{filename}_vectors.npy"".\n\n        """"""\n        items, _ = zip(*sorted(self.items.items(), key=lambda x: x[1]))\n        items = {""items"": items,\n                 ""unk_index"": self.unk_index,\n                 ""name"": self.name}\n\n        json.dump(items, open(""{}_items.json"".format(filename), \'w\'))\n        np.save(open(""{}_vectors.npy"".format(filename), \'wb\'), self.vectors)\n\n    @staticmethod\n    def load_fast_format(filename):\n        """"""\n        Load a reach instance in fast format.\n\n        As described above, the fast format stores the words and vectors of the\n        Reach instance separately, and is drastically faster than loading from\n        .txt files.\n\n        Parameters\n        ----------\n        filename : str\n            The filename prefix from which to load. Note that this is not a\n            real filepath as such, but a shared prefix for both files.\n            In order for this to work, both {filename}_words.json and\n            {filename}_vectors.npy should be present.\n\n        """"""\n        words, unk_index, name, vectors = Reach._load_fast(filename)\n        return Reach(vectors, words, unk_index=unk_index, name=name)\n\n    @staticmethod\n    def _load_fast(filename):\n        """"""Sub for fast loader.""""""\n        it = json.load(open(""{}_items.json"".format(filename)))\n        words, unk_index, name = it[""items""], it[""unk_index""], it[""name""]\n        vectors = np.load(open(""{}_vectors.npy"".format(filename), \'rb\'))\n        return words, unk_index, name, vectors\n'"
