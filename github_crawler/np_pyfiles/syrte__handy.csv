file_path,api_count,code
__init__.py,0,"b'from .stats import *\nfrom .interpolate import *\nfrom .integrate import *\nfrom .optimize import *\nfrom .misc import *\nfrom .funcs import *\nfrom .cython import *\n\nfrom .hist import *\nfrom .scatter import *\nfrom .line import *\nfrom .axes import *\nfrom .helper import *\n\n__all__ = []\nfor mod in [stats, interpolate, integrate, optimize, misc, funcs,\n            cython, hist, scatter, line, axes, helper]:\n    __all__.extend(mod.__all__)\ndel mod\n'"
axes.py,0,"b'from __future__ import division\nfrom matplotlib import pyplot as plt\n\n__all__ = [\'twin_axes\']\n\n\ndef twin_axes(show=""xy"", ax=None):\n    """"""\n    Create a twin of Axes for generating a plot with a shared\n    x-axis and y-axis.\n    The x-axis (y-axis) of ax will have ticks on bottom (left)\n    and the returned axes will have ticks on the top (right).\n\n    It will have wrong behaiver when the axis-limits are\n    changed by setting ticks. This can be corrected by call\n    `xlim` to reset the limits.\n    Refer this issue:\n        https://github.com/matplotlib/matplotlib/issues/6863\n\n    Need better meganism for only show \'y\', should same as twinx.\n    """"""\n    assert show in [\'x\', \'y\', \'xy\']\n    if ax is None:\n        ax = plt.gca()\n\n    ax2 = ax._make_twin_axes()\n    ax2._shared_x_axes.join(ax2, ax)\n    ax2._shared_y_axes.join(ax2, ax)\n    ax2._adjustable = \'datalim\'\n    ax2.set_xlim(ax.get_xlim(), emit=False, auto=False)\n    ax2.set_ylim(ax.get_ylim(), emit=False, auto=False)\n    ax2.xaxis._set_scale(ax.xaxis.get_scale())\n    ax2.yaxis._set_scale(ax.yaxis.get_scale())\n\n    ax.xaxis.tick_bottom()\n    ax.yaxis.tick_left()\n    ax2.xaxis.tick_top()\n    ax2.yaxis.tick_right()\n    ax2.xaxis.set_label_position(\'top\')\n    ax2.yaxis.set_label_position(\'right\')\n    ax2.yaxis.set_offset_position(\'right\')\n\n    ax2.patch.set_visible(False)\n    if show == \'x\':\n        ax2.yaxis.set_visible(False)\n    elif show == \'y\':\n        ax2.xaxis.set_visible(False)\n    return ax2\n'"
cluster.py,21,"b'from __future__ import division, print_function, absolute_import\nimport numpy as np\nfrom scipy.special import gamma as gamma_func\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.neighbors import KDTree\nfrom matplotlib import pyplot as plt\n# from collections import namedtuple\n\n__all__ = [\'DensPeakFinder\']\n\n\nclass DensPeakFinder:\n    """"""\n    Fast clustering with k-d tree.\n    Clustering By Fast Search And Find Of Density Peaks. Alex Rodriguez, Alessandro Laio. Science, 2014\n\n    Examples\n    --------\n    import numpy as np\n    from matplotlib import pyplot as plt\n\n    n, d = 1000, 3\n    pts = np.random.randn(n, d)\n    pts[:int(n*0.3), :2] *= 0.5\n    pts[:int(n*0.3), :2] += [3, 1]\n    pts[int(n*0.3):int(n*0.5), :2] += [1, 3]\n\n    dpeak = DensPeakFinder(pts, k=10)\n    peak, ix_peak, group = dpeak.plot_peak()\n    # if you don\'t want the plot\n    peak, ix_peak, group = dpeak.find_peak(400, cluster=True)\n    """"""\n\n    def __init__(self, pts, k=None, r=None, kmax=None, rmax=None):\n        """"""\n        Parameters\n        ----------\n        pts : array, shape(n, d)\n            Data points. Should be already normalized if necessary.\n        k : int\n            Neighbors used to estimate the local density rho.\n        kmax : int\n            If given, only search the nearest kmax neighbors to calculate delta.\n            kmax is equivalent to search a sphere of size about kmax**(1/d) times\n            the local average separation between points.\n            Default is to search all points.\n        rmax : float\n            If given, only search the neighbors within rmax to calculate delta.\n            Default is to search all points.\n\n        Todos\n        -----\n        Optimal choice of k and gamma\n        Performance optimization with Cython or Numba\n        Substructure within density saddle point\n        Labeling the noise\n        """"""\n        if (k is not None) and (r is not None):\n            raise ValueError(""Only one of \'k\' or \'r\' can be specified!"")\n        if (kmax is not None) and (rmax is not None):\n            raise ValueError(""Only one of \'kmax\' or \'rmax\' can be specified!"")\n\n        pts = np.asfarray(pts)\n        npts, ndim = pts.shape\n        Rmax = np.linalg.norm(pts.max(0) - pts.min(0))\n        tree = KDTree(pts)\n\n        # density\n        if r is not None:\n            k = tree.query_radius(pts, r, count_only=True)\n        elif k is not None:\n            r = tree.query(pts, k)[0][:, -1]\n\n        sphere_coeff = np.pi**(0.5 * ndim) / gamma_func(0.5 * ndim + 1)\n        rho = k / (sphere_coeff * r**ndim)\n        rho[rho == 0] = rho[rho > 0].min() / 2  # reduce by an arbitrary factor\n\n        # delta\n        delta = np.full(npts, Rmax, dtype=\'float\')\n        chief = np.full(npts, -1, dtype=\'int\')  # superior neighbor\n        if kmax is not None or rmax is not None:\n            if kmax is not None:\n                dists, index = tree.query(\n                    pts, kmax, return_distance=True, sort_results=True)\n            else:\n                index, dists = tree.query_radius(\n                    pts, rmax, return_distance=True, sort_results=True)\n            for i in range(npts):\n                rho_i = rho[i]\n                for j, dist in zip(index[i], dists[i]):\n                    if (rho[j] > rho_i):\n                        chief_i, delta_i = j, dist\n                        break\n                chief[i], delta[i] = chief_i, delta_i\n        else:\n            dists = squareform(pdist(pts))\n            for i in range(npts):\n                rho_i, delta_i = rho[i], delta[i]\n                for j, dist in enumerate(dists[i]):\n                    if (rho[j] > rho_i) and (dist < delta_i):\n                        chief_i, delta_i = j, dist\n                chief[i], delta[i] = chief_i, delta_i\n\n        # gamma\n        gamma = sphere_coeff * rho * delta**ndim  # need sphere_coeff?\n        sorted_index = np.argsort(gamma)\n        sorted_gamma = gamma[sorted_index]\n\n        # properties\n        self.npts = npts\n        self.ndim = ndim\n        self.pts = pts\n        self.rho = rho\n        self.delta = delta\n        self.gamma = gamma\n        self.chief = chief\n        self.sorted_index = sorted_index\n        self.sorted_gamma = sorted_gamma\n\n    def get_gamma_threshold(self, gamma_th=None):\n        # XXX\n        if gamma_th is None:\n            gamma = self.gamma\n            lg_gamma = np.log10(gamma)\n            gamma_threshold = 10**(np.nanmean(lg_gamma) + 4.5 * np.nanstd(lg_gamma))\n            return gamma_threshold\n        else:\n            return gamma_th\n\n    def find_peak(self, gamma_th=None, npeak=None, rho_th=None, cluster=False):\n        """"""\n        Parameters\n        ----------\n        gamma_th : float\n            Threshold for peak identification.\n        rho_th : float\n            Threshold for noisy points.\n        cluster : bool\n            If true, the groupid will also be returned.\n\n        Returns\n        -------\n        peak : array\n            Position of peak points\n        ix_peak : array\n            Index of peak points in the original array.\n        group : array\n            Group id of each point, start from 0.\n        """"""\n        if rho_th is None:\n            sorted_index = self.sorted_index\n            sorted_gamma = self.sorted_gamma\n        else:\n            ix_rho_th = np.where(self.rho[self.sorted_index] >= rho_th)[0]\n            sorted_index = self.sorted_index[ix_rho_th]\n            sorted_gamma = self.sorted_gamma[ix_rho_th]\n\n        if npeak is not None:\n            ix_peak = sorted_index[-npeak:][::-1]\n        else:\n            if gamma_th is None:\n                gamma_th = self.get_gamma_threshold()\n            ix_th = np.searchsorted(sorted_gamma, gamma_th, side=\'right\')\n            ix_peak = sorted_index[ix_th:][::-1]\n        peak = self.pts[ix_peak]\n        npeak = len(ix_peak)\n\n        if cluster:\n            if rho_th is None:\n                chief = self.chief\n            else:\n                # don\'t assign group for low density points\n                chief = self.chief.copy()\n                chief[self.rho < rho_th] = -1\n            group = np.full(self.npts, -1, dtype=\'int\')\n            group[ix_peak] = np.arange(npeak)\n\n            ix_sort_rho = np.argsort(self.rho)[::-1]\n            for i in ix_sort_rho:\n                j = chief[i]\n                if j != -1 and group[i] == -1:\n                    group[i] = group[j]\n            return peak, ix_peak, group\n        else:\n            return peak, ix_peak\n\n    def plot_peak(self, gamma_th=None, npeak=None, rho_th=None, axes=[0, 1]):\n        """"""\n        Show the decision graph and return peaks.\n\n        Parameters\n        ----------\n        gamma_th : float\n            Threshold for peak identification.\n        rho_th : float\n            Threshold for noisy points.\n        axes : list of length 2\n            Specify the axes of n-d data points to show.\n        """"""\n        if npeak is None and gamma_th is None:\n            gamma_th = self.get_gamma_threshold()\n        if len(axes) != 2:\n            raise ValueError(""Argument \'axes\' should be shape (2,)"")\n\n        dpeak = self\n        peak, ix_peak, group = dpeak.find_peak(\n            gamma_th, npeak=npeak, rho_th=rho_th, cluster=True)\n        npeak = len(peak)\n\n        xlims, ylims = np.percentile(dpeak.pts, q=[5, 95], axis=0).T[axes]\n\n        plt.figure(figsize=(12, 8))\n        plt.subplots_adjust(wspace=0.2, hspace=0.2)\n        plt.subplot(222)\n        plt.scatter(*dpeak.pts.T[axes], c=group, s=5,\n                    cmap=plt.get_cmap(lut=npeak + 1), vmin=-1.5, vmax=npeak - 0.5)\n        plt.colorbar(ticks=np.arange(-1, npeak), label=\'group\')\n        plt.scatter(*peak.T[axes], s=150, lw=3, c=\'k\', marker=\'x\')\n        plt.xlim(xlims)\n        plt.ylim(ylims)\n        plt.xlabel(r\'$X%d$\' % axes[0])\n        plt.ylabel(r\'$X%d$\' % axes[1])\n\n        plt.subplot(224)\n        plt.scatter(*dpeak.pts.T[axes], c=dpeak.rho, s=5, vmax=np.percentile(dpeak.rho, q=90))\n        plt.colorbar(label=r\'$\\rho$\')\n        plt.scatter(*peak.T[axes], s=150, lw=3, c=\'k\', marker=\'x\')\n        plt.xlim(xlims)\n        plt.ylim(ylims)\n        plt.xlabel(r\'$X%d$\' % axes[0])\n        plt.ylabel(r\'$X%d$\' % axes[1])\n\n        plt.subplot(221)\n        plt.xscale(\'log\')\n        plt.yscale(\'log\')\n        plt.scatter(dpeak.rho, dpeak.delta, s=5)\n        plt.scatter(dpeak.rho[ix_peak], dpeak.delta[ix_peak], s=150, lw=3, c=\'k\', marker=\'x\')\n        if gamma_th is not None:\n            xlims = np.array(plt.gca().get_xlim())\n            plt.plot(xlims, (gamma_th / xlims)**(1 / dpeak.ndim), ls=\'--\', color=\'gray\')\n        if rho_th is not None:\n            plt.axvline(rho_th, ls=\'--\', color=\'gray\')\n        plt.xlabel(r\'$\\rho$\')\n        plt.ylabel(r\'$\\delta$\')\n\n        plt.subplot(223)\n        gamma_sorted = dpeak.sorted_gamma\n        gamma_sorted_mid = np.sqrt(gamma_sorted[1:] * gamma_sorted[:-1])\n        n_cum = np.arange(dpeak.npts, 0, -1)\n        dlngamma_dlnN = -np.diff(np.log(n_cum)) / np.diff(np.log(gamma_sorted))\n\n        plt.xscale(\'log\')\n        plt.yscale(\'log\')\n        plt.scatter(gamma_sorted, n_cum, s=5)\n        if gamma_th is not None:\n            plt.axvline(gamma_th, ls=\'--\', color=\'gray\')\n        plt.xlabel(r\'$\\gamma=\\delta^d\\rho$\')\n        plt.ylabel(r\'$N(>\\gamma)$\')\n\n        plt.twinx()\n        plt.plot(gamma_sorted_mid[-10:], dlngamma_dlnN[-10:], ls=\'--\', lw=0.75, color=\'gray\')\n        plt.ylim(0, 2)\n        return peak, ix_peak, group\n'"
cmap.py,10,"b'from __future__ import division, print_function, absolute_import\nimport numpy as np\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\n\n\n__all__ = [""make_rainbow"", ""make_cubehelix"", ""show_cmap"", ""make_cmap_ref""]\n\n\ndef grayify_cmap(cmap, register=False):\n    """"""Return a grayscale version of the colormap\n    copy from https://jakevdp.github.io/blog/2014/10/16/how-bad-is-your-colormap/\n    """"""\n    cmap = plt.cm.get_cmap(cmap)\n    colors = cmap(np.arange(cmap.N))\n\n    # convert RGBA to perceived greyscale luminance\n    # cf. http://alienryderflex.com/hsp.html\n    RGB_weight = [0.299, 0.587, 0.114]\n    luminance = np.sqrt(np.dot(colors[:, :3]**2, RGB_weight))\n    colors[:, :3] = luminance[:, np.newaxis]\n\n    cmap_g = cmap.from_list(cmap.name + ""_g"", colors, cmap.N)\n    if register:\n        plt.register_cmap(cmap=cmap_g)\n    return cmap_g\n\n\ndef make_rainbow(a=0.75, b=0.2, name=\'custom_rainbow\', register=False):\n    """"""\n    Use a=0.7, b=0.2 for a darker end.\n\n    when 0.5<=a<=1.5, should have b >= (a-0.5)/2 or 0 <= b <= (a-1)/3\n    when 0<=a<=0.5, should have b >= (0.5-a)/2 or 0<= b<= -a/3\n    to assert the monoique\n\n    To show the parameter dependencies interactively in notebook\n    ```\n    %matplotlib inline\n    from ipywidgets import interact\n    def func(a=0.75, b=0.2):\n        cmap = gene_rainbow(a=a, b=b)\n        show_cmap(cmap)\n    interact(func, a=(0, 1, 0.05), b=(0.1, 0.5, 0.05))\n    ```\n    """"""\n    def gfunc(a, b, c=1):\n        def func(x):\n            return c * np.exp(-0.5 * (x - a)**2 / b**2)\n        return func\n\n    cdict = {""red"": gfunc(a, b),\n             ""green"": gfunc(0.5, b),\n             ""blue"": gfunc(1 - a, b)\n             }\n    cmap = mpl.colors.LinearSegmentedColormap(name, cdict)\n    if register:\n        plt.register_cmap(cmap=cmap)\n        plt.rc(\'image\', cmap=cmap.name)\n    return cmap\n\n\ndef make_cubehelix(*args, **kwargs):\n    """"""make_cubehelix(start=0.5, rotation=-1.5, gamma=1.0,\n                   start_hue=None, end_hue=None,\n                   sat=None, min_sat=1.2, max_sat=1.2,\n                   min_light=0., max_light=1.,\n                   n=256., reverse=False, name=\'custom_cubehelix\')\n    """"""\n    from palettable.cubehelix import Cubehelix\n    cmap = Cubehelix.make(*args, **kwargs).mpl_colormap\n    register = kwargs.setdefault(""register"", False)\n    if register:\n        plt.register_cmap(cmap=cmap)\n        plt.rc(\'image\', cmap=cmap.name)\n    return cmap\n\n\ndef show_cmap(cmap, coeff=(0.3, 0.59, 0.11)):\n    coeff = np.asarray(coeff, \'f\').reshape(-1, 1) / np.sum(coeff)\n\n    x = np.linspace(0, 1, 257)\n    rgba = cmap(x).T\n\n    plt.figure(figsize=(6, 4))\n    plt.axes([0.1, 0.25, 0.7, 0.65])\n\n    # components\n    for c, y in zip([""red"", ""green"", ""blue""], rgba):\n        plt.plot(x, y, lw=2, label=c, color=c)\n\n    # alpha\n    y = rgba[3]\n    if not np.allclose(y, 1):\n        plt.plot(x, y, lw=2, label=""alpha"", color=\'m\', ls="":"")\n\n    # total brightness\n    y = np.mean(rgba[:3], 0)\n    plt.plot(x, y, \'k--\', lw=2, label=""L"")\n\n    # total perceived brightness\n    y = np.sum(rgba[:3] * coeff, 0)\n    plt.plot(x, y, \'c--\', lw=2, label=""L(eye)"")\n\n    plt.xlim(0, 1)\n    plt.ylim(0, 1.1)\n    plt.xticks([])\n    plt.legend(loc=(1, 0.1), frameon=False, handlelength=1.5)\n\n    # cmap\n    plt.axes([0.1, 0.1, 0.7, 0.15])\n    plt.imshow([x], extent=[0, 1, 0, 1], vmin=0, vmax=1, aspect=\'auto\', cmap=cmap)\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)\n    plt.yticks([])\n\n\ndef make_cmap_ref(**fig_kwargs):\n    maps = [m for m in mpl.cm.datad if not m.endswith(""_r"")]\n    maps.sort()\n    n = len(maps) + 1\n    a = [np.linspace(0, 1, 101)]\n\n    fig_kwargs.setdefault(""figsize"", (5, 20))\n    fig = plt.figure(**fig_kwargs)\n    fig.subplots_adjust(top=0.8, bottom=0.05, left=0.01, right=0.85)\n\n    for i, m in enumerate(maps):\n        ax = plt.subplot(n, 1, i + 1)\n        ax.imshow(a, cmap=plt.get_cmap(m), aspect=\'auto\')\n        ax.text(1.05, 0.2, m, fontsize=10, transform=ax.transAxes)\n        ax.axis(""off"")\n    return fig\n'"
cython.py,0,"b'""""""\ncythonmagic can easily compile your cython snippets on the fly,\nwithout writing the tedious setup files or makefile.\nIt is a standalone package originally inspired by IPython Cython magic.\n""""""\nfrom __future__ import absolute_import, print_function\n\nimport re\nimport io\nimport os\nimport sys\nimport hashlib\nimport inspect\nimport contextlib\nfrom distutils.core import Extension\n\nimport Cython\nfrom Cython.Utils import captured_fd, get_cython_cache_dir\nfrom Cython.Build import cythonize\nfrom Cython.Build.Inline import to_unicode, strip_common_indent\nfrom Cython.Build.Inline import _get_build_extension\n\n\n__all__ = [\'cythonmagic\']\n\n\ndef _append_args(kwargs, key, value):\n    kwargs[key] = [value] + kwargs.get(key, [])\n\n\ndef _extend_args(kwargs, key, value_list):\n    kwargs[key] = value_list + kwargs.get(key, [])\n\n\ndef _export_all(source, target):\n    """"""Import all variables from the namespace `source` to `target`.\n    Both arguments must be dict-like objects.\n    If `source[\'__all__\']` is defined, only variables in it will be imported, otherwise\n    all variables not starting with \'_\' will be imported.\n    """"""\n    if \'__all__\' in source:\n        keys = source[\'__all__\']\n    else:\n        keys = [k for k in source if not k.startswith(\'_\')]\n\n    for k in keys:\n        try:\n            target[k] = source[k]\n        except KeyError:\n            msg = ""\'module\' object has no attribute \'%s\'"" % k\n            raise AttributeError(msg)\n\n\ndef join_path(path1, path2):\n    """"""Join and normalize two paths.\n    """"""\n    return os.path.normpath(os.path.join(\n        path1, os.path.expanduser(path2)))\n\n\ndef get_basename(path):\n    """"""Get the base name of the file, e.g. \'abc\' for \'dir/abc.py\'.\n    """"""\n    return os.path.splitext(os.path.basename(path))[0]\n\n\ndef get_frame_dir(depth=0):\n    """"""Return the source file directory of a frame in the call stack.\n    """"""\n    if hasattr(sys, ""_getframe""):\n        frame = sys._getframe(depth + 1)  # +1 for this function itself\n    else:\n        raise NotImplementedError(""Support CPython only."")\n    file = inspect.getabsfile(frame)\n    return os.path.dirname(file)\n\n\ndef so_ext():\n    """"""Get extension for the compiled library.\n    """"""\n    if not hasattr(so_ext, \'ext\'):\n        so_ext.ext = _get_build_extension().get_ext_filename(\'\')\n    return so_ext.ext\n\n\ndef load_dynamic(name, path):\n    """"""Load and initialize a module implemented as a dynamically loadable\n    shared library and return its module object. If the module was already\n    initialized, it will be initialized again.\n    """"""\n    # imp module is deprecated since Python 3.4\n    if (sys.version_info >= (3, 4)):\n        from importlib.machinery import ExtensionFileLoader\n        from importlib.util import spec_from_loader, module_from_spec\n        loader = ExtensionFileLoader(name, path)\n        spec = spec_from_loader(name, loader, origin=path)\n        module = module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module\n    else:\n        import imp\n        return imp.load_dynamic(name, path)\n\n\n@contextlib.contextmanager\ndef set_env(**environ):\n    """"""\n    Temporarily set the environment variables.\n    source: http://stackoverflow.com/a/34333710/\n\n    Examples\n    --------\n    >>> with set_env(PLUGINS_DIR=u\'plugins\'):\n    ...   ""PLUGINS_DIR"" in os.environ\n    True\n    >>> ""PLUGINS_DIR"" in os.environ\n    False\n    """"""\n    try:\n        if environ:\n            old_environ = dict(os.environ)\n            os.environ.update(environ)\n        yield\n    finally:\n        if environ:\n            os.environ.clear()\n            os.environ.update(old_environ)\n\n\n@contextlib.contextmanager\ndef _suppress_output(quiet=True):\n    """"""Suppress any output/error/warning in compiling\n    if quiet is True and no exception raised.\n    """"""\n    try:\n        # `captured_fd` only captures the default IO streams, we must redirect\n        # the streams to defaults for jupyter notebook to enable capturing.\n        old_stream = sys.stdout, sys.stderr\n        sys.stdout, sys.stderr = sys.__stdout__, sys.__stderr__\n\n        get_outs = get_errs = lambda: None  # backup for failure\n        with captured_fd(1) as get_outs:\n            with captured_fd(2) as get_errs:\n                yield\n\n    except Exception:\n        quiet = False\n        raise\n\n    finally:\n        sys.stdout, sys.stderr = old_stream\n\n        if not quiet:\n            outs, errs = get_outs(), get_errs()\n            if outs:\n                print(""Compiler Output\\n==============="",\n                      outs.decode(\'utf8\'), sep=\'\\n\', file=sys.stdout)\n            if errs:\n                print(""Compiler Error/Warning\\n======================"",\n                      errs.decode(\'utf8\'), sep=\'\\n\', file=sys.stderr)\n\n\ndef _update_flag(code, args, smart=True):\n    """"""Update compiler options for numpy and openmp.\n    Helper function for cythonmagic.\n    """"""\n    numpy = args.pop(\'numpy\', None)\n    openmp = args.pop(\'openmp\', None)\n\n    if numpy is None and smart:\n        reg_numpy = re.compile(r""""""\n            ^\\s* cimport \\s+ numpy |\n            ^\\s* from \\s+ numpy \\s+ cimport\n            """""", re.M | re.X)\n        numpy = reg_numpy.search(code)\n\n    if openmp is None and smart:\n        reg_openmp = re.compile(r""""""\n            ^\\s* c?import \\s+cython\\.parallel |\n            ^\\s* from \\s+ cython\\.parallel \\s+ c?import |\n            ^\\s* from \\s+ cython \\s+ c?import \\s+ parallel\n            """""", re.M | re.X)\n        openmp = reg_openmp.search(code)\n\n    if numpy:\n        import numpy\n        _append_args(args, \'include_dirs\', numpy.get_include())\n\n    if openmp:\n        if hasattr(openmp, \'startswith\'):\n            openmp_flag = openmp  # openmp is string\n        else:\n            openmp_flag = \'-fopenmp\'\n        _append_args(args, \'extra_compile_args\', openmp_flag)\n        _append_args(args, \'extra_link_args\', openmp_flag)\n\n\ndef cython_build(name, file=None, force=False, quiet=True, cythonize_args={},\n                 lib_dir=os.path.join(get_cython_cache_dir(), \'inline/lib\'),\n                 tmp_dir=os.path.join(get_cython_cache_dir(), \'inline/tmp\'),\n                 **extension_args):\n    """"""Build a cython extension.\n    """"""\n    if file is not None:\n        _append_args(extension_args, \'sources\', file)\n\n    with _suppress_output(quiet=quiet):\n        extension = Extension(name, **extension_args)\n        extensions = cythonize([extension], force=force, **cythonize_args)\n\n        build_extension = _get_build_extension()\n        build_extension.extensions = extensions\n        build_extension.build_lib = lib_dir\n        build_extension.build_temp = tmp_dir\n        build_extension.run()\n\n        # ext_file = os.path.join(lib_dir, name + so_ext())\n        # module = load_dynamic(name, ext_file)\n        # return module\n\n\ndef cythonmagic(code, export=None, name=None, force=False,\n                quiet=True, smart=True, fast_indexing=False,\n                directives={}, cimport_dirs=[], cythonize_args={},\n                lib_dir=os.path.join(get_cython_cache_dir(), \'inline/lib\'),\n                tmp_dir=os.path.join(get_cython_cache_dir(), \'inline/tmp\'),\n                environ={}, **extension_args):\n    """"""Compile a code snippet in string.\n    The contents of the code are written to a `.pyx` file in the\n    cython cache directory using a filename with the hash of the\n    code. This file is then cythonized and compiled.\n\n    Parameters\n    ----------\n    code : str\n        The code to compile.\n        It can also be a file path, but must start with ""./"", ""/"", ""X:"", or ""~"",\n        and end with "".py"" or "".pyx"".\n        Strings like ""import abc.pyx"" or ""a=1; b=a.pyx"" will be treated as\n        code snippet.\n    export : dict\n        Export the variables from the compiled module to a dict.\n        `export=globals()` is equivalent to `from module import *`.\n    name : str, optional\n        Name of compiled module. If not given, it will be generated\n        automatically by hash of the code and options (recommended).\n    force : bool\n        Force the compilation of a new module, even if the source\n        has been previously compiled.\n    quiet : bool\n        Suppress compiler\'s outputs/warnings.\n    smart : bool\n        If True, numpy and openmp will be auto-detected from the code.\n    fast_indexing : bool\n        If True, `boundscheck` and `wraparound` are turned off\n        for better array indexing performance (at cost of safety).\n        This setting can be overridden by `directives`.\n    directives : dict\n        Cython compiler directives, including\n            binding, boundscheck, wraparound, initializedcheck, nonecheck,\n            overflowcheck, overflowcheck.fold, embedsignature, cdivision, cdivision_warnings,\n            always_allow_keywords, profile, linetrace, infer_types, language_level, etc.\n        Ref http://docs.cython.org/en/latest/src/userguide/source_files_and_compilation.html#compiler-directives\n        This setting can be overridden by `cythonize_args[\'compiler_directives\']`.\n    cimport_dirs : list of str\n        Directories for finding cimport modules (.pxd files).\n        This setting can be overridden by `cythonize_args[\'include_path\']`.\n    cythonize_args : dict\n        Arguments for `Cython.Build.cythonize`, including\n            aliases, quiet, force, language, annotate, build_dir, output_file,\n            include_path, compiler_directives, etc.\n        Ref http://docs.cython.org/en/latest/src/userguide/source_files_and_compilation.html#cythonize-arguments\n    environ : dict\n        Temporary environment variables for compilation.\n    lib_dir : str\n        Directory to put the compiled module.\n    tmp_dir : str\n        Directory to put the temporary files.\n    **extension_args :\n        Arguments for `distutils.core.Extension`, including\n            name, sources, define_macros, undef_macros,\n            include_dirs, library_dirs, runtime_library_dirs,\n            libraries, extra_compile_args, extra_link_args,\n            extra_objects, export_symbols, depends, language\n        Ref https://docs.python.org/2/distutils/apiref.html#distutils.core.Extension\n\n    Examples\n    --------\n    Basic usage:\n        code = r\'\'\'\n        def func(x):\n            return 2.0 * x\n        \'\'\'\n        pyx = cythonmagic(code)\n        pyx.func(1)\n    Raw string is recommended to avoid breaking escape character.\n\n    Export the names from compiled module:\n        cythonmagic(code, globals())\n        func(1)\n\n    Get better performance (with risk) with arrays:\n        cythonmagic(code, fast_indexing=True)\n\n    Compile OpenMP codes with gcc:\n        cythonmagic(openmpcode, openmp=\'-fopenmp\')\n        # or equivalently\n        cythonmagic(openmpcode,\n                    extra_compile_args=[\'-fopenmp\'],\n                    extra_link_args=[\'-fopenmp\'],\n                    )\n        # use \'-openmp\' or \'-qopenmp\' (>=15.0) for Intel\n        # use \'/openmp\' for Microsoft Visual C++ Compiler\n        # use \'-fopenmp=libomp\' for Clang\n\n    Use icc to compile:\n        cythonmagic(code, environ={\'CC\':\'icc\', \'LDSHARED\':\'icc -shared\'})\n    Ref https://software.intel.com/en-us/articles/thread-parallelism-in-cython\n\n    Set directory for searching cimport (.pxd file):\n        cythonmagic(code, cimport_dirs=[custom_path]})\n        # or equivalently\n        cythonmagic(code, cythonize_args={\'include_path\': [custom_path]})\n    Try setting `cimport_dirs=sys.path` if Cython can not find installed\n    cimport module.\n\n    The cython `directives` and distutils `extension_args` can also be\n    set in a directive comment at the top of the code, e.g.:\n        # cython: boundscheck=False, wraparound=False, cdivision=True\n        # distutils: extra_compile_args = -fopenmp\n        # distutils: extra_link_args = -fopenmp\n        ...code...\n\n    Example of using gsl library, assuming gsl is installed at /opt/gsl/\n        code = r\'\'\'\n        cdef extern from ""gsl/gsl_math.h"":\n            double gsl_pow_int (double x, int n)\n\n        def pow(double x, int n):\n            y = gsl_pow_int(x, n)\n            return y\n        \'\'\'\n        pyx = cythonmagic(\n            code,\n            include_dirs=[\'/opt/gsl/include/\'],\n            library_dirs=[\'/opt/gsl/lib\'],\n            libraries=[\'gsl\', \'gslcblas\']\n        )\n        pyx.pow(2, 6)\n\n    References\n    ----------\n    https://github.com/cython/cython/blob/master/Cython/Build/IpythonMagic.py\n    https://github.com/cython/cython/blob/master/Cython/Build/Inline.py\n    """"""\n    # get working directories\n    # assume all paths are relative to the directory of the caller\'s frame\n    cur_dir = get_frame_dir(depth=1)  # where cythonmagic is called\n\n    lib_dir = join_path(cur_dir, lib_dir)\n    tmp_dir = join_path(cur_dir, tmp_dir)\n\n    if not os.path.isdir(lib_dir):\n        os.makedirs(lib_dir)\n    if not os.path.isdir(tmp_dir):\n        os.makedirs(tmp_dir)\n\n    # check if `code` is a code snippet or a .pyx/.py file\n    reg_pyx = re.compile(r""^ ( ~ | [\\.]? [/\\\\] | [a-zA-Z]:) .* \\.pyx? $ | ""\n                         r""^ [^\\s=;]+ \\.pyx? $"", re.X | re.S)\n    is_file = reg_pyx.match(code)\n\n    if is_file:\n        file = join_path(cur_dir, code)\n        code = io.open(file, \'r\', encoding=\'utf-8\').read()\n        if name is None:\n            name = get_basename(file)\n        # it might exist related .pyd file in the same directory\n        cimport_dirs = cimport_dirs + [os.path.dirname(file)]\n    else:\n        cimport_dirs = cimport_dirs + [cur_dir]\n    code = strip_common_indent(to_unicode(code))\n\n    # update arguments\n    directives = directives.copy()\n    if fast_indexing:\n        directives.setdefault(\'boundscheck\', False)\n        directives.setdefault(\'wraparound\', False)\n    directives.setdefault(\'embedsignature\', True)  # recommended setting\n\n    cythonize_args = cythonize_args.copy()\n    cythonize_args.setdefault(\'compiler_directives\', directives)\n    cythonize_args.setdefault(\'include_path\', cimport_dirs)\n\n    # if any extra dependencies\n    extra_depends = any(extension_args.get(k, [])\n                        for k in [\'sources\', \'extra_objects\', \'depends\'])\n\n    # module signature\n    key = (code, name, cythonize_args, extension_args, environ, os.environ,\n           sys.executable, sys.version_info, Cython.__version__)\n    key_bytes = u""{}"".format(key).encode(\'utf-8\')   # for 2, 3 compatibility\n    signature = hashlib.md5(key_bytes).hexdigest()\n\n    # embed module signature?\n    # code = u""{}\\n\\n# added by cythonmagic\\n{} = \'{}\'"".format(\n    #     code, \'__cythonmagic_signature__\', signature)\n\n    # module name and path\n    pyx_name = ""__cythonmagic__{}"".format(signature)\n    ext_name = pyx_name if name is None else name\n\n    pyx_file = os.path.join(tmp_dir, pyx_name + \'.pyx\')  # path of source file\n    ext_file = os.path.join(lib_dir, ext_name + so_ext())  # path of extension\n\n    # write pyx file\n    if force or not os.path.isfile(pyx_file):\n        with io.open(pyx_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(code)\n        if os.path.isfile(ext_file):\n            os.remove(ext_file)  # dangerous?\n\n    # build\n    # if existing extra depends, let distutils to decide whether rebuild or not\n    if not os.path.isfile(ext_file) or extra_depends:\n        with set_env(**environ):\n            _update_flag(code, extension_args, smart=smart)\n            cython_build(ext_name, file=pyx_file, force=force,\n                         quiet=quiet, cythonize_args=cythonize_args,\n                         lib_dir=lib_dir, tmp_dir=tmp_dir,\n                         **extension_args)\n\n    # import\n    module = load_dynamic(ext_name, ext_file)\n    # module.__pyx_file__ = pyx_file\n    if export is not None:\n        _export_all(module.__dict__, export)\n    return module\n'"
distribution.py,4,"b'from __future__ import division\nfrom scipy.stats import rv_continuous\nfrom numpy import log, exp, nan\nfrom numba import vectorize\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n__all__ = [\'powdist\', \'expdist\']\n\n\nclass rv_custom(rv_continuous):\n    def ppf(self, *args, **kwargs):\n        return self._ppf(*args, **kwargs)\n\n    def isf(self, *args, **kwargs):\n        return self._isf(*args, **kwargs)\n\n    def fit(self, *args, **kwargs):\n        kwargs.update(floc=0, fscale=1)\n        return super(rv_custom, self).fit(*args, **kwargs)[:-2]\n\n    def plot_pdf(self, n, a, b, *args, **kwargs):\n        x = np.linspace(a, b, kwargs.pop(\'num\', 50))\n        y = self.pdf(x, n, a, b)\n        return plt.plot(x, y, *args, **kwargs)\n\n    def plot_cdf(self, n, a, b, *args, **kwargs):\n        x = np.linspace(a, b, kwargs.pop(\'num\', 50))\n        y = self.cdf(x, n, a, b)\n        return plt.plot(x, y, *args, **kwargs)\n\n\nclass powlaw_gen(rv_custom):\n    """"""\n    A power-function continuous random variable.\n    The probability density function is\n        powdist.pdf(x, n, a, b) = A * x**n\n    for `0 <= a <= x <= b, n > -1`\n    or  `0 < a <= x <= b, n <= -1`,\n    where A is normalization constant.\n\n    Examples\n    --------\n    n, a, b = 2, 0, 1\n    p = powdist(n, a, b)\n    x = np.linspace(a, b, 121)\n    plt.hist(p.rvs(100000), 30, normed=True)\n    plt.plot(x, p.pdf(x))\n    """"""\n    @staticmethod\n    @vectorize(""f8(f8, f8, f8, f8)"")\n    def _pdf(x, n, a, b):\n        if x < a or x > b:\n            return 0.\n        elif n != -1:\n            return (n + 1) / (b**(n + 1) - a**(n + 1)) * x**n\n        else:\n            return 1 / (log(b) - log(a)) / x\n\n    @staticmethod\n    @vectorize(""f8(f8, f8, f8, f8)"")\n    def _cdf(x, n, a, b):\n        if x <= a:\n            return 0.\n        elif x >= b:\n            return 1.\n        elif n != -1:\n            x, a, b = x**(n + 1), a**(n + 1), b**(n + 1)\n            return (x - a) / (b - a)\n        else:\n            x, a, b = log(x), log(a), log(b)\n            return (x - a) / (b - a)\n\n    @staticmethod\n    @vectorize(""f8(f8, f8, f8, f8)"")\n    def _ppf(q, n, a, b):\n        if q < 0 or q > 1:\n            return nan\n        if n != -1:\n            return ((1 - q) * a**(n + 1) + q * b**(n + 1)) ** (1 / (n + 1))\n        else:\n            return exp((1 - q) * log(a) + q * log(b))\n\n    @staticmethod\n    @vectorize(""b1(f8, f8, f8)"")\n    def _argcheck(n, a, b):\n        if n > -1:\n            return (0 <= a < b)\n        else:\n            return (0 < a < b)\n\n\nclass expon_gen(rv_custom):\n    """"""\n    A Exponential continuous random variable.\n    The probability density function is\n        powlaw.pdf(x, n, a, b) = A * exp(n*x)\n    for ``0 <= a <= x <= b``, where A is normalization constant.\n\n    Examples\n    --------\n    n, a, b = 2, 0, 1\n    p = expdist(n, a, b)\n    x = np.linspace(a, b, 121)\n    plt.hist(p.rvs(100000), 30, normed=True)\n    plt.plot(x, p.pdf(x))\n    """"""\n    @staticmethod\n    @vectorize(""f8(f8, f8, f8, f8)"")\n    def _pdf(x, n, a, b):\n        if x < a or x > b:\n            return 0.\n        elif n == 0:\n            return 1 / (b - a)\n        else:\n            return n * exp(n * x) / (exp(n * b) - exp(n * a))\n\n    @staticmethod\n    @vectorize(""f8(f8, f8, f8, f8)"")\n    def _cdf(x, n, a, b):\n        if x <= a:\n            return 0.\n        elif x >= b:\n            return 1.\n        elif n == 0:\n            return (x - a) / (b - a)\n        else:\n            a, b, x = exp(n * a), exp(n * b), exp(n * x)\n            return (x - a) / (b - a)\n\n    @staticmethod\n    @vectorize(""f8(f8, f8, f8, f8)"")\n    def _ppf(q, n, a, b):\n        if n == 0:\n            return (1 - q) * a + q * b\n        else:\n            return log((1 - q) * exp(n * a) + q * exp(n * b)) / n\n\n    @staticmethod\n    @vectorize(""b1(f8, f8, f8)"")\n    def _argcheck(n, a, b):\n        return (0 <= a < b)\n\n\npowdist = powlaw_gen(name=""powerlaw"", shapes=""n, a, b"")\nexpdist = expon_gen(name=""exponential"", shapes=""n, a, b"")\n'"
fastmcd.py,12,"b'import numpy as np\nfrom numpy.linalg import slogdet\nfrom scipy.linalg import pinvh\nfrom scipy.stats import chi2\nfrom handy import quantile\n\n__all__ = [\'fast_mcd\']\n\n\ndef _fit_cov(X, weights=None, index=slice(None)):\n    ""Only X[index] is used for calculation""\n    X1 = X[index]\n    if weights is None:\n        loc = np.mean(X1, axis=0)\n        cov = np.cov(X1.T, bias=True)\n    else:\n        weights = weights[index]\n        loc = np.sum(X1.T * weights, axis=1) / np.sum(weights)\n        cov = np.cov(X1.T, aweights=weights, bias=True)\n\n    cov = np.atleast_2d(cov)\n    dX = X - loc\n    dist = (np.dot(dX, pinvh(cov)) * dX).sum(axis=1)\n\n    return loc, cov, dist\n\n\ndef _c_step(X, weights, index, h, nstep):\n    """"""\n    Parameters\n    ----------\n    X, weights:\n        Data and weights [optional].\n    index:\n        initial index.\n    h:\n        subsample size for covariance estimation.\n    nstep:\n        maximum iterations.\n    """"""\n    det_best = np.inf\n    for i in range(nstep):\n        loc, cov, dist = _fit_cov(X, weights=weights, index=index)\n        index = np.argsort(dist)[:h]\n        sign, det = slogdet(cov)\n\n        if sign <= 0 or np.isnan(det):\n            break\n        elif np.isclose(det, det_best):\n            break\n        else:\n            det_best = det\n    return loc, cov, det, dist, index\n\n\ndef fast_mcd(X, weights=None, alpha_mcd=None, alpha_wgt=0.975, exact_cut=False,\n             niter=50, nsamp_trial=10, niter_trial=5):\n    """"""\n    Algorithm\n    ---------\n    - trials:\n        run c-step for `nsamp_trial` trial subsamples of `p + 1` cases,\n        iterate `niter_trial` times for each at most,\n        the best result is taken as input of the following\n    - raw MCD:\n        run c-step with `alpha_mcd * n` cases until converge or `niter` times\n    - weighted MCD:\n        calculate result with `alpha_wgt` cases [optional]\n\n    Parameters\n    ----------\n    X, weights:\n        Data and weights [optional].\n    alpha_mcd:\n        The fraction of points used for MCD, default is (n + p + 1) / 2N.\n    alpha_wgt:\n        The weighted fraction of points used for re-weighting.\n        Set None to disable re-weighting.\n        Treat (1 - alpha_wgt) as outliers.\n    exact_cut:\n        If True, use the actual percentile of dist to estimate consistency factor.\n        Otherwise, an estimation based on chi2 of alpha is used.\n    niter:\n        Maximum iteration for MCD stage.\n    nsamp_trial:\n        Use the best result of `nsamp_trial` trial subsample to start.\n    niter_trial:\n        Maximum iteration for initialization stage.\n\n    References\n    ----------\n    Hubert et al. 2017, Minimum covariance determinant and extensions\n    """"""\n    n, p = X.shape\n\n    if n <= 2 * p:\n        raise ValueError(\'n_sample must be larger than 2*n_feature!\')\n\n    if alpha_mcd is None:\n        h = (n + p + 1) // 2\n    else:\n        h = int(n * alpha_mcd)\n    alpha_mcd = h / n\n\n    if weights is not None:\n        weights = weights / weights.sum()\n\n    # trials\n    det_best = np.inf\n    for i in range(nsamp_trial):\n        index = np.random.choice(n, p + 1, replace=False, p=weights)\n        loc, cov, det, dist, index = _c_step(X, weights, index, h, nstep=niter_trial)\n        if det < det_best:\n            index_best = index\n    loc, cov, det, dist, index = _c_step(X, weights, index_best, h, nstep=niter)\n\n    # the actual fraction within MCD\n    if weights is None:\n        alpha_tmp = alpha_mcd\n    else:\n        alpha_tmp = weights[index].sum()\n\n    # consistency factor\n    if exact_cut:\n        # factor = quantile(dist, weights=weights, q=alpha_tmp) / chi2(p).ppf(alpha_tmp)\n        factor = dist[index[-1]] / chi2(p).ppf(alpha_tmp)\n    else:\n        factor = alpha_tmp / chi2(p + 2).cdf(chi2(p).ppf(alpha_tmp))\n    dist /= factor\n    cov *= factor\n\n    # weight to enhance the asymptotic efficiency\n    if alpha_wgt is not None:\n        index = dist < chi2(p).ppf(alpha_wgt)\n        loc, cov, dist = _fit_cov(X, weights=weights, index=index)\n\n        # consistency factor\n        if exact_cut:\n            factor = quantile(dist, weights=weights, q=alpha_wgt) / chi2(p).ppf(alpha_wgt)\n        else:\n            factor = alpha_wgt / chi2(p + 2).cdf(chi2(p).ppf(alpha_wgt))\n        dist /= factor\n        cov *= factor\n\n    return loc, cov, dist\n'"
funcs.py,0,"b'from __future__ import division, print_function, absolute_import\nfrom collections import Mapping, Iterable\nfrom functools import wraps\nimport traceback\nimport sys\nimport gc\nimport signal\nfrom concurrent.futures import ThreadPoolExecutor, TimeoutError\n\n__all__ = [\'unpack_args\', \'callback_gc\', \'catch_exception\', \'full_traceback\',\n           \'print_flush\', \'timeout\']\n\n\ndef unpack_args(func):\n    @wraps(func)\n    def wrapper(args):\n        if isinstance(args, Mapping):\n            return func(**args)\n        elif isinstance(args, Iterable):\n            return func(*args)\n        else:\n            return func(args)\n    return wrapper\n\n\ndef callback_gc(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        res = func(*args, **kwargs)\n        gc.collect()\n        return res\n    return wrapper\n\n\ndef catch_exception(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            msg = ""Failed:\\n  {}(*{}, **{})\\n{}"".format(\n                func.__name__, args, kwargs, traceback.format_exc())\n            print(msg)\n            return e\n    return wrapper\n\n\ndef full_traceback(func):\n    """"""\n    Seems to not not necessary in Python 3\n    http://stackoverflow.com/a/29442282\n    http://bugs.python.org/issue13831\n    """"""\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            msg = ""{}\\n\\nOriginal {}"".format(e, traceback.format_exc())\n            raise type(e)(msg)\n    return wrapper\n\n\ndef print_flush(*args, **kwargs):\n    flush = kwargs.pop(\'flush\', True)\n    print(*args, **kwargs)\n    file = kwargs.get(\'file\', sys.stdout)\n    if flush and file is not None:\n        file.flush()\n\n\nclass timeout:\n    """"""\n    Handling timeout.\n\n    Note that `signal.signal` can only be called from the main thread\n    in Unix-like system, and can not be nested,\n    i.e. when timeout is called under another timeout, the first one will overridden.\n\n    To avoid above issues, one should use the decorator mode with `thread=True`.\n\n    Examples\n    --------\n        import time\n\n        # context mode\n        with timeout(seconds=1):\n            time.sleep(4)\n\n        # decorator mode\n        @timeout(1)\n        def func():\n            time.sleep(4)\n        func()\n\n    Reference\n    ---------\n    https://stackoverflow.com/a/22348885/ for decorator\n    https://stackoverflow.com/a/2282656/ for context\n    https://stackoverflow.com/a/11901541/ for `signal.setitimer`\n    """"""\n\n    def __init__(self, seconds=1, exception=TimeoutError(\'Timeout.\'),\n                 thread=True):\n        """"""\n        seconds :\n            Note `signal.alarm`\n        thread :\n            If True, `concurrent.futures.ThreadPoolExecutor` is used,\n            otherwise `signal.signal` is used.\n            Only takes effect in decorator mode.\n        """"""\n        self.seconds = seconds\n        self.thread = thread\n\n        if isinstance(exception, type) and issubclass(exception, Exception):\n            self.exception = exception(\'Timeout.\')\n        elif isinstance(exception, Exception):\n            self.exception = exception\n        else:\n            self.exception = TimeoutError(exception)\n\n    def handler(self, signum, frame):\n        raise self.exception\n\n    def __enter__(self):\n        signal.signal(signal.SIGALRM, self.handler)\n        # signal.alarm(self.seconds)\n        signal.setitimer(signal.ITIMER_REAL, self.seconds)\n\n    def __exit__(self, type, value, traceback):\n        signal.alarm(0)\n\n    def __call__(self, func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if self.thread:\n                with ThreadPoolExecutor(1) as pool:\n                    res = pool.submit(func, *args, **kwargs)\n                    res.set_exception(self.exception)\n                    return res.result(self.seconds)\n            else:\n                with self:\n                    return func(*args, **kwargs)\n\n        return wrapper\n'"
h5file.py,3,"b'""""""\nEasier access to hdf5 subgroups/datasets\nby using `group.key` instead of `group[\'key\']`.\n\nUsage:\n    hg = H5File(\'data.h5\')\n    a = hg.a\n    a = hg[\'a\']\n    a_b = hg.a.b\n    a_b = hg[\'a/b\']\n    a_b_arr = hg.a.b.value  # using .value to load the array\n\n    # list available properties\n    dir(hg)\n\n    # or print\n    print(hg)\n\n    # attrs\n    a = hg.attrs.a\n\n    # non-lazy mode\n    hg = H5File(\'data.h5\', lazy=False)\n    a_b_arr = hg.a.b  # no need of .value\n\n    # add new property (original file will not be changed)\n    hg.b = 1\n\n    # access properties starting with non-alphabetic\n    a_1 = hg.a.1  # SyntaxError: invalid syntax\n    a_1 = hg.a[\'1\']\n    a_1 = hg[\'a/1\']\n\n    # slicing\n    sl = hg[slice]\n    sl.x == hg.x[slice]\n    sl.y == hg.y[slice]\n\n    # slicing only takes effect on direct dataset\n    sl.dataset == hg.dataset[slice]\n    sl.group.dataset == hg.group.dataset\n\n    # slice of slice\n    hg[slice1][slice2].x == hg.x[slice1][slice2]\n    # slice of slice is not efficient, don\'t use it too much.\n""""""\n\nfrom __future__ import print_function\nimport os\nimport h5py\nimport numpy as np\nfrom six import string_types\n\n\n__all__ = [\'H5File\']\n\n\nclass H5Group(object):\n    \'\'\'Wrap of hdf5 group for quick access.\n    \'\'\'\n\n    def __init__(self, file, lazy=True):\n        """"""\n        Parameters\n        ----------\n        file : h5py.Group or file path.\n        lazy : bool\n        """"""\n        if isinstance(file, string_types):\n            file = h5py.File(os.path.expanduser(file), \'r\')\n\n        self.__dict__[\'_data_\'] = file\n        self.__dict__[\'_lazy_\'] = lazy\n        self.__dict__[\'_keys_\'] = list(file.keys())\n\n        if hasattr(file, \'attrs\') and file.attrs:\n            self.__dict__[\'_keys_\'] += [\'attrs\']\n\n    def __dir__(self):\n        return self._keys_\n\n    def __str__(self):\n        return ""file:\\t{file}\\nname:\\t{name}\\nkeys:\\t{keys}"".format(\n            file=self._data_.file.filename,\n            name=self._data_.name,\n            keys=""\\n\\t"".join(self._keys_)\n        )\n\n    def __getattr__(self, key):\n        return self[key]\n\n    def __setattr__(self, key, value):\n        self[key] = value\n\n    def __delattr__(self, key):\n        del self[key]\n\n    def __getitem__(self, key):\n        # slice\n        if not isinstance(key, string_types):\n            return H5Slice(self, key)\n\n        # hierarchical key\n        elif \'/\' in key:\n            keys = key.strip(\'/\').split(\'/\')\n            value = self\n            for key in keys:\n                value = value[key]\n            return value\n\n        # simple key\n        else:\n            if key not in self._keys_:\n                raise AttributeError(""no attribute: \'%s\'"" % key)\n            elif key in self.__dict__:\n                return self.__dict__[key]\n            else:\n                return self._load_(key)\n\n    def __setitem__(self, key, value):\n        if not isinstance(key, string_types):\n            raise TypeError(""key must be a string"")\n        elif \'/\' in key:\n            raise ValueError(""key with \'/\' is not supported"")\n        else:\n            self.__dict__[key] = value\n            if key not in self._keys_:\n                self._keys_.append(key)\n\n    def __delitem__(self, key):\n        if not isinstance(key, string_types):\n            raise TypeError(""key must be a string"")\n        elif \'/\' in key:\n            raise ValueError(""key with \'/\' is not supported"")\n        else:\n            if key not in self._keys_:\n                raise AttributeError(""No attribute: \'%s\'"" % key)\n            elif key in self.__dict__:\n                del self.__dict__[key]\n\n    def _load_(self, key):\n        if key == \'attrs\':\n            value = H5Attrs(self._data_.attrs)\n        else:\n            value = self._data_[key]\n            if isinstance(value, h5py.Group):\n                value = H5Group(value, lazy=self._lazy_)\n            elif not self._lazy_ and isinstance(value, h5py.Dataset):\n                value = value.value\n        self.__dict__[key] = value\n        return value\n\n    def _show_(self):\n        for key in self._keys_:\n            value = self[key]\n            if isinstance(value, (h5py.Dataset, np.ndarray)):\n                print(""{}:\\n\\t{:>5s} {}"".format(\n                    key, value.dtype.str.strip("">|<""), value.shape)\n                )\n            else:\n                print(""{}:\\n\\t{}"".format(key, value))\n\n\nclass H5Slice(H5Group):\n    \'\'\'Slice of H5Group\n    \'\'\'\n\n    def __init__(self, group, slice):\n        slice = slice if isinstance(slice, tuple) else (slice,)\n\n        fancy = True\n        if group._lazy_:\n            for sl in slice:\n                # fancy array slice does not support lazy mode\n                if isinstance(sl, np.ndarray):\n                    fancy = False\n                    break\n\n        self.__dict__[\'_data_\'] = group\n        self.__dict__[\'_lazy_\'] = False\n        self.__dict__[\'_keys_\'] = dir(group)\n        self.__dict__[\'_slice_\'] = slice\n        self.__dict__[\'_fancy_\'] = fancy\n\n    def __str__(self):\n        return ""{original}\\nslice:\\t{slice}"".format(\n            original=str(self._data_),\n            slice=self._slice_\n        )\n\n    def _load_(self, key):\n        if self._fancy_:\n            value = self._data_[key]\n        else:\n            value = self._data_[key].value\n        if isinstance(value, (h5py.Dataset, np.ndarray)) and value.shape:\n            sliced = value[self._slice_]\n            self.__dict__[key] = sliced  # only cache sliced dataset\n        return sliced\n\n\nclass H5Attrs(H5Group):\n    \'\'\'Wrap of hdf5 attrs for quick access.\n    \'\'\'\n\n    def __str__(self):\n        return ""\\n"".join(\n            ""%s:\\t%s"" % (key, getattr(self, key)) for key in dir(self)\n        )\n\n\nclass H5File(H5Group):\n    \'\'\'Wrap of hdf5 file for quick access.\n    \'\'\'\n    pass\n'"
h5tricks.py,3,"b'""""""\nh5py tricks\n""""""\nimport h5py\nimport pickle\nimport numpy as np\nfrom sklearn.neighbors import KDTree\nfrom itertools import product\n\n__all__ = [""save_vlen_array"", ""KDTreeH5""]\n\n\ndef save_vlen_array(group, name, array_list):\n    """"""Equivalent to group[name] = array_list\n    group : h5py.Group\n    name : str\n    array_list : array/list of arrays\n        Elements in array_list must have the same dtype!\n\n    Examples\n    --------\n    import h5py\n    from numpy import array\n\n    # a can be list of array\n    a = [array([0]), array([0, 1]), array([0, 1, 2])]\n\n    # or array of array\n    a = array([[array([0]), array([0, 1]), array([0, 1, 2])],\n               [array([0, 1, 2]), array([0, 1]), array([0])]], dtype=object)\n\n    # or created as below\n    a = np.empty((2, 3), \'O\')\n    for i in range(2):\n        for j in range(3):\n            a[i, j] = np.arange(i * 3 + j)\n\n    with h5py.File(\'test_tmp.h5\') as f:\n        save_vlen_array(f, \'a\', a)\n    """"""\n    array_list = np.asarray(array_list)\n    if array_list.dtype.kind == \'O\':\n        shape = array_list.shape\n    else:\n        shape = array_list.shape[:-1]\n    ix_0 = tuple(0 for _ in shape)  # the index of the first array element in array_list\n\n    dtype = h5py.special_dtype(vlen=array_list[ix_0].dtype)\n    dset = group.create_dataset(name, shape, dtype=dtype)\n    try:\n        for ix in product(*map(range, shape)):\n            dset[ix] = array_list[ix]\n    except Exception:\n        del group[name]\n\n\n# length of KDTree.__getstate__()\n# Check the source code of KDTree at\n# https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/neighbors/binary_tree.pxi\nKDTREE_STATE_LEN = 12\n\n\nclass KDTreeH5(KDTree):\n    def dump(self, file):\n        """"""\n        file: str or HDF group\n\n        Examples\n        --------\n        # dump KDTree object\n        KDTreeH5.dump(tree, filepath)\n        """"""\n        if not isinstance(file, h5py.Group):\n            file = h5py.File(file)\n\n        state = list(self.__getstate__())\n        assert len(state) == KDTREE_STATE_LEN\n\n        # convert dist_metric to string for hdf5 storage\n        state[-1] = pickle.dumps(state[-1])\n        for i, v in enumerate(state):\n            file[str(i)] = v\n\n    @classmethod\n    def load(cls, file):\n        """"""\n        file: str or HDF group\n        """"""\n        if not isinstance(file, h5py.Group):\n            file = h5py.File(file, \'r\')\n\n        state = [None] * len(file)\n        assert len(state) == KDTREE_STATE_LEN\n\n        for i, _ in enumerate(state):\n            state[i] = file[str(i)].value\n        # recover dist_metric from string\n        state[-1] = pickle.loads(state[-1])\n\n        obj = cls.__new__(cls)\n        obj.__setstate__(state)\n        return obj\n'"
helper.py,2,"b'from __future__ import division, print_function, absolute_import\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import axes, docstring\n\n__all__ = [\'axtext\', \'mulegend\', \'errorbar2\', \'get_aspect\']\n\n\n@docstring.copy_dedent(axes.Axes.text)\ndef axtext(x, y, s, *args, **kwargs):\n    ax = plt.gca()\n    kwargs.setdefault(\'transform\', ax.transAxes)\n    return ax.text(x, y, s, *args, **kwargs)\n\n\n@docstring.copy_dedent(axes.Axes.legend)\ndef mulegend(*args, **kwargs):\n    """"""Multiple legend\n    """"""\n    ax = plt.gca()\n    ret = plt.legend(*args, **kwargs)\n    ax.add_artist(ret)\n    return ret\n\n\ndef errorbar2(x, y, yerr=None, xerr=None, **kwds):\n    if yerr is not None:\n        assert len(yerr) == 2\n        ymin, ymax = np.atleast_1d(*yerr)\n        yerr = y - ymin, ymax - y\n    if xerr is not None:\n        assert len(xerr) == 2\n        xmin, xmax = np.atleast_1d(*xerr)\n        xerr = x - xmin, xmax - x\n    return plt.errorbar(x, y, yerr=yerr, xerr=xerr, **kwds)\n\n\ndef get_aspect(ax=None):\n    """"""get aspect of given axes\n    """"""\n    if ax is None:\n        ax = plt.gca()\n\n    A, B = ax.get_figure().get_size_inches()\n    w, h = ax.get_position().bounds[2:]\n    disp_ratio = (B * h) / (A * w)\n\n    sub = lambda x, y: x - y\n    data_ratio = sub(*ax.get_ylim()) / sub(*ax.get_xlim())\n\n    return disp_ratio / data_ratio\n'"
hist.py,49,"b'from __future__ import division, print_function, absolute_import\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom .stats import binstats, binquantile, generate_bins\nfrom .helper import errorbar2\n\n__all__ = [\'pcolorshow\', \'hist_stats\', \'hist2d_stats\', \'steps\',\n           \'cdfsteps\', \'pdfsteps\', \'compare\', \'compare_violin\',\n           \'compare_median\']\n\n\ndef _pcolorshow_args(x, m):\n    """"""Helper function for `pcolorshow`.\n    Check the shape of input and return its range.\n    """"""\n    if x.ndim != 1:\n        raise ValueError(""unexpected array dimensions"")\n    elif x.size > 1:\n        dx = x[1] - x[0]\n    else:\n        dx = 1\n\n    if not np.allclose(np.diff(x), dx):\n        raise ValueError(""the bin size must be equal."")\n\n    if x.size == m:\n        return np.min(x) - 0.5 * dx, np.max(x) + 0.5 * dx\n    elif x.size == m + 1:\n        return np.min(x), np.max(x)\n    else:\n        raise ValueError(""unexpected array shape"")\n\n\ndef pcolorshow(*args, **kwargs):\n    """"""pcolorshow([x, y], z, interpolation=\'nearest\', **kwargs)\n    similar to pcolormesh but using `imshow` as backend.\n    It renders faster than `pcolor(mesh)` and supports more interpolation\n    schemes, but only works with equal bins.\n\n    Parameters\n    ----------\n    x, y : array like, optional\n        Coordinates of bins.\n    z :\n        The color array. z should be in shape (ny, nx) or (ny + 1, nx + 1)\n        when x, y are given.\n    interpolation : string, optional\n        Acceptable values are \'nearest\', \'bilinear\', \'bicubic\',\n        \'spline16\', \'spline36\', \'hanning\', \'hamming\', \'hermite\', \'kaiser\',\n        \'quadric\', \'catrom\', \'gaussian\', \'bessel\', \'mitchell\', \'sinc\',\n        \'lanczos\'\n    vmin, vmax : scalar, optional, default: None\n        `vmin` and `vmax` are used in conjunction with norm to normalize\n        luminance data.  Note if you pass a `norm` instance, your\n        settings for `vmin` and `vmax` will be ignored.\n\n    Example\n    -------\n    a = np.arange(10)\n    pcolorshow(a, 0.5, a)\n    """"""\n    z = np.atleast_2d(args[-1])\n    n, m = z.shape\n\n    if len(args) == 1:\n        xmin, xmax = 0, m\n        ymin, ymax = 0, n\n    elif len(args) == 3:\n        x, y = np.atleast_1d(*args[:2])\n        xmin, xmax = _pcolorshow_args(x, m)\n        ymin, ymax = _pcolorshow_args(y, n)\n    else:\n        raise ValueError(""should input `x, y, z` or `z`"")\n\n    kwargs.setdefault(""origin"", \'lower\')\n    kwargs.setdefault(""aspect"", plt.gca().get_aspect())\n    kwargs.setdefault(""extent"", (xmin, xmax, ymin, ymax))\n    kwargs.setdefault(\'interpolation\', \'nearest\')\n\n    return plt.imshow(z, **kwargs)\n\n\ndef hist_stats(x, y, bins=10, func=np.mean, nmin=1, style=""plot"", **kwargs):\n    """"""\n    Similar to `plt.hist` but show the binned statistics instead of\n    simple number count.\n\n    Parameters\n    ----------\n    x, y, bins, func, nmin :\n        See doc of `binstats`.\n    style : {\'plot\' | \'scatter\' | \'step\'}\n        Style of line.\n    kwargs :\n        Parameters for style above.\n\n    Example\n    -------\n    import numpy as np\n    n = 10000\n    x, s = np.random.randn(2, n)\n    y = x * 2 + s / 2\n    hist_stats(x, y, func=lambda x:np.percentile(x, [50, 15, 85]),\n            ls=[\'-\', \'--\', \'--\'], lw=[2, 1, 1], color=[\'k\', \'b\', \'b\'])\n    """"""\n    stats, edges, count = binstats(x, y, bins=bins, func=func, nmin=nmin)\n    stats = np.atleast_2d(stats)\n    assert len(edges) == 1\n    assert stats.ndim == 2\n\n    style_dict = {\'plot\': plt.plot,\n                  \'scatter\': plt.scatter,\n                  \'step\': steps}\n    plot = style_dict[style]\n\n    if style == \'step\':\n        X = edges[0]\n    else:\n        X = (edges[0][:-1] + edges[0][1:]) / 2.\n\n    lines = []\n    for i, Y in enumerate(stats):\n        args = {k: (v if np.isscalar(v) else v[i])\n                for k, v in kwargs.items()}\n        lines += plot(X, Y, **args)\n    return lines\n\n\ndef hist2d_stats(x, y, z, bins=10, func=np.mean, nmin=1, **kwargs):\n    """"""\n    Similar to `plt.hist2d` but show the binned statistics instead of\n    simple number count.\n\n    Parameters\n    ----------\n    x, y :\n        Coordinates of points.\n    z :\n        Data for statistics. \n    bins, func, nmin :\n        See doc of `binstats`.\n    kwargs :\n        `pcolormesh` parameters\n\n    """"""\n    stats, edges, count = binstats([x, y], z, bins=bins, func=func, nmin=nmin)\n    assert len(edges) == 2\n    assert stats.ndim == 2\n\n    (X, Y), Z = edges, stats.T\n    mask = ~np.isfinite(Z)\n    Z = np.ma.array(Z, mask=mask)\n    kwargs.setdefault(\'vmin\', Z.min())\n    kwargs.setdefault(\'vmax\', Z.max())\n    return plt.pcolormesh(X, Y, Z, **kwargs)\n\n\ndef steps(x, y, *args, **kwargs):\n    """"""steps(x, y, *args, style=\'line\', bottom=0, guess=True, \n             orientation=\'vertical\', **kwargs)\n    Make a step plot.\n    The interval from x[i] to x[i+1] has level y[i]\n    This function is useful for show the results of np.histogram.\n\n    Parameters\n    ----------\n    x, y : 1-D sequences\n        Data to plot.\n        - If len(x) == len(y) + 1\n            y keeps y[i] at interval from x[i] to x[i+1].\n        - If len(x) == len(y)\n            y jumps from y[i] to y[i+1] at (x[i] + x[i+1])/2.\n    style : [\'default\' | \'step\' | \'filled\' | \'bar\' | \'line\'], optional\n        The type of steps to draw.\n        - \'default\': step line plot\n        - \'step\': step line with vertical line at borders.\n        - \'filled\': filled step line plot\n        - \'bar\': traditional bar-type histogram\n        - \'line\': polygonal line\n        See the example below for a visual explanation.\n    bottom : float\n        The bottom baseline of the plot.\n    guess : bool\n        Option works only for case len(x) == len(y).\n        If True, the marginal bin edges of x will be guessed \n        with assuming equal bin. Otherwise x[0], x[-1] are used.\n    orientation : [\'horizontal\' | \'vertical\'], optional\n        Orientation.\n    args, kwargs :\n        Same as `plt.plot` if `style` in [\'default\', \'step\', \'line\'], or\n        same as `plt.fill` if `style` in [\'filled\', \'bar\'].\n\n    Example\n    -------\n    np.random.seed(1)\n    a = np.random.rand(50)\n    b = np.linspace(0.1, 0.9, 6)\n    h, bins = np.histogram(a, b)\n    for i, style in enumerate([\'default\', \'step\', \'filled\', \'bar\', \'line\']):\n        steps(bins + i, h, style=style, lw=2, bottom=1)\n        plt.text(i + 0.5, 14, style)\n    plt.xlim(0, 5)\n    plt.ylim(-1, 16)\n    """"""\n    style = kwargs.pop(\'style\', \'default\')\n    bottom = kwargs.pop(\'bottom\', 0)\n    guess = kwargs.pop(\'guess\', True)\n    orientation = kwargs.pop(\'orientation\', \'vertical\')\n\n    # a workaround for case \'line\'\n    if style == \'line\':\n        guess = True\n\n    m, n = len(x), len(y)\n    if m == n:\n        if guess and m >= 2:\n            xmin, xmax = x[0] * 1.5 - x[1] * 0.5, x[-1] * 1.5 - x[-2] * 0.5\n        else:\n            xmin, xmax = x[0], x[-1]\n        x = np.hstack([xmin, (x[1:] + x[:-1]) * 0.5, xmax])\n    elif m == n + 1:\n        pass\n    else:\n        raise ValueError(""x, y shape not matched."")\n\n    if style == \'default\':\n        x, y = np.repeat(x, 2), np.repeat(y, 2)\n        x = x[1:-1]\n    elif style in [\'step\', \'filled\']:\n        x, y = np.repeat(x, 2), np.repeat(y, 2)\n        y = np.hstack([bottom, y, bottom])\n    elif style == \'bar\':\n        x, y = np.repeat(x, 3), np.repeat(y, 3)\n        x, y = x[1:-1], np.hstack([y, bottom])\n        y[::3] = bottom\n    elif style == \'line\':\n        x = (x[1:] + x[:-1]) / 2\n    else:\n        raise ValueError(""invalid style: %s"" % style)\n\n    if orientation == \'vertical\':\n        pass\n    elif orientation == \'horizontal\':\n        x, y = y, x\n    else:\n        raise ValueError(""orientation must be `vertical` or `horizontal`"")\n\n    if style in [\'default\', \'step\', \'line\']:\n        return plt.plot(x, y, *args, **kwargs)\n    else:\n        return plt.fill(x, y, *args, **kwargs)\n\n\ndef cdfsteps(x, *args, **kwargs):\n    """"""cdfsteps(x, *args, weights=None, side=\'left\', \n                normed=True, sorted=Fasle, **kwargs)\n\n    Parameters\n    ----------\n    x:\n        Input.\n    weights : array, Optional\n        Weighting.\n    side: [\'left\' | \'right\']\n        \'left\': ascending steps,\n        \'right\' : descending steps.\n    normed: bool\n        If normalize to 1.\n    sorted: bool\n        Set True, if x follows increasing order.\n        Otherwise, sorting will be performed to x.\n    """"""\n    weights = kwargs.pop(\'weights\', None)\n    side = kwargs.pop(\'side\', \'left\')\n    normed = kwargs.pop(\'normed\', True)\n    sorted = kwargs.pop(\'sorted\', False)\n    assert side in [\'right\', \'left\']\n\n    x = np.asarray(x).ravel()\n    if not sorted:\n        x = np.sort(x)\n\n    if weights is None:\n        h = np.arange(0., x.size + 1.)\n    else:\n        h = np.hstack([0., np.cumsum(weights)])\n    if normed:\n        h = h / h[-1]\n    if side == \'right\':\n        h = h[::-1]\n    x = np.hstack([x[0], x, x[-1]])\n    return steps(x, h, *args, **kwargs)\n\n\ndef pdfsteps(x, *args, **kwds):\n    sorted = kwds.pop(\'sorted\', False)\n\n    x = np.asarray(x).ravel()\n    if not sorted:\n        x = np.sort(x)\n    h = 1. / x.size / np.diff(x)\n    return steps(x, h, *args, border=True, **kwds)\n\n\ndef _expand_args(args, i_idx, j_key):\n    """"""Helper function for `compare`.\n    Expand the args for given index.\n    """"""\n    res = {}\n    for k, v in args.items():\n        if np.isscalar(v):\n            res[k] = v\n        elif isinstance(v, dict):\n            res[k] = v[j_key]\n        else:\n            res[k] = v[i_idx]\n    return res\n\n\ndef compare(x, y, xbins=None, ybins=None, weights=None, nmin=3, nanas=None,\n            dots=[0], ebar=[], line=[0, 1, 2], fill=[], zorder=2,\n            dots_args={}, ebar_args={}, fill_args={}, **line_args):\n    """"""Show the correlation between two data sets.\n    Plot the median and 1, 2 sigma regions of the conditional distribution\n    p(y|x) for given x bins or p(x|y) for given y bins.\n\n    Parameters\n    ----------\n    x, y : 1-D sequences\n        Data sets to compare.\n    xbins, ybins : int or 1-D sequences\n        Binning edges. Only one of them can be given.\n    weights :\n        Weights of data.\n    nmin, nanas :\n        See doc of `binquantile`.\n\n    Example\n    -------\n    import numpy as np\n    n = 10000\n    x, s = np.random.randn(2, n)\n    y = x * 2 + s / 2\n    compare(x, y, 10, dots=0, line=[0, 1], fill=1, ebar=1)\n    """"""\n    # format inputs\n    x, y = np.asarray(x).ravel(), np.asarray(y).ravel()\n    if weights is not None:\n        weights = np.asarray(weights).ravel()\n\n    if ybins is None:\n        if xbins is None:\n            xbins = 10\n        w, z, bins = x, y, xbins\n    else:\n        if xbins is not None:\n            raise ValueError(""Only one of \'xbins\' or \'ybins\' can be given."")\n        w, z, bins = y, x, ybins\n\n    dots = [dots] if np.isscalar(dots) else dots\n    ebar = [ebar] if np.isscalar(ebar) else ebar\n    line = [line] if np.isscalar(line) else line\n    fill = [fill] if np.isscalar(fill) else fill\n    if 0 in ebar or 0 in fill:\n        raise ValueError(""`ebar` and `fill` can only set to 1 or 2"")\n\n    # prepare data\n    zs = binquantile(w, z, bins=bins, nsig=[0, -1, -2, 1, 2], shape=\'stats\',\n                     weights=weights, nmin=nmin, nanas=nanas).stats\n    ws = binquantile(w, w, bins=bins, q=0.5,\n                     weights=weights, nmin=nmin, nanas=nanas).stats\n\n    # default style\n    dots_args.setdefault(\'s\', 20)\n    dots_args.setdefault(\'c\', \'k\')\n    dots_args.setdefault(\'edgecolor\', \'none\')\n    dots_args.setdefault(\'zorder\', zorder + 0.3)\n\n    ebar_args.setdefault(\'ecolor\', {1: \'k\', 2: \'c\'})\n    ebar_args.setdefault(\'fmt\', \'none\')\n    ebar_args.setdefault(\'zorder\', zorder + 0.2)\n\n    line_args.setdefault(\'fmt\', {0: \'k-\', 1: \'b--\', 2: \'g-.\'})\n    line_args.setdefault(\'zorder\', zorder)\n\n    fill_args.setdefault(\'color\', {1: \'b\', 2: \'g\'})\n    fill_args.setdefault(\'alpha\', {1: 0.5, 2: 0.3})\n    fill_args.setdefault(\'edgecolor\', \'none\')\n    fill_args.setdefault(\'zorder\', zorder - 1)\n\n    # prepare plots\n    ax = plt.gca()\n    if xbins is not None:\n        xs, ys = [ws] * 5, zs\n        fill_between = ax.fill_between\n        err = \'yerr\'\n    else:\n        xs, ys = zs, [ws] * 5\n        fill_between = ax.fill_betweenx\n        err = \'xerr\'\n\n    # dots\n    for i, k in enumerate(dots):\n        args = _expand_args(dots_args, i, k)\n        ax.scatter(xs[k], ys[k], **args)\n\n    # ebar\n    for i, k in enumerate(ebar):\n        args = _expand_args(ebar_args, i, k)\n        args[err] = zs[0] - zs[k], zs[k + 2] - zs[0]\n        args[\'zorder\'] = args[\'zorder\'] + 0.1 * (1.5 - k)\n        ax.errorbar(xs[0], ys[0], **args)\n\n    # line\n    for i, k in enumerate(line):\n        args = _expand_args(line_args, i, k)\n        fmt = args.pop(\'fmt\', \'\')\n        ax.plot(xs[k], ys[k], fmt, **args)\n        if k != 0:\n            args.pop(\'label\', None)\n            ax.plot(xs[k + 2], ys[k + 2], fmt, **args)\n\n    # fill\n    for i, k in enumerate(fill):\n        args = _expand_args(fill_args, i, k)\n        fill_between(ws, zs[k], zs[k + 2], **args)\n\n    return\n\n\ndef compare_violin(x, y, xbins=None, ybins=None, nmin=1, nmax=10000,\n                   xpos=\'median\', side=\'both\', widths=0.5, violin_args={},\n                   ebar_args={}, **fill_args):\n    """"""Show the conditional violin plot for two data sets.\n\n    xpos : [\'center\'|\'median\']\n    """"""\n    violin_args = violin_args.copy()\n    violin_args.setdefault(\'vert\', True)\n    violin_args.setdefault(\'showmedians\', True)\n    # violin_args.setdefault(\'showextrema\', False)\n\n    if ybins is None:\n        if xbins is None:\n            xbins = 10\n    else:\n        if xbins is not None:\n            raise ValueError(""Only one of \'xbins\' or \'ybins\' can be given."")\n        violin_args[\'vert\'] = not violin_args[\'vert\']\n        return compare_violin(y, x, xbins=ybins, nmin=nmin, nmax=nmax, side=side,\n                              widths=widths, violin_args=violin_args,\n                              ebar_args=ebar_args, **fill_args)\n\n    nmin, nmax = int(nmin), int(nmax)\n\n    ix = (np.isfinite(x) & np.isfinite(y)).nonzero()\n    x, y = x[ix], y[ix]\n\n    bins = generate_bins(x, xbins)\n    bins_mid = (bins[:-1] + bins[1:]) * 0.5\n\n    idx = np.searchsorted(bins, x, side=\'right\') - 1\n    dat, pos = [], []\n    for i in range(len(bins) - 1):\n        ix = (idx == i).nonzero()[0]\n        if ix.size >= nmin:\n            if xpos == \'center\':\n                a, b = y[ix], bins_mid[i]\n            elif xpos == \'median\':\n                a, b = y[ix], np.median(x[ix])\n            if a.size > nmax:\n                a = np.random.choice(a, nmax, replace=False)\n            dat.append(a)\n            pos.append(b)\n\n    collection = plt.violinplot(\n        dataset=dat, positions=pos, widths=widths, **violin_args)\n\n    if side in [\'left\', \'right\', \'bottom\', \'top\']:\n        # https://stackoverflow.com/a/29781988/\n        for body in collection[\'bodies\']:\n            if violin_args[\'vert\']:\n                p = body.get_paths()[0].vertices[:, 0]\n            else:\n                p = body.get_paths()[0].vertices[:, 1]\n\n            if side in [\'left\', \'bottom\']:\n                p[:] = np.clip(p, None, p.mean())\n            else:\n                p[:] = np.clip(p, p.mean(), None)\n\n    for key, value in collection.items():\n        if key == \'bodies\':\n            if fill_args:\n                plt.setp(value, **fill_args)\n        else:\n            if ebar_args:\n                plt.setp(value, **ebar_args)\n    return collection\n\n\ndef compare_median(x, y, bins=10, nmin=3, alpha=0.33, show=[\'line\', \'fill\'],\n                   fill_args={}, ebar_args={}, **line_args):\n    """"""\n    alpha : float, optional\n        Confidence level of the intervals.\n    """"""\n    from scipy.stats.mstats import median_cihs, hdmedian\n\n    bins = generate_bins(x, bins)\n    nbins = len(bins) - 1\n    idx = bins.searchsorted(x, side=\'right\') - 1\n\n    xx, yy, lo, hi = np.full((4, nbins), np.nan, \'float\')\n\n    for i in range(nbins):\n        ix = (idx == i).nonzero()\n        if ix[0].size < nmin:\n            continue\n        x_, y_ = x[ix], y[ix]\n        xx[i], yy[i] = np.median(x_), np.median(y_)\n        lo[i], hi[i] = median_cihs(y_, alpha=alpha)\n\n        # an alternative variance estimator is hdmedian:\n        # sig = hdmedian(y_, var=True).data[1]**0.5\n\n    if \'line\' in show:\n        plt.plot(xx, yy, **line_args)\n    if \'fill\' in show:\n        fill_args.setdefault(\'alpha\', 0.5)\n        #fill_args.setdefault(\'edgecolor\', \'none\')\n        plt.fill_between(xx, lo, hi, **fill_args)\n    if \'ebar\' in show:\n        errorbar2(xx, yy, (lo, hi), **ebar_args)\n    return xx, yy, lo, hi\n'"
integrate.py,20,"b'from __future__ import division\nimport numpy as np\n\n__all__ = [\'trapz1d\', \'simps1d\', \'trapz2d\', \'simps2d\']\n\n""""""\nImplementation notes\n    http://mathfaculty.fullerton.edu/mathews/n2003/SimpsonsRule2DMod.html\n\nTiming\n```\nimport os\nos.environ[""OMP_NUM_THREADS""] = ""1""\nos.environ[""MKL_NUM_THREADS""] = ""1""\n\nimport numpy as np\nfrom scipy.integrate import trapz, simps\nfrom handy import trapz1d, simps1d, trapz2d, simps2d\na = np.random.rand(1001, 1025)\nb = np.random.rand(1000, 1024)\n\n# 1d\nassert np.allclose(trapz(a), trapz1d(a), atol=0, rtol=1e-8)\nassert np.allclose(simps(a), simps1d(a), atol=0, rtol=1e-8)\n\n# 1d even\nassert np.allclose(trapz(b), trapz1d(b), atol=0, rtol=1e-8)\nassert np.allclose(simps(b), simps1d(b), atol=0, rtol=1e-8)\nassert np.allclose(simps(b, even=\'first\'), simps1d(b, even=\'first\'), atol=0, rtol=1e-8)\nassert np.allclose(simps(b, even=\'last\'), simps1d(b, even=\'last\'), atol=0, rtol=1e-8)\nassert np.allclose(simps(b, axis=0, even=\'first\'), simps1d(b, axis=0, even=\'first\'), atol=0, rtol=1e-8)\nassert np.allclose(simps(b, axis=0, even=\'last\'), simps1d(b, axis=0, even=\'last\'), atol=0, rtol=1e-8)\n\n# 2d\nassert np.allclose(trapz2d(a), trapz1d(trapz1d(a)), atol=0, rtol=1e-8)\nassert np.allclose(simps2d(a), simps1d(simps1d(a)), atol=0, rtol=1e-8)\n\na = np.random.rand(100, 1001, 1025)\n%timeit np.sum(a)\n%timeit trapz(a)\n%timeit simps(a)\n%timeit trapz1d(a)\n%timeit simps1d(a)\n```\n""""""\n\n\ndef slice_set(ix, ndim, axis):\n    ix_list = [slice(None)] * ndim\n    ix_list[axis] = ix\n    return tuple(ix_list)\n\n\ndef trapz1d(y, dx=1.0, axis=-1):\n    y = np.asarray(y)\n    ndim = y.ndim\n\n    ix0 = slice_set(0, ndim, axis)\n    ix1 = slice_set(-1, ndim, axis)\n    ix2 = slice_set(slice(1, -1), ndim, axis)\n\n    out = ((y[ix0] + y[ix1]) + 2 * y[ix2].sum(axis)) * (dx / 2)\n    return out\n\n\ndef simps1d(y, dx=1.0, axis=-1, even=\'avg\'):\n    y = np.asarray(y)\n    ndim = y.ndim\n\n    # when shape of y is odd\n    if y.shape[axis] % 2 == 1:\n        ix0 = slice_set(0, ndim, axis)\n        ix1 = slice_set(-1, ndim, axis)\n        ixo = slice_set(slice(1, -1, 2), ndim, axis)  # odd\n        ixe = slice_set(slice(2, -2, 2), ndim, axis)  # even\n        out = (y[ix0] + y[ix1] + 4 * y[ixo].sum(axis) + 2 * y[ixe].sum(axis)) * (dx / 3)\n        return out\n    elif even == \'avg\':\n        ix0 = slice_set(0, ndim, axis)\n        ix1 = slice_set(-1, ndim, axis)\n        ix2 = slice_set(1, ndim, axis)\n        ix3 = slice_set(-2, ndim, axis)\n        ix4 = slice_set(slice(2, -2), ndim, axis)\n        out = (2.5 * (y[ix0] + y[ix1]) + 6.5 * (y[ix2] + y[ix3]) +\n               6 * y[ix4].sum(axis)) * (dx / 6)\n        return out\n    elif even == \'first\':\n        ix0 = slice_set(-1, ndim, axis)\n        ix1 = slice_set(-2, ndim, axis)\n        ix3 = slice_set(slice(None, -1), ndim, axis)\n        return simps1d(y[ix3], dx, axis) + 0.5 * dx * (y[ix0] + y[ix1])\n    elif even == \'last\':\n        ix0 = slice_set(0, ndim, axis)\n        ix1 = slice_set(1, ndim, axis)\n        ix3 = slice_set(slice(1, None), ndim, axis)\n        return simps1d(y[ix3], dx, axis) + 0.5 * dx * (y[ix0] + y[ix1])\n    else:\n        raise ValueError(""\'even\' must be one of \'avg\', \'first\' or \'last\'"")\n\n\ndef sum2d(a):\n    """"""sum of last two dimensions\n    """"""\n    return a.reshape(*a.shape[:-2], -1).sum(-1)\n\n\ndef trapz2d(z, dx=1, dy=1):\n    """"""integrate over last two dimensions\n\n    >>> trapz2d(np.ones((5, 5)))\n    16.0\n    """"""\n    z = np.asarray(z)\n    ix = slice(1, -1)\n\n    s1 = (z[..., 0, 0] + z[..., 0, -1] + z[..., -1, 0] + z[..., -1, -1])\n    s2 = 2 * (z[..., 0, ix].sum(-1) + z[..., -1, ix].sum(-1) +\n              z[..., ix, 0].sum(-1) + z[..., ix, -1].sum(-1))\n    s3 = 4 * sum2d(z[..., ix, ix])\n\n    out = (s1 + s2 + s3) * (dx * dy / 4)\n    return out\n\n\ndef simps2d(z, dx=1, dy=1):\n    """"""integrate over last two dimensions\n\n    >>> simps2d(np.ones((5, 5)))\n    16.0\n    """"""\n    z = np.asarray(z)\n    nx, ny = z.shape[-2:]\n    if nx % 2 != 1 or ny % 2 != 1:\n        raise ValueError(\'input array should be odd shape\')\n\n    ixo = slice(1, -1, 2)  # odd\n    ixe = slice(2, -2, 2)  # even\n\n    # corner points, with weight 1\n    s1 = (z[..., 0, 0] + z[..., 0, -1] + z[..., -1, 0] + z[..., -1, -1])\n\n    # edges excluding corners, with weight 2 or 4\n    s2 = 2 * (z[..., 0, ixe].sum(-1) + z[..., -1, ixe].sum(-1) +\n              z[..., ixe, 0].sum(-1) + z[..., ixe, -1].sum(-1))\n    s3 = 4 * (z[..., 0, ixo].sum(-1) + z[..., -1, ixo].sum(-1) +\n              z[..., ixo, 0].sum(-1) + z[..., ixo, -1].sum(-1))\n\n    # interior points, with weight 4, 8 or 16\n    s4 = (4 * sum2d(z[..., ixe, ixe]) + 16 * sum2d(z[..., ixo, ixo]) +\n          8 * sum2d(z[..., ixe, ixo]) + 8 * sum2d(z[..., ixo, ixe]))\n\n    out = (s1 + s2 + s3 + s4) * (dx * dy / 9)\n    return out\n'"
interpolate.py,40,"b'from __future__ import division, print_function, absolute_import\nimport numpy as np\nfrom scipy import ndimage\n\n__all__ = [""interp_grid"", ""EqualGridInterpolator""]\n\n\ndef interp_grid(coord, grids, value, order=1, padding=\'constant\',\n                fill_value=np.nan):\n    """"""Interpolation on a equal spaced regular grid in arbitrary dimensions.\n\n    Parameters\n    ----------\n    coord : tuple of ndarray\n        The coordinates to interpolate.\n    grids : tuple of ndarray, shape (m1,), ..., (mn,)\n        The points defining the equal regular grid in n dimensions.\n    value : array_like, shape (m1, ..., mn)\n        The data on the regular grid in n dimensions.\n    order : int\n        The order of the spline interpolation, default is 1.\n        The order has to be in the range [0, 5].\n        0 means nearest interpolation.\n    padding : str\n        Points outside the boundaries of the input are filled according\n        to the given mode (\'constant\', \'nearest\', \'reflect\' or \'wrap\').\n    fill_value : number, optional\n        If provided, the value to use for points outside of the \n        interpolation domain.\n\n    Examples\n    --------\n    1D example:\n        x = np.linspace(-3, 3, 2000)\n        y = np.sin(x)\n        xi = np.random.rand(10000) * 6 -3\n        yi = interp_grid(xi, x, y)\n\n        y2 = np.interp(xi, x, y)\n        print(np.allclose(yi, y2))\n        # True\n\n        # Timing\n        %timeit -n10 -r1 interp_grid(xi, x, y)\n        %timeit -n10 -r1 np.interp(xi, x, y)\n\n    2D example:\n        f = lambda x, y: x**2- y**2\n        x, y = np.linspace(-2, 3, 5), np.linspace(-3, 2, 6)\n        z = f(*np.meshgrid(x, y, indexing=\'ij\'))\n\n        xi, yi = np.random.rand(100), np.random.rand(100)\n        zi = interp_grid((xi, yi), (x, y), z, order=1)\n\n    See also\n    --------\n    numpy.interp\n    scipy.interpolate.RegularGridInterpolator\n    scipy.ndimage.map_coordinates\n\n    References\n    ----------\n    NI_GeometricTransform at\n        https://github.com/scipy/scipy/blob/master/scipy/ndimage/src/ni_interpolation.c\n    """"""\n    #coord = [np.asarray(xi) for xi in coord]\n\n    if value.ndim == 1:\n        if len(coord) != 1:\n            coord = [coord]\n        if grids is not None and len(grids) != 1:\n            grids = [grids]\n\n    if grids is None:\n        xi = coord\n    else:\n        xi = [(x - b[0]) / (b[1] - b[0]) for x, b in zip(coord, grids)]\n\n    if len(xi) == 1:\n        xi = xi[0][np.newaxis]\n    else:\n        xi = np.asarray(xi)\n\n    yi = ndimage.map_coordinates(value, xi, order=order, mode=padding,\n                                 cval=fill_value)\n    return yi\n\n\nclass EqualGridInterpolator(object):\n    """"""\n    Interpolation on a equal spaced regular grid in arbitrary dimensions.\n    Fock from https://github.com/JohannesBuchner/regulargrid\n    """"""\n\n    def __init__(self, points, values, order=1, padding=\'constant\',\n                 fill_value=np.nan):\n        \'\'\'\n        Parameters\n        ----------\n        points : tuple of ndarray, shape (m1, ), ..., (mn, )\n            The points defining the equal regular grid in n dimensions.\n        values : array_like, shape (m1, ..., mn)\n            The data on the regular grid in n dimensions.\n        order : int\n            The order of the spline interpolation, default is 1. \n            The order has to be in the range 0 to 5. \n            0 means nearest interpolation.\n        padding : str\n            Points outside the boundaries of the input are filled according\n            to the given mode (\'constant\', \'nearest\', \'reflect\' or \'wrap\').\n        fill_value : number, optional\n            If provided, the value to use for points outside of the \n            interpolation domain.\n\n        Examples\n        --------\n        import numpy as np\n        f = lambda x, y: 1 - x + y\n        x, y = np.linspace(-2, 3, 5), np.linspace(-3, 2, 6)\n        z = f(*np.meshgrid(x, y, indexing=\'ij\'))\n        f_interp = EqualGridInterpolator((x, y), z, order=1)\n\n        xi, yi = np.meshgrid(np.linspace(-2, 3, 50), np.linspace(-3, 2, 60),\n                             indexing=\'ij\')\n        zi_true = f(xi, yi)\n        zi_interp = f_interp([xi, yi])\n        np.allclose(zi_true, zi_interp)\n        \'\'\'\n        values = np.asfarray(values)\n        if len(points) != values.ndim:\n            raise ValueError(\'invalid shape for points array\')\n        points = [np.asarray(p) for p in points]\n\n        for i, p in enumerate(points):\n            if p.ndim != 1 or p.size <= 1:\n                raise ValueError(\'invalid shape for points array\')\n            if p[0] == p[1] or not np.allclose(np.diff(p), p[1] - p[0]):\n                raise ValueError(\'points array should be equally spaced!\')\n            if p.size != values.shape[i]:\n                raise ValueError(\'inconsistent shape for points and values\')\n\n        self.order = order\n        self.padding = padding\n        self.fill_value = fill_value\n\n        self.ndim = len(points)\n        self.grid = tuple(points)\n        self.values = values\n        self.edges = tuple([p[0] for p in points])\n        self.steps = tuple([p[1] - p[0] for p in points])\n        self.coeffs = {0: self.values, 1: self.values}\n\n    def __call__(self, xi, order=None):\n        \'\'\'\n        xi : tuple of ndarray\n            The coordinates to sample the gridded data at.\n        order : int\n            The order of the spline interpolation.\n        \'\'\'\n        if len(xi) != self.ndim:\n            raise ValueError(""input array has unmatched shape!"")\n        xi = [(xi[i] - self.edges[i]) / self.steps[i] for i in range(self.ndim)]\n        xi = np.broadcast_arrays(*xi)\n\n        xi = np.array(xi, dtype=\'float\')\n        scalar = (xi.ndim == 1)\n        if scalar:\n            xi = xi[..., np.newaxis]\n\n        order = self.order if order is None else order\n        values = self._coeffs(order)\n        yi = ndimage.map_coordinates(values, xi, order=order,\n                                     prefilter=False,\n                                     mode=self.padding,\n                                     cval=self.fill_value)\n        if scalar:\n            return yi[0]\n        else:\n            return yi\n\n    def _coeffs(self, order):\n        if order not in self.coeffs:\n            coeff = ndimage.spline_filter(self.values, order=order)\n            self.coeffs[order] = coeff\n        return self.coeffs[order]\n\n\nif __name__ == ""__main__"":\n    import numpy as np\n    from scipy.interpolate import RegularGridInterpolator\n    from matplotlib import pyplot as plt\n\n    # Example 1\n    f = lambda x, y: np.sin(x / 2) - np.sin(y)\n    x, y = np.linspace(-2, 3, 5), np.linspace(-3, 2, 6)\n    z = f(*np.meshgrid(x, y))\n\n    xi, yi = np.meshgrid(np.linspace(-2, 3, 50), np.linspace(-3, 2, 60))\n    zi1 = EqualGridInterpolator((x, y), z.T, order=1)((xi, yi))\n    zi2 = RegularGridInterpolator((x, y), z.T)((xi, yi))\n    assert np.allclose(zi1, zi2)\n    zi1 = EqualGridInterpolator((x, y), z.T, order=0)((xi, yi))\n    zi2 = RegularGridInterpolator((x, y), z.T, method=\'nearest\')((xi, yi))\n    assert np.allclose(zi1, zi2)\n\n    f = lambda x, y: x * y\n    mid = lambda x: (x[1:] + x[:-1]) / 2.\n    x = np.linspace(0, 2, 10)\n    y = np.linspace(0, 2, 15)\n    x_, y_ = mid(x), mid(y)\n    z = f(*np.meshgrid(x_, y_))\n\n    # Example 2\n    xi = np.linspace(0, 2, 40)\n    yi = np.linspace(0, 2, 60)\n    xi_, yi_ = mid(xi), mid(yi)\n\n    zi1 = EqualGridInterpolator((x_, y_), z.T, order=3)(np.meshgrid(xi_, yi_))\n    zi2 = EqualGridInterpolator((x_, y_), z.T)(np.meshgrid(xi_, yi_))\n    zi3 = EqualGridInterpolator((x_, y_), z.T, padding=\'nearest\')(np.meshgrid(xi_, yi_))\n\n    plt.figure(figsize=(9, 9))\n    plt.viridis()\n    plt.subplot(221)\n    plt.pcolormesh(x, y, z)\n    plt.subplot(222)\n    plt.pcolormesh(xi, yi, np.ma.array(zi1, mask=np.isnan(zi1)))\n    plt.subplot(223)\n    plt.pcolormesh(xi, yi, np.ma.array(zi2, mask=np.isnan(zi2)))\n    plt.subplot(224)\n    plt.pcolormesh(xi, yi, zi3)\n    plt.show()\n'"
kde.py,12,"b'import numpy as np\nfrom sklearn.neighbors import KDTree\nfrom numba import vectorize\n\nPI2 = np.pi * 2\nSQRT_PI2 = np.sqrt(PI2)\n\n\n@vectorize([""float64(float64, float64, float64)""])\ndef _norm1d(xi, x, xsig):\n    X = (xi - x) / xsig\n    p = np.exp(-0.5 * X**2) / (SQRT_PI2 * xsig)\n    return p\n\n\n@vectorize([""float64(float64, float64, float64, float64, float64, float64, float64)""])\ndef _norm2d(xi, yi, x, y, xsig, ysig, coef=0):\n    X, Y = (xi - x) / xsig, (yi - y) / ysig\n    if coef == 0:\n        p = np.exp(-0.5 * (X**2 + Y**2)) / (PI2 * xsig * ysig)\n    else:\n        coef2_1 = 1 - coef**2\n        p = np.exp(-0.5 * (X**2 + Y**2 - 2 * X * Y * coef) / coef2_1) / (PI2 * xsig * ysig * coef2_1**0.5)\n    return p\n\n\nclass AdapKDE1D:\n    def __init__(self, x, n_eps=2, n_ngb=None):\n        """"""\n        Only Gaussian kernel supported which is inefficiency for very large data set.\n\n        n_ngb:\n            sqrt(len(x)) by default.\n        """"""\n        x = np.ravel(x)\n        pts = x.reshape(-1, 1)\n\n        if n_ngb is None:\n            n_ngb = int(np.sqrt(len(x)))\n\n        eps = KDTree(pts).query(pts, n_ngb)[0].T[-1] / (n_ngb / 2)\n        xsig = n_eps * eps\n\n        self.x = x\n        self.xsig = xsig\n\n    def density(self, xi):\n        xi = xi[..., None]\n        p = _norm1d(xi, self.x, self.xsig)\n        return p.mean(-1)\n\n\nclass AdapKDE2D:\n    def __init__(self, x, y, n_eps=2, n_ngb=None, scale=1):\n        """"""\n        Only Gaussian kernel supported which is inefficiency for very large data set.\n\n        n_ngb:\n            sqrt(len(x)) by default.\n        scale: scalar\n            1 by default. Naively, one should use scale=y.std()/x.std().\n        """"""\n        x, y = np.ravel(x), np.ravel(y)\n\n        if scale == \'std\':\n            scale = y.std() / x.std()\n\n        if scale == 1:\n            pts = np.vstack([x, y]).T\n        else:\n            pts = np.vstack([x, y / scale]).T\n\n        if n_ngb is None:\n            n_ngb = int(np.sqrt(len(x)))\n\n        eps = KDTree(pts).query(pts, n_ngb)[0].T[-1] / np.sqrt(n_ngb / np.pi)\n        kern = n_eps * eps\n        xsig = kern\n        ysig = kern if scale == 1 else kern * scale\n\n        self.x = x\n        self.y = y\n        self.xsig = xsig\n        self.ysig = ysig\n\n    def density(self, xi, yi):\n        xi, yi = xi[..., None], yi[..., None]\n        p = _norm2d(xi, yi, self.x, self.y, self.xsig, self.ysig, 0.)\n        return p.mean(-1)\n\n    def density_mesh(self, xi, yi):\n        xi, yi = xi[..., None, None], yi[..., None, :, None]\n        p = _norm2d(xi, yi, self.x, self.y, self.xsig, self.ysig, 0.)\n        return p.mean(-1)\n'"
kdtree.py,10,"b'import numpy as np\nfrom itertools import product\nfrom collections import namedtuple\n\n__all__ = [\'query_radius_periodic\']\n\n\ndef repeat_periodic(points, boxsize):\n    """"""Repeat data to mock periodic boundaries.\n    points (m, n) -> repeated_points (m, 3**n, n)\n\n    Each point (x1, ..., xn) will have to 3**n copies:\n        (x1, ..., xn)\n        (x1, ..., xn-L)\n        (x1, ..., xn+L)\n        ...\n        (x1+L, ..., xn)\n        (x1+L, ..., xn-L)\n        (x1+L, ..., xn+L)\n    """"""\n    points = np.asarray(points)\n    ndim = points.shape[-1]\n\n    shift = np.array(list(product([0, -1, 1], repeat=ndim))) * boxsize\n    repeated_points = points[..., np.newaxis, :] + shift\n\n    return repeated_points\n\n\ndef query_radius_periodic(tree, points, radius, boxsize=None, merge=False):\n    """"""\n    tree: sklearn.neighbors.KDTree instance\n    points : array-like\n        An array of points to query.\n    radius : float or array-like\n        Distance within which neighbors are returned.\n    boxsize : float or array-like\n        Periodic boxsize.\n    merge : bool\n        If True, all outputs will be merged into single array.\n    """"""\n    ndim = tree.data.shape[-1]\n    nrep = 3**ndim\n    if points.shape[-1] != ndim:\n        raise ValueError(""Incompatible shape."")\n\n    if boxsize is None:\n        periodic = False\n    else:\n        periodic = True\n        points = repeat_periodic(points, boxsize=boxsize).reshape(-1, ndim)\n        if not np.isscalar(radius):\n            radius = np.repeat(radius, nrep)\n\n    idx, dis = tree.query_radius(points, radius, return_distance=True)\n    cnt = np.array(list(map(len, idx)))\n    if periodic:\n        cnt = cnt.reshape(-1, nrep).sum(-1)\n\n    if merge:\n        idx = np.concatenate(idx)\n        dis = np.concatenate(dis)\n    elif periodic:\n        idx = np.array(list(map(np.concatenate, idx.reshape(-1, nrep))))\n        dis = np.array(list(map(np.concatenate, dis.reshape(-1, nrep))))\n\n    type = namedtuple(\'KDTreeQuery\', [\'count\', \'index\', \'distance\'])\n    return type(cnt, idx, dis)\n'"
line.py,12,"b'from __future__ import division\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nfrom functools import wraps\n\n__all__ = [""abline"", ""ABLine2D"", ""axline""]\n\n\nclass ABLine2D(Line2D):\n    """"""\n    Draw a line based on a point and slope or two points.\n    Originally fock from http://stackoverflow.com/a/14348481/2144720 by ali_m\n    """"""\n\n    def __init__(self, a, b, *args, **kwargs):\n        """"""\n        a, b: scalar or tuple\n            Acceptable forms are\n            y0, b:\n                y = y0 + b * x\n            (x0, y0), b:\n                y = y0 + b * (x - x0)\n            (x0, y0), (x1, y1):\n                y = y0 + (y1 - y0) / (x1 - x0) * (x - x0)\n        Additional arguments are passed to the <matplotlib.lines.Line2D> constructor.\n\n        It will have wrong behavior when the axis-limits are\n        changed by setting ticks. This can be corrected by call\n        `xlim` to reset the limits.\n        Refer this issue:\n            https://github.com/matplotlib/matplotlib/issues/6863\n        """"""\n        if np.isscalar(a):\n            assert np.isscalar(b)\n            point = (0, a)\n            slope = b\n        elif np.isscalar(b):\n            assert len(a) == 2\n            point = a\n            slope = b\n        else:\n            assert len(a) == len(b) == 2\n            point = a\n            slope = (b[1] - a[1]) / np.float64(b[0] - a[0])\n            # use np.float64 to get inf when dividing by zero\n\n        assert ""transform"" not in kwargs\n        if \'axes\' in kwargs:\n            ax = kwargs[\'axes\']\n        else:\n            ax = plt.gca()\n        if not (\'color\' in kwargs or \'c\' in kwargs):\n            kwargs.update(next(ax._get_lines.prop_cycler))\n\n        super(ABLine2D, self).__init__([], [], *args, **kwargs)\n        self._point = tuple(point)\n        self._slope = float(slope)\n\n        # draw the line for the first time\n        ax.add_line(self)\n        self._auto_scaleview()\n        self._update_lim(None)\n\n        # connect to axis callbacks\n        self.axes.callbacks.connect(\'xlim_changed\', self._update_lim)\n        self.axes.callbacks.connect(\'ylim_changed\', self._update_lim)\n\n    def _update_lim(self, event):\n        """"""Update line range when the limits of the axes change.""""""\n        (x0, y0), b = self._point, self._slope\n        xlim = np.array(self.axes.get_xbound())\n        ylim = np.array(self.axes.get_ybound())\n        isflat = (xlim[1] - xlim[0]) * abs(b) <= (ylim[1] - ylim[0])\n        if isflat:\n            y = (xlim - x0) * b + y0\n            self.set_data(xlim, y)\n        else:\n            x = (ylim - y0) / b + x0\n            self.set_data(x, ylim)\n\n    def _auto_scaleview(self):\n        """"""Autoscale the axis view to the line.\n        This will make (x0, y0) in the axes range.\n        """"""\n        self.axes.plot(*self._point).pop(0).remove()\n\n\n@wraps(ABLine2D.__init__, assigned=[\'__doc__\'], updated=[])\ndef abline(a, b, *args, **kwargs):\n    return ABLine2D(a, b, *args, **kwargs)\n\n\ndef axline(a, b, **kwargs):\n    """"""\n    Add an infinite straight line across an axis.\n\n    Parameters\n    ----------\n    a, b: scalar or tuple\n        Acceptable forms are\n        y0, b:\n            y = y0 + b * x\n        (x0, y0), b:\n            y = y0 + b * (x - x0)\n        (x0, y0), (x1, y1):\n            y = y0 + (y1 - y0) / (x1 - x0) * (x - x0)\n    Additional arguments are passed to the <matplotlib.lines.Line2D> constructor.\n\n    Returns\n    -------\n    :class:`~matplotlib.lines.Line2D`\n\n    Other Parameters\n    ----------------\n    Valid kwargs are :class:`~matplotlib.lines.Line2D` properties,\n    with the exception of \'transform\':\n    %(Line2D)s\n\n    Examples\n    --------\n    * Draw a thick red line with slope 1 and y-intercept 0::\n        >>> axline(0, 1, linewidth=4, color=\'r\')\n    * Draw a default line with slope 1 and y-intercept 1::\n        >>> axline(1, 1)\n\n    See Also\n    --------\n    axhline : for horizontal lines\n    axvline : for vertical lines\n\n    Notes\n    -----\n    Currently this method does not work properly with log scaled axes.\n    Taken from https://github.com/matplotlib/matplotlib/pull/9321\n    """"""\n    from matplotlib import pyplot as plt\n    import matplotlib.transforms as mtransforms\n    import matplotlib.lines as mlines\n\n    if np.isscalar(a):\n        if not np.isscalar(b):\n            raise ValueError(""Invalid line parameters."")\n        point, slope = (0, a), b\n    elif np.isscalar(b):\n        if not len(a) == 2:\n            raise ValueError(""Invalid line parameters."")\n        point, slope = a, b\n    else:\n        if not len(a) == len(b) == 2:\n            raise ValueError(""Invalid line parameters."")\n        if b[0] != a[0]:\n            point, slope = a, (b[1] - a[1]) / (b[0] - a[0])\n        else:\n            point, slope = a, np.inf\n\n    ax = plt.gca()\n    if ""transform"" in kwargs:\n        raise ValueError(""\'transform\' is not allowed as a kwarg; ""\n                         ""axline generates its own transform."")\n\n    if slope == 0:\n        return ax.axhline(point[1], **kwargs)\n    elif np.isinf(slope):\n        return ax.axvline(point[0], **kwargs)\n\n    xtrans = mtransforms.BboxTransformTo(ax.viewLim)\n    viewLimT = mtransforms.TransformedBbox(\n        ax.viewLim,\n        mtransforms.Affine2D().rotate_deg(90).scale(-1, 1))\n    ytrans = (mtransforms.BboxTransformTo(viewLimT) +\n              mtransforms.Affine2D().scale(slope).translate(*point))\n    trans = mtransforms.blended_transform_factory(xtrans, ytrans)\n    line = mlines.Line2D([0, 1], [0, 1],\n                         transform=trans + ax.transData,\n                         **kwargs)\n    ax.add_line(line)\n    return line\n'"
misc.py,37,"b'from __future__ import division, print_function, absolute_import\nimport numpy as np\nfrom math import log10, floor\n\n\n__all__ = [\'slicer\', \'keys\', \'argmax_nd\', \'argmin_nd\', \'indexed\', \'argclip\', \'amap\',\n           \'atleast_nd\', \'dyadic\', \'altcumsum\', \'altcumprod\', \'siground\',\n           \'DictToClass\', \'DefaultDictToClass\']\n\n\nclass Slicer(object):\n    """"""Quick making slice object.\n\n    Examples\n    --------\n    slicer = Slicer()\n\n    slicer[0:5:1]\n    # equivalent to slice(0, 5, 1)\n\n    slicer[::1, ::2]\n    # equivalent to (slice(None, None, 1), slice(None, None, 2))\n    """"""\n\n    def __getitem__(self, slice):\n        return slice\n\n\nslicer = Slicer()\n\n\ndef keys(x):\n    if hasattr(x, \'keys\'):\n        return list(x.keys())\n    elif hasattr(x, \'dtype\'):\n        return x.dtype.names\n    else:\n        return list(x.__dict__.keys())\n\n\ndef argmax_nd(a, axis=None):\n    """"""Returns the indice of the maximum value.\n\n    Examples\n    --------\n    a = np.random.rand(3, 4, 5)\n    assert np.all(a[argmax_nd(a)] == a.max())\n    assert np.all(a[argmax_nd(a, axis=1)] == a.max(axis=1))\n    """"""\n    a = np.asarray(a)\n    ix = a.argmax(axis=axis)\n    if axis is None:\n        return np.unravel_index(ix, a.shape)\n    else:\n        shape = list(a.shape)\n        shape.pop(axis)\n        indices = list(np.indices(shape))\n        indices.insert(axis, ix)\n        return tuple(indices)\n\n\ndef argmin_nd(a, axis=None):\n    """"""Returns the indice of the minimum value.\n\n    Examples\n    --------\n    a = np.random.rand(3, 4, 5)\n    assert np.all(a[argmin_nd(a)] == a.min())\n    assert np.all(a[argmin_nd(a, axis=1)] == a.min(axis=1))\n    """"""\n    a = np.asarray(a)\n    ix = a.argmin(axis=axis)\n    if axis is None:\n        return np.unravel_index(ix, a.shape)\n    else:\n        shape = list(a.shape)\n        shape.pop(axis)\n        indices = list(np.indices(shape))\n        indices.insert(axis, ix)\n        return tuple(indices)\n\n\ndef indexed(x, y, missing=\'raise\', return_missing=False):\n    """"""Find elements in an un-sorted array.\n    Return index such that x[index] == y, the first index found is returned,\n    when multiple indices satisfy this condition.\n\n    Parameters\n    ----------\n    x : 1-D array_like\n        Input array.\n    y : array_like\n        Values to search in `x`.\n    missing : {\'raise\', \'ignore\', \'mask\' or int}\n        The elements of `y` are present in `x` is named missing.\n        If \'raise\', a ValueError is raised for missing elements.\n        If \'mask\', a masked array is returned, where missing elements are masked out.\n        If \'ignore\', no missing element is assumed, and output is undefined otherwise.\n        If integer, value set for missing elements.\n    return_missing : bool, optional\n        If True, also return the indices of the missing elements of `y`.\n\n    Returns\n    -------\n    indices : ndarray, [y.shape], int\n        The indices such that x[indices] == y\n    indices_missing : ndarray, [y.shape], optional\n        The indices such that y[indices_missing] not in x\n\n    See Also\n    --------\n    searchsorted : Find elements in a sorted array.\n\n    Notes\n    -----\n    This code is originally taken from\n    https://stackoverflow.com/a/8251757/2144720 by HYRY\n    https://github.com/EelcoHoogendoorn/Numpy_arraysetops_EP by Eelco Hoogendoorn\n    """"""\n    x, y = np.asarray(x), np.asarray(y)\n\n    x_index = np.argsort(x)\n    y_index_sorted = np.searchsorted(x[x_index], y, side=\'left\')\n    index = np.take(x_index, y_index_sorted, mode=""clip"")\n\n    if missing != \'ignore\' or return_missing:\n        invalid = x[index] != y\n\n    if missing != \'ignore\':\n        if missing == \'raise\':\n            if np.any(invalid):\n                raise ValueError(\'Not all elements in `y` are present in `x`\')\n        elif missing == \'mask\':\n            index = np.ma.array(index, mask=invalid)\n        else:\n            index[invalid] = missing\n\n    if return_missing:\n        return index, invalid\n    else:\n        return index\n\n\ndef argclip(a, amin=None, amax=None, closed=\'both\'):\n    """"""argclip(a, amin, amax) == (a >= amin) & (a <= amax)\n\n    Parameters\n    ----------\n    amin, amax : float\n    closed : {\'both\', \'left\', \'right\', \'none\'}\n    """"""\n    a = np.asarray(a)\n    if closed == \'both\':\n        gt_min, lt_max = np.greater_equal, np.less_equal\n    elif closed == \'left\':\n        gt_min, lt_max = np.greater_equal, np.less\n    elif closed == \'right\':\n        gt_min, lt_max = np.greater, np.less_equal\n    elif closed == \'none\':\n        gt_min, lt_max = np.greater, np.less\n    else:\n        raise ValueError(\n            ""keywords \'closed\' should be one of \'both\', \'left\', \'right\', \'none\'."")\n\n    if amin is None:\n        if amax is None:\n            return np.ones_like(a, dtype=\'bool\')\n        else:\n            return lt_max(a, amax)\n    else:\n        if amax is None:\n            return gt_min(a, amin)\n        else:\n            return gt_min(a, amin) & lt_max(a, amax)\n\n\ndef amap(func, *args):\n    \'\'\'Array version of build-in map\n    amap(function, sequence[, sequence, ...]) -> array\n    Examples\n    --------\n    >>> amap(lambda x: x**2, 1)\n    array(1)\n    >>> amap(lambda x: x**2, [1, 2])\n    array([1, 4])\n    >>> amap(lambda x,y: y**2 + x**2, 1, [1, 2])\n    array([2, 5])\n    >>> amap(lambda x: (x, x), 1)\n    array([1, 1])\n    >>> amap(lambda x,y: [x**2, y**2], [1,2], [3,4])\n    array([[1, 9], [4, 16]])\n    \'\'\'\n    args = np.broadcast(*args)\n    res = np.array([func(*arg) for arg in args])\n    shape = args.shape + res.shape[1:]\n    return res.reshape(shape)\n\n\ndef atleast_nd(a, nd, keep=\'right\'):\n    a = np.asanyarray(a)\n    if a.ndim < nd:\n        if keep == \'right\' or keep == -1:\n            shape = (1,) * (nd - a.ndim) + a.shape\n        elif keep == \'left\' or keep == 0:\n            shape = a.shape + (1,) * (nd - a.ndim)\n        else:\n            raise ValueError(""keep must be one of [\'left\', \'right\', 0, -1]"")\n        return a.reshape(shape)\n    else:\n        return a\n\n\ndef raise_dims(a, n=0, m=0):\n    a = np.asanyarray(a)\n    shape = (1,) * n + a.shape + (1,) * m\n    return a.reshape(shape)\n\n\ndef dyadic(a, b):\n    """"""Dyadic product.\n    a: shape (n1, ..., np)\n    b: shape (m1, ..., mq)\n    dyadic(a, b) : shape (n1, ..., np, m1, ..., mq)\n    """"""\n    a, b = np.asarray(a), np.asarray(b)\n    shape = a.shape + (1,) * b.ndim\n    return a.reshape(shape) * b\n\n\ndef shiftaxis(a, shift):\n    """"""Roll the dimensions of an array.\n    """"""\n    a = np.asarray(a)\n    if not -a.ndim <= shift < a.ndim:\n        raise ValueError(""shift should be in range [%d, %d)"" %\n                         (-a.ndim, a.ndim))\n    axes = np.roll(range(a.ndim), shift)\n    return a.transpose(axes)\n\n\ndef altcumsum(a, base=0, **kwargs):\n    out = np.cumsum(a, **kwargs)\n    if base is None:\n        return out\n    else:\n        out[1:] = base + out[:-1]\n        out[0] = base\n        return out\n\n\ndef altcumprod(a, base=1, **kwargs):\n    out = np.cumprod(a, **kwargs)\n    if base is None:\n        return out\n    else:\n        out[1:] = base * out[:-1]\n        out[0] = base\n        return out\n\n\ndef siground(x, n):\n    x, n = float(x), int(n)\n    if n <= 0:\n        raise ValueError(""n must be positive."")\n\n    if x == 0:\n        p = 0\n    else:\n        m = 10 ** floor(log10(abs(x)))\n        x = round(x / m, n - 1) * m\n        p = int(floor(log10(abs(x))))\n\n    if -3 < p < n:\n        return ""{:.{:d}f}"".format(x, n - 1 - p)\n    else:\n        return ""{:.{:d}f}e{:+d}"".format(x / 10**p, n - 1, p)\n\n\ndef find_numbers(string):\n    """"""http://stackoverflow.com/a/29581287\n    """"""\n    import re\n    return re.findall(""[-+]?\\d+[\\.]?\\d*[eE]?[-+]?\\d*"", string)\n\n\nclass DictToClass(object):\n    def __init__(self, *args, **kwds):\n        self.__dict__ = dict(*args, **kwds)\n\n\nclass DefaultDictToClass(object):\n    def __init__(self, default_factory, *args, **kwds):\n        from collections import defaultdict\n        self.__dict__ = defaultdict(default_factory, *args, **kwds)\n\n    def __getattr__(self, key):\n        return self.__dict__[key]\n\n\ndef is_scalar(x):\n    """"""\n    >>> np.isscalar(np.array(1))\n    False\n    >>> is_scalar(np.array(1))\n    True\n    """"""\n    if np.isscalar(x):\n        return True\n    elif isinstance(x, np.ndarray):\n        return not x.ndim\n    else:\n        return False\n        # return hasattr(x, ""__len__"")\n'"
optimize.py,12,"b'from __future__ import division, print_function, absolute_import\nimport numpy as np\n\n__all__ = [\'try_minimize\', \'findroot\']\n\n\ndef try_minimize(func, guess, args=(), method=None, quiet=False, timeout=5,\n                 unpack=False, max_show=10, **kwds):\n    \'\'\'Minimization of scalar function of one or more variables.\n    See the docstring of `scipy.optimize.minimize`.\n\n    Example\n    -------\n    from scipy.optimize import rosen\n    res = try_minimize(rosen, [0.5, 0.5])\n    \'\'\'\n    from scipy.optimize import minimize\n    from numpy import array2string\n    from time import clock\n    from .funcs import timeout as timer\n\n    guess = np.asarray(guess)\n    if unpack:\n        func_original = func\n        func = lambda x: func_original(*x)\n\n    if method is None:\n        methods = [\'Nelder-Mead\', \'Powell\', \'CG\', \'BFGS\', \'Newton-CG\',\n                   \'L-BFGS-B\', \'TNC\', \'COBYLA\', \'SLSQP\', \'dogleg\', \'trust-ncg\']\n    elif np.isscalar(method):\n        methods = [method]\n    else:\n        methods = method\n\n    results = []\n    for method in methods:\n        try:\n            time = clock()\n            if timeout > 0:\n                with timer(timeout, ValueError):\n                    res = minimize(func, guess, args=args, method=method, **kwds)\n            else:\n                res = minimize(func, guess, args=args, method=method, **kwds)\n            res.time = clock() - time\n            res.method = method\n            results.append(res)\n        except (ValueError, MemoryError, TypeError) as err:\n            if not quiet:\n                print(""{:>12s}: {}"".format(method, err))\n            continue\n\n    results.sort(key=lambda res: res.fun)\n    if not quiet:\n        print(""---------------------------------------------"")\n        param_len = min(guess.size, max_show) * 10 + 1\n        print(""{:>12s}  {:^5s}  {:^10s}  {:^{}s}  {:^s}"".format(\n            ""method"", ""OK"", ""fun"", ""x"", param_len, ""time""))\n        for res in results:\n            formatter = {\'all\': (lambda x: ""%9.3g"" % x)}\n            x = array2string(res.x[:max_show], formatter=formatter, separator=\',\')\n            out = (res.method, str(res.success), float(res.fun), x, res.time)\n            print(""{:>12s}: {:5s}  {:10.4g}  {}  {:.1e}"".format(*out))\n    if results:\n        return results[0]\n    else:\n        raise ValueError(\'Failed.\')\n\n\ndef findroot(y0, x, y):\n    """"""\n    find multiple roots.\n    y0: scalar\n    x: 1D array\n    y: function or 1D array\n    """"""\n    x = np.asarray(x)\n    assert x.ndim == 1\n\n    if callable(y):\n        y = y(x)\n    y = np.asarray(y)\n    assert x.shape == y.shape\n\n    ix1 = np.diff(y - y0 >= 0).nonzero()[0]\n    ix2 = ix1 + 1\n    x1, x2 = x[ix1], x[ix2]\n    y1, y2 = y[ix1], y[ix2]\n    x0 = (y0 - y1) / (y2 - y1) * (x2 - x1) + x1\n    return x0\n\n\ndef root_safe(func, dfunc, x1, x2, rtol=1e-5, xtol=1e-8, ntol=0, maxiter=100, report=False):\n    """"""\n    Find root for vector function in given intervals.\n\n    Adopted from Numerical Recipe 3rd P.460, function `rtsafe`\n    Not fully optimized yet, though seems well workable.\n\n    Parameters\n    ----------\n    func, dfunc : function\n        Input function and its first derivative,\n            y = func(x), dy/dx = dfunc(x)\n        where x, y both have shape (n,).\n    x1, x2 : ndarray, shape (n,)\n        Boundaries. Find roots in given intervals x1 < x < x2 for each elements,\n        therefor inputs should satisfy func(x1) * func(x2) < 0.\n    rtol : float\n        Relative tolerance, |x - x_true| < rtol * |x2 - x1|\n    xtol : float\n        Absolute tolerance, |x - x_true| < xtol\n    ntol : int\n        Allow ntol values not converge in result.\n    maxiter : int, optional\n        If convergence is not achieved in maxiter iterations, an error is raised.\n    report : bool\n        Report the iter number and convergence rate.\n\n    Examples\n    --------\n    def f(x):\n        return x*(x-1)*(x-2)\n\n    def j(x):\n        return 3*x**2 - 6*x + 2\n\n    import numpy as np\n    x1 = np.random.rand(1000000)\n    x = root_safe(f, j, x1, x1 + 1, report=True)\n    """"""\n    # initial check\n    x1, x2 = np.array(x1), np.array(x2)\n    f1, f2 = func(x1), func(x2)\n    if (f1 * f2 > 0).sum() > ntol:\n        # allow ntol invalid intervals\n        raise ValueError(""func(x1) and func(x2) must have different sign."")\n    ix = (f1 > f2).nonzero()\n    x1[ix], x2[ix] = x2[ix], x1[ix]  # Orient the search so that f(x1) < 0.\n\n    # initial guess\n    rt = 0.5 * (x1 + x2)\n    dx = np.abs(x2 - x1)\n    tol = np.fmax(xtol, dx * rtol)\n    ix_status = np.ones_like(rt, dtype=\'bool\')  # False means convergence\n\n    # quick return\n    ix = (f1 == 0).nonzero()\n    rt[ix] = x1[ix]\n    # dx[ix] = 0\n    ix_status[ix] = False\n\n    ix = (f2 == 0).nonzero()\n    rt[ix] = x2[ix]\n    # dx[ix] = 0\n    ix_status[ix] = False\n\n    for i in range(maxiter):\n        ix_select = ix_status.nonzero()[0]\n\n        f = func(rt)\n        df = dfunc(rt)\n\n        # Update the bracket\n        ix_low = f < 0\n        ix = (ix_low).nonzero()\n        x1[ix] = rt[ix]\n        ix = (~ix_low).nonzero()\n        x2[ix] = rt[ix]\n\n        # select the non-convergence ones\n        # xs -> one selection, xss -> double selection\n        x1s, x2s, dxs = x1[ix_select], x2[ix_select], dx[ix_select]\n        dxs_new = f[ix_select] / df[ix_select]\n        rts_new = rt[ix_select] - dxs_new\n        dxs_new = np.abs(dxs_new)\n        dxs_bis = 0.5 * (x2s - x1s)\n        rts_bis = x1s + dxs_bis\n        dxs_bis = np.abs(dxs_bis)\n\n        # Bisect if Newton out of range, or not decreasing fast enough.\n        ixs_bisect = ((rts_new - x1s) * (rts_new - x2s) > 0) | (dxs_new > 0.5 * dxs)\n\n        # Newton\n        ixs_newton = (~ixs_bisect).nonzero()[0]\n        ixss = ix_select[ixs_newton]\n        dx[ixss] = dxs_new[ixs_newton]\n        rt[ixss] = rts_new[ixs_newton]\n\n        # Bisect\n        ixs_bisect = (ixs_bisect).nonzero()[0]\n        ixss = ix_select[ixs_bisect]\n        dx[ixss] = dxs_bis[ixs_bisect]\n        rt[ixss] = rts_bis[ixs_bisect]\n\n        # convergence criterion\n        ix_status[dx < tol] = False\n        if report:\n            print (i, ix_status.mean())\n        if ix_status.sum() <= ntol:\n            break\n    else:\n        raise ValueError(""Maximum number of iterations exceeded"")\n\n    return rt\n'"
robustgp.py,9,"b'import GPy\nimport numpy as np\nfrom scipy.stats import norm, chi2\n\n__all__ = [\'robust_GP\']\n\n\ndef robust_gp_old(X, Y, nsigs=np.repeat(2, 5), callback=None, callback_args=(),\n                  **kwargs):\n    """"""\n    Robust Gaussian process for data with outliers.\n\n    Parameters\n    ----------\n    X: array shape (n, p)\n    Y: array shape (n, 1)\n        Input data.\n    nsigs: array shape (niter,)\n        List of n-sigma for iterations, should be a decreasing list.\n        Setting the last several n-sigma to be the same can give better\n        self-consistency.\n        Default: [2, 2, 2, 2, 2]\n        Alternative: 2**np.array([1, 0.8, 0.6, 0.4, 0.2, 0, 0, 0])\n    callback: callable\n        Function for checking the iteration process. It takes\n        the iteration number `i` and GPRegression object `gp` as input\n        e.g.\n            callback=lambda gp, i: print(i, gp.num_data, gp.param_array)\n        or\n            callback=lambda gp, i: gp.plot()\n    callback_args:\n        Extra parameters for callback.\n    **kwargs:\n        GPy.models.GPRegression parameters.\n\n    Returns\n    -------\n    gp:\n        GPy.models.GPRegression object.\n    """"""\n    n, p = Y.shape\n    if p != 1:\n        raise ValueError(""Y is expected in shape (n, 1)."")\n    if (np.asarray(nsigs) <= 0).any():\n        raise ValueError(""nsigs should be positive array."")\n    if (np.diff(nsigs) > 0).any():\n        raise ValueError(""nsigs should be decreasing array."")\n\n    gp = GPy.models.GPRegression(X, Y, **kwargs)\n    gp.optimize()\n    if callback is not None:\n        callback(gp, 0, *callback_args)\n\n    niter = len(nsigs)\n    for i in range(niter):\n        mean, var = gp.predict(X)\n        if i > 0:\n            # reference: Croux & Haesbroeck 1999\n            alpha = 2 * norm.cdf(nsigs[i - 1]) - 1\n            consistency_factor = alpha / chi2(p + 2).cdf(chi2(p).ppf(alpha))\n            var = var * consistency_factor\n        width = var**0.5 * nsigs[i]\n        ix = ((Y >= mean - width) & (Y <= mean + width)).ravel()\n\n        if i == 0:\n            ix_old = ix\n        elif (nsigs[i - 1] == nsigs[-1]) and (ix == ix_old).all():\n            break\n        else:\n            ix_old = ix\n\n        gp = GPy.models.GPRegression(X[ix], Y[ix], **kwargs)\n        gp.optimize()\n        if callback is not None:\n            callback(gp, i + 1, *callback_args)\n\n    return gp\n\n\ndef robust_GP(X, Y, alpha1=0.50, alpha2=0.95, alpha3=0.95,\n              niter0=0, niter1=10, niter2=1, exact=True,\n              callback=None, callback_args=(),\n              **kwargs):\n    """"""\n    Robust Gaussian process for data with outliers.\n\n    Three steps:\n        1. contraction\n        2. refinement\n        3. outlier detection\n\n    Parameters\n    ----------\n    X: array shape (n, p)\n    Y: array shape (n, 1)\n        Input data.\n    alpha1, alpha2:\n        Coverage fraction used in contraction step and refinement step respectively.\n    alpha3:\n        Outlier threshold.\n    niter0:\n        Extra iteration before start.\n    niter1, niter2:\n        Maximum iteration allowed in contraction step and refinement step respectively.\n    callback: callable\n        Function for checking the iteration process. It takes\n        the GPRegression object `gp`, consistency factor and iteration number `i` as input\n        e.g.\n            callback=lambda gp, c, i: print(i, gp.num_data, gp.param_array)\n        or\n            callback=lambda gp, c, i: gp.plot()\n    callback_args:\n        Extra parameters for callback.\n    **kwargs:\n        GPy.core.GP parameters.\n\n    Returns\n    -------\n    gp:\n        GPy.core.GP object.\n    consistency:\n        Consistency factor.\n    ix_out:\n        Boolean index for outliers.\n    """"""\n    n, p = Y.shape\n    if p != 1:\n        raise ValueError(""Y is expected in shape (n, 1)."")\n\n    kwargs.setdefault(\'likelihood\', GPy.likelihoods.Gaussian(variance=1.0))\n    kwargs.setdefault(\'kernel\', GPy.kern.RBF(X.shape[1]))\n    kwargs.setdefault(\'name\', \'Robust GP regression\')\n\n    # first iteration\n    gp = GPy.core.GP(X, Y, **kwargs)\n    gp.optimize()\n    consistency = 1\n    mean, var = gp.predict(X)\n    dist = np.ravel((Y - mean)**2 / (var))\n\n    if callback is not None:\n        callback(gp, consistency, 0, *callback_args)\n\n    ix_old = None\n    niter1 = niter0 + niter1\n\n    # contraction step\n    for i in range(niter1):\n        if i < niter0:\n            alpha_ = alpha1 + (1 - alpha1) * ((niter0 - 1 - i) / niter0)\n        else:\n            alpha_ = alpha1\n        h = min(int(np.ceil(n * alpha_)), n) - 1\n        dist_th = np.partition(dist, h)[h]\n        eta_sq1 = chi2(p).ppf(alpha_)\n        ix_sub = dist <= dist_th\n\n        if (i > niter0) and (ix_sub == ix_old).all():\n            break  # converged\n        ix_old = ix_sub\n\n        gp = GPy.core.GP(X[ix_sub], Y[ix_sub], **kwargs)\n        gp.optimize()\n        consistency = alpha_ / chi2(p + 2).cdf(eta_sq1)\n        mean, var = gp.predict(X)\n        dist = np.ravel((Y - mean)**2 / (var * consistency))\n\n        if callback is not None:\n            callback(gp, consistency, i + 1, *callback_args)\n\n    # refinement step\n    for i in range(niter1, niter1 + niter2):\n        eta_sq2 = chi2(p).ppf(alpha2)\n        ix_sub = dist <= eta_sq2 * consistency\n\n        if (i > niter1) and (ix_sub == ix_old).all():\n            break  # converged\n        ix_old = ix_sub\n\n        gp = GPy.core.GP(X[ix_sub], Y[ix_sub], **kwargs)\n        gp.optimize()\n        consistency = alpha2 / chi2(p + 2).cdf(eta_sq2)\n        mean, var = gp.predict(X)\n        dist = np.ravel((Y - mean)**2 / (var * consistency))\n\n        if callback is not None:\n            callback(gp, consistency, i + 1, *callback_args)\n\n    # outlier detection\n    score = dist**0.5\n\n    eta_sq3 = chi2(p).ppf(alpha3)\n    ix_out = dist > eta_sq3\n\n    return gp, consistency, score, ix_out\n'"
scatter.py,52,"b'from __future__ import division, print_function, absolute_import\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, Ellipse, Rectangle\nfrom matplotlib.collections import PatchCollection, LineCollection\nfrom scipy.stats import gaussian_kde, norm, chi2\nfrom collections import OrderedDict, namedtuple\nfrom .stats import quantile\n\n\n__all__ = [\'circles\', \'ellipses\', \'rectangles\', \'lines\', \'colorline\',\n           \'cov_ellipses\', \'mcd_ellipses\', \'densmap\']\n\n\ndef circles(x, y, s, c=\'b\', vmin=None, vmax=None, **kwargs):\n    """"""\n    Make a scatter plot of circles. \n    Similar to plt.scatter, but the size of circles are in data scale.\n\n    Parameters\n    ----------\n    x, y : scalar or array_like, shape (n, )\n        Input data\n    s : scalar or array_like, shape (n, ) \n        Radius of circles.\n    c : color or sequence of color, optional, default : \'b\'\n        `c` can be a single color format string, or a sequence of color\n        specifications of length `N`, or a sequence of `N` numbers to be\n        mapped to colors using the `cmap` and `norm` specified via kwargs.\n        Note that `c` should not be a single numeric RGB or RGBA sequence \n        because that is indistinguishable from an array of values\n        to be colormapped. (If you insist, use `color` instead.)  \n        `c` can be a 2-D array in which the rows are RGB or RGBA, however. \n    vmin, vmax : scalar, optional, default: None\n        `vmin` and `vmax` are used in conjunction with `norm` to normalize\n        luminance data.  If either are `None`, the min and max of the\n        color array is used.\n    kwargs : `~matplotlib.collections.Collection` properties\n        Eg. alpha, edgecolor(ec), facecolor(fc), linewidth(lw), linestyle(ls), \n        norm, cmap, transform, etc.\n\n    Returns\n    -------\n    paths : `~matplotlib.collections.PathCollection`\n\n    Examples\n    --------\n    a = np.arange(11)\n    circles(a, a, s=a*0.2, c=a, alpha=0.5, ec=\'none\')\n    plt.colorbar()\n\n    License\n    --------\n    This code is under [The BSD 3-Clause License]\n    (http://opensource.org/licenses/BSD-3-Clause)\n    """"""\n\n    if np.isscalar(c):\n        kwargs.setdefault(\'color\', c)\n        c = None\n\n    if \'fc\' in kwargs:\n        kwargs.setdefault(\'facecolor\', kwargs.pop(\'fc\'))\n    if \'ec\' in kwargs:\n        kwargs.setdefault(\'edgecolor\', kwargs.pop(\'ec\'))\n    if \'ls\' in kwargs:\n        kwargs.setdefault(\'linestyle\', kwargs.pop(\'ls\'))\n    if \'lw\' in kwargs:\n        kwargs.setdefault(\'linewidth\', kwargs.pop(\'lw\'))\n    # You can set `facecolor` with an array for each patch,\n    # while you can only set `facecolors` with a value for all.\n\n    zipped = np.broadcast(x, y, s)\n    patches = [Circle((x_, y_), s_)\n               for x_, y_, s_ in zipped]\n    collection = PatchCollection(patches, **kwargs)\n    if c is not None:\n        c = np.broadcast_to(c, zipped.shape).ravel()\n        collection.set_array(c)\n        collection.set_clim(vmin, vmax)\n\n    ax = plt.gca()\n    ax.add_collection(collection)\n    ax.autoscale_view()\n    plt.draw_if_interactive()\n    if c is not None:\n        plt.sci(collection)\n    return collection\n\n\ndef ellipses(x, y, w, h=None, rot=0.0, c=\'b\', vmin=None, vmax=None, **kwargs):\n    """"""\n    Make a scatter plot of ellipses. \n    Parameters\n    ----------\n    x, y : scalar or array_like, shape (n, )\n        Center of ellipses.\n    w, h : scalar or array_like, shape (n, )\n        Total length (diameter) of horizontal/vertical axis.\n        `h` is set to be equal to `w` by default, ie. circle.\n    rot : scalar or array_like, shape (n, )\n        Rotation in degrees (anti-clockwise).\n    c : color or sequence of color, optional, default : \'b\'\n        `c` can be a single color format string, or a sequence of color\n        specifications of length `N`, or a sequence of `N` numbers to be\n        mapped to colors using the `cmap` and `norm` specified via kwargs.\n        Note that `c` should not be a single numeric RGB or RGBA sequence\n        because that is indistinguishable from an array of values\n        to be colormapped. (If you insist, use `color` instead.)\n        `c` can be a 2-D array in which the rows are RGB or RGBA, however.\n    vmin, vmax : scalar, optional, default: None\n        `vmin` and `vmax` are used in conjunction with `norm` to normalize\n        luminance data.  If either are `None`, the min and max of the\n        color array is used.\n    kwargs : `~matplotlib.collections.Collection` properties\n        Eg. alpha, edgecolor(ec), facecolor(fc), linewidth(lw), linestyle(ls),\n        norm, cmap, transform, etc.\n\n    Returns\n    -------\n    paths : `~matplotlib.collections.PathCollection`\n\n    Examples\n    --------\n    a = np.arange(11)\n    ellipses(a, a, w=4, h=a, rot=a*30, c=a, alpha=0.5, ec=\'none\')\n    plt.colorbar()\n\n    License\n    --------\n    This code is under [The BSD 3-Clause License]\n    (http://opensource.org/licenses/BSD-3-Clause)\n    """"""\n    if np.isscalar(c):\n        kwargs.setdefault(\'color\', c)\n        c = None\n\n    if \'fc\' in kwargs:\n        kwargs.setdefault(\'facecolor\', kwargs.pop(\'fc\'))\n    if \'ec\' in kwargs:\n        kwargs.setdefault(\'edgecolor\', kwargs.pop(\'ec\'))\n    if \'ls\' in kwargs:\n        kwargs.setdefault(\'linestyle\', kwargs.pop(\'ls\'))\n    if \'lw\' in kwargs:\n        kwargs.setdefault(\'linewidth\', kwargs.pop(\'lw\'))\n    # You can set `facecolor` with an array for each patch,\n    # while you can only set `facecolors` with a value for all.\n\n    if h is None:\n        h = w\n\n    zipped = np.broadcast(x, y, w, h, rot)\n    patches = [Ellipse((x_, y_), w_, h_, rot_)\n               for x_, y_, w_, h_, rot_ in zipped]\n    collection = PatchCollection(patches, **kwargs)\n    if c is not None:\n        c = np.broadcast_to(c, zipped.shape).ravel()\n        collection.set_array(c)\n        collection.set_clim(vmin, vmax)\n\n    ax = plt.gca()\n    ax.add_collection(collection)\n    ax.autoscale_view()\n    plt.draw_if_interactive()\n    if c is not None:\n        plt.sci(collection)\n    return collection\n\n\ndef rectangles(x, y, w, h=None, rot=0.0, c=\'b\', pivot=\'center\',\n               vmin=None, vmax=None, **kwargs):\n    """"""\n    Make a scatter plot of rectangles.\n\n    Parameters\n    ----------\n    x, y : scalar or array_like, shape (n, )\n        Coordinates of rectangles.\n    w, h : scalar or array_like, shape (n, )\n        Width, Height.\n        `h` is set to be equal to `w` by default, ie. squares.\n    rot : scalar or array_like, shape (n, )\n        Rotation in degrees (anti-clockwise).\n    c : color or sequence of color, optional, default : \'b\'\n        `c` can be a single color format string, or a sequence of color\n        specifications of length `N`, or a sequence of `N` numbers to be\n        mapped to colors using the `cmap` and `norm` specified via kwargs.\n        Note that `c` should not be a single numeric RGB or RGBA sequence\n        because that is indistinguishable from an array of values\n        to be colormapped. (If you insist, use `color` instead.)\n        `c` can be a 2-D array in which the rows are RGB or RGBA, however.\n    pivot : \'center\', \'lower-left\'\n        The part of the rectangle that is at the coordinate.\n        The rectangle rotate about this point, hence the name *pivot*.\n    vmin, vmax : scalar, optional, default: None\n        `vmin` and `vmax` are used in conjunction with `norm` to normalize\n        luminance data.  If either are `None`, the min and max of the\n        color array is used.\n    kwargs : `~matplotlib.collections.Collection` properties\n        Eg. alpha, edgecolor(ec), facecolor(fc), linewidth(lw), linestyle(ls),\n        norm, cmap, transform, etc.\n\n    Returns\n    -------\n    paths : `~matplotlib.collections.PathCollection`\n\n    Examples\n    --------\n    a = np.arange(11)\n    rectangles(a, a, w=5, h=6, rot=a*30, c=a, alpha=0.5, ec=\'none\')\n    plt.colorbar()\n\n    License\n    --------\n    This code is under [The BSD 3-Clause License]\n    (http://opensource.org/licenses/BSD-3-Clause)\n    """"""\n    if np.isscalar(c):\n        kwargs.setdefault(\'color\', c)\n        c = None\n\n    if \'fc\' in kwargs:\n        kwargs.setdefault(\'facecolor\', kwargs.pop(\'fc\'))\n    if \'ec\' in kwargs:\n        kwargs.setdefault(\'edgecolor\', kwargs.pop(\'ec\'))\n    if \'ls\' in kwargs:\n        kwargs.setdefault(\'linestyle\', kwargs.pop(\'ls\'))\n    if \'lw\' in kwargs:\n        kwargs.setdefault(\'linewidth\', kwargs.pop(\'lw\'))\n    # You can set `facecolor` with an array for each patch,\n    # while you can only set `facecolors` with a value for all.\n\n    if h is None:\n        h = w\n    if pivot == \'center\':\n        d = np.sqrt(np.square(w) + np.square(h)) * 0.5\n        t = np.deg2rad(rot) + np.arctan2(h, w)\n        x, y = x - d * np.cos(t), y - d * np.sin(t)\n\n    zipped = np.broadcast(x, y, w, h, rot)\n    patches = [Rectangle((x_, y_), w_, h_, rot_)\n               for x_, y_, w_, h_, rot_ in zipped]\n    collection = PatchCollection(patches, **kwargs)\n    if c is not None:\n        c = np.broadcast_to(c, zipped.shape).ravel()\n        collection.set_array(c)\n        collection.set_clim(vmin, vmax)\n\n    ax = plt.gca()\n    ax.add_collection(collection)\n    ax.autoscale_view()\n    plt.draw_if_interactive()\n    if c is not None:\n        plt.sci(collection)\n    return collection\n\n\ndef lines(xy, c=\'b\', vmin=None, vmax=None, **kwargs):\n    """"""\n    xy : sequence of array \n        Coordinates of points in lines.\n        `xy` is a sequence of array (line0, line1, ..., lineN) where\n            line = [(x0, y0), (x1, y1), ... (xm, ym)]\n    c : color or sequence of color, optional, default : \'b\'\n        `c` can be a single color format string, or a sequence of color\n        specifications of length `N`, or a sequence of `N` numbers to be\n        mapped to colors using the `cmap` and `norm` specified via kwargs.\n        Note that `c` should not be a single numeric RGB or RGBA sequence\n        because that is indistinguishable from an array of values\n        to be colormapped. (If you insist, use `color` instead.)\n        `c` can be a 2-D array in which the rows are RGB or RGBA, however.\n    vmin, vmax : scalar, optional, default: None\n        `vmin` and `vmax` are used in conjunction with `norm` to normalize\n        luminance data.  If either are `None`, the min and max of the\n        color array is used.\n    kwargs : `~matplotlib.collections.Collection` properties\n        Eg. alpha, linewidth(lw), linestyle(ls), norm, cmap, transform, etc.\n\n    Returns\n    -------\n    collection : `~matplotlib.collections.LineCollection`\n    """"""\n    if np.isscalar(c):\n        kwargs.setdefault(\'color\', c)\n        c = None\n\n    if \'ls\' in kwargs:\n        kwargs.setdefault(\'linestyle\', kwargs.pop(\'ls\'))\n    if \'lw\' in kwargs:\n        kwargs.setdefault(\'linewidth\', kwargs.pop(\'lw\'))\n\n    collection = LineCollection(xy, **kwargs)\n    if c is not None:\n        collection.set_array(np.asarray(c))\n        collection.set_clim(vmin, vmax)\n\n    ax = plt.gca()\n    ax.add_collection(collection)\n    ax.autoscale_view()\n    plt.draw_if_interactive()\n    if c is not None:\n        plt.sci(collection)\n    return collection\n\n\ndef colorline(x, y, c=\'b\', **kwargs):\n    """"""Draw a colored line.\n\n    Parameters\n    ----------\n    x, y : array (n,)\n        Coordinates.\n    c : array (n,) | array (n-1,) | scalar\n        Colors.\n\n    Returns\n    -------\n    collection : `~matplotlib.collections.LineCollection`\n\n    Examples\n    --------\n    x = np.linspace(0.01, 30, 100)\n    y = np.sin(x) / x\n    colorline(x, y, c=x, lw=20)\n    """"""\n    if not np.isscalar(c) and len(c) == len(x):\n        c = np.asarray(c)\n        c = (c[:-1] + c[1:]) * 0.5\n    x, y = np.concatenate([x, x[-1:]]), np.concatenate([y, y[-1:]])\n    xy = [x[:-2], y[:-2], x[1:-1], y[1:-1], x[2:], y[2:]]\n    xy = np.stack(xy, -1).reshape(-1, 3, 2)\n    return lines(xy, c=c, **kwargs)\n\n\ndef cov_ellipses(x, y, cov_mat=None, cov_tri=None, q=None, nsig=None, dist=None,\n                 plot_ellipse=True, plot_cross=False, plot_center=False, aspect=1,\n                 cross_kwargs={}, center_kwargs={}, **kwargs):\n    """"""Draw covariance error ellipses.\n\n    Parameters\n    ----------\n    x, y : array (n,)\n        Center of covariance ellipses.\n    cov_mat : array (n, 2, 2), optional\n        Covariance matrix.\n    cov_tri : list of array (n,), optional\n        Covariance matrix in flat form of (xvar, yvar, xycov).\n    q : scalar or array\n        Wanted (quantile) probability enclosed in error ellipse.\n    nsig : scalar or array\n        Probability in unit of standard error. Eg. `nsig = 1` means `q = 0.683`.\n    dist : scaler or array\n        Threshold of mahalanobis distance, equivalent to chi2.ppf(q, 2)\n        It overwrites `q` or `nsig`.\n    aspect : float\n        Aspect of the axes, set this to assure the cross is orthogonal.\n    kwargs :\n        `ellipses` properties.\n        Eg. c, vmin, vmax, alpha, edgecolor(ec), facecolor(fc), \n        linewidth(lw), linestyle(ls), norm, cmap, transform, etc.\n\n    Examples\n    --------\n    from sklearn.covariance import EllipticEnvelope\n    X = np.random.randn(1000, 2)\n    q = 2 - norm.cdf(1) * 2\n    mcd = EllipticEnvelope(contamination=q).fit(X)\n    cov_ellipses(*mcd.location_, cov_mat=mcd.covariance_, dist=mcd.threshold_, fc=\'none\')\n\n    Reference\n    ---------\n    [1]: http://www.visiondummy.com/2014/04/draw-error-ellipse-representing-covariance-matrix\n    [2]: http://stackoverflow.com/questions/12301071/multidimensional-confidence-intervals\n    """"""\n    if cov_mat is not None:\n        cov_mat = np.asarray(cov_mat)\n    elif cov_tri is not None:\n        assert len(cov_tri) == 3\n        cov_mat = np.array([[cov_tri[0], cov_tri[2]],\n                            [cov_tri[2], cov_tri[1]]])\n        cov_mat = cov_mat.transpose(list(range(2, cov_mat.ndim)) + [0, 1])\n        # Roll the first two dimensions (2, 2) to end.\n    else:\n        raise ValueError(\'One of `cov_mat` and `cov_tri` should be specified.\')\n\n    x, y = np.asarray(x), np.asarray(y)\n    if not (cov_mat.shape[:-2] == x.shape == y.shape):\n        raise ValueError(\'The shape of x, y and covariance are incompatible.\')\n    if not (cov_mat.shape[-2:] == (2, 2)):\n        raise ValueError(\'Invalid covariance matrix shape.\')\n\n    if dist is not None:\n        rho = np.asarray(dist)\n    else:\n        if q is not None:\n            q = np.asarray(q)\n        elif nsig is not None:\n            q = 2 * norm.cdf(nsig) - 1\n        else:\n            raise ValueError(\'One of `q` and `nsig` should be specified.\')\n        rho = chi2.ppf(q, 2)\n        rho = rho.reshape(rho.shape + (1,) * x.ndim)  # raise dimensions\n\n    if plot_ellipse or (plot_cross and aspect == 1):\n        val, vec = np.linalg.eigh(cov_mat)  # (n, 2), (n, 2, 2)\n        w = 2 * np.sqrt(val[..., 0] * rho)\n        h = 2 * np.sqrt(val[..., 1] * rho)\n        rot = np.degrees(np.arctan2(vec[..., 1, 0], vec[..., 0, 0]))\n\n    res = []\n    if plot_ellipse:\n        ellip = ellipses(x, y, w, h, rot=rot, **kwargs)\n        res.append(ellip)\n    if plot_cross:\n        if aspect != 1:\n            cov_mat = cov_mat.copy()\n            cov_mat[..., 1, 0] *= aspect\n            cov_mat[..., 0, 1] *= aspect\n            cov_mat[..., 1, 1] *= aspect**2\n            val, vec = np.linalg.eigh(cov_mat)\n            vec[..., -1, :] /= aspect\n            w = 2 * np.sqrt(val[..., 0] * rho)\n            h = 2 * np.sqrt(val[..., 1] * rho)\n\n        xy = np.stack([x, y], -1)[..., None, :]\n        wline = xy + vec[..., None, :, 0] * w[..., None, None] * np.array([[-0.5], [0.5]])\n        hline = xy + vec[..., None, :, 1] * h[..., None, None] * np.array([[-0.5], [0.5]])\n        res.append(lines(wline, **cross_kwargs))\n        res.append(lines(hline, **cross_kwargs))\n    if plot_center:\n        center = plt.scatter(x, y, **center_kwargs)\n        res.append(center)\n\n    if len(res) == 1:\n        return res[0]\n    else:\n        return res\n\n\ndef mcd_ellipses(X, Y=None, q=None, nsig=None, support_fraction=None, **kwargs):\n    """"""Draw the minimum covariance determinant ellipses for data sample.\n\n    X : array of shape (n,), (m, n), (n, 2), (m, n, 2)\n        m samples (can be 1) with n points in each\n    Y : None, array of shape (n,), (m, n)\n    """"""\n    from sklearn.covariance import EllipticEnvelope\n    if q is not None:\n        q = np.asarray(q)\n    elif nsig is not None:\n        q = 2 * norm.cdf(nsig) - 1\n    else:\n        raise ValueError(\'One of `q` and `nsig` should be specified.\')\n\n    if Y is not None:\n        X = np.stack([X, Y], -1)\n    X = np.asarray(X)\n    if X.ndim < 3:\n        X = X[None, ...]\n\n    x, y, mcov, dist = [], [], [], []\n    for _ in X:\n        mcd = EllipticEnvelope(support_fraction=support_fraction,\n                               contamination=1 - q).fit(_)\n        x.append(mcd.location_[0])\n        y.append(mcd.location_[1])\n        mcov.append(mcd.covariance_)\n        dist.append(mcd.threshold_)\n    x, y, mcov, dist = map(np.array, [x, y, mcov, dist])\n\n    return x, y, mcov, dist, cov_ellipses(x, y, cov_mat=mcov, dist=dist, **kwargs)\n\n\ndef densmap(x, y, scale=None, style=\'scatter\', sort=False, levels=10,\n            **kwargs):\n    """"""Show the number density of points in plane.\n    The density is calculated by kernel-density estimate with Gaussian kernel.\n\n    Parameters\n    ----------\n    x, y : array like\n        Position of data points.\n    scale : None, float or callable\n        Scale the density by\n            z = z * scale - for float\n            z = scale(z) - for callable\n    style :\n        \'scatter\', \'contour\', \'contourf\' and their combination.\n        Note that the contour mode is implemented through `plt.tricontour`,\n        it may give *misleading result* when the point distributed in \n        *concave* polygon shape. This problem can be avoid by performing\n        `plt.contour` on `np.histograme` output if the point number is large.\n    sort : bool\n        If `sort` is True, the points with higher density are plotted on top.\n        Argument only for `scatter` mode.\n    levels : int or sequence\n        Contour levels. \n        Argument only for `contour` and `contourf` mode.\n\n    See Also\n    --------\n    astroML.plotting.scatter_contour\n\n    References\n    ----------\n    .. [1] Joe Kington, http://stackoverflow.com/a/20107592/2144720\n\n    Examples\n    --------\n    from numpy.random import randn\n    x, y = randn(2, 1000)\n    r = densmap(x, y, style=[\'contourf\', \'scatter\'],\n                levels=arange(0.02, 0.2, 0.01))\n    """"""\n    x, y = np.asarray(x), np.asarray(y)\n    xy = np.vstack([x, y])\n    z = gaussian_kde(xy)(xy)\n    if np.isscalar(scale):\n        z = z * scale\n    elif callable(scale):\n        z = scale(z)\n\n    if np.isscalar(style):\n        style = [style]\n    if \'scatter\' in style and sort:\n        idx = z.argsort()\n        x, y, z = x[idx], y[idx], z[idx]\n    if \'contour\' in style or \'contourf\' in style:\n        q = kwargs.pop(""q"", None)\n        nsig = kwargs.pop(""nsig"", None)\n        if q is not None or nsig is not None:\n            levels = quantile(z, q=q, nsig=nsig, origin=\'high\')\n            levels = np.atleast_1d(levels)\n        elif np.isscalar(levels):\n            levels = np.linspace(z.min(), z.max(), levels)\n        else:\n            levels = np.sort(levels)\n\n    kwargs.setdefault(\'edgecolor\', \'none\')\n    kwargs.setdefault(\'zorder\', 1)\n    kwargs.setdefault(\'vmin\', z.min())\n    kwargs.setdefault(\'vmax\', z.max())\n    colors = kwargs.pop(\'colors\', None)  # keywords for contour only.\n\n    result = OrderedDict(density=z)\n    for sty in style:\n        if sty == \'scatter\':\n            im = plt.scatter(x, y, c=z, **kwargs)\n        elif sty == \'contour\':\n            im = plt.tricontour(x, y, z, levels=levels, colors=colors,\n                                **kwargs)\n        elif sty == \'contourf\':\n            im = plt.tricontourf(x, y, z, levels=levels, colors=colors,\n                                 **kwargs)\n        else:\n            msg = ""style must be one of \'scatter\', \'contour\', \'contourf\'.""\n            raise ValueError(msg)\n        result[sty] = im\n    return namedtuple(""DensMap"", result)(**result)\n'"
spline.py,15,"b'from __future__ import division, print_function, unicode_literals\nimport numpy as np\nfrom scipy.interpolate import CubicSpline\n\n__all__ = [\'GridCubicSpline\']\n\nfactorial = np.array([1, 1, 2, 6])\nderiv_coeff = [np.array([1, 1, 1, 1]), np.array([3, 2, 1]), np.array([6, 2]),\n               np.array([6])]\ninteg_power = np.array([4, 3, 2, 1], dtype=float).reshape(-1, 1)\ninteg_coeff = 1 / integ_power\n\n\nclass GridCubicSpline:\n    """"""\n    Examples\n    --------\n    x = np.linspace(0, 10, 100)\n    y = np.sin(x)\n    f1 = CubicSpline(x, y)\n    f2 = GridCubicSpline(x, y)\n\n    for k in range(4):\n        assert np.allclose(f1.derivative(k)(x), f2.derivative(k))\n    assert np.allclose(f1.antiderivative()(x), f2.antiderivative())\n\n    f2.antiderivative(forward=True) + f2.antiderivative(forward=False)\n    """"""\n\n    def __init__(self, x, y, bc_type=\'not-a-knot\'):\n        ""check docstring of scipy.interpolate.CubicSpline""\n        self.spline = CubicSpline(x, y, bc_type=bc_type)\n        self.x = np.asfarray(x)\n        self.y = y\n        self.c = self.spline.c\n\n    def derivative(self, k=1):\n        ""k = 0, 1, 2, 3""\n        x, c = self.x, self.c\n        y_der = np.empty_like(x)\n\n        if k < 2:\n            y_der[:-1] = c[3 - k]\n        else:\n            y_der[:-1] = c[3 - k] * factorial[k]\n        y_der[-1] = np.poly1d(c[:4 - k, -1] * deriv_coeff[k])(x[-1] - x[-2])\n\n        return y_der\n\n    def antiderivative(self, forward=True):\n        ""forward: integrate from x[0] to x,\\nbackward: integrate from x[-1] to x.""\n        x, c = self.x, self.c\n        y_int = np.empty_like(x)\n\n        dy_int = (integ_coeff * c * np.diff(x)**integ_power).sum(0)\n        if forward:\n            y_int[0] = 0\n            y_int[1:] = np.cumsum(dy_int)\n        else:\n            y_int[-1] = 0\n            y_int[:-1] = np.cumsum(dy_int[::-1])[::-1]\n\n        return y_int\n'"
stats.py,130,"b'from __future__ import division, print_function, absolute_import\nimport warnings\nimport numpy as np\nfrom scipy.stats import norm\nfrom collections import namedtuple\nfrom itertools import product\n\n__all__ = [\'mid\', \'uniquefy\', \'binstats\', \'quantile\', \'nanquantile\',\n           \'conflevel\', \'hdr1d\', \'binquantile\', \'alterbinstats\']\n\n\nBinStats = namedtuple(\'BinStats\',\n                      (\'stats\', \'edges\', \'count\'))\n\n\ndef mid(x, axis=0, base=None):\n    """"""Return mean value of adjacent number in array.\n    Useful for plotting bin counts.\n\n    Examples\n    --------\n    >>> mid(np.arange(6))\n    array([0.5, 1.5, 2.5, 3.5, 4.5])\n\n    >>> mid(np.arange(6).reshape(2,3), 0)\n    array([[ 1.5,  2.5,  3.5]])\n\n    >>> mid(np.arange(6).reshape(2,3), 1)\n    array([[ 0.5,  1.5],\n           [ 3.5,  4.5]])\n    """"""\n    x = np.asarray(x)\n\n    if base is not None:\n        if base == \'log\':\n            return np.exp(mid(np.log(x), axis=axis))\n        elif base == \'exp\':\n            return np.log(mid(np.exp(x), axis=axis))\n        elif base <= 0:\n            raise ValueError(""`base` must be positive"")\n        elif base == 1:\n            return mid(x, axis=axis)\n        else:\n            return np.log(mid(base**x, axis=axis)) / np.log(base)\n\n    idx1, idx2 = [slice(None)] * x.ndim, [slice(None)] * x.ndim\n    idx1[axis] = slice(1, None)\n    idx2[axis] = slice(None, -1)\n    return 0.5 * (x[tuple(idx1)] + x[tuple(idx2)])\n\n\ndef uniquefy(x, weights=None):\n    """"""Merge repeated values in flattened array, designed for quantile and conflevel.\n\n    Examples\n    --------\n    x = np.random.randint(100, size=1000)\n    unique, unique_weights = uniquefy(x, weights=weights)\n\n    # with weights\n    weights = np.ones_like(x)\n    unique, unique_weights = uniquefy(x, weights=weights)\n    """"""\n    if weights is not None:\n        weights = np.asarray(weights).ravel()\n        unique, index = np.unique(x, return_inverse=True)\n        unique_weights = np.bincount(index, weights=weights)\n    else:\n        unique, count = np.unique(x, return_counts=True)\n        unique_weights = count\n    return unique, unique_weights\n\n\ndef generate_bins(x, bins):\n    """"""Generate bins automatically.\n    Helper function for binstats.\n    """"""\n    if bins is None:\n        bins = 10\n    if np.isscalar(bins):\n        ix = np.isfinite(x)\n        if not ix.all():\n            x = x[ix]  # drop nan, inf\n        if len(x) > 0:\n            xmin, xmax = np.min(x), np.max(x)\n        else:\n            # failed to determine range, so use 0-1.\n            xmin, xmax = 0, 1\n        if xmin == xmax:\n            xmin = xmin - 0.5\n            xmax = xmax + 0.5\n        return np.linspace(xmin, xmax, bins + 1)\n    else:\n        return np.asarray(bins)\n\n\ndef binstats(xs, ys, bins=10, func=np.mean, nmin=1, shape=\'stats\'):\n    """"""Make binned statistics for multidimensional data.\n    xs: array_like or list of array_like\n        Data to histogram passed as a sequence of D arrays of length N, or\n        as an (D, N) array.\n    ys: array_like or list of array_like\n        The data on which the `func` will be computed.  This must be\n        the same shape as `x`, or a list of sequences - each with the same\n        shape as `x`.  If `values` is a list, the `func` will treat them as\n        multiple arguments.\n    bins : sequence or int, optional\n        The bin specification must be in one of the following forms:\n          * A sequence of arrays describing the bin edges along each dimension.\n          * The number of bins for each dimension (n1, n2, ... = bins).\n          * The number of bins for all dimensions (n1 = n2 = ... = bins).\n    func: callable\n        User-defined function which takes a sequence of arrays as input,\n        and outputs a scalar or an array with *fixed shape*. This function\n        will be called on the values in each bin func(y1, y2, ...).\n        Empty bins will be represented by func([], [], ...) or NaNs if this\n        returns an error.\n    nmin: int\n        The bin with data point counts smaller than nmin will be \n        treated as empty bin.\n    shape : \'bins\' | \'stats\'\n        Put which axes first in the result:\n            \'bins\' - the shape of bins\n            \'stats\' - the shape of func output\n\n    Returns\n    -------\n    stats: ndarray\n        The values of the selected statistic in each bin.\n    edges: list of ndarray\n        A list of D arrays describing the (nxi + 1) bin edges for each\n        dimension.\n    count: ndarray\n        Number count in each bin.\n\n    See Also\n    --------\n    numpy.histogramdd, scipy.stats.binned_statistic_dd\n\n    Example\n    -------\n    import numpy as np\n    from numpy.random import rand\n    x, y = rand(2, 1000)\n    b = np.linspace(0, 1, 11)\n    binstats(x, y, 10, np.mean)\n    binstats(x, y, b, np.mean)\n    binstats(x, [x, y], 10, lambda x, y: np.mean(y - x))\n    binstats(x, [x, y], 10, lambda x, y: [np.median(x), np.std(y)])\n    """"""\n    # check the inputs\n    if not callable(func):\n        raise TypeError(\'`func` must be callable.\')\n    if shape != \'bins\' and shape != \'stats\':\n        raise ValueError(""`shape` must be \'bins\' or \'stats\'"")\n\n    if len(xs) == 0:\n        raise ValueError(""`xs` must be non empty"")\n    if len(ys) == 0:\n        raise ValueError(""`ys` must be non empty"")\n    if np.isscalar(xs[0]):\n        xs = [xs]\n        bins = [bins]\n    if np.isscalar(ys[0]):\n        ys = [ys]\n    if np.isscalar(bins):\n        bins = [bins] * len(xs)\n\n    xs = [np.asarray(x) for x in xs]\n    ys = [np.asarray(y) for y in ys]\n\n    D, N = len(xs), len(xs[0])\n    # `D`: number of dimensions\n    # `N`: number of elements along each dimension\n    for x in xs:\n        if len(x) != N:\n            raise ValueError(""x should have the same length"")\n        if x.ndim != 1:\n            raise ValueError(""x should be 1D array"")\n    for y in ys:\n        if len(y) != N:\n            raise ValueError(""y should have the same length as x"")\n    if len(bins) != D:\n        raise ValueError(""bins should have the same number as xs"")\n\n    # prepare the edges\n    edges = [generate_bins(x, bin) for x, bin in zip(xs, bins)]\n    dims = tuple(len(edge) - 1 for edge in edges)\n    nbin = np.prod(dims)\n\n    # statistical value for empty bin\n    with warnings.catch_warnings():\n        # Numpy generates a warnings for mean/std/... with empty list\n        warnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n        try:\n            yselect = [y[:0] for y in ys]\n            null = np.asarray(func(*yselect))\n        except Exception:\n            yselect = [y[:1] for y in ys]\n            temp = np.asarray(func(*yselect))\n            null = np.full_like(temp, np.nan, dtype=\'float\')\n\n    # get the index\n    indexes = np.empty((D, N), dtype=\'int\')\n    for i in range(D):\n        ix = np.searchsorted(edges[i], xs[i], side=\'right\') - 1\n        ix[(xs[i] >= edges[i][-1])] = -1  # give outliers index < 0\n        ix[(xs[i] == edges[i][-1])] = -1 + dims[i]  # include points on edge\n        indexes[i] = ix\n\n    # convert nd-index to flattened index\n    index = indexes[0]\n    ix_out = (indexes < 0).any(axis=0)  # outliers\n    for i in range(1, D):\n        index *= dims[i]\n        index += indexes[i]\n    index[ix_out] = nbin  # put outliers in an extra bin\n\n    # make statistics on each bin\n    stats = np.empty((nbin,) + null.shape, dtype=null.dtype)\n    count = np.bincount(index, minlength=nbin + 1)[:nbin]\n    for i in range(nbin):\n        if count[i] >= nmin:\n            ix = (index == i).nonzero()\n            yselect = [y[ix] for y in ys]\n            stats[i] = func(*yselect)\n        else:\n            stats[i] = null\n\n    # change to proper shape\n    if shape == \'bins\':\n        stats = stats.reshape(dims + null.shape)\n    elif shape == \'stats\':\n        stats = np.moveaxis(stats, 0, -1).reshape(null.shape + dims)\n    count = count.reshape(dims)\n    return BinStats(stats, edges, count)\n\n\ndef quantile(a, weights=None, q=None, nsig=None, origin=\'middle\',\n             axis=None, keepdims=False, sorted=False, nmin=0,\n             nanas=None, shape=\'stats\'):\n    \'\'\'Compute the quantile of the data.\n    Be careful when q is very small or many numbers repeat in a.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    weights : array_like, optional\n        Weighting of a.\n    q : float or float array in range of [0,1], optional\n        Quantile to compute. One of `q` and `nsig` must be specified.\n    nsig : float, optional\n        Quantile in unit of standard deviation.\n        Ignored when `q` is given.\n    origin : [\'middle\'| \'high\'| \'low\'], optional\n        Control how to interpret `nsig` to `q`.\n    axis : int, optional\n        Axis along which the quantiles are computed. The default is to\n        compute the quantiles of the flattened array.\n    sorted : bool\n        If True, the input array is assumed to be in increasing order.\n    nmin : int or None\n        Return `nan` when the tail probability is less than `nmin/a.size`.\n        Set `nmin` if you want to make result more reliable.\n        - nmin = None will turn off the check.\n        - nmin = 0 will return NaN for q not in [0, 1].\n        - nmin >= 3 is recommended for statistical use.\n        It is *not* well defined when `weights` is given.\n    nanas : None, float, \'ignore\'\n        - None : do nothing. Note default sorting puts `nan` after `inf`.\n        - float : `nan`s will be replaced by given value.\n        - \'ignore\' : `nan`s will be excluded before any calculation.\n    shape : \'data\' | \'stats\'\n        Put which axes first in the result:\n            \'data\' - the shape of data\n            \'stats\' - the shape of `q` or `nsig`\n        Only works for case where axis is not None.\n\n    Returns\n    -------\n    quantile : scalar or ndarray\n        The first axes of the result corresponds to the quantiles,\n        the rest are the axes that remain after the reduction of `a`.\n\n    See Also\n    --------\n    numpy.percentile\n    conflevel\n\n    Examples\n    --------\n    >>> np.random.seed(0)\n    >>> x = np.random.randn(3, 100)\n\n    >>> quantile(x, q=0.5)\n    0.024654858649703838\n    >>> quantile(x, nsig=0)\n    0.024654858649703838\n    >>> quantile(x, nsig=1)\n    1.0161711040272021\n    >>> quantile(x, nsig=[0, 1])\n    array([ 0.02465486,  1.0161711 ])\n\n    >>> quantile(np.abs(x), nsig=1, origin=\'low\')\n    1.024490097937702\n    >>> quantile(-np.abs(x), nsig=1, origin=\'high\')\n    -1.0244900979377023\n\n    >>> quantile(x, q=0.5, axis=1)\n    array([ 0.09409612,  0.02465486, -0.07535884])\n    >>> quantile(x, q=0.5, axis=1).shape\n    (3,)\n    >>> quantile(x, q=0.5, axis=1, keepdims=True).shape\n    (3, 1)\n    >>> quantile(x, q=[0.2, 0.8], axis=1).shape\n    (2, 3)\n    >>> quantile(x, q=[0.2, 0.8], axis=1, shape=\'stats\').shape\n    (3, 2)\n    \'\'\'\n    # check input\n    a = np.asarray(a)\n    if weights is not None:\n        weights = np.asarray(weights)\n        if weights.shape != a.shape:\n            raise ValueError(""`weights` should have same shape as `a`."")\n\n    # convert nsig to q\n    if q is not None:\n        q = np.asarray(q)\n    elif nsig is not None:\n        if origin == \'middle\':\n            q = norm.cdf(nsig)\n        elif origin == \'high\':\n            q = 2 - 2 * norm.cdf(nsig)\n        elif origin == \'low\':\n            q = 2 * norm.cdf(nsig) - 1\n        else:\n            raise ValueError(""`origin` should be \'center\', \'high\' or \'low\'."")\n        q = np.asarray(q)\n    else:\n        raise ValueError(""One of `q` and `nsig` must be specified."")\n\n    # check q and nmin\n    # nmin = 0 will assert return nan for q not in [0, 1]\n    if nmin is not None and a.size:\n        tol = 1 - 1e-5\n        if axis is None:\n            threshold = nmin * tol / a.size\n        else:\n            threshold = nmin * tol / a.shape[axis]\n        ix = np.fmin(q, 1 - q) < threshold\n        if np.any(ix):\n            q = np.array(q, dtype=""float"")  # make copy of `q`\n            q[ix] = np.nan\n\n    # result shape\n    if axis is None:\n        res_shape = q.shape\n    else:\n        extra_dims = list(a.shape)\n        if keepdims:\n            extra_dims[axis] = 1\n        else:\n            extra_dims.pop(axis)\n\n        if shape == \'data\':\n            res_shape = tuple(extra_dims) + q.shape\n        elif shape == \'stats\':\n            res_shape = q.shape + tuple(extra_dims)\n        else:\n            raise ValueError(""`shape` must be \'data\' or \'stats\'"")\n\n    # quick return for empty input array\n    if a.size == 0 or q.size == 0:\n        return np.full(res_shape, np.nan, dtype=\'float\')\n    elif a.size == 1 and axis is None:\n        # fix bug of np.interp when len(a) == 1\n        res = np.full_like(q, a.ravel()[0], dtype=\'float\')\n        res[np.isnan(q)] = np.nan\n        return res\n\n    # handle the nans\n    # nothing to do when nanas is None.\n    if nanas is None:\n        pass\n    elif nanas != \'ignore\':\n        ix = np.isnan(a)\n        if ix.any():\n            a = np.array(a, dtype=""float"")  # make copy of `a`\n            a[ix] = float(nanas)\n        nanas = None\n    elif nanas == \'ignore\' and axis is None:\n        ix = np.isnan(a)\n        if ix.any():\n            ix = (~ix).nonzero()\n            a = a[ix]\n            if weights is not None:\n                weights = weights[ix]\n        nanas = None\n    # if nanas == \'ignore\' and axis is not None:\n        # leave the nans to later recursion on axis.\n\n    if axis is None:\n        # sort and interpolate\n        a = a.ravel()\n        if weights is None:\n            if not sorted:\n                a = np.sort(a)\n            pcum = np.arange(0.5, a.size) / a.size\n        else:\n            weights = weights.ravel()\n            if not sorted:\n                ix = np.argsort(a)\n                a, weights = a[ix], weights[ix]\n            pcum = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights)\n\n        res = np.interp(q, pcum, a)\n        return res\n\n    else:\n        # handle the axis\n        # move the target axis to the last and flatten the rest axes for map\n        a_ = np.moveaxis(a, axis, -1).reshape(-1, a.shape[axis])\n        if weights is None:\n            func = lambda x: quantile(x, q=q, sorted=sorted,\n                                      nmin=None, nanas=nanas)\n            res = list(map(func, a_))\n        else:\n            w_ = np.moveaxis(weights, axis, -1).reshape(a_.shape)\n            func = lambda x, w: quantile(x, weights=w, q=q, sorted=sorted,\n                                         nmin=None, nanas=nanas)\n            res = list(map(func, a_, w_))\n\n        if shape == \'data\':\n            res = np.array(res).reshape(res_shape)\n        elif shape == \'stats\':\n            # put the shape of quantile first\n            res = np.moveaxis(res, 0, -1).reshape(res_shape)\n        return res\n\n\ndef nanquantile(a, weights=None, q=None, nsig=None, origin=\'middle\',\n                axis=None, keepdims=False, sorted=False, nmin=0,\n                nanas=\'ignore\', shape=\'stats\'):\n    """"""Compute the quantile of the data, ignoring NaNs by default.\n\n    Refer to `quantile` for full documentation.\n\n    See Also\n    --------\n    quantile : Not ignoring NaNs by default.\n    """"""\n    return quantile(a, weights=weights, q=q, nsig=nsig, origin=origin,\n                    axis=axis, keepdims=keepdims, sorted=sorted, nmin=nmin,\n                    nanas=nanas, shape=shape)\n\n\ndef conflevel(p, weights=None, q=None, nsig=None, sorted=False,\n              norm=1, uniquefy=False):\n    \'\'\'Calculate the lower confidence bounds with given levels for 2d contour.\n    Be careful when q is very small or many numbers repeat in p.\n\n    conflevel is equivalent to\n        quantile(p, weights=p*weights, q=1-q)\n    or\n        quantile(p, weights=p*weights, nsig=nsig, origin=\'high\')\n\n    Parameters\n    ----------\n    p : array_like\n        Input array. Usually `p` is the probability in grids.\n    weights:\n        Should be bin size/area of corresponding p.\n        Can be ignored for equal binning.\n    q : float or float array in range of [0,1], optional\n        Quantile to compute. One of `q` and `nsig` must be specified.\n    nsig : float, optional\n        Quantile in unit of standard deviation.\n        If `q` is not specified, then `scipy.stats.norm.cdf(nsig)` is used.\n    sorted : bool\n        If True, then the input array is assumed to be in increasing order.\n    norm : float in (0, 1]\n        The weights will be normalized as sum(p * weights) = norm.\n        This is useful when the data points do not cover full probability.\n        See `Examples` for more detail.\n    uniquefy : bool\n        If True, then the repeated items in p will be merged. \n        May improve the precision for low quantile `q` and assure output levels\n        monotonically decreasing with `q`.\n\n    See Also\n    --------\n    quantile\n\n    Examples\n    --------\n    >>> n = 10000\n    >>> x, y = np.random.randn(2, n)\n    >>> xbin, ybin = np.linspace(-2, 2, 10), np.linspace(-2, 2, 15)\n    >>> area = np.diff(xbin)[:, np.newaxis] * np.diff(ybin)\n    >>> h = np.histogram2d(x, y, [xbin, ybin])[0]\n    >>> p = h / n / area\n    >>> levels = conflevel(p, area, q=[0.2, 0.5, 0.8], norm=h.sum()/n)\n    >>> plt.pcolormesh(xbin, ybin, p.T)\n    >>> plt.contour(mid(xbin), mid(ybin), p.T, levels,\n        colors=\'k\', linewidths=2, linestyles=[\'-\', \'--\', \'-.\'])\n    Note that h.sum() is not necessary equal to n.\n\n    # uniquefy\n    >>> np.random.seed(1)\n    >>> x = np.random.randint(100, size=10000)\n    >>> handy.conflevel(a, q=[0.2, 0.5, 0.8])\n    array([ 89.,  71.,  45.])\n    >>> handy.conflevel(a, q=[0.2, 0.5, 0.8], uniquefy=True)\n    array([ 89.19323017,  70.7062598 ,  45.06312568])\n    \'\'\'\n    if q is not None:\n        q = 1 - np.asarray(q)\n\n    if weights is None:\n        weights = p\n    else:\n        weights = weights * p\n\n    if uniquefy:\n        p, weights = globals()[\'uniquefy\'](p, weights)\n\n    if norm == 1:\n        pass\n    elif 0 < norm < 1:\n        # add an extra ""pseudo"" point to cover the probability out of box.\n        # appended array will be flattened\n        p = np.append(0, p)\n        weights = np.append((1 - norm) / norm * np.sum(weights), weights)\n    else:\n        raise ValueError(""`norm` must be in (0, 1]."")\n\n    return quantile(p, weights=weights, q=q, nsig=nsig, origin=\'high\',\n                    sorted=sorted, nmin=None)\n\n\ndef hdregion(x, p, weights=None, q=None, nsig=None):\n    """"""Highest Density Region (HDR), obsoleted by hdr1d.\n    find x s.t.\n        p(x) = sig_level\n    weights:\n        Should be bin size of corresponding p.\n        Can be ignored for equal binning.\n    """"""\n    from .optimize import findroot\n    from .misc import amap\n\n    assert (np.diff(x) >= 0).all()\n\n    levels = conflevel(p, weights=weights, q=q, nsig=nsig)\n    x = np.hstack([x[0], x, x[-1]])\n    p = np.hstack([0, p, 0])\n\n    intervals = amap(lambda lv: findroot(lv, x, p), levels)\n    return intervals\n\n\ndef hdr1d(a, weights=None, q=None, nsig=None, ret_mode=False,\n          grids=100, bw_method=None, span=4):\n    """"""Search the Highest Density Region (HDR) and the Mode for data sample.\n    Important: this snippet is designed for 1D unimodal distribution.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    weights : array_like, optional\n        Weighting of a.\n    q : float or float array in range of [0,1], optional\n        Quantile to compute. One of `q` and `nsig` must be specified.\n    nsig : float, optional\n        Quantile in unit of standard deviation.\n        Ignored when `q` is given.\n    grids : int\n        Number of grid point to calculate the KDE curve.\n    bw_method : str, scalar or callable\n        See docstring of `scipy.stats.gaussian_kde`.\n    span: float or (float, float)\n        lower and upper bounds giving the fraction of (weighted) samples to include.\n        acceptable inputs:\n            - length-2 tuple containing lower and upper bounds\n            - float in (0, 1], giving the fraction of (weighted) samples to include\n            - float > 1, specifying the n-sigma credible interval\n        In the two later cases, the bounds are chosen to be equal-tailed.\n        Default is 4-sigma credible interval.\n    """"""\n    from scipy.stats import gaussian_kde\n    from scipy.interpolate import CubicSpline\n\n    if span is None:\n        xmin, xmax = a.min(), a.max()\n    elif np.isscalar(span):\n        if span > 1:\n            span = 2 * norm.cdf(span) - 1\n        xmin, xmax = quantile(\n            a, weights=weights, q=[(1 - span) * 0.5, (1 + span) * 0.5])\n    else:\n        xmin, xmax = span\n\n    x = np.linspace(xmin, xmax, grids)\n    ix_in = (a >= xmin) & (a <= xmax)\n    if weights is None:\n        y = gaussian_kde(a, bw_method=bw_method)(x)\n        psum = ix_in.mean()\n    else:\n        # known bug, only works for new scipy version!\n        y = gaussian_kde(a, weights=weights, bw_method=bw_method)(x)\n        psum = (weights[ix_in]).sum() / weights.sum()\n    level = conflevel(y, q=q, nsig=nsig, norm=psum)\n\n    f = CubicSpline(x, y - level, extrapolate=False)\n    peak = f.derivative(1).roots()\n    root = f.roots()\n    root = root[~np.isin(root, peak)]  # drop double roots\n\n    # check boundaries\n    root = [root]\n    if f(xmin) > 0:\n        root.insert(0, xmin)\n    if f(xmax) > 0:\n        root.append(xmax)\n    if len(root) == 1:\n        root = root[0]\n    else:\n        root = np.hstack(root)\n\n    if ret_mode:\n        if len(peak) == 1:\n            mode = peak[0]\n        else:\n            # find the maxima closest to the rough peak\n            peak_max = peak[f.derivative(2)(peak) <= 0]\n            if len(peak_max) == 1:\n                mode = peak_max[0]\n            else:\n                peak_raw = x[np.argmax(y)]\n                mode = peak_max[np.argmin(np.abs(peak_max - peak_raw))]\n\n        return root, mode\n    else:\n        return root\n\n\ndef binquantile(x, y, bins=10, weights=None, q=None, nsig=None,\n                origin=\'middle\', nmin=0, nanas=None, shape=\'stats\'):\n    """"""\n    x, y : array_like\n        Input data.\n    bins : array_like or int\n        Bins to compute quantile.\n    weights : array_like, optional\n        Weighting of data.\n    q : float or float array in range of [0,1], optional\n        Quantile to compute. One of `q` and `nsig` must be specified.\n    nsig : float, optional\n        Quantile in unit of standard deviation. Ignored when `q` is given.\n    origin, nmin, nanas:\n        Refer to `quantile` for full documentation.\n    shape : {\'bins\', \'stats\'}\n        Put which axes first in the result:\n            \'bins\' - the shape of bins\n            \'stats\' - the shape of quantiles\n    """"""\n    if weights is None:\n        func = lambda a: quantile(a, q=q, nsig=nsig, origin=origin,\n                                  nmin=nmin, nanas=nanas)\n        stats = binstats(x, [y], bins=bins,\n                         func=func, shape=shape)\n    else:\n        func = lambda a, w: quantile(a, w, q=q, nsig=nsig, origin=origin,\n                                     nmin=nmin, nanas=nanas)\n        stats = binstats(x, [y, weights], bins=bins,\n                         func=func, shape=shape)\n\n    return stats\n\n\ndef alterbinstats(xs, ys, bins=10, func=np.mean, nmin=1, shape=\'stats\'):\n    """"""Make binned statistics for multidimensional data.\n    It allows discontinuous or overlap binning like [[1,5], [3,7], [5,9]]\n    at the cost of speed.\n\n    Refer to `binstats` for full documentation.\n    """"""\n    # check the inputs\n    if not callable(func):\n        raise TypeError(\'`func` must be callable.\')\n    if shape != \'bins\' and shape != \'stats\':\n        raise ValueError(""`shape` must be \'bins\' or \'stats\'"")\n\n    if len(xs) == 0:\n        raise ValueError(""`xs` must be non empty"")\n    if len(ys) == 0:\n        raise ValueError(""`ys` must be non empty"")\n    if np.isscalar(xs[0]):\n        xs = [xs]\n        bins = [bins]\n    if np.isscalar(ys[0]):\n        ys = [ys]\n    if np.isscalar(bins):\n        bins = [bins] * len(xs)\n\n    xs = [np.asarray(x) for x in xs]\n    ys = [np.asarray(y) for y in ys]\n\n    D, N = len(xs), len(xs[0])\n    # `D`: number of dimensions\n    # `N`: number of elements along each dimension\n    for x in xs:\n        if len(x) != N:\n            raise ValueError(""x should have the same length"")\n        if x.ndim != 1:\n            raise ValueError(""x should be 1D array"")\n    for y in ys:\n        if len(y) != N:\n            raise ValueError(""y should have the same length as x"")\n    if len(bins) != D:\n        raise ValueError(""bins should have the same number as xs"")\n\n    # prepare the edges\n    edges = [None] * D\n    for i, bin in enumerate(bins):\n        if np.isscalar(bin):\n            x = xs[i][np.isfinite(xs[i])]  # drop nan, inf\n            if len(x) > 0:\n                xmin, xmax = np.min(x), np.max(x)\n            else:\n                # failed to determine range, so use 0-1.\n                xmin, xmax = 0, 1\n            if xmin == xmax:\n                xmin = xmin - 0.5\n                xmax = xmax + 0.5\n            edge = np.linspace(xmin, xmax, bin + 1)\n        else:\n            edge = np.asarray(bin)\n        if edge.ndim == 1:\n            edge = np.stack([edge[:-1], edge[1:]], -1)\n        edges[i] = edge\n    dims = tuple(len(edge) for edge in edges)\n\n    # statistical value for empty bin\n    with warnings.catch_warnings():\n        # Numpy generates a warnings for mean/std/... with empty list\n        warnings.filterwarnings(\'ignore\', category=RuntimeWarning)\n        try:\n            yselect = [y[:0] for y in ys]\n            null = np.asarray(func(*yselect))\n        except Exception:\n            yselect = [y[:1] for y in ys]\n            temp = np.asarray(func(*yselect))\n            null = np.full_like(temp, np.nan, dtype=\'float\')\n\n    # prepare the results\n    count = np.empty(dims, dtype=\'int\')\n    stats = np.empty(dims + null.shape, dtype=null.dtype)\n\n    # prepare the bin index\n    idx = [None] * D\n    strides = np.array(count.strides) / count.itemsize\n    iter_ij = product(*[range(n) for n in dims])\n\n    # cache indexes of last dimension\n    last_index = [None] * dims[-1]\n    for j in range(dims[-1]):\n        ix0 = (xs[-1] >= edges[-1][j, 0])\n        ix1 = (xs[-1] <= edges[-1][j, 1])\n        last_index[j] = ix0 & ix1\n\n    # make statistics on each bin\n    for n, ij in enumerate(iter_ij):\n        idx[-1] = last_index[ij[-1]]\n        if D > 1:\n            for i, j in enumerate(ij[:-1]):\n                if n % strides[i] == 0:\n                    ix0 = (xs[i] >= edges[i][j, 0])\n                    ix1 = (xs[i] <= edges[i][j, 1])\n                    idx[i] = ix0 & ix1\n                    if i > 0:\n                        idx[i] = idx[i] & idx[i - 1]\n            idx[-1] = idx[-1] & idx[-2]\n\n        ix = idx[-1].nonzero()[0]\n        count[ij] = ix.size\n\n        if count[ij] >= nmin:\n            yselect = [y[ix] for y in ys]\n            stats[ij] = func(*yselect)\n        else:\n            stats[ij] = null\n\n    # change to proper shape\n    if shape == \'stats\':\n        stats = stats.reshape((-1,) + null.shape)\n        stats = np.moveaxis(stats, 0, -1).reshape(null.shape + dims)\n    return BinStats(stats, edges, count)\n\n\nWStats = namedtuple(\'WStats\',\n                    \'avg, std, med, sigs, sig1, sig2, sig3,\'\n                    \'x, w, var, mean, median,\'\n                    \'sig1a, sig1b, sig2a, sig2b, sig3a, sig3b\')\n\n\ndef wstats(x, weights=None, axis=None, keepdims=False):\n    """"""\n    a = wstats(randn(100))\n    """"""\n    x = np.asarray(x)\n    if weights is not None:\n        weights = np.asarray(weights)\n        if weights.shape != x.shape:\n            raise ValueError(""weights must have same shape with x"")\n    else:\n        weights = np.ones_like(x)\n    w = weights / np.sum(weights, axis=axis, keepdims=True)\n\n    avg = np.sum(x * w, axis=axis, keepdims=keepdims)\n    var = np.sum(x**2 * w, axis=axis, keepdims=keepdims) - avg**2\n    std = var**0.5\n    sig = quantile(x, w, nsig=[0, -1, 1, -2, 2, -3, 3], axis=axis, keepdims=keepdims)\n\n    med, sig1a, sig1b, sig2a, sig2b, sig3a, sig3b = sig\n    sig1, sig2, sig3 = sig[1:3], sig[3:5], sig[5:7]\n    mean, median = avg, med\n\n    return WStats(avg, std, med, sig, sig1, sig2, sig3,\n                  x, weights, var, mean, median,\n                  sig1a, sig1b, sig2a, sig2b, sig3a, sig3b)\n\n\nif __name__ == \'__main__\':\n    import numpy as np\n    from numpy.random import randn\n    x, y, z = randn(3, 1000)\n    b = np.linspace(-2, 2, 11)\n    binstats(x, y, 10, np.mean)\n    binstats(x, y, b, np.mean)\n    binstats(x, y, b, np.mean, nmin=100)\n    binstats(x, [y, z], 10, lambda x, y: np.mean(x + y))\n    binstats(x, [y, z], 10, lambda x, y: [np.mean(x), np.std(y)])\n    binstats([x, y], z, (10, 10), np.mean)\n    binstats([x, y], z, [b, b], np.mean)\n    binstats([x, y], [z, z], 10, lambda x, y: [np.mean(x), np.std(y)])\n\n    b1 = np.linspace(-2, 2, 11)\n    b2 = np.linspace(-2, 2, 21)\n    binstats([x, y], [z, z], [b1, b2], lambda x, y: [np.mean(x), np.std(y)])\n\n    from scipy.stats import binned_statistic_dd\n    s1 = binned_statistic_dd(x, x, \'std\', bins=[b])[0]\n    s2 = binstats(x, x, bins=b, func=np.std)[0]\n    # print(s1, s2)\n    assert np.allclose(s1, s2)\n\n    s1 = binned_statistic_dd([x, y], z, \'sum\', bins=[b, b])[0]\n    s2 = binstats([x, y], z, bins=[b, b], func=np.sum)[0]\n    # print(s1, s2)\n    assert np.allclose(s1, s2)\n\n    a = quantile(np.arange(10), q=[0.1, 0.5, 0.85])\n    assert np.allclose(a, [0.5, 4.5, 8.])\n    a = np.arange(12).reshape(3, 4)\n    b = quantile(a, q=0.5, axis=0)\n    c = quantile(a, q=0.5, axis=1)\n    assert np.allclose(b, [4., 5., 6., 7.])\n    assert np.allclose(c, [1.5, 5.5, 9.5])\n'"
