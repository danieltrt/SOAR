file_path,api_count,code
__init__.py,0,b'from ndtest import *\n'
ndtest.py,14,"b""from __future__ import division\nimport numpy as np\nfrom numpy import random\nfrom scipy.spatial.distance import pdist, cdist\nfrom scipy.stats import kstwobign, pearsonr\nfrom scipy.stats import genextreme\n\n__all__ = ['ks2d2s', 'estat', 'estat2d']\n\n\ndef ks2d2s(x1, y1, x2, y2, nboot=None, extra=False):\n    '''Two-dimensional Kolmogorov-Smirnov test on two samples. \n    Parameters\n    ----------\n    x1, y1 : ndarray, shape (n1, )\n        Data of sample 1.\n    x2, y2 : ndarray, shape (n2, )\n        Data of sample 2. Size of two samples can be different.\n    extra: bool, optional\n        If True, KS statistic is also returned. Default is False.\n\n    Returns\n    -------\n    p : float\n        Two-tailed p-value.\n    D : float, optional\n        KS statistic. Returned if keyword `extra` is True.\n\n    Notes\n    -----\n    This is the two-sided K-S test. Small p-values means that the two samples are significantly different. Note that the p-value is only an approximation as the analytic distribution is unkonwn. The approximation is accurate enough when N > ~20 and p-value < ~0.20 or so. When p-value > 0.20, the value may not be accurate, but it certainly implies that the two samples are not significantly different. (cf. Press 2007)\n\n    References\n    ----------\n    Peacock, J.A. 1983, Two-Dimensional Goodness-of-Fit Testing in Astronomy, Monthly Notices of the Royal Astronomical Society, vol. 202, pp. 615-627\n    Fasano, G. and Franceschini, A. 1987, A Multidimensional Version of the Kolmogorov-Smirnov Test, Monthly Notices of the Royal Astronomical Society, vol. 225, pp. 155-170\n    Press, W.H. et al. 2007, Numerical Recipes, section 14.8\n\n    '''\n    assert (len(x1) == len(y1)) and (len(x2) == len(y2))\n    n1, n2 = len(x1), len(x2)\n    D = avgmaxdist(x1, y1, x2, y2)\n\n    if nboot is None:\n        sqen = np.sqrt(n1 * n2 / (n1 + n2))\n        r1 = pearsonr(x1, y1)[0]\n        r2 = pearsonr(x2, y2)[0]\n        r = np.sqrt(1 - 0.5 * (r1**2 + r2**2))\n        d = D * sqen / (1 + r * (0.25 - 0.75 / sqen))\n        p = kstwobign.sf(d)\n    else:\n        n = n1 + n2\n        x = np.concatenate([x1, x2])\n        y = np.concatenate([y1, y2])\n        d = np.empty(nboot, 'f')\n        for i in range(nboot):\n            idx = random.choice(n, n, replace=True)\n            ix1, ix2 = idx[:n1], idx[n1:]\n            #ix1 = random.choice(n, n1, replace=True)\n            #ix2 = random.choice(n, n2, replace=True)\n            d[i] = avgmaxdist(x[ix1], y[ix1], x[ix2], y[ix2])\n        p = np.sum(d > D).astype('f') / nboot\n    if extra:\n        return p, D\n    else:\n        return p\n\n\ndef avgmaxdist(x1, y1, x2, y2):\n    D1 = maxdist(x1, y1, x2, y2)\n    D2 = maxdist(x2, y2, x1, y1)\n    return (D1 + D2) / 2\n\n\ndef maxdist(x1, y1, x2, y2):\n    n1 = len(x1)\n    D1 = np.empty((n1, 4))\n    for i in range(n1):\n        a1, b1, c1, d1 = quadct(x1[i], y1[i], x1, y1)\n        a2, b2, c2, d2 = quadct(x1[i], y1[i], x2, y2)\n        D1[i] = [a1 - a2, b1 - b2, c1 - c2, d1 - d2]\n\n    # re-assign the point to maximize difference,\n    # the discrepancy is significant for N < ~50\n    D1[:, 0] -= 1 / n1\n\n    dmin, dmax = -D1.min(), D1.max() + 1 / n1\n    return max(dmin, dmax)\n\n\ndef quadct(x, y, xx, yy):\n    n = len(xx)\n    ix1, ix2 = xx <= x, yy <= y\n    a = np.sum(ix1 & ix2) / n\n    b = np.sum(ix1 & ~ix2) / n\n    c = np.sum(~ix1 & ix2) / n\n    d = 1 - a - b - c\n    return a, b, c, d\n\n\ndef estat2d(x1, y1, x2, y2, **kwds):\n    return estat(np.c_[x1, y1], np.c_[x2, y2], **kwds)\n\n\ndef estat(x, y, nboot=1000, replace=False, method='log', fitting=False):\n    '''\n    Energy distance statistics test.\n    Reference\n    ---------\n    Aslan, B, Zech, G (2005) Statistical energy as a tool for binning-free\n      multivariate goodness-of-fit tests, two-sample comparison and unfolding.\n      Nuc Instr and Meth in Phys Res A 537: 626-636\n    Szekely, G, Rizzo, M (2014) Energy statistics: A class of statistics\n      based on distances. J Stat Planning & Infer 143: 1249-1272\n    Brian Lau, multdist, https://github.com/brian-lau/multdist\n\n    '''\n    n, N = len(x), len(x) + len(y)\n    stack = np.vstack([x, y])\n    stack = (stack - stack.mean(0)) / stack.std(0)\n    if replace:\n        rand = lambda x: random.randint(x, size=x)\n    else:\n        rand = random.permutation\n\n    en = energy(stack[:n], stack[n:], method)\n    en_boot = np.zeros(nboot, 'f')\n    for i in range(nboot):\n        idx = rand(N)\n        en_boot[i] = energy(stack[idx[:n]], stack[idx[n:]], method)\n\n    if fitting:\n        param = genextreme.fit(en_boot)\n        p = genextreme.sf(en, *param)\n        return p, en, param\n    else:\n        p = (en_boot >= en).sum() / nboot\n        return p, en, en_boot\n\n\ndef energy(x, y, method='log'):\n    dx, dy, dxy = pdist(x), pdist(y), cdist(x, y)\n    n, m = len(x), len(y)\n    if method == 'log':\n        dx, dy, dxy = np.log(dx), np.log(dy), np.log(dxy)\n    elif method == 'gaussian':\n        raise NotImplementedError\n    elif method == 'linear':\n        pass\n    else:\n        raise ValueError\n    z = dxy.sum() / (n * m) - dx.sum() / n**2 - dy.sum() / m**2\n    # z = ((n*m)/(n+m)) * z # ref. SR\n    return z\n"""
