file_path,api_count,code
main.py,0,"b'\'\'\'\nML-Fomo ~ Written by Taha HICHRI <hishri.taha@gmail.com>, March 2019\n\nThis software is GPL licensed. The work based off of it must be released as open source.\n\nThis program is free software: you can redistribute it and/or modify it under the terms of the \nGNU General Public License as published by the Free Software Foundation, either version 3 of the License,\nor (at your option) any later version.\n\nThis file is subject to the terms and conditions defined in\nfile \'LICENSE.txt\', which is part of this source code package.\n\'\'\'\nimport re \nimport sys\nimport nltk\nimport tweepy \nimport numpy as np\nfrom tabulate import tabulate\nfrom textblob import TextBlob\nfrom langdetect import detect\nfrom tweepy import OAuthHandler \nfrom collections import Counter\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.parse.generate import generate, demo_grammar\nfrom nltk.parse import ShiftReduceParser\nfrom nltk import CFG\nimport language_check\nfrom textblob.decorators import requires_nltk_corpus\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n\nclass TwitterAnalyzer(object): \n\t_classifier = None\n\n\t_checkLang = \'\'\n\n\t# The ignored words are twitter hashtags, links, smileys, and the search word themselves\n\t# This set is not final, stopwords from the wordCloud , NLTK are included also\n\tignored_words = {\'RT\', \'#\', \'https\', \'_twt\'}\n\n\t# the detected langs are sets of unique elements\n\tdetected_langs = set()\n\n\t# words dictionary\n\twords = []\n\tsearch_words = []\n\n\tstop_words = []\n\n\tdef __init__(self):\n\t\tself._checkLang =  language_check.LanguageTool(\'en-US\')\n\n\t\tprint(f\'\\nDownloading/fetching stopwords ..\')\n\t\tnltk.download(\'stopwords\')\n\t\tprint(f\'Crunching data ..\\n\')\n\n\t\t# TODO insert your Twitter API keys here\n\t\t# Create a developer account and request access\n\t\t# @link{ https://developer.twitter.com/en/apply-for-access.html} \n\t\tconsumer_key        = \'<consumer_key>\'\n\t\tconsumer_secret     = \'<consumer_secret>\'\n\t\taccess_token        = \'<access_token>\'\n\t\taccess_token_secret = \'<access_token_secret>\'\n\n\t\ttry: \n\t\t\tself.auth = OAuthHandler(consumer_key, consumer_secret) \n\t\t\tself.auth.set_access_token(access_token, access_token_secret) \n\t\t\tself.api = tweepy.API(self.auth)  \n\t\t\t# print(self.api.auth._get_request_token.value)\n\n\t\texcept: \n\t\t\tprint(""Error: Authentication Failed"") \n\n\t\t\n\t\n\tdef sanitize_text(self, text):\n\t\ttry:\n\t\t\tif detect(text) == \'en\':\n\t\t\t\tallow_in_dict = True\n\t\t\telse:\n\t\t\t\tallow_in_dict = False\n\t\texcept:\n\t\t\tallow_in_dict = False\n\n\t\t# remove non-words\n\t\tsanitized_text = \' \'.join(re.sub(""(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t]) |(\\w+:\\/\\/\\S+)"", "" "", text).split()) \n\n\t\tself.stop_words = set(stopwords.words(\'english\'))\n\t\tself.stop_words.update(STOPWORDS)\n\t\tself.stop_words.update(self.ignored_words)\n\t\t\n\t\tword_tokens = word_tokenize(sanitized_text) \n  \n\t\t#filtered_sentence = [w for w in word_tokens if not w in stop_words and len(w) > 1] \n  \n\t\tfiltered_sentence = [] \n\t\t# not ignored and > 1 (punctation and stuff)\n\t\tfor w in word_tokens: \n\t\t    if w not in self.stop_words and len(w) > 3 and allow_in_dict : \n\t\t        filtered_sentence.append(w) \n\t\t#print (filtered_sentence)\n\n\t\t# add words without stopwords to list\n\t\tself.words += filtered_sentence\n\n\t\t# I am going to need the whole text for a better classification\n\t\treturn sanitized_text\n\n\n\tdef train(self):\n\t\tsuper(NaiveBayesAnalyzer, self).train()\n\t\tself._classifier = nltk.classify.NaiveBayesClassifier.train(train_data)\n\t\t\n\n\t# Classify by polarity and subjectivity using TextBlob\n\tdef get_sentiment(self, text):\n\t\t# Keep idomatic text\n\t\ttext = self.sanitize_text(text)\n\n\t\tanalysis = TextBlob(text)\n\n\t\t# set sentiment \n\t\tif analysis.sentiment.polarity > 0: \n\t\t\treturn \'positive\'\n\t\telif analysis.sentiment.polarity == 0: \n\t\t\treturn \'neutral\'\n\t\telse: \n\t\t\treturn \'negative\'\n\n\n\tdef guess_the_news(self, words):\n\t\ttemp = set()\n\t\tfor word in words:\n\t\t\ttemp.add(word[0])\n\t\n\t\tmatches = self._checkLang.check(\' \'.join(temp))\n\t\t\n\t\tprint (f\'\\nHere is an auto-generated guess of what people are saying:\\n\')\n\n\t\tprint (language_check.correct(\' \'.join(temp), matches))\n\t\t\n\n\n\n\tdef fetch_tweets(self, query, count = 500): \n\t\t# empty list to store parsed tweets \n\t\ttweets = [] \n\n\t\t# the words included in the query should be ignored from most frequently used words\n\t\tself.ignored_words.update(query.split())\n\t\t#print (self.ignored_words)\n\t\n\t\ttry: \n\t\t\t# fetch tweets \n\t\t\tfetched_tweets = self.api.search(q = query, count=count) \n\t\t\t# extract tweet body and guess sentiment \n\t\t\tfor tweet in fetched_tweets: \n\t\t\t\t# empty dictionary for tweet, sentiment \n\t\t\t\tparsed_tweet = {} \n\t\t\t\tparsed_tweet[\'text\'] = tweet.text.lower()\n\t\t\t\tparsed_tweet[\'sentiment\'] = self.get_sentiment(tweet.text) \n\n\t\t\t\t# Exclude retweets\n\t\t\t\tif tweet.retweet_count > 0: \n\t\t\t\t\tif parsed_tweet not in tweets: \n\t\t\t\t\t\ttweets.append(parsed_tweet) \n\t\t\t\telse: \n\t\t\t\t\ttweets.append(parsed_tweet) \n\n\t\t\t# Parsed tweets \n\t\t\treturn tweets \n\n\t\texcept tweepy.TweepError as e: \n\t\t\tprint(""Error : "" + str(e)) \n\n\ndef main(): \n\t# creating object of TwitterClient Class \n\tapi = TwitterAnalyzer() \n\t# calling function to get tweets \n\ttweets = api.fetch_tweets(query = sys.argv[1], count = sys.argv[2] if len(sys.argv) < 2 else 500)\n\n\t# most occuring real words\n\tterms_occurence = Counter(api.words)\n\tprint(f\'\\nMost frequently used words\')\n\tprint(terms_occurence.most_common(5))\n\n\n\t# picking positive tweets from tweets \n\tptweets = [tweet for tweet in tweets if tweet[\'sentiment\'] == \'positive\'] \n\tntweets = [tweet for tweet in tweets if tweet[\'sentiment\'] == \'negative\']  \n\n\tpositive_tweet_percentage = 100 * len(ptweets)/len(tweets)\n\tnegative_tweet_percentage = 100 * len(ntweets)/len(tweets)\n\tnatural_tweet_percentage  = 100 * ( len(tweets) - len(ntweets) - len(ptweets) ) / len(tweets)\n\n\ttable = [[""Positive"",len(ptweets),positive_tweet_percentage],\n\t[""Negative"",len(ntweets),negative_tweet_percentage],\n\t[""Neutral"",( len(tweets) - len(ntweets) - len(ptweets)),natural_tweet_percentage],\n\t[""Total"", len(tweets), 100 * len(tweets)/len(tweets) ]]\n\n\t# print a grid-formatted table with stats.\n\tprint (f\'\\nProcessed tweets stats (non english and REs ignored).\\n\')\n\tprint(tabulate(table, headers=[""Polarity"",""Number"", ""Percentage""],tablefmt=""grid""))\n\n\t\n\tdictionary_str = \' \'.join(api.words)\n\n\tapi.guess_the_news(terms_occurence.most_common(15))\n\n\n\t# Config and show cloud of most used words\n\twordcloud = WordCloud(stopwords=api.stop_words, max_font_size=40).generate(dictionary_str)\n\tplt.figure()\n\tplt.imshow(wordcloud, interpolation=""bilinear"")\n\tplt.axis(""off"")\n\tplt.show()\n\n\n\n\n#one argument should be passed of type string, read on function on main\nif __name__ == ""__main__"": \n\t# calling main function \n\tmain() '"
install/install.py,0,"b'import os\nimport sys\nimport subprocess\n\ndef main():\n    # where am I?\n    location = os.path.dirname(os.path.realpath(__file__))\n    \n    # try to install\n    try:\n        os.system(""pip install -r "" + location + ""/dependencies.txt"")\n    except Exception as e:\n        print(""Error Occured: "" + str(e))\n        return\n\n\nif __name__ == \'__main__\':\n    main()'"
