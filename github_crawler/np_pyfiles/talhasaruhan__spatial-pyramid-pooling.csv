file_path,api_count,code
spp_layer.py,11,"b'import numpy as np\r\nimport math\r\nimport tensorflow as tf\r\n\r\n# input feature maps is of the form: N-C-(WH)/(HW)\r\n# ex. spatial_pyramid:\r\n#\t[[1, 1], [2, 2], [3, 3], [4, 5]]\r\n# each row is a level of pyramid with nxm pooling\r\ndef np_spatial_pyramid_pooling(input_feature_maps, spatial_pyramid, dtype=np.float32):\r\n\tassert input_feature_maps.ndim == 4\r\n\tassert spatial_pyramid.ndim == 2\r\n\tassert spatial_pyramid.shape[1] == 2\r\n\r\n\tbatch_size = input_feature_maps.shape[0]\r\n\tnum_channels = input_feature_maps.shape[1]\r\n\th = input_feature_maps.shape[2]\r\n\tw = input_feature_maps.shape[3]\r\n\r\n\tnum_levels = spatial_pyramid.shape[0]\r\n\r\n\t# N-C-W*H\r\n\tflattened_feature_maps = np.reshape(input_feature_maps, (batch_size, num_channels, -1))\r\n\tnum_px = flattened_feature_maps.shape[2]\r\n\r\n\tbins_per_level = np.prod(spatial_pyramid, axis=1)\r\n\tnum_bins = np.sum(bins_per_level)\r\n\tstack = []\r\n\r\n\t# stride tricks, then max pool along one dimension \r\n\t# then stride tricks again and max pool along the other dimension\r\n\t# but whats the length and stride?\r\n\t# ceil(w/n) for window size, floor(w/n) for stride,\r\n\t# where w is the original dim, and n is the number of bins along the dim \r\n\t# but this implementation may leave out some pixels (consider w = 5, n = 3)\r\n\r\n\tsizeof_item = np.dtype(dtype).itemsize\r\n\r\n\tfor i in range(num_levels):\r\n\t\tn_h = spatial_pyramid[i][0]\r\n\t\tn_w = spatial_pyramid[i][1]\r\n\t\t\r\n\t\tl = math.ceil(w/n_w)\r\n\t\ts = math.floor(w/n_w)\r\n\r\n\t\tar = np.lib.stride_tricks.as_strided(flattened_feature_maps, (batch_size, num_channels, h, n_w, l), \r\n\t\t\t(sizeof_item*num_px*num_channels, sizeof_item*num_px, sizeof_item*w, sizeof_item*s, sizeof_item))\r\n\t\tar = np.transpose(np.amax(ar, axis=4), (0, 1, 3, 2)).copy()\r\n\r\n\t\tl = math.ceil(h/n_h)\r\n\t\ts = math.floor(h/n_h)\r\n\r\n\t\tar = np.lib.stride_tricks.as_strided(ar, (batch_size, num_channels, n_w, n_h, l), \r\n\t\t\t(sizeof_item*n_w*h*num_channels, sizeof_item*n_w*h, sizeof_item*h, sizeof_item*s, sizeof_item))\r\n\t\tar = np.transpose(np.amax(ar, axis=4), (0, 1, 3, 2))\r\n\r\n\t\t# for debugging purposes, monitor ""ar"" here, before flattening\r\n\r\n\t\tstack.append(np.reshape(ar, (batch_size, num_channels, -1)))\r\n\r\n\tstack = np.concatenate(stack, axis=2)\r\n\tprint(stack.shape)\r\n\r\n\treturn stack\r\n\r\ndef tf_spatial_pyramid_pooling(tf_input_feature_maps, tf_spatial_pyramid, dtype=tf.float32):\r\n\treturn tf.py_func(np_spatial_pyramid_pooling, [tf_input_feature_maps, tf_spatial_pyramid], dtype)\r\n'"
test.py,3,"b'## TEST\r\n\r\nfrom spp_layer import np_spatial_pyramid_pooling, tf_spatial_pyramid_pooling\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\na = np.random.randint(100, size=(1, 3, 6, 5)).astype(np.int32)\r\n\r\nspt = np.array([\r\n[3, 5], [2, 2]\r\n])\r\n\r\nprint(a.shape)\r\nprint(a)\r\n\r\nfixed_size_representation = np_spatial_pyramid_pooling(a, spt, dtype=np.int32)\r\nprint(fixed_size_representation)\r\n\r\nwith tf.Session() as sess:\r\n\ttf_a = tf.constant(a)\r\n\ttf_spt = tf.constant(spt)\r\n\ty = tf_spatial_pyramid_pooling(tf_a, tf_spt, tf.int32)\r\n\tprint(sess.run(y))'"
