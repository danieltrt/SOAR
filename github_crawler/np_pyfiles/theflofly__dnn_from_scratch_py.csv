file_path,api_count,code
dnn_from_scratch.py,40,"b'import numpy as np\nimport csv\nimport predict as util\n\n\nclass NeuralNetwork:\n    def __init__(self):\n\n        # load the dataset from the CSV file\n        reader = csv.reader(open(""normalized_car_features.csv"", ""r""), delimiter="","")\n        x = list(reader)\n        features = np.array(x[2:]).astype(""float"")\n        np.random.shuffle(features)\n\n        # car attribute and price are splitted, note that 1 is appended at each car for the bias\n        data_x = np.concatenate((features[:, :3], np.ones((features.shape[0], 1))), axis=1)\n        data_y = features[:, 3:]\n\n        # we save the dataset metadata for the prediction part of the network\n        self.predict = util.Predict(float(x[0][0]), float(x[0][1]), float(x[0][2]), float(x[0][3]), float(x[0][4]),\n                                    float(x[0][5]))\n\n        # we set a threshold at 80% of the data\n        self.m = float(features.shape[0])\n        self.m_train_set = int(self.m * 0.8)\n\n        # we split the train and test set using the threshold\n        self.x, self.x_test = data_x[:self.m_train_set, :], data_x[self.m_train_set:, :]\n        self.y, self.y_test = data_y[:self.m_train_set, :], data_y[self.m_train_set:, :]\n\n        # we init the network parameters\n        self.z2, self.a2, self.z3, self.a3, self.z4, self.a4 = (None,) * 6\n        self.delta2, self.delta3, self.delta4 = (None,) * 3\n        self.djdw1, self.djdw2, self.djdw3 = (None,) * 3\n        self.gradient, self.numericalGradient = (None,) * 2\n        self.Lambda = 0.01\n        self.learning_rate = 0.01\n\n        # we init the weights using the blog post values\n        self.w1 = np.matrix([\n            [0.01, 0.05, 0.07],\n            [0.2, 0.041, 0.11],\n            [0.04, 0.56, 0.13],\n            [0.1, 0.1, 0.1]\n        ])\n\n        self.w2 = np.matrix([\n            [0.04, 0.78],\n            [0.4, 0.45],\n            [0.65, 0.23],\n            [0.1, 0.1]\n        ])\n\n        self.w3 = np.matrix([\n            [0.04],\n            [0.41],\n            [0.1]\n        ])\n\n    def forward(self):\n\n        # first layer\n        self.z2 = np.dot(self.x, self.w1)\n        self.a2 = np.tanh(self.z2)\n\n        # we add the the 1 unit (bias) at the output of the first layer\n        ba2 = np.ones((self.x.shape[0], 1))\n        self.a2 = np.concatenate((self.a2, ba2), axis=1)\n\n        # second layer\n        self.z3 = np.dot(self.a2, self.w2)\n        self.a3 = np.tanh(self.z3)\n\n        # we add the the 1 unit (bias) at the output of the second layer\n        ba3 = np.ones((self.a3.shape[0], 1))\n        self.a3 = np.concatenate((self.a3, ba3), axis=1)\n\n        # output layer, prediction of our network\n        self.z4 = np.dot(self.a3, self.w3)\n        self.a4 = np.tanh(self.z4)\n\n    def backward(self):\n\n        # gradient of the cost function with regards to W3\n        self.delta4 = np.multiply(-(self.y - self.a4), tanh_prime(self.z4))\n        self.djdw3 = (self.a3.T * self.delta4) / self.m_train_set + self.Lambda * self.w3\n\n        # gradient of the cost function with regards to W2\n        self.delta3 = np.multiply(self.delta4 * self.w3.T, tanh_prime(np.concatenate((self.z3, np.ones((self.z3.shape[0], 1))), axis=1)))\n        self.djdw2 = (self.a2.T * np.delete(self.delta3, 2, axis=1)) / self.m_train_set + self.Lambda * self.w2\n\n        # gradient of the cost function with regards to W1\n        self.delta2 = np.multiply(np.delete(self.delta3, 2, axis=1) * self.w2.T, tanh_prime(np.concatenate((self.z2, np.ones((self.z2.shape[0], 1))), axis=1)))\n        self.djdw1 = (self.x.T * np.delete(self.delta2, 3, axis=1)) / self.m_train_set + self.Lambda * self.w1\n\n    def update_gradient(self):\n        self.w1 -= self.learning_rate * self.djdw1\n        self.w2 -= self.learning_rate * self.djdw2\n        self.w3 -= self.learning_rate * self.djdw3\n\n    def cost_function(self):\n        return 0.5 * sum(np.square((self.y - self.a4))) / self.m_train_set + (self.Lambda / 2) * (\n            np.sum(np.square(self.w1)) +\n            np.sum(np.square(self.w2)) +\n            np.sum(np.square(self.w3))\n        )\n\n    def set_weights(self, weights):\n        self.w1 = np.reshape(weights[0:12], (4, 3))\n        self.w2 = np.reshape(weights[12:20], (4, 2))\n        self.w3 = np.reshape(weights[20:23], (3, 1))\n\n    def compute_gradients(self):\n        nn.forward()\n        nn.backward()\n        self.gradient = np.concatenate((self.djdw1.ravel(), self.djdw2.ravel(), self.djdw3.ravel()), axis=1).T\n\n    def compute_numerical_gradients(self):\n        weights = np.concatenate((self.w1.ravel(), self.w2.ravel(), self.w3.ravel()), axis=1).T\n\n        self.numericalGradient = np.zeros(weights.shape)\n        perturbation = np.zeros(weights.shape)\n        e = 1e-4\n\n        for p in range(len(weights)):\n            # Set perturbation vector\n            perturbation[p] = e\n\n            self.set_weights(weights + perturbation)\n            self.forward()\n            loss2 = self.cost_function()\n\n            self.set_weights(weights - perturbation)\n            self.forward()\n            loss1 = self.cost_function()\n\n            self.numericalGradient[p] = (loss2 - loss1) / (2 * e)\n\n            perturbation[p] = 0\n\n        self.set_weights(weights)\n\n    def check_gradients(self):\n        self.compute_gradients()\n        self.compute_numerical_gradients()\n        print(""Gradient checked: "" + str(np.linalg.norm(self.gradient - self.numericalGradient) / np.linalg.norm(\n            self.gradient + self.numericalGradient)))\n\n    def predict(self, X):\n        self.x = X\n        self.forward()\n        return self.a4\n\n    def r2(self):\n        y_mean = np.mean(self.y)\n        ss_res = np.sum(np.square(self.y - self.a4))\n        ss_tot = np.sum(np.square(self.y - y_mean))\n        return 1 - (ss_res / ss_tot)\n\n    def summary(self, step):\n        print(""Iteration: %d, Loss %f"" % (step, self.cost_function()))\n        print(""RMSE: "" + str(np.sqrt(np.mean(np.square(self.a4 - self.y)))))\n        print(""MAE: "" + str(np.sum(np.absolute(self.a4 - self.y)) / self.m_train_set))\n        print(""R2: "" + str(self.r2()))\n\n    def predict_price(self, km, fuel, age):\n        self.x = np.concatenate((self.predict.input(km, fuel, age), np.ones((1, 1))), axis=1)\n        nn.forward()\n        print(""Predicted price: "" + str(self.predict.output(self.a4[0])))\n\n\ndef tanh_prime(x):\n    return 1.0 - np.square(np.tanh(x))\n\n\nnn = NeuralNetwork()\n\nprint(""### Gradient checking ###"")\nnn.check_gradients()\n\nprint(""### Training data ###"")\nnb_it = 5000\nfor step in range(nb_it):\n\n    nn.forward()\n    nn.backward()\n    nn.update_gradient()\n\n    if step % 100 == 0:\n        nn.summary(step)\n\nprint(""### Testing data ###"")\nnn.x = nn.x_test\nnn.y = nn.y_test\nnn.forward()\n\nprint(""### Testing summary ###"")\nnn.summary(nb_it)\n\nprint(""### Predict ###"")\nnn.predict_price(168000, ""Diesel"", 5)\n'"
dnn_from_scratch_tensorflow.py,2,"b'import csv\nimport os\nimport shutil\nimport numpy as np\nimport tensorflow as tf\nimport predict as util\n\nsaved_model_directory = ""saved_model""\n\n# if a saved model directory exist we delete it because TF\n# will not overwrite it (and throw an error)\nif os.path.isdir(saved_model_directory):\n    shutil.rmtree(saved_model_directory)\n\n# read the data from the CSV\nreader = csv.reader(open(""normalized_car_features.csv"", ""r""), delimiter="","")\nx = list(reader)\nfeatures = np.array(x[2:]).astype(""float"")\nnp.random.shuffle(features)\n\npredict = util.Predict(float(x[0][0]), float(x[0][1]), float(x[0][2]), float(x[0][3]), float(x[0][4]),\n                       float(x[0][5]))\n\ndata_x = features[:, :3]\ndata_y = features[:, 3:]\n\n# size of the dataset\nm = float(features.shape[0])\n\n# size of the train set\ntrain_set_size = int(m * 0.8)\n\n# the data are splitted between the train and test set\nx_data, x_test = data_x[:train_set_size, :], data_x[train_set_size:, :]\ny_data, y_test = data_y[:train_set_size, :], data_y[train_set_size:, :]\n\n# regularization strength\nLambda = 0.01\nlearning_rate = 0.01\n\nwith tf.name_scope(\'input\'):\n    # training data\n    x = tf.placeholder(""float"", name=""cars"")\n    y = tf.placeholder(""float"", name=""prices"")\n\nwith tf.name_scope(\'weights\'):\n    w1 = tf.Variable(tf.random_normal([3, 3]), name=""W1"")\n    w2 = tf.Variable(tf.random_normal([3, 2]), name=""W2"")\n    w3 = tf.Variable(tf.random_normal([2, 1]), name=""W3"")\n\nwith tf.name_scope(\'biases\'):\n    # biases (we separate them from the weights because it is easier to do that when using TensorFlow)\n    b1 = tf.Variable(tf.random_normal([1, 3]), name=""b1"")\n    b2 = tf.Variable(tf.random_normal([1, 2]), name=""b2"")\n    b3 = tf.Variable(tf.random_normal([1, 1]), name=""b3"")\n\nwith tf.name_scope(\'layer_1\'):\n    # three hidden layer\n    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, w1), b1))\n\nwith tf.name_scope(\'layer_2\'):\n    layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, w2), b2))\n\nwith tf.name_scope(\'layer_3\'):\n    layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, w3), b3))\n\nwith tf.name_scope(\'regularization\'):\n    # L2 regularization applied on each weight\n    regularization = tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(w3)\n\nwith tf.name_scope(\'loss\'):\n    # loss function + regularization value\n    loss = tf.reduce_mean(tf.square(layer_3 - y)) + Lambda * regularization\n    loss = tf.Print(loss, [loss], ""loss"")\n\nwith tf.name_scope(\'train\'):\n    # we\'ll use gradient descent as optimization algorithm\n    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\n# launching the previously defined model begins here\ninit = tf.global_variables_initializer()\n\n# we\'ll saved the model once the training is done\nbuilder = tf.saved_model.builder.SavedModelBuilder(saved_model_directory)\n\nwith tf.Session() as session:\n    session.run(init)\n\n    # we\'ll make 5000 gradient descent iteration\n    for i in range(10000):\n        session.run(train_op, feed_dict={x: x_data, y: y_data})\n\n    builder.add_meta_graph_and_variables(session, [""dnn_from_scratch_tensorflow""])\n\n    # testing the network\n    print(""Testing data"")\n    print(""Loss: "" + str(session.run([loss], feed_dict={x: x_test, y: y_test})[0]))\n\n    # do a forward pass\n    print(""Predicted price: "" + str(predict.output(session.run(layer_3,\n                                                               feed_dict={x: predict.input(168000, ""Diesel"", 5)}))))\n\n# saving the model\nbuilder.save()\n'"
dnn_from_scratch_tensorflow_load_model.py,0,"b'import csv\nimport tensorflow as tf\nimport predict as util\n\nreader = csv.reader(open(""normalized_car_features.csv"", ""r""), delimiter="","")\nx = list(reader)\n\npredict = util.Predict(float(x[0][0]), float(x[0][1]), float(x[0][2]), float(x[0][3]), float(x[0][4]),\n                       float(x[0][5]))\n\nsaved_model_directory = ""saved_model""\n\nwith tf.Session() as session:\n    tf.saved_model.loader.load(session, [""dnn_from_scratch_tensorflow""], saved_model_directory)\n\n    # do a forward pass\n    print(""Predicted price: "" + str(predict.output(session.run(\'layer_3/Tanh:0\',\n                                                               {\'input/cars:0\': predict.input(168000, ""Diesel"", 5)}))))\n'"
download_lbc_cars_data.py,0,"b'# coding=utf-8\n\n""""""\nThis script download data from leboncoin.fr and save them into a CSV file named car_features.csv.\n""""""\nfrom bs4 import BeautifulSoup\nimport requests\ntry:\n    from urllib.parse import urlparse, parse_qs\nexcept ImportError:\n    from urlparse import urlparse, parse_qs\nimport csv\n\n# the BMW1 serie 1 page is used\nurl = ""https://www.leboncoin.fr/voitures/offres/rhone_alpes/occasions/?o=0&brd=Bmw&mdl=Serie%201""\nr = requests.get(url)\ndata = r.text\n\nsoup = BeautifulSoup(data, ""html.parser"")\ncarLinks = set()\npageLinks = set()\ndata_set = []\n\nparsed = urlparse(soup.select(\'a#last\')[0].get(\'href\'))\nnbPage = parse_qs(parsed.query)[\'o\'][0]\nprint(""There are "" + str(nbPage) + "" web pages to process"")\n\n# for each web page that contains a grid of car offers\nfor i in range(1, int(nbPage), 1):\n\n    print(""Processing web page: "" + str(i))\n\n    # each car offer link is saved into the carLinks\n    for link in soup.select(\'#listingAds > section > section > ul > li > a\'):\n        carLinks.add(link.get(\'href\').replace(""//"", ""http://""))\n\n    # the next url page is set\n    url = ""https://www.leboncoin.fr/voitures/offres/rhone_alpes/occasions/?o="" + str(i) + ""&brd=Bmw&mdl=Serie%201""\n    r = requests.get(url)\n    data = r.text\n    soup = BeautifulSoup(data, ""html.parser"")\n\n# for each car link\nfor carLink in carLinks:\n\n    print(""Processing car page: "" + carLink)\n\n    # we load the car page\n    r = requests.get(carLink)\n    data = r.text\n    soup = BeautifulSoup(data, ""html.parser"")\n    km = 0\n    fuel = """"\n    age = 0\n    price = 0\n\n    # for each attribute of the car\n    for info in soup.select(""div.line h2""):\n\n        # we keep the ones that we need\n        if info.select(\'.property\')[0].text == u\'Kilom\xc3\xa9trage\':\n            km = int(info.select(\'.value\')[0].text.replace("" "", """").replace(""KM"", """"))\n        if info.select(\'.property\')[0].text == u\'Carburant\':\n            fuel = info.select(\'.value\')[0].text\n        if info.select(\'.property\')[0].text == u\'Ann\xc3\xa9e-mod\xc3\xa8le\':\n            age = 2017 - int(info.select(\'.value\')[0].text)\n        if info.select(\'.property\')[0].text == u\'Prix\':\n            price = int(info.select(\'.value\')[0].text.replace("" "", """").replace(u""\xe2\x82\xac"", """"))\n\n    # each car is an array of four features added to the data_set\n    data_set.append([km, fuel, age, price])\n\n# the data_set is save into the CSV file\nfl = open(\'car_features.csv\', \'w\')\nwriter = csv.writer(fl)\nwriter.writerow([\'km\', \'fuel\', \'age\', \'price\'])\nfor values in data_set:\n    writer.writerow(values)\n\nfl.close()\n'"
normalize_lbc_cars_data.py,8,"b'""""""\nThis script normalize the car dataset and produce the normalized_car_features.csv.\n""""""\nimport csv\nimport numpy as np\n\nreader = csv.reader(open(""car_features.csv"", ""r""), delimiter="","")\nx = list(reader)\nfeatures = np.array(x).astype(""str"")\n\nfeature_cleaned = []\n\n# cleaning: we keep the car Diesel and Essence for which the price is higher than 1000 euros\n# also removing the headers column\nfor feature in features[1:, :]:\n    if (feature[1] == \'Diesel\' or feature[1] == \'Essence\') and int(feature[3]) > 1000:\n        feature_cleaned.append(feature)\n\nprint(""Original dataset size: "" + str(features.shape[0] - 1))\nfeatures = np.array(feature_cleaned).astype(""str"")\nprint(""Cleaned dataset size: "" + str(features.shape[0]))\n\n# standardize kilometers: (x - mean)/std\nkm = features[:, 0].astype(""int"")\nmean_km = np.mean(km)\nstd_km = np.std(km)\nkm = (km - mean_km)/std_km\nfeatures[:, 0] = km\n\n# binary convert fuel: Diesel = -1, Essence = 1\nfeatures[:, 1] = [-1 if x == \'Diesel\' else 1 for x in features[:,1]]\n\n# standardize age: (x - mean)/std\nage = features[:, 2].astype(""int"")\nmean_age = np.mean(age)\nstd_age = np.std(age)\nage = (age - mean_age)/std_age\nfeatures[:, 2] = age\n\n# standardize price: (x - min)/(max - min)\nprice = features[:, 3].astype(""float"")\nmin_price = np.min(price)\nmax_price = np.max(price)\nfeatures[:, 3] = (price - min_price)/(max_price - min_price)\n\n# summary\nprint(""Mean km: "" + str(mean_km))\nprint(""Std km: "" + str(std_km))\nprint(""Mean age: "" + str(mean_age))\nprint(""Std age: "" + str(std_age))\nprint(""Min price: "" + str(min_price))\nprint(""Max price: "" + str(max_price))\n\nfl = open(\'normalized_car_features.csv\', \'w\')\n\nwriter = csv.writer(fl)\n# the first line contains the normalization metadata\nwriter.writerow([mean_km, std_km, mean_age, std_age, min_price, max_price])\nwriter.writerow([\'km\', \'fuel\', \'age\', \'price\'])\nfor values in features:\n    writer.writerow(values)\n\nfl.close()'"
predict.py,1,"b'""""""\nThe goal of this file is to normalize raw car attributes for the prediction and transform the price using the inverse\nof the standardize process.\n""""""\nimport numpy as np\n\n\nclass Predict:\n    """"""\n    We save the data set metadata.\n    """"""\n    def __init__(self, mean_km, std_km, mean_age, std_age, min_price, max_price):\n        self.mean_km = mean_km\n        self.std_km = std_km\n        self.mean_age = mean_age\n        self.std_age = std_age\n        self.min_price = min_price\n        self.max_price = max_price\n\n    """"""\n    This method returns the car\'s data normalized using the data set metadata.\n    """"""\n    def input(self, km, fuel, age):\n        km = (km - self.mean_km) / self.std_km\n        fuel = -1 if fuel == \'Diesel\' else 1\n        age = (age - self.mean_age) / self.std_age\n        return np.matrix([[\n            km, fuel, age\n        ]])\n\n    """"""\n    This method returns the price in euro from the output of the network. The inverse of the standardize process for the\n    price is applied.\n    """"""\n    def output(self, price):\n        return price * (self.max_price - self.min_price) + self.min_price'"
