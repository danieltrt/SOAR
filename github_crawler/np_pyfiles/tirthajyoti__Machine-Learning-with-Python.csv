file_path,api_count,code
OOP_in_ML/Class_MyLinearRegression.py,36,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n\n\nclass Metrics:\n    """"""\n    Methods for computing useful regression metrics\n    \n    sse: Sum of squared errors\n    sst: Total sum of squared errors (actual vs avg(actual))\n    r_squared: Regression coefficient (R^2)\n    adj_r_squared: Adjusted R^2\n    mse: Mean sum of squared errors\n    AIC: Akaike information criterion\n    BIC: Bayesian information criterion\n    """"""\n\n    def sse(self):\n        """"""Returns sum of squared errors (model vs. actual)""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        squared_errors = (self.resid_) ** 2\n        self.sq_error_ = np.sum(squared_errors)\n        return self.sq_error_\n\n    def sst(self):\n        """"""Returns total sum of squared errors (actual vs avg(actual))""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        avg_y = np.mean(self.target_)\n        squared_errors = (self.target_ - avg_y) ** 2\n        self.sst_ = np.sum(squared_errors)\n        return self.sst_\n\n    def r_squared(self):\n        """"""Returns calculated value of r^2""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        self.r_sq_ = 1 - self.sse() / self.sst()\n        return self.r_sq_\n\n    def adj_r_squared(self):\n        """"""Returns calculated value of adjusted r^2""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        self.adj_r_sq_ = 1 - (self.sse() / self.dfe_) / (self.sst() / self.dft_)\n        return self.adj_r_sq_\n\n    def mse(self):\n        """"""Returns calculated value of mse""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        self.mse_ = np.mean((self.predict(self.features_) - self.target_) ** 2)\n        return self.mse_\n\n    def aic(self):\n        """"""\n        Returns AIC (Akaike information criterion)\n        """"""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        return lm.aic\n\n    def bic(self):\n        """"""\n        Returns BIC (Bayesian information criterion)\n        """"""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        return lm.bic\n\n    def print_metrics(self):\n        """"""Prints a report of the useful metrics for a given model object""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        items = (\n            (""sse:"", self.sse()),\n            (""sst:"", self.sst()),\n            (""mse:"", self.mse()),\n            (""r^2:"", self.r_squared()),\n            (""adj_r^2:"", self.adj_r_squared()),\n            (""AIC:"", self.aic()),\n            (""BIC:"", self.bic()),\n        )\n        for item in items:\n            print(""{0:8} {1:.4f}"".format(item[0], item[1]))\n\n    def summary_metrics(self):\n        """"""Returns a dictionary of the useful metrics""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        metrics = {}\n        items = (\n            (""sse"", self.sse()),\n            (""sst"", self.sst()),\n            (""mse"", self.mse()),\n            (""r^2"", self.r_squared()),\n            (""adj_r^2:"", self.adj_r_squared()),\n            (""AIC:"", self.aic()),\n            (""BIC:"", self.bic()),\n        )\n        for item in items:\n            metrics[item[0]] = item[1]\n        return metrics\n\n\nclass Inference:\n    """"""\n    Inferential statistics: \n        standard error, \n        p-values\n        t-test statistics\n        F-statistics and p-value of F-test\n    """"""\n\n    def __init__():\n        pass\n\n    def std_err(self):\n        """"""\n        Returns standard error values of the features\n        """"""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        return lm.bse\n\n    def pvalues(self):\n        """"""\n        Returns p-values of the features\n        """"""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        return lm.pvalues\n\n    def tvalues(self):\n        """"""\n        Returns t-test values of the features\n        """"""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        return lm.tvalues\n\n    def ftest(self):\n        """"""\n        Returns the F-statistic of the overall regression and corresponding p-value\n        """"""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        return (lm.fvalue, lm.f_pvalue)\n\n\nclass Diagnostics_plots:\n    """"""\n    Diagnostics plots and methods\n    \n    Arguments:\n    fitted_vs_residual: Plots fitted values vs. residuals\n    fitted_vs_features: Plots residuals vs all feature variables in a grid\n    histogram_resid: Plots a histogram of the residuals (can be normalized)\n    shapiro_test: Performs Shapiro-Wilk normality test on the residuals\n    qqplot_resid: Creates a quantile-quantile plot for residuals comparing with a normal distribution    \n    """"""\n\n    def __init__():\n        pass\n\n    def fitted_vs_residual(self):\n        """"""Plots fitted values vs. residuals""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        plt.title(""Fitted vs. residuals plot"", fontsize=14)\n        plt.scatter(self.fitted_, self.resid_, edgecolor=""k"")\n        plt.hlines(\n            y=0,\n            xmin=np.amin(self.fitted_),\n            xmax=np.amax(self.fitted_),\n            color=""k"",\n            linestyle=""dashed"",\n        )\n        plt.xlabel(""Fitted values"")\n        plt.ylabel(""Residuals"")\n        plt.show()\n\n    def fitted_vs_features(self):\n        """"""Plots residuals vs all feature variables in a grid""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        num_plots = self.features_.shape[1]\n        if num_plots % 3 == 0:\n            nrows = int(num_plots / 3)\n        else:\n            nrows = int(num_plots / 3) + 1\n        ncols = 3\n        fig, ax = plt.subplots(nrows, ncols, figsize=(15, nrows * 3.5))\n        axes = ax.ravel()\n        for i in range(num_plots, nrows * ncols):\n            axes[i].set_visible(False)\n        for i in range(num_plots):\n            axes[i].scatter(\n                self.features_.T[i],\n                self.resid_,\n                color=""orange"",\n                edgecolor=""k"",\n                alpha=0.8,\n            )\n            axes[i].grid(True)\n            axes[i].set_xlabel(""Feature X[{}]"".format(i))\n            axes[i].set_ylabel(""Residuals"")\n            axes[i].hlines(\n                y=0,\n                xmin=np.amin(self.features_.T[i]),\n                xmax=np.amax(self.features_.T[i]),\n                color=""k"",\n                linestyle=""dashed"",\n            )\n        plt.show()\n\n    def histogram_resid(self, normalized=True):\n        """"""Plots a histogram of the residuals (can be normalized)""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        if normalized:\n            norm_r = self.resid_ / np.linalg.norm(self.resid_)\n        else:\n            norm_r = self.resid_\n        num_bins = min(20, int(np.sqrt(self.features_.shape[0])))\n        plt.title(""Histogram of the normalized residuals"")\n        plt.hist(norm_r, bins=num_bins, edgecolor=""k"")\n        plt.xlabel(""Normalized residuals"")\n        plt.ylabel(""Count"")\n        plt.show()\n\n    def shapiro_test(self, normalized=True):\n        """"""Performs Shapiro-Wilk normality test on the residuals""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        from scipy.stats import shapiro\n\n        if normalized:\n            norm_r = self.resid_ / np.linalg.norm(self.resid_)\n        else:\n            norm_r = self.resid_\n        _, p = shapiro(norm_r)\n        if p > 0.01:\n            print(""The residuals seem to have come from a Gaussian process"")\n        else:\n            print(\n                ""The residuals does not seem to have come from a Gaussian process.\\nNormality assumptions of the linear regression may have been violated.""\n            )\n\n    def qqplot_resid(self, normalized=True):\n        """"""Creates a quantile-quantile plot for residuals comparing with a normal distribution""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        from scipy.stats import probplot\n\n        if normalized:\n            norm_r = self.resid_ / np.linalg.norm(self.resid_)\n        else:\n            norm_r = self.resid_\n        plt.title(""Q-Q plot of the normalized residuals"")\n        probplot(norm_r, dist=""norm"", plot=plt)\n        plt.xlabel(""Theoretical quantiles"")\n        plt.ylabel(""Residual quantiles"")\n        plt.show()\n\n\nclass Data_plots:\n    """"""\n    Methods for data related plots\n    \n    pairplot: Creates pairplot of all variables and the target\n    plot_fitted: Plots fitted values against the true output values from the data\n    """"""\n\n    def __init__():\n        pass\n\n    def pairplot(self):\n        """"""Creates pairplot of all variables and the target using the Seaborn library""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n\n        print(""This may take a little time. Have patience..."")\n        from seaborn import pairplot\n        from pandas import DataFrame\n\n        df = DataFrame(np.hstack((self.features_, self.target_.reshape(-1, 1))))\n        pairplot(df)\n        plt.show()\n\n    def plot_fitted(self, reference_line=False):\n        """"""\n        Plots fitted values against the true output values from the data\n        \n        Arguments:\n        reference_line: A Boolean switch to draw a 45-degree reference line on the plot\n        """"""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        plt.title(""True vs. fitted values"", fontsize=14)\n        plt.scatter(y, self.fitted_, s=100, alpha=0.75, color=""red"", edgecolor=""k"")\n        if reference_line:\n            plt.plot(y, y, c=""k"", linestyle=""dotted"")\n        plt.xlabel(""True values"")\n        plt.ylabel(""Fitted values"")\n        plt.grid(True)\n        plt.show()\n\n\nclass Outliers:\n    """"""\n    Methods for plotting outliers, leverage, influence points\n    \n    cook_distance: Computes and plots Cook\'s distance\n    influence_plot: Creates the influence plot\n    leverage_resid_plot: Plots leverage vs normalized residuals\' square\n    """"""\n\n    def __init__():\n        pass\n\n    def cook_distance(self):\n        """"""Computes and plots Cook\\\'s distance""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        import statsmodels.api as sm\n        from statsmodels.stats.outliers_influence import OLSInfluence as influence\n\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        inf = influence(lm)\n        (c, p) = inf.cooks_distance\n        plt.figure(figsize=(8, 5))\n        plt.title(""Cook\'s distance plot for the residuals"", fontsize=14)\n        plt.stem(np.arange(len(c)), c, markerfmt="","", use_line_collection=True)\n        plt.grid(True)\n        plt.show()\n\n    def influence_plot(self):\n        """"""Creates the influence plot""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        import statsmodels.api as sm\n\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        fig, ax = plt.subplots(figsize=(10, 8))\n        fig = sm.graphics.influence_plot(lm, ax=ax, criterion=""cooks"")\n        plt.show()\n\n    def leverage_resid_plot(self):\n        """"""Plots leverage vs normalized residuals\' square""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        import statsmodels.api as sm\n\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        fig, ax = plt.subplots(figsize=(10, 8))\n        fig = sm.graphics.plot_leverage_resid2(lm, ax=ax)\n        plt.show()\n\n\nclass Multicollinearity:\n    """"""\n    Methods for checking multicollinearity in the dataset features\n    \n    vif:Computes variance influence factors for each feature variable\n    """"""\n\n    def __init__():\n        pass\n\n    def vif(self):\n        """"""Computes variance influence factors for each feature variable""""""\n        if not self.is_fitted:\n            print(""Model not fitted yet!"")\n            return None\n        import statsmodels.api as sm\n        from statsmodels.stats.outliers_influence import (\n            variance_inflation_factor as vif,\n        )\n\n        lm = sm.OLS(self.target_, sm.add_constant(self.features_)).fit()\n        for i in range(self.features_.shape[1]):\n            v = vif(np.matrix(self.features_), i)\n            print(""Variance inflation factor for feature {}: {}"".format(i, round(v, 2)))\n\n\nclass MyLinearRegression(\n    Metrics, Diagnostics_plots, Data_plots, Outliers, Multicollinearity, Inference\n):\n    def __init__(self, fit_intercept=True):\n        self.coef_ = None\n        self.intercept_ = None\n        self.fit_intercept_ = fit_intercept\n        self.is_fitted = False\n        self.features_ = None\n        self.target_ = None\n\n    def __repr__(self):\n        return ""I am a Linear Regression model!""\n\n    def ingest_data(self, X, y):\n        """"""\n       Ingests the given data\n        \n        Arguments:\n        X: 1D or 2D numpy array \n        y: 1D numpy array\n        """"""\n        # check if X is 1D or 2D array\n        if len(X.shape) == 1:\n            X = X.reshape(-1, 1)\n\n        # features and data\n        self.features_ = X\n        self.target_ = y\n\n    def fit(self, X=None, y=None, fit_intercept_=True):\n        """"""\n        Fit model coefficients.\n        Arguments:\n        X: 1D or 2D numpy array \n        y: 1D numpy array\n        """"""\n\n        if X != None:\n            if len(X.shape) == 1:\n                X = X.reshape(-1, 1)\n            self.features_ = X\n        if y != None:\n            self.target_ = y\n\n        # degrees of freedom of population dependent variable variance\n        self.dft_ = self.features_.shape[0] - 1\n        # degrees of freedom of population error variance\n        self.dfe_ = self.features_.shape[0] - self.features_.shape[1] - 1\n\n        # add bias if fit_intercept is True\n        if self.fit_intercept_:\n            X_biased = np.c_[np.ones(self.features_.shape[0]), self.features_]\n        else:\n            X_biased = self.features_\n        # Assign target_ to a local variable y\n        y = self.target_\n\n        # closed form solution\n        xTx = np.dot(X_biased.T, X_biased)\n        inverse_xTx = np.linalg.inv(xTx)\n        xTy = np.dot(X_biased.T, y)\n        coef = np.dot(inverse_xTx, xTy)\n\n        # set attributes\n        if self.fit_intercept_:\n            self.intercept_ = coef[0]\n            self.coef_ = coef[1:]\n        else:\n            self.intercept_ = 0\n            self.coef_ = coef\n\n        # Predicted/fitted y\n        self.fitted_ = np.dot(self.features_, self.coef_) + self.intercept_\n\n        # Residuals\n        residuals = self.target_ - self.fitted_\n        self.resid_ = residuals\n\n        # Set is_fitted to True\n        self.is_fitted = True\n\n    def fit(self, X=None, y=None, fit_intercept_=True):\n        """"""\n        Fits model coefficients.\n        \n        Arguments:\n        X: 1D or 2D numpy array \n        y: 1D numpy array\n        fit_intercept: Boolean, whether an intercept term will be included in the fit\n        """"""\n\n        if X != None:\n            if len(X.shape) == 1:\n                X = X.reshape(-1, 1)\n            self.features_ = X\n        if y != None:\n            self.target_ = y\n\n        # degrees of freedom of population dependent variable variance\n        self.dft_ = self.features_.shape[0] - 1\n        # degrees of freedom of population error variance\n        self.dfe_ = self.features_.shape[0] - self.features_.shape[1] - 1\n\n        # add bias if fit_intercept is True\n        if self.fit_intercept_:\n            X_biased = np.c_[np.ones(self.features_.shape[0]), self.features_]\n        else:\n            X_biased = self.features_\n        # Assign target_ to a local variable y\n        y = self.target_\n\n        # closed form solution\n        xTx = np.dot(X_biased.T, X_biased)\n        inverse_xTx = np.linalg.inv(xTx)\n        xTy = np.dot(X_biased.T, y)\n        coef = np.dot(inverse_xTx, xTy)\n\n        # set attributes\n        if self.fit_intercept_:\n            self.intercept_ = coef[0]\n            self.coef_ = coef[1:]\n        else:\n            self.intercept_ = 0\n            self.coef_ = coef\n\n        # Predicted/fitted y\n        self.fitted_ = np.dot(self.features_, self.coef_) + self.intercept_\n\n        # Residuals\n        residuals = self.target_ - self.fitted_\n        self.resid_ = residuals\n\n        # Set is_fitted to True\n        self.is_fitted = True\n\n    def fit_dataframe(self, X, y, dataframe, fit_intercept_=True):\n        """"""\n        Fit model coefficients from a Pandas DataFrame.\n        \n        Arguments:\n        X: A list of columns of the dataframe acting as features. Must be only numerical.\n        y: Name of the column of the dataframe acting as the target\n        fit_intercept: Boolean, whether an intercept term will be included in the fit\n        """"""\n\n        assert (\n            type(X) == list\n        ), ""X must be a list of the names of the numerical feature/predictor columns""\n        assert (\n            type(y) == str\n        ), ""y must be a string - name of the column you want as target""\n\n        self.features_ = np.array(dataframe[X])\n        self.target_ = np.array(dataframe[y])\n\n        # degrees of freedom of population dependent variable variance\n        self.dft_ = self.features_.shape[0] - 1\n        # degrees of freedom of population error variance\n        self.dfe_ = self.features_.shape[0] - self.features_.shape[1] - 1\n\n        # add bias if fit_intercept is True\n        if self.fit_intercept_:\n            X_biased = np.c_[np.ones(self.features_.shape[0]), self.features_]\n        else:\n            X_biased = self.features_\n        # Assign target_ to a local variable y\n        y = self.target_\n\n        # closed form solution\n        xTx = np.dot(X_biased.T, X_biased)\n        inverse_xTx = np.linalg.inv(xTx)\n        xTy = np.dot(X_biased.T, y)\n        coef = np.dot(inverse_xTx, xTy)\n\n        # set attributes\n        if self.fit_intercept_:\n            self.intercept_ = coef[0]\n            self.coef_ = coef[1:]\n        else:\n            self.intercept_ = 0\n            self.coef_ = coef\n\n        # Predicted/fitted y\n        self.fitted_ = np.dot(self.features_, self.coef_) + self.intercept_\n\n        # Residuals\n        residuals = self.target_ - self.fitted_\n        self.resid_ = residuals\n\n        # Set is_fitted to True\n        self.is_fitted = True\n\n    def predict(self, X):\n        """"""Output model prediction.\n        Arguments:\n        X: 1D or 2D numpy array\n        """"""\n        # check if X is 1D or 2D array\n        if len(X.shape) == 1:\n            X = X.reshape(-1, 1)\n        self.predicted_ = self.intercept_ + np.dot(X, self.coef_)\n        return self.predicted_\n\n    def run_diagnostics(self):\n        """"""Runs diagnostics tests and plots""""""\n        Diagnostics_plots.fitted_vs_residual(self)\n        Diagnostics_plots.histogram_resid(self)\n        Diagnostics_plots.qqplot_resid(self)\n        print()\n        Diagnostics_plots.shapiro_test(self)\n\n    def outlier_plots(self):\n        """"""Creates various outlier plots""""""\n        Outliers.cook_distance(self)\n        Outliers.influence_plot(self)\n        Outliers.leverage_resid_plot(self)\n'"
Random Function Generator/Symbolic_regression_classification_generator.py,19,"b'\n# coding: utf-8\n\n# Evaluate a polynomial string\n\ndef symbolize(s):\n    """"""\n    Converts a a string (equation) to a SymPy symbol object\n    """"""\n    from sympy import sympify\n    s1=s.replace(\'.\',\'*\')\n    s2=s1.replace(\'^\',\'**\')\n    s3=sympify(s2)\n    \n    return(s3)\n\n\ndef eval_multinomial(s,vals=None,symbolic_eval=False):\n    """"""\n    Evaluates polynomial at vals.\n    vals can be simple list, dictionary, or tuple of values.\n    vals can also contain symbols instead of real values provided those symbols have been declared before using SymPy\n    """"""\n    from sympy import Symbol\n    sym_s=symbolize(s)\n    sym_set=sym_s.atoms(Symbol)\n    sym_lst=[]\n    for s in sym_set:\n        sym_lst.append(str(s))\n    sym_lst.sort()\n    if symbolic_eval==False and len(sym_set)!=len(vals):\n        print(""Length of the input values did not match number of variables and symbolic evaluation is not selected"")\n        return None\n    else:\n        if type(vals)==list:\n            sub=list(zip(sym_lst,vals))\n        elif type(vals)==dict:\n            l=list(vals.keys())\n            l.sort()\n            lst=[]\n            for i in l:\n                lst.append(vals[i])\n            sub=list(zip(sym_lst,lst))\n        elif type(vals)==tuple:\n            sub=list(zip(sym_lst,list(vals)))\n        result=sym_s.subs(sub)\n    \n    return result\n\n\n# ### Helper function for flipping binary values of a _ndarray_\n\ndef flip(y,p):\n    import numpy as np\n    lst=[]\n    for i in range(len(y)):\n        f=np.random.choice([1,0],p=[p,1-p])\n        lst.append(f)\n    lst=np.array(lst)\n    return np.array(np.logical_xor(y,lst),dtype=int)\n\n\n# ### Classification sample generation based on a symbolic expression\n\ndef gen_classification_symbolic(m=None,n_samples=100,n_features=2,flip_y=0.0):\n    """"""\n    Generates classification sample based on a symbolic expression.\n    Calculates the output of the symbolic expression at randomly generated (Gaussian distribution) points and\n    assigns binary classification based on sign.\n    m: The symbolic expression. Needs x1, x2, etc as variables and regular python arithmatic symbols to be used.\n    n_samples: Number of samples to be generated\n    n_features: Number of variables. This is automatically inferred from the symbolic expression. So this is ignored \n                in case a symbolic expression is supplied. However if no symbolic expression is supplied then a \n                default simple polynomial can be invoked to generate classification samples with n_features.\n    flip_y: Probability of flipping the classification labels randomly. A higher value introduces more noise and make\n            the classification problem harder.\n    Returns a numpy ndarray with dimension (n_samples,n_features+1). Last column is the response vector.\n    """"""\n    \n    import numpy as np\n    from sympy import Symbol,sympify\n    \n    if m==None:\n        m=\'\'\n        for i in range(1,n_features+1):\n            c=\'x\'+str(i)\n            c+=np.random.choice([\'+\',\'-\'],p=[0.5,0.5])\n            m+=c\n        m=m[:-1]\n    sym_m=sympify(m)\n    n_features=len(sym_m.atoms(Symbol))\n    evals=[]\n    lst_features=[]\n    for i in range(n_features):\n        lst_features.append(np.random.normal(scale=5,size=n_samples))\n    lst_features=np.array(lst_features)\n    lst_features=lst_features.T\n    for i in range(n_samples):\n        evals.append(eval_multinomial(m,vals=list(lst_features[i])))\n    \n    evals=np.array(evals)\n    evals_binary=evals>0\n    evals_binary=evals_binary.flatten()\n    evals_binary=np.array(evals_binary,dtype=int)\n    evals_binary=flip(evals_binary,p=flip_y)\n    evals_binary=evals_binary.reshape(n_samples,1)\n    \n    lst_features=lst_features.reshape(n_samples,n_features)\n    x=np.hstack((lst_features,evals_binary))\n    \n    return (x)\n\n# ### Regression sample generation based on a symbolic expression\n\n\ndef gen_regression_symbolic(m=None,n_samples=100,n_features=2,noise=0.0,noise_dist=\'normal\'):\n    """"""\n    Generates regression sample based on a symbolic expression. Calculates the output of the symbolic expression \n    at randomly generated (drawn from a Gaussian distribution) points\n    m: The symbolic expression. Needs x1, x2, etc as variables and regular python arithmatic symbols to be used.\n    n_samples: Number of samples to be generated\n    n_features: Number of variables. This is automatically inferred from the symbolic expression. So this is ignored \n                in case a symbolic expression is supplied. However if no symbolic expression is supplied then a \n                default simple polynomial can be invoked to generate regression samples with n_features.\n    noise: Magnitude of Gaussian noise to be introduced (added to the output).\n    noise_dist: Type of the probability distribution of the noise signal. \n    Currently supports: Normal, Uniform, t, Beta, Gamma, Poission, Laplace\n\n    Returns a numpy ndarray with dimension (n_samples,n_features+1). Last column is the response vector.\n    """"""\n    \n    import numpy as np\n    from sympy import Symbol,sympify\n    \n    if m==None:\n        m=\'\'\n        for i in range(1,n_features+1):\n            c=\'x\'+str(i)\n            c+=np.random.choice([\'+\',\'-\'],p=[0.5,0.5])\n            m+=c\n        m=m[:-1]\n    \n    sym_m=sympify(m)\n    n_features=len(sym_m.atoms(Symbol))\n    evals=[]\n    lst_features=[]\n    \n    for i in range(n_features):\n        lst_features.append(np.random.normal(scale=5,size=n_samples))\n    lst_features=np.array(lst_features)\n    lst_features=lst_features.T\n    lst_features=lst_features.reshape(n_samples,n_features)\n    \n    for i in range(n_samples):\n        evals.append(eval_multinomial(m,vals=list(lst_features[i])))\n    \n    evals=np.array(evals)\n    evals=evals.reshape(n_samples,1)\n    \n    if noise_dist==\'normal\':\n        noise_sample=noise*np.random.normal(loc=0,scale=1.0,size=n_samples)\n    elif noise_dist==\'uniform\':\n        noise_sample=noise*np.random.uniform(low=0,high=1.0,size=n_samples)\n    elif noise_dist==\'beta\':\n        noise_sample=noise*np.random.beta(a=0.5,b=1.0,size=n_samples)\n    elif noise_dist==\'Gamma\':\n        noise_sample=noise*np.random.gamma(shape=1.0,scale=1.0,size=n_samples)\n    elif noise_dist==\'laplace\':\n        noise_sample=noise*np.random.laplace(loc=0.0,scale=1.0,size=n_samples)\n        \n    noise_sample=noise_sample.reshape(n_samples,1)\n    evals=evals+noise_sample\n        \n    x=np.hstack((lst_features,evals))\n    \n    return (x)\n'"
Synthetic_data_generation/Symbolic_regression_classification_generator.py,19,"b'from sympy import sympify, Symbol\nimport numpy as np\n\ndef symbolize(s):\n    """"""\n    Converts a a string (equation) to a SymPy symbol object\n    """"""\n    s1=s.replace(\'.\',\'*\')\n    s2=s1.replace(\'^\',\'**\')\n    s3=sympify(s2)\n    \n    return(s3)\n\ndef eval_multinomial(s,vals=None,symbolic_eval=False):\n    """"""\n    Evaluates polynomial at vals.\n    vals can be simple list, dictionary, or tuple of values.\n    vals can also contain symbols instead of real values provided those symbols have been declared before using SymPy\n    """"""\n    sym_s=symbolize(s)\n    sym_set=sym_s.atoms(Symbol)\n    sym_lst=[]\n    for s in sym_set:\n        sym_lst.append(str(s))\n    sym_lst.sort()\n    if symbolic_eval==False and len(sym_set)!=len(vals):\n        print(""Length of the input values did not match number of variables and symbolic evaluation is not selected"")\n        return None\n    else:\n        if type(vals)==list:\n            sub=list(zip(sym_lst,vals))\n        elif type(vals)==dict:\n            l=list(vals.keys())\n            l.sort()\n            lst=[]\n            for i in l:\n                lst.append(vals[i])\n            sub=list(zip(sym_lst,lst))\n        elif type(vals)==tuple:\n            sub=list(zip(sym_lst,list(vals)))\n        result=sym_s.subs(sub)\n    \n    return result\n\ndef flip(y,p):\n    """"""\n    Flips random bit (used to make a classification problem haredr)\n    """"""\n    lst=[]\n    for i in range(len(y)):\n        f=np.random.choice([1,0],p=[p,1-p])\n        lst.append(f)\n    lst=np.array(lst)\n    return np.array(np.logical_xor(y,lst),dtype=int)\n\n\ndef gen_classification_symbolic(m=None,n_samples=100,n_features=2,flip_y=0.0):\n    """"""\n    Generates classification sample based on a symbolic expression.\n    Calculates the output of the symbolic expression at randomly generated (Gaussian distribution) points and\n    assigns binary classification based on sign.\n    m: The symbolic expression. Needs x1, x2, etc as variables and regular python arithmatic symbols to be used.\n    n_samples: Number of samples to be generated\n    n_features: Number of variables. This is automatically inferred from the symbolic expression. So this is ignored \n                in case a symbolic expression is supplied. However if no symbolic expression is supplied then a \n                default simple polynomial can be invoked to generate classification samples with n_features.\n    flip_y: Probability of flipping the classification labels randomly. A higher value introduces more noise and make\n            the classification problem harder.\n    Returns a numpy ndarray with dimension (n_samples,n_features+1). Last column is the response vector.\n    """"""\n    \n    if m==None:\n        m=\'\'\n        for i in range(1,n_features+1):\n            c=\'x\'+str(i)\n            c+=np.random.choice([\'+\',\'-\'],p=[0.5,0.5])\n            m+=c\n        m=m[:-1]\n    sym_m=sympify(m)\n    n_features=len(sym_m.atoms(Symbol))\n    evals=[]\n    lst_features=[]\n    for i in range(n_features):\n        lst_features.append(np.random.normal(scale=5,size=n_samples))\n    lst_features=np.array(lst_features)\n    lst_features=lst_features.T\n    for i in range(n_samples):\n        evals.append(eval_multinomial(m,vals=list(lst_features[i])))\n    \n    evals=np.array(evals)\n    evals_binary=evals>0\n    evals_binary=evals_binary.flatten()\n    evals_binary=np.array(evals_binary,dtype=int)\n    evals_binary=flip(evals_binary,p=flip_y)\n    evals_binary=evals_binary.reshape(n_samples,1)\n    \n    lst_features=lst_features.reshape(n_samples,n_features)\n    x=np.hstack((lst_features,evals_binary))\n    \n    return (x)\n\n\ndef gen_regression_symbolic(m=None,n_samples=100,n_features=2,noise=0.0,noise_dist=\'normal\'):\n    """"""\n    Generates regression sample based on a symbolic expression. Calculates the output of the symbolic expression \n    at randomly generated (drawn from a Gaussian distribution) points\n    m: The symbolic expression. Needs x1, x2, etc as variables and regular python arithmatic symbols to be used.\n    n_samples: Number of samples to be generated\n    n_features: Number of variables. This is automatically inferred from the symbolic expression. So this is ignored \n                in case a symbolic expression is supplied. However if no symbolic expression is supplied then a \n                default simple polynomial can be invoked to generate regression samples with n_features.\n    noise: Magnitude of Gaussian noise to be introduced (added to the output).\n    noise_dist: Type of the probability distribution of the noise signal. \n    Currently supports: Normal, Uniform, t, Beta, Gamma, Poission, Laplace\n\n    Returns a numpy ndarray with dimension (n_samples,n_features+1). Last column is the response vector.\n    """"""\n    \n    if m==None:\n        m=\'\'\n        for i in range(1,n_features+1):\n            c=\'x\'+str(i)\n            c+=np.random.choice([\'+\',\'-\'],p=[0.5,0.5])\n            m+=c\n        m=m[:-1]\n    \n    sym_m=sympify(m)\n    n_features=len(sym_m.atoms(Symbol))\n    evals=[]\n    lst_features=[]\n    \n    for i in range(n_features):\n        lst_features.append(np.random.normal(scale=5,size=n_samples))\n    lst_features=np.array(lst_features)\n    lst_features=lst_features.T\n    lst_features=lst_features.reshape(n_samples,n_features)\n    \n    for i in range(n_samples):\n        evals.append(eval_multinomial(m,vals=list(lst_features[i])))\n    \n    evals=np.array(evals)\n    evals=evals.reshape(n_samples,1)\n    \n    if noise_dist==\'normal\':\n        noise_sample=noise*np.random.normal(loc=0,scale=1.0,size=n_samples)\n    elif noise_dist==\'uniform\':\n        noise_sample=noise*np.random.uniform(low=0,high=1.0,size=n_samples)\n    elif noise_dist==\'beta\':\n        noise_sample=noise*np.random.beta(a=0.5,b=1.0,size=n_samples)\n    elif noise_dist==\'Gamma\':\n        noise_sample=noise*np.random.gamma(shape=1.0,scale=1.0,size=n_samples)\n    elif noise_dist==\'laplace\':\n        noise_sample=noise*np.random.laplace(loc=0.0,scale=1.0,size=n_samples)\n        \n    noise_sample=noise_sample.reshape(n_samples,1)\n    evals=evals+noise_sample\n        \n    x=np.hstack((lst_features,evals))\n    \n    return (x)'"
Utilities/ML-Python-utils.py,7,"b'import numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\n\r\ndef plot_decision_boundaries(X, y, model_class, **model_params):\r\n    """"""\r\n    Function to plot the decision boundaries of a classification model.\r\n    This uses just the first two columns of the data for fitting \r\n    the model as we need to find the predicted value for every point in \r\n    scatter plot.\r\n\r\n    Arguments:\r\n            X: Feature data as a NumPy-type array.\r\n            y: Label data as a NumPy-type array.\r\n            model_class: A Scikit-learn ML estimator class \r\n            e.g. GaussianNB (imported from sklearn.naive_bayes) or\r\n            LogisticRegression (imported from sklearn.linear_model)\r\n            **model_params: Model parameters to be passed on to the ML estimator\r\n    \r\n    Typical code example:\r\n            plt.figure()\r\n            plt.title(""KNN decision boundary with neighbros: 5"",fontsize=16)\r\n            plot_decision_boundaries(X_train,y_train,KNeighborsClassifier,n_neighbors=5)\r\n            plt.show()\r\n    """"""\r\n    try:\r\n        X = np.array(X)\r\n        y = np.array(y).flatten()\r\n    except:\r\n        print(""Coercing input data to NumPy arrays failed"")\r\n    # Reduces to the first two columns of data\r\n    reduced_data = X[:, :2]\r\n    # Instantiate the model object\r\n    model = model_class(**model_params)\r\n    # Fits the model with the reduced data\r\n    model.fit(reduced_data, y)\r\n\r\n    # Step size of the mesh. Decrease to increase the quality of the VQ.\r\n    h = .02     # point in the mesh [x_min, m_max]x[y_min, y_max].    \r\n\r\n    # Plot the decision boundary. For that, we will assign a color to each\r\n    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\r\n    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\r\n    # Meshgrid creation\r\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\r\n\r\n    # Obtain labels for each point in mesh using the model.\r\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])    \r\n\r\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\r\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\r\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\r\n                         np.arange(y_min, y_max, 0.1))\r\n\r\n    # Predictions to obtain the classification results\r\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\r\n\r\n    # Plotting\r\n    plt.contourf(xx, yy, Z, alpha=0.4)\r\n    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\r\n    plt.xlabel(""Feature-1"",fontsize=15)\r\n    plt.ylabel(""Feature-2"",fontsize=15)\r\n    plt.xticks(fontsize=14)\r\n    plt.yticks(fontsize=14)\r\n    return plt'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Machine-Learning-with-Python\'\ncopyright = \'2019, Tirthajyoti Sarkar\'\nauthor = \'Tirthajyoti Sarkar\'\n\n# The full version, including alpha/beta/rc tags\n# release = \'1.0.5\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\n\nextensions = [\'recommonmark\']\n\nmaster_doc = \'index\'\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_path = [""_themes"", ]\nhtml_theme_options = {\n  \'navigation_depth\': 4,\n  \'collapse_navigation\': False,\n  \'sticky_navigation\': False,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n'"
Deployment/Linear_regression/request_pred.py,0,"b'import json\nimport requests\nimport pandas as pd\n\n# Setting the headers to send and accept json responses\nheader = {\'Content-Type\': \'application/json\', \'Accept\': \'application/json\'}\n\n# Read test dataset\ntest_df = pd.read_csv(""data/housing_test.csv"")\n# For demo purpose, only 6 data points are passed on to the prediction server endpoint.\n# Feel free to change this index or use the whole test dataset\ntest_df = test_df.iloc[40:46]\n\n# Drop the first column\ntest_df.drop(test_df.columns[0],axis=1,inplace=True)\n\n# Converting Pandas Dataframe to json\ndata = test_df.to_json()\n\n#print(data)\nprint(""Sending data..."")\n\nresp = requests.post(""http://0.0.0.0:5000/predict"", \\\n                    data = json.dumps(data),\\\n                    headers= header)\n\nif (str(resp.status_code)==\'200\'):\n\tprint(""Response received correctly."")\n\tprint()\n\t\nx=resp.json()\nj = json.loads(x)\nd = dict(j)\n\nfor k,v in (d.items()):\n\tprint(""{}: {}"".format(k,round(v,2)))\n\tprint()\n'"
Deployment/Linear_regression/server_lm.py,0,"b'import os\nimport pandas as pd\nimport dill as pickle\nfrom flask import Flask, jsonify, request\n\napp = Flask(__name__)\n\n@app.route(\'/predict\', methods=[\'POST\'])\ndef apicall():\n\t""""""\n\tAPI Call\n\tPandas dataframe (sent as a payload) from API Call\n\t""""""\n\ttry:\n\t\ttest_json = request.get_json()\n\t\ttest = pd.read_json(test_json)\n\t\tprint(""The test data received are as follows..."")\n\t\tprint(test)\n\t\tprint()\n\n\texcept Exception as e:\n\t\traise e\n\n\tclf = \'lm_model_v1.pk\'\n\t\n\tif test.empty:\n\t\treturn(bad_request())\n\telse:\n\t\t#Load the saved model\n\t\tprint(""Loading the model..."")\n\t\tloaded_model = None\n\t\twith open(\'./models/\'+clf,\'rb\') as f:\n\t\t\tloaded_model = pickle.load(f)\n\n\t\tprint(""The model has been loaded...doing predictions now..."")\n\t\tprint()\n\t\tpredictions = loaded_model.predict(test)\n\t\t\t\n\t\tprediction_series = pd.Series(predictions)\n\t\tresponse = jsonify(prediction_series.to_json())\n\t\tresponse.status_code = 200\n\t\treturn (response)\n\n@app.errorhandler(400)\ndef bad_request(error=None):\n\tmessage = {\n\t\t\t\'status\': 400,\n\t\t\t\'message\': \'Bad Request: \' + request.url + \'--> Please check your data payload...\',\n\t}\n\tresp = jsonify(message)\n\tresp.status_code = 400\n\n\treturn resp\n'"
Deployment/Linear_regression/training_housing.py,0,"b'import numpy as np\nimport pandas as pd\n\nimport os \nimport json\nimport io\nimport requests\n\nimport dill as pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\ndata_filename = \'USA_housing.csv\'\ncwd = os.getcwd()\n\n# Checks if the dataset is in the local \'/data\' folder\n# If not present, pulls from Github repo, otherwise reads from the local folder\nif not os.path.isdir(cwd+""/data"") or data_filename not in os.listdir(cwd+""/data""):\n\turl=""https://raw.githubusercontent.com/tirthajyoti/Machine-Learning-with-Python/master/Datasets/USA_Housing.csv""\n\tprint(""Downloading data from {} "".format(url))\n\ts=requests.get(url).content\n\n\tdf = pd.read_csv(io.StringIO(s.decode(\'utf-8\')))\n\tprint(""Dataset is downloaded."")\n\t# Save the data in local \'/data\' folder\n\tif not os.path.isdir(cwd+""/data""):\n\t\tos.makedirs(cwd+""/data"")\n\tdf.to_csv(""data/USA_housing.csv"")\n\tprint()\nelse:\n\tdf = pd.read_csv(""data/USA_housing.csv"")\n\tprint(""Dataset loaded from local directory"")\n\tprint()\n\n# Make a list of data frame column names\nl_column = list(df.columns) # Making a list out of column names\nlen_feature = len(l_column) # Length of column vector list\n\n# Put all the numerical features in X and Price in y, \n# Ignore Address which is string for linear regression\nX = df[l_column[0:len_feature-2]]\ny = df[l_column[len_feature-2]]\n\n#print(""Feature set size:"",X.shape)\n#print(""Variable set size:"",y.shape)\n#print()\nprint(""Features variables: "",l_column[0:len_feature-2])\nprint()\n\n# Create X and y train and test splits in one command using a split ratio and a random seed\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\nprint(""Training feature set size:"",X_train.shape)\nprint(""Test feature set size:"",X_test.shape)\nprint(""Training variable set size:"",y_train.shape)\nprint(""Test variable set size:"",y_test.shape)\nprint()\n\n# Model fit and training\nlm = LinearRegression() # Creating a Linear Regression object \'lm\'\nlm.fit(X_train,y_train)\nprint(""Model training done..."")\nprint()\n\n# Print the intercept and coefficients of the linear model\nprint(""The intercept term of the linear model:"", round(lm.intercept_,3))\nprint(""The coefficients of the linear model:"", [round(c,3) for c in lm.coef_])\nprint()\n\n# R-square coefficient\ntrain_pred=lm.predict(X_train)\nprint(""R-squared value of this fit (on the training set):"",round(metrics.r2_score(y_train,train_pred),3))\n# Test score\n#test_score=lm.score(X_test,y_test)\n#print(""Test score: "",round(test_score,3))\nprint()\n\n# Main\n# Trains and saves the model in a serialized format\n# If either the data or models directory does not exist, creates them\n# Saves test data in a CSV file in a local \'/data\' folder\nif __name__ == \'__main__\':\n\tfilename = \'lm_model_v1.pk\'\n\tprint(""Now saving the model to a serialized format (pickle)..."")\n\tif not os.path.isdir(cwd+""/models""):\n\t\tos.makedirs(cwd+""/models"")\n\twith open(\'models/\'+filename, \'wb\') as file:\n\t\tpickle.dump(lm, file)\n\t# Save some of the test data in a CSV\n\tprint(""Saving test data to a file..."")\n\tprint()\n\tif os.path.isdir(cwd+""/data""):\n\t\tX_test.to_csv(""data/housing_test.csv"")\n\telse:\n\t\tos.makedirs(cwd+""/data"")\n\t\tX_test.to_csv(""data/housing_test.csv"")\n\t'"
Deployment/rnn_app/form.py,0,"b'from wtforms import (Form, TextField, validators, SubmitField, \nDecimalField, IntegerField)\n\nclass ReusableForm(Form):\n    """"""User entry form for entering specifics for generation""""""\n    # Starting seed\n    seed = TextField(""Enter a seed string or \'random\':"", validators=[\n                     validators.InputRequired()])\n    # Diversity of predictions\n    diversity = DecimalField(\'Enter diversity:\', default=0.8,\n                             validators=[validators.InputRequired(),\n                                         validators.NumberRange(min=0.5, max=5.0,\n                                         message=\'Diversity must be between 0.5 and 5.\')])\n    # Number of words\n    words = IntegerField(\'Enter number of words to generate:\',\n                         default=50, validators=[validators.InputRequired(),\n                                                 validators.NumberRange(min=10, max=100, \n                                                 message=\'Number of words must be between 10 and 100\')])\n    # Submit button\n    submit = SubmitField(""Enter"")'"
Deployment/rnn_app/keras_server.py,0,"b'from utils import generate_random_start, generate_from_seed\nfrom keras.models import load_model\nimport tensorflow as tf\nfrom flask import Flask, render_template, request\nfrom wtforms import Form, TextField, validators, SubmitField, DecimalField, IntegerField\n\n# Create app\napp = Flask(__name__)\n\n\nclass ReusableForm(Form):\n    """"""User entry form for entering specifics for generation""""""\n    # Starting seed\n    seed = TextField(""Enter a seed string or \'random\':"", validators=[\n                     validators.InputRequired()])\n    # Diversity of predictions\n    diversity = DecimalField(\'Enter diversity:\', default=0.8,\n                             validators=[validators.InputRequired(),\n                                         validators.NumberRange(min=0.5, max=5.0,\n                                                                message=\'Diversity must be between 0.5 and 5.\')])\n    # Number of words\n    words = IntegerField(\'Enter number of words to generate:\',\n                         default=50, validators=[validators.InputRequired(),\n                                                 validators.NumberRange(min=10, max=100, message=\'Number of words must be between 10 and 100\')])\n    # Submit button\n    submit = SubmitField(""Enter"")\n\n\ndef load_keras_model():\n    """"""Load in the pre-trained model""""""\n    global model\n    model = load_model(\'models/train-embeddings-rnn.h5\')\n    # Required for model to work\n    global graph\n    graph = tf.get_default_graph()\n\n\n# Home page\n@app.route(""/"", methods=[\'GET\', \'POST\'])\ndef home():\n    """"""Home page of app with form""""""\n    # Create form\n    form = ReusableForm(request.form)\n\n    # On form entry and all conditions met\n    if request.method == \'POST\' and form.validate():\n        # Extract information\n        seed = request.form[\'seed\']\n        diversity = float(request.form[\'diversity\'])\n        words = int(request.form[\'words\'])\n        # Generate a random sequence\n        if seed == \'random\':\n            return render_template(\'random.html\', input=generate_random_start(model=model, graph=graph, new_words=words, diversity=diversity))\n        # Generate starting from a seed sequence\n        else:\n            return render_template(\'seeded.html\', input=generate_from_seed(model=model, graph=graph, seed=seed, new_words=words, diversity=diversity))\n    # Send template information to index.html\n    return render_template(\'index.html\', form=form)\n\n\nif __name__ == ""__main__"":\n    print((""* Loading Keras model and Flask starting server...""\n           ""please wait until server has fully started""))\n    load_keras_model()\n    # Run app\n    app.run(host=""0.0.0.0"", port=5000)'"
Deployment/rnn_app/nothing_much.py,0,"b'from flask import Flask\napp = Flask(__name__)\n\n@app.route(""/"")\ndef hello():\n    return ""<h1>Not Much Going On Here</h1><br> \\\n\tBut at least <b>you could fire up a web page</b> and put some <i>HTML code</i> in \\\n\tusing <a href=\\""http://flask.pocoo.org/\\"">Flask</a>.""\napp.run(host=\'0.0.0.0\', port=50000)'"
Deployment/rnn_app/utils.py,10,"b'import numpy as np\nimport random\nimport json\nimport re\n\n\ndef generate_random_start(model, graph, seed_length=50,new_words=50,diversity=1,return_output=False,n_gen=1):\n    """"""Generate `new_words` words of output from a trained model and format into HTML.""""""\n\n    word_idx = json.load(open(\'data/word-index.json\'))\n    idx_word = {idx: word for word, idx in word_idx.items()}\n\n    sequences = json.load(open(\'data/sequences.json\'))\n\n    # Choose a random sequence\n    seq = random.choice(sequences)\n\n    # Choose a random starting point\n    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n    # Ending index for seed\n    end_idx = seed_idx + seed_length\n\n    gen_list = []\n    with graph.as_default():\n        for n in range(n_gen):\n            # Extract the seed sequence\n            seed = seq[seed_idx:end_idx]\n            original_sequence = [idx_word[i] for i in seed]\n            generated = seed[:] + [\'#\']\n\n            # Find the actual entire sequence\n            actual = generated[:] + seq[end_idx:end_idx + new_words]\n\n            # Keep adding new words\n            for i in range(new_words):\n\n                # Make a prediction from the seed\n                preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(np.float64)\n\n                # Diversify\n                preds = np.log(preds) / diversity\n                exp_preds = np.exp(preds)\n\n                # Softmax\n                preds = exp_preds / sum(exp_preds)\n\n                # Choose the next word\n                probas = np.random.multinomial(1, preds, 1)[0]\n\n                next_idx = np.argmax(probas)\n\n                # New seed adds on old word\n                #             seed = seed[1:] + [next_idx]\n                seed += [next_idx]\n                generated.append(next_idx)\n\n        # Showing generated and actual abstract\n        n = []\n\n        for i in generated:\n            n.append(idx_word.get(i, \'===\'))\n\n        gen_list.append(n)\n\n    a = []\n\n    for i in actual:\n        a.append(idx_word.get(i, \'===\'))\n\n    a = a[seed_length:]\n\n    gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n\n    if return_output:\n        return original_sequence, gen_list, a\n\t\n    # HTML formatting\n    seed_html = \'\'\n    seed_html = addContent(seed_html, header(\n        \'Seed Sequence\', color=\'darkblue\'))\n    seed_html = addContent(seed_html,box(remove_spaces(\' \'.join(original_sequence))))\n\n    gen_html = \'\'\n    gen_html = addContent(gen_html, header(\'RNN Generated\', color=\'darkred\'))\n    gen_html = addContent(gen_html, box(remove_spaces(\' \'.join(gen_list[0]))))\n\n    a_html = \'\'\n    a_html = addContent(a_html, header(\'Actual\', color=\'darkgreen\'))\n    a_html = addContent(a_html, box(remove_spaces(\' \'.join(a))))\n\t\n    st = ""<div>"" + seed_html + ""</div><div>"" + gen_html + ""</div><div>"" + a_html + ""</div>"" \n    #return f""<div>{seed_html}</div><div>{gen_html}</div><div>{a_html}</div>""\n    return st\n\ndef generate_from_seed(model, graph, seed,new_words=50, diversity=0.75):\n    """"""Generate output from a sequence""""""\n\n    # Mapping of words to integers\n    word_idx = json.load(open(\'data/word-index.json\')) \n    idx_word = {idx: word for word, idx in word_idx.items()}\n\n    # Original formated text\n    start = format_sequence(seed).split()\n    gen = []\n    s = start[:]\n\n    with graph.as_default():\n\n        # Generate output\n        for _ in range(new_words):\n            # Conver to array\n            x = np.array([word_idx.get(word, 0)\n                          for word in s]).reshape((1, -1))\n\n            # Make predictions\n            preds = model.predict(x)[0].astype(float)\n\n            # Diversify\n            preds = np.log(preds) / diversity\n            exp_preds = np.exp(preds)\n            # Softmax\n            preds = exp_preds / np.sum(exp_preds)\n\n            # Pick next index\n            next_idx = np.argmax(np.random.multinomial(1, preds, size=1))\n            s.append(idx_word[next_idx])\n            gen.append(idx_word[next_idx])\n\n    # Formatting in html\n    start = remove_spaces(\' \'.join(start)) + \' \'\n    gen = remove_spaces(\' \'.join(gen))\n    html = \'\'\n    html = addContent(html, header(\n        \'Input Seed \', color=\'black\', gen_text=\'Network Output\'))\n    html = addContent(html, box(start, gen))\n    st = ""<div>""+html+""</div>""\n    return st\n\n\ndef header(text, color=\'black\', gen_text=None):\n    """"""Create an HTML header""""""\n\n    if gen_text:\n        raw_html = \'<h1 style=""margin-top:16px;color: {color};font-size:54px""><center>\' + str(\n            text) + \'<span style=""color: red"">\' + str(gen_text) + \'</center></h1>\'\n    else:\n        raw_html = \'<h1 style=""margin-top:12px;color: {color};font-size:54px""><center>\' + str(\n            text) + \'</center></h1>\'\n    return raw_html\n\n\ndef box(text, gen_text=None):\n    """"""Create an HTML box of text""""""\n\n    if gen_text:\n        raw_html = \'<div style=""padding:8px;font-size:28px;margin-top:28px;margin-bottom:14px;"">\' + str(\n            text) + \'<span style=""color: red"">\' + str(gen_text) + \'</div>\'\n\n    else:\n        raw_html = \'<div style=""border-bottom:1px inset black;border-top:1px inset black;padding:8px;font-size: 28px;"">\' + str(\n            text) + \'</div>\'\n    return raw_html\n\n\ndef addContent(old_html, raw_html):\n    """"""Add html content together""""""\n\n    old_html += raw_html\n    return old_html\n\n\ndef format_sequence(s):\n    """"""Add spaces around punctuation and remove references to images/citations.""""""\n\n    # Add spaces around punctuation\n    s = re.sub(r\'(?<=[^\\s0-9])(?=[.,;?])\', r\' \', s)\n\n    # Remove references to figures\n    s = re.sub(r\'\\((\\d+)\\)\', r\'\', s)\n\n    # Remove double spaces\n    s = re.sub(r\'\\s\\s\', \' \', s)\n    return s\n\n\ndef remove_spaces(s):\n    """"""Remove spaces around punctuation""""""\n\n    s = re.sub(r\'\\s+([.,;?])\', r\'\\1\', s)\n\n    return s'"
