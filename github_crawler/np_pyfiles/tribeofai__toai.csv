file_path,api_count,code
setup.py,0,"b'import os\nfrom ast import literal_eval\n\nfrom setuptools import find_packages, setup\n\n\ndef get_version(version_tuple):\n    if not isinstance(version_tuple[-1], int):\n        return ""."".join(map(str, version_tuple[:-1])) + version_tuple[-1]\n    return ""."".join(map(str, version_tuple))\n\n\ninit = os.path.join(os.path.dirname(__file__), ""toai"", ""__init__.py"")\n\nversion_line = list(filter(lambda l: l.startswith(""VERSION""), open(init)))[0]\n\n# VERSION is a tuple so we need to eval \'version_line\'.\n# We could simply import it from the package but we\n# cannot be sure that this package is importable before\n# installation is done.\nPKG_VERSION = get_version(literal_eval(version_line.split("" = "")[-1]))\n\nwith open(""README.md"", ""r"") as f:\n    long_description = f.read()\n\nsetup(\n    name=""toai"",\n    version=PKG_VERSION,\n    description=""To AI helper library"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    author=""Dovydas Ceilutka"",\n    author_email=""dovydas.ceilutka@gmail.com"",\n    url=""https://github.com/tribeofai/toai"",\n    download_url=""https://github.com/tribeofai/toai"",\n    license=""MIT"",\n    install_requires=[\n        ""attrs"",\n        ""fastparquet"",\n        ""fastprogress"",\n        ""funcy"",\n        ""imagehash"",\n        ""joblib"",\n        ""kaggle"",\n        ""lightgbm"",\n        ""matplotlib"",\n        ""nb_black"",\n        ""numpy"",\n        ""pandas-profiling"",\n        ""pandas"",\n        ""Pillow"",\n        ""pyarrow"",\n        ""scikit-image"",\n        ""scikit-learn"",\n        ""scikit-optimize"",\n        ""seaborn"",\n    ],\n    extras_require={\n        ""tests"": [\n            ""pytest"",\n            ""pytest-pep8"",\n            ""pytest-xdist"",\n            ""pytest-cov"",\n            ""pytest-timeout"",\n        ]\n    },\n    classifiers=[\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Education"",\n        ""Intended Audience :: Science/Research"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n    ],\n    packages=find_packages(),\n)\n'"
toai/__init__.py,0,"b'VERSION = (0, 6, 3)\n__version__ = ""."".join([str(x) for x in VERSION])\n'"
toai/imports.py,0,"b'# pylama:ignore=W0611,W0401\n\nimport copy\nimport dataclasses\nimport gc\nimport json\nimport math\nimport operator\nimport os\nimport pickle  # nosec\nimport re\nimport shutil\nimport time\nimport warnings\nfrom collections import defaultdict, namedtuple\nfrom functools import partial, reduce\nfrom glob import glob\nfrom pathlib import Path\nfrom typing import *\n\nimport attr\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport requests\nimport seaborn as sns\nimport sklearn as sk\nimport skopt\nfrom fastprogress import master_bar, progress_bar\nfrom PIL import Image\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.ensemble import (\n    GradientBoostingClassifier,\n    GradientBoostingRegressor,\n    RandomForestClassifier,\n    RandomForestRegressor,\n)\nfrom sklearn.impute import MissingIndicator, SimpleImputer\nfrom sklearn.linear_model import ElasticNet, LinearRegression, LogisticRegression\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    confusion_matrix,\n    explained_variance_score,\n    f1_score,\n    log_loss,\n    mean_absolute_error,\n    mean_squared_error,\n    mean_squared_log_error,\n    precision_score,\n    r2_score,\n    recall_score,\n)\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (\n    Binarizer,\n    LabelEncoder,\n    MinMaxScaler,\n    MultiLabelBinarizer,\n    Normalizer,\n    OneHotEncoder,\n    OrdinalEncoder,\n    PolynomialFeatures,\n    PowerTransformer,\n    RobustScaler,\n    StandardScaler,\n)\nfrom sklearn.svm import SVC, SVR, LinearSVC, LinearSVR, OneClassSVM\n\ntry:\n    import kaggle\nexcept OSError as error:\n    warnings.warn(str(error))\n'"
toai/data/DataBundle.py,10,"b'import math\nfrom typing import Dict, Optional, Tuple, Sequence\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\n\n\nclass DataBundle:\n    def __init__(self, x: np.ndarray, y: np.ndarray):\n        self.x = x\n        self.y = y\n\n    @classmethod\n    def from_unbalanced(\n        cls,\n        data_bundle: ""DataBundle"",\n        target_class_size: int,\n        value_counts: Dict[int, int],\n    ) -> ""DataBundle"":\n        x = []\n        y = []\n        for label, current_size in value_counts.items():\n            current_indices = np.argwhere(data_bundle.y == label).flatten()\n            next_indices = []\n            for _ in range(target_class_size // current_size):\n                next_indices.append(np.random.permutation(current_indices))\n            next_indices.append(\n                np.random.choice(current_indices, target_class_size % current_size)\n            )\n            next_indices = np.concatenate(next_indices)\n            x.append(data_bundle.x[next_indices])\n            y.append(data_bundle.y[next_indices])\n\n        return cls(np.concatenate(x), np.concatenate(y))\n\n    @classmethod\n    def split(\n        cls,\n        data_bundle: ""DataBundle"",\n        fracs: Sequence[float],\n        random: Optional[bool] = True,\n    ) -> Tuple[""DataBundle"", ...]:\n        x = data_bundle.x\n        y = data_bundle.y\n\n        if random:\n            random_indices = np.random.permutation(len(data_bundle))\n            x = x[random_indices]\n            y = y[random_indices]\n\n        result = []\n        current_index = 0\n        for frac in fracs:\n            dx = math.ceil(len(data_bundle) * frac)\n            split_data_bundle = cls(\n                x=x[current_index : current_index + dx],\n                y=y[current_index : current_index + dx],\n            )\n            result.append(split_data_bundle)\n            current_index += dx\n\n        return tuple(result)\n\n    @classmethod\n    def from_dataframe(\n        cls, dataframe: pd.DataFrame, x_col: str, y_col: str\n    ) -> ""DataBundle"":\n        return cls(dataframe[x_col].values, dataframe[y_col].values)\n\n    def __len__(self) -> int:\n        return len(self.y)\n\n    def value_counts(self) -> Dict[str, int]:\n        values, counts = np.unique(self.y, return_counts=True)\n        return dict(zip(values.tolist(), counts.tolist()))\n\n    def make_label_map(self) -> Dict[str, int]:\n        return {value: key for key, value in enumerate(np.unique(self.y))}\n\n    def apply_label_map(self, label_map: Dict[str, int]) -> None:\n        self.y = np.vectorize(label_map.get)(self.y)\n\n    def make_label_scaler(self) -> sklearn.base.BaseEstimator:\n        scaler = sklearn.preprocessing.RobustScaler()\n        scaler.fit(self.y)\n        return scaler\n'"
toai/data/DataParams.py,0,b'from typing import List\nimport attr\n\n\n@attr.s(auto_attribs=True)\nclass DataParams:\n    target_col: str\n    cat_cols: List[str] = attr.Factory(list)\n    cont_cols: List[str] = attr.Factory(list)\n    text_cols: List[str] = attr.Factory(list)\n    img_cols: List[str] = attr.Factory(list)\n    feature_cols: List[str] = attr.ib(init=False)\n\n    def __attrs_post_init__(self):\n        self.feature_cols = (\n            self.cat_cols + self.cont_cols + self.text_cols + self.img_cols\n        )\n'
toai/data/__init__.py,0,b'# pylama:ignore=W0611\n\nfrom .DataParams import DataParams\nfrom .DataBundle import DataBundle\nfrom .utils import split_df\n'
toai/data/utils.py,0,"b'from typing import Tuple, Optional\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\ndef split_df(\n    data: pd.DataFrame,\n    test_size: float,\n    target_col: Optional[str] = None,\n    random_state: int = 42,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    stratify = data[target_col] if target_col else None\n    train_data, test_data = train_test_split(\n        data, test_size=test_size, stratify=stratify, random_state=random_state\n    )\n    test_stratify = test_data[target_col] if target_col else None\n    val_data, test_data = train_test_split(\n        test_data, test_size=0.5, stratify=test_stratify, random_state=random_state\n    )\n    for df in train_data, val_data, test_data:\n        df.reset_index(drop=True, inplace=True)\n    return train_data, val_data, test_data\n\n\ndef balance_df_labels(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n    max_value = df[col_name].value_counts().max()\n    result = [\n        df[df[col_name] == value]\n        for value, count in df[col_name].value_counts().items()\n        for _ in range(max_value // count)\n    ]\n    return pd.concat(result).reset_index(drop=True)\n'"
toai/encode/CategoricalEncoder.py,0,"b'from typing import List, Optional\n\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, columns: Optional[List[str]] = None):\n        self.columns = columns\n\n    def fit(self, data: pd.DataFrame) -> ""CategoricalEncoder"":\n        data = data.astype(""category"")\n        self.categories_ = {\n            column: dict(enumerate(data[column].cat.categories, 1))\n            for column in self.columns or data.columns\n        }\n        return self\n\n    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n        data = data.copy()\n        for column in self.categories_:\n            data[column] = (\n                data[column]\n                .map({value: key for key, value in self.categories_[column].items()})\n                .fillna(value=0)\n                .astype(int)\n            )\n        return data\n'"
toai/encode/__init__.py,0,b'# pylama:ignore=W0611\n\nfrom .CategoricalEncoder import CategoricalEncoder\n'
toai/extract/Extractor.py,0,"b'from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\nfrom typing import Dict\n\n\nclass Extractor(BaseEstimator, TransformerMixin):\n    def __init__(\n        self, source_column: str, patterns: Dict[str, str], drop_source: bool = False\n    ):\n        self.patterns = patterns\n        self.source_column = source_column\n        self.drop_source = drop_source\n\n    def fit(self, data: pd.DataFrame) -> ""Extractor"":\n        return self\n\n    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n        data = data.copy()\n        col_index = data.columns.get_loc(self.source_column)\n        for i, (column, pattern) in enumerate(self.patterns.items(), 1):\n            data.insert(\n                col_index + i, column, data[self.source_column].str.extract(pattern)\n            )\n        if self.drop_source:\n            data.drop(self.source_column, axis=1)\n        return data\n'"
toai/extract/__init__.py,0,b'# pylama:ignore=W0611\n\nfrom .Extractor import Extractor\n'
toai/inpute/CategoricalInputer.py,0,"b'from typing import List, Optional\n\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass CategoricalInputer(BaseEstimator, TransformerMixin):\n    def __init__(self, columns: Optional[List[str]] = None, suffix: str = ""_na""):\n        self.columns = columns\n        self.suffix = suffix\n\n    def fit(self, data: pd.DataFrame) -> ""CategoricalInputer"":\n        self.statistics_ = {\n            column: data[column].mode().values[0]\n            for column in self.columns or data.columns\n        }\n        return self\n\n    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n        data = data.copy()\n        for column in self.statistics_:\n            data.insert(\n                data.columns.get_loc(column) + 1,\n                column + self.suffix,\n                data[column].isna().astype(int),\n            )\n            data[column].fillna(value=self.statistics_[column], inplace=True)\n        return data\n'"
toai/inpute/ImageInputer.py,0,"b'import os\nfrom typing import List, Optional, Any\n\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass ImageInputer(BaseEstimator, TransformerMixin):\n    def __init__(self, value: Any, columns: Optional[List[str]] = None):\n        self.value = value\n        self.columns = columns\n\n    def fit(self, data: pd.DataFrame) -> ""ImageInputer"":\n        return self\n\n    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n        def fill_missing_with_value(value: Any):\n            def inner(image_path: str):\n                return image_path if os.path.isfile(image_path) else value\n\n            return inner\n\n        data = data.copy()\n        for column in self.columns or data.columns:\n            data[column] = (\n                data[column].apply(fill_missing_with_value(self.value)).values\n            )\n        return data\n'"
toai/inpute/NumericInputer.py,1,"b'from typing import List, Optional\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass NumericInputer(BaseEstimator, TransformerMixin):\n    IMPLEMENTED_STRATEGIES = {""median"": np.nanmedian, ""mean"": np.nanmean}\n\n    def __init__(\n        self,\n        columns: Optional[List[str]] = None,\n        strategy: str = ""median"",\n        suffix: str = ""_na"",\n    ):\n        if strategy not in self.IMPLEMENTED_STRATEGIES:\n            raise NotImplementedError(""This strategy is not implemented."")\n        self.strategy = strategy\n        self.columns = columns\n        self.suffix = suffix\n\n    def fit(self, data: pd.DataFrame) -> ""NumericInputer"":\n        self.statistics_ = {\n            column: self.IMPLEMENTED_STRATEGIES[self.strategy](data[column])\n            for column in self.columns or data.columns\n        }\n        return self\n\n    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n        data = data.copy()\n        for column in self.statistics_:\n            data.insert(\n                data.columns.get_loc(column) + 1,\n                column + self.suffix,\n                data[column].isna().astype(int),\n            )\n            data[column].fillna(value=self.statistics_[column], inplace=True)\n        return data\n'"
toai/inpute/__init__.py,0,b'# pylama:ignore=W0611\n\nfrom .CategoricalInputer import CategoricalInputer\nfrom .NumericInputer import NumericInputer\nfrom .ImageInputer import ImageInputer\n'
toai/metrics/__init__.py,0,b'# pylama:ignore=W0611\n\nfrom .error_rate import error_rate\nfrom .rmse import rmse\n'
toai/metrics/error_rate.py,3,"b'from typing import Optional\n\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n\ndef error_rate(\n    y_true: np.ndarray,\n    y_pred: np.ndarray,\n    normalize: bool = True,\n    sample_weight: Optional[np.ndarray] = None,\n):\n    return 1 - accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)\n'"
toai/metrics/rmse.py,1,"b'import numpy as np\nfrom sklearn.metrics import mean_squared_error\n\n\ndef rmse(y_true, y_pred, sample_weight=None, multioutput=""uniform_average""):\n    return np.sqrt(\n        mean_squared_error(\n            y_true=y_true,\n            y_pred=y_pred,\n            sample_weight=sample_weight,\n            multioutput=multioutput,\n        )\n    )\n'"
toai/tensorflow/DataContainer.py,0,"b'from typing import Dict, Optional\n\nimport tensorflow as tf\n\n\nclass DataContainer:\n    def __init__(\n        self,\n        base: tf.data.Dataset,\n        label_map: Dict[str, int],\n        train: Optional[tf.data.Dataset] = None,\n        train_steps: Optional[int] = None,\n        validation: Optional[tf.data.Dataset] = None,\n        validation_steps: Optional[int] = None,\n        test: Optional[tf.data.Dataset] = None,\n        test_steps: Optional[int] = None,\n        n_classes: Optional[int] = None,\n    ):\n        self.base = base\n        self.train = train\n        self.train_steps = train_steps\n        self.validation = validation\n        self.validation_steps = validation_steps\n        self.test = test\n        self.test_steps = test_steps\n        self.label_map = label_map\n        if n_classes:\n            self.n_classes = n_classes\n        elif label_map:\n            self.n_classes = len(label_map.keys())\n'"
toai/tensorflow/ImageAugmentor.py,0,"b'import attr\nfrom typing import Optional\nimport tensorflow as tf\n\n\n@attr.s(auto_attribs=True)\nclass ImageAugmentor:\n    level: int = 0\n    flips: Optional[str] = None\n    rotate: bool = True\n\n    def __call__(self, image: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n        if self.flips in [""horizontal"", ""both""]:\n            image = tf.image.random_flip_left_right(image)\n        if self.flips in [""vertical"", ""both""]:\n            image = tf.image.random_flip_up_down(image)\n\n        if self.level > 0:\n            lower = 1 - 0.1 * self.level\n            upper = 1 + 0.1 * self.level\n            min_jpeg_quality = max(0, int((lower - 0.5) * 100))\n            max_jpeg_quality = min(100, int((upper - 0.5) * 100))\n            image = tf.image.random_jpeg_quality(\n                image,\n                min_jpeg_quality=min_jpeg_quality,\n                max_jpeg_quality=max_jpeg_quality,\n            )\n            image = tf.image.random_contrast(image, lower=lower, upper=upper)\n            image = tf.image.random_saturation(image, lower=lower, upper=upper)\n            image = tf.clip_by_value(image, 0.0, 1.0)\n        return image, label\n'"
toai/tensorflow/ImageDataBundle.py,2,"b'import os\nimport re\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom ..data import DataBundle\n\n\nclass ImageDataBundle(DataBundle):\n    @classmethod\n    def from_subfolders(cls, path: Union[Path, str]) -> ""ImageDataBundle"":\n        path = Path(path)\n        paths = []\n        labels = []\n        for label in os.listdir(path):\n            for image_path in os.listdir(path / label):\n                paths.append(str(path / label / image_path))\n                labels.append(label)\n\n        return cls(np.asarray(paths), np.asarray(labels))\n\n    @classmethod\n    def from_re(\n        cls,\n        path: Union[Path, str],\n        regex: str,\n        default: Optional[Union[int, float, str, bool]] = None,\n    ) -> ""ImageDataBundle"":\n        paths = []\n        labels = []\n        for value in os.listdir(path):\n            match = re.match(regex, value)\n            if match:\n                labels.append(match.group(1))\n            elif default:\n                labels.append(str(default))\n            else:\n                raise ValueError(\n                    f""No match found and no default value provided for value: {value}""\n                )\n            paths.append(f""{path}/{value}"")\n\n        return cls(np.asarray(paths), np.asarray(labels))\n'"
toai/tensorflow/ImageDataContainer.py,0,"b'import matplotlib.pyplot as plt\n\nfrom .DataContainer import DataContainer\n\n\nclass ImageDataContainer(DataContainer):\n    def show(self, mode: str, rows: int = 3, cols: int = 6, debug: bool = False):\n        if debug and self.label_map:\n            reverse_label_map = {value: key for key, value in self.label_map.items()}\n        figsize = (4 * cols, 5 * rows) if debug else (3 * cols, 3 * rows)\n        _, ax = plt.subplots(rows, cols, figsize=figsize)\n\n        for i, (x, y) in enumerate(getattr(self, mode).unbatch().take(rows * cols)):\n            x = x.numpy()\n            y = y.numpy()\n            idx = (i // cols, i % cols) if rows > 1 else i % cols\n            ax[idx].axis(""off"")\n            ax[idx].imshow(x)\n            if debug and self.label_map:\n                title = (\n                    f""{reverse_label_map[y][:40]}\\nLabel code: {y}\\nShape: {x.shape}""\n                )\n            elif debug:\n                title = f""Label code: {y}\\nShape: {x.shape}""\n            else:\n                title = y\n            ax[idx].set_title(title)\n'"
toai/tensorflow/ImageLearner.py,3,"b'import os\nimport shutil\nfrom pathlib import Path\nfrom typing import Callable, Dict, Iterable, List, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom .load_model import load_keras_model\nfrom .save_model import save_keras_model\n\n\nclass ImageLearner:\n    def __init__(\n        self,\n        path: Union[Path, str],\n        base_model: keras.Model,\n        input_shape: Iterable[int],\n        output_shape: Iterable[int],\n        activation: Union[str, Callable],\n        loss: Union[str, Callable],\n        metrics: List[Callable],\n        dropout: float = 0.0,\n        l1: Optional[float] = None,\n        l2: Optional[float] = None,\n        override: bool = False,\n        load: bool = False,\n        class_weight: Optional[Dict[int, float]] = None,\n        sample_weight: Optional[np.ndarray] = None,\n    ):\n        self.path = str(path)\n        self.weights_path = f""{self.path}/weights.h5""\n        self.architecture_path = f""{self.path}/model.json""\n        self.logs_path = f""{self.path}/logs""\n\n        self.input_shape = input_shape\n        self.output_shape = output_shape\n        self.activation = activation\n        self.loss = loss\n        self.metrics = metrics\n        self.dropout = dropout\n        self.l1 = l1\n        self.l2 = l2\n        self.class_weight = class_weight\n        self.sample_weight = sample_weight\n\n        self.base_model = base_model(include_top=False, input_shape=input_shape)\n        self.concat_layer = keras.layers.concatenate(\n            [\n                keras.layers.GlobalAvgPool2D()(self.base_model.output),\n                keras.layers.GlobalMaxPool2D()(self.base_model.output),\n            ]\n        )\n        self.bn_layer = keras.layers.BatchNormalization()(self.concat_layer)\n        self.dropout_layer = keras.layers.Dropout(dropout)(self.bn_layer)\n\n        if self.l1 is not None and self.l2 is not None:\n            kernel_regularizer = keras.regularizers.l1_l2(self.l1, self.l2)\n        elif self.l1 is None and self.l2 is not None:\n            kernel_regularizer = keras.regularizers.l2(self.l2)\n        elif self.l1 is not None and self.l2 is None:\n            kernel_regularizer = keras.regularizers.l1(self.l1)\n        else:\n            kernel_regularizer = None\n\n        self.output_layers = [\n            keras.layers.Dense(\n                output_size,\n                kernel_regularizer=kernel_regularizer,\n                activation=activation,\n            )(self.dropout_layer)\n            for output_size in self.output_shape\n        ]\n\n        self.model = keras.Model(\n            inputs=self.base_model.inputs, outputs=self.output_layers\n        )\n\n        if os.path.exists(self.path):\n            if load:\n                self.load()\n            elif override:\n                shutil.rmtree(self.path)\n                os.makedirs(self.path)\n        else:\n            os.makedirs(self.path)\n\n        self.save()\n\n    def save(self) -> None:\n        save_keras_model(self.model, self.architecture_path, self.weights_path)\n\n    def load(self, weights_only: bool = False) -> None:\n        if weights_only:\n            self.model.load_weights(self.weights_path)\n        else:\n            self.model = load_keras_model(self.architecture_path, self.weights_path)\n\n    def compile(self, optimizer: keras.optimizers.Optimizer, lr: float) -> None:\n        self.model.compile(\n            optimizer=optimizer(lr), loss=self.loss, metrics=self.metrics\n        )\n\n    def freeze(self, n_layers: int = 1) -> None:\n        for layer in self.model.layers[:-n_layers]:\n            layer.trainable = False\n\n    def unfreeze(self) -> None:\n        for layer in self.model.layers:\n            layer.trainable = True\n\n    def fit(\n        self,\n        epochs: int,\n        train_dataset: tf.data.Dataset,\n        train_dataset_steps: int,\n        validation_dataset: tf.data.Dataset,\n        validation_dataset_steps: Optional[int] = None,\n        verbose: int = 1,\n    ) -> None:\n        reduce_lr_patience = max(2, epochs // 4)\n        early_stopping_patience = reduce_lr_patience * 2\n\n        self.history = self.model.fit(\n            x=train_dataset,\n            steps_per_epoch=train_dataset_steps,\n            validation_data=validation_dataset,\n            validation_steps=validation_dataset_steps,\n            epochs=epochs,\n            callbacks=[\n                keras.callbacks.ModelCheckpoint(\n                    self.weights_path, save_best_only=True, save_weights_only=True\n                ),\n                keras.callbacks.ReduceLROnPlateau(\n                    factor=0.3, patience=reduce_lr_patience\n                ),\n                keras.callbacks.EarlyStopping(\n                    patience=early_stopping_patience, restore_best_weights=True\n                ),\n            ],\n            verbose=verbose,\n            class_weight=self.class_weight,\n            sample_weight=self.sample_weight,\n        )\n        self.load(weights_only=True)\n\n    def predict(\n        self,\n        pipeline: List[Callable],\n        image_paths: Optional[Iterable[str]] = None,\n        images: Optional[np.ndarray] = None,\n    ) -> np.ndarray:\n        if images is None:\n            images = tf.data.Dataset.from_tensor_slices(image_paths)\n        for fun in pipeline:\n            images = images.map(fun, num_parallel_calls=1)\n        images = images.batch(1)\n        return self.model.predict(images)\n\n    def show_history(self, contains: str, skip: int = 0) -> None:\n        history_df = pd.DataFrame(self.history.history)\n        history_df[list(history_df.filter(regex=contains))].iloc[skip:].plot()\n'"
toai/tensorflow/ImageParser.py,0,"b'import attr\n\nimport tensorflow as tf\n\n\n@attr.s(auto_attribs=True)\nclass ImageParser:\n    n_channels: int = 3\n\n    def __call__(self, filename: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n        image = tf.image.decode_jpeg(\n            tf.io.read_file(filename), channels=self.n_channels\n        )\n        image = tf.image.convert_image_dtype(image, tf.float32)\n\n        return image, label\n'"
toai/tensorflow/ImageResizer.py,0,"b'from typing import Optional, Tuple\n\nimport attr\n\nimport tensorflow as tf\n\n\n@attr.s(auto_attribs=True)\nclass ImageResizer:\n    img_dims: Tuple[int, int, int]\n    resize: Optional[str] = None\n    crop_adjustment: float = 1\n\n    def __call__(self, image: tf.Tensor, label: tf.Tensor) -> tf.Tensor:\n        height, width, _ = self.img_dims\n        if self.resize == ""stretch"":\n            image = tf.image.resize(image, (height, width))\n        elif self.resize == ""crop"":\n            crop_height, crop_width = [\n                int(x * self.crop_adjustment) for x in (height, width)\n            ]\n            image = tf.image.resize(\n                images=image, size=(crop_height, crop_width), preserve_aspect_ratio=True\n            )\n            image = tf.image.resize_with_crop_or_pad(image, height, width)\n        elif self.resize == ""random_crop"":\n            crop_height, crop_width = [\n                int(x * self.crop_adjustment) for x in (height, width)\n            ]\n            image = tf.image.resize(image, (crop_height, crop_width))\n            image = tf.image.random_crop(image, self.img_dims)\n\n        return image, label\n'"
toai/tensorflow/ImageTrainer.py,3,"b'import time\nfrom typing import Iterable, Optional\n\nimport attr\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom sklearn.metrics import classification_report\n\nfrom .ImageTrainingCycle import ImageTrainingCycle\nfrom .ImageLearner import ImageLearner\nfrom .DataContainer import DataContainer\n\n\n@attr.s(auto_attribs=True)\nclass ImageTrainer:\n    learner: ImageLearner\n    data_container: DataContainer\n\n    def train(\n        self,\n        cycles: Iterable[ImageTrainingCycle],\n        template: str = (\n            ""Name: {} Train Time: {:.1f} min. ""\n            ""Eval Time: {:.2f}s Loss: {:.4f} Accuracy: {:.2%}""\n        ),\n    ) -> None:\n        start_time = time.time()\n        for cycle in cycles:\n            self.learner.freeze() if cycle.freeze else self.learner.unfreeze()\n            self.learner.compile(optimizer=cycle.optimizer, lr=cycle.lr)\n            self.learner.fit(\n                cycle.n_epochs,\n                cycle.data,\n                cycle.steps,\n                self.data_container.validation,\n                self.data_container.validation_steps,\n            )\n        end_time = time.time()\n\n        eval_start_time = time.time()\n        evaluation_results = self.evaluate(\n            self.data_container.validation,\n            self.data_container.validation_steps,\n            verbose=0,\n        )\n        eval_end_time = time.time()\n\n        print(""-"".center(80, ""-""))\n        print(\n            template.format(\n                self.learner.base_model.name,\n                (end_time - start_time) / 60,\n                (eval_end_time - eval_start_time),\n                *evaluation_results,\n            )\n        )\n        print(""-"".center(80, ""-""))\n\n    def evaluate(\n        self, dataset: tf.data.Dataset, steps: Optional[int] = None, verbose: int = 1\n    ) -> np.ndarray:\n        return self.learner.model.evaluate(dataset, steps=steps, verbose=verbose)\n\n    def predict(\n        self, dataset: tf.data.Dataset, steps: Optional[int] = None, verbose: int = 0\n    ):\n        return self.learner.model.predict(dataset, steps=steps, verbose=verbose)\n\n    def report(\n        self, dataset: tf.data.Dataset, steps: Optional[int] = None, verbose: int = 0\n    ):\n        return classification_report(\n            [label.numpy() for _, label in dataset.take(steps).unbatch()],\n            self.learner.model.predict(dataset, steps=steps).argmax(axis=1),\n        )\n\n    def analyse(\n        self, dataset: tf.data.Dataset, steps: Optional[int] = None, verbose: int = 0\n    ):\n        reverse_label_map = {\n            value: key for key, value in self.data_container.label_map.items()\n        }\n        images = []\n        label_codes = []\n        for image, label_code in dataset.take(steps).unbatch():\n            label_codes.append(label_code.numpy())\n            images.append(image.numpy())\n        labels = [reverse_label_map[label_code] for label_code in label_codes]\n        probs = self.learner.model.predict(dataset, steps=steps)\n        pred_codes = probs.argmax(axis=1)\n        preds = [reverse_label_map[pred_code] for pred_code in pred_codes]\n        return pd.DataFrame.from_dict(\n            {\n                ""image"": images,\n                ""label"": labels,\n                ""label_code"": label_codes,\n                ""pred"": preds,\n                ""pred_code"": pred_codes,\n                ""label_probs"": probs[:, label_codes][np.eye(len(labels), dtype=bool)],\n                ""pred_probs"": probs[:, pred_codes][np.eye(len(pred_codes), dtype=bool)],\n            }\n        )\n\n    def show_predictions(\n        self,\n        dataset: tf.data.Dataset,\n        steps: int,\n        correct: bool = False,\n        ascending: bool = True,\n        rows: int = 4,\n        cols: int = 4,\n    ):\n        df = self.analyse(dataset=dataset, steps=steps)\n        df = df[(df.label == df.pred) if correct else (df.label != df.pred)]\n        df.sort_values(by=[""label_probs""], ascending=ascending, inplace=True)\n        _, ax = plt.subplots(rows, cols, figsize=(4 * cols, 5 * rows))\n        for i, row in enumerate(df.head(cols * rows).itertuples()):\n            idx = (i // cols, i % cols) if rows > 1 else i % cols\n            ax[idx].axis(""off"")\n            ax[idx].imshow(row.image)\n            ax[idx].set_title(\n                f""{row.label}\\n{row.pred}\\n{row.label_probs:.4f}\\n{row.pred_probs:.4f}""\n            )\n'"
toai/tensorflow/ImageTrainingCycle.py,0,b'import attr\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\n\n@attr.s(auto_attribs=True)\nclass ImageTrainingCycle:\n    data: tf.data.Dataset\n    steps: int\n    n_epochs: int\n    lr: float\n    optimizer: keras.optimizers.Optimizer\n    freeze: bool = False\n'
toai/tensorflow/LearningRateFinder.py,1,"b'import tempfile\nfrom typing import Optional\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom typing import List\n\n\nclass LearningRateFinder:\n    def __init__(self, model: keras.Model, stop_factor: int = 4, beta: float = 0.98):\n        self.model = model\n        self.stop_factor = stop_factor\n        self.beta = beta\n        self.lrs: List[float] = []\n        self.losses: List[float] = []\n        self.lr_multiplier = 1.0\n        self.avg_loss = 0\n        self.best_loss = 1e9\n        self.batch_num = 0\n        self.weights_path: Optional[str] = None\n\n    def reset(self) -> None:\n        self.lrs = []\n        self.losses = []\n        self.lr_multiplier = 1.0\n        self.avg_loss = 0\n        self.best_loss = 1e9\n        self.batch_num = 0\n        self.weights_path = None\n\n    def on_batch_end(self, batch, logs):\n        lr = self.model.optimizer.lr\n        self.lrs.append(lr.numpy())\n        loss = logs[""loss""]\n        self.batch_num += 1\n        self.avg_loss = (self.beta * self.avg_loss) + ((1 - self.beta) * loss)\n        smooth = self.avg_loss / (1 - (self.beta ** self.batch_num))\n        self.losses.append(smooth)\n\n        stop_loss = self.stop_factor * self.best_loss\n\n        if self.batch_num > 1 and smooth > stop_loss:\n            self.model.stop_training = True\n            return\n\n        if self.batch_num == 1 or smooth < self.best_loss:\n            self.best_loss = smooth\n\n        lr = lr * self.lr_multiplier\n        self.model.optimizer.lr = lr\n\n    def find(\n        self,\n        x: tf.data.Dataset,\n        start_lr: float,\n        end_lr: float,\n        epochs: Optional[int] = None,\n        steps_per_epoch: int = 1,\n        batch_size: int = 32,\n        sample_size: int = 2048,\n        verbose: int = 0,\n    ):\n        self.reset()\n        if epochs is None:\n            epochs = int(np.ceil(sample_size / steps_per_epoch))\n\n        num_batch_updates = epochs * steps_per_epoch\n\n        self.lr_multiplier = (end_lr / start_lr) ** (1.0 / num_batch_updates)\n\n        self.weights_path = tempfile.mkstemp()[1]\n        self.model.save_weights(self.weights_path)\n\n        original_lr = self.model.optimizer.lr.numpy()\n        self.model.optimizer.lr = start_lr\n\n        self.model.fit(\n            x,\n            epochs=epochs,\n            steps_per_epoch=steps_per_epoch,\n            callbacks=[keras.callbacks.LambdaCallback(on_batch_end=self.on_batch_end)],\n            verbose=verbose,\n        )\n\n        self.model.load_weights(self.weights_path)\n        self.model.optimizer.lr = original_lr\n\n    def plot_loss(\n        self, skip_begin: int = 10, skip_end: int = 1, title: Optional[str] = None\n    ):\n        lrs = self.lrs[skip_begin:-skip_end]\n        losses = self.losses[skip_begin:-skip_end]\n\n        plt.plot(lrs, losses)\n        plt.xscale(""log"")\n        plt.xlabel(""Learning Rate (Log Scale)"")\n        plt.ylabel(""Loss"")\n\n        if title:\n            plt.title(title)\n'"
toai/tensorflow/__init__.py,0,b'# pylama:ignore=W0611\n\nfrom .DataContainer import DataContainer\nfrom .ImageAugmentor import ImageAugmentor\nfrom .ImageDataContainer import ImageDataContainer\nfrom .ImageDataBundle import ImageDataBundle\nfrom .ImageLearner import ImageLearner\nfrom .ImageParser import ImageParser\nfrom .ImageResizer import ImageResizer\nfrom .ImageTrainer import ImageTrainer\nfrom .ImageTrainingCycle import ImageTrainingCycle\nfrom .LearningRateFinder import LearningRateFinder\nfrom .load_model import load_keras_model\nfrom .save_model import save_keras_model\n'
toai/tensorflow/load_model.py,0,"b'from pathlib import Path\nfrom typing import Union, Optional\nfrom tensorflow import keras\n\n\ndef load_keras_model(\n    architecture_path: Union[Path, str],\n    weights_path: Union[Path, str],\n    custom_objects: Optional[dict] = None,\n):\n    with open(architecture_path, ""r"") as f:\n        model = keras.models.model_from_json(f.read(), custom_objects=custom_objects)\n    model.load_weights(weights_path)\n    return model\n'"
toai/tensorflow/save_model.py,0,"b'from pathlib import Path\nfrom typing import Union\n\nfrom tensorflow import keras\n\n\ndef save_keras_model(\n    model: keras.Model,\n    architecture_path: Union[Path, str],\n    weights_path: Union[Path, str],\n):\n    model.save_weights(weights_path)\n    with open(architecture_path, ""w"") as f:\n        f.write(model.to_json())\n'"
toai/utils/__init__.py,0,b'# pylama:ignore=W0611\n\nfrom .download_file import download_file\nfrom .load_file import load_file\nfrom .save_file import save_file\n'
toai/utils/download_file.py,0,"b'import os\nimport shutil\nfrom pathlib import Path\nfrom typing import Union\n\nimport requests\n\n\ndef download_file(\n    url: str, path: Union[Path, str] = Path("".""), override: bool = False\n) -> str:\n    local_filename = f""{str(path)}/{url.split(\'/\')[-1]}""\n    if os.path.exists(local_filename) and not override:\n        raise FileExistsError(f""File exists: {local_filename}"")\n    with requests.get(url, stream=True) as r:\n        with open(local_filename, ""wb"") as f:\n            shutil.copyfileobj(r.raw, f)\n\n    return local_filename\n'"
toai/utils/load_file.py,0,"b'import pickle  # nosec\nfrom typing import Any, Union\nfrom pathlib import Path\n\n\ndef load_file(filename: Union[Path, str], mode: str = ""rb"") -> Any:\n    with open(str(filename), mode=mode) as f:\n        return pickle.load(f)  # nosec\n'"
toai/utils/save_file.py,0,"b'import pickle  # nosec\nfrom typing import Any, Union\nfrom pathlib import Path\n\n\ndef save_file(obj: Any, filename: Union[Path, str], mode: str = ""wb"") -> None:\n    with open(str(filename), mode=mode) as f:\n        pickle.dump(obj, f)\n'"
toai/utils/unzip.py,0,"b'import os\nimport shutil\nfrom pathlib import Path\nfrom typing import Union, Optional\n\n\ndef all_files_in_dir(\n    path: Union[Path, str], extension: Optional[str] = None, keep_original: bool = False\n):\n    path = Path(path)\n    pathstring = str(path)\n    for filename in os.listdir(pathstring):\n        if extension and not filename.endswith(extension):\n            continue\n        shutil.unpack_archive(filename=str(path / filename), extract_dir=pathstring)\n        if not keep_original:\n            os.remove(str(path / filename))\n'"
