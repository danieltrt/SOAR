file_path,api_count,code
cnn-tensorflow/helper.py,7,"b'import pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\n\n\ndef _load_label_names():\n    """"""\n    Load the label names from file\n    """"""\n    return [\'airplane\', \'automobile\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\']\n\n\ndef load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n    """"""\n    Load a batch of the dataset\n    """"""\n    with open(cifar10_dataset_folder_path + \'/data_batch_\' + str(batch_id), mode=\'rb\') as file:\n        batch = pickle.load(file, encoding=\'latin1\')\n\n    features = batch[\'data\'].reshape((len(batch[\'data\']), 3, 32, 32)).transpose(0, 2, 3, 1)\n    labels = batch[\'labels\']\n\n    return features, labels\n\n\ndef display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n    """"""\n    Display Stats of the the dataset\n    """"""\n    batch_ids = list(range(1, 6))\n\n    if batch_id not in batch_ids:\n        print(\'Batch Id out of Range. Possible Batch Ids: {}\'.format(batch_ids))\n        return None\n\n    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n\n    if not (0 <= sample_id < len(features)):\n        print(\'{} samples in batch {}.  {} is out of range.\'.format(len(features), batch_id, sample_id))\n        return None\n\n    print(\'\\nStats of batch {}:\'.format(batch_id))\n    print(\'Samples: {}\'.format(len(features)))\n    print(\'Label Counts: {}\'.format(dict(zip(*np.unique(labels, return_counts=True)))))\n    print(\'First 20 Labels: {}\'.format(labels[:20]))\n\n    sample_image = features[sample_id]\n    sample_label = labels[sample_id]\n    label_names = _load_label_names()\n\n    print(\'\\nExample of Image {}:\'.format(sample_id))\n    print(\'Image - Min Value: {} Max Value: {}\'.format(sample_image.min(), sample_image.max()))\n    print(\'Image - Shape: {}\'.format(sample_image.shape))\n    print(\'Label - Label Id: {} Name: {}\'.format(sample_label, label_names[sample_label]))\n    plt.axis(\'off\')\n    plt.imshow(sample_image)\n\n\ndef _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n    """"""\n    Preprocess data and save it to file\n    """"""\n    features = normalize(features)\n    labels = one_hot_encode(labels)\n\n    pickle.dump((features, labels), open(filename, \'wb\'))\n\n\ndef preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n    """"""\n    Preprocess Training and Validation Data\n    """"""\n    n_batches = 5\n    valid_features = []\n    valid_labels = []\n\n    for batch_i in range(1, n_batches + 1):\n        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n        validation_count = int(len(features) * 0.1)\n\n        # Prprocess and save a batch of training data\n        _preprocess_and_save(\n            normalize,\n            one_hot_encode,\n            features[:-validation_count],\n            labels[:-validation_count],\n            \'preprocess_batch_\' + str(batch_i) + \'.p\')\n\n        # Use a portion of training batch for validation\n        valid_features.extend(features[-validation_count:])\n        valid_labels.extend(labels[-validation_count:])\n\n    # Preprocess and Save all validation data\n    _preprocess_and_save(\n        normalize,\n        one_hot_encode,\n        np.array(valid_features),\n        np.array(valid_labels),\n        \'preprocess_validation.p\')\n\n    with open(cifar10_dataset_folder_path + \'/test_batch\', mode=\'rb\') as file:\n        batch = pickle.load(file, encoding=\'latin1\')\n\n    # load the training data\n    test_features = batch[\'data\'].reshape((len(batch[\'data\']), 3, 32, 32)).transpose(0, 2, 3, 1)\n    test_labels = batch[\'labels\']\n\n    # Preprocess and Save all training data\n    _preprocess_and_save(\n        normalize,\n        one_hot_encode,\n        np.array(test_features),\n        np.array(test_labels),\n        \'preprocess_training.p\')\n\n\ndef batch_features_labels(features, labels, batch_size):\n    """"""\n    Split features and labels into batches\n    """"""\n    for start in range(0, len(features), batch_size):\n        end = min(start + batch_size, len(features))\n        yield features[start:end], labels[start:end]\n\n\ndef load_preprocess_training_batch(batch_id, batch_size):\n    """"""\n    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n    """"""\n    filename = \'preprocess_batch_\' + str(batch_id) + \'.p\'\n    features, labels = pickle.load(open(filename, mode=\'rb\'))\n\n    # Return the training data in batches of size <batch_size> or less\n    return batch_features_labels(features, labels, batch_size)\n\n\ndef display_image_predictions(features, labels, predictions):\n    n_classes = 10\n    label_names = _load_label_names()\n    label_binarizer = LabelBinarizer()\n    label_binarizer.fit(range(n_classes))\n    label_ids = label_binarizer.inverse_transform(np.array(labels))\n\n    fig, axies = plt.subplots(nrows=4, ncols=2)\n    fig.tight_layout()\n    fig.suptitle(\'Softmax Predictions\', fontsize=20, y=1.1)\n\n    n_predictions = 3\n    margin = 0.05\n    ind = np.arange(n_predictions)\n    width = (1. - 2. * margin) / n_predictions\n\n    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n        correct_name = label_names[label_id]\n\n        axies[image_i][0].imshow(feature*255)\n        axies[image_i][0].set_title(correct_name)\n        axies[image_i][0].set_axis_off()\n\n        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n        axies[image_i][1].set_yticks(ind + margin)\n        axies[image_i][1].set_yticklabels(pred_names[::-1])\n        axies[image_i][1].set_xticks([0, 0.5, 1.0])\n'"
cnn-tensorflow/problem_unittests.py,12,"b'import os\nimport numpy as np\nimport tensorflow as tf\nimport random\nfrom unittest.mock import MagicMock\n\n\ndef _print_success_message():\n    return print(\'Tests Passed\')\n\n\ndef test_folder_path(cifar10_dataset_folder_path):\n    assert cifar10_dataset_folder_path is not None,\\\n        \'Cifar-10 data folder not set.\'\n    assert cifar10_dataset_folder_path[-1] != \'/\',\\\n        \'The ""/"" shouldn\\\'t be added to the end of the path.\'\n    assert os.path.exists(cifar10_dataset_folder_path),\\\n        \'Path not found.\'\n    assert os.path.isdir(cifar10_dataset_folder_path),\\\n        \'{} is not a folder.\'.format(os.path.basename(cifar10_dataset_folder_path))\n\n    train_files = [cifar10_dataset_folder_path + \'/data_batch_\' + str(batch_id) for batch_id in range(1, 6)]\n    other_files = [cifar10_dataset_folder_path + \'/batches.meta\', cifar10_dataset_folder_path + \'/test_batch\']\n    missing_files = [path for path in train_files + other_files if not os.path.exists(path)]\n\n    assert not missing_files,\\\n        \'Missing files in directory: {}\'.format(missing_files)\n\n    print(\'All files found!\')\n\n\ndef test_normalize(normalize):\n    test_shape = (np.random.choice(range(1000)), 32, 32, 3)\n    test_numbers = np.random.choice(range(256), test_shape)\n    normalize_out = normalize(test_numbers)\n\n    assert type(normalize_out).__module__ == np.__name__,\\\n        \'Not Numpy Object\'\n\n    assert normalize_out.shape == test_shape,\\\n        \'Incorrect Shape. {} shape found\'.format(normalize_out.shape)\n\n    assert normalize_out.max() <= 1 and normalize_out.min() >= 0,\\\n        \'Incorect Range. {} to {} found\'.format(normalize_out.min(), normalize_out.max())\n\n    _print_success_message()\n\n\ndef test_one_hot_encode(one_hot_encode):\n    test_shape = np.random.choice(range(1000))\n    test_numbers = np.random.choice(range(10), test_shape)\n    one_hot_out = one_hot_encode(test_numbers)\n\n    assert type(one_hot_out).__module__ == np.__name__,\\\n        \'Not Numpy Object\'\n\n    assert one_hot_out.shape == (test_shape, 10),\\\n        \'Incorrect Shape. {} shape found\'.format(one_hot_out.shape)\n\n    n_encode_tests = 5\n    test_pairs = list(zip(test_numbers, one_hot_out))\n    test_indices = np.random.choice(len(test_numbers), n_encode_tests)\n    labels = [test_pairs[test_i][0] for test_i in test_indices]\n    enc_labels = np.array([test_pairs[test_i][1] for test_i in test_indices])\n    new_enc_labels = one_hot_encode(labels)\n\n    assert np.array_equal(enc_labels, new_enc_labels),\\\n        \'Encodings returned different results for the same numbers.\\n\' \\\n        \'For the first call it returned:\\n\' \\\n        \'{}\\n\' \\\n        \'For the second call it returned\\n\' \\\n        \'{}\\n\' \\\n        \'Make sure you save the map of labels to encodings outside of the function.\'.format(enc_labels, new_enc_labels)\n\n    _print_success_message()\n\n\ndef test_nn_image_inputs(neural_net_image_input):\n    image_shape = (32, 32, 3)\n    nn_inputs_out_x = neural_net_image_input(image_shape)\n\n    assert nn_inputs_out_x.get_shape().as_list() == [None, image_shape[0], image_shape[1], image_shape[2]],\\\n        \'Incorrect Image Shape.  Found {} shape\'.format(nn_inputs_out_x.get_shape().as_list())\n\n    assert nn_inputs_out_x.op.type == \'Placeholder\',\\\n        \'Incorrect Image Type.  Found {} type\'.format(nn_inputs_out_x.op.type)\n\n    assert nn_inputs_out_x.name == \'x:0\', \\\n        \'Incorrect Name.  Found {}\'.format(nn_inputs_out_x.name)\n\n    print(\'Image Input Tests Passed.\')\n\n\ndef test_nn_label_inputs(neural_net_label_input):\n    n_classes = 10\n    nn_inputs_out_y = neural_net_label_input(n_classes)\n\n    assert nn_inputs_out_y.get_shape().as_list() == [None, n_classes],\\\n        \'Incorrect Label Shape.  Found {} shape\'.format(nn_inputs_out_y.get_shape().as_list())\n\n    assert nn_inputs_out_y.op.type == \'Placeholder\',\\\n        \'Incorrect Label Type.  Found {} type\'.format(nn_inputs_out_y.op.type)\n\n    assert nn_inputs_out_y.name == \'y:0\', \\\n        \'Incorrect Name.  Found {}\'.format(nn_inputs_out_y.name)\n\n    print(\'Label Input Tests Passed.\')\n\n\ndef test_nn_keep_prob_inputs(neural_net_keep_prob_input):\n    nn_inputs_out_k = neural_net_keep_prob_input()\n\n    assert nn_inputs_out_k.get_shape().ndims is None,\\\n        \'Too many dimensions found for keep prob.  Found {} dimensions.  It should be a scalar (0-Dimension Tensor).\'.format(nn_inputs_out_k.get_shape().ndims)\n\n    assert nn_inputs_out_k.op.type == \'Placeholder\',\\\n        \'Incorrect keep prob Type.  Found {} type\'.format(nn_inputs_out_k.op.type)\n\n    assert nn_inputs_out_k.name == \'keep_prob:0\', \\\n        \'Incorrect Name.  Found {}\'.format(nn_inputs_out_k.name)\n\n    print(\'Keep Prob Tests Passed.\')\n\n\ndef test_con_pool(conv2d_maxpool):\n    test_x = tf.placeholder(tf.float32, [None, 32, 32, 5])\n    test_num_outputs = 10\n    test_con_k = (2, 2)\n    test_con_s = (4, 4)\n    test_pool_k = (2, 2)\n    test_pool_s = (2, 2)\n\n    conv2d_maxpool_out = conv2d_maxpool(test_x, test_num_outputs, test_con_k, test_con_s, test_pool_k, test_pool_s)\n\n    assert conv2d_maxpool_out.get_shape().as_list() == [None, 4, 4, 10],\\\n        \'Incorrect Shape.  Found {} shape\'.format(conv2d_maxpool_out.get_shape().as_list())\n\n    _print_success_message()\n\n\ndef test_flatten(flatten):\n    test_x = tf.placeholder(tf.float32, [None, 10, 30, 6])\n    flat_out = flatten(test_x)\n\n    assert flat_out.get_shape().as_list() == [None, 10*30*6],\\\n        \'Incorrect Shape.  Found {} shape\'.format(flat_out.get_shape().as_list())\n\n    _print_success_message()\n\n\ndef test_fully_conn(fully_conn):\n    test_x = tf.placeholder(tf.float32, [None, 128])\n    test_num_outputs = 40\n\n    fc_out = fully_conn(test_x, test_num_outputs)\n\n    assert fc_out.get_shape().as_list() == [None, 40],\\\n        \'Incorrect Shape.  Found {} shape\'.format(fc_out.get_shape().as_list())\n\n    _print_success_message()\n\n\ndef test_output(output):\n    test_x = tf.placeholder(tf.float32, [None, 128])\n    test_num_outputs = 40\n\n    output_out = output(test_x, test_num_outputs)\n\n    assert output_out.get_shape().as_list() == [None, 40],\\\n        \'Incorrect Shape.  Found {} shape\'.format(output_out.get_shape().as_list())\n\n    _print_success_message()\n\n\ndef test_conv_net(conv_net):\n    test_x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n    test_k = tf.placeholder(tf.float32)\n\n    logits_out = conv_net(test_x, test_k)\n\n    assert logits_out.get_shape().as_list() == [None, 10],\\\n        \'Incorrect Model Output.  Found {}\'.format(logits_out.get_shape().as_list())\n\n    print(\'Neural Network Built!\')\n\n\ndef test_train_nn(train_neural_network):\n    mock_session = tf.Session()\n    test_x = np.random.rand(128, 32, 32, 3)\n    test_y = np.random.rand(128, 10)\n    test_k = np.random.rand(1)\n    test_optimizer = tf.train.AdamOptimizer()\n\n    mock_session.run = MagicMock()\n    train_neural_network(mock_session, test_optimizer, test_k, test_x, test_y)\n\n    assert mock_session.run.called, \'Session not used\'\n\n    _print_success_message()\n'"
