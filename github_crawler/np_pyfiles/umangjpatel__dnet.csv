file_path,api_count,code
dnet/activations.py,0,"b'from typing import Callable, Tuple\n\nimport jax.numpy as tensor\nfrom jax import jit\nfrom jax.experimental.stax import Sigmoid, Relu, Tanh, LogSoftmax\nfrom jax.nn import functions\n\n\n@jit\ndef sigmoid() -> Tuple[Callable, Callable]:\n    return Sigmoid\n\n\n@jit\ndef tanh() -> Tuple[Callable, Callable]:\n    return Tanh\n\n\n@jit\ndef relu() -> Tuple[Callable, Callable]:\n    return Relu\n\n\n@jit\ndef softmax() -> Tuple[Callable, Callable]:\n    return LogSoftmax\n\n\n@jit\ndef mish() -> Tuple[Callable, Callable]:\n    init_fun: Callable = lambda rng, input_shape: (input_shape, ())\n    apply_fun: Callable = lambda params, inputs, **kwargs: inputs * tensor.tanh(functions.softplus(inputs))\n    return init_fun, apply_fun\n\n\n@jit\ndef linear() -> Tuple[Callable, Callable]:\n    init_fun: Callable = lambda rng, input_shape: (input_shape, ())\n    apply_fun: Callable = lambda params, inputs, **kwargs: inputs\n    return init_fun, apply_fun\n'"
dnet/archs.py,0,"b'from dnet.layers import Conv2D, MaxPool2D, FC, Flatten\nfrom dnet.models import Sequential\n\n\nclass Arch:\n    pass\n\n\nclass LeNet5(Arch):\n\n    def __init__(self) -> None:\n        self.model: Sequential = Sequential()\n        self.model.add(Conv2D(filters=6, kernel_size=(5, 5), activation=""relu""))\n        self.model.add(MaxPool2D(pool_size=(2, 2)))\n        self.model.add(Conv2D(filters=16, kernel_size=(5, 5), activation=""relu""))\n        self.model.add(MaxPool2D(pool_size=(2, 2)))\n        self.model.add(Flatten())\n        self.model.add(FC(units=120, activation=""relu""))\n        self.model.add(FC(units=84, activation=""relu""))\n\n    def __call__(self) -> Sequential:\n        return self.model\n\n\nclass VGG16(Arch):\n\n    def __init__(self) -> None:\n        self.model: Sequential = Sequential()\n        self.model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(MaxPool2D(pool_size=(2, 2)))\n        self.model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(MaxPool2D(pool_size=(2, 2)))\n        self.model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(MaxPool2D(pool_size=(2, 2)))\n        self.model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(MaxPool2D(pool_size=(2, 2)))\n        self.model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(Conv2D(filters=512, kernel_size=(3, 3), padding=""same"", activation=""relu""))\n        self.model.add(MaxPool2D(pool_size=(2, 2)))\n        self.model.add(Flatten())\n        self.model.add(FC(units=4096, activation=""relu""))\n        self.model.add(FC(units=4096, activation=""relu""))\n\n    def __call__(self) -> Sequential:\n        return self.model\n'"
dnet/dataloaders.py,0,"b'from typing import Generator\n\nimport jax.numpy as tensor\nfrom numpy.random import RandomState\n\n\nclass BatchLoader:\n\n    def __init__(self, inputs: tensor.array, targets: tensor.array, batch_size: int) -> None:\n        self.random_generator: RandomState = RandomState(0)\n        self.inputs: tensor.array = inputs\n        self.targets: tensor.array = targets\n        self.batch_size: int = batch_size\n        self.perform_batching()\n\n    def perform_batching(self) -> None:\n        num_complete_batches, leftover = divmod(self.inputs.shape[0], self.batch_size)\n        self.num_batches: int = num_complete_batches + bool(leftover)\n\n    def load_batch(self) -> Generator:\n        while True:\n            permutation: tensor.array = tensor.array(self.random_generator.permutation(self.inputs.shape[0]))\n            for i in range(self.num_batches):\n                index: int = permutation[i * self.batch_size: (i + 1) * self.batch_size]\n                yield self.inputs[index], self.targets[index]\n'"
dnet/datasets.py,1,"b'import os\nfrom pathlib import Path\nfrom typing import Dict, Tuple\n\nimport jax.numpy as tensor\nimport numpy as np\nimport pandas as pd\nimport tensorflow_datasets as tfds\n\ntfds.disable_progress_bar()\n\n\ndef mnist(flatten: bool = False, one_hot_encoding: bool = False,\n          data_dir: str = os.path.join("".."", ""datasets"", ""mnist"")):\n    path: Path = Path(data_dir)\n    downloaded_data, info = tfds.load(name=""mnist"", batch_size=-1, data_dir=path, with_info=True)\n    mnist_data: Dict[str, Dict[str, np.array]] = tfds.as_numpy(downloaded_data)\n    train_data, valid_data = mnist_data.get(""train""), mnist_data.get(""test"")\n    input_shape: Tuple[int, ...] = info.features[""image""].shape\n    train_images, train_labels = tensor.asarray(train_data.get(""image""), dtype=tensor.float32), tensor.asarray(\n        train_data.get(""label""), dtype=tensor.float32).reshape(-1, 1)\n    valid_images, valid_labels = tensor.asarray(valid_data.get(""image""), dtype=tensor.float32), tensor.asarray(\n        valid_data.get(""label""), dtype=tensor.float32).reshape(-1, 1)\n    if flatten:\n        train_images = train_images.reshape(-1, tensor.prod(list(input_shape)))\n        valid_images = valid_images.reshape(-1, tensor.prod(list(input_shape)))\n    if one_hot_encoding:\n        train_labels = tensor.asarray(pd.get_dummies(train_labels), dtype=tensor.float32)\n        valid_labels = tensor.asarray(pd.get_dummies(valid_labels), dtype=tensor.float32)\n    return (train_images, train_labels), (valid_images, valid_labels)\n\n\ndef tiny_mnist(flatten: bool = True, one_hot_encoding: bool = False,\n               data_dir: str = os.path.join("".."", ""datasets"", ""tiny_mnist"")):\n    path: Path = Path(data_dir)\n    train_data: tensor.array = tensor.asarray(pd.read_csv(path / ""train.csv"", header=None).values, dtype=tensor.float32)\n    valid_data: tensor.array = tensor.asarray(pd.read_csv(path / ""test.csv"", header=None).values, dtype=tensor.float32)\n    train_images: tensor.array = train_data[:, 1:] / 255.0\n    train_labels: tensor.array = train_data[:, 0].reshape(-1, 1)\n    valid_images: tensor.array = valid_data[:, 1:] / 255.0\n    valid_labels: tensor.array = valid_data[:, 0].reshape(-1, 1)\n    if not flatten:\n        train_images = train_images.reshape(-1, 28, 28, 1)\n        valid_images = valid_images.reshape(-1, 28, 28, 1)\n    if one_hot_encoding:\n        train_labels = tensor.asarray(pd.get_dummies(train_labels), dtype=tensor.float32)\n        valid_labels = tensor.asarray(pd.get_dummies(valid_labels), dtype=tensor.float32)\n    return (train_images, train_labels), (valid_images, valid_labels)\n'"
dnet/evaluators.py,0,"b'import jax.numpy as tensor\nfrom jax import jit\n\n\n@jit\ndef binary_crossentropy(outputs: tensor.array, targets: tensor.array) -> float:\n    output_labels: tensor.array = tensor.where(outputs > 0.50, 1.0, 0.0)\n    return tensor.mean(output_labels == targets)\n\n\n@jit\ndef categorical_crossentropy(outputs: tensor.array, targets: tensor.array) -> float:\n    target_class = tensor.argmax(targets, axis=1)\n    predicted_class = tensor.argmax(outputs, axis=1)\n    return tensor.mean(predicted_class == target_class)\n'"
dnet/layers.py,0,"b'from typing import Callable, List, Tuple\n\nfrom jax.experimental.stax import Dense, Flatten as Flat, Conv, MaxPool, BatchNorm as BatchNormalisation\n\nfrom dnet import activations\n\n\nclass Layer:\n    pass\n\n\nclass FC(Layer):\n\n    def __init__(self, units: int, activation: str = ""linear"") -> None:\n        layer_activation: Tuple[Callable, Callable] = getattr(activations, activation)()\n        self.layer: List = [Dense(out_dim=units), layer_activation]\n\n\nclass Flatten(Layer):\n\n    def __init__(self) -> None:\n        self.layer: List = [Flat]\n\n\nclass Conv2D(Layer):\n\n    def __init__(self, filters: int, kernel_size: Tuple[int, int], strides: Tuple[int, int] = (1, 1),\n                 padding: str = ""valid"",\n                 activation: str = ""linear"") -> None:\n        layer_activation: Tuple[Callable, Callable] = getattr(activations, activation)()\n        self.layer: List = [Conv(out_chan=filters, filter_shape=kernel_size, strides=strides, padding=padding.upper()),\n                            layer_activation]\n\n\nclass MaxPool2D(Layer):\n\n    def __init__(self, pool_size: Tuple[int, int], padding: str = ""valid"") -> None:\n        self.layer: List = [MaxPool(window_shape=pool_size, padding=padding.upper(), spec=""NHWC"")]\n\n\nclass BatchNorm(Layer):\n\n    def __init__(self):\n        self.layer: List = [BatchNormalisation()]\n'"
dnet/losses.py,0,"b'import jax.numpy as tensor\nfrom jax import jit\n\n\n@jit\ndef binary_crossentropy(outputs: tensor.array, targets: tensor.array) -> float:\n    return -tensor.mean(targets * tensor.log(outputs) + (1 - targets) * tensor.log(1 - outputs))\n\n\n@jit\ndef categorical_crossentropy(outputs: tensor.array, targets: tensor.array) -> float:\n    return -tensor.mean(tensor.sum(outputs * targets, axis=1))\n'"
dnet/models.py,0,"b'from typing import List, Callable, Tuple, Optional\n\nimport jax.numpy as tensor\nimport matplotlib.pyplot as plt\nfrom jax.experimental.stax import serial\n\nfrom dnet import evaluators\nfrom dnet import losses\nfrom dnet import optimizers\nfrom dnet.layers import Layer\nfrom dnet.trainer import Trainer\n\n\nclass Model:\n    pass\n\n\nclass Sequential(Model):\n\n    def __init__(self, network_layers: List[Optional[Layer]] = None):\n        self.layers: List = []\n        if network_layers is not None:\n            for network_layer in network_layers:\n                self.add(network_layer)\n\n    def add(self, network_layer: Layer) -> None:\n        self.layers.extend(network_layer.layer)\n\n    def add_layers(self, network_layers: List[Layer]) -> None:\n        for network_layer in network_layers:\n            self.layers.extend(network_layer.layer)\n\n    def compile(self, loss: str, optimizer: str, lr: float = 1e-02, bs: int = 32) -> None:\n        self.lr: float = lr\n        self.bs: int = bs\n        self.loss: Callable[[tensor.array, tensor.array], float] = getattr(losses, loss)\n        self.optimizer: Callable[[float], Tuple[Callable, ...]] = getattr(optimizers, optimizer)\n        self.evaluator: Callable[[tensor.array, tensor.array], float] = getattr(evaluators, loss)\n        self.serial_model: serial = serial(*self.layers)\n\n    def fit(self, inputs: tensor.array, targets: tensor.array, epochs: int,\n            validation_data: Tuple[tensor.array, tensor.array]) -> None:\n        self.epochs: int = epochs\n        self.inputs: tensor.array = inputs\n        self.targets: tensor.array = targets\n        self.val_inputs, self.val_targets = validation_data\n        self.trainer: Trainer = Trainer(self.__dict__)\n        self.trainer.train()\n\n    def plot_losses(self) -> None:\n        plt.plot(range(self.epochs), self.trainer.training_cost, color=""red"", marker=""o"", label=""Training loss"")\n        plt.plot(range(self.epochs), self.trainer.validation_cost, color=""green"", label=""Validation loss"")\n        plt.title(""Loss Curve"")\n        plt.xlabel(""Epochs"")\n        plt.ylabel(""Loss"")\n        plt.legend()\n        plt.show()\n\n    def plot_accuracy(self) -> None:\n        plt.plot(range(self.epochs), self.trainer.training_accuracy, color=""red"", marker=""o"", label=""Training accuracy"")\n        plt.plot(range(self.epochs), self.trainer.validation_accuracy, color=""green"", label=""Validation accuracy"")\n        plt.ylim([0.0, 1.05])\n        plt.title(""Accuracy Curve"")\n        plt.xlabel(""Epochs"")\n        plt.ylabel(""Accuracy"")\n        plt.legend()\n        plt.show()\n'"
dnet/optimizers.py,0,"b'import functools\nfrom typing import Tuple, Callable\n\nfrom jax import jit\nfrom jax.experimental import optimizers\n\n\n@functools.partial(jit, static_argnums=0)\ndef sgd(lr: float) -> Tuple[Callable, ...]:\n    return optimizers.sgd(step_size=lr)\n\n\n@functools.partial(jit, static_argnums=0)\ndef rmsprop(lr: float) -> Tuple[Callable, ...]:\n    beta: float = 0.9\n    return optimizers.rmsprop(step_size=lr, gamma=beta)\n\n\n@functools.partial(jit, static_argnums=0)\ndef momentum(lr: float) -> Tuple[Callable, ...]:\n    beta: float = 0.9\n    return optimizers.momentum(step_size=lr, mass=beta)\n\n\n@functools.partial(jit, static_argnums=0)\ndef adam(lr: float) -> Tuple[Callable, ...]:\n    beta1: float = 0.9\n    beta2: float = 0.999\n    return optimizers.adam(step_size=lr, b1=beta1, b2=beta2)\n\n\n@functools.partial(jit, static_argnums=0)\ndef adagrad(lr: float) -> Tuple[Callable, ...]:\n    beta: float = 0.9\n    return optimizers.adagrad(step_size=lr, momentum=beta)\n\n\n@functools.partial(jit, static_argnums=0)\ndef sm3(lr: float) -> Tuple[Callable, ...]:\n    beta: float = 0.9\n    return optimizers.sm3(step_size=lr, momentum=beta)\n'"
dnet/trainer.py,0,"b'import functools\nimport itertools\nfrom typing import Dict\nfrom typing import Generator, Callable, Iterator, Tuple, List\n\nimport jax.numpy as tensor\nfrom jax import grad\nfrom jax import jit\nfrom jax import random\nfrom jax.tree_util import register_pytree_node\nfrom tqdm import tqdm\n\nfrom dnet.dataloaders import BatchLoader\n\n\nclass Trainer:\n\n    @functools.partial(jit, static_argnums=(0, 1))\n    def __init__(self, model: Dict) -> None:\n        for k, v in model.items():\n            self.__dict__[k] = v\n        self.init_params_fun, self.predict_fun = self.serial_model\n        self.data_loader: BatchLoader = BatchLoader(self.inputs, self.targets, batch_size=self.bs)\n        self.data_batches: Generator = self.data_loader.load_batch()\n        self.opt_init, self.opt_update, self.get_params = self.optimizer(self.lr)\n        self.init_network()\n\n    @functools.partial(jit, static_argnums=0)\n    def init_network(self) -> None:\n        input_shape: Tuple[int, ...] = tuple([-1] + list(self.inputs.shape)[1:])\n        _, self.params = self.init_params_fun(random.PRNGKey(0), input_shape)\n        self.opt_state: Callable = self.opt_init(self.params)\n        self.count: Iterator[int] = itertools.count()\n        self.training_accuracy: List[float] = []\n        self.training_cost: List[float] = []\n        self.validation_accuracy: List[float] = []\n        self.validation_cost: List[float] = []\n\n    @jit\n    def compute_predictions(self, params: List[Tuple[tensor.array, tensor.array]],\n                            inputs: tensor.array) -> tensor.array:\n        return self.predict_fun(params, inputs)\n\n    @jit\n    def compute_cost(self, params: List[Tuple[tensor.array, tensor.array]],\n                     batch: Tuple[tensor.array, tensor.array]) -> float:\n        inputs, targets = batch\n        outputs: tensor.array = self.compute_predictions(params, inputs)\n        return self.loss(outputs, targets)\n\n    @jit\n    def compute_accuracy(self, params: List[Tuple[tensor.array, tensor.array]],\n                         batch: Tuple[tensor.array, tensor.array]) -> float:\n        inputs, targets = batch\n        outputs: tensor.array = self.compute_predictions(params, inputs)\n        return self.evaluator(outputs, targets)\n\n    @jit\n    def update(self, i: int, opt_state: Callable, batch: Tuple[tensor.array, tensor.array]) -> Callable:\n        params: List[Tuple[tensor.array, tensor.array]] = self.get_params(opt_state)\n        return self.opt_update(i, grad(self.compute_cost)(params, batch), opt_state)\n\n    @functools.partial(jit, static_argnums=0)\n    def train(self) -> None:\n        epoch_bar: tqdm = tqdm(range(self.epochs))\n        for epoch in epoch_bar:\n            for batch in range(self.data_loader.num_batches):\n                epoch_bar.set_description_str(desc=f""Epoch {epoch + 1}, Batch {batch + 1}"")\n                self.opt_state = self.update(next(self.count), self.opt_state, next(self.data_batches))\n            params: List[Tuple[tensor.array, tensor.array]] = self.get_params(self.opt_state)\n            self.update_metrics(epoch_bar, params)\n\n    def update_metrics(self, epoch_bar: tqdm, params: List[Tuple[tensor.array, tensor.array]]) -> None:\n        train_acc: float = self.compute_accuracy(params, (self.inputs, self.targets))\n        train_cost: float = self.compute_cost(params, (self.inputs, self.targets))\n        val_acc: float = self.compute_accuracy(params, (self.val_inputs, self.val_targets))\n        val_cost: float = self.compute_cost(params, (self.val_inputs, self.val_targets))\n        epoch_bar.set_postfix_str(s=f""Validation accuracy => {val_acc}"")\n        self.training_cost.append(train_cost)\n        self.training_accuracy.append(train_acc)\n        self.validation_cost.append(val_cost)\n        self.validation_accuracy.append(val_acc)\n\n\ndef trainer_tree_flatten(v):\n    aux_data = v.__dict__\n    nodes = v.params\n    return nodes, aux_data\n\n\ndef trainer_tree_unflatten(aux_data, nodes):\n    m = Trainer(aux_data)\n    m.params = nodes\n    return m\n\n\nregister_pytree_node(Trainer, trainer_tree_flatten, trainer_tree_unflatten)\n'"
examples/cnn_classification.py,0,"b'from dnet import datasets\nfrom dnet.layers import Conv2D, MaxPool2D, Flatten, FC\nfrom dnet.models import Sequential\n\n(x_train, y_train), (x_val, y_val) = datasets.tiny_mnist(flatten=False, one_hot_encoding=True)\n\nmodel = Sequential([\n    Conv2D(filters=6, kernel_size=(5, 5), activation=""relu""),\n    MaxPool2D(pool_size=(2, 2)),\n    Conv2D(filters=16, kernel_size=(5, 5), activation=""relu""),\n    MaxPool2D(pool_size=(2, 2)),\n    Flatten(),\n    FC(units=120, activation=""relu""),\n    FC(units=84, activation=""relu""),\n    FC(units=10, activation=""softmax"")\n])\nmodel.compile(loss=""categorical_crossentropy"", optimizer=""adam"", lr=1e-03, bs=512)\nmodel.fit(inputs=x_train, targets=y_train, epochs=10, validation_data=(x_val, y_val))\n\nmodel.plot_losses()\nmodel.plot_accuracy()\n'"
examples/ffnn_classification.py,0,"b'from dnet import datasets\nfrom dnet.layers import FC\nfrom dnet.models import Sequential\n\n(x_train, y_train), (x_val, y_val) = datasets.tiny_mnist(flatten=True, one_hot_encoding=True)\n\nmodel = Sequential()\nmodel.add(FC(units=500, activation=""relu""))\nmodel.add(FC(units=50, activation=""relu""))\nmodel.add(FC(units=10, activation=""softmax""))\nmodel.compile(loss=""categorical_crossentropy"", optimizer=""rmsprop"", lr=1e-03, bs=512)\nmodel.fit(inputs=x_train, targets=y_train, epochs=20, validation_data=(x_val, y_val))\n\nmodel.plot_losses()\nmodel.plot_accuracy()\n'"
examples/lenet5_example.py,0,"b'from dnet import datasets\nfrom dnet.archs import LeNet5\nfrom dnet.layers import FC\n\n(x_train, y_train), (x_val, y_val) = datasets.tiny_mnist(flatten=False, one_hot_encoding=True)\n\nmodel = LeNet5()()\nmodel.add(FC(units=10, activation=""softmax""))\nmodel.compile(loss=""categorical_crossentropy"", optimizer=""adam"", lr=1e-03, bs=512)\nmodel.fit(inputs=x_train, targets=y_train, epochs=10, validation_data=(x_val, y_val))\n\nmodel.plot_losses()\nmodel.plot_accuracy()\n'"
examples/vgg16_example.py,0,"b'from dnet import datasets\nfrom dnet.archs import VGG16\nfrom dnet.layers import FC\n\n(x_train, y_train), (x_val, y_val) = datasets.tiny_mnist(flatten=False, one_hot_encoding=True)\n\nmodel = VGG16()()\nmodel.add(FC(units=10, activation=""softmax""))\nmodel.compile(loss=""categorical_crossentropy"", optimizer=""adam"", lr=1e-03, bs=512)\nmodel.fit(inputs=x_train, targets=y_train, epochs=10, validation_data=(x_val, y_val))\n\nmodel.plot_losses()\nmodel.plot_accuracy()\n'"
