file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nfrom numpy.distutils.core import setup, Extension\n\ntry: \n    from distutils.command import bdist_conda\nexcept ImportError:\n    pass\n\n\nimport os\nimport io\nimport subprocess\nimport platform \n\n# in development set version to none and ...\nPYPI_VERSION = ""0.9.0b""\n\ndef git_version():\n    \n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in [\'SYSTEMROOT\', \'PATH\']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(cmd, stdout = subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'--short\', \'HEAD\'])\n        GIT_REVISION = out.strip().decode(\'ascii\')\n    except OSError:\n        GIT_REVISION = ""Unknown""\n\n    return GIT_REVISION\n\n\nif PYPI_VERSION is None:\n    PYPI_VERSION = git_version()\n\n\n\next = Extension(name    = \'quagmire._fortran\',\n                sources = [\'src/quagmire.pyf\',\'src/trimesh.f90\'])\n\n\nthis_directory = os.path.abspath(os.path.dirname(__file__))\nwith io.open(os.path.join(this_directory, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\n\nif __name__ == ""__main__"":\n    setup(name = \'quagmire\',\n          author            = ""Ben Mather"",\n          author_email      = ""ben.mather@sydney.edu.au"",\n          url               = ""https://github.com/underworldcode/quagmire"",\n          version           = PYPI_VERSION,\n          description       = ""Python surface process framework on highly scalable unstructured meshes"",\n          long_description  = long_description,\n          long_description_content_type=\'text/markdown\',\n          ext_modules       = [ext],\n          packages          = [\'quagmire\',\n                               \'quagmire.tools\',\n                               \'quagmire.equation_systems\',\n                               \'quagmire.function\',\n                               \'quagmire.scaling\',\n                               \'quagmire.mesh\',\n                               \'quagmire.topomesh\'],\n          install_requires  = [\'numpy>=1.16.0\', \'scipy>=1.0.0\', \'stripy>=1.2\', \'petsc4py\', \'mpi4py\', \'h5py\', \'pint\'],\n          classifiers       = [\'Programming Language :: Python :: 2\',\n                               \'Programming Language :: Python :: 2.7\',\n                               \'Programming Language :: Python :: 3\',\n                               \'Programming Language :: Python :: 3.5\',\n                               \'Programming Language :: Python :: 3.6\',\n                               \'Programming Language :: Python :: 3.7\',]\n          )\n'"
Sympy/sympy_physics.py,2,"b'import sympy\nimport numpy as np\nfrom sympy.vector import CoordSys3D, CoordSysCartesian\nfrom sympy.core.sympify import sympify\nfrom sympy.core.function import FunctionClass, AppliedUndef\nfrom sympy import Symbol\n\n\nR = CoordSys3D(\'R\')\nR2 = CoordSys3D(\'R2\')\n\nv = 3*R.i + 4*R.j + 5*R.k\n\n\nF = sympy.Function(""F"")(R.x, R.y, R.z)\n\nF.data = np.array((10,3))\n\n\nX, Y, t = sympy.symbols(\'xi eta t\')\n\n\n## this is based on the sympy UndefinedFunction class\n\n\nclass MeshVar(FunctionClass):\n   """"""\n   Mesh Variables\n   """"""\n\n   from sympy.core.function import UndefSageHelper\n\n   _undef_sage_helper = UndefSageHelper()\n\n   def __new__(mcl, name, bases=(AppliedUndef,), __dict__=None, **kwargs):\n       from sympy.core.symbol import _filter_assumptions\n\n\n       from sympy.core.function import UndefSageHelper\n\n       _undef_sage_helper = UndefSageHelper()\n\n       # Allow Function(\'f\', real=True)\n       # and/or Function(Symbol(\'f\', real=True))\n       assumptions, kwargs = _filter_assumptions(kwargs)\n       if isinstance(name, Symbol):\n           assumptions = name._merge(assumptions)\n           name = name.name\n       elif not isinstance(name, str):\n           raise TypeError(\'expecting string or Symbol for name\')\n       else:\n           commutative = assumptions.get(\'commutative\', None)\n           assumptions = Symbol(name, **assumptions).assumptions0\n           if commutative is None:\n               assumptions.pop(\'commutative\')\n       __dict__ = __dict__ or {}\n       # put the `is_*` for into __dict__\n       __dict__.update({\'is_%s\' % k: v for k, v in assumptions.items()})\n       # You can add other attributes, although they do have to be hashable\n       # (but seriously, if you want to add anything other than assumptions,\n       # just subclass Function)\n       __dict__.update(kwargs)\n       # add back the sanitized assumptions without the is_ prefix\n       kwargs.update(assumptions)\n       # Save these for __eq__\n       __dict__.update({\'_kwargs\': kwargs})\n       # do this for pickling\n       __dict__[\'__module__\'] = None\n       obj = super(MeshVar, mcl).__new__(mcl, name, bases, __dict__)\n       obj.name = name\n       obj._sage_ = _undef_sage_helper\n       return obj\n\n   def __instancecheck__(cls, instance):\n       return cls in type(instance).__mro__\n\n   _kwargs = {}  # type: tDict[str, Optional[bool]]\n\n   def __hash__(self):\n       return hash((self.class_key(), frozenset(self._kwargs.items())))\n\n   def __eq__(self, other):\n       return (isinstance(other, self.__class__) and\n           self.class_key() == other.class_key() and\n           self._kwargs == other._kwargs)\n\n   def __ne__(self, other):\n       return not self == other\n\n   @property\n   def _diff_wrt(self):\n       return False\n\n\n\ndef qderiv(A,B):\n\n    return np.float(1.0)\n\nA = MeshVar(""A"")(X,Y)\nB = sympy.diff(A,X)\n\n\n\ndAdx = sympy.lambdify((X,Y), B , [{""Derivative"" : qderiv}, \'math\'] )\n'"
Sympy/sympy_testing.py,1,"b'# -*- coding: utf-8 -*-\n\nimport sympy\nimport quagmire\nimport numpy as np\nfrom quagmire import function as fn\n\nsympy.init_printing()\n\nX, Y, t = sympy.symbols(\'xi eta t\')\n\nZ = sympy.simplify((X+Y)*(X-Y))\nZZ = sympy.sin(Z)\n\n# sympy.lambdify(args, expr)\n\n\nF = sympy.lambdify((X,Y), ZZ, modules=""numpy"" )\n\nprint(F(4.0,3.0))\n\n\n\nclass qsymbol(sympy.Symbol):\n\n    def evaluate(self):\n        pass\n\n\n\n# class qfunction(sympy.Function):\n\n#     __new__\n\n\ndef qderiv(A,B):\n\n    return np.float(1.0)\n\n\n\nH = sympy.Function(\'height\')(X,Y)\nH2 = sympy.sin(X)**2 + 3\n\n\ndhdx = sympy.lambdify((X,Y), H.diff(X), [{""Derivative"" : qderiv}, \'scipy\', \'numpy\'] )\ndh2dx = sympy.lambdify((X,Y), H2.diff(X), [{""Derivative"" : qderiv}, \'scipy\', \'numpy\'] )\n\n\n\nclass meshVariable(sympy.Function):\n    """"""mesh variable""""""\n\n    def __new__(cls, name, *args, **options):\n        # instance = sympy.Function(name)(X,Y)\n        instance = super(meshVariable, cls).__new__(cls, name, *args, **options)\n        return instance\n\n\n    def __init__(self, *args, **options):\n        """""" Who am i ?""""""\n\n        print(""INIT"")\n\n        # self.dims = dims\n\n\n\nA = meshVariable(""A"", (X,Y))\n\n\n\n\n\n# from sympy.core.cache import cacheit\n\n# class Dimension(sympy.Symbol):\n\n#     def __new_stage2__(cls, name, minVal, maxVal):\n#         obj = super(Dimension, cls).__xnew__(cls, name, real=True)\n#         obj.params = (minVal, maxVal)\n#         # obj.grid = numpy.linspace(start, stop, points, endpoint=False)\n#         return obj\n\n#     def __new__(cls, name, *args, **kwds):\n#         obj = Dimension.__xnew_cached_(cls, name, *args, **kwds)\n#         return obj\n\n#     __xnew__ = staticmethod(__new_stage2__)\n#     __xnew_cached_ = staticmethod(cacheit(__new_stage2__))\n\n#     def _hashable_content(self):\n#         return (Symbol._hashable_content(self), self.params)\n\n\n# D = Dimension(""delta"", 0, 1.0)\n\n# B = sympy.Function(""\\\\varepsilon"")(D)\n\n\n# def Variation(path_, st_, en_, v_):\n\n#     class Variation(sympy.Function):\n\n#         nargs = 2\n\n#         path = path_\n#         st = st_\n#         en = en_\n#         ends = [st, en]\n#         v = v_\n\n#         @classmethod\n#         def eval(cls, tt, ss):\n#             if tt in cls.ends:\n#                 return cls.path(tt)\n\n#             if ss == 0:\n#                 return cls.path(tt)\n\n#             return cls.v(tt, ss)\n\n#         def __init__(self):\n\n#             print(""INIT V"")\n\n#             return\n\n#     return Variation\n\n# s,t,a,b = sympy.symbols(\'s t a b\')\n# c = sympy.Function(\'c\')\n\n# Var = Variation(c, a, b, sympy.Function(r\'\\vartheta\'))\n\n\n\n# T = sympy.Function(""T"")(X,Y,t)\n\n# eta = 10.0 * sympy.exp(-2.0*T)\n\n\n\n\n\n'"
quagmire/__init__.py,0,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n#\n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\n<img src=""https://raw.githubusercontent.com/underworldcode/quagmire/dev/docs/images/AusFlow.png"" style=""display: block; margin: 0 auto"">\n\n**Quagmire is a Python surface process framework for building erosion and deposition models on highly parallel, decomposed structured and unstructured meshes.**\n\nQuagmire is structured into three major tiers that inherit methods and attributes from different classes:\n\n<img src=""https://raw.githubusercontent.com/underworldcode/quagmire/dev/docs/images/quagmire-flowchart.png"" style=""width: 321px; float:right"">\n\n1. `SurfaceProcessMesh`\n    - Calculate erosion-deposition rates\n    - Landscape analysis\n    - Long range flow models\n2. `TopoMesh`\n    - Assemble downhill connectivity matrix\n    - Calculate upstream area\n    - Compute slope\n    - Identify flat spots, low points, high points.\n3. `FlatMesh`\n    - calculating spatial derivatives\n    - identifying node neighbour relationships\n    - interpolation / extrapolation\n    - smoothing operators\n    - importing and saving mesh information\n\nThe `quagmire.surfmesh.surfmesh.SurfMesh` class (1) inherits from\nthe `quagmire.topomesh.topomesh.TopoMesh` class (2), which in turn inherits from\nthe `quagmire.mesh.pixmesh.PixMesh`, `quagmire.mesh.trimesh.TriMesh`, or\n`quagmire.mesh.strimesh.sTriMesh` class (3) depending on the type of mesh.\n\n\n## Installation\n\nNumpy and a fortran compiler, preferably [gfortran](https://gcc.gnu.org/wiki/GFortran), are required to install Quagmire.\n\n- ``python setup.py build``\n   - If you change the fortran compiler, you may have to add the\nflags `config_fc --fcompiler=<compiler name>` when setup.py is run\n(see docs for [numpy.distutils](http://docs.scipy.org/doc/numpy-dev/f2py/distutils.html)).\n- ``python setup.py install``\n\n## Dependencies\n\nRunning this code requires the following packages to be installed. The visualisation options are required for the notebooks and are included in the docker image.\n\n- Python 2.7.x and above\n- Numpy 1.9 and above\n- Scipy 0.15 and above\n- [mpi4py](http://pythonhosted.org/mpi4py/usrman/index.html)\n- [petsc4py](https://pythonhosted.org/petsc4py/usrman/install.html)\n- [stripy](https://github.com/University-of-Melbourne-Geodynamics/stripy)\n- [h5py](http://docs.h5py.org/en/latest/mpi.html#building-against-parallel-hdf5) (optional - for saving parallel data)\n- Matplotlib (optional - for visualisation)\n- lavavu (optional - for visualisation)\n\n### PETSc installation\n\nPETSc is used extensively via the Python frontend, petsc4py. It is required that PETSc be configured and installed on your local machine prior to using Quagmire. You can use pip or petsc to install petsc4py and its dependencies with consistent versions.\n\n```sh\n[sudo] pip install numpy mpi4py\n[sudo] pip install petsc petsc4py\n```\n\nIf that fails you must compile these manually.\n\n### HDF5 installation\n\nThis is an optional installation, but it is very useful for saving data that is distributed across multiple processes. If you are compiling HDF5 from [source](https://support.hdfgroup.org/downloads/index.html) it should be configured with the `--enable-parallel` flag:\n\n```sh\nCC=/usr/local/mpi/bin/mpicc ./configure --enable-parallel --enable-shared --prefix=<install-directory>\nmake    # build the library\nmake check  # verify the correctness\nmake install\n```\n\nYou can then point to this install directory when you install [h5py](http://docs.h5py.org/en/latest/mpi.html#building-against-parallel-hdf5).\n\n## Usage\n\nQuagmire is scalable in parallel. All of the python scripts in the *tests* subdirectory can be run in parallel, e.g.\n\n```sh\nmpirun -np 4 python stream_power.py\n```\n\nwhere the number after the `-np` flag specifies the number of processors.\n\n## Tutorials\n\nTutorials with worked examples can be found in the *Notebooks* subdirectory. These are Jupyter Notebooks that can be run locally.\nWe recommend installing [FFmpeg](https://ffmpeg.org/) to create videos in some of the notebooks.\n\nThe topics covered in the Notebooks include:\n\n**Meshing**\n\nTriangulations and regular 2d arrays are included and have the same API.\n\n- Square mesh\n- Elliptical mesh\n- Mesh refinement (e.g. Lloyd\'s mesh improvement)\n\n**Flow algorithms**\n\n- Single and multiple downhill pathways\n- Accumulating flow\n\n**Erosion and deposition**\n\nOperators for\n\n- Long-range stream flow models\n- Short-range diffusive evolution\n\n**Landscape evolution**\n\n_Work in progress_\n\n- Explicit timestepping and numerical stability\n- Landscape equilibrium metrics\n- Basement uplift\n\n**Benchmarking**\n\n_Work in progress_\n""""""\n\nfrom .mesh import PixMesh as _PixMesh\nfrom .mesh import TriMesh as _TriMesh\nfrom .mesh import sTriMesh as _sTriMesh\nfrom petsc4py import PETSc as _PETSc\nfrom .topomesh import TopoMesh as _TopoMeshClass\n\nfrom . import documentation\nfrom . import tools\nfrom . import function\nfrom . import scaling\nfrom . import equation_systems\n\ntry:\n    import lavavu\nexcept:\n    pass\n\n_display = None\n\nfrom mpi4py import MPI as _MPI\nmpi_rank = _MPI.COMM_WORLD.rank\nmpi_size = _MPI.COMM_WORLD.size\n\n\nclass _xvfb_runner(object):\n    """"""\n    This class will initialise the X virtual framebuffer (Xvfb).\n    Xvfb is useful on headless systems. Note that xvfb will need to be\n    installed, as will pyvirtualdisplay.\n    This class also manages the lifetime of the virtual display driver. When\n    the object is garbage collected, the driver is stopped.\n    """"""\n    def __init__(self):\n        from pyvirtualdisplay import Display\n        self._xvfb = Display(visible=0, size=(1600, 1200))\n        self._xvfb.start()\n\n    def __del__(self):\n        if not self._xvfb is None :\n            self._xvfb.stop()\n\nimport os as _os\n\n# disable collection of data if requested\nif ""GLUCIFER_USE_XVFB"" in _os.environ:\n    from mpi4py import MPI as _MPI\n    _comm = _MPI.COMM_WORLD\n    if _comm.rank == 0:\n        _display = _xvfb_runner()\n\n\nknown_basemesh_classes = {""PixMesh""  : _PixMesh, \\\n                          ""TriMesh""  : _TriMesh, \\\n                          ""sTriMesh"" : _sTriMesh}\n\n\ndef _get_label(DM):\n    """"""\n    Retrieves all points in the DM that is marked with a specific label.\n    e.g. ""boundary"", ""coarse""\n    """"""\n\n    n = DM.getNumLabels()\n    success = False\n\n    for i in range(n):\n        label = DM.getLabelName(i)\n        if label in known_basemesh_classes:\n            success = True\n            break\n\n    if not success:\n        raise NameError(""Cannot identify mesh type. DM is not valid."")\n\n    return label\n\n\n\ndef QuagMesh(DM, downhill_neighbours=2, verbose=True, *args, **kwargs):\n    """"""\n    Instantiates a mesh in the form required for quagmire computation.\n    QuagMesh identifies the type of DM and builds the necessary\n    data structures for landscape processing and analysis.\n\n\n    Parameters\n    ----------\n    DM : PETSc DM object\n        Either a DMDA or DMPlex object created using the meshing\n        functions within `tools.meshtools`.\n    downhill_neighbours : int (default: 2)\n        Number of downhill neighbours to initialise with topography.\n    verbose : bool (default: True)\n        Print detailed information on Quagmire routines and timings.\n\n    Returns\n    -------\n    QuagMesh : object\n        Inherits methods and attributes from:\n\n        - `mesh.commonmesh.CommonMesh`\n        - `mesh.pixmesh.PixMesh` (if `DM` is a regularly-spaced Cartesian grid)\n        - `mesh.trimesh.TriMesh` (if `DM` is an unstructred Cartesian mesh)\n        - `mesh.strimesh.sTriMesh` (if `DM` is an unstructured spherical mesh)\n        - `topomesh.topomesh.TopoMesh`\n    """"""\n\n    BaseMeshType = _get_label(DM)\n\n    if BaseMeshType in known_basemesh_classes:\n        class QuagMeshClass(known_basemesh_classes[BaseMeshType], _TopoMeshClass):\n            def __init__(self, dm, *args, **kwargs):\n                if verbose:\n                    print(""Underlying Mesh type: {}"".format(BaseMeshType))\n                known_basemesh_classes[BaseMeshType].__init__(self, dm, verbose, *args, **kwargs)\n                _TopoMeshClass.__init__(self, downhill_neighbours, *args, **kwargs)\n                # super(QuagMeshClass, self).__init__(dm, *args, **kwargs)\n\n        return QuagMeshClass(DM, *args, **kwargs)\n\n    else:\n        raise TypeError(""Mesh type {:s} unknown\\n\\\n            Known mesh types: {}"".format(BaseMeshType, list(known_basemesh_classes.keys())))\n\n    return\n'"
quagmire/documentation.py,0,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\nimport pkg_resources as _pkg_resources\nfrom distutils import dir_util as _dir_util\nimport os\n\n\ndef install_documentation(path=""./Quagmire-Notebooks""):\n   """"""Install the example notebooks for quagmire in the given location\n\n   WARNING: If the path exists, the Notebook files will be written into the path\n   and will overwrite any older, existing files with which they collide. The default\n   path (""./Quagmire-Notebooks"") is chosen to make collision less likely / problematic\n\n   The documentation for quagmire is in the form of jupyter notebooks.\n\n   Some dependencies exist for the notebooks to be useful:\n\n      - matplotlib: for some diagrams\n      - lavavu: for plotting 3D interactive surfaces\n\n   Quagmire dependencies may be explicitly imported into the notebooks including:\n\n      - stripy (for triangulation / interpolation)\n      - numpy\n      - scipy (for k-d tree point location)\n\n   """"""\n\n   ## TODO: download / clone the repo on demand to the require path\n\n   print(""Documentation is availble at https://github.com/underworld-community/quagmire-examples-and-workflows"")\n\n   return\n'"
scripts/run-jupyter.py,0,"b'#!/usr/bin/env python\n\nfrom subprocess import call\nimport os\nimport time\n\n# We want to start a server from each www directory\n# where everything was built by the site-builder script\n\n# Make sure jupyter defaults are correct (globally)\n\ncall(""jupyter nbextension enable hide_input/main"", shell=True)\ncall(""jupyter nbextension enable rubberband/main"", shell=True)\ncall(""jupyter nbextension enable exercise/main"", shell=True)\n\n# We want to start the server from the _site directory\n# where everything was built by the docker-site-builder script\n\ndir      = ""www""\npassword = ""quagmire-0""\nport     = 8080\ncall( ""cd {:s} && nohup jupyter notebook --port={:d} --ip=\'*\' --no-browser \\\n       --NotebookApp.token={:s} --NotebookApp.default_url=/files/index.html &"".format(dir, port, password), shell=True )\n\n# Don\'t exit\n\nwhile True:\n    time.sleep(10)\n'"
scripts/run-jupyter8.py,0,"b'#!/usr/bin/env python\n\nfrom subprocess import call\nimport os\nimport time\n\n# We want to start a server from each www directory\n# where everything was built by the site-builder script\n\n# Make sure jupyter defaults are correct (globally)\n\ncall(""jupyter nbextension enable hide_input/main"", shell=True)\ncall(""jupyter nbextension enable rubberband/main"", shell=True)\ncall(""jupyter nbextension enable exercise/main"", shell=True)\n\n# This could be automated, but I am not sure how well the number of\n# servers will scale ... so leave at 8 ... and hand build\n\n# The root user is www\n\nusers   = { ""www""       : [""vieps-pye-boss"", 8080 ],\n            ""build/www1"": [""vieps-pye-1"",    8081 ],\n            ""build/www2"": [""vieps-pye-2"",    8082 ],\n            ""build/www3"": [""vieps-pye-3"",    8083 ],\n            ""build/www4"": [""vieps-pye-4"",    8084 ],\n            ""build/www5"": [""vieps-pye-5"",    8085 ],\n            ""build/www6"": [""vieps-pye-6"",    8086 ],\n            ""build/www7"": [""vieps-pye-7"",    8087 ],\n            ""build/www8"": [""vieps-pye-8"",    8088 ],\n            ""build/www9"": [""vieps-pye-9"",    8089 ],\n            ""build/www10"": [""vieps-pye-10"",  8090 ],\n            ""build/www11"": [""vieps-pye-11"",  8091 ],\n            ""build/www12"": [""vieps-pye-12"",  8092 ] }\n\n# Maybe we need to quote the password in case it has odd characters in it\n\nfor dir in list(users.keys()):\n    password = users[dir][0]\n    port     = users[dir][1]\n    call( ""cd {:s} && nohup jupyter notebook --port={:d} --ip=\'*\' --no-browser\\\n           --NotebookApp.token={:s} --NotebookApp.default_url=/files/index.html &"".format(dir, port, password), shell=True )\n\n# Don\'t exit\n\nwhile True:\n    time.sleep(10)\n'"
scripts/run-sitebuilder.py,0,"b'#! /usr/bin/env python2\n#\n\nfrom subprocess import call\nimport os\n\n## Build local files into the www location to be served by jupyter\n\nsitePath = os.path.normpath(os.path.dirname((os.path.join(os.getcwd(),os.path.dirname(__file__) ) ) ) )\nsiteDir = os.path.join(sitePath,""www"")\n\n# Copy Notebooks to the user space (leaving originals unchanged)\n# Make symlinks for Data and Documentation so that these are visible\n# to the web server started in the www build directory\n\nprint((""Building {:s}"".format(siteDir)))\n\n## mkdocs to build the site\ncall(""cd {:s} && mkdocs build --theme united --clean"".format(sitePath), shell=True)\n\n## copy pdf documentation and other helpful information that is not in mkdocs\ncall(""ln -s {:s}/Documentation/ {:s}"".format(sitePath, siteDir), shell=True)\n\n## All the notebook examples are  installed from module calls (should use os.join / os.path etc)\ncall(""mkdir -p {:s}/Notebooks"".format(siteDir), shell=True)\n\nprint((""Adding {:s}"".format(os.path.join(siteDir,""Notebooks"",""Stripy""))))\nimport stripy\nstripy.documentation.install_documentation(os.path.join(siteDir,""Notebooks"",""Stripy""))\n\nprint((""Adding {:s}"".format(os.path.join(siteDir,""Notebooks"",""Quagmire""))))\n\nimport quagmire\nquagmire.documentation.install_documentation(os.path.join(siteDir,""Notebooks"",""Quagmire""))\n\n## copy pdf documentation and other helpful information that is not in mkdocs\ncall(""ln -s {:s}/quagmire/Examples/data {:s}/Notebooks"".format(sitePath, siteDir), shell=True)\n\n\n## Any data to copy over ?\n\n# call(""ln -s {:s}/Data/ {:s}"".format(sitePath, siteDir), shell=True)\n# call(""cp -r {:s}/Notebooks/ {:s}/Notebooks"".format(sitePath,siteDir), shell=True)\n\n\n## Trust all notebooks\ncall(""find {} -name \\*.ipynb  -print0 | xargs -0 jupyter trust"".format(siteDir), shell=True)\n'"
scripts/run-sitebuilder8.py,0,"b'#! /usr/bin/env python\n#\n\nfrom subprocess import call\nimport os\n\nsitePath = os.path.normpath(os.path.dirname((os.path.join(os.getcwd(),os.path.dirname(__file__) ) ) ) )\nnotebookPath = os.path.join(sitePath,""Notebooks"")\n\n# Copy Notebooks to the user space (leaving originals unchanged)\n# Make symlinks for Data and Documentation so that these are visible\n# to the web server started in the www build directory\n\n# MULTIUSER: the ""root"" user is in www and has a symlink to the notebooks\n# which allows that user to modify the master copies of the notebooks.\n# Use a different theme for this one !!\n\n# Note, such changes are not effected unless the site is rebuilt, but also,\n# the rebuild does not nuke changes by the root user.\n\nsiteDir = os.path.join(sitePath,""www"")\n\nprint(""Building {:s}"".format(siteDir))\n\ncall(""cd {:s} && mkdocs build --theme united --clean"".format(sitePath), shell=True)\ncall(""ln -s {:s}/Data/ {:s}"".format(sitePath, siteDir), shell=True)\ncall(""ln -s {:s}/Notebooks/ {:s}"".format(sitePath, siteDir), shell=True)\ncall(""ln -s {:s}/Documentation/ {:s}"".format(sitePath, siteDir), shell=True)\n\n\nuserList = [ str(i) for i in range(1,13) ]\n\nfor user in userList:\n    siteDir = os.path.join(sitePath,""build"",""www""+user)\n    print(""Building {:s}"".format(siteDir))\n    call(""cd {:s} && mkdocs build --site-dir {:s}"".format(sitePath, siteDir), shell=True)\n    call(""ln -s {:s}/Data/ {:s}"".format(sitePath, siteDir), shell=True)\n    call(""ln -s {:s}/Documentation/ {:s}"".format(sitePath, siteDir), shell=True)\n    call(""cp -r {:s}/Notebooks {:s}/Notebooks"".format(sitePath,siteDir), shell=True)  # Hey ! should using os.path.join !!\n\ncall(""find . -name \\*.ipynb  -print0 | xargs -0 jupyter trust"", shell=True)\n'"
tests/conftest.py,0,"b'import pytest\nimport numpy as np\nimport quagmire\nfrom quagmire.tools import meshtools\n\n@pytest.fixture(scope=""module"")\ndef load_triangulated_mesh():\n    minX, maxX = -5.0, 5.0\n    minY, maxY = -5.0, 5.0\n    spacingX = 0.1\n    spacingY = 0.1\n\n    # construct an elliptical mesh\n    x, y, simplices = meshtools.elliptical_mesh(minX, maxX, minY, maxY, spacingX, spacingY)\n\n    # return as a dictionary\n    mesh_dict = {\'x\': x, \'y\': y, \'simplices\': simplices}\n    return mesh_dict\n\n\n@pytest.fixture(scope=""module"")\ndef load_triangulated_mesh_DM():\n    minX, maxX = -5.0, 5.0\n    minY, maxY = -5.0, 5.0\n    spacingX = 0.1\n    spacingY = 0.1\n\n    # construct an elliptical mesh\n    x, y, simplices = meshtools.elliptical_mesh(minX, maxX, minY, maxY, spacingX, spacingY)\n\n    return meshtools.create_DMPlex(x, y, simplices)\n\n\n@pytest.fixture(scope=""module"")\ndef load_triangulated_spherical_mesh():\n    import stripy\n\n    # construct a spherical stripy mesh\n    sm = stripy.spherical_meshes.icosahedral_mesh(refinement_levels=3)\n\n    # return as a dictionary\n    mesh_dict = {\'lons\': sm.lons, \'lats\': sm.lats, \'simplices\': sm.simplices}\n    return mesh_dict\n\n\n@pytest.fixture(scope=""module"")\ndef load_triangulated_spherical_mesh_DM():\n    import stripy\n\n    # construct a spherical stripy mesh\n    sm = stripy.spherical_meshes.icosahedral_mesh(refinement_levels=3)\n\n    return meshtools.create_spherical_DMPlex(sm.lons, sm.lats, sm.simplices)\n\n\n@pytest.fixture(scope=""module"")\ndef load_pixelated_mesh_DM():\n    minX, maxX = 0., 1.\n    minY, maxY = 0., 1.\n    resX, resY = 50, 50\n    return meshtools.create_DMDA(minX, maxX, minY, maxY, resX, resY)'"
tests/test_0_imports.py,0,"b'# -*- coding: utf-8 -*-\n\nimport pytest\n\n# ==========================\n\ndef test_numpy_import():\n    import numpy\n    return\n\ndef test_scipy_import():\n    import scipy\n    print(""\\t\\t You have scipy version {}"".format(scipy.__version__))\n    return\n\ndef test_sympy_import():\n    import sympy\n    print(""\\t\\t You have sympy version {}"".format(sympy.__version__))\n    return\n\ndef test_pint_import():\n    import pint\n    return\n\ndef test_stripy_import():\n    import stripy\n    from stripy import spherical_meshes\n    from stripy import cartesian_meshes\n    from stripy import sTriangulation\n    from stripy import Triangulation\n\ndef test_quagmire_modules():\n    import quagmire\n    from quagmire import documentation\n    from quagmire import function\n    from quagmire import mesh\n    from quagmire import tools\n    from quagmire import scaling\n    from quagmire import QuagMesh\n    from quagmire import _fortran\n\ndef test_jupyter_available():\n    from subprocess import check_output\n    try:\n        result = str(check_output([\'which\', \'jupyter\']))[2:-3]\n    except:\n        print( ""jupyter notebook system is not installed"" )\n        print( ""  - This is needed for notebook examples"")\n'"
tests/test_1_mesh_setup.py,0,"b'# -*- coding: utf-8 -*-\n\nimport pytest\n\n# ==========================\n\nDM = None\n\n\n## First of all test the ""pixmesh""\n\ndef test_DMDA():\n    from quagmire.tools import meshtools\n\n    global DM\n\n    minX, maxX = -5.0, 5.0\n    minY, maxY = -5.0, 5.0\n\n    resX = 75\n    resY = 75\n\n    DM = meshtools.create_DMDA(minX, maxX, minY, maxY, resX, resY)\n\n    assert DM.sizes == (75, 75)\n\n    return\n\n\ndef test_flatmesh_from_DMDA():\n\n    from quagmire import QuagMesh\n\n    global DM\n\n    mesh = QuagMesh(DM)\n    assert mesh.sizes == ((5625, 5625), (5625, 5625))\n\n    return\n\n## Now test the triangulated mesh\n\n\ndef test_DMPlex():\n\n    from quagmire.tools import meshtools\n\n    global DM\n\n    minX, maxX = -5.0, 5.0\n    minY, maxY = -5.0, 5.0\n\n    spacingX = 0.1\n    spacingY = 0.1\n\n    x, y, simplices = meshtools.elliptical_mesh(minX, maxX, minY, maxY, spacingX, spacingY)\n    DM = meshtools.create_DMPlex(x, y, simplices)\n    assert (DM.getCoordinates().array.shape) == (4880,)\n\n\n\ndef test_flatmesh_from_DMPlex():\n\n    from quagmire import QuagMesh\n\n    global DM\n\n    mesh = QuagMesh(DM)\n    assert mesh.sizes == ((2440, 2440), (2440, 2440))\n\n    return\n\n# We should test the Lloyd mesh improvement at this point\n# TODO: test mesh improvement\n\n\n## Test refinement of triangulated meshes (use existing mesh from previous test)\n\ndef test_DM_refinement():\n\n    from quagmire.tools import meshtools\n\n    minX, maxX = -5.0, 5.0\n    minY, maxY = -5.0, 5.0\n\n    spacingX = 0.5\n    spacingY = 0.5\n\n    x, y, simplices = meshtools.elliptical_mesh(minX, maxX, minY, maxY, spacingX, spacingY)\n    DM = meshtools.create_DMPlex(x, y, simplices)\n\n    # ToDo: Figure out this recent change\n    # The following now fails with\n    # E   petsc4py.PETSc.Error: error code 62\n    # E   [0] DMRefine() line 1885 in .../interface/dm.c\n    # E   [0] DMRefine_Plex() line 10496 in .../src/dm/impls/plex/plexrefine.c\n    # E   [0] DMPlexRefineUniform_Internal() line 10204 in .../src/dm/impls/plex/plexrefine.c\n    # E   [0] Invalid argument\n    # E   [0] Mesh must be interpolated for regular refinement\n\n    # DM_r1 = meshtools._refine_DM(DM, refinement_levels=1)\n    # DM_r2 = meshtools._refine_DM(DM, refinement_levels=2)\n\n    DM_r1 = meshtools.create_DMPlex(x, y, simplices, refinement_levels=1)\n    DM_r2 = meshtools.create_DMPlex(x, y, simplices, refinement_levels=2)\n\n    assert (DM.getCoordinates().array.shape)    == (182,)\n    assert (DM_r1.getCoordinates().array.shape) == (686,)\n    assert (DM_r2.getCoordinates().array.shape) == (2666,)\n\n    return DM\n\n\n\n\n\n\n\n'"
tests/test_2_mesh_variables.py,9,"b'# -*- coding: utf-8 -*-\n\nimport pytest\n\n# ==========================\n\nfrom quagmire.tools import meshtools\nfrom quagmire import QuagMesh\nfrom quagmire.mesh import MeshVariable\nimport numpy as np\n\nfrom conftest import load_triangulated_mesh_DM\n\n\ndef test_mesh_variable_instance(load_triangulated_mesh_DM):\n\n    mesh = QuagMesh(load_triangulated_mesh_DM, downhill_neighbours=1)\n\n    phi = mesh.add_variable(name=""PHI(X,Y)"")\n    psi = mesh.add_variable(name=""PSI(X,Y)"")\n\n    # check the variable data is available\n\n    phi.data = np.cos(mesh.coords[:,0])**2.0 + np.sin(mesh.coords[:,0])**2.0\n    psi.data = np.cos(mesh.coords[:,1])**2.0 + np.sin(mesh.coords[:,1])**2.0\n\n    # check that evaluation / interpolation is possible\n\n    assert np.isclose(phi.interpolate(0.01,1.0), 1.0)\n    assert np.isclose(psi.interpolate(0.01,1.0), 1.0)\n\n    assert np.isclose(phi.evaluate(0.01,1.0),    1.0)\n    assert np.isclose(psi.evaluate(0.01,1.0),    1.0)\n\n    ## This is the alternate interface to access the same code\n\n    phi1 = MeshVariable(name=""PHI(X,Y)"", mesh=mesh)\n\n    return\n\ndef test_mesh_variable_properties(load_triangulated_mesh_DM):\n\n    mesh = QuagMesh(load_triangulated_mesh_DM, downhill_neighbours=1)\n\n    ## Don\'t specify a name\n\n    phi = mesh.add_variable()\n\n    ## a locked array\n\n    phi = mesh.add_variable(name=""phi"", locked=True)\n    assert ""RO"" in phi.__repr__()\n\n\n    try:\n        phi.data[:] = 0.0\n    except ValueError:\n        ## This is the expected behaviour !\n        pass\n\n\n    phi.unlock()\n    assert ""RW"" in phi.__repr__()\n\n    phi.data = 1.0\n    assert phi.data.mean() == 1.0\n\n\n    return\n\n\n\ndef test_mesh_variable_derivative(load_triangulated_mesh_DM):\n\n    mesh = QuagMesh(load_triangulated_mesh_DM, downhill_neighbours=1)\n\n    # Functions we can differentiate easily\n\n    phi = mesh.add_variable(name=""PHI(X,Y)"")\n    psi = mesh.add_variable(name=""PSI(X,Y)"")\n\n\n    phi.data = np.sin(mesh.coords[:,0])\n    psi.data = np.cos(mesh.coords[:,0])\n\n    assert(np.isclose(phi.fn_gradient[0].evaluate(0.0,0.0), psi.evaluate(0.0,0.0), rtol=1.0e-3))\n\n\n    return\n\n\n'"
tests/test_3_quagmire_functions.py,3,"b'# -*- coding: utf-8 -*-\n\nimport pytest\n\n# ==========================\n\n# This has already been tested by now\n\nfrom quagmire.tools import meshtools\nfrom quagmire import QuagMesh\nfrom quagmire.mesh import MeshVariable\nfrom quagmire import function as fn\nimport numpy as np\n\nfrom conftest import load_triangulated_mesh_DM\n\ndef test_parameters():\n\n    ## Assignment\n\n    A = fn.parameter(10.0)\n\n\n    ## And this is how to update A\n\n    A.value = 100.0\n    assert fn.math.exp(A).evaluate(0.0,0.0) == np.exp(100.0)\n\n    ## This works too ... and note the floating point conversion\n    A(101)\n    assert fn.math.exp(A).evaluate(0.0,0.0) == np.exp(101.0)\n\n    ## More complicated examples\n    assert (fn.math.sin(A)**2.0 + fn.math.cos(A)**2.0).evaluate(0.0,0.0) == 1.0\n\n\n\ndef test_fn_description():\n\n\n    A = fn.parameter(10.0)\n    B = fn.parameter(3)\n\n\n    ## These will flush out any changes in the interface\n\n    assert A.description == ""10.0""\n    assert B.description == ""3.0""\n\n    assert (fn.math.sin(A)+fn.math.cos(B)).description == ""(sin(10.0))+(cos(3.0))""\n    assert (fn.math.sin(A)**2.0 + fn.math.cos(A)**2.0).description == ""((sin(10.0))**(2.0))+((cos(10.0))**(2.0))""\n\n\n    return\n\ndef test_fn_mesh_variables(load_triangulated_mesh_DM):\n\n    mesh = QuagMesh(load_triangulated_mesh_DM, down_neighbours=2)\n\n    height = mesh.add_variable(name=""h(X,Y)"")\n    height.data = np.ones(mesh.npoints)\n    height.lock()\n\n    h_scale  = fn.parameter(2.0)\n    h_offset = fn.parameter(1.0)\n\n    scaled_height = height * h_scale + h_offset\n\n    assert scaled_height.evaluate(mesh).max() == 3.0\n\n    h_offset.value = 10.0\n    height.unlock()\n\n    assert scaled_height.evaluate(mesh).max() == 12.0\n\n\n'"
tests/test_4_mesh_creation.py,12,"b'\nimport pytest\nimport numpy as np\nimport numpy.testing as npt\nimport quagmire\nfrom quagmire.tools import meshtools\nfrom mpi4py import MPI\ncomm = MPI.COMM_WORLD\n\nfrom conftest import load_triangulated_mesh\nfrom conftest import load_triangulated_spherical_mesh\n\n\ndef test_DMPlex_creation(load_triangulated_mesh):\n    x = load_triangulated_mesh[\'x\']\n    y = load_triangulated_mesh[\'y\']\n    simplices = load_triangulated_mesh[\'simplices\']\n    DM = meshtools.create_DMPlex(x, y, simplices)\n    coords = DM.getCoordinatesLocal().array.reshape(-1,2)\n\n    if comm.size == 1:\n        npt.assert_allclose(coords[:,0], x, err_msg=""DMPlex x coordinates are not identical to input x coordinates"")\n        npt.assert_allclose(coords[:,1], y, err_msg=""DMPlex y coordinates are not identical to input y coordinates"")\n\n\ndef test_DMPlex_creation_from_points(load_triangulated_mesh):\n    x = load_triangulated_mesh[\'x\']\n    y = load_triangulated_mesh[\'y\']\n    simplices = load_triangulated_mesh[\'simplices\']\n    DM = meshtools.create_DMPlex_from_points(x, y)\n    coords = DM.getCoordinatesLocal().array.reshape(-1,2)\n\n    if comm.size == 1:\n        npt.assert_allclose(coords[:,0], x, err_msg=""DMPlex x coordinates are not identical to input x coordinates"")\n        npt.assert_allclose(coords[:,1], y, err_msg=""DMPlex y coordinates are not identical to input y coordinates"")\n\n\ndef test_DMPlex_refinement(load_triangulated_mesh):\n    x = load_triangulated_mesh[\'x\']\n    y = load_triangulated_mesh[\'y\']\n    simplices = load_triangulated_mesh[\'simplices\']\n    DM = meshtools.create_DMPlex(x, y, simplices)\n\n    DM_r1 = meshtools.refine_DM(DM, refinement_levels=1)\n    DM_r2 = meshtools.refine_DM(DM, refinement_levels=2)\n    DM_r3 = meshtools.refine_DM(DM, refinement_levels=3)\n\n    v1 = DM_r1.createLocalVector().getSize()\n    v2 = DM_r2.createLocalVector().getSize()\n    v3 = DM_r3.createLocalVector().getSize()\n\n    assert v1 < v2 < v3, ""Mesh refinement is not working""\n\n\ndef test_DMPlex_spherical_creation(load_triangulated_spherical_mesh):\n    lons = load_triangulated_spherical_mesh[\'lons\']\n    lats = load_triangulated_spherical_mesh[\'lats\']\n    simplices = load_triangulated_spherical_mesh[\'simplices\']\n    DM = meshtools.create_spherical_DMPlex(lons, lats, simplices)\n\n\ndef test_DMPlex_creation_from_spherical_points(load_triangulated_spherical_mesh):\n    lons = load_triangulated_spherical_mesh[\'lons\']\n    lats = load_triangulated_spherical_mesh[\'lats\']\n    simplices = load_triangulated_spherical_mesh[\'simplices\']\n    DM = meshtools.create_DMPlex_from_spherical_points(lons, lats)\n\n\n# def test_DMPlex_creation_from_box():\n#     minX, maxX = -5., 5.\n#     minY, maxY = -5., 5.\n#     resX, resY = 0.1, 0.1\n#     DM = meshtools.create_DMPlex_from_box(minX, maxX, minY, maxY, resX, resY, refinement=None)\n#     coords = DM.getCoordinatesLocal().array.reshape(-1,2)\n\n#     if comm.size == 1:\n#         assert coords.size >= 4, ""Mesh creation from bounding box failed""\n\n\ndef test_DMDA_creation():\n    minX, maxX = -5., 5.\n    minY, maxY = -5., 5.\n    Nx, Ny = 10, 10\n    DM = meshtools.create_DMDA(minX, maxX, minY, maxY, Nx, Ny)\n    coords = DM.getCoordinatesLocal().array.reshape(-1,2)\n\n    # create expected grid coordinates\n    xcoords = np.linspace(minX, maxX, Nx)\n    ycoords = np.linspace(minY, maxY, Ny)\n    xq, yq = np.meshgrid(xcoords, ycoords)\n    coords0 = np.column_stack([xq.ravel(), yq.ravel()])\n\n    if comm.size == 1:\n        err_msg = ""Coordinates are not a regularly-spaced grid""\n        npt.assert_allclose(coords, coords0, err_msg=err_msg)\n\n\ndef test_mesh_improvement(load_triangulated_mesh):\n    from quagmire import QuagMesh\n    from quagmire.tools import lloyd_mesh_improvement\n\n    x = np.array([0., 0., 1., 1.])\n    y = np.array([0., 1., 0., 1.])\n\n    x = np.hstack([x, 0.5*np.random.random(size=100)])\n    y = np.hstack([y, 0.5*np.random.random(size=100)])\n\n\n    DM = meshtools.create_DMPlex_from_points(x, y)\n    mesh = QuagMesh(DM)\n\n    bmask = mesh.bmask.copy()\n\n    # perturb with Lloyd\'s mesh improvement algorithm\n    x1, y1 = lloyd_mesh_improvement(x, y, bmask, 3)\n    DM1 = meshtools.create_DMPlex_from_points(x1, y1)\n    mesh1 = QuagMesh(DM1)\n\n    mesh_equant = mesh.neighbour_cloud_distances.mean(axis=1) / ( np.sqrt(mesh.area))\n    mesh1_equant = mesh1.neighbour_cloud_distances.mean(axis=1) / ( np.sqrt(mesh1.area))\n\n    mask_bbox = np.ones(mesh.npoints, dtype=bool)\n    mask_bbox[x1 < x.min()] = False\n    mask_bbox[x1 > x.max()] = False\n    mask_bbox[y1 < y.min()] = False\n    mask_bbox[y1 > y.max()] = False\n\n    mesh_equant = mesh_equant[mask_bbox]\n    mesh1_equant = mesh1_equant[mask_bbox]\n\n    assert np.std(mesh1_equant) < np.std(mesh_equant), ""Mesh points are not more evenly spaced than previously""\n\n\n# This fails in conda (there is no PETSc build with hdf5)\n\ndef test_mesh_save_to_hdf5(load_triangulated_mesh):\n    import petsc4py\n\n    x = load_triangulated_mesh[\'x\']\n    y = load_triangulated_mesh[\'y\']\n    simplices = load_triangulated_mesh[\'simplices\']\n\n    DM = meshtools.create_DMPlex(x, y, simplices)\n\n    # save to hdf5 file\n    try:\n        meshtools.save_DM_to_hdf5(DM, ""test_mesh.h5"")\n\n    # TODO: This is  an installation problem we\n    # ought to catch elsewhere \n    except petsc4py.PETSc.Error:\n        print(""This error means that PETSc was not installed with hdf5"")\n\n\n# This fails in conda (we need our own PETSc build with hdf5)\n\ndef test_mesh_load_from_hdf5():\n    from quagmire import QuagMesh\n    import petsc4py\n\n    try:\n        DM = meshtools.create_DMPlex_from_hdf5(""test_mesh.h5"")\n\n    except petsc4py.PETSc.Error:\n        print(""This error means that PETSc was not installed with hdf5"")\n    \n    else:\n        mesh = QuagMesh(DM)\n        assert mesh.npoints > 0, ""mesh could not be successfully loaded""\n\n '"
tests/test_5_FlatMesh_object.py,9,"b'\nimport pytest\nimport numpy as np\nimport quagmire\nfrom quagmire import QuagMesh\nfrom quagmire import function as fn\nfrom quagmire.tools import meshtools\n\nfrom conftest import load_triangulated_mesh_DM\n\n\ndef test_derivatives_and_interpolation(load_triangulated_mesh_DM):\n    mesh = QuagMesh(load_triangulated_mesh_DM)\n\n    height = mesh.add_variable(name=""h(x,y)"")\n    height.data = mesh.tri.x**2 + mesh.tri.y**2\n\n    dhdx = height.fn_gradient[0]\n    dhdy = height.fn_gradient[1]\n\n    # interpolate onto a straight line\n    # bounding box should be [-5,5,-5,5]\n    xy_pts = np.linspace(-5,5,10)\n\n    interp_x = dhdx.evaluate(xy_pts, np.zeros_like(xy_pts))\n    interp_y = dhdy.evaluate(np.zeros_like(xy_pts), xy_pts)\n\n    ascending_x = ( np.diff(interp_x) > 0 ).all()\n    ascending_y = ( np.diff(interp_y) > 0 ).all()\n\n    assert ascending_x, ""Derivative evaluation failed in the x direction""\n    assert ascending_y, ""Derivative evaluation failed in the y direction""\n\n\ndef test_smoothing(load_triangulated_mesh_DM):\n    mesh = QuagMesh(load_triangulated_mesh_DM)\n\n    noise = np.random.rand(mesh.npoints)\n    smooth_noise = mesh.local_area_smoothing(noise, its=1)\n    smoother_noise = mesh.local_area_smoothing(noise, its=2)\n\n    err_msg = ""Smoothing random noise using RBF is not working""\n    assert np.std(noise) > np.std(smooth_noise) > np.std(smoother_noise), err_msg\n\n\ndef test_spherical_area():\n    import stripy\n    cm = stripy.spherical_meshes.icosahedral_mesh(refinement_levels=2)\n    DM = meshtools.create_spherical_DMPlex(cm.lons, cm.lats, cm.simplices)\n    mesh = QuagMesh(DM)\n\n    # adjust radius of the sphere\n    # this should re-calculate pointwise area and weights\n    R1 = 1.0\n    R2 = 2.0\n\n    mesh.radius = R1\n    area1 = mesh.pointwise_area.sum()\n    assert np.isclose(area1,4.*np.pi), ""Area of the unit-sphere is incorrect, {}"".format(area1)\n\n    mesh.radius = R2\n    area2 = mesh.pointwise_area.sum()\n    assert np.isclose(area2,4.*np.pi*R2**2), ""Area of sphere with r=2 is incorrect, {}"".format(area2)'"
tests/test_6_TopoMesh_object.py,19,"b'import pytest\nimport numpy as np\nimport quagmire\nfrom quagmire import function as fn\nfrom quagmire import QuagMesh\nfrom petsc4py import PETSc\n\nfrom conftest import load_triangulated_mesh_DM\n\n\ndef test_height_mesh_variable(load_triangulated_mesh_DM):\n    mesh = QuagMesh(load_triangulated_mesh_DM)\n    x, y = mesh.tri.x, mesh.tri.y\n\n    radius  = np.sqrt((x**2 + y**2))\n    theta   = np.arctan2(y,x) + 0.1\n\n    height  = np.exp(-0.025*(x**2 + y**2)**2) + 0.25 * (0.2*radius)**4  * np.cos(5.0*theta)**2 ## Less so\n    height  += 0.5 * (1.0-0.2*radius)\n\n    with mesh.deform_topography():\n        print(""Update topography data array (automatically rebuilds matrices)"")\n        mesh.topography.data = height\n\n    # make sure the adjacency matrices are updated\n    mat_info = mesh.downhillMat.getInfo()\n    mat_size = mesh.downhillMat.getSize()\n\n    assert mat_info[\'nz_used\'] >= mat_size[0], ""Downhill matrix is not initialised correctly\\n{}"".format(mat_info)\n\n\ndef test_downhill_neighbours(load_triangulated_mesh_DM):\n    mesh = QuagMesh(load_triangulated_mesh_DM, downhill_neighbours=1)\n    x, y = mesh.tri.x, mesh.tri.y\n\n    radius  = np.sqrt((x**2 + y**2))\n    theta   = np.arctan2(y,x) + 0.1\n\n    height  = np.exp(-0.025*(x**2 + y**2)**2) + 0.25 * (0.2*radius)**4  * np.cos(5.0*theta)**2 ## Less so\n    height  += 0.5 * (1.0-0.2*radius)\n\n    nz = []\n\n    # increase number of downhill neighbours from 1-3\n    for i in range(1,4):\n        mesh = QuagMesh(load_triangulated_mesh_DM, downhill_neighbours=i)\n\n        with mesh.deform_topography():\n            mesh.topography.data = height\n\n        mat_info = mesh.downhillMat.getInfo()\n        nz.append(mat_info[\'nz_used\'])\n\n    mat_size = mesh.downhillMat.getSize()\n    ascending = (np.diff(np.array(nz) - mat_size[0]) > 0).all()\n\n    err_msg = ""Downhill matrix is not denser with more downhill neighbours: {}""\n    assert ascending, err_msg.format(list(enumerate(nz)))\n\n\ndef test_cumulative_flow(load_triangulated_mesh_DM):\n\n    def identify_outflow_points(self):\n        """"""\n        Identify the (boundary) outflow points and return an array of (local) node indices\n        """"""\n\n        # nodes = np.arange(0, self.npoints, dtype=np.int)\n        # low_nodes = self.down_neighbour[1]\n        # mask = np.logical_and(nodes == low_nodes, self.bmask == False)\n        #\n\n        i = self.downhill_neighbours\n\n        o = (np.logical_and(self.down_neighbour[i] == np.indices(self.down_neighbour[i].shape), self.bmask == False)).ravel()\n        outflow_nodes = o.nonzero()[0]\n\n        return outflow_nodes\n\n    mesh = QuagMesh(load_triangulated_mesh_DM)\n\n    x, y = mesh.tri.x, mesh.tri.y\n\n    radius  = np.sqrt((x**2 + y**2))\n    theta   = np.arctan2(y,x) + 0.1\n\n    height  = np.exp(-0.025*(x**2 + y**2)**2) + 0.25 * (0.2*radius)**4  * np.cos(5.0*theta)**2 ## Less so\n    height  += 0.5 * (1.0-0.2*radius)\n\n    with mesh.deform_topography():\n        mesh.topography.data = height\n\n    # constant rainfall everywhere\n    rainfall = mesh.add_variable(name=\'rainfall\')\n    rainfall.data = np.ones(mesh.npoints)\n\n    upstream_precipitation_integral_fn = mesh.upstream_integral_fn(rainfall)\n    upstream_rain = upstream_precipitation_integral_fn.evaluate(mesh)\n\n    # compare cumulative rainfall at the outflow nodes to the rest of the domain\n    outflow_indices = identify_outflow_points(mesh)\n\n    err_msg = ""cumulative rain outflow is less than the mean""\n    assert upstream_rain[outflow_indices].mean() > upstream_rain.mean(), err_msg\n\n\ndef test_streamwise_smoothing(load_triangulated_mesh_DM):\n    mesh = QuagMesh(load_triangulated_mesh_DM)\n\n    x, y = mesh.tri.x, mesh.tri.y\n\n    radius  = np.sqrt((x**2 + y**2))\n    theta   = np.arctan2(y,x) + 0.1\n\n    height  = np.exp(-0.025*(x**2 + y**2)**2) + 0.25 * (0.2*radius)**4  * np.cos(5.0*theta)**2 ## Less so\n    height  += 0.5 * (1.0-0.2*radius)\n\n    with mesh.deform_topography():\n        mesh.topography.data = height\n\n    # find steeper regions\n    flat_area_mask = fn.misc.levelset(mesh.slope, 0.1, invert=True)\n\n    # seed rainfall\n    rainfall = mesh.add_variable(""rainfall"")\n    rainfall.data = flat_area_mask.evaluate(mesh)\n\n    sm1 = np.std(rainfall.data)\n\n    for i in range(1,4):\n        sm0 = float(sm1)\n        smooth_fn = mesh.streamwise_smoothing_fn(rainfall, its=i)\n        sm1 = np.std(smooth_fn.evaluate(mesh))\n\n        assert sm1 < sm0, ""streamwise smoothing at its={} no smoother than its={}"".format(i, i-1)'"
tests/test_7_save_load_QuagMesh_object.py,0,"b'\nimport pytest\nimport numpy as np\nimport numpy.testing as npt\nimport h5py\nimport quagmire\nimport petsc4py\nfrom quagmire.tools import io\nfrom mpi4py import MPI\ncomm = MPI.COMM_WORLD\n\nfrom conftest import load_triangulated_mesh_DM\nfrom conftest import load_triangulated_spherical_mesh_DM\nfrom conftest import load_pixelated_mesh_DM\n\n\ndef test_save_QuagMesh_trimesh_object(load_triangulated_mesh_DM):\n    mesh = quagmire.QuagMesh(load_triangulated_mesh_DM)\n    mesh.topography.unlock()\n    mesh.topography.data = 1\n    mesh.topography.lock()\n\n    try:\n        mesh.save_quagmire_project(""my_trimesh_project.h5"")\n\n    except petsc4py.PETSc.Error:\n        print(""This error means that PETSc was not installed with hdf5"")\n\n    else:\n        with h5py.File(\'my_trimesh_project.h5\', \'r\') as h5:\n            keys = list(h5.keys())\n\n        assert \'quagmire\' in keys, ""quagmire mesh info not found in HDF5 file""\n        assert \'fields\' in keys, ""no fields are found in the HDF5 file""\n\n\n        # load mesh object in again\n        mesh = io.load_quagmire_project(""my_trimesh_project.h5"")\n        assert mesh.id.startswith(""trimesh""), ""could not load trimesh object""\n        assert int(round(mesh.topography.sum())) == mesh.npoints, ""could not load topography MeshVariable""\n\n\ndef test_save_QuagMesh_strimesh_object(load_triangulated_spherical_mesh_DM):\n    mesh = quagmire.QuagMesh(load_triangulated_spherical_mesh_DM)\n    mesh.topography.unlock()\n    mesh.topography.data = 1\n    mesh.topography.lock()\n\n    try:\n        mesh.save_quagmire_project(""my_strimesh_project.h5"")\n\n    except petsc4py.PETSc.Error:\n        print(""This error means that PETSc was not installed with hdf5"")\n\n    else:\n        with h5py.File(\'my_strimesh_project.h5\', \'r\') as h5:\n            keys = list(h5.keys())\n\n        assert \'quagmire\' in keys, ""quagmire mesh info not found in HDF5 file""\n        assert \'fields\' in keys, ""no fields are found in the HDF5 file""\n\n\n        # load mesh object in again\n        mesh = io.load_quagmire_project(""my_strimesh_project.h5"")\n        assert mesh.id.startswith(""strimesh""), ""could not load trimesh object""\n        assert int(round(mesh.topography.sum())) == mesh.npoints, ""could not load topography MeshVariable""\n\n\ndef test_save_QuagMesh_pixmesh_object(load_pixelated_mesh_DM):\n    mesh = quagmire.QuagMesh(load_pixelated_mesh_DM)\n    mesh.topography.unlock()\n    mesh.topography.data = 1\n    mesh.topography.lock()\n\n    try:\n        mesh.save_quagmire_project(""my_pixmesh_project.h5"")\n\n    except petsc4py.PETSc.Error:\n        print(""This error means that PETSc was not installed with hdf5"")\n\n    else:\n        with h5py.File(\'my_pixmesh_project.h5\', \'r\') as h5:\n            keys = list(h5.keys())\n\n        assert \'quagmire\' in keys, ""quagmire mesh info not found in HDF5 file""\n        assert \'h(x,y)\' in keys, ""no fields are found in the HDF5 file""\n\n\n        # load mesh object in again\n        mesh = io.load_quagmire_project(""my_pixmesh_project.h5"")\n        assert mesh.id.startswith(""pixmesh""), ""could not load trimesh object""\n        assert int(round(mesh.topography.sum())) == mesh.npoints, ""could not load topography MeshVariable""    \n'"
quagmire/equation_systems/__init__.py,0,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom .diffusion import DiffusionEquation\nfrom .erosion_deposition import ErosionDepositionEquation'"
quagmire/equation_systems/diffusion.py,2,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n\nfrom quagmire import function as _fn\n\n\nimport numpy as np\nfrom mpi4py import MPI\ncomm = MPI.COMM_WORLD\n\n# To do ... an interface for (iteratively) dealing with\n# boundaries with normals that are not aligned to the coordinates\n\nclass DiffusionEquation(object):\n\n    ## it is helpful to have a unique ID for each instance so we\n    ## can autogenerate unique names if we wish\n\n    __count = 0\n\n    @classmethod\n    def _count(cls):\n        DiffusionEquation.__count += 1\n        return DiffusionEquation.__count\n\n    @property\n    def id(self):\n        return self.__id\n\n    def __init__(self,\n                 mesh=None,\n                 diffusivity_fn=None,\n                 dirichlet_mask=None,\n                 neumann_x_mask=None,\n                 neumann_y_mask=None,\n                 non_linear=False ):\n\n        self.__id = ""diffusion_solver_{}"".format(self._count())\n        self.mesh = mesh\n\n        # These we initialise to None first of all\n        self._diffusivity = None\n        self._neumann_x_mask = None\n        self._neumann_y_mask = None\n        self._dirichlet_mask = None\n        self._non_linear = False\n\n        if diffusivity_fn is not None:\n            self.diffusivity = diffusivity_fn\n\n        if neumann_x_mask is not None:\n            self.neumann_x_mask = neumann_x_mask\n\n        if neumann_y_mask is not None:\n            self.neumann_y_mask = neumann_y_mask\n\n        if dirichlet_mask is not None:\n            self.dirichlet_mask = dirichlet_mask\n\n\n    @property\n    def mesh(self):\n        return self._mesh\n\n    @mesh.setter\n    def mesh(self, meshobject):\n        self._mesh = meshobject\n        self._phi   = meshobject.add_variable(name=""phi_{}"".format(self.id))\n        return\n\n    @property\n    def phi(self):\n        return self._phi\n    # No setter for this ... it is defined via the mesh.setter\n\n    @property\n    def diffusivity(self):\n        return self._diffusivity\n\n    @diffusivity.setter\n    def diffusivity(self, fn_object):\n        self._diffusivity = fn_object\n\n        if _fn.check_dependency(self._diffusivity, self._phi):\n            # Set the flag directly, bypassing .setter checks\n            self._non_linear = True\n\n        return\n\n    @property\n    def neumann_x_mask(self):\n        return self._neumann_x_mask\n\n    @neumann_x_mask.setter\n    def neumann_x_mask(self, fn_object):\n        _fn.check_object_is_a_q_function_and_raise(fn_object)\n        self._neumann_x_mask = fn_object\n        return\n\n    @property\n    def neumann_y_mask(self):\n        return self._neumann_y_mask\n\n    @neumann_y_mask.setter\n    def neumann_y_mask(self, fn_object):\n        _fn.check_object_is_a_q_function_and_raise(fn_object)\n        self._neumann_y_mask = fn_object\n        return\n\n    @property\n    def dirichlet_mask(self):\n        return self._dirichlet_mask\n\n    @dirichlet_mask.setter\n    def dirichlet_mask(self, fn_object):\n        _fn.check_object_is_a_q_function_and_raise(fn_object)\n        self._dirichlet_mask = fn_object\n        return\n\n    @property\n    def non_linear(self):\n        return self._non_linear\n\n    @non_linear.setter\n    def non_linear(self, bool_object):\n        # Warn if non-linearity exists\n        # This flag is set whenever the diffusivity function is changed\n        # and will need to be over-ridden each time.\n        self._non_linear = bool_object\n        if _fn.check_dependency(self._diffusivity, self._phi):\n            print(""Over-riding non-linear solver path despite diffusivity being non-linear"")\n\n        return\n\n\n\n    def verify(self):\n        """"""Verify solver is ready for launch""""""\n\n        # Check to see we have provided everything in the correct form\n\n        return\n\n\n    def diffusion_rate_fn(self, lazyFn):\n        ## !! create stand alone function\n\n        from quagmire import function as fn\n\n        dx_fn, dy_fn = fn.math.grad(lazyFn)\n        kappa_dx_fn  = fn.misc.where(self.neumann_x_mask,\n                                     self.diffusivity  * dx_fn,\n                                     fn.parameter(0.0))\n        kappa_dy_fn  = fn.misc.where(self.neumann_y_mask,\n                                     self.diffusivity  * dy_fn,\n                                     fn.parameter(0.0))\n\n        dPhi_dt_fn   = fn.misc.where(self.dirichlet_mask, fn.math.div(kappa_dx_fn, kappa_dy_fn), fn.parameter(0.0))\n\n        return dPhi_dt_fn\n\n\n    def diffusion_timestep(self):\n\n        local_diff_timestep = (self._mesh.area / self._diffusivity.evaluate(self._mesh)).min()\n\n        # synchronise ...\n\n        local_diff_timestep = np.array(local_diff_timestep)\n        global_diff_timestep = np.array(0.0)\n        comm.Allreduce([local_diff_timestep, MPI.DOUBLE], [global_diff_timestep, MPI.DOUBLE], op=MPI.MIN)\n\n        return global_diff_timestep\n\n\n\n    def time_integration(self, timestep, steps=1, Delta_t=None, feedback=None):\n\n        from quagmire import function as fn\n\n        if Delta_t is not None:\n            steps = Delta_t // timestep\n            timestep = Delta_t / steps\n\n        elapsed_time = 0.0\n\n        for step in range(0, int(steps)):\n\n            ## Non-linear loop will need to go here\n            ## and update timestep somehow.\n\n            dPhi_dt_fn   = self.diffusion_rate_fn(self.phi)\n\n\n            phi0 = self.phi.copy()\n            self.phi.data = self.phi.data  +  0.5 * timestep * dPhi_dt_fn.evaluate(self._mesh)\n            self.phi.data = phi0.data +  timestep * dPhi_dt_fn.evaluate(self._mesh)\n\n            elapsed_time += timestep\n\n            if feedback is not None and step%feedback == 0 or step == steps:\n                print(""{:05d} - t = {:.3g}"".format(step, elapsed_time))\n\n\n\n        return steps, elapsed_time\n'"
quagmire/equation_systems/erosion_deposition.py,4,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n\nimport numpy as np\nfrom mpi4py import MPI\ncomm = MPI.COMM_WORLD\n\nfrom quagmire.function import LazyEvaluation as _LazyEvaluation\n\n# To do ... an interface for (iteratively) dealing with\n# boundaries with normals that are not aligned to the coordinates\n\nclass ErosionDepositionEquation(object):\n\n    ## it is helpful to have a unique ID for each instance so we\n    ## can autogenerate unique names if we wish\n\n    __count = 0\n\n    @classmethod\n    def _count(cls):\n        ErosionDepositionEquation.__count += 1\n        return ErosionDepositionEquation.__count\n\n    def __init__(self):\n        self.__id = self._count()\n\n    @property\n    def id(self):\n        return self.__id\n\n    def __init__(self,\n                 mesh=None,\n                 rainfall_fn=None,\n                 m=1.0,\n                 n=1.0 ):\n\n        self.__id = self._count()\n\n        # These should become properties ...\n\n\n        self.mesh = mesh\n        self.rainfall = rainfall_fn\n        self.m = m\n        self.n = n\n\n        # This one can be generated and regenerated if mesh changes\n\n\n    @property\n    def mesh(self):\n        return self._mesh\n\n    @mesh.setter\n    def mesh(self, meshobject):\n        self._mesh = meshobject\n        self._erosion_rate   = meshobject.add_variable(name=""qs_{}"".format(self.id))\n        self._deposition_rate   = meshobject.add_variable(name=""ed_{}"".format(self.id))\n        return\n\n    @property\n    def erostion_rate(self):\n        return self._erosion_rate\n    # No setter for this ... it is defined via the mesh.setter\n    @property\n    def deposition_rate(self):\n        return self._deposition_rate\n    # No setter for this ... it is defined via the mesh.setter\n\n    @property\n    def rainfall(self):\n        return self._rainfall\n\n    @rainfall.setter\n    def rainfall(self, fn_object):\n        # Should verify it is a function\n        self._rainfall = fn_object\n        return\n\n    @property\n    def m(self):\n        return self._m\n\n    @m.setter\n    def m(self, scalar):\n        # Should verify it is a function\n        self._m = scalar\n        return\n\n    @property\n    def n(self):\n        return self._n\n\n    @n.setter\n    def n(self, fn_object):\n        # Should verify it is a function\n        self._n = fn_object\n        return\n\n    @property\n    def erosion_deposition_fn(self):\n        return self._erosion_deposition_fn\n\n    @erosion_deposition_fn.setter\n    def erosion_deposition_fn(self, fn_object):\n        if hasattr(self, fn_object):\n            self._erosion_deposition_fn = fn_object\n        else:\n            raise ValueError(""Choose a valid erosion-deposition function"")\n    \n\n    def verify(self):\n\n        # Check to see we have provided everything in the correct form\n\n        return\n\n\n    def stream_power_fn(self):\n        """"""\n        Compute the stream power\n\n        qs = UpInt(rainfall)^m * (grad H(x,y))^2\n        """"""\n        rainfall_fn = self._rainfall\n        m = self._m\n        n = self._n\n\n        # integrate upstream rainfall\n        upstream_precipitation_integral_fn = self._mesh.upstream_integral_fn(rainfall_fn)\n\n        # create stream power function\n        stream_power_law_fn = upstream_precipitation_integral_fn**m * self._mesh.slope**n * self._mesh.mask\n        return stream_power_law_fn\n\n\n## Built-in erosion / deposition functions\n\n    def erosion_deposition_local_equilibrium(self, efficiency):\n        """"""\n        Local equilibrium model\n        """"""\n\n        stream_power_fn = self.stream_power_fn()\n        erosion_rate_fn = efficiency*stream_power_fn\n\n        # store erosion rate so we do not have to evaluate it\n        # again to compute the deposition rate\n        erosion_rate = self._erosion_rate\n        erosion_rate.unlock()\n        erosion_rate.data = erosion_rate_fn.evaluate(self._mesh)\n        erosion_rate.lock()\n\n        deposition_rate_fn = self._mesh.upstream_integral_fn(erosion_rate)\n\n        # might as well store deposition rate too\n        deposition_rate = self._deposition_rate\n        deposition_rate.unlock()\n        deposition_rate.data = deposition_rate_fn.evaluate(self._mesh)\n        deposition_rate.lock()\n\n        # erosion_deposition_fn = deposition_rate - erosion_rate\n        return erosion_rate.data, deposition_rate.data\n\n\n    def fn_local_equilibrium(self, efficiency):\n\n        import quagmire\n\n        def new_fn(*args, **kwargs):\n            \n            erosion_rate, deposition_rate = self.erosion_deposition_local_equilibrium(efficiency)\n            dHdt = deposition_rate - erosion_rate\n            # dHdt = self._mesh.sync(dHdt)\n\n            if len(args) == 1 and args[0] == self._mesh:\n                return dHdt\n            elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n                mesh = args[0]\n                return self.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=dHdt, **kwargs)\n            else:\n                xi = np.atleast_1d(args[0])  # .resize(-1,1)\n                yi = np.atleast_1d(args[1])  # .resize(-1,1)\n                i, e = self.interpolate(xi, yi, zdata=dHdt, **kwargs)\n                return i\n\n        newLazyFn = _LazyEvaluation(mesh=self._mesh)\n        newLazyFn.evaluate = new_fn\n        newLazyFn.description = ""dH/dt""\n        newLazyFn.dependency_list = set([self.erosion_rate, self.deposition_rate])\n\n        return newLazyFn\n\n\n    def erosion_deposition_saltation_length(self):\n        """"""\n        Saltation length\n        """"""\n        raise NotImplementedError(""Check back again soon!"")\n\n\n    def erosion_deposition_transport_limited_flow(self, efficiency, alpha):\n        """"""\n        Transport-limited\n        """"""\n\n        rainfall_fn = self._rainfall\n        m = self._m\n        n = self._n\n        area = self._mesh.pointwise_area\n\n        # integrate upstream / calculate rate\n        upstream_precipitation_integral_fn = self._mesh.upstream_integral_fn(rainfall_fn)\n        upstream_precipitation_rate_fn = upstream_precipitation_integral_fn / area\n\n        # construct stream power function\n        stream_power_fn = upstream_precipitation_integral_fn**m * self._mesh.slope**n * self._mesh.mask\n\n        # only some is eroded\n        erosion_rate_fn = stream_power_fn*efficiency\n\n        # integrate upstream / calculate rate\n        upstream_eroded_material_integral_fn = self._mesh.upstream_integral_fn(erosion_rate_fn)\n        upstream_eroded_material_rate_fn = upstream_eroded_material_integral_fn / area\n\n        deposition_rate_fn = upstream_precipitation_rate_fn / (alpha * upstream_eroded_material_rate_fn)\n\n        erosion_rate = self._erosion_rate\n        erosion_rate.unlock()\n        erosion_rate.data = erosion_rate_fn.evaluate(self._mesh)\n        erosion_rate.lock()\n\n        deposition_rate = self._deposition_rate\n        deposition_rate.unlock()\n        deposition_rate.data = deposition_rate_fn.evaluate(self._mesh)\n        deposition_rate.lock()\n\n        return erosion_rate, deposition_rate\n\n\n    def erosion_deposition_timestep(self):\n\n        from quagmire import function as fn\n\n        # mesh variables\n        erosion_rate = self._erosion_rate\n        deposition_rate = self._deposition_rate\n        slope = self._mesh.slope\n\n        # protect against dividing by zero\n        min_slope = fn.parameter(1e-3)\n        min_depo  = fn.parameter(1e-6)\n        typical_l = fn.math.sqrt(self._mesh.pointwise_area)\n\n        erosion_timestep    = ((slope + min_slope) * typical_l / (erosion_rate + min_depo))\n        deposition_timestep = ((slope + min_slope) * typical_l / (deposition_rate + min_depo))\n\n        dt_erosion_local     = (erosion_timestep.evaluate(self._mesh)).min()\n        dt_deposition_local  = (deposition_timestep.evaluate(self._mesh)).min()\n        dt_erosion_global    = np.array(1e12)\n        dt_deposition_global = np.array(1e12)\n\n        comm.Allreduce([dt_erosion_local, MPI.DOUBLE], [dt_erosion_global, MPI.DOUBLE], op=MPI.MIN)\n        comm.Allreduce([dt_deposition_local, MPI.DOUBLE], [dt_deposition_global, MPI.DOUBLE], op=MPI.MIN)\n\n        return min(dt_erosion_global, dt_deposition_global)\n\n\n\n    def time_integration(self, timestep, steps=1, Delta_t=None, feedback=None):\n\n        from quagmire import function as fn\n\n        topography = self._mesh.topography\n\n        if Delta_t is not None:\n            steps = Delta_t // timestep\n            timestep = Delta_t / steps\n\n        elapsed_time = 0.0\n\n        for step in range(0, int(steps)):\n\n            # deal with local drainage\n            # mesh.low_points_local_flood_fill()\n\n            erosion_rate, deposition_rate = self.erosion_deposition_local_equilibrium()\n\n            # half timestep\n            topography0 = topography.copy()\n            topography.unlock()\n            topography.data = topography.data + 0.5*timestep*(deposition_rate - erosion_rate)\n            topography.lock()\n\n            # full timestep\n            erosion_rate, deposition_rate = self.erosion_deposition_local_equilibrium()\n\n            with self._mesh.deform_topography():\n                # rebuild downhill matrix structure\n                topography.data = topography0.data + timestep*(deposition_rate - erosion_rate)\n\n            elapsed_time += timestep\n\n            if feedback is not None and step%feedback == 0 or step == steps:\n                print(""{:05d} - t = {:.3g}"".format(step, elapsed_time))\n\n\n\n        return steps, elapsed_time\n'"
quagmire/function/__init__.py,0,"b'from .function_classes import LazyEvaluation, LazyGradientEvaluation, parameter\nfrom . import math\nfrom . import misc\nfrom . import stats\n\n\ndef check_dependency(this_fn, that_fn):\n    """"""Does this_fn depend upon that_fn ?""""""\n\n    return id(that_fn) in this_fn.dependency_list\n\ndef check_object_is_a_q_function(fn_object):\n    """"""Is this object a quagmire LazyEvaluation function ?""""""\n\n    return isinstance(fn_object, LazyEvaluation)\n\ndef check_object_is_a_q_function_and_raise(fn_object):\n    """"""If this object is not a quagmire LazyEvaluation function then everything must die !""""""\n\n    if not isinstance(fn_object, LazyEvaluation):\n        raise RuntimeError(""Expecting a quagmire.function object"")\n\ndef check_object_is_a_mesh_variable(fn_object):\n\t"""""" Is this object a quagmire MeshVariable or VectorMeshVariable ?""""""\n\n\treturn fn_object.mesh_data'"
quagmire/function/function_classes.py,11,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\nimport numpy as np\nimport quagmire\n\n\nclass LazyEvaluation(object):\n\n    __count = 0\n\n    @classmethod\n    def _count(cls):\n        LazyEvaluation.__count += 1\n        return LazyEvaluation.__count\n\n    @property\n    def id(self):\n        return self.__id\n\n\n    def __init__(self, mesh=None):\n        """"""Lazy evaluation of mesh variables / parameters\n           If no mesh is provided then no gradient function can be implemented""""""\n\n        self.__id = ""q_fn_{}"".format(self._count())\n\n        self.description = """"\n        self._mesh = mesh\n        self.mesh_data = False\n        self.dependency_list = set([self.id])\n\n        return\n\n    def __repr__(self):\n        return(""quagmire.fn: {}"".format(self.description))\n\n    @staticmethod\n    def convert(obj):\n        """"""\n        This method will attempt to convert the provided input into an\n        equivalent quagmire function. If the provided input is already\n        of LazyEvaluation type, it is immediately returned. Likewise if\n        the input is of None type, it is also returned.\n\n        Parameters\n        ----------\n\n        obj: The object to be converted\n\n        Returns\n        -------\n\n        LazyEvaluation function or None.\n        """"""\n\n        if isinstance(obj, (LazyEvaluation, type(None))):\n            return obj\n        else:\n            try:\n                return parameter(obj)\n            except Exception as e:\n                raise e\n\n    def evaluate(self, *args, **kwargs):\n        raise(NotImplementedError)\n\n\n    @property\n    def fn_gradient(self):\n        mesh = self._mesh\n        newLazyGradient = LazyGradientEvaluation(mesh=mesh)\n        newLazyGradient.evaluate = self._fn_gradient\n        newLazyGradient.description = ""d({0})/dX,d({0})/dY"".format(self.description)\n        newLazyGradient.dependency_list = self.dependency_list\n        return newLazyGradient\n    \n\n    def _fn_gradient(self, *args, **kwargs):\n\n        import quagmire\n\n        if self._mesh is None and mesh is None:\n            raise RuntimeError(""fn_gradient is a numerical differentiation routine based on derivatives of a fitted spline function on a mesh. The function {} has no associated mesh. To obtain *numerical* derivatives of this function, you can provide a mesh to the gradient function. The usual reason for this error is that your function is not based upon mesh variables and can, perhaps, be differentiated without resort to interpolating splines. "".format(self.__repr__()))\n\n\n        elif self._mesh is not None:\n            diff_mesh = self._mesh\n        else:\n            quagmire.mesh.check_object_is_a_q_mesh_and_raise(mesh)\n            diff_mesh = mesh\n\n        local_array = self.evaluate(diff_mesh)\n\n        df_tuple = diff_mesh.derivative_grad(local_array, nit=10, tol=1e-8)\n        ndim = len(df_tuple)\n\n        if len(args) == 1 and args[0] == diff_mesh:\n            return df_tuple\n        elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n            mesh = args[0]\n            df_interp = []\n            for df in df_tuple:\n                result = diff_mesh.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=df, **kwargs)\n                df_interp.append(result)\n            return df_interp\n        elif len(args) > 1:\n            xi = np.atleast_1d(args[0])  # .resize(-1,1)\n            yi = np.atleast_1d(args[1])  # .resize(-1,1)\n            df_interp = []\n            for df in df_tuple:\n                i, e = diff_mesh.interpolate(xi, yi, zdata=df, **kwargs)\n                df_interp.append(i)\n            return df_interp\n        else:\n            err_msg = ""Invalid number of arguments\\n""\n            err_msg += ""Input a valid mesh or coordinates in x,y directions""\n            raise ValueError(err_msg)\n\n\n    @property\n    def description(self):\n        return self._description\n\n    @description.setter\n    def description(self, value):\n        self._description = ""{}"".format(value)\n\n## Arithmetic operations\n\n    def __mul__(self, other):\n        other = self.convert(other)\n        mesh = self._mesh\n        if mesh == None:\n            mesh = other._mesh\n        newLazyFn = LazyEvaluation(mesh=mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : self.evaluate(*args, **kwargs) * other.evaluate(*args, **kwargs)\n        newLazyFn.description = ""({})*({})"".format(self.description, other.description)\n        newLazyFn.dependency_list |= self.dependency_list | other.dependency_list\n        return newLazyFn\n    \n    def __rmul__(self, other):\n        other = self.convert(other)\n        mesh = self._mesh\n        if mesh == None:\n            mesh = other._mesh\n        newLazyFn = LazyEvaluation(mesh=mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : self.evaluate(*args, **kwargs) * other.evaluate(*args, **kwargs)\n        newLazyFn.description = ""({})*({})"".format(other.description, self.description)\n        newLazyFn.dependency_list |= other.dependency_list | self.dependency_list\n        return newLazyFn\n\n    def __add__(self, other):\n        other = self.convert(other)\n        mesh = self._mesh\n        if mesh == None:\n            mesh = other._mesh\n        newLazyFn = LazyEvaluation(mesh=mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : self.evaluate(*args, **kwargs) + other.evaluate(*args, **kwargs)\n        newLazyFn.description = ""({})+({})"".format(self.description, other.description)\n        newLazyFn.dependency_list |= self.dependency_list | other.dependency_list\n        return newLazyFn\n    \n    def __radd__(self, other):\n        other = self.convert(other)\n        mesh = self._mesh\n        if mesh == None:\n            mesh = other._mesh\n        newLazyFn = LazyEvaluation(mesh=mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : self.evaluate(*args, **kwargs) + other.evaluate(*args, **kwargs)\n        newLazyFn.description = ""({})+({})"".format(other.description, self.description)\n        newLazyFn.dependency_list |= other.dependency_list | self.dependency_list\n        return newLazyFn\n\n    def __truediv__(self, other):\n        other = self.convert(other)\n        mesh = self._mesh\n        if mesh == None:\n            mesh = other._mesh\n        newLazyFn = LazyEvaluation(mesh=mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : self.evaluate(*args, **kwargs) / other.evaluate(*args, **kwargs)\n        newLazyFn.description = ""({})/({})"".format(self.description, other.description)\n        newLazyFn.dependency_list |= self.dependency_list | other.dependency_list\n\n        return newLazyFn\n\n    def __sub__(self, other):\n        other = self.convert(other)\n        mesh = self._mesh\n        if mesh == None:\n            mesh = other._mesh\n        newLazyFn = LazyEvaluation(mesh=mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : self.evaluate(*args, **kwargs) - other.evaluate(*args, **kwargs)\n        newLazyFn.description = ""({})-({})"".format(self.description, other.description)\n        newLazyFn.dependency_list |= self.dependency_list | other.dependency_list\n        return newLazyFn\n    \n    def __rsub__(self, other):\n        other = self.convert(other)\n        mesh = self._mesh\n        if mesh == None:\n            mesh = other._mesh\n        newLazyFn = LazyEvaluation(mesh=mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : self.evaluate(*args, **kwargs) - other.evaluate(*args, **kwargs)\n        newLazyFn.description = ""({})-({})"".format(other.description, self.description)\n        newLazyFn.dependency_list |= other.dependency_list | self.dependency_list\n        return newLazyFn\n\n    def __neg__(self):\n        newLazyFn = LazyEvaluation(mesh=self._mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : -1.0 * self.evaluate(*args, **kwargs)\n        newLazyFn.description = ""-({})"".format(self.description)\n        newLazyFn.dependency_list |= self.dependency_list\n\n        return newLazyFn\n\n    def __pow__(self, exponent):\n        if isinstance(exponent, (float, int)):\n            exponent = parameter(float(exponent))\n        newLazyFn = LazyEvaluation(mesh=self._mesh)\n        newLazyFn.evaluate = lambda *args, **kwargs : np.power(self.evaluate(*args, **kwargs), exponent.evaluate(*args, **kwargs))\n        newLazyFn.description = ""({})**({})"".format(self.description, exponent.description)\n        newLazyFn.dependency_list |= self.dependency_list | exponent.dependency_list\n        return newLazyFn\n\n\nclass LazyGradientEvaluation(LazyEvaluation):\n\n    def __init__(self, mesh=None):\n        super(LazyGradientEvaluation, self).__init__(mesh=mesh)\n\n\n    def __getitem__(self, dirn):\n        return self._fn_gradient(dirn=dirn, mesh=self._mesh)\n\n\n    @property\n    def fn_gradient(self):\n        raise NotImplementedError(""gradient operations on the \'LazyGradientEvaluation\' sub-class are not supported"")\n\n    def _fn_gradient(self, dirn=None, mesh=None):\n        """"""\n        The generic mechanism for obtaining the gradient of a lazy variable is\n        to evaluate the values on the mesh at the time in question and use the mesh gradient\n        operators to compute the new values.\n\n        Sub classes may have more efficient approaches. MeshVariables have\n        stored data and don\'t need to evaluate values. Parameters have Gradients\n        that are identically zero ... etc\n        """"""\n\n        import quagmire\n\n        if self._mesh is None and mesh is None:\n            raise RuntimeError(""fn_gradient is a numerical differentiation routine based on derivatives of a fitted spline function on a mesh. The function {} has no associated mesh. To obtain *numerical* derivatives of this function, you can provide a mesh to the gradient function. The usual reason for this error is that your function is not based upon mesh variables and can, perhaps, be differentiated without resort to interpolating splines. "".format(self.__repr__()))\n\n\n        elif self._mesh is not None:\n            diff_mesh = self._mesh\n        else:\n            quagmire.mesh.check_object_is_a_q_mesh_and_raise(mesh)\n            diff_mesh = mesh\n\n        def new_fn_x(*args, **kwargs):\n            dx, dy = self.evaluate(diff_mesh)\n\n            if len(args) == 1 and args[0] == diff_mesh:\n                return dx\n\n            elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n                mesh = args[0]\n                return diff_mesh.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=dx, **kwargs)\n            elif len(args) > 1:\n                xi = np.atleast_1d(args[0])\n                yi = np.atleast_1d(args[1])\n                i, e = diff_mesh.interpolate(xi, yi, zdata=dx, **kwargs)\n                return i\n            else:\n                err_msg = ""Invalid number of arguments\\n""\n                err_msg += ""Input a valid mesh or coordinates in x,y directions""\n                raise ValueError(err_msg)\n\n        def new_fn_y(*args, **kwargs):\n            dx, dy = self.evaluate(diff_mesh)\n\n            if len(args) == 1 and args[0] == diff_mesh:\n                return dy\n            elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n                mesh = args[0]\n                return diff_mesh.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=dy, **kwargs)\n            elif len(args) > 1:\n                xi = np.atleast_1d(args[0])  # .resize(-1,1)\n                yi = np.atleast_1d(args[1])  # .resize(-1,1)\n                i, e = diff_mesh.interpolate(xi, yi, zdata=dy, **kwargs)\n                return i\n            else:\n                err_msg = ""Invalid number of arguments\\n""\n                err_msg += ""Input a valid mesh or coordinates in x,y directions""\n                raise ValueError(err_msg)\n\n\n        d1 = len(self.description)//2\n        d2 = d1 + 1\n\n        newLazyFn_dx = LazyEvaluation(mesh=diff_mesh)\n        newLazyFn_dx.evaluate = new_fn_x\n        newLazyFn_dx.description = self.description[:d1]\n        newLazyFn_dy = LazyEvaluation(mesh=diff_mesh)\n        newLazyFn_dy.evaluate = new_fn_y\n        newLazyFn_dy.description = self.description[d2:]\n\n        if dirn == 0:\n            return newLazyFn_dx\n        elif dirn == 1:\n            return newLazyFn_dy\n        else:\n            raise ValueError(""dirn can only be set to 0 or 1"")\n\n\n\n## need a fn.coord to extract (x or y) ??\n\n# class variable(LazyEvaluation):\n#     """"""Lazy evaluation of Mesh Variables""""""\n#     def __init__(self, meshVariable):\n#         super(variable, self).__init__()\n#         self._mesh_variable = meshVariable\n#         self.description = meshVariable._name\n#         return\n#\n#     def evaluate(self, *args, coords=None):\n#         return self._mesh_variable.evaluate(*args)\n\nclass parameter(LazyEvaluation):\n    """"""Floating point parameter / coefficient for lazy evaluation of functions""""""\n\n    def __init__(self, value, *args, **kwargs):\n        super(parameter, self).__init__(*args, **kwargs)\n        self.value = value\n        return\n\n\n    @property\n    def fn_gradient(self):\n        mesh = self._mesh\n        newLazyGradient = LazyGradientEvaluation(mesh=mesh)\n        newLazyGradient.evaluate = self.evaluate\n        newLazyGradient._fn_gradient = self._fn_gradient\n        newLazyGradient.description = ""d({0})/dX,d({0})/dY"".format(self.description)\n        newLazyGradient.dependency_list = self.dependency_list\n        return newLazyGradient\n\n    def _fn_gradient(self, dirn=None, mesh=None):\n        """"""Gradients information is not provided by default for lazy evaluation objects:\n           it is necessary to implement the gradient method""""""\n\n        px = parameter(0.0)\n        px.description = ""d({})/dX===0.0"".format(self.description)\n        py = parameter(0.0)\n        py.description = ""d({})/dY===0.0"".format(self.description)\n\n        p = [px, py]\n\n        if dirn is None:\n            # not sure about dimensions here, a parameter is always 1-d so it should\n            # always return the same derivative\n            return px\n        else:\n            return p[dirn]\n\n    def __call__(self, value=None):\n        """"""Set value (X) of this parameter (equivalent to Parameter.value=X)""""""\n        if value is not None:\n            self.value = value\n        return\n\n    def __repr__(self):\n        return(""quagmire lazy evaluation parameter: {}"".format(self._value))\n\n    @property\n    def value(self):\n        return self._value\n\n    @value.setter\n    def value(self, value):\n        self._value = float(value)\n        self.description = ""{}"".format(self._value)\n\n    def evaluate(self, *args, **kwargs):\n\n        if len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n            mesh = args[0]\n            return self.value * np.ones(mesh.npoints)\n\n        elif len(args) >= 1:\n            xi = np.atleast_1d(args[0])\n            yi = np.atleast_1d(args[1])\n\n            return self.value * np.ones_like(xi)\n\n        else:\n            return self.value\n'"
quagmire/function/math.py,30,"b'""""""\nCopyright 2016-2019 Louis Moresi, Ben Mather, Romain Beucher\n\nThis file is part of Quagmire.\n\nQuagmire is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or any later version.\n\nQuagmire is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Lesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport numpy as _np\nfrom .function_classes import LazyEvaluation as _LazyEvaluation\n\n## Functions of a single variable\n\ndef _make_npmath_op(op, name, lazyFn):\n    newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n    newLazyFn.evaluate = lambda *args, **kwargs : op(lazyFn.evaluate(*args, **kwargs))\n    newLazyFn.gradient = lambda *args, **kwargs : op(lazyFn.gradient(*args, **kwargs))\n    newLazyFn.description = ""{}({})"".format(name,lazyFn.description)\n    newLazyFn.dependency_list = lazyFn.dependency_list\n    return newLazyFn\n\n## Trig\n\ndef sin(lazyFn):\n    return _make_npmath_op(_np.sin, ""sin"", lazyFn)\n\ndef asin(lazyFn):\n    return _make_npmath_op(_np.arcsin, ""asin"", lazyFn)\n\ndef arcsin(lazyFn):\n    return _make_npmath_op(_np.arcsin, ""arcsin"", lazyFn)\n\ndef cos(lazyFn):\n    return _make_npmath_op(_np.cos, ""cos"", lazyFn)\n\ndef acos(lazyFn):\n    return _make_npmath_op(_np.arccos, ""acos"", lazyFn)\n\ndef arccos(lazyFn):\n    return _make_npmath_op(_np.arccos, ""arccos"", lazyFn)\n\ndef tan(lazyFn):\n    return _make_npmath_op(_np.tan, ""tan"", lazyFn)\n\ndef atan(lazyFn):\n    return _make_npmath_op(_np.arctan, ""atan"", lazyFn)\n\ndef arctan(lazyFn):\n    return _make_npmath_op(_np.arctan, ""arctan"", lazyFn)\n\n\n## Hyperbolic\n\ndef sinh(lazyFn):\n    return _make_npmath_op(_np.sinh, ""sinh"", lazyFn)\n\ndef asinh(lazyFn):\n    return _make_npmath_op(_np.arcsinh, ""asinh"", lazyFn)\n\ndef arcsinh(lazyFn):\n    return _make_npmath_op(_np.arcsinh, ""arcsinh"", lazyFn)\n\ndef cosh(lazyFn):\n    return _make_npmath_op(_np.cosh, ""cosh"", lazyFn)\n\ndef acosh(lazyFn):\n    return _make_npmath_op(_np.arccosh, ""acosh"", lazyFn)\n\ndef arccosh(lazyFn):\n    return _make_npmath_op(_np.arccosh, ""arccosh"", lazyFn)\n\ndef tanh(lazyFn):\n    return _make_npmath_op(_np.tanh, ""tanh"", lazyFn)\n\ndef atanh(lazyFn):\n    return _make_npmath_op(_np.arctanh, ""atanh"", lazyFn)\n\ndef arctanh(lazyFn):\n    return _make_npmath_op(_np.arctanh, ""arctanh"", lazyFn)\n\n\n## exponentiation\n\ndef exp(lazyFn):\n    return _make_npmath_op(_np.exp, ""exp"", lazyFn)\n\ndef log(lazyFn):\n    return _make_npmath_op(_np.log, ""log"", lazyFn)\n\ndef log10(lazyFn):\n    return _make_npmath_op(_np.log10, ""log10"", lazyFn)\n\n# misc\n\ndef fabs(lazyFn):\n    return _make_npmath_op(_np.fabs, ""fabs"", lazyFn)\n\ndef sqrt(lazyFn):\n    return _make_npmath_op(_np.sqrt, ""sqrt"", lazyFn)\n\ndef degrees(lazyFn):\n    return _make_npmath_op(_np.degrees, ""180/pi*"", lazyFn)\n\ndef radians(lazyFn):\n    return _make_npmath_op(_np.radians, ""pi/180*"", lazyFn)\n\ndef rad2deg(lazyFn):\n    return degrees(lazyFn)\n\ndef deg2rad(lazyFn):\n    return radians(lazyFn)\n\n\n# grad\n\ndef grad(lazyFn):\n    """"""Lazy evaluation of gradient operator on a scalar field""""""\n    return lazyFn.fn_gradient[0], lazyFn.fn_gradient[1]\n\ndef div(*args):\n    """"""Lazy evaluation of divergence operator on a N-D vector field""""""\n    def _div(lazyFn_list, *args, **kwargs):\n        lazy_ev = 0.0\n        for lazyFn in lazyFn_list:\n            lazy_ev += lazyFn.evaluate(*args, **kwargs)\n        return lazy_ev\n\n    dims = [\'X\', \'Y\', \'Z\']\n    lazyFn_id = set()\n    lazyFn_list = []\n    lazyFn_description = """"\n    lazyFn_dependency = set()\n    for f, lazyFn in enumerate(args):\n        lazyFn_id.add(lazyFn._mesh.id)\n        lazyFn_list.append(lazyFn.fn_gradient[f])\n        lazyFn_description += ""diff({},{}) + "".format(lazyFn.description, dims[f])\n        lazyFn_dependency.union(lazyFn.dependency_list)\n    lazyFn_description = lazyFn_description[:-3]\n    if len(lazyFn_id) > 1:\n        raise ValueError(""Meshes must be identical"")\n\n    newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n    newLazyFn.evaluate = lambda *args, **kwargs : _div(lazyFn_list, *args, **kwargs)\n    newLazyFn.description = lazyFn_description\n    newLazyFn.dependency_list = lazyFn_dependency\n\n    return newLazyFn\n\ndef curl(*args):\n    """"""Lazy evaluation of curl operator on a 2D vector field""""""\n\n    if len(args) == 2:\n        lazyFn_x = args[0]\n        lazyFn_y = args[1]\n        if lazyFn_x._mesh.id != lazyFn_y._mesh.id:\n            raise ValueError(""Both meshes must be identical"")\n        \n        newLazyFn = _LazyEvaluation(mesh=lazyFn_x._mesh)\n        fn_dvydx = lazyFn_y.fn_gradient[0]\n        fn_dvxdy = lazyFn_x.fn_gradient[1]\n        newLazyFn.evaluate = lambda *args, **kwargs : fn_dvydx.evaluate(*args, **kwargs) - fn_dvxdy.evaluate(*args, **kwargs)\n        newLazyFn.description = ""diff({},X) - diff({},Y)"".format(lazyFn_y.description, lazyFn_x.description)\n        newLazyFn.dependency_list = lazyFn_x.dependency_list | lazyFn_y.dependency_list\n\n    elif len(args) == 3:\n        lazyFn_x = args[0]\n        lazyFn_y = args[1]\n        lazyFn_z = args[2]\n        if lazyFn_x._mesh.id != lazyFn_y._mesh.id != lazyFn_z._mesh.id :\n            raise ValueError(""All meshes must be identical"")\n        \n        newLazyFn = _LazyEvaluation(mesh=lazyFn_x._mesh)\n        fn_dvxdx, fn_dvxdy, fn_dvxdz = lazyFn_x.fn_gradient\n        fn_dvydx, fn_dvydy, fn_dvydz = lazyFn_y.fn_gradient\n        fn_dvzdx, fn_dvzdy, fn_dvzdz = lazyFn_z.fn_gradient\n        fn_curl = (fn_dvzdy-fn_dvydz) + (fn_dvxdz-fn_dvzdx) + (fn_dvydx-fn_dvxdy)\n        newLazyFn.evaluate = lambda *args, **kwargs : fn_curl.evaluate(*args, **kwargs)\n        desc = ""(diff({},dy) - diff({},dz)) + (diff({},dz) - diff({},dx)) + (diff({},dx) - diff({},dy))""\n        newLazyFn.description = desc.format(\\\n            lazyFn_z.description, lazyFn_y.description, \\\n            lazyFn_x.description, lazyFn_z.description, \\\n            lazyFn_y.description, lazyFn_x.description)\n        newLazyFn.dependency_list = lazyFn_x.dependency_list | lazyFn_y.dependency_list | lazyFn_z.dependency_list\n\n    else:\n        raise ValueError(""Enter a valid number of arguments"")\n\n    return newLazyFn\n\ndef hypot(*args):\n    """"""Lazy evaluation of hypot operator on N fields""""""\n    def _hyp(lazyFn_list, *args, **kwargs):\n        lazy_ev = []\n        for lazyFn in lazyFn_list:\n            lazy_ev.append( lazyFn.evaluate(*args, **kwargs) )\n        return _np.hypot(*lazy_ev)\n    lazyFn_list = []\n    lazyFn_description = """"\n    lazyFn_dependency = set()\n    for lazyFn in args:\n        lazyFn_list.append(lazyFn)\n        lazyFn_description += ""{}^2 + "".format(lazyFn.description)\n        lazyFn_dependency.union(lazyFn.dependency_list)\n    lazyFn_description = lazyFn_description[:-3]\n\n    newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n    newLazyFn.evaluate = lambda *args, **kwargs : _hyp(lazyFn_list, *args, **kwargs)\n    newLazyFn.description = ""sqrt({})"".format(lazyFn_description)\n    newLazyFn.dependency_list = lazyFn_dependency\n\n    return newLazyFn\n\n\ndef arctan2(*args):\n    """"""Lazy evaluation of arctan2 operator on N fields""""""\n    def _arctan2(lazyFn_list, *args, **kwargs):\n        lazy_ev = []\n        for lazyFn in lazyFn_list:\n            lazy_ev.append( lazyFn.evaluate(*args, **kwargs) )\n        return _np.arctan2(*lazy_ev)\n    lazyFn_list = []\n    lazyFn_description = """"\n    lazyFn_dependency = set()\n    for lazyFn in args:\n        lazyFn_list.append(lazyFn)\n        lazyFn_description += ""{},"".format(lazyFn.description)\n        lazyFn_dependency.union(lazyFn.dependency_list)\n    lazyFn_description = lazyFn_description[:-1]\n\n    newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n    newLazyFn.evaluate = lambda *args, **kwargs : _arctan2(lazyFn_list, *args, **kwargs)\n    newLazyFn.description = ""arctan2({})"".format(lazyFn_description)\n    newLazyFn.dependency_list = lazyFn_dependency\n\n    return newLazyFn\n\n\ndef slope(lazyFn):\n    """"""Lazy evaluation of the slope of a scalar field""""""\n    \n    # need to take a few lines from `function_classes` gradient method\n\n    if lazyFn._mesh is None:\n        raise RuntimeError(""fn_gradient is a numerical differentiation routine based on "" + \\\n            ""derivatives of a fitted spline function on a mesh. "" + \\\n            ""The function {} has no associated mesh. "".format(lazyFn.__repr__())) + \\\n            ""To obtain *numerical* derivatives of this function, "" + \\\n            ""you can provide a mesh to the gradient function. "" + \\\n            ""The usual reason for this error is that your function is not based upon "" + \\\n            ""mesh variables and can, perhaps, be differentiated without resort to interpolating splines. ""\n\n    elif lazyFn._mesh is not None:\n        diff_mesh = lazyFn._mesh\n    else:\n        import quagmire\n        quagmire.mesh.check_object_is_a_q_mesh_and_raise(mesh)\n        diff_mesh = mesh\n\n    def new_fn_slope(*args, **kwargs):\n        local_array = lazyFn.evaluate(diff_mesh)\n        df_tuple = diff_mesh._derivative_grad_cartesian(local_array, nit=10, tol=1e-8)\n\n        grad_f = _np.hypot(*df_tuple)/diff_mesh._radius\n\n        if len(args) == 1 and args[0] == diff_mesh:\n            return grad_f\n        elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n            mesh = args[0]\n            return diff_mesh.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=grad_f, **kwargs)\n        elif len(args) > 1:\n            xi = np.atleast_1d(args[0])  # .resize(-1,1)\n            yi = np.atleast_1d(args[1])  # .resize(-1,1)\n            i, e = diff_mesh.interpolate(xi, yi, zdata=grad_f, **kwargs)\n            return i\n        else:\n            err_msg = ""Invalid number of arguments\\n""\n            err_msg += ""Input a valid mesh or coordinates in x,y directions""\n            raise ValueError(err_msg)\n\n    newLazyFn = _LazyEvaluation(mesh=diff_mesh)\n    newLazyFn.evaluate = new_fn_slope\n    newLazyFn.description = ""sqrt(d({0})/dX^2 + d({0})/dY^2)"".format(lazyFn.description)\n    newLazyFn.dependency_list = lazyFn.dependency_list\n    return newLazyFn\n\n## These are not defined yet (LM)\n\n# unwrap(p[, discont, axis])\tUnwrap by changing deltas between values to 2*pi complement.'"
quagmire/function/misc.py,5,"b'""""""\nCopyright 2016-2019 Louis Moresi, Ben Mather, Romain Beucher\n\nThis file is part of Quagmire.\n\nQuagmire is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or any later version.\n\nQuagmire is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Lesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport numpy as _np\nimport quagmire\nfrom .function_classes import LazyEvaluation as _LazyEvaluation\n\n\ndef _make_npmath_op(op, name, lazyFn):\n    newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n    newLazyFn.evaluate = lambda *args, **kwargs : op(lazyFn.evaluate(*args, **kwargs))\n    newLazyFn.gradient = lambda *args, **kwargs : op(lazyFn.gradient(*args, **kwargs))\n    newLazyFn.description = ""{}({})"".format(name, lazyFn.description)\n    newLazyFn.dependency_list = lazyFn.dependency_list\n\n    return newLazyFn\n\n## Trig\n\ndef coord(dirn):\n\n    def extract_xs(*args, **kwargs):\n        """""" If no arguments or the argument is the mesh, return the\n            coords at the nodes. In all other cases pass through the\n            coordinates given """"""\n\n        if len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh(args[0]):\n            mesh = args[0]\n            return mesh.coords[:,0]\n        else:\n            return args[0]\n\n    def extract_ys(*args, **kwargs):\n        """""" If no arguments or the argument is the mesh, return the\n            coords at the nodes. In all other cases pass through the\n            coordinates given """"""\n\n        if len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh(args[0]):\n            mesh = args[0]\n            return mesh.coords[:,1]\n        else:\n            return args[1]\n\n\n    newLazyFn_xs = _LazyEvaluation(mesh=None)\n    newLazyFn_xs.evaluate = extract_xs\n    newLazyFn_xs.description = ""X""\n\n    newLazyFn_ys = _LazyEvaluation(mesh=None)\n    newLazyFn_ys.evaluate = extract_ys\n    newLazyFn_ys.description = ""Y""\n\n    if dirn == 0:\n        return newLazyFn_xs\n    else:\n        return newLazyFn_ys\n\n\ndef levelset(lazyFn, alpha=0.5, invert=False):\n\n    newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n\n    def threshold(*args, **kwargs):\n        if not invert:\n            values = (lazyFn.evaluate(*args, **kwargs) > alpha).astype(float)\n        else:\n            values = (lazyFn.evaluate(*args, **kwargs) < alpha).astype(float)\n\n        return values\n\n    newLazyFn.evaluate = threshold\n    newLazyFn.dependency_list = lazyFn.dependency_list\n    if not invert:\n        newLazyFn.description = ""(level({}) > {}"".format(lazyFn.description, alpha)\n    else:\n        newLazyFn.description = ""(level({}) < {}"".format(lazyFn.description, alpha)\n\n    return newLazyFn\n\n\ndef maskNaN(lazyFn, invert=False):\n\n    newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n\n    def threshold(*args, **kwargs):\n        if invert:\n            values = _np.isnan(lazyFn.evaluate(*args, **kwargs)).astype(float)\n        else:\n            values = _np.logical_not(_np.isnan(lazyFn.evaluate(*args, **kwargs))).astype(float)\n\n        return values\n\n    newLazyFn.evaluate = threshold\n    newLazyFn.dependency_list = lazyFn.dependency_list\n\n    if invert:\n        newLazyFn.description = ""isNaN({})"".format(lazyFn.description)\n    else:\n        newLazyFn.description = ""notNan({})"".format(lazyFn.description)\n\n    return newLazyFn\n\n\ndef replaceNan(lazyFn1, lazyFn2):\n\n    def replaceNan_nodebynode(*args, **kwargs):\n\n        values1     = lazyFn1.evaluate(*args, **kwargs)\n        values2     = lazyFn2.evaluate(*args, **kwargs)  # Note where we evaluate !\n        mask_values = _np.isnan(*args, **kwargs)\n        replaced_values  = _np.where(mask_values, values2, values1)\n        return replaced_values\n\n    newLazyFn = _LazyEvaluation(mesh=lazyFn1._mesh)\n    newLazyFn.evaluate = replaceNan_nodebynode\n    newLazyFn.description = ""Nan([{}]<-[{}])"".format(lazyFn1.description, lazyFn2.description)\n    newLazyFn.dependency_list = lazyFn1.dependency_list | lazyFn2.dependency_list\n\n    return newLazyFn\n\n\ndef where(maskFn, lazyFn1, lazyFn2):\n\n    def mask_nodebynode(*args, **kwargs):\n        values1     = lazyFn1.evaluate(*args, **kwargs)\n        values2     = lazyFn2.evaluate(*args, **kwargs)\n        mask_values = maskFn.evaluate(*args, **kwargs)\n\n        replaced_values  = _np.where(mask_values < 0.5, values1, values2)\n        return replaced_values\n\n    newLazyFn = _LazyEvaluation(mesh=lazyFn1._mesh)\n    newLazyFn.evaluate = mask_nodebynode\n    newLazyFn.description = ""where({}: [{}]<-[{}])"".format(maskFn.description, lazyFn1.description, lazyFn2.description)\n    newLazyFn.dependency_list = maskFn.dependency_list |     lazyFn1.dependency_list | lazyFn2.dependency_list\n\n\n    return newLazyFn\n'"
quagmire/function/stats.py,4,"b'""""""\nCopyright 2016-2019 Louis Moresi, Ben Mather, Romain Beucher\n\nThis file is part of Quagmire.\n\nQuagmire is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or any later version.\n\nQuagmire is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Lesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License\nalong with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport quagmire\nimport numpy as _np\nfrom .function_classes import LazyEvaluation as _LazyEvaluation\nfrom mpi4py import MPI as _MPI\n_comm = _MPI.COMM_WORLD\n\nsupported_operations = {""MIN"" : (_np.min, _MPI.MIN), \\\n                        ""MAX"" : (_np.max, _MPI.MAX), \\\n                        ""SUM"" : (_np.sum, _MPI.SUM)}\n\ndef _get_array_size(lazyFn):\n    return lazyFn.evaluate().size\n\ndef _all_reduce(array, name):\n\n    np_reduce, mpi_reduce = supported_operations[name]\n\n    # We should check if it is a mesh variable first\n    # and use those in-built variables\n\n    larray = _np.array(np_reduce(array))\n    garray = larray.copy()\n    _comm.Allreduce([larray, _MPI.DOUBLE], [garray, _MPI.DOUBLE], op=mpi_reduce)\n    return garray\n\ndef _make_reduce_op(name, lazyFn):\n    newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n    newLazyFn.evaluate = lambda *args, **kwargs : _all_reduce(lazyFn.evaluate(*args, **kwargs), name)\n    newLazyFn.gradient = lambda *args, **kwargs : _all_reduce(lazyFn.gradient(*args, **kwargs), name)\n    newLazyFn.description = ""{}({})"".format(name, lazyFn.description)\n    newLazyFn.dependency_list = lazyFn.dependency_list\n    return newLazyFn\n\n\n## global-safe reduction operations\n\ndef min(lazyFn, axis=None):\n    return _make_reduce_op(""MIN"", lazyFn)\n\ndef max(lazyFn, axis=None):\n    return _make_reduce_op(""MAX"", lazyFn)\n\ndef sum(lazyFn, axis=None):\n    return _make_reduce_op(""SUM"", lazyFn)\n\ndef mean(lazyFn, axis=None):\n    from quagmire import function as fn\n    size = _get_array_size(lazyFn)\n    return sum(lazyFn) / fn.parameter(size)\n\ndef std(lazyFn, axis=None):\n    from quagmire import function as fn\n    size = _get_array_size(lazyFn)\n    return fn.math.sqrt(sum((lazyFn - mean(lazyFn))**2)) / fn.parameter(size)\n\n'"
quagmire/mesh/__init__.py,0,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nThe mesh module provides 3 fundamental spatial data structures\n\n<img src=""https://raw.githubusercontent.com/underworldcode/quagmire/dev/docs/images/quagmire-flowchart-flatmesh.png"" style=""width: 321px; float:right"">\n\n- `pixmesh.PixMesh`: for structured data on a regular grid.\n- `trimesh.TriMesh`: for unstructured data in Cartesian coordinates.\n- `strimesh.sTriMesh`: for unstructured data on the sphere.\n\nEach of these data structures are built on top of a `PETSc DM` object\n(created from `quagmire.tools.meshtools`) and implement the general functionality of:\n\n- calculating spatial derivatives\n- identifying node neighbour relationships\n- interpolation / extrapolation\n- smoothing operators\n- importing and saving mesh information\n\n""""""\n\nfrom .trimesh import TriMesh\nfrom .pixmesh import PixMesh\nfrom .strimesh import sTriMesh\nfrom .basemesh import MeshVariable\nfrom .basemesh import VectorMeshVariable\n\n\ndef check_object_is_a_q_mesh(mesh_object):\n    """"""\n    Is this object a `quagmire.mesh` of some kind?\n\n    Parameters\n    ----------\n    mesh_object : object\n        Checks if one of `trimesh.TriMesh`, `pixmesh.PixMesh`, `strimesh.sTriMesh`\n\n    Returns\n    -------\n    statement : bool\n        True or False\n    """"""\n\n    return isinstance(mesh_object, (TriMesh, PixMesh, sTriMesh))\n\ndef check_object_is_a_q_mesh_and_raise(mesh_object):\n    """"""\n    If this object is not a `quagmire.mesh` then raises a RuntimeError\n\n    Parameters\n    ----------\n    mesh_object : object\n        Checks if one of `trimesh.TriMesh`, `pixmesh.PixMesh`, `strimesh.sTriMesh`\n\n    Returns\n    -------\n    statement : bool\n        True or False\n    """"""\n\n    if not check_object_is_a_q_mesh(mesh_object):\n        raise RuntimeError(""Expecting a quagmire.mesh object"")\n\n    return True\n'"
quagmire/mesh/basemesh.py,7,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nCreate mesh variables that interface with functions\n\nInitialise a `MeshVariable` for scalar fields or `VectorMeshVariable` for vectors\non the mesh. These inherit the `quagmire.function` classes that exploit lazy evaluation.\n""""""\n\ntry: range = xrange\nexcept: pass\n\n\n## These also need to be lazy evaluation objects\n\nfrom ..function import LazyEvaluation as _LazyEvaluation\n\n\n\nclass MeshVariable(_LazyEvaluation):\n    """"""\n    The MeshVariable class generates a variable supported on the mesh.\n\n    To set/read nodal values, use the numpy interface via the \'self.data\' property.\n\n    Parameters\n    ----------\n    name : str\n        Assign the MeshVariable a unique identifier\n    mesh : quagmire mesh object\n        The supporting mesh for the variable\n    """"""\n    def __init__(self, name=None, mesh=None, locked=False):\n        super(MeshVariable, self).__init__()\n\n        self._mesh = mesh\n        self._dm = mesh.dm\n        self._name = str(name)\n        self.description = self._name\n        self._locked = locked\n        self.mesh_data = True\n\n        # mesh variable vector\n        self._ldata = self._dm.createLocalVector()\n        self._ldata.setName(name)\n        return\n\n    def copy(self, name=None, locked=None):\n        """"""\n        Create a copy of this MeshVariable\n\n        Parameters\n        ----------\n        name : str\n            set a name to this MeshVariable, otherwise ""copy""\n            is appended to the original name\n        locked : bool (default: False)\n            lock the mesh variable from accidental modification\n\n        Returns\n        -------\n        MeshVariable : object\n            Instantiate a MeshVariable copy.\n        """"""\n\n        if name is None:\n            name = self._name+""_copy""\n\n        if locked is None:\n            locked = self._locked\n\n        new_mesh_variable = MeshVariable(name=name, mesh=self._mesh, locked=False)\n        new_mesh_variable.data = self.data\n\n        if locked:\n            new_mesh_variable.lock()\n\n        return new_mesh_variable\n\n    def lock(self):\n        self._locked = True\n\n    def unlock(self):\n        self._locked = False\n\n## This is a redundancy - @property definition is nuked by the @ .getter\n## LM: See this: https://stackoverflow.com/questions/51244348/use-of-propertys-getter-in-python\n\n## Don\'t sync on get / set as this prevents doing a series of computations on the array and\n## doing the sync when finished. I can also imagine this going wrong if sync nukes values\n## in the shadow zone unexpectedly. Also get is called for any indexing operation ...  ugh !\n\n    @property\n    def data(self):\n        """""" View of MeshVariable data of shape (n,) """"""\n        pass\n\n    @data.getter\n    def data(self):\n        # This step is necessary because the numpy array is writeable\n        # through to the petsc vector and this cannot be blocked.\n        # Access to the numpy array will not automatically be sync\'d and this\n        # would also be a way to circumvent access to locked arrays - where such\n        # locking is intended to ensure we update dependent data when the variable is\n        # updated\n\n# if self._locked:\n        view = self._ldata.array[:]\n        view.setflags(write=False)\n        return view\n\n#        else:\n#            return self._ldata.array\n\n    @data.setter\n    def data(self, val):\n        if self._locked:\n            import quagmire\n            if quagmire.mpi_rank == 0:\n                print(""quagmire.MeshVariable: {} - is locked"".format(self.description))\n            return\n\n        if type(val) is float:\n                self._ldata.set(val)\n        else:\n            from petsc4py import PETSc\n            self._ldata.setArray(val)\n\n\n\n    ## For printing and other introspection we actually want to look through to the\n    ## meshVariable\'s own description\n\n    def __repr__(self):\n        if self._locked:\n            return ""quagmire.MeshVariable: {} - RO"".format(self.description)\n        else:\n            return ""quagmire.MeshVariable: {} - RW"".format(self.description)\n\n\n    def getGlobalVector(self, gdata):\n        """"""\n        Obtain a PETSc global vector of the MeshVariable\n        """"""\n        from petsc4py import PETSc\n        self._dm.localToGlobal(self._ldata, gdata, addv=PETSc.InsertMode.INSERT_VALUES)\n        return\n\n\n    def sync(self, mergeShadow=False):\n        """"""\n        Explicit global sync of data\n        """"""\n\n        from petsc4py import PETSc\n\n        if mergeShadow:\n            addv = PETSc.InsertMode.ADD_VALUES\n        else:\n            addv = PETSc.InsertMode.INSERT_VALUES\n\n        # need a global vector\n        gdata = self._dm.getGlobalVec()\n\n        # self._dm.localToLocal(self._ldata, self._gdata)\n        self._dm.localToGlobal(self._ldata, gdata, addv=addv)\n        self._dm.globalToLocal(gdata, self._ldata)\n\n        self._dm.restoreGlobalVec(gdata)\n\n        return\n\n\n    def xdmf(self, hdf5_filename, hdf5_mesh_filename=None, xdmf_filename=None):\n        """"""\n        Creates an xdmf file associating the saved HDF5 file with the mesh\n\n        """"""\n        from quagmire.tools.generate_xdmf import generateXdmf\n        from quagmire import mpi_rank\n\n        hdf5_filename = str(hdf5_filename)\n        if not hdf5_filename.endswith(\'.h5\'):\n            hdf5_filename += \'.h5\'\n\n        if mpi_rank == 0:\n            generateXdmf(hdf5_filename, hdf5_mesh_filename, xdmf_filename)\n        return\n\n\n    def save(self, hdf5_filename, append=True):\n        """"""\n        Save the MeshVariable to disk.\n\n        Parameters\n        ----------\n        hdf5_filename : str (optional)\n            The name of the output file. Relative or absolute paths may be\n            used, but all directories must exist.\n        append : bool (default is True)\n            Append to existing file if it exists\n\n        Notes\n        -----\n        This method must be called collectively by all processes.\n        """"""\n        from petsc4py import PETSc\n        import os\n\n        hdf5_filename = str(hdf5_filename)\n        if not hdf5_filename.endswith(\'.h5\'):\n            hdf5_filename += \'.h5\'\n\n        if self._mesh.id.startswith(""pixmesh""):\n            # save mesh info to the hdf5 if DMDA object\n            # this is so tiny that I/O hit is negligible\n            self._mesh.save_mesh_to_hdf5(hdf5_filename)\n\n        mode = ""w""\n        if append and os.path.exists(hdf5_filename):\n            mode = ""a""\n\n        # need a global vector\n        vname = self._ldata.getName()\n        gdata = self._dm.getGlobalVec()\n        gdata.setName(vname)\n        self._dm.localToGlobal(self._ldata, gdata)\n\n        ViewHDF5 = PETSc.Viewer()\n        ViewHDF5.createHDF5(hdf5_filename, mode=mode)\n        ViewHDF5(gdata)\n        ViewHDF5.destroy()\n        ViewHDF5 = None\n\n        self._dm.restoreGlobalVec(gdata)\n\n        return\n\n    def load(self, hdf5_filename):\n        """"""\n        Load the MeshVariable from disk.\n\n        Parameters\n        ----------\n        hdf5_filename: str\n            The filename for the saved file. Relative or absolute paths may be\n            used, but all directories must exist.\n\n        Notes\n        -----\n        Provided files must be in hdf5 format, and contain a vector the same\n        size and with the same name as the current MeshVariable\n        """"""\n        from petsc4py import PETSc\n        # need a global vector\n        gdata = self._dm.getGlobalVec()\n        gdata.setName(self._ldata.getName())\n\n        ViewHDF5 = PETSc.Viewer()\n        ViewHDF5.createHDF5(str(hdf5_filename), mode=\'r\')\n        gdata.load(ViewHDF5)\n        ViewHDF5.destroy()\n        ViewHDF5 = None\n\n        self._dm.globalToLocal(gdata, self._ldata)\n        self._dm.restoreGlobalVec(gdata)\n\n\n    def gradient(self, nit=10, tol=1e-8):\n        """"""\n        Compute values of the derivatives of PHI in the x, y directions at the nodal points.\n        This routine uses SRFPACK to compute derivatives on a C-1 bivariate function.\n\n        Parameters\n        ----------\n        PHI : ndarray of floats, shape (n,)\n            compute the derivative of this array\n        nit : int optional (default: 10)\n            number of iterations to reach convergence\n        tol : float optional (default: 1e-8)\n            convergence is reached when this tolerance is met\n        Returns\n        -------\n        PHIx : ndarray of floats, shape (n,)\n            first partial derivative of PHI in x direction\n        PHIy : ndarray of floats, shape (n,)\n            first partial derivative of PHI in y direction\n        """"""\n\n        dx, dy = self._mesh.derivative_grad(self._ldata.array, nit, tol)\n\n        return dx, dy\n\n\n    def gradient_patch(self):\n        """"""\n        Compute values of the derivatives of PHI in the x, y directions at the nodal points.\n        This routine uses SRFPACK to compute derivatives on a C-1 bivariate function.\n\n        Parameters\n        ----------\n        PHI : ndarray of floats, shape (n,)\n            compute the derivative of this array\n\n        Returns\n        -------\n        PHIx : ndarray of floats, shape(n,)\n            first partial derivative of PHI in x direction\n        PHIy : ndarray of floats, shape(n,)\n            first partial derivative of PHI in y direction\n        """"""\n\n        def bf_gradient_node(node):\n\n            xx = self.coords[node,0]\n            yy = self.coords[node,1]\n\n            from scipy.optimize import curve_fit\n\n            def linear_fit_2D(X, a, b, c):\n                # (1+x) * (1+y) etc\n                x,y = X\n                fit = a + b * x + c * y\n                return fit\n\n            location = np.array([xx,yy]).T\n\n            ## Just try near neighbours ?\n            stencil_size=mesh.near_neighbours[node]\n\n\n            d, patch_points = self.cKDTree.query(location, k=stencil_size)\n            x,y = self.coords[patch_points].T\n            data = temperature.evaluate(x, y)\n            popt, pcov = curve_fit(linear_fit_2D, (x,y), data)\n            ddx = popt[1]\n            ddy = popt[2]\n\n            return(ddx, ddy)\n\n\n        dx = np.empty(self.npoints)\n\n        dx, dy = self._mesh.derivative_grad(self._ldata.array, nit, tol)\n\n        return dx, dy\n\n\n\n    def interpolate(self, xi, yi, err=False, **kwargs):\n        """"""\n        Interpolate mesh data to a set of coordinates\n\n        This method just passes the coordinates to the interpolation\n        methods on the mesh object.\n\n        Parameters\n        ----------\n        xi : array of floats, shape (l,)\n            interpolation coordinates in the x direction\n        yi : array of floats, shape (l,)\n            interpolation coordinates in the y direction\n        err : bool (default: False)\n            toggle whether to return error information\n        **kwargs : keyword arguments\n            optional arguments to pass through to the interpolation method\n\n        Returns\n        -------\n        interp : array of floats, shape (l,)\n            interpolated values of MeshVariable at xi,yi coordinates\n        err : array of ints, shape (l,)\n            error information to diagnose interpolation / extrapolation\n        """"""\n        ## pass through for the mesh\'s interpolate method\n        import numpy as np\n\n        mesh = self._mesh\n        PHI = self._ldata.array\n        xi_array = np.array(xi).reshape(-1,1)\n        yi_array = np.array(yi).reshape(-1,1)\n\n        i, e = mesh.interpolate(xi_array, yi_array, zdata=PHI, **kwargs)\n\n        if err:\n            return i, e\n        else:\n            return i\n\n\n    def evaluate(self, *args, **kwargs):\n        """"""\n        If the argument is a mesh, return the values at the nodes.\n        In all other cases call the `interpolate` method.\n        """"""\n\n        import quagmire\n\n        if len(args) == 1 and args[0] == self._mesh:\n            return self._ldata.array\n        elif len(args) == 1 and isinstance(args[0], (quagmire.mesh.trimesh.TriMesh, quagmire.mesh.pixmesh.PixMesh) ):\n            mesh = args[0]\n            return self.interpolate(mesh.coords[:,0], mesh.coords[:,1], **kwargs)\n        else:\n            return self.interpolate(*args, **kwargs)\n\n\n\n    ## Basic global operations provided by petsc4py\n\n    def max(self):\n        """""" Retrieve the maximum value """"""\n        gdata = self._dm.getGlobalVec()\n        self._dm.localToGlobal(self._ldata, gdata)\n        idx, val = gdata.max()\n        return val\n\n    def min(self):\n        """""" Retrieve the minimum value """"""\n        gdata = self._dm.getGlobalVec()\n        self._dm.localToGlobal(self._ldata, gdata)\n        idx, val = gdata.min()\n        return val\n\n    def sum(self):\n        """""" Calculate the sum of all entries """"""\n        gdata = self._dm.getGlobalVec()\n        self._dm.localToGlobal(self._ldata, gdata)\n        return gdata.sum()\n\n    def mean(self):\n        """""" Calculate the mean value """"""\n        gdata = self._dm.getGlobalVec()\n        size = gdata.getSize()\n        self._dm.localToGlobal(self._ldata, gdata)\n        return gdata.sum()/size\n\n    def std(self):\n        """""" Calculate the standard deviation """"""\n        from math import sqrt\n        gdata = self._dm.getGlobalVec()\n        size = gdata.getSize()\n        self._dm.localToGlobal(self._ldata, gdata)\n        mu = gdata.sum()/size\n        gdata -= mu\n        return sqrt((gdata.sum())**2) / size\n\n\nclass VectorMeshVariable(MeshVariable):\n    """"""\n    The VectorMeshVariable class generates a vector variable supported on the mesh.\n\n    To set/read nodal values, use the numpy interface via the \'self.data\' property.\n\n    Parameters\n    ----------\n    name : str\n        Assign the MeshVariable a unique identifier\n    mesh : quagmire mesh object\n        The supporting mesh for the variable\n\n    Notes\n    -----\n    This class inherits several methods from the `MeshVariable` class.\n    """"""\n    def __init__(self, name, mesh):\n        self._mesh = mesh\n        self._dm = mesh.dm.getCoordinateDM()\n\n        name = str(name)\n\n        # mesh variable vector\n        self._ldata = self._dm.createLocalVector()\n        self._ldata.setName(name)\n        return\n\n    @property\n    def data(self):\n        """""" View of MeshVariable data of shape (n,) """"""\n        pass\n\n    @data.getter\n    def data(self):\n        return self._ldata.array.reshape(-1,2)\n\n    @data.setter\n    def data(self, val):\n        import numpy as np\n        if type(val) is float:\n            self._ldata.set(val)\n        elif np.shape(val) == (self._mesh.npoints,2):\n            self._ldata.setArray(np.ravel(val))\n        else:\n            raise ValueError(""NumPy array must be of shape ({},{})"".format(self._mesh.npoints,2))\n\n    def gradient(self):\n        raise TypeError(""VectorMeshVariable does not currently support gradient operations"")\n\n    def interpolate(self, xi, yi, err=False, **kwargs):\n        raise TypeError(""VectorMeshVariable does not currently support interpolate operations"")\n\n    def evaluate(self, xi, yi, err=False, **kwargs):\n        """"""\n        If the argument is a mesh, return the values at the nodes.\n        In all other cases call the `interpolate` method.\n        """"""\n        return self.interpolate(*args, **kwargs)\n\n\n    def norm(self, axis=1):\n        """""" evaluate the normal vector of the data along the specified axis """"""\n        import numpy as np\n        return np.linalg.norm(self.data, axis=axis)\n\n\n    # We should wait to do this one for global operations\n'"
quagmire/mesh/commonmesh.py,4,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nRoutines common to all mesh types.\n\n<img src=""https://raw.githubusercontent.com/underworldcode/quagmire/dev/docs/images/quagmire-flowchart-commonmesh.png"" style=""width: 321px; float:right"">\n\n`CommonMesh` implements the following functionality:\n\n- creating Quagmire mesh variables\n- setting and reading node labels on the PETSc DM\n- saving the mesh and mesh variables to HDF5 files\n- handling global and local synchronisation operations\n\nSupply a `PETSc DM` object (created from `quagmire.tools.meshtools`) to initialise the object.\n""""""\n\nimport numpy as np\nfrom mpi4py import MPI\nimport sys,petsc4py\npetsc4py.init(sys.argv)\nfrom petsc4py import PETSc\n# comm = MPI.COMM_WORLD\nfrom time import perf_counter\n\ntry: range = xrange\nexcept: pass\n\n\nclass CommonMesh(object):\n    """"""\n    Build routines on top of a PETSc DM mesh object common to:\n\n    - `quagmire.mesh.pixmesh.PixMesh`\n    - `quagmire.mesh.trimesh.TriMesh`\n    - `quagmire.mesh.strimesh.sTriMesh`\n\n    The above classes inherit `CommonMesh` to:\n\n    - create `quagmire.mesh.basemesh.MeshVariable` objects\n    - save the mesh and mesh variables to HDF5 files\n    - retrieving and setting labels on the DM\n    - synchronise local mesh information to all processors\n\n    Parameters\n    ----------\n    DM : PETSc DM object\n        Build this mesh object using one of the functions in\n        `quagmire.tools.meshtools`\n    verbose : bool\n        Flag toggles verbose output\n    *args : optional arguments\n    **kwargs : optional keyword arguments\n\n    Attributes\n    ----------\n    dm : PETSc DM object\n        structured Cartesian grid or unstructured Cartesian/\n        spherical mesh object\n    log : PETSc log object\n        contains logs for performance benchmarks\n    gvec : PETSc global vector\n        used to synchronise vectors across multiple processors\n    lvec : PETSc local vector\n        used to synchromise local information to the global vector\n    sizes : tuple\n        size of the local and global domains\n    comm : object\n        MPI COMM object for controlling global communications\n    rank : int\n        COMM rank is hte number assigned to each processor\n    """"""\n\n    def __init__(self, dm, verbose=True,  *args, **kwargs):\n        self.mesh_type = \'FlatMesh\'\n\n        self.timings = dict() # store times\n\n        self.log = PETSc.Log()\n        self.log.begin()\n\n        self.verbose = verbose\n\n        self.dm = dm\n        self.gvec = dm.createGlobalVector()\n        self.lvec = dm.createLocalVector()\n        self.sect = dm.getDefaultSection()\n        self.sizes = self.gvec.getSizes(), self.gvec.getSizes()\n\n        self.comm = self.dm.comm\n        self.rank = self.dm.comm.rank\n\n        lgmap_r = dm.getLGMap()\n        l2g = lgmap_r.indices.copy()\n        offproc = l2g < 0\n\n        l2g[offproc] = -(l2g[offproc] + 1)\n        lgmap_c = PETSc.LGMap().create(l2g, comm=self.dm.comm)\n\n        self.lgmap_row = lgmap_r\n        self.lgmap_col = lgmap_c\n\n\n        return\n\n    def add_variable(self, name=None, locked=False):\n        """"""\n        Create a Quagmire mesh variable.\n\n        Parameters\n        ----------\n        name : str\n            name for the mesh variable\n        locked : bool (default: False)\n            lock the mesh variable from accidental modification\n\n        Returns\n        -------\n        MeshVariable : object\n            Instantiate a `quagmire.mesh.basemesh.MeshVariable`.\n        """"""\n        from quagmire.mesh import MeshVariable\n        return MeshVariable(name=name, mesh=self, locked=locked)\n\n    def get_label(self, label):\n        """"""\n        Retrieves all points in the DM that is marked with a specific label.\n        e.g. ""boundary"", ""coarse""\n\n        Parameters\n        ----------\n        label : str\n            retrieve indices on the DM marked with `label`.\n\n        Returns\n        -------\n        indices : list of ints\n            list of indices corresponding to the label\n        """"""\n        pStart, pEnd = self.dm.getDepthStratum(0)\n\n\n        labels = []\n        for i in range(self.dm.getNumLabels()):\n            labels.append(self.dm.getLabelName(i))\n\n        if label not in labels:\n            raise ValueError(""There is no {} label in the DM"".format(label))\n\n\n        stratSize = self.dm.getStratumSize(label, 1)\n        if stratSize > 0:\n            labelIS = self.dm.getStratumIS(label, 1)\n            pt_range = np.logical_and(labelIS.indices >= pStart, labelIS.indices < pEnd)\n            indices = labelIS.indices[pt_range] - pStart\n        else:\n            indices = np.zeros((0,), dtype=np.int)\n\n        return indices\n\n\n\n    def set_label(self, label, indices):\n        """"""\n        Marks local indices in the DM with a label.\n\n        Parameters\n        ----------\n        label : str\n            mark indices on the DM with `label`.\n        indices : list of ints\n            indices on the DM\n        """"""\n        pStart, pEnd = self.dm.getDepthStratum(0)\n        indices += pStart\n\n        labels = []\n        for i in range(self.dm.getNumLabels()):\n            labels.append(self.dm.getLabelName(i))\n\n        if label not in labels:\n            self.dm.createLabel(label)\n        for ind in indices:\n            self.dm.setLabelValue(label, ind, 1)\n        return\n\n\n    def get_boundary(self, marker=""boundary""):\n        """"""\n        Find the nodes on the boundary from the DM\n        If marker does not exist then the convex hull is used.\n\n        Parameters\n        ----------\n        marker : str (default: \'boundary\')\n            name of the boundary label\n        \n        Returns\n        -------\n        mask : array of bools, shape (n,)\n            mask of interior nodes\n        """"""\n\n        pStart, pEnd = self.dm.getDepthStratum(0)\n        bmask = np.ones(self.npoints, dtype=bool)\n\n\n        try:\n            boundary_indices = self.get_label(marker)\n\n        except ValueError:\n            self.dm.markBoundaryFaces(marker) # marks line segments\n            boundary_indices = self.tri.convex_hull()\n            for ind in boundary_indices:\n                self.dm.setLabelValue(marker, ind + pStart, 1)\n\n\n        bmask[boundary_indices] = False\n        return bmask\n\n\n    def save_quagmire_project(self, file):\n\n        import h5py\n        from mpi4py import MPI\n        comm = MPI.COMM_WORLD\n\n        file = str(file)\n        if not file.endswith(\'.h5\'):\n            file += \'.h5\'\n\n        # first save the mesh\n        self.save_mesh_to_hdf5(file)\n\n        # then save the topography\n        self.topography.save(file, append=True)\n\n        # handle saving radius to file for spherical mesh\n        if np.array(self._radius).size == self.npoints:\n            radius_meshVariable = self.add_variable(\'radius\')\n            radius_meshVariable.data = self._radius\n            radius_meshVariable.save(file, append=True)\n            radius = False\n        else:\n            radius = self._radius\n\n        # now save important parameters we need to reconstruct\n        # data structures. For this we need to crack open the HDF5\n        # file we just saved and write attributes on a \'quagmire\' group\n\n        with h5py.File(file, mode=\'r+\', driver=\'mpio\', comm=comm) as h5:\n            quag = h5.create_group(\'quagmire\')\n            quag.attrs[\'id\']                  = self.id\n            quag.attrs[\'verbose\']             = self.verbose\n            quag.attrs[\'radius\']              = radius\n            quag.attrs[\'downhill_neighbours\'] = self.downhill_neighbours\n            quag.attrs[\'topography_modified\'] = self._topography_modified_count\n\n        return\n\n    def xdmf(self, hdf5_filename, xdmf_filename=None):\n        """"""\n        Creates an xdmf file associating the saved HDF5 file with the mesh\n\n        If no xdmf filename is not provided, a xdmf file is generated from\n        the hdf5 filename with a trailing `.xdmf` extension.\n        """"""\n        from quagmire.tools.generate_xdmf import generateXdmf\n        from quagmire import mpi_rank\n\n        hdf5_filename = str(hdf5_filename)\n        if not hdf5_filename.endswith(\'.h5\'):\n            hdf5_filename += \'.h5\'\n\n        if mpi_rank == 0:\n            generateXdmf(hdf5_filename, xdmfFilename=xdmf_filename)\n        return\n\n\n    def save(self, hdf5_filename):\n        return self.save_quagmire_project(hdf5_filename)\n\n\n    def save_mesh_to_hdf5(self, hdf5_filename):\n        """"""\n        Saves mesh information stored in the DM to HDF5 filename\n        If the filename already exists, it is overwritten.\n\n        Parameters\n        ----------\n        filename : str\n            Save the mesh to an HDF5 filename with this name\n        """"""\n        hdf5_filename = str(hdf5_filename)\n        if not hdf5_filename.endswith(\'.h5\'):\n            hdf5_filename += \'.h5\'\n\n        ViewHDF5 = PETSc.Viewer()\n        ViewHDF5.createHDF5(hdf5_filename, mode=\'w\')\n        ViewHDF5.view(obj=self.dm)\n        ViewHDF5.destroy()\n        ViewHDF5 = None\n\n\n        if self.id.startswith(""pixmesh""):\n            import h5py\n            from mpi4py import MPI\n            comm = MPI.COMM_WORLD\n\n            (minX, maxX), (minY, maxY) = self.dm.getBoundingBox()\n            resX, resY = self.dm.getSizes()\n\n            with h5py.File(hdf5_filename, mode=\'r+\', driver=\'mpio\', comm=comm) as h5:\n                geom = h5.create_group(\'geometry\')\n                geom.attrs[\'minX\'] = minX\n                geom.attrs[\'maxX\'] = maxX\n                geom.attrs[\'minY\'] = minY\n                geom.attrs[\'maxY\'] = maxY\n                geom.attrs[\'resX\'] = resX\n                geom.attrs[\'resY\'] = resY\n\n\n    def save_field_to_hdf5(self, hdf5_filename, *args, **kwargs):\n        """"""\n        Saves data on the mesh to an HDF5 file\n        e.g. height, rainfall, sea level, etc.\n\n        Pass these as arguments or keyword arguments for\n        their names to be saved to the hdf5 file\n\n        Parameters\n        ----------\n        hdf5_filename : str\n            Save the mesh variables to an HDF5 file with this name\n        *args : arguments\n        **kwargs : keyword arguments\n        """"""\n        import os.path\n\n        hdf5_filename = str(hdf5_filename)\n        if not hdf5_filename.endswith(\'.h5\'):\n            hdf5_filename += \'.h5\'\n\n        kwdict = kwargs\n        for i, arg in enumerate(args):\n            key = ""arr_{}"".format(i)\n            if key in list(kwdict.keys()):\n                raise ValueError(""Cannot use un-named variables\\\n                                  and keyword: {}"".format(key))\n            kwdict[key] = arg\n\n        vec = self.dm.createGlobalVec()\n\n        if os.path.isfile(hdf5_filename):\n            mode = ""a""\n        else:\n            mode = ""w""\n\n\n        for key in kwdict:\n            val = kwdict[key]\n            try:\n                vec.setArray(val)\n            except:\n                self.lvec.setArray(val)\n                self.dm.localToGlobal(self.lvec, vec)\n\n            vec.setName(key)\n            if self.rank == 0 and self.verbose:\n                print(""Saving {} to hdf5"".format(key))\n\n            ViewHDF5 = PETSc.Viewer()\n            ViewHDF5.createHDF5(hdf5_filename, mode=mode)\n            ViewHDF5.view(obj=vec)\n            ViewHDF5.destroy()\n            mode = ""a""\n\n        vec.destroy()\n        vec = None\n\n\n    def _gather_root(self):\n        """"""\n        MPI gather operation to root process\n        """"""\n        self.tozero, self.zvec = PETSc.Scatter.toZero(self.gvec)\n\n\n        # Gather x,y points\n        pts = self.tri.points\n        self.lvec.setArray(pts[:,0])\n        self.dm.localToGlobal(self.lvec, self.gvec)\n        self.tozero.scatter(self.gvec, self.zvec)\n\n        self.root_x = self.zvec.array.copy()\n\n        self.lvec.setArray(pts[:,1])\n        self.dm.localToGlobal(self.lvec, self.gvec)\n        self.tozero.scatter(self.gvec, self.zvec)\n\n        self.root_y = self.zvec.array.copy()\n\n        self.root = True # yes we have gathered everything\n\n\n    def gather_data(self, data):\n        """"""\n        Gather data on root process\n        """"""\n\n        # check if we already gathered pts on root\n        if not self.root:\n            self._gather_root()\n\n        self.lvec.setArray(data)\n        self.dm.localToGlobal(self.lvec, self.gvec)\n        self.tozero.scatter(self.gvec, self.zvec)\n\n        return self.zvec.array.copy()\n\n    def scatter_data(self, data):\n        """"""\n        Scatter data to all processes\n        """"""\n\n        toAll, zvec = PETSc.Scatter.toAll(self.gvec)\n\n        self.lvec.setArray(data)\n        self.dm.localToGlobal(self.lvec, self.gvec)\n        toAll.scatter(self.gvec, zvec)\n\n        return zvec.array.copy()\n\n    def sync(self, vector):\n        """"""\n        Synchronise the local domain with the global domain\n\n        Parameters\n        ----------\n        vector : array of floats, shape (n,)\n            local vector to be synchronised\n\n        Returns\n        -------\n        vector : array of floats, shape (n,)\n            local vector synchronised with the global mesh\n        """"""\n\n        if self.dm.comm.Get_size() == 1:\n            return vector\n        else:\n\n            # Is this the same under 3.10 ?\n\n            self.lvec.setArray(vector)\n            # self.dm.localToLocal(self.lvec, self.gvec)\n            self.dm.localToGlobal(self.lvec, self.gvec)\n            self.dm.globalToLocal(self.gvec, self.lvec)\n\n            return self.lvec.array.copy()\n\n'"
quagmire/mesh/pixmesh.py,16,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nFor structured data on a regular grid.\n\n<img src=""https://raw.githubusercontent.com/underworldcode/quagmire/dev/docs/images/quagmire-flowchart-pixmesh.png"" style=""width: 321px; float:right"">\n\n`PixMesh` implements the following functionality:\n\n- calculating spatial derivatives\n- identifying node neighbour relationships\n- interpolation / extrapolation\n- smoothing operators\n- importing and saving mesh information\n\nSupply a `PETSc DM` object (created from `quagmire.tools.meshtools`) to initialise the object.\n""""""\n\nimport numpy as np\nfrom mpi4py import MPI\nimport sys,petsc4py\npetsc4py.init(sys.argv)\nfrom petsc4py import PETSc\n# comm = MPI.COMM_WORLD\nfrom time import perf_counter\nfrom .commonmesh import CommonMesh as _CommonMesh\n\ntry: range = xrange\nexcept: pass\n\n\nclass PixMesh(_CommonMesh):\n    """"""\n    Build spatial data structures on an __structured Cartesian grid__.\n\n    Use `PixMesh` for:\n\n    - calculating spatial derivatives\n    - identifying node neighbour relationships\n    - interpolation / extrapolation\n    - smoothing operators\n    - importing and saving mesh information\n\n    Each of these data structures are built on top of a `PETSc DM` object\n    (created from `quagmire.tools.meshtools`).\n\n    Parameters\n    ----------\n    DM : PETSc DMDA object\n        Build this unstructured Cartesian mesh object using one of:\n\n        - `quagmire.tools.meshtools.create_DMDA`\n    verbose : bool\n        Flag toggles verbose output\n    *args : optional arguments\n    **kwargs : optional keyword arguments\n\n    Attributes\n    ----------\n    dx : float\n        spacing in the x direction\n    dy : float\n        spacing in the y direction\n    npoints : int\n        Number of points (n) in the mesh\n    pointwise_area : Quagmire MeshVariable\n        `quagmire.mesh.basemesh.MeshVariable` of point-wise area\n    mask : Quagmire MeshVariable\n        `quagmire.mesh.basemesh.MeshVariable` to denote points on the boundary\n    data : array of floats, shape (n,2)\n        Cartesian mesh coordinates in x,y directions\n    coords : array of floats, shape (n,2)\n        Same as `data`\n    neighbour_array : array of ints, shape(n,25)\n        array of node neighbours\n    timings : dict\n        Timing information for each Quagmire routine\n    """"""\n\n    ## This is a count of all the instances of the class that are launched\n    ## so that we have a way to name / identify them\n\n    __count = 0\n\n    @classmethod\n    def _count(cls):\n        PixMesh.__count += 1\n        return PixMesh.__count\n\n    @property\n    def id(self):\n        return self.__id\n\n\n    def __init__(self, dm, verbose=True, *args, **kwargs):\n        from scipy.spatial import cKDTree as _cKDTree\n\n        # initialise base mesh class\n        super(PixMesh, self).__init__(dm, verbose)\n\n        self.__id = ""pixmesh_{}"".format(self._count())\n\n        (minX, maxX), (minY, maxY) = dm.getBoundingBox()\n        Nx, Ny = dm.getSizes()\n\n        dx = (maxX - minX)/(Nx - 1)\n        dy = (maxY - minY)/(Ny - 1)\n        # assert dx == dy, ""Uh oh! Each cell should be square, not rectangular.""\n\n        self.area = np.array(dx*dy)\n        self.adjacency_weight = 0.5\n        self.pointwise_area = self.add_variable(name=""area"")\n        self.pointwise_area.data = self.area\n        self.pointwise_area.lock()\n\n        self.bc = dict()\n        self.bc[""top""]    = (1,maxY)\n        self.bc[""bottom""] = (1,minY)\n        self.bc[""left""]   = (0,minX)\n        self.bc[""right""]  = (0,maxX)\n\n        (minI, maxI), (minJ, maxJ) = dm.getGhostRanges()\n\n        nx = maxI - minI\n        ny = maxJ - minJ\n\n        self.dx, self.dy = dx, dy\n        self.nx, self.ny = nx, ny\n\n\n        # Get local coordinates\n        self.coords = dm.getCoordinatesLocal().array.reshape(-1,2)\n        self.data = self.coords\n\n        (minX, maxX), (minY, maxY) = dm.getLocalBoundingBox()\n\n        self.minX, self.maxX = minX, maxX\n        self.minY, self.maxY = minY, maxY\n\n        self.npoints = nx*ny\n\n\n        # cKDTree\n        t = perf_counter()\n        self.cKDTree = _cKDTree(self.coords)\n        self.timings[\'cKDTree\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank == 0 and self.verbose:\n            print(""{} - cKDTree {}s"".format(self.dm.comm.rank, perf_counter()-t))\n\n\n        # Find boundary points\n        t = perf_counter()\n        self.bmask = self.get_boundary()\n        self.mask = self.add_variable(name=""Mask"")\n        self.mask.data = self.bmask.astype(PETSc.ScalarType)\n        self.mask.lock()\n        self.timings[\'find boundaries\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank == 0 and self.verbose:\n            print(""{} - Find boundaries {}s"".format(self.dm.comm.rank, perf_counter()-t))\n\n\n        # Find neighbours\n        t = perf_counter()\n        self.construct_neighbour_cloud()\n        self.timings[\'construct neighbour cloud\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n\n        t1 = perf_counter()\n        self.construct_natural_neighbour_cloud()\n        self.timings[\'construct natural neighbour cloud\'] = [perf_counter()-t1, self.log.getCPUTime(), self.log.getFlops()]\n \n        if self.rank==0 and self.verbose:\n            print(""{} - Construct neighbour cloud arrays {}s, ({}s + {}s)"".format(self.dm.comm.rank,  perf_counter()-t,\n                                                                                self.timings[\'construct neighbour cloud\'][0],\n                                                                                self.timings[\'construct natural neighbour cloud\'][0]  ))\n\n\n        # RBF smoothing operator\n        t = perf_counter()\n        self._construct_rbf_weights()\n        self.timings[\'construct rbf weights\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank == 0 and self.verbose:\n            print(""{} - Construct rbf weights {}s"".format(self.dm.comm.rank, perf_counter()-t))\n\n\n        self.root = False\n\n        # functions / parameters that are required for compatibility among FlatMesh types\n        self._derivative_grad_cartesian = self.derivative_grad\n        self._radius = 1.0\n\n\n    def derivative_grad(self, PHI):\n        """"""\n        Compute derivatives of PHI in the x, y directions.\n        This routine uses SRFPACK to compute derivatives on a C-1 bivariate function.\n\n        Arguments\n        ---------\n        PHI : ndarray of floats, shape (n,)\n            compute the derivative of this array\n        nit : int optional (default: 10)\n            number of iterations to reach convergence\n        tol : float optional (default: 1e-8)\n            convergence is reached when this tolerance is met\n\n        Returns\n        -------\n        PHIx : ndarray of floats, shape(n,)\n            first partial derivative of PHI in x direction\n        PHIy : ndarray of floats, shape(n,)\n            first partial derivative of PHI in y direction\n        """"""\n        u = PHI.reshape(self.ny, self.nx)\n        u_x, u_y = np.gradient(u, self.dx, self.dy)\n\n        return u_x.ravel(), u_y.ravel()\n\n\n    def derivative_div(self, PHIx, PHIy):\n        """"""\n        Compute second order derivative from flux fields PHIx, PHIy\n        We evaluate the gradient on these fields using the derivative-grad method.\n\n        Arguments\n        ---------\n        PHIx : ndarray of floats, shape (n,)\n            array of first partial derivatives in x direction\n        PHIy : ndarray of floats, shape (n,)\n            array of first partial derivatives in y direction\n        kwargs : optional keyword-argument specifiers\n            keyword arguments to be passed onto derivative_grad\n            e.g. nit=5, tol=1e-3\n\n        Returns\n        -------\n        del2PHI : ndarray of floats, shape (n,)\n            second derivative of PHI\n        """"""\n        u_xx, u_xy = self.derivative_grad(PHIx)\n        u_yx, u_yy = self.derivative_grad(PHIy)\n\n        return u_xx + u_yy\n\n\n    def sort_nodes_by_field2(self, field):\n        """"""\n        Generate an array of the two lowest nodes and a highest node\n\n        Sorting on masked arrays always returns with masked points last.\n        Each node has at least 3 closed neighbour nodes, so we can vectorise\n        this to some extent.\n        """"""\n\n        mask = self.neighbour_block.mask\n        n_offgrid = mask.sum(axis=1)\n\n        nfield = np.ma.array(field[self.neighbour_block], mask=mask)\n        order = nfield.argsort(axis=1)\n\n\n        # We know there is going to be at least 3 entries\n        # [lowest, 2nd lowest, highest]\n        neighbour_array_l2h = np.empty((self.npoints,3), dtype=PETSc.IntType)\n\n        for i in range(0,3):\n            node_mask = n_offgrid == i\n\n            n0 = order[node_mask,0]    # lowest\n            n1 = order[node_mask,1]    # second lowest\n            n2 = order[node_mask,-1-i] # highest\n\n            neighbour_array_l2h[node_mask,0] = self.neighbour_block[node_mask,n0]\n            neighbour_array_l2h[node_mask,1] = self.neighbour_block[node_mask,n1]\n            neighbour_array_l2h[node_mask,2] = self.neighbour_block[node_mask,n2]\n\n        self.neighbour_array_l2h = neighbour_array_l2h\n\n\n    def construct_natural_neighbour_cloud(self):\n        """"""\n        Find the natural neighbours for each node in the triangulation and store in a\n        numpy array for efficient lookup.\n        """"""\n\n        natural_neighbours = np.empty((self.npoints, 9), dtype=np.int)\n        nodes = np.arange(0, self.npoints, dtype=np.int).reshape(self.ny,self.nx)\n        index = np.pad(nodes, (1,1), constant_values=-1)\n\n\n        natural_neighbours[:,0] = index[1:-1,1:-1].flat  # self\n        natural_neighbours[:,1] = index[2:,  1:-1].flat  # right\n        natural_neighbours[:,2] = index[1:-1, :-2].flat  # bottom\n        natural_neighbours[:,3] = index[0:-2,1:-1].flat  # left\n        natural_neighbours[:,4] = index[1:-1,2:  ].flat  # top\n        natural_neighbours[:,5] = index[2:  ,2:  ].flat  # top right\n        natural_neighbours[:,6] = index[2:  , :-2].flat  # bottom right\n        natural_neighbours[:,7] = index[ :-2, :-2].flat  # bottom left\n        natural_neighbours[:,8] = index[ :-2,2:  ].flat  # top left\n\n        # shuffle -1 entries to the end\n        natural_neighbour_mask = natural_neighbours != -1\n        natural_neighbours_count = np.count_nonzero(natural_neighbour_mask, axis=1)\n\n        for i in range(0, self.npoints):\n            nnc = natural_neighbours_count[i]\n            nnm = natural_neighbour_mask[i]\n\n            natural_neighbours[i,:nnc], natural_neighbours[i,nnc:] = natural_neighbours[i,nnm], natural_neighbours[i,~nnm]\n\n\n        self.natural_neighbours       = natural_neighbours\n        self.natural_neighbours_count = natural_neighbours_count\n        self.natural_neighbours_mask  = natural_neighbours != -1\n\n\n    def construct_neighbour_cloud(self, size=25):\n        """"""\n        Find neighbours from distance cKDTree.\n\n        Parameters\n        ----------\n        size : int\n            Number of neighbours to search for\n\n        Notes\n        -----\n        Use this method to search for neighbours that are not\n        necessarily immediate node neighbours (i.e. neighbours\n        connected by a line segment). Extended node neighbours\n        should be captured by the search depending on how large\n        `size` is set to.\n        """"""\n        nndist, nncloud = self.cKDTree.query(self.coords, k=size)\n\n        self.neighbour_cloud = nncloud\n        self.neighbour_cloud_distances = nndist\n\n        # identify corner nodes\n        corners = [0, self.nx-1, -self.nx, -1]\n\n        # interior nodes have 3*3 neighbours (including self)\n        neighbours = np.full(self.npoints, 9, dtype=np.int)\n        neighbours[~self.bmask] = 6 # walls have 3*2 neighbours\n        neighbours[corners] = 4 # corners have 4 neighbours\n\n        self.near_neighbours = neighbours + 2\n        self.extended_neighbours = np.full_like(neighbours, size)\n\n        # self.near_neighbour_mask = np.zeros_like(self.neighbour_cloud, dtype=np.bool)\n\n        # for node in range(0,self.npoints):\n        #     self.near_neighbour_mask[node, 0:self.near_neighbours[node]] = True\n\n        return\n\n\n    def local_area_smoothing(self, data, its=1, centre_weight=0.75):\n        """"""\n        Local area smoothing using radial-basis function smoothing kernel\n\n        Parameters\n        ----------\n        data : array of floats, shape (n,)\n            field variable to be smoothed\n        its : int\n            number of iterations (default: 1)\n        centre_weight : float\n            weight to apply to centre nodes (default: 0.75)\n            other nodes are weighted by (1 - `centre_weight`)\n\n        Returns\n        -------\n        sm : array of floats, shape (n,)\n            smoothed field variable\n        """"""\n\n        smooth_data = data.copy()\n        smooth_data_old = data.copy()\n\n        for i in range(0, its):\n            smooth_data_old[:] = smooth_data\n            smooth_data = centre_weight*smooth_data_old + \\\n                          (1.0 - centre_weight)*self.rbf_smoother(smooth_data)\n\n        return smooth_data\n\n\n    def get_boundary(self):\n        """"""\n        Get boundary information on the mesh\n\n        Returns\n        -------\n        bmask : array of bools, shape (n,)\n            mask out the boundary nodes. Interior nodes are True;\n            Boundary nodes are False.\n        """"""\n        bmask = np.ones(self.npoints, dtype=bool)\n\n        for key in self.bc:\n            i, wall = self.bc[key]\n            mask = self.coords[:,i] == wall\n            bmask[mask] = False\n\n        return bmask\n\n\n    def _construct_rbf_weights(self, delta=None):\n\n        if delta == None:\n            delta = self.neighbour_cloud_distances[:, 1].mean()\n\n        self.delta  = delta\n        self.gaussian_dist_w = self._rbf_weights(delta)\n\n        return\n\n    def _rbf_weights(self, delta=None):\n\n        neighbour_cloud_distances = self.neighbour_cloud_distances\n\n        if delta == None:\n            delta = self.neighbour_cloud_distances[:, 1].mean()\n\n        # Initialise the interpolants\n\n        gaussian_dist_w       = np.zeros_like(neighbour_cloud_distances)\n        gaussian_dist_w[:,:]  = np.exp(-np.power(neighbour_cloud_distances[:,:]/delta, 2.0))\n        gaussian_dist_w[:,:] /= gaussian_dist_w.sum(axis=1).reshape(-1,1)\n\n        return gaussian_dist_w\n\n    def rbf_smoother(self, vector, iterations=1, delta=None):\n        """"""\n        Smoothing using a radial-basis function smoothing kernel\n\n        Arguments\n        ---------\n        vector : array of floats, shape (n,)\n            field variable to be smoothed\n        iterations : int\n            number of iterations to smooth vector\n        delta : float / array of floats shape (n,)\n            distance weights to apply the Gaussian interpolants\n\n        Returns\n        -------\n        smooth_vec : array of floats, shape (n,)\n            smoothed version of input vector\n        """"""\n\n\n        if type(delta) != type(None):\n            self._construct_rbf_weights(delta)\n\n        vector = self.sync(vector)\n\n        for i in range(0, iterations):\n            # print self.dm.comm.rank, "": RBF "",vector.max(), vector.min()\n\n            vector_smoothed = (vector[self.neighbour_cloud[:,:]] * self.gaussian_dist_w[:,:]).sum(axis=1)\n            vector = self.sync(vector_smoothed)\n\n        return vector\n\n\n    def build_rbf_smoother(self, delta, iterations=1):\n\n        pixmesh_self = self\n\n        class _rbf_smoother_object(object):\n\n            def __init__(inner_self, delta, iterations=1):\n\n                if delta == None:\n                    pixmesh_self.lvec.setArray(pixmesh_self.neighbour_cloud_distances[:, 1])\n                    pixmesh_self.dm.localToGlobal(pixmesh_self.lvec, pixmesh_self.gvec)\n                    delta = pixmesh_self.gvec.sum() / pixmesh_self.gvec.getSize()\n\n                inner_self._mesh = pixmesh_self\n                inner_self.delta = delta\n                inner_self.iterations = iterations\n                inner_self.gaussian_dist_w = inner_self._mesh._rbf_weights(delta)\n\n                return\n\n\n            def _apply_rbf_on_my_mesh(inner_self, lazyFn, iterations=1):\n                import quagmire\n\n                smooth_node_values = lazyFn.evaluate(inner_self._mesh)\n\n                for i in range(0, iterations):\n                    smooth_node_values = (smooth_node_values[inner_self._mesh.neighbour_cloud[:,:]] * inner_self.gaussian_dist_w[:,:]).sum(axis=1)\n                    smooth_node_values = inner_self._mesh.sync(smooth_node_values)\n\n                return smooth_node_values\n\n\n            def smooth_fn(inner_self, lazyFn, iterations=None):\n\n                if iterations is None:\n                        iterations = inner_self.iterations\n\n                def smoother_fn(*args, **kwargs):\n                    import quagmire\n\n                    smooth_node_values = inner_self._apply_rbf_on_my_mesh(lazyFn, iterations=iterations)\n\n                    if len(args) == 1 and args[0] == lazyFn._mesh:\n                        return smooth_node_values\n                    elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh(args[0]):\n                        mesh = args[0]\n                        return inner_self._mesh.interpolate(lazyFn._mesh.coords[:,0], lazyFn._mesh.coords[:,1], zdata=smooth_node_values, **kwargs)\n                    else:\n                        xi = np.atleast_1d(args[0])\n                        yi = np.atleast_1d(args[1])\n                        i, e = inner_self._mesh.interpolate(xi, yi, zdata=smooth_node_values, **kwargs)\n                        return i\n\n\n                newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n                newLazyFn.evaluate = smoother_fn\n                newLazyFn.description = ""RBFsmooth({}, d={}, i={})"".format(lazyFn.description, inner_self.delta, iterations)\n\n                return newLazyFn\n\n        return _rbf_smoother_object(delta, iterations)'"
quagmire/mesh/strimesh.py,33,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nFor unstructured data on the sphere.\n\n<img src=""https://raw.githubusercontent.com/underworldcode/quagmire/dev/docs/images/quagmire-flowchart-strimesh.png"" style=""width: 321px; float:right"">\n\n`sTriMesh` implements the following functionality:\n\n- calculating spatial derivatives\n- identifying node neighbour relationships\n- interpolation / extrapolation\n- smoothing operators\n- importing and saving mesh information\n\nSupply a `PETSc DM` object (created from `quagmire.tools.meshtools`) to initialise the object.\n""""""\n\nimport numpy as np\nfrom mpi4py import MPI\nimport sys,petsc4py\npetsc4py.init(sys.argv)\nfrom petsc4py import PETSc\n# comm = MPI.COMM_WORLD\nfrom time import perf_counter\nfrom .commonmesh import CommonMesh as _CommonMesh\n\ntry: range = xrange\nexcept: pass\n\nfrom quagmire import function as fn\nfrom quagmire.function import LazyEvaluation as _LazyEvaluation\n\n\nclass sTriMesh(_CommonMesh):\n    """"""\n    Build spatial data structures on an __unstructured spherical mesh__.\n\n    Use `sTriMesh` for:\n\n    - calculating spatial derivatives\n    - identifying node neighbour relationships\n    - interpolation / extrapolation\n    - smoothing operators\n    - importing and saving mesh information\n\n    Each of these data structures are built on top of a `PETSc DM` object\n    (created from `quagmire.tools.meshtools`).\n\n    Parameters\n    ----------\n    DM : PETSc DMPlex object\n        Build this unstructured spherical mesh object using one of:\n\n        - `quagmire.tools.meshtools.create_spherical_DMPlex`\n        - `quagmire.tools.meshtools.create_DMPlex_from_spherical_points`\n    verbose : bool\n        Flag toggles verbose output\n    *args : optional arguments\n    **kwargs : optional keyword arguments\n\n    Attributes\n    ----------\n    tri : stripy Triangulation object\n        Cartesian mesh object generated by `stripy`\n    npoints : int\n        Number of points (n) in the mesh\n    pointwise_area : Quagmire MeshVariable\n        `quagmire.mesh.basemesh.MeshVariable` of point-wise area\n    mask : Quagmire MeshVariable\n        `quagmire.mesh.basemesh.MeshVariable` to denote points on the boundary\n    data : array of floats, shape (n,2)\n        Cartesian mesh coordinates in x,y directions\n    coords : array of floats, shape (n,2)\n        Same as `data`\n    neighbour_cloud : array of ints, shape(n,25)\n        array of nearest node neighbours by distance\n    near_neighbour_mask : array of bools, shape(n,25)\n        mask immediate node neighbours in `neighbour_cloud`\n    timings : dict\n        Timing information for each Quagmire routine\n    """"""\n\n    __count = 0\n\n    @classmethod\n    def _count(cls):\n        sTriMesh.__count += 1\n        return sTriMesh.__count\n\n    @property\n    def id(self):\n        return self.__id\n\n    def __init__(self, dm, r1=6384.4e3, r2=6352.8e3, verbose=True, *args, **kwargs):\n        import stripy\n        from scipy.spatial import cKDTree as _cKDTree\n\n        # initialise base mesh class\n        super(sTriMesh, self).__init__(dm, verbose)\n\n        self.__id = ""strimesh_{}"".format(self._count())\n\n\n        # Delaunay triangulation\n        t = perf_counter()\n        coords = dm.getCoordinatesLocal().array.reshape(-1,3)\n        minX, minY, minZ = coords.min(axis=0)\n        maxX, maxY, maxZ = coords.max(axis=0)\n        length_scale = np.sqrt((maxX - minX)*(maxY - minY)/coords.shape[0])\n\n        # coords += np.random.random(coords.shape) * 0.0001 * length_scale # This should be aware of the point spacing (small perturbation)\n\n        # r = np.sqrt(coords[:,0]**2 + coords[:,1]**2 + coords[:,2]**2) # should just equal 1\n        # r = 1.0\n        # lons = np.arctan2(coords[:,1], coords[:,0])\n        # lats = np.arcsin(coords[:,2]/r)\n        lons, lats = stripy.spherical.xyz2lonlat(coords[:,0], coords[:,1], coords[:,2])\n\n        self.tri = stripy.sTriangulation(lons, lats)\n        self.npoints = self.tri.npoints\n        self.timings[\'triangulation\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank == 0 and self.verbose:\n            print(""{} - Delaunay triangulation {}s"".format(self.dm.comm.rank, perf_counter()-t))\n\n\n        # Calculate geocentric radius\n        self._radius = geocentric_radius(self.tri.lats, r1, r2)\n\n\n        # Calculate weigths and pointwise area\n        t = perf_counter()\n        self.area, self.weight = self.calculate_area_weights()\n        self.pointwise_area = self.add_variable(name=""area"")\n        self.pointwise_area.data = self.area\n        self.pointwise_area.lock()\n\n        self.timings[\'area weights\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank == 0 and self.verbose:\n            print(""{} - Calculate node weights and area {}s"".format(self.dm.comm.rank, perf_counter()-t))\n\n\n        # Find boundary points\n        t = perf_counter()\n        self.bmask = self.get_boundary()\n        self.mask = self.add_variable(name=""Mask"")\n        self.mask.data = self.bmask.astype(PETSc.ScalarType)\n        self.mask.lock()\n        self.timings[\'find boundaries\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank == 0 and self.verbose:\n            print(""{} - Find boundaries {}s"".format(self.dm.comm.rank, perf_counter()-t))\n\n\n        # cKDTree\n        t = perf_counter()\n        self.cKDTree = _cKDTree(self.tri.points, balanced_tree=False)\n        self.timings[\'cKDTree\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank == 0 and self.verbose:\n            print(""{} - cKDTree {}s"".format(self.dm.comm.rank, perf_counter()-t))\n\n\n        # Find neighbours\n        t = perf_counter()\n        self.construct_neighbour_cloud()\n        self.timings[\'construct neighbour cloud\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n\n        t1 = perf_counter()\n        self.construct_natural_neighbour_cloud()\n        self.timings[\'construct natural neighbour cloud\'] = [perf_counter()-t1, self.log.getCPUTime(), self.log.getFlops()]\n \n        if self.rank==0 and self.verbose:\n            print(""{} - Construct neighbour cloud arrays {}s, ({}s + {}s)"".format(self.dm.comm.rank,  perf_counter()-t,\n                                                                                self.timings[\'construct neighbour cloud\'][0],\n                                                                                self.timings[\'construct natural neighbour cloud\'][0]  ))\n\n        # RBF smoothing operator\n        t = perf_counter()\n        self._construct_rbf_weights()\n        self.timings[\'construct rbf weights\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank == 0 and self.verbose:\n            print(""{} - Construct rbf weights {}s"".format(self.dm.comm.rank, perf_counter()-t))\n\n\n        self.root = False\n        self.coords = np.c_[self.tri.lons, self.tri.lats]\n        self.data = self.tri.points\n        self.interpolate = self.tri.interpolate\n\n        # functions / parameters that are required for compatibility among FlatMesh types\n        self._derivative_grad_cartesian = self.tri.gradient_xyz\n\n\n\n    @property\n    def radius(self):\n        """"""\n        The radius of the sphere.\n\n        Use `geocentric_radius` to compute the radius with distortion between the poles\n        and the equator, otherwise Quagmire uses Earth values by default. i.e.\n\n        ```\n        radius = geocentric_radius(r1=6384.4e3, r2=6352.8e3)\n        ```\n\n        Setting a new value of radius updates the point-wise area calculation.\n        """"""\n        return self._radius\n\n    @radius.setter\n    def radius(self, value):\n        self._radius = value\n\n        # re-evalutate mesh area\n        self.area, self.weight = self.calculate_area_weights()\n        self.pointwise_area.unlock()\n        self.pointwise_area.data = self.area\n        self.pointwise_area.lock()\n\n\n    def calculate_area_weights(self):\n        """"""\n        Calculate pointwise weights and area\n\n        Returns\n        -------\n        area : array of floats, shape (n,)\n            point-wise area\n        weights : array of ints, shape(n,)\n            weighting for each point\n\n        Notes\n        -----\n        This calls a fortran 90 routine which computes the weight and area\n        for each point in the mesh using the geocentric radius of the sphere\n        at the equator `r1` and the poles `r2` (defaults to Earth values:\n        6384.4km and 6352.8km, respectively).\n        """"""\n\n        from quagmire._fortran import ntriw_s\n\n        R = self._radius\n        tri_area = self.tri.areas()\n\n        # find surface area and weights on the unit sphere\n        area, weight = ntriw_s(self.npoints, self.tri.simplices.T+1, tri_area)\n\n        # project to the radius of the sphere\n        area *= R**2\n        \n        return area, weight\n\n\n    def node_neighbours(self, point):\n        """"""\n        Returns a list of neighbour nodes for a given point in the \n        Delaunay triangulation.\n\n        Parameters\n        ----------\n        point : int\n            point on the mesh\n        \n        Returns\n        -------\n        neighbours : list\n            list of neighbours that are connected by line segments\n            to the specified `point`\n        """"""\n        return self.natural_neighbours[point,1:self.natural_neighbours_count[point]]\n\n\n    def derivative_grad(self, PHI, nit=10, tol=1e-8):\n        """"""\n        Compute derivatives of PHI in the x, y directions.\n        This routine uses SRFPACK to compute derivatives on a C-1 bivariate function.\n\n        Arguments\n        ---------\n        PHI : ndarray of floats, shape (n,)\n            compute the derivative of this array\n        nit : int optional (default: 10)\n            number of iterations to reach convergence\n        tol : float optional (default: 1e-8)\n            convergence is reached when this tolerance is met\n\n        Returns\n        -------\n        PHIx : ndarray of floats, shape(n,)\n            first partial derivative of PHI in x direction\n        PHIy : ndarray of floats, shape(n,)\n            first partial derivative of PHI in y direction\n        """"""\n        return self.tri.gradient_lonlat(PHI, nit, tol)\n\n\n    def derivative_div(self, PHIx, PHIy, PHIz, **kwargs):\n        """"""\n        Compute second order derivative from flux fields PHIx, PHIy\n        We evaluate the gradient on these fields using the derivative-grad method.\n\n        Arguments\n        ---------\n        PHIx : ndarray of floats, shape (n,)\n            array of first partial derivatives in x direction\n        PHIy : ndarray of floats, shape (n,)\n            array of first partial derivatives in y direction\n        PHIz : ndarray of floats, shape (n,)\n            array of first partial derivatives in z direction\n        kwargs : optional keyword-argument specifiers\n            keyword arguments to be passed onto derivative_grad\n            e.g. nit=5, tol=1e-3\n\n        Returns\n        -------\n        del2PHI : ndarray of floats, shape (n,)\n            second derivative of PHI\n        """"""\n        u_xx, u_xy, u_zz = self.derivative_grad(PHIx, **kwargs)\n        u_yx, u_yy, u_zz = self.derivative_grad(PHIy, **kwargs)\n        u_zx, u_zy, u_zz = self.derivative_grad(PHIz, **kwargs)\n\n        return u_xx + u_yy + u_zz\n\n\n    def construct_natural_neighbour_cloud(self):\n        """"""\n        Find the natural neighbours for each node in the triangulation and store in a\n        numpy array for efficient lookup.\n        """"""\n\n        # # maximum size of nn array node-by-node (it\'s almost certainly +1 because\n        # # it is only +2 if the neighbours do not form a closed loop around the node)\n\n        # max_neighbours = np.bincount(self.tri.simplices.ravel(), minlength=self.npoints).max() + 2\n\n        # s = self.tri.simplices\n        # nns = -1 * np.ones((self.npoints, max_neighbours), dtype=int)\n        # nnm = np.zeros((self.npoints, max_neighbours), dtype=bool)\n        # nn  = np.empty((self.npoints), dtype=int)\n\n        # for node in range(0, self.npoints):\n        #     w = np.where(s==node)\n        #     l = np.unique(s[w[0]])\n        #     # l = np.roll(l,-np.argwhere(l == node)[0])\n        #     nn[node] = l.shape[0]\n        #     nns[node,0:nn[node]] = l\n        #     nnm[node,0:nn[node]] = True\n\n        # self.natural_neighbours       = nns\n        # self.natural_neighbours_count = nn\n        # self.natural_neighbours_mask   = nnm\n\n\n        \n        # Find all the segments in the triangulation and sort the array\n        # (note this differs from the stripy routine because is treates m-n and n-m as distinct)\n        \n\n        lst  = self.tri.lst\n        lend = self.tri.lend\n        lptr = self.tri.lptr\n\n        segments_array = np.empty((len(lptr),2),dtype=np.int)\n        segments_array[:,0] = np.abs(lst[:]) - 1\n        segments_array[:,1] = np.abs(lst[lptr[:]-1]) - 1\n        \n        valid = np.logical_and(segments_array[:,0] >= 0,  segments_array[:,1] >= 0)\n        segments = segments_array[valid,:]\n\n        deshuffled = self.tri._deshuffle_simplices(segments)\n        smsi = np.argsort(deshuffled[:,0])\n        sms  = np.zeros_like(deshuffled)\n        sms[np.indices((deshuffled.shape[0],)),:] = deshuffled[smsi,:]\n\n        natural_neighbours_count = np.bincount(sms[:,0])\n        iend = np.cumsum(natural_neighbours_count)\n        istart = np.zeros_like(iend)\n        istart[1:] = iend[:-1]\n\n        natural_neighbours = np.full((self.npoints, natural_neighbours_count.max()+1), -1, dtype=np.int)\n\n        for j in range(0,self.npoints):\n            natural_neighbours[j, 0] = j\n            natural_neighbours[j, 1:natural_neighbours_count[j]+1] = sms[istart[j]:istart[j]+natural_neighbours_count[j],1]\n\n        natural_neighbours_count += 1 \n\n        self.natural_neighbours       = natural_neighbours\n        self.natural_neighbours_count = natural_neighbours_count\n        self.natural_neighbours_mask  = natural_neighbours != -1\n\n        return\n\n\n    def construct_neighbour_cloud(self, size=25):\n        """"""\n        Find neighbours from distance cKDTree.\n\n        Parameters\n        ----------\n        size : int\n            Number of neighbours to search for\n\n        Notes\n        -----\n        Use this method to search for neighbours that are not\n        necessarily immediate node neighbours (i.e. neighbours\n        connected by a line segment). Extended node neighbours\n        should be captured by the search depending on how large\n        `size` is set to.\n        """"""\n        nndist, nncloud = self.cKDTree.query(self.tri.points, k=size)\n        self.neighbour_cloud = nncloud\n        self.neighbour_cloud_distances = nndist\n\n        neighbours = np.bincount(self.tri.simplices.ravel(), minlength=self.npoints)\n        self.near_neighbours = neighbours + 2\n        self.extended_neighbours = np.full_like(neighbours, size)\n\n\n    def local_area_smoothing(self, data, its=1, centre_weight=0.75):\n        """"""\n        Local area smoothing using radial-basis function smoothing kernel\n\n        Parameters\n        ----------\n        data : array of floats, shape (n,)\n            field variable to be smoothed\n        its : int\n            number of iterations (default: 1)\n        centre_weight : float\n            weight to apply to centre nodes (default: 0.75)\n            other nodes are weighted by (1 - `centre_weight`)\n\n        Returns\n        -------\n        sm : array of floats, shape (n,)\n            smoothed field variable\n        """"""\n\n        smooth_data = data.copy()\n        smooth_data_old = data.copy()\n\n        for i in range(0, its):\n            smooth_data_old[:] = smooth_data\n            smooth_data = centre_weight*smooth_data_old + \\\n                          (1.0 - centre_weight)*self.rbf_smoother(smooth_data)\n\n        return smooth_data\n\n\n    def _construct_rbf_weights(self, delta=None):\n\n        if delta == None:\n            delta = self.neighbour_cloud_distances[:, 1].mean()\n\n        self.delta  = delta\n        self.gaussian_dist_w = self._rbf_weights(delta)\n\n        return\n\n    def _rbf_weights(self, delta=None):\n\n        neighbour_cloud_distances = self.neighbour_cloud_distances\n\n        if delta == None:\n            delta = self.neighbour_cloud_distances[:, 1].mean()\n\n        # Initialise the interpolants\n\n        gaussian_dist_w       = np.zeros_like(neighbour_cloud_distances)\n        gaussian_dist_w[:,:]  = np.exp(-np.power(neighbour_cloud_distances[:,:]/delta, 2.0))\n        gaussian_dist_w[:,:] /= gaussian_dist_w.sum(axis=1).reshape(-1,1)\n\n        return gaussian_dist_w\n\n    def rbf_smoother(self, vector, iterations=1, delta=None):\n        """"""\n        Smoothing using a radial-basis function smoothing kernel\n\n        Arguments\n        ---------\n        vector : array of floats, shape (n,)\n            field variable to be smoothed\n        iterations : int\n            number of iterations to smooth vector\n        delta : float / array of floats shape (n,)\n            distance weights to apply the Gaussian interpolants\n\n        Returns\n        -------\n        smooth_vec : array of floats, shape (n,)\n            smoothed version of input vector\n        """"""\n\n\n        if type(delta) != type(None):\n            self._construct_rbf_weights(delta)\n\n        vector = self.sync(vector)\n\n        for i in range(0, iterations):\n            # print self.dm.comm.rank, "": RBF "",vector.max(), vector.min()\n\n            vector_smoothed = (vector[self.neighbour_cloud[:,:]] * self.gaussian_dist_w[:,:]).sum(axis=1)\n            vector = self.sync(vector_smoothed)\n\n        return vector\n\n    def build_rbf_smoother(self, delta, iterations=1):\n\n        strimesh_self = self\n\n        class _rbf_smoother_object(object):\n\n            def __init__(inner_self, delta, iterations=1):\n\n                if delta == None:\n                    strimesh_self.lvec.setArray(strimesh_self.neighbour_cloud_distances[:, 1])\n                    strimesh_self.dm.localToGlobal(strimesh_self.lvec, strimesh_self.gvec)\n                    delta = strimesh_self.gvec.sum() / strimesh_self.gvec.getSize()\n\n                inner_self._mesh = strimesh_self\n                inner_self.delta = delta\n                inner_self.iterations = iterations\n                inner_self.gaussian_dist_w = inner_self._mesh._rbf_weights(delta)\n\n                return\n\n\n            def _apply_rbf_on_my_mesh(inner_self, lazyFn, iterations=1):\n                import quagmire\n\n                smooth_node_values = lazyFn.evaluate(inner_self._mesh)\n\n                for i in range(0, iterations):\n                    smooth_node_values = (smooth_node_values[inner_self._mesh.neighbour_cloud[:,:]] * inner_self.gaussian_dist_w[:,:]).sum(axis=1)\n                    smooth_node_values = inner_self._mesh.sync(smooth_node_values)\n\n                return smooth_node_values\n\n\n            def smooth_fn(inner_self, lazyFn, iterations=None):\n\n                if iterations is None:\n                        iterations = inner_self.iterations\n\n                def smoother_fn(*args, **kwargs):\n                    import quagmire\n\n                    smooth_node_values = inner_self._apply_rbf_on_my_mesh(lazyFn, iterations=iterations)\n\n                    if len(args) == 1 and args[0] == lazyFn._mesh:\n                        return smooth_node_values\n                    elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh(args[0]):\n                        mesh = args[0]\n                        return inner_self._mesh.interpolate(lazyFn._mesh.coords[:,0], lazyFn._mesh.coords[:,1], zdata=smooth_node_values, **kwargs)\n                    else:\n                        xi = np.atleast_1d(args[0])\n                        yi = np.atleast_1d(args[1])\n                        i, e = inner_self._mesh.interpolate(xi, yi, zdata=smooth_node_values, **kwargs)\n                        return i\n\n\n                newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n                newLazyFn.evaluate = smoother_fn\n                newLazyFn.description = ""RBFsmooth({}, d={}, i={})"".format(lazyFn.description, inner_self.delta, iterations)\n\n                return newLazyFn\n\n        return _rbf_smoother_object(delta, iterations)\n\n\n\n\ndef geocentric_radius(lat, r1=6384.4e3, r2=6352.8e3):\n    """"""\n    Calculate the radius of an oblate spheroid (like the earth)\n\n    Parameters\n    ----------\n    lat : array of floats\n        latitudinal coordinates in radians\n    r1 : float\n        radius at the equator (in metres)\n    r2 : float\n        radius at the poles (in metres)\n\n    Returns\n    -------\n    r : array of floats\n        radius at provided latitudes `lat` in metres\n    """"""\n    coslat = np.cos(lat)\n    sinlat = np.sin(lat)\n    num = (r1**2*coslat)**2 + (r2**2*sinlat)**2\n    den = (r1*coslat)**2 + (r2*sinlat)**2\n    return np.sqrt(num/den)\n\n'"
quagmire/mesh/trimesh.py,30,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nFor unstructured data in Cartesian coordinates.\n\n<img src=""https://raw.githubusercontent.com/underworldcode/quagmire/dev/docs/images/quagmire-flowchart-trimesh.png"" style=""width: 321px; float:right"">\n\n`TriMesh` implements the following functionality:\n\n- calculating spatial derivatives\n- identifying node neighbour relationships\n- interpolation / extrapolation\n- smoothing operators\n- importing and saving mesh information\n\nSupply a `PETSc DM` object (created from `quagmire.tools.meshtools`) to initialise the object.\n""""""\n\nimport numpy as np\nfrom mpi4py import MPI\nimport sys,petsc4py\npetsc4py.init(sys.argv)\nfrom petsc4py import PETSc\n# comm = MPI.COMM_WORLD\nfrom time import perf_counter\nfrom .commonmesh import CommonMesh as _CommonMesh\n\ntry: range = xrange\nexcept: pass\n\nfrom quagmire import function as fn\nfrom quagmire.function import LazyEvaluation as _LazyEvaluation\n\n\nclass TriMesh(_CommonMesh):\n    """"""\n    Build spatial data structures on an __unstructured Cartesian mesh__.\n\n    Use `TriMesh` for:\n\n    - calculating spatial derivatives\n    - identifying node neighbour relationships\n    - interpolation / extrapolation\n    - smoothing operators\n    - importing and saving mesh information\n\n    Each of these data structures are built on top of a `PETSc DM` object\n    (created from `quagmire.tools.meshtools`).\n\n    Parameters\n    ----------\n    DM : PETSc DMPlex object\n        Build this unstructured Cartesian mesh object using one of:\n\n        - `quagmire.tools.meshtools.create_DMPlex`\n        - `quagmire.tools.meshtools.create_DMPlex_from_points`\n        - `quagmire.tools.meshtools.create_DMPlex_from_box`\n    verbose : bool\n        Flag toggles verbose output\n    *args : optional arguments\n    **kwargs : optional keyword arguments\n\n    Attributes\n    ----------\n    tri : stripy Triangulation object\n        Cartesian mesh object generated by `stripy`\n    npoints : int\n        Number of points (n) in the mesh\n    pointwise_area : Quagmire MeshVariable\n        `quagmire.mesh.basemesh.MeshVariable` of point-wise area\n    mask : Quagmire MeshVariable\n        `quagmire.mesh.basemesh.MeshVariable` to denote points on the boundary\n    data : array of floats, shape (n,2)\n        Cartesian mesh coordinates in x,y directions\n    coords : array of floats, shape (n,2)\n        Same as `data`\n    neighbour_cloud : array of ints, shape(n,25)\n        array of nearest node neighbours by distance\n    near_neighbour_mask : array of bools, shape(n,25)\n        mask immediate node neighbours in `neighbour_cloud`  (They may not be there !! )\n    timings : dict\n        Timing information for each Quagmire routine\n    """"""\n\n    ## This is a count of all the instances of the class that are launched\n    ## so that we have a way to name / identify them\n\n    __count = 0\n\n    @classmethod\n    def _count(cls):\n        TriMesh.__count += 1\n        return TriMesh.__count\n\n    @property\n    def id(self):\n        return self.__id\n\n\n    def __init__(self, dm, verbose=True, *args, **kwargs):\n        import stripy\n        from scipy.spatial import cKDTree as _cKDTree\n\n        # initialise base mesh class\n        super(TriMesh, self).__init__(dm, verbose)\n\n        self.__id = ""trimesh_{}"".format(self._count())\n\n        # Delaunay triangulation\n        t = perf_counter()\n        coords = self.dm.getCoordinatesLocal().array.reshape(-1,2)\n\n        # minX, minY = coords.min(axis=0)\n        # maxX, maxY = coords.max(axis=0)\n        # length_scale = np.sqrt((maxX - minX)*(maxY - minY)/coords.shape[0])\n        # # coords += np.random.random(coords.shape) * 0.0001 * length_scale # This should be aware of the point spacing (small perturbation)\n\n        self.tri = stripy.Triangulation(coords[:,0], coords[:,1], permute=True)\n        self.npoints = self.tri.npoints\n        self.timings[\'triangulation\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank==0 and self.verbose:\n            print((""{} - Delaunay triangulation {}s"".format(self.dm.comm.rank, perf_counter()-t)))\n\n\n        # Calculate weigths and pointwise area\n        t = perf_counter()\n        self.area, self.weight = self.calculate_area_weights()\n        self.pointwise_area = self.add_variable(name=""area"")\n        self.pointwise_area.data = self.area\n        self.pointwise_area.lock()\n        \n        self.timings[\'area weights\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank==0 and self.verbose:\n            print((""{} - Calculate node weights and area {}s"".format(self.dm.comm.rank, perf_counter()-t)))\n\n\n        # Find boundary points\n        t = perf_counter()\n        self.bmask = self.get_boundary()\n        self.mask = self.add_variable(name=""Mask"")\n        self.mask.data = self.bmask.astype(PETSc.ScalarType)\n        self.mask.lock()\n        self.timings[\'find boundaries\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank==0 and self.verbose:\n            print((""{} - Find boundaries {}s"".format(self.dm.comm.rank, perf_counter()-t)))\n\n\n        # cKDTree\n        t = perf_counter()\n        self.cKDTree = _cKDTree(self.tri.points, balanced_tree=False)\n        self.timings[\'cKDTree\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank==0 and self.verbose:\n            print((""{} - cKDTree {}s"".format(self.dm.comm.rank, perf_counter()-t)))\n\n        # Find neighbours\n        t = perf_counter()\n        self.construct_neighbour_cloud()\n        self.timings[\'construct neighbour cloud\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n\n        t1 = perf_counter()\n        self.construct_natural_neighbour_cloud()\n        self.timings[\'construct natural neighbour cloud\'] = [perf_counter()-t1, self.log.getCPUTime(), self.log.getFlops()]\n \n        if self.rank==0 and self.verbose:\n            print(""{} - Construct neighbour cloud arrays {}s, ({}s + {}s)"".format(self.dm.comm.rank,  perf_counter()-t,\n                                                                                self.timings[\'construct neighbour cloud\'][0],\n                                                                                self.timings[\'construct natural neighbour cloud\'][0]  ))\n\n        # sync smoothing operator\n        t = perf_counter()\n        self._construct_rbf_weights()\n        self.timings[\'construct rbf weights\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n        if self.rank==0 and self.verbose:\n            print((""{} - Construct rbf weights {}s"".format(self.dm.comm.rank, perf_counter()-t)))\n\n        self.root = False\n        self.coords = self.tri.points\n        self.data = self.coords\n        self.interpolate = self.tri.interpolate\n\n        # functions / parameters that are required for compatibility among FlatMesh types\n        self._derivative_grad_cartesian = self.derivative_grad\n        self._radius = 1.0\n\n\n    def calculate_area_weights(self):\n        """"""\n        Calculate pointwise area and weigths\n\n        Returns\n        -------\n        area : array of floats, shape (n,)\n            point-wise area\n        weight : array of ints, shape (n,)\n            point-wise weight\n\n        Notes\n        -----\n        This calls a fortran 90 routine which computes the weight and area\n        for each point in the mesh.\n        """"""\n\n        from quagmire._fortran import ntriw\n\n        area, weight = ntriw(self.tri.x, self.tri.y, self.tri.simplices.T+1)\n\n        return area, weight\n\n\n    def node_neighbours(self, point):\n        """"""\n        Returns a list of neighbour nodes for a given point in the \n        Delaunay triangulation.\n\n        Parameters\n        ----------\n        point : int\n            point on the mesh\n        \n        Returns\n        -------\n        neighbours : list\n            list of neighbours that are connected by line segments\n            to the specified `point`\n        """"""\n        return self.natural_neighbours[point,1:self.natural_neighbours_count[point]]\n\n\n    def derivative_grad(self, PHI, nit=10, tol=1e-8):\n        """"""\n        Compute derivatives of PHI in the x, y directions.\n        This routine uses SRFPACK to compute derivatives on a C-1 bivariate function.\n\n        Arguments\n        ---------\n        PHI : ndarray of floats, shape (n,)\n            compute the derivative of this array\n        nit : int optional (default: 10)\n            number of iterations to reach convergence\n        tol : float optional (default: 1e-8)\n            convergence is reached when this tolerance is met\n\n        Returns\n        -------\n        PHIx : ndarray of floats, shape(n,)\n            first partial derivative of PHI in x direction\n        PHIy : ndarray of floats, shape(n,)\n            first partial derivative of PHI in y direction\n        """"""\n        return self.tri.gradient(PHI, nit, tol)\n\n\n    def derivative_div(self, PHIx, PHIy, **kwargs):\n        """"""\n        Compute second order derivative from flux fields PHIx, PHIy\n        We evaluate the gradient on these fields using the `derivative_grad` method.\n\n        Arguments\n        ---------\n        PHIx : ndarray of floats, shape (n,)\n            array of first partial derivatives in x direction\n        PHIy : ndarray of floats, shape (n,)\n            array of first partial derivatives in y direction\n        kwargs : optional keyword-argument specifiers\n            keyword arguments to be passed onto derivative_grad\n            e.g. nit=5, tol=1e-3\n\n        Returns\n        -------\n        del2PHI : ndarray of floats, shape (n,)\n            second derivative of PHI\n        """"""\n        u_xx, u_xy = self.derivative_grad(PHIx, **kwargs)\n        u_yx, u_yy = self.derivative_grad(PHIy, **kwargs)\n\n        return u_xx + u_yy\n\n\n    def construct_natural_neighbour_cloud(self):\n        """"""\n        Find the natural neighbours for each node in the triangulation and store in a\n        numpy array for efficient lookup.\n        """"""\n\n        # # maximum size of nn array node-by-node (it\'s almost certainly +1 because\n        # # it is only +2 if the neighbours do not form a closed loop around the node)\n\n        # max_neighbours = np.bincount(self.tri.simplices.ravel(), minlength=self.npoints).max() + 2\n\n        # s = self.tri.simplices\n        # nns = -1 * np.ones((self.npoints, max_neighbours), dtype=int)\n        # nnm = np.zeros((self.npoints, max_neighbours), dtype=bool)\n        # nn  = np.empty((self.npoints), dtype=int)\n\n        # for node in range(0, self.npoints):\n        #     w = np.where(s==node)\n        #     l = np.unique(s[w[0]])\n        #     # l = np.roll(l,-np.argwhere(l == node)[0])\n        #     nn[node] = l.shape[0]\n        #     nns[node,0:nn[node]] = l\n        #     nnm[node,0:nn[node]] = True\n\n        # self.natural_neighbours       = nns\n        # self.natural_neighbours_count = nn\n        # self.natural_neighbours_mask   = nnm\n\n\n        \n        # Find all the segments in the triangulation and sort the array\n        # (note this differs from the stripy routine because is treates m-n and n-m as distinct)\n        \n\n        lst  = self.tri.lst\n        lend = self.tri.lend\n        lptr = self.tri.lptr\n\n        segments_array = np.empty((len(lptr),2),dtype=np.int)\n        segments_array[:,0] = np.abs(lst[:]) - 1\n        segments_array[:,1] = np.abs(lst[lptr[:]-1]) - 1\n        \n        valid = np.logical_and(segments_array[:,0] >= 0,  segments_array[:,1] >= 0)\n        segments = segments_array[valid,:]\n\n        deshuffled = self.tri._deshuffle_simplices(segments)\n        smsi = np.argsort(deshuffled[:,0])\n        sms  = np.zeros_like(deshuffled)\n        sms[np.indices((deshuffled.shape[0],)),:] = deshuffled[smsi,:]\n\n        natural_neighbours_count = np.bincount(sms[:,0])\n        iend = np.cumsum(natural_neighbours_count)\n        istart = np.zeros_like(iend)\n        istart[1:] = iend[:-1]\n\n        natural_neighbours = np.full((self.npoints, natural_neighbours_count.max()+1), -1, dtype=np.int)\n\n        for j in range(0,self.npoints):\n            natural_neighbours[j, 0] = j\n            natural_neighbours[j, 1:natural_neighbours_count[j]+1] = sms[istart[j]:istart[j]+natural_neighbours_count[j],1]\n\n        natural_neighbours_count += 1 \n\n        self.natural_neighbours       = natural_neighbours\n        self.natural_neighbours_count = natural_neighbours_count\n        self.natural_neighbours_mask  = natural_neighbours != -1\n\n        return\n\n\n    def construct_neighbour_cloud(self, size=25):\n        """"""\n        Find neighbours from distance cKDTree.\n\n        Parameters\n        ----------\n        size : int\n            Number of neighbours to search for\n\n        Notes\n        -----\n        Use this method to search for neighbours that are not\n        necessarily immediate node neighbours (i.e. neighbours\n        connected by a line segment). Extended node neighbours\n        should be captured by the search depending on how large\n        `size` is set to.\n        """"""\n        nndist, nncloud = self.cKDTree.query(self.tri.points, k=size)\n        self.neighbour_cloud = nncloud\n        self.neighbour_cloud_distances = nndist\n\n        neighbours = np.bincount(self.tri.simplices.ravel(), minlength=self.npoints)\n        self.near_neighbours = neighbours + 2\n        self.extended_neighbours = np.full_like(neighbours, size)\n\n        # self.near_neighbour_mask = np.zeros_like(nncloud, dtype=np.bool)\n\n        # for node in range(0,self.npoints):\n        #     self.near_neighbour_mask[node, 0:self.near_neighbours[node]] = True\n\n        #     row, col = np.nonzero(self.tri.simplices == node)\n        #     natural_neighbours = list(np.unique(np.hstack(self.tri.simplices[row])))\n\n        #     # move self node to start of the list\n        #     natural_neighbours.remove(node)\n        #     natural_neighbours.insert(0, node)\n\n        #     # identify distance neighbours from cloud and join lists\n        #     distance_neighbours = list(set(nncloud[node]) - set(natural_neighbours))\n        #     node_neighbours = np.hstack([natural_neighbours, distance_neighbours])\n\n        #     # size of node_nncloud will be >= ncols\n        #     nncloud[node] = node_neighbours[0:size]\n\n        #     ## Nothing is done about the nndist array, yet...\n\n\n        return\n\n\n    def local_area_smoothing(self, data, its=1, centre_weight=0.75):\n        """"""\n        Local area smoothing using radial-basis function smoothing kernel\n\n        Parameters\n        ----------\n        data : array of floats, shape (n,)\n            field variable to be smoothed\n        its : int\n            number of iterations (default: 1)\n        centre_weight : float\n            weight to apply to centre nodes (default: 0.75)\n            other nodes are weighted by (1 - `centre_weight`)\n\n        Returns\n        -------\n        sm : array of floats, shape (n,)\n            smoothed field variable\n        """"""\n\n        smooth_data = data.copy()\n        smooth_data_old = data.copy()\n\n        for i in range(0, its):\n            smooth_data_old[:] = smooth_data\n            smooth_data = centre_weight*smooth_data_old + \\\n                          (1.0 - centre_weight)*self.rbf_smoother(smooth_data)\n\n        return smooth_data\n\n\n    def _construct_rbf_weights(self, delta=None):\n\n        if delta == None:\n            delta = self.neighbour_cloud_distances[:, 1].mean()\n\n        self.delta  = delta\n        self.gaussian_dist_w = self._rbf_weights(delta)\n\n        return\n\n    def _rbf_weights(self, delta=None):\n\n        neighbour_cloud_distances = self.neighbour_cloud_distances\n\n        if delta == None:\n            delta = self.neighbour_cloud_distances[:, 1].mean()\n\n        # Initialise the interpolants\n\n        gaussian_dist_w       = np.zeros_like(neighbour_cloud_distances)\n        gaussian_dist_w[:,:]  = np.exp(-np.power(neighbour_cloud_distances[:,:]/delta, 2.0))\n        gaussian_dist_w[:,:] /= gaussian_dist_w.sum(axis=1).reshape(-1,1)\n\n        return gaussian_dist_w\n\n\n    def rbf_smoother(self, vector, iterations=1, delta=None):\n        """"""\n        Smoothing using a radial-basis function smoothing kernel\n\n        Arguments\n        ---------\n        vector : array of floats, shape (n,)\n            field variable to be smoothed\n        iterations : int\n            number of iterations to smooth vector\n        delta : float / array of floats shape (n,)\n            distance weights to apply the Gaussian interpolants\n\n        Returns\n        -------\n        smooth_vec : array of floats, shape (n,)\n            smoothed version of input vector\n        """"""\n\n\n        if type(delta) != type(None):\n            self._construct_rbf_weights(delta)\n\n        vector = self.sync(vector)\n\n        for i in range(0, iterations):\n            # print self.dm.comm.rank, "": RBF "",vector.max(), vector.min()\n\n            vector_smoothed = (vector[self.neighbour_cloud[:,:]] * self.gaussian_dist_w[:,:]).sum(axis=1)\n            vector = self.sync(vector_smoothed)\n\n        return vector\n\n\n\n\n    def build_rbf_smoother(self, delta, iterations=1):\n\n        trimesh_self = self\n\n        class _rbf_smoother_object(object):\n\n            def __init__(inner_self, delta, iterations=1):\n\n                if delta == None:\n                    trimesh_self.lvec.setArray(trimesh_self.neighbour_cloud_distances[:, 1])\n                    trimesh_self.dm.localToGlobal(trimesh_self.lvec, trimesh_self.gvec)\n                    delta = trimesh_self.gvec.sum() / trimesh_self.gvec.getSize()\n\n                inner_self._mesh = trimesh_self\n                inner_self.delta = delta\n                inner_self.iterations = iterations\n                inner_self.gaussian_dist_w = inner_self._mesh._rbf_weights(delta)\n\n                return\n\n\n            def _apply_rbf_on_my_mesh(inner_self, lazyFn, iterations=1):\n                import quagmire\n\n                smooth_node_values = lazyFn.evaluate(inner_self._mesh)\n\n                for i in range(0, iterations):\n                    smooth_node_values = (smooth_node_values[inner_self._mesh.neighbour_cloud[:,:]] * inner_self.gaussian_dist_w[:,:]).sum(axis=1)\n                    smooth_node_values = inner_self._mesh.sync(smooth_node_values)\n\n                return smooth_node_values\n\n\n            def smooth_fn(inner_self, lazyFn, iterations=None):\n\n                if iterations is None:\n                        iterations = inner_self.iterations\n\n                def smoother_fn(*args, **kwargs):\n                    import quagmire\n\n                    smooth_node_values = inner_self._apply_rbf_on_my_mesh(lazyFn, iterations=iterations)\n\n                    if len(args) == 1 and args[0] == lazyFn._mesh:\n                        return smooth_node_values\n                    elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh(args[0]):\n                        mesh = args[0]\n                        return inner_self._mesh.interpolate(lazyFn._mesh.coords[:,0], lazyFn._mesh.coords[:,1], zdata=smooth_node_values, **kwargs)\n                    else:\n                        xi = np.atleast_1d(args[0])\n                        yi = np.atleast_1d(args[1])\n                        i, e = inner_self._mesh.interpolate(xi, yi, zdata=smooth_node_values, **kwargs)\n                        return i\n\n\n                newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n                newLazyFn.evaluate = smoother_fn\n                newLazyFn.description = ""RBFsmooth({}, d={}, i={})"".format(lazyFn.description, inner_self.delta, iterations)\n\n                return newLazyFn\n\n        return _rbf_smoother_object(delta, iterations)\n'"
quagmire/scaling/__init__.py,0,"b'##~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~##\n##                                                               ##\n##  This file forms part of the Quagmire geophysics modelling    ##\n##  application.                                                 ##\n##                                                               ##\n##  For full license and copyright information,                  ##\n##  please refer to the LICENSE.md file located at the           ##\n##   project root, or contact the authors.                       ##\n##     (Corresponding Author: Romain Beucher)                    ##\n##                                                               ##\n##                                                               ##\n##~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~##\n\n""""""\nThe scaling module provides units and scaling capabilities.\n""""""\n\nfrom ._scaling import non_dimensionalise\nfrom ._scaling import dimensionalise\nfrom ._scaling import u as units\nfrom ._scaling import get_coefficients\n'"
quagmire/scaling/_scaling.py,0,"b'##~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~##\n##                                                                                   ##\n##  This file forms part of the Underworld/Quagmire geophysics modelling application ##\n##                                                                                   ##\n##  For full license and copyright information, please refer to the LICENSE.md file  ##\n##  located at the project root, or contact the authors.                             ##\n##                                                                                   ##\n##~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~##\n\n""""""\nUtilities to convert between dimensional and non-dimensional values.\n""""""\nfrom __future__ import print_function, absolute_import\nfrom ._utils import TransformedDict\nfrom pint import UnitRegistry\n\nu = UnitRegistry()\n\nCOEFFICIENTS = None\ndef get_coefficients():\n    """"""\n    Returns the global scaling dictionary.\n    """"""\n    global COEFFICIENTS\n    if COEFFICIENTS is None:\n        COEFFICIENTS = TransformedDict()\n        COEFFICIENTS[""[length]""] = 1.0 * u.meter\n        COEFFICIENTS[""[mass]""] = 1.0 * u.kilogram\n        COEFFICIENTS[""[time]""] = 1.0 * u.year\n        COEFFICIENTS[""[temperature]""] = 1.0 * u.degK\n        COEFFICIENTS[""[substance]""] = 1.0 * u.mole\n    return COEFFICIENTS\n\ndef non_dimensionalise(dimValue):\n    """"""\n    Non-dimensionalize (scale) provided quantity.\n\n    This function uses pint to perform a dimension analysis and\n    return a value scaled according to a set of scaling coefficients.\n\n    Parameters\n    ----------\n    dimValue : pint quantity\n\n    Returns\n    -------\n    float: The scaled value.\n\n    Example:\n    --------\n    >>> import quagmire\n    >>> u = quagmire.scaling.units\n\n    >>> # Characteristic values of the system\n    >>> half_rate = 0.5 * u.centimeter / u.year\n    >>> model_height = 600e3 * u.meter\n    >>> refViscosity = 1e24 * u.pascal * u.second\n    >>> surfaceTemp = 0. * u.kelvin\n    >>> baseModelTemp = 1330. * u.kelvin\n    >>> baseCrustTemp = 550. * u.kelvin\n\n    >>> KL_meters = model_height\n    >>> KT_seconds = KL_meters / half_rate\n    >>> KM_kilograms = refViscosity * KL_meters * KT_seconds\n    >>> Kt_degrees = (baseModelTemp - surfaceTemp)\n    >>> K_substance = 1. * u.mole\n\n    >>> scaling_coefficients = quagmire.scaling.get_coefficients()\n    >>> scaling_coefficients[""[time]""] = KT_seconds\n    >>> scaling_coefficients[""[length]""] = KL_meters\n    >>> scaling_coefficients[""[mass]""] = KM_kilograms\n    >>> scaling_coefficients[""[temperature]""] = Kt_degrees\n    >>> scaling_coefficients[""[substance]""] -= K_substance\n\n    >>> # Get a scaled value:\n    >>> gravity = quagmire.scaling.non_dimensionalise(9.81 * u.meter / u.second**2)\n    """"""\n    try:\n        val = dimValue.unitless\n        if val:\n            return dimValue\n    except AttributeError:\n        return dimValue\n\n    dimValue = dimValue.to_base_units()\n\n    scaling_coefficients = get_coefficients()\n\n    length = scaling_coefficients[""[length]""]\n    time = scaling_coefficients[""[time]""]\n    mass = scaling_coefficients[""[mass]""]\n    temperature = scaling_coefficients[""[temperature]""]\n    substance = scaling_coefficients[""[substance]""]\n\n    length = length.to_base_units()\n    time = time.to_base_units()\n    mass = mass.to_base_units()\n    temperature = temperature.to_base_units()\n    substance = substance.to_base_units()\n\n    @u.check(\'[length]\', \'[time]\', \'[mass]\', \'[temperature]\', \'[substance]\')\n    def check(length, time, mass, temperature, substance):\n        return\n\n    check(length, time, mass, temperature, substance)\n\n    # Get dimensionality\n    dlength = dimValue.dimensionality[\'[length]\']\n    dtime = dimValue.dimensionality[\'[time]\']\n    dmass = dimValue.dimensionality[\'[mass]\']\n    dtemp = dimValue.dimensionality[\'[temperature]\']\n    dsubstance = dimValue.dimensionality[\'[substance]\']\n    factor = (length**(-dlength) *\n              time**(-dtime) *\n              mass**(-dmass) *\n              temperature**(-dtemp) *\n              substance**(-dsubstance))\n\n    dimValue *= factor\n\n    if dimValue.unitless:\n        return dimValue.magnitude\n    else:\n        raise ValueError(\'Dimension Error\')\n\ndef dimensionalise(value, units):\n    """"""\n    Dimensionalise a value.\n\n    Parameters\n    ----------\n    value : float, int\n        The value to be assigned units.\n    units : pint units\n        The units to be assigned.\n\n    Returns\n    -------\n    pint quantity: dimensionalised value.\n\n    Example\n    -------\n    >>> import quagmire\n    >>> A = quagmire.scaling.dimensionalise(1.0, u.metre)\n    """"""\n    import quagmire.function as fn\n\n    unit = (1.0 * units).to_base_units()\n\n    scaling_coefficients = get_coefficients()\n\n    length = scaling_coefficients[""[length]""]\n    time = scaling_coefficients[""[time]""]\n    mass = scaling_coefficients[""[mass]""]\n    temperature = scaling_coefficients[""[temperature]""]\n    substance = scaling_coefficients[""[substance]""]\n\n    length = length.to_base_units()\n    time = time.to_base_units()\n    mass = mass.to_base_units()\n    temperature = temperature.to_base_units()\n    substance = substance.to_base_units()\n\n    @u.check(\'[length]\', \'[time]\', \'[mass]\', \'[temperature]\', \'[substance]\')\n    def check(length, time, mass, temperature, substance):\n        return\n\n    # Check that the scaling parameters have the correct dimensions\n    check(length, time, mass, temperature, substance)\n\n    # Get dimensionality\n    dlength = unit.dimensionality[\'[length]\']\n    dtime = unit.dimensionality[\'[time]\']\n    dmass = unit.dimensionality[\'[mass]\']\n    dtemp = unit.dimensionality[\'[temperature]\']\n    dsubstance = unit.dimensionality[\'[substance]\']\n    factor = (length**(dlength) *\n              time**(dtime) *\n              mass**(dmass) *\n              temperature**(dtemp) *\n              substance**(dsubstance))\n\n    if fn.check_object_is_a_q_function(value):\n        if fn.check_object_is_a_mesh_variable(value):\n            tempVar = value.copy()\n            tempVar.data[...] = (value.data[...] * factor).to(units)\n            return tempVar\n    else:\n        return (value * factor).to(units)\n\n\ndef ndargs(f):\n    """""" Decorator used to non-dimensionalise the arguments of a function""""""\n\n    def convert(obj):\n        if isinstance(obj, (list, tuple)):\n            return type(obj)([convert(val) for val in obj])\n        else:\n            return non_dimensionalise(obj)\n\n    def new_f(*args, **kwargs):\n        nd_args = [convert(arg) for arg in args]\n        nd_kwargs = {name:convert(val) for name, val in kwargs.items()}\n        return f(*nd_args, **nd_kwargs)\n    new_f.__name__ = f.__name__\n    return new_f\n'"
quagmire/scaling/_utils.py,0,"b'##~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~##\n##                                                                                   ##\n##  This file forms part of the Underworld geophysics modelling application.         ##\n##                                                                                   ##\n##  For full license and copyright information, please refer to the LICENSE.md file  ##\n##  located at the project root, or contact the authors.                             ##\n##                                                                                   ##\n##~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~#~##\n\nfrom __future__ import print_function,  absolute_import\nfrom itertools import chain\nfrom collections import OrderedDict\n\ntry:              # Python 2\n    str_base = basestring\n    items = \'iteritems\'\nexcept NameError: # Python 3\n    str_base = str, bytes, bytearray\n    items = \'items\'\n\n_RaiseKeyError = object()  # singleton for no-default behavior\n\n\n\ndef ensure_lower(maybe_str):\n    """"""dict keys can be any hashable object - only call lower if str""""""\n    return maybe_str.lower() if isinstance(maybe_str, str_base) else maybe_str\n\n\ndef ensure_to_base_units(val):\n    return val.to_base_units()\n\n\nclass TransformedDict(dict):  # dicts take a mapping or iterable as their optional first argument\n    __slots__ = () # no __dict__ - that would be redundant\n\n    @staticmethod # because this doesn\'t make sense as a global function.\n    def _process_args(mapping=(), **kwargs):\n        if hasattr(mapping, items):\n            mapping = getattr(mapping, items)()\n        return ((ensure_lower(k), ensure_to_base_units(v)) for k, v in chain(mapping, getattr(kwargs, items)()))\n\n    def __init__(self, mapping=(), **kwargs):\n        super(TransformedDict, self).__init__(self._process_args(mapping, **kwargs))\n\n    def __getitem__(self, k):\n        return super(TransformedDict, self).__getitem__(ensure_lower(k))\n\n    def __setitem__(self, k, v):\n        return super(TransformedDict, self).__setitem__(ensure_lower(k), ensure_to_base_units(v))\n\n    def __delitem__(self, k):\n        return super(TransformedDict, self).__delitem__(ensure_lower(k))\n\n    def get(self, k, default=None):\n        return super(TransformedDict, self).get(ensure_lower(k), default)\n\n    def setdefault(self, k, default=None):\n        return super(TransformedDict, self).setdefault(ensure_lower(k), default)\n\n    def pop(self, k, v=_RaiseKeyError):\n        if v is _RaiseKeyError:\n            return super(TransformedDict, self).pop(ensure_lower(k))\n        return super(TransformedDict, self).pop(ensure_lower(k), v)\n\n    def update(self, mapping=(), **kwargs):\n        super(TransformedDict, self).update(self._process_args(mapping, **kwargs))\n\n    def __contains__(self, k):\n        return super(TransformedDict, self).__contains__(ensure_lower(k))\n\n    def copy(self): # don\'t delegate w/ super - dict.copy() -> dict :(\n        return type(self)(self)\n\n    @classmethod\n    def fromkeys(cls, keys, v=None):\n        return super(TransformedDict, cls).fromkeys((ensure_lower(k) for k in keys), v)\n\n    def _repr_html_(self):\n        attributes  = OrderedDict()\n        attributes[""[mass]""] = self[""[mass]""]\n        attributes[""[length]""] = self[""[length]""]\n        attributes[""[temperature]""] = self[""[temperature]""]\n        attributes[""[time]""] = self[""[time]""]\n        attributes[""[substance]""] = self[""[substance]""]\n        header = ""<table>""\n        footer = ""</table>""\n        html = """"\n        for key, val in attributes.items():\n            html += ""<tr><td>{0}</td><td>{1}</td></tr>"".format(key, val)\n\n        return header + html + footer\n'"
quagmire/tools/__init__.py,0,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nTools for creating and saving meshes\n""""""\n\nfrom .meshtools import *\nfrom .generate_xdmf import generateXdmf as generate_xdmf\nfrom .io import *\n'"
quagmire/tools/generate_xdmf.py,2,"b'#!/usr/bin/env python\nimport h5py\nimport numpy as np\nimport os, sys\n\nclass Xdmf:\n  def __init__(self, filename):\n    self.filename = filename\n    self.cellMap  = {1 : {1 : \'Polyvertex\', 2 : \'Polyline\'}, 2 : {3 : \'Triangle\', 4 : \'Quadrilateral\'}, 3 : {4 : \'Tetrahedron\', 6: \'Wedge\', 8 : \'Hexahedron\'}}\n    self.typeMap  = {\'scalar\' : \'Scalar\', \'vector\' : \'Vector\', \'tensor\' : \'Tensor6\', \'matrix\' : \'Matrix\', \\\n                    b\'scalar\' : \'Scalar\',b\'vector\' : \'Vector\',b\'tensor\' : \'Tensor6\',b\'matrix\' : \'Matrix\'}\n    self.typeExt  = {2 : {\'vector\' : [\'x\', \'y\'], \'tensor\' : [\'xx\', \'yy\', \'xy\']}, 3 : {\'vector\' : [\'x\', \'y\', \'z\'], \'tensor\' : [\'xx\', \'yy\', \'zz\', \'xy\', \'yz\', \'xz\']}}\n    return\n\n  def writeHeader(self, fp, hdfFilename):\n    fp.write(\'\'\'\\\n<?xml version=""1.0"" ?>\n<!DOCTYPE Xdmf SYSTEM ""Xdmf.dtd"" [\n<!ENTITY HeavyData ""%s"">\n]>\n\'\'\' % os.path.basename(hdfFilename))\n    fp.write(\'\\n<Xdmf>\\n  <Domain Name=""domain"">\\n\')\n    return\n\n  def writeCells(self, fp, topologyPath, numCells, numCorners):\n    fp.write(\'\'\'\\\n    <DataItem Name=""cells""\n\t      ItemType=""Uniform""\n\t      Format=""HDF""\n\t      NumberType=""Float"" Precision=""8""\n\t      Dimensions=""%d %d"">\n      &HeavyData;:/%s/cells\n    </DataItem>\n\'\'\' % (numCells, numCorners, topologyPath))\n    return\n\n  def writeVertices(self, fp, geometryPath, numVertices, spaceDim):\n    fp.write(\'\'\'\\\n    <DataItem Name=""vertices""\n\t      Format=""HDF""\n\t      Dimensions=""%d %d"">\n      &HeavyData;:/%s/vertices\n    </DataItem>\n    <!-- ============================================================ -->\n\'\'\' % (numVertices, spaceDim, geometryPath))\n    return\n\n  def writeLocations(self, fp, numParticles, spaceDim):\n    fp.write(\'\'\'\\\n    <DataItem Name=""xcoord""\n\t      Format=""HDF""\n\t      Dimensions=""%d"">\n      &HeavyData;:/particles/xcoord\n    </DataItem>\n\'\'\' % (numParticles))\n    if spaceDim == 1: return\n    fp.write(\'\'\'\\\n    <DataItem Name=""ycoord""\n\t      Format=""HDF""\n\t      Dimensions=""%d"">\n      &HeavyData;:/particles/ycoord\n    </DataItem>\n\'\'\' % (numParticles))\n    if spaceDim == 2: return\n    fp.write(\'\'\'\\\n    <DataItem Name=""zcoord""\n\t      Format=""HDF""\n\t      Dimensions=""%d"">\n      &HeavyData;:/particles/zcoord\n    </DataItem>\n\'\'\' % (numParticles))\n    return\n\n  def writeTimeGridHeader(self, fp, time):\n    fp.write(\'\'\'\\\n    <Grid Name=""TimeSeries"" GridType=""Collection"" CollectionType=""Temporal"">\n      <Time TimeType=""List"">\n        <DataItem Format=""XML"" NumberType=""Float"" Dimensions=""%d"">\n          \'\'\' % (len(time)))\n    fp.write(\' \'.join(map(str, list(map(float, time)))))\n    fp.write(\'\'\'\n        </DataItem>\n      </Time>\n\'\'\')\n    return\n\n  def writeSpaceGridHeader(self, fp, numCells, numCorners, cellDim, spaceDim):\n    fp.write(\'\'\'\\\n      <Grid Name=""domain"" GridType=""Uniform"">\n\t<Topology\n\t   TopologyType=""%s""\n\t   NumberOfElements=""%d"">\n\t  <DataItem Reference=""XML"">\n\t    /Xdmf/Domain/DataItem[@Name=""cells""]\n\t  </DataItem>\n\t</Topology>\n\t<Geometry GeometryType=""%s"">\n\t  <DataItem Reference=""XML"">\n\t    /Xdmf/Domain/DataItem[@Name=""vertices""]\n\t  </DataItem>\n\t</Geometry>\n\'\'\' % (self.cellMap[cellDim][numCorners], numCells, ""XYZ"" if spaceDim > 2 else ""XY""))\n    return\n\n  def writeFieldSingle(self, fp, numSteps, timestep, spaceDim, name, f, domain):\n    if len(f[1].shape) > 2:\n      dof = f[1].shape[1]\n      bs  = f[1].shape[2]\n    elif len(f[1].shape) > 1:\n      if numSteps > 1:\n        dof = f[1].shape[1]\n        bs  = 1\n      else:\n        dof = f[1].shape[0]\n        bs  = f[1].shape[1]\n    else:\n      dof = f[1].shape[0]\n      bs  = 1\n    fp.write(\'\'\'\\\n\t<Attribute\n\t   Name=""%s""\n\t   Type=""%s""\n\t   Center=""%s"">\n          <DataItem ItemType=""HyperSlab""\n\t\t    Dimensions=""1 %d %d""\n\t\t    Type=""HyperSlab"">\n            <DataItem\n\t       Dimensions=""3 3""\n\t       Format=""XML"">\n              %d 0 0\n              1 1 1\n              1 %d %d\n\t    </DataItem>\n\t    <DataItem\n\t       DataType=""Float"" Precision=""8""\n\t       Dimensions=""%d %d %d""\n\t       Format=""HDF"">\n\t      %s\n\t    </DataItem>\n\t  </DataItem>\n\t</Attribute>\n\'\'\' % (f[0], self.typeMap[f[1].attrs[\'vector_field_type\']], domain, dof, bs, timestep, dof, bs, numSteps, dof, bs, name))\n    return\n\n  def writeFieldComponents(self, fp, numSteps, timestep, spaceDim, name, f, domain):\n    vtype = f[1].attrs[\'vector_field_type\']\n    if len(f[1].shape) > 2:\n      dof    = f[1].shape[1]\n      bs     = f[1].shape[2]\n      cdims  = \'1 %d 1\' % dof\n      dims   = \'%d %d %d\' % (numSteps, dof, bs)\n      stride = \'1 1 1\'\n      size   = \'1 %d 1\' % dof\n    else:\n      dof    = f[1].shape[0]\n      bs     = f[1].shape[1]\n      cdims  = \'%d 1\' % dof\n      dims   = \'%d %d\' % (dof, bs)\n      stride = \'1 1\'\n      size   = \'%d 1\' % dof\n    for c in range(bs):\n      ext = self.typeExt[spaceDim][vtype][c]\n      if len(f[1].shape) > 2: start  = \'%d 0 %d\' % (timestep, c)\n      else:                   start  = \'0 %d\' % c\n      fp.write(\'\'\'\\\n\t<Attribute\n\t   Name=""%s""\n\t   Type=""Scalar""\n\t   Center=""%s"">\n          <DataItem ItemType=""HyperSlab""\n\t\t    Dimensions=""%s""\n\t\t    Type=""HyperSlab"">\n            <DataItem\n\t       Dimensions=""3 %d""\n\t       Format=""XML"">\n              %s\n              %s\n              %s\n\t    </DataItem>\n\t    <DataItem\n\t       DataType=""Float"" Precision=""8""\n\t       Dimensions=""%s""\n\t       Format=""HDF"">\n\t      %s\n\t    </DataItem>\n\t  </DataItem>\n\t</Attribute>\n\'\'\' % (f[0]+\'_\'+ext, domain, cdims, len(f[1].shape), start, stride, size, dims, name))\n    return\n\n  def writeField(self, fp, numSteps, timestep, cellDim, spaceDim, name, f, domain):\n    ctypes = [\'tensor\', \'matrix\']\n    if spaceDim == 2 or cellDim != spaceDim: ctypes.append(\'vector\')\n    if f[1].attrs[\'vector_field_type\'] in ctypes:\n      self.writeFieldComponents(fp, numSteps, timestep, spaceDim, name, f, domain)\n    else:\n      self.writeFieldSingle(fp, numSteps, timestep, spaceDim, name, f, domain)\n    return\n\n  def writeSpaceGridFooter(self, fp):\n    fp.write(\'      </Grid>\\n\')\n    return\n\n  def writeParticleGridHeader(self, fp, numParticles, spaceDim):\n    fp.write(\'\'\'\\\n      <Grid Name=""particle_domain"" GridType=""Uniform"">\n\t<Topology TopologyType=""Polyvertex"" NodesPerElement=""%d"" />\n\t<Geometry GeometryType=""%s"">\n\t  <DataItem Reference=""XML"">/Xdmf/Domain/DataItem[@Name=""xcoord""]</DataItem>\n\t  <DataItem Reference=""XML"">/Xdmf/Domain/DataItem[@Name=""ycoord""]</DataItem>\n\t  %s\n\t</Geometry>\n\'\'\' % (numParticles, ""X_Y_Z"" if spaceDim > 2 else ""X_Y"", ""<DataItem Reference=\\""XML\\"">/Xdmf/Domain/DataItem[@Name=\\""zcoord\\""]</DataItem>"" if spaceDim > 2 else """"))\n    return\n\n  def writeParticleField(self, fp, numParticles, spaceDim):\n    fp.write(\'\'\'\\\n    <Attribute Name=""particles/icoord"">\n      <DataItem Name=""icoord""\n                Format=""HDF""\n                Dimensions=""%d"">\n                &HeavyData;:/particles/icoord\n      </DataItem>\n    </Attribute>\'\'\' % (numParticles))\n    return\n\n  def writeTimeGridFooter(self, fp):\n    fp.write(\'    </Grid>\\n\')\n    return\n\n  def writeFooter(self, fp):\n    fp.write(\'  </Domain>\\n</Xdmf>\\n\')\n    return\n\n  def write(self, hdfFilename, hdfmeshFilename, topologyPath, numCells, numCorners, cellDim, geometryPath, numVertices, spaceDim, time, vfields, cfields, numParticles):\n    useTime = not (len(time) < 2 and time[0] == -1)\n    with open(self.filename, \'w\') as fp:\n      self.writeHeader(fp, hdfmeshFilename)\n      # Field information\n      self.writeCells(fp, topologyPath, numCells, numCorners)\n      self.writeVertices(fp, geometryPath, numVertices, spaceDim)\n      if useTime: self.writeTimeGridHeader(fp, time)\n      for t in range(len(time)):\n        self.writeSpaceGridHeader(fp, numCells, numCorners, cellDim, spaceDim)\n        for vf in vfields: self.writeField(fp, len(time), t, cellDim, spaceDim, hdfFilename+\':/vertex_fields/\'+vf[0], vf, \'Node\')\n        for cf in cfields: self.writeField(fp, len(time), t, cellDim, spaceDim, hdfFilename+\':/cell_fields/\'+cf[0], cf, \'Cell\')\n        self.writeSpaceGridFooter(fp)\n      if useTime: self.writeTimeGridFooter(fp)\n      if numParticles:\n        self.writeLocations(fp, numParticles, spaceDim)\n        if useTime: self.writeTimeGridHeader(fp, time)\n        for t in range(len(time)):\n          self.writeParticleGridHeader(fp, numParticles, spaceDim)\n          self.writeSpaceGridFooter(fp)\n        if useTime: self.writeTimeGridFooter(fp)\n      self.writeFooter(fp)\n    return\n\ndef generate_dmplex_xdmf(hdfFilename, xdmfFilename=None):\n  if xdmfFilename is None:\n    xdmfFilename = os.path.splitext(hdfFilename)[0] + \'.xdmf\'\n  # Read mesh\n  h5          = h5py.File(hdfFilename, \'r\')\n  if \'viz\' in h5 and \'geometry\' in h5[\'viz\']:\n    geomPath  = \'viz/geometry\'\n    geom      = h5[\'viz\'][\'geometry\']\n  else:\n    geomPath  = \'geometry\'\n    geom      = h5[\'geometry\']\n  if \'viz\' in h5 and \'topology\' in h5[\'viz\']:\n    topoPath  = \'viz/topology\'\n    topo      = h5[\'viz\'][\'topology\']\n  else:\n    topoPath  = \'topology\'\n    topo      = h5[\'topology\']\n  vertices    = geom[\'vertices\']\n  numVertices = vertices.shape[0]\n  spaceDim    = vertices.shape[1]\n  cells       = topo[\'cells\']\n  numCells    = cells.shape[0]\n  numCorners  = cells.shape[1]\n  cellDim     = topo[\'cells\'].attrs[\'cell_dim\']\n  if \'time\' in h5:\n    time      = np.array(h5[\'time\']).flatten()\n  else:\n    time      = [-1]\n  vfields     = []\n  cfields     = []\n  if \'vertex_fields\' in h5: vfields = list(h5[\'vertex_fields\'].items())\n  if \'cell_fields\' in h5: cfields = list(h5[\'cell_fields\'].items())\n  numParticles = 0\n  if \'particles\' in h5: numParticles = h5[\'particles\'][\'xcoord\'].shape[0]\n\n  # Write Xdmf\n  Xdmf(xdmfFilename).write(hdfFilename, hdfFilename, topoPath, numCells, numCorners, cellDim, geomPath, numVertices, spaceDim, time, vfields, cfields, numParticles)\n  h5.close()\n  return\n\n\ndef generate_dmplex_field_xdmf(hdfFilename, hdfmeshFilename, xdmfFilename=None):\n  if xdmfFilename is None:\n    xdmfFilename = os.path.splitext(hdfFilename)[0] + \'.xdmf\'\n\n  # Read mesh\n  h5          = h5py.File(hdfFilename, \'r\')\n  h5m         = h5py.File(hdfmeshFilename, \'r\')\n  if \'viz\' in h5m and \'geometry\' in h5m[\'viz\']:\n    geomPath  = \'viz/geometry\'\n    geom      = h5m[\'viz\'][\'geometry\']\n  else:\n    geomPath  = \'geometry\'\n    geom      = h5m[\'geometry\']\n  if \'viz\' in h5m and \'topology\' in h5m[\'viz\']:\n    topoPath  = \'viz/topology\'\n    topo      = h5m[\'viz\'][\'topology\']\n  else:\n    topoPath  = \'topology\'\n    topo      = h5m[\'topology\']\n  vertices    = geom[\'vertices\']\n  numVertices = vertices.shape[0]\n  spaceDim    = vertices.shape[1]\n  cells       = topo[\'cells\']\n  numCells    = cells.shape[0]\n  numCorners  = cells.shape[1]\n  cellDim     = topo[\'cells\'].attrs[\'cell_dim\']\n\n  # Read fields\n  if \'time\' in h5:\n    time      = np.array(h5[\'time\']).flatten()\n  else:\n    time      = [-1]\n  vfields     = []\n  cfields     = []\n  if \'vertex_fields\' in h5: vfields = list(h5[\'vertex_fields\'].items())\n  if \'cell_fields\' in h5: cfields = list(h5[\'cell_fields\'].items())\n  numParticles = 0\n  if \'particles\' in h5: numParticles = h5[\'particles\'][\'xcoord\'].shape[0]\n\n  # Write Xdmf\n  Xdmf(xdmfFilename).write(hdfFilename, hdfmeshFilename, topoPath, numCells, numCorners, cellDim, geomPath, numVertices, spaceDim, time, vfields, cfields, numParticles)\n  h5.close()\n  h5m.close()\n  return\n\n\ndef generate_dmda_xdmf(hdfFilename, xdmfFilename=None):\n  """"""\n  Generate a XDMF file to visualise HDF5 fields and vectors in Paraview.\n\n  Writes a XDMF file to the working directory.\n  """"""\n\n  if xdmfFilename is None:\n    xdmfFilename = os.path.splitext(hdfFilename)[0] + \'.xdmf\'\n\n  f = open(xdmfFilename, \'w\')\n\n  def write_header(f):\n    f.write(\'\'\'<?xml version=""1.0"" ?>\\n\\\n<!DOCTYPE Xdmf SYSTEM ""Xdmf.dtd"" []>\\n\\\n<Xdmf xmlns:xi=""http://www.w3.org/2003/XInclude"" Version=""2.2"">\\n\\\n  <Information Name=""SampleLocation"" Value=""4""/>\\n\\\n  <Domain>\\n\\\n    <Grid Name=""Structured Grid"" GridType=""Uniform"">\\n\'\'\')\n\n  def write_footer(f):\n    f.write(\'\'\'    </Grid>\\n  </Domain>\\n</Xdmf>\'\'\')\n\n  def write_geometry(f, dim, shape, origin, stride):\n    f.write(\'\'\'      <Topology TopologyType=""2DCORECTMesh"" NumberOfElements=""{1}""/>\\n\\\n  <Geometry GeometryType=""ORIGIN_DXDY"">\\n\\\n    <DataItem Dimensions=""{0}"" NumberType=""Float"" Precision=""4"" Format=""XML"">\\n\\\n      {2}\\n\\\n    </DataItem>\\n\\\n    <DataItem Dimensions=""{0}"" NumberType=""Float"" Precision=""4"" Format=""XML"">\\n\\\n      {3}\\n\\\n    </DataItem>\\n\\\n  </Geometry>\\n\'\'\'.format(dim, shape, origin, stride))\n\n  def write_attribute(f, attributeName, arrtype, dshape, dpath):\n    f.write(\'\'\'      <Attribute Name=""{0}"" AttributeType=""{1}"" Center=""Node"">\'\\n\\\n    <DataItem Dimensions=""{2}"" NumberType=""Float"" Precision=""4"" Format=""HDF"">{3}</DataItem>\\n\\\n  </Attribute>\\n\'\'\'.format(attributeName, arrtype, dshape, dpath))\n\n  def array_to_string(array):\n    s = """"\n    for i in array:\n        s += ""{} "".format(i)\n    return s\n\n\n  h5file = h5py.File(hdfFilename, \'r\')\n  basename = os.path.basename(hdfFilename)\n\n  # get topology attributes\n  geom = h5file[\'geometry\']\n  minX = geom.attrs[\'minX\']\n  maxX = geom.attrs[\'maxX\']\n  minY = geom.attrs[\'minY\']\n  maxY = geom.attrs[\'maxY\']\n  resX = geom.attrs[\'resX\']\n  resY = geom.attrs[\'resY\']\n  \n  minCoords = minX, minY\n  maxCoords = maxX, maxY\n  \n  shape = resY, resX\n  nodes = resX*resY\n\n  stride = [(maxY - minY)/(resY-1), (maxX - minX)/(resX-1)]\n\n  tshape = array_to_string(shape)\n  torigin = array_to_string(minCoords)\n  tstride = array_to_string(stride)\n\n  write_header(f)\n  write_geometry(f, len(shape), tshape, torigin, tstride)\n\n  for key in h5file:\n    dset = h5file[key]\n\n    # We only want datasets, topology group is not required\n    if type(dset) is h5py.Dataset:\n      # if all(dset.shape != shape):\n      #     raise ValueError(\'Dataset should be of shape {}\'.format(shape))\n\n      dpath = basename + "":"" + dset.name\n      dname = dset.name[1:]\n      dshape = array_to_string(dset.shape)\n\n      dnodes = 1\n      for n in dset.shape:\n        dnodes *= n\n\n      if dnodes%nodes != 0:\n        raise ValueError(""Dataset {} is not a valid shape"".format(dname))\n      elif dnodes/nodes > 1:\n        arrtype = ""Vector""\n      elif dnodes/nodes == 1:\n        arrtype = ""Scalar""\n\n      write_attribute(f, dname, arrtype, dshape, dpath)\n\n  write_footer(f)\n  f.close()\n  h5file.close()\n  return\n\n\ndef generateXdmf(hdfFilename, hdfmeshFilename=None, xdmfFilename=None):\n\n  if xdmfFilename is None:\n    xdmfFilename = os.path.splitext(hdfFilename)[0] + \'.xdmf\'\n\n\n  DMPLEX = False\n  with h5py.File(hdfFilename, \'r\') as h5:\n    if \'vertex_fields\' in h5:\n      DMPLEX = True\n      if \'geometry\' not in h5 and hdfmeshFilename is None:\n        err_msg = ""mesh info not found in HDF5, save mesh info to this HDF5 file""\n        err_msg += "" or point to an externl HDF5 file containing the mesh info.""\n        raise TypeError(err_msg)\n\n\n  if DMPLEX:\n    if hdfmeshFilename is None:\n      generate_dmplex_xdmf(hdfFilename, xdmfFilename)\n    else:\n      generate_dmplex_field_xdmf(hdfFilename, hdfmeshFilename, xdmfFilename)\n\n  else:\n    generate_dmda_xdmf(hdfFilename, xdmfFilename)\n\nif __name__ == \'__main__\':\n  for f in sys.argv[1:]:\n    generateXdmf(f)\n'"
quagmire/tools/io.py,0,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom .meshtools import create_DMPlex_from_hdf5 as _create_DMPlex_from_hdf5\nfrom .meshtools import create_DMDA as _create_DMDA\nfrom mpi4py import MPI as _MPI\n_comm = _MPI.COMM_WORLD\n\ndef load_quagmire_project(filename):\n    """"""\n    Load a Quagmire project from a HDF5 file.\n\n    Detects which mesh object was saved, i.e.\n\n    - `quagmire.FlatMesh`\n    - `quagmire.TopoMesh`\n    - `quagmire.SurfaceProcessMesh`\n\n    and rebuilds all data structures onto the mesh object.\n\n    Parameters\n    ----------\n    filename : str\n        path of the HDF5 from which to load the Quagmire project.\n\n    Returns\n    -------\n    mesh : object\n        Quagmire mesh object. One of:\n\n        - `quagmire.FlatMesh`\n        - `quagmire.TopoMesh`\n        - `quagmire.SurfaceProcessMesh`\n    """"""\n\n    from quagmire import QuagMesh\n    import h5py\n\n    filename = str(filename)\n    if not filename.endswith(\'.h5\'):\n        filename += \'.h5\'\n\n    with h5py.File(filename, mode=\'r\', driver=\'mpio\', comm=_comm) as h5:\n        quag = h5[\'quagmire\']\n        verbose         = quag.attrs[\'verbose\']\n        mesh_id         = quag.attrs[\'id\']\n        radius          = quag.attrs[\'radius\']\n        down_neighbours = quag.attrs[\'downhill_neighbours\']\n        topo_modified   = quag.attrs[\'topography_modified\']\n\n        if mesh_id.startswith(\'pixmesh\'):\n            geom = h5[\'geometry\']\n            minX, maxX = geom.attrs[\'minX\'], geom.attrs[\'maxX\']\n            minY, maxY = geom.attrs[\'minY\'], geom.attrs[\'maxY\']\n            resX, resY = geom.attrs[\'resX\'], geom.attrs[\'resY\']\n\n\n    # create DM object\n    if mesh_id.startswith(""pixmesh""):\n        DM = _create_DMDA(minX, maxX, minY, maxY, resX, resY)\n    else:\n        DM = _create_DMPlex_from_hdf5(filename)\n\n\n    mesh = QuagMesh(DM, downhill_neighbours=down_neighbours, verbose=verbose)\n    mesh.__id = mesh_id\n    mesh._topography_modified_count = topo_modified\n    if mesh.id.startswith(\'strimesh\'):    \n        if not radius:\n            radius_meshVariable = mesh.add_variable(""radius"")\n            radius_meshVariable.load(filename)\n            radius = radius_meshVariable.data\n        mesh.radius = radius\n\n    mesh.topography.unlock()\n    mesh.topography.load(filename)\n    mesh.topography.lock()\n\n    if mesh._topography_modified_count > 0:\n        # this should trigger a rebuild of downhill matrices\n        mesh.downhill_neighbours = down_neighbours\n\n\n    print(""Quagmire project is successfully rebuilt on {}"".format(mesh.id))\n\n    return mesh\n\n\ndef load_saved_MeshVariables(mesh, filename, mesh_variable_list):\n    """"""\n    Loads all mesh variables saved onto the HDF5 file.\n\n    Parameters\n    ----------\n    mesh : object\n        Quagmire mesh object\n    filename : str\n        path of the HDF5 from which to load the mesh variables.\n    mesh_variable_list : list\n        list of mesh variables to load from the HDF5 file\n        each entry should be a string.\n    """"""\n\n    MeshVariable_list = []\n    for field_name in mesh_variable_list:\n        mvar = mesh.add_variable(field_name)\n        mvar.load(filename)\n        MeshVariable_list.append(mvar)\n\n    return MeshVariable_list\n'"
quagmire/tools/meshtools.py,68,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\n""""""\nTools for creating Quagmire meshes.\n\nQuagmire constructs meshes as `PETSc DMPlex` and `DMDA` objects.\n\n## Regular Meshes\n\nCreate a __regularly-spaced Cartesian__ grid with `PETSc DMDA` using:\n\n- `create_DMDA`\n\n## Unstructured Cartesian meshes\n\nCreate an __unstructured Cartesian__ mesh with `PETSc DMPlex` using:\n\n- `create_DMPlex`\n- `create_DMPlex_from_points`\n- `create_DMPlex_from_box`\n\n## Unstructured Spherical meshes\n\nCreate an __unstructured Spherical__ mesh with `PETSc DMPlex` using:\n\n- `create_spherical_DMPlex`\n- `create_DMPlex_from_spherical_points`\n\nReconstruct a mesh that has been saved to an HDF5 file:\n\n- `create_DMPlex_from_hdf5`\n\n\n### Additional tools\n\nRefine any mesh with `refine_DM`. This adds the midpoint of all line\nsegments to the mesh.\n\nSave any mesh to an HDF5 file with `save_DM_to_hdf5`.\n\n""""""\n\nimport numpy as _np\n\ntry: range = xrange\nexcept: pass\n\n\ndef points_to_edges(tri, boundary):\n    """"""\n    Finds the edges connecting two boundary points\n    """"""\n    i1 = _np.sort([tri.simplices[:,0], tri.simplices[:,1]], axis=0)\n    i2 = _np.sort([tri.simplices[:,0], tri.simplices[:,2]], axis=0)\n    i3 = _np.sort([tri.simplices[:,1], tri.simplices[:,2]], axis=0)\n\n    a = _np.hstack([i1, i2, i3]).T\n\n    # find unique rows in numpy array\n    edges = _np.unique(a, axis=0)\n\n    # label where boundary nodes are\n    ix = _np.in1d(edges.ravel(), boundary).reshape(edges.shape)\n    boundary2 = ix.sum(axis=1)\n\n    # both points are boundary points that share the line segment\n    boundary_edges = edges[boundary2==2]\n    return boundary_edges\n\n\ndef find_boundary_segments(simplices):\n    """"""\n    Finds all boundary segments in the triangulation.\n\n    Boundary segments should not share a segment with another triangle.\n\n    Parameters\n    ----------\n    simplices : array shape (nt,3)\n        list of simplices in a triangulaton.\n\n    Returns\n    -------\n    segments : array shape (n,2)\n        list of segments that define the convex hull (boundary)\n        of the triangulation.\n\n    Notes\n    -----\n    Spherical meshes generated with stripy will not have any boundary\n    segments, thus an empty list is returned.\n    """"""\n    i1 = _np.sort([simplices[:,0], simplices[:,1]], axis=0)\n    i2 = _np.sort([simplices[:,0], simplices[:,2]], axis=0)\n    i3 = _np.sort([simplices[:,1], simplices[:,2]], axis=0)\n    \n    a = _np.hstack([i1, i2, i3]).T\n\n    # find unique rows in numpy array\n    edges, counts = _np.unique(a, return_counts=True, axis=0)\n    return edges[counts < 2]\n\n\ndef create_DMPlex_from_points(x, y, bmask=None, refinement_levels=0):\n    """"""\n    Triangulates x,y coordinates on rank 0 and creates a PETSc DMPlex object\n    from the cells and vertices to distribute among processors.\n\n    Parameters\n    ----------\n    x : array of floats, shape (n,)\n        x coordinates\n    y : array of floats, shape (n,)\n        y coordinates\n    bmask : array of bools, shape (n,)\n        boundary mask where points along the boundary\n        equal False, and the interior equal True\n        if bmask=None (default) then the convex hull of points is used\n    refinement_levels : int\n        number of iterations to refine the mesh (default: 0)\n\n    Returns\n    -------\n    DM : PETSc DMPlex object\n\n    Notes\n    -----\n    `x` and `y` are shuffled on input to aid triangulation efficiency\n\n    Refinement adds the midpoints of every line segment to the DM.\n    Boundary markers are automatically updated with each iteration.\n    """"""\n    from stripy import Triangulation\n\n    tri = Triangulation(x,y, permute=True)\n\n    if bmask is None:\n        hull = tri.convex_hull()\n        boundary_vertices = _np.column_stack([hull, _np.hstack([hull[1:], hull[0]])])\n    else:\n        boundary_indices = _np.nonzero(~bmask)[0]\n        boundary_vertices = points_to_edges(tri, boundary_indices)\n\n    return _create_DMPlex(tri.points, tri.simplices, boundary_vertices, refinement_levels)\n\n\ndef create_DMPlex_from_spherical_points(lons, lats, bmask=None, refinement_levels=0):\n    """"""\n    Triangulates lon,lat coordinates on rank 0 and creates a PETSc DMPlex object\n    from the cells and vertices to distribute among processors.\n\n    Parameters\n    ----------\n    lons : array of floats, shape (n,)\n        longitudinal coordinates in radians\n    lats : array of floats, shape (n,)\n        latitudinal coordinates in radians\n    bmask : array of bools, shape (n,)\n        boundary mask where points along the boundary\n        equal False, and the interior equal True\n        if bmask=None (default) then the convex hull of points is used\n    refinement_levels : int\n        number of iterations to refine the mesh (default: 0)\n\n    Returns\n    -------\n    DM : PETSc DMPlex object\n\n    Notes\n    -----\n    `lons` and `lats` are shuffled on input to aid triangulation efficiency\n\n    Refinement adds the midpoints of every line segment to the DM.\n    Boundary markers are automatically updated with each iteration.\n\n    """"""\n    from stripy import sTriangulation\n\n    tri = sTriangulation(lons, lats, permute=True)\n\n    if bmask is None:\n        boundary_vertices = find_boundary_segments(tri.simplices)\n    else:\n        boundary_indices = _np.nonzero(~bmask)[0]\n        boundary_vertices = points_to_edges(tri, boundary_indices)\n\n    return _create_DMPlex(tri.points, tri.simplices, boundary_vertices, refinement_levels)\n\n\n\ndef set_DMPlex_boundary_points(dm):\n    """"""\n    Finds the points that join the edges that have been\n    marked as ""boundary"" faces in the DAG then sets them\n    as boundaries.\n    """"""\n\n    pStart, pEnd = dm.getDepthStratum(0) # points\n    eStart, eEnd = dm.getDepthStratum(1) # edges\n    edgeIS = dm.getStratumIS(\'boundary\', 1)\n\n    if eEnd == eStart:\n        ## CAUTION: don\'t do this if any of the dm calls have barriers\n        return\n\n    edge_mask = _np.logical_and(edgeIS.indices >= eStart, edgeIS.indices < eEnd)\n    boundary_edges = edgeIS.indices[edge_mask]\n\n    # query the DAG for points that join an edge\n    for edge in boundary_edges:\n        vertices = dm.getCone(edge)\n        # mark the boundary points\n        for vertex in vertices:\n            dm.setLabelValue(""boundary"", vertex, 1)\n\n    return\n\ndef set_DMPlex_boundary_points_and_edges(dm, boundary_vertices):\n    """""" Label boundary points and edges """"""\n\n    from petsc4py import PETSc\n\n    if _np.ndim(boundary_vertices) != 2 and _np.shape(boundary_vertices)[1] != 2:\n        raise ValueError(""boundary vertices must be of shape (n,2)"")\n\n    # points in DAG\n    pStart, pEnd = dm.getDepthStratum(0)\n\n    if pStart == pEnd:\n        ## CAUTION not if there is a barrier in any of the dm calls that lie ahead.\n        return\n\n    # convert to DAG ordering\n    boundary_edges = _np.array(boundary_vertices + pStart, dtype=PETSc.IntType)\n    boundary_indices = _np.array(_np.unique(boundary_edges), dtype=PETSc.IntType)\n\n    # mark edges\n    for edge in boundary_edges:\n        # join is the common edge to which they are connected\n        join = dm.getJoin(edge)\n        for j in join:\n            dm.setLabelValue(""boundary"", j, 1)\n\n    # mark points\n    for ind in boundary_indices:\n        dm.setLabelValue(""boundary"", ind, 1)\n\ndef get_boundary_points(dm):\n\n    pStart, pEnd = dm.getDepthStratum(0) # points\n    eStart, eEnd = dm.getDepthStratum(1) # edges\n    edgeIS = dm.getStratumIS(\'boundary\', 1)\n\n    edge_mask = _np.logical_and(edgeIS.indices >= eStart, edgeIS.indices < eEnd)\n    boundary_edges = edgeIS.indices[edge_mask]\n\n    boundary_vertices = _np.empty((boundary_edges.size,2), dtype=PETSc.IntType)\n\n    # query the DAG for points that join an edge\n    for idx, edge in enumerate(boundary_edges):\n        boundary_vertices[idx] = dm.getCone(edge)\n\n    # convert to local point ordering\n    boundary_vertices -= pStart\n    return _np.unique(boundary_vertices)\n\n\n\ndef create_DMPlex_from_hdf5(file):\n    """"""\n    Creates a DMPlex object from an HDF5 file.\n    This is useful for rebuilding a mesh that is saved from a\n    previous simulation.\n\n    Parameters\n    ----------\n    file : string\n        point to the location of hdf5 file\n\n    Returns\n    -------\n    DM : PETSc DMPlex object\n\n    Notes\n    -----\n    This function requires petsc4py >= 3.8\n    """"""\n    from petsc4py import PETSc\n\n    file = str(file)\n    if not file.endswith(\'.h5\'):\n        file += \'.h5\'\n\n    dm = PETSc.DMPlex().createFromFile(file)\n\n    # define one DoF on the nodes\n    dm.setNumFields(1)\n    origSect = dm.createSection(1, [1,0,0])\n    origSect.setFieldName(0, ""points"")\n    origSect.setUp()\n    dm.setDefaultSection(origSect)\n\n    origVec = dm.createGlobalVector()\n\n    if PETSc.COMM_WORLD.size > 1:\n        # Distribute to other processors\n        sf = dm.distribute(overlap=1)\n        newSect, newVec = dm.distributeField(sf, origSect, origVec)\n        dm.setDefaultSection(newSect)\n\n    return dm\n\n\ndef create_DMPlex_from_box(minX, maxX, minY, maxY, resX, resY, refinement=None):\n    """"""\n    Create a box and fill with triangles up to a specified refinement\n\n    Parameters\n    ----------\n    minX : float\n        minimum x-coordinate\n    maxX : float\n        maximum x-coordinate\n    minY : float\n        minimum y-coordinate\n    maxY : float\n        maximum y-coordinate\n    resX : float\n        resolution in the x direction\n    resY : float\n        resolution in the y direction\n    refinement : float (default: None)\n        (optional) set refinement limit\n\n    Returns\n    -------\n    DM : PETSc DMPlex object\n\n    Notes\n    -----\n    This only works if PETSc was configured with triangle\n    """"""\n    from petsc4py import PETSc\n\n    nx = int((maxX - minX)/resX)\n    ny = int((maxY - minY)/resY)\n\n    dm = PETSc.DMPlex().create()\n    dm.setDimension(1)\n    boundary = dm.createSquareBoundary([minX,minY], [maxX,maxY], [nx,ny])\n    dm.generate(boundary, name=\'triangle\')\n    if refinement:\n        dm.setRefinementLimit(refinement) # Maximum cell volume\n        dm = dm.refine()\n    dm.markBoundaryFaces(\'boundary\')\n\n    pStart, pEnd = dm.getChart()\n\n    dm.setNumFields(1)\n    origSect = dm.createSection(1, [1,0,0])\n    origSect.setFieldName(0, ""points"")\n    origSect.setUp()\n    dm.setDefaultSection(origSect)\n\n    origVec = dm.createGlobalVec()\n\n    if PETSc.COMM_WORLD.size > 1:\n        sf = dm.distribute(overlap=1)\n        newSect, newVec = dm.distributeField(sf, origSect, origVec)\n        dm.setDefaultSection(newSect)\n\n    return dm\n\n\ndef create_DMDA(minX, maxX, minY, maxY, resX, resY):\n    """"""\n    Create a PETSc DMDA mesh object from the bounding box of the\n    regularly-spaced grid.\n\n    Parameters\n    ----------\n    minX : float\n        minimum x-coordinate\n    maxX : float\n        maximum x-coordinate\n    minY : float\n        minimum y-coordinate\n    maxY : float\n        maximum y-coordinate\n    resX : float\n        resolution in the x direction\n    resY : float\n        resolution in the y direction\n    refinement : float (default: None)\n        (optional) set refinement limit\n\n    Returns\n    -------\n    DM : PETSc DMDA object\n    """"""\n    from petsc4py import PETSc\n\n    dx = (maxX - minX)/resX\n    dy = (maxY - minY)/resY\n\n    if dx != dy:\n        raise ValueError(""Spacing must be uniform in x and y directions [{:.4f}, {:.4f}]"".format(dx,dy))\n\n    dim = 2\n    dm = PETSc.DMDA().create(dim, sizes=(resX, resY), stencil_width=1)\n    dm.setUniformCoordinates(minX, maxX, minY, maxY)\n    dm.createLabel(""PixMesh"")\n    return dm\n\n\ndef _create_DMPlex(points, simplices, boundary_vertices=None, refinement_levels=0):\n    """"""\n    Create a PETSc DMPlex object on root processor\n    and distribute to other processors\n\n    Parameters\n    ----------\n     points : array of floats, shape (n,dim) coordinates\n     simplices : connectivity of the mesh\n     boundary_vertices : array of ints, shape(l,2)\n        (optional) boundary edges\n\n    Returns\n    -------\n     DM : PETSc DMPlex object\n    """"""\n    from petsc4py import PETSc\n\n    ndim = _np.shape(points)[1]\n    mesh_type = {2: ""TriMesh"", 3: ""sTriMesh""}\n\n    if PETSc.COMM_WORLD.rank == 0:\n        coords = _np.array(points, dtype=_np.float)\n        cells  = simplices.astype(PETSc.IntType)\n    else:\n        coords = _np.zeros((0,ndim), dtype=_np.float)\n        cells  = _np.zeros((0,3), dtype=PETSc.IntType)\n\n    dim = 2\n    dm = PETSc.DMPlex().createFromCellList(dim, cells, coords)\n\n    # create labels\n    # these can be accessed with dm.getLabelName(i) where i=0,1,2\n    # and the last label added is the 0-th index.\n    dm.createLabel(""boundary"")\n    dm.createLabel(""coarse"")\n    dm.createLabel(mesh_type[ndim])\n\n    ## label boundary\n    if boundary_vertices is None:\n        # boundary is convex hull\n        # mark edges and points\n\n        dm.markBoundaryFaces(""boundary"")\n        set_DMPlex_boundary_points(dm)\n\n    else:\n        # boundary is convex hull\n        set_DMPlex_boundary_points_and_edges(dm, boundary_vertices)\n\n\n    ## label coarse DM in case it is ever needed again\n    pStart, pEnd = dm.getDepthStratum(0)\n    for pt in range(pStart, pEnd):\n        dm.setLabelValue(""coarse"", pt, 1)\n\n    # define one DoF on the nodes\n    dm.setNumFields(1)\n    origSect = dm.createSection(1, [1,0,0])\n    origSect.setFieldName(0, ""points"")\n    origSect.setUp()\n    dm.setDefaultSection(origSect)\n\n    origVec = dm.createGlobalVector()\n\n    if PETSc.COMM_WORLD.size > 1:\n        # Distribute to other processors\n        sf = dm.distribute(overlap=1)\n        newSect, newVec = dm.distributeField(sf, origSect, origVec)\n        dm.setDefaultSection(newSect)\n\n    # parallel mesh refinement\n    dm = refine_DM(dm, refinement_levels)\n\n    return dm\n\n\ndef create_DMPlex(x, y, simplices, boundary_vertices=None, refinement_levels=0):\n    """"""\n    Create a PETSc DMPlex object on root processor\n    and distribute to other processors\n\n    Parameters\n    ----------\n    x : array of floats shape (n,)\n        x coordinates\n    y : array of floats shape (n,)\n        y coordinates\n    simplices : array of ints shape (nt,3)\n        connectivity of the mesh\n    boundary_vertices : array of ints, shape(l,2)\n        (optional) boundary edges\n\n    Returns\n    -------\n    DM : PETSc DMPlex object\n    """"""\n    points = _np.c_[x,y]\n    return _create_DMPlex(points, simplices, boundary_vertices, refinement_levels)\n\n\ndef create_spherical_DMPlex(lons, lats, simplices, boundary_vertices=None, refinement_levels=0):\n    """"""\n    Create a PETSc DMPlex object on root processor\n    and distribute to other processors\n\n    Parameters\n    ----------\n    lons : array of floats shape (n,)\n        longitudinal coordinates\n    lats : array of floats shape (n,)\n        latitudinal coordinates\n    simplices : connectivity of the mesh\n    boundary_vertices : array of ints, shape(l,2)\n        (optional) boundary edges\n\n    Returns\n    -------\n    DM : PETSc DMPlex object\n    """"""\n    from stripy.spherical import lonlat2xyz\n\n    # convert to xyz to construct the DM\n    x,y,z = lonlat2xyz(lons, lats)\n    points = _np.c_[x,y,z]\n\n    # PETSc\'s markBoundaryFaces routine cannot detect boundary segments\n    # for our spherical implementation. We do it here instead.\n    if boundary_vertices is None:\n        boundary_vertices = find_boundary_segments(simplices)\n\n    return _create_DMPlex(points, simplices, boundary_vertices, refinement_levels)\n\n\ndef save_DM_to_hdf5(dm, file):\n    """"""\n    Saves mesh information stored in the DM to HDF5 file\n    If the file already exists, it is overwritten.\n    """"""\n    from petsc4py import PETSc\n\n    file = str(file)\n    if not file.endswith(\'.h5\'):\n        file += \'.h5\'\n\n    ViewHDF5 = PETSc.Viewer()\n    ViewHDF5.createHDF5(file, mode=\'w\')\n    ViewHDF5.view(obj=dm)\n    ViewHDF5.destroy()\n    return\n\n\ndef refine_DM(dm, refinement_levels=1):\n    """"""\n    Refine DM a specified number of refinement steps\n    For each step, the midpoint of every line segment is added\n    to the DM.\n    """"""\n    from stripy.spherical import lonlat2xyz, xyz2lonlat\n\n    for i in range(0, refinement_levels):\n        dm = dm.refine()\n\n    dm.setNumFields(1)\n    origSect = dm.createSection(1, [1,0,0]) # define one DoF on the nodes\n    origSect.setFieldName(0, ""points"")\n    origSect.setUp()\n    dm.setDefaultSection(origSect)\n\n\n    # puff refined vertices to unit sphere\n    # but first we need to check if the DM is spherical\n    if refinement_levels > 0:\n\n        nlabels = dm.getNumLabels()\n        is_spherical = False\n        for i in range(0, nlabels):\n            if dm.getLabelName(i) == ""sTriMesh"":\n                is_spherical = True\n                break\n\n        if is_spherical:\n            # get coordinate array from DM\n            global_coord_vec = dm.getCoordinates()\n            global_coords = global_coord_vec.array.reshape(-1,3)\n\n            # xyz - lonlat - xyz conversion\n            lon, lat = xyz2lonlat(global_coords[:,0], global_coords[:,1], global_coords[:,2])\n            xs,ys,zs = lonlat2xyz(lon, lat)\n\n            # set puffed coordinate array to DM\n            global_puffed_coords = _np.column_stack([xs,ys,zs])\n            global_coord_vec.setArray(global_puffed_coords.ravel())\n            dm.setCoordinates(global_coord_vec)\n\n    return dm\n\n\n\ndef lloyd_mesh_improvement(x, y, bmask, iterations):\n    """"""\n    Applies Lloyd\'s algorithm of iterated voronoi construction\n    to improve the mesh point locations (assumes no current triangulation)\n\n    (e.g. see http://en.wikipedia.org/wiki/Lloyd\'s_algorithm )\n\n    This can be very slow for anything but a small mesh.\n\n    We do not move boundary points, but some issues can arise near\n    boundaries if the initial mesh is poorly constructed with non-boundary points\n    very close to the boundary such that the centroid of the cell falls outside the boundary.\n\n    Caveat emptor !\n    """"""\n\n    from scipy.spatial import Voronoi  as __Voronoi\n\n\n    points = _np.c_[x,y]\n\n    for i in range(0,iterations):\n        vor = __Voronoi(points)\n        new_coords = vor.points.copy()\n\n        for centre_point, coords in enumerate(vor.points):\n            region = vor.regions[vor.point_region[centre_point]]\n            if not -1 in region and bmask[centre_point]:\n                polygon = vor.vertices[region]\n                new_coords[centre_point] = [polygon[:,0].sum() / len(region), polygon[:,1].sum() / len(region)]\n\n        points = new_coords\n\n    x = _np.array(new_coords[:,0])\n    y = _np.array(new_coords[:,1])\n\n    return x, y\n\n## These are not very well cooked - we need boundary points etc\n\ndef square_mesh(minX, maxX, minY, maxY, spacingX, spacingY, random_scale=0.0, refinement_levels=0):\n    """"""\n    Generate a square mesh using stripy\n    """"""\n    from stripy import cartesian_meshes\n\n    extent_xy = [minX, maxX, minY, maxY]\n\n    tri = cartesian_meshes.square_mesh(extent_xy, spacingX, spacingY, random_scale, refinement_levels)\n\n    return tri.x, tri.y, tri.simplices\n\n\ndef elliptical_mesh(minX, maxX, minY, maxY, spacingX, spacingY, random_scale=0.0, refinement_levels=0):\n    """"""\n    Generate an elliptical mesh using stripy\n    """"""\n    from stripy import cartesian_meshes\n\n    extent_xy = [minX, maxX, minY, maxY]\n\n    tri = cartesian_meshes.elliptical_mesh(extent_xy, spacingX, spacingY, random_scale, refinement_levels)\n\n    return tri.x, tri.y, tri.simplices\n\n\ndef generate_square_points(minX, maxX, minY, maxY, spacingX, spacingY, samples, boundary_samples ):\n\n    lin_samples = int(_np.sqrt(samples))\n\n    tiX = _np.linspace(minX + 0.75 * spacingX, maxX - 0.75 * spacingX, lin_samples)\n    tiY = _np.linspace(minY + 0.75 * spacingY, maxY - 0.75 * spacingY, lin_samples)\n\n    x,y = _np.meshgrid(tiX, tiY)\n\n    x = x.ravel()\n    y = y.ravel()\n\n    xscale = (x.max()-x.min()) / (2.0 * _np.sqrt(samples))\n    yscale = (y.max()-y.min()) / (2.0 * _np.sqrt(samples))\n\n    x += xscale * (0.5 - _np.random.rand(x.size))\n    y += yscale * (0.5 - _np.random.rand(y.size))\n\n\n    bmask = _np.ones_like(x, dtype=bool) # It\'s all true !\n\n    # add boundary points too\n\n    xc = _np.linspace(minX, maxX, boundary_samples)\n    yc = _np.linspace(minY, maxY, boundary_samples)\n\n    i = 1.0 - _np.linspace(-0.5, 0.5, boundary_samples)**2\n\n    x = _np.append(x, xc)\n    y = _np.append(y, minY - spacingY*i)\n\n    x = _np.append(x, xc)\n    y = _np.append(y, maxY + spacingY*i)\n\n    x = _np.append(x, minX - spacingX*i[1:-1])\n    y = _np.append(y, yc[1:-1])\n\n    x = _np.append(x, maxX + spacingX*i[1:-1])\n    y = _np.append(y, yc[1:-1])\n\n    bmask = _np.append(bmask, _np.zeros(2*i.size + 2*(i.size-2), dtype=bool))\n\n    return x, y, bmask\n\n\ndef generate_elliptical_points(minX, maxX, minY, maxY, spacingX, spacingY, samples, boundary_samples ):\n\n    originX = 0.5 * (maxX + minX)\n    originY = 0.5 * (maxY + minY)\n    radiusX = 0.5 * (maxX - minX)\n    aspect = 0.5 * (maxY - minY) / radiusX\n\n    print(""Origin = "", originX, originY, ""Radius = "", radiusX, ""Aspect = "", aspect)\n\n    lin_samples = int(_np.sqrt(samples))\n\n    tiX = _np.linspace(minX + 0.75 * spacingX, maxX - 0.75 * spacingX, lin_samples)\n    tiY = _np.linspace(minY + 0.75 * spacingY, maxY - 0.75 * spacingY, lin_samples)\n\n    x,y = _np.meshgrid(tiX, tiY)\n\n    x = _np.reshape(x,len(x)*len(x[0]))\n    y = _np.reshape(y,len(y)*len(y[0]))\n\n    xscale = (x.max()-x.min()) / (2.0 * _np.sqrt(samples))\n    yscale = (y.max()-y.min()) / (2.0 * _np.sqrt(samples))\n\n    x += xscale * (0.5 - _np.random.rand(len(x)))\n    y += yscale * (0.5 - _np.random.rand(len(y)))\n\n    mask = _np.where( (x**2 + y**2 / aspect**2) < (radiusX-0.5*spacingX)**2 )\n\n    X = x[mask]\n    Y = y[mask]\n    bmask = _np.ones_like(X, dtype=bool)\n\n    # Now add boundary points\n\n    theta = _np.array( [ 2.0 * _np.pi * i / (3 * boundary_samples) for i in range(0, 3 * boundary_samples) ])\n\n    X = _np.append(X, 1.001 * radiusX * _np.sin(theta))\n    Y = _np.append(Y, 1.001 * radiusX * aspect * _np.cos(theta))\n    bmask = _np.append(bmask, _np.zeros_like(theta, dtype=bool))\n\n    return X, Y, bmask\n'"
quagmire/topomesh/__init__.py,0,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom .topomesh import TopoMesh'"
quagmire/topomesh/topomesh.py,76,"b'# Copyright 2016-2020 Louis Moresi, Ben Mather, Romain Beucher\n# \n# This file is part of Quagmire.\n# \n# Quagmire is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or any later version.\n# \n# Quagmire is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public License\n# along with Quagmire.  If not, see <http://www.gnu.org/licenses/>.\n\nimport numpy as np\nimport numpy.ma as ma\nfrom mpi4py import MPI\nimport sys,petsc4py\npetsc4py.init(sys.argv)\nfrom petsc4py import PETSc\n# comm = MPI.COMM_WORLD\nfrom time import perf_counter\n\ntry: range = xrange\nexcept: pass\n\nfrom quagmire import function as fn\nfrom quagmire.mesh import MeshVariable as _MeshVariable\nfrom quagmire.function import LazyEvaluation as _LazyEvaluation\n\nclass TopoMesh(object):\n    def __init__(self, downhill_neighbours=2, *args, **kwargs):\n\n        # Initialise cumulative flow vectors\n        self._DX0 = self.gvec.duplicate()\n        self._DX1 = self.gvec.duplicate()\n        self._dDX = self.gvec.duplicate()\n\n        self.downhillMat = None\n\n        # There is no topography yet set, so we need to avoid\n        # triggering the matrix rebuilding in the setter of this property\n        self._downhill_neighbours = downhill_neighbours\n        self._topography_modified_count = 0\n\n        # create a variable for the height (data) and fn_height\n        # a context manager to ensure the matrices are updated when h(x,y) changes\n\n        self.topography        = self.add_variable(name=""h(x,y)"", locked=True)\n        self.upstream_area     = self.add_variable(name=""A(x,y)"", locked=True)\n        self.deform_topography = self._height_update_context_manager_generator()\n\n        ## TODO: Should be able to fix upstream area as a ""partial"" function of upstream area\n        ## with a parameter value of 1.0 in the argument (or 1.0 above a ref height)\n\n        # Slope (function)\n        self.slope = fn.math.slope(self.topography)\n\n        self._heightVariable = self.topography\n\n\n\n    @property\n    def downhill_neighbours(self):\n        return self._downhill_neighbours\n\n    @downhill_neighbours.setter\n    def downhill_neighbours(self, value):\n        self._downhill_neighbours = int(value)\n        self.deform_topography = self._height_update_context_manager_generator()\n\n        # if the topography is unlocked, don\'t rebuild anything\n        # as it might break something\n\n        if self.topography._locked == True:\n            self._update_height()\n            self._update_height_for_surface_flows()\n\n        return\n\n\n    def _update_height(self):\n        """"""\n        Update height field\n        """"""\n\n        t = perf_counter()\n        self._build_downhill_matrix_iterate()\n        self.timings[\'downhill matrices\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n\n        if self.rank==0 and self.verbose:\n            print((""{} - Build downhill matrices {}s"".format(self.dm.comm.rank, perf_counter()-t)))\n        return\n\n    def _update_height_for_surface_flows(self):\n\n        #self.rainfall_pattern = rainfall_pattern.copy()\n        #self.sediment_distribution = sediment_distribution.copy()\n\n        t = perf_counter()\n\n        self.upstream_area.unlock()\n        self.upstream_area.data = self.cumulative_flow(self.pointwise_area.data)  ## Should be upstream integral of 1.0\n        self.upstream_area.lock()\n\n        self.timings[\'Upstream area\'] = [perf_counter()-t, self.log.getCPUTime(), self.log.getFlops()]\n\n        if self.verbose:\n            print((""{} - Build upstream areas {}s"".format(self.dm.comm.rank, perf_counter()-t)))\n\n        # Find low points\n        self.low_points = self.identify_low_points()\n\n        # Find high points\n        self.outflow_points = self.identify_outflow_points()\n\n    def _height_update_context_manager_generator(self):\n        """"""Builds a context manager on the current mesh object to control when matrices are to be updated""""""\n\n        topomesh = self\n        topographyVariable = self.topography\n        downhill_neighbours = self.downhill_neighbours\n\n        class Topomesh_Height_Update_Manager(object):\n            """"""Manage when changes to the height information trigger a rebuild\n            of the topomesh matrices and other internal data.\n            """"""\n\n            def __init__(inner_self, downhill_neighbours=downhill_neighbours):\n                inner_self._topomesh = topomesh\n                inner_self._topovar  = topographyVariable\n                inner_self._downhill_neighbours = int(downhill_neighbours)\n                return\n\n            def __enter__(inner_self):\n                # unlock\n                inner_self._topovar.unlock()\n                inner_self._topomesh._downhill_neighbours = inner_self._downhill_neighbours\n                return\n\n            def __exit__(inner_self, *args):\n                inner_self._topomesh._update_height()\n                inner_self._topomesh._update_height_for_surface_flows()\n                inner_self._topovar.lock()\n                inner_self._topomesh._topography_modified_count += 1\n                return\n\n        return Topomesh_Height_Update_Manager\n\n        self._build_adjacency_matrix_iterate()\n        if self.rank==0 and self.verbose:\n            print(("" - Partial rebuild of downhill matrices {}s"".format(perf_counter()-t)))\n\n        # revert to specified n-neighbours\n        self.downhill_neighbours = neighbours\n\n        return\n\n    def _sort_nodes_by_field(self, height):\n\n        # Sort neighbours by gradient\n        indptr, indices = self.vertex_neighbour_vertices\n        # gradH = height[indices]/self.vertex_neighbour_distance\n\n        self.node_high_to_low = np.argsort(height)[::-1]\n\n        neighbour_array_lo_hi = self.neighbour_array.copy()\n        neighbour_array_2_low = np.empty((self.npoints, 2), dtype=PETSc.IntType)\n\n        for i in range(indptr.size-1):\n            # start, end = indptr[i], indptr[i+1]\n            # neighbours = np.hstack([i, indices[start:end]])\n            # order = height[neighbours].argsort()\n            # neighbour_array_lo_hi[i] = neighbours[order]\n            # neighbour_array_2_low[i] = neighbour_array_lo_hi[i][:2]\n\n            neighbours = self.neighbour_array[i]\n            order = height[neighbours].argsort()\n            neighbour_array_lo_hi[i] = neighbours[order]\n            neighbour_array_2_low[i] = neighbour_array_lo_hi[i][:2]\n\n        self.neighbour_array_lo_hi = neighbour_array_lo_hi\n        self.neighbour_array_2_low = neighbour_array_2_low\n\n\n\n    def _adjacency_matrix_template(self, nnz=(1,1)):\n\n        matrix = PETSc.Mat().create(comm=self.dm.comm)\n        matrix.setType(\'aij\')\n        matrix.setSizes(self.sizes)\n        matrix.setLGMap(self.lgmap_row, self.lgmap_col)\n        matrix.setFromOptions()\n        matrix.setPreallocationNNZ(nnz)\n\n        return matrix\n\n\n## This is the lowest near node / lowest extended neighbour\n\n# hnear = np.ma.array(mesh.height[mesh.neighbour_cloud], mask=mesh.near_neighbours_mask)\n# low_neighbours = np.argmin(hnear, axis=1)\n#\n# hnear = np.ma.array(mesh.height[mesh.neighbour_cloud], mask=mesh.extended_neighbours_mask)\n# low_eneighbours = np.argmin(hnear, axis=1)\n#\n#\n\n    def _build_down_neighbour_arrays(self, nearest=True):\n\n        nodes = list(range(0,self.npoints))\n        nheight  = self._heightVariable.data[self.natural_neighbours]\n        nheight[np.where(self.natural_neighbours == -1)] = np.finfo(nheight.dtype).max\n\n        nheightidx = np.argsort(nheight, axis=1)\n\n        ## How many low neighbours are there in each ?\n\n        idxrange  = np.where(nheightidx==0)[1]\n\n        ## First the STD, 1-neighbour\n\n        idx  = nheightidx[:,0]\n        index1 = self.natural_neighbours[nodes, idx[nodes]]\n\n        # store in neighbour dictionary\n        self.down_neighbour = dict()\n        self.down_neighbour[1] = index1.astype(PETSc.IntType)\n\n        ## Now all next-lower-level neighours\n\n        for i in range(1, self.downhill_neighbours):\n            n = i + 1\n\n            idx  = nheightidx[:,i]\n            indexN = self.natural_neighbours[nodes, idx[nodes]]\n\n            failed = np.where(idxrange < n)\n            indexN[failed] = index1[failed]\n\n            # store in neighbour dictionary\n            self.down_neighbour[n] = indexN.astype(PETSc.IntType)\n\n\n    def _build_adjacency_matrix_iterate(self):\n\n        self._build_down_neighbour_arrays(nearest=True)\n\n        self.adjacency = dict()\n        self.uphill = dict()\n        data = np.ones(self.npoints)\n\n        indptr = np.arange(0, self.npoints+1, dtype=PETSc.IntType)\n        nodes = indptr[:-1]\n\n        for i in range(1, self.downhill_neighbours+1):\n\n            data[self.down_neighbour[i]==nodes] = 0.0\n\n            adjacency = self._adjacency_matrix_template()\n            adjacency.assemblyBegin()\n            adjacency.setValuesLocalCSR(indptr, self.down_neighbour[i], data)\n            adjacency.assemblyEnd()\n\n            self.uphill[i] = adjacency.copy()\n            self.adjacency[i] = adjacency.transpose()\n\n            # self.down_neighbour[i] = down_neighbour.copy()\n\n    def _build_downhill_matrix_iterate(self):\n\n        self._build_adjacency_matrix_iterate()\n        weights = np.empty((self.downhill_neighbours, self.npoints))\n\n        height = self._heightVariable.data\n\n        # Process weights\n        for i in range(0, self.downhill_neighbours):\n            down_N = self.down_neighbour[i+1]\n            grad = np.abs(height - height[down_N]+1.0e-10) / (1.0e-10 + \\\n                   np.hypot(self.coords[:,0] - self.coords[down_N,0],\n                            self.coords[:,1] - self.coords[down_N,1] ))\n\n            weights[i,:] = np.sqrt(grad)\n\n        weights /= weights.sum(axis=0)\n        w = self.gvec.duplicate()\n\n\n        # Store weighted downhill matrices\n        downhill_matrices = [None]*self.downhill_neighbours\n        for i in range(0, self.downhill_neighbours):\n            N = i + 1\n            self.lvec.setArray(weights[i])\n            self.dm.localToGlobal(self.lvec, w)\n\n            D = self.adjacency[N].copy()\n            D.diagonalScale(R=w)\n            downhill_matrices[i] = D\n\n\n        # Sum downhill matrices\n        self.downhillMat = downhill_matrices[0]\n        for i in range(1, self.downhill_neighbours):\n            self.downhillMat += downhill_matrices[i]\n            downhill_matrices[i].destroy()\n\n        return\n\n\n    def build_cumulative_downhill_matrix(self):\n        """"""\n        Build non-sparse, single hit matrices to cumulative_flow information downhill\n        (self.sweepDownToOutflowMat and self.downhillCumulativeMat)\n\n        This may be expensive in terms of storage so this is only done if\n        self.storeDense == True and the matrices are also out of date (which they\n        will be if the height field is changed)\n\n        downhillCumulativeMat = I + D + D**2 + D**3 + ... D**N where N is the length of the graph\n\n        """"""\n\n        comm = self.dm.comm\n\n        downSweepMat    = self.accumulatorMat.copy()\n        downHillaccuMat = self.downhillMat.copy()\n        accuM           = self.downhillMat.copy()   # work matrix\n\n        DX0 = self._DX0\n        DX1 = self._DX1\n        dDX = self._dDX\n        DX0.set(1.0)\n\n        err = np.array([True])\n        err_proc = np.ones(comm.size, dtype=bool)\n\n        while err_proc.any():\n            downSweepMat    = downSweepMat*self.accumulatorMat  # N applications of the accumulator\n            accuM           = accuM*self.downhillMat\n            downHillaccuMat = downHillaccuMat + accuM\n            DX0 = self.downhillMat*DX1\n\n            err[0] = np.any(DX0)\n            comm.Allgather([err, MPI.BOOL], [err_proc, MPI.BOOL])\n\n        # add identity matrix\n        I = np.arange(0, self.npoints+1, dtype=PETSc.IntType)\n        J = np.arange(0, self.npoints, dtype=PETSc.IntType)\n        V = np.ones(self.npoints)\n        identityMat = self._adjacency_matrix_template()\n        identityMat.assemblyBegin()\n        identityMat.setValuesLocalCSR(I, J, V)\n        identityMat.assemblyEnd()\n\n        downHillaccuMat += identityMat\n\n        self.downhillCumulativeMat = downHillaccuMat\n        self.sweepDownToOutflowMat = downSweepMat\n\n\n    def upstream_integral_fn(self, lazyFn):\n        """"""Upsream integral implemented as an area-weighted upstream summation""""""\n\n        import quagmire\n\n        def integral_fn(*args, **kwargs):\n            node_values = lazyFn.evaluate(lazyFn._mesh) * lazyFn._mesh.area\n            node_integral = self.cumulative_flow(node_values)\n\n            if len(args) == 1 and args[0] == lazyFn._mesh:\n                return node_integral\n            elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n                mesh = args[0]\n                return lazyFn._mesh.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=node_integral, **kwargs)\n            else:\n                xi = np.atleast_1d(args[0])\n                yi = np.atleast_1d(args[1])\n                i, e = lazyFn._mesh.interpolate(xi, yi, zdata=node_integral, **kwargs)\n                return i\n\n        newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n        newLazyFn.evaluate = integral_fn\n        newLazyFn.description = ""UpInt({})dA"".format(lazyFn.description)\n        newLazyFn.dependency_list |= lazyFn.dependency_list\n\n        return newLazyFn\n\n\n    def cumulative_flow(self, vector, *args, **kwargs):\n\n\n        niter, cumulative_flow_vector = self._cumulative_flow_verbose(vector, **kwargs)\n        return cumulative_flow_vector\n\n\n    def _cumulative_flow_verbose(self, vector, verbose=False, maximum_its=None, uphill=False):\n\n        if not maximum_its:\n            maximum_its = 1000000000000\n\n        # This is what happens if the mesh topography has never been set\n        if not self.downhillMat:\n            print(""No downhill matrix exists "")\n            temp_vec = self.lvec.array.copy()\n            temp_vec = 0.0\n            return 0, temp_vec\n\n\n        # downhillMat2 = self.downhillMat * self.downhillMat\n        # downhillMat4 = downhillMat2 * downhillMat2\n        # downhillMat8 = downhillMat4 * downhillMat4\n        if uphill:\n            downhillMat = self.downhillMat.copy()\n            downhillMat = downhillMat.transpose()\n        else:\n            downhillMat = self.downhillMat\n\n        DX0 = self._DX0\n        DX1 = self._DX1\n        dDX = self._dDX\n\n        self.lvec.setArray(vector)\n        self.dm.localToGlobal(self.lvec, DX0, addv=PETSc.InsertMode.INSERT_VALUES)\n\n        DX1.setArray(DX0)\n        # DX0.assemble()\n        # DX1.assemble()\n\n        niter = 0\n        equal = False\n\n        tolerance = 1e-8 * DX1.max()[1]\n\n        while not equal and niter < maximum_its:\n            dDX.setArray(DX1)\n            #dDX.assemble()\n\n            downhillMat.mult(DX1, self.gvec)\n            DX1.setArray(self.gvec)\n            DX1.assemble()\n\n            DX0 += DX1\n\n            dDX.axpy(-1.0, DX1)\n            dDX.abs()\n            max_dDX = dDX.max()[1]\n\n            equal = max_dDX < tolerance\n\n            if self.dm.comm.rank==0 and verbose and niter%10 == 0:\n                print(""{}: Max Delta - {} "".format(niter, max_dDX))\n\n            niter += 1\n\n        if self.dm.comm.Get_size() == 1:\n            return niter, DX0.array.copy()\n        else:\n            self.dm.globalToLocal(DX0, self.lvec)\n            return niter, self.lvec.array.copy()\n\n\n    def downhill_smoothing_fn(self, lazyFn, its=1, centre_weight=0.75):\n\n        import quagmire\n\n        def new_fn(*args, **kwargs):\n            local_array = lazyFn.evaluate(self)\n            smoothed = self._downhill_smoothing(local_array, its=its, centre_weight=centre_weight)\n            smoothed = self.sync(smoothed)\n\n            if len(args) == 1 and args[0] == lazyFn._mesh:\n                return smoothed\n            elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n\n                mesh = args[0]\n                return self.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=smoothed, **kwargs)\n            else:\n                xi = np.atleast_1d(args[0])  # .resize(-1,1)\n                yi = np.atleast_1d(args[1])  # .resize(-1,1)\n                i, e = self.interpolate(xi, yi, zdata=smoothed, **kwargs)\n                return i\n\n        newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n        newLazyFn.evaluate = new_fn\n        newLazyFn.description = ""DnHSmooth({}), i={}, w={}"".format(lazyFn.description,  its, centre_weight)\n        newLazyFn.dependency_list |= lazyFn.dependency_list\n\n\n        return newLazyFn\n\n\n    def _downhill_smoothing(self, data, its, centre_weight=0.75):\n\n        downhillMat = self.downhillMat\n\n        norm = self.gvec.duplicate()\n        smooth_data = self.gvec.duplicate()\n\n        self.gvec.set(1.0)\n        self.downhillMat.mult(self.gvec, norm)\n\n        mask = norm.array == 0.0\n\n        self.lvec.setArray(data)\n        self.dm.localToGlobal(self.lvec, smooth_data)\n        for i in range(0, its):\n            self.downhillMat.mult(smooth_data, self.gvec)\n            smooth_data.setArray((1.0 - centre_weight) * self.gvec.array + \\\n                                  smooth_data.array*np.where(mask, 1.0, centre_weight))\n\n        if self.dm.comm.Get_size() == 1:\n            return smooth_data.array.copy()\n        else:\n            self.dm.globalToLocal(smooth_data, self.lvec)\n            return self.lvec.array.copy()\n\n\n    def uphill_smoothing_fn(self, lazyFn, its=1, centre_weight=0.75):\n\n        import quagmire\n\n        def new_fn(*args, **kwargs):\n            local_array = lazyFn.evaluate(self)\n            smoothed = self._uphill_smoothing(local_array, its=its, centre_weight=centre_weight)\n            smoothed = self.sync(smoothed)\n\n            if len(args) == 1 and args[0] == lazyFn._mesh:\n                return smoothed\n            elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n                mesh = args[0]\n                return self.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=smoothed, **kwargs)\n            else:\n                xi = np.atleast_1d(args[0])  # .resize(-1,1)\n                yi = np.atleast_1d(args[1])  # .resize(-1,1)\n                i, e = self.interpolate(xi, yi, zdata=smoothed, **kwargs)\n                return i\n\n        newLazyFn = LazyEvaluation(mesh=lazyFn._mesh)\n        newLazyFn.evaluate = new_fn\n        newLazyFn.description = ""UpHSmooth({}), i={}, w={}"".format(lazyFn.description, )\n        newLazyFn.dependency_list |= lazyFn.dependency_list\n\n\n        return newLazyFn\n\n\n    def _uphill_smoothing(self, data, its, centre_weight=0.75):\n\n        downhillMat = self.downhillMat\n\n\n        norm2 = self.gvec.duplicate()\n        smooth_data = self.gvec.duplicate()\n\n        self.gvec.set(1.0)\n        self.downhillMat.multTranspose(self.gvec, norm2)\n\n        mask = norm2.array == 0.0\n        norm2.array[~mask] = 1.0/norm2.array[~mask]\n\n        self.lvec.setArray(data)\n        self.dm.localToGlobal(self.lvec, smooth_data)\n        for i in range(0, its):\n            self.downhillMat.multTranspose(smooth_data, self.gvec)\n            smooth_data.setArray((1.0 - centre_weight) * self.gvec.array * norm2 + \\\n                                  smooth_data.array*np.where(mask, 1.0, centre_weight))\n\n\n        if self.dm.comm.Get_size() == 1:\n            smooth_data *= data.mean()/smooth_data.array.mean()\n            return smooth_data.copy()\n        else:\n\n            self.dm.globalToLocal(smooth_data, self.lvec)\n            self.lvec *= data.mean()/self.lvec.array.mean()\n            return self.lvec.array.copy()\n\n\n\n    def streamwise_smoothing_fn(self, lazyFn, its=1, centre_weight=0.75):\n\n        import quagmire\n\n        def new_fn(*args, **kwargs):\n            local_array = lazyFn.evaluate(self)\n            smoothed = self._streamwise_smoothing(local_array, its=its, centre_weight=centre_weight)\n            smoothed = self.sync(smoothed)\n\n            if len(args) == 1 and args[0] == lazyFn._mesh:\n                return smoothed\n            elif len(args) == 1 and quagmire.mesh.check_object_is_a_q_mesh_and_raise(args[0]):\n                mesh = args[0]\n                return self.interpolate(mesh.coords[:,0], mesh.coords[:,1], zdata=smoothed, **kwargs)\n            else:\n                xi = np.atleast_1d(args[0])  # .resize(-1,1)\n                yi = np.atleast_1d(args[1])  # .resize(-1,1)\n                i, e = self.interpolate(xi, yi, zdata=smoothed, **kwargs)\n                return i\n\n        newLazyFn = _LazyEvaluation(mesh=lazyFn._mesh)\n        newLazyFn.evaluate = new_fn\n        newLazyFn.description = ""StmSmooth({}), i={}, w={}"".format(lazyFn.description, its, centre_weight)\n        newLazyFn.dependency_list |= lazyFn.dependency_list\n\n\n        return newLazyFn\n\n\n\n\n    def _streamwise_smoothing(self, data, its, centre_weight=0.75):\n        """"""\n        A smoothing operator that is limited to the uphill / downhill nodes for each point. It\'s hard to build\n        a conservative smoothing operator this way since ""boundaries"" occur at irregular internal points associated\n        with watersheds etc. Upstream and downstream smoothing operations bracket the original data (over and under,\n        respectively) and we use this to find a smooth field with the same mean value as the original data. This is\n        done for each application of the smoothing.\n        """"""\n\n        smooth_data_d = self._downhill_smoothing(data, its, centre_weight=centre_weight)\n        smooth_data_u = self._uphill_smoothing(data, its, centre_weight=centre_weight)\n\n        return 0.5*(smooth_data_d + smooth_data_u)\n\n\n\n    def _node_lowest_neighbour(self, node):\n        """"""\n        Find the lowest node in the neighbour list of the given node\n        """"""\n\n        lowest = self.neighbour_array_lo_hi[node][0]\n\n        if lowest != node:\n            return lowest\n        else:\n            return -1\n\n\n\n    def _node_highest_neighbour(self, node):\n        """"""\n        Find the highest node in the neighbour list of the given node\n        """"""\n\n        highest = self.neighbour_array_lo_hi[node][-1]\n\n        if highest != node:\n            return highest\n        else:\n            return -1\n\n\n    def _node_walk_downhill(self, node):\n        """"""\n        Walks downhill terminating when the downhill node is already claimed\n        """"""\n\n        chain = -np.ones(self.npoints, dtype=np.int32)\n\n        idx = 0\n        max_idx = self.npoints\n        chain[idx] = node\n        low_neighbour = self._node_lowest_neighbour(node)\n        junction = -1\n\n        while low_neighbour != -1:\n            idx += 1\n            chain[idx] = low_neighbour\n            if self.node_chain_lookup[low_neighbour] != -1:\n                junction = self.node_chain_lookup[low_neighbour]\n                break\n\n            low_neighbour = self._node_lowest_neighbour(low_neighbour)\n\n        return junction, chain[0:idx+1]\n\n\n    def build_node_chains(self):\n        """""" NEEDS WORK\n        Builds all the chains for the mesh which flow from high to low and terminate\n        when they meet with an existing chain.\n\n        The following data structures become available once this function has been called:\n\n            self.node_chain_lookup - tells you the chain in which a given node number lies\n            self.node_chain_list   - is a list of the chains of nodes (each of which is an list)\n\n        The terminating node of a chain may be the junction with another (pre-exisiting) chain\n        and will be a member of that chain. Backbone chains which run from the highest level\n        to the base level or the boundary are those whose terminal node is also a member of the same chain.\n\n        Nodes which are at a base level given by self.base, are collected separately\n        into chain number 0.\n        """"""\n\n        self.node_chain_lookup = -np.ones(self.npoints, dtype=np.int32)\n        self.node_chain_list = []\n\n\n        node_chain_idx = 1\n\n        self.node_chain_list.append([]) # placeholder for any isolated base-level nodes\n\n        for node1 in self.node_high_to_low:\n            if self.node_chain_lookup[node1] != -1:\n                continue\n\n            junction, this_chain = self._node_walk_downhill(node1)\n\n            if len(this_chain) > 1:\n                self.node_chain_list.append(this_chain)\n\n                self.node_chain_lookup[this_chain[0:-1]] = node_chain_idx\n                if self.node_chain_lookup[this_chain[-1]] == -1:\n                    self.node_chain_lookup[this_chain[-1]] = node_chain_idx\n\n                node_chain_idx += 1\n\n            else:\n                self.node_chain_list[0].append(this_chain[0])\n                self.node_chain_lookup[this_chain[0]] = 0\n\n\n### Methods copied over from obseleted SurfMesh class\n\n    def low_points_local_flood_fill(self, its=99999, scale=1.0, smoothing_steps=2, ref_height=0.0):\n        """"""\n        Fill low points with a local flooding algorithm.\n          - its is the number of uphill propagation steps\n          - scale\n        """"""\n\n        t = perf_counter()\n        if self.rank==0 and self.verbose:\n            print(""Low point local flood fill"")\n\n        my_low_points = self.identify_low_points(ref_height=ref_height)\n\n        self.topography.unlock()\n        h = self.topography.data\n\n        fill_height =  h[self.natural_neighbours[i,1:self.natural_neighbours_count[i]]].min(axis=1) - h[my_low_points]\n\n        new_h = self.uphill_propagation(my_low_points,  fill_height, scale=scale,  its=its, fill=0.0)\n        new_h = self.sync(new_h)\n\n        smoothed_new_height = self.rbf_smoother(new_h, iterations=smoothing_steps)\n        self.topography.data = np.maximum(0.0, smoothed_new_height) + h\n        self.topography.sync()\n        self.topography.lock()\n        self._update_height()\n\n        if self.rank==0 and self.verbose:\n            print(""Low point local flood fill "",  perf_counter()-t, "" seconds"")\n\n        self._update_height_for_surface_flows()\n\n        return\n\n    def low_points_local_patch_fill(self, its=1, smoothing_steps=1, ref_height=0.0, fraction=0.01):\n        """"""\n        Local patch algorithm to fill low points - raises the topography at a local minimum to be\n        a small fraction higher than the lowest neighbour. If smoothing is used, then that correction\n        in topography is spread around all the local nodes. \n        """"""\n\n        from petsc4py import PETSc\n        t = perf_counter()\n        if self.rank==0 and self.verbose:\n            print(""Low point local patch fill"")\n\n        for iteration in range(0,its):\n            low_points = self.identify_low_points(ref_height=ref_height)\n\n            h = self.topography.data\n            delta_height = np.zeros_like(h)\n\n            ## Note, the smoother has a communication barrier so needs to be called even if it has no work to do on this process\n\n            for i in low_points:\n                delta_height[i] =  ( (1.0-fraction) * (h[self.natural_neighbours[i,1:self.natural_neighbours_count[i]]]).min() +\n                                          fraction  * (h[self.natural_neighbours[i,1:self.natural_neighbours_count[i]]]).max() \n                                              - h[i] )\n\n            ## Note, the smoother has a communication barrier so needs to be called even\n            ## if len(low_points==0) and there is no work to do on this process\n            smoothed_height = h + self.rbf_smoother(delta_height, iterations=smoothing_steps)\n\n            self.topography.unlock()\n            self.topography.data = np.maximum(smoothed_height, h)\n            self.topography.sync()\n            self.topography.lock()\n            self._update_height()\n\n        if self.rank==0 and self.verbose:\n            print(""Low point local patch fill "",  perf_counter()-t, "" seconds"")\n\n        ## and now we need to rebuild the surface process information\n        self._update_height_for_surface_flows()\n\n        return\n\n\n    def low_points_swamp_fill(self, its=1000, saddles=None, ref_height=0.0, ref_gradient=0.001, fluctuation_strength=0.05):\n\n        import petsc4py\n        from petsc4py import PETSc\n        from mpi4py import MPI\n\n        comm = MPI.COMM_WORLD\n        size = comm.Get_size()\n        rank = comm.Get_rank()\n\n        t0 = perf_counter()\n\n        # I\'m not sure the two ref_height values really refer to the same quantity \n        # and perhaps should be separated out \n\n        my_low_points = self.identify_low_points(ref_height=ref_height)\n        my_glow_points = self.lgmap_row.apply(my_low_points.astype(PETSc.IntType))\n\n        t = perf_counter()\n        ctmt = self.uphill_propagation(my_low_points,  my_glow_points, its=its, fill=-999999).astype(np.int)\n\n        height = self.topography.data.copy()\n\n        if self.rank==0 and self.verbose:\n            print(""Build low point catchments - "", perf_counter() - t, "" seconds"")\n  \n\n# Possible problem - low point that should flow out the side of the domain boundary. \n\n        # Find all catchment-edge points (mix of catchments in the natural neighbours)\n        ctmt2 = (ctmt[self.natural_neighbours] - ctmt.reshape(-1,1)) * (self.natural_neighbours_mask)\n\n        # filter out points that whose other-catchment neighbours are not lower\n        dh = (height[self.natural_neighbours] - height.reshape(-1,1)) * (self.natural_neighbours_mask)\n\n        ctmt3 = ctmt2 * (dh < 0.0)\n        cedges = np.where(ctmt3.any(axis=1))[0]\n\n        outer_edges = np.where(~self.bmask)[0]\n        edges = np.unique(np.hstack((cedges,outer_edges)))\n\n        ## In parallel this is all the low points where this process may have a spill-point\n        my_catchments = np.unique(ctmt)\n\n        spills = np.empty((edges.shape[0]),\n                         dtype=np.dtype([(\'c\', int), (\'h\', float), (\'x\', float), (\'y\', float)]))\n\n        ii = 0\n        for l, this_low in enumerate(my_catchments):\n            this_low_spills = edges[np.where(ctmt[edges] == this_low)]  ## local numbering\n\n            for spill in this_low_spills:\n                spills[\'c\'][ii] = this_low\n                spills[\'h\'][ii] = height[spill]\n                spills[\'x\'][ii] = self.coords[spill,0]\n                spills[\'y\'][ii] = self.coords[spill,1]\n                ii += 1\n\n        t = perf_counter()\n\n        spills.sort(axis=0)  # Sorts by catchment then height ...\n        s, indices = np.unique(spills[\'c\'], return_index=True)\n        spill_points = spills[indices]\n\n        if self.rank == 0 and self.verbose:\n            print(self.rank, "" Sort spills - "", perf_counter() - t)\n\n        # Gather lists to process 0, stack and remove duplicates\n\n        t = perf_counter()\n        list_of_spills = comm.gather(spill_points,   root=0)\n\n        if self.rank == 0 and self.verbose:\n            print(self.rank, "" Gather spill data - "", perf_counter() - t)\n\n        if self.rank == 0:\n            t = perf_counter()\n\n            all_spills = np.hstack(list_of_spills)\n            all_spills.sort(axis=0) # Sorts by catchment then height ...\n            s, indices = np.unique(all_spills[\'c\'], return_index=True)\n            all_spill_points = all_spills[indices]\n\n            if self.verbose:\n                print(rank, "" Sort all spills - "", perf_counter() - t)\n\n        else:\n            all_spill_points = None\n            pass\n\n        # Broadcast lists to everyone\n\n        global_spill_points = comm.bcast(all_spill_points, root=0)\n\n        height2 = np.zeros_like(height) \n\n        for i, spill in enumerate(global_spill_points):\n            this_catchment = int(spill[\'c\'])\n\n            ## -ve values indicate that the point is connected\n            ## to the outflow of the mesh and needs no modification\n            if this_catchment < 0:\n                continue\n\n            catchment_nodes = np.where(ctmt == this_catchment)\n            separation_x = (self.coords[catchment_nodes,0] - spill[\'x\'])\n            separation_y = (self.coords[catchment_nodes,1] - spill[\'y\'])\n            distance = np.hypot(separation_x, separation_y)\n            fluctuations = 1.0 + np.random.random(size=distance.shape) * distance.mean() * ref_gradient * fluctuation_strength\n\n            ## Todo: this gradient needs to be relative to typical ones nearby and resolvable in a geotiff !\n            height2[catchment_nodes] = spill[\'h\'] + ref_gradient * distance * fluctuations # A \'small\' gradient but this should depend on local conditions\n\n        height2 = self.sync(height2)\n\n        new_height = np.maximum(height, height2)\n        new_height = self.sync(new_height)\n\n\n        # We only need to update the height not all\n        # surface process information that is associated with it.\n        self.topography.unlock()\n        self.topography.data = new_height\n        self._update_height()\n        self.topography.lock()\n\n\n        if self.rank==0 and self.verbose:\n            print(""Low point swamp fill "",  perf_counter()-t0, "" seconds"")\n\n        ## but now we need to rebuild the surface process information\n        self._update_height_for_surface_flows()\n        return\n\n\n    def backfill_points(self, fill_points, heights, its):\n        """"""\n        Handles *selected* low points by backfilling height array.\n        This can be used to block a stream path, for example, or to locate lakes\n        """"""\n\n        if len(fill_points) == 0:\n            return self.heightVariable.data\n\n        new_height = self.lvec.duplicate()\n        new_height.setArray(heights)\n        height = np.maximum(self.height, new_height.array)\n\n        # Now march the new height to all the uphill nodes of these nodes\n        # height = np.maximum(self.height, delta_height.array)\n\n        self.dm.localToGlobal(new_height, self.gvec)\n        global_dH = self.gvec.copy()\n\n        for p in range(0, its):\n            self.adjacency[1].multTranspose(global_dH, self.gvec)\n            global_dH.setArray(self.gvec)\n            global_dH.scale(1.001)  # Maybe !\n            self.dm.globalToLocal(global_dH, new_height)\n\n            height = np.maximum(height, new_height.array)\n\n        return height\n\n    def uphill_propagation(self, points, values, scale=1.0, its=1000, fill=-1):\n\n        t0 = perf_counter()\n\n        local_ID = self.lvec.copy()\n        global_ID = self.gvec.copy()\n\n        local_ID.set(fill+1)\n        global_ID.set(fill+1)\n\n        identifier = np.empty_like(self.topography.data)\n        identifier.fill(fill+1)\n\n        if len(points):\n            identifier[points] = values + 1\n\n        local_ID.setArray(identifier)\n        self.dm.localToGlobal(local_ID, global_ID)\n\n        delta = global_ID.copy()\n        delta.abs()\n        rtolerance = delta.max()[1] * 1.0e-10\n\n        for p in range(0, its):\n\n            # self.adjacency[1].multTranspose(global_ID, self.gvec)\n            gvec = self.uphill[1] * global_ID\n            delta = global_ID - gvec\n            delta.abs()\n            max_delta = delta.max()[1]\n\n            if max_delta < rtolerance:\n                break\n\n            self.gvec.scale(scale)\n\n            if self.dm.comm.Get_size() == 1:\n                local_ID.array[:] = gvec.array[:]\n            else:\n                self.dm.globalToLocal(gvec, local_ID)\n\n            global_ID.array[:] = gvec.array[:]\n\n            identifier = np.maximum(identifier, local_ID.array)\n            identifier = self.sync(identifier)\n\n        # Note, the -1 is used to identify out of bounds values\n\n        if self.rank == 0 and self.verbose:\n            print(p, "" iterations, time = "", perf_counter() - t0)\n\n        return identifier - 1\n\n\n\n    def identify_low_points(self, include_shadows=False, ref_height=0):\n        """"""\n        Identify if the mesh has (internal) local minima and return an array of node indices\n        """"""\n\n        # from petsc4py import PETSc\n\n        nodes = np.arange(0, self.npoints, dtype=np.int)\n        gnodes = self.lgmap_row.apply(nodes.astype(PETSc.IntType))\n\n        low_nodes = self.down_neighbour[1]\n        mask = np.logical_and(nodes == low_nodes, self.bmask == True)\n        mask = np.logical_and(mask, self.topography.data > ref_height)\n\n        if not include_shadows:\n            mask = np.logical_and(mask, gnodes >= 0)\n\n        return nodes[mask]\n\n    def identify_global_low_points(self, global_array=False, ref_height=0.0):\n        """"""\n        Identify if the mesh as a whole has (internal) local minima and return an array of local lows in global\n        index format.\n\n        If global_array is True, then lows for the whole mesh are returned\n        """"""\n\n        import petsc4py\n        from petsc4py import PETSc\n        from mpi4py import MPI\n\n        comm = MPI.COMM_WORLD\n        size = comm.Get_size()\n        rank = comm.Get_rank()\n\n        # from petsc4py import PETSc\n\n        nodes = np.arange(0, self.npoints, dtype=np.int)\n        gnodes = self.lgmap_row.apply(nodes.astype(PETSc.IntType))\n\n        low_nodes = self.down_neighbour[1]\n        mask = np.logical_and(nodes == low_nodes, self.bmask == True)\n        mask = np.logical_and(mask, self.topography.data > ref_height)\n        mask = np.logical_and(mask, gnodes >= 0)\n\n        number_of_lows = np.count_nonzero(mask)\n        low_gnodes = self.lgmap_row.apply(low_nodes.astype(PETSc.IntType))\n\n        # gather/scatter numbers\n\n        list_of_nlows  = comm.gather(number_of_lows,   root=0)\n        if self.rank == 0:\n            all_low_counts = np.hstack(list_of_nlows)\n            no_global_lows0 = all_low_counts.sum()\n\n        else:\n            no_global_lows0 = None\n\n        no_global_lows = comm.bcast(no_global_lows0, root=0)\n\n\n        if global_array:\n\n            list_of_lglows = comm.gather(low_gnodes,   root=0)\n\n            if self.rank == 0:\n                all_glows = np.hstack(list_of_lglows)\n                global_lows0 = np.unique(all_glows)\n\n            else:\n                global_lows0 = None\n\n            low_gnodes = comm.bcast(global_lows0, root=0)\n\n        return no_global_lows, low_gnodes\n\n\n    def identify_outflow_points(self):\n        """"""\n        Identify the (boundary) outflow points and return an array of (local) node indices\n        """"""\n\n        # nodes = np.arange(0, self.npoints, dtype=np.int)\n        # low_nodes = self.down_neighbour[1]\n        # mask = np.logical_and(nodes == low_nodes, self.bmask == False)\n        #\n\n        i = self.downhill_neighbours\n\n        o = (np.logical_and(self.down_neighbour[i] == np.indices(self.down_neighbour[i].shape), self.bmask == False)).ravel()\n        outflow_nodes = o.nonzero()[0]\n\n        return outflow_nodes\n\n\n    def identify_flat_spots(self):\n\n        slope = self.slope.evaluate(self.slope._mesh)\n        smooth_grad1 = self.local_area_smoothing(slope, its=1, centre_weight=0.5)\n\n        # flat_spot_field = np.where(smooth_grad1 < smooth_grad1.max()/100, 0.0, 1.0)\n\n        flat_spots = np.where(smooth_grad1 < smooth_grad1.max()/1000.0, True, False)\n\n        return flat_spots'"
tests/earlier/diffusion.py,1,"b'import numpy as np\nfrom quagmire.mesh import TriMesh\nfrom quagmire import tools as meshtools\nfrom mpi4py import MPI\ncomm = MPI.COMM_WORLD\n\nclass Diffusion(TriMesh):\n    """"""\n    Diffusion class extends the TriMesh structure to facilitate diffusion\n    problems with explicit timestepping.\n\n    Mesh points should be uniformly-spaced (by using lloyd mesh improvement\n    or poisson disc sampling) to avoid degeneracies in the solution.\n    """"""\n    def __init__(self, dm):\n        """"""\n        Initialise TriMesh from the DM\n        """"""\n        super(Diffusion, self).__init__(dm)\n        self.history = []\n\n    def initial_conditions(self, t, u):\n        """"""\n        Evaluate initial conditions on vector u\n        """"""\n        if t != 0.0:\n            raise ValueError(""Initial conditions only for t=0.0"")\n\n        pts = self.tri.points\n        self.lvec.setArray(np.exp(-0.025*(pts[:,0]**2 + pts[:,1]**2)**2) + 0.0001)\n        self.dm.localToGlobal(self.lvec, u)\n\n        # append to history\n        self.history.append((0, 0.0, u.array.copy()))\n\n\n    def monitor(self, i, t, u):\n        """"""\n        i    = number of timestep\n        t    = time at timestep\n        u    = state vector\n        """"""\n        last_i, last_t, last_u = self.history[-1]\n        if i - last_i == 10:\n            uu = u.copy()\n            self.history.append((i, t, uu.array))\n            print((""cpu {}  - step = {:3d},  time = {:2.3f}"".format(comm.rank, i, t)))\n\n\n    def plotHistory(self):\n        """"""\n        Moves global informaton to the root processor to plot timesteps.\n        """"""\n        self._gather_root()\n        x = self.root_x\n        y = self.root_y\n\n        if comm.rank == 0:\n            import matplotlib.pyplot as plt\n            import matplotlib.tri as mtri\n\n            # re-triangulate\n            triang = mtri.Triangulation(x, y)\n\n        for i,t,u in self.history:\n            self.gvec.setArray(u)\n            self.tozero.scatter(self.gvec, self.zvec)\n\n            if comm.rank == 0:\n                print((""plotting timestep {:3d}, t = {:2.3f}"".format(i, t)))\n                fig = plt.figure(1)\n                ax = fig.add_subplot(111, xlim=[x.min(), x.max()], \\\n                                          ylim=[y.min(), y.max()])\n                im = ax.tripcolor(triang, self.zvec.array, vmin=0, vmax=1)\n                fig.colorbar(im)\n                plt.savefig(\'diffusion_{}.png\'.format(i))\n                plt.cla()\n                plt.clf()\n\n\n    def diffusion_rate(self, t, u, kappa, bval, f):\n        self.dm.globalToLocal(u, self.lvec)\n\n        tstep = (self.area/kappa).min()\n        u_x, u_y = self.derivative_grad(self.lvec.array)\n\n        flux_x = kappa * u_x\n        flux_y = kappa * u_y\n        flux_y[~self.bmask] = bval\n        flux_x[~self.bmask] = bval\n\n        d2u = self.derivative_div(flux_x, flux_y)\n\n        self.lvec.setArray(d2u)\n        self.dm.localToGlobal(self.lvec, f)\n\n        return tstep\n\n\n# Setup distributed mesh\nminX, maxX = -5., 5.\nminY, maxY = -5., 5.\ndx, dy = 0.1, 0.1\n\nx, y, bmask = meshtools.square_mesh(minX, maxX, minY, maxY, dx, dy, 10000, 100)\nx, y = meshtools.lloyd_mesh_improvement(x, y, bmask, iterations=3)\ndm = meshtools.create_DMPlex_from_points(x, y, bmask, refinement_levels=0)\n\node = Diffusion(dm)\nf = ode.gvec.duplicate()\nd2f = ode.gvec.duplicate()\n\ntime = 0.0\node.initial_conditions(time, f)\n\n# kappa can be spatially variable or constant\nkappa = 1.0\n\nsteps = 100\nfor step in range(0,steps):\n    tstep = ode.diffusion_rate(time, f, kappa, 0.0, d2f)\n    \n    d2f.scale(tstep)\n    f.axpy(1., d2f)\n\n    time += tstep\n\n    ode.monitor(step, time, f)\n\node.plotHistory()\n'"
tests/earlier/distributed_mesh.py,6,"b'import numpy as np\n# import matplotlib.pyplot as plt\nfrom quagmire import FlatMesh\nfrom quagmire import tools as meshtools\nfrom mpi4py import MPI\ncomm = MPI.COMM_WORLD\n\n\nminX, maxX = -5., 5.\nminY, maxY = -5., 5.\nspacing = 0.05\n\nptsx, ptsy, bmask = meshtools.poisson_elliptical_mesh(minX, maxX, minY, maxY, spacing, 500)\ndm = meshtools.create_DMPlex_from_points(ptsx, ptsy, bmask, refinement_levels=1)\n\nmesh = FlatMesh(dm)\n\n#if comm.rank == 0:\nprint(""Number of nodes in mesh - {}: {}"".format(comm.rank, mesh.npoints))\n\n# retrieve local mesh\nx = mesh.tri.x\ny = mesh.tri.y\n\n# create height field\nradius  = np.sqrt((x**2 + y**2))\ntheta   = np.arctan2(y,x)\n\nheight  = np.exp(-0.025*(x**2 + y**2)**2) + \\\n          0.25 * (0.2*radius)**4  * np.cos(10.0*theta)**2\nheight  += 0.5 * (1.0-0.2*radius)\n\n\nrank = np.ones_like(height)*comm.rank\nshadow = np.zeros_like(height)\n\n\n# get shadow zones\nshadow_zones = mesh.lgmap_row.indices < 0\nshadow[shadow_zones] = 1\nshadow_vec = mesh.gvec.duplicate()\n\nmesh.lvec.setArray(shadow)\nmesh.dm.localToGlobal(mesh.lvec, shadow_vec, addv=True)\n\n\n# Save fields to file\nfile = ""spiral.h5""\nmesh.save_mesh_to_hdf5(file)\nmesh.save_field_to_hdf5(file, height=height, rank=rank, shadow=shadow_vec)\n\n# Generate XDMF file for visualising in Paraview\nmeshtools.generate_xdmf(file)\n'"
tests/earlier/refine_dm.py,3,"b'""""""\nDistribute a DM from points then refine.\nModify the number of refinements with\n refine = N\n\nRun script with\n mpirun -np <procs> python refine_dm.py\n""""""\nrefine = 0\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom quagmire import FlatMesh\nfrom quagmire import tools as meshtools\n\nminX, maxX = -1., 1.\nminY, maxY = -1., 1.\ndx, dy = 0.01, 0.01\n\nx, y, bmask = meshtools.elliptical_mesh(minX, maxX, minY, maxY, dx, dy, 100, 10)\n\ninverse_bmask = ~bmask\nx = np.hstack([x, 1.1*x[inverse_bmask]])\ny = np.hstack([y, 1.1*y[inverse_bmask]])\nbmask = np.hstack([bmask, bmask[inverse_bmask]])\n\ndm = meshtools.create_DMPlex_from_points(x,y,bmask, refinement_levels=refine)\n\nmesh = FlatMesh(dm)\nmesh.save_mesh_to_hdf5(\'coarse_mesh.h5\')\n\nlocal_x = mesh.tri.x\nlocal_y = mesh.tri.y\nsimplices = mesh.tri.simplices\nbmask = mesh.bmask\ncoarse = mesh.get_boundary(""coarse"")\n\nfig = plt.figure(1)\nax = fig.add_subplot(111)\nax.triplot(local_x, local_y, simplices, c=\'b\', zorder=1)\nax.scatter(local_x[~mesh.bmask], local_y[~mesh.bmask], c=\'r\', s=100, zorder=2, label=""boundary points"")\n# ax.scatter(local_x[~coarse], local_y[~coarse], c=\'g\', s=100, zorder=3, label=""coarse points"")\nplt.legend()\nplt.show()'"
tests/earlier/surfaceprocesses.py,3,"b'""""""\nParallel test\n""""""\n\n\nimport numpy as np\nfrom mpi4py import MPI\ncomm = MPI.COMM_WORLD\n\nfrom quagmire import SurfaceProcessMesh\nfrom quagmire import tools as meshtools\n\n\n# Setup distributed mesh\n\nminX, maxX = -5., 5.\nminY, maxY = -5., 5.\n\nx, y, bmask = meshtools.elliptical_mesh(minX, maxX, minY, maxY, 0.01, 0.01, 100000, 500)\nDM = meshtools.create_DMPlex_from_points(x, y, bmask, refinement_levels=1)\n\nmesh = SurfaceProcessMesh(DM)\nx, y, simplices, bmask = mesh.get_local_mesh()\n\nheight = np.exp(-0.025*(x**2 + y**2)**2) + 0.0001\nrain   = height**2\n\nmesh.update_height(height)\nmesh.update_surface_processes(rain, np.zeros_like(rain))\n\n\nerosion_rate, deposition_rate, stream_power = \\\n         mesh.stream_power_erosion_deposition_rate_old(efficiency=0.01, \n                                                      smooth_power=0, \n                                                      smooth_low_points=2, \n                                                      smooth_erosion_rate=0, \n                                                      smooth_deposition_rate=2, \n                                                      smooth_operator=mesh.downhill_smoothing,\n                                                      centre_weight_u=0.75, centre_weight=0.5)\n\n\ndhdt = erosion_rate - deposition_rate\n\n# Save mesh to hdf5 file\nfile = ""erosion-deposition.h5""\n\nmesh.save_mesh_to_hdf5(file)\nmesh.save_field_to_hdf5(file, height=height, rain=rain,\\\n                              dhdt=dhdt,\\\n                              stream_power=stream_power,\\\n                              rank = np.ones_like(x)*comm.rank)\nmeshtools.generate_xdmf(file)'"
Documentation/Scripts/LandscapePreprocessing/ETOPO1-workflow.py,16,"b'\n# coding: utf-8\n\n# # Meshing ETOPO1\n#\n# In this notebook we:\n#\n# 1. Find the land surface in a region by filtering ETOPO1\n# 2. Optionally correct for the geoid (important in low-gradient / low-lying areas)\n# 4. Create a DM object and refine a few times\n# 5. Save the mesh to HDF5 file\n\n# In[1]:\n\nfrom osgeo import gdal\n\nimport numpy as np\n\nimport quagmire\nfrom quagmire import tools as meshtools\n\nfrom scipy.ndimage import imread\nfrom scipy.ndimage.filters import gaussian_filter\n\n\n\nAusBounds = (110, -45 , 155, -5)\nNZbounds  = (85.0, 18.0, 105.0, 36.0)\n\n## Define region of interest (here NZ)\n\nminX, minY, maxX, maxY = AusBounds\n\nxres = 250\nyres = 250\n\nxx = np.linspace(minX, maxX, xres)\nyy = np.linspace(minY, maxY, yres)\nx1, y1 = np.meshgrid(xx,yy)\nx1 += np.random.random(x1.shape) * 0.2 * (maxX-minX) / xres\ny1 += np.random.random(y1.shape) * 0.2 * (maxY-minY) / yres\n\nx1 = x1.flatten()\ny1 = y1.flatten()\n\npts = np.stack((x1, y1)).T\n\n\n##  The alternative to this is to pre-process ETOPO and save it as h5 or npz\n##  That would eliminate the gdal dependency which is quite annoying on some machines\n\ngtiff = gdal.Open(""../Notebooks/data/ETOPO1_Ice_c_geotiff.tif"")\n\nwidth = gtiff.RasterXSize\nheight = gtiff.RasterYSize\ngt = gtiff.GetGeoTransform()\nimg = gtiff.GetRasterBand(1).ReadAsArray().T\n\nimg = np.fliplr(img)\n\nsliceLeft   = int(180+minX) * 60\nsliceRight  = int(180+maxX) * 60\nsliceBottom = int(90+minY) * 60\nsliceTop    = int(90+maxY) * 60\n\nLandImg = img[ sliceLeft:sliceRight, sliceBottom:sliceTop].T\nLandImg = np.flipud(LandImg)\n\n\n# In[37]:\n\ncoords = np.stack((y1, x1)).T\n\nim_coords = coords.copy()\nim_coords[:,0] -= minY\nim_coords[:,1] -= minX\n\nim_coords[:,0] *= LandImg.shape[0] / (maxY-minY)\nim_coords[:,1] *= LandImg.shape[1] / (maxX-minX)\nim_coords[:,0] =  LandImg.shape[0] - im_coords[:,0]\n\n\n# In[38]:\n\nfrom scipy import ndimage\n\nmeshheights = ndimage.map_coordinates(LandImg, im_coords.T, order=3, mode=\'nearest\').astype(np.float)\n\n# Fake geoid for this particular region\n# meshheights -= 40.0 * (y1 - minY) / (maxY - minY)\n\n\n# In[39]:\n\n## Filter out the points we don\'t want at all\n\npoints = meshheights > -50\n\nm1s = meshheights[points]\nx1s = x1[points]\ny1s = y1[points]\n\nsubmarine = m1s < 0.0\nsubaerial = m1s >= 0.0\n\n\n# ### 3. Create the DM\n#\n# The points are now read into a DM and refined so that we can achieve very high resolutions. Refinement is achieved by adding midpoints along line segments connecting each point.\n\n# In[75]:\n\nDM = meshtools.create_DMPlex_from_points(x1s, y1s, submarine, refinement_levels=3)\nmesh = quagmire.SurfaceProcessMesh(DM, verbose=True)\n\n\n# In[77]:\n\nx2r = mesh.tri.x\ny2r = mesh.tri.y\nsimplices = mesh.tri.simplices\nbmaskr = mesh.bmask\n\n\n# In[78]:\n\n## Now re-do the allocation of points to the surface.\n## In parallel this will be done process by process for a sub-set of points\n\ncoords = np.stack((y2r, x2r)).T\n\nim_coords = coords.copy()\nim_coords[:,0] -= minY\nim_coords[:,1] -= minX\n\nim_coords[:,0] *= LandImg.shape[0] / (maxY-minY)\nim_coords[:,1] *= LandImg.shape[1] / (maxX-minX)\nim_coords[:,0] =  LandImg.shape[0] - im_coords[:,0]\n\n\n# In[79]:\n\nfrom scipy import ndimage\n\nspacing = 1.0\ncoords = np.stack((y2r, x2r)).T / spacing\n\nmeshheights = ndimage.map_coordinates(LandImg, im_coords.T, order=3, mode=\'nearest\')\nmeshheights = mesh.rbf_smoother(meshheights, iterations=2)\n\nsubaerial =  meshheights >= 0.0\nsubmarine = ~subaerial\nmesh.bmask = subaerial\n\n\nmesh.update_height(meshheights*0.001)\nraw_height = mesh.height\n\nfor ii in range(0,2):\n\n\tnew_heights=mesh.low_points_local_patch_fill(its=2, smoothing_steps=2)\n\tmesh._update_height_partial(new_heights)\n\n\n\tfor iii in range(0, 5):\n\t\tnew_heights = mesh.low_points_swamp_fill()\n\t\tmesh._update_height_partial(new_heights)\n\t\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\t\tif mesh.rank == 0:\n\t\t\tprint(""Global low points: "",glows)\n\n\t\tif glows == 0:\n\t\t\tbreak\n\n\n\tnew_heights=mesh.low_points_local_flood_fill(its=10, scale=1.0001)\n\tmesh._update_height_partial(new_heights)\n\n\n\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\tif mesh.rank == 0:\n\t\tprint(""Global low points: "",glows)\n\n\tif glows == 0:\n\t\tbreak\n\n\nmesh.update_height(new_heights)\n\nnits, flowpaths = mesh.cumulative_flow_verbose(mesh.area*np.ones_like(mesh.height), verbose=True, maximum_its=2500)\nflowpaths2 = mesh.rbf_smoother(flowpaths, iterations=1)\n\n\n# ## 5. Save to HDF5\n#\n# Save the mesh to an HDF5 file so that it can be visualised in Paraview or read into Quagmire another time. There are two ways to do this:\n#\n# 1. Using the `save_DM_to_hdf5` function in meshtools, or\n# 2. Directly from trimesh interface using `save_mesh_to_hdf5` method.\n#\n# Remember to execute `petsc_gen_xdmf.py austopo.h5` to create the XML file structure necessary to visualise the mesh in paraview.\n\n# In[85]:\n\nfilename = \'AusFlow1a.h5\'\n\ndecomp = np.ones_like(mesh.height) * mesh.dm.comm.rank\n\nlow_points = mesh.identify_low_points()\nglow_points = mesh.lgmap_row.apply(low_points.astype(PETSc.IntType))\nctmt = mesh.uphill_propagation(low_points,  glow_points, scale=1.0, its=250, fill=-1).astype(np.int)\n\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height0=raw_height,\n\t\t\t\t\t\t\t\t  height=mesh.height,\n                                  slope=mesh.slope,\t\t\n                                  flowLP=np.sqrt(flowpaths2),\n                                  decomp=decomp,\n                                  lpcatch=ctmt,\n                                  lakes=mesh.height-raw_height)\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n'"
Documentation/Scripts/LandscapePreprocessing/EggBoxFlow.py,12,"b'\n# coding: utf-8\n\n# # TriMeshes\n#\n#\n\n# In[1]:\n\n# import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.ndimage import imread\nfrom quagmire import tools as meshtools\nfrom petsc4py import PETSc\nfrom mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size()\nrank = comm.Get_rank()\n\n\n# get_ipython().magic(\'matplotlib inline\')\n\n\nfrom quagmire import FlatMesh\nfrom quagmire import TopoMesh # all routines we need are within this class\nfrom quagmire import SurfaceProcessMesh\n\nminX, maxX = 0.0, 10.0\nminY, maxY = 0.0, 10.0,\n\nx1, y1, bmask1 = meshtools.square_mesh(minX, maxX, minY, maxY, 0.1, 0.1, 50001, 200)\n\n# In[41]:\n\nDM = meshtools.create_DMPlex_from_points(x1, y1, bmask1, refinement_levels=2)\nmesh = SurfaceProcessMesh(DM)  ## cloud array etc can surely be done better ...\n\nprint(mesh.dm.comm.rank, ""Number of nodes - "", mesh.npoints)\n# print mesh.dm.comm.rank, ""Range - "", mesh.lgmap_row.apply([0, mesh.npoints-1])\n# print mesh.dm.comm.rank, ""Range2 - "", mesh.lgmap_col.apply([0, mesh.npoints-1])\n\n\n# In[42]:\n\nx = mesh.coords[:,0]\ny = mesh.coords[:,1]\nbmask = mesh.bmask\n\nradius  = np.sqrt((x**2 + y**2))\ntheta   = np.arctan2(y,x)\n\nheight  = 1.0 - (np.sin(0.5*x*np.pi)**2 * np.sin(0.5*y*np.pi)**2)\nheight  += np.random.random(height.shape) * 0.001\n\n## Steep eggbox - algorithm is quick\n# height  += 1.0 * (x + y) \n## Flatter eggbox - takes longer to find catchment spills and iterate\nheight  += 0.25 * (x + 0.5*y) \n\n\nheight0 = height.copy()\nnew_heights = height.copy()\n\n\nrainfall = np.ones_like(height)\nrainfall = height ** 2.0\n# rainfall[np.where( radius > 1.0)] = 0.0\n\nmesh.update_height(height0)\t    \n\nnew_heights = mesh.low_points_swamp_fill(saddles=False)\nmesh._update_height_partial(new_heights)\n\n\nprint(rank, "" : "", ""Swamp fill ... "")\n\nfor ii in range(0,3):\n\n\tnew_heights=mesh.low_points_local_patch_fill(its=2, smoothing_steps=2)\n\n\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\tif rank == 0:\n\t\tprint(""gLows: "",glows)\n\n\tfor iii in range(0, 5):\n\t\tnew_heights = mesh.low_points_swamp_fill(saddles=True)\n\t\tmesh._update_height_partial(new_heights)\n\t\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\t\tif rank == 0:\n\t\t\tprint(""gLows: "",glows)\n\n\t\tif glows == 0:\n\t\t\tbreak\n\n\t\tnew_heights = mesh.low_points_swamp_fill(saddles=False)\n\t\tmesh._update_height_partial(new_heights)\n\n\n\t# new_heights=mesh.low_points_local_flood_fill(its=1000, scale=1.0001)\n\t# mesh._update_height_partial(new_heights)\n\n\n\t# glows, glow_points = mesh.identify_global_low_points(global_array=False)\n\t# if rank == 0:\n\t# \tprint ""gLows: "",glows\n\n\tif glows == 0:\n\t\tbreak\n\n\nprint(""Compute downhill flow ... "")\n\n# In[44]:\n\nmesh.downhill_neighbours = 2\nmesh.update_height(new_heights)\nlow_points1= mesh.identify_low_points()\nprint(rank,"" : Lows "", low_points1.shape[0])\n\nits, flowpaths = mesh.cumulative_flow_verbose(mesh.area)\nflowpaths = mesh.rbf_smoother(flowpaths, iterations=1)\nsqrtpaths = np.sqrt(flowpaths)\n\ndecomp = np.ones_like(mesh.height) * mesh.dm.comm.rank\n\nlow_points = mesh.identify_low_points(include_shadows=False)\nglow_points = mesh.lgmap_row.apply(low_points.astype(PETSc.IntType))\nctmt = mesh.uphill_propagation(low_points,  glow_points, scale=1.000001, its=1000, fill=-1).astype(np.int)\n\nlist_of_lows = comm.gather(glow_points, root=0)\n\nif rank == 0:\n   for i in range(size):\n       print(""Proc "",i,"":"",list_of_lows[i], mesh.npoints)\n\n   lows = np.hstack(list_of_lows)\n   lows = np.unique(lows)\n   print(lows)\n\nelse:\n   pass\n\n\n\nfilename=""Eggbox.h5""\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height0=height0,\n\t\t\t\t\t\t\t\t  height=mesh.height,\n\t\t\t\t\t\t\t\t  swamps=mesh.height-height0,\n\t\t\t\t\t\t\t\t  deltah=new_heights-height0,\n                                  slope=mesh.slope,\n                                  flow=np.sqrt(flowpaths),\n                                  catchments=ctmt,\n                                  bmask=mesh.bmask,\n                                  decomp=decomp)\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n'"
Documentation/Scripts/LandscapePreprocessing/Low_points.py,8,"b'\n# coding: utf-8\n\n# # Poisson disc sampling\n# \n# Quality meshes are important for producing reliable solution in surface process modelling. For any given node in an unstructured mesh, its neighbours should be spaced more or less at an equal radius. For this we turn to Poisson disc sampling using an efficient $O(N)$ [algorithm](http://www.cs.ubc.ca/~rbridson/docs/bridson-siggraph07-poissondisk.pdf).\n# \n# The premise of this algorithm is to ensure that points are tightly packed together, but no closer than a specified minimum distance. This distance can be uniform across the entire domain, or alternatively a 2D numpy array of radius lengths can be used to bunch and relax the spacing of nodes.\n\n# In[1]:\n\n# import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.ndimage import imread\nfrom quagmire import tools as meshtools\nfrom petsc4py import PETSc\nfrom mpi4py import MPI\nfrom quagmire import SurfaceProcessMesh\n\n\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size()\nrank = comm.Get_rank()\n\n\n# ## Uniform spacing\n\n# ### Landscape\n# \n# In this example we create higher resolution where the slope is steeper.\n\n# In[2]:\n\ndem = imread(\'../Notebooks/data/port_macquarie.tif\', mode=\'F\')\n\nrows, columns = dem.shape\naspect_ratio = float(columns) / float(rows)\n\nspacing = 5.0\n\nminX, maxX = 0.0, spacing*dem.shape[1]\nminY, maxY = 0.0, spacing*dem.shape[0]\n\n\n# fig = plt.figure(1, figsize=(10*aspect_ratio,10))\n# ax = fig.add_subplot(111)\n# ax.axis(\'off\')\n# im = ax.imshow(dem, cmap=\'terrain_r\', origin=\'lower\', aspect=aspect_ratio)\n# fig.colorbar(im, ax=ax, label=\'height\')\n\n\n# In[3]:\n\ngradX, gradY = np.gradient(dem, 5., 5.) # 5m resolution in each direction\nslope = np.hypot(gradX, gradY)\n\nprint((""min/max slope {}"".format((slope.min(), slope.max()))))\n\n\n# In[4]:\n\nheight, width = slope.shape\n\nradius_min = 50.0\nradius_max = 100.0\n\nradius = 1.0/(slope + 0.02)\nradius = (radius - radius.min()) / (radius.max() - radius.min()) \nradius = radius * (radius_max-radius_min) + radius_min\n\n# apply gaussian filter for better results\nfrom scipy.ndimage import gaussian_filter\nradius2 = gaussian_filter(radius, 5.)\n\n# In[5]:\n\nx, y, bmask = meshtools.poisson_square_mesh(minX, maxX, minY, maxY, spacing*2.0, boundary_samples=500, r_grid=radius2*2.0)\nprint((""{} samples"".format(x.size)))\n\n\n# In[6]:\n\nfrom scipy import ndimage\n\ncoords = np.stack((y, x)).T / spacing\nmeshheights = ndimage.map_coordinates(dem, coords.T, order=3, mode=\'nearest\')\n\n\n\ndm = meshtools.create_DMPlex_from_points(x, y, bmask, refinement_levels=2)\nmesh = SurfaceProcessMesh(dm)\n\n# Triangulation reorders points\n\nprint(rank, "" : "", ""Map DEM to local triangles"")\n\ncoords = np.stack((mesh.tri.points[:,1], mesh.tri.points[:,0])).T / spacing\nmeshheights = ndimage.map_coordinates(dem, coords.T, order=3, mode=\'nearest\')\n\nprint(rank, "" : "", ""Rebuild height information"")\n\n\nmesh.downhill_neighbours = 2\nmesh.update_height(meshheights)\n\nnew_heights = mesh.low_points_swamp_fill(saddles=False)\nmesh._update_height_partial(new_heights)\n\nmesh.height = mesh.rbf_smoother(mesh.height, iterations=1)\nmesh.update_height(mesh.height)\nraw_heights = mesh.height.copy()\n\ngradient_max = mesh.slope.max()\ngradient_mean = mesh.slope.mean()\nflat_spots = np.where(mesh.slope < gradient_mean*0.01)[0]\nlow_points = mesh.identify_low_points()\n\n# print statistics\nprint((""mean gradient {}\\nnumber of flat spots {}\\nnumber of low points {}"".format(gradient_mean,\n                                                                                  flat_spots.size,\n                                                                                  low_points.shape[0])))\n\n\n# In[18]:\n\nfor ii in range(0,5):\n\n\n\tnew_heights=mesh.low_points_local_patch_fill(its=2, smoothing_steps=2)\n\n\n\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\tif rank == 0:\n\t\tprint(""gLows: "",glows)\n\n\tfor iii in range(0, 5):\n\t\tnew_heights = mesh.low_points_swamp_fill()\n\t\tmesh._update_height_partial(new_heights)\n\t\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\t\tif rank == 0:\n\t\t\tprint(""gLows: "",glows)\n\n\t\tif glows == 0:\n\t\t\tbreak\n\n\n\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\tif rank == 0:\n\t\tprint(""gLows: "",glows)\n\n\tif glows == 0:\n\t\tbreak\n\n\nmesh.downhill_neighbours = 2\nmesh.update_height(mesh.height)\n\nits, flowpaths2 = mesh.cumulative_flow_verbose(mesh.area, maximum_its=2000, verbose=True)\nflowpaths2 = mesh.rbf_smoother(flowpaths2)\n\ndecomp = np.ones_like(mesh.height) * mesh.dm.comm.rank\n\nlow_points = mesh.identify_low_points()\nglow_points = mesh.lgmap_row.apply(low_points.astype(PETSc.IntType))\n\nctmt = mesh.uphill_propagation(low_points,  glow_points, scale=1.0, its=250, fill=-1).astype(np.int)\n\n\nfilename = \'portmacca1.h5\'\n\nmesh.save_mesh_to_hdf5(filename)\n\nmesh.save_field_to_hdf5(filename, \n\t\t\t\t\t\theight0=raw_heights,\n\t\t\t\t\t\theight=mesh.height, \n\t\t\t\t\t\tslope=mesh.slope, \n\t\t\t\t\t\tflow=np.sqrt(flowpaths2), \n\t\t\t\t\t\tlakes=(mesh.height - raw_heights),\n\t\t\t\t\t\tcatchments=ctmt,\n\t\t\t\t\t\tdecomp=decomp,\n\t\t\t\t\t\tbmask=mesh.bmask)\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n\n\n\n\n\n'"
Documentation/Scripts/Meshes/Import.py,0,"b'# Just a quick check to see that all the relevant packages work\n\n\n# %% define function to import and report ""path""\n\ndef import_tester(module_name):\n\n    import importlib\n    print(""Attempting to import module: {}"".format(module_name), end="""")\n    try:\n        imported = importlib.import_module(module_name)\n        print(""\\tSuccess - {}"".format(imported.__file__))\n    except:\n        print (""\\tFailure !"")\n\n# %% Test required imports\n\nimport_tester(""numpy"")\nimport_tester(""scipy"")\nimport_tester(""stripy"")\nimport_tester(""mpi4py"")\nimport_tester(""petsc4py"")\nimport_tester(""quagmire"")\nimport_tester(""lavavu"")\n\n# %%\n'"
Documentation/Scripts/Meshes/MeshVariable.py,4,"b'from quagmire.tools import meshtools\nfrom quagmire import FlatMesh, TopoMesh, SurfaceProcessMesh\nfrom quagmire.mesh import MeshVariable, VectorMeshVariable\n\nimport petsc4py\nimport mpi4py\nimport quagmire\n\nimport numpy as np\n\ncomm = mpi4py.MPI.COMM_WORLD\n\nminX, maxX = -5.0, 5.0\nminY, maxY = -5.0, 5.0,\ndx, dy = 0.1, 0.1\n\nx, y, simplices = meshtools.elliptical_mesh(minX, maxX, minY, maxY, dx, dy)\n\nDM = meshtools.create_DMPlex_from_points(x, y, bmask=None)\n\nmesh = TopoMesh(DM, downhill_neighbours=1, verbose=False)\n\nprint(""{}: mesh size:{}"".format(comm.rank, mesh.npoints))\n\n\nv = MeshVariable(""stuff"", mesh)\nh = np.ones(mesh.npoints) * comm.rank\n\nv.data = h\n# v.data[1] = 2.0\n\ngdata = DM.getGlobalVec()\nv.getGlobalVector(gdata)\n\nprint(""{}: vl min/max = {}/{}"".format(comm.rank, v.data.min(), v.data.max()))\nprint(""{}: vg min/max = {}/{}"".format(comm.rank, gdata.min(), gdata.max()))\n\nv.data = mesh.sync(v.data)\n\n\nprint(""{}: vlSync min/max = {}/{}"".format(comm.rank, v.data.min(), v.data.max()))\nprint(""{}: vgSync min/max = {}/{}"".format(comm.rank, gdata.min(), gdata.max()))\n\nv.sync(mergeShadow=False)\n\nprint(v.data)\nv.save()\n\nv2 = MeshVariable(""stuff"", mesh)\nv2.load(""stuff.h5"")\nprint(""{}: save/load "".format(comm.rank), np.all(v2.data == v.data))\n\ndv = v.gradient()\nprint(type(dv))\n\n\nprint(""INTERP"")\nprint(v.interpolate([0.1, 10.0], [0.1, 10.0], err=False))\nprint(v.interpolate(0.1, 0.1))\nprint(v.evaluate(0.1, 0.1))\n\nv = VectorMeshVariable(""more_stuff"", mesh)\nh = np.ones((mesh.npoints, 2)) * comm.rank\n\nv.data = h\n\ngdata = DM.getCoordinates().duplicate()\nv.getGlobalVector(gdata)\n\nprint(""VectorMeshVariable\\n-------"")\nprint(""{}: vl min/max = {}/{}"".format(comm.rank, v.data.min(), v.data.max()))\nprint(""{}: vg min/max = {}/{}"".format(comm.rank, gdata.min(), gdata.max()))\n\nv.sync()\nv.sync()\n\nprint(""{}: vlSync min/max = {}/{}"".format(comm.rank, v.data.min(), v.data.max()))\nprint(""{}: vgSync min/max = {}/{}"".format(comm.rank, gdata.min(), gdata.max()))\n\nprint(v.data)\nv.save()\n\n\nv2 = VectorMeshVariable(""more_stuff"", mesh)\nv2.load(""more_stuff.h5"")\nprint(""{}: save/load "".format(comm.rank), np.all(v2.data == v.data))\n'"
Documentation/Scripts/Meshes/PixMeshLandscape.py,11,"b'\n# coding: utf-8\n\n# # PixMeshes\n# \n# A pixmesh is an optimised regular mesh for handling high resolution DEM data of a small region. The issues of mesh regularity on the landscape analysis need to be considered (multiple descent pathways and smoothing, for example). \n\n# In[1]:\n\n# import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.ndimage import imread\nfrom quagmire import tools as meshtools\n# get_ipython().magic(\'matplotlib inline\')\n\n\n# ### Landscape\n# \n# \n\n# In[2]:\n\ndem = imread(\'data/port_macquarie.tif\', mode=\'F\')\n#dem = np.fliplr(dem)\n\nrows, columns = dem.shape\naspect_ratio = float(columns) / float(rows)\n\nspacing = 5.0\n\n#minX, maxX = 0.0, spacing*dem.shape[1]\n#minY, maxY = 0.0, spacing*dem.shape[0]\n\n\n#fig = plt.figure(1, figsize=(10*aspect_ratio,10))\n#ax = fig.add_subplot(111)\n#ax.axis(\'off\')\n#im = ax.imshow(dem, cmap=\'terrain_r\', origin=\'upper\')\n#fig.colorbar(im, ax=ax, label=\'height\')\n\n\n# In[3]:\n\n# dem.shape, dem.size\n\n\n# In[ ]:\n\n\n\n\n# ## PixMesh\n# \n# We saw how to triangulate, but this is how to use the regular equivalent. 18000000 points is still high, so let\'s work at 1/4 that size. This is still over four million points, so this gives us a sense of the trade off between regular meshes and triangulations.\n# \n# The creation of the distributed object, like the triangulation, reorders x,y points.\n\n# In[4]:\n\nfrom quagmire import FlatMesh \nfrom quagmire import TopoMesh # all routines we need are within this class\nfrom quagmire import SurfaceProcessMesh\n\nDM = meshtools.create_DMDA(0.0, 5124.0, 0.0, 3612.0, 5124/2, 3612/2 )\n\n\n# In[9]:\n\nmesh = SurfaceProcessMesh(DM, verbose=True, neighbour_cloud_size=25)  ## cloud array etc can surely be done better ... \nmesh.downhill_neighbours = 3\n\n\n# In[ ]:\n\n\n\n\n# In[10]:\n\n# Creating the DM reorders points\n\nprint(mesh.neighbour_array.shape)\nprint(mesh.coords.shape)\nprint(dem[::2,::2].shape)\n\n\n# In[11]:\n\nmesh.update_height(dem[::2,::2].reshape(-1))\n\n\n# In[22]:\n\nmesh_heights_no_lows = mesh.handle_low_points(its=50)\n\n\n# In[23]:\n\nflowpaths = mesh.cumulative_flow(np.ones_like(mesh.height))\nflowpathsS = mesh.rbf_smoother(flowpaths)\nlow_points = mesh.identify_low_points()\n\nprint(low_points.shape[0])\n\n# In[24]:\n\nflats = np.where(mesh.identify_flat_spots())[0]\nprint(flats.shape[0])\n\n\n# In[36]:\n\nmanifold = np.reshape(mesh.coords, (-1,2))\nmanifold = np.insert(manifold, 2, values=mesh.height*0.2, axis=1)\n\n\n# In[39]:\n\nfrom LavaVu import lavavu\n\nlv = lavavu.Viewer(border=False, background=""#FFFFFF"", resolution=[1000,600], near=-10.0)\n\ntopo = lv.points(pointsize=1.0, pointtype=\'flat\')\ntopo.vertices(manifold)\ntopo.values(mesh.height, label=\'height\')\n# topo.values(np.sqrt(flowpaths), label=\'flow\')\n\n\ntopo2 = lv.points(pointsize=1.0, pointtype=\'flat\')\ntopo2.vertices(manifold+(0.0,0.0,1.0))\n# topo.values(mesh.height, label=\'height\')\ntopo2.values(np.sqrt(flowpathsS), label=\'flow\')\n\n\ntopo.colourmap([""#004420"", ""#FFFFFF"", ""#444444""] , logscale=False, range=[-200.0, 400.0])   # Apply a built in colourmap\n# topo.colourmap([""#FFFFFF:0.0"", ""#0033FF:0.3"", ""#000033""], logscale=False)   # Apply a built in colourmap\ntopo2.colourmap([""#FFFFFF:0.0"", ""#0033FF:0.1"", ""#000033""], logscale=True)   # Apply a built in colourmap\n\n\n\n\n# In[40]:\n\n## Viewer\n\nlv.window()\n\n# tris.control.Range(property=\'zmin\', range=(-1,1), step=0.001)\n# lv.control.Range(command=\'background\', range=(0,1), step=0.1, value=1)\n# lv.control.Range(property=\'near\', range=[-10,10], step=2.0)\nlv.control.Checkbox(property=\'axis\')\nlv.control.Command()\nlv.control.ObjectList()\nlv.control.show()\n\n\n# Landscape analysis statistics\n\n# In[ ]:\n\ngradient_max = mesh.slope.max()\ngradient_mean = mesh.slope.mean()\nflat_spots = np.where(mesh.slope < gradient_mean*0.001)[0]\nlow_points = mesh.identify_low_points()\n\nnodes = np.arange(0, mesh.npoints)\nlows =  np.where(np.logical_and(mesh.down_neighbour[1] == nodes, mesh.height > 0.5))[0]\n\n# print statistics\nprint((""mean gradient {}\\nnumber of flat spots {}\\nnumber of low points {}"".format(gradient_mean,\n                                                                                  flat_spots.size,\n                                                                                  low_points.shape[0])))\n\n\n# In[ ]:\n\nfilename = \'port_macquarie_pixmesh.h5\'\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height=mesh.height, slope=mesh.slope, flow=np.sqrt(flowpaths))\n\n# to view in Paraview\n# meshtools.generate_xdmf(filename)\n\n\n# In[ ]:\n\n\n\n\n# In[ ]:\n\n\n\n'"
Documentation/Scripts/Meshes/Read-hdf5-mesh.py,3,"b'\n# coding: utf-8\n\n# In[4]:\n\nfrom quagmire import SurfaceProcessMesh\nfrom quagmire import tools as meshtools\nimport numpy as np\n\nimport petsc4py\nfrom mpi4py import MPI\nfrom petsc4py import PETSc\n\n\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size()\nrank = comm.Get_rank()\n\n## pdoc is trying to read this and it is breaking !\n\n\n\n# In[5]:\n\n# import h5py\n\n# meshFile = h5py.File(name=""./portmacca.h5"", mode=""r"")\n# points = meshFile[""geometry""][""vertices""]\n# x1 = points.value[:,0]\n# y1 = points.value[:,1]\n\n# print(""Available data in "", meshFile)\n# print(list(meshFile[\'fields\'].keys()))\n\n# bmask = meshFile[""fields""][""bmask""].value[:].astype(bool)\n# height = meshFile[""fields""][""height""].value[:]\n# lakes = meshFile[""fields""][""swamps""].value[:]\n\n\n# # In[6]:\n\n# dm  = meshtools.create_DMPlex_from_points(x1,y1,bmask=bmask)\n# SPM = SurfaceProcessMesh(dm)\n# x = SPM.coords[:,0]\n# y = SPM.coords[:,1]\n\n\n# # In[7]:\n\n# from scipy.spatial import ckdtree\n# old_nodes = ckdtree.cKDTree( points.value )\n\n\n# # In[8]:\n\n# distance, mapping = old_nodes.query(SPM.coords)\n# SPM.update_height(height[mapping])\n# lakefill = lakes[mapping]\n\n\n# # In[9]:\n\n# its, flowpaths2 = SPM.cumulative_flow_verbose(SPM.area, maximum_its=2000, verbose=True)\n\n\n# # In[10]:\n\n# low_points = SPM.identify_low_points()\n# low_points_plus = SPM.identify_low_points(include_shadows=True)\n# print(dm.comm.rank,""Lows: "", low_points.shape[0], low_points_plus.shape[0])\n\n# glow_points = SPM.lgmap_row.apply(low_points.astype(PETSc.IntType))\n# list_of_lows = comm.gather(glow_points, root=0)\n\n\n# if rank == 0:\n#    for i in range(size):\n#        print(""Proc "",i,"":"",list_of_lows[i], SPM.npoints)\n\n#    lows = np.hstack(list_of_lows)\n#    lows = np.unique(lows)\n#    print(lows)\n\n# else:\n#    pass\n\n\n# # In[12]:\n\n# filename = \'portmacca-from-hdf.h5\'\n\n# SPM.save_mesh_to_hdf5(filename)\n# SPM.save_field_to_hdf5(filename, \n# \t\t\t\t\t\theight=SPM.height,\n# \t\t\t\t\t\tflow=np.sqrt(flowpaths2), \n# \t\t\t\t\t\tlakes=lakefill)\n\n# # to view in Paraview\n# meshtools.generate_xdmf(filename)\n\n\n\n# # In[ ]:\n\n\n\n'"
Documentation/Scripts/Meshes/TriMeshOctoPants.py,13,"b'\n# coding: utf-8\n\n# # TriMeshes\n#\n#\n\n# In[1]:\n\n# import matplotlib.pyplot as plt\nimport numpy as np\nfrom quagmire import tools as meshtools\nfrom petsc4py import PETSc\nfrom mpi4py import MPI\n\ncomm = MPI.COMM_WORLD\nsize = comm.Get_size()\nrank = comm.Get_rank()\n\n\n# get_ipython().magic(\'matplotlib inline\')\n\n\nfrom quagmire import FlatMesh\nfrom quagmire import TopoMesh # all routines we need are within this class\nfrom quagmire import SurfaceProcessMesh\n\nminX, maxX = -5.0, 5.0\nminY, maxY = -5.0, 5.0,\n\nx1, y1, bmask1 = meshtools.poisson_elliptical_mesh(minX, maxX, minY, maxY, 0.1, 500, r_grid=None)\n\n# In[41]:\n\nDM = meshtools.create_DMPlex_from_points(x1, y1, bmask1, refinement_levels=2)\nmesh = SurfaceProcessMesh(DM)  ## cloud array etc can surely be done better ...\n\nprint(mesh.dm.comm.rank, ""Number of nodes - "", mesh.npoints)\nprint(mesh.dm.comm.rank, ""Range - "", mesh.lgmap_row.apply([0, mesh.npoints-1]))\nprint(mesh.dm.comm.rank, ""Range2 - "", mesh.lgmap_col.apply([0, mesh.npoints-1]))\n\n\n# In[42]:\n\nx = mesh.coords[:,0]\ny = mesh.coords[:,1]\nbmask = mesh.bmask\n\nradius  = np.sqrt((x**2 + y**2))\ntheta   = np.arctan2(y,x)\n\nheight  = np.exp(-0.025*(x**2 + y**2)**2) + 0.25 * (0.2*(radius))**4  * np.cos(7.0*theta)**2 ## Less so\nheight  += 0.1 * (1.0-0.5*radius)**2\nheight  += np.random.random(height.shape) * 0.001\n\nheight0 = height.copy()\nnew_heights = height.copy()\n\n\nrainfall = np.ones_like(height)\nrainfall = height ** 2.0\n# rainfall[np.where( radius > 1.0)] = 0.0\n\nmesh.update_height(height0)\t    \n\n\nlow_points = mesh.identify_low_points(include_shadows=False)\nglow_points = mesh.lgmap_row.apply(low_points.astype(PETSc.IntType))\nctmt0 = mesh.uphill_propagation(low_points,  glow_points, scale=1.000001, its=1000, fill=-1).astype(np.int)\n\n\nnew_heights = mesh.low_points_swamp_fill(saddles=False)\nmesh._update_height_partial(new_heights)\n\n\nprint(rank, "" : "", ""Swamp fill ... "")\n\nfor ii in range(0,5):\n\n\tnew_heights=mesh.low_points_local_patch_fill(its=2, smoothing_steps=2)\n\n\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\tif rank == 0:\n\t\tprint(""gLows: "",glows)\n\n\tfor iii in range(0, 10):\n\t\tnew_heights = mesh.low_points_swamp_fill(saddles=True)\n\t\tmesh._update_height_partial(new_heights)\n\n\t\tnew_heights = mesh.low_points_swamp_fill(saddles=False)\n\t\tmesh._update_height_partial(new_heights)\n\t\n\t\tglows, glow_points = mesh.identify_global_low_points(global_array=False)\n\t\tif rank == 0:\n\t\t\tprint(""gLows: "",glows)\n\n\t\tif glows == 0:\n\t\t\tbreak\n\n\n\n\t# new_heights=mesh.low_points_local_flood_fill(its=1000, scale=1.0001)\n\t# mesh._update_height_partial(new_heights)\n\n\n\t# glows, glow_points = mesh.identify_global_low_points(global_array=False)\n\t# if rank == 0:\n\t# \tprint ""gLows: "",glows\n\n\tif glows == 0:\n\t\tbreak\n\n\nprint(""Compute downhill flow ... "")\n\n# In[44]:\n\nmesh.downhill_neighbours = 2\nmesh.update_height(new_heights)\nlow_points1= mesh.identify_low_points()\nprint(rank,"" : Lows "", low_points1.shape[0])\n\nits, flowpaths = mesh.cumulative_flow_verbose(mesh.area, maximum_its=500, verbose=True)\nsqrtpaths = np.sqrt(flowpaths)\n\ndecomp = np.ones_like(mesh.height) * mesh.dm.comm.rank\n\nlow_points = mesh.identify_low_points(include_shadows=False)\nglow_points = mesh.lgmap_row.apply(low_points.astype(PETSc.IntType))\nctmt = mesh.uphill_propagation(low_points,  glow_points, scale=1.000001, its=1000, fill=-1).astype(np.int)\n\nlist_of_lows = comm.gather(glow_points, root=0)\n\nif rank == 0:\n   for i in range(size):\n       print(""Proc "",i,"":"",list_of_lows[i], mesh.npoints)\n\n   lows = np.hstack(list_of_lows)\n   lows = np.unique(lows)\n   print(lows)\n\nelse:\n   pass\n\n\n\nfilename=""Octopants.h5""\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height0=height0,\n\t\t\t\t\t\t\t\t  height=mesh.height,\n\t\t\t\t\t\t\t\t  swamps=mesh.height-height0,\n\t\t\t\t\t\t\t\t  deltah=new_heights-height0,\n                                  slope=mesh.slope,\n                                  flow=np.sqrt(flowpaths),\n                                  catchments0=ctmt0,\n                                  catchments=ctmt,\n                                  bmask=mesh.bmask,\n                                  decomp=decomp)\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n'"
Documentation/Scripts/Meshes/TrimeshLandscape.py,21,"b'\n# coding: utf-8\n\n# # Manipulation of a landscape with a triangular mesh\n#\n# Quality meshes are important for producing reliable solution in surface process modelling. For any given node in an unstructured mesh, its neighbours should be spaced more or less at an equal radius. For this we turn to Poisson disc sampling using an efficient $O(N)$ [algorithm](http://www.cs.ubc.ca/~rbridson/docs/bridson-siggraph07-poissondisk.pdf).\n#\n# The premise of this algorithm is to ensure that points are tightly packed together,\n# but no closer than a specified minimum distance. This distance can be uniform across\n# the entire domain, or alternatively a 2D numpy array of radius lengths can be used\n# to bunch and relax the spacing of nodes.\n\n\n# In[1]:\n\n# import matplotlib.pyplot as plt\nimport numpy as np\n# from matplotlib.pyplot import imread\nimport imageio\nfrom quagmire import tools as meshtools\n\n# get_ipython().magic(\'matplotlib inline\')\n\n\n# ### Landscape\n#\n# In this example we create higher resolution according to the laplacian of the topography which is related to the local-slope-diffusion term.\n\n# In[2]:\n\n!pwd\n\ndem = imageio.imread(\'../../data/port_macquarie.tif\')\ndem = np.fliplr(dem)\n\ndem.shape\nrows, columns = dem.shape\naspect_ratio = float(columns) / float(rows)\n\nspacing = 5.0\n\nminX, maxX = 0.0, spacing*dem.shape[1]\nminY, maxY = 0.0, spacing*dem.shape[0]\n\n\n#fig = plt.figure(1, figsize=(10*aspect_ratio,10))\n#ax = fig.add_subplot(111)\n#ax.axis(\'off\')\n#im = ax.imshow(dem, cmap=\'terrain_r\', origin=\'lower\')\n#fig.colorbar(im, ax=ax, label=\'height\')\n\n\n# In[3]:\n\ngradX, gradY = np.gradient(dem, 5., 5.) # 5m resolution in each direction\ngradXX, gradXY = np.gradient(gradX, 5., 5.)\ngradYX, gradYY = np.gradient(gradY, 5., 5.)\n\nlaplacian = np.abs(gradYY + gradXX)\nslope = np.hypot(gradX, gradY)\n\n# In[4]:\n\nheight, width = slope.shape\n\nradius_min = 50.0\nradius_max = 150.0\n\nradius = 1.0/(laplacian + 0.02)\nradius = (radius - radius.min()) / (radius.max() - radius.min())\nradius = radius * (radius_max-radius_min) + radius_min\n\n# apply gaussian filter for better results\nfrom scipy.ndimage import gaussian_filter\nradius2 = gaussian_filter(radius, 5.)\n\n\n#fig = plt.figure(1, figsize=(10*aspect_ratio, 10))\n#ax = fig.add_subplot(111)\n#ax.axis(\'off\')\n#im = ax.imshow((radius2), cmap=\'jet\', origin=\'lower\', aspect=aspect_ratio)\n#fig.colorbar(im, ax=ax, label=\'radius2\')\n\n#plt.show()\n\n\n# In[5]:\n\nx1, y1, bmask1 = meshtools.poisson_square_mesh(minX, maxX, minY, maxY, spacing*20,\n                                            boundary_samples=100)\nprint((""{} samples"".format(x1.size)))\n\n# randomize\n\nx = np.zeros_like(x1)\ny = np.zeros_like(y1)\nbmask = bmask1.copy()\n\nindex = np.random.permutation(x.shape[0])\n\nx[:] = x1[index]\ny[:] = y1[index]\nbmask[:] = bmask1[index]\n\n# x += np.random.random(x.shape) * 0.001\n# y += np.random.random(y.shape) * 0.001\n\n\n# In[6]:\n\nfrom scipy import ndimage\n\ncoords = np.stack((y, x)).T / spacing\nmeshheights = ndimage.map_coordinates(dem, coords.T, order=3, mode=\'nearest\')\n\n#fig = plt.figure(1, figsize=(10*aspect_ratio, 10))\n#ax = fig.add_subplot(111)\n#ax.axis(\'off\')\n#sc = ax.scatter(x[bmask], y[bmask], s=1, c=meshheights[bmask])\n#sc = ax.scatter(x[~bmask], y[~bmask], s=5, c=meshheights[~bmask])\n\n#fig.colorbar(sc, ax=ax, label=\'height\')\n#plt.show()\n\n\n# ## TriMesh\n#\n# Now the points can be triangulated to become a quality unstructured mesh.\n# Triangulation reorders x,y points - be careful!\n\n# In[55]:\n\nfrom quagmire import FlatMesh\nfrom quagmire import TopoMesh # all routines we need are within this class\nfrom quagmire import SurfaceProcessMesh\n\ndm = meshtools.create_DMPlex_from_points(x, y, bmask, refinement_levels=2)\n\nprint(""gLPoints: "", dm.getCoordinates().array.shape[0]/2)\nprint("" LPoints: "", dm.getCoordinatesLocal().array.shape[0]/2)\n\n\n# In[68]:\n\nmesh = SurfaceProcessMesh(dm, verbose=True, neighbour_cloud_size=33)\ncoords = np.stack((mesh.tri.points[:,1], mesh.tri.points[:,0])).T / spacing\nmeshheights = ndimage.map_coordinates(dem, coords.T, order=3, mode=\'nearest\')\n\n\n# In[70]:\n\nmesh.update_height(meshheights)\nmeshheights = mesh.handle_low_points(its=500, smoothing_steps=3)\nlow_points = mesh.identify_low_points()\nflats = np.where(mesh.identify_flat_spots())[0]\n\n\n# In[71]:\n\nflowpaths = mesh.cumulative_flow(np.ones_like(mesh.height))\nflow2 = mesh.rbf_smoother(flowpaths)\n\n\n# In[75]:\n\nmanifold = np.reshape(mesh.coords, (-1,2))\nmanifold = np.insert(manifold, 2, values=mesh.height*3.0, axis=1)\nlow_cloud = manifold[low_points]\n\n\n# In[79]:\n\nfrom LavaVu import lavavu\n\nlv = lavavu.Viewer(border=False, background=""#FFFFFF"", resolution=[1000,600], near=-10.0)\n\ntopo  = lv.triangles(""topography"",  wireframe=False)\ntopo.vertices(manifold)\ntopo.indices(mesh.tri.simplices)\n\nflowverlay = lv.triangles(""flow_surface"", wireframe=False)\nflowverlay.vertices(manifold + (0.0,0.0, 5.0))\nflowverlay.indices(mesh.tri.simplices)\n\nflowverlay2 = lv.triangles(""flow_surface_smooth"", wireframe=False)\nflowverlay2.vertices(manifold + (0.0,0.0, 5.0))\nflowverlay2.indices(mesh.tri.simplices)\n\n\n\n# Add properties to manifolds\n\ntopo.values(mesh.height, \'topography\')\nflowverlay.values(np.sqrt(flowpaths), \'flowpaths\')\nflowverlay2.values(np.sqrt(flow2), \'flowpaths\')\n\n\ncb = topo.colourbar(""topocolourbar"", visible=False) # Add a colour bar\ncb2 = flowverlay.colourbar(""flowcolourbar"", visible=False) # Add a colour bar\ncb3 = flowverlay2.colourbar(""flowcolourbar2"", visible=False) # Add a colour bar\n\ncm = topo.colourmap([""#004420"", ""#FFFFFF"", ""#444444""] , logscale=False, range=[-200.0, 400.0])   # Apply a built in colourmap\ncm2 = flowverlay.colourmap([""#FFFFFF:0.0"", ""#0033FF:0.3"", ""#000033""], logscale=True)   # Apply a built in colourmap\ncm3 = flowverlay2.colourmap([""#FFFFFF:0.0"", ""#0033FF:0.3"", ""#000033""], logscale=True)   # Apply a built in colourmap\n\n\n\n#Filter by min height value\ntopo[""zmin""] = -10.0\n\nlows = lv.points(pointsize=5.0, pointtype=""shiny"", opacity=0.75)\nlows.vertices(low_cloud+(0.0,0.0,20.0))\nlows.values(mesh.height[low_points])\nlows.colourmap(lavavu.cubeHelix()) #, range=[0,0.1])\nlows.colourbar(visible=True)\n\n\n# In[80]:\n\n## Viewer\n\nlv.window()\n\n# topo.control.Checkbox(\'wireframe\',  label=""Topography wireframe"")\n# flowverlay.control.Checkbox(\'wireframe\', label=""Flow wireframe"")\n\n# tris.control.Range(property=\'zmin\', range=(-1,1), step=0.001)\n# lv.control.Range(command=\'background\', range=(0,1), step=0.1, value=1)\n# lv.control.Range(property=\'near\', range=[-10,10], step=2.0)\n\nlv.control.Checkbox(property=\'axis\')\nlv.control.Command()\nlv.control.ObjectList()\nlv.control.show()\n\n\n# Landscape analysis statistics\n\n# In[20]:\n\ngradient_max = mesh.slope.max()\ngradient_mean = mesh.slope.mean()\nflat_spots = np.where(mesh.slope < gradient_mean*0.001)[0]\nlow_points = mesh.identify_low_points()\n\n# print statistics\nprint((""mean gradient {}\\nnumber of flat spots {}\\nnumber of low points {}"".format(gradient_mean,\n                                                                                  flat_spots.size,\n                                                                                  low_points.shape[0])))\n\n\n# In[21]:\n\nfilename = \'port_macquarie_mesh_lows500x3.h5\'\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height=mesh.height, slope=mesh.slope, flow=np.sqrt(flow2))\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n'"
Documentation/Scripts/Unsupported/AusDEM.py,27,"b'\n# coding: utf-8\n\n# # Meshing Australia\n#\n# In this notebook we:\n#\n# 1. Import a coastline from an ESRI shapefile\n# 2. Sample its interior using the poisson disc generator\n# 3. Resample the interior using a DEM\n# 4. Create a DM object and refine a few times\n# 5. Save the mesh to HDF5 file\n\n# In[1]:\n\nfrom osgeo import gdal\n\nimport numpy as np\n# import matplotlib.pyplot as plt\n# from matplotlib import cm\n#get_ipython().magic(\'matplotlib inline\')\n\nimport quagmire\nfrom quagmire import tools as meshtools\n\nimport shapefile\nfrom shapely.geometry import Point\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\n\nfrom scipy.ndimage import imread\nfrom scipy.ndimage.filters import gaussian_filter\n#from matplotlib.colors import LightSource\nfrom petsc4py import PETSc\n\n\n# ## 1. Import coastline shapefile\n#\n# This requires pyshp to be installed. We scale the points to match the dimensions of the DEM we\'ll use later.\n\n# In[2]:\n\n# def remove_duplicates(a):\n#     """"""\n#     find unique rows in numpy array\n#     <http://stackoverflow.com/questions/16970982/find-unique-rows-in-numpy-array>\n#     """"""\n#     b = np.ascontiguousarray(a).view(np.dtype((np.void, a.dtype.itemsize * a.shape[1])))\n#     dedup = np.unique(b).view(a.dtype).reshape(-1, a.shape[1])\n#     return dedup\n#\n# coast_shape = shapefile.Reader(""../Notebooks/data/AustCoast/AustCoast2.shp"")\n# shapeRecs = coast_shape.shapeRecords()\n# coords = []\n# for record in shapeRecs:\n#     coords.append(record.shape.points[:])\n#\n# coords = np.vstack(coords)\n#\n# # Remove duplicates\n# points = remove_duplicates(coords)\n\n\n# In[3]:\n\nne_land = shapefile.Reader(""../Notebooks/data/ne_110m_land/ne_110m_land.shp"")\nland_shapes = ne_land.shapeRecords()\n\npolyList = []\nfor i,s in  enumerate(ne_land.shapes()):\n    if len(s.points) < 3:\n        print(""Dodgy Polygon "", i, s)\n    else:\n        p = Polygon(s.points)\n        if p.is_valid:\n            polyList.append(p)\n\npAll_ne110 = MultiPolygon(polyList)\ntas_poly_ne110 = 11\nausmain_poly_ne110 = 21\n\nAusLandPolygon_ne110 = MultiPolygon([polyList[tas_poly_ne110], polyList[ausmain_poly_ne110]])\n\n\n# In[4]:\n\nne_land = shapefile.Reader(""../Notebooks/data/ne_50m_land/ne_50m_land.shp"")\nland_shapes = ne_land.shapeRecords()\n\npolyList = []\nfor i,s in  enumerate(ne_land.shapes()):\n    if len(s.points) < 3:\n        print(""Dodgy Polygon "", i, s)\n    else:\n        p = Polygon(s.points)\n        if p.is_valid:\n            polyList.append(p)\n\npAll_ne50 = MultiPolygon(polyList)\ntas_poly_ne50 = 89\nausmain_poly_ne50 = 6\n\nAusLandPolygon_ne50 = MultiPolygon([polyList[tas_poly_ne50], polyList[ausmain_poly_ne50]])\n\n\n# In[5]:\n\n# AusLandPolygon_ne50\n\n\n# In[6]:\n\nfrom shapely.geometry import Point\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\n\nausBounds = [110, -45 , 155, -5]\nminX, minY, maxX, maxY = ausBounds\n\n\n## All of this should be done on Rank 0 (the DM is built only on rank 0 )\n\nif PETSc.COMM_WORLD.rank == 0 or PETSc.COMM_WORLD.size == 1:\n\n    print(""Build grid points"")\n\n#    x1, y1, bmask = meshtools.poisson_disc_sampler(minX, maxX, minY, maxY, 0.25)\n\n    xx = np.linspace(minX, maxX, 500)\n    yy = np.linspace(minY, maxY, 250)\n    x1, y1 = np.meshgrid(xx,yy)\n    x1 += np.random.random(x1.shape) * 0.025 * (maxX-minX) / 500.0\n    y1 += np.random.random(y1.shape) * 0.025 * (maxY-minY) / 250.0\n\n    x1 = x1.flatten()\n    y1 = y1.flatten()\n\n    pts = np.stack((x1, y1)).T\n    mpt = MultiPoint(points=pts)\n\n    print(""Find Coastline / Interior"")\n\n    interior_mpts = mpt.intersection(AusLandPolygon_ne50)\n    interior_points = np.array(interior_mpts)\n\n    fatAus = AusLandPolygon_ne50.buffer(0.5) # A puffed up zone around the interior points\n    ausBoundary = fatAus.difference(AusLandPolygon_ne50)\n    inBuffer = mpt.intersection(ausBoundary)\n\n    buffer_points = np.array(inBuffer)\n\n    ## Make a new collection of points to stuff into a DM\n\n    ibmask = np.ones((interior_points.shape[0]), dtype=np.bool)\n    bbmask = np.zeros((buffer_points.shape[0]), dtype=np.bool)\n\n    bmask = np.hstack((ibmask, bbmask))\n    pts = np.vstack((interior_points, buffer_points))\n\n    x1 = pts[:,0]\n    y1 = pts[:,1]\n\n    # ### 3. Create the DM\n    #\n    # The points are now read into a DM and refined so that we can achieve very high resolutions. Refinement is achieved by adding midpoints along line segments connecting each point.\n\nprint(""Create DM"")\n\n\nif not (PETSc.COMM_WORLD.rank == 0 or PETSc.COMM_WORLD.size == 1):\n    x1 = None\n    y1 = None\n    bmask = None\n\nDM = meshtools.create_DMPlex_from_points(x1, y1, bmask, refinement_levels=2)\n\ndel x1, y1, bmask\n\nprint(""Built and distributed DM"")\n\nmesh = quagmire.SurfaceProcessMesh(DM, verbose=True)\nprint(mesh.dm.comm.rank, "": Points: "", mesh.npoints)\n\n# In[71]:\n\nx2r = mesh.tri.x\ny2r = mesh.tri.y\nsimplices = mesh.tri.simplices\nbmaskr = mesh.bmask\ncoords = np.stack((y2r, x2r)).T\n\nprint(""Map DEM to points"")\n\ngtiff = gdal.Open(""../Notebooks/data/ausbath_09_v4.tiff"")\nwidth = gtiff.RasterXSize\nheight = gtiff.RasterYSize\ngt = gtiff.GetGeoTransform()\nminX = gt[0]\nminY = gt[3] + width*gt[4] + height*gt[5]\nmaxX = gt[0] + width*gt[1] + height*gt[2]\nmaxY = gt[3]\n\nimg = gtiff.GetRasterBand(1).ReadAsArray()\n\nim_coords = coords.copy()\nim_coords[:,0] -= minY\nim_coords[:,1] -= minX\n\nim_coords[:,0] *= img.shape[0] / (maxY-minY)\nim_coords[:,1] *= img.shape[1] / (maxX-minX)\n\nim_coords[:,0] =  img.shape[0] - im_coords[:,0]\n\n\n# In[18]:\n\n# print coords.shape\n# print bmaskr.shape\n# print bmaskr.nonzero()[0].shape\n\n\n# In[117]:\n\nfrom scipy import ndimage\n\nspacing = 1.0\ncoords = np.stack((y2r, x2r)).T / spacing\n\n## Heights from DEM and add geoid.\n\nmeshheights = ndimage.map_coordinates(img, im_coords.T, order=3, mode=\'nearest\')\nmeshheights = np.maximum(-100.0, meshheights)\nmeshheights = mesh.rbf_smoother(meshheights, iterations=10)\nmeshheights += 40.0*(mesh.coords[:,0]-minX)/(maxX-minX) + 40.0*(mesh.coords[:,1]-minY)/(maxY-minY)\n\nquestionable = np.logical_and(bmaskr, meshheights < 10.0)\nqindex = np.where(questionable)[0]\n\nfor index in qindex:\n    point = Point(mesh.coords[index])\n    if not AusLandPolygon_ne50.contains(point):\n         bmaskr[index] =  False\n\nmeshheightsS1 = mesh.rbf_smoother(meshheights, iterations=20)\nmeshheightsS2 = mesh.rbf_smoother(meshheightsS1, iterations=20)\n\n\nprint(""Downhill Flow"")\n\n# m v km !\n\nmesh.downhill_neighbours=2\nmesh.update_height(meshheights*0.001)\n#mesh.handle_low_points(its=25)\n\nprint(""Flowpaths 1"")\nnits, flowpaths = mesh.cumulative_flow_verbose(mesh.area*np.ones_like(mesh.height), verbose=True)\nflowpaths = mesh.rbf_smoother(flowpaths, iterations=1)\nflowpaths[~bmaskr] = flowpaths.max()\n\nmesh.update_height(meshheightsS1*0.001)\n#mesh.handle_low_points(its=25)\n\nprint(""Flowpaths 2"")\nnits, flowpathsS1 = mesh.cumulative_flow_verbose(mesh.area*np.ones_like(mesh.height), verbose=True)\nflowpathsS1 = mesh.rbf_smoother(flowpathsS1, iterations=1)\nflowpathsS1[~bmaskr] = flowpathsS1.max()\n\nmesh.update_height(meshheightsS2*0.001)\n#mesh.handle_low_points(its=25)\n\nprint(""Flowpaths 3"")\nnits, flowpathsS2 = mesh.cumulative_flow_verbose(mesh.area*np.ones_like(mesh.height), verbose=True)\nflowpathsS2 = mesh.rbf_smoother(flowpathsS2, iterations=1)\nflowpathsS2[~bmaskr] = flowpathsS2.max()\n\nprint(""Downhill Flow - complete"")\n\n\nfilename = \'austopo.h5\'\n\ndecomp = np.ones_like(mesh.height) * mesh.dm.comm.rank\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height=meshheights*0.001,\n                                  slope=mesh.slope,\n                                  flow=np.sqrt(flowpaths),\n                                  flowS1=np.sqrt(flowpathsS1),\n                                  flowS2=np.sqrt(flowpathsS2),\n                                  decomp=decomp)\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n\n\n# In[ ]:\n'"
Documentation/Scripts/Unsupported/AusDEM2.py,26,"b'\n# coding: utf-8\n\n# # Meshing Australia\n#\n# In this notebook we:\n#\n# 1. Import a coastline from an ESRI shapefile\n# 2. Sample its interior using the poisson disc generator\n# 3. Resample the interior using a DEM\n# 4. Create a DM object and refine a few times\n# 5. Save the mesh to HDF5 file\n\n# In[1]:\n\nfrom osgeo import gdal\n\nimport numpy as np\n# import matplotlib.pyplot as plt\n# from matplotlib import cm\n#get_ipython().magic(\'matplotlib inline\')\n\nimport quagmire\nfrom quagmire import tools as meshtools\n\nimport shapefile\nfrom shapely.geometry import Point\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\n\nfrom scipy.ndimage import imread\nfrom scipy.ndimage.filters import gaussian_filter\n#from matplotlib.colors import LightSource\nfrom petsc4py import PETSc\n\n\n# ## 1. Import coastline shapefile\n#\n# This requires pyshp to be installed. We scale the points to match the dimensions of the DEM we\'ll use later.\n\n# In[2]:\n\n# def remove_duplicates(a):\n#     """"""\n#     find unique rows in numpy array\n#     <http://stackoverflow.com/questions/16970982/find-unique-rows-in-numpy-array>\n#     """"""\n#     b = np.ascontiguousarray(a).view(np.dtype((np.void, a.dtype.itemsize * a.shape[1])))\n#     dedup = np.unique(b).view(a.dtype).reshape(-1, a.shape[1])\n#     return dedup\n#\n# coast_shape = shapefile.Reader(""../Notebooks/data/AustCoast/AustCoast2.shp"")\n# shapeRecs = coast_shape.shapeRecords()\n# coords = []\n# for record in shapeRecs:\n#     coords.append(record.shape.points[:])\n#\n# coords = np.vstack(coords)\n#\n# # Remove duplicates\n# points = remove_duplicates(coords)\n\n\n# In[3]:\n\nne_land = shapefile.Reader(""../Notebooks/data/ne_110m_land/ne_110m_land.shp"")\nland_shapes = ne_land.shapeRecords()\n\npolyList = []\nfor i,s in  enumerate(ne_land.shapes()):\n    if len(s.points) < 3:\n        print(""Dodgy Polygon "", i, s)\n    else:\n        p = Polygon(s.points)\n        if p.is_valid:\n            polyList.append(p)\n\npAll_ne110 = MultiPolygon(polyList)\ntas_poly_ne110 = 11\nausmain_poly_ne110 = 21\n\nAusLandPolygon_ne110 = MultiPolygon([polyList[tas_poly_ne110], polyList[ausmain_poly_ne110]])\n\n\n# In[4]:\n\nne_land = shapefile.Reader(""../Notebooks/data/ne_50m_land/ne_50m_land.shp"")\nland_shapes = ne_land.shapeRecords()\n\npolyList = []\nfor i,s in  enumerate(ne_land.shapes()):\n    if len(s.points) < 3:\n        print(""Dodgy Polygon "", i, s)\n    else:\n        p = Polygon(s.points)\n        if p.is_valid:\n            polyList.append(p)\n\npAll_ne50 = MultiPolygon(polyList)\ntas_poly_ne50 = 89\nausmain_poly_ne50 = 6\n\nAusLandPolygon_ne50 = MultiPolygon([polyList[tas_poly_ne50], polyList[ausmain_poly_ne50]])\n\n\n# In[5]:\n\n# AusLandPolygon_ne50\n\n\n# In[6]:\n\nfrom shapely.geometry import Point\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\n\nausBounds = [110, -45 , 155, -5]\nminX, minY, maxX, maxY = ausBounds\n\n\n## All of this should be done on Rank 0 (the DM is built only on rank 0 )\n\nif PETSc.COMM_WORLD.rank == 0 or PETSc.COMM_WORLD.size == 1:\n\n    print(""Build grid points"")\n\n#    x1, y1, bmask = meshtools.poisson_disc_sampler(minX, maxX, minY, maxY, 0.25)\n\n    xx = np.linspace(minX, maxX, 500)\n    yy = np.linspace(minY, maxY, 250)\n    x1, y1 = np.meshgrid(xx,yy)\n    x1 += np.random.random(x1.shape) * 0.05 * (maxX-minX) / 500.0\n    y1 += np.random.random(y1.shape) * 0.05 * (maxY-minY) / 250.0\n\n    x1 = x1.flatten()\n    y1 = y1.flatten()\n\n    pts = np.stack((x1, y1)).T\n    mpt = MultiPoint(points=pts)\n\n    print(""Find Coastline / Interior"")\n\n    interior_mpts = mpt.intersection(AusLandPolygon_ne50)\n    interior_points = np.array(interior_mpts)\n\n    fatAus = AusLandPolygon_ne50.buffer(0.5) # A puffed up zone around the interior points\n    ausBoundary = fatAus.difference(AusLandPolygon_ne50)\n    inBuffer = mpt.intersection(ausBoundary)\n\n    buffer_points = np.array(inBuffer)\n\n    ## Make a new collection of points to stuff into a DM\n\n    ibmask = np.ones((interior_points.shape[0]), dtype=np.bool)\n    bbmask = np.zeros((buffer_points.shape[0]), dtype=np.bool)\n\n    bmask = np.hstack((ibmask, bbmask))\n    pts = np.vstack((interior_points, buffer_points))\n\n    x1 = pts[:,0]\n    y1 = pts[:,1]\n\n    # ### 3. Create the DM\n    #\n    # The points are now read into a DM and refined so that we can achieve very high resolutions. Refinement is achieved by adding midpoints along line segments connecting each point.\n\nprint(""Create DM"")\n\n\nif not (PETSc.COMM_WORLD.rank == 0 or PETSc.COMM_WORLD.size == 1):\n    x1 = None\n    y1 = None\n    bmask = None\n\nDM = meshtools.create_DMPlex_from_points(x1, y1, bmask, refinement_levels=3)\n\ndel x1, y1, bmask\n\nprint(""Built and distributed DM"")\n\nmesh = quagmire.SurfaceProcessMesh(DM, verbose=True)\nprint(mesh.dm.comm.rank, "": Points: "", mesh.npoints)\n\n# In[71]:\n\nx2r = mesh.tri.x\ny2r = mesh.tri.y\nsimplices = mesh.tri.simplices\nbmaskr = mesh.bmask\ncoords = np.stack((y2r, x2r)).T\n\nprint(""Map DEM to points"")\n\ngtiff = gdal.Open(""../Notebooks/data/ausbath_09_v4.tiff"")\nwidth = gtiff.RasterXSize\nheight = gtiff.RasterYSize\ngt = gtiff.GetGeoTransform()\nminX = gt[0]\nminY = gt[3] + width*gt[4] + height*gt[5]\nmaxX = gt[0] + width*gt[1] + height*gt[2]\nmaxY = gt[3]\n\nimg = gtiff.GetRasterBand(1).ReadAsArray()\n\nim_coords = coords.copy()\nim_coords[:,0] -= minY\nim_coords[:,1] -= minX\n\nim_coords[:,0] *= img.shape[0] / (maxY-minY)\nim_coords[:,1] *= img.shape[1] / (maxX-minX)\n\nim_coords[:,0] =  img.shape[0] - im_coords[:,0]\n\n\nfrom scipy import ndimage\n\nspacing = 1.0\ncoords = np.stack((y2r, x2r)).T / spacing\n\n## Heights from DEM and add geoid.\n\nmeshheights = ndimage.map_coordinates(img, im_coords.T, order=3, mode=\'nearest\')\nmeshheights = np.maximum(-100.0, meshheights)\nmeshheights = mesh.rbf_smoother(meshheights, iterations=10)\nmeshheights += 40.0*(mesh.coords[:,0]-minX)/(maxX-minX) + 40.0*(mesh.coords[:,1]-minY)/(maxY-minY)\n\nquestionable = np.logical_and(bmaskr, meshheights < 10.0)\nqindex = np.where(questionable)[0]\n\nfor index in qindex:\n    point = Point(mesh.coords[index])\n    if not AusLandPolygon_ne50.contains(point):\n         bmaskr[index] =  False\n\n\nprint(""Downhill Flow"")\n\n# m v km !\n\nmesh.downhill_neighbours=2\nmesh.update_height(meshheights*0.001)\n\n# print ""Flowpaths ""\n# nits, flowpaths = mesh.cumulative_flow_verbose(np.ones_like(mesh.height), verbose=True, maximum_its=1500)\n# flowpaths = mesh.rbf_smoother(flowpaths, iterations=1)\n# flowpaths[~bmaskr] = 0.0\n\nmesh.handle_low_points(its=250)\n\nprint(""Flowpaths - Low point"")\nnits, flowpaths = mesh.cumulative_flow_verbose(mesh.area*np.ones_like(mesh.height), verbose=True, maximum_its=1500)\nflowpaths = mesh.rbf_smoother(flowpaths, iterations=1)\nflowpaths[~bmaskr] = 0.0\n\nprint(""Smooth topography with RBF (500)"")\nsuper_smooth_topo = mesh.rbf_smoother(mesh.height, iterations=500)\nmesh.update_height(super_smooth_topo)\n\nprint(""Flowpaths - Smooth"")\nnits, flowpathsSmooth = mesh.cumulative_flow_verbose(mesh.area*np.ones_like(mesh.height), verbose=True, maximum_its=1500)\nflowpathsSmooth = mesh.rbf_smoother(flowpathsSmooth, iterations=1)\nflowpathsSmooth[~bmaskr] = 0.0\n\n\nprint(""Downhill Flow - complete"")\n\nfilename = \'austopo-v-smooth500.h5\'\n\ndecomp = np.ones_like(mesh.height) * mesh.dm.comm.rank\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height=mesh.heights,\n                                  slope=mesh.slope,\n                                  flowLP=np.sqrt(flowpaths),\n                                  flowSmooth=np.sqrt(flowpathsSmooth),\n                                  decomp=decomp)\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n\n\n# In[ ]:\n'"
Documentation/Scripts/Unsupported/AusETOPO.py,29,"b'\n# coding: utf-8\n\n# # Meshing Australia\n#\n# In this notebook we:\n#\n# 1. Import a coastline from an ESRI shapefile\n# 2. Sample its interior using the poisson disc generator\n# 3. Resample the interior using a DEM\n# 4. Create a DM object and refine a few times\n# 5. Save the mesh to HDF5 file\n\n# In[1]:\n\nfrom osgeo import gdal\n\nimport numpy as np\n# import matplotlib.pyplot as plt\n# from matplotlib import cm\n#get_ipython().magic(\'matplotlib inline\')\n\nimport quagmire\nfrom quagmire import tools as meshtools\n\nimport shapefile\nfrom shapely.geometry import Point\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\n\nfrom scipy.ndimage import imread\nfrom scipy.ndimage.filters import gaussian_filter\n#from matplotlib.colors import LightSource\nfrom petsc4py import PETSc\n\n\n# ## 1. Import coastline shapefile\n#\n# This requires pyshp to be installed. We scale the points to match the dimensions of the DEM we\'ll use later.\n\n# In[2]:\n\n# def remove_duplicates(a):\n#     """"""\n#     find unique rows in numpy array\n#     <http://stackoverflow.com/questions/16970982/find-unique-rows-in-numpy-array>\n#     """"""\n#     b = np.ascontiguousarray(a).view(np.dtype((np.void, a.dtype.itemsize * a.shape[1])))\n#     dedup = np.unique(b).view(a.dtype).reshape(-1, a.shape[1])\n#     return dedup\n#\n# coast_shape = shapefile.Reader(""../Notebooks/data/AustCoast/AustCoast2.shp"")\n# shapeRecs = coast_shape.shapeRecords()\n# coords = []\n# for record in shapeRecs:\n#     coords.append(record.shape.points[:])\n#\n# coords = np.vstack(coords)\n#\n# # Remove duplicates\n# points = remove_duplicates(coords)\n\n\n# In[3]:\n\nne_land = shapefile.Reader(""../Notebooks/data/ne_110m_land/ne_110m_land.shp"")\nland_shapes = ne_land.shapeRecords()\n\npolyList = []\nfor i,s in  enumerate(ne_land.shapes()):\n    if len(s.points) < 3:\n        print(""Dodgy Polygon "", i, s)\n    else:\n        p = Polygon(s.points)\n        if p.is_valid:\n            polyList.append(p)\n\npAll_ne110 = MultiPolygon(polyList)\ntas_poly_ne110 = 11\nausmain_poly_ne110 = 21\n\nAusLandPolygon_ne110 = MultiPolygon([polyList[tas_poly_ne110], polyList[ausmain_poly_ne110]])\n\n\n# In[4]:\n\nne_land = shapefile.Reader(""../Notebooks/data/ne_50m_land/ne_50m_land.shp"")\nland_shapes = ne_land.shapeRecords()\n\npolyList = []\nfor i,s in  enumerate(ne_land.shapes()):\n    if len(s.points) < 3:\n        print(""Dodgy Polygon "", i, s)\n    else:\n        p = Polygon(s.points)\n        if p.is_valid:\n            polyList.append(p)\n\npAll_ne50 = MultiPolygon(polyList)\ntas_poly_ne50 = 89\nausmain_poly_ne50 = 6\n\nAusLandPolygon_ne50 = MultiPolygon([polyList[tas_poly_ne50], polyList[ausmain_poly_ne50]])\n\n\n# In[5]:\n\n# AusLandPolygon_ne50\n\n\n# In[6]:\n\nfrom shapely.geometry import Point\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\n\nausBounds = [110, -45 , 155, -5]\nminX, minY, maxX, maxY = ausBounds\n\n\n## All of this should be done on Rank 0 (the DM is built only on rank 0 )\n\nif PETSc.COMM_WORLD.rank == 0 or PETSc.COMM_WORLD.size == 1:\n\n    print(""Build grid points"")\n\n#    x1, y1, bmask = meshtools.poisson_disc_sampler(minX, maxX, minY, maxY, 0.25)\n\n    xx = np.linspace(minX, maxX, 500)\n    yy = np.linspace(minY, maxY, 250)\n    x1, y1 = np.meshgrid(xx,yy)\n    x1 += np.random.random(x1.shape) * 0.05 * (maxX-minX) / 500.0\n    y1 += np.random.random(y1.shape) * 0.05 * (maxY-minY) / 250.0\n\n    x1 = x1.flatten()\n    y1 = y1.flatten()\n\n    pts = np.stack((x1, y1)).T\n    mpt = MultiPoint(points=pts)\n\n    print(""Find Coastline / Interior"")\n\n    interior_mpts = mpt.intersection(AusLandPolygon_ne50)\n    interior_points = np.array(interior_mpts)\n\n    fatAus = AusLandPolygon_ne50.buffer(0.5) # A puffed up zone around the interior points\n    ausBoundary = fatAus.difference(AusLandPolygon_ne50)\n    inBuffer = mpt.intersection(ausBoundary)\n\n    buffer_points = np.array(inBuffer)\n\n    ## Make a new collection of points to stuff into a DM\n\n    ibmask = np.ones((interior_points.shape[0]), dtype=np.bool)\n    bbmask = np.zeros((buffer_points.shape[0]), dtype=np.bool)\n\n    bmask = np.hstack((ibmask, bbmask))\n    pts = np.vstack((interior_points, buffer_points))\n\n    x1 = pts[:,0]\n    y1 = pts[:,1]\n\n    # ### 3. Create the DM\n    #\n    # The points are now read into a DM and refined so that we can achieve very high resolutions. Refinement is achieved by adding midpoints along line segments connecting each point.\n\nprint(""Create DM"")\n\n\nif not (PETSc.COMM_WORLD.rank == 0 or PETSc.COMM_WORLD.size == 1):\n    x1 = None\n    y1 = None\n    bmask = None\n\nDM = meshtools.create_DMPlex_from_points(x1, y1, bmask, refinement_levels=3)\n\ndel x1, y1, bmask\n\nprint(""Built and distributed DM"")\n\nmesh = quagmire.SurfaceProcessMesh(DM, verbose=True)\nprint(mesh.dm.comm.rank, "": Points: "", mesh.npoints)\n\n# In[71]:\n\nx2r = mesh.tri.x\ny2r = mesh.tri.y\nsimplices = mesh.tri.simplices\nbmaskr = mesh.bmask\ncoords = np.stack((y2r, x2r)).T\n\nprint(""Map DEM to points"")\n\ngtiff = gdal.Open(""../Notebooks/data/ETOPO1_Ice_c_geotiff.tiff"")\n\nwidth = gtiff.RasterXSize\nheight = gtiff.RasterYSize\ngt = gtiff.GetGeoTransform()\nminX = gt[0]\nminY = gt[3] + width*gt[4] + height*gt[5]\nmaxX = gt[0] + width*gt[1] + height*gt[2]\nmaxY = gt[3]\n\nimg = gtiff.GetRasterBand(1).ReadAsArray().T\n# img = np.flipud(img).astype(float)\n\nimg = np.fliplr(img)\n\nprint(minX, minY, maxX, maxY)\n\nausBounds = [110, -45 , 155, -5]\nminX, minY, maxX, maxY = ausBounds\n\nsliceLeft   = int(180+minX) * 60\nsliceRight  = int(180+maxX) * 60\nsliceBottom = int(90+minY) * 60\nsliceTop    = int(90+maxY) * 60\n\nAusImg = img[ sliceLeft:sliceRight, sliceBottom:sliceTop].T\nAusImg = np.flipud(AusImg)\n\nprint(AusImg.shape)\n\nimg = AusImg\n\n#\n#\n# gtiff = gdal.Open(""../Notebooks/data/ausbath_09_v4.tiff"")\n# width = gtiff.RasterXSize\n# height = gtiff.RasterYSize\n# gt = gtiff.GetGeoTransform()\n# minX = gt[0]\n# minY = gt[3] + width*gt[4] + height*gt[5]\n# maxX = gt[0] + width*gt[1] + height*gt[2]\n# maxY = gt[3]\n#\n# img = gtiff.GetRasterBand(1).ReadAsArray()\n\nim_coords = coords.copy()\nim_coords[:,0] -= minY\nim_coords[:,1] -= minX\n\nim_coords[:,0] *= img.shape[0] / (maxY-minY)\nim_coords[:,1] *= img.shape[1] / (maxX-minX)\n\nim_coords[:,0] =  img.shape[0] - im_coords[:,0]\n\n\nfrom scipy import ndimage\n\nspacing = 1.0\ncoords = np.stack((y2r, x2r)).T / spacing\n\n## Heights from DEM and add geoid.\n\nmeshheights = ndimage.map_coordinates(img, im_coords.T, order=3, mode=\'nearest\')\nmeshheights = np.maximum(-100.0, meshheights)\nmeshheights = mesh.rbf_smoother(meshheights, iterations=10)\nmeshheights += 40.0*(mesh.coords[:,0]-minX)/(maxX-minX) + 40.0*(mesh.coords[:,1]-minY)/(maxY-minY)\n\nquestionable = np.logical_and(bmaskr, meshheights < 10.0)\nqindex = np.where(questionable)[0]\n\nfor index in qindex:\n    point = Point(mesh.coords[index])\n    if not AusLandPolygon_ne50.contains(point):\n         bmaskr[index] =  False\n\n\nprint(""Downhill Flow"")\n\n# m v km !\n\nmesh.downhill_neighbours=2\nmesh.update_height(meshheights*0.001)\n\n# print ""Flowpaths ""\n# nits, flowpaths = mesh.cumulative_flow_verbose(np.ones_like(mesh.height), verbose=True, maximum_its=1500)\n# flowpaths = mesh.rbf_smoother(flowpaths, iterations=1)\n# flowpaths[~bmaskr] = 0.0\n\nmesh.handle_low_points(its=200)\n\nprint(""Flowpaths - Low point"")\nnits, flowpaths = mesh.cumulative_flow_verbose(np.ones_like(mesh.height), verbose=True, maximum_its=1500)\nflowpaths = mesh.rbf_smoother(flowpaths, iterations=1)\nflowpaths[~bmaskr] = 0.0\n\n\n# super_smooth_topo = mesh.rbf_smoother(mesh.height, iterations=100)\n# mesh.update_height(super_smooth_topo)\n#\n# print ""Flowpaths - Smooth""\n# nits, flowpathsSmooth = mesh.cumulative_flow_verbose(np.ones_like(mesh.height), verbose=True, maximum_its=1500)\n# flowpathsSmooth = mesh.rbf_smoother(flowpathsSmooth, iterations=1)\n# flowpathsSmooth[~bmaskr] = 0.0\n\n\nprint(""Downhill Flow - complete"")\n\n\nfilename = \'ausEtopo-ETOPO.h5\'\n\ndecomp = np.ones_like(mesh.height) * mesh.dm.comm.rank\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height=meshheights,\n                                  slope=mesh.slope,\n                                  flowLP=np.sqrt(flowpaths),\n                                  # flowSmooth=np.sqrt(flowpathsSmooth),\n                                  decomp=decomp)\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n\n\n# In[ ]:\n'"
Documentation/Scripts/Unsupported/NZETOPO1.py,28,"b'\n# coding: utf-8\n\n# # Meshing Australia\n#\n# In this notebook we:\n#\n# 1. Import a coastline from an ESRI shapefile\n# 2. Sample its interior using the poisson disc generator\n# 3. Resample the interior using a DEM\n# 4. Create a DM object and refine a few times\n# 5. Save the mesh to HDF5 file \n\n# In[1]:\n\nfrom osgeo import gdal\n\nimport numpy as np\n# import matplotlib.pyplot as plt\n# from matplotlib import cm\n#get_ipython().magic(\'matplotlib inline\')\n\nimport quagmire\nfrom quagmire import tools as meshtools\n\nimport shapefile\nfrom shapely.geometry import Point\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\n\nfrom scipy.ndimage import imread\nfrom scipy.ndimage.filters import gaussian_filter\n#from matplotlib.colors import LightSource\nfrom petsc4py import PETSc\n\n\n\n\n# ne_land = shapefile.Reader(""../Notebooks/data/ne_110m_land/ne_110m_land.shp"")\n# land_shapes = ne_land.shapeRecords()\n#\n# polyList = []\n# for i,s in  enumerate(ne_land.shapes()):\n#     if len(s.points) < 3:\n#         print ""Dodgy Polygon "", i, s\n#     else:\n#         p = Polygon(s.points)\n#         if p.is_valid:\n#             polyList.append(p)\n#\n# pAll_ne110 = MultiPolygon(polyList)\n# tas_poly_ne110 = 11\n# ausmain_poly_ne110 = 21\n#\n# AusLandPolygon_ne110 = MultiPolygon([polyList[tas_poly_ne110], polyList[ausmain_poly_ne110]])\n\n\n# In[4]:\n\nne_land = shapefile.Reader(""../Notebooks/data/ne_50m_land/ne_50m_land.shp"")\nland_shapes = ne_land.shapeRecords()\n\npolyList = []\nfor i,s in  enumerate(ne_land.shapes()):\n    if len(s.points) < 3:\n        print(""Dodgy Polygon "", i, s)\n    else:\n        p = Polygon(s.points)\n        if p.is_valid:\n            polyList.append(p)\n\nNZNorthI_poly_ne50 = 96\nNZSouthI_poly_ne50 = 97\nNZLandPolygon_ne50 = MultiPolygon([polyList[NZNorthI_poly_ne50], polyList[NZSouthI_poly_ne50]])\n\nLandAreaPolygon_ne50 = NZLandPolygon_ne50\n\n# In[5]:\n\n# AusLandPolygon_ne50\n\n\n# In[6]:\n\nfrom shapely.geometry import Point\nfrom shapely.geometry import MultiPoint\nfrom shapely.geometry import Polygon\nfrom shapely.geometry import MultiPolygon\n\nbounds = LandAreaPolygon_ne50.bounds\nminX, minY, maxX, maxY = bounds\n\n\n## All of this should be done on Rank 0 (the DM is built only on rank 0 )\n\nif PETSc.COMM_WORLD.rank == 0 or PETSc.COMM_WORLD.size == 1:\n\n    print(""Build grid points"")\n\n#    x1, y1, bmask = meshtools.poisson_disc_sampler(minX, maxX, minY, maxY, 0.25)\n\n    xres = 500\n    yres = 500\n\n    xx = np.linspace(minX, maxX, xres)\n    yy = np.linspace(minY, maxY, yres)\n    x1, y1 = np.meshgrid(xx,yy)\n    x1 += np.random.random(x1.shape) * 0.2 * (maxX-minX) / xres\n    y1 += np.random.random(y1.shape) * 0.2 * (maxY-minY) / yres\n\n    x1 = x1.flatten()\n    y1 = y1.flatten()\n\n    pts = np.stack((x1, y1)).T\n    mpt = MultiPoint(points=pts)\n\n    print(""Find Coastline / Interior"")\n\n    interior_mpts = mpt.intersection(NZLandPolygon_ne50)\n    interior_points = np.array(interior_mpts)\n\n    fatBoundary = LandAreaPolygon_ne50.buffer(0.5) # A puffed up zone around the interior points\n    boundary = fatBoundary.difference(LandAreaPolygon_ne50)\n    inBuffer = mpt.intersection(boundary)\n\n    buffer_points = np.array(inBuffer)\n\n    ## Make a new collection of points to stuff into a DM\n\n    ibmask = np.ones((interior_points.shape[0]), dtype=np.bool)\n    bbmask = np.zeros((buffer_points.shape[0]), dtype=np.bool)\n\n    bmask = np.hstack((ibmask, bbmask))\n    pts = np.vstack((interior_points, buffer_points))\n\n    x1 = pts[:,0]\n    y1 = pts[:,1]\n\n    # ### 3. Create the DM\n    #\n    # The points are now read into a DM and refined so that we can achieve very high resolutions. Refinement is achieved by adding midpoints along line segments connecting each point.\n\nprint(""Create DM"")\n\n\nif not (PETSc.COMM_WORLD.rank == 0 or PETSc.COMM_WORLD.size == 1):\n    x1 = None\n    y1 = None\n    bmask = None\n\nDM = meshtools.create_DMPlex_from_points(x1, y1, bmask, refinement_levels=1)\n\ndel x1, y1, bmask\n\nprint(""Built and distributed DM"")\n\nmesh = quagmire.SurfaceProcessMesh(DM, verbose=True)\nprint(mesh.dm.comm.rank, "": Points: "", mesh.npoints)\n\n# In[71]:\n\nx2r = mesh.tri.x\ny2r = mesh.tri.y\nsimplices = mesh.tri.simplices\nbmaskr = mesh.bmask\ncoords = np.stack((y2r, x2r)).T\n\nprint(""Map DEM to points"")\n\ngtiff = gdal.Open(""../Notebooks/data/ETOPO1_Ice_c_geotiff.tif"")\n\nwidth = gtiff.RasterXSize\nheight = gtiff.RasterYSize\ngt = gtiff.GetGeoTransform()\n# minX = gt[0]\n# minY = gt[3] + width*gt[4] + height*gt[5]\n# maxX = gt[0] + width*gt[1] + height*gt[2]\n# maxY = gt[3]\n\nimg = gtiff.GetRasterBand(1).ReadAsArray().T\n# img = np.flipud(img).astype(float)\n\nimg = np.fliplr(img)\n\n# print minX, minY, maxX, maxY\n\nbounds = LandAreaPolygon_ne50.bounds\nminX, minY, maxX, maxY = bounds\n\nsliceLeft   = int(180+minX) * 60\nsliceRight  = int(180+maxX) * 60\nsliceBottom = int(90+minY) * 60\nsliceTop    = int(90+maxY) * 60\n\nLandImg = img[ sliceLeft:sliceRight, sliceBottom:sliceTop].T\nLandImg = np.flipud(LandImg)\n\nprint(LandImg.shape)\n\nimg = LandImg\n\n#\n#\n# gtiff = gdal.Open(""../Notebooks/data/ausbath_09_v4.tiff"")\n# width = gtiff.RasterXSize\n# height = gtiff.RasterYSize\n# gt = gtiff.GetGeoTransform()\n# minX = gt[0]\n# minY = gt[3] + width*gt[4] + height*gt[5]\n# maxX = gt[0] + width*gt[1] + height*gt[2]\n# maxY = gt[3]\n#\n# img = gtiff.GetRasterBand(1).ReadAsArray()\n\nim_coords = coords.copy()\nim_coords[:,0] -= minY\nim_coords[:,1] -= minX\n\nim_coords[:,0] *= img.shape[0] / (maxY-minY)\nim_coords[:,1] *= img.shape[1] / (maxX-minX)\n\nim_coords[:,0] =  img.shape[0] - im_coords[:,0]\n\n\nfrom scipy import ndimage\n\nspacing = 1.0\ncoords = np.stack((y2r, x2r)).T / spacing\n\n## Heights from DEM and add geoid.\n\nmeshheights = ndimage.map_coordinates(img, im_coords.T, order=3, mode=\'nearest\')\nmeshheights = np.maximum(-100.0, meshheights)\nmeshheights = mesh.rbf_smoother(meshheights, iterations=2)\n# meshheights += 40.0*(mesh.coords[:,0]-minX)/(maxX-minX) + 40.0*(mesh.coords[:,1]-minY)/(maxY-minY)\n\n# Some bug in the bmask after refinement means we need to check this\n\nquestionable = np.logical_and(bmaskr, meshheights < 10.0)\nqindex = np.where(questionable)[0]\n\nfor index in qindex:\n    point = Point(mesh.coords[index])\n    if not LandAreaPolygon_ne50.contains(point):\n         bmaskr[index] =  False\n\n# and this (the reverse condition)\n\nquestionable = np.logical_and(~bmaskr, meshheights > -1.0)\nqindex = np.where(questionable)[0]\n\nfor index in qindex:\n     point = Point(mesh.coords[index])\n     if  LandAreaPolygon_ne50.contains(point):\n          bmaskr[index] =  True\n\n\n\n\nprint(""Downhill Flow"")\n\n# m v km !\n\nmesh.downhill_neighbours=2\nmesh.update_height(meshheights*0.001)\n\n\nprint(""Flowpaths 1 - Lows included"")\n\nnits, flowpaths = mesh.cumulative_flow_verbose(mesh.area*np.ones_like(mesh.height), verbose=True, maximum_its=2500)\nflowpaths = mesh.rbf_smoother(flowpaths, iterations=1)\nflowpaths[~bmaskr] = -1.0\n\n\n# super_smooth_topo = mesh.rbf_smoother(mesh.height, iterations=100)\n# mesh.update_height(super_smooth_topo)\n#\n# print ""Flowpaths - Smooth""\n# nits, flowpathsSmooth = mesh.cumulative_flow_verbose(np.ones_like(mesh.height), verbose=True, maximum_its=1500)\n# flowpathsSmooth = mesh.rbf_smoother(flowpathsSmooth, iterations=1)\n# flowpathsSmooth[~bmaskr] = 0.0\n\nnew_heights=mesh.low_points_local_fill(its=2, smoothing_steps=2)\nmesh._update_height_partial(new_heights)\nlow_points2 = mesh.identify_low_points()\nprint(""Low Points"", low_points2.shape)\n\n\nfor i in range(0,10):\n    new_heights = mesh.low_points_swamp_fill()\n    mesh._update_height_partial(new_heights)\n    # mesh.update_height(new_heights)\n    low_points2 = mesh.identify_low_points()\n    print(low_points2.shape)\n\nprint(""Low Points"", low_points2.shape)\n\nprint(""Flowpaths 2 - Lows patched"")\n\nraw_heights=mesh.height\nmesh.update_height(new_heights)\n\n\nprint(""Flowpaths 1 - Lows included"")\n\nnits, flowpaths2 = mesh.cumulative_flow_verbose(mesh.area*np.ones_like(mesh.height), verbose=True, maximum_its=2500)\nflowpaths2 = mesh.rbf_smoother(flowpaths2, iterations=1)\nflowpaths2[~bmaskr] = -1.0\n\n\nprint(""Downhill Flow - complete"")\n\n\nfilename = \'NZ-ETOPO.h5\'\n\ndecomp = np.ones_like(mesh.height) * mesh.dm.comm.rank\n\nmesh.save_mesh_to_hdf5(filename)\nmesh.save_field_to_hdf5(filename, height=meshheights*0.001,\n                                  slope=mesh.slope,\n                                  flowLP=np.sqrt(flowpaths),\n                                  flow=np.sqrt(flowpaths1),\n                                  lakes=mesh.height - raw_heights,\n                                  decomp=decomp)\n\n# to view in Paraview\nmeshtools.generate_xdmf(filename)\n\n\n# In[ ]:\n'"
