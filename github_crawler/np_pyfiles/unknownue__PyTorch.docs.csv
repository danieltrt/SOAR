file_path,api_count,code
numpy/reference/routines-polynomials-classes-1.py,1,"b'import matplotlib.pyplot as plt\nfrom numpy.polynomial import Chebyshev as T\nx = np.linspace(-1, 1, 100)\nfor i in range(6): ax = plt.plot(x, T.basis(i)(x), lw=2, label=""$T_%d$""%i)\n# ...\nplt.legend(loc=""upper left"")\n# <matplotlib.legend.Legend object at 0x3b3ee10>\nplt.show()\n'"
numpy/reference/routines-polynomials-classes-2.py,1,"b'import matplotlib.pyplot as plt\nfrom numpy.polynomial import Chebyshev as T\nx = np.linspace(-2, 2, 100)\nfor i in range(6): ax = plt.plot(x, T.basis(i)(x), lw=2, label=""$T_%d$""%i)\n# ...\nplt.legend(loc=""lower right"")\n# <matplotlib.legend.Legend object at 0x3b3ee10>\nplt.show()\n'"
numpy/reference/routines-polynomials-classes-3.py,3,"b""import numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial import Chebyshev as T\nnp.random.seed(11)\nx = np.linspace(0, 2*np.pi, 20)\ny = np.sin(x) + np.random.normal(scale=.1, size=x.shape)\np = T.fit(x, y, 5)\nplt.plot(x, y, 'o')\n# [<matplotlib.lines.Line2D object at 0x2136c10>]\nxx, yy = p.linspace()\nplt.plot(xx, yy, lw=2)\n# [<matplotlib.lines.Line2D object at 0x1cf2890>]\np.domain\n# array([ 0.        ,  6.28318531])\np.window\n# array([-1.,  1.])\nplt.show()\n"""
numpy/user/quickstart-1.py,3,"b'import numpy as np\nimport matplotlib.pyplot as plt\ndef mandelbrot( h,w, maxit=20 ):\n    """"""Returns an image of the Mandelbrot fractal of size (h,w).""""""\n    y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ]\n    c = x+y*1j\n    z = c\n    divtime = maxit + np.zeros(z.shape, dtype=int)\n# ...\n    for i in range(maxit):\n        z = z**2 + c\n        diverge = z*np.conj(z) > 2**2            # who is diverging\n        div_now = diverge & (divtime==maxit)  # who is diverging now\n        divtime[div_now] = i                  # note when\n        z[diverge] = 2                        # avoid diverging too much\n# ...\n    return divtime\nplt.imshow(mandelbrot(400,400))\nplt.show()\n'"
numpy/user/quickstart-2.py,2,"b'import numpy as np\nimport matplotlib.pyplot as plt\n# Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2\nmu, sigma = 2, 0.5\nv = np.random.normal(mu,sigma,10000)\n# Plot a normalized histogram with 50 bins\nplt.hist(v, bins=50, density=1)       # matplotlib version (plot)\nplt.show()\n# Compute the histogram with numpy and then plot it\n(n, bins) = np.histogram(v, bins=50, density=True)  # NumPy version (no plot)\nplt.plot(.5*(bins[1:]+bins[:-1]), n)\nplt.show()\n'"
numpy/reference/generated/numpy-absolute-1.py,7,"b""x = np.array([-1.2, 1.2])\nnp.absolute(x)\n# array([ 1.2,  1.2])\nnp.absolute(1.2 + 1j)\n# 1.5620499351813308\n\n# Plot the function over ``[-10, 10]``:\n\nimport matplotlib.pyplot as plt\n\nx = np.linspace(start=-10, stop=10, num=101)\nplt.plot(x, np.absolute(x))\nplt.show()\n\n# Plot the function over the complex plane:\n\nxx = x + 1j * x[:, np.newaxis]\nplt.imshow(np.abs(xx), extent=[-10, 10, -10, 10], cmap='gray')\nplt.show()\n"""
numpy/reference/generated/numpy-arccos-1.py,3,"b""# We expect the arccos of 1 to be 0, and of -1 to be pi:\n\nnp.arccos([1, -1])\n# array([ 0.        ,  3.14159265])\n\n# Plot arccos:\n\nimport matplotlib.pyplot as plt\nx = np.linspace(-1, 1, num=100)\nplt.plot(x, np.arccos(x))\nplt.axis('tight')\nplt.show()\n"""
numpy/reference/generated/numpy-arctan-1.py,4,"b""# We expect the arctan of 0 to be 0, and of 1 to be pi/4:\n\nnp.arctan([0, 1])\n# array([ 0.        ,  0.78539816])\n\nnp.pi/4\n# 0.78539816339744828\n\n# Plot arctan:\n\nimport matplotlib.pyplot as plt\nx = np.linspace(-10, 10)\nplt.plot(x, np.arctan(x))\nplt.axis('tight')\nplt.show()\n"""
numpy/reference/generated/numpy-bartlett-1.py,7,"b'import matplotlib.pyplot as plt\nnp.bartlett(12)\n# array([ 0.        ,  0.18181818,  0.36363636,  0.54545455,  0.72727273, # may vary\n# 0.90909091,  0.90909091,  0.72727273,  0.54545455,  0.36363636,\n# 0.18181818,  0.        ])\n\n# Plot the window and its frequency response (requires SciPy and matplotlib):\n\nfrom numpy.fft import fft, fftshift\nwindow = np.bartlett(51)\nplt.plot(window)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Bartlett window"")\n# Text(0.5, 1.0, \'Bartlett window\')\nplt.ylabel(""Amplitude"")\n# Text(0, 0.5, \'Amplitude\')\nplt.xlabel(""Sample"")\n# Text(0.5, 0, \'Sample\')\nplt.show()\n\nplt.figure()\n# <Figure size 640x480 with 0 Axes>\nA = fft(window, 2048) / 25.5\nmag = np.abs(fftshift(A))\nfreq = np.linspace(-0.5, 0.5, len(A))\nwith np.errstate(divide=\'ignore\', invalid=\'ignore\'):\n    response = 20 * np.log10(mag)\n# ...\nresponse = np.clip(response, -100, 100)\nplt.plot(freq, response)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Frequency response of Bartlett window"")\n# Text(0.5, 1.0, \'Frequency response of Bartlett window\')\nplt.ylabel(""Magnitude [dB]"")\n# Text(0, 0.5, \'Magnitude [dB]\')\nplt.xlabel(""Normalized frequency [cycles per sample]"")\n# Text(0.5, 0, \'Normalized frequency [cycles per sample]\')\n_ = plt.axis(\'tight\')\nplt.show()\n'"
numpy/reference/generated/numpy-blackman-1.py,7,"b'import matplotlib.pyplot as plt\nnp.blackman(12)\n# array([-1.38777878e-17,   3.26064346e-02,   1.59903635e-01, # may vary\n# 4.14397981e-01,   7.36045180e-01,   9.67046769e-01,\n# 9.67046769e-01,   7.36045180e-01,   4.14397981e-01,\n# 1.59903635e-01,   3.26064346e-02,  -1.38777878e-17])\n\n# Plot the window and the frequency response:\n\nfrom numpy.fft import fft, fftshift\nwindow = np.blackman(51)\nplt.plot(window)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Blackman window"")\n# Text(0.5, 1.0, \'Blackman window\')\nplt.ylabel(""Amplitude"")\n# Text(0, 0.5, \'Amplitude\')\nplt.xlabel(""Sample"")\n# Text(0.5, 0, \'Sample\')\nplt.show()\n\nplt.figure()\n# <Figure size 640x480 with 0 Axes>\nA = fft(window, 2048) / 25.5\nmag = np.abs(fftshift(A))\nfreq = np.linspace(-0.5, 0.5, len(A))\nwith np.errstate(divide=\'ignore\', invalid=\'ignore\'):\n    response = 20 * np.log10(mag)\n# ...\nresponse = np.clip(response, -100, 100)\nplt.plot(freq, response)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Frequency response of Blackman window"")\n# Text(0.5, 1.0, \'Frequency response of Blackman window\')\nplt.ylabel(""Magnitude [dB]"")\n# Text(0, 0.5, \'Magnitude [dB]\')\nplt.xlabel(""Normalized frequency [cycles per sample]"")\n# Text(0.5, 0, \'Normalized frequency [cycles per sample]\')\n_ = plt.axis(\'tight\')\nplt.show()\n'"
numpy/reference/generated/numpy-cosh-1.py,3,"b'np.cosh(0)\n# 1.0\n\n# The hyperbolic cosine describes the shape of a hanging cable:\n\nimport matplotlib.pyplot as plt\nx = np.linspace(-4, 4, 1000)\nplt.plot(x, np.cosh(x))\nplt.show()\n'"
numpy/reference/generated/numpy-exp-1.py,7,"b""# Plot the magnitude and phase of ``exp(x)`` in the complex plane:\n\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-2*np.pi, 2*np.pi, 100)\nxx = x + 1j * x[:, np.newaxis] # a + ib over complex plane\nout = np.exp(xx)\n\nplt.subplot(121)\nplt.imshow(np.abs(out),\n           extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='gray')\nplt.title('Magnitude of exp(x)')\n\nplt.subplot(122)\nplt.imshow(np.angle(out),\n           extent=[-2*np.pi, 2*np.pi, -2*np.pi, 2*np.pi], cmap='hsv')\nplt.title('Phase (angle) of exp(x)')\nplt.show()\n"""
numpy/reference/generated/numpy-fft-fft-1.py,4,"b'np.fft.fft(np.exp(2j * np.pi * np.arange(8) / 8))\n# array([-2.33486982e-16+1.14423775e-17j,  8.00000000e+00-1.25557246e-15j,\n# 2.33486982e-16+2.33486982e-16j,  0.00000000e+00+1.22464680e-16j,\n# -1.14423775e-17+2.33486982e-16j,  0.00000000e+00+5.20784380e-16j,\n# 1.14423775e-17+1.14423775e-17j,  0.00000000e+00+1.22464680e-16j])\n\n# In this example, real input has an FFT which is Hermitian, i.e., symmetric\n# in the real part and anti-symmetric in the imaginary part, as described in\n# the `numpy.fft` documentation:\n\nimport matplotlib.pyplot as plt\nt = np.arange(256)\nsp = np.fft.fft(np.sin(t))\nfreq = np.fft.fftfreq(t.shape[-1])\nplt.plot(freq, sp.real, freq, sp.imag)\n# [<matplotlib.lines.Line2D object at 0x...>, <matplotlib.lines.Line2D object at 0x...>]\nplt.show()\n'"
numpy/reference/generated/numpy-fft-fftn-1.py,8,"b'a = np.mgrid[:3, :3, :3][0]\nnp.fft.fftn(a, axes=(1, 2))\n# array([[[ 0.+0.j,   0.+0.j,   0.+0.j], # may vary\n# [ 0.+0.j,   0.+0.j,   0.+0.j],\n# [ 0.+0.j,   0.+0.j,   0.+0.j]],\n# [[ 9.+0.j,   0.+0.j,   0.+0.j],\n# [ 0.+0.j,   0.+0.j,   0.+0.j],\n# [ 0.+0.j,   0.+0.j,   0.+0.j]],\n# [[18.+0.j,   0.+0.j,   0.+0.j],\n# [ 0.+0.j,   0.+0.j,   0.+0.j],\n# [ 0.+0.j,   0.+0.j,   0.+0.j]]])\nnp.fft.fftn(a, (2, 2), axes=(0, 1))\n# array([[[ 2.+0.j,  2.+0.j,  2.+0.j], # may vary\n# [ 0.+0.j,  0.+0.j,  0.+0.j]],\n# [[-2.+0.j, -2.+0.j, -2.+0.j],\n# [ 0.+0.j,  0.+0.j,  0.+0.j]]])\n\nimport matplotlib.pyplot as plt\n[X, Y] = np.meshgrid(2 * np.pi * np.arange(200) / 12,\n                     2 * np.pi * np.arange(200) / 34)\nS = np.sin(X) + np.cos(Y) + np.random.uniform(0, 1, X.shape)\nFS = np.fft.fftn(S)\nplt.imshow(np.log(np.abs(np.fft.fftshift(FS))**2))\n# <matplotlib.image.AxesImage object at 0x...>\nplt.show()\n'"
numpy/reference/generated/numpy-fft-ifft-1.py,5,"b""np.fft.ifft([0, 4, 0, 0])\n# array([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j]) # may vary\n\n# Create and plot a band-limited signal with random phases:\n\nimport matplotlib.pyplot as plt\nt = np.arange(400)\nn = np.zeros((400,), dtype=complex)\nn[40:60] = np.exp(1j*np.random.uniform(0, 2*np.pi, (20,)))\ns = np.fft.ifft(n)\nplt.plot(t, s.real, 'b-', t, s.imag, 'r--')\n# [<matplotlib.lines.Line2D object at ...>, <matplotlib.lines.Line2D object at ...>]\nplt.legend(('real', 'imaginary'))\n# <matplotlib.legend.Legend object at ...>\nplt.show()\n"""
numpy/reference/generated/numpy-fft-ifftn-1.py,5,"b'a = np.eye(4)\nnp.fft.ifftn(np.fft.fftn(a, axes=(0,)), axes=(1,))\n# array([[1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j], # may vary\n# [0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n# [0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n# [0.+0.j,  0.+0.j,  0.+0.j,  1.+0.j]])\n\n# Create and plot an image with band-limited frequency content:\n\nimport matplotlib.pyplot as plt\nn = np.zeros((200,200), dtype=complex)\nn[60:80, 20:40] = np.exp(1j*np.random.uniform(0, 2*np.pi, (20, 20)))\nim = np.fft.ifftn(n).real\nplt.imshow(im)\n# <matplotlib.image.AxesImage object at 0x...>\nplt.show()\n'"
numpy/reference/generated/numpy-geomspace-1.py,13,"b""np.geomspace(1, 1000, num=4)\n# array([    1.,    10.,   100.,  1000.])\nnp.geomspace(1, 1000, num=3, endpoint=False)\n# array([   1.,   10.,  100.])\nnp.geomspace(1, 1000, num=4, endpoint=False)\n# array([   1.        ,    5.62341325,   31.6227766 ,  177.827941  ])\nnp.geomspace(1, 256, num=9)\n# array([   1.,    2.,    4.,    8.,   16.,   32.,   64.,  128.,  256.])\n\n# Note that the above may not produce exact integers:\n\nnp.geomspace(1, 256, num=9, dtype=int)\n# array([  1,   2,   4,   7,  16,  32,  63, 127, 256])\nnp.around(np.geomspace(1, 256, num=9)).astype(int)\n# array([  1,   2,   4,   8,  16,  32,  64, 128, 256])\n\n# Negative, decreasing, and complex inputs are allowed:\n\nnp.geomspace(1000, 1, num=4)\n# array([1000.,  100.,   10.,    1.])\nnp.geomspace(-1000, -1, num=4)\n# array([-1000.,  -100.,   -10.,    -1.])\nnp.geomspace(1j, 1000j, num=4)  # Straight line\n# array([0.   +1.j, 0.  +10.j, 0. +100.j, 0.+1000.j])\nnp.geomspace(-1+0j, 1+0j, num=5)  # Circle\n# array([-1.00000000e+00+1.22464680e-16j, -7.07106781e-01+7.07106781e-01j,\n# 6.12323400e-17+1.00000000e+00j,  7.07106781e-01+7.07106781e-01j,\n# 1.00000000e+00+0.00000000e+00j])\n\n# Graphical illustration of ``endpoint`` parameter:\n\nimport matplotlib.pyplot as plt\nN = 10\ny = np.zeros(N)\nplt.semilogx(np.geomspace(1, 1000, N, endpoint=True), y + 1, 'o')\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.semilogx(np.geomspace(1, 1000, N, endpoint=False), y + 2, 'o')\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.axis([0.5, 2000, 0, 3])\n# [0.5, 2000, 0, 3]\nplt.grid(True, color='0.7', linestyle='-', which='both', axis='both')\nplt.show()\n"""
numpy/reference/generated/numpy-hamming-1.py,6,"b'np.hamming(12)\n# array([ 0.08      ,  0.15302337,  0.34890909,  0.60546483,  0.84123594, # may vary\n# 0.98136677,  0.98136677,  0.84123594,  0.60546483,  0.34890909,\n# 0.15302337,  0.08      ])\n\n# Plot the window and the frequency response:\n\nimport matplotlib.pyplot as plt\nfrom numpy.fft import fft, fftshift\nwindow = np.hamming(51)\nplt.plot(window)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Hamming window"")\n# Text(0.5, 1.0, \'Hamming window\')\nplt.ylabel(""Amplitude"")\n# Text(0, 0.5, \'Amplitude\')\nplt.xlabel(""Sample"")\n# Text(0.5, 0, \'Sample\')\nplt.show()\n\nplt.figure()\n# <Figure size 640x480 with 0 Axes>\nA = fft(window, 2048) / 25.5\nmag = np.abs(fftshift(A))\nfreq = np.linspace(-0.5, 0.5, len(A))\nresponse = 20 * np.log10(mag)\nresponse = np.clip(response, -100, 100)\nplt.plot(freq, response)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Frequency response of Hamming window"")\n# Text(0.5, 1.0, \'Frequency response of Hamming window\')\nplt.ylabel(""Magnitude [dB]"")\n# Text(0, 0.5, \'Magnitude [dB]\')\nplt.xlabel(""Normalized frequency [cycles per sample]"")\n# Text(0.5, 0, \'Normalized frequency [cycles per sample]\')\nplt.axis(\'tight\')\n# ...\nplt.show()\n'"
numpy/reference/generated/numpy-hanning-1.py,7,"b'np.hanning(12)\n# array([0.        , 0.07937323, 0.29229249, 0.57115742, 0.82743037,\n# 0.97974649, 0.97974649, 0.82743037, 0.57115742, 0.29229249,\n# 0.07937323, 0.        ])\n\n# Plot the window and its frequency response:\n\nimport matplotlib.pyplot as plt\nfrom numpy.fft import fft, fftshift\nwindow = np.hanning(51)\nplt.plot(window)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Hann window"")\n# Text(0.5, 1.0, \'Hann window\')\nplt.ylabel(""Amplitude"")\n# Text(0, 0.5, \'Amplitude\')\nplt.xlabel(""Sample"")\n# Text(0.5, 0, \'Sample\')\nplt.show()\n\nplt.figure()\n# <Figure size 640x480 with 0 Axes>\nA = fft(window, 2048) / 25.5\nmag = np.abs(fftshift(A))\nfreq = np.linspace(-0.5, 0.5, len(A))\nwith np.errstate(divide=\'ignore\', invalid=\'ignore\'):\n    response = 20 * np.log10(mag)\n# ...\nresponse = np.clip(response, -100, 100)\nplt.plot(freq, response)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Frequency response of the Hann window"")\n# Text(0.5, 1.0, \'Frequency response of the Hann window\')\nplt.ylabel(""Magnitude [dB]"")\n# Text(0, 0.5, \'Magnitude [dB]\')\nplt.xlabel(""Normalized frequency [cycles per sample]"")\n# Text(0.5, 0, \'Normalized frequency [cycles per sample]\')\nplt.axis(\'tight\')\n# ...\nplt.show()\n'"
numpy/reference/generated/numpy-histogram-1.py,9,"b'np.histogram([1, 2, 1], bins=[0, 1, 2, 3])\n# (array([0, 2, 1]), array([0, 1, 2, 3]))\nnp.histogram(np.arange(4), bins=np.arange(5), density=True)\n# (array([0.25, 0.25, 0.25, 0.25]), array([0, 1, 2, 3, 4]))\nnp.histogram([[1, 2, 1], [1, 0, 1]], bins=[0,1,2,3])\n# (array([1, 4, 1]), array([0, 1, 2, 3]))\n\na = np.arange(5)\nhist, bin_edges = np.histogram(a, density=True)\nhist\n# array([0.5, 0. , 0.5, 0. , 0. , 0.5, 0. , 0.5, 0. , 0.5])\nhist.sum()\n# 2.4999999999999996\nnp.sum(hist * np.diff(bin_edges))\n# 1.0\n\n# .. versionadded:: 1.11.0\n\n# Automated Bin Selection Methods example, using 2 peak random data\n# with 2000 points:\n\nimport matplotlib.pyplot as plt\nrng = np.random.RandomState(10)  # deterministic random data\na = np.hstack((rng.normal(size=1000),\n               rng.normal(loc=5, scale=2, size=1000)))\n_ = plt.hist(a, bins=\'auto\')  # arguments are passed to np.histogram\nplt.title(""Histogram with \'auto\' bins"")\n# Text(0.5, 1.0, ""Histogram with \'auto\' bins"")\nplt.show()\n'"
numpy/reference/generated/numpy-histogram2d-1.py,4,"b""from matplotlib.image import NonUniformImage\nimport matplotlib.pyplot as plt\n\n# Construct a 2-D histogram with variable bin width. First define the bin\n# edges:\n\nxedges = [0, 1, 3, 5]\nyedges = [0, 2, 3, 4, 6]\n\n# Next we create a histogram H with random bin content:\n\nx = np.random.normal(2, 1, 100)\ny = np.random.normal(1, 1, 100)\nH, xedges, yedges = np.histogram2d(x, y, bins=(xedges, yedges))\nH = H.T  # Let each row list bins with common y range.\n\n# :func:`imshow <matplotlib.pyplot.imshow>` can only display square bins:\n\nfig = plt.figure(figsize=(7, 3))\nax = fig.add_subplot(131, title='imshow: square bins')\nplt.imshow(H, interpolation='nearest', origin='low',\n        extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])\n# <matplotlib.image.AxesImage object at 0x...>\n\n# :func:`pcolormesh <matplotlib.pyplot.pcolormesh>` can display actual edges:\n\nax = fig.add_subplot(132, title='pcolormesh: actual edges',\n        aspect='equal')\nX, Y = np.meshgrid(xedges, yedges)\nax.pcolormesh(X, Y, H)\n# <matplotlib.collections.QuadMesh object at 0x...>\n\n# :class:`NonUniformImage <matplotlib.image.NonUniformImage>` can be used to\n# display actual bin edges with interpolation:\n\nax = fig.add_subplot(133, title='NonUniformImage: interpolated',\n        aspect='equal', xlim=xedges[[0, -1]], ylim=yedges[[0, -1]])\nim = NonUniformImage(ax, interpolation='bilinear')\nxcenters = (xedges[:-1] + xedges[1:]) / 2\nycenters = (yedges[:-1] + yedges[1:]) / 2\nim.set_data(xcenters, ycenters, H)\nax.images.append(im)\nplt.show()\n"""
numpy/reference/generated/numpy-interp-1.py,9,"b""xp = [1, 2, 3]\nfp = [3, 2, 0]\nnp.interp(2.5, xp, fp)\n# 1.0\nnp.interp([0, 1, 1.5, 2.72, 3.14], xp, fp)\n# array([3.  , 3.  , 2.5 , 0.56, 0.  ])\nUNDEF = -99.0\nnp.interp(3.14, xp, fp, right=UNDEF)\n# -99.0\n\n# Plot an interpolant to the sine function:\n\nx = np.linspace(0, 2*np.pi, 10)\ny = np.sin(x)\nxvals = np.linspace(0, 2*np.pi, 50)\nyinterp = np.interp(xvals, x, y)\nimport matplotlib.pyplot as plt\nplt.plot(x, y, 'o')\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.plot(xvals, yinterp, '-x')\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.show()\n\n# Interpolation with periodic x-coordinates:\n\nx = [-180, -170, -185, 185, -10, -5, 0, 365]\nxp = [190, -190, 350, -350]\nfp = [5, 10, 3, 4]\nnp.interp(x, xp, fp, period=360)\n# array([7.5 , 5.  , 8.75, 6.25, 3.  , 3.25, 3.5 , 3.75])\n\n# Complex interpolation:\n\nx = [1.5, 4.0]\nxp = [2,3,5]\nfp = [1.0j, 0, 2+3j]\nnp.interp(x, xp, fp)\n# array([0.+1.j , 1.+1.5j])\n"""
numpy/reference/generated/numpy-kaiser-1.py,6,"b'import matplotlib.pyplot as plt\nnp.kaiser(12, 14)\n# array([7.72686684e-06, 3.46009194e-03, 4.65200189e-02, # may vary\n# 2.29737120e-01, 5.99885316e-01, 9.45674898e-01,\n# 9.45674898e-01, 5.99885316e-01, 2.29737120e-01,\n# 4.65200189e-02, 3.46009194e-03, 7.72686684e-06])\n\n# Plot the window and the frequency response:\n\nfrom numpy.fft import fft, fftshift\nwindow = np.kaiser(51, 14)\nplt.plot(window)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Kaiser window"")\n# Text(0.5, 1.0, \'Kaiser window\')\nplt.ylabel(""Amplitude"")\n# Text(0, 0.5, \'Amplitude\')\nplt.xlabel(""Sample"")\n# Text(0.5, 0, \'Sample\')\nplt.show()\n\nplt.figure()\n# <Figure size 640x480 with 0 Axes>\nA = fft(window, 2048) / 25.5\nmag = np.abs(fftshift(A))\nfreq = np.linspace(-0.5, 0.5, len(A))\nresponse = 20 * np.log10(mag)\nresponse = np.clip(response, -100, 100)\nplt.plot(freq, response)\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Frequency response of Kaiser window"")\n# Text(0.5, 1.0, \'Frequency response of Kaiser window\')\nplt.ylabel(""Magnitude [dB]"")\n# Text(0, 0.5, \'Magnitude [dB]\')\nplt.xlabel(""Normalized frequency [cycles per sample]"")\n# Text(0.5, 0, \'Normalized frequency [cycles per sample]\')\nplt.axis(\'tight\')\n# (-0.5, 0.5, -100.0, ...) # may vary\nplt.show()\n'"
numpy/reference/generated/numpy-linalg-lstsq-1.py,4,"b""# Fit a line, ``y = mx + c``, through some noisy data-points:\n\nx = np.array([0, 1, 2, 3])\ny = np.array([-1, 0.2, 0.9, 2.1])\n\n# By examining the coefficients, we see that the line should have a\n# gradient of roughly 1 and cut the y-axis at, more or less, -1.\n\n# We can rewrite the line equation as ``y = Ap``, where ``A = [[x 1]]``\n# and ``p = [[m], [c]]``.  Now use `lstsq` to solve for `p`:\n\nA = np.vstack([x, np.ones(len(x))]).T\nA\n# array([[ 0.,  1.],\n# [ 1.,  1.],\n# [ 2.,  1.],\n# [ 3.,  1.]])\n\nm, c = np.linalg.lstsq(A, y, rcond=None)[0]\nm, c\n# (1.0 -0.95) # may vary\n\n# Plot the data along with the fitted line:\n\nimport matplotlib.pyplot as plt\n_ = plt.plot(x, y, 'o', label='Original data', markersize=10)\n_ = plt.plot(x, m*x + c, 'r', label='Fitted line')\n_ = plt.legend()\nplt.show()\n"""
numpy/reference/generated/numpy-linspace-1.py,6,"b""np.linspace(2.0, 3.0, num=5)\n# array([2.  , 2.25, 2.5 , 2.75, 3.  ])\nnp.linspace(2.0, 3.0, num=5, endpoint=False)\n# array([2. ,  2.2,  2.4,  2.6,  2.8])\nnp.linspace(2.0, 3.0, num=5, retstep=True)\n# (array([2.  ,  2.25,  2.5 ,  2.75,  3.  ]), 0.25)\n\n# Graphical illustration:\n\nimport matplotlib.pyplot as plt\nN = 8\ny = np.zeros(N)\nx1 = np.linspace(0, 10, N, endpoint=True)\nx2 = np.linspace(0, 10, N, endpoint=False)\nplt.plot(x1, y, 'o')\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.plot(x2, y + 0.5, 'o')\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.ylim([-0.5, 1])\n# (-0.5, 1)\nplt.show()\n"""
numpy/reference/generated/numpy-logspace-1.py,6,"b""np.logspace(2.0, 3.0, num=4)\n# array([ 100.        ,  215.443469  ,  464.15888336, 1000.        ])\nnp.logspace(2.0, 3.0, num=4, endpoint=False)\n# array([100.        ,  177.827941  ,  316.22776602,  562.34132519])\nnp.logspace(2.0, 3.0, num=4, base=2.0)\n# array([4.        ,  5.0396842 ,  6.34960421,  8.        ])\n\n# Graphical illustration:\n\nimport matplotlib.pyplot as plt\nN = 10\nx1 = np.logspace(0.1, 1, N, endpoint=True)\nx2 = np.logspace(0.1, 1, N, endpoint=False)\ny = np.zeros(N)\nplt.plot(x1, y, 'o')\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.plot(x2, y + 0.5, 'o')\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.ylim([-0.5, 1])\n# (-0.5, 1)\nplt.show()\n"""
numpy/reference/generated/numpy-ma-polyfit-1.py,7,"b""import warnings\nx = np.array([0.0, 1.0, 2.0, 3.0,  4.0,  5.0])\ny = np.array([0.0, 0.8, 0.9, 0.1, -0.8, -1.0])\nz = np.polyfit(x, y, 3)\nz\n# array([ 0.08703704, -0.81349206,  1.69312169, -0.03968254]) # may vary\n\n# It is convenient to use `poly1d` objects for dealing with polynomials:\n\np = np.poly1d(z)\np(0.5)\n# 0.6143849206349179 # may vary\np(3.5)\n# -0.34732142857143039 # may vary\np(10)\n# 22.579365079365115 # may vary\n\n# High-order polynomials may oscillate wildly:\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', np.RankWarning)\n    p30 = np.poly1d(np.polyfit(x, y, 30))\n# ...\np30(4)\n# -0.80000000000000204 # may vary\np30(5)\n# -0.99999999999999445 # may vary\np30(4.5)\n# -0.10547061179440398 # may vary\n\n# Illustration:\n\nimport matplotlib.pyplot as plt\nxp = np.linspace(-2, 6, 100)\n_ = plt.plot(x, y, '.', xp, p(xp), '-', xp, p30(xp), '--')\nplt.ylim(-2,2)\n# (-2, 2)\nplt.show()\n"""
numpy/reference/generated/numpy-meshgrid-1.py,8,"b'nx, ny = (3, 2)\nx = np.linspace(0, 1, nx)\ny = np.linspace(0, 1, ny)\nxv, yv = np.meshgrid(x, y)\nxv\n# array([[0. , 0.5, 1. ],\n# [0. , 0.5, 1. ]])\nyv\n# array([[0.,  0.,  0.],\n# [1.,  1.,  1.]])\nxv, yv = np.meshgrid(x, y, sparse=True)  # make sparse output arrays\nxv\n# array([[0. ,  0.5,  1. ]])\nyv\n# array([[0.],\n# [1.]])\n\n# `meshgrid` is very useful to evaluate functions on a grid.\n\nimport matplotlib.pyplot as plt\nx = np.arange(-5, 5, 0.1)\ny = np.arange(-5, 5, 0.1)\nxx, yy = np.meshgrid(x, y, sparse=True)\nz = np.sin(xx**2 + yy**2) / (xx**2 + yy**2)\nh = plt.contourf(x,y,z)\nplt.show()\n'"
numpy/reference/generated/numpy-percentile-1.py,3,"b""import matplotlib.pyplot as plt\n\na = np.arange(4)\np = np.linspace(0, 100, 6001)\nax = plt.gca()\nlines = [\n    ('linear', None),\n    ('higher', '--'),\n    ('lower', '--'),\n    ('nearest', '-.'),\n    ('midpoint', '-.'),\n]\nfor interpolation, style in lines:\n    ax.plot(\n        p, np.percentile(a, p, interpolation=interpolation),\n        label=interpolation, linestyle=style)\nax.set(\n    title='Interpolation methods for list: ' + str(a),\n    xlabel='Percentile',\n    ylabel='List item returned',\n    yticks=a)\nax.legend()\nplt.show()"""
numpy/reference/generated/numpy-polyfit-1.py,7,"b""import warnings\nx = np.array([0.0, 1.0, 2.0, 3.0,  4.0,  5.0])\ny = np.array([0.0, 0.8, 0.9, 0.1, -0.8, -1.0])\nz = np.polyfit(x, y, 3)\nz\n# array([ 0.08703704, -0.81349206,  1.69312169, -0.03968254]) # may vary\n\n# It is convenient to use `poly1d` objects for dealing with polynomials:\n\np = np.poly1d(z)\np(0.5)\n# 0.6143849206349179 # may vary\np(3.5)\n# -0.34732142857143039 # may vary\np(10)\n# 22.579365079365115 # may vary\n\n# High-order polynomials may oscillate wildly:\n\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore', np.RankWarning)\n    p30 = np.poly1d(np.polyfit(x, y, 30))\n# ...\np30(4)\n# -0.80000000000000204 # may vary\np30(5)\n# -0.99999999999999445 # may vary\np30(4.5)\n# -0.10547061179440398 # may vary\n\n# Illustration:\n\nimport matplotlib.pyplot as plt\nxp = np.linspace(-2, 6, 100)\n_ = plt.plot(x, y, '.', xp, p(xp), '-', xp, p30(xp), '--')\nplt.ylim(-2,2)\n# (-2, 2)\nplt.show()\n"""
numpy/reference/generated/numpy-sin-1.py,4,"b""# Print sine of one angle:\n\nnp.sin(np.pi/2.)\n# 1.0\n\n# Print sines of an array of angles given in degrees:\n\nnp.sin(np.array((0., 30., 45., 60., 90.)) * np.pi / 180. )\n# array([ 0.        ,  0.5       ,  0.70710678,  0.8660254 ,  1.        ])\n\n# Plot the sine function:\n\nimport matplotlib.pylab as plt\nx = np.linspace(-np.pi, np.pi, 201)\nplt.plot(x, np.sin(x))\nplt.xlabel('Angle [rad]')\nplt.ylabel('sin(x)')\nplt.axis('tight')\nplt.show()\n"""
numpy/reference/generated/numpy-sinc-1.py,3,"b'import matplotlib.pyplot as plt\nx = np.linspace(-4, 4, 41)\nnp.sinc(x)\n# array([-3.89804309e-17,  -4.92362781e-02,  -8.40918587e-02, # may vary\n# -8.90384387e-02,  -5.84680802e-02,   3.89804309e-17,\n# 6.68206631e-02,   1.16434881e-01,   1.26137788e-01,\n# 8.50444803e-02,  -3.89804309e-17,  -1.03943254e-01,\n# -1.89206682e-01,  -2.16236208e-01,  -1.55914881e-01,\n# 3.89804309e-17,   2.33872321e-01,   5.04551152e-01,\n# 7.56826729e-01,   9.35489284e-01,   1.00000000e+00,\n# 9.35489284e-01,   7.56826729e-01,   5.04551152e-01,\n# 2.33872321e-01,   3.89804309e-17,  -1.55914881e-01,\n# -2.16236208e-01,  -1.89206682e-01,  -1.03943254e-01,\n# -3.89804309e-17,   8.50444803e-02,   1.26137788e-01,\n# 1.16434881e-01,   6.68206631e-02,   3.89804309e-17,\n# -5.84680802e-02,  -8.90384387e-02,  -8.40918587e-02,\n# -4.92362781e-02,  -3.89804309e-17])\n\nplt.plot(x, np.sinc(x))\n# [<matplotlib.lines.Line2D object at 0x...>]\nplt.title(""Sinc Function"")\n# Text(0.5, 1.0, \'Sinc Function\')\nplt.ylabel(""Amplitude"")\n# Text(0, 0.5, \'Amplitude\')\nplt.xlabel(""X"")\n# Text(0.5, 0, \'X\')\nplt.show()\n'"
scikit-learn/_downloads/009a12ef9894f97d534f6158f5500946/plot_random_dataset.py,0,"b'""""""\n==============================================\nPlot randomly generated classification dataset\n==============================================\n\nPlot several randomly generated 2D classification datasets.\nThis example illustrates the :func:`datasets.make_classification`\n:func:`datasets.make_blobs` and :func:`datasets.make_gaussian_quantiles`\nfunctions.\n\nFor ``make_classification``, three binary and two multi-class classification\ndatasets are generated, with different numbers of informative features and\nclusters per class.  """"""\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_gaussian_quantiles\n\nplt.figure(figsize=(8, 8))\nplt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n\nplt.subplot(321)\nplt.title(""One informative feature, one cluster per class"", fontsize=\'small\')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=1,\n                             n_clusters_per_class=1)\nplt.scatter(X1[:, 0], X1[:, 1], marker=\'o\', c=Y1,\n            s=25, edgecolor=\'k\')\n\nplt.subplot(322)\nplt.title(""Two informative features, one cluster per class"", fontsize=\'small\')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1)\nplt.scatter(X1[:, 0], X1[:, 1], marker=\'o\', c=Y1,\n            s=25, edgecolor=\'k\')\n\nplt.subplot(323)\nplt.title(""Two informative features, two clusters per class"",\n          fontsize=\'small\')\nX2, Y2 = make_classification(n_features=2, n_redundant=0, n_informative=2)\nplt.scatter(X2[:, 0], X2[:, 1], marker=\'o\', c=Y2,\n            s=25, edgecolor=\'k\')\n\nplt.subplot(324)\nplt.title(""Multi-class, two informative features, one cluster"",\n          fontsize=\'small\')\nX1, Y1 = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                             n_clusters_per_class=1, n_classes=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker=\'o\', c=Y1,\n            s=25, edgecolor=\'k\')\n\nplt.subplot(325)\nplt.title(""Three blobs"", fontsize=\'small\')\nX1, Y1 = make_blobs(n_features=2, centers=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker=\'o\', c=Y1,\n            s=25, edgecolor=\'k\')\n\nplt.subplot(326)\nplt.title(""Gaussian divided into three quantiles"", fontsize=\'small\')\nX1, Y1 = make_gaussian_quantiles(n_features=2, n_classes=3)\nplt.scatter(X1[:, 0], X1[:, 1], marker=\'o\', c=Y1,\n            s=25, edgecolor=\'k\')\n\nplt.show()\n'"
scikit-learn/_downloads/0158b811b8a337ef0f0afea64aa8a87c/plot_digits_pipe.py,2,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nPipelining: chaining a PCA and a logistic regression\n=========================================================\n\nThe PCA does an unsupervised dimensionality reduction, while the logistic\nregression does the prediction.\n\nWe use a GridSearchCV to set the dimensionality of the PCA\n\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Define a pipeline to search for the best combination of PCA truncation\n# and classifier regularization.\npca = PCA()\n# set the tolerance to a large value to make the example faster\nlogistic = LogisticRegression(max_iter=10000, tol=0.1)\npipe = Pipeline(steps=[(\'pca\', pca), (\'logistic\', logistic)])\n\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\n\n# Parameters of pipelines can be set using \xe2\x80\x98__\xe2\x80\x99 separated parameter names:\nparam_grid = {\n    \'pca__n_components\': [5, 15, 30, 45, 64],\n    \'logistic__C\': np.logspace(-4, 4, 4),\n}\nsearch = GridSearchCV(pipe, param_grid, n_jobs=-1)\nsearch.fit(X_digits, y_digits)\nprint(""Best parameter (CV score=%0.3f):"" % search.best_score_)\nprint(search.best_params_)\n\n# Plot the PCA spectrum\npca.fit(X_digits)\n\nfig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\nax0.plot(np.arange(1, pca.n_components_ + 1),\n         pca.explained_variance_ratio_, \'+\', linewidth=2)\nax0.set_ylabel(\'PCA explained variance ratio\')\n\nax0.axvline(search.best_estimator_.named_steps[\'pca\'].n_components,\n            linestyle=\':\', label=\'n_components chosen\')\nax0.legend(prop=dict(size=12))\n\n# For each number of components, find the best classifier results\nresults = pd.DataFrame(search.cv_results_)\ncomponents_col = \'param_pca__n_components\'\nbest_clfs = results.groupby(components_col).apply(\n    lambda g: g.nlargest(1, \'mean_test_score\'))\n\nbest_clfs.plot(x=components_col, y=\'mean_test_score\', yerr=\'std_test_score\',\n               legend=False, ax=ax1)\nax1.set_ylabel(\'Classification accuracy (val)\')\nax1.set_xlabel(\'n_components\')\n\nplt.xlim(-1, 70)\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/02208511847191c698792d2abac1047f/plot_spectral_coclustering.py,3,"b'""""""\n==============================================\nA demo of the Spectral Co-Clustering algorithm\n==============================================\n\nThis example demonstrates how to generate a dataset and bicluster it\nusing the Spectral Co-Clustering algorithm.\n\nThe dataset is generated using the ``make_biclusters`` function, which\ncreates a matrix of small values and implants bicluster with large\nvalues. The rows and columns are then shuffled and passed to the\nSpectral Co-Clustering algorithm. Rearranging the shuffled matrix to\nmake biclusters contiguous shows how accurately the algorithm found\nthe biclusters.\n\n""""""\nprint(__doc__)\n\n# Author: Kemal Eren <kemal@kemaleren.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_biclusters\nfrom sklearn.cluster import SpectralCoclustering\nfrom sklearn.metrics import consensus_score\n\ndata, rows, columns = make_biclusters(\n    shape=(300, 300), n_clusters=5, noise=5,\n    shuffle=False, random_state=0)\n\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title(""Original dataset"")\n\n# shuffle clusters\nrng = np.random.RandomState(0)\nrow_idx = rng.permutation(data.shape[0])\ncol_idx = rng.permutation(data.shape[1])\ndata = data[row_idx][:, col_idx]\n\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title(""Shuffled dataset"")\n\nmodel = SpectralCoclustering(n_clusters=5, random_state=0)\nmodel.fit(data)\nscore = consensus_score(model.biclusters_,\n                        (rows[:, row_idx], columns[:, col_idx]))\n\nprint(""consensus score: {:.3f}"".format(score))\n\nfit_data = data[np.argsort(model.row_labels_)]\nfit_data = fit_data[:, np.argsort(model.column_labels_)]\n\nplt.matshow(fit_data, cmap=plt.cm.Blues)\nplt.title(""After biclustering; rearranged to show biclusters"")\n\nplt.show()\n'"
scikit-learn/_downloads/02a9b888aa307dd3b3e0557a3259edf9/plot_faces_decomposition.py,0,"b'""""""\n============================\nFaces dataset decompositions\n============================\n\nThis example applies to :ref:`olivetti_faces_dataset` different unsupervised\nmatrix decomposition (dimension reduction) methods from the module\n:py:mod:`sklearn.decomposition` (see the documentation chapter\n:ref:`decompositions`) .\n\n""""""\nprint(__doc__)\n\n# Authors: Vlad Niculae, Alexandre Gramfort\n# License: BSD 3 clause\n\nimport logging\nfrom time import time\n\nfrom numpy.random import RandomState\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn import decomposition\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format=\'%(asctime)s %(levelname)s %(message)s\')\nn_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\nrng = RandomState(0)\n\n# #############################################################################\n# Load faces data\nfaces, _ = fetch_olivetti_faces(return_X_y=True, shuffle=True,\n                                random_state=rng)\nn_samples, n_features = faces.shape\n\n# global centering\nfaces_centered = faces - faces.mean(axis=0)\n\n# local centering\nfaces_centered -= faces_centered.mean(axis=1).reshape(n_samples, -1)\n\nprint(""Dataset consists of %d faces"" % n_samples)\n\n\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\n    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n    plt.suptitle(title, size=16)\n    for i, comp in enumerate(images):\n        plt.subplot(n_row, n_col, i + 1)\n        vmax = max(comp.max(), -comp.min())\n        plt.imshow(comp.reshape(image_shape), cmap=cmap,\n                   interpolation=\'nearest\',\n                   vmin=-vmax, vmax=vmax)\n        plt.xticks(())\n        plt.yticks(())\n    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n\n# #############################################################################\n# List of the different estimators, whether to center and transpose the\n# problem, and whether the transformer uses the clustering API.\nestimators = [\n    (\'Eigenfaces - PCA using randomized SVD\',\n     decomposition.PCA(n_components=n_components, svd_solver=\'randomized\',\n                       whiten=True),\n     True),\n\n    (\'Non-negative components - NMF\',\n     decomposition.NMF(n_components=n_components, init=\'nndsvda\', tol=5e-3),\n     False),\n\n    (\'Independent components - FastICA\',\n     decomposition.FastICA(n_components=n_components, whiten=True),\n     True),\n\n    (\'Sparse comp. - MiniBatchSparsePCA\',\n     decomposition.MiniBatchSparsePCA(n_components=n_components, alpha=0.8,\n                                      n_iter=100, batch_size=3,\n                                      random_state=rng),\n     True),\n\n    (\'MiniBatchDictionaryLearning\',\n        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n                                                  n_iter=50, batch_size=3,\n                                                  random_state=rng),\n     True),\n\n    (\'Cluster centers - MiniBatchKMeans\',\n        MiniBatchKMeans(n_clusters=n_components, tol=1e-3, batch_size=20,\n                        max_iter=50, random_state=rng),\n     True),\n\n    (\'Factor Analysis components - FA\',\n     decomposition.FactorAnalysis(n_components=n_components, max_iter=20),\n     True),\n]\n\n\n# #############################################################################\n# Plot a sample of the input data\n\nplot_gallery(""First centered Olivetti faces"", faces_centered[:n_components])\n\n# #############################################################################\n# Do the estimation and plot it\n\nfor name, estimator, center in estimators:\n    print(""Extracting the top %d %s..."" % (n_components, name))\n    t0 = time()\n    data = faces\n    if center:\n        data = faces_centered\n    estimator.fit(data)\n    train_time = (time() - t0)\n    print(""done in %0.3fs"" % train_time)\n    if hasattr(estimator, \'cluster_centers_\'):\n        components_ = estimator.cluster_centers_\n    else:\n        components_ = estimator.components_\n\n    # Plot an image representing the pixelwise variance provided by the\n    # estimator e.g its noise_variance_ attribute. The Eigenfaces estimator,\n    # via the PCA decomposition, also provides a scalar noise_variance_\n    # (the mean of pixelwise variance) that cannot be displayed as an image\n    # so we skip it.\n    if (hasattr(estimator, \'noise_variance_\') and\n            estimator.noise_variance_.ndim > 0):  # Skip the Eigenfaces case\n        plot_gallery(""Pixelwise variance"",\n                     estimator.noise_variance_.reshape(1, -1), n_col=1,\n                     n_row=1)\n    plot_gallery(\'%s - Train time %.1fs\' % (name, train_time),\n                 components_[:n_components])\n\nplt.show()\n\n# #############################################################################\n# Various positivity constraints applied to dictionary learning.\nestimators = [\n    (\'Dictionary learning\',\n        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n                                                  n_iter=50, batch_size=3,\n                                                  random_state=rng),\n     True),\n    (\'Dictionary learning - positive dictionary\',\n        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n                                                  n_iter=50, batch_size=3,\n                                                  random_state=rng,\n                                                  positive_dict=True),\n     True),\n    (\'Dictionary learning - positive code\',\n        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n                                                  n_iter=50, batch_size=3,\n                                                  fit_algorithm=\'cd\',\n                                                  random_state=rng,\n                                                  positive_code=True),\n     True),\n    (\'Dictionary learning - positive dictionary & code\',\n        decomposition.MiniBatchDictionaryLearning(n_components=15, alpha=0.1,\n                                                  n_iter=50, batch_size=3,\n                                                  fit_algorithm=\'cd\',\n                                                  random_state=rng,\n                                                  positive_dict=True,\n                                                  positive_code=True),\n     True),\n]\n\n\n# #############################################################################\n# Plot a sample of the input data\n\nplot_gallery(""First centered Olivetti faces"", faces_centered[:n_components],\n             cmap=plt.cm.RdBu)\n\n# #############################################################################\n# Do the estimation and plot it\n\nfor name, estimator, center in estimators:\n    print(""Extracting the top %d %s..."" % (n_components, name))\n    t0 = time()\n    data = faces\n    if center:\n        data = faces_centered\n    estimator.fit(data)\n    train_time = (time() - t0)\n    print(""done in %0.3fs"" % train_time)\n    components_ = estimator.components_\n    plot_gallery(name, components_[:n_components], cmap=plt.cm.RdBu)\n\nplt.show()\n'"
scikit-learn/_downloads/03ac5ae108d1a626d1be1e7bc190af73/plot_multi_metric_evaluation.py,2,"b'""""""\n============================================================================\nDemonstration of multi-metric evaluation on cross_val_score and GridSearchCV\n============================================================================\n\nMultiple metric parameter search can be done by setting the ``scoring``\nparameter to a list of metric scorer names or a dict mapping the scorer names\nto the scorer callables.\n\nThe scores of all the scorers are available in the ``cv_results_`` dict at keys\nending in ``\'_<scorer_name>\'`` (``\'mean_test_precision\'``,\n``\'rank_test_precision\'``, etc...)\n\nThe ``best_estimator_``, ``best_index_``, ``best_score_`` and ``best_params_``\ncorrespond to the scorer (key) that is set to the ``refit`` attribute.\n""""""\n\n# Author: Raghav RV <rvraghav93@gmail.com>\n# License: BSD\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_hastie_10_2\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nprint(__doc__)\n\n###############################################################################\n# Running ``GridSearchCV`` using multiple evaluation metrics\n# ----------------------------------------------------------\n#\n\nX, y = make_hastie_10_2(n_samples=8000, random_state=42)\n\n# The scorers can be either be one of the predefined metric strings or a scorer\n# callable, like the one returned by make_scorer\nscoring = {\'AUC\': \'roc_auc\', \'Accuracy\': make_scorer(accuracy_score)}\n\n# Setting refit=\'AUC\', refits an estimator on the whole dataset with the\n# parameter setting that has the best cross-validated AUC score.\n# That estimator is made available at ``gs.best_estimator_`` along with\n# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n# ``gs.best_index_``\ngs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid={\'min_samples_split\': range(2, 403, 10)},\n                  scoring=scoring, refit=\'AUC\', return_train_score=True)\ngs.fit(X, y)\nresults = gs.cv_results_\n\n###############################################################################\n# Plotting the result\n# -------------------\n\nplt.figure(figsize=(13, 13))\nplt.title(""GridSearchCV evaluating using multiple scorers simultaneously"",\n          fontsize=16)\n\nplt.xlabel(""min_samples_split"")\nplt.ylabel(""Score"")\n\nax = plt.gca()\nax.set_xlim(0, 402)\nax.set_ylim(0.73, 1)\n\n# Get the regular numpy array from the MaskedArray\nX_axis = np.array(results[\'param_min_samples_split\'].data, dtype=float)\n\nfor scorer, color in zip(sorted(scoring), [\'g\', \'k\']):\n    for sample, style in ((\'train\', \'--\'), (\'test\', \'-\')):\n        sample_score_mean = results[\'mean_%s_%s\' % (sample, scorer)]\n        sample_score_std = results[\'std_%s_%s\' % (sample, scorer)]\n        ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n                        sample_score_mean + sample_score_std,\n                        alpha=0.1 if sample == \'test\' else 0, color=color)\n        ax.plot(X_axis, sample_score_mean, style, color=color,\n                alpha=1 if sample == \'test\' else 0.7,\n                label=""%s (%s)"" % (scorer, sample))\n\n    best_index = np.nonzero(results[\'rank_test_%s\' % scorer] == 1)[0][0]\n    best_score = results[\'mean_test_%s\' % scorer][best_index]\n\n    # Plot a dotted vertical line at the best score for that scorer marked by x\n    ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n            linestyle=\'-.\', color=color, marker=\'x\', markeredgewidth=3, ms=8)\n\n    # Annotate the best score for that scorer\n    ax.annotate(""%0.2f"" % best_score,\n                (X_axis[best_index], best_score + 0.005))\n\nplt.legend(loc=""best"")\nplt.grid(False)\nplt.show()\n'"
scikit-learn/_downloads/0453686a7e6980396df404f320e2d839/plot_adaboost_regression.py,3,"b'""""""\n======================================\nDecision Tree Regression with AdaBoost\n======================================\n\nA decision tree is boosted using the AdaBoost.R2 [1]_ algorithm on a 1D\nsinusoidal dataset with a small amount of Gaussian noise.\n299 boosts (300 decision trees) is compared with a single decision tree\nregressor. As the number of boosts is increased the regressor can fit more\ndetail.\n\n.. [1] H. Drucker, ""Improving Regressors using Boosting Techniques"", 1997.\n\n""""""\nprint(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\n# importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Create the dataset\nrng = np.random.RandomState(1)\nX = np.linspace(0, 6, 100)[:, np.newaxis]\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=4)\n\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n                          n_estimators=300, random_state=rng)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c=""k"", label=""training samples"")\nplt.plot(X, y_1, c=""g"", label=""n_estimators=1"", linewidth=2)\nplt.plot(X, y_2, c=""r"", label=""n_estimators=300"", linewidth=2)\nplt.xlabel(""data"")\nplt.ylabel(""target"")\nplt.title(""Boosted Decision Tree Regression"")\nplt.legend()\nplt.show()\n'"
scikit-learn/_downloads/04661a84c5951b7ed3d7fa490759da97/plot_gmm_selection.py,13,"b'""""""\n================================\nGaussian Mixture Model Selection\n================================\n\nThis example shows that model selection can be performed with\nGaussian Mixture Models using information-theoretic criteria (BIC).\nModel selection concerns both the covariance type\nand the number of components in the model.\nIn that case, AIC also provides the right result (not shown to save time),\nbut BIC is better suited if the problem is to identify the right model.\nUnlike Bayesian procedures, such inferences are prior-free.\n\nIn that case, the model with 2 components and full covariance\n(which corresponds to the true generative model) is selected.\n""""""\n\nimport numpy as np\nimport itertools\n\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\nprint(__doc__)\n\n# Number of samples per component\nn_samples = 500\n\n# Generate random sample, two components\nnp.random.seed(0)\nC = np.array([[0., -0.1], [1.7, .4]])\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = [\'spherical\', \'tied\', \'diag\', \'full\']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a Gaussian mixture with EM\n        gmm = mixture.GaussianMixture(n_components=n_components,\n                                      covariance_type=cv_type)\n        gmm.fit(X)\n        bic.append(gmm.bic(X))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n\nbic = np.array(bic)\ncolor_iter = itertools.cycle([\'navy\', \'turquoise\', \'cornflowerblue\',\n                              \'darkorange\'])\nclf = best_gmm\nbars = []\n\n# Plot the BIC scores\nplt.figure(figsize=(8, 6))\nspl = plt.subplot(2, 1, 1)\nfor i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n    xpos = np.array(n_components_range) + .2 * (i - 2)\n    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n                                  (i + 1) * len(n_components_range)],\n                        width=.2, color=color))\nplt.xticks(n_components_range)\nplt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\nplt.title(\'BIC score per model\')\nxpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n    .2 * np.floor(bic.argmin() / len(n_components_range))\nplt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), \'*\', fontsize=14)\nspl.set_xlabel(\'Number of components\')\nspl.legend([b[0] for b in bars], cv_types)\n\n# Plot the winner\nsplot = plt.subplot(2, 1, 2)\nY_ = clf.predict(X)\nfor i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,\n                                           color_iter)):\n    v, w = linalg.eigh(cov)\n    if not np.any(Y_ == i):\n        continue\n    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n    # Plot an ellipse to show the Gaussian component\n    angle = np.arctan2(w[0][1], w[0][0])\n    angle = 180. * angle / np.pi  # convert to degrees\n    v = 2. * np.sqrt(2.) * np.sqrt(v)\n    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n    ell.set_clip_box(splot.bbox)\n    ell.set_alpha(.5)\n    splot.add_artist(ell)\n\nplt.xticks(())\nplt.yticks(())\nplt.title(\'Selected GMM: full model, 2 components\')\nplt.subplots_adjust(hspace=.35, bottom=.02)\nplt.show()\n'"
scikit-learn/_downloads/053f254dd90167f5b71cb4009a40b78d/plot_hashing_vs_dict_vectorizer.py,1,"b'""""""\n===========================================\nFeatureHasher and DictVectorizer Comparison\n===========================================\n\nCompares FeatureHasher and DictVectorizer by using both to vectorize\ntext documents.\n\nThe example demonstrates syntax and speed only; it doesn\'t actually do\nanything useful with the extracted vectors. See the example scripts\n{document_classification_20newsgroups,clustering}.py for actual learning\non text documents.\n\nA discrepancy between the number of terms reported for DictVectorizer and\nfor FeatureHasher is to be expected due to hash collisions.\n""""""\n\n# Author: Lars Buitinck\n# License: BSD 3 clause\nfrom collections import defaultdict\nimport re\nimport sys\nfrom time import time\n\nimport numpy as np\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction import DictVectorizer, FeatureHasher\n\n\ndef n_nonzero_columns(X):\n    """"""Returns the number of non-zero columns in a CSR matrix X.""""""\n    return len(np.unique(X.nonzero()[1]))\n\n\ndef tokens(doc):\n    """"""Extract tokens from doc.\n\n    This uses a simple regex to break strings into tokens. For a more\n    principled approach, see CountVectorizer or TfidfVectorizer.\n    """"""\n    return (tok.lower() for tok in re.findall(r""\\w+"", doc))\n\n\ndef token_freqs(doc):\n    """"""Extract a dict mapping tokens from doc to their frequencies.""""""\n    freq = defaultdict(int)\n    for tok in tokens(doc):\n        freq[tok] += 1\n    return freq\n\n\ncategories = [\n    \'alt.atheism\',\n    \'comp.graphics\',\n    \'comp.sys.ibm.pc.hardware\',\n    \'misc.forsale\',\n    \'rec.autos\',\n    \'sci.space\',\n    \'talk.religion.misc\',\n]\n# Uncomment the following line to use a larger set (11k+ documents)\n# categories = None\n\nprint(__doc__)\nprint(""Usage: %s [n_features_for_hashing]"" % sys.argv[0])\nprint(""    The default number of features is 2**18."")\nprint()\n\ntry:\n    n_features = int(sys.argv[1])\nexcept IndexError:\n    n_features = 2 ** 18\nexcept ValueError:\n    print(""not a valid number of features: %r"" % sys.argv[1])\n    sys.exit(1)\n\n\nprint(""Loading 20 newsgroups training data"")\nraw_data, _ = fetch_20newsgroups(subset=\'train\', categories=categories,\n                                 return_X_y=True)\ndata_size_mb = sum(len(s.encode(\'utf-8\')) for s in raw_data) / 1e6\nprint(""%d documents - %0.3fMB"" % (len(raw_data), data_size_mb))\nprint()\n\nprint(""DictVectorizer"")\nt0 = time()\nvectorizer = DictVectorizer()\nvectorizer.fit_transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\nprint(""done in %fs at %0.3fMB/s"" % (duration, data_size_mb / duration))\nprint(""Found %d unique terms"" % len(vectorizer.get_feature_names()))\nprint()\n\nprint(""FeatureHasher on frequency dicts"")\nt0 = time()\nhasher = FeatureHasher(n_features=n_features)\nX = hasher.transform(token_freqs(d) for d in raw_data)\nduration = time() - t0\nprint(""done in %fs at %0.3fMB/s"" % (duration, data_size_mb / duration))\nprint(""Found %d unique terms"" % n_nonzero_columns(X))\nprint()\n\nprint(""FeatureHasher on raw tokens"")\nt0 = time()\nhasher = FeatureHasher(n_features=n_features, input_type=""string"")\nX = hasher.transform(tokens(d) for d in raw_data)\nduration = time() - t0\nprint(""done in %fs at %0.3fMB/s"" % (duration, data_size_mb / duration))\nprint(""Found %d unique terms"" % n_nonzero_columns(X))\n'"
scikit-learn/_downloads/056da08a9040b7beb128bb03483bd278/plot_gradient_boosting_quantile.py,11,"b'""""""\n=====================================================\nPrediction Intervals for Gradient Boosting Regression\n=====================================================\n\nThis example shows how quantile regression can be used\nto create prediction intervals.\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nnp.random.seed(1)\n\n\ndef f(x):\n    """"""The function to predict.""""""\n    return x * np.sin(x)\n\n#----------------------------------------------------------------------\n#  First the noiseless case\nX = np.atleast_2d(np.random.uniform(0, 10.0, size=100)).T\nX = X.astype(np.float32)\n\n# Observations\ny = f(X).ravel()\n\ndy = 1.5 + 1.0 * np.random.random(y.shape)\nnoise = np.random.normal(0, dy)\ny += noise\ny = y.astype(np.float32)\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nxx = np.atleast_2d(np.linspace(0, 10, 1000)).T\nxx = xx.astype(np.float32)\n\nalpha = 0.95\n\nclf = GradientBoostingRegressor(loss=\'quantile\', alpha=alpha,\n                                n_estimators=250, max_depth=3,\n                                learning_rate=.1, min_samples_leaf=9,\n                                min_samples_split=9)\n\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_upper = clf.predict(xx)\n\nclf.set_params(alpha=1.0 - alpha)\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_lower = clf.predict(xx)\n\nclf.set_params(loss=\'ls\')\nclf.fit(X, y)\n\n# Make the prediction on the meshed x-axis\ny_pred = clf.predict(xx)\n\n# Plot the function, the prediction and the 90% confidence interval based on\n# the MSE\nfig = plt.figure()\nplt.plot(xx, f(xx), \'g:\', label=r\'$f(x) = x\\,\\sin(x)$\')\nplt.plot(X, y, \'b.\', markersize=10, label=u\'Observations\')\nplt.plot(xx, y_pred, \'r-\', label=u\'Prediction\')\nplt.plot(xx, y_upper, \'k-\')\nplt.plot(xx, y_lower, \'k-\')\nplt.fill(np.concatenate([xx, xx[::-1]]),\n         np.concatenate([y_upper, y_lower[::-1]]),\n         alpha=.5, fc=\'b\', ec=\'None\', label=\'90% prediction interval\')\nplt.xlabel(\'$x$\')\nplt.ylabel(\'$f(x)$\')\nplt.ylim(-10, 20)\nplt.legend(loc=\'upper left\')\nplt.show()\n'"
scikit-learn/_downloads/05c8e4ba69558b0f524c1190bd913917/plot_digits_linkage.py,5,"b'""""""\n=============================================================================\nVarious Agglomerative Clustering on a 2D embedding of digits\n=============================================================================\n\nAn illustration of various linkage option for agglomerative clustering on\na 2D embedding of the digits dataset.\n\nThe goal of this example is to show intuitively how the metrics behave, and\nnot to find good clusters for the digits. This is why the example works on a\n2D embedding.\n\nWhat this example shows us is the behavior ""rich getting richer"" of\nagglomerative clustering that tends to create uneven cluster sizes.\nThis behavior is pronounced for the average linkage strategy,\nthat ends up with a couple of singleton clusters, while in the case\nof single linkage we get a single central cluster with all other clusters\nbeing drawn from noise points around the fringes.\n""""""\n\n# Authors: Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2014\n\nprint(__doc__)\nfrom time import time\n\nimport numpy as np\nfrom scipy import ndimage\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import manifold, datasets\n\nX, y = datasets.load_digits(return_X_y=True)\nn_samples, n_features = X.shape\n\nnp.random.seed(0)\n\ndef nudge_images(X, y):\n    # Having a larger dataset shows more clearly the behavior of the\n    # methods, but we multiply the size of the dataset only by 2, as the\n    # cost of the hierarchical clustering methods are strongly\n    # super-linear in n_samples\n    shift = lambda x: ndimage.shift(x.reshape((8, 8)),\n                                  .3 * np.random.normal(size=2),\n                                  mode=\'constant\',\n                                  ).ravel()\n    X = np.concatenate([X, np.apply_along_axis(shift, 1, X)])\n    Y = np.concatenate([y, y], axis=0)\n    return X, Y\n\n\nX, y = nudge_images(X, y)\n\n\n#----------------------------------------------------------------------\n# Visualize the clustering\ndef plot_clustering(X_red, labels, title=None):\n    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n    X_red = (X_red - x_min) / (x_max - x_min)\n\n    plt.figure(figsize=(6, 4))\n    for i in range(X_red.shape[0]):\n        plt.text(X_red[i, 0], X_red[i, 1], str(y[i]),\n                 color=plt.cm.nipy_spectral(labels[i] / 10.),\n                 fontdict={\'weight\': \'bold\', \'size\': 9})\n\n    plt.xticks([])\n    plt.yticks([])\n    if title is not None:\n        plt.title(title, size=17)\n    plt.axis(\'off\')\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n#----------------------------------------------------------------------\n# 2D embedding of the digits dataset\nprint(""Computing embedding"")\nX_red = manifold.SpectralEmbedding(n_components=2).fit_transform(X)\nprint(""Done."")\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nfor linkage in (\'ward\', \'average\', \'complete\', \'single\'):\n    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=10)\n    t0 = time()\n    clustering.fit(X_red)\n    print(""%s :\\t%.2fs"" % (linkage, time() - t0))\n\n    plot_clustering(X_red, clustering.labels_, ""%s linkage"" % linkage)\n\n\nplt.show()\n'"
scikit-learn/_downloads/07c755a638b73aaeb8a366766642818c/plot_iris_dataset.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nThe Iris Dataset\n=========================================================\nThis data sets consists of 3 different types of irises\'\n(Setosa, Versicolour, and Virginica) petal and sepal\nlength, stored in a 150x4 numpy.ndarray\n\nThe rows being the samples and the columns being:\nSepal Length, Sepal Width, Petal Length and Petal Width.\n\nThe below plot uses the first two features.\nSee `here <https://en.wikipedia.org/wiki/Iris_flower_data_set>`_ for more\ninformation on this dataset.\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\ny = iris.target\n\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n\nplt.figure(2, figsize=(8, 6))\nplt.clf()\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,\n            edgecolor=\'k\')\nplt.xlabel(\'Sepal length\')\nplt.ylabel(\'Sepal width\')\n\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\n\n# To getter a better understanding of interaction of the dimensions\n# plot the first three PCA dimensions\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\nX_reduced = PCA(n_components=3).fit_transform(iris.data)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=y,\n           cmap=plt.cm.Set1, edgecolor=\'k\', s=40)\nax.set_title(""First three PCA directions"")\nax.set_xlabel(""1st eigenvector"")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(""2nd eigenvector"")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(""3rd eigenvector"")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()\n'"
scikit-learn/_downloads/08a963c28efcfdb0ed734395c8a935a5/plot_polynomial_interpolation.py,7,"b'#!/usr/bin/env python\n""""""\n========================\nPolynomial interpolation\n========================\n\nThis example demonstrates how to approximate a function with a polynomial of\ndegree n_degree by using ridge regression. Concretely, from n_samples 1d\npoints, it suffices to build the Vandermonde matrix, which is n_samples x\nn_degree+1 and has the following form:\n\n[[1, x_1, x_1 ** 2, x_1 ** 3, ...],\n [1, x_2, x_2 ** 2, x_2 ** 3, ...],\n ...]\n\nIntuitively, this matrix can be interpreted as a matrix of pseudo features (the\npoints raised to some power). The matrix is akin to (but different from) the\nmatrix induced by a polynomial kernel.\n\nThis example shows that you can do non-linear regression with a linear model,\nusing a pipeline to add non-linear features. Kernel methods extend this idea\nand can induce very high (even infinite) dimensional feature spaces.\n""""""\nprint(__doc__)\n\n# Author: Mathieu Blondel\n#         Jake Vanderplas\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\ndef f(x):\n    """""" function to approximate by polynomial interpolation""""""\n    return x * np.sin(x)\n\n\n# generate points used to plot\nx_plot = np.linspace(0, 10, 100)\n\n# generate points and keep a subset of them\nx = np.linspace(0, 10, 100)\nrng = np.random.RandomState(0)\nrng.shuffle(x)\nx = np.sort(x[:20])\ny = f(x)\n\n# create matrix versions of these arrays\nX = x[:, np.newaxis]\nX_plot = x_plot[:, np.newaxis]\n\ncolors = [\'teal\', \'yellowgreen\', \'gold\']\nlw = 2\nplt.plot(x_plot, f(x_plot), color=\'cornflowerblue\', linewidth=lw,\n         label=""ground truth"")\nplt.scatter(x, y, color=\'navy\', s=30, marker=\'o\', label=""training points"")\n\nfor count, degree in enumerate([3, 4, 5]):\n    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n    model.fit(X, y)\n    y_plot = model.predict(X_plot)\n    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw,\n             label=""degree %d"" % degree)\n\nplt.legend(loc=\'lower left\')\n\nplt.show()\n'"
scikit-learn/_downloads/094cf0946e5e4656e0668f8affb47dc3/plot_agglomerative_clustering_metrics.py,11,"b'""""""\nAgglomerative clustering with different metrics\n===============================================\n\nDemonstrates the effect of different metrics on the hierarchical clustering.\n\nThe example is engineered to show the effect of the choice of different\nmetrics. It is applied to waveforms, which can be seen as\nhigh-dimensional vector. Indeed, the difference between metrics is\nusually more pronounced in high dimension (in particular for euclidean\nand cityblock).\n\nWe generate data from three groups of waveforms. Two of the waveforms\n(waveform 1 and waveform 2) are proportional one to the other. The cosine\ndistance is invariant to a scaling of the data, as a result, it cannot\ndistinguish these two waveforms. Thus even with no noise, clustering\nusing this distance will not separate out waveform 1 and 2.\n\nWe add observation noise to these waveforms. We generate very sparse\nnoise: only 6% of the time points contain noise. As a result, the\nl1 norm of this noise (ie ""cityblock"" distance) is much smaller than it\'s\nl2 norm (""euclidean"" distance). This can be seen on the inter-class\ndistance matrices: the values on the diagonal, that characterize the\nspread of the class, are much bigger for the Euclidean distance than for\nthe cityblock distance.\n\nWhen we apply clustering to the data, we find that the clustering\nreflects what was in the distance matrices. Indeed, for the Euclidean\ndistance, the classes are ill-separated because of the noise, and thus\nthe clustering does not separate the waveforms. For the cityblock\ndistance, the separation is good and the waveform classes are recovered.\nFinally, the cosine distance does not separate at all waveform 1 and 2,\nthus the clustering puts them in the same cluster.\n""""""\n# Author: Gael Varoquaux\n# License: BSD 3-Clause or CC-0\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import pairwise_distances\n\nnp.random.seed(0)\n\n# Generate waveform data\nn_features = 2000\nt = np.pi * np.linspace(0, 1, n_features)\n\n\ndef sqr(x):\n    return np.sign(np.cos(x))\n\nX = list()\ny = list()\nfor i, (phi, a) in enumerate([(.5, .15), (.5, .6), (.3, .2)]):\n    for _ in range(30):\n        phase_noise = .01 * np.random.normal()\n        amplitude_noise = .04 * np.random.normal()\n        additional_noise = 1 - 2 * np.random.rand(n_features)\n        # Make the noise sparse\n        additional_noise[np.abs(additional_noise) < .997] = 0\n\n        X.append(12 * ((a + amplitude_noise)\n                 * (sqr(6 * (t + phi + phase_noise)))\n                 + additional_noise))\n        y.append(i)\n\nX = np.array(X)\ny = np.array(y)\n\nn_clusters = 3\n\nlabels = (\'Waveform 1\', \'Waveform 2\', \'Waveform 3\')\n\n# Plot the ground-truth labelling\nplt.figure()\nplt.axes([0, 0, 1, 1])\nfor l, c, n in zip(range(n_clusters), \'rgb\',\n                   labels):\n    lines = plt.plot(X[y == l].T, c=c, alpha=.5)\n    lines[0].set_label(n)\n\nplt.legend(loc=\'best\')\n\nplt.axis(\'tight\')\nplt.axis(\'off\')\nplt.suptitle(""Ground truth"", size=20)\n\n\n# Plot the distances\nfor index, metric in enumerate([""cosine"", ""euclidean"", ""cityblock""]):\n    avg_dist = np.zeros((n_clusters, n_clusters))\n    plt.figure(figsize=(5, 4.5))\n    for i in range(n_clusters):\n        for j in range(n_clusters):\n            avg_dist[i, j] = pairwise_distances(X[y == i], X[y == j],\n                                                metric=metric).mean()\n    avg_dist /= avg_dist.max()\n    for i in range(n_clusters):\n        for j in range(n_clusters):\n            plt.text(i, j, \'%5.3f\' % avg_dist[i, j],\n                     verticalalignment=\'center\',\n                     horizontalalignment=\'center\')\n\n    plt.imshow(avg_dist, interpolation=\'nearest\', cmap=plt.cm.gnuplot2,\n               vmin=0)\n    plt.xticks(range(n_clusters), labels, rotation=45)\n    plt.yticks(range(n_clusters), labels)\n    plt.colorbar()\n    plt.suptitle(""Interclass %s distances"" % metric, size=18)\n    plt.tight_layout()\n\n\n# Plot clustering results\nfor index, metric in enumerate([""cosine"", ""euclidean"", ""cityblock""]):\n    model = AgglomerativeClustering(n_clusters=n_clusters,\n                                    linkage=""average"", affinity=metric)\n    model.fit(X)\n    plt.figure()\n    plt.axes([0, 0, 1, 1])\n    for l, c in zip(np.arange(model.n_clusters), \'rgbk\'):\n        plt.plot(X[model.labels_ == l].T, c=c, alpha=.5)\n    plt.axis(\'tight\')\n    plt.axis(\'off\')\n    plt.suptitle(""AgglomerativeClustering(affinity=%s)"" % metric, size=20)\n\n\nplt.show()\n'"
scikit-learn/_downloads/0979c75ff8197714e853b351bed4cb00/plot_digits_classification.py,0,"b'""""""\n================================\nRecognizing hand-written digits\n================================\n\nAn example showing how the scikit-learn can be used to recognize images of\nhand-written digits.\n\nThis example is commented in the\n:ref:`tutorial section of the user manual <introduction>`.\n\n""""""\nprint(__doc__)\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n# License: BSD 3 clause\n\n# Standard scientific Python imports\nimport matplotlib.pyplot as plt\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, metrics\nfrom sklearn.model_selection import train_test_split\n\n# The digits dataset\ndigits = datasets.load_digits()\n\n# The data that we are interested in is made of 8x8 images of digits, let\'s\n# have a look at the first 4 images, stored in the `images` attribute of the\n# dataset.  If we were working from image files, we could load them using\n# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n# images, we know which digit they represent: it is given in the \'target\' of\n# the dataset.\n_, axes = plt.subplots(2, 4)\nimages_and_labels = list(zip(digits.images, digits.target))\nfor ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\'nearest\')\n    ax.set_title(\'Training: %i\' % label)\n\n# To apply a classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create a classifier: a support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# Split data into train and test subsets\nX_train, X_test, y_train, y_test = train_test_split(\n    data, digits.target, test_size=0.5, shuffle=False)\n\n# We learn the digits on the first half of the digits\nclassifier.fit(X_train, y_train)\n\n# Now predict the value of the digit on the second half:\npredicted = classifier.predict(X_test)\n\nimages_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\nfor ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n    ax.set_axis_off()\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\'nearest\')\n    ax.set_title(\'Prediction: %i\' % prediction)\n\nprint(""Classification report for classifier %s:\\n%s\\n""\n      % (classifier, metrics.classification_report(y_test, predicted)))\ndisp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\ndisp.figure_.suptitle(""Confusion Matrix"")\nprint(""Confusion matrix:\\n%s"" % disp.confusion_matrix)\n\nplt.show()\n'"
scikit-learn/_downloads/09922b136772564ee5093bad09e136a7/plot_gradient_boosting_early_stopping.py,4,"b'""""""\n===================================\nEarly stopping of Gradient Boosting\n===================================\n\nGradient boosting is an ensembling technique where several weak learners\n(regression trees) are combined to yield a powerful single model, in an\niterative fashion.\n\nEarly stopping support in Gradient Boosting enables us to find the least number\nof iterations which is sufficient to build a model that generalizes well to\nunseen data.\n\nThe concept of early stopping is simple. We specify a ``validation_fraction``\nwhich denotes the fraction of the whole dataset that will be kept aside from\ntraining to assess the validation loss of the model. The gradient boosting\nmodel is trained using the training set and evaluated using the validation set.\nWhen each additional stage of regression tree is added, the validation set is\nused to score the model.  This is continued until the scores of the model in\nthe last ``n_iter_no_change`` stages do not improve by atleast `tol`. After\nthat the model is considered to have converged and further addition of stages\nis ""stopped early"".\n\nThe number of stages of the final model is available at the attribute\n``n_estimators_``.\n\nThis example illustrates how the early stopping can used in the\n:class:`sklearn.ensemble.GradientBoostingClassifier` model to achieve\nalmost the same accuracy as compared to a model built without early stopping\nusing many fewer estimators. This can significantly reduce training time,\nmemory usage and prediction latency.\n""""""\n\n# Authors: Vighnesh Birodkar <vighneshbirodkar@nyu.edu>\n#          Raghav RV <rvraghav93@gmail.com>\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\nprint(__doc__)\n\ndata_list = [datasets.load_iris(), datasets.load_digits()]\ndata_list = [(d.data, d.target) for d in data_list]\ndata_list += [datasets.make_hastie_10_2()]\nnames = [\'Iris Data\', \'Digits Data\', \'Hastie Data\']\n\nn_gb = []\nscore_gb = []\ntime_gb = []\nn_gbes = []\nscore_gbes = []\ntime_gbes = []\n\nn_estimators = 500\n\nfor X, y in data_list:\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                        random_state=0)\n\n    # We specify that if the scores don\'t improve by atleast 0.01 for the last\n    # 10 stages, stop fitting additional stages\n    gbes = ensemble.GradientBoostingClassifier(n_estimators=n_estimators,\n                                               validation_fraction=0.2,\n                                               n_iter_no_change=5, tol=0.01,\n                                               random_state=0)\n    gb = ensemble.GradientBoostingClassifier(n_estimators=n_estimators,\n                                             random_state=0)\n    start = time.time()\n    gb.fit(X_train, y_train)\n    time_gb.append(time.time() - start)\n\n    start = time.time()\n    gbes.fit(X_train, y_train)\n    time_gbes.append(time.time() - start)\n\n    score_gb.append(gb.score(X_test, y_test))\n    score_gbes.append(gbes.score(X_test, y_test))\n\n    n_gb.append(gb.n_estimators_)\n    n_gbes.append(gbes.n_estimators_)\n\nbar_width = 0.2\nn = len(data_list)\nindex = np.arange(0, n * bar_width, bar_width) * 2.5\nindex = index[0:n]\n\n#######################################################################\n# Compare scores with and without early stopping\n# ----------------------------------------------\n\nplt.figure(figsize=(9, 5))\n\nbar1 = plt.bar(index, score_gb, bar_width, label=\'Without early stopping\',\n               color=\'crimson\')\nbar2 = plt.bar(index + bar_width, score_gbes, bar_width,\n               label=\'With early stopping\', color=\'coral\')\n\nplt.xticks(index + bar_width, names)\nplt.yticks(np.arange(0, 1.3, 0.1))\n\n\ndef autolabel(rects, n_estimators):\n    """"""\n    Attach a text label above each bar displaying n_estimators of each model\n    """"""\n    for i, rect in enumerate(rects):\n        plt.text(rect.get_x() + rect.get_width() / 2.,\n                 1.05 * rect.get_height(), \'n_est=%d\' % n_estimators[i],\n                 ha=\'center\', va=\'bottom\')\n\n\nautolabel(bar1, n_gb)\nautolabel(bar2, n_gbes)\n\nplt.ylim([0, 1.3])\nplt.legend(loc=\'best\')\nplt.grid(True)\n\nplt.xlabel(\'Datasets\')\nplt.ylabel(\'Test score\')\n\nplt.show()\n\n\n#######################################################################\n# Compare fit times with and without early stopping\n# -------------------------------------------------\n\nplt.figure(figsize=(9, 5))\n\nbar1 = plt.bar(index, time_gb, bar_width, label=\'Without early stopping\',\n               color=\'crimson\')\nbar2 = plt.bar(index + bar_width, time_gbes, bar_width,\n               label=\'With early stopping\', color=\'coral\')\n\nmax_y = np.amax(np.maximum(time_gb, time_gbes))\n\nplt.xticks(index + bar_width, names)\nplt.yticks(np.linspace(0, 1.3 * max_y, 13))\n\nautolabel(bar1, n_gb)\nautolabel(bar2, n_gbes)\n\nplt.ylim([0, 1.3 * max_y])\nplt.legend(loc=\'best\')\nplt.grid(True)\n\nplt.xlabel(\'Datasets\')\nplt.ylabel(\'Fit Time\')\n\nplt.show()\n'"
scikit-learn/_downloads/09a0aed667e1a7ad827737801bb81bda/plot_calibration_curve.py,0,"b'""""""\n==============================\nProbability Calibration curves\n==============================\n\nWhen performing classification one often wants to predict not only the class\nlabel, but also the associated probability. This probability gives some\nkind of confidence on the prediction. This example demonstrates how to display\nhow well calibrated the predicted probabilities are and how to calibrate an\nuncalibrated classifier.\n\nThe experiment is performed on an artificial dataset for binary classification\nwith 100,000 samples (1,000 of them are used for model fitting) with 20\nfeatures. Of the 20 features, only 2 are informative and 10 are redundant. The\nfirst figure shows the estimated probabilities obtained with logistic\nregression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic\ncalibration and sigmoid calibration. The calibration performance is evaluated\nwith Brier score, reported in the legend (the smaller the better). One can\nobserve here that logistic regression is well calibrated while raw Gaussian\nnaive Bayes performs very badly. This is because of the redundant features\nwhich violate the assumption of feature-independence and result in an overly\nconfident classifier, which is indicated by the typical transposed-sigmoid\ncurve.\n\nCalibration of the probabilities of Gaussian naive Bayes with isotonic\nregression can fix this issue as can be seen from the nearly diagonal\ncalibration curve. Sigmoid calibration also improves the brier score slightly,\nalbeit not as strongly as the non-parametric isotonic regression. This can be\nattributed to the fact that we have plenty of calibration data such that the\ngreater flexibility of the non-parametric model can be exploited.\n\nThe second figure shows the calibration curve of a linear support-vector\nclassifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian\nnaive Bayes: the calibration curve has a sigmoid curve, which is typical for\nan under-confident classifier. In the case of LinearSVC, this is caused by the\nmargin property of the hinge loss, which lets the model focus on hard samples\nthat are close to the decision boundary (the support vectors).\n\nBoth kinds of calibration can fix this issue and yield nearly identical\nresults. This shows that sigmoid calibration can deal with situations where\nthe calibration curve of the base classifier is sigmoid (e.g., for LinearSVC)\nbut not where it is transposed-sigmoid (e.g., Gaussian naive Bayes).\n""""""\nprint(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n                             f1_score)\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\nfrom sklearn.model_selection import train_test_split\n\n\n# Create dataset of classification task with many redundant and few\n# informative features\nX, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=2, n_redundant=10,\n                                    random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,\n                                                    random_state=42)\n\n\ndef plot_calibration_curve(est, name, fig_index):\n    """"""Plot calibration curve for est w/o and with calibration. """"""\n    # Calibrated with isotonic calibration\n    isotonic = CalibratedClassifierCV(est, cv=2, method=\'isotonic\')\n\n    # Calibrated with sigmoid calibration\n    sigmoid = CalibratedClassifierCV(est, cv=2, method=\'sigmoid\')\n\n    # Logistic regression with no calibration as baseline\n    lr = LogisticRegression(C=1.)\n\n    fig = plt.figure(fig_index, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n\n    ax1.plot([0, 1], [0, 1], ""k:"", label=""Perfectly calibrated"")\n    for clf, name in [(lr, \'Logistic\'),\n                      (est, name),\n                      (isotonic, name + \' + Isotonic\'),\n                      (sigmoid, name + \' + Sigmoid\')]:\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        if hasattr(clf, ""predict_proba""):\n            prob_pos = clf.predict_proba(X_test)[:, 1]\n        else:  # use decision function\n            prob_pos = clf.decision_function(X_test)\n            prob_pos = \\\n                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n\n        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n        print(""%s:"" % name)\n        print(""\\tBrier: %1.3f"" % (clf_score))\n        print(""\\tPrecision: %1.3f"" % precision_score(y_test, y_pred))\n        print(""\\tRecall: %1.3f"" % recall_score(y_test, y_pred))\n        print(""\\tF1: %1.3f\\n"" % f1_score(y_test, y_pred))\n\n        fraction_of_positives, mean_predicted_value = \\\n            calibration_curve(y_test, prob_pos, n_bins=10)\n\n        ax1.plot(mean_predicted_value, fraction_of_positives, ""s-"",\n                 label=""%s (%1.3f)"" % (name, clf_score))\n\n        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n                 histtype=""step"", lw=2)\n\n    ax1.set_ylabel(""Fraction of positives"")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc=""lower right"")\n    ax1.set_title(\'Calibration plots  (reliability curve)\')\n\n    ax2.set_xlabel(""Mean predicted value"")\n    ax2.set_ylabel(""Count"")\n    ax2.legend(loc=""upper center"", ncol=2)\n\n    plt.tight_layout()\n\n# Plot calibration curve for Gaussian Naive Bayes\nplot_calibration_curve(GaussianNB(), ""Naive Bayes"", 1)\n\n# Plot calibration curve for Linear SVC\nplot_calibration_curve(LinearSVC(max_iter=10000), ""SVC"", 2)\n\nplt.show()\n'"
scikit-learn/_downloads/0a62843c477a1862d3bebb9e2e270fd7/plot_roc.py,4,"b'""""""\n=======================================\nReceiver Operating Characteristic (ROC)\n=======================================\n\nExample of Receiver Operating Characteristic (ROC) metric to evaluate\nclassifier output quality.\n\nROC curves typically feature true positive rate on the Y axis, and false\npositive rate on the X axis. This means that the top left corner of the plot is\nthe ""ideal"" point - a false positive rate of zero, and a true positive rate of\none. This is not very realistic, but it does mean that a larger area under the\ncurve (AUC) is usually better.\n\nThe ""steepness"" of ROC curves is also important, since it is ideal to maximize\nthe true positive rate while minimizing the false positive rate.\n\nROC curves are typically used in binary classification to study the output of\na classifier. In order to extend ROC curve and ROC area to multi-label\nclassification, it is necessary to binarize the output. One ROC\ncurve can be drawn per label, but one can also draw a ROC curve by considering\neach element of the label indicator matrix as a binary prediction\n(micro-averaging).\n\nAnother evaluation measure for multi-label classification is\nmacro-averaging, which gives equal weight to the classification of each\nlabel.\n\n.. note::\n\n    See also :func:`sklearn.metrics.roc_auc_score`,\n             :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn.metrics import roc_auc_score\n\n# Import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Binarize the output\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n# Add noisy features to make the problem harder\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# shuffle and split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n                                                    random_state=0)\n\n# Learn to predict each class against the other\nclassifier = OneVsRestClassifier(svm.SVC(kernel=\'linear\', probability=True,\n                                 random_state=random_state))\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])\n\n\n##############################################################################\n# Plot of a ROC curve for a specific class\nplt.figure()\nlw = 2\nplt.plot(fpr[2], tpr[2], color=\'darkorange\',\n         lw=lw, label=\'ROC curve (area = %0.2f)\' % roc_auc[2])\nplt.plot([0, 1], [0, 1], color=\'navy\', lw=lw, linestyle=\'--\')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Receiver operating characteristic example\')\nplt.legend(loc=""lower right"")\nplt.show()\n\n\n##############################################################################\n# Plot ROC curves for the multilabel problem\n# ..........................................\n# Compute macro-average ROC curve and ROC area\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[""macro""] = all_fpr\ntpr[""macro""] = mean_tpr\nroc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr[""micro""], tpr[""micro""],\n         label=\'micro-average ROC curve (area = {0:0.2f})\'\n               \'\'.format(roc_auc[""micro""]),\n         color=\'deeppink\', linestyle=\':\', linewidth=4)\n\nplt.plot(fpr[""macro""], tpr[""macro""],\n         label=\'macro-average ROC curve (area = {0:0.2f})\'\n               \'\'.format(roc_auc[""macro""]),\n         color=\'navy\', linestyle=\':\', linewidth=4)\n\ncolors = cycle([\'aqua\', \'darkorange\', \'cornflowerblue\'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label=\'ROC curve of class {0} (area = {1:0.2f})\'\n             \'\'.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], \'k--\', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\'False Positive Rate\')\nplt.ylabel(\'True Positive Rate\')\nplt.title(\'Some extension of Receiver operating characteristic to multi-class\')\nplt.legend(loc=""lower right"")\nplt.show()\n\n\n##############################################################################\n# Area under ROC for the multiclass problem\n# .........................................\n# The :func:`sklearn.metrics.roc_auc_score` function can be used for\n# multi-class classification. The multi-class One-vs-One scheme compares every\n# unique pairwise combination of classes. In this section, we calcuate the AUC\n# using the OvR and OvO schemes. We report a macro average, and a\n# prevalence-weighted average.\ny_prob = classifier.predict_proba(X_test)\n\nmacro_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=""ovo"",\n                                  average=""macro"")\nweighted_roc_auc_ovo = roc_auc_score(y_test, y_prob, multi_class=""ovo"",\n                                     average=""weighted"")\nmacro_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=""ovr"",\n                                  average=""macro"")\nweighted_roc_auc_ovr = roc_auc_score(y_test, y_prob, multi_class=""ovr"",\n                                     average=""weighted"")\nprint(""One-vs-One ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} ""\n      ""(weighted by prevalence)""\n      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))\nprint(""One-vs-Rest ROC AUC scores:\\n{:.6f} (macro),\\n{:.6f} ""\n      ""(weighted by prevalence)""\n      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))\n'"
scikit-learn/_downloads/0a8f657023610c6d6442af282c096fe9/plot_mlp_alpha.py,6,"b'""""""\n================================================\nVarying regularization in Multi-layer Perceptron\n================================================\n\nA comparison of different values for regularization parameter \'alpha\' on\nsynthetic datasets. The plot shows that different alphas yield different\ndecision functions.\n\nAlpha is a parameter for regularization term, aka penalty term, that combats\noverfitting by constraining the size of the weights. Increasing alpha may fix\nhigh variance (a sign of overfitting) by encouraging smaller weights, resulting\nin a decision boundary plot that appears with lesser curvatures.\nSimilarly, decreasing alpha may fix high bias (a sign of underfitting) by\nencouraging larger weights, potentially resulting in a more complicated\ndecision boundary.\n""""""\nprint(__doc__)\n\n\n# Author: Issam H. Laradji\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\n\nh = .02  # step size in the mesh\n\nalphas = np.logspace(-5, 3, 5)\nnames = [\'alpha \' + str(i) for i in alphas]\n\nclassifiers = []\nfor i in alphas:\n    classifiers.append(MLPClassifier(solver=\'lbfgs\', alpha=i, random_state=1,\n                                     hidden_layer_sizes=[100, 100]))\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=0, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable]\n\nfigure = plt.figure(figsize=(17, 9))\ni = 1\n# iterate over datasets\nfor X, y in datasets:\n    # preprocess dataset, split into training and test part\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap([\'#FF0000\', \'#0000FF\'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright)\n    # and testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, ""decision_function""):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot also the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors=\'black\', s=25)\n        # and testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   alpha=0.6, edgecolors=\'black\', s=25)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, (\'%.2f\' % score).lstrip(\'0\'),\n                size=15, horizontalalignment=\'right\')\n        i += 1\n\nfigure.subplots_adjust(left=.02, right=.98)\nplt.show()\n'"
scikit-learn/_downloads/0b211c88bccf2c8a2de312cab3610015/plot_species_kde.py,11,"b'""""""\n================================================\nKernel Density Estimate of Species Distributions\n================================================\nThis shows an example of a neighbors-based query (in particular a kernel\ndensity estimate) on geospatial data, using a Ball Tree built upon the\nHaversine distance metric -- i.e. distances over points in latitude/longitude.\nThe dataset is provided by Phillips et. al. (2006).\nIf available, the example uses\n`basemap <https://matplotlib.org/basemap/>`_\nto plot the coast lines and national boundaries of South America.\n\nThis example does not perform any learning over the data\n(see :ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py` for\nan example of classification based on the attributes in this dataset).  It\nsimply shows the kernel density estimate of observed data points in\ngeospatial coordinates.\n\nThe two species are:\n\n - `""Bradypus variegatus""\n   <http://www.iucnredlist.org/apps/redlist/details/3038/0>`_ ,\n   the Brown-throated Sloth.\n\n - `""Microryzomys minutus""\n   <http://www.iucnredlist.org/details/13408/0>`_ ,\n   also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n   Colombia, Ecuador, Peru, and Venezuela.\n\nReferences\n----------\n\n * `""Maximum entropy modeling of species geographic distributions""\n   <http://rob.schapire.net/papers/ecolmod.pdf>`_\n   S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n   190:231-259, 2006.\n""""""\n# Author: Jake Vanderplas <jakevdp@cs.washington.edu>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_species_distributions\nfrom sklearn.neighbors import KernelDensity\n\n# if basemap is available, we\'ll use it.\n# otherwise, we\'ll improvise later...\ntry:\n    from mpl_toolkits.basemap import Basemap\n    basemap = True\nexcept ImportError:\n    basemap = False\n\n\ndef construct_grids(batch):\n    """"""Construct the map grid from the batch object\n\n    Parameters\n    ----------\n    batch : Batch object\n        The object returned by :func:`fetch_species_distributions`\n\n    Returns\n    -------\n    (xgrid, ygrid) : 1-D arrays\n        The grid corresponding to the values in batch.coverages\n    """"""\n    # x,y coordinates for corner cells\n    xmin = batch.x_left_lower_corner + batch.grid_size\n    xmax = xmin + (batch.Nx * batch.grid_size)\n    ymin = batch.y_left_lower_corner + batch.grid_size\n    ymax = ymin + (batch.Ny * batch.grid_size)\n\n    # x coordinates of the grid cells\n    xgrid = np.arange(xmin, xmax, batch.grid_size)\n    # y coordinates of the grid cells\n    ygrid = np.arange(ymin, ymax, batch.grid_size)\n\n    return (xgrid, ygrid)\n\n\n# Get matrices/arrays of species IDs and locations\ndata = fetch_species_distributions()\nspecies_names = [\'Bradypus Variegatus\', \'Microryzomys Minutus\']\n\nXtrain = np.vstack([data[\'train\'][\'dd lat\'],\n                    data[\'train\'][\'dd long\']]).T\nytrain = np.array([d.decode(\'ascii\').startswith(\'micro\')\n                  for d in data[\'train\'][\'species\']], dtype=\'int\')\nXtrain *= np.pi / 180.  # Convert lat/long to radians\n\n# Set up the data grid for the contour plot\nxgrid, ygrid = construct_grids(data)\nX, Y = np.meshgrid(xgrid[::5], ygrid[::5][::-1])\nland_reference = data.coverages[6][::5, ::5]\nland_mask = (land_reference > -9999).ravel()\n\nxy = np.vstack([Y.ravel(), X.ravel()]).T\nxy = xy[land_mask]\nxy *= np.pi / 180.\n\n# Plot map of South America with distributions of each species\nfig = plt.figure()\nfig.subplots_adjust(left=0.05, right=0.95, wspace=0.05)\n\nfor i in range(2):\n    plt.subplot(1, 2, i + 1)\n\n    # construct a kernel density estimate of the distribution\n    print("" - computing KDE in spherical coordinates"")\n    kde = KernelDensity(bandwidth=0.04, metric=\'haversine\',\n                        kernel=\'gaussian\', algorithm=\'ball_tree\')\n    kde.fit(Xtrain[ytrain == i])\n\n    # evaluate only on the land: -9999 indicates ocean\n    Z = np.full(land_mask.shape[0], -9999, dtype=\'int\')\n    Z[land_mask] = np.exp(kde.score_samples(xy))\n    Z = Z.reshape(X.shape)\n\n    # plot contours of the density\n    levels = np.linspace(0, Z.max(), 25)\n    plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n\n    if basemap:\n        print("" - plot coastlines using basemap"")\n        m = Basemap(projection=\'cyl\', llcrnrlat=Y.min(),\n                    urcrnrlat=Y.max(), llcrnrlon=X.min(),\n                    urcrnrlon=X.max(), resolution=\'c\')\n        m.drawcoastlines()\n        m.drawcountries()\n    else:\n        print("" - plot coastlines from coverage"")\n        plt.contour(X, Y, land_reference,\n                    levels=[-9998], colors=""k"",\n                    linestyles=""solid"")\n        plt.xticks([])\n        plt.yticks([])\n\n    plt.title(species_names[i])\n\nplt.show()\n'"
scikit-learn/_downloads/0bfbf531803adec2e1bd5815fb425973/plot_grid_search_refit_callable.py,3,"b'""""""\n==================================================\nBalance model complexity and cross-validated score\n==================================================\n\nThis example balances model complexity and cross-validated score by\nfinding a decent accuracy within 1 standard deviation of the best accuracy\nscore while minimising the number of PCA components [1].\n\nThe figure shows the trade-off between cross-validated score and the number\nof PCA components. The balanced case is when n_components=10 and accuracy=0.88,\nwhich falls into the range within 1 standard deviation of the best accuracy\nscore.\n\n[1] Hastie, T., Tibshirani, R.,, Friedman, J. (2001). Model Assessment and\nSelection. The Elements of Statistical Learning (pp. 219-260). New York,\nNY, USA: Springer New York Inc..\n""""""\n# Author: Wenhao Zhang <wenhaoz@ucla.edu>\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\n\n\ndef lower_bound(cv_results):\n    """"""\n    Calculate the lower bound within 1 standard deviation\n    of the best `mean_test_scores`.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy(masked) ndarrays\n        See attribute cv_results_ of `GridSearchCV`\n\n    Returns\n    -------\n    float\n        Lower bound within 1 standard deviation of the\n        best `mean_test_score`.\n    """"""\n    best_score_idx = np.argmax(cv_results[\'mean_test_score\'])\n\n    return (cv_results[\'mean_test_score\'][best_score_idx]\n            - cv_results[\'std_test_score\'][best_score_idx])\n\n\ndef best_low_complexity(cv_results):\n    """"""\n    Balance model complexity with cross-validated score.\n\n    Parameters\n    ----------\n    cv_results : dict of numpy(masked) ndarrays\n        See attribute cv_results_ of `GridSearchCV`.\n\n    Return\n    ------\n    int\n        Index of a model that has the fewest PCA components\n        while has its test score within 1 standard deviation of the best\n        `mean_test_score`.\n    """"""\n    threshold = lower_bound(cv_results)\n    candidate_idx = np.flatnonzero(cv_results[\'mean_test_score\'] >= threshold)\n    best_idx = candidate_idx[cv_results[\'param_reduce_dim__n_components\']\n                             [candidate_idx].argmin()]\n    return best_idx\n\n\npipe = Pipeline([\n        (\'reduce_dim\', PCA(random_state=42)),\n        (\'classify\', LinearSVC(random_state=42, C=0.01)),\n])\n\nparam_grid = {\n    \'reduce_dim__n_components\': [6, 8, 10, 12, 14]\n}\n\ngrid = GridSearchCV(pipe, cv=10, n_jobs=1, param_grid=param_grid,\n                    scoring=\'accuracy\', refit=best_low_complexity)\nX, y = load_digits(return_X_y=True)\ngrid.fit(X, y)\n\nn_components = grid.cv_results_[\'param_reduce_dim__n_components\']\ntest_scores = grid.cv_results_[\'mean_test_score\']\n\nplt.figure()\nplt.bar(n_components, test_scores, width=1.3, color=\'b\')\n\nlower = lower_bound(grid.cv_results_)\nplt.axhline(np.max(test_scores), linestyle=\'--\', color=\'y\',\n            label=\'Best score\')\nplt.axhline(lower, linestyle=\'--\', color=\'.5\', label=\'Best score - 1 std\')\n\nplt.title(""Balance model complexity and cross-validated score"")\nplt.xlabel(\'Number of PCA components used\')\nplt.ylabel(\'Digit classification accuracy\')\nplt.xticks(n_components.tolist())\nplt.ylim((0, 1.0))\nplt.legend(loc=\'upper left\')\n\nbest_index_ = grid.best_index_\n\nprint(""The best_index_ is %d"" % best_index_)\nprint(""The n_components selected is %d"" % n_components[best_index_])\nprint(""The corresponding accuracy score is %.2f""\n      % grid.cv_results_[\'mean_test_score\'][best_index_])\nplt.show()\n'"
scikit-learn/_downloads/0c59b87937b88d30cac6d6dab4f0b8f6/plot_feature_selection.py,6,"b'""""""\n============================\nUnivariate Feature Selection\n============================\n\nAn example showing univariate feature selection.\n\nNoisy (non informative) features are added to the iris data and\nunivariate feature selection is applied. For each feature, we plot the\np-values for the univariate feature selection and the corresponding\nweights of an SVM. We can see that univariate feature selection\nselects the informative features and that these have larger SVM weights.\n\nIn the total set of features, only the 4 first ones are significant. We\ncan see that they have the highest score with univariate feature\nselection. The SVM assigns a large weight to one of these features, but also\nSelects many of the non-informative features.\nApplying univariate feature selection before the SVM\nincreases the SVM weight attributed to the significant features, and will\nthus improve classification.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n# #############################################################################\n# Import some data to play with\n\n# The iris dataset\nX, y = load_iris(return_X_y=True)\n\n# Some noisy data not correlated\nE = np.random.RandomState(42).uniform(0, 0.1, size=(X.shape[0], 20))\n\n# Add the noisy data to the informative features\nX = np.hstack((X, E))\n\n# Split dataset to select feature and evaluate the classifier\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, stratify=y, random_state=0\n)\n\nplt.figure(1)\nplt.clf()\n\nX_indices = np.arange(X.shape[-1])\n\n# #############################################################################\n# Univariate feature selection with F-test for feature scoring\n# We use the default selection function to select the four\n# most significant features\nselector = SelectKBest(f_classif, k=4)\nselector.fit(X_train, y_train)\nscores = -np.log10(selector.pvalues_)\nscores /= scores.max()\nplt.bar(X_indices - .45, scores, width=.2,\n        label=r\'Univariate score ($-Log(p_{value})$)\', color=\'darkorange\',\n        edgecolor=\'black\')\n\n# #############################################################################\n# Compare to the weights of an SVM\nclf = make_pipeline(MinMaxScaler(), LinearSVC())\nclf.fit(X_train, y_train)\nprint(\'Classification accuracy without selecting features: {:.3f}\'\n      .format(clf.score(X_test, y_test)))\n\nsvm_weights = np.abs(clf[-1].coef_).sum(axis=0)\nsvm_weights /= svm_weights.sum()\n\nplt.bar(X_indices - .25, svm_weights, width=.2, label=\'SVM weight\',\n        color=\'navy\', edgecolor=\'black\')\n\nclf_selected = make_pipeline(\n        SelectKBest(f_classif, k=4), MinMaxScaler(), LinearSVC()\n)\nclf_selected.fit(X_train, y_train)\nprint(\'Classification accuracy after univariate feature selection: {:.3f}\'\n      .format(clf_selected.score(X_test, y_test)))\n\nsvm_weights_selected = np.abs(clf_selected[-1].coef_).sum(axis=0)\nsvm_weights_selected /= svm_weights_selected.sum()\n\nplt.bar(X_indices[selector.get_support()] - .05, svm_weights_selected,\n        width=.2, label=\'SVM weights after selection\', color=\'c\',\n        edgecolor=\'black\')\n\n\nplt.title(""Comparing feature selection"")\nplt.xlabel(\'Feature number\')\nplt.yticks(())\nplt.axis(\'tight\')\nplt.legend(loc=\'upper right\')\nplt.show()\n'"
scikit-learn/_downloads/0fda28796f5b7d27e96ba7eb0f458054/plot_digits_last_image.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nThe Digit Dataset\n=========================================================\n\nThis dataset is made up of 1797 8x8 images. Each image,\nlike the one shown below, is of a hand-written digit.\nIn order to utilize an 8x8 figure like this, we\'d have to\nfirst transform it into a feature vector with length 64.\n\nSee `here\n<https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits>`_\nfor more information about this dataset.\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nfrom sklearn import datasets\n\nimport matplotlib.pyplot as plt\n\n#Load the digits dataset\ndigits = datasets.load_digits()\n\n#Display the first digit\nplt.figure(1, figsize=(3, 3))\nplt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation=\'nearest\')\nplt.show()\n'"
scikit-learn/_downloads/10aacf585f645eed063e86526e855375/plot_topics_extraction_with_nmf_lda.py,0,"b'""""""\n=======================================================================================\nTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\n=======================================================================================\n\nThis is an example of applying :class:`sklearn.decomposition.NMF` and\n:class:`sklearn.decomposition.LatentDirichletAllocation` on a corpus\nof documents and extract additive models of the topic structure of the\ncorpus.  The output is a list of topics, each represented as a list of\nterms (weights are not shown).\n\nNon-negative Matrix Factorization is applied with two different objective\nfunctions: the Frobenius norm, and the generalized Kullback-Leibler divergence.\nThe latter is equivalent to Probabilistic Latent Semantic Indexing.\n\nThe default parameters (n_samples / n_features / n_components) should make\nthe example runnable in a couple of tens of seconds. You can try to\nincrease the dimensions of the problem, but be aware that the time\ncomplexity is polynomial in NMF. In LDA, the time complexity is\nproportional to (n_samples * iterations).\n\n""""""\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n#         Lars Buitinck\n#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n# License: BSD 3 clause\n\nfrom time import time\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.datasets import fetch_20newsgroups\n\nn_samples = 2000\nn_features = 1000\nn_components = 10\nn_top_words = 20\n\n\ndef print_top_words(model, feature_names, n_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        message = ""Topic #%d: "" % topic_idx\n        message += "" "".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()\n\n\n# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n# to filter out useless terms early on: the posts are stripped of headers,\n# footers and quoted replies, and common English words, words occurring in\n# only one document or in at least 95% of the documents are removed.\n\nprint(""Loading dataset..."")\nt0 = time()\ndata, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n                             remove=(\'headers\', \'footers\', \'quotes\'),\n                             return_X_y=True)\ndata_samples = data[:n_samples]\nprint(""done in %0.3fs."" % (time() - t0))\n\n# Use tf-idf features for NMF.\nprint(""Extracting tf-idf features for NMF..."")\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n                                   max_features=n_features,\n                                   stop_words=\'english\')\nt0 = time()\ntfidf = tfidf_vectorizer.fit_transform(data_samples)\nprint(""done in %0.3fs."" % (time() - t0))\n\n# Use tf (raw term count) features for LDA.\nprint(""Extracting tf features for LDA..."")\ntf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n                                max_features=n_features,\n                                stop_words=\'english\')\nt0 = time()\ntf = tf_vectorizer.fit_transform(data_samples)\nprint(""done in %0.3fs."" % (time() - t0))\nprint()\n\n# Fit the NMF model\nprint(""Fitting the NMF model (Frobenius norm) with tf-idf features, ""\n      ""n_samples=%d and n_features=%d...""\n      % (n_samples, n_features))\nt0 = time()\nnmf = NMF(n_components=n_components, random_state=1,\n          alpha=.1, l1_ratio=.5).fit(tfidf)\nprint(""done in %0.3fs."" % (time() - t0))\n\nprint(""\\nTopics in NMF model (Frobenius norm):"")\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, n_top_words)\n\n# Fit the NMF model\nprint(""Fitting the NMF model (generalized Kullback-Leibler divergence) with ""\n      ""tf-idf features, n_samples=%d and n_features=%d...""\n      % (n_samples, n_features))\nt0 = time()\nnmf = NMF(n_components=n_components, random_state=1,\n          beta_loss=\'kullback-leibler\', solver=\'mu\', max_iter=1000, alpha=.1,\n          l1_ratio=.5).fit(tfidf)\nprint(""done in %0.3fs."" % (time() - t0))\n\nprint(""\\nTopics in NMF model (generalized Kullback-Leibler divergence):"")\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nprint_top_words(nmf, tfidf_feature_names, n_top_words)\n\nprint(""Fitting LDA models with tf features, ""\n      ""n_samples=%d and n_features=%d...""\n      % (n_samples, n_features))\nlda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n                                learning_method=\'online\',\n                                learning_offset=50.,\n                                random_state=0)\nt0 = time()\nlda.fit(tf)\nprint(""done in %0.3fs."" % (time() - t0))\n\nprint(""\\nTopics in LDA model:"")\ntf_feature_names = tf_vectorizer.get_feature_names()\nprint_top_words(lda, tf_feature_names, n_top_words)\n'"
scikit-learn/_downloads/10c8778be36a90b7425b59a0a6a6460f/plot_iris_svc.py,3,"b'""""""\n==================================================\nPlot different SVM classifiers in the iris dataset\n==================================================\n\nComparison of different linear SVM classifiers on a 2D projection of the iris\ndataset. We only consider the first 2 features of this dataset:\n\n- Sepal length\n- Sepal width\n\nThis example shows how to plot the decision surface for four SVM classifiers\nwith different kernels.\n\nThe linear models ``LinearSVC()`` and ``SVC(kernel=\'linear\')`` yield slightly\ndifferent decision boundaries. This can be a consequence of the following\ndifferences:\n\n- ``LinearSVC`` minimizes the squared hinge loss while ``SVC`` minimizes the\n  regular hinge loss.\n\n- ``LinearSVC`` uses the One-vs-All (also known as One-vs-Rest) multiclass\n  reduction while ``SVC`` uses the One-vs-One multiclass reduction.\n\nBoth linear models have linear decision boundaries (intersecting hyperplanes)\nwhile the non-linear kernel models (polynomial or Gaussian RBF) have more\nflexible non-linear decision boundaries with shapes that depend on the kind of\nkernel and its parameters.\n\n.. NOTE:: while plotting the decision function of classifiers for toy 2D\n   datasets can help get an intuitive understanding of their respective\n   expressive power, be aware that those intuitions don\'t always generalize to\n   more realistic high-dimensional problems.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\n\ndef make_meshgrid(x, y, h=.02):\n    """"""Create a mesh of points to plot in\n\n    Parameters\n    ----------\n    x: data to base x-axis meshgrid on\n    y: data to base y-axis meshgrid on\n    h: stepsize for meshgrid, optional\n\n    Returns\n    -------\n    xx, yy : ndarray\n    """"""\n    x_min, x_max = x.min() - 1, x.max() + 1\n    y_min, y_max = y.min() - 1, y.max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    return xx, yy\n\n\ndef plot_contours(ax, clf, xx, yy, **params):\n    """"""Plot the decision boundaries for a classifier.\n\n    Parameters\n    ----------\n    ax: matplotlib axes object\n    clf: a classifier\n    xx: meshgrid ndarray\n    yy: meshgrid ndarray\n    params: dictionary of params to pass to contourf, optional\n    """"""\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    out = ax.contourf(xx, yy, Z, **params)\n    return out\n\n\n# import some data to play with\niris = datasets.load_iris()\n# Take the first two features. We could avoid this by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nC = 1.0  # SVM regularization parameter\nmodels = (svm.SVC(kernel=\'linear\', C=C),\n          svm.LinearSVC(C=C, max_iter=10000),\n          svm.SVC(kernel=\'rbf\', gamma=0.7, C=C),\n          svm.SVC(kernel=\'poly\', degree=3, gamma=\'auto\', C=C))\nmodels = (clf.fit(X, y) for clf in models)\n\n# title for the plots\ntitles = (\'SVC with linear kernel\',\n          \'LinearSVC (linear kernel)\',\n          \'SVC with RBF kernel\',\n          \'SVC with polynomial (degree 3) kernel\')\n\n# Set-up 2x2 grid for plotting.\nfig, sub = plt.subplots(2, 2)\nplt.subplots_adjust(wspace=0.4, hspace=0.4)\n\nX0, X1 = X[:, 0], X[:, 1]\nxx, yy = make_meshgrid(X0, X1)\n\nfor clf, title, ax in zip(models, titles, sub.flatten()):\n    plot_contours(ax, clf, xx, yy,\n                  cmap=plt.cm.coolwarm, alpha=0.8)\n    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\'k\')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xlabel(\'Sepal length\')\n    ax.set_ylabel(\'Sepal width\')\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_title(title)\n\nplt.show()\n'"
scikit-learn/_downloads/1229301576715e83d63d4f799d7c73a9/plot_kde_1d.py,17,"b'""""""\n===================================\nSimple 1D Kernel Density Estimation\n===================================\nThis example uses the :class:`sklearn.neighbors.KernelDensity` class to\ndemonstrate the principles of Kernel Density Estimation in one dimension.\n\nThe first plot shows one of the problems with using histograms to visualize\nthe density of points in 1D. Intuitively, a histogram can be thought of as a\nscheme in which a unit ""block"" is stacked above each point on a regular grid.\nAs the top two panels show, however, the choice of gridding for these blocks\ncan lead to wildly divergent ideas about the underlying shape of the density\ndistribution.  If we instead center each block on the point it represents, we\nget the estimate shown in the bottom left panel.  This is a kernel density\nestimation with a ""top hat"" kernel.  This idea can be generalized to other\nkernel shapes: the bottom-right panel of the first figure shows a Gaussian\nkernel density estimate over the same distribution.\n\nScikit-learn implements efficient kernel density estimation using either\na Ball Tree or KD Tree structure, through the\n:class:`sklearn.neighbors.KernelDensity` estimator.  The available kernels\nare shown in the second figure of this example.\n\nThe third figure compares kernel density estimates for a distribution of 100\nsamples in 1 dimension.  Though this example uses 1D distributions, kernel\ndensity estimation is easily and efficiently extensible to higher dimensions\nas well.\n""""""\n# Author: Jake Vanderplas <jakevdp@cs.washington.edu>\n#\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom distutils.version import LooseVersion\nfrom scipy.stats import norm\nfrom sklearn.neighbors import KernelDensity\n\n# `normed` is being deprecated in favor of `density` in histograms\nif LooseVersion(matplotlib.__version__) >= \'2.1\':\n    density_param = {\'density\': True}\nelse:\n    density_param = {\'normed\': True}\n\n# ----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\nnp.random.seed(1)\nN = 20\nX = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),\n                    np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\nbins = np.linspace(-5, 10, 10)\n\nfig, ax = plt.subplots(2, 2, sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n# histogram 1\nax[0, 0].hist(X[:, 0], bins=bins, fc=\'#AAAAFF\', **density_param)\nax[0, 0].text(-3.5, 0.31, ""Histogram"")\n\n# histogram 2\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc=\'#AAAAFF\', **density_param)\nax[0, 1].text(-3.5, 0.31, ""Histogram, bins shifted"")\n\n# tophat KDE\nkde = KernelDensity(kernel=\'tophat\', bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 0].fill(X_plot[:, 0], np.exp(log_dens), fc=\'#AAAAFF\')\nax[1, 0].text(-3.5, 0.31, ""Tophat Kernel Density"")\n\n# Gaussian KDE\nkde = KernelDensity(kernel=\'gaussian\', bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 1].fill(X_plot[:, 0], np.exp(log_dens), fc=\'#AAAAFF\')\nax[1, 1].text(-3.5, 0.31, ""Gaussian Kernel Density"")\n\nfor axi in ax.ravel():\n    axi.plot(X[:, 0], np.full(X.shape[0], -0.01), \'+k\')\n    axi.set_xlim(-4, 9)\n    axi.set_ylim(-0.02, 0.34)\n\nfor axi in ax[:, 0]:\n    axi.set_ylabel(\'Normalized Density\')\n\nfor axi in ax[1, :]:\n    axi.set_xlabel(\'x\')\n\n# ----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = np.linspace(-6, 6, 1000)[:, None]\nX_src = np.zeros((1, 1))\n\nfig, ax = plt.subplots(2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return \'0\'\n    elif x == 1:\n        return \'h\'\n    elif x == -1:\n        return \'-h\'\n    else:\n        return \'%ih\' % x\n\nfor i, kernel in enumerate([\'gaussian\', \'tophat\', \'epanechnikov\',\n                            \'exponential\', \'linear\', \'cosine\']):\n    axi = ax.ravel()[i]\n    log_dens = KernelDensity(kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], np.exp(log_dens), \'-k\', fc=\'#AAAAFF\')\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title(\'Available Kernels\')\n\n# ----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\nnp.random.seed(1)\nX = np.concatenate((np.random.normal(0, 1, int(0.3 * N)),\n                    np.random.normal(5, 1, int(0.7 * N))))[:, np.newaxis]\n\nX_plot = np.linspace(-5, 10, 1000)[:, np.newaxis]\n\ntrue_dens = (0.3 * norm(0, 1).pdf(X_plot[:, 0])\n             + 0.7 * norm(5, 1).pdf(X_plot[:, 0]))\n\nfig, ax = plt.subplots()\nax.fill(X_plot[:, 0], true_dens, fc=\'black\', alpha=0.2,\n        label=\'input distribution\')\ncolors = [\'navy\', \'cornflowerblue\', \'darkorange\']\nkernels = [\'gaussian\', \'tophat\', \'epanechnikov\']\nlw = 2\n\nfor color, kernel in zip(colors, kernels):\n    kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(X_plot[:, 0], np.exp(log_dens), color=color, lw=lw,\n            linestyle=\'-\', label=""kernel = \'{0}\'"".format(kernel))\n\nax.text(6, 0.38, ""N={0} points"".format(N))\n\nax.legend(loc=\'upper left\')\nax.plot(X[:, 0], -0.005 - 0.01 * np.random.random(X.shape[0]), \'+k\')\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\nplt.show()\n'"
scikit-learn/_downloads/122ebff9d67a560bf91e9c7f7ce04249/plot_sgd_weighted_samples.py,6,"b'""""""\n=====================\nSGD: Weighted samples\n=====================\n\nPlot decision function of a weighted dataset, where the size of points\nis proportional to its weight.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\n# we create 20 points\nnp.random.seed(0)\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\ny = [1] * 10 + [-1] * 10\nsample_weight = 100 * np.abs(np.random.randn(20))\n# and assign a bigger weight to the last 10 samples\nsample_weight[:10] *= 10\n\n# plot the weighted data points\nxx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\nplt.figure()\nplt.scatter(X[:, 0], X[:, 1], c=y, s=sample_weight, alpha=0.9,\n            cmap=plt.cm.bone, edgecolor=\'black\')\n\n# fit the unweighted model\nclf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)\nclf.fit(X, y)\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nno_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=[\'solid\'])\n\n# fit the weighted model\nclf = linear_model.SGDClassifier(alpha=0.01, max_iter=100)\nclf.fit(X, y, sample_weight=sample_weight)\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\nsamples_weights = plt.contour(xx, yy, Z, levels=[0], linestyles=[\'dashed\'])\n\nplt.legend([no_weights.collections[0], samples_weights.collections[0]],\n           [""no weights"", ""with weights""], loc=""lower left"")\n\nplt.xticks(())\nplt.yticks(())\nplt.show()\n'"
scikit-learn/_downloads/129ffd121af947c0be1d871a0966ee3d/plot_kernel_pca.py,3,"b'""""""\n==========\nKernel PCA\n==========\n\nThis example shows that Kernel PCA is able to find a projection of the data\nthat makes data linearly separable.\n""""""\nprint(__doc__)\n\n# Authors: Mathieu Blondel\n#          Andreas Mueller\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.datasets import make_circles\n\nnp.random.seed(0)\n\nX, y = make_circles(n_samples=400, factor=.3, noise=.05)\n\nkpca = KernelPCA(kernel=""rbf"", fit_inverse_transform=True, gamma=10)\nX_kpca = kpca.fit_transform(X)\nX_back = kpca.inverse_transform(X_kpca)\npca = PCA()\nX_pca = pca.fit_transform(X)\n\n# Plot results\n\nplt.figure()\nplt.subplot(2, 2, 1, aspect=\'equal\')\nplt.title(""Original space"")\nreds = y == 0\nblues = y == 1\n\nplt.scatter(X[reds, 0], X[reds, 1], c=""red"",\n            s=20, edgecolor=\'k\')\nplt.scatter(X[blues, 0], X[blues, 1], c=""blue"",\n            s=20, edgecolor=\'k\')\nplt.xlabel(""$x_1$"")\nplt.ylabel(""$x_2$"")\n\nX1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\nX_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n# projection on the first principal component (in the phi space)\nZ_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)\nplt.contour(X1, X2, Z_grid, colors=\'grey\', linewidths=1, origin=\'lower\')\n\nplt.subplot(2, 2, 2, aspect=\'equal\')\nplt.scatter(X_pca[reds, 0], X_pca[reds, 1], c=""red"",\n            s=20, edgecolor=\'k\')\nplt.scatter(X_pca[blues, 0], X_pca[blues, 1], c=""blue"",\n            s=20, edgecolor=\'k\')\nplt.title(""Projection by PCA"")\nplt.xlabel(""1st principal component"")\nplt.ylabel(""2nd component"")\n\nplt.subplot(2, 2, 3, aspect=\'equal\')\nplt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=""red"",\n            s=20, edgecolor=\'k\')\nplt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=""blue"",\n            s=20, edgecolor=\'k\')\nplt.title(""Projection by KPCA"")\nplt.xlabel(r""1st principal component in space induced by $\\phi$"")\nplt.ylabel(""2nd component"")\n\nplt.subplot(2, 2, 4, aspect=\'equal\')\nplt.scatter(X_back[reds, 0], X_back[reds, 1], c=""red"",\n            s=20, edgecolor=\'k\')\nplt.scatter(X_back[blues, 0], X_back[blues, 1], c=""blue"",\n            s=20, edgecolor=\'k\')\nplt.title(""Original space after inverse transform"")\nplt.xlabel(""$x_1$"")\nplt.ylabel(""$x_2$"")\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/134898d7f1f7483235dade3a5fd65919/plot_missing_values.py,12,"b'""""""\n====================================================\nImputing missing values before building an estimator\n====================================================\n\nMissing values can be replaced by the mean, the median or the most frequent\nvalue using the basic :class:`sklearn.impute.SimpleImputer`.\nThe median is a more robust estimator for data with high magnitude variables\nwhich could dominate results (otherwise known as a \'long tail\').\n\nWith ``KNNImputer``, missing values can be imputed using the weighted\nor unweighted mean of the desired number of nearest neighbors.\n\nAnother option is the :class:`sklearn.impute.IterativeImputer`. This uses\nround-robin linear regression, treating every variable as an output in\nturn. The version implemented assumes Gaussian (output) variables. If your\nfeatures are obviously non-Normal, consider transforming them to look more\nNormal so as to potentially improve performance.\n\nIn addition of using an imputing method, we can also keep an indication of the\nmissing information using :func:`sklearn.impute.MissingIndicator` which might\ncarry some information.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# To use the experimental IterativeImputer, we need to explicitly ask for it:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.datasets import load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.impute import (\n    SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator)\nfrom sklearn.model_selection import cross_val_score\n\nrng = np.random.RandomState(0)\n\nN_SPLITS = 5\nREGRESSOR = RandomForestRegressor(random_state=0)\n\n\ndef get_scores_for_imputer(imputer, X_missing, y_missing):\n    estimator = make_pipeline(\n        make_union(imputer, MissingIndicator(missing_values=0)),\n        REGRESSOR)\n    impute_scores = cross_val_score(estimator, X_missing, y_missing,\n                                    scoring=\'neg_mean_squared_error\',\n                                    cv=N_SPLITS)\n    return impute_scores\n\n\ndef get_results(dataset):\n    X_full, y_full = dataset.data, dataset.target\n    n_samples = X_full.shape[0]\n    n_features = X_full.shape[1]\n\n    # Estimate the score on the entire dataset, with no missing values\n    full_scores = cross_val_score(REGRESSOR, X_full, y_full,\n                                  scoring=\'neg_mean_squared_error\',\n                                  cv=N_SPLITS)\n\n    # Add missing values in 75% of the lines\n    missing_rate = 0.75\n    n_missing_samples = int(np.floor(n_samples * missing_rate))\n    missing_samples = np.hstack((np.zeros(n_samples - n_missing_samples,\n                                          dtype=np.bool),\n                                 np.ones(n_missing_samples,\n                                         dtype=np.bool)))\n    rng.shuffle(missing_samples)\n    missing_features = rng.randint(0, n_features, n_missing_samples)\n    X_missing = X_full.copy()\n    X_missing[np.where(missing_samples)[0], missing_features] = 0\n    y_missing = y_full.copy()\n\n    # Estimate the score after replacing missing values by 0\n    imputer = SimpleImputer(missing_values=0,\n                            strategy=\'constant\',\n                            fill_value=0)\n    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n\n    # Estimate the score after imputation (mean strategy) of the missing values\n    imputer = SimpleImputer(missing_values=0, strategy=""mean"")\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n\n    # Estimate the score after kNN-imputation of the missing values\n    imputer = KNNImputer(missing_values=0)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n\n    # Estimate the score after iterative imputation of the missing values\n    imputer = IterativeImputer(missing_values=0,\n                               random_state=0,\n                               n_nearest_features=5,\n                               sample_posterior=True)\n    iterative_impute_scores = get_scores_for_imputer(imputer,\n                                                     X_missing,\n                                                     y_missing)\n\n    return ((full_scores.mean(), full_scores.std()),\n            (zero_impute_scores.mean(), zero_impute_scores.std()),\n            (mean_impute_scores.mean(), mean_impute_scores.std()),\n            (knn_impute_scores.mean(), knn_impute_scores.std()),\n            (iterative_impute_scores.mean(), iterative_impute_scores.std()))\n\n\nresults_diabetes = np.array(get_results(load_diabetes()))\nmses_diabetes = results_diabetes[:, 0] * -1\nstds_diabetes = results_diabetes[:, 1]\n\nresults_boston = np.array(get_results(load_boston()))\nmses_boston = results_boston[:, 0] * -1\nstds_boston = results_boston[:, 1]\n\nn_bars = len(mses_diabetes)\nxval = np.arange(n_bars)\n\nx_labels = [\'Full data\',\n            \'Zero imputation\',\n            \'Mean Imputation\',\n            \'KNN Imputation\',\n            \'Iterative Imputation\']\ncolors = [\'r\', \'g\', \'b\', \'orange\', \'black\']\n\n# plot diabetes results\nplt.figure(figsize=(12, 6))\nax1 = plt.subplot(121)\nfor j in xval:\n    ax1.barh(j, mses_diabetes[j], xerr=stds_diabetes[j],\n             color=colors[j], alpha=0.6, align=\'center\')\n\nax1.set_title(\'Imputation Techniques with Diabetes Data\')\nax1.set_xlim(left=np.min(mses_diabetes) * 0.9,\n             right=np.max(mses_diabetes) * 1.1)\nax1.set_yticks(xval)\nax1.set_xlabel(\'MSE\')\nax1.invert_yaxis()\nax1.set_yticklabels(x_labels)\n\n# plot boston results\nax2 = plt.subplot(122)\nfor j in xval:\n    ax2.barh(j, mses_boston[j], xerr=stds_boston[j],\n             color=colors[j], alpha=0.6, align=\'center\')\n\nax2.set_title(\'Imputation Techniques with Boston Data\')\nax2.set_yticks(xval)\nax2.set_xlabel(\'MSE\')\nax2.invert_yaxis()\nax2.set_yticklabels([\'\'] * n_bars)\n\nplt.show()\n'"
scikit-learn/_downloads/14a3fa0e4be2afed01152579971a74ab/plot_forest_importances.py,2,"b'""""""\n=========================================\nFeature importances with forests of trees\n=========================================\n\nThis examples shows the use of forests of trees to evaluate the importance of\nfeatures on an artificial classification task. The red bars are the feature\nimportances of the forest, along with their inter-trees variability.\n\nAs expected, the plot suggests that 3 features are informative, while the\nremaining are not.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000,\n                           n_features=10,\n                           n_informative=3,\n                           n_redundant=0,\n                           n_repeated=0,\n                           n_classes=2,\n                           random_state=0,\n                           shuffle=False)\n\n# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(""Feature ranking:"")\n\nfor f in range(X.shape[1]):\n    print(""%d. feature %d (%f)"" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(""Feature importances"")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=""r"", yerr=std[indices], align=""center"")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()\n'"
scikit-learn/_downloads/150c83d8907fbd0e58a7717e3b58e7d7/plot_cluster_iris.py,3,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nK-means Clustering\n=========================================================\n\nThe plots display firstly what a K-means algorithm would yield\nusing three clusters. It is then shown what the effect of a bad\ninitialization is on the classification process:\nBy setting n_init to only 1 (default is 10), the amount of\ntimes that the algorithm will be run with different centroid\nseeds is reduced.\nThe next plot displays what using eight clusters would deliver\nand finally the ground truth.\n\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Though the following import is not directly being used, it is required\n# for 3D projection to work\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\n\nnp.random.seed(5)\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nestimators = [(\'k_means_iris_8\', KMeans(n_clusters=8)),\n              (\'k_means_iris_3\', KMeans(n_clusters=3)),\n              (\'k_means_iris_bad_init\', KMeans(n_clusters=3, n_init=1,\n                                               init=\'random\'))]\n\nfignum = 1\ntitles = [\'8 clusters\', \'3 clusters\', \'3 clusters, bad initialization\']\nfor name, est in estimators:\n    fig = plt.figure(fignum, figsize=(4, 3))\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n    est.fit(X)\n    labels = est.labels_\n\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n               c=labels.astype(np.float), edgecolor=\'k\')\n\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n    ax.set_xlabel(\'Petal width\')\n    ax.set_ylabel(\'Sepal length\')\n    ax.set_zlabel(\'Petal length\')\n    ax.set_title(titles[fignum - 1])\n    ax.dist = 12\n    fignum = fignum + 1\n\n# Plot the ground truth\nfig = plt.figure(fignum, figsize=(4, 3))\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nfor name, label in [(\'Setosa\', 0),\n                    (\'Versicolour\', 1),\n                    (\'Virginica\', 2)]:\n    ax.text3D(X[y == label, 3].mean(),\n              X[y == label, 0].mean(),\n              X[y == label, 2].mean() + 2, name,\n              horizontalalignment=\'center\',\n              bbox=dict(alpha=.2, edgecolor=\'w\', facecolor=\'w\'))\n# Reorder the labels to have colors matching the cluster results\ny = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\'k\')\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\nax.set_xlabel(\'Petal width\')\nax.set_ylabel(\'Sepal length\')\nax.set_zlabel(\'Petal length\')\nax.set_title(\'Ground Truth\')\nax.dist = 12\n\nfig.show()\n'"
scikit-learn/_downloads/1949f2d0fbbff64ae82cfad18287827c/plot_stock_market.py,10,"b'""""""\n=======================================\nVisualizing the stock market structure\n=======================================\n\nThis example employs several unsupervised learning techniques to extract\nthe stock market structure from variations in historical quotes.\n\nThe quantity that we use is the daily variation in quote price: quotes\nthat are linked tend to cofluctuate during a day.\n\n.. _stock_market:\n\nLearning a graph structure\n--------------------------\n\nWe use sparse inverse covariance estimation to find which quotes are\ncorrelated conditionally on the others. Specifically, sparse inverse\ncovariance gives us a graph, that is a list of connection. For each\nsymbol, the symbols that it is connected too are those useful to explain\nits fluctuations.\n\nClustering\n----------\n\nWe use clustering to group together quotes that behave similarly. Here,\namongst the :ref:`various clustering techniques <clustering>` available\nin the scikit-learn, we use :ref:`affinity_propagation` as it does\nnot enforce equal-size clusters, and it can choose automatically the\nnumber of clusters from the data.\n\nNote that this gives us a different indication than the graph, as the\ngraph reflects conditional relations between variables, while the\nclustering reflects marginal properties: variables clustered together can\nbe considered as having a similar impact at the level of the full stock\nmarket.\n\nEmbedding in 2D space\n---------------------\n\nFor visualization purposes, we need to lay out the different symbols on a\n2D canvas. For this we use :ref:`manifold` techniques to retrieve 2D\nembedding.\n\n\nVisualization\n-------------\n\nThe output of the 3 models are combined in a 2D graph where nodes\nrepresents the stocks and edges the:\n\n- cluster labels are used to define the color of the nodes\n- the sparse covariance model is used to display the strength of the edges\n- the 2D embedding is used to position the nodes in the plan\n\nThis example has a fair amount of visualization-related code, as\nvisualization is crucial here to display the graph. One of the challenge\nis to position the labels minimizing overlap. For this we use an\nheuristic based on the direction of the nearest neighbor along each\naxis.\n""""""\n\n# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n# License: BSD 3 clause\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nimport pandas as pd\n\nfrom sklearn import cluster, covariance, manifold\n\nprint(__doc__)\n\n\n# #############################################################################\n# Retrieve the data from Internet\n\n# The data is from 2003 - 2008. This is reasonably calm: (not too long ago so\n# that we get high-tech firms, and before the 2008 crash). This kind of\n# historical data can be obtained for from APIs like the quandl.com and\n# alphavantage.co ones.\n\nsymbol_dict = {\n    \'TOT\': \'Total\',\n    \'XOM\': \'Exxon\',\n    \'CVX\': \'Chevron\',\n    \'COP\': \'ConocoPhillips\',\n    \'VLO\': \'Valero Energy\',\n    \'MSFT\': \'Microsoft\',\n    \'IBM\': \'IBM\',\n    \'TWX\': \'Time Warner\',\n    \'CMCSA\': \'Comcast\',\n    \'CVC\': \'Cablevision\',\n    \'YHOO\': \'Yahoo\',\n    \'DELL\': \'Dell\',\n    \'HPQ\': \'HP\',\n    \'AMZN\': \'Amazon\',\n    \'TM\': \'Toyota\',\n    \'CAJ\': \'Canon\',\n    \'SNE\': \'Sony\',\n    \'F\': \'Ford\',\n    \'HMC\': \'Honda\',\n    \'NAV\': \'Navistar\',\n    \'NOC\': \'Northrop Grumman\',\n    \'BA\': \'Boeing\',\n    \'KO\': \'Coca Cola\',\n    \'MMM\': \'3M\',\n    \'MCD\': \'McDonald\\\'s\',\n    \'PEP\': \'Pepsi\',\n    \'K\': \'Kellogg\',\n    \'UN\': \'Unilever\',\n    \'MAR\': \'Marriott\',\n    \'PG\': \'Procter Gamble\',\n    \'CL\': \'Colgate-Palmolive\',\n    \'GE\': \'General Electrics\',\n    \'WFC\': \'Wells Fargo\',\n    \'JPM\': \'JPMorgan Chase\',\n    \'AIG\': \'AIG\',\n    \'AXP\': \'American express\',\n    \'BAC\': \'Bank of America\',\n    \'GS\': \'Goldman Sachs\',\n    \'AAPL\': \'Apple\',\n    \'SAP\': \'SAP\',\n    \'CSCO\': \'Cisco\',\n    \'TXN\': \'Texas Instruments\',\n    \'XRX\': \'Xerox\',\n    \'WMT\': \'Wal-Mart\',\n    \'HD\': \'Home Depot\',\n    \'GSK\': \'GlaxoSmithKline\',\n    \'PFE\': \'Pfizer\',\n    \'SNY\': \'Sanofi-Aventis\',\n    \'NVS\': \'Novartis\',\n    \'KMB\': \'Kimberly-Clark\',\n    \'R\': \'Ryder\',\n    \'GD\': \'General Dynamics\',\n    \'RTN\': \'Raytheon\',\n    \'CVS\': \'CVS\',\n    \'CAT\': \'Caterpillar\',\n    \'DD\': \'DuPont de Nemours\'}\n\n\nsymbols, names = np.array(sorted(symbol_dict.items())).T\n\nquotes = []\n\nfor symbol in symbols:\n    print(\'Fetching quote history for %r\' % symbol, file=sys.stderr)\n    url = (\'https://raw.githubusercontent.com/scikit-learn/examples-data/\'\n           \'master/financial-data/{}.csv\')\n    quotes.append(pd.read_csv(url.format(symbol)))\n\nclose_prices = np.vstack([q[\'close\'] for q in quotes])\nopen_prices = np.vstack([q[\'open\'] for q in quotes])\n\n# The daily variations of the quotes are what carry most information\nvariation = close_prices - open_prices\n\n\n# #############################################################################\n# Learn a graphical structure from the correlations\nedge_model = covariance.GraphicalLassoCV()\n\n# standardize the time series: using correlations rather than covariance\n# is more efficient for structure recovery\nX = variation.copy().T\nX /= X.std(axis=0)\nedge_model.fit(X)\n\n# #############################################################################\n# Cluster using affinity propagation\n\n_, labels = cluster.affinity_propagation(edge_model.covariance_)\nn_labels = labels.max()\n\nfor i in range(n_labels + 1):\n    print(\'Cluster %i: %s\' % ((i + 1), \', \'.join(names[labels == i])))\n\n# #############################################################################\n# Find a low-dimension embedding for visualization: find the best position of\n# the nodes (the stocks) on a 2D plane\n\n# We use a dense eigen_solver to achieve reproducibility (arpack is\n# initiated with random vectors that we don\'t control). In addition, we\n# use a large number of neighbors to capture the large-scale structure.\nnode_position_model = manifold.LocallyLinearEmbedding(\n    n_components=2, eigen_solver=\'dense\', n_neighbors=6)\n\nembedding = node_position_model.fit_transform(X.T).T\n\n# #############################################################################\n# Visualization\nplt.figure(1, facecolor=\'w\', figsize=(10, 8))\nplt.clf()\nax = plt.axes([0., 0., 1., 1.])\nplt.axis(\'off\')\n\n# Display a graph of the partial correlations\npartial_correlations = edge_model.precision_.copy()\nd = 1 / np.sqrt(np.diag(partial_correlations))\npartial_correlations *= d\npartial_correlations *= d[:, np.newaxis]\nnon_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n\n# Plot the nodes using the coordinates of our embedding\nplt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n            cmap=plt.cm.nipy_spectral)\n\n# Plot the edges\nstart_idx, end_idx = np.where(non_zero)\n# a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [[embedding[:, start], embedding[:, stop]]\n            for start, stop in zip(start_idx, end_idx)]\nvalues = np.abs(partial_correlations[non_zero])\nlc = LineCollection(segments,\n                    zorder=0, cmap=plt.cm.hot_r,\n                    norm=plt.Normalize(0, .7 * values.max()))\nlc.set_array(values)\nlc.set_linewidths(15 * values)\nax.add_collection(lc)\n\n# Add a label to each node. The challenge here is that we want to\n# position the labels to avoid overlap with other labels\nfor index, (name, label, (x, y)) in enumerate(\n        zip(names, labels, embedding.T)):\n\n    dx = x - embedding[0]\n    dx[index] = 1\n    dy = y - embedding[1]\n    dy[index] = 1\n    this_dx = dx[np.argmin(np.abs(dy))]\n    this_dy = dy[np.argmin(np.abs(dx))]\n    if this_dx > 0:\n        horizontalalignment = \'left\'\n        x = x + .002\n    else:\n        horizontalalignment = \'right\'\n        x = x - .002\n    if this_dy > 0:\n        verticalalignment = \'bottom\'\n        y = y + .002\n    else:\n        verticalalignment = \'top\'\n        y = y - .002\n    plt.text(x, y, name, size=10,\n             horizontalalignment=horizontalalignment,\n             verticalalignment=verticalalignment,\n             bbox=dict(facecolor=\'w\',\n                       edgecolor=plt.cm.nipy_spectral(label / float(n_labels)),\n                       alpha=.6))\n\nplt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n         embedding[0].max() + .10 * embedding[0].ptp(),)\nplt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n         embedding[1].max() + .03 * embedding[1].ptp())\n\nplt.show()\n'"
scikit-learn/_downloads/198258e5a182b2fc03594a1b00be3a1d/plot_robust_vs_empirical_covariance.py,21,"b'r""""""\n=======================================\nRobust vs Empirical covariance estimate\n=======================================\n\nThe usual covariance maximum likelihood estimate is very sensitive to the\npresence of outliers in the data set. In such a case, it would be better to\nuse a robust estimator of covariance to guarantee that the estimation is\nresistant to ""erroneous"" observations in the data set. [1]_, [2]_\n\nMinimum Covariance Determinant Estimator\n----------------------------------------\nThe Minimum Covariance Determinant estimator is a robust, high-breakdown point\n(i.e. it can be used to estimate the covariance matrix of highly contaminated\ndatasets, up to\n:math:`\\frac{n_\\text{samples} - n_\\text{features}-1}{2}` outliers) estimator of\ncovariance. The idea is to find\n:math:`\\frac{n_\\text{samples} + n_\\text{features}+1}{2}`\nobservations whose empirical covariance has the smallest determinant, yielding\na ""pure"" subset of observations from which to compute standards estimates of\nlocation and covariance. After a correction step aiming at compensating the\nfact that the estimates were learned from only a portion of the initial data,\nwe end up with robust estimates of the data set location and covariance.\n\nThe Minimum Covariance Determinant estimator (MCD) has been introduced by\nP.J.Rousseuw in [3]_.\n\nEvaluation\n----------\nIn this example, we compare the estimation errors that are made when using\nvarious types of location and covariance estimates on contaminated Gaussian\ndistributed data sets:\n\n- The mean and the empirical covariance of the full dataset, which break\n  down as soon as there are outliers in the data set\n- The robust MCD, that has a low error provided\n  :math:`n_\\text{samples} > 5n_\\text{features}`\n- The mean and the empirical covariance of the observations that are known\n  to be good ones. This can be considered as a ""perfect"" MCD estimation,\n  so one can trust our implementation by comparing to this case.\n\n\nReferences\n----------\n.. [1] Johanna Hardin, David M Rocke. The distribution of robust distances.\n    Journal of Computational and Graphical Statistics. December 1, 2005,\n    14(4): 928-946.\n.. [2] Zoubir A., Koivunen V., Chakhchoukh Y. and Muma M. (2012). Robust\n    estimation in signal processing: A tutorial-style treatment of\n    fundamental concepts. IEEE Signal Processing Magazine 29(4), 61-80.\n.. [3] P. J. Rousseeuw. Least median of squares regression. Journal of American\n    Statistical Ass., 79:871, 1984.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\n# example settings\nn_samples = 80\nn_features = 5\nrepeat = 10\n\nrange_n_outliers = np.concatenate(\n    (np.linspace(0, n_samples / 8, 5),\n     np.linspace(n_samples / 8, n_samples / 2, 5)[1:-1])).astype(np.int)\n\n# definition of arrays to store results\nerr_loc_mcd = np.zeros((range_n_outliers.size, repeat))\nerr_cov_mcd = np.zeros((range_n_outliers.size, repeat))\nerr_loc_emp_full = np.zeros((range_n_outliers.size, repeat))\nerr_cov_emp_full = np.zeros((range_n_outliers.size, repeat))\nerr_loc_emp_pure = np.zeros((range_n_outliers.size, repeat))\nerr_cov_emp_pure = np.zeros((range_n_outliers.size, repeat))\n\n# computation\nfor i, n_outliers in enumerate(range_n_outliers):\n    for j in range(repeat):\n\n        rng = np.random.RandomState(i * j)\n\n        # generate data\n        X = rng.randn(n_samples, n_features)\n        # add some outliers\n        outliers_index = rng.permutation(n_samples)[:n_outliers]\n        outliers_offset = 10. * \\\n            (np.random.randint(2, size=(n_outliers, n_features)) - 0.5)\n        X[outliers_index] += outliers_offset\n        inliers_mask = np.ones(n_samples).astype(bool)\n        inliers_mask[outliers_index] = False\n\n        # fit a Minimum Covariance Determinant (MCD) robust estimator to data\n        mcd = MinCovDet().fit(X)\n        # compare raw robust estimates with the true location and covariance\n        err_loc_mcd[i, j] = np.sum(mcd.location_ ** 2)\n        err_cov_mcd[i, j] = mcd.error_norm(np.eye(n_features))\n\n        # compare estimators learned from the full data set with true\n        # parameters\n        err_loc_emp_full[i, j] = np.sum(X.mean(0) ** 2)\n        err_cov_emp_full[i, j] = EmpiricalCovariance().fit(X).error_norm(\n            np.eye(n_features))\n\n        # compare with an empirical covariance learned from a pure data set\n        # (i.e. ""perfect"" mcd)\n        pure_X = X[inliers_mask]\n        pure_location = pure_X.mean(0)\n        pure_emp_cov = EmpiricalCovariance().fit(pure_X)\n        err_loc_emp_pure[i, j] = np.sum(pure_location ** 2)\n        err_cov_emp_pure[i, j] = pure_emp_cov.error_norm(np.eye(n_features))\n\n# Display results\nfont_prop = matplotlib.font_manager.FontProperties(size=11)\nplt.subplot(2, 1, 1)\nlw = 2\nplt.errorbar(range_n_outliers, err_loc_mcd.mean(1),\n             yerr=err_loc_mcd.std(1) / np.sqrt(repeat),\n             label=""Robust location"", lw=lw, color=\'m\')\nplt.errorbar(range_n_outliers, err_loc_emp_full.mean(1),\n             yerr=err_loc_emp_full.std(1) / np.sqrt(repeat),\n             label=""Full data set mean"", lw=lw, color=\'green\')\nplt.errorbar(range_n_outliers, err_loc_emp_pure.mean(1),\n             yerr=err_loc_emp_pure.std(1) / np.sqrt(repeat),\n             label=""Pure data set mean"", lw=lw, color=\'black\')\nplt.title(""Influence of outliers on the location estimation"")\nplt.ylabel(r""Error ($||\\mu - \\hat{\\mu}||_2^2$)"")\nplt.legend(loc=""upper left"", prop=font_prop)\n\nplt.subplot(2, 1, 2)\nx_size = range_n_outliers.size\nplt.errorbar(range_n_outliers, err_cov_mcd.mean(1),\n             yerr=err_cov_mcd.std(1),\n             label=""Robust covariance (mcd)"", color=\'m\')\nplt.errorbar(range_n_outliers[:(x_size // 5 + 1)],\n             err_cov_emp_full.mean(1)[:(x_size // 5 + 1)],\n             yerr=err_cov_emp_full.std(1)[:(x_size // 5 + 1)],\n             label=""Full data set empirical covariance"", color=\'green\')\nplt.plot(range_n_outliers[(x_size // 5):(x_size // 2 - 1)],\n         err_cov_emp_full.mean(1)[(x_size // 5):(x_size // 2 - 1)],\n         color=\'green\', ls=\'--\')\nplt.errorbar(range_n_outliers, err_cov_emp_pure.mean(1),\n             yerr=err_cov_emp_pure.std(1),\n             label=""Pure data set empirical covariance"", color=\'black\')\nplt.title(""Influence of outliers on the covariance estimation"")\nplt.xlabel(""Amount of contamination (%)"")\nplt.ylabel(""RMSE"")\nplt.legend(loc=""upper center"", prop=font_prop)\n\nplt.show()\n'"
scikit-learn/_downloads/198bb9d6a4ac000311d4ee4606491271/plot_separating_hyperplane_unbalanced.py,4,"b'""""""\n=================================================\nSVM: Separating hyperplane for unbalanced classes\n=================================================\n\nFind the optimal separating hyperplane using an SVC for classes that\nare unbalanced.\n\nWe first find the separating plane with a plain SVC and then plot\n(dashed) the separating hyperplane with automatically correction for\nunbalanced classes.\n\n.. currentmodule:: sklearn.linear_model\n\n.. note::\n\n    This example will also work by replacing ``SVC(kernel=""linear"")``\n    with ``SGDClassifier(loss=""hinge"")``. Setting the ``loss`` parameter\n    of the :class:`SGDClassifier` equal to ``hinge`` will yield behaviour\n    such as that of a SVC with a linear kernel.\n\n    For example try instead of the ``SVC``::\n\n        clf = SGDClassifier(n_iter=100, alpha=0.01)\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs\n\n# we create two clusters of random points\nn_samples_1 = 1000\nn_samples_2 = 100\ncenters = [[0.0, 0.0], [2.0, 2.0]]\nclusters_std = [1.5, 0.5]\nX, y = make_blobs(n_samples=[n_samples_1, n_samples_2],\n                  centers=centers,\n                  cluster_std=clusters_std,\n                  random_state=0, shuffle=False)\n\n# fit the model and get the separating hyperplane\nclf = svm.SVC(kernel=\'linear\', C=1.0)\nclf.fit(X, y)\n\n# fit the model and get the separating hyperplane using weighted classes\nwclf = svm.SVC(kernel=\'linear\', class_weight={1: 10})\nwclf.fit(X, y)\n\n# plot the samples\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors=\'k\')\n\n# plot the decision functions for both classifiers\nax = plt.gca()\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# create grid to evaluate model\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\nYY, XX = np.meshgrid(yy, xx)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\n\n# get the separating hyperplane\nZ = clf.decision_function(xy).reshape(XX.shape)\n\n# plot decision boundary and margins\na = ax.contour(XX, YY, Z, colors=\'k\', levels=[0], alpha=0.5, linestyles=[\'-\'])\n\n# get the separating hyperplane for weighted classes\nZ = wclf.decision_function(xy).reshape(XX.shape)\n\n# plot decision boundary and margins for weighted classes\nb = ax.contour(XX, YY, Z, colors=\'r\', levels=[0], alpha=0.5, linestyles=[\'-\'])\n\nplt.legend([a.collections[0], b.collections[0]], [""non weighted"", ""weighted""],\n           loc=""upper right"")\nplt.show()\n'"
scikit-learn/_downloads/1b1ae1d791b1d5198ff84fc8ff0ddbaa/plot_sparse_logistic_regression_mnist.py,2,"b'""""""\n=====================================================\nMNIST classfification using multinomial logistic + L1\n=====================================================\n\nHere we fit a multinomial logistic regression with L1 penalty on a subset of\nthe MNIST digits classification task. We use the SAGA algorithm for this\npurpose: this a solver that is fast when the number of samples is significantly\nlarger than the number of features and is able to finely optimize non-smooth\nobjective functions which is the case with the l1-penalty. Test accuracy\nreaches > 0.8, while weight vectors remains *sparse* and therefore more easily\n*interpretable*.\n\nNote that this accuracy of this l1-penalized linear model is significantly\nbelow what can be reached by an l2-penalized linear model or a non-linear\nmulti-layer perceptron model on this dataset.\n\n""""""\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import check_random_state\n\nprint(__doc__)\n\n# Author: Arthur Mensch <arthur.mensch@m4x.org>\n# License: BSD 3 clause\n\n# Turn down for faster convergence\nt0 = time.time()\ntrain_samples = 5000\n\n# Load data from https://www.openml.org/d/554\nX, y = fetch_openml(\'mnist_784\', version=1, return_X_y=True)\n\nrandom_state = check_random_state(0)\npermutation = random_state.permutation(X.shape[0])\nX = X[permutation]\ny = y[permutation]\nX = X.reshape((X.shape[0], -1))\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=train_samples, test_size=10000)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Turn up tolerance for faster convergence\nclf = LogisticRegression(\n    C=50. / train_samples, penalty=\'l1\', solver=\'saga\', tol=0.1\n)\nclf.fit(X_train, y_train)\nsparsity = np.mean(clf.coef_ == 0) * 100\nscore = clf.score(X_test, y_test)\n# print(\'Best C % .4f\' % clf.C_)\nprint(""Sparsity with L1 penalty: %.2f%%"" % sparsity)\nprint(""Test score with L1 penalty: %.4f"" % score)\n\ncoef = clf.coef_.copy()\nplt.figure(figsize=(10, 5))\nscale = np.abs(coef).max()\nfor i in range(10):\n    l1_plot = plt.subplot(2, 5, i + 1)\n    l1_plot.imshow(coef[i].reshape(28, 28), interpolation=\'nearest\',\n                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n    l1_plot.set_xticks(())\n    l1_plot.set_yticks(())\n    l1_plot.set_xlabel(\'Class %i\' % i)\nplt.suptitle(\'Classification vector for...\')\n\nrun_time = time.time() - t0\nprint(\'Example run in %.3f s\' % run_time)\nplt.show()\n'"
scikit-learn/_downloads/1ba574b58cc44d5a7e54c155305a0289/plot_ridge_coeffs.py,1,"b'""""""\n==============================================================\nPlot Ridge coefficients as a function of the L2 regularization\n==============================================================\n\n.. currentmodule:: sklearn.linear_model\n\n:class:`Ridge` Regression is the estimator used in this example.\nEach color in the left plot represents one different dimension of the\ncoefficient vector, and this is displayed as a function of the\nregularization parameter. The right plot shows how exact the solution\nis. This example illustrates how a well defined solution is\nfound by Ridge regression and how regularization affects the\ncoefficients and their values. The plot on the right shows how\nthe difference of the coefficients from the estimator changes\nas a function of regularization.\n\nIn this example the dependent variable Y is set as a function\nof the input features: y = X*w + c. The coefficient vector w is\nrandomly sampled from a normal distribution, whereas the bias term c is\nset to a constant.\n\nAs alpha tends toward zero the coefficients found by Ridge\nregression stabilize towards the randomly sampled vector w.\nFor big alpha (strong regularisation) the coefficients\nare smaller (eventually converging at 0) leading to a\nsimpler and biased solution.\nThese dependencies can be observed on the left plot.\n\nThe right plot shows the mean squared error between the\ncoefficients found by the model and the chosen vector w.\nLess regularised models retrieve the exact\ncoefficients (error is equal to 0), stronger regularised\nmodels increase the error.\n\nPlease note that in this example the data is non-noisy, hence\nit is possible to extract the exact coefficients.\n""""""\n\n# Author: Kornel Kielczewski -- <kornel.k@plusnet.pl>\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nclf = Ridge()\n\nX, y, w = make_regression(n_samples=10, n_features=10, coef=True,\n                          random_state=1, bias=3.5)\n\ncoefs = []\nerrors = []\n\nalphas = np.logspace(-6, 6, 200)\n\n# Train the model with different regularisation strengths\nfor a in alphas:\n    clf.set_params(alpha=a)\n    clf.fit(X, y)\n    coefs.append(clf.coef_)\n    errors.append(mean_squared_error(clf.coef_, w))\n\n# Display results\nplt.figure(figsize=(20, 6))\n\nplt.subplot(121)\nax = plt.gca()\nax.plot(alphas, coefs)\nax.set_xscale(\'log\')\nplt.xlabel(\'alpha\')\nplt.ylabel(\'weights\')\nplt.title(\'Ridge coefficients as a function of the regularization\')\nplt.axis(\'tight\')\n\nplt.subplot(122)\nax = plt.gca()\nax.plot(alphas, errors)\nax.set_xscale(\'log\')\nplt.xlabel(\'alpha\')\nplt.ylabel(\'error\')\nplt.title(\'Coefficient error as a function of the regularization\')\nplt.axis(\'tight\')\n\nplt.show()\n'"
scikit-learn/_downloads/1ca4e0106dc9e298a293c956061be470/plot_validation_curve.py,5,"b'""""""\n==========================\nPlotting Validation Curves\n==========================\n\nIn this plot you can see the training scores and validation scores of an SVM\nfor different values of the kernel parameter gamma. For very low values of\ngamma, you can see that both the training score and the validation score are\nlow. This is called underfitting. Medium values of gamma will result in high\nvalues for both scores, i.e. the classifier is performing fairly well. If gamma\nis too high, the classifier will overfit, which means that the training score\nis good but the validation score is poor.\n""""""\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import validation_curve\n\nX, y = load_digits(return_X_y=True)\n\nparam_range = np.logspace(-6, -1, 5)\ntrain_scores, test_scores = validation_curve(\n    SVC(), X, y, param_name=""gamma"", param_range=param_range,\n    scoring=""accuracy"", n_jobs=1)\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.title(""Validation Curve with SVM"")\nplt.xlabel(r""$\\gamma$"")\nplt.ylabel(""Score"")\nplt.ylim(0.0, 1.1)\nlw = 2\nplt.semilogx(param_range, train_scores_mean, label=""Training score"",\n             color=""darkorange"", lw=lw)\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, alpha=0.2,\n                 color=""darkorange"", lw=lw)\nplt.semilogx(param_range, test_scores_mean, label=""Cross-validation score"",\n             color=""navy"", lw=lw)\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std, alpha=0.2,\n                 color=""navy"", lw=lw)\nplt.legend(loc=""best"")\nplt.show()\n'"
scikit-learn/_downloads/1cd34b42521bf6ec8ac384cda11ee2b5/plot_kmeans_digits.py,4,"b'""""""\n===========================================================\nA demo of K-Means clustering on the handwritten digits data\n===========================================================\n\nIn this example we compare the various initialization strategies for\nK-means in terms of runtime and quality of the results.\n\nAs the ground truth is known here, we also apply different cluster\nquality metrics to judge the goodness of fit of the cluster labels to the\nground truth.\n\nCluster quality metrics evaluated (see :ref:`clustering_evaluation` for\ndefinitions and discussions of the metrics):\n\n=========== ========================================================\nShorthand    full name\n=========== ========================================================\nhomo         homogeneity score\ncompl        completeness score\nv-meas       V measure\nARI          adjusted Rand index\nAMI          adjusted mutual information\nsilhouette   silhouette coefficient\n=========== ========================================================\n\n""""""\nprint(__doc__)\n\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import load_digits\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\n\nnp.random.seed(42)\n\nX_digits, y_digits = load_digits(return_X_y=True)\ndata = scale(X_digits)\n\nn_samples, n_features = data.shape\nn_digits = len(np.unique(y_digits))\nlabels = y_digits\n\nsample_size = 300\n\nprint(""n_digits: %d, \\t n_samples %d, \\t n_features %d""\n      % (n_digits, n_samples, n_features))\n\n\nprint(82 * \'_\')\nprint(\'init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette\')\n\n\ndef bench_k_means(estimator, name, data):\n    t0 = time()\n    estimator.fit(data)\n    print(\'%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\'\n          % (name, (time() - t0), estimator.inertia_,\n             metrics.homogeneity_score(labels, estimator.labels_),\n             metrics.completeness_score(labels, estimator.labels_),\n             metrics.v_measure_score(labels, estimator.labels_),\n             metrics.adjusted_rand_score(labels, estimator.labels_),\n             metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n             metrics.silhouette_score(data, estimator.labels_,\n                                      metric=\'euclidean\',\n                                      sample_size=sample_size)))\n\nbench_k_means(KMeans(init=\'k-means++\', n_clusters=n_digits, n_init=10),\n              name=""k-means++"", data=data)\n\nbench_k_means(KMeans(init=\'random\', n_clusters=n_digits, n_init=10),\n              name=""random"", data=data)\n\n# in this case the seeding of the centers is deterministic, hence we run the\n# kmeans algorithm only once with n_init=1\npca = PCA(n_components=n_digits).fit(data)\nbench_k_means(KMeans(init=pca.components_, n_clusters=n_digits, n_init=1),\n              name=""PCA-based"",\n              data=data)\nprint(82 * \'_\')\n\n# #############################################################################\n# Visualize the results on PCA-reduced data\n\nreduced_data = PCA(n_components=2).fit_transform(data)\nkmeans = KMeans(init=\'k-means++\', n_clusters=n_digits, n_init=10)\nkmeans.fit(reduced_data)\n\n# Step size of the mesh. Decrease to increase the quality of the VQ.\nh = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n\n# Plot the decision boundary. For that, we will assign a color to each\nx_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\ny_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Obtain labels for each point in mesh. Use last trained model.\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1)\nplt.clf()\nplt.imshow(Z, interpolation=\'nearest\',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect=\'auto\', origin=\'lower\')\n\nplt.plot(reduced_data[:, 0], reduced_data[:, 1], \'k.\', markersize=2)\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker=\'x\', s=169, linewidths=3,\n            color=\'w\', zorder=10)\nplt.title(\'K-means clustering on the digits dataset (PCA-reduced data)\\n\'\n          \'Centroids are marked with white cross\')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()\n'"
scikit-learn/_downloads/1d7a0ec3de2ee8ea660c1f23058ac0da/plot_mlp_training_curves.py,0,"b'""""""\n========================================================\nCompare Stochastic learning strategies for MLPClassifier\n========================================================\n\nThis example visualizes some training loss curves for different stochastic\nlearning strategies, including SGD and Adam. Because of time-constraints, we\nuse several small datasets, for which L-BFGS might be more suitable. The\ngeneral trend shown in these examples seems to carry over to larger datasets,\nhowever.\n\nNote that those results can be highly dependent on the value of\n``learning_rate_init``.\n""""""\n\nprint(__doc__)\n\nimport warnings\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import datasets\nfrom sklearn.exceptions import ConvergenceWarning\n\n# different learning rate schedules and momentum parameters\nparams = [{\'solver\': \'sgd\', \'learning_rate\': \'constant\', \'momentum\': 0,\n           \'learning_rate_init\': 0.2},\n          {\'solver\': \'sgd\', \'learning_rate\': \'constant\', \'momentum\': .9,\n           \'nesterovs_momentum\': False, \'learning_rate_init\': 0.2},\n          {\'solver\': \'sgd\', \'learning_rate\': \'constant\', \'momentum\': .9,\n           \'nesterovs_momentum\': True, \'learning_rate_init\': 0.2},\n          {\'solver\': \'sgd\', \'learning_rate\': \'invscaling\', \'momentum\': 0,\n           \'learning_rate_init\': 0.2},\n          {\'solver\': \'sgd\', \'learning_rate\': \'invscaling\', \'momentum\': .9,\n           \'nesterovs_momentum\': True, \'learning_rate_init\': 0.2},\n          {\'solver\': \'sgd\', \'learning_rate\': \'invscaling\', \'momentum\': .9,\n           \'nesterovs_momentum\': False, \'learning_rate_init\': 0.2},\n          {\'solver\': \'adam\', \'learning_rate_init\': 0.01}]\n\nlabels = [""constant learning-rate"", ""constant with momentum"",\n          ""constant with Nesterov\'s momentum"",\n          ""inv-scaling learning-rate"", ""inv-scaling with momentum"",\n          ""inv-scaling with Nesterov\'s momentum"", ""adam""]\n\nplot_args = [{\'c\': \'red\', \'linestyle\': \'-\'},\n             {\'c\': \'green\', \'linestyle\': \'-\'},\n             {\'c\': \'blue\', \'linestyle\': \'-\'},\n             {\'c\': \'red\', \'linestyle\': \'--\'},\n             {\'c\': \'green\', \'linestyle\': \'--\'},\n             {\'c\': \'blue\', \'linestyle\': \'--\'},\n             {\'c\': \'black\', \'linestyle\': \'-\'}]\n\n\ndef plot_on_dataset(X, y, ax, name):\n    # for each dataset, plot learning for each learning strategy\n    print(""\\nlearning on dataset %s"" % name)\n    ax.set_title(name)\n\n    X = MinMaxScaler().fit_transform(X)\n    mlps = []\n    if name == ""digits"":\n        # digits is larger but converges fairly quickly\n        max_iter = 15\n    else:\n        max_iter = 400\n\n    for label, param in zip(labels, params):\n        print(""training: %s"" % label)\n        mlp = MLPClassifier(random_state=0,\n                            max_iter=max_iter, **param)\n\n        # some parameter combinations will not converge as can be seen on the\n        # plots so they are ignored here\n        with warnings.catch_warnings():\n            warnings.filterwarnings(""ignore"", category=ConvergenceWarning,\n                                    module=""sklearn"")\n            mlp.fit(X, y)\n\n        mlps.append(mlp)\n        print(""Training set score: %f"" % mlp.score(X, y))\n        print(""Training set loss: %f"" % mlp.loss_)\n    for mlp, label, args in zip(mlps, labels, plot_args):\n        ax.plot(mlp.loss_curve_, label=label, **args)\n\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n# load / generate some toy datasets\niris = datasets.load_iris()\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\ndata_sets = [(iris.data, iris.target),\n             (X_digits, y_digits),\n             datasets.make_circles(noise=0.2, factor=0.5, random_state=1),\n             datasets.make_moons(noise=0.3, random_state=0)]\n\nfor ax, data, name in zip(axes.ravel(), data_sets, [\'iris\', \'digits\',\n                                                    \'circles\', \'moons\']):\n    plot_on_dataset(*data, ax=ax, name=name)\n\nfig.legend(ax.get_lines(), labels, ncol=3, loc=""upper center"")\nplt.show()\n'"
scikit-learn/_downloads/1e80fa38f2712d6ceda1d5ce742a9017/plot_partial_dependence_visualization_api.py,0,"b'""""""\n=========================================\nAdvanced Plotting With Partial Dependence\n=========================================\nThe :func:`~sklearn.inspection.plot_partial_dependence` function returns a\n:class:`~sklearn.inspection.PartialDependenceDisplay` object that can be used\nfor plotting without needing to recalculate the partial dependence. In this\nexample, we show how to plot partial dependence plots and how to quickly\ncustomize the plot with the visualization API.\n\n.. note::\n\n    See also :ref:`sphx_glr_auto_examples_plot_roc_curve_visualization_api.py`\n\n""""""\nprint(__doc__)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_boston\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.inspection import plot_partial_dependence\n\n\n##############################################################################\n# Train models on the boston housing price dataset\n# ================================================\n#\n# First, we train a decision tree and a multi-layer perceptron on the boston\n# housing price dataset.\n\nboston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\n\ntree = DecisionTreeRegressor()\nmlp = make_pipeline(StandardScaler(),\n                    MLPRegressor(hidden_layer_sizes=(100, 100),\n                                 tol=1e-2, max_iter=500, random_state=0))\ntree.fit(X, y)\nmlp.fit(X, y)\n\n\n##############################################################################\n# Plotting partial dependence for two features\n# ============================================\n#\n# We plot partial dependence curves for features ""LSTAT"" and ""RM"" for\n# the decision tree. With two features,\n# :func:`~sklearn.inspection.plot_partial_dependence` expects to plot two\n# curves. Here the plot function place a grid of two plots using the space\n# defined by `ax` .\nfig, ax = plt.subplots(figsize=(12, 6))\nax.set_title(""Decision Tree"")\ntree_disp = plot_partial_dependence(tree, X, [""LSTAT"", ""RM""], ax=ax)\n\n##############################################################################\n# The partial depdendence curves can be plotted for the multi-layer perceptron.\n# In this case, `line_kw` is passed to\n# :func:`~sklearn.inspection.plot_partial_dependence` to change the color of\n# the curve.\nfig, ax = plt.subplots(figsize=(12, 6))\nax.set_title(""Multi-layer Perceptron"")\nmlp_disp = plot_partial_dependence(mlp, X, [""LSTAT"", ""RM""], ax=ax,\n                                   line_kw={""c"": ""red""})\n\n##############################################################################\n# Plotting partial dependence of the two models together\n# ======================================================\n#\n# The `tree_disp` and `mlp_disp`\n# :class:`~sklearn.inspection.PartialDependenceDisplay` objects contain all the\n# computed information needed to recreate the partial dependence curves. This\n# means we can easily create additional plots without needing to recompute the\n# curves.\n#\n# One way to plot the curves is to place them in the same figure, with the\n# curves of each model on each row. First, we create a figure with two axes\n# within two rows and one column. The two axes are passed to the\n# :func:`~sklearn.inspection.PartialDependenceDisplay.plot` functions of\n# `tree_disp` and `mlp_disp`. The given axes will be used by the plotting\n# function to draw the partial dependence. The resulting plot places the\n# decision tree partial dependence curves in the first row of the\n# multi-layer perceptron in the second row.\n\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\ntree_disp.plot(ax=ax1)\nax1.set_title(""Decision Tree"")\nmlp_disp.plot(ax=ax2, line_kw={""c"": ""red""})\nax2.set_title(""Multi-layer Perceptron"")\n\n##############################################################################\n# Another way to compare the curves is to plot them on top of each other. Here,\n# we create a figure with one row and two columns. The axes are passed into the\n# :func:`~sklearn.inspection.PartialDependenceDisplay.plot` function as a list,\n# which will plot the partial dependence curves of each model on the same axes.\n# The length of the axes list must be equal to the number of plots drawn.\n\n# Sets this image as the thumbnail for sphinx gallery\n# sphinx_gallery_thumbnail_number = 4\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))\ntree_disp.plot(ax=[ax1, ax2], line_kw={""label"": ""Decision Tree""})\nmlp_disp.plot(ax=[ax1, ax2], line_kw={""label"": ""Multi-layer Perceptron"",\n                                      ""c"": ""red""})\nax1.legend()\nax2.legend()\n\n##############################################################################\n# `tree_disp.axes_` is a numpy array container the axes used to draw the\n# partial dependence plots. This can be passed to `mlp_disp` to have the same\n# affect of drawing the plots on top of each other. Furthermore, the\n# `mlp_disp.figure_` stores the figure, which allows for resizing the figure\n# after calling `plot`. In this case `tree_disp.axes_` has two dimensions, thus\n# `plot` will only show the y label and y ticks on the left most plot.\n\ntree_disp.plot(line_kw={""label"": ""Decision Tree""})\nmlp_disp.plot(line_kw={""label"": ""Multi-layer Perceptron"", ""c"": ""red""},\n              ax=tree_disp.axes_)\ntree_disp.figure_.set_size_inches(10, 6)\ntree_disp.axes_[0, 0].legend()\ntree_disp.axes_[0, 1].legend()\nplt.show()\n\n\n##############################################################################\n# Plotting partial dependence for one feature\n# ===========================================\n#\n# Here, we plot the partial dependence curves for a single feature, ""LSTAT"", on\n# the same axes. In this case, `tree_disp.axes_` is passed into the second\n# plot function.\ntree_disp = plot_partial_dependence(tree, X, [""LSTAT""])\nmlp_disp = plot_partial_dependence(mlp, X, [""LSTAT""],\n                                   ax=tree_disp.axes_, line_kw={""c"": ""red""})\n'"
scikit-learn/_downloads/1edc7a95f1e23fb10b4902de17bf4c9b/plot_agglomerative_clustering.py,6,"b'""""""\nAgglomerative clustering with and without structure\n===================================================\n\nThis example shows the effect of imposing a connectivity graph to capture\nlocal structure in the data. The graph is simply the graph of 20 nearest\nneighbors.\n\nTwo consequences of imposing a connectivity can be seen. First clustering\nwith a connectivity matrix is much faster.\n\nSecond, when using a connectivity matrix, single, average and complete\nlinkage are unstable and tend to create a few clusters that grow very\nquickly. Indeed, average and complete linkage fight this percolation behavior\nby considering all the distances between two clusters when merging them (\nwhile single linkage exaggerates the behaviour by considering only the\nshortest distance between clusters). The connectivity graph breaks this\nmechanism for average and complete linkage, making them resemble the more\nbrittle single linkage. This effect is more pronounced for very sparse graphs\n(try decreasing the number of neighbors in kneighbors_graph) and with\ncomplete linkage. In particular, having a very small number of neighbors in\nthe graph, imposes a geometry that is close to that of single linkage,\nwhich is well known to have this percolation instability. """"""\n# Authors: Gael Varoquaux, Nelle Varoquaux\n# License: BSD 3 clause\n\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.neighbors import kneighbors_graph\n\n# Generate sample data\nn_samples = 1500\nnp.random.seed(0)\nt = 1.5 * np.pi * (1 + 3 * np.random.rand(1, n_samples))\nx = t * np.cos(t)\ny = t * np.sin(t)\n\n\nX = np.concatenate((x, y))\nX += .7 * np.random.randn(2, n_samples)\nX = X.T\n\n# Create a graph capturing local connectivity. Larger number of neighbors\n# will give more homogeneous clusters to the cost of computation\n# time. A very large number of neighbors gives more evenly distributed\n# cluster sizes, but may not impose the local manifold structure of\n# the data\nknn_graph = kneighbors_graph(X, 30, include_self=False)\n\nfor connectivity in (None, knn_graph):\n    for n_clusters in (30, 3):\n        plt.figure(figsize=(10, 4))\n        for index, linkage in enumerate((\'average\',\n                                         \'complete\',\n                                         \'ward\',\n                                         \'single\')):\n            plt.subplot(1, 4, index + 1)\n            model = AgglomerativeClustering(linkage=linkage,\n                                            connectivity=connectivity,\n                                            n_clusters=n_clusters)\n            t0 = time.time()\n            model.fit(X)\n            elapsed_time = time.time() - t0\n            plt.scatter(X[:, 0], X[:, 1], c=model.labels_,\n                        cmap=plt.cm.nipy_spectral)\n            plt.title(\'linkage=%s\\n(time %.2fs)\' % (linkage, elapsed_time),\n                      fontdict=dict(verticalalignment=\'top\'))\n            plt.axis(\'equal\')\n            plt.axis(\'off\')\n\n            plt.subplots_adjust(bottom=0, top=.89, wspace=0,\n                                left=0, right=1)\n            plt.suptitle(\'n_cluster=%i, connectivity=%r\' %\n                         (n_clusters, connectivity is not None), size=17)\n\n\nplt.show()\n'"
scikit-learn/_downloads/1fc5f08e03ddd91e91f4642d46ba0640/plot_gradient_boosting_regularization.py,4,"b'""""""\n================================\nGradient Boosting regularization\n================================\n\nIllustration of the effect of different regularization strategies\nfor Gradient Boosting. The example is taken from Hastie et al 2009 [1]_.\n\nThe loss function used is binomial deviance. Regularization via\nshrinkage (``learning_rate < 1.0``) improves performance considerably.\nIn combination with shrinkage, stochastic gradient boosting\n(``subsample < 1.0``) can produce more accurate models by reducing the\nvariance via bagging.\nSubsampling without shrinkage usually does poorly.\nAnother strategy to reduce the variance is by subsampling the features\nanalogous to the random splits in Random Forests\n(via the ``max_features`` parameter).\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, ""Elements of Statistical\n    Learning Ed. 2"", Springer, 2009.\n""""""\nprint(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\n\n\nX, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\nX = X.astype(np.float32)\n\n# map labels from {-1, 1} to {0, 1}\nlabels, y = np.unique(y, return_inverse=True)\n\nX_train, X_test = X[:2000], X[2000:]\ny_train, y_test = y[:2000], y[2000:]\n\noriginal_params = {\'n_estimators\': 1000, \'max_leaf_nodes\': 4, \'max_depth\': None, \'random_state\': 2,\n                   \'min_samples_split\': 5}\n\nplt.figure()\n\nfor label, color, setting in [(\'No shrinkage\', \'orange\',\n                               {\'learning_rate\': 1.0, \'subsample\': 1.0}),\n                              (\'learning_rate=0.1\', \'turquoise\',\n                               {\'learning_rate\': 0.1, \'subsample\': 1.0}),\n                              (\'subsample=0.5\', \'blue\',\n                               {\'learning_rate\': 1.0, \'subsample\': 0.5}),\n                              (\'learning_rate=0.1, subsample=0.5\', \'gray\',\n                               {\'learning_rate\': 0.1, \'subsample\': 0.5}),\n                              (\'learning_rate=0.1, max_features=2\', \'magenta\',\n                               {\'learning_rate\': 0.1, \'max_features\': 2})]:\n    params = dict(original_params)\n    params.update(setting)\n\n    clf = ensemble.GradientBoostingClassifier(**params)\n    clf.fit(X_train, y_train)\n\n    # compute test set deviance\n    test_deviance = np.zeros((params[\'n_estimators\'],), dtype=np.float64)\n\n    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n        # clf.loss_ assumes that y_test[i] in {0, 1}\n        test_deviance[i] = clf.loss_(y_test, y_pred)\n\n    plt.plot((np.arange(test_deviance.shape[0]) + 1)[::5], test_deviance[::5],\n            \'-\', color=color, label=label)\n\nplt.legend(loc=\'upper left\')\nplt.xlabel(\'Boosting Iterations\')\nplt.ylabel(\'Test Set Deviance\')\n\nplt.show()\n'"
scikit-learn/_downloads/201bec2fe83074e59ebbb3adba1fbcdf/plot_manifold_sphere.py,7,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=============================================\nManifold Learning methods on a severed sphere\n=============================================\n\nAn application of the different :ref:`manifold` techniques\non a spherical data-set. Here one can see the use of\ndimensionality reduction in order to gain some intuition\nregarding the manifold learning methods. Regarding the dataset,\nthe poles are cut from the sphere, as well as a thin slice down its\nside. This enables the manifold learning techniques to\n\'spread it open\' whilst projecting it onto two dimensions.\n\nFor a similar example, where the methods are applied to the\nS-curve dataset, see :ref:`sphx_glr_auto_examples_manifold_plot_compare_methods.py`\n\nNote that the purpose of the :ref:`MDS <multidimensional_scaling>` is\nto find a low-dimensional representation of the data (here 2D) in\nwhich the distances respect well the distances in the original\nhigh-dimensional space, unlike other manifold-learning algorithms,\nit does not seeks an isotropic representation of the data in\nthe low-dimensional space. Here the manifold problem matches fairly\nthat of representing a flat map of the Earth, as with\n`map projection <https://en.wikipedia.org/wiki/Map_projection>`_\n""""""\n\n# Author: Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold\nfrom sklearn.utils import check_random_state\n\n# Next line to silence pyflakes.\nAxes3D\n\n# Variables for manifold learning.\nn_neighbors = 10\nn_samples = 1000\n\n# Create our sphere.\nrandom_state = check_random_state(0)\np = random_state.rand(n_samples) * (2 * np.pi - 0.55)\nt = random_state.rand(n_samples) * np.pi\n\n# Sever the poles from the sphere.\nindices = ((t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8))))\ncolors = p[indices]\nx, y, z = np.sin(t[indices]) * np.cos(p[indices]), \\\n    np.sin(t[indices]) * np.sin(p[indices]), \\\n    np.cos(t[indices])\n\n# Plot our dataset.\nfig = plt.figure(figsize=(15, 8))\nplt.suptitle(""Manifold Learning with %i points, %i neighbors""\n             % (1000, n_neighbors), fontsize=14)\n\nax = fig.add_subplot(251, projection=\'3d\')\nax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)\nax.view_init(40, -10)\n\nsphere_data = np.array([x, y, z]).T\n\n# Perform Locally Linear Embedding Manifold learning\nmethods = [\'standard\', \'ltsa\', \'hessian\', \'modified\']\nlabels = [\'LLE\', \'LTSA\', \'Hessian LLE\', \'Modified LLE\']\n\nfor i, method in enumerate(methods):\n    t0 = time()\n    trans_data = manifold\\\n        .LocallyLinearEmbedding(n_neighbors, 2,\n                                method=method).fit_transform(sphere_data).T\n    t1 = time()\n    print(""%s: %.2g sec"" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n    plt.title(""%s (%.2g sec)"" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis(\'tight\')\n\n# Perform Isomap Manifold learning.\nt0 = time()\ntrans_data = manifold.Isomap(n_neighbors, n_components=2)\\\n    .fit_transform(sphere_data).T\nt1 = time()\nprint(""%s: %.2g sec"" % (\'ISO\', t1 - t0))\n\nax = fig.add_subplot(257)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title(""%s (%.2g sec)"" % (\'Isomap\', t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\n# Perform Multi-dimensional scaling.\nt0 = time()\nmds = manifold.MDS(2, max_iter=100, n_init=1)\ntrans_data = mds.fit_transform(sphere_data).T\nt1 = time()\nprint(""MDS: %.2g sec"" % (t1 - t0))\n\nax = fig.add_subplot(258)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title(""MDS (%.2g sec)"" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\n# Perform Spectral Embedding.\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=2,\n                                n_neighbors=n_neighbors)\ntrans_data = se.fit_transform(sphere_data).T\nt1 = time()\nprint(""Spectral Embedding: %.2g sec"" % (t1 - t0))\n\nax = fig.add_subplot(259)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title(""Spectral Embedding (%.2g sec)"" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\n# Perform t-distributed stochastic neighbor embedding.\nt0 = time()\ntsne = manifold.TSNE(n_components=2, init=\'pca\', random_state=0)\ntrans_data = tsne.fit_transform(sphere_data).T\nt1 = time()\nprint(""t-SNE: %.2g sec"" % (t1 - t0))\n\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title(""t-SNE (%.2g sec)"" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\nplt.show()\n'"
scikit-learn/_downloads/217d3c4d92276aef3ae6ca22ac03355a/plot_compare_reduction.py,2,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n""""""\n=================================================================\nSelecting dimensionality reduction with Pipeline and GridSearchCV\n=================================================================\n\nThis example constructs a pipeline that does dimensionality\nreduction followed by prediction with a support vector\nclassifier. It demonstrates the use of ``GridSearchCV`` and\n``Pipeline`` to optimize over different classes of estimators in a\nsingle CV run -- unsupervised ``PCA`` and ``NMF`` dimensionality\nreductions are compared to univariate feature selection during\nthe grid search.\n\nAdditionally, ``Pipeline`` can be instantiated with the ``memory``\nargument to memoize the transformers within the pipeline, avoiding to fit\nagain the same transformers over and over.\n\nNote that the use of ``memory`` to enable caching becomes interesting when the\nfitting of a transformer is costly.\n\n###############################################################################\nIllustration of ``Pipeline`` and ``GridSearchCV``\n###############################################################################\n\nThis section illustrates the use of a ``Pipeline`` with ``GridSearchCV``\n""""""\n\n# Authors: Robert McGibbon, Joel Nothman, Guillaume Lemaitre\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import PCA, NMF\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nprint(__doc__)\n\npipe = Pipeline([\n    # the reduce_dim stage is populated by the param_grid\n    (\'reduce_dim\', \'passthrough\'),\n    (\'classify\', LinearSVC(dual=False, max_iter=10000))\n])\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \'reduce_dim\': [PCA(iterated_power=7), NMF()],\n        \'reduce_dim__n_components\': N_FEATURES_OPTIONS,\n        \'classify__C\': C_OPTIONS\n    },\n    {\n        \'reduce_dim\': [SelectKBest(chi2)],\n        \'reduce_dim__k\': N_FEATURES_OPTIONS,\n        \'classify__C\': C_OPTIONS\n    },\n]\nreducer_labels = [\'PCA\', \'NMF\', \'KBest(chi2)\']\n\ngrid = GridSearchCV(pipe, n_jobs=1, param_grid=param_grid)\nX, y = load_digits(return_X_y=True)\ngrid.fit(X, y)\n\nmean_scores = np.array(grid.cv_results_[\'mean_test_score\'])\n# scores are in the order of param_grid iteration, which is alphabetical\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n# select score for best C\nmean_scores = mean_scores.max(axis=0)\nbar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n               (len(reducer_labels) + 1) + .5)\n\nplt.figure()\nCOLORS = \'bgrcmyk\'\nfor i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n\nplt.title(""Comparing feature reduction techniques"")\nplt.xlabel(\'Reduced number of features\')\nplt.xticks(bar_offsets + len(reducer_labels) / 2, N_FEATURES_OPTIONS)\nplt.ylabel(\'Digit classification accuracy\')\nplt.ylim((0, 1))\nplt.legend(loc=\'upper left\')\n\nplt.show()\n\n###############################################################################\n# Caching transformers within a ``Pipeline``\n###############################################################################\n# It is sometimes worthwhile storing the state of a specific transformer\n# since it could be used again. Using a pipeline in ``GridSearchCV`` triggers\n# such situations. Therefore, we use the argument ``memory`` to enable caching.\n#\n# .. warning::\n#     Note that this example is, however, only an illustration since for this\n#     specific case fitting PCA is not necessarily slower than loading the\n#     cache. Hence, use the ``memory`` constructor parameter when the fitting\n#     of a transformer is costly.\n\nfrom joblib import Memory\nfrom shutil import rmtree\n\n# Create a temporary folder to store the transformers of the pipeline\nlocation = \'cachedir\'\nmemory = Memory(location=location, verbose=10)\ncached_pipe = Pipeline([(\'reduce_dim\', PCA()),\n                        (\'classify\', LinearSVC(dual=False, max_iter=10000))],\n                       memory=memory)\n\n# This time, a cached pipeline will be used within the grid search\n\n\n# Delete the temporary cache before exiting\nmemory.clear(warn=False)\nrmtree(location)\n\n###############################################################################\n# The ``PCA`` fitting is only computed at the evaluation of the first\n# configuration of the ``C`` parameter of the ``LinearSVC`` classifier. The\n# other configurations of ``C`` will trigger the loading of the cached ``PCA``\n# estimator data, leading to save processing time. Therefore, the use of\n# caching the pipeline using ``memory`` is highly beneficial when fitting\n# a transformer is costly.\n'"
scikit-learn/_downloads/21b67b43a1399611f03bc2315a82e440/plot_sgd_early_stopping.py,2,"b'""""""\n=============================================\nEarly stopping of Stochastic Gradient Descent\n=============================================\n\nStochastic Gradient Descent is an optimization technique which minimizes a loss\nfunction in a stochastic fashion, performing a gradient descent step sample by\nsample. In particular, it is a very efficient method to fit linear models.\n\nAs a stochastic method, the loss function is not necessarily decreasing at each\niteration, and convergence is only guaranteed in expectation. For this reason,\nmonitoring the convergence on the loss function can be difficult.\n\nAnother approach is to monitor convergence on a validation score. In this case,\nthe input data is split into a training set and a validation set. The model is\nthen fitted on the training set and the stopping criterion is based on the\nprediction score computed on the validation set. This enables us to find the\nleast number of iterations which is sufficient to build a model that\ngeneralizes well to unseen data and reduces the chance of over-fitting the\ntraining data.\n\nThis early stopping strategy is activated if ``early_stopping=True``; otherwise\nthe stopping criterion only uses the training loss on the entire input data. To\nbetter control the early stopping strategy, we can specify a parameter\n``validation_fraction`` which set the fraction of the input dataset that we\nkeep aside to compute the validation score. The optimization will continue\nuntil the validation score did not improve by at least ``tol`` during the last\n``n_iter_no_change`` iterations. The actual number of iterations is available\nat the attribute ``n_iter_``.\n\nThis example illustrates how the early stopping can used in the\n:class:`sklearn.linear_model.SGDClassifier` model to achieve almost the same\naccuracy as compared to a model built without early stopping. This can\nsignificantly reduce training time. Note that scores differ between the\nstopping criteria even from early iterations because some of the training data\nis held out with the validation stopping criterion.\n""""""\n# Authors: Tom Dupre la Tour\n#\n# License: BSD 3 clause\nimport time\nimport sys\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.utils import shuffle\n\nprint(__doc__)\n\n\ndef load_mnist(n_samples=None, class_0=\'0\', class_1=\'8\'):\n    """"""Load MNIST, select two classes, shuffle and return only n_samples.""""""\n    # Load data from http://openml.org/d/554\n    mnist = fetch_openml(\'mnist_784\', version=1)\n\n    # take only two classes for binary classification\n    mask = np.logical_or(mnist.target == class_0, mnist.target == class_1)\n\n    X, y = shuffle(mnist.data[mask], mnist.target[mask], random_state=42)\n    if n_samples is not None:\n        X, y = X[:n_samples], y[:n_samples]\n    return X, y\n\n\n@ignore_warnings(category=ConvergenceWarning)\ndef fit_and_score(estimator, max_iter, X_train, X_test, y_train, y_test):\n    """"""Fit the estimator on the train set and score it on both sets""""""\n    estimator.set_params(max_iter=max_iter)\n    estimator.set_params(random_state=0)\n\n    start = time.time()\n    estimator.fit(X_train, y_train)\n\n    fit_time = time.time() - start\n    n_iter = estimator.n_iter_\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return fit_time, n_iter, train_score, test_score\n\n\n# Define the estimators to compare\nestimator_dict = {\n    \'No stopping criterion\':\n    linear_model.SGDClassifier(n_iter_no_change=3),\n    \'Training loss\':\n    linear_model.SGDClassifier(early_stopping=False, n_iter_no_change=3,\n                               tol=0.1),\n    \'Validation score\':\n    linear_model.SGDClassifier(early_stopping=True, n_iter_no_change=3,\n                               tol=0.0001, validation_fraction=0.2)\n}\n\n# Load the dataset\nX, y = load_mnist(n_samples=10000)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n                                                    random_state=0)\n\nresults = []\nfor estimator_name, estimator in estimator_dict.items():\n    print(estimator_name + \': \', end=\'\')\n    for max_iter in range(1, 50):\n        print(\'.\', end=\'\')\n        sys.stdout.flush()\n\n        fit_time, n_iter, train_score, test_score = fit_and_score(\n            estimator, max_iter, X_train, X_test, y_train, y_test)\n\n        results.append((estimator_name, max_iter, fit_time, n_iter,\n                        train_score, test_score))\n    print(\'\')\n\n# Transform the results in a pandas dataframe for easy plotting\ncolumns = [\n    \'Stopping criterion\', \'max_iter\', \'Fit time (sec)\', \'n_iter_\',\n    \'Train score\', \'Test score\'\n]\nresults_df = pd.DataFrame(results, columns=columns)\n\n# Define what to plot (x_axis, y_axis)\nlines = \'Stopping criterion\'\nplot_list = [\n    (\'max_iter\', \'Train score\'),\n    (\'max_iter\', \'Test score\'),\n    (\'max_iter\', \'n_iter_\'),\n    (\'max_iter\', \'Fit time (sec)\'),\n]\n\nnrows = 2\nncols = int(np.ceil(len(plot_list) / 2.))\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(6 * ncols,\n                                                            4 * nrows))\naxes[0, 0].get_shared_y_axes().join(axes[0, 0], axes[0, 1])\n\nfor ax, (x_axis, y_axis) in zip(axes.ravel(), plot_list):\n    for criterion, group_df in results_df.groupby(lines):\n        group_df.plot(x=x_axis, y=y_axis, label=criterion, ax=ax)\n    ax.set_title(y_axis)\n    ax.legend(title=lines)\n\nfig.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/2236d9cd2d5df53f248c72f8cc55b50f/plot_omp.py,1,"b'""""""\n===========================\nOrthogonal Matching Pursuit\n===========================\n\nUsing orthogonal matching pursuit for recovering a sparse signal from a noisy\nmeasurement encoded with a dictionary\n""""""\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\nfrom sklearn.linear_model import OrthogonalMatchingPursuitCV\nfrom sklearn.datasets import make_sparse_coded_signal\n\nn_components, n_features = 512, 100\nn_nonzero_coefs = 17\n\n# generate the data\n\n# y = Xw\n# |x|_0 = n_nonzero_coefs\n\ny, X, w = make_sparse_coded_signal(n_samples=1,\n                                   n_components=n_components,\n                                   n_features=n_features,\n                                   n_nonzero_coefs=n_nonzero_coefs,\n                                   random_state=0)\n\nidx, = w.nonzero()\n\n# distort the clean signal\ny_noisy = y + 0.05 * np.random.randn(len(y))\n\n# plot the sparse signal\nplt.figure(figsize=(7, 7))\nplt.subplot(4, 1, 1)\nplt.xlim(0, 512)\nplt.title(""Sparse signal"")\nplt.stem(idx, w[idx], use_line_collection=True)\n\n# plot the noise-free reconstruction\nomp = OrthogonalMatchingPursuit(n_nonzero_coefs=n_nonzero_coefs)\nomp.fit(X, y)\ncoef = omp.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 2)\nplt.xlim(0, 512)\nplt.title(""Recovered signal from noise-free measurements"")\nplt.stem(idx_r, coef[idx_r], use_line_collection=True)\n\n# plot the noisy reconstruction\nomp.fit(X, y_noisy)\ncoef = omp.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 3)\nplt.xlim(0, 512)\nplt.title(""Recovered signal from noisy measurements"")\nplt.stem(idx_r, coef[idx_r], use_line_collection=True)\n\n# plot the noisy reconstruction with number of non-zeros set by CV\nomp_cv = OrthogonalMatchingPursuitCV()\nomp_cv.fit(X, y_noisy)\ncoef = omp_cv.coef_\nidx_r, = coef.nonzero()\nplt.subplot(4, 1, 4)\nplt.xlim(0, 512)\nplt.title(""Recovered signal from noisy measurements with CV"")\nplt.stem(idx_r, coef[idx_r], use_line_collection=True)\n\nplt.subplots_adjust(0.06, 0.04, 0.94, 0.90, 0.20, 0.38)\nplt.suptitle(\'Sparse signal recovery with Orthogonal Matching Pursuit\',\n             fontsize=16)\nplt.show()\n'"
scikit-learn/_downloads/22a2d860425472cd24ca7e752479a789/plot_gradient_boosting_regression.py,6,"b'""""""\n============================\nGradient Boosting regression\n============================\n\nDemonstrate Gradient Boosting on the Boston housing dataset.\n\nThis example fits a Gradient Boosting model with least squares loss and\n500 regression trees of depth 4.\n""""""\nprint(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\n\n# #############################################################################\n# Load data\nboston = datasets.load_boston()\nX, y = shuffle(boston.data, boston.target, random_state=13)\nX = X.astype(np.float32)\noffset = int(X.shape[0] * 0.9)\nX_train, y_train = X[:offset], y[:offset]\nX_test, y_test = X[offset:], y[offset:]\n\n# #############################################################################\n# Fit regression model\nparams = {\'n_estimators\': 500, \'max_depth\': 4, \'min_samples_split\': 2,\n          \'learning_rate\': 0.01, \'loss\': \'ls\'}\nclf = ensemble.GradientBoostingRegressor(**params)\n\nclf.fit(X_train, y_train)\nmse = mean_squared_error(y_test, clf.predict(X_test))\nprint(""MSE: %.4f"" % mse)\n\n# #############################################################################\n# Plot training deviance\n\n# compute test set deviance\ntest_score = np.zeros((params[\'n_estimators\'],), dtype=np.float64)\n\nfor i, y_pred in enumerate(clf.staged_predict(X_test)):\n    test_score[i] = clf.loss_(y_test, y_pred)\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\'Deviance\')\nplt.plot(np.arange(params[\'n_estimators\']) + 1, clf.train_score_, \'b-\',\n         label=\'Training Set Deviance\')\nplt.plot(np.arange(params[\'n_estimators\']) + 1, test_score, \'r-\',\n         label=\'Test Set Deviance\')\nplt.legend(loc=\'upper right\')\nplt.xlabel(\'Boosting Iterations\')\nplt.ylabel(\'Deviance\')\n\n# #############################################################################\n# Plot feature importance\nfeature_importance = clf.feature_importances_\n# make importances relative to max importance\nfeature_importance = 100.0 * (feature_importance / feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.subplot(1, 2, 2)\nplt.barh(pos, feature_importance[sorted_idx], align=\'center\')\nplt.yticks(pos, boston.feature_names[sorted_idx])\nplt.xlabel(\'Relative Importance\')\nplt.title(\'Variable Importance\')\nplt.show()\n'"
scikit-learn/_downloads/22baf723d7f8d4b5c6172b13dca263c0/plot_select_from_model_boston.py,1,"b'""""""\n===================================================\nFeature selection using SelectFromModel and LassoCV\n===================================================\n\nUse SelectFromModel meta-transformer along with Lasso to select the best\ncouple of features from the Boston dataset.\n""""""\n# Author: Manoj Kumar <mks542@nyu.edu>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LassoCV\n\n# Load the boston dataset.\nX, y = load_boston(return_X_y=True)\n\n# We use the base estimator LassoCV since the L1 norm promotes sparsity of features.\nclf = LassoCV()\n\n# Set a minimum threshold of 0.25\nsfm = SelectFromModel(clf, threshold=0.25)\nsfm.fit(X, y)\nn_features = sfm.transform(X).shape[1]\n\n# Reset the threshold till the number of features equals two.\n# Note that the attribute can be set directly instead of repeatedly\n# fitting the metatransformer.\nwhile n_features > 2:\n    sfm.threshold += 0.1\n    X_transform = sfm.transform(X)\n    n_features = X_transform.shape[1]\n\n# Plot the selected two features from X.\nplt.title(\n    ""Features selected from Boston using SelectFromModel with ""\n    ""threshold %0.3f."" % sfm.threshold)\nfeature1 = X_transform[:, 0]\nfeature2 = X_transform[:, 1] \nplt.plot(feature1, feature2, \'r.\')\nplt.xlabel(""Feature number 1"")\nplt.ylabel(""Feature number 2"")\nplt.ylim([np.min(feature2), np.max(feature2)])\nplt.show()\n'"
scikit-learn/_downloads/22d5a9f2d9d599bcb61067f28ea3c114/plot_tree_regression_multioutput.py,4,"b'""""""\n===================================================================\nMulti-output Decision Tree Regression\n===================================================================\n\nAn example to illustrate multi-output regression with decision tree.\n\nThe :ref:`decision trees <tree>`\nis used to predict simultaneously the noisy x and y observations of a circle\ngiven a single underlying feature. As a result, it learns local linear\nregressions approximating the circle.\n\nWe can see that if the maximum depth of the tree (controlled by the\n`max_depth` parameter) is set too high, the decision trees learn too fine\ndetails of the training data and learn from the noise, i.e. they overfit.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(200 * rng.rand(100, 1) - 100, axis=0)\ny = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\ny[::5, :] += (0.5 - rng.rand(20, 2))\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_3 = DecisionTreeRegressor(max_depth=8)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\nregr_3.fit(X, y)\n\n# Predict\nX_test = np.arange(-100.0, 100.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\ny_3 = regr_3.predict(X_test)\n\n# Plot the results\nplt.figure()\ns = 25\nplt.scatter(y[:, 0], y[:, 1], c=""navy"", s=s,\n            edgecolor=""black"", label=""data"")\nplt.scatter(y_1[:, 0], y_1[:, 1], c=""cornflowerblue"", s=s,\n            edgecolor=""black"", label=""max_depth=2"")\nplt.scatter(y_2[:, 0], y_2[:, 1], c=""red"", s=s,\n            edgecolor=""black"", label=""max_depth=5"")\nplt.scatter(y_3[:, 0], y_3[:, 1], c=""orange"", s=s,\n            edgecolor=""black"", label=""max_depth=8"")\nplt.xlim([-6, 6])\nplt.ylim([-6, 6])\nplt.xlabel(""target 1"")\nplt.ylabel(""target 2"")\nplt.title(""Multi-output Decision Tree Regression"")\nplt.legend(loc=""best"")\nplt.show()\n'"
scikit-learn/_downloads/230c308621e75a6bed6282675aa7c43c/plot_custom_kernel.py,4,"b'""""""\n======================\nSVM with custom kernel\n======================\n\nSimple usage of Support Vector Machines to classify a sample. It will\nplot the decision surface and the support vectors.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm, datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\nY = iris.target\n\n\ndef my_kernel(X, Y):\n    """"""\n    We create a custom kernel:\n\n                 (2  0)\n    k(X, Y) = X  (    ) Y.T\n                 (0  1)\n    """"""\n    M = np.array([[2, 0], [0, 1.0]])\n    return np.dot(np.dot(X, M), Y.T)\n\n\nh = .02  # step size in the mesh\n\n# we create an instance of SVM and fit out data.\nclf = svm.SVC(kernel=my_kernel)\nclf.fit(X, Y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors=\'k\')\nplt.title(\'3-Class classification using Support Vector Machine with custom\'\n          \' kernel\')\nplt.axis(\'tight\')\nplt.show()\n'"
scikit-learn/_downloads/242c4cdc940eca863a764cc078db886c/plot_lasso_dense_vs_sparse_data.py,0,"b'""""""\n==============================\nLasso on dense and sparse data\n==============================\n\nWe show that linear_model.Lasso provides the same results for dense and sparse\ndata and that in the case of sparse data the speed is improved.\n\n""""""\nprint(__doc__)\n\nfrom time import time\nfrom scipy import sparse\nfrom scipy import linalg\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import Lasso\n\n\n# #############################################################################\n# The two Lasso implementations on Dense data\nprint(""--- Dense matrices"")\n\nX, y = make_regression(n_samples=200, n_features=5000, random_state=0)\nX_sp = sparse.coo_matrix(X)\n\nalpha = 1\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=1000)\n\nt0 = time()\nsparse_lasso.fit(X_sp, y)\nprint(""Sparse Lasso done in %fs"" % (time() - t0))\n\nt0 = time()\ndense_lasso.fit(X, y)\nprint(""Dense Lasso done in %fs"" % (time() - t0))\n\nprint(""Distance between coefficients : %s""\n      % linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))\n\n# #############################################################################\n# The two Lasso implementations on Sparse data\nprint(""--- Sparse matrices"")\n\nXs = X.copy()\nXs[Xs < 2.5] = 0.0\nXs = sparse.coo_matrix(Xs)\nXs = Xs.tocsc()\n\nprint(""Matrix density : %s %%"" % (Xs.nnz / float(X.size) * 100))\n\nalpha = 0.1\nsparse_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = time()\nsparse_lasso.fit(Xs, y)\nprint(""Sparse Lasso done in %fs"" % (time() - t0))\n\nt0 = time()\ndense_lasso.fit(Xs.toarray(), y)\nprint(""Dense Lasso done in %fs"" % (time() - t0))\n\nprint(""Distance between coefficients : %s""\n      % linalg.norm(sparse_lasso.coef_ - dense_lasso.coef_))\n'"
scikit-learn/_downloads/24efe2aae88f606f99d103f1f226d49a/plot_adjusted_for_chance_measures.py,5,"b'""""""\n==========================================================\nAdjustment for chance in clustering performance evaluation\n==========================================================\n\nThe following plots demonstrate the impact of the number of clusters and\nnumber of samples on various clustering performance evaluation metrics.\n\nNon-adjusted measures such as the V-Measure show a dependency between\nthe number of clusters and the number of samples: the mean V-Measure\nof random labeling increases significantly as the number of clusters is\ncloser to the total number of samples used to compute the measure.\n\nAdjusted for chance measure such as ARI display some random variations\ncentered around a mean score of 0.0 for any number of samples and\nclusters.\n\nOnly adjusted measures can hence safely be used as a consensus index\nto evaluate the average stability of clustering algorithms for a given\nvalue of k on various overlapping sub-samples of the dataset.\n\n""""""\nprint(__doc__)\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom sklearn import metrics\n\ndef uniform_labelings_scores(score_func, n_samples, n_clusters_range,\n                             fixed_n_classes=None, n_runs=5, seed=42):\n    """"""Compute score for 2 random uniform cluster labelings.\n\n    Both random labelings have the same number of clusters for each value\n    possible value in ``n_clusters_range``.\n\n    When fixed_n_classes is not None the first labeling is considered a ground\n    truth class assignment with fixed number of classes.\n    """"""\n    random_labels = np.random.RandomState(seed).randint\n    scores = np.zeros((len(n_clusters_range), n_runs))\n\n    if fixed_n_classes is not None:\n        labels_a = random_labels(low=0, high=fixed_n_classes, size=n_samples)\n\n    for i, k in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            if fixed_n_classes is None:\n                labels_a = random_labels(low=0, high=k, size=n_samples)\n            labels_b = random_labels(low=0, high=k, size=n_samples)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores\n\n\ndef ami_score(U, V):\n    return metrics.adjusted_mutual_info_score(U, V)\n\nscore_funcs = [\n    metrics.adjusted_rand_score,\n    metrics.v_measure_score,\n    ami_score,\n    metrics.mutual_info_score,\n]\n\n# 2 independent random clusterings with equal cluster number\n\nn_samples = 100\nn_clusters_range = np.linspace(2, n_samples, 10).astype(np.int)\n\nplt.figure(1)\n\nplots = []\nnames = []\nfor score_func in score_funcs:\n    print(""Computing %s for %d values of n_clusters and n_samples=%d""\n          % (score_func.__name__, len(n_clusters_range), n_samples))\n\n    t0 = time()\n    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range)\n    print(""done in %0.3fs"" % (time() - t0))\n    plots.append(plt.errorbar(\n        n_clusters_range, np.median(scores, axis=1), scores.std(axis=1))[0])\n    names.append(score_func.__name__)\n\nplt.title(""Clustering measures for 2 random uniform labelings\\n""\n          ""with equal number of clusters"")\nplt.xlabel(\'Number of clusters (Number of samples is fixed to %d)\' % n_samples)\nplt.ylabel(\'Score value\')\nplt.legend(plots, names)\nplt.ylim(bottom=-0.05, top=1.05)\n\n\n# Random labeling with varying n_clusters against ground class labels\n# with fixed number of clusters\n\nn_samples = 1000\nn_clusters_range = np.linspace(2, 100, 10).astype(np.int)\nn_classes = 10\n\nplt.figure(2)\n\nplots = []\nnames = []\nfor score_func in score_funcs:\n    print(""Computing %s for %d values of n_clusters and n_samples=%d""\n          % (score_func.__name__, len(n_clusters_range), n_samples))\n\n    t0 = time()\n    scores = uniform_labelings_scores(score_func, n_samples, n_clusters_range,\n                                      fixed_n_classes=n_classes)\n    print(""done in %0.3fs"" % (time() - t0))\n    plots.append(plt.errorbar(\n        n_clusters_range, scores.mean(axis=1), scores.std(axis=1))[0])\n    names.append(score_func.__name__)\n\nplt.title(""Clustering measures for random uniform labeling\\n""\n          ""against reference assignment with %d classes"" % n_classes)\nplt.xlabel(\'Number of clusters (Number of samples is fixed to %d)\' % n_samples)\nplt.ylabel(\'Score value\')\nplt.ylim(bottom=-0.05, top=1.05)\nplt.legend(plots, names)\nplt.show()\n'"
scikit-learn/_downloads/2616d8f56b0a2688b99a05dbc4490fc3/plot_cost_complexity_pruning.py,0,"b'""""""\n========================================================\nPost pruning decision trees with cost complexity pruning\n========================================================\n\n.. currentmodule:: sklearn.tree\n\nThe :class:`DecisionTreeClassifier` provides parameters such as\n``min_samples_leaf`` and ``max_depth`` to prevent a tree from overfiting. Cost\ncomplexity pruning provides another option to control the size of a tree. In\n:class:`DecisionTreeClassifier`, this pruning technique is parameterized by the\ncost complexity parameter, ``ccp_alpha``. Greater values of ``ccp_alpha``\nincrease the number of nodes pruned. Here we only show the effect of\n``ccp_alpha`` on regularizing the trees and how to choose a ``ccp_alpha``\nbased on validation scores.\n\nSee also :ref:`minimal_cost_complexity_pruning` for details on pruning.\n""""""\n\nprint(__doc__)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.tree import DecisionTreeClassifier\n\n###############################################################################\n# Total impurity of leaves vs effective alphas of pruned tree\n# ---------------------------------------------------------------\n# Minimal cost complexity pruning recursively finds the node with the ""weakest\n# link"". The weakest link is characterized by an effective alpha, where the\n# nodes with the smallest effective alpha are pruned first. To get an idea of\n# what values of ``ccp_alpha`` could be appropriate, scikit-learn provides\n# :func:`DecisionTreeClassifier.cost_complexity_pruning_path` that returns the\n# effective alphas and the corresponding total leaf impurities at each step of\n# the pruning process. As alpha increases, more of the tree is pruned, which\n# increases the total impurity of its leaves.\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = DecisionTreeClassifier(random_state=0)\npath = clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n###############################################################################\n# In the following plot, the maximum effective alpha value is removed, because\n# it is the trivial tree with only one node.\nfig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker=\'o\', drawstyle=""steps-post"")\nax.set_xlabel(""effective alpha"")\nax.set_ylabel(""total impurity of leaves"")\nax.set_title(""Total Impurity vs effective alpha for training set"")\n\n###############################################################################\n# Next, we train a decision tree using the effective alphas. The last value\n# in ``ccp_alphas`` is the alpha value that prunes the whole tree,\n# leaving the tree, ``clfs[-1]``, with one node.\nclfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(""Number of nodes in the last tree is: {} with ccp_alpha: {}"".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))\n\n###############################################################################\n# For the remainder of this example, we remove the last element in\n# ``clfs`` and ``ccp_alphas``, because it is the trivial tree with only one\n# node. Here we show that the number of nodes and tree depth decreases as alpha\n# increases.\nclfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]\nfig, ax = plt.subplots(2, 1)\nax[0].plot(ccp_alphas, node_counts, marker=\'o\', drawstyle=""steps-post"")\nax[0].set_xlabel(""alpha"")\nax[0].set_ylabel(""number of nodes"")\nax[0].set_title(""Number of nodes vs alpha"")\nax[1].plot(ccp_alphas, depth, marker=\'o\', drawstyle=""steps-post"")\nax[1].set_xlabel(""alpha"")\nax[1].set_ylabel(""depth of tree"")\nax[1].set_title(""Depth vs alpha"")\nfig.tight_layout()\n\n###############################################################################\n# Accuracy vs alpha for training and testing sets\n# ----------------------------------------------------\n# When ``ccp_alpha`` is set to zero and keeping the other default parameters\n# of :class:`DecisionTreeClassifier`, the tree overfits, leading to\n# a 100% training accuracy and 88% testing accuracy. As alpha increases, more\n# of the tree is pruned, thus creating a decision tree that generalizes better.\n# In this example, setting ``ccp_alpha=0.015`` maximizes the testing accuracy.\ntrain_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = plt.subplots()\nax.set_xlabel(""alpha"")\nax.set_ylabel(""accuracy"")\nax.set_title(""Accuracy vs alpha for training and testing sets"")\nax.plot(ccp_alphas, train_scores, marker=\'o\', label=""train"",\n        drawstyle=""steps-post"")\nax.plot(ccp_alphas, test_scores, marker=\'o\', label=""test"",\n        drawstyle=""steps-post"")\nax.legend()\nplt.show()\n'"
scikit-learn/_downloads/27016884f56f70f801cca9b34da99d58/plot_lda.py,2,"b'""""""\n====================================================================\nNormal and Shrinkage Linear Discriminant Analysis for classification\n====================================================================\n\nShows how shrinkage improves classification.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n\nn_train = 20  # samples for training\nn_test = 200  # samples for testing\nn_averages = 50  # how often to repeat classification\nn_features_max = 75  # maximum number of features\nstep = 4  # step size for the calculation\n\n\ndef generate_data(n_samples, n_features):\n    """"""Generate random blob-ish data with noisy features.\n\n    This returns an array of input data with shape `(n_samples, n_features)`\n    and an array of `n_samples` target labels.\n\n    Only one feature contains discriminative information, the other features\n    contain only noise.\n    """"""\n    X, y = make_blobs(n_samples=n_samples, n_features=1, centers=[[-2], [2]])\n\n    # add non-discriminative features\n    if n_features > 1:\n        X = np.hstack([X, np.random.randn(n_samples, n_features - 1)])\n    return X, y\n\nacc_clf1, acc_clf2 = [], []\nn_features_range = range(1, n_features_max + 1, step)\nfor n_features in n_features_range:\n    score_clf1, score_clf2 = 0, 0\n    for _ in range(n_averages):\n        X, y = generate_data(n_train, n_features)\n\n        clf1 = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=\'auto\').fit(X, y)\n        clf2 = LinearDiscriminantAnalysis(solver=\'lsqr\', shrinkage=None).fit(X, y)\n\n        X, y = generate_data(n_test, n_features)\n        score_clf1 += clf1.score(X, y)\n        score_clf2 += clf2.score(X, y)\n\n    acc_clf1.append(score_clf1 / n_averages)\n    acc_clf2.append(score_clf2 / n_averages)\n\nfeatures_samples_ratio = np.array(n_features_range) / n_train\n\nplt.plot(features_samples_ratio, acc_clf1, linewidth=2,\n         label=""Linear Discriminant Analysis with shrinkage"", color=\'navy\')\nplt.plot(features_samples_ratio, acc_clf2, linewidth=2,\n         label=""Linear Discriminant Analysis"", color=\'gold\')\n\nplt.xlabel(\'n_features / n_samples\')\nplt.ylabel(\'Classification accuracy\')\n\nplt.legend(loc=1, prop={\'size\': 12})\nplt.suptitle(\'Linear Discriminant Analysis vs. \\\nshrinkage Linear Discriminant Analysis (1 discriminative feature)\')\nplt.show()\n'"
scikit-learn/_downloads/2891c599798cacd32c2b2d14a6bab9ef/plot_cv_indices.py,6,"b'""""""\nVisualizing cross-validation behavior in scikit-learn\n=====================================================\n\nChoosing the right cross-validation object is a crucial part of fitting a\nmodel properly. There are many ways to split data into training and test\nsets in order to avoid model overfitting, to standardize the number of\ngroups in test sets, etc.\n\nThis example visualizes the behavior of several common scikit-learn objects\nfor comparison.\n""""""\n\nfrom sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n                                     StratifiedKFold, GroupShuffleSplit,\n                                     GroupKFold, StratifiedShuffleSplit)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nnp.random.seed(1338)\ncmap_data = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\nn_splits = 4\n\n###############################################################################\n# Visualize our data\n# ------------------\n#\n# First, we must understand the structure of our data. It has 100 randomly\n# generated input datapoints, 3 classes split unevenly across datapoints,\n# and 10 ""groups"" split evenly across datapoints.\n#\n# As we\'ll see, some cross-validation objects do specific things with\n# labeled data, others behave differently with grouped data, and others\n# do not use this information.\n#\n# To begin, we\'ll visualize our data.\n\n# Generate the class/group data\nn_points = 100\nX = np.random.randn(100, 10)\n\npercentiles_classes = [.1, .3, .6]\ny = np.hstack([[ii] * int(100 * perc)\n               for ii, perc in enumerate(percentiles_classes)])\n\n# Evenly spaced groups repeated once\ngroups = np.hstack([[ii] * 10 for ii in range(10)])\n\n\ndef visualize_groups(classes, groups, name):\n    # Visualize dataset groups\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(groups)),  [.5] * len(groups), c=groups, marker=\'_\',\n               lw=50, cmap=cmap_data)\n    ax.scatter(range(len(groups)),  [3.5] * len(groups), c=classes, marker=\'_\',\n               lw=50, cmap=cmap_data)\n    ax.set(ylim=[-1, 5], yticks=[.5, 3.5],\n           yticklabels=[\'Data\\ngroup\', \'Data\\nclass\'], xlabel=""Sample index"")\n\n\nvisualize_groups(y, groups, \'no groups\')\n\n###############################################################################\n# Define a function to visualize cross-validation behavior\n# --------------------------------------------------------\n#\n# We\'ll define a function that lets us visualize the behavior of each\n# cross-validation object. We\'ll perform 4 splits of the data. On each\n# split, we\'ll visualize the indices chosen for the training set\n# (in blue) and the test set (in red).\n\n\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    """"""Create a sample plot for indices of a cross-validation object.""""""\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n                   c=indices, marker=\'_\', lw=lw, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2)\n\n    # Plot the data classes and groups at the end\n    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n               c=y, marker=\'_\', lw=lw, cmap=cmap_data)\n\n    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n               c=group, marker=\'_\', lw=lw, cmap=cmap_data)\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + [\'class\', \'group\']\n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n           xlabel=\'Sample index\', ylabel=""CV iteration"",\n           ylim=[n_splits+2.2, -.2], xlim=[0, 100])\n    ax.set_title(\'{}\'.format(type(cv).__name__), fontsize=15)\n    return ax\n\n\n###############################################################################\n# Let\'s see how it looks for the :class:`~sklearn.model_selection.KFold`\n# cross-validation object:\n\nfig, ax = plt.subplots()\ncv = KFold(n_splits)\nplot_cv_indices(cv, X, y, groups, ax, n_splits)\n\n###############################################################################\n# As you can see, by default the KFold cross-validation iterator does not\n# take either datapoint class or group into consideration. We can change this\n# by using the ``StratifiedKFold`` like so.\n\nfig, ax = plt.subplots()\ncv = StratifiedKFold(n_splits)\nplot_cv_indices(cv, X, y, groups, ax, n_splits)\n\n###############################################################################\n# In this case, the cross-validation retained the same ratio of classes across\n# each CV split. Next we\'ll visualize this behavior for a number of CV\n# iterators.\n#\n# Visualize cross-validation indices for many CV objects\n# ------------------------------------------------------\n#\n# Let\'s visually compare the cross validation behavior for many\n# scikit-learn cross-validation objects. Below we will loop through several\n# common cross-validation objects, visualizing the behavior of each.\n#\n# Note how some use the group/class information while others do not.\n\ncvs = [KFold, GroupKFold, ShuffleSplit, StratifiedKFold,\n       GroupShuffleSplit, StratifiedShuffleSplit, TimeSeriesSplit]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = plt.subplots(figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n              [\'Testing set\', \'Training set\'], loc=(1.02, .8))\n    # Make the legend fit\n    plt.tight_layout()\n    fig.subplots_adjust(right=.7)\nplt.show()\n'"
scikit-learn/_downloads/29b94f5f478f00ec343e14164da6557b/plot_gpc_xor.py,5,"b'""""""\n========================================================================\nIllustration of Gaussian process classification (GPC) on the XOR dataset\n========================================================================\n\nThis example illustrates GPC on XOR data. Compared are a stationary, isotropic\nkernel (RBF) and a non-stationary kernel (DotProduct). On this particular\ndataset, the DotProduct kernel obtains considerably better results because the\nclass-boundaries are linear and coincide with the coordinate axes. In general,\nstationary kernels often obtain better results.\n""""""\nprint(__doc__)\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF, DotProduct\n\n\nxx, yy = np.meshgrid(np.linspace(-3, 3, 50),\n                     np.linspace(-3, 3, 50))\nrng = np.random.RandomState(0)\nX = rng.randn(200, 2)\nY = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n\n# fit the model\nplt.figure(figsize=(10, 5))\nkernels = [1.0 * RBF(length_scale=1.0), 1.0 * DotProduct(sigma_0=1.0)**2]\nfor i, kernel in enumerate(kernels):\n    clf = GaussianProcessClassifier(kernel=kernel, warm_start=True).fit(X, Y)\n\n    # plot the decision function for each datapoint on the grid\n    Z = clf.predict_proba(np.vstack((xx.ravel(), yy.ravel())).T)[:, 1]\n    Z = Z.reshape(xx.shape)\n\n    plt.subplot(1, 2, i + 1)\n    image = plt.imshow(Z, interpolation=\'nearest\',\n                       extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n                       aspect=\'auto\', origin=\'lower\', cmap=plt.cm.PuOr_r)\n    contours = plt.contour(xx, yy, Z, levels=[0.5], linewidths=2,\n                           colors=[\'k\'])\n    plt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,\n                edgecolors=(0, 0, 0))\n    plt.xticks(())\n    plt.yticks(())\n    plt.axis([-3, 3, -3, 3])\n    plt.colorbar(image)\n    plt.title(""%s\\n Log-Marginal-Likelihood:%.3f""\n              % (clf.kernel_, clf.log_marginal_likelihood(clf.kernel_.theta)),\n              fontsize=12)\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/2d283b4b498e77828333c266774f5f59/plot_mds.py,11,"b'""""""\n=========================\nMulti-dimensional scaling\n=========================\n\nAn illustration of the metric and non-metric MDS on generated noisy data.\n\nThe reconstructed points using the metric MDS and non metric MDS are slightly\nshifted to avoid overlapping.\n""""""\n\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n# License: BSD\n\nprint(__doc__)\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn import manifold\nfrom sklearn.metrics import euclidean_distances\nfrom sklearn.decomposition import PCA\n\nEPSILON = np.finfo(np.float32).eps\nn_samples = 20\nseed = np.random.RandomState(seed=3)\nX_true = seed.randint(0, 20, 2 * n_samples).astype(np.float)\nX_true = X_true.reshape((n_samples, 2))\n# Center the data\nX_true -= X_true.mean()\n\nsimilarities = euclidean_distances(X_true)\n\n# Add noise to the similarities\nnoise = np.random.rand(n_samples, n_samples)\nnoise = noise + noise.T\nnoise[np.arange(noise.shape[0]), np.arange(noise.shape[0])] = 0\nsimilarities += noise\n\nmds = manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=seed,\n                   dissimilarity=""precomputed"", n_jobs=1)\npos = mds.fit(similarities).embedding_\n\nnmds = manifold.MDS(n_components=2, metric=False, max_iter=3000, eps=1e-12,\n                    dissimilarity=""precomputed"", random_state=seed, n_jobs=1,\n                    n_init=1)\nnpos = nmds.fit_transform(similarities, init=pos)\n\n# Rescale the data\npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum())\nnpos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum())\n\n# Rotate the data\nclf = PCA(n_components=2)\nX_true = clf.fit_transform(X_true)\n\npos = clf.fit_transform(pos)\n\nnpos = clf.fit_transform(npos)\n\nfig = plt.figure(1)\nax = plt.axes([0., 0., 1., 1.])\n\ns = 100\nplt.scatter(X_true[:, 0], X_true[:, 1], color=\'navy\', s=s, lw=0,\n            label=\'True Position\')\nplt.scatter(pos[:, 0], pos[:, 1], color=\'turquoise\', s=s, lw=0, label=\'MDS\')\nplt.scatter(npos[:, 0], npos[:, 1], color=\'darkorange\', s=s, lw=0, label=\'NMDS\')\nplt.legend(scatterpoints=1, loc=\'best\', shadow=False)\n\nsimilarities = similarities.max() / (similarities + EPSILON) * 100\nnp.fill_diagonal(similarities, 0)\n# Plot the edges\nstart_idx, end_idx = np.where(pos)\n# a sequence of (*line0*, *line1*, *line2*), where::\n#            linen = (x0, y0), (x1, y1), ... (xm, ym)\nsegments = [[X_true[i, :], X_true[j, :]]\n            for i in range(len(pos)) for j in range(len(pos))]\nvalues = np.abs(similarities)\nlc = LineCollection(segments,\n                    zorder=0, cmap=plt.cm.Blues,\n                    norm=plt.Normalize(0, values.max()))\nlc.set_array(similarities.flatten())\nlc.set_linewidths(np.full(len(segments), 0.5))\nax.add_collection(lc)\n\nplt.show()\n'"
scikit-learn/_downloads/2f8b13c6df949d1e6c58ffb57b2c5689/plot_bicluster_newsgroups.py,7,"b'""""""\n================================================================\nBiclustering documents with the Spectral Co-clustering algorithm\n================================================================\n\nThis example demonstrates the Spectral Co-clustering algorithm on the\ntwenty newsgroups dataset. The \'comp.os.ms-windows.misc\' category is\nexcluded because it contains many posts containing nothing but data.\n\nThe TF-IDF vectorized posts form a word frequency matrix, which is\nthen biclustered using Dhillon\'s Spectral Co-Clustering algorithm. The\nresulting document-word biclusters indicate subsets words used more\noften in those subsets documents.\n\nFor a few of the best biclusters, its most common document categories\nand its ten most important words get printed. The best biclusters are\ndetermined by their normalized cut. The best words are determined by\ncomparing their sums inside and outside the bicluster.\n\nFor comparison, the documents are also clustered using\nMiniBatchKMeans. The document clusters derived from the biclusters\nachieve a better V-measure than clusters found by MiniBatchKMeans.\n\n""""""\nfrom collections import defaultdict\nimport operator\nfrom time import time\n\nimport numpy as np\n\nfrom sklearn.cluster import SpectralCoclustering\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.cluster import v_measure_score\n\nprint(__doc__)\n\n\ndef number_normalizer(tokens):\n    """""" Map all numeric tokens to a placeholder.\n\n    For many applications, tokens that begin with a number are not directly\n    useful, but the fact that such a token exists can be relevant.  By applying\n    this form of dimensionality reduction, some methods may perform better.\n    """"""\n    return (""#NUMBER"" if token[0].isdigit() else token for token in tokens)\n\n\nclass NumberNormalizingVectorizer(TfidfVectorizer):\n    def build_tokenizer(self):\n        tokenize = super().build_tokenizer()\n        return lambda doc: list(number_normalizer(tokenize(doc)))\n\n\n# exclude \'comp.os.ms-windows.misc\'\ncategories = [\'alt.atheism\', \'comp.graphics\',\n              \'comp.sys.ibm.pc.hardware\', \'comp.sys.mac.hardware\',\n              \'comp.windows.x\', \'misc.forsale\', \'rec.autos\',\n              \'rec.motorcycles\', \'rec.sport.baseball\',\n              \'rec.sport.hockey\', \'sci.crypt\', \'sci.electronics\',\n              \'sci.med\', \'sci.space\', \'soc.religion.christian\',\n              \'talk.politics.guns\', \'talk.politics.mideast\',\n              \'talk.politics.misc\', \'talk.religion.misc\']\nnewsgroups = fetch_20newsgroups(categories=categories)\ny_true = newsgroups.target\n\nvectorizer = NumberNormalizingVectorizer(stop_words=\'english\', min_df=5)\ncocluster = SpectralCoclustering(n_clusters=len(categories),\n                                 svd_method=\'arpack\', random_state=0)\nkmeans = MiniBatchKMeans(n_clusters=len(categories), batch_size=20000,\n                         random_state=0)\n\nprint(""Vectorizing..."")\nX = vectorizer.fit_transform(newsgroups.data)\n\nprint(""Coclustering..."")\nstart_time = time()\ncocluster.fit(X)\ny_cocluster = cocluster.row_labels_\nprint(""Done in {:.2f}s. V-measure: {:.4f}"".format(\n    time() - start_time,\n    v_measure_score(y_cocluster, y_true)))\n\nprint(""MiniBatchKMeans..."")\nstart_time = time()\ny_kmeans = kmeans.fit_predict(X)\nprint(""Done in {:.2f}s. V-measure: {:.4f}"".format(\n    time() - start_time,\n    v_measure_score(y_kmeans, y_true)))\n\nfeature_names = vectorizer.get_feature_names()\ndocument_names = list(newsgroups.target_names[i] for i in newsgroups.target)\n\n\ndef bicluster_ncut(i):\n    rows, cols = cocluster.get_indices(i)\n    if not (np.any(rows) and np.any(cols)):\n        import sys\n        return sys.float_info.max\n    row_complement = np.nonzero(np.logical_not(cocluster.rows_[i]))[0]\n    col_complement = np.nonzero(np.logical_not(cocluster.columns_[i]))[0]\n    # Note: the following is identical to X[rows[:, np.newaxis],\n    # cols].sum() but much faster in scipy <= 0.16\n    weight = X[rows][:, cols].sum()\n    cut = (X[row_complement][:, cols].sum() +\n           X[rows][:, col_complement].sum())\n    return cut / weight\n\n\ndef most_common(d):\n    """"""Items of a defaultdict(int) with the highest values.\n\n    Like Counter.most_common in Python >=2.7.\n    """"""\n    return sorted(d.items(), key=operator.itemgetter(1), reverse=True)\n\n\nbicluster_ncuts = list(bicluster_ncut(i)\n                       for i in range(len(newsgroups.target_names)))\nbest_idx = np.argsort(bicluster_ncuts)[:5]\n\nprint()\nprint(""Best biclusters:"")\nprint(""----------------"")\nfor idx, cluster in enumerate(best_idx):\n    n_rows, n_cols = cocluster.get_shape(cluster)\n    cluster_docs, cluster_words = cocluster.get_indices(cluster)\n    if not len(cluster_docs) or not len(cluster_words):\n        continue\n\n    # categories\n    counter = defaultdict(int)\n    for i in cluster_docs:\n        counter[document_names[i]] += 1\n    cat_string = "", "".join(""{:.0f}% {}"".format(float(c) / n_rows * 100, name)\n                           for name, c in most_common(counter)[:3])\n\n    # words\n    out_of_cluster_docs = cocluster.row_labels_ != cluster\n    out_of_cluster_docs = np.where(out_of_cluster_docs)[0]\n    word_col = X[:, cluster_words]\n    word_scores = np.array(word_col[cluster_docs, :].sum(axis=0) -\n                           word_col[out_of_cluster_docs, :].sum(axis=0))\n    word_scores = word_scores.ravel()\n    important_words = list(feature_names[cluster_words[i]]\n                           for i in word_scores.argsort()[:-11:-1])\n\n    print(""bicluster {} : {} documents, {} words"".format(\n        idx, n_rows, n_cols))\n    print(""categories   : {}"".format(cat_string))\n    print(""words        : {}\\n"".format(\', \'.join(important_words)))\n'"
scikit-learn/_downloads/30c193cbf9c63006412bbabc67d11db6/plot_column_transformer_mixed_types.py,1,"b'""""""\n===================================\nColumn Transformer with Mixed Types\n===================================\n\nThis example illustrates how to apply different preprocessing and\nfeature extraction pipelines to different subsets of features,\nusing :class:`sklearn.compose.ColumnTransformer`.\nThis is particularly handy for the case of datasets that contain\nheterogeneous data types, since we may want to scale the\nnumeric features and one-hot encode the categorical ones.\n\nIn this example, the numeric data is standard-scaled after\nmean-imputation, while the categorical data is one-hot\nencoded after imputing missing values with a new category\n(``\'missing\'``).\n\nFinally, the preprocessing pipeline is integrated in a\nfull prediction pipeline using :class:`sklearn.pipeline.Pipeline`,\ntogether with a simple classification model.\n""""""\n\n# Author: Pedro Morales <part.morales@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nnp.random.seed(0)\n\n# Load data from https://www.openml.org/d/40945\nX, y = fetch_openml(""titanic"", version=1, as_frame=True, return_X_y=True)\n\n# Alternatively X and y can be obtained directly from the frame attribute:\n# X = titanic.frame.drop(\'survived\', axis=1)\n# y = titanic.frame[\'survived\']\n\n# We will train our classifier with the following features:\n# Numeric Features:\n# - age: float.\n# - fare: float.\n# Categorical Features:\n# - embarked: categories encoded as strings {\'C\', \'S\', \'Q\'}.\n# - sex: categories encoded as strings {\'female\', \'male\'}.\n# - pclass: ordinal integers {1, 2, 3}.\n\n# We create the preprocessing pipelines for both numeric and categorical data.\nnumeric_features = [\'age\', \'fare\']\nnumeric_transformer = Pipeline(steps=[\n    (\'imputer\', SimpleImputer(strategy=\'median\')),\n    (\'scaler\', StandardScaler())])\n\ncategorical_features = [\'embarked\', \'sex\', \'pclass\']\ncategorical_transformer = Pipeline(steps=[\n    (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')),\n    (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\'num\', numeric_transformer, numeric_features),\n        (\'cat\', categorical_transformer, categorical_features)])\n\n# Append classifier to preprocessing pipeline.\n# Now we have a full prediction pipeline.\nclf = Pipeline(steps=[(\'preprocessor\', preprocessor),\n                      (\'classifier\', LogisticRegression())])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nclf.fit(X_train, y_train)\nprint(""model score: %.3f"" % clf.score(X_test, y_test))\n\n\n###############################################################################\n# Using the prediction pipeline in a grid search\n###############################################################################\n# Grid search can also be performed on the different preprocessing steps\n# defined in the ``ColumnTransformer`` object, together with the classifier\'s\n# hyperparameters as part of the ``Pipeline``.\n# We will search for both the imputer strategy of the numeric preprocessing\n# and the regularization parameter of the logistic regression using\n# :class:`sklearn.model_selection.GridSearchCV`.\n\n\nparam_grid = {\n    \'preprocessor__num__imputer__strategy\': [\'mean\', \'median\'],\n    \'classifier__C\': [0.1, 1.0, 10, 100],\n}\n\ngrid_search = GridSearchCV(clf, param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\nprint((""best logistic regression from grid search: %.3f""\n       % grid_search.score(X_test, y_test)))\n'"
scikit-learn/_downloads/3153522a91a7f357bdeabe25e1c6b9cb/plot_calibration_multiclass.py,6,"b'""""""\n==================================================\nProbability Calibration for 3-class classification\n==================================================\n\nThis example illustrates how sigmoid calibration changes predicted\nprobabilities for a 3-class classification problem. Illustrated is the\nstandard 2-simplex, where the three corners correspond to the three classes.\nArrows point from the probability vectors predicted by an uncalibrated\nclassifier to the probability vectors predicted by the same classifier after\nsigmoid calibration on a hold-out validation set. Colors indicate the true\nclass of an instance (red: class 1, green: class 2, blue: class 3).\n\nThe base classifier is a random forest classifier with 25 base estimators\n(trees). If this classifier is trained on all 800 training datapoints, it is\noverly confident in its predictions and thus incurs a large log-loss.\nCalibrating an identical classifier, which was trained on 600 datapoints, with\nmethod=\'sigmoid\' on the remaining 200 datapoints reduces the confidence of the\npredictions, i.e., moves the probability vectors from the edges of the simplex\ntowards the center. This calibration results in a lower log-loss. Note that an\nalternative would have been to increase the number of base estimators which\nwould have resulted in a similar decrease in log-loss.\n""""""\nprint(__doc__)\n\n# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import log_loss\n\nnp.random.seed(0)\n\n# Generate data\nX, y = make_blobs(n_samples=1000, random_state=42, cluster_std=5.0)\nX_train, y_train = X[:600], y[:600]\nX_valid, y_valid = X[600:800], y[600:800]\nX_train_valid, y_train_valid = X[:800], y[:800]\nX_test, y_test = X[800:], y[800:]\n\n# Train uncalibrated random forest classifier on whole train and validation\n# data and evaluate on test data\nclf = RandomForestClassifier(n_estimators=25)\nclf.fit(X_train_valid, y_train_valid)\nclf_probs = clf.predict_proba(X_test)\nscore = log_loss(y_test, clf_probs)\n\n# Train random forest classifier, calibrate on validation data and evaluate\n# on test data\nclf = RandomForestClassifier(n_estimators=25)\nclf.fit(X_train, y_train)\nclf_probs = clf.predict_proba(X_test)\nsig_clf = CalibratedClassifierCV(clf, method=""sigmoid"", cv=""prefit"")\nsig_clf.fit(X_valid, y_valid)\nsig_clf_probs = sig_clf.predict_proba(X_test)\nsig_score = log_loss(y_test, sig_clf_probs)\n\n# Plot changes in predicted probabilities via arrows\nplt.figure()\ncolors = [""r"", ""g"", ""b""]\nfor i in range(clf_probs.shape[0]):\n    plt.arrow(clf_probs[i, 0], clf_probs[i, 1],\n              sig_clf_probs[i, 0] - clf_probs[i, 0],\n              sig_clf_probs[i, 1] - clf_probs[i, 1],\n              color=colors[y_test[i]], head_width=1e-2)\n\n# Plot perfect predictions\nplt.plot([1.0], [0.0], \'ro\', ms=20, label=""Class 1"")\nplt.plot([0.0], [1.0], \'go\', ms=20, label=""Class 2"")\nplt.plot([0.0], [0.0], \'bo\', ms=20, label=""Class 3"")\n\n# Plot boundaries of unit simplex\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], \'k\', label=""Simplex"")\n\n# Annotate points on the simplex\nplt.annotate(r\'($\\frac{1}{3}$, $\\frac{1}{3}$, $\\frac{1}{3}$)\',\n             xy=(1.0/3, 1.0/3), xytext=(1.0/3, .23), xycoords=\'data\',\n             arrowprops=dict(facecolor=\'black\', shrink=0.05),\n             horizontalalignment=\'center\', verticalalignment=\'center\')\nplt.plot([1.0/3], [1.0/3], \'ko\', ms=5)\nplt.annotate(r\'($\\frac{1}{2}$, $0$, $\\frac{1}{2}$)\',\n             xy=(.5, .0), xytext=(.5, .1), xycoords=\'data\',\n             arrowprops=dict(facecolor=\'black\', shrink=0.05),\n             horizontalalignment=\'center\', verticalalignment=\'center\')\nplt.annotate(r\'($0$, $\\frac{1}{2}$, $\\frac{1}{2}$)\',\n             xy=(.0, .5), xytext=(.1, .5), xycoords=\'data\',\n             arrowprops=dict(facecolor=\'black\', shrink=0.05),\n             horizontalalignment=\'center\', verticalalignment=\'center\')\nplt.annotate(r\'($\\frac{1}{2}$, $\\frac{1}{2}$, $0$)\',\n             xy=(.5, .5), xytext=(.6, .6), xycoords=\'data\',\n             arrowprops=dict(facecolor=\'black\', shrink=0.05),\n             horizontalalignment=\'center\', verticalalignment=\'center\')\nplt.annotate(r\'($0$, $0$, $1$)\',\n             xy=(0, 0), xytext=(.1, .1), xycoords=\'data\',\n             arrowprops=dict(facecolor=\'black\', shrink=0.05),\n             horizontalalignment=\'center\', verticalalignment=\'center\')\nplt.annotate(r\'($1$, $0$, $0$)\',\n             xy=(1, 0), xytext=(1, .1), xycoords=\'data\',\n             arrowprops=dict(facecolor=\'black\', shrink=0.05),\n             horizontalalignment=\'center\', verticalalignment=\'center\')\nplt.annotate(r\'($0$, $1$, $0$)\',\n             xy=(0, 1), xytext=(.1, 1), xycoords=\'data\',\n             arrowprops=dict(facecolor=\'black\', shrink=0.05),\n             horizontalalignment=\'center\', verticalalignment=\'center\')\n# Add grid\nplt.grid(False)\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    plt.plot([0, x], [x, 0], \'k\', alpha=0.2)\n    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], \'k\', alpha=0.2)\n    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], \'k\', alpha=0.2)\n\nplt.title(""Change of predicted probabilities after sigmoid calibration"")\nplt.xlabel(""Probability class 1"")\nplt.ylabel(""Probability class 2"")\nplt.xlim(-0.05, 1.05)\nplt.ylim(-0.05, 1.05)\nplt.legend(loc=""best"")\n\nprint(""Log-loss of"")\nprint("" * uncalibrated classifier trained on 800 datapoints: %.3f ""\n      % score)\nprint("" * classifier trained on 600 datapoints and calibrated on ""\n      ""200 datapoint: %.3f"" % sig_score)\n\n# Illustrate calibrator\nplt.figure()\n# generate grid over 2-simplex\np1d = np.linspace(0, 1, 20)\np0, p1 = np.meshgrid(p1d, p1d)\np2 = 1 - p0 - p1\np = np.c_[p0.ravel(), p1.ravel(), p2.ravel()]\np = p[p[:, 2] >= 0]\n\ncalibrated_classifier = sig_clf.calibrated_classifiers_[0]\nprediction = np.vstack([calibrator.predict(this_p)\n                        for calibrator, this_p in\n                        zip(calibrated_classifier.calibrators_, p.T)]).T\nprediction /= prediction.sum(axis=1)[:, None]\n\n# Plot modifications of calibrator\nfor i in range(prediction.shape[0]):\n    plt.arrow(p[i, 0], p[i, 1],\n              prediction[i, 0] - p[i, 0], prediction[i, 1] - p[i, 1],\n              head_width=1e-2, color=colors[np.argmax(p[i])])\n# Plot boundaries of unit simplex\nplt.plot([0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], \'k\', label=""Simplex"")\n\nplt.grid(False)\nfor x in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n    plt.plot([0, x], [x, 0], \'k\', alpha=0.2)\n    plt.plot([0, 0 + (1-x)/2], [x, x + (1-x)/2], \'k\', alpha=0.2)\n    plt.plot([x, x + (1-x)/2], [0, 0 + (1-x)/2], \'k\', alpha=0.2)\n\nplt.title(""Illustration of sigmoid calibrator"")\nplt.xlabel(""Probability class 1"")\nplt.ylabel(""Probability class 2"")\nplt.xlim(-0.05, 1.05)\nplt.ylim(-0.05, 1.05)\n\nplt.show()\n'"
scikit-learn/_downloads/32612210a8674395ac93da6bdd8f3aeb/plot_feature_transformation.py,1,"b'""""""\n===============================================\nFeature transformations with ensembles of trees\n===============================================\n\nTransform your features into a higher dimensional, sparse space. Then\ntrain a linear model on these features.\n\nFirst fit an ensemble of trees (totally random trees, a random\nforest, or gradient boosted trees) on the training set. Then each leaf\nof each tree in the ensemble is assigned a fixed arbitrary feature\nindex in a new feature space. These leaf indices are then encoded in a\none-hot fashion.\n\nEach sample goes through the decisions of each tree of the ensemble\nand ends up in one leaf per tree. The sample is encoded by setting\nfeature values for these leaves to 1 and the other feature values to 0.\n\nThe resulting transformer has then learned a supervised, sparse,\nhigh-dimensional categorical embedding of the data.\n\n""""""\n\n# Author: Tim Head <betatim@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nnp.random.seed(10)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (RandomTreesEmbedding, RandomForestClassifier,\n                              GradientBoostingClassifier)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.pipeline import make_pipeline\n\nn_estimator = 10\nX, y = make_classification(n_samples=80000)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n\n# It is important to train the ensemble of trees on a different subset\n# of the training data than the linear regression model to avoid\n# overfitting, in particular if the total number of leaves is\n# similar to the number of training samples\nX_train, X_train_lr, y_train, y_train_lr = train_test_split(\n    X_train, y_train, test_size=0.5)\n\n# Unsupervised transformation based on totally random trees\nrt = RandomTreesEmbedding(max_depth=3, n_estimators=n_estimator,\n                          random_state=0)\n\nrt_lm = LogisticRegression(max_iter=1000)\npipeline = make_pipeline(rt, rt_lm)\npipeline.fit(X_train, y_train)\ny_pred_rt = pipeline.predict_proba(X_test)[:, 1]\nfpr_rt_lm, tpr_rt_lm, _ = roc_curve(y_test, y_pred_rt)\n\n# Supervised transformation based on random forests\nrf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\nrf_enc = OneHotEncoder()\nrf_lm = LogisticRegression(max_iter=1000)\nrf.fit(X_train, y_train)\nrf_enc.fit(rf.apply(X_train))\nrf_lm.fit(rf_enc.transform(rf.apply(X_train_lr)), y_train_lr)\n\ny_pred_rf_lm = rf_lm.predict_proba(rf_enc.transform(rf.apply(X_test)))[:, 1]\nfpr_rf_lm, tpr_rf_lm, _ = roc_curve(y_test, y_pred_rf_lm)\n\n# Supervised transformation based on gradient boosted trees\ngrd = GradientBoostingClassifier(n_estimators=n_estimator)\ngrd_enc = OneHotEncoder()\ngrd_lm = LogisticRegression(max_iter=1000)\ngrd.fit(X_train, y_train)\ngrd_enc.fit(grd.apply(X_train)[:, :, 0])\ngrd_lm.fit(grd_enc.transform(grd.apply(X_train_lr)[:, :, 0]), y_train_lr)\n\ny_pred_grd_lm = grd_lm.predict_proba(\n    grd_enc.transform(grd.apply(X_test)[:, :, 0]))[:, 1]\nfpr_grd_lm, tpr_grd_lm, _ = roc_curve(y_test, y_pred_grd_lm)\n\n# The gradient boosted model by itself\ny_pred_grd = grd.predict_proba(X_test)[:, 1]\nfpr_grd, tpr_grd, _ = roc_curve(y_test, y_pred_grd)\n\n# The random forest model by itself\ny_pred_rf = rf.predict_proba(X_test)[:, 1]\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_rf)\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], \'k--\')\nplt.plot(fpr_rt_lm, tpr_rt_lm, label=\'RT + LR\')\nplt.plot(fpr_rf, tpr_rf, label=\'RF\')\nplt.plot(fpr_rf_lm, tpr_rf_lm, label=\'RF + LR\')\nplt.plot(fpr_grd, tpr_grd, label=\'GBT\')\nplt.plot(fpr_grd_lm, tpr_grd_lm, label=\'GBT + LR\')\nplt.xlabel(\'False positive rate\')\nplt.ylabel(\'True positive rate\')\nplt.title(\'ROC curve\')\nplt.legend(loc=\'best\')\nplt.show()\n\nplt.figure(2)\nplt.xlim(0, 0.2)\nplt.ylim(0.8, 1)\nplt.plot([0, 1], [0, 1], \'k--\')\nplt.plot(fpr_rt_lm, tpr_rt_lm, label=\'RT + LR\')\nplt.plot(fpr_rf, tpr_rf, label=\'RF\')\nplt.plot(fpr_rf_lm, tpr_rf_lm, label=\'RF + LR\')\nplt.plot(fpr_grd, tpr_grd, label=\'GBT\')\nplt.plot(fpr_grd_lm, tpr_grd_lm, label=\'GBT + LR\')\nplt.xlabel(\'False positive rate\')\nplt.ylabel(\'True positive rate\')\nplt.title(\'ROC curve (zoomed in at top left)\')\nplt.legend(loc=\'best\')\nplt.show()\n'"
scikit-learn/_downloads/32fd8f237423c6b9292fbf7356065822/plot_voting_probas.py,5,"b'""""""\n===========================================================\nPlot class probabilities calculated by the VotingClassifier\n===========================================================\n\n.. currentmodule:: sklearn\n\nPlot the class probabilities of the first sample in a toy dataset predicted by\nthree different classifiers and averaged by the\n:class:`~ensemble.VotingClassifier`.\n\nFirst, three examplary classifiers are initialized\n(:class:`~linear_model.LogisticRegression`, :class:`~naive_bayes.GaussianNB`,\nand :class:`~ensemble.RandomForestClassifier`) and used to initialize a\nsoft-voting :class:`~ensemble.VotingClassifier` with weights `[1, 1, 5]`, which\nmeans that the predicted probabilities of the\n:class:`~ensemble.RandomForestClassifier` count 5 times as much as the weights\nof the other classifiers when the averaged probability is calculated.\n\nTo visualize the probability weighting, we fit each classifier on the training\nset and plot the predicted class probabilities for the first sample in this\nexample dataset.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nclf1 = LogisticRegression(max_iter=1000, random_state=123)\nclf2 = RandomForestClassifier(n_estimators=100, random_state=123)\nclf3 = GaussianNB()\nX = np.array([[-1.0, -1.0], [-1.2, -1.4], [-3.4, -2.2], [1.1, 1.2]])\ny = np.array([1, 1, 2, 2])\n\neclf = VotingClassifier(estimators=[(\'lr\', clf1), (\'rf\', clf2), (\'gnb\', clf3)],\n                        voting=\'soft\',\n                        weights=[1, 1, 5])\n\n# predict class probabilities for all classifiers\nprobas = [c.fit(X, y).predict_proba(X) for c in (clf1, clf2, clf3, eclf)]\n\n# get class probabilities for the first sample in the dataset\nclass1_1 = [pr[0, 0] for pr in probas]\nclass2_1 = [pr[0, 1] for pr in probas]\n\n\n# plotting\n\nN = 4  # number of groups\nind = np.arange(N)  # group positions\nwidth = 0.35  # bar width\n\nfig, ax = plt.subplots()\n\n# bars for classifier 1-3\np1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,\n            color=\'green\', edgecolor=\'k\')\np2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,\n            color=\'lightgreen\', edgecolor=\'k\')\n\n# bars for VotingClassifier\np3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,\n            color=\'blue\', edgecolor=\'k\')\np4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,\n            color=\'steelblue\', edgecolor=\'k\')\n\n# plot annotations\nplt.axvline(2.8, color=\'k\', linestyle=\'dashed\')\nax.set_xticks(ind + width)\nax.set_xticklabels([\'LogisticRegression\\nweight 1\',\n                    \'GaussianNB\\nweight 1\',\n                    \'RandomForestClassifier\\nweight 5\',\n                    \'VotingClassifier\\n(average probabilities)\'],\n                   rotation=40,\n                   ha=\'right\')\nplt.ylim([0, 1])\nplt.title(\'Class probabilities for sample 1 by different classifiers\')\nplt.legend([p1[0], p2[0]], [\'class 1\', \'class 2\'], loc=\'upper left\')\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/33cc0ef66c67c4b02c4d665b99dbb61e/plot_document_classification_20newsgroups.py,5,"b'""""""\n======================================================\nClassification of text documents using sparse features\n======================================================\n\nThis is an example showing how scikit-learn can be used to classify documents\nby topics using a bag-of-words approach. This example uses a scipy.sparse\nmatrix to store the features and demonstrates various classifiers that can\nefficiently handle sparse matrices.\n\nThe dataset used in this example is the 20 newsgroups dataset. It will be\nautomatically downloaded, then cached.\n\n""""""\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Olivier Grisel <olivier.grisel@ensta.org>\n#         Mathieu Blondel <mathieu@mblondel.org>\n#         Lars Buitinck\n# License: BSD 3 clause\nimport logging\nimport numpy as np\nfrom optparse import OptionParser\nimport sys\nfrom time import time\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.extmath import density\nfrom sklearn import metrics\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format=\'%(asctime)s %(levelname)s %(message)s\')\n\nop = OptionParser()\nop.add_option(""--report"",\n              action=""store_true"", dest=""print_report"",\n              help=""Print a detailed classification report."")\nop.add_option(""--chi2_select"",\n              action=""store"", type=""int"", dest=""select_chi2"",\n              help=""Select some number of features using a chi-squared test"")\nop.add_option(""--confusion_matrix"",\n              action=""store_true"", dest=""print_cm"",\n              help=""Print the confusion matrix."")\nop.add_option(""--top10"",\n              action=""store_true"", dest=""print_top10"",\n              help=""Print ten most discriminative terms per class""\n                   "" for every classifier."")\nop.add_option(""--all_categories"",\n              action=""store_true"", dest=""all_categories"",\n              help=""Whether to use all categories or not."")\nop.add_option(""--use_hashing"",\n              action=""store_true"",\n              help=""Use a hashing vectorizer."")\nop.add_option(""--n_features"",\n              action=""store"", type=int, default=2 ** 16,\n              help=""n_features when using the hashing vectorizer."")\nop.add_option(""--filtered"",\n              action=""store_true"",\n              help=""Remove newsgroup information that is easily overfit: ""\n                   ""headers, signatures, and quoting."")\n\n\ndef is_interactive():\n    return not hasattr(sys.modules[\'__main__\'], \'__file__\')\n\n\n# work-around for Jupyter notebook and IPython console\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(""this script takes no arguments."")\n    sys.exit(1)\n\nprint(__doc__)\nop.print_help()\nprint()\n\n\n##############################################################################\n# Load data from the training set\n# ------------------------------------\n# Let\'s load data from the newsgroups dataset which comprises around 18000\n# newsgroups posts on 20 topics split in two subsets: one for training (or\n# development) and the other one for testing (or for performance evaluation).\nif opts.all_categories:\n    categories = None\nelse:\n    categories = [\n        \'alt.atheism\',\n        \'talk.religion.misc\',\n        \'comp.graphics\',\n        \'sci.space\',\n    ]\n\nif opts.filtered:\n    remove = (\'headers\', \'footers\', \'quotes\')\nelse:\n    remove = ()\n\nprint(""Loading 20 newsgroups dataset for categories:"")\nprint(categories if categories else ""all"")\n\ndata_train = fetch_20newsgroups(subset=\'train\', categories=categories,\n                                shuffle=True, random_state=42,\n                                remove=remove)\n\ndata_test = fetch_20newsgroups(subset=\'test\', categories=categories,\n                               shuffle=True, random_state=42,\n                               remove=remove)\nprint(\'data loaded\')\n\n# order of labels in `target_names` can be different from `categories`\ntarget_names = data_train.target_names\n\n\ndef size_mb(docs):\n    return sum(len(s.encode(\'utf-8\')) for s in docs) / 1e6\n\n\ndata_train_size_mb = size_mb(data_train.data)\ndata_test_size_mb = size_mb(data_test.data)\n\nprint(""%d documents - %0.3fMB (training set)"" % (\n    len(data_train.data), data_train_size_mb))\nprint(""%d documents - %0.3fMB (test set)"" % (\n    len(data_test.data), data_test_size_mb))\nprint(""%d categories"" % len(target_names))\nprint()\n\n# split a training set and a test set\ny_train, y_test = data_train.target, data_test.target\n\nprint(""Extracting features from the training data using a sparse vectorizer"")\nt0 = time()\nif opts.use_hashing:\n    vectorizer = HashingVectorizer(stop_words=\'english\', alternate_sign=False,\n                                   n_features=opts.n_features)\n    X_train = vectorizer.transform(data_train.data)\nelse:\n    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n                                 stop_words=\'english\')\n    X_train = vectorizer.fit_transform(data_train.data)\nduration = time() - t0\nprint(""done in %fs at %0.3fMB/s"" % (duration, data_train_size_mb / duration))\nprint(""n_samples: %d, n_features: %d"" % X_train.shape)\nprint()\n\nprint(""Extracting features from the test data using the same vectorizer"")\nt0 = time()\nX_test = vectorizer.transform(data_test.data)\nduration = time() - t0\nprint(""done in %fs at %0.3fMB/s"" % (duration, data_test_size_mb / duration))\nprint(""n_samples: %d, n_features: %d"" % X_test.shape)\nprint()\n\n# mapping from integer feature name to original token string\nif opts.use_hashing:\n    feature_names = None\nelse:\n    feature_names = vectorizer.get_feature_names()\n\nif opts.select_chi2:\n    print(""Extracting %d best features by a chi-squared test"" %\n          opts.select_chi2)\n    t0 = time()\n    ch2 = SelectKBest(chi2, k=opts.select_chi2)\n    X_train = ch2.fit_transform(X_train, y_train)\n    X_test = ch2.transform(X_test)\n    if feature_names:\n        # keep selected feature names\n        feature_names = [feature_names[i] for i\n                         in ch2.get_support(indices=True)]\n    print(""done in %fs"" % (time() - t0))\n    print()\n\nif feature_names:\n    feature_names = np.asarray(feature_names)\n\n\ndef trim(s):\n    """"""Trim string to fit on terminal (assuming 80-column display)""""""\n    return s if len(s) <= 80 else s[:77] + ""...""\n\n\n##############################################################################\n# Benchmark classifiers\n# ------------------------------------\n# We train and test the datasets with 15 different classification models\n# and get performance results for each model.\ndef benchmark(clf):\n    print(\'_\' * 80)\n    print(""Training: "")\n    print(clf)\n    t0 = time()\n    clf.fit(X_train, y_train)\n    train_time = time() - t0\n    print(""train time: %0.3fs"" % train_time)\n\n    t0 = time()\n    pred = clf.predict(X_test)\n    test_time = time() - t0\n    print(""test time:  %0.3fs"" % test_time)\n\n    score = metrics.accuracy_score(y_test, pred)\n    print(""accuracy:   %0.3f"" % score)\n\n    if hasattr(clf, \'coef_\'):\n        print(""dimensionality: %d"" % clf.coef_.shape[1])\n        print(""density: %f"" % density(clf.coef_))\n\n        if opts.print_top10 and feature_names is not None:\n            print(""top 10 keywords per class:"")\n            for i, label in enumerate(target_names):\n                top10 = np.argsort(clf.coef_[i])[-10:]\n                print(trim(""%s: %s"" % (label, "" "".join(feature_names[top10]))))\n        print()\n\n    if opts.print_report:\n        print(""classification report:"")\n        print(metrics.classification_report(y_test, pred,\n                                            target_names=target_names))\n\n    if opts.print_cm:\n        print(""confusion matrix:"")\n        print(metrics.confusion_matrix(y_test, pred))\n\n    print()\n    clf_descr = str(clf).split(\'(\')[0]\n    return clf_descr, score, train_time, test_time\n\n\nresults = []\nfor clf, name in (\n        (RidgeClassifier(tol=1e-2, solver=""sag""), ""Ridge Classifier""),\n        (Perceptron(max_iter=50), ""Perceptron""),\n        (PassiveAggressiveClassifier(max_iter=50),\n         ""Passive-Aggressive""),\n        (KNeighborsClassifier(n_neighbors=10), ""kNN""),\n        (RandomForestClassifier(), ""Random forest"")):\n    print(\'=\' * 80)\n    print(name)\n    results.append(benchmark(clf))\n\nfor penalty in [""l2"", ""l1""]:\n    print(\'=\' * 80)\n    print(""%s penalty"" % penalty.upper())\n    # Train Liblinear model\n    results.append(benchmark(LinearSVC(penalty=penalty, dual=False,\n                                       tol=1e-3)))\n\n    # Train SGD model\n    results.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n                                           penalty=penalty)))\n\n# Train SGD with Elastic Net penalty\nprint(\'=\' * 80)\nprint(""Elastic-Net penalty"")\nresults.append(benchmark(SGDClassifier(alpha=.0001, max_iter=50,\n                                       penalty=""elasticnet"")))\n\n# Train NearestCentroid without threshold\nprint(\'=\' * 80)\nprint(""NearestCentroid (aka Rocchio classifier)"")\nresults.append(benchmark(NearestCentroid()))\n\n# Train sparse Naive Bayes classifiers\nprint(\'=\' * 80)\nprint(""Naive Bayes"")\nresults.append(benchmark(MultinomialNB(alpha=.01)))\nresults.append(benchmark(BernoulliNB(alpha=.01)))\nresults.append(benchmark(ComplementNB(alpha=.1)))\n\nprint(\'=\' * 80)\nprint(""LinearSVC with L1-based feature selection"")\n# The smaller C, the stronger the regularization.\n# The more regularization, the more sparsity.\nresults.append(benchmark(Pipeline([\n  (\'feature_selection\', SelectFromModel(LinearSVC(penalty=""l1"", dual=False,\n                                                  tol=1e-3))),\n  (\'classification\', LinearSVC(penalty=""l2""))])))\n\n\n##############################################################################\n# Add plots\n# ------------------------------------\n# The bar plot indicates the accuracy, training time (normalized) and test time\n# (normalized) of each classifier.\nindices = np.arange(len(results))\n\nresults = [[x[i] for x in results] for i in range(4)]\n\nclf_names, score, training_time, test_time = results\ntraining_time = np.array(training_time) / np.max(training_time)\ntest_time = np.array(test_time) / np.max(test_time)\n\nplt.figure(figsize=(12, 8))\nplt.title(""Score"")\nplt.barh(indices, score, .2, label=""score"", color=\'navy\')\nplt.barh(indices + .3, training_time, .2, label=""training time"",\n         color=\'c\')\nplt.barh(indices + .6, test_time, .2, label=""test time"", color=\'darkorange\')\nplt.yticks(())\nplt.legend(loc=\'best\')\nplt.subplots_adjust(left=.25)\nplt.subplots_adjust(top=.95)\nplt.subplots_adjust(bottom=.05)\n\nfor i, c in zip(indices, clf_names):\n    plt.text(-.3, i, c)\n\nplt.show()\n'"
scikit-learn/_downloads/3471d630d42778e8ec2683f7424fa2cb/plot_digits_classification_exercise.py,0,"b'""""""\n================================\nDigits Classification Exercise\n================================\n\nA tutorial exercise regarding the use of classification techniques on\nthe Digits dataset.\n\nThis exercise is used in the :ref:`clf_tut` part of the\n:ref:`supervised_learning_tut` section of the\n:ref:`stat_learn_tut_index`.\n""""""\nprint(__doc__)\n\nfrom sklearn import datasets, neighbors, linear_model\n\nX_digits, y_digits = datasets.load_digits(return_X_y=True)\nX_digits = X_digits / X_digits.max()\n\nn_samples = len(X_digits)\n\nX_train = X_digits[:int(.9 * n_samples)]\ny_train = y_digits[:int(.9 * n_samples)]\nX_test = X_digits[int(.9 * n_samples):]\ny_test = y_digits[int(.9 * n_samples):]\n\nknn = neighbors.KNeighborsClassifier()\nlogistic = linear_model.LogisticRegression(max_iter=1000)\n\nprint(\'KNN score: %f\' % knn.fit(X_train, y_train).score(X_test, y_test))\nprint(\'LogisticRegression score: %f\'\n      % logistic.fit(X_train, y_train).score(X_test, y_test))\n'"
scikit-learn/_downloads/357a61f10f83c16a73ac1262994651f1/plot_cluster_comparison.py,6,"b'""""""\n=========================================================\nComparing different clustering algorithms on toy datasets\n=========================================================\n\nThis example shows characteristics of different\nclustering algorithms on datasets that are ""interesting""\nbut still in 2D. With the exception of the last dataset,\nthe parameters of each of these dataset-algorithm pairs\nhas been tuned to produce good clustering results. Some\nalgorithms are more sensitive to parameter values than\nothers.\n\nThe last dataset is an example of a \'null\' situation for\nclustering: the data is homogeneous, and there is no good\nclustering. For this example, the null dataset uses the\nsame parameters as the dataset in the row above it, which\nrepresents a mismatch in the parameter values and the\ndata structure.\n\nWhile these examples give some intuition about the\nalgorithms, this intuition might not apply to very high\ndimensional data.\n""""""\nprint(__doc__)\n\nimport time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\n\nnp.random.seed(0)\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 1500\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nno_structure = np.random.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(n_samples=n_samples,\n                             cluster_std=[1.0, 2.5, 0.5],\n                             random_state=random_state)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(9 * 2 + 3, 12.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n                    hspace=.01)\n\nplot_num = 1\n\ndefault_base = {\'quantile\': .3,\n                \'eps\': .3,\n                \'damping\': .9,\n                \'preference\': -200,\n                \'n_neighbors\': 10,\n                \'n_clusters\': 3,\n                \'min_samples\': 20,\n                \'xi\': 0.05,\n                \'min_cluster_size\': 0.1}\n\ndatasets = [\n    (noisy_circles, {\'damping\': .77, \'preference\': -240,\n                     \'quantile\': .2, \'n_clusters\': 2,\n                     \'min_samples\': 20, \'xi\': 0.25}),\n    (noisy_moons, {\'damping\': .75, \'preference\': -220, \'n_clusters\': 2}),\n    (varied, {\'eps\': .18, \'n_neighbors\': 2,\n              \'min_samples\': 5, \'xi\': 0.035, \'min_cluster_size\': .2}),\n    (aniso, {\'eps\': .15, \'n_neighbors\': 2,\n             \'min_samples\': 20, \'xi\': 0.1, \'min_cluster_size\': .2}),\n    (blobs, {}),\n    (no_structure, {})]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\'quantile\'])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\'n_neighbors\'], include_self=False)\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(n_clusters=params[\'n_clusters\'])\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\'n_clusters\'], linkage=\'ward\',\n        connectivity=connectivity)\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\'n_clusters\'], eigen_solver=\'arpack\',\n        affinity=""nearest_neighbors"")\n    dbscan = cluster.DBSCAN(eps=params[\'eps\'])\n    optics = cluster.OPTICS(min_samples=params[\'min_samples\'],\n                            xi=params[\'xi\'],\n                            min_cluster_size=params[\'min_cluster_size\'])\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\'damping\'], preference=params[\'preference\'])\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=""average"", affinity=""cityblock"",\n        n_clusters=params[\'n_clusters\'], connectivity=connectivity)\n    birch = cluster.Birch(n_clusters=params[\'n_clusters\'])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\'n_clusters\'], covariance_type=\'full\')\n\n    clustering_algorithms = (\n        (\'MiniBatchKMeans\', two_means),\n        (\'AffinityPropagation\', affinity_propagation),\n        (\'MeanShift\', ms),\n        (\'SpectralClustering\', spectral),\n        (\'Ward\', ward),\n        (\'AgglomerativeClustering\', average_linkage),\n        (\'DBSCAN\', dbscan),\n        (\'OPTICS\', optics),\n        (\'Birch\', birch),\n        (\'GaussianMixture\', gmm)\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                ""ignore"",\n                message=""the number of connected components of the "" +\n                ""connectivity matrix is [0-9]{1,2}"" +\n                "" > 1. Completing it to avoid stopping the tree early."",\n                category=UserWarning)\n            warnings.filterwarnings(\n                ""ignore"",\n                message=""Graph is not fully connected, spectral embedding"" +\n                "" may not work as expected."",\n                category=UserWarning)\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \'labels_\'):\n            y_pred = algorithm.labels_.astype(np.int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(list(islice(cycle([\'#377eb8\', \'#ff7f00\', \'#4daf4a\',\n                                             \'#f781bf\', \'#a65628\', \'#984ea3\',\n                                             \'#999999\', \'#e41a1c\', \'#dede00\']),\n                                      int(max(y_pred) + 1))))\n        # add black color for outliers (if any)\n        colors = np.append(colors, [""#000000""])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(.99, .01, (\'%.2fs\' % (t1 - t0)).lstrip(\'0\'),\n                 transform=plt.gca().transAxes, size=15,\n                 horizontalalignment=\'right\')\n        plot_num += 1\n\nplt.show()\n'"
scikit-learn/_downloads/36006be60f704b93bbd618cdf0204034/plot_compare_calibration.py,1,"b'""""""\n========================================\nComparison of Calibration of Classifiers\n========================================\n\nWell calibrated classifiers are probabilistic classifiers for which the output\nof the predict_proba method can be directly interpreted as a confidence level.\nFor instance a well calibrated (binary) classifier should classify the samples\nsuch that among the samples to which it gave a predict_proba value close to\n0.8, approx. 80% actually belong to the positive class.\n\nLogisticRegression returns well calibrated predictions as it directly\noptimizes log-loss. In contrast, the other methods return biased probabilities,\nwith different biases per method:\n\n* GaussianNaiveBayes tends to push probabilities to 0 or 1 (note the counts in\n  the histograms). This is mainly because it makes the assumption that features\n  are conditionally independent given the class, which is not the case in this\n  dataset which contains 2 redundant features.\n\n* RandomForestClassifier shows the opposite behavior: the histograms show\n  peaks at approx. 0.2 and 0.9 probability, while probabilities close to 0 or 1\n  are very rare. An explanation for this is given by Niculescu-Mizil and Caruana\n  [1]_: ""Methods such as bagging and random forests that average predictions\n  from a base set of models can have difficulty making predictions near 0 and 1\n  because variance in the underlying base models will bias predictions that\n  should be near zero or one away from these values. Because predictions are\n  restricted to the interval [0,1], errors caused by variance tend to be one-\n  sided near zero and one. For example, if a model should predict p = 0 for a\n  case, the only way bagging can achieve this is if all bagged trees predict\n  zero. If we add noise to the trees that bagging is averaging over, this noise\n  will cause some trees to predict values larger than 0 for this case, thus\n  moving the average prediction of the bagged ensemble away from 0. We observe\n  this effect most strongly with random forests because the base-level trees\n  trained with random forests have relatively high variance due to feature\n  subsetting."" As a result, the calibration curve shows a characteristic\n  sigmoid shape, indicating that the classifier could trust its ""intuition""\n  more and return probabilities closer to 0 or 1 typically.\n\n* Support Vector Classification (SVC) shows an even more sigmoid curve as\n  the  RandomForestClassifier, which is typical for maximum-margin methods\n  (compare Niculescu-Mizil and Caruana [1]_), which focus on hard samples\n  that are close to the decision boundary (the support vectors).\n\n.. topic:: References:\n\n    .. [1] Predicting Good Probabilities with Supervised Learning,\n          A. Niculescu-Mizil & R. Caruana, ICML 2005\n""""""\nprint(__doc__)\n\n# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport numpy as np\nnp.random.seed(0)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import calibration_curve\n\nX, y = datasets.make_classification(n_samples=100000, n_features=20,\n                                    n_informative=2, n_redundant=2)\n\ntrain_samples = 100  # Samples used for training the models\n\nX_train = X[:train_samples]\nX_test = X[train_samples:]\ny_train = y[:train_samples]\ny_test = y[train_samples:]\n\n# Create classifiers\nlr = LogisticRegression()\ngnb = GaussianNB()\nsvc = LinearSVC(C=1.0)\nrfc = RandomForestClassifier()\n\n\n# #############################################################################\n# Plot calibration plots\n\nplt.figure(figsize=(10, 10))\nax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax1.plot([0, 1], [0, 1], ""k:"", label=""Perfectly calibrated"")\nfor clf, name in [(lr, \'Logistic\'),\n                  (gnb, \'Naive Bayes\'),\n                  (svc, \'Support Vector Classification\'),\n                  (rfc, \'Random Forest\')]:\n    clf.fit(X_train, y_train)\n    if hasattr(clf, ""predict_proba""):\n        prob_pos = clf.predict_proba(X_test)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(X_test)\n        prob_pos = \\\n            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n    fraction_of_positives, mean_predicted_value = \\\n        calibration_curve(y_test, prob_pos, n_bins=10)\n\n    ax1.plot(mean_predicted_value, fraction_of_positives, ""s-"",\n             label=""%s"" % (name, ))\n\n    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n             histtype=""step"", lw=2)\n\nax1.set_ylabel(""Fraction of positives"")\nax1.set_ylim([-0.05, 1.05])\nax1.legend(loc=""lower right"")\nax1.set_title(\'Calibration plots  (reliability curve)\')\n\nax2.set_xlabel(""Mean predicted value"")\nax2.set_ylabel(""Count"")\nax2.legend(loc=""upper center"", ncol=2)\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/36301a9e1493af8b080687da818aaa61/plot_nca_classification.py,3,"b'""""""\n=============================================================================\nComparing Nearest Neighbors with and without Neighborhood Components Analysis\n=============================================================================\n\nAn example comparing nearest neighbors classification with and without\nNeighborhood Components Analysis.\n\nIt will plot the class decision boundaries given by a Nearest Neighbors\nclassifier when using the Euclidean distance on the original features, versus\nusing the Euclidean distance after the transformation learned by Neighborhood\nComponents Analysis. The latter aims to find a linear transformation that\nmaximises the (stochastic) nearest neighbor classification accuracy on the\ntraining set.\n""""""\n\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import (KNeighborsClassifier,\n                               NeighborhoodComponentsAnalysis)\nfrom sklearn.pipeline import Pipeline\n\n\nprint(__doc__)\n\nn_neighbors = 1\n\ndataset = datasets.load_iris()\nX, y = dataset.data, dataset.target\n\n# we only take two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = X[:, [0, 2]]\n\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, stratify=y, test_size=0.7, random_state=42)\n\nh = .01  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap([\'#FFAAAA\', \'#AAFFAA\', \'#AAAAFF\'])\ncmap_bold = ListedColormap([\'#FF0000\', \'#00FF00\', \'#0000FF\'])\n\nnames = [\'KNN\', \'NCA, KNN\']\n\nclassifiers = [Pipeline([(\'scaler\', StandardScaler()),\n                         (\'knn\', KNeighborsClassifier(n_neighbors=n_neighbors))\n                         ]),\n               Pipeline([(\'scaler\', StandardScaler()),\n                         (\'nca\', NeighborhoodComponentsAnalysis()),\n                         (\'knn\', KNeighborsClassifier(n_neighbors=n_neighbors))\n                         ])\n               ]\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nfor name, clf in zip(names, classifiers):\n\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, alpha=.8)\n\n    # Plot also the training and testing points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\'k\', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(""{} (k = {})"".format(name, n_neighbors))\n    plt.text(0.9, 0.1, \'{:.2f}\'.format(score), size=15,\n             ha=\'center\', va=\'center\', transform=plt.gca().transAxes)\n\nplt.show()\n'"
scikit-learn/_downloads/39e14dad35ff7831bfe3c51473e641c6/plot_ward_structured_vs_unstructured.py,4,"b'""""""\n===========================================================\nHierarchical clustering: structured vs unstructured ward\n===========================================================\n\nExample builds a swiss roll dataset and runs\nhierarchical clustering on their position.\n\nFor more information, see :ref:`hierarchical_clustering`.\n\nIn a first step, the hierarchical clustering is performed without connectivity\nconstraints on the structure and is solely based on distance, whereas in\na second step the clustering is restricted to the k-Nearest Neighbors\ngraph: it\'s a hierarchical clustering with structure prior.\n\nSome of the clusters learned without connectivity constraints do not\nrespect the structure of the swiss roll and extend across different folds of\nthe manifolds. On the opposite, when opposing connectivity constraints,\nthe clusters form a nice parcellation of the swiss roll.\n""""""\n\n# Authors : Vincent Michel, 2010\n#           Alexandre Gramfort, 2010\n#           Gael Varoquaux, 2010\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport time as time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mpl_toolkits.mplot3d.axes3d as p3\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_swiss_roll\n\n# #############################################################################\n# Generate data (swiss roll dataset)\nn_samples = 1500\nnoise = 0.05\nX, _ = make_swiss_roll(n_samples, noise)\n# Make it thinner\nX[:, 1] *= .5\n\n# #############################################################################\n# Compute clustering\nprint(""Compute unstructured hierarchical clustering..."")\nst = time.time()\nward = AgglomerativeClustering(n_clusters=6, linkage=\'ward\').fit(X)\nelapsed_time = time.time() - st\nlabel = ward.labels_\nprint(""Elapsed time: %.2fs"" % elapsed_time)\nprint(""Number of points: %i"" % label.size)\n\n# #############################################################################\n# Plot result\nfig = plt.figure()\nax = p3.Axes3D(fig)\nax.view_init(7, -80)\nfor l in np.unique(label):\n    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n               color=plt.cm.jet(np.float(l) / np.max(label + 1)),\n               s=20, edgecolor=\'k\')\nplt.title(\'Without connectivity constraints (time %.2fs)\' % elapsed_time)\n\n\n# #############################################################################\n# Define the structure A of the data. Here a 10 nearest neighbors\nfrom sklearn.neighbors import kneighbors_graph\nconnectivity = kneighbors_graph(X, n_neighbors=10, include_self=False)\n\n# #############################################################################\n# Compute clustering\nprint(""Compute structured hierarchical clustering..."")\nst = time.time()\nward = AgglomerativeClustering(n_clusters=6, connectivity=connectivity,\n                               linkage=\'ward\').fit(X)\nelapsed_time = time.time() - st\nlabel = ward.labels_\nprint(""Elapsed time: %.2fs"" % elapsed_time)\nprint(""Number of points: %i"" % label.size)\n\n# #############################################################################\n# Plot result\nfig = plt.figure()\nax = p3.Axes3D(fig)\nax.view_init(7, -80)\nfor l in np.unique(label):\n    ax.scatter(X[label == l, 0], X[label == l, 1], X[label == l, 2],\n               color=plt.cm.jet(float(l) / np.max(label + 1)),\n               s=20, edgecolor=\'k\')\nplt.title(\'With connectivity constraints (time %.2fs)\' % elapsed_time)\n\nplt.show()\n'"
scikit-learn/_downloads/3bd4cb56326b365a0f5c6f6edaa25928/plot_isotonic_regression.py,6,"b'""""""\n===================\nIsotonic Regression\n===================\n\nAn illustration of the isotonic regression on generated data. The\nisotonic regression finds a non-decreasing approximation of a function\nwhile minimizing the mean squared error on the training data. The benefit\nof such a model is that it does not assume any form for the target\nfunction such as linearity. For comparison a linear regression is also\npresented.\n\n""""""\nprint(__doc__)\n\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\nn = 100\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log1p(np.arange(n))\n\n# #############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\nir = IsotonicRegression()\n\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n\n# #############################################################################\n# Plot result\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(np.full(n, 0.5))\n\nfig = plt.figure()\nplt.plot(x, y, \'r.\', markersize=12)\nplt.plot(x, y_, \'b.-\', markersize=12)\nplt.plot(x, lr.predict(x[:, np.newaxis]), \'b-\')\nplt.gca().add_collection(lc)\nplt.legend((\'Data\', \'Isotonic Fit\', \'Linear Fit\'), loc=\'lower right\')\nplt.title(\'Isotonic regression\')\nplt.show()\n'"
scikit-learn/_downloads/3c4b03bbaccbf24951990921772f5dda/plot_birch_vs_minibatchkmeans.py,7,"b'""""""\n=================================\nCompare BIRCH and MiniBatchKMeans\n=================================\n\nThis example compares the timing of Birch (with and without the global\nclustering step) and MiniBatchKMeans on a synthetic dataset having\n100,000 samples and 2 features generated using make_blobs.\n\nIf ``n_clusters`` is set to None, the data is reduced from 100,000\nsamples to a set of 158 clusters. This can be viewed as a preprocessing\nstep before the final (global) clustering step that further reduces these\n158 clusters to 100 clusters.\n""""""\n\n# Authors: Manoj Kumar <manojkumarsivaraj334@gmail.com\n#          Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nfrom itertools import cycle\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\n\nfrom sklearn.cluster import Birch, MiniBatchKMeans\nfrom sklearn.datasets import make_blobs\n\n\n# Generate centers for the blobs so that it forms a 10 X 10 grid.\nxx = np.linspace(-22, 22, 10)\nyy = np.linspace(-22, 22, 10)\nxx, yy = np.meshgrid(xx, yy)\nn_centres = np.hstack((np.ravel(xx)[:, np.newaxis],\n                       np.ravel(yy)[:, np.newaxis]))\n\n# Generate blobs to do a comparison between MiniBatchKMeans and Birch.\nX, y = make_blobs(n_samples=100000, centers=n_centres, random_state=0)\n\n# Use all colors that matplotlib provides by default.\ncolors_ = cycle(colors.cnames.keys())\n\nfig = plt.figure(figsize=(12, 4))\nfig.subplots_adjust(left=0.04, right=0.98, bottom=0.1, top=0.9)\n\n# Compute clustering with Birch with and without the final clustering step\n# and plot.\nbirch_models = [Birch(threshold=1.7, n_clusters=None),\n                Birch(threshold=1.7, n_clusters=100)]\nfinal_step = [\'without global clustering\', \'with global clustering\']\n\nfor ind, (birch_model, info) in enumerate(zip(birch_models, final_step)):\n    t = time()\n    birch_model.fit(X)\n    time_ = time() - t\n    print(""Birch %s as the final step took %0.2f seconds"" % (\n          info, (time() - t)))\n\n    # Plot result\n    labels = birch_model.labels_\n    centroids = birch_model.subcluster_centers_\n    n_clusters = np.unique(labels).size\n    print(""n_clusters : %d"" % n_clusters)\n\n    ax = fig.add_subplot(1, 3, ind + 1)\n    for this_centroid, k, col in zip(centroids, range(n_clusters), colors_):\n        mask = labels == k\n        ax.scatter(X[mask, 0], X[mask, 1],\n                   c=\'w\', edgecolor=col, marker=\'.\', alpha=0.5)\n        if birch_model.n_clusters is None:\n            ax.scatter(this_centroid[0], this_centroid[1], marker=\'+\',\n                       c=\'k\', s=25)\n    ax.set_ylim([-25, 25])\n    ax.set_xlim([-25, 25])\n    ax.set_autoscaley_on(False)\n    ax.set_title(\'Birch %s\' % info)\n\n# Compute clustering with MiniBatchKMeans.\nmbk = MiniBatchKMeans(init=\'k-means++\', n_clusters=100, batch_size=100,\n                      n_init=10, max_no_improvement=10, verbose=0,\n                      random_state=0)\nt0 = time()\nmbk.fit(X)\nt_mini_batch = time() - t0\nprint(""Time taken to run MiniBatchKMeans %0.2f seconds"" % t_mini_batch)\nmbk_means_labels_unique = np.unique(mbk.labels_)\n\nax = fig.add_subplot(1, 3, 3)\nfor this_centroid, k, col in zip(mbk.cluster_centers_,\n                                 range(n_clusters), colors_):\n    mask = mbk.labels_ == k\n    ax.scatter(X[mask, 0], X[mask, 1], marker=\'.\',\n               c=\'w\', edgecolor=col, alpha=0.5)\n    ax.scatter(this_centroid[0], this_centroid[1], marker=\'+\',\n               c=\'k\', s=25)\nax.set_xlim([-25, 25])\nax.set_ylim([-25, 25])\nax.set_title(""MiniBatchKMeans"")\nax.set_autoscaley_on(False)\nplt.show()\n'"
scikit-learn/_downloads/3c5aa0365ca20c2512c0294614c07d31/plot_random_forest_regression_multioutput.py,3,"b'""""""\n============================================================\nComparing random forests and the multi-output meta estimator\n============================================================\n\nAn example to compare multi-output regression with random forest and\nthe :ref:`multioutput.MultiOutputRegressor <multiclass>` meta-estimator.\n\nThis example illustrates the use of the\n:ref:`multioutput.MultiOutputRegressor <multiclass>` meta-estimator\nto perform multi-output regression. A random forest regressor is used,\nwhich supports multi-output regression natively, so the results can be\ncompared.\n\nThe random forest regressor will only ever predict values within the\nrange of observations or closer to zero for each of the targets. As a\nresult the predictions are biased towards the centre of the circle.\n\nUsing a single underlying feature the model learns both the\nx and y coordinate as output.\n\n""""""\nprint(__doc__)\n\n# Author: Tim Head <betatim@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\n\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(200 * rng.rand(600, 1) - 100, axis=0)\ny = np.array([np.pi * np.sin(X).ravel(), np.pi * np.cos(X).ravel()]).T\ny += (0.5 - rng.rand(*y.shape))\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=400, test_size=200, random_state=4)\n\nmax_depth = 30\nregr_multirf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,\n                                                          max_depth=max_depth,\n                                                          random_state=0))\nregr_multirf.fit(X_train, y_train)\n\nregr_rf = RandomForestRegressor(n_estimators=100, max_depth=max_depth,\n                                random_state=2)\nregr_rf.fit(X_train, y_train)\n\n# Predict on new data\ny_multirf = regr_multirf.predict(X_test)\ny_rf = regr_rf.predict(X_test)\n\n# Plot the results\nplt.figure()\ns = 50\na = 0.4\nplt.scatter(y_test[:, 0], y_test[:, 1], edgecolor=\'k\',\n            c=""navy"", s=s, marker=""s"", alpha=a, label=""Data"")\nplt.scatter(y_multirf[:, 0], y_multirf[:, 1], edgecolor=\'k\',\n            c=""cornflowerblue"", s=s, alpha=a,\n            label=""Multi RF score=%.2f"" % regr_multirf.score(X_test, y_test))\nplt.scatter(y_rf[:, 0], y_rf[:, 1], edgecolor=\'k\',\n            c=""c"", s=s, marker=""^"", alpha=a,\n            label=""RF score=%.2f"" % regr_rf.score(X_test, y_test))\nplt.xlim([-6, 6])\nplt.ylim([-6, 6])\nplt.xlabel(""target 1"")\nplt.ylabel(""target 2"")\nplt.title(""Comparing random forests and the multi-output meta estimator"")\nplt.legend()\nplt.show()\n'"
scikit-learn/_downloads/3eb7801c461e56c2b5e53188283a922a/plot_ica_blind_source_separation.py,10,"b'""""""\n=====================================\nBlind source separation using FastICA\n=====================================\n\nAn example of estimating sources from noisy data.\n\n:ref:`ICA` is used to estimate sources given noisy measurements.\nImagine 3 instruments playing simultaneously and 3 microphones\nrecording the mixed signals. ICA is used to recover the sources\nie. what is played by each instrument. Importantly, PCA fails\nat recovering our `instruments` since the related signals reflect\nnon-Gaussian processes.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import signal\n\nfrom sklearn.decomposition import FastICA, PCA\n\n# #############################################################################\n# Generate sample data\nnp.random.seed(0)\nn_samples = 2000\ntime = np.linspace(0, 8, n_samples)\n\ns1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\ns2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\ns3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n\nS = np.c_[s1, s2, s3]\nS += 0.2 * np.random.normal(size=S.shape)  # Add noise\n\nS /= S.std(axis=0)  # Standardize data\n# Mix data\nA = np.array([[1, 1, 1], [0.5, 2, 1.0], [1.5, 1.0, 2.0]])  # Mixing matrix\nX = np.dot(S, A.T)  # Generate observations\n\n# Compute ICA\nica = FastICA(n_components=3)\nS_ = ica.fit_transform(X)  # Reconstruct signals\nA_ = ica.mixing_  # Get estimated mixing matrix\n\n# We can `prove` that the ICA model applies by reverting the unmixing.\nassert np.allclose(X, np.dot(S_, A_.T) + ica.mean_)\n\n# For comparison, compute PCA\npca = PCA(n_components=3)\nH = pca.fit_transform(X)  # Reconstruct signals based on orthogonal components\n\n# #############################################################################\n# Plot results\n\nplt.figure()\n\nmodels = [X, S, S_, H]\nnames = [\'Observations (mixed signal)\',\n         \'True Sources\',\n         \'ICA recovered signals\', \n         \'PCA recovered signals\']\ncolors = [\'red\', \'steelblue\', \'orange\']\n\nfor ii, (model, name) in enumerate(zip(models, names), 1):\n    plt.subplot(4, 1, ii)\n    plt.title(name)\n    for sig, color in zip(model.T, colors):\n        plt.plot(sig, color=color)\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/3eb810ea6391ebfae795f0f7e941c079/plot_gpr_noisy_targets.py,12,"b'""""""\n=========================================================\nGaussian Processes regression: basic introductory example\n=========================================================\n\nA simple one-dimensional regression example computed in two different ways:\n\n1. A noise-free case\n2. A noisy case with known noise-level per datapoint\n\nIn both cases, the kernel\'s parameters are estimated using the maximum\nlikelihood principle.\n\nThe figures illustrate the interpolating property of the Gaussian Process\nmodel as well as its probabilistic nature in the form of a pointwise 95%\nconfidence interval.\n\nNote that the parameter ``alpha`` is applied as a Tikhonov\nregularization of the assumed covariance between the training points.\n""""""\nprint(__doc__)\n\n# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n#         Jake Vanderplas <vanderplas@astro.washington.edu>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>s\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n\nnp.random.seed(1)\n\n\ndef f(x):\n    """"""The function to predict.""""""\n    return x * np.sin(x)\n\n# ----------------------------------------------------------------------\n#  First the noiseless case\nX = np.atleast_2d([1., 3., 5., 6., 7., 8.]).T\n\n# Observations\ny = f(X).ravel()\n\n# Mesh the input space for evaluations of the real function, the prediction and\n# its MSE\nx = np.atleast_2d(np.linspace(0, 10, 1000)).T\n\n# Instantiate a Gaussian Process model\nkernel = C(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2))\ngp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n\n# Fit to data using Maximum Likelihood Estimation of the parameters\ngp.fit(X, y)\n\n# Make the prediction on the meshed x-axis (ask for MSE as well)\ny_pred, sigma = gp.predict(x, return_std=True)\n\n# Plot the function, the prediction and the 95% confidence interval based on\n# the MSE\nplt.figure()\nplt.plot(x, f(x), \'r:\', label=r\'$f(x) = x\\,\\sin(x)$\')\nplt.plot(X, y, \'r.\', markersize=10, label=\'Observations\')\nplt.plot(x, y_pred, \'b-\', label=\'Prediction\')\nplt.fill(np.concatenate([x, x[::-1]]),\n         np.concatenate([y_pred - 1.9600 * sigma,\n                        (y_pred + 1.9600 * sigma)[::-1]]),\n         alpha=.5, fc=\'b\', ec=\'None\', label=\'95% confidence interval\')\nplt.xlabel(\'$x$\')\nplt.ylabel(\'$f(x)$\')\nplt.ylim(-10, 20)\nplt.legend(loc=\'upper left\')\n\n# ----------------------------------------------------------------------\n# now the noisy case\nX = np.linspace(0.1, 9.9, 20)\nX = np.atleast_2d(X).T\n\n# Observations and noise\ny = f(X).ravel()\ndy = 0.5 + 1.0 * np.random.random(y.shape)\nnoise = np.random.normal(0, dy)\ny += noise\n\n# Instantiate a Gaussian Process model\ngp = GaussianProcessRegressor(kernel=kernel, alpha=dy ** 2,\n                              n_restarts_optimizer=10)\n\n# Fit to data using Maximum Likelihood Estimation of the parameters\ngp.fit(X, y)\n\n# Make the prediction on the meshed x-axis (ask for MSE as well)\ny_pred, sigma = gp.predict(x, return_std=True)\n\n# Plot the function, the prediction and the 95% confidence interval based on\n# the MSE\nplt.figure()\nplt.plot(x, f(x), \'r:\', label=r\'$f(x) = x\\,\\sin(x)$\')\nplt.errorbar(X.ravel(), y, dy, fmt=\'r.\', markersize=10, label=\'Observations\')\nplt.plot(x, y_pred, \'b-\', label=\'Prediction\')\nplt.fill(np.concatenate([x, x[::-1]]),\n         np.concatenate([y_pred - 1.9600 * sigma,\n                        (y_pred + 1.9600 * sigma)[::-1]]),\n         alpha=.5, fc=\'b\', ec=\'None\', label=\'95% confidence interval\')\nplt.xlabel(\'$x$\')\nplt.ylabel(\'$f(x)$\')\nplt.ylim(-10, 20)\nplt.legend(loc=\'upper left\')\n\nplt.show()\n'"
scikit-learn/_downloads/3f73907fe647254d0957e71cefbd33f9/plot_affinity_propagation.py,0,"b'""""""\n=================================================\nDemo of affinity propagation clustering algorithm\n=================================================\n\nReference:\nBrendan J. Frey and Delbert Dueck, ""Clustering by Passing Messages\nBetween Data Points"", Science Feb. 2007\n\n""""""\nprint(__doc__)\n\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\n\n# #############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,\n                            random_state=0)\n\n# #############################################################################\n# Compute Affinity Propagation\naf = AffinityPropagation(preference=-50).fit(X)\ncluster_centers_indices = af.cluster_centers_indices_\nlabels = af.labels_\n\nn_clusters_ = len(cluster_centers_indices)\n\nprint(\'Estimated number of clusters: %d\' % n_clusters_)\nprint(""Homogeneity: %0.3f"" % metrics.homogeneity_score(labels_true, labels))\nprint(""Completeness: %0.3f"" % metrics.completeness_score(labels_true, labels))\nprint(""V-measure: %0.3f"" % metrics.v_measure_score(labels_true, labels))\nprint(""Adjusted Rand Index: %0.3f""\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint(""Adjusted Mutual Information: %0.3f""\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint(""Silhouette Coefficient: %0.3f""\n      % metrics.silhouette_score(X, labels, metric=\'sqeuclidean\'))\n\n# #############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.close(\'all\')\nplt.figure(1)\nplt.clf()\n\ncolors = cycle(\'bgrcmykbgrcmykbgrcmykbgrcmyk\')\nfor k, col in zip(range(n_clusters_), colors):\n    class_members = labels == k\n    cluster_center = X[cluster_centers_indices[k]]\n    plt.plot(X[class_members, 0], X[class_members, 1], col + \'.\')\n    plt.plot(cluster_center[0], cluster_center[1], \'o\', markerfacecolor=col,\n             markeredgecolor=\'k\', markersize=14)\n    for x in X[class_members]:\n        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n\nplt.title(\'Estimated number of clusters: %d\' % n_clusters_)\nplt.show()\n'"
scikit-learn/_downloads/43da202a3d3424ab1f7eedc2a8af7e21/plot_pca_3d.py,10,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nPrincipal components analysis (PCA)\n=========================================================\n\nThese figures aid in illustrating how a point cloud\ncan be very flat in one direction--which is where PCA\ncomes in to choose a direction that is not flat.\n\n""""""\nprint(__doc__)\n\n# Authors: Gael Varoquaux\n#          Jaques Grobler\n#          Kevin Hughes\n# License: BSD 3 clause\n\nfrom sklearn.decomposition import PCA\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n\n# #############################################################################\n# Create the data\n\ne = np.exp(1)\nnp.random.seed(4)\n\n\ndef pdf(x):\n    return 0.5 * (stats.norm(scale=0.25 / e).pdf(x)\n                  + stats.norm(scale=4 / e).pdf(x))\n\ny = np.random.normal(scale=0.5, size=(30000))\nx = np.random.normal(scale=0.5, size=(30000))\nz = np.random.normal(scale=0.1, size=len(x))\n\ndensity = pdf(x) * pdf(y)\npdf_z = pdf(5 * z)\n\ndensity *= pdf_z\n\na = x + y\nb = 2 * y\nc = a - b + z\n\nnorm = np.sqrt(a.var() + b.var())\na /= norm\nb /= norm\n\n\n# #############################################################################\n# Plot the figures\ndef plot_figs(fig_num, elev, azim):\n    fig = plt.figure(fig_num, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=elev, azim=azim)\n\n    ax.scatter(a[::10], b[::10], c[::10], c=density[::10], marker=\'+\', alpha=.4)\n    Y = np.c_[a, b, c]\n\n    # Using SciPy\'s SVD, this would be:\n    # _, pca_score, V = scipy.linalg.svd(Y, full_matrices=False)\n\n    pca = PCA(n_components=3)\n    pca.fit(Y)\n    pca_score = pca.explained_variance_ratio_\n    V = pca.components_\n\n    x_pca_axis, y_pca_axis, z_pca_axis = 3 * V.T\n    x_pca_plane = np.r_[x_pca_axis[:2], - x_pca_axis[1::-1]]\n    y_pca_plane = np.r_[y_pca_axis[:2], - y_pca_axis[1::-1]]\n    z_pca_plane = np.r_[z_pca_axis[:2], - z_pca_axis[1::-1]]\n    x_pca_plane.shape = (2, 2)\n    y_pca_plane.shape = (2, 2)\n    z_pca_plane.shape = (2, 2)\n    ax.plot_surface(x_pca_plane, y_pca_plane, z_pca_plane)\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n\n\nelev = -40\nazim = -80\nplot_figs(1, elev, azim)\n\nelev = 30\nazim = 20\nplot_figs(2, elev, azim)\n\nplt.show()\n'"
scikit-learn/_downloads/450c5e26cf78b5ec46b5ddbe1011b97b/plot_lof_outlier_detection.py,6,"b'""""""\n=================================================\nOutlier detection with Local Outlier Factor (LOF)\n=================================================\n\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection\nmethod which computes the local density deviation of a given data point with\nrespect to its neighbors. It considers as outliers the samples that have a\nsubstantially lower density than their neighbors. This example shows how to\nuse LOF for outlier detection which is the default use case of this estimator\nin scikit-learn. Note that when LOF is used for outlier detection it has no\npredict, decision_function and score_samples methods. See\n:ref:`User Guide <outlier_detection>`: for details on the difference between\noutlier detection and novelty detection and how to use LOF for novelty\ndetection.\n\nThe number of neighbors considered (parameter n_neighbors) is typically\nset 1) greater than the minimum number of samples a cluster has to contain,\nso that other samples can be local outliers relative to this cluster, and 2)\nsmaller than the maximum number of close by samples that can potentially be\nlocal outliers.\nIn practice, such informations are generally not available, and taking\nn_neighbors=20 appears to work well in general.\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import LocalOutlierFactor\n\nprint(__doc__)\n\nnp.random.seed(42)\n\n# Generate train data\nX_inliers = 0.3 * np.random.randn(100, 2)\nX_inliers = np.r_[X_inliers + 2, X_inliers - 2]\n\n# Generate some outliers\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\nX = np.r_[X_inliers, X_outliers]\n\nn_outliers = len(X_outliers)\nground_truth = np.ones(len(X), dtype=int)\nground_truth[-n_outliers:] = -1\n\n# fit the model for outlier detection (default)\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n# use fit_predict to compute the predicted labels of the training samples\n# (when LOF is used for outlier detection, the estimator has no predict,\n# decision_function and score_samples methods).\ny_pred = clf.fit_predict(X)\nn_errors = (y_pred != ground_truth).sum()\nX_scores = clf.negative_outlier_factor_\n\nplt.title(""Local Outlier Factor (LOF)"")\nplt.scatter(X[:, 0], X[:, 1], color=\'k\', s=3., label=\'Data points\')\n# plot circles with radius proportional to the outlier scores\nradius = (X_scores.max() - X_scores) / (X_scores.max() - X_scores.min())\nplt.scatter(X[:, 0], X[:, 1], s=1000 * radius, edgecolors=\'r\',\n            facecolors=\'none\', label=\'Outlier scores\')\nplt.axis(\'tight\')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.xlabel(""prediction errors: %d"" % (n_errors))\nlegend = plt.legend(loc=\'upper left\')\nlegend.legendHandles[0]._sizes = [10]\nlegend.legendHandles[1]._sizes = [20]\nplt.show()\n'"
scikit-learn/_downloads/456bfa6a999e82b41e010d943ceb4b35/plot_sgd_comparison.py,4,"b'""""""\n==================================\nComparing various online solvers\n==================================\n\nAn example showing how different online solvers perform\non the hand-written digits dataset.\n\n""""""\n# Author: Rob Zinkov <rob at zinkov dot com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier, Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nheldout = [0.95, 0.90, 0.75, 0.50, 0.01]\nrounds = 20\nX, y = datasets.load_digits(return_X_y=True)\n\nclassifiers = [\n    (""SGD"", SGDClassifier(max_iter=100)),\n    (""ASGD"", SGDClassifier(average=True)),\n    (""Perceptron"", Perceptron()),\n    (""Passive-Aggressive I"", PassiveAggressiveClassifier(loss=\'hinge\',\n                                                         C=1.0, tol=1e-4)),\n    (""Passive-Aggressive II"", PassiveAggressiveClassifier(loss=\'squared_hinge\',\n                                                          C=1.0, tol=1e-4)),\n    (""SAG"", LogisticRegression(solver=\'sag\', tol=1e-1, C=1.e4 / X.shape[0]))\n]\n\nxx = 1. - np.array(heldout)\n\nfor name, clf in classifiers:\n    print(""training %s"" % name)\n    rng = np.random.RandomState(42)\n    yy = []\n    for i in heldout:\n        yy_ = []\n        for r in range(rounds):\n            X_train, X_test, y_train, y_test = \\\n                train_test_split(X, y, test_size=i, random_state=rng)\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            yy_.append(1 - np.mean(y_pred == y_test))\n        yy.append(np.mean(yy_))\n    plt.plot(xx, yy, label=name)\n\nplt.legend(loc=""upper right"")\nplt.xlabel(""Proportion train"")\nplt.ylabel(""Test Error Rate"")\nplt.show()\n'"
scikit-learn/_downloads/4643581371494a074ff9180ed4a4c052/plot_feature_union.py,0,"b'""""""\n=================================================\nConcatenating multiple feature extraction methods\n=================================================\n\nIn many real-world examples, there are many ways to extract features from a\ndataset. Often it is beneficial to combine several methods to obtain good\nperformance. This example shows how to use ``FeatureUnion`` to combine\nfeatures obtained by PCA and univariate selection.\n\nCombining features using this transformer has the benefit that it allows\ncross validation and grid searches over the whole process.\n\nThe combination used in this example is not particularly helpful on this\ndataset and is only used to illustrate the usage of FeatureUnion.\n""""""\n\n# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#\n# License: BSD 3 clause\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest\n\niris = load_iris()\n\nX, y = iris.data, iris.target\n\n# This dataset is way too high-dimensional. Better do PCA:\npca = PCA(n_components=2)\n\n# Maybe some original features where good, too?\nselection = SelectKBest(k=1)\n\n# Build estimator from PCA and Univariate selection:\n\ncombined_features = FeatureUnion([(""pca"", pca), (""univ_select"", selection)])\n\n# Use combined features to transform dataset:\nX_features = combined_features.fit(X, y).transform(X)\nprint(""Combined space has"", X_features.shape[1], ""features"")\n\nsvm = SVC(kernel=""linear"")\n\n# Do grid search over k, n_components and C:\n\npipeline = Pipeline([(""features"", combined_features), (""svm"", svm)])\n\nparam_grid = dict(features__pca__n_components=[1, 2, 3],\n                  features__univ_select__k=[1, 2],\n                  svm__C=[0.1, 1, 10])\n\ngrid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\ngrid_search.fit(X, y)\nprint(grid_search.best_estimator_)\n'"
scikit-learn/_downloads/4b8e7addc5ad2e5accb6e27a4df31aec/plot_gradient_boosting_oob.py,14,"b'""""""\n======================================\nGradient Boosting Out-of-Bag estimates\n======================================\n\nOut-of-bag (OOB) estimates can be a useful heuristic to estimate\nthe ""optimal"" number of boosting iterations.\nOOB estimates are almost identical to cross-validation estimates but\nthey can be computed on-the-fly without the need for repeated model\nfitting.\nOOB estimates are only available for Stochastic Gradient Boosting\n(i.e. ``subsample < 1.0``), the estimates are derived from the improvement\nin loss based on the examples not included in the bootstrap sample\n(the so-called out-of-bag examples).\nThe OOB estimator is a pessimistic estimator of the true\ntest loss, but remains a fairly good approximation for a small number of trees.\n\nThe figure shows the cumulative sum of the negative OOB improvements\nas a function of the boosting iteration. As you can see, it tracks the test\nloss for the first hundred iterations but then diverges in a\npessimistic way.\nThe figure also shows the performance of 3-fold cross validation which\nusually gives a better estimate of the test loss\nbut is computationally more demanding.\n""""""\nprint(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import ensemble\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy.special import expit\n\n# Generate data (adapted from G. Ridgeway\'s gbm example)\nn_samples = 1000\nrandom_state = np.random.RandomState(13)\nx1 = random_state.uniform(size=n_samples)\nx2 = random_state.uniform(size=n_samples)\nx3 = random_state.randint(0, 4, size=n_samples)\n\np = expit(np.sin(3 * x1) - 4 * x2 + x3)\ny = random_state.binomial(1, p, size=n_samples)\n\nX = np.c_[x1, x2, x3]\n\nX = X.astype(np.float32)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n                                                    random_state=9)\n\n# Fit classifier with out-of-bag estimates\nparams = {\'n_estimators\': 1200, \'max_depth\': 3, \'subsample\': 0.5,\n          \'learning_rate\': 0.01, \'min_samples_leaf\': 1, \'random_state\': 3}\nclf = ensemble.GradientBoostingClassifier(**params)\n\nclf.fit(X_train, y_train)\nacc = clf.score(X_test, y_test)\nprint(""Accuracy: {:.4f}"".format(acc))\n\nn_estimators = params[\'n_estimators\']\nx = np.arange(n_estimators) + 1\n\n\ndef heldout_score(clf, X_test, y_test):\n    """"""compute deviance scores on ``X_test`` and ``y_test``. """"""\n    score = np.zeros((n_estimators,), dtype=np.float64)\n    for i, y_pred in enumerate(clf.staged_decision_function(X_test)):\n        score[i] = clf.loss_(y_test, y_pred)\n    return score\n\n\ndef cv_estimate(n_splits=None):\n    cv = KFold(n_splits=n_splits)\n    cv_clf = ensemble.GradientBoostingClassifier(**params)\n    val_scores = np.zeros((n_estimators,), dtype=np.float64)\n    for train, test in cv.split(X_train, y_train):\n        cv_clf.fit(X_train[train], y_train[train])\n        val_scores += heldout_score(cv_clf, X_train[test], y_train[test])\n    val_scores /= n_splits\n    return val_scores\n\n\n# Estimate best n_estimator using cross-validation\ncv_score = cv_estimate(3)\n\n# Compute best n_estimator for test data\ntest_score = heldout_score(clf, X_test, y_test)\n\n# negative cumulative sum of oob improvements\ncumsum = -np.cumsum(clf.oob_improvement_)\n\n# min loss according to OOB\noob_best_iter = x[np.argmin(cumsum)]\n\n# min loss according to test (normalize such that first loss is 0)\ntest_score -= test_score[0]\ntest_best_iter = x[np.argmin(test_score)]\n\n# min loss according to cv (normalize such that first loss is 0)\ncv_score -= cv_score[0]\ncv_best_iter = x[np.argmin(cv_score)]\n\n# color brew for the three curves\noob_color = list(map(lambda x: x / 256.0, (190, 174, 212)))\ntest_color = list(map(lambda x: x / 256.0, (127, 201, 127)))\ncv_color = list(map(lambda x: x / 256.0, (253, 192, 134)))\n\n# plot curves and vertical lines for best iterations\nplt.plot(x, cumsum, label=\'OOB loss\', color=oob_color)\nplt.plot(x, test_score, label=\'Test loss\', color=test_color)\nplt.plot(x, cv_score, label=\'CV loss\', color=cv_color)\nplt.axvline(x=oob_best_iter, color=oob_color)\nplt.axvline(x=test_best_iter, color=test_color)\nplt.axvline(x=cv_best_iter, color=cv_color)\n\n# add three vertical lines to xticks\nxticks = plt.xticks()\nxticks_pos = np.array(xticks[0].tolist() +\n                      [oob_best_iter, cv_best_iter, test_best_iter])\nxticks_label = np.array(list(map(lambda t: int(t), xticks[0])) +\n                        [\'OOB\', \'CV\', \'Test\'])\nind = np.argsort(xticks_pos)\nxticks_pos = xticks_pos[ind]\nxticks_label = xticks_label[ind]\nplt.xticks(xticks_pos, xticks_label)\n\nplt.legend(loc=\'upper right\')\nplt.ylabel(\'normalized loss\')\nplt.xlabel(\'number of iterations\')\n\nplt.show()\n'"
scikit-learn/_downloads/4bee606562b89a350cbe3be0a43afe55/plot_gmm_sin.py,14,"b'""""""\n=================================\nGaussian Mixture Model Sine Curve\n=================================\n\nThis example demonstrates the behavior of Gaussian mixture models fit on data\nthat was not sampled from a mixture of Gaussian random variables. The dataset\nis formed by 100 points loosely spaced following a noisy sine curve. There is\ntherefore no ground truth value for the number of Gaussian components.\n\nThe first model is a classical Gaussian Mixture Model with 10 components fit\nwith the Expectation-Maximization algorithm.\n\nThe second model is a Bayesian Gaussian Mixture Model with a Dirichlet process\nprior fit with variational inference. The low value of the concentration prior\nmakes the model favor a lower number of active components. This models\n""decides"" to focus its modeling power on the big picture of the structure of\nthe dataset: groups of points with alternating directions modeled by\nnon-diagonal covariance matrices. Those alternating directions roughly capture\nthe alternating nature of the original sine signal.\n\nThe third model is also a Bayesian Gaussian mixture model with a Dirichlet\nprocess prior but this time the value of the concentration prior is higher\ngiving the model more liberty to model the fine-grained structure of the data.\nThe result is a mixture with a larger number of active components that is\nsimilar to the first model where we arbitrarily decided to fix the number of\ncomponents to 10.\n\nWhich model is the best is a matter of subjective judgement: do we want to\nfavor models that only capture the big picture to summarize and explain most of\nthe structure of the data while ignoring the details or do we prefer models\nthat closely follow the high density regions of the signal?\n\nThe last two panels show how we can sample from the last two models. The\nresulting samples distributions do not look exactly like the original data\ndistribution. The difference primarily stems from the approximation error we\nmade by using a model that assumes that the data was generated by a finite\nnumber of Gaussian components instead of a continuous noisy sine curve.\n\n""""""\n\nimport itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\nprint(__doc__)\n\ncolor_iter = itertools.cycle([\'navy\', \'c\', \'cornflowerblue\', \'gold\',\n                              \'darkorange\'])\n\n\ndef plot_results(X, Y, means, covariances, index, title):\n    splot = plt.subplot(5, 1, 1 + index)\n    for i, (mean, covar, color) in enumerate(zip(\n            means, covariances, color_iter)):\n        v, w = linalg.eigh(covar)\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        u = w[0] / linalg.norm(w[0])\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn\'t plot the redundant\n        # components.\n        if not np.any(Y == i):\n            continue\n        plt.scatter(X[Y == i, 0], X[Y == i, 1], .8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = np.arctan(u[1] / u[0])\n        angle = 180. * angle / np.pi  # convert to degrees\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(0.5)\n        splot.add_artist(ell)\n\n    plt.xlim(-6., 4. * np.pi - 6.)\n    plt.ylim(-5., 5.)\n    plt.title(title)\n    plt.xticks(())\n    plt.yticks(())\n\n\ndef plot_samples(X, Y, n_components, index, title):\n    plt.subplot(5, 1, 4 + index)\n    for i, color in zip(range(n_components), color_iter):\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn\'t plot the redundant\n        # components.\n        if not np.any(Y == i):\n            continue\n        plt.scatter(X[Y == i, 0], X[Y == i, 1], .8, color=color)\n\n    plt.xlim(-6., 4. * np.pi - 6.)\n    plt.ylim(-5., 5.)\n    plt.title(title)\n    plt.xticks(())\n    plt.yticks(())\n\n\n# Parameters\nn_samples = 100\n\n# Generate random sample following a sine curve\nnp.random.seed(0)\nX = np.zeros((n_samples, 2))\nstep = 4. * np.pi / n_samples\n\nfor i in range(X.shape[0]):\n    x = i * step - 6.\n    X[i, 0] = x + np.random.normal(0, 0.1)\n    X[i, 1] = 3. * (np.sin(x) + np.random.normal(0, .2))\n\nplt.figure(figsize=(10, 10))\nplt.subplots_adjust(bottom=.04, top=0.95, hspace=.2, wspace=.05,\n                    left=.03, right=.97)\n\n# Fit a Gaussian mixture with EM using ten components\ngmm = mixture.GaussianMixture(n_components=10, covariance_type=\'full\',\n                              max_iter=100).fit(X)\nplot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0,\n             \'Expectation-maximization\')\n\ndpgmm = mixture.BayesianGaussianMixture(\n    n_components=10, covariance_type=\'full\', weight_concentration_prior=1e-2,\n    weight_concentration_prior_type=\'dirichlet_process\',\n    mean_precision_prior=1e-2, covariance_prior=1e0 * np.eye(2),\n    init_params=""random"", max_iter=100, random_state=2).fit(X)\nplot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,\n             ""Bayesian Gaussian mixture models with a Dirichlet process prior ""\n             r""for $\\gamma_0=0.01$."")\n\nX_s, y_s = dpgmm.sample(n_samples=2000)\nplot_samples(X_s, y_s, dpgmm.n_components, 0,\n             ""Gaussian mixture with a Dirichlet process prior ""\n             r""for $\\gamma_0=0.01$ sampled with $2000$ samples."")\n\ndpgmm = mixture.BayesianGaussianMixture(\n    n_components=10, covariance_type=\'full\', weight_concentration_prior=1e+2,\n    weight_concentration_prior_type=\'dirichlet_process\',\n    mean_precision_prior=1e-2, covariance_prior=1e0 * np.eye(2),\n    init_params=""kmeans"", max_iter=100, random_state=2).fit(X)\nplot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 2,\n             ""Bayesian Gaussian mixture models with a Dirichlet process prior ""\n             r""for $\\gamma_0=100$"")\n\nX_s, y_s = dpgmm.sample(n_samples=2000)\nplot_samples(X_s, y_s, dpgmm.n_components, 1,\n             ""Gaussian mixture with a Dirichlet process prior ""\n             r""for $\\gamma_0=100$ sampled with $2000$ samples."")\n\nplt.show()\n'"
scikit-learn/_downloads/4c14050c72c23bc27c190daa738a2e1b/plot_voting_regressor.py,0,"b'""""""\n=================================================\nPlot individual and voting regression predictions\n=================================================\n\n.. currentmodule:: sklearn\n\nPlot individual and averaged regression predictions for Boston dataset.\n\nFirst, three exemplary regressors are initialized\n(:class:`~ensemble.GradientBoostingRegressor`,\n:class:`~ensemble.RandomForestRegressor`, and\n:class:`~linear_model.LinearRegression`) and used to initialize a\n:class:`~ensemble.VotingRegressor`.\n\nThe red starred dots are the averaged predictions.\n\n""""""\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor\n\n# Loading some example data\nX, y = datasets.load_boston(return_X_y=True)\n\n# Training classifiers\nreg1 = GradientBoostingRegressor(random_state=1, n_estimators=10)\nreg2 = RandomForestRegressor(random_state=1, n_estimators=10)\nreg3 = LinearRegression()\nereg = VotingRegressor([(\'gb\', reg1), (\'rf\', reg2), (\'lr\', reg3)])\nreg1.fit(X, y)\nreg2.fit(X, y)\nreg3.fit(X, y)\nereg.fit(X, y)\n\nxt = X[:20]\n\nplt.figure()\nplt.plot(reg1.predict(xt), \'gd\', label=\'GradientBoostingRegressor\')\nplt.plot(reg2.predict(xt), \'b^\', label=\'RandomForestRegressor\')\nplt.plot(reg3.predict(xt), \'ys\', label=\'LinearRegression\')\nplt.plot(ereg.predict(xt), \'r*\', label=\'VotingRegressor\')\nplt.tick_params(axis=\'x\', which=\'both\', bottom=False, top=False,\n                labelbottom=False)\nplt.ylabel(\'predicted\')\nplt.xlabel(\'training samples\')\nplt.legend(loc=""best"")\nplt.title(\'Comparison of individual predictions with averaged\')\nplt.show()\n'"
scikit-learn/_downloads/4d319dd5d98dfa90af6158a16b37746e/plot_isolation_forest.py,5,"b'""""""\n==========================================\nIsolationForest example\n==========================================\n\nAn example using :class:`sklearn.ensemble.IsolationForest` for anomaly\ndetection.\n\nThe IsolationForest \'isolates\' observations by randomly selecting a feature\nand then randomly selecting a split value between the maximum and minimum\nvalues of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a measure\nof normality and our decision function.\n\nRandom partitioning produces noticeable shorter paths for anomalies.\nHence, when a forest of random trees collectively produce shorter path lengths\nfor particular samples, they are highly likely to be anomalies.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\n\nrng = np.random.RandomState(42)\n\n# Generate train data\nX = 0.3 * rng.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * rng.randn(20, 2)\nX_test = np.r_[X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n\n# fit the model\nclf = IsolationForest(max_samples=100, random_state=rng)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\n\n# plot the line, the samples, and the nearest vectors to the plane\nxx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title(""IsolationForest"")\nplt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\'white\',\n                 s=20, edgecolor=\'k\')\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\'green\',\n                 s=20, edgecolor=\'k\')\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\'red\',\n                s=20, edgecolor=\'k\')\nplt.axis(\'tight\')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend([b1, b2, c],\n           [""training observations"",\n            ""new regular observations"", ""new abnormal observations""],\n           loc=""upper left"")\nplt.show()\n'"
scikit-learn/_downloads/4d8030e9bbbc5364747805d6baf8f322/plot_classifier_chain_yeast.py,2,"b'""""""\n============================\nClassifier Chain\n============================\nExample of using classifier chain on a multilabel dataset.\n\nFor this example we will use the `yeast\n<https://www.openml.org/d/40597>`_ dataset which contains\n2417 datapoints each with 103 features and 14 possible labels. Each\ndata point has at least one label. As a baseline we first train a logistic\nregression classifier for each of the 14 labels. To evaluate the performance of\nthese classifiers we predict on a held-out test set and calculate the\n:ref:`jaccard score <jaccard_similarity_score>` for each sample.\n\nNext we create 10 classifier chains. Each classifier chain contains a\nlogistic regression model for each of the 14 labels. The models in each\nchain are ordered randomly. In addition to the 103 features in the dataset,\neach model gets the predictions of the preceding models in the chain as\nfeatures (note that by default at training time each model gets the true\nlabels as features). These additional features allow each chain to exploit\ncorrelations among the classes. The Jaccard similarity score for each chain\ntends to be greater than that of the set independent logistic models.\n\nBecause the models in each chain are arranged randomly there is significant\nvariation in performance among the chains. Presumably there is an optimal\nordering of the classes in a chain that will yield the best performance.\nHowever we do not know that ordering a priori. Instead we can construct an\nvoting ensemble of classifier chains by averaging the binary predictions of\nthe chains and apply a threshold of 0.5. The Jaccard similarity score of the\nensemble is greater than that of the independent models and tends to exceed\nthe score of each chain in the ensemble (although this is not guaranteed\nwith randomly ordered chains).\n""""""\n\n# Author: Adam Kleczewski\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.multioutput import ClassifierChain\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.linear_model import LogisticRegression\n\nprint(__doc__)\n\n# Load a multi-label dataset from https://www.openml.org/d/40597\nX, Y = fetch_openml(\'yeast\', version=4, return_X_y=True)\nY = Y == \'TRUE\'\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2,\n                                                    random_state=0)\n\n# Fit an independent logistic regression model for each class using the\n# OneVsRestClassifier wrapper.\nbase_lr = LogisticRegression()\novr = OneVsRestClassifier(base_lr)\novr.fit(X_train, Y_train)\nY_pred_ovr = ovr.predict(X_test)\novr_jaccard_score = jaccard_score(Y_test, Y_pred_ovr, average=\'samples\')\n\n# Fit an ensemble of logistic regression classifier chains and take the\n# take the average prediction of all the chains.\nchains = [ClassifierChain(base_lr, order=\'random\', random_state=i)\n          for i in range(10)]\nfor chain in chains:\n    chain.fit(X_train, Y_train)\n\nY_pred_chains = np.array([chain.predict(X_test) for chain in\n                          chains])\nchain_jaccard_scores = [jaccard_score(Y_test, Y_pred_chain >= .5,\n                                      average=\'samples\')\n                        for Y_pred_chain in Y_pred_chains]\n\nY_pred_ensemble = Y_pred_chains.mean(axis=0)\nensemble_jaccard_score = jaccard_score(Y_test,\n                                       Y_pred_ensemble >= .5,\n                                       average=\'samples\')\n\nmodel_scores = [ovr_jaccard_score] + chain_jaccard_scores\nmodel_scores.append(ensemble_jaccard_score)\n\nmodel_names = (\'Independent\',\n               \'Chain 1\',\n               \'Chain 2\',\n               \'Chain 3\',\n               \'Chain 4\',\n               \'Chain 5\',\n               \'Chain 6\',\n               \'Chain 7\',\n               \'Chain 8\',\n               \'Chain 9\',\n               \'Chain 10\',\n               \'Ensemble\')\n\nx_pos = np.arange(len(model_names))\n\n# Plot the Jaccard similarity scores for the independent model, each of the\n# chains, and the ensemble (note that the vertical axis on this plot does\n# not begin at 0).\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.grid(True)\nax.set_title(\'Classifier Chain Ensemble Performance Comparison\')\nax.set_xticks(x_pos)\nax.set_xticklabels(model_names, rotation=\'vertical\')\nax.set_ylabel(\'Jaccard Similarity Score\')\nax.set_ylim([min(model_scores) * .9, max(model_scores) * 1.1])\ncolors = [\'r\'] + [\'b\'] * len(chain_jaccard_scores) + [\'g\']\nax.bar(x_pos, model_scores, alpha=0.5, color=colors)\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/4f33e234e6af958ec0046222fd7b542f/plot_lof_novelty_detection.py,9,"b'""""""\n=================================================\nNovelty detection with Local Outlier Factor (LOF)\n=================================================\n\nThe Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection\nmethod which computes the local density deviation of a given data point with\nrespect to its neighbors. It considers as outliers the samples that have a\nsubstantially lower density than their neighbors. This example shows how to\nuse LOF for novelty detection. Note that when LOF is used for novelty\ndetection you MUST not use predict, decision_function and score_samples on the\ntraining set as this would lead to wrong results. You must only use these\nmethods on new unseen data (which are not in the training set). See\n:ref:`User Guide <outlier_detection>`: for details on the difference between\noutlier detection and novelty detection and how to use LOF for outlier\ndetection.\n\nThe number of neighbors considered, (parameter n_neighbors) is typically\nset 1) greater than the minimum number of samples a cluster has to contain,\nso that other samples can be local outliers relative to this cluster, and 2)\nsmaller than the maximum number of close by samples that can potentially be\nlocal outliers.\nIn practice, such informations are generally not available, and taking\nn_neighbors=20 appears to work well in general.\n""""""\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import LocalOutlierFactor\n\nprint(__doc__)\n\nnp.random.seed(42)\n\nxx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n# Generate normal (not abnormal) training observations\nX = 0.3 * np.random.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n# Generate new normal (not abnormal) observations\nX = 0.3 * np.random.randn(20, 2)\nX_test = np.r_[X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n\n# fit the model for novelty detection (novelty=True)\nclf = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1)\nclf.fit(X_train)\n# DO NOT use predict, decision_function and score_samples on X_train as this\n# would give wrong results but only on new unseen data (not used in X_train),\n# e.g. X_test, X_outliers or the meshgrid\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\n# plot the learned frontier, the points, and the nearest vectors to the plane\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title(""Novelty Detection with LOF"")\nplt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\na = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\'darkred\')\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors=\'palevioletred\')\n\ns = 40\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\'white\', s=s, edgecolors=\'k\')\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\'blueviolet\', s=s,\n                 edgecolors=\'k\')\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\'gold\', s=s,\n                edgecolors=\'k\')\nplt.axis(\'tight\')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend([a.collections[0], b1, b2, c],\n           [""learned frontier"", ""training observations"",\n            ""new regular observations"", ""new abnormal observations""],\n           loc=""upper left"",\n           prop=matplotlib.font_manager.FontProperties(size=11))\nplt.xlabel(\n    ""errors novel regular: %d/40 ; errors novel abnormal: %d/40""\n    % (n_error_test, n_error_outliers))\nplt.show()\n'"
scikit-learn/_downloads/4f8c1325781c64a78f771150a3a765c0/plot_kmeans_assumptions.py,2,"b'""""""\n====================================\nDemonstration of k-means assumptions\n====================================\n\nThis example is meant to illustrate situations where k-means will produce\nunintuitive and possibly unexpected clusters. In the first three plots, the\ninput data does not conform to some implicit assumption that k-means makes and\nundesirable clusters are produced as a result. In the last plot, k-means\nreturns intuitive clusters despite unevenly sized blobs.\n""""""\nprint(__doc__)\n\n# Author: Phil Roth <mr.phil.roth@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nplt.figure(figsize=(12, 12))\n\nn_samples = 1500\nrandom_state = 170\nX, y = make_blobs(n_samples=n_samples, random_state=random_state)\n\n# Incorrect number of clusters\ny_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n\nplt.subplot(221)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.title(""Incorrect Number of Blobs"")\n\n# Anisotropicly distributed data\ntransformation = [[0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\nX_aniso = np.dot(X, transformation)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n\nplt.subplot(222)\nplt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\nplt.title(""Anisotropicly Distributed Blobs"")\n\n# Different variance\nX_varied, y_varied = make_blobs(n_samples=n_samples,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=random_state)\ny_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_varied)\n\nplt.subplot(223)\nplt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\nplt.title(""Unequal Variance"")\n\n# Unevenly sized blobs\nX_filtered = np.vstack((X[y == 0][:500], X[y == 1][:100], X[y == 2][:10]))\ny_pred = KMeans(n_clusters=3,\n                random_state=random_state).fit_predict(X_filtered)\n\nplt.subplot(224)\nplt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred)\nplt.title(""Unevenly Sized Blobs"")\n\nplt.show()\n'"
scikit-learn/_downloads/4fa3b0b897422a444d8b044e4dfecfaf/plot_svm_scale_c.py,5,"b'r""""""\n==============================================\nScaling the regularization parameter for SVCs\n==============================================\n\nThe following example illustrates the effect of scaling the\nregularization parameter when using :ref:`svm` for\n:ref:`classification <svm_classification>`.\nFor SVC classification, we are interested in a risk minimization for the\nequation:\n\n\n.. math::\n\n    C \\sum_{i=1, n} \\mathcal{L} (f(x_i), y_i) + \\Omega (w)\n\nwhere\n\n    - :math:`C` is used to set the amount of regularization\n    - :math:`\\mathcal{L}` is a `loss` function of our samples\n      and our model parameters.\n    - :math:`\\Omega` is a `penalty` function of our model parameters\n\nIf we consider the loss function to be the individual error per\nsample, then the data-fit term, or the sum of the error for each sample, will\nincrease as we add more samples. The penalization term, however, will not\nincrease.\n\nWhen using, for example, :ref:`cross validation <cross_validation>`, to\nset the amount of regularization with `C`, there will be a\ndifferent amount of samples between the main problem and the smaller problems\nwithin the folds of the cross validation.\n\nSince our loss function is dependent on the amount of samples, the latter\nwill influence the selected value of `C`.\nThe question that arises is `How do we optimally adjust C to\naccount for the different amount of training samples?`\n\nThe figures below are used to illustrate the effect of scaling our\n`C` to compensate for the change in the number of samples, in the\ncase of using an `l1` penalty, as well as the `l2` penalty.\n\nl1-penalty case\n-----------------\nIn the `l1` case, theory says that prediction consistency\n(i.e. that under given hypothesis, the estimator\nlearned predicts as well as a model knowing the true distribution)\nis not possible because of the bias of the `l1`. It does say, however,\nthat model consistency, in terms of finding the right set of non-zero\nparameters as well as their signs, can be achieved by scaling\n`C1`.\n\nl2-penalty case\n-----------------\nThe theory says that in order to achieve prediction consistency, the\npenalty parameter should be kept constant\nas the number of samples grow.\n\nSimulations\n------------\n\nThe two figures below plot the values of `C` on the `x-axis` and the\ncorresponding cross-validation scores on the `y-axis`, for several different\nfractions of a generated data-set.\n\nIn the `l1` penalty case, the cross-validation-error correlates best with\nthe test-error, when scaling our `C` with the number of samples, `n`,\nwhich can be seen in the first figure.\n\nFor the `l2` penalty case, the best result comes from the case where `C`\nis not scaled.\n\n.. topic:: Note:\n\n    Two separate datasets are used for the two different plots. The reason\n    behind this is the `l1` case works better on sparse data, while `l2`\n    is better suited to the non-sparse case.\n""""""\nprint(__doc__)\n\n\n# Author: Andreas Mueller <amueller@ais.uni-bonn.de>\n#         Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.utils import check_random_state\nfrom sklearn import datasets\n\nrnd = check_random_state(1)\n\n# set up dataset\nn_samples = 100\nn_features = 300\n\n# l1 data (only 5 informative features)\nX_1, y_1 = datasets.make_classification(n_samples=n_samples,\n                                        n_features=n_features, n_informative=5,\n                                        random_state=1)\n\n# l2 data: non sparse, but less features\ny_2 = np.sign(.5 - rnd.rand(n_samples))\nX_2 = rnd.randn(n_samples, n_features // 5) + y_2[:, np.newaxis]\nX_2 += 5 * rnd.randn(n_samples, n_features // 5)\n\nclf_sets = [(LinearSVC(penalty=\'l1\', loss=\'squared_hinge\', dual=False,\n                       tol=1e-3),\n             np.logspace(-2.3, -1.3, 10), X_1, y_1),\n            (LinearSVC(penalty=\'l2\', loss=\'squared_hinge\', dual=True),\n             np.logspace(-4.5, -2, 10), X_2, y_2)]\n\ncolors = [\'navy\', \'cyan\', \'darkorange\']\nlw = 2\n\nfor clf, cs, X, y in clf_sets:\n    # set up the plot for each regressor\n    fig, axes = plt.subplots(nrows=2, sharey=True, figsize=(9, 10))\n\n    for k, train_size in enumerate(np.linspace(0.3, 0.7, 3)[::-1]):\n        param_grid = dict(C=cs)\n        # To get nice curve, we need a large number of iterations to\n        # reduce the variance\n        grid = GridSearchCV(clf, refit=False, param_grid=param_grid,\n                            cv=ShuffleSplit(train_size=train_size,\n                                            test_size=.3,\n                                            n_splits=250, random_state=1))\n        grid.fit(X, y)\n        scores = grid.cv_results_[\'mean_test_score\']\n\n        scales = [(1, \'No scaling\'),\n                  ((n_samples * train_size), \'1/n_samples\'),\n                  ]\n\n        for ax, (scaler, name) in zip(axes, scales):\n            ax.set_xlabel(\'C\')\n            ax.set_ylabel(\'CV Score\')\n            grid_cs = cs * float(scaler)  # scale the C\'s\n            ax.semilogx(grid_cs, scores, label=""fraction %.2f"" %\n                        train_size, color=colors[k], lw=lw)\n            ax.set_title(\'scaling=%s, penalty=%s, loss=%s\' %\n                         (name, clf.penalty, clf.loss))\n\n    plt.legend(loc=""best"")\nplt.show()\n'"
scikit-learn/_downloads/4fef451c686c00e6d2807c1e224dac88/plot_precision_recall.py,4,"b'""""""\n================\nPrecision-Recall\n================\n\nExample of Precision-Recall metric to evaluate classifier output quality.\n\nPrecision-Recall is a useful measure of success of prediction when the\nclasses are very imbalanced. In information retrieval, precision is a\nmeasure of result relevancy, while recall is a measure of how many truly\nrelevant results are returned.\n\nThe precision-recall curve shows the tradeoff between precision and\nrecall for different threshold. A high area under the curve represents\nboth high recall and high precision, where high precision relates to a\nlow false positive rate, and high recall relates to a low false negative\nrate. High scores for both show that the classifier is returning accurate\nresults (high precision), as well as returning a majority of all positive\nresults (high recall).\n\nA system with high recall but low precision returns many results, but most of\nits predicted labels are incorrect when compared to the training labels. A\nsystem with high precision but low recall is just the opposite, returning very\nfew results, but most of its predicted labels are correct when compared to the\ntraining labels. An ideal system with high precision and high recall will\nreturn many results, with all results labeled correctly.\n\nPrecision (:math:`P`) is defined as the number of true positives (:math:`T_p`)\nover the number of true positives plus the number of false positives\n(:math:`F_p`).\n\n:math:`P = \\\\frac{T_p}{T_p+F_p}`\n\nRecall (:math:`R`) is defined as the number of true positives (:math:`T_p`)\nover the number of true positives plus the number of false negatives\n(:math:`F_n`).\n\n:math:`R = \\\\frac{T_p}{T_p + F_n}`\n\nThese quantities are also related to the (:math:`F_1`) score, which is defined\nas the harmonic mean of precision and recall.\n\n:math:`F1 = 2\\\\frac{P \\\\times R}{P+R}`\n\nNote that the precision may not decrease with recall. The\ndefinition of precision (:math:`\\\\frac{T_p}{T_p + F_p}`) shows that lowering\nthe threshold of a classifier may increase the denominator, by increasing the\nnumber of results returned. If the threshold was previously set too high, the\nnew results may all be true positives, which will increase precision. If the\nprevious threshold was about right or too low, further lowering the threshold\nwill introduce false positives, decreasing precision.\n\nRecall is defined as :math:`\\\\frac{T_p}{T_p+F_n}`, where :math:`T_p+F_n` does\nnot depend on the classifier threshold. This means that lowering the classifier\nthreshold may increase recall, by increasing the number of true positive\nresults. It is also possible that lowering the threshold may leave recall\nunchanged, while the precision fluctuates.\n\nThe relationship between recall and precision can be observed in the\nstairstep area of the plot - at the edges of these steps a small change\nin the threshold considerably reduces precision, with only a minor gain in\nrecall.\n\n**Average precision** (AP) summarizes such a plot as the weighted mean of\nprecisions achieved at each threshold, with the increase in recall from the\nprevious threshold used as the weight:\n\n:math:`\\\\text{AP} = \\\\sum_n (R_n - R_{n-1}) P_n`\n\nwhere :math:`P_n` and :math:`R_n` are the precision and recall at the\nnth threshold. A pair :math:`(R_k, P_k)` is referred to as an\n*operating point*.\n\nAP and the trapezoidal area under the operating points\n(:func:`sklearn.metrics.auc`) are common ways to summarize a precision-recall\ncurve that lead to different results. Read more in the\n:ref:`User Guide <precision_recall_f_measure_metrics>`.\n\nPrecision-recall curves are typically used in binary classification to study\nthe output of a classifier. In order to extend the precision-recall curve and\naverage precision to multi-class or multi-label classification, it is necessary\nto binarize the output. One curve can be drawn per label, but one can also draw\na precision-recall curve by considering each element of the label indicator\nmatrix as a binary prediction (micro-averaging).\n\n.. note::\n\n    See also :func:`sklearn.metrics.average_precision_score`,\n             :func:`sklearn.metrics.recall_score`,\n             :func:`sklearn.metrics.precision_score`,\n             :func:`sklearn.metrics.f1_score`\n""""""\n###############################################################################\n# In binary classification settings\n# --------------------------------------------------------\n#\n# Create simple data\n# ..................\n#\n# Try to differentiate the two first classes of the iris data\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# Limit to the two first classes, and split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n                                                    test_size=.5,\n                                                    random_state=random_state)\n\n# Create a simple classifier\nclassifier = svm.LinearSVC(random_state=random_state)\nclassifier.fit(X_train, y_train)\ny_score = classifier.decision_function(X_test)\n\n###############################################################################\n# Compute the average precision score\n# ...................................\nfrom sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score)\n\nprint(\'Average precision-recall score: {0:0.2f}\'.format(\n      average_precision))\n\n###############################################################################\n# Plot the Precision-Recall curve\n# ................................\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(classifier, X_test, y_test)\ndisp.ax_.set_title(\'2-class Precision-Recall curve: \'\n                   \'AP={0:0.2f}\'.format(average_precision))\n\n###############################################################################\n# In multi-label settings\n# ------------------------\n#\n# Create multi-label data, fit, and predict\n# ...........................................\n#\n# We create a multi-label dataset, to illustrate the precision-recall in\n# multi-label settings\n\nfrom sklearn.preprocessing import label_binarize\n\n# Use label_binarize to be multi-label like settings\nY = label_binarize(y, classes=[0, 1, 2])\nn_classes = Y.shape[1]\n\n# Split into training and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,\n                                                    random_state=random_state)\n\n# We use OneVsRestClassifier for multi-label prediction\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Run classifier\nclassifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))\nclassifier.fit(X_train, Y_train)\ny_score = classifier.decision_function(X_test)\n\n\n###############################################################################\n# The average precision score in multi-label settings\n# ....................................................\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n                                                        y_score[:, i])\n    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n\n# A ""micro-average"": quantifying score on all classes jointly\nprecision[""micro""], recall[""micro""], _ = precision_recall_curve(Y_test.ravel(),\n    y_score.ravel())\naverage_precision[""micro""] = average_precision_score(Y_test, y_score,\n                                                     average=""micro"")\nprint(\'Average precision score, micro-averaged over all classes: {0:0.2f}\'\n      .format(average_precision[""micro""]))\n\n###############################################################################\n# Plot the micro-averaged Precision-Recall curve\n# ...............................................\n#\n\nplt.figure()\nplt.step(recall[\'micro\'], precision[\'micro\'], where=\'post\')\n\nplt.xlabel(\'Recall\')\nplt.ylabel(\'Precision\')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(\n    \'Average precision score, micro-averaged over all classes: AP={0:0.2f}\'\n    .format(average_precision[""micro""]))\n\n###############################################################################\n# Plot Precision-Recall curve for each class and iso-f1 curves\n# .............................................................\n#\nfrom itertools import cycle\n# setup plot details\ncolors = cycle([\'navy\', \'turquoise\', \'darkorange\', \'cornflowerblue\', \'teal\'])\n\nplt.figure(figsize=(7, 8))\nf_scores = np.linspace(0.2, 0.8, num=4)\nlines = []\nlabels = []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x / (2 * x - f_score)\n    l, = plt.plot(x[y >= 0], y[y >= 0], color=\'gray\', alpha=0.2)\n    plt.annotate(\'f1={0:0.1f}\'.format(f_score), xy=(0.9, y[45] + 0.02))\n\nlines.append(l)\nlabels.append(\'iso-f1 curves\')\nl, = plt.plot(recall[""micro""], precision[""micro""], color=\'gold\', lw=2)\nlines.append(l)\nlabels.append(\'micro-average Precision-recall (area = {0:0.2f})\'\n              \'\'.format(average_precision[""micro""]))\n\nfor i, color in zip(range(n_classes), colors):\n    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n    lines.append(l)\n    labels.append(\'Precision-recall for class {0} (area = {1:0.2f})\'\n                  \'\'.format(i, average_precision[i]))\n\nfig = plt.gcf()\nfig.subplots_adjust(bottom=0.25)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\'Recall\')\nplt.ylabel(\'Precision\')\nplt.title(\'Extension of Precision-Recall curve to multi-class\')\nplt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n\n\nplt.show()\n'"
scikit-learn/_downloads/503b6915c8072f51f4d438ea5c77ea1f/plot_species_distribution_modeling.py,13,"b'""""""\n=============================\nSpecies distribution modeling\n=============================\n\nModeling species\' geographic distributions is an important\nproblem in conservation biology. In this example we\nmodel the geographic distribution of two south american\nmammals given past observations and 14 environmental\nvariables. Since we have only positive examples (there are\nno unsuccessful observations), we cast this problem as a\ndensity estimation problem and use the :class:`sklearn.svm.OneClassSVM`\nas our modeling tool. The dataset is provided by Phillips et. al. (2006).\nIf available, the example uses\n`basemap <https://matplotlib.org/basemap/>`_\nto plot the coast lines and national boundaries of South America.\n\nThe two species are:\n\n - `""Bradypus variegatus""\n   <http://www.iucnredlist.org/details/3038/0>`_ ,\n   the Brown-throated Sloth.\n\n - `""Microryzomys minutus""\n   <http://www.iucnredlist.org/details/13408/0>`_ ,\n   also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n   Colombia, Ecuador, Peru, and Venezuela.\n\nReferences\n----------\n\n * `""Maximum entropy modeling of species geographic distributions""\n   <http://rob.schapire.net/papers/ecolmod.pdf>`_\n   S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n   190:231-259, 2006.\n""""""\n\n# Authors: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#          Jake Vanderplas <vanderplas@astro.washington.edu>\n#\n# License: BSD 3 clause\n\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.utils import Bunch\nfrom sklearn.datasets import fetch_species_distributions\nfrom sklearn import svm, metrics\n\n# if basemap is available, we\'ll use it.\n# otherwise, we\'ll improvise later...\ntry:\n    from mpl_toolkits.basemap import Basemap\n    basemap = True\nexcept ImportError:\n    basemap = False\n\nprint(__doc__)\n\n\ndef construct_grids(batch):\n    """"""Construct the map grid from the batch object\n\n    Parameters\n    ----------\n    batch : Batch object\n        The object returned by :func:`fetch_species_distributions`\n\n    Returns\n    -------\n    (xgrid, ygrid) : 1-D arrays\n        The grid corresponding to the values in batch.coverages\n    """"""\n    # x,y coordinates for corner cells\n    xmin = batch.x_left_lower_corner + batch.grid_size\n    xmax = xmin + (batch.Nx * batch.grid_size)\n    ymin = batch.y_left_lower_corner + batch.grid_size\n    ymax = ymin + (batch.Ny * batch.grid_size)\n\n    # x coordinates of the grid cells\n    xgrid = np.arange(xmin, xmax, batch.grid_size)\n    # y coordinates of the grid cells\n    ygrid = np.arange(ymin, ymax, batch.grid_size)\n\n    return (xgrid, ygrid)\n\n\ndef create_species_bunch(species_name, train, test, coverages, xgrid, ygrid):\n    """"""Create a bunch with information about a particular organism\n\n    This will use the test/train record arrays to extract the\n    data specific to the given species name.\n    """"""\n    bunch = Bunch(name=\' \'.join(species_name.split(""_"")[:2]))\n    species_name = species_name.encode(\'ascii\')\n    points = dict(test=test, train=train)\n\n    for label, pts in points.items():\n        # choose points associated with the desired species\n        pts = pts[pts[\'species\'] == species_name]\n        bunch[\'pts_%s\' % label] = pts\n\n        # determine coverage values for each of the training & testing points\n        ix = np.searchsorted(xgrid, pts[\'dd long\'])\n        iy = np.searchsorted(ygrid, pts[\'dd lat\'])\n        bunch[\'cov_%s\' % label] = coverages[:, -iy, ix].T\n\n    return bunch\n\n\ndef plot_species_distribution(species=(""bradypus_variegatus_0"",\n                                       ""microryzomys_minutus_0"")):\n    """"""\n    Plot the species distribution.\n    """"""\n    if len(species) > 2:\n        print(""Note: when more than two species are provided,""\n              "" only the first two will be used"")\n\n    t0 = time()\n\n    # Load the compressed data\n    data = fetch_species_distributions()\n\n    # Set up the data grid\n    xgrid, ygrid = construct_grids(data)\n\n    # The grid in x,y coordinates\n    X, Y = np.meshgrid(xgrid, ygrid[::-1])\n\n    # create a bunch for each species\n    BV_bunch = create_species_bunch(species[0],\n                                    data.train, data.test,\n                                    data.coverages, xgrid, ygrid)\n    MM_bunch = create_species_bunch(species[1],\n                                    data.train, data.test,\n                                    data.coverages, xgrid, ygrid)\n\n    # background points (grid coordinates) for evaluation\n    np.random.seed(13)\n    background_points = np.c_[np.random.randint(low=0, high=data.Ny,\n                                                size=10000),\n                              np.random.randint(low=0, high=data.Nx,\n                                                size=10000)].T\n\n    # We\'ll make use of the fact that coverages[6] has measurements at all\n    # land points.  This will help us decide between land and water.\n    land_reference = data.coverages[6]\n\n    # Fit, predict, and plot for each species.\n    for i, species in enumerate([BV_bunch, MM_bunch]):\n        print(""_"" * 80)\n        print(""Modeling distribution of species \'%s\'"" % species.name)\n\n        # Standardize features\n        mean = species.cov_train.mean(axis=0)\n        std = species.cov_train.std(axis=0)\n        train_cover_std = (species.cov_train - mean) / std\n\n        # Fit OneClassSVM\n        print("" - fit OneClassSVM ... "", end=\'\')\n        clf = svm.OneClassSVM(nu=0.1, kernel=""rbf"", gamma=0.5)\n        clf.fit(train_cover_std)\n        print(""done."")\n\n        # Plot map of South America\n        plt.subplot(1, 2, i + 1)\n        if basemap:\n            print("" - plot coastlines using basemap"")\n            m = Basemap(projection=\'cyl\', llcrnrlat=Y.min(),\n                        urcrnrlat=Y.max(), llcrnrlon=X.min(),\n                        urcrnrlon=X.max(), resolution=\'c\')\n            m.drawcoastlines()\n            m.drawcountries()\n        else:\n            print("" - plot coastlines from coverage"")\n            plt.contour(X, Y, land_reference,\n                        levels=[-9998], colors=""k"",\n                        linestyles=""solid"")\n            plt.xticks([])\n            plt.yticks([])\n\n        print("" - predict species distribution"")\n\n        # Predict species distribution using the training data\n        Z = np.ones((data.Ny, data.Nx), dtype=np.float64)\n\n        # We\'ll predict only for the land points.\n        idx = np.where(land_reference > -9999)\n        coverages_land = data.coverages[:, idx[0], idx[1]].T\n\n        pred = clf.decision_function((coverages_land - mean) / std)\n        Z *= pred.min()\n        Z[idx[0], idx[1]] = pred\n\n        levels = np.linspace(Z.min(), Z.max(), 25)\n        Z[land_reference == -9999] = -9999\n\n        # plot contours of the prediction\n        plt.contourf(X, Y, Z, levels=levels, cmap=plt.cm.Reds)\n        plt.colorbar(format=\'%.2f\')\n\n        # scatter training/testing points\n        plt.scatter(species.pts_train[\'dd long\'], species.pts_train[\'dd lat\'],\n                    s=2 ** 2, c=\'black\',\n                    marker=\'^\', label=\'train\')\n        plt.scatter(species.pts_test[\'dd long\'], species.pts_test[\'dd lat\'],\n                    s=2 ** 2, c=\'black\',\n                    marker=\'x\', label=\'test\')\n        plt.legend()\n        plt.title(species.name)\n        plt.axis(\'equal\')\n\n        # Compute AUC with regards to background points\n        pred_background = Z[background_points[0], background_points[1]]\n        pred_test = clf.decision_function((species.cov_test - mean) / std)\n        scores = np.r_[pred_test, pred_background]\n        y = np.r_[np.ones(pred_test.shape), np.zeros(pred_background.shape)]\n        fpr, tpr, thresholds = metrics.roc_curve(y, scores)\n        roc_auc = metrics.auc(fpr, tpr)\n        plt.text(-35, -70, ""AUC: %.3f"" % roc_auc, ha=""right"")\n        print(""\\n Area under the ROC curve : %f"" % roc_auc)\n\n    print(""\\ntime elapsed: %.2fs"" % (time() - t0))\n\n\nplot_species_distribution()\nplt.show()\n'"
scikit-learn/_downloads/50bff762380cd495e6ca1b4af4a4eebc/plot_adaboost_hastie_10_2.py,8,"b'""""""\n=============================\nDiscrete versus Real AdaBoost\n=============================\n\nThis example is based on Figure 10.2 from Hastie et al 2009 [1]_ and\nillustrates the difference in performance between the discrete SAMME [2]_\nboosting algorithm and real SAMME.R boosting algorithm. Both algorithms are\nevaluated on a binary classification task where the target Y is a non-linear\nfunction of 10 input features.\n\nDiscrete SAMME AdaBoost adapts based on errors in predicted class labels\nwhereas real SAMME.R uses the predicted class probabilities.\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, ""Elements of Statistical\n    Learning Ed. 2"", Springer, 2009.\n\n.. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, ""Multi-class AdaBoost"", 2009.\n\n""""""\nprint(__doc__)\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,\n#         Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import zero_one_loss\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\nn_estimators = 400\n# A learning rate of 1. may not be optimal for both SAMME and SAMME.R\nlearning_rate = 1.\n\nX, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n\nX_test, y_test = X[2000:], y[2000:]\nX_train, y_train = X[:2000], y[:2000]\n\ndt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\ndt_stump.fit(X_train, y_train)\ndt_stump_err = 1.0 - dt_stump.score(X_test, y_test)\n\ndt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\ndt.fit(X_train, y_train)\ndt_err = 1.0 - dt.score(X_test, y_test)\n\nada_discrete = AdaBoostClassifier(\n    base_estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=""SAMME"")\nada_discrete.fit(X_train, y_train)\n\nada_real = AdaBoostClassifier(\n    base_estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=""SAMME.R"")\nada_real.fit(X_train, y_train)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nax.plot([1, n_estimators], [dt_stump_err] * 2, \'k-\',\n        label=\'Decision Stump Error\')\nax.plot([1, n_estimators], [dt_err] * 2, \'k--\',\n        label=\'Decision Tree Error\')\n\nada_discrete_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n    ada_discrete_err[i] = zero_one_loss(y_pred, y_test)\n\nada_discrete_err_train = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n    ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train)\n\nada_real_err = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n    ada_real_err[i] = zero_one_loss(y_pred, y_test)\n\nada_real_err_train = np.zeros((n_estimators,))\nfor i, y_pred in enumerate(ada_real.staged_predict(X_train)):\n    ada_real_err_train[i] = zero_one_loss(y_pred, y_train)\n\nax.plot(np.arange(n_estimators) + 1, ada_discrete_err,\n        label=\'Discrete AdaBoost Test Error\',\n        color=\'red\')\nax.plot(np.arange(n_estimators) + 1, ada_discrete_err_train,\n        label=\'Discrete AdaBoost Train Error\',\n        color=\'blue\')\nax.plot(np.arange(n_estimators) + 1, ada_real_err,\n        label=\'Real AdaBoost Test Error\',\n        color=\'orange\')\nax.plot(np.arange(n_estimators) + 1, ada_real_err_train,\n        label=\'Real AdaBoost Train Error\',\n        color=\'green\')\n\nax.set_ylim((0.0, 0.5))\nax.set_xlabel(\'n_estimators\')\nax.set_ylabel(\'error rate\')\n\nleg = ax.legend(loc=\'upper right\', fancybox=True)\nleg.get_frame().set_alpha(0.7)\n\nplt.show()\n'"
scikit-learn/_downloads/50d5483eef5cb5e563c5516b5912e0e8/plot_out_of_core_classification.py,7,"b'""""""\n======================================================\nOut-of-core classification of text documents\n======================================================\n\nThis is an example showing how scikit-learn can be used for classification\nusing an out-of-core approach: learning from data that doesn\'t fit into main\nmemory. We make use of an online classifier, i.e., one that supports the\npartial_fit method, that will be fed with batches of examples. To guarantee\nthat the features space remains the same over time we leverage a\nHashingVectorizer that will project each example into the same feature space.\nThis is especially useful in the case of text classification where new\nfeatures (words) may appear in each batch.\n""""""\n\n# Authors: Eustache Diemert <eustache@diemert.fr>\n#          @FedericoV <https://github.com/FedericoV/>\n# License: BSD 3 clause\n\nfrom glob import glob\nimport itertools\nimport os.path\nimport re\nimport tarfile\nimport time\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nfrom html.parser import HTMLParser\nfrom urllib.request import urlretrieve\nfrom sklearn.datasets import get_data_home\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.naive_bayes import MultinomialNB\n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return \'__file__\' in globals()\n\n###############################################################################\n# Reuters Dataset related routines\n# --------------------------------\n#\n# The dataset used in this example is Reuters-21578 as provided by the UCI ML\n# repository. It will be automatically downloaded and uncompressed on first\n# run.\n\n\n\nclass ReutersParser(HTMLParser):\n    """"""Utility class to parse a SGML file and yield documents one at a time.""""""\n\n    def __init__(self, encoding=\'latin-1\'):\n        HTMLParser.__init__(self)\n        self._reset()\n        self.encoding = encoding\n\n    def handle_starttag(self, tag, attrs):\n        method = \'start_\' + tag\n        getattr(self, method, lambda x: None)(attrs)\n\n    def handle_endtag(self, tag):\n        method = \'end_\' + tag\n        getattr(self, method, lambda: None)()\n\n    def _reset(self):\n        self.in_title = 0\n        self.in_body = 0\n        self.in_topics = 0\n        self.in_topic_d = 0\n        self.title = """"\n        self.body = """"\n        self.topics = []\n        self.topic_d = """"\n\n    def parse(self, fd):\n        self.docs = []\n        for chunk in fd:\n            self.feed(chunk.decode(self.encoding))\n            for doc in self.docs:\n                yield doc\n            self.docs = []\n        self.close()\n\n    def handle_data(self, data):\n        if self.in_body:\n            self.body += data\n        elif self.in_title:\n            self.title += data\n        elif self.in_topic_d:\n            self.topic_d += data\n\n    def start_reuters(self, attributes):\n        pass\n\n    def end_reuters(self):\n        self.body = re.sub(r\'\\s+\', r\' \', self.body)\n        self.docs.append({\'title\': self.title,\n                          \'body\': self.body,\n                          \'topics\': self.topics})\n        self._reset()\n\n    def start_title(self, attributes):\n        self.in_title = 1\n\n    def end_title(self):\n        self.in_title = 0\n\n    def start_body(self, attributes):\n        self.in_body = 1\n\n    def end_body(self):\n        self.in_body = 0\n\n    def start_topics(self, attributes):\n        self.in_topics = 1\n\n    def end_topics(self):\n        self.in_topics = 0\n\n    def start_d(self, attributes):\n        self.in_topic_d = 1\n\n    def end_d(self):\n        self.in_topic_d = 0\n        self.topics.append(self.topic_d)\n        self.topic_d = """"\n\n\ndef stream_reuters_documents(data_path=None):\n    """"""Iterate over documents of the Reuters dataset.\n\n    The Reuters archive will automatically be downloaded and uncompressed if\n    the `data_path` directory does not exist.\n\n    Documents are represented as dictionaries with \'body\' (str),\n    \'title\' (str), \'topics\' (list(str)) keys.\n\n    """"""\n\n    DOWNLOAD_URL = (\'http://archive.ics.uci.edu/ml/machine-learning-databases/\'\n                    \'reuters21578-mld/reuters21578.tar.gz\')\n    ARCHIVE_FILENAME = \'reuters21578.tar.gz\'\n\n    if data_path is None:\n        data_path = os.path.join(get_data_home(), ""reuters"")\n    if not os.path.exists(data_path):\n        """"""Download the dataset.""""""\n        print(""downloading dataset (once and for all) into %s"" %\n              data_path)\n        os.mkdir(data_path)\n\n        def progress(blocknum, bs, size):\n            total_sz_mb = \'%.2f MB\' % (size / 1e6)\n            current_sz_mb = \'%.2f MB\' % ((blocknum * bs) / 1e6)\n            if _not_in_sphinx():\n                sys.stdout.write(\n                    \'\\rdownloaded %s / %s\' % (current_sz_mb, total_sz_mb))\n\n        archive_path = os.path.join(data_path, ARCHIVE_FILENAME)\n        urlretrieve(DOWNLOAD_URL, filename=archive_path,\n                    reporthook=progress)\n        if _not_in_sphinx():\n            sys.stdout.write(\'\\r\')\n        print(""untarring Reuters dataset..."")\n        tarfile.open(archive_path, \'r:gz\').extractall(data_path)\n        print(""done."")\n\n    parser = ReutersParser()\n    for filename in glob(os.path.join(data_path, ""*.sgm"")):\n        for doc in parser.parse(open(filename, \'rb\')):\n            yield doc\n\n\n###############################################################################\n# Main\n# ----\n#\n# Create the vectorizer and limit the number of features to a reasonable\n# maximum\n\nvectorizer = HashingVectorizer(decode_error=\'ignore\', n_features=2 ** 18,\n                               alternate_sign=False)\n\n\n# Iterator over parsed Reuters SGML files.\ndata_stream = stream_reuters_documents()\n\n# We learn a binary classification between the ""acq"" class and all the others.\n# ""acq"" was chosen as it is more or less evenly distributed in the Reuters\n# files. For other datasets, one should take care of creating a test set with\n# a realistic portion of positive instances.\nall_classes = np.array([0, 1])\npositive_class = \'acq\'\n\n# Here are some classifiers that support the `partial_fit` method\npartial_fit_classifiers = {\n    \'SGD\': SGDClassifier(max_iter=5),\n    \'Perceptron\': Perceptron(),\n    \'NB Multinomial\': MultinomialNB(alpha=0.01),\n    \'Passive-Aggressive\': PassiveAggressiveClassifier(),\n}\n\n\ndef get_minibatch(doc_iter, size, pos_class=positive_class):\n    """"""Extract a minibatch of examples, return a tuple X_text, y.\n\n    Note: size is before excluding invalid docs with no topics assigned.\n\n    """"""\n    data = [(\'{title}\\n\\n{body}\'.format(**doc), pos_class in doc[\'topics\'])\n            for doc in itertools.islice(doc_iter, size)\n            if doc[\'topics\']]\n    if not len(data):\n        return np.asarray([], dtype=int), np.asarray([], dtype=int)\n    X_text, y = zip(*data)\n    return X_text, np.asarray(y, dtype=int)\n\n\ndef iter_minibatches(doc_iter, minibatch_size):\n    """"""Generator of minibatches.""""""\n    X_text, y = get_minibatch(doc_iter, minibatch_size)\n    while len(X_text):\n        yield X_text, y\n        X_text, y = get_minibatch(doc_iter, minibatch_size)\n\n\n# test data statistics\ntest_stats = {\'n_test\': 0, \'n_test_pos\': 0}\n\n# First we hold out a number of examples to estimate accuracy\nn_test_documents = 1000\ntick = time.time()\nX_test_text, y_test = get_minibatch(data_stream, 1000)\nparsing_time = time.time() - tick\ntick = time.time()\nX_test = vectorizer.transform(X_test_text)\nvectorizing_time = time.time() - tick\ntest_stats[\'n_test\'] += len(y_test)\ntest_stats[\'n_test_pos\'] += sum(y_test)\nprint(""Test set is %d documents (%d positive)"" % (len(y_test), sum(y_test)))\n\n\ndef progress(cls_name, stats):\n    """"""Report progress information, return a string.""""""\n    duration = time.time() - stats[\'t0\']\n    s = ""%20s classifier : \\t"" % cls_name\n    s += ""%(n_train)6d train docs (%(n_train_pos)6d positive) "" % stats\n    s += ""%(n_test)6d test docs (%(n_test_pos)6d positive) "" % test_stats\n    s += ""accuracy: %(accuracy).3f "" % stats\n    s += ""in %.2fs (%5d docs/s)"" % (duration, stats[\'n_train\'] / duration)\n    return s\n\n\ncls_stats = {}\n\nfor cls_name in partial_fit_classifiers:\n    stats = {\'n_train\': 0, \'n_train_pos\': 0,\n             \'accuracy\': 0.0, \'accuracy_history\': [(0, 0)], \'t0\': time.time(),\n             \'runtime_history\': [(0, 0)], \'total_fit_time\': 0.0}\n    cls_stats[cls_name] = stats\n\nget_minibatch(data_stream, n_test_documents)\n# Discard test set\n\n# We will feed the classifier with mini-batches of 1000 documents; this means\n# we have at most 1000 docs in memory at any time.  The smaller the document\n# batch, the bigger the relative overhead of the partial fit methods.\nminibatch_size = 1000\n\n# Create the data_stream that parses Reuters SGML files and iterates on\n# documents as a stream.\nminibatch_iterators = iter_minibatches(data_stream, minibatch_size)\ntotal_vect_time = 0.0\n\n# Main loop : iterate on mini-batches of examples\nfor i, (X_train_text, y_train) in enumerate(minibatch_iterators):\n\n    tick = time.time()\n    X_train = vectorizer.transform(X_train_text)\n    total_vect_time += time.time() - tick\n\n    for cls_name, cls in partial_fit_classifiers.items():\n        tick = time.time()\n        # update estimator with examples in the current mini-batch\n        cls.partial_fit(X_train, y_train, classes=all_classes)\n\n        # accumulate test accuracy stats\n        cls_stats[cls_name][\'total_fit_time\'] += time.time() - tick\n        cls_stats[cls_name][\'n_train\'] += X_train.shape[0]\n        cls_stats[cls_name][\'n_train_pos\'] += sum(y_train)\n        tick = time.time()\n        cls_stats[cls_name][\'accuracy\'] = cls.score(X_test, y_test)\n        cls_stats[cls_name][\'prediction_time\'] = time.time() - tick\n        acc_history = (cls_stats[cls_name][\'accuracy\'],\n                       cls_stats[cls_name][\'n_train\'])\n        cls_stats[cls_name][\'accuracy_history\'].append(acc_history)\n        run_history = (cls_stats[cls_name][\'accuracy\'],\n                       total_vect_time + cls_stats[cls_name][\'total_fit_time\'])\n        cls_stats[cls_name][\'runtime_history\'].append(run_history)\n\n        if i % 3 == 0:\n            print(progress(cls_name, cls_stats[cls_name]))\n    if i % 3 == 0:\n        print(\'\\n\')\n\n\n###############################################################################\n# Plot results\n# ------------\n#\n# The plot represents the learning curve of the classifier: the evolution\n# of classification accuracy over the course of the mini-batches. Accuracy is\n# measured on the first 1000 samples, held out as a validation set.\n#\n# To limit the memory consumption, we queue examples up to a fixed amount\n# before feeding them to the learner.\n\n\ndef plot_accuracy(x, y, x_legend):\n    """"""Plot accuracy as a function of x.""""""\n    x = np.array(x)\n    y = np.array(y)\n    plt.title(\'Classification accuracy as a function of %s\' % x_legend)\n    plt.xlabel(\'%s\' % x_legend)\n    plt.ylabel(\'Accuracy\')\n    plt.grid(True)\n    plt.plot(x, y)\n\n\nrcParams[\'legend.fontsize\'] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\nplt.figure()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\'accuracy_history\'])\n    plot_accuracy(n_examples, accuracy, ""training examples (#)"")\n    ax = plt.gca()\n    ax.set_ylim((0.8, 1))\nplt.legend(cls_names, loc=\'best\')\n\nplt.figure()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\'runtime_history\'])\n    plot_accuracy(runtime, accuracy, \'runtime (s)\')\n    ax = plt.gca()\n    ax.set_ylim((0.8, 1))\nplt.legend(cls_names, loc=\'best\')\n\n# Plot fitting times\nplt.figure()\nfig = plt.gcf()\ncls_runtime = [stats[\'total_fit_time\']\n               for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\'Vectorization\')\nbar_colors = [\'b\', \'g\', \'r\', \'c\', \'m\', \'y\']\n\nax = plt.subplot(111)\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n                     color=bar_colors)\n\nax.set_xticks(np.linspace(0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\'runtime (s)\')\nax.set_title(\'Training Times\')\n\n\ndef autolabel(rectangles):\n    """"""attach some text vi autolabel on rectangles.""""""\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width() / 2.,\n                1.05 * height, \'%.4f\' % height,\n                ha=\'center\', va=\'bottom\')\n        plt.setp(plt.xticks()[1], rotation=30)\n\n\nautolabel(rectangles)\nplt.tight_layout()\nplt.show()\n\n# Plot prediction times\nplt.figure()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\'prediction_time\'])\ncls_runtime.append(parsing_time)\ncls_names.append(\'Read/Parse\\n+Feat.Extr.\')\ncls_runtime.append(vectorizing_time)\ncls_names.append(\'Hashing\\n+Vect.\')\n\nax = plt.subplot(111)\nrectangles = plt.bar(range(len(cls_names)), cls_runtime, width=0.5,\n                     color=bar_colors)\n\nax.set_xticks(np.linspace(0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\nplt.setp(plt.xticks()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\'runtime (s)\')\nax.set_title(\'Prediction Times (%d instances)\' % n_test_documents)\nautolabel(rectangles)\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/52b87fb2c1b749713af99e8117093278/plot_weighted_samples.py,6,"b'""""""\n=====================\nSVM: Weighted samples\n=====================\n\nPlot decision function of a weighted dataset, where the size of points\nis proportional to its weight.\n\nThe sample weighting rescales the C parameter, which means that the classifier\nputs more emphasis on getting these points right. The effect might often be\nsubtle.\nTo emphasize the effect here, we particularly weight outliers, making the\ndeformation of the decision boundary very visible.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\ndef plot_decision_function(classifier, sample_weight, axis, title):\n    # plot the decision function\n    xx, yy = np.meshgrid(np.linspace(-4, 5, 500), np.linspace(-4, 5, 500))\n\n    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    axis.contourf(xx, yy, Z, alpha=0.75, cmap=plt.cm.bone)\n    axis.scatter(X[:, 0], X[:, 1], c=y, s=100 * sample_weight, alpha=0.9,\n                 cmap=plt.cm.bone, edgecolors=\'black\')\n\n    axis.axis(\'off\')\n    axis.set_title(title)\n\n\n# we create 20 points\nnp.random.seed(0)\nX = np.r_[np.random.randn(10, 2) + [1, 1], np.random.randn(10, 2)]\ny = [1] * 10 + [-1] * 10\nsample_weight_last_ten = abs(np.random.randn(len(X)))\nsample_weight_constant = np.ones(len(X))\n# and bigger weights to some outliers\nsample_weight_last_ten[15:] *= 5\nsample_weight_last_ten[9] *= 15\n\n# for reference, first fit without sample weights\n\n# fit the model\nclf_weights = svm.SVC(gamma=1)\nclf_weights.fit(X, y, sample_weight=sample_weight_last_ten)\n\nclf_no_weights = svm.SVC(gamma=1)\nclf_no_weights.fit(X, y)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nplot_decision_function(clf_no_weights, sample_weight_constant, axes[0],\n                       ""Constant weights"")\nplot_decision_function(clf_weights, sample_weight_last_ten, axes[1],\n                       ""Modified weights"")\n\nplt.show()\n'"
scikit-learn/_downloads/5307f83836ef32fe2f33c74772c03541/plot_rfe_digits.py,0,"b'""""""\n=============================\nRecursive feature elimination\n=============================\n\nA recursive feature elimination example showing the relevance of pixels in\na digit classification task.\n\n.. note::\n\n    See also :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`\n\n""""""\nprint(__doc__)\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.feature_selection import RFE\nimport matplotlib.pyplot as plt\n\n# Load the digits dataset\ndigits = load_digits()\nX = digits.images.reshape((len(digits.images), -1))\ny = digits.target\n\n# Create the RFE object and rank each pixel\nsvc = SVC(kernel=""linear"", C=1)\nrfe = RFE(estimator=svc, n_features_to_select=1, step=1)\nrfe.fit(X, y)\nranking = rfe.ranking_.reshape(digits.images[0].shape)\n\n# Plot pixel ranking\nplt.matshow(ranking, cmap=plt.cm.Blues)\nplt.colorbar()\nplt.title(""Ranking of pixels with RFE"")\nplt.show()\n'"
scikit-learn/_downloads/538566ee0761b87278fccb47791b3284/plot_spectral_biclustering.py,5,"b'""""""\n=============================================\nA demo of the Spectral Biclustering algorithm\n=============================================\n\nThis example demonstrates how to generate a checkerboard dataset and\nbicluster it using the Spectral Biclustering algorithm.\n\nThe data is generated with the ``make_checkerboard`` function, then\nshuffled and passed to the Spectral Biclustering algorithm. The rows\nand columns of the shuffled matrix are rearranged to show the\nbiclusters found by the algorithm.\n\nThe outer product of the row and column label vectors shows a\nrepresentation of the checkerboard structure.\n\n""""""\nprint(__doc__)\n\n# Author: Kemal Eren <kemal@kemaleren.com>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_checkerboard\nfrom sklearn.cluster import SpectralBiclustering\nfrom sklearn.metrics import consensus_score\n\n\nn_clusters = (4, 3)\ndata, rows, columns = make_checkerboard(\n    shape=(300, 300), n_clusters=n_clusters, noise=10,\n    shuffle=False, random_state=0)\n\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title(""Original dataset"")\n\n# shuffle clusters\nrng = np.random.RandomState(0)\nrow_idx = rng.permutation(data.shape[0])\ncol_idx = rng.permutation(data.shape[1])\ndata = data[row_idx][:, col_idx]\n\nplt.matshow(data, cmap=plt.cm.Blues)\nplt.title(""Shuffled dataset"")\n\nmodel = SpectralBiclustering(n_clusters=n_clusters, method=\'log\',\n                             random_state=0)\nmodel.fit(data)\nscore = consensus_score(model.biclusters_,\n                        (rows[:, row_idx], columns[:, col_idx]))\n\nprint(""consensus score: {:.1f}"".format(score))\n\nfit_data = data[np.argsort(model.row_labels_)]\nfit_data = fit_data[:, np.argsort(model.column_labels_)]\n\nplt.matshow(fit_data, cmap=plt.cm.Blues)\nplt.title(""After biclustering; rearranged to show biclusters"")\n\nplt.matshow(np.outer(np.sort(model.row_labels_) + 1,\n                     np.sort(model.column_labels_) + 1),\n            cmap=plt.cm.Blues)\nplt.title(""Checkerboard structure of rearranged data"")\n\nplt.show()\n'"
scikit-learn/_downloads/55f9a88ed39b0e8292a10808fc8695f1/plot_confusion_matrix.py,1,"b'""""""\n================\nConfusion matrix\n================\n\nExample of confusion matrix usage to evaluate the quality\nof the output of a classifier on the iris data set. The\ndiagonal elements represent the number of points for which\nthe predicted label is equal to the true label, while\noff-diagonal elements are those that are mislabeled by the\nclassifier. The higher the diagonal values of the confusion\nmatrix the better, indicating many correct predictions.\n\nThe figures show the confusion matrix with and without\nnormalization by class support size (number of elements\nin each class). This kind of normalization can be\ninteresting in case of class imbalance to have a more\nvisual interpretation of which class is being misclassified.\n\nHere the results are not as good as they could be as our\nchoice for the regularization parameter C was not the best.\nIn real life applications this parameter is usually chosen\nusing :ref:`grid_search`.\n\n""""""\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nclass_names = iris.target_names\n\n# Split the data into a training set and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Run classifier, using a model that is too regularized (C too low) to see\n# the impact on the results\nclassifier = svm.SVC(kernel=\'linear\', C=0.01).fit(X_train, y_train)\n\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\ntitles_options = [(""Confusion matrix, without normalization"", None),\n                  (""Normalized confusion matrix"", \'true\')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(classifier, X_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()\n'"
scikit-learn/_downloads/56a1dcd2aa7cc2e2605218499f4d445b/plot_randomized_search.py,3,"b'""""""\n=========================================================================\nComparing randomized search and grid search for hyperparameter estimation\n=========================================================================\n\nCompare randomized search and grid search for optimizing hyperparameters of a\nrandom forest.\nAll parameters that influence the learning are searched simultaneously\n(except for the number of estimators, which poses a time / quality tradeoff).\n\nThe randomized search and the grid search explore exactly the same space of\nparameters. The result in parameter settings is quite similar, while the run\ntime for randomized search is drastically lower.\n\nThe performance is may slightly worse for the randomized search, and is likely\ndue to a noise effect and would not carry over to a held-out test set.\n\nNote that in practice, one would not search over this many different parameters\nsimultaneously using grid search, but pick only the ones deemed most important.\n""""""\nprint(__doc__)\n\nimport numpy as np\n\nfrom time import time\nimport scipy.stats as stats\nfrom sklearn.utils.fixes import loguniform\n\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.linear_model import SGDClassifier\n\n# get some data\nX, y = load_digits(return_X_y=True)\n\n# build a classifier\nclf = SGDClassifier(loss=\'hinge\', penalty=\'elasticnet\',\n                    fit_intercept=True)\n\n\n# Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results[\'rank_test_score\'] == i)\n        for candidate in candidates:\n            print(""Model with rank: {0}"".format(i))\n            print(""Mean validation score: {0:.3f} (std: {1:.3f})""\n                  .format(results[\'mean_test_score\'][candidate],\n                          results[\'std_test_score\'][candidate]))\n            print(""Parameters: {0}"".format(results[\'params\'][candidate]))\n            print("""")\n\n\n# specify parameters and distributions to sample from\nparam_dist = {\'average\': [True, False],\n              \'l1_ratio\': stats.uniform(0, 1),\n              \'alpha\': loguniform(1e-4, 1e0)}\n\n# run randomized search\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search)\n\nstart = time()\nrandom_search.fit(X, y)\nprint(""RandomizedSearchCV took %.2f seconds for %d candidates""\n      "" parameter settings."" % ((time() - start), n_iter_search))\nreport(random_search.cv_results_)\n\n# use a full grid over all parameters\nparam_grid = {\'average\': [True, False],\n              \'l1_ratio\': np.linspace(0, 1, num=10),\n              \'alpha\': np.power(10, np.arange(-4, 1, dtype=float))}\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid)\nstart = time()\ngrid_search.fit(X, y)\n\nprint(""GridSearchCV took %.2f seconds for %d candidate parameter settings.""\n      % (time() - start, len(grid_search.cv_results_[\'params\'])))\nreport(grid_search.cv_results_)\n'"
scikit-learn/_downloads/5848bcc44087bb59a68b408075ab5e88/plot_theilsen.py,10,"b'""""""\n====================\nTheil-Sen Regression\n====================\n\nComputes a Theil-Sen Regression on a synthetic dataset.\n\nSee :ref:`theil_sen_regression` for more information on the regressor.\n\nCompared to the OLS (ordinary least squares) estimator, the Theil-Sen\nestimator is robust against outliers. It has a breakdown point of about 29.3%\nin case of a simple linear regression which means that it can tolerate\narbitrary corrupted data (outliers) of up to 29.3% in the two-dimensional\ncase.\n\nThe estimation of the model is done by calculating the slopes and intercepts\nof a subpopulation of all possible combinations of p subsample points. If an\nintercept is fitted, p must be greater than or equal to n_features + 1. The\nfinal slope and intercept is then defined as the spatial median of these\nslopes and intercepts.\n\nIn certain cases Theil-Sen performs better than :ref:`RANSAC\n<ransac_regression>` which is also a robust method. This is illustrated in the\nsecond example below where outliers with respect to the x-axis perturb RANSAC.\nTuning the ``residual_threshold`` parameter of RANSAC remedies this but in\ngeneral a priori knowledge about the data and the nature of the outliers is\nneeded.\nDue to the computational complexity of Theil-Sen it is recommended to use it\nonly for small problems in terms of number of samples and features. For larger\nproblems the ``max_subpopulation`` parameter restricts the magnitude of all\npossible combinations of p subsample points to a randomly chosen subset and\ntherefore also limits the runtime. Therefore, Theil-Sen is applicable to larger\nproblems with the drawback of losing some of its mathematical properties since\nit then works on a random subset.\n""""""\n\n# Author: Florian Wilhelm -- <florian.wilhelm@gmail.com>\n# License: BSD 3 clause\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression, TheilSenRegressor\nfrom sklearn.linear_model import RANSACRegressor\n\nprint(__doc__)\n\nestimators = [(\'OLS\', LinearRegression()),\n              (\'Theil-Sen\', TheilSenRegressor(random_state=42)),\n              (\'RANSAC\', RANSACRegressor(random_state=42)), ]\ncolors = {\'OLS\': \'turquoise\', \'Theil-Sen\': \'gold\', \'RANSAC\': \'lightgreen\'}\nlw = 2\n\n# #############################################################################\n# Outliers only in the y direction\n\nnp.random.seed(0)\nn_samples = 200\n# Linear model y = 3*x + N(2, 0.1**2)\nx = np.random.randn(n_samples)\nw = 3.\nc = 2.\nnoise = 0.1 * np.random.randn(n_samples)\ny = w * x + c + noise\n# 10% outliers\ny[-20:] += -20 * x[-20:]\nX = x[:, np.newaxis]\n\nplt.scatter(x, y, color=\'indigo\', marker=\'x\', s=40)\nline_x = np.array([-3, 3])\nfor name, estimator in estimators:\n    t0 = time.time()\n    estimator.fit(X, y)\n    elapsed_time = time.time() - t0\n    y_pred = estimator.predict(line_x.reshape(2, 1))\n    plt.plot(line_x, y_pred, color=colors[name], linewidth=lw,\n             label=\'%s (fit time: %.2fs)\' % (name, elapsed_time))\n\nplt.axis(\'tight\')\nplt.legend(loc=\'upper left\')\nplt.title(""Corrupt y"")\n\n# #############################################################################\n# Outliers in the X direction\n\nnp.random.seed(0)\n# Linear model y = 3*x + N(2, 0.1**2)\nx = np.random.randn(n_samples)\nnoise = 0.1 * np.random.randn(n_samples)\ny = 3 * x + 2 + noise\n# 10% outliers\nx[-20:] = 9.9\ny[-20:] += 22\nX = x[:, np.newaxis]\n\nplt.figure()\nplt.scatter(x, y, color=\'indigo\', marker=\'x\', s=40)\n\nline_x = np.array([-3, 10])\nfor name, estimator in estimators:\n    t0 = time.time()\n    estimator.fit(X, y)\n    elapsed_time = time.time() - t0\n    y_pred = estimator.predict(line_x.reshape(2, 1))\n    plt.plot(line_x, y_pred, color=colors[name], linewidth=lw,\n             label=\'%s (fit time: %.2fs)\' % (name, elapsed_time))\n\nplt.axis(\'tight\')\nplt.legend(loc=\'upper left\')\nplt.title(""Corrupt x"")\nplt.show()\n'"
scikit-learn/_downloads/5a1f414e70da1616e838e311e7fb33d8/plot_model_complexity_influence.py,6,"b'""""""\n==========================\nModel Complexity Influence\n==========================\n\nDemonstrate how model complexity influences both prediction accuracy and\ncomputational performance.\n\nThe dataset is the Boston Housing dataset (resp. 20 Newsgroups) for\nregression (resp. classification).\n\nFor each class of models we make the model complexity vary through the choice\nof relevant model parameters and measure the influence on both computational\nperformance (latency) and predictive power (MSE or Hamming Loss).\n""""""\n\nprint(__doc__)\n\n# Author: Eustache Diemert <eustache@diemert.fr>\n# License: BSD 3 clause\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1.parasite_axes import host_subplot\nfrom mpl_toolkits.axisartist.axislines import Axes\nfrom scipy.sparse.csr import csr_matrix\n\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.svm import NuSVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import hamming_loss\n\n# #############################################################################\n# Routines\n\n\n# Initialize random generator\nnp.random.seed(0)\n\n\ndef generate_data(case, sparse=False):\n    """"""Generate regression/classification data.""""""\n    if case == \'regression\':\n        X, y = datasets.load_boston(return_X_y=True)\n    elif case == \'classification\':\n        X, y = datasets.fetch_20newsgroups_vectorized(subset=\'all\',\n                                                      return_X_y=True)\n    X, y = shuffle(X, y)\n    offset = int(X.shape[0] * 0.8)\n    X_train, y_train = X[:offset], y[:offset]\n    X_test, y_test = X[offset:], y[offset:]\n    if sparse:\n        X_train = csr_matrix(X_train)\n        X_test = csr_matrix(X_test)\n    else:\n        X_train = np.array(X_train)\n        X_test = np.array(X_test)\n    y_test = np.array(y_test)\n    y_train = np.array(y_train)\n    data = {\'X_train\': X_train, \'X_test\': X_test, \'y_train\': y_train,\n            \'y_test\': y_test}\n    return data\n\n\ndef benchmark_influence(conf):\n    """"""\n    Benchmark influence of :changing_param: on both MSE and latency.\n    """"""\n    prediction_times = []\n    prediction_powers = []\n    complexities = []\n    for param_value in conf[\'changing_param_values\']:\n        conf[\'tuned_params\'][conf[\'changing_param\']] = param_value\n        estimator = conf[\'estimator\'](**conf[\'tuned_params\'])\n        print(""Benchmarking %s"" % estimator)\n        estimator.fit(conf[\'data\'][\'X_train\'], conf[\'data\'][\'y_train\'])\n        conf[\'postfit_hook\'](estimator)\n        complexity = conf[\'complexity_computer\'](estimator)\n        complexities.append(complexity)\n        start_time = time.time()\n        for _ in range(conf[\'n_samples\']):\n            y_pred = estimator.predict(conf[\'data\'][\'X_test\'])\n        elapsed_time = (time.time() - start_time) / float(conf[\'n_samples\'])\n        prediction_times.append(elapsed_time)\n        pred_score = conf[\'prediction_performance_computer\'](\n            conf[\'data\'][\'y_test\'], y_pred)\n        prediction_powers.append(pred_score)\n        print(""Complexity: %d | %s: %.4f | Pred. Time: %fs\\n"" % (\n            complexity, conf[\'prediction_performance_label\'], pred_score,\n            elapsed_time))\n    return prediction_powers, prediction_times, complexities\n\n\ndef plot_influence(conf, mse_values, prediction_times, complexities):\n    """"""\n    Plot influence of model complexity on both accuracy and latency.\n    """"""\n    plt.figure(figsize=(12, 6))\n    host = host_subplot(111, axes_class=Axes)\n    plt.subplots_adjust(right=0.75)\n    par1 = host.twinx()\n    host.set_xlabel(\'Model Complexity (%s)\' % conf[\'complexity_label\'])\n    y1_label = conf[\'prediction_performance_label\']\n    y2_label = ""Time (s)""\n    host.set_ylabel(y1_label)\n    par1.set_ylabel(y2_label)\n    p1, = host.plot(complexities, mse_values, \'b-\', label=""prediction error"")\n    p2, = par1.plot(complexities, prediction_times, \'r-\',\n                    label=""latency"")\n    host.legend(loc=\'upper right\')\n    host.axis[""left""].label.set_color(p1.get_color())\n    par1.axis[""right""].label.set_color(p2.get_color())\n    plt.title(\'Influence of Model Complexity - %s\' % conf[\'estimator\'].__name__)\n    plt.show()\n\n\ndef _count_nonzero_coefficients(estimator):\n    a = estimator.coef_.toarray()\n    return np.count_nonzero(a)\n\n# #############################################################################\n# Main code\nregression_data = generate_data(\'regression\')\nclassification_data = generate_data(\'classification\', sparse=True)\nconfigurations = [\n    {\'estimator\': SGDClassifier,\n     \'tuned_params\': {\'penalty\': \'elasticnet\', \'alpha\': 0.001, \'loss\':\n                      \'modified_huber\', \'fit_intercept\': True, \'tol\': 1e-3},\n     \'changing_param\': \'l1_ratio\',\n     \'changing_param_values\': [0.25, 0.5, 0.75, 0.9],\n     \'complexity_label\': \'non_zero coefficients\',\n     \'complexity_computer\': _count_nonzero_coefficients,\n     \'prediction_performance_computer\': hamming_loss,\n     \'prediction_performance_label\': \'Hamming Loss (Misclassification Ratio)\',\n     \'postfit_hook\': lambda x: x.sparsify(),\n     \'data\': classification_data,\n     \'n_samples\': 30},\n    {\'estimator\': NuSVR,\n     \'tuned_params\': {\'C\': 1e3, \'gamma\': 2 ** -15},\n     \'changing_param\': \'nu\',\n     \'changing_param_values\': [0.1, 0.25, 0.5, 0.75, 0.9],\n     \'complexity_label\': \'n_support_vectors\',\n     \'complexity_computer\': lambda x: len(x.support_vectors_),\n     \'data\': regression_data,\n     \'postfit_hook\': lambda x: x,\n     \'prediction_performance_computer\': mean_squared_error,\n     \'prediction_performance_label\': \'MSE\',\n     \'n_samples\': 30},\n    {\'estimator\': GradientBoostingRegressor,\n     \'tuned_params\': {\'loss\': \'ls\'},\n     \'changing_param\': \'n_estimators\',\n     \'changing_param_values\': [10, 50, 100, 200, 500],\n     \'complexity_label\': \'n_trees\',\n     \'complexity_computer\': lambda x: x.n_estimators,\n     \'data\': regression_data,\n     \'postfit_hook\': lambda x: x,\n     \'prediction_performance_computer\': mean_squared_error,\n     \'prediction_performance_label\': \'MSE\',\n     \'n_samples\': 30},\n]\nfor conf in configurations:\n    prediction_performances, prediction_times, complexities = \\\n        benchmark_influence(conf)\n    plot_influence(conf, prediction_performances, prediction_times,\n                   complexities)\n'"
scikit-learn/_downloads/5c62ff5f186ed5efc25fe773fe6aa3a7/plot_svm_tie_breaking.py,5,"b'""""""\n=========================================================\nSVM Tie Breaking Example\n=========================================================\nTie breaking is costly if ``decision_function_shape=\'ovr\'``, and therefore it\nis not enabled by default. This example illustrates the effect of the\n``break_ties`` parameter for a multiclass classification problem and\n``decision_function_shape=\'ovr\'``.\n\nThe two plots differ only in the area in the middle where the classes are\ntied. If ``break_ties=False``, all input in that area would be classified as\none class, whereas if ``break_ties=True``, the tie-breaking mechanism will\ncreate a non-convex decision boundary in that area.\n""""""\nprint(__doc__)\n\n\n# Code source: Andreas Mueller, Adrin Jalali\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(random_state=27)\n\nfig, sub = plt.subplots(2, 1, figsize=(5, 8))\ntitles = (""break_ties = False"",\n          ""break_ties = True"")\n\nfor break_ties, title, ax in zip((False, True), titles, sub.flatten()):\n\n    svm = SVC(kernel=""linear"", C=1, break_ties=break_ties,\n              decision_function_shape=\'ovr\').fit(X, y)\n\n    xlim = [X[:, 0].min(), X[:, 0].max()]\n    ylim = [X[:, 1].min(), X[:, 1].max()]\n\n    xs = np.linspace(xlim[0], xlim[1], 1000)\n    ys = np.linspace(ylim[0], ylim[1], 1000)\n    xx, yy = np.meshgrid(xs, ys)\n\n    pred = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    colors = [plt.cm.Accent(i) for i in [0, 4, 7]]\n\n    points = ax.scatter(X[:, 0], X[:, 1], c=y, cmap=""Accent"")\n    classes = [(0, 1), (0, 2), (1, 2)]\n    line = np.linspace(X[:, 1].min() - 5, X[:, 1].max() + 5)\n    ax.imshow(-pred.reshape(xx.shape), cmap=""Accent"", alpha=.2,\n              extent=(xlim[0], xlim[1], ylim[1], ylim[0]))\n\n    for coef, intercept, col in zip(svm.coef_, svm.intercept_, classes):\n        line2 = -(line * coef[1] + intercept) / coef[0]\n        ax.plot(line2, line, ""-"", c=colors[col[0]])\n        ax.plot(line2, line, ""--"", c=colors[col[1]])\n    ax.set_xlim(xlim)\n    ax.set_ylim(ylim)\n    ax.set_title(title)\n    ax.set_aspect(""equal"")\n\nplt.show()\n'"
scikit-learn/_downloads/5cca0824f9ad80a054d57d0ec833514c/plot_gpr_noisy.py,19,"b'""""""\n=============================================================\nGaussian process regression (GPR) with noise-level estimation\n=============================================================\n\nThis example illustrates that GPR with a sum-kernel including a WhiteKernel can\nestimate the noise level of data. An illustration of the\nlog-marginal-likelihood (LML) landscape shows that there exist two local\nmaxima of LML. The first corresponds to a model with a high noise level and a\nlarge length scale, which explains all variations in the data by noise. The\nsecond one has a smaller noise level and shorter length scale, which explains\nmost of the variation by the noise-free functional relationship. The second\nmodel has a higher likelihood; however, depending on the initial value for the\nhyperparameters, the gradient-based optimization might also converge to the\nhigh-noise solution. It is thus important to repeat the optimization several\ntimes for different initializations.\n""""""\nprint(__doc__)\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import LogNorm\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, WhiteKernel\n\n\nrng = np.random.RandomState(0)\nX = rng.uniform(0, 5, 20)[:, np.newaxis]\ny = 0.5 * np.sin(3 * X[:, 0]) + rng.normal(0, 0.5, X.shape[0])\n\n# First run\nplt.figure()\nkernel = 1.0 * RBF(length_scale=100.0, length_scale_bounds=(1e-2, 1e3)) \\\n    + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))\ngp = GaussianProcessRegressor(kernel=kernel,\n                              alpha=0.0).fit(X, y)\nX_ = np.linspace(0, 5, 100)\ny_mean, y_cov = gp.predict(X_[:, np.newaxis], return_cov=True)\nplt.plot(X_, y_mean, \'k\', lw=3, zorder=9)\nplt.fill_between(X_, y_mean - np.sqrt(np.diag(y_cov)),\n                 y_mean + np.sqrt(np.diag(y_cov)),\n                 alpha=0.5, color=\'k\')\nplt.plot(X_, 0.5*np.sin(3*X_), \'r\', lw=3, zorder=9)\nplt.scatter(X[:, 0], y, c=\'r\', s=50, zorder=10, edgecolors=(0, 0, 0))\nplt.title(""Initial: %s\\nOptimum: %s\\nLog-Marginal-Likelihood: %s""\n          % (kernel, gp.kernel_,\n             gp.log_marginal_likelihood(gp.kernel_.theta)))\nplt.tight_layout()\n\n# Second run\nplt.figure()\nkernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \\\n    + WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e+1))\ngp = GaussianProcessRegressor(kernel=kernel,\n                              alpha=0.0).fit(X, y)\nX_ = np.linspace(0, 5, 100)\ny_mean, y_cov = gp.predict(X_[:, np.newaxis], return_cov=True)\nplt.plot(X_, y_mean, \'k\', lw=3, zorder=9)\nplt.fill_between(X_, y_mean - np.sqrt(np.diag(y_cov)),\n                 y_mean + np.sqrt(np.diag(y_cov)),\n                 alpha=0.5, color=\'k\')\nplt.plot(X_, 0.5*np.sin(3*X_), \'r\', lw=3, zorder=9)\nplt.scatter(X[:, 0], y, c=\'r\', s=50, zorder=10, edgecolors=(0, 0, 0))\nplt.title(""Initial: %s\\nOptimum: %s\\nLog-Marginal-Likelihood: %s""\n          % (kernel, gp.kernel_,\n             gp.log_marginal_likelihood(gp.kernel_.theta)))\nplt.tight_layout()\n\n# Plot LML landscape\nplt.figure()\ntheta0 = np.logspace(-2, 3, 49)\ntheta1 = np.logspace(-2, 0, 50)\nTheta0, Theta1 = np.meshgrid(theta0, theta1)\nLML = [[gp.log_marginal_likelihood(np.log([0.36, Theta0[i, j], Theta1[i, j]]))\n        for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]\nLML = np.array(LML).T\n\nvmin, vmax = (-LML).min(), (-LML).max()\nvmax = 50\nlevel = np.around(np.logspace(np.log10(vmin), np.log10(vmax), 50), decimals=1)\nplt.contour(Theta0, Theta1, -LML,\n            levels=level, norm=LogNorm(vmin=vmin, vmax=vmax))\nplt.colorbar()\nplt.xscale(""log"")\nplt.yscale(""log"")\nplt.xlabel(""Length-scale"")\nplt.ylabel(""Noise-level"")\nplt.title(""Log-marginal-likelihood"")\nplt.tight_layout()\n\nplt.show()\n'"
scikit-learn/_downloads/5d0af61c770c556c3be396352092c7ac/plot_kernel_ridge_regression.py,9,"b'""""""\n=============================================\nComparison of kernel ridge regression and SVR\n=============================================\n\nBoth kernel ridge regression (KRR) and SVR learn a non-linear function by\nemploying the kernel trick, i.e., they learn a linear function in the space\ninduced by the respective kernel which corresponds to a non-linear function in\nthe original space. They differ in the loss functions (ridge versus\nepsilon-insensitive loss). In contrast to SVR, fitting a KRR can be done in\nclosed-form and is typically faster for medium-sized datasets. On the other\nhand, the learned model is non-sparse and thus slower than SVR at\nprediction-time.\n\nThis example illustrates both methods on an artificial dataset, which\nconsists of a sinusoidal target function and strong noise added to every fifth\ndatapoint. The first figure compares the learned model of KRR and SVR when both\ncomplexity/regularization and bandwidth of the RBF kernel are optimized using\ngrid-search. The learned functions are very similar; however, fitting KRR is\napprox. seven times faster than fitting SVR (both with grid-search). However,\nprediction of 100000 target values is more than tree times faster with SVR\nsince it has learned a sparse model using only approx. 1/3 of the 100 training\ndatapoints as support vectors.\n\nThe next figure compares the time for fitting and prediction of KRR and SVR for\ndifferent sizes of the training set. Fitting KRR is faster than SVR for medium-\nsized training sets (less than 1000 samples); however, for larger training sets\nSVR scales better. With regard to prediction time, SVR is faster than\nKRR for all sizes of the training set because of the learned sparse\nsolution. Note that the degree of sparsity and thus the prediction time depends\non the parameters epsilon and C of the SVR.\n""""""\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\n\nimport time\n\nimport numpy as np\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.kernel_ridge import KernelRidge\nimport matplotlib.pyplot as plt\n\nrng = np.random.RandomState(0)\n\n# #############################################################################\n# Generate sample data\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n\nX_plot = np.linspace(0, 5, 100000)[:, None]\n\n# #############################################################################\n# Fit regression model\ntrain_size = 100\nsvr = GridSearchCV(SVR(kernel=\'rbf\', gamma=0.1),\n                   param_grid={""C"": [1e0, 1e1, 1e2, 1e3],\n                               ""gamma"": np.logspace(-2, 2, 5)})\n\nkr = GridSearchCV(KernelRidge(kernel=\'rbf\', gamma=0.1),\n                  param_grid={""alpha"": [1e0, 0.1, 1e-2, 1e-3],\n                              ""gamma"": np.logspace(-2, 2, 5)})\n\nt0 = time.time()\nsvr.fit(X[:train_size], y[:train_size])\nsvr_fit = time.time() - t0\nprint(""SVR complexity and bandwidth selected and model fitted in %.3f s""\n      % svr_fit)\n\nt0 = time.time()\nkr.fit(X[:train_size], y[:train_size])\nkr_fit = time.time() - t0\nprint(""KRR complexity and bandwidth selected and model fitted in %.3f s""\n      % kr_fit)\n\nsv_ratio = svr.best_estimator_.support_.shape[0] / train_size\nprint(""Support vector ratio: %.3f"" % sv_ratio)\n\nt0 = time.time()\ny_svr = svr.predict(X_plot)\nsvr_predict = time.time() - t0\nprint(""SVR prediction for %d inputs in %.3f s""\n      % (X_plot.shape[0], svr_predict))\n\nt0 = time.time()\ny_kr = kr.predict(X_plot)\nkr_predict = time.time() - t0\nprint(""KRR prediction for %d inputs in %.3f s""\n      % (X_plot.shape[0], kr_predict))\n\n\n# #############################################################################\n# Look at the results\nsv_ind = svr.best_estimator_.support_\nplt.scatter(X[sv_ind], y[sv_ind], c=\'r\', s=50, label=\'SVR support vectors\',\n            zorder=2, edgecolors=(0, 0, 0))\nplt.scatter(X[:100], y[:100], c=\'k\', label=\'data\', zorder=1,\n            edgecolors=(0, 0, 0))\nplt.plot(X_plot, y_svr, c=\'r\',\n         label=\'SVR (fit: %.3fs, predict: %.3fs)\' % (svr_fit, svr_predict))\nplt.plot(X_plot, y_kr, c=\'g\',\n         label=\'KRR (fit: %.3fs, predict: %.3fs)\' % (kr_fit, kr_predict))\nplt.xlabel(\'data\')\nplt.ylabel(\'target\')\nplt.title(\'SVR versus Kernel Ridge\')\nplt.legend()\n\n# Visualize training and prediction time\nplt.figure()\n\n# Generate sample data\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\nsizes = np.logspace(1, 4, 7).astype(np.int)\nfor name, estimator in {""KRR"": KernelRidge(kernel=\'rbf\', alpha=0.1,\n                                           gamma=10),\n                        ""SVR"": SVR(kernel=\'rbf\', C=1e1, gamma=10)}.items():\n    train_time = []\n    test_time = []\n    for train_test_size in sizes:\n        t0 = time.time()\n        estimator.fit(X[:train_test_size], y[:train_test_size])\n        train_time.append(time.time() - t0)\n\n        t0 = time.time()\n        estimator.predict(X_plot[:1000])\n        test_time.append(time.time() - t0)\n\n    plt.plot(sizes, train_time, \'o-\', color=""r"" if name == ""SVR"" else ""g"",\n             label=""%s (train)"" % name)\n    plt.plot(sizes, test_time, \'o--\', color=""r"" if name == ""SVR"" else ""g"",\n             label=""%s (test)"" % name)\n\nplt.xscale(""log"")\nplt.yscale(""log"")\nplt.xlabel(""Train size"")\nplt.ylabel(""Time (seconds)"")\nplt.title(\'Execution Time\')\nplt.legend(loc=""best"")\n\n# Visualize learning curves\nplt.figure()\n\nsvr = SVR(kernel=\'rbf\', C=1e1, gamma=0.1)\nkr = KernelRidge(kernel=\'rbf\', alpha=0.1, gamma=0.1)\ntrain_sizes, train_scores_svr, test_scores_svr = \\\n    learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring=""neg_mean_squared_error"", cv=10)\ntrain_sizes_abs, train_scores_kr, test_scores_kr = \\\n    learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring=""neg_mean_squared_error"", cv=10)\n\nplt.plot(train_sizes, -test_scores_svr.mean(1), \'o-\', color=""r"",\n         label=""SVR"")\nplt.plot(train_sizes, -test_scores_kr.mean(1), \'o-\', color=""g"",\n         label=""KRR"")\nplt.xlabel(""Train size"")\nplt.ylabel(""Mean Squared Error"")\nplt.title(\'Learning curves\')\nplt.legend(loc=""best"")\n\nplt.show()\n'"
scikit-learn/_downloads/5fcb257996ac064cb453946056faa4fb/grid_search_text_feature_extraction.py,0,"b'\n""""""\n==========================================================\nSample pipeline for text feature extraction and evaluation\n==========================================================\n\nThe dataset used in this example is the 20 newsgroups dataset which will be\nautomatically downloaded and then cached and reused for the document\nclassification example.\n\nYou can adjust the number of categories by giving their names to the dataset\nloader or setting them to None to get the 20 of them.\n\nHere is a sample output of a run on a quad-core machine::\n\n  Loading 20 newsgroups dataset for categories:\n  [\'alt.atheism\', \'talk.religion.misc\']\n  1427 documents\n  2 categories\n\n  Performing grid search...\n  pipeline: [\'vect\', \'tfidf\', \'clf\']\n  parameters:\n  {\'clf__alpha\': (1.0000000000000001e-05, 9.9999999999999995e-07),\n   \'clf__max_iter\': (10, 50, 80),\n   \'clf__penalty\': (\'l2\', \'elasticnet\'),\n   \'tfidf__use_idf\': (True, False),\n   \'vect__max_n\': (1, 2),\n   \'vect__max_df\': (0.5, 0.75, 1.0),\n   \'vect__max_features\': (None, 5000, 10000, 50000)}\n  done in 1737.030s\n\n  Best score: 0.940\n  Best parameters set:\n      clf__alpha: 9.9999999999999995e-07\n      clf__max_iter: 50\n      clf__penalty: \'elasticnet\'\n      tfidf__use_idf: True\n      vect__max_n: 2\n      vect__max_df: 0.75\n      vect__max_features: 50000\n\n""""""\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n#         Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Mathieu Blondel <mathieu@mblondel.org>\n# License: BSD 3 clause\nfrom pprint import pprint\nfrom time import time\nimport logging\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format=\'%(asctime)s %(levelname)s %(message)s\')\n\n\n# #############################################################################\n# Load some categories from the training set\ncategories = [\n    \'alt.atheism\',\n    \'talk.religion.misc\',\n]\n# Uncomment the following to do the analysis on all the categories\n#categories = None\n\nprint(""Loading 20 newsgroups dataset for categories:"")\nprint(categories)\n\ndata = fetch_20newsgroups(subset=\'train\', categories=categories)\nprint(""%d documents"" % len(data.filenames))\nprint(""%d categories"" % len(data.target_names))\nprint()\n\n# #############################################################################\n# Define a pipeline combining a text feature extractor with a simple\n# classifier\npipeline = Pipeline([\n    (\'vect\', CountVectorizer()),\n    (\'tfidf\', TfidfTransformer()),\n    (\'clf\', SGDClassifier()),\n])\n\n# uncommenting more parameters will give better exploring power but will\n# increase processing time in a combinatorial way\nparameters = {\n    \'vect__max_df\': (0.5, 0.75, 1.0),\n    # \'vect__max_features\': (None, 5000, 10000, 50000),\n    \'vect__ngram_range\': ((1, 1), (1, 2)),  # unigrams or bigrams\n    # \'tfidf__use_idf\': (True, False),\n    # \'tfidf__norm\': (\'l1\', \'l2\'),\n    \'clf__max_iter\': (20,),\n    \'clf__alpha\': (0.00001, 0.000001),\n    \'clf__penalty\': (\'l2\', \'elasticnet\'),\n    # \'clf__max_iter\': (10, 50, 80),\n}\n\nif __name__ == ""__main__"":\n    # multiprocessing requires the fork to happen in a __main__ protected\n    # block\n\n    # find the best parameters for both the feature extraction and the\n    # classifier\n    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n\n    print(""Performing grid search..."")\n    print(""pipeline:"", [name for name, _ in pipeline.steps])\n    print(""parameters:"")\n    pprint(parameters)\n    t0 = time()\n    grid_search.fit(data.data, data.target)\n    print(""done in %0.3fs"" % (time() - t0))\n    print()\n\n    print(""Best score: %0.3f"" % grid_search.best_score_)\n    print(""Best parameters set:"")\n    best_parameters = grid_search.best_estimator_.get_params()\n    for param_name in sorted(parameters.keys()):\n        print(""\\t%s: %r"" % (param_name, best_parameters[param_name]))\n'"
scikit-learn/_downloads/600a6a4930d02fa139d7605c8171d538/approximate_nearest_neighbors.py,9,"b'""""""\n=====================================\nApproximate nearest neighbors in TSNE\n=====================================\n\nThis example presents how to chain KNeighborsTransformer and TSNE in a\npipeline. It also shows how to wrap the packages `annoy` and `nmslib` to\nreplace KNeighborsTransformer and perform approximate nearest neighbors.\nThese packages can be installed with `pip install annoy nmslib`.\n\nNote: Currently `TSNE(metric=\'precomputed\')` does not modify the precomputed\ndistances, and thus assumes that precomputed euclidean distances are squared.\nIn future versions, a parameter in TSNE will control the optional squaring of\nprecomputed distances (see #12401).\n\nNote: In KNeighborsTransformer we use the definition which includes each\ntraining point as its own neighbor in the count of `n_neighbors`, and for\ncompatibility reasons, one extra neighbor is computed when\n`mode == \'distance\'`. Please note that we do the same in the proposed wrappers.\n\nSample output::\n\n    Benchmarking on MNIST_2000:\n    ---------------------------\n    AnnoyTransformer:                    0.583 sec\n    NMSlibTransformer:                   0.321 sec\n    KNeighborsTransformer:               1.225 sec\n    TSNE with AnnoyTransformer:          4.903 sec\n    TSNE with NMSlibTransformer:         5.009 sec\n    TSNE with KNeighborsTransformer:     6.210 sec\n    TSNE with internal NearestNeighbors: 6.365 sec\n\n    Benchmarking on MNIST_10000:\n    ----------------------------\n    AnnoyTransformer:                    4.457 sec\n    NMSlibTransformer:                   2.080 sec\n    KNeighborsTransformer:               30.680 sec\n    TSNE with AnnoyTransformer:          30.225 sec\n    TSNE with NMSlibTransformer:         43.295 sec\n    TSNE with KNeighborsTransformer:     64.845 sec\n    TSNE with internal NearestNeighbors: 64.984 sec\n\n""""""\n# Author: Tom Dupre la Tour\n#\n# License: BSD 3 clause\nimport time\nimport sys\n\ntry:\n    import annoy\nexcept ImportError:\n    print(""The package \'annoy\' is required to run this example."")\n    sys.exit()\n\ntry:\n    import nmslib\nexcept ImportError:\n    print(""The package \'nmslib\' is required to run this example."")\n    sys.exit()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nfrom scipy.sparse import csr_matrix\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.neighbors import KNeighborsTransformer\nfrom sklearn.utils._testing import assert_array_almost_equal\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.manifold import TSNE\nfrom sklearn.utils import shuffle\n\nprint(__doc__)\n\n\nclass NMSlibTransformer(TransformerMixin, BaseEstimator):\n    """"""Wrapper for using nmslib as sklearn\'s KNeighborsTransformer""""""\n\n    def __init__(self, n_neighbors=5, metric=\'euclidean\', method=\'sw-graph\',\n                 n_jobs=1):\n        self.n_neighbors = n_neighbors\n        self.method = method\n        self.metric = metric\n        self.n_jobs = n_jobs\n\n    def fit(self, X):\n        self.n_samples_fit_ = X.shape[0]\n\n        # see more metric in the manual\n        # https://github.com/nmslib/nmslib/tree/master/manual\n        space = {\n            \'sqeuclidean\': \'l2\',\n            \'euclidean\': \'l2\',\n            \'cosine\': \'cosinesimil\',\n            \'l1\': \'l1\',\n            \'l2\': \'l2\',\n        }[self.metric]\n\n        self.nmslib_ = nmslib.init(method=self.method, space=space)\n        self.nmslib_.addDataPointBatch(X)\n        self.nmslib_.createIndex()\n        return self\n\n    def transform(self, X):\n        n_samples_transform = X.shape[0]\n\n        # For compatibility reasons, as each sample is considered as its own\n        # neighbor, one extra neighbor will be computed.\n        n_neighbors = self.n_neighbors + 1\n\n        results = self.nmslib_.knnQueryBatch(X, k=n_neighbors,\n                                             num_threads=self.n_jobs)\n        indices, distances = zip(*results)\n        indices, distances = np.vstack(indices), np.vstack(distances)\n\n        if self.metric == \'sqeuclidean\':\n            distances **= 2\n\n        indptr = np.arange(0, n_samples_transform * n_neighbors + 1,\n                           n_neighbors)\n        kneighbors_graph = csr_matrix((distances.ravel(), indices.ravel(),\n                                       indptr), shape=(n_samples_transform,\n                                                       self.n_samples_fit_))\n\n        return kneighbors_graph\n\n\nclass AnnoyTransformer(TransformerMixin, BaseEstimator):\n    """"""Wrapper for using annoy.AnnoyIndex as sklearn\'s KNeighborsTransformer""""""\n\n    def __init__(self, n_neighbors=5, metric=\'euclidean\', n_trees=10,\n                 search_k=-1):\n        self.n_neighbors = n_neighbors\n        self.n_trees = n_trees\n        self.search_k = search_k\n        self.metric = metric\n\n    def fit(self, X):\n        self.n_samples_fit_ = X.shape[0]\n        metric = self.metric if self.metric != \'sqeuclidean\' else \'euclidean\'\n        self.annoy_ = annoy.AnnoyIndex(X.shape[1], metric=metric)\n        for i, x in enumerate(X):\n            self.annoy_.add_item(i, x.tolist())\n        self.annoy_.build(self.n_trees)\n        return self\n\n    def transform(self, X):\n        return self._transform(X)\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X)._transform(X=None)\n\n    def _transform(self, X):\n        """"""As `transform`, but handles X is None for faster `fit_transform`.""""""\n\n        n_samples_transform = self.n_samples_fit_ if X is None else X.shape[0]\n\n        # For compatibility reasons, as each sample is considered as its own\n        # neighbor, one extra neighbor will be computed.\n        n_neighbors = self.n_neighbors + 1\n\n        indices = np.empty((n_samples_transform, n_neighbors),\n                           dtype=np.int)\n        distances = np.empty((n_samples_transform, n_neighbors))\n\n        if X is None:\n            for i in range(self.annoy_.get_n_items()):\n                ind, dist = self.annoy_.get_nns_by_item(\n                    i, n_neighbors, self.search_k, include_distances=True)\n\n                indices[i], distances[i] = ind, dist\n        else:\n            for i, x in enumerate(X):\n                indices[i], distances[i] = self.annoy_.get_nns_by_vector(\n                    x.tolist(), n_neighbors, self.search_k,\n                    include_distances=True)\n\n        if self.metric == \'sqeuclidean\':\n            distances **= 2\n\n        indptr = np.arange(0, n_samples_transform * n_neighbors + 1,\n                           n_neighbors)\n        kneighbors_graph = csr_matrix((distances.ravel(), indices.ravel(),\n                                       indptr), shape=(n_samples_transform,\n                                                       self.n_samples_fit_))\n\n        return kneighbors_graph\n\n\ndef test_transformers():\n    """"""Test that AnnoyTransformer and KNeighborsTransformer give same results\n    """"""\n    X = np.random.RandomState(42).randn(10, 2)\n\n    knn = KNeighborsTransformer()\n    Xt0 = knn.fit_transform(X)\n\n    ann = AnnoyTransformer()\n    Xt1 = ann.fit_transform(X)\n\n    nms = NMSlibTransformer()\n    Xt2 = nms.fit_transform(X)\n\n    assert_array_almost_equal(Xt0.toarray(), Xt1.toarray(), decimal=5)\n    assert_array_almost_equal(Xt0.toarray(), Xt2.toarray(), decimal=5)\n\n\ndef load_mnist(n_samples):\n    """"""Load MNIST, shuffle the data, and return only n_samples.""""""\n    mnist = fetch_openml(data_id=41063)\n    X, y = shuffle(mnist.data, mnist.target, random_state=42)\n    return X[:n_samples], y[:n_samples]\n\n\ndef run_benchmark():\n    datasets = [\n        (\'MNIST_2000\', load_mnist(n_samples=2000)),\n        (\'MNIST_10000\', load_mnist(n_samples=10000)),\n    ]\n\n    n_iter = 500\n    perplexity = 30\n    # TSNE requires a certain number of neighbors which depends on the\n    # perplexity parameter.\n    # Add one since we include each sample as its own neighbor.\n    n_neighbors = int(3. * perplexity + 1) + 1\n\n    transformers = [\n        (\'AnnoyTransformer\', AnnoyTransformer(n_neighbors=n_neighbors,\n                                              metric=\'sqeuclidean\')),\n        (\'NMSlibTransformer\', NMSlibTransformer(n_neighbors=n_neighbors,\n                                                metric=\'sqeuclidean\')),\n        (\'KNeighborsTransformer\', KNeighborsTransformer(\n            n_neighbors=n_neighbors, mode=\'distance\', metric=\'sqeuclidean\')),\n        (\'TSNE with AnnoyTransformer\', make_pipeline(\n            AnnoyTransformer(n_neighbors=n_neighbors, metric=\'sqeuclidean\'),\n            TSNE(metric=\'precomputed\', perplexity=perplexity,\n                 method=""barnes_hut"", random_state=42, n_iter=n_iter), )),\n        (\'TSNE with NMSlibTransformer\', make_pipeline(\n            NMSlibTransformer(n_neighbors=n_neighbors, metric=\'sqeuclidean\'),\n            TSNE(metric=\'precomputed\', perplexity=perplexity,\n                 method=""barnes_hut"", random_state=42, n_iter=n_iter), )),\n        (\'TSNE with KNeighborsTransformer\', make_pipeline(\n            KNeighborsTransformer(n_neighbors=n_neighbors, mode=\'distance\',\n                                  metric=\'sqeuclidean\'),\n            TSNE(metric=\'precomputed\', perplexity=perplexity,\n                 method=""barnes_hut"", random_state=42, n_iter=n_iter), )),\n        (\'TSNE with internal NearestNeighbors\',\n         TSNE(metric=\'sqeuclidean\', perplexity=perplexity, method=""barnes_hut"",\n              random_state=42, n_iter=n_iter)),\n    ]\n\n    # init the plot\n    nrows = len(datasets)\n    ncols = np.sum([1 for name, model in transformers if \'TSNE\' in name])\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, squeeze=False,\n                             figsize=(5 * ncols, 4 * nrows))\n    axes = axes.ravel()\n    i_ax = 0\n\n    for dataset_name, (X, y) in datasets:\n\n        msg = \'Benchmarking on %s:\' % dataset_name\n        print(\'\\n%s\\n%s\' % (msg, \'-\' * len(msg)))\n\n        for transformer_name, transformer in transformers:\n            start = time.time()\n            Xt = transformer.fit_transform(X)\n            duration = time.time() - start\n\n            # print the duration report\n            longest = np.max([len(name) for name, model in transformers])\n            whitespaces = \' \' * (longest - len(transformer_name))\n            print(\'%s: %s%.3f sec\' % (transformer_name, whitespaces, duration))\n\n            # plot TSNE embedding which should be very similar across methods\n            if \'TSNE\' in transformer_name:\n                axes[i_ax].set_title(transformer_name + \'\\non \' + dataset_name)\n                axes[i_ax].scatter(Xt[:, 0], Xt[:, 1], c=y, alpha=0.2,\n                                   cmap=plt.cm.viridis)\n                axes[i_ax].xaxis.set_major_formatter(NullFormatter())\n                axes[i_ax].yaxis.set_major_formatter(NullFormatter())\n                axes[i_ax].axis(\'tight\')\n                i_ax += 1\n\n    fig.tight_layout()\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    test_transformers()\n    run_benchmark()\n'"
scikit-learn/_downloads/619c2ee76a48d6c7682ebfc85b26da37/plot_random_forest_embedding.py,3,"b'""""""\n=========================================================\nHashing feature transformation using Totally Random Trees\n=========================================================\n\nRandomTreesEmbedding provides a way to map data to a\nvery high-dimensional, sparse representation, which might\nbe beneficial for classification.\nThe mapping is completely unsupervised and very efficient.\n\nThis example visualizes the partitions given by several\ntrees and shows how the transformation can also be used for\nnon-linear dimensionality reduction or non-linear classification.\n\nPoints that are neighboring often share the same leaf of a tree and therefore\nshare large parts of their hashed representation. This allows to\nseparate two concentric circles simply based on the principal components\nof the transformed data with truncated SVD.\n\nIn high-dimensional spaces, linear classifiers often achieve\nexcellent accuracy. For sparse binary data, BernoulliNB\nis particularly well-suited. The bottom row compares the\ndecision boundary obtained by BernoulliNB in the transformed\nspace with an ExtraTreesClassifier forests learned on the\noriginal data.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_circles\nfrom sklearn.ensemble import RandomTreesEmbedding, ExtraTreesClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.naive_bayes import BernoulliNB\n\n# make a synthetic dataset\nX, y = make_circles(factor=0.5, random_state=0, noise=0.05)\n\n# use RandomTreesEmbedding to transform data\nhasher = RandomTreesEmbedding(n_estimators=10, random_state=0, max_depth=3)\nX_transformed = hasher.fit_transform(X)\n\n# Visualize result after dimensionality reduction using truncated SVD\nsvd = TruncatedSVD(n_components=2)\nX_reduced = svd.fit_transform(X_transformed)\n\n# Learn a Naive Bayes classifier on the transformed data\nnb = BernoulliNB()\nnb.fit(X_transformed, y)\n\n\n# Learn an ExtraTreesClassifier for comparison\ntrees = ExtraTreesClassifier(max_depth=3, n_estimators=10, random_state=0)\ntrees.fit(X, y)\n\n\n# scatter plot of original and reduced data\nfig = plt.figure(figsize=(9, 8))\n\nax = plt.subplot(221)\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\'k\')\nax.set_title(""Original Data (2d)"")\nax.set_xticks(())\nax.set_yticks(())\n\nax = plt.subplot(222)\nax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=50, edgecolor=\'k\')\nax.set_title(""Truncated SVD reduction (2d) of transformed data (%dd)"" %\n             X_transformed.shape[1])\nax.set_xticks(())\nax.set_yticks(())\n\n# Plot the decision in original space. For that, we will assign a color\n# to each point in the mesh [x_min, x_max]x[y_min, y_max].\nh = .01\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# transform grid using RandomTreesEmbedding\ntransformed_grid = hasher.transform(np.c_[xx.ravel(), yy.ravel()])\ny_grid_pred = nb.predict_proba(transformed_grid)[:, 1]\n\nax = plt.subplot(223)\nax.set_title(""Naive Bayes on Transformed data"")\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\'k\')\nax.set_ylim(-1.4, 1.4)\nax.set_xlim(-1.4, 1.4)\nax.set_xticks(())\nax.set_yticks(())\n\n# transform grid using ExtraTreesClassifier\ny_grid_pred = trees.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\nax = plt.subplot(224)\nax.set_title(""ExtraTrees predictions"")\nax.pcolormesh(xx, yy, y_grid_pred.reshape(xx.shape))\nax.scatter(X[:, 0], X[:, 1], c=y, s=50, edgecolor=\'k\')\nax.set_ylim(-1.4, 1.4)\nax.set_xlim(-1.4, 1.4)\nax.set_xticks(())\nax.set_yticks(())\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/6a4fbbbb637380a1d4858a158a956f91/plot_partial_dependence.py,1,"b'""""""\n========================\nPartial Dependence Plots\n========================\n\nPartial dependence plots show the dependence between the target function [2]_\nand a set of \'target\' features, marginalizing over the values of all other\nfeatures (the complement features). Due to the limits of human perception, the\nsize of the target feature set must be small (usually, one or two) thus the\ntarget features are usually chosen among the most important features.\n\nThis example shows how to obtain partial dependence plots from a\n:class:`~sklearn.neural_network.MLPRegressor` and a\n:class:`~sklearn.ensemble.HistGradientBoostingRegressor` trained on the\nCalifornia housing dataset. The example is taken from [1]_.\n\nThe plots show four 1-way and two 1-way partial dependence plots (omitted for\n:class:`~sklearn.neural_network.MLPRegressor` due to computation time). The\ntarget variables for the one-way PDP are: median income (`MedInc`), average\noccupants per household (`AvgOccup`), median house age (`HouseAge`), and\naverage rooms per household (`AveRooms`).\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, ""Elements of Statistical\n       Learning Ed. 2"", Springer, 2009.\n\n.. [2] For classification you can think of it as the regression score before\n       the link function.\n""""""\nprint(__doc__)\n\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.inspection import partial_dependence\nfrom sklearn.inspection import plot_partial_dependence\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.datasets import fetch_california_housing\n\n\n##############################################################################\n# California Housing data preprocessing\n# -------------------------------------\n#\n# Center target to avoid gradient boosting init bias: gradient boosting\n# with the \'recursion\' method does not account for the initial estimator\n# (here the average target, by default)\n\ncal_housing = fetch_california_housing()\nX = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\ny = cal_housing.target\n\ny -= y.mean()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,\n                                                    random_state=0)\n\n##############################################################################\n# Partial Dependence computation for multi-layer perceptron\n# ---------------------------------------------------------\n#\n# Let\'s fit a MLPRegressor and compute single-variable partial dependence\n# plots\n\nprint(""Training MLPRegressor..."")\ntic = time()\nest = make_pipeline(QuantileTransformer(),\n                    MLPRegressor(hidden_layer_sizes=(50, 50),\n                                 learning_rate_init=0.01,\n                                 early_stopping=True))\nest.fit(X_train, y_train)\nprint(""done in {:.3f}s"".format(time() - tic))\nprint(""Test R2 score: {:.2f}"".format(est.score(X_test, y_test)))\n\n##############################################################################\n# We configured a pipeline to scale the numerical input features and tuned the\n# neural network size and learning rate to get a reasonable compromise between\n# training time and predictive performance on a test set.\n#\n# Importantly, this tabular dataset has very different dynamic ranges for its\n# features. Neural networks tend to be very sensitive to features with varying\n# scales and forgetting to preprocess the numeric feature would lead to a very\n# poor model.\n#\n# It would be possible to get even higher predictive performance with a larger\n# neural network but the training would also be significantly more expensive.\n#\n# Note that it is important to check that the model is accurate enough on a\n# test set before plotting the partial dependence since there would be little\n# use in explaining the impact of a given feature on the prediction function of\n# a poor model.\n#\n# Let\'s now compute the partial dependence plots for this neural network using\n# the model-agnostic (brute-force) method:\n\nprint(\'Computing partial dependence plots...\')\ntic = time()\n# We don\'t compute the 2-way PDP (5, 1) here, because it is a lot slower\n# with the brute method.\nfeatures = [\'MedInc\', \'AveOccup\', \'HouseAge\', \'AveRooms\']\nplot_partial_dependence(est, X_train, features,\n                        n_jobs=3, grid_resolution=20)\nprint(""done in {:.3f}s"".format(time() - tic))\nfig = plt.gcf()\nfig.suptitle(\'Partial dependence of house value on non-location features\\n\'\n             \'for the California housing dataset, with MLPRegressor\')\nfig.subplots_adjust(hspace=0.3)\n\n##############################################################################\n# Partial Dependence computation for Gradient Boosting\n# ----------------------------------------------------\n#\n# Let\'s now fit a GradientBoostingRegressor and compute the partial dependence\n# plots either or one or two variables at a time.\n\nprint(""Training GradientBoostingRegressor..."")\ntic = time()\nest = HistGradientBoostingRegressor()\nest.fit(X_train, y_train)\nprint(""done in {:.3f}s"".format(time() - tic))\nprint(""Test R2 score: {:.2f}"".format(est.score(X_test, y_test)))\n\n##############################################################################\n# Here, we used the default hyperparameters for the gradient boosting model\n# without any preprocessing as tree-based models are naturally robust to\n# monotonic transformations of numerical features.\n#\n# Note that on this tabular dataset, Gradient Boosting Machines are both\n# significantly faster to train and more accurate than neural networks. It is\n# also significantly cheaper to tune their hyperparameters (the default tend to\n# work well while this is not often the case for neural networks).\n#\n# Finally, as we will see next, computing partial dependence plots tree-based\n# models is also orders of magnitude faster making it cheap to compute partial\n# dependence plots for pairs of interacting features:\n\nprint(\'Computing partial dependence plots...\')\ntic = time()\nfeatures = [\'MedInc\', \'AveOccup\', \'HouseAge\', \'AveRooms\',\n            (\'AveOccup\', \'HouseAge\')]\nplot_partial_dependence(est, X_train, features,\n                        n_jobs=3, grid_resolution=20)\nprint(""done in {:.3f}s"".format(time() - tic))\nfig = plt.gcf()\nfig.suptitle(\'Partial dependence of house value on non-location features\\n\'\n             \'for the California housing dataset, with Gradient Boosting\')\nfig.subplots_adjust(wspace=0.4, hspace=0.3)\n\n\n##############################################################################\n# Analysis of the plots\n# ---------------------\n#\n# We can clearly see that the median house price shows a linear relationship\n# with the median income (top left) and that the house price drops when the\n# average occupants per household increases (top middle).\n# The top right plot shows that the house age in a district does not have\n# a strong influence on the (median) house price; so does the average rooms\n# per household.\n# The tick marks on the x-axis represent the deciles of the feature values\n# in the training data.\n#\n# We also observe that :class:`~sklearn.neural_network.MLPRegressor` has much\n# smoother predictions than\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`. For the plots to be\n# comparable, it is necessary to subtract the average value of the target\n# ``y``: The \'recursion\' method, used by default for\n# :class:`~sklearn.ensemble.HistGradientBoostingRegressor`, does not account\n# for the initial predictor (in our case the average target). Setting the\n# target average to 0 avoids this bias.\n#\n# Partial dependence plots with two target features enable us to visualize\n# interactions among them. The two-way partial dependence plot shows the\n# dependence of median house price on joint values of house age and average\n# occupants per household. We can clearly see an interaction between the\n# two features: for an average occupancy greater than two, the house price is\n# nearly independent of the house age, whereas for values less than two there\n# is a strong dependence on age.\n\n##############################################################################\n# 3D interaction plots\n# --------------------\n#\n# Let\'s make the same partial dependence plot for the 2 features interaction,\n# this time in 3 dimensions.\n\nfig = plt.figure()\n\nfeatures = (\'AveOccup\', \'HouseAge\')\npdp, axes = partial_dependence(est, X_train, features=features,\n                               grid_resolution=20)\nXX, YY = np.meshgrid(axes[0], axes[1])\nZ = pdp[0].T\nax = Axes3D(fig)\nsurf = ax.plot_surface(XX, YY, Z, rstride=1, cstride=1,\n                       cmap=plt.cm.BuPu, edgecolor=\'k\')\nax.set_xlabel(features[0])\nax.set_ylabel(features[1])\nax.set_zlabel(\'Partial dependence\')\n#  pretty init view\nax.view_init(elev=22, azim=122)\nplt.colorbar(surf)\nplt.suptitle(\'Partial dependence of house value on median\\n\'\n             \'age and average occupancy, with Gradient Boosting\')\nplt.subplots_adjust(top=0.9)\n\nplt.show()\n'"
scikit-learn/_downloads/6ae10ab656fe9a2fa2e0e8219999f17f/plot_logistic_l1_l2_sparsity.py,5,"b'""""""\n==============================================\nL1 Penalty and Sparsity in Logistic Regression\n==============================================\n\nComparison of the sparsity (percentage of zero coefficients) of solutions when\nL1, L2 and Elastic-Net penalty are used for different values of C. We can see\nthat large values of C give more freedom to the model.  Conversely, smaller\nvalues of C constrain the model more. In the L1 penalty case, this leads to\nsparser solutions. As expected, the Elastic-Net penalty sparsity is between\nthat of L1 and L2.\n\nWe classify 8x8 images of digits into two classes: 0-4 against 5-9.\nThe visualization shows coefficients of the models for varying C.\n""""""\n\nprint(__doc__)\n\n# Authors: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Andreas Mueller <amueller@ais.uni-bonn.de>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\nX, y = datasets.load_digits(return_X_y=True)\n\nX = StandardScaler().fit_transform(X)\n\n# classify small against large digits\ny = (y > 4).astype(np.int)\n\nl1_ratio = 0.5  # L1 weight in the Elastic-Net regularization\n\nfig, axes = plt.subplots(3, 3)\n\n# Set regularization parameter\nfor i, (C, axes_row) in enumerate(zip((1, 0.1, 0.01), axes)):\n    # turn down tolerance for short training time\n    clf_l1_LR = LogisticRegression(C=C, penalty=\'l1\', tol=0.01, solver=\'saga\')\n    clf_l2_LR = LogisticRegression(C=C, penalty=\'l2\', tol=0.01, solver=\'saga\')\n    clf_en_LR = LogisticRegression(C=C, penalty=\'elasticnet\', solver=\'saga\',\n                                   l1_ratio=l1_ratio, tol=0.01)\n    clf_l1_LR.fit(X, y)\n    clf_l2_LR.fit(X, y)\n    clf_en_LR.fit(X, y)\n\n    coef_l1_LR = clf_l1_LR.coef_.ravel()\n    coef_l2_LR = clf_l2_LR.coef_.ravel()\n    coef_en_LR = clf_en_LR.coef_.ravel()\n\n    # coef_l1_LR contains zeros due to the\n    # L1 sparsity inducing norm\n\n    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n    sparsity_en_LR = np.mean(coef_en_LR == 0) * 100\n\n    print(""C=%.2f"" % C)\n    print(""{:<40} {:.2f}%"".format(""Sparsity with L1 penalty:"", sparsity_l1_LR))\n    print(""{:<40} {:.2f}%"".format(""Sparsity with Elastic-Net penalty:"",\n                                  sparsity_en_LR))\n    print(""{:<40} {:.2f}%"".format(""Sparsity with L2 penalty:"", sparsity_l2_LR))\n    print(""{:<40} {:.2f}"".format(""Score with L1 penalty:"",\n                                 clf_l1_LR.score(X, y)))\n    print(""{:<40} {:.2f}"".format(""Score with Elastic-Net penalty:"",\n                                 clf_en_LR.score(X, y)))\n    print(""{:<40} {:.2f}"".format(""Score with L2 penalty:"",\n                                 clf_l2_LR.score(X, y)))\n\n    if i == 0:\n        axes_row[0].set_title(""L1 penalty"")\n        axes_row[1].set_title(""Elastic-Net\\nl1_ratio = %s"" % l1_ratio)\n        axes_row[2].set_title(""L2 penalty"")\n\n    for ax, coefs in zip(axes_row, [coef_l1_LR, coef_en_LR, coef_l2_LR]):\n        ax.imshow(np.abs(coefs.reshape(8, 8)), interpolation=\'nearest\',\n                  cmap=\'binary\', vmax=1, vmin=0)\n        ax.set_xticks(())\n        ax.set_yticks(())\n\n    axes_row[0].set_ylabel(\'C = %s\' % C)\n\nplt.show()\n'"
scikit-learn/_downloads/6af3317867d60493a0489650957d2169/plot_mahalanobis_distances.py,15,"b'r""""""\n================================================================\nRobust covariance estimation and Mahalanobis distances relevance\n================================================================\n\nAn example to show covariance estimation with the Mahalanobis\ndistances on Gaussian distributed data.\n\nFor Gaussian distributed data, the distance of an observation\n:math:`x_i` to the mode of the distribution can be computed using its\nMahalanobis distance: :math:`d_{(\\mu,\\Sigma)}(x_i)^2 = (x_i -\n\\mu)\'\\Sigma^{-1}(x_i - \\mu)` where :math:`\\mu` and :math:`\\Sigma` are\nthe location and the covariance of the underlying Gaussian\ndistribution.\n\nIn practice, :math:`\\mu` and :math:`\\Sigma` are replaced by some\nestimates.  The usual covariance maximum likelihood estimate is very\nsensitive to the presence of outliers in the data set and therefor,\nthe corresponding Mahalanobis distances are. One would better have to\nuse a robust estimator of covariance to guarantee that the estimation is\nresistant to ""erroneous"" observations in the data set and that the\nassociated Mahalanobis distances accurately reflect the true\norganisation of the observations.\n\nThe Minimum Covariance Determinant estimator is a robust,\nhigh-breakdown point (i.e. it can be used to estimate the covariance\nmatrix of highly contaminated datasets, up to\n:math:`\\frac{n_\\text{samples}-n_\\text{features}-1}{2}` outliers)\nestimator of covariance. The idea is to find\n:math:`\\frac{n_\\text{samples}+n_\\text{features}+1}{2}`\nobservations whose empirical covariance has the smallest determinant,\nyielding a ""pure"" subset of observations from which to compute\nstandards estimates of location and covariance.\n\nThe Minimum Covariance Determinant estimator (MCD) has been introduced\nby P.J.Rousseuw in [1].\n\nThis example illustrates how the Mahalanobis distances are affected by\noutlying data: observations drawn from a contaminating distribution\nare not distinguishable from the observations coming from the real,\nGaussian distribution that one may want to work with. Using MCD-based\nMahalanobis distances, the two populations become\ndistinguishable. Associated applications are outliers detection,\nobservations ranking, clustering, ...\nFor visualization purpose, the cubic root of the Mahalanobis distances\nare represented in the boxplot, as Wilson and Hilferty suggest [2]\n\n[1] P. J. Rousseeuw. Least median of squares regression. J. Am\n    Stat Ass, 79:871, 1984.\n[2] Wilson, E. B., & Hilferty, M. M. (1931). The distribution of chi-square.\n    Proceedings of the National Academy of Sciences of the United States\n    of America, 17, 684-688.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.covariance import EmpiricalCovariance, MinCovDet\n\nn_samples = 125\nn_outliers = 25\nn_features = 2\n\n# generate data\ngen_cov = np.eye(n_features)\ngen_cov[0, 0] = 2.\nX = np.dot(np.random.randn(n_samples, n_features), gen_cov)\n# add some outliers\noutliers_cov = np.eye(n_features)\noutliers_cov[np.arange(1, n_features), np.arange(1, n_features)] = 7.\nX[-n_outliers:] = np.dot(np.random.randn(n_outliers, n_features), outliers_cov)\n\n# fit a Minimum Covariance Determinant (MCD) robust estimator to data\nrobust_cov = MinCovDet().fit(X)\n\n# compare estimators learnt from the full data set with true parameters\nemp_cov = EmpiricalCovariance().fit(X)\n\n# #############################################################################\n# Display results\nfig = plt.figure()\nplt.subplots_adjust(hspace=-.1, wspace=.4, top=.95, bottom=.05)\n\n# Show data set\nsubfig1 = plt.subplot(3, 1, 1)\ninlier_plot = subfig1.scatter(X[:, 0], X[:, 1],\n                              color=\'black\', label=\'inliers\')\noutlier_plot = subfig1.scatter(X[:, 0][-n_outliers:], X[:, 1][-n_outliers:],\n                               color=\'red\', label=\'outliers\')\nsubfig1.set_xlim(subfig1.get_xlim()[0], 11.)\nsubfig1.set_title(""Mahalanobis distances of a contaminated data set:"")\n\n# Show contours of the distance functions\nxx, yy = np.meshgrid(np.linspace(plt.xlim()[0], plt.xlim()[1], 100),\n                     np.linspace(plt.ylim()[0], plt.ylim()[1], 100))\nzz = np.c_[xx.ravel(), yy.ravel()]\n\nmahal_emp_cov = emp_cov.mahalanobis(zz)\nmahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\nemp_cov_contour = subfig1.contour(xx, yy, np.sqrt(mahal_emp_cov),\n                                  cmap=plt.cm.PuBu_r,\n                                  linestyles=\'dashed\')\n\nmahal_robust_cov = robust_cov.mahalanobis(zz)\nmahal_robust_cov = mahal_robust_cov.reshape(xx.shape)\nrobust_contour = subfig1.contour(xx, yy, np.sqrt(mahal_robust_cov),\n                                 cmap=plt.cm.YlOrBr_r, linestyles=\'dotted\')\n\nsubfig1.legend([emp_cov_contour.collections[1], robust_contour.collections[1],\n                inlier_plot, outlier_plot],\n               [\'MLE dist\', \'robust dist\', \'inliers\', \'outliers\'],\n               loc=""upper right"", borderaxespad=0)\nplt.xticks(())\nplt.yticks(())\n\n# Plot the scores for each point\nemp_mahal = emp_cov.mahalanobis(X - np.mean(X, 0)) ** (0.33)\nsubfig2 = plt.subplot(2, 2, 3)\nsubfig2.boxplot([emp_mahal[:-n_outliers], emp_mahal[-n_outliers:]], widths=.25)\nsubfig2.plot(np.full(n_samples - n_outliers, 1.26),\n             emp_mahal[:-n_outliers], \'+k\', markeredgewidth=1)\nsubfig2.plot(np.full(n_outliers, 2.26),\n             emp_mahal[-n_outliers:], \'+k\', markeredgewidth=1)\nsubfig2.axes.set_xticklabels((\'inliers\', \'outliers\'), size=15)\nsubfig2.set_ylabel(r""$\\sqrt[3]{\\rm{(Mahal. dist.)}}$"", size=16)\nsubfig2.set_title(""1. from non-robust estimates\\n(Maximum Likelihood)"")\nplt.yticks(())\n\nrobust_mahal = robust_cov.mahalanobis(X - robust_cov.location_) ** (0.33)\nsubfig3 = plt.subplot(2, 2, 4)\nsubfig3.boxplot([robust_mahal[:-n_outliers], robust_mahal[-n_outliers:]],\n                widths=.25)\nsubfig3.plot(np.full(n_samples - n_outliers, 1.26),\n             robust_mahal[:-n_outliers], \'+k\', markeredgewidth=1)\nsubfig3.plot(np.full(n_outliers, 2.26),\n             robust_mahal[-n_outliers:], \'+k\', markeredgewidth=1)\nsubfig3.axes.set_xticklabels((\'inliers\', \'outliers\'), size=15)\nsubfig3.set_ylabel(r""$\\sqrt[3]{\\rm{(Mahal. dist.)}}$"", size=16)\nsubfig3.set_title(""2. from robust estimates\\n(Minimum Covariance Determinant)"")\nplt.yticks(())\n\nplt.show()\n'"
scikit-learn/_downloads/6c2c5aad4aaffd759e5057f6c862d744/plot_classification_probability.py,5,"b'""""""\n===============================\nPlot classification probability\n===============================\n\nPlot the classification probability for different classifiers. We use a 3 class\ndataset, and we classify it with a Support Vector classifier, L1 and L2\npenalized logistic regression with either a One-Vs-Rest or multinomial setting,\nand Gaussian process classification.\n\nLinear SVC is not a probabilistic classifier by default but it has a built-in\ncalibration option enabled in this example (`probability=True`).\n\nThe logistic regression with One-Vs-Rest is not a multiclass classifier out of\nthe box. As a result it has more trouble in separating class 2 and 3 than the\nother estimators.\n""""""\nprint(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:, 0:2]  # we only take the first two features for visualization\ny = iris.target\n\nn_features = X.shape[1]\n\nC = 10\nkernel = 1.0 * RBF([1.0, 1.0])  # for GPC\n\n# Create different classifiers.\nclassifiers = {\n    \'L1 logistic\': LogisticRegression(C=C, penalty=\'l1\',\n                                      solver=\'saga\',\n                                      multi_class=\'multinomial\',\n                                      max_iter=10000),\n    \'L2 logistic (Multinomial)\': LogisticRegression(C=C, penalty=\'l2\',\n                                                    solver=\'saga\',\n                                                    multi_class=\'multinomial\',\n                                                    max_iter=10000),\n    \'L2 logistic (OvR)\': LogisticRegression(C=C, penalty=\'l2\',\n                                            solver=\'saga\',\n                                            multi_class=\'ovr\',\n                                            max_iter=10000),\n    \'Linear SVC\': SVC(kernel=\'linear\', C=C, probability=True,\n                      random_state=0),\n    \'GPC\': GaussianProcessClassifier(kernel)\n}\n\nn_classifiers = len(classifiers)\n\nplt.figure(figsize=(3 * 2, n_classifiers * 2))\nplt.subplots_adjust(bottom=.2, top=.95)\n\nxx = np.linspace(3, 9, 100)\nyy = np.linspace(1, 5, 100).T\nxx, yy = np.meshgrid(xx, yy)\nXfull = np.c_[xx.ravel(), yy.ravel()]\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classifier.fit(X, y)\n\n    y_pred = classifier.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n    print(""Accuracy (train) for %s: %0.1f%% "" % (name, accuracy * 100))\n\n    # View probabilities:\n    probas = classifier.predict_proba(Xfull)\n    n_classes = np.unique(y_pred).size\n    for k in range(n_classes):\n        plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)\n        plt.title(""Class %d"" % k)\n        if k == 0:\n            plt.ylabel(name)\n        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),\n                                   extent=(3, 9, 1, 5), origin=\'lower\')\n        plt.xticks(())\n        plt.yticks(())\n        idx = (y_pred == k)\n        if idx.any():\n            plt.scatter(X[idx, 0], X[idx, 1], marker=\'o\', c=\'w\', edgecolor=\'k\')\n\nax = plt.axes([0.15, 0.04, 0.7, 0.05])\nplt.title(""Probability"")\nplt.colorbar(imshow_handle, cax=ax, orientation=\'horizontal\')\n\nplt.show()\n'"
scikit-learn/_downloads/6d5fa5ac14a60abab6cce3166f4d355d/plot_f_test_vs_mi.py,5,"b'""""""\n===========================================\nComparison of F-test and mutual information\n===========================================\n\nThis example illustrates the differences between univariate F-test statistics\nand mutual information.\n\nWe consider 3 features x_1, x_2, x_3 distributed uniformly over [0, 1], the\ntarget depends on them as follows:\n\ny = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third features is\ncompletely irrelevant.\n\nThe code below plots the dependency of y against individual x_i and normalized\nvalues of univariate F-tests statistics and mutual information.\n\nAs F-test captures only linear dependency, it rates x_1 as the most\ndiscriminative feature. On the other hand, mutual information can capture any\nkind of dependency between variables and it rates x_2 as the most\ndiscriminative feature, which probably agrees better with our intuitive\nperception for this example. Both methods correctly marks x_3 as irrelevant.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\n\nnp.random.seed(0)\nX = np.random.rand(1000, 3)\ny = X[:, 0] + np.sin(6 * np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)\n\nf_test, _ = f_regression(X, y)\nf_test /= np.max(f_test)\n\nmi = mutual_info_regression(X, y)\nmi /= np.max(mi)\n\nplt.figure(figsize=(15, 5))\nfor i in range(3):\n    plt.subplot(1, 3, i + 1)\n    plt.scatter(X[:, i], y, edgecolor=\'black\', s=20)\n    plt.xlabel(""$x_{}$"".format(i + 1), fontsize=14)\n    if i == 0:\n        plt.ylabel(""$y$"", fontsize=14)\n    plt.title(""F-test={:.2f}, MI={:.2f}"".format(f_test[i], mi[i]),\n              fontsize=16)\nplt.show()\n'"
scikit-learn/_downloads/6e25d87a796f11b2a1b1b53b56f63114/plot_gpc.py,13,"b'""""""\n====================================================================\nProbabilistic predictions with Gaussian process classification (GPC)\n====================================================================\n\nThis example illustrates the predicted probability of GPC for an RBF kernel\nwith different choices of the hyperparameters. The first figure shows the\npredicted probability of GPC with arbitrarily chosen hyperparameters and with\nthe hyperparameters corresponding to the maximum log-marginal-likelihood (LML).\n\nWhile the hyperparameters chosen by optimizing LML have a considerable larger\nLML, they perform slightly worse according to the log-loss on test data. The\nfigure shows that this is because they exhibit a steep change of the class\nprobabilities at the class boundaries (which is good) but have predicted\nprobabilities close to 0.5 far away from the class boundaries (which is bad)\nThis undesirable effect is caused by the Laplace approximation used\ninternally by GPC.\n\nThe second figure shows the log-marginal-likelihood for different choices of\nthe kernel\'s hyperparameters, highlighting the two choices of the\nhyperparameters used in the first figure by black dots.\n""""""\nprint(__doc__)\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n\n\n# Generate data\ntrain_size = 50\nrng = np.random.RandomState(0)\nX = rng.uniform(0, 5, 100)[:, np.newaxis]\ny = np.array(X[:, 0] > 2.5, dtype=int)\n\n# Specify Gaussian Processes with fixed and optimized hyperparameters\ngp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0),\n                                   optimizer=None)\ngp_fix.fit(X[:train_size], y[:train_size])\n\ngp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\ngp_opt.fit(X[:train_size], y[:train_size])\n\nprint(""Log Marginal Likelihood (initial): %.3f""\n      % gp_fix.log_marginal_likelihood(gp_fix.kernel_.theta))\nprint(""Log Marginal Likelihood (optimized): %.3f""\n      % gp_opt.log_marginal_likelihood(gp_opt.kernel_.theta))\n\nprint(""Accuracy: %.3f (initial) %.3f (optimized)""\n      % (accuracy_score(y[:train_size], gp_fix.predict(X[:train_size])),\n         accuracy_score(y[:train_size], gp_opt.predict(X[:train_size]))))\nprint(""Log-loss: %.3f (initial) %.3f (optimized)""\n      % (log_loss(y[:train_size], gp_fix.predict_proba(X[:train_size])[:, 1]),\n         log_loss(y[:train_size], gp_opt.predict_proba(X[:train_size])[:, 1])))\n\n\n# Plot posteriors\nplt.figure()\nplt.scatter(X[:train_size, 0], y[:train_size], c=\'k\', label=""Train data"",\n            edgecolors=(0, 0, 0))\nplt.scatter(X[train_size:, 0], y[train_size:], c=\'g\', label=""Test data"",\n            edgecolors=(0, 0, 0))\nX_ = np.linspace(0, 5, 100)\nplt.plot(X_, gp_fix.predict_proba(X_[:, np.newaxis])[:, 1], \'r\',\n         label=""Initial kernel: %s"" % gp_fix.kernel_)\nplt.plot(X_, gp_opt.predict_proba(X_[:, np.newaxis])[:, 1], \'b\',\n         label=""Optimized kernel: %s"" % gp_opt.kernel_)\nplt.xlabel(""Feature"")\nplt.ylabel(""Class 1 probability"")\nplt.xlim(0, 5)\nplt.ylim(-0.25, 1.5)\nplt.legend(loc=""best"")\n\n# Plot LML landscape\nplt.figure()\ntheta0 = np.logspace(0, 8, 30)\ntheta1 = np.logspace(-1, 1, 29)\nTheta0, Theta1 = np.meshgrid(theta0, theta1)\nLML = [[gp_opt.log_marginal_likelihood(np.log([Theta0[i, j], Theta1[i, j]]))\n        for i in range(Theta0.shape[0])] for j in range(Theta0.shape[1])]\nLML = np.array(LML).T\nplt.plot(np.exp(gp_fix.kernel_.theta)[0], np.exp(gp_fix.kernel_.theta)[1],\n         \'ko\', zorder=10)\nplt.plot(np.exp(gp_opt.kernel_.theta)[0], np.exp(gp_opt.kernel_.theta)[1],\n         \'ko\', zorder=10)\nplt.pcolor(Theta0, Theta1, LML)\nplt.xscale(""log"")\nplt.yscale(""log"")\nplt.colorbar()\nplt.xlabel(""Magnitude"")\nplt.ylabel(""Length-scale"")\nplt.title(""Log-marginal-likelihood"")\n\nplt.show()\n'"
scikit-learn/_downloads/6eef77c20d72e6f373ed597b95c57338/plot_swissroll.py,0,"b'""""""\n===================================\nSwiss Roll reduction with LLE\n===================================\n\nAn illustration of Swiss Roll reduction\nwith locally linear embedding\n""""""\n\n# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n# License: BSD 3 clause (C) INRIA 2011\n\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\n\n# This import is needed to modify the way figure behaves\nfrom mpl_toolkits.mplot3d import Axes3D\nAxes3D\n\n#----------------------------------------------------------------------\n# Locally linear embedding of the swiss roll\n\nfrom sklearn import manifold, datasets\nX, color = datasets.make_swiss_roll(n_samples=1500)\n\nprint(""Computing LLE embedding"")\nX_r, err = manifold.locally_linear_embedding(X, n_neighbors=12,\n                                             n_components=2)\nprint(""Done. Reconstruction error: %g"" % err)\n\n#----------------------------------------------------------------------\n# Plot result\n\nfig = plt.figure()\n\nax = fig.add_subplot(211, projection=\'3d\')\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n\nax.set_title(""Original data"")\nax = fig.add_subplot(212)\nax.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.axis(\'tight\')\nplt.xticks([]), plt.yticks([])\nplt.title(\'Projected data\')\nplt.show()\n'"
scikit-learn/_downloads/6f18eee52f89fb7b9e2952e1fc8bf8fe/plot_pca_iris.py,2,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nPCA example with Iris Data-set\n=========================================================\n\nPrincipal Component Analysis applied to the Iris dataset.\n\nSee `here <https://en.wikipedia.org/wiki/Iris_flower_data_set>`_ for more\ninformation on this dataset.\n\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\nfrom sklearn import decomposition\nfrom sklearn import datasets\n\nnp.random.seed(5)\n\ncenters = [[1, 1], [-1, -1], [1, -1]]\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nfig = plt.figure(1, figsize=(4, 3))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\npca = decomposition.PCA(n_components=3)\npca.fit(X)\nX = pca.transform(X)\n\nfor name, label in [(\'Setosa\', 0), (\'Versicolour\', 1), (\'Virginica\', 2)]:\n    ax.text3D(X[y == label, 0].mean(),\n              X[y == label, 1].mean() + 1.5,\n              X[y == label, 2].mean(), name,\n              horizontalalignment=\'center\',\n              bbox=dict(alpha=.5, edgecolor=\'w\', facecolor=\'w\'))\n# Reorder the labels to have colors matching the cluster results\ny = np.choose(y, [1, 2, 0]).astype(np.float)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,\n           edgecolor=\'k\')\n\nax.w_xaxis.set_ticklabels([])\nax.w_yaxis.set_ticklabels([])\nax.w_zaxis.set_ticklabels([])\n\nplt.show()\n'"
scikit-learn/_downloads/6f8d0ceb357327c891808f8366cccc00/plot_sgd_loss_functions.py,5,"b'""""""\n==========================\nSGD: convex loss functions\n==========================\n\nA plot that compares the various convex loss functions supported by\n:class:`sklearn.linear_model.SGDClassifier` .\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef modified_huber_loss(y_true, y_pred):\n    z = y_pred * y_true\n    loss = -4 * z\n    loss[z >= -1] = (1 - z[z >= -1]) ** 2\n    loss[z >= 1.] = 0\n    return loss\n\n\nxmin, xmax = -4, 4\nxx = np.linspace(xmin, xmax, 100)\nlw = 2\nplt.plot([xmin, 0, 0, xmax], [1, 1, 0, 0], color=\'gold\', lw=lw,\n         label=""Zero-one loss"")\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0), color=\'teal\', lw=lw,\n         label=""Hinge loss"")\nplt.plot(xx, -np.minimum(xx, 0), color=\'yellowgreen\', lw=lw,\n         label=""Perceptron loss"")\nplt.plot(xx, np.log2(1 + np.exp(-xx)), color=\'cornflowerblue\', lw=lw,\n         label=""Log loss"")\nplt.plot(xx, np.where(xx < 1, 1 - xx, 0) ** 2, color=\'orange\', lw=lw,\n         label=""Squared hinge loss"")\nplt.plot(xx, modified_huber_loss(xx, 1), color=\'darkorchid\', lw=lw,\n         linestyle=\'--\', label=""Modified Huber loss"")\nplt.ylim((0, 8))\nplt.legend(loc=""upper right"")\nplt.xlabel(r""Decision function $f(x)$"")\nplt.ylabel(""$L(y=1, f(x))$"")\nplt.show()\n'"
scikit-learn/_downloads/703f13ad055abb274ac9be49c6b56b2a/plot_discretization_strategies.py,7,"b'# -*- coding: utf-8 -*-\n""""""\n==========================================================\nDemonstrating the different strategies of KBinsDiscretizer\n==========================================================\n\nThis example presents the different strategies implemented in KBinsDiscretizer:\n\n- \'uniform\': The discretization is uniform in each feature, which means that\n  the bin widths are constant in each dimension.\n- quantile\': The discretization is done on the quantiled values, which means\n  that each bin has approximately the same number of samples.\n- \'kmeans\': The discretization is based on the centroids of a KMeans clustering\n  procedure.\n\nThe plot shows the regions where the discretized encoding is constant.\n""""""\n\n# Author: Tom Dupr\xc3\xa9 la Tour\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.datasets import make_blobs\n\nprint(__doc__)\n\nstrategies = [\'uniform\', \'quantile\', \'kmeans\']\n\nn_samples = 200\ncenters_0 = np.array([[0, 0], [0, 5], [2, 4], [8, 8]])\ncenters_1 = np.array([[0, 0], [3, 1]])\n\n# construct the datasets\nrandom_state = 42\nX_list = [\n    np.random.RandomState(random_state).uniform(-3, 3, size=(n_samples, 2)),\n    make_blobs(n_samples=[n_samples // 10, n_samples * 4 // 10,\n                          n_samples // 10, n_samples * 4 // 10],\n               cluster_std=0.5, centers=centers_0,\n               random_state=random_state)[0],\n    make_blobs(n_samples=[n_samples // 5, n_samples * 4 // 5],\n               cluster_std=0.5, centers=centers_1,\n               random_state=random_state)[0],\n]\n\nfigure = plt.figure(figsize=(14, 9))\ni = 1\nfor ds_cnt, X in enumerate(X_list):\n\n    ax = plt.subplot(len(X_list), len(strategies) + 1, i)\n    ax.scatter(X[:, 0], X[:, 1], edgecolors=\'k\')\n    if ds_cnt == 0:\n        ax.set_title(""Input data"", size=14)\n\n    xx, yy = np.meshgrid(\n        np.linspace(X[:, 0].min(), X[:, 0].max(), 300),\n        np.linspace(X[:, 1].min(), X[:, 1].max(), 300))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    i += 1\n    # transform the dataset with KBinsDiscretizer\n    for strategy in strategies:\n        enc = KBinsDiscretizer(n_bins=4, encode=\'ordinal\', strategy=strategy)\n        enc.fit(X)\n        grid_encoded = enc.transform(grid)\n\n        ax = plt.subplot(len(X_list), len(strategies) + 1, i)\n\n        # horizontal stripes\n        horizontal = grid_encoded[:, 0].reshape(xx.shape)\n        ax.contourf(xx, yy, horizontal, alpha=.5)\n        # vertical stripes\n        vertical = grid_encoded[:, 1].reshape(xx.shape)\n        ax.contourf(xx, yy, vertical, alpha=.5)\n\n        ax.scatter(X[:, 0], X[:, 1], edgecolors=\'k\')\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(""strategy=\'%s\'"" % (strategy, ), size=14)\n\n        i += 1\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/70909f18c6e29e366d43ee4279dab5f8/plot_ica_vs_pca.py,6,"b'""""""\n==========================\nFastICA on 2D point clouds\n==========================\n\nThis example illustrates visually in the feature space a comparison by\nresults using two different component analysis techniques.\n\n:ref:`ICA` vs :ref:`PCA`.\n\nRepresenting ICA in the feature space gives the view of \'geometric ICA\':\nICA is an algorithm that finds directions in the feature space\ncorresponding to projections with high non-Gaussianity. These directions\nneed not be orthogonal in the original feature space, but they are\northogonal in the whitened feature space, in which all directions\ncorrespond to the same variance.\n\nPCA, on the other hand, finds orthogonal directions in the raw feature\nspace that correspond to directions accounting for maximum variance.\n\nHere we simulate independent sources using a highly non-Gaussian\nprocess, 2 student T with a low number of degrees of freedom (top left\nfigure). We mix them to create observations (top right figure).\nIn this raw observation space, directions identified by PCA are\nrepresented by orange vectors. We represent the signal in the PCA space,\nafter whitening by the variance corresponding to the PCA vectors (lower\nleft). Running ICA corresponds to finding a rotation in this space to\nidentify the directions of largest non-Gaussianity (lower right).\n""""""\nprint(__doc__)\n\n# Authors: Alexandre Gramfort, Gael Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA, FastICA\n\n# #############################################################################\n# Generate sample data\nrng = np.random.RandomState(42)\nS = rng.standard_t(1.5, size=(20000, 2))\nS[:, 0] *= 2.\n\n# Mix data\nA = np.array([[1, 1], [0, 2]])  # Mixing matrix\n\nX = np.dot(S, A.T)  # Generate observations\n\npca = PCA()\nS_pca_ = pca.fit(X).transform(X)\n\nica = FastICA(random_state=rng)\nS_ica_ = ica.fit(X).transform(X)  # Estimate the sources\n\nS_ica_ /= S_ica_.std(axis=0)\n\n\n# #############################################################################\n# Plot results\n\ndef plot_samples(S, axis_list=None):\n    plt.scatter(S[:, 0], S[:, 1], s=2, marker=\'o\', zorder=10,\n                color=\'steelblue\', alpha=0.5)\n    if axis_list is not None:\n        colors = [\'orange\', \'red\']\n        for color, axis in zip(colors, axis_list):\n            axis /= axis.std()\n            x_axis, y_axis = axis\n            # Trick to get legend to work\n            plt.plot(0.1 * x_axis, 0.1 * y_axis, linewidth=2, color=color)\n            plt.quiver(0, 0, x_axis, y_axis, zorder=11, width=0.01, scale=6,\n                       color=color)\n\n    plt.hlines(0, -3, 3)\n    plt.vlines(0, -3, 3)\n    plt.xlim(-3, 3)\n    plt.ylim(-3, 3)\n    plt.xlabel(\'x\')\n    plt.ylabel(\'y\')\n\nplt.figure()\nplt.subplot(2, 2, 1)\nplot_samples(S / S.std())\nplt.title(\'True Independent Sources\')\n\naxis_list = [pca.components_.T, ica.mixing_]\nplt.subplot(2, 2, 2)\nplot_samples(X / np.std(X), axis_list=axis_list)\nlegend = plt.legend([\'PCA\', \'ICA\'], loc=\'upper right\')\nlegend.set_zorder(100)\n\nplt.title(\'Observations\')\n\nplt.subplot(2, 2, 3)\nplot_samples(S_pca_ / np.std(S_pca_, axis=0))\nplt.title(\'PCA recovered signals\')\n\nplt.subplot(2, 2, 4)\nplot_samples(S_ica_ / np.std(S_ica_))\nplt.title(\'ICA recovered signals\')\n\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.36)\nplt.show()\n'"
scikit-learn/_downloads/7336da14a38a187a73d7d7dbc1400876/plot_logistic_path.py,3,"b'#!/usr/bin/env python\n""""""\n==============================================\nRegularization path of L1- Logistic Regression\n==============================================\n\n\nTrain l1-penalized logistic regression models on a binary classification\nproblem derived from the Iris dataset.\n\nThe models are ordered from strongest regularized to least regularized. The 4\ncoefficients of the models are collected and plotted as a ""regularization\npath"": on the left-hand side of the figure (strong regularizers), all the\ncoefficients are exactly 0. When regularization gets progressively looser,\ncoefficients can get non-zero values one after the other.\n\nHere we choose the SAGA solver because it can efficiently optimize for the\nLogistic Regression loss with a non-smooth, sparsity inducing l1 penalty.\n\nAlso note that we set a low value for the tolerance to make sure that the model\nhas converged before collecting the coefficients.\n\nWe also use warm_start=True which means that the coefficients of the models are\nreused to initialize the next model fit to speed-up the computation of the\nfull-path.\n\n""""""\nprint(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\nfrom sklearn.svm import l1_min_c\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX = X[y != 2]\ny = y[y != 2]\n\nX /= X.max()  # Normalize X to speed-up convergence\n\n# #############################################################################\n# Demo path functions\n\ncs = l1_min_c(X, y, loss=\'log\') * np.logspace(0, 7, 16)\n\n\nprint(""Computing regularization path ..."")\nstart = time()\nclf = linear_model.LogisticRegression(penalty=\'l1\', solver=\'saga\',\n                                      tol=1e-6, max_iter=int(1e6),\n                                      warm_start=True)\ncoefs_ = []\nfor c in cs:\n    clf.set_params(C=c)\n    clf.fit(X, y)\n    coefs_.append(clf.coef_.ravel().copy())\nprint(""This took %0.3fs"" % (time() - start))\n\ncoefs_ = np.array(coefs_)\nplt.plot(np.log10(cs), coefs_, marker=\'o\')\nymin, ymax = plt.ylim()\nplt.xlabel(\'log(C)\')\nplt.ylabel(\'Coefficients\')\nplt.title(\'Logistic Regression Path\')\nplt.axis(\'tight\')\nplt.show()\n'"
scikit-learn/_downloads/7360c8c22efd626719261a646c01a40a/plot_iris_logistic.py,2,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nLogistic Regression 3-class Classifier\n=========================================================\n\nShow below is a logistic-regression classifiers decision boundaries on the\nfirst two dimensions (sepal length and width) of the `iris\n<https://en.wikipedia.org/wiki/Iris_flower_data_set>`_ dataset. The datapoints\nare colored according to their labels.\n\n""""""\nprint(__doc__)\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\nlogreg = LogisticRegression(C=1e5)\n\n# Create an instance of Logistic Regression Classifier and fit the data.\nlogreg.fit(X, Y)\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nh = .02  # step size in the mesh\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors=\'k\', cmap=plt.cm.Paired)\nplt.xlabel(\'Sepal length\')\nplt.ylabel(\'Sepal width\')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n'"
scikit-learn/_downloads/749c7d8845202e936290238fa6c6f455/plot_ard.py,14,"b'""""""\n==================================================\nAutomatic Relevance Determination Regression (ARD)\n==================================================\n\nFit regression model with Bayesian Ridge Regression.\n\nSee :ref:`bayesian_ridge_regression` for more information on the regressor.\n\nCompared to the OLS (ordinary least squares) estimator, the coefficient\nweights are slightly shifted toward zeros, which stabilises them.\n\nThe histogram of the estimated weights is very peaked, as a sparsity-inducing\nprior is implied on the weights.\n\nThe estimation of the model is done by iteratively maximizing the\nmarginal log-likelihood of the observations.\n\nWe also plot predictions and uncertainties for ARD\nfor one dimensional regression using polynomial feature expansion.\nNote the uncertainty starts going up on the right side of the plot.\nThis is because these test samples are outside of the range of the training\nsamples.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import ARDRegression, LinearRegression\n\n# #############################################################################\n# Generating simulated data with Gaussian weights\n\n# Parameters of the example\nnp.random.seed(0)\nn_samples, n_features = 100, 100\n# Create Gaussian data\nX = np.random.randn(n_samples, n_features)\n# Create weights with a precision lambda_ of 4.\nlambda_ = 4.\nw = np.zeros(n_features)\n# Only keep 10 weights of interest\nrelevant_features = np.random.randint(0, n_features, 10)\nfor i in relevant_features:\n    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n# Create noise with a precision alpha of 50.\nalpha_ = 50.\nnoise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n# Create the target\ny = np.dot(X, w) + noise\n\n# #############################################################################\n# Fit the ARD Regression\nclf = ARDRegression(compute_score=True)\nclf.fit(X, y)\n\nols = LinearRegression()\nols.fit(X, y)\n\n# #############################################################################\n# Plot the true weights, the estimated weights, the histogram of the\n# weights, and predictions with standard deviations\nplt.figure(figsize=(6, 5))\nplt.title(""Weights of the model"")\nplt.plot(clf.coef_, color=\'darkblue\', linestyle=\'-\', linewidth=2,\n         label=""ARD estimate"")\nplt.plot(ols.coef_, color=\'yellowgreen\', linestyle=\':\', linewidth=2,\n         label=""OLS estimate"")\nplt.plot(w, color=\'orange\', linestyle=\'-\', linewidth=2, label=""Ground truth"")\nplt.xlabel(""Features"")\nplt.ylabel(""Values of the weights"")\nplt.legend(loc=1)\n\nplt.figure(figsize=(6, 5))\nplt.title(""Histogram of the weights"")\nplt.hist(clf.coef_, bins=n_features, color=\'navy\', log=True)\nplt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),\n            color=\'gold\', marker=\'o\', label=""Relevant features"")\nplt.ylabel(""Features"")\nplt.xlabel(""Values of the weights"")\nplt.legend(loc=1)\n\nplt.figure(figsize=(6, 5))\nplt.title(""Marginal log-likelihood"")\nplt.plot(clf.scores_, color=\'navy\', linewidth=2)\nplt.ylabel(""Score"")\nplt.xlabel(""Iterations"")\n\n\n# Plotting some predictions for polynomial regression\ndef f(x, noise_amount):\n    y = np.sqrt(x) * np.sin(x)\n    noise = np.random.normal(0, 1, len(x))\n    return y + noise_amount * noise\n\n\ndegree = 10\nX = np.linspace(0, 10, 100)\ny = f(X, noise_amount=1)\nclf_poly = ARDRegression(threshold_lambda=1e5)\nclf_poly.fit(np.vander(X, degree), y)\n\nX_plot = np.linspace(0, 11, 25)\ny_plot = f(X_plot, noise_amount=0)\ny_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)\nplt.figure(figsize=(6, 5))\nplt.errorbar(X_plot, y_mean, y_std, color=\'navy\',\n             label=""Polynomial ARD"", linewidth=2)\nplt.plot(X_plot, y_plot, color=\'gold\', linewidth=2,\n         label=""Ground Truth"")\nplt.ylabel(""Output y"")\nplt.xlabel(""Feature X"")\nplt.legend(loc=""lower left"")\nplt.show()\n'"
scikit-learn/_downloads/74fc266a8bb1d918ae5faf607cad4b17/plot_t_sne_perplexity.py,3,"b'""""""\n=============================================================================\nt-SNE: The effect of various perplexity values on the shape\n=============================================================================\n\nAn illustration of t-SNE on the two concentric circles and the S-curve\ndatasets for different perplexity values.\n\nWe observe a tendency towards clearer shapes as the preplexity value increases.\n\nThe size, the distance and the shape of clusters may vary upon initialization,\nperplexity values and does not always convey a meaning.\n\nAs shown below, t-SNE for higher perplexities finds meaningful topology of\ntwo concentric circles, however the size and the distance of the circles varies\nslightly from the original. Contrary to the two circles dataset, the shapes\nvisually diverge from S-curve topology on the S-curve dataset even for\nlarger perplexity values.\n\nFor further details, ""How to Use t-SNE Effectively""\nhttps://distill.pub/2016/misread-tsne/ provides a good discussion of the\neffects of various parameters, as well as interactive plots to explore\nthose effects.\n""""""\n\n# Author: Narine Kokhlikyan <narine@slice.com>\n# License: BSD\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom matplotlib.ticker import NullFormatter\nfrom sklearn import manifold, datasets\nfrom time import time\n\nn_samples = 300\nn_components = 2\n(fig, subplots) = plt.subplots(3, 5, figsize=(15, 8))\nperplexities = [5, 30, 50, 100]\n\nX, y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n\nred = y == 0\ngreen = y == 1\n\nax = subplots[0][0]\nax.scatter(X[red, 0], X[red, 1], c=""r"")\nax.scatter(X[green, 0], X[green, 1], c=""g"")\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\nfor i, perplexity in enumerate(perplexities):\n    ax = subplots[0][i + 1]\n\n    t0 = time()\n    tsne = manifold.TSNE(n_components=n_components, init=\'random\',\n                         random_state=0, perplexity=perplexity)\n    Y = tsne.fit_transform(X)\n    t1 = time()\n    print(""circles, perplexity=%d in %.2g sec"" % (perplexity, t1 - t0))\n    ax.set_title(""Perplexity=%d"" % perplexity)\n    ax.scatter(Y[red, 0], Y[red, 1], c=""r"")\n    ax.scatter(Y[green, 0], Y[green, 1], c=""g"")\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    ax.axis(\'tight\')\n\n# Another example using s-curve\nX, color = datasets.make_s_curve(n_samples, random_state=0)\n\nax = subplots[1][0]\nax.scatter(X[:, 0], X[:, 2], c=color)\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\n\nfor i, perplexity in enumerate(perplexities):\n    ax = subplots[1][i + 1]\n\n    t0 = time()\n    tsne = manifold.TSNE(n_components=n_components, init=\'random\',\n                         random_state=0, perplexity=perplexity)\n    Y = tsne.fit_transform(X)\n    t1 = time()\n    print(""S-curve, perplexity=%d in %.2g sec"" % (perplexity, t1 - t0))\n\n    ax.set_title(""Perplexity=%d"" % perplexity)\n    ax.scatter(Y[:, 0], Y[:, 1], c=color)\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    ax.axis(\'tight\')\n\n\n# Another example using a 2D uniform grid\nx = np.linspace(0, 1, int(np.sqrt(n_samples)))\nxx, yy = np.meshgrid(x, x)\nX = np.hstack([\n    xx.ravel().reshape(-1, 1),\n    yy.ravel().reshape(-1, 1),\n])\ncolor = xx.ravel()\nax = subplots[2][0]\nax.scatter(X[:, 0], X[:, 1], c=color)\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\n\nfor i, perplexity in enumerate(perplexities):\n    ax = subplots[2][i + 1]\n\n    t0 = time()\n    tsne = manifold.TSNE(n_components=n_components, init=\'random\',\n                         random_state=0, perplexity=perplexity)\n    Y = tsne.fit_transform(X)\n    t1 = time()\n    print(""uniform grid, perplexity=%d in %.2g sec"" % (perplexity, t1 - t0))\n\n    ax.set_title(""Perplexity=%d"" % perplexity)\n    ax.scatter(Y[:, 0], Y[:, 1], c=color)\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    ax.axis(\'tight\')\n\n\nplt.show()\n'"
scikit-learn/_downloads/7787a3921674a5352271fb7623a65003/plot_gmm_pdf.py,10,"b'""""""\n=========================================\nDensity Estimation for a Gaussian mixture\n=========================================\n\nPlot the density estimation of a mixture of two Gaussians. Data is\ngenerated from two Gaussians with different centers and covariance\nmatrices.\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\nfrom sklearn import mixture\n\nn_samples = 300\n\n# generate random sample, two components\nnp.random.seed(0)\n\n# generate spherical data centered on (20, 20)\nshifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])\n\n# generate zero centered stretched Gaussian data\nC = np.array([[0., -0.7], [3.5, .7]])\nstretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)\n\n# concatenate the two datasets into the final training set\nX_train = np.vstack([shifted_gaussian, stretched_gaussian])\n\n# fit a Gaussian Mixture Model with two components\nclf = mixture.GaussianMixture(n_components=2, covariance_type=\'full\')\nclf.fit(X_train)\n\n# display predicted scores by the model as a contour plot\nx = np.linspace(-20., 30.)\ny = np.linspace(-20., 40.)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)\nZ = Z.reshape(X.shape)\n\nCS = plt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),\n                 levels=np.logspace(0, 3, 10))\nCB = plt.colorbar(CS, shrink=0.8, extend=\'both\')\nplt.scatter(X_train[:, 0], X_train[:, 1], .8)\n\nplt.title(\'Negative log-likelihood predicted by a GMM\')\nplt.axis(\'tight\')\nplt.show()\n'"
scikit-learn/_downloads/77ef5ec6aff4ab24e8b1ab80d925eb79/plot_nca_illustration.py,3,"b'""""""\n=============================================\nNeighborhood Components Analysis Illustration\n=============================================\n\nThis example illustrates a learned distance metric that maximizes\nthe nearest neighbors classification accuracy. It provides a visual\nrepresentation of this metric compared to the original point\nspace. Please refer to the :ref:`User Guide <nca>` for more information.\n""""""\n\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import NeighborhoodComponentsAnalysis\nfrom matplotlib import cm\nfrom sklearn.utils.fixes import logsumexp\n\nprint(__doc__)\n\n##############################################################################\n# Original points\n# ---------------\n# First we create a data set of 9 samples from 3 classes, and plot the points\n# in the original space. For this example, we focus on the classification of\n# point no. 3. The thickness of a link between point no. 3 and another point\n# is proportional to their distance.\n\nX, y = make_classification(n_samples=9, n_features=2, n_informative=2,\n                           n_redundant=0, n_classes=3, n_clusters_per_class=1,\n                           class_sep=1.0, random_state=0)\n\nplt.figure(1)\nax = plt.gca()\nfor i in range(X.shape[0]):\n    ax.text(X[i, 0], X[i, 1], str(i), va=\'center\', ha=\'center\')\n    ax.scatter(X[i, 0], X[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax.set_title(""Original points"")\nax.axes.get_xaxis().set_visible(False)\nax.axes.get_yaxis().set_visible(False)\nax.axis(\'equal\')  # so that boundaries are displayed correctly as circles\n\n\ndef link_thickness_i(X, i):\n    diff_embedded = X[i] - X\n    dist_embedded = np.einsum(\'ij,ij->i\', diff_embedded,\n                              diff_embedded)\n    dist_embedded[i] = np.inf\n\n    # compute exponentiated distances (use the log-sum-exp trick to\n    # avoid numerical instabilities\n    exp_dist_embedded = np.exp(-dist_embedded -\n                               logsumexp(-dist_embedded))\n    return exp_dist_embedded\n\n\ndef relate_point(X, i, ax):\n    pt_i = X[i]\n    for j, pt_j in enumerate(X):\n        thickness = link_thickness_i(X, i)\n        if i != j:\n            line = ([pt_i[0], pt_j[0]], [pt_i[1], pt_j[1]])\n            ax.plot(*line, c=cm.Set1(y[j]),\n                    linewidth=5*thickness[j])\n\n\ni = 3\nrelate_point(X, i, ax)\nplt.show()\n\n##############################################################################\n# Learning an embedding\n# ---------------------\n# We use :class:`~sklearn.neighbors.NeighborhoodComponentsAnalysis` to learn an\n# embedding and plot the points after the transformation. We then take the\n# embedding and find the nearest neighbors.\n\nnca = NeighborhoodComponentsAnalysis(max_iter=30, random_state=0)\nnca = nca.fit(X, y)\n\nplt.figure(2)\nax2 = plt.gca()\nX_embedded = nca.transform(X)\nrelate_point(X_embedded, i, ax2)\n\nfor i in range(len(X)):\n    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i),\n             va=\'center\', ha=\'center\')\n    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[[i]]),\n                alpha=0.4)\n\nax2.set_title(""NCA embedding"")\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\nax2.axis(\'equal\')\nplt.show()\n'"
scikit-learn/_downloads/785080f04875d35c2d0306842392d7d4/plot_rbf_parameters.py,8,"b'\'\'\'\n==================\nRBF SVM parameters\n==================\n\nThis example illustrates the effect of the parameters ``gamma`` and ``C`` of\nthe Radial Basis Function (RBF) kernel SVM.\n\nIntuitively, the ``gamma`` parameter defines how far the influence of a single\ntraining example reaches, with low values meaning \'far\' and high values meaning\n\'close\'. The ``gamma`` parameters can be seen as the inverse of the radius of\ninfluence of samples selected by the model as support vectors.\n\nThe ``C`` parameter trades off correct classification of training examples\nagainst maximization of the decision function\'s margin. For larger values of\n``C``, a smaller margin will be accepted if the decision function is better at\nclassifying all training points correctly. A lower ``C`` will encourage a\nlarger margin, therefore a simpler decision function, at the cost of training\naccuracy. In other words``C`` behaves as a regularization parameter in the\nSVM.\n\nThe first plot is a visualization of the decision function for a variety of\nparameter values on a simplified classification problem involving only 2 input\nfeatures and 2 possible target classes (binary classification). Note that this\nkind of plot is not possible to do for problems with more features or target\nclasses.\n\nThe second plot is a heatmap of the classifier\'s cross-validation accuracy as a\nfunction of ``C`` and ``gamma``. For this example we explore a relatively large\ngrid for illustration purposes. In practice, a logarithmic grid from\n:math:`10^{-3}` to :math:`10^3` is usually sufficient. If the best parameters\nlie on the boundaries of the grid, it can be extended in that direction in a\nsubsequent search.\n\nNote that the heat map plot has a special colorbar with a midpoint value close\nto the score values of the best performing models so as to make it easy to tell\nthem apart in the blink of an eye.\n\nThe behavior of the model is very sensitive to the ``gamma`` parameter. If\n``gamma`` is too large, the radius of the area of influence of the support\nvectors only includes the support vector itself and no amount of\nregularization with ``C`` will be able to prevent overfitting.\n\nWhen ``gamma`` is very small, the model is too constrained and cannot capture\nthe complexity or ""shape"" of the data. The region of influence of any selected\nsupport vector would include the whole training set. The resulting model will\nbehave similarly to a linear model with a set of hyperplanes that separate the\ncenters of high density of any pair of two classes.\n\nFor intermediate values, we can see on the second plot that good models can\nbe found on a diagonal of ``C`` and ``gamma``. Smooth models (lower ``gamma``\nvalues) can be made more complex by increasing the importance of classifying\neach point correctly (larger ``C`` values) hence the diagonal of good\nperforming models.\n\nFinally one can also observe that for some intermediate values of ``gamma`` we\nget equally performing models when ``C`` becomes very large: it is not\nnecessary to regularize by enforcing a larger margin. The radius of the RBF\nkernel alone acts as a good structural regularizer. In practice though it\nmight still be interesting to simplify the decision function with a lower\nvalue of ``C`` so as to favor models that use less memory and that are faster\nto predict.\n\nWe should also note that small differences in scores results from the random\nsplits of the cross-validation procedure. Those spurious variations can be\nsmoothed out by increasing the number of CV iterations ``n_splits`` at the\nexpense of compute time. Increasing the value number of ``C_range`` and\n``gamma_range`` steps will increase the resolution of the hyper-parameter heat\nmap.\n\n\'\'\'\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Utility function to move the midpoint of a colormap to be around\n# the values of interest.\n\nclass MidpointNormalize(Normalize):\n\n    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n        self.midpoint = midpoint\n        Normalize.__init__(self, vmin, vmax, clip)\n\n    def __call__(self, value, clip=None):\n        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n        return np.ma.masked_array(np.interp(value, x, y))\n\n# #############################################################################\n# Load and prepare data set\n#\n# dataset for grid search\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Dataset for decision function visualization: we only keep the first two\n# features in X and sub-sample the dataset to keep only 2 classes and\n# make it a binary classification problem.\n\nX_2d = X[:, :2]\nX_2d = X_2d[y > 0]\ny_2d = y[y > 0]\ny_2d -= 1\n\n# It is usually a good idea to scale the data for SVM training.\n# We are cheating a bit in this example in scaling all of the data,\n# instead of fitting the transformation on the training set and\n# just applying it on the test set.\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_2d = scaler.fit_transform(X_2d)\n\n# #############################################################################\n# Train classifiers\n#\n# For an initial search, a logarithmic grid with basis\n# 10 is often helpful. Using a basis of 2, a finer\n# tuning can be achieved but at a much higher cost.\n\nC_range = np.logspace(-2, 10, 13)\ngamma_range = np.logspace(-9, 3, 13)\nparam_grid = dict(gamma=gamma_range, C=C_range)\ncv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\ngrid.fit(X, y)\n\nprint(""The best parameters are %s with a score of %0.2f""\n      % (grid.best_params_, grid.best_score_))\n\n# Now we need to fit a classifier for all parameters in the 2d version\n# (we use a smaller set of parameters here because it takes a while to train)\n\nC_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = SVC(C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))\n\n# #############################################################################\n# Visualization\n#\n# draw visualization of parameter effects\n\nplt.figure(figsize=(8, 6))\nxx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\nfor (k, (C, gamma, clf)) in enumerate(classifiers):\n    # evaluate decision function in a grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # visualize decision function for these parameters\n    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n    plt.title(""gamma=10^%d, C=10^%d"" % (np.log10(gamma), np.log10(C)),\n              size=\'medium\')\n\n    # visualize parameter\'s effect on decision function\n    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r,\n                edgecolors=\'k\')\n    plt.xticks(())\n    plt.yticks(())\n    plt.axis(\'tight\')\n\nscores = grid.cv_results_[\'mean_test_score\'].reshape(len(C_range),\n                                                     len(gamma_range))\n\n# Draw heatmap of the validation accuracy as a function of gamma and C\n#\n# The score are encoded as colors with the hot colormap which varies from dark\n# red to bright yellow. As the most interesting scores are all located in the\n# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\n# as to make it easier to visualize the small variations of score values in the\n# interesting range while not brutally collapsing all the low score values to\n# the same color.\n\nplt.figure(figsize=(8, 6))\nplt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\nplt.imshow(scores, interpolation=\'nearest\', cmap=plt.cm.hot,\n           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\nplt.xlabel(\'gamma\')\nplt.ylabel(\'C\')\nplt.colorbar()\nplt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\nplt.yticks(np.arange(len(C_range)), C_range)\nplt.title(\'Validation accuracy\')\nplt.show()\n'"
scikit-learn/_downloads/789b72c0a73600eeda779169a396384e/plot_learning_curve.py,8,"b'""""""\n========================\nPlotting Learning Curves\n========================\nIn the first column, first row the learning curve of a naive Bayes classifier\nis shown for the digits dataset. Note that the training score and the\ncross-validation score are both not very good at the end. However, the shape\nof the curve can be found in more complex datasets very often: the training\nscore is very high at the beginning and decreases and the cross-validation\nscore is very low at the beginning and increases. In the second column, first\nrow we see the learning curve of an SVM with RBF kernel. We can see clearly\nthat the training score is still around the maximum and the validation score\ncould be increased with more training samples. The plots in the second row\nshow the times required by the models to train with various sizes of training\ndataset. The plots in the third row show how much time was required to train\nthe models for each training sizes.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    """"""\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the ""fit"" and ""predict"" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    """"""\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(""Training examples"")\n    axes[0].set_ylabel(""Score"")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=""r"")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=""g"")\n    axes[0].plot(train_sizes, train_scores_mean, \'o-\', color=""r"",\n                 label=""Training score"")\n    axes[0].plot(train_sizes, test_scores_mean, \'o-\', color=""g"",\n                 label=""Cross-validation score"")\n    axes[0].legend(loc=""best"")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, \'o-\')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(""Training examples"")\n    axes[1].set_ylabel(""fit_times"")\n    axes[1].set_title(""Scalability of the model"")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, \'o-\')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(""fit_times"")\n    axes[2].set_ylabel(""Score"")\n    axes[2].set_title(""Performance of the model"")\n\n    return plt\n\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 15))\n\nX, y = load_digits(return_X_y=True)\n\ntitle = ""Learning Curves (Naive Bayes)""\n# Cross validation with 100 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = GaussianNB()\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 0], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\ntitle = r""Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)""\n# SVC is more expensive so we do a lower number of CV iterations:\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nestimator = SVC(gamma=0.001)\nplot_learning_curve(estimator, title, X, y, axes=axes[:, 1], ylim=(0.7, 1.01),\n                    cv=cv, n_jobs=4)\n\nplt.show()\n'"
scikit-learn/_downloads/78cff89f070c4f5c408c780a2c06b9ea/plot_sparse_coding.py,17,"b'""""""\n===========================================\nSparse coding with a precomputed dictionary\n===========================================\n\nTransform a signal as a sparse combination of Ricker wavelets. This example\nvisually compares different sparse coding methods using the\n:class:`sklearn.decomposition.SparseCoder` estimator. The Ricker (also known\nas Mexican hat or the second derivative of a Gaussian) is not a particularly\ngood kernel to represent piecewise constant signals like this one. It can\ntherefore be seen how much adding different widths of atoms matters and it\ntherefore motivates learning the dictionary to best fit your type of signals.\n\nThe richer dictionary on the right is not larger in size, heavier subsampling\nis performed in order to stay on the same order of magnitude.\n""""""\nprint(__doc__)\n\nfrom distutils.version import LooseVersion\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import SparseCoder\n\n\ndef ricker_function(resolution, center, width):\n    """"""Discrete sub-sampled Ricker (Mexican hat) wavelet""""""\n    x = np.linspace(0, resolution - 1, resolution)\n    x = ((2 / (np.sqrt(3 * width) * np.pi ** .25))\n         * (1 - (x - center) ** 2 / width ** 2)\n         * np.exp(-(x - center) ** 2 / (2 * width ** 2)))\n    return x\n\n\ndef ricker_matrix(width, resolution, n_components):\n    """"""Dictionary of Ricker (Mexican hat) wavelets""""""\n    centers = np.linspace(0, resolution - 1, n_components)\n    D = np.empty((n_components, resolution))\n    for i, center in enumerate(centers):\n        D[i] = ricker_function(resolution, center, width)\n    D /= np.sqrt(np.sum(D ** 2, axis=1))[:, np.newaxis]\n    return D\n\n\nresolution = 1024\nsubsampling = 3  # subsampling factor\nwidth = 100\nn_components = resolution // subsampling\n\n# Compute a wavelet dictionary\nD_fixed = ricker_matrix(width=width, resolution=resolution,\n                        n_components=n_components)\nD_multi = np.r_[tuple(ricker_matrix(width=w, resolution=resolution,\n                      n_components=n_components // 5)\n                for w in (10, 50, 100, 500, 1000))]\n\n# Generate a signal\ny = np.linspace(0, resolution - 1, resolution)\nfirst_quarter = y < resolution / 4\ny[first_quarter] = 3.\ny[np.logical_not(first_quarter)] = -1.\n\n# List the different sparse coding methods in the following format:\n# (title, transform_algorithm, transform_alpha,\n#  transform_n_nozero_coefs, color)\nestimators = [(\'OMP\', \'omp\', None, 15, \'navy\'),\n              (\'Lasso\', \'lasso_lars\', 2, None, \'turquoise\'), ]\nlw = 2\n# Avoid FutureWarning about default value change when numpy >= 1.14\nlstsq_rcond = None if LooseVersion(np.__version__) >= \'1.14\' else -1\n\nplt.figure(figsize=(13, 6))\nfor subplot, (D, title) in enumerate(zip((D_fixed, D_multi),\n                                         (\'fixed width\', \'multiple widths\'))):\n    plt.subplot(1, 2, subplot + 1)\n    plt.title(\'Sparse coding against %s dictionary\' % title)\n    plt.plot(y, lw=lw, linestyle=\'--\', label=\'Original signal\')\n    # Do a wavelet approximation\n    for title, algo, alpha, n_nonzero, color in estimators:\n        coder = SparseCoder(dictionary=D, transform_n_nonzero_coefs=n_nonzero,\n                            transform_alpha=alpha, transform_algorithm=algo)\n        x = coder.transform(y.reshape(1, -1))\n        density = len(np.flatnonzero(x))\n        x = np.ravel(np.dot(x, D))\n        squared_error = np.sum((y - x) ** 2)\n        plt.plot(x, color=color, lw=lw,\n                 label=\'%s: %s nonzero coefs,\\n%.2f error\'\n                 % (title, density, squared_error))\n\n    # Soft thresholding debiasing\n    coder = SparseCoder(dictionary=D, transform_algorithm=\'threshold\',\n                        transform_alpha=20)\n    x = coder.transform(y.reshape(1, -1))\n    _, idx = np.where(x != 0)\n    x[0, idx], _, _, _ = np.linalg.lstsq(D[idx, :].T, y, rcond=lstsq_rcond)\n    x = np.ravel(np.dot(x, D))\n    squared_error = np.sum((y - x) ** 2)\n    plt.plot(x, color=\'darkorange\', lw=lw,\n             label=\'Thresholding w/ debiasing:\\n%d nonzero coefs, %.2f error\'\n             % (len(idx), squared_error))\n    plt.axis(\'tight\')\n    plt.legend(shadow=False, loc=\'best\')\nplt.subplots_adjust(.04, .07, .97, .90, .09, .2)\nplt.show()\n'"
scikit-learn/_downloads/7b9a2ffae80e32616abc304f8da9eea1/plot_all_scaling.py,4,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\n=============================================================\nCompare the effect of different scalers on data with outliers\n=============================================================\n\nFeature 0 (median income in a block) and feature 5 (number of households) of\nthe `California housing dataset\n<https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html>`_ have very\ndifferent scales and contain some very large outliers. These two\ncharacteristics lead to difficulties to visualize the data and, more\nimportantly, they can degrade the predictive performance of many machine\nlearning algorithms. Unscaled data can also slow down or even prevent the\nconvergence of many gradient-based estimators.\n\nIndeed many estimators are designed with the assumption that each feature takes\nvalues close to zero or more importantly that all features vary on comparable\nscales. In particular, metric-based and gradient-based estimators often assume\napproximately standardized data (centered features with unit variances). A\nnotable exception are decision tree-based estimators that are robust to\narbitrary scaling of the data.\n\nThis example uses different scalers, transformers, and normalizers to bring the\ndata within a pre-defined range.\n\nScalers are linear (or more precisely affine) transformers and differ from each\nother in the way to estimate the parameters used to shift and scale each\nfeature.\n\n``QuantileTransformer`` provides non-linear transformations in which distances\nbetween marginal outliers and inliers are shrunk. ``PowerTransformer`` provides\nnon-linear transformations in which data is mapped to a normal distribution to\nstabilize variance and minimize skewness.\n\nUnlike the previous transformations, normalization refers to a per sample\ntransformation instead of a per feature transformation.\n\nThe following code is a bit verbose, feel free to jump directly to the analysis\nof the results_.\n\n""""""\n\n# Author:  Raghav RV <rvraghav93@gmail.com>\n#          Guillaume Lemaitre <g.lemaitre58@gmail.com>\n#          Thomas Unterthiner\n# License: BSD 3 clause\n\nimport numpy as np\n\nimport matplotlib as mpl\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\n\nfrom sklearn.datasets import fetch_california_housing\n\nprint(__doc__)\n\ndataset = fetch_california_housing()\nX_full, y_full = dataset.data, dataset.target\n\n# Take only 2 features to make visualization easier\n# Feature of 0 has a long tail distribution.\n# Feature 5 has a few but very large outliers.\n\nX = X_full[:, [0, 5]]\n\ndistributions = [\n    (\'Unscaled data\', X),\n    (\'Data after standard scaling\',\n        StandardScaler().fit_transform(X)),\n    (\'Data after min-max scaling\',\n        MinMaxScaler().fit_transform(X)),\n    (\'Data after max-abs scaling\',\n        MaxAbsScaler().fit_transform(X)),\n    (\'Data after robust scaling\',\n        RobustScaler(quantile_range=(25, 75)).fit_transform(X)),\n    (\'Data after power transformation (Yeo-Johnson)\',\n     PowerTransformer(method=\'yeo-johnson\').fit_transform(X)),\n    (\'Data after power transformation (Box-Cox)\',\n     PowerTransformer(method=\'box-cox\').fit_transform(X)),\n    (\'Data after quantile transformation (gaussian pdf)\',\n        QuantileTransformer(output_distribution=\'normal\')\n        .fit_transform(X)),\n    (\'Data after quantile transformation (uniform pdf)\',\n        QuantileTransformer(output_distribution=\'uniform\')\n        .fit_transform(X)),\n    (\'Data after sample-wise L2 normalizing\',\n        Normalizer().fit_transform(X)),\n]\n\n# scale the output between 0 and 1 for the colorbar\ny = minmax_scale(y_full)\n\n# plasma does not exist in matplotlib < 1.5\ncmap = getattr(cm, \'plasma_r\', cm.hot_r)\n\ndef create_axes(title, figsize=(16, 6)):\n    fig = plt.figure(figsize=figsize)\n    fig.suptitle(title)\n\n    # define the axis for the first plot\n    left, width = 0.1, 0.22\n    bottom, height = 0.1, 0.7\n    bottom_h = height + 0.15\n    left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.1]\n    rect_histy = [left_h, bottom, 0.05, height]\n\n    ax_scatter = plt.axes(rect_scatter)\n    ax_histx = plt.axes(rect_histx)\n    ax_histy = plt.axes(rect_histy)\n\n    # define the axis for the zoomed-in plot\n    left = width + left + 0.2\n    left_h = left + width + 0.02\n\n    rect_scatter = [left, bottom, width, height]\n    rect_histx = [left, bottom_h, width, 0.1]\n    rect_histy = [left_h, bottom, 0.05, height]\n\n    ax_scatter_zoom = plt.axes(rect_scatter)\n    ax_histx_zoom = plt.axes(rect_histx)\n    ax_histy_zoom = plt.axes(rect_histy)\n\n    # define the axis for the colorbar\n    left, width = width + left + 0.13, 0.01\n\n    rect_colorbar = [left, bottom, width, height]\n    ax_colorbar = plt.axes(rect_colorbar)\n\n    return ((ax_scatter, ax_histy, ax_histx),\n            (ax_scatter_zoom, ax_histy_zoom, ax_histx_zoom),\n            ax_colorbar)\n\n\ndef plot_distribution(axes, X, y, hist_nbins=50, title="""",\n                      x0_label="""", x1_label=""""):\n    ax, hist_X1, hist_X0 = axes\n\n    ax.set_title(title)\n    ax.set_xlabel(x0_label)\n    ax.set_ylabel(x1_label)\n\n    # The scatter plot\n    colors = cmap(y)\n    ax.scatter(X[:, 0], X[:, 1], alpha=0.5, marker=\'o\', s=5, lw=0, c=colors)\n\n    # Removing the top and the right spine for aesthetics\n    # make nice axis layout\n    ax.spines[\'top\'].set_visible(False)\n    ax.spines[\'right\'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines[\'left\'].set_position((\'outward\', 10))\n    ax.spines[\'bottom\'].set_position((\'outward\', 10))\n\n    # Histogram for axis X1 (feature 5)\n    hist_X1.set_ylim(ax.get_ylim())\n    hist_X1.hist(X[:, 1], bins=hist_nbins, orientation=\'horizontal\',\n                 color=\'grey\', ec=\'grey\')\n    hist_X1.axis(\'off\')\n\n    # Histogram for axis X0 (feature 0)\n    hist_X0.set_xlim(ax.get_xlim())\n    hist_X0.hist(X[:, 0], bins=hist_nbins, orientation=\'vertical\',\n                 color=\'grey\', ec=\'grey\')\n    hist_X0.axis(\'off\')\n\n###############################################################################\n# Two plots will be shown for each scaler/normalizer/transformer. The left\n# figure will show a scatter plot of the full data set while the right figure\n# will exclude the extreme values considering only 99 % of the data set,\n# excluding marginal outliers. In addition, the marginal distributions for each\n# feature will be shown on the side of the scatter plot.\n\n\ndef make_plot(item_idx):\n    title, X = distributions[item_idx]\n    ax_zoom_out, ax_zoom_in, ax_colorbar = create_axes(title)\n    axarr = (ax_zoom_out, ax_zoom_in)\n    plot_distribution(axarr[0], X, y, hist_nbins=200,\n                      x0_label=""Median Income"",\n                      x1_label=""Number of households"",\n                      title=""Full data"")\n\n    # zoom-in\n    zoom_in_percentile_range = (0, 99)\n    cutoffs_X0 = np.percentile(X[:, 0], zoom_in_percentile_range)\n    cutoffs_X1 = np.percentile(X[:, 1], zoom_in_percentile_range)\n\n    non_outliers_mask = (\n        np.all(X > [cutoffs_X0[0], cutoffs_X1[0]], axis=1) &\n        np.all(X < [cutoffs_X0[1], cutoffs_X1[1]], axis=1))\n    plot_distribution(axarr[1], X[non_outliers_mask], y[non_outliers_mask],\n                      hist_nbins=50,\n                      x0_label=""Median Income"",\n                      x1_label=""Number of households"",\n                      title=""Zoom-in"")\n\n    norm = mpl.colors.Normalize(y_full.min(), y_full.max())\n    mpl.colorbar.ColorbarBase(ax_colorbar, cmap=cmap,\n                              norm=norm, orientation=\'vertical\',\n                              label=\'Color mapping for values of y\')\n\n\n########################################################################\n# .. _results:\n#\n# Original data\n# -------------\n#\n# Each transformation is plotted showing two transformed features, with the\n# left plot showing the entire dataset, and the right zoomed-in to show the\n# dataset without the marginal outliers. A large majority of the samples are\n# compacted to a specific range, [0, 10] for the median income and [0, 6] for\n# the number of households. Note that there are some marginal outliers (some\n# blocks have more than 1200 households). Therefore, a specific pre-processing\n# can be very beneficial depending of the application. In the following, we\n# present some insights and behaviors of those pre-processing methods in the\n# presence of marginal outliers.\n\nmake_plot(0)\n\n#######################################################################\n# StandardScaler\n# --------------\n#\n# ``StandardScaler`` removes the mean and scales the data to unit variance.\n# However, the outliers have an influence when computing the empirical mean and\n# standard deviation which shrink the range of the feature values as shown in\n# the left figure below. Note in particular that because the outliers on each\n# feature have different magnitudes, the spread of the transformed data on\n# each feature is very different: most of the data lie in the [-2, 4] range for\n# the transformed median income feature while the same data is squeezed in the\n# smaller [-0.2, 0.2] range for the transformed number of households.\n#\n# ``StandardScaler`` therefore cannot guarantee balanced feature scales in the\n# presence of outliers.\n\nmake_plot(1)\n\n##########################################################################\n# MinMaxScaler\n# ------------\n#\n# ``MinMaxScaler`` rescales the data set such that all feature values are in\n# the range [0, 1] as shown in the right panel below. However, this scaling\n# compress all inliers in the narrow range [0, 0.005] for the transformed\n# number of households.\n#\n# As ``StandardScaler``, ``MinMaxScaler`` is very sensitive to the presence of\n# outliers.\n\nmake_plot(2)\n\n#############################################################################\n# MaxAbsScaler\n# ------------\n#\n# ``MaxAbsScaler`` differs from the previous scaler such that the absolute\n# values are mapped in the range [0, 1]. On positive only data, this scaler\n# behaves similarly to ``MinMaxScaler`` and therefore also suffers from the\n# presence of large outliers.\n\nmake_plot(3)\n\n##############################################################################\n# RobustScaler\n# ------------\n#\n# Unlike the previous scalers, the centering and scaling statistics of this\n# scaler are based on percentiles and are therefore not influenced by a few\n# number of very large marginal outliers. Consequently, the resulting range of\n# the transformed feature values is larger than for the previous scalers and,\n# more importantly, are approximately similar: for both features most of the\n# transformed values lie in a [-2, 3] range as seen in the zoomed-in figure.\n# Note that the outliers themselves are still present in the transformed data.\n# If a separate outlier clipping is desirable, a non-linear transformation is\n# required (see below).\n\nmake_plot(4)\n\n##############################################################################\n# PowerTransformer\n# ----------------\n#\n# ``PowerTransformer`` applies a power transformation to each feature to make\n# the data more Gaussian-like. Currently, ``PowerTransformer`` implements the\n# Yeo-Johnson and Box-Cox transforms. The power transform finds the optimal\n# scaling factor to stabilize variance and mimimize skewness through maximum\n# likelihood estimation. By default, ``PowerTransformer`` also applies\n# zero-mean, unit variance normalization to the transformed output. Note that\n# Box-Cox can only be applied to strictly positive data. Income and number of\n# households happen to be strictly positive, but if negative values are present\n# the Yeo-Johnson transformed is to be preferred.\n\nmake_plot(5)\nmake_plot(6)\n\n##############################################################################\n# QuantileTransformer (Gaussian output)\n# -------------------------------------\n#\n# ``QuantileTransformer`` has an additional ``output_distribution`` parameter\n# allowing to match a Gaussian distribution instead of a uniform distribution.\n# Note that this non-parametetric transformer introduces saturation artifacts\n# for extreme values.\n\nmake_plot(7)\n\n###################################################################\n# QuantileTransformer (uniform output)\n# ------------------------------------\n#\n# ``QuantileTransformer`` applies a non-linear transformation such that the\n# probability density function of each feature will be mapped to a uniform\n# distribution. In this case, all the data will be mapped in the range [0, 1],\n# even the outliers which cannot be distinguished anymore from the inliers.\n#\n# As ``RobustScaler``, ``QuantileTransformer`` is robust to outliers in the\n# sense that adding or removing outliers in the training set will yield\n# approximately the same transformation on held out data. But contrary to\n# ``RobustScaler``, ``QuantileTransformer`` will also automatically collapse\n# any outlier by setting them to the a priori defined range boundaries (0 and\n# 1).\n\nmake_plot(8)\n\n##############################################################################\n# Normalizer\n# ----------\n#\n# The ``Normalizer`` rescales the vector for each sample to have unit norm,\n# independently of the distribution of the samples. It can be seen on both\n# figures below where all samples are mapped onto the unit circle. In our\n# example the two selected features have only positive values; therefore the\n# transformed data only lie in the positive quadrant. This would not be the\n# case if some original features had a mix of positive and negative values.\n\nmake_plot(9)\n\nplt.show()\n'"
scikit-learn/_downloads/7e7dc8613609fa2465099e917bef5fe7/plot_svm_kernels.py,3,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nSVM-Kernels\n=========================================================\n\nThree different types of SVM-Kernels are displayed below.\nThe polynomial and RBF are especially useful when the\ndata-points are not linearly separable.\n\n\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n\n# Our dataset and targets\nX = np.c_[(.4, -.7),\n          (-1.5, -1),\n          (-1.4, -.9),\n          (-1.3, -1.2),\n          (-1.1, -.2),\n          (-1.2, -.4),\n          (-.5, 1.2),\n          (-1.5, 2.1),\n          (1, 1),\n          # --\n          (1.3, .8),\n          (1.2, .5),\n          (.2, -2),\n          (.5, -2.4),\n          (.2, -2.3),\n          (0, -2.7),\n          (1.3, 2.1)].T\nY = [0] * 8 + [1] * 8\n\n# figure number\nfignum = 1\n\n# fit the model\nfor kernel in (\'linear\', \'poly\', \'rbf\'):\n    clf = svm.SVC(kernel=kernel, gamma=2)\n    clf.fit(X, Y)\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors=\'none\', zorder=10, edgecolors=\'k\')\n    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n                edgecolors=\'k\')\n\n    plt.axis(\'tight\')\n    x_min = -3\n    x_max = 3\n    y_min = -3\n    y_max = 3\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n    plt.contour(XX, YY, Z, colors=[\'k\', \'k\', \'k\'], linestyles=[\'--\', \'-\', \'--\'],\n                levels=[-.5, 0, .5])\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\nplt.show()\n'"
scikit-learn/_downloads/7ee06b9c95213c02799b0c9fca7f3171/plot_face_recognition.py,0,"b'""""""\n===================================================\nFaces recognition example using eigenfaces and SVMs\n===================================================\n\nThe dataset used in this example is a preprocessed excerpt of the\n""Labeled Faces in the Wild"", aka LFW_:\n\n  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n\n.. _LFW: http://vis-www.cs.umass.edu/lfw/\n\nExpected results for the top 5 most represented people in the dataset:\n\n================== ============ ======= ========== =======\n                   precision    recall  f1-score   support\n================== ============ ======= ========== =======\n     Ariel Sharon       0.67      0.92      0.77        13\n     Colin Powell       0.75      0.78      0.76        60\n  Donald Rumsfeld       0.78      0.67      0.72        27\n    George W Bush       0.86      0.86      0.86       146\nGerhard Schroeder       0.76      0.76      0.76        25\n      Hugo Chavez       0.67      0.67      0.67        15\n       Tony Blair       0.81      0.69      0.75        36\n\n      avg / total       0.80      0.80      0.80       322\n================== ============ ======= ========== =======\n\n""""""\nfrom time import time\nimport logging\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\n\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(message)s\')\n\n\n# #############################################################################\n# Download the data, if not already on disk and load it as numpy arrays\n\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint(""Total dataset size:"")\nprint(""n_samples: %d"" % n_samples)\nprint(""n_features: %d"" % n_features)\nprint(""n_classes: %d"" % n_classes)\n\n\n# #############################################################################\n# Split into a training set and a test set using a stratified k fold\n\n# split into a training and testing set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n\n\n# #############################################################################\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n# dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 150\n\nprint(""Extracting the top %d eigenfaces from %d faces""\n      % (n_components, X_train.shape[0]))\nt0 = time()\npca = PCA(n_components=n_components, svd_solver=\'randomized\',\n          whiten=True).fit(X_train)\nprint(""done in %0.3fs"" % (time() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(""Projecting the input data on the eigenfaces orthonormal basis"")\nt0 = time()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(""done in %0.3fs"" % (time() - t0))\n\n\n# #############################################################################\n# Train a SVM classification model\n\nprint(""Fitting the classifier to the training set"")\nt0 = time()\nparam_grid = {\'C\': [1e3, 5e3, 1e4, 5e4, 1e5],\n              \'gamma\': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(\n    SVC(kernel=\'rbf\', class_weight=\'balanced\'), param_grid\n)\nclf = clf.fit(X_train_pca, y_train)\nprint(""done in %0.3fs"" % (time() - t0))\nprint(""Best estimator found by grid search:"")\nprint(clf.best_estimator_)\n\n\n# #############################################################################\n# Quantitative evaluation of the model quality on the test set\n\nprint(""Predicting people\'s names on the test set"")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(""done in %0.3fs"" % (time() - t0))\n\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\n\n# #############################################################################\n# Qualitative evaluation of the predictions using matplotlib\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    """"""Helper function to plot a gallery of portraits""""""\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\n\n# plot the result of the prediction on a portion of the test set\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\' \', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\' \', 1)[-1]\n    return \'predicted: %s\\ntrue:      %s\' % (pred_name, true_name)\n\nprediction_titles = [title(y_pred, y_test, target_names, i)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n# plot the gallery of the most significative eigenfaces\n\neigenface_titles = [""eigenface %d"" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\nplt.show()\n'"
scikit-learn/_downloads/817483cec6a27acf5a18582f3c5df83a/plot_lle_digits.py,6,"b'""""""\n=============================================================================\nManifold learning on handwritten digits: Locally Linear Embedding, Isomap...\n=============================================================================\n\nAn illustration of various embeddings on the digits dataset.\n\nThe RandomTreesEmbedding, from the :mod:`sklearn.ensemble` module, is not\ntechnically a manifold embedding method, as it learn a high-dimensional\nrepresentation on which we apply a dimensionality reduction method.\nHowever, it is often useful to cast a dataset into a representation in\nwhich the classes are linearly-separable.\n\nt-SNE will be initialized with the embedding that is generated by PCA in\nthis example, which is not the default setting. It ensures global stability\nof the embedding, i.e., the embedding does not depend on random\ninitialization.\n\nLinear Discriminant Analysis, from the :mod:`sklearn.discriminant_analysis`\nmodule, and Neighborhood Components Analysis, from the :mod:`sklearn.neighbors`\nmodule, are supervised dimensionality reduction method, i.e. they make use of\nthe provided labels, contrary to other methods.\n""""""\n\n# Authors: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#          Gael Varoquaux\n# License: BSD 3 clause (C) INRIA 2011\n\nfrom time import time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nfrom sklearn import (manifold, datasets, decomposition, ensemble,\n                     discriminant_analysis, random_projection, neighbors)\nprint(__doc__)\n\ndigits = datasets.load_digits(n_class=6)\nX = digits.data\ny = digits.target\nn_samples, n_features = X.shape\nn_neighbors = 30\n\n\n# ----------------------------------------------------------------------\n# Scale and visualize the embedding vectors\ndef plot_embedding(X, title=None):\n    x_min, x_max = np.min(X, 0), np.max(X, 0)\n    X = (X - x_min) / (x_max - x_min)\n\n    plt.figure()\n    ax = plt.subplot(111)\n    for i in range(X.shape[0]):\n        plt.text(X[i, 0], X[i, 1], str(y[i]),\n                 color=plt.cm.Set1(y[i] / 10.),\n                 fontdict={\'weight\': \'bold\', \'size\': 9})\n\n    if hasattr(offsetbox, \'AnnotationBbox\'):\n        # only print thumbnails with matplotlib > 1.0\n        shown_images = np.array([[1., 1.]])  # just something big\n        for i in range(X.shape[0]):\n            dist = np.sum((X[i] - shown_images) ** 2, 1)\n            if np.min(dist) < 4e-3:\n                # don\'t show points that are too close\n                continue\n            shown_images = np.r_[shown_images, [X[i]]]\n            imagebox = offsetbox.AnnotationBbox(\n                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r),\n                X[i])\n            ax.add_artist(imagebox)\n    plt.xticks([]), plt.yticks([])\n    if title is not None:\n        plt.title(title)\n\n\n# ----------------------------------------------------------------------\n# Plot images of the digits\nn_img_per_row = 20\nimg = np.zeros((10 * n_img_per_row, 10 * n_img_per_row))\nfor i in range(n_img_per_row):\n    ix = 10 * i + 1\n    for j in range(n_img_per_row):\n        iy = 10 * j + 1\n        img[ix:ix + 8, iy:iy + 8] = X[i * n_img_per_row + j].reshape((8, 8))\n\nplt.imshow(img, cmap=plt.cm.binary)\nplt.xticks([])\nplt.yticks([])\nplt.title(\'A selection from the 64-dimensional digits dataset\')\n\n\n# ----------------------------------------------------------------------\n# Random 2D projection using a random unitary matrix\nprint(""Computing random projection"")\nrp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\nX_projected = rp.fit_transform(X)\nplot_embedding(X_projected, ""Random Projection of the digits"")\n\n\n# ----------------------------------------------------------------------\n# Projection on to the first 2 principal components\n\nprint(""Computing PCA projection"")\nt0 = time()\nX_pca = decomposition.TruncatedSVD(n_components=2).fit_transform(X)\nplot_embedding(X_pca,\n               ""Principal Components projection of the digits (time %.2fs)"" %\n               (time() - t0))\n\n# ----------------------------------------------------------------------\n# Projection on to the first 2 linear discriminant components\n\nprint(""Computing Linear Discriminant Analysis projection"")\nX2 = X.copy()\nX2.flat[::X.shape[1] + 1] += 0.01  # Make X invertible\nt0 = time()\nX_lda = discriminant_analysis.LinearDiscriminantAnalysis(n_components=2\n                                                         ).fit_transform(X2, y)\nplot_embedding(X_lda,\n               ""Linear Discriminant projection of the digits (time %.2fs)"" %\n               (time() - t0))\n\n\n# ----------------------------------------------------------------------\n# Isomap projection of the digits dataset\nprint(""Computing Isomap projection"")\nt0 = time()\nX_iso = manifold.Isomap(n_neighbors, n_components=2).fit_transform(X)\nprint(""Done."")\nplot_embedding(X_iso,\n               ""Isomap projection of the digits (time %.2fs)"" %\n               (time() - t0))\n\n\n# ----------------------------------------------------------------------\n# Locally linear embedding of the digits dataset\nprint(""Computing LLE embedding"")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method=\'standard\')\nt0 = time()\nX_lle = clf.fit_transform(X)\nprint(""Done. Reconstruction error: %g"" % clf.reconstruction_error_)\nplot_embedding(X_lle,\n               ""Locally Linear Embedding of the digits (time %.2fs)"" %\n               (time() - t0))\n\n\n# ----------------------------------------------------------------------\n# Modified Locally linear embedding of the digits dataset\nprint(""Computing modified LLE embedding"")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method=\'modified\')\nt0 = time()\nX_mlle = clf.fit_transform(X)\nprint(""Done. Reconstruction error: %g"" % clf.reconstruction_error_)\nplot_embedding(X_mlle,\n               ""Modified Locally Linear Embedding of the digits (time %.2fs)"" %\n               (time() - t0))\n\n\n# ----------------------------------------------------------------------\n# HLLE embedding of the digits dataset\nprint(""Computing Hessian LLE embedding"")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method=\'hessian\')\nt0 = time()\nX_hlle = clf.fit_transform(X)\nprint(""Done. Reconstruction error: %g"" % clf.reconstruction_error_)\nplot_embedding(X_hlle,\n               ""Hessian Locally Linear Embedding of the digits (time %.2fs)"" %\n               (time() - t0))\n\n\n# ----------------------------------------------------------------------\n# LTSA embedding of the digits dataset\nprint(""Computing LTSA embedding"")\nclf = manifold.LocallyLinearEmbedding(n_neighbors, n_components=2,\n                                      method=\'ltsa\')\nt0 = time()\nX_ltsa = clf.fit_transform(X)\nprint(""Done. Reconstruction error: %g"" % clf.reconstruction_error_)\nplot_embedding(X_ltsa,\n               ""Local Tangent Space Alignment of the digits (time %.2fs)"" %\n               (time() - t0))\n\n# ----------------------------------------------------------------------\n# MDS  embedding of the digits dataset\nprint(""Computing MDS embedding"")\nclf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\nt0 = time()\nX_mds = clf.fit_transform(X)\nprint(""Done. Stress: %f"" % clf.stress_)\nplot_embedding(X_mds,\n               ""MDS embedding of the digits (time %.2fs)"" %\n               (time() - t0))\n\n# ----------------------------------------------------------------------\n# Random Trees embedding of the digits dataset\nprint(""Computing Totally Random Trees embedding"")\nhasher = ensemble.RandomTreesEmbedding(n_estimators=200, random_state=0,\n                                       max_depth=5)\nt0 = time()\nX_transformed = hasher.fit_transform(X)\npca = decomposition.TruncatedSVD(n_components=2)\nX_reduced = pca.fit_transform(X_transformed)\n\nplot_embedding(X_reduced,\n               ""Random forest embedding of the digits (time %.2fs)"" %\n               (time() - t0))\n\n# ----------------------------------------------------------------------\n# Spectral embedding of the digits dataset\nprint(""Computing Spectral embedding"")\nembedder = manifold.SpectralEmbedding(n_components=2, random_state=0,\n                                      eigen_solver=""arpack"")\nt0 = time()\nX_se = embedder.fit_transform(X)\n\nplot_embedding(X_se,\n               ""Spectral embedding of the digits (time %.2fs)"" %\n               (time() - t0))\n\n# ----------------------------------------------------------------------\n# t-SNE embedding of the digits dataset\nprint(""Computing t-SNE embedding"")\ntsne = manifold.TSNE(n_components=2, init=\'pca\', random_state=0)\nt0 = time()\nX_tsne = tsne.fit_transform(X)\n\nplot_embedding(X_tsne,\n               ""t-SNE embedding of the digits (time %.2fs)"" %\n               (time() - t0))\n\n# ----------------------------------------------------------------------\n# NCA projection of the digits dataset\nprint(""Computing NCA projection"")\nnca = neighbors.NeighborhoodComponentsAnalysis(init=\'random\',\n                                               n_components=2, random_state=0)\nt0 = time()\nX_nca = nca.fit_transform(X, y)\n\nplot_embedding(X_nca,\n               ""NCA embedding of the digits (time %.2fs)"" %\n               (time() - t0))\n\nplt.show()\n'"
scikit-learn/_downloads/81ff0ed945f786392c60081a46000453/plot_gmm_covariances.py,11,"b'""""""\n===============\nGMM covariances\n===============\n\nDemonstration of several covariances types for Gaussian mixture models.\n\nSee :ref:`gmm` for more information on the estimator.\n\nAlthough GMM are often used for clustering, we can compare the obtained\nclusters with the actual classes from the dataset. We initialize the means\nof the Gaussians with the means of the classes from the training set to make\nthis comparison valid.\n\nWe plot predicted labels on both training and held out test data using a\nvariety of GMM covariance types on the iris dataset.\nWe compare GMMs with spherical, diagonal, full, and tied covariance\nmatrices in increasing order of performance. Although one would\nexpect full covariance to perform best in general, it is prone to\noverfitting on small datasets and does not generalize well to held out\ntest data.\n\nOn the plots, train data is shown as dots, while test data is shown as\ncrosses. The iris dataset is four-dimensional. Only the first two\ndimensions are shown here, and thus some points are separated in other\ndimensions.\n""""""\n\n# Author: Ron Weiss <ronweiss@gmail.com>, Gael Varoquaux\n# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import StratifiedKFold\n\nprint(__doc__)\n\ncolors = [\'navy\', \'turquoise\', \'darkorange\']\n\n\ndef make_ellipses(gmm, ax):\n    for n, color in enumerate(colors):\n        if gmm.covariance_type == \'full\':\n            covariances = gmm.covariances_[n][:2, :2]\n        elif gmm.covariance_type == \'tied\':\n            covariances = gmm.covariances_[:2, :2]\n        elif gmm.covariance_type == \'diag\':\n            covariances = np.diag(gmm.covariances_[n][:2])\n        elif gmm.covariance_type == \'spherical\':\n            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n        v, w = np.linalg.eigh(covariances)\n        u = w[0] / np.linalg.norm(w[0])\n        angle = np.arctan2(u[1], u[0])\n        angle = 180 * angle / np.pi  # convert to degrees\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n                                  180 + angle, color=color)\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(0.5)\n        ax.add_artist(ell)\n        ax.set_aspect(\'equal\', \'datalim\')\n\niris = datasets.load_iris()\n\n# Break up the dataset into non-overlapping training (75%) and testing\n# (25%) sets.\nskf = StratifiedKFold(n_splits=4)\n# Only take the first fold.\ntrain_index, test_index = next(iter(skf.split(iris.data, iris.target)))\n\n\nX_train = iris.data[train_index]\ny_train = iris.target[train_index]\nX_test = iris.data[test_index]\ny_test = iris.target[test_index]\n\nn_classes = len(np.unique(y_train))\n\n# Try GMMs using different types of covariances.\nestimators = {cov_type: GaussianMixture(n_components=n_classes,\n              covariance_type=cov_type, max_iter=20, random_state=0)\n              for cov_type in [\'spherical\', \'diag\', \'tied\', \'full\']}\n\nn_estimators = len(estimators)\n\nplt.figure(figsize=(3 * n_estimators // 2, 6))\nplt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n                    left=.01, right=.99)\n\n\nfor index, (name, estimator) in enumerate(estimators.items()):\n    # Since we have class labels for the training data, we can\n    # initialize the GMM parameters in a supervised manner.\n    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n                                    for i in range(n_classes)])\n\n    # Train the other parameters using the EM algorithm.\n    estimator.fit(X_train)\n\n    h = plt.subplot(2, n_estimators // 2, index + 1)\n    make_ellipses(estimator, h)\n\n    for n, color in enumerate(colors):\n        data = iris.data[iris.target == n]\n        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n                    label=iris.target_names[n])\n    # Plot the test data with crosses\n    for n, color in enumerate(colors):\n        data = X_test[y_test == n]\n        plt.scatter(data[:, 0], data[:, 1], marker=\'x\', color=color)\n\n    y_train_pred = estimator.predict(X_train)\n    train_accuracy = np.mean(y_train_pred.ravel() == y_train.ravel()) * 100\n    plt.text(0.05, 0.9, \'Train accuracy: %.1f\' % train_accuracy,\n             transform=h.transAxes)\n\n    y_test_pred = estimator.predict(X_test)\n    test_accuracy = np.mean(y_test_pred.ravel() == y_test.ravel()) * 100\n    plt.text(0.05, 0.8, \'Test accuracy: %.1f\' % test_accuracy,\n             transform=h.transAxes)\n\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(name)\n\nplt.legend(scatterpoints=1, loc=\'lower right\', prop=dict(size=12))\n\n\nplt.show()\n'"
scikit-learn/_downloads/82aaa81729fede43b9196afec7384974/plot_transformed_target.py,8,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\n======================================================\nEffect of transforming the targets in regression model\n======================================================\n\nIn this example, we give an overview of the\n:class:`sklearn.compose.TransformedTargetRegressor`. Two examples\nillustrate the benefit of transforming the targets before learning a linear\nregression model. The first example uses synthetic data while the second\nexample is based on the Boston housing data set.\n\n""""""\n\n# Author: Guillaume Lemaitre <guillaume.lemaitre@inria.fr>\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom distutils.version import LooseVersion\n\nprint(__doc__)\n\n###############################################################################\n# Synthetic example\n###############################################################################\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.metrics import median_absolute_error, r2_score\n\n\n# `normed` is being deprecated in favor of `density` in histograms\nif LooseVersion(matplotlib.__version__) >= \'2.1\':\n    density_param = {\'density\': True}\nelse:\n    density_param = {\'normed\': True}\n\n###############################################################################\n# A synthetic random regression problem is generated. The targets ``y`` are\n# modified by: (i) translating all targets such that all entries are\n# non-negative and (ii) applying an exponential function to obtain non-linear\n# targets which cannot be fitted using a simple linear model.\n#\n# Therefore, a logarithmic (`np.log1p`) and an exponential function\n# (`np.expm1`) will be used to transform the targets before training a linear\n# regression model and using it for prediction.\n\nX, y = make_regression(n_samples=10000, noise=100, random_state=0)\ny = np.exp((y + abs(y.min())) / 200)\ny_trans = np.log1p(y)\n\n###############################################################################\n# The following illustrate the probability density functions of the target\n# before and after applying the logarithmic functions.\n\nf, (ax0, ax1) = plt.subplots(1, 2)\n\nax0.hist(y, bins=100, **density_param)\nax0.set_xlim([0, 2000])\nax0.set_ylabel(\'Probability\')\nax0.set_xlabel(\'Target\')\nax0.set_title(\'Target distribution\')\n\nax1.hist(y_trans, bins=100, **density_param)\nax1.set_ylabel(\'Probability\')\nax1.set_xlabel(\'Target\')\nax1.set_title(\'Transformed target distribution\')\n\nf.suptitle(""Synthetic data"", y=0.035)\nf.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n###############################################################################\n# At first, a linear model will be applied on the original targets. Due to the\n# non-linearity, the model trained will not be precise during the\n# prediction. Subsequently, a logarithmic function is used to linearize the\n# targets, allowing better prediction even with a similar linear model as\n# reported by the median absolute error (MAE).\n\nf, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\n\nregr = RidgeCV()\nregr.fit(X_train, y_train)\ny_pred = regr.predict(X_test)\n\nax0.scatter(y_test, y_pred)\nax0.plot([0, 2000], [0, 2000], \'--k\')\nax0.set_ylabel(\'Target predicted\')\nax0.set_xlabel(\'True Target\')\nax0.set_title(\'Ridge regression \\n without target transformation\')\nax0.text(100, 1750, r\'$R^2$=%.2f, MAE=%.2f\' % (\n    r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))\nax0.set_xlim([0, 2000])\nax0.set_ylim([0, 2000])\n\nregr_trans = TransformedTargetRegressor(regressor=RidgeCV(),\n                                        func=np.log1p,\n                                        inverse_func=np.expm1)\nregr_trans.fit(X_train, y_train)\ny_pred = regr_trans.predict(X_test)\n\nax1.scatter(y_test, y_pred)\nax1.plot([0, 2000], [0, 2000], \'--k\')\nax1.set_ylabel(\'Target predicted\')\nax1.set_xlabel(\'True Target\')\nax1.set_title(\'Ridge regression \\n with target transformation\')\nax1.text(100, 1750, r\'$R^2$=%.2f, MAE=%.2f\' % (\n    r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))\nax1.set_xlim([0, 2000])\nax1.set_ylim([0, 2000])\n\nf.suptitle(""Synthetic data"", y=0.035)\nf.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n\n###############################################################################\n# Real-world data set\n###############################################################################\n\n###############################################################################\n# In a similar manner, the boston housing data set is used to show the impact\n# of transforming the targets before learning a model. In this example, the\n# targets to be predicted corresponds to the weighted distances to the five\n# Boston employment centers.\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.preprocessing import QuantileTransformer, quantile_transform\n\ndataset = load_boston()\ntarget = np.array(dataset.feature_names) == ""DIS""\nX = dataset.data[:, np.logical_not(target)]\ny = dataset.data[:, target].squeeze()\ny_trans = quantile_transform(dataset.data[:, target],\n                             n_quantiles=300,\n                             output_distribution=\'normal\',\n                             copy=True).squeeze()\n\n###############################################################################\n# A :class:`sklearn.preprocessing.QuantileTransformer` is used such that the\n# targets follows a normal distribution before applying a\n# :class:`sklearn.linear_model.RidgeCV` model.\n\nf, (ax0, ax1) = plt.subplots(1, 2)\n\nax0.hist(y, bins=100, **density_param)\nax0.set_ylabel(\'Probability\')\nax0.set_xlabel(\'Target\')\nax0.set_title(\'Target distribution\')\n\nax1.hist(y_trans, bins=100, **density_param)\nax1.set_ylabel(\'Probability\')\nax1.set_xlabel(\'Target\')\nax1.set_title(\'Transformed target distribution\')\n\nf.suptitle(""Boston housing data: distance to employment centers"", y=0.035)\nf.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n###############################################################################\n# The effect of the transformer is weaker than on the synthetic data. However,\n# the transform induces a decrease of the MAE.\n\nf, (ax0, ax1) = plt.subplots(1, 2, sharey=True)\n\nregr = RidgeCV()\nregr.fit(X_train, y_train)\ny_pred = regr.predict(X_test)\n\nax0.scatter(y_test, y_pred)\nax0.plot([0, 10], [0, 10], \'--k\')\nax0.set_ylabel(\'Target predicted\')\nax0.set_xlabel(\'True Target\')\nax0.set_title(\'Ridge regression \\n without target transformation\')\nax0.text(1, 9, r\'$R^2$=%.2f, MAE=%.2f\' % (\n    r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))\nax0.set_xlim([0, 10])\nax0.set_ylim([0, 10])\n\nregr_trans = TransformedTargetRegressor(\n    regressor=RidgeCV(),\n    transformer=QuantileTransformer(n_quantiles=300,\n                                    output_distribution=\'normal\'))\nregr_trans.fit(X_train, y_train)\ny_pred = regr_trans.predict(X_test)\n\nax1.scatter(y_test, y_pred)\nax1.plot([0, 10], [0, 10], \'--k\')\nax1.set_ylabel(\'Target predicted\')\nax1.set_xlabel(\'True Target\')\nax1.set_title(\'Ridge regression \\n with target transformation\')\nax1.text(1, 9, r\'$R^2$=%.2f, MAE=%.2f\' % (\n    r2_score(y_test, y_pred), median_absolute_error(y_test, y_pred)))\nax1.set_xlim([0, 10])\nax1.set_ylim([0, 10])\n\nf.suptitle(""Boston housing data: distance to employment centers"", y=0.035)\nf.tight_layout(rect=[0.05, 0.05, 0.95, 0.95])\n\nplt.show()\n'"
scikit-learn/_downloads/846ef94a6b3dbe76438ec33a7bcbb38d/plot_underfitting_overfitting.py,8,"b'""""""\n============================\nUnderfitting vs. Overfitting\n============================\n\nThis example demonstrates the problems of underfitting and overfitting and\nhow we can use linear regression with polynomial features to approximate\nnonlinear functions. The plot shows the function that we want to approximate,\nwhich is a part of the cosine function. In addition, the samples from the\nreal function and the approximations of different models are displayed. The\nmodels have polynomial features of different degrees. We can see that a\nlinear function (polynomial with degree 1) is not sufficient to fit the\ntraining samples. This is called **underfitting**. A polynomial of degree 4\napproximates the true function almost perfectly. However, for higher degrees\nthe model will **overfit** the training data, i.e. it learns the noise of the\ntraining data.\nWe evaluate quantitatively **overfitting** / **underfitting** by using\ncross-validation. We calculate the mean squared error (MSE) on the validation\nset, the higher, the less likely the model generalizes correctly from the\ntraining data.\n""""""\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\n\ndef true_fun(X):\n    return np.cos(1.5 * np.pi * X)\n\nnp.random.seed(0)\n\nn_samples = 30\ndegrees = [1, 4, 15]\n\nX = np.sort(np.random.rand(n_samples))\ny = true_fun(X) + np.random.randn(n_samples) * 0.1\n\nplt.figure(figsize=(14, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n    plt.setp(ax, xticks=(), yticks=())\n\n    polynomial_features = PolynomialFeatures(degree=degrees[i],\n                                             include_bias=False)\n    linear_regression = LinearRegression()\n    pipeline = Pipeline([(""polynomial_features"", polynomial_features),\n                         (""linear_regression"", linear_regression)])\n    pipeline.fit(X[:, np.newaxis], y)\n\n    # Evaluate the models using crossvalidation\n    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n                             scoring=""neg_mean_squared_error"", cv=10)\n\n    X_test = np.linspace(0, 1, 100)\n    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=""Model"")\n    plt.plot(X_test, true_fun(X_test), label=""True function"")\n    plt.scatter(X, y, edgecolor=\'b\', s=20, label=""Samples"")\n    plt.xlabel(""x"")\n    plt.ylabel(""y"")\n    plt.xlim((0, 1))\n    plt.ylim((-2, 2))\n    plt.legend(loc=""best"")\n    plt.title(""Degree {}\\nMSE = {:.2e}(+/- {:.2e})"".format(\n        degrees[i], -scores.mean(), scores.std()))\nplt.show()\n'"
scikit-learn/_downloads/85a2787b267bb02a704ab41f01164e1d/plot_compare_methods.py,0,"b'""""""\n=========================================\nComparison of Manifold Learning methods\n=========================================\n\nAn illustration of dimensionality reduction on the S-curve dataset\nwith various manifold learning methods.\n\nFor a discussion and comparison of these algorithms, see the\n:ref:`manifold module page <manifold>`\n\nFor a similar example, where the methods are applied to a\nsphere dataset, see :ref:`sphx_glr_auto_examples_manifold_plot_manifold_sphere.py`\n\nNote that the purpose of the MDS is to find a low-dimensional\nrepresentation of the data (here 2D) in which the distances respect well\nthe distances in the original high-dimensional space, unlike other\nmanifold-learning algorithms, it does not seeks an isotropic\nrepresentation of the data in the low-dimensional space.\n""""""\n\n# Author: Jake Vanderplas -- <vanderplas@astro.washington.edu>\n\nprint(__doc__)\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold, datasets\n\n# Next line to silence pyflakes. This import is needed.\nAxes3D\n\nn_points = 1000\nX, color = datasets.make_s_curve(n_points, random_state=0)\nn_neighbors = 10\nn_components = 2\n\nfig = plt.figure(figsize=(15, 8))\nplt.suptitle(""Manifold Learning with %i points, %i neighbors""\n             % (1000, n_neighbors), fontsize=14)\n\n\nax = fig.add_subplot(251, projection=\'3d\')\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\nax.view_init(4, -72)\n\nmethods = [\'standard\', \'ltsa\', \'hessian\', \'modified\']\nlabels = [\'LLE\', \'LTSA\', \'Hessian LLE\', \'Modified LLE\']\n\nfor i, method in enumerate(methods):\n    t0 = time()\n    Y = manifold.LocallyLinearEmbedding(n_neighbors, n_components,\n                                        eigen_solver=\'auto\',\n                                        method=method).fit_transform(X)\n    t1 = time()\n    print(""%s: %.2g sec"" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n    plt.title(""%s (%.2g sec)"" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis(\'tight\')\n\nt0 = time()\nY = manifold.Isomap(n_neighbors, n_components).fit_transform(X)\nt1 = time()\nprint(""Isomap: %.2g sec"" % (t1 - t0))\nax = fig.add_subplot(257)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title(""Isomap (%.2g sec)"" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\n\nt0 = time()\nmds = manifold.MDS(n_components, max_iter=100, n_init=1)\nY = mds.fit_transform(X)\nt1 = time()\nprint(""MDS: %.2g sec"" % (t1 - t0))\nax = fig.add_subplot(258)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title(""MDS (%.2g sec)"" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\n\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=n_components,\n                                n_neighbors=n_neighbors)\nY = se.fit_transform(X)\nt1 = time()\nprint(""SpectralEmbedding: %.2g sec"" % (t1 - t0))\nax = fig.add_subplot(259)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title(""SpectralEmbedding (%.2g sec)"" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\nt0 = time()\ntsne = manifold.TSNE(n_components=n_components, init=\'pca\', random_state=0)\nY = tsne.fit_transform(X)\nt1 = time()\nprint(""t-SNE: %.2g sec"" % (t1 - t0))\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\nplt.title(""t-SNE (%.2g sec)"" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\'tight\')\n\nplt.show()\n'"
scikit-learn/_downloads/8773883cbaaf7fb587bda079beae1296/plot_nearest_centroid.py,4,"b'""""""\n===============================\nNearest Centroid Classification\n===============================\n\nSample usage of Nearest Centroid classification.\nIt will plot the decision boundaries for each class.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import datasets\nfrom sklearn.neighbors import NearestCentroid\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap([\'orange\', \'cyan\', \'cornflowerblue\'])\ncmap_bold = ListedColormap([\'darkorange\', \'c\', \'darkblue\'])\n\nfor shrinkage in [None, .2]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = NearestCentroid(shrink_threshold=shrinkage)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    print(shrinkage, np.mean(y == y_pred))\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor=\'k\', s=20)\n    plt.title(""3-Class classification (shrink_threshold=%r)""\n              % shrinkage)\n    plt.axis(\'tight\')\n\nplt.show()\n'"
scikit-learn/_downloads/8ae495e1de9227463d99576cf3707eb0/plot_caching_nearest_neighbors.py,0,"b'""""""\n=========================\nCaching nearest neighbors\n=========================\n\nThis examples demonstrates how to precompute the k nearest neighbors before\nusing them in KNeighborsClassifier. KNeighborsClassifier can compute the\nnearest neighbors internally, but precomputing them can have several benefits,\nsuch as finer parameter control, caching for multiple use, or custom\nimplementations.\n\nHere we use the caching property of pipelines to cache the nearest neighbors\ngraph between multiple fits of KNeighborsClassifier. The first call is slow\nsince it computes the neighbors graph, while subsequent call are faster as they\ndo not need to recompute the graph. Here the durations are small since the\ndataset is small, but the gain can be more substantial when the dataset grows\nlarger, or when the grid of parameter to search is large.\n""""""\n# Author: Tom Dupre la Tour\n#\n# License: BSD 3 clause\nfrom tempfile import TemporaryDirectory\nimport matplotlib.pyplot as plt\n\nfrom sklearn.neighbors import KNeighborsTransformer, KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import load_digits\nfrom sklearn.pipeline import Pipeline\n\nprint(__doc__)\n\nX, y = load_digits(return_X_y=True)\nn_neighbors_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n# The transformer computes the nearest neighbors graph using the maximum number\n# of neighbors necessary in the grid search. The classifier model filters the\n# nearest neighbors graph as required by its own n_neighbors parameter.\ngraph_model = KNeighborsTransformer(n_neighbors=max(n_neighbors_list),\n                                    mode=\'distance\')\nclassifier_model = KNeighborsClassifier(metric=\'precomputed\')\n\n# Note that we give `memory` a directory to cache the graph computation\n# that will be used several times when tuning the hyperparameters of the\n# classifier.\nwith TemporaryDirectory(prefix=""sklearn_graph_cache_"") as tmpdir:\n    full_model = Pipeline(\n        steps=[(\'graph\', graph_model), (\'classifier\', classifier_model)],\n        memory=tmpdir)\n\n    param_grid = {\'classifier__n_neighbors\': n_neighbors_list}\n    grid_model = GridSearchCV(full_model, param_grid)\n    grid_model.fit(X, y)\n\n# Plot the results of the grid search.\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\naxes[0].errorbar(x=n_neighbors_list,\n                 y=grid_model.cv_results_[\'mean_test_score\'],\n                 yerr=grid_model.cv_results_[\'std_test_score\'])\naxes[0].set(xlabel=\'n_neighbors\', title=\'Classification accuracy\')\naxes[1].errorbar(x=n_neighbors_list, y=grid_model.cv_results_[\'mean_fit_time\'],\n                 yerr=grid_model.cv_results_[\'std_fit_time\'], color=\'r\')\naxes[1].set(xlabel=\'n_neighbors\', title=\'Fit time (with caching)\')\nfig.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/8d4b336fda06cb4189abfa859351d6cb/plot_tree_regression.py,4,"b'""""""\n===================================================================\nDecision Tree Regression\n===================================================================\n\nA 1D regression with decision tree.\n\nThe :ref:`decision trees <tree>` is\nused to fit a sine curve with addition noisy observation. As a result, it\nlearns local linear regressions approximating the sine curve.\n\nWe can see that if the maximum depth of the tree (controlled by the\n`max_depth` parameter) is set too high, the decision trees learn too fine\ndetails of the training data and learn from the noise, i.e. they overfit.\n""""""\nprint(__doc__)\n\n# Import the necessary modules and libraries\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# Create a random dataset\nrng = np.random.RandomState(1)\nX = np.sort(5 * rng.rand(80, 1), axis=0)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(16))\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=2)\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\ny_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, s=20, edgecolor=""black"",\n            c=""darkorange"", label=""data"")\nplt.plot(X_test, y_1, color=""cornflowerblue"",\n         label=""max_depth=2"", linewidth=2)\nplt.plot(X_test, y_2, color=""yellowgreen"", label=""max_depth=5"", linewidth=2)\nplt.xlabel(""data"")\nplt.ylabel(""target"")\nplt.title(""Decision Tree Regression"")\nplt.legend()\nplt.show()\n'"
scikit-learn/_downloads/8d6964b3a7690fe7ac80f294e89c7b0b/plot_calibration.py,6,"b'""""""\n======================================\nProbability calibration of classifiers\n======================================\n\nWhen performing classification you often want to predict not only\nthe class label, but also the associated probability. This probability\ngives you some kind of confidence on the prediction. However, not all\nclassifiers provide well-calibrated probabilities, some being over-confident\nwhile others being under-confident. Thus, a separate calibration of predicted\nprobabilities is often desirable as a postprocessing. This example illustrates\ntwo different methods for this calibration and evaluates the quality of the\nreturned probabilities using Brier\'s score\n(see https://en.wikipedia.org/wiki/Brier_score).\n\nCompared are the estimated probability using a Gaussian naive Bayes classifier\nwithout calibration, with a sigmoid calibration, and with a non-parametric\nisotonic calibration. One can observe that only the non-parametric model is\nable to provide a probability calibration that returns probabilities close\nto the expected 0.5 for most of the samples belonging to the middle\ncluster with heterogeneous labels. This results in a significantly improved\nBrier score.\n""""""\nprint(__doc__)\n\n# Author: Mathieu Blondel <mathieu@mblondel.org>\n#         Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n#         Balazs Kegl <balazs.kegl@gmail.com>\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD Style.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import train_test_split\n\n\nn_samples = 50000\nn_bins = 3  # use 3 bins for calibration_curve as we have 3 clusters here\n\n# Generate 3 blobs with 2 classes where the second blob contains\n# half positive samples and half negative samples. Probability in this\n# blob is therefore 0.5.\ncenters = [(-5, -5), (0, 0), (5, 5)]\nX, y = make_blobs(n_samples=n_samples, centers=centers, shuffle=False,\n                  random_state=42)\n\ny[:n_samples // 2] = 0\ny[n_samples // 2:] = 1\nsample_weight = np.random.RandomState(42).rand(y.shape[0])\n\n# split train, test for calibration\nX_train, X_test, y_train, y_test, sw_train, sw_test = \\\n    train_test_split(X, y, sample_weight, test_size=0.9, random_state=42)\n\n# Gaussian Naive-Bayes with no calibration\nclf = GaussianNB()\nclf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights\nprob_pos_clf = clf.predict_proba(X_test)[:, 1]\n\n# Gaussian Naive-Bayes with isotonic calibration\nclf_isotonic = CalibratedClassifierCV(clf, cv=2, method=\'isotonic\')\nclf_isotonic.fit(X_train, y_train, sw_train)\nprob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]\n\n# Gaussian Naive-Bayes with sigmoid calibration\nclf_sigmoid = CalibratedClassifierCV(clf, cv=2, method=\'sigmoid\')\nclf_sigmoid.fit(X_train, y_train, sw_train)\nprob_pos_sigmoid = clf_sigmoid.predict_proba(X_test)[:, 1]\n\nprint(""Brier scores: (the smaller the better)"")\n\nclf_score = brier_score_loss(y_test, prob_pos_clf, sw_test)\nprint(""No calibration: %1.3f"" % clf_score)\n\nclf_isotonic_score = brier_score_loss(y_test, prob_pos_isotonic, sw_test)\nprint(""With isotonic calibration: %1.3f"" % clf_isotonic_score)\n\nclf_sigmoid_score = brier_score_loss(y_test, prob_pos_sigmoid, sw_test)\nprint(""With sigmoid calibration: %1.3f"" % clf_sigmoid_score)\n\n# #############################################################################\n# Plot the data and the predicted probabilities\nplt.figure()\ny_unique = np.unique(y)\ncolors = cm.rainbow(np.linspace(0.0, 1.0, y_unique.size))\nfor this_y, color in zip(y_unique, colors):\n    this_X = X_train[y_train == this_y]\n    this_sw = sw_train[y_train == this_y]\n    plt.scatter(this_X[:, 0], this_X[:, 1], s=this_sw * 50,\n                c=color[np.newaxis, :],\n                alpha=0.5, edgecolor=\'k\',\n                label=""Class %s"" % this_y)\nplt.legend(loc=""best"")\nplt.title(""Data"")\n\nplt.figure()\norder = np.lexsort((prob_pos_clf, ))\nplt.plot(prob_pos_clf[order], \'r\', label=\'No calibration (%1.3f)\' % clf_score)\nplt.plot(prob_pos_isotonic[order], \'g\', linewidth=3,\n         label=\'Isotonic calibration (%1.3f)\' % clf_isotonic_score)\nplt.plot(prob_pos_sigmoid[order], \'b\', linewidth=3,\n         label=\'Sigmoid calibration (%1.3f)\' % clf_sigmoid_score)\nplt.plot(np.linspace(0, y_test.size, 51)[1::2],\n         y_test[order].reshape(25, -1).mean(1),\n         \'k\', linewidth=3, label=r\'Empirical\')\nplt.ylim([-0.05, 1.05])\nplt.xlabel(""Instances sorted according to predicted probability ""\n           ""(uncalibrated GNB)"")\nplt.ylabel(""P(y=1)"")\nplt.legend(loc=""upper left"")\nplt.title(""Gaussian naive Bayes probabilities"")\n\nplt.show()\n'"
scikit-learn/_downloads/8d6a9cd9f5b8262abb262a978733fb5f/plot_pca_vs_lda.py,0,"b'""""""\n=======================================================\nComparison of LDA and PCA 2D projection of Iris dataset\n=======================================================\n\nThe Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour\nand Virginica) with 4 attributes: sepal length, sepal width, petal length\nand petal width.\n\nPrincipal Component Analysis (PCA) applied to this data identifies the\ncombination of attributes (principal components, or directions in the\nfeature space) that account for the most variance in the data. Here we\nplot the different samples on the 2 first principal components.\n\nLinear Discriminant Analysis (LDA) tries to identify attributes that\naccount for the most variance *between classes*. In particular,\nLDA, in contrast to PCA, is a supervised method, using known class labels.\n""""""\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\niris = datasets.load_iris()\n\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\npca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)\n\nlda = LinearDiscriminantAnalysis(n_components=2)\nX_r2 = lda.fit(X, y).transform(X)\n\n# Percentage of variance explained for each components\nprint(\'explained variance ratio (first two components): %s\'\n      % str(pca.explained_variance_ratio_))\n\nplt.figure()\ncolors = [\'navy\', \'turquoise\', \'darkorange\']\nlw = 2\n\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=lw,\n                label=target_name)\nplt.legend(loc=\'best\', shadow=False, scatterpoints=1)\nplt.title(\'PCA of IRIS dataset\')\n\nplt.figure()\nfor color, i, target_name in zip(colors, [0, 1, 2], target_names):\n    plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,\n                label=target_name)\nplt.legend(loc=\'best\', shadow=False, scatterpoints=1)\nplt.title(\'LDA of IRIS dataset\')\n\nplt.show()\n'"
scikit-learn/_downloads/8e549c70443a312f22166d2313453173/plot_forest_iris.py,11,"b'""""""\n====================================================================\nPlot the decision surfaces of ensembles of trees on the iris dataset\n====================================================================\n\nPlot the decision surfaces of forests of randomized trees trained on pairs of\nfeatures of the iris dataset.\n\nThis plot compares the decision surfaces learned by a decision tree classifier\n(first column), by a random forest classifier (second column), by an extra-\ntrees classifier (third column) and by an AdaBoost classifier (fourth column).\n\nIn the first row, the classifiers are built using the sepal width and\nthe sepal length features only, on the second row using the petal length and\nsepal length only, and on the third row using the petal width and the\npetal length only.\n\nIn descending order of quality, when trained (outside of this example) on all\n4 features using 30 estimators and scored using 10 fold cross validation,\nwe see::\n\n    ExtraTreesClassifier()  # 0.95 score\n    RandomForestClassifier()  # 0.94 score\n    AdaBoost(DecisionTree(max_depth=3))  # 0.94 score\n    DecisionTree(max_depth=None)  # 0.94 score\n\nIncreasing `max_depth` for AdaBoost lowers the standard deviation of\nthe scores (but the average score does not improve).\n\nSee the console\'s output for further details about each model.\n\nIn this example you might try to:\n\n1) vary the ``max_depth`` for the ``DecisionTreeClassifier`` and\n   ``AdaBoostClassifier``, perhaps try ``max_depth=3`` for the\n   ``DecisionTreeClassifier`` or ``max_depth=None`` for ``AdaBoostClassifier``\n2) vary ``n_estimators``\n\nIt is worth noting that RandomForests and ExtraTrees can be fitted in parallel\non many cores as each tree is built independently of the others. AdaBoost\'s\nsamples are built sequentially and so do not use multiple cores.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Parameters\nn_classes = 3\nn_estimators = 30\ncmap = plt.cm.RdYlBu\nplot_step = 0.02  # fine step width for decision surface contours\nplot_step_coarser = 0.5  # step widths for coarse classifier guesses\nRANDOM_SEED = 13  # fix the seed on each iteration\n\n# Load data\niris = load_iris()\n\nplot_idx = 1\n\nmodels = [DecisionTreeClassifier(max_depth=None),\n          RandomForestClassifier(n_estimators=n_estimators),\n          ExtraTreesClassifier(n_estimators=n_estimators),\n          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n                             n_estimators=n_estimators)]\n\nfor pair in ([0, 1], [0, 2], [2, 3]):\n    for model in models:\n        # We only take the two corresponding features\n        X = iris.data[:, pair]\n        y = iris.target\n\n        # Shuffle\n        idx = np.arange(X.shape[0])\n        np.random.seed(RANDOM_SEED)\n        np.random.shuffle(idx)\n        X = X[idx]\n        y = y[idx]\n\n        # Standardize\n        mean = X.mean(axis=0)\n        std = X.std(axis=0)\n        X = (X - mean) / std\n\n        # Train\n        model.fit(X, y)\n\n        scores = model.score(X, y)\n        # Create a title for each column and the console by using str() and\n        # slicing away useless parts of the string\n        model_title = str(type(model)).split(\n            ""."")[-1][:-2][:-len(""Classifier"")]\n\n        model_details = model_title\n        if hasattr(model, ""estimators_""):\n            model_details += "" with {} estimators"".format(\n                len(model.estimators_))\n        print(model_details + "" with features"", pair,\n              ""has a score of"", scores)\n\n        plt.subplot(3, 4, plot_idx)\n        if plot_idx <= len(models):\n            # Add a title at the top of each column\n            plt.title(model_title, fontsize=9)\n\n        # Now plot the decision boundary using a fine mesh as input to a\n        # filled contour plot\n        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                             np.arange(y_min, y_max, plot_step))\n\n        # Plot either a single DecisionTreeClassifier or alpha blend the\n        # decision surfaces of the ensemble of classifiers\n        if isinstance(model, DecisionTreeClassifier):\n            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n        else:\n            # Choose alpha blend level with respect to the number\n            # of estimators\n            # that are in use (noting that AdaBoost can use fewer estimators\n            # than its maximum if it achieves a good enough fit early on)\n            estimator_alpha = 1.0 / len(model.estimators_)\n            for tree in model.estimators_:\n                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n                Z = Z.reshape(xx.shape)\n                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n\n        # Build a coarser grid to plot a set of ensemble classifications\n        # to show how these are different to what we see in the decision\n        # surfaces. These points are regularly space and do not have a\n        # black outline\n        xx_coarser, yy_coarser = np.meshgrid(\n            np.arange(x_min, x_max, plot_step_coarser),\n            np.arange(y_min, y_max, plot_step_coarser))\n        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n                                         yy_coarser.ravel()]\n                                         ).reshape(xx_coarser.shape)\n        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n                                c=Z_points_coarser, cmap=cmap,\n                                edgecolors=""none"")\n\n        # Plot the training points, these are clustered together and have a\n        # black outline\n        plt.scatter(X[:, 0], X[:, 1], c=y,\n                    cmap=ListedColormap([\'r\', \'y\', \'b\']),\n                    edgecolor=\'k\', s=20)\n        plot_idx += 1  # move on to the next plot in sequence\n\nplt.suptitle(""Classifiers on feature subsets of the Iris dataset"", fontsize=12)\nplt.axis(""tight"")\nplt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\nplt.show()\n'"
scikit-learn/_downloads/8f1f3f299822bee3ad0d7280db4dc5a4/plot_iris_dtc.py,4,"b'""""""\n================================================================\nPlot the decision surface of a decision tree on the iris dataset\n================================================================\n\nPlot the decision surface of a decision tree trained on pairs\nof features of the iris dataset.\n\nSee :ref:`decision tree <tree>` for more information on the estimator.\n\nFor each pair of iris features, the decision tree learns decision\nboundaries made of combinations of simple thresholding rules inferred from\nthe training samples.\n\nWe also show the tree structure of a model built on all of the features.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\n# Parameters\nn_classes = 3\nplot_colors = ""ryb""\nplot_step = 0.02\n\n# Load data\niris = load_iris()\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n                                [1, 2], [1, 3], [2, 3]]):\n    # We only take the two corresponding features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # Train\n    clf = DecisionTreeClassifier().fit(X, y)\n\n    # Plot the decision boundary\n    plt.subplot(2, 3, pairidx + 1)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n\n    plt.xlabel(iris.feature_names[pair[0]])\n    plt.ylabel(iris.feature_names[pair[1]])\n\n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                    cmap=plt.cm.RdYlBu, edgecolor=\'black\', s=15)\n\nplt.suptitle(""Decision surface of a decision tree using paired features"")\nplt.legend(loc=\'lower right\', borderpad=0, handletextpad=0)\nplt.axis(""tight"")\n\nplt.figure()\nclf = DecisionTreeClassifier().fit(iris.data, iris.target)\nplot_tree(clf, filled=True)\nplt.show()\n'"
scikit-learn/_downloads/8fdff8d3babac48b6d2e21dae8cc7524/plot_optics.py,11,"b'""""""\n===================================\nDemo of OPTICS clustering algorithm\n===================================\nFinds core samples of high density and expands clusters from them.\nThis example uses data that is generated so that the clusters have\ndifferent densities.\nThe :class:`sklearn.cluster.OPTICS` is first used with its Xi cluster detection\nmethod, and then setting specific thresholds on the reachability, which\ncorresponds to :class:`sklearn.cluster.DBSCAN`. We can see that the different\nclusters of OPTICS\'s Xi method can be recovered with different choices of\nthresholds in DBSCAN.\n""""""\n\n# Authors: Shane Grigsby <refuge@rocktalus.com>\n#          Adrin Jalali <adrin.jalali@gmail.com>\n# License: BSD 3 clause\n\n\nfrom sklearn.cluster import OPTICS, cluster_optics_dbscan\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate sample data\n\nnp.random.seed(0)\nn_points_per_cluster = 250\n\nC1 = [-5, -2] + .8 * np.random.randn(n_points_per_cluster, 2)\nC2 = [4, -1] + .1 * np.random.randn(n_points_per_cluster, 2)\nC3 = [1, -2] + .2 * np.random.randn(n_points_per_cluster, 2)\nC4 = [-2, 3] + .3 * np.random.randn(n_points_per_cluster, 2)\nC5 = [3, -2] + 1.6 * np.random.randn(n_points_per_cluster, 2)\nC6 = [5, 6] + 2 * np.random.randn(n_points_per_cluster, 2)\nX = np.vstack((C1, C2, C3, C4, C5, C6))\n\nclust = OPTICS(min_samples=50, xi=.05, min_cluster_size=.05)\n\n# Run the fit\nclust.fit(X)\n\nlabels_050 = cluster_optics_dbscan(reachability=clust.reachability_,\n                                   core_distances=clust.core_distances_,\n                                   ordering=clust.ordering_, eps=0.5)\nlabels_200 = cluster_optics_dbscan(reachability=clust.reachability_,\n                                   core_distances=clust.core_distances_,\n                                   ordering=clust.ordering_, eps=2)\n\nspace = np.arange(len(X))\nreachability = clust.reachability_[clust.ordering_]\nlabels = clust.labels_[clust.ordering_]\n\nplt.figure(figsize=(10, 7))\nG = gridspec.GridSpec(2, 3)\nax1 = plt.subplot(G[0, :])\nax2 = plt.subplot(G[1, 0])\nax3 = plt.subplot(G[1, 1])\nax4 = plt.subplot(G[1, 2])\n\n# Reachability plot\ncolors = [\'g.\', \'r.\', \'b.\', \'y.\', \'c.\']\nfor klass, color in zip(range(0, 5), colors):\n    Xk = space[labels == klass]\n    Rk = reachability[labels == klass]\n    ax1.plot(Xk, Rk, color, alpha=0.3)\nax1.plot(space[labels == -1], reachability[labels == -1], \'k.\', alpha=0.3)\nax1.plot(space, np.full_like(space, 2., dtype=float), \'k-\', alpha=0.5)\nax1.plot(space, np.full_like(space, 0.5, dtype=float), \'k-.\', alpha=0.5)\nax1.set_ylabel(\'Reachability (epsilon distance)\')\nax1.set_title(\'Reachability Plot\')\n\n# OPTICS\ncolors = [\'g.\', \'r.\', \'b.\', \'y.\', \'c.\']\nfor klass, color in zip(range(0, 5), colors):\n    Xk = X[clust.labels_ == klass]\n    ax2.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\nax2.plot(X[clust.labels_ == -1, 0], X[clust.labels_ == -1, 1], \'k+\', alpha=0.1)\nax2.set_title(\'Automatic Clustering\\nOPTICS\')\n\n# DBSCAN at 0.5\ncolors = [\'g\', \'greenyellow\', \'olive\', \'r\', \'b\', \'c\']\nfor klass, color in zip(range(0, 6), colors):\n    Xk = X[labels_050 == klass]\n    ax3.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3, marker=\'.\')\nax3.plot(X[labels_050 == -1, 0], X[labels_050 == -1, 1], \'k+\', alpha=0.1)\nax3.set_title(\'Clustering at 0.5 epsilon cut\\nDBSCAN\')\n\n# DBSCAN at 2.\ncolors = [\'g.\', \'m.\', \'y.\', \'c.\']\nfor klass, color in zip(range(0, 4), colors):\n    Xk = X[labels_200 == klass]\n    ax4.plot(Xk[:, 0], Xk[:, 1], color, alpha=0.3)\nax4.plot(X[labels_200 == -1, 0], X[labels_200 == -1, 1], \'k+\', alpha=0.1)\nax4.set_title(\'Clustering at 2.0 epsilon cut\\nDBSCAN\')\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/90d21a82a72d6839ff9e206118a7e0f8/plot_release_highlights_0_22_0.py,4,"b'""""""\n========================================\nRelease Highlights for scikit-learn 0.22\n========================================\n\n.. currentmodule:: sklearn\n\nWe are pleased to announce the release of scikit-learn 0.22, which comes\nwith many bug fixes and new features! We detail below a few of the major\nfeatures of this release. For an exhaustive list of all the changes, please\nrefer to the :ref:`release notes <changes_0_22>`.\n\nTo install the latest version (with pip)::\n\n    pip install --upgrade scikit-learn\n\nor with conda::\n\n    conda install scikit-learn\n""""""\n\n##############################################################################\n# New plotting API\n# ----------------\n#\n# A new plotting API is available for creating visualizations. This new API\n# allows for quickly adjusting the visuals of a plot without involving any\n# recomputation. It is also possible to add different plots to the same\n# figure. The following example illustrates :class:`~metrics.plot_roc_curve`,\n# but other plots utilities are supported like\n# :class:`~inspection.plot_partial_dependence`,\n# :class:`~metrics.plot_precision_recall_curve`, and\n# :class:`~metrics.plot_confusion_matrix`. Read more about this new API in the\n# :ref:`User Guide <visualizations>`.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nimport matplotlib.pyplot as plt\n\nX, y = make_classification(random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nsvc = SVC(random_state=42)\nsvc.fit(X_train, y_train)\nrfc = RandomForestClassifier(random_state=42)\nrfc.fit(X_train, y_train)\n\nsvc_disp = plot_roc_curve(svc, X_test, y_test)\nrfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=svc_disp.ax_)\nrfc_disp.figure_.suptitle(""ROC curve comparison"")\n\nplt.show()\n\n############################################################################\n# Stacking Classifier and Regressor\n# ---------------------------------\n# :class:`~ensemble.StackingClassifier` and\n# :class:`~ensemble.StackingRegressor`\n# allow you to have a stack of estimators with a final classifier or\n# a regressor.\n# Stacked generalization consists in stacking the output of individual\n# estimators and use a classifier to compute the final prediction. Stacking\n# allows to use the strength of each individual estimator by using their output\n# as input of a final estimator.\n# Base estimators are fitted on the full ``X`` while\n# the final estimator is trained using cross-validated predictions of the\n# base estimators using ``cross_val_predict``.\n#\n# Read more in the :ref:`User Guide <stacking>`.\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_iris(return_X_y=True)\nestimators = [\n    (\'rf\', RandomForestClassifier(n_estimators=10, random_state=42)),\n    (\'svr\', make_pipeline(StandardScaler(),\n                          LinearSVC(random_state=42)))\n]\nclf = StackingClassifier(\n    estimators=estimators, final_estimator=LogisticRegression()\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=42\n)\nclf.fit(X_train, y_train).score(X_test, y_test)\n\n##############################################################################\n# Permutation-based feature importance\n# ------------------------------------\n#\n# The :func:`inspection.permutation_importance` can be used to get an\n# estimate of the importance of each feature, for any fitted estimator:\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\n\nX, y = make_classification(random_state=0, n_features=5, n_informative=3)\nrf = RandomForestClassifier(random_state=0).fit(X, y)\nresult = permutation_importance(rf, X, y, n_repeats=10, random_state=0,\n                                n_jobs=-1)\n\nfig, ax = plt.subplots()\nsorted_idx = result.importances_mean.argsort()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=range(X.shape[1]))\nax.set_title(""Permutation Importance of each feature"")\nax.set_ylabel(""Features"")\nfig.tight_layout()\nplt.show()\n\n##############################################################################\n# Native support for missing values for gradient boosting\n# -------------------------------------------------------\n#\n# The :class:`ensemble.HistGradientBoostingClassifier`\n# and :class:`ensemble.HistGradientBoostingRegressor` now have native\n# support for missing values (NaNs). This means that there is no need for\n# imputing data when training or predicting.\n\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nimport numpy as np\n\nX = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\ny = [0, 0, 1, 1]\n\ngbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\nprint(gbdt.predict(X))\n\n############################################################################\n# Precomputed sparse nearest neighbors graph\n# ------------------------------------------\n# Most estimators based on nearest neighbors graphs now accept precomputed\n# sparse graphs as input, to reuse the same graph for multiple estimator fits.\n# To use this feature in a pipeline, one can use the `memory` parameter, along\n# with one of the two new transformers,\n# :class:`neighbors.KNeighborsTransformer` and\n# :class:`neighbors.RadiusNeighborsTransformer`. The precomputation\n# can also be performed by custom estimators to use alternative\n# implementations, such as approximate nearest neighbors methods.\n# See more details in the :ref:`User Guide <neighbors_transformer>`.\n\nfrom tempfile import TemporaryDirectory\nfrom sklearn.neighbors import KNeighborsTransformer\nfrom sklearn.manifold import Isomap\nfrom sklearn.pipeline import make_pipeline\n\nX, y = make_classification(random_state=0)\n\nwith TemporaryDirectory(prefix=""sklearn_cache_"") as tmpdir:\n    estimator = make_pipeline(\n        KNeighborsTransformer(n_neighbors=10, mode=\'distance\'),\n        Isomap(n_neighbors=10, metric=\'precomputed\'),\n        memory=tmpdir)\n    estimator.fit(X)\n\n    # We can decrease the number of neighbors and the graph will not be\n    # recomputed.\n    estimator.set_params(isomap__n_neighbors=5)\n    estimator.fit(X)\n\n##############################################################################\n# KNN Based Imputation\n# ------------------------------------\n# We now support imputation for completing missing values using k-Nearest\n# Neighbors.\n#\n# Each sample\'s missing values are imputed using the mean value from\n# ``n_neighbors`` nearest neighbors found in the training set. Two samples are\n# close if the features that neither is missing are close.\n# By default, a euclidean distance metric\n# that supports missing values,\n# :func:`~metrics.nan_euclidean_distances`, is used to find the nearest\n# neighbors.\n#\n# Read more in the :ref:`User Guide <knnimpute>`.\n\nimport numpy as np\nfrom sklearn.impute import KNNImputer\n\nX = [[1, 2, np.nan], [3, 4, 3], [np.nan, 6, 5], [8, 8, 7]]\nimputer = KNNImputer(n_neighbors=2)\nprint(imputer.fit_transform(X))\n\n#############################################################################\n# Tree pruning\n# ------------\n#\n# It is now possible to prune most tree-based estimators once the trees are\n# built. The pruning is based on minimal cost-complexity. Read more in the\n# :ref:`User Guide <minimal_cost_complexity_pruning>` for details.\n\nX, y = make_classification(random_state=0)\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0).fit(X, y)\nprint(""Average number of nodes without pruning {:.1f}"".format(\n    np.mean([e.tree_.node_count for e in rf.estimators_])))\n\nrf = RandomForestClassifier(random_state=0, ccp_alpha=0.05).fit(X, y)\nprint(""Average number of nodes with pruning {:.1f}"".format(\n    np.mean([e.tree_.node_count for e in rf.estimators_])))\n\n############################################################################\n# Retrieve dataframes from OpenML\n# -------------------------------\n# :func:`datasets.fetch_openml` can now return pandas dataframe and thus\n# properly handle datasets with heterogeneous data:\n\nfrom sklearn.datasets import fetch_openml\n\ntitanic = fetch_openml(\'titanic\', version=1, as_frame=True)\nprint(titanic.data.head()[[\'pclass\', \'embarked\']])\n\n############################################################################\n# Checking scikit-learn compatibility of an estimator\n# ---------------------------------------------------\n# Developers can check the compatibility of their scikit-learn compatible\n# estimators using :func:`~utils.estimator_checks.check_estimator`. For\n# instance, the ``check_estimator(LinearSVC)`` passes.\n#\n# We now provide a ``pytest`` specific decorator which allows ``pytest``\n# to run all checks independently and report the checks that are failing.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.utils.estimator_checks import parametrize_with_checks\n\n\n@parametrize_with_checks([LogisticRegression, DecisionTreeRegressor])\ndef test_sklearn_compatible_estimator(estimator, check):\n    check(estimator)\n\n############################################################################\n# ROC AUC now supports multiclass classification\n# ----------------------------------------------\n# The :func:`roc_auc_score` function can also be used in multi-class\n# classification. Two averaging strategies are currently supported: the\n# one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\n# the one-vs-rest algorithm computes the average of the ROC AUC scores for each\n# class against all other classes. In both cases, the multiclass ROC AUC scores\n# are computed from the probability estimates that a sample belongs to a\n# particular class according to the model. The OvO and OvR algorithms support\n# weighting uniformly (``average=\'macro\'``) and weighting by the prevalence\n# (``average=\'weighted\'``).\n#\n# Read more in the :ref:`User Guide <roc_metrics>`.\n\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score\n\nX, y = make_classification(n_classes=4, n_informative=16)\nclf = SVC(decision_function_shape=\'ovo\', probability=True).fit(X, y)\nprint(roc_auc_score(y, clf.predict_proba(X), multi_class=\'ovo\'))\n'"
scikit-learn/_downloads/91c4ac675a102102eb279fe5337d8469/plot_separating_hyperplane.py,4,"b'""""""\n=========================================\nSVM: Maximum margin separating hyperplane\n=========================================\n\nPlot the maximum margin separating hyperplane within a two-class\nseparable dataset using a Support Vector Machine classifier with\nlinear kernel.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs\n\n\n# we create 40 separable points\nX, y = make_blobs(n_samples=40, centers=2, random_state=6)\n\n# fit the model, don\'t regularize for illustration purposes\nclf = svm.SVC(kernel=\'linear\', C=1000)\nclf.fit(X, y)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# plot the decision function\nax = plt.gca()\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# create grid to evaluate model\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\nYY, XX = np.meshgrid(yy, xx)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\nZ = clf.decision_function(xy).reshape(XX.shape)\n\n# plot decision boundary and margins\nax.contour(XX, YY, Z, colors=\'k\', levels=[-1, 0, 1], alpha=0.5,\n           linestyles=[\'--\', \'-\', \'--\'])\n# plot support vectors\nax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n           linewidth=1, facecolors=\'none\', edgecolors=\'k\')\nplt.show()\n'"
scikit-learn/_downloads/9650dca0d5e6710a7b8cc6e2c9d67f1c/plot_prediction_latency.py,12,"b'""""""\n==================\nPrediction Latency\n==================\n\nThis is an example showing the prediction latency of various scikit-learn\nestimators.\n\nThe goal is to measure the latency one can expect when doing predictions\neither in bulk or atomic (i.e. one by one) mode.\n\nThe plots represent the distribution of the prediction latency as a boxplot.\n\n""""""\n\n# Authors: Eustache Diemert <eustache@diemert.fr>\n# License: BSD 3 clause\n\nfrom collections import defaultdict\n\nimport time\nimport gc\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.utils import shuffle\n\n\ndef _not_in_sphinx():\n    # Hack to detect whether we are running by the sphinx builder\n    return \'__file__\' in globals()\n\n\ndef atomic_benchmark_estimator(estimator, X_test, verbose=False):\n    """"""Measure runtime prediction of each instance.""""""\n    n_instances = X_test.shape[0]\n    runtimes = np.zeros(n_instances, dtype=np.float)\n    for i in range(n_instances):\n        instance = X_test[[i], :]\n        start = time.time()\n        estimator.predict(instance)\n        runtimes[i] = time.time() - start\n    if verbose:\n        print(""atomic_benchmark runtimes:"", min(runtimes), np.percentile(\n            runtimes, 50), max(runtimes))\n    return runtimes\n\n\ndef bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats, verbose):\n    """"""Measure runtime prediction of the whole input.""""""\n    n_instances = X_test.shape[0]\n    runtimes = np.zeros(n_bulk_repeats, dtype=np.float)\n    for i in range(n_bulk_repeats):\n        start = time.time()\n        estimator.predict(X_test)\n        runtimes[i] = time.time() - start\n    runtimes = np.array(list(map(lambda x: x / float(n_instances), runtimes)))\n    if verbose:\n        print(""bulk_benchmark runtimes:"", min(runtimes), np.percentile(\n            runtimes, 50), max(runtimes))\n    return runtimes\n\n\ndef benchmark_estimator(estimator, X_test, n_bulk_repeats=30, verbose=False):\n    """"""\n    Measure runtimes of prediction in both atomic and bulk mode.\n\n    Parameters\n    ----------\n    estimator : already trained estimator supporting `predict()`\n    X_test : test input\n    n_bulk_repeats : how many times to repeat when evaluating bulk mode\n\n    Returns\n    -------\n    atomic_runtimes, bulk_runtimes : a pair of `np.array` which contain the\n    runtimes in seconds.\n\n    """"""\n    atomic_runtimes = atomic_benchmark_estimator(estimator, X_test, verbose)\n    bulk_runtimes = bulk_benchmark_estimator(estimator, X_test, n_bulk_repeats,\n                                             verbose)\n    return atomic_runtimes, bulk_runtimes\n\n\ndef generate_dataset(n_train, n_test, n_features, noise=0.1, verbose=False):\n    """"""Generate a regression dataset with the given parameters.""""""\n    if verbose:\n        print(""generating dataset..."")\n\n    X, y, coef = make_regression(n_samples=n_train + n_test,\n                                 n_features=n_features, noise=noise, coef=True)\n\n    random_seed = 13\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, train_size=n_train, test_size=n_test, random_state=random_seed)\n    X_train, y_train = shuffle(X_train, y_train, random_state=random_seed)\n\n    X_scaler = StandardScaler()\n    X_train = X_scaler.fit_transform(X_train)\n    X_test = X_scaler.transform(X_test)\n\n    y_scaler = StandardScaler()\n    y_train = y_scaler.fit_transform(y_train[:, None])[:, 0]\n    y_test = y_scaler.transform(y_test[:, None])[:, 0]\n\n    gc.collect()\n    if verbose:\n        print(""ok"")\n    return X_train, y_train, X_test, y_test\n\n\ndef boxplot_runtimes(runtimes, pred_type, configuration):\n    """"""\n    Plot a new `Figure` with boxplots of prediction runtimes.\n\n    Parameters\n    ----------\n    runtimes : list of `np.array` of latencies in micro-seconds\n    cls_names : list of estimator class names that generated the runtimes\n    pred_type : \'bulk\' or \'atomic\'\n\n    """"""\n\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    bp = plt.boxplot(runtimes, )\n\n    cls_infos = [\'%s\\n(%d %s)\' % (estimator_conf[\'name\'],\n                                  estimator_conf[\'complexity_computer\'](\n                                      estimator_conf[\'instance\']),\n                                  estimator_conf[\'complexity_label\']) for\n                 estimator_conf in configuration[\'estimators\']]\n    plt.setp(ax1, xticklabels=cls_infos)\n    plt.setp(bp[\'boxes\'], color=\'black\')\n    plt.setp(bp[\'whiskers\'], color=\'black\')\n    plt.setp(bp[\'fliers\'], color=\'red\', marker=\'+\')\n\n    ax1.yaxis.grid(True, linestyle=\'-\', which=\'major\', color=\'lightgrey\',\n                   alpha=0.5)\n\n    ax1.set_axisbelow(True)\n    ax1.set_title(\'Prediction Time per Instance - %s, %d feats.\' % (\n        pred_type.capitalize(),\n        configuration[\'n_features\']))\n    ax1.set_ylabel(\'Prediction Time (us)\')\n\n    plt.show()\n\n\ndef benchmark(configuration):\n    """"""Run the whole benchmark.""""""\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration[\'n_train\'], configuration[\'n_test\'],\n        configuration[\'n_features\'])\n\n    stats = {}\n    for estimator_conf in configuration[\'estimators\']:\n        print(""Benchmarking"", estimator_conf[\'instance\'])\n        estimator_conf[\'instance\'].fit(X_train, y_train)\n        gc.collect()\n        a, b = benchmark_estimator(estimator_conf[\'instance\'], X_test)\n        stats[estimator_conf[\'name\']] = {\'atomic\': a, \'bulk\': b}\n\n    cls_names = [estimator_conf[\'name\'] for estimator_conf in configuration[\n        \'estimators\']]\n    runtimes = [1e6 * stats[clf_name][\'atomic\'] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, \'atomic\', configuration)\n    runtimes = [1e6 * stats[clf_name][\'bulk\'] for clf_name in cls_names]\n    boxplot_runtimes(runtimes, \'bulk (%d)\' % configuration[\'n_test\'],\n                     configuration)\n\n\ndef n_feature_influence(estimators, n_train, n_test, n_features, percentile):\n    """"""\n    Estimate influence of the number of features on prediction time.\n\n    Parameters\n    ----------\n\n    estimators : dict of (name (str), estimator) to benchmark\n    n_train : nber of training instances (int)\n    n_test : nber of testing instances (int)\n    n_features : list of feature-space dimensionality to test (int)\n    percentile : percentile at which to measure the speed (int [0-100])\n\n    Returns:\n    --------\n\n    percentiles : dict(estimator_name,\n                       dict(n_features, percentile_perf_in_us))\n\n    """"""\n    percentiles = defaultdict(defaultdict)\n    for n in n_features:\n        print(""benchmarking with %d features"" % n)\n        X_train, y_train, X_test, y_test = generate_dataset(n_train, n_test, n)\n        for cls_name, estimator in estimators.items():\n            estimator.fit(X_train, y_train)\n            gc.collect()\n            runtimes = bulk_benchmark_estimator(estimator, X_test, 30, False)\n            percentiles[cls_name][n] = 1e6 * np.percentile(runtimes,\n                                                           percentile)\n    return percentiles\n\n\ndef plot_n_features_influence(percentiles, percentile):\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n    colors = [\'r\', \'g\', \'b\']\n    for i, cls_name in enumerate(percentiles.keys()):\n        x = np.array(sorted([n for n in percentiles[cls_name].keys()]))\n        y = np.array([percentiles[cls_name][n] for n in x])\n        plt.plot(x, y, color=colors[i], )\n    ax1.yaxis.grid(True, linestyle=\'-\', which=\'major\', color=\'lightgrey\',\n                   alpha=0.5)\n    ax1.set_axisbelow(True)\n    ax1.set_title(\'Evolution of Prediction Time with #Features\')\n    ax1.set_xlabel(\'#Features\')\n    ax1.set_ylabel(\'Prediction Time at %d%%-ile (us)\' % percentile)\n    plt.show()\n\n\ndef benchmark_throughputs(configuration, duration_secs=0.1):\n    """"""benchmark throughput for different estimators.""""""\n    X_train, y_train, X_test, y_test = generate_dataset(\n        configuration[\'n_train\'], configuration[\'n_test\'],\n        configuration[\'n_features\'])\n    throughputs = dict()\n    for estimator_config in configuration[\'estimators\']:\n        estimator_config[\'instance\'].fit(X_train, y_train)\n        start_time = time.time()\n        n_predictions = 0\n        while (time.time() - start_time) < duration_secs:\n            estimator_config[\'instance\'].predict(X_test[[0]])\n            n_predictions += 1\n        throughputs[estimator_config[\'name\']] = n_predictions / duration_secs\n    return throughputs\n\n\ndef plot_benchmark_throughput(throughputs, configuration):\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors = [\'r\', \'g\', \'b\']\n    cls_infos = [\'%s\\n(%d %s)\' % (estimator_conf[\'name\'],\n                                  estimator_conf[\'complexity_computer\'](\n                                      estimator_conf[\'instance\']),\n                                  estimator_conf[\'complexity_label\']) for\n                 estimator_conf in configuration[\'estimators\']]\n    cls_values = [throughputs[estimator_conf[\'name\']] for estimator_conf in\n                  configuration[\'estimators\']]\n    plt.bar(range(len(throughputs)), cls_values, width=0.5, color=colors)\n    ax.set_xticks(np.linspace(0.25, len(throughputs) - 0.75, len(throughputs)))\n    ax.set_xticklabels(cls_infos, fontsize=10)\n    ymax = max(cls_values) * 1.2\n    ax.set_ylim((0, ymax))\n    ax.set_ylabel(\'Throughput (predictions/sec)\')\n    ax.set_title(\'Prediction Throughput for different estimators (%d \'\n                 \'features)\' % configuration[\'n_features\'])\n    plt.show()\n\n\n# #############################################################################\n# Main code\n\nstart_time = time.time()\n\n# #############################################################################\n# Benchmark bulk/atomic prediction speed for various regressors\nconfiguration = {\n    \'n_train\': int(1e3),\n    \'n_test\': int(1e2),\n    \'n_features\': int(1e2),\n    \'estimators\': [\n        {\'name\': \'Linear Model\',\n         \'instance\': SGDRegressor(penalty=\'elasticnet\', alpha=0.01,\n                                  l1_ratio=0.25, tol=1e-4),\n         \'complexity_label\': \'non-zero coefficients\',\n         \'complexity_computer\': lambda clf: np.count_nonzero(clf.coef_)},\n        {\'name\': \'RandomForest\',\n         \'instance\': RandomForestRegressor(),\n         \'complexity_label\': \'estimators\',\n         \'complexity_computer\': lambda clf: clf.n_estimators},\n        {\'name\': \'SVR\',\n         \'instance\': SVR(kernel=\'rbf\'),\n         \'complexity_label\': \'support vectors\',\n         \'complexity_computer\': lambda clf: len(clf.support_vectors_)},\n    ]\n}\nbenchmark(configuration)\n\n# benchmark n_features influence on prediction speed\npercentile = 90\npercentiles = n_feature_influence({\'ridge\': Ridge()},\n                                  configuration[\'n_train\'],\n                                  configuration[\'n_test\'],\n                                  [100, 250, 500], percentile)\nplot_n_features_influence(percentiles, percentile)\n\n# benchmark throughput\nthroughputs = benchmark_throughputs(configuration)\nplot_benchmark_throughput(throughputs, configuration)\n\nstop_time = time.time()\nprint(""example run in %.2fs"" % (stop_time - start_time))\n'"
scikit-learn/_downloads/96907ff587384386d4ea9b1bbb7802ba/svm_gui.py,7,"b'""""""\n==========\nLibsvm GUI\n==========\n\nA simple graphical frontend for Libsvm mainly intended for didactic\npurposes. You can create data points by point and click and visualize\nthe decision region induced by different kernels and parameter settings.\n\nTo create positive examples click the left mouse button; to create\nnegative examples click the right button.\n\nIf all examples are from the same class, it uses a one-class SVM.\n\n""""""\n\nprint(__doc__)\n\n# Author: Peter Prettenhoer <peter.prettenhofer@gmail.com>\n#\n# License: BSD 3 clause\n\nimport matplotlib\nmatplotlib.use(\'TkAgg\')\n\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nfrom matplotlib.backends.backend_tkagg import NavigationToolbar2TkAgg\nfrom matplotlib.figure import Figure\nfrom matplotlib.contour import ContourSet\n\nimport sys\nimport numpy as np\nimport tkinter as Tk\n\nfrom sklearn import svm\nfrom sklearn.datasets import dump_svmlight_file\n\ny_min, y_max = -50, 50\nx_min, x_max = -50, 50\n\n\nclass Model:\n    """"""The Model which hold the data. It implements the\n    observable in the observer pattern and notifies the\n    registered observers on change event.\n    """"""\n\n    def __init__(self):\n        self.observers = []\n        self.surface = None\n        self.data = []\n        self.cls = None\n        self.surface_type = 0\n\n    def changed(self, event):\n        """"""Notify the observers. """"""\n        for observer in self.observers:\n            observer.update(event, self)\n\n    def add_observer(self, observer):\n        """"""Register an observer. """"""\n        self.observers.append(observer)\n\n    def set_surface(self, surface):\n        self.surface = surface\n\n    def dump_svmlight_file(self, file):\n        data = np.array(self.data)\n        X = data[:, 0:2]\n        y = data[:, 2]\n        dump_svmlight_file(X, y, file)\n\n\nclass Controller:\n    def __init__(self, model):\n        self.model = model\n        self.kernel = Tk.IntVar()\n        self.surface_type = Tk.IntVar()\n        # Whether or not a model has been fitted\n        self.fitted = False\n\n    def fit(self):\n        print(""fit the model"")\n        train = np.array(self.model.data)\n        X = train[:, 0:2]\n        y = train[:, 2]\n\n        C = float(self.complexity.get())\n        gamma = float(self.gamma.get())\n        coef0 = float(self.coef0.get())\n        degree = int(self.degree.get())\n        kernel_map = {0: ""linear"", 1: ""rbf"", 2: ""poly""}\n        if len(np.unique(y)) == 1:\n            clf = svm.OneClassSVM(kernel=kernel_map[self.kernel.get()],\n                                  gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X)\n        else:\n            clf = svm.SVC(kernel=kernel_map[self.kernel.get()], C=C,\n                          gamma=gamma, coef0=coef0, degree=degree)\n            clf.fit(X, y)\n        if hasattr(clf, \'score\'):\n            print(""Accuracy:"", clf.score(X, y) * 100)\n        X1, X2, Z = self.decision_surface(clf)\n        self.model.clf = clf\n        self.model.set_surface((X1, X2, Z))\n        self.model.surface_type = self.surface_type.get()\n        self.fitted = True\n        self.model.changed(""surface"")\n\n    def decision_surface(self, cls):\n        delta = 1\n        x = np.arange(x_min, x_max + delta, delta)\n        y = np.arange(y_min, y_max + delta, delta)\n        X1, X2 = np.meshgrid(x, y)\n        Z = cls.decision_function(np.c_[X1.ravel(), X2.ravel()])\n        Z = Z.reshape(X1.shape)\n        return X1, X2, Z\n\n    def clear_data(self):\n        self.model.data = []\n        self.fitted = False\n        self.model.changed(""clear"")\n\n    def add_example(self, x, y, label):\n        self.model.data.append((x, y, label))\n        self.model.changed(""example_added"")\n\n        # update decision surface if already fitted.\n        self.refit()\n\n    def refit(self):\n        """"""Refit the model if already fitted. """"""\n        if self.fitted:\n            self.fit()\n\n\nclass View:\n    """"""Test docstring. """"""\n    def __init__(self, root, controller):\n        f = Figure()\n        ax = f.add_subplot(111)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlim((x_min, x_max))\n        ax.set_ylim((y_min, y_max))\n        canvas = FigureCanvasTkAgg(f, master=root)\n        canvas.show()\n        canvas.get_tk_widget().pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas._tkcanvas.pack(side=Tk.TOP, fill=Tk.BOTH, expand=1)\n        canvas.mpl_connect(\'button_press_event\', self.onclick)\n        toolbar = NavigationToolbar2TkAgg(canvas, root)\n        toolbar.update()\n        self.controllbar = ControllBar(root, controller)\n        self.f = f\n        self.ax = ax\n        self.canvas = canvas\n        self.controller = controller\n        self.contours = []\n        self.c_labels = None\n        self.plot_kernels()\n\n    def plot_kernels(self):\n        self.ax.text(-50, -60, ""Linear: $u^T v$"")\n        self.ax.text(-20, -60, r""RBF: $\\exp (-\\gamma \\| u-v \\|^2)$"")\n        self.ax.text(10, -60, r""Poly: $(\\gamma \\, u^T v + r)^d$"")\n\n    def onclick(self, event):\n        if event.xdata and event.ydata:\n            if event.button == 1:\n                self.controller.add_example(event.xdata, event.ydata, 1)\n            elif event.button == 3:\n                self.controller.add_example(event.xdata, event.ydata, -1)\n\n    def update_example(self, model, idx):\n        x, y, l = model.data[idx]\n        if l == 1:\n            color = \'w\'\n        elif l == -1:\n            color = \'k\'\n        self.ax.plot([x], [y], ""%so"" % color, scalex=0.0, scaley=0.0)\n\n    def update(self, event, model):\n        if event == ""examples_loaded"":\n            for i in range(len(model.data)):\n                self.update_example(model, i)\n\n        if event == ""example_added"":\n            self.update_example(model, -1)\n\n        if event == ""clear"":\n            self.ax.clear()\n            self.ax.set_xticks([])\n            self.ax.set_yticks([])\n            self.contours = []\n            self.c_labels = None\n            self.plot_kernels()\n\n        if event == ""surface"":\n            self.remove_surface()\n            self.plot_support_vectors(model.clf.support_vectors_)\n            self.plot_decision_surface(model.surface, model.surface_type)\n\n        self.canvas.draw()\n\n    def remove_surface(self):\n        """"""Remove old decision surface.""""""\n        if len(self.contours) > 0:\n            for contour in self.contours:\n                if isinstance(contour, ContourSet):\n                    for lineset in contour.collections:\n                        lineset.remove()\n                else:\n                    contour.remove()\n            self.contours = []\n\n    def plot_support_vectors(self, support_vectors):\n        """"""Plot the support vectors by placing circles over the\n        corresponding data points and adds the circle collection\n        to the contours list.""""""\n        cs = self.ax.scatter(support_vectors[:, 0], support_vectors[:, 1],\n                             s=80, edgecolors=""k"", facecolors=""none"")\n        self.contours.append(cs)\n\n    def plot_decision_surface(self, surface, type):\n        X1, X2, Z = surface\n        if type == 0:\n            levels = [-1.0, 0.0, 1.0]\n            linestyles = [\'dashed\', \'solid\', \'dashed\']\n            colors = \'k\'\n            self.contours.append(self.ax.contour(X1, X2, Z, levels,\n                                                 colors=colors,\n                                                 linestyles=linestyles))\n        elif type == 1:\n            self.contours.append(self.ax.contourf(X1, X2, Z, 10,\n                                                  cmap=matplotlib.cm.bone,\n                                                  origin=\'lower\', alpha=0.85))\n            self.contours.append(self.ax.contour(X1, X2, Z, [0.0], colors=\'k\',\n                                                 linestyles=[\'solid\']))\n        else:\n            raise ValueError(""surface type unknown"")\n\n\nclass ControllBar:\n    def __init__(self, root, controller):\n        fm = Tk.Frame(root)\n        kernel_group = Tk.Frame(fm)\n        Tk.Radiobutton(kernel_group, text=""Linear"", variable=controller.kernel,\n                       value=0, command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(kernel_group, text=""RBF"", variable=controller.kernel,\n                       value=1, command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(kernel_group, text=""Poly"", variable=controller.kernel,\n                       value=2, command=controller.refit).pack(anchor=Tk.W)\n        kernel_group.pack(side=Tk.LEFT)\n\n        valbox = Tk.Frame(fm)\n        controller.complexity = Tk.StringVar()\n        controller.complexity.set(""1.0"")\n        c = Tk.Frame(valbox)\n        Tk.Label(c, text=""C:"", anchor=""e"", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(c, width=6, textvariable=controller.complexity).pack(\n            side=Tk.LEFT)\n        c.pack()\n\n        controller.gamma = Tk.StringVar()\n        controller.gamma.set(""0.01"")\n        g = Tk.Frame(valbox)\n        Tk.Label(g, text=""gamma:"", anchor=""e"", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(g, width=6, textvariable=controller.gamma).pack(side=Tk.LEFT)\n        g.pack()\n\n        controller.degree = Tk.StringVar()\n        controller.degree.set(""3"")\n        d = Tk.Frame(valbox)\n        Tk.Label(d, text=""degree:"", anchor=""e"", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(d, width=6, textvariable=controller.degree).pack(side=Tk.LEFT)\n        d.pack()\n\n        controller.coef0 = Tk.StringVar()\n        controller.coef0.set(""0"")\n        r = Tk.Frame(valbox)\n        Tk.Label(r, text=""coef0:"", anchor=""e"", width=7).pack(side=Tk.LEFT)\n        Tk.Entry(r, width=6, textvariable=controller.coef0).pack(side=Tk.LEFT)\n        r.pack()\n        valbox.pack(side=Tk.LEFT)\n\n        cmap_group = Tk.Frame(fm)\n        Tk.Radiobutton(cmap_group, text=""Hyperplanes"",\n                       variable=controller.surface_type, value=0,\n                       command=controller.refit).pack(anchor=Tk.W)\n        Tk.Radiobutton(cmap_group, text=""Surface"",\n                       variable=controller.surface_type, value=1,\n                       command=controller.refit).pack(anchor=Tk.W)\n\n        cmap_group.pack(side=Tk.LEFT)\n\n        train_button = Tk.Button(fm, text=\'Fit\', width=5,\n                                 command=controller.fit)\n        train_button.pack()\n        fm.pack(side=Tk.LEFT)\n        Tk.Button(fm, text=\'Clear\', width=5,\n                  command=controller.clear_data).pack(side=Tk.LEFT)\n\n\ndef get_parser():\n    from optparse import OptionParser\n    op = OptionParser()\n    op.add_option(""--output"",\n                  action=""store"", type=""str"", dest=""output"",\n                  help=""Path where to dump data."")\n    return op\n\n\ndef main(argv):\n    op = get_parser()\n    opts, args = op.parse_args(argv[1:])\n    root = Tk.Tk()\n    model = Model()\n    controller = Controller(model)\n    root.wm_title(""Scikit-learn Libsvm GUI"")\n    view = View(root, controller)\n    model.add_observer(view)\n    Tk.mainloop()\n\n    if opts.output:\n        model.dump_svmlight_file(opts.output)\n\nif __name__ == ""__main__"":\n    main(sys.argv)\n'"
scikit-learn/_downloads/97de47fe6488040c86acdcb8c97252e3/plot_lasso_model_selection.py,10,"b'""""""\n===================================================\nLasso model selection: Cross-Validation / AIC / BIC\n===================================================\n\nUse the Akaike information criterion (AIC), the Bayes Information\ncriterion (BIC) and cross-validation to select an optimal value\nof the regularization parameter alpha of the :ref:`lasso` estimator.\n\nResults obtained with LassoLarsIC are based on AIC/BIC criteria.\n\nInformation-criterion based model selection is very fast, but it\nrelies on a proper estimation of degrees of freedom, are\nderived for large samples (asymptotic results) and assume the model\nis correct, i.e. that the data are actually generated by this model.\nThey also tend to break when the problem is badly conditioned\n(more features than samples).\n\nFor cross-validation, we use 20-fold with 2 algorithms to compute the\nLasso path: coordinate descent, as implemented by the LassoCV class, and\nLars (least angle regression) as implemented by the LassoLarsCV class.\nBoth algorithms give roughly the same results. They differ with regards\nto their execution speed and sources of numerical errors.\n\nLars computes a path solution only for each kink in the path. As a\nresult, it is very efficient when there are only of few kinks, which is\nthe case if there are few features or samples. Also, it is able to\ncompute the full path without setting any meta parameter. On the\nopposite, coordinate descent compute the path points on a pre-specified\ngrid (here we use the default). Thus it is more efficient if the number\nof grid points is smaller than the number of kinks in the path. Such a\nstrategy can be interesting if the number of features is really large\nand there are enough samples to select a large amount. In terms of\nnumerical errors, for heavily correlated variables, Lars will accumulate\nmore errors, while the coordinate descent algorithm will only sample the\npath on a grid.\n\nNote how the optimal value of alpha varies for each fold. This\nillustrates why nested-cross validation is necessary when trying to\nevaluate the performance of a method for which a parameter is chosen by\ncross-validation: this choice of parameter may not be optimal for unseen\ndata.\n""""""\nprint(__doc__)\n\n# Author: Olivier Grisel, Gael Varoquaux, Alexandre Gramfort\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\nfrom sklearn import datasets\n\n# This is to avoid division by zero while doing np.log10\nEPSILON = 1e-4\n\nX, y = datasets.load_diabetes(return_X_y=True)\n\nrng = np.random.RandomState(42)\nX = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features\n\n# normalize data as done by Lars to allow for comparison\nX /= np.sqrt(np.sum(X ** 2, axis=0))\n\n# #############################################################################\n# LassoLarsIC: least angle regression with BIC/AIC criterion\n\nmodel_bic = LassoLarsIC(criterion=\'bic\')\nt1 = time.time()\nmodel_bic.fit(X, y)\nt_bic = time.time() - t1\nalpha_bic_ = model_bic.alpha_\n\nmodel_aic = LassoLarsIC(criterion=\'aic\')\nmodel_aic.fit(X, y)\nalpha_aic_ = model_aic.alpha_\n\n\ndef plot_ic_criterion(model, name, color):\n    alpha_ = model.alpha_ + EPSILON\n    alphas_ = model.alphas_ + EPSILON\n    criterion_ = model.criterion_\n    plt.plot(-np.log10(alphas_), criterion_, \'--\', color=color,\n             linewidth=3, label=\'%s criterion\' % name)\n    plt.axvline(-np.log10(alpha_), color=color, linewidth=3,\n                label=\'alpha: %s estimate\' % name)\n    plt.xlabel(\'-log(alpha)\')\n    plt.ylabel(\'criterion\')\n\n\nplt.figure()\nplot_ic_criterion(model_aic, \'AIC\', \'b\')\nplot_ic_criterion(model_bic, \'BIC\', \'r\')\nplt.legend()\nplt.title(\'Information-criterion for model selection (training time %.3fs)\'\n          % t_bic)\n\n# #############################################################################\n# LassoCV: coordinate descent\n\n# Compute paths\nprint(""Computing regularization path using the coordinate descent lasso..."")\nt1 = time.time()\nmodel = LassoCV(cv=20).fit(X, y)\nt_lasso_cv = time.time() - t1\n\n# Display results\nm_log_alphas = -np.log10(model.alphas_ + EPSILON)\n\nplt.figure()\nymin, ymax = 2300, 3800\nplt.plot(m_log_alphas, model.mse_path_, \':\')\nplt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), \'k\',\n         label=\'Average across the folds\', linewidth=2)\nplt.axvline(-np.log10(model.alpha_ + EPSILON), linestyle=\'--\', color=\'k\',\n            label=\'alpha: CV estimate\')\n\nplt.legend()\n\nplt.xlabel(\'-log(alpha)\')\nplt.ylabel(\'Mean square error\')\nplt.title(\'Mean square error on each fold: coordinate descent \'\n          \'(train time: %.2fs)\' % t_lasso_cv)\nplt.axis(\'tight\')\nplt.ylim(ymin, ymax)\n\n# #############################################################################\n# LassoLarsCV: least angle regression\n\n# Compute paths\nprint(""Computing regularization path using the Lars lasso..."")\nt1 = time.time()\nmodel = LassoLarsCV(cv=20).fit(X, y)\nt_lasso_lars_cv = time.time() - t1\n\n# Display results\nm_log_alphas = -np.log10(model.cv_alphas_ + EPSILON)\n\nplt.figure()\nplt.plot(m_log_alphas, model.mse_path_, \':\')\nplt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), \'k\',\n         label=\'Average across the folds\', linewidth=2)\nplt.axvline(-np.log10(model.alpha_), linestyle=\'--\', color=\'k\',\n            label=\'alpha CV\')\nplt.legend()\n\nplt.xlabel(\'-log(alpha)\')\nplt.ylabel(\'Mean square error\')\nplt.title(\'Mean square error on each fold: Lars (train time: %.2fs)\'\n          % t_lasso_lars_cv)\nplt.axis(\'tight\')\nplt.ylim(ymin, ymax)\n\nplt.show()\n'"
scikit-learn/_downloads/993047d73c54c28aad81f5316aa07a11/plot_permutation_test_for_classification.py,3,"b'""""""\n=================================================================\nTest with permutations the significance of a classification score\n=================================================================\n\nIn order to test if a classification score is significative a technique\nin repeating the classification procedure after randomizing, permuting,\nthe labels. The p-value is then given by the percentage of runs for\nwhich the score obtained is greater than the classification score\nobtained in the first place.\n\n""""""\n\n# Author:  Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import permutation_test_score\nfrom sklearn import datasets\n\n\n# #############################################################################\n# Loading a dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nn_classes = np.unique(y).size\n\n# Some noisy data not correlated\nrandom = np.random.RandomState(seed=0)\nE = random.normal(size=(len(X), 2200))\n\n# Add noisy data to the informative features for make the task harder\nX = np.c_[X, E]\n\nsvm = SVC(kernel=\'linear\')\ncv = StratifiedKFold(2)\n\nscore, permutation_scores, pvalue = permutation_test_score(\n    svm, X, y, scoring=""accuracy"", cv=cv, n_permutations=100, n_jobs=1)\n\nprint(""Classification score %s (pvalue : %s)"" % (score, pvalue))\n\n# #############################################################################\n# View histogram of permutation scores\nplt.hist(permutation_scores, 20, label=\'Permutation scores\',\n         edgecolor=\'black\')\nylim = plt.ylim()\n# BUG: vlines(..., linestyle=\'--\') fails on older versions of matplotlib\n# plt.vlines(score, ylim[0], ylim[1], linestyle=\'--\',\n#          color=\'g\', linewidth=3, label=\'Classification Score\'\n#          \' (pvalue %s)\' % pvalue)\n# plt.vlines(1.0 / n_classes, ylim[0], ylim[1], linestyle=\'--\',\n#          color=\'k\', linewidth=3, label=\'Luck\')\nplt.plot(2 * [score], ylim, \'--g\', linewidth=3,\n         label=\'Classification Score\'\n         \' (pvalue %s)\' % pvalue)\nplt.plot(2 * [1. / n_classes], ylim, \'--k\', linewidth=3, label=\'Luck\')\n\nplt.ylim(ylim)\nplt.legend()\nplt.xlabel(\'Score\')\nplt.show()\n'"
scikit-learn/_downloads/9d85412f24bfcead3c587a678bfc6bf8/plot_svm_margin.py,8,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nSVM Margins Example\n=========================================================\nThe plots below illustrate the effect the parameter `C` has\non the separation line. A large value of `C` basically tells\nour model that we do not have that much faith in our data\'s\ndistribution, and will only consider points close to line\nof separation.\n\nA small value of `C` includes more/all the observations, allowing\nthe margins to be calculated using all the data in the area.\n\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\n# we create 40 separable points\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\nY = [0] * 20 + [1] * 20\n\n# figure number\nfignum = 1\n\n# fit the model\nfor name, penalty in ((\'unreg\', 1), (\'reg\', 0.05)):\n\n    clf = svm.SVC(kernel=\'linear\', C=penalty)\n    clf.fit(X, Y)\n\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(-5, 5)\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n\n    # plot the parallels to the separating hyperplane that pass through the\n    # support vectors (margin away from hyperplane in direction\n    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in\n    # 2-d.\n    margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))\n    yy_down = yy - np.sqrt(1 + a ** 2) * margin\n    yy_up = yy + np.sqrt(1 + a ** 2) * margin\n\n    # plot the line, the points, and the nearest vectors to the plane\n    plt.figure(fignum, figsize=(4, 3))\n    plt.clf()\n    plt.plot(xx, yy, \'k-\')\n    plt.plot(xx, yy_down, \'k--\')\n    plt.plot(xx, yy_up, \'k--\')\n\n    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n                facecolors=\'none\', zorder=10, edgecolors=\'k\')\n    plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired,\n                edgecolors=\'k\')\n\n    plt.axis(\'tight\')\n    x_min = -4.8\n    x_max = 4.2\n    y_min = -6\n    y_max = 6\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.figure(fignum, figsize=(4, 3))\n    plt.pcolormesh(XX, YY, Z, cmap=plt.cm.Paired)\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    plt.xticks(())\n    plt.yticks(())\n    fignum = fignum + 1\n\nplt.show()\n'"
scikit-learn/_downloads/9dee0df6ab266151120674ee30bfe57f/plot_lda_qda.py,15,"b'""""""\n====================================================================\nLinear and Quadratic Discriminant Analysis with covariance ellipsoid\n====================================================================\n\nThis example plots the covariance ellipsoids of each class and\ndecision boundary learned by LDA and QDA. The ellipsoids display\nthe double standard deviation for each class. With LDA, the\nstandard deviation is the same for all the classes, while each\nclass has its own standard deviation with QDA.\n""""""\nprint(__doc__)\n\nfrom scipy import linalg\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom matplotlib import colors\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n# #############################################################################\n# Colormap\ncmap = colors.LinearSegmentedColormap(\n    \'red_blue_classes\',\n    {\'red\': [(0, 1, 1), (1, 0.7, 0.7)],\n     \'green\': [(0, 0.7, 0.7), (1, 0.7, 0.7)],\n     \'blue\': [(0, 0.7, 0.7), (1, 1, 1)]})\nplt.cm.register_cmap(cmap=cmap)\n\n\n# #############################################################################\n# Generate datasets\ndef dataset_fixed_cov():\n    \'\'\'Generate 2 Gaussians samples with the same covariance matrix\'\'\'\n    n, dim = 300, 2\n    np.random.seed(0)\n    C = np.array([[0., -0.23], [0.83, .23]])\n    X = np.r_[np.dot(np.random.randn(n, dim), C),\n              np.dot(np.random.randn(n, dim), C) + np.array([1, 1])]\n    y = np.hstack((np.zeros(n), np.ones(n)))\n    return X, y\n\n\ndef dataset_cov():\n    \'\'\'Generate 2 Gaussians samples with different covariance matrices\'\'\'\n    n, dim = 300, 2\n    np.random.seed(0)\n    C = np.array([[0., -1.], [2.5, .7]]) * 2.\n    X = np.r_[np.dot(np.random.randn(n, dim), C),\n              np.dot(np.random.randn(n, dim), C.T) + np.array([1, 4])]\n    y = np.hstack((np.zeros(n), np.ones(n)))\n    return X, y\n\n\n# #############################################################################\n# Plot functions\ndef plot_data(lda, X, y, y_pred, fig_index):\n    splot = plt.subplot(2, 2, fig_index)\n    if fig_index == 1:\n        plt.title(\'Linear Discriminant Analysis\')\n        plt.ylabel(\'Data with\\n fixed covariance\')\n    elif fig_index == 2:\n        plt.title(\'Quadratic Discriminant Analysis\')\n    elif fig_index == 3:\n        plt.ylabel(\'Data with\\n varying covariances\')\n\n    tp = (y == y_pred)  # True Positive\n    tp0, tp1 = tp[y == 0], tp[y == 1]\n    X0, X1 = X[y == 0], X[y == 1]\n    X0_tp, X0_fp = X0[tp0], X0[~tp0]\n    X1_tp, X1_fp = X1[tp1], X1[~tp1]\n\n    # class 0: dots\n    plt.scatter(X0_tp[:, 0], X0_tp[:, 1], marker=\'.\', color=\'red\')\n    plt.scatter(X0_fp[:, 0], X0_fp[:, 1], marker=\'x\',\n                s=20, color=\'#990000\')  # dark red\n\n    # class 1: dots\n    plt.scatter(X1_tp[:, 0], X1_tp[:, 1], marker=\'.\', color=\'blue\')\n    plt.scatter(X1_fp[:, 0], X1_fp[:, 1], marker=\'x\',\n                s=20, color=\'#000099\')  # dark blue\n\n    # class 0 and 1 : areas\n    nx, ny = 200, 100\n    x_min, x_max = plt.xlim()\n    y_min, y_max = plt.ylim()\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, nx),\n                         np.linspace(y_min, y_max, ny))\n    Z = lda.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z[:, 1].reshape(xx.shape)\n    plt.pcolormesh(xx, yy, Z, cmap=\'red_blue_classes\',\n                   norm=colors.Normalize(0., 1.), zorder=0)\n    plt.contour(xx, yy, Z, [0.5], linewidths=2., colors=\'white\')\n\n    # means\n    plt.plot(lda.means_[0][0], lda.means_[0][1],\n             \'*\', color=\'yellow\', markersize=15, markeredgecolor=\'grey\')\n    plt.plot(lda.means_[1][0], lda.means_[1][1],\n             \'*\', color=\'yellow\', markersize=15, markeredgecolor=\'grey\')\n\n    return splot\n\n\ndef plot_ellipse(splot, mean, cov, color):\n    v, w = linalg.eigh(cov)\n    u = w[0] / linalg.norm(w[0])\n    angle = np.arctan(u[1] / u[0])\n    angle = 180 * angle / np.pi  # convert to degrees\n    # filled Gaussian at 2 standard deviation\n    ell = mpl.patches.Ellipse(mean, 2 * v[0] ** 0.5, 2 * v[1] ** 0.5,\n                              180 + angle, facecolor=color,\n                              edgecolor=\'black\', linewidth=2)\n    ell.set_clip_box(splot.bbox)\n    ell.set_alpha(0.2)\n    splot.add_artist(ell)\n    splot.set_xticks(())\n    splot.set_yticks(())\n\n\ndef plot_lda_cov(lda, splot):\n    plot_ellipse(splot, lda.means_[0], lda.covariance_, \'red\')\n    plot_ellipse(splot, lda.means_[1], lda.covariance_, \'blue\')\n\n\ndef plot_qda_cov(qda, splot):\n    plot_ellipse(splot, qda.means_[0], qda.covariance_[0], \'red\')\n    plot_ellipse(splot, qda.means_[1], qda.covariance_[1], \'blue\')\n\n\nplt.figure(figsize=(10, 8), facecolor=\'white\')\nplt.suptitle(\'Linear Discriminant Analysis vs Quadratic Discriminant Analysis\',\n             y=0.98, fontsize=15)\nfor i, (X, y) in enumerate([dataset_fixed_cov(), dataset_cov()]):\n    # Linear Discriminant Analysis\n    lda = LinearDiscriminantAnalysis(solver=""svd"", store_covariance=True)\n    y_pred = lda.fit(X, y).predict(X)\n    splot = plot_data(lda, X, y, y_pred, fig_index=2 * i + 1)\n    plot_lda_cov(lda, splot)\n    plt.axis(\'tight\')\n\n    # Quadratic Discriminant Analysis\n    qda = QuadraticDiscriminantAnalysis(store_covariance=True)\n    y_pred = qda.fit(X, y).predict(X)\n    splot = plot_data(qda, X, y, y_pred, fig_index=2 * i + 2)\n    plot_qda_cov(qda, splot)\n    plt.axis(\'tight\')\nplt.tight_layout()\nplt.subplots_adjust(top=0.92)\nplt.show()\n'"
scikit-learn/_downloads/9e0c5410e772a5deb9b4ca66201ad8d5/plot_sparse_logistic_regression_20newsgroups.py,3,"b'""""""\n=====================================================\nMulticlass sparse logisitic regression on newgroups20\n=====================================================\n\nComparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression\nto classify documents from the newgroups20 dataset. Multinomial logistic\nregression yields more accurate results and is faster to train on the larger\nscale dataset.\n\nHere we use the l1 sparsity that trims the weights of not informative\nfeatures to zero. This is good if the goal is to extract the strongly\ndiscriminative vocabulary of each class. If the goal is to get the best\npredictive accuracy, it is better to use the non sparsity-inducing l2 penalty\ninstead.\n\nA more traditional (and possibly better) way to predict on a sparse subset of\ninput features would be to use univariate feature selection followed by a\ntraditional (l2-penalised) logistic regression model.\n""""""\nimport timeit\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.exceptions import ConvergenceWarning\n\nprint(__doc__)\n# Author: Arthur Mensch\n\nwarnings.filterwarnings(""ignore"", category=ConvergenceWarning,\n                        module=""sklearn"")\nt0 = timeit.default_timer()\n\n# We use SAGA solver\nsolver = \'saga\'\n\n# Turn down for faster run time\nn_samples = 10000\n\n# Memorized fetch_rcv1 for faster access\nX, y = fetch_20newsgroups_vectorized(\'all\', return_X_y=True)\nX = X[:n_samples]\ny = y[:n_samples]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    random_state=42,\n                                                    stratify=y,\n                                                    test_size=0.1)\ntrain_samples, n_features = X_train.shape\nn_classes = np.unique(y).shape[0]\n\nprint(\'Dataset 20newsgroup, train_samples=%i, n_features=%i, n_classes=%i\'\n      % (train_samples, n_features, n_classes))\n\nmodels = {\'ovr\': {\'name\': \'One versus Rest\', \'iters\': [1, 2, 4]},\n          \'multinomial\': {\'name\': \'Multinomial\', \'iters\': [1, 3, 7]}}\n\nfor model in models:\n    # Add initial chance-level values for plotting purpose\n    accuracies = [1 / n_classes]\n    times = [0]\n    densities = [1]\n\n    model_params = models[model]\n\n    # Small number of epochs for fast runtime\n    for this_max_iter in model_params[\'iters\']:\n        print(\'[model=%s, solver=%s] Number of epochs: %s\' %\n              (model_params[\'name\'], solver, this_max_iter))\n        lr = LogisticRegression(solver=solver,\n                                multi_class=model,\n                                penalty=\'l1\',\n                                max_iter=this_max_iter,\n                                random_state=42,\n                                )\n        t1 = timeit.default_timer()\n        lr.fit(X_train, y_train)\n        train_time = timeit.default_timer() - t1\n\n        y_pred = lr.predict(X_test)\n        accuracy = np.sum(y_pred == y_test) / y_test.shape[0]\n        density = np.mean(lr.coef_ != 0, axis=1) * 100\n        accuracies.append(accuracy)\n        densities.append(density)\n        times.append(train_time)\n    models[model][\'times\'] = times\n    models[model][\'densities\'] = densities\n    models[model][\'accuracies\'] = accuracies\n    print(\'Test accuracy for model %s: %.4f\' % (model, accuracies[-1]))\n    print(\'%% non-zero coefficients for model %s, \'\n          \'per class:\\n %s\' % (model, densities[-1]))\n    print(\'Run time (%i epochs) for model %s:\'\n          \'%.2f\' % (model_params[\'iters\'][-1], model, times[-1]))\n\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nfor model in models:\n    name = models[model][\'name\']\n    times = models[model][\'times\']\n    accuracies = models[model][\'accuracies\']\n    ax.plot(times, accuracies, marker=\'o\',\n            label=\'Model: %s\' % name)\n    ax.set_xlabel(\'Train time (s)\')\n    ax.set_ylabel(\'Test accuracy\')\nax.legend()\nfig.suptitle(\'Multinomial vs One-vs-Rest Logistic L1\\n\'\n             \'Dataset %s\' % \'20newsgroups\')\nfig.tight_layout()\nfig.subplots_adjust(top=0.85)\nrun_time = timeit.default_timer() - t0\nprint(\'Example run in %.3f s\' % run_time)\nplt.show()\n'"
scikit-learn/_downloads/9e2f8d95ea05761a1d7aa7024022bb98/plot_color_quantization.py,3,"b'# -*- coding: utf-8 -*-\n""""""\n==================================\nColor Quantization using K-Means\n==================================\n\nPerforms a pixel-wise Vector Quantization (VQ) of an image of the summer palace\n(China), reducing the number of colors required to show the image from 96,615\nunique colors to 64, while preserving the overall appearance quality.\n\nIn this example, pixels are represented in a 3D-space and K-means is used to\nfind 64 color clusters. In the image processing literature, the codebook\nobtained from K-means (the cluster centers) is called the color palette. Using\na single byte, up to 256 colors can be addressed, whereas an RGB encoding\nrequires 3 bytes per pixel. The GIF file format, for example, uses such a\npalette.\n\nFor comparison, a quantized image using a random codebook (colors picked up\nrandomly) is also shown.\n""""""\n# Authors: Robert Layton <robertlayton@gmail.com>\n#          Olivier Grisel <olivier.grisel@ensta.org>\n#          Mathieu Blondel <mathieu@mblondel.org>\n#\n# License: BSD 3 clause\n\nprint(__doc__)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise_distances_argmin\nfrom sklearn.datasets import load_sample_image\nfrom sklearn.utils import shuffle\nfrom time import time\n\nn_colors = 64\n\n# Load the Summer Palace photo\nchina = load_sample_image(""china.jpg"")\n\n# Convert to floats instead of the default 8 bits integer coding. Dividing by\n# 255 is important so that plt.imshow behaves works well on float data (need to\n# be in the range [0-1])\nchina = np.array(china, dtype=np.float64) / 255\n\n# Load Image and transform to a 2D numpy array.\nw, h, d = original_shape = tuple(china.shape)\nassert d == 3\nimage_array = np.reshape(china, (w * h, d))\n\nprint(""Fitting model on a small sub-sample of the data"")\nt0 = time()\nimage_array_sample = shuffle(image_array, random_state=0)[:1000]\nkmeans = KMeans(n_clusters=n_colors, random_state=0).fit(image_array_sample)\nprint(""done in %0.3fs."" % (time() - t0))\n\n# Get labels for all points\nprint(""Predicting color indices on the full image (k-means)"")\nt0 = time()\nlabels = kmeans.predict(image_array)\nprint(""done in %0.3fs."" % (time() - t0))\n\n\ncodebook_random = shuffle(image_array, random_state=0)[:n_colors]\nprint(""Predicting color indices on the full image (random)"")\nt0 = time()\nlabels_random = pairwise_distances_argmin(codebook_random,\n                                          image_array,\n                                          axis=0)\nprint(""done in %0.3fs."" % (time() - t0))\n\n\ndef recreate_image(codebook, labels, w, h):\n    """"""Recreate the (compressed) image from the code book & labels""""""\n    d = codebook.shape[1]\n    image = np.zeros((w, h, d))\n    label_idx = 0\n    for i in range(w):\n        for j in range(h):\n            image[i][j] = codebook[labels[label_idx]]\n            label_idx += 1\n    return image\n\n# Display all results, alongside original image\nplt.figure(1)\nplt.clf()\nplt.axis(\'off\')\nplt.title(\'Original image (96,615 colors)\')\nplt.imshow(china)\n\nplt.figure(2)\nplt.clf()\nplt.axis(\'off\')\nplt.title(\'Quantized image (64 colors, K-Means)\')\nplt.imshow(recreate_image(kmeans.cluster_centers_, labels, w, h))\n\nplt.figure(3)\nplt.clf()\nplt.axis(\'off\')\nplt.title(\'Quantized image (64 colors, Random)\')\nplt.imshow(recreate_image(codebook_random, labels_random, w, h))\nplt.show()\n'"
scikit-learn/_downloads/9f0a5cec4bceab41d2e889ba2cff52f0/plot_tomography_l1_reconstruction.py,17,"b'""""""\n======================================================================\nCompressive sensing: tomography reconstruction with L1 prior (Lasso)\n======================================================================\n\nThis example shows the reconstruction of an image from a set of parallel\nprojections, acquired along different angles. Such a dataset is acquired in\n**computed tomography** (CT).\n\nWithout any prior information on the sample, the number of projections\nrequired to reconstruct the image is of the order of the linear size\n``l`` of the image (in pixels). For simplicity we consider here a sparse\nimage, where only pixels on the boundary of objects have a non-zero\nvalue. Such data could correspond for example to a cellular material.\nNote however that most images are sparse in a different basis, such as\nthe Haar wavelets. Only ``l/7`` projections are acquired, therefore it is\nnecessary to use prior information available on the sample (its\nsparsity): this is an example of **compressive sensing**.\n\nThe tomography projection operation is a linear transformation. In\naddition to the data-fidelity term corresponding to a linear regression,\nwe penalize the L1 norm of the image to account for its sparsity. The\nresulting optimization problem is called the :ref:`lasso`. We use the\nclass :class:`sklearn.linear_model.Lasso`, that uses the coordinate descent\nalgorithm. Importantly, this implementation is more computationally efficient\non a sparse matrix, than the projection operator used here.\n\nThe reconstruction with L1 penalization gives a result with zero error\n(all pixels are successfully labeled with 0 or 1), even if noise was\nadded to the projections. In comparison, an L2 penalization\n(:class:`sklearn.linear_model.Ridge`) produces a large number of labeling\nerrors for the pixels. Important artifacts are observed on the\nreconstructed image, contrary to the L1 penalization. Note in particular\nthe circular artifact separating the pixels in the corners, that have\ncontributed to fewer projections than the central disk.\n""""""\n\nprint(__doc__)\n\n# Author: Emmanuelle Gouillart <emmanuelle.gouillart@nsup.org>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom scipy import sparse\nfrom scipy import ndimage\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\n\n\ndef _weights(x, dx=1, orig=0):\n    x = np.ravel(x)\n    floor_x = np.floor((x - orig) / dx).astype(np.int64)\n    alpha = (x - orig - floor_x * dx) / dx\n    return np.hstack((floor_x, floor_x + 1)), np.hstack((1 - alpha, alpha))\n\n\ndef _generate_center_coordinates(l_x):\n    X, Y = np.mgrid[:l_x, :l_x].astype(np.float64)\n    center = l_x / 2.\n    X += 0.5 - center\n    Y += 0.5 - center\n    return X, Y\n\n\ndef build_projection_operator(l_x, n_dir):\n    """""" Compute the tomography design matrix.\n\n    Parameters\n    ----------\n\n    l_x : int\n        linear size of image array\n\n    n_dir : int\n        number of angles at which projections are acquired.\n\n    Returns\n    -------\n    p : sparse matrix of shape (n_dir l_x, l_x**2)\n    """"""\n    X, Y = _generate_center_coordinates(l_x)\n    angles = np.linspace(0, np.pi, n_dir, endpoint=False)\n    data_inds, weights, camera_inds = [], [], []\n    data_unravel_indices = np.arange(l_x ** 2)\n    data_unravel_indices = np.hstack((data_unravel_indices,\n                                      data_unravel_indices))\n    for i, angle in enumerate(angles):\n        Xrot = np.cos(angle) * X - np.sin(angle) * Y\n        inds, w = _weights(Xrot, dx=1, orig=X.min())\n        mask = np.logical_and(inds >= 0, inds < l_x)\n        weights += list(w[mask])\n        camera_inds += list(inds[mask] + i * l_x)\n        data_inds += list(data_unravel_indices[mask])\n    proj_operator = sparse.coo_matrix((weights, (camera_inds, data_inds)))\n    return proj_operator\n\n\ndef generate_synthetic_data():\n    """""" Synthetic binary data """"""\n    rs = np.random.RandomState(0)\n    n_pts = 36\n    x, y = np.ogrid[0:l, 0:l]\n    mask_outer = (x - l / 2.) ** 2 + (y - l / 2.) ** 2 < (l / 2.) ** 2\n    mask = np.zeros((l, l))\n    points = l * rs.rand(2, n_pts)\n    mask[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n    mask = ndimage.gaussian_filter(mask, sigma=l / n_pts)\n    res = np.logical_and(mask > mask.mean(), mask_outer)\n    return np.logical_xor(res, ndimage.binary_erosion(res))\n\n\n# Generate synthetic images, and projections\nl = 128\nproj_operator = build_projection_operator(l, l // 7)\ndata = generate_synthetic_data()\nproj = proj_operator * data.ravel()[:, np.newaxis]\nproj += 0.15 * np.random.randn(*proj.shape)\n\n# Reconstruction with L2 (Ridge) penalization\nrgr_ridge = Ridge(alpha=0.2)\nrgr_ridge.fit(proj_operator, proj.ravel())\nrec_l2 = rgr_ridge.coef_.reshape(l, l)\n\n# Reconstruction with L1 (Lasso) penalization\n# the best value of alpha was determined using cross validation\n# with LassoCV\nrgr_lasso = Lasso(alpha=0.001)\nrgr_lasso.fit(proj_operator, proj.ravel())\nrec_l1 = rgr_lasso.coef_.reshape(l, l)\n\nplt.figure(figsize=(8, 3.3))\nplt.subplot(131)\nplt.imshow(data, cmap=plt.cm.gray, interpolation=\'nearest\')\nplt.axis(\'off\')\nplt.title(\'original image\')\nplt.subplot(132)\nplt.imshow(rec_l2, cmap=plt.cm.gray, interpolation=\'nearest\')\nplt.title(\'L2 penalization\')\nplt.axis(\'off\')\nplt.subplot(133)\nplt.imshow(rec_l1, cmap=plt.cm.gray, interpolation=\'nearest\')\nplt.title(\'L1 penalization\')\nplt.axis(\'off\')\n\nplt.subplots_adjust(hspace=0.01, wspace=0.01, top=1, bottom=0, left=0,\n                    right=1)\n\nplt.show()\n'"
scikit-learn/_downloads/9feef3f627858772423f75b0f4d22178/plot_logistic_multinomial.py,5,"b'""""""\n====================================================\nPlot multinomial and One-vs-Rest Logistic Regression\n====================================================\n\nPlot decision surface of multinomial and One-vs-Rest Logistic Regression.\nThe hyperplanes corresponding to the three One-vs-Rest (OVR) classifiers\nare represented by the dashed lines.\n""""""\nprint(__doc__)\n# Authors: Tom Dupre la Tour <tom.dupre-la-tour@m4x.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\n\n# make 3-class dataset for classification\ncenters = [[-5, 0], [0, 1.5], [5, -1]]\nX, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\ntransformation = [[0.4, 0.2], [-0.4, 1.2]]\nX = np.dot(X, transformation)\n\nfor multi_class in (\'multinomial\', \'ovr\'):\n    clf = LogisticRegression(solver=\'sag\', max_iter=100, random_state=42,\n                             multi_class=multi_class).fit(X, y)\n\n    # print the training scores\n    print(""training score : %.3f (%s)"" % (clf.score(X, y), multi_class))\n\n    # create a mesh to plot in\n    h = .02  # step size in the mesh\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.title(""Decision surface of LogisticRegression (%s)"" % multi_class)\n    plt.axis(\'tight\')\n\n    # Plot also the training points\n    colors = ""bry""\n    for i, color in zip(clf.classes_, colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired,\n                    edgecolor=\'black\', s=20)\n\n    # Plot the three one-against-all classifiers\n    xmin, xmax = plt.xlim()\n    ymin, ymax = plt.ylim()\n    coef = clf.coef_\n    intercept = clf.intercept_\n\n    def plot_hyperplane(c, color):\n        def line(x0):\n            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n        plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n                 ls=""--"", color=color)\n\n    for i, color in zip(clf.classes_, colors):\n        plot_hyperplane(i, color)\n\nplt.show()\n'"
scikit-learn/_downloads/a0f7bfbdd6b937de393369605d8e1bf9/plot_ensemble_oob.py,0,"b'""""""\n=============================\nOOB Errors for Random Forests\n=============================\n\nThe ``RandomForestClassifier`` is trained using *bootstrap aggregation*, where\neach new tree is fit from a bootstrap sample of the training observations\n:math:`z_i = (x_i, y_i)`. The *out-of-bag* (OOB) error is the average error for\neach :math:`z_i` calculated using predictions from the trees that do not\ncontain :math:`z_i` in their respective bootstrap sample. This allows the\n``RandomForestClassifier`` to be fit and validated whilst being trained [1]_.\n\nThe example below demonstrates how the OOB error can be measured at the\naddition of each new tree during training. The resulting plot allows a\npractitioner to approximate a suitable value of ``n_estimators`` at which the\nerror stabilizes.\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman, ""Elements of Statistical\n       Learning Ed. 2"", p592-593, Springer, 2009.\n\n""""""\nimport matplotlib.pyplot as plt\n\nfrom collections import OrderedDict\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Author: Kian Ho <hui.kian.ho@gmail.com>\n#         Gilles Louppe <g.louppe@gmail.com>\n#         Andreas Mueller <amueller@ais.uni-bonn.de>\n#\n# License: BSD 3 Clause\n\nprint(__doc__)\n\nRANDOM_STATE = 123\n\n# Generate a binary classification dataset.\nX, y = make_classification(n_samples=500, n_features=25,\n                           n_clusters_per_class=1, n_informative=15,\n                           random_state=RANDOM_STATE)\n\n# NOTE: Setting the `warm_start` construction parameter to `True` disables\n# support for parallelized ensembles but is necessary for tracking the OOB\n# error trajectory during training.\nensemble_clfs = [\n    (""RandomForestClassifier, max_features=\'sqrt\'"",\n        RandomForestClassifier(warm_start=True, oob_score=True,\n                               max_features=""sqrt"",\n                               random_state=RANDOM_STATE)),\n    (""RandomForestClassifier, max_features=\'log2\'"",\n        RandomForestClassifier(warm_start=True, max_features=\'log2\',\n                               oob_score=True,\n                               random_state=RANDOM_STATE)),\n    (""RandomForestClassifier, max_features=None"",\n        RandomForestClassifier(warm_start=True, max_features=None,\n                               oob_score=True,\n                               random_state=RANDOM_STATE))\n]\n\n# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.\nerror_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n\n# Range of `n_estimators` values to explore.\nmin_estimators = 15\nmax_estimators = 175\n\nfor label, clf in ensemble_clfs:\n    for i in range(min_estimators, max_estimators + 1):\n        clf.set_params(n_estimators=i)\n        clf.fit(X, y)\n\n        # Record the OOB error for each `n_estimators=i` setting.\n        oob_error = 1 - clf.oob_score_\n        error_rate[label].append((i, oob_error))\n\n# Generate the ""OOB error rate"" vs. ""n_estimators"" plot.\nfor label, clf_err in error_rate.items():\n    xs, ys = zip(*clf_err)\n    plt.plot(xs, ys, label=label)\n\nplt.xlim(min_estimators, max_estimators)\nplt.xlabel(""n_estimators"")\nplt.ylabel(""OOB error rate"")\nplt.legend(loc=""upper right"")\nplt.show()\n'"
scikit-learn/_downloads/a1b4e8f079eed1ddcb8761457bf1f3a4/plot_image_denoising.py,6,"b'""""""\n=========================================\nImage denoising using dictionary learning\n=========================================\n\nAn example comparing the effect of reconstructing noisy fragments\nof a raccoon face image using firstly online :ref:`DictionaryLearning` and\nvarious transform methods.\n\nThe dictionary is fitted on the distorted left half of the image, and\nsubsequently used to reconstruct the right half. Note that even better\nperformance could be achieved by fitting to an undistorted (i.e.\nnoiseless) image, but here we start from the assumption that it is not\navailable.\n\nA common practice for evaluating the results of image denoising is by looking\nat the difference between the reconstruction and the original image. If the\nreconstruction is perfect this will look like Gaussian noise.\n\nIt can be seen from the plots that the results of :ref:`omp` with two\nnon-zero coefficients is a bit less biased than when keeping only one\n(the edges look less prominent). It is in addition closer from the ground\ntruth in Frobenius norm.\n\nThe result of :ref:`least_angle_regression` is much more strongly biased: the\ndifference is reminiscent of the local intensity value of the original image.\n\nThresholding is clearly not useful for denoising, but it is here to show that\nit can produce a suggestive output with very high speed, and thus be useful\nfor other tasks such as object classification, where performance is not\nnecessarily related to visualisation.\n\n""""""\nprint(__doc__)\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy as sp\n\nfrom sklearn.decomposition import MiniBatchDictionaryLearning\nfrom sklearn.feature_extraction.image import extract_patches_2d\nfrom sklearn.feature_extraction.image import reconstruct_from_patches_2d\n\n\ntry:  # SciPy >= 0.16 have face in misc\n    from scipy.misc import face\n    face = face(gray=True)\nexcept ImportError:\n    face = sp.face(gray=True)\n\n# Convert from uint8 representation with values between 0 and 255 to\n# a floating point representation with values between 0 and 1.\nface = face / 255.\n\n# downsample for higher speed\nface = face[::4, ::4] + face[1::4, ::4] + face[::4, 1::4] + face[1::4, 1::4]\nface /= 4.0\nheight, width = face.shape\n\n# Distort the right half of the image\nprint(\'Distorting image...\')\ndistorted = face.copy()\ndistorted[:, width // 2:] += 0.075 * np.random.randn(height, width // 2)\n\n# Extract all reference patches from the left half of the image\nprint(\'Extracting reference patches...\')\nt0 = time()\npatch_size = (7, 7)\ndata = extract_patches_2d(distorted[:, :width // 2], patch_size)\ndata = data.reshape(data.shape[0], -1)\ndata -= np.mean(data, axis=0)\ndata /= np.std(data, axis=0)\nprint(\'done in %.2fs.\' % (time() - t0))\n\n# #############################################################################\n# Learn the dictionary from reference patches\n\nprint(\'Learning the dictionary...\')\nt0 = time()\ndico = MiniBatchDictionaryLearning(n_components=100, alpha=1, n_iter=500)\nV = dico.fit(data).components_\ndt = time() - t0\nprint(\'done in %.2fs.\' % dt)\n\nplt.figure(figsize=(4.2, 4))\nfor i, comp in enumerate(V[:100]):\n    plt.subplot(10, 10, i + 1)\n    plt.imshow(comp.reshape(patch_size), cmap=plt.cm.gray_r,\n               interpolation=\'nearest\')\n    plt.xticks(())\n    plt.yticks(())\nplt.suptitle(\'Dictionary learned from face patches\\n\' +\n             \'Train time %.1fs on %d patches\' % (dt, len(data)),\n             fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n\n# #############################################################################\n# Display the distorted image\n\ndef show_with_diff(image, reference, title):\n    """"""Helper function to display denoising""""""\n    plt.figure(figsize=(5, 3.3))\n    plt.subplot(1, 2, 1)\n    plt.title(\'Image\')\n    plt.imshow(image, vmin=0, vmax=1, cmap=plt.cm.gray,\n               interpolation=\'nearest\')\n    plt.xticks(())\n    plt.yticks(())\n    plt.subplot(1, 2, 2)\n    difference = image - reference\n\n    plt.title(\'Difference (norm: %.2f)\' % np.sqrt(np.sum(difference ** 2)))\n    plt.imshow(difference, vmin=-0.5, vmax=0.5, cmap=plt.cm.PuOr,\n               interpolation=\'nearest\')\n    plt.xticks(())\n    plt.yticks(())\n    plt.suptitle(title, size=16)\n    plt.subplots_adjust(0.02, 0.02, 0.98, 0.79, 0.02, 0.2)\n\nshow_with_diff(distorted, face, \'Distorted image\')\n\n# #############################################################################\n# Extract noisy patches and reconstruct them using the dictionary\n\nprint(\'Extracting noisy patches... \')\nt0 = time()\ndata = extract_patches_2d(distorted[:, width // 2:], patch_size)\ndata = data.reshape(data.shape[0], -1)\nintercept = np.mean(data, axis=0)\ndata -= intercept\nprint(\'done in %.2fs.\' % (time() - t0))\n\ntransform_algorithms = [\n    (\'Orthogonal Matching Pursuit\\n1 atom\', \'omp\',\n     {\'transform_n_nonzero_coefs\': 1}),\n    (\'Orthogonal Matching Pursuit\\n2 atoms\', \'omp\',\n     {\'transform_n_nonzero_coefs\': 2}),\n    (\'Least-angle regression\\n5 atoms\', \'lars\',\n     {\'transform_n_nonzero_coefs\': 5}),\n    (\'Thresholding\\n alpha=0.1\', \'threshold\', {\'transform_alpha\': .1})]\n\nreconstructions = {}\nfor title, transform_algorithm, kwargs in transform_algorithms:\n    print(title + \'...\')\n    reconstructions[title] = face.copy()\n    t0 = time()\n    dico.set_params(transform_algorithm=transform_algorithm, **kwargs)\n    code = dico.transform(data)\n    patches = np.dot(code, V)\n\n    patches += intercept\n    patches = patches.reshape(len(data), *patch_size)\n    if transform_algorithm == \'threshold\':\n        patches -= patches.min()\n        patches /= patches.max()\n    reconstructions[title][:, width // 2:] = reconstruct_from_patches_2d(\n        patches, (height, width // 2))\n    dt = time() - t0\n    print(\'done in %.2fs.\' % dt)\n    show_with_diff(reconstructions[title], face,\n                   title + \' (time: %.1fs)\' % dt)\n\nplt.show()\n'"
scikit-learn/_downloads/a219541989b80a36fd8557977fc03cda/plot_forest_importances_faces.py,0,"b'""""""\n=================================================\nPixel importances with a parallel forest of trees\n=================================================\n\nThis example shows the use of forests of trees to evaluate the importance\nof the pixels in an image classification task (faces). The hotter the pixel,\nthe more important.\n\nThe code below also illustrates how the construction and the computation\nof the predictions can be parallelized within multiple jobs.\n""""""\nprint(__doc__)\n\nfrom time import time\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Number of cores to use to perform parallel fitting of the forest model\nn_jobs = 1\n\n# Load the faces dataset\ndata = fetch_olivetti_faces()\nX, y = data.data, data.target\n\nmask = y < 5  # Limit to 5 classes\nX = X[mask]\ny = y[mask]\n\n# Build a forest and compute the pixel importances\nprint(""Fitting ExtraTreesClassifier on faces data with %d cores..."" % n_jobs)\nt0 = time()\nforest = ExtraTreesClassifier(n_estimators=1000,\n                              max_features=128,\n                              n_jobs=n_jobs,\n                              random_state=0)\n\nforest.fit(X, y)\nprint(""done in %0.3fs"" % (time() - t0))\nimportances = forest.feature_importances_\nimportances = importances.reshape(data.images[0].shape)\n\n# Plot pixel importances\nplt.matshow(importances, cmap=plt.cm.hot)\nplt.title(""Pixel importances with forests of trees"")\nplt.show()\n'"
scikit-learn/_downloads/a274372ce8dcf3c3fc38fbf952e5c699/plot_sgd_separating_hyperplane.py,5,"b'""""""\n=========================================\nSGD: Maximum margin separating hyperplane\n=========================================\n\nPlot the maximum margin separating hyperplane within a two-class\nseparable dataset using a linear Support Vector Machines classifier\ntrained using SGD.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.datasets import make_blobs\n\n# we create 50 separable points\nX, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n\n# fit the model\nclf = SGDClassifier(loss=""hinge"", alpha=0.01, max_iter=200)\n\nclf.fit(X, Y)\n\n# plot the line, the points, and the nearest vectors to the plane\nxx = np.linspace(-1, 5, 10)\nyy = np.linspace(-1, 5, 10)\n\nX1, X2 = np.meshgrid(xx, yy)\nZ = np.empty(X1.shape)\nfor (i, j), val in np.ndenumerate(X1):\n    x1 = val\n    x2 = X2[i, j]\n    p = clf.decision_function([[x1, x2]])\n    Z[i, j] = p[0]\nlevels = [-1.0, 0.0, 1.0]\nlinestyles = [\'dashed\', \'solid\', \'dashed\']\ncolors = \'k\'\nplt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,\n            edgecolor=\'black\', s=20)\n\nplt.axis(\'tight\')\nplt.show()\n'"
scikit-learn/_downloads/a2cb48441ddfb7427df120812ca8134f/plot_regression.py,5,"b'""""""\n============================\nNearest Neighbors regression\n============================\n\nDemonstrate the resolution of a regression problem\nusing a k-Nearest Neighbor and the interpolation of the\ntarget using both barycenter and constant weights.\n\n""""""\nprint(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#         Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#\n# License: BSD 3 clause (C) INRIA\n\n\n# #############################################################################\n# Generate sample data\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import neighbors\n\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\nT = np.linspace(0, 5, 500)[:, np.newaxis]\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 1 * (0.5 - np.random.rand(8))\n\n# #############################################################################\n# Fit regression model\nn_neighbors = 5\n\nfor i, weights in enumerate([\'uniform\', \'distance\']):\n    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)\n    y_ = knn.fit(X, y).predict(T)\n\n    plt.subplot(2, 1, i + 1)\n    plt.scatter(X, y, color=\'darkorange\', label=\'data\')\n    plt.plot(T, y_, color=\'navy\', label=\'prediction\')\n    plt.axis(\'tight\')\n    plt.legend()\n    plt.title(""KNeighborsRegressor (k = %i, weights = \'%s\')"" % (n_neighbors,\n                                                                weights))\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/a349e1855d5e8d2e481522149820ce8b/plot_classification.py,3,"b'""""""\n================================\nNearest Neighbors Classification\n================================\n\nSample usage of Nearest Neighbors classification.\nIt will plot the decision boundaries for each class.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 15\n\n# import some data to play with\niris = datasets.load_iris()\n\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap([\'orange\', \'cyan\', \'cornflowerblue\'])\ncmap_bold = ListedColormap([\'darkorange\', \'c\', \'darkblue\'])\n\nfor weights in [\'uniform\', \'distance\']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor=\'k\', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(""3-Class classification (k = %i, weights = \'%s\')""\n              % (n_neighbors, weights))\n\nplt.show()\n'"
scikit-learn/_downloads/a56c6d82805efc1a9894a54689506013/plot_gpr_co2.py,4,"b'""""""\n========================================================\nGaussian process regression (GPR) on Mauna Loa CO2 data.\n========================================================\n\nThis example is based on Section 5.4.3 of ""Gaussian Processes for Machine\nLearning"" [RW2006]. It illustrates an example of complex kernel engineering and\nhyperparameter optimization using gradient ascent on the\nlog-marginal-likelihood. The data consists of the monthly average atmospheric\nCO2 concentrations (in parts per million by volume (ppmv)) collected at the\nMauna Loa Observatory in Hawaii, between 1958 and 2001. The objective is to\nmodel the CO2 concentration as a function of the time t.\n\nThe kernel is composed of several terms that are responsible for explaining\ndifferent properties of the signal:\n\n- a long term, smooth rising trend is to be explained by an RBF kernel. The\n  RBF kernel with a large length-scale enforces this component to be smooth;\n  it is not enforced that the trend is rising which leaves this choice to the\n  GP. The specific length-scale and the amplitude are free hyperparameters.\n\n- a seasonal component, which is to be explained by the periodic\n  ExpSineSquared kernel with a fixed periodicity of 1 year. The length-scale\n  of this periodic component, controlling its smoothness, is a free parameter.\n  In order to allow decaying away from exact periodicity, the product with an\n  RBF kernel is taken. The length-scale of this RBF component controls the\n  decay time and is a further free parameter.\n\n- smaller, medium term irregularities are to be explained by a\n  RationalQuadratic kernel component, whose length-scale and alpha parameter,\n  which determines the diffuseness of the length-scales, are to be determined.\n  According to [RW2006], these irregularities can better be explained by\n  a RationalQuadratic than an RBF kernel component, probably because it can\n  accommodate several length-scales.\n\n- a ""noise"" term, consisting of an RBF kernel contribution, which shall\n  explain the correlated noise components such as local weather phenomena,\n  and a WhiteKernel contribution for the white noise. The relative amplitudes\n  and the RBF\'s length scale are further free parameters.\n\nMaximizing the log-marginal-likelihood after subtracting the target\'s mean\nyields the following kernel with an LML of -83.214::\n\n   34.4**2 * RBF(length_scale=41.8)\n   + 3.27**2 * RBF(length_scale=180) * ExpSineSquared(length_scale=1.44,\n                                                      periodicity=1)\n   + 0.446**2 * RationalQuadratic(alpha=17.7, length_scale=0.957)\n   + 0.197**2 * RBF(length_scale=0.138) + WhiteKernel(noise_level=0.0336)\n\nThus, most of the target signal (34.4ppm) is explained by a long-term rising\ntrend (length-scale 41.8 years). The periodic component has an amplitude of\n3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay\ntime indicates that we have a locally very close to periodic seasonal\ncomponent. The correlated noise has an amplitude of 0.197ppm with a length\nscale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the\noverall noise level is very small, indicating that the data can be very well\nexplained by the model. The figure shows also that the model makes very\nconfident predictions until around 2015.\n""""""\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#\n# License: BSD 3 clause\n\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels \\\n    import RBF, WhiteKernel, RationalQuadratic, ExpSineSquared\n\nprint(__doc__)\n\n\ndef load_mauna_loa_atmospheric_co2():\n    ml_data = fetch_openml(data_id=41187)\n    months = []\n    ppmv_sums = []\n    counts = []\n\n    y = ml_data.data[:, 0]\n    m = ml_data.data[:, 1]\n    month_float = y + (m - 1) / 12\n    ppmvs = ml_data.target\n\n    for month, ppmv in zip(month_float, ppmvs):\n        if not months or month != months[-1]:\n            months.append(month)\n            ppmv_sums.append(ppmv)\n            counts.append(1)\n        else:\n            # aggregate monthly sum to produce average\n            ppmv_sums[-1] += ppmv\n            counts[-1] += 1\n\n    months = np.asarray(months).reshape(-1, 1)\n    avg_ppmvs = np.asarray(ppmv_sums) / counts\n    return months, avg_ppmvs\n\n\nX, y = load_mauna_loa_atmospheric_co2()\n\n# Kernel with parameters given in GPML book\nk1 = 66.0**2 * RBF(length_scale=67.0)  # long term smooth rising trend\nk2 = 2.4**2 * RBF(length_scale=90.0) \\\n    * ExpSineSquared(length_scale=1.3, periodicity=1.0)  # seasonal component\n# medium term irregularity\nk3 = 0.66**2 \\\n    * RationalQuadratic(length_scale=1.2, alpha=0.78)\nk4 = 0.18**2 * RBF(length_scale=0.134) \\\n    + WhiteKernel(noise_level=0.19**2)  # noise terms\nkernel_gpml = k1 + k2 + k3 + k4\n\ngp = GaussianProcessRegressor(kernel=kernel_gpml, alpha=0,\n                              optimizer=None, normalize_y=True)\ngp.fit(X, y)\n\nprint(""GPML kernel: %s"" % gp.kernel_)\nprint(""Log-marginal-likelihood: %.3f""\n      % gp.log_marginal_likelihood(gp.kernel_.theta))\n\n# Kernel with optimized parameters\nk1 = 50.0**2 * RBF(length_scale=50.0)  # long term smooth rising trend\nk2 = 2.0**2 * RBF(length_scale=100.0) \\\n    * ExpSineSquared(length_scale=1.0, periodicity=1.0,\n                     periodicity_bounds=""fixed"")  # seasonal component\n# medium term irregularities\nk3 = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\nk4 = 0.1**2 * RBF(length_scale=0.1) \\\n    + WhiteKernel(noise_level=0.1**2,\n                  noise_level_bounds=(1e-3, np.inf))  # noise terms\nkernel = k1 + k2 + k3 + k4\n\ngp = GaussianProcessRegressor(kernel=kernel, alpha=0,\n                              normalize_y=True)\ngp.fit(X, y)\n\nprint(""\\nLearned kernel: %s"" % gp.kernel_)\nprint(""Log-marginal-likelihood: %.3f""\n      % gp.log_marginal_likelihood(gp.kernel_.theta))\n\nX_ = np.linspace(X.min(), X.max() + 30, 1000)[:, np.newaxis]\ny_pred, y_std = gp.predict(X_, return_std=True)\n\n# Illustration\nplt.scatter(X, y, c=\'k\')\nplt.plot(X_, y_pred)\nplt.fill_between(X_[:, 0], y_pred - y_std, y_pred + y_std,\n                 alpha=0.5, color=\'k\')\nplt.xlim(X_.min(), X_.max())\nplt.xlabel(""Year"")\nplt.ylabel(r""CO$_2$ in ppm"")\nplt.title(r""Atmospheric CO$_2$ concentration at Mauna Loa"")\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/a5bd8fd9cc5167bb706abad1879e8a2f/plot_beta_divergence.py,2,"b'""""""\n==============================\nBeta-divergence loss functions\n==============================\n\nA plot that compares the various Beta-divergence loss functions supported by\nthe Multiplicative-Update (\'mu\') solver in :class:`sklearn.decomposition.NMF`.\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition._nmf import _beta_divergence\n\nprint(__doc__)\n\nx = np.linspace(0.001, 4, 1000)\ny = np.zeros(x.shape)\n\ncolors = \'mbgyr\'\nfor j, beta in enumerate((0., 0.5, 1., 1.5, 2.)):\n    for i, xi in enumerate(x):\n        y[i] = _beta_divergence(1, xi, 1, beta)\n    name = ""beta = %1.1f"" % beta\n    plt.plot(x, y, label=name, color=colors[j])\n\nplt.xlabel(""x"")\nplt.title(""beta-divergence(1, x)"")\nplt.legend(loc=0)\nplt.axis([0, 4, 0, 3])\nplt.show()\n'"
scikit-learn/_downloads/a6c16156a42ef4653f106db7640bef0e/plot_outlier_detection_housing.py,4,"b'""""""\n====================================\nOutlier detection on a real data set\n====================================\n\nThis example illustrates the need for robust covariance estimation\non a real data set. It is useful both for outlier detection and for\na better understanding of the data structure.\n\nWe selected two sets of two variables from the Boston housing data set\nas an illustration of what kind of analysis can be done with several\noutlier detection tools. For the purpose of visualization, we are working\nwith two-dimensional examples, but one should be aware that things are\nnot so trivial in high-dimension, as it will be pointed out.\n\nIn both examples below, the main result is that the empirical covariance\nestimate, as a non-robust one, is highly influenced by the heterogeneous\nstructure of the observations. Although the robust covariance estimate is\nable to focus on the main mode of the data distribution, it sticks to the\nassumption that the data should be Gaussian distributed, yielding some biased\nestimation of the data structure, but yet accurate to some extent.\nThe One-Class SVM does not assume any parametric form of the data distribution\nand can therefore model the complex shape of the data much better.\n\nFirst example\n-------------\nThe first example illustrates how robust covariance estimation can help\nconcentrating on a relevant cluster when another one exists. Here, many\nobservations are confounded into one and break down the empirical covariance\nestimation.\nOf course, some screening tools would have pointed out the presence of two\nclusters (Support Vector Machines, Gaussian Mixture Models, univariate\noutlier detection, ...). But had it been a high-dimensional example, none\nof these could be applied that easily.\n\nSecond example\n--------------\nThe second example shows the ability of the Minimum Covariance Determinant\nrobust estimator of covariance to concentrate on the main mode of the data\ndistribution: the location seems to be well estimated, although the covariance\nis hard to estimate due to the banana-shaped distribution. Anyway, we can\nget rid of some outlying observations.\nThe One-Class SVM is able to capture the real data structure, but the\ndifficulty is to adjust its kernel bandwidth parameter so as to obtain\na good compromise between the shape of the data scatter matrix and the\nrisk of over-fitting the data.\n\n""""""\nprint(__doc__)\n\n# Author: Virgile Fritsch <virgile.fritsch@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.svm import OneClassSVM\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn.datasets import load_boston\n\n# Get data\nX1 = load_boston()[\'data\'][:, [8, 10]]  # two clusters\nX2 = load_boston()[\'data\'][:, [5, 12]]  # ""banana""-shaped\n\n# Define ""classifiers"" to be used\nclassifiers = {\n    ""Empirical Covariance"": EllipticEnvelope(support_fraction=1.,\n                                             contamination=0.261),\n    ""Robust Covariance (Minimum Covariance Determinant)"":\n    EllipticEnvelope(contamination=0.261),\n    ""OCSVM"": OneClassSVM(nu=0.261, gamma=0.05)}\ncolors = [\'m\', \'g\', \'b\']\nlegend1 = {}\nlegend2 = {}\n\n# Learn a frontier for outlier detection with several classifiers\nxx1, yy1 = np.meshgrid(np.linspace(-8, 28, 500), np.linspace(3, 40, 500))\nxx2, yy2 = np.meshgrid(np.linspace(3, 10, 500), np.linspace(-5, 45, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    plt.figure(1)\n    clf.fit(X1)\n    Z1 = clf.decision_function(np.c_[xx1.ravel(), yy1.ravel()])\n    Z1 = Z1.reshape(xx1.shape)\n    legend1[clf_name] = plt.contour(\n        xx1, yy1, Z1, levels=[0], linewidths=2, colors=colors[i])\n    plt.figure(2)\n    clf.fit(X2)\n    Z2 = clf.decision_function(np.c_[xx2.ravel(), yy2.ravel()])\n    Z2 = Z2.reshape(xx2.shape)\n    legend2[clf_name] = plt.contour(\n        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i])\n\nlegend1_values_list = list(legend1.values())\nlegend1_keys_list = list(legend1.keys())\n\n# Plot the results (= shape of the data points cloud)\nplt.figure(1)  # two clusters\nplt.title(""Outlier detection on a real data set (boston housing)"")\nplt.scatter(X1[:, 0], X1[:, 1], color=\'black\')\nbbox_args = dict(boxstyle=""round"", fc=""0.8"")\narrow_args = dict(arrowstyle=""->"")\nplt.annotate(""several confounded points"", xy=(24, 19),\n             xycoords=""data"", textcoords=""data"",\n             xytext=(13, 10), bbox=bbox_args, arrowprops=arrow_args)\nplt.xlim((xx1.min(), xx1.max()))\nplt.ylim((yy1.min(), yy1.max()))\nplt.legend((legend1_values_list[0].collections[0],\n            legend1_values_list[1].collections[0],\n            legend1_values_list[2].collections[0]),\n           (legend1_keys_list[0], legend1_keys_list[1], legend1_keys_list[2]),\n           loc=""upper center"",\n           prop=matplotlib.font_manager.FontProperties(size=12))\nplt.ylabel(""accessibility to radial highways"")\nplt.xlabel(""pupil-teacher ratio by town"")\n\nlegend2_values_list = list(legend2.values())\nlegend2_keys_list = list(legend2.keys())\n\nplt.figure(2)  # ""banana"" shape\nplt.title(""Outlier detection on a real data set (boston housing)"")\nplt.scatter(X2[:, 0], X2[:, 1], color=\'black\')\nplt.xlim((xx2.min(), xx2.max()))\nplt.ylim((yy2.min(), yy2.max()))\nplt.legend((legend2_values_list[0].collections[0],\n            legend2_values_list[1].collections[0],\n            legend2_values_list[2].collections[0]),\n           (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),\n           loc=""upper center"",\n           prop=matplotlib.font_manager.FontProperties(size=12))\nplt.ylabel(""% lower status of the population"")\nplt.xlabel(""average number of rooms per dwelling"")\n\nplt.show()\n'"
scikit-learn/_downloads/a79980d263aca8f1d2e7f322889c76ca/plot_coin_segmentation.py,1,"b'""""""\n================================================\nSegmenting the picture of greek coins in regions\n================================================\n\nThis example uses :ref:`spectral_clustering` on a graph created from\nvoxel-to-voxel difference on an image to break this image into multiple\npartly-homogeneous regions.\n\nThis procedure (spectral clustering on an image) is an efficient\napproximate solution for finding normalized graph cuts.\n\nThere are two options to assign labels:\n\n* with \'kmeans\' spectral clustering will cluster samples in the embedding space\n  using a kmeans algorithm\n* whereas \'discrete\' will iteratively search for the closest partition\n  space to the embedding space.\n""""""\nprint(__doc__)\n\n# Author: Gael Varoquaux <gael.varoquaux@normalesup.org>, Brian Cheung\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nfrom distutils.version import LooseVersion\nfrom scipy.ndimage.filters import gaussian_filter\nimport matplotlib.pyplot as plt\nimport skimage\nfrom skimage.data import coins\nfrom skimage.transform import rescale\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import spectral_clustering\n\n# these were introduced in skimage-0.14\nif LooseVersion(skimage.__version__) >= \'0.14\':\n    rescale_params = {\'anti_aliasing\': False, \'multichannel\': False}\nelse:\n    rescale_params = {}\n\n# load the coins as a numpy array\norig_coins = coins()\n\n# Resize it to 20% of the original size to speed up the processing\n# Applying a Gaussian filter for smoothing prior to down-scaling\n# reduces aliasing artifacts.\nsmoothened_coins = gaussian_filter(orig_coins, sigma=2)\nrescaled_coins = rescale(smoothened_coins, 0.2, mode=""reflect"",\n                         **rescale_params)\n\n# Convert the image into a graph with the value of the gradient on the\n# edges.\ngraph = image.img_to_graph(rescaled_coins)\n\n# Take a decreasing function of the gradient: an exponential\n# The smaller beta is, the more independent the segmentation is of the\n# actual image. For beta=1, the segmentation is close to a voronoi\nbeta = 10\neps = 1e-6\ngraph.data = np.exp(-beta * graph.data / graph.data.std()) + eps\n\n# Apply spectral clustering (this step goes much faster if you have pyamg\n# installed)\nN_REGIONS = 25\n\n#############################################################################\n# Visualize the resulting regions\n\nfor assign_labels in (\'kmeans\', \'discretize\'):\n    t0 = time.time()\n    labels = spectral_clustering(graph, n_clusters=N_REGIONS,\n                                 assign_labels=assign_labels, random_state=42)\n    t1 = time.time()\n    labels = labels.reshape(rescaled_coins.shape)\n\n    plt.figure(figsize=(5, 5))\n    plt.imshow(rescaled_coins, cmap=plt.cm.gray)\n    for l in range(N_REGIONS):\n        plt.contour(labels == l,\n                    colors=[plt.cm.nipy_spectral(l / float(N_REGIONS))])\n    plt.xticks(())\n    plt.yticks(())\n    title = \'Spectral clustering: %s, %.2fs\' % (assign_labels, (t1 - t0))\n    print(title)\n    plt.title(title)\nplt.show()\n'"
scikit-learn/_downloads/a8ac3e0872800feebe50a748a23a3997/plot_gpc_isoprobability.py,6,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=================================================================\nIso-probability lines for Gaussian Processes classification (GPC)\n=================================================================\n\nA two-dimensional classification example showing iso-probability lines for\nthe predicted probabilities.\n""""""\nprint(__doc__)\n\n# Author: Vincent Dubourg <vincent.dubourg@gmail.com>\n# Adapted to GaussianProcessClassifier:\n#         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import DotProduct, ConstantKernel as C\n\n# A few constants\nlim = 8\n\n\ndef g(x):\n    """"""The function to predict (classification will then consist in predicting\n    whether g(x) <= 0 or not)""""""\n    return 5. - x[:, 1] - .5 * x[:, 0] ** 2.\n\n# Design of experiments\nX = np.array([[-4.61611719, -6.00099547],\n              [4.10469096, 5.32782448],\n              [0.00000000, -0.50000000],\n              [-6.17289014, -4.6984743],\n              [1.3109306, -6.93271427],\n              [-5.03823144, 3.10584743],\n              [-2.87600388, 6.74310541],\n              [5.21301203, 4.26386883]])\n\n# Observations\ny = np.array(g(X) > 0, dtype=int)\n\n# Instantiate and fit Gaussian Process Model\nkernel = C(0.1, (1e-5, np.inf)) * DotProduct(sigma_0=0.1) ** 2\ngp = GaussianProcessClassifier(kernel=kernel)\ngp.fit(X, y)\nprint(""Learned kernel: %s "" % gp.kernel_)\n\n# Evaluate real function and the predicted probability\nres = 50\nx1, x2 = np.meshgrid(np.linspace(- lim, lim, res),\n                     np.linspace(- lim, lim, res))\nxx = np.vstack([x1.reshape(x1.size), x2.reshape(x2.size)]).T\n\ny_true = g(xx)\ny_prob = gp.predict_proba(xx)[:, 1]\ny_true = y_true.reshape((res, res))\ny_prob = y_prob.reshape((res, res))\n\n# Plot the probabilistic classification iso-values\nfig = plt.figure(1)\nax = fig.gca()\nax.axes.set_aspect(\'equal\')\nplt.xticks([])\nplt.yticks([])\nax.set_xticklabels([])\nax.set_yticklabels([])\nplt.xlabel(\'$x_1$\')\nplt.ylabel(\'$x_2$\')\n\ncax = plt.imshow(y_prob, cmap=cm.gray_r, alpha=0.8,\n                 extent=(-lim, lim, -lim, lim))\nnorm = plt.matplotlib.colors.Normalize(vmin=0., vmax=0.9)\ncb = plt.colorbar(cax, ticks=[0., 0.2, 0.4, 0.6, 0.8, 1.], norm=norm)\ncb.set_label(r\'${\\rm \\mathbb{P}}\\left[\\widehat{G}(\\mathbf{x}) \\leq 0\\right]$\')\nplt.clim(0, 1)\n\nplt.plot(X[y <= 0, 0], X[y <= 0, 1], \'r.\', markersize=12)\n\nplt.plot(X[y > 0, 0], X[y > 0, 1], \'b.\', markersize=12)\n\nplt.contour(x1, x2, y_true, [0.], colors=\'k\', linestyles=\'dashdot\')\n\ncs = plt.contour(x1, x2, y_prob, [0.666], colors=\'b\',\n                 linestyles=\'solid\')\nplt.clabel(cs, fontsize=11)\n\ncs = plt.contour(x1, x2, y_prob, [0.5], colors=\'k\',\n                 linestyles=\'dashed\')\nplt.clabel(cs, fontsize=11)\n\ncs = plt.contour(x1, x2, y_prob, [0.334], colors=\'r\',\n                 linestyles=\'solid\')\nplt.clabel(cs, fontsize=11)\n\nplt.show()\n'"
scikit-learn/_downloads/a8b8cc8522c1a21953d0c1ace3eb13c1/plot_multilabel.py,7,"b'# Authors: Vlad Niculae, Mathieu Blondel\n# License: BSD 3 clause\n""""""\n=========================\nMultilabel classification\n=========================\n\nThis example simulates a multi-label document classification problem. The\ndataset is generated randomly based on the following process:\n\n    - pick the number of labels: n ~ Poisson(n_labels)\n    - n times, choose a class c: c ~ Multinomial(theta)\n    - pick the document length: k ~ Poisson(length)\n    - k times, choose a word: w ~ Multinomial(theta_c)\n\nIn the above process, rejection sampling is used to make sure that n is more\nthan 2, and that the document length is never zero. Likewise, we reject classes\nwhich have already been chosen.  The documents that are assigned to both\nclasses are plotted surrounded by two colored circles.\n\nThe classification is performed by projecting to the first two principal\ncomponents found by PCA and CCA for visualisation purposes, followed by using\nthe :class:`sklearn.multiclass.OneVsRestClassifier` metaclassifier using two\nSVCs with linear kernels to learn a discriminative model for each class.\nNote that PCA is used to perform an unsupervised dimensionality reduction,\nwhile CCA is used to perform a supervised one.\n\nNote: in the plot, ""unlabeled samples"" does not mean that we don\'t know the\nlabels (as in semi-supervised learning) but that the samples simply do *not*\nhave a label.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import CCA\n\n\ndef plot_hyperplane(clf, min_x, max_x, linestyle, label):\n    # get the separating hyperplane\n    w = clf.coef_[0]\n    a = -w[0] / w[1]\n    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n    yy = a * xx - (clf.intercept_[0]) / w[1]\n    plt.plot(xx, yy, linestyle, label=label)\n\n\ndef plot_subfigure(X, Y, subplot, title, transform):\n    if transform == ""pca"":\n        X = PCA(n_components=2).fit_transform(X)\n    elif transform == ""cca"":\n        X = CCA(n_components=2).fit(X, Y).transform(X)\n    else:\n        raise ValueError\n\n    min_x = np.min(X[:, 0])\n    max_x = np.max(X[:, 0])\n\n    min_y = np.min(X[:, 1])\n    max_y = np.max(X[:, 1])\n\n    classif = OneVsRestClassifier(SVC(kernel=\'linear\'))\n    classif.fit(X, Y)\n\n    plt.subplot(2, 2, subplot)\n    plt.title(title)\n\n    zero_class = np.where(Y[:, 0])\n    one_class = np.where(Y[:, 1])\n    plt.scatter(X[:, 0], X[:, 1], s=40, c=\'gray\', edgecolors=(0, 0, 0))\n    plt.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors=\'b\',\n                facecolors=\'none\', linewidths=2, label=\'Class 1\')\n    plt.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors=\'orange\',\n                facecolors=\'none\', linewidths=2, label=\'Class 2\')\n\n    plot_hyperplane(classif.estimators_[0], min_x, max_x, \'k--\',\n                    \'Boundary\\nfor class 1\')\n    plot_hyperplane(classif.estimators_[1], min_x, max_x, \'k-.\',\n                    \'Boundary\\nfor class 2\')\n    plt.xticks(())\n    plt.yticks(())\n\n    plt.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n    plt.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n    if subplot == 2:\n        plt.xlabel(\'First principal component\')\n        plt.ylabel(\'Second principal component\')\n        plt.legend(loc=""upper left"")\n\n\nplt.figure(figsize=(8, 6))\n\nX, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                      allow_unlabeled=True,\n                                      random_state=1)\n\nplot_subfigure(X, Y, 1, ""With unlabeled samples + CCA"", ""cca"")\nplot_subfigure(X, Y, 2, ""With unlabeled samples + PCA"", ""pca"")\n\nX, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n                                      allow_unlabeled=False,\n                                      random_state=1)\n\nplot_subfigure(X, Y, 3, ""Without unlabeled samples + CCA"", ""cca"")\nplot_subfigure(X, Y, 4, ""Without unlabeled samples + PCA"", ""pca"")\n\nplt.subplots_adjust(.04, .02, .97, .94, .09, .2)\nplt.show()\n'"
scikit-learn/_downloads/a8f05730b3d8418d58eb0693d8c71fd0/plot_gpc_iris.py,5,"b'""""""\n=====================================================\nGaussian process classification (GPC) on iris dataset\n=====================================================\n\nThis example illustrates the predicted probability of GPC for an isotropic\nand anisotropic RBF kernel on a two-dimensional version for the iris-dataset.\nThe anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by\nassigning different length-scales to the two feature dimensions.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\n\n# import some data to play with\niris = datasets.load_iris()\nX = iris.data[:, :2]  # we only take the first two features.\ny = np.array(iris.target, dtype=int)\n\nh = .02  # step size in the mesh\n\nkernel = 1.0 * RBF([1.0])\ngpc_rbf_isotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\nkernel = 1.0 * RBF([1.0, 1.0])\ngpc_rbf_anisotropic = GaussianProcessClassifier(kernel=kernel).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\ntitles = [""Isotropic RBF"", ""Anisotropic RBF""]\nplt.figure(figsize=(10, 5))\nfor i, clf in enumerate((gpc_rbf_isotropic, gpc_rbf_anisotropic)):\n    # Plot the predicted probabilities. For that, we will assign a color to\n    # each point in the mesh [x_min, m_max]x[y_min, y_max].\n    plt.subplot(1, 2, i + 1)\n\n    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape((xx.shape[0], xx.shape[1], 3))\n    plt.imshow(Z, extent=(x_min, x_max, y_min, y_max), origin=""lower"")\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=np.array([""r"", ""g"", ""b""])[y],\n                edgecolors=(0, 0, 0))\n    plt.xlabel(\'Sepal length\')\n    plt.ylabel(\'Sepal width\')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(""%s, LML: %.3f"" %\n              (titles[i], clf.log_marginal_likelihood(clf.kernel_.theta)))\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/a97925cf600e40918a1f95b2d2d1c785/plot_cv_predict.py,0,"b'""""""\n====================================\nPlotting Cross-Validated Predictions\n====================================\n\nThis example shows how to use\n:func:`~sklearn.model_selection.cross_val_predict` to visualize prediction\nerrors.\n\n""""""\nfrom sklearn import datasets\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n\nlr = linear_model.LinearRegression()\nX, y = datasets.load_boston(return_X_y=True)\n\n# cross_val_predict returns an array of the same size as `y` where each entry\n# is a prediction obtained by cross validation:\npredicted = cross_val_predict(lr, X, y, cv=10)\n\nfig, ax = plt.subplots()\nax.scatter(y, predicted, edgecolors=(0, 0, 0))\nax.plot([y.min(), y.max()], [y.min(), y.max()], \'k--\', lw=4)\nax.set_xlabel(\'Measured\')\nax.set_ylabel(\'Predicted\')\nplt.show()\n'"
scikit-learn/_downloads/aba12475fc672944f65bd879c3021259/plot_incremental_pca.py,1,"b'""""""\n\n===============\nIncremental PCA\n===============\n\nIncremental principal component analysis (IPCA) is typically used as a\nreplacement for principal component analysis (PCA) when the dataset to be\ndecomposed is too large to fit in memory. IPCA builds a low-rank approximation\nfor the input data using an amount of memory which is independent of the\nnumber of input data samples. It is still dependent on the input data features,\nbut changing the batch size allows for control of memory usage.\n\nThis example serves as a visual check that IPCA is able to find a similar\nprojection of the data to PCA (to a sign flip), while only processing a\nfew samples at a time. This can be considered a ""toy example"", as IPCA is\nintended for large datasets which do not fit in main memory, requiring\nincremental approaches.\n\n""""""\nprint(__doc__)\n\n# Authors: Kyle Kastner\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.decomposition import PCA, IncrementalPCA\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\nn_components = 2\nipca = IncrementalPCA(n_components=n_components, batch_size=10)\nX_ipca = ipca.fit_transform(X)\n\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X)\n\ncolors = [\'navy\', \'turquoise\', \'darkorange\']\n\nfor X_transformed, title in [(X_ipca, ""Incremental PCA""), (X_pca, ""PCA"")]:\n    plt.figure(figsize=(8, 8))\n    for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n        plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],\n                    color=color, lw=2, label=target_name)\n\n    if ""Incremental"" in title:\n        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\n        plt.title(title + "" of iris dataset\\nMean absolute unsigned error ""\n                  ""%.6f"" % err)\n    else:\n        plt.title(title + "" of iris dataset"")\n    plt.legend(loc=""best"", shadow=False, scatterpoints=1)\n    plt.axis([-4, 4, -1.5, 1.5])\n\nplt.show()\n'"
scikit-learn/_downloads/ac1d5acba50726cf83ef00f558d45ed5/plot_voting_decision_regions.py,3,"b'""""""\n==================================================\nPlot the decision boundaries of a VotingClassifier\n==================================================\n\n.. currentmodule:: sklearn\n\nPlot the decision boundaries of a :class:`~ensemble.VotingClassifier` for two\nfeatures of the Iris dataset.\n\nPlot the class probabilities of the first sample in a toy dataset predicted by\nthree different classifiers and averaged by the\n:class:`~ensemble.VotingClassifier`.\n\nFirst, three exemplary classifiers are initialized\n(:class:`~tree.DecisionTreeClassifier`,\n:class:`~neighbors.KNeighborsClassifier`, and :class:`~svm.SVC`) and used to\ninitialize a soft-voting :class:`~ensemble.VotingClassifier` with weights `[2,\n1, 2]`, which means that the predicted probabilities of the\n:class:`~tree.DecisionTreeClassifier` and :class:`~svm.SVC` each count 2 times\nas much as the weights of the :class:`~neighbors.KNeighborsClassifier`\nclassifier when the averaged probability is calculated.\n\n""""""\nprint(__doc__)\n\nfrom itertools import product\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\n# Loading some example data\niris = datasets.load_iris()\nX = iris.data[:, [0, 2]]\ny = iris.target\n\n# Training classifiers\nclf1 = DecisionTreeClassifier(max_depth=4)\nclf2 = KNeighborsClassifier(n_neighbors=7)\nclf3 = SVC(gamma=.1, kernel=\'rbf\', probability=True)\neclf = VotingClassifier(estimators=[(\'dt\', clf1), (\'knn\', clf2),\n                                    (\'svc\', clf3)],\n                        voting=\'soft\', weights=[2, 1, 2])\n\nclf1.fit(X, y)\nclf2.fit(X, y)\nclf3.fit(X, y)\neclf.fit(X, y)\n\n# Plotting decision regions\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(2, 2, sharex=\'col\', sharey=\'row\', figsize=(10, 8))\n\nfor idx, clf, tt in zip(product([0, 1], [0, 1]),\n                        [clf1, clf2, clf3, eclf],\n                        [\'Decision Tree (depth=4)\', \'KNN (k=7)\',\n                         \'Kernel SVM\', \'Soft Voting\']):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx[0], idx[1]].scatter(X[:, 0], X[:, 1], c=y,\n                                  s=20, edgecolor=\'k\')\n    axarr[idx[0], idx[1]].set_title(tt)\n\nplt.show()\n'"
scikit-learn/_downloads/af22e5dafd7225b078cf08c179924d9d/plot_nca_dim_reduction.py,1,"b'""""""\n==============================================================\nDimensionality Reduction with Neighborhood Components Analysis\n==============================================================\n\nSample usage of Neighborhood Components Analysis for dimensionality reduction.\n\nThis example compares different (linear) dimensionality reduction methods\napplied on the Digits data set. The data set contains images of digits from\n0 to 9 with approximately 180 samples of each class. Each image is of\ndimension 8x8 = 64, and is reduced to a two-dimensional data point.\n\nPrincipal Component Analysis (PCA) applied to this data identifies the\ncombination of attributes (principal components, or directions in the\nfeature space) that account for the most variance in the data. Here we\nplot the different samples on the 2 first principal components.\n\nLinear Discriminant Analysis (LDA) tries to identify attributes that\naccount for the most variance *between classes*. In particular,\nLDA, in contrast to PCA, is a supervised method, using known class labels.\n\nNeighborhood Components Analysis (NCA) tries to find a feature space such\nthat a stochastic nearest neighbor algorithm will give the best accuracy.\nLike LDA, it is a supervised method.\n\nOne can see that NCA enforces a clustering of the data that is visually\nmeaningful despite the large reduction in dimension.\n""""""\n\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import (KNeighborsClassifier,\n                               NeighborhoodComponentsAnalysis)\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nprint(__doc__)\n\nn_neighbors = 3\nrandom_state = 0\n\n# Load Digits dataset\nX, y = datasets.load_digits(return_X_y=True)\n\n# Split into train/test\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.5, stratify=y,\n                     random_state=random_state)\n\ndim = len(X[0])\nn_classes = len(np.unique(y))\n\n# Reduce dimension to 2 with PCA\npca = make_pipeline(StandardScaler(),\n                    PCA(n_components=2, random_state=random_state))\n\n# Reduce dimension to 2 with LinearDiscriminantAnalysis\nlda = make_pipeline(StandardScaler(),\n                    LinearDiscriminantAnalysis(n_components=2))\n\n# Reduce dimension to 2 with NeighborhoodComponentAnalysis\nnca = make_pipeline(StandardScaler(),\n                    NeighborhoodComponentsAnalysis(n_components=2,\n                                                   random_state=random_state))\n\n# Use a nearest neighbor classifier to evaluate the methods\nknn = KNeighborsClassifier(n_neighbors=n_neighbors)\n\n# Make a list of the methods to be compared\ndim_reduction_methods = [(\'PCA\', pca), (\'LDA\', lda), (\'NCA\', nca)]\n\n# plt.figure()\nfor i, (name, model) in enumerate(dim_reduction_methods):\n    plt.figure()\n    # plt.subplot(1, 3, i + 1, aspect=1)\n\n    # Fit the method\'s model\n    model.fit(X_train, y_train)\n\n    # Fit a nearest neighbor classifier on the embedded training set\n    knn.fit(model.transform(X_train), y_train)\n\n    # Compute the nearest neighbor accuracy on the embedded test set\n    acc_knn = knn.score(model.transform(X_test), y_test)\n\n    # Embed the data set in 2 dimensions using the fitted model\n    X_embedded = model.transform(X)\n\n    # Plot the projected points and show the evaluation score\n    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, s=30, cmap=\'Set1\')\n    plt.title(""{}, KNN (k={})\\nTest accuracy = {:.2f}"".format(name,\n                                                              n_neighbors,\n                                                              acc_knn))\nplt.show()\n'"
scikit-learn/_downloads/b0dd6dde92265b50b9f701f6cf0e94bb/plot_kmeans_stability_low_dim_dense.py,6,"b'""""""\n============================================================\nEmpirical evaluation of the impact of k-means initialization\n============================================================\n\nEvaluate the ability of k-means initializations strategies to make\nthe algorithm convergence robust as measured by the relative standard\ndeviation of the inertia of the clustering (i.e. the sum of squared\ndistances to the nearest cluster center).\n\nThe first plot shows the best inertia reached for each combination\nof the model (``KMeans`` or ``MiniBatchKMeans``) and the init method\n(``init=""random""`` or ``init=""kmeans++""``) for increasing values of the\n``n_init`` parameter that controls the number of initializations.\n\nThe second plot demonstrate one single run of the ``MiniBatchKMeans``\nestimator using a ``init=""random""`` and ``n_init=1``. This run leads to\na bad convergence (local optimum) with estimated centers stuck\nbetween ground truth clusters.\n\nThe dataset used for evaluation is a 2D grid of isotropic Gaussian\nclusters widely spaced.\n""""""\nprint(__doc__)\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom sklearn.utils import shuffle\nfrom sklearn.utils import check_random_state\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\n\nrandom_state = np.random.RandomState(0)\n\n# Number of run (with randomly generated dataset) for each strategy so as\n# to be able to compute an estimate of the standard deviation\nn_runs = 5\n\n# k-means models can do several random inits so as to be able to trade\n# CPU time for convergence robustness\nn_init_range = np.array([1, 5, 10, 15, 20])\n\n# Datasets generation parameters\nn_samples_per_center = 100\ngrid_size = 3\nscale = 0.1\nn_clusters = grid_size ** 2\n\n\ndef make_data(random_state, n_samples_per_center, grid_size, scale):\n    random_state = check_random_state(random_state)\n    centers = np.array([[i, j]\n                        for i in range(grid_size)\n                        for j in range(grid_size)])\n    n_clusters_true, n_features = centers.shape\n\n    noise = random_state.normal(\n        scale=scale, size=(n_samples_per_center, centers.shape[1]))\n\n    X = np.concatenate([c + noise for c in centers])\n    y = np.concatenate([[i] * n_samples_per_center\n                        for i in range(n_clusters_true)])\n    return shuffle(X, y, random_state=random_state)\n\n# Part 1: Quantitative evaluation of various init methods\n\nplt.figure()\nplots = []\nlegends = []\n\ncases = [\n    (KMeans, \'k-means++\', {}),\n    (KMeans, \'random\', {}),\n    (MiniBatchKMeans, \'k-means++\', {\'max_no_improvement\': 3}),\n    (MiniBatchKMeans, \'random\', {\'max_no_improvement\': 3, \'init_size\': 500}),\n]\n\nfor factory, init, params in cases:\n    print(""Evaluation of %s with %s init"" % (factory.__name__, init))\n    inertia = np.empty((len(n_init_range), n_runs))\n\n    for run_id in range(n_runs):\n        X, y = make_data(run_id, n_samples_per_center, grid_size, scale)\n        for i, n_init in enumerate(n_init_range):\n            km = factory(n_clusters=n_clusters, init=init, random_state=run_id,\n                         n_init=n_init, **params).fit(X)\n            inertia[i, run_id] = km.inertia_\n    p = plt.errorbar(n_init_range, inertia.mean(axis=1), inertia.std(axis=1))\n    plots.append(p[0])\n    legends.append(""%s with %s init"" % (factory.__name__, init))\n\nplt.xlabel(\'n_init\')\nplt.ylabel(\'inertia\')\nplt.legend(plots, legends)\nplt.title(""Mean inertia for various k-means init across %d runs"" % n_runs)\n\n# Part 2: Qualitative visual inspection of the convergence\n\nX, y = make_data(random_state, n_samples_per_center, grid_size, scale)\nkm = MiniBatchKMeans(n_clusters=n_clusters, init=\'random\', n_init=1,\n                     random_state=random_state).fit(X)\n\nplt.figure()\nfor k in range(n_clusters):\n    my_members = km.labels_ == k\n    color = cm.nipy_spectral(float(k) / n_clusters, 1)\n    plt.plot(X[my_members, 0], X[my_members, 1], \'o\', marker=\'.\', c=color)\n    cluster_center = km.cluster_centers_[k]\n    plt.plot(cluster_center[0], cluster_center[1], \'o\',\n             markerfacecolor=color, markeredgecolor=\'k\', markersize=6)\n    plt.title(""Example cluster allocation with a single random init\\n""\n              ""with MiniBatchKMeans"")\n\nplt.show()\n'"
scikit-learn/_downloads/b0fe3be5cfb8256403c21e22b1c18017/plot_column_transformer.py,1,"b'""""""\n==================================================\nColumn Transformer with Heterogeneous Data Sources\n==================================================\n\nDatasets can often contain components of that require different feature\nextraction and processing pipelines.  This scenario might occur when:\n\n1. Your dataset consists of heterogeneous data types (e.g. raster images and\n   text captions)\n2. Your dataset is stored in a Pandas DataFrame and different columns\n   require different processing pipelines.\n\nThis example demonstrates how to use\n:class:`sklearn.compose.ColumnTransformer` on a dataset containing\ndifferent types of features.  We use the 20-newsgroups dataset and compute\nstandard bag-of-words features for the subject line and body in separate\npipelines as well as ad hoc features on the body. We combine them (with\nweights) using a ColumnTransformer and finally train a classifier on the\ncombined set of features.\n\nThe choice of features is not particularly helpful, but serves to illustrate\nthe technique.\n""""""\n\n# Author: Matt Terry <matt.terry@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import LinearSVC\n\n\nclass TextStats(TransformerMixin, BaseEstimator):\n    """"""Extract features from each document for DictVectorizer""""""\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, posts):\n        return [{\'length\': len(text),\n                 \'num_sentences\': text.count(\'.\')}\n                for text in posts]\n\n\nclass SubjectBodyExtractor(TransformerMixin, BaseEstimator):\n    """"""Extract the subject & body from a usenet post in a single pass.\n\n    Takes a sequence of strings and produces a dict of sequences.  Keys are\n    `subject` and `body`.\n    """"""\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, posts):\n        # construct object dtype array with two columns\n        # first column = \'subject\' and second column = \'body\'\n        features = np.empty(shape=(len(posts), 2), dtype=object)\n        for i, text in enumerate(posts):\n            headers, _, bod = text.partition(\'\\n\\n\')\n            features[i, 1] = bod\n\n            prefix = \'Subject:\'\n            sub = \'\'\n            for line in headers.split(\'\\n\'):\n                if line.startswith(prefix):\n                    sub = line[len(prefix):]\n                    break\n            features[i, 0] = sub\n\n        return features\n\n\npipeline = Pipeline([\n    # Extract the subject & body\n    (\'subjectbody\', SubjectBodyExtractor()),\n\n    # Use ColumnTransformer to combine the features from subject and body\n    (\'union\', ColumnTransformer(\n        [\n            # Pulling features from the post\'s subject line (first column)\n            (\'subject\', TfidfVectorizer(min_df=50), 0),\n\n            # Pipeline for standard bag-of-words model for body (second column)\n            (\'body_bow\', Pipeline([\n                (\'tfidf\', TfidfVectorizer()),\n                (\'best\', TruncatedSVD(n_components=50)),\n            ]), 1),\n\n            # Pipeline for pulling ad hoc features from post\'s body\n            (\'body_stats\', Pipeline([\n                (\'stats\', TextStats()),  # returns a list of dicts\n                (\'vect\', DictVectorizer()),  # list of dicts -> feature matrix\n            ]), 1),\n        ],\n\n        # weight components in ColumnTransformer\n        transformer_weights={\n            \'subject\': 0.8,\n            \'body_bow\': 0.5,\n            \'body_stats\': 1.0,\n        }\n    )),\n\n    # Use a SVC classifier on the combined features\n    (\'svc\', LinearSVC(dual=False)),\n], verbose=True)\n\n# limit the list of categories to make running this example faster.\ncategories = [\'alt.atheism\', \'talk.religion.misc\']\nX_train, y_train = fetch_20newsgroups(random_state=1,\n                                      subset=\'train\',\n                                      categories=categories,\n                                      remove=(\'footers\', \'quotes\'),\n                                      return_X_y=True)\nX_test, y_test = fetch_20newsgroups(random_state=1,\n                                    subset=\'test\',\n                                    categories=categories,\n                                    remove=(\'footers\', \'quotes\'),\n                                    return_X_y=True)\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\nprint(classification_report(y_test, y_pred))\n'"
scikit-learn/_downloads/b27cadd7650d4a430ad5d99f53f73cea/plot_lasso_and_elasticnet.py,9,"b'""""""\n========================================\nLasso and Elastic Net for Sparse Signals\n========================================\n\nEstimates Lasso and Elastic-Net regression models on a manually generated\nsparse signal corrupted with an additive noise. Estimated coefficients are\ncompared with the ground-truth.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import r2_score\n\n# #############################################################################\n# Generate some sparse data to play with\nnp.random.seed(42)\n\nn_samples, n_features = 50, 100\nX = np.random.randn(n_samples, n_features)\n\n# Decreasing coef w. alternated signs for visualization\nidx = np.arange(n_features)\ncoef = (-1) ** idx * np.exp(-idx / 10)\ncoef[10:] = 0  # sparsify coef\ny = np.dot(X, coef)\n\n# Add noise\ny += 0.01 * np.random.normal(size=n_samples)\n\n# Split data in train set and test set\nn_samples = X.shape[0]\nX_train, y_train = X[:n_samples // 2], y[:n_samples // 2]\nX_test, y_test = X[n_samples // 2:], y[n_samples // 2:]\n\n# #############################################################################\n# Lasso\nfrom sklearn.linear_model import Lasso\n\nalpha = 0.1\nlasso = Lasso(alpha=alpha)\n\ny_pred_lasso = lasso.fit(X_train, y_train).predict(X_test)\nr2_score_lasso = r2_score(y_test, y_pred_lasso)\nprint(lasso)\nprint(""r^2 on test data : %f"" % r2_score_lasso)\n\n# #############################################################################\n# ElasticNet\nfrom sklearn.linear_model import ElasticNet\n\nenet = ElasticNet(alpha=alpha, l1_ratio=0.7)\n\ny_pred_enet = enet.fit(X_train, y_train).predict(X_test)\nr2_score_enet = r2_score(y_test, y_pred_enet)\nprint(enet)\nprint(""r^2 on test data : %f"" % r2_score_enet)\n\nm, s, _ = plt.stem(np.where(enet.coef_)[0], enet.coef_[enet.coef_ != 0],\n                   markerfmt=\'x\', label=\'Elastic net coefficients\',\n                   use_line_collection=True)\nplt.setp([m, s], color=""#2ca02c"")\nm, s, _ = plt.stem(np.where(lasso.coef_)[0], lasso.coef_[lasso.coef_ != 0],\n                   markerfmt=\'x\', label=\'Lasso coefficients\',\n                   use_line_collection=True)\nplt.setp([m, s], color=\'#ff7f0e\')\nplt.stem(np.where(coef)[0], coef[coef != 0], label=\'true coefficients\',\n         markerfmt=\'bx\', use_line_collection=True)\n\nplt.legend(loc=\'best\')\nplt.title(""Lasso $R^2$: %.3f, Elastic Net $R^2$: %.3f""\n          % (r2_score_lasso, r2_score_enet))\nplt.show()\n'"
scikit-learn/_downloads/b2a29951e2910bb082d6958358ec27cb/plot_digits_kde_sampling.py,1,"b'""""""\n=========================\nKernel Density Estimation\n=========================\n\nThis example shows how kernel density estimation (KDE), a powerful\nnon-parametric density estimation technique, can be used to learn\na generative model for a dataset.  With this generative model in place,\nnew samples can be drawn.  These new samples reflect the underlying model\nof the data.\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.neighbors import KernelDensity\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\n\n# load the data\ndigits = load_digits()\n\n# project the 64-dimensional data to a lower dimension\npca = PCA(n_components=15, whiten=False)\ndata = pca.fit_transform(digits.data)\n\n# use grid search cross-validation to optimize the bandwidth\nparams = {\'bandwidth\': np.logspace(-1, 1, 20)}\ngrid = GridSearchCV(KernelDensity(), params)\ngrid.fit(data)\n\nprint(""best bandwidth: {0}"".format(grid.best_estimator_.bandwidth))\n\n# use the best estimator to compute the kernel density estimate\nkde = grid.best_estimator_\n\n# sample 44 new points from the data\nnew_data = kde.sample(44, random_state=0)\nnew_data = pca.inverse_transform(new_data)\n\n# turn data into a 4x11 grid\nnew_data = new_data.reshape((4, 11, -1))\nreal_data = digits.data[:44].reshape((4, 11, -1))\n\n# plot real digits and resampled digits\nfig, ax = plt.subplots(9, 11, subplot_kw=dict(xticks=[], yticks=[]))\nfor j in range(11):\n    ax[4, j].set_visible(False)\n    for i in range(4):\n        im = ax[i, j].imshow(real_data[i, j].reshape((8, 8)),\n                             cmap=plt.cm.binary, interpolation=\'nearest\')\n        im.set_clim(0, 16)\n        im = ax[i + 5, j].imshow(new_data[i, j].reshape((8, 8)),\n                                 cmap=plt.cm.binary, interpolation=\'nearest\')\n        im.set_clim(0, 16)\n\nax[0, 5].set_title(\'Selection from the input data\')\nax[5, 5].set_title(\'""New"" digits drawn from the kernel density model\')\n\nplt.show()\n'"
scikit-learn/_downloads/b33596b5b7fb2f7f0f85930e7220b260/plot_ridge_path.py,3,"b'""""""\n===========================================================\nPlot Ridge coefficients as a function of the regularization\n===========================================================\n\nShows the effect of collinearity in the coefficients of an estimator.\n\n.. currentmodule:: sklearn.linear_model\n\n:class:`Ridge` Regression is the estimator used in this example.\nEach color represents a different feature of the\ncoefficient vector, and this is displayed as a function of the\nregularization parameter.\n\nThis example also shows the usefulness of applying Ridge regression\nto highly ill-conditioned matrices. For such matrices, a slight\nchange in the target variable can cause huge variances in the\ncalculated weights. In such cases, it is useful to set a certain\nregularization (alpha) to reduce this variation (noise).\n\nWhen alpha is very large, the regularization effect dominates the\nsquared loss function and the coefficients tend to zero.\nAt the end of the path, as alpha tends toward zero\nand the solution tends towards the ordinary least squares, coefficients\nexhibit big oscillations. In practise it is necessary to tune alpha\nin such a way that a balance is maintained between both.\n""""""\n\n# Author: Fabian Pedregosa -- <fabian.pedregosa@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\n\n# X is the 10x10 Hilbert matrix\nX = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])\ny = np.ones(10)\n\n# #############################################################################\n# Compute paths\n\nn_alphas = 200\nalphas = np.logspace(-10, -2, n_alphas)\n\ncoefs = []\nfor a in alphas:\n    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)\n\n# #############################################################################\n# Display results\n\nax = plt.gca()\n\nax.plot(alphas, coefs)\nax.set_xscale(\'log\')\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.xlabel(\'alpha\')\nplt.ylabel(\'weights\')\nplt.title(\'Ridge coefficients as a function of the regularization\')\nplt.axis(\'tight\')\nplt.show()\n'"
scikit-learn/_downloads/b4bbe4c0686161332ccd44edb6bcd8b9/plot_label_propagation_versus_svm_iris.py,6,"b'""""""\n=====================================================================\nDecision boundary of label propagation versus SVM on the Iris dataset\n=====================================================================\n\nComparison for decision boundary generated on iris dataset\nbetween Label Propagation and SVM.\n\nThis demonstrates Label Propagation learning a good boundary\neven with a small amount of labeled data.\n\n""""""\nprint(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn import svm\nfrom sklearn.semi_supervised import LabelSpreading\n\nrng = np.random.RandomState(0)\n\niris = datasets.load_iris()\n\nX = iris.data[:, :2]\ny = iris.target\n\n# step size in the mesh\nh = .02\n\ny_30 = np.copy(y)\ny_30[rng.rand(len(y)) < 0.3] = -1\ny_50 = np.copy(y)\ny_50[rng.rand(len(y)) < 0.5] = -1\n# we create an instance of SVM and fit out data. We do not scale our\n# data since we want to plot the support vectors\nls30 = (LabelSpreading().fit(X, y_30), y_30)\nls50 = (LabelSpreading().fit(X, y_50), y_50)\nls100 = (LabelSpreading().fit(X, y), y)\nrbf_svc = (svm.SVC(kernel=\'rbf\', gamma=.5).fit(X, y), y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# title for the plots\ntitles = [\'Label Spreading 30% data\',\n          \'Label Spreading 50% data\',\n          \'Label Spreading 100% data\',\n          \'SVC with rbf kernel\']\n\ncolor_map = {-1: (1, 1, 1), 0: (0, 0, .9), 1: (1, 0, 0), 2: (.8, .6, 0)}\n\nfor i, (clf, y_train) in enumerate((ls30, ls50, ls100, rbf_svc)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    plt.subplot(2, 2, i + 1)\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.axis(\'off\')\n\n    # Plot also the training points\n    colors = [color_map[y] for y in y_train]\n    plt.scatter(X[:, 0], X[:, 1], c=colors, edgecolors=\'black\')\n\n    plt.title(titles[i])\n\nplt.suptitle(""Unlabeled points are colored white"", y=0.1)\nplt.show()\n'"
scikit-learn/_downloads/b4c5d9ecdff9573f2d2be0c4d42abdb0/plot_changed_only_pprint_parameter.py,0,"b'""""""\n=================================\nCompact estimator representations\n=================================\n\nThis example illustrates the use of the print_changed_only global parameter.\n\nSetting print_changed_only to True will alterate the representation of\nestimators to only show the parameters that have been set to non-default\nvalues. This can be used to have more compact representations.\n""""""\nprint(__doc__)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import set_config\n\n\nlr = LogisticRegression(penalty=\'l1\')\nprint(\'Default representation:\')\nprint(lr)\n# LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n#                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n#                    multi_class=\'auto\', n_jobs=None, penalty=\'l1\',\n#                    random_state=None, solver=\'warn\', tol=0.0001, verbose=0,\n#                    warm_start=False)\n\nset_config(print_changed_only=True)\nprint(\'\\nWith changed_only option:\')\nprint(lr)\n# LogisticRegression(penalty=\'l1\')\n'"
scikit-learn/_downloads/b50aa52d149afc036303322deb93e679/plot_kmeans_silhouette_analysis.py,1,"b'""""""\n===============================================================================\nSelecting the number of clusters with silhouette analysis on KMeans clustering\n===============================================================================\n\nSilhouette analysis can be used to study the separation distance between the\nresulting clusters. The silhouette plot displays a measure of how close each\npoint in one cluster is to points in the neighboring clusters and thus provides\na way to assess parameters like number of clusters visually. This measure has a\nrange of [-1, 1].\n\nSilhouette coefficients (as these values are referred to as) near +1 indicate\nthat the sample is far away from the neighboring clusters. A value of 0\nindicates that the sample is on or very close to the decision boundary between\ntwo neighboring clusters and negative values indicate that those samples might\nhave been assigned to the wrong cluster.\n\nIn this example the silhouette analysis is used to choose an optimal value for\n``n_clusters``. The silhouette plot shows that the ``n_clusters`` value of 3, 5\nand 6 are a bad pick for the given data due to the presence of clusters with\nbelow average silhouette scores and also due to wide fluctuations in the size\nof the silhouette plots. Silhouette analysis is more ambivalent in deciding\nbetween 2 and 4.\n\nAlso from the thickness of the silhouette plot the cluster size can be\nvisualized. The silhouette plot for cluster 0 when ``n_clusters`` is equal to\n2, is bigger in size owing to the grouping of the 3 sub clusters into one big\ncluster. However when the ``n_clusters`` is equal to 4, all the plots are more\nor less of similar thickness and hence are of similar sizes as can be also\nverified from the labelled scatter plot on the right.\n""""""\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n# Generating the sample data from make_blobs\n# This particular setting has one distinct cluster and 3 clusters placed close\n# together.\nX, y = make_blobs(n_samples=500,\n                  n_features=2,\n                  centers=4,\n                  cluster_std=1,\n                  center_box=(-10.0, 10.0),\n                  shuffle=True,\n                  random_state=1)  # For reproducibility\n\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(""For n_clusters ="", n_clusters,\n          ""The average silhouette_score is :"", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(""The silhouette plot for the various clusters."")\n    ax1.set_xlabel(""The silhouette coefficient values"")\n    ax1.set_ylabel(""Cluster label"")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=""red"", linestyle=""--"")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker=\'.\', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor=\'k\')\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(centers[:, 0], centers[:, 1], marker=\'o\',\n                c=""white"", alpha=1, s=200, edgecolor=\'k\')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\'$%d$\' % i, alpha=1,\n                    s=50, edgecolor=\'k\')\n\n    ax2.set_title(""The visualization of the clustered data."")\n    ax2.set_xlabel(""Feature space for the 1st feature"")\n    ax2.set_ylabel(""Feature space for the 2nd feature"")\n\n    plt.suptitle((""Silhouette analysis for KMeans clustering on sample data ""\n                  ""with n_clusters = %d"" % n_clusters),\n                 fontsize=14, fontweight=\'bold\')\n\nplt.show()\n'"
scikit-learn/_downloads/b549d4a5a773c51e38d6018cb058e1b4/plot_gpr_prior_posterior.py,9,"b'""""""\n==========================================================================\nIllustration of prior and posterior Gaussian process for different kernels\n==========================================================================\n\nThis example illustrates the prior and posterior of a GPR with different\nkernels. Mean, standard deviation, and 10 samples are shown for both prior\nand posterior.\n""""""\nprint(__doc__)\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n#\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,\n                                              ExpSineSquared, DotProduct,\n                                              ConstantKernel)\n\n\nkernels = [1.0 * RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0)),\n           1.0 * RationalQuadratic(length_scale=1.0, alpha=0.1),\n           1.0 * ExpSineSquared(length_scale=1.0, periodicity=3.0,\n                                length_scale_bounds=(0.1, 10.0),\n                                periodicity_bounds=(1.0, 10.0)),\n           ConstantKernel(0.1, (0.01, 10.0))\n               * (DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),\n           1.0 * Matern(length_scale=1.0, length_scale_bounds=(1e-1, 10.0),\n                        nu=1.5)]\n\nfor kernel in kernels:\n    # Specify Gaussian Process\n    gp = GaussianProcessRegressor(kernel=kernel)\n\n    # Plot prior\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    X_ = np.linspace(0, 5, 100)\n    y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)\n    plt.plot(X_, y_mean, \'k\', lw=3, zorder=9)\n    plt.fill_between(X_, y_mean - y_std, y_mean + y_std,\n                     alpha=0.2, color=\'k\')\n    y_samples = gp.sample_y(X_[:, np.newaxis], 10)\n    plt.plot(X_, y_samples, lw=1)\n    plt.xlim(0, 5)\n    plt.ylim(-3, 3)\n    plt.title(""Prior (kernel:  %s)"" % kernel, fontsize=12)\n\n    # Generate data and fit GP\n    rng = np.random.RandomState(4)\n    X = rng.uniform(0, 5, 10)[:, np.newaxis]\n    y = np.sin((X[:, 0] - 2.5) ** 2)\n    gp.fit(X, y)\n\n    # Plot posterior\n    plt.subplot(2, 1, 2)\n    X_ = np.linspace(0, 5, 100)\n    y_mean, y_std = gp.predict(X_[:, np.newaxis], return_std=True)\n    plt.plot(X_, y_mean, \'k\', lw=3, zorder=9)\n    plt.fill_between(X_, y_mean - y_std, y_mean + y_std,\n                     alpha=0.2, color=\'k\')\n\n    y_samples = gp.sample_y(X_[:, np.newaxis], 10)\n    plt.plot(X_, y_samples, lw=1)\n    plt.scatter(X[:, 0], y, c=\'r\', s=50, zorder=10, edgecolors=(0, 0, 0))\n    plt.xlim(0, 5)\n    plt.ylim(-3, 3)\n    plt.title(""Posterior (kernel: %s)\\n Log-Likelihood: %.3f""\n              % (gp.kernel_, gp.log_marginal_likelihood(gp.kernel_.theta)),\n              fontsize=12)\n    plt.tight_layout()\n\nplt.show()\n'"
scikit-learn/_downloads/b5ccc9b80cb9fa7b5d4e14de2eb210fd/plot_compare_cross_decomposition.py,18,"b'""""""\n===================================\nCompare cross decomposition methods\n===================================\n\nSimple usage of various cross decomposition algorithms:\n- PLSCanonical\n- PLSRegression, with multivariate response, a.k.a. PLS2\n- PLSRegression, with univariate response, a.k.a. PLS1\n- CCA\n\nGiven 2 multivariate covarying two-dimensional datasets, X, and Y,\nPLS extracts the \'directions of covariance\', i.e. the components of each\ndatasets that explain the most shared variance between both datasets.\nThis is apparent on the **scatterplot matrix** display: components 1 in\ndataset X and dataset Y are maximally correlated (points lie around the\nfirst diagonal). This is also true for components 2 in both dataset,\nhowever, the correlation across datasets for different components is\nweak: the point cloud is very spherical.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cross_decomposition import PLSCanonical, PLSRegression, CCA\n\n# #############################################################################\n# Dataset based latent variables model\n\nn = 500\n# 2 latents vars:\nl1 = np.random.normal(size=n)\nl2 = np.random.normal(size=n)\n\nlatents = np.array([l1, l1, l2, l2]).T\nX = latents + np.random.normal(size=4 * n).reshape((n, 4))\nY = latents + np.random.normal(size=4 * n).reshape((n, 4))\n\nX_train = X[:n // 2]\nY_train = Y[:n // 2]\nX_test = X[n // 2:]\nY_test = Y[n // 2:]\n\nprint(""Corr(X)"")\nprint(np.round(np.corrcoef(X.T), 2))\nprint(""Corr(Y)"")\nprint(np.round(np.corrcoef(Y.T), 2))\n\n# #############################################################################\n# Canonical (symmetric) PLS\n\n# Transform data\n# ~~~~~~~~~~~~~~\nplsca = PLSCanonical(n_components=2)\nplsca.fit(X_train, Y_train)\nX_train_r, Y_train_r = plsca.transform(X_train, Y_train)\nX_test_r, Y_test_r = plsca.transform(X_test, Y_test)\n\n# Scatter plot of scores\n# ~~~~~~~~~~~~~~~~~~~~~~\n# 1) On diagonal plot X vs Y scores on each components\nplt.figure(figsize=(12, 8))\nplt.subplot(221)\nplt.scatter(X_train_r[:, 0], Y_train_r[:, 0], label=""train"",\n            marker=""o"", c=""b"", s=25)\nplt.scatter(X_test_r[:, 0], Y_test_r[:, 0], label=""test"",\n            marker=""o"", c=""r"", s=25)\nplt.xlabel(""x scores"")\nplt.ylabel(""y scores"")\nplt.title(\'Comp. 1: X vs Y (test corr = %.2f)\' %\n          np.corrcoef(X_test_r[:, 0], Y_test_r[:, 0])[0, 1])\nplt.xticks(())\nplt.yticks(())\nplt.legend(loc=""best"")\n\nplt.subplot(224)\nplt.scatter(X_train_r[:, 1], Y_train_r[:, 1], label=""train"",\n            marker=""o"", c=""b"", s=25)\nplt.scatter(X_test_r[:, 1], Y_test_r[:, 1], label=""test"",\n            marker=""o"", c=""r"", s=25)\nplt.xlabel(""x scores"")\nplt.ylabel(""y scores"")\nplt.title(\'Comp. 2: X vs Y (test corr = %.2f)\' %\n          np.corrcoef(X_test_r[:, 1], Y_test_r[:, 1])[0, 1])\nplt.xticks(())\nplt.yticks(())\nplt.legend(loc=""best"")\n\n# 2) Off diagonal plot components 1 vs 2 for X and Y\nplt.subplot(222)\nplt.scatter(X_train_r[:, 0], X_train_r[:, 1], label=""train"",\n            marker=""*"", c=""b"", s=50)\nplt.scatter(X_test_r[:, 0], X_test_r[:, 1], label=""test"",\n            marker=""*"", c=""r"", s=50)\nplt.xlabel(""X comp. 1"")\nplt.ylabel(""X comp. 2"")\nplt.title(\'X comp. 1 vs X comp. 2 (test corr = %.2f)\'\n          % np.corrcoef(X_test_r[:, 0], X_test_r[:, 1])[0, 1])\nplt.legend(loc=""best"")\nplt.xticks(())\nplt.yticks(())\n\nplt.subplot(223)\nplt.scatter(Y_train_r[:, 0], Y_train_r[:, 1], label=""train"",\n            marker=""*"", c=""b"", s=50)\nplt.scatter(Y_test_r[:, 0], Y_test_r[:, 1], label=""test"",\n            marker=""*"", c=""r"", s=50)\nplt.xlabel(""Y comp. 1"")\nplt.ylabel(""Y comp. 2"")\nplt.title(\'Y comp. 1 vs Y comp. 2 , (test corr = %.2f)\'\n          % np.corrcoef(Y_test_r[:, 0], Y_test_r[:, 1])[0, 1])\nplt.legend(loc=""best"")\nplt.xticks(())\nplt.yticks(())\nplt.show()\n\n# #############################################################################\n# PLS regression, with multivariate response, a.k.a. PLS2\n\nn = 1000\nq = 3\np = 10\nX = np.random.normal(size=n * p).reshape((n, p))\nB = np.array([[1, 2] + [0] * (p - 2)] * q).T\n# each Yj = 1*X1 + 2*X2 + noize\nY = np.dot(X, B) + np.random.normal(size=n * q).reshape((n, q)) + 5\n\npls2 = PLSRegression(n_components=3)\npls2.fit(X, Y)\nprint(""True B (such that: Y = XB + Err)"")\nprint(B)\n# compare pls2.coef_ with B\nprint(""Estimated B"")\nprint(np.round(pls2.coef_, 1))\npls2.predict(X)\n\n# PLS regression, with univariate response, a.k.a. PLS1\n\nn = 1000\np = 10\nX = np.random.normal(size=n * p).reshape((n, p))\ny = X[:, 0] + 2 * X[:, 1] + np.random.normal(size=n * 1) + 5\npls1 = PLSRegression(n_components=3)\npls1.fit(X, y)\n# note that the number of components exceeds 1 (the dimension of y)\nprint(""Estimated betas"")\nprint(np.round(pls1.coef_, 1))\n\n# #############################################################################\n# CCA (PLS mode B with symmetric deflation)\n\ncca = CCA(n_components=2)\ncca.fit(X_train, Y_train)\nX_train_r, Y_train_r = cca.transform(X_train, Y_train)\nX_test_r, Y_test_r = cca.transform(X_test, Y_test)\n'"
scikit-learn/_downloads/b6ac5801d8000dc889f28f14fc8cacdf/plot_classifier_comparison.py,5,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=====================\nClassifier comparison\n=====================\n\nA comparison of a several classifiers in scikit-learn on synthetic datasets.\nThe point of this example is to illustrate the nature of decision boundaries\nof different classifiers.\nThis should be taken with a grain of salt, as the intuition conveyed by\nthese examples does not necessarily carry over to real datasets.\n\nParticularly in high-dimensional spaces, data can more easily be separated\nlinearly and the simplicity of classifiers such as naive Bayes and linear SVMs\nmight lead to better generalization than is achieved by other classifiers.\n\nThe plots show training points in solid colors and testing points\nsemi-transparent. The lower right shows the classification accuracy on the test\nset.\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n#              Andreas M\xc3\xbcller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\n\nnames = [""Nearest Neighbors"", ""Linear SVM"", ""RBF SVM"", ""Gaussian Process"",\n         ""Decision Tree"", ""Random Forest"", ""Neural Net"", ""AdaBoost"",\n         ""Naive Bayes"", ""QDA""]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=""linear"", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            linearly_separable\n            ]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap([\'#FF0000\', \'#0000FF\'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(""Input data"")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors=\'k\')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors=\'k\')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, ""decision_function""):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors=\'k\')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors=\'k\', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, (\'%.2f\' % score).lstrip(\'0\'),\n                size=15, horizontalalignment=\'right\')\n        i += 1\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/b8111206047c829d2d0aaedf5f067f15/plot_inductive_clustering.py,3,"b'""""""\n==============================================\nInductive Clustering\n==============================================\n\nClustering can be expensive, especially when our dataset contains millions\nof datapoints. Many clustering algorithms are not :term:`inductive` and so\ncannot be directly applied to new data samples without recomputing the\nclustering, which may be intractable. Instead, we can use clustering to then\nlearn an inductive model with a classifier, which has several benefits:\n\n- it allows the clusters to scale and apply to new data\n- unlike re-fitting the clusters to new samples, it makes sure the labelling\n  procedure is consistent over time\n- it allows us to use the inferential capabilities of the classifier to\n  describe or explain the clusters\n\nThis example illustrates a generic implementation of a meta-estimator which\nextends clustering by inducing a classifier from the cluster labels.\n""""""\n# Authors: Chirag Nagpal\n#          Christos Aridas\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, clone\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.metaestimators import if_delegate_has_method\n\n\nN_SAMPLES = 5000\nRANDOM_STATE = 42\n\n\nclass InductiveClusterer(BaseEstimator):\n    def __init__(self, clusterer, classifier):\n        self.clusterer = clusterer\n        self.classifier = classifier\n\n    def fit(self, X, y=None):\n        self.clusterer_ = clone(self.clusterer)\n        self.classifier_ = clone(self.classifier)\n        y = self.clusterer_.fit_predict(X)\n        self.classifier_.fit(X, y)\n        return self\n\n    @if_delegate_has_method(delegate=\'classifier_\')\n    def predict(self, X):\n        return self.classifier_.predict(X)\n\n    @if_delegate_has_method(delegate=\'classifier_\')\n    def decision_function(self, X):\n        return self.classifier_.decision_function(X)\n\n\ndef plot_scatter(X,  color, alpha=0.5):\n    return plt.scatter(X[:, 0],\n                       X[:, 1],\n                       c=color,\n                       alpha=alpha,\n                       edgecolor=\'k\')\n\n\n# Generate some training data from clustering\nX, y = make_blobs(n_samples=N_SAMPLES,\n                  cluster_std=[1.0, 1.0, 0.5],\n                  centers=[(-5, -5), (0, 0), (5, 5)],\n                  random_state=RANDOM_STATE)\n\n\n# Train a clustering algorithm on the training data and get the cluster labels\nclusterer = AgglomerativeClustering(n_clusters=3)\ncluster_labels = clusterer.fit_predict(X)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(131)\nplot_scatter(X, cluster_labels)\nplt.title(""Ward Linkage"")\n\n\n# Generate new samples and plot them along with the original dataset\nX_new, y_new = make_blobs(n_samples=10,\n                          centers=[(-7, -1), (-2, 4), (3, 6)],\n                          random_state=RANDOM_STATE)\n\nplt.subplot(132)\nplot_scatter(X, cluster_labels)\nplot_scatter(X_new, \'black\', 1)\nplt.title(""Unknown instances"")\n\n\n# Declare the inductive learning model that it will be used to\n# predict cluster membership for unknown instances\nclassifier = RandomForestClassifier(random_state=RANDOM_STATE)\ninductive_learner = InductiveClusterer(clusterer, classifier).fit(X)\n\nprobable_clusters = inductive_learner.predict(X_new)\n\n\nplt.subplot(133)\nplot_scatter(X, cluster_labels)\nplot_scatter(X_new, probable_clusters)\n\n# Plotting decision regions\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nZ = inductive_learner.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.4)\nplt.title(""Classify unknown instances"")\n\nplt.show()\n'"
scikit-learn/_downloads/b8290fb0641fb508e34a1046d1164e4f/plot_logistic.py,6,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n\n""""""\n=========================================================\nLogistic function\n=========================================================\n\nShown in the plot is how the logistic regression would, in this\nsynthetic dataset, classify values as either 0 or 1,\ni.e. class one or two, using the logistic curve.\n\n""""""\nprint(__doc__)\n\n\n# Code source: Gael Varoquaux\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom scipy.special import expit\n\n# General a toy dataset:s it\'s just a straight line with some Gaussian noise:\nxmin, xmax = -5, 5\nn_samples = 100\nnp.random.seed(0)\nX = np.random.normal(size=n_samples)\ny = (X > 0).astype(np.float)\nX[X > 0] *= 4\nX += .3 * np.random.normal(size=n_samples)\n\nX = X[:, np.newaxis]\n\n# Fit the classifier\nclf = linear_model.LogisticRegression(C=1e5)\nclf.fit(X, y)\n\n# and plot the result\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.scatter(X.ravel(), y, color=\'black\', zorder=20)\nX_test = np.linspace(-5, 10, 300)\n\nloss = expit(X_test * clf.coef_ + clf.intercept_).ravel()\nplt.plot(X_test, loss, color=\'red\', linewidth=3)\n\nols = linear_model.LinearRegression()\nols.fit(X, y)\nplt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\nplt.axhline(.5, color=\'.5\')\n\nplt.ylabel(\'y\')\nplt.xlabel(\'X\')\nplt.xticks(range(-5, 10))\nplt.yticks([0, 0.5, 1])\nplt.ylim(-.25, 1.25)\nplt.xlim(-4, 10)\nplt.legend((\'Logistic Regression Model\', \'Linear Regression Model\'),\n           loc=""lower right"", fontsize=\'small\')\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/bb9ab87ec703112bca402bf07e4e393e/plot_label_propagation_digits.py,5,"b'""""""\n===================================================\nLabel Propagation digits: Demonstrating performance\n===================================================\n\nThis example demonstrates the power of semisupervised learning by\ntraining a Label Spreading model to classify handwritten digits\nwith sets of very few labels.\n\nThe handwritten digit dataset has 1797 total points. The model will\nbe trained using all points, but only 30 will be labeled. Results\nin the form of a confusion matrix and a series of metrics over each\nclass will be very good.\n\nAt the end, the top 10 most uncertain predictions will be shown.\n""""""\nprint(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import LabelSpreading\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndigits = datasets.load_digits()\nrng = np.random.RandomState(2)\nindices = np.arange(len(digits.data))\nrng.shuffle(indices)\n\nX = digits.data[indices[:340]]\ny = digits.target[indices[:340]]\nimages = digits.images[indices[:340]]\n\nn_total_samples = len(y)\nn_labeled_points = 40\n\nindices = np.arange(n_total_samples)\n\nunlabeled_set = indices[n_labeled_points:]\n\n# #############################################################################\n# Shuffle everything around\ny_train = np.copy(y)\ny_train[unlabeled_set] = -1\n\n# #############################################################################\n# Learn with LabelSpreading\nlp_model = LabelSpreading(gamma=.25, max_iter=20)\nlp_model.fit(X, y_train)\npredicted_labels = lp_model.transduction_[unlabeled_set]\ntrue_labels = y[unlabeled_set]\n\ncm = confusion_matrix(true_labels, predicted_labels, labels=lp_model.classes_)\n\nprint(""Label Spreading model: %d labeled & %d unlabeled points (%d total)"" %\n      (n_labeled_points, n_total_samples - n_labeled_points, n_total_samples))\n\nprint(classification_report(true_labels, predicted_labels))\n\nprint(""Confusion matrix"")\nprint(cm)\n\n# #############################################################################\n# Calculate uncertainty values for each transduced distribution\npred_entropies = stats.distributions.entropy(lp_model.label_distributions_.T)\n\n# #############################################################################\n# Pick the top 10 most uncertain labels\nuncertainty_index = np.argsort(pred_entropies)[-10:]\n\n# #############################################################################\n# Plot\nf = plt.figure(figsize=(7, 5))\nfor index, image_index in enumerate(uncertainty_index):\n    image = images[image_index]\n\n    sub = f.add_subplot(2, 5, index + 1)\n    sub.imshow(image, cmap=plt.cm.gray_r)\n    plt.xticks([])\n    plt.yticks([])\n    sub.set_title(\'predict: %i\\ntrue: %i\' % (\n        lp_model.transduction_[image_index], y[image_index]))\n\nf.suptitle(\'Learning with small amount of labeled data\')\nplt.show()\n'"
scikit-learn/_downloads/bbd2debed0189051ed6fe26cd1f276af/plot_nested_cross_validation_iris.py,2,"b'""""""\n=========================================\nNested versus non-nested cross-validation\n=========================================\n\nThis example compares non-nested and nested cross-validation strategies on a\nclassifier of the iris data set. Nested cross-validation (CV) is often used to\ntrain a model in which hyperparameters also need to be optimized. Nested CV\nestimates the generalization error of the underlying model and its\n(hyper)parameter search. Choosing the parameters that maximize non-nested CV\nbiases the model to the dataset, yielding an overly-optimistic score.\n\nModel selection without nested CV uses the same data to tune model parameters\nand evaluate model performance. Information may thus ""leak"" into the model\nand overfit the data. The magnitude of this effect is primarily dependent on\nthe size of the dataset and the stability of the model. See Cawley and Talbot\n[1]_ for an analysis of these issues.\n\nTo avoid this problem, nested CV effectively uses a series of\ntrain/validation/test set splits. In the inner loop (here executed by\n:class:`GridSearchCV <sklearn.model_selection.GridSearchCV>`), the score is\napproximately maximized by fitting a model to each training set, and then\ndirectly maximized in selecting (hyper)parameters over the validation set. In\nthe outer loop (here in :func:`cross_val_score\n<sklearn.model_selection.cross_val_score>`), generalization error is estimated\nby averaging test set scores over several dataset splits.\n\nThe example below uses a support vector classifier with a non-linear kernel to\nbuild a model with optimized hyperparameters by grid search. We compare the\nperformance of non-nested and nested CV strategies by taking the difference\nbetween their scores.\n\n.. topic:: See Also:\n\n    - :ref:`cross_validation`\n    - :ref:`grid_search`\n\n.. topic:: References:\n\n    .. [1] `Cawley, G.C.; Talbot, N.L.C. On over-fitting in model selection and\n     subsequent selection bias in performance evaluation.\n     J. Mach. Learn. Res 2010,11, 2079-2107.\n     <http://jmlr.csail.mit.edu/papers/volume11/cawley10a/cawley10a.pdf>`_\n\n""""""\nfrom sklearn.datasets import load_iris\nfrom matplotlib import pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold\nimport numpy as np\n\nprint(__doc__)\n\n# Number of random trials\nNUM_TRIALS = 30\n\n# Load the dataset\niris = load_iris()\nX_iris = iris.data\ny_iris = iris.target\n\n# Set up possible values of parameters to optimize over\np_grid = {""C"": [1, 10, 100],\n          ""gamma"": [.01, .1]}\n\n# We will use a Support Vector Classifier with ""rbf"" kernel\nsvm = SVC(kernel=""rbf"")\n\n# Arrays to store scores\nnon_nested_scores = np.zeros(NUM_TRIALS)\nnested_scores = np.zeros(NUM_TRIALS)\n\n# Loop for each trial\nfor i in range(NUM_TRIALS):\n\n    # Choose cross-validation techniques for the inner and outer loops,\n    # independently of the dataset.\n    # E.g ""GroupKFold"", ""LeaveOneOut"", ""LeaveOneGroupOut"", etc.\n    inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n    outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)\n\n    # Non_nested parameter search and scoring\n    clf = GridSearchCV(estimator=svm, param_grid=p_grid, cv=inner_cv)\n    clf.fit(X_iris, y_iris)\n    non_nested_scores[i] = clf.best_score_\n\n    # Nested CV with parameter optimization\n    nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)\n    nested_scores[i] = nested_score.mean()\n\nscore_difference = non_nested_scores - nested_scores\n\nprint(""Average difference of {:6f} with std. dev. of {:6f}.""\n      .format(score_difference.mean(), score_difference.std()))\n\n# Plot scores on each trial for nested and non-nested CV\nplt.figure()\nplt.subplot(211)\nnon_nested_scores_line, = plt.plot(non_nested_scores, color=\'r\')\nnested_line, = plt.plot(nested_scores, color=\'b\')\nplt.ylabel(""score"", fontsize=""14"")\nplt.legend([non_nested_scores_line, nested_line],\n           [""Non-Nested CV"", ""Nested CV""],\n           bbox_to_anchor=(0, .4, .5, 0))\nplt.title(""Non-Nested and Nested Cross Validation on Iris Dataset"",\n          x=.5, y=1.1, fontsize=""15"")\n\n# Plot bar chart of the difference.\nplt.subplot(212)\ndifference_plot = plt.bar(range(NUM_TRIALS), score_difference)\nplt.xlabel(""Individual Trial #"")\nplt.legend([difference_plot],\n           [""Non-Nested CV - Nested CV Score""],\n           bbox_to_anchor=(0, 1, .8, 0))\nplt.ylabel(""score difference"", fontsize=""14"")\n\nplt.show()\n'"
scikit-learn/_downloads/bc2db73fa5fb6e0b8da5412ca93b1198/plot_lasso_coordinate_descent_path.py,4,"b'""""""\n=====================\nLasso and Elastic Net\n=====================\n\nLasso and elastic net (L1 and L2 penalisation) implemented using a\ncoordinate descent.\n\nThe coefficients can be forced to be positive.\n""""""\nprint(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nfrom itertools import cycle\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import lasso_path, enet_path\nfrom sklearn import datasets\n\n\nX, y = datasets.load_diabetes(return_X_y=True)\n\n\nX /= X.std(axis=0)  # Standardize data (easier to set the l1_ratio parameter)\n\n# Compute paths\n\neps = 5e-3  # the smaller it is the longer is the path\n\nprint(""Computing regularization path using the lasso..."")\nalphas_lasso, coefs_lasso, _ = lasso_path(X, y, eps, fit_intercept=False)\n\nprint(""Computing regularization path using the positive lasso..."")\nalphas_positive_lasso, coefs_positive_lasso, _ = lasso_path(\n    X, y, eps, positive=True, fit_intercept=False)\nprint(""Computing regularization path using the elastic net..."")\nalphas_enet, coefs_enet, _ = enet_path(\n    X, y, eps=eps, l1_ratio=0.8, fit_intercept=False)\n\nprint(""Computing regularization path using the positive elastic net..."")\nalphas_positive_enet, coefs_positive_enet, _ = enet_path(\n    X, y, eps=eps, l1_ratio=0.8, positive=True, fit_intercept=False)\n\n# Display results\n\nplt.figure(1)\ncolors = cycle([\'b\', \'r\', \'g\', \'c\', \'k\'])\nneg_log_alphas_lasso = -np.log10(alphas_lasso)\nneg_log_alphas_enet = -np.log10(alphas_enet)\nfor coef_l, coef_e, c in zip(coefs_lasso, coefs_enet, colors):\n    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)\n    l2 = plt.plot(neg_log_alphas_enet, coef_e, linestyle=\'--\', c=c)\n\nplt.xlabel(\'-Log(alpha)\')\nplt.ylabel(\'coefficients\')\nplt.title(\'Lasso and Elastic-Net Paths\')\nplt.legend((l1[-1], l2[-1]), (\'Lasso\', \'Elastic-Net\'), loc=\'lower left\')\nplt.axis(\'tight\')\n\n\nplt.figure(2)\nneg_log_alphas_positive_lasso = -np.log10(alphas_positive_lasso)\nfor coef_l, coef_pl, c in zip(coefs_lasso, coefs_positive_lasso, colors):\n    l1 = plt.plot(neg_log_alphas_lasso, coef_l, c=c)\n    l2 = plt.plot(neg_log_alphas_positive_lasso, coef_pl, linestyle=\'--\', c=c)\n\nplt.xlabel(\'-Log(alpha)\')\nplt.ylabel(\'coefficients\')\nplt.title(\'Lasso and positive Lasso\')\nplt.legend((l1[-1], l2[-1]), (\'Lasso\', \'positive Lasso\'), loc=\'lower left\')\nplt.axis(\'tight\')\n\n\nplt.figure(3)\nneg_log_alphas_positive_enet = -np.log10(alphas_positive_enet)\nfor (coef_e, coef_pe, c) in zip(coefs_enet, coefs_positive_enet, colors):\n    l1 = plt.plot(neg_log_alphas_enet, coef_e, c=c)\n    l2 = plt.plot(neg_log_alphas_positive_enet, coef_pe, linestyle=\'--\', c=c)\n\nplt.xlabel(\'-Log(alpha)\')\nplt.ylabel(\'coefficients\')\nplt.title(\'Elastic-Net and positive Elastic-Net\')\nplt.legend((l1[-1], l2[-1]), (\'Elastic-Net\', \'positive Elastic-Net\'),\n           loc=\'lower left\')\nplt.axis(\'tight\')\nplt.show()\n'"
scikit-learn/_downloads/bcc2d375de9da808b455b7d1657945fe/plot_lasso_lars.py,1,"b'#!/usr/bin/env python\n""""""\n=====================\nLasso path using LARS\n=====================\n\nComputes Lasso Path along the regularization parameter using the LARS\nalgorithm on the diabetes dataset. Each color represents a different\nfeature of the coefficient vector, and this is displayed as a function\nof the regularization parameter.\n\n""""""\nprint(__doc__)\n\n# Author: Fabian Pedregosa <fabian.pedregosa@inria.fr>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\n\nX, y = datasets.load_diabetes(return_X_y=True)\n\nprint(""Computing regularization path using the LARS ..."")\n_, _, coefs = linear_model.lars_path(X, y, method=\'lasso\', verbose=True)\n\nxx = np.sum(np.abs(coefs.T), axis=1)\nxx /= xx[-1]\n\nplt.plot(xx, coefs.T)\nymin, ymax = plt.ylim()\nplt.vlines(xx, ymin, ymax, linestyle=\'dashed\')\nplt.xlabel(\'|coef| / max|coef|\')\nplt.ylabel(\'Coefficients\')\nplt.title(\'LASSO Path\')\nplt.axis(\'tight\')\nplt.show()\n'"
scikit-learn/_downloads/bdf78e1ebfda5c4a3778389dbc7f51ab/plot_cv_diabetes.py,3,"b'""""""\n===============================================\nCross-validation on diabetes Dataset Exercise\n===============================================\n\nA tutorial exercise which uses cross-validation with linear models.\n\nThis exercise is used in the :ref:`cv_estimators_tut` part of the\n:ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`.\n""""""\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nX, y = datasets.load_diabetes(return_X_y=True)\nX = X[:150]\ny = y[:150]\n\nlasso = Lasso(random_state=0, max_iter=10000)\nalphas = np.logspace(-4, -0.5, 30)\n\ntuned_parameters = [{\'alpha\': alphas}]\nn_folds = 5\n\nclf = GridSearchCV(lasso, tuned_parameters, cv=n_folds, refit=False)\nclf.fit(X, y)\nscores = clf.cv_results_[\'mean_test_score\']\nscores_std = clf.cv_results_[\'std_test_score\']\nplt.figure().set_size_inches(8, 6)\nplt.semilogx(alphas, scores)\n\n# plot error lines showing +/- std. errors of the scores\nstd_error = scores_std / np.sqrt(n_folds)\n\nplt.semilogx(alphas, scores + std_error, \'b--\')\nplt.semilogx(alphas, scores - std_error, \'b--\')\n\n# alpha=0.2 controls the translucency of the fill color\nplt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n\nplt.ylabel(\'CV score +/- std error\')\nplt.xlabel(\'alpha\')\nplt.axhline(np.max(scores), linestyle=\'--\', color=\'.5\')\nplt.xlim([alphas[0], alphas[-1]])\n\n# #############################################################################\n# Bonus: how much can you trust the selection of alpha?\n\n# To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\nlasso_cv = LassoCV(alphas=alphas, random_state=0, max_iter=10000)\nk_fold = KFold(3)\n\nprint(""Answer to the bonus question:"",\n      ""how much can you trust the selection of alpha?"")\nprint()\nprint(""Alpha parameters maximising the generalization score on different"")\nprint(""subsets of the data:"")\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\n    lasso_cv.fit(X[train], y[train])\n    print(""[fold {0}] alpha: {1:.5f}, score: {2:.5f}"".\n          format(k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])))\nprint()\nprint(""Answer: Not very much since we obtained different alphas for different"")\nprint(""subsets of the data and moreover, the scores for these alphas differ"")\nprint(""quite substantially."")\n\nplt.show()\n'"
scikit-learn/_downloads/bec3858ff94f8c5d38b707e6e3e14673/plot_rbm_logistic_classification.py,5,"b'""""""\n==============================================================\nRestricted Boltzmann Machine features for digit classification\n==============================================================\n\nFor greyscale image data where pixel values can be interpreted as degrees of\nblackness on a white background, like handwritten digit recognition, the\nBernoulli Restricted Boltzmann machine model (:class:`BernoulliRBM\n<sklearn.neural_network.BernoulliRBM>`) can perform effective non-linear\nfeature extraction.\n\nIn order to learn good latent representations from a small dataset, we\nartificially generate more labeled data by perturbing the training data with\nlinear shifts of 1 pixel in each direction.\n\nThis example shows how to build a classification pipeline with a BernoulliRBM\nfeature extractor and a :class:`LogisticRegression\n<sklearn.linear_model.LogisticRegression>` classifier. The hyperparameters\nof the entire model (learning rate, hidden layer size, regularization)\nwere optimized by grid search, but the search is not reproduced here because\nof runtime constraints.\n\nLogistic regression on raw pixel values is presented for comparison. The\nexample shows that the features extracted by the BernoulliRBM help improve the\nclassification accuracy.\n""""""\nprint(__doc__)\n\n# Authors: Yann N. Dauphin, Vlad Niculae, Gabriel Synnaeve\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.ndimage import convolve\nfrom sklearn import linear_model, datasets, metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import BernoulliRBM\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import clone\n\n\n# #############################################################################\n# Setting up\n\ndef nudge_dataset(X, Y):\n    """"""\n    This produces a dataset 5 times bigger than the original one,\n    by moving the 8x8 images in X around by 1px to left, right, down, up\n    """"""\n    direction_vectors = [\n        [[0, 1, 0],\n         [0, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [1, 0, 0],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 1],\n         [0, 0, 0]],\n\n        [[0, 0, 0],\n         [0, 0, 0],\n         [0, 1, 0]]]\n\n    def shift(x, w):\n        return convolve(x.reshape((8, 8)), mode=\'constant\', weights=w).ravel()\n\n    X = np.concatenate([X] +\n                       [np.apply_along_axis(shift, 1, X, vector)\n                        for vector in direction_vectors])\n    Y = np.concatenate([Y for _ in range(5)], axis=0)\n    return X, Y\n\n\n# Load Data\nX, y = datasets.load_digits(return_X_y=True)\nX = np.asarray(X, \'float32\')\nX, Y = nudge_dataset(X, y)\nX = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n\nX_train, X_test, Y_train, Y_test = train_test_split(\n    X, Y, test_size=0.2, random_state=0)\n\n# Models we will use\nlogistic = linear_model.LogisticRegression(solver=\'newton-cg\', tol=1)\nrbm = BernoulliRBM(random_state=0, verbose=True)\n\nrbm_features_classifier = Pipeline(\n    steps=[(\'rbm\', rbm), (\'logistic\', logistic)])\n\n# #############################################################################\n# Training\n\n# Hyper-parameters. These were set by cross-validation,\n# using a GridSearchCV. Here we are not performing cross-validation to\n# save time.\nrbm.learning_rate = 0.06\nrbm.n_iter = 10\n# More components tend to give better prediction performance, but larger\n# fitting time\nrbm.n_components = 100\nlogistic.C = 6000\n\n# Training RBM-Logistic Pipeline\nrbm_features_classifier.fit(X_train, Y_train)\n\n# Training the Logistic regression classifier directly on the pixel\nraw_pixel_classifier = clone(logistic)\nraw_pixel_classifier.C = 100.\nraw_pixel_classifier.fit(X_train, Y_train)\n\n# #############################################################################\n# Evaluation\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(""Logistic regression using RBM features:\\n%s\\n"" % (\n    metrics.classification_report(Y_test, Y_pred)))\n\nY_pred = raw_pixel_classifier.predict(X_test)\nprint(""Logistic regression using raw pixel features:\\n%s\\n"" % (\n    metrics.classification_report(Y_test, Y_pred)))\n\n# #############################################################################\n# Plotting\n\nplt.figure(figsize=(4.2, 4))\nfor i, comp in enumerate(rbm.components_):\n    plt.subplot(10, 10, i + 1)\n    plt.imshow(comp.reshape((8, 8)), cmap=plt.cm.gray_r,\n               interpolation=\'nearest\')\n    plt.xticks(())\n    plt.yticks(())\nplt.suptitle(\'100 components extracted by RBM\', fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\nplt.show()\n'"
scikit-learn/_downloads/c195b57124cc243ae523d3445f602535/plot_bias_variance.py,17,"b'""""""\n============================================================\nSingle estimator versus bagging: bias-variance decomposition\n============================================================\n\nThis example illustrates and compares the bias-variance decomposition of the\nexpected mean squared error of a single estimator against a bagging ensemble.\n\nIn regression, the expected mean squared error of an estimator can be\ndecomposed in terms of bias, variance and noise. On average over datasets of\nthe regression problem, the bias term measures the average amount by which the\npredictions of the estimator differ from the predictions of the best possible\nestimator for the problem (i.e., the Bayes model). The variance term measures\nthe variability of the predictions of the estimator when fit over different\ninstances LS of the problem. Finally, the noise measures the irreducible part\nof the error which is due the variability in the data.\n\nThe upper left figure illustrates the predictions (in dark red) of a single\ndecision tree trained over a random dataset LS (the blue dots) of a toy 1d\nregression problem. It also illustrates the predictions (in light red) of other\nsingle decision trees trained over other (and different) randomly drawn\ninstances LS of the problem. Intuitively, the variance term here corresponds to\nthe width of the beam of predictions (in light red) of the individual\nestimators. The larger the variance, the more sensitive are the predictions for\n`x` to small changes in the training set. The bias term corresponds to the\ndifference between the average prediction of the estimator (in cyan) and the\nbest possible model (in dark blue). On this problem, we can thus observe that\nthe bias is quite low (both the cyan and the blue curves are close to each\nother) while the variance is large (the red beam is rather wide).\n\nThe lower left figure plots the pointwise decomposition of the expected mean\nsquared error of a single decision tree. It confirms that the bias term (in\nblue) is low while the variance is large (in green). It also illustrates the\nnoise part of the error which, as expected, appears to be constant and around\n`0.01`.\n\nThe right figures correspond to the same plots but using instead a bagging\nensemble of decision trees. In both figures, we can observe that the bias term\nis larger than in the previous case. In the upper right figure, the difference\nbetween the average prediction (in cyan) and the best possible model is larger\n(e.g., notice the offset around `x=2`). In the lower right figure, the bias\ncurve is also slightly higher than in the lower left figure. In terms of\nvariance however, the beam of predictions is narrower, which suggests that the\nvariance is lower. Indeed, as the lower right figure confirms, the variance\nterm (in green) is lower than for single decision trees. Overall, the bias-\nvariance decomposition is therefore no longer the same. The tradeoff is better\nfor bagging: averaging several decision trees fit on bootstrap copies of the\ndataset slightly increases the bias term but allows for a larger reduction of\nthe variance, which results in a lower overall mean squared error (compare the\nred curves int the lower figures). The script output also confirms this\nintuition. The total error of the bagging ensemble is lower than the total\nerror of a single decision tree, and this difference indeed mainly stems from a\nreduced variance.\n\nFor further details on bias-variance decomposition, see section 7.3 of [1]_.\n\nReferences\n----------\n\n.. [1] T. Hastie, R. Tibshirani and J. Friedman,\n       ""Elements of Statistical Learning"", Springer, 2009.\n\n""""""\nprint(__doc__)\n\n# Author: Gilles Louppe <g.louppe@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Settings\nn_repeat = 50       # Number of iterations for computing expectations\nn_train = 50        # Size of the training set\nn_test = 1000       # Size of the test set\nnoise = 0.1         # Standard deviation of the noise\nnp.random.seed(0)\n\n# Change this for exploring the bias-variance decomposition of other\n# estimators. This should work well for estimators with high variance (e.g.,\n# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n# linear models).\nestimators = [(""Tree"", DecisionTreeRegressor()),\n              (""Bagging(Tree)"", BaggingRegressor(DecisionTreeRegressor()))]\n\nn_estimators = len(estimators)\n\n\n# Generate data\ndef f(x):\n    x = x.ravel()\n\n    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n\n\ndef generate(n_samples, noise, n_repeat=1):\n    X = np.random.rand(n_samples) * 10 - 5\n    X = np.sort(X)\n\n    if n_repeat == 1:\n        y = f(X) + np.random.normal(0.0, noise, n_samples)\n    else:\n        y = np.zeros((n_samples, n_repeat))\n\n        for i in range(n_repeat):\n            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n\n    X = X.reshape((n_samples, 1))\n\n    return X, y\n\n\nX_train = []\ny_train = []\n\nfor i in range(n_repeat):\n    X, y = generate(n_samples=n_train, noise=noise)\n    X_train.append(X)\n    y_train.append(y)\n\nX_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n\nplt.figure(figsize=(10, 8))\n\n# Loop over estimators to compare\nfor n, (name, estimator) in enumerate(estimators):\n    # Compute predictions\n    y_predict = np.zeros((n_test, n_repeat))\n\n    for i in range(n_repeat):\n        estimator.fit(X_train[i], y_train[i])\n        y_predict[:, i] = estimator.predict(X_test)\n\n    # Bias^2 + Variance + Noise decomposition of the mean squared error\n    y_error = np.zeros(n_test)\n\n    for i in range(n_repeat):\n        for j in range(n_repeat):\n            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n\n    y_error /= (n_repeat * n_repeat)\n\n    y_noise = np.var(y_test, axis=1)\n    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n    y_var = np.var(y_predict, axis=1)\n\n    print(""{0}: {1:.4f} (error) = {2:.4f} (bias^2) ""\n          "" + {3:.4f} (var) + {4:.4f} (noise)"".format(name,\n                                                      np.mean(y_error),\n                                                      np.mean(y_bias),\n                                                      np.mean(y_var),\n                                                      np.mean(y_noise)))\n\n    # Plot figures\n    plt.subplot(2, n_estimators, n + 1)\n    plt.plot(X_test, f(X_test), ""b"", label=""$f(x)$"")\n    plt.plot(X_train[0], y_train[0], "".b"", label=""LS ~ $y = f(x)+noise$"")\n\n    for i in range(n_repeat):\n        if i == 0:\n            plt.plot(X_test, y_predict[:, i], ""r"", label=r""$\\^y(x)$"")\n        else:\n            plt.plot(X_test, y_predict[:, i], ""r"", alpha=0.05)\n\n    plt.plot(X_test, np.mean(y_predict, axis=1), ""c"",\n             label=r""$\\mathbb{E}_{LS} \\^y(x)$"")\n\n    plt.xlim([-5, 5])\n    plt.title(name)\n\n    if n == n_estimators - 1:\n        plt.legend(loc=(1.1, .5))\n\n    plt.subplot(2, n_estimators, n_estimators + n + 1)\n    plt.plot(X_test, y_error, ""r"", label=""$error(x)$"")\n    plt.plot(X_test, y_bias, ""b"", label=""$bias^2(x)$""),\n    plt.plot(X_test, y_var, ""g"", label=""$variance(x)$""),\n    plt.plot(X_test, y_noise, ""c"", label=""$noise(x)$"")\n\n    plt.xlim([-5, 5])\n    plt.ylim([0, 0.1])\n\n    if n == n_estimators - 1:\n\n        plt.legend(loc=(1.1, .5))\n\nplt.subplots_adjust(right=.75)\nplt.show()\n'"
scikit-learn/_downloads/c277300a1e76913937609445e7a5fb8b/plot_compare_gpr_krr.py,6,"b'""""""\n==========================================================\nComparison of kernel ridge and Gaussian process regression\n==========================================================\n\nBoth kernel ridge regression (KRR) and Gaussian process regression (GPR) learn\na target function by employing internally the ""kernel trick"". KRR learns a\nlinear function in the space induced by the respective kernel which corresponds\nto a non-linear function in the original space. The linear function in the\nkernel space is chosen based on the mean-squared error loss with\nridge regularization. GPR uses the kernel to define the covariance of\na prior distribution over the target functions and uses the observed training\ndata to define a likelihood function. Based on Bayes theorem, a (Gaussian)\nposterior distribution over target functions is defined, whose mean is used\nfor prediction.\n\nA major difference is that GPR can choose the kernel\'s hyperparameters based\non gradient-ascent on the marginal likelihood function while KRR needs to\nperform a grid search on a cross-validated loss function (mean-squared error\nloss). A further difference is that GPR learns a generative, probabilistic\nmodel of the target function and can thus provide meaningful confidence\nintervals and posterior samples along with the predictions while KRR only\nprovides predictions.\n\nThis example illustrates both methods on an artificial dataset, which\nconsists of a sinusoidal target function and strong noise. The figure compares\nthe learned model of KRR and GPR based on a ExpSineSquared kernel, which is\nsuited for learning periodic functions. The kernel\'s hyperparameters control\nthe smoothness (l) and periodicity of the kernel (p). Moreover, the noise level\nof the data is learned explicitly by GPR by an additional WhiteKernel component\nin the kernel and by the regularization parameter alpha of KRR.\n\nThe figure shows that both methods learn reasonable models of the target\nfunction. GPR correctly identifies the periodicity of the function to be\nroughly 2*pi (6.28), while KRR chooses the doubled periodicity 4*pi. Besides\nthat, GPR provides reasonable confidence bounds on the prediction which are not\navailable for KRR. A major difference between the two methods is the time\nrequired for fitting and predicting: while fitting KRR is fast in principle,\nthe grid-search for hyperparameter optimization scales exponentially with the\nnumber of hyperparameters (""curse of dimensionality""). The gradient-based\noptimization of the parameters in GPR does not suffer from this exponential\nscaling and is thus considerable faster on this example with 3-dimensional\nhyperparameter space. The time for predicting is similar; however, generating\nthe variance of the predictive distribution of GPR takes considerable longer\nthan just predicting the mean.\n""""""\nprint(__doc__)\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\n\nimport time\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared\n\nrng = np.random.RandomState(0)\n\n# Generate sample data\nX = 15 * rng.rand(100, 1)\ny = np.sin(X).ravel()\ny += 3 * (0.5 - rng.rand(X.shape[0]))  # add noise\n\n# Fit KernelRidge with parameter selection based on 5-fold cross validation\nparam_grid = {""alpha"": [1e0, 1e-1, 1e-2, 1e-3],\n              ""kernel"": [ExpSineSquared(l, p)\n                         for l in np.logspace(-2, 2, 10)\n                         for p in np.logspace(0, 2, 10)]}\nkr = GridSearchCV(KernelRidge(), param_grid=param_grid)\nstime = time.time()\nkr.fit(X, y)\nprint(""Time for KRR fitting: %.3f"" % (time.time() - stime))\n\ngp_kernel = ExpSineSquared(1.0, 5.0, periodicity_bounds=(1e-2, 1e1)) \\\n    + WhiteKernel(1e-1)\ngpr = GaussianProcessRegressor(kernel=gp_kernel)\nstime = time.time()\ngpr.fit(X, y)\nprint(""Time for GPR fitting: %.3f"" % (time.time() - stime))\n\n# Predict using kernel ridge\nX_plot = np.linspace(0, 20, 10000)[:, None]\nstime = time.time()\ny_kr = kr.predict(X_plot)\nprint(""Time for KRR prediction: %.3f"" % (time.time() - stime))\n\n# Predict using gaussian process regressor\nstime = time.time()\ny_gpr = gpr.predict(X_plot, return_std=False)\nprint(""Time for GPR prediction: %.3f"" % (time.time() - stime))\n\nstime = time.time()\ny_gpr, y_std = gpr.predict(X_plot, return_std=True)\nprint(""Time for GPR prediction with standard-deviation: %.3f""\n      % (time.time() - stime))\n\n# Plot results\nplt.figure(figsize=(10, 5))\nlw = 2\nplt.scatter(X, y, c=\'k\', label=\'data\')\nplt.plot(X_plot, np.sin(X_plot), color=\'navy\', lw=lw, label=\'True\')\nplt.plot(X_plot, y_kr, color=\'turquoise\', lw=lw,\n         label=\'KRR (%s)\' % kr.best_params_)\nplt.plot(X_plot, y_gpr, color=\'darkorange\', lw=lw,\n         label=\'GPR (%s)\' % gpr.kernel_)\nplt.fill_between(X_plot[:, 0], y_gpr - y_std, y_gpr + y_std, color=\'darkorange\',\n                 alpha=0.2)\nplt.xlabel(\'data\')\nplt.ylabel(\'target\')\nplt.xlim(0, 20)\nplt.ylim(-4, 4)\nplt.title(\'GPR versus Kernel Ridge\')\nplt.legend(loc=""best"",  scatterpoints=1, prop={\'size\': 8})\nplt.show()\n'"
scikit-learn/_downloads/c3f19269a327134d6d5019f6d3d882c9/plot_ols_3d.py,3,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nSparsity Example: Fitting only features 1  and 2\n=========================================================\n\nFeatures 1 and 2 of the diabetes-dataset are fitted and\nplotted below. It illustrates that although feature 2\nhas a strong coefficient on the full model, it does not\ngive us much regarding `y` when compared to just feature 1\n\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn import datasets, linear_model\n\nX, y = datasets.load_diabetes(return_X_y=True)\nindices = (0, 1)\n\nX_train = X[:-20, indices]\nX_test = X[-20:, indices]\ny_train = y[:-20]\ny_test = y[-20:]\n\nols = linear_model.LinearRegression()\nols.fit(X_train, y_train)\n\n\n# #############################################################################\n# Plot the figure\ndef plot_figs(fig_num, elev, azim, X_train, clf):\n    fig = plt.figure(fig_num, figsize=(4, 3))\n    plt.clf()\n    ax = Axes3D(fig, elev=elev, azim=azim)\n\n    ax.scatter(X_train[:, 0], X_train[:, 1], y_train, c=\'k\', marker=\'+\')\n    ax.plot_surface(np.array([[-.1, -.1], [.15, .15]]),\n                    np.array([[-.1, .15], [-.1, .15]]),\n                    clf.predict(np.array([[-.1, -.1, .15, .15],\n                                          [-.1, .15, -.1, .15]]).T\n                                ).reshape((2, 2)),\n                    alpha=.5)\n    ax.set_xlabel(\'X_1\')\n    ax.set_ylabel(\'X_2\')\n    ax.set_zlabel(\'Y\')\n    ax.w_xaxis.set_ticklabels([])\n    ax.w_yaxis.set_ticklabels([])\n    ax.w_zaxis.set_ticklabels([])\n\n\n# Generate the three different figures from different views\nelev = 43.5\nazim = -110\nplot_figs(1, elev, azim, X_train, ols)\n\nelev = -.5\nazim = 0\nplot_figs(2, elev, azim, X_train, ols)\n\nelev = -.5\nazim = 90\nplot_figs(3, elev, azim, X_train, ols)\n\nplt.show()\n'"
scikit-learn/_downloads/c4541681aef7162a4553dee7532b9f6a/plot_kernel_approximation.py,6,"b'""""""\n==================================================\nExplicit feature map approximation for RBF kernels\n==================================================\n\nAn example illustrating the approximation of the feature map\nof an RBF kernel.\n\n.. currentmodule:: sklearn.kernel_approximation\n\nIt shows how to use :class:`RBFSampler` and :class:`Nystroem` to\napproximate the feature map of an RBF kernel for classification with an SVM on\nthe digits dataset. Results using a linear SVM in the original space, a linear\nSVM using the approximate mappings and using a kernelized SVM are compared.\nTimings and accuracy for varying amounts of Monte Carlo samplings (in the case\nof :class:`RBFSampler`, which uses random Fourier features) and different sized\nsubsets of the training set (for :class:`Nystroem`) for the approximate mapping\nare shown.\n\nPlease note that the dataset here is not large enough to show the benefits\nof kernel approximation, as the exact SVM is still reasonably fast.\n\nSampling more dimensions clearly leads to better classification results, but\ncomes at a greater cost. This means there is a tradeoff between runtime and\naccuracy, given by the parameter n_components. Note that solving the Linear\nSVM and also the approximate kernel SVM could be greatly accelerated by using\nstochastic gradient descent via :class:`sklearn.linear_model.SGDClassifier`.\nThis is not easily possible for the case of the kernelized SVM.\n\n""""""\n\n###########################################################################\n# Python package and dataset imports, load dataset\n# ---------------------------------------------------\n\n\n# Author: Gael Varoquaux <gael dot varoquaux at normalesup dot org>\n#         Andreas Mueller <amueller@ais.uni-bonn.de>\n# License: BSD 3 clause\n\nprint(__doc__)\n\n# Standard scientific Python imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom time import time\n\n# Import datasets, classifiers and performance metrics\nfrom sklearn import datasets, svm, pipeline\nfrom sklearn.kernel_approximation import (RBFSampler,\n                                          Nystroem)\nfrom sklearn.decomposition import PCA\n\n# The digits dataset\ndigits = datasets.load_digits(n_class=9)\n\n\n##################################################################\n# Timing and accuracy plots\n# --------------------------------------------------\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.data)\ndata = digits.data / 16.\ndata -= data.mean(axis=0)\n\n# We learn the digits on the first half of the digits\ndata_train, targets_train = (data[:n_samples // 2],\n                             digits.target[:n_samples // 2])\n\n\n# Now predict the value of the digit on the second half:\ndata_test, targets_test = (data[n_samples // 2:],\n                           digits.target[n_samples // 2:])\n# data_test = scaler.transform(data_test)\n\n# Create a classifier: a support vector classifier\nkernel_svm = svm.SVC(gamma=.2)\nlinear_svm = svm.LinearSVC()\n\n# create pipeline from kernel approximation\n# and linear svm\nfeature_map_fourier = RBFSampler(gamma=.2, random_state=1)\nfeature_map_nystroem = Nystroem(gamma=.2, random_state=1)\nfourier_approx_svm = pipeline.Pipeline([(""feature_map"", feature_map_fourier),\n                                        (""svm"", svm.LinearSVC())])\n\nnystroem_approx_svm = pipeline.Pipeline([(""feature_map"", feature_map_nystroem),\n                                        (""svm"", svm.LinearSVC())])\n\n# fit and predict using linear and kernel svm:\n\nkernel_svm_time = time()\nkernel_svm.fit(data_train, targets_train)\nkernel_svm_score = kernel_svm.score(data_test, targets_test)\nkernel_svm_time = time() - kernel_svm_time\n\nlinear_svm_time = time()\nlinear_svm.fit(data_train, targets_train)\nlinear_svm_score = linear_svm.score(data_test, targets_test)\nlinear_svm_time = time() - linear_svm_time\n\nsample_sizes = 30 * np.arange(1, 10)\nfourier_scores = []\nnystroem_scores = []\nfourier_times = []\nnystroem_times = []\n\nfor D in sample_sizes:\n    fourier_approx_svm.set_params(feature_map__n_components=D)\n    nystroem_approx_svm.set_params(feature_map__n_components=D)\n    start = time()\n    nystroem_approx_svm.fit(data_train, targets_train)\n    nystroem_times.append(time() - start)\n\n    start = time()\n    fourier_approx_svm.fit(data_train, targets_train)\n    fourier_times.append(time() - start)\n\n    fourier_score = fourier_approx_svm.score(data_test, targets_test)\n    nystroem_score = nystroem_approx_svm.score(data_test, targets_test)\n    nystroem_scores.append(nystroem_score)\n    fourier_scores.append(fourier_score)\n\n# plot the results:\nplt.figure(figsize=(16, 4))\naccuracy = plt.subplot(121)\n# second y axis for timings\ntimescale = plt.subplot(122)\n\naccuracy.plot(sample_sizes, nystroem_scores, label=""Nystroem approx. kernel"")\ntimescale.plot(sample_sizes, nystroem_times, \'--\',\n               label=\'Nystroem approx. kernel\')\n\naccuracy.plot(sample_sizes, fourier_scores, label=""Fourier approx. kernel"")\ntimescale.plot(sample_sizes, fourier_times, \'--\',\n               label=\'Fourier approx. kernel\')\n\n# horizontal lines for exact rbf and linear kernels:\naccuracy.plot([sample_sizes[0], sample_sizes[-1]],\n              [linear_svm_score, linear_svm_score], label=""linear svm"")\ntimescale.plot([sample_sizes[0], sample_sizes[-1]],\n               [linear_svm_time, linear_svm_time], \'--\', label=\'linear svm\')\n\naccuracy.plot([sample_sizes[0], sample_sizes[-1]],\n              [kernel_svm_score, kernel_svm_score], label=""rbf svm"")\ntimescale.plot([sample_sizes[0], sample_sizes[-1]],\n               [kernel_svm_time, kernel_svm_time], \'--\', label=\'rbf svm\')\n\n# vertical line for dataset dimensionality = 64\naccuracy.plot([64, 64], [0.7, 1], label=""n_features"")\n\n# legends and labels\naccuracy.set_title(""Classification accuracy"")\ntimescale.set_title(""Training times"")\naccuracy.set_xlim(sample_sizes[0], sample_sizes[-1])\naccuracy.set_xticks(())\naccuracy.set_ylim(np.min(fourier_scores), 1)\ntimescale.set_xlabel(""Sampling steps = transformed feature dimension"")\naccuracy.set_ylabel(""Classification accuracy"")\ntimescale.set_ylabel(""Training time in seconds"")\naccuracy.legend(loc=\'best\')\ntimescale.legend(loc=\'best\')\nplt.tight_layout()\nplt.show()\n\n\n############################################################################\n# Decision Surfaces of RBF Kernel SVM and Linear SVM\n# --------------------------------------------------------\n# The second plot visualized the decision surfaces of the RBF kernel SVM and\n# the linear SVM with approximate kernel maps.\n# The plot shows decision surfaces of the classifiers projected onto\n# the first two principal components of the data. This visualization should\n# be taken with a grain of salt since it is just an interesting slice through\n# the decision surface in 64 dimensions. In particular note that\n# a datapoint (represented as a dot) does not necessarily be classified\n# into the region it is lying in, since it will not lie on the plane\n# that the first two principal components span.\n# The usage of :class:`RBFSampler` and :class:`Nystroem` is described in detail\n# in :ref:`kernel_approximation`.\n\n# visualize the decision surface, projected down to the first\n# two principal components of the dataset\npca = PCA(n_components=8).fit(data_train)\n\nX = pca.transform(data_train)\n\n# Generate grid along first two principal components\nmultiples = np.arange(-2, 2, 0.1)\n# steps along first component\nfirst = multiples[:, np.newaxis] * pca.components_[0, :]\n# steps along second component\nsecond = multiples[:, np.newaxis] * pca.components_[1, :]\n# combine\ngrid = first[np.newaxis, :, :] + second[:, np.newaxis, :]\nflat_grid = grid.reshape(-1, data.shape[1])\n\n# title for the plots\ntitles = [\'SVC with rbf kernel\',\n          \'SVC (linear kernel)\\n with Fourier rbf feature map\\n\'\n          \'n_components=100\',\n          \'SVC (linear kernel)\\n with Nystroem rbf feature map\\n\'\n          \'n_components=100\']\n\nplt.figure(figsize=(18, 7.5))\nplt.rcParams.update({\'font.size\': 14})\n# predict and plot\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm,\n                         fourier_approx_svm)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    plt.subplot(1, 3, i + 1)\n    Z = clf.predict(flat_grid)\n\n    # Put the result into a color plot\n    Z = Z.reshape(grid.shape[:-1])\n    plt.contourf(multiples, multiples, Z, cmap=plt.cm.Paired)\n    plt.axis(\'off\')\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=targets_train, cmap=plt.cm.Paired,\n                edgecolors=(0, 0, 0))\n\n    plt.title(titles[i])\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/c478821023d08b75c406a87d94b271c3/plot_agglomerative_dendrogram.py,2,"b'# Authors: Mathew Kallada, Andreas Mueller\n# License: BSD 3 clause\n""""""\n=========================================\nPlot Hierarchical Clustering Dendrogram\n=========================================\nThis example plots the corresponding dendrogram of a hierarchical clustering\nusing AgglomerativeClustering and the dendrogram method available in scipy.\n""""""\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import AgglomerativeClustering\n\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n\n\niris = load_iris()\nX = iris.data\n\n# setting distance_threshold=0 ensures we compute the full tree.\nmodel = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n\nmodel = model.fit(X)\nplt.title(\'Hierarchical Clustering Dendrogram\')\n# plot the top three levels of the dendrogram\nplot_dendrogram(model, truncate_mode=\'level\', p=3)\nplt.xlabel(""Number of points in node (or index of point if no parenthesis)."")\nplt.show()\n'"
scikit-learn/_downloads/c4ae17a5b17e3bdde338b38ca21bfdf8/plot_gmm.py,8,"b'""""""\n=================================\nGaussian Mixture Model Ellipsoids\n=================================\n\nPlot the confidence ellipsoids of a mixture of two Gaussians\nobtained with Expectation Maximisation (``GaussianMixture`` class) and\nVariational Inference (``BayesianGaussianMixture`` class models with\na Dirichlet process prior).\n\nBoth models have access to five components with which to fit the data. Note\nthat the Expectation Maximisation model will necessarily use all five\ncomponents while the Variational Inference model will effectively only use as\nmany as are needed for a good fit. Here we can see that the Expectation\nMaximisation model splits some components arbitrarily, because it is trying to\nfit too many components, while the Dirichlet Process model adapts it number of\nstate automatically.\n\nThis example doesn\'t show it, as we\'re in a low-dimensional space, but\nanother advantage of the Dirichlet process model is that it can fit\nfull covariance matrices effectively even when there are less examples\nper cluster than there are dimensions in the data, due to\nregularization properties of the inference algorithm.\n""""""\n\nimport itertools\n\nimport numpy as np\nfrom scipy import linalg\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn import mixture\n\ncolor_iter = itertools.cycle([\'navy\', \'c\', \'cornflowerblue\', \'gold\',\n                              \'darkorange\'])\n\n\ndef plot_results(X, Y_, means, covariances, index, title):\n    splot = plt.subplot(2, 1, 1 + index)\n    for i, (mean, covar, color) in enumerate(zip(\n            means, covariances, color_iter)):\n        v, w = linalg.eigh(covar)\n        v = 2. * np.sqrt(2.) * np.sqrt(v)\n        u = w[0] / linalg.norm(w[0])\n        # as the DP will not use every component it has access to\n        # unless it needs it, we shouldn\'t plot the redundant\n        # components.\n        if not np.any(Y_ == i):\n            continue\n        plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n\n        # Plot an ellipse to show the Gaussian component\n        angle = np.arctan(u[1] / u[0])\n        angle = 180. * angle / np.pi  # convert to degrees\n        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n        ell.set_clip_box(splot.bbox)\n        ell.set_alpha(0.5)\n        splot.add_artist(ell)\n\n    plt.xlim(-9., 5.)\n    plt.ylim(-3., 6.)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(title)\n\n\n# Number of samples per component\nn_samples = 500\n\n# Generate random sample, two components\nnp.random.seed(0)\nC = np.array([[0., -0.1], [1.7, .4]])\nX = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n\n# Fit a Gaussian mixture with EM using five components\ngmm = mixture.GaussianMixture(n_components=5, covariance_type=\'full\').fit(X)\nplot_results(X, gmm.predict(X), gmm.means_, gmm.covariances_, 0,\n             \'Gaussian Mixture\')\n\n# Fit a Dirichlet process Gaussian mixture using five components\ndpgmm = mixture.BayesianGaussianMixture(n_components=5,\n                                        covariance_type=\'full\').fit(X)\nplot_results(X, dpgmm.predict(X), dpgmm.means_, dpgmm.covariances_, 1,\n             \'Bayesian Gaussian Mixture with a Dirichlet process prior\')\n\nplt.show()\n'"
scikit-learn/_downloads/c5f78418d947efdfcd8779e8974e3b03/plot_segmentation_toy.py,7,"b'""""""\n===========================================\nSpectral clustering for image segmentation\n===========================================\n\nIn this example, an image with connected circles is generated and\nspectral clustering is used to separate the circles.\n\nIn these settings, the :ref:`spectral_clustering` approach solves the problem\nknow as \'normalized graph cuts\': the image is seen as a graph of\nconnected voxels, and the spectral clustering algorithm amounts to\nchoosing graph cuts defining regions while minimizing the ratio of the\ngradient along the cut, and the volume of the region.\n\nAs the algorithm tries to balance the volume (ie balance the region\nsizes), if we take circles with different sizes, the segmentation fails.\n\nIn addition, as there is no useful information in the intensity of the image,\nor its gradient, we choose to perform the spectral clustering on a graph\nthat is only weakly informed by the gradient. This is close to performing\na Voronoi partition of the graph.\n\nIn addition, we use the mask of the objects to restrict the graph to the\noutline of the objects. In this example, we are interested in\nseparating the objects one from the other, and not from the background.\n""""""\nprint(__doc__)\n\n# Authors:  Emmanuelle Gouillart <emmanuelle.gouillart@normalesup.org>\n#           Gael Varoquaux <gael.varoquaux@normalesup.org>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import spectral_clustering\n\nl = 100\nx, y = np.indices((l, l))\n\ncenter1 = (28, 24)\ncenter2 = (40, 50)\ncenter3 = (67, 58)\ncenter4 = (24, 70)\n\nradius1, radius2, radius3, radius4 = 16, 14, 15, 14\n\ncircle1 = (x - center1[0]) ** 2 + (y - center1[1]) ** 2 < radius1 ** 2\ncircle2 = (x - center2[0]) ** 2 + (y - center2[1]) ** 2 < radius2 ** 2\ncircle3 = (x - center3[0]) ** 2 + (y - center3[1]) ** 2 < radius3 ** 2\ncircle4 = (x - center4[0]) ** 2 + (y - center4[1]) ** 2 < radius4 ** 2\n\n# #############################################################################\n# 4 circles\nimg = circle1 + circle2 + circle3 + circle4\n\n# We use a mask that limits to the foreground: the problem that we are\n# interested in here is not separating the objects from the background,\n# but separating them one from the other.\nmask = img.astype(bool)\n\nimg = img.astype(float)\nimg += 1 + 0.2 * np.random.randn(*img.shape)\n\n# Convert the image into a graph with the value of the gradient on the\n# edges.\ngraph = image.img_to_graph(img, mask=mask)\n\n# Take a decreasing function of the gradient: we take it weakly\n# dependent from the gradient the segmentation is close to a voronoi\ngraph.data = np.exp(-graph.data / graph.data.std())\n\n# Force the solver to be arpack, since amg is numerically\n# unstable on this example\nlabels = spectral_clustering(graph, n_clusters=4, eigen_solver=\'arpack\')\nlabel_im = np.full(mask.shape, -1.)\nlabel_im[mask] = labels\n\nplt.matshow(img)\nplt.matshow(label_im)\n\n# #############################################################################\n# 2 circles\nimg = circle1 + circle2\nmask = img.astype(bool)\nimg = img.astype(float)\n\nimg += 1 + 0.2 * np.random.randn(*img.shape)\n\ngraph = image.img_to_graph(img, mask=mask)\ngraph.data = np.exp(-graph.data / graph.data.std())\n\nlabels = spectral_clustering(graph, n_clusters=2, eigen_solver=\'arpack\')\nlabel_im = np.full(mask.shape, -1.)\nlabel_im[mask] = labels\n\nplt.matshow(img)\nplt.matshow(label_im)\n\nplt.show()\n'"
scikit-learn/_downloads/c84397b432f3c018c37f56c42624f952/plot_gpr_on_structured_data.py,19,"b'""""""\n==========================================================================\nGaussian processes on discrete data structures\n==========================================================================\n\nThis example illustrates the use of Gaussian processes for regression and\nclassification tasks on data that are not in fixed-length feature vector form.\nThis is achieved through the use of kernel functions that operates directly\non discrete structures such as variable-length sequences, trees, and graphs.\n\nSpecifically, here the input variables are some gene sequences stored as\nvariable-length strings consisting of letters \'A\', \'T\', \'C\', and \'G\',\nwhile the output variables are floating point numbers and True/False labels\nin the regression and classification tasks, respectively.\n\nA kernel between the gene sequences is defined using R-convolution [1]_ by\nintegrating a binary letter-wise kernel over all pairs of letters among a pair\nof strings.\n\nThis example will generate three figures.\n\nIn the first figure, we visualize the value of the kernel, i.e. the similarity\nof the sequences, using a colormap. Brighter color here indicates higher\nsimilarity.\n\nIn the second figure, we show some regression result on a dataset of 6\nsequences. Here we use the 1st, 2nd, 4th, and 5th sequences as the training set\nto make predictions on the 3rd and 6th sequences.\n\nIn the third figure, we demonstrate a classification model by training on 6\nsequences and make predictions on another 5 sequences. The ground truth here is\nsimply  whether there is at least one \'A\' in the sequence. Here the model makes\nfour correct classifications and fails on one.\n\n.. [1] Haussler, D. (1999). Convolution kernels on discrete structures\n       (Vol. 646). Technical report, Department of Computer Science, University\n       of California at Santa Cruz.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.gaussian_process.kernels import Kernel, Hyperparameter\nfrom sklearn.gaussian_process.kernels import GenericKernelMixin\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.base import clone\n\n\nclass SequenceKernel(GenericKernelMixin, Kernel):\n    \'\'\'\n    A minimal (but valid) convolutional kernel for sequences of variable\n    lengths.\'\'\'\n    def __init__(self,\n                 baseline_similarity=0.5,\n                 baseline_similarity_bounds=(1e-5, 1)):\n        self.baseline_similarity = baseline_similarity\n        self.baseline_similarity_bounds = baseline_similarity_bounds\n\n    @property\n    def hyperparameter_baseline_similarity(self):\n        return Hyperparameter(""baseline_similarity"",\n                              ""numeric"",\n                              self.baseline_similarity_bounds)\n\n    def _f(self, s1, s2):\n        \'\'\'\n        kernel value between a pair of sequences\n        \'\'\'\n        return sum([1.0 if c1 == c2 else self.baseline_similarity\n                   for c1 in s1\n                   for c2 in s2])\n\n    def _g(self, s1, s2):\n        \'\'\'\n        kernel derivative between a pair of sequences\n        \'\'\'\n        return sum([0.0 if c1 == c2 else 1.0\n                    for c1 in s1\n                    for c2 in s2])\n\n    def __call__(self, X, Y=None, eval_gradient=False):\n        if Y is None:\n            Y = X\n\n        if eval_gradient:\n            return (np.array([[self._f(x, y) for y in Y] for x in X]),\n                    np.array([[[self._g(x, y)] for y in Y] for x in X]))\n        else:\n            return np.array([[self._f(x, y) for y in Y] for x in X])\n\n    def diag(self, X):\n        return np.array([self._f(x, x) for x in X])\n\n    def is_stationary(self):\n        return False\n\n    def clone_with_theta(self, theta):\n        cloned = clone(self)\n        cloned.theta = theta\n        return cloned\n\n\nkernel = SequenceKernel()\n\n\'\'\'\nSequence similarity matrix under the kernel\n===========================================\n\'\'\'\n\nX = np.array([\'AGCT\', \'AGC\', \'AACT\', \'TAA\', \'AAA\', \'GAACA\'])\n\nK = kernel(X)\nD = kernel.diag(X)\n\nplt.figure(figsize=(8, 5))\nplt.imshow(np.diag(D**-0.5).dot(K).dot(np.diag(D**-0.5)))\nplt.xticks(np.arange(len(X)), X)\nplt.yticks(np.arange(len(X)), X)\nplt.title(\'Sequence similarity under the kernel\')\n\n\'\'\'\nRegression\n==========\n\'\'\'\n\nX = np.array([\'AGCT\', \'AGC\', \'AACT\', \'TAA\', \'AAA\', \'GAACA\'])\nY = np.array([1.0, 1.0, 2.0, 2.0, 3.0, 3.0])\n\ntraining_idx = [0, 1, 3, 4]\ngp = GaussianProcessRegressor(kernel=kernel)\ngp.fit(X[training_idx], Y[training_idx])\n\nplt.figure(figsize=(8, 5))\nplt.bar(np.arange(len(X)), gp.predict(X), color=\'b\', label=\'prediction\')\nplt.bar(training_idx, Y[training_idx], width=0.2, color=\'r\',\n        alpha=1, label=\'training\')\nplt.xticks(np.arange(len(X)), X)\nplt.title(\'Regression on sequences\')\nplt.legend()\n\n\'\'\'\nClassification\n==============\n\'\'\'\n\nX_train = np.array([\'AGCT\', \'CGA\', \'TAAC\', \'TCG\', \'CTTT\', \'TGCT\'])\n# whether there are \'A\'s in the sequence\nY_train = np.array([True, True, True, False, False, False])\n\ngp = GaussianProcessClassifier(kernel)\ngp.fit(X_train, Y_train)\n\nX_test = [\'AAA\', \'ATAG\', \'CTC\', \'CT\', \'C\']\nY_test = [True, True, False, False, False]\n\nplt.figure(figsize=(8, 5))\nplt.scatter(np.arange(len(X_train)), [1.0 if c else -1.0 for c in Y_train],\n            s=100, marker=\'o\', edgecolor=\'none\', facecolor=(1, 0.75, 0),\n            label=\'training\')\nplt.scatter(len(X_train) + np.arange(len(X_test)),\n            [1.0 if c else -1.0 for c in Y_test],\n            s=100, marker=\'o\', edgecolor=\'none\', facecolor=\'r\', label=\'truth\')\nplt.scatter(len(X_train) + np.arange(len(X_test)),\n            [1.0 if c else -1.0 for c in gp.predict(X_test)],\n            s=100, marker=\'x\', edgecolor=(0, 1.0, 0.3), linewidth=2,\n            label=\'prediction\')\nplt.xticks(np.arange(len(X_train) + len(X_test)),\n           np.concatenate((X_train, X_test)))\nplt.yticks([-1, 1], [False, True])\nplt.title(\'Classification on sequences\')\nplt.legend()\n\nplt.show()\n'"
scikit-learn/_downloads/c9ff3a911cb8885d61ab4bf5c39f74e5/plot_cv_digits.py,5,"b'""""""\n=============================================\nCross-validation on Digits Dataset Exercise\n=============================================\n\nA tutorial exercise using Cross-validation with an SVM on the Digits dataset.\n\nThis exercise is used in the :ref:`cv_generators_tut` part of the\n:ref:`model_selection_tut` section of the :ref:`stat_learn_tut_index`.\n""""""\nprint(__doc__)\n\n\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import datasets, svm\n\nX, y = datasets.load_digits(return_X_y=True)\n\nsvc = svm.SVC(kernel=\'linear\')\nC_s = np.logspace(-10, 0, 10)\n\nscores = list()\nscores_std = list()\nfor C in C_s:\n    svc.C = C\n    this_scores = cross_val_score(svc, X, y, n_jobs=1)\n    scores.append(np.mean(this_scores))\n    scores_std.append(np.std(this_scores))\n\n# Do the plotting\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.semilogx(C_s, scores)\nplt.semilogx(C_s, np.array(scores) + np.array(scores_std), \'b--\')\nplt.semilogx(C_s, np.array(scores) - np.array(scores_std), \'b--\')\nlocs, labels = plt.yticks()\nplt.yticks(locs, list(map(lambda x: ""%g"" % x, locs)))\nplt.ylabel(\'CV score\')\nplt.xlabel(\'Parameter C\')\nplt.ylim(0, 1.1)\nplt.show()\n'"
scikit-learn/_downloads/cb0017410c57062d64997a7bb19b6cbc/plot_train_error_vs_test_error.py,7,"b'""""""\n=========================\nTrain error vs Test error\n=========================\n\nIllustration of how the performance of an estimator on unseen data (test data)\nis not the same as the performance on training data. As the regularization\nincreases the performance on train decreases while the performance on test\nis optimal within a range of values of the regularization parameter.\nThe example with an Elastic-Net regression model and the performance is\nmeasured using the explained variance a.k.a. R^2.\n\n""""""\nprint(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport numpy as np\nfrom sklearn import linear_model\n\n# #############################################################################\n# Generate sample data\nn_samples_train, n_samples_test, n_features = 75, 150, 500\nnp.random.seed(0)\ncoef = np.random.randn(n_features)\ncoef[50:] = 0.0  # only the top 10 features are impacting the model\nX = np.random.randn(n_samples_train + n_samples_test, n_features)\ny = np.dot(X, coef)\n\n# Split train and test data\nX_train, X_test = X[:n_samples_train], X[n_samples_train:]\ny_train, y_test = y[:n_samples_train], y[n_samples_train:]\n\n# #############################################################################\n# Compute train and test errors\nalphas = np.logspace(-5, 1, 60)\nenet = linear_model.ElasticNet(l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = np.argmax(test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(""Optimal regularization parameter : %s"" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_\n\n# #############################################################################\n# Plot results functions\n\nimport matplotlib.pyplot as plt\nplt.subplot(2, 1, 1)\nplt.semilogx(alphas, train_errors, label=\'Train\')\nplt.semilogx(alphas, test_errors, label=\'Test\')\nplt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color=\'k\',\n           linewidth=3, label=\'Optimum on test\')\nplt.legend(loc=\'lower left\')\nplt.ylim([0, 1.2])\nplt.xlabel(\'Regularization parameter\')\nplt.ylabel(\'Performance\')\n\n# Show estimated coef_ vs true coef\nplt.subplot(2, 1, 2)\nplt.plot(coef, label=\'True coef\')\nplt.plot(coef_, label=\'Estimated coef\')\nplt.legend()\nplt.subplots_adjust(0.09, 0.04, 0.94, 0.94, 0.26, 0.26)\nplt.show()\n'"
scikit-learn/_downloads/cb153f48c91d022766a45f229d8756fd/plot_linearsvc_support_vectors.py,5,"b'""""""\n=====================================\nPlot the support vectors in LinearSVC\n=====================================\n\nUnlike SVC (based on LIBSVM), LinearSVC (based on LIBLINEAR) does not provide\nthe support vectors. This example demonstrates how to obtain the support\nvectors in LinearSVC.\n\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.svm import LinearSVC\n\nX, y = make_blobs(n_samples=40, centers=2, random_state=0)\n\nplt.figure(figsize=(10, 5))\nfor i, C in enumerate([1, 100]):\n    # ""hinge"" is the standard SVM loss\n    clf = LinearSVC(C=C, loss=""hinge"", random_state=42).fit(X, y)\n    # obtain the support vectors through the decision function\n    decision_function = clf.decision_function(X)\n    # we can also calculate the decision function manually\n    # decision_function = np.dot(X, clf.coef_[0]) + clf.intercept_[0]\n    support_vector_indices = np.where((2 * y - 1) * decision_function <= 1)[0]\n    support_vectors = X[support_vector_indices]\n\n    plt.subplot(1, 2, i + 1)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n    ax = plt.gca()\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n                         np.linspace(ylim[0], ylim[1], 50))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, colors=\'k\', levels=[-1, 0, 1], alpha=0.5,\n                linestyles=[\'--\', \'-\', \'--\'])\n    plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100,\n                linewidth=1, facecolors=\'none\', edgecolors=\'k\')\n    plt.title(""C="" + str(C))\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/cb1b1638effc9f459620a0098aff4fb3/plot_svm_anova.py,4,"b'""""""\n=================================================\nSVM-Anova: SVM with univariate feature selection\n=================================================\n\nThis example shows how to perform univariate feature selection before running a\nSVC (support vector classifier) to improve the classification scores. We use\nthe iris dataset (4 features) and add 36 non-informative features. We can find\nthat our model achieves best performance when we select around 10% of features.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n\n# #############################################################################\n# Import some data to play with\nX, y = load_iris(return_X_y=True)\n# Add non-informative features\nnp.random.seed(0)\nX = np.hstack((X, 2 * np.random.random((X.shape[0], 36))))\n\n# #############################################################################\n# Create a feature-selection transform, a scaler and an instance of SVM that we\n# combine together to have an full-blown estimator\nclf = Pipeline([(\'anova\', SelectPercentile(chi2)),\n                (\'scaler\', StandardScaler()),\n                (\'svc\', SVC(gamma=""auto""))])\n\n# #############################################################################\n# Plot the cross-validation score as a function of percentile of features\nscore_means = list()\nscore_stds = list()\npercentiles = (1, 3, 6, 10, 15, 20, 30, 40, 60, 80, 100)\n\nfor percentile in percentiles:\n    clf.set_params(anova__percentile=percentile)\n    this_scores = cross_val_score(clf, X, y)\n    score_means.append(this_scores.mean())\n    score_stds.append(this_scores.std())\n\nplt.errorbar(percentiles, score_means, np.array(score_stds))\nplt.title(\n    \'Performance of the SVM-Anova varying the percentile of features selected\')\nplt.xticks(np.linspace(0, 100, 11, endpoint=True))\nplt.xlabel(\'Percentile\')\nplt.ylabel(\'Accuracy Score\')\nplt.axis(\'tight\')\nplt.show()\n'"
scikit-learn/_downloads/cb21c2f2a9be6a2911f563ae17d622bc/plot_unveil_tree_structure.py,3,"b'""""""\n=========================================\nUnderstanding the decision tree structure\n=========================================\n\nThe decision tree structure can be analysed to gain further insight on the\nrelation between the features and the target to predict. In this example, we\nshow how to retrieve:\n\n- the binary tree structure;\n- the depth of each node and whether or not it\'s a leaf;\n- the nodes that were reached by a sample using the ``decision_path`` method;\n- the leaf that was reached by a sample using the apply method;\n- the rules that were used to predict a sample;\n- the decision path shared by a group of samples.\n\n""""""\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\niris = load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nestimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\nestimator.fit(X_train, y_train)\n\n# The decision estimator has an attribute called tree_  which stores the entire\n# tree structure and allows access to low level attributes. The binary tree\n# tree_ is represented as a number of parallel arrays. The i-th element of each\n# array holds information about the node `i`. Node 0 is the tree\'s root. NOTE:\n# Some of the arrays only apply to either leaves or split nodes, resp. In this\n# case the values of nodes of the other type are arbitrary!\n#\n# Among those arrays, we have:\n#   - left_child, id of the left child of the node\n#   - right_child, id of the right child of the node\n#   - feature, feature used for splitting the node\n#   - threshold, threshold value at the node\n#\n\n# Using those arrays, we can parse the tree structure:\n\nn_nodes = estimator.tree_.node_count\nchildren_left = estimator.tree_.children_left\nchildren_right = estimator.tree_.children_right\nfeature = estimator.tree_.feature\nthreshold = estimator.tree_.threshold\n\n\n# The tree structure can be traversed to compute various properties such\n# as the depth of each node and whether or not it is a leaf.\nnode_depth = np.zeros(shape=n_nodes, dtype=np.int64)\nis_leaves = np.zeros(shape=n_nodes, dtype=bool)\nstack = [(0, -1)]  # seed is the root node id and its parent depth\nwhile len(stack) > 0:\n    node_id, parent_depth = stack.pop()\n    node_depth[node_id] = parent_depth + 1\n\n    # If we have a test node\n    if (children_left[node_id] != children_right[node_id]):\n        stack.append((children_left[node_id], parent_depth + 1))\n        stack.append((children_right[node_id], parent_depth + 1))\n    else:\n        is_leaves[node_id] = True\n\nprint(""The binary tree structure has %s nodes and has ""\n      ""the following tree structure:""\n      % n_nodes)\nfor i in range(n_nodes):\n    if is_leaves[i]:\n        print(""%snode=%s leaf node."" % (node_depth[i] * ""\\t"", i))\n    else:\n        print(""%snode=%s test node: go to node %s if X[:, %s] <= %s else to ""\n              ""node %s.""\n              % (node_depth[i] * ""\\t"",\n                 i,\n                 children_left[i],\n                 feature[i],\n                 threshold[i],\n                 children_right[i],\n                 ))\nprint()\n\n# First let\'s retrieve the decision path of each sample. The decision_path\n# method allows to retrieve the node indicator functions. A non zero element of\n# indicator matrix at the position (i, j) indicates that the sample i goes\n# through the node j.\n\nnode_indicator = estimator.decision_path(X_test)\n\n# Similarly, we can also have the leaves ids reached by each sample.\n\nleave_id = estimator.apply(X_test)\n\n# Now, it\'s possible to get the tests that were used to predict a sample or\n# a group of samples. First, let\'s make it for the sample.\n\nsample_id = 0\nnode_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n                                    node_indicator.indptr[sample_id + 1]]\n\nprint(\'Rules used to predict sample %s: \' % sample_id)\nfor node_id in node_index:\n    if leave_id[sample_id] == node_id:\n        continue\n\n    if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n        threshold_sign = ""<=""\n    else:\n        threshold_sign = "">""\n\n    print(""decision id node %s : (X_test[%s, %s] (= %s) %s %s)""\n          % (node_id,\n             sample_id,\n             feature[node_id],\n             X_test[sample_id, feature[node_id]],\n             threshold_sign,\n             threshold[node_id]))\n\n# For a group of samples, we have the following common node.\nsample_ids = [0, 1]\ncommon_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==\n                len(sample_ids))\n\ncommon_node_id = np.arange(n_nodes)[common_nodes]\n\nprint(""\\nThe following samples %s share the node %s in the tree""\n      % (sample_ids, common_node_id))\nprint(""It is %s %% of all nodes."" % (100 * len(common_node_id) / n_nodes,))\n'"
scikit-learn/_downloads/cb7173ff9aab7efa3856271d0f9283de/plot_ols_ridge_variance.py,4,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nOrdinary Least Squares and Ridge Regression Variance\n=========================================================\nDue to the few points in each dimension and the straight\nline that linear regression uses to follow these points\nas well as it can, noise on the observations will cause\ngreat variance as shown in the first plot. Every line\'s slope\ncan vary quite a bit for each prediction due to the noise\ninduced in the observations.\n\nRidge regression is basically minimizing a penalised version\nof the least-squared function. The penalising `shrinks` the\nvalue of the regression coefficients.\nDespite the few data points in each dimension, the slope\nof the prediction is much more stable and the variance\nin the line itself is greatly reduced, in comparison to that\nof the standard linear regression\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\nX_train = np.c_[.5, 1].T\ny_train = [.5, 1]\nX_test = np.c_[0, 2].T\n\nnp.random.seed(0)\n\nclassifiers = dict(ols=linear_model.LinearRegression(),\n                   ridge=linear_model.Ridge(alpha=.1))\n\nfor name, clf in classifiers.items():\n    fig, ax = plt.subplots(figsize=(4, 3))\n\n    for _ in range(6):\n        this_X = .1 * np.random.normal(size=(2, 1)) + X_train\n        clf.fit(this_X, y_train)\n\n        ax.plot(X_test, clf.predict(X_test), color=\'gray\')\n        ax.scatter(this_X, y_train, s=3, c=\'gray\', marker=\'o\', zorder=10)\n\n    clf.fit(X_train, y_train)\n    ax.plot(X_test, clf.predict(X_test), linewidth=2, color=\'blue\')\n    ax.scatter(X_train, y_train, s=30, c=\'red\', marker=\'+\', zorder=10)\n\n    ax.set_title(name)\n    ax.set_xlim(0, 2)\n    ax.set_ylim((0, 1.6))\n    ax.set_xlabel(\'X\')\n    ax.set_ylabel(\'y\')\n\n    fig.tight_layout()\n\nplt.show()\n'"
scikit-learn/_downloads/cb8c7d25e68212ed7d29123b59827116/plot_permutation_importance_multicollinear.py,3,"b'""""""\n=================================================================\nPermutation Importance with Multicollinear or Correlated Features\n=================================================================\n\nIn this example, we compute the permutation importance on the Wisconsin\nbreast cancer dataset using :func:`~sklearn.inspection.permutation_importance`.\nThe :class:`~sklearn.ensemble.RandomForestClassifier` can easily get about 97%\naccuracy on a test dataset. Because this dataset contains multicollinear\nfeatures, the permutation importance will show that none of the features are\nimportant. One approach to handling multicollinearity is by performing\nhierarchical clustering on the features\' Spearman rank-order correlations,\npicking a threshold, and keeping a single feature from each cluster.\n\n.. note::\n    See also\n    :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`\n""""""\nprint(__doc__)\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import spearmanr\nfrom scipy.cluster import hierarchy\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\n\n##############################################################################\n# Random Forest Feature Importance on Breast Cancer Data\n# ------------------------------------------------------\n# First, we train a random forest on the breast cancer dataset and evaluate\n# its accuracy on a test set:\ndata = load_breast_cancer()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(X_train, y_train)\nprint(""Accuracy on test data: {:.2f}"".format(clf.score(X_test, y_test)))\n\n##############################################################################\n# Next, we plot the tree based feature importance and the permutation\n# importance. The permutation importance plot shows that permuting a feature\n# drops the accuracy by at most `0.012`, which would suggest that none of the\n# features are important. This is in contradiction with the high test accuracy\n# computed above: some feature must be important. The permutation importance\n# is calculated on the training set to show how much the model relies on each\n# feature during training.\nresult = permutation_importance(clf, X_train, y_train, n_repeats=10,\n                                random_state=42)\nperm_sorted_idx = result.importances_mean.argsort()\n\ntree_importance_sorted_idx = np.argsort(clf.feature_importances_)\ntree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\nax1.barh(tree_indices,\n         clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\nax1.set_yticklabels(data.feature_names)\nax1.set_yticks(tree_indices)\nax1.set_ylim((0, len(clf.feature_importances_)))\nax2.boxplot(result.importances[perm_sorted_idx].T, vert=False,\n            labels=data.feature_names)\nfig.tight_layout()\nplt.show()\n\n##############################################################################\n# Handling Multicollinear Features\n# --------------------------------\n# When features are collinear, permutating one feature will have little\n# effect on the models performance because it can get the same information\n# from a correlated feature. One way to handle multicollinear features is by\n# performing hierarchical clustering on the Spearman rank-order correlations,\n# picking a threshold, and keeping a single feature from each cluster. First,\n# we plot a heatmap of the correlated features:\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\ncorr = spearmanr(X).correlation\ncorr_linkage = hierarchy.ward(corr)\ndendro = hierarchy.dendrogram(corr_linkage, labels=data.feature_names, ax=ax1,\n                              leaf_rotation=90)\ndendro_idx = np.arange(0, len(dendro[\'ivl\']))\n\nax2.imshow(corr[dendro[\'leaves\'], :][:, dendro[\'leaves\']])\nax2.set_xticks(dendro_idx)\nax2.set_yticks(dendro_idx)\nax2.set_xticklabels(dendro[\'ivl\'], rotation=\'vertical\')\nax2.set_yticklabels(dendro[\'ivl\'])\nfig.tight_layout()\nplt.show()\n\n##############################################################################\n# Next, we manually pick a threshold by visual inspection of the dendrogram\n# to group our features into clusters and choose a feature from each cluster to\n# keep, select those features from our dataset, and train a new random forest.\n# The test accuracy of the new random forest did not change much compared to\n# the random forest trained on the complete dataset.\ncluster_ids = hierarchy.fcluster(corr_linkage, 1, criterion=\'distance\')\ncluster_id_to_feature_ids = defaultdict(list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = RandomForestClassifier(n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(""Accuracy on test data with features removed: {:.2f}"".format(\n      clf_sel.score(X_test_sel, y_test)))\n'"
scikit-learn/_downloads/cc4416f4c20469b375081695760118b7/plot_discretization_classification.py,12,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\n======================\nFeature discretization\n======================\n\nA demonstration of feature discretization on synthetic classification datasets.\nFeature discretization decomposes each feature into a set of bins, here equally\ndistributed in width. The discrete values are then one-hot encoded, and given\nto a linear classifier. This preprocessing enables a non-linear behavior even\nthough the classifier is linear.\n\nOn this example, the first two rows represent linearly non-separable datasets\n(moons and concentric circles) while the third is approximately linearly\nseparable. On the two linearly non-separable datasets, feature discretization\nlargely increases the performance of linear classifiers. On the linearly\nseparable dataset, feature discretization decreases the performance of linear\nclassifiers. Two non-linear classifiers are also shown for comparison.\n\nThis example should be taken with a grain of salt, as the intuition conveyed\ndoes not necessarily carry over to real datasets. Particularly in\nhigh-dimensional spaces, data can more easily be separated linearly. Moreover,\nusing feature discretization and one-hot encoding increases the number of\nfeatures, which easily lead to overfitting when the number of samples is small.\n\nThe plots show training points in solid colors and testing points\nsemi-transparent. The lower right shows the classification accuracy on the test\nset.\n""""""\n# Code source: Tom Dupr\xc3\xa9 la Tour\n# Adapted from plot_classifier_comparison by Ga\xc3\xabl Varoquaux and Andreas M\xc3\xbcller\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\nprint(__doc__)\n\nh = .02  # step size in the mesh\n\n\ndef get_name(estimator):\n    name = estimator.__class__.__name__\n    if name == \'Pipeline\':\n        name = [get_name(est[1]) for est in estimator.steps]\n        name = \' + \'.join(name)\n    return name\n\n\n# list of (estimator, param_grid), where param_grid is used in GridSearchCV\nclassifiers = [\n    (LogisticRegression(random_state=0), {\n        \'C\': np.logspace(-2, 7, 10)\n    }),\n    (LinearSVC(random_state=0), {\n        \'C\': np.logspace(-2, 7, 10)\n    }),\n    (make_pipeline(\n        KBinsDiscretizer(encode=\'onehot\'),\n        LogisticRegression(random_state=0)), {\n            \'kbinsdiscretizer__n_bins\': np.arange(2, 10),\n            \'logisticregression__C\': np.logspace(-2, 7, 10),\n        }),\n    (make_pipeline(\n        KBinsDiscretizer(encode=\'onehot\'), LinearSVC(random_state=0)), {\n            \'kbinsdiscretizer__n_bins\': np.arange(2, 10),\n            \'linearsvc__C\': np.logspace(-2, 7, 10),\n        }),\n    (GradientBoostingClassifier(n_estimators=50, random_state=0), {\n        \'learning_rate\': np.logspace(-4, 0, 10)\n    }),\n    (SVC(random_state=0), {\n        \'C\': np.logspace(-2, 7, 10)\n    }),\n]\n\nnames = [get_name(e) for e, g in classifiers]\n\nn_samples = 100\ndatasets = [\n    make_moons(n_samples=n_samples, noise=0.2, random_state=0),\n    make_circles(n_samples=n_samples, noise=0.2, factor=0.5, random_state=1),\n    make_classification(n_samples=n_samples, n_features=2, n_redundant=0,\n                        n_informative=2, random_state=2,\n                        n_clusters_per_class=1)\n]\n\nfig, axes = plt.subplots(nrows=len(datasets), ncols=len(classifiers) + 1,\n                         figsize=(21, 9))\n\ncm = plt.cm.PiYG\ncm_bright = ListedColormap([\'#b30065\', \'#178000\'])\n\n# iterate over datasets\nfor ds_cnt, (X, y) in enumerate(datasets):\n    print(\'\\ndataset %d\\n---------\' % ds_cnt)\n\n    # preprocess dataset, split into training and test part\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=.5, random_state=42)\n\n    # create the grid for background colors\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n    # plot the dataset first\n    ax = axes[ds_cnt, 0]\n    if ds_cnt == 0:\n        ax.set_title(""Input data"")\n    # plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors=\'k\')\n    # and testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors=\'k\')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n\n    # iterate over classifiers\n    for est_idx, (name, (estimator, param_grid)) in \\\n            enumerate(zip(names, classifiers)):\n        ax = axes[ds_cnt, est_idx + 1]\n\n        clf = GridSearchCV(estimator=estimator, param_grid=param_grid)\n        with ignore_warnings(category=ConvergenceWarning):\n            clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        print(\'%s: %.2f\' % (name, score))\n\n        # plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]*[y_min, y_max].\n        if hasattr(clf, ""decision_function""):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors=\'k\')\n        # and testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors=\'k\', alpha=0.6)\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n\n        if ds_cnt == 0:\n            ax.set_title(name.replace(\' + \', \'\\n\'))\n        ax.text(0.95, 0.06, (\'%.2f\' % score).lstrip(\'0\'), size=15,\n                bbox=dict(boxstyle=\'round\', alpha=0.8, facecolor=\'white\'),\n                transform=ax.transAxes, horizontalalignment=\'right\')\n\n\nplt.tight_layout()\n\n# Add suptitles above the figure\nplt.subplots_adjust(top=0.90)\nsuptitles = [\n    \'Linear classifiers\',\n    \'Feature discretization and linear classifiers\',\n    \'Non-linear classifiers\',\n]\nfor i, suptitle in zip([1, 3, 5], suptitles):\n    ax = axes[0, i]\n    ax.text(1.05, 1.25, suptitle, transform=ax.transAxes,\n            horizontalalignment=\'center\', size=\'x-large\')\nplt.show()\n'"
scikit-learn/_downloads/cc4905319b55d2a3a3a35f96b779b4e1/plot_adaboost_twoclass.py,6,"b'""""""\n==================\nTwo-class AdaBoost\n==================\n\nThis example fits an AdaBoosted decision stump on a non-linearly separable\nclassification dataset composed of two ""Gaussian quantiles"" clusters\n(see :func:`sklearn.datasets.make_gaussian_quantiles`) and plots the decision\nboundary and decision scores. The distributions of decision scores are shown\nseparately for samples of class A and B. The predicted class label for each\nsample is determined by the sign of the decision score. Samples with decision\nscores greater than zero are classified as B, and are otherwise classified\nas A. The magnitude of a decision score determines the degree of likeness with\nthe predicted class label. Additionally, a new dataset could be constructed\ncontaining a desired purity of class B, for example, by only selecting samples\nwith a decision score above some value.\n\n""""""\nprint(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_gaussian_quantiles\n\n\n# Construct dataset\nX1, y1 = make_gaussian_quantiles(cov=2.,\n                                 n_samples=200, n_features=2,\n                                 n_classes=2, random_state=1)\nX2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,\n                                 n_samples=300, n_features=2,\n                                 n_classes=2, random_state=1)\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, - y2 + 1))\n\n# Create and fit an AdaBoosted decision tree\nbdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n                         algorithm=""SAMME"",\n                         n_estimators=200)\n\nbdt.fit(X, y)\n\nplot_colors = ""br""\nplot_step = 0.02\nclass_names = ""AB""\n\nplt.figure(figsize=(10, 5))\n\n# Plot the decision boundaries\nplt.subplot(121)\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                     np.arange(y_min, y_max, plot_step))\n\nZ = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.axis(""tight"")\n\n# Plot the training points\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1],\n                c=c, cmap=plt.cm.Paired,\n                s=20, edgecolor=\'k\',\n                label=""Class %s"" % n)\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.legend(loc=\'upper right\')\nplt.xlabel(\'x\')\nplt.ylabel(\'y\')\nplt.title(\'Decision Boundary\')\n\n# Plot the two-class decision scores\ntwoclass_output = bdt.decision_function(X)\nplot_range = (twoclass_output.min(), twoclass_output.max())\nplt.subplot(122)\nfor i, n, c in zip(range(2), class_names, plot_colors):\n    plt.hist(twoclass_output[y == i],\n             bins=10,\n             range=plot_range,\n             facecolor=c,\n             label=\'Class %s\' % n,\n             alpha=.5,\n             edgecolor=\'k\')\nx1, x2, y1, y2 = plt.axis()\nplt.axis((x1, x2, y1, y2 * 1.2))\nplt.legend(loc=\'upper right\')\nplt.ylabel(\'Samples\')\nplt.xlabel(\'Score\')\nplt.title(\'Decision Scores\')\n\nplt.tight_layout()\nplt.subplots_adjust(wspace=0.35)\nplt.show()\n'"
scikit-learn/_downloads/cee5b5161693e977f698a3d705ea79ba/plot_feature_agglomeration_vs_univariate_selection.py,7,"b'""""""\n==============================================\nFeature agglomeration vs. univariate selection\n==============================================\n\nThis example compares 2 dimensionality reduction strategies:\n\n- univariate feature selection with Anova\n\n- feature agglomeration with Ward hierarchical clustering\n\nBoth methods are compared in a regression problem using\na BayesianRidge as supervised estimator.\n""""""\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport shutil\nimport tempfile\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg, ndimage\nfrom joblib import Memory\n\nfrom sklearn.feature_extraction.image import grid_to_graph\nfrom sklearn import feature_selection\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\n# #############################################################################\n# Generate data\nn_samples = 200\nsize = 40  # image size\nroi_size = 15\nsnr = 5.\nnp.random.seed(0)\nmask = np.ones([size, size], dtype=np.bool)\n\ncoef = np.zeros((size, size))\ncoef[0:roi_size, 0:roi_size] = -1.\ncoef[-roi_size:, -roi_size:] = 1.\n\nX = np.random.randn(n_samples, size ** 2)\nfor x in X:  # smooth data\n    x[:] = ndimage.gaussian_filter(x.reshape(size, size), sigma=1.0).ravel()\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\ny = np.dot(X, coef.ravel())\nnoise = np.random.randn(y.shape[0])\nnoise_coef = (linalg.norm(y, 2) / np.exp(snr / 20.)) / linalg.norm(noise, 2)\ny += noise_coef * noise  # add noise\n\n# #############################################################################\n# Compute the coefs of a Bayesian Ridge with GridSearch\ncv = KFold(2)  # cross-validation generator for model selection\nridge = BayesianRidge()\ncachedir = tempfile.mkdtemp()\nmem = Memory(location=cachedir, verbose=1)\n\n# Ward agglomeration followed by BayesianRidge\nconnectivity = grid_to_graph(n_x=size, n_y=size)\nward = FeatureAgglomeration(n_clusters=10, connectivity=connectivity,\n                            memory=mem)\nclf = Pipeline([(\'ward\', ward), (\'ridge\', ridge)])\n# Select the optimal number of parcels with grid search\nclf = GridSearchCV(clf, {\'ward__n_clusters\': [10, 20, 30]}, n_jobs=1, cv=cv)\nclf.fit(X, y)  # set the best parameters\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_)\ncoef_agglomeration_ = coef_.reshape(size, size)\n\n# Anova univariate feature selection followed by BayesianRidge\nf_regression = mem.cache(feature_selection.f_regression)  # caching function\nanova = feature_selection.SelectPercentile(f_regression)\nclf = Pipeline([(\'anova\', anova), (\'ridge\', ridge)])\n# Select the optimal percentage of features with grid search\nclf = GridSearchCV(clf, {\'anova__percentile\': [5, 10, 20]}, cv=cv)\nclf.fit(X, y)  # set the best parameters\ncoef_ = clf.best_estimator_.steps[-1][1].coef_\ncoef_ = clf.best_estimator_.steps[0][1].inverse_transform(coef_.reshape(1, -1))\ncoef_selection_ = coef_.reshape(size, size)\n\n# #############################################################################\n# Inverse the transformation to plot the results on an image\nplt.close(\'all\')\nplt.figure(figsize=(7.3, 2.7))\nplt.subplot(1, 3, 1)\nplt.imshow(coef, interpolation=""nearest"", cmap=plt.cm.RdBu_r)\nplt.title(""True weights"")\nplt.subplot(1, 3, 2)\nplt.imshow(coef_selection_, interpolation=""nearest"", cmap=plt.cm.RdBu_r)\nplt.title(""Feature Selection"")\nplt.subplot(1, 3, 3)\nplt.imshow(coef_agglomeration_, interpolation=""nearest"", cmap=plt.cm.RdBu_r)\nplt.title(""Feature Agglomeration"")\nplt.subplots_adjust(0.04, 0.0, 0.98, 0.94, 0.16, 0.26)\nplt.show()\n\n# Attempt to remove the temporary cachedir, but don\'t worry if it fails\nshutil.rmtree(cachedir, ignore_errors=True)\n'"
scikit-learn/_downloads/cf70421d0921c09068084ae7500b3849/plot_scaling_importance.py,0,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n""""""\n=========================================================\nImportance of Feature Scaling\n=========================================================\n\nFeature scaling through standardization (or Z-score normalization)\ncan be an important preprocessing step for many machine learning\nalgorithms. Standardization involves rescaling the features such\nthat they have the properties of a standard normal distribution\nwith a mean of zero and a standard deviation of one.\n\nWhile many algorithms (such as SVM, K-nearest neighbors, and logistic\nregression) require features to be normalized, intuitively we can\nthink of Principle Component Analysis (PCA) as being a prime example\nof when normalization is important. In PCA we are interested in the\ncomponents that maximize the variance. If one component (e.g. human\nheight) varies less than another (e.g. weight) because of their\nrespective scales (meters vs. kilos), PCA might determine that the\ndirection of maximal variance more closely corresponds with the\n\'weight\' axis, if those features are not scaled. As a change in\nheight of one meter can be considered much more important than the\nchange in weight of one kilogram, this is clearly incorrect.\n\nTo illustrate this, PCA is performed comparing the use of data with\n:class:`StandardScaler <sklearn.preprocessing.StandardScaler>` applied,\nto unscaled data. The results are visualized and a clear difference noted.\nThe 1st principal component in the unscaled set can be seen. It can be seen\nthat feature #13 dominates the direction, being a whole two orders of\nmagnitude above the other features. This is contrasted when observing\nthe principal component for the scaled version of the data. In the scaled\nversion, the orders of magnitude are roughly the same across all the features.\n\nThe dataset used is the Wine Dataset available at UCI. This dataset\nhas continuous features that are heterogeneous in scale due to differing\nproperties that they measure (i.e alcohol content, and malic acid).\n\nThe transformed data is then used to train a naive Bayes classifier, and a\nclear difference in prediction accuracies is observed wherein the dataset\nwhich is scaled before PCA vastly outperforms the unscaled version.\n\n""""""\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.pipeline import make_pipeline\nprint(__doc__)\n\n# Code source: Tyler Lanigan <tylerlanigan@gmail.com>\n#              Sebastian Raschka <mail@sebastianraschka.com>\n\n# License: BSD 3 clause\n\nRANDOM_STATE = 42\nFIG_SIZE = (10, 7)\n\n\nfeatures, target = load_wine(return_X_y=True)\n\n# Make a train/test split using 30% test size\nX_train, X_test, y_train, y_test = train_test_split(features, target,\n                                                    test_size=0.30,\n                                                    random_state=RANDOM_STATE)\n\n# Fit to data and predict using pipelined GNB and PCA.\nunscaled_clf = make_pipeline(PCA(n_components=2), GaussianNB())\nunscaled_clf.fit(X_train, y_train)\npred_test = unscaled_clf.predict(X_test)\n\n# Fit to data and predict using pipelined scaling, GNB and PCA.\nstd_clf = make_pipeline(StandardScaler(), PCA(n_components=2), GaussianNB())\nstd_clf.fit(X_train, y_train)\npred_test_std = std_clf.predict(X_test)\n\n# Show prediction accuracies in scaled and unscaled data.\nprint(\'\\nPrediction accuracy for the normal test dataset with PCA\')\nprint(\'{:.2%}\\n\'.format(metrics.accuracy_score(y_test, pred_test)))\n\nprint(\'\\nPrediction accuracy for the standardized test dataset with PCA\')\nprint(\'{:.2%}\\n\'.format(metrics.accuracy_score(y_test, pred_test_std)))\n\n# Extract PCA from pipeline\npca = unscaled_clf.named_steps[\'pca\']\npca_std = std_clf.named_steps[\'pca\']\n\n# Show first principal components\nprint(\'\\nPC 1 without scaling:\\n\', pca.components_[0])\nprint(\'\\nPC 1 with scaling:\\n\', pca_std.components_[0])\n\n# Use PCA without and with scale on X_train data for visualization.\nX_train_transformed = pca.transform(X_train)\nscaler = std_clf.named_steps[\'standardscaler\']\nX_train_std_transformed = pca_std.transform(scaler.transform(X_train))\n\n# visualize standardized vs. untouched dataset with PCA performed\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=FIG_SIZE)\n\n\nfor l, c, m in zip(range(0, 3), (\'blue\', \'red\', \'green\'), (\'^\', \'s\', \'o\')):\n    ax1.scatter(X_train_transformed[y_train == l, 0],\n                X_train_transformed[y_train == l, 1],\n                color=c,\n                label=\'class %s\' % l,\n                alpha=0.5,\n                marker=m\n                )\n\nfor l, c, m in zip(range(0, 3), (\'blue\', \'red\', \'green\'), (\'^\', \'s\', \'o\')):\n    ax2.scatter(X_train_std_transformed[y_train == l, 0],\n                X_train_std_transformed[y_train == l, 1],\n                color=c,\n                label=\'class %s\' % l,\n                alpha=0.5,\n                marker=m\n                )\n\nax1.set_title(\'Training dataset after PCA\')\nax2.set_title(\'Standardized training dataset after PCA\')\n\nfor ax in (ax1, ax2):\n    ax.set_xlabel(\'1st principal component\')\n    ax.set_ylabel(\'2nd principal component\')\n    ax.legend(loc=\'upper right\')\n    ax.grid()\n\nplt.tight_layout()\n\nplt.show()\n'"
scikit-learn/_downloads/d1e8e2b45aa2ea0b042d9c00faf39e99/plot_stack_predictors.py,5,"b'""""""\n=================================\nCombine predictors using stacking\n=================================\n\nStacking refers to a method to blend estimators. In this strategy, some\nestimators are individually fitted on some training data while a final\nestimator is trained using the stacked predictions of these base estimators.\n\nIn this example, we illustrate the use case in which different regressors are\nstacked together and a final linear penalized regressor is used to output the\nprediction. We compare the performance of each individual regressor with the\nstacking strategy. Stacking slightly improves the overall performance.\n\n""""""\nprint(__doc__)\n\n# Authors: Guillaume Lemaitre <g.lemaitre58@gmail.com>\n# License: BSD 3 clause\n\n###############################################################################\n# The function ``plot_regression_results`` is used to plot the predicted and\n# true targets.\n\nimport matplotlib.pyplot as plt\n\n\ndef plot_regression_results(ax, y_true, y_pred, title, scores, elapsed_time):\n    """"""Scatter plot of the predicted vs true targets.""""""\n    ax.plot([y_true.min(), y_true.max()],\n            [y_true.min(), y_true.max()],\n            \'--r\', linewidth=2)\n    ax.scatter(y_true, y_pred, alpha=0.2)\n\n    ax.spines[\'top\'].set_visible(False)\n    ax.spines[\'right\'].set_visible(False)\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.spines[\'left\'].set_position((\'outward\', 10))\n    ax.spines[\'bottom\'].set_position((\'outward\', 10))\n    ax.set_xlim([y_true.min(), y_true.max()])\n    ax.set_ylim([y_true.min(), y_true.max()])\n    ax.set_xlabel(\'Measured\')\n    ax.set_ylabel(\'Predicted\')\n    extra = plt.Rectangle((0, 0), 0, 0, fc=""w"", fill=False,\n                          edgecolor=\'none\', linewidth=0)\n    ax.legend([extra], [scores], loc=\'upper left\')\n    title = title + \'\\n Evaluation in {:.2f} seconds\'.format(elapsed_time)\n    ax.set_title(title)\n\n\n###############################################################################\n# Stack of predictors on a single data set\n###############################################################################\n# It is sometimes tedious to find the model which will best perform on a given\n# dataset. Stacking provide an alternative by combining the outputs of several\n# learners, without the need to choose a model specifically. The performance of\n# stacking is usually close to the best model and sometimes it can outperform\n# the prediction performance of each individual model.\n#\n# Here, we combine 3 learners (linear and non-linear) and use a ridge regressor\n# to combine their outputs together.\n\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\n\nestimators = [\n    (\'Random Forest\', RandomForestRegressor(random_state=42)),\n    (\'Lasso\', LassoCV()),\n    (\'Gradient Boosting\', HistGradientBoostingRegressor(random_state=0))\n]\nstacking_regressor = StackingRegressor(\n    estimators=estimators, final_estimator=RidgeCV()\n)\n\n\n###############################################################################\n# We used the Boston data set (prediction of house prices). We check the\n# performance of each individual predictor as well as the stack of the\n# regressors.\n\nimport time\nimport numpy as np\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import cross_validate, cross_val_predict\n\nX, y = load_boston(return_X_y=True)\n\nfig, axs = plt.subplots(2, 2, figsize=(9, 7))\naxs = np.ravel(axs)\n\nfor ax, (name, est) in zip(axs, estimators + [(\'Stacking Regressor\',\n                                               stacking_regressor)]):\n    start_time = time.time()\n    score = cross_validate(est, X, y,\n                           scoring=[\'r2\', \'neg_mean_absolute_error\'],\n                           n_jobs=-1, verbose=0)\n    elapsed_time = time.time() - start_time\n\n    y_pred = cross_val_predict(est, X, y, n_jobs=-1, verbose=0)\n    plot_regression_results(\n        ax, y, y_pred,\n        name,\n        (r\'$R^2={:.2f} \\pm {:.2f}$\' + \'\\n\' + r\'$MAE={:.2f} \\pm {:.2f}$\')\n        .format(np.mean(score[\'test_r2\']),\n                np.std(score[\'test_r2\']),\n                -np.mean(score[\'test_neg_mean_absolute_error\']),\n                np.std(score[\'test_neg_mean_absolute_error\'])),\n        elapsed_time)\n\nplt.suptitle(\'Single predictors versus stacked predictors\')\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)\nplt.show()\n\n###############################################################################\n# The stacked regressor will combine the strengths of the different regressors.\n# However, we also see that training the stacked regressor is much more\n# computationally expensive.\n'"
scikit-learn/_downloads/d2ec514564566317b971f0f6c2b13d71/plot_ols.py,1,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nLinear Regression Example\n=========================================================\nThis example uses the only the first feature of the `diabetes` dataset, in\norder to illustrate a two-dimensional plot of this regression technique. The\nstraight line can be seen in the plot, showing how linear regression attempts\nto draw a straight line that will best minimize the residual sum of squares\nbetween the observed responses in the dataset, and the responses predicted by\nthe linear approximation.\n\nThe coefficients, the residual sum of squares and the coefficient\nof determination are also calculated.\n\n""""""\nprint(__doc__)\n\n\n# Code source: Jaques Grobler\n# License: BSD 3 clause\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n\n# Use only one feature\ndiabetes_X = diabetes_X[:, np.newaxis, 2]\n\n# Split the data into training/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training/testing sets\ndiabetes_y_train = diabetes_y[:-20]\ndiabetes_y_test = diabetes_y[-20:]\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint(\'Coefficients: \\n\', regr.coef_)\n# The mean squared error\nprint(\'Mean squared error: %.2f\'\n      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\'Coefficient of determination: %.2f\'\n      % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color=\'black\')\nplt.plot(diabetes_X_test, diabetes_y_pred, color=\'blue\', linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n'"
scikit-learn/_downloads/d32868f3033d64f20baae35d3f96d37c/plot_coin_ward_segmentation.py,3,"b'""""""\n======================================================================\nA demo of structured Ward hierarchical clustering on an image of coins\n======================================================================\n\nCompute the segmentation of a 2D image with Ward hierarchical\nclustering. The clustering is spatially constrained in order\nfor each segmented region to be in one piece.\n""""""\n\n# Author : Vincent Michel, 2010\n#          Alexandre Gramfort, 2011\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport time as time\n\nimport numpy as np\nfrom distutils.version import LooseVersion\nfrom scipy.ndimage.filters import gaussian_filter\n\nimport matplotlib.pyplot as plt\n\nimport skimage\nfrom skimage.data import coins\nfrom skimage.transform import rescale\n\nfrom sklearn.feature_extraction.image import grid_to_graph\nfrom sklearn.cluster import AgglomerativeClustering\n\n# these were introduced in skimage-0.14\nif LooseVersion(skimage.__version__) >= \'0.14\':\n    rescale_params = {\'anti_aliasing\': False, \'multichannel\': False}\nelse:\n    rescale_params = {}\n\n# #############################################################################\n# Generate data\norig_coins = coins()\n\n# Resize it to 20% of the original size to speed up the processing\n# Applying a Gaussian filter for smoothing prior to down-scaling\n# reduces aliasing artifacts.\nsmoothened_coins = gaussian_filter(orig_coins, sigma=2)\nrescaled_coins = rescale(smoothened_coins, 0.2, mode=""reflect"",\n                         **rescale_params)\n\nX = np.reshape(rescaled_coins, (-1, 1))\n\n# #############################################################################\n# Define the structure A of the data. Pixels connected to their neighbors.\nconnectivity = grid_to_graph(*rescaled_coins.shape)\n\n# #############################################################################\n# Compute clustering\nprint(""Compute structured hierarchical clustering..."")\nst = time.time()\nn_clusters = 27  # number of regions\nward = AgglomerativeClustering(n_clusters=n_clusters, linkage=\'ward\',\n                               connectivity=connectivity)\nward.fit(X)\nlabel = np.reshape(ward.labels_, rescaled_coins.shape)\nprint(""Elapsed time: "", time.time() - st)\nprint(""Number of pixels: "", label.size)\nprint(""Number of clusters: "", np.unique(label).size)\n\n# #############################################################################\n# Plot the results on an image\nplt.figure(figsize=(5, 5))\nplt.imshow(rescaled_coins, cmap=plt.cm.gray)\nfor l in range(n_clusters):\n    plt.contour(label == l,\n                colors=[plt.cm.nipy_spectral(l / float(n_clusters)), ])\nplt.xticks(())\nplt.yticks(())\nplt.show()\n'"
scikit-learn/_downloads/d5b3697b1c79100e31e977690e5e90fe/plot_svm_regression.py,5,"b'""""""\n===================================================================\nSupport Vector Regression (SVR) using linear and non-linear kernels\n===================================================================\n\nToy example of 1D regression using linear, polynomial and RBF kernels.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nfrom sklearn.svm import SVR\nimport matplotlib.pyplot as plt\n\n# #############################################################################\n# Generate sample data\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\n\n# #############################################################################\n# Add noise to targets\ny[::5] += 3 * (0.5 - np.random.rand(8))\n\n# #############################################################################\n# Fit regression model\nsvr_rbf = SVR(kernel=\'rbf\', C=100, gamma=0.1, epsilon=.1)\nsvr_lin = SVR(kernel=\'linear\', C=100, gamma=\'auto\')\nsvr_poly = SVR(kernel=\'poly\', C=100, gamma=\'auto\', degree=3, epsilon=.1,\n               coef0=1)\n\n# #############################################################################\n# Look at the results\nlw = 2\n\nsvrs = [svr_rbf, svr_lin, svr_poly]\nkernel_label = [\'RBF\', \'Linear\', \'Polynomial\']\nmodel_color = [\'m\', \'c\', \'g\']\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)\nfor ix, svr in enumerate(svrs):\n    axes[ix].plot(X, svr.fit(X, y).predict(X), color=model_color[ix], lw=lw,\n                  label=\'{} model\'.format(kernel_label[ix]))\n    axes[ix].scatter(X[svr.support_], y[svr.support_], facecolor=""none"",\n                     edgecolor=model_color[ix], s=50,\n                     label=\'{} support vectors\'.format(kernel_label[ix]))\n    axes[ix].scatter(X[np.setdiff1d(np.arange(len(X)), svr.support_)],\n                     y[np.setdiff1d(np.arange(len(X)), svr.support_)],\n                     facecolor=""none"", edgecolor=""k"", s=50,\n                     label=\'other training data\')\n    axes[ix].legend(loc=\'upper center\', bbox_to_anchor=(0.5, 1.1),\n                    ncol=1, fancybox=True, shadow=True)\n\nfig.text(0.5, 0.04, \'data\', ha=\'center\', va=\'center\')\nfig.text(0.06, 0.5, \'target\', ha=\'center\', va=\'center\', rotation=\'vertical\')\nfig.suptitle(""Support Vector Regression"", fontsize=14)\nplt.show()\n'"
scikit-learn/_downloads/d6755de7e5532308ecab20699f996090/plot_linkage_comparison.py,5,"b'""""""\n================================================================\nComparing different hierarchical linkage methods on toy datasets\n================================================================\n\nThis example shows characteristics of different linkage\nmethods for hierarchical clustering on datasets that are\n""interesting"" but still in 2D.\n\nThe main observations to make are:\n\n- single linkage is fast, and can perform well on\n  non-globular data, but it performs poorly in the\n  presence of noise.\n- average and complete linkage perform well on\n  cleanly separated globular clusters, but have mixed\n  results otherwise.\n- Ward is the most effective method for noisy data.\n\nWhile these examples give some intuition about the\nalgorithms, this intuition might not apply to very high\ndimensional data.\n""""""\nprint(__doc__)\n\nimport time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom itertools import cycle, islice\n\nnp.random.seed(0)\n\n######################################################################\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n\nn_samples = 1500\nnoisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n                                      noise=.05)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\nno_structure = np.random.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(n_samples=n_samples,\n                             cluster_std=[1.0, 2.5, 0.5],\n                             random_state=random_state)\n\n######################################################################\n# Run the clustering and plot\n\n# Set up cluster parameters\nplt.figure(figsize=(9 * 1.3 + 2, 14.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n                    hspace=.01)\n\nplot_num = 1\n\ndefault_base = {\'n_neighbors\': 10,\n                \'n_clusters\': 3}\n\ndatasets = [\n    (noisy_circles, {\'n_clusters\': 2}),\n    (noisy_moons, {\'n_clusters\': 2}),\n    (varied, {\'n_neighbors\': 2}),\n    (aniso, {\'n_neighbors\': 2}),\n    (blobs, {}),\n    (no_structure, {})]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\'n_clusters\'], linkage=\'ward\')\n    complete = cluster.AgglomerativeClustering(\n        n_clusters=params[\'n_clusters\'], linkage=\'complete\')\n    average = cluster.AgglomerativeClustering(\n        n_clusters=params[\'n_clusters\'], linkage=\'average\')\n    single = cluster.AgglomerativeClustering(\n        n_clusters=params[\'n_clusters\'], linkage=\'single\')\n\n    clustering_algorithms = (\n        (\'Single Linkage\', single),\n        (\'Average Linkage\', average),\n        (\'Complete Linkage\', complete),\n        (\'Ward Linkage\', ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                ""ignore"",\n                message=""the number of connected components of the "" +\n                ""connectivity matrix is [0-9]{1,2}"" +\n                "" > 1. Completing it to avoid stopping the tree early."",\n                category=UserWarning)\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \'labels_\'):\n            y_pred = algorithm.labels_.astype(np.int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        colors = np.array(list(islice(cycle([\'#377eb8\', \'#ff7f00\', \'#4daf4a\',\n                                             \'#f781bf\', \'#a65628\', \'#984ea3\',\n                                             \'#999999\', \'#e41a1c\', \'#dede00\']),\n                                      int(max(y_pred) + 1))))\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(.99, .01, (\'%.2fs\' % (t1 - t0)).lstrip(\'0\'),\n                 transform=plt.gca().transAxes, size=15,\n                 horizontalalignment=\'right\')\n        plot_num += 1\n\nplt.show()\n'"
scikit-learn/_downloads/d67bbbfe789f56777b71ffb9a3800a03/plot_concentration_prior.py,12,"b'""""""\n========================================================================\nConcentration Prior Type Analysis of Variation Bayesian Gaussian Mixture\n========================================================================\n\nThis example plots the ellipsoids obtained from a toy dataset (mixture of three\nGaussians) fitted by the ``BayesianGaussianMixture`` class models with a\nDirichlet distribution prior\n(``weight_concentration_prior_type=\'dirichlet_distribution\'``) and a Dirichlet\nprocess prior (``weight_concentration_prior_type=\'dirichlet_process\'``). On\neach figure, we plot the results for three different values of the weight\nconcentration prior.\n\nThe ``BayesianGaussianMixture`` class can adapt its number of mixture\ncomponents automatically. The parameter ``weight_concentration_prior`` has a\ndirect link with the resulting number of components with non-zero weights.\nSpecifying a low value for the concentration prior will make the model put most\nof the weight on few components set the remaining components weights very close\nto zero. High values of the concentration prior will allow a larger number of\ncomponents to be active in the mixture.\n\nThe Dirichlet process prior allows to define an infinite number of components\nand automatically selects the correct number of components: it activates a\ncomponent only if it is necessary.\n\nOn the contrary the classical finite mixture model with a Dirichlet\ndistribution prior will favor more uniformly weighted components and therefore\ntends to divide natural clusters into unnecessary sub-components.\n""""""\n# Author: Thierry Guillemot <thierry.guillemot.work@gmail.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom sklearn.mixture import BayesianGaussianMixture\n\nprint(__doc__)\n\n\ndef plot_ellipses(ax, weights, means, covars):\n    for n in range(means.shape[0]):\n        eig_vals, eig_vecs = np.linalg.eigh(covars[n])\n        unit_eig_vec = eig_vecs[0] / np.linalg.norm(eig_vecs[0])\n        angle = np.arctan2(unit_eig_vec[1], unit_eig_vec[0])\n        # Ellipse needs degrees\n        angle = 180 * angle / np.pi\n        # eigenvector normalization\n        eig_vals = 2 * np.sqrt(2) * np.sqrt(eig_vals)\n        ell = mpl.patches.Ellipse(means[n], eig_vals[0], eig_vals[1],\n                                  180 + angle, edgecolor=\'black\')\n        ell.set_clip_box(ax.bbox)\n        ell.set_alpha(weights[n])\n        ell.set_facecolor(\'#56B4E9\')\n        ax.add_artist(ell)\n\n\ndef plot_results(ax1, ax2, estimator, X, y, title, plot_title=False):\n    ax1.set_title(title)\n    ax1.scatter(X[:, 0], X[:, 1], s=5, marker=\'o\', color=colors[y], alpha=0.8)\n    ax1.set_xlim(-2., 2.)\n    ax1.set_ylim(-3., 3.)\n    ax1.set_xticks(())\n    ax1.set_yticks(())\n    plot_ellipses(ax1, estimator.weights_, estimator.means_,\n                  estimator.covariances_)\n\n    ax2.get_xaxis().set_tick_params(direction=\'out\')\n    ax2.yaxis.grid(True, alpha=0.7)\n    for k, w in enumerate(estimator.weights_):\n        ax2.bar(k, w, width=0.9, color=\'#56B4E9\', zorder=3,\n                align=\'center\', edgecolor=\'black\')\n        ax2.text(k, w + 0.007, ""%.1f%%"" % (w * 100.),\n                 horizontalalignment=\'center\')\n    ax2.set_xlim(-.6, 2 * n_components - .4)\n    ax2.set_ylim(0., 1.1)\n    ax2.tick_params(axis=\'y\', which=\'both\', left=False,\n                    right=False, labelleft=False)\n    ax2.tick_params(axis=\'x\', which=\'both\', top=False)\n\n    if plot_title:\n        ax1.set_ylabel(\'Estimated Mixtures\')\n        ax2.set_ylabel(\'Weight of each component\')\n\n# Parameters of the dataset\nrandom_state, n_components, n_features = 2, 3, 2\ncolors = np.array([\'#0072B2\', \'#F0E442\', \'#D55E00\'])\n\ncovars = np.array([[[.7, .0], [.0, .1]],\n                   [[.5, .0], [.0, .1]],\n                   [[.5, .0], [.0, .1]]])\nsamples = np.array([200, 500, 200])\nmeans = np.array([[.0, -.70],\n                  [.0, .0],\n                  [.0, .70]])\n\n# mean_precision_prior= 0.8 to minimize the influence of the prior\nestimators = [\n    (""Finite mixture with a Dirichlet distribution\\nprior and ""\n     r""$\\gamma_0=$"", BayesianGaussianMixture(\n        weight_concentration_prior_type=""dirichlet_distribution"",\n        n_components=2 * n_components, reg_covar=0, init_params=\'random\',\n        max_iter=1500, mean_precision_prior=.8,\n        random_state=random_state), [0.001, 1, 1000]),\n    (""Infinite mixture with a Dirichlet process\\n prior and"" r""$\\gamma_0=$"",\n     BayesianGaussianMixture(\n        weight_concentration_prior_type=""dirichlet_process"",\n        n_components=2 * n_components, reg_covar=0, init_params=\'random\',\n        max_iter=1500, mean_precision_prior=.8,\n        random_state=random_state), [1, 1000, 100000])]\n\n# Generate data\nrng = np.random.RandomState(random_state)\nX = np.vstack([\n    rng.multivariate_normal(means[j], covars[j], samples[j])\n    for j in range(n_components)])\ny = np.concatenate([np.full(samples[j], j, dtype=int)\n                    for j in range(n_components)])\n\n# Plot results in two different figures\nfor (title, estimator, concentrations_prior) in estimators:\n    plt.figure(figsize=(4.7 * 3, 8))\n    plt.subplots_adjust(bottom=.04, top=0.90, hspace=.05, wspace=.05,\n                        left=.03, right=.99)\n\n    gs = gridspec.GridSpec(3, len(concentrations_prior))\n    for k, concentration in enumerate(concentrations_prior):\n        estimator.weight_concentration_prior = concentration\n        estimator.fit(X)\n        plot_results(plt.subplot(gs[0:2, k]), plt.subplot(gs[2, k]), estimator,\n                     X, y, r""%s$%.1e$"" % (title, concentration),\n                     plot_title=k == 0)\n\nplt.show()\n'"
scikit-learn/_downloads/dac19052f713f6b506316ee1109fad4c/plot_digits_agglomeration.py,3,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nFeature agglomeration\n=========================================================\n\nThese images how similar features are merged together using\nfeature agglomeration.\n""""""\nprint(__doc__)\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import datasets, cluster\nfrom sklearn.feature_extraction.image import grid_to_graph\n\ndigits = datasets.load_digits()\nimages = digits.images\nX = np.reshape(images, (len(images), -1))\nconnectivity = grid_to_graph(*images[0].shape)\n\nagglo = cluster.FeatureAgglomeration(connectivity=connectivity,\n                                     n_clusters=32)\n\nagglo.fit(X)\nX_reduced = agglo.transform(X)\n\nX_restored = agglo.inverse_transform(X_reduced)\nimages_restored = np.reshape(X_restored, images.shape)\nplt.figure(1, figsize=(4, 3.5))\nplt.clf()\nplt.subplots_adjust(left=.01, right=.99, bottom=.01, top=.91)\nfor i in range(4):\n    plt.subplot(3, 4, i + 1)\n    plt.imshow(images[i], cmap=plt.cm.gray, vmax=16, interpolation=\'nearest\')\n    plt.xticks(())\n    plt.yticks(())\n    if i == 1:\n        plt.title(\'Original data\')\n    plt.subplot(3, 4, 4 + i + 1)\n    plt.imshow(images_restored[i], cmap=plt.cm.gray, vmax=16,\n               interpolation=\'nearest\')\n    if i == 1:\n        plt.title(\'Agglomerated data\')\n    plt.xticks(())\n    plt.yticks(())\n\nplt.subplot(3, 4, 10)\nplt.imshow(np.reshape(agglo.labels_, images[0].shape),\n           interpolation=\'nearest\', cmap=plt.cm.nipy_spectral)\nplt.xticks(())\nplt.yticks(())\nplt.title(\'Labels\')\nplt.show()\n'"
scikit-learn/_downloads/db23ea307303ad4711c15a6422fdaa80/wikipedia_principal_eigenvector.py,10,"b'""""""\n===============================\nWikipedia principal eigenvector\n===============================\n\nA classical way to assert the relative importance of vertices in a\ngraph is to compute the principal eigenvector of the adjacency matrix\nso as to assign to each vertex the values of the components of the first\neigenvector as a centrality score:\n\n    https://en.wikipedia.org/wiki/Eigenvector_centrality\n\nOn the graph of webpages and links those values are called the PageRank\nscores by Google.\n\nThe goal of this example is to analyze the graph of links inside\nwikipedia articles to rank articles by relative importance according to\nthis eigenvector centrality.\n\nThe traditional way to compute the principal eigenvector is to use the\npower iteration method:\n\n    https://en.wikipedia.org/wiki/Power_iteration\n\nHere the computation is achieved thanks to Martinsson\'s Randomized SVD\nalgorithm implemented in scikit-learn.\n\nThe graph data is fetched from the DBpedia dumps. DBpedia is an extraction\nof the latent structured data of the Wikipedia content.\n""""""\n\n# Author: Olivier Grisel <olivier.grisel@ensta.org>\n# License: BSD 3 clause\n\nfrom bz2 import BZ2File\nimport os\nfrom datetime import datetime\nfrom pprint import pprint\nfrom time import time\n\nimport numpy as np\n\nfrom scipy import sparse\n\nfrom joblib import Memory\n\nfrom sklearn.decomposition import randomized_svd\nfrom urllib.request import urlopen\n\n\nprint(__doc__)\n\n# #############################################################################\n# Where to download the data, if not already on disk\nredirects_url = ""http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2""\nredirects_filename = redirects_url.rsplit(""/"", 1)[1]\n\npage_links_url = ""http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2""\npage_links_filename = page_links_url.rsplit(""/"", 1)[1]\n\nresources = [\n    (redirects_url, redirects_filename),\n    (page_links_url, page_links_filename),\n]\n\nfor url, filename in resources:\n    if not os.path.exists(filename):\n        print(""Downloading data from \'%s\', please wait..."" % url)\n        opener = urlopen(url)\n        open(filename, \'wb\').write(opener.read())\n        print()\n\n\n# #############################################################################\n# Loading the redirect files\n\nmemory = Memory(cachedir=""."")\n\n\ndef index(redirects, index_map, k):\n    """"""Find the index of an article name after redirect resolution""""""\n    k = redirects.get(k, k)\n    return index_map.setdefault(k, len(index_map))\n\n\nDBPEDIA_RESOURCE_PREFIX_LEN = len(""http://dbpedia.org/resource/"")\nSHORTNAME_SLICE = slice(DBPEDIA_RESOURCE_PREFIX_LEN + 1, -1)\n\n\ndef short_name(nt_uri):\n    """"""Remove the < and > URI markers and the common URI prefix""""""\n    return nt_uri[SHORTNAME_SLICE]\n\n\ndef get_redirects(redirects_filename):\n    """"""Parse the redirections and build a transitively closed map out of it""""""\n    redirects = {}\n    print(""Parsing the NT redirect file"")\n    for l, line in enumerate(BZ2File(redirects_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print(""ignoring malformed line: "" + line)\n            continue\n        redirects[short_name(split[0])] = short_name(split[2])\n        if l % 1000000 == 0:\n            print(""[%s] line: %08d"" % (datetime.now().isoformat(), l))\n\n    # compute the transitive closure\n    print(""Computing the transitive closure of the redirect relation"")\n    for l, source in enumerate(redirects.keys()):\n        transitive_target = None\n        target = redirects[source]\n        seen = {source}\n        while True:\n            transitive_target = target\n            target = redirects.get(target)\n            if target is None or target in seen:\n                break\n            seen.add(target)\n        redirects[source] = transitive_target\n        if l % 1000000 == 0:\n            print(""[%s] line: %08d"" % (datetime.now().isoformat(), l))\n\n    return redirects\n\n\n# disabling joblib as the pickling of large dicts seems much too slow\n#@memory.cache\ndef get_adjacency_matrix(redirects_filename, page_links_filename, limit=None):\n    """"""Extract the adjacency graph as a scipy sparse matrix\n\n    Redirects are resolved first.\n\n    Returns X, the scipy sparse adjacency matrix, redirects as python\n    dict from article names to article names and index_map a python dict\n    from article names to python int (article indexes).\n    """"""\n\n    print(""Computing the redirect map"")\n    redirects = get_redirects(redirects_filename)\n\n    print(""Computing the integer index map"")\n    index_map = dict()\n    links = list()\n    for l, line in enumerate(BZ2File(page_links_filename)):\n        split = line.split()\n        if len(split) != 4:\n            print(""ignoring malformed line: "" + line)\n            continue\n        i = index(redirects, index_map, short_name(split[0]))\n        j = index(redirects, index_map, short_name(split[2]))\n        links.append((i, j))\n        if l % 1000000 == 0:\n            print(""[%s] line: %08d"" % (datetime.now().isoformat(), l))\n\n        if limit is not None and l >= limit - 1:\n            break\n\n    print(""Computing the adjacency matrix"")\n    X = sparse.lil_matrix((len(index_map), len(index_map)), dtype=np.float32)\n    for i, j in links:\n        X[i, j] = 1.0\n    del links\n    print(""Converting to CSR representation"")\n    X = X.tocsr()\n    print(""CSR conversion done"")\n    return X, redirects, index_map\n\n\n# stop after 5M links to make it possible to work in RAM\nX, redirects, index_map = get_adjacency_matrix(\n    redirects_filename, page_links_filename, limit=5000000)\nnames = {i: name for name, i in index_map.items()}\n\nprint(""Computing the principal singular vectors using randomized_svd"")\nt0 = time()\nU, s, V = randomized_svd(X, 5, n_iter=3)\nprint(""done in %0.3fs"" % (time() - t0))\n\n# print the names of the wikipedia related strongest components of the\n# principal singular vector which should be similar to the highest eigenvector\nprint(""Top wikipedia pages according to principal singular vectors"")\npprint([names[i] for i in np.abs(U.T[0]).argsort()[-10:]])\npprint([names[i] for i in np.abs(V[0]).argsort()[-10:]])\n\n\ndef centrality_scores(X, alpha=0.85, max_iter=100, tol=1e-10):\n    """"""Power iteration computation of the principal eigenvector\n\n    This method is also known as Google PageRank and the implementation\n    is based on the one from the NetworkX project (BSD licensed too)\n    with copyrights by:\n\n      Aric Hagberg <hagberg@lanl.gov>\n      Dan Schult <dschult@colgate.edu>\n      Pieter Swart <swart@lanl.gov>\n    """"""\n    n = X.shape[0]\n    X = X.copy()\n    incoming_counts = np.asarray(X.sum(axis=1)).ravel()\n\n    print(""Normalizing the graph"")\n    for i in incoming_counts.nonzero()[0]:\n        X.data[X.indptr[i]:X.indptr[i + 1]] *= 1.0 / incoming_counts[i]\n    dangle = np.asarray(np.where(np.isclose(X.sum(axis=1), 0),\n                                 1.0 / n, 0)).ravel()\n\n    scores = np.full(n, 1. / n, dtype=np.float32)  # initial guess\n    for i in range(max_iter):\n        print(""power iteration #%d"" % i)\n        prev_scores = scores\n        scores = (alpha * (scores * X + np.dot(dangle, prev_scores))\n                  + (1 - alpha) * prev_scores.sum() / n)\n        # check convergence: normalized l_inf norm\n        scores_max = np.abs(scores).max()\n        if scores_max == 0.0:\n            scores_max = 1.0\n        err = np.abs(scores - prev_scores).max() / scores_max\n        print(""error: %0.6f"" % err)\n        if err < n * tol:\n            return scores\n\n    return scores\n\nprint(""Computing principal eigenvector score using a power iteration method"")\nt0 = time()\nscores = centrality_scores(X, max_iter=100)\nprint(""done in %0.3fs"" % (time() - t0))\npprint([names[i] for i in np.abs(scores).argsort()[-10:]])\n'"
scikit-learn/_downloads/db9ffcaedfa9f222e08014026619ba35/plot_roc_curve_visualization_api.py,0,"b'""""""\n================================\nROC Curve with Visualization API\n================================\nScikit-learn defines a simple API for creating visualizations for machine\nlearning. The key features of this API is to allow for quick plotting and\nvisual adjustments without recalculation. In this example, we will demonstrate\nhow to use the visualization API by comparing ROC curves.\n""""""\nprint(__doc__)\n\n##############################################################################\n# Load Data and Train a SVC\n# -------------------------\n# First, we load the wine dataset and convert it to a binary classification\n# problem. Then, we train a support vector classifier on a training dataset.\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_wine(return_X_y=True)\ny = y == 2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nsvc = SVC(random_state=42)\nsvc.fit(X_train, y_train)\n\n##############################################################################\n# Plotting the ROC Curve\n# ----------------------\n# Next, we plot the ROC curve with a single call to\n# :func:`sklearn.metrics.plot_roc_curve`. The returned `svc_disp` object allows\n# us to continue using the already computed ROC curve for the SVC in future\n# plots.\nsvc_disp = plot_roc_curve(svc, X_test, y_test)\nplt.show()\n\n##############################################################################\n# Training a Random Forest and Plotting the ROC Curve\n# --------------------------------------------------------\n# We train a random forest classifier and create a plot comparing it to the SVC\n# ROC curve. Notice how `svc_disp` uses\n# :func:`~sklearn.metrics.RocCurveDisplay.plot` to plot the SVC ROC curve\n# without recomputing the values of the roc curve itself. Futhermore, we\n# pass `alpha=0.8` to the plot functions to adjust the alpha values of the\n# curves.\nrfc = RandomForestClassifier(n_estimators=10, random_state=42)\nrfc.fit(X_train, y_train)\nax = plt.gca()\nrfc_disp = plot_roc_curve(rfc, X_test, y_test, ax=ax, alpha=0.8)\nsvc_disp.plot(ax=ax, alpha=0.8)\nplt.show()\n'"
scikit-learn/_downloads/dcc7ed652e9a99ed5d1f904c5b81f9f5/plot_ransac.py,5,"b'""""""\n===========================================\nRobust linear model estimation using RANSAC\n===========================================\n\nIn this example we see how to robustly fit a linear model to faulty data using\nthe RANSAC algorithm.\n\n""""""\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import linear_model, datasets\n\n\nn_samples = 1000\nn_outliers = 50\n\n\nX, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n                                      n_informative=1, noise=10,\n                                      coef=True, random_state=0)\n\n# Add outlier data\nnp.random.seed(0)\nX[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\ny[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n\n# Fit line using all data\nlr = linear_model.LinearRegression()\nlr.fit(X, y)\n\n# Robustly fit linear model with RANSAC algorithm\nransac = linear_model.RANSACRegressor()\nransac.fit(X, y)\ninlier_mask = ransac.inlier_mask_\noutlier_mask = np.logical_not(inlier_mask)\n\n# Predict data of estimated models\nline_X = np.arange(X.min(), X.max())[:, np.newaxis]\nline_y = lr.predict(line_X)\nline_y_ransac = ransac.predict(line_X)\n\n# Compare estimated coefficients\nprint(""Estimated coefficients (true, linear regression, RANSAC):"")\nprint(coef, lr.coef_, ransac.estimator_.coef_)\n\nlw = 2\nplt.scatter(X[inlier_mask], y[inlier_mask], color=\'yellowgreen\', marker=\'.\',\n            label=\'Inliers\')\nplt.scatter(X[outlier_mask], y[outlier_mask], color=\'gold\', marker=\'.\',\n            label=\'Outliers\')\nplt.plot(line_X, line_y, color=\'navy\', linewidth=lw, label=\'Linear regressor\')\nplt.plot(line_X, line_y_ransac, color=\'cornflowerblue\', linewidth=lw,\n         label=\'RANSAC regressor\')\nplt.legend(loc=\'lower right\')\nplt.xlabel(""Input"")\nplt.ylabel(""Response"")\nplt.show()\n'"
scikit-learn/_downloads/dd15b8360dac257e4497c6abd5e3eaf7/plot_iterative_imputer_variants_comparison.py,5,"b'""""""\n=========================================================\nImputing missing values with variants of IterativeImputer\n=========================================================\n\nThe :class:`sklearn.impute.IterativeImputer` class is very flexible - it can be\nused with a variety of estimators to do round-robin regression, treating every\nvariable as an output in turn.\n\nIn this example we compare some estimators for the purpose of missing feature\nimputation with :class:`sklearn.impute.IterativeImputer`:\n\n* :class:`~sklearn.linear_model.BayesianRidge`: regularized linear regression\n* :class:`~sklearn.tree.DecisionTreeRegressor`: non-linear regression\n* :class:`~sklearn.ensemble.ExtraTreesRegressor`: similar to missForest in R\n* :class:`~sklearn.neighbors.KNeighborsRegressor`: comparable to other KNN\n  imputation approaches\n\nOf particular interest is the ability of\n:class:`sklearn.impute.IterativeImputer` to mimic the behavior of missForest, a\npopular imputation package for R. In this example, we have chosen to use\n:class:`sklearn.ensemble.ExtraTreesRegressor` instead of\n:class:`sklearn.ensemble.RandomForestRegressor` (as in missForest) due to its\nincreased speed.\n\nNote that :class:`sklearn.neighbors.KNeighborsRegressor` is different from KNN\nimputation, which learns from samples with missing values by using a distance\nmetric that accounts for missing values, rather than imputing them.\n\nThe goal is to compare different estimators to see which one is best for the\n:class:`sklearn.impute.IterativeImputer` when using a\n:class:`sklearn.linear_model.BayesianRidge` estimator on the California housing\ndataset with a single value randomly removed from each row.\n\nFor this particular pattern of missing values we see that\n:class:`sklearn.ensemble.ExtraTreesRegressor` and\n:class:`sklearn.linear_model.BayesianRidge` give the best results.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# To use this experimental feature, we need to explicitly ask for it:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\n\nN_SPLITS = 5\n\nrng = np.random.RandomState(0)\n\nX_full, y_full = fetch_california_housing(return_X_y=True)\n# ~2k samples is enough for the purpose of the example.\n# Remove the following two lines for a slower run with different error bars.\nX_full = X_full[::10]\ny_full = y_full[::10]\nn_samples, n_features = X_full.shape\n\n# Estimate the score on the entire dataset, with no missing values\nbr_estimator = BayesianRidge()\nscore_full_data = pd.DataFrame(\n    cross_val_score(\n        br_estimator, X_full, y_full, scoring=\'neg_mean_squared_error\',\n        cv=N_SPLITS\n    ),\n    columns=[\'Full Data\']\n)\n\n# Add a single missing value to each row\nX_missing = X_full.copy()\ny_missing = y_full\nmissing_samples = np.arange(n_samples)\nmissing_features = rng.choice(n_features, n_samples, replace=True)\nX_missing[missing_samples, missing_features] = np.nan\n\n# Estimate the score after imputation (mean and median strategies)\nscore_simple_imputer = pd.DataFrame()\nfor strategy in (\'mean\', \'median\'):\n    estimator = make_pipeline(\n        SimpleImputer(missing_values=np.nan, strategy=strategy),\n        br_estimator\n    )\n    score_simple_imputer[strategy] = cross_val_score(\n        estimator, X_missing, y_missing, scoring=\'neg_mean_squared_error\',\n        cv=N_SPLITS\n    )\n\n# Estimate the score after iterative imputation of the missing values\n# with different estimators\nestimators = [\n    BayesianRidge(),\n    DecisionTreeRegressor(max_features=\'sqrt\', random_state=0),\n    ExtraTreesRegressor(n_estimators=10, random_state=0),\n    KNeighborsRegressor(n_neighbors=15)\n]\nscore_iterative_imputer = pd.DataFrame()\nfor impute_estimator in estimators:\n    estimator = make_pipeline(\n        IterativeImputer(random_state=0, estimator=impute_estimator),\n        br_estimator\n    )\n    score_iterative_imputer[impute_estimator.__class__.__name__] = \\\n        cross_val_score(\n            estimator, X_missing, y_missing, scoring=\'neg_mean_squared_error\',\n            cv=N_SPLITS\n        )\n\nscores = pd.concat(\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\n    keys=[\'Original\', \'SimpleImputer\', \'IterativeImputer\'], axis=1\n)\n\n# plot boston results\nfig, ax = plt.subplots(figsize=(13, 6))\nmeans = -scores.mean()\nerrors = scores.std()\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_title(\'California Housing Regression with Different Imputation Methods\')\nax.set_xlabel(\'MSE (smaller is better)\')\nax.set_yticks(np.arange(means.shape[0]))\nax.set_yticklabels(["" w/ "".join(label) for label in means.index.get_values()])\nplt.tight_layout(pad=1)\nplt.show()\n'"
scikit-learn/_downloads/ddd5a8871cf29f80256aafedc27fb45a/plot_grid_search_digits.py,0,"b'""""""\n============================================================\nParameter estimation using grid search with cross-validation\n============================================================\n\nThis examples shows how a classifier is optimized by cross-validation,\nwhich is done using the :class:`sklearn.model_selection.GridSearchCV` object\non a development set that comprises only half of the available labeled data.\n\nThe performance of the selected hyper-parameters and trained model is\nthen measured on a dedicated evaluation set that was not used during\nthe model selection step.\n\nMore details on tools available for model selection can be found in the\nsections on :ref:`cross_validation` and :ref:`grid_search`.\n\n""""""\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\n\nprint(__doc__)\n\n# Loading the Digits dataset\ndigits = datasets.load_digits()\n\n# To apply an classifier on this data, we need to flatten the image, to\n# turn the data in a (samples, feature) matrix:\nn_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target\n\n# Split the dataset in two equal parts\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.5, random_state=0)\n\n# Set the parameters by cross-validation\ntuned_parameters = [{\'kernel\': [\'rbf\'], \'gamma\': [1e-3, 1e-4],\n                     \'C\': [1, 10, 100, 1000]},\n                    {\'kernel\': [\'linear\'], \'C\': [1, 10, 100, 1000]}]\n\nscores = [\'precision\', \'recall\']\n\nfor score in scores:\n    print(""# Tuning hyper-parameters for %s"" % score)\n    print()\n\n    clf = GridSearchCV(\n        SVC(), tuned_parameters, scoring=\'%s_macro\' % score\n    )\n    clf.fit(X_train, y_train)\n\n    print(""Best parameters set found on development set:"")\n    print()\n    print(clf.best_params_)\n    print()\n    print(""Grid scores on development set:"")\n    print()\n    means = clf.cv_results_[\'mean_test_score\']\n    stds = clf.cv_results_[\'std_test_score\']\n    for mean, std, params in zip(means, stds, clf.cv_results_[\'params\']):\n        print(""%0.3f (+/-%0.03f) for %r""\n              % (mean, std * 2, params))\n    print()\n\n    print(""Detailed classification report:"")\n    print()\n    print(""The model is trained on the full development set."")\n    print(""The scores are computed on the full evaluation set."")\n    print()\n    y_true, y_pred = y_test, clf.predict(X_test)\n    print(classification_report(y_true, y_pred))\n    print()\n\n# Note the problem is too easy: the hyperparameter plateau is too flat and the\n# output model is the same for precision and recall with ties in quality.\n'"
scikit-learn/_downloads/df966ddf24d7570fbb454a82f5196477/plot_lw_vs_oas.py,9,"b'""""""\n=============================\nLedoit-Wolf vs OAS estimation\n=============================\n\nThe usual covariance maximum likelihood estimate can be regularized\nusing shrinkage. Ledoit and Wolf proposed a close formula to compute\nthe asymptotically optimal shrinkage parameter (minimizing a MSE\ncriterion), yielding the Ledoit-Wolf covariance estimate.\n\nChen et al. proposed an improvement of the Ledoit-Wolf shrinkage\nparameter, the OAS coefficient, whose convergence is significantly\nbetter under the assumption that the data are Gaussian.\n\nThis example, inspired from Chen\'s publication [1], shows a comparison\nof the estimated MSE of the LW and OAS methods, using Gaussian\ndistributed data.\n\n[1] ""Shrinkage Algorithms for MMSE Covariance Estimation""\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import toeplitz, cholesky\n\nfrom sklearn.covariance import LedoitWolf, OAS\n\nnp.random.seed(0)\n###############################################################################\nn_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = toeplitz(r ** np.arange(n_features))\ncoloring_matrix = cholesky(real_cov)\n\nn_samples_range = np.arange(6, 31, 1)\nrepeat = 100\nlw_mse = np.zeros((n_samples_range.size, repeat))\noa_mse = np.zeros((n_samples_range.size, repeat))\nlw_shrinkage = np.zeros((n_samples_range.size, repeat))\noa_shrinkage = np.zeros((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = np.dot(\n            np.random.normal(size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = LedoitWolf(store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = OAS(store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\nplt.subplot(2, 1, 1)\nplt.errorbar(n_samples_range, lw_mse.mean(1), yerr=lw_mse.std(1),\n             label=\'Ledoit-Wolf\', color=\'navy\', lw=2)\nplt.errorbar(n_samples_range, oa_mse.mean(1), yerr=oa_mse.std(1),\n             label=\'OAS\', color=\'darkorange\', lw=2)\nplt.ylabel(""Squared error"")\nplt.legend(loc=""upper right"")\nplt.title(""Comparison of covariance estimators"")\nplt.xlim(5, 31)\n\n# plot shrinkage coefficient\nplt.subplot(2, 1, 2)\nplt.errorbar(n_samples_range, lw_shrinkage.mean(1), yerr=lw_shrinkage.std(1),\n             label=\'Ledoit-Wolf\', color=\'navy\', lw=2)\nplt.errorbar(n_samples_range, oa_shrinkage.mean(1), yerr=oa_shrinkage.std(1),\n             label=\'OAS\', color=\'darkorange\', lw=2)\nplt.xlabel(""n_samples"")\nplt.ylabel(""Shrinkage"")\nplt.legend(loc=""lower right"")\nplt.ylim(plt.ylim()[0], 1. + (plt.ylim()[1] - plt.ylim()[0]) / 10.)\nplt.xlim(5, 31)\n\nplt.show()\n'"
scikit-learn/_downloads/df9a72a05df9edc15e4bf149d007c350/plot_bayesian_ridge.py,14,"b'""""""\n=========================\nBayesian Ridge Regression\n=========================\n\nComputes a Bayesian Ridge Regression on a synthetic dataset.\n\nSee :ref:`bayesian_ridge_regression` for more information on the regressor.\n\nCompared to the OLS (ordinary least squares) estimator, the coefficient\nweights are slightly shifted toward zeros, which stabilises them.\n\nAs the prior on the weights is a Gaussian prior, the histogram of the\nestimated weights is Gaussian.\n\nThe estimation of the model is done by iteratively maximizing the\nmarginal log-likelihood of the observations.\n\nWe also plot predictions and uncertainties for Bayesian Ridge Regression\nfor one dimensional regression using polynomial feature expansion.\nNote the uncertainty starts going up on the right side of the plot.\nThis is because these test samples are outside of the range of the training\nsamples.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import BayesianRidge, LinearRegression\n\n# #############################################################################\n# Generating simulated data with Gaussian weights\nnp.random.seed(0)\nn_samples, n_features = 100, 100\nX = np.random.randn(n_samples, n_features)  # Create Gaussian data\n# Create weights with a precision lambda_ of 4.\nlambda_ = 4.\nw = np.zeros(n_features)\n# Only keep 10 weights of interest\nrelevant_features = np.random.randint(0, n_features, 10)\nfor i in relevant_features:\n    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(lambda_))\n# Create noise with a precision alpha of 50.\nalpha_ = 50.\nnoise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_samples)\n# Create the target\ny = np.dot(X, w) + noise\n\n# #############################################################################\n# Fit the Bayesian Ridge Regression and an OLS for comparison\nclf = BayesianRidge(compute_score=True)\nclf.fit(X, y)\n\nols = LinearRegression()\nols.fit(X, y)\n\n# #############################################################################\n# Plot true weights, estimated weights, histogram of the weights, and\n# predictions with standard deviations\nlw = 2\nplt.figure(figsize=(6, 5))\nplt.title(""Weights of the model"")\nplt.plot(clf.coef_, color=\'lightgreen\', linewidth=lw,\n         label=""Bayesian Ridge estimate"")\nplt.plot(w, color=\'gold\', linewidth=lw, label=""Ground truth"")\nplt.plot(ols.coef_, color=\'navy\', linestyle=\'--\', label=""OLS estimate"")\nplt.xlabel(""Features"")\nplt.ylabel(""Values of the weights"")\nplt.legend(loc=""best"", prop=dict(size=12))\n\nplt.figure(figsize=(6, 5))\nplt.title(""Histogram of the weights"")\nplt.hist(clf.coef_, bins=n_features, color=\'gold\', log=True,\n         edgecolor=\'black\')\nplt.scatter(clf.coef_[relevant_features], np.full(len(relevant_features), 5.),\n            color=\'navy\', label=""Relevant features"")\nplt.ylabel(""Features"")\nplt.xlabel(""Values of the weights"")\nplt.legend(loc=""upper left"")\n\nplt.figure(figsize=(6, 5))\nplt.title(""Marginal log-likelihood"")\nplt.plot(clf.scores_, color=\'navy\', linewidth=lw)\nplt.ylabel(""Score"")\nplt.xlabel(""Iterations"")\n\n\n# Plotting some predictions for polynomial regression\ndef f(x, noise_amount):\n    y = np.sqrt(x) * np.sin(x)\n    noise = np.random.normal(0, 1, len(x))\n    return y + noise_amount * noise\n\n\ndegree = 10\nX = np.linspace(0, 10, 100)\ny = f(X, noise_amount=0.1)\nclf_poly = BayesianRidge()\nclf_poly.fit(np.vander(X, degree), y)\n\nX_plot = np.linspace(0, 11, 25)\ny_plot = f(X_plot, noise_amount=0)\ny_mean, y_std = clf_poly.predict(np.vander(X_plot, degree), return_std=True)\nplt.figure(figsize=(6, 5))\nplt.errorbar(X_plot, y_mean, y_std, color=\'navy\',\n             label=""Polynomial Bayesian Ridge Regression"", linewidth=lw)\nplt.plot(X_plot, y_plot, color=\'gold\', linewidth=lw,\n         label=""Ground Truth"")\nplt.ylabel(""Output y"")\nplt.xlabel(""Feature X"")\nplt.legend(loc=""lower left"")\nplt.show()\n'"
scikit-learn/_downloads/dfc2d84cae328d6a0b1d8cfe33a13205/plot_label_propagation_digits_active_learning.py,10,"b'""""""\n========================================\nLabel Propagation digits active learning\n========================================\n\nDemonstrates an active learning technique to learn handwritten digits\nusing label propagation.\n\nWe start by training a label propagation model with only 10 labeled points,\nthen we select the top five most uncertain points to label. Next, we train\nwith 15 labeled points (original 10 + 5 new ones). We repeat this process\nfour times to have a model trained with 30 labeled examples. Note you can\nincrease this to label more than 30 by changing `max_iterations`. Labeling\nmore than 30 can be useful to get a sense for the speed of convergence of\nthis active learning technique.\n\nA plot will appear showing the top 5 most uncertain digits for each iteration\nof training. These may or may not contain mistakes, but we will train the next\nmodel with their true labels.\n""""""\nprint(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn import datasets\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndigits = datasets.load_digits()\nrng = np.random.RandomState(0)\nindices = np.arange(len(digits.data))\nrng.shuffle(indices)\n\nX = digits.data[indices[:330]]\ny = digits.target[indices[:330]]\nimages = digits.images[indices[:330]]\n\nn_total_samples = len(y)\nn_labeled_points = 40\nmax_iterations = 5\n\nunlabeled_indices = np.arange(n_total_samples)[n_labeled_points:]\nf = plt.figure()\n\nfor i in range(max_iterations):\n    if len(unlabeled_indices) == 0:\n        print(""No unlabeled items left to label."")\n        break\n    y_train = np.copy(y)\n    y_train[unlabeled_indices] = -1\n\n    lp_model = LabelSpreading(gamma=0.25, max_iter=20)\n    lp_model.fit(X, y_train)\n\n    predicted_labels = lp_model.transduction_[unlabeled_indices]\n    true_labels = y[unlabeled_indices]\n\n    cm = confusion_matrix(true_labels, predicted_labels,\n                          labels=lp_model.classes_)\n\n    print(""Iteration %i %s"" % (i, 70 * ""_""))\n    print(""Label Spreading model: %d labeled & %d unlabeled (%d total)""\n          % (n_labeled_points, n_total_samples - n_labeled_points,\n             n_total_samples))\n\n    print(classification_report(true_labels, predicted_labels))\n\n    print(""Confusion matrix"")\n    print(cm)\n\n    # compute the entropies of transduced label distributions\n    pred_entropies = stats.distributions.entropy(\n        lp_model.label_distributions_.T)\n\n    # select up to 5 digit examples that the classifier is most uncertain about\n    uncertainty_index = np.argsort(pred_entropies)[::-1]\n    uncertainty_index = uncertainty_index[\n        np.in1d(uncertainty_index, unlabeled_indices)][:5]\n\n    # keep track of indices that we get labels for\n    delete_indices = np.array([], dtype=int)\n\n    # for more than 5 iterations, visualize the gain only on the first 5\n    if i < 5:\n        f.text(.05, (1 - (i + 1) * .183),\n               ""model %d\\n\\nfit with\\n%d labels"" %\n               ((i + 1), i * 5 + 10), size=10)\n    for index, image_index in enumerate(uncertainty_index):\n        image = images[image_index]\n\n        # for more than 5 iterations, visualize the gain only on the first 5\n        if i < 5:\n            sub = f.add_subplot(5, 5, index + 1 + (5 * i))\n            sub.imshow(image, cmap=plt.cm.gray_r, interpolation=\'none\')\n            sub.set_title(""predict: %i\\ntrue: %i"" % (\n                lp_model.transduction_[image_index], y[image_index]), size=10)\n            sub.axis(\'off\')\n\n        # labeling 5 points, remote from labeled set\n        delete_index, = np.where(unlabeled_indices == image_index)\n        delete_indices = np.concatenate((delete_indices, delete_index))\n\n    unlabeled_indices = np.delete(unlabeled_indices, delete_indices)\n    n_labeled_points += len(uncertainty_index)\n\nf.suptitle(""Active learning with Label Propagation.\\nRows show 5 most ""\n           ""uncertain labels to learn with the next model."", y=1.15)\nplt.subplots_adjust(left=0.2, bottom=0.03, right=0.9, top=0.9, wspace=0.2,\n                    hspace=0.85)\nplt.show()\n'"
scikit-learn/_downloads/e13e25cdf86b0accfcb0898adf974b73/plot_anomaly_comparison.py,8,"b'""""""\n============================================================================\nComparing anomaly detection algorithms for outlier detection on toy datasets\n============================================================================\n\nThis example shows characteristics of different anomaly detection algorithms\non 2D datasets. Datasets contain one or two modes (regions of high density)\nto illustrate the ability of algorithms to cope with multimodal data.\n\nFor each dataset, 15% of samples are generated as random uniform noise. This\nproportion is the value given to the nu parameter of the OneClassSVM and the\ncontamination parameter of the other outlier detection algorithms.\nDecision boundaries between inliers and outliers are displayed in black\nexcept for Local Outlier Factor (LOF) as it has no predict method to be applied\non new data when it is used for outlier detection.\n\nThe :class:`sklearn.svm.OneClassSVM` is known to be sensitive to outliers and\nthus does not perform very well for outlier detection. This estimator is best\nsuited for novelty detection when the training set is not contaminated by\noutliers. That said, outlier detection in high-dimension, or without any\nassumptions on the distribution of the inlying data is very challenging, and a\nOne-class SVM might give useful results in these situations depending on the\nvalue of its hyperparameters.\n\n:class:`sklearn.covariance.EllipticEnvelope` assumes the data is Gaussian and\nlearns an ellipse. It thus degrades when the data is not unimodal. Notice\nhowever that this estimator is robust to outliers.\n\n:class:`sklearn.ensemble.IsolationForest` and\n:class:`sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably well\nfor multi-modal data sets. The advantage of\n:class:`sklearn.neighbors.LocalOutlierFactor` over the other estimators is\nshown for the third data set, where the two modes have different densities.\nThis advantage is explained by the local aspect of LOF, meaning that it only\ncompares the score of abnormality of one sample with the scores of its\nneighbors.\n\nFinally, for the last data set, it is hard to say that one sample is more\nabnormal than another sample as they are uniformly distributed in a\nhypercube. Except for the :class:`sklearn.svm.OneClassSVM` which overfits a\nlittle, all estimators present decent solutions for this situation. In such a\ncase, it would be wise to look more closely at the scores of abnormality of\nthe samples as a good estimator should assign similar scores to all the\nsamples.\n\nWhile these examples give some intuition about the algorithms, this\nintuition might not apply to very high dimensional data.\n\nFinally, note that parameters of the models have been here handpicked but\nthat in practice they need to be adjusted. In the absence of labelled data,\nthe problem is completely unsupervised so model selection can be a challenge.\n""""""\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n#         Albert Thomas <albert.thomas@telecom-paristech.fr>\n# License: BSD 3 clause\n\nimport time\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm\nfrom sklearn.datasets import make_moons, make_blobs\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\nprint(__doc__)\n\nmatplotlib.rcParams[\'contour.negative_linestyle\'] = \'solid\'\n\n# Example settings\nn_samples = 300\noutliers_fraction = 0.15\nn_outliers = int(outliers_fraction * n_samples)\nn_inliers = n_samples - n_outliers\n\n# define outlier/anomaly detection methods to be compared\nanomaly_algorithms = [\n    (""Robust covariance"", EllipticEnvelope(contamination=outliers_fraction)),\n    (""One-Class SVM"", svm.OneClassSVM(nu=outliers_fraction, kernel=""rbf"",\n                                      gamma=0.1)),\n    (""Isolation Forest"", IsolationForest(contamination=outliers_fraction,\n                                         random_state=42)),\n    (""Local Outlier Factor"", LocalOutlierFactor(\n        n_neighbors=35, contamination=outliers_fraction))]\n\n# Define datasets\nblobs_params = dict(random_state=0, n_samples=n_inliers, n_features=2)\ndatasets = [\n    make_blobs(centers=[[0, 0], [0, 0]], cluster_std=0.5,\n               **blobs_params)[0],\n    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[0.5, 0.5],\n               **blobs_params)[0],\n    make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[1.5, .3],\n               **blobs_params)[0],\n    4. * (make_moons(n_samples=n_samples, noise=.05, random_state=0)[0] -\n          np.array([0.5, 0.25])),\n    14. * (np.random.RandomState(42).rand(n_samples, 2) - 0.5)]\n\n# Compare given classifiers under given settings\nxx, yy = np.meshgrid(np.linspace(-7, 7, 150),\n                     np.linspace(-7, 7, 150))\n\nplt.figure(figsize=(len(anomaly_algorithms) * 2 + 3, 12.5))\nplt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n                    hspace=.01)\n\nplot_num = 1\nrng = np.random.RandomState(42)\n\nfor i_dataset, X in enumerate(datasets):\n    # Add outliers\n    X = np.concatenate([X, rng.uniform(low=-6, high=6,\n                       size=(n_outliers, 2))], axis=0)\n\n    for name, algorithm in anomaly_algorithms:\n        t0 = time.time()\n        algorithm.fit(X)\n        t1 = time.time()\n        plt.subplot(len(datasets), len(anomaly_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name, size=18)\n\n        # fit the data and tag outliers\n        if name == ""Local Outlier Factor"":\n            y_pred = algorithm.fit_predict(X)\n        else:\n            y_pred = algorithm.fit(X).predict(X)\n\n        # plot the levels lines and the points\n        if name != ""Local Outlier Factor"":  # LOF does not implement predict\n            Z = algorithm.predict(np.c_[xx.ravel(), yy.ravel()])\n            Z = Z.reshape(xx.shape)\n            plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\'black\')\n\n        colors = np.array([\'#377eb8\', \'#ff7f00\'])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y_pred + 1) // 2])\n\n        plt.xlim(-7, 7)\n        plt.ylim(-7, 7)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(.99, .01, (\'%.2fs\' % (t1 - t0)).lstrip(\'0\'),\n                 transform=plt.gca().transAxes, size=15,\n                 horizontalalignment=\'right\')\n        plot_num += 1\n\nplt.show()\n'"
scikit-learn/_downloads/e164b68d22a8dba1e810004d87253547/plot_map_data_to_normal.py,2,"b'""""""\n=================================\nMap data to a normal distribution\n=================================\n\n.. currentmodule:: sklearn.preprocessing\n\nThis example demonstrates the use of the Box-Cox and Yeo-Johnson transforms\nthrough :class:`~PowerTransformer` to map data from various\ndistributions to a normal distribution.\n\nThe power transform is useful as a transformation in modeling problems where\nhomoscedasticity and normality are desired. Below are examples of Box-Cox and\nYeo-Johnwon applied to six different probability distributions: Lognormal,\nChi-squared, Weibull, Gaussian, Uniform, and Bimodal.\n\nNote that the transformations successfully map the data to a normal\ndistribution when applied to certain datasets, but are ineffective with others.\nThis highlights the importance of visualizing the data before and after\ntransformation.\n\nAlso note that even though Box-Cox seems to perform better than Yeo-Johnson for\nlognormal and chi-squared distributions, keep in mind that Box-Cox does not\nsupport inputs with negative values.\n\nFor comparison, we also add the output from\n:class:`~QuantileTransformer`. It can force any arbitrary\ndistribution into a gaussian, provided that there are enough training samples\n(thousands). Because it is a non-parametric method, it is harder to interpret\nthan the parametric ones (Box-Cox and Yeo-Johnson).\n\nOn ""small"" datasets (less than a few hundred points), the quantile transformer\nis prone to overfitting. The use of the power transform is then recommended.\n""""""\n\n# Author: Eric Chang <ericchang2017@u.northwestern.edu>\n#         Nicolas Hug <contact@nicolas-hug.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import train_test_split\n\nprint(__doc__)\n\n\nN_SAMPLES = 1000\nFONT_SIZE = 6\nBINS = 30\n\n\nrng = np.random.RandomState(304)\nbc = PowerTransformer(method=\'box-cox\')\nyj = PowerTransformer(method=\'yeo-johnson\')\n# n_quantiles is set to the training set size rather than the default value\n# to avoid a warning being raised by this example\nqt = QuantileTransformer(n_quantiles=500, output_distribution=\'normal\',\n                         random_state=rng)\nsize = (N_SAMPLES, 1)\n\n\n# lognormal distribution\nX_lognormal = rng.lognormal(size=size)\n\n# chi-squared distribution\ndf = 3\nX_chisq = rng.chisquare(df=df, size=size)\n\n# weibull distribution\na = 50\nX_weibull = rng.weibull(a=a, size=size)\n\n# gaussian distribution\nloc = 100\nX_gaussian = rng.normal(loc=loc, size=size)\n\n# uniform distribution\nX_uniform = rng.uniform(low=0, high=1, size=size)\n\n# bimodal distribution\nloc_a, loc_b = 100, 105\nX_a, X_b = rng.normal(loc=loc_a, size=size), rng.normal(loc=loc_b, size=size)\nX_bimodal = np.concatenate([X_a, X_b], axis=0)\n\n\n# create plots\ndistributions = [\n    (\'Lognormal\', X_lognormal),\n    (\'Chi-squared\', X_chisq),\n    (\'Weibull\', X_weibull),\n    (\'Gaussian\', X_gaussian),\n    (\'Uniform\', X_uniform),\n    (\'Bimodal\', X_bimodal)\n]\n\ncolors = [\'#D81B60\', \'#0188FF\', \'#FFC107\',\n          \'#B7A2FF\', \'#000000\', \'#2EC5AC\']\n\nfig, axes = plt.subplots(nrows=8, ncols=3, figsize=plt.figaspect(2))\naxes = axes.flatten()\naxes_idxs = [(0, 3, 6, 9), (1, 4, 7, 10), (2, 5, 8, 11), (12, 15, 18, 21),\n             (13, 16, 19, 22), (14, 17, 20, 23)]\naxes_list = [(axes[i], axes[j], axes[k], axes[l])\n             for (i, j, k, l) in axes_idxs]\n\n\nfor distribution, color, axes in zip(distributions, colors, axes_list):\n    name, X = distribution\n    X_train, X_test = train_test_split(X, test_size=.5)\n\n    # perform power transforms and quantile transform\n    X_trans_bc = bc.fit(X_train).transform(X_test)\n    lmbda_bc = round(bc.lambdas_[0], 2)\n    X_trans_yj = yj.fit(X_train).transform(X_test)\n    lmbda_yj = round(yj.lambdas_[0], 2)\n    X_trans_qt = qt.fit(X_train).transform(X_test)\n\n    ax_original, ax_bc, ax_yj, ax_qt = axes\n\n    ax_original.hist(X_train, color=color, bins=BINS)\n    ax_original.set_title(name, fontsize=FONT_SIZE)\n    ax_original.tick_params(axis=\'both\', which=\'major\', labelsize=FONT_SIZE)\n\n    for ax, X_trans, meth_name, lmbda in zip(\n            (ax_bc, ax_yj, ax_qt),\n            (X_trans_bc, X_trans_yj, X_trans_qt),\n            (\'Box-Cox\', \'Yeo-Johnson\', \'Quantile transform\'),\n            (lmbda_bc, lmbda_yj, None)):\n        ax.hist(X_trans, color=color, bins=BINS)\n        title = \'After {}\'.format(meth_name)\n        if lmbda is not None:\n            title += r\'\\n$\\lambda$ = {}\'.format(lmbda)\n        ax.set_title(title, fontsize=FONT_SIZE)\n        ax.tick_params(axis=\'both\', which=\'major\', labelsize=FONT_SIZE)\n        ax.set_xlim([-3.5, 3.5])\n\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/e18fb3106f0738f0955f2afc290201c7/plot_sgd_penalties.py,3,"b'""""""\n==============\nSGD: Penalties\n==============\n\nContours of where the penalty is equal to 1\nfor the three penalties L1, L2 and elastic-net.\n\nAll of the above are supported by :class:`~sklearn.linear_model.SGDClassifier`\nand :class:`~sklearn.linear_model.SGDRegressor`.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nl1_color = ""navy""\nl2_color = ""c""\nelastic_net_color = ""darkorange""\n\nline = np.linspace(-1.5, 1.5, 1001)\nxx, yy = np.meshgrid(line, line)\n\nl2 = xx ** 2 + yy ** 2\nl1 = np.abs(xx) + np.abs(yy)\nrho = 0.5\nelastic_net = rho * l1 + (1 - rho) * l2\n\nplt.figure(figsize=(10, 10), dpi=100)\nax = plt.gca()\n\nelastic_net_contour = plt.contour(xx, yy, elastic_net, levels=[1],\n                                  colors=elastic_net_color)\nl2_contour = plt.contour(xx, yy, l2, levels=[1], colors=l2_color)\nl1_contour = plt.contour(xx, yy, l1, levels=[1], colors=l1_color)\nax.set_aspect(""equal"")\nax.spines[\'left\'].set_position(\'center\')\nax.spines[\'right\'].set_color(\'none\')\nax.spines[\'bottom\'].set_position(\'center\')\nax.spines[\'top\'].set_color(\'none\')\n\nplt.clabel(elastic_net_contour, inline=1, fontsize=18,\n           fmt={1.0: \'elastic-net\'}, manual=[(-1, -1)])\nplt.clabel(l2_contour, inline=1, fontsize=18,\n           fmt={1.0: \'L2\'}, manual=[(-1, -1)])\nplt.clabel(l1_contour, inline=1, fontsize=18,\n           fmt={1.0: \'L1\'}, manual=[(-1, -1)])\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/e3d8e16e52814bd761ffd18fcc4ce14f/plot_document_clustering.py,1,"b'""""""\n=======================================\nClustering text documents using k-means\n=======================================\n\nThis is an example showing how the scikit-learn can be used to cluster\ndocuments by topics using a bag-of-words approach. This example uses\na scipy.sparse matrix to store the features instead of standard numpy arrays.\n\nTwo feature extraction methods can be used in this example:\n\n  - TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most\n    frequent words to features indices and hence compute a word occurrence\n    frequency (sparse) matrix. The word frequencies are then reweighted using\n    the Inverse Document Frequency (IDF) vector collected feature-wise over\n    the corpus.\n\n  - HashingVectorizer hashes word occurrences to a fixed dimensional space,\n    possibly with collisions. The word count vectors are then normalized to\n    each have l2-norm equal to one (projected to the euclidean unit-ball) which\n    seems to be important for k-means to work in high dimensional space.\n\n    HashingVectorizer does not provide IDF weighting as this is a stateless\n    model (the fit method does nothing). When IDF weighting is needed it can\n    be added by pipelining its output to a TfidfTransformer instance.\n\nTwo algorithms are demoed: ordinary k-means and its more scalable cousin\nminibatch k-means.\n\nAdditionally, latent semantic analysis can also be used to reduce\ndimensionality and discover latent patterns in the data.\n\nIt can be noted that k-means (and minibatch k-means) are very sensitive to\nfeature scaling and that in this case the IDF weighting helps improve the\nquality of the clustering by quite a lot as measured against the ""ground truth""\nprovided by the class label assignments of the 20 newsgroups dataset.\n\nThis improvement is not visible in the Silhouette Coefficient which is small\nfor both as this measure seem to suffer from the phenomenon called\n""Concentration of Measure"" or ""Curse of Dimensionality"" for high dimensional\ndatasets such as text data. Other measures such as V-measure and Adjusted Rand\nIndex are information theoretic based evaluation scores: as they are only based\non cluster assignments rather than distances, hence not affected by the curse\nof dimensionality.\n\nNote: as k-means is optimizing a non-convex objective function, it will likely\nend up in a local optimum. Several runs with independent random init might be\nnecessary to get a good convergence.\n\n""""""\n\n# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n#         Lars Buitinck\n# License: BSD 3 clause\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import metrics\n\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\n\nimport logging\nfrom optparse import OptionParser\nimport sys\nfrom time import time\n\nimport numpy as np\n\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO,\n                    format=\'%(asctime)s %(levelname)s %(message)s\')\n\n# parse commandline arguments\nop = OptionParser()\nop.add_option(""--lsa"",\n              dest=""n_components"", type=""int"",\n              help=""Preprocess documents with latent semantic analysis."")\nop.add_option(""--no-minibatch"",\n              action=""store_false"", dest=""minibatch"", default=True,\n              help=""Use ordinary k-means algorithm (in batch mode)."")\nop.add_option(""--no-idf"",\n              action=""store_false"", dest=""use_idf"", default=True,\n              help=""Disable Inverse Document Frequency feature weighting."")\nop.add_option(""--use-hashing"",\n              action=""store_true"", default=False,\n              help=""Use a hashing feature vectorizer"")\nop.add_option(""--n-features"", type=int, default=10000,\n              help=""Maximum number of features (dimensions)""\n                   "" to extract from text."")\nop.add_option(""--verbose"",\n              action=""store_true"", dest=""verbose"", default=False,\n              help=""Print progress reports inside k-means algorithm."")\n\nprint(__doc__)\nop.print_help()\n\n\ndef is_interactive():\n    return not hasattr(sys.modules[\'__main__\'], \'__file__\')\n\n\n# work-around for Jupyter notebook and IPython console\nargv = [] if is_interactive() else sys.argv[1:]\n(opts, args) = op.parse_args(argv)\nif len(args) > 0:\n    op.error(""this script takes no arguments."")\n    sys.exit(1)\n\n\n# #############################################################################\n# Load some categories from the training set\ncategories = [\n    \'alt.atheism\',\n    \'talk.religion.misc\',\n    \'comp.graphics\',\n    \'sci.space\',\n]\n# Uncomment the following to do the analysis on all the categories\n# categories = None\n\nprint(""Loading 20 newsgroups dataset for categories:"")\nprint(categories)\n\ndataset = fetch_20newsgroups(subset=\'all\', categories=categories,\n                             shuffle=True, random_state=42)\n\nprint(""%d documents"" % len(dataset.data))\nprint(""%d categories"" % len(dataset.target_names))\nprint()\n\nlabels = dataset.target\ntrue_k = np.unique(labels).shape[0]\n\nprint(""Extracting features from the training dataset ""\n      ""using a sparse vectorizer"")\nt0 = time()\nif opts.use_hashing:\n    if opts.use_idf:\n        # Perform an IDF normalization on the output of HashingVectorizer\n        hasher = HashingVectorizer(n_features=opts.n_features,\n                                   stop_words=\'english\', alternate_sign=False,\n                                   norm=None)\n        vectorizer = make_pipeline(hasher, TfidfTransformer())\n    else:\n        vectorizer = HashingVectorizer(n_features=opts.n_features,\n                                       stop_words=\'english\',\n                                       alternate_sign=False, norm=\'l2\')\nelse:\n    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n                                 min_df=2, stop_words=\'english\',\n                                 use_idf=opts.use_idf)\nX = vectorizer.fit_transform(dataset.data)\n\nprint(""done in %fs"" % (time() - t0))\nprint(""n_samples: %d, n_features: %d"" % X.shape)\nprint()\n\nif opts.n_components:\n    print(""Performing dimensionality reduction using LSA"")\n    t0 = time()\n    # Vectorizer results are normalized, which makes KMeans behave as\n    # spherical k-means for better results. Since LSA/SVD results are\n    # not normalized, we have to redo the normalization.\n    svd = TruncatedSVD(opts.n_components)\n    normalizer = Normalizer(copy=False)\n    lsa = make_pipeline(svd, normalizer)\n\n    X = lsa.fit_transform(X)\n\n    print(""done in %fs"" % (time() - t0))\n\n    explained_variance = svd.explained_variance_ratio_.sum()\n    print(""Explained variance of the SVD step: {}%"".format(\n        int(explained_variance * 100)))\n\n    print()\n\n\n# #############################################################################\n# Do the actual clustering\n\nif opts.minibatch:\n    km = MiniBatchKMeans(n_clusters=true_k, init=\'k-means++\', n_init=1,\n                         init_size=1000, batch_size=1000, verbose=opts.verbose)\nelse:\n    km = KMeans(n_clusters=true_k, init=\'k-means++\', max_iter=100, n_init=1,\n                verbose=opts.verbose)\n\nprint(""Clustering sparse data with %s"" % km)\nt0 = time()\nkm.fit(X)\nprint(""done in %0.3fs"" % (time() - t0))\nprint()\n\nprint(""Homogeneity: %0.3f"" % metrics.homogeneity_score(labels, km.labels_))\nprint(""Completeness: %0.3f"" % metrics.completeness_score(labels, km.labels_))\nprint(""V-measure: %0.3f"" % metrics.v_measure_score(labels, km.labels_))\nprint(""Adjusted Rand-Index: %.3f""\n      % metrics.adjusted_rand_score(labels, km.labels_))\nprint(""Silhouette Coefficient: %0.3f""\n      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n\nprint()\n\n\nif not opts.use_hashing:\n    print(""Top terms per cluster:"")\n\n    if opts.n_components:\n        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n        order_centroids = original_space_centroids.argsort()[:, ::-1]\n    else:\n        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n\n    terms = vectorizer.get_feature_names()\n    for i in range(true_k):\n        print(""Cluster %d:"" % i, end=\'\')\n        for ind in order_centroids[i, :10]:\n            print(\' %s\' % terms[ind], end=\'\')\n        print()\n'"
scikit-learn/_downloads/e459665fc4d02a2d3123a1b2173cd099/plot_pca_vs_fa_model_selection.py,10,"b'""""""\n===============================================================\nModel selection with Probabilistic PCA and Factor Analysis (FA)\n===============================================================\n\nProbabilistic PCA and Factor Analysis are probabilistic models.\nThe consequence is that the likelihood of new data can be used\nfor model selection and covariance estimation.\nHere we compare PCA and FA with cross-validation on low rank data corrupted\nwith homoscedastic noise (noise variance\nis the same for each feature) or heteroscedastic noise (noise variance\nis the different for each feature). In a second step we compare the model\nlikelihood to the likelihoods obtained from shrinkage covariance estimators.\n\nOne can observe that with homoscedastic noise both FA and PCA succeed\nin recovering the size of the low rank subspace. The likelihood with PCA\nis higher than FA in this case. However PCA fails and overestimates\nthe rank when heteroscedastic noise is present. Under appropriate\ncircumstances the low rank models are more likely than shrinkage models.\n\nThe automatic estimation from\nAutomatic Choice of Dimensionality for PCA. NIPS 2000: 598-604\nby Thomas P. Minka is also compared.\n\n""""""\n\n# Authors: Alexandre Gramfort\n#          Denis A. Engemann\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\n\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.covariance import ShrunkCovariance, LedoitWolf\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nprint(__doc__)\n\n# #############################################################################\n# Create the data\n\nn_samples, n_features, rank = 1000, 50, 10\nsigma = 1.\nrng = np.random.RandomState(42)\nU, _, _ = linalg.svd(rng.randn(n_features, n_features))\nX = np.dot(rng.randn(n_samples, rank), U[:, :rank].T)\n\n# Adding homoscedastic noise\nX_homo = X + sigma * rng.randn(n_samples, n_features)\n\n# Adding heteroscedastic noise\nsigmas = sigma * rng.rand(n_features) + sigma / 2.\nX_hetero = X + rng.randn(n_samples, n_features) * sigmas\n\n# #############################################################################\n# Fit the models\n\nn_components = np.arange(0, n_features, 5)  # options for n_components\n\n\ndef compute_scores(X):\n    pca = PCA(svd_solver=\'full\')\n    fa = FactorAnalysis()\n\n    pca_scores, fa_scores = [], []\n    for n in n_components:\n        pca.n_components = n\n        fa.n_components = n\n        pca_scores.append(np.mean(cross_val_score(pca, X)))\n        fa_scores.append(np.mean(cross_val_score(fa, X)))\n\n    return pca_scores, fa_scores\n\n\ndef shrunk_cov_score(X):\n    shrinkages = np.logspace(-2, 0, 30)\n    cv = GridSearchCV(ShrunkCovariance(), {\'shrinkage\': shrinkages})\n    return np.mean(cross_val_score(cv.fit(X).best_estimator_, X))\n\n\ndef lw_score(X):\n    return np.mean(cross_val_score(LedoitWolf(), X))\n\n\nfor X, title in [(X_homo, \'Homoscedastic Noise\'),\n                 (X_hetero, \'Heteroscedastic Noise\')]:\n    pca_scores, fa_scores = compute_scores(X)\n    n_components_pca = n_components[np.argmax(pca_scores)]\n    n_components_fa = n_components[np.argmax(fa_scores)]\n\n    pca = PCA(svd_solver=\'full\', n_components=\'mle\')\n    pca.fit(X)\n    n_components_pca_mle = pca.n_components_\n\n    print(""best n_components by PCA CV = %d"" % n_components_pca)\n    print(""best n_components by FactorAnalysis CV = %d"" % n_components_fa)\n    print(""best n_components by PCA MLE = %d"" % n_components_pca_mle)\n\n    plt.figure()\n    plt.plot(n_components, pca_scores, \'b\', label=\'PCA scores\')\n    plt.plot(n_components, fa_scores, \'r\', label=\'FA scores\')\n    plt.axvline(rank, color=\'g\', label=\'TRUTH: %d\' % rank, linestyle=\'-\')\n    plt.axvline(n_components_pca, color=\'b\',\n                label=\'PCA CV: %d\' % n_components_pca, linestyle=\'--\')\n    plt.axvline(n_components_fa, color=\'r\',\n                label=\'FactorAnalysis CV: %d\' % n_components_fa,\n                linestyle=\'--\')\n    plt.axvline(n_components_pca_mle, color=\'k\',\n                label=\'PCA MLE: %d\' % n_components_pca_mle, linestyle=\'--\')\n\n    # compare with other covariance estimators\n    plt.axhline(shrunk_cov_score(X), color=\'violet\',\n                label=\'Shrunk Covariance MLE\', linestyle=\'-.\')\n    plt.axhline(lw_score(X), color=\'orange\',\n                label=\'LedoitWolf MLE\' % n_components_pca_mle, linestyle=\'-.\')\n\n    plt.xlabel(\'nb of components\')\n    plt.ylabel(\'CV scores\')\n    plt.legend(loc=\'lower right\')\n    plt.title(title)\n\nplt.show()\n'"
scikit-learn/_downloads/e510f3980ea5a0a7009d94b09649abcf/plot_bayesian_ridge_curvefit.py,6,"b'""""""\n============================================\nCurve Fitting with Bayesian Ridge Regression\n============================================\n\nComputes a Bayesian Ridge Regression of Sinusoids.\n\nSee :ref:`bayesian_ridge_regression` for more information on the regressor.\n\nIn general, when fitting a curve with a polynomial by Bayesian ridge\nregression, the selection of initial values of\nthe regularization parameters (alpha, lambda) may be important.\nThis is because the regularization parameters are determined by an iterative\nprocedure that depends on initial values.\n\nIn this example, the sinusoid is approximated by a polynomial using different\npairs of initial values.\n\nWhen starting from the default values (alpha_init = 1.90, lambda_init = 1.),\nthe bias of the resulting curve is large, and the variance is small.\nSo, lambda_init should be relatively small (1.e-3) so as to reduce the bias.\n\nAlso, by evaluating log marginal likelihood (L) of\nthese models, we can determine which one is better.\nIt can be concluded that the model with larger L is more likely.\n""""""\nprint(__doc__)\n\n# Author: Yoshihiro Uchida <nimbus1after2a1sun7shower@gmail.com>\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import BayesianRidge\n\n\ndef func(x): return np.sin(2*np.pi*x)\n\n\n# #############################################################################\n# Generate sinusoidal data with noise\nsize = 25\nrng = np.random.RandomState(1234)\nx_train = rng.uniform(0., 1., size)\ny_train = func(x_train) + rng.normal(scale=0.1, size=size)\nx_test = np.linspace(0., 1., 100)\n\n\n# #############################################################################\n# Fit by cubic polynomial\nn_order = 3\nX_train = np.vander(x_train, n_order + 1, increasing=True)\nX_test = np.vander(x_test, n_order + 1, increasing=True)\n\n# #############################################################################\n# Plot the true and predicted curves with log marginal likelihood (L)\nreg = BayesianRidge(tol=1e-6, fit_intercept=False, compute_score=True)\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\nfor i, ax in enumerate(axes):\n    # Bayesian ridge regression with different initial value pairs\n    if i == 0:\n        init = [1 / np.var(y_train), 1.]  # Default values\n    elif i == 1:\n        init = [1., 1e-3]\n        reg.set_params(alpha_init=init[0], lambda_init=init[1])\n    reg.fit(X_train, y_train)\n    ymean, ystd = reg.predict(X_test, return_std=True)\n\n    ax.plot(x_test, func(x_test), color=""blue"", label=""sin($2\\\\pi x$)"")\n    ax.scatter(x_train, y_train, s=50, alpha=0.5, label=""observation"")\n    ax.plot(x_test, ymean, color=""red"", label=""predict mean"")\n    ax.fill_between(x_test, ymean-ystd, ymean+ystd,\n                    color=""pink"", alpha=0.5, label=""predict std"")\n    ax.set_ylim(-1.3, 1.3)\n    ax.legend()\n    title = ""$\\\\alpha$_init$={:.2f},\\\\ \\\\lambda$_init$={}$"".format(\n            init[0], init[1])\n    if i == 0:\n        title += "" (Default)""\n    ax.set_title(title, fontsize=12)\n    text = ""$\\\\alpha={:.1f}$\\n$\\\\lambda={:.3f}$\\n$L={:.1f}$"".format(\n           reg.alpha_, reg.lambda_, reg.scores_[-1])\n    ax.text(0.05, -1.0, text, fontsize=12)\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/e63cb15633db9fafb915645ee15886a1/plot_label_propagation_structure.py,4,"b'""""""\n==============================================\nLabel Propagation learning a complex structure\n==============================================\n\nExample of LabelPropagation learning a complex internal structure\nto demonstrate ""manifold learning"". The outer circle should be\nlabeled ""red"" and the inner circle ""blue"". Because both label groups\nlie inside their own distinct shape, we can see that the labels\npropagate correctly around the circle.\n""""""\nprint(__doc__)\n\n# Authors: Clay Woolam <clay@woolam.org>\n#          Andreas Mueller <amueller@ais.uni-bonn.de>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.datasets import make_circles\n\n# generate ring with inner box\nn_samples = 200\nX, y = make_circles(n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = np.full(n_samples, -1.)\nlabels[0] = outer\nlabels[-1] = inner\n\n# #############################################################################\n# Learn with LabelSpreading\nlabel_spread = LabelSpreading(kernel=\'knn\', alpha=0.8)\nlabel_spread.fit(X, labels)\n\n# #############################################################################\n# Plot output labels\noutput_labels = label_spread.transduction_\nplt.figure(figsize=(8.5, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(X[labels == outer, 0], X[labels == outer, 1], color=\'navy\',\n            marker=\'s\', lw=0, label=""outer labeled"", s=10)\nplt.scatter(X[labels == inner, 0], X[labels == inner, 1], color=\'c\',\n            marker=\'s\', lw=0, label=\'inner labeled\', s=10)\nplt.scatter(X[labels == -1, 0], X[labels == -1, 1], color=\'darkorange\',\n            marker=\'.\', label=\'unlabeled\')\nplt.legend(scatterpoints=1, shadow=False, loc=\'upper right\')\nplt.title(""Raw data (2 classes=outer and inner)"")\n\nplt.subplot(1, 2, 2)\noutput_label_array = np.asarray(output_labels)\nouter_numbers = np.where(output_label_array == outer)[0]\ninner_numbers = np.where(output_label_array == inner)[0]\nplt.scatter(X[outer_numbers, 0], X[outer_numbers, 1], color=\'navy\',\n            marker=\'s\', lw=0, s=10, label=""outer learned"")\nplt.scatter(X[inner_numbers, 0], X[inner_numbers, 1], color=\'c\',\n            marker=\'s\', lw=0, s=10, label=""inner learned"")\nplt.legend(scatterpoints=1, shadow=False, loc=\'upper right\')\nplt.title(""Labels learned with Label Spreading (KNN)"")\n\nplt.subplots_adjust(left=0.07, bottom=0.07, right=0.93, top=0.92)\nplt.show()\n'"
scikit-learn/_downloads/e69a5f46213caef01193a1f8bb3d435d/plot_random_multilabel_dataset.py,2,"b'""""""\n==============================================\nPlot randomly generated multilabel dataset\n==============================================\n\nThis illustrates the :func:`~sklearn.datasets.make_multilabel_classification`\ndataset generator. Each sample consists of counts of two features (up to 50 in\ntotal), which are differently distributed in each of two classes.\n\nPoints are labeled as follows, where Y means the class is present:\n\n    =====  =====  =====  ======\n      1      2      3    Color\n    =====  =====  =====  ======\n      Y      N      N    Red\n      N      Y      N    Blue\n      N      N      Y    Yellow\n      Y      Y      N    Purple\n      Y      N      Y    Orange\n      Y      Y      N    Green\n      Y      Y      Y    Brown\n    =====  =====  =====  ======\n\nA star marks the expected sample for each class; its size reflects the\nprobability of selecting that class label.\n\nThe left and right examples highlight the ``n_labels`` parameter:\nmore of the samples in the right plot have 2 or 3 labels.\n\nNote that this two-dimensional example is very degenerate:\ngenerally the number of features would be much greater than the\n""document length"", while here we have much larger documents than vocabulary.\nSimilarly, with ``n_classes > n_features``, it is much less likely that a\nfeature distinguishes a particular class.\n""""""\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_multilabel_classification as make_ml_clf\n\nprint(__doc__)\n\nCOLORS = np.array([\'!\',\n                   \'#FF3333\',  # red\n                   \'#0198E1\',  # blue\n                   \'#BF5FFF\',  # purple\n                   \'#FCD116\',  # yellow\n                   \'#FF7216\',  # orange\n                   \'#4DBD33\',  # green\n                   \'#87421F\'   # brown\n                   ])\n\n# Use same random seed for multiple calls to make_multilabel_classification to\n# ensure same distributions\nRANDOM_SEED = np.random.randint(2 ** 10)\n\n\ndef plot_2d(ax, n_labels=1, n_classes=3, length=50):\n    X, Y, p_c, p_w_c = make_ml_clf(n_samples=150, n_features=2,\n                                   n_classes=n_classes, n_labels=n_labels,\n                                   length=length, allow_unlabeled=False,\n                                   return_distributions=True,\n                                   random_state=RANDOM_SEED)\n\n    ax.scatter(X[:, 0], X[:, 1], color=COLORS.take((Y * [1, 2, 4]\n                                                    ).sum(axis=1)),\n               marker=\'.\')\n    ax.scatter(p_w_c[0] * length, p_w_c[1] * length,\n               marker=\'*\', linewidth=.5, edgecolor=\'black\',\n               s=20 + 1500 * p_c ** 2,\n               color=COLORS.take([1, 2, 4]))\n    ax.set_xlabel(\'Feature 0 count\')\n    return p_c, p_w_c\n\n\n_, (ax1, ax2) = plt.subplots(1, 2, sharex=\'row\', sharey=\'row\', figsize=(8, 4))\nplt.subplots_adjust(bottom=.15)\n\np_c, p_w_c = plot_2d(ax1, n_labels=1)\nax1.set_title(\'n_labels=1, length=50\')\nax1.set_ylabel(\'Feature 1 count\')\n\nplot_2d(ax2, n_labels=3)\nax2.set_title(\'n_labels=3, length=50\')\nax2.set_xlim(left=0, auto=True)\nax2.set_ylim(bottom=0, auto=True)\n\nplt.show()\n\nprint(\'The data was generated from (random_state=%d):\' % RANDOM_SEED)\nprint(\'Class\', \'P(C)\', \'P(w0|C)\', \'P(w1|C)\', sep=\'\\t\')\nfor k, p, p_w in zip([\'red\', \'blue\', \'yellow\'], p_c, p_w_c.T):\n    print(\'%s\\t%0.2f\\t%0.2f\\t%0.2f\' % (k, p, p_w[0], p_w[1]))\n'"
scikit-learn/_downloads/e6c8e543b40d462bbdd7e638de9e2a77/plot_face_compress.py,6,"b'#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n""""""\n=========================================================\nVector Quantization Example\n=========================================================\n\nFace, a 1024 x 768 size image of a raccoon face,\nis used here to illustrate how `k`-means is\nused for vector quantization.\n\n""""""\nprint(__doc__)\n\n\n# Code source: Ga\xc3\xabl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster\n\n\ntry:  # SciPy >= 0.16 have face in misc\n    from scipy.misc import face\n    face = face(gray=True)\nexcept ImportError:\n    face = sp.face(gray=True)\n\nn_clusters = 5\nnp.random.seed(0)\n\nX = face.reshape((-1, 1))  # We need an (n_sample, n_feature) array\nk_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)\nk_means.fit(X)\nvalues = k_means.cluster_centers_.squeeze()\nlabels = k_means.labels_\n\n# create an array from labels and values\nface_compressed = np.choose(labels, values)\nface_compressed.shape = face.shape\n\nvmin = face.min()\nvmax = face.max()\n\n# original face\nplt.figure(1, figsize=(3, 2.2))\nplt.imshow(face, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n\n# compressed face\nplt.figure(2, figsize=(3, 2.2))\nplt.imshow(face_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n\n# equal bins face\nregular_values = np.linspace(0, 256, n_clusters + 1)\nregular_labels = np.searchsorted(regular_values, face) - 1\nregular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\nregular_face = np.choose(regular_labels.ravel(), regular_values, mode=""clip"")\nregular_face.shape = face.shape\nplt.figure(3, figsize=(3, 2.2))\nplt.imshow(regular_face, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n\n# histogram\nplt.figure(4, figsize=(3, 2.2))\nplt.clf()\nplt.axes([.01, .01, .98, .98])\nplt.hist(X, bins=256, color=\'.5\', edgecolor=\'.5\')\nplt.yticks(())\nplt.xticks(regular_values)\nvalues = np.sort(values)\nfor center_1, center_2 in zip(values[:-1], values[1:]):\n    plt.axvline(.5 * (center_1 + center_2), color=\'b\')\n\nfor center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n    plt.axvline(.5 * (center_1 + center_2), color=\'b\', linestyle=\'--\')\n\nplt.show()\n'"
scikit-learn/_downloads/e6d55b6d1ca82cfa835326942c15340f/plot_dict_face_patches.py,5,"b'""""""\nOnline learning of a dictionary of parts of faces\n==================================================\n\nThis example uses a large dataset of faces to learn a set of 20 x 20\nimages patches that constitute faces.\n\nFrom the programming standpoint, it is interesting because it shows how\nto use the online API of the scikit-learn to process a very large\ndataset by chunks. The way we proceed is that we load an image at a time\nand extract randomly 50 patches from this image. Once we have accumulated\n500 of these patches (using 10 images), we run the\n:func:`~sklearn.cluster.MiniBatchKMeans.partial_fit` method\nof the online KMeans object, MiniBatchKMeans.\n\nThe verbose setting on the MiniBatchKMeans enables us to see that some\nclusters are reassigned during the successive calls to\npartial-fit. This is because the number of patches that they represent\nhas become too low, and it is better to choose a random new\ncluster.\n""""""\nprint(__doc__)\n\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nfrom sklearn import datasets\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.image import extract_patches_2d\n\nfaces = datasets.fetch_olivetti_faces()\n\n# #############################################################################\n# Learn the dictionary of images\n\nprint(\'Learning the dictionary... \')\nrng = np.random.RandomState(0)\nkmeans = MiniBatchKMeans(n_clusters=81, random_state=rng, verbose=True)\npatch_size = (20, 20)\n\nbuffer = []\nt0 = time.time()\n\n# The online learning part: cycle over the whole dataset 6 times\nindex = 0\nfor _ in range(6):\n    for img in faces.images:\n        data = extract_patches_2d(img, patch_size, max_patches=50,\n                                  random_state=rng)\n        data = np.reshape(data, (len(data), -1))\n        buffer.append(data)\n        index += 1\n        if index % 10 == 0:\n            data = np.concatenate(buffer, axis=0)\n            data -= np.mean(data, axis=0)\n            data /= np.std(data, axis=0)\n            kmeans.partial_fit(data)\n            buffer = []\n        if index % 100 == 0:\n            print(\'Partial fit of %4i out of %i\'\n                  % (index, 6 * len(faces.images)))\n\ndt = time.time() - t0\nprint(\'done in %.2fs.\' % dt)\n\n# #############################################################################\n# Plot the results\nplt.figure(figsize=(4.2, 4))\nfor i, patch in enumerate(kmeans.cluster_centers_):\n    plt.subplot(9, 9, i + 1)\n    plt.imshow(patch.reshape(patch_size), cmap=plt.cm.gray,\n               interpolation=\'nearest\')\n    plt.xticks(())\n    plt.yticks(())\n\n\nplt.suptitle(\'Patches of faces\\nTrain time %.1fs on %d patches\' %\n             (dt, 8 * len(faces.images)), fontsize=16)\nplt.subplots_adjust(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\nplt.show()\n'"
scikit-learn/_downloads/e7b5f24302e590af9b48989d9ef122ef/plot_adaboost_multiclass.py,0,"b'r""""""\n=====================================\nMulti-class AdaBoosted Decision Trees\n=====================================\n\nThis example reproduces Figure 1 of Zhu et al [1]_ and shows how boosting can\nimprove prediction accuracy on a multi-class problem. The classification\ndataset is constructed by taking a ten-dimensional standard normal distribution\nand defining three classes separated by nested concentric ten-dimensional\nspheres such that roughly equal numbers of samples are in each class (quantiles\nof the :math:`\\chi^2` distribution).\n\nThe performance of the SAMME and SAMME.R [1]_ algorithms are compared. SAMME.R\nuses the probability estimates to update the additive model, while SAMME  uses\nthe classifications only. As the example illustrates, the SAMME.R algorithm\ntypically converges faster than SAMME, achieving a lower test error with fewer\nboosting iterations. The error of each algorithm on the test set after each\nboosting iteration is shown on the left, the classification error on the test\nset of each tree is shown in the middle, and the boost weight of each tree is\nshown on the right. All trees have a weight of one in the SAMME.R algorithm and\ntherefore are not shown.\n\n.. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, ""Multi-class AdaBoost"", 2009.\n\n""""""\nprint(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_gaussian_quantiles\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nX, y = make_gaussian_quantiles(n_samples=13000, n_features=10,\n                               n_classes=3, random_state=1)\n\nn_split = 3000\n\nX_train, X_test = X[:n_split], X[n_split:]\ny_train, y_test = y[:n_split], y[n_split:]\n\nbdt_real = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=600,\n    learning_rate=1)\n\nbdt_discrete = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=2),\n    n_estimators=600,\n    learning_rate=1.5,\n    algorithm=""SAMME"")\n\nbdt_real.fit(X_train, y_train)\nbdt_discrete.fit(X_train, y_train)\n\nreal_test_errors = []\ndiscrete_test_errors = []\n\nfor real_test_predict, discrete_train_predict in zip(\n        bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):\n    real_test_errors.append(\n        1. - accuracy_score(real_test_predict, y_test))\n    discrete_test_errors.append(\n        1. - accuracy_score(discrete_train_predict, y_test))\n\nn_trees_discrete = len(bdt_discrete)\nn_trees_real = len(bdt_real)\n\n# Boosting might terminate early, but the following arrays are always\n# n_estimators long. We crop them to the actual number of trees here:\ndiscrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]\nreal_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]\ndiscrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nplt.plot(range(1, n_trees_discrete + 1),\n         discrete_test_errors, c=\'black\', label=\'SAMME\')\nplt.plot(range(1, n_trees_real + 1),\n         real_test_errors, c=\'black\',\n         linestyle=\'dashed\', label=\'SAMME.R\')\nplt.legend()\nplt.ylim(0.18, 0.62)\nplt.ylabel(\'Test Error\')\nplt.xlabel(\'Number of Trees\')\n\nplt.subplot(132)\nplt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,\n         ""b"", label=\'SAMME\', alpha=.5)\nplt.plot(range(1, n_trees_real + 1), real_estimator_errors,\n         ""r"", label=\'SAMME.R\', alpha=.5)\nplt.legend()\nplt.ylabel(\'Error\')\nplt.xlabel(\'Number of Trees\')\nplt.ylim((.2,\n         max(real_estimator_errors.max(),\n             discrete_estimator_errors.max()) * 1.2))\nplt.xlim((-20, len(bdt_discrete) + 20))\n\nplt.subplot(133)\nplt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,\n         ""b"", label=\'SAMME\')\nplt.legend()\nplt.ylabel(\'Weight\')\nplt.xlabel(\'Number of Trees\')\nplt.ylim((0, discrete_estimator_weights.max() * 1.2))\nplt.xlim((-20, n_trees_discrete + 20))\n\n# prevent overlapping y-axis labels\nplt.subplots_adjust(wspace=0.25)\nplt.show()\n'"
scikit-learn/_downloads/ea18ceab147141ca0096648d19dd53d4/plot_mini_batch_kmeans.py,2,"b'""""""\n====================================================================\nComparison of the K-Means and MiniBatchKMeans clustering algorithms\n====================================================================\n\nWe want to compare the performance of the MiniBatchKMeans and KMeans:\nthe MiniBatchKMeans is faster, but gives slightly different results (see\n:ref:`mini_batch_kmeans`).\n\nWe will cluster a set of data, first with KMeans and then with\nMiniBatchKMeans, and plot the results.\nWe will also plot the points that are labelled differently between the two\nalgorithms.\n""""""\nprint(__doc__)\n\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import MiniBatchKMeans, KMeans\nfrom sklearn.metrics.pairwise import pairwise_distances_argmin\nfrom sklearn.datasets import make_blobs\n\n# #############################################################################\n# Generate sample data\nnp.random.seed(0)\n\nbatch_size = 45\ncenters = [[1, 1], [-1, -1], [1, -1]]\nn_clusters = len(centers)\nX, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\n\n# #############################################################################\n# Compute clustering with Means\n\nk_means = KMeans(init=\'k-means++\', n_clusters=3, n_init=10)\nt0 = time.time()\nk_means.fit(X)\nt_batch = time.time() - t0\n\n# #############################################################################\n# Compute clustering with MiniBatchKMeans\n\nmbk = MiniBatchKMeans(init=\'k-means++\', n_clusters=3, batch_size=batch_size,\n                      n_init=10, max_no_improvement=10, verbose=0)\nt0 = time.time()\nmbk.fit(X)\nt_mini_batch = time.time() - t0\n\n# #############################################################################\n# Plot result\n\nfig = plt.figure(figsize=(8, 3))\nfig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\ncolors = [\'#4EACC5\', \'#FF9C34\', \'#4E9A06\']\n\n# We want to have the same colors for the same cluster from the\n# MiniBatchKMeans and the KMeans algorithm. Let\'s pair the cluster centers per\n# closest one.\nk_means_cluster_centers = k_means.cluster_centers_\norder = pairwise_distances_argmin(k_means.cluster_centers_,\n                                  mbk.cluster_centers_)\nmbk_means_cluster_centers = mbk.cluster_centers_[order]\n\nk_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)\nmbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)\n\n# KMeans\nax = fig.add_subplot(1, 3, 1)\nfor k, col in zip(range(n_clusters), colors):\n    my_members = k_means_labels == k\n    cluster_center = k_means_cluster_centers[k]\n    ax.plot(X[my_members, 0], X[my_members, 1], \'w\',\n            markerfacecolor=col, marker=\'.\')\n    ax.plot(cluster_center[0], cluster_center[1], \'o\', markerfacecolor=col,\n            markeredgecolor=\'k\', markersize=6)\nax.set_title(\'KMeans\')\nax.set_xticks(())\nax.set_yticks(())\nplt.text(-3.5, 1.8,  \'train time: %.2fs\\ninertia: %f\' % (\n    t_batch, k_means.inertia_))\n\n# MiniBatchKMeans\nax = fig.add_subplot(1, 3, 2)\nfor k, col in zip(range(n_clusters), colors):\n    my_members = mbk_means_labels == k\n    cluster_center = mbk_means_cluster_centers[k]\n    ax.plot(X[my_members, 0], X[my_members, 1], \'w\',\n            markerfacecolor=col, marker=\'.\')\n    ax.plot(cluster_center[0], cluster_center[1], \'o\', markerfacecolor=col,\n            markeredgecolor=\'k\', markersize=6)\nax.set_title(\'MiniBatchKMeans\')\nax.set_xticks(())\nax.set_yticks(())\nplt.text(-3.5, 1.8, \'train time: %.2fs\\ninertia: %f\' %\n         (t_mini_batch, mbk.inertia_))\n\n# Initialise the different array to all False\ndifferent = (mbk_means_labels == 4)\nax = fig.add_subplot(1, 3, 3)\n\nfor k in range(n_clusters):\n    different += ((k_means_labels == k) != (mbk_means_labels == k))\n\nidentic = np.logical_not(different)\nax.plot(X[identic, 0], X[identic, 1], \'w\',\n        markerfacecolor=\'#bbbbbb\', marker=\'.\')\nax.plot(X[different, 0], X[different, 1], \'w\',\n        markerfacecolor=\'m\', marker=\'.\')\nax.set_title(\'Difference\')\nax.set_xticks(())\nax.set_yticks(())\n\nplt.show()\n'"
scikit-learn/_downloads/ebb4116b89dbc37b6f618b29ed03b354/plot_mnist_filters.py,0,"b'""""""\n=====================================\nVisualization of MLP weights on MNIST\n=====================================\n\nSometimes looking at the learned coefficients of a neural network can provide\ninsight into the learning behavior. For example if weights look unstructured,\nmaybe some were not used at all, or if very large coefficients exist, maybe\nregularization was too low or the learning rate too high.\n\nThis example shows how to plot some of the first layer weights in a\nMLPClassifier trained on the MNIST dataset.\n\nThe input data consists of 28x28 pixel handwritten digits, leading to 784\nfeatures in the dataset. Therefore the first layer weight matrix have the shape\n(784, hidden_layer_sizes[0]).  We can therefore visualize a single column of\nthe weight matrix as a 28x28 pixel image.\n\nTo make the example run faster, we use very few hidden units, and train only\nfor a very short time. Training longer would result in weights with a much\nsmoother spatial appearance.\n""""""\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.neural_network import MLPClassifier\n\nprint(__doc__)\n\n# Load data from https://www.openml.org/d/554\nX, y = fetch_openml(\'mnist_784\', version=1, return_X_y=True)\nX = X / 255.\n\n# rescale the data, use the traditional train/test split\nX_train, X_test = X[:60000], X[60000:]\ny_train, y_test = y[:60000], y[60000:]\n\nmlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n                    solver=\'sgd\', verbose=10, random_state=1,\n                    learning_rate_init=.1)\n\nmlp.fit(X_train, y_train)\nprint(""Training set score: %f"" % mlp.score(X_train, y_train))\nprint(""Test set score: %f"" % mlp.score(X_test, y_test))\n\nfig, axes = plt.subplots(4, 4)\n# use global min / max to ensure all weights are shown on the same scale\nvmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\nfor coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n               vmax=.5 * vmax)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nplt.show()\n'"
scikit-learn/_downloads/ebb7755891edac7bd24948cf64712e03/plot_johnson_lindenstrauss_bound.py,8,"b'r""""""\n=====================================================================\nThe Johnson-Lindenstrauss bound for embedding with random projections\n=====================================================================\n\n\nThe `Johnson-Lindenstrauss lemma`_ states that any high dimensional\ndataset can be randomly projected into a lower dimensional Euclidean\nspace while controlling the distortion in the pairwise distances.\n\n.. _`Johnson-Lindenstrauss lemma`: https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma\n""""""\n\nprint(__doc__)\n\nimport sys\nfrom time import time\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom distutils.version import LooseVersion\nfrom sklearn.random_projection import johnson_lindenstrauss_min_dim\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.datasets import fetch_20newsgroups_vectorized\nfrom sklearn.datasets import load_digits\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n# `normed` is being deprecated in favor of `density` in histograms\nif LooseVersion(matplotlib.__version__) >= \'2.1\':\n    density_param = {\'density\': True}\nelse:\n    density_param = {\'normed\': True}\n\n##########################################################\n# Theoretical bounds\n# ==================\n# The distortion introduced by a random projection `p` is asserted by\n# the fact that `p` is defining an eps-embedding with good probability\n# as defined by:\n#\n# .. math::\n#    (1 - eps) \\|u - v\\|^2 < \\|p(u) - p(v)\\|^2 < (1 + eps) \\|u - v\\|^2\n#\n# Where u and v are any rows taken from a dataset of shape [n_samples,\n# n_features] and p is a projection by a random Gaussian N(0, 1) matrix\n# with shape [n_components, n_features] (or a sparse Achlioptas matrix).\n#\n# The minimum number of components to guarantees the eps-embedding is\n# given by:\n#\n# .. math::\n#    n\\_components >= 4 log(n\\_samples) / (eps^2 / 2 - eps^3 / 3)\n#\n#\n# The first plot shows that with an increasing number of samples ``n_samples``,\n# the minimal number of dimensions ``n_components`` increased logarithmically\n# in order to guarantee an ``eps``-embedding.\n\n# range of admissible distortions\neps_range = np.linspace(0.1, 0.99, 5)\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = np.logspace(1, 9, 9)\n\nplt.figure()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples_range, eps=eps)\n    plt.loglog(n_samples_range, min_n_components, color=color)\n\nplt.legend([""eps = %0.1f"" % eps for eps in eps_range], loc=""lower right"")\nplt.xlabel(""Number of observations to eps-embed"")\nplt.ylabel(""Minimum number of dimensions"")\nplt.title(""Johnson-Lindenstrauss bounds:\\nn_samples vs n_components"")\nplt.show()\n\n\n##########################################################\n# The second plot shows that an increase of the admissible\n# distortion ``eps`` allows to reduce drastically the minimal number of\n# dimensions ``n_components`` for a given number of samples ``n_samples``\n\n# range of admissible distortions\neps_range = np.linspace(0.01, 0.99, 100)\n\n# range of number of samples (observation) to embed\nn_samples_range = np.logspace(2, 6, 5)\ncolors = plt.cm.Blues(np.linspace(0.3, 1.0, len(n_samples_range)))\n\nplt.figure()\nfor n_samples, color in zip(n_samples_range, colors):\n    min_n_components = johnson_lindenstrauss_min_dim(n_samples, eps=eps_range)\n    plt.semilogy(eps_range, min_n_components, color=color)\n\nplt.legend([""n_samples = %d"" % n for n in n_samples_range], loc=""upper right"")\nplt.xlabel(""Distortion eps"")\nplt.ylabel(""Minimum number of dimensions"")\nplt.title(""Johnson-Lindenstrauss bounds:\\nn_components vs eps"")\nplt.show()\n\n##########################################################\n# Empirical validation\n# ====================\n#\n# We validate the above bounds on the 20 newsgroups text document\n# (TF-IDF word frequencies) dataset or on the digits dataset:\n#\n# - for the 20 newsgroups dataset some 500 documents with 100k\n#   features in total are projected using a sparse random matrix to smaller\n#   euclidean spaces with various values for the target number of dimensions\n#   ``n_components``.\n#\n# - for the digits dataset, some 8x8 gray level pixels data for 500\n#   handwritten digits pictures are randomly projected to spaces for various\n#   larger number of dimensions ``n_components``.\n#\n# The default dataset is the 20 newsgroups dataset. To run the example on the\n# digits dataset, pass the ``--use-digits-dataset`` command line argument to\n# this script.\n\nif \'--use-digits-dataset\' in sys.argv:\n    data = load_digits().data[:500]\nelse:\n    data = fetch_20newsgroups_vectorized().data[:500]\n\n##########################################################\n# For each value of ``n_components``, we plot:\n#\n# - 2D distribution of sample pairs with pairwise distances in original\n#   and projected spaces as x and y axis respectively.\n#\n# - 1D histogram of the ratio of those distances (projected / original).\n\nn_samples, n_features = data.shape\nprint(""Embedding %d samples with dim %d using various random projections""\n      % (n_samples, n_features))\n\nn_components_range = np.array([300, 1000, 10000])\ndists = euclidean_distances(data, squared=True).ravel()\n\n# select only non-identical samples pairs\nnonzero = dists != 0\ndists = dists[nonzero]\n\nfor n_components in n_components_range:\n    t0 = time()\n    rp = SparseRandomProjection(n_components=n_components)\n    projected_data = rp.fit_transform(data)\n    print(""Projected %d samples from %d to %d in %0.3fs""\n          % (n_samples, n_features, n_components, time() - t0))\n    if hasattr(rp, \'components_\'):\n        n_bytes = rp.components_.data.nbytes\n        n_bytes += rp.components_.indices.nbytes\n        print(""Random matrix with size: %0.3fMB"" % (n_bytes / 1e6))\n\n    projected_dists = euclidean_distances(\n        projected_data, squared=True).ravel()[nonzero]\n\n    plt.figure()\n    min_dist = min(projected_dists.min(), dists.min())\n    max_dist = max(projected_dists.max(), dists.max())\n    plt.hexbin(dists, projected_dists, gridsize=100, cmap=plt.cm.PuBu,\n               extent=[min_dist, max_dist, min_dist, max_dist])\n    plt.xlabel(""Pairwise squared distances in original space"")\n    plt.ylabel(""Pairwise squared distances in projected space"")\n    plt.title(""Pairwise distances distribution for n_components=%d"" %\n              n_components)\n    cb = plt.colorbar()\n    cb.set_label(\'Sample pairs counts\')\n\n    rates = projected_dists / dists\n    print(""Mean distances rate: %0.2f (%0.2f)""\n          % (np.mean(rates), np.std(rates)))\n\n    plt.figure()\n    plt.hist(rates, bins=50, range=(0., 2.), edgecolor=\'k\', **density_param)\n    plt.xlabel(""Squared distances rate: projected / original"")\n    plt.ylabel(""Distribution of samples pairs"")\n    plt.title(""Histogram of pairwise distance rates for n_components=%d"" %\n              n_components)\n\n    # TODO: compute the expected value of eps and add them to the previous plot\n    # as vertical lines / region\n\nplt.show()\n\n\n##########################################################\n# We can see that for low values of ``n_components`` the distribution is wide\n# with many distorted pairs and a skewed distribution (due to the hard\n# limit of zero ratio on the left as distances are always positives)\n# while for larger values of n_components the distortion is controlled\n# and the distances are well preserved by the random projection.\n\n\n##########################################################\n# Remarks\n# =======\n#\n# According to the JL lemma, projecting 500 samples without too much distortion\n# will require at least several thousands dimensions, irrespective of the\n# number of features of the original dataset.\n#\n# Hence using random projections on the digits dataset which only has 64\n# features in the input space does not make sense: it does not allow\n# for dimensionality reduction in this case.\n#\n# On the twenty newsgroups on the other hand the dimensionality can be\n# decreased from 56436 down to 10000 while reasonably preserving\n# pairwise distances.\n'"
scikit-learn/_downloads/ebeb12f9c7bf58f10647d335ac94aa7d/plot_robust_fit.py,9,"b'""""""\nRobust linear estimator fitting\n===============================\n\nHere a sine function is fit with a polynomial of order 3, for values\nclose to zero.\n\nRobust fitting is demoed in different situations:\n\n- No measurement errors, only modelling errors (fitting a sine with a\n  polynomial)\n\n- Measurement errors in X\n\n- Measurement errors in y\n\nThe median absolute deviation to non corrupt new data is used to judge\nthe quality of the prediction.\n\nWhat we can see that:\n\n- RANSAC is good for strong outliers in the y direction\n\n- TheilSen is good for small outliers, both in direction X and y, but has\n  a break point above which it performs worse than OLS.\n\n- The scores of HuberRegressor may not be compared directly to both TheilSen\n  and RANSAC because it does not attempt to completely filter the outliers\n  but lessen their effect.\n\n""""""\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import (\n    LinearRegression, TheilSenRegressor, RANSACRegressor, HuberRegressor)\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nnp.random.seed(42)\n\nX = np.random.normal(size=400)\ny = np.sin(X)\n# Make sure that it X is 2D\nX = X[:, np.newaxis]\n\nX_test = np.random.normal(size=200)\ny_test = np.sin(X_test)\nX_test = X_test[:, np.newaxis]\n\ny_errors = y.copy()\ny_errors[::3] = 3\n\nX_errors = X.copy()\nX_errors[::3] = 3\n\ny_errors_large = y.copy()\ny_errors_large[::3] = 10\n\nX_errors_large = X.copy()\nX_errors_large[::3] = 10\n\nestimators = [(\'OLS\', LinearRegression()),\n              (\'Theil-Sen\', TheilSenRegressor(random_state=42)),\n              (\'RANSAC\', RANSACRegressor(random_state=42)),\n              (\'HuberRegressor\', HuberRegressor())]\ncolors = {\'OLS\': \'turquoise\', \'Theil-Sen\': \'gold\', \'RANSAC\': \'lightgreen\', \'HuberRegressor\': \'black\'}\nlinestyle = {\'OLS\': \'-\', \'Theil-Sen\': \'-.\', \'RANSAC\': \'--\', \'HuberRegressor\': \'--\'}\nlw = 3\n\nx_plot = np.linspace(X.min(), X.max())\nfor title, this_X, this_y in [\n        (\'Modeling Errors Only\', X, y),\n        (\'Corrupt X, Small Deviants\', X_errors, y),\n        (\'Corrupt y, Small Deviants\', X, y_errors),\n        (\'Corrupt X, Large Deviants\', X_errors_large, y),\n        (\'Corrupt y, Large Deviants\', X, y_errors_large)]:\n    plt.figure(figsize=(5, 4))\n    plt.plot(this_X[:, 0], this_y, \'b+\')\n\n    for name, estimator in estimators:\n        model = make_pipeline(PolynomialFeatures(3), estimator)\n        model.fit(this_X, this_y)\n        mse = mean_squared_error(model.predict(X_test), y_test)\n        y_plot = model.predict(x_plot[:, np.newaxis])\n        plt.plot(x_plot, y_plot, color=colors[name], linestyle=linestyle[name],\n                 linewidth=lw, label=\'%s: error = %.3f\' % (name, mse))\n\n    legend_title = \'Error of Mean\\nAbsolute Deviation\\nto Non-corrupt Data\'\n    legend = plt.legend(loc=\'upper right\', frameon=False, title=legend_title,\n                        prop=dict(size=\'x-small\'))\n    plt.xlim(-4, 10.2)\n    plt.ylim(-2, 10.2)\n    plt.title(title)\nplt.show()\n'"
scikit-learn/_downloads/ed18dfe43538fdabf98d1532b39f0c82/plot_multi_task_lasso_support.py,6,"b'#!/usr/bin/env python\n""""""\n=============================================\nJoint feature selection with multi-task Lasso\n=============================================\n\nThe multi-task lasso allows to fit multiple regression problems\njointly enforcing the selected features to be the same across\ntasks. This example simulates sequential measurements, each task\nis a time instant, and the relevant features vary in amplitude\nover time while being the same. The multi-task lasso imposes that\nfeatures that are selected at one time point are select for all time\npoint. This makes feature selection by the Lasso more stable.\n\n""""""\nprint(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import MultiTaskLasso, Lasso\n\nrng = np.random.RandomState(42)\n\n# Generate some 2D coefficients with sine waves with random frequency and phase\nn_samples, n_features, n_tasks = 100, 30, 40\nn_relevant_features = 5\ncoef = np.zeros((n_tasks, n_features))\ntimes = np.linspace(0, 2 * np.pi, n_tasks)\nfor k in range(n_relevant_features):\n    coef[:, k] = np.sin((1. + rng.randn(1)) * times + 3 * rng.randn(1))\n\nX = rng.randn(n_samples, n_features)\nY = np.dot(X, coef.T) + rng.randn(n_samples, n_tasks)\n\ncoef_lasso_ = np.array([Lasso(alpha=0.5).fit(X, y).coef_ for y in Y.T])\ncoef_multi_task_lasso_ = MultiTaskLasso(alpha=1.).fit(X, Y).coef_\n\n# #############################################################################\n# Plot support and time series\nfig = plt.figure(figsize=(8, 5))\nplt.subplot(1, 2, 1)\nplt.spy(coef_lasso_)\nplt.xlabel(\'Feature\')\nplt.ylabel(\'Time (or Task)\')\nplt.text(10, 5, \'Lasso\')\nplt.subplot(1, 2, 2)\nplt.spy(coef_multi_task_lasso_)\nplt.xlabel(\'Feature\')\nplt.ylabel(\'Time (or Task)\')\nplt.text(10, 5, \'MultiTaskLasso\')\nfig.suptitle(\'Coefficient non-zero location\')\n\nfeature_to_plot = 0\nplt.figure()\nlw = 2\nplt.plot(coef[:, feature_to_plot], color=\'seagreen\', linewidth=lw,\n         label=\'Ground truth\')\nplt.plot(coef_lasso_[:, feature_to_plot], color=\'cornflowerblue\', linewidth=lw,\n         label=\'Lasso\')\nplt.plot(coef_multi_task_lasso_[:, feature_to_plot], color=\'gold\', linewidth=lw,\n         label=\'MultiTaskLasso\')\nplt.legend(loc=\'upper center\')\nplt.axis(\'tight\')\nplt.ylim([-1.1, 1.1])\nplt.show()\n'"
scikit-learn/_downloads/ee0ec20765dc877f0c987ff30d473cd4/plot_iris_exercise.py,5,"b'""""""\n================================\nSVM Exercise\n================================\n\nA tutorial exercise for using different SVM kernels.\n\nThis exercise is used in the :ref:`using_kernels_tut` part of the\n:ref:`supervised_learning_tut` section of the :ref:`stat_learn_tut_index`.\n""""""\nprint(__doc__)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, svm\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX = X[y != 0, :2]\ny = y[y != 0]\n\nn_sample = len(X)\n\nnp.random.seed(0)\norder = np.random.permutation(n_sample)\nX = X[order]\ny = y[order].astype(np.float)\n\nX_train = X[:int(.9 * n_sample)]\ny_train = y[:int(.9 * n_sample)]\nX_test = X[int(.9 * n_sample):]\ny_test = y[int(.9 * n_sample):]\n\n# fit the model\nfor kernel in (\'linear\', \'rbf\', \'poly\'):\n    clf = svm.SVC(kernel=kernel, gamma=10)\n    clf.fit(X_train, y_train)\n\n    plt.figure()\n    plt.clf()\n    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.Paired,\n                edgecolor=\'k\', s=20)\n\n    # Circle out the test data\n    plt.scatter(X_test[:, 0], X_test[:, 1], s=80, facecolors=\'none\',\n                zorder=10, edgecolor=\'k\')\n\n    plt.axis(\'tight\')\n    x_min = X[:, 0].min()\n    x_max = X[:, 0].max()\n    y_min = X[:, 1].min()\n    y_max = X[:, 1].max()\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n    plt.contour(XX, YY, Z, colors=[\'k\', \'k\', \'k\'],\n                linestyles=[\'--\', \'-\', \'--\'], levels=[-.5, 0, .5])\n\n    plt.title(kernel)\nplt.show()\n'"
scikit-learn/_downloads/ef1f093367558e52c29aa60d42ab930f/plot_multioutput_face_completion.py,2,"b'""""""\n==============================================\nFace completion with a multi-output estimators\n==============================================\n\nThis example shows the use of multi-output estimator to complete images.\nThe goal is to predict the lower half of a face given its upper half.\n\nThe first column of images shows true faces. The next columns illustrate\nhow extremely randomized trees, k nearest neighbors, linear\nregression and ridge regression complete the lower half of those faces.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn.utils.validation import check_random_state\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import RidgeCV\n\n# Load the faces datasets\ndata, targets = fetch_olivetti_faces(return_X_y=True)\n\ntrain = data[targets < 30]\ntest = data[targets >= 30]  # Test on independent people\n\n# Test on a subset of people\nn_faces = 5\nrng = check_random_state(4)\nface_ids = rng.randint(test.shape[0], size=(n_faces, ))\ntest = test[face_ids, :]\n\nn_pixels = data.shape[1]\n# Upper half of the faces\nX_train = train[:, :(n_pixels + 1) // 2]\n# Lower half of the faces\ny_train = train[:, n_pixels // 2:]\nX_test = test[:, :(n_pixels + 1) // 2]\ny_test = test[:, n_pixels // 2:]\n\n# Fit estimators\nESTIMATORS = {\n    ""Extra trees"": ExtraTreesRegressor(n_estimators=10, max_features=32,\n                                       random_state=0),\n    ""K-nn"": KNeighborsRegressor(),\n    ""Linear regression"": LinearRegression(),\n    ""Ridge"": RidgeCV(),\n}\n\ny_test_predict = dict()\nfor name, estimator in ESTIMATORS.items():\n    estimator.fit(X_train, y_train)\n    y_test_predict[name] = estimator.predict(X_test)\n\n# Plot the completed faces\nimage_shape = (64, 64)\n\nn_cols = 1 + len(ESTIMATORS)\nplt.figure(figsize=(2. * n_cols, 2.26 * n_faces))\nplt.suptitle(""Face completion with multi-output estimators"", size=16)\n\nfor i in range(n_faces):\n    true_face = np.hstack((X_test[i], y_test[i]))\n\n    if i:\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1)\n    else:\n        sub = plt.subplot(n_faces, n_cols, i * n_cols + 1,\n                          title=""true faces"")\n\n    sub.axis(""off"")\n    sub.imshow(true_face.reshape(image_shape),\n               cmap=plt.cm.gray,\n               interpolation=""nearest"")\n\n    for j, est in enumerate(sorted(ESTIMATORS)):\n        completed_face = np.hstack((X_test[i], y_test_predict[est][i]))\n\n        if i:\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j)\n\n        else:\n            sub = plt.subplot(n_faces, n_cols, i * n_cols + 2 + j,\n                              title=est)\n\n        sub.axis(""off"")\n        sub.imshow(completed_face.reshape(image_shape),\n                   cmap=plt.cm.gray,\n                   interpolation=""nearest"")\n\nplt.show()\n'"
scikit-learn/_downloads/ef602a5db9f748e7735a011cda8b6438/plot_mean_shift.py,1,"b'""""""\n=============================================\nA demo of the mean-shift clustering algorithm\n=============================================\n\nReference:\n\nDorin Comaniciu and Peter Meer, ""Mean Shift: A robust approach toward\nfeature space analysis"". IEEE Transactions on Pattern Analysis and\nMachine Intelligence. 2002. pp. 603-619.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nfrom sklearn.datasets import make_blobs\n\n# #############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)\n\n# #############################################################################\n# Compute clustering with MeanShift\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\n\nlabels_unique = np.unique(labels)\nn_clusters_ = len(labels_unique)\n\nprint(""number of estimated clusters : %d"" % n_clusters_)\n\n# #############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nplt.figure(1)\nplt.clf()\n\ncolors = cycle(\'bgrcmykbgrcmykbgrcmykbgrcmyk\')\nfor k, col in zip(range(n_clusters_), colors):\n    my_members = labels == k\n    cluster_center = cluster_centers[k]\n    plt.plot(X[my_members, 0], X[my_members, 1], col + \'.\')\n    plt.plot(cluster_center[0], cluster_center[1], \'o\', markerfacecolor=col,\n             markeredgecolor=\'k\', markersize=14)\nplt.title(\'Estimated number of clusters: %d\' % n_clusters_)\nplt.show()\n'"
scikit-learn/_downloads/f1af72f521fc5fce71a16e8218d7d843/plot_huber_vs_ridge.py,4,"b'""""""\n=======================================================\nHuberRegressor vs Ridge on dataset with strong outliers\n=======================================================\n\nFit Ridge and HuberRegressor on a dataset with outliers.\n\nThe example shows that the predictions in ridge are strongly influenced\nby the outliers present in the dataset. The Huber regressor is less\ninfluenced by the outliers since the model uses the linear loss for these.\nAs the parameter epsilon is increased for the Huber regressor, the decision\nfunction approaches that of the ridge.\n""""""\n\n# Authors: Manoj Kumar mks542@nyu.edu\n# License: BSD 3 clause\n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import HuberRegressor, Ridge\n\n# Generate toy data.\nrng = np.random.RandomState(0)\nX, y = make_regression(n_samples=20, n_features=1, random_state=0, noise=4.0,\n                       bias=100.0)\n\n# Add four strong outliers to the dataset.\nX_outliers = rng.normal(0, 0.5, size=(4, 1))\ny_outliers = rng.normal(0, 2.0, size=4)\nX_outliers[:2, :] += X.max() + X.mean() / 4.\nX_outliers[2:, :] += X.min() - X.mean() / 4.\ny_outliers[:2] += y.min() - y.mean() / 4.\ny_outliers[2:] += y.max() + y.mean() / 4.\nX = np.vstack((X, X_outliers))\ny = np.concatenate((y, y_outliers))\nplt.plot(X, y, \'b.\')\n\n# Fit the huber regressor over a series of epsilon values.\ncolors = [\'r-\', \'b-\', \'y-\', \'m-\']\n\nx = np.linspace(X.min(), X.max(), 7)\nepsilon_values = [1.35, 1.5, 1.75, 1.9]\nfor k, epsilon in enumerate(epsilon_values):\n    huber = HuberRegressor(alpha=0.0, epsilon=epsilon)\n    huber.fit(X, y)\n    coef_ = huber.coef_ * x + huber.intercept_\n    plt.plot(x, coef_, colors[k], label=""huber loss, %s"" % epsilon)\n\n# Fit a ridge regressor to compare it to huber regressor.\nridge = Ridge(alpha=0.0, random_state=0, normalize=True)\nridge.fit(X, y)\ncoef_ridge = ridge.coef_\ncoef_ = ridge.coef_ * x + ridge.intercept_\nplt.plot(x, coef_, \'g-\', label=""ridge regression"")\n\nplt.title(""Comparison of HuberRegressor vs Ridge"")\nplt.xlabel(""X"")\nplt.ylabel(""y"")\nplt.legend(loc=0)\nplt.show()\n'"
scikit-learn/_downloads/f51f1beefb171ab22378476d4408eeef/plot_feature_selection_pipeline.py,0,"b'""""""\n==================\nPipeline Anova SVM\n==================\n\nSimple usage of Pipeline that runs successively a univariate\nfeature selection with anova and then a SVM of the selected features.\n\nUsing a sub-pipeline, the fitted coefficients can be mapped back into\nthe original feature space.\n""""""\nfrom sklearn import svm\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\nprint(__doc__)\n\n# import some data to play with\nX, y = make_classification(\n    n_features=20, n_informative=3, n_redundant=0, n_classes=4,\n    n_clusters_per_class=2)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n# ANOVA SVM-C\n# 1) anova filter, take 3 best ranked features\nanova_filter = SelectKBest(f_regression, k=3)\n# 2) svm\nclf = svm.LinearSVC()\n\nanova_svm = make_pipeline(anova_filter, clf)\nanova_svm.fit(X_train, y_train)\ny_pred = anova_svm.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\ncoef = anova_svm[:-1].inverse_transform(anova_svm[\'linearsvc\'].coef_)\nprint(coef)\n'"
scikit-learn/_downloads/f5f4701f92ff240252b77472f43a2bdd/plot_sparse_cov.py,8,"b'""""""\n======================================\nSparse inverse covariance estimation\n======================================\n\nUsing the GraphicalLasso estimator to learn a covariance and sparse precision\nfrom a small number of samples.\n\nTo estimate a probabilistic model (e.g. a Gaussian model), estimating the\nprecision matrix, that is the inverse covariance matrix, is as important\nas estimating the covariance matrix. Indeed a Gaussian model is\nparametrized by the precision matrix.\n\nTo be in favorable recovery conditions, we sample the data from a model\nwith a sparse inverse covariance matrix. In addition, we ensure that the\ndata is not too much correlated (limiting the largest coefficient of the\nprecision matrix) and that there a no small coefficients in the\nprecision matrix that cannot be recovered. In addition, with a small\nnumber of observations, it is easier to recover a correlation matrix\nrather than a covariance, thus we scale the time series.\n\nHere, the number of samples is slightly larger than the number of\ndimensions, thus the empirical covariance is still invertible. However,\nas the observations are strongly correlated, the empirical covariance\nmatrix is ill-conditioned and as a result its inverse --the empirical\nprecision matrix-- is very far from the ground truth.\n\nIf we use l2 shrinkage, as with the Ledoit-Wolf estimator, as the number\nof samples is small, we need to shrink a lot. As a result, the\nLedoit-Wolf precision is fairly close to the ground truth precision, that\nis not far from being diagonal, but the off-diagonal structure is lost.\n\nThe l1-penalized estimator can recover part of this off-diagonal\nstructure. It learns a sparse precision. It is not able to\nrecover the exact sparsity pattern: it detects too many non-zero\ncoefficients. However, the highest non-zero coefficients of the l1\nestimated correspond to the non-zero coefficients in the ground truth.\nFinally, the coefficients of the l1 precision estimate are biased toward\nzero: because of the penalty, they are all smaller than the corresponding\nground truth value, as can be seen on the figure.\n\nNote that, the color range of the precision matrices is tweaked to\nimprove readability of the figure. The full range of values of the\nempirical precision is not displayed.\n\nThe alpha parameter of the GraphicalLasso setting the sparsity of the model is\nset by internal cross-validation in the GraphicalLassoCV. As can be\nseen on figure 2, the grid to compute the cross-validation score is\niteratively refined in the neighborhood of the maximum.\n""""""\nprint(__doc__)\n# author: Gael Varoquaux <gael.varoquaux@inria.fr>\n# License: BSD 3 clause\n# Copyright: INRIA\n\nimport numpy as np\nfrom scipy import linalg\nfrom sklearn.datasets import make_sparse_spd_matrix\nfrom sklearn.covariance import GraphicalLassoCV, ledoit_wolf\nimport matplotlib.pyplot as plt\n\n# #############################################################################\n# Generate the data\nn_samples = 60\nn_features = 20\n\nprng = np.random.RandomState(1)\nprec = make_sparse_spd_matrix(n_features, alpha=.98,\n                              smallest_coef=.4,\n                              largest_coef=.7,\n                              random_state=prng)\ncov = linalg.inv(prec)\nd = np.sqrt(np.diag(cov))\ncov /= d\ncov /= d[:, np.newaxis]\nprec *= d\nprec *= d[:, np.newaxis]\nX = prng.multivariate_normal(np.zeros(n_features), cov, size=n_samples)\nX -= X.mean(axis=0)\nX /= X.std(axis=0)\n\n# #############################################################################\n# Estimate the covariance\nemp_cov = np.dot(X.T, X) / n_samples\n\nmodel = GraphicalLassoCV()\nmodel.fit(X)\ncov_ = model.covariance_\nprec_ = model.precision_\n\nlw_cov_, _ = ledoit_wolf(X)\nlw_prec_ = linalg.inv(lw_cov_)\n\n# #############################################################################\n# Plot the results\nplt.figure(figsize=(10, 6))\nplt.subplots_adjust(left=0.02, right=0.98)\n\n# plot the covariances\ncovs = [(\'Empirical\', emp_cov), (\'Ledoit-Wolf\', lw_cov_),\n        (\'GraphicalLassoCV\', cov_), (\'True\', cov)]\nvmax = cov_.max()\nfor i, (name, this_cov) in enumerate(covs):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(this_cov, interpolation=\'nearest\', vmin=-vmax, vmax=vmax,\n               cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(\'%s covariance\' % name)\n\n\n# plot the precisions\nprecs = [(\'Empirical\', linalg.inv(emp_cov)), (\'Ledoit-Wolf\', lw_prec_),\n         (\'GraphicalLasso\', prec_), (\'True\', prec)]\nvmax = .9 * prec_.max()\nfor i, (name, this_prec) in enumerate(precs):\n    ax = plt.subplot(2, 4, i + 5)\n    plt.imshow(np.ma.masked_equal(this_prec, 0),\n               interpolation=\'nearest\', vmin=-vmax, vmax=vmax,\n               cmap=plt.cm.RdBu_r)\n    plt.xticks(())\n    plt.yticks(())\n    plt.title(\'%s precision\' % name)\n    if hasattr(ax, \'set_facecolor\'):\n        ax.set_facecolor(\'.7\')\n    else:\n        ax.set_axis_bgcolor(\'.7\')\n\n# plot the model selection metric\nplt.figure(figsize=(4, 3))\nplt.axes([.2, .15, .75, .7])\nplt.plot(model.cv_alphas_, np.mean(model.grid_scores_, axis=1), \'o-\')\nplt.axvline(model.alpha_, color=\'.5\')\nplt.title(\'Model selection\')\nplt.ylabel(\'Cross-validation score\')\nplt.xlabel(\'alpha\')\n\nplt.show()\n'"
scikit-learn/_downloads/f67d8c12591e982bd103bf88cd5b55d1/plot_rfe_with_cross_validation.py,0,"b'""""""\n===================================================\nRecursive feature elimination with cross-validation\n===================================================\n\nA recursive feature elimination example with automatic tuning of the\nnumber of features selected with cross-validation.\n""""""\nprint(__doc__)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.datasets import make_classification\n\n# Build a classification task using 3 informative features\nX, y = make_classification(n_samples=1000, n_features=25, n_informative=3,\n                           n_redundant=2, n_repeated=0, n_classes=8,\n                           n_clusters_per_class=1, random_state=0)\n\n# Create the RFE object and compute a cross-validated score.\nsvc = SVC(kernel=""linear"")\n# The ""accuracy"" scoring is proportional to the number of correct\n# classifications\nrfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n              scoring=\'accuracy\')\nrfecv.fit(X, y)\n\nprint(""Optimal number of features : %d"" % rfecv.n_features_)\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(""Number of features selected"")\nplt.ylabel(""Cross validation score (nb of correct classifications)"")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()\n'"
scikit-learn/_downloads/f685ae60d0f16249b244f7b09be5f6c7/plot_roc_crossval.py,8,"b'""""""\n=============================================================\nReceiver Operating Characteristic (ROC) with cross validation\n=============================================================\n\nExample of Receiver Operating Characteristic (ROC) metric to evaluate\nclassifier output quality using cross-validation.\n\nROC curves typically feature true positive rate on the Y axis, and false\npositive rate on the X axis. This means that the top left corner of the plot is\nthe ""ideal"" point - a false positive rate of zero, and a true positive rate of\none. This is not very realistic, but it does mean that a larger area under the\ncurve (AUC) is usually better.\n\nThe ""steepness"" of ROC curves is also important, since it is ideal to maximize\nthe true positive rate while minimizing the false positive rate.\n\nThis example shows the ROC response of different datasets, created from K-fold\ncross-validation. Taking all of these curves, it is possible to calculate the\nmean area under curve, and see the variance of the curve when the\ntraining set is split into different subsets. This roughly shows how the\nclassifier output is affected by changes in the training data, and how\ndifferent the splits generated by K-fold cross-validation are from one another.\n\n.. note::\n\n    See also :func:`sklearn.metrics.roc_auc_score`,\n             :func:`sklearn.model_selection.cross_val_score`,\n             :ref:`sphx_glr_auto_examples_model_selection_plot_roc.py`,\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nfrom scipy import interp\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import StratifiedKFold\n\n# #############################################################################\n# Data IO and generation\n\n# Import some data to play with\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX, y = X[y != 2], y[y != 2]\nn_samples, n_features = X.shape\n\n# Add noisy features\nrandom_state = np.random.RandomState(0)\nX = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n\n# #############################################################################\n# Classification and ROC analysis\n\n# Run classifier with cross-validation and plot ROC curves\ncv = StratifiedKFold(n_splits=6)\nclassifier = svm.SVC(kernel=\'linear\', probability=True,\n                     random_state=random_state)\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\n\nfig, ax = plt.subplots()\nfor i, (train, test) in enumerate(cv.split(X, y)):\n    classifier.fit(X[train], y[train])\n    viz = plot_roc_curve(classifier, X[test], y[test],\n                         name=\'ROC fold {}\'.format(i),\n                         alpha=0.3, lw=1, ax=ax)\n    interp_tpr = interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n\nax.plot([0, 1], [0, 1], linestyle=\'--\', lw=2, color=\'r\',\n        label=\'Chance\', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nax.plot(mean_fpr, mean_tpr, color=\'b\',\n        label=r\'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\' % (mean_auc, std_auc),\n        lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nax.fill_between(mean_fpr, tprs_lower, tprs_upper, color=\'grey\', alpha=.2,\n                label=r\'$\\pm$ 1 std. dev.\')\n\nax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n       title=""Receiver operating characteristic example"")\nax.legend(loc=""lower right"")\nplt.show()\n'"
scikit-learn/_downloads/f6d96eb4505af37d36e80572a988ed21/plot_discretization.py,3,"b'# -*- coding: utf-8 -*-\n\n""""""\n================================================================\nUsing KBinsDiscretizer to discretize continuous features\n================================================================\n\nThe example compares prediction result of linear regression (linear model)\nand decision tree (tree based model) with and without discretization of\nreal-valued features.\n\nAs is shown in the result before discretization, linear model is fast to\nbuild and relatively straightforward to interpret, but can only model\nlinear relationships, while decision tree can build a much more complex model\nof the data. One way to make linear model more powerful on continuous data\nis to use discretization (also known as binning). In the example, we\ndiscretize the feature and one-hot encode the transformed data. Note that if\nthe bins are not reasonably wide, there would appear to be a substantially\nincreased risk of overfitting, so the discretizer parameters should usually\nbe tuned under cross validation.\n\nAfter discretization, linear regression and decision tree make exactly the\nsame prediction. As features are constant within each bin, any model must\npredict the same value for all points within a bin. Compared with the result\nbefore discretization, linear model become much more flexible while decision\ntree gets much less flexible. Note that binning features generally has no\nbeneficial effect for tree-based models, as these models can learn to split\nup the data anywhere.\n\n""""""\n\n# Author: Andreas M\xc3\xbcller\n#         Hanmin Qin <qinhanmin2005@sina.com>\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeRegressor\n\nprint(__doc__)\n\n# construct the dataset\nrnd = np.random.RandomState(42)\nX = rnd.uniform(-3, 3, size=100)\ny = np.sin(X) + rnd.normal(size=len(X)) / 3\nX = X.reshape(-1, 1)\n\n# transform the dataset with KBinsDiscretizer\nenc = KBinsDiscretizer(n_bins=10, encode=\'onehot\')\nX_binned = enc.fit_transform(X)\n\n# predict with original dataset\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))\nline = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\nreg = LinearRegression().fit(X, y)\nax1.plot(line, reg.predict(line), linewidth=2, color=\'green\',\n         label=""linear regression"")\nreg = DecisionTreeRegressor(min_samples_split=3, random_state=0).fit(X, y)\nax1.plot(line, reg.predict(line), linewidth=2, color=\'red\',\n         label=""decision tree"")\nax1.plot(X[:, 0], y, \'o\', c=\'k\')\nax1.legend(loc=""best"")\nax1.set_ylabel(""Regression output"")\nax1.set_xlabel(""Input feature"")\nax1.set_title(""Result before discretization"")\n\n# predict with transformed dataset\nline_binned = enc.transform(line)\nreg = LinearRegression().fit(X_binned, y)\nax2.plot(line, reg.predict(line_binned), linewidth=2, color=\'green\',\n         linestyle=\'-\', label=\'linear regression\')\nreg = DecisionTreeRegressor(min_samples_split=3,\n                            random_state=0).fit(X_binned, y)\nax2.plot(line, reg.predict(line_binned), linewidth=2, color=\'red\',\n         linestyle=\':\', label=\'decision tree\')\nax2.plot(X[:, 0], y, \'o\', c=\'k\')\nax2.vlines(enc.bin_edges_[0], *plt.gca().get_ylim(), linewidth=1, alpha=.2)\nax2.legend(loc=""best"")\nax2.set_xlabel(""Input feature"")\nax2.set_title(""Result after discretization"")\n\nplt.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/f6ed4f92ecc0562e80870c69cd204478/plot_permutation_importance.py,3,"b'""""""\n================================================================\nPermutation Importance vs Random Forest Feature Importance (MDI)\n================================================================\n\nIn this example, we will compare the impurity-based feature importance of\n:class:`~sklearn.ensemble.RandomForestClassifier` with the\npermutation importance on the titanic dataset using\n:func:`~sklearn.inspection.permutation_importance`. We will show that the\nimpurity-based feature importance can inflate the importance of numerical\nfeatures.\n\nFurthermore, the impurity-based feature importance of random forests suffers\nfrom being computed on statistics derived from the training dataset: the\nimportances can be high even for features that are not predictive of the target\nvariable, as long as the model has the capacity to use them to overfit.\n\nThis example shows how to use Permutation Importances as an alternative that\ncan mitigate those limitations.\n\n.. topic:: References:\n\n   [1] L. Breiman, ""Random Forests"", Machine Learning, 45(1), 5-32,\n       2001. https://doi.org/10.1023/A:1010933404324\n""""""\nprint(__doc__)\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n##############################################################################\n# Data Loading and Feature Engineering\n# ------------------------------------\n# Let\'s use pandas to load a copy of the titanic dataset. The following shows\n# how to apply separate preprocessing on numerical and categorical features.\n#\n# We further include two random variables that are not correlated in any way\n# with the target variable (``survived``):\n#\n# - ``random_num`` is a high cardinality numerical variable (as many unique\n#   values as records).\n# - ``random_cat`` is a low cardinality categorical variable (3 possible\n#   values).\nX, y = fetch_openml(""titanic"", version=1, as_frame=True, return_X_y=True)\nrng = np.random.RandomState(seed=42)\nX[\'random_cat\'] = rng.randint(3, size=X.shape[0])\nX[\'random_num\'] = rng.randn(X.shape[0])\n\ncategorical_columns = [\'pclass\', \'sex\', \'embarked\', \'random_cat\']\nnumerical_columns = [\'age\', \'sibsp\', \'parch\', \'fare\', \'random_num\']\n\nX = X[categorical_columns + numerical_columns]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=42)\n\ncategorical_pipe = Pipeline([\n    (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')),\n    (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\'))\n])\nnumerical_pipe = Pipeline([\n    (\'imputer\', SimpleImputer(strategy=\'mean\'))\n])\n\npreprocessing = ColumnTransformer(\n    [(\'cat\', categorical_pipe, categorical_columns),\n     (\'num\', numerical_pipe, numerical_columns)])\n\nrf = Pipeline([\n    (\'preprocess\', preprocessing),\n    (\'classifier\', RandomForestClassifier(random_state=42))\n])\nrf.fit(X_train, y_train)\n\n##############################################################################\n# Accuracy of the Model\n# ---------------------\n# Prior to inspecting the feature importances, it is important to check that\n# the model predictive performance is high enough. Indeed there would be little\n# interest of inspecting the important features of a non-predictive model.\n#\n# Here one can observe that the train accuracy is very high (the forest model\n# has enough capacity to completely memorize the training set) but it can still\n# generalize well enough to the test set thanks to the built-in bagging of\n# random forests.\n#\n# It might be possible to trade some accuracy on the training set for a\n# slightly better accuracy on the test set by limiting the capacity of the\n# trees (for instance by setting ``min_samples_leaf=5`` or\n# ``min_samples_leaf=10``) so as to limit overfitting while not introducing too\n# much underfitting.\n#\n# However let\'s keep our high capacity random forest model for now so as to\n# illustrate some pitfalls with feature importance on variables with many\n# unique values.\nprint(""RF train accuracy: %0.3f"" % rf.score(X_train, y_train))\nprint(""RF test accuracy: %0.3f"" % rf.score(X_test, y_test))\n\n\n##############################################################################\n# Tree\'s Feature Importance from Mean Decrease in Impurity (MDI)\n# --------------------------------------------------------------\n# The impurity-based feature importance ranks the numerical features to be the\n# most important features. As a result, the non-predictive ``random_num``\n# variable is ranked the most important!\n#\n# This problem stems from two limitations of impurity-based feature\n# importances:\n#\n# - impurity-based importances are biased towards high cardinality features;\n# - impurity-based importances are computed on training set statistics and\n#   therefore do not reflect the ability of feature to be useful to make\n#   predictions that generalize to the test set (when the model has enough\n#   capacity).\nohe = (rf.named_steps[\'preprocess\']\n         .named_transformers_[\'cat\']\n         .named_steps[\'onehot\'])\nfeature_names = ohe.get_feature_names(input_features=categorical_columns)\nfeature_names = np.r_[feature_names, numerical_columns]\n\ntree_feature_importances = (\n    rf.named_steps[\'classifier\'].feature_importances_)\nsorted_idx = tree_feature_importances.argsort()\n\ny_ticks = np.arange(0, len(feature_names))\nfig, ax = plt.subplots()\nax.barh(y_ticks, tree_feature_importances[sorted_idx])\nax.set_yticklabels(feature_names[sorted_idx])\nax.set_yticks(y_ticks)\nax.set_title(""Random Forest Feature Importances (MDI)"")\nfig.tight_layout()\nplt.show()\n\n\n##############################################################################\n# As an alternative, the permutation importances of ``rf`` are computed on a\n# held out test set. This shows that the low cardinality categorical feature,\n# ``sex`` is the most important feature.\n#\n# Also note that both random features have very low importances (close to 0) as\n# expected.\nresult = permutation_importance(rf, X_test, y_test, n_repeats=10,\n                                random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_test.columns[sorted_idx])\nax.set_title(""Permutation Importances (test set)"")\nfig.tight_layout()\nplt.show()\n\n##############################################################################\n# It is also possible to compute the permutation importances on the training\n# set. This reveals that ``random_num`` gets a significantly higher importance\n# ranking than when computed on the test set. The difference between those two\n# plots is a confirmation that the RF model has enough capacity to use that\n# random numerical feature to overfit. You can further confirm this by\n# re-running this example with constrained RF with min_samples_leaf=10.\nresult = permutation_importance(rf, X_train, y_train, n_repeats=10,\n                                random_state=42, n_jobs=2)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_train.columns[sorted_idx])\nax.set_title(""Permutation Importances (train set)"")\nfig.tight_layout()\nplt.show()\n'"
scikit-learn/_downloads/fac50d1107ffe38aaba127928fd89d9f/plot_dbscan.py,2,"b'# -*- coding: utf-8 -*-\n""""""\n===================================\nDemo of DBSCAN clustering algorithm\n===================================\n\nFinds core samples of high density and expands clusters from them.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\n\n\n# #############################################################################\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n                            random_state=0)\n\nX = StandardScaler().fit_transform(X)\n\n# #############################################################################\n# Compute DBSCAN\ndb = DBSCAN(eps=0.3, min_samples=10).fit(X)\ncore_samples_mask = np.zeros_like(db.labels_, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\nlabels = db.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nn_noise_ = list(labels).count(-1)\n\nprint(\'Estimated number of clusters: %d\' % n_clusters_)\nprint(\'Estimated number of noise points: %d\' % n_noise_)\nprint(""Homogeneity: %0.3f"" % metrics.homogeneity_score(labels_true, labels))\nprint(""Completeness: %0.3f"" % metrics.completeness_score(labels_true, labels))\nprint(""V-measure: %0.3f"" % metrics.v_measure_score(labels_true, labels))\nprint(""Adjusted Rand Index: %0.3f""\n      % metrics.adjusted_rand_score(labels_true, labels))\nprint(""Adjusted Mutual Information: %0.3f""\n      % metrics.adjusted_mutual_info_score(labels_true, labels))\nprint(""Silhouette Coefficient: %0.3f""\n      % metrics.silhouette_score(X, labels))\n\n# #############################################################################\n# Plot result\nimport matplotlib.pyplot as plt\n\n# Black removed and is used for noise instead.\nunique_labels = set(labels)\ncolors = [plt.cm.Spectral(each)\n          for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], \'o\', markerfacecolor=tuple(col),\n             markeredgecolor=\'k\', markersize=14)\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], \'o\', markerfacecolor=tuple(col),\n             markeredgecolor=\'k\', markersize=6)\n\nplt.title(\'Estimated number of clusters: %d\' % n_clusters_)\nplt.show()\n'"
scikit-learn/_downloads/fb0e3849237642a7eeb316448b0e7576/plot_svm_nonlinear.py,6,"b'""""""\n==============\nNon-linear SVM\n==============\n\nPerform binary classification using non-linear SVC\nwith RBF kernel. The target to predict is a XOR of the\ninputs.\n\nThe color map illustrates the decision function learned by the SVC.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\nxx, yy = np.meshgrid(np.linspace(-3, 3, 500),\n                     np.linspace(-3, 3, 500))\nnp.random.seed(0)\nX = np.random.randn(300, 2)\nY = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\n\n# fit the model\nclf = svm.NuSVC(gamma=\'auto\')\nclf.fit(X, Y)\n\n# plot the decision function for each datapoint on the grid\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.imshow(Z, interpolation=\'nearest\',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()), aspect=\'auto\',\n           origin=\'lower\', cmap=plt.cm.PuOr_r)\ncontours = plt.contour(xx, yy, Z, levels=[0], linewidths=2,\n                       linestyles=\'dashed\')\nplt.scatter(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired,\n            edgecolors=\'k\')\nplt.xticks(())\nplt.yticks(())\nplt.axis([-3, 3, -3, 3])\nplt.show()\n'"
scikit-learn/_downloads/fc03cba5740cc0785ef77921295608b9/plot_oneclass.py,8,"b'""""""\n==========================================\nOne-class SVM with non-linear kernel (RBF)\n==========================================\n\nAn example using a one-class SVM for novelty detection.\n\n:ref:`One-class SVM <svm_outlier_detection>` is an unsupervised\nalgorithm that learns a decision function for novelty detection:\nclassifying new data as similar or different to the training set.\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager\nfrom sklearn import svm\n\nxx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n# Generate train data\nX = 0.3 * np.random.randn(100, 2)\nX_train = np.r_[X + 2, X - 2]\n# Generate some regular novel observations\nX = 0.3 * np.random.randn(20, 2)\nX_test = np.r_[X + 2, X - 2]\n# Generate some abnormal novel observations\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n\n# fit the model\nclf = svm.OneClassSVM(nu=0.1, kernel=""rbf"", gamma=0.1)\nclf.fit(X_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\nn_error_train = y_pred_train[y_pred_train == -1].size\nn_error_test = y_pred_test[y_pred_test == -1].size\nn_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n\n# plot the line, the points, and the nearest vectors to the plane\nZ = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title(""Novelty Detection"")\nplt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\na = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors=\'darkred\')\nplt.contourf(xx, yy, Z, levels=[0, Z.max()], colors=\'palevioletred\')\n\ns = 40\nb1 = plt.scatter(X_train[:, 0], X_train[:, 1], c=\'white\', s=s, edgecolors=\'k\')\nb2 = plt.scatter(X_test[:, 0], X_test[:, 1], c=\'blueviolet\', s=s,\n                 edgecolors=\'k\')\nc = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=\'gold\', s=s,\n                edgecolors=\'k\')\nplt.axis(\'tight\')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend([a.collections[0], b1, b2, c],\n           [""learned frontier"", ""training observations"",\n            ""new regular observations"", ""new abnormal observations""],\n           loc=""upper left"",\n           prop=matplotlib.font_manager.FontProperties(size=11))\nplt.xlabel(\n    ""error train: %d/200 ; errors novel regular: %d/40 ; ""\n    ""errors novel abnormal: %d/40""\n    % (n_error_train, n_error_test, n_error_outliers))\nplt.show()\n'"
scikit-learn/_downloads/fd635a686d0a1ee3f629dcfe4c67ab23/plot_function_transformer.py,6,"b'""""""\n=========================================================\nUsing FunctionTransformer to select columns\n=========================================================\n\nShows how to use a function transformer in a pipeline. If you know your\ndataset\'s first principle component is irrelevant for a classification task,\nyou can use the FunctionTransformer to select all but the first column of the\nPCA transformed data.\n""""""\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\n\ndef _generate_vector(shift=0.5, noise=15):\n    return np.arange(1000) + (np.random.rand(1000) - shift) * noise\n\n\ndef generate_dataset():\n    """"""\n    This dataset is two lines with a slope ~ 1, where one has\n    a y offset of ~100\n    """"""\n    return np.vstack((\n        np.vstack((\n            _generate_vector(),\n            _generate_vector() + 100,\n        )).T,\n        np.vstack((\n            _generate_vector(),\n            _generate_vector(),\n        )).T,\n    )), np.hstack((np.zeros(1000), np.ones(1000)))\n\n\ndef all_but_first_column(X):\n    return X[:, 1:]\n\n\ndef drop_first_component(X, y):\n    """"""\n    Create a pipeline with PCA and the column selector and use it to\n    transform the dataset.\n    """"""\n    pipeline = make_pipeline(\n        PCA(), FunctionTransformer(all_but_first_column),\n    )\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    pipeline.fit(X_train, y_train)\n    return pipeline.transform(X_test), y_test\n\n\nif __name__ == \'__main__\':\n    X, y = generate_dataset()\n    lw = 0\n    plt.figure()\n    plt.scatter(X[:, 0], X[:, 1], c=y, lw=lw)\n    plt.figure()\n    X_transformed, y_transformed = drop_first_component(*generate_dataset())\n    plt.scatter(\n        X_transformed[:, 0],\n        np.zeros(len(X_transformed)),\n        c=y_transformed,\n        lw=lw,\n        s=60\n    )\n    plt.show()\n'"
scikit-learn/_downloads/fe42e482acd6648d497b6b19f0193660/plot_covariance_estimation.py,12,"b'""""""\n=======================================================================\nShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood\n=======================================================================\n\nWhen working with covariance estimation, the usual approach is to use\na maximum likelihood estimator, such as the\n:class:`sklearn.covariance.EmpiricalCovariance`. It is unbiased, i.e. it\nconverges to the true (population) covariance when given many\nobservations. However, it can also be beneficial to regularize it, in\norder to reduce its variance; this, in turn, introduces some bias. This\nexample illustrates the simple regularization used in\n:ref:`shrunk_covariance` estimators. In particular, it focuses on how to\nset the amount of regularization, i.e. how to choose the bias-variance\ntrade-off.\n\nHere we compare 3 approaches:\n\n* Setting the parameter by cross-validating the likelihood on three folds\n  according to a grid of potential shrinkage parameters.\n\n* A close formula proposed by Ledoit and Wolf to compute\n  the asymptotically optimal regularization parameter (minimizing a MSE\n  criterion), yielding the :class:`sklearn.covariance.LedoitWolf`\n  covariance estimate.\n\n* An improvement of the Ledoit-Wolf shrinkage, the\n  :class:`sklearn.covariance.OAS`, proposed by Chen et al. Its\n  convergence is significantly better under the assumption that the data\n  are Gaussian, in particular for small samples.\n\nTo quantify estimation error, we plot the likelihood of unseen data for\ndifferent values of the shrinkage parameter. We also show the choices by\ncross-validation, or with the LedoitWolf and OAS estimates.\n\nNote that the maximum likelihood estimate corresponds to no shrinkage,\nand thus performs poorly. The Ledoit-Wolf estimate performs really well,\nas it is close to the optimal and is computational not costly. In this\nexample, the OAS estimate is a bit further away. Interestingly, both\napproaches outperform cross-validation, which is significantly most\ncomputationally costly.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import linalg\n\nfrom sklearn.covariance import LedoitWolf, OAS, ShrunkCovariance, \\\n    log_likelihood, empirical_covariance\nfrom sklearn.model_selection import GridSearchCV\n\n\n# #############################################################################\n# Generate sample data\nn_features, n_samples = 40, 20\nnp.random.seed(42)\nbase_X_train = np.random.normal(size=(n_samples, n_features))\nbase_X_test = np.random.normal(size=(n_samples, n_features))\n\n# Color samples\ncoloring_matrix = np.random.normal(size=(n_features, n_features))\nX_train = np.dot(base_X_train, coloring_matrix)\nX_test = np.dot(base_X_test, coloring_matrix)\n\n# #############################################################################\n# Compute the likelihood on test data\n\n# spanning a range of possible shrinkage coefficient values\nshrinkages = np.logspace(-2, 0, 30)\nnegative_logliks = [-ShrunkCovariance(shrinkage=s).fit(X_train).score(X_test)\n                    for s in shrinkages]\n\n# under the ground-truth model, which we would not have access to in real\n# settings\nreal_cov = np.dot(coloring_matrix.T, coloring_matrix)\nemp_cov = empirical_covariance(X_train)\nloglik_real = -log_likelihood(emp_cov, linalg.inv(real_cov))\n\n# #############################################################################\n# Compare different approaches to setting the parameter\n\n# GridSearch for an optimal shrinkage coefficient\ntuned_parameters = [{\'shrinkage\': shrinkages}]\ncv = GridSearchCV(ShrunkCovariance(), tuned_parameters)\ncv.fit(X_train)\n\n# Ledoit-Wolf optimal shrinkage coefficient estimate\nlw = LedoitWolf()\nloglik_lw = lw.fit(X_train).score(X_test)\n\n# OAS coefficient estimate\noa = OAS()\nloglik_oa = oa.fit(X_train).score(X_test)\n\n# #############################################################################\n# Plot results\nfig = plt.figure()\nplt.title(""Regularized covariance: likelihood and shrinkage coefficient"")\nplt.xlabel(\'Regularization parameter: shrinkage coefficient\')\nplt.ylabel(\'Error: negative log-likelihood on test data\')\n# range shrinkage curve\nplt.loglog(shrinkages, negative_logliks, label=""Negative log-likelihood"")\n\nplt.plot(plt.xlim(), 2 * [loglik_real], \'--r\',\n         label=""Real covariance likelihood"")\n\n# adjust view\nlik_max = np.amax(negative_logliks)\nlik_min = np.amin(negative_logliks)\nymin = lik_min - 6. * np.log((plt.ylim()[1] - plt.ylim()[0]))\nymax = lik_max + 10. * np.log(lik_max - lik_min)\nxmin = shrinkages[0]\nxmax = shrinkages[-1]\n# LW likelihood\nplt.vlines(lw.shrinkage_, ymin, -loglik_lw, color=\'magenta\',\n           linewidth=3, label=\'Ledoit-Wolf estimate\')\n# OAS likelihood\nplt.vlines(oa.shrinkage_, ymin, -loglik_oa, color=\'purple\',\n           linewidth=3, label=\'OAS estimate\')\n# best CV estimator likelihood\nplt.vlines(cv.best_estimator_.shrinkage, ymin,\n           -cv.best_estimator_.score(X_test), color=\'cyan\',\n           linewidth=3, label=\'Cross-validation best estimate\')\n\nplt.ylim(ymin, ymax)\nplt.xlim(xmin, xmax)\nplt.legend()\n\nplt.show()\n'"
scikit-learn/_downloads/ff976b7d92c6d5a06966992caa97bf21/plot_sgd_iris.py,7,"b'""""""\n========================================\nPlot multi-class SGD on the iris dataset\n========================================\n\nPlot decision surface of multi-class SGD on iris dataset.\nThe hyperplanes corresponding to the three one-versus-all (OVA) classifiers\nare represented by the dashed lines.\n\n""""""\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.linear_model import SGDClassifier\n\n# import some data to play with\niris = datasets.load_iris()\n\n# we only take the first two features. We could\n# avoid this ugly slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\ncolors = ""bry""\n\n# shuffle\nidx = np.arange(X.shape[0])\nnp.random.seed(13)\nnp.random.shuffle(idx)\nX = X[idx]\ny = y[idx]\n\n# standardize\nmean = X.mean(axis=0)\nstd = X.std(axis=0)\nX = (X - mean) / std\n\nh = .02  # step size in the mesh\n\nclf = SGDClassifier(alpha=0.001, max_iter=100).fit(X, y)\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.axis(\'tight\')\n\n# Plot also the training points\nfor i, color in zip(clf.classes_, colors):\n    idx = np.where(y == i)\n    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                cmap=plt.cm.Paired, edgecolor=\'black\', s=20)\nplt.title(""Decision surface of multi-class SGD"")\nplt.axis(\'tight\')\n\n# Plot the three one-against-all classifiers\nxmin, xmax = plt.xlim()\nymin, ymax = plt.ylim()\ncoef = clf.coef_\nintercept = clf.intercept_\n\n\ndef plot_hyperplane(c, color):\n    def line(x0):\n        return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n\n    plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n             ls=""--"", color=color)\n\n\nfor i, color in zip(clf.classes_, colors):\n    plot_hyperplane(i, color)\nplt.legend()\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-Generator-dirichlet-1.py,1,"b'# Taking an example cited in Wikipedia, this distribution can be used if\n# one wanted to cut strings (each of initial length 1.0) into K pieces\n# with different lengths, where each piece had, on average, a designated\n# average length, but allowing some variation in the relative sizes of\n# the pieces.\n\ns = np.random.default_rng().dirichlet((10, 5, 3), 20).transpose()\n\nimport matplotlib.pyplot as plt\nplt.barh(range(20), s[0])\nplt.barh(range(20), s[1], left=s[0], color=\'g\')\nplt.barh(range(20), s[2], left=s[0]+s[1], color=\'r\')\nplt.title(""Lengths of Strings"")\n'"
numpy/reference/random/generated/numpy-random-Generator-gamma-1.py,2,"b""# Draw samples from the distribution:\n\nshape, scale = 2., 2.  # mean=4, std=2*sqrt(2)\ns = np.random.default_rng().gamma(shape, scale, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nimport scipy.special as sps  # doctest: +SKIP\ncount, bins, ignored = plt.hist(s, 50, density=True)\ny = bins**(shape-1)*(np.exp(-bins/scale) /  # doctest: +SKIP\n                     (sps.gamma(shape)*scale**shape))\nplt.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-gumbel-1.py,9,"b""# Draw samples from the distribution:\n\nrng = np.random.default_rng()\nmu, beta = 0, 0.1 # location and scale\ns = rng.gumbel(mu, beta, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)\n         * np.exp( -np.exp( -(bins - mu) /beta) ),\n         linewidth=2, color='r')\nplt.show()\n\n# Show how an extreme value distribution can arise from a Gaussian process\n# and compare to a Gaussian:\n\nmeans = []\nmaxima = []\nfor i in range(0,1000) :\n   a = rng.normal(mu, beta, 1000)\n   means.append(a.mean())\n   maxima.append(a.max())\ncount, bins, ignored = plt.hist(maxima, 30, density=True)\nbeta = np.std(maxima) * np.sqrt(6) / np.pi\nmu = np.mean(maxima) - 0.57721*beta\nplt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)\n         * np.exp(-np.exp(-(bins - mu)/beta)),\n         linewidth=2, color='r')\nplt.plot(bins, 1/(beta * np.sqrt(2 * np.pi))\n         * np.exp(-(bins - mu)**2 / (2 * beta**2)),\n         linewidth=2, color='g')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-laplace-1.py,5,"b'# Draw samples from the distribution\n\nloc, scale = 0., 1.\ns = np.random.default_rng().laplace(loc, scale, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nx = np.arange(-8., 8., .01)\npdf = np.exp(-abs(x-loc)/scale)/(2.*scale)\nplt.plot(x, pdf)\n\n# Plot Gaussian for comparison:\n\ng = (1/(scale * np.sqrt(2 * np.pi)) *\n     np.exp(-(x - loc)**2 / (2 * scale**2)))\nplt.plot(x,g)\n'"
numpy/reference/random/generated/numpy-random-Generator-logistic-1.py,2,"b'# Draw samples from the distribution:\n\nloc, scale = 10, 1\ns = np.random.default_rng().logistic(loc, scale, 10000)\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, bins=50)\n\n# #   plot against distribution\n\ndef logist(x, loc, scale):\n    return np.exp((loc-x)/scale)/(scale*(1+np.exp((loc-x)/scale))**2)\nlgst_val = logist(bins, loc, scale)\nplt.plot(bins, lgst_val * count.max() / lgst_val.max())\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-Generator-lognormal-1.py,11,"b""# Draw samples from the distribution:\n\nrng = np.random.default_rng()\nmu, sigma = 3., 1. # mean and standard deviation\ns = rng.lognormal(mu, sigma, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 100, density=True, align='mid')\n\nx = np.linspace(min(bins), max(bins), 10000)\npdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n       / (x * sigma * np.sqrt(2 * np.pi)))\n\nplt.plot(x, pdf, linewidth=2, color='r')\nplt.axis('tight')\nplt.show()\n\n# Demonstrate that taking the products of random samples from a uniform\n# distribution can be fit well by a log-normal probability density\n# function.\n\n# Generate a thousand samples: each is the product of 100 random\n# values, drawn from a normal distribution.\nrng = rng\nb = []\nfor i in range(1000):\n   a = 10. + rng.standard_normal(100)\n   b.append(np.product(a))\n\nb = np.array(b) / np.min(b) # scale values to be positive\ncount, bins, ignored = plt.hist(b, 100, density=True, align='mid')\nsigma = np.std(np.log(b))\nmu = np.mean(np.log(b))\n\nx = np.linspace(min(bins), max(bins), 10000)\npdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n       / (x * sigma * np.sqrt(2 * np.pi)))\n\nplt.plot(x, pdf, color='r', linewidth=2)\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-logseries-1.py,2,"b""# Draw samples from the distribution:\n\na = .6\ns = np.random.default_rng().logseries(a, 10000)\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s)\n\n# #   plot against distribution\n\ndef logseries(k, p):\n    return -p**k/(k*np.log(1-p))\nplt.plot(bins, logseries(bins, a) * count.max()/\n         logseries(bins, a).max(), 'r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-noncentral_chisquare-1.py,3,"b""# Draw values from the distribution and plot the histogram\n\nrng = np.random.default_rng()\nimport matplotlib.pyplot as plt\nvalues = plt.hist(rng.noncentral_chisquare(3, 20, 100000),\n                  bins=200, density=True)\nplt.show()\n\n# Draw values from a noncentral chisquare with very small noncentrality,\n# and compare to a chisquare.\n\nplt.figure()\nvalues = plt.hist(rng.noncentral_chisquare(3, .0000001, 100000),\n                  bins=np.arange(0., 25, .1), density=True)\nvalues2 = plt.hist(rng.chisquare(3, 100000),\n                   bins=np.arange(0., 25, .1), density=True)\nplt.plot(values[1][0:-1], values[0]-values2[0], 'ob')\nplt.show()\n\n# Demonstrate how large values of non-centrality lead to a more symmetric\n# distribution.\n\nplt.figure()\nvalues = plt.hist(rng.noncentral_chisquare(3, 20, 100000),\n                  bins=200, density=True)\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-noncentral_f-1.py,3,"b""# In a study, testing for a specific alternative to the null hypothesis\n# requires use of the Noncentral F distribution. We need to calculate the\n# area in the tail of the distribution that exceeds the value of the F\n# distribution for the null hypothesis.  We'll plot the two probability\n# distributions for comparison.\n\nrng = np.random.default_rng()\ndfnum = 3 # between group deg of freedom\ndfden = 20 # within groups degrees of freedom\nnonc = 3.0\nnc_vals = rng.noncentral_f(dfnum, dfden, nonc, 1000000)\nNF = np.histogram(nc_vals, bins=50, density=True)\nc_vals = rng.f(dfnum, dfden, 1000000)\nF = np.histogram(c_vals, bins=50, density=True)\nimport matplotlib.pyplot as plt\nplt.plot(F[1][1:], F[0])\nplt.plot(NF[1][1:], NF[0])\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-normal-1.py,6,"b""# Draw samples from the distribution:\n\nmu, sigma = 0, 0.1 # mean and standard deviation\ns = np.random.default_rng().normal(mu, sigma, 1000)\n\n# Verify the mean and the variance:\n\nabs(mu - np.mean(s))\n# 0.0  # may vary\n\nabs(sigma - np.std(s, ddof=1))\n# 0.1  # may vary\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n         linewidth=2, color='r')\nplt.show()\n\n# Two-by-four array of samples from N(3, 6.25):\n\nnp.random.default_rng().normal(3, 2.5, size=(2, 4))\n# array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n# [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n"""
numpy/reference/random/generated/numpy-random-Generator-pareto-1.py,1,"b""# Draw samples from the distribution:\n\na, m = 3., 2.  # shape and mode\ns = (np.random.default_rng().pareto(a, 1000) + 1) * m\n\n# Display the histogram of the samples, along with the probability\n# density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, _ = plt.hist(s, 100, density=True)\nfit = a*m**a / bins**(a+1)\nplt.plot(bins, max(count)*fit/max(fit), linewidth=2, color='r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-poisson-1.py,1,"b'# Draw samples from the distribution:\n\nimport numpy as np\nrng = np.random.default_rng()\ns = rng.poisson(5, 10000)\n\n# Display histogram of the sample:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 14, density=True)\nplt.show()\n\n# Draw each 100 values for lambda 100 and 500:\n\ns = rng.poisson(lam=(100., 500.), size=(100, 2))\n'"
numpy/reference/random/generated/numpy-random-Generator-power-1.py,4,"b""# Draw samples from the distribution:\n\nrng = np.random.default_rng()\na = 5. # shape\nsamples = 1000\ns = rng.power(a, samples)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, bins=30)\nx = np.linspace(0, 1, 100)\ny = a*x**(a-1.)\nnormed_y = samples*np.diff(bins)[0]*y\nplt.plot(x, normed_y)\nplt.show()\n\n# Compare the power function distribution to the inverse of the Pareto.\n\nfrom scipy import stats  # doctest: +SKIP\nrvs = rng.power(5, 1000000)\nrvsp = rng.pareto(5, 1000000)\nxx = np.linspace(0,1,100)\npowpdf = stats.powerlaw.pdf(xx,5)  # doctest: +SKIP\n\nplt.figure()\nplt.hist(rvs, bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('power(5)')\n\nplt.figure()\nplt.hist(1./(1.+rvsp), bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('inverse of 1 + Generator.pareto(5)')\n\nplt.figure()\nplt.hist(1./(1.+rvsp), bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('inverse of stats.pareto(5)')\n"""
numpy/reference/random/generated/numpy-random-Generator-standard_cauchy-1.py,1,"b'# Draw samples and plot the distribution:\n\nimport matplotlib.pyplot as plt\ns = np.random.default_rng().standard_cauchy(1000000)\ns = s[(s>-25) & (s<25)]  # truncate distribution so it plots well\nplt.hist(s, bins=100)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-Generator-standard_gamma-1.py,2,"b""# Draw samples from the distribution:\n\nshape, scale = 2., 1. # mean and width\ns = np.random.default_rng().standard_gamma(shape, 1000000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nimport scipy.special as sps  # doctest: +SKIP\ncount, bins, ignored = plt.hist(s, 50, density=True)\ny = bins**(shape-1) * ((np.exp(-bins/scale))/  # doctest: +SKIP\n                      (sps.gamma(shape) * scale**shape))\nplt.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-standard_t-1.py,5,"b'# From Dalgaard page 83 [Rb7c952f3992e-1]_, suppose the daily energy intake for 11\n# women in kilojoules (kJ) is:\n\nintake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \\\n                   7515, 8230, 8770])\n\n# Does their energy intake deviate systematically from the recommended\n# value of 7725 kJ?\n\n# We have 10 degrees of freedom, so is the sample mean within 95% of the\n# recommended value?\n\ns = np.random.default_rng().standard_t(10, size=100000)\nnp.mean(intake)\n# 6753.636363636364\nintake.std(ddof=1)\n# 1142.1232221373727\n\n# Calculate the t statistic, setting the ddof parameter to the unbiased\n# value so the divisor in the standard deviation will be degrees of\n# freedom, N-1.\n\nt = (np.mean(intake)-7725)/(intake.std(ddof=1)/np.sqrt(len(intake)))\nimport matplotlib.pyplot as plt\nh = plt.hist(s, bins=100, density=True)\n\n# For a one-sided t-test, how far out in the distribution does the t\n# statistic appear?\n\nnp.sum(s<t) / float(len(s))\n# 0.0090699999999999999  #random\n\n# So the p-value is about 0.009, which says the null hypothesis has a\n# probability of about 99% of being true.\n'"
numpy/reference/random/generated/numpy-random-Generator-triangular-1.py,1,"b'# Draw values from the distribution and plot the histogram:\n\nimport matplotlib.pyplot as plt\nh = plt.hist(np.random.default_rng().triangular(-3, 0, 8, 100000), bins=200,\n             density=True)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-Generator-uniform-1.py,4,"b""# Draw samples from the distribution:\n\ns = np.random.default_rng().uniform(-1,0,1000)\n\n# All values are within the given interval:\n\nnp.all(s >= -1)\n# True\nnp.all(s < 0)\n# True\n\n# Display the histogram of the samples, along with the\n# probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 15, density=True)\nplt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-vonmises-1.py,3,"b""# Draw samples from the distribution:\n\nmu, kappa = 0.0, 4.0 # mean and dispersion\ns = np.random.default_rng().vonmises(mu, kappa, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import i0  # doctest: +SKIP\nplt.hist(s, 50, density=True)\nx = np.linspace(-np.pi, np.pi, num=51)\ny = np.exp(kappa*np.cos(x-mu))/(2*np.pi*i0(kappa))  # doctest: +SKIP\nplt.plot(x, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-Generator-wald-1.py,1,"b'# Draw values from the distribution and plot the histogram:\n\nimport matplotlib.pyplot as plt\nh = plt.hist(np.random.default_rng().wald(3, 2, 100000), bins=200, density=True)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-Generator-weibull-1.py,4,"b'# Draw samples from the distribution:\n\nrng = np.random.default_rng()\na = 5. # shape\ns = rng.weibull(a, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nx = np.arange(1,100.)/50.\ndef weib(x,n,a):\n    return (a / n) * (x / n)**(a - 1) * np.exp(-(x / n)**a)\n\ncount, bins, ignored = plt.hist(rng.weibull(5.,1000))\nx = np.arange(1,100.)/50.\nscale = count.max()/weib(x, 1., 5.).max()\nplt.plot(x, weib(x, 1., 5.)*scale)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-Generator-zipf-1.py,2,"b""# Draw samples from the distribution:\n\na = 2. # parameter\ns = np.random.default_rng().zipf(a, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nfrom scipy import special  # doctest: +SKIP\n\n# Truncate s values at 50 so plot is interesting:\n\ncount, bins, ignored = plt.hist(s[s<50],\n        50, density=True)\nx = np.arange(1., 50.)\ny = x**(-a) / special.zetac(a)  # doctest: +SKIP\nplt.plot(x, y/max(y), linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-dirichlet-1.py,1,"b'# Taking an example cited in Wikipedia, this distribution can be used if\n# one wanted to cut strings (each of initial length 1.0) into K pieces\n# with different lengths, where each piece had, on average, a designated\n# average length, but allowing some variation in the relative sizes of\n# the pieces.\n\ns = np.random.dirichlet((10, 5, 3), 20).transpose()\n\nimport matplotlib.pyplot as plt\nplt.barh(range(20), s[0])\nplt.barh(range(20), s[1], left=s[0], color=\'g\')\nplt.barh(range(20), s[2], left=s[0]+s[1], color=\'r\')\nplt.title(""Lengths of Strings"")\n'"
numpy/reference/random/generated/numpy-random-RandomState-gamma-1.py,2,"b""# Draw samples from the distribution:\n\nshape, scale = 2., 2.  # mean=4, std=2*sqrt(2)\ns = np.random.gamma(shape, scale, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nimport scipy.special as sps  # doctest: +SKIP\ncount, bins, ignored = plt.hist(s, 50, density=True)\ny = bins**(shape-1)*(np.exp(-bins/scale) /  # doctest: +SKIP\n                     (sps.gamma(shape)*scale**shape))\nplt.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-gumbel-1.py,10,"b""# Draw samples from the distribution:\n\nmu, beta = 0, 0.1 # location and scale\ns = np.random.gumbel(mu, beta, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)\n         * np.exp( -np.exp( -(bins - mu) /beta) ),\n         linewidth=2, color='r')\nplt.show()\n\n# Show how an extreme value distribution can arise from a Gaussian process\n# and compare to a Gaussian:\n\nmeans = []\nmaxima = []\nfor i in range(0,1000) :\n   a = np.random.normal(mu, beta, 1000)\n   means.append(a.mean())\n   maxima.append(a.max())\ncount, bins, ignored = plt.hist(maxima, 30, density=True)\nbeta = np.std(maxima) * np.sqrt(6) / np.pi\nmu = np.mean(maxima) - 0.57721*beta\nplt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)\n         * np.exp(-np.exp(-(bins - mu)/beta)),\n         linewidth=2, color='r')\nplt.plot(bins, 1/(beta * np.sqrt(2 * np.pi))\n         * np.exp(-(bins - mu)**2 / (2 * beta**2)),\n         linewidth=2, color='g')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-laplace-1.py,5,"b'# Draw samples from the distribution\n\nloc, scale = 0., 1.\ns = np.random.laplace(loc, scale, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nx = np.arange(-8., 8., .01)\npdf = np.exp(-abs(x-loc)/scale)/(2.*scale)\nplt.plot(x, pdf)\n\n# Plot Gaussian for comparison:\n\ng = (1/(scale * np.sqrt(2 * np.pi)) *\n     np.exp(-(x - loc)**2 / (2 * scale**2)))\nplt.plot(x,g)\n'"
numpy/reference/random/generated/numpy-random-RandomState-logistic-1.py,2,"b'# Draw samples from the distribution:\n\nloc, scale = 10, 1\ns = np.random.logistic(loc, scale, 10000)\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, bins=50)\n\n# #   plot against distribution\n\ndef logist(x, loc, scale):\n    return np.exp((loc-x)/scale)/(scale*(1+np.exp((loc-x)/scale))**2)\nlgst_val = logist(bins, loc, scale)\nplt.plot(bins, lgst_val * count.max() / lgst_val.max())\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-RandomState-lognormal-1.py,12,"b""# Draw samples from the distribution:\n\nmu, sigma = 3., 1. # mean and standard deviation\ns = np.random.lognormal(mu, sigma, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 100, density=True, align='mid')\n\nx = np.linspace(min(bins), max(bins), 10000)\npdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n       / (x * sigma * np.sqrt(2 * np.pi)))\n\nplt.plot(x, pdf, linewidth=2, color='r')\nplt.axis('tight')\nplt.show()\n\n# Demonstrate that taking the products of random samples from a uniform\n# distribution can be fit well by a log-normal probability density\n# function.\n\n# Generate a thousand samples: each is the product of 100 random\n# values, drawn from a normal distribution.\nb = []\nfor i in range(1000):\n   a = 10. + np.random.standard_normal(100)\n   b.append(np.product(a))\n\nb = np.array(b) / np.min(b) # scale values to be positive\ncount, bins, ignored = plt.hist(b, 100, density=True, align='mid')\nsigma = np.std(np.log(b))\nmu = np.mean(np.log(b))\n\nx = np.linspace(min(bins), max(bins), 10000)\npdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n       / (x * sigma * np.sqrt(2 * np.pi)))\n\nplt.plot(x, pdf, color='r', linewidth=2)\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-logseries-1.py,2,"b""# Draw samples from the distribution:\n\na = .6\ns = np.random.logseries(a, 10000)\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s)\n\n# #   plot against distribution\n\ndef logseries(k, p):\n    return -p**k/(k*np.log(1-p))\nplt.plot(bins, logseries(bins, a)*count.max()/\n         logseries(bins, a).max(), 'r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-noncentral_chisquare-1.py,6,"b""# Draw values from the distribution and plot the histogram\n\nimport matplotlib.pyplot as plt\nvalues = plt.hist(np.random.noncentral_chisquare(3, 20, 100000),\n                  bins=200, density=True)\nplt.show()\n\n# Draw values from a noncentral chisquare with very small noncentrality,\n# and compare to a chisquare.\n\nplt.figure()\nvalues = plt.hist(np.random.noncentral_chisquare(3, .0000001, 100000),\n                  bins=np.arange(0., 25, .1), density=True)\nvalues2 = plt.hist(np.random.chisquare(3, 100000),\n                   bins=np.arange(0., 25, .1), density=True)\nplt.plot(values[1][0:-1], values[0]-values2[0], 'ob')\nplt.show()\n\n# Demonstrate how large values of non-centrality lead to a more symmetric\n# distribution.\n\nplt.figure()\nvalues = plt.hist(np.random.noncentral_chisquare(3, 20, 100000),\n                  bins=200, density=True)\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-noncentral_f-1.py,4,"b""# In a study, testing for a specific alternative to the null hypothesis\n# requires use of the Noncentral F distribution. We need to calculate the\n# area in the tail of the distribution that exceeds the value of the F\n# distribution for the null hypothesis.  We'll plot the two probability\n# distributions for comparison.\n\ndfnum = 3 # between group deg of freedom\ndfden = 20 # within groups degrees of freedom\nnonc = 3.0\nnc_vals = np.random.noncentral_f(dfnum, dfden, nonc, 1000000)\nNF = np.histogram(nc_vals, bins=50, density=True)\nc_vals = np.random.f(dfnum, dfden, 1000000)\nF = np.histogram(c_vals, bins=50, density=True)\nimport matplotlib.pyplot as plt\nplt.plot(F[1][1:], F[0])\nplt.plot(NF[1][1:], NF[0])\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-normal-1.py,6,"b""# Draw samples from the distribution:\n\nmu, sigma = 0, 0.1 # mean and standard deviation\ns = np.random.normal(mu, sigma, 1000)\n\n# Verify the mean and the variance:\n\nabs(mu - np.mean(s))\n# 0.0  # may vary\n\nabs(sigma - np.std(s, ddof=1))\n# 0.1  # may vary\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n         linewidth=2, color='r')\nplt.show()\n\n# Two-by-four array of samples from N(3, 6.25):\n\nnp.random.normal(3, 2.5, size=(2, 4))\n# array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n# [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n"""
numpy/reference/random/generated/numpy-random-RandomState-pareto-1.py,1,"b""# Draw samples from the distribution:\n\na, m = 3., 2.  # shape and mode\ns = (np.random.pareto(a, 1000) + 1) * m\n\n# Display the histogram of the samples, along with the probability\n# density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, _ = plt.hist(s, 100, density=True)\nfit = a*m**a / bins**(a+1)\nplt.plot(bins, max(count)*fit/max(fit), linewidth=2, color='r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-poisson-1.py,2,"b'# Draw samples from the distribution:\n\nimport numpy as np\ns = np.random.poisson(5, 10000)\n\n# Display histogram of the sample:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 14, density=True)\nplt.show()\n\n# Draw each 100 values for lambda 100 and 500:\n\ns = np.random.poisson(lam=(100., 500.), size=(100, 2))\n'"
numpy/reference/random/generated/numpy-random-RandomState-power-1.py,8,"b""# Draw samples from the distribution:\n\na = 5. # shape\nsamples = 1000\ns = np.random.power(a, samples)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, bins=30)\nx = np.linspace(0, 1, 100)\ny = a*x**(a-1.)\nnormed_y = samples*np.diff(bins)[0]*y\nplt.plot(x, normed_y)\nplt.show()\n\n# Compare the power function distribution to the inverse of the Pareto.\n\nfrom scipy import stats # doctest: +SKIP\nrvs = np.random.power(5, 1000000)\nrvsp = np.random.pareto(5, 1000000)\nxx = np.linspace(0,1,100)\npowpdf = stats.powerlaw.pdf(xx,5)  # doctest: +SKIP\n\nplt.figure()\nplt.hist(rvs, bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('np.random.power(5)')\n\nplt.figure()\nplt.hist(1./(1.+rvsp), bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('inverse of 1 + np.random.pareto(5)')\n\nplt.figure()\nplt.hist(1./(1.+rvsp), bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('inverse of stats.pareto(5)')\n"""
numpy/reference/random/generated/numpy-random-RandomState-random_integers-1.py,6,"b""np.random.random_integers(5)\n# 4 # random\ntype(np.random.random_integers(5))\n# <class 'numpy.int64'>\nnp.random.random_integers(5, size=(3,2))\n# array([[5, 4], # random\n# [3, 3],\n# [4, 5]])\n\n# Choose five random numbers from the set of five evenly-spaced\n# numbers between 0 and 2.5, inclusive (*i.e.*, from the set\n# :math:`{0, 5/8, 10/8, 15/8, 20/8}`):\n\n2.5 * (np.random.random_integers(5, size=(5,)) - 1) / 4.\n# array([ 0.625,  1.25 ,  0.625,  0.625,  2.5  ]) # random\n\n# Roll two six sided dice 1000 times and sum the results:\n\nd1 = np.random.random_integers(1, 6, 1000)\nd2 = np.random.random_integers(1, 6, 1000)\ndsums = d1 + d2\n\n# Display results as a histogram:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(dsums, 11, density=True)\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-standard_cauchy-1.py,1,"b'# Draw samples and plot the distribution:\n\nimport matplotlib.pyplot as plt\ns = np.random.standard_cauchy(1000000)\ns = s[(s>-25) & (s<25)]  # truncate distribution so it plots well\nplt.hist(s, bins=100)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-RandomState-standard_gamma-1.py,2,"b""# Draw samples from the distribution:\n\nshape, scale = 2., 1. # mean and width\ns = np.random.standard_gamma(shape, 1000000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nimport scipy.special as sps  # doctest: +SKIP\ncount, bins, ignored = plt.hist(s, 50, density=True)\ny = bins**(shape-1) * ((np.exp(-bins/scale))/  # doctest: +SKIP\n                      (sps.gamma(shape) * scale**shape))\nplt.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-standard_t-1.py,5,"b'# From Dalgaard page 83 [R89f5270d198b-1]_, suppose the daily energy intake for 11\n# women in kilojoules (kJ) is:\n\nintake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \\\n                   7515, 8230, 8770])\n\n# Does their energy intake deviate systematically from the recommended\n# value of 7725 kJ?\n\n# We have 10 degrees of freedom, so is the sample mean within 95% of the\n# recommended value?\n\ns = np.random.standard_t(10, size=100000)\nnp.mean(intake)\n# 6753.636363636364\nintake.std(ddof=1)\n# 1142.1232221373727\n\n# Calculate the t statistic, setting the ddof parameter to the unbiased\n# value so the divisor in the standard deviation will be degrees of\n# freedom, N-1.\n\nt = (np.mean(intake)-7725)/(intake.std(ddof=1)/np.sqrt(len(intake)))\nimport matplotlib.pyplot as plt\nh = plt.hist(s, bins=100, density=True)\n\n# For a one-sided t-test, how far out in the distribution does the t\n# statistic appear?\n\nnp.sum(s<t) / float(len(s))\n# 0.0090699999999999999  #random\n\n# So the p-value is about 0.009, which says the null hypothesis has a\n# probability of about 99% of being true.\n'"
numpy/reference/random/generated/numpy-random-RandomState-triangular-1.py,1,"b'# Draw values from the distribution and plot the histogram:\n\nimport matplotlib.pyplot as plt\nh = plt.hist(np.random.triangular(-3, 0, 8, 100000), bins=200,\n             density=True)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-RandomState-uniform-1.py,4,"b""# Draw samples from the distribution:\n\ns = np.random.uniform(-1,0,1000)\n\n# All values are within the given interval:\n\nnp.all(s >= -1)\n# True\nnp.all(s < 0)\n# True\n\n# Display the histogram of the samples, along with the\n# probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 15, density=True)\nplt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-vonmises-1.py,3,"b""# Draw samples from the distribution:\n\nmu, kappa = 0.0, 4.0 # mean and dispersion\ns = np.random.vonmises(mu, kappa, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import i0  # doctest: +SKIP\nplt.hist(s, 50, density=True)\nx = np.linspace(-np.pi, np.pi, num=51)\ny = np.exp(kappa*np.cos(x-mu))/(2*np.pi*i0(kappa))  # doctest: +SKIP\nplt.plot(x, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-RandomState-wald-1.py,1,"b'# Draw values from the distribution and plot the histogram:\n\nimport matplotlib.pyplot as plt\nh = plt.hist(np.random.wald(3, 2, 100000), bins=200, density=True)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-RandomState-weibull-1.py,5,"b'# Draw samples from the distribution:\n\na = 5. # shape\ns = np.random.weibull(a, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nx = np.arange(1,100.)/50.\ndef weib(x,n,a):\n    return (a / n) * (x / n)**(a - 1) * np.exp(-(x / n)**a)\n\ncount, bins, ignored = plt.hist(np.random.weibull(5.,1000))\nx = np.arange(1,100.)/50.\nscale = count.max()/weib(x, 1., 5.).max()\nplt.plot(x, weib(x, 1., 5.)*scale)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-RandomState-zipf-1.py,2,"b""# Draw samples from the distribution:\n\na = 2. # parameter\ns = np.random.zipf(a, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nfrom scipy import special  # doctest: +SKIP\n\n# Truncate s values at 50 so plot is interesting:\n\ncount, bins, ignored = plt.hist(s[s<50], 50, density=True)\nx = np.arange(1., 50.)\ny = x**(-a) / special.zetac(a)  # doctest: +SKIP\nplt.plot(x, y/max(y), linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-dirichlet-1.py,1,"b'# Taking an example cited in Wikipedia, this distribution can be used if\n# one wanted to cut strings (each of initial length 1.0) into K pieces\n# with different lengths, where each piece had, on average, a designated\n# average length, but allowing some variation in the relative sizes of\n# the pieces.\n\ns = np.random.dirichlet((10, 5, 3), 20).transpose()\n\nimport matplotlib.pyplot as plt\nplt.barh(range(20), s[0])\nplt.barh(range(20), s[1], left=s[0], color=\'g\')\nplt.barh(range(20), s[2], left=s[0]+s[1], color=\'r\')\nplt.title(""Lengths of Strings"")\n'"
numpy/reference/random/generated/numpy-random-gamma-1.py,2,"b""# Draw samples from the distribution:\n\nshape, scale = 2., 2.  # mean=4, std=2*sqrt(2)\ns = np.random.gamma(shape, scale, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nimport scipy.special as sps  # doctest: +SKIP\ncount, bins, ignored = plt.hist(s, 50, density=True)\ny = bins**(shape-1)*(np.exp(-bins/scale) /  # doctest: +SKIP\n                     (sps.gamma(shape)*scale**shape))\nplt.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-gumbel-1.py,10,"b""# Draw samples from the distribution:\n\nmu, beta = 0, 0.1 # location and scale\ns = np.random.gumbel(mu, beta, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)\n         * np.exp( -np.exp( -(bins - mu) /beta) ),\n         linewidth=2, color='r')\nplt.show()\n\n# Show how an extreme value distribution can arise from a Gaussian process\n# and compare to a Gaussian:\n\nmeans = []\nmaxima = []\nfor i in range(0,1000) :\n   a = np.random.normal(mu, beta, 1000)\n   means.append(a.mean())\n   maxima.append(a.max())\ncount, bins, ignored = plt.hist(maxima, 30, density=True)\nbeta = np.std(maxima) * np.sqrt(6) / np.pi\nmu = np.mean(maxima) - 0.57721*beta\nplt.plot(bins, (1/beta)*np.exp(-(bins - mu)/beta)\n         * np.exp(-np.exp(-(bins - mu)/beta)),\n         linewidth=2, color='r')\nplt.plot(bins, 1/(beta * np.sqrt(2 * np.pi))\n         * np.exp(-(bins - mu)**2 / (2 * beta**2)),\n         linewidth=2, color='g')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-laplace-1.py,5,"b'# Draw samples from the distribution\n\nloc, scale = 0., 1.\ns = np.random.laplace(loc, scale, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nx = np.arange(-8., 8., .01)\npdf = np.exp(-abs(x-loc)/scale)/(2.*scale)\nplt.plot(x, pdf)\n\n# Plot Gaussian for comparison:\n\ng = (1/(scale * np.sqrt(2 * np.pi)) *\n     np.exp(-(x - loc)**2 / (2 * scale**2)))\nplt.plot(x,g)\n'"
numpy/reference/random/generated/numpy-random-logistic-1.py,2,"b'# Draw samples from the distribution:\n\nloc, scale = 10, 1\ns = np.random.logistic(loc, scale, 10000)\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, bins=50)\n\n# #   plot against distribution\n\ndef logist(x, loc, scale):\n    return np.exp((loc-x)/scale)/(scale*(1+np.exp((loc-x)/scale))**2)\nlgst_val = logist(bins, loc, scale)\nplt.plot(bins, lgst_val * count.max() / lgst_val.max())\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-lognormal-1.py,12,"b""# Draw samples from the distribution:\n\nmu, sigma = 3., 1. # mean and standard deviation\ns = np.random.lognormal(mu, sigma, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 100, density=True, align='mid')\n\nx = np.linspace(min(bins), max(bins), 10000)\npdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n       / (x * sigma * np.sqrt(2 * np.pi)))\n\nplt.plot(x, pdf, linewidth=2, color='r')\nplt.axis('tight')\nplt.show()\n\n# Demonstrate that taking the products of random samples from a uniform\n# distribution can be fit well by a log-normal probability density\n# function.\n\n# Generate a thousand samples: each is the product of 100 random\n# values, drawn from a normal distribution.\nb = []\nfor i in range(1000):\n   a = 10. + np.random.standard_normal(100)\n   b.append(np.product(a))\n\nb = np.array(b) / np.min(b) # scale values to be positive\ncount, bins, ignored = plt.hist(b, 100, density=True, align='mid')\nsigma = np.std(np.log(b))\nmu = np.mean(np.log(b))\n\nx = np.linspace(min(bins), max(bins), 10000)\npdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2))\n       / (x * sigma * np.sqrt(2 * np.pi)))\n\nplt.plot(x, pdf, color='r', linewidth=2)\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-logseries-1.py,2,"b""# Draw samples from the distribution:\n\na = .6\ns = np.random.logseries(a, 10000)\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s)\n\n# #   plot against distribution\n\ndef logseries(k, p):\n    return -p**k/(k*np.log(1-p))\nplt.plot(bins, logseries(bins, a)*count.max()/\n         logseries(bins, a).max(), 'r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-noncentral_chisquare-1.py,6,"b""# Draw values from the distribution and plot the histogram\n\nimport matplotlib.pyplot as plt\nvalues = plt.hist(np.random.noncentral_chisquare(3, 20, 100000),\n                  bins=200, density=True)\nplt.show()\n\n# Draw values from a noncentral chisquare with very small noncentrality,\n# and compare to a chisquare.\n\nplt.figure()\nvalues = plt.hist(np.random.noncentral_chisquare(3, .0000001, 100000),\n                  bins=np.arange(0., 25, .1), density=True)\nvalues2 = plt.hist(np.random.chisquare(3, 100000),\n                   bins=np.arange(0., 25, .1), density=True)\nplt.plot(values[1][0:-1], values[0]-values2[0], 'ob')\nplt.show()\n\n# Demonstrate how large values of non-centrality lead to a more symmetric\n# distribution.\n\nplt.figure()\nvalues = plt.hist(np.random.noncentral_chisquare(3, 20, 100000),\n                  bins=200, density=True)\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-noncentral_f-1.py,4,"b""# In a study, testing for a specific alternative to the null hypothesis\n# requires use of the Noncentral F distribution. We need to calculate the\n# area in the tail of the distribution that exceeds the value of the F\n# distribution for the null hypothesis.  We'll plot the two probability\n# distributions for comparison.\n\ndfnum = 3 # between group deg of freedom\ndfden = 20 # within groups degrees of freedom\nnonc = 3.0\nnc_vals = np.random.noncentral_f(dfnum, dfden, nonc, 1000000)\nNF = np.histogram(nc_vals, bins=50, density=True)\nc_vals = np.random.f(dfnum, dfden, 1000000)\nF = np.histogram(c_vals, bins=50, density=True)\nimport matplotlib.pyplot as plt\nplt.plot(F[1][1:], F[0])\nplt.plot(NF[1][1:], NF[0])\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-normal-1.py,6,"b""# Draw samples from the distribution:\n\nmu, sigma = 0, 0.1 # mean and standard deviation\ns = np.random.normal(mu, sigma, 1000)\n\n# Verify the mean and the variance:\n\nabs(mu - np.mean(s))\n# 0.0  # may vary\n\nabs(sigma - np.std(s, ddof=1))\n# 0.1  # may vary\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 30, density=True)\nplt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n               np.exp( - (bins - mu)**2 / (2 * sigma**2) ),\n         linewidth=2, color='r')\nplt.show()\n\n# Two-by-four array of samples from N(3, 6.25):\n\nnp.random.normal(3, 2.5, size=(2, 4))\n# array([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n# [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n"""
numpy/reference/random/generated/numpy-random-pareto-1.py,1,"b""# Draw samples from the distribution:\n\na, m = 3., 2.  # shape and mode\ns = (np.random.pareto(a, 1000) + 1) * m\n\n# Display the histogram of the samples, along with the probability\n# density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, _ = plt.hist(s, 100, density=True)\nfit = a*m**a / bins**(a+1)\nplt.plot(bins, max(count)*fit/max(fit), linewidth=2, color='r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-poisson-1.py,2,"b'# Draw samples from the distribution:\n\nimport numpy as np\ns = np.random.poisson(5, 10000)\n\n# Display histogram of the sample:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 14, density=True)\nplt.show()\n\n# Draw each 100 values for lambda 100 and 500:\n\ns = np.random.poisson(lam=(100., 500.), size=(100, 2))\n'"
numpy/reference/random/generated/numpy-random-power-1.py,8,"b""# Draw samples from the distribution:\n\na = 5. # shape\nsamples = 1000\ns = np.random.power(a, samples)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, bins=30)\nx = np.linspace(0, 1, 100)\ny = a*x**(a-1.)\nnormed_y = samples*np.diff(bins)[0]*y\nplt.plot(x, normed_y)\nplt.show()\n\n# Compare the power function distribution to the inverse of the Pareto.\n\nfrom scipy import stats # doctest: +SKIP\nrvs = np.random.power(5, 1000000)\nrvsp = np.random.pareto(5, 1000000)\nxx = np.linspace(0,1,100)\npowpdf = stats.powerlaw.pdf(xx,5)  # doctest: +SKIP\n\nplt.figure()\nplt.hist(rvs, bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('np.random.power(5)')\n\nplt.figure()\nplt.hist(1./(1.+rvsp), bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('inverse of 1 + np.random.pareto(5)')\n\nplt.figure()\nplt.hist(1./(1.+rvsp), bins=50, density=True)\nplt.plot(xx,powpdf,'r-')  # doctest: +SKIP\nplt.title('inverse of stats.pareto(5)')\n"""
numpy/reference/random/generated/numpy-random-random_integers-1.py,6,"b""np.random.random_integers(5)\n# 4 # random\ntype(np.random.random_integers(5))\n# <class 'numpy.int64'>\nnp.random.random_integers(5, size=(3,2))\n# array([[5, 4], # random\n# [3, 3],\n# [4, 5]])\n\n# Choose five random numbers from the set of five evenly-spaced\n# numbers between 0 and 2.5, inclusive (*i.e.*, from the set\n# :math:`{0, 5/8, 10/8, 15/8, 20/8}`):\n\n2.5 * (np.random.random_integers(5, size=(5,)) - 1) / 4.\n# array([ 0.625,  1.25 ,  0.625,  0.625,  2.5  ]) # random\n\n# Roll two six sided dice 1000 times and sum the results:\n\nd1 = np.random.random_integers(1, 6, 1000)\nd2 = np.random.random_integers(1, 6, 1000)\ndsums = d1 + d2\n\n# Display results as a histogram:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(dsums, 11, density=True)\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-standard_cauchy-1.py,1,"b'# Draw samples and plot the distribution:\n\nimport matplotlib.pyplot as plt\ns = np.random.standard_cauchy(1000000)\ns = s[(s>-25) & (s<25)]  # truncate distribution so it plots well\nplt.hist(s, bins=100)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-standard_gamma-1.py,2,"b""# Draw samples from the distribution:\n\nshape, scale = 2., 1. # mean and width\ns = np.random.standard_gamma(shape, 1000000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nimport scipy.special as sps  # doctest: +SKIP\ncount, bins, ignored = plt.hist(s, 50, density=True)\ny = bins**(shape-1) * ((np.exp(-bins/scale))/  # doctest: +SKIP\n                      (sps.gamma(shape) * scale**shape))\nplt.plot(bins, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-standard_t-1.py,5,"b'# From Dalgaard page 83 [R755c9bae090e-1]_, suppose the daily energy intake for 11\n# women in kilojoules (kJ) is:\n\nintake = np.array([5260., 5470, 5640, 6180, 6390, 6515, 6805, 7515, \\\n                   7515, 8230, 8770])\n\n# Does their energy intake deviate systematically from the recommended\n# value of 7725 kJ?\n\n# We have 10 degrees of freedom, so is the sample mean within 95% of the\n# recommended value?\n\ns = np.random.standard_t(10, size=100000)\nnp.mean(intake)\n# 6753.636363636364\nintake.std(ddof=1)\n# 1142.1232221373727\n\n# Calculate the t statistic, setting the ddof parameter to the unbiased\n# value so the divisor in the standard deviation will be degrees of\n# freedom, N-1.\n\nt = (np.mean(intake)-7725)/(intake.std(ddof=1)/np.sqrt(len(intake)))\nimport matplotlib.pyplot as plt\nh = plt.hist(s, bins=100, density=True)\n\n# For a one-sided t-test, how far out in the distribution does the t\n# statistic appear?\n\nnp.sum(s<t) / float(len(s))\n# 0.0090699999999999999  #random\n\n# So the p-value is about 0.009, which says the null hypothesis has a\n# probability of about 99% of being true.\n'"
numpy/reference/random/generated/numpy-random-triangular-1.py,1,"b'# Draw values from the distribution and plot the histogram:\n\nimport matplotlib.pyplot as plt\nh = plt.hist(np.random.triangular(-3, 0, 8, 100000), bins=200,\n             density=True)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-uniform-1.py,4,"b""# Draw samples from the distribution:\n\ns = np.random.uniform(-1,0,1000)\n\n# All values are within the given interval:\n\nnp.all(s >= -1)\n# True\nnp.all(s < 0)\n# True\n\n# Display the histogram of the samples, along with the\n# probability density function:\n\nimport matplotlib.pyplot as plt\ncount, bins, ignored = plt.hist(s, 15, density=True)\nplt.plot(bins, np.ones_like(bins), linewidth=2, color='r')\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-vonmises-1.py,3,"b""# Draw samples from the distribution:\n\nmu, kappa = 0.0, 4.0 # mean and dispersion\ns = np.random.vonmises(mu, kappa, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import i0  # doctest: +SKIP\nplt.hist(s, 50, density=True)\nx = np.linspace(-np.pi, np.pi, num=51)\ny = np.exp(kappa*np.cos(x-mu))/(2*np.pi*i0(kappa))  # doctest: +SKIP\nplt.plot(x, y, linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
numpy/reference/random/generated/numpy-random-wald-1.py,1,"b'# Draw values from the distribution and plot the histogram:\n\nimport matplotlib.pyplot as plt\nh = plt.hist(np.random.wald(3, 2, 100000), bins=200, density=True)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-weibull-1.py,5,"b'# Draw samples from the distribution:\n\na = 5. # shape\ns = np.random.weibull(a, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nx = np.arange(1,100.)/50.\ndef weib(x,n,a):\n    return (a / n) * (x / n)**(a - 1) * np.exp(-(x / n)**a)\n\ncount, bins, ignored = plt.hist(np.random.weibull(5.,1000))\nx = np.arange(1,100.)/50.\nscale = count.max()/weib(x, 1., 5.).max()\nplt.plot(x, weib(x, 1., 5.)*scale)\nplt.show()\n'"
numpy/reference/random/generated/numpy-random-zipf-1.py,2,"b""# Draw samples from the distribution:\n\na = 2. # parameter\ns = np.random.zipf(a, 1000)\n\n# Display the histogram of the samples, along with\n# the probability density function:\n\nimport matplotlib.pyplot as plt\nfrom scipy import special  # doctest: +SKIP\n\n# Truncate s values at 50 so plot is interesting:\n\ncount, bins, ignored = plt.hist(s[s<50], 50, density=True)\nx = np.arange(1., 50.)\ny = x**(-a) / special.zetac(a)  # doctest: +SKIP\nplt.plot(x, y/max(y), linewidth=2, color='r')  # doctest: +SKIP\nplt.show()\n"""
