file_path,api_count,code
classify.py,6,"b'""""""\nclassify.py\n""""""\n\nimport re\nimport pickle\nimport configparser\nfrom collections import Counter, defaultdict\nfrom itertools import chain, combinations\nimport glob\nimport numpy as np\nimport os\nfrom scipy.sparse import csr_matrix\nfrom sklearn.cross_validation import KFold\nfrom sklearn.linear_model import LogisticRegression\n\ndef get_unique_tweets(filename):\n    print(""getting unique tweets from pickle file"")\n    readtweets = open(filename, \'rb\')\n    tweets = pickle.load(readtweets)\n    readtweets.close()\n    utlist = set()\n    for t in tweets:\n        utlist.add(t[\'text\'].encode(\'utf8\').decode(\'unicode_escape\').encode(\'ascii\',\'ignore\').decode(""utf-8""))\n    print(""found %d unique tweets from file"" % len(utlist))\n\n    return list(utlist)\n\ndef get_afinn_sentiment(affin_filename):\n    print(""forming pos and neg word list from affin sentiment file"")\n    pos = []\n    neg = []\n    with open(affin_filename) as f:\n        for line in f:\n            tl = re.split(r\'\\t+\', line.rstrip(\'\\t\'))\n            if int(tl[1]) > 0:\n                pos.append(tl[0].encode(\'ascii\',\'ignore\').decode(""utf-8""))\n            elif int(tl[1]) < 0:\n                neg.append(tl[0].encode(\'ascii\',\'ignore\').decode(""utf-8""))\n\n    return pos,neg\n\ndef dsum(*dicts):\n    ret = defaultdict(int)\n    for d in dicts:\n        for k, v in d.items():\n            ret[k] += v\n    return dict(ret)\n\ndef read_data(path):\n    fnames = sorted([f for f in glob.glob(os.path.join(path, \'pos\', \'*.txt\'))])\n    data = [(1, open(f).readlines()[0]) for f in sorted(fnames[:1000])]\n    fnames = sorted([f for f in glob.glob(os.path.join(path, \'neg\', \'*.txt\'))])\n    data += [(0, open(f).readlines()[0]) for f in sorted(fnames[:1000])]\n    data = sorted(data, key=lambda x: x[1])\n    return np.array([d[1] for d in data]), np.array([d[0] for d in data])\n\ndef tokenize(doc):\n    tnp = []\n    for x in doc.lower().split():\n        tnp.append(re.sub(\'^\\W+\', \'\',re.sub(\'\\W+$\', \'\',x)))\n    #return tnp\n    return np.array(tnp)\n\ndef token_features(tokens, feats,pos,neg):\n    feats.update(dsum(dict(Counter(Counter([""token="" + s for s in tokens]))),feats))\n\ndef token_pair_features(tokens,feats,pos,neg):\n    k=3\n    for i in range(len(tokens)-k+1):\n        for e in list(combinations(list(tokens[i:k+i]), 2)):\n            feats[\'token_pair=\'+e[0]+\'__\'+e[1]] += 1\n\ndef lexicon_features(tokens,feats,pos_words,neg_words):\n    feats.update(dsum(dict(Counter({\'pos_words\': len([x for x in list(s.lower() for s in tokens) if x in pos_words]) , \'neg_words\' : len([x for x in list(s.lower() for s in tokens) if x in neg_words]) })),feats))\n\ndef featurize(tokens,feature_fns,pos,neg):\n    feats = defaultdict(lambda : 0)\n    for fn in feature_fns:\n        fn(tokens,feats,pos,neg)\n    return sorted(list(feats.items()), key=lambda x: (x[0]))\n\ndef vectorize(tokens_list,pos,neg,vocab=None):\n    feature_fns = [token_pair_features,lexicon_features]\n    #feature_fns = [token_pair_features,lexicon_features,token_features]\n    min_freq = 2\n\n    vf = []\n    vocabSec = {}\n\n    for t in tokens_list:\n        vf.append(list(featurize(t,feature_fns,pos,neg)))\n\n    if vocab is None:\n        vocabSec = {i:x for x,i in enumerate(sorted(list([k for k,v in dict(Counter(list([e[0] for e in list(chain(*vf)) if e[1]>0]))).items() if v >=min_freq])))}\n    else:\n        vocabSec = vocab\n\n    column=[]\n    data=[]\n    rows=[]\n\n    row=0\n    for f in vf:\n        for e in f:\n            if e[0] in vocabSec:\n                rows.append(row)\n                column.append(vocabSec[e[0]])\n                data.append(e[1])\n        row+=1\n\n    data=np.array(data,dtype=\'int64\')\n    rows=np.array(rows,dtype=\'int64\')\n    column=np.array(column,dtype=\'int64\')\n    X=csr_matrix((data, (rows,column)), shape=(len(tokens_list), len(vocabSec)))\n\n    #print (X.toarray())\n    #print (sorted(vocabSec.items(), key=lambda x: x[1]))\n\n    # for x in vocabSec:\n    #     x1 = re.sub(\'token=\', \'\', x)\n    #     line = re.sub(\'token_pair=\', \'\', x1)\n    #     print(line.encode(\'ascii\',\'ignore\').decode(""utf-8""))\n\n    return X,vocabSec\n\ndef fit_train_classifier(docs, labels,pos,neg):\n    tokens_list = [ tokenize(d) for d in docs ]\n    X,vocab = vectorize(tokens_list,pos,neg)\n    model = LogisticRegression()\n    model.fit(X,labels)\n\n    return model,vocab\n\ndef parse_test_data(tweets,vocab,pos,neg):\n    tokenslist = [ tokenize(d) for d in tweets ]\n    X_test,vocb=vectorize(tokenslist,pos,neg,vocab)\n    return X_test\n\ndef print_classification(tweets,X_test,clf):\n    predicted = clf.predict(X_test)\n    saveclassifydata = {}\n    print(""Number of pos and neg tweets: ""+str(Counter(predicted)))\n    for idx,t in  enumerate(tweets[:10]):\n        label = \'\'\n        if predicted[idx] == 1:\n            label = ""Positive""\n            saveclassifydata[\'positivetweet\'] = t\n        elif predicted[idx] == 0:\n            label = ""Negative""\n            saveclassifydata[\'negativetweet\']  = t\n        print(""Classified as : %s Tweet Text: %s "" % (label,t))\n\n    saveclassifydata[\'pos\'] = dict(Counter(predicted))[1]\n    saveclassifydata[\'neg\'] = dict(Counter(predicted))[0]\n\n    outputfile = open(\'dataclassify.pkl\', \'wb\')\n    pickle.dump(saveclassifydata, outputfile)\n    outputfile.close()\n\n\ndef main():\n    config =  configparser.ConfigParser()\n    config.read(\'twitter.cfg\')\n    internalData =  str(config.get(\'twitter\', \'useDataFile\'))\n    affin_filename = \'data/affin/AFINN-111.txt\'\n    filename = \'\'\n    if internalData == ""True"":\n        print(""As internalData is set to True we will use tweets in file mytweets.pkl"")\n        filename = \'data/mydownloadeddata/mytweets.pkl\'\n    elif internalData == ""False"":\n        print(""As internalData is set to False we will use new tweets file newmytweets.pkl"")\n        filename = \'newmytweets.pkl\'\n    tweets = get_unique_tweets(filename)\n    pos,neg = get_afinn_sentiment(affin_filename)\n    print(""Total pos words are %d"" % int(len(pos)))\n    print(""Total neg words are %d"" % int(len(neg)))\n    print(""Reading and fitting train data"")\n    docs, labels = read_data(os.path.join(\'data\', \'train\'))\n    clf, vocab = fit_train_classifier(docs,labels,pos,neg)\n    print (""Length of vocab is %d"" % int(len(vocab)))\n    X_test = parse_test_data(tweets,vocab,pos,neg)\n    print_classification(tweets,X_test,clf)\n\nif __name__ == \'__main__\':\n    main()\n'"
cluster.py,0,"b'""""""\ncluster.py\n""""""\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\nimport configparser\nimport pickle\nimport time\nimport datetime\nimport sys\nimport os\nimport networkx as nx\nimport itertools\nfrom itertools import islice\nfrom collections import Counter\n\ndef get_friends(limit_users,internalData):\n    filename = \'\'\n    if internalData == ""False"":\n        print(""as internalData is set to false we will read from new users file: newmyusers.pkl"")\n        filename = \'newmyusers.pkl\'\n    elif internalData == ""True"":\n        print(""as internalData is set to true we will read from my users file: myusers.pkl from data/mydownloadeddata"")\n        filename = ""data/mydownloadeddata/myusers.pkl""\n\n    readusers = open(filename, \'rb\')\n    userFidsDict = pickle.load(readusers)\n    readusers.close()\n\n    if len(userFidsDict) < limit_users:\n        limit_users = len(userFidsDict)\n\n    n_items = list(islice(userFidsDict.items(),limit_users))\n    userFidsDict = dict(n_items)\n    return userFidsDict\n\ndef jaccard_similarity(x,y):\n    if len(x)>0 and len(y)>0:\n\t       return len(set.intersection(*[set(x), set(y)]))/len(set.union(*[set(x), set(y)]))\n    else:\n        return 0\n\ndef create_graph(users):\n\n    graph = nx.Graph()\n    userlist = list(users.keys())\n    for u in userlist:\n        graph.add_node(u)\n\n    for u in combinations(userlist,2):\n        fid1 = [users[u[0]]]\n        fid2 = [users[u[1]]]\n        ew = jaccard_similarity(fid1[0],fid2[0])\n        if ew > 0.009 :\n            graph.add_edge(u[0],u[1],weight=ew)\n\n    remove = [node for node,degree in graph.degree().items() if degree <= 1]\n    graph.remove_nodes_from(remove)\n    print(""Removed %d nodes from graph as they had less than 1 degree to reduce clutter in graph"" % int(len(remove)))\n    print(""Also edge weight threshold is set to 0.009 to reduce clutter in graph"")\n\n    return graph\n\ndef find_best_edge(graph):\n    centrality = nx.edge_betweenness_centrality(graph, weight=\'weight\')\n    return max(centrality, key=centrality.get)\n\ndef get_communities(G,no_communities):\n    print(""Total communities to identify are: %d"" % no_communities )\n    print(""Graph has %d nodes and %d edges"" % (int(G.number_of_nodes()),int(G.number_of_edges())))\n    components = [c for c in nx.connected_component_subgraphs(G)]\n    count = 0\n    while len(components) < no_communities:\n        edge_to_remove = find_best_edge(G)\n        #print(\'removing \' + str(edge_to_remove))\n        G.remove_edge(*edge_to_remove)\n        components = [c for c in nx.connected_component_subgraphs(G)]\n        count +=1\n\n    result = [c.nodes() for c in components]\n    #print(\'components=\' + str(result))\n    print(""Total edges removed by girvan_newman algo from graph are: ""+str(count))\n    return result\n\ndef main():\n    config =  configparser.ConfigParser()\n    config.read(\'twitter.cfg\')\n    no_communities = int(config.get(\'twitter\', \'communities\'))\n    limit_users =  int(config.get(\'twitter\', \'clusterUserLimit\'))\n    internalData =  str(config.get(\'twitter\', \'useDataFile\'))\n    userinfo = get_friends(limit_users,internalData)\n    graph = create_graph(userinfo)\n    result = get_communities(graph,no_communities)\n    saveclusterdata = {}\n    #print(\'resulting communities are :\' + str(result))\n    total_users = graph.number_of_nodes()\n    communities_list_count = dict(Counter([len(l) for l in result]))\n    #print(""Count of communities by users is: ""+ str(communities_list_count))\n    size = str(total_users*(sum((d[0]*d[1])/total_users for d in communities_list_count.items())/no_communities))\n    print(\'Average number of users per community :\'+ size )\n    saveclusterdata[\'communities\']  = result\n    saveclusterdata[\'size\']  = size\n    outputfile = open(\'datacluster.pkl\', \'wb\')\n    pickle.dump(saveclusterdata, outputfile)\n    outputfile.close()\n\nif __name__ == \'__main__\':\n    main()\n'"
collect.py,0,"b'""""""\ncollect.py\n""""""\nfrom TwitterAPI import TwitterAPI\nimport configparser\nimport pickle\nimport time\nimport datetime\nimport sys\n\ndef get_twitter(config):\n    twitter = TwitterAPI(\n                   config.get(\'twitter\', \'consumer_key\'),\n                   config.get(\'twitter\', \'consumer_secret\'),\n                   config.get(\'twitter\', \'access_token\'),\n                   config.get(\'twitter\', \'access_token_secret\'))\n    return twitter\n\ndef get_tweets(twitter,no_tweets,search_keyword):\n    tweets = []\n    users = []\n    idx=0;\n    print(""total number tweets to fetch are: %d"" % no_tweets)\n    print(""keyword to search in tweets: %s"" % search_keyword)\n\n    while  len(tweets) < no_tweets:\n        until=(datetime.datetime.now()-datetime.timedelta(days=idx)).strftime(\'%Y-%m-%d\')\n        #print(""until for fetching set to: %s and loop count is %d"" % (until,idx+1))\n        for res in robust_request(twitter, \'search/tweets\', {\'q\': search_keyword, \'count\': 100 , \'lang\': \'en\',\'until\':until}):\n            #if res[\'user\'][\'screen_name\'] not in users and res[\'retweet_count\']==0:\n            #if res[\'retweet_count\']==0:\n            if res[\'user\'][\'screen_name\'] not in users:\n                users.append(res[\'user\'][\'screen_name\'])\n                tweets.append(res)\n                if len(tweets) % 50 == 0:\n                    print(\'%d tweets fetched\' % len(tweets))\n                    #print(\'sleeping for 10 second before next request\')\n                    outputfile = open(\'newmytweets.pkl\', \'wb\')\n                    pickle.dump(tweets, outputfile)\n                    outputfile.close()\n        idx +=1\n        if idx >= 8:\n            print(""max limit reached for fetching tweets"")\n            print(""tweets collected are %s , rest of the program will run on this tweets data"" % len(tweets))\n            print(""if require more tweets please run program again for keyword which is more common"")\n            break\n\n    print(\'fetched %d tweets from unique users\' % len(tweets))\n    print (""tweets saved to pickle file newmytweets.pkl"")\n    return tweets\n\ndef get_friends(tweets,twitter,limit_users,internalData):\n    print (""limiting user info as lookup for /friends/ids will take long time to fetch data for all user"")\n    print (""user limit to fetch is: %d , you can change the limit in twitter.cfg for paramter clusterUserLimit"" % limit_users)\n    if internalData == ""False"":\n        print(""Total tweets with unique users are: %d"" % len(tweets))\n        snlist = list(set(d[\'user\'][\'screen_name\'] for d in tweets[:limit_users]))\n        users = [{\'screen_name\':s} for s in snlist]\n        print(""Starting to get following(friends ids) according to user limit set in twitter.cfg :""+str(len(snlist)))\n        userFidsDict = {}\n        #print(snlist)\n        #print(users)\n        for user in users:\n            friendids = robust_request(twitter, \'friends/ids\', {\'screen_name\':user[\'screen_name\'], \'count\': 5000})\n            fids = friendids.json()\n            sfids = sorted(fids[\'ids\'])\n            userFidsDict[user[\'screen_name\']] = sfids\n            if len(userFidsDict) % 5 == 0:\n                print(\'%d friends ids fetched\' % len(userFidsDict))\n            outputfile = open(\'newmyusers.pkl\', \'wb\')\n            pickle.dump(userFidsDict, outputfile)\n            outputfile.close()\n\n        print (""followers ids saved to pickle file newmyusers.pkl"")\n\ndef search_tweet_time_required(twitter):\n    rate_limit = twitter.request(\'application/rate_limit_status\')\n    tlist = [r for r in rate_limit]\n    return str(datetime.datetime.fromtimestamp(tlist[0][\'resources\'][\'search\'][\'/search/tweets\'][\'reset\']) - datetime.datetime.now()).split(\':\')\n\ndef friends_ids_time_required(twitter):\n    rate_limit = twitter.request(\'application/rate_limit_status\')\n    tlist = [r for r in rate_limit]\n    return str(datetime.datetime.fromtimestamp(tlist[0][\'resources\'][\'friends\'][\'/friends/ids\'][\'reset\']) - datetime.datetime.now()).split(\':\')\n\ndef robust_request(twitter, resource, params, max_tries=50):\n    for i in range(max_tries):\n        request = twitter.request(resource, params)\n        if request.status_code == 200:\n            return request\n        else:\n            print(\'Got error %s \\nneed to sleep for some time.\' % request.text)\n            sys.stderr.flush()\n            if resource == ""search/tweets"":\n                remaining_time = search_tweet_time_required(twitter)\n                print (\'We need to wait for %s Hours %s minutes and %s seconds before your next search/tweets API request\' % (remaining_time[0], remaining_time[1], remaining_time[2]))\n                print (\'sleeping...\')\n                time.sleep(1 + (65 * int(remaining_time[1])))\n            elif resource == ""friends/ids"":\n                remaining_time = friends_ids_time_required(twitter)\n                print (\'We need to wait for %s Hours %s minutes and %s seconds before your next friends/ids API request\' % (remaining_time[0], remaining_time[1], remaining_time[2]))\n                print (\'sleeping...\')\n                print (\'You can exit at any time to continue to run program on retrieved data...\')\n                time.sleep(1 + (65 * int(remaining_time[1])))\n\ndef main():\n    config =  configparser.ConfigParser()\n    config.read(\'twitter.cfg\')\n    internalData = str(config.get(\'twitter\', \'useDataFile\'))\n\n    if internalData == ""True"":\n        print(""No need to collect data as useDataFile is set to true in twitter.cfg"")\n        print(""we will use already generated data files - collect.py - mytweets.pkl and myusers.pkl"")\n        print(""if you still want to run collect.py please set useDataFile to true in twitter.cfg"")\n        print(""keyword data in tweets: cubs"")\n\n        filename = \'data/mydownloadeddata/mytweets.pkl\'\n\n        readtweets = open(filename, \'rb\')\n        tweets = pickle.load(readtweets)\n        readtweets.close()\n\n        outputfile = open(\'datacollect.pkl\', \'wb\')\n        pickle.dump(tweets, outputfile)\n        outputfile.close()\n\n    elif internalData == ""False"":\n        print(""collecting new data as useDataFile is set to false"")\n        twitter = get_twitter(config)\n        no_tweets =  int(config.get(\'twitter\', \'numberOfTweets\'))\n        search_keyword = str(config.get(\'twitter\', \'keywordForTweets\'))\n        tweets = get_tweets (twitter,no_tweets,search_keyword)\n        outputfile = open(\'datacollect.pkl\', \'wb\')\n        pickle.dump(tweets, outputfile)\n        outputfile.close()\n        limit_users =  int(config.get(\'twitter\', \'clusterUserLimit\'))\n        get_friends(tweets,twitter,limit_users,internalData)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
summarize.py,0,"b'""""""\nsumarize.py\n""""""\n\nimport pickle\n\nfile= open(""datacollect.pkl"", ""rb"")\ntweets=pickle.load(file)\nfile.close()\n\nfile= open(""datacluster.pkl"", ""rb"")\ncomponents=pickle.load(file)\nfile.close()\n\nfile= open(""dataclassify.pkl"", ""rb"")\nclassify=pickle.load(file)\nfile.close()\n\ntext_file = open(""summary.txt"", ""w"")\ntext_file.write(""Number of users collected: %s ""%(len(tweets)))\ntext_file.write(""\\nNumber of messages collected: %s ""%(len(tweets)))\ntext_file.write(""\\nNumber of communities discovered: %s""%(len(components[\'communities\'])))\ntext_file.write(""\\nAverage number of users per community: %s ""%(components[\'size\']))\ntext_file.write(""\\nNumber of instances per class found: positive tweets: %d negative tweets:%d""%(classify[\'pos\'],classify[\'neg\']))\ntext_file.write(""\\nOne example from each class:\\npositive tweet: %s \\nnegative tweet: %s""%(classify[\'positivetweet\'],classify[\'negativetweet\']))\ntext_file.close()\n'"
