file_path,api_count,code
clean.py,0,"b'from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n\nadditional_words = {\'thank\', \'thanks\', \'(unclear)\', \'bye\', \'goodbye\', \'good-bye\', \'ok\', \'okay\'}\nadditional_words.union(ENGLISH_STOP_WORDS)\n\nwith open(""clean.txt"", \'r\') as f:\n    list = []\n    text = f.read().splitlines()\n    for s in text:\n        list.append(s.split(\' \'))\n\nwith open(\'nostop.txt\', \'w\') as f:\n    tokens = []\n    for sentence in list:\n        s = []\n        for w in sentence:\n            if w not in additional_words:\n                s.append(w)\n        if len(s) > 2:\n            f.write(\' \'.join(s) + \'\\n\')\n'"
cluster.py,4,"b'import numpy as np\nimport shutil\nimport json\nimport math\nimport os\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom tqdm import *\n\n\ndef kmeans(params):\n    \'\'\'\n    Cluster arrays using KMeans.\n    :params.embedding_file: 2D NumPy arrays\n    \'\'\'\n\n    vectors = np.load(params.embedding_file)\n    sentences = open(params.sentence_file, \'r\').read().splitlines()\n\n    # clear current directory of groups\n    if os.path.exists(params.kmeans_dir):\n        shutil.rmtree(params.kmeans_dir)\n    os.makedirs(params.kmeans_dir)\n\n    # compute KMeans\n    km = KMeans(n_clusters=params.n_clusters,\n                n_init=params.n_init,\n                max_iter=params.max_iter,\n                verbose=params.verbose,\n                n_jobs=params.n_jobs,\n                algorithm=params.algorithm).fit(vectors)\n    labels = km.labels_\n    inertia = km.inertia_\n    silhouette = silhouette_score(vectors, labels)\n    print(""Clusters: %d \\t Inertia: %.5f \\t \'Silhouette: %.5f\\n"" % (params.n_clusters, inertia, silhouette))\n\n    # write list of labels to output\n    with open(params.km_labels_file, \'w\') as f:\n        json.dump(labels, f)\n    \n    # write labels to .txt files\n    for i in tqdm(range(len(labels))):\n        label = labels[i]\n        file_name = str(label) + \'.txt\'\n        with open(os.path.join(params.kmeans_dir, file_name), \'a\') as f:\n            f.write(sentences[i] + \'\\n\')\n\n\ndef opt_k(params):\n    \'\'\'\n    Returns inertia and silhouette scores of k clusters in specified range.\n    :params.embedding_file: 2D NumPy arrays\n    :params.min_k: minimum k value to start\n    :params.max_k: maximum k value to start\n    :params.n_k: number of k values to try within range\n    \'\'\'\n\n    vectors = np.load(params.embedding_file)\n\n    opt = []\n    incr = int((params.max_k - params.min_k) / params.n_k)\n\n    # loop through different k values\n    for i in tqdm(range(params.n_k + 1)):\n        cur_n = params.max_k - i * incr\n        km = KMeans(n_clusters=cur_n,\n                    n_init=params.n_init,\n                    max_iter=params.max_iter,\n                    verbose=params.verbose,\n                    n_jobs=params.n_jobs,\n                    algorithm=params.algorithm).fit(vectors)\n        labels = km.labels_\n        inertia = km.inertia_\n        silhouette = silhouette_score(vectors, labels)\n        opt.append(str(cur_n) + \'\\t\' + str(inertia) + \'\\t\' + str(silhouette) + \'\\n\')\n        print(""Clusters: %d \\t Inertia: %.5f \\t Silhouette: %.5f \\n"" % (cur_n, inertia, silhouette))\n\n    with open(params.km_opt_file, \'w\') as f:\n        f.writelines(opt)\n\n\ndef hierarch_k(params):\n    \'\'\'\n    Computes KMeans hierarchically.\n    :params.embedding_file: 2D Numpy Arrays\n    :params.snetence_file: file of sentences\n    :params.n_iter: number of hierarchy levels\n    :params.split_size: number of clusters per level\n    \'\'\'\n\n    vectors = np.load(params.embedding_file)\n    sentences = open(params.sentence_file, \'r\').read().splitlines()\n\n    # clear current directory of groups\n    if os.path.exists(params.hierarch_dir):\n        shutil.rmtree(params.hierarch_dir)\n    os.makedirs(params.hierarch_dir)\n\n    # cluster iteratively\n    master = [\'\']*len(sentences)\n    ids = [i for i in range(len(sentences))]\n    iter_k(params, master, ids, vectors, sentences, params.n_iter, \'\')\n    with open(params.km_labels_file, \'w\')as f:\n        json.dump(master, f)\n\n\ndef iter_k(params, master, ids, vectors, sentences, i, filename):\n    \'\'\'\n    Helper function for hierarch_k to perform iterations.\n    :vectors: NumPy arrays to be clustered\n    :sentences: list of sentences corresponding to vectors\n    :i: ith iteration in hierarchy\n    :filename: file to write \n    \'\'\'\n\n    if i == 0:\n        return\n\n    km = KMeans(n_clusters=params.split_size,\n                n_init=params.n_init,\n                max_iter=params.max_iter,\n                verbose=params.verbose,\n                n_jobs=params.n_jobs,\n                algorithm=params.algorithm).fit(vectors)\n    labels = km.labels_\n\n    # separate vectors by label\n    for j in range(params.split_size):\n        cluster_s, cluster_v, idx = [], [], []\n        for k in range(len(labels)):\n            if labels[k] == j:\n                cluster_s.append(sentences[k])\n                cluster_v.append(vectors[k])\n                idx.append(ids[k])\n                master[ids[k]] += str(j)\n\n        # split if possible\n        if len(cluster_s) >= params.split_size:\n            iter_k(params, master, idx, np.stack(cluster_v), cluster_s, i-1, filename + str(j))\n\n        # write clusters at each hierarchy\n        with open(os.path.join(params.hierarch_dir, filename + str(j)), \'w\') as f:\n            f.writelines([s + \'\\n\' for s in cluster_s])\n\n'"
embed.py,15,"b'import numpy as np\nimport re, os\nfrom allennlp.commands.elmo import ElmoEmbedder\nfrom tqdm import *\n\n\ndef file_len(f):\n    """"""\n    Returns number of lines in a file.\n    :f: file object reader\n    """"""\n\n    for n, l in enumerate(f, 1):\n        pass\n    f.seek(0) # rewind\n    return n\n\ndef concat_word_vecs(sentence_vec, max_len=50):\n    \'\'\'\n    Concatenate word embeddings together to get sentence/transcription vector.\n    :sentence_vec: 2D Tensor of word embeddings\n    :max_len: length of each sentence to pad/truncate to\n    :return: a 1D Tensor for the sentence/transcription\n    \'\'\'\n\n    # pad/truncate to same shape\n    sentence_len = sentence_vec.shape[0]\n    if sentence_len > max_len:\n        sentence_vec = sentence_vec[0:max_len, :]\n    elif sentence_len < max_len:\n        pad_width = np.array([[0, max_len - sentence_len], [0, 0]])\n        pad_values = np.array([[0, 0], [0, 0]])\n        sentence_vec = np.pad(sentence_vec, pad_width, \'constant\', constant_values=pad_values)\n    \n    return np.concat([sentence_vec[i] for i in range(max_len)], axis=0)\n\n\ndef load_glove(glove_file):\n    \'\'\'\n    Create a dictionary for GloVe lookup.\n    :glove_file: location of GloVe text file\n    :return: a Python dictionary of GloVe word embeddings\n    \'\'\'\n    with open(glove_file, \'r\', encoding=""utf-8"") as f:\n        word_dict = {}\n        lines = f.read().splitlines()\n        for i in tqdm(range(len(lines))):\n            line = lines[i].split()\n            word = \'\'.join(line[:-300])\n            embedding = np.asarray(line[-300:], dtype=\'float32\')\n            word_dict[word] = embedding\n    return word_dict\n\n\ndef embed(params):\n    \'\'\'\n    Embed a list of sentences using ELMo or GloVe.\n    :params.elmo: use ELMo\n    :params.glove: use GloVe\n    :params.concat_word_vecs: concatenate word vectors\n    :params.sum_word_vecs: sum word vectors\n    :params.avg_word_vecs: average word vectors\n    :return: a Tensor of Tensors (sentence/transcription embeddings)\n    \'\'\'\n\n    def compress(emb):\n        """"""\n        Compress a matrix of word vectors into a sentence vector.\n        """"""\n        if params.sum_word_vecs:\n            return np.sum(emb, axis=0)\n        if params.max_pool_word_vecs:\n            return np.amax(emb, axis=0)\n        if params.concat_word_vecs:\n            return concat_word_vecs(emb, params.max_transcript_len)\n        if params.avg_word_vecs:\n            return np.mean(emb, axis=0)\n\n    prefix, suffix = os.path.splitext(params.embedding_file)\n    # f = open(params.sentence_file, \'r\', encoding=params.encoding, errors=params.errors)\n    f = open(params.sentence_file, \'r\')\n    num = file_len(f)\n\n    # initialize embedding methods\n    if params.elmo:\n        elmo = ElmoEmbedder(params.elmo_options_file, params.elmo_weights_file, params.elmo_cuda_device)\n    if params.glove:\n        word_dict = load_glove(params.glove_word_file)\n\n    e_emb, g_emb = [], []\n\n    # tokenize each line\n    for i, s in tqdm(enumerate(f, 1), total=num):\n        s = s.replace(""\'"", """")\n        s = re.findall(r""[\\w]+|[.,!?;:()%$&#]"", s)\n        if len(s) == 0:\n            continue\n\n        # embed with ELMo\n        if params.elmo:\n\n            # write to prevent OOM\n            if i % 500000 == 0:\n                np.save(prefix + \'_elmo%d\'%(i//500000) + suffix, np.stack(e_emb, axis=0))\n                print(\'Wrote to \' + prefix + \'_elmo%d\'%(i//500000) + suffix)\n                e_emb = []\n\n            emb = np.array(elmo.embed_sentence(s), dtype=np.float32)\n\n            # reduce word vectors: 3 -> 1\n            if params.bilm_layer_index == -1:\n                emb = np.mean(emb, axis=0)\n            elif params.bilm_layer_index <= 2 and params.bilm_layer_index >= 0:\n                emb = emb[params.bilm_layer_index, :, :] # shape: [n, 1024]\n\n            # reduce sentence vectors -> 1\n            e_emb.append(compress(emb))\n\n        # embed with GloVe\n        if params.glove:\n\n            # write to prevent OOM\n            if i % 500000 == 0:\n                np.save(prefix + \'_glove%d\'%(i//500000) + suffix, np.stack(e_emb, axis=0))\n                print(\'Wrote to \' + prefix + \'_glove%d\'%(i//500000) + suffix)\n                g_emb = []\n\n            emb = np.stack([word_dict[word]\n                                   if word in word_dict.keys()\n                                   else word_dict[\'OOV\']\n                                   for word in s\n                                  ], axis=0)\n\n            # reduce sentence vectors -> 1\n            g_emb.append(compress(emb))\n\n    if len(e_emb) > 0:\n        np.save(prefix + \'_elmo%d\'%(i//500000) + suffix, np.stack(e_emb, axis=0))\n        print(\'Wrote to \' + prefix + \'_elmo%d\'%(i//500000) + suffix)\n    if len(g_emb) > 0:\n        np.save(prefix + \'_glove%d\'%(i//500000) + suffix, np.stack(g_emb, axis=0))\n        print(\'Wrote to \' + prefix + \'_glove%d\'%(i//500000) + suffix)\n'"
main.py,0,"b'import numpy as np\n\nfrom embed import embed\nfrom sif import sif\nfrom cluster import kmeans, opt_k, hierarch_k\nfrom project import pca, tsne\nfrom meta import write_meta\nfrom tensorboard import tensorboard\n\nimport argparse, os\n\n# ELMo model to create embeddings\nelmo_options_file = os.path.join(os.getcwd(), ""model"", ""options.json"")\nelmo_weights_file = os.path.join(os.getcwd(), ""model"", ""weights.hdf5"")\n\n# GloVe model to create embeddings\nglove_word_file = os.path.join(os.getcwd(), ""model"", ""glove.840B.300d.txt"")\n# glove_char_file = os.path.join(os.getcwd(), ""model"", ""glove.840B.300d-char.txt"")\n\n# output files\nfilename = ""medica-s.txt""\nsentence_dir = os.path.join(os.getcwd(), ""data"")\n# sentence_dir = os.getcwd()\nsentence_file = os.path.join(sentence_dir, filename)\nif sentence_file == os.path.join(sentence_dir, """"):\n    print(""Specify sentence file."")\n    raise NameError\n\noutput_dir = os.path.join(os.getcwd(), ""rapids"", os.path.splitext(filename)[0])\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nembedding_file = os.path.join(output_dir, ""embeddings.npy"")\nsif_file = os.path.join(output_dir, ""embeddings_sif.npy"")\npca_file = os.path.join(output_dir, ""embeddings_pc.npy"")\ntsne_file = os.path.join(output_dir, ""embeddings_ts.npy"")\nkm_labels_file = os.path.join(output_dir, ""km_labels.json"")\nkm_opt_file = os.path.join(output_dir, ""km_opt.csv"")\nmetadata_file = os.path.join(output_dir, ""metadata.tsv"")\n\n# parse arguments at runtime\nparser = argparse.ArgumentParser()\n\n# files\nparser.add_argument(""--sentence_file"", nargs=\'?\', default=sentence_file, type=str, help=""file for embedding"")\nparser.add_argument(""--embedding_file"", nargs=\'?\', default=embedding_file, type=str, help=""file for sentence embeddings"")\nparser.add_argument(""--sif_file"", nargs=\'?\', default=sif_file, type=str, help=""file for SIF embeddings"")\nparser.add_argument(""--tsne_file"", nargs=\'?\', default=tsne_file, type=str, help=""file for t-SNE embeddings"")\nparser.add_argument(""--km_labels_file"", nargs=\'?\', default=km_labels_file, type=str, help=""file for KMeans cluster labels"")\nparser.add_argument(""--km_opt_file"", nargs=\'?\', default=km_opt_file, type=str, help=""file for KMeans cluster-inertia-silhouette output"")\nparser.add_argument(""--metadata_file"", nargs=\'?\', default=metadata_file, type=str, help=""file for TensorBoard metadata"")\n\n# mode\nparser.add_argument(""--mode"", nargs=\'?\', default=""embed"", type=str, help=""embed, sif, cluster, project, metadata, tensorboard"")\n\n# embed\nparser.add_argument(""--encoding"", nargs=\'?\', default=""ascii"", type=str, help=""encoding of input sentence file"")\nparser.add_argument(""--errors"", nargs=\'?\', default=""ignore"", type=str, help=""error-handling of reading encoded input"")\n\nparser.add_argument(""--elmo"", nargs=\'?\', default=True, type=bool, help=""use ELMo for embeddings"")\nparser.add_argument(""--elmo_options_file"", nargs=\'?\', default=elmo_options_file, type=str, help=""options file for ELMo embedding"")\nparser.add_argument(""--elmo_weights_file"", nargs=\'?\', default=elmo_weights_file, type=str, help=""weights file for ELMo embedding"")\nparser.add_argument(""--elmo_cuda_device"", nargs=\'?\', default=0, type=int, help=""GPU device to run on"")\n\nparser.add_argument(""--glove"", nargs=\'?\', default=False, type=bool, help=""use GloVe for embeddings"")\nparser.add_argument(""--glove_word_file"", nargs=\'?\', default=glove_word_file, type=str, help=""word file for GloVe embedding"")\n# parser.add_argument(""--glove_char_file"", nargs=\'?\', default=glove_char_file, type=str, help=""char file for GloVe embedding"")\n\nparser.add_argument(""--bilm_layer_index"", nargs=\'?\', default=2, type=int, help=""which bilm layer of ELMo to use, indexed from 0 (-1 for average)"")\nparser.add_argument(""--sum_word_vecs"", nargs=\'?\', default=False, type=bool, help=""sum word vectors in the same sentence"")\nparser.add_argument(""--avg_word_vecs"", nargs=\'?\', default=True, type=bool, help=""average word vectors in same sentence"")\nparser.add_argument(""--concat_word_vecs"", nargs=\'?\', default=False, type=bool, help=""concatenate word vectors in the same sentence"")\nparser.add_argument(""--max_pool_word_vecs"", nargs=\'?\', default=False, type=bool, help=""max pooling across word vectors in the same sentence"")\nparser.add_argument(""--max_transcript_len"", nargs=\'?\', default=30, type=int, help=""if concatenating, length to pad/truncate to"")\n\n# sif\nparser.add_argument(""--sif_rmpc"", nargs=\'?\', default=1, type=int, help=""number of principal components to remove"")\n\n# cluster\nparser.add_argument(""--kmeans"", nargs=\'?\', default=False, type=bool, help=""use KMeans clustering"")\nparser.add_argument(""--kmeans_dir"", nargs=\'?\', default=os.path.join(output_dir, ""kmeans""), type=str, help=""output file for KMeans clusters"")\nparser.add_argument(""--n_clusters"", nargs=\'?\', default=10, type=int, help=""n_clusters in KMeans function"")\nparser.add_argument(""--n_init"", nargs=\'?\', default=10, type=int, help=""n_init in KMeans function"")\nparser.add_argument(""--max_iter"", nargs=\'?\', default=300, type=int, help=""max_iter in KMeans function"")\nparser.add_argument(""--verbose"", nargs=\'?\', default=False, type=bool, help=""verbose in KMeans function"")\nparser.add_argument(""--n_jobs"", nargs=\'?\', default=-1, type=int, help=""n_jobs in KMeans function"")\nparser.add_argument(""--algorithm"", nargs=\'?\', default=""auto"", type=str, help=""algorithm in KMeans function"")\n\nparser.add_argument(""--opt_k"", nargs=\'?\', default=True, type=bool, help=""find optimal k"")\nparser.add_argument(""--min_k"", nargs=\'?\', default=10, type=int, help=""minimum k to try"")\nparser.add_argument(""--max_k"", nargs=\'?\', default=110, type=int, help=""maximum k to try"")\nparser.add_argument(""--n_k"", nargs=\'?\', default=10, type=int, help=""number of k\'s to try"")\n\nparser.add_argument(""--hierarch_k"", nargs=\'?\', default=False, type=bool, help=""compute kmeans hierarchically"")\nparser.add_argument(""--hierarch_dir"", nargs=\'?\', default=os.path.join(output_dir, ""hierarchy""), type=str, help=""directory for hierarchy clusters"")\nparser.add_argument(""--split_size"", nargs=\'?\', default=2, type=int, help=""number of clusters at each level"")\nparser.add_argument(""--n_iter"", nargs=\'?\', default=4, type=int, help=""number of levels of hierarchy"")\n\n# project\nparser.add_argument(""--pca"", nargs=\'?\', default=False, type=bool, help=""use pca for visualization"")\nparser.add_argument(""--pc_n_components"", nargs=\'?\', default=3, type=int, help=""n_components in PCA function"")\n\nparser.add_argument(""--tsne"", nargs=\'?\', default=True, type=bool, help=""use tsne for visualization"")\nparser.add_argument(""--ts_n_components"", nargs=\'?\', default=3, type=int, help=""n_components in TSNE function"")\nparser.add_argument(""--ts_perplexity"", nargs=\'?\', default=50, help=""perplexity n TSNE function"")\nparser.add_argument(""--ts_learning_rate"", nargs=\'?\', default=10, type=int, help=""learning_rate in TSNE function"")\nparser.add_argument(""--ts_n_iter"", nargs=\'?\', default=5000, type=int, help=""n_iter in TSNE function"")\n\n# metadata\nparser.add_argument(""--meta_labels"", nargs=\'?\', default=True, type=bool, help=""use labels in metadata"")\nparser.add_argument(""--meta_labels_file"", nargs=\'?\', default=km_labels_file, type=str, help=""labels file to be used in metadata"")\n\n# tensorboard\nparser.add_argument(""--log_dir"", nargs=\'?\', default=os.path.join(output_dir, ""tensorboard""), type=str, help=""log directory for TensorBoard"")\n\nparams = parser.parse_args()\n\n\nif __name__ == ""__main__"":\n\n    if params.mode == ""embed"":\n        embed(params)\n\n    if params.mode == ""sif"":\n        sif(params)\n\n    if params.mode == ""cluster"":\n        if params.kmeans:\n            kmeans(params)\n        if params.opt_k:\n            opt_k(params)\n        if params.hierarch_k:\n            hierarch_k(params)\n\n    if params.mode == ""project"":\n        if params.pca:\n            pca(params)\n        if params.tsne:\n            tsne(params)\n\n    if params.mode == ""metadata"":\n        # optional write cluster labels to metadata\n        if not params.meta_labels:\n            params.meta_labels_file = None\n        write_meta(params)\n\n    if params.mode == ""tensorboard"":\n        tensorboard(params)\n'"
meta.py,0,"b""import json\n\ndef write_meta(params):\n    '''\n    Write metadata of sentences (and labels) to a file.\n    :params.meta_labels_file: file of sentence labels\n    :params.sentence_file: file of sentences\n    :params.metadata_file: output file\n    '''\n\n    if not params.meta_labels_file == None:\n        sentences = [sentence.rstrip('\\n') for sentence in open(params.sentence_file)]\n\n        with open(params.meta_labels_file, 'r') as f:\n            labels = json.load(f) # from labels generated by clustering\n\n        meta = ['Sentence\\tLabel\\n']\n        meta.extend([s + '\\t' + str(l) + '\\n' for s, l in zip(sentences, labels)])\n    \n        with open(params.metadata_file, 'w') as f:\n            f.writelines(meta)\n\n    elif params.meta_labels_file == None:\n        with open(params.sentence_file, 'r') as f:\n            contents = f.read()\n        with open(params.metadata_file, 'w') as f:\n            f.write(contents)\n"""
project.py,4,"b""import numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\n\ndef pca(params):\n    '''\n    Computes PCA to reduce dimensionality.\n    :params.embedding_file: embeddings to reduce\n    :params.pca_file: output file for reduced embeddings\n    '''\n\n    vectors = np.load(params.embedding_file)\n\n    pc = PCA(n_components=params_pc_n_components)\n\n    vectors_pc = pc.fit_transform(vectors)\n    np.save(params.pca_file, vectors_pc)\n\ndef tsne(params):\n    '''\n    Computes t-SNE to reduce dimensionality.\n    :params.embedding_file: embeddings to reduce\n    :params.tsne_file: output file for reduced embeddings\n    '''\n\n    vectors = np.load(params.embedding_file)\n\n    ts = TSNE(n_components=params.ts_n_components,\n              perplexity=params.ts_perplexity,\n              learning_rate=params.ts_learning_rate,\n              n_iter=params.ts_n_iter)\n\n    vectors_ts = ts.fit_transform(vectors)\n    np.save(params.tsne_file, vectors_ts)\n"""
sif.py,2,"b'\'\'\'\nTaken from https://github.com/PrincetonML/SIF/blob/master/src/SIF_embedding.py\nPlace in pipeline: to better existing sentence embeddings to be used for post analysis\n\'\'\'\n\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\n\ndef compute_pc(vectors,npc=1):\n    """"""\n    Compute the principal components. DO NOT MAKE THE DATA ZERO MEAN!\n    :param vectors: vectors[i,:] is a data point\n    :param npc: number of principal components to remove\n    :return: component_[i,:] is the i-th pc\n    """"""\n\n    svd = TruncatedSVD(n_components=npc, n_iter=7, random_state=0)\n    svd.fit(vectors)\n    return svd.components_\n\ndef remove_pc(vectors, npc=1):\n    """"""\n    Remove the projection on the principal components\n    :param vectors: vectors[i,:] is a data point\n    :param npc: number of principal components to remove\n    :return: vectors_pc[i, :] is the data point after removing its projection\n    """"""\n\n    pc = compute_pc(vectors, npc)\n    if npc==1:\n        vectors_pc = vectors - vectors.dot(pc.transpose()) * pc\n    else:\n        vectors_pc = vectors - vectors.dot(pc.transpose()).dot(pc)\n    return vectors_pc\n\n\ndef sif(params):\n    """"""\n    Compute the scores between pairs of sentences using weighted average + removing the projection on the first principal component\n    :param params.embedding_file: numpy array of sentence embeddings\n    :param params.rmpc: if >0, remove the projections of the sentence embeddings to their first principal component\n    """"""\n\n    vectors = np.load(params.embedding_file)\n    if  params.sif_rmpc > 0:\n        vectors_pc = remove_pc(vectors, params.sif_rmpc)\n\n    np.save(params.sif_file, vectors_pc)\n'"
tensorboard.py,1,"b'import tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\nimport numpy as np\nimport os\n\ndef tensorboard(params):\n    \'\'\'\n    Create a TensorBoard instance for embedding visualization.\n    :params.log_dir: directory for output logs\n    :params.embedding_file: file for embeddings\n    :params.metadata_file: file for metadata\n    \'\'\'\n    LOG_DIR = params.log_dir\n    if not os.path.exists(LOG_DIR):\n        os.makedirs(LOG_DIR)\n\n    tmp = np.load(params.embedding_file)\n    embedding_var = tf.Variable(tmp, trainable=False, name=""embeddings"")\n\n    config = projector.ProjectorConfig()\n\n    # add one embedding\n    embedding = config.embeddings.add()\n    embedding.tensor_name = embedding_var.name\n\n    # link to metadata\n    embedding.metadata_path = params.metadata_file\n\n    # write to LOG_DIR\n    summary_writer = tf.summary.FileWriter(LOG_DIR)\n\n    # creates a projector_config.pbtxt in LOG_DIR\n    projector.visualize_embeddings(summary_writer, config)\n\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    saver.save(sess, os.path.join(LOG_DIR, ""embed.ckpt""), 1)\n'"
