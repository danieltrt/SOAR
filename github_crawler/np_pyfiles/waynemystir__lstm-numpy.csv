file_path,api_count,code
nl.py,69,"b'import numpy as np\n#%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython import display\nplt.style.use(\'seaborn-white\')\n\n\n# Read and process data\n\ndata = open(\'input.txt\', \'r\').read()\n\nchars = list(set(data))\ndata_size, X_size = len(data), len(chars)\nprint(""data has %d characters, %d unique"" % (data_size, X_size))\nchar_to_idx = {ch:i for i,ch in enumerate(chars)}\nidx_to_char = {i:ch for i,ch in enumerate(chars)}\n\n\n# Parameters\n\nH_size = 100 # Size of the hidden layer\nT_steps = 25 # Number of time steps (length of the sequence) used for training\nlearning_rate = 1e-1 # Learning rate\nweight_sd = 0.1 # Standard deviation of weights for initialization\nz_size = H_size + X_size # Size of concatenate(H, X) vector\n\n\n# Activations and derivs\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\ndef dsigmoid(y):\n    return y * (1 - y)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef dtanh(y):\n    return 1 - y * y\n\n\n# Init weights\n\nW_f = np.random.randn(H_size, z_size) * weight_sd + 0.5\nb_f = np.zeros((H_size, 1))\n\nW_i = np.random.randn(H_size, z_size) * weight_sd + 0.5\nb_i = np.zeros((H_size, 1))\n\nW_C = np.random.randn(H_size, z_size) * weight_sd\nb_C = np.zeros((H_size, 1))\n\nW_o = np.random.randn(H_size, z_size) * weight_sd + 0.5\nb_o = np.zeros((H_size, 1))\n\n#For final layer to predict the next character\nW_y = np.random.randn(X_size, H_size) * weight_sd\nb_y = np.zeros((X_size, 1))\n\n\n# Gradients\n\ndW_f = np.zeros_like(W_f)\ndW_i = np.zeros_like(W_i)\ndW_C = np.zeros_like(W_C)\n\ndW_o = np.zeros_like(W_o)\ndW_y = np.zeros_like(W_y)\n\ndb_f = np.zeros_like(b_f)\ndb_i = np.zeros_like(b_i)\ndb_C = np.zeros_like(b_C)\n\ndb_o = np.zeros_like(b_o)\ndb_y = np.zeros_like(b_y)\n\n\ndef forward(x, h_prev, C_prev):\n    assert x.shape == (X_size, 1)\n    assert h_prev.shape == (H_size, 1)\n    assert C_prev.shape == (H_size, 1)\n\n    z = np.row_stack((h_prev, x))\n    f = sigmoid(np.dot(W_f, z) + b_f)\n    i = sigmoid(np.dot(W_i, z) + b_i)\n    C_bar = tanh(np.dot(W_C, z) + b_C)\n\n    C = f * C_prev + i * C_bar\n    o = sigmoid(np.dot(W_o, z) + b_o)\n    h = o * tanh(C)\n\n    y = np.dot(W_y, h) + b_y\n    p = np.exp(y) / np.sum(np.exp(y))\n\n    return z, f, i, C_bar, C, o, h, y, p\n\ndef backward(target, dh_next, dC_next, C_prev, z, f, i, C_bar, C, o, h, y, p):\n\n    global dW_f, dW_i, dW_C, dW_o, dW_y\n    global db_f, db_i, db_C, db_o, db_y\n\n    assert z.shape == (X_size + H_size, 1)\n    assert y.shape == (X_size, 1)\n    assert p.shape == (X_size, 1)\n\n    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n        assert param.shape == (H_size, 1)\n\n    dy = np.copy(p)\n    dy[target] -= 1\n\n    dW_y += np.dot(dy, h.T)\n    db_y += dy\n\n    dh = np.dot(W_y.T, dy)\n    dh += dh_next\n    do = dh * tanh(C)\n    do = dsigmoid(o) * do\n    dW_o += np.dot(do, z.T)\n    db_o += do\n\n    dC = np.copy(dC_next)\n    dC += dh * o * dtanh(tanh(C))\n    dC_bar = dC * i\n    dC_bar = dC_bar * dtanh(C_bar)\n    dW_C += np.dot(dC_bar, z.T)\n    db_C += dC_bar\n\n    di = dC * C_bar\n    di = dsigmoid(i) * di\n    dW_i += np.dot(di, z.T)\n    db_i += di\n\n    df = dC * C_prev\n    df = dsigmoid(f) * df\n    dW_f += np.dot(df, z.T)\n    db_f += df\n\n    dz = np.dot(W_f.T, df) \\\n        + np.dot(W_i.T, di) \\\n        + np.dot(W_C.T, dC_bar) \\\n        + np.dot(W_o.T, do)\n    dh_prev = dz[:H_size, :]\n    dC_prev = f * dC\n\n    return dh_prev, dC_prev\n\ndef forward_backward(inputs, targets, h_prev, C_prev):\n    # To store the values for each time step\n    x_s, z_s, f_s, i_s, C_bar_s, C_s, o_s, h_s, y_s, p_s = {}, {}, {}, {}, {}, {}, {}, {}, {}, {}\n\n    # Values at t - 1\n    h_s[-1] = np.copy(h_prev)\n    C_s[-1] = np.copy(C_prev)\n\n    loss = 0\n    # Loop through time steps\n    assert len(inputs) == T_steps\n    for t in range(len(inputs)):\n        x_s[t] = np.zeros((X_size, 1))\n        x_s[t][inputs[t]] = 1 # Input character\n\n        z_s[t], f_s[t], i_s[t], C_bar_s[t], C_s[t], o_s[t], h_s[t], y_s[t], p_s[t] \\\n            = forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n\n        loss += -np.log(p_s[t][targets[t], 0]) # Loss for at t\n\n\n    for dparam in [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y]:\n        dparam.fill(0)\n\n    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n\n    for t in reversed(range(len(inputs))):\n        # Backward pass\n        dh_next, dC_next = backward(target = targets[t], dh_next = dh_next, dC_next = dC_next, C_prev = C_s[t-1],\n                 z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t], C = C_s[t], o = o_s[t],\n                 h = h_s[t], y = y_s[t], p = p_s[t])\n\n    # Clip gradients to mitigate exploding gradients\n    for dparam in [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y]:\n        np.clip(dparam, -1, 1, out=dparam)\n\n    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]\n\ndef sample(h_prev, C_prev, first_char_idx, sentence_length):\n    x = np.zeros((X_size, 1))\n    x[first_char_idx] = 1\n\n    h = h_prev\n    C = C_prev\n\n    indexes = []\n\n    for t in range(sentence_length):\n        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n        idx = np.random.choice(range(X_size), p=p.ravel())\n        x = np.zeros((X_size, 1))\n        x[idx] = 1\n        indexes.append(idx)\n\n    return indexes\n\ndef update_status(inputs, h_prev, C_prev):\n    #initialized later\n    global plot_iter, plot_loss\n    global smooth_loss\n\n    # Get predictions for 200 letters with current model\n    display.clear_output(wait=True)\n\n    sample_idx = sample(h_prev, C_prev, inputs[0], 2000)\n    txt = \'\'.join(idx_to_char[idx] for idx in sample_idx)\n\n    # Clear and plot\n    plt.plot(plot_iter, plot_loss)\n    display.display(plt.gcf())\n\n    #Print prediction and loss\n    print(""----\\n %s \\n----"" % (txt, ))\n    print(""iter %d, loss %f"" % (iteration, smooth_loss))\n\n\n# Memory variables for Adagrad\n\nmW_f = np.zeros_like(W_f)\nmW_i = np.zeros_like(W_i)\nmW_C = np.zeros_like(W_C)\nmW_o = np.zeros_like(W_o)\nmW_y = np.zeros_like(W_y)\n\nmb_f = np.zeros_like(b_f)\nmb_i = np.zeros_like(b_i)\nmb_C = np.zeros_like(b_C)\nmb_o = np.zeros_like(b_o)\nmb_y = np.zeros_like(b_y)\n\n\n# Exponential average of loss\n# Initialize to a error of a random model\nsmooth_loss = -np.log(1.0 / X_size) * T_steps\n\niteration, p = 0, 0\n\n# For the graph\nplot_iter = np.zeros((0))\nplot_loss = np.zeros((0))\n\n\nwhile True:\n    # Try catch for interruption\n    try:\n        # Reset\n        if p + T_steps >= len(data) or iteration == 0:\n            g_h_prev = np.zeros((H_size, 1))\n            g_C_prev = np.zeros((H_size, 1))\n            p = 0\n\n\n        inputs = [char_to_idx[ch] for ch in data[p: p + T_steps]]\n        targets = [char_to_idx[ch] for ch in data[p + 1: p + T_steps + 1]]\n#        print(""INPUTS typ({}) len({}) ins({})"".format(type(inputs), len(inputs), inputs))\n\n        loss, g_h_prev, g_C_prev =  forward_backward(inputs, targets, g_h_prev, g_C_prev)\n        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n\n        # Print every hundred steps\n        if iteration % 100 == 0:\n            update_status(inputs, g_h_prev, g_C_prev)\n\n\n        # Update weights\n        for param, dparam, mem in zip([W_f, W_i, W_C, W_o, W_y, b_f, b_i, b_C, b_o, b_y],\n                                      [dW_f, dW_i, dW_C, dW_o, dW_y, db_f, db_i, db_C, db_o, db_y],\n                                      [mW_f, mW_i, mW_C, mW_o, mW_y, mb_f, mb_i, mb_C, mb_o, mb_y]):\n            mem += dparam * dparam # Calculate sum of gradients\n            #print(learning_rate * dparam)\n            param += -(learning_rate * dparam / np.sqrt(mem + 1e-8))\n\n        plot_iter = np.append(plot_iter, [iteration])\n        plot_loss = np.append(plot_loss, [loss])\n\n        p += T_steps\n        iteration += 1\n    except KeyboardInterrupt:\n        update_status(inputs, g_h_prev, g_C_prev)\n        plt.show()\n        break\n\n'"
