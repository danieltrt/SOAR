file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n\nfrom setuptools import find_packages, setup, Extension\nexec(open(\'./mobula/version.py\').read())\n\nsetup(\n    name=""MobulaOP"",\n    version=__version__,\n    url=\'https://github.com/wkcn/mobulaop\',\n    description=\'A Simple & Flexible Cross Framework Operators Toolkit\',\n    packages=find_packages(exclude=(\'tests\',)),\n    install_requires=[\n        \'numpy\',\n        \'portalocker\',\n    ],\n    tests_require=[\n        \'nose\',\n        \'coveralls\'\n    ],\n)\n'"
benchmark/benchmark_empty_op.py,3,"b""import mobula\nimport os\nimport time\nimport mxnet as mx\nimport numpy as np\nmobula.op.load('./EmptyOP', path=os.path.dirname(__file__))\nEmptyOP = mobula.op.EmptyOP\n\nT = np.float32\nmobula.func.empty_forward.build('cpu', ['float'])\n\nTIMES = 3000\n\n\ndef test_empty_op():\n    a = np.array([1, 2, 3], dtype=T)\n    tic = time.time()\n    op = EmptyOP[np.ndarray]()\n    for _ in range(TIMES):\n        a = op(a)\n    toc = time.time()\n\n    used_time = toc - tic\n    print('Used Time(s): %.3f' % used_time)\n\n\ndef test_hybrid_op():\n    class TestBlock(mx.gluon.block.HybridBlock):\n        def hybrid_forward(self, F, x):\n            return EmptyOP(x)\n    block = TestBlock()\n    a = mx.nd.array([1, 2, 3], dtype=T)\n\n    # prepare\n    for _ in range(10):\n        a = block(a)\n    mx.nd.waitall()\n\n    a = mx.nd.array([1, 2, 3], dtype=T)\n    tic = time.time()\n    for _ in range(TIMES):\n        a = block(a)\n    mx.nd.waitall()\n    toc = time.time()\n    used_time = toc - tic\n    print('Imperative Time(s): %.3f' % used_time)\n\n    block.hybridize()\n    # prepare\n    for _ in range(10):\n        a = block(a)\n    mx.nd.waitall()\n\n    a = mx.nd.array([1, 2, 3], dtype=T)\n    tic = time.time()\n    for _ in range(TIMES):\n        a = block(a)\n    mx.nd.waitall()\n    toc = time.time()\n    used_time = toc - tic\n    print('Symbolic Time(s): %.3f' % used_time)\n\n\nif __name__ == '__main__':\n    test_empty_op()\n    test_hybrid_op()\n"""
benchmark/benchmark_roi_align.py,4,"b'import time\nimport mxnet as mx\nimport numpy as np\nimport mobula\nfrom mobula.testing import assert_almost_equal\n\nmobula.op.load(\'ROIAlign\')\n\nT = np.float32\n\n\ndef test_roi_align_sym(op, times):\n    dtype = np.float32\n\n    N, C, H, W = 2, 1024, 14, 14\n    num_rois = 512\n\n    data = np.arange(N * C * H * W).astype(dtype).reshape((N, C, H, W))\n    rois = np.empty((num_rois, 5))\n    for i in range(num_rois):\n        rois[i] = [i % N, 0, 0, i % H, i % W]\n\n    data_sym = mx.sym.Variable(\'data\')\n    rois_sym = mx.sym.Variable(\'rois\')\n\n    output_sym = mobula.op.ROIAlign(data=data_sym, rois=rois_sym, pooled_size=(\n        2, 2), spatial_scale=1.0, sampling_ratio=1)\n    output_sym = mx.sym.MakeLoss(output_sym)\n\n    exe = output_sym.simple_bind(\n        ctx=mx.context.current_context(), data=data.shape, rois=rois.shape)\n    exe.forward(data=data, rois=rois)\n    res = exe.outputs[0].asnumpy()\n    for t in range(times):\n        if t == 1:\n            tic = time.time()\n        exe.forward(data=data, rois=rois)\n        res = exe.outputs[0].asnumpy()\n        exe.backward()\n        mx.nd.waitall()\n    cost = time.time() - tic\n    return cost\n\n\ndef test_roi_align():\n    TIMES = 50\n    for op in [mobula.op.ROIAlign, mx.sym.contrib.ROIAlign]:\n        cost = test_roi_align_sym(op, TIMES)\n        print(op, cost)\n\n\nif __name__ == \'__main__\':\n    print(""===cpu==="")\n    test_roi_align()\n\n    if mobula.utils.list_gpus():\n        print(""===gpu==="")\n        ctx = mx.gpu(0)\n        mx.test_utils.set_default_context(ctx)\n        test_roi_align()\n'"
examples/ConstantOP.py,2,"b""'''\nNotice:\n    ConstantOP only supports CPU.\n    For supporting cross-device, please use ConstantOP2\n'''\nimport sys\nsys.path.append('../')  # Add MobulaOP path\nimport mobula\nimport numpy as np\n\n# ConstantOP only supports CPU.\n\n\n@mobula.op.register(need_top_grad=False)\nclass ConstantOP:\n    def __init__(self, constant):\n        self.constant = self.F.array(constant)\n\n    def forward(self):\n        return self.constant\n\n    def backward(self, dy):\n        return []\n\n    def infer_shape(self, in_shape):\n        return [], [self.constant.shape]\n\n\n@mobula.op.register(need_top_grad=False)\nclass ConstantOP2:\n    def __init__(self, constant):\n        self.constant = self.F.array(constant)\n        self.constant_buffer = dict()\n\n    def forward(self, x):\n        ctx = x.context\n        return self.constant_buffer.get(ctx, self.constant.as_in_context(ctx))\n\n    def backward(self, dy):\n        return [0]\n\n    def infer_shape(self, in_shape):\n        return in_shape, [self.constant.shape]\n\n\nif __name__ == '__main__':\n    import mxnet as mx\n    import numpy as np\n\n    # ConstantOP only supports CPU.\n    if mx.current_context() == mx.cpu():\n        # NDArray\n        a = mx.nd.array([1, 2, 3])\n        b = mx.nd.array([4, 5, 6])\n        c = a + ConstantOP[mx.nd.NDArray](b)\n        print(c)  # [5,7,9]\n\n        # Symbol\n        a_sym = mx.sym.Variable('a')\n        output_sym = a_sym + ConstantOP[mx.sym.Symbol](b)\n        exe = output_sym.simple_bind(\n            ctx=mx.context.current_context(), a=a.shape)\n        exe.forward(a=np.array([1, 2, 3]))\n\n        print(exe.outputs[0].asnumpy())  # [5,7,9]\n\n    '''\n    ConstantOP2: accept a variable for getting the context information\n    '''\n\n    # NDArray\n    a = mx.nd.array([1, 2, 3])\n    b = mx.nd.array([4, 5, 6])\n    c = a + ConstantOP2(a, constant=b)\n    print(c)  # [5,7,9]\n\n    # Symbol\n    a_sym = mx.sym.Variable('a')\n    # declare input_type explicitly because the inputs includes mx.sym.Symbol and mx.nd.NDArray\n    output_sym = a_sym + ConstantOP2[mx.sym.Symbol](a_sym, constant=b)\n    exe = output_sym.simple_bind(ctx=mx.context.current_context(), a=a.shape)\n    exe.forward(a=np.array([1, 2, 3]))\n\n    print(exe.outputs[0].asnumpy())  # [5,7,9]\n"""
examples/MyFirstOP.py,6,"b""import sys\nsys.path.append('../')  # Add MobulaOP path\nimport mobula\n\n\n@mobula.op.register\nclass MyFirstOP:\n    def forward(self, x, y):\n        return x + y\n\n    def backward(self, dy):\n        return [dy, dy]\n\n    def infer_shape(self, in_shape):\n        assert in_shape[0] == in_shape[1]\n        return in_shape, [in_shape[0]]\n\n\ntry:\n    import mxnet as mx\n    print('MXNet:')\n    print('mx.nd.NDArray:')\n    a = mx.nd.array([1, 2, 3])\n    b = mx.nd.array([4, 5, 6])\n    c = MyFirstOP(a, b)\n    print('a + b = c\\n{} + {} = {}\\n'.format(a.asnumpy(),\n                                             b.asnumpy(), c.asnumpy()))  # [5, 7, 9]\n\n    if hasattr(mx, 'numpy'):\n        # MXNet NumPy-compatible API\n        print('mx.np.ndarray:')\n        a = mx.np.array([1, 2, 3])\n        b = mx.np.array([4, 5, 6])\n        c = MyFirstOP(a, b)\n        print('a + b = c\\n{} + {} = {}\\n'.format(a.asnumpy(),\n                                                 b.asnumpy(), c.asnumpy()))  # [5, 7, 9]\nexcept ImportError:\n    pass\n\ntry:\n    import numpy as np\n    print('NumPy:')\n    a = np.array([1, 2, 3])\n    b = np.array([4, 5, 6])\n    op = MyFirstOP[np.ndarray]()\n    c = op(a, b)\n    print('a + b = c\\n{} + {} = {}\\n'.format(a, b, c))  # [5, 7, 9]\nexcept ImportError:\n    pass\n\ntry:\n    import torch\n    print('PyTorch:')\n    a = torch.tensor([1, 2, 3])\n    b = torch.tensor([4, 5, 6])\n    c = MyFirstOP(a, b)\n    print('a + b = c\\n{} + {} = {}\\n'.format(a, b, c))  # [5, 7, 9]\nexcept ImportError:\n    pass\n\ntry:\n    import cupy as cp\n    print('CuPy:')\n    a = cp.array([1, 2, 3])\n    b = cp.array([4, 5, 6])\n    op = MyFirstOP[cp.ndarray]()\n    c = op(a, b)\n    print('a + b = c\\n{} + {} = {}\\n'.format(a, b, c))  # [5, 7, 9]\nexcept ImportError:\n    pass\n"""
examples/RunROIAlign.py,3,"b""# Use ROIAlign operator\nimport sys\nsys.path.append('../')  # Add MobulaOP path\nimport mxnet as mx\nimport numpy as np\nimport mobula\n# Load ROIAlign Module\nmobula.op.load('ROIAlign')\n\nctx = mx.cpu(0)\ndtype = np.float32\nN, C, H, W = 2, 3, 4, 4\n\ndata = mx.nd.array(\n    np.arange(N * C * H * W).astype(dtype).reshape((N, C, H, W)))\nrois = mx.nd.array(np.array([[0, 1, 1, 3, 3]], dtype=dtype))\n\ndata = data.as_in_context(ctx)\nrois = rois.as_in_context(ctx)\n\ndata.attach_grad()\nwith mx.autograd.record():\n    # mx.nd.NDArray and mx.sym.Symbol are both available as the inputs.\n    output = mobula.op.ROIAlign(data=data, rois=rois, pooled_size=(\n        2, 2), spatial_scale=1.0, sampling_ratio=1)\n\noutput.backward()\n\nprint(output.asnumpy(), data.grad.asnumpy())\n"""
examples/TVMOp.py,0,"b'"""""" Example for using TVM generated function """"""\nimport sys\nsys.path.append(\'../\')  # Add MobulaOP path\nimport mobula\n\nimport tvm\nimport topi\nto_dummy_func = lambda *args, **kwargs: None\ntry:\n    import mxnet as mx\n    from tvm.contrib.mxnet import to_mxnet_func\nexcept ImportError:\n    to_mxnet_func = to_dummy_func\n\ntry:\n    import torch\n    from tvm.contrib.dlpack import to_pytorch_func\nexcept ImportError:\n    to_pytorch_func = to_dummy_func\n\n\ndef get_tvm_add():\n    # define compute\n    n = tvm.var(\'n\')\n    A = tvm.placeholder(n, name=\'A\', dtype=\'float32\')\n    B = tvm.placeholder(n, name=\'B\', dtype=\'float32\')\n    C = tvm.compute((n,), lambda i: A[i] + B[i], name=\'C\')\n\n    # build function (with parallel support)\n    with tvm.target.create(\'llvm\'):\n        s = topi.generic.schedule_injective([C])\n        func_cpu = tvm.build(s, [A, B, C])\n\n    if mobula.utils.list_gpus():\n        with tvm.target.create(\'cuda\'):\n            s = topi.generic.schedule_injective([C])\n            func_gpu = tvm.build(s, [A, B, C])\n    else:\n        func_gpu = None\n\n    return func_cpu, func_gpu\n\n\n@mobula.op.register\nclass TVMAddOp:\n    def __init__(self):\n        func_cpu, func_gpu = get_tvm_add()\n\n        self.func = {\n            \'mx\': {\n                \'cpu\': to_mxnet_func(func_cpu, const_loc=[0, 1]),\n                \'gpu\': to_mxnet_func(func_gpu, const_loc=[0, 1]) if func_gpu is not None else None,\n            },\n            \'th\': {\n                \'cpu\': to_pytorch_func(func_cpu),\n                \'gpu\': to_pytorch_func(func_gpu) if func_gpu is not None else None,\n            }\n        }\n\n    def forward(self, x, y):\n        glue_mod = mobula.glue.backend.get_var_glue(x)\n        backend = (glue_mod.__name__.split(\'.\')[-1])\n        device_type = \'cpu\' if glue_mod.dev_id(x) is None else \'gpu\'\n\n        self.func[backend][device_type](x, y, self.Y[0])\n\n    def backward(self, dy):\n        return [dy, dy]\n\n    def infer_shape(self, in_shape):\n        assert in_shape[0] == in_shape[1]\n        return in_shape, [in_shape[0]]\n\n\ntry:\n    import mxnet as mx\n    print(\'===== MXNet =====\')\n    ctxs = [mx.cpu()]\n    if mobula.utils.list_gpus():\n        ctxs.append([mx.gpu()])\n    for ctx in ctxs:\n        print(ctx)\n        a = mx.nd.array([1.0, 2.0, 3.0], ctx=ctx)\n        b = mx.nd.array([4.0, 5.0, 6.0], ctx=ctx)\n        c = TVMAddOp(a, b)\n        print(\'a + b = c\\n{} + {} = {}\\n\'.format(a.asnumpy(),\n                                                 b.asnumpy(), c.asnumpy()))  # [5.0, 7.0, 9.0]\nexcept ImportError:\n    pass\n\ntry:\n    import torch\n    print(\'===== PyTorch =====\')\n    devices = [torch.device(\'cpu\')]\n    if mobula.utils.list_gpus():\n        devices.append([torch.device(\'gpu\')])\n    for device in devices:\n        print(device)\n        a = torch.tensor([1.0, 2.0, 3.0], device=device)\n        b = torch.tensor([4.0, 5.0, 6.0], device=device)\n        c = TVMAddOp(a, b)\n        print(\'a + b = c\\n{} + {} = {}\\n\'.format(a, b, c))  # [5.0, 7.0, 9.0]\nexcept ImportError:\n    pass\n'"
mobula/__init__.py,0,b'from .version import __version__\nfrom . import func\nfrom . import op\nfrom . import testing\nfrom .config import config\n\nfunction = func\noperator = op\n'
mobula/build.py,0,"b'""""""Building Implementation""""""\nimport os\nimport multiprocessing\ntry:\n    from .build_utils import *\nexcept Exception:\n    from build_utils import *\n\nNUM_CPU_CORE = multiprocessing.cpu_count()\nHOST_NUM_THREADS = config.HOST_NUM_THREADS if config.HOST_NUM_THREADS > 0 else NUM_CPU_CORE\nCOMMON_FLAGS = Flags().add_definition(\'HOST_NUM_THREADS\', HOST_NUM_THREADS)\n\nif config.USING_OPTIMIZATION:\n    COMMON_FLAGS.add_string(\'-O3\')\nif config.DEBUG:\n    COMMON_FLAGS.add_string(\'-g\')\nCOMMON_FLAGS.add_definition(\'USING_CBLAS\', config.USING_CBLAS)\nINC_PATHS.extend([\'./inc\', \'../3rdparty/dlpack/include\',\n                  \'../3rdparty/tvm_packed_func\'])\nfor path in INC_PATHS:\n    p = os.path.join(ENV_PATH, path)\n    if p:\n        COMMON_FLAGS.add_string(\'-I{}\'.format(p))\n\nCFLAGS = Flags(\'-std=c++11\').add_definition(\'USING_CUDA\', 0).\\\n    add_definition(\'USING_HIP\', 0).add_definition(\'USING_OPENMP\', config.USING_OPENMP).\\\n    add_string(COMMON_FLAGS)\nif not OS_IS_WINDOWS:\n    CFLAGS.add_string(\'-fPIC\')\nLDFLAGS = Flags(\'-lpthread -shared\')\nif config.USING_CBLAS:\n    LDFLAGS.add_string(\'-lopenblas\')\n\nCU_FLAGS = Flags(\'-std=c++11 -x cu -Wno-deprecated-gpu-targets -dc \\\n-D_MWAITXINTRIN_H_INCLUDED -D_FORCE_INLINES --expt-extended-lambda\').\\\n    add_definition(\'USING_CUDA\', 1).\\\n    add_definition(\'USING_HIP\', 0).\\\n    add_string(COMMON_FLAGS)\nif not OS_IS_WINDOWS:\n    CU_FLAGS.add_string(\'--compiler-options ""-fPIC""\')\nCU_LDFLAGS = Flags(\'-shared -Wno-deprecated-gpu-targets \\\n-L%s/lib64 -lcuda -lcudart\' % config.CUDA_DIR)\nif config.USING_CBLAS:\n    CU_LDFLAGS.add_string(\'-lcublas\')\n\nHIP_FLAGS = Flags(\'-std=c++11 -Wno-deprecated-gpu-targets -Wno-deprecated-declarations -dc \\\n--expt-extended-lambda\').\\\n    add_definition(\'USING_CUDA\', 0).\\\n    add_definition(\'USING_HIP\', 1).\\\n    add_string(COMMON_FLAGS)\nif not OS_IS_WINDOWS:\n    HIP_FLAGS.add_string(\'--compiler-options ""-fPIC""\')\nHIP_LDFLAGS = Flags(\'-shared -Wno-deprecated-gpu-targets\')\nif config.USING_CBLAS:\n    HIP_LDFLAGS.add_string(\'-lhipblas\')\n\nif config.USING_OPENMP:\n    CFLAGS.add_string(\'-fopenmp\')\n    LDFLAGS.add_string(\'-fopenmp\')\n\nif config.USING_HIGH_LEVEL_WARNINGS:\n    CFLAGS.add_string(\'-Werror -Wall -Wextra -pedantic -Wcast-align -Wcast-qual \\\n-Wctor-dtor-privacy -Wdisabled-optimization -Wformat=2 -Winit-self -Wmissing-include-dirs \\\n-Wold-style-cast -Woverloaded-virtual -Wredundant-decls -Wshadow \\\n-Wsign-promo -Wundef -fdiagnostics-show-option\')\n\n\ndef source_to_o(build_path, src_obj, compiler=config.CXX, cflags=CFLAGS):\n    mkdir(build_path)\n    existed_dirs = set()\n    updated = False\n    commands = []\n    for src, obj in src_obj:\n        dir_name, obj_name = os.path.split(obj)\n        build_dir_name = os.path.join(build_path, dir_name)\n        build_name = os.path.join(build_path, dir_name, obj_name)\n        if file_is_latest(src) and os.path.exists(build_name):\n            continue\n        updated = True\n        if build_dir_name not in existed_dirs:\n            mkdir(build_dir_name)\n            existed_dirs.add(build_dir_name)\n        if OS_IS_WINDOWS and not command_exists(compiler):\n            inc_flags = Flags()\n            for path in INC_PATHS:\n                p = os.path.join(ENV_PATH, path)\n                inc_flags.add_string(\'-I{}\'.format(p))\n            cflags_sp = str(cflags).split()\n            def_flags = \' \'.join(\n                [s for s in cflags_sp if len(s) > 2 and s[:2] == \'-D\'])\n            command = \'cl /EHsc /O2 %s %s -c %s -Fo%s\' % (\n                def_flags, inc_flags, src, build_name)\n        else:\n            command = \'%s %s %s -c -o %s\' % (compiler, src, cflags, build_name)\n        commands.append(command)\n    run_command_parallel(commands)\n    return updated\n\n\ndef o_to_so(target_name, objs, linker, ldflags=LDFLAGS):\n    if OS_IS_WINDOWS and not command_exists(linker):\n        command = \'link -DLL %s -out:%s\' % (\' \'.join(objs), target_name)\n    else:\n        command = \'%s %s %s -o %s\' % (linker,\n                                      \' \'.join(objs), ldflags, target_name)\n    run_command(command)\n\n\ndef source_to_so(build_path, srcs, target_name, compiler, cflags, ldflags, buildin_o=None):\n    objs = change_exts(srcs, [(\'cpp\', \'o\')])\n    if source_to_o(build_path, zip(srcs, objs), compiler, cflags) or\\\n            not os.path.exists(target_name):\n        if buildin_o is not None:\n            objs.extend(buildin_o)\n        abs_objs = add_path(build_path, objs)\n        o_to_so(target_name, abs_objs, compiler, ldflags)\n\n\nBUILD_FLAGS = dict(\n    cpu=(config.CXX, CFLAGS, LDFLAGS),\n    cuda=(config.NVCC, CU_FLAGS, CU_LDFLAGS),\n    hip=(config.HIPCC, HIP_FLAGS, HIP_LDFLAGS)\n)\n\n\ndef source_to_so_ctx(build_path, srcs, target_name, ctx_name):\n    assert ctx_name in BUILD_FLAGS, ValueError(\n        \'Unsupported Context: {} -(\'.format(ctx_name))\n    flags = BUILD_FLAGS[ctx_name]\n    compiler, cflags, ldflags = flags[:3]\n\n    buildin_path = os.path.join(ENV_PATH, config.BUILD_PATH, ctx_name)\n    buildin_o = []\n    buildin_cpp = []\n    for src in [\'defines.cpp\', \'context.cpp\']:\n        fname = os.path.join(\'src\', src)\n        buildin_o.append(os.path.join(buildin_path, fname))\n        buildin_cpp.append(os.path.join(ENV_PATH, fname))\n    buildin_o = change_exts(buildin_o, [(\'cpp\', \'o\')])\n\n    for fname in buildin_o:\n        if not os.path.exists(fname):\n            with build_context():\n                source_to_o(build_path, zip(\n                    buildin_cpp, buildin_o), compiler, cflags)\n    flags += (buildin_o, )\n    source_to_so(build_path, srcs, target_name, *flags)\n'"
mobula/build_utils.py,0,"b'""""""Building Utils""""""\n__all__ = [""pass_argv"", ""get_include_file"", ""wildcard"",\n           ""change_ext"", ""change_exts"", ""mkdir"", ""rmdir"", ""add_path"",\n           ""file_is_changed"", ""file_is_latest"",\n           ""run_command"", ""run_command_parallel"", ""command_exists"",\n           ""config"", ""Flags"", ""INC_PATHS"", ""ENV_PATH"",\n           ""OS_IS_WINDOWS"", ""OS_IS_LINUX"", ""build_context""]\n\nfrom .config import config\nfrom .utils import makedirs\nimport ast\nimport os\nimport threading\nimport platform\nimport re\nfrom subprocess import Popen, PIPE\ntry:\n    import Queue\nexcept ImportError:\n    import queue as Queue\nif not hasattr(Queue.Queue, \'clear\'):\n    def _queue_clear(self):\n        with self.mutex:\n            self.queue.clear()\n    setattr(Queue.Queue, \'clear\', _queue_clear)\n\nOS_NAME = platform.system()\nOS_IS_WINDOWS = OS_NAME == \'Windows\'\nOS_IS_LINUX = OS_NAME in [\'Linux\', \'Darwin\']\nassert OS_IS_WINDOWS or OS_IS_LINUX,\\\n    Exception(\'Unsupported Operator System: {}\'.format(OS_NAME))\n\nINC_PATHS = [\'./\']\n\n# Load Config File\nENV_PATH = os.path.dirname(__file__)\nif not os.path.dirname(config.BUILD_PATH):\n    config.BUILD_PATH = os.path.join(ENV_PATH, config.BUILD_PATH)\n\n\ndef pass_argv(argv):\n    """"""Read Config from argv""""""\n    for p in argv:\n        if p[0] == \'-\' and \'=\' in p:\n            k, v = p[1:].split(\'=\')\n            k = k.strip()\n            assert hasattr(config, k), KeyError(\n                \'Key `%s` not found in config\' % k)\n            setattr(config, k, ast.literal_eval(v))\n            print(\'Set %s to %s\' % (k, v))\n\n\ndef save_code_hash(obj, fname):\n    with open(fname, \'w\') as f:\n        for k, v in obj.items():\n            f.write(\'%s %s\\n\' % (k, v))\n\n\ndef load_code_hash(fname):\n    data = dict()\n    try:\n        with open(fname, \'r\') as f:\n            for line in f:\n                sp = line.split(\' \')\n                data[sp[0]] = sp[1].strip()\n    except Exception:\n        pass\n    return data\n\n\ndef save_dependant(obj, fname):\n    with open(fname, \'w\') as f:\n        for k, v in obj.items():\n            if v:\n                s = \'{} {}\\n\'.format(k, \',\'.join(v))\n                f.write(s)\n\n\ndef load_dependant(fname):\n    data = dict()\n    try:\n        with open(fname, \'r\') as f:\n            for line in f:\n                sp = line.strip().split(\' \')\n                data[sp[0]] = sp[1].split(\',\')\n    except Exception:\n        pass\n    return data\n\n\ndef update_build_path(build_path):\n    global code_hash, code_hash_filename, code_hash_updated\n    global dependant, dependant_filename, dependant_updated\n\n    makedirs(build_path, exist_ok=True)\n\n    config.BUILD_PATH = build_path\n\n    code_hash = dict()\n    code_hash_filename = os.path.join(config.BUILD_PATH, \'code.hash\')\n    if os.path.exists(code_hash_filename):\n        code_hash = load_code_hash(code_hash_filename)\n    code_hash_updated = False\n\n    dependant = dict()\n    dependant_filename = os.path.join(config.BUILD_PATH, \'code.dependant\')\n    if os.path.exists(dependant_filename):\n        dependant = load_dependant(dependant_filename)\n    dependant_updated = False\n\n\nupdate_build_path(os.path.join(ENV_PATH, config.BUILD_PATH))\n\n\ndef build_exit():\n    if code_hash_updated:\n        save_code_hash(code_hash, code_hash_filename)\n    if dependant_updated:\n        save_dependant(dependant, dependant_filename)\n\n\nclass build_context:\n    def __enter__(self):\n        pass\n\n    def __exit__(self, *dummy):\n        build_exit()\n\n\nclass Flags:\n    def __init__(self, s=\'\'):\n        self.flags = s\n\n    def add_definition(self, key, value):\n        if isinstance(value, bool):\n            value = int(value)\n        self.flags += \' -D%s=%s\' % (key, str(value))\n        return self\n\n    def add_string(self, s):\n        self.flags += \' %s\' % str(s)\n        return self\n\n    def __str__(self):\n        return self.flags\n\n\nINCLUDE_FILE_REG = re.compile(r\'^\\s*#include\\s*(?:""|<)\\s*(.*?)\\s*(?:""|>)\\s*\')\n\n\ndef get_include_file(fname):\n    res = []\n    for line in open(fname):\n        u = INCLUDE_FILE_REG.search(line)\n        if u is not None:\n            inc_fname = u.groups()[0]\n            res.append(inc_fname)\n    return res\n\n\ndef wildcard(path, ext):\n    if isinstance(path, (list, tuple)):\n        res = []\n        for p in path:\n            res.extend(wildcard(p, ext))\n        return res\n    res = []\n    for name in os.listdir(path):\n        e = os.path.splitext(name)[1]\n        if e == \'.\' + ext:\n            res.append(os.path.join(path, name))\n    return res\n\n\ndef change_exts(lst, rules):\n    res = []\n    mappings = dict(rules)\n    for name in lst:\n        sp = os.path.splitext(name)\n        if sp[1] and sp[1][0] == \'.\':\n            ext = sp[1][1:]\n            if ext in mappings:\n                new_ext = mappings[ext]\n                name = sp[0] + \'.\' + new_ext\n        res.append(name)\n    return res\n\n\ndef change_ext(lst, origin, target):\n    return change_exts(lst, [(origin, target)])\n\n\ndef run_command(command):\n    print(command)\n    return os.system(command)\n\n\ndef mkdir(dir_name):\n    if not os.path.exists(dir_name):\n        print(\'mkdir -p %s\' % dir_name)\n        makedirs(dir_name, exist_ok=True)\n\n\nif OS_IS_LINUX:\n    rmdir_command = \'rm -rf\'\nelif OS_IS_WINDOWS:\n    rmdir_command = \'rd /s /q\'\n\n\ndef rmdir(dir_name):\n    # we use shell command to remove the non-empty or empry directory\n    if os.path.exists(dir_name):\n        command = \'%s %s\' % (rmdir_command, dir_name)\n        run_command(command)\n\n\ndef get_file_hash(fname):\n    return str(int(os.path.getmtime(fname)))\n\n\ndef file_is_changed(fname):\n    fname = os.path.abspath(fname)\n    global code_hash_updated\n    new_hash = get_file_hash(fname)\n    if fname not in code_hash or new_hash != code_hash[fname]:\n        code_hash_updated = True\n        code_hash[fname] = new_hash\n        return True\n    return False\n\n\ndef update_file_hash(fname):\n    fname = os.path.abspath(fname)\n    global code_hash_updated\n    new_hash = get_file_hash(fname)\n    if fname not in code_hash or new_hash != code_hash[fname]:\n        code_hash_updated = True\n        code_hash[fname] = new_hash\n\n\ndef find_include(inc):\n    for path in INC_PATHS:\n        fname = os.path.relpath(os.path.join(\n            ENV_PATH, path, inc), start=ENV_PATH)\n        if os.path.exists(fname):\n            return fname\n    return None\n\n\ndef is_c_file(fname):\n    return os.path.splitext(fname)[-1] not in [\'.cpp\', \'.c\', \'.cu\']\n\n\ndef update_dependant(fname):\n    if is_c_file(fname):\n        return\n    fname = os.path.abspath(fname)\n    global dependant_updated\n    dependant_updated = True\n    inc_files = get_include_file(fname)\n    res = []\n    for inc in inc_files:\n        inc_fname = find_include(inc)\n        if inc_fname is not None:\n            inc_fname = os.path.abspath(inc_fname)\n            res.append(inc_fname)\n    dependant[fname] = res\n\n\ndef dependant_changed(fname):\n    if is_c_file(fname):\n        return False\n    fname = os.path.abspath(fname)\n    if fname not in dependant:\n        update_dependant(fname)\n    includes = dependant[fname]\n    changed = False\n    for inc in includes:\n        inc_fname = find_include(inc)\n        if inc_fname is not None:\n            abs_inc_fname = os.path.abspath(inc_fname)\n            if not file_is_latest(abs_inc_fname):\n                changed = True\n    return changed\n\n\nFILE_CHECK_LIST = dict()\n\n\ndef file_is_latest(source):\n    source = os.path.abspath(source)\n    if source in FILE_CHECK_LIST:\n        t = FILE_CHECK_LIST[source]\n        assert t is not None, RuntimeError(\n            \'Error: Cycle Reference {}\'.format(source))\n        return t\n    FILE_CHECK_LIST[source] = None\n    latest = True\n    if file_is_changed(source):\n        latest = False\n        update_dependant(source)\n    if dependant_changed(source):\n        latest = False\n    FILE_CHECK_LIST[source] = latest\n    return latest\n\n\ndef run_command_parallel(commands, allow_error=False):\n    command_queue = Queue.Queue()\n    info_queue = Queue.Queue()\n    for c in commands:\n        command_queue.put(c)\n    max_worker_num = min(config.MAX_BUILDING_WORKER_NUM, len(commands))\n    for _ in range(max_worker_num):\n        command_queue.put(None)\n\n    def worker(command_queue, info_queue):\n        while not command_queue.empty():\n            e = command_queue.get()\n            if e is None:\n                break\n            rtn = run_command(e)\n            if rtn != 0:\n                # Error\n                command_queue.clear()\n                info_queue.put(Exception(\'Error, terminated :-(\'))\n    workers = [threading.Thread(target=worker, args=(command_queue, info_queue))\n               for _ in range(max_worker_num)]\n    for w in workers:\n        w.daemon = True\n    for w in workers:\n        w.start()\n    for w in workers:\n        w.join()\n    while not info_queue.empty():\n        info = info_queue.get()\n        if isinstance(info, Exception) and not allow_error:\n            raise RuntimeError(info)\n\n\ndef add_path(path, files):\n    return list(map(lambda x: os.path.join(path, x), files))\n\n\ndef command_exists(command):\n    try:\n        Popen([command], stdout=PIPE, stderr=PIPE, stdin=PIPE)\n    except Exception:\n        return False\n    return True\n'"
mobula/config.py,0,"b""from .utils import with_metaclass\n\n\nclass ConfigMeta(type):\n    def __setattr__(cls, name, value):\n        cdict = super(ConfigMeta, cls).__dict__\n        if name not in cdict:\n            raise AttributeError('Config has no attribute `{}`.'.format(name))\n        data = cdict[name]\n        target_type = type(data)\n        value_type = type(value)\n        if target_type is not value_type:\n            raise TypeError('The type of config attribute `{}` is not consistent, target {} vs value {}.'.format(\n                name, target_type, value_type))\n        super(ConfigMeta, cls).__setattr__(name, value)\n\n\nclass Config(with_metaclass(ConfigMeta)):\n    TARGET = 'mobula_op'\n    BUILD_PATH = 'build'\n    MAX_BUILDING_WORKER_NUM = 8\n\n    DEBUG = False\n    USING_OPENMP = True\n    USING_CBLAS = False\n    HOST_NUM_THREADS = 0  # 0 : auto\n    USING_HIGH_LEVEL_WARNINGS = True\n    USING_OPTIMIZATION = True\n    USING_ASYNC_EXEC = True\n    GPU_BACKEND = 'cuda'\n\n    CXX = 'g++'\n    NVCC = 'nvcc'\n    HIPCC = 'hipcc'\n    CUDA_DIR = '/opt/cuda'\n    HIP_DIR = '/opt/rocm/hip'\n\n\n# alias\nconfig = Config\n"""
mobula/const.py,0,"b""class req:\n    null = 'null'\n    write = 'write'\n    inplace = 'inplace'\n    add = 'add'\n"""
mobula/dtype.py,0,"b""import ctypes\n\nCTYPE_INTS = [ctypes.c_short, ctypes.c_int, ctypes.c_long, ctypes.c_longlong]\nCTYPE_UINTS = [ctypes.c_ushort, ctypes.c_uint,\n               ctypes.c_ulong, ctypes.c_ulonglong]\n\n\ndef get_ctype_name(ctype):\n    # ctype.__name__ = 'c_xxx'\n    if ctype in CTYPE_INTS[2:]:\n        return 'int{}_t'.format(ctypes.sizeof(ctype) * 8)\n    if ctype in CTYPE_UINTS[2:]:\n        return 'uint{}_t'.format(ctypes.sizeof(ctype) * 8)\n    return ctype.__name__[2:]\n\n\nclass DType:\n    _EXTRA_ATTRS = dict()\n\n    def __init__(self, ctype, is_const=False):\n        self.ctype = ctype\n        self.is_const = is_const\n        self.cname, self.is_pointer = self._get_extra_attr()\n\n    def _get_extra_attr(self):\n        idcode = (self.ctype, self.is_const)\n        if idcode not in DType._EXTRA_ATTRS:\n            # Assignment for self.is_pointer and self.cname\n            if self.ctype.__name__.startswith('LP'):\n                is_pointer = True\n                basic_type = self.ctype._type_\n            else:\n                is_pointer = False\n                basic_type = self.ctype\n\n            ctype_name = get_ctype_name(basic_type)\n\n            if self.is_const:\n                ctype_name = 'const {}'.format(ctype_name)\n            if is_pointer:\n                ctype_name += '*'\n            cname = ctype_name\n            DType._EXTRA_ATTRS[idcode] = (cname, is_pointer)\n        return DType._EXTRA_ATTRS[idcode]\n\n    def __repr__(self):\n        return self.cname\n\n    def __call__(self, value):\n        return self.ctype(value)\n\n\nclass UnknownCType:\n    def __init__(self, tname):\n        self.tname = tname\n        self.is_const = False\n\n\nclass TemplateType:\n    def __init__(self, tname, is_pointer, is_const):\n        self.tname = tname\n        self.is_pointer = is_pointer\n        self.is_const = is_const\n\n    def __repr__(self):\n        return '<typename {const}{tname}{pointer}>'.format(\n            const='const ' if self.is_const else '',\n            tname=self.tname,\n            pointer='*' if self.is_pointer else '')\n\n    def __call__(self, ctype):\n        if self.is_pointer:\n            ctype = ctypes.POINTER(ctype)\n        return DType(ctype, is_const=self.is_const)\n"""
mobula/edict.py,0,"b'""""""easy dict""""""\n\n\nclass edict(dict):\n    """"""a dict class which could be accessed by attribute""""""\n\n    def __setattr__(self, key, value):\n        super(edict, self).__setitem__(key, value)\n\n    def __getattr__(self, key):\n        try:\n            return super(edict, self).__getitem__(key)\n        except KeyError:\n            raise AttributeError(key)\n\n    def __delattr__(self, key):\n        super(edict, self).__delitem__(key)\n'"
mobula/func.py,0,"b'""""""A `Module` implement the `MobulaFunc` class.""""""\n__all__ = [\'MobulaFunc\', \'bind\']\n\n\nimport ctypes\nimport hashlib\nimport warnings\nfrom . import glue\nfrom .dtype import DType, TemplateType, UnknownCType\nfrom .build_utils import config\n\n\ndef get_func_idcode(func_name, arg_types):\n    """"""Get Function IDCode\n\n    Parameters\n    ----------\n    func_name: str\n        the name of function\n    arg_types: list of DType\n\n    Returns\n    -------\n    idcode: str\n        IDCode\n    """"""\n    arg_types_str = \',\'.join([e.cname for e in arg_types])\n    idcode = \'{func_name}:{arg_types_str}\'.format(\n        func_name=func_name,\n        arg_types_str=arg_types_str)\n    return idcode\n\n\ndef get_idcode_hash(idcode):\n    """"""Get the hash string of IDCode\n\n    Parameters\n    ----------\n    idcode: str\n    arg_types: list of DType\n\n    Returns\n    -------\n    Hash String of IDCode: str\n    """"""\n    idcode_sp = idcode.split(\':\')\n    func_name = idcode_sp[0]\n    md5 = hashlib.md5()\n    md5.update(idcode[len(func_name) + 1:].encode(\'utf-8\'))\n    return \'{}_{}\'.format(func_name, md5.hexdigest()[:8])\n\n\nclass CFuncTensor:\n    def __init__(self, var, ptype, glue_mod):\n        self.var = var\n        self.ptype = ptype\n        self.glue_mod = glue_mod\n\n    @property\n    def is_const(self):\n        return self.ptype.is_const\n\n\ndef _wait_to_read(var):\n    if hasattr(var, \'wait_to_read\'):\n        var.wait_to_read()\n\n\ndef _wait_to_write(var):\n    if hasattr(var, \'wait_to_write\'):\n        var.wait_to_write()\n\n\ndef _get_raw_pointer(arg, const_vars, mutable_vars):\n    if isinstance(arg, CFuncTensor):\n        p = arg.glue_mod.get_pointer(arg.var)\n        if isinstance(p, (list, tuple)):\n            p, v = p\n            if arg.is_const:\n                const_vars.append(v)\n            else:\n                mutable_vars.append((arg.var, v))\n        return p\n    return arg\n\n\ndef _get_async_pointer(arg):\n    if isinstance(arg, CFuncTensor):\n        return arg.glue_mod.get_async_pointer(arg.var)\n    return arg\n\n\ndef _get_raw_pointers(args, const_vars, mutable_vars):\n    return [_get_raw_pointer(a, const_vars, mutable_vars) for a in args]\n\n\ndef _get_async_pointers(args):\n    return list(map(_get_async_pointer, args))\n\n\ndef _arg_wait_to_rw(arg):\n    if isinstance(arg, CFuncTensor):\n        if arg.is_const:\n            _wait_to_read(arg)\n        else:\n            _wait_to_write(arg)\n\n\ndef _args_wait_to_rw(args):\n    for arg in args:\n        _arg_wait_to_rw(arg)\n\n\nclass CFuncDef:\n    """"""The definition of CFunction.""""""\n    KERNEL = 1\n    FUNC = 2\n\n    def __init__(self, func_name, func_kind, arg_names=None, arg_types=None, rtn_type=None,\n                 template_list=None, loader=None, loader_kwargs=None):\n        self.func_name = func_name\n        self.func_kind = func_kind\n        self.arg_names = arg_names or list()\n        self.arg_types = arg_types\n        self.rtn_type = rtn_type\n        self.template_list = template_list or list()\n        self.loader = loader\n        self.loader_kwargs = loader_kwargs\n\n    def __call__(self, arg_datas, arg_types, dev_id, glue_mod=None, using_async=False):\n        if dev_id is None:\n            ctx = \'cpu\'\n            dev_id = -1\n        else:\n            ctx = config.GPU_BACKEND\n        # function loader\n        func = self.loader(self, arg_types, ctx, **self.loader_kwargs)\n        if using_async and glue_mod is not None:\n            async_func = func.get_async_func(glue_mod)\n            if async_func is not None:\n                return async_func(*_get_async_pointers(arg_datas))\n\n        _args_wait_to_rw(arg_datas)\n        const_vars = []\n        mutable_vars = []\n        raw_pointers = _get_raw_pointers(arg_datas, const_vars, mutable_vars)\n        if self.func_kind == self.KERNEL:\n            out = func(dev_id, *raw_pointers)\n        else:\n            out = func(*raw_pointers)\n        for target, value in mutable_vars:\n            target[:] = value\n        return out\n\n\nclass MobulaFunc:\n    """"""An encapsulation for CFunction\n\n    Parameters:\n    -----------\n    name: str\n        function name\n    func: CFuncDef\n    """"""\n\n    def __init__(self, name, func):\n        self.name = name\n        self.func = func\n\n        self.wait_to_read_list = []\n        self.wait_to_write_list = []\n        for i, ptype in enumerate(self.func.arg_types):\n            if ptype.is_pointer:\n                if ptype.is_const:\n                    self.wait_to_read_list.append(i)\n                else:\n                    self.wait_to_write_list.append(i)\n\n    def __call__(self, *args, **kwargs):\n        # move kwargs into args\n        args = list(args)\n        for name in self.func.arg_names[len(args):]:\n            args.append(kwargs[name])\n\n        # type check\n        arg_datas = []\n        dev_id = None\n        arg_types = []\n        template_mapping = dict()\n\n        glue_mod = self._get_glue_mod(args)\n        using_async = config.USING_ASYNC_EXEC and glue_mod is not None and hasattr(\n            glue_mod, \'get_async_func\')\n\n        if not using_async:\n            # Pre-process\n            for i in self.wait_to_read_list:\n                _wait_to_read(args[i])\n            for i in self.wait_to_write_list:\n                _wait_to_write(args[i])\n\n        try:\n            for var, ptype in zip(args, self.func.arg_types):\n                if ptype.is_pointer:\n                    # The type of `var` is Tensor.\n                    data, var_dev_id, ctype = self._get_tensor_info(\n                        var, ptype, template_mapping, using_async)\n                else:\n                    # The type of `var` is Scalar.\n                    data, var_dev_id, ctype = self._get_scalar_info(var, ptype)\n\n                arg_datas.append(data)\n                if isinstance(ctype, UnknownCType):\n                    ctype.is_const = ptype.is_const\n                    arg_types.append(ctype)\n                else:\n                    arg_types.append(DType(ctype, is_const=ptype.is_const))\n\n                # update `dev_id`\n                if var_dev_id is not None:\n                    if dev_id is not None:\n                        assert var_dev_id == dev_id, ValueError(\n                            ""Don\'t use multiple devices in a call :-("")\n                    else:\n                        dev_id = var_dev_id\n\n            # try to know the unknown ctype\n            for i, vtype in enumerate(arg_types):\n                if isinstance(vtype, UnknownCType):\n                    assert vtype.tname in template_mapping,\\\n                        Exception(\n                            \'Unknown template name: {}\'.format(vtype.tname))\n                    ctype = template_mapping[vtype.tname]._type_\n                    arg_types[i] = DType(ctype, vtype.is_const)\n                    arg_datas[i] = ctype(arg_datas[i])\n        except TypeError:\n            raise TypeError(\'Unmatched parameters list of the function `{}`:\\n\\t{}\\n\\t\\tvs\\n\\t{}\'.format(\n                self.name, self.func.arg_types, list(map(type, args))))\n\n        rtn = self.func(arg_datas=arg_datas,\n                        arg_types=arg_types,\n                        dev_id=dev_id,\n                        glue_mod=glue_mod,\n                        using_async=using_async)\n        return rtn\n\n    @staticmethod\n    def _get_tensor_info(var, ptype, template_mapping, using_async=False):\n        """"""Get tensor info\n\n        Parameters\n        ----------\n        var: object\n            input variable\n        ptype: DType | TemplateType\n            the type of argument\n        template_mapping: dict\n            the mapping from template name to ctype\n        using_async: bool\n            whether to use asynchronous execution\n\n        Returns\n        -------\n        data: CFuncTensor\n        dev_id: int | None\n            the id of device\n        ctype: ctypes.POINTER | ctypes.c_*\n            the ctype of data\n        """"""\n        glue_mod = glue.backend.get_var_glue(var)\n        if glue_mod is None:\n            raise TypeError()\n        data = CFuncTensor(var, ptype, glue_mod)\n        dev_id = glue_mod.dev_id(var)\n        ctype = ctypes.POINTER(glue_mod.get_ctype(var))\n        if isinstance(ptype, DType):\n            expected_ctype = ptype.ctype\n        elif ptype.tname in template_mapping:\n            expected_ctype = template_mapping[ptype.tname]\n        else:\n            template_mapping[ptype.tname] = expected_ctype = ctype\n        assert ctype == expected_ctype,\\\n            TypeError(\'Expected Type {} instead of {}\'.format(\n                expected_ctype, ctype))\n        return data, dev_id, ctype\n\n    @staticmethod\n    def _get_scalar_info(var, ptype):\n        """"""Get scalar info\n\n        Parameters\n        ----------\n        var: object\n            input variable\n        ptype: DType | TemplateType\n            the type of argument\n\n        Returns\n        -------\n        data: ctyoes.c_void_p\n            the pointer of data\n        dev_id: int | None\n            the id of device\n        ctype: ctypes.POINTER | ctypes.c_*\n            the ctype of data\n        """"""\n\n        dev_id = None\n        if isinstance(ptype, TemplateType):\n            data = var\n            ctype = type(var) if hasattr(\n                var, \'_type_\') else UnknownCType(ptype.tname)\n        else:\n            data = var if isinstance(\n                var, ctypes.c_void_p) else ptype.ctype(var)\n            ctype = ptype.ctype\n        return data, dev_id, ctype\n\n    @staticmethod\n    def _get_glue_mod(datas):\n        mods = map(glue.backend.get_var_glue, datas)\n        mods = list(filter(lambda x: x is not None, mods))\n        if mods:\n            glue_mod = mods[0]\n            # all glue modules in datas are consistent\n            if all(map(lambda x: x == glue_mod, mods)):\n                return glue_mod\n        return None\n\n    def build(self, ctx, template_types=None):\n        """"""Build this function\n\n        Parameters\n        ----------\n        ctx: str\n            context Name\n        template_types: list or tuple or dict, default: []\n            list: a list of template type Names\n            tuple: a tuple of template type Names\n            dict: a mapping from template name to type name\n\n        Examples\n        --------\n        >>> mobula.func.add.build(\'cpu\', [\'float\'])\n        """"""\n        arg_types = []\n        par_type = self.func.arg_types\n        if template_types is None:\n            template_types = list()\n        if isinstance(template_types, (list, tuple)):\n            template_mapping = dict()  # tname -> ctype\n            for vtype in par_type:\n                if isinstance(vtype, TemplateType):\n                    tname = vtype.tname\n                    if tname in template_mapping:\n                        ctype = template_mapping[tname]\n                    else:\n                        ctype = getattr(ctypes, \'c_{}\'.format(\n                            template_types.pop(0)))\n                        template_mapping[tname] = ctype\n                    arg_types.append(vtype(ctype))\n                else:\n                    arg_types.append(vtype)\n            assert not template_types, Exception(\'redundant type\')\n        else:\n            assert isinstance(template_types, dict), TypeError(\n                \'The type of template_types should be list or tuple or dict.\')\n            template_name = set()\n            for vtype in par_type:\n                if isinstance(vtype, TemplateType):\n                    tname = vtype.tname\n                    assert tname in template_types, KeyError(\n                        \'Unknown Template Type: {}\'.format(tname))\n                    template_name.add(tname)\n                    ctype = getattr(ctypes, \'c_{}\'.format(\n                        template_types[tname]))\n                    arg_types.append(vtype(ctype))\n                else:\n                    arg_types.append(vtype)\n            assert len(template_name) == len(template_types), Exception(\n                \'Different template name: {} vs {}\'.format(\n                    template_name, set(template_types.keys())))\n        func = self.func\n        func.loader(func, arg_types, ctx, **func.loader_kwargs)\n\n\n_binded_functions = dict()\n\n\ndef bind(functions):\n    global _binded_functions\n    """"""Bind Functions to mobula.func.<function name>\n\n    Parameters\n    ----------\n    functions: dict\n        name -> CFuncDef\n    """"""\n    for k, func in functions.items():\n        if k in _binded_functions:\n            warnings.warn(\'Duplicated function name {}\'.format(k))\n        func = MobulaFunc(k, func)\n        globals()[k] = func\n        _binded_functions[k] = func\n'"
mobula/testing.py,16,"b'import copy\nimport os\nimport sys\nimport numpy as np\nfrom .glue.common import MobulaOperator\n\nif sys.version_info[0] < 3:\n    FileNotFoundError = IOError\nelse:\n    long = int\n\n\ndef to_numpy(data):\n    if isinstance(data, np.ndarray):\n        return data\n    if hasattr(data, \'asnumpy\'):\n        return data.asnumpy()\n    if hasattr(data, \'numpy\'):\n        return data.numpy()\n    if isinstance(data, (list, tuple)):\n        return np.array(data)\n    raise TypeError(\'Unsupported Type: {}\'.format(type(data)))\n\n\ndef to_tuple(data):\n    if isinstance(data, tuple):\n        return data\n    if isinstance(data, list):\n        return tuple(data)\n    return (data, )\n\n\ndef assert_almost_equal(a, b, rtol=1e-5, atol=1e-8):\n    def check_value(data, other):\n        if isinstance(data, (int, long, float)):\n            if hasattr(other, \'shape\'):\n                return np.full(other.shape, fill_value=data)\n            else:\n                return np.array(a)\n        return data\n    a = check_value(a, b)\n    b = check_value(b, a)\n    a = to_numpy(a)\n    b = to_numpy(b)\n    # Check Shape\n    # If the shapes don\'t match, raise AssertionError and print the shapes\n    assert a.shape == b.shape,\\\n        AssertionError(\'Unmatched Shape: {} vs {}\'.format(a.shape, b.shape))\n\n    # Compute Absolute Error |a - b|\n    error = a - b\n    abs_error = np.abs(error)\n    max_abs_error = abs_error.max()\n\n    def raise_error(abs_error, info):\n        # tell where is maximum absolute error and the value\n        loc = np.argmax(abs_error)\n        idx = np.unravel_index(loc, abs_error.shape)\n        out = \'\'\n\n        def get_array_R(data, name, idx, R):\n            axes = [-1] if data.ndim == 1 else [-1, -2]\n            shape = data.shape\n            slice_list = list(idx)\n            sidx = list(idx[-2:])\n            for i in axes:\n                axis_len = shape[i]\n                axis_i = slice_list[i]\n                start = max(0, axis_i - R + 1)\n                stop = min(axis_len, axis_i + R)\n                slice_list[i] = slice(start, stop)\n                sidx[i] -= start\n\n            def str_slice_list(slice_list):\n                return \', \'.join([str(s) if not isinstance(s, slice) else\n                                  \'{}:{}\'.format(s.start, s.stop) for s in slice_list])\n            sdata = data.round(5)\n            return \'{name}[{slice_list}]:\\n{data}\\n\'.format(name=name, slice_list=str_slice_list(slice_list),\n                                                            data=sdata)\n\n        R = 5\n        out += \'Location of maximum error: {}\\n\'.format(idx)\n        out += \'{}\\n{}\\n{}\'.format(info,\n                                   get_array_R(\n                                       a, \'a\', idx, R),\n                                   get_array_R(\n                                       b, \'b\', idx, R),\n                                   )\n        raise AssertionError(out)\n\n    # Check Absolute Error\n    if atol is not None:\n        if max_abs_error > atol:\n            # If absolute error >= atol, raise AssertionError,\n            idx = abs_error.argmax()\n            raise_error(abs_error, \'Maximum Absolute Error({}) > atol({}): {} vs {}\'.\n                        format(max_abs_error, atol, a.ravel()[idx], b.ravel()[idx]))\n\n    # Compute Relative Error |(a-b)/b|\n    try:\n        eps = np.finfo(b.dtype).eps\n    except ValueError:\n        eps = np.finfo(np.float32).eps\n    relative_error = abs_error / (np.abs(b) + eps)\n    max_relative_error = relative_error.max()\n\n    # Check Relative Error\n    if max_relative_error > rtol:\n        # If relative error >= rtol, raise AssertionError,\n        idx = relative_error.argmax()\n        raise_error(relative_error, \'Maximum Relative Error({}) > rtol({}): {} vs {}\'.\n                    format(max_relative_error, rtol, a.ravel()[idx], b.ravel()[idx]))\n\n\ndef assert_file_exists(fname):\n    assert os.path.exists(fname), IOError(""{} not found"".format(fname))\n\n\ndef gradcheck(func, inputs, kwargs=None, eps=1e-6, rtol=1e-2, atol=None, sampling=None):\n    assert isinstance(func, MobulaOperator)\n    if kwargs is None:\n        kwargs = dict()\n    assert isinstance(kwargs, dict)\n    if not isinstance(inputs, (tuple, list)):\n        inputs = (inputs, )\n    # To NumPy Tensor\n    inputs = [to_numpy(x) for x in inputs]\n    func = func[np.ndarray](**kwargs)\n    ori_out = to_tuple(func(*copy.deepcopy(inputs)))\n    assert isinstance(ori_out, (tuple, list)), type(ori_out)\n    dys = [np.random.normal(0, 0.01, size=out_i.shape).astype(out_i.dtype) +\n           0.1 for out_i in ori_out]\n    assert len(dys) == len(ori_out), \'{} vs {}\'.format(len(dys), len(ori_out))\n    grad = to_tuple(func.backward(copy.deepcopy(dys)))\n    for i, x in enumerate(inputs):\n        size = inputs[i].size\n        sample_grad = np.empty_like(inputs[i])\n        sample_grad_ravel = sample_grad.ravel()\n        samples = np.arange(size)\n        if sampling is not None:\n            if isinstance(sampling, int):\n                num_samples = sampling\n            else:\n                num_samples = int(sampling * size)\n            samples = np.random.choice(samples, min(size, num_samples))\n        for k in samples:\n            x_ravel = x.ravel()\n            old_elem_value = x_ravel[k]\n            x_ravel[k] = old_elem_value + eps / 2\n            pos_out = to_tuple(func(*copy.deepcopy(inputs), **kwargs))\n            x_ravel[k] = old_elem_value - eps / 2\n            neg_out = to_tuple(func(*copy.deepcopy(inputs), **kwargs))\n            x_ravel[k] = old_elem_value\n            assert len(pos_out) == len(neg_out)\n            assert len(pos_out) == len(ori_out)\n            numerical_grad_k = np.sum(\n                [dy * (pos - neg) / eps for pos, neg, dy in zip(pos_out, neg_out, dys)])\n            sample_grad_ravel[k] = numerical_grad_k\n        numerical_grad = grad[i].copy()\n        numerical_grad.ravel()[samples] = sample_grad.ravel()[samples]\n        assert_almost_equal(numerical_grad, grad[i], rtol=rtol, atol=atol)\n'"
mobula/utils.py,0,"b'import os\nimport subprocess\n\nENV_PATH = os.path.dirname(__file__)\n__all__ = [\'ENV_PATH\', \'list_gpus\', \'get_git_hash\']\n\n\ndef list_gpus_impl():\n    result = \'\'\n    nvidia_smi = [\'nvidia-smi\', \'/usr/bin/nvidia-smi\',\n                  \'/usr/local/nvidia/bin/nvidia-smi\']\n    for cmd in nvidia_smi:\n        try:\n            result = subprocess.check_output(\n                [cmd, ""-L""], universal_newlines=True)\n            break\n        except Exception:\n            pass\n    else:\n        return range(0)\n    return list(range(len([i for i in result.split(\'\\n\') if \'GPU\' in i])))\n\n\nGPUS_LIST = list_gpus_impl()\n\n\ndef list_gpus():\n    """"""Return a list of GPUs\n        Adapted from [MXNet](https://github.com/apache/incubator-mxnet)\n\n    Returns\n    -------\n    list of int:\n        If there are n GPUs, then return a list [0,1,...,n-1]. Otherwise returns\n        [].\n    """"""\n    return GPUS_LIST\n\n\ndef get_git_hash():\n    try:\n        GIT_HEAD_PATH = os.path.join(ENV_PATH, \'..\', \'.git\')\n        line = open(os.path.join(GIT_HEAD_PATH, \'HEAD\')\n                    ).readline().strip()\n        if line[:4] == \'ref:\':\n            ref = line[5:]\n            return open(os.path.join(GIT_HEAD_PATH, ref)).readline().strip()[:7]\n        return line[:7]\n    except FileNotFoundError:\n        return \'custom\'\n\n\ndef with_metaclass(meta, *bases):\n    class metaclass(type):\n        def __new__(cls, name, _bases, attrs):\n            return meta(name, bases, attrs)\n\n        @classmethod\n        def __prepare__(cls, name, _bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, \'class\', (), {})\n\n\ndef makedirs(name, mode=511, exist_ok=False):\n    \'\'\'makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works like\nmkdir, except that any intermediate path segment (not just the rightmost)\nwill be created if it does not exist. If the target directory already\nexists, raise an OSError if exist_ok is False. Otherwise no exception is\nraised.  This is recursive.\'\'\'\n    if exist_ok and os.path.exists(name):\n        return\n    os.makedirs(name, mode)\n'"
mobula/version.py,0,"b'""""""version information""""""\n__version__ = 2.32\n\nOP_LOAD_MODULE_BUILD_VERSION = __version__\n'"
tests/test_config.py,0,"b""import mobula\nfrom mobula import config\nfrom nose.tools import assert_raises\n\n\ndef test_config():\n    new_target_name = 'mobula_op_bak'\n    config.TARGET = new_target_name\n    assert config.TARGET == new_target_name, (config.TARGET, new_target_name)\n\n    config.DEBUG = True\n    assert config.DEBUG\n    config.DEBUG = False\n\n    assert_raises(AttributeError, lambda: config.UNKNOWN_ATTR)\n\n    def assign_target_num():\n        config.TARGET = 10\n    assert_raises(TypeError, assign_target_num)\n"""
tests/test_ctx.py,10,"b'import mobula\nfrom mobula.testing import assert_almost_equal\nimport numpy as np\n\n\n@mobula.op.register\nclass MulOP:\n    def forward(self, a, b):\n        return a * b\n\n    def backward(self, dy):\n        return dy * self.X[1], dy * self.X[0]\n\n    def infer_shape(self, in_shape):\n        assert in_shape[0] == in_shape[1]\n        return in_shape, [in_shape[0]]\n\n\ndef test_ctx_mxnet():\n    try:\n        import mxnet as mx\n    except ImportError:\n        return\n    shape = (5, 5)\n\n    a_np = np.random.random(shape)\n    b_np = np.random.random(shape)\n    dy_np = np.random.random(shape)\n\n    a = mx.nd.array(a_np)\n    b = mx.nd.array(b_np)\n    dy = mx.nd.array(dy_np)\n\n    a.attach_grad()\n    b.attach_grad()\n    with mx.autograd.record():\n        c = MulOP(a, b)\n    c.backward(dy)\n    assert_almost_equal(a.grad, b * dy)\n    assert_almost_equal(b.grad, a * dy)\n    assert_almost_equal(a * b, c)\n\n\ndef test_ctx_torch():\n    try:\n        import torch\n    except ImportError:\n        return\n    shape = (5, 5)\n\n    a_np = np.random.random(shape)\n    b_np = np.random.random(shape)\n    dy_np = np.random.random(shape)\n\n    a = torch.tensor(a_np, requires_grad=True)\n    b = torch.tensor(b_np, requires_grad=True)\n    dy = torch.tensor(dy_np)\n    c = MulOP(a, b)\n    c.backward(dy)\n    assert_almost_equal(a.grad.data, (b * dy).data)\n    assert_almost_equal(b.grad.data, (a * dy).data)\n    assert_almost_equal((a * b).data, c.data)\n\n\ndef test_ctx_np():\n    shape = (5, 5)\n    a = np.random.random(shape)\n    b = np.random.random(shape)\n    dy = np.random.random(shape)\n    op = MulOP[np.ndarray]()\n    c = op.forward(a, b)\n    a_grad, b_grad = op.backward(dy)\n    assert_almost_equal(a_grad, b * dy)\n    assert_almost_equal(b_grad, a * dy)\n    assert_almost_equal(a * b, c)\n'"
tests/test_examples.py,0,"b""import mobula\nimport importlib\nimport runpy\nimport os\nimport sys\n\n\ndef packages_exist(pkg_names):\n    for pkg in pkg_names:\n        try:\n            importlib.import_module(pkg)\n        except ImportError:\n            return False\n    return True\n\n\ndef test_examples():\n    EXAMPLES_PATH = os.path.join(os.path.dirname(__file__), '../examples')\n    examples = [\n        ([], ['MyFirstOP.py', '../docs/tutorial/test_mul_func.py',\n              '../docs/tutorial/test_mul_op.py']),\n        (['mxnet'], ['ConstantOP.py', 'RunROIAlign.py',\n                     'dynamic_import_op/dynamic_import_op.py',\n                     '../opzoo/Softmax/test_softmax.py',\n                     ]),\n        (['mxnet', 'tvm'], ['TVMOp.py']),\n    ]\n    sys.path.append('./')\n    record = []\n    for dep_pkgs, examples in examples:\n        if packages_exist(dep_pkgs):\n            for example in examples:\n                print('testing... {}'.format(example))\n                subpath, script_name = os.path.split(example)\n                fullpath = os.path.join(EXAMPLES_PATH, subpath)\n                old_workpath = os.getcwd()\n                os.chdir(fullpath)\n                try:\n                    runpy.run_path(script_name, {}, '__main__')\n                except Exception as e:\n                    record.append((example, e))\n                os.chdir(old_workpath)\n    assert len(record) == 0, record\n"""
tests/test_testing.py,15,"b'import mobula\nfrom mobula.testing import assert_almost_equal, gradcheck\nimport numpy as np\nfrom nose.tools import assert_raises\n\n\ndef check_almost_euqal_expection_raise(a, b, info, rtol=1e-5, atol=1e-8):\n    assert_raises(AssertionError, assert_almost_equal,\n                  a, b, rtol=rtol, atol=atol)\n\n\ndef test_almost_equal_shape():\n    shape1 = (2, 2, 3)\n    a = np.random.random(shape1)\n    b = a.copy()\n    c = a[1]\n    assert_almost_equal(a, b)\n    check_almost_euqal_expection_raise(a, c, ""No exception raised"")\n\n\ndef test_almost_equal_value():\n    shape1 = (2, 2, 3)\n    a = np.random.random(shape1)\n    b = a.copy()\n    atol = 1e-3\n    assert_almost_equal(a, b, atol=0)\n    assert_almost_equal(a, b, atol=atol)\n    b[0, 0, 0] += atol\n    b[0, 1, 2] -= atol\n    assert_almost_equal(a, b, rtol=np.inf, atol=atol * 2.0)\n    check_almost_euqal_expection_raise(\n        a, b, \'Absolute Error Check failed\', rtol=np.inf, atol=atol / 2.0)\n    eps = np.finfo(b.dtype).eps\n    rtol = np.max(np.abs((a - b) / (b + eps)))\n    assert_almost_equal(a, b, rtol=rtol * 2.0, atol=atol * 2.0)\n    check_almost_euqal_expection_raise(\n        a, b, \'Relative Error Check failed\', rtol=rtol * 2.0, atol=atol / 2.0)\n\n\ndef test_gradcheck():\n    @mobula.op.register\n    class SquareOP:\n        def __init__(self, err=0):\n            self.err = err\n\n        def forward(self, x):\n            return self.F.square(x)\n\n        def backward(self, dy):\n            return 2 * self.x * dy + self.err\n\n        def infer_shape(self, in_shape):\n            assert len(in_shape) == 1\n            return in_shape, in_shape\n    shape = (3, 4, 5, 6)\n    gradcheck(SquareOP, [np.random.normal(size=shape)], dict(err=0))\n    assert_raises(AssertionError, gradcheck, SquareOP, [\n                  np.random.normal(size=shape)], dict(err=100))\n\n    @mobula.op.register\n    class SingleOutput:\n        def forward(self, x):\n            ar = self.F.arange(x.size)\n            return (x * ar + ar).sum() / (x.size - 1)\n\n        def backward(self, dy):\n            ar = self.F.arange(self.x.size)\n            return (ar * dy) / (self.x.size - 1)\n\n        def infer_shape(self, in_shape):\n            return in_shape, [(1, )]\n\n    gradcheck(SingleOutput, [np.random.normal(size=(64, ))])\n\n    @mobula.op.register\n    class TwoOutput:\n        def forward(self, x, y):\n            return x * y, x + y\n\n        def backward(self, da, db):\n            return da * self.X[1] + db, da * self.X[0] + db\n\n        def infer_shape(self, in_shape):\n            return in_shape, in_shape\n\n    gradcheck(TwoOutput, [np.random.normal(\n        size=shape), np.random.normal(size=shape)])\n\n    gradcheck(TwoOutput, [np.random.normal(\n        size=shape), np.random.normal(size=shape)], sampling=0.8)\n\n    gradcheck(TwoOutput, [np.random.normal(\n        size=shape), np.random.normal(size=shape)], sampling=10)\n'"
tests/test_utils.py,0,"b""from mobula.utils import get_git_hash\n\n\ndef test_get_git_hash():\n    git_hash = get_git_hash()\n    assert type(git_hash) == str, (git_hash, type(git_hash))\n    assert len(git_hash) == 7, git_hash\n\n\ndef test_edict():\n    from mobula.edict import edict\n    data = edict(a=3, b=4)\n    assert 'a' in data\n    assert hasattr(data, 'a')\n    assert 'b' in data\n    assert hasattr(data, 'b')\n    assert len(data) == 2\n    assert data['a'] == 3\n    assert data['b'] == 4\n\n    data.a = 5\n    assert data['a'] == 5\n    data.a += 3\n    assert data['a'] == 8\n\n    data.update(dict(c=6))\n    assert 'c' in data\n    assert data['c'] == 6\n    data['c'] += 1\n    assert data['c'] == 7\n\n    del data.b\n    assert 'b' not in data\n    assert not hasattr(data, 'b')\n    assert len(data) == 2\n\n    del data['a']\n    assert 'a' not in data\n    assert len(data) == 1\n"""
utils/autoformat.py,0,"b'import os\nfrom mobula.build_utils import build_context, file_is_changed, update_file_hash, update_build_path\n\n\ndef find_all_file(path, exts):\n    result = []\n    for name in os.listdir(path):\n        if name.startswith(\'.\'):\n            # ignore the hidden directory\n            continue\n        fname = os.path.join(path, name)\n        if os.path.isdir(fname):\n            result.extend(find_all_file(fname, exts))\n        else:\n            ext = os.path.splitext(name)[1]\n            if ext in exts:\n                result.append(fname)\n    return result\n\n\ndef clang_format(fnames):\n    for fname in fnames:\n        if file_is_changed(fname):\n            print(\'Format {}\'.format(fname))\n            script = \'clang-format -style=""{BasedOnStyle: Google, Standard: Cpp11}"" -i \' + fname\n            if os.system(script) == 0:\n                update_file_hash(fname)\n\n\ndef autopep8(fnames):\n    for fname in fnames:\n        if file_is_changed(fname):\n            print(\'Format {}\'.format(fname))\n            if os.system(\'autopep8 --ignore E402 --in-place {}\'.format(fname)) == 0:\n                update_file_hash(fname)\n\n\ndef filter_ignore_path(fnames, ignore_path):\n    abs_ignore_path = os.path.abspath(ignore_path)\n\n    def is_not_ignore_path(fname):\n        abs_fname = os.path.abspath(fname)\n        return not abs_fname.startswith(abs_ignore_path)\n\n    return filter(is_not_ignore_path, fnames)\n\n\nif __name__ == \'__main__\':\n    update_build_path(\'./autoformat_code\')\n    ignore_path = \'./mobula/op/templates/\'\n    with build_context():\n        cpp_res = find_all_file(\'./\', [\'.cpp\', \'.h\'])\n        cpp_res = filter_ignore_path(cpp_res, ignore_path)\n        clang_format(cpp_res)\n\n        py_res = find_all_file(\'./\', [\'.py\'])\n        py_res = filter_ignore_path(py_res, ignore_path)\n        autopep8(py_res)\n'"
utils/clean_all_build.py,0,"b""import os\nfrom mobula.build_utils import rmdir\n\n\ndef find_all_build(path):\n    result = []\n    for name in os.listdir(path):\n        fname = os.path.join(path, name)\n        if os.path.isdir(fname):\n            if os.path.basename(fname) == 'build':\n                result.append(fname)\n            else:\n                result.extend(find_all_build(fname))\n    return result\n\n\nif __name__ == '__main__':\n    res = find_all_build('./')\n    for r in res:\n        rmdir(r)\n"""
benchmark/EmptyOP/EmptyOP.py,0,"b'import mobula\n\n\n@mobula.op.register\nclass EmptyOP:\n    def forward(self, a):\n        mobula.func.empty_forward(a.size, a, self.y)\n\n    def backward(self, dy):\n        mobula.func.empty_forward(a.size, a, self.dx)\n\n    def infer_shape(self, in_shape):\n        assert len(in_shape) == 1\n        return in_shape, in_shape\n'"
docs/tutorial/test_mul_func.py,0,"b""import mobula\nmobula.op.load('MulElemWise')\n\nimport mxnet as mx\na = mx.nd.array([1, 2, 3])\nb = mx.nd.array([4, 5, 6])\nout = mx.nd.empty(a.shape)\nmobula.func.mul_elemwise(a.size, a, b, out)\nprint(out)  # [4, 10, 18]\n"""
docs/tutorial/test_mul_op.py,3,"b""import mobula\nmobula.op.load('MulElemWise')\n\n\ndef test_numpy():\n    print('==========')\n    print('NumPy')\n    a_np = np.array([1, 2, 3])\n    b_np = np.array([4, 5, 6])\n    op = mobula.op.MulElemWise[np.ndarray]()\n    c_np = op(a_np, b_np)\n    print(c_np)\n    print('gradients:', op.backward())\n\n\ndef test_cupy():\n    print('==========')\n    print('CuPy')\n    a = cp.array([1, 2, 3])\n    b = cp.array([4, 5, 6])\n    op = mobula.op.MulElemWise[cp.ndarray]()\n    c = op(a, b)\n    print(c)  # [4, 10, 18]\n    print('gradients:', op.backward())\n\n\ndef test_mxnet_ndarray():\n    print('==========')\n    print('MXNet NDArray')\n    a = mx.nd.array([1, 2, 3])\n    b = mx.nd.array([4, 5, 6])\n    a.attach_grad()\n    b.attach_grad()\n    with mx.autograd.record():\n        c = mobula.op.MulElemWise(a, b)\n        c.backward()\n    print(c)  # [4, 10, 18]\n    print('gradients:', a.grad, b.grad)\n\n\ndef test_mxnet_symbol():\n    print('==========')\n    print('MXNet Symbol')\n    a = mx.nd.array([1, 2, 3])\n    b = mx.nd.array([4, 5, 6])\n    a_sym = mx.sym.Variable('a')\n    b_sym = mx.sym.Variable('b')\n    c_sym = mobula.op.MulElemWise(a_sym, b_sym)\n    exe = c_sym.simple_bind(\n        ctx=mx.context.current_context(), a=a.shape, b=b.shape)\n    exe.forward(a=a, b=b)\n    print(exe.outputs[0])\n\n\ndef test_mxnet_gluon():\n    print('==========')\n    print('MXNet Gluon')\n    a = mx.nd.array([1, 2, 3])\n    b = mx.nd.array([4, 5, 6])\n\n    class MulElemWiseBlock(mx.gluon.nn.HybridBlock):\n        def hybrid_forward(self, F, a, b):\n            return mobula.op.MulElemWise(a, b)\n    block = MulElemWiseBlock()\n    print(block(a, b))\n\n\ndef test_pytorch():\n    print('==========')\n    print('PyTorch')\n    a = torch.tensor([1., 2., 3.], requires_grad=True)\n    b = torch.tensor([4., 5., 6.], requires_grad=True)\n    c = mobula.op.MulElemWise(a, b)  # c = a + b\n    c.sum().backward()\n    print(c)\n    print('gradients:', a.grad, b.grad)\n\n\nif __name__ == '__main__':\n    import numpy as np\n    test_numpy()\n\n    try:\n        import cupy as cp\n        test_cupy()\n    except ImportError:\n        pass\n\n    try:\n        import mxnet as mx\n        test_mxnet_ndarray()\n        test_mxnet_symbol()\n        test_mxnet_gluon()\n    except ImportError:\n        pass\n\n    try:\n        import torch\n        test_pytorch()\n    except ImportError:\n        pass\n"""
examples/dynamic_import_op/dynamic_import_op.py,0,"b""import sys\nsys.path.append('../../')  # Add MobulaOP Path\nimport mobula\nfrom mobula.testing import assert_almost_equal\n# Import Custom Operator Dynamically\nmobula.op.load('./AdditionOP')\nAdditionOP = mobula.op.AdditionOP\n\nimport mxnet as mx\na = mx.nd.array([1, 2, 3])\nb = mx.nd.array([4, 5, 6])\n\na.attach_grad()\nb.attach_grad()\n\nwith mx.autograd.record():\n    c = AdditionOP(a, b)\n\ndc = mx.nd.array([7, 8, 9])\nc.backward(dc)\n\nassert_almost_equal(a + b, c)\nassert_almost_equal(a.grad, dc)\nassert_almost_equal(b.grad, dc)\n\nprint('Okay :-)')\nprint('a + b = c \\n {} + {} = {}'.format(a.asnumpy(), b.asnumpy(), c.asnumpy()))\n"""
mobula/glue/__init__.py,0,"b'from . import backend\nfrom .common import register, CUSTOM_OP_LIST\nfrom . import common\ncommon.backend = backend\n'"
mobula/glue/backend.py,0,"b'""""""Backend Manager.""""""\nimport importlib\nfrom itertools import chain\n\nDTYPE_TO_GLUE = dict()  # dtype -> glue_mod\nGLUE_NAME_TO_GLUE = dict()  # glue_name -> glue_mod\nPKG_NAME_TO_GLUE_ARGS = dict()  # package_name -> (glue_name, types_name)\n\n\ndef _check_glue(glue_mod):\n    """"""Check Glue Module.\n\n    Parameters\n    ----------\n    glue_mod: module\n    """"""\n    func_names = [\'get_pointer\', \'get_ctype\', \'dev_id\', \'OpGen\']\n    for name in func_names:\n        assert hasattr(glue_mod, name), AttributeError(\n            \'Attribute {} not found\'.format(name))\n    assert hasattr(glue_mod.OpGen, \'__call__\')\n    assert hasattr(glue_mod.OpGen, \'register\')\n\n\ndef _register_glue_real(glue_name, types_name):\n    global DTYPE_TO_GLUE, GLUE_NAME_TO_GLUE\n    if not isinstance(types_name, list):\n        types_name = [types_name]\n    glue = None\n    try:\n        glue = importlib.import_module(\'.\' + glue_name, __package__)\n    except ImportError:\n        pass\n    if glue is not None:\n        for tname in types_name:\n            tname_sp = tname.split(\'.\')\n            try:\n                module = importlib.import_module(tname_sp[0])\n                for sub_name in tname_sp[1:]:\n                    module = getattr(module, sub_name, None)\n                    if module is None:\n                        raise ImportError\n                # create generators cache\n                glue.gen_cache = dict()\n                DTYPE_TO_GLUE[module] = glue\n            except ImportError:\n                pass\n            _check_glue(glue)\n            GLUE_NAME_TO_GLUE[glue_name] = glue\n\n\ndef register_glue(glue_name, type_names):\n    """"""Register a glue module.\n\n    Parameters\n    ----------\n    glue_name: str\n        The name of glue module.\n    type_names: list of str\n        The list of inputs\' class names.\n    """"""\n    global PKG_NAME_TO_GLUE_ARGS\n    assert type_names, ValueError(\'type_names should be not empty\')\n    pkg_names = [cls_name.split(\'.\')[0] for cls_name in type_names]\n    pkg_name = pkg_names[0]\n    assert all(map(lambda x: x == pkg_name, pkg_names)), TypeError(\n        \'The name of package should be consistent in `types_name`: {}\'.format(type_names))\n    PKG_NAME_TO_GLUE_ARGS[pkg_name] = (glue_name, type_names)\n\n\n# register glue modules.\nregister_glue(\'mx\', [\'mxnet.nd.NDArray\',\n                     \'mxnet.sym.Symbol\', \'mxnet.numpy.ndarray\'])\nregister_glue(\'np\', [\'numpy.ndarray\'])\nregister_glue(\'th\', [\'torch.Tensor\'])\nregister_glue(\'cp\', [\'cupy.core.core.ndarray\'])\n\n\ndef get_var_type_glue(vtype):\n    """"""Get glue module from variable\'s type.\n\n    Parameters\n    ----------\n    vtype: data type\n\n    Returns\n    -------\n    Glue Module if glue exists, otherwise None.\n    """"""\n    global DTYPE_TO_GLUE, PKG_NAME_TO_GLUE_ARGS\n    glue_mod = DTYPE_TO_GLUE.get(vtype, None)\n    if glue_mod is not None:\n        return glue_mod\n    pkg_name = vtype.__module__.split(\'.\')[0]\n    if pkg_name not in PKG_NAME_TO_GLUE_ARGS:\n        return None\n    # try to register glue_mod\n    _register_glue_real(*PKG_NAME_TO_GLUE_ARGS[pkg_name])\n    return DTYPE_TO_GLUE[vtype]\n\n\ndef get_var_glue(var):\n    """"""Get glue module from variable.\n\n    Parameters\n    ----------\n    var: variable\n\n    Returns\n    -------\n    Glue Module if glue exists, otherwise None.\n    """"""\n\n    return get_var_type_glue(type(var))\n\n\ndef get_args_glue(*args, **kwargs):\n    """"""Get glue module from args and kwargs.\n\n    Parameters\n    ----------\n    *args\n    **kwargs\n\n    Returns\n    -------\n    Glue Module if glue exists, otherwise None.\n    """"""\n    glue_mods = map(get_var_glue, chain(args, kwargs.values()))\n    glue_mods = list(filter(lambda x: x is not None, glue_mods))\n    if glue_mods:\n        glue_mod = glue_mods[0]\n        assert all(map(lambda x: x == glue_mod, glue_mods)),\\\n            TypeError(\n                \'Support only 1 backend in a call, now: {}\'.format(glue_mods))\n        return glue_mod\n    return None\n\n\ndef op_gen(glue_mod, op, name):\n    """""" Get operator generator of glue module.\n\n    Parameters\n    ----------\n    glue_mod: Glue Module\n    op: object\n        The object of custom operator.\n    name: str\n        The name of custom operator.\n\n    Returns\n    -------\n    The operator generator of glue module.\n    """"""\n    if name not in glue_mod.gen_cache:\n        glue_mod.gen_cache[name] = glue_mod.OpGen(op=op, name=name)\n    return glue_mod.gen_cache[name]\n\n\ndef get_glue_modules():\n    return GLUE_NAME_TO_GLUE.values()\n'"
mobula/glue/common.py,7,"b'""""""Common Functions for mobula.glue""""""\nimport sys\nimport pickle\nimport inspect\nimport base64\nimport ctypes\nimport functools\nimport warnings\n\n\ndef pars_encode(data):\n    """"""Encode data to base64 string.\n\n    Parameters\n    ----------\n    data: object\n\n    Returns\n    -------\n    str\n        The encoding base64 string.\n    """"""\n    return base64.b64encode(pickle.dumps(data)).decode(\'utf-8\')\n\n\ndef pars_decode(data):\n    """"""Decode base64 string to object.\n\n    Parameters\n    ----------\n    data: str\n        The encoding base64 string.\n\n    Returns\n    -------\n    object\n        The decoding object.\n    """"""\n    return pickle.loads(base64.b64decode(data.encode(\'utf-8\')))\n\n\nif sys.version_info[0] >= 3:\n    getargspec = inspect.getfullargspec\nelse:\n    getargspec = inspect.getargspec\n\n\ndef get_varnames(func):\n    """"""Get the name list in parameter list of a function.\n\n    Parameters\n    ----------\n    func: function\n\n    Returns\n    -------\n    list of str\n        The name list in parameter list of a function.\n    """"""\n    return getargspec(func).args[1:]\n\n\nCUSTOM_OP_LIST = dict()\nOP_MODULE_GLOBALS = None\n\n\ndef get_in_data(*args, **kwargs):\n    \'\'\'\n    return:\n        inputs: input variances\n        pars: parameters of the operator\n    \'\'\'\n    op = kwargs.pop(\'op\')\n    input_names = get_varnames(op.forward)\n    num_inputs = len(input_names)\n    defaults = getargspec(op.forward).defaults\n    num_defaults = len(defaults) if defaults is not None else 0\n    # define input variances in the forward function\n    # And the input variances may be in args or kwargs\n    if len(args) >= num_inputs:\n        inputs = args[:num_inputs]\n        pars = [args[num_inputs:], kwargs]\n    else:\n        # len(args) <= num_inputs\n        inputs = [None for _ in range(num_inputs)]\n        for i, arg in enumerate(args):\n            assert input_names[i] not in kwargs\n            inputs[i] = arg\n        # the rest of parameters\n        for i in range(len(args), num_inputs - num_defaults):\n            name = input_names[i]\n            assert name in kwargs, ""Variable %s not found"" % name\n            inputs[i] = kwargs.pop(name)\n        num_valid_inputs = num_inputs - num_defaults\n        for i in range(num_inputs - num_defaults, num_inputs):\n            name = input_names[i]\n            if name not in kwargs:\n                break\n            inputs[i] = kwargs.pop(name)\n            num_valid_inputs += 1\n        inputs = inputs[:num_valid_inputs]\n        pars = [[], kwargs]\n\n    return inputs, pars\n\n\ndef get_in_shape(in_data):\n    """"""Get shapes of input datas.\n\n    Parameters\n    ----------\n    in_data: Tensor\n        input datas.\n\n    Returns\n    -------\n    list of shape\n        The shapes of input datas.\n    """"""\n    return [d.shape for d in in_data]\n\n\ndef assign(_, dst, req, src):\n    """"""Helper function for assigning into dst depending on requirements.""""""\n    if req == \'null\':\n        return\n    if req in (\'write\', \'inplace\'):\n        dst[:] = src\n    elif req == \'add\':\n        dst[:] += src\n\n\nbackend = None  # wait for importing in __init__.py\n\n\nclass MobulaOperator:\n    """"""Mobula Operator.\n\n    Parameters\n    ----------\n    op: Custom Operator Class\n    name: str\n        The name of custom operator.\n    *attrs:\n        The attribute of custom operator.\n    """"""\n\n    def __init__(self, op, name, **attrs):\n        self.op = op\n        self.name = name\n        self.attrs = attrs\n\n    def __call__(self, *args, **kwargs):\n        glue_mod = backend.get_args_glue(*args, **kwargs)\n        assert glue_mod is not None, ValueError(\'No explict backend\')\n        new_kwargs = kwargs.copy()\n        new_kwargs.update(self.attrs)\n        return backend.op_gen(glue_mod, op=self.op, name=self.name)(*args, **new_kwargs)\n\n    def __getitem__(self, input_type):\n        glue_mod = backend.get_var_type_glue(input_type)\n        assert glue_mod is not None, ValueError(\n            \'The backend of {} is not found\'.format(input_type))\n\n        def wrapper(*args, **kwargs):\n            new_kwargs = kwargs.copy()\n            new_kwargs.update(self.attrs)\n            new_kwargs[\'__input_type__\'] = input_type\n            return backend.op_gen(glue_mod, op=self.op, name=self.name)(*args, **new_kwargs)\n        return wrapper\n\n\ndef register(op_name=None, **attrs):\n    """"""Regiseter a custom operator\n    1. @register\n       class XXX\n    2. @register(""OP"")\n       class XXX\n    3. @register(a = 3)\n       class XXX\n    """"""\n    def decorator(op_name, op):\n        if op_name is None:\n            op_name = op.__name__\n        op_inst = MobulaOperator(op=op, name=op_name, **attrs)\n        if op_name in CUSTOM_OP_LIST:\n            warnings.warn(\n                \'Duplicate operator name {}, please rename it\'.format(op_name))\n        CUSTOM_OP_LIST[op_name] = op_inst\n        OP_MODULE_GLOBALS[op_name] = op_inst\n        return op_inst\n    if op_name is not None and not isinstance(op_name, str):\n        return decorator(None, op_name)\n    return functools.partial(decorator, op_name)\n\n\nINPUT_FUNCS = dict(\n    X=property(lambda self: self.in_data),\n    Y=property(lambda self: self.out_data),\n    dX=property(lambda self: self.in_grad),\n    dY=property(lambda self: self.out_grad),\n    x=property(lambda self: self.in_data[0]),\n    y=property(lambda self: self.out_data[0]),\n    dx=property(lambda self: self.in_grad[0]),\n    dy=property(lambda self: self.out_grad[0]),\n)\n\'\'\'\nOpGen:\n    in_data, out_data, in_grad, out_grad\n    req[write/add/null]\n    X,Y,dX,dY,x,y,dx,dy\n    F\n\'\'\'\ntry:\n    import numpy as np\n\n    def _get_numpy_type():\n        name2ctype = dict()\n        pairs = [\n            (np.dtype(\'int8\'), ctypes.c_int8),\n            (np.dtype(\'int16\'), ctypes.c_int16),\n            (np.dtype(\'int32\'), ctypes.c_int32),\n            (np.dtype(\'int64\'), ctypes.c_int64),  # alias: np.int\n            (np.dtype(\'float32\'), ctypes.c_float),\n            (np.dtype(\'float64\'), ctypes.c_double),  # alias: np.float\n        ]\n        for dtype, ctype in pairs:\n            name2ctype[dtype.name] = ctype\n        return name2ctype\n    NP_DTYPE_NAME2CTYPE = _get_numpy_type()\n\n    def NPDTYPE2CTYPE(dtype):\n        """"""Convert numpy data type into ctype.\n\n        Parameters\n        ----------\n        dtype: numpy.dtype\n\n        Returns\n        -------\n        ctype\n        """"""\n        ctype = NP_DTYPE_NAME2CTYPE.get(np.dtype(dtype).name, None)\n        assert ctype is not None, TypeError(\'Unknown Type: {}\'.format(dtype))\n        return ctype\nexcept ImportError:\n    pass\n'"
mobula/glue/cp.py,0,"b""import ctypes\nimport cupy as cp\nfrom .common import *\n\n\ndef get_pointer(v):\n    def p(e):\n        return ctypes.c_void_p(e.data.ptr)\n    if not v.flags.c_contiguous:\n        c = cp.ascontiguousarray(v)\n        return p(c), c\n    return p(v)\n\n\ndef get_ctype(v):\n    return NPDTYPE2CTYPE(v.dtype)\n\n\ndef dev_id(a):\n    if isinstance(a, cp.ndarray):\n        return a.device.id\n    return None\n\n\nclass OpGen(object):\n    def __init__(self, op, name):\n        self.op = op\n        self.name = name\n        self.cache = dict()\n\n    def __call__(self, *args, **kwargs):\n        if self.name not in self.cache:\n            # register operator\n            self.cache[self.name] = self.register()\n        kwargs.pop('__input_type__')\n        return self.cache[self.name](*args, **kwargs)\n\n    def register(self):\n        def forward(self, *args, **kwargs):\n            inputs, pars = get_in_data(op=self.op, *args, **kwargs)\n\n            self.in_data = inputs\n            dtype = self.in_data[0].dtype\n            self.req = ['write' for _ in self.in_data]\n            in_shape = get_in_shape(self.in_data)\n            out_shape = self.infer_shape(in_shape)[1]\n            self.out_data = [self.F.empty(s, dtype=dtype) for s in out_shape]\n            out = self._forward(*inputs)\n            if out is not None:\n                if not isinstance(out, (list, tuple)):\n                    out = [out]\n                for i, x in enumerate(out):\n                    self.assign(self.out_data[i], self.req[i], x)\n            if len(self.out_data) == 1:\n                return self.out_data[0]\n            return self.out_data\n\n        def backward(self, out_grad=None, in_data=None, out_data=None, in_grad=None, req=None):\n\n            if in_data is not None:\n                self.in_data = in_data\n            if out_data is not None:\n                self.out_data = out_data\n\n            dtype = self.in_data[0].dtype\n\n            if in_grad is None:\n                in_grad = [self.F.empty_like(d, dtype=dtype)\n                           for d in self.in_data]\n            else:\n                if not isinstance(in_grad, (list, tuple)):\n                    in_grad = [in_grad]\n            self.in_grad = in_grad\n\n            if out_grad is None:\n                out_grad = [self.F.ones_like(d, dtype=dtype)\n                            for d in self.out_data]\n            else:\n                if not isinstance(out_grad, (list, tuple)):\n                    out_grad = [out_grad]\n            self.out_grad = out_grad\n\n            if req is None:\n                self.req = ['write' for _ in self.in_data]\n            else:\n                assert len(req) == len(self.in_data),\\\n                    ValueError('len(req) should be %d' % len(self.in_data))\n                self.req = req\n            out = self._backward(*out_grad)\n            if out is not None:\n                if not isinstance(out, (list, tuple)):\n                    out = [out]\n                num_inputs = len(get_varnames(self._forward))\n                for i in range(num_inputs):\n                    self.assign(in_grad[i], self.req[i], out[i])\n            if len(in_grad) == 1:\n                return in_grad[0]\n            return self.in_grad\n\n        np_op_dict = dict(\n            __call__=forward,\n            forward=forward,\n            backward=backward,\n            _forward=self.op.forward,\n            _backward=self.op.backward,\n            infer_shape=self.op.infer_shape,\n            assign=assign,\n            F=property(lambda self: cp),\n            op=property(lambda dummy: self.op)\n        )\n        if hasattr(self.op, '__init__'):\n            np_op_dict['__init__'] = self.op.__init__\n        np_op_dict.update(INPUT_FUNCS)\n        np_op = type('_%s_CP_OP' % self.name,\n                     (self.op, object),\n                     np_op_dict)\n        return np_op\n\n\nF = cp\n"""
mobula/glue/mx.py,2,"b'from .common import *\nimport numpy as np\nimport mxnet as mx\nfrom mxnet.base import _LIB\n\n\nif not hasattr(mx.nd.NDArray, \'empty_like\'):\n    mx.nd.empty_like = lambda x: mx.nd.empty(\n        x.shape, dtype=x.dtype, ctx=x.context)\nif not hasattr(mx.nd.NDArray, \'wait_to_write\'):\n    mx.nd.NDArray.wait_to_write = lambda self: _LIB.MXNDArrayWaitToWrite(\n        self.handle)\n\n\ndef get_pointer(v):\n    cp = ctypes.c_void_p()\n    _LIB.MXNDArrayGetData(v.handle, ctypes.byref(cp))\n    return cp\n\n\ndef get_async_pointer(v):\n    return v.handle\n\n\ndef get_ctype(v):\n    return NPDTYPE2CTYPE(v.dtype)\n\n\ndef dev_id(a):\n    if isinstance(a, mx.nd.NDArray):\n        return a.context.device_id if a.context.device_type == \'gpu\' else None\n    return None\n\n\nasync_name = \'mx\'\n\ntry:\n    MX_LIB_APIS = [_LIB.MXShallowCopyNDArray, _LIB.MXNDArrayFree,\n                   _LIB.MXNDArrayGetContext, _LIB.MXNDArrayToDLPack,\n                   _LIB.MXEnginePushSyncND]\nexcept AttributeError as e:\n    warnings.warn(""""""Fail to enable asynchronous execution for MXNet, since the version of MXNet is old. It will drop the performance.\nIn order to improve the performance, please install MXNet whose version >= 1.6.0b20190809"""""")\n    MX_LIB_APIS = None\n\n\ndef get_async_func(cpp_info, func_idcode_hash):\n    if MX_LIB_APIS is None:\n        return None\n    cpp_info.dll.RegisterMXAPI.argtypes = [ctypes.c_void_p] * len(MX_LIB_APIS)\n    cpp_info.dll.RegisterMXAPI(*MX_LIB_APIS)\n    register_func_for_mx = getattr(\n        cpp_info.dll, func_idcode_hash + \'_register_mx\', None)\n    if register_func_for_mx is None:\n        return None\n    async_func_for_mx = getattr(cpp_info.dll, func_idcode_hash + \'_async_mx\')\n    register_func_for_mx.restype = ctypes.c_void_p\n    packed_func_mx = ctypes.c_void_p(register_func_for_mx())\n    func = lambda *args: async_func_for_mx(packed_func_mx, *args)\n    return func\n\n\nclass OpGen(object):\n    def __init__(self, op, name):\n        self.op = op\n        self.name = name\n        self.cache = dict()\n\n    def __call__(self, *args, **kwargs):\n        inputs, pars = get_in_data(op=self.op, *args, **kwargs)\n        op_type = self.name\n        name = pars[1].pop(\'name\', None)\n        input_type = pars[1].pop(\'__input_type__\', None)\n        if input_type is None:\n            input_type = type(inputs[0])\n        if op_type not in self.cache:\n            # register operator\n            self.cache[op_type] = True\n            self.register()\n        if input_type is mx.sym.Symbol:\n            return mx.sym.Custom(*inputs, mobula_pars=pars_encode(pars), op_type=op_type)\n        if hasattr(mx, \'numpy\'):\n            inputs = [x.as_nd_ndarray() if isinstance(\n                x, mx.np.ndarray) else x for x in inputs]\n        return mx.nd.Custom(*inputs, mobula_pars=pars_encode(pars), op_type=op_type, name=name)\n\n    def register(self):\n        op = self.op\n        op_name = self.name\n\n        def get_mx_op(op):\n            def __init__(self, *args, **kwargs):\n                self.__mx_prop__ = kwargs.pop(\'__mx_prop__\')\n                mx.operator.CustomOp.__init__(self)\n\n            def __getattr__(self, name):\n                return self.__dict__.get(name, getattr(self.__mx_prop__, name))\n\n            def forward(self, is_train, req, in_data, out_data, aux):\n                self.in_data = in_data\n                self.out_data = out_data\n                self.req = req\n                out = self._forward(*in_data)\n                if out is not None:\n                    if not isinstance(out, (list, tuple)):\n                        out = [out]\n                    for i, x in enumerate(out):\n                        self.assign(out_data[i], req[i], x)\n\n            def backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n                self.in_grad = in_grad\n                self.out_grad = out_grad\n                self.req = req\n                out = self._backward(*out_grad)\n                if out is not None:\n                    if not isinstance(out, (list, tuple)):\n                        out = [out]\n                    num_inputs = len(get_varnames(self._forward))\n                    for i in range(num_inputs):\n                        self.assign(in_grad[i], req[i], out[i])\n            mx_op_dict = dict(\n                __init__=__init__,\n                __getattr__=__getattr__,\n                forward=forward,\n                backward=backward,\n                _forward=op.forward,\n                _backward=op.backward,\n                F=property(lambda self: mx.nd),\n            )\n            mx_op_dict.update(INPUT_FUNCS)\n            mx_op = type(\'_%s_MX_OP\' % op_name,\n                         (op, mx.operator.CustomOp),\n                         mx_op_dict)\n            return mx_op\n\n        def get_mx_prop(op, mx_op):\n            def __init__(self, mobula_pars):\n                self._args, self._kwargs = pars_decode(mobula_pars)\n                mx.operator.CustomOpProp.__init__(\n                    self, need_top_grad=self._kwargs.pop(\'need_top_grad\', True))\n                if hasattr(op, \'__init__\'):\n                    op.__init__(self, *self._args, **self._kwargs)\n\n            def list_outputs(self, func):\n                num_outputs = getattr(\n                    self, \'num_outputs\', len(get_varnames(func)))\n                if num_outputs == 0:\n                    return []\n                if num_outputs == 1:\n                    return [\'output\']\n                return [\'output%d\' % i for i in range(num_outputs)]\n\n            def create_operator(self, ctx, shapes, dtypes):\n                with ctx:\n                    self._kwargs[\'__mx_prop__\'] = self\n                    rtn = mx_op(*self._args, **self._kwargs)\n                return rtn\n\n            def infer_type(self, in_type, func):\n                num_outputs = getattr(\n                    self, \'num_outputs\', len(get_varnames(func)))\n                dtype = in_type[0] if in_type else np.float32\n                return in_type, [dtype] * num_outputs\n\n            mx_prop_dict = dict(\n                __init__=__init__,\n                list_arguments=lambda self: get_varnames(op.forward),\n                list_outputs=lambda self: list_outputs(self, op.backward),\n                infer_shape=op.infer_shape,\n                create_operator=create_operator,\n                infer_type=lambda self, in_type: infer_type(\n                    self, in_type, op.backward),\n                F=property(lambda self: mx.nd),\n            )\n            optional_list = [\'list_arguments\', \'list_outputs\', \'infer_type\']\n            for o in optional_list:\n                if hasattr(op, o):\n                    mx_prop_dict[o] = getattr(op, o)\n\n            mx_prop = type(\'_%s_MX_OP_PROP\' % op_name,\n                           (op, mx.operator.CustomOpProp),\n                           mx_prop_dict)\n            return mx_prop\n\n        mx_op = get_mx_op(op)\n        mx_prop = get_mx_prop(op, mx_op)\n        mx.operator.register(op_name)(mx_prop)\n\n\nF = mx.nd\n'"
mobula/glue/np.py,1,"b""import ctypes\nimport numpy as np\nfrom .common import *\n\n\ndef get_pointer(v):\n    def p(e):\n        return e.ctypes.data_as(ctypes.c_void_p)\n    if not v.flags.c_contiguous:\n        c = np.ascontiguousarray(v)\n        return p(c), c\n    return p(v)\n\n\ndef get_ctype(v):\n    return NPDTYPE2CTYPE(v.dtype)\n\n\ndef dev_id(a):\n    return None\n\n\nclass OpGen(object):\n    def __init__(self, op, name):\n        self.op = op\n        self.name = name\n        self.cache = dict()\n\n    def __call__(self, *args, **kwargs):\n        if self.name not in self.cache:\n            # register operator\n            self.cache[self.name] = self.register()\n        kwargs.pop('__input_type__')\n        return self.cache[self.name](*args, **kwargs)\n\n    def register(self):\n        def forward(self, *args, **kwargs):\n            inputs, pars = get_in_data(op=self.op, *args, **kwargs)\n\n            self.in_data = inputs\n            dtype = self.in_data[0].dtype\n            self.req = ['write' for _ in self.in_data]\n            in_shape = get_in_shape(self.in_data)\n            out_shape = self.infer_shape(in_shape)[1]\n            self.out_data = [self.F.empty(s, dtype=dtype) for s in out_shape]\n            out = self._forward(*inputs)\n            if out is not None:\n                if not isinstance(out, (list, tuple)):\n                    out = [out]\n                for i, x in enumerate(out):\n                    self.assign(self.out_data[i], self.req[i], x)\n            if len(self.out_data) == 1:\n                return self.out_data[0]\n            return self.out_data\n\n        def backward(self, out_grad=None, in_data=None, out_data=None, in_grad=None, req=None):\n\n            if in_data is not None:\n                self.in_data = in_data\n            if out_data is not None:\n                self.out_data = out_data\n\n            dtype = self.in_data[0].dtype\n\n            if in_grad is None:\n                in_grad = [self.F.empty_like(d, dtype=dtype)\n                           for d in self.in_data]\n            else:\n                if not isinstance(in_grad, (list, tuple)):\n                    in_grad = [in_grad]\n            self.in_grad = in_grad\n\n            if out_grad is None:\n                out_grad = [self.F.ones_like(d, dtype=dtype)\n                            for d in self.out_data]\n            else:\n                if not isinstance(out_grad, (list, tuple)):\n                    out_grad = [out_grad]\n            self.out_grad = out_grad\n\n            if req is None:\n                self.req = ['write' for _ in self.in_data]\n            else:\n                assert len(req) == len(self.in_data),\\\n                    ValueError('len(req) should be %d' % len(self.in_data))\n                self.req = req\n            out = self._backward(*out_grad)\n            if out is not None:\n                if not isinstance(out, (list, tuple)):\n                    out = [out]\n                num_inputs = len(get_varnames(self._forward))\n                for i in range(num_inputs):\n                    self.assign(in_grad[i], self.req[i], out[i])\n            if len(in_grad) == 1:\n                return in_grad[0]\n            return self.in_grad\n\n        np_op_dict = dict(\n            __call__=forward,\n            forward=forward,\n            backward=backward,\n            _forward=self.op.forward,\n            _backward=self.op.backward,\n            infer_shape=self.op.infer_shape,\n            assign=assign,\n            F=property(lambda self: np),\n            op=property(lambda dummy: self.op)\n        )\n        if hasattr(self.op, '__init__'):\n            np_op_dict['__init__'] = self.op.__init__\n        np_op_dict.update(INPUT_FUNCS)\n        np_op = type('_%s_NP_OP' % self.name,\n                     (self.op, object),\n                     np_op_dict)\n        return np_op\n\n\nF = np\n"""
mobula/glue/th.py,0,"b""import ctypes\nimport torch\nfrom .common import *\n\n\ndef get_pointer(v):\n    def p(e):\n        return ctypes.c_void_p(e.data_ptr())\n    if not v.is_contiguous():\n        c = v.contiguous()\n        return p(c), c\n    return p(v)\n\n\nTHDTYPE2CTYPE_MAP = dict()\nTHDTYPE2CTYPE_MAP[torch.int] = ctypes.c_int\nTHDTYPE2CTYPE_MAP[torch.float] = ctypes.c_float\nTHDTYPE2CTYPE_MAP[torch.double] = ctypes.c_double\n\n\ndef get_ctype(v):\n    dtype = v.dtype\n    ctype = THDTYPE2CTYPE_MAP.get(dtype, None)\n    assert ctype is not None, TypeError('Unknown Type: {}'.format(dtype))\n    return ctype\n\n\ndef dev_id(a):\n    if isinstance(a, torch.Tensor):\n        dev = a.device\n        return None if dev.type == 'cpu' else dev.index\n    return None\n\n\nclass OpGen(object):\n    def __init__(self, op, name):\n        self.op = op\n        self.name = name\n        self.cache = dict()\n\n    def __call__(self, *args, **kwargs):\n        if self.name not in self.cache:\n            # register operator\n            self.cache[self.name] = self.register()\n        inputs, pars = get_in_data(op=self.op, *args, **kwargs)\n        return self.cache[self.name](*pars[0], **pars[1])(*inputs)\n\n    def register(self):\n        op = self.op\n        op_name = self.name\n\n        def get_torch_func(op):\n            def forward(ctx, self, *args, **kwargs):\n                ctx.self = self\n                self.in_data = args\n                self.req = ['write' for _ in self.in_data]\n                in_shape = get_in_shape(self.in_data)\n                out_shape = self.infer_shape(in_shape)[1]\n                dtype = self.in_data[0].dtype if self.in_data else torch.float32\n                device = self.in_data[0].device if self.in_data else torch.device(\n                    'cpu')\n                self.out_data = [self.F.empty(\n                    s, dtype=dtype, device=device) for s in out_shape]\n                out = self._forward(*args, **kwargs)\n                if out is not None:\n                    if not isinstance(out, (list, tuple)):\n                        out = [out]\n                    for i, x in enumerate(out):\n                        self.assign(self.out_data[i], self.req[i], x)\n                if len(self.out_data) == 1:\n                    return self.out_data[0]\n                return tuple(self.out_data)\n\n            def backward(ctx, *args, **kwargs):\n                self = ctx.self\n                dtype = self.in_data[0].dtype if self.in_data else torch.float32\n                device = self.in_data[0].device if self.in_data else torch.device(\n                    'cpu')\n                self.in_grad = [self.F.empty_like(d, dtype=dtype, device=device) if d.grad is None\n                                else d.grad for d in self.in_data]\n                self.out_grad = args\n                out = self._backward(*args, **kwargs)\n                if out is not None:\n                    if not isinstance(out, (list, tuple)):\n                        out = [out]\n                    num_inputs = len(get_varnames(self._forward))\n                    for i in range(num_inputs):\n                        self.assign(self.in_grad[i], self.req[i], out[i])\n\n                if len(self.in_grad) == 1:\n                    return None, self.in_grad[0]\n                return (None, ) + tuple(self.in_grad)\n\n            torch_func_dict = dict(\n                forward=staticmethod(forward),\n                backward=staticmethod(backward),\n            )\n            torch_func = type('_%s_TORCH_FUNC' % op_name,\n                              (op, torch.autograd.Function),\n                              torch_func_dict)\n            return torch_func\n\n        def get_torch_nn_module(op, torch_func):\n            def __init__(self, *args, **kwargs):\n                torch.nn.Module.__init__(self)\n                if hasattr(op, '__init__'):\n                    op.__init__(self, *args, **kwargs)\n\n            def forward(self, *args, **kwargs):\n                return torch_func.apply(self, *args, **kwargs)\n\n            torch_nn_module_dict = dict(\n                __init__=__init__,\n                forward=forward,\n                _forward=op.forward,\n                _backward=op.backward,\n                assign=assign,\n                F=property(lambda self: torch),\n            )\n\n            torch_nn_module_dict.update(INPUT_FUNCS)\n\n            torch_nn_module = type('_%s_TORCH_NN_MODULE' % op_name,\n                                   (op, torch.nn.Module),\n                                   torch_nn_module_dict)\n            return torch_nn_module\n\n        torch_func = get_torch_func(op)\n        torch_nn_module = get_torch_nn_module(op, torch_func)\n        return torch_nn_module\n\n\nF = torch\n"""
mobula/op/__init__.py,0,"b""__all__ = ['register', 'load']\n\n\nfrom .. import glue\nglue.common.OP_MODULE_GLOBALS = globals()\nfrom .register import register\nfrom .loader import load\n"""
mobula/op/custom.py,0,"b'""""""Custom Operator.""""""\nfrom .. import glue\n\n\ndef Custom(op_name):\n    """"""Get custom operator whose name is `op_name`.\n\n    Parameters\n    ----------\n    op_name: str\n        The name of custom operator.\n\n    Returns\n    -------\n    Custom Operator\n    """"""\n    assert op_name in glue.CUSTOM_OP_LIST, KeyError(\n        \'Operator {} not found\'.format(op_name))\n    return glue.CUSTOM_OP_LIST[op_name]\n\n\ndef CustomList():\n    """"""Get the name list of custom operators\n    Returns\n    -------\n    list of str\n        The name list of custom operators\n    """"""\n    return glue.CUSTOM_OP_LIST.keys()\n'"
mobula/op/gen_code.py,0,"b'import copy\nimport os\nimport re\n\nREPLACE_MARK_PATTERN = re.compile(r\'\\$\\s*{\\s*(.*?)\\s*}\')\n\n\nclass CodeGenerator:\n    def __init__(self, fname):\n        self.fname = fname\n        with open(fname) as fin:\n            self.code = fin.read()\n            self.pit = re.finditer(REPLACE_MARK_PATTERN, self.code)\n\n    def __call__(self, **kwargs):\n        code = """"\n        i = 0\n        for m in self.pit:\n            span = m.span()\n            code += self.code[i:span[0]]\n            i = span[1]\n            name = m.groups()[0]\n            if name not in kwargs:\n                raise Exception(\'There are some variables which are assigned:\\n{} in {}\'.format(\n                    name, self.fname))\n            code += str(kwargs[name])\n        code += self.code[i:]\n        return code\n\n\ndef gen_code(fname):\n    return CodeGenerator(fname)\n\n\ndef get_gen_rel_code(path):\n    def gen_rel_code(fname):\n        return gen_code(os.path.join(path, fname))\n    return gen_rel_code\n'"
mobula/op/loader.py,0,"b'""""""Operator Loader.""""""\nfrom collections import namedtuple\nimport os\nimport sys\nimport re\nimport time\nimport ctypes\nimport json\nimport warnings\nimport portalocker\nfrom ..edict import edict\nfrom ..func import CFuncDef, bind, get_func_idcode, get_idcode_hash\nfrom ..build import config, source_to_so_ctx, build_context, file_is_changed, ENV_PATH\nfrom ..utils import get_git_hash, makedirs\nfrom ..dtype import DType, TemplateType\nfrom ..version import OP_LOAD_MODULE_BUILD_VERSION\nfrom ..glue.backend import get_glue_modules\nfrom .gen_code import get_gen_rel_code\n\ngen_code = get_gen_rel_code(os.path.dirname(__file__))\n\n\nif sys.version_info[0] >= 3:\n    import importlib.util\n\n    def load_module(name, pathname):\n        """"""Load Module.\n\n        Paramters\n        ---------\n        name: str\n            the name of module.\n        pathname:\n            the name of path.\n\n        Returns\n        -------\n        Module\n        """"""\n        spec = importlib.util.spec_from_file_location(name, pathname)\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n        return module\nelse:\n    import imp\n\n    def load_module(name, pathname):\n        """"""Load Module.\n\n        Paramters\n        ---------\n        name: str\n            the name of module.\n        pathname:\n            the name of path.\n\n        Returns\n        -------\n        Module\n        """"""\n        module = imp.load_source(name, pathname)\n        return module\n\n\ndef _get_func_head_reg(name):\n    """"""Get a pattern object for CFunction Head.\n\n    Paramters\n    ---------\n    name: str\n        Function name.\n\n    Returns\n    -------\n    A pattern object\n    """"""\n    return re.compile(r\'^\\s*{}\\s*(.*)\'.format(name))\n\n\nMOBULA_KERNEL_REG = _get_func_head_reg(\'MOBULA_(KERNEL|FUNC)\')\n\nFUNC_REG = re.compile(\n    r\'^\\s*(.*?)\\s*\\((.*?)\\)(?:.*?)*\')\nCPP_TEMPLATE_REG = re.compile(r\'^\\s*template\\s*\\<(.*?)\\>\\s*\')\n\n\ndef _get_template_decl(code):\n    match = CPP_TEMPLATE_REG.search(code)\n    if match is None:\n        return None\n    blocks = match.groups()[0].split(\',\')\n    templates = []\n    for block in blocks:\n        block_sp = block.split()\n        dtype, dname = block_sp\n        if dtype.strip() == \'typename\':\n            templates.append(dname.strip())\n    return templates\n\n\ndef parse_parameter_decl(decl):\n    """"""Parse the code of parameter declaration\n\n    Parameters\n    ----------\n    decl : str\n        The C++ code of parameter declaration\n\n    Returns\n    -------\n    Tuple\n        (DType Instance,  variable name)\n    """"""\n    num_star = decl.count(\'*\')\n    assert num_star <= 1,\\\n        Exception(\'Only support pass-by-value or pass-by-1-level-pointer, \\\n            Error declaration: {}\'.format(decl))\n    is_pointer = num_star > 0\n    if is_pointer:\n        decl = decl.replace(\'*\', \'\')\n    decl = decl.strip()\n    if decl.startswith(\'const \'):\n        is_const = True\n        decl = decl[len(\'const \'):]\n    else:\n        is_const = False\n    decl_sp = decl.split(\' \')\n\n    # type_name and variable_name in C++ code\n    type_name, var_name = decl_sp\n\n    # void* func(...)\n    if type_name == \'void\':\n        assert is_pointer\n        return DType(ctypes.c_void_p, is_const=is_const), var_name\n\n    # ctype func(...)\n    ctype_name = \'c_{}\'.format(type_name)\n    if hasattr(ctypes, ctype_name):\n        ctype = getattr(ctypes, ctype_name)\n        if is_pointer:\n            ctype = ctypes.POINTER(ctype)\n        return DType(ctype, is_const=is_const), var_name\n\n    # template type\n    return TemplateType(tname=type_name, is_pointer=is_pointer, is_const=is_const), var_name\n\n\ndef parse_parameters_list(plist):\n    """"""Parse the code of parameter declaration list\n\n    Parameters\n    ----------\n    plist : str\n        The code of parameter declaration list\n\n    Returns\n    -------\n    rtn_type :\n        The type of return value\n    func_name : str\n        function name\n    pars_list: list\n        [(DType|TemplateType, variable name), ...]\n    """"""\n\n    match = FUNC_REG.search(plist)\n    head, plist = match.groups()\n    head_split = re.split(r\'\\s+\', head)\n    plist_split = re.split(r\'\\s*,\\s*\', plist)\n    func_name = head_split[-1]\n    rtn_type = head_split[-2] if len(head_split) == 3 else None\n    pars_list = list(map(parse_parameter_decl, plist_split))\n    return rtn_type, func_name, pars_list\n\n\n# runtime\nFuncInfo = namedtuple(\'FuncInfo\', [\'func\', \'cpp_info\'])\nCTX_FUNC_MAP = dict()  # CTX_FUNC_MAP[ctx][cpp_fname] -> FuncInfo\n\n\nclass CPPInfo:\n    """"""The class of the C++ file\'s information.\n\n    Parameters\n    ----------\n    cpp_fname: str\n        the filename of C++ file.\n    """"""\n\n    def __init__(self, cpp_fname):\n        self.cpp_fname = cpp_fname\n        self.function_args = dict()\n        self.dll = None\n\n    def load_dll(self, dll_fname):\n        """"""Load Dynamic-Link Library(*.so or *.dll).\n\n        Parameters\n        ----------\n        dll_fname:\n            The name of Dynamic-Link Library.\n        """"""\n        # keep reference\n        self.dll = ctypes.CDLL(dll_fname)\n\n\ndef _build_lib(cpp_fname, code_buffer, ctx, target_name):\n    cpp_path, cpp_basename = os.path.split(cpp_fname)\n    build_path = os.path.join(cpp_path, \'build\')\n    create_time = time.strftime(\'%a %Y-%m-%d %H:%M:%S (%z)\', time.localtime())\n    git_hash = get_git_hash()\n    extra_code = gen_code(\'./templates/header.cpp\')(\n        cpp_fname=cpp_fname,\n        git_hash=git_hash,\n        create_time=create_time,\n        inc_fname=os.path.normpath(os.path.join(\'../..\', cpp_basename)),\n        code=code_buffer)\n\n    build_path_ctx = os.path.join(build_path, ctx)\n    makedirs(build_path_ctx, exist_ok=True)\n\n    # build so\n    cpp_wrapper_fname = os.path.join(build_path_ctx,\n                                     os.path.splitext(cpp_basename)[0] + \'_wrapper.cpp\')\n    with open(cpp_wrapper_fname, \'w\') as fout:\n        fout.write(extra_code)\n    # build lib\n    srcs = [cpp_wrapper_fname]\n\n    source_to_so_ctx(build_path, srcs, target_name, ctx)\n\n\ndef _dtype_to_tvm_value_type(dtype):\n    if dtype.is_pointer:\n        return \'v_handle\'\n    if \'int\' in dtype.cname:\n        return \'v_int64\'\n    return \'v_float64\'\n\n\ndef _get_args_inst_mx(i, t):\n    s = \'args.values[%d].%s\' % (i, _dtype_to_tvm_value_type(t))\n    if t.is_pointer:\n        return \'\'\'\n          static_cast<{dtype}>(\n            static_cast<DLTensor*>({tv})->data)\'\'\'.format(dtype=t.cname, tv=s)\n    else:\n        s = \'\\n          \' + s\n    return s\n\n\ndef _generate_kernel_code(func_idcode_hash, arg_types, arg_names, func_name):\n    args_def = \', \'.join([\'{ctype} {name}\'.format(\n        ctype=dtype.cname,\n        name=name\n    ) for dtype, name in zip(arg_types, arg_names)])\n    args_inst = \', \'.join(arg_names)\n\n    kernel_code = gen_code(\'./templates/kernel_code.cpp\')(\n        func_idcode_hash=func_idcode_hash,\n        args_def=args_def,\n        func_name=func_name,\n        args_inst=args_inst)\n    kernel_code += \'\\n\'\n\n    args_def_async_mx = \', \'.join([\'{ctype} {name}\'.format(\n        ctype=\'NDArrayHandle\' if dtype.is_pointer else dtype.cname,\n        name=name\n    ) for dtype, name in zip(arg_types, arg_names)])\n\n    using_async_mx = all(\n        map(lambda dtype: \'void\' not in dtype.cname, arg_types))\n    if using_async_mx:\n        args_inst_mx = [_get_args_inst_mx(i, t)\n                        for i, t in enumerate(arg_types)]\n        const_loc = []\n        for i, dtype in enumerate(arg_types):\n            if dtype.is_const and dtype.is_pointer:\n                const_loc.append(i)\n        num_const = len(const_loc)\n        const_loc_code = \'nullptr\' if num_const == 0 else \'std::array<int, %d>({%s}).data()\' % (\n            num_const, \',\'.join([str(u) for u in const_loc]))\n        async_mx_code = gen_code(\'./templates/async_mx_code.cpp\')(\n            func_idcode_hash=func_idcode_hash,\n            func_name=func_name,\n            args_inst=args_inst,\n            args_inst_mx=\',\'.join(args_inst_mx),\n            num_const=num_const,\n            const_loc_code=const_loc_code,\n            args_def_async_mx=args_def_async_mx,\n        )\n        async_mx_code += \'\\n\'\n        kernel_code += async_mx_code\n    return kernel_code\n\n\ndef _generate_func_code(func_idcode_hash, rtn_type, arg_types, arg_names, func_name):\n    if rtn_type is None:\n        rtn_type = \'void\'\n\n    args_def = \', \'.join([\'{ctype} {name}\'.format(\n        ctype=dtype.cname,\n        name=name\n    ) for dtype, name in zip(arg_types, arg_names)])\n    args_inst = \', \'.join(arg_names)\n\n    code = \'\'\'\nMOBULA_DLL %s %s(%s) {\n\'\'\' % (rtn_type, func_idcode_hash, args_def)\n    if rtn_type != \'void\':\n        code += \'  return \'\n    code += \'%s(%s);\\n}\\n\' % (func_name, args_inst)\n    return code\n\n\ndef _generate_ordinary_code(cpp_info):\n    code_buffer = \'\'\n    # generate ordinary functions code\n    for func_name, ord_cfunc in cpp_info.function_args.items():\n        if ord_cfunc.template_list:\n            continue\n        func_idcode = get_func_idcode(func_name, ord_cfunc.arg_types)\n        func_idcode_hash = get_idcode_hash(func_idcode)\n        func_kind = ord_cfunc.func_kind\n        if func_kind == CFuncDef.KERNEL:\n            code_buffer += _generate_kernel_code(\n                func_idcode_hash, ord_cfunc.arg_types, ord_cfunc.arg_names, \'{}_kernel\'.format(func_name))\n            code_buffer += \'\\n\'\n    return code_buffer\n\n\ndef _get_ordinary_functions(cpp_info):\n    res = list()\n    for func_name, ord_cfunc in cpp_info.function_args.items():\n        if ord_cfunc.template_list:\n            continue\n        func_idcode = get_func_idcode(func_name, ord_cfunc.arg_types)\n        res.append(func_idcode)\n    return res\n\n\ndef _update_template_inst_map(idcode, template_functions, cfunc, arg_types):\n    # template function\n    func_name = cfunc.func_name\n    func_idcode_hash = get_idcode_hash(idcode)\n    # Check Template Type Mapping\n    template_mapping = dict()\n    for rtype, dtype in zip(arg_types, cfunc.arg_types):\n        if not isinstance(dtype, TemplateType):\n            continue\n        tname = dtype.tname\n        rtype = str(rtype).replace(\n            \'const\', \'\').replace(\'*\', \'\').strip()\n        if tname in template_mapping:\n            assert template_mapping[tname] == rtype,\\\n                Exception(\'Excepted template type {} instead of {}\'.\n                          format(template_mapping[tname], rtype))\n        else:\n            template_mapping[tname] = rtype\n    assert len(template_mapping) == len(cfunc.template_list),\\\n        Exception(\'Template List: {}, mapping: {}\'.\n                  format(cfunc.template_list, template_mapping))\n\n    template_inst = [template_mapping[tname]\n                     for tname in cfunc.template_list]\n    template_post = \'<%s>\' % (\', \'.join(template_inst))\n    rtn_type = cfunc.rtn_type\n    if rtn_type in template_mapping:\n        rtn_type = template_mapping[rtn_type]\n\n    func_kind = cfunc.func_kind\n    if func_kind == CFuncDef.KERNEL:\n        code = _generate_kernel_code(func_idcode_hash, arg_types, cfunc.arg_names, \'({}_kernel{})\'.format(\n            func_name, template_post))\n    else:\n        code = _generate_func_code(\n            func_idcode_hash, rtn_type, arg_types, cfunc.arg_names, func_name + template_post)\n    template_functions[idcode] = code\n\n\ndef _add_function(func_map, func_idcode, cpp_info, dll_fname):\n    func_idcode_hash = get_idcode_hash(func_idcode)\n    func = getattr(cpp_info.dll, func_idcode_hash, None)\n    assert func is not None,\\\n        Exception(\'No function `{}` in DLL {}\'.format(\n            func_idcode, dll_fname))\n\n    old_func = func_map.get(func_idcode, None)\n    if old_func is not None:\n        if old_func.cpp_info.cpp_fname != cpp_info.cpp_fname:\n            warnings.warn(\'The function `{}` in `{}` will be overridden by that in `{}`\'.format(\n                func_idcode, old_func.cpp_info.cpp_fname, cpp_info.cpp_fname))\n\n    func_map[func_idcode] = FuncInfo(func=func, cpp_info=cpp_info)\n\n\nclass OpLoader:\n    \'\'\'Import Operator Loader.\n    It\'s actual to load the operator.\n\n    Parameters\n    ----------\n    cfunc: CFuncDef\n        The definition of function to call.\n    arg_types: list of {DType|TemplateType}\n        Argument declaration list.\n    ctx: str\n        Building context.\n    cpp_info: CPPInfo\n        Related to cfunc.\n\n    Returns\n    -------\n    CTX_FUNC_MAP[ctx][fname][idcode] : FuncInfo\n    \'\'\'\n\n    def __init__(self, cfunc, arg_types, ctx, cpp_info):\n        idcode = get_func_idcode(cfunc.func_name, arg_types)\n        if ctx not in CTX_FUNC_MAP:\n            CTX_FUNC_MAP[ctx] = dict()\n        cpp_fname = cpp_info.cpp_fname\n        if cpp_fname not in CTX_FUNC_MAP[ctx]:\n            CTX_FUNC_MAP[ctx][cpp_fname] = dict()\n        # func_map: dict mapping idcode to CFunction\n        func_map = CTX_FUNC_MAP[ctx][cpp_fname]\n\n        if idcode not in func_map:\n            \'\'\'\n            *load function* when one of the following conditions is True:\n            1. idcode is not loaded\n            2. loading the function with same function name but different cpp filename\n            \'\'\'\n            cpp_path, cpp_basename = os.path.split(cpp_fname)\n            build_path = os.path.join(cpp_path, \'build\')\n\n            use_template = bool(cfunc.template_list)\n            makedirs(build_path, exist_ok=True)\n            build_info_fname = os.path.join(\n                build_path, os.path.splitext(cpp_basename)[0] + \'.json\')\n            build_info_fs = open(build_info_fname, \'a+\')\n            portalocker.lock(build_info_fs, portalocker.LOCK_EX)\n            build_info_fs.seek(0)\n            js_data = build_info_fs.read()\n            if js_data:\n                map_data = json.loads(js_data)\n            else:\n                map_data = dict(version=OP_LOAD_MODULE_BUILD_VERSION)\n            del js_data\n\n            # try to load the instance of template function\n            # map_data is a dict which records build information\n            if map_data.get(\'version\') > OP_LOAD_MODULE_BUILD_VERSION:\n                portalocker.unlock(build_info_fs)\n                raise Exception(\n                    """"""Unsupported higher version %s of wrapper file (Current MobulaOP ver: %s) :-(.\nPlease update MobulaOP."""""" % (map_data.get(\'version\'), OP_LOAD_MODULE_BUILD_VERSION))\n            build_id = map_data.get(\'build_id\', 0)\n            is_old_version = map_data.get(\n                \'version\') < OP_LOAD_MODULE_BUILD_VERSION\n            # load the information of template functions\n            ORDINARY_FUNCTION_NAME = \'ordinary_functions\'\n            TEMPLATE_FUNCTION_NAME = \'template_functions\'\n            if is_old_version:\n                ordinary_functions = list()\n                template_functions = dict()\n            else:\n                ordinary_functions = map_data.get(\n                    ORDINARY_FUNCTION_NAME, list())\n                template_functions = map_data.get(\n                    TEMPLATE_FUNCTION_NAME, dict())\n\n            so_prefix = os.path.join(\n                cpp_path, \'build\', os.path.splitext(cpp_basename)[0])\n            # The filename of build target\n            dll_fname_format = \'{prefix}_{ctx}\'.format(\n                prefix=so_prefix, ctx=ctx) + \'_{build_id}.so\'\n            dll_fname = dll_fname_format.format(build_id=build_id)\n\n            file_changed = file_is_changed(cpp_fname)\n            dll_existed = os.path.exists(dll_fname)\n            func_existed = idcode in template_functions or idcode in ordinary_functions\n\n            if file_changed or not dll_existed or not func_existed or is_old_version:\n                # Rebuild DLL file\n                try:\n                    # try to remove old DLL file\n                    os.remove(dll_fname)\n                except:\n                    pass\n                if file_changed:\n                    # clear template_functions since some functions may have been deleted or renamed after codefile is changed.\n                    template_functions.clear()\n                if file_changed or not func_existed:\n                    \'\'\'\n                    we increase `build_id` by 1 when one of the following conditions is True:\n                    1. the cpp file has been changed\n                    2. new idcode\n\n                    When the cpp file is not changed, and idcode exists in template_functions,\n                    `build_id` will be not changed.\n                    \'\'\'\n                    build_id += 1\n                dll_fname = dll_fname_format.format(build_id=build_id)\n                # build code\n                code_buffer = _generate_ordinary_code(cpp_info)\n                ordinary_functions = _get_ordinary_functions(cpp_info)\n                if use_template:\n                    if idcode not in template_functions:\n                        _update_template_inst_map(\n                            idcode, template_functions, cfunc, arg_types)\n                    # add template instances code into code_buffer\n                    code_buffer += \'\'.join(template_functions.values())\n\n                with build_context():\n                    try:\n                        _build_lib(cpp_fname, code_buffer, ctx, dll_fname)\n                    except:\n                        # if build fail, unlock the build info file\n                        portalocker.unlock(build_info_fs)\n                        raise\n                # update template_functions\n                map_data = dict(version=OP_LOAD_MODULE_BUILD_VERSION,\n                                build_id=build_id)\n                map_data[ORDINARY_FUNCTION_NAME] = ordinary_functions\n                map_data[TEMPLATE_FUNCTION_NAME] = template_functions\n                # clear the old context and write json data\n                build_info_fs.seek(0)\n                build_info_fs.truncate()\n                json.dump(map_data, build_info_fs)\n                build_info_fs.flush()\n                os.fsync(build_info_fs.fileno())\n            portalocker.unlock(build_info_fs)\n\n            # load all functions in the dll\n            cpp_info.load_dll(dll_fname)\n\n            # import all functions\n            # ordinary functions\n            for func_name, ord_cfunc in cpp_info.function_args.items():\n                if not ord_cfunc.template_list:\n                    func_idcode = get_func_idcode(\n                        func_name, ord_cfunc.arg_types)\n                    _add_function(func_map,\n                                  func_idcode, cpp_info, dll_fname)\n\n            # template functions\n            for func_idcode in template_functions.keys():\n                _add_function(func_map,\n                              func_idcode, cpp_info, dll_fname)\n\n        self.func = func_map[idcode].func\n        self.cpp_info = func_map[idcode].cpp_info\n        self.idcode_hash = get_idcode_hash(idcode)\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    def get_async_func(self, glue_mod):\n        async_name = getattr(glue_mod, \'async_name\', None)\n        if async_name is None:\n            return None\n        return glue_mod.get_async_func(self.cpp_info, self.idcode_hash)\n\n\ndef _get_functions_from_cpp(cpp_fname):\n    unmatched_brackets = 0\n    func_def = \'\'\n    func_kind = \'\'\n    func_started = False\n    template_list = []\n    cpp_info = CPPInfo(cpp_fname=cpp_fname)\n    function_args = cpp_info.function_args\n    for line in open(cpp_fname):\n        if not func_started:\n            current_template_list = _get_template_decl(line)\n            if current_template_list is not None:\n                template_list = current_template_list\n            match = MOBULA_KERNEL_REG.search(line)\n            if match is not None:\n                func_def = \'\'\n                func_kind_str = match.groups()[0]\n                if func_kind_str == \'KERNEL\':\n                    func_kind = CFuncDef.KERNEL\n                elif func_kind_str == \'FUNC\':\n                    func_kind = CFuncDef.FUNC\n                else:\n                    raise TypeError(\n                        \'Unknown kind of function: %s\' % func_kind_str)\n                func_started = True\n        # In a declaration of a function\n        if func_started:\n            unmatched_brackets += line.count(\'(\') - line.count(\')\')\n            func_def += line\n            if unmatched_brackets == 0:\n                func_def = func_def.replace(\'\\n\', \'\').replace(\'\\r\', \'\')\n                func_started = False\n                rtn_type, kernel_name, par_list = parse_parameters_list(\n                    func_def)\n                # template name check\n                template_set = set(template_list)\n                assert len(template_set) == len(template_list),\\\n                    Exception(\'Duplicated template name in {}\'.format(\n                        \', \'.join(template_list)))\n                use_template = False\n                for dtype, _ in par_list:\n                    if isinstance(dtype, TemplateType):\n                        assert dtype.tname in template_set,\\\n                            Exception(\n                                ""template name \'{}\' is not defined"".format(dtype.tname))\n                        use_template = True\n                if not use_template:\n                    template_list = []\n\n                if func_kind == CFuncDef.KERNEL:\n                    assert kernel_name.endswith(\'_kernel\'),\\\n                        Exception(\'the postfix of a MOBULA_KERNEL name must be `_kernel`, \\\n                            e.g. addition_forward_kernel\')\n                    func_name = kernel_name[:-len(\'_kernel\')]\n                elif func_kind == CFuncDef.FUNC:\n                    func_name = kernel_name\n                else:\n                    raise Exception(\n                        \'Unknown function kind: {}\'.format(func_kind))\n\n                # Arguments\n                funcdef_args = edict(func_name=func_name,\n                                     func_kind=func_kind,\n                                     arg_names=[t[1] for t in par_list],\n                                     arg_types=[t[0] for t in par_list],\n                                     rtn_type=rtn_type,\n                                     template_list=template_list,\n                                     loader=OpLoader,\n                                     loader_kwargs=dict(\n                                         cpp_info=cpp_info,\n                                     )\n                                     )\n                template_list = []\n                function_args[func_name] = funcdef_args\n\n    assert unmatched_brackets == 0,\\\n        Exception(\'# unmatched brackets: {}\'.format(unmatched_brackets))\n\n    # Load dynamic file\n    functions = dict(\n        (name, CFuncDef(**kwargs)) for name, kwargs in function_args.items())\n    # Load dynamic function for MXNet\n    return functions\n\n\ndef load(module_name, path=\'\'):\n    """"""Load Operator Module\n\n    Parameters\n    ----------\n    module_name: str\n        The name of Operator Module\n    path: str\n        The path of Operator Module [default = current path]\n    """"""\n    op_name = os.path.basename(module_name)\n    if not path:\n        # Find Operator Module in custom directory first\n        custom_path = os.path.join(os.path.dirname(__file__), \'../../opzoo\')\n        if os.path.exists(os.path.join(custom_path, op_name)):\n            path = custom_path\n    path = os.path.join(path, module_name)\n\n    found = False\n    cpp_fname = os.path.join(path, op_name + \'.cpp\')\n    if os.path.exists(cpp_fname):\n        found = True\n        # Get functions\n        functions = _get_functions_from_cpp(cpp_fname)\n        bind(functions)\n\n    py_fname = os.path.join(path, op_name + \'.py\')\n    if not os.path.exists(py_fname):\n        py_fname = os.path.join(path, \'__init__.py\')\n\n    if os.path.exists(py_fname):\n        found = True\n        # Create Operator\n        load_module(op_name, py_fname)\n    assert found,\\\n        IOError(""{op_name}.cpp or {op_name}.py or __init__.py not found\\\n in the path {path}"".format(op_name=op_name, path=path))\n'"
mobula/op/register.py,0,"b'""""""Register for Custom Operator.""""""\nfrom .. import glue\n\n\ndef register(*args, **kwargs):\n    """"""The Wrapper to regiser Custom Operator\n\n    Parameters\n    ----------\n    *args\n    **kwargs\n    """"""\n    return glue.register(*args, **kwargs)\n'"
opzoo/ROIAlign/ROIAlign.py,2,"b'import mobula\nfrom mobula.const import req\n\nimport numpy as np\n\n\n@mobula.op.register\nclass ROIAlign:\n    def __init__(self, pooled_size, spatial_scale, sampling_ratio):\n        self.pooled_size = pooled_size\n        self.spatial_scale = spatial_scale\n        self.sampling_ratio = sampling_ratio\n\n    def forward(self, data, rois):\n        if self.req[0] == req.null:\n            return\n\n        out = self.y\n        out_size = np.prod(out.size()) if callable(out.size) else out.size\n\n        if self.req[0] == req.add:\n            out_temp = self.F.empty_like(out)\n            mobula.func.roi_align_forward(out_size, data, self.spatial_scale, data.shape[1], data.shape[\n                                          2], data.shape[3], self.pooled_size[0], self.pooled_size[1], self.sampling_ratio, rois, out_temp)\n            self.y[:] += out_temp\n        else:\n            mobula.func.roi_align_forward(out_size, data, self.spatial_scale, data.shape[1], data.shape[\n                                          2], data.shape[3], self.pooled_size[0], self.pooled_size[1], self.sampling_ratio, rois, self.y)\n\n    def backward(self, dy):\n        if self.req[0] == req.null:\n            return\n        if self.req[0] != req.add:\n            self.dX[0][:] = 0\n        data, rois = self.X\n\n        dy_size = np.prod(dy.size()) if callable(dy.size) else dy.size\n        mobula.func.roi_align_backward(dy_size, dy, self.spatial_scale, data.shape[1], data.shape[2], data.shape[\n                                       3], self.pooled_size[0], self.pooled_size[1], self.sampling_ratio, self.dX[0], rois)\n\n        if self.req[1] not in [req.null, req.add]:\n            self.dX[1][:] = 0\n\n    def infer_shape(self, in_shape):\n        dshape, rshape = in_shape\n        assert len(dshape) == 4\n        assert len(rshape) == 2\n        assert rshape[1] == 5\n        oshape = [rshape[0], dshape[1],\n                  self.pooled_size[0], self.pooled_size[1]]\n        return [dshape, rshape], [oshape]\n'"
opzoo/Softmax/Softmax.py,0,"b'import mobula\nfrom mobula.const import req\n\n\n# this softmax support 1 or 2-dim input and the reduce on axis 0\n@mobula.op.register\nclass Softmax:\n    def forward(self, x):\n        if x.ndim == 2:\n            N, C = x.shape\n        else:\n            N, C = 1, x.size\n        if C > N:\n            tmp = self.F.empty_like(x)\n            mobula.func.softmax_channel_forward(C, N, x, tmp, self.y)\n        else:\n            mobula.func.softmax_batch_forward(N, C, x, self.y)\n\n    def backward(self, dy):\n        if self.req[0] == req.null:\n            return\n        elif self.req[0] == req.write:\n            self.dx[:] = 0\n        if dy.ndim == 2:\n            N, C = dy.shape\n        else:\n            N, C = 1, dy.size\n        mobula.func.softmax_backward(N, C, self.y, dy, self.dx)\n\n    def infer_shape(self, in_shape):\n        assert len(in_shape[0]) in [1, 2]\n        return in_shape, in_shape\n'"
opzoo/Softmax/test_softmax.py,1,"b""import time\nimport mxnet as mx\nimport numpy as np\nimport mobula\nfrom mobula.testing import assert_almost_equal, gradcheck\n\nmobula.op.load('Softmax')\n\nT = np.float32\natol = 1e-3\n\n\ndef test_softmax1d():\n    N = 20\n    data = mx.random.uniform(0, 1, shape=(N, ))\n    out = mobula.op.Softmax(data)\n    gt = mx.nd.softmax(data)\n    exp_data = mx.nd.exp(data - data.max())\n    math_gt = exp_data / exp_data.sum()\n\n    assert_almost_equal(math_gt, gt, atol=atol)\n    assert_almost_equal(out, gt, atol=atol)\n    # gradcheck(mobula.op.Softmax, data)\n\n\ndef test_softmax2d():\n    def softmax2d(N, C):\n        data = mx.random.uniform(0, 1, shape=(N, C))\n        out = mobula.op.Softmax(data)\n        gt = mx.nd.softmax(data, axis=-1)\n        assert_almost_equal(out, gt, atol=atol)\n        # gradcheck(mobula.op.Softmax, data, eps=1e-4)\n    softmax2d(3, 10)\n    softmax2d(10, 3)\n\n\ndef test_softmax2d_grad():\n    def softmax2d_grad(N, C):\n        data = mx.random.uniform(0, 1, shape=(N, C))\n        data2 = data.copy()\n        data.attach_grad()\n        data2.attach_grad()\n\n        dy = mx.random.uniform(0, 1, shape=(N, C)) * 1000\n        with mx.autograd.record():\n            out = mobula.op.Softmax(data)\n        out.backward(dy)\n        with mx.autograd.record():\n            gt = mx.nd.softmax(data2, axis=-1)\n        gt.backward(dy)\n        assert_almost_equal(out, gt, atol=atol)\n        assert_almost_equal(data.grad, data2.grad, atol=atol)\n    softmax2d_grad(3, 10)\n    softmax2d_grad(10, 3)\n\n\nif __name__ == '__main__':\n    test_softmax1d()\n    test_softmax2d()\n    test_softmax2d_grad()\n"""
tests/gpu/test_gpu.py,3,"b""import sys\nimport os\nimport mxnet as mx\nimport numpy as np\nfrom nose.core import TestProgram\nimport unittest\nimport mobula\n\n\ndef test_gpu():\n    ctx = mx.gpu(0)\n    mx.test_utils.set_default_context(ctx)\n    assert mx.current_context() == ctx\n    path = os.path.join(os.path.dirname(__file__), '../')\n    TestProgram(defaultTest=path, argv=[path, '-s'], exit=False)\n    assert mx.current_context() == ctx\n\n\n@unittest.skipIf(len(mobula.utils.list_gpus()) < 2, 'The number of GPUs is not enough 2')\ndef test_multiple_gpus():\n    mobula.op.load('ROIAlign')\n    dtype = np.float32\n    N, C, H, W = 2, 3, 4, 4\n\n    data_cpu = mx.nd.array(\n        np.arange(N * C * H * W).astype(dtype).reshape((N, C, H, W)))\n    rois_cpu = mx.nd.array(np.array([[0, 1, 1, 3, 3]], dtype=dtype))\n\n    outs = []\n    for ctx in [mx.gpu(0), mx.gpu(1)]:\n        data = data_cpu.as_in_context(ctx)\n        rois = rois_cpu.as_in_context(ctx)\n        data.attach_grad()\n        with mx.autograd.record():\n            # mx.nd.NDArray and mx.sym.Symbol are both available as the inputs.\n            output = mobula.op.ROIAlign(data=data, rois=rois, pooled_size=(\n                2, 2), spatial_scale=1.0, sampling_ratio=1)\n\n        output.backward()\n        assert output.context == ctx, (output.context, ctx)\n        outs.append((output.asnumpy(), data.grad.asnumpy()))\n    for a, b in zip(*outs):\n        mobula.testing.assert_almost_equal(a, b)\n"""
tests/test_func/test_func.py,33,"b""import mobula\nfrom mobula.testing import assert_almost_equal\nimport numpy as np\nimport os\nimport ctypes\nmobula.op.load('./utils', os.path.dirname(__file__))\n\n\ndef test_non_c_contiguous():\n    a = np.random.random((5, 5))\n    b = np.random.random((5, 5))\n    c = np.empty((5, 5))\n    s = (slice(None), slice(2, 4))\n    a_part = a[s]\n    b_part = b[s]\n    c_part = c[s]\n    assert a_part.flags.c_contiguous == False\n    assert b_part.flags.c_contiguous == False\n    assert c_part.flags.c_contiguous == False\n    mobula.func.mul_elemwise(a_part.size, a_part, b_part, c_part)\n    assert_almost_equal(a_part * b_part, c_part)\n    assert_almost_equal(c[s], c_part)\n\n\ndef test_func_kwargs():\n    a = np.random.random((5, 5))\n    b = np.random.random((5, 5))\n    c = np.empty((5, 5))\n    mobula.func.mul_elemwise(n=a.size, a=a, b=b, c=c)\n    assert_almost_equal(a * b, c)\n\n\ndef test_default_value_op():\n    a = np.random.random((5, 5))\n    b = np.random.random((5, 5))\n    value = np.random.random((5, 5))\n    op = mobula.op.default_add_op[np.ndarray](value=value)\n    c = op(a, b)\n    assert_almost_equal(a + b, c)\n    c = op(a)  # a+b[default=value]\n    assert_almost_equal(a + value, c)\n\n\ndef test_thread():\n    n = 300\n    out_1 = np.empty(n // 1)\n    out_2 = np.empty(n // 2)\n    out_3 = np.empty(n // 3)\n    out_4 = np.empty(n * 2)\n    out_5 = np.empty(n * 3)\n    mobula.func.test_thread(n, out_1, out_2, out_3, out_4, out_5)\n    assert_almost_equal(np.arange(n // 1) * 1, out_1)\n    assert_almost_equal(np.arange(n // 2) * 2, out_2)\n    assert_almost_equal(np.arange(n // 3) * 3, out_3)\n    assert_almost_equal(np.arange(n * 2) * 2, out_4)\n    assert_almost_equal(np.arange(n * 3) * 3, out_5)\n\n\ndef test_const_template():\n    shape = (5, 5)\n    value = 3939\n    cs = [ctypes.c_int, ctypes.c_float, ctypes.c_double]\n    vs = [3, 9.9, 3.9]\n    atols = [0, 1e-6, 1e-6]\n    for ctype, value, atol in zip(cs, vs, atols):\n        c_value = ctype(value)\n        a = np.empty(shape)\n        mobula.func.test_const_template(a.size, c_value, a)\n        assert_almost_equal(np.tile(value, shape), a, atol=atol)\n\n\ndef test_infer_type_for_const():\n    ns = [np.int32, np.int64, np.float32, np.float64]\n    N = 3\n    V = 39.39\n    for dtype in ns:\n        out = np.empty(N, dtype=dtype)\n        rv = dtype(V).tolist()\n        mobula.func.infer_type_for_const(N, rv, out)\n        assert_almost_equal(out, rv)\n\n\ndef test_void_pointer():\n    pv = 3939\n    p = ctypes.c_void_p(pv)\n    out = np.zeros(1, dtype=np.int64)\n    mobula.func.test_void_pointer(1, p, out)\n    assert out == pv\n\n\ndef test_mobula_func():\n    # skip float temporarily\n    ns = [np.int32, np.int64]  # , np.float32, np.float64]\n    pv = 39.39\n    for dtype in ns:\n        a = np.array([pv], dtype=dtype)\n        b = np.empty(a.shape, dtype=dtype)\n        rtn = mobula.func.set_and_return(a, b)\n        assert_almost_equal(a, b)\n        assert_almost_equal(a, rtn)\n\n\ndef test_build():\n    mobula.func.mul_elemwise.build('cpu', ['float'])\n    mobula.func.mul_elemwise.build('cpu', dict(T='int'))\n    code_fname = os.path.join(os.path.dirname(\n        __file__), 'utils/build/cpu/utils_wrapper.cpp')\n    code = open(code_fname).read()\n    '''\n    In windows, `ctypes.c_int` is the same as `ctypes.c_long`, whose name is `c_long`. The function of `get_ctype_name` will return `int32_t` :(\n    '''\n    assert 'mul_elemwise_kernel<float>' in code, code\n    assert 'mul_elemwise_kernel<int' in code, code\n\n\ndef test_atomic_add():\n    I = U = J = 100\n    import time\n    for _ in range(10):\n        tic = time.time()\n        dtype = np.float32\n        a = np.random.uniform(size=(I, U)).astype(dtype).round(1)\n        b = np.random.uniform(size=(U, J)).astype(dtype).round(1)\n        out = np.zeros((I, J), dtype=dtype)\n        mobula.func.test_atomic_add_by_gemm(U, I, J, a, b, out)\n        target = np.dot(a, b)\n        assert_almost_equal(out, target, atol=1e-3)\n        # print('test_atomic_add', time.time() - tic)\n"""
tests/test_op/test_constant_op.py,2,"b""'''\nNotice:\n    ConstantOP doesn't support cross-device.\n    For supporting cross-device, please use ConstantOP2\n'''\nimport mobula\nimport mxnet as mx\nimport numpy as np\n\n\n@mobula.op.register(need_top_grad=False)\nclass ConstantOP:\n    def __init__(self, constant):\n        self.constant = self.F.array(constant)\n\n    def forward(self):\n        return self.constant\n\n    def backward(self, dy):\n        return []\n\n    def infer_shape(self, in_shape):\n        return [], [self.constant.shape]\n\n\n@mobula.op.register(need_top_grad=False)\nclass ConstantOP2:\n    def __init__(self, constant):\n        self.constant = self.F.array(constant)\n        self.constant_buffer = dict()\n\n    def forward(self, x):\n        ctx = x.context\n        return self.constant_buffer.get(ctx, self.constant.as_in_context(ctx))\n\n    def backward(self, dy):\n        return [0]\n\n    def infer_shape(self, in_shape):\n        return in_shape, [self.constant.shape]\n\n\ndef test_constant_op():\n    # ConstantOP only supports mx.cpu()\n    if mx.context.current_context() == mx.cpu():\n        # NDArray\n        a = mx.nd.array([1, 2, 3])\n        b = mx.nd.array([4, 5, 6])\n        c = a + ConstantOP[mx.nd.NDArray](b)\n        assert (c.asnumpy() == [5, 7, 9]).all()\n\n        # Symbol\n        a_sym = mx.sym.Variable('a')\n        output_sym = a_sym + ConstantOP[mx.sym.Symbol](b)\n        exe = output_sym.simple_bind(\n            ctx=mx.context.current_context(), a=a.shape)\n        exe.forward(a=np.array([1, 2, 3]))\n\n        assert (exe.outputs[0].asnumpy() == [5, 7, 9]).all()\n\n    '''\n    ConstantOP2: accept a variable for getting the context information\n    '''\n\n    # NDArray\n    a = mx.nd.array([1, 2, 3])\n    b = mx.nd.array([4, 5, 6])\n    c = a + ConstantOP2(a, constant=b)\n    assert (c.asnumpy() == [5, 7, 9]).all()\n\n    # Symbol\n    a_sym = mx.sym.Variable('a')\n    # declare input_type explicitly because the inputs includes mx.sym.Symbol and mx.nd.NDArray\n    output_sym = a_sym + ConstantOP2[mx.sym.Symbol](a_sym, constant=b)\n    exe = output_sym.simple_bind(ctx=mx.context.current_context(), a=a.shape)\n    exe.forward(a=np.array([1, 2, 3]))\n\n    assert (exe.outputs[0].asnumpy() == [5, 7, 9]).all()\n"""
tests/test_op/test_roi_align_op.py,18,"b""import mxnet as mx\nimport numpy as np\nimport mobula\nfrom mobula.testing import assert_almost_equal\n\nmobula.op.load('ROIAlign')\n\nT = np.float32\n\n\ndef bilinear_interpolate(bottom, height, width, y, x):\n    if y < -1.0 or y > height or x < -1.0 or x > width:\n        return T(0.0), []\n    x = T(max(0.0, x))\n    y = T(max(0.0, y))\n    x_low = int(x)\n    y_low = int(y)\n    if x_low >= width - 1:\n        x_low = x_high = width - 1\n        x = T(x_low)\n    else:\n        x_high = x_low + 1\n\n    if y_low >= height - 1:\n        y_low = y_high = height - 1\n        y = T(y_low)\n    else:\n        y_high = y_low + 1\n\n    ly = y - T(y_low)\n    lx = x - T(x_low)\n    hy = T(1.0) - ly\n    hx = T(1.0) - lx\n\n    v1 = bottom[y_low, x_low]\n    v2 = bottom[y_low, x_high]\n    v3 = bottom[y_high, x_low]\n    v4 = bottom[y_high, x_high]\n\n    '''\n    ----------->x\n    |hx hy | lx hy\n    |------+------\n    |hx ly | lx ly\n    V\n    y\n\n    v1|v2\n    --+--\n    v3|v4\n    '''\n    w1 = hy * hx\n    w2 = hy * lx\n    w3 = ly * hx\n    w4 = ly * lx\n\n    assert w1.dtype == T\n    assert w2.dtype == T\n    assert w3.dtype == T\n    assert w4.dtype == T\n\n    val = w1 * v1 + w2 * v2 + w3 * v3 + w4 * v4\n    assert val.dtype == T\n    grad = [(y_low, x_low, w1), (y_low, x_high, w2),\n            (y_high, x_low, w3), (y_high, x_high, w4)\n            ]\n    return val, grad\n\n\ndef bilinear_interpolate_gradient(height, width, y, x):\n    '''\n    return:\n        w1, w2, w3, w4\n        x_low, x_high, y_low, y_high\n    '''\n    if y < -1.0 or y > height or x < -1.0 or x > width:\n        return T(0), T(0), T(0), T(0), -1, -1, -1, -1\n    x = T(max(0.0, x))\n    y = T(max(0.0, y))\n    x_low = int(x)\n    y_low = int(y)\n    if x_low >= width - 1:\n        x_low = x_high = width - 1\n        x = T(x_low)\n    else:\n        x_high = x_low + 1\n\n    if y_low >= height - 1:\n        y_low = y_high = height - 1\n        y = T(y_low)\n    else:\n        y_high = y_low + 1\n\n    ly = y - T(y_low)\n    lx = x - T(x_low)\n    hy = T(1.0) - ly\n    hx = T(1.0) - lx\n\n    w1 = hy * hx\n    w2 = hy * lx\n    w3 = ly * hx\n    w4 = ly * lx\n    return w1, w2, w3, w4, x_low, x_high, y_low, y_high\n\n\ndef roialign_forward_backward(data, rois, pooled_size, spatial_scale, sampling_ratio, dy):\n    N, C, H, W = data.shape\n    R = rois.shape[0]\n    PH, PW = pooled_size\n    assert len(rois.shape) == 2\n    assert rois.shape[1] == 5\n    assert data.dtype == T\n    assert rois.dtype == T\n\n    out = np.zeros((R, C, PH, PW), dtype=T)\n    dx = np.zeros_like(data)\n    drois = np.zeros_like(rois)\n\n    for r in range(R):\n        batch_ind = int(rois[r, 0])\n        sw, sh, ew, eh = rois[r, 1:5] * T(spatial_scale)\n        roi_w = T(max(ew - sw, 1.0))\n        roi_h = T(max(eh - sh, 1.0))\n        bin_h = roi_h / T(PH)\n        bin_w = roi_w / T(PW)\n        bdata = data[batch_ind]\n        if sampling_ratio > 0:\n            roi_bin_grid_h = roi_bin_grid_w = sampling_ratio\n        else:\n            roi_bin_grid_h = int(np.ceil(roi_h / T(PH)))\n            roi_bin_grid_w = int(np.ceil(roi_w / T(PW)))\n        count = T(roi_bin_grid_h * roi_bin_grid_w)\n        for c in range(C):\n            for ph in range(PH):\n                for pw in range(PW):\n                    val = T(0.0)\n                    for iy in range(roi_bin_grid_h):\n                        y = sh + T(ph) * bin_h + (T(iy) + T(0.5)) * \\\n                            bin_h / T(roi_bin_grid_h)\n                        for ix in range(roi_bin_grid_w):\n                            x = sw + T(pw) * bin_w + (T(ix) + T(0.5)) * \\\n                                bin_w / T(roi_bin_grid_w)\n                            v, g = bilinear_interpolate(bdata[c], H, W, y, x)\n                            assert v.dtype == T\n                            val += v\n                            # compute grad\n                            for qy, qx, qw in g:\n                                assert qw.dtype == T\n                                dx[batch_ind, c, qy, qx] += dy[r,\n                                                               c, ph, pw] * qw / count\n\n                    out[r, c, ph, pw] = val / count\n    assert out.dtype == T, out.dtype\n    return out, [dx, drois]\n\n\ndef roialign_backward(bottom_diff, rois, pooled_size, spatial_scale, sampling_ratio, top_diff):\n    N, C, H, W = bottom_diff.shape\n    R = rois.shape[0]\n    PH, PW = pooled_size\n    assert len(rois.shape) == 2\n    assert rois.shape[1] == 5\n    assert rois.dtype == T\n\n    for r in range(R):\n        batch_ind = int(rois[r, 0])\n        sw, sh, ew, eh = rois[r, 1:5] * T(spatial_scale)\n        roi_w = max(ew - sw, T(1.0))\n        roi_h = max(eh - sh, T(1.0))\n        bin_h = roi_h / T(PH)\n        bin_w = roi_w / T(PW)\n        if sampling_ratio > 0:\n            roi_bin_grid_h = roi_bin_grid_w = int(sampling_ratio)\n        else:\n            roi_bin_grid_h = int(np.ceil(roi_h / T(PH)))\n            roi_bin_grid_w = int(np.ceil(roi_w / T(PW)))\n        count = T(roi_bin_grid_h * roi_bin_grid_w)\n        for c in range(C):\n            for ph in range(PH):\n                for pw in range(PW):\n                    for iy in range(roi_bin_grid_h):\n                        y = sh + T(ph) * bin_h + (T(iy) + T(0.5)) * \\\n                            bin_h / T(roi_bin_grid_h)\n                        for ix in range(roi_bin_grid_w):\n                            x = sw + T(pw) * bin_w + (T(ix) + T(0.5)) * \\\n                                bin_w / T(roi_bin_grid_w)\n                            w1, w2, w3, w4, x_low, x_high, y_low, y_high = bilinear_interpolate_gradient(\n                                H, W, y, x)\n                            dtop = top_diff[r, c, ph, pw]\n                            if x_low >= 0 and x_high >= 0 and y_low >= 0 and y_high >= 0:\n                                bottom_diff[batch_ind, c, y_low,\n                                            x_low] += dtop * w1 / count\n                                bottom_diff[batch_ind, c, y_low,\n                                            x_high] += dtop * w2 / count\n                                bottom_diff[batch_ind, c, y_high,\n                                            x_low] += dtop * w3 / count\n                                bottom_diff[batch_ind, c, y_high,\n                                            x_high] += dtop * w4 / count\n\n\ndef test_roi_align_sym():\n    dtype = np.float32\n\n    N, C, H, W = 2, 3, 4, 4\n\n    data = np.arange(N * C * H * W).astype(dtype).reshape((N, C, H, W))\n    rois = np.array([[0, 1, 1, 3, 3], [1, 2, 2, 3, 3]], dtype=dtype)\n\n    data_sym = mx.sym.Variable('data')\n    rois_sym = mx.sym.Variable('rois')\n\n    output_sym = mobula.op.ROIAlign(data=data_sym, rois=rois_sym, pooled_size=(\n        2, 2), spatial_scale=1.0, sampling_ratio=1)\n    output_sym = mx.sym.MakeLoss(output_sym)\n\n    exe = output_sym.simple_bind(\n        ctx=mx.context.current_context(), data=data.shape, rois=rois.shape)\n    exe.forward(data=data, rois=rois)\n\n    res = exe.outputs[0].asnumpy()\n\n    exe.backward()\n    mx.nd.waitall()\n\n\ndef test_roi_align_nd():\n    dtype = np.float32\n\n    N, C, H, W = 2, 3, 4, 4\n\n    data = mx.nd.array(\n        np.arange(N * C * H * W).astype(dtype).reshape((N, C, H, W)))\n    rois = mx.nd.array(np.array([[0, 1, 1, 3, 3]], dtype=dtype))\n\n    data.attach_grad()\n    with mx.autograd.record():\n        output = mobula.op.ROIAlign(data=data, rois=rois, pooled_size=(\n            2, 2), spatial_scale=1.0, sampling_ratio=1)\n    output.backward()\n    mx.nd.waitall()\n\n\ndef test_roi_align_value():\n    dtype = np.float32\n\n    dlen = 224\n    N, C, H, W = 5, 3, 16, 16\n    assert H == W\n    R = 7\n    pooled_size = (3, 4)\n\n    spatial_scale = H * 1.0 / dlen\n    sampling_ratio = 0\n    data = mx.nd.array(\n        np.arange(N * C * W * H).reshape((N, C, H, W)), dtype=dtype)\n    # data = mx.nd.random.uniform(0, 1, (N, C, H, W), dtype = dtype)\n    center_xy = mx.nd.random.uniform(0, dlen, (R, 2), dtype=dtype)\n    wh = mx.nd.random.uniform(0, dlen, (R, 2), dtype=dtype)\n    batch_ind = mx.nd.array(np.random.randint(0, N, size=(R, 1)))\n    pos = mx.nd.concat(center_xy - wh / 2, center_xy + wh / 2, dim=1)\n    rois = mx.nd.concat(batch_ind, pos, dim=1)\n\n    data.attach_grad()\n    rois.attach_grad()\n    with mx.autograd.record():\n        output = mobula.op.ROIAlign(data=data, rois=rois, pooled_size=pooled_size,\n                                    spatial_scale=spatial_scale, sampling_ratio=sampling_ratio)\n    dy = mx.nd.random.uniform(-1, 1, (R, C) + pooled_size, dtype=dtype)\n    output.backward(dy)\n    real_output, [dx, drois] = roialign_forward_backward(data.asnumpy(\n    ), rois.asnumpy(), pooled_size, spatial_scale, sampling_ratio, dy.asnumpy())\n\n    bottom_diff = np.zeros(data.shape, dtype=T)\n    roialign_backward(bottom_diff, rois.asnumpy(), pooled_size,\n                      spatial_scale, sampling_ratio, dy.asnumpy())\n    assert_almost_equal(dx, bottom_diff)\n\n    atol = 1e-3\n    rtol = 1e-3\n    assert_almost_equal(output.asnumpy(), real_output, atol=atol, rtol=rtol)\n    assert_almost_equal(data.grad.asnumpy(), dx, atol=atol, rtol=rtol)\n    assert_almost_equal(rois.grad.asnumpy(), drois, atol=atol, rtol=rtol)\n\n\nif __name__ == '__main__':\n    test_roi_align_value()\n    test_roi_align_sym()\n    test_roi_align_nd()\n"""
docs/tutorial/MulElemWise/MulElemWise.py,2,"b""import mobula\nimport numpy as np\n\n\n@mobula.op.register\nclass MulElemWise:\n    def forward(self, a, b):\n        '''\n            np.array([1,2,3]).size = 3\n            mx.nd.array([1,2,3]).size = 3\n            torch.tensor([1,2,3]).numel() = 3\n        '''\n        size = a.numel() if hasattr(a, 'numel') else a.size\n        mobula.func.mul_elemwise(size, a, b, self.y)\n\n    def backward(self, dy):\n        if hasattr(self.F, 'multiply'):\n            # np.multiply, mx.nd.multiply\n            self.dX[0][:] = self.F.multiply(dy, self.X[1])\n        else:\n            # torch.mul\n            self.dX[0][:] = self.F.mul(dy, self.X[1])\n        size = dy.numel() if hasattr(dy, 'numel') else dy.size\n        mobula.func.mul_elemwise(size, dy, self.X[0], self.dX[1])\n\n    def infer_shape(self, in_shape):\n        assert in_shape[0] == in_shape[1]\n        return in_shape, [in_shape[0]]\n"""
examples/dynamic_import_op/AdditionOP/AdditionOP.py,0,"b""import sys\nsys.path.append('../../../')  # Add MobulaOP Path\nimport mobula\n\n\n@mobula.op.register\nclass AdditionOP:\n    def forward(self, a, b):\n        c = self.y\n        mobula.func.addition_op_forward(a.size, a, b, c)\n\n    def backward(self, dc):\n        return [dc, dc]\n\n    def infer_shape(self, in_shape):\n        assert list(in_shape[0]) == list(in_shape[1])\n        return in_shape, [in_shape[0]]\n"""
tests/test_func/utils/utils.py,0,"b'import mobula\n\n\n@mobula.op.register\nclass mul_elemwise:\n    def forward(self, a, b):\n        mobula.func.mul_elemwise(a.size, a, b, self.y)\n\n    def backward(self, dy):\n        mobula.func.mul_elemwise(dy.size, dy, b, self.dX[0])\n        mobula.func.mul_elemwise(dy.size, dy, a, self.dX[1])\n\n    def infer_shape(self, in_shape):\n        assert in_shape[0] == in_shape[1]\n        return in_shape, [in_shape[0]]\n\n\n@mobula.op.register\nclass default_add_op:\n    def __init__(self, value):\n        self.value = value\n\n    def forward(self, a, b=None):\n        if b is None:\n            b = self.value\n        return a + b\n\n    def backward(self, dy):\n        return [dy, dy]\n\n    def infer_shape(self, in_shape):\n        return in_shape, [in_shape[0]]\n'"
tests/test_op/test_dynamic_import_op/test_addition.py,0,"b""import mobula\nfrom mobula.testing import assert_almost_equal\n# Import Custom Operator Dynamically\nimport os\nmobula.op.load('./AdditionOP', path=os.path.dirname(__file__))\nAdditionOP = mobula.op.AdditionOP\n\nimport mxnet as mx\n\n\ndef test_addition():\n    a = mx.nd.array([1, 2, 3])\n    b = mx.nd.array([4, 5, 6])\n\n    a.attach_grad()\n    b.attach_grad()\n\n    with mx.autograd.record():\n        c = AdditionOP(a, b)\n\n    dc = mx.nd.array([7, 8, 9])\n    c.backward(dc)\n\n    assert_almost_equal(a + b, c)\n    assert_almost_equal(a.grad, dc)\n    assert_almost_equal(b.grad, dc)\n"""
tests/test_op/test_dynamic_import_op/test_template.py,2,"b""import mobula\nfrom mobula.testing import assert_almost_equal\n# Import Custom Operator Dynamically\nimport os\nmobula.op.load('./TemplateOP', path=os.path.dirname(__file__))\n\nimport mxnet as mx\nimport numpy as np\n\n\ndef test_template_1type():\n    shape = (2, 3, 4)\n    for dtype in [np.int32, np.int64, np.float32, np.float64]:\n        a = mx.nd.random.uniform(0, 100, shape=shape).astype(dtype)\n        b = mx.nd.random.uniform(0, 100, shape=shape).astype(dtype)\n        c = mx.nd.empty(shape, dtype=dtype)\n        mobula.func.maximum(a.size, a, b, c)\n        assert_almost_equal(mx.nd.maximum(a, b).asnumpy(), c.asnumpy())\n\n\ndef test_template_3type():\n    shape = (2, 3, 4)\n    t1, t2, t3 = np.int32, np.float32, np.float64\n    a = mx.nd.random.uniform(0, 100, shape=shape).astype(t1)\n    b = mx.nd.random.uniform(0, 100, shape=shape).astype(t2)\n    c = mx.nd.empty(shape, dtype=t3)\n    mobula.func.maximum_3type(a.size, a, b, c)\n    assert_almost_equal(mx.nd.maximum(\n        a.astype(t2), b).asnumpy().astype(t3), c.asnumpy())\n"""
tests/test_op/test_dynamic_import_op/AdditionOP/AdditionOP.py,0,"b'import mobula\n\n\n@mobula.op.register\nclass AdditionOP:\n    def forward(self, a, b):\n        c = self.y\n        mobula.func.addition_op_forward(a.size, a, b, c)\n\n    def backward(self, dc):\n        return [dc, dc]\n\n    def infer_shape(self, in_shape):\n        assert list(in_shape[0]) == list(in_shape[1])\n        return in_shape, [in_shape[0]]\n'"
