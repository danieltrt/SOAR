file_path,api_count,code
book/Python_for_Data_Analysis/ch03/ipython_bug.py,0,b'def works_fine():\n    a = 5\n    b = 6\n    assert(a + b == 11)\n\ndef throws_an_exception():\n    a = 5\n    b = 6\n    assert(a + b == 10)\n\ndef calling_things():\n    works_fine()\n    throws_an_exception()\n\ncalling_things()\n'
book/Python_for_Data_Analysis/ch07/analysis.py,1,"b'from pandas import *\nfrom pandas.util.decorators import cache_readonly\nimport numpy as np\nimport os\n\nbase = \'ml-100k\'\n\nclass IndexedFrame(object):\n    """"""\n\n    """"""\n\n    def __init__(self, frame, field):\n        self.frame = frame\n\n    def _build_index(self):\n        pass\n\nclass Movielens(object):\n\n    def __init__(self, base=\'ml-100k\'):\n        self.base = base\n\n    @cache_readonly\n    def data(self):\n        names = [\'user_id\', \'item_id\', \'rating\', \'timestamp\']\n        path = os.path.join(self.base, \'u.data\')\n        return read_table(path, header=None, names=names)\n\n    @cache_readonly\n    def users(self):\n        names = [\'user_id\', \'age\', \'gender\', \'occupation\', \'zip\']\n        path = os.path.join(self.base, \'u.user\')\n        return read_table(path, sep=\'|\', header=None, names=names)\n\n    @cache_readonly\n    def items(self):\n        names = [\'item_id\', \'title\', \'release_date\', \'video_date\',\n                 \'url\', \'unknown\', \'Action\', \'Adventure\', \'Animation\',\n                 ""Children\'s"", \'Comedy\', \'Crime\', \'Documentary\', \'Drama\',\n                 \'Fantasy\', \'Film-Noir\', \'Horror\', \'Musical\', \'Mystery\',\n                 \'Romance\', \'Sci-Fi\', \'Thriller\', \'War\', \'Western\']\n        path = os.path.join(self.base, \'u.item\')\n        return read_table(path, sep=\'|\', header=None, names=names)\n\n    @cache_readonly\n    def genres(self):\n        names = [\'name\', \'id\']\n        path = os.path.join(self.base, \'u.genre\')\n        data = read_table(path, sep=\'|\', header=None, names=names)[:-1]\n        return Series(data.name, data.id)\n\n    @cache_readonly\n    def joined(self):\n        merged = merge(self.data, self.users)\n        merged = merge(merged, self.items)\n        return merged\n\n    def movie_stats(self, title):\n        data = self.joined[self.joined.title == title]\n\n        return data.groupby(\'gender\').rating.mean()\n\ndef biggest_gender_discrep(data):\n    nobs = data.pivot_table(\'rating\', rows=\'title\',\n                            cols=\'gender\', aggfunc=len, fill_value=0)\n    mask = (nobs.values > 10).all(1)\n    titles = nobs.index[mask]\n\n    mean_ratings = data.pivot_table(\'rating\', rows=\'title\',\n                                    cols=\'gender\', aggfunc=\'mean\')\n    mean_ratings = mean_ratings.ix[titles]\n\n    diff = mean_ratings.M - mean_ratings.F\n    return diff[np.abs(diff).argsort()[::-1]]\n\nbuckets = [0, 18, 25, 35, 50, 80]\n\nml = Movielens()\ntitle = \'Cable Guy, The (1996)\'\n'"
book/collective_intelligence/ch02/recommendations.py,0,"b""#! /usr/bin/env python2\n# -*- coding: utf-8 -*-\n\nfrom math import sqrt\n\ncritics={'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n                       'Just My Luck': 3.0, 'Superman Returns': 3.5, \n                       'You, Me and Dupree': 2.5, 'The Night Listener': 3.0},\n         'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5,\n                          'Just My Luck': 1.5, 'Superman Returns': 5.0, \n                          'The Night Listener': 3.0, 'You, Me and Dupree': 3.5},\n         'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n                              'Superman Returns': 3.5, 'The Night Listener': 4.0},\n         'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n                          'The Night Listener': 4.5, 'Superman Returns': 4.0,\n                          'You, Me and Dupree': 2.5},\n         'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n                          'Just My Luck': 2.0, 'Superman Returns': 3.0, \n                          'The Night Listener': 3.0, 'You, Me and Dupree': 2.0},\n         'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n                           'The Night Listener': 3.0, 'Superman Returns': 5.0, \n                           'You, Me and Dupree': 3.5},\n         'Toby': {'Snakes on a Plane':4.5, 'You, Me and Dupree':1.0,\n                  'Superman Returns':4.0}}\n\n# Return a distance-based similarity score for person1 and person2\ndef sim_distance(prefs, person1, person2):\n    # Get the list of shared items\n    si = {}\n    for item in prefs[person1]:\n        for item in prefs[person2]:\n            si[item] = 1\n    # if they have no rating in common, return 0\n    if len(si) == 0:\n        return 0\n    \n    # Add up the squares of all the differences\n    sum_of_squares = sum([pow(prefs[person1][item] - prefs[person2][item], 2)\n                          for item in prefs[person1] if item in prefs[person2]])\n    \n    return 1/(1 + sqrt(sum_of_squares))\n\n# Return the pearson correlation score for person1 and person2\ndef sim_pearson(prefs, person1, person2):\n    # Get the list of shared items\n    si = {}\n    for item in prefs[person1]:\n        if item in prefs[person2]:\n            si[item] = 1\n    \n    # print si\n    \n    # if they have no rating in common, return 0\n    n = len(si)\n    if n == 0:\n        return 0\n    \n    # Add up the squares of all the differences\n    sum1 = sum([prefs[person1][item] for item in si.keys()])\n    sum2 = sum([prefs[person2][item] for item in si.keys()])\n    \n    # Sum up the squares\n    sum1sq = sum([pow(prefs[person1][item], 2) for item in si.keys()])\n    sum2sq = sum([pow(prefs[person2][item], 2) for item in si.keys()])\n    \n    # Sum up the products\n    psum = sum([prefs[person1][item] * prefs[person2][item] for item in si.keys()])\n    \n    # Calculate pearson score\n    num = psum - (sum1*sum2/n)\n    den = sqrt((sum1sq - pow(sum1, 2)/n) * (sum2sq - pow(sum2, 2)/n))\n    \n    if den == 0:\n        return 0\n    \n    r = num/den\n    return r\n\n# Returns the best matches for people from the prefs dictionay\n# Number of results and similarity function are optional paras.\ndef top_match(prefs, person, n = 5, similarity = sim_pearson):\n    score = [(similarity(prefs, person, other), other) for other in prefs if other != person]\n    \n    score.sort()\n    score.reverse()\n    \n    return score[0: n]\n\ndef get_recommendation(prefs, person, similarity = sim_pearson):\n    total = {}\n    sim_sum = {}\n    for other in prefs:\n        if other == person:\n            continue\n        sim = similarity(prefs, person, other)\n        \n        if sim <= 0:\n            continue\n    \n        for item in prefs[other]:\n            if item not in prefs[person] or prefs[person][item] == 0:\n                total.setdefault(item, 0)\n                total[item] += prefs[other][item] * sim\n                sim_sum.setdefault(item, 0)\n                sim_sum[item] += sim\n                \n    ranking = [(total / sim_sum[item], item) for item, total in total.items()]\n\n    ranking.sort()\n    ranking.reverse()\n\n    return ranking\n\ndef transformPrefs(prefs):\n    result = {}\n    for person in prefs:\n        for item in prefs[person]:\n            result.setdefault(item, {})\n\n            result[item][person] = prefs[person][item]\n    return result\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
book/collective_intelligence/ch03/clusters.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport random\nfrom math import sqrt\nfrom PIL import Image, ImageDraw\n\ndef readfile(filename):\n    lines = [line for line in file(filename)]\n    \n    # first line the column title\n    colnames = lines[0].strip().split('\\t')[1: ]\n    rownames = []\n    data = []\n    \n    for line in lines[1: ]:\n        p = line.strip().split('\\t')\n        # First column in each row is the row name\n        rownames.append(p[0])\n        data.append([float(x) for x in p[1: ]])\n    return rownames, colnames, data\n\ndef pearson(v1, v2):\n    # simple sum\n    sum1 = sum(v1)\n    sum2 = sum(v2)\n    \n    # sum of the squares\n    sum1sq = sum([pow(v, 2) for v in v1])\n    sum2sq = sum([pow(v, 2) for v in v2])\n    \n    # sum of the product\n    psum = sum([v1[i]*v2[i] for i in range(len(v1))])\n    \n    # calculate r\n    num = psum - (sum1*sum2/len(v1))\n    den = sqrt((sum1sq - pow(sum1, 2)/len(v1)) * (sum2sq - pow(sum2, 2)/len(v2)))\n    \n    if den == 0:\n        return 0\n    return 1.0 - num/den\n\nclass bicluster:\n    def __init__(self, vec, left = None, right = None, distance = 0, id = None):\n        self.left = left\n        self.right = right\n        self.vec = vec\n        self.id = id\n        self.distance = distance\n\n\ndef hcluster(rows, distance = pearson):\n    distances = {}\n    currentclustid = -1\n    \n    # clusters are initially just the rows\n    clust = [bicluster(rows[i], id = i) for i in range(len(rows))]\n    \n    while len(clust) > 1:\n        lowestpair = (0, 1)\n        closest = distance(clust[0].vec, clust[1].vec)\n        \n        ## Loop through every pair looking for the smallest distance\n        for i in range(len(clust)):\n            for j in range(i + 1, len(clust)):\n                # distance is the cache of distance calculation\n                if (clust[i].id, clust[j].id) not in distances:\n                    distances[(clust[i].id, clust[j].id)] = distance(clust[i].vec, clust[j].vec)\n                \n                d = distances[(clust[i].id, clust[j].id)]\n                \n                if d < closest:\n                    closest = d\n                    lowestpair = (i, j)\n        # calculate the average of two clusters\n        mergevec = [\n            (clust[lowestpair[0]].vec[i] + clust[lowestpair[1]].vec[i])/2.0 for i in range(len(clust[0].vec))\n        ]\n        \n        # create the new cluster\n        newcluster  = bicluster(mergevec, left = clust[lowestpair[0]], right= clust[lowestpair[1]], distance = closest, id = currentclustid)\n        \n        # cluster ids that weren't in the original set are nagtive\n        currentclustid -= 1\n        del clust[lowestpair[1]]\n        del clust[lowestpair[0]]\n        clust.append(newcluster)\n\n    return clust[0]\n\ndef printclust(clust, labels = None, n = 0):\n    # ident to make a hierarchy layout\n    for i in range(n): \n        print ' ',\n    if clust.id < 0:\n        # negative id means that this is branch\n        print '-'\n    else:\n        #  positive id means this is an endpoint\n        if labels is None:\n            print clust.id\n        else:\n            print labels[clust.id]\n    # Now print the left and right branches\n    if clust.left != None:\n        print printclust(clust.left, labels = labels, n = n + 1)\n    if clust.right != None:\n        print printclust(clust.right, labels = labels, n = n + 1)\n\ndef getheight(clust):\n    ## Is this an endpoint? The the height is just -1\n    if clust.left == None and clust.right == None:\n        return 1\n    \n    # otherwise the height is the same height of each branch\n    return getheight(clust.left) + getheight(clust.right)\n\ndef getdepth(clust):\n    # The distance of the endpoint is 0\n    if clust.left == None and clust.right == None:\n        return 0\n    \n    # The distance of the branch is the greater if its two sides\n    # Plus its own distance\n    return max(getdepth(clust.left), getdepth(clust.right)) + clust.distance\n\ndef drawdendrogram(clust, labels, jpeg = 'clusters.jpg'):\n    # height and width\n    h = getheight(clust) * 20\n    w = 120\n    depth = getdepth(clust)\n    \n    # width is fixed, so scale distance accordingly\n    scaling = float(w - 150)/depth\n    \n    # Create a new image with a white backgroud\n    img = Image.new('RGB', (w, h), (255, 255, 255))\n    draw = ImageDraw.Draw(img)\n    \n    draw.line((0, h/2, 10, h/2), fill = (255, 0, 0))\n    \n    # draw the first node\n    drawnode(draw, clust, 10, (h/2), scaling, labels)\n    img.save(jpeg, 'JPEG')\n\ndef drawnode(draw,clust,x,y,scaling,labels):\n  if clust.id<0:\n    h1=getheight(clust.left)*20\n    h2=getheight(clust.right)*20\n    top=y-(h1+h2)/2\n    bottom=y+(h1+h2)/2\n    # Line length\n    ll=clust.distance*scaling\n    # Vertical line from this cluster to children    \n    draw.line((x,top+h1/2,x,bottom-h2/2),fill=(255,0,0))    \n    \n    # Horizontal line to left item\n    draw.line((x,top+h1/2,x+ll,top+h1/2),fill=(255,0,0))    \n\n    # Horizontal line to right item\n    draw.line((x,bottom-h2/2,x+ll,bottom-h2/2),fill=(255,0,0))        \n\n    # Call the function to draw the left and right nodes    \n    drawnode(draw,clust.left,x+ll,top+h1/2,scaling,labels)\n    drawnode(draw,clust.right,x+ll,bottom-h2/2,scaling,labels)\n  else:   \n    # If this is an endpoint, draw the item label\n    draw.text((x+5,y-7),labels[clust.id],(0,0,0))\n\ndef rotatematrix(data):\n    newdata = []\n    for i in range(len(data[0])):\n        newrow = [data[j][i] for j in range(len(data))]\n        newdata.append(newrow)\n    return newdata\n\ndef kcluster(rows, distance = pearson, k = 4):\n    # Determine the minimum and maximum values for each point\n    ranges=[(min([row[i] for row in rows]), max([row[i] for row in rows])) \n    for i in range(len(rows[0]))] \n\n    # Create k randomly placed centroids\n    clusters=[[random.random() * (ranges[i][1] - ranges[i][0]) + ranges[i][0] \n    for i in range(len(rows[0]))] for j in range(k)]\n    \n    lastmatches = None\n    for t in range(100):\n        print 'Iteration %d' % t\n        bestmatches=[[] for i in range(k)]\n      \n        # Find which centroid is the closest for each row\n        for j in range(len(rows)):\n            row = rows[j]\n            bestmatch = 0\n            for i in range(k):\n                d = distance(clusters[i],row)\n                if d < distance(clusters[bestmatch],row): bestmatch=i\n            bestmatches[bestmatch].append(j)    \n\n        # If the results are the same as last time, this is complete\n        if bestmatches == lastmatches: break\n        lastmatches = bestmatches\n      \n        # Move the centroids to the average of their members\n        for i in range(k):\n            avgs=[0.0]*len(rows[0])\n            if len(bestmatches[i])>0:\n                for rowid in bestmatches[i]:\n                  for m in range(len(rows[rowid])):\n                      avgs[m]+=rows[rowid][m]\n                for j in range(len(avgs)):\n                    avgs[j]/=len(bestmatches[i])\n            clusters[i]=avgs\n        \n    return bestmatches\n"""
book/collective_intelligence/ch03/generatefeedvector.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport feedparser\nimport re\n\ndef getwordcount(url):\n    # parse the feed\n    d = feedparser.parse(url)\n    wc = {}\n    \n    # loop over all entries\n    for e in d.entries:\n        if 'summary' in e:\n            summary = e.summary\n        else:\n            summary = e.description\n            \n        # Extract a list of words\n        words = getwords(e.title + ' ' + summary)\n        for word in words:\n            wc.setdefault(word, 0)\n            wc[word] += 1\n    return getattr(d.feed, 'title', 'Unknown title'), wc\n\ndef getwords(html):\n    # remove all the html tags\n    txt = re.compile(r'<[^>]+>').sub('', html)\n    # split words by all non-alpha characters\n    words = re.compile(r'[^A-Z^a-z]+').split(txt)\n    # convert to lowcase\n    return [word.lower() for word in words if word != '']\n\n"""
book/collective_intelligence/ch03/get_data.py,0,"b""#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom generatefeedvector import getwordcount\n\napcount = {}\nwordcount = {}\nfeedlist = [line for line in file('feedlist.txt')]\nfor feedurl in feedlist:\n    try:\n        print 'feedurl: ', feedurl\n        title, wc = getwordcount(feedurl)\n        wordcount[title] = wc\n        for word, count in wc.items():\n            apcount.setdefault(word, 0)\n            if count > 0:\n                apcount[word] += 1\n    except:\n        print 'Failed to parse the feedurl.'\n\nwordlist = []\nfor w, bc in apcount.items():\n    frac = float(bc)/len(feedlist)\n    if frac > 0.1 and frac < 0.5:\n        wordlist.append(w)\n\nout = file('blogdata.txt', 'w')\nout.write('Blog')\nfor word in wordlist:\n    out.write('\\t%s' %word)\nout.write('\\n')\n\nfor blog, wc in wordcount.items():\n    out.write(blog)\n    for word in wordlist:\n        if word in wc:\n            out.write('\\t%d' %wc[word])\n        else:\n            out.write('\\t0')\n    out.write('\\n')\n"""
book/collective_intelligence/ch04/nn.py,0,"b'#! /usr/bin/env python\nfrom math import tanh\nfrom pysqlite2 import dbapi2 as sqlite\n\nclass searchnet():\n    def __init__(self, dbname):\n        self.con = sqlite.connect(dbname)\n\n    def __del__(self):\n        self.con.close()\n\n    def __maketables(self):\n        self.con.execute(""create table if not exists hiddennode(create_key)"")\n        self.con.execute(""create table if not exists wordhidden(fromid, toid, strength)"")\n        self.con.execute(""create table if not exists hiddenurl(fromid, toid, strength)"")\n        self.con.commit()\n\n\n    def getstrength(self, fromid, toid, layer):\n        if layer == 0:\n            table = \'wordhidden\'\n        else:\n            table = \'hiddenurl\'\n\n        res = self.con.execute(""\\\n                select strength from %s where fromid = %d and toid = %d"" %(table, fromid, toid)).fetchone()\n\n        if res == None:\n            if layer == 0:\n                return -2\n            if layer == 1:\n                return 0\n\n        return res[0]\n\n    def setstrength(self, fromid, toid, layer, strength):\n        if layer == 0:\n            table = \'wordhidden\'\n        else:\n            table = \'hiddenurl\'\n\n        res = self.con.execute(""\\\n                select rowid from %s  where fromid = %d and toid = %d"" %(table, fromid, toid)).fetchone()\n\n        if res == None:\n            self.con.execute(""\\\n                    insert into %s (fromid, toid, strength) values (%d, %d, %f)"" %(table, fromid, toid, strength))\n        else:\n            rowid = res[0]\n            self.con.execute(""\\\n                    update %s set strength = %f where rowid = %d"" %(table, strength, rowid))\n\n    def generatehiddennode(self, wordids, urls):\n        if len(wordids) > 3:\n            return None\n        # check if we already created a node for this set of words\n        createkey = \'_\'.join(sorted([str(wi) for wi in wordids]))\n        res = self.con.execute(""\\\n                select rowid from hiddennode where create_key = \'%s\'"" % createkey).fetchone()\n\n        # if not, create it\n        if res == None:\n            cur = self.con.execute(""\\\n                    insert into hiddennode (create_key) values (\'%s\')"" % createkey)\n            hidderid = cur.lastrowid\n\n            # puts in some default weights\n            for wordid in wordids:\n                self.setstrength(wordid, hidderid, 0, 1.0/len(wordids))\n            for urlid in urls:\n                self.setstrength(hidderid, urlid, 1, 0.1)\n            self.con.commit()\n\n    def getallhiddenids(self, wordids, urlids):\n        l1 = {}\n        for wordid in wordids:\n            cur = self.con.execute(""\\\n                    select toid from wordhidden where fromid = %d"" % wordid)\n            for row in cur: \n                l1[row[0]] = 1\n        for urlid in urlids:\n            cur = self.con.execute(""\\\n                    select fromid from hiddenurl where toid = %d"" % urlid)\n            for row in cur:\n                l1[row[0]] = 1\n        return l1.keys()\n\n\n    def setupnetwork(self, wordids, urlids):\n        # value list\n        self.wordids = wordids\n        self.hiddenids = self.getallhiddenids(wordids, urlids)\n        self.urlids = urlids\n\n        # node output\n        self.ai = [1.0]*len(self.wordids)\n        self.ah = [1.0]*len(self.hiddenids)\n        self.ao = [1.0]*len(self.urlids)\n\n        # create weight matrix\n        self.wi = [[self.getstrength(wordid, hidderid, 0)\n            for hidderid in hiddenids]\n            for wordid in wordids]\n        self.wo = [[self.getstrength(hidderid, urlid, 1)\n            for urlid in self.urlids]\n            for hidderid in self.hiddenids]\n\n    def feedforward(self):\n        # The only input are the query words \n        for i in range(len(self.wordids)):\n            self.ai[i] = 1.0\n        \n        # hidden activitions\n        for j in range(len(self.hiddenids)):\n            sum = 0.0\n            for i in range(len(self.wordids)):\n                sum += self.ai[i] * self.wi[i][j]\n            self.ah[j] = tanh(sum)\n\n        # output activitions\n        for k in range(len(self.urlids)):\n            sum = 0.0\n            for j in range(len(self.hiddenids)):\n                sum += self.ah[j] * self.wo[j][k]\n            self.ao[k] = tanh(sum)\n        return self.ao[:]\n\n    def getresult(self, wordids, urlids):\n        self.setupnetwork(wordids, urlids)\n        return self.feedforward()\n\n    def dtanh(y):\n        return 1.0 - y * y\n\n\n \n    def backpropagate(self, targets, N = 0.5):\n        # calculate error for output \n        output_deltas = [0.0] * len(self.urlids)\n        for k in range(len(self.urlids)):\n            error = targets[k] - self.ao[k]\n            output_deltas[k] = dtanh(self.ao[k]) * error\n\n        # calculate error for hidden layer\n        hidden_deltas = [0.0] * len(self.hiddenids)\n        for j in range(len(self.hiddenids)):\n            error = 0.0\n            for k in range(len(self.urlids)):\n                error += output_deltas[k] * self.wo[j][k]\n            hidden_deltas[j] = dtanh(self.ah[j]) * error\n\n        # update output weights\n        for j in range(len(self.hiddenids)):\n            for k in range(len(self.urlids)):\n                change = output_deltas[k] * self.ah[j]\n                self.wo[j][k] = self.wo[j][k] + N * change\n\n        # update hidden weight\n        for i in range(len(self.wordids)):\n            for j in range(len(self.hiddenids)):\n                change = hidden_deltas[j] * self.ai[i]\n                self.wi[i][j] = self.wi[i][j] + N * change\n\n\n    def trainquery(self, wordids, urlids, selecturl):\n        # generate a hidden node if necessary\n        self.generatehiddennode(wordids, urlids)\n\n        self.setupnetwork(wordids, urlids)\n        self.feedforward()\n        targets = [0.0] * len(urlids)\n        targets[urlids.index(selecturl)] = 1.0\n        error = self.backpropagate(targets)\n        self.setupnetwork()\n\n\n    def updatedatabase(self):\n        # set them to database value\n        for i in range(len(self.wordids)):\n            for j in range(len(self.hiddenids)):\n                self.setstrength(self.wordids[i], self.hiddenids[j], 0, self.wi[i][j])\n        for j in range(len(self.hiddenids)):\n            for k in range(len(self.urlids)):\n                self.setstrength(self.hiddenids[j], self.urlids[k], 1, self.wo[j][k])\n        self.con.commit()\n\n\n'"
book/collective_intelligence/ch04/searchengine.py,0,"b'#! /usr/bin/env python\nimport urllib2\nimport re\nfrom bs4 import BeautifulSoup\nfrom urlparse import urljoin\nfrom pysqlite2 import dbapi2 as sqlite\n\n# create a list of words to ignore\nignorewords = set([\'the\', \'of\', \'to\', \'and\', \'a\', \'in\', \'is\', \'it\'])\n\nclass crawler():\n    #Initialize the crawler with the name of the database\n    def __init__(self, dbname):\n        self.con = sqlite.connect(dbname)\n\n    def __del__(self):\n        self.con.close()\n\n    def dbcommit(self):\n        self.con.commit()\n\n    # Auxilliary function for getting an entry id and adding\n    # it if it\'s not present\n    def getentryid(self, table, field, value, createnew = True):\n        cur = self.con.execute(""\\\n                select rowid from %s where %s = \'%s\'"" %(table, field, value))\n        res = cur.fetchone()\n        if res == None:\n            cur = self.con.execute(""\\\n                    insert into %s (%s) values (\'%s\')"" % (table, field, value))\n            return cur.lastrowid\n        else:\n            return res[0]\n\n    # Index and individual page\n    def addtoindex(self, url, soup):\n        if self.isindexed(url): return\n        print \'Indexing %s\' % url\n\n        # Get the individual words\n        text = self.gettextonly(soup)\n        words = self.separatewords(text)\n\n        # Get the url id\n        urlid = self.getentryid(\'urllist\', \'url\', url)\n\n        # Link each word to this url\n        for i in range(len(words)):\n            word = words[i]\n            if word in ignorewords:\n                continue\n            wordid = self.getentryid(\'wordlist\', \'word\', word)\n            self.con.execute(""\\\n                    insert into wordlocation(urlid, wordid, location)\\\n                            values (%d, %d, %d)"" % (urlid, wordid, i))\n\n    # Extract the text from an HTML page(no tags)\n    def gettextonly(self, soup):\n        v = soup.string\n        if v == None:\n            v = soup.contents\n            resulttext = \'\'\n            for t in v:\n                subtext = self.gettextonly(t)\n                resulttext += subtext + \'\\n\'\n            return resulttext\n        else:\n            return v.strip()\n\n    # Separate the words by any non-whitespace character\n    def separatewords(self, text):\n        spliter = re.compile(\'\\\\W*\')\n        return [s.lower() for s in spliter.split(text) if s != \'\']\n\n    # Return True if this url is already indexed\n    def isindexed(self, url):\n        print self.con.execute(""select * from urllist"")\n        u = self.con.execute(""select rowid from urllist where url = \'%s\'"" % url).fetchone()\n        if u != None:\n            # Check if it has been crawled\n            v = self.con.execute(""\\\n                    select * from wordlocation where urlid = %d\\\n                    "" % u[0]).fetchone()\n            if v != None:\n                return True\n        return False\n\n    # Add a link between two pages\n    def addlinkref(self, urlFrom, urlTo, linkText):\n        words = self.separatewords(linkText)\n        fromid = self.getentryid(\'urllist\', \'url\', urlFrom)\n        toid = self.getentryid(\'urllist\', \'url\', urlTo)\n        if fromid == toid: return\n        cur = self.con.execute(""\\\n                insert into link(fromid, toid) values (%d, %d)\\\n                "" %(fromid, toid))\n        linkid = cur.lastrowid\n        for word in words:\n            if word in ignorewords:\n                continue\n            wordid = self.getentryid(\'wordlist\', \'word\', word)\n            self.con.execute(""\\\n                    insert into linkwords(linkid, wordid) values (%d, %d)\\\n                    "" %(linkid, wordid))\n\n    # Starting with a list of pages, do a breadth\n    # first search to the given depth, indexing pages as we go\n    def crawl(self, pages, depth = 2):\n        for i in range(depth):\n            newpages = set()\n            for page in pages:\n                try:\n                    c = urllib2.urlopen(page)\n                except:\n                    print \'Could not open %s\' % page\n                    continue                  \n                soup = BeautifulSoup(c.read())\n                # self.addtoindex(page, soup)\n\n                links = soup(\'a\')\n                for link in links:\n                    if (\'href\' in dict(link.attrs)):\n                        url = urljoin(page, link[\'href\'])\n                        if url.find(""\'"") != -1:\n                            continue\n                        url = url.split(\'#\')[0] # remove location partion\n\n                        if url[0: 4] == \'http\' and not self.isindexed(url):\n                            newpages.add(url)\n\n                        linkText = self.gettextonly(link)\n                        self.addlinkref(page, url, linkText)\n\n                self.dbcommit()\n\n            pages = newpages\n\n    # Create the database tables\n    def createindextables(self):\n        self.con.execute(\'create table if not exists urllist(url)\')\n        self.con.execute(\'create table if not exists wordlist(word)\')\n        self.con.execute(\'create table if not exists wordlocation(urlid, wordid, location)\')\n        self.con.execute(\'create table if not exists link(fromid integer, toid integer)\')\n        self.con.execute(\'create table if not exists linkwords(wordid, linkid)\')\n        self.con.execute(\'create index if not exists wordidx on wordlist(word)\')\n        self.con.execute(\'create index if not exists urlidx on urllist(url)\')\n        self.con.execute(\'create index if not exists wordurlidx on wordlocation(wordid)\')\n        self.con.execute(\'create index if not exists urltoidx on link(toid)\')\n        self.con.execute(\'create index if not exists urlfromidx on link(fromid)\')\n        self.dbcommit()\n\nclass searcher():\n    def __init__(self, dbname):\n        self.con = sqlite.connect(dbname)\n\n    def __del__(self):\n        self.con.close()\n\n    def dbcommit(self):\n        self.con.commit()\n\n    def getmatchrows(self, q):\n        # String to build the query\n        fieldlist = \'w0.urlid\'\n        tablelist = \'\'\n        clauselist = \'\'\n        wordids = []\n\n        # split the words by space\n        words = q.split(\' \')\n        tablenumber = 0\n\n        for word in words:\n            # Get the wordid\n            print word\n            wordrow = self.con.execute(""\\\n            select rowid from wordlist where word = \'%s\'\\\n            "" % word).fetchone()\n\n            if wordrow != None:\n                wordid = wordrow[0]\n                print wordrow[0]\n                wordids.append(wordid)\n\n                if tablenumber > 0:\n                    tablelist += \',\'\n                    clauselist += \' and \'\n                    clauselist += \'w%d.urlid = w%d.urlid and \' %(tablenumber - 1, tablenumber)\n                fieldlist += \', w%d.location\' % tablenumber\n                tablelist += \'wordlocation w%d\' % tablenumber\n                clauselist += \'w%d.wordid = %d\' % (tablenumber, wordid)\n                tablenumber += 1\n                print \'fieldlist: \', fieldlist\n                print \'tablelist: \', tablelist\n                print \'clauselist: \', clauselist\n\n        # Create the query from separate parts\n        fullquery = \'select %s from %s where %s\' % (fieldlist, tablelist, clauselist)\n        cur = self.con.execute(fullquery)\n        print \'cur: \', cur\n        rows = [row for row in cur]\n\n        return rows, wordids\n\n    def getscoredlist(self, rows, wordids):\n        totalscores = dict([(row[0], 0) for row in rows])\n\n        # This is where you will later put the scoring function\n        weights = [(1.0, self.locationscore(rows)),\n                (1.0, self.frequencyscore(rows)),\n                (1.0, self.pagerankscore(rows)),\n                (1.0, self.linktextscore(rows))]\n\n        for (weight, scores) in weights:\n            for url in totalscores:\n                totalscores[url] += weight * scores[url]\n\n        return totalscores\n\n    def geturlname(self, id):\n        return self.con.execute(""\\\n                select url from urllist where rowid = %d\\\n                "" % id).fetchone()[0]\n\n    def query(self, q):\n        rows, wordids = self.getmatches(q)\n        scores = self.getscoredlist(rows, wordids)\n        rankedscores = sorted([(score, url) for (url, score) in scores.items()], reverse = 1)\n        for (score, urlid) in rankedscores[1: 10]:\n            print \'%f\\t%s\' % (score, self.geturlname(urlid))\n        # return wordids, [r[i] for r in rankedscores[0: 10]]\n\n    def normalizescores(self, scores, smallIsBetter = 0):\n        vsmall = 0.00001 # Avoid division by zero errors\n        if smallIsBetter:\n            minscore = min(scores.values())\n            return dict([(u, float(minscore)/max(vsmall, 1)) for (u, l) in scores.items()])\n        else:\n            maxscore = max(scores.values())\n            if maxscore == 0:\n                maxscore = vsmall\n            return dict([(u, float(c)/maxscore) for (u, c) in scores.items()])\n\n    def frequencyscore(self, rows):\n        contents = dict([(row[0], 0) for row in rows])\n        for row in rows:\n            contents[row[0]] += 1\n        return self.normalizescores(counts)\n\n    def locationscore(self, rows):\n        locations = dict([(row[0], 1000000) for row in rows])\n        for row in rows:\n            loc = sum(row[1: ])\n            if loc < locations[row[0]]:\n                locations[row[0]] = loc \n        return self.normalizescores(locations, smallIsBetter = 1)\n\n    # weights = [(1.0, self.locationscore(rows))]\n\n    def worddistance(self, rows):\n        # If there\'s only one word, everyone wins\n        if len(range(rows)) < 2:\n            return dict([(row[0], 1.0) for row in rows])\n\n        # Initialize the dictionary with large value\n        mindistance = dict([(row[0], 1000000) for row in rows])\n\n        for row in rows:\n            dist = sum([abs(row[i] - row[i - 1]) for i in range(2, len(row))])\n            if dist < mindistance[row[0]]:\n                mindistance[row[0]] = dist \n        return self.normalizescores(mindistance, smallIsBetter = 1)\n\n    def inboundlinkscore(self, rows):\n        uniqueurls = set([row[0] for row in rows])\n        inboundcount = dict([(u, self.con.execute(""\\\n            select count(*) from link where toid = %d\\\n            "" % u).fieldlist()[0]) for u in uniqueurls])\n        return self.normalizescores(inboundcount)\n\n    def calculatepagerank(self, iterations = 20):\n        # clear out the current pagerank tables\n        self.con.execute(""drop table if exists pagerank"")\n        self.con.execute(""create table pagerank(urlid primary key, score)"")\n\n        # Initialize every url with a pagerank of 1\n        self.exe.execute(""insert into pagerank select rowid, 1.0 from urllist"")\n        self.dbcommit()\n\n        for i in range(iterations):\n            print ""Iteration %d"" % (i)\n            for (urlid, ) in self.con.execute(""select distinct fromid from link where toid = %d"" % urlid):\n\n                # Get the PageRank of the linker\n                linkingpr = self.con.execute(""select score from pagerank where urlid = %d"" % linker).fetchone()[0]\n\n                # Get the total number of link from the linker\n                linkingcount = self.con.execute(""\\\n                    select count(*) from link where urlid = %d"" % linker).fetchone()[0]\n                pr += 0.85 * (linkingpr/linkingcount)\n        self.con.execute(""\\\n                update pagerank set score = %f where urlid = %d"" %(pr, urlid))\n        self.dbcommit()\n\n    def pagerankscore(self, rows):\n        pageranks = dict([(row[0], self.con.execute(""\\\n            select score from pagerank where urlid = %d"" % row[0]).fetchone()[0]) for row in rows])\n        maxrank = max(pagerank.values())\n        normalizescores = dict([(u, float(l)/maxrank) for (u, l) in pageraaks.items()])\n        return normalizescores\n\n    def linktextscore(self, rows, wordids):\n        linkscores = dict([(row[0], 0) for row in rows])\n        for wordid in wordids:\n            cur = self.con.execute(""\\\n                    select link.fromid, link.toid from linkwords, link where wordid = %d and linkwords.linkid = link.rowid"" % wordid)\n            for toid in linkscores:\n                pr = self.con.execute(""\\\n                        select score from pagerank where urlid = %d\\\n                        "" % fromid).fetchone()[0]\n                linkscore[toid] += pr \n\n        maxscore = max(linkscores.values())\n        normalizescores = dict([(u, float(l)/maxscore) for (u, l) in linkscores.items()])\n        return normalizescores\n\n\n\n\n'"
book/collective_intelligence/ch05/dorm.py,0,"b""#! /usr/bin/env python\n\nimport random\nimport math\n\n# The dorms, each has two available rooms\ndorms = ['Zeus', 'Athena', 'Hercules', 'Bacchus', 'Pluto']\n\n# People, along with their first and second choices\nprefs = ['Toby', ('Bacchus', 'Hercules'),\n        'Steve', ('Zeus', 'Pluto'),\n        'Andrea', ('Athena', 'Zeus'),\n        'Sarah', ('Zeus', 'Pluto'),\n        'Dave', ('Athena', 'Bacchus'),\n        'Jeff', ('Hercules', 'Pluto'),\n        'Fred', ('Pluto', 'Athena'),\n        'Suzie', ('Bacchus', 'Hercules'),\n        'Laure', ('Bacchus', 'Hercules'),\n        'Neil', ('Hercules', 'Athena')]\n\ndomain = [(0, (len(dorms)*2) -i -1) for i in range(0, len(dorms)*2)]\n\ndef printsolution(vec):\n    slots = []\n    # create two slots for each dorms\n    for i in range(len(dorms)):\n        slots += [i, i]\n\n    # Loop over each students assignment\n    for i in range(len(students)):\n        x = int(vec[i])\n\n        # choose the slot from the remaining ones\n        dorm = slots[x]\n\n        print prefs[i][0], dorm\n\n        # remove this slot\n        del slots[x]\n\ndef dormcost(vec):\n    cost = 0\n    slots = []\n    for i in range(len(dorms)):\n        slots += [i, i]\n\n    # loop over each student\n    for i in range(len(vec)):\n        x = int(vec[i])\n        dorm = dorms[slots[x]]\n        pref = prefs[i][1]\n\n        if dorm == pref[0]:\n            cost = 0\n        elif dorm == pref[1]:\n            cost += 1\n        else:\n            cost += 3\n\n        # remove this slot\n        del slot[x]\n\n    return cost\n"""
book/collective_intelligence/ch05/optimization.py,0,"b""#! /usr/bin/env python\n\nimport time\nimport random\nimport math\n\npeople= [('Setmor', 'BOS'),\n        ('Franny', 'DAL'),\n        ('Zooey', 'CAK'),\n        ('Walt', 'ORD'),\n        ('Buddy', 'ORD'),\n        ('Les', 'OMA')]\n\n# LaGuardia airport in New York\ndestination = 'LGA'\n\nflights = {}\n\nfor line in file('schedule.txt'):\n    origin, dest, depart, arrive, price = line.strip().split(',')\n    flights.setdefault((origin, dest), [])\n\n    # add details to the list of possiable flights\n    flights[(origin, dest)].append((depart, arrive, int(price)))\n\ndef getminutes(t):\n    x = time.strptime(t, '%H:%M')\n    return x[3]*60 + x[4]\n\ndef printschedule(r):\n    for d in range(len(r)/2):\n        name = people[d][0]\n        origin = people[d][1]\n        out = flights[(origin, destination)][r[d]]\n        ret = flights[(destination, origin)][r[d + 1]]\n        print '%10s%10s %5s%5s $%3s %5s-%5s $%3s' % (name, origin,\n                out[0], out[1], out[2], ret[0], ret[1], ret[2])\n\ndef schedulecost(sol):\n    totalprice = 0\n    latestarrival = 0\n    earliestdep = 24 * 60\n\n    for d in range(len(sol)/2):\n        # Get the inbound and outbound flights\n        origin = people[d][1]\n        print flights[(origin, destination)]\n        print int(sol[d])\n        print 'd', d\n        print 'sol: ', sol\n        print flights[(origin, destination)][int(sol[d])]\n        outbound = flights[(origin, destination)][int(sol[d])]\n        # print outbound\n        print flights[(destination, origin)]\n        print 'returnf: ', [int(sol[d])]\n        returnf = flights[(destination, origin)][int(sol[d])]\n\n        # Total price is the price of all outbound and return flights\n        totalprice += outbound[2]\n        totalprice += returnf[2]\n\n        # Track the latest arrival and earliest departure\n        if latestarrival < getminutes(outbound[1]):\n            latestarrival = getminutes(outbound[1])\n        if earliestdep > getminutes(outbound[0]):\n            earliestdep = getminutes(returnf[0])\n\n    # Every Person must wait at the airport until the latest person arrives\n    # They also must arrive at the same time and wait for their flights\n    totalwait = 0\n    for d in range(len(sol)/2):\n        origin = people[d][1]\n        outbound = flights[(origin, destination)][int(sol[d])]\n        returnf = flights[(destination, origin)][int(sol[d])]\n        totalwait += latestarrival - getminutes(outbound[1])\n        totalwait += earliestdep - getminutes(returnf[0]) # - earliestdep\n\n    # Does this solution require an extra day of car rental?\n    # That will be 50 $\n    if latestarrival > earliestdep:\n        totalprice += 50\n        \n    return totalprice + totalwait\n\ndef randomoptimize(domain, costf):\n    best = 999999999\n    bestr = None\n    for i in range(10000):\n        # create a random solution\n        r = [random.randint(domain[i][0], domain[i][1]) for i in range(len(domain))]\n\n        # get the cost\n        cost = costf(r)\n\n        # compare with the best one so far\n        if cost < best:\n            best = cost\n            bestr = r\n    return r\n\ndef hillclimb(domain, costf):\n    # create a random solution\n    sol = [random.randint(domain[i][0], domain[i][1]) for i in range(len(domain))]\n    print 'sol: ', sol\n\n    # Main loop\n    while 1:\n        # create list of neighboring solution\n        neighbors = []\n        for j in range(len(domain)):\n\n            # One way in each direction\n            if sol[j] > domain[j][0]:\n                neighbors.append(sol[0: j] + [sol[j] + 1] + sol[j + 1: ])\n            if sol[j] < domain[j][1]:\n                neighbors.append(sol[0: j] + [sol[j] - 1] + sol[j + 1: ])\n\n        # see what the best solution among the neighbors is\n        current = costf(sol)\n        best = current\n        for j in range(len(neighbors)):\n            print neighbors[j]\n            cost = costf(neighbors[j])\n            if cost < best:\n                best = cost\n                sol = neighbors[j]\n\n        # If there's no improvement, then we have reached the top\n        if best == current:\n            break\n\n    return sol\n\ndef annealingoptimize(domain, costf, T = 10000.0, cool = 0.95, step = 1):\n    # Initialize the values randomly\n    vec = [float(random.randint(domain[i][0], domain[i][1])) for i in range(len(domain))]\n\n    while T > 0.1:\n        # Choose one of the indices\n        i = random.randint(0, len(domain) - 1)\n\n        # choose a direction to change it\n        dir = random.randint(-step, step)\n\n        # create a new list with one of the values changed\n        vecb = vec[:]\n        vecb[i] += dir\n        if vecb[i] < domain[i][0]:\n            vecb[i] = domain[i][0]\n        elif vecb[i] > domain[i][1]:\n            vecb[i] = domain[i][1]\n\n        # calculate the current cost and the new cost\n        ea = costf(vec)\n        eb = costf(vecb)\n        p = pow(math.e, (-eb-ea)/T)\n\n        # Is this better or does it make the probability cutoff\n        if (eb < ea or random.random() < p):\n            vec = vecb\n\n        # decrease the temperature\n        T = T*cool\n    return vec\n\ndef geneticoptimize(domain, costf, popsize = 50, step = 1, \n        mutprod = 0.2, elite = 0.2, maxiter = 100):\n    # Mutation operation\n    def mutate(vec):\n        i = random.randint(0, len(domain) - 1)\n        print 'i value: ', i\n        print 'vec[i]: ', vec[i]\n        print 'domain[i][0]: ', domain[i][0]\n        if random.random() < 0.5 and vec[i] > domain[i][0]:\n            print 'test mutate 1'\n            return vec[0: i] + [vec[i] - step] + vec[i + 1: ]\n        else: # if vec[i] < domain[i][1]:\n            print 'test mutate 2'\n            return vec[0: i] + [vec[i] + step] + vec[i + 1: ]\n\n    # Crossover operation\n    def crossover(r1, r2):\n        i = random.randint(1, len(domain) - 2)\n        return r1[0: i] + r2[i: ]\n\n    # Build the Initial population\n    pop = []\n    for i in range(popsize):\n        vec = [random.randint(domain[i][0], domain[i][1]) for i in range(len(domain))]\n        pop.append(vec)\n\n    # for p in pop:\n        # print p\n        # pscore = [costf(p), p]\n        # print pscore\n        # l = range(len(p)/2)\n        # print l\n\n    # How many winners for each generation\n    topelite = int(elite * popsize)\n\n    # Main loop\n    for i in range(maxiter):\n        # print i, '\\n'\n        # print '\\n'\n        # print pop\n        # for p in pop:\n        #     s = costf(p)\n        #     print p, s\n        #    print '\\n'\n        # print 'test'\n        scores = [(costf(v), v) for v in pop]\n        # print 'get score %d', i , scores \n        scores.sort()\n        # print 'scores sort: ', scores\n        ranked = [v for (s, v) in scores]\n\n        # start with the pure winner\n        pop = ranked[0: topelite]\n        print 'pop: ', pop\n\n        # Add mutate and bred forms of the winner\n        while len(pop) < popsize:\n            if random.random() < mutprod:\n                # Mutation\n                c = random.randint(0, topelite)\n                print 't1c:', c\n                print 'ranked c: ', ranked[c]\n                print mutate(ranked[c])\n                pop.append(mutate(ranked[c]))\n                print 'pop append 1: ', pop\n            else:\n                #Crossover\n                c1 = random.randint(0, topelite)\n                c2 = random.randint(0, topelite)\n                print 't2c1: ', c1\n                print 'ranked[c1]', ranked[c1]\n                print 'ranked[c2]', ranked[c2]\n                print crossover(ranked[c1], ranked[c2])\n                pop.append(crossover(ranked[c1], ranked[c2]))\n                print 'pop append 2: ', pop\n        print 'ttttttttttt'\n        print pop\n        # print current best score\n        print scores[0][0]\n\n    return scores[0][1]\n\n"""
book/collective_intelligence/ch07/test.py,0,"b'#! /usr/bin/.env python2\n\nimport treepredict\n\nprint ""Gini impurity\\n""\nprint treepredict.giniimpurity(treepredict.my_data)\n\nprint ""\\n""\n\nprint ""treepredict.entropy\\n""\nprint treepredict.entropy(treepredict.my_data)\n\nprint ""\\n""\n\nset1, set2 = treepredict.divideset(treepredict.my_data, 2, \'yes\')\n\nprint ""Gini impurity\\n""\nprint treepredict.giniimpurity(set1)\nprint ""treepredict.entropy\\n""\nprint treepredict.entropy(set1)\n\nprint \'\\n\'\ntree = treepredict.buildtree(treepredict.my_data)\nprint \'tree: \', tree\n\nprint \'\\n\'\n\nprint \'classify: \', treepredict.classify([\'(direct)\', \'USA\', \'yes\', 5], tree)\n'"
book/collective_intelligence/ch07/treepredict.py,0,"b""#! /usr/bin/env python2\r\n\r\nmy_data=[['slashdot','USA','yes',18,'None'],\r\n        ['google','France','yes',23,'Premium'],\r\n        ['digg','USA','yes',24,'Basic'],\r\n        ['kiwitobes','France','yes',23,'Basic'],\r\n        ['google','UK','no',21,'Premium'],\r\n        ['(direct)','New Zealand','no',12,'None'],\r\n        ['(direct)','UK','no',21,'Basic'],\r\n        ['google','USA','no',24,'Premium'],\r\n        ['slashdot','France','yes',19,'None'],\r\n        ['digg','USA','no',18,'None'],\r\n        ['google','UK','no',18,'None'],\r\n        ['kiwitobes','UK','no',19,'None'],\r\n        ['digg','New Zealand','yes',12,'Basic'],\r\n        ['slashdot','UK','no',21,'None'],\r\n        ['google','UK','yes',18,'Basic'],\r\n        ['kiwitobes','France','yes',19,'Basic']]\r\n\r\n\r\nclass decisionnode:\r\n    def __init__(self, col = -1, value = None, results = None, tb = None, fb = None):\r\n        self.col = col\r\n        self.value = value\r\n        self.results = results\r\n        self.tb = tb\r\n        self.fb = fb\r\n    \r\n# Divides a set on a specific column, Can handle numeric \r\n# or nominal values\r\ndef divideset(rows, column, value):\r\n    # Make a function that tells us if a row is in \r\n    # The first group (true) or the second group (false)\r\n    split_function = None\r\n    if isinstance(value, int) or isinstance(value, float):\r\n        split_function = lambda row: row[column] >= value\r\n    else:\r\n        split_function = lambda row: row[column] == value\r\n\r\n    # Divides the rows into two sets and return them\r\n    set1 = [row for row in rows if split_function(row)]\r\n    set2 = [row for row in rows if not split_function(row)]\r\n\r\n    return (set1, set2)\r\n\r\n# Create counts of possible results \r\n# The last column of each row is the result \r\ndef uniquecounts(rows):\r\n    results = {}\r\n    for row in rows:\r\n        # The result is the last column\r\n        r = row[len(row) - 1]\r\n        if r not in results:\r\n            results[r] = 0\r\n        results[r] += 1\r\n    return results\r\n\r\n# Probability that a randomly placed item will\r\n# be in the wrong category\r\ndef giniimpurity(rows):\r\n    total = len(rows)\r\n    counts = uniquecounts(rows)\r\n    imp = 0\r\n    for k1 in counts:\r\n        p1 = float(counts[k1])/total\r\n        for k2 in counts:\r\n            if k1 == k2:\r\n                continue\r\n            else:\r\n                p2 = float(counts[k2])/total\r\n            imp += p1*p2\r\n    return imp\r\n\r\n# Entropy is the sum of p(x)log(p(x)) across all \r\n# the different result\r\n\r\ndef entropy(rows):\r\n    from math import log\r\n    log2 = lambda x:log(x)/log(2)\r\n    results = uniquecounts(rows)\r\n    \r\n    #Now calculate the entropy\r\n    ent = 0.0\r\n    for r in results.keys():\r\n        p = float(results[r])/len(rows)\r\n        ent = ent - p*log2(p)\r\n    return ent\r\n\r\ndef buildtree(rows, scoref = entropy):\r\n    if len(rows) == 0:\r\n        return decisionnode()\r\n\r\n    current_score = scoref(rows)\r\n\r\n    # Set up some variable to track the best criteria\r\n    best_gain = 0.0\r\n    best_criteria = None\r\n    best_sets = None\r\n\r\n    column_count = len(rows[0]) - 1\r\n    for col in range(0, column_count):\r\n        # Generate the different values in that column\r\n        column_values = {}\r\n        for row in rows:\r\n            print 'row: ', row\r\n            print 'col: ', col\r\n            column_values[row[col]] = 1\r\n        # Now try dividing the rows up for each value\r\n        # in this column\r\n        for value in column_values.keys():\r\n            (set1, set2) = divideset(rows, col, value)\r\n\r\n            # Information gain\r\n            p = float(len(set1))/len(rows)\r\n            gain = current_score - p*scoref(set1) - (1 - p)*scoref(set2)\r\n\r\n            if gain > best_gain and len(set1) > 0 and len(set2) > 0:\r\n                best_gain = gain\r\n                best_criteria = (col, value)\r\n                best_sets = (set1, set2)\r\n\r\n    # create the subbranches\r\n    if best_gain > 0:\r\n        true_branch = buildtree(best_sets[0])\r\n        false_branch = buildtree(best_sets[1])\r\n        return decisionnode(col = best_criteria[0], value = best_criteria[1], tb = true_branch, fb = false_branch)\r\n    else:\r\n        return decisionnode(results = uniquecounts(rows))\r\n\r\ndef classify(observation, tree):\r\n    if tree.result != None:\r\n        return tree.result\r\n    else:\r\n        v = observation[tree.col]\r\n        branch = None\r\n        if isinstance(v, int) or isinstance(v, float):\r\n            if v >= tree.value:\r\n                branch = tree.tb\r\n            else:\r\n                branch = tree.tb\r\n        else:\r\n            if v == tree.value:\r\n                branch = tree.tb\r\n            else:\r\n                branch = tree.fb\r\n    return classify(observation, branch)\r\n\r\ndef prune(tree, mingain):\r\n    # If the branches aren't leaves, then prune them\r\n    if tree.tb.result == None:\r\n        prune(tree.tb, mingain)\r\n    else:\r\n        prune(tree.fb, mingain)\r\n\r\n    # If both the subbranches are now leaves, see if they should \r\n    # merged\r\n    if tree.tb.result != None and tree.fb.result != None:\r\n        # build a combined dataset\r\n        tb, fb = [], []\r\n        for v, c in tree.tb.results.items():\r\n            tb += [[v]]*c\r\n        for v, c in tree.fb.results.items():\r\n            fb += [[v]]*c\r\n\r\n        # Test the reduction in entropy\r\n        delta = entropy(tb + fb) - ((entropy(tb) + entropy(fb))/2)\r\n        if delta < mingain:\r\n            # Merge the branches\r\n            tree.tb, tree.fb = None, None\r\n            tree.results = uniquecounts(tb + fb)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"""
book/collective_intelligence/ch08/numpredict.py,0,"b""from random import random, randint\nimport math\n\ndef wineprice(rating, age):\n    peak_age = rating - 50\n\n    # Calculate prices based on rating\n    price = rating / 2\n    if age > peak_age:\n        # Past its peak age, gone bad in 5 years\n        price = price * (5 - (age - peak_age))\n    else:\n        # increase to 5x original value as it \n        # approaches its peak\n        price = price * (5*((age + 1)/peak_age))\n\n    if price < 0:\n        price = 0\n    return price\n\ndef wineset1():\n    rows = []\n    for i in range(300):\n        # create a random age and rating\n        rating = random()*50 + 50\n        age = random()*50\n\n        # Get reference price\n        price = wineprice(rating, age)\n\n        # Add some noise\n        price *= (random()*0.4 + 0.8)\n\n        # Add to the dataset\n        rows.append({'input': (rating, age), 'result': price})\n\n    return rows\n\ndef euclidean(v1, v2):\n    d = 0.0\n    for i in range(len(v1)):\n        d += (v1[i] - v2[i])**2\n    return math.sqrt(d)\n\ndef getdistance(data, vec1):\n    distancelist = []\n    for i in range(len(data)):\n        vec2 = data[i]['input']\n        distancelist.append((euclidean(vec1, vec2), i))\n    distancelist.sort()\n    return distancelist\n\ndef knnestimate(data, vec1, k = 3):\n    # Get sorted distance\n    dlist = getdistance(data, vec1)\n\n    avg = 0.0\n\n    # Take the average of the top k results\n    for i in range(k):\n        idx = dlist[i][1]\n        avg += data[idx]['result']\n    avg = avg/k\n\n    return avg\n\ndef inverseweight(dist, num = 1.0, const = 0.1):\n    return num/(dist + const)\n\ndef subtractweight(dist, const = 1.0):\n    if dist > const:\n        return 0\n    else:\n        return const - dist\n\ndef gaussian(dist, sigma = 10.0):\n    return math.e**(-dist**2/(2*sigma**2))\n    \n    \ndef weightknn(data, vec1, k = 5, weightf = gaussian):\n    # Get distances\n    dlist = getdistance(data, vec1)\n    avg = 0.0\n    totalweight = 0.0\n\n    # Get weighted average\n    for i in range(k):\n        dist = dlist[i][0]\n        idx = dlist[i][1]\n        weight = weightf(dist)\n\n        avg += weight * data[idx]['result']\n        totalweight += weight\n    avg = avg/totalweight\n\n    return avg\n\n\ndef dividedata(data, test = 0.05):\n    trainset = []\n    testset = []\n    for row in data:\n        if random() < test:\n            testset.append(row)\n        else:\n            trainset.append(row)\n    return trainset, testset\n\ndef testalgorithm(algf, trainset, testset):\n    error = 0.0\n    for row in testset:\n        guess = algf(trainset, row['input'])\n        error += (row['result'] - guess)**2\n    return error/len(testset)\n\ndef crossvalidate(algf, data, trials = 100, test = 0.05):\n    error = 0.0\n    for i in range(trials):\n        trainset, testset = dividedata(data, test)\n        error += testalgorithm(algf, trainset, testset)\n    return error/trials\n"""
book/collective_intelligence/ch09/advancedclassify.py,0,"b""#! /usr/bin/env python2\n\nimport sys\nfrom pylab import *\n\nclass matchrow(object):\n    def __init__(self, row, allnum = False):\n        if allnum:\n            self.data = [float(row[i]) for i in range(len(row) - 1)]\n        else:\n            self.data = row[0: len(row) - 1]\n        self.match = int(row[len(row) - 1])\n\ndef loadmatch(f, allnum = False):\n    rows = []\n    for line in file(f):\n        rows.append(matchrow(line.split(','), allnum))\n    return rows\n\ndef plotagematches(rows):\n    xdm, ydm = [r.data[0] for r in rows if r.match == 1], [r.data[1] for r in rows if r.match == 1]\n    xdn, ydn = [r.data[0] for r in rows if r.match == 0], [r.data[1] for r in rows if r.match == 0]\n    plot(xdm, ydm, 'go')\n    plot(xdn, ydn, 'ro')\n    show()\n\ndef lineartrain(rows):\n    averages = {}\n    counts = {}\n    for row in rows:\n        cl = row.match\n\n        averages.setdefault(cl, [0.0]*(len(row.data)))\n        counts.setdefault(cl, 0)\n\n        ## Add this point to the averages\n        for i in range(len(row.data)):\n            averages[cl][i] += float(row.data[i])\n\n        ## Keep track of how many pomits in each class\n        counts[cl] += 1\n\n    # divides sums by counts to get the averags\n    for cl, avg in averages.items():\n        for i in range(len(avg)):\n            avg[i] /=counts[cl]\n\n    return averages\n\ndef dotproduct(v1, v2):\n    return sum([v1[i]*v2[i] for i in range(len(v1))])\n\ndef dpclassify(point, avgs):\n    b = (dotproduct(avgs[1], avgs[1]) - dotproduct(avgs[0], avgs[0]))/2\n    y = dotproduct(point,avgs[0]) - dotproduct(point, avgs[1]) + b\n    if y > 0:\n        return 0\n    else:\n        return 1\n\ndef yesno(v):\n    if v == 'yes':\n        return 1\n    elif v == 'no':\n        return -1\n    else:\n        return 0\n\ndef matchcount(interest1, interest2):\n    l1 = interest1.split(':')\n    l2 = interest2.split(':')\n\n    x = 0\n\n    for v in l1:\n        if v in l2:\n            x += 1\n    return x\n\ndef milesdistance(a1, a2):\n    return 0\n\ndef loadnumeric():\n    oldrows = loadmatch('./matchmaker.csv')\n    newrows = []\n    for row in oldrows:\n        d = row.data\n        data = [float(d[0]), yesno(d[1]), yesno(d[2]), float(d[5]),\n        yesno(d[6]), yesno(d[7]), matchcount(d[3], d[8]), milesdistance(d[4], d[9]), row.match]\n        newrows.append(matchrow(data))\n    return newrows\n\ndef scaledata(rows):\n    low = [99999999.0]*len(rows[0].data)\n    high = [-99999999.0]*len(rows[0].data)\n    for row in rows:\n        d = row.data\n        for i in range(len(d)):\n            if d[i] < low[i]:\n                low[i] = d[i]\n            if d[i] > high[i]:\n                high[i] = d[i]\n\n    def scaleinput(d):\n        return [(d[i] - low[i])/(high[i] - low[i]) for i in range(len(low) - 1)]\n\n    newrows = [matchrow(scaleinput(row.data) + [row.match]) for row in rows]\n\n    return newrows, scaleinput\n\ndef veclength(v):\n    print 'v: ', v\n    return sum([v[i] for i in range(len(v))])**0.5\n\ndef rbf(v1, v2, gamma = 20):\n    dv = [v1[i] - v2[i] for i in range(len(v1))]\n    l = veclength(dv)\n    return math.e**(-gamma*l)\n    \ndef nlclassify(point, rows, offset, gamma = 10):\n    sum0 = 0.0\n    sum1 = 0.0\n    count0 = 0.0\n    count1 = 0.0\n\n    offset = getoffset(rows)\n    print 'offset: ', offset\n\n    for row in rows:\n        if row.match == 0:\n            sum0 += rbf(point, row.data, gamma)\n        else:\n            sum1 += rbf(point, row.data, gamma)\n\n    y = (1.0/count0)*sum0 - (1.0/count1)*sum1 + offset\n\n    if y < 0:\n        return 0\n    else:\n        return 1\n\n\ndef getoffset(rows, gamma = 10):\n    l0 = []\n    l1 = []\n    for row in rows:\n        if row.match == 0:\n            l0.append(row.data)\n        else:\n            l1.append(row.data)\n\n    sum0 = sum(sum([rbf(v1, v2, gamma) for v1 in l0]) for v2 in l0)\n    sum1 = sum(sum([rbf(v1, v2, gamma) for v1 in l1]) for v2 in l1)\n    print 'sum0: ', sum0\n    print 'sum1: ', sum1\n\n    return (1.0/(len(l1)**2))*sum1 - (1.0/(len(l0)**2))*sum0\n\n\n\n\n\n\n\n\n\n\n\n"""
