file_path,api_count,code
cupy/__init__.py,0,b''
cupy/main.py,0,"b'from modules import *\nimport time, os, argparse\nimport cupy as cp\nfrom mnist import MNIST\nfrom modules import CapsNet, CapsLoss\nfrom optim import AdamOptimizer\n\n    \ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Cupy Capsnet\')\n    parser.add_argument(\'--bs\', dest=\'bs\',\n                      help=\'batch size\',\n                      default=\'100\', type=int)\n    parser.add_argument(\'--lr\', dest=\'lr\',\n                      help=\'learning rate\',\n                      default=1e-2, type=float)\n    parser.add_argument(\'--opt\', dest=\'opt\',\n                      help=\'optimizer\',\n                      default=\'adam\', type=str)\n    parser.add_argument(\'--disp\', dest=\'disp_interval\',\n                      help=\'interval to display training loss\',\n                      default=\'10\', type=int)\n    parser.add_argument(\'--num_epochs\', dest=\'num_epochs\',\n                      help=\'num epochs to train\',\n                      default=\'100\', type=int)\n    parser.add_argument(\'--val_epoch\', dest=\'val_epoch\',\n                      help=\'num epochs to run validation\',\n                      default=\'1\', type=int)\n    \n    args = parser.parse_args()\n    \n    return args\n\nif __name__ == \'__main__\':\n    \n    args = parse_args()\n    \n    mnist = MNIST(bs=args.bs, shuffle=True)\n    eye = cp.eye(mnist.num_classes)\n    model = CapsNet()\n\n    criterion = CapsLoss()\n    if args.opt == \'adam\':\n        optimizer = AdamOptimizer(lr=args.lr)\n        \n    print(\'Training started!\')\n\n    for epoch in range(args.num_epochs):\n        start = time.time()\n\n        # train\n        correct = 0\n        for batch_idx, (imgs, targets) in enumerate(mnist.train_dataset):\n            optimizer.step()\n            if imgs.shape[0] != args.bs:\n                continue\n\n            targets = eye[targets]\n            scores, reconst = model(imgs)\n            loss, grad = criterion(scores, targets, reconst, imgs)\n            model.backward(grad, optimizer)\n\n            classes = cp.argmax(scores, axis=1)\n            predicted = eye[cp.squeeze(classes), :]\n\n            predicted_idx = cp.argmax(predicted, 1)\n            label_idx = cp.argmax(targets, 1)\n            correct = cp.sum(predicted_idx == label_idx)\n\n            # info\n            if batch_idx % args.disp_interval == 0:\n                end = time.time()\n                print(""[epoch %2d][iter %4d] loss: %.4f, acc: %.4f%% (%d/%d)"" \\\n                                % (epoch, batch_idx, loss, 100.*correct/args.bs, correct, args.bs))\n\n        # val\n        if epoch % args.val_epoch == 0:\n            print(\'Validating...\')\n            correct = 0\n            total = 0\n\n            for batch_idx, (imgs, targets) in enumerate(mnist.eval_dataset):\n                if imgs.shape[0] != args.bs:\n                    continue\n\n                targets = eye[targets]\n                scores, reconst = model(imgs)\n                loss, grad = criterion(scores, targets, reconst, imgs)\n                model.backward(grad, optimizer)\n\n                classes = cp.argmax(scores, axis=1)\n                predicted = eye[cp.squeeze(classes, axis=1), :]\n\n                predicted_idx = cp.argmax(predicted, 1)\n                label_idx = cp.argmax(targets, 1)\n                correct += cp.sum(predicted_idx == label_idx)\n                total += targets.shape[0]\n\n            print(""[epoch %2d] val acc: %.4f%% (%d/%d)"" \\\n                                    % (epoch, 100.*correct/total, correct, total))\n'"
cupy/mnist.py,6,"b'import time, os\nimport numpy as np\nimport cupy as cp\nfrom urllib import request\nimport gzip\nimport pickle\n\n\nclass MNIST:\n    def __init__(self, path=\'data\', bs=1, shuffle=False):\n        self.filename = [\n        [""training_images"",""train-images-idx3-ubyte.gz""],\n        [""test_images"",""t10k-images-idx3-ubyte.gz""],\n        [""training_labels"",""train-labels-idx1-ubyte.gz""],\n        [""test_labels"",""t10k-labels-idx1-ubyte.gz""]\n        ]\n        self.mean = 0.1307\n        self.std = 0.3081\n        self.num_classes = 10\n        self.bs = bs\n        self.path = path\n        if not os.path.exists(self.path):\n            os.mkdir(self.path)\n        if not os.path.exists(self.path+\'/mnist.pkl\'):\n            self.download_mnist()\n        self.load(shuffle=shuffle)\n        print(\'Loading complete.\')\n        \n    def download_mnist(self):\n        base_url = ""http://yann.lecun.com/exdb/mnist/""\n        for name in self.filename:\n            print(""Downloading ""+name[1]+""..."")\n            request.urlretrieve(base_url+name[1], self.path+\'/\'+name[1])\n        print(""Download complete."")\n        self.save_mnist()\n\n    def save_mnist(self):\n        mnist = {}\n        for name in self.filename[:2]:\n            with gzip.open(self.path+\'/\'+name[1], \'rb\') as f:\n                mnist[name[0]] = ((np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28))/255.-self.mean)/self.std\n        for name in self.filename[-2:]:\n            with gzip.open(self.path+\'/\'+name[1], \'rb\') as f:\n                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n        with open(self.path+\'/\'+""mnist.pkl"", \'wb\') as f:\n            pickle.dump(mnist,f)\n        print(""Save complete."")\n\n    def chunks(self, l):\n        for i in range(0, len(l), self.bs):\n            yield l[i:i + self.bs]\n\n    def load(self, shuffle=False):\n        with open(self.path+""/mnist.pkl"",\'rb\') as f:\n            mnist = pickle.load(f)\n        if shuffle:\n            n = mnist[\'training_images\'].shape[0]\n            idxs = np.arange(n)\n            np.random.shuffle(idxs)\n            mnist[\'training_images\'] = mnist[\'training_images\'].reshape((-1,1,28,28))\n            mnist[\'training_images\'] = list(self.chunks(mnist[\'training_images\'][idxs]))\n            mnist[\'training_labels\'] = list(self.chunks(mnist[\'training_labels\'][idxs]))\n            self.train_dataset = zip(cp.array(mnist[\'training_images\']), cp.array(mnist[\'training_labels\']))\n            \n            n = mnist[\'test_images\'].shape[0]\n            idxs = np.arange(n)\n            np.random.shuffle(idxs)\n            mnist[\'test_images\'] = mnist[\'test_images\'].reshape((-1,1,28,28))\n            mnist[\'test_images\'] = list(self.chunks(mnist[\'test_images\'][idxs]))\n            mnist[\'test_labels\'] = list(self.chunks(mnist[\'test_labels\'][idxs]))\n            self.eval_dataset = zip(cp.array(mnist[\'test_images\']), cp.array(mnist[\'test_labels\']))\n\n            '"
cupy/modules.py,0,"b""\n# im2col functions adapted from https://github.com/Burton2000/CS231n-2017/blob/master/assignment2/cs231n/im2col.py\n\nimport cupy as cp\nimport time, os\n\n\ndef tile(arr, copy, axis):\n    return cp.concatenate([arr] * copy, axis=axis)\n\n        \nclass Module(object):\n    def __init__(self, trainable=False):\n        self.trainable = trainable\n        pass\n    \n    def forward(self, x):\n        raise NotImplementedError\n        \n    def backward(self, grad, optimizer=None):\n        raise NotImplementedError\n        \n    def __call__(self, *input, **kwargs):\n        return self.forward(*input, **kwargs)\n\n    \nclass Sequence(Module):\n    def __init__(self, modules):\n        self._modules = modules\n        \n    def forward(self, inpt):\n        t = time.time()\n        for module in self._modules:\n            inpt = module(inpt)\n            cur = time.time()\n            t = cur\n            if module.trainable:\n                self.trainable = True\n        return inpt\n    \n    def backward(self, grad, optimizer=None):\n        for module in self._modules[::-1]:\n            if module.trainable:\n                grad = module.backward(grad, optimizer)\n            else:\n                grad = module.backward(grad)\n            \n        return grad\n    \n    def modules(self):\n        return self._modules\n    \n    def trainable_modules(self):\n        return [i for i in self._modules if i.trainable]\n\n    \nclass Linear(Module):\n    def __init__(self, in_channel, out_channel):\n        super(Linear, self).__init__(trainable=True)\n        std = 1/cp.sqrt(in_channel)\n        self.w = cp.random.uniform(-std, std, (out_channel, in_channel))\n        self.b = cp.random.uniform(-std, std, (1, out_channel))\n        self.x = None\n        \n    def _set_params(self, params):\n        w, b = params\n        self.w = w\n        self.b = b\n        if len(self.b.shape) < 2:\n            self.b = self.b[None,:]\n        \n    def forward(self, x):\n        out = x.dot(self.w.T) + self.b\n        self.x = x\n        return out\n    \n    def backward(self, grad, optimizer=None):\n        dw = (self.x.T @ grad).T\n        db = cp.sum(grad, axis=0, keepdims=True)\n        # update parameters\n        if optimizer is not None:\n            self.w = optimizer(self.w, dw)\n            self.b = optimizer(self.b, db)\n        \n        dx = grad @ self.w\n        dx = cp.reshape(dx, self.x.shape)\n        return dx\n   \n\nclass ReLU(Module):\n    def __init__(self, alpha=0):\n        super(ReLU, self).__init__()\n        self.alpha = alpha\n        self.x = None\n        \n    def forward(self, x):\n        out = x.copy()\n        if self.alpha > 0:\n            out[out<0] = self.alpha*x\n        else:\n            out[out<0] = 0\n        self.x = x\n        return out\n    \n    def backward(self, grad):\n        dx = grad.copy()\n        dx[self.x < 0] = 0\n        return dx\n\nclass Sigmoid(Module):\n    def __init__(self):\n        super(Sigmoid, self).__init__()\n        self.s = None\n        \n    def forward(self, x):\n        self.s = 1/(1 + cp.exp(-x))\n        return self.s\n    \n    def backward(self, grad):\n        return grad * (self.s * (1-self.s))\n\n    \nclass Conv2d(Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, pad=0, eps=1e-4):\n        super(Conv2d, self).__init__(trainable=True)\n        self.ic = in_channels\n        self.oc = out_channels\n        self.k = kernel_size\n        self.s = stride\n        self.p = pad\n        \n        std = 1/(cp.sqrt(self.ic* self.k**2))\n        self.W = cp.random.uniform(-std, std, (self.oc,self.ic,self.k,self.k))\n        self.b = cp.random.uniform(-std, std, (self.oc, 1))\n        \n        self.X_col = None\n        self.x_shape = None\n        \n    def _set_params(self, params):\n        W, b = params\n        self.W = W\n        self.b = b\n        \n    def forward(self, X):\n        NF, CF, HF, WF = self.W.shape\n        NX, DX, HX, WX = X.shape\n        self.x_shape = X.shape\n        h_out = int((HX - HF + 2 * self.p) / self.s + 1)\n        w_out = int((WX - WF + 2 * self.p) / self.s + 1)\n\n        X_col = self.im2col_indices(X)\n        self.X_col = X_col\n        W_col = self.W.reshape(NF, -1)\n\n        out = W_col @ self.X_col + self.b\n        out = out.reshape(NF, h_out, w_out, NX)\n        out = out.transpose(3, 0, 1, 2)\n\n        return out\n\n\n    def backward(self, dout, optimizer=None):\n        NF, CF, HF, WF = self.W.shape\n\n        db = cp.sum(dout, axis=(0, 2, 3))\n        db = db.reshape(NF, -1)\n\n        dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(NF, -1)\n        dW = dout_reshaped @ self.X_col.T\n        dW = dW.reshape(self.W.shape)\n        \n        if optimizer is not None:\n            self.b = optimizer(self.b, db)\n            self.W = optimizer(self.W, dW)\n\n        W_reshape = self.W.reshape(NF, -1)\n        dX_col = W_reshape.T @ dout_reshaped\n        dX = self.col2im_indices(dX_col.astype(cp.float32))\n\n        return dX\n\n    def get_im2col_indices(self):\n        padding, stride, field_height, field_width, x_shape = self.p, self.s, self.k, self.k, self.x_shape\n        N, C, H, W = x_shape\n#         assert (H + 2 * padding - field_height) % stride == 0\n#         assert (W + 2 * padding - field_height) % stride == 0\n        out_height = int((H + 2 * padding - field_height) / stride + 1)\n        out_width = int((W + 2 * padding - field_width) / stride + 1)\n\n        i0 = cp.repeat(cp.arange(field_height), field_width)\n        i0 = cp.tile(i0, C)\n        i1 = stride * cp.repeat(cp.arange(out_height), out_width)\n        j0 = cp.tile(cp.arange(field_width), field_height * C)\n        j1 = stride * cp.tile(cp.arange(out_width), out_height)\n        i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n        j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n\n        k = cp.repeat(cp.arange(C), field_height * field_width).reshape(-1, 1)\n        \n        return (k.astype(cp.int32), i.astype(cp.int32), j.astype(cp.int32))\n\n\n    def im2col_indices(self, x):\n        p, stride, field_height, field_width = self.p, self.s, self.k, self.k\n        x_padded = cp.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n\n        k, i, j = self.get_im2col_indices()\n\n        cols = x_padded[:, k, i, j]\n        C = x.shape[1]\n        cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n        return cols\n\n\n    def col2im_indices(self, cols):\n        field_height, field_width, padding, stride = self.k, self.k, self.p, self.s\n        N, C, H, W = self.x_shape\n        H_padded, W_padded = H + 2 * padding, W + 2 * padding\n        x_padded = cp.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n        k, i, j = self.get_im2col_indices()\n        cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n        cols_reshaped = cols_reshaped.transpose(2, 0, 1).astype(cp.float32)\n        cp.scatter_add(x_padded, (slice(None), k, i, j), cols_reshaped)\n        if padding == 0:\n            return x_padded\n        return x_padded[:, :, padding:-padding, padding:-padding]\n\n    \nclass Softmax(Module):\n    def __init__(self, dim=-1):\n        super(Softmax, self).__init__()\n        self.s = None\n        self.dim = dim\n        self.squeeze_len = None\n        \n    def forward(self, x, dim=None):\n        if dim is not None:\n            self.dim = dim\n        if self.dim < 0:\n            self.dim = len(x.shape)+self.dim\n        self.squeeze_len = x.shape[self.dim]\n        y = cp.exp(x)\n        s = y/cp.sum(y, axis=self.dim, keepdims=True)\n        self.s = s\n        return s\n    \n    def backward(self, grad): \n        self.s = cp.expand_dims(self.s.swapaxes(self.dim,-1), -1)\n        grad = cp.expand_dims(grad.swapaxes(self.dim,-1), -1)\n        mat = self.s @ self.s.swapaxes(-1,-2)\n        mat = (-mat + cp.eye(mat.shape[-1]) * (mat**0.5))\n        grad = mat @ grad\n        self.s = self.s.swapaxes(self.dim,-1).squeeze(-1)\n        return grad.swapaxes(self.dim,-2).squeeze(-1)\n    \n    \nclass Squash(Module):\n    def __init__(self, dim=-1):\n        super(Squash, self).__init__()\n        self.dim = dim\n        self.squeeze_len = None\n        self.s = None\n        \n    def forward(self, s):\n        self.s = s\n        self.squeeze_len = s.shape[self.dim]\n        norm2 = cp.sum((s)**2, axis=self.dim, keepdims=True)\n        return (cp.sqrt(norm2) / (1.0 + norm2)) * s\n        \n    def backward(self, grad):\n        norm2 = cp.sum((self.s)**2, axis=self.dim, keepdims=True)\n        norm = cp.sqrt(norm2)\n        temp = tile((1/(2*(1.+norm2)*norm) - norm/(1.+norm2)**2), self.squeeze_len, self.dim)\n        dnorm2 = cp.sum(self.s * temp, axis=-1, keepdims=True)\n        factor = norm/(1+norm2)\n        return grad * dnorm2 * (2.*self.s) + grad * factor\n\nclass MSELoss(Module):\n    def __init__(self):\n        super(MSELoss, self).__init__()\n        self.x = None\n        self.y = None\n        \n    def forward(self, x, y):\n        self.x = x\n        self.y = y\n        return cp.sum((x - y)**2)/float(x.size), 2*(x - y)/float(x.size)\n\n\nclass PrimaryCaps(Module):\n    def __init__(self, use_cuda=False, out_channels=32, in_channels=256, mapsize=6, ndim=8, kernel_size=9, stride=2, padding=0):\n        super(PrimaryCaps, self).__init__(trainable=True)\n        self.ndim = ndim\n        self.caps = [Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, pad=padding) for _ in\nrange(ndim)]\n        \n        self.out_channels = out_channels\n        self.mapsize = mapsize\n        self.ncaps = out_channels * mapsize**2\n        self.squash = Squash()\n        self.x_size = None\n    \n    def _set_params(self, params):\n        for i, c in enumerate(self.caps):\n            c._set_params(params[i])\n    \n    def forward(self, x):\n        t = time.time()\n        # output (bs, ncaps, ndim)\n        self.x_size = x.shape\n        u = cp.concatenate([cap(x).reshape((x.shape[0], -1, 1)) for cap in self.caps], axis=-1)\n        return self.squash(u)\n    \n    def backward(self, grads, optimizer=None):\n        t = time.time()\n        grads = self.squash.backward(grads)\n        grads = grads.reshape((self.x_size[0],self.out_channels, self.mapsize, self.mapsize,-1))\n        grads = cp.concatenate([cp.expand_dims(self.caps[i].backward(\n            grads[:,:,:,:,i], optimizer=optimizer), -1) for i in range(self.ndim)], axis=-1)\n        out = cp.sum(grads, axis=-1)\n        return out\n     \n        \nclass Decoder(Module):\n    def __init__(self):\n        super(Decoder, self).__init__(trainable=True)\n        self.net = Sequence([\n            Linear(16*10,512),\n            ReLU(),\n            Linear(512,1024),\n            ReLU(),\n            Linear(1024,784),\n            Sigmoid()\n        ])\n        self.x_shape = None\n    \n    def forward(self, x):\n        self.x_shape = x.shape\n        x = x.reshape(x.shape[0],-1)\n        \n        return self.net(x)\n    \n    def _set_params(self, params):\n        for i, l in enumerate(self.net.trainable_modules()):\n            l._set_params(params[i])\n    \n    def backward(self, grad, optimizer):\n        return self.net.backward(grad, optimizer).reshape(self.x_shape)\n    \n\nclass DigitCaps(Module):\n    def __init__(self, ncaps=10, ncaps_prev=32 * 6 * 6, ndim_prev=8, ndim=16):\n        super(DigitCaps, self).__init__(trainable=True)\n        self.ndim_prev = ndim_prev\n        self.ncaps_prev = ncaps_prev\n        self.ncaps = ncaps\n        self.route_iter = 2\n        self.W = cp.random.randn(1, ncaps_prev, ncaps, ndim, ndim_prev)\n        self.softmaxs = [Softmax() for _ in range(self.route_iter)]\n        self.squashs = [Squash() for _ in range(self.route_iter)]\n        self.u_hat = None\n        self.bs = None\n        self.b = [None] * self.route_iter\n        self.v = [None] * self.route_iter\n        self.x = None\n        \n    def _set_params(self, params):\n        self.W = params\n\n    def forward(self, x):\n        t = time.time()\n        self.bs = x.shape[0]\n        self.x = x\n        x = tile(x[:,:,None,:,None], self.ncaps, 2)\n        W = tile(self.W, self.bs, 0)\n        u_hat = W @ x\n        self.u_hat = u_hat\n        b = cp.zeros((1, self.ncaps_prev, self.ncaps, 1, 1))\n\n        for r in range(self.route_iter):\n            self.b[r] = b\n            c = self.softmaxs[r](b, dim=1)\n\n            c = tile(c, self.bs, 0)\n            s = cp.sum(c * u_hat, axis=1, keepdims=True)\n            v = self.squashs[r](s)\n            if r == self.route_iter - 1:\n                return cp.squeeze(v, axis=1)\n            \n            self.v[r] = v\n            p = u_hat.swapaxes(-1, -2) @ tile(v, self.ncaps_prev, 1)\n            b = b + cp.mean(p, axis=0, keepdims=True)\n                \n            \n    def backward(self, grad, optimizer=None):\n        t = time.time()\n        grad_accum = cp.zeros_like(self.u_hat)\n        b_grad_accum = None\n        grad = grad[:,None,:,:,:]\n        for r in range(self.route_iter)[::-1]:\n            if r < self.route_iter-1:\n                grad = b_grad_accum\n                grad = tile(grad, self.bs, 0)/self.bs\n                p_grad = tile(self.v[r], self.ncaps_prev, 1) * grad\n\n                grad_accum += p_grad\n                \n                grad = self.u_hat * grad\n                grad = cp.sum(grad, axis=1, keepdims=True)\n\n            grad = self.squashs[r].backward(grad)\n            grad = tile(grad, self.ncaps_prev, 1)\n            c = self.softmaxs[r].s\n            grad_accum += tile(c, self.bs, 0) * grad\n            grad = self.u_hat.swapaxes(-1,-2) @ grad\n            \n            if r > 0:\n                grad = cp.sum(grad, axis=0, keepdims=True)\n                grad = self.softmaxs[r].backward(grad)\n                if b_grad_accum is None:\n                    b_grad_accum = grad\n                else:\n                    b_grad_accum += grad\n        \n        x = tile(self.x[:,:,None,:,None], self.ncaps, 2)\n        dW = cp.sum(grad_accum @ x.swapaxes(-1,-2), axis=0, keepdims=True)\n        if optimizer is not None:\n            self.W = optimizer(self.W, dW)\n\n        grad_accum = cp.squeeze(self.W.swapaxes(-1,-2) @ grad_accum, axis=-1)\n        dx = cp.sum(grad_accum, axis=2)\n        return dx\n    \n\nclass CapsNet(Module):\n    def __init__(self, use_cuda=False, kernel_size=9, stride=1):       \n        super(CapsNet, self).__init__(trainable=True)\n        self.net = Sequence([\n            Conv2d(1,256,kernel_size=kernel_size,stride=stride),\n            ReLU(),\n            PrimaryCaps(),\n            DigitCaps()\n        ])\n        self.decoder = Decoder()\n        self.x = None\n        self.digit_ndim = 16\n        self.softmax = Softmax()\n        \n    def _set_params(self, params):\n        for i, m in enumerate(self.net.trainable_modules() + [self.decoder]):\n            m._set_params(params)\n        \n    def forward(self, x):\n        x = self.net(x)\n        self.x = x\n        reconst = self.decoder(x)\n        scores = cp.sqrt((x ** 2).sum(2)).squeeze()\n        return scores, reconst\n    \n    def backward(self, grad, optimizer):\n        scores_grad, reconst_grad = grad\n\n        scores_grad = scores_grad[:,:,None, None]\n        t = 0.5 * ((self.x ** 2).sum(2, keepdims=True) ** (-0.5))\n        scores_grad *= 0.5 * ((self.x ** 2).sum(2, keepdims=True) ** (-0.5))\n        scores_grad = tile(scores_grad, self.digit_ndim, 2) # tile at dimension 2\n        scores_grad *= 2*self.x\n        t = time.time()\n\n        reconst_grad = self.decoder.backward(reconst_grad, optimizer)    \n        grad = scores_grad + reconst_grad\n\n        grad = self.net.backward(grad, optimizer=optimizer)\n        return grad\n        \n        \nclass CapsLoss(Module):\n    def __init__(self):\n        super(CapsLoss, self).__init__()\n        self.mse_loss = MSELoss()\n        self.relu1 = ReLU()\n        self.relu2 = ReLU()\n        self.reconst_factor = 0.0005\n        \n        \n    def forward(self, norms, labels, reconst, inpt):\n        self.labels = labels\n\n        int1 = self.relu1(0.9 - norms)\n        int2 = self.relu2(norms - 0.1)\n        margin_loss = labels * int1**2 + 0.5*(1-labels) * int2**2\n        bs, ndim_prev = margin_loss.shape[0], margin_loss.shape[-1]\n        margin_loss = cp.sum(margin_loss, axis=-1).mean()\n        \n        reconst_loss, reconst_grad = self.mse_loss(reconst.reshape(reconst.shape[0],-1), inpt.reshape(inpt.shape[0],-1))\n        loss = margin_loss + self.reconst_factor * reconst_loss\n        \n        margin_grad = cp.ones((bs, ndim_prev)) / float(bs)\n        margin_grad_pos = -self.relu1.backward(margin_grad * labels * (2*int1))\n        margin_grad_neg = self.relu2.backward(margin_grad * 0.5*(1-labels) * (2*int2))\n\n        margin_grad = margin_grad_pos + margin_grad_neg\n        reconst_grad *= self.reconst_factor\n        \n        return loss, (margin_grad, reconst_grad)"""
cupy/optim.py,0,"b'import cupy as cp\n\nclass Optimizer:\n    def __init__(self):\n        self.t = 0\n        \n    def step(self):\n        self.t += 1\n        \n    def update_val(self, x, dx):\n        raise NotImplementedError\n        \n    def __call__(self, *input, **kwargs):\n        return self.update_val(*input, **kwargs)\n        \n        \nclass AdamOptimizer(Optimizer):\n    def __init__(self, lr=1e-2, beta=(0.9,0.999), eps=1e-8):\n        super(AdamOptimizer, self).__init__()\n        self.lr = lr\n        self.beta = beta\n        self.eps = eps\n        self.m = None\n        self.v = None\n    \n    def update_val(self, x, dx):\n        self.m = cp.zeros_like(x)\n        self.v = cp.zeros_like(x)\n        m,v,lr,eps = self.m,self.v,self.lr,self.eps\n        beta1, beta2 = self.beta\n        m = beta1 * m + (1 - beta1) * dx\n        v = beta2 * v + (1 - beta2) * dx**2\n        alpha = lr * cp.sqrt(1 - beta2 ** self.t) / (1 - beta1 ** self.t)\n        x -= alpha * (m / (cp.sqrt(v) + eps))\n        self.m = m\n        self.v = v\n        return x\n        \n        '"
numpy/__init__.py,0,b''
numpy/main.py,11,"b'from modules import *\nimport time, os, argparse\nimport numpy as np\nfrom mnist import MNIST\nfrom modules import CapsNet, CapsLoss\nfrom optim import AdamOptimizer\nimport multiprocessing as mp\n    \ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Cupy Capsnet\')\n    parser.add_argument(\'--bs\', dest=\'bs\',\n                      help=\'batch size\',\n                      default=\'100\', type=int)\n    parser.add_argument(\'--lr\', dest=\'lr\',\n                      help=\'learning rate\',\n                      default=1e-2, type=float)\n    parser.add_argument(\'--opt\', dest=\'opt\',\n                      help=\'optimizer\',\n                      default=\'adam\', type=str)\n    parser.add_argument(\'--disp\', dest=\'disp_interval\',\n                      help=\'interval to display training loss\',\n                      default=\'1\', type=int)\n    parser.add_argument(\'--num_epochs\', dest=\'num_epochs\',\n                      help=\'num epochs to train\',\n                      default=\'100\', type=int)\n    parser.add_argument(\'--val_epoch\', dest=\'val_epoch\',\n                      help=\'num epochs to run validation\',\n                      default=\'1\', type=int)\n    \n    args = parser.parse_args()\n    \n    return args\n\nif __name__ == \'__main__\':\n    mp.set_start_method(\'spawn\')\n    args = parse_args()\n    \n    mnist = MNIST(bs=args.bs, shuffle=True)\n    eye = np.eye(mnist.num_classes)\n    model = CapsNet()\n\n    criterion = CapsLoss()\n    if args.opt == \'adam\':\n        optimizer = AdamOptimizer(lr=args.lr)\n        \n    print(\'Training started!\')\n\n    for epoch in range(args.num_epochs):\n        start = time.time()\n\n        # train\n        correct = 0\n        for batch_idx, (imgs, targets) in enumerate(mnist.train_dataset):\n            optimizer.step()\n            if imgs.shape[0] != args.bs:\n                continue\n\n            targets = eye[targets]\n            scores, reconst = model(imgs)\n            loss, grad = criterion(scores, targets, reconst, imgs)\n            model.backward(grad, optimizer)\n\n            classes = np.argmax(scores, axis=1)\n            predicted = eye[np.squeeze(classes), :]\n\n            predicted_idx = np.argmax(predicted, 1)\n            label_idx = np.argmax(targets, 1)\n            correct = np.sum(predicted_idx == label_idx)\n\n            # info\n            if batch_idx % args.disp_interval == 0:\n                end = time.time()\n                print(""[epoch %2d][iter %4d] loss: %.4f, acc: %.4f%% (%d/%d)"" \\\n                                % (epoch, batch_idx, loss, 100.*correct/args.bs, correct, args.bs))\n\n        # val\n        if epoch % args.val_epoch == 0:\n            print(\'Validating...\')\n            correct = 0\n            total = 0\n\n            for batch_idx, (imgs, targets) in enumerate(mnist.eval_dataset):\n                if imgs.shape[0] != args.bs:\n                    continue\n\n                targets = eye[targets]\n                scores, reconst = model(imgs)\n                classes = np.argmax(scores, axis=1)\n                predicted = eye[np.squeeze(classes, axis=1), :]\n\n                predicted_idx = np.argmax(predicted, 1)\n                label_idx = np.argmax(targets, 1)\n                correct += np.sum(predicted_idx == label_idx)\n                total += targets.shape[0]\n\n            print(""[epoch %2d] val acc: %.4f%% (%d/%d)"" \\\n                                    % (epoch, 100.*correct/total, correct, total))\n'"
numpy/mnist.py,6,"b'import time, os\nimport numpy as np\nfrom urllib import request\nimport gzip\nimport pickle\n\n\nclass MNIST:\n    def __init__(self, path=\'data\', bs=1, shuffle=False):\n        self.filename = [\n        [""training_images"",""train-images-idx3-ubyte.gz""],\n        [""test_images"",""t10k-images-idx3-ubyte.gz""],\n        [""training_labels"",""train-labels-idx1-ubyte.gz""],\n        [""test_labels"",""t10k-labels-idx1-ubyte.gz""]\n        ]\n        self.mean = 0.1307\n        self.std = 0.3081\n        self.num_classes = 10\n        self.bs = bs\n        self.path = path\n        \n        if not os.path.exists(self.path):\n            os.mkdir(self.path)\n        if not os.path.exists(self.path+\'/mnist.pkl\'):\n            self.download_mnist()\n        self.load(shuffle=shuffle)\n        print(\'Loading complete.\')\n        \n    def download_mnist(self):\n        base_url = ""http://yann.lecun.com/exdb/mnist/""\n        for name in self.filename:\n            print(""Downloading ""+name[1]+""..."")\n            request.urlretrieve(base_url+name[1], self.path+\'/\'+name[1])\n        print(""Download complete."")\n        self.save_mnist()\n\n    def save_mnist(self):\n        mnist = {}\n        for name in self.filename[:2]:\n            with gzip.open(self.path+\'/\'+name[1], \'rb\') as f:\n                mnist[name[0]] = ((np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28))/255.-self.mean)/self.std\n        for name in self.filename[-2:]:\n            with gzip.open(self.path+\'/\'+name[1], \'rb\') as f:\n                mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n        with open(self.path+\'/\'+""mnist.pkl"", \'wb\') as f:\n            pickle.dump(mnist,f)\n        print(""Save complete."")\n\n    def chunks(self, l):\n        for i in range(0, len(l), self.bs):\n            yield l[i:i + self.bs]\n\n    def load(self, shuffle=False):\n        with open(self.path+""/mnist.pkl"",\'rb\') as f:\n            mnist = pickle.load(f)\n        if shuffle:\n            n = mnist[\'training_images\'].shape[0]\n            idxs = np.arange(n)\n            np.random.shuffle(idxs)\n            mnist[\'training_images\'] = mnist[\'training_images\'].reshape((-1,1,28,28))\n            mnist[\'training_images\'] = list(self.chunks(mnist[\'training_images\'][idxs]))\n            mnist[\'training_labels\'] = list(self.chunks(mnist[\'training_labels\'][idxs]))\n            self.train_dataset = zip(mnist[\'training_images\'], mnist[\'training_labels\'])\n            \n            n = mnist[\'test_images\'].shape[0]\n            idxs = np.arange(n)\n            np.random.shuffle(idxs)\n            mnist[\'test_images\'] = mnist[\'test_images\'].reshape((-1,1,28,28))\n            mnist[\'test_images\'] = list(self.chunks(mnist[\'test_images\'][idxs]))\n            mnist[\'test_labels\'] = list(self.chunks(mnist[\'test_labels\'][idxs]))\n            self.eval_dataset = zip(mnist[\'test_images\'], mnist[\'test_labels\'])\n\n            '"
numpy/modules.py,52,"b""\n# im2col functions adapted from https://github.com/Burton2000/CS231n-2017/blob/master/assignment2/cs231n/im2col.py\n\nimport numpy as np\nimport time, os\nimport multiprocessing as mp\nfrom functools import partial\n\ndef tile(arr, copy, axis):\n    return np.concatenate([arr] * copy, axis=axis)\n\n        \nclass Module(object):\n    def __init__(self, trainable=False):\n        self.trainable = trainable\n        pass\n    \n    def forward(self, x):\n        raise NotImplementedError\n        \n    def backward(self, grad, optimizer=None):\n        raise NotImplementedError\n        \n    def __call__(self, *input, **kwargs):\n        return self.forward(*input, **kwargs)\n\n    \nclass Sequence(Module):\n    def __init__(self, modules):\n        self._modules = modules\n        \n    def forward(self, inpt):\n        t = time.time()\n        for module in self._modules:\n            inpt = module(inpt)\n            cur = time.time()\n            t = cur\n            if module.trainable:\n                self.trainable = True\n        return inpt\n    \n    def backward(self, grad, optimizer=None):\n        for module in self._modules[::-1]:\n            if module.trainable:\n                grad = module.backward(grad, optimizer)\n            else:\n                grad = module.backward(grad)\n            \n        return grad\n    \n    def modules(self):\n        return self._modules\n    \n    def trainable_modules(self):\n        return [i for i in self._modules if i.trainable]\n\n    \nclass Linear(Module):\n    def __init__(self, in_channel, out_channel):\n        super(Linear, self).__init__(trainable=True)\n        std = 1/np.sqrt(in_channel)\n        self.w = np.random.uniform(-std, std, (out_channel, in_channel))\n        self.b = np.random.uniform(-std, std, (1, out_channel))\n        self.x = None\n        \n    def _set_params(self, params):\n        w, b = params\n        self.w = w\n        self.b = b\n        if len(self.b.shape) < 2:\n            self.b = self.b[None,:]\n        \n    def forward(self, x):\n        out = x.dot(self.w.T) + self.b\n        self.x = x\n        return out\n    \n    def backward(self, grad, optimizer=None):\n        dw = (self.x.T @ grad).T\n        db = np.sum(grad, axis=0, keepdims=True)\n        # update parameters\n        if optimizer is not None:\n            self.w = optimizer(self.w, dw)\n            self.b = optimizer(self.b, db)\n        \n        dx = grad @ self.w\n        dx = np.reshape(dx, self.x.shape)\n        return dx\n   \n\nclass ReLU(Module):\n    def __init__(self, alpha=0):\n        super(ReLU, self).__init__()\n        self.alpha = alpha\n        self.x = None\n        \n    def forward(self, x):\n        out = x.copy()\n        if self.alpha > 0:\n            out[out<0] = self.alpha*x\n        else:\n            out[out<0] = 0\n        self.x = x\n        return out\n    \n    def backward(self, grad):\n        dx = grad.copy()\n        dx[self.x < 0] = 0\n        return dx\n\nclass Sigmoid(Module):\n    def __init__(self):\n        super(Sigmoid, self).__init__()\n        self.s = None\n        \n    def forward(self, x):\n        self.s = 1/(1 + np.exp(-x))\n        return self.s\n    \n    def backward(self, grad):\n        return grad * (self.s * (1-self.s))\n\n    \nclass Conv2d(Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, pad=0, eps=1e-4):\n        super(Conv2d, self).__init__(trainable=True)\n        self.ic = in_channels\n        self.oc = out_channels\n        self.k = kernel_size\n        self.s = stride\n        self.p = pad\n        \n        std = 1/(np.sqrt(self.ic* self.k**2))\n        self.W = np.random.uniform(-std, std, (self.oc,self.ic,self.k,self.k))\n        self.b = np.random.uniform(-std, std, (self.oc, 1))\n        \n        self.X_col = None\n        self.x_shape = None\n        \n    def _set_params(self, params):\n        W, b = params\n        self.W = W\n        self.b = b\n    \n    def _set_input(self, x):\n        self.x_shape = x.shape\n        self.X_col = self.im2col_indices(x)\n        \n    def forward(self, X):\n        NF, CF, HF, WF = self.W.shape\n        NX, DX, HX, WX = X.shape\n        self.x_shape = X.shape\n        h_out = int((HX - HF + 2 * self.p) / self.s + 1)\n        w_out = int((WX - WF + 2 * self.p) / self.s + 1)\n\n        X_col = self.im2col_indices(X)\n        self.X_col = X_col\n        W_col = self.W.reshape(NF, -1)\n\n        out = W_col @ self.X_col + self.b\n        out = out.reshape(NF, h_out, w_out, NX)\n        out = out.transpose(3, 0, 1, 2)\n\n        return out\n\n\n    def backward(self, dout, optimizer=None):\n        NF, CF, HF, WF = self.W.shape\n\n        db = np.sum(dout, axis=(0, 2, 3))\n        db = db.reshape(NF, -1)\n\n        dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(NF, -1)\n        dW = dout_reshaped @ self.X_col.T\n        dW = dW.reshape(self.W.shape)\n        \n        if optimizer is not None:\n            self.b = optimizer(self.b, db)\n            self.W = optimizer(self.W, dW)\n\n        W_reshape = self.W.reshape(NF, -1)\n        dX_col = W_reshape.T @ dout_reshaped\n        dX = self.col2im_indices(dX_col)\n\n        return dX\n\n    def get_im2col_indices(self):\n        padding, stride, field_height, field_width, x_shape = self.p, self.s, self.k, self.k, self.x_shape\n        N, C, H, W = x_shape\n#         assert (H + 2 * padding - field_height) % stride == 0\n#         assert (W + 2 * padding - field_height) % stride == 0\n        out_height = int((H + 2 * padding - field_height) / stride + 1)\n        out_width = int((W + 2 * padding - field_width) / stride + 1)\n\n        i0 = np.repeat(np.arange(field_height), field_width)\n        i0 = np.tile(i0, C)\n        i1 = stride * np.repeat(np.arange(out_height), out_width)\n        j0 = np.tile(np.arange(field_width), field_height * C)\n        j1 = stride * np.tile(np.arange(out_width), out_height)\n        i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n        j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n\n        k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n        \n        return (k.astype(np.int), i.astype(np.int), j.astype(np.int))\n\n\n    def im2col_indices(self, x):\n        p, stride, field_height, field_width = self.p, self.s, self.k, self.k\n        x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n\n        k, i, j = self.get_im2col_indices()\n\n        cols = x_padded[:, k, i, j]\n        C = x.shape[1]\n        cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n        return cols\n\n\n    def col2im_indices(self, cols):\n        field_height, field_width, padding, stride = self.k, self.k, self.p, self.s\n        N, C, H, W = self.x_shape\n        H_padded, W_padded = H + 2 * padding, W + 2 * padding\n        x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n        k, i, j = self.get_im2col_indices()\n        cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n        cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n        np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n        if padding == 0:\n            return x_padded\n        return x_padded[:, :, padding:-padding, padding:-padding]\n\n    \nclass Softmax(Module):\n    def __init__(self, dim=-1):\n        super(Softmax, self).__init__()\n        self.s = None\n        self.dim = dim\n        self.squeeze_len = None\n        \n    def forward(self, x, dim=None):\n        if dim is not None:\n            self.dim = dim\n        if self.dim < 0:\n            self.dim = len(x.shape)+self.dim\n        self.squeeze_len = x.shape[self.dim]\n        y = np.exp(x)\n        s = y/np.sum(y, axis=self.dim, keepdims=True)\n        self.s = s\n        return s\n    \n    def backward(self, grad): \n        self.s = np.expand_dims(self.s.swapaxes(self.dim,-1), -1)\n        grad = np.expand_dims(grad.swapaxes(self.dim,-1), -1)\n        mat = self.s @ self.s.swapaxes(-1,-2)\n        mat = (-mat + np.eye(mat.shape[-1]) * (mat**0.5))\n        grad = mat @ grad\n        self.s = self.s.swapaxes(self.dim,-1).squeeze(-1)\n        return grad.swapaxes(self.dim,-2).squeeze(-1)\n    \n    \nclass Squash(Module):\n    def __init__(self, dim=-1):\n        super(Squash, self).__init__()\n        self.dim = dim\n        self.squeeze_len = None\n        self.s = None\n        \n    def forward(self, s):\n        self.s = s\n        self.squeeze_len = s.shape[self.dim]\n        norm2 = np.sum((s)**2, axis=self.dim, keepdims=True)\n        return (np.sqrt(norm2) / (1.0 + norm2)) * s\n        \n    def backward(self, grad):\n        norm2 = np.sum((self.s)**2, axis=self.dim, keepdims=True)\n        norm = np.sqrt(norm2)\n        temp = tile((1/(2*(1.+norm2)*norm) - norm/(1.+norm2)**2), self.squeeze_len, self.dim)\n        dnorm2 = np.sum(self.s * temp, axis=-1, keepdims=True)\n        factor = norm/(1+norm2)\n        return grad * dnorm2 * (2.*self.s) + grad * factor\n\nclass MSELoss(Module):\n    def __init__(self):\n        super(MSELoss, self).__init__()\n        self.x = None\n        self.y = None\n        \n    def forward(self, x, y):\n        self.x = x\n        self.y = y\n        return np.sum((x - y)**2)/float(x.size), 2*(x - y)/float(x.size)\n\n\nclass PrimaryCaps(Module):\n    def __init__(self, use_cuda=False, out_channels=32, in_channels=256, mapsize=6, ndim=8, kernel_size=9, stride=2, padding=0):\n        super(PrimaryCaps, self).__init__(trainable=True)\n        self.ndim = ndim\n        self.caps = [Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, pad=padding) for _ in\nrange(ndim)]\n        \n        self.out_channels = out_channels\n        self.mapsize = mapsize\n        self.ncaps = out_channels * mapsize**2\n        self.squash = Squash()\n        self.x = None\n    \n    def _set_params(self, params):\n        for i, c in enumerate(self.caps):\n            c._set_params(params[i])\n    \n    def cap_forward(self, i, x):\n        out = self.caps[i](x).reshape((x.shape[0], -1, 1))\n        return out\n    \n    def cap_backward(self, i, grads, x, optimizer):\n        self.caps[i]._set_input(x)\n        out = np.expand_dims(self.caps[i].backward(\n            grads[:,:,:,:,i], optimizer=optimizer), -1)\n        return out\n    \n    def forward(self, x):\n        t = time.time()\n        # output (bs, ncaps, ndim)\n        self.x_size = x.shape\n        self.x = x\n        with mp.Pool() as pool:\n            u = pool.map(partial(self.cap_forward, x=x), np.arange(len(self.caps)))\n        u = np.concatenate(u, axis=-1)\n\n        return self.squash(u)\n    \n    def backward(self, grads, optimizer=None):\n        t = time.time()\n        grads = self.squash.backward(grads)\n        grads = grads.reshape((self.x_size[0],self.out_channels, self.mapsize, self.mapsize,-1))\n        \n        with mp.Pool() as pool:\n            grads = pool.map(partial(self.cap_backward, grads=grads, x=self.x, optimizer=optimizer), np.arange(len(self.caps)))\n        grads = np.concatenate(grads, axis=-1)\n        out = np.sum(grads, axis=-1)\n        \n        return out\n     \n        \nclass Decoder(Module):\n    def __init__(self):\n        super(Decoder, self).__init__(trainable=True)\n        self.net = Sequence([\n            Linear(16*10,512),\n            ReLU(),\n            Linear(512,1024),\n            ReLU(),\n            Linear(1024,784),\n            Sigmoid()\n        ])\n        self.x_shape = None\n    \n    def forward(self, x):\n        self.x_shape = x.shape\n        x = x.reshape(x.shape[0],-1)\n        \n        return self.net(x)\n    \n    def _set_params(self, params):\n        for i, l in enumerate(self.net.trainable_modules()):\n            l._set_params(params[i])\n    \n    def backward(self, grad, optimizer):\n        return self.net.backward(grad, optimizer).reshape(self.x_shape)\n    \n\nclass DigitCaps(Module):\n    def __init__(self, ncaps=10, ncaps_prev=32 * 6 * 6, ndim_prev=8, ndim=16):\n        super(DigitCaps, self).__init__(trainable=True)\n        self.ndim_prev = ndim_prev\n        self.ncaps_prev = ncaps_prev\n        self.ncaps = ncaps\n        self.route_iter = 2\n        self.W = np.random.randn(1, ncaps_prev, ncaps, ndim, ndim_prev)\n        self.softmaxs = [Softmax() for _ in range(self.route_iter)]\n        self.squashs = [Squash() for _ in range(self.route_iter)]\n        self.u_hat = None\n        self.bs = None\n        self.b = [None] * self.route_iter\n        self.v = [None] * self.route_iter\n        self.x = None\n        \n    def _set_params(self, params):\n        self.W = params\n\n    def forward(self, x):\n        t = time.time()\n        self.bs = x.shape[0]\n        self.x = x\n        x = tile(x[:,:,None,:,None], self.ncaps, 2)\n        W = tile(self.W, self.bs, 0)\n        u_hat = W @ x\n        self.u_hat = u_hat\n        b = np.zeros((1, self.ncaps_prev, self.ncaps, 1, 1))\n\n        for r in range(self.route_iter):\n            self.b[r] = b\n            c = self.softmaxs[r](b, dim=1)\n\n            c = tile(c, self.bs, 0)\n            s = np.sum(c * u_hat, axis=1, keepdims=True)\n            v = self.squashs[r](s)\n            if r == self.route_iter - 1:\n                return np.squeeze(v, axis=1)\n            \n            self.v[r] = v\n            p = u_hat.swapaxes(-1, -2) @ tile(v, self.ncaps_prev, 1)\n            b = b + np.mean(p, axis=0, keepdims=True)\n                \n            \n    def backward(self, grad, optimizer=None):\n        t = time.time()\n        grad_accum = np.zeros_like(self.u_hat)\n        b_grad_accum = None\n        grad = grad[:,None,:,:,:]\n        for r in range(self.route_iter)[::-1]:\n            if r < self.route_iter-1:\n                grad = b_grad_accum\n                grad = tile(grad, self.bs, 0)/self.bs\n                p_grad = tile(self.v[r], self.ncaps_prev, 1) * grad\n\n                grad_accum += p_grad\n                \n                grad = self.u_hat * grad\n                grad = np.sum(grad, axis=1, keepdims=True)\n\n            grad = self.squashs[r].backward(grad)\n            grad = tile(grad, self.ncaps_prev, 1)\n            c = self.softmaxs[r].s\n            grad_accum += tile(c, self.bs, 0) * grad\n            grad = self.u_hat.swapaxes(-1,-2) @ grad\n            \n            if r > 0:\n                grad = np.sum(grad, axis=0, keepdims=True)\n                grad = self.softmaxs[r].backward(grad)\n                if b_grad_accum is None:\n                    b_grad_accum = grad\n                else:\n                    b_grad_accum += grad\n        \n        x = tile(self.x[:,:,None,:,None], self.ncaps, 2)\n        dW = np.sum(grad_accum @ x.swapaxes(-1,-2), axis=0, keepdims=True)\n        if optimizer is not None:\n            self.W = optimizer(self.W, dW)\n\n        grad_accum = np.squeeze(self.W.swapaxes(-1,-2) @ grad_accum, axis=-1)\n        dx = np.sum(grad_accum, axis=2)\n        return dx\n    \n\nclass CapsNet(Module):\n    def __init__(self, use_cuda=False, kernel_size=9, stride=1):       \n        super(CapsNet, self).__init__(trainable=True)\n        self.net = Sequence([\n            Conv2d(1,256,kernel_size=kernel_size,stride=stride),\n            ReLU(),\n            PrimaryCaps(),\n            DigitCaps()\n        ])\n        self.decoder = Decoder()\n        self.x = None\n        self.digit_ndim = 16\n        self.softmax = Softmax()\n        \n    def _set_params(self, params):\n        for i, m in enumerate(self.net.trainable_modules() + [self.decoder]):\n            m._set_params(params)\n        \n    def forward(self, x):\n        x = self.net(x)\n        self.x = x\n        reconst = self.decoder(x)\n        scores = np.sqrt((x ** 2).sum(2)).squeeze()\n        return scores, reconst\n    \n    def backward(self, grad, optimizer):\n        scores_grad, reconst_grad = grad\n\n        scores_grad = scores_grad[:,:,None, None]\n        t = 0.5 * ((self.x ** 2).sum(2, keepdims=True) ** (-0.5))\n        scores_grad *= 0.5 * ((self.x ** 2).sum(2, keepdims=True) ** (-0.5))\n        scores_grad = tile(scores_grad, self.digit_ndim, 2) # tile at dimension 2\n        scores_grad *= 2*self.x\n        t = time.time()\n\n        reconst_grad = self.decoder.backward(reconst_grad, optimizer)    \n        grad = scores_grad + reconst_grad\n\n        grad = self.net.backward(grad, optimizer=optimizer)\n        return grad\n        \n        \nclass CapsLoss(Module):\n    def __init__(self):\n        super(CapsLoss, self).__init__()\n        self.mse_loss = MSELoss()\n        self.relu1 = ReLU()\n        self.relu2 = ReLU()\n        self.reconst_factor = 0.0005\n        \n        \n    def forward(self, norms, labels, reconst, inpt):\n        self.labels = labels\n\n        int1 = self.relu1(0.9 - norms)\n        int2 = self.relu2(norms - 0.1)\n        margin_loss = labels * int1**2 + 0.5*(1-labels) * int2**2\n        bs, ndim_prev = margin_loss.shape[0], margin_loss.shape[-1]\n        margin_loss = np.sum(margin_loss, axis=-1).mean()\n        \n        reconst_loss, reconst_grad = self.mse_loss(reconst.reshape(reconst.shape[0],-1), inpt.reshape(inpt.shape[0],-1))\n        loss = margin_loss + self.reconst_factor * reconst_loss\n        \n        margin_grad = np.ones((bs, ndim_prev)) / float(bs)\n        margin_grad_pos = -self.relu1.backward(margin_grad * labels * (2*int1))\n        margin_grad_neg = self.relu2.backward(margin_grad * 0.5*(1-labels) * (2*int2))\n\n        margin_grad = margin_grad_pos + margin_grad_neg\n        reconst_grad *= self.reconst_factor\n        \n        return loss, (margin_grad, reconst_grad)"""
numpy/optim.py,4,"b'import numpy as np\n\nclass Optimizer:\n    def __init__(self):\n        self.t = 0\n        \n    def step(self):\n        self.t += 1\n        \n    def update_val(self, x, dx):\n        raise NotImplementedError\n        \n    def __call__(self, *input, **kwargs):\n        return self.update_val(*input, **kwargs)\n        \n        \nclass AdamOptimizer(Optimizer):\n    def __init__(self, lr=1e-2, beta=(0.9,0.999), eps=1e-8):\n        super(AdamOptimizer, self).__init__()\n        self.lr = lr\n        self.beta = beta\n        self.eps = eps\n        self.m = None\n        self.v = None\n    \n    def update_val(self, x, dx):\n        self.m = np.zeros_like(x)\n        self.v = np.zeros_like(x)\n        m,v,lr,eps = self.m,self.v,self.lr,self.eps\n        beta1, beta2 = self.beta\n        m = beta1 * m + (1 - beta1) * dx\n        v = beta2 * v + (1 - beta2) * dx**2\n        alpha = lr * np.sqrt(1 - beta2 ** self.t) / (1 - beta1 ** self.t)\n        x -= alpha * (m / (np.sqrt(v) + eps))\n        self.m = m\n        self.v = v\n        return x\n        \n        '"
pytorch/__init__.py,0,b' \n'
pytorch/main.py,6,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nimport time, os, argparse\nfrom torch.autograd import Variable\nfrom modules import *\n\n\nclass MNIST:\n    def __init__(self, bs=1):\n        dataset_transform = transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])\n\n        train_dataset = datasets.MNIST(\'data\', train=True, download=True, transform=dataset_transform)\n        eval_dataset = datasets.MNIST(\'data\', train=False, download=True, transform=dataset_transform)\n        \n        self.num_classes = 10\n        self.train_dataloader  = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n        self.eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=bs, shuffle=True)\n\ndef parse_args():\n    """"""\n    Parse input arguments\n    """"""\n    parser = argparse.ArgumentParser(description=\'Cupy Capsnet\')\n    parser.add_argument(\'--bs\', dest=\'bs\',\n                      help=\'batch size\',\n                      default=\'100\', type=int)\n    parser.add_argument(\'--lr\', dest=\'lr\',\n                      help=\'learning rate\',\n                      default=1e-2, type=float)\n    parser.add_argument(\'--opt\', dest=\'optimizer\',\n                      help=\'optimizer\',\n                      default=\'adam\', type=str)\n    parser.add_argument(\'--disp\', dest=\'disp_interval\',\n                      help=\'interval to display training loss\',\n                      default=1, type=int)\n    parser.add_argument(\'--num_epochs\', dest=\'num_epochs\',\n                      help=\'num epochs to train\',\n                      default=100, type=int)\n    parser.add_argument(\'--val_epoch\', dest=\'val_epoch\',\n                      help=\'num epochs to run validation\',\n                      default=1, type=int)\n    parser.add_argument(\'--save_epoch\', dest=\'save_epoch\',\n                      help=\'num epochs to save model\',\n                      default=1, type=int)\n    parser.add_argument(\'--use_cuda\', dest=\'use_cuda\',\n                      help=\'whether or not to use cuda\',\n                      default=True, type=bool)\n    parser.add_argument(\'--save_dir\', dest=\'save_dir\',\n                      help=\'directory to save trained models\',\n                      default=True, type=bool)\n    \n    args = parser.parse_args()\n    \n    return args\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n\n    mnist = MNIST(bs=args.bs)\n     # Variables\n    inputs = torch.FloatTensor(1)\n    labels = torch.FloatTensor(1)\n    eye = Variable(torch.eye(mnist.num_classes))\n    inputs = Variable(inputs)\n    labels = Variable(labels)\n\n    # Model\n    model = CapsNet(use_cuda=args.use_cuda)\n\n    # cuda\n    if args.use_cuda:\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n        model = model.cuda()\n        eye = eye.cuda()\n\n    params = []\n\n    for key, value in dict(model.named_parameters()).items():\n        if value.requires_grad:\n            params += [{\'params\':[value],\'lr\':args.lr}]\n\n    # optimizer\n    if args.optimizer == ""adam"":\n        optimizer = torch.optim.Adam(model.parameters())\n    elif args.optimizer == ""sgd"":\n        optimizer = torch.optim.SGD(params)\n\n    criterion = CapsLoss()\n\n    print(\'Training started!\')\n\n    for epoch in range(args.num_epochs):\n        start = time.time()\n\n        # train\n        model.train()\n        correct = 0\n        train_loss = 0\n        for batch_idx, (imgs, targets) in enumerate(mnist.train_dataloader):\n            if imgs.size(0) != args.bs:\n                continue\n                \n            targets = eye.cpu().data.index_select(dim=0, index=targets)\n            inputs.data.resize_(imgs.size()).copy_(imgs)\n            labels.data.resize_(targets.size()).copy_(targets)\n\n            optimizer.zero_grad()\n            outputs, reconst = model(inputs)\n\n            scores = torch.sqrt((outputs ** 2).sum(2))\n            loss = criterion(scores, labels, reconst, inputs)\n            train_loss = loss.data.cpu().numpy()[0]\n\n            # backward\n            loss.backward()\n            optimizer.step()\n\n            scores, classes = F.softmax(scores).max(dim=1)\n            predicted = eye.index_select(dim=0, index=classes.squeeze(1))\n\n            predicted_idx = np.argmax(predicted.data.cpu().numpy(),1)\n            label_idx = np.argmax(targets.numpy(), 1)\n            correct = np.sum(predicted_idx == label_idx)\n\n            # info\n            if batch_idx % args.disp_interval == 0:\n                end = time.time()\n                print(""[epoch %2d][iter %4d] loss: %.4f, acc: %.4f%% (%d/%d)"" \\\n                                % (epoch, batch_idx, train_loss/(batch_idx+1), 100.*correct/args.bs, correct, args.bs))\n\n        save_name = os.path.join(args.save_dir, \'{}_{}.pth\'.format(project_id, epoch))\n        if args.save_epoch > 0 and batch_idx % args.save_epoch == 0:\n            torch.save({\n              \'epoch\': epoch,\n            }, save_name)\n\n        # val\n        if epoch % args.val_epoch == 0:\n            print(\'Validating...\')\n            correct = 0\n            total = 0\n            model.eval()\n            for batch_idx, (imgs, targets) in enumerate(mnist.eval_dataloader):\n                if imgs.size(0) != args.bs:\n                    continue\n                targets = eye.cpu().data.index_select(dim=0, index=targets)\n                inputs.data.resize_(imgs.size()).copy_(imgs)\n                labels.data.resize_(targets.size()).copy_(targets)\n\n                outputs, reconst = model(inputs)\n                scores = torch.sqrt((outputs ** 2).sum(2))\n                scores, classes = F.softmax(scores).max(dim=1)\n                predicted = eye.index_select(dim=0, index=classes.squeeze(1))\n\n                predicted_idx = np.argmax(predicted.data.cpu().numpy(),1)\n                label_idx = np.argmax(targets.numpy(), 1)\n                correct += np.sum(predicted_idx == label_idx)\n                total += targets.size(0)\n            print(""[epoch %2d] val acc: %.4f%% (%d/%d)"" \\\n                                    % (epoch, 100.*correct/total, correct, total))\n'"
pytorch/modules.py,0,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nimport time, os\nfrom torch.autograd import Variable\n\ndef squash(s, dim=-1):\n    norm2 = torch.sum(s**2, dim=dim, keepdim=True)\n    norm = torch.sqrt(norm2)\n    return (norm2 / (1.0 + norm2)) * (s / norm)\n    \nclass PrimaryCaps(nn.Module):\n    def __init__(self, use_cuda=False, out_channels=32, in_channels=256, ncaps=32*6*6, ndim=8, kernel_size=9, stride=2, padding=0):\n        super(PrimaryCaps, self).__init__()\n        self.ncaps = ncaps\n        self.ndim = ndim\n        self.caps = nn.ModuleList(\n            [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding) for _ in\nrange(ndim)])\n    \n    def forward(self, x):\n        u = torch.cat([cap(x).view(x.size(0), -1, 1) for cap in self.caps], dim=-1)\n        # output (bs, ncaps, ndim)\n        return squash(u)\n\n    \nclass DigitCaps(nn.Module):\n    def __init__(self, use_cuda=False, ncaps=10, ncaps_prev=32 * 6 * 6, ndim_prev=8, ndim=16):\n        super(DigitCaps, self).__init__()\n        self.use_cuda = use_cuda\n        self.ndim_prev = ndim_prev\n        self.ncaps_prev = ncaps_prev\n        self.ncaps = ncaps\n        self.route_iter = 3\n        self.W = nn.Parameter(torch.randn(1, ncaps_prev, ncaps, ndim, ndim_prev))\n\n    def forward(self, x):\n        bs = x.size(0)\n        x = torch.stack([x] * self.ncaps, dim=2).unsqueeze(-1)\n        W = torch.cat([self.W] * bs, dim=0)\n        u_hat = W @ x\n        \n        b = Variable(torch.zeros(1, self.ncaps_prev, self.ncaps, 1))\n        if self.use_cuda:\n            b = b.cuda()\n\n        for i in range(self.route_iter):\n            c = F.softmax(b)\n            c = torch.cat([c] * bs, dim=0).unsqueeze(-1)\n\n            s = (c * u_hat).sum(dim=1, keepdim=True)\n            v = squash(s)\n            \n            if i < self.route_iter - 1:\n                b = b + torch.matmul(u_hat.transpose(-1, -2), torch.cat([v] * self.ncaps_prev, dim=1)) \\\n                    .squeeze(-1).mean(dim=0, keepdim=True)\n                return v.squeeze(1)\n    \n            \nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(16*10,512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512,1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024,784),\n            nn.Sigmoid()\n        )\n    \n    def forward(self,x):\n        x = x.view(x.size(0),-1)\n        x = self.net(x)\n        return x\n    \nclass CapsNet(nn.Module):\n    def __init__(self, use_cuda=False, kernel_size=9, stride=1):\n        super(CapsNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(1,256,kernel_size,stride=stride)\n        self.primary_caps = PrimaryCaps(use_cuda=use_cuda)\n        self.digit_caps = DigitCaps(use_cuda=use_cuda)\n        self.decoder = Decoder()\n        \n    def forward(self, inpt):\n        start = time.time()\n        x = F.relu(self.conv1(inpt), inplace=True)\n        x = self.primary_caps(x)\n        x = self.digit_caps(x)\n        reconst = self.decoder(x)\n        return x, reconst\n\nclass CapsLoss(nn.Module):\n    def __init__(self):\n        super(CapsLoss, self).__init__()\n        self.mse_loss = nn.MSELoss()\n        self.reconst_factor = 0.0005\n    def forward(self, scores, labels, reconst, inpt):\n        norms = torch.sqrt(scores).squeeze()\n        margin_loss = labels * ( F.relu(0.9 - norms, inplace=True) )**2 + 0.5*(1-labels) * ( F.relu(norms - 0.1, inplace=True) )**2\n        margin_loss = margin_loss.sum(dim=-1).mean()\n        reconst_loss = self.mse_loss(reconst.view(reconst.size(0),-1), inpt.view(inpt.size(0),-1))\n        return margin_loss + self.reconst_factor * reconst_loss\n\n'"
