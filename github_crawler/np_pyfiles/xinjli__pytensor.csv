file_path,api_count,code
setup.py,0,"b""from setuptools import setup,find_packages\n\nsetup(\n   name='pytensor',\n   version='0.1.0',\n   description='A deep learning framework using pure numpy',\n   author='Xinjian Li',\n   author_email='lixinjian1217@gmail.com',\n   packages=find_packages(),\n   install_requires=['scikit-learn', 'numpy'], #external packages as dependencies\n)\n"""
pytensor/__init__.py,0,b'from pytensor.network.graph import Graph\nfrom pytensor.network.tensor import *'
pytensor/data/__init__.py,0,b''
pytensor/data/digit_dataset.py,0,"b'from sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\ndef digit_dataset():\n\n    digits = load_digits()\n    digits.data /= 16.0\n    data_train, data_test, label_train, label_test = train_test_split(digits.data, digits.target)\n\n    return data_train, data_test, label_train, label_test\n'"
pytensor/data/ptb.py,0,"b""from pytensor.utils.vocabulary import *\nimport os\n\ndef load_ptb():\n    file_path=os.path.dirname(os.path.abspath(__file__))\n    ptb_file_path = file_path+'/corpus/ptb.train.txt'\n    raw_sentences = open(ptb_file_path, 'r').readlines()\n\n    sentences = []\n    for raw_sentence in raw_sentences:\n        words = raw_sentence.strip().split()\n        sentences.append(words)\n\n    vocab = create_vocabulary(sentences)\n\n    id_sentences = []\n\n    for sentence in sentences:\n        id_sentence = vocab.get_ids(sentence)\n        id_sentences.append(id_sentence)\n\n    return id_sentences, vocab\n"""
pytensor/data/toy_sequence.py,2,"b'import numpy as np\n\n\ndef generate_toy_sequence(num):\n    """"""\n    Generate a toy sequence for debugging\n\n    y = 2*x_1 + 3*x_2 + noise\n\n    :param num: number of dataset\n    :return: x, y\n    """"""\n\n    x = []\n    y = []\n\n    for i in range(num):\n\n        new_x = np.array([[np.random.uniform(), np.random.uniform()]])\n        new_y = np.array([[new_x[0][0]*2 + new_x[0][1]*3 + np.random.normal(0, scale=0.1)]])\n        x.append(new_x)\n        y.append(new_y)\n\n    return x, y\n'"
pytensor/model/__init__.py,0,b''
pytensor/model/linear.py,0,"b'from pytensor.network.graph import Graph\nfrom pytensor.network.tensor import Tensor, LongTensor\n\nclass Linear(Graph):\n\n    def __init__(self, input_size, output_size):\n        super().__init__(""linear"")\n\n        # make graph\n        self.affine = self.get_operation(\'Affine\', {\'input_size\' : input_size, \'hidden_size\': output_size})\n        self.softmaxloss = self.get_operation(\'SoftmaxLoss\')\n\n    def forward(self, input_tensor):\n        affine_tensor = self.affine.forward(input_tensor)\n        return self.softmaxloss.forward(affine_tensor)\n\n    def loss(self, target_tensor):\n        return self.softmaxloss.loss(target_tensor)\n'"
pytensor/model/lstm.py,0,"b'from pytensor import *\n\nclass LSTMClassifier(Graph):\n\n    def __init__(self, vocab_size, input_size, hidden_size):\n        """"""\n        RNN classification\n\n        :param vocab_size:\n        :param input_size:\n        :param hidden_size:\n        """"""\n\n        super().__init__(\'LSTMClassifier\')\n\n        # embedding size\n        self.vocab_size = vocab_size\n        self.word_dim = input_size\n\n        # network size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = vocab_size\n\n        # num steps\n        self.max_num_steps = 100\n        self.num_steps = 0\n\n        # word embedding\n        embed_argument = {\'vocab_size\': self.vocab_size, \'embed_size\': self.input_size}\n        self.word_embedding = self.get_operation(\'Embedding\', embed_argument)\n\n        # rnn\n        rnn_argument = {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, \'max_num_steps\': self.max_num_steps}\n        self.lstm = self.get_operation(\'LSTM\', rnn_argument)\n\n        # affines\n        self.affine = self.get_operation(\'Affine\', {\'input_size\': self.hidden_size, \'hidden_size\': self.output_size}, ""Affine"")\n\n        # softmax\n        self.softmaxLoss = self.get_operation(\'SoftmaxLoss\')\n\n    def forward(self, word_lst):\n\n        # get num steps\n        self.num_steps = min(len(word_lst), self.max_num_steps)\n\n        # create embeddings\n        self.embedding_tensors = []\n        for word_id in word_lst:\n            self.embedding_tensors.append(self.word_embedding.forward([LongTensor([word_id])]))\n\n        # run RNN\n        self.rnn_tensors = self.lstm.forward(self.embedding_tensors)\n\n        # affine\n        self.output_tensor = self.affine.forward(self.rnn_tensors[-1])\n\n        self.softmax_tensor = self.softmaxLoss.forward(self.output_tensor)\n\n        return self.softmax_tensor\n\n    def loss(self, target_id):\n\n        ce_loss = self.softmaxLoss.loss(LongTensor([target_id]))\n        return ce_loss\n\n\n\nclass LSTMLM(Graph):\n\n    def __init__(self, vocab_size, input_size, hidden_size):\n        super().__init__(""LSTM"")\n\n        # embedding size\n        self.vocab_size = vocab_size\n        self.word_dim = input_size\n\n        # network size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = vocab_size\n\n        # num steps\n        self.max_num_steps = 100\n        self.num_steps = 0\n\n        # word embedding\n        embed_argument = {\'vocab_size\': self.vocab_size, \'embed_size\': self.input_size}\n        self.word_embedding = self.get_operation(\'Embedding\', embed_argument)\n\n        # rnn\n        rnn_argument = {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, \'max_num_steps\': self.max_num_steps}\n        self.rnn = self.get_operation(\'LSTM\', rnn_argument)\n\n        # affines\n        affine_argument = {\'input_size\': self.hidden_size, \'hidden_size\': self.output_size}\n        self.affines = [self.get_operation(\'Affine\', affine_argument, ""Affine"") for i in range(self.max_num_steps)]\n\n        # softmax\n        self.softmaxLosses = [self.get_operation(\'SoftmaxLoss\') for i in range(self.max_num_steps)]\n\n    def forward(self, word_lst):\n\n        # get num steps\n        self.num_steps = min(len(word_lst), self.max_num_steps)\n\n        # create embeddings\n        embedding_tensors = []\n        for word_id in word_lst:\n            embedding_tensors.append(self.word_embedding.forward([LongTensor([word_id])]))\n\n        # run RNN\n        rnn_tensors = self.rnn.forward(embedding_tensors)\n\n        # softmax tensors\n        softmax_tensors = []\n\n        for i in range(self.num_steps):\n            output_tensor = self.affines[i].forward(rnn_tensors[i])\n            softmax_tensor = self.softmaxLosses[i].forward(output_tensor)\n            softmax_tensors.append(softmax_tensor)\n\n        return softmax_tensors\n\n    def loss(self, target_ids):\n\n        ce_loss = 0.0\n\n        for i in range(self.num_steps):\n            cur_ce_loss = self.softmaxLosses[i].loss(LongTensor([target_ids[i]]))\n            ce_loss += cur_ce_loss\n\n        return ce_loss\n\n'"
pytensor/model/mlp.py,0,"b'from pytensor import *\n\nclass MLP(Graph):\n\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__(""mlp"")\n\n        # make graph\n        self.affine1 = self.get_operation(\'Affine\', {\'input_size\': input_size, \'hidden_size\': hidden_size})\n        self.sigmoid = self.get_operation(\'Sigmoid\')\n        self.affine2 = self.get_operation(\'Affine\', {\'input_size\': hidden_size, \'hidden_size\': output_size})\n        self.softmaxloss = self.get_operation(\'SoftmaxLoss\')\n\n    def forward(self, input_tensor):\n        affine1_tensor = self.affine1.forward(input_tensor)\n        sigmoid_tensor = self.sigmoid.forward(affine1_tensor)\n        affine2_tensor = self.affine2.forward(sigmoid_tensor)\n\n        return self.softmaxloss.forward(affine2_tensor)\n\n    def loss(self, target_tensor):\n        return self.softmaxloss.loss(target_tensor)\n'"
pytensor/model/rnn.py,0,"b'from pytensor import *\n\nclass RNNClassifier(Graph):\n\n    def __init__(self, vocab_size, input_size, hidden_size):\n        """"""\n        RNN classification\n\n        :param vocab_size:\n        :param input_size:\n        :param hidden_size:\n        """"""\n\n        super().__init__(\'RNNClassifier\')\n\n        # embedding size\n        self.vocab_size = vocab_size\n        self.word_dim = input_size\n\n        # network size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = vocab_size\n\n        # num steps\n        self.max_num_steps = 100\n        self.num_steps = 0\n\n        # word embedding\n        embed_argument = {\'vocab_size\': self.vocab_size, \'embed_size\': self.input_size}\n        self.word_embedding = self.get_operation(\'Embedding\', embed_argument)\n\n        # rnn\n        rnn_argument = {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, \'max_num_steps\': self.max_num_steps}\n        self.rnn = self.get_operation(\'RNN\', rnn_argument)\n\n        # affines\n        self.affine = self.get_operation(\'Affine\', {\'input_size\': self.hidden_size, \'hidden_size\': self.output_size}, ""Affine"")\n\n        # softmax\n        self.softmaxLoss = self.get_operation(\'SoftmaxLoss\')\n\n    def forward(self, word_lst):\n\n        # get num steps\n        self.num_steps = min(len(word_lst), self.max_num_steps)\n\n        # create embeddings\n        self.embedding_tensors = []\n        for word_id in word_lst:\n            self.embedding_tensors.append(self.word_embedding.forward([LongTensor([word_id])]))\n\n        # run RNN\n        self.rnn_tensors = self.rnn.forward(self.embedding_tensors)\n\n        # affine\n        self.output_tensor = self.affine.forward(self.rnn_tensors[-1])\n\n        self.softmax_tensor = self.softmaxLoss.forward(self.output_tensor)\n\n        return self.softmax_tensor\n\n    def loss(self, target_id):\n\n        ce_loss = self.softmaxLoss.loss(LongTensor([target_id]))\n        return ce_loss\n\n\nclass RNNLM(Graph):\n\n    def __init__(self, vocab_size, input_size, hidden_size):\n        super().__init__(\'RNN\')\n\n        # embedding size\n        self.vocab_size = vocab_size\n        self.word_dim = input_size\n\n        # network size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = vocab_size\n\n        # num steps\n        self.max_num_steps = 100\n        self.num_steps = 0\n\n        # word embedding\n        embed_argument = {\'vocab_size\': self.vocab_size, \'embed_size\': self.input_size}\n        self.word_embedding = self.get_operation(\'Embedding\', embed_argument)\n\n        # rnn\n        rnn_argument = {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, \'max_num_steps\': self.max_num_steps}\n        self.rnn = self.get_operation(\'RNN\', rnn_argument)\n\n        # affines\n        affine_argument = {\'input_size\': self.hidden_size, \'hidden_size\': self.output_size}\n        self.affines = [self.get_operation(\'Affine\', affine_argument, ""Affine"") for i in range(self.max_num_steps)]\n\n        # softmax\n        self.softmaxLosses = [self.get_operation(\'SoftmaxLoss\') for i in range(self.max_num_steps)]\n\n    def forward(self, word_lst):\n\n        # get num steps\n        self.num_steps = min(len(word_lst), self.max_num_steps)\n\n        # create embeddings\n        embedding_tensors = []\n        for word_id in word_lst:\n            embedding_tensors.append(self.word_embedding.forward([LongTensor([word_id])]))\n\n        # run RNN\n        rnn_tensors = self.rnn.forward(embedding_tensors)\n\n        # softmax tensors\n        softmax_tensors = []\n\n        for i in range(self.num_steps):\n            output_tensor = self.affines[i].forward(rnn_tensors[i])\n            softmax_tensor = self.softmaxLosses[i].forward(output_tensor)\n            softmax_tensors.append(softmax_tensor)\n\n        return softmax_tensors\n\n    def loss(self, target_ids):\n\n        ce_loss = 0.0\n\n        for i in range(self.num_steps):\n            cur_ce_loss = self.softmaxLosses[i].loss(LongTensor([target_ids[i]]))\n            ce_loss += cur_ce_loss\n\n        return ce_loss'"
pytensor/network/__init__.py,0,b''
pytensor/network/functions.py,10,"b'# coding: utf-8\nimport numpy as np\n\n\ndef identity_function(x):\n    return x\n\n\ndef step_function(x):\n    return np.array(x > 0, dtype=np.int)\n\n\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\n\ndef sigmoid_grad(x):\n    return (1.0 - sigmoid(x)) * sigmoid(x)\n\n\ndef relu(x):\n    return np.maximum(0, x)\n\n\ndef relu_grad(x):\n    grad = np.zeros(x)\n    grad[x >= 0] = 1\n    return grad\n\n\ndef softmax(x):\n    if x.ndim == 2:\n        x = x.T\n        x = x - np.max(x, axis=0)\n        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n        return y.T\n\n    x = x - np.max(x) # for preventing overflow\n    return np.exp(x) / np.sum(np.exp(x))\n\n\ndef mean_squared_error(y, t):\n    """"""\n    mean square error\n\n    :param y:\n    :param t:\n    :return:\n    """"""\n    return 0.5 * np.sum((y - t) ** 2)\n\n\ndef cross_entropy_error(y, t):\n\n    # add dimension if it is not batch\n    #if y.ndim == 1:\n    #    y = y.reshape(1, y.size)\n\n    batch_size = y.shape[0]\n\n    return -np.sum(np.log2(y[np.arange(batch_size), t]))\n\n\ndef softmax_loss(X, t):\n    y = softmax(X)\n    return cross_entropy_error(y, t)\n'"
pytensor/network/gradient.py,3,"b'import numpy as np\n\ndef numerical_gradient_check(model, input_tensors, target_tensor):\n    """"""\n    validate numerical gradient with respect to model\n\n    :param model: the model we want to validate\n    :param input_tensors: input tensors for gradient check\n    :param target_tensor: target tensor for gradient check\n    :return:\n    """"""\n\n    # clear gradient before validation\n    model.graph.parameter.clear_grads()\n\n    # run normal procedures to compute automatic gradient\n    model.forward(input_tensors)\n    model.loss(target_tensor)\n    model.graph.backward()\n\n    # tensors we need to check\n    tensor_dict = model.graph.parameter.tensor_dict\n\n    for var_name, var in tensor_dict.items():\n        print(""Now checking "", var)\n\n        h = 1e-4\n        v = var.value\n\n        numerical_grad = np.zeros_like(v)\n\n        # compute numerical gradient of this tensor\n        it = np.nditer(v, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n        while not it.finished:\n            idx = it.multi_index\n            tmp_val = v[idx]\n\n            # f(x+h)\n            v[idx] = float(tmp_val) + h\n            model.forward(input_tensors)\n            loss_1 = model.loss(target_tensor)\n\n            # f(x-h)\n            v[idx] = tmp_val - h\n            model.forward(input_tensors)\n            loss_2 = model.loss(target_tensor)\n\n            numerical_grad[idx] = (loss_1 - loss_2) / (2 * h)\n\n            v[idx] = tmp_val\n            it.iternext()\n\n            # clear ops\n            model.graph.clear()\n\n        # compare numerical grad with auto grad\n        diff = np.sum(var.grad - numerical_grad)\n\n        if diff < 0.001:\n            print(var_name, "" match"")\n        else:\n            print(var_name, "" NOT MATCH !"")\n            print(""Auto grad\\n"", var.grad)\n            print(""Numerical grad\\n"", numerical_grad)'"
pytensor/network/graph.py,0,"b'from pytensor.network.optimizer import *\nfrom collections import defaultdict\n\nfrom pytensor.ops import *\n\n\nclass Graph:\n\n    def __init__(self, name=\'graph\'):\n        """"""\n        graph is a data structure to manage parameter and execution order\n\n        """"""\n\n        # graph name\n        self.name = name\n\n        # state\n        self.train_state = True\n\n        # forward operation stack\n        self.forward_ops = []\n\n        # parameter and optimizer\n        self.parameter = Parameter()\n\n        # index for different operations\n        self.ops_index = defaultdict(int)\n\n    def get_operation(self, ops_type, ops_argument=None, ops_name=None):\n        """"""\n        create operation\n\n        :return:\n\n        """"""\n        # generate the default name\n        if ops_name is None:\n            ops_name = ops_type.lower()+""_""+str(self.ops_index[ops_type])\n\n        # increment index\n        self.ops_index[ops_type] += 1\n\n        # reflection\n        cls = globals()[ops_type]\n        ops = cls(ops_name, ops_argument, self)\n\n        return ops\n\n    def get_tensor(self, name, shape):\n        return self.parameter.get_tensor(name, shape)\n\n\n    def train(self):\n        """"""\n        set the state to the training state.\n        In the training state, the forward operation stack will be enabled for backprop.\n        operations such as dropout and batch norm will be enabled\n\n        :return:\n        """"""\n        self.train_state = True\n\n    def eval(self):\n        """"""\n        set the state to the eval state\n\n        :return:\n        """"""\n\n        self.train_state = False\n\n    def forward(self, input_tensors):\n        """"""\n        input the forward function for the model\n\n        :param input_tensors:\n        :return:\n        """"""\n\n        return NotImplementedError\n\n    def loss(self, target_tensors):\n        """"""\n        implement the loss function for the model\n\n        :param target_tensors:\n        :return:\n        """"""\n\n        return NotImplementedError\n\n    def register(self, ops):\n        """"""\n        register a forward ops to the stack (for backward computation)\n\n        :param ops:\n        :return:\n        """"""\n\n        if self.train_state:\n            self.forward_ops.append(ops)\n\n\n    def clear(self):\n        """"""\n        clear operation stack\n\n        :return:\n        """"""\n\n        self.forward_ops = []\n\n\n    def backward(self):\n        """"""\n        backward the error in reverse topological order\n\n        :return:\n        """"""\n\n        for ops in reversed(self.forward_ops):\n            ops.backward()\n\n        # clear all operation\n        self.clear()'"
pytensor/network/operation.py,0,"b'from pytensor.network.tensor import *\nfrom pytensor.network.functions import *\nfrom pytensor.network.parameter import *\n\nclass Operation:\n    def __init__(self, name, arguments, graph):\n\n        self.name = name\n        self.graph = graph\n        self.arguments = arguments\n\n    def __call__(self, input_tensors):\n        """"""\n        shortcut method of forward\n\n        :param input_tensors:\n        :return:\n        """"""\n        return self.forward(input_tensors)\n\n\n    def register(self, input_tensors):\n        """"""\n        forward computation\n\n        :param input_tensors: a list of input tensors\n        """"""\n\n        self.input_tensors = input_tensors\n\n        # register the ops\n        if self.graph is not None:\n            self.graph.register(self)\n\n    def forward(self, input_tensors):\n\n        raise NotImplementedError\n\n\n    def backward(self):\n        """"""\n        backprop loss and update\n\n        :return:\n        """"""\n        raise NotImplementedError'"
pytensor/network/optimizer.py,0,"b'# coding: utf-8\nimport numpy as np\nimport logging\nfrom pytensor.network.parameter import *\n\nclass Optimizer:\n\n    def step(self):\n        raise NotImplementedError\n\n    def zero_grad(self):\n        raise NotImplementedError\n\n\nclass SGD(Optimizer):\n\n    def __init__(self, parameter, lr=0.01):\n        self.parameter = parameter\n        self.lr = lr\n\n    def step(self):\n\n        for param_name in self.parameter.tensor_dict.keys():\n            # update param value\n            param = self.parameter.tensor_dict[param_name]\n\n            # update\n            if param.trainable:\n                param.value -= self.lr * param.grad\n\n        for temp_tensor in self.parameter.temp_tensors:\n\n            # update temp_tensor\n            if temp_tensor:\n                temp_tensor.value -= self.lr * temp_tensor.grad\n\n    def zero_grad(self):\n\n        # clear all gradients\n        self.parameter.clear_grads()\n\n        # clear temp tensors\n        self.parameter.clear_temp_tensor()\n\n'"
pytensor/network/parameter.py,3,"b'from pytensor.network.tensor import *\nimport numpy as np\nimport pickle\n\ndef write_parameter(parameter, path):\n    """"""\n    save the parameter\n\n    :param parameter: a Parameter instance\n    :param path: path to save\n    :return:\n    """"""\n\n    np.save(str(path), parameter.tensor_dict)\n\n\ndef read_parameter(path):\n    """"""\n    load the parameter from disk\n\n    :param path: path to a npy file\n    :return: Parameter instance\n    """"""\n\n    parameter = Parameter()\n\n    # setup parameter\n    parameter.tensor_dict = np.load(str(path))[()]\n\n    return parameter\n\n\nclass Parameter:\n    """"""\n    Parameter is a structure to manage all trainable tensors in the graph.\n\n    Each trainable tensor should be initialized using Parameter\n    """"""\n\n    def __init__(self):\n\n        # a dictionary mapping names to tensors\n        self.tensor_dict = dict()\n\n        # embedding for partial updates\n        self.embeddings = None\n        self.temp_tensors = []\n\n    def get_tensor(self, name, shape):\n        """"""\n        retrieve a tensor with its name\n\n        :param name: name of the tensor\n        :param shape: desired shape\n        :return:\n        """"""\n\n        if name in self.tensor_dict:\n            # if the tensor exists in the dictionary,\n            # retrieve it directly\n            return self.tensor_dict[name]\n        else:\n            # if not created yet, initialize a new tensor for it\n            value = np.random.standard_normal(shape) / np.sqrt(shape[0])\n            tensor = Tensor(value, name=name)\n\n            # register the tensor\n            self.tensor_dict[name] = tensor\n\n            return tensor\n\n    def get_embedding(self, name, shape):\n        """"""\n        retrieve a embedding with its name\n\n        :param name: name for embeding\n        :param shape: (vocab_size, word_dim)\n        :return:\n        """"""\n\n        # get current embedding if it is created already\n        if self.embeddings != None:\n            return self.embeddings\n\n        # shape should be (vocab_size, word_dim)\n        assert len(shape) == 2\n        vocab_size, word_dim = shape\n\n        # get tensor for embedding\n        embedding_tensor = self.get_tensor(name, shape)\n\n        # initialize the embedding\n        self.embeddings = []\n\n        # embedding is implemented as a list of tensors\n        # this is for efficient update\n        for i in range(vocab_size):\n\n            # create reference to embedding tensor\n            value = embedding_tensor.value[i].reshape(1,word_dim)\n            grad = embedding_tensor.grad[i].reshape(1, word_dim)\n\n            embedding = Tensor(value=value, grad=grad)\n            self.embeddings.append(embedding)\n\n        return self.embeddings\n\n\n    def add_temp_tensor(self, temp_tensor):\n        """"""\n        register temporary tensor for optimizer to update tensor\n        this is mainly for word embedding training\n\n        :param temp_tensor: a trainable tensor (usually a tensor in the embedding)\n        :return:\n        """"""\n\n        self.temp_tensors.append(temp_tensor)\n\n\n    def clear_temp_tensor(self):\n        """"""\n        clear temporary tensor\n\n        :return:\n        """"""\n        self.temp_tensors = []\n\n\n    def clear_grads(self):\n        """"""\n        clear gradients of all tensors\n\n        :return:\n        """"""\n\n        for k, v in self.tensor_dict.items():\n            v.clear_grad()\n\n        for v in self.temp_tensors:\n            v.clear_grad()\n'"
pytensor/network/tensor.py,4,"b'import numpy as np\n\nclass LongTensor:\n    """"""\n    LongTensor is a type of Tensor to keep integers\n\n    """"""\n\n    def __init__(self, value, name=\'LongTensor\', trainable=False):\n        """"""\n        :param value: long value\n        :param name:\n        :param trainable:\n        """"""\n\n        self.value = np.array(value, dtype=np.int32)\n        self.name = name\n\n    def clear_grad(self):\n        return\n\n    def reshape(self, array):\n        return\n\n\nclass Tensor:\n    """"""\n    Tensor is the basic structure in the computation graph\n    It holds value for forward computation and grad for backward propagation\n\n    """"""\n\n    def __init__(self, value, name=\'Tensor\', dtype=np.float32, trainable=True, grad=None):\n        """"""\n        :param value: numpy val\n        :param name: name for the Tensor\n        :param trainable: whether the Tensor can be trained or not\n        """"""\n\n        # value for forward computation\n        if isinstance(value, list):\n            self.value = np.array(value, dtype=dtype)\n        else:\n            self.value = value\n\n        # value for backward computation\n        if grad is not None:\n            self.grad = grad\n        else:\n            self.grad = np.zeros(self.value.shape, dtype=np.float32)\n\n        # name for the Tensor (which will used in parameter for registration)\n        self.name = name\n\n        # whether the Tensor can be updated\n        self.trainable = trainable\n\n    def __str__(self):\n        return ""Tensor {name: ""+self.name+""}\\n- value    : ""+str(self.value)+""\\n- gradient : ""+str(self.grad)+""""\n\n    def __repr__(self):\n        return self.__str__()\n\n    def clear_grad(self):\n        self.grad.fill(0.0)\n\n    def reshape(self, array):\n        self.value.reshape(array)\n        self.grad.reshape(array)\n'"
pytensor/network/trainer.py,1,"b'from pytensor.network.graph import *\nfrom pytensor.network.gradient import *\nfrom pytensor.network.optimizer import *\n\n\nclass Trainer:\n\n    def __init__(self, model):\n        """"""\n        A trainer example using graph for autodiff\n        :param model:\n        """"""\n\n        self.model = model\n        self.optimizer = SGD(self.model.parameter)\n\n    def train(self, x_train, y_train, x_test=None, y_test=None, epoch=40):\n\n        for ii in range(epoch):\n\n            self.model.train()\n\n            loss = 0.0\n\n            for i in range(len(x_train)):\n\n                # clear grad\n                self.optimizer.zero_grad()\n\n                # extract data set\n                input_tensors = Tensor([x_train[i]])\n                target_tensor = Tensor([y_train[i]])\n\n                # dynamic forward\n                self.model.forward(input_tensors)\n\n                # loss\n                loss += self.model.loss(target_tensor)\n\n                # automatic differentiation\n                self.model.backward()\n\n                # optimization\n                self.optimizer.step()\n\n            accuracy = self.test(x_test, y_test)\n            print(""\\repoch {}: loss {}, acc {}"".format(ii, loss, accuracy), end=\'\')\n\n    def test(self, x_test, y_test):\n\n        self.model.eval()\n\n        acc_cnt = 0.0\n        all_cnt = len(x_test)\n\n        for i in range(len(x_test)):\n\n            v = Tensor([x_test[i]])\n            output_tensor = self.model.forward(v)\n\n            y = np.argmax(output_tensor.value[0])\n            if y == y_test[i]:\n                acc_cnt += 1.0\n\n        return acc_cnt / all_cnt'"
pytensor/ops/__init__.py,0,b'from pytensor.ops.array_ops import *\nfrom pytensor.ops.embedding_ops import *\nfrom pytensor.ops.loss_ops import *\nfrom pytensor.ops.lstm_ops import *\nfrom pytensor.ops.math_ops import *\nfrom pytensor.ops.rnn_ops import *\nfrom pytensor.ops.rnn_util_ops import *'
pytensor/ops/array_ops.py,5,"b'from pytensor.network.operation import *\n\nclass HStack:\n\n    def __init__(self, name=""HStack""):\n        self.name = name\n\n    def forward(self, input_tensors):\n        """"""\n        Hstack  tensors into one tensor\n\n        For instance, it can hstack [1,2], [3,4] into [1,2,3,4]\n\n        :param input_tensors:\n        :return:\n        """"""\n\n        self.input_tensors = input_tensors\n        self.input_cnt = len(self.input_tensors)\n        self.output_tensor = Tensor(np.hstack([input_tensor.value for input_tensor in self.input_tensors]))\n\n        return self.output_tensor\n\n    def backward(self):\n\n        """"""\n        backward output grad into each grad\n        :return:\n        """"""\n        grads = np.hsplit(self.output_tensor, self.input_cnt)\n\n        for i, grad in enumerate(grads):\n            self.input_tensors[i].grad += grad\n\n\nclass VStack:\n\n    def __init__(self, name=""VStack""):\n        self.name = name\n\n    def forward(self, input_tensors):\n        """"""\n        VStack tensors into one tensor\n\n        :param input_tensors:\n        :return:\n        """"""\n        self.input_tensors = input_tensors\n        self.input_cnt = len(self.input_tensors)\n        self.output_tensor = Tensor(np.vstack([input_tensor.value for input_tensor in self.input_tensors]))\n\n        return self.output_tensor\n\n    def backward(self):\n        """"""\n        :return:\n        """"""\n\n        grads = np.vsplit(self.output_tensor.grad, self.input_cnt)\n\n        for i, grad in enumerate(grads):\n            self.input_tensors[i].grad += np.squeeze(grad)\n'"
pytensor/ops/embedding_ops.py,0,"b'from pytensor.network.operation import *\n\nclass Embedding(Operation):\n\n    def __init__(self, name=\'embedding\', argument=None, graph=None):\n        """"""\n        :param name:\n        :param argument:\n        - vocab_size: vocabulary size\n        - embed_size: embedding size\n        - trainable: whether the embedding can be trained\n\n        :param graph:\n        """"""\n\n        super(Embedding, self).__init__(name, argument, graph)\n\n        self.vocab_size = argument[\'vocab_size\']\n        self.embed_dim = argument[\'embed_size\']\n\n        if \'trainable\' in argument:\n            self.trainable = argument[\'trainable\']\n        else:\n            self.trainable = True\n\n        self.embedding_tensors = self.graph.parameter.get_embedding(name, (self.vocab_size, self.embed_dim))\n\n    def forward(self, input_tensors):\n        """"""\n        get the embedding\n\n        :param input_tensors: input tensor is a LongTensor containing word ids\n        :return: embedding\n        """"""\n        self.register(input_tensors)\n\n        # embedding only takes 1 input tensor\n        assert(len(input_tensors) == 1)\n\n        word_id = input_tensors[0].value[0]\n\n        assert (word_id < self.vocab_size)\n        output_tensor = self.embedding_tensors[word_id]\n\n        if self.trainable:\n            self.graph.parameter.add_temp_tensor(output_tensor)\n\n        return output_tensor\n\n    def backward(self):\n        return'"
pytensor/ops/loss_ops.py,1,"b'from pytensor.network.operation import *\n\nclass Loss(Operation):\n    def loss(self, target_tensor):\n        raise NotImplementedError\n\n\nclass SoftmaxLoss(Loss):\n    def __init__(self, name=""SoftmaxWithLoss"", argument=None, graph=None):\n        super(SoftmaxLoss, self).__init__(name, argument, graph)\n\n    def forward(self, input_tensors):\n        self.register(input_tensors)\n\n        out_value = softmax(self.input_tensors.value)\n        self.output_tensor = Tensor(out_value)\n\n        return self.output_tensor\n\n    def loss(self, target_tensor):\n\n        self.target_tensor = target_tensor\n        self.batch_size = len(self.target_tensor.value)\n\n        self.error = cross_entropy_error(self.output_tensor.value, target_tensor.value)\n        return self.error\n\n    def backward(self):\n\n        self.input_tensors.grad = self.output_tensor.value.copy()\n        self.input_tensors.grad[np.arange(self.batch_size), self.target_tensor.value] -= 1.0\n\n        self.input_tensors.grad *= 1.4426950408889634 # log2(e)\n\n\nclass SquareLoss(Loss):\n\n    def __init__(self, name=""SquareLoss"", argument=None, graph=None):\n        super(SquareLoss, self).__init__(name, argument, graph)\n\n    def forward(self, input_tensors):\n        self.register(input_tensors)\n\n        out_value = softmax(self.input_tensors.value)\n        self.output_tensor = Tensor(out_value)\n\n        return self.output_tensor\n\n    def loss(self, target_tensor):\n        self.target_tensor = target_tensor\n        loss_val =  mean_squared_error(self.input_tensors.value, self.target_tensor.value)\n        return loss_val\n\n    def backward(self):\n        # update grad\n        self.input_tensors.grad = self.input_tensors.value - self.target_tensor.value\n'"
pytensor/ops/lstm_ops.py,24,"b'from pytensor.network.tensor import *\nfrom pytensor.network.parameter import *\nfrom pytensor.network.operation import *\nfrom pytensor.ops.math_ops import *\n\nclass RawLSTMCell(Operation):\n\n    def __init__(self,  name=\'RawLSTMCell\', argument=None, graph=None):\n\n        super(RawLSTMCell, self).__init__(name, argument, graph)\n\n        # intialize size\n        self.input_size = argument[\'input_size\']\n        self.hidden_size = argument[\'hidden_size\']\n\n        # batch size\n        self.batch_size = 1\n\n        # create tensors\n        # forget gate weight\n        self.Wf_h = graph.get_tensor(name+""LSTMCell_Wf_h"", (self.hidden_size, self.hidden_size))\n        self.Wf_i = graph.get_tensor(name+""LSTMCell_Wf_i"", (self.input_size, self.hidden_size))\n\n        # input gate weight\n        self.Wi_h = graph.get_tensor(name+""LSTMCell_Wi_h"", (self.hidden_size, self.hidden_size))\n        self.Wi_i = graph.get_tensor(name+""LSTMCell_Wi_i"", (self.input_size, self.hidden_size))\n\n        # output gate weight\n        self.Wo_h = graph.get_tensor(name+""LSTMCell_Wo_h"", (self.hidden_size, self.hidden_size))\n        self.Wo_i = graph.get_tensor(name+""LSTMCell_Wo_i"", (self.input_size, self.hidden_size))\n\n        # cell gate weight\n        self.Wc_h = graph.get_tensor(name+""LSTMCell_Wc_h"", (self.hidden_size, self.hidden_size))\n        self.Wc_i = graph.get_tensor(name+""LSTMCell_Wc_i"", (self.input_size, self.hidden_size))\n\n        # tensors\n        self.input_tensor = None\n\n        # current state tensors\n        self.hidden_state = None\n        self.cell_state = None\n        self.cell_input_state = None\n\n        # previous state tensors\n        self.prev_hidden_state = None\n        self.prev_cell_state = None\n\n        # forget, input, output\n        self.forget_tensor = None\n        self.in_tensor = None\n        self.output_tensor = None\n        self.cell_tensor = None\n        self.cell_hidden_tensor = None\n\n        self.forget_gate = Sigmoid(name=""Forget_sigmoid"")\n        self.in_gate = Sigmoid(name=""In_sigmoid"")\n        self.output_gate = Sigmoid(name=""Output_sigmoid"")\n        self.cell_gate = Tanh(name=""Cell_tanh"")\n        self.cell_hidden_gate = Tanh(name=""Cell_hidden_tanh"")\n\n        self.forget_gate_tensor = None\n        self.in_gate_tensor = None\n        self.output_gate_tensor = None\n        self.cell_gate_tensor = None\n\n\n    def forward(self, input_tensors):\n        """"""\n        forward computation of LSTM\n\n        f(t) = sigmoid(h(t-1)*Wf_h + input(t)*Wf_i)\n        i(t) = sigmoid(h(t-1)*Wi_h + input(t)*Wi_i)\n        o(t) = sigmoid(h(t-1)*Wo_h + input(t)*Wo_i)\n        c(t) = tanh(h(t-1)*Wc_h + input(t)*Wc_i)\n\n        cell(t) = f(t)*cell(t-1) * i(t)*c(t)\n        hidden(t) = o(t)*tanh(cell(t))\n\n        input_tensors should contain:\n        - prev_hidden_state\n        - prev_cell_state\n        - input tensor\n\n        """"""\n\n        self.batch_size = input_tensors[2].value.shape[0]\n\n        # initialize prev_hidden_state if not provided\n        if input_tensors[0] is None:\n            input_tensors[0] = Tensor(np.zeros((self.batch_size, self.hidden_size)))\n\n        # initialize prev_cell_state if not provided\n        if input_tensors[1] is None:\n            input_tensors[1] = Tensor(np.zeros((self.batch_size, self.hidden_size)))\n\n        self.register(input_tensors)\n\n        # remember tensors\n        self.prev_hidden_state = self.input_tensors[0]\n        self.prev_cell_state = self.input_tensors[1]\n        self.input_tensor = self.input_tensors[2]\n\n        # compute forget gate\n        # f(t) = sigmoid(h(t-1)*Wf_h + input(t)*Wf_i)\n        self.forget_tensor = Tensor(np.dot(self.prev_hidden_state.value, self.Wf_h.value) + np.dot(self.input_tensor.value, self.Wf_i.value))\n        self.forget_gate_tensor = self.forget_gate.forward(self.forget_tensor)\n\n        # compute input gate\n        # i(t) = sigmoid(h(t-1)*Wi_h + input(t)*Wi_i)\n        self.in_tensor = Tensor(np.dot(self.prev_hidden_state.value, self.Wi_h.value) + np.dot(self.input_tensor.value, self.Wi_i.value))\n        self.in_gate_tensor = self.in_gate.forward(self.in_tensor)\n\n        # compute output gate\n        # o(t) = sigmoid(h(t-1)*Wo_h + input(t)*Wo_i)\n        self.output_tensor = Tensor(np.dot(self.prev_hidden_state.value, self.Wo_h.value) + np.dot(self.input_tensor.value, self.Wo_i.value))\n        self.output_gate_tensor = self.output_gate.forward(self.output_tensor)\n\n        # update cell gate\n        # c(t) = tanh(h(t-1)*Wc_h + input(t)*Wc_i)\n        # cell(t) = f(t)*cell(t-1) * i(t)*c(t)\n        self.cell_tensor = Tensor(np.dot(self.prev_hidden_state.value, self.Wc_h.value) + np.dot(self.input_tensor.value, self.Wc_i.value))\n        self.cell_gate_tensor = self.cell_gate.forward(self.cell_tensor)\n\n        # compute current cell state\n        # cell(t) = f(t)*cell(t-1) * i(t)*c(t)\n        self.cell_state = Tensor(self.prev_cell_state.value * self.forget_gate_tensor.value +\n                                   self.cell_gate_tensor.value * self.in_gate_tensor.value)\n\n        # update hidden state\n        # hidden(t) = o(t) * tanh(cell(t))\n        self.cell_hidden_tensor = self.cell_hidden_gate.forward(self.cell_state)\n        self.hidden_state = Tensor(self.output_gate_tensor.value * self.cell_hidden_tensor.value)\n\n        # return hidden and cell\n        return self.hidden_state, self.cell_state\n\n\n    def backward(self):\n        """"""\n        LSTM backward computation\n\n        self.hidden_state.grad should be computed in advance by higher layers\n        self.cell.grad should also be set if current element is not the last element\n\n        """"""\n\n        # update gradient of following formulas\n        # hidden(t) = o(t)*tanh(cell(t))\n        self.output_gate_tensor.grad = self.hidden_state.grad * self.cell_hidden_tensor.value\n        self.cell_hidden_tensor.grad = self.hidden_state.grad * self.output_gate_tensor.value\n        self.cell_hidden_gate.backward()\n\n        # update following gradient\n        # cell(t) = f(t)*cell(t-1) + i(t)*c(t)\n        self.prev_cell_state.grad = self.cell_state.grad * self.forget_gate_tensor.value\n        self.forget_gate_tensor.grad = self.cell_state.grad * self.prev_cell_state.value\n        self.in_gate_tensor.grad = self.cell_state.grad * self.cell_gate_tensor.value\n        self.cell_gate_tensor.grad = self.cell_state.grad * self.in_gate_tensor.value\n\n        # update following gradient\n        # c(t) = tanh(h(t-1)*Wc_h + input(t)*Wc_i)\n        self.cell_gate.backward()\n        self.Wc_h.grad += np.outer(self.prev_hidden_state.value, self.cell_tensor.grad)\n        self.Wc_i.grad += np.outer(self.input_tensor.value, self.cell_tensor.grad)\n\n        self.prev_hidden_state.grad += np.dot(self.cell_tensor.grad, self.Wc_h.value.T)\n        self.input_tensor.grad += np.dot(self.cell_tensor.grad, self.Wc_i.value.T)\n\n        # update following gradient\n        # o(t) = sigmoid(h(t-1)*Wo_h + input(t)*Wo_i)\n        self.output_gate.backward()\n        self.Wo_h.grad += np.outer(self.prev_hidden_state.value, self.output_tensor.grad)\n        self.Wo_i.grad += np.outer(self.input_tensor.value, self.output_tensor.grad)\n\n        self.prev_hidden_state.grad += np.dot(self.output_tensor.grad, self.Wo_h.value.T)\n        self.input_tensor.grad += np.dot(self.output_tensor.grad, self.Wo_i.value.T)\n\n        # update following gradient\n        # i(t) = sigmoid(h(t-1)*Wi_h + input(t)*Wi_i)\n        self.in_gate.backward()\n        self.Wi_h.grad += np.outer(self.prev_hidden_state.value, self.in_tensor.grad)\n        self.Wi_i.grad += np.outer(self.input_tensor.value, self.in_tensor.grad)\n\n        self.prev_hidden_state.grad += np.dot(self.in_tensor.grad, self.Wi_h.value.T)\n        self.input_tensor.grad += np.dot(self.in_tensor.grad, self.Wi_i.value.T)\n\n        # update following gradient\n        # f(t) = sigmoid(h(t-1)*Wf_h + input(t)*Wf_i)\n        self.forget_gate.backward()\n        self.Wf_h.grad += np.outer(self.prev_hidden_state.value, self.forget_tensor.grad)\n        self.Wf_i.grad += np.outer(self.input_tensor.value, self.forget_tensor.grad)\n\n        self.prev_hidden_state.grad += np.dot(self.forget_tensor.grad, self.Wf_h.value.T)\n        self.input_tensor.grad += np.dot(self.forget_tensor.grad, self.Wf_i.value.T)\n\nclass LSTMCell(Operation):\n\n    def __init__(self,  name=\'LSTMCell\', argument=None, graph=None):\n\n        super(LSTMCell, self).__init__(name, argument, graph)\n\n        # intialize size\n        self.input_size = argument[\'input_size\']\n        self.hidden_size = argument[\'hidden_size\']\n\n        # batch size\n        self.batch_size = 1\n\n        self.graph = graph\n\n        # set forget bias to 1 to prevent gradient vanishing\n        self.forget_gate  = self.graph.get_operation(""RNNAffine"", {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, ""nonlinear"": ""Sigmoid"", ""bias"": 1.0}, ""LSTMForget"")\n        self.input_gate   = self.graph.get_operation(""RNNAffine"", {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, ""nonlinear"": ""Sigmoid""}, ""LSTMInput"")\n        self.output_gate  = self.graph.get_operation(""RNNAffine"", {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, ""nonlinear"": ""Sigmoid""}, ""LSTMOutput"")\n        self.cell_gate    = self.graph.get_operation(""RNNAffine"", {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, ""nonlinear"": ""Tanh""}, ""LSTMCell"")\n        self.forget_multi = self.graph.get_operation(""Multiply"")\n        self.input_multi  = self.graph.get_operation(""Multiply"")\n        self.output_multi = self.graph.get_operation(""Multiply"")\n        self.tanh         = self.graph.get_operation(""Tanh"")\n        self.add          = self.graph.get_operation(""Add"")\n\n        # tensors\n        self.input_tensor = None\n\n        # current state tensors\n        self.hidden_state = None\n        self.cell_state = None\n\n        # previous state tensors\n        self.prev_hidden_state = None\n        self.prev_cell_state = None\n\n        # forget, input, output\n        self.forget_state = None\n        self.input_state = None\n        self.output_state = None\n        self.hidden_tensor = None\n        self.cell_hidden_tensor = None\n\n        self.forget_gate_tensor = None\n        self.in_gate_tensor = None\n        self.output_gate_tensor = None\n        self.cell_gate_tensor = None\n\n\n    def forward(self, input_tensors):\n        """"""\n        forward computation of LSTM\n\n        f(t) = sigmoid(h(t-1)*Wf_h + input(t)*Wf_i)\n        i(t) = sigmoid(h(t-1)*Wi_h + input(t)*Wi_i)\n        o(t) = sigmoid(h(t-1)*Wo_h + input(t)*Wo_i)\n        c(t) = tanh(h(t-1)*Wc_h + input(t)*Wc_i)\n\n        cell(t) = f(t)*cell(t-1) * i(t)*c(t)\n        hidden(t) = o(t)*tanh(cell(t))\n\n        input_tensors should contain:\n        - prev_hidden_state\n        - prev_cell_state\n        - input tensor\n\n        """"""\n\n        self.batch_size = input_tensors[2].value.shape[0]\n\n        # initialize prev_hidden_state if not provided\n        if input_tensors[0] is None:\n            input_tensors[0] = Tensor(np.zeros((self.batch_size, self.hidden_size)))\n\n        # initialize prev_cell_state if not provided\n        if input_tensors[1] is None:\n            input_tensors[1] = Tensor(np.zeros((self.batch_size, self.hidden_size)))\n\n        self.register(input_tensors)\n\n        # remember tensors\n        self.prev_hidden_state = self.input_tensors[0]\n        self.prev_cell_state = self.input_tensors[1]\n        self.input_tensor = self.input_tensors[2]\n        input_hidden_pair = [self.prev_hidden_state, self.input_tensor]\n\n        self.forget_gate_tensor = self.forget_gate.forward(input_hidden_pair)\n        self.input_gate_tensor = self.input_gate.forward(input_hidden_pair)\n        self.output_gate_tensor = self.output_gate.forward(input_hidden_pair)\n        self.cell_gate_tensor = self.cell_gate.forward(input_hidden_pair)\n\n        # compute current cell state\n        # cell(t) = f(t)*cell(t-1) + i(t)*c(t)\n        self.forget_state = self.forget_multi.forward([self.forget_gate_tensor, self.prev_cell_state])\n        self.input_state  = self.input_multi.forward([self.input_gate_tensor, self.cell_gate_tensor])\n        self.cell_state   = self.add.forward([self.forget_state, self.input_state])\n\n        # update hidden state\n        # hidden(t) = o(t) * tanh(cell(t))\n        self.cell_hidden_tensor = self.tanh.forward(self.cell_state)\n        self.hidden_state = self.output_multi.forward([self.output_gate_tensor, self.cell_hidden_tensor])\n\n        # return hidden and cell\n        return self.hidden_state, self.cell_state\n\n\n    def backward(self):\n        return\n\n\nclass LSTM(Operation):\n\n    def __init__(self, name=\'LSTM\', argument=None, graph=None):\n        super(LSTM, self).__init__(name, argument, graph)\n        self.input_size = argument[\'input_size\']\n        self.hidden_size = argument[\'hidden_size\']\n\n        # max num steps to run RNN\n        # this is to prevent initializing RNNCell everytime\n        self.max_num_steps = argument[\'max_num_steps\']\n        self.num_steps = 0\n\n        # create empty cell list\n        self.cells = [LSTMCell(\'LSTMCell\', argument, graph) for i in range(self.max_num_steps)]\n\n        self.input_tensors = []\n        self.state_tensors = []\n\n    def forward(self, input_tensors):\n        """"""\n\n        :param input_tensors: a list of var_input from word embedding\n        :return:\n        """"""\n\n        self.register(input_tensors)\n\n        self.hidden_states = []\n        self.cell_states = []\n\n        self.num_steps = min(len(input_tensors), self.max_num_steps)\n\n        self.last_hidden = None\n        self.last_cell = None\n\n        for i in range(self.num_steps):\n            self.last_hidden, self.last_cell = self.cells[i].forward([self.last_hidden, self.last_cell, input_tensors[i]])\n            self.hidden_states.append(self.last_hidden)\n            self.cell_states.append(self.last_cell)\n\n        return self.hidden_states\n\n    def backward(self):\n        return'"
pytensor/ops/math_ops.py,12,"b'from pytensor.network.operation import *\n\nclass Add(Operation):\n\n    def __init__(self, name=\'add\', argument=None, graph=None):\n        super(Add, self).__init__(name, argument, graph)\n\n\n    def forward(self, input_tensors):\n        """"""\n        Add all tensors in the input_tensor\n\n        :param input_tensors:\n        :return:\n        """"""\n        self.register(input_tensors)\n\n        # value for the output tensor\n        value = np.zeros_like(self.input_tensors[0].value)\n\n        for input_tensor in self.input_tensors:\n            value += input_tensor.value\n\n        self.output_tensor = Tensor(value)\n\n        return self.output_tensor\n\n    def backward(self):\n        """"""\n        backward grad into each input tensor\n\n        :return:\n        """"""\n\n        for input_tensor in self.input_tensors:\n            input_tensor.grad += self.output_tensor.grad\n\n\nclass Multiply(Operation):\n\n    def __init__(self, name=\'multiply\', argument=None, graph=None):\n        super(Multiply, self).__init__(name, argument, graph)\n\n\n    def forward(self, input_tensors):\n        """"""\n        element multiplication of input_tensors\n\n        :param input_tensors:\n        :return:\n        """"""\n        self.register(input_tensors)\n\n        # only multiplication of two elements is supported now\n        assert(len(input_tensors)==2)\n\n        self.input_tensors = input_tensors\n\n        # validate both input_tensors have same shape\n        self.x = input_tensors[0]\n        self.y = input_tensors[1]\n        assert(self.x.value.shape == self.y.value.shape)\n\n        # value for the output tensor\n        value = np.multiply(self.x.value, self.y.value)\n        self.output_tensor = Tensor(value)\n\n        return self.output_tensor\n\n    def backward(self):\n        """"""\n        backward grad into each input tensors\n\n        :return:\n        """"""\n\n        # update gradient\n        self.x.grad += np.multiply(self.output_tensor.grad, self.y.value)\n        self.y.grad += np.multiply(self.output_tensor.grad, self.x.value)\n\n\nclass Matmul(Operation):\n\n    def __init__(self, name=\'matmul\', argument=None, graph=None):\n        super(Matmul, self).__init__(name, argument, graph)\n\n    def forward(self, input_tensors):\n        self.register(input_tensors)\n\n        # only multiplication of two elements is supported now\n        assert(len(input_tensors)==2)\n\n        self.x = input_tensors[0]\n        self.y = input_tensors[1]\n\n        out = np.dot(self.x.value, self.y.value)\n\n        self.output_tensor = Tensor(out)\n        return self.output_tensor\n\n    def backward(self):\n\n        # update gradient\n        self.x.grad += np.dot(self.output_tensor.grad, self.y.value.T)\n        self.y.grad += np.dot(self.x.value.T, self.output_tensor.grad)\n\n\nclass Relu(Operation):\n    def __init__(self, name=""Relu"", argument=None, graph=None):\n        super(Relu, self).__init__(name, argument, graph)\n\n        self.mask = None\n\n    def forward(self, input_tensors):\n        """"""\n        :param input_tensor:\n        :return:\n        """"""\n\n        self.register(input_tensors)\n\n        # compute mask\n        self.mask = (input_tensors.value <= 0)\n\n        # create output tensor\n        out = input_tensors.value.copy()\n        out[self.mask] = 0\n        self.output_tensor = Tensor(out)\n\n        # return\n        return self.output_tensor\n\n    def backward(self):\n        self.input_tensors.grad = self.output_tensor.grad\n        self.input_tensors.grad[self.mask] = 0\n\n\nclass Sigmoid(Operation):\n\n    def __init__(self, name=\'sigmoid\', argument=None, graph=None):\n        super(Sigmoid, self).__init__(name, argument, graph)\n\n    def forward(self, input_tensors):\n        self.register(input_tensors)\n\n        # compute sigmoid\n        value = sigmoid(self.input_tensors.value)\n        self.output_tensor = Tensor(value)\n\n        return self.output_tensor\n\n    def backward(self):\n        self.input_tensors.grad += self.output_tensor.grad * (1.0 - self.output_tensor.value) * self.output_tensor.value\n\n\nclass Tanh(Operation):\n\n    def __init__(self, name=\'tanh\', argument=None, graph=None):\n        super(Tanh, self).__init__(name, argument, graph)\n\n    def forward(self, input_tensors):\n        self.register(input_tensors)\n\n        self.input_tensor = input_tensors\n        out = np.tanh(self.input_tensor.value)\n\n        self.output_tensor = Tensor(out)\n        return self.output_tensor\n\n    def backward(self):\n        self.input_tensor.grad += self.output_tensor.grad * (1.0 - self.output_tensor.value * self.output_tensor.value)\n\n\nclass Affine(Operation):\n    def __init__(self, name=""affine"", argument=None, graph=None):\n        """"""\n        Affine transformation: y=wx+b\n\n        :param argument: [input_size, hidden_size, bias (optional)]\n        """"""\n\n        super(Affine, self).__init__(name, argument, graph)\n\n        # arg should contains two int\n        # one for input_size and the other for hidden_size\n        assert(len(argument)==2 or len(argument)==3)\n\n        self.input_size = argument[\'input_size\']\n        self.hidden_size = argument[\'hidden_size\']\n\n        # bias is disabled\n        if \'bias\' in argument and argument[\'bias\'] == \'None\':\n            self.b = None\n        else:\n            b_name = self.name + ""_b""\n            self.b = self.graph.parameter.get_tensor(b_name, (self.hidden_size,))\n\n        # bias initialization\n        if \'bias\' in argument and isinstance(argument[\'bias\'], float):\n            self.b.value[::] = float(argument[\'bias\'])\n\n        W_name = self.name + ""_W""\n        self.W = self.graph.parameter.get_tensor(W_name, (self.input_size, self.hidden_size))\n\n\n    def forward(self, input_tensors):\n        self.register(input_tensors)\n\n        # check input size\n        assert input_tensors.value.shape[1] == self.input_size, ""expected: ""+str(self.input_size)+"" actual: ""+str(input_tensors.value.shape[1])\n\n        # apply affine transformation\n        value = np.dot(self.input_tensors.value, self.W.value)\n\n        # add bias\n        if self.b:\n            value += self.b.value\n\n        self.output_tensor = Tensor(value)\n        return self.output_tensor\n\n    def backward(self):\n        self.input_tensors.grad += np.dot(self.output_tensor.grad, self.W.value.T)\n        self.W.grad += np.dot(self.input_tensors.value.T, self.output_tensor.grad)\n\n        if self.b:\n            self.b.grad += np.sum(self.output_tensor.grad, axis=0)'"
pytensor/ops/rnn_ops.py,9,"b'from pytensor.network.tensor import *\nfrom pytensor.network.parameter import *\nfrom pytensor.network.operation import *\nfrom pytensor.ops.rnn_util_ops import *\n\nclass RawRNNCell(Operation):\n\n    def __init__(self,  name=\'RawRNNCell\', argument=None, graph=None):\n\n        super(RawRNNCell, self).__init__(name, argument, graph)\n\n        # intialize size\n        self.input_size = argument[\'input_size\']\n        self.hidden_size = argument[\'hidden_size\']\n\n        # batch size\n        self.batch_size = 1\n\n        # create tensors\n        self.U = graph.parameter.get_tensor(""RNNCell_U"", (self.input_size, self.hidden_size))\n        self.W = graph.parameter.get_tensor(""RNNCell_W"", (self.hidden_size, self.hidden_size))\n\n        self.input_tensor = None\n        self.state_tensor = None\n        self.prev_state_tensor = None\n\n    def forward(self, input_tensors):\n        """"""\n        forward computation of RNNCell\n        State(t) = tanh(State(t-1)*W + Input(t)*U)\n\n        :param input_tensors: input_tensors should contain 2 tensors: State(t-1) and Input(t)\n        , State(t-1) is from previous cell and Input(t) is the current cell input tensor.\n        :return: State(t)\n        """"""\n\n        # initialize State(t-1) if it is not provided\n        if input_tensors[0] is None:\n            self.batch_size = input_tensors[1].value.shape[0]\n\n            prev_state_tensor = Tensor(np.zeros((self.batch_size, self.hidden_size)))\n            input_tensors[0] = prev_state_tensor\n\n        # forward registration\n        self.register(input_tensors)\n\n        # remember tensors\n        self.prev_state_tensor = self.input_tensors[0]\n        self.input_tensor = self.input_tensors[1]\n\n        # input to hidden\n        state_value = np.dot(self.input_tensor.value, self.U.value)\n\n        # hidden to hidden\n        state_value += np.dot(self.prev_state_tensor.value, self.W.value)\n\n        # nonlinear\n        state_value = np.tanh(state_value)\n\n        # create tensor\n        self.state_tensor = Tensor(state_value)\n        return self.state_tensor\n\n    def backward(self):\n        """"""\n        :param input_tensor: Input(t)\n        :param state_tensor: State(t)\n        :param loss_tensor: overall loss from State(t)\n        :return: input_loss_tensor, prev_loss_tensor\n        """"""\n\n        # loss of State(t-1)*W+Input(t)*U\n        dL = self.state_tensor.grad * (1.0 - self.state_tensor.value * self.state_tensor.value)\n\n        dLdS = np.dot(dL, self.W.value.T)\n        dLdW = np.dot(self.prev_state_tensor.value.T, dL)\n        dLdI = np.dot(dL, self.U.value.T)\n        dLdU = np.dot(self.input_tensor.value.T, dL)\n\n        # add grad to U and W\n        self.W.grad += dLdW\n        self.U.grad += dLdU\n\n        # update previous state\'s gradient\n        self.prev_state_tensor.grad += dLdS\n\n        # update input tensor\'s gradient\n        self.input_tensor.grad += dLdI\n\n\nclass RNNCell(Operation):\n\n    def __init__(self,  name, argument, graph):\n\n        super(RNNCell, self).__init__(name, argument, graph)\n\n        self.graph = graph\n\n        # intialize size\n        self.input_size = argument[\'input_size\']\n        self.hidden_size = argument[\'hidden_size\']\n\n        # batch size\n        self.batch_size = 1\n\n        self.rnn_affine = self.graph.get_operation(""RNNAffine"", {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, ""nonlinear"": ""Tanh""}, ""RNNAffine"")\n\n        self.input_tensor = None\n\n    def forward(self, input_tensors):\n        """"""\n        forward computation of RNNCell\n        State(t) = tanh(State(t-1)*W + Input(t)*U)\n\n        :param input_tensors: input_tensors should contain 2 tensors: State(t-1) and Input(t)\n        , State(t-1) is from previous cell and Input(t) is the current cell input tensor.\n        :return: State(t)\n        """"""\n\n        # initialize State(t-1) if it is not provided\n        if input_tensors[0] is None:\n            self.batch_size = input_tensors[1].value.shape[0]\n\n            prev_state_tensor = Tensor(np.zeros((self.batch_size, self.hidden_size)))\n            input_tensors[0] = prev_state_tensor\n\n        # forward registration\n        self.register(input_tensors)\n        self.state_tensor = self.rnn_affine.forward(self.input_tensors)\n        return self.state_tensor\n\n    def backward(self):\n        return\n\nclass RNN(Operation):\n\n    def __init__(self, name=\'RNN\', argument=None, graph=None):\n        super(RNN, self).__init__(name, argument, graph)\n        self.input_size = argument[\'input_size\']\n        self.hidden_size = argument[\'hidden_size\']\n\n        # max num steps to run RNN\n        # this is to prevent initializing RNNCell everytime\n        self.max_num_steps = argument[\'max_num_steps\']\n        self.num_steps = 0\n\n        # create empty cell list\n        self.cells = [RNNCell(\'RNNCell\', argument, graph) for i in range(self.max_num_steps)]\n\n        self.input_tensors = []\n        self.state_tensors = []\n\n    def forward(self, input_tensors):\n        """"""\n\n        :param input_tensors: a list of var_input from word embedding\n        :return:\n        """"""\n\n        self.register(input_tensors)\n\n        self.state_tensors = []\n\n        self.num_steps = min(len(input_tensors), self.max_num_steps)\n\n        self.last_state = None\n\n        for i in range(self.num_steps):\n            self.last_state = self.cells[i].forward([self.last_state, input_tensors[i]])\n            self.state_tensors.append(self.last_state)\n\n        return self.state_tensors\n\n    def backward(self):\n        return'"
pytensor/ops/rnn_util_ops.py,6,"b'from pytensor.network.operation import *\n\n\nclass RNNAffine(Operation):\n\n    def __init__(self, name=""RNNAffine"", argument=None, graph=None):\n        """"""\n        Affine transformation: y= nonlinear(U x1 + W x2 + b)\n        This is a utility operation for RNN and LSTM\n\n        :param argument: {\'input_size\', \'hidden_size\', \'nonlinear\'(optional), \'bias\'(optional) }\n        """"""\n\n        super(RNNAffine, self).__init__(name, argument, graph)\n\n        # one for input_size and the other for hidden_size\n        assert (\'input_size\' in argument and \'hidden_size\' in argument)\n\n        self.input_size = argument[\'input_size\']\n        self.hidden_size = argument[\'hidden_size\']\n\n        self.graph = graph\n\n        # bias is disabled\n        if \'bias\' in argument and argument[\'bias\'] == \'None\':\n            self.b = None\n        else:\n            b_name = self.name + ""_b""\n            self.b = self.graph.parameter.get_tensor(b_name, (self.hidden_size,))\n\n        # bias initialization\n        if \'bias\' in argument and isinstance(argument[\'bias\'], float):\n            self.b.value[::] = float(argument[\'bias\'])\n\n        # hidden to hidden\n        self.U = self.graph.parameter.get_tensor(self.name+\'_U\', (self.hidden_size, self.hidden_size))\n\n        # input to hidden\n        self.W = self.graph.parameter.get_tensor(self.name+\'_W\', (self.input_size, self.hidden_size))\n\n        # nonlinear operation\n        self.nonlinear = None\n        if \'nonlinear\' in argument:\n            nonlinear = argument[\'nonlinear\']\n            self.nonlinear = self.graph.get_operation(nonlinear, None, name+""_""+nonlinear)\n\n        # output tensors\n        self.add_tensor = None\n        self.nonlinear_tensor = None\n\n\n    def forward(self, input_tensors):\n        self.register(input_tensors)\n\n        # check input size\n        assert input_tensors[1].value.shape[1] == self.input_size, ""expected: "" + str(\n            self.input_size) + "" actual: "" + str(input_tensors[1].value.shape[1])\n\n        # check input size\n        assert input_tensors[0].value.shape[1] == self.hidden_size, ""expected: "" + str(\n            self.input_size) + "" actual: "" + str(input_tensors[0].value.shape[1])\n\n        # apply affine transformation\n        value = np.dot(self.input_tensors[0].value, self.U.value) + np.dot(self.input_tensors[1].value, self.W.value)\n\n        # add bias\n        if self.b:\n            value += self.b.value\n\n        self.add_tensor = Tensor(value)\n\n        if self.nonlinear:\n            self.nonlinear_tensor = self.nonlinear.forward(self.add_tensor)\n            return self.nonlinear_tensor\n        else:\n            return self.add_tensor\n\n    def backward(self):\n\n        self.input_tensors[1].grad += np.dot(self.add_tensor.grad, self.W.value.T)\n        self.input_tensors[0].grad += np.dot(self.add_tensor.grad, self.U.value.T)\n\n        self.W.grad += np.dot(self.input_tensors[1].value.T, self.add_tensor.grad)\n        self.U.grad += np.dot(self.input_tensors[0].value.T, self.add_tensor.grad)\n\n        if self.b:\n            self.b.grad += np.sum(self.add_tensor.grad, axis=0)'"
pytensor/test/__init__.py,0,b''
pytensor/test/common.py,3,"b'import unittest\nimport numpy as np\n\ndef gradient_generator(model, input_tensors, target_tensor):\n    """"""\n    generate diference for testing purpose\n\n    :param model: the model we want to validate\n    :param input_tensors: input tensors for gradient check\n    :param target_tensor: target tensor for gradient check\n    :return:\n    """"""\n\n    # clear gradient before validation\n    model.parameter.clear_grads()\n\n    # run normal procedures to compute automatic gradient\n    model.forward(input_tensors)\n    model.loss(target_tensor)\n    model.backward()\n\n    # tensors we need to check\n    tensor_dict = model.parameter.tensor_dict\n\n    for var_name, var in tensor_dict.items():\n\n        h = 1e-4\n        v = var.value\n\n        numerical_grad = np.zeros_like(v)\n\n        # compute numerical gradient of this tensor\n        it = np.nditer(v, flags=[\'multi_index\'], op_flags=[\'readwrite\'])\n        while not it.finished:\n            idx = it.multi_index\n            tmp_val = v[idx]\n\n            # f(x+h)\n            v[idx] = float(tmp_val) + h\n            model.forward(input_tensors)\n            loss_1 = model.loss(target_tensor)\n\n            # f(x-h)\n            v[idx] = tmp_val - h\n            model.forward(input_tensors)\n            loss_2 = model.loss(target_tensor)\n\n            numerical_grad[idx] = (loss_1 - loss_2) / (2 * h)\n\n            v[idx] = tmp_val\n            it.iternext()\n\n            # clear ops\n            model.clear()\n\n        # compare numerical grad with auto grad\n        diff = np.sum(var.grad - numerical_grad)\n\n        yield (var_name, numerical_grad, var.grad)'"
pytensor/test/test_linear.py,1,"b'from pytensor.data.digit_dataset import *\nfrom pytensor.test.common import *\nfrom pytensor.model.linear import *\n\n\nclass Linear(Graph):\n\n    def __init__(self, input_size, output_size):\n        super().__init__(""Linear"")\n\n        # make graph\n        self.affine = self.get_operation(\'Affine\', {\'input_size\' : input_size, \'hidden_size\': output_size})\n        self.softmaxloss = self.get_operation(\'SoftmaxLoss\')\n\n    def forward(self, input_tensor):\n        affine_tensor = self.affine.forward(input_tensor)\n        return self.softmaxloss.forward(affine_tensor)\n\n    def loss(self, target_tensor):\n        return self.softmaxloss.loss(target_tensor)\n\n\nclass TestLinearModel(unittest.TestCase):\n\n    def test_gradient(self):\n\n        """"""\n        validate model\'s gradient with numerical methods\n\n        :return:\n        """"""\n\n        data_train, data_test, label_train, label_test = digit_dataset()\n        model = Linear(64,10)\n\n        grad_info = gradient_generator(model, Tensor([data_train[0]]), LongTensor([label_train[0]]))\n\n        for var, expected_grad, actual_grad in grad_info:\n            diff = np.sum(np.abs(expected_grad - actual_grad))\n            print(""Now checking: "", var)\n            self.assertLessEqual(diff, 0.001)\n\nif __name__ == \'__main__\':\n    unittest.main()'"
pytensor/test/test_lstm.py,3,"b'from pytensor.data.digit_dataset import *\nfrom pytensor.test.common import *\nfrom pytensor.model.lstm import *\n\n\nclass TestLSTMModel(unittest.TestCase):\n\n    def test_gradient(self):\n        """"""\n        validate model\'s gradient with numerical methods\n\n        :return:\n        """"""\n\n        input_lst = [np.random.randint(5) for i in range(10)]\n        output_lst = [np.random.randint(5) for i in range(10)]\n\n        model = LSTMLM(5, 5, 10)\n\n        grad_info = gradient_generator(model, input_lst, output_lst)\n\n        for var, expected_grad, actual_grad in grad_info:\n            diff = np.sum(np.abs(expected_grad - actual_grad))\n            print(""Now checking "", var)\n            self.assertLessEqual(diff, 0.001)\n\nif __name__ == \'__main__\':\n    unittest.main()'"
pytensor/test/test_mlp.py,1,"b'from pytensor.data.digit_dataset import *\nfrom pytensor.test.common import *\nfrom pytensor.model.mlp import *\n\n\nclass TestMLPModel(unittest.TestCase):\n\n    def test_gradient(self):\n        """"""\n        validate model\'s gradient with numerical methods\n\n        :return:\n        """"""\n\n        data_train, data_test, label_train, label_test = digit_dataset()\n        model = MLP(64, 30, 10)\n\n        grad_info = gradient_generator(model, Tensor([data_train[0]]), LongTensor([label_train[0]]))\n\n        for var, expected_grad, actual_grad in grad_info:\n            diff = np.sum(np.abs(expected_grad - actual_grad))\n            print(""Now checking "", var)\n            self.assertLessEqual(diff, 0.001)\n\nif __name__ == \'__main__\':\n    unittest.main()'"
pytensor/test/test_rnn.py,3,"b'from pytensor.data.digit_dataset import *\nfrom pytensor.test.common import *\nfrom pytensor.model.rnn import *\n\n\nclass TestRNNModel(unittest.TestCase):\n\n    def test_gradient(self):\n        """"""\n        validate model\'s gradient with numerical methods\n\n        :return:\n        """"""\n\n        input_lst = [np.random.randint(5) for i in range(10)]\n        output_lst = [np.random.randint(5) for i in range(10)]\n\n        model = RNNLM(5, 5, 10)\n\n        grad_info = gradient_generator(model, input_lst, output_lst)\n\n        for var, expected_grad, actual_grad in grad_info:\n            diff = np.sum(np.abs(expected_grad - actual_grad))\n            print(""Now checking "", var)\n            self.assertLessEqual(diff, 0.001)\n\nif __name__ == \'__main__\':\n    unittest.main()'"
pytensor/tutorial/__init__.py,0,b''
pytensor/utils/__init__.py,0,b''
pytensor/utils/vocabulary.py,4,"b'from collections import defaultdict\nimport numpy as np\n\ndef create_vocabulary(inputs, key_words=[]):\n    """"""\n    Create a vocabulary object from a sentence or a list of sentence\n\n    :param inputs:\n    :param vocab_size:\n    :param key_words:\n    :return:\n    """"""\n\n    vocab = Vocabulary()\n    vocab.set_key_words(key_words)\n\n    if isinstance(inputs[0], list):\n\n        # inputs is a list of sentence\n        for sentence in inputs:\n            vocab.update_words(sentence)\n\n    else:\n        # inputs is a list of words\n        vocab.update_words(inputs)\n\n    vocab.sort()\n    return vocab\n\n\nclass Vocabulary:\n\n    def __init__(self, max_vocab_size=None):\n        """"""\n        init the vocabulary\n        \n        Vocabulary is a data structure to transform between word and its word index \n\n        :param vocab_size: size limitation of vocabulary\n        """"""\n\n        # word to id\n        self.word_id = defaultdict()\n\n        # id to word\n        self.words = []\n\n        # count distinct word in the dictionary\n        self.vocab_size = 0\n\n        # max vocabulary size\n        self.max_vocab_size = max_vocab_size\n\n        # count word frequencies\n        self.word_freq = defaultdict(float)\n        self.word_freq_sum = 0.0\n\n        # stop word list\n        # all words stored in this list should be excluded\n        self.stop_words = []\n\n        # key word list\n        # all words stored in this list should be included in the dictionary despite their frequencies\n        self.key_words = []\n\n    def __str__(self):\n        return \'<Vocabulary: \'+str(len(self.words))+\' words>\'\n\n    def __repr__(self):\n        return self.__str__()\n\n    def set_stop_words(self, word_lst):\n        """"""\n        set up the stop words\n        stop words will always be removed from sentences\n\n        :param word_lst:\n        :return:\n        """"""\n        self.stop_words = word_lst\n\n    def set_key_words(self, word_lst):\n        """"""\n        set up the key words\n        key words will never be removed from sentences despite their frequencies\n\n        :param word_lst: a list containing key words\n        :return:\n        """"""\n        self.key_words = word_lst\n\n    def update_word(self, word):\n        """"""\n        Update the frequency for a word\n        :param word: a single word\n        :return:\n        """"""\n\n        if word not in self.word_id:\n            self.words.append(word)\n            self.word_id[word] = self.vocab_size\n            self.vocab_size += 1\n\n        self.word_freq[word] += 1.0\n        self.word_freq_sum += 1.0\n\n    def update_words(self, words):\n        """"""\n        update the frequency for words\n\n        :param words: a list of word\n        :return:\n        """"""\n\n        for word in words:\n            self.update_word(word)\n\n    def get_id(self, word):\n        """"""\n        get the id for a specific word\n\n        :param word:\n        :return: assigned word id or -1 if not found in the vocabulary boundary\n        """"""\n\n        if word in self.word_id:\n            word_id = self.word_id[word]\n            return word_id\n\n        # return id for <UNK> if we have it\n        if \'<UNK>\' in self.word_id:\n            return self.word_id[\'<UNK>\']\n        else:\n            return -1\n\n    def get_ids(self, words):\n        """"""\n        get index for a word list\n        :param words:\n        :return:\n        """"""\n\n        ids = []\n        for word in words:\n            word_id = self.get_id(word)\n            if word_id >= 0:\n                ids.append(self.get_id(word))\n\n        return ids\n\n    def has_word(self, word):\n        return word in self.word_id\n\n    def get_word(self, word_id):\n        if 0<= word_id < self.vocab_size:\n            return self.words[word_id]\n        else:\n            return ""<UNK>""\n\n    def get_words(self, ids):\n        """"""\n        get original word from ids\n\n        :param ids: word id list\n        :return: a list of word\n        """"""\n\n        words = []\n        for id in ids:\n            words.append(self.get_word(id))\n\n        return words\n\n    def convert(self, x):\n        """"""\n        convert x into word or word id\n\n        :param x: word or word id or its list or its list of list\n        :return:\n        """"""\n\n        if len(x) == 0:\n            return []\n\n        result = []\n\n        if isinstance(x, list) or isinstance(x, tuple) or isinstance(x, np.ndarray):\n            if isinstance(x[0], int) or isinstance(x[0], np.int64) or isinstance(x[0], np.int32):\n                result = self.get_words(x)\n            elif isinstance(x[0], str):\n                result = self.get_ids(x)\n            else:\n                #x is a list of list\n                for lst in x:\n                    if isinstance(lst[0], int) or isinstance(x[0], np.int64) or isinstance(x[0], np.int32):\n                        result.append(self.get_words(lst))\n                    else:\n                        result.append(self.get_ids(lst))\n\n        else:\n            if isinstance(x, int) or isinstance(x[0], np.int64) or isinstance(x[0], np.int32):\n                result = self.get_word(x)\n            else:\n                result = self.get_id(x)\n\n        return result\n\n\n    def sort(self):\n\n        """"""\n        rerank the whole dictionary based on there frequence\n        however, stop word should always be excluded from the dictionary,\n        and key words should always be included in the dictionary\n\n        :return:\n        """"""\n\n        # update the frequency for words\n        sort_word_freq = sorted(self.word_freq.items(), key=lambda x:x[1], reverse=True)\n\n        # unique word count\n        word_index = 0\n\n        # recreate word list and word id\n        self.words = []\n        self.word_id = defaultdict()\n\n        # append the key words firstly\n        for word in self.key_words:\n            self.word_id[word] = word_index\n            self.words.append(word)\n            word_index += 1\n\n        # remap the wordid\n        for word_freq_pair in sort_word_freq:\n            word = word_freq_pair[0]\n\n            # skip the word if the word is in key words or stop words\n            if word in self.word_id or word in self.stop_words:\n                continue\n\n            self.word_id[word] = word_index\n            self.words.append(word)\n\n            word_index += 1\n\n        self.vocab_size = len(self.words)'"
pytensor/tutorial/part1/__init__.py,0,b''
pytensor/tutorial/part1/linear.py,0,"b'from pytensor.ops.math_ops import *\nfrom pytensor.ops.loss_ops import *\nfrom pytensor.tutorial.part1.trainer import *\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\n\nclass LinearModel:\n\n    def __init__(self, input_size, output_size):\n        """"""\n        a simple linear model: y = w*x\n\n        :param input_size:\n        :param output_size:\n        """"""\n\n        # initialize size\n        self.input_size = input_size\n        self.output_size = output_size\n\n        # initialize parameters\n        self.parameter = Parameter()\n        self.W = self.parameter.get_tensor(\'weight\', [self.input_size, self.output_size])\n\n        # ops and loss\n        self.matmul = Matmul()\n        self.loss_ops = SoftmaxLoss()\n\n    def forward(self, input_tensor):\n        output_tensor = self.matmul.forward([input_tensor, self.W])\n        self.loss_ops.forward(output_tensor)\n\n        return output_tensor\n\n    def loss(self, target_tensor):\n        loss_val = self.loss_ops.loss(target_tensor)\n        return loss_val\n\n    def backward(self):\n        self.loss_ops.backward()\n        self.matmul.backward()\n\n\nif __name__ == \'__main__\':\n\n    digits = load_digits()\n    digits.data /= 16.0\n    x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target)\n\n    model = LinearModel(64, 10)\n    trainer = Trainer(model)\n    trainer.train(x_train, y_train, x_test, y_test)'"
pytensor/tutorial/part1/trainer.py,1,"b'from pytensor.network.optimizer import *\n\nclass Trainer:\n\n    def __init__(self, model):\n        """"""\n        A trainer example\n\n        :param model:\n        """"""\n\n        self.model = model\n        self.optimizer = SGD(self.model.parameter)\n\n    def train(self, x_train, y_train, x_test=None, y_test=None, epoch=40, iteration=10000):\n\n        for ii in range(epoch):\n\n            loss = 0.0\n\n            for i in range(len(x_train)):\n\n                self.optimizer.zero_grad()\n\n                # extract data set\n                input_tensors = Tensor([x_train[i]])\n                target_tensor = Tensor([y_train[i]])\n\n                # forward\n                self.model.forward(input_tensors)\n\n                # loss\n                loss += self.model.loss(target_tensor)\n\n                # backward\n                self.model.backward()\n\n                self.optimizer.step()\n\n            accuracy = self.test(x_test, y_test)\n            print(""\\repoch {}: loss {}, acc {}"".format(ii, loss, accuracy), end=\'\')\n\n\n    def test(self, x_test, y_test):\n\n        acc_cnt = 0.0\n        all_cnt = len(x_test)\n\n        for i in range(len(x_test)):\n\n            v = Tensor([x_test[i]])\n            output_tensor = self.model.forward(v)\n\n            y = np.argmax(output_tensor.value[0])\n            if y == y_test[i]:\n                acc_cnt += 1.0\n\n        return acc_cnt/all_cnt\n'"
pytensor/tutorial/part2/__init__.py,0,b''
pytensor/tutorial/part2/linear.py,0,"b'from pytensor.tutorial.part2.trainer import *\nfrom pytensor.data.digit_dataset import *\n\n\nclass Linear(Graph):\n\n    def __init__(self, input_size, output_size):\n        super().__init__(""linear"")\n\n        # make graph\n        self.affine = self.get_operation(\'Affine\', {\'input_size\' : input_size, \'hidden_size\': output_size})\n        self.softmaxloss = self.get_operation(\'SoftmaxLoss\')\n\n    def forward(self, input_tensor):\n        affine_tensor = self.affine.forward(input_tensor)\n        return self.softmaxloss.forward(affine_tensor)\n\n    def loss(self, target_tensor):\n        return self.softmaxloss.loss(target_tensor)\n\n\n\ndef linear_train():\n    data_train, data_test, label_train, label_test = digit_dataset()\n    model = Linear(64, 10)\n\n    trainer = Trainer(model)\n    trainer.train(data_train, label_train, data_test, label_test, 40)\n\n\nif __name__ == \'__main__\':\n    linear_train()'"
pytensor/tutorial/part2/mlp.py,0,"b'from pytensor.tutorial.part2.trainer import *\nfrom pytensor.data.digit_dataset import *\n\n\nclass MLP(Graph):\n\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__(""mlp"")\n\n        # make graph\n        self.affine1 = self.get_operation(\'Affine\', {\'input_size\': input_size, \'hidden_size\': hidden_size})\n        self.sigmoid = self.get_operation(\'Sigmoid\')\n        self.affine2 = self.get_operation(\'Affine\', {\'input_size\': hidden_size, \'hidden_size\': output_size})\n        self.softmaxloss = self.get_operation(\'SoftmaxLoss\')\n\n    def forward(self, input_tensor):\n        affine1_tensor = self.affine1.forward(input_tensor)\n        sigmoid_tensor = self.sigmoid.forward(affine1_tensor)\n        affine2_tensor = self.affine2.forward(sigmoid_tensor)\n\n        return self.softmaxloss.forward(affine2_tensor)\n\n    def loss(self, target_tensor):\n        return self.softmaxloss.loss(target_tensor)\n\n\ndef mlp_train():\n\n    data_train, data_test, label_train, label_test = digit_dataset()\n    model = MLP(64, 30, 10)\n\n    trainer = Trainer(model)\n    trainer.train(data_train, label_train, data_test, label_test, 40)\n\n\nif __name__ == \'__main__\':\n    mlp_train()'"
pytensor/tutorial/part2/trainer.py,1,"b'from pytensor.network.graph import *\nfrom pytensor.network.gradient import *\nfrom pytensor.network.optimizer import *\n\nclass Trainer:\n\n    def __init__(self, model):\n        """"""\n        A trainer example using graph for autodiff\n        :param model:\n        """"""\n\n        self.model = model\n        self.optimizer = SGD(self.model.parameter)\n\n    def train(self, x_train, y_train, x_test=None, y_test=None, epoch=40):\n\n        for ii in range(epoch):\n\n            self.model.train()\n\n            loss = 0.0\n\n            for i in range(len(x_train)):\n\n                self.optimizer.zero_grad()\n\n                # extract data set\n                input_tensors = Tensor([x_train[i]])\n                target_tensor = Tensor([y_train[i]])\n\n                # dynamic forward\n                self.model.forward(input_tensors)\n\n                # loss\n                loss += self.model.loss(target_tensor)\n\n                # automatic differentiation\n                self.model.backward()\n\n                # optimization\n                self.optimizer.step()\n\n            accuracy = self.test(x_test, y_test)\n            print(""\\repoch {}: loss {}, acc {}"".format(ii, loss, accuracy), end=\'\')\n\n    def test(self, x_test, y_test):\n\n        self.model.eval()\n\n        acc_cnt = 0.0\n        all_cnt = len(x_test)\n\n        for i in range(len(x_test)):\n\n            v = Tensor([x_test[i]])\n            output_tensor = self.model.forward(v)\n\n            y = np.argmax(output_tensor.value[0])\n            if y == y_test[i]:\n                acc_cnt += 1.0\n\n        return acc_cnt / all_cnt\n'"
pytensor/tutorial/part3/__init__.py,0,b''
pytensor/tutorial/part3/lstmlm.py,0,"b'from pytensor.data.ptb import *\nfrom pytensor.tutorial.part3.trainer import *\n\nclass LSTMLM(Graph):\n\n    def __init__(self, vocab_size, input_size, hidden_size):\n        super().__init__(""LSTM"")\n\n        # embedding size\n        self.vocab_size = vocab_size\n        self.word_dim = input_size\n\n        # network size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = vocab_size\n\n        # num steps\n        self.max_num_steps = 100\n        self.num_steps = 0\n\n        # word embedding\n        embed_argument = {\'vocab_size\': self.vocab_size, \'embed_size\': self.input_size}\n        self.word_embedding = self.get_operation(\'Embedding\', embed_argument)\n\n        # rnn\n        rnn_argument = {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, \'max_num_steps\': self.max_num_steps}\n        self.rnn = self.get_operation(\'LSTM\', rnn_argument)\n\n        # affines\n        affine_argument = {\'input_size\': self.hidden_size, \'hidden_size\': self.output_size}\n        self.affines = [self.get_operation(\'Affine\', affine_argument, ""Affine"") for i in range(self.max_num_steps)]\n\n        # softmax\n        self.softmaxLosses = [self.get_operation(\'SoftmaxLoss\') for i in range(self.max_num_steps)]\n\n    def forward(self, word_lst):\n\n        # get num steps\n        self.num_steps = min(len(word_lst), self.max_num_steps)\n\n        # create embeddings\n        embedding_tensors = []\n        for word_id in word_lst:\n            embedding_tensors.append(self.word_embedding.forward([LongTensor([word_id])]))\n\n        # run RNN\n        rnn_tensors = self.rnn.forward(embedding_tensors)\n\n        # softmax tensors\n        softmax_tensors = []\n\n        for i in range(self.num_steps):\n            output_tensor = self.affines[i].forward(rnn_tensors[i])\n            softmax_tensor = self.softmaxLosses[i].forward(output_tensor)\n            softmax_tensors.append(softmax_tensor)\n\n        return softmax_tensors\n\n    def loss(self, target_ids):\n\n        ce_loss = 0.0\n\n        for i in range(self.num_steps):\n            cur_ce_loss = self.softmaxLosses[i].loss(LongTensor([target_ids[i]]))\n            ce_loss += cur_ce_loss\n\n        return ce_loss\n\n\n\ndef lstm_train():\n\n    sentences, vocab = load_ptb()\n    input_lst = []\n    output_lst = []\n\n    for sentence in sentences:\n        input_ids = sentence[:-1]\n        output_ids = sentence[1:]\n\n        input_lst.append(input_ids)\n        output_lst.append(output_ids)\n\n    model = LSTMLM(10000, 100, 100)\n    trainer = Trainer(model)\n\n    trainer.train(input_lst, output_lst, None, None)\n\n\nif __name__ == \'__main__\':\n    lstm_train()\n\n\n'"
pytensor/tutorial/part3/rnnlm.py,0,"b'from pytensor.data.ptb import *\nfrom pytensor.tutorial.part3.trainer import *\n\nclass RNNLM(Graph):\n\n    def __init__(self, vocab_size, input_size, hidden_size):\n        super().__init__(\'RNN\')\n\n        # embedding size\n        self.vocab_size = vocab_size\n        self.word_dim = input_size\n\n        # network size\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = vocab_size\n\n        # num steps\n        self.max_num_steps = 100\n        self.num_steps = 0\n\n        # word embedding\n        embed_argument = {\'vocab_size\': self.vocab_size, \'embed_size\': self.input_size}\n        self.word_embedding = self.get_operation(\'Embedding\', embed_argument)\n\n        # rnn\n        rnn_argument = {\'input_size\': self.input_size, \'hidden_size\': self.hidden_size, \'max_num_steps\': self.max_num_steps}\n        self.rnn = self.get_operation(\'RNN\', rnn_argument)\n\n        # affines\n        affine_argument = {\'input_size\': self.hidden_size, \'hidden_size\': self.output_size}\n        self.affines = [self.get_operation(\'Affine\', affine_argument, ""Affine"") for i in range(self.max_num_steps)]\n\n        # softmax\n        self.softmaxLosses = [self.get_operation(\'SoftmaxLoss\') for i in range(self.max_num_steps)]\n\n    def forward(self, word_lst):\n\n        # get num steps\n        self.num_steps = min(len(word_lst), self.max_num_steps)\n\n        # create embeddings\n        embedding_tensors = []\n        for word_id in word_lst:\n            embedding_tensors.append(self.word_embedding.forward([LongTensor([word_id])]))\n\n        # run RNN\n        rnn_tensors = self.rnn.forward(embedding_tensors)\n\n        # softmax tensors\n        softmax_tensors = []\n\n        for i in range(self.num_steps):\n            output_tensor = self.affines[i].forward(rnn_tensors[i])\n            softmax_tensor = self.softmaxLosses[i].forward(output_tensor)\n            softmax_tensors.append(softmax_tensor)\n\n        return softmax_tensors\n\n    def loss(self, target_ids):\n\n        ce_loss = 0.0\n\n        for i in range(self.num_steps):\n            cur_ce_loss = self.softmaxLosses[i].loss(LongTensor([target_ids[i]]))\n            ce_loss += cur_ce_loss\n\n        return ce_loss\n\ndef rnn_train():\n\n    sentences, vocab = load_ptb()\n    input_lst = []\n    output_lst = []\n\n    for sentence in sentences:\n        input_ids = sentence[:-1]\n        output_ids = sentence[1:]\n\n        input_lst.append(input_ids)\n        output_lst.append(output_ids)\n\n\n    model = RNNLM(10000, 100, 100)\n    trainer = Trainer(model)\n\n    trainer.train(input_lst, output_lst, None, None)\n\n\nif __name__ == \'__main__\':\n    rnn_train()\n\n\n'"
pytensor/tutorial/part3/trainer.py,1,"b'from pytensor.network.graph import *\nfrom pytensor.network.gradient import *\nfrom pytensor.network.optimizer import *\n\nclass Trainer:\n\n    def __init__(self, model):\n        """"""\n        A trainer example using graph for autodiff\n        :param model:\n        """"""\n\n        self.model = model\n        self.optimizer = SGD(self.model.parameter)\n\n    def train(self, x_train, y_train, x_test=None, y_test=None, epoch=40, iteration=10):\n\n        for ii in range(epoch):\n\n            self.model.train()\n\n            loss = 0.0\n            it_loss = 0.0\n            word_cnt = 0\n            it_word_cnt = 0\n\n            for i in range(len(x_train)):\n\n                self.optimizer.zero_grad()\n\n                # extract data set\n                input_tensors = x_train[i]\n                target_tensor = y_train[i]\n\n                word_cnt += len(x_train[i])\n                it_word_cnt += len(x_train[i])\n\n                # dynamic forward\n                self.model.forward(input_tensors)\n                cur_loss = self.model.loss(target_tensor)\n\n                it_loss += cur_loss\n                loss += cur_loss\n\n                # automatic differentiation\n                self.model.backward()\n\n                # optimization\n                self.optimizer.step()\n\n                if (i+1) % iteration == 0:\n                    # report iteration\n                    print(""\\repoch {} iteration ({}/{}) : loss {} "".format(ii,i+1,len(x_train), it_loss/it_word_cnt), end=\'\')\n\n                    # clear ce loss\n                    it_loss = 0.0\n                    it_word_cnt = 0\n\n            train_ppl = 2**(loss / word_cnt)\n            print(""epoch {}:  train_ppl "".format(ii, train_ppl))\n\n    def test(self, x_test, y_test):\n\n        self.model.eval()\n\n        acc_cnt = 0.0\n        all_cnt = len(x_test)\n\n        for i in range(len(x_test)):\n\n            v = Tensor([x_test[i]])\n            output_tensor = self.model.forward(v)\n\n            y = np.argmax(output_tensor.value[0])\n            if y == y_test[i]:\n                acc_cnt += 1.0\n\n        print(""test accuracy "", acc_cnt / all_cnt)\n'"
