file_path,api_count,code
LARS.py,16,"b""\n# coding: utf-8\n\n# In[221]:\n\nget_ipython().magic('matplotlib inline')\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy import linalg\nfrom scipy.linalg.lapack import get_lapack_funcs\n\nfrom sklearn import linear_model\n\n\n# In[177]:\n\nX, y = [[-1, 1], [0, 0], [1, 1]], [-1.1111, 0, -1.1111]\n\nX = np.array(X)\n\ny = np.array(y)\ny = y.reshape(-1, 1)\n\n\n# ### Linear regression\n\n# In[178]:\n\nmdl = linear_model.LinearRegression().fit(X, y)\nA = mdl.coef_\nb = mdl.intercept_\n\nnp.allclose(np.dot(X, A.T) + b, y)\n\n\n# ### Lars in sklearn\n\n# In[179]:\n\nmdl = linear_model.Lars().fit(X, y)\nA = mdl.coef_\nb = mdl.intercept_\n\nnp.allclose(np.dot(X, A) + b, y.T)\n\n\n# ### Inspect lars\n\n# #### inside lars\n\n# In[186]:\n\nmdl.fit\n\n\n# In[194]:\n\ny.shape\n\n\n# In[196]:\n\nlinear_model.least_angle.lars_path(X, y)\n\n\n# In[200]:\n\nXy=None\nGram=None\nmax_iter=500\nalpha_min=0\nmethod='lar'\ncopy_X=True,\neps=np.finfo(np.float).eps\ncopy_Gram=True\nverbose=0\nreturn_path=True\nreturn_n_iter=False\npositive=False\n\n\n# In[204]:\n\nn_features = X.shape[1]\nn_samples = y.size\nmax_features = min(max_iter, n_features)\n\nif return_path:\n    coefs = np.zeros((max_features + 1, n_features))\n    alphas = np.zeros(max_features + 1)\nelse:\n    coef, prev_coef = np.zeros(n_features), np.zeros(n_features)\n    alpha, prev_alpha = np.array([0.]), np.array([0.])  # better ideas?\n\nn_iter, n_active = 0, 0\nactive, indices = list(), np.arange(n_features)\n# holds the sign of covariance\nsign_active = np.empty(max_features, dtype=np.int8)\ndrop = False\n\n\n# In[205]:\n\nreturn_path\n\n\n# In[206]:\n\nX.shape\n\n\n# In[207]:\n\ny.size\n\n\n# In[212]:\n\nsign_active\n\n\n# In[214]:\n\nactive, indices\n\n\n# In[222]:\n\nL = np.zeros((max_features, max_features), dtype=X.dtype)\nswap, nrm2 = linalg.get_blas_funcs(('swap', 'nrm2'), (X,))\nsolve_cholesky, = get_lapack_funcs(('potrs',), (X,))\n\n\n# In[224]:\n\nXy, Gram\n\n\n# In[225]:\n\ncopy_X\n\n\n# In[227]:\n\nX = X.copy('F')\n\n\n# In[228]:\n\n\n\n\n# In[229]:\n\nif Xy is None:\n    Cov = np.dot(X.T, y)\n\n\n# In[230]:\n\nX.T\n\n\n# In[231]:\n\ny\n\n\n# In[232]:\n\nCov.size\n\n\n# In[233]:\n\npositive\n\n\n# In[235]:\n\nif Cov.size:\n    if positive:\n        C_idx = np.argmax(Cov)\n    else:\n        C_idx = np.argmax(np.abs(Cov))\n\n    C_ = Cov[C_idx]\n    if positive:\n        C = C_\n    else:\n        C = np.fabs(C_)\n\n\n# In[236]:\n\nmethod\n\n\n# In[238]:\n\nreturn_path\n\n\n# In[239]:\n\nmdl.coef_\n\n\n# In[240]:\n\nmdl.coef_path_\n\n\n# In[ ]:\n\n\n\n"""
numpy_newaxis.py,25,"b""\n# coding: utf-8\n\n# In[95]:\n\nimport numpy as np\n\n\n# In[96]:\n\ndef show_array(y):\n    print('array:', y)\n    print('array.ndim:', y.ndim)\n    print('array.shape:', y.shape)\n\n\n# ### 0-D\n\n# In[97]:\n\nx = np.array(5)\nshow_array(x)\n\n\n# #### 0-D to 1-D\n\n# In[98]:\n\ny = np.array(x)[np.newaxis]\nshow_array(y)\n\n\n# In[99]:\n\ny = np.expand_dims(x, axis=0)\nshow_array(y)\n\n\n# Any number >= 0 does the same.\n\n# In[100]:\n\ny = np.expand_dims(x, axis=123456)\nshow_array(y)\n\n\n# In[101]:\n\ny = x.reshape(-1,)\nshow_array(y)\n\n\n# #### 0-D to 2-D\n\n# In[102]:\n\ny = np.array(x)[np.newaxis, np.newaxis]\nshow_array(y)\n\n\n# In[103]:\n\ny = np.expand_dims(x, axis=0)\ny = np.expand_dims(y, axis=0)\nshow_array(y)\n\n\n# In[104]:\n\ny = x.reshape(-1, 1)\nshow_array(y)\n\n\n# ### 1-D\n\n# In[105]:\n\nx = np.array([5, 6, 7])\nshow_array(x)\n\n\n# #### 1-D to 2-D\n\n# ##### Vector to row matrix\n\n# In[106]:\n\ny = np.array(x)[np.newaxis, :]\nshow_array(y)\n\n\n# In[107]:\n\ny = np.array(x)[np.newaxis] # This is short hand of y = np.array(x)[np.newaxis, :]\nshow_array(y)\n\n\n# In[108]:\n\ny = np.expand_dims(x, axis=0)\nshow_array(y)\n\n\n# In[109]:\n\ny = x.reshape(1, -1)\nshow_array(y)\n\n\n# ##### Vector to column matrix\n\n# In[110]:\n\ny = np.array(x)[:, np.newaxis]\nshow_array(y)\n\n\n# In[111]:\n\ny = np.expand_dims(x, axis=1)\nshow_array(y)\n\n\n# Any number >= 1 does the same.\n\n# In[112]:\n\ny = np.expand_dims(x, axis=123456)\nshow_array(y)\n\n\n# In[113]:\n\ny = x.reshape(-1, 1)\nshow_array(y)\n\n\n# ### 2-D\n\n# In[114]:\n\nx = np.array([[1, 2, 3], [4, 5, 6]])\nshow_array(x)\n\n\n# #### 2-D to 3-D\n\n# ##### Case 1\n\n# In[115]:\n\ny = np.array(x)[np.newaxis, :, :]\nshow_array(y)\n\n\n# In[116]:\n\ny = np.array(x)[np.newaxis, :]\nshow_array(y)\n\n\n# In[117]:\n\ny = np.array(x)[np.newaxis]\nshow_array(y)\n\n\n# In[118]:\n\ny = np.expand_dims(x, axis=0)\nshow_array(y)\n\n\n# In[119]:\n\ny = x.reshape(-1, 2, 3)\nshow_array(y)\n\n\n# In[126]:\n\ny = x.reshape(-1, *x.shape)\nshow_array(y)\n\n\n# ##### Case 2\n\n# In[121]:\n\ny = np.array(x)[:, np.newaxis, :]\nshow_array(y)\n\n\n# In[122]:\n\ny = np.array(x)[:, np.newaxis]\nshow_array(y)\n\n\n# In[123]:\n\ny = np.expand_dims(x, axis=1)\nshow_array(y)\n\n\n# In[124]:\n\ny = x.reshape(2, 1, 3)\nshow_array(y)\n\n\n# In[127]:\n\ny = x.reshape(x.shape[0], -1, x.shape[1])\nshow_array(y)\n\n\n# ##### Case 3\n\n# In[24]:\n\ny = np.array(x)[:, :, np.newaxis]\nshow_array(y)\n\n\n# In[25]:\n\ny = np.expand_dims(x, axis=2)\nshow_array(y)\n\n\n# Any number >= 2 does the same.\n\n# In[26]:\n\ny = np.expand_dims(x, axis=123456)\nshow_array(y)\n\n\n# In[128]:\n\ny = x.reshape(*x.shape, -1)\nshow_array(y)\n\n\n# In[ ]:\n\n\n\n"""
numpy_tips.py,40,"b'\n# coding: utf-8\n\n# In[3]:\n\nimport numpy as np\n\n\n# ### ndarray.size\n\n# In[9]:\n\na = np.arange(12).reshape(3, 4)\n\nprint(a.shape)\nprint(a.size)\n\n\n# ### `np.fromfunction()` \n\n# In[10]:\n\ndef f(i, j):\n    return i*100 + j\n\nnp.fromfunction(f, (3, 4), dtype=int)\n\n\n# ###  One-dimensional `ndarray[start:end:step]` \n\n# In[14]:\n\ntype(np.arange(10))\n\n\n# In[15]:\n\nnp.arange(100)[0:50:3]\n\n\n# In[16]:\n\nnp.arange(100)[::3]\n\n\n# In[104]:\n\nnp.arange(100)[::-3]\n\n\n# In[105]:\n\nnp.arange(100)[::-1]\n\n\n# In[107]:\n\nprint range(100)[::-1]\n\n\n# ### Multidimensional ndarray indexing using ""`...`""\n\n# In[18]:\n\ndef f(i, j, k, m):\n    return (i+1)*1000 + (j+1)*100 + (k+1)*10 + (m+1)\n\nb = np.fromfunction(f, (3, 4, 5, 6), dtype=int)\nprint(b.shape)\nprint(b)\n\n\n# In[158]:\n\nb[2]\n\n\n# In[177]:\n\nb[2, :, :, 3]\n\n\n# In[178]:\n\nb[2,..., 3]\n\n\n# ### flatten\n\n# In[41]:\n\nb = np.arange(12).reshape(3, 4)\n\n\n# In[42]:\n\nb.reshape(b.size,)\n\n\n# In[24]:\n\nb.ravel()\n\n\n# In[27]:\n\nbf = b.flat\nprint(bf)\n[i for i in bf]\n\n\n# ### `resize()`\n\n# In[29]:\n\na = np.arange(12).reshape(3, 4)\nprint(a)\na.resize(4, 3)\nprint(a)\n\n\n# ### `reshape(i,-1)`\n\n# In[35]:\n\na = np.arange(12).reshape(3, 4)\nprint(a)\nprint(a.reshape(4, -1))\nprint(a.reshape(-1,))\n\n\n# ### `squeeze()`\n\n# In[37]:\n\nnp.array([[[1, 2]]]).squeeze()\n\n\n# ### newaxis\n\n# In[3]:\n\nimport numpy as np\nfrom numpy import newaxis\n\n\n# In[5]:\n\na = np.array([1, 2])\na\n\n\n# In[6]:\n\na.reshape(2, 1)\n\n\n# In[7]:\n\na[:, newaxis]\n\n\n# In[9]:\n\na[newaxis, :]\na[newaxis, :].shape\n\n\n# In[10]:\n\na[:, newaxis, newaxis]\na[:, newaxis, newaxis].shape\n\n\n# ### Stacking\n\n# In[243]:\n\nnp.vstack((np.array([1,2]), np.array([3, 4])))\n\n\n# In[244]:\n\nnp.hstack((np.array([1,2]), np.array([3, 4])))\n\n\n# In[246]:\n\nnp.vstack((np.array([1,2]).reshape(2,1), np.array([3, 4]).reshape(2,1)))\n\n\n# In[247]:\n\nnp.hstack((np.array([1,2]).reshape(2,1), np.array([3, 4]).reshape(2,1)))\n\n\n# In[248]:\n\nnp.column_stack((np.array([1,2]), np.array([3, 4])))\n\n\n# ### `c_[]`\n\n# In[18]:\n\na1 = np.arange(10)\na2 = np.arange(10, 20)\n\nb = np.concatenate([a1.reshape(-1,1), a2.reshape(-1,1)], axis=1)\nb\n\n\n# In[17]:\n\nb = np.c_[a1, a2]\nb\n\n\n# ### Assign, view, and copy\n\n# In[249]:\n\na = np.arange(12)\n\n\n# In[250]:\n\nb = a\nc = a.view()\nd = a.copy()\n\n\n# In[252]:\n\nb is a\nc is a\nc.base is a\n\n\n# In[253]:\n\nb.shape = 3, 4\n\n\n# In[254]:\n\na.shape\n\n\n# #### View: share data but different shape\n\n# In[257]:\n\nc.resize(2, 6)\nc[0,4]=1234\n\n\n# In[258]:\n\nc\n\n\n# In[259]:\n\na\n\n\n# ### Indexing by array \n\n# In[267]:\n\na = np.arange(12)*2\na\n\n\n# In[270]:\n\nb = np.array([[1, 3, 7, 5]])\na[b]\n\n\n# In[271]:\n\nb = np.array([[1, 3], [7, 5]])\na[b]\n\n\n# #### `mgrid`\n\n# In[274]:\n\nnp.mgrid[0:5,0:5]\n\n\n# In[275]:\n\nnp.mgrid[0:5:1,0:5:1]\n\n\n# In[278]:\n\nnp.mgrid[0:5:3j,0:5:10j]\n\n\n# In[279]:\n\nnp.ogrid[0:5, 0:5]\n\n\n# In[280]:\n\nnp.ogrid[0:5:3j,0:5:10j]\n\n\n# ### `vectorize()`\n\n# In[282]:\n\ndef foo(x, y):\n    return max(x-y, y-x)\n\n\n# In[284]:\n\nvec_foo = np.vectorize(foo)\n\n\n# In[287]:\n\nvec_foo([1, 3, 5], [2, 1, 5])\n\n\n# ### `cast[\'type\']()`\n\n# In[292]:\n\ntype(np.pi)\n\n\n# In[288]:\n\nnp.cast[\'i\'](np.pi)\n\n\n# In[295]:\n\nnp.cast[\'f\'](np.pi)\n\n\n# ### `select()`\n\n# In[299]:\n\nx = np.arange(10)\n\n\n# In[300]:\n\nx\n\n\n# In[308]:\n\nnp.select([x<3, x>5], [x, x**2], default=12345)\n\n\n# ### Use `np.source()` to see sourcecode\n\n# In[310]:\n\nnp.source(np.select)\n\n\n# ### References\n# - https://docs.scipy.org/doc/numpy-dev/user/quickstart.html\n# - https://docs.scipy.org/doc/scipy/reference/tutorial/basic.html\n'"
plotting.py,0,"b""\n# coding: utf-8\n\n# In[1]:\n\nget_ipython().magic('matplotlib inline')\n\n\n# In[2]:\n\nimport matplotlib.pyplot as plt\n\n\n# ### Example with size and legend\n\n# In[3]:\n\ndef average(series):\n    return float(sum(series)) / len(series)\n\n\n# moving average using n last points\ndef moving_average(series, n):\n    return average(series[-n:])\n\n\ndef average(series, n=None):\n    if n is None:\n        return average(series, len(series))\n    return float(sum(series[-n:])) / n\n\n\n# weighted average, weights is a list of weights\ndef weighted_average(series, weights):\n    result = 0.0\n    weights.reverse()\n    for n in range(len(weights)):\n        result += series[-n - 1] * weights[n]\n    return result\n\n\nweights = [0.1, 0.2, 0.3, 0.4]\n\n\n# given a series and alpha, return series of smoothed points\ndef exponential_smoothing(series, alpha):\n    result = [series[0]]  # first value is same as series\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n - 1])\n    return result\n\n\n# given a series and alpha, return series of smoothed points\ndef double_exponential_smoothing(series, alpha, beta):\n    result = [series[0]]\n    for n in range(1, len(series) + 1):\n        if n == 1:\n            level, trend = series[0], series[1] - series[0]\n        if n >= len(series):  # we are forecasting\n            value = result[-1]\n        else:\n            value = series[n]\n        last_level, level = level, alpha * value + (1 - alpha) * (level + trend\n                                                                  )\n        trend = beta * (level - last_level) + (1 - beta) * trend\n        result.append(level + trend)\n    return result\n\n\nseries = [\n    30, 21, 29, 31, 40, 48, 53, 47, 37, 39, 31, 29, 17, 9, 20, 24, 27, 35, 41,\n    38, 27, 31, 27, 26, 21, 13, 21, 18, 33, 35, 40, 36, 22, 24, 21, 20, 17, 14,\n    17, 19, 26, 29, 40, 31, 20, 24, 18, 26, 17, 9, 17, 21, 28, 32, 46, 33, 23,\n    28, 22, 27, 18, 8, 17, 21, 31, 34, 44, 38, 31, 30, 26, 32\n]\nn = len(series)\n\n\ndef initial_trend(series, slen):\n    sum = 0.0\n    for i in range(slen):\n        sum += float(series[i + slen] - series[i]) / slen\n    return sum / slen\n\n\ndef initial_seasonal_components(series, slen):\n    seasonals = {}\n    season_averages = []\n    n_seasons = int(len(series) / slen)\n    # compute season averages\n    for j in range(n_seasons):\n        season_averages.append(\n            sum(series[slen * j:slen * j + slen]) / float(slen))\n    # compute initial values\n    for i in range(slen):\n        sum_of_vals_over_avg = 0.0\n        for j in range(n_seasons):\n            sum_of_vals_over_avg += series[slen * j + i] - season_averages[j]\n        seasonals[i] = sum_of_vals_over_avg / n_seasons\n    return seasonals\n\n\ndef triple_exponential_smoothing(series, slen, alpha, beta, gamma, n_preds):\n    result = []\n    seasonals = initial_seasonal_components(series, slen)\n    for i in range(len(series) + n_preds):\n        if i == 0:  # initial values\n            smooth = series[0]\n            trend = initial_trend(series, slen)\n            result.append(series[0])\n            continue\n        if i >= len(series):  # we are forecasting\n            m = i - len(series) + 1\n            result.append((smooth + m * trend) + seasonals[i % slen])\n        else:\n            val = series[i]\n            last_smooth, smooth = smooth, alpha * (\n                val - seasonals[i % slen]) + (1 - alpha) * (smooth + trend)\n            trend = beta * (smooth - last_smooth) + (1 - beta) * trend\n            seasonals[i % slen] = gamma * (val - smooth) + (\n                1 - gamma) * seasonals[i % slen]\n            result.append(smooth + trend + seasonals[i % slen])\n    return result\n\n\n# Reference:\n# - https://grisha.org/blog/2016/01/29/triple-exponential-smoothing-forecasting/\n\n# In[4]:\n\nfig, ax = plt.subplots(figsize=(20, 5)) \nax.plot(series, '-o', label='Time Series')\nax.set_ylim([-5, 70])\nax.plot([n-1, n], [series[-1], average(series)], '-o', label='average')\nax.plot([n-1, n], [series[-1], moving_average(series, 3)], '-o', label='moving average 3')\nax.plot([n-1, n], [series[-1], weighted_average(series, weights)], '-o', label='weighted moving average 4')\nax.plot(exponential_smoothing(series, 0.1), '-o', label='exponential_smoothing 0.1')\nax.plot(exponential_smoothing(series, 0.9), '-o', label='exponential_smoothing 0.9')\nax.plot(double_exponential_smoothing(series, alpha=0.9, beta=0.9), '-o', label='double_exponential_smoothing')\nax.plot(triple_exponential_smoothing(series, 12, 0.716, 0.029, 0.993, 24), '-o', label='triple_exponential_smoothing')\nax.legend(loc=4)\n\n\n# ### Change plot style\n\n# In[ ]:\n\nplt.style.available\n\n\n# In[5]:\n\nplt.style.use('ggplot')\n\n\n# In[6]:\n\nfig, ax = plt.subplots(figsize=(20, 5)) \nax.plot(series, '-o', label='Time Series')\nax.set_ylim([-5, 70])\nax.plot([n-1, n], [series[-1], average(series)], '-o', label='average')\nax.plot([n-1, n], [series[-1], moving_average(series, 3)], '-o', label='moving average 3')\nax.plot([n-1, n], [series[-1], weighted_average(series, weights)], '-o', label='weighted moving average 4')\nax.plot(exponential_smoothing(series, 0.1), '-o', label='exponential_smoothing 0.1')\nax.plot(exponential_smoothing(series, 0.9), '-o', label='exponential_smoothing 0.9')\nax.plot(double_exponential_smoothing(series, alpha=0.9, beta=0.9), '-o', label='double_exponential_smoothing')\nax.plot(triple_exponential_smoothing(series, 12, 0.716, 0.029, 0.993, 24), '-o', label='triple_exponential_smoothing')\nax.legend(loc=4)\n\n\n# ### Reference:\n# - http://pbpython.com/effective-matplotlib.html\n"""
python_tips.py,4,"b""\n# coding: utf-8\n\n# In[2]:\n\nfrom ds_utils.imports import *\n\n\n# ### `expanduser`\n\n# In[6]:\n\nos.path.expanduser(os.path.join('~', 'tmp'))\n\n\n# ### Plot image data\n\n# In[30]:\n\nplt.imshow(np.random.randn(1000, 1000, 3));\n\n\n# In[31]:\n\nplt.imshow(np.random.randn(1000, 1000));\n\n\n# In[32]:\n\nplt.imshow(np.random.randn(1000, 1000), cmap=plt.cm.gray_r);\n\n\n# In[39]:\n\nfrom sklearn import datasets\ndigits = sklearn.datasets.load_digits()\nimage = digits.images[np.random.choice(digits.images.shape[0])]\n\n\n# In[40]:\n\nplt.imshow(image)\n\n\n# In[41]:\n\nplt.imshow(image, cmap=plt.cm.gray_r)\n\n\n# In[ ]:\n\n\n\n"""
rpy2_demo.py,0,"b'\n# coding: utf-8\n\n# In[ ]:\n\nget_ipython().magic(\'matplotlib inline\')\n\n\n# # R packages\n\n# ## Importing packages\n\n# In[ ]:\n\nfrom rpy2.robjects.packages import importr\n\n\n# In[ ]:\n\nbase = importr(\'base\')\n\n\n# In[ ]:\n\nutils = importr(\'utils\')\n\n\n# ## Installing packages\n\n# In[2]:\n\nimport rpy2.robjects.packages as rpackages\n\n\n# In[3]:\n\nutils = rpackages.importr(\'utils\')\n\n\n# In[4]:\n\nutils.chooseCRANmirror(ind=1)\n\n\n# In[5]:\n\npacknames = (\'gbm\', \'glmnet\')\n\n\n# In[6]:\n\nfrom rpy2.robjects.vectors import StrVector\n\n\n# In[7]:\n\nnames_to_install = [x for x in packnames if not rpackages.isinstalled(x)]\n\n\n# In[8]:\n\nif len(names_to_install)>0:\n    utils.install_packages(StrVector(names_to_install))\n\n\n# In[9]:\n\nimportr(\'glmnet\')\n\n\n# # The r instance\n\n# In[ ]:\n\nfrom rpy2 import robjects\n\n\n# In[ ]:\n\npi = robjects.r(\'pi\')\n\n\n# In[ ]:\n\npi\n\n\n# In[ ]:\n\ntype(pi)\n\n\n# In[ ]:\n\nprint pi\n\n\n# In[ ]:\n\npi[0]\n\n\n# In[ ]:\n\nrobjects.r(\'\'\'\n        # create a function `f`\n        f <- function(r, verbose=FALSE) {\n            if (verbose) {\n                cat(""I am calling f().\\n"")\n            }\n            2 * pi * r\n        }\n        # call the function `f` with argument value 3\n        f(3)\n        \'\'\')\n\n\n# In[ ]:\n\nr_f = robjects.globalenv[\'f\']\n\n\n# In[ ]:\n\nprint r_f.r_repr()\n\n\n# In[ ]:\n\nr_f = robjects.r[\'f\']\n\n\n# In[ ]:\n\nres = r_f(3)\n\n\n# In[ ]:\n\nres\n\n\n# ## Interpolating R objects into R code strings\n\n# In[ ]:\n\nletters = robjects.r[\'letters\']\n\n\n# In[ ]:\n\nrcode = \'paste(%s, collapse=""--"")\' % (letters.r_repr())\n\n\n# In[ ]:\n\nres = robjects.r(rcode)\n\n\n# In[ ]:\n\nprint res\n\n\n# # R vectors\n\n# In[ ]:\n\nres = robjects.StrVector([\'abc\', \'def\'])\nprint res.r_repr()\n\n\n# In[ ]:\n\nres = robjects.IntVector([1, 2.8, 3])\nprint res.r_repr()\n\n\n# In[ ]:\n\nres = robjects.FloatVector([1.1, 2, 3])\nprint res.r_repr()\n\n\n# In[ ]:\n\n[1.1, 2, 3]\n\n\n# # Calling R functions\n\n# In[ ]:\n\nrsum = robjects.r[\'sum\']\n\n\n# In[ ]:\n\nrsum\n\n\n# In[ ]:\n\nrsum(robjects.IntVector([1, 2, 3]))[0]\n\n\n# In[ ]:\n\nrobjects.r.sum(robjects.IntVector([1, 2, 3]))\n\n\n# # Examples\n\n# ## Graphics and plots\n\n# In[ ]:\n\nr = robjects.r\n\nx = robjects.IntVector(range(10))\n\nprint x\n\n\n# In[ ]:\n\ny = r.rnorm(10)\n\nr.rnorm\n\nr.layout(r.matrix(robjects.IntVector([1,2,3,2]), nrow=2))\n\nr.plot(r.runif(10), y, xlab=\'runif\', ylab=\'foo/bar\', col=\'red\')\n\n\n# ## Linear models\n\n# R code:\n# ```\n# ctl <- c(4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14)\n# trt <- c(4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69)\n# group <- gl(2, 10, 20, labels = c(""Ctl"",""Trt""))\n# weight <- c(ctl, trt)\n# \n# anova(lm.D9 <- lm(weight ~ group))\n# \n# summary(lm.D90 <- lm(weight ~ group - 1))# omitting intercept\n# ```\n\n# In[ ]:\n\nfrom rpy2.robjects import FloatVector\nfrom rpy2.robjects.packages import importr\nstats = importr(\'stats\')\nbase = importr(\'base\')\n\n\n# In[ ]:\n\nctl = FloatVector([4.17,5.58,5.18,6.11,4.50,4.61,5.17,4.53,5.33,5.14])\ntrt = FloatVector([4.81,4.17,4.41,3.59,5.87,3.83,6.03,4.89,4.32,4.69])\ngroup = base.gl(2, 10, 20, labels = [""Ctl"",""Trt""])\nweight = ctl + trt\n\n\n# In[ ]:\n\nrobjects.globalenv[""weight""] = weight\nrobjects.globalenv[""group""] = group\nlm_D9 = stats.lm(""weight ~ group"")\nprint stats.anova(lm_D9)\n\n\n# In[ ]:\n\n# omitting the intercept\nlm_D90 = stats.lm(""weight ~ group - 1"")\nprint base.summary(lm_D90)\n\n\n# In[ ]:\n\nlm_D9.rclass\n\n\n# In[ ]:\n\nlm_D9\n\n\n# In[ ]:\n\nlm_D9.r_repr()\n\n\n# In[ ]:\n\nprint lm_D9.rclass\n\n\n# In[ ]:\n\nprint lm_D9.names\n\n\n# # References: \n# - [Introduction to rpy2](https://rpy2.readthedocs.io/en/version_2.8.x/introduction.html#introduction-to-rpy2)\n\n# In[ ]:\n\n\n\n'"
scikit_learn_build_estimators.py,0,"b""\n# coding: utf-8\n\n# ## Check if estimator adhere to sklearn interface using `check_estimator`\n\n# In[1]:\n\nfrom sklearn.utils.estimator_checks import check_estimator\nfrom sklearn.svm import LinearSVC\ncheck_estimator(LinearSVC)\n\n\n# ## Templates provided by [scikit-learn-contrib project](https://github.com/scikit-learn-contrib/project-template/blob/master/skltemplate/template.py)\n\n# In[2]:\n\nimport sys\nsys.path.insert(1, '/Users/yangzhang/git/project-template/skltemplate/')\nfrom template import TemplateEstimator\ncheck_estimator(TemplateEstimator)\n\n\n# ## Use template on data\n\n# In[3]:\n\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX_train = iris.data\ny_train = iris.target\n\n\n# In[4]:\n\nfrom template import TemplateClassifier\n\n\n# In[5]:\n\nmdl = TemplateClassifier()\nmdl.fit(X_train, y_train)\nmdl.predict(X_train)\n\n\n# ## Use template in cross validation\n\n# In[7]:\n\nimport sklearn.model_selection\nmdl = TemplateClassifier()\nsklearn.model_selection.cross_val_score(mdl, X_train, y_train, cv=5)\n\n\n# ## Use template in a pipeline\n\n# In[8]:\n\nimport sklearn.pipeline\nimport sklearn.preprocessing\npipeline = sklearn.pipeline.make_pipeline(\n    sklearn.preprocessing.StandardScaler(),    \n    TemplateClassifier(),\n)\n\n\n# In[10]:\n\npipeline.fit(X_train, y_train)\n\n\n# In[11]:\n\npipeline.predict(X_train)\n\n\n# ## Use template in a pipeline in cross validation\n\n# In[12]:\n\nsklearn.model_selection.cross_val_score(pipeline, X_train, y_train, cv=5)\n\n\n# ## References\n# - http://scikit-learn.org/dev/developers/contributing.html#rolling-your-own-estimator\n# - http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/\n"""
scikit_learn_metrics.py,5,"b'\n# coding: utf-8\n\n# # Setup\n\n# In[136]:\n\nimport pandas as pd\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nimport sklearn.dummy\nimport sklearn.linear_model\nimport sklearn.preprocessing\n\n\n# # logloss\n\n# ### Binary\n\n# In[42]:\n\niris = sklearn.datasets.load_iris()\n\n\n# In[62]:\n\nind_bin = iris.target < 2\nX = iris.data[ind_bin]\ny = iris.target[ind_bin]\n\n\n# In[63]:\n\nsklearn.metrics.log_loss(y,  y)\n\n\n# In[64]:\n\nsklearn.metrics.log_loss(y,  np.clip(y, 0.05, 0.95))\n\n\n# In[65]:\n\nsklearn.metrics.log_loss(y,  np.ones_like(y)*y.mean())\n\n\n# In[86]:\n\ny_duplicated = np.concatenate([y]*5)\n\nsklearn.metrics.log_loss(y_duplicated,  np.ones_like(y_duplicated)*y.mean())\n\n\n# In[69]:\n\nsklearn.metrics.log_loss(y,  np.random.uniform(low=0.0, high=1.0, size=len(y)))\n\n\n# In[83]:\n\ny\n\n\n# In[88]:\n\ndef flip(y):\n    return -(y-0.5) + 0.5\n\n\n# In[89]:\n\ny_flip = flip(y)\nsklearn.metrics.log_loss(y,  y_flip)\n\n\n# In[90]:\n\ny_duplicated_flip = flip(y_duplicated)\nsklearn.metrics.log_loss(y_duplicated,  y_duplicated_flip)\n\n\n# In[108]:\n\nmdl = sklearn.dummy.DummyClassifier()\nmdl = sklearn.linear_model.LogisticRegression()\nmdl = sklearn.svm.SVC(probability=True)\n\n\n# In[109]:\n\nmdl.fit(X, y)\n\n\n# In[110]:\n\npred = mdl.predict(X)\npred_prob = mdl.predict_proba(X)\n\n\n# In[111]:\n\nsklearn.metrics.log_loss(y, pred)\nsklearn.metrics.log_loss(y, pred_prob)\n\n\n# ### Multiple class\n\n# In[120]:\n\nX = iris.data\ny = iris.target\n\n\n# In[155]:\n\ny_encoded = sklearn.preprocessing.LabelBinarizer().fit_transform(y)\nsklearn.metrics.log_loss(y, y_encoded)\n\n\n# In[177]:\n\nmdl = sklearn.dummy.DummyClassifier()\n# mdl = sklearn.linear_model.LogisticRegression()\n# mdl = sklearn.svm.SVC(probability=True)\n\n\n# In[178]:\n\nmdl.fit(X, y)\n\n\n# In[179]:\n\npred = mdl.predict(X)\npred_encoded = sklearn.preprocessing.LabelBinarizer().fit_transform(pred)\n\npred_prob = mdl.predict_proba(X)\n\n\n# In[180]:\n\nsklearn.metrics.log_loss(y, pred_encoded)\n\n\n# In[181]:\n\nsklearn.metrics.log_loss(y, pred_prob)\n\n'"
scikit_learn_save_model.py,1,"b'\n# coding: utf-8\n\n# In[6]:\n\nimport numpy as np\nimport sklearn.datasets\nimport pickle\nimport xgboost as xgb\n\n\n# In[7]:\n\nboston = sklearn.datasets.load_boston()\ny = boston.target\nX = boston.data\nclf = xgb.XGBRegressor().fit(X, y)\npickle.dump(clf, open(""xgb_boston.pkl"", ""wb""))\nclf2 = pickle.load(open(""xgb_boston.pkl"", ""rb""))\nprint(np.allclose(clf.predict(X), clf2.predict(X)))\n\n'"
scikit_learn_user_guide.py,0,"b""\n# coding: utf-8\n\n# scikit-learn [User Guide](http://scikit-learn.org/stable/user_guide.html)\n\n# #  Supervised learning\n\n# ## Generalized Linear Models\n\n# ### Least Angle Regression\n# The advantages of LARS are:\n# It is numerically efficient in contexts where p >> n (i.e., when the number of dimensions is significantly greater than the number of points)\n\n# ### Bayesian Regression\n# A good introduction to Bayesian methods is given in C. Bishop: Pattern Recognition and Machine learning\n# \n\n# #### Bayesian Ridge Regression\n\n# In[31]:\n\nfrom sklearn.linear_model import BayesianRidge, LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn import datasets\nboston = datasets.load_boston()\nX = boston.data\nX = Normalizer().fit_transform(X)\ny = boston.target\nbayesian_ridge = BayesianRidge()\nlinear_regression = LinearRegression()\n\ncross_val_score(bayesian_ridge, X, y, cv=5)\ncross_val_score(linear_regression, X, y, cv=5)\n\n\n# In[32]:\n\ncross_val_score(bayesian_ridge, X, y, scoring='r2', cv=5)\ncross_val_score(linear_regression, X, y, scoring='r2', cv=5)\n\n\n# In[34]:\n\ncross_val_score(bayesian_ridge, X, y, scoring='neg_mean_squared_error', cv=5)\ncross_val_score(linear_regression, X, y, scoring='neg_mean_squared_error', cv=5)\n\n\n# ### Logistic regression\n# For large dataset, you may also consider using `SGDClassifier` with `log` loss.\n\n# ## Linear and Quadratic Discriminant Analysis\n\n# ## Kernel ridge regression\n\n# ## Support Vector Machines\n\n# ## Stochastic Gradient Descent\n\n# ## Nearest Neighbors\n\n# ## Gaussian Processes\n\n# ## Cross decomposition\n\n# ## Naive Bayes\n\n# ## Decision Trees\n\n# ## Ensemble methods\n\n# ## Multiclass and multilabel algorithms\n\n# ## Feature selection\n\n# ## Semi-Supervised\n\n# ## Isotonic regression\n\n# ## Probability calibration\n\n# ## Neural network models (supervised)\n\n# # Unsupervised learning\n\n# ## Gaussian mixture models\n# ## Manifold learning\n# ## Clustering\n# ## Biclustering\n# ## Decomposing signals in components (matrix factorization problems)\n# ## Covariance estimation\n# ## Novelty and Outlier Detection\n# ## Density Estimation\n# ## Neural network models (unsupervised)\n# # Model selection and evaluation\n# ## Cross-validation: evaluating estimator performance\n# ## Tuning the hyper-parameters of an estimator\n# ## Model evaluation: quantifying the quality of predictions\n# ## Model persistence\n# ## Validation curves: plotting scores to evaluate models\n# # Dataset transformations\n# ## Pipeline and FeatureUnion: combining estimators\n# ## Feature extraction\n# ## Preprocessing data\n# ## Unsupervised dimensionality reduction\n# ## Random Projection\n# ## Kernel Approximation\n# ## Pairwise metrics, Affinities and Kernels\n# ## Transforming the prediction target (y)\n# # Dataset loading utilities\n# ## General dataset API\n# ## Toy datasets\n# ## Sample images\n# ## Sample generators\n# ## Datasets in svmlight / libsvm format\n# ## Loading from external datasets\n# ## The Olivetti faces dataset\n# ## The 20 newsgroups text dataset\n# ## Downloading datasets from the mldata.org repository\n# ## The Labeled Faces in the Wild face recognition dataset\n# ## Forest covertypes\n# ## RCV1 dataset\n# ## Boston House Prices dataset\n# ## Breast Cancer Wisconsin (Diagnostic) Database\n# ## Diabetes dataset\n# ## Optical Recognition of Handwritten Digits Data Set\n# ## Iris Plants Database\n# ## Linnerrud dataset\n# # Strategies to scale computationally: bigger data\n# ## Scaling with instances using out-of-core learning\n# # Computational Performance\n# ## Prediction Latency\n# ## Prediction Throughput\n# ## Tips and Tricks\n\n# In[ ]:\n\n\n\n"""
xgboost_early_stopping.py,0,"b'\n# coding: utf-8\n\n# # Setup\n\n# In[8]:\n\nget_ipython().magic(\'matplotlib inline\')\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nimport xgboost as xgb\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\n\n\n# # Early stopping \n# https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py\n# \n\n# In[11]:\n\n# Early-stopping\ndigits = sklearn.datasets.load_digits(2)\n\nX = digits[\'data\']\ny = digits[\'target\']\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n    X, y, random_state=0)\nclf = xgb.XGBClassifier()\nclf.fit(X_train,\n        y_train,\n        early_stopping_rounds=10,\n        eval_metric=""auc"",\n        eval_set=[(X_test, y_test)])\n\n\n# # Grid search does not support early stopping\n# See: https://www.kaggle.com/c/liberty-mutual-group-property-inspection-prediction/discussion/15235\n\n# In[10]:\n\n# Early-stopping does not support early stopping\nboston = sklearn.datasets.load_boston()\n\ny = boston[\'target\']\nX = boston[\'data\']\nxgb_model = xgb.XGBRegressor()\nclf = sklearn.model_selection.GridSearchCV(\n    xgb_model, {\n        \'max_depth\': [2, 4, 6],\n        \'n_estimators\': [50, 100, 200],\n        \'early_stopping_rounds\': [10, 100]\n    },\n    verbose=1)\nclf.fit(X, y)\nprint(clf.best_score_)\nprint(clf.best_params_)\n\n'"
xgboost_parallel.py,1,"b'\n# coding: utf-8\n\n# In[32]:\n\nimport os\nimport time\n\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.datasets import load_boston\nimport xgboost\n\n\n# In[33]:\n\nrng = np.random.RandomState(31337)\n\nprint(""Parallel Parameter optimization"")\n\n\n# In[34]:\n\nos.environ[""OMP_NUM_THREADS""] = ""2""  # or to whatever you want\n\n\n# In[35]:\n\nos.environ[""OMP_NUM_THREADS""] = ""3""  # or to whatever you want\n\n\n# In[36]:\n\nboston = load_boston()\n\ny = boston[\'target\']\nX = boston[\'data\']\nxgb_model = xgboost.XGBRegressor()\nclf = GridSearchCV(xgb_model, {\'max_depth\': [2, 4, 6],\n                               \'n_estimators\': [50, 100, 200]}, verbose=1,\n                   n_jobs=3)\nclf.fit(X, y)\nprint(clf.best_score_)\nprint(clf.best_params_)\n\n\n# In[38]:\n\nresults = []\nnum_threads = [1, 2, 3, 4, -1]\nfor n in num_threads:\n    start = time.time()\n    model = xgboost.XGBRegressor(nthread=n)\n    model.fit(X, y)\n    elapsed = time.time() - start\n    print(n, elapsed)\n    results.append(elapsed)\n\n\n# In[39]:\n\nresults = []\nn_jobs = [1, 2, 3, 4, -1]\nfor n in n_jobs:\n    start = time.time()\n    clf = GridSearchCV(xgb_model, {\'max_depth\': [2, 4, 6],\n                               \'n_estimators\': [50, 100, 200]}, verbose=1,\n                   n_jobs=n)\n    clf.fit(X, y)\n    elapsed = time.time() - start\n    print(n, elapsed)\n    results.append(elapsed)\n\n\n# References\n# - https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_parallel.py\n# - http://machinelearningmastery.com/best-tune-multithreading-support-xgboost-python/\n'"
xgboost_tutorial.py,3,"b'\n# coding: utf-8\n\n# # Setup\n\n# In[1]:\n\nget_ipython().magic(\'matplotlib inline\')\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nimport xgboost as xgb\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\n\n\n# # [Basic sklearn examples](https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py)\n\n# ## models\n\n# In[2]:\n\nrng = np.random.RandomState(31337)\n\n\n# In[3]:\n\nprint(""Zeros and Ones from the Digits dataset: binary classification"")\ndigits = sklearn.datasets.load_digits(2)\ny = digits[\'target\']\nX = digits[\'data\']\nkf = sklearn.model_selection.KFold(n_splits=2, shuffle=True, random_state=rng)\nfor train_index, test_index in kf.split(X):\n    xgb_model = xgb.XGBClassifier().fit(X[train_index],y[train_index])\n    predictions = xgb_model.predict(X[test_index])\n    actuals = y[test_index]\n    print(sklearn.metrics.confusion_matrix(actuals, predictions))\n\n\n# In[4]:\n\nprint(""Iris: multiclass classification"")\niris = sklearn.datasets.load_iris()\ny = iris[\'target\']\nX = iris[\'data\']\nkf = sklearn.model_selection.KFold(n_splits=2, shuffle=True, random_state=rng)\nfor train_index, test_index in kf.split(X):\n    xgb_model = xgb.XGBClassifier().fit(X[train_index],y[train_index])\n    predictions = xgb_model.predict(X[test_index])\n    actuals = y[test_index]\n    print(sklearn.metrics.confusion_matrix(actuals, predictions))\n\n\n# In[5]:\n\nprint(""Boston Housing: regression"")\nboston = sklearn.datasets.load_boston()\ny = boston[\'target\']\nX = boston[\'data\']\nkf = sklearn.model_selection.KFold(n_splits=2, shuffle=True, random_state=rng)\nfor train_index, test_index in kf.split(X):\n    xgb_model = xgb.XGBRegressor().fit(X[train_index],y[train_index])\n    predictions = xgb_model.predict(X[test_index])\n    actuals = y[test_index]\n    print(sklearn.metrics.mean_squared_error(actuals, predictions))\n\n\n# ## grid search\n\n# In[6]:\n\nprint(""Parameter optimization"")\ny = boston[\'target\']\nX = boston[\'data\']\nxgb_model = xgb.XGBRegressor()\nclf = sklearn.model_selection.GridSearchCV(xgb_model,\n                   {\'max_depth\': [2,4,6],\n                    \'n_estimators\': [50,100,200]}, verbose=1)\nclf.fit(X,y)\nprint(clf.best_score_)\nprint(clf.best_params_)\n\n\n# ## pickle model\n\n# In[7]:\n\n# The sklearn API models are picklable\nprint(""Pickling sklearn API models"")\n# must open in binary format to pickle\npickle.dump(clf, open(""best_boston.pkl"", ""wb""))\nclf2 = pickle.load(open(""best_boston.pkl"", ""rb""))\nprint(np.allclose(clf.predict(X), clf2.predict(X)))\n\n\n# ## early stopping\n\n# In[8]:\n\n# Early-stopping\n\nX = digits[\'data\']\ny = digits[\'target\']\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n    X, y, random_state=0)\nclf = xgb.XGBClassifier()\nclf.fit(X_train,\n        y_train,\n        early_stopping_rounds=10,\n        eval_metric=""auc"",\n        eval_set=[(X_test, y_test)])\n\n\n# # [Mini course](http://machinelearningmastery.com/xgboost-python-mini-course/)\n\n# ## Early stopping\n\n# In[21]:\n\ndata = pd.read_csv(\n    \'https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\',\n    header=None)\ndata = np.array(data)\nX = data[:, :-1]\ny = data[:, -1]\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n    X, y, test_size=0.3)\nmdl = xgb.XGBClassifier()\neval_set = [(X_test, y_test),]\nmdl.fit(X_train,\n        y_train,\n        early_stopping_rounds=10,\n        eval_metric=""logloss"",\n        eval_set=eval_set,\n        verbose=True)\n\ny_pred = mdl.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = sklearn.metrics.accuracy_score(y_test, predictions)\nprint(""Accuracy: %.2f%%"" % (accuracy * 100.0))\n\n\n# ## Feature Importance\n\n# In[23]:\n\nmdl.feature_importances_\n\nxgb.plot_importance(mdl)\n\n\n# ## How to Configure Gradient Boosting\n\n# A number of configuration heuristics were published in the original gradient boosting papers. They can be summarized as:\n# \n# Learning rate or shrinkage (learning_rate in XGBoost) should be set to 0.1 or lower, and smaller values will require the addition of more trees.\n# The depth of trees (tree_depth in XGBoost) should be configured in the range of 2-to-8, where not much benefit is seen with deeper trees.\n# Row sampling (subsample in XGBoost) should be configured in the range of 30% to 80% of the training dataset, and compared to a value of 100% for no sampling.\n\n# ## Hyperparameter Tuning\n\n# In[26]:\n\nn_estimators = [50, 100, 150, 200]\nmax_depth = [2, 4, 6, 8]\nparam_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\n\nkfold = sklearn.model_selection.StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ngrid_search = sklearn.model_selection.GridSearchCV(mdl, param_grid, scoring=""neg_log_loss"", n_jobs=-1, cv=kfold, verbose=1)\nresult = grid_search.fit(X, y)\n\n\n# The parameters to consider tuning are:\n# \n# - The number and size of trees (n_estimators and max_depth).\n# - The learning rate and number of trees (learning_rate and n_estimators).\n# - The row and column subsampling rates (subsample, colsample_bytree and colsample_bylevel).\n\n# In[49]:\n\nresult.best_estimator_\n\n\n# # References\n# - https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py\n# - http://machinelearningmastery.com/xgboost-python-mini-course/\n# - https://github.com/yang-zhang/xgboost/tree/master/demo\n# - https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs\n# - [Owen Zhang Slides](https://www.slideshare.net/odsc/owen-zhangopen-sourcetoolsanddscompetitions1/12)\n# - [XGBoost Parameters](http://xgboost.readthedocs.io/en/latest//parameter.html)\n# -[Notes on Parameter Tuning](https://xgboost.readthedocs.io/en/latest//how_to/param_tuning.html)\n# - [How to tune hyperparameters of xgboost trees?](http://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees)\n'"
xgboost_visual_explanation.py,1,"b""\n# coding: utf-8\n\n# # Setup\n\n# In[24]:\n\nget_ipython().magic('matplotlib inline')\nimport numpy as np\nimport pandas as pd\n\nimport xgboost \n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport sklearn.metrics\n\n\n# # [Example 1](http://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/)\n\n# In[25]:\n\ndata = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data',\n           header=None)\ndata = np.array(data)\n\nX_train = data[:, :-1]\n\ny_train = data[:, -1]\n\nmdl = xgboost.XGBClassifier()\n\nmdl.fit(X_train, y_train)\n\n\n# In[26]:\n\nxgboost.plot_importance(mdl)\n\n\n# In[27]:\n\nxgboost.plot_tree(mdl)\n\n\n# # Example 2: Iris\n\n# In[28]:\n\ndata = sklearn.datasets.load_iris()\n\nX_train = data.data\n\ny_train = data.target\n\nmdl = xgboost.XGBClassifier()\n\nmdl.fit(X_train, y_train)\n\n\n# In[29]:\n\nxgboost.plot_tree(mdl)\n\n\n# In[30]:\n\nxgboost.plot_importance(mdl)\n\n"""
xgboost_xgbfi.py,0,"b""\n# coding: utf-8\n\n# In[3]:\n\nimport sklearn.datasets\nimport sklearn.model_selection\nimport xgboost\nimport xgbfir\n\n\n# In[59]:\n\niris = sklearn.datasets.load_iris()\nX = iris.data\ny = iris.target\n\nmdl = xgboost.XGBClassifier().fit(X, y)\nxgbfir.saveXgbFI(mdl, feature_names=iris.feature_names, OutputXlsxFile = 'irisFI.xlsx')\n\n\n# In[62]:\n\nboston = sklearn.datasets.load_boston()\nX = boston.data\ny = boston.target\n\nmdl = xgboost.XGBRegressor().fit(X, y)\nxgbfir.saveXgbFI(mdl, feature_names=boston.feature_names, OutputXlsxFile = 'bostonFI.xlsx')\n\n\n# In[ ]:\n\nboston = sklearn.datasets.load_boston()\nX = boston.data\ny = boston.target\n\nmdl = sklearn.model_selection.GridSearchCV(\n    estimator = xgboost.XGBRegressor(),\n    param_grid = {'max_depth': [2, 4, 6], 'n_estimators': [50, 100, 200]},\n)\nmdl.fit(X, y)\n\nxgbfir.saveXgbFI(mdl.best_estimator_, feature_names=boston.feature_names, OutputXlsxFile = 'bostonFI_grid.xlsx')\n\n\n# In[8]:\n\nls *.xlsx\n\n\n# References\n# - https://github.com/limexp/xgbfir\n# - http://projects.rajivshah.com/blog/2016/08/01/xgbfi/\n"""
