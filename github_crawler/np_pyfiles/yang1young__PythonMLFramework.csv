file_path,api_count,code
data_prepare.py,0,"b'#!/usr/bin/python\n# coding=utf-8\nimport pandas as pd\nfrom sklearn.cross_validation import train_test_split\n\nTRAIN_PATH = \'/home/yangqiao/pythonProject/PythonMLFramework/wine.csv\'\nTEST_PATH = \'/home/yangqiao/pythonProject/PythonMLFramework/wine.csv\'\n# if you want to delete some columns, fill the list with the index\nDELETE_INDEX = []\n# the lable index\nLABLE_INDEX = 0\n\n\n# remove unwanted columns and set the last columns as label\ndef data_clean(data_file):\n    csv = pd.read_csv(data_file)\n    csv.columns = range(0, len(csv.columns), 1)\n    if (LABLE_INDEX != -1):\n        Y = csv.iloc[:, LABLE_INDEX]\n        X = csv.drop(LABLE_INDEX, axis=1)\n    else:\n        X = csv.iloc[:, :len(csv.columns.tolist()) - 1]\n        Y = csv.iloc[:, -1]\n    if (DELETE_INDEX != None):\n        # delete some columns\n        X = X.drop(DELETE_INDEX, axis=1)\n    data = pd.concat([X, Y], axis=1)\n    return data\n\n\n# if unbalanced data, you need to sample some data from the higher count class\n# sub_sample_time means the times of Most classes than Few class\n# such as sub_sample_time=10, means Most classes is 10 times than Few class\ndef sub_sample(data_file, sub_sample_time):\n    train_black = data_file[data_file.iloc[:, len(data_file.columns) - 1] == 1]\n    train_white = data_file[data_file.iloc[:, len(data_file.columns) - 1] == 0]\n    # sampletime\n    percentage = float(train_black.shape[0] * sub_sample_time) / float(train_white.shape[0])\n    train_white_sample = train_white.sample(frac=percentage, replace=False)\n    train = pd.DataFrame(pd.concat([train_white_sample, train_black], axis=0))\n    x_train = train.iloc[:, :len(train.columns.tolist()) - 1]\n    y_train = train.iloc[:, -1]\n    return x_train, y_train\n\n\n# train test split giving a percent\ndef get_train_test_split(data_file, split_percentage):\n    x_train, x_test, y_train, y_test = train_test_split(data_file.iloc[:, :len(data_file.columns.tolist()) - 1],\n                                                        data_file.iloc[:, -1], test_size=split_percentage)\n    return x_train, x_test, y_train, y_test\n\n\n# prepare data\n# save_file_path: if set to \'\' or None, means no saving, or will save train and test file into folder\n# log_path: the path of log\n# sub_sample_time: the time of Most classes than Few class, if set to 0, means no sample\n# train_test_split_percentage: train test split, if set to 0, means no need to do\ndef data_prepare(save_file_path, log_path, sub_sample_time, train_test_split_percentage):\n    data = data_clean(TRAIN_PATH)\n    # if if unbanlanced data you need do sample\n    if (sub_sample_time != 0):\n        data = sub_sample(data, sub_sample_time)\n    if (train_test_split_percentage != 0):\n        x_train, x_test, y_train, y_test = get_train_test_split(data, train_test_split_percentage)\n    else:\n        test = data_clean(TEST_PATH)\n        x_test = test.iloc[:, :len(test.columns.tolist()) - 1]\n        y_test = test.iloc[:, -1]\n        x_train = data.iloc[:, :len(data.columns.tolist()) - 1]\n        y_train = data.iloc[:, -1]\n    if (save_file_path):\n        csv_train = pd.concat([x_train, y_train], axis=1)\n        csv_test = pd.concat([x_test, y_test], axis=1)\n        csv_train.to_csv(save_file_path + \'train.csv\', index=False)\n        csv_test.to_csv(save_file_path + \'test.csv\', index=False)\n\n    fp = open(log_path + \'log.txt\', \'a\')\n    fp.write(\'trainData shape is \' + str(x_train.shape) + \'\\n\')\n    fp.write(\'testData shape is \' + str(x_test.shape) + \'\\n\')\n    fp.close()\n\n    if (save_file_path):\n        csv_train = pd.concat([x_train, y_train], axis=1)\n        csv_test = pd.concat([x_test, y_test], axis=1)\n        csv_train.to_csv(save_file_path + \'train.csv\', index=False)\n        csv_test.to_csv(save_file_path + \'test.csv\', index=False)\n\n    return x_train, x_test, y_train, y_test\n\n\ndir = \'/home/yangqiao/pythonProject/PythonMLFramework/log/\'\nif __name__ == ""__main__"":\n    x_train, x_test, y_train, y_test = data_prepare(None, dir, 0, 0)\n    print x_train.shape\n    print x_test.shape\n'"
ml_utils.py,3,"b""# !/usr/bin/python\n# coding=utf-8\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport train_models as M\nfrom itertools import product\nimport matplotlib.pyplot as plt\n\n\n# dump models into files\ndef modelDump(model, path):\n    with open(path, 'wb') as f:\n        pickle.dump(model, f)\n\n\n# reload trained models\ndef modelReload(path):\n    with open(path, 'rb') as f:\n        model2 = pickle.load(f)\n    return model2\n\n\n# print models' decision Margin, first choose two most important dimention of data\ndef plotMargin(dir, index1, index2):\n    csv = pd.read_csv(dir + 'train.csv')\n    csv.columns = range(0, len(csv.columns), 1)\n    X = csv.iloc[:, [index1, index2]]\n    Y = csv.iloc[:, -1]\n    print X.shape\n\n    # temporary is four decision margin\n    RandomForest = M.trainRF(X, X, Y, Y, dir)\n    SVM = M.trainSVM(X, X, Y, Y, dir)\n    GBDT = M.trainGBDT(X, X, Y, Y, dir)\n    DecisionTree = M.trainDT(X, X, Y, Y, dir)\n\n    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    print xx.shape, yy.shape\n    f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n    for idx, clf, tt in zip(product([0, 1], [0, 1]),\n                            [RandomForest, SVM, GBDT, DecisionTree],\n                            ['RandomForest', 'SVM(RBF)',\n                             'GBDT', 'DecisionTree']):\n        temp = pd.DataFrame(np.c_[xx.ravel(), yy.ravel()])\n        Z = clf.predict(temp)\n        Z = Z.reshape(xx.shape)\n\n        axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n        axarr[idx[0], idx[1]].scatter(X.iloc[:, 0], X.iloc[:, 1], c=Y, alpha=0.8)\n        axarr[idx[0], idx[1]].set_title(tt)\n\n    plt.show()\n"""
model_zoo.py,10,"b'#!/usr/bin/python\r\n# coding=utf-8\r\nimport datetime\r\nimport pandas as pd\r\nimport numpy as np\r\nimport sklearn.svm.libsvm as libsvm\r\nfrom sklearn import linear_model\r\nfrom sklearn.tree import export_graphviz\r\nfrom sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.tree import DecisionTreeClassifier\r\nfrom sklearn.ensemble import GradientBoostingClassifier\r\nfrom sklearn.metrics import accuracy_score, precision_recall_curve, precision_score, recall_score\r\n\r\n# super class\r\nclass Models():\r\n    model = None\r\n\r\n    def __init__(self, name):\r\n        self.name = name\r\n\r\n    def _build(self, x_train, y_train, path):\r\n        fp = open(path + \'log.txt\', \'a\')\r\n        fp.write(\'Start training \' + self.name + \'\\n\')\r\n        start = datetime.datetime.now()\r\n        self.model.fit(x_train, y_train)\r\n        end = datetime.datetime.now()\r\n        print self.name + \' training time is \' + str(end - start)\r\n        fp.write(self.name + \' training time is \' + str(end - start) + \'\\n\')\r\n        fp.close()\r\n\r\n    # evaluate models\r\n    def modelEvaluate(self, x_test, y_test, path):\r\n        start = datetime.datetime.now()\r\n        pred_y = self.model.predict(x_test)\r\n        end = datetime.datetime.now()\r\n        fp = open(path + \'log.txt\', \'a\')\r\n        fp.write(\'Start evaluate \' + self.name + \'\\n\')\r\n        fp.write(self.name + \' predicte time is \' + str(end - start) + \'\\n\')\r\n        print self.name + \' predicte time is \' + str(end - start)\r\n\r\n        print \'crosstab:{0}\'.format(pd.crosstab(y_test, pred_y))\r\n        print \'accuracy_score:{0}\'.format(accuracy_score(y_test, pred_y))\r\n        print \'0precision_score:{0}\'.format(precision_score(y_test, pred_y, pos_label=0))\r\n        print \'0recall_score:{0}\'.format(recall_score(y_test, pred_y, pos_label=0))\r\n        print \'1precision_score:{0}\'.format(precision_score(y_test, pred_y, pos_label=1))\r\n        print \'1recall_score:{0}\'.format(recall_score(y_test, pred_y, pos_label=1))\r\n\r\n        fp.write(\'crosstab:{0}\'.format(pd.crosstab(y_test, pred_y)) + \'\\n\')\r\n        fp.write(\'accuracy_score:{0}\'.format(accuracy_score(y_test, pred_y)) + \'\\n\')\r\n        fp.write(\'0precision_score:{0}\'.format(precision_score(y_test, pred_y, pos_label=0)) + \'\\n\')\r\n        fp.write(\'0recall_score:{0}\'.format(recall_score(y_test, pred_y, pos_label=0)) + \'\\n\')\r\n        fp.write(\'1precision_score:{0}\'.format(precision_score(y_test, pred_y, pos_label=1)) + \'\\n\')\r\n        fp.write(\'1recall_score:{0}\'.format(recall_score(y_test, pred_y, pos_label=1)) + \'\\n\')\r\n\r\n        fp.close()\r\n\r\n    # predict probality,is a list,such as [0.1,0.9]\r\n    def predictP(self, input):\r\n        input = np.array(input).reshape(1, -1)\r\n        result = self.model.predict_proba(input)\r\n        return result\r\n\r\n    # predict label\r\n    def predict(self, input):\r\n        input = np.array(input)\r\n        result = self.model.predict(input)\r\n        return result\r\n\r\n    # return top NUM probality items index\r\n    def top_probality(self, x_test, path, top_num):\r\n        predication_prob = self.model.predict_proba(x_test)[:, 1]\r\n        index = np.argsort(predication_prob)[-top_num:][::-1]\r\n        return index\r\n\r\n\r\n# this is decision tree\r\nclass DecisionTree(Models):\r\n    def build(self, x_train, y_train, path, **parameter):\r\n        self.model = DecisionTreeClassifier(**parameter)\r\n        self._build(x_train, y_train, path)\r\n\r\n    # if you want save tree to dot file\r\n    def saveTree(self, path):\r\n        with open(path + ""dt.dot"", \'w\') as f:\r\n            export_graphviz(self.model, out_file=f, feature_names=\r\n            [\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\', \'8\', \'9\', \'10\', \'11\', \'12\', \'13\', \'14\'])\r\n\r\n\r\n# gradient boosting tree\r\nclass GBDT(Models):\r\n    def build(self, x_train, y_train, path, **parameter):\r\n        self.model = GradientBoostingClassifier(**parameter)\r\n        self._build(x_train, y_train, path)\r\n\r\n\r\n# randomforest tree\r\nclass RF(Models):\r\n    def build(self, x_train, y_train, path, **parameter):\r\n        self.model = RandomForestClassifier(**parameter)\r\n        self._build(x_train, y_train, path)\r\n\r\n    # feature inportance of RF\r\n    def featureImportance(self, x_train, path):\r\n        importance = self.model.feature_importances_\r\n        indices = np.argsort(importance)[::-1]\r\n        # print indices\r\n        col_list = x_train.columns\r\n        fp = open(path + \'log.txt\', \'a\')\r\n        for f, name, _ in zip(range(x_train.shape[1]), col_list, range(50)):\r\n            print ""%d. feature %d (%f)(name = %s)"" % (f + 1, indices[f], importance[indices[f]], col_list[indices[f]])\r\n            fp.write(\r\n                ""%d. feature %d (%f)(name = %s)\\n"" % (f + 1, indices[f], importance[indices[f]], col_list[indices[f]]))\r\n        fp.close()\r\n\r\n\r\n# a wrapper for libsvm library\r\nclass SVM(Models):\r\n    def build(self, x_train, y_train, path, **parameter):\r\n        x = x_train.as_matrix()\r\n        x = x.copy(order=\'C\').astype(np.float64)\r\n        y = y_train.as_matrix().astype(np.float64)\r\n\r\n        self.model = libsvm.fit(x, y, **parameter)\r\n\r\n    # predict probality\r\n    def predictP(self, input):\r\n        input = np.array(input).reshape(1, -1)\r\n        result = libsvm.predict_proba(input, *(self.model))\r\n        return result\r\n\r\n    # predict lable\r\n    def predict(self, input):\r\n        input = np.array(input)\r\n        result = libsvm.predict(input, *(self.model))\r\n        return result\r\n\r\n    # return top NUM probality items index\r\n    def top_probality(self, x_test, path, top_num):\r\n        predication_prob = libsvm.predict_proba(x_test, *(self.model))[:, 1]\r\n        index = np.argsort(predication_prob)[-top_num:][::-1]\r\n        print index\r\n\r\n    # evaluate\r\n    def modelEvaluate(self, x_test, y_test, path):\r\n        start = datetime.datetime.now()\r\n        pred_y = libsvm.predict(x_test.as_matrix().copy(order=\'C\').astype(np.float64), *(self.model))\r\n        end = datetime.datetime.now()\r\n        print self.name + \' predicte time is \' + str(end - start)\r\n        print \'crosstab:{0}\'.format(pd.crosstab(y_test, pred_y))\r\n        print \'accuracy_score:{0}\'.format(accuracy_score(y_test, pred_y))\r\n        print \'0precision_score:{0}\'.format(precision_score(y_test, pred_y, pos_label=0))\r\n        print \'0recall_score:{0}\'.format(recall_score(y_test, pred_y, pos_label=0))\r\n        print \'1precision_score:{0}\'.format(precision_score(y_test, pred_y, pos_label=1))\r\n        print \'1recall_score:{0}\'.format(recall_score(y_test, pred_y, pos_label=1))\r\n        fp = open(path + \'log.txt\', \'a\')\r\n        fp.write(\'crosstab:{0}\'.format(pd.crosstab(y_test, pred_y)) + \'\\n\')\r\n        fp.write(\'accuracy_score:{0}\'.format(accuracy_score(y_test, pred_y)) + \'\\n\')\r\n        fp.write(\'0precision_score:{0}\'.format(precision_score(y_test, pred_y, pos_label=0)) + \'\\n\')\r\n        fp.write(\'0recall_score:{0}\'.format(recall_score(y_test, pred_y, pos_label=0)) + \'\\n\')\r\n        fp.write(\'1precision_score:{0}\'.format(precision_score(y_test, pred_y, pos_label=1)) + \'\\n\')\r\n        fp.write(\'1recall_score:{0}\'.format(recall_score(y_test, pred_y, pos_label=1)) + \'\\n\')\r\n        fp.close()\r\n\r\n\r\n# lasso model, a wrapper of liblinear\r\n# L1 normalize logistic regression\r\nclass lasso(Models):\r\n    def build(self, x_train, y_train, path, **parameter):\r\n        self.model = linear_model.LogisticRegression(**parameter)\r\n        self._build(x_train, y_train, path)\r\n'"
train_models.py,1,"b'#!/usr/bin/python\r\n# coding=utf-8\r\nimport model_zoo\r\nimport numpy as np\r\nimport ml_utils as ml\r\nimport data_prepare as dp\r\n\r\nMODEL_PATH = \'/home/yangqiao/pythonProject/PythonMLFramework/model/\'\r\nLOG_PATH = \'/home/yangqiao/pythonProject/PythonMLFramework/log/\'\r\n\r\n\r\n# train decision tree\r\ndef trainDT(x_train, x_test, y_train, y_test, path):\r\n    dt = model_zoo.DecisionTree(\'dt\')\r\n    dt.build(x_train, y_train, path, max_depth=10)\r\n    dt.modelEvaluate(x_test, y_test, path)\r\n    dt.saveTree(path)\r\n    ml.modelDump(dt, path + \'dt.txt\')\r\n    return dt\r\n\r\n\r\n# train svm model\r\ndef trainSVM(x_train, x_test, y_train, y_test, path):\r\n    svm = model_zoo.SVM(\'SVM\')\r\n    svm.build(x_train, y_train, path, C=10.0, kernel=\'rbf\', class_weight=np.asarray([1.0, 10.0]), gamma=1,\r\n              shrinking=True, probability=True)\r\n    svm.modelEvaluate(x_test, y_test, path)\r\n    ml.modelDump(svm, path + ""SVM.txt"")\r\n    return svm\r\n\r\n\r\n# train GBDT\r\ndef trainGBDT(x_train, x_test, y_train, y_test, path):\r\n    GBDT = model_zoo.GBDT(\'GBDT\')\r\n    GBDT.build(x_train, y_train, path, subsample=0.7)\r\n    GBDT.modelEvaluate(x_test, y_test, path)\r\n    ml.modelDump(GBDT, path + ""gbdt.txt"")\r\n    return GBDT\r\n\r\n\r\n# train random forest\r\ndef trainRF(x_train, x_test, y_train, y_test, path):\r\n    rf = model_zoo.RF(\'RF\')\r\n    rf.build(x_train, y_train, path, n_estimators=200, max_depth=6)\r\n    rf.modelEvaluate(x_test, y_test, path)\r\n    rf.featureImportance(x_train, path)\r\n    ml.modelDump(rf, path + ""rf.txt"")\r\n    return rf\r\n\r\n\r\n# train lasso\r\ndef trainLasso(x_train, x_test, y_train, y_test, path):\r\n    lasso = model_zoo.lasso(\'lasso\')\r\n    lasso.build(x_train, y_train, path, penalty=\'l1\', C=1, class_weight={0: 1, 1: 20}, max_iter=100)\r\n    lasso.modelEvaluate(x_test, y_test, path)\r\n    # lasso.top_probality(x_test,path)\r\n    ml.modelDump(lasso, path + ""lasso.txt"")\r\n    return lasso\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    x_train, x_test, y_train, y_test = dp.data_prepare(LOG_PATH, LOG_PATH, 0, 20)\r\n    # trainRF(x_train, x_test, y_train, y_test,MODEL_PATH)\r\n    # trainLasso(x_train, x_test, y_train, y_test,MODEL_PATH)\r\n    # trainGBDT(x_train, x_test, y_train, y_test,MODEL_PATH)\r\n    # trainDT(x_train, x_test, y_train, y_test,MODEL_PATH)\r\n    ml.plotMargin(LOG_PATH, 0, 1)\r\n'"
pandas_learn_demo/Pandas.py,20,"b'import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n#******************************************************************************************\n\ndef create():\n\n    #create Series\n    s = pd.Series([1,3,5,np.nan,6,8])\n    print s\n\n    #create dataframe\n    dates = pd.date_range(\'20130101\', periods=6)\n    df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(\'ABCD\'))\n    print df\n\n    #Creating a DataFrame by passing a dict of objects that can be converted to series-like.\n    df2 = pd.DataFrame({ \'A\' : 1.,\n                        \'B\' : pd.Timestamp(\'20130102\'),\n                        \'C\' : pd.Series(1,index=list(range(4)),dtype=\'float32\'),\n                        \'D\' : np.array([3] * 4,dtype=\'int32\'),\n                        \'E\' : pd.Categorical([""test"",""train"",""test"",""train""]),\n                        \'F\' : \'foo\' })\n    print df2\n    #Having specific dtypes\n    print df2.dtypes\n\n\n#******************************************************************************************\ndef see():\n\n    dates = pd.date_range(\'20130101\', periods=6)\n    df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(\'ABCD\'))\n    print df\n\n    #See the top & bottom rows of the frame\'\'\'\n    print df.head(2)\n    print df.tail(1)\n\n    #Display the index, columns, and the underlying numpy data,num of line and col\n    print df.index\n    print df.columns\n    print df.values\n    print df.shape[0]\n    print df.shape[1]\n\n    #Describe shows a quick statistic summary of your data\n    print df.describe()\n\n    #Transposing your data\n    print df.T\n\n    #Sorting by an axis,0 is y,1 is x,ascending True is zhengxv,false is daoxv\n    print df.sort_index(axis=0, ascending=False)\n\n    #Sorting by values\n    print df.sort(column=\'B\')\n\n    #see valuenums\n    print df[0].value_counts()\n    print df[u\'hah\'].value_counts()\n\n    #see type and change\n    df.dtypes\n    df[[\'two\', \'three\']] = df[[\'two\', \'three\']].astype(float)\n\n\n#******************************************************************************************\n\ndef selection():\n\n    dates = pd.date_range(\'20130101\', periods=6)\n    df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(\'ABCD\'))\n    print df\n\n    #Selecting a single column, which yields a Series, equivalent to df.A\n    print df[\'A\']\n    print df.A\n\n    #Selecting via [], which slices the rows.\n    print df[0:3]\n    print df[\'20130102\':\'20130104\']\n\n    #Selection by Label\n\n    #For getting a cross section using a label\n    print df.loc[dates[0]]\n\n    #Selecting on a multi-axis by label\n    print df.loc[:,[\'A\',\'B\']]\n\n    #Showing label slicing, both endpoints are included\n    print df.loc[\'20130102\':\'20130104\',[\'A\',\'B\']]\n\n    #For getting a scalar value\n    print df.loc[dates[0],\'A\']\n    print df.at[dates[0],\'A\']\n\n\n    #Selection by Position\n\n    #Select via the position of the passed integers\n    print df.iloc[3]\n\n    #By integer slices, acting similar to numpy/python\n    print df.iloc[3:5,0:2]\n\n    #By lists of integer position locations, similar to the numpy/python style\n    print df.iloc[[1,2,4],[0,2]]\n\n    #For slicing rows explicitly\n    print df.iloc[1:3,:]\n\n    #For getting a value explicitly\n    print df.iloc[1,1]\n    print df.iat[1,1]\n\n\n    #Boolean Indexing\n\n    #Using a single column\'s values to select data.\n    print df[df.A > 0]\n\n    #Using the isin() method for filtering:\n    df2 = df.copy()\n    df2[\'E\'] = [\'one\', \'one\',\'two\',\'three\',\'four\',\'three\']\n    print df2[df2[\'E\'].isin([\'two\',\'four\'])]\n\n    #A where operation for getting.\n    print df[df > 0]\n    df2[df2 > 0] = -df2\n\n    #Setting\n    #Setting a new column automatically aligns the data by the indexes\n    s1 = pd.Series([1,2,3,4,5,6], index=pd.date_range(\'20130102\', periods=6))\n    df[\'F\'] = s1\n    print df\n\n    #Setting values by label/index\n    df.at[dates[0],\'A\'] = 0\n    df.iat[0,1] = 0\n    print df\n\n    #Setting by assigning with a numpy array\n    df.loc[:,\'D\'] = np.array([5] * len(df))\n    print df\n\n\n\n#******************************************************************************************\n# pandas primarily uses the value np.nan to represent missing data.\ndef missing():\n    dates = pd.date_range(\'20130101\', periods=6)\n    df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\'ABCD\'))\n    print df\n\n    # Reindexing allows you to change/add/delete the index on a specified axis. This returns a copy of the data.\n    df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [\'E\'])\n    df1.loc[dates[0]:dates[1], \'E\'] = 1\n    print df1\n\n    # Filling missing data\n    df2 = df1.fillna(value=5)\n    print df2\n\n    #To drop any rows that have missing data.\n    df3 = df1.dropna(how=\'any\')\n    print df3\n\n    #To get the boolean mask where values are nan\n    print pd.isnull(df1)\n\n\n\n#******************************************************************************************\n\ndef operate():\n    dates = pd.date_range(\'20130101\', periods=6)\n    df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list(\'ABCD\'))\n    print df\n\n    #Performing a descriptive statistic\n    print df.mean()\n    print df.mean(1)\n\n    #Applying functions to the data\n    print df.apply(np.cumsum)\n    print df.apply(lambda x: x.max() - x.min())\n\n    #String Methods\n    s = pd.Series([\'A\', \'B\', \'C\', \'Aaba\', \'Baca\', np.nan, \'CABA\', \'dog\', \'cat\'])\n    print s.str.lower()\n\n\n\n\n#******************************************************************************************\ndef join():\n    #concat():\n    #Concatenating pandas objects together with concat():\n    df = pd.DataFrame(np.random.randn(10, 4))\n    print df\n\n    pieces = [df[:3], df[3:7], df[7:]]\n    pd.concat(pieces)\n\n    #concact two dataFrame\n    #data = pd.concat([data0,data1],ignore_index=True)\n\n    #join\n    left = pd.DataFrame({\'key\': [\'foo\', \'foo\'], \'lval\': [1, 2]})\n    right = pd.DataFrame({\'key\': [\'foo\', \'foo\'], \'rval\': [4, 5]})\n    print pd.merge(left, right, on=\'key\')\n\n    #Append\n    df = pd.DataFrame(np.random.randn(8, 4), columns=[\'A\',\'B\',\'C\',\'D\'])\n    print df\n    s = df.iloc[3]\n    print s\n    df1 =  df.append(s, ignore_index=True)\n    print df1\n\n    #Grouping,group by\n    #Splitting :the data into groups based on some criteria\n    #Applying a function to each group independently\n    #Combining the results into a data structure\n\n    df = pd.DataFrame({\'A\' : [\'foo\', \'bar\', \'foo\', \'bar\',\n                           \'foo\', \'bar\', \'foo\', \'foo\'],\n                            \'B\' : [\'one\', \'one\', \'two\', \'three\',\n                            \'two\', \'two\', \'one\', \'three\'],\n                            \'C\' : np.random.randn(8),\n                            \'D\' : np.random.randn(8)})\n    print df\n\n    print df.groupby(\'A\').sum()\n\n    print df.groupby([\'A\',\'B\']).sum()\n\n    #groupBy 2:\n    i =0\n    for data in df.groupby(df[3]):\n        i=i+1\n        print i\n        print data\n\n    #Time Series\n    rng = pd.date_range(\'1/1/2012\', periods=100, freq=\'S\')\n    print rng\n    ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)\n    print ts\n    print ts.resample(\'5Min\').sum()\n\n\n\n#******************************************************************************************\n\ndef plot():\n    ts = pd.Series(np.random.randn(1000), index=pd.date_range(\'1/1/2000\', periods=1000))\n    print ts\n\n    plt.figure()\n    ts.plot()\n\n    df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index,\n                      columns=[\'A\', \'B\', \'C\', \'D\'])\n    df = df.cumsum()\n    plt.figure(); df.plot(); plt.legend(loc=\'best\')\n    plt.show()\n\n\n#******************************************************************************************\n\ndef file():\n    ts = pd.Series(np.random.randn(1000), index=pd.date_range(\'1/1/2000\', periods=1000))\n    df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index,\n                      columns=[\'A\', \'B\', \'C\', \'D\'])\n    pd.read_csv(\'foo.csv\')\n    df.to_csv(\'foo.csv\')\nif __name__ == ""__main__"":\n   print \'\'\n'"
