file_path,api_count,code
src/__init__.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Mar 10 15:08:05 2017\n\n@author: yaric\n""""""\n'"
src/config.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nHolds common configuration parameters\n\n@author: yaric\n""""""\n\n# The root path to the data directory\ndata_dir = ""../data""\n# The output directory\nout_dir = ""../out""\n# The intermediate output directory\nintermediate_dir = out_dir + ""/intermediate"" \n# The directory to store unit test results\nunit_tests_dir = intermediate_dir + ""/u_tests""\n# The directory to store rained models\nmodels_dir = intermediate_dir + ""/models""\n\n#\n# The train raw corpora\n#\nsentence_train_path = data_dir + ""/sentence_train.txt""\nparse_train_path = data_dir + ""/parse_train.txt""\nglove_train_path = data_dir + ""/glove_train.txt""\ncorrections_train_path = data_dir + ""/corrections_train.txt""\npos_tags_train_path = data_dir + ""/pos_tags_train.txt""\n\n#\n# The validate raw corpora\n#\nsentence_validate_path = data_dir + ""/sentence_test.txt""\nparse_validate_path = data_dir + ""/parse_test.txt""\nglove_validate_path = data_dir + ""/glove_test.txt""\ncorrections_validate_path = data_dir + ""/corrections_test.txt""\npos_tags_validate_path = data_dir + ""/pos_tags_test.txt""\n\n#\n# The test raw corpora\n#\nsentence_test_path = data_dir + ""/sentence_private_test.txt""\nparse_test_path = data_dir + ""/parse_private_test.txt""\nglove_test_path = data_dir + ""/glove_private_test.txt""\npos_tags_test_path = data_dir + ""/pos_tags_private_test.txt""\n\n#\n# The train processed corpora\n#\ntrain_features_path = intermediate_dir + ""/train_features.npy""\ntrain_labels_path = intermediate_dir + ""/train_labels.npy""\n\n#\n# The validate processed corpora\n#\nvalidate_features_path = intermediate_dir + ""/validate_features.npy""\nvalidate_labels_path = intermediate_dir + ""/validate_labels.npy""\n\n#\n# The test processed corpora\n#\ntest_features_path = intermediate_dir + ""/test_features.npy""\ntest_labels_prob_path = intermediate_dir + ""/test_labels_prob.npy""\n\n# The trained n-gram model file\nngram_model_path = ""../out/counter.pkl""\n# The test results file\ntest_reults_path = out_dir + ""/submission_test.txt""'"
src/data_set.py,14,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nThe data set generatorion and results persistence routines. \nAs generator it will create numeric features matrix by chunking Noun Phrases \n(NP) from provided data corpus and substituting words in the NP with\ncorresponding indices from Glove vectors.\n\nAs results persistence it will encode predictions as list of lists in format \n[[null, null, null, null, null, null, null, [""the"", 0.552054389029137], null]],\nwhere null means no correction at this unit and [""the"", 0.552054389029137] means\ncorrection suggested to ""the\' with confidence 0.552054389029137\n\n@author: yaric\n""""""\nfrom enum import IntEnum\nimport numpy as np\nimport os\nimport argparse\n\nimport tree_dict as td\nimport utils\nimport config\n\n# The Part-of-Speech enumerations\nclass POS(IntEnum):\n    CC = 1\n    CD = 2\n    DT = 3\n    EX = 4\n    FW = 5\n    IN = 6\n    JJ = 7\n    JJR = 8\n    JJS = 9\n    LS = 10\n    MD = 11\n    NN = 12\n    NNS = 13\n    NNP = 14\n    NNPS = 15\n    PDT = 16\n    POS = 17\n    PRP = 18\n    PRP_ = 19\n    RB = 20\n    RBR = 21\n    RBS = 22\n    RP = 23\n    SYM = 24\n    TO = 25\n    UH = 26\n    VB = 27\n    VBD = 28\n    VBG = 29\n    VBN = 30\n    VBP = 31\n    VBZ = 32\n    WDT = 33\n    WP = 34\n    WP_ = 35\n    WRB = 36\n    \n    @classmethod\n    def valueByName(cls, name):\n        """"""\n        Find enum value by the name\n        Raise:\n            exception in case if the name not found\n        """"""\n        return cls.__members__[name].value\n    \n    @classmethod\n    def hasPOSName(cls, name):\n        """"""\n        Finds if provided name is known as POS\n        """"""\n        return name in cls.__members__\n\n# The determinant article labels enumeration\nclass DT(IntEnum):\n    A = 1\n    AN = 2\n    THE = 3\n    \n    @classmethod\n    def valueByName(cls, name):\n        """"""\n        Find enum value by the name\n        Raise:\n            exception in case if the name not found\n        """"""\n        return cls.__members__[name.upper()].value\n    \n    @classmethod\n    def nameByValue(cls, value):\n        """"""\n        Find enum name by the value\n        """"""\n        if value < cls.A or value > cls.THE:\n            raise Exception(""Wrong value for enumeration: "" + value)\n            \n        return [member.name for _, member in cls.__members__.items() if member.value == value][0]\n\n# The offset for POS features start    \noffset = 2\n# The number of features extracted\nn_features = offset + len(POS.__members__) + 1\n\ndef extractFeatures(node, sentence, glove, corrections = None):\n    """"""\n    Method to extract features from provided node and store it in features array\n    Arguments:\n        node: the parse tree node with sentence\n        sentence: the sentence corpora to extract features from\n        glove: the glove indices map\n        corrections: the list of corrections [optional] if building training data set\n    Return:\n        tuple with array of features for found determiner phrases with articles and\n        labels or None\n    """"""\n    """"""\n    Features map:\n    DT glove index | NN(S) glove index | if DT at the sentence start |\n    CC | CD | DT | EX | FW | IN | JJ | JJR | JJS| LS | MD | NN | NNS | NNP | NNPS | PDT\t| POS | PRP | PRP$ | RB | RBR | RBS | RP | SYM |\n    TO | UH | VB | VBD | VBG | VBN | VBP | VBZ | WDT | WP | WP$ | WRB\n    """"""\n    labels = None\n    \n    dpa_subtrees = node.dpaSubtrees()\n    features = np.zeros((len(dpa_subtrees), n_features), dtype = \'f\')\n    if corrections != None:\n        labels = np.zeros((len(dpa_subtrees),), dtype = \'int\')\n        \n    # collect features\n    row = 0\n    for st in dpa_subtrees:\n        nn_node = None\n        for node in st.leaves():\n            # collect POS type\n            pos_name = node.pos.replace(""$"", ""_"")\n            if POS.hasPOSName(pos_name):\n                pos_index = POS.valueByName(pos_name)\n                features[row, offset + pos_index] += 1\n            \n            if node.pos == \'DT\':\n                # found DT with article\n                features[row, 0] = glove[node.s_index]\n                # store flag to mark if DT at the start of sentence\n                if node.s_index == 0:\n                    features[row, 2] = 1\n                # store correction label if appropriate\n                if corrections != None and corrections[node.s_index] != None:\n                    labels[row] = DT.valueByName(corrections[node.s_index])\n                    \n            elif nn_node == None and any(node.pos == pos for pos in [\'NN\', \'NNS\', \'NNP\', \'NNPS\']):\n                # found first (proper) noun\n                features[row, 1] = glove[node.s_index]\n                nn_node = node\n                \n        # increment row index\n        row += 1\n        \n    return (features, labels)\n \n# The number of fetures with NGram\nn_features_pos_tags = 16#14#13#11\n\ndef extractPosTagsFeatures(sentence, pos_tags, glove, corrections = None, train_on_errors_only = True):\n    """"""\n    Extracts features for specified sentence\n    Arguments:\n        sentence: the sentence corpora to extract features from\n        pos_tags: the part-of-speech tags for sentence\n        glove: the glove indices map\n        corrections: the list of corrections [optional] if building training data set\n        train_on_errors_only: if true than features set generated for training based on error detection (only corrections considered for labels), \n                              otherwise features set generated for general correct DT articles detection (existing articles in sentences and \n                              corrections considered for labels)\n    Return:\n        tuple with array of features for found determiner phrases with articles and\n        labels or None\n    \n    Features map:   \n    PrW | PrW POS | DTa | FlW | FlW POS | FlW2 | FlW2 POS | FlNNs (i > 0) | FlNNs (i > 0) POS |\n    PrW2 | PrW2 POS | PrW3 (VB, VBD) | PrW3 (VB, VBD) POS | Vowel ([0, 1] | FlW (VB,VBD,VBG,VBN,VBP,VBZ) | FlW (VB,VBD,VBG,VBN,VBP,VBZ) POS\n    \n    """"""\n    articles = 0\n    for w in sentence:\n        if w.lower() in [\'a\', \'an\', \'the\']:\n            articles += 1\n    \n    features = np.zeros((articles, n_features_pos_tags), dtype = \'f\')\n    if corrections != None:\n        labels = np.zeros((articles,), dtype = \'int\')\n    else:\n        labels = None\n        \n    prev_w = np.zeros((6,), dtype = \'int\')\n    next_word_ind = [-1, -1, -1]\n    row = -1\n    dta_s_index = -1\n    next_noun_index = -1\n    for i in range(len(sentence)):\n        p_tos = pos_tags[i]\n        w_g = glove[i]\n        cor = corrections[i] if corrections != None else None\n        w = sentence[i].lower()\n        if w in [\'a\', \'an\', \'the\']:\n            # collect features\n            dta_s_index = i\n            row += 1\n            next_noun_index = i + 1\n            \n            # store allegedly incorrect DT article index to features\n            features[row, 2] = w_g\n                        \n            if train_on_errors_only == False:\n                # store sentence DT article class in labels (it may be overwritten by correction later)\n                labels[row] = DT.valueByName(w)\n                \n            if prev_w[1] > 0:\n                # add previous if its set\n                features[row, 0] = prev_w[0] # preceding word index\n                features[row, 1] = prev_w[1] # preceding word TOS\n                \n            if prev_w[3] > 0:\n                # add previous - 1 if its set\n                features[row, 9] = prev_w[2] # preceding - 1 word index\n                features[row, 10] = prev_w[3] # preceding - 1 word TOS\n  \n            if prev_w[5] > 0 and prev_w[5] in [POS.VB, POS.VBD]:\n                # add previous - 1 if its set\n                features[row, 11] = prev_w[4] # preceding - 2 word index\n                features[row, 12] = prev_w[5] # preceding - 2 word TOS\n\n            # find index of next feature word\n            next_word_ind[0] = dta_s_index + 1\n                         \n            # store collect label if any (possibly overwrite incorrect one set previously from sentence)\n            if cor != None:\n                labels[row] = DT.valueByName(cor)\n                \n        elif i == next_word_ind[0]:\n            if POS.hasPOSName(p_tos):\n                features[row, 3] = w_g # following word index\n                features[row, 4] = POS.valueByName(p_tos) # following word TOS\n                next_word_ind[1] = i + 1\n                # vowel\n                if w[0] in \'aeiou\':\n                    features[row, 13] = 1\n            else:\n                # not a tagged POS found\n                next_word_ind[0] += 1\n        elif i == next_word_ind[1]:\n            if POS.hasPOSName(p_tos):\n                features[row, 5] = w_g # following word + 1 index\n                features[row, 6] = POS.valueByName(p_tos) # following word + 1 TOS\n                next_word_ind[2] = i + 1\n            else:\n                # not a tagged POS found\n                next_word_ind[1] += 1\n        elif i == next_word_ind[2] and p_tos in [POS.VB, POS.VBD, POS.VBG, POS.VBN, POS.VBP, POS.VBZ]:\n            if POS.hasPOSName(p_tos):\n                features[row, 14] = w_g # following word + 2 index\n                features[row, 15] = POS.valueByName(p_tos) # following word + 2 TOS\n            else:\n                # not a tagged POS found\n                next_word_ind[2] += 1\n            \n        if i == next_noun_index and dta_s_index > 0:\n            # find noun following DTa if DTa is not the first word in the sentence\n            if p_tos in [\'NN\', \'NNS\']:\n                features[row, 7] = w_g # word index\n                features[row, 8] = POS.valueByName(p_tos) # PoS\n            else:\n                # increment index util reach noun\n                next_noun_index += 1\n            \n        \n        # store preceding\n        if prev_w[3] > 0:\n            prev_w[4] = prev_w[2]\n            prev_w[5] = prev_w[3]\n            \n        if prev_w[1] > 0:\n            prev_w[2] = prev_w[0]\n            prev_w[3] = prev_w[1]\n            \n        if POS.hasPOSName(p_tos):\n            prev_w[0] = w_g\n            prev_w[1] = POS.valueByName(p_tos)\n        \n       \n    return (features, labels)\n    \n           \ndef create(corpus_file, parse_tree_file, glove_file, corrections_file, test = False):\n    """"""\n    Creates new data set from provided files using NP acquired from constituency parse trees\n    Arguments:\n        corpus_file: the file text corpus\n        parse_tree_file: the file with constituency parse trees build over data corpus\n        glove_file: the file with GloVe vectors indexes for data corpus\n        corrections_file: the file with labeled corrections\n        test: the flag to indicate whether test data set should be constructed\n    Return:\n        (features, labels): the tuple with features and labels. If test parameter is True then labels\n        wil be None\n    """"""\n    text_data = utils.read_json(corpus_file)\n    parse_trees_list = utils.read_json(parse_tree_file)\n    glove_indices = utils.read_json(glove_file)\n    \n    if test == False:\n        corrections = utils.read_json(corrections_file)\n    \n    # The sanity checks\n    #\n    if len(text_data) != len(parse_trees_list):\n        raise Exception(""Text data corpora lenght: %d, not equals to the parse trees count: %d"" \n                        % (len(text_data), len(parse_trees_list)))\n    if test == False and len(corrections) != len(parse_trees_list):\n        raise Exception(""Corrections list lenght: %d, not equals to the parse trees count: %d"" \n              % (len(corrections), len(parse_trees_list)))\n    if len(glove_indices) != len(parse_trees_list):\n        raise Exception(""Glove indices list lenght: %d, not equals to the parse trees count: %d"" \n              % (len(corrections), len(parse_trees_list)))\n    \n    # iterate over constituency parse trees and extract features\n    features = None\n    labels = None\n    index = 0\n    corrected_sentences = 0\n    for tree_dict in parse_trees_list:\n        # get corrections list for the sentence\n        if test == False:\n            s_corr = corrections[index]\n        else:\n            s_corr = None\n        # get glove indices list for the sentence\n        g_list = glove_indices[index]\n        # get text corpora list for sentence\n        t_list = text_data[index]\n        # get parse tree for sentence\n        tree, _ = td.treeFromDict(tree_dict)\n\n        # do sanity checks\n        #\n        leaves = tree.leaves()\n        if test == False and len(s_corr) != len(leaves):\n            raise Exception(""Corrections list lenght: %d not equal the tree leaves count: %d at index: %d"" \n                            % (len(s_corr), len(leaves), index))\n        if len(g_list) != len(leaves):\n            raise Exception(""Glove indices list lenght: %d not equal the tree leaves count: %d at index: %d"" \n                            % (len(g_list), len(leaves), index))\n        if len(t_list) != len(leaves):\n            raise Exception(""Text corpora list lenght: %d not equal the tree leaves count: %d at index: %d"" \n                            % (len(t_list), len(leaves), index))\n        \n        # check if sentence has corrections\n        #\n        if test == False and any(w != None for w in s_corr):\n            corrected_sentences += 1\n            \n        # generate features and labels\n        #\n        f, l = extractFeatures(node = tree, sentence = t_list, glove = g_list, corrections = s_corr)\n        if index == 0:\n            features = f\n        else:\n            features = np.concatenate((features, f), axis=0)\n        \n        if index == 0:\n            labels = l\n        elif test == False:\n            labels = np.concatenate((labels, l))\n        \n        index += 1\n        \n        \n    print(""Features collected: %d"" % (len(features)))\n    \n    return (features, labels)\n\ndef createWithPosTags(corpus_file, pos_tags_file, glove_file, corrections_file, test = False):\n    """"""\n    Creates new features set from provided files using pos tags\n    Arguments:\n        corpus_file: the file text corpus\n        pos_tags_file: the file with pos tags for data corpus\n        glove_file: the file with GloVe vectors indexes for data corpus\n        corrections_file: the file with labeled corrections\n        test: the flag to indicate whether test data set should be constructed\n    Return:\n        (features, labels): the tuple with features and labels. If test parameter is True then labels\n        wil be None\n    """"""\n    text_data = utils.read_json(corpus_file)\n    pos_tags = utils.read_json(pos_tags_file)\n    glove_indices = utils.read_json(glove_file)\n    \n    if test == False:\n        corrections = utils.read_json(corrections_file)\n    \n    # The sanity checks\n    #\n    if len(text_data) != len(pos_tags):\n        raise Exception(""Text data corpora lenght: %d, not equals to the pos tags lists length: %d"" \n                        % (len(text_data), len(pos_tags)))\n    if test == False and len(corrections) != len(pos_tags):\n        raise Exception(""Corrections list lenght: %d, not equals to the pos tags lists length: %d"" \n              % (len(corrections), len(pos_tags)))\n    if len(glove_indices) != len(pos_tags):\n        raise Exception(""Glove indices list lenght: %d, not equals to the pos tags lists length: %d"" \n              % (len(corrections), len(pos_tags)))\n    \n    features = None\n    labels = None\n    index = 0\n    for index in range(len(pos_tags)):\n        # get corrections list for the sentence\n        if test == False:\n            s_corr = corrections[index]\n        else:\n            s_corr = None\n        # get glove indices list for the sentence\n        g_list = glove_indices[index]\n        # get text corpora list for sentence\n        t_list = text_data[index]\n        # the pos tags for sentence\n        pt_list = pos_tags[index]\n        \n        # do sanity checks\n        #\n        if test == False and len(s_corr) != len(pt_list):\n            raise Exception(""Corrections list lenght: %d not equal the pos tags count: %d at index: %d"" \n                            % (len(s_corr), len(pt_list), index))\n        if len(g_list) != len(pt_list):\n            raise Exception(""Glove indices list lenght: %d not equal the pos tags count: %d at index: %d"" \n                            % (len(g_list), len(pt_list), index))\n        if len(t_list) != len(pt_list):\n            raise Exception(""Text corpora list lenght: %d not equal the pos tags count: %d at index: %d"" \n                            % (len(t_list), len(pt_list), index))\n            \n        # generate features and labels\n        #\n        f, l = extractPosTagsFeatures(pos_tags = pt_list, sentence = t_list, glove = g_list, \n                                      corrections = s_corr, train_on_errors_only = True)\n        if index == 0:\n            features = f\n        else:\n            features = np.concatenate((features, f), axis=0)\n        \n        if index == 0:\n            labels = l\n        elif test == False:\n            labels = np.concatenate((labels, l))\n    \n    print(""Features collected: %d with dimension: %d"" % (features.shape[0], features.shape[1]))\n        \n    return (features, labels)\n        \nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'corpora\', help = \'the name of data coprora to process\')\n    parser.add_argument(\'--f_type\', default = \'tree\', \n                        help = \'the type of features to be generated [tree, tags]\')\n    args = parser.parse_args()\n    \n    \n    if args.f_type in [\'tree\', \'tags\'] == False:\n        raise Exception(""Unknown features type: "" + args.f_type)\n    \n    # Create output directory\n    if os.path.exists(config.intermediate_dir) == False:\n        os.makedirs(config.intermediate_dir)\n        \n    print(""Making %s features with type [%s]"" % (args.corpora, args.f_type))\n    \n    # Create train data corpus\n    #\n    if args.corpora == \'train\':\n        if args.f_type == \'tree\':\n            features, labels = create(corpus_file = config.sentence_train_path, \n                                 parse_tree_file = config.parse_train_path,\n                                 glove_file = config.glove_train_path, \n                                 corrections_file = config.corrections_train_path)\n        elif args.f_type == \'tags\':\n            features, labels = createWithPosTags(corpus_file = config.sentence_train_path, \n                                 pos_tags_file = config.pos_tags_train_path,\n                                 glove_file = config.glove_train_path, \n                                 corrections_file = config.corrections_train_path)\n            \n        np.save(config.train_features_path, features)\n        np.save(config.train_labels_path, labels)\n    \n    # Create validate data corpus\n    #\n    elif args.corpora == \'validate\':\n        if args.f_type == \'tree\':\n            features, labels = create(corpus_file = config.sentence_validate_path, \n                                 parse_tree_file = config.parse_validate_path,\n                                 glove_file = config.glove_validate_path, \n                                 corrections_file = config.corrections_validate_path)\n        elif args.f_type == \'tags\':\n            features, labels = createWithPosTags(corpus_file = config.sentence_validate_path, \n                                 pos_tags_file = config.pos_tags_validate_path,\n                                 glove_file = config.glove_validate_path, \n                                 corrections_file = config.corrections_validate_path)\n            \n        np.save(config.validate_features_path, features)\n        np.save(config.validate_labels_path, labels)\n    \n    # Create test data features\n    #\n    elif args.corpora == \'test\':\n        if args.f_type == \'tree\':\n            features, _ = create(corpus_file = config.sentence_test_path, \n                             parse_tree_file = config.parse_test_path,\n                             glove_file = config.glove_test_path, \n                             corrections_file = config.corrections_test_path,\n                             test = True)\n        elif args.f_type == \'tags\':\n            features, _ = createWithPosTags(corpus_file = config.sentence_test_path, \n                             pos_tags_file = config.pos_tags_test_path,\n                             glove_file = config.glove_test_path, \n                             corrections_file = None,\n                             test = True)\n            \n        np.save(config.test_features_path, features)\n    else:\n        raise Exception(""Unknown coprpora type: "" + args.corpora)\n'"
src/data_set_test.py,10,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nThe test cases for data set implementation\n\n@author: yaric\n""""""\nimport unittest\nimport numpy as np\n\nimport utils\nimport config\nimport data_set as ds\nimport tree_dict as td\n\nclass TestDataSetMethods(unittest.TestCase):\n    \n    def test_extract_features(self):\n        text_data = utils.read_json(config.sentence_train_path)\n        corrections = utils.read_json(config.corrections_train_path)\n        parse_trees_list = utils.read_json(config.parse_train_path)\n        glove_indices = utils.read_json(config.glove_train_path)\n        \n        index = 1\n        tree, _ = td.treeFromDict(parse_trees_list[index])\n        \n        features, labels = ds.extractFeatures(node = tree, \n                                              sentence = text_data[index],\n                                              glove = glove_indices[index],\n                                              corrections = corrections[index])\n        \n        self.assertEqual(len(features), 3, ""Wrong features list size"")\n        self.assertEqual(features.shape[1], ds.n_features,\n                          ""Wrong feature dimensions: %d"" % features.shape[1])\n        \n        # check labels\n        labels_test = np.array([0, ds.DT.A, ds.DT.THE], dtype = ""int"")\n        self.assertTrue(np.all(labels == labels_test), ""Wrong labels generated"")\n        \n        # check features\n        features_test = np.zeros((3, ds.n_features), dtype = ""f"")\n        features_test[0, 0] = 2\n        features_test[0, 1] = 15189\n        features_test[0, 2] = 0\n        features_test[0, ds.offset + ds.POS.DT.value] = 1\n        features_test[0, ds.offset + ds.POS.NN.value] = 1\n        \n        features_test[1, 0] = 2\n        features_test[1, 1] = 19803\n        features_test[1, 2] = 0\n        features_test[1, ds.offset + ds.POS.DT.value] = 1\n        features_test[1, ds.offset + ds.POS.NNP.value] = 1\n        features_test[1, ds.offset + ds.POS.NN.value] = 1\n        \n        features_test[2, 0] = 6\n        features_test[2, 1] = 2062\n        features_test[2, 2] = 0\n        features_test[2, ds.offset + ds.POS.DT.value] = 1\n        features_test[2, ds.offset + ds.POS.NN.value] = 2\n        features_test[2, ds.offset + ds.POS.IN.value] = 1\n        features_test[2, ds.offset + ds.POS.PRP_.value] = 1\n        \n        select = features == features_test\n        if np.all(select) == False:\n            print(np.argmin(select, axis = 0))\n            self.fail(""Wrong features generated"")\n    \n    def test_extract_Pos_Tags_Features(self):\n        text_data = utils.read_json(config.sentence_train_path)\n        corrections = utils.read_json(config.corrections_train_path)\n        glove_indices = utils.read_json(config.glove_train_path)\n        pos_tags = utils.read_json(config.pos_tags_train_path)\n        \n        \n        s_index = 1\n        \n        features, labels = ds.extractPosTagsFeatures(text_data[s_index], pos_tags[s_index], \n                                                     glove_indices[s_index], corrections[s_index])\n        self.assertEqual(len(features), 3, ""Wrong features list size"")\n        self.assertEqual(features.shape[1], ds.n_features_pos_tags,\n                          ""Wrong feature dimensions: %d"" % features.shape[1])\n        \n        # check labels\n        labels_test = np.array([0, ds.DT.A, ds.DT.THE], dtype = ""int"")\n        self.assertTrue(np.all(labels == labels_test), ""Wrong labels generated"")\n        \n        # check features\n        """"""\n        PrW | PrW POS | DTa | FlW | FlW POS | FlW2 | FlW2 POS | FlNNs (i > 0) | FlNNs (i > 0) POS |\n        PrW2 | PrW2 POS | PrW3 (VB, VBD) | PrW3 (VB, VBD) POS | Vowel ([0, 1] | FlW (VB,VBD,VBG,VBN,VBP,VBZ) | FlW (VB,VBD,VBG,VBN,VBP,VBZ) POS\n        """"""\n        features_test = np.array([\n                [21176.0, 27.0, 2.0, 15189.0, 12.0, 28.0, 6.0, 15189.0, 12.0, 18.0, 18.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n                [28.0, 6.0, 2.0, 19803.0, 14.0, 16560.0, 12.0, 16560.0, 12.0, 15189.0, 12.0, 0.0, 0.0, 0.0, 0.0, 0.0], \n                [365.0, 27.0, 6.0, 2062.0, 12.0, 5.0, 6.0, 2062.0, 12.0, 4.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0]], dtype = ""f"")\n        \n        select = features == features_test\n        if np.all(select) == False:\n            print(np.argmin(select, axis = 0))\n            self.fail(""Wrong features generated"")\n        \n    def test_create_train_data_set_WithPosTags(self):\n        print(""Train -- "")\n        features, labels = ds.createWithPosTags(\n                 corpus_file = config.sentence_train_path, \n                 pos_tags_file = config.pos_tags_train_path,\n                 glove_file = config.glove_train_path, \n                 corrections_file = config.corrections_train_path)\n        \n        self.assertEqual(len(features), len(labels), \n                          ""The train features list has size not equal to the labels"")\n        self.assertEqual(features.shape[1], ds.n_features_pos_tags,\n                          ""Wrong train feature dimensions: %d"" % features.shape[1])\n        \n    def test_create_validate_data_set_WithPosTags(self):\n        print(""Validate -- "")\n        features, labels = ds.createWithPosTags(\n                 corpus_file = config.sentence_validate_path, \n                 pos_tags_file = config.pos_tags_validate_path,\n                 glove_file = config.glove_validate_path, \n                 corrections_file = config.corrections_validate_path)\n        \n        self.assertEqual(len(features), len(labels), \n                          ""The validate features list has size not equal to the labels"")\n        self.assertEqual(features.shape[1], ds.n_features_pos_tags,\n                          ""Wrong validate feature dimensions: %d"" % features.shape[1])\n        \n    def test_create_test_data_set_WithPosTags(self):\n        print(""Test -- "")\n        features, labels = ds.createWithPosTags(\n                 corpus_file = config.sentence_test_path, \n                 pos_tags_file = config.pos_tags_test_path,\n                 glove_file = config.glove_test_path, \n                 corrections_file = None,\n                 test = True)\n        \n        self.assertIsNone(labels, ""Labels should not be returned"")\n        self.assertGreater(features.shape[0], 0, ""Empty test features returned"")\n        self.assertEqual(features.shape[1], ds.n_features_pos_tags,\n                          ""Wrong test feature dimensions: %d"" % features.shape[1])\n\n    def test_create_train_data_set(self):\n        print(""Train - "")\n        features, labels = ds.create(\n                corpus_file = config.sentence_train_path, \n                parse_tree_file = config.parse_train_path,\n                glove_file = config.glove_train_path, \n                corrections_file = config.corrections_train_path)\n        \n        self.assertEqual(len(features), len(labels), \n                          ""The train features list has size not equal to the labels"")\n        self.assertEqual(features.shape[1], ds.n_features,\n                          ""Wrong feature dimensions: %d"" % features.shape[1])\n       \n    def test_create_validate_data_set(self):\n        print(""Validate - "")\n        features, labels = ds.create(\n                corpus_file = config.sentence_validate_path, \n                parse_tree_file = config.parse_validate_path,\n                glove_file = config.glove_validate_path, \n                corrections_file = config.corrections_validate_path)\n        \n        self.assertEqual(len(features), len(labels), \n                          ""The validate features list has size not equal to the labels"")\n        self.assertEqual(features.shape[1], ds.n_features,\n                          ""Wrong feature dimensions: %d"" % features.shape[1])\n        \n    def test_create_test_data_set(self):\n        print(""Test - "")\n        features, labels = ds.create(\n                corpus_file = config.sentence_test_path, \n                parse_tree_file = config.parse_test_path,\n                glove_file = config.glove_test_path, \n                corrections_file = None,\n                test = True)\n        \n        self.assertIsNone(labels, ""Labels should not be returned"")\n        self.assertGreater(features.shape[0], 0, ""Empty features returned"")\n        self.assertEqual(features.shape[1], ds.n_features,\n                          ""Wrong feature dimensions: %d"" % features.shape[1])\n    \nif __name__ == \'__main__\':\n    unittest.main()'"
src/evaluate.py,0,"b'""""""\nPerform prediction results evaluation\n""""""\nimport json\nimport argparse\n\ndef evaluate(text_file, correct_file, submission_file):\n    with open(text_file) as f:\n        text = json.load(f)\n    with open(correct_file) as f:\n        correct = json.load(f)\n    with open(submission_file) as f:\n        submission = json.load(f)\n    count_fp = {""a"" : 0, ""an"" : 0, ""the"" : 0}\n    count_fn = {""a"" : 0, ""an"" : 0, ""the"" : 0}\n    count_tp = {""a"" : 0, ""an"" : 0, ""the"" : 0}\n    count_tn = {""a"" : 0, ""an"" : 0, ""the"" : 0}\n    data = []\n    for sent, cor, sub in zip(text, correct, submission):\n        for w, c, s in zip(sent, cor, sub):\n            w = w.lower()\n            if w in [\'a\', \'an\', \'the\']:\n                if c is not None:\n                    if s is None:\n                        count_fn[w] += 1\n                    elif s[0] != c:\n                        count_fp[s[0]] += 1\n                    else:\n                        count_tp[s[0]] += 1\n                elif s is not None:\n                    count_fp[s[0]] +=1\n                else:\n                    count_tn[w] += 1\n                        \n                if s is None or s[0] == w:\n                    s = [\'\', float(\'-inf\')]\n                data.append((-s[1], s[0] == c, c is not None)) # (confidence, TP, TP + FN)\n                \n    data.sort()\n    fp2 = 0\n    fp = 0\n    tp = 0\n    all_mistakes = sum(x[2] for x in data)\n    score = 0\n    acc = 0\n    for _, c, r in data:\n        fp2 += not c #=~TP\n        fp += not r #=~(TP + FN)\n        tp += c\n        acc = max(acc, 1 - (0. + fp + all_mistakes - tp) / len(data))\n        if fp2 * 1. / len(data) <= 0.02:\n            score = tp * 1. / all_mistakes\n\n    print(\'tp: %d, fp: %d, fp2: %d, from: %d\' % (tp, fp, fp2, len(data)))\n    print(\'FP counts: %s \\nFN counts: %s\\nTP counts: %s\\nTN counts: %s\' % (count_fp, count_fn, count_tp, count_tn))\n    print( \'>>> target score = %.2f %%\' % (score * 100))\n    print( \'>>> accuracy (just for info) = %.2f %%\' % (acc * 100))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'The prediction results evaluator\')\n    parser.add_argument(\'--results_file\', required = True, \n                        help=\'the path to the file with predictions resulst\')\n    parser.add_argument(\'--test_sentences_file\', required = True, \n                        help=""the text\'s corpora file for test data"")\n    parser.add_argument(\'--test_corrections_file\', required = True, \n                        help=\'the path to the file with ethalon corrections\')\n    args = parser.parse_args()\n    \n    evaluate(text_file = args.test_sentences_file, \n             correct_file = args.test_corrections_file, \n             submission_file = args.results_file)\n'"
src/ngram_res.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Mar 15 18:38:47 2017\n\n@author: yaric\n""""""\n\nimport pickle\nimport argparse\nimport json\n\nimport numpy as np\nimport config\nimport utils\n\nfrom nltk_ngram import LidstoneNgramModel, NgramModelVocabulary, NgramCounter, MLENgramModel\n\nn_gram_left = 3\ndt_list = [\'a\', \'an\', \'the\']\nconfidence_threshold = 0#.1\n\ndef buildPredictions(text_data, predictor):\n    res_list = list()\n    for sentence in text_data:\n        row_data = list()\n        for i in range(len(sentence)):\n            w = sentence[i].lower()\n            if w in dt_list:\n                # collect previous words\n                if i >= n_gram_left:\n                    context = sentence[i - n_gram_left:i]\n                else:\n                    context = sentence[:i]\n                if len(context) > 0:\n                    # do predictions\n                    predictions = [predictor(dt, context) for dt in dt_list]\n                    max_lab_ind = np.argmax(predictions) # the most confident prediction\n                    max_dt = dt_list[max_lab_ind]\n                    if max_dt != w and predictions[max_lab_ind] > confidence_threshold:\n                       row_data.append([max_dt, predictions[max_lab_ind]]) \n                    else:\n                      # predicted already present DT or too low confidence\n                      row_data.append(None) \n                else:\n                    # what happens when DT at the beginning of sentence?\n                    predictions = None\n                    row_data.append(None) \n            else:\n                # ordinary word\n                row_data.append(None)\n                \n        # store results\n        res_list.append(row_data)\n        \n    return res_list\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'The results generator based on traine n-gram model\')\n    parser.add_argument(\'--out_file\', default=config.test_reults_path, \n                        help=\'the file to store predictions results in custom format\')\n    parser.add_argument(\'--test_sentences_file\', default=config.sentence_test_path, \n                        help=""the text\'s corpora file for test data"")\n    parser.add_argument(\'--model_file\', default=config.ngram_model_path, \n                        help=\'the path to the file with predictions label Numpy array\')\n    \n    args = parser.parse_args()\n    \n    print(""Generating test results, model: [%s], text: [%s]"" % (args.model_file, args.test_sentences_file))\n    \n    with open(args.model_file, \'rb\') as f:\n        counter = pickle.load(f)\n        \n    # The predictor\n    predictor = lambda x, context: counter.ngrams[len(context) + 1][tuple(context)].freq(x)\n    \n    text_data = utils.read_json(args.test_sentences_file)\n    result_list = buildPredictions(text_data, predictor)\n    \n    if len(result_list) != len(text_data):\n        raise Exception(""Text list size not equal to results list size, %d != %d"" % (len(text_data), len(result_list)))\n    \n    # Save results\n    with open(args.out_file, mode = \'w\') as f:\n        json.dump(result_list, f)\n    \n    print(""Prediction results saved to: "" + args.out_file)\n    \n    """"""\n    words = [\'tax\', \'deductible\', \'to\']\n    \n    the_prob = counter.ngrams[4][(\'tax\', \'deductible\', \'to\',)].freq(\'the\')\n    a_prob = counter.ngrams[4][(\'tax\', \'deductible\', \'to\',)].freq(\'a\')\n    print(""Probability: the = %f, a = %f"" % (the_prob, a_prob))\n    \n    counter.ngrams[2][(\'leaving\',)].freq(\'the\')\n    counter.ngrams[2][(\'leaving\',)].freq(\'a\')\n    """"""\n'"
src/ngram_res_test.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Thu Mar 16 13:21:58 2017\n\n@author: yaric\n""""""\n\nimport unittest\n\nimport ngram_res as nr\n\nclass TestNgramResultsMethods(unittest.TestCase):\n    \n    def test_buildPredictions(self):\n        text_data =  [[""Since"", ""you"", ""see"", ""that"", ""in"", ""most"", ""notionally"", ""ideological"", ""autocracies"", "","", ""whether"", ""\\"""", ""religious"", ""\\"""", ""or"", "","", ""e.g."", "","", ""\\"""", ""Communist"", ""\\"""", "","", ""I"", ""do"", ""n\\\'t"", ""think"", ""its"", ""unreasonable"", "","", ""though"", ""Iran"", ""could"", ""be"", ""exceptional"", "".""], \n        [""Dice"", "","", ""funny"", ""that"", ""you"", ""redefine"", ""the"", ""govt"", ""as"", ""the"", ""Shia"", ""faction"", "","", ""then"", ""use"", ""your"", ""newly"", ""coined"", ""definition"", ""to"", ""change"", ""a"", ""meaning"", ""of"", ""my"", ""post"", "","", ""then"", ""call"", ""me"", ""\\"""", ""deeply"", ""ignorant"", ""\\"""", ""for"", ""saying"", ""something"", ""other"", ""than"", ""what"", ""I"", ""did"", "".""], \n        [""But"", ""those"", ""are"", ""a"", ""rules"", ""on"", ""a"", ""Lefty"", ""site"", ""where"", ""logic"", ""takes"", ""second"", ""place"", ""to"", ""name"", ""calling"", ""and"", ""sophistry"", "".""], \n        [""So"", ""then"", ""where"", ""is"", ""the"", ""civil"", ""war"", ""?""]]\n        \n        predictor = lambda x, context: 0.7 if x == \'a\' else 0.6 if x == \'the\' else 0.3 if x == \'an\' else -1\n                \n        results = nr.buildPredictions(text_data, predictor)\n        \n        \n        self.assertEqual(len(results), 4, ""Wrong results list length"")\n        for s, r in zip(text_data, results):\n            self.assertEqual(len(r), len(s), ""Prediction results for sentence has wrong legth"")\n            for w, p in zip(s, r):\n                if w in nr.dt_list:\n                    if w != \'a\':\n                        self.assertEqual(p[1], 0.7, ""Wrong confidence: "" + str(p))\n                else:\n                    self.assertIsNone(p, ""Prediction should be none for word: "" + w)\n        \n        \nif __name__ == \'__main__\':\n    unittest.main()'"
src/nltk_ngram.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Wed Mar 15 12:58:14 2017\n\n@author: yaric\n""""""\nimport json\nfrom math import log\nimport pickle\n\nfrom nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer\n\nfrom collections import Counter, defaultdict\nfrom copy import copy\nfrom itertools import chain\n\nfrom nltk.util import ngrams\nfrom nltk.probability import FreqDist, ConditionalFreqDist\n\nimport config\n\ndef build_vocabulary(cutoff, *texts):\n    combined_texts = chain(*texts)\n    return NgramModelVocabulary(cutoff, combined_texts)\n\n\ndef count_ngrams(order, vocabulary, *training_texts, **counter_kwargs):\n    counter = NgramCounter(order, vocabulary, **counter_kwargs)\n    for text in training_texts:\n        counter.train_counts(text)\n    return counter\n\n\nclass NgramModelVocabulary(Counter):\n    """"""Stores language model vocabulary.\n    Satisfies two common language modeling requirements for a vocabulary:\n    - When checking membership and calculating its size, filters items by comparing\n      their counts to a cutoff value.\n    - Adds 1 to its size so as to account for ""unknown"" tokens.\n    """"""\n\n    def __init__(self, unknown_cutoff, *counter_args):\n        Counter.__init__(self, *counter_args)\n        self.cutoff = unknown_cutoff\n\n    def __contains__(self, item):\n        """"""Only consider items with counts GE to cutoff as being in the vocabulary.""""""\n        return self[item] >= self.cutoff\n\n    def __len__(self):\n        """"""This should reflect a) filtering items by count, b) accounting for unknowns.\n        The first is achieved by relying on the membership check implementation.\n        The second is achieved by adding 1 to vocabulary size.\n        """"""\n        # the if-clause here looks a bit dumb, should we make it clearer?\n        return sum(1 for item in self if item in self) + 1\n\n    def __copy__(self):\n        return self.__class__(self.cutoff, self)\n\n\nclass EmptyVocabularyError(Exception):\n    pass\n\n\nclass NgramCounter(object):\n    """"""Class for counting ngrams""""""\n\n    def __init__(self, order, vocabulary, unk_cutoff=None, unk_label=""<UNK>"", **ngrams_kwargs):\n        """"""\n        :type training_text: List[List[str]]\n        """"""\n\n        if order < 1:\n            message = ""Order of NgramCounter cannot be less than 1. Got: {0}""\n            raise ValueError(message.format(order))\n\n        self.order = order\n        self.unk_label = unk_label\n\n        # Preset some common defaults...\n        self.ngrams_kwargs = {\n            ""pad_left"": True,\n            ""pad_right"": True,\n            ""left_pad_symbol"": ""<s>"",\n            ""right_pad_symbol"": ""</s>""\n        }\n        # While allowing whatever the user passes to override them\n        self.ngrams_kwargs.update(ngrams_kwargs)\n        # Set up the vocabulary\n        self._set_up_vocabulary(vocabulary, unk_cutoff)\n\n        self.ngrams = defaultdict(ConditionalFreqDist)\n        self.unigrams = FreqDist()\n\n    def _set_up_vocabulary(self, vocabulary, unk_cutoff):\n        self.vocabulary = copy(vocabulary)  # copy needed to prevent state sharing\n        if unk_cutoff is not None:\n            # If cutoff value is provided, override vocab\'s cutoff\n            self.vocabulary.cutoff = unk_cutoff\n\n        if self.ngrams_kwargs[\'pad_left\']:\n            lpad_sym = self.ngrams_kwargs.get(""left_pad_symbol"")\n            self.vocabulary[lpad_sym] = self.vocabulary.cutoff\n\n        if self.ngrams_kwargs[\'pad_right\']:\n            rpad_sym = self.ngrams_kwargs.get(""right_pad_symbol"")\n            self.vocabulary[rpad_sym] = self.vocabulary.cutoff\n\n    def _enumerate_ngram_orders(self):\n        return enumerate(range(self.order, 1, -1))\n\n    def train_counts(self, training_text):\n        # Note here ""1"" indicates an empty vocabulary!\n        # See NgramModelVocabulary __len__ method for more.\n        if len(self.vocabulary) <= 1:\n            raise EmptyVocabularyError(""Cannot start counting ngrams until ""\n                                       ""vocabulary contains more than one item."")\n\n        for sent in training_text:\n            checked_sent = (self.check_against_vocab(word) for word in sent)\n            sent_start = True\n            for ngram in self.to_ngrams(checked_sent):\n                context, word = tuple(ngram[:-1]), ngram[-1]\n\n                if sent_start:\n                    for context_word in context:\n                        self.unigrams[context_word] += 1\n                    sent_start = False\n\n                for trunc_index, ngram_order in self._enumerate_ngram_orders():\n                    trunc_context = context[trunc_index:]\n                    # note that above line doesn\'t affect context on first iteration\n                    self.ngrams[ngram_order][trunc_context][word] += 1\n                self.unigrams[word] += 1\n\n    def check_against_vocab(self, word):\n        if word in self.vocabulary:\n            return word\n        return self.unk_label\n\n    def to_ngrams(self, sequence):\n        """"""Wrapper around util.ngrams with usefull options saved during initialization.\n        :param sequence: same as nltk.util.ngrams\n        :type sequence: any iterable\n        """"""\n        return ngrams(sequence, self.order, **self.ngrams_kwargs)\n    \nNEG_INF = float(""-inf"")\n\n\nclass BaseNgramModel(object):\n    """"""An example of how to consume NgramCounter to create a language model.\n    This class isn\'t intended to be used directly, folks should inherit from it\n    when writing their own ngram models.\n    """"""\n\n    def __init__(self, ngram_counter):\n\n        self.ngram_counter = ngram_counter\n        # for convenient access save top-most ngram order ConditionalFreqDist\n        self.ngrams = ngram_counter.ngrams[ngram_counter.order]\n        self._ngrams = ngram_counter.ngrams\n        self._order = ngram_counter.order\n\n        self._check_against_vocab = self.ngram_counter.check_against_vocab\n\n    def check_context(self, context):\n        """"""Makes sure context not longer than model\'s ngram order and is a tuple.""""""\n        if len(context) >= self._order:\n            raise ValueError(""Context is too long for this ngram order: {0}"".format(context))\n        # ensures the context argument is a tuple\n        return tuple(context)\n\n    def score(self, word, context):\n        """"""\n        This is a dummy implementation. Child classes should define their own\n        implementations.\n        :param word: the word to get the probability of\n        :type word: str\n        :param context: the context the word is in\n        :type context: Tuple[str]\n        """"""\n        return 0.5\n\n    def logscore(self, word, context):\n        """"""\n        Evaluate the log probability of this word in this context.\n        This implementation actually works, child classes don\'t have to\n        redefine it.\n        :param word: the word to get the probability of\n        :type word: str\n        :param context: the context the word is in\n        :type context: Tuple[str]\n        """"""\n        score = self.score(word, context)\n        if score == 0.0:\n            return NEG_INF\n        return log(score, 2)\n\n    def entropy(self, text):\n        """"""\n        Calculate the approximate cross-entropy of the n-gram model for a\n        given evaluation text.\n        This is the average log probability of each word in the text.\n        :param text: words to use for evaluation\n        :type text: Iterable[str]\n        """"""\n\n        normed_text = (self._check_against_vocab(word) for word in text)\n        H = 0.0     # entropy is conventionally denoted by ""H""\n        processed_ngrams = 0\n        for ngram in self.ngram_counter.to_ngrams(normed_text):\n            context, word = tuple(ngram[:-1]), ngram[-1]\n            H += self.logscore(word, context)\n            processed_ngrams += 1\n        return - (H / processed_ngrams)\n\n    def perplexity(self, text):\n        """"""\n        Calculates the perplexity of the given text.\n        This is simply 2 ** cross-entropy for the text.\n        :param text: words to calculate perplexity of\n        :type text: Iterable[str]\n        """"""\n\n        return pow(2.0, self.entropy(text))\n\n\nclass MLENgramModel(BaseNgramModel):\n    """"""Class for providing MLE ngram model scores.\n    Inherits initialization from BaseNgramModel.\n    """"""\n\n    def score(self, word, context):\n        """"""Returns the MLE score for a word given a context.\n        Args:\n        - word is expcected to be a string\n        - context is expected to be something reasonably convertible to a tuple\n        """"""\n        context = self.check_context(context)\n        return self.ngrams[context].freq(word)\n\nclass LidstoneNgramModel(BaseNgramModel):\n    """"""Provides Lidstone-smoothed scores.\n    In addition to initialization arguments from BaseNgramModel also requires\n    a number by which to increase the counts, gamma.\n    """"""\n\n    def __init__(self, gamma, *args):\n        super(LidstoneNgramModel, self).__init__(*args)\n        self.gamma = gamma\n        # This gets added to the denominator to normalize the effect of gamma\n        self.gamma_norm = len(self.ngram_counter.vocabulary) * gamma\n\n    def score(self, word, context):\n        context = self.check_context(context)\n        context_freqdist = self.ngrams[context]\n        word_count = context_freqdist[word]\n        ctx_count = context_freqdist.N()\n        return (word_count + self.gamma) / (ctx_count + self.gamma_norm)\n\nclass LaplaceNgramModel(LidstoneNgramModel):\n    """"""Implements Laplace (add one) smoothing.\n    Initialization identical to BaseNgramModel because gamma is always 1.\n    """"""\n\n    def __init__(self, *args):\n        super(LaplaceNgramModel, self).__init__(1, *args)\n    \nif __name__ == \'__main__\':\n\n    with open(\'../data/ngrams.txt\') as f:\n        ngrams_txt = json.load(f)\n        \n    corpora = list()\n    words = list()\n    for key, value in ngrams_txt.items():\n        ws = key.split()\n        corpora.append(ws)\n        words.extend(ws)\n\n    print(""Collected %d ngrams with %d words"" % (len(corpora), len(words)))\n    #with open(""../out/corpora.pkl"", ""wb"") as f:\n    #    pickle.dump(corpora, f)\n\n    vocab = build_vocabulary(1, words)\n    print(""Vocabulary built"")\n    counter = count_ngrams(5, vocab, corpora)\n    print(""Counter ready"")\n    with open(config.ngram_model_path, ""wb"") as f:\n        pickle.dump(counter, f)\n'"
src/predictor.py,6,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nThe predictive models runner. Encapsulates common functionality which can be applied\nto different predictors.\n\n@author: yaric\n""""""\nimport os\nimport shutil\nimport argparse\n\nimport numpy as np\nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\n\nfrom random_forest_model import RandomForest\nimport config\n\ndef predict(predictor_name, X_test, save_model = False, validate_model = True, save_labels = False):\n    """"""\n    Invoked to predict labels for provided test data features\n    Arguments:\n        predictor_name: the name of predictor to use\n        X_test: the test data features [n_samples, n_features] for labels prediction\n        save_model: flag to indicate whether to save trained model\n        validate_model: flag to indicate whether to run trained model against validation data\n        save_labels: the flag to indicate whether to save predicted labels array\n    Return:\n        tuple with predicted labels and validation score\n    """"""\n    corpora = __loadTrainCorpora()\n    if predictor_name == \'RandomForest\':\n        predictor = RandomForest(n_estimators = 1500)\n    else:\n        raise Exception(""Unknown predictor name: "" + predictor_name)\n        \n    # statndardize features\n    X_scaler = StandardScaler(with_mean = False)\n    X_train = X_scaler.fit_transform(corpora[""train""][""features""])\n    Y_train = corpora[""train""][""labels""]\n    \n    # train model\n    model = predictor.train(X_train, Y_train)\n    v_score = None    \n    if validate_model:\n        v_score = __validate(corpora[""validate""][""features""], corpora[""validate""][""labels""], model, X_scaler)\n        print(""validate score = %.3f"" % (v_score))\n    \n    # predict\n    labels = __predict(X_test, model, X_scaler)\n    if save_labels:\n        np.save(config.test_labels_prob_path, labels)\n        print(""Predicted labels saved to: "" + config.test_labels_prob_path)\n        \n    if save_model:\n        predictor.X_scaler = X_scaler\n        __savePredictorModel(predictor)\n\n    return (labels, v_score)\n    \ndef __validate(X_test, labels, model, X_scaler):\n    """"""\n    Run trained models against validation data\n    Arguments:\n        X: the test data [n_samples, n_features]\n        labels: the GT labels [n_samples, n_classes]\n        model: the prediction model\n        X_scaler: the standard scaler to scale input features\n    Return:\n         the mean accuracy on the given test data and labels\n    """"""\n    X_test = X_scaler.transform(X_test)\n    return model.score(X_test, labels)\n\ndef __predict(X_test, model, X_scaler):\n    """"""\n    Do prediction for provided fetures\n    Arguments:\n        X_test: the test data [n_samples, n_features]\n        model: the classification predictive model\n        X_scaler: the standard scaler used to scale train features\n    Return:\n        predicted labels as array of shape = [n_samples, n_classes] with probabilities\n        of each class\n    """"""\n    X_test = X_scaler.transform(X_test)\n    labels = model.predict_proba(X_test)\n    return labels\n\ndef __savePredictorModel(predictor):\n    """"""\n    Saves trained model \n    """"""\n    if predictor.model == None or predictor.X_scaler == None:\n        raise Exception(""Model not trained yet - nothing to save"")\n        \n    # Create output directory\n    model_dir = os.path.dirname(predictor.model_path)\n    if os.path.exists(model_dir) == True:\n        shutil.rmtree(model_dir)  \n    os.makedirs(model_dir)\n    \n    # save model and scaler \n    joblib.dump(predictor.model, predictor.model_path)\n    joblib.dump(predictor.X_scaler, predictor.scaler_path)\n    \n    print(""Model saved to: "" + model_dir)\n    \ndef __loadPredictorModel(predictor):\n    """"""\n    Loads model from predefined location\n    Return:\n        loaded model\n    """"""\n    predictor.model = joblib.load(predictor.model_path)\n    predictor.X_scaler = joblib.load(predictor.scaler_path)\n    \ndef __loadTrainCorpora():\n    # check that train corpora exists\n    if any([os.path.exists(path) == False for path in \n            [config.train_features_path, config.train_labels_path, \n             config.validate_features_path, config.validate_labels_path]]):\n        raise Exception(""Necessary train corpora files not found"")\n        \n    # Load data\n    train_features = np.load(config.train_features_path)\n    train_labels = np.load(config.train_labels_path)\n    validate_features = np.load(config.validate_features_path)\n    validate_labels = np.load(config.validate_labels_path)\n    \n    return {""train"":{""features"":train_features, ""labels"":train_labels},\n            ""validate"":{""features"":validate_features, ""labels"":validate_labels}}\n\nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser(description=\'The predictive model runner\')\n    parser.add_argument(\'predictor_name\', \n                        help=\'the name of predictor\')\n    parser.add_argument(\'--test_data\', default=config.test_features_path, \n                        help=\'the path to the test data file to make predictions for\')\n    parser.add_argument(\'--save_model\', action=\'store_true\', \n                        help=\'if set then trained model will be saved\')\n    parser.add_argument(\'--validate_model\', action=\'store_true\', \n                        help=\'if set then trained model will be validated against validate data\')\n    parser.add_argument(\'--save_labels\', action=\'store_true\', \n                        help=\'if set then predicted labels will be saved\')\n    args = parser.parse_args()\n    \n    # Do prediction\n    #\n    print(""Start \'%s\' predictor for [%s] data set"" % (args.predictor_name, args.test_data))\n    test_features = np.load(args.test_data)\n    labels, _ = predict(args.predictor_name, test_features, \n                        save_model = args.save_model, \n                        validate_model = args.validate_model, \n                        save_labels = args.save_labels)\n    \n    \n    \n    \n    '"
src/random_forest_model.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nThe predictor model based on RandomForestClassifier\n\n@author: yaric\n""""""\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport config\n\nRANDOM_STATE = 123\n\nclass RandomForest(object):\n    \n    def __init__(self, n_estimators = 300, model_path = config.models_dir +  ""/random_forest/model.pkl"", \n                 scaler_path = config.models_dir + ""/random_forest/scaler.pkl""):\n        self.model_path = model_path\n        self.scaler_path = scaler_path\n        self.n_estimators = n_estimators\n    \n\n    def train(self, X_train, labels):\n        """"""\n        Train model with given data corpus\n        Arguments:\n            X_train: the train data [n_samples, n_features]\n            labels: the GT labels [n_samples, n_classes]\n        Return:\n            return trained model\n        """"""\n        # train estimator\n        clf = RandomForestClassifier(n_estimators = self.n_estimators, random_state = RANDOM_STATE, n_jobs = -1)\n        self.model = clf.fit(X_train, labels)\n        \n        train_score = self.model.score(X_train, labels)\n        print(""RandomForest:\\ntrain score = %.3f, n_estimators = %d"" % (train_score, self.n_estimators))\n        \n        return self.model\n\n\n\n    \n\n'"
src/results_generator.py,3,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nThe utility to convert prediction results from Numpy array of class labels with confidence\nto list of lists where each child list corresponds to the text corpora sentence\nwith None at ordinary positions and list with class/confindece at predicted correction\nof determinant (English article) wrong usage\n\n@author: yaric\n""""""\nimport json\nimport argparse\n\nimport numpy as np\n\nimport tree_dict as td\nimport data_set as ds\nimport config\nimport utils\n\nconfidence_threshold = 0.\n\ndef saveSubmissionResults(f_type,\n                          out_file = config.test_reults_path, \n                          labels_file = config.test_labels_prob_path, \n                          test_senetnces_file = config.sentence_test_path, \n                          test_parse_tree_file = config.parse_test_path,):\n    """"""\n    Saves submission results from provided prediction labels\n    Argument:\n        f_type: the type of features used for training\n        out_file: the file to store predictions results in custom format\n        labels_file: the path to the file with predcitions label Numpy array\n        test_senetnces_file: the text\'s corpora file for test data\n        test_parse_tree_file: the parse tree file for test data\n    """"""\n    labels = np.load(labels_file)\n    text_data = utils.read_json(test_senetnces_file)\n    parse_trees_list = utils.read_json(test_parse_tree_file)\n    \n    # generate predictions result\n    if f_type == \'tree\':\n        predictions = predictionsFromLabels(labels, text_data, parse_trees_list)\n    elif f_type == \'tags\':\n        predictions = predictionsFromTagLabels(text_data = text_data, labels = labels)\n    else:\n        raise Exception(\'Unknown features type: \' + f_type)\n    print(""Generated %d prediction sentences"" % len(predictions))\n    \n    # sanity checks\n    if len(predictions) != len(text_data):\n        raise Exception(""Number of sentences in predictions not equal to in test corpora: %d != %d"" \n                        % (len(predictions), len(text_data)))\n        \n    for pr_s, text_s in zip(predictions, text_data):\n        if len(pr_s) != len(text_s):\n            raise Exception(""Predictions sentence length not equal to text sentence length: %d != %d\\nsentence: %s""\n                            % (len(pr_s), len(text_s), text_s))\n    \n    # save to the file\n    savePredictions(predictions, out_file)\n\ndef predictionsForSentence(sentence, labels, dpa_nodes):\n    """"""\n    Creates predictions list for specific sentence\n    Arguments:\n        sentence: the list of units in sentence\n        labels: the labels list for given sentence\n        dpa_nodes: the DT with English article nodes for given sentence\n    Return:\n        list of results in form [None, None, [class, confidence], None, ...]\n    """"""\n    # sanity check\n    if len(labels) != len(dpa_nodes):\n        raise Exception(""labels lenght not equal to dpa nodes"")\n        \n    dt_indices = list()\n    for node in dpa_nodes:\n        for dt_n in node.leavesWithPOS(\'DT\'):\n            dt_indices.append(dt_n.s_index)\n            \n    l_index = 0  \n    res = list()      \n    for i in range(len(sentence)):\n        s_w = sentence[i]\n        if i in dt_indices:\n            # DP found\n            if utils.dtIsArticle(s_w) == False:\n                raise Exception(""The determiner [%s] index at wrong position: [%d] for sentence: %s"" % (s_w, i, sentence)) # sanity check\n            max_lab_ind = np.argmax(labels[l_index]) # the most confident prediction\n            res.append([max_lab_ind, labels[l_index, max_lab_ind]]) # [class, probability]\n            l_index += 1 # move to the next prediction label\n        else:\n            # ordinary word\n            res.append(None)\n            \n    # sanity check\n    if l_index != len(labels):\n        raise Exception(""Not all labels was processed"")\n    \n    return res        \n\ndef predictionsFromLabels(labels, text_data, parse_trees_list):\n    """"""\n    Create predictions results to be accepted by savePredictions based on features set\n    generated with constituency parse trees\n    Arguments:\n        labels: the predicted labels from predictive model\n        text_data: the text corpora as loaded from JSON\n        parse_trees_list: the parse tree list as loaded from JSON for given text corpora\n    Returns:\n        list of predictions per sentence to be accepted by savePredictions\n    """"""\n    res_list = list()\n    l_index = 0    \n    for i in range(len(parse_trees_list)):\n        sentence = text_data[i]\n        node, _ = td.treeFromDict(parse_trees_list[i]) # the parse tree for sentence\n        dpa_nodes = node.dpaSubtrees()\n        s_list = predictionsForSentence(sentence, labels[l_index : l_index + len(dpa_nodes),], dpa_nodes)\n        res_list.append(s_list)\n        # move to the next sentence labels\n        l_index += len(dpa_nodes)\n        \n    return res_list \n\ndef predictionsFromTagLabels(text_data, labels):\n    """"""\n    Create predictions results to be accepted by savePredictions based on features set\n    generated with pos tags\n    Arguments:\n        labels: the predicted labels from predictive model\n        text_data: the text corpora as loaded from JSON\n    Returns:\n        list of predictions per sentence to be accepted by savePredictions\n    """"""\n    res_list = list()\n    l_index = 0\n    for s in text_data:\n        row_data = list()\n        for w in s:\n            if w.lower() in [\'a\', \'an\', \'the\']:\n                max_lab_ind = np.argmax(labels[l_index]) # the most confident prediction\n                if labels[l_index, max_lab_ind] > confidence_threshold:\n                    row_data.append([max_lab_ind, labels[l_index, max_lab_ind]]) # [class, probability]\n                else:\n                    row_data.append([0, 0]) # too low confidence\n                l_index += 1 # move to the next prediction label\n            else:\n                # ordinary word\n                row_data.append(None)\n        res_list.append(row_data)\n        \n    # sanity check\n    if l_index != len(labels):\n        raise Exception(""Not all labels was processed"")\n        \n    return res_list \n    \ndef savePredictions(predictions, file):\n    """"""\n    Method to save predictions results.\n    Arguments:\n        predictions: the list of lists with predictions per sentences for each unit\n        [\n             [None,[class, confidece],None],\n             [None,None,[class, confidece]],\n             ...\n             [[class, confidece],None,None]\n        ]\n        file: the file path to save\n    """"""\n    out = list()\n    for s in predictions:\n        out_s = list()\n        for w in s:\n            if w == None:\n                # None -> None\n                out_s.append(None)\n            elif w[0] < 0 or w[0] > ds.DT.THE.value:\n                # sanity check\n                raise Exception(""Wrong class found: "" + w[0])\n            elif w[0] == 0:\n                # no corrections was suggested\n                out_s.append(None)\n            else:\n                # class -> article\n                class_label = ds.DT.nameByValue(w[0]).lower()\n                out_s.append([class_label, float(w[1])])\n        # Append sentence\n        out.append(out_s)\n        \n    # save result to JSON\n    with open(file, mode = \'w\') as f:\n        json.dump(out, f)\n    \n    print(""Prediction results saved to: "" + file)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'The results generator transforming predicted labes array into submission compatible format\')\n    parser.add_argument(\'--out_file\', default=config.test_reults_path, \n                        help=\'the file to store predictions results in custom format\')\n    parser.add_argument(\'--labels_file\', default=config.test_labels_prob_path, \n                        help=\'the path to the file with predictions label Numpy array\')\n    parser.add_argument(\'--test_sentences_file\', default=config.sentence_test_path, \n                        help=""the text\'s corpora file for test data"")\n    parser.add_argument(\'--test_parse_tree_file\', default=config.parse_test_path, \n                        help=\'the parse tree file for test data\')\n    parser.add_argument(\'--f_type\', default=\'tree\',\n                        help=\'the type of features used for training [tree, tags]\')\n    args = parser.parse_args()\n    \n    print(""Generating test results for features with type: %s"" % args.f_type)\n    \n    saveSubmissionResults(f_type = args.f_type,\n                          out_file = args.out_file, \n                          labels_file = args.labels_file, \n                          test_senetnces_file = args.test_sentences_file, \n                          test_parse_tree_file = args.test_parse_tree_file)\n'"
src/results_generator_test.py,2,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Mar 13 17:33:45 2017\n\n@author: yaric\n""""""\nimport os\nimport unittest\nimport itertools\nimport json\n\nimport numpy as np\n\nimport tree_dict as td\nimport data_set as ds\nimport results_generator as rg\nimport config\nimport utils\n\nclass TestResultsGeneratorMethods(unittest.TestCase):\n    \n    @classmethod\n    def setUpClass(cls):\n        cls.labels = np.array([[ 0.9       ,  0.1       ,  0.        ,  0.        ],\n                               [ 0.55      ,  0.45      ,  0.        ,  0.        ],\n                               [ 0.        ,  0.        ,  0.        ,  1.        ]], dtype = \'f\')\n        cls.text_data = json.loads(\'[[""Silly"", ""libs"", ""--"", ""the"", ""government"", ""owns"", ""the"", ""uterus"", ""of"", ""all"", ""women"", "".""], \\\n                                [""It"", ""is"", ""in"", ""a"", ""Constitution"", ""!""]]\')\n        cls.parse_trees_list = json.loads(\'[{""name"": ""TOP"", ""children"": [{""name"": ""S"", ""children"": [{""name"": ""NP"", ""children"": [{""name"": ""JJ"", ""children"": [{""name"": ""Silly"", ""children"": []}]}, {""name"": ""NNS"", ""children"": [{""name"": ""libs"", ""children"": []}]}]}, {""name"": "":"", ""children"": [{""name"": ""--"", ""children"": []}]}, {""name"": ""NP"", ""children"": [{""name"": ""DT"", ""children"": [{""name"": ""the"", ""children"": []}]}, {""name"": ""NN"", ""children"": [{""name"": ""government"", ""children"": []}]}]}, {""name"": ""VP"", ""children"": [{""name"": ""VBZ"", ""children"": [{""name"": ""owns"", ""children"": []}]}, {""name"": ""NP"", ""children"": [{""name"": ""NP"", ""children"": [{""name"": ""DT"", ""children"": [{""name"": ""the"", ""children"": []}]}, {""name"": ""NN"", ""children"": [{""name"": ""uterus"", ""children"": []}]}]}, {""name"": ""PP"", ""children"": [{""name"": ""IN"", ""children"": [{""name"": ""of"", ""children"": []}]}, {""name"": ""NP"", ""children"": [{""name"": ""DT"", ""children"": [{""name"": ""all"", ""children"": []}]}, {""name"": ""NNS"", ""children"": [{""name"": ""women"", ""children"": []}]}]}]}]}]}, {""name"": ""."", ""children"": [{""name"": ""."", ""children"": []}]}]}]}, \\\n                                        {""name"": ""TOP"", ""children"": [{""name"": ""S"", ""children"": [{""name"": ""NP"", ""children"": [{""name"": ""PRP"", ""children"": [{""name"": ""It"", ""children"": []}]}]}, {""name"": ""VP"", ""children"": [{""name"": ""VBZ"", ""children"": [{""name"": ""is"", ""children"": []}]}, {""name"": ""PP"", ""children"": [{""name"": ""IN"", ""children"": [{""name"": ""in"", ""children"": []}]}, {""name"": ""NP"", ""children"": [{""name"": ""DT"", ""children"": [{""name"": ""the"", ""children"": []}]}, {""name"": ""NNP"", ""children"": [{""name"": ""Constitution"", ""children"": []}]}]}]}]}, {""name"": ""."", ""children"": [{""name"": ""!"", ""children"": []}]}]}]}]\')\n        \n        cls.test_list = [[None, None, None, [0, 0.9], None, None, [0, 0.55], None, None, None, None, None], \n                     [None, None, None, [3, 1.0], None, None]]\n        \n    def test_predictions_from_labels(self):\n        test_list = itertools.chain(*self.test_list)\n        out_list = rg.predictionsFromLabels(self.labels, self.text_data, self.parse_trees_list)\n        out_list = itertools.chain(*out_list)\n        \n        # compare results with test\n        for test, res in zip(test_list, out_list):\n            if test != None:\n                self.assertEqual(test[0], res[0], ""Not equal class labels: %s != %s"" % (test[0], res[0]))\n                self.assertAlmostEqual(test[1], res[1], 5, ""Not equal confidence values: %s != %s"" % (test[1], res[1]))\n            else:\n                self.assertEqual(test, res, ""Not equal items: %s != %s"" % (test, res))\n        \n    \n    def test_predictions_for_sentence(self):    \n        test_list = itertools.chain(*self.test_list)\n\n        out_list = list()\n        l_index = 0    \n        for i in range(len(self.parse_trees_list)):\n            sentence = self.text_data[i]\n            node, _ = td.treeFromDict(self.parse_trees_list[i]) # the parse tree for sentence\n            dpa_nodes = node.dpaSubtrees()\n            s_list = rg.predictionsForSentence(sentence, self.labels[l_index : l_index + len(dpa_nodes),], dpa_nodes)\n            out_list.append(s_list)\n            # move to the next sentence labels\n            l_index += len(dpa_nodes)\n    \n        out_list = itertools.chain(*out_list)\n\n        # compare results with test\n        for test, res in zip(test_list, out_list):\n            if test != None:\n                self.assertEqual(test[0], res[0], ""Not equal class labels: %s != %s"" % (test[0], res[0]))\n                self.assertAlmostEqual(test[1], res[1], 5, ""Not equal confidence values: %s != %s"" % (test[1], res[1]))\n            else:\n                self.assertEqual(test, res, ""Not equal items: %s != %s"" % (test, res))\n    \n    def test_predictionsFromTagLabels(self):\n        test_list = itertools.chain(*self.test_list)\n        out_list = rg.predictionsFromTagLabels(self.text_data, self.labels)\n        out_list = itertools.chain(*out_list)\n        \n        # compare results with test\n        for test, res in zip(test_list, out_list):\n            if test != None:\n                self.assertEqual(test[0], res[0], ""Not equal class labels: %s != %s"" % (test[0], res[0]))\n                self.assertAlmostEqual(test[1], res[1], 5, ""Not equal confidence values: %s != %s"" % (test[1], res[1]))\n            else:\n                self.assertEqual(test, res, ""Not equal items: %s != %s"" % (test, res))\n        \n    \n    def test_save_predictions(self):\n        predictions = np.array(\n                [\n                        [[0,1], [0,1], [ds.DT.A, 0.6],[0,1], [0,1],[0,1], [0,1], [ds.DT.THE, 0.8]],\n                        [[0,1], [ds.DT.AN, 0.6],[0,1], [0,1], [0,1], [ds.DT.A, 0.8],[0,1], [0,1]],\n                        [[0,1], [0,1],[0,1], [0,1], [0,1], [ds.DT.A, 0.8],[0,1], [0,1]]\n                ], dtype = ""f"")\n        file = config.unit_tests_dir + ""/save_predictions_test.txt""\n        if os.path.exists(config.unit_tests_dir) == False:\n            os.makedirs(config.unit_tests_dir)\n        \n        rg.savePredictions(predictions, file)\n        \n        # test saved predictions\n        saved = utils.read_json(file)\n        self.assertIsNotNone(saved, ""Failed to save predictions"")\n        self.assertEqual(len(predictions), len(saved), ""Wrong size of saved sentences"")\n        \n        for i in range(len(predictions)):\n            pr = predictions[i]\n            s_pr = saved[i]\n            self.assertEqual(len(pr), len(s_pr), \n                             ""Wrong senetence length [%d] in saved predictions at index: %d"" \n                             % (len(s_pr), i))\n            for j in range(len(pr)):\n                if pr[j][0] == 0:\n                    self.assertIsNone(s_pr[j], \n                                      ""Wrong saved None prediction class at: %d, %d"" % (i, j))\n                else:\n                    art = ds.DT.nameByValue(pr[j][0]).lower()\n                    self.assertEqual(art, s_pr[j][0],\n                                     ""Wrong article prediction class at: %d, %d"" % (i, j))\n                    self.assertAlmostEqual(pr[j][1], s_pr[j][1], places = 2,\n                                     msg = ""Wrong article confidence value at: %d, %d"" % (i, j))\n                    \n\nif __name__ == \'__main__\':\n    unittest.main()'"
src/tree_dict.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nThe parse tree dictionary parser\n\n@author: yaric\n""""""\nimport json\nimport utils\n\nclass SNode(object):\n    """"""\n    Represents specific node of parse tree \n    """"""\n    \n    def __init__(self, name, s_index = -1, pos = None):\n        """"""\n        Creates new node\n        Arguments:\n            name: the node name\n            s_index: the index of unit in sentence for leaf nodes [optional]\n            pos: the part of speech for leaf nodes [optional]\n        """"""\n        self.name = name\n        self.s_index = s_index\n        self.pos = pos\n        self.children = list()\n        \n    def __str__(self):\n        """"""\n        Returns string representation of this node.\n        """"""\n        return self.name + "" | "" + str(self.s_index) + "" | "" + str(self.pos)\n        \n    def leaves(self):\n        """"""\n        Returns list of all leaves in this node\n        """"""\n        return [n for n in walk(self) if n.isLeaf()]\n    \n    def leavesWithPOS(self, pos):\n        """"""\n        Returns list of all tree leaves with specified POS\n        """"""\n        return [n for n in walk(self) if n.isLeaf() and n.pos == pos]\n    \n    def subtrees(self, min_childs = 1):\n        """"""\n        Returns all subtrees in this node\n        Arguments:\n            min_childs: the minimal number of childs per tree\n        """"""\n        return [n for n in walk(self) if n.isLeaf() == False and len(n.children) >= min_childs]\n    \n    def dpaSubtrees(self):\n        """"""\n        Returns all leaves containing exactly one determiner (DT) in form of article [a, an, the]\n        """"""\n        known_indices = list() # holds already added DT to avoid double adding shorted DP forms included into bigger\n        dpa_trees = list()\n        subtrees = self.subtrees()\n        for st in subtrees:\n            if st.name == \'NP\':\n                dt_leaves = st.leavesWithPOS(\'DT\')\n                if len(dt_leaves) == 1 and all(dt_leaves[0].s_index != index for index in known_indices) \\\n                    and utils.dtIsArticle(dt_leaves[0].name):\n                    dpa_trees.append(st)\n                    known_indices.append(dt_leaves[0].s_index)\n                \n        return dpa_trees\n                \n    def deepNPSubtrees(self):\n        """"""\n        Returns list of deep NP subtrees which is the shortest ones inside complex NP\n        """"""\n        np_subtrees = list()\n        subtrees = self.subtrees()\n        for st in subtrees:\n            if st.name == \'NP\':\n                if any(child.name == \'NP\' for child in st.children):\n                    # Intermediate NP\n                    dpa_subtrees = st.dpaSubtrees() # N.B. can be optimized by direct check\n                    if len(dpa_subtrees) == 0:\n                        # Add NP as whole\n                        np_subtrees.append(st)\n                else:\n                    # Deepest NP\n                    np_subtrees.append(st)\n                \n        return np_subtrees\n        \n        \n    def isLeaf(self):\n        """"""\n        Returns True if this node is leaf\n        """"""\n        return len(self.children) == 0\n        \n\ndef walk(node):\n    """""" \n    Iterates tree node in pre-order depth-first search order \n    Argument:\n        node: the tree node\n    Return:\n        the generator to iterate over tree node in pre-order depth-first search order \n    """"""\n    yield node\n    for child in node.children:\n        for n in walk(child):\n            yield n\n            \ndef printNode(node):\n    """"""\n    Print treen node\n    Arguments:\n        node: the tree node\n    """"""\n    print(node)\n            \ndef treeFromDict(d, s_index = 0, root = None):\n    """"""\n    Builds tree of SNode from provided dictionary\n    Arguments:\n        d: the dictionary with tree representation\n        s_index: the sentence index of last leaf node\n        root: the root node\n    Return:\n        the tuple with root node of the tree and the sentence index of last leaf node\n    """"""\n    if root == None:\n        root = SNode(d[""name""]) \n      \n    children = d[""children""]    \n    for child in children:\n        if len(child[""children""]) == 0:\n            # the leaf node found\n            node = SNode(child[""name""], s_index, root.name)\n            root.children.append(node)\n            s_index += 1\n        else:\n            # the interior node\n            node = SNode(child[""name""])\n            _, s_index = treeFromDict(child, s_index, node)            \n            root.children.append(node)\n                \n    return (root, s_index)\n\ndef treeFromList(l):\n    """"""\n    Builds tree of SNode from provided list\n    Arguments:\n        l: the list with tree representation\n    Return:\n        the tuple with root node of the tree and the sentence index of last leaf node\n    """"""\n    root = SNode(""S"")\n    s_index = 0\n    for child in l:\n        node = SNode(child[""name""])\n        _, s_index = treeFromDict(child, s_index, node)\n        root.children.append(node)\n        \n    return (root, s_index)\n        \n\ndef treeFromJSON(json_str):\n    """"""\n    Builds tree from JSON string\n    Arguments:\n        json_str: the JSON string with tree data\n    Return:\n        the tree build from provided JSON structure\n    """"""\n    if json_str[""name""] == \'TOP\':\n        return treeFromDict(json_str[\'children\'][0])\n    elif json_str[""name""] == \'INC\':\n        return treeFromList(json_str[\'children\'])\n    else:\n        raise(""Unknown tree format found: "" + json_str[""name""])\n    \n    \n\ndef treeStringFromDict(d):\n    """"""\n    Builds tree definition string from dictionary\n    Arguments:\n        d: the dictionary\n    Return: the string representation of tree\n    """"""\n    children = d[""children""]\n    acc = "" ""\n    for child in children:\n        if len(child[""children""]) == 0:\n            acc = acc + child[""name""]\n        else:\n            acc_c = treeStringFromDict(child)\n            acc = acc + ""("" + child[""name""] + acc_c + "")"" \n\n    return acc\n       \n    \n    \nif __name__ == ""__main__"":\n    with open(""../data/parse_train.txt"") as f:\n        data = json.load(f)\n    tree_str = json.dumps(data[1]) # 723\n    print(tree_str)\n    tree =  json.loads(tree_str)\n    acc = treeStringFromDict(tree[\'children\'][0])\n    \n    print(""+++++++++++++++++++++ Tree string"")\n    print(acc)\n    \n    root, index = treeFromJSON(tree)\n    tree_nodes = walk(root)\n    print(""+++++++++++++++++++++ Nodes"")\n    print(""Last index: "" + str(index))\n    for n in tree_nodes:\n        printNode(n)\n\n    print(""+++++++++++++++++++++ Leaves"")\n    leaves = root.leaves()\n    for n in leaves:\n        printNode(n)\n        \n    print(""+++++++++++++++++++++ Subtrees"")\n    subtrees = root.subtrees()\n    for st in subtrees:\n        printNode(st)\n        \n    print(""+++++++++++++++++++++ NP Subtrees"")\n    subtrees = root.subtrees()\n    for st in subtrees:\n        if st.name == \'NP\':\n            print(""---"")\n            printNode(st)\n            for l in st.leaves():\n                printNode(l)\n    \n    print(""+++++++++++++++++++++ Deep NP Subtrees"")\n    subtrees = root.deepNPSubtrees()\n    for st in subtrees:\n        print(""---"")\n        for l in st.leaves():\n            printNode(l)\n\n    print(""+++++++++++++++++++++ Leaves with POS"")\n    leaves = root.leavesWithPOS(\'DT\')\n    for n in leaves:\n        printNode(n)\n            \n    print(""+++++++++++++++++++++ DPA Subtrees"")\n    subtrees = root.dpaSubtrees()\n    for st in subtrees:\n        print(""---"")\n        for l in st.leaves():\n            printNode(l)'"
src/tree_dict_test.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nThe tests for tree implementation\n\n@author: yaric\n""""""\n\nimport unittest\n\nimport tree_dict as td\nimport config\nimport utils\n\nclass TestDeepTreeMethods(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        data = utils.read_json(config.parse_train_path)\n        tree_dict = data[1]\n        root, index = td.treeFromJSON(tree_dict)\n        cls.root = root\n\n    def test_walk(self):\n        nodes = [n for n in td.walk(self.root)]\n        self.assertEqual(len(nodes), 125, ""Nodes in the ROOT"")\n        \n    def test_leaves(self):\n        leaves = self.root.leaves()\n        self.assertEqual(len(leaves), 43, ""Leaves in the ROOT"")\n        \n    def test_leaves_s_indexes(self):\n        leaves = self.root.leaves()\n        self.assertEqual(len(leaves), 43, ""Leaves in the ROOT"")\n        \n        index = 0\n        for l in leaves:\n            self.assertEqual(l.s_index, index, ""Index of leaf"")\n            index += 1\n        \n    def test_subtrees(self):\n        subtrees = self.root.subtrees()\n        self.assertEqual(len(subtrees), 82, ""Subtrees in the ROOT [min_childs = 1]"")\n        \n        subtrees = self.root.subtrees(min_childs = 2)\n        self.assertEqual(len(subtrees), 28, ""Subtrees in the ROOT [min_childs = 2]"")\n    \n    def test_np_subtrees(self):\n        subtrees = self.root.subtrees()\n        np_subtrees = 0\n        for st in subtrees:\n            if st.name == \'NP\':\n                np_subtrees += 1\n                \n        self.assertEqual(np_subtrees, 13, ""NP Subtrees in the ROOT"")\n        \n    def test_deepNPSubtrees(self):\n        subtrees = self.root.deepNPSubtrees()\n        self.assertEqual(len(subtrees), 11, ""Deep NP Subtrees in the ROOT"")\n        \n    def test_leaves_with_pos(self):\n        leaves = self.root.leavesWithPOS(\'DT\')\n        self.assertEqual(len(leaves), 3, ""Leaves with POS \'DT\' in the ROOT"")\n        \n    def test_dpaSubtrees(self):\n        subtrees = self.root.dpaSubtrees()\n        self.assertEqual(len(subtrees), 3, ""DPA Subtrees in the ROOT"")\n \nclass TestShallowTreeMethods(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        data = utils.read_json(config.parse_train_path)\n        tree_dict = data[723]\n        root, index = td.treeFromJSON(tree_dict)\n        cls.root = root\n        \n    def test_walk(self):\n        nodes = [n for n in td.walk(self.root)]\n        self.assertEqual(len(nodes), 33, ""Nodes in the ROOT"")\n        \n    def test_leaves(self):\n        leaves = self.root.leaves()\n        self.assertEqual(len(leaves), 14, ""Leaves in the ROOT"")\n        \n    def test_leaves_s_indexes(self):\n        leaves = self.root.leaves()\n        self.assertEqual(len(leaves), 14, ""Leaves in the ROOT"")\n        \n        index = 0\n        for l in leaves:\n            self.assertEqual(l.s_index, index, ""Index of leaf"")\n            index += 1\n        \n    def test_subtrees(self):\n        subtrees = self.root.subtrees()\n        self.assertEqual(len(subtrees), 19, ""Subtrees in the ROOT [min_childs = 1]"")\n        subtrees = self.root.subtrees(min_childs = 2)\n        self.assertEqual(len(subtrees), 2, ""Subtrees in the ROOT [min_childs = 2]"")\n    \n    def test_np_subtrees(self):\n        subtrees = self.root.subtrees()\n        np_subtrees = 0\n        for st in subtrees:\n            if st.name == \'NP\':\n                np_subtrees += 1\n                \n        self.assertEqual(np_subtrees, 4, ""NP Subtrees in the ROOT"")\n        \n    def test_deepNPSubtrees(self):\n        subtrees = self.root.deepNPSubtrees()\n        self.assertEqual(len(subtrees), 4, ""Deep NP Subtrees in the ROOT"")\n        \n    def test_leaves_with_pos(self):\n        leaves = self.root.leavesWithPOS(\'DT\')\n        self.assertEqual(len(leaves), 1, ""Leaves with POS \'DT\' in the ROOT"")\n        \n    def test_dpaSubtrees(self):\n        subtrees = self.root.dpaSubtrees()\n        self.assertEqual(len(subtrees), 0, ""DPA Subtrees in the ROOT"")\n    \nif __name__ == \'__main__\':\n    unittest.main()\n    \n\n    \n        '"
src/utils.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Fri Mar 10 21:40:08 2017\n\n@author: yaric\n""""""\nimport json\nimport pandas as pd\n    \nimport config\n\ndef read_json(file):\n    """"""\n    Loads JSON data from file\n    Arguments:\n        file: the path to the JSON file\n    Return:\n        the dictionary with parsed JSON\n    """"""\n    with open(file) as f:\n        data = json.load(f)\n    return data\n\ndef dtIsArticle(dt):\n    """"""\n    Checks if determiner is indefinite or definite English article\n    Argument:\n        dt: the determiner name\n    Return:\n        True if determiner is indefinite or definite English article\n    """"""\n    return dt.lower() in [\'a\', \'an\', \'the\']\n\ndef checkDataCorporaSanity(data_dir, corpora_name):\n    """"""\n    Method to  quick data corpus sanity check. It checks if there is no intersections\n    between corrected acrticles and text\'s articles, i.e. test if we really have  \n    corrected articles in text corpora present for training.\n    Arguments:\n        data_dir: the directory to look for corpora data\n        corpora_name: the name of corpora to test\n    """"""\n    corrections_file = ""%s/corrections_%s.txt"" % (data_dir, corpora_name)\n    corpus_file = ""%s/sentence_%s.txt"" % (data_dir, corpora_name)\n    cor_df = pd.read_json(corrections_file, dtype=""string"")\n    text_df = pd.read_json(corpus_file, dtype=""string"")\n    \n    # select all corrected articles\n    cor_sel = cor_df.isnull() == False\n    \n    text_art = text_df[cor_sel]\n    cor_art = cor_df[cor_sel]\n    \n    # find intersection between two\n    intersection_df = text_art == cor_art\n    \n    intersection = intersection_df[intersection_df == True].sum().sum()\n    \n    print(""The number of intersections: %d in corpora: %s"" \n          % (intersection, corpora_name))\n    if intersection > 0:\n        raise Exception(""The text and corrections has the same Articles at the same position"")\n    \n        \nif __name__ == \'__main__\':\n    checkDataCorporaSanity(config.data_dir, ""train"")\n    checkDataCorporaSanity(config.data_dir, ""test"")'"
