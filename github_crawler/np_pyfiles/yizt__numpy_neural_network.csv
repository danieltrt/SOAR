file_path,api_count,code
setup.py,2,"b'import numpy as np\nimport setuptools\nfrom setuptools.extension import Extension\n\nextensions = [\n    Extension(\n        \'nn.clayers\',\n        [\'nn/clayers.pyx\'],\n        include_dirs=[np.get_include()]\n    ),\n    Extension(\n        \'nn.clayers_v2\',\n        [\'nn/clayers_v2.pyx\'],\n        include_dirs=[np.get_include()]\n    )\n]\n\nsetuptools.setup(\n    name=\'numpy_neuron_network\',\n    version=\'0.1\',\n    description=\'numpy \xe6\x9e\x84\xe5\xbb\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\',\n    url=\'https://github.com/yizt/numpy_neuron_network\',\n    author=\'yizt\',\n    author_email=\'csuyzt@163.com\',\n    packages=setuptools.find_packages(),\n    install_requires=[\'numpy\', \'cython\'],\n    ext_modules=extensions,\n    setup_requires=[""cython>=0.28""]\n)\n'"
nn/__init__.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on 2018/8/19 15:03\r\n\r\n@author: mick.yi\r\n\r\n""""""\r\nimport os\r\nimport sys\r\n\r\nsys.path.append(os.path.dirname(__file__))\r\n'"
nn/activations.py,12,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on 2018/8/31 20:33\r\n\r\n@author: mick.yi\r\n\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\r\n\xe5\x8f\x82\xe8\x80\x83\xef\xbc\x9ahttps://blog.csdn.net/csuyzt/article/details/82320589 \xe4\xb8\xad\xe7\x9a\x84\xe6\x8e\xa8\xe5\xaf\xbc\xe5\x85\xac\xe5\xbc\x8f\r\n""""""\r\nimport numpy as np\r\n\r\n\r\ndef sigmoid_forward(z):\r\n    """"""\r\n    sigmoid\xe6\xbf\x80\xe6\xb4\xbb\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\r\n    :param z:\r\n    :return:\r\n    """"""\r\n    return 1 / (1 + np.exp(-z))\r\n\r\n\r\ndef sigmoid_backward(next_dz, z):\r\n    """"""\r\n    sigmoid\xe6\xbf\x80\xe6\xb4\xbb\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\r\n    :param next_dz:\r\n    :param z:\r\n    :return:\r\n    """"""\r\n    return sigmoid_forward(z) * (1 - sigmoid_forward(z)) * next_dz\r\n\r\n\r\ndef tanh_forward(z):\r\n    """"""\r\n    tanh\xe6\xbf\x80\xe6\xb4\xbb\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\r\n    :param z:\r\n    :return:\r\n    """"""\r\n    return np.tanh(z)\r\n\r\n\r\ndef tanh_backward(next_dz):\r\n    """"""\r\n    tanh\xe6\xbf\x80\xe6\xb4\xbb\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\r\n    :param next_dz:\r\n    :return:\r\n    """"""\r\n    return 1 - np.square(np.tanh(next_dz))\r\n\r\n\r\ndef relu_forward(z):\r\n    """"""\r\n    relu\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    :param z: \xe5\xbe\x85\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\r\n    :return: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\r\n    """"""\r\n    return np.maximum(0, z)\r\n\r\n\r\ndef relu_backward(next_dz, z):\r\n    """"""\r\n    relu\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    :param next_dz: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x90\x8e\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\r\n    :param z: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x89\x8d\xe7\x9a\x84\xe5\x80\xbc\r\n    :return:\r\n    """"""\r\n    dz = np.where(np.greater(z, 0), next_dz, 0)\r\n    return dz\r\n\r\n\r\ndef lrelu_forward(z, alpha=0.1):\r\n    """"""\r\n    leaky relu\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    :param z: \xe5\xbe\x85\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\r\n    :param alpha: \xe5\xb8\xb8\xe9\x87\x8f\xe5\x9b\xa0\xe5\xad\x90\r\n    :return: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\r\n    """"""\r\n    return np.where(np.greater(z, 0), z, alpha * z)\r\n\r\n\r\ndef lrelu_backward(next_dz, z, alpha=0.1):\r\n    """"""\r\n    leaky relu\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    :param next_dz: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x90\x8e\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\r\n    :param z: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x89\x8d\xe7\x9a\x84\xe5\x80\xbc\r\n    :param alpha: \xe5\xb8\xb8\xe9\x87\x8f\xe5\x9b\xa0\xe5\xad\x90\r\n    :return:\r\n    """"""\r\n    dz = np.where(np.greater(z, 0), next_dz, alpha * next_dz)\r\n    return dz\r\n\r\n\r\ndef prelu_forward(z, alpha):\r\n    """"""\r\n    PReLu \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    :param z: \xe8\xbe\x93\xe5\x85\xa5\r\n    :param alpha:\xe9\x9c\x80\xe8\xa6\x81\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\r\n    :return:\r\n    """"""\r\n    return np.where(np.greater(z, 0), z, alpha * z)\r\n\r\n\r\ndef prelu_backwark(next_dz, z, alpha):\r\n    """"""\r\n    PReLu \xe5\x90\x8e\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    :param next_dz: \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\r\n    :param z: \xe8\xbe\x93\xe5\x85\xa5\r\n    :param alpha:\xe9\x9c\x80\xe8\xa6\x81\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\r\n    :return:\r\n    """"""\r\n    dz = np.where(np.greater(z, 0), next_dz, alpha * next_dz)\r\n    dalpha = np.where(np.greater(z, 0), 0, z * next_dz)\r\n    return dalpha, dz\r\n\r\n\r\ndef elu_forward(z, alpha=0.1):\r\n    """"""\r\n    elu\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    :param z: \xe8\xbe\x93\xe5\x85\xa5\r\n    :param alpha: \xe5\xb8\xb8\xe9\x87\x8f\xe5\x9b\xa0\xe5\xad\x90\r\n    :return:\r\n    """"""\r\n    return np.where(np.greater(z, 0), z, alpha * (np.exp(z) - 1))\r\n\r\n\r\ndef elu_backward(next_dz, z, alpha=0.1):\r\n    """"""\r\n    elu\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n    :param next_dz: \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6\r\n    :param z: \xe8\xbe\x93\xe5\x85\xa5\r\n    :param alpha: \xe5\xb8\xb8\xe9\x87\x8f\xe5\x9b\xa0\xe5\xad\x90\r\n    :return:\r\n    """"""\r\n    return np.where(np.greater(z, 0), next_dz, alpha * next_dz * np.exp(z))\r\n'"
nn/cnn.py,9,"b'# -*- coding: utf-8 -*-\n""""""\n @File    : cnn.py\n @Time    : 2020/4/18 \xe4\xb8\x8b\xe5\x8d\x885:54\n @Author  : yizuotian\n @Description    : \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\n""""""\nimport argparse\nimport os\nimport time\n\nfrom losses import cross_entropy_loss\nfrom optimizers import *\nfrom utils import load_cifar, save_weights, load_weights\nfrom vgg import VGG\n\n\ndef get_accuracy(net, xs, ys, batch_size=128):\n    """"""\n\n    :param net:\n    :param xs:\n    :param ys:\n    :param batch_size:\n    :return:\n    """"""\n    scores = np.zeros_like(ys, dtype=np.float)\n    num = xs.shape[0]\n    for i in range(num // batch_size):\n        s = i * batch_size\n        e = s + batch_size\n        scores[s:e] = net.forward(xs[s:e])\n    # \xe4\xbd\x99\xe6\x95\xb0\xe5\xa4\x84\xe7\x90\x86\n    remain = num % batch_size\n    if remain > 0:\n        scores[-remain:] = net.forward(xs[-remain:])\n    # \xe8\xae\xa1\xe7\xae\x97\xe7\xb2\xbe\xe5\xba\xa6\n    acc = np.mean(np.argmax(scores, axis=1) == np.argmax(ys, axis=1))\n    return acc\n\n\ndef main(args):\n    # \xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\n    (x_train, y_train), (x_test, y_test) = load_cifar(args.cifar_root)\n\n    # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x80\x89\xe6\x8b\xa9\xe8\xae\xad\xe7\xbb\x83\xe6\xa0\xb7\xe6\x9c\xac\n    train_num = x_train.shape[0]\n\n    def next_batch(batch_size):\n        idx = np.random.choice(train_num, batch_size)\n        return x_train[idx], y_train[idx]\n\n    # \xe7\xbd\x91\xe7\xbb\x9c\n    vgg = VGG(image_size=32, name=\'vgg11\')\n    # opt = RmsProp(vgg.weights, lr=args.lr, decay=1e-3)\n    opt = SGD(vgg.weights, lr=args.lr, decay=args.decay)\n    opt.iterations = args.init_step\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9d\x83\xe9\x87\x8d\n    if args.checkpoint:\n        weights = load_weights(args.checkpoint)\n        vgg.load_weights(weights)\n        print(""load weights done"")\n\n    # \xe8\xaf\x84\xe4\xbc\xb0\n    if args.eval_only:\n        indices = np.random.choice(len(x_test), args.eval_num, replace=False)\n        print(\'{} start evaluate\'.format(time.asctime(time.localtime(time.time()))))\n        acc = get_accuracy(vgg, x_test[indices], y_test[indices], args.batch_size)\n        print(\'{} acc on test dataset is :{:.3f}\'.format(time.asctime(time.localtime(time.time())),\n                                                         acc))\n        return\n\n    # \xe8\xae\xad\xe7\xbb\x83\n    num_steps = args.steps\n    for step in range(args.init_step, num_steps):\n        x, y_true = next_batch(args.batch_size)\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        y_predict = vgg.forward(x.astype(np.float))\n        # print(\'y_pred: min{},max{},mean:{}\'.format(np.min(y_predict, axis=-1),\n        #                                            np.max(y_predict, axis=-1),\n        #                                            np.mean(y_predict, axis=-1)))\n        # print(\'y_pred: {}\'.format(y_predict))\n        acc = np.mean(np.argmax(y_predict, axis=1) == np.argmax(y_true, axis=1))\n        # \xe8\xae\xa1\xe7\xae\x97loss\n        loss, gradient = cross_entropy_loss(y_predict, y_true)\n\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        vgg.backward(gradient)\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n        opt.iterate(vgg)\n\n        # \xe6\x89\x93\xe5\x8d\xb0\xe4\xbf\xa1\xe6\x81\xaf\n        print(\'{} step:{},loss:{:.4f},acc:{:.4f}\'.format(time.asctime(time.localtime(time.time())),\n                                                         step,\n                                                         loss,\n                                                         acc))\n\n        # \xe4\xbf\x9d\xe5\xad\x98\xe6\x9d\x83\xe9\x87\x8d\n        if step % 100 == 0:\n            save_weights(os.path.join(args.save_dir, \'weights-{:03d}.pkl\'.format(step)),\n                         vgg.weights)\n\n\ndef test(path):\n    (x_train, y_train), (x_test, y_test) = load_cifar(path)\n    print(x_train[0][0])\n    print(y_train[0])\n    vgg = VGG(name=\'vgg11\')\n    import utils\n    utils.save_weights(\'./w.pkl\', vgg.weights)\n    w = utils.load_weights(\'./w.pkl\')\n    print(type(w))\n    print(w.keys())\n\n\nif __name__ == \'__main__\':\n    # cifar_root = \'/Users/yizuotian/dataset/cifar-10-batches-py\'\n    # test(cifar_root)\n\n    parse = argparse.ArgumentParser()\n    parse.add_argument(\'-d\', \'--cifar-root\', type=str,\n                       default=\'/Users/yizuotian/dataset/cifar-10-batches-py\')\n    parse.add_argument(\'-o\', \'--save-dir\', type=str, default=\'/tmp\')\n    parse.add_argument(\'-c\', \'--checkpoint\', type=str, default=None)\n    parse.add_argument(\'-b\', \'--batch-size\', type=int, default=32)\n    parse.add_argument(\'--lr\', type=float, default=1e-2)\n    parse.add_argument(\'--decay\', type=float, default=1e-3)\n    parse.add_argument(\'-s\', \'--steps\', type=int, default=10000)\n    parse.add_argument(\'--eval-only\', action=\'store_true\', default=False)\n    parse.add_argument(\'--eval-num\', type=int, default=100)\n    parse.add_argument(\'--init-step\', type=int, default=0)\n    arguments = parse.parse_args()\n    main(arguments)\n'"
nn/dnn.py,14,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on 2018/9/1 22:22\r\n\r\n@author: mick.yi\r\n\r\ndnn\xe4\xbe\x8b\xe5\xad\x90\r\n\r\n""""""\r\nimport numpy as np\r\nfrom losses import cross_entropy_loss, mean_squared_loss\r\nfrom layers import fc_forward, fc_backward\r\nfrom activations import relu_forward, relu_backward\r\n\r\n\r\nclass Mnist(object):\r\n    def __init__(self):\r\n        weight_scale = 1e-3\r\n        self.weights = {}\r\n        self.weights[""W1""] = weight_scale * np.random.randn(28 * 28, 256)\r\n        self.weights[""b1""] = np.zeros(256)\r\n\r\n        self.weights[""W2""] = weight_scale * np.random.randn(256, 256)\r\n        self.weights[""b2""] = np.zeros(256)\r\n\r\n        self.weights[""W3""] = weight_scale * np.random.randn(256, 10)\r\n        self.weights[""b3""] = np.zeros(10)\r\n\r\n        # \xe5\xad\x98\xe6\x94\xbe\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe5\x80\xbc\r\n        self.nurons = {}\r\n\r\n        # \xe5\xad\x98\xe6\x94\xbe\xe6\xa2\xaf\xe5\xba\xa6\r\n        self.gradients = {}\r\n\r\n    def forward(self, train_data):\r\n        self.nurons[""z2""] = fc_forward(train_data, self.weights[""W1""], self.weights[""b1""])\r\n        self.nurons[""z2_relu""] = relu_forward(self.nurons[""z2""])\r\n        self.nurons[""z3""] = fc_forward(self.nurons[""z2_relu""], self.weights[""W2""], self.weights[""b2""])\r\n        self.nurons[""z3_relu""] = relu_forward(self.nurons[""z3""])\r\n        self.nurons[""y""] = fc_forward(self.nurons[""z3_relu""], self.weights[""W3""], self.weights[""b3""])\r\n        return self.nurons[""y""]\r\n\r\n    def backward(self, train_data, y_true):\r\n        loss, self.gradients[""y""] = cross_entropy_loss(self.nurons[""y""], y_true)\r\n        self.gradients[""W3""], self.gradients[""b3""], self.gradients[""z3_relu""] = fc_backward(self.gradients[""y""],\r\n                                                                                            self.weights[""W3""],\r\n                                                                                            self.nurons[""z3_relu""])\r\n        self.gradients[""z3""] = relu_backward(self.gradients[""z3_relu""], self.nurons[""z3""])\r\n        self.gradients[""W2""], self.gradients[""b2""], self.gradients[""z2_relu""] = fc_backward(self.gradients[""z3""],\r\n                                                                                            self.weights[""W2""],\r\n                                                                                            self.nurons[""z2_relu""])\r\n        self.gradients[""z2""] = relu_backward(self.gradients[""z2_relu""], self.nurons[""z2""])\r\n        self.gradients[""W1""], self.gradients[""b1""], _ = fc_backward(self.gradients[""z2""],\r\n                                                                    self.weights[""W1""],\r\n                                                                    train_data)\r\n        return loss\r\n\r\n    def get_accuracy(self, train_data, y_true):\r\n        score = self.forward(train_data)\r\n        acc = np.mean(np.argmax(score, axis=1) == np.argmax(y_true, axis=1))\r\n        return acc\r\n\r\n\r\nclass LinearRegression(object):\r\n    """"""\r\n    \xe6\xa8\xa1\xe6\x8b\x9f\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90 y=Wx+b\r\n    """"""\r\n\r\n    def __init__(self):\r\n        # \xe5\xae\x9e\xe9\x99\x85\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\r\n        self.W = np.array([[3, 7, 4],\r\n                           [5, 2, 6]])\r\n        self.b = np.array([2, 9, 3])\r\n        # \xe4\xba\xa7\xe7\x94\x9f\xe8\xae\xad\xe7\xbb\x83\xe6\xa0\xb7\xe6\x9c\xac\r\n        self.x_data = np.random.randint(0, 10, 1000).reshape(500, 2)\r\n        self.y_data = np.dot(self.x_data, self.W) + self.b\r\n\r\n    def next_sample(self, batch_size=1):\r\n        """"""\r\n        \xe9\x9a\x8f\xe6\x9c\xba\xe4\xba\xa7\xe7\x94\x9f\xe4\xb8\x8b\xe4\xb8\x80\xe6\x89\xb9\xe6\xa0\xb7\xe6\x9c\xac\r\n        :param batch_size:\r\n        :return:\r\n        """"""\r\n        idx = np.random.randint(500 - batch_size)\r\n        return self.x_data[idx:idx + batch_size], self.y_data[idx:idx + batch_size]\r\n\r\n    def train(self):\r\n        # \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0\r\n        W1 = np.random.randn(2, 3)\r\n        b1 = np.zeros([3])\r\n        loss = 100.0\r\n        lr = 0.01\r\n        i = 0\r\n\r\n        while loss > 1e-15:\r\n            x, y_true = self.next_sample(2)  # \xe8\x8e\xb7\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe6\xa0\xb7\xe6\x9c\xac\r\n            # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\r\n            y = fc_forward(x, W1, b1)\r\n            # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\r\n            loss, dy = mean_squared_loss(y, y_true)\r\n            dw, db, _ = fc_backward(dy, self.W, x)\r\n\r\n            # \xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaabatch\xe4\xb8\xad\xe6\xa2\xaf\xe5\xba\xa6\xe5\x8f\x96\xe5\x9d\x87\xe5\x80\xbc\r\n            # print(dw)\r\n\r\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\r\n            W1 -= lr * dw\r\n            b1 -= lr * db\r\n\r\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\r\n            i += 1\r\n            if i % 1000 == 0:\r\n                print(""\\n\xe8\xbf\xad\xe4\xbb\xa3{}\xe6\xac\xa1\xef\xbc\x8c\xe5\xbd\x93\xe5\x89\x8dloss:{}, \xe5\xbd\x93\xe5\x89\x8d\xe6\x9d\x83\xe9\x87\x8d:{},\xe5\xbd\x93\xe5\x89\x8d\xe5\x81\x8f\xe7\xbd\xae{}"".format(i, loss, W1, b1))\r\n\r\n                # \xe6\x89\x93\xe5\x8d\xb0\xe6\x9c\x80\xe7\xbb\x88\xe7\xbb\x93\xe6\x9e\x9c\r\n        print(""\\n\xe8\xbf\xad\xe4\xbb\xa3{}\xe6\xac\xa1\xef\xbc\x8c\xe5\xbd\x93\xe5\x89\x8dloss:{}, \xe5\xbd\x93\xe5\x89\x8d\xe6\x9d\x83\xe9\x87\x8d:{},\xe5\xbd\x93\xe5\x89\x8d\xe5\x81\x8f\xe7\xbd\xae{}"".format(i, loss, W1, b1))\r\n\r\n        return W1, b1\r\n'"
nn/layers.py,87,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on 2018/8/19 15:03\n\n@author: mick.yi\n\n\xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\n""""""\nimport numpy as np\nimport pyximport\n\npyximport.install()\nfrom clayers import conv_forward\n\n\ndef fc_forward(z, W, b):\n    """"""\n    \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    :param z: \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba,\xe5\xbd\xa2\xe7\x8a\xb6 (N,ln)\n    :param W: \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n    :param b: \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84\xe5\x81\x8f\xe7\xbd\xae\n    :return: \xe4\xb8\x8b\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n    """"""\n    return np.dot(z, W) + b\n\n\ndef fc_backward(next_dz, W, z):\n    """"""\n    \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    :param next_dz: \xe4\xb8\x8b\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    :param W: \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n    :param z: \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n    :return:\n    """"""\n    N = z.shape[0]\n    dz = np.dot(next_dz, W.T)  # \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    dw = np.dot(z.T, next_dz)  # \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    db = np.sum(next_dz, axis=0)  # \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe5\x81\x8f\xe7\xbd\xae\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6, N\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb1\x82\xe5\x92\x8c\n    return dw / N, db / N, dz\n\n\ndef _single_channel_conv(z, K, b=0, padding=(0, 0), strides=(1, 1)):\n    """"""\n    \xe5\xbd\x93\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5\n    :param K: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n    :param b: \xe5\x81\x8f\xe7\xbd\xae\n    :param padding: padding\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :return: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\n    """"""\n    padding_z = np.lib.pad(z, ((padding[0], padding[0]), (padding[1], padding[1])), \'constant\', constant_values=0)\n    height, width = padding_z.shape\n    k1, k2 = K.shape\n    assert (height - k1) % strides[0] == 0, \'\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\x8d\xe4\xb8\xba1\xe6\x97\xb6\xef\xbc\x8c\xe6\xad\xa5\xe9\x95\xbf\xe5\xbf\x85\xe9\xa1\xbb\xe5\x88\x9a\xe5\xa5\xbd\xe8\x83\xbd\xe5\xa4\x9f\xe8\xa2\xab\xe6\x95\xb4\xe9\x99\xa4\'\n    assert (width - k2) % strides[1] == 0, \'\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\x8d\xe4\xb8\xba1\xe6\x97\xb6\xef\xbc\x8c\xe6\xad\xa5\xe9\x95\xbf\xe5\xbf\x85\xe9\xa1\xbb\xe5\x88\x9a\xe5\xa5\xbd\xe8\x83\xbd\xe5\xa4\x9f\xe8\xa2\xab\xe6\x95\xb4\xe9\x99\xa4\'\n    conv_z = np.zeros((1 + (height - k1) // strides[0], 1 + (width - k2) // strides[1]))\n    for h in np.arange(height - k1 + 1)[::strides[0]]:\n        for w in np.arange(width - k2 + 1)[::strides[1]]:\n            conv_z[h // strides[0], w // strides[1]] = np.sum(padding_z[h:h + k1, w:w + k2] * K)\n    return conv_z + b\n\n\ndef _remove_padding(z, padding):\n    """"""\n    \xe7\xa7\xbb\xe9\x99\xa4padding\n    :param z: (N,C,H,W)\n    :param paddings: (p1,p2)\n    :return:\n    """"""\n    if padding[0] > 0 and padding[1] > 0:\n        return z[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n    elif padding[0] > 0:\n        return z[:, :, padding[0]:-padding[0], :]\n    elif padding[1] > 0:\n        return z[:, :, :, padding[1]:-padding[1]]\n    else:\n        return z\n\n\ndef conv_forward_bak(z, K, b, padding=(0, 0), strides=(1, 1)):\n    """"""\n    \xe5\xa4\x9a\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param K: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8,\xe5\xbd\xa2\xe7\x8a\xb6(C,D,k1,k2), C\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8cD\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param b: \xe5\x81\x8f\xe7\xbd\xae,\xe5\xbd\xa2\xe7\x8a\xb6(D,)\n    :param padding: padding\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :return: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\n    """"""\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), \'constant\',\n                           constant_values=0)\n    N, _, height, width = padding_z.shape\n    C, D, k1, k2 = K.shape\n    assert (height - k1) % strides[0] == 0, \'\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\x8d\xe4\xb8\xba1\xe6\x97\xb6\xef\xbc\x8c\xe6\xad\xa5\xe9\x95\xbf\xe5\xbf\x85\xe9\xa1\xbb\xe5\x88\x9a\xe5\xa5\xbd\xe8\x83\xbd\xe5\xa4\x9f\xe8\xa2\xab\xe6\x95\xb4\xe9\x99\xa4\'\n    assert (width - k2) % strides[1] == 0, \'\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\x8d\xe4\xb8\xba1\xe6\x97\xb6\xef\xbc\x8c\xe6\xad\xa5\xe9\x95\xbf\xe5\xbf\x85\xe9\xa1\xbb\xe5\x88\x9a\xe5\xa5\xbd\xe8\x83\xbd\xe5\xa4\x9f\xe8\xa2\xab\xe6\x95\xb4\xe9\x99\xa4\'\n    conv_z = np.zeros((N, D, 1 + (height - k1) // strides[0], 1 + (width - k2) // strides[1]))\n    for n in np.arange(N):\n        for d in np.arange(D):\n            for h in np.arange(height - k1 + 1)[::strides[0]]:\n                for w in np.arange(width - k2 + 1)[::strides[1]]:\n                    conv_z[n, d, h // strides[0], w // strides[1]] = np.sum(\n                        padding_z[n, :, h:h + k1, w:w + k2] * K[:, d]) + b[d]\n    return conv_z\n\n\ndef _insert_zeros(dz, strides):\n    """"""\n    \xe6\x83\xb3\xe5\xa4\x9a\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\xa4\xe4\xbd\x8d\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe8\xa1\x8c\xe5\x88\x97\xe4\xb9\x8b\xe9\x97\xb4\xe5\xa2\x9e\xe5\x8a\xa0\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xe7\x9a\x84\xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    :param dz: (N,D,H,W),H,W\xe4\xb8\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :return:\n    """"""\n    _, _, H, W = dz.shape\n    pz = dz\n    if strides[0] > 1:\n        for h in np.arange(H - 1, 0, -1):\n            for o in np.arange(strides[0] - 1):\n                pz = np.insert(pz, h, 0, axis=2)\n    if strides[1] > 1:\n        for w in np.arange(W - 1, 0, -1):\n            for o in np.arange(strides[1] - 1):\n                pz = np.insert(pz, w, 0, axis=3)\n    return pz\n\n\ndef conv_backward(next_dz, K, z, padding=(0, 0), strides=(1, 1)):\n    """"""\n    \xe5\xa4\x9a\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param next_dz: \xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6,(N,D,H,W),H,W\xe4\xb8\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n    :param K: \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8c(C,D,k1,k2)\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param padding: padding\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    C, D, k1, k2 = K.shape\n\n    # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\xa2\xaf\xe5\xba\xa6\n    # dK = np.zeros((C, D, k1, k2))\n    padding_next_dz = _insert_zeros(next_dz, strides)\n\n    # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\xe7\xbf\xbb\xe8\xbd\xac180\xe5\xba\xa6\n    flip_K = np.flip(K, (2, 3))\n    # \xe4\xba\xa4\xe6\x8d\xa2C,D\xe4\xb8\xbaD,C\xef\xbc\x9bD\xe5\x8f\x98\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xba\x86\xef\xbc\x8cC\xe5\x8f\x98\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xba\x86\n    swap_flip_K = np.swapaxes(flip_K, 0, 1)\n    # \xe5\xa2\x9e\xe5\x8a\xa0\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa60\xe5\xa1\xab\xe5\x85\x85\n    ppadding_next_dz = np.lib.pad(padding_next_dz, ((0, 0), (0, 0), (k1 - 1, k1 - 1), (k2 - 1, k2 - 1)), \'constant\',\n                                  constant_values=0)\n    dz = conv_forward(ppadding_next_dz,\n                      swap_flip_K,\n                      np.zeros((C,), dtype=np.float))\n\n    # \xe6\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x92\x8c\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6dK\n    swap_z = np.swapaxes(z, 0, 1)  # \xe5\x8f\x98\xe4\xb8\xba(C,N,H,W)\xe4\xb8\x8e\n    dK = conv_forward(swap_z, padding_next_dz, np.zeros((D,), dtype=np.float))\n\n    # \xe5\x81\x8f\xe7\xbd\xae\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    db = np.sum(np.sum(np.sum(next_dz, axis=-1), axis=-1), axis=0)  # \xe5\x9c\xa8\xe9\xab\x98\xe5\xba\xa6\xe3\x80\x81\xe5\xae\xbd\xe5\xba\xa6\xe4\xb8\x8a\xe7\x9b\xb8\xe5\x8a\xa0\xef\xbc\x9b\xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\x8a\xe7\x9b\xb8\xe5\x8a\xa0\n\n    # \xe6\x8a\x8apadding\xe5\x87\x8f\xe6\x8e\x89\n    dz = _remove_padding(dz, padding)  # dz[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n\n    return dK / N, db / N, dz\n\n\ndef max_pooling_forward_bak(z, pooling, strides=(2, 2), padding=(0, 0)):\n    """"""\n    \xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param pooling: \xe6\xb1\xa0\xe5\x8c\x96\xe5\xa4\xa7\xe5\xb0\x8f(k1,k2)\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :param padding: 0\xe5\xa1\xab\xe5\x85\x85\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), \'constant\',\n                           constant_values=0)\n\n    # \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n    out_h = (H + 2 * padding[0] - pooling[0]) // strides[0] + 1\n    out_w = (W + 2 * padding[1] - pooling[1]) // strides[1] + 1\n\n    pool_z = np.zeros((N, C, out_h, out_w), dtype=np.float32)\n\n    for n in np.arange(N):\n        for c in np.arange(C):\n            for i in np.arange(out_h):\n                for j in np.arange(out_w):\n                    pool_z[n, c, i, j] = np.max(padding_z[n, c,\n                                                strides[0] * i:strides[0] * i + pooling[0],\n                                                strides[1] * j:strides[1] * j + pooling[1]])\n    return pool_z\n\n\ndef max_pooling_backward_bak(next_dz, z, pooling, strides=(2, 2), padding=(0, 0)):\n    """"""\n    \xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param next_dz\xef\xbc\x9a\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x85\xb3\xe4\xba\x8e\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param pooling: \xe6\xb1\xa0\xe5\x8c\x96\xe5\xa4\xa7\xe5\xb0\x8f(k1,k2)\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :param padding: 0\xe5\xa1\xab\xe5\x85\x85\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    _, _, out_h, out_w = next_dz.shape\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), \'constant\',\n                           constant_values=0)\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\xe5\x90\x8e\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    padding_dz = np.zeros_like(padding_z)\n\n    for n in np.arange(N):\n        for c in np.arange(C):\n            for i in np.arange(out_h):\n                for j in np.arange(out_w):\n                    # \xe6\x89\xbe\xe5\x88\xb0\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe5\xb0\x86\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbc\xa0\xe7\xbb\x99\xe8\xbf\x99\xe4\xb8\xaa\xe5\x9d\x90\xe6\xa0\x87\n                    flat_idx = np.argmax(padding_z[n, c,\n                                         strides[0] * i:strides[0] * i + pooling[0],\n                                         strides[1] * j:strides[1] * j + pooling[1]])\n                    h_idx = strides[0] * i + flat_idx // pooling[1]\n                    w_idx = strides[1] * j + flat_idx % pooling[1]\n                    padding_dz[n, c, h_idx, w_idx] += next_dz[n, c, i, j]\n    # \xe8\xbf\x94\xe5\x9b\x9e\xe6\x97\xb6\xe5\x89\x94\xe9\x99\xa4\xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    return _remove_padding(padding_dz, padding)  # padding_z[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n\n\ndef avg_pooling_forward(z, pooling, strides=(2, 2), padding=(0, 0)):\n    """"""\n    \xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param pooling: \xe6\xb1\xa0\xe5\x8c\x96\xe5\xa4\xa7\xe5\xb0\x8f(k1,k2)\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :param padding: 0\xe5\xa1\xab\xe5\x85\x85\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), \'constant\',\n                           constant_values=0)\n\n    # \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n    out_h = (H + 2 * padding[0] - pooling[0]) // strides[0] + 1\n    out_w = (W + 2 * padding[1] - pooling[1]) // strides[1] + 1\n\n    pool_z = np.zeros((N, C, out_h, out_w), dtype=np.float32)\n\n    for n in np.arange(N):\n        for c in np.arange(C):\n            for i in np.arange(out_h):\n                for j in np.arange(out_w):\n                    pool_z[n, c, i, j] = np.mean(padding_z[n, c,\n                                                 strides[0] * i:strides[0] * i + pooling[0],\n                                                 strides[1] * j:strides[1] * j + pooling[1]])\n    return pool_z\n\n\ndef avg_pooling_backward(next_dz, z, pooling, strides=(2, 2), padding=(0, 0)):\n    """"""\n    \xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param next_dz\xef\xbc\x9a\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x85\xb3\xe4\xba\x8e\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param pooling: \xe6\xb1\xa0\xe5\x8c\x96\xe5\xa4\xa7\xe5\xb0\x8f(k1,k2)\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :param padding: 0\xe5\xa1\xab\xe5\x85\x85\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    _, _, out_h, out_w = next_dz.shape\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), \'constant\',\n                           constant_values=0)\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\xe5\x90\x8e\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    padding_dz = np.zeros_like(padding_z)\n\n    for n in np.arange(N):\n        for c in np.arange(C):\n            for i in np.arange(out_h):\n                for j in np.arange(out_w):\n                    # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe5\x9d\x87\xe5\x88\x86\xe6\xa2\xaf\xe5\xba\xa6\n                    padding_dz[n, c,\n                    strides[0] * i:strides[0] * i + pooling[0],\n                    strides[1] * j:strides[1] * j + pooling[1]] += next_dz[n, c, i, j] / (pooling[0] * pooling[1])\n    # \xe8\xbf\x94\xe5\x9b\x9e\xe6\x97\xb6\xe5\x89\x94\xe9\x99\xa4\xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    return _remove_padding(padding_dz, padding)  # padding_z[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n\n\ndef global_max_pooling_forward(z):\n    """"""\n    \xe5\x85\xa8\xe5\xb1\x80\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :return:\n    """"""\n    return np.max(np.max(z, axis=-1), -1)\n\n\ndef global_max_pooling_forward(next_dz, z):\n    """"""\n    \xe5\x85\xa8\xe5\xb1\x80\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param next_dz: \xe5\x85\xa8\xe5\xb1\x80\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6(N,C)\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    dz = np.zeros_like(z)\n    for n in np.arange(N):\n        for c in np.arange(C):\n            # \xe6\x89\xbe\xe5\x88\xb0\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe6\x89\x80\xe5\x9c\xa8\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbc\xa0\xe7\xbb\x99\xe8\xbf\x99\xe4\xb8\xaa\xe5\x9d\x90\xe6\xa0\x87\n            idx = np.argmax(z[n, c, :, :])\n            h_idx = idx // W\n            w_idx = idx % W\n            dz[n, c, h_idx, w_idx] = next_dz[n, c]\n    return dz\n\n\ndef global_avg_pooling_forward(z):\n    """"""\n    \xe5\x85\xa8\xe5\xb1\x80\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :return:\n    """"""\n    return np.mean(np.mean(z, axis=-1), axis=-1)\n\n\ndef global_avg_pooling_backward(next_dz, z):\n    """"""\n    \xe5\x85\xa8\xe5\xb1\x80\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param next_dz: \xe5\x85\xa8\xe5\xb1\x80\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6(N,C)\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    dz = np.zeros_like(z)\n    for n in np.arange(N):\n        for c in np.arange(C):\n            # \xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe5\x88\x86\xe7\xbb\x99\xe7\x9b\xb8\xe5\x85\xb3\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\n            dz[n, c, :, :] += next_dz[n, c] / (H * W)\n    return dz\n\n\ndef flatten_forward(z):\n    """"""\n    \xe5\xb0\x86\xe5\xa4\x9a\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe6\x89\x93\xe5\xb9\xb3\xef\xbc\x8c\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    :param z: \xe5\xa4\x9a\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84,\xe5\xbd\xa2\xe7\x8a\xb6(N,d1,d2,..)\n    :return:\n    """"""\n    N = z.shape[0]\n    return np.reshape(z, (N, -1))\n\n\ndef flatten_backward(next_dz, z):\n    """"""\n    \xe6\x89\x93\xe5\xb9\xb3\xe5\xb1\x82\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    :param next_dz:\n    :param z:\n    :return:\n    """"""\n    return np.reshape(next_dz, z.shape)\n\n\ndef main():\n    z = np.ones((5, 5))\n    k = np.ones((3, 3))\n    b = 3\n    # print(_single_channel_conv(z, k,padding=(1,1)))\n    # print(_single_channel_conv(z, k, strides=(2, 2)))\n    assert _single_channel_conv(z, k).shape == (3, 3)\n    assert _single_channel_conv(z, k, padding=(1, 1)).shape == (5, 5)\n    assert _single_channel_conv(z, k, strides=(2, 2)).shape == (2, 2)\n    assert _single_channel_conv(z, k, strides=(2, 2), padding=(1, 1)).shape == (3, 3)\n    assert _single_channel_conv(z, k, strides=(2, 2), padding=(1, 0)).shape == (3, 2)\n    assert _single_channel_conv(z, k, strides=(2, 1), padding=(1, 1)).shape == (3, 5)\n\n    dz = np.ones((1, 1, 3, 3))\n    assert _insert_zeros(dz, (1, 1)).shape == (1, 1, 3, 3)\n    print(_insert_zeros(dz, (3, 2)))\n    assert _insert_zeros(dz, (1, 2)).shape == (1, 1, 3, 5)\n    assert _insert_zeros(dz, (2, 2)).shape == (1, 1, 5, 5)\n    assert _insert_zeros(dz, (2, 4)).shape == (1, 1, 5, 9)\n\n    z = np.ones((8, 16, 5, 5))\n    k = np.ones((16, 32, 3, 3))\n    b = np.ones((32))\n    assert conv_forward(z, k, b).shape == (8, 32, 3, 3)\n    print(conv_forward(z, k, b)[0, 0])\n\n    print(np.argmax(np.array([[1, 2], [3, 4]])))\n\n\ndef test_conv():\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe5\x8d\xb7\xe7\xa7\xaf\n    z = np.random.randn(3, 3, 28, 28).astype(np.float)\n    K = np.random.randn(3, 4, 3, 3).astype(np.float) * 1e-3\n    b = np.zeros(4).astype(np.float)\n\n    next_z = conv_forward(z, K, b)\n    y_true = np.ones_like(next_z)\n\n    from nn.losses import mean_squared_loss\n    for i in range(10000):\n        # \xe5\x89\x8d\xe5\x90\x91\n        next_z = conv_forward(z, K, b)\n        # \xe5\x8f\x8d\xe5\x90\x91\n        loss, dy = mean_squared_loss(next_z, y_true)\n        dK, db, _ = conv_backward(dy, K, z)\n        K -= 0.001 * dK\n        b -= 0.001 * db\n\n        if i % 10 == 0:\n            print(""i:{},loss:{},mindy:{},maxdy:{}"".format(i, loss, np.mean(dy), np.max(dy)))\n\n        if np.allclose(y_true, next_z):\n            print(""yes"")\n            break\n\n\ndef test_conv_and_max_pooling():\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x92\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\n    z = np.random.randn(3, 3, 28, 28).astype(np.float)\n    K = np.random.randn(3, 4, 3, 3).astype(np.float) * 1e-3\n    b = np.zeros(4).astype(np.float)\n\n    next_z = conv_forward(z, K, b)\n    y_pred = max_pooling_forward_bak(next_z, pooling=(2, 2))\n    y_true = np.ones_like(y_pred)\n\n    from nn.losses import mean_squared_loss\n    for i in range(10000):\n        # \xe5\x89\x8d\xe5\x90\x91\n        next_z = conv_forward(z, K, b)\n        y_pred = max_pooling_forward_bak(next_z, pooling=(2, 2))\n        # \xe5\x8f\x8d\xe5\x90\x91\n        loss, dy = mean_squared_loss(y_pred, y_true)\n        next_dz = max_pooling_backward_bak(dy, next_z, pooling=(2, 2))\n        dK, db, _ = conv_backward(next_dz, K, z)\n        K -= 0.001 * dK\n        b -= 0.001 * db\n\n        if i % 10 == 0:\n            print(""i:{},loss:{},mindy:{},maxdy:{}"".format(i, loss, np.mean(dy), np.max(dy)))\n\n        if np.allclose(y_true, y_pred):\n            print(""yes"")\n            break\n\n\nif __name__ == ""__main__"":\n    # main()\n    test_conv()\n'"
nn/layers_v2.py,72,"b'# -*- coding: utf-8 -*-\n""""""\n @File    : layers_v2.py\n @Time    : 2020/4/25 \xe4\xb8\x8a\xe5\x8d\x889:15\n @Author  : yizuotian\n @Description    : v2\xe7\x89\x88\xe5\x89\x8d\xe5\x90\x91\xe3\x80\x81\xe5\x8f\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x9b\xe8\xa7\xa3\xe5\x86\xb3\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xae\xa1\xe7\xae\x97\xe9\x80\x9f\xe5\xba\xa6\xe6\x85\xa2\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\n""""""\nimport time\n\nimport numpy as np\nimport pyximport\n\nfrom layers import _insert_zeros, _remove_padding\n\npyximport.install()\nfrom clayers_v2 import conv_forward as c_conv_forward\n\n\ndef _single_channel_conv_v1(z, K, b=0, padding=(0, 0)):\n    """"""\n    \xe5\xbd\x93\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5\n    :param K: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n    :param b: \xe5\x81\x8f\xe7\xbd\xae\n    :param padding: padding\n    :return: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\n    """"""\n    padding_z = np.lib.pad(z, ((padding[0], padding[0]), (padding[1], padding[1])), \'constant\', constant_values=0)\n    height, width = padding_z.shape\n    k1, k2 = K.shape\n    conv_z = np.zeros((1 + (height - k1), 1 + (width - k2)))\n    for h in np.arange(height - k1 + 1):\n        for w in np.arange(width - k2 + 1):\n            conv_z[h, w] = np.sum(padding_z[h:h + k1, w:w + k2] * K)\n    return conv_z + b\n\n\ndef _single_channel_conv(z, K, b=0, padding=(0, 0)):\n    """"""\n    \xe5\xbd\x93\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5\n    :param K: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n    :param b: \xe5\x81\x8f\xe7\xbd\xae\n    :param padding: padding\n    :return: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\n    """"""\n    padding_z = np.lib.pad(z, ((padding[0], padding[0]), (padding[1], padding[1])), \'constant\', constant_values=0)\n    height, width = padding_z.shape\n    k1, k2 = K.shape\n    oh, ow = (1 + (height - k1), 1 + (width - k2))  # \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n    conv_z = np.zeros((1 + (height - k1), 1 + (width - k2)))\n    # \xe9\x81\x8d\xe5\x8e\x86\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xaf\x94\xe9\x81\x8d\xe5\x8e\x86\xe7\x89\xb9\xe5\xbe\x81\xe9\xab\x98\xe6\x95\x88\n    for i in range(k1):\n        for j in range(k2):\n            conv_z += padding_z[i:i + oh, j:j + ow] * K[i, j]\n\n    return conv_z + b\n\n\ndef conv_forward_v1(z, K, b, padding=(0, 0)):\n    """"""\n    \xe5\xa4\x9a\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param K: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8,\xe5\xbd\xa2\xe7\x8a\xb6(C,D,k1,k2), C\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8cD\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param b: \xe5\x81\x8f\xe7\xbd\xae,\xe5\xbd\xa2\xe7\x8a\xb6(D,)\n    :param padding: padding\n    :return: conv_z: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c[N,D,oH,oW]\n    """"""\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), \'constant\',\n                           constant_values=0)\n    N, _, height, width = padding_z.shape\n    C, D, k1, k2 = K.shape\n    oh, ow = (1 + (height - k1), 1 + (width - k2))  # \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n    conv_z = np.zeros((N, D, oh, ow))\n    for n in np.arange(N):\n        for d in np.arange(D):\n            for h in np.arange(oh):\n                for w in np.arange(oh):\n                    conv_z[n, d, h, w] = np.sum(\n                        padding_z[n, :, h:h + k1, w:w + k2] * K[:, d]) + b[d]\n    return conv_z\n\n\ndef _conv_forward_old(z, K, b, padding=(0, 0)):\n    """"""\n    \xe5\x8d\xa0\xe7\x94\xa8\xe5\xa4\xaa\xe5\xa4\x9a\xe5\x86\x85\xe5\xad\x98\xe5\x8f\x8d\xe8\x80\x8c\xe6\x85\xa2\n    \xe5\xa4\x9a\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b;\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param K: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8,\xe5\xbd\xa2\xe7\x8a\xb6(C,D,k1,k2), C\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8cD\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param b: \xe5\x81\x8f\xe7\xbd\xae,\xe5\xbd\xa2\xe7\x8a\xb6(D,)\n    :param padding: padding\n    :return: conv_z: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c[N,D,oH,oW]\n    """"""\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), \'constant\',\n                           constant_values=0)\n    N, _, height, width = padding_z.shape\n    C, D, k1, k2 = K.shape\n    oh, ow = (1 + (height - k1), 1 + (width - k2))  # \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n\n    # \xe6\x89\xa9\xe7\xbb\xb4\n    padding_z = padding_z[:, :, np.newaxis, :, :]  # \xe6\x89\xa9\xe7\xbb\xb4[N,C,1,H,W] \xe4\xb8\x8eK [C,D,K1,K2] \xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb9\xbf\xe6\x92\xad\n    K = K[:, :, :, :, np.newaxis, np.newaxis]\n    conv_z = np.zeros((N, C, D, oh, ow))\n\n    # \xe6\x89\xb9\xe9\x87\x8f\xe5\x8d\xb7\xe7\xa7\xaf\n    for i in range(k1):\n        for j in range(k2):\n            # [N,C,1,oh,ow]*[C,D,1,1] =>[N,C,D,oh,ow]\n            conv_z += padding_z[:, :, :, i:i + oh, j:j + ow] * K[:, :, i, j]\n\n    conv_z = np.sum(conv_z, axis=1)  # [N, C, D, oh, ow] => [N, D, oh, ow]\n    # \xe5\xa2\x9e\xe5\x8a\xa0\xe5\x81\x8f\xe7\xbd\xae [N, D, oh, ow]+[D, 1, 1]\n    conv_z += b[:, np.newaxis, np.newaxis]\n    return conv_z\n\n\ndef _conv_forward(z, K, b, padding=(0, 0)):\n    """"""\n    \xe5\xa4\x9a\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param K: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8,\xe5\xbd\xa2\xe7\x8a\xb6(C,D,k1,k2), C\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8cD\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param b: \xe5\x81\x8f\xe7\xbd\xae,\xe5\xbd\xa2\xe7\x8a\xb6(D,)\n    :param padding: padding\n    :return: conv_z: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c[N,D,oH,oW]\n    """"""\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (padding[0], padding[0]), (padding[1], padding[1])), \'constant\',\n                           constant_values=0)\n    N, _, height, width = padding_z.shape\n    C, D, k1, k2 = K.shape\n    oh, ow = (1 + (height - k1), 1 + (width - k2))  # \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n\n    # \xe6\x89\xa9\xe7\xbb\xb4\n    padding_z = padding_z[:, :, np.newaxis, :, :]  # \xe6\x89\xa9\xe7\xbb\xb4[N,C,1,H,W] \xe4\xb8\x8eK [C,D,K1,K2] \xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb9\xbf\xe6\x92\xad\n    conv_z = np.zeros((N, D, oh, ow))\n\n    # \xe6\x89\xb9\xe9\x87\x8f\xe5\x8d\xb7\xe7\xa7\xaf\n    if k1 * k2 < oh * ow * 10:\n        K = K[:, :, :, :, np.newaxis, np.newaxis]\n        for c in range(C):\n            for i in range(k1):\n                for j in range(k2):\n                    # [N,1,oh,ow]*[D,1,1] =>[N,D,oh,ow]\n                    conv_z += padding_z[:, c, :, i:i + oh, j:j + ow] * K[c, :, i, j]\n    else:  # \xe5\xa4\xa7\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8c\xe9\x81\x8d\xe5\x8e\x86\xe7\xa9\xba\xe9\x97\xb4\xe6\x9b\xb4\xe9\xab\x98\xe6\x95\x88\n        # print(\'\xe5\xa4\xa7\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8c\xe9\x81\x8d\xe5\x8e\x86\xe7\xa9\xba\xe9\x97\xb4\xe6\x9b\xb4\xe9\xab\x98\xe6\x95\x88\')\n        for c in range(C):\n            for h in range(oh):\n                for w in range(ow):\n                    # [N,1,k1,k2]*[D,k1,k2] =>[N,D,k1,k2] => [N,D]\n                    conv_z[:, :, h, w] += np.sum(padding_z[:, c, :, h:h + k1, w:w + k2] * K[c], axis=(2, 3))\n\n    # \xe5\xa2\x9e\xe5\x8a\xa0\xe5\x81\x8f\xe7\xbd\xae [N, D, oh, ow]+[D, 1, 1]\n    conv_z += b[:, np.newaxis, np.newaxis]\n    return conv_z\n\n\ndef conv_forward(z, K, b, padding=(0, 0), strides=(1, 1)):\n    """"""\n    \xe5\xa4\x9a\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param K: \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8,\xe5\xbd\xa2\xe7\x8a\xb6(C,D,k1,k2), C\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8cD\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param b: \xe5\x81\x8f\xe7\xbd\xae,\xe5\xbd\xa2\xe7\x8a\xb6(D,)\n    :param padding: padding\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :return: conv_z: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c[N,D,oH,oW]\n    """"""\n    # \xe9\x95\xbf\xe5\xae\xbd\xe6\x96\xb9\xe5\x90\x91\xe6\xad\xa5\xe9\x95\xbf\n    sh, sw = strides\n    # origin_conv_z = _conv_forward(z, K, b, padding)\n    origin_conv_z = c_conv_forward(z, K, b, padding)  # \xe4\xbd\xbf\xe7\x94\xa8cython\n    # \xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1\xe6\x97\xb6\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb0\xba\xe5\xaf\xb8\n    N, D, oh, ow = origin_conv_z.shape\n    if sh * sw == 1:\n        return origin_conv_z\n    # \xe9\xab\x98\xe5\xba\xa6\xe6\x96\xb9\xe5\x90\x91\xe6\xad\xa5\xe9\x95\xbf\xe5\xa4\xa7\xe4\xba\x8e1\n    elif sw == 1:\n        conv_z = np.zeros((N, D, oh // sh, ow))\n        for i in range(oh // sh):\n            conv_z[:, :, i, :] = origin_conv_z[:, :, i * sh, :]\n        return conv_z\n    # \xe5\xae\xbd\xe5\xba\xa6\xe6\x96\xb9\xe5\x90\x91\xe6\xad\xa5\xe9\x95\xbf\xe5\xa4\xa7\xe4\xba\x8e1\n    elif sh == 1:\n        conv_z = np.zeros((N, D, oh, ow // sw))\n        for j in range(ow // sw):\n            conv_z[:, :, :, j] = origin_conv_z[:, :, :, j * sw]\n        return conv_z\n    # \xe9\xab\x98\xe5\xba\xa6\xe5\xae\xbd\xe5\xba\xa6\xe6\x96\xb9\xe5\x90\x91\xe6\xad\xa5\xe9\x95\xbf\xe9\x83\xbd\xe5\xa4\xa7\xe4\xba\x8e1\n    else:\n        conv_z = np.zeros((N, D, oh // sh, ow // sw))\n        for i in range(oh // sh):\n            for j in range(ow // sw):\n                conv_z[:, :, i, j] = origin_conv_z[:, :, i * sh, j * sw]\n        return conv_z\n\n\ndef conv_backward(next_dz, K, z, padding=(0, 0), strides=(1, 1)):\n    """"""\n    \xe5\xa4\x9a\xe9\x80\x9a\xe9\x81\x93\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param next_dz: \xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6,(N,D,H,W),H,W\xe4\xb8\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n    :param K: \xe5\xbd\x93\xe5\x89\x8d\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x8c(C,D,k1,k2)\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param padding: padding\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    C, D, k1, k2 = K.shape\n\n    # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\xa2\xaf\xe5\xba\xa6\n    # dK = np.zeros((C, D, k1, k2))\n    padding_next_dz = _insert_zeros(next_dz, strides)\n\n    # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\xe7\xbf\xbb\xe8\xbd\xac180\xe5\xba\xa6\n    flip_K = np.flip(K, (2, 3))\n    # \xe4\xba\xa4\xe6\x8d\xa2C,D\xe4\xb8\xbaD,C\xef\xbc\x9bD\xe5\x8f\x98\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xba\x86\xef\xbc\x8cC\xe5\x8f\x98\xe4\xb8\xba\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xba\x86\n    swap_flip_K = np.swapaxes(flip_K, 0, 1)\n    # \xe5\xa2\x9e\xe5\x8a\xa0\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa60\xe5\xa1\xab\xe5\x85\x85\n    ppadding_next_dz = np.lib.pad(padding_next_dz, ((0, 0), (0, 0), (k1 - 1, k1 - 1), (k2 - 1, k2 - 1)), \'constant\',\n                                  constant_values=0)\n    dz = conv_forward(ppadding_next_dz,\n                      swap_flip_K,\n                      np.zeros((C,), dtype=np.float))\n\n    # \xe6\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x92\x8c\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6dK\n    swap_z = np.swapaxes(z, 0, 1)  # \xe5\x8f\x98\xe4\xb8\xba(C,N,H,W)\xe4\xb8\x8e\n    dK = conv_forward(swap_z, padding_next_dz, np.zeros((D,), dtype=np.float))\n\n    # \xe5\x81\x8f\xe7\xbd\xae\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6,[N,D,H,W]=>[D]\n    db = np.sum(next_dz, axis=(0, 2, 3))  # \xe5\x9c\xa8\xe9\xab\x98\xe5\xba\xa6\xe3\x80\x81\xe5\xae\xbd\xe5\xba\xa6\xe4\xb8\x8a\xe7\x9b\xb8\xe5\x8a\xa0\xef\xbc\x9b\xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\x8a\xe7\x9b\xb8\xe5\x8a\xa0\n\n    # \xe6\x8a\x8apadding\xe5\x87\x8f\xe6\x8e\x89\n    dz = _remove_padding(dz, padding)  # dz[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n\n    return dK / N, db / N, dz\n\n\ndef max_pooling_forward(z, pooling, strides=(2, 2), padding=(0, 0)):\n    """"""\n    \xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param pooling: \xe6\xb1\xa0\xe5\x8c\x96\xe5\xa4\xa7\xe5\xb0\x8f(k1,k2)\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :param padding: 0\xe5\xa1\xab\xe5\x85\x85\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    pad_h, pad_w = padding\n    sh, sw = strides\n    kh, kw = pooling\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w)), \'constant\',\n                           constant_values=0)\n\n    # \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n    out_h = (H + 2 * pad_h - kh) // sh + 1\n    out_w = (W + 2 * pad_w - kw) // sw + 1\n\n    pool_z = np.zeros((N, C, out_h, out_w), dtype=np.float)\n\n    for i in np.arange(out_h):\n        for j in np.arange(out_w):\n            pool_z[:, :, i, j] = np.max(padding_z[:, :, sh * i:sh * i + kh, sw * j:sw * j + kw],\n                                        axis=(2, 3))\n    return pool_z\n\n\ndef max_pooling_backward(next_dz, z, pooling, strides=(2, 2), padding=(0, 0)):\n    """"""\n    \xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param next_dz\xef\xbc\x9a\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x85\xb3\xe4\xba\x8e\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param pooling: \xe6\xb1\xa0\xe5\x8c\x96\xe5\xa4\xa7\xe5\xb0\x8f(k1,k2)\n    :param strides: \xe6\xad\xa5\xe9\x95\xbf\n    :param padding: 0\xe5\xa1\xab\xe5\x85\x85\n    :return:\n    """"""\n    N, C, H, W = z.shape\n    pad_h, pad_w = padding\n    sh, sw = strides\n    kh, kw = pooling\n    _, _, out_h, out_w = next_dz.shape\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    padding_z = np.lib.pad(z, ((0, 0), (0, 0), (pad_h, pad_h), (pad_w, pad_w)), \'constant\',\n                           constant_values=0)\n    # \xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\xe5\x90\x8e\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n    padding_dz = np.zeros_like(padding_z)\n    zeros = np.zeros((N, C, sh, sw))\n    for i in np.arange(out_h):\n        for j in np.arange(out_w):\n            # \xe6\x89\xbe\xe5\x88\xb0\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe5\xb0\x86\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbc\xa0\xe7\xbb\x99\xe8\xbf\x99\xe4\xb8\xaa\xe5\x9d\x90\xe6\xa0\x87\n            cur_padding_z = padding_z[:, :, sh * i:sh * i + kh, sw * j:sw * j + kw]\n            cur_padding_dz = padding_dz[:, :, sh * i:sh * i + kh, sw * j:sw * j + kw]\n            max_val = np.max(cur_padding_z, axis=(2, 3))  # [N,C]\n            cur_padding_dz += np.where(cur_padding_z == max_val[:, :, np.newaxis, np.newaxis],\n                                       next_dz[:, :, i:i + 1, j:j + 1],\n                                       zeros)\n    # \xe8\xbf\x94\xe5\x9b\x9e\xe6\x97\xb6\xe5\x89\x94\xe9\x99\xa4\xe9\x9b\xb6\xe5\xa1\xab\xe5\x85\x85\n    return _remove_padding(padding_dz, padding)  # padding_z[:, :, padding[0]:-padding[0], padding[1]:-padding[1]]\n\n\ndef global_avg_pooling_backward(next_dz, z):\n    """"""\n    \xe5\x85\xa8\xe5\xb1\x80\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\n    :param next_dz: \xe5\x85\xa8\xe5\xb1\x80\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6(N,C)\n    :param z: \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9f\xa9\xe9\x98\xb5,\xe5\xbd\xa2\xe7\x8a\xb6(N,C,H,W)\xef\xbc\x8cN\xe4\xb8\xbabatch_size\xef\xbc\x8cC\xe4\xb8\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :return dz: [N,C,H,W]\n    """"""\n    _, _, H, W = z.shape\n    dz = np.zeros_like(z)\n    # \xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe5\x88\x86\xe7\xbb\x99\xe7\x9b\xb8\xe5\x85\xb3\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\n    dz += next_dz[:, :, np.newaxis, np.newaxis] / (H * W)\n    return dz\n\n\ndef test_single_conv():\n    """"""\n    \xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x80\x9f\xe5\xba\xa6\xe7\x9b\xb8\xe5\xb7\xae\xe7\x99\xbe\xe5\x80\x8d\xe4\xbb\xa5\xe4\xb8\x8a\n    :return:\n    """"""\n    z = np.random.randn(224, 224)\n    K = np.random.randn(3, 3)\n\n    s = time.time()\n    o1 = _single_channel_conv_v1(z, K)\n    print(""v1 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n    s = time.time()\n    o2 = _single_channel_conv(z, K)\n    print(""v2 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n\n    print(np.allclose(o1, o2))\n\n\ndef test_conv():\n    """"""\n    \xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x80\x9f\xe5\xba\xa6\xe7\x9b\xb8\xe5\xb7\xae\xe5\x87\xa0\xe5\x8d\x81\xe5\x80\x8d\n    :return:\n    """"""\n    z = np.random.randn(4, 3, 112, 112)\n    K = np.random.randn(3, 32, 3, 3)\n    b = np.random.randn(32)\n    s = time.time()\n    o1 = conv_forward_v1(z, K, b)\n    print(""v1 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n    s = time.time()\n    o2 = _conv_forward(z, K, b)\n    print(""v2 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n\n    import pyximport\n    pyximport.install()\n    from clayers_v2 import conv_forward as c_conv_forward\n    s = time.time()\n    o3 = c_conv_forward(z, K, b)\n    print(""cython v2 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n\n    print(np.allclose(o1, o2), np.allclose(o2, o3))\n\n\ndef test_conv_backward():\n    """"""\n    \xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    :return:\n    """"""\n    z = np.random.randn(4, 3, 224, 224)\n    K = np.random.randn(3, 64, 3, 3)\n    next_dz = np.random.randn(4, 64, 224, 224)\n\n    from layers import conv_backward as conv_backward_v1\n    s = time.time()\n    dk1, db1, dz1 = conv_backward_v1(next_dz, K, z, padding=(1, 1))\n    print(""v1 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n    s = time.time()\n    dk2, db2, dz2 = conv_backward(next_dz, K, z, padding=(1, 1))\n    print(""v2 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n\n    print(np.allclose(dk1, dk2),\n          np.allclose(db1, db2),\n          np.allclose(dz1, dz2))\n\n\ndef test_max_pooling():\n    """"""\n    \xe6\xb1\xa0\xe5\x8c\x96\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x80\x9f\xe5\xba\xa6\xe5\xb7\xae\xe6\x95\xb0\xe5\x8d\x81\xe5\x80\x8d\n    :return:\n    """"""\n    z = np.random.randn(4, 24, 224, 224)\n    from layers import max_pooling_forward_bak\n    s = time.time()\n    o1 = max_pooling_forward_bak(z, (2, 2))\n    print(""max pooling v1 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n    s = time.time()\n    o2 = max_pooling_forward(z, (2, 2))\n    print(""max pooling v2 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n\n    print(np.allclose(o1, o2))\n\n\ndef test_max_pooling_backward():\n    """"""\n    \xe6\xb1\xa0\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x80\x9f\xe5\xba\xa6\xe5\xb7\xae\xe7\xba\xa6\xe5\x8d\x81\xe5\x80\x8d\n    :return:\n    """"""\n    next_dz = np.random.randn(4, 24, 112, 112)\n    z = np.random.randn(4, 24, 224, 224)\n    from layers import max_pooling_backward_bak\n    s = time.time()\n    o1 = max_pooling_backward_bak(next_dz, z, (2, 2))\n    print(""max pooling backward v1 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n    s = time.time()\n    o2 = max_pooling_backward(next_dz, z, (2, 2))\n    print(""max pooling backward v2 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n\n    print(np.allclose(o1, o2))\n\n\ndef test_global_avg_pooling_backward():\n    """"""\n    \xe6\xb1\xa0\xe5\x8c\x96\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x80\x9f\xe5\xba\xa6\xe5\xb7\xae\xe7\xba\xa6\xe5\x8d\x81\xe5\x80\x8d\n    :return:\n    """"""\n    next_dz = np.random.randn(32, 512)\n    z = np.random.randn(32, 512, 7, 7)\n    from layers import global_avg_pooling_backward as global_avg_pooling_backward_v1\n    s = time.time()\n    o1 = global_avg_pooling_backward_v1(next_dz, z)\n    print(""global avg pooling backward v1 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n    s = time.time()\n    o2 = global_avg_pooling_backward(next_dz, z)\n    print(""global avg pooling backward v2 \xe8\x80\x97\xe6\x97\xb6:{}"".format(time.time() - s))\n\n    print(np.allclose(o1, o2))\n\n\nif __name__ == \'__main__\':\n    # test_single_conv()\n    # test_conv()\n    test_conv_backward()\n    # test_max_pooling()\n    # test_max_pooling_backward()\n    # test_global_avg_pooling_backward()\n'"
nn/load_mnist.py,0,"b'# Author: Thang Vu\n# Date: 25/Nove/2017\n# Description: Load datasets\n\nimport gzip\nfrom six.moves import cPickle as pickle\nimport os\nimport platform\n\n\n# load pickle based on python version 2 or 3\ndef load_pickle(f):\n    version = platform.python_version_tuple()\n    if version[0] == \'2\':\n        return pickle.load(f)\n    elif version[0] == \'3\':\n        return pickle.load(f, encoding=\'latin1\')\n    raise ValueError(""invalid python version: {}"".format(version))\n\n\ndef load_mnist_datasets(path=\'data/mnist.pkl.gz\'):\n    if not os.path.exists(path):\n        raise Exception(\'Cannot find %s\' % path)\n    with gzip.open(path, \'rb\') as f:\n        train_set, val_set, test_set = load_pickle(f)\n        return train_set, val_set, test_set\n'"
nn/losses.py,5,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on 2018/8/19 15:03\r\n\r\n@author: mick.yi\r\n\r\n\xe5\xae\x9a\xe4\xb9\x89\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\r\n""""""\r\nimport numpy as np\r\n\r\n\r\ndef mean_squared_loss(y_predict, y_true):\r\n    """"""\r\n    \xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\r\n    :param y_predict: \xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc,shape (N,d)\xef\xbc\x8cN\xe4\xb8\xba\xe6\x89\xb9\xe9\x87\x8f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\r\n    :param y_true: \xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\xbc\r\n    :return:\r\n    """"""\r\n    loss = np.mean(np.sum(np.square(y_predict - y_true), axis=-1))  # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc\r\n    dy = y_predict - y_true  # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x85\xb3\xe4\xba\x8e\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\r\n    return loss, dy\r\n\r\n\r\ndef cross_entropy_loss(y_predict, y_true):\r\n    """"""\r\n    \xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\r\n    :param y_predict: \xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc,shape (N,d)\xef\xbc\x8cN\xe4\xb8\xba\xe6\x89\xb9\xe9\x87\x8f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\r\n    :param y_true: \xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\xbc,shape(N,d)\r\n    :return:\r\n    """"""\r\n\r\n    y_shift = y_predict - np.max(y_predict, axis=-1, keepdims=True)\r\n    y_exp = np.exp(y_shift)\r\n    y_probability = y_exp / np.sum(y_exp, axis=-1,keepdims=True)\r\n    loss = np.mean(np.sum(-y_true * np.log(y_probability), axis=-1))  # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\r\n    dy = y_probability - y_true\r\n    return loss, dy\r\n'"
nn/main.py,1,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on 2018/9/01 15:03\n\n@author: mick.yi\n\n\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa1\x88\xe4\xbe\x8b\n""""""\nimport numpy as np\nfrom dnn import Mnist, LinearRegression\n\nfrom load_mnist import load_mnist_datasets\nimport utils\n\n\ndef dnn_mnist():\n    # load datasets\n    path = \'mnist.pkl.gz\'\n    train_set, val_set, test_set = load_mnist_datasets(path)\n    X_train, y_train = train_set\n    X_val, y_val = val_set\n    X_test, y_test = test_set\n\n    # \xe8\xbd\xac\xe4\xb8\xba\xe7\xa8\x80\xe7\x96\x8f\xe5\x88\x86\xe7\xb1\xbb\n    y_train, y_val,y_test =utils.to_categorical(y_train,10),utils.to_categorical(y_val,10),utils.to_categorical(y_test,10)\n\n    # bookeeping for best model based on validation set\n    best_val_acc = -1\n    mnist = Mnist()\n\n    # Train\n    batch_size = 32\n    lr = 1e-1\n    for epoch in range(10):\n        num_train = X_train.shape[0]\n        num_batch = num_train // batch_size\n        for batch in range(num_batch):\n            # get batch data\n            batch_mask = np.random.choice(num_train, batch_size)\n            X_batch = X_train[batch_mask]\n            y_batch = y_train[batch_mask]\n            # \xe5\x89\x8d\xe5\x90\x91\xe5\x8f\x8a\xe5\x8f\x8d\xe5\x90\x91\n            mnist.forward(X_batch)\n            loss = mnist.backward(X_batch, y_batch)\n            if batch % 200 == 0:\n                print(""Epoch %2d Iter %3d Loss %.5f"" % (epoch, batch, loss))\n\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n            for w in [""W1"", ""b1"", ""W2"", ""b2"", ""W3"", ""b3""]:\n                mnist.weights[w] -= lr * mnist.gradients[w]\n\n        train_acc = mnist.get_accuracy(X_train, y_train)\n        val_acc = mnist.get_accuracy(X_val, y_val)\n\n        if(best_val_acc < val_acc):\n            best_val_acc = val_acc\n\n        # store best model based n acc_val\n        print(\'Epoch finish. \')\n        print(\'Train acc %.3f\' % train_acc)\n        print(\'Val acc %.3f\' % val_acc)\n        print(\'-\' * 30)\n        print(\'\')\n\n    print(\'Train finished. Best acc %.3f\' % best_val_acc)\n    test_acc = mnist.get_accuracy(X_test, y_test)\n    print(\'Test acc %.3f\' % test_acc)\n\n\nif __name__ == \'__main__\':\n    #dnn_mnist()\n    m = LinearRegression()\n    m.train()\n'"
nn/modules.py,16,"b'# -*- coding: utf-8 -*-\n""""""\n @File    : modules.py\n @Time    : 2020/4/18 \xe4\xb8\x8a\xe5\x8d\x888:28\n @Author  : yizuotian\n @Description    :\n""""""\nfrom typing import List\n\nfrom activations import *\nfrom layers import fc_forward, fc_backward, global_avg_pooling_forward, flatten_forward, flatten_backward\nfrom layers_v2 import conv_forward, conv_backward, max_pooling_forward, max_pooling_backward, \\\n    global_avg_pooling_backward\nfrom losses import *\nfrom optimizers import *\n\n\n# pyximport.install()\n# from clayers import *\n\n\nclass BaseModule(object):\n    def __init__(self, name=\'\'):\n        """"""\n\n        :param name: \xe5\xb1\x82\xe5\x90\x8d\n        """"""\n        self.name = name\n        self.weights = dict()  # \xe6\x9d\x83\xe9\x87\x8d\xe5\x8f\x82\xe6\x95\xb0\xe5\xad\x97\xe5\x85\xb8\n        self.gradients = dict()  # \xe6\xa2\xaf\xe5\xba\xa6\xe5\xad\x97\xe5\x85\xb8\n        self.in_features = None  # \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84feature map\n\n    def forward(self, x):\n        pass\n\n    def backward(self, in_gradient):\n        pass\n\n    def update_gradient(self, lr):\n        pass\n\n    def load_weights(self, weights):\n        """"""\n        \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9d\x83\xe9\x87\x8d\n        :param weights:\n        :return:\n        """"""\n        for key in self.weights.keys():\n            self.weights[key] = weights[key]\n\n\nclass Model(BaseModule):\n    """"""\n    \xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\n    """"""\n\n    def __init__(self, layers: List[BaseModule], **kwargs):\n        super(Model, self).__init__(**kwargs)\n        self.layers = layers\n        # \xe6\x94\xb6\xe9\x9b\x86\xe6\x89\x80\xe6\x9c\x89\xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe6\xa2\xaf\xe5\xba\xa6\n        for l in self.layers:\n            self.weights.update(l.weights)\n            self.gradients.update(l.gradients)\n\n    def forward(self, x):\n        for l in self.layers:\n            x = l.forward(x)\n            # print(\'forward layer:{},feature:{}\'.format(l.name, np.max(x)))\n        # \xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbf\x94\xe5\x9b\x9e\n        return x\n\n    def backward(self, in_gradient):\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        for l in self.layers[::-1]:\n            in_gradient = l.backward(in_gradient)\n            # print(\'backward layer:{},gradient:{}\'.format(l.name, np.max(in_gradient)))\n\n    def update_gradient(self, lr):\n        for l in self.layers:\n            l.update_gradient(lr)\n\n    def load_weights(self, weights):\n        """"""\n        \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\x83\xe9\x87\x8d\n        :param weights:\n        :return:\n        """"""\n        # \xe9\x80\x90\xe5\xb1\x82\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9d\x83\xe9\x87\x8d\n        for l in self.layers:\n            l.load_weights(weights)\n\n\nclass Linear(BaseModule):\n    """"""\n    \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    """"""\n\n    def __init__(self, in_units, out_units, **kwargs):\n        """"""\n\n        :param in_units: \xe8\xbe\x93\xe5\x85\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe6\x95\xb0\n        :param out_units: \xe8\xbe\x93\xe5\x87\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe6\x95\xb0\n        """"""\n        super(Linear, self).__init__(**kwargs)\n        # \xe6\x9d\x83\xe9\x87\x8d\xe5\x8f\x82\xe6\x95\xb0\n        weight = np.random.randn(in_units, out_units) * np.sqrt(2 / in_units)\n        bias = np.zeros(out_units)\n        # \xe6\x9d\x83\xe9\x87\x8d\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        g_weight = np.zeros_like(weight)\n        g_bias = np.zeros_like(bias)\n        # \xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\xad\x97\xe5\x85\xb8\n        self.weights = {""{}_weight"".format(self.name): weight,\n                        ""{}_bias"".format(self.name): bias}\n\n        self.gradients = {""{}_weight"".format(self.name): g_weight,\n                          ""{}_bias"".format(self.name): g_bias}\n\n    @property\n    def weight(self):\n        return self.weights[""{}_weight"".format(self.name)]\n\n    @property\n    def bias(self):\n        return self.weights[""{}_bias"".format(self.name)]\n\n    def set_gradient(self, name, gradient):\n        """"""\n        \xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n        :param name: weight \xe6\x88\x96 bias \xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\n        :param gradient:\n        :return:\n        """"""\n        self.gradients[""{}_{}"".format(self.name, name)] = gradient\n\n    def forward(self, x):\n        """"""\n\n        :param x: [B,in_units]\n        :return output: [B,out_units]\n        """"""\n        self.in_features = x\n        output = fc_forward(x, self.weight, self.bias)\n        return output\n\n    def backward(self, in_gradient):\n        """"""\n        \xe6\xa2\xaf\xe5\xba\xa6\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        :param in_gradient: \xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbc\xa0\xe9\x80\x92\xe8\xbf\x87\xe6\x9d\xa5\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c[B,out_units]\n        :return out_gradient: \xe4\xbc\xa0\xe9\x80\x92\xe7\xbb\x99\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c[B,in_units]\n        """"""\n        g_weight, g_bias, out_gradient = fc_backward(in_gradient,\n                                                     self.weight,\n                                                     self.in_features)\n        self.set_gradient(\'weight\', g_weight)\n        self.set_gradient(\'bias\', g_bias)\n        return out_gradient\n\n    def update_gradient(self, lr):\n        """"""\n        \xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n        :param lr:\n        :return:\n        """"""\n        self.weight -= self.g_weight * lr\n        self.bias -= self.g_bias * lr\n\n\nclass Conv2D(BaseModule):\n    """"""\n    2D\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n    """"""\n\n    def __init__(self, in_filters, out_filters, kernel=(3, 3), padding=(1, 1), stride=(1, 1), **kwargs):\n        super(Conv2D, self).__init__(**kwargs)\n        self.in_filters = in_filters\n        self.out_filters = out_filters\n        self.kernel = kernel\n        self.padding = padding\n        self.stride = stride\n\n        # \xe6\x9d\x83\xe9\x87\x8d\xe5\x8f\x82\xe6\x95\xb0\n        fan_in = in_filters * kernel[0] * kernel[1]  # \xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\xe9\x87\x8f\n        fan_out = out_filters * kernel[0] * kernel[1]  # \xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\xe9\x87\x8f\n        weight = np.random.randn(in_filters,\n                                 out_filters,\n                                 *kernel) * np.sqrt(2 / (fan_in + fan_out))\n        bias = np.zeros(out_filters)\n        # \xe6\xa2\xaf\xe5\xba\xa6\n        g_weight = np.zeros_like(weight)\n        g_bias = np.zeros_like(bias)\n        # \xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\xad\x97\xe5\x85\xb8\n        self.weights = {""{}_weight"".format(self.name): weight,\n                        ""{}_bias"".format(self.name): bias}\n\n        self.gradients = {""{}_weight"".format(self.name): g_weight,\n                          ""{}_bias"".format(self.name): g_bias}\n\n    @property\n    def weight(self):\n        return self.weights[""{}_weight"".format(self.name)]\n\n    @property\n    def bias(self):\n        return self.weights[""{}_bias"".format(self.name)]\n\n    def set_gradient(self, name, gradient):\n        """"""\n        \xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n        :param name: weight \xe6\x88\x96 bias \xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\n        :param gradient:\n        :return:\n        """"""\n        self.gradients[""{}_{}"".format(self.name, name)] = gradient\n\n    def forward(self, x):\n        """"""\n\n        :param x: [B,in_filters,H,W]\n        :return output:  [B,out_filters,H,W]\n        """"""\n        self.in_features = x\n        output = conv_forward(x, self.weight, self.bias, self.padding, self.stride)\n        return output\n\n    def backward(self, in_gradient):\n        """"""\n\n        :param in_gradient: \xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbc\xa0\xe9\x80\x92\xe8\xbf\x87\xe6\x9d\xa5\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c[B,out_filters,H,W]\n        :return out_gradient: \xe4\xbc\xa0\xe9\x80\x92\xe7\xbb\x99\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c[B,in_filters,H,W]\n        """"""\n        g_weight, g_bias, out_gradient = conv_backward(in_gradient,\n                                                       self.weight,\n                                                       self.in_features,\n                                                       self.padding, self.stride)\n        self.set_gradient(\'weight\', g_weight)\n        self.set_gradient(\'bias\', g_bias)\n        return out_gradient\n\n    def update_gradient(self, lr):\n        self.weight -= self.g_weight * lr\n        self.bias -= self.g_bias * lr\n\n\nclass ReLU(BaseModule):\n    def __init__(self, **kwargs):\n        super(ReLU, self).__init__(**kwargs)\n\n    def forward(self, x):\n        self.in_features = x\n        return relu_forward(x)\n\n    def backward(self, in_gradient):\n        """"""\n\n        :param in_gradient: \xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbc\xa0\xe9\x80\x92\xe8\xbf\x87\xe6\x9d\xa5\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        :return out_gradient: \xe4\xbc\xa0\xe9\x80\x92\xe7\xbb\x99\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        """"""\n        out_gradient = relu_backward(in_gradient, self.in_features)\n        return out_gradient\n\n\nclass MaxPooling2D(BaseModule):\n    """"""\n    \xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n    """"""\n\n    def __init__(self, kernel=(2, 2), stride=(2, 2), padding=(0, 0), **kwargs):\n        """"""\n\n        :param kernel: \xe6\xb1\xa0\xe5\x8c\x96\xe5\xb0\xba\xe5\xaf\xb8\n        :param stride: \xe6\xad\xa5\xe9\x95\xbf\n        :param padding: padding\n        :param kwargs:\n        """"""\n        super(MaxPooling2D, self).__init__(**kwargs)\n        self.kernel = kernel\n        self.stride = stride\n        self.padding = padding\n\n    def forward(self, x):\n        """"""\n\n        :param x: [B,C,H,W]\n        :return output : [B,C,H\',W\']\n        """"""\n        self.in_features = x\n        output = max_pooling_forward(x, self.kernel, self.stride, self.padding)\n        return output\n\n    def backward(self, in_gradient):\n        """"""\n\n        :param in_gradient: \xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbc\xa0\xe9\x80\x92\xe8\xbf\x87\xe6\x9d\xa5\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        :return out_gradient: \xe4\xbc\xa0\xe9\x80\x92\xe7\xbb\x99\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        """"""\n        out_gradient = max_pooling_backward(in_gradient,\n                                            self.in_features,\n                                            self.kernel,\n                                            self.stride,\n                                            self.padding)\n        return out_gradient\n\n\nclass GlobalAvgPooling2D(BaseModule):\n    """"""\n    \xe5\x85\xa8\xe5\xb1\x80\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\n    """"""\n\n    def __init__(self, **kwargs):\n        super(GlobalAvgPooling2D, self).__init__(**kwargs)\n\n    def forward(self, x):\n        """"""\n\n        :param x: [B,C,H,W]\n        :return output : [B,C,H\',W\']\n        """"""\n        self.in_features = x\n        output = global_avg_pooling_forward(x)\n        return output\n\n    def backward(self, in_gradient):\n        """"""\n\n        :param in_gradient: \xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbc\xa0\xe9\x80\x92\xe8\xbf\x87\xe6\x9d\xa5\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        :return out_gradient: \xe4\xbc\xa0\xe9\x80\x92\xe7\xbb\x99\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        """"""\n        out_gradient = global_avg_pooling_backward(in_gradient,\n                                                   self.in_features)\n        return out_gradient\n\n\nclass Flatten(BaseModule):\n    """"""\n    \xe6\x89\x93\xe5\xb9\xb3\xe5\xb1\x82\n    """"""\n\n    def __init__(self, **kwargs):\n        super(Flatten, self).__init__(**kwargs)\n\n    def forward(self, x):\n        self.in_features = x\n        return flatten_forward(x)\n\n    def backward(self, in_gradient):\n        """"""\n\n        :param in_gradient: \xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbc\xa0\xe9\x80\x92\xe8\xbf\x87\xe6\x9d\xa5\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        :return out_gradient: \xe4\xbc\xa0\xe9\x80\x92\xe7\xbb\x99\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        """"""\n        out_gradient = flatten_backward(in_gradient, self.in_features)\n        return out_gradient\n\n\ndef test_linear():\n    # \xe5\xae\x9e\xe9\x99\x85\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe5\x81\x8f\xe7\xbd\xae\n    W = np.array([[3, 7, 4],\n                  [5, 2, 6]])\n    b = np.array([2, 9, 3])\n    # \xe4\xba\xa7\xe7\x94\x9f\xe8\xae\xad\xe7\xbb\x83\xe6\xa0\xb7\xe6\x9c\xac\n    x_data = np.random.randn(500, 2)\n    y_data = np.dot(x_data, W) + b\n\n    def next_sample(batch_size=1):\n        idx = np.random.randint(500)\n        return x_data[idx:idx + batch_size], y_data[idx:idx + batch_size]\n\n    fc_layer = Linear(2, 3, name=\'fc1\')\n    # fc_layer.weights[\'fc1_weight\'] *= 1e-2  # \xe5\x8d\x95\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xa6\x81\xe5\xb0\x8f\n    m = Model([fc_layer])\n    sgd = SGD(m.weights, lr=1e-3)\n    i = 0\n    loss = 1\n    while loss > 1e-15:\n        x, y_true = next_sample(4)  # \xe8\x8e\xb7\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe6\xa0\xb7\xe6\x9c\xac\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        y = m.forward(x)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n        loss, dy = mean_squared_loss(y, y_true)\n        m.backward(dy)\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n        sgd.iterate(m)\n\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\n        i += 1\n        if i % 10000 == 0:\n            print(""y_pred\xef\xbc\x9a{},y_true:{}"".format(y, y_true))\n            print(""\\n\xe8\xbf\xad\xe4\xbb\xa3{}\xe6\xac\xa1\xef\xbc\x8c\xe5\xbd\x93\xe5\x89\x8dloss:{}, \xe5\xbd\x93\xe5\x89\x8d\xe6\x9d\x83\xe9\x87\x8d:{},\xe5\xbd\x93\xe5\x89\x8d\xe5\x81\x8f\xe7\xbd\xae{},\xe6\xa2\xaf\xe5\xba\xa6:{}"".format(i, loss,\n                                                                   m.layers[0].weight,\n                                                                   m.layers[0].bias,\n                                                                   m.layers[0].gradients))\n            # print(m.weights)\n\n    print(\'\xe8\xbf\xad\xe4\xbb\xa3{}\xe6\xac\xa1,\xe5\xbd\x93\xe5\x89\x8d\xe6\x9d\x83\xe9\x87\x8d:{} \'.format(i, m.layers[0].weights))\n\n\nif __name__ == \'__main__\':\n    test_linear()\n'"
nn/optimizers.py,5,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on 2018/9/4 22:26\r\n\r\n@author: mick.yi\r\n\r\n\xe4\xbc\x98\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\r\n\r\n""""""\r\nimport numpy as np\r\n\r\nfrom modules import Model\r\n\r\n\r\ndef _copy_weights_to_zeros(weights):\r\n    result = {}\r\n    result.keys()\r\n    for key in weights.keys():\r\n        result[key] = np.zeros_like(weights[key])\r\n    return result\r\n\r\n\r\nclass SGD(object):\r\n    """"""\r\n    \xe5\xb0\x8f\xe6\x89\xb9\xe9\x87\x8f\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\r\n    """"""\r\n\r\n    def __init__(self, weights, lr=0.01, momentum=0.9, decay=1e-5):\r\n        """"""\r\n\r\n        :param weights: \xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe5\xad\x97\xe5\x85\xb8\xe7\xb1\xbb\xe5\x9e\x8b\r\n        :param lr: \xe5\x88\x9d\xe5\xa7\x8b\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\r\n        :param momentum: \xe5\x8a\xa8\xe9\x87\x8f\xe5\x9b\xa0\xe5\xad\x90\r\n        :param decay: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\r\n        """"""\r\n        self.v = _copy_weights_to_zeros(weights)  # \xe7\xb4\xaf\xe7\xa7\xaf\xe5\x8a\xa8\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\r\n        self.iterations = 0  # \xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\r\n        self.lr = self.init_lr = lr\r\n        self.momentum = momentum\r\n        self.decay = decay\r\n\r\n    def iterate(self, m: Model):\r\n        """"""\r\n        \xe8\xbf\xad\xe4\xbb\xa3\xe4\xb8\x80\xe6\xac\xa1\r\n        :param m: \xe6\xa8\xa1\xe5\x9e\x8b\r\n        :return:\r\n        """"""\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\r\n        self.lr = self.init_lr / (1 + self.iterations * self.decay)\r\n\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8a\xa8\xe9\x87\x8f\xe5\x92\x8c\xe6\xa2\xaf\xe5\xba\xa6\r\n        for layer in m.layers:\r\n            for key in layer.weights.keys():\r\n                self.v[key] = self.momentum * self.v[key] + self.lr * layer.gradients[key]\r\n                layer.weights[key] -= self.v[key]\r\n\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\r\n        self.iterations += 1\r\n\r\n\r\nclass AdaGrad(object):\r\n    def __init__(self, weights, lr=0.01, epsilon=1e-6, decay=0):\r\n        """"""\r\n\r\n        :param weights: \xe6\x9d\x83\xe9\x87\x8d\r\n        :param lr: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\r\n        :param epsilon: \xe5\xb9\xb3\xe6\xbb\x91\xe6\x95\xb0\r\n        :param decay: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\r\n        """"""\r\n        self.s = _copy_weights_to_zeros(weights)  # \xe6\x9d\x83\xe9\x87\x8d\xe5\xb9\xb3\xe6\x96\xb9\xe5\x92\x8c\xe7\xb4\xaf\xe5\x8a\xa0\xe9\x87\x8f\r\n        self.iterations = 0  # \xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\r\n        self.lr = self.init_lr = lr\r\n        self.epsilon = epsilon\r\n        self.decay = decay\r\n\r\n    def iterate(self, m: Model):\r\n        """"""\r\n        \xe8\xbf\xad\xe4\xbb\xa3\xe4\xb8\x80\xe6\xac\xa1\r\n        :param m: \xe6\xa8\xa1\xe5\x9e\x8b\r\n        :return:\r\n        """"""\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\r\n        self.lr = self.init_lr / (1 + self.iterations * self.decay)\r\n\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\xe5\xb9\xb3\xe6\x96\xb9\xe5\x92\x8c\xe7\xb4\xaf\xe5\x8a\xa0\xe9\x87\x8f \xe5\x92\x8c \xe6\xa2\xaf\xe5\xba\xa6\r\n        for layer in m.layers:\r\n            for key in layer.weights.keys():\r\n                self.s[key] += np.square(layer.weights[key])\r\n                layer.weights[key] -= self.lr * layer.gradients[key] / np.sqrt(self.s[key] + self.epsilon)\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\r\n        self.iterations += 1\r\n\r\n\r\nclass RmsProp(object):\r\n    def __init__(self, weights, gamma=0.9, lr=0.01, epsilon=1e-6, decay=0):\r\n        """"""\r\n\r\n        :param weights: \xe6\x9d\x83\xe9\x87\x8d\r\n        :param gamma: \xe6\x8c\x87\xe6\x95\xb0\r\n        :param lr: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\r\n        :param epsilon: \xe5\xb9\xb3\xe6\xbb\x91\xe6\x95\xb0\r\n        :param decay: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\r\n        """"""\r\n        self.s = _copy_weights_to_zeros(weights)  # \xe6\x9d\x83\xe9\x87\x8d\xe5\xb9\xb3\xe6\x96\xb9\xe5\x92\x8c\xe7\xb4\xaf\xe5\x8a\xa0\xe9\x87\x8f\r\n        self.gamma = gamma\r\n        self.iterations = 0  # \xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\r\n        self.lr = self.init_lr = lr\r\n        self.epsilon = epsilon\r\n        self.decay = decay\r\n\r\n    def iterate(self, m: Model):\r\n        """"""\r\n        \xe8\xbf\xad\xe4\xbb\xa3\xe4\xb8\x80\xe6\xac\xa1\r\n        :param m: \xe6\xa8\xa1\xe5\x9e\x8b\r\n        :return:\r\n        """"""\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\r\n        self.lr = self.init_lr / (1 + self.iterations * self.decay)\r\n\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe6\x9d\x83\xe9\x87\x8d\xe5\xb9\xb3\xe6\x96\xb9\xe5\x92\x8c\xe7\xb4\xaf\xe5\x8a\xa0\xe9\x87\x8f \xe5\x92\x8c \xe6\xa2\xaf\xe5\xba\xa6\r\n        for layer in m.layers:\r\n            for key in layer.weights.keys():\r\n                self.s[key] = self.gamma * self.s[key] + (1 - self.gamma) * np.square(layer.weights[key])\r\n                layer.weights[key] -= self.lr * layer.gradients[key] / np.sqrt(self.s[key] + self.epsilon)\r\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\r\n        self.iterations += 1\r\n'"
nn/utils.py,17,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nCreated on 2018/9/2 9:30\r\n\r\n@author: mick.yi\r\n\xe5\xb7\xa5\xe5\x85\xb7\xe7\xb1\xbb\r\n""""""\r\nimport pickle\r\nfrom six.moves import cPickle\r\nimport numpy as np\r\nimport os\r\n\r\n\r\ndef to_categorical(y, num_classes=None):\r\n    """"""\xe4\xbb\x8ekeras\xe4\xb8\xad\xe5\xa4\x8d\xe5\x88\xb6\xe8\x80\x8c\xe6\x9d\xa5\r\n    Converts a class vector (integers) to binary class matrix.\r\n\r\n    E.g. for use with categorical_crossentropy.\r\n\r\n    # Arguments\r\n        y: class vector to be converted into a matrix\r\n            (integers from 0 to num_classes).\r\n        num_classes: total number of classes.\r\n\r\n    # Returns\r\n        A binary matrix representation of the input. The classes axis\r\n        is placed last.\r\n    """"""\r\n    y = np.array(y, dtype=\'int\')\r\n    input_shape = y.shape\r\n    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\r\n        input_shape = tuple(input_shape[:-1])\r\n    y = y.ravel()\r\n    if not num_classes:\r\n        num_classes = np.max(y) + 1\r\n    n = y.shape[0]\r\n    categorical = np.zeros((n, num_classes), dtype=np.float32)\r\n    categorical[np.arange(n), y] = 1\r\n    output_shape = input_shape + (num_classes,)\r\n    categorical = np.reshape(categorical, output_shape)\r\n    return categorical\r\n\r\n\r\ndef save_weights(file_path, weights):\r\n    """"""\r\n    \xe4\xbf\x9d\xe5\xad\x98\xe6\x9d\x83\xe9\x87\x8d\r\n    :param file_path:\r\n    :param weights:\r\n    :return:\r\n    """"""\r\n    f = open(file_path, \'wb\')\r\n    pickle.dump(weights, f)\r\n    f.close()\r\n\r\n\r\ndef load_weights(file_path):\r\n    """"""\r\n    \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9d\x83\xe9\x87\x8d\r\n    :param file_path:\r\n    :return:\r\n    """"""\r\n    f = open(file_path, \'rb\')\r\n    weights = pickle.load(f)\r\n    return weights\r\n\r\n\r\ndef load_batch(fpath, label_key=\'labels\'):\r\n    """"""Internal utility for parsing CIFAR data.\r\n\r\n    # Arguments\r\n        fpath: path the file to parse.\r\n        label_key: key for label data in the retrieve\r\n            dictionary.\r\n\r\n    # Returns\r\n        A tuple `(data, labels)`.\r\n    """"""\r\n    with open(fpath, \'rb\') as f:\r\n        d = cPickle.load(f, encoding=\'bytes\')\r\n        # decode utf8\r\n        d_decoded = {}\r\n        for k, v in d.items():\r\n            d_decoded[k.decode(\'utf8\')] = v\r\n        d = d_decoded\r\n    data = d[\'data\']\r\n    labels = d[label_key]\r\n\r\n    data = data.reshape(data.shape[0], 3, 32, 32)\r\n    return data, labels\r\n\r\n\r\ndef load_cifar(path):\r\n    """"""Loads CIFAR10 dataset.\r\n\r\n    # Returns\r\n        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\r\n    """"""\r\n\r\n    num_train_samples = 50000\r\n\r\n    x_train = np.empty((num_train_samples, 3, 32, 32), dtype=\'uint8\')\r\n    y_train = np.empty((num_train_samples,), dtype=\'uint8\')\r\n\r\n    for i in range(1, 6):\r\n        fpath = os.path.join(path, \'data_batch_\' + str(i))\r\n        (x_train[(i - 1) * 10000: i * 10000, :, :, :],\r\n         y_train[(i - 1) * 10000: i * 10000]) = load_batch(fpath)\r\n\r\n    fpath = os.path.join(path, \'test_batch\')\r\n    x_test, y_test = load_batch(fpath)\r\n\r\n    y_train = np.reshape(y_train, (len(y_train), 1))\r\n    y_test = np.reshape(y_test, (len(y_test), 1))\r\n    # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\r\n    # x_train = x_train.astype(np.float) / 255. - 1.\r\n    # x_test = x_test.astype(np.float) / 255. - 1.\r\n    mean = np.array([123.680, 116.779, 103.939])\r\n    x_train = x_train.astype(np.float) - mean[:, np.newaxis, np.newaxis]\r\n    x_test = x_test.astype(np.float) - mean[:, np.newaxis, np.newaxis]\r\n    x_train /= 255.\r\n    x_test /= 255\r\n    std = np.array([0.24580306, 0.24236229, 0.2603115])\r\n    x_train /= std[:, np.newaxis, np.newaxis]\r\n    x_test /= std[:, np.newaxis, np.newaxis]\r\n    return (x_train, to_categorical(y_train)), (x_test, to_categorical(y_test))\r\n'"
nn/vgg.py,2,"b'# -*- coding: utf-8 -*-\n""""""\n @File    : vgg.py\n @Time    : 2020/4/18 \xe4\xb8\x8a\xe5\x8d\x8811:01\n @Author  : yizuotian\n @Description    :\n""""""\nfrom modules import *\n\ncfgs = {\'vgg11\': [1, 1, 2, 2, 2],\n        \'vgg13\': [2, 2, 2, 2, 2],\n        \'vgg16\': [2, 2, 3, 3, 3],\n        \'vgg19\': [2, 2, 4, 4, 4]}\n\n\nclass VGG(Model):\n    """"""\n    VGG \xe6\xa8\xa1\xe5\x9e\x8b\n    """"""\n\n    def __init__(self, image_size=224, in_channels=3, num_classes=10, name=\'\', **kwargs):\n        """"""\n\n        :param image_size: \xe5\x9b\xbe\xe5\x83\x8f\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x8c\xe5\x81\x87\xe5\xae\x9a\xe9\x95\xbf\xe5\xae\xbd\xe4\xb8\x80\xe8\x87\xb4\xef\xbc\x8c\xe4\xb8\x94\xe5\xb0\xba\xe5\xaf\xb8\xe8\x83\xbd\xe5\xa4\x9f\xe8\xa2\xab32\xe6\x95\xb4\xe9\x99\xa4\n        :param in_channels: \xe5\x9b\xbe\xe5\x83\x8f\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        :param num_classes: \xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\n        :param name:\n        :param kwargs:\n        """"""\n        self.image_size = image_size\n        self.num_block_layers = cfgs[name]\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        # block 1~5\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\x88\x86\xe5\xb8\x83\xe4\xb8\xba64\xef\xbc\x8c128\xef\xbc\x8c256\xef\xbc\x8c512\xef\xbc\x8c512\n        self.block_channel_list = [64, 128, 256, 512, 512]\n        layers = self.make_layers()\n        super(VGG, self).__init__(layers, name=name, **kwargs)\n\n    def make_layers(self):\n        layers = []\n        in_filters = self.in_channels\n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n        for b_idx, num_layers in enumerate(self.num_block_layers):\n            out_filters = self.block_channel_list[b_idx]\n            for l_idx in range(num_layers):\n                layers.append(Conv2D(in_filters, out_filters,\n                                     name=\'Conv_{}_{}\'.format(b_idx + 1, l_idx + 1)))\n                layers.append(ReLU(name=\'ReLU_{}_{}\'.format(b_idx + 1, l_idx + 1)))\n                # \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\x8b\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n                in_filters = out_filters\n            # \xe6\xaf\x8f\xe4\xb8\xaablock\xe4\xbb\xa5max pooling\xe7\xbb\x93\xe5\xb0\xbe\n            layers.append(MaxPooling2D(kernel=(2, 2),\n                                       stride=(2, 2),\n                                       name=\'MaxPooling_{}\'.format(b_idx + 1)))\n        # \xe5\x85\xa8\xe5\xb1\x80\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\n        layers.append(GlobalAvgPooling2D(name=\'Global_Avg_Pooling\'))\n        # \xe4\xb8\xa4\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\n        layers.append(Linear(out_filters, 4096, name=\'fc_6\'))\n        layers.append(ReLU(name=\'ReLU_6\'))\n        layers.append(Linear(4096, 4096, name=\'fc_7\'))\n        layers.append(ReLU(name=\'ReLU_7\'))\n        # \xe5\x88\x86\xe7\xb1\xbb\n        layers.append(Linear(4096, self.num_classes, name=\'cls_logit\'))\n        return layers\n\n\ndef test():\n    import time\n    vgg16 = VGG(name=\'vgg11\')\n    start = time.time()\n    y = vgg16.forward(np.random.randn(6, 3, 32, 32))\n    loss, dy = cross_entropy_loss(y, np.abs(np.random.randn(6, 10)))\n    vgg16.backward(dy)\n    print(\'\xe8\x80\x97\xe6\x97\xb6:{}\'.format(time.time() - start))\n\n\nif __name__ == \'__main__\':\n    test()\n'"
