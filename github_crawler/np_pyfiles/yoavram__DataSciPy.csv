file_path,api_count,code
download_data.py,0,"b'try:\n    import keras\nexcept ModuleNotFoundError:\n    from tensorflow import keras\nimport requests\nimport os\nimport shutil\nimport zipfile\nrequests.packages.urllib3.disable_warnings()\n\ndef download_file(url, fname):\n    r = requests.get(url, stream=True, verify=False)\n    with open(fname, \'wb\') as f:\n        shutil.copyfileobj(r.raw, f)\n\n# MNIST and Fasihon MNIST, day 2 and 3\nprint(\'* MNIST (Keras)...\')\nkeras.datasets.mnist.load_data()\nprint(\'* Fashion-MNIST (Keras)...\')\nkeras.datasets.fashion_mnist.load_data()\n\n# ResNet50, day 3\nprint(\'* ResNet50...\')\nkeras.applications.resnet50.ResNet50(weights=\'imagenet\')\n\n# ESC-50, day 3\nurl = \'https://github.com/karoldvl/ESC-50/archive/master.zip\'\nfname = \'data/ESC.zip\'\nprint(\'* ESC-50...\')\nif not os.path.exists(fname):\n\tdownload_file(url, fname)\ntry:\n    with zipfile.ZipFile(fname) as z:\n        z.extractall(\'data/\')\nexcept zipfile.BadZipFile:\n    print(""Problem with file {}, delete it and try again"".format(fname))\n\n# SpeechEmotion, day 3\nurl = \'https://github.com/yoavram/SpeechEmotion/archive/master.zip\'\nfname = \'data/SpeechEmotion.zip\'\nprint(\'* SpeechEmotion...\')\nif not os.path.exists(fname):\n\tdownload_file(url, fname)\nwith zipfile.ZipFile(fname) as z:\n    z.extractall(\'data/\')\n\n'"
solutions/FFN.py,1,"b'import numpy as np\nimport scipy.stats\n\n## ReLU\n\ndef ReLU(X):\n    return np.maximum(X, 0.0)\n\ndef dReLU(X):\n    return (X > 0).astype(float)\n\n## Feed Forward with loop\n\ndef feed_forward(Ws, X, keep_prob=1):\n    layers = [X] # input layer\n    for i, W in enumerate(Ws):\n        X = X @ W \n        layers.append(X)\n        if i < len(Ws) - 1:\n            # hidden layer\n            if keep_prob < 1:\n                X = drop(X, keep_prob=keep_prob)\n            X = ReLU(X) \n        else:\n            # readout layer\n            X = softmax(X) \n        layers.append(X)\n    return layers\n\n## Back propagation with loop\n\ndef back_propagation(Ws, X, Y, keep_prob=1):\n    layers = feed_forward(Ws, X, keep_prob=keep_prob) # X1, Z1, X2, Z2, Yhat\n    gradients = []\n    \n    for i in range(len(Ws)):\n        Z = layers.pop()\n        if i == 0:\n            # readout layer, Z=Yhat\n            \xce\xb4 = Z - Y\n            layers.pop()\n        else:\n            # hidden layers, Z = X @ W\n            W = Ws[-i]\n            \xce\xb4 = (\xce\xb4 @ W.T) * dReLU(Z) # \xce\xb4 = \xce\xb4 * W * ReLU(Z)\n        X = layers.pop()\n        dW = X.T @ \xce\xb4 # dC/dW = \xce\xb4 * X\n        gradients.append(dW)\n    \n    gradients.reverse()\n    # sanity checks\n    assert len(gradients) == len(Ws), (len(gradients), len(Ws))\n    for dW, W in zip(gradients, Ws):\n        assert dW.shape == W.shape, (dW.shape, W.shape)\n    return gradients\n'"
solutions/FFN_GenModel.py,8,"b'def feed_forward(Ws, X, keep_prob=1):\n    layers = [X]\n    for i, W in enumerate(Ws):\n        X = X @ W\n        layers.append(X)\n        if i < len(Ws) - 1:\n            # hidden layer\n            if keep_prob < 1:\n                X = drop(X, keep_prob=keep_prob)\n            X = np.tanh(X)\n        else:\n            # readout layer\n            X = softmax(X)\n        layers.append(X)\n    return layers\n\ndef back_propagation(Ws, X, Y, keep_prob=1):\n    layers = feed_forward(Ws, X, keep_prob=keep_prob)\n\n    gradients = []\n    # readout layer\n    Yhat = layers.pop() # softmax layer\n    layers.pop() # linear layer\n    \xce\xb4 = Yhat - Y # derivative of loss wrt softmax layer\n    \xce\xb4 = \xce\xb4[:,np.newaxis,:]\n    X = layers.pop() # previous layer, after Relu\n    X = X[:, :, np.newaxis] \n    dW = (\xce\xb4 * X).mean(axis=0) # \n    gradients.append(dW)\n\n    # hidden layers\n    for W in reversed(Ws[1:]):\n        Z = layers.pop()\n        dZ = dtanh(Z)\n        reverseW = W.T[np.newaxis, :, :]\n        \xce\xb4 = (\xce\xb4 @ reverseW).squeeze() * dZ # squeeze removes dimensions with size 1\n        \xce\xb4 = \xce\xb4[:, np.newaxis, :]\n        X = layers.pop()\n        X = X[:, :, np.newaxis]\n        dW = (\xce\xb4 * X).mean(axis=0)\n        gradients.append(dW)\n \n    gradients = gradients[::-1] # reverse gradients list\n    # sanity checks\n    assert len(gradients) == len(Ws), (len(gradients), len(Ws))\n    for dW, W in zip(gradients, Ws):\n        assert dW.shape == W.shape, (dW.shape, W.shape)\n    return gradients\n\ndef feed_backward(Ws, X):\n    for i, W in enumerate(reversed(Ws)):\n        if i > 0:\n            # hidden layer\n            X = np.arctanh(X) # X = tanh(X)\n        else:\n            # readout layer\n            X = -np.log(X) # X = softmax(X)\n            X /= X.sum() # normalize\n        X = X @ W.T # X = X @ W\n    return X\n'"
solutions/linear-model.py,0,"b'import urllib.request\nimport zipfile\nimport os.path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats\n\n\nfname = \'../data/anage_data.txt\'\nmax_mass = 3e5\n\nif __name__ == \'__main__\':\n\tdf = pd.read_table(fname)\n\tdf = df[df[\'Body mass (g)\'] < max_mass]\n\tdf[\'Body mass ^ 3/4\'] = df[\'Body mass (g)\'] ** (3/4)\n\tprint(\'scatterplot...\')\n\tsns.regplot(\'Body mass ^ 3/4\', \'Metabolic rate (W)\', data=df)\n\tplt.show()\n\n\tprint(\'lmplot...\')\n\tsns.lmplot(x=\'Body mass ^ 3/4\', y=\'Metabolic rate (W)\', hue=\'Class\', data=df, col=\'Class\', \n\t\tsharex=False, sharey=False, scatter_kws=dict(alpha=0.3))\n\tplt.ylabel(\'Metabolic rate (W)\')\n\tplt.show()\n\n\tprint(\'linear regression...\')\n\tfor clazz, grp in df.groupby(\'Class\'):\n\t    res = scipy.stats.linregress(grp[\'Body mass ^ 3/4\'], grp[\'Metabolic rate (W)\'])\n\t    print(""{:s}: W = {:.2g} * g + {:.2g}"".format(clazz, res.slope, res.intercept))\n\t'"
solutions/logistic-model.py,3,"b'def _cross_entropy(X, Y, a):\n    nsamples = Y.shape[0] \n    Z = logodds(X, a)\n    return -sum(\n        -z*(1-y) - np.log(1+np.exp(-z)) \n        for z, y \n        in zip(Z, Y)\n    ) / nsamples\n        \ndef cross_entropy(X, Y, a):\n    Z = logodds(X, a)\n    logliks = -Z * (1 - Y) - np.log(1 + np.exp(-Z))\n    return -logliks.mean()\n\ndef gradient_descent(X, Y, a, \xce\xb7=0.01):\n    nsamples = Y.shape[0]\n    \n    Z = X @ a\n    Yhat = 1 / (1 + np.exp(-Z))\n    \xce\xb4 = Yhat - Y\n    grad = X.T @ \xce\xb4 / nsamples\n    assert grad.shape == a.shape\n    return a - \xce\xb7 * grad'"
solutions/logistic-tennis.py,1,"b'import zipfile\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n\nfilename = \'../data/tennis.zip\'\nplayer = \'Rafael-Nadal\'\nfeatures = [\'player1 aces\', \'player1 double faults\']\nmodel_filename = \'logistic_tennis.model\'\n\nif __name__ == \'__main__\':\n\ttennis_zip = zipfile.ZipFile(filename)    \n\tpath = os.path.join(\'data\', \'{}.csv\')\n\tpath = path.format(player.replace(\' \', \'-\'))\n\twith tennis_zip.open(path) as f:\n\t    df = pd.read_csv(f)\n\n\tdf[\'win\'] = df[\'player1 name\'] == df[\'winner\']\n\ttarget = \'win\'\n\tidx = np.isfinite(df[features]).all(axis=1)\n\tdf_ = df[idx]\n\n\tX_train, X_test, y_train, y_test = model_selection.train_test_split(\n\t\tdf_[features], df_[target], test_size=0.75)\n\n\tmodel = LogisticRegression()\n\tmodel.fit(X_train, y_train)\n\tprint(""Accuracy:"", model.score(X_test, y_test))\n\twith open(model_filename, \'wb\') as f:\n\t\tpickle.dump(model, f)\n\t\tprint(""Saving model to"", model_filename)\n\tprint(\'Prediction:\')\n\tprint(X_test.iloc[0], y_test.iloc[0])\n'"
solutions/softmax-model.py,0,"b""from sklearn.linear_model import LogisticRegression\n\ndef sklearn_softmax_model(X, Y):\n    model = LogisticRegression(C=1e12, multi_class='multinomial', solver='lbfgs', n_jobs=-1)\n    model.fit(X, Y.argmax(axis=1))\n    return model"""
